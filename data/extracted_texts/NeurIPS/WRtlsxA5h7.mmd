# Orthogonal Non-negative Tensor Factorization based Multi-view Clustering

Jing Li

Xidian University

Xi'an, Shaanxi, China

jinglxd@stu.xidian.edu.cn

&Quanxue Gao

Xidian University

Xi'an, Shaanxi, China

qxgao@xidian.edu.cn

&Qianqian Wang

Xidian University

Xi'an, Shaanxi, China

qqwang@xidian.edu.cn

&Ming Yang

Harbin Engineering University

Harbin, Heilongjiang, China

yangmingmath@gmail.com

&Wei Xia

Xidian University

Xi'an, Shaanxi, China

xdweixia@gmail.com

Corresponding author.

###### Abstract

Multi-view clustering (MVC) based on non-negative matrix factorization (NMF) and its variants have attracted much attention due to their advantages in clustering interpretability. However, existing NMF-based multi-view clustering methods perform NMF on each view respectively and ignore the impact of between-view. Thus, they can't well exploit the within-view spatial structure and between-view complementary information. To resolve this issue, we present orthogonal non-negative tensor factorization (Orth-NTF) and develop a novel multi-view clustering based on Orth-NTF with one-side orthogonal constraint. Our model directly performs Orth-NTF on the 3rd-order tensor which is composed of anchor graphs of views. Thus, our model directly considers the between-view relationship. Moreover, we use the tensor Schatten \(p\)-norm regularization as a rank approximation of the 3rd-order tensor which characterizes the cluster structure of multi-view data and exploits the between-view complementary information. In addition, we provide an optimization algorithm for the proposed method and prove mathematically that the algorithm always converges to the stationary KKT point. Extensive experiments on various benchmark datasets indicate that our proposed method is able to achieve satisfactory clustering performance.

## 1 Introduction

As one of the most typical methods in unsupervised learning, clustering has a wide scope of application [26; 4; 1] to assign data to different clusters according to the information describing the objects and their relationships. Non-negative matrix factorization (NMF)  is one of the representative methods of clustering, which is proved to be equivalent to K-means clustering . Despite the widespread use of NMF, there are some drawbacks that have prompted some variants of NMF [8; 5; 27; 9; 3].

In particular, the one-side G-orthogonal NMF  can guarantee the uniqueness of the solution of matrix factorization and has excellent clustering interpretation. Also, Ding _et al._proposed the semi-NMF . The data matrix and one of the factor matrices are unconstrained, which allows semi-NMF to be more suitable for applications where the input data is mixed with positive and negative numbers.

Although the above methods can achieve outstanding clustering performance, they are all single-view clustering methods and cannot be adopted straightforwardly for multi-view clustering.

Multi-view clustering tends to achieve superior performance compared to traditional single-view clustering owing to the capability to leverage the complementary information embedded in the different views. Considering the superiority of MVC and NMF, lots of NMF-based multi-view clustering methods have been proposed [13; 28; 23; 33; 15; 29; 14; 37]. The NMF-based multi-view clustering methods can save time and space because it is unnecessary to construct affinity graphs while graph-based methods have to. However, usually, they decompose the original data matrix directly, which leads to a dramatic reduction in the efficiency of the algorithm when the dimension of the original data is huge.

Inspired by the idea of anchor graph. the above issues can be solve by carrying out NMF on the anchor graph . Due to the fact that the dimension of the anchor graph is considerably smaller than the original affinity graph, it follows that the clustering efficiency can be improved. However, as is well-known, there exist two ways of NMF-based multi-view clustering methods. One is to integrate different views first and then implement the NMF on the integrated matrix; the other is to perform the NMF on different views separately and then integrate the result from each view. Both ways are essentially applications of NMF on a single view, and both need to reduce the multi-view data into matrices in the process, which causes the loss of the original spatial information.

To fix the aforesaid issues, we proposed a novel multi-view clustering based on Orth-NTF with one-side orthogonal constraint. Specifically, Non-negative Matrix Factorization (NMF) is tailored primarily for second-order matrices. When processing third-order tensors, there's a need to first transform the tensor into a matrix before applying NMF. This step can lead to a loss of inherent spatial structural information from the third-order tensor. In contrast, Non-negative Tensor Factorization (NTF) sidesteps this issue. NTF directly decomposes third-order tensors. This ensures that the NTF not only acknowledges the relationships between the views but also harnesses the complementary information they offer. Fig 1 delineates the distinction between traditional NMF-based clustering techniques and our NTF-based approach. Furthermore, by incorporating an orthogonal constraint, our model offers distinct physical interpretability for clustering. This suggests that each row of the indicator matrix contains a single non-zero element, and the position of this element directly corresponds to the label of the respective sample. A large number of experiments have shown that our methods have excellent clustering performance.

The main contributions are summarized below:

* We introduce orthogonal non-negative tensor factorization, which considers the between-view relationship directly. Also, we use tensor Schatten \(p\)-norm regularization to characterize the cluster structure of multi-view data and can exploit the complementary information of between views.
* We regard the anchor graph obtained from the original data as the input of the non-negative matrix factorization, which reduces the complexity of our proposed algorithm considerably.
* We provide an optimization algorithm for the proposed method and prove it always converges to the KKT stationary point mathematically. The effectiveness of its application on tensorial G-orthogonal non-negative matrix factorization is demonstrated by extensive experiments.

## 2 Related work

In recent years, multi-view clustering (MVC) has received increasing attention due to its excellent clustering performance. Also, non-negative matrix factorization (NMF) is an efficient technique in single-view clustering, which can generate excellent clustering results that are easy to interpret, and many NMF-based variants have been proposed. Therefore, multi-view clustering-based NMF and its variants have attracted tremendous interest recently.

As the first investigation of the multi-view clustering method based on joint NMF, multiNMF  implements NMF at each view and pushes the different clustering results of each view to a consensus. It provides a new viewpoint for the subsequent NMF-based MVC methods. Influenced by multiNMF, He _et al._proposed a multi-view clustering method combining NMF with similarity . It implements NMF on each view as in multiNMF. In addition, it sets a weight for a different view and introducesa similarity matrix of data points to extract consistent information from different views. To better detect the geometric structure of inner-view space, Wang _et al._ introduced graph regularization into the NMF-based multi-view clustering method to improve clustering performance. Considering the above work, Wang _et al._ proposed a graph regularization multi-view clustering method based on concept factorization (CF). CF is a variant of NMF and it is suitable for handling data containing negative.

As the size of data grows, lots of methods to accelerate matrix factorization are presented. Wang _et al._ proposed a fast non-negative matrix triple factorization method. It constrains the factor matrix of NMF to a clustering indicator matrix, thereby avoiding the post-processing of the factor matrix. Inspired by the work of Wang, Han _et al._ constrained the intermediate factor matrix in the triple factorization to a diagonal matrix, reducing the number of matrix multiplications in the solution process. Another idea to deal with large-scale multi-view data is to introduce anchor graphs, since the number of anchor points is much smaller than the number of original data, multi-view clustering methods based on anchor graphs tend to reduce the computational complexity and thus are able to deal with large-scale data [22; 17; 21]. Considering that previous NMF-based multi-view clustering methods are performed directly on the original data, Yang _et al._ introduced an anchor graph as the input of G-orthogonal NMF. The efficiency of matrix factorization is indeed improved due to the introduction of anchor graph.

Despite the fact that existing NMF-based multi-view clustering methods can perform the clustering tasks excellently, they apply NMF to each view independently. Subsequently, they combine the low-dimensional representations from different perspectives to arrive at a unified shared representation. This approach often overlooks the interrelationships between the views, which are crucial for clustering.

## 3 Notations

We introduce the notations used throughout this paper. We use bold calligraphy letters for 3rd-order tensors, \(}^{n_{1} n_{2} n_{3}}\), bold upper case letters for matrices, \(\), bold lower case letters for vectors, \(\), and lower case letters such as \(h_{ijk}\) for the entries of \(}\). Moreover, the \(i\)-th frontal slice of \(}\) is \(}^{(i)}\). \(}}\) is the discrete Fourier transform (DFT) of \(}\) along the third dimension, \(}}=(},[\ ],3)\). Thus, \(}=(}},[\ ],3)\). The trace and transpose of matrix \(\) are expressed as \(()\) and \(^{}\). The F-norm of \(}\) is denoted by \(\|}\|_{F}\).

**Definition 1** (t-product ).: _Suppose \(}^{n_{1} m n_{3}}\) and \(}^{m n_{2} n_{3}}\), the t-product \(}*}^{n_{1} n_{2} n_{3}}\) is given by_

\[}*}=((}}),[\ ],3),\]

_where \(}=(}})\) and it denotes the block diagonal matrix. The blocks of \(}\) are frontal slices of \(}}\)._

**Definition 2**.: _[_12_]_ _Given \(}^{n_{1} n_{2} n_{3}}\), \(h=(n_{1},n_{2})\), the tensor Schatten \(p\)-norm of \(}\) is defined as_

\[\|}\|_{}}= (_{i=1}^{n_{3}}\|}}^{(i)}_{ }}\|_{ }}^{p})^{}=(_{i= 1}^{n_{3}}_{j=1}^{h}_{j}}}^{( i)}^{p})^{},\] (1)

_where, \(0<p 1\), \(_{j}(}}^{(i)})\) denotes the \(j\)-th singular value of \(}}^{(i)}\)._

It should be pointed out that for \(0<p 1\) when \(p\) is appropriately chosen, the Schatten \(p\)-norm provides quite effective improvements for a tighter approximation of the rank function [39; 36].

## 4 Methodology

### Motivation and Objective

Non-negative matrix factorization (NMF) was initially presented as a dimensionality reduction method, and it is commonly employed as an efficient latent feature learning technique recently. Generally speaking, given a non-negative matrix \(\), the target of NMF is to decompose \(\) into two non-negative matrices,

\[^{}\] (2)

where \(_{+}^{n p}\), \(_{+}^{n k}\) and \(_{+}^{p k}\). \(_{+}^{n p}\) means \(n\)-by-\(p\) matrices with elements are all nonnegative. \(n\) and \(k\) means the number of samples and the number of clusters, respectively.

In order to approximate the matrix before and after factorization, \(_{2}\)-norm and F-norm are frequently adopted as the objective function for the NMF. Considering that F-norm can make the model optimization easier, we use F-norm to construct the objective function.

With the extensive use of NMF, more and more variants of NMF have emerged, among which are G-orthogonal NMF  and Semi-NMF . By imposing an orthogonality constraint on one of the factor matrices in NMF, we obtain the objective function of the one-side orthogonal NMF,

\[_{ 0, 0}\|- ^{}\|_{F}^{2},^{ }=.\] (3)

If we relax the nonnegative constraint on one of the factor matrices in the NMF and the input matrix \(\) can also be mixed positive and negative, then we can get Semi-NMF. Semi-NMF can be adapted to process input data that has mixed symbols. For G-orthogonal NMF and Semi-NMF, Ding _et al._ presented the following lemma:

**Lemma 1**.: _G-orthogonal NMF and Semi-NMF are all relaxation of K-means clustering, and the main advantages of G-orthogonal NMF are (1) Uniqueness of the solution; (2) Excellent clustering interpretability._

Taking into account the one-side orthogonal NMF, we relax the nonnegative constraints on \(\) and \(\). Moreover, inspired by FMCNOF , we construct the anchor graph \(\) obtained from the original data \(\) as the input of matrix factorization. Compared with the original data, the number of anchors is much smaller, therefore, by adopting the anchor graph constructed by anchors and original data points as the input of matrix factorization, we can reduce the computational complexity of the algorithm effectively.

\[_{ 0}\|-^{} \|_{F}^{2},^{}= ,\] (4)

where \(^{n m}\), \(^{n k}\) and \(^{m k}\), \(m\) is the number of anchors and we consider \(\) as the cluster indicator matrix for clustering rows as described in . We will introduce the details of anchor selection and the construction of the anchor graph in the appendix.

As described in the previous section, the existing NMF-based multi-view clustering methods are essentially a matrix factorization on a single view combined with the integration of multiple views. It causes the loss of the original spatial structure of the multi-view data. We extend NMF to the 3rd-order tensor, which can process the multi-view data directly and can also take full advantage of the original spatial structure of the multi-view data. The objective function of tensorial one-side orthogonal non-negative matrix factorization is written in the following form:

\[_{} 0}\|}-}*}^{}\|_{F}^{2}, }^{}*}=},\] (5)The 3rd-order tensor construction process is illustrated in Fig 2.

In order to better exploit the complementary information and spatial structure between different views, we get inspiration from the excellent performance of the tensor Schatten p-norm [12; 38]. We introduce tensor Schatten p-norm regularization on the tensorial form of the cluster indicator matrix. Our objective function is formulated as follows:

\[\|}-}*}^{} \|_{F}^{2}+\|}\|_{}}^{p} } 0,}^{}*}=}\] (6)

where \(0<p 1\), \(\) is the hyperparameter of the Schatten \(p\)-norm term.

**Remark 1**.: _The regularizer in the proposed objective (6) is used to explore the complementary information embedded in inter-views cluster assignment matrices \(^{(v)}\) (\(v=1,2,,V\)). Fig. 2 shows the construction of tensor \(}\), it can be seen that the \(k\)-th frontal slice \(^{(k)}\) describes the similarity between \(N\) sample points and the \(k\)-th cluster in different views. The idea cluster assignment matrix \(^{(v)}\) should satisfy that the relationship between \(N\) data points and the \(k\)-th cluster is consistent in different views. Since different views usually show different cluster structures, we impose tensor Schatten p-norm minimization  constraint on \(}\), which can make sure each \(^{(k)}\) has spatial low-rank structure. Thus \(^{(k)}\) can well characterize the complementary information embedded in inter-views._

### Optimization

Inspired by Augmented Lagrange Multiplier (ALM), we introduce two auxiliary variables \(}\) and \(}\) and let \(}=}\), \(}=}\), respectively, where \(} 0\). Then, we rewrite the model as the following unconstrained problem:

\[(},},},})\] (7) \[=_{} 0,}^{} *}=}}\|}-}* }^{}\|_{F}^{2}+\|}\|_{ }}^{p}+\|}-}+ _{1}}}{}\|_{F}^{2}+\|}-}+_{2}}}{}\|_{F}^{2},\]

where \(_{1}}\), \(_{2}}\) represent Lagrange multipliers and \(\), \(\) are the penalty parameters. The optimization process can therefore be separated into four steps:

\(\)**Solve \(}\) with fixed \(},},}\).** (7) becomes:

\[\|}-}*}^{} \|_{F}^{2}\] (8)

After being implemented with discrete Fourier transform (DFT) along the third dimension. the equivalent representation of (8) in the frequency domain becomes:

\[_{v=1}^{V}\|}}^{(v)}-}}^{(v)}(}}^{(v)})^{}\|_{ F}^{2},\] (9)

where \(}}=(},[\,],3)\), and the others in the same way.

Let \(=\|}}^{(v)}-}}^{(v )}(}}^{(v)})^{}\|_{F}^{2}\), we can obviously get the following equation:

\[=((}}^{(v)})^{}}}^{(v)})-2((}}^{(v)})^{}}}^{(v)}}}^{(v)})+((}}^{(v)}) ^{}}}^{(v)}).\] (10)

Setting the derivative \(/}}^{(v)}=0\) gives \(2}}^{(v)}-2(}}^{(v)})^{ }}}^{(v)}=0\). So the solution of (9) is:

\[}}^{(v)}=(}}^{(v)})^{}}}^{(v)}\] (11)

\(\)**Solve \(}\) with fixed \(},},}\).** (7) becomes:

\[_{}^{}*}=}}\| }-}*}^{}\|_{F}^{2 }+\|}-}+ }_{1}}{}\|_{F}^{2}+\|}-}+_{2}}}{}\|_{F}^{2}\] (12)And (12) is equivalent to the following in the frequency domain:

\[_{(}}^{(v)})^{} }}^{(v)}=}&_{v=1}^{V}\| }}^{(v)}-}}^{(v)}( {}}^{(v)})^{}\|_{F}^{2}\\ &+_{v=1}^{V}}}^ {(v)}-}}^{(v)}+}}_{1}^ {(v)}}{}_{F}^{2}+_{v=1}^{V}}}^{(v)}-}}^{(v)}+}}_{2}^{(v)}}{}_{F}^{2},\] (13)

where \(}}=(},[\,],3)\), and the others in the same way.

And (13) can be reduced to:

\[_{(}}^{(v)})^{}}}^{(v)}=}-2(}}^{(v)}( }}^{(v)})^{}}}^{( v)})-((}}^{(v)})^{} }}_{1}^{(v)})-((}}^{(v)})^{}}}_{2}^{(v)})\] (14)

where \(}}_{1}^{(v)}=}}^{(v)}- }}_{1}^{(v)}}{}\) and \(}}_{2}^{(v)}=}}^{(v)}- }}_{2}^{(v)}}{}\).

and it also can be reduced to:

\[_{(}}^{(v)})^{}}}^{(v)}=}((}}^{(v)})^{ }}}^{(v)})\] (15)

where \(}}^{(v)}=2}}^{(v)} {}}^{(v)}+}}_{1}^{(v)}+ }}_{2}^{(v)}\).

To solve (15), we introduce the following Theorem:

**Theorem 1**.: _Given \(\) and \(\), where \(()^{}=\) and \(\) has the singular value decomposition \(=()^{}\), then the optimal solution of_

\[_{()^{}=}( )\] (16)

_is \(^{*}=[,]()^{}\)._

Proof.: From the SVD \(=()^{}\) and together with (16), it is evident that

\[()=(()^{})=(()^{} )=()=_{i}s_{ii}h_{ii},\] (17)

where \(=()^{}\), \(s_{ii}\) and \(h_{ii}\) are the \((i,i)\) elements of \(\) and \(\), respectively. It can be easily verified that \(()^{}=\), where \(\) is an identity matrix. Therefore \(-1 h_{ii} 1\) and \(s_{ii} 0\), Thus we have:

\[()=_{i}s_{ii}h_{ii}_{i}s_{ii}.\] (18)

The equality holds when \(\) is an identity matrix. \(()\) reaches the maximum when \(=[,]\). 

So the solution of (15) is:

\[}}^{(v)}=}^{(v)}(^{ (v)})^{}\] (19)

where \(}^{(v)}\) and \(}^{(v)}\) can be obtained by SVD \(}}^{(v)}=}^{(v)}( }^{(v)})^{}\)

\(\)**Solve \(}\) with fixed \(},},}\).** (7) becomes:

\[_{}} 0}}-}+}_{1}}{}_{F}^{2}\] (20)

(20) is obviously equivalent to:

\[_{}} 0}}-(}+}_{1}}{})_{F}^{2}\] (21)According to , the solution of (21) is:

\[}=(}+}_{1}}{})_{+}\] (22)

\(\)**Solve \(}\) with fixed \(},},}\).** (7) becomes:

\[\|}\|_{}}^{p}+ }-}+}_{2}}{}_ {F}^{2},\] (23)

after completing the square regarding \(}\), we can deduce

\[}^{*}=\|}+}_{2}}{}-}\|_{F}^{2}+ \|}\|_{}}^{p},\] (24)

which has a closed-form solution as Lemma 2:

**Lemma 2**.: _Let \(^{n_{1} n_{2} n_{3}}\) have a t-SVD \(=**^{}\), then the optimal solution for_

\[_{}\|-\|_{F}^{2}+ \|\|_{}}^{p}.\] (25)

_is \(^{*}=_{}()=*(P_{} (}))*^{}\), where \(P_{}(})\) is an f-diagonal 3rd-order tensor, whose diagonal elements can be found by using the GST algorithm introduced in ._

Now the solution of (24) is:

\[}^{*}=_{}(}+}_{2}}{}).\] (26)

Finally, the optimization procedure for Multi-View Clustering via Orthogonal non-negative Tensor Factorization (Orth-NTF) is outlined in Algorithm 1.

``` Input: Data matrices \(\{^{(v)}\}_{v=1}^{V}^{N d_{v}}\); anchors numbers \(m\); cluster number \(K\). Output: Cluster labels \(\) of each data points. Initialize:\(=10^{-5}\), \(=10^{-5}\), \(=1.6\), \(}_{1}=0\), \(}_{2}=0\), \(}^{(*)}\) is identity matrix;
1: Compute graph matrix \(^{(v)}\) of each views;
2:while not condition do
3: Update \(}}^{(*)}\) by solving (11);
4: Update \(}}^{(*)}\) by solving (19);
5: Update \(}}^{(*)}\) by solving (22);
6: Update \(}\) by using (24);
7: Update \(}_{1}\), \(}_{2}\), \(\) and \(\): \(}_{1}=}_{1}+(}-})\), \(_{2}=}_{2}+(}-})\), \(=(,10^{13})\);
8:endwhile
9: Calculate the \(K\) clusters by using \(=_{v=1}^{V}^{(v)}/V\);
10:return Clustering result (The position of the largest element in each row of the indicator matrix is the label of the corresponding sample). ```

**Algorithm 1** Multi-View Clustering via Orthogonal non-negative Tensor Factorization (Orth-NTF)

### Convergence Analysis

**Theorem 2**.: _[Convergence Analysis of Algorithm 1] Let \(_{k}=\{}_{k},}_{k},}_ {k},}_{k},}_{2,k},}_{1,k}\},\;1  k<\) in (7) be a sequence generated by Algorithm 1, then_

1. \(_{k}\) _is bounded with the assumption_ \(_{k 0}\{_{k},_{k}\}(}_{k+1}^{(v)}-}_{k}^{(v)})=0\)_;_
2. _Any accumulation point of_ \(_{k}\) _is a stationary KKT point of (_7_)._

The proof will be provided in the appendix and we need to mention that the KKT conditions can be used to determine the stop conditions for Algorithm 1, which are \(\|}_{k}-}_{k}\|_{}\), \(\|}_{k}-}_{k}\|_{}\).

### Complexity Analysis

For Orth-NTF, the storage requirements for \(\), \(\), \(\), \(\), \(_{}\) and \(_{}\) have complexities of \((V(m+k)n)\), \((V(n+k)k)\), \((Vnk)\), \((Vnk)\), \((Vnk)\) and \((Vnk)\), respectively. Combining these, the total storage complexity for Orth-NTF is \((Vnm+vk^{2}+6Vnk)\).

For the computational complexity, the process of constructing \(\) has a computational complexity of \((Vnmd+Vnm(m))\). When updating the four variables, \(\), \(\), \(\) and \(\), their respective computational complexities are \((Vnmd+Vnm(m))\), \((Vm^{2}k+Vmk^{2})\), \((Vnk)\) and \((2Vnk(Vk)+V^{2}kn)\). Given that \(m\), \(n\), \(k\) and \(V\) are relatively small constants, the primary computational cost associated with updating the variables stands at \((Vnkm+Vm^{2}k)\). Summing it all up, the overall computational complexity of our proposed method is \((Vnmd+Vm^{2}k)\).

## 5 Experiments

In this section, we demonstrate the performance of our proposed method through extensive experiments. It is compared with plenty of state-of-art multi-view clustering algorithms on some multi-view datasets. We evaluate the clustering performance by applying 7 metrics used widely, _i.e._, 1) ACC; 2) NMI; 3) Purity; 4) PRE; 5) REC; 6) F-score; and 7) ARI. The higher the value the better the clustering results for all metrics mentioned above. Detailed experimental configurations and hyper-parameters on each dataset are in the appendix.

### Datasets and Compared Baselines Methods

The following multi-view datasets are selected to examine our proposed method. The details of the datasets are shown in Table 1. **MSRC**; **HandWritten4**; **Mnist4**; **AWA**; **Reuters**; **Noisy MNIST**; We choose the following 8 state-of-art multi-view clustering algorithms to compare with our proposed methods: **AMGL**; **MVGL**; **CSMSC**; **GMC**; **LMVSC**; **SMSC**; **SFMC** **FMCNOF**; **FPMVS-CAG**; **ETLMSC**; **MSC-BG**;

### Experiments Result

The clustering performances are listed in Table 2 and Table 3. They contain four medium-scale datasets and two large-scale datasets. The corresponding experimental configurations and descriptions are included in the appendix. It is clear that our algorithm outperforms the other baseline algorithms on most of the datasets.

This advantage may stem from the fact that our model directly factorizes the tensorized anchor graph--comprised of anchor graphs from various views--into the product of two non-negative tensors, one being an index tensor. As a result, our model effectively captures both the spatial structural information and the complementary data present in the anchor graphs from different

   \#Dataset & \#Samples & \#View & \#Class & \#Feature \\  MSRC & 210 & 5 & 7 & 24,576,512, 256,254 \\ HandWritten4 & 2000 & 4 & 10 & 76,216, 47, 6 \\ Mnist4 & 4000 & 3 & 4 & 30, 9, 30 \\ Reuters & 18758 & 5 & 6 & 21531, 24892, 34251, 15506, 11547 \\ Noisy MNIST & 50000 & 2 & 10 & 784, 784 \\   

Table 1: Multi-view datasets used in our experiments

Figure 3: Convergence experiments on MSRC, HandWritten4, Mnist4 and AWA.

[MISSING_PAGE_FAIL:9]

[MISSING_PAGE_FAIL:10]