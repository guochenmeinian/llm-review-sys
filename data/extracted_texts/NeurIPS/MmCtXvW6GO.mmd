# Trade-off Between Efficiency and Consistency

for Removal-based Explanations

 Yifan Zhang\({}^{*1}\)  Haowei He\({}^{*1}\)  Zhiquan Tan\({}^{2}\)  Yang Yuan\({}^{1,3,4}\)

\({}^{1}\)IIIS, Tsinghua University

\({}^{2}\)Department of Math, Tsinghua University

\({}^{3}\)Shanghai Artificial Intelligence Laboratory

\({}^{4}\)Shanghai Qizhi Institute

{zhangyif21,hhw19,tanzq21}@mails.tsinghua.edu.cn

yuanyang@tsinghua.edu.cn

Equal Contribution.Corresponding AuthorCode is available at https://github.com/trusty-ai/efficient-consistent-explanations.

###### Abstract

In the current landscape of explanation methodologies, most predominant approaches, such as SHAP and LIME, employ removal-based techniques to evaluate the impact of individual features by simulating various scenarios with specific features omitted. Nonetheless, these methods primarily emphasize efficiency in the original context, often resulting in general inconsistencies. In this paper, we demonstrate that such inconsistency is an inherent aspect of these approaches by establishing the Impossible Trinity Theorem, which posits that interpretability, efficiency, and consistency cannot hold simultaneously. Recognizing that the attainment of an ideal explanation remains elusive, we propose the utilization of interpretation error as a metric to gauge inefficiencies and inconsistencies. To this end, we present two novel algorithms founded on the standard polynomial basis, aimed at minimizing interpretation error. Our empirical findings indicate that the proposed methods achieve a substantial reduction in interpretation error, up to 31.8 times lower when compared to alternative techniques1.

## 1 Introduction

Most existing explanation approaches are removal-based , which involve the sequential process of eliminating certain input features, examining the subsequent alterations in the model's behavior, and ascertaining each feature's impact through observation. However, most of these methods  primarily focus on the original input with all features, often yielding inconsistent outcomes in alternative scenarios. Consequently, the resulting interpretations may not explain the network's behavior consistently even in a small neighborhood of the input, as demonstrated in Figure 1.

Inconsistency is a non-trivial concern. Imagine a doctor treating a diabetic patient with the help of an AI system. The patient has features A, B, and C, representing three positive signals from various tests. The AI recommends administering \(4\) units of insulin with the following explanation: A, B, and C have weights \(1\), \(1\), and \(2\), respectively, amounting to \(4\) units in total. The doctor might then ask the AI: _what if_ the patient only has A and B, but not C? One might expect the answer to be close to \(2\), as A+B has a weight of \(2\). However, the network, being highly non-linear, might output a different suggestion like \(3\) units, explaining that both A and B have a weight of \(1.5\). Such inconsistentbehaviors can significantly reduce the doctor's confidence in the interpretations, limiting the practical value of the AI system.

Consistency (see Definition 2.2) is certainly not the only objective for interpretability. Being equally important, efficiency is a commonly used axiom in the attribution methods , also called local accuracy  or completeness , stating that the model's output should be equal to the network's output for the given input (see Definition 2.3). Naturally, one may ask the following question:

**Q1: Can we generate an interpreting model that is both consistent and efficient?**

Unfortunately, this is generally unattainable. We have proved the following theorem in Section 2:

**Theorem 1**(Impossible trinity, informal version).: _For post-hoc interpreting models, interpretability, efficiency and consistency cannot hold simultaneously._

A few examples following Theorem 1:

1. Most attribution methods are interpretable and efficient, but not consistent.
2. The original (deep) network is consistent and efficient, but not interpretable.
3. If one model is interpretable and consistent, it cannot be efficient.

However, consistency is necessary for many scenarios, leading to the follow-up question:

**Q2: For consistent interpreting models, can they be approximately efficient?**

The answer depends on the definition of "approximately efficient". We introduce a new notion called _truthfulness_, which serves as a natural relaxation of efficiency, or partial efficiency. We divide the functional space of a network \(f\) into two subspaces: the readable part and the unreadable part. We call the interpreting model \(g\) truthful if it can accurately represent the readable part of \(f\), denoted as \(V\) (see Definition 2.4). The unreadable part is not merely a "fitting error"; it truthfully represents the higher-order non-linearities in the network that our interpretation model \(g\), even at its best, cannot cover. In short, what \(g\) conveys is true, though it may not encompass the entire truth. Due to Theorem 1, **this is essentially the best that consistent algorithms can achieve.**

Truthfulness is a parameterized notion, depending on the choice of the readable subspace. While there are theoretically infinite possible choices, we follow previous researchers on interpretability with non-linearities , using the basis that induces interpretable terms like \(x_{i}x_{j}\) or \(x_{i}x_{j}x_{k}\). These capture higher-order correlations and are easy to understand. The resulting subspace has the standard polynomial basis (or equivalently, the Fourier basis).

When truthfulness is parameterized with the polynomial basis, designing consistent and truthful interpreting models is equivalent to learning the Fourier spectrum (see Algorithm 1 and Lemma 3.4). However, exact consistency is not always necessary for most real-world applications, as approximate consistency is usually sufficient. We formally use the number of different interpreting models (in log scale) as a metric for inconsistency. Our last question is:

**Q3: When a little inconsistency is allowed, can we get better interpreting models?**

We affirmatively answer this question in Section 4 by introducing a new algorithm called Harmonica-local. We then apply it multiple times to develop Harmonic-anchor and Harmonic-anchor-constrained, which offer smaller interpretation errors with a small degree of inconsistency.

Figure 1: Interpretations generated by SHAP on movie review.

In this paper, we focus on removal-based explanations , which implies that \(f\) and \(g\) are Boolean functions. We remark that empirically most networks are not Boolean functions, i.e., the input variables are real valued. However, for every explanation algorithm (including LIME  and SHAP ) in the removal-based framework, the input \(x\) is not modified to an arbitrary real value. Instead, \(x\) is fixed, and the algorithm only retain or remove each feature of \(x\) for generating the explanations. When \(x\) is fixed with \(n\) features, it becomes natural to use Boolean functions to represent both the network \(f\) and the interpreting model \(g\) for their outputs in all \(2^{n}\) feature-removal scenarios. In other words, the algorithms in the framework only consider a fixed \(x\) each time, and given this \(x\), there are only \(2^{n}\) possible outcomes, even for the models with real inputs. Therefore, treating \(f\) and \(g\) as Boolean functions is not a simplification, but an accurate characterization of the removal-based framework (see Figure 1 in  for an illustration).

Our new algorithms, including Harmonica-local, Harmonica-anchor, and Harmonica-anchor-constrained, are all based on the Harmonica algorithm from Boolean functional analysis , which has rigorous theoretical guarantees on recovery performance and sampling complexities. In Section 5, we demonstrate that on datasets like IMDb and ImageNet, our algorithms achieve up to \(31.8\)x lower interpretation error compared with other methods.

In summary, our contributions are:

* We prove the impossible trinity theorem for removal-based explanations, demonstrating that interpretable algorithms cannot be consistent and efficient simultaneously.
* When a small inconsistency is allowed, we propose new algorithms using Harmonica-local and empirically demonstrate that these algorithms achieve significantly lower interpretation errors compared with other methods.
* For interpretable algorithms that are consistent but not efficient, we introduce a new notion called truthfulness, which can be regarded as partial efficiency. Due to the impossible trinity theorem, this is the best achievable outcome when consistency is required.

## 2 Our Framework on Interpretability

We consider a Hilbert space \(\) equipped with inner product \(,\), and induced norm \(\|\|\). We denote the input space by \(\), the output space by \(\), which means \(\). We use \(\) to denote the set of interpretable functions, and \(\) to denote the set of machine learning models that need interpretation. In this paper, if not mentioned otherwise we focus on models that are not self-interpretable, i.e., \(f\).

**Definition 2.1** (Interpretable and Interpretation Algorithm).: We call A model \(g\) is _interpretable_, if \(g\). An _interpretation algorithm_\(\) takes \(f,x\) as inputs, and outputs \((f,x)\) for interpreting \(f\) on \(x\).

As we mentioned previously, for many interdisciplinary fields, the interpretation algorithm should be consistent.

**Definition 2.2** (Consistent).: Given \(f\), an interpretation algorithm \(\) is _consistent_ with respect to \(f\), if \((f,x)\) remains the same (function) for every \(x\).

Efficiency is an important property of the attribution methods.

**Definition 2.3** (Efficient).: A model \(g\) is _efficient_ with respect to \(f\) on \(x\), if \(g(x)=f(x)\).

The following theorem states that one cannot expect to achieve the best of all three worlds.

_Theorem_1 (**Impossible Trinity for Removal-based Explanations**).: For any interpretation algorithm \(\) and function sets \(\), there exists \(f\) such that with respect to \(f\), either \(\) is not consistent, or \((f,x)\) is not efficient on \(x\) for some \(x\).

Proof.: Please refer to Appendix B for all the proofs. 

Theorem 1 says efficiency is too restrictive for consistent interpretations. However, being inefficient does not mean the interpretation is wrong, it can still be truthful. Recall a subspace \(V\) is _closed_ if whenever \(\{f_{n}\} V\) converges to some \(f\), then \(f V\). We have:

**Definition 2.4** (Truthful gap and truthful).: Given a closed subspace \(V\), \(g V\) and \(f\), the _truthful gap_ of \(g\) to \(f\) for \(V\) is:

\[_{V}(f,g)\|f-g\|^{2}-_{v V}\|f-v\|^{2}.\] (1)

When \(_{V}(f,g)=0\), we say \(g\) is _truthful_ for subspace \(V\) with respect to \(f\), and we know (see e.g. Lemma 4.1 in ) \( v V, f-g,v=0\).

Truthfulness means \(g\) fully captures the information in the subspace \(V\) of \(f\), therefore it can be seen as a natural relaxation of efficiency. To characterize the interpretation quality, we introduce the following notion.

**Definition 2.5** (Interpretation error).: Given functions \(f,g\), the interpretation error between \(f\) and \(g\) with respect to measure \(\) is

\[_{p,}(f,g)(_{}|f(x)-g(x)|^{p}d( x))^{1/p}.\] (2)

Notice that interpretation error is only a _loss function_ that measures the quality of the interpretation, instead of a metric in \(_{p}\) space. Therefore, \(\) can be a non-uniform weight distribution following the data distribution. If \(\) is uniform distribution over \(\), we abbreviate \(_{p,}(f,g)\) as \(_{p}(f,g)\). For real-world applications, interpreting the model over the whole \(\) is unnecessary, so \(\) is usually defined as a uniform distribution on the neighborhood of input \(x\) (under a certain metric), in which case we denote the distribution as \(_{x}\).

## 3 Applying Our Framework to Removal-based Explanations

Now we focus on interpreting removal-based explanations [38; 18]. Removal-based explanations are post-hoc, which means they are generated based on a target network for a fixed input, by removing features from that input. There are three choices affecting the removal-based explanations: how features are removed, what behavior is analyzed after feature removal, and how to summarize the feature influence. For example, SHAP  considers all possible subsets of the features, and analyzes how holding out different features affects functional value, and finally summarizes the differences based on the Shapley value calculation.

Therefore, removal-based explanations can be represented as Boolean functions, as feature subset \(S[n]\) can be represented as Boolean input: \(f\{-1,1\}^{n}\), where \(-1\) means the specific feature \(i S\) is removed, \(1\) means the specific feature is retained. We use \(-1/1\) instead of \(0/1\) to represent the binary variables for ease of exposition using the Fourier basis.

### Fourier Basis and Truthful Gap

Fourier analysis is a handy tool for analyzing Boolean functions. Due to the space limit, we defer the more comprehensive introduction on Fourier analysis for Boolean functions to Appendix A, and only present the necessary notions here.

**Definition 3.1** (Fourier basis).: For any subset of variables \(S[n]\), we define the corresponding Fourier basis as \(_{S}(x)_{i S}x_{i}\ \{-1,1\}^{n}\{-1,1\}\).

The Fourier basis is also called polynomial basis in the literature. It is a complete orthonormal basis for Boolean functions, under the uniform distribution on \(\{-1,1\}^{n}\). We remark that this uniform distribution is used for theoretical analysis and algorithm design, and is different from the measure \(\) for interpretation quality assessment in Definition 2.5.

**Definition 3.2** (Fourier expansion).: Any Boolean function \(f\{-1,1\}^{n}\) can be expanded as

\[f(x)=_{S[n]}_{S}_{S}(x),\]

where \(_{S}= f,_{S}\) is the Fourier coefficient on \(S\).

Now we define the notion of \(C\)-Readable function.

**Definition 3.3** (\(C\)-Readable function).: Given a set of Fourier bases \(C\), a function \(f\) is \(C\)-readable if it is supported on \(C\). That is, for any \(_{S} C\), \( f,_{S}=0\). Denote the corresponding subspace as \(V_{C}\).

The Readable notion is parameterized with \(C\), because it may differ case by case. If we set \(C\) to be all the single variable bases, only linear functions are readable; if we set \(C\) to be all the bases with the degree at most \(2\), functions with pairwise interactions are also readable. Moreover, if we further add one higher order term to \(C\), e.g., \(_{\{x_{1},x_{2},x_{3},x_{4}\}}\), it means we can also reason about the factor \(x_{1}x_{2}x_{3}x_{4}\) in the interpretation, which might be an important empirical factor that people can easily understand. Starting from the bases set \(C\), we have the following formula for computing the truthful gap.

**Lemma 3.4** (Truthful gap for Boolean functions).: _Given a set of Fourier bases \(C\), two functions \(f,g\{-1,1\}^{n}\), the truthful gap of \(g\) to \(f\) for \(C\) is_

\[_{V_{C}}(f,g)=_{_{S} C} f-g,_{S}^{2}.\] (3)

With the previous definitions, it becomes clear that finding a truthful interpretation \(g\) is equivalent to accurately learning a Boolean function with respect to the readable bases set \(C\). Intuitively, it means we want to find algorithms that can compute the coefficients for the bases in \(C\). In other words, we want to find the importance of the bases like \(x_{1},x_{2}x_{5},x_{2}x_{6}x_{7}\), etc.

### Representative Algorithms

Applying the impossible trinity theorem to removal-based explanations, there are two notable algorithms on the extremes.

**Shapley values: efficient but not consistent.** Let \(N=\{1,2,...,n\}\) represent a set of \(n\) players, and let \(v:2^{N}\) be a characteristic function that assigns a real value to each coalition of players. The Shapley value of player \(i\) is given by:

\[_{i}(v)=_{S N i}[v(S  i)-v(S)],\] (4)

where \(|S|\) is the number of players in coalition \(S\), and the sum is taken over all possible coalitions \(S\) that do not include player \(i\). One of the most important properties of Shapley values is _efficiency_, i.e. \(_{i N}_{i}(v)=v(N)-v()\) (or denoting explanation function \(g_{i N}_{i}(v)+v()\)). As Shapley value based explanations do **not** focus on consistency, its explanation is efficient only for the original input, but not for other scenarios when certain features are removed.

**Harmonica: consistent but not efficient.** What can we do if we want to address consistency? As mentioned in Definition 2.4, we do not expect the algorithm to be efficient, but it can still be truthful with respect to a subspace \(V\), which is naturally represented with the polynomial basis. Therefore, in order to learn truthful explanations for given subspace \(V\), it is natural to consider LASSO regression over the coefficients on the polynomial basis. Harmonica (Algorithm 1) fulfills this requirement  and has superior complexity shown in Theorem 2.

```
1. Given uniformly randomly sampled \(x_{1},,x_{T}\), evaluate them on \(f\): \(\{f(x_{1}),....,f(x_{T})\}\).
2. Solve the following regularized regression problem. \[*{argmin}_{^{|C|}}\{_{i=1}^{T}( _{S,_{S} C}_{S}_{S}(x_{i})-f(x_{i}))^{2}+ \|\|_{1}\}\] (5)
3. Output the polynomial \(g(x)=_{S,_{S} C}_{S}_{S}(x)\). ```

**Algorithm 1**Harmonica 

_Theorem_ 2 (Complexity of Harmonica).: Given \(f\{-1,1\}^{n}\), a \((/4,s,C)\)-bounded function, Algorithm 1 finds a function \(g\) with interpretation error at most \(\) in time \(O((T+|C|/)|C|)\) and sample complexity \(T=(s^{2}/|C|)\).

[MISSING_PAGE_FAIL:6]

This theorem illuminates the inherent trade-offs in designing interpretable models. Specifically, to reduce \(_{}\), one would typically need to increase \(_{}\) unless \(f\) itself is close to an interpretable model \(\). Theorem 4 can also be seen as a quantitative version of the Impossible Trinity Theorem 1 for removal-based explanations.

Now we established the lower bound of \(_{}+_{}\) by \(_{2}(f,g^{})\) (the same as \(_{2}(f,g^{})\)), showing the trade-off between efficiency and consistency, and also bridging the locally consistent interpretations \(g_{i}\) and globally consistent interpretation \(g^{}\).

## 4 Trade-off Between Efficiency and Consistency

Harmonica recovers functions across the entire function space. However, in our setting, if \(_{x}\) is small, it is sufficient to recover a small neighborhood of the function. This inspires us to apply Harmonica to a local space instead of the entire space. By doing so, we obtain more concentrated samples in the local neighborhood. Consequently, minimizing the interpretation error in this neighborhood becomes more manageable. We refer to Harmonica with samples in the local neighborhood as Harmonica-local.

If we relax the consistency requirement, meaning that users are willing to accept minor inconsistencies in interpretations when a slight modification is made to the input, we can achieve smaller interpretation error with Harmonica-anchor, as shown in Algorithm 2. Intuitively, Harmonic-anchor applies multiple Harmonica-local algorithms to different subspaces of the input. Specifically, for a given neighborhood region \(x\), we now randomly select \(k_{x}\) bases \(b_{x,i}\), \((i=1,2,3,...,k_{x})\) as interpretation anchors instead of only one basis as Harmonica does. We calculate \(k_{x}\) interpretation models \(g_{x,i}\) on each anchor and denote the coefficient of \(g_{x,i}\) as \(_{x,i}\). For simplicity, we omit the subscript \(x\) in the following.

``` Input: anchor number \(k\), model \(f\), a distance metric function \(d(,):(_{S},_{S})\) which calculates distance between two anchors (e.g., \(L_{p}\) norm or Hamming distance), sampling number \(T\), regularization coefficient \(_{1}\). Output: interpretation models \(g_{i}\) with coefficients \(_{i}^{|C|}\), \(i=1,2,3,...,k\).
1: Fix random bases \(b_{i},i=1,2,3,...,k\) as anchors.
2: Randomly sample \(T\) bases \(b_{1},b_{2},...,b_{T}\) and accordingly, we calculate \(f(x_{1}),f(x_{2}),...,f(x_{T})\).
3: Assign each basis to the anchor with minimal distance and index using \(d_{i}=\{*{argmin}_{j}d(b_{i},b_{j}^{})\},i=1,2,3,...T\) and \(j=1,2,3,...,k\).
4:for\(i=1,2,3,...,k\)do
5: Solve the following regularized regression problem: \[*{argmin}_{_{i}^{|C|}}_{n=1}^{T} (i=d_{n})_{S,_{S} C}_{i,S}_{S} (x_{n})-f(x_{n})^{2}+\|_{i}\|_{1}}.\] (8)
6:endfor ```

**Algorithm 2**Harmonica-anchor

The number of bases \(k\) is highly related to consistency, so we could define the log value \( k\) to evaluate the inconsistency of the interpretation models. The minimum inconsistency value is \(0\), as in Harmonica (\(k=1\)), while for attribution methods, the inconsistency should be almost \(n\), which is the number of input variables. The maximum inconsistency value depends on the neighborhood region \(_{x}\), since the number of interpretation models should not exceed the number of points in \(_{x}\). As a special case, the Harmonica algorithm achieves \(0\) inconsistency, making it the most consistent algorithm.

However, if one only considers the number of different interpretation algorithms, users may still encounter a high level of inconsistency empirically when the interpretations significantly differ from one another. Thus, another critical constraint to add is the spectrum distance constraint among different interpretations of different anchors. Based on Theorem 4, we introduce Harmonica-anchora-constrained in Algorithm 3 (in Appendix). In this algorithm, \(_{2}\) serves as a penalty coefficient for the difference between \(g_{i}\), and we solve the problem using iterative gradient descent. Intuitively, a larger\(_{2}\) restricts the expressive power of the interpretation models \(g_{i}\), leading to a larger interpretation error.

## 5 Experiments

### Analysis on Polynomial Functions

To investigate the performance of different interpretation methods, we _manually_ examine the output of various algorithms including LIME , SHAP , Shapley Interaction Index , Shapley Taylor [61; 26], Faith-SHAP , Harmonica, and Low-degree (Appendix D) for lower-order polynomial functions.

We observe that all algorithms can accurately learn the coefficients of the first-order polynomial. For the second-order polynomial function, only Shapley Taylor, Faith-SHAP, Harmonica, and Low-degree can learn all the coefficients accurately. For the third-order polynomial function, only Faith-SHAP, Harmonica, and Low-degree succeed. Due to space constraints, we defer the details to Appendix F.

### Experimental Setup

In the rest of this section, we conduct experiments to evaluate the interpretation error \(_{p_{v}}(f,g)\) and truthful gap \(_{V_{C}}(f,g)\) of Harmonica and other baseline algorithms on language and vision tasks quantitatively. In our experiments, we choose 2nd order and 3rd order Harmonica algorithms, which correspond to setting \(C\) to include all terms with order at most \(2\) and \(3\).

The baseline algorithms chosen for comparison include LIME , Integrated Gradients , SHAP , Integrated Hessians , Shapley Taylor interaction index, and Faith-SHAP, where the first three are first-order algorithms, and the last three are second-order algorithms.

The two language tasks we select are the SST-2  dataset for sentiment analysis and the IMDb  dataset for movie review classification. The vision task is the ImageNet  for image classification. To demonstrate the capability of our interpretation framework applied to vision tasks, we have generated two examples in Figure 2, compared with LIME, Integrated Gradients (IG), and SHAP. Note that all of these methods are applied to the same ground-truth image segmentation provided by the MS-COCO dataset . For ablations on using the SLIC superpixels , please refer to Appendix H.

For the SST-2 dataset, we attempt to interpret a convolutional neural network (see details in Appendix G) trained with the Adam  optimizer for 10 epochs. The IMDb dataset contains long paragraphs, and each paragraph has multiple sentences. By default, we use periods, colons, and exclamations to separate sentences. For the ImageNet  dataset, we aim to provide class-specific interpretation, meaning that only the class with the maximum predicted probability is considered for each sample. We use the official pre-trained ResNet-101  model from PyTorch.

Figure 2: Illustrative examples for applying our interpretation method on MS-COCO dataset.

### Results on Interpretation Error

For a given input sentence \(x\) with length \(l\), we define the induced neighborhood \(_{x}\) by introducing a masking operation on this sentence. The radius \(0 r l\) is defined as the maximum number of masked words.

Figure 3 displays the \(L^{2}\) interpretation error evaluated under different neighborhoods with a radius ranging from \(1\) to \(\) for all the considered datasets. Here, \(\) represents the maximum sentence length, which may vary for different data points. We also inspect \(L^{1}\) and \(L^{0}\) norms. Here, \(L^{2}\) and \(L^{1}\) are defined according to Eqn. (2) with \(p=2\) and \(p=1\), respectively. And \(L^{0}\) denotes \(_{}|f(x)-g(x)| 0.1d(x)\). We can see that Harmonic consistently outperforms all the other baselines on all radii.

Note that for IMDb in Figure 3, we make a slight modification such that the masking operation is performed on sentences in one input paragraph instead (we also change the definition of radii accordingly). For ImageNet in Figure 3, the interpretation error is evaluated on 1000 random images, and the masking operation is performed on 16 superpixels in one input image instead (we also change the definition of radii accordingly). We can see that when the neighborhood's radius is greater than 1, Harmonic outperforms all the other baselines. Specifically, when evaluated on ImageNet dataset, Harmonic-anchor (\(k=9\)) achieves \(31.8\) times lower interpretation error \(_{2,\,_{1}}\) compare to Integrated Gradients . Limited by space, the detailed numerical results and interpretation error under other norms are presented in Appendix I. In contrast, Harmonic-local achieves more accurate local consistency but high error outside the neighborhood. The relevant results are deferred in Appendix J due to limited space.

As mentioned in Section 4, Harmonic-anchor further reduces the interpretation error by learning each subspace separately. Figure 3 shows that the \(L_{2}\) interpretation error of Harmonic-anchor achieves lower error than Harmonic on various datasets, and the error further declines as we increase the number of anchors. Full results of other norms and the results of Harmonic-anchor-constrained are shown in Appendix L.

### Results on Truthful Gap

For convenience, we define the set of bases \(C^{d}\) up to degree \(d\) as \(C^{d}=\{_{S}|S[n],|S| d\}\). We evaluate the truthful gap on the set of bases \(C^{3}\), \(C^{2}\), and \(C^{1}\). For implementation details on

   Method &  Harm. \\ 2nd \\  &  Harm. \\ 3rd \\  &  LIME \\  &  SHAP \\  &  IG \\  &  IH \\  &  Shapley \\ Taylor \\  & 
 Faith- \\ Shap \\  \\  IMDb & 0.540 & **0.174** & 5.343 & 1.390 & 1.438 & 1.948 & 7.005 & 15.528 \\ ImageNet & 0.660 & **0.246** & 0.738 & 2.023 & 1.848 & 175.368 & 0.474 & 0.430 \\   

Table 1: Comparison of \(C^{3}\) truthful gap \(_{C}(f,g)\) results evaluated on IMDb and ImageNet datasets (lower truthful gap means better truthfulness of the interpretation).

Figure 3: Visualization of \(L^{2}\) interpretation error \(_{p,\,_{x}}(f,g)\) of several state-of-the-art interpretation methods evaluated on IMDb and ImageNet datasets.

calculating the truthful gap, please refer to Appendix K. Table 1 shows the \(C^{3}\) truthful gap evaluated on different datasets. We can see that Harmonic outperforms all the other baseline algorithms. Due to space limitations, \(C^{2}\) and \(C^{1}\) results are provided in Appendix K.

### More Experimental Results

For more experimental results on different image segmentation methods, different neural network architectures, and different choices of baselines, please refer to Appendix H. For more results on the Low-degree algorithm, please refer to Appendix M.

## 6 Related Work

Interpretability is a critical topic in machine learning, and we refer the reader to [20; 37] for insightful general discussions. Below we discuss different types of interpretable models.

**Removal-based explanations.** Covert et al.  presents a unified framework for removal-based model explanation methods, connecting 26 existing techniques such as LIME , SHAP , Meaningful Perturbations , and permutation tests . LIME  is a classical method that samples data points following a predefined sampling distribution and computes a function that empirically satisfies local fidelity.

The Shapley value [50; 72; 24] originates from cooperative game theory and is used to allocate the total value generated by a group of players among individual players. It is the unique kind of method that satisfies a few important properties, including efficiency, symmetry, dummy, additivity, etc. Shapley values have been extensively applied to machine learning model explanations [38; 39; 57; 59; 67; 78; 23; 77] and feature importance . Recent research has focused on developing efficient approximation methods for Shapley values [38; 15; 5; 16; 31; 26; 68]. Many works have generalized Shapley values to higher-order feature interactions [45; 24; 61; 41; 63; 1; 62].

**Gradient-based explanations.** Gradient-based explanations are popular for deep learning models, such as CNNs. These methods include SmoothGrad, Integrated Gradient, GradCAM, DeepLift, LRP, etc. [53; 54; 52; 60; 74; 49; 6; 51; 13; 42; 52; 48]. Although these methods have seen extensive use in various areas, gradient-based methods are often time-consuming and can be insensitive to random model parameterization .

**Model-specific interpretable models.** Interpretable or transparent (white-box) models are inherently ante-hoc and model-specific. One primary goal of utilizing interpretable models is to achieve inherent model interpretability. Prominent approaches include Decision Trees ([66; 7; 75]), Decision Rules ([69; 58]), Decision Sets ([34; 70]), and Linear Models ([64; 65]). Moreover, another research direction attempts to use an interpretable model surrogate to approximate the original black-box models.  employs a two-layer additive risk model for interpreting credit risk assessments.  suggests an approach called model extraction that greedily learns a decision tree to approximate \(f\). However, these methods primarily rely on heuristics and lack theoretical guarantees.

## 7 Conclusion

In this paper, we tackled the problem of generating consistent interpretations and introduced the impossible trinity theorem of interpretability, under a formal framework for understanding the interplay between interpretability, consistency, and efficiency. Since a consistent interpretation cannot be efficient, we relaxed efficiency to truthfulness, meaning the interpretation matches the target function in a specific subspace. This led to the problem of learning Boolean functions and the proposal of new algorithms based on the Fourier spectrum and a localized version of Harmonic. Our methods showed lower interpretation errors and improved consistency compared to the existing approaches.

While our work offers theoretical insights, many open questions and challenges remain in building more interpretable, consistent, and efficient models. We hope our work serves as a foundation for future research in the area of explainable AI.

#### Acknowledgments

Thanks to Jiaye Teng for the useful discussions. This work is supported by the Ministry of Science and Technology of the People's Republic of China, the 2030 Innovation Megaprojects "Program on New Generation Artificial Intelligence" (Grant No. 2021AAA0150000).