# Participatory Personalization in Classification

Hailey Joren

UC San Diego

Chirag Nagpal

Google Research

Katherine Heller

Google Research

Berk Ustun

UC San Diego

###### Abstract

Machine learning models are often personalized with information that is protected, sensitive, self-reported, or costly to acquire. These models use information about people but do not facilitate nor inform their _consent_. Individuals cannot opt out of reporting personal information to a model, nor tell if they benefit from personalization in the first place. We introduce a family of classification models, called _participatory systems_, that let individuals opt into personalization at prediction time. We present a model-agnostic algorithm to learn participatory systems for personalization with categorical group attributes. We conduct a comprehensive empirical study of participatory systems in clinical prediction tasks, benchmarking them with common approaches for personalization and imputation. Our results demonstrate that participatory systems can facilitate and inform consent while improving performance and data use across all groups who report personal data.

## 1 Introduction

Machine learning models routinely assign predictions to _people_ - be it to screen a patient for a mental illness , their risk of mortality in an ICU , or their likelihood of responding to treatment . Many models in such applications are designed to target heterogeneous subpopulations using features that explicitly encode personal information. Typically, models are _personalized_ with categorical attributes that define groups [i.e., "categorization" as per 27]. In medicine, for example, clinical prediction models use _group attributes_ that are _protected_ (e.g., sex in the ASCVD Score for cardiovascular disease), _sensitive_ (e.g., HIV_status in the VA COVID-19 Mortality Score), _self-reported_ (e.g., alcohol_use in the HAS-BLED Score for Major Bleeding Risk), or _costly_ to acquire (e.g., leukocytosis in the Alvarado Appendicitis Score).

Individuals expect the right to opt out of providing personal data and the ability to understand how it will be used [see, e.g., personal data guidelines in GDPR, OECD privacy guidelines 26, 40]. In many contexts, personalized models do not provide such functionality: individuals cannot opt out of reporting data used to personalize their predictions nor tell if it would improve their predictions. At the same time, practitioners assume that data available for training will be available at inference time. In practice, this assumption has led to a proliferation of models that use information that individuals may be unwilling or unable to report at prediction time [see e.g., the Denver HIV Risk Score 29, which asks patients to report age, gender, race, and sexual_practices]. In tasks where individuals self-report, they may not voluntarily report information that could improve their predictions or may report incorrect information.

The broader need to facilitate and inform consent in personalized prediction tasks stems from the fact that personalization may not improve performance for each group that reports personal data . In practice, a personalized model can perform _worse_ or the same as a _generic model_ fit without personal information for a group with specific characteristics. Such models violate the implicit promise of personalization as individuals report personal information without receiving a tailored performance gain in return. These instances of "worsenalization" are prevalent, hard to detect, and hard to resolve [see 42, 51]. However, they would be resolved if individuals could opt out of personalization and understand its expected gains (see Fig. 1).

This work introduces a family of classification models that operationalize informed consent called _participatory systems_. Participatory systems _facilitate consent_ by allowing individuals to report personal information at prediction time. Moreover, they _inform consent_ by showing how reporting personal information will change their predictions. Models that facilitate consent operate as markets in which individuals trade personal information for performance gains. This work seeks to develop systems that perform as well as possible both when individuals opt-in - to incentivize voluntary reporting - and when they opt out - to safeguard against abstention. Our main contributions include:

1. We present a variety of participatory systems that provide opportunities for individuals to make informed decisions about data provision. Each system ensures that individuals who opt into personalization will receive the most accurate possible predictions possible.
2. We develop a model-agnostic algorithm to learn participatory systems. Our approach can produce a variety of systems that promote participation and minimize data use in deployment.
3. We conduct a comprehensive study of participatory systems in real-world clinical prediction tasks. The results show how our approach can facilitate and inform consent in a way that improves performance and minimizes data use.
4. We provide a Python library to build and evaluate participatory systems.

Related WorkParticipatory systems support modern principles of responsible data use articulated in OECD privacy guidelines , the GDPR , and the California Consumer Privacy Act . These include: _informed consent_, i.e., that data should be collected with the data subject's consent; and _collection limitation_, i.e., that data collected should be restricted to only what is necessary. These principles stem from extensive work on the right to data privacy . They are motivated, in part, by research showing that individuals care deeply about their ability to control personal data  but differ considerably in their desire or capacity to share it [see e.g. 5, 7, 9, 17, 18, 39, 41]. Our proposed systems let decision subjects report personal data in exchange for performance, which is aligned with principles articulated in recent work on data privacy  and related to work in designing incentive-compatible prediction functions .

We consider models that are personalized with categorical attributes that encode personal characteristics [i.e., "categorization" rather than "individualization" as per 27]. Modern techniques for learning with categorical attributes [see e.g., 2, 48] use them to improve performance at a population level - e.g., by accounting for higher-order interaction effects  or recursive partitioning . Our methods can be used to achieve these goals in tasks where models use features that are optional or costly to acquire [see e.g., 6, 7, 52, 61].

Our work is related to algorithmic fairness in that we seek to improve model performance at a group level. Recent work shows that personalization with group attributes does not uniformly improve performance and can reduce accuracy at a group level [see 42, 51, 57]. Our systems can safeguard against such instances of "worsenalization" by informing users of the gains in reporting and allowing

Figure 1: Classification task where participation improves accuracy and minimizes data use. We consider a dataset that has no features, two group attributes \(=,n^{-}=51\) negative examples and \(n^{+}=50\) positive examples. Here, the best personalized linear model \(h:\) with a one-hot encoding of \(\) makes 24 mistakes, and the best generic model \(h_{0}:\) makes 50 mistakes as it predicts the majority class (\(-\)). Under traditional personalization, individuals report group membership to receive personalized predictions from \(h\). As shown, personalization benefits the population as a whole by reducing overall error from 50 to 24 (\( R_{}(h,h_{0})=26\)). However, personalization has a detrimental effect on [female,old], who receive less accurate predictions from the personalized model (\( R_{}(h,h_{0})=\ -24\)), and no effect on [male,young] who receive the same predictions from the personalized and generic models(\( R_{}(h,h_{0})=\ \)). In a minimal participatory system, individuals _opt in_ to personalization, choosing to receive predictions from \(h\) or \(h_{0}\). Here, individuals in groups [female,old] and [male,young] opt out of personalization, leading to an overall error of 0 (\( R_{}(h,h_{0})=\ 50\)) and a reduction in unnecessary data collection (\(\)).

them to opt out of reporting. This line of broad work complements research on preference-based fairness [22; 36; 57; 59; 62], on ensuring fairness across complex group structures [28; 30; 34], and promoting privacy across subpopulations [13; 53].

## 2 Participatory Systems

We consider a classification task where we personalize a model with categorical attributes. We start with a dataset \(\{(_{i},y_{i},_{i})\}_{i=1}^{n}\) where each example consists of a feature vector \(_{i}^{d}\), a label \(y_{i}\), and a vector of \(m\) categorical attributes \(_{i}=[g_{i,1},,g_{i,m}]_{1} _{m}=\). We refer to \(\) as _group attributes_, and to \(_{i}\) as the _group membership_ of person \(i\). We use \(n_{}:=|\{i\,|\,_{i}=\}|\) denote the size of group \(\), and use \(|_{k}|\) to denote the number of categories for group attribute \(k\).

We use the dataset to train a personalized model \(h:\) via empirical risk minimization with a loss function \(:_{+}\). Given a model \(h\), we denote its empirical risk and true risk as \((h)\) and \(R(h)\), respectively, and evaluate model performance at the group level. We denote the empirical risk and true risk of a model \(h\) on group \(\) as

\[R_{}(h(,)):=[(h(,),y) ],_{}(h(,)):=}}_{i: _{i}=}(h(,),y_{i}).\]

We consider tasks where every individual prefers more accurate predictions.

**Assumption 1**.: Given models \(h\) and \(h^{}\), individuals in group \(\) prefer \(h\) to \(h^{}\) when \(R_{}(h)<R_{}(h^{})\).

Assumption 1 holds in settings where every individual prefers more accurate predictions - e.g., clinical prediction tasks such as screening or diagnosing illnesses [49; 56]. It does not hold in applications where some individuals prefer predictions that may be inaccurate - e.g., such as predicting the risk of organ failure for a transplant (see e.g., 43, for other "polar" clinical applications).

Operationalizing ConsentWe consider models where individuals consent to personalization by deciding whether or not to report their group attributes at prediction time. We let \(\) denote an attribute that was not reported, and let \(_{i}=[r_{i,1},,r_{i,k}] \). For example, a person with \(_{i}=[,=+]\) would report \(_{i}=[,]\) if they only disclose sex, and would report \(_{i}=:=[,]\) if they opt out of reporting entirely.

We associate each model with a set of _reporting options_\(\). A traditional model, which requires each person to report group attributes, has \(=\). A model where each person could report any subset of group attributes has \(=\). We represent individual decisions to opt into personalization at prediction time through a _reporting interface_ defined below.

**Definition 1**.: Given a personalized classification task with group attributes \(\), a _reporting interface_ is a tree \(T\) whose nodes represent attributes reported at prediction time. The tree is rooted at \((T)=[,,]\) and branches as a person reports personal attributes. Given a node \(\), we denote its parent as \(()\). Each parent-child pair represents a _reporting decision_, and the height of the tree represents the maximum number of reporting decisions.

**Definition 2**.: Given a personalized classification task with group attributes \(\), a _participatory system_ with reporting interface \(T\) is a prediction model \(f_{T}:\) that obeys the following properties:

1. _Baseline Performance_: Opting out of personalization entirely guarantees the expected performance from a _generic model_ trained without group attributes \(h_{0}_{h}R(h)\). \[R_{}(f_{T}(,))=R_{}(h_{0}).\]
2. _Incentive Compatibility_: Opting into personalization improves expected performance \[R_{}(f_{T}(,))<R_{}(f_{T}(,^{})),^{}^{}=().\]

Here, the _Baseline Performance_ property ensures that individuals who choose not to share personal information receive the performance of a generic model - i.e., the most accurate model that could be trained without this information. This property also ensures individuals retain the ability to opt out of personalization - i.e., \(\). The _Incentive Compatibility_ property ensures that personalization will improve expected performance - i.e., when individuals report personal data, the system can effectively leverage that data to deliver more accurate predictions in expectation. Together, these properties lead to data minimization, as systems that obey these properties will not request data from a reporting group when it will not lead to an improvement in expected performance.

On Data Minimization via ImputationAn alternative approach to allow individuals to opt out of reporting personal information at prediction time is to impute their group membership. Imputation allows individuals to opt out of personalization but does not guarantee the accuracy of their predictions. As a result, individuals who opt out of personalization by reporting \(=\) may receive a less accurate prediction than they would receive from a generic model. In the best-case scenario where we could perfectly impute group membership, a group might be assigned better predictions from a generic model (see Fig. 1). In the worst case, imputation may be incorrect, leading to even more inaccurate predictions than those of the generic or personalized model. We highlight these effects on real-world datasets in our experiments in Section 4.

Characterizing System PerformanceOne of the key differences between traditional models and participatory systems is that their performance depends on individual reporting decisions. In what follows, we characterize the performance under a general model of individual disclosure. Given a participatory system \(f_{T}\), we assume that each individual reports personal information to maximize an individual utility function of the form:

\[u_{i}(;f_{T})=b_{i}(;f_{T})-c_{i}()\] (1)

Here, \(c_{i}()\) and \(b_{i}()\) denote the cost and benefit that individual \(i\) receives from reporting \(\) to \(f_{T}\) respectively. We assume that individuals incur no cost when they do not report any attributes such that \(c_{i}()=0\), and incur costs that increase monotonically with information disclosed such that \(c_{i}() c_{i}(^{})\) for \(^{}\). We assume that benefits increase monotonically with expected gains in true risk so that \(R_{}(f_{T}(,))<R_{}(f_{T}( ,^{})) b_{i}(,f_{T})>b_{i}( ^{},f_{T})\).

Figure 2: Participatory systems for a personalized classification task with group attributes \(=[,][ ,]\). Each system allows a person to opt out of personalization by reporting \(\) and informs their choice by showing the expected gains of personalization (e.g., +0.2% gain in accuracy). Systems minimize data use by removing reporting options that do not improve accuracy (see grey-striped boxes). Here, \([,]\) is pruned in all systems as it leads to a gain \(\) 0.0%.

In Fig. 3, we show how the system performance for each reporting group can change with respect to participation when we simulate individual disclosure decisions from a model that satisfies the assumptions listed above. When a personalized model \(h\) requires individuals to report information that reduces performance as in Fig. 1, individuals incur a cost of disclosure without receiving a benefit in return. In such cases, individuals who interact with a minimal system would opt out of worsenalization and receive more accurate predictions from a generic model, thereby improving the overall performance of the system.

We observe that the maximum utility that each individual can receive from a participatory system can only increase as we add more reporting options. Thus, flat and sequential systems should exhibit better performance than a minimal system.

Given a participatory system \(f_{T}\) with reporting options \(\), a participatory system \(f_{T^{}}\) with more reporting options \(^{}\) can only improve performance, - i.e., \(R(f_{T^{}}) R(f_{T})\). Similarly, the system with more reporting options can only improve utility, - i.e., \(u_{i}(;f_{T^{}}) u_{i}(;f_{T})\) for all individuals \(i\).

## 3 Learning Participatory Systems

This section describes a model-agnostic algorithm to learn participatory systems that ensures incentive compatibility and baseline performance in deployment. We outline our procedure in Algorithm 1 to learn the three kinds of participatory systems in Fig. 2. The procedure takes as input a pool of candidate models \(\), a dataset for model assignment \(^{}\), and a dataset for pruning \(^{}\). It outputs a collection of participatory systems that obey the properties described in Definition 2 on test data. The procedure combines three routines to (1) generate viable reporting interfaces (Line 1); (2) assign models over the interface (Line 3); (3) prune the system to limit unnecessary data collection (Line 4). We present complete procedures for each routine in Appendix A and discuss them below.

Figure 3: Performance profile of participatory systems for the saps dataset for each intersectional group in the saps dataset. We plot out-of-sample performance for different levels of participation in the target population. We control participation by varying the reporting cost in a simulated model of individual disclosure. As shown, minimal and sequential systems outperform a generic model at a group level regardless of participation. In regimes where the cost of disclosure is low, participation is high. Consequently, a minimal system will achieve the same performance as a personalized model, and a sequential system will achieve the performance of the component model for each subgroup. We provide details and results in Appendix D.

Model PoolOur procedure takes as input a _pool of candidate models_\(\) to assign over a reporting interface. At a minimum, every pool should contain two models: a personalized model \(h\) for individuals who opt into personalization, and a generic model \(h_{0}\) for individuals who opt out of personalization. A single personalized model can perform unreliably across reporting groups due to differences in the data distribution or trade-offs between groups. Using a pool of models safeguards against these effects by drawing on models from different model classes that have been personalized using different techniques for each reporting group. By default, we include models trained specifically on the data for each reporting group, as such models can perform well on heterogeneous subgroups .

Enumerating InterfacesWe call the ViableTrees routine in Line 1 to enumerate _viable_ reporting interfaces. We only call this routine for sequential systems since minimal and flat systems use a single reporting interface that is known a priori. ViableTrees takes as input a group attributes \(\) and a dataset \(^{}\). It returns all \(m\)-ary trees that obey constraints on sample size and reporting (e.g., users who report male should report age before HIV). By default, we only generate trees so that we have sufficient data to estimate gains at each node of the reporting interface1. In general, ViableTrees scales to tasks with \( 8\) group attributes. Beyond this limit, one can reduce the enumeration size by specifying ordering constraints or a threshold number of trees to enumerate before stopping. For a task with three binary group attributes, \(\) contains \(24\) 3-ary trees of depth 3. Given a complete ordering of all \(3\) group attributes, however, \(\) would have \(1\) tree. We can also consider a greedy algorithm (see Appendix A.4), which may be practical for large-scale problems.

Model AssignmentWe assign each reporting group a model using the AssignModels routine in Line 3. Given a reporting group \(\), we consider all models that could use any subset of group attributes in \(\). Thus, a group that reports age and sex could be assigned predictions from a model that requires age, sex, both, or neither. This implies that we can always assign the generic model to any reporting group, ensuring that the model at each node performs as well as the generic model on out-of-sample data (i.e., _baseline performance_ in Definition 2).

Pruning Reporting OptionsAssignModels may output trees that violate incentive compatibility by requesting personal information that fails to improve performance. This can happen when the routine assigns a model that performs equally well to nested reporting groups - see, e.g., Fig. 2 where the Flat system assigns \(h_{0}\) to \([,]\) and \([,]\).

We can avoid requesting data from reporting groups in such cases by calling the Prune routine in Line 4. This routine takes as input a participatory system \(f_{T}\) and a pruning dataset \(^{}\) and outputs a system \(f_{T^{}}\) with a pruned interface \(T^{} T\). The routine uses a bottom-up pruning procedure that calls a one-sided hypothesis test at each node:

\[H_{0}:_{}(,()) 0 H_{A}:_{}(,())>0\]

The test checks if each reporting group \(\) receives more accurate predictions from the personalized model assigned to its current node or \(\) its parent \(()\). Here, \(H_{0}\) assumes a reporting group prefers the parent model. Thus, we reject \(H_{0}\) when we can reliably tell that \(f_{T}(,)\) performs better for \(\) on the pruning dataset. The exact test should chosen based on the performance metric for the underlying prediction task. In general, we can use a bootstrap hypothesis test  and draw on more powerful tests for salient performance metrics [e.g., 19, 21, 50, for accuracy and AUC].

On ComputationOur approach provides several options to moderate the computation cost of training a pool of models. For example, we can train only two models and build a minimal system. Alternatively, we can also build a flat or sequential system using a limited number of models in the pool. In practice, the primary bottleneck when building participatory systems is _data_ rather than _compute_. Given a finite sample dataset, we are limited in the number of categorical attributes used for personalization. This is because we require a minimum number of samples for each intersectional group to train a personalized model and evaluate its performance. Given that the number of intersectional groups increases exponentially with each attribute, we quickly enter a regime where we cannot reliably evaluate model performance for assignment and pruning [see 42].

On CustomizationOur procedure allows practitioners to learn systems for prediction tasks by specifying the performance metric used in assignment and pruning. A suitable performance metric should represent the gains we would show users (e.g., error for a diagnosis, AUC for triage, ECE for risk assessment). Using a pool of models allows practitioners to optimize performance across groups, which translates to gains at the population level. For sequential systems, the procedure outputs all configurations, allowing practitioners to choose between systems based on criteria not known at training time. For example, one can swap the trees to use a system that always requests HIV status last. By default, we select the configuration that minimizes data collection across groups, such that the ordering of attributes results leads to the most significant number of data requests pruned.

## 4 Experiments

We benchmark participatory systems on real-world clinical prediction tasks. Our goal is to evaluate these approaches in terms of performance, data usage, and consent in applications where individuals have a low reporting cost. We include code to reproduce these results in an Python library.

### Setup

We consider six classification tasks for clinical decision support where we personalize a model with group attributes that are protected or sensitive (see Table 2 and Appendix B). Each task pertains to an application where we expect individuals to have a low cost of reporting and to report personal information when there is any expected gain. This is because the information used for personalization is readily available, relevant to the prediction task, and likely to be disclosed given legal protections related to the confidentiality of health data . One exception is cardio_eicu and cardio_mimic, which are personalized based on race and ethnicity. 2 We split each dataset into a test sample (20% for evaluating out-of-sample performance) and a training sample (80% for training, pruning, assignment, and estimating gains to show users). We train three kinds of personalized models for each dataset:

* _Static_: These models are personalized using a one-hot encoding of group attributes (1Hot), and a one-hot encoding of intersectional groups (mHot)
* _Imputed_: These are variants of static models where we impute the group membership for each person (KNN-1Hot, KNN-mHot). In practice, personalized systems with imputation will range between the performance for these systems and the performance of 1Hot and mHot.
* _Participatory_: These are participatory systems built using our approach. These include Minimal, a minimal system built from 1Hot and its generic counterpart; and Flat and Seq, flat and sequential systems built from 1Hot, mHot and their generic counterparts.

We train all models - personalized models and the components of participatory systems - from the same model class and evaluate them using the metrics in Table 1. We repeat the experiments four times, varying the model class (logistic regression, random forests) and the target performance metric (error rate for decision-making tasks, AUC for ranking tasks) to evaluate the sensitivity of our findings with respect to model classes and use cases.

### Discussion

We show results for logistic regression models and error rate in Table 2 and results for other model classes and classification tasks in Appendix C. In what follows, we discuss these results.

On PerformanceOur results in Table 2 show that participatory systems can improve performance across reporting groups. Here, Flat and Seq achieve the best overall performance on 6/6 datasets and improve the gains from personalization for every reporting group on 5/6 datasets. In contrast, traditional models improve overall performance while reducing performance at a group level (see rationality violations on five datasets for 1Hot, mHot). The performance benefits from participatory systems stem from (i) allowing users to opt out of these instances of "worsenalization" and (ii) assigning personalized predictions with multiple models. Using Table 2, we can measure the impact of (i) by comparing the performance of Minimal vs. 1Hot, and the impact of (ii) by comparing the performance of Minimal to Flat (or Seq). For example, on apnea, 1Hot exhibits a significant rationality violation for group [30_to_60,male], meaning they would have been better off with a generic model. By comparing the performance of 1Hot to Minimal, we see that allowing users to opt out of worsenalization reduces test error from 29.1% to 28.9%. By comparing the performance on Minimal to Flat and Seq, we see that using multiple models can further reduce test error from 28.9% to 24.1%.

On Informed ConsentOur results show how Flat and Seq systems can inform consent by allowing users to report a subset of group attributes (e.g., by including reporting options such as [30+, 2] or [2, HIV+]). Although both Flat and Seq systems allow for partial personalization, their capacity to inform consent differs. In a flat system, users may inaccurately gauge the marginal benefit of reporting an attribute by comparing the gains between reporting options. For example, in Fig. 4, users who are HIV positive would see a gain of \(3.7\%\) for reporting [2, HIV+], and \(16.7\%\) for reporting [30+, HIV+] and may mistakenly conclude that the gain of reporting age is \(16.7\%-3.7\%=13.0\%\). This estimate incorrectly presumes that the gains of \(3.7\%\) were distributed equally across age groups. Sequential systems directly inform users of the gains for partial reporting. In the sequential system, group [30+, HIV+] is informed that they would see a marginal gain of 21.5% for reporting age, while group [<30, HIV+] is informed they would see a marginal gain of reporting age of \(0.0\%\).

On Data MinimizationOur results show that participatory systems perform better across all groups while requesting less personal data on 6/6 datasets. For example, on cardio_eicu, Seq reduces error by \(11.3\%\) compared to 1Hot while requesting, on average, \(83.3\%\) of the data needed by 1Hot. In general, participatory systems can limit data use where personalization does not improve

**Metric** & **Definition** & **Description** \\  Overall & \(_{g}}{n}R_{g}(h_{g})\) & Population-level performance of a personalized system/model, computed as a weighted average over all groups \\  Overall Gain & \(_{g}}{n}_{g}(,)\) & Population-level gain in performance of a personalized system/model over its generic counterpart \\  Group Gains & \(_{g}/_{g}_{g}( ,)\) & Range of group-level gains of a personalized system/model over its generic counterpart across all groups \\  Rationality & \(_{g}|}\ H_{0}|\) & Number of rationality violations detected using a bootstrap test with 100 resamples at a significance of 10\% where \(H_{0}:_{g}(,) 0\). \\  Imputation & \(_{g}_{g}(,^{{}^{}})\) & Worst-case loss in performance due to incorrect imputation. This metric can only be computed for static models \\  Options & \(|-|(h)|.}{ ||}\) & Proportion of reporting options pruned from a system/model. Here, \(\) denotes all reporting options and \((h)\) denotes those after \(h\) is pruned \\  Data Use & \(_{g}}{n}}(,)}{}()}\) & Proportion of group attributes requested by \(h\) from each group, averaged over all groups in \(\) \\ 

Table 1: Metrics used to evaluate performance, data use, and consent of personalized models and systems. We report performance on a held-out test sample. We assume that individuals report group membership to static models, do not report group membership to imputed models, and only report to participatory systems when informed that it would lead to a strictly positive gain, as computed on the validation set in the training sample.

[MISSING_PAGE_FAIL:9]

model that exhibits "worsenalization" in Fig. 1. Even if one could correctly impute the group membership for every person, individuals may receive more accurate predictions from a generic model \(h_{0}\). In practice, imputation is imperfect - as individuals who opt out of reporting their group membership to a personalized model may be assigned "worse" predictions because they are imputed the group membership of a different group. In such cases, opting out may be beneficial, making it difficult for model developers to promote participation while informing consent. Our results highlight the prevalence of these effects in practice. For example, on cardio_eicu the estimated "risk of imputation" is \(-4.6\%\), indicating that every intersectional group can experience an increase of \(4.6\%\) in the error rate as a result of incorrect imputation. The results for KNN-1Hot show that this potential harm can be realized in practice using KNN-imputation, as we find that the imputed system leads to rationality violations on 5/6 datasets.

## 5 Concluding Remarks

We introduced a new family of classification models that allow individuals to report personal data at prediction time. Our work focuses on personalization with group attributes; our approach could be used to facilitate and inform consent in a broader class of prediction tasks. In such cases, the key requirement for building a participatory system is that we can reliably estimate the gains of personalization for each person who reports personal data.

Our results show that participatory systems can inform consent while improving performance and reducing data use across groups. Reaping these benefits in practice will hinge on the ability to effectively inform decision subjects on the impact of their reporting decisions. . Even as there may be good "default practices" for what kind of information we should show decision subjects, practitioners should tailor this information to the application and target audience .

One common concern in using a participatory system arises when practitioners wish to collect data from a model in deployment to improve its performance in the future. In practice, a participatory system can thwart data collection in such settings by allowing individuals to opt out. In such cases, we would note that this issue should be resolved in a way that is aligned with the principle of _purpose specification_. If the goal of data collection is to improve a model, then individuals could always be asked to report information voluntarily for this purpose. If the goal of data collection is to personalize predictions, then individuals should be able to opt out, especially when it may lead to worse performance.

Figure 4: Participatory systems for the saps dataset. These models predict ICU mortality for groups defined by \(==\) using logistic regression component models. Here, \(h_{0}\) is a generic model, \(h_{1}\) is a 1Hot model fit with a one-hot encoding of \(\), and \(h_{2} h_{m}\) are 1Hot and mHot models fit for each reporting group. We show the gains of each reporting option above each box and highlight pruned options in grey. For example, in Seq, the group (HIV+, 30+) sees an estimated \(21.5\%\) error reduction after reporting HIV if they report age. In contrast, the group (HIV+, <30) sees no gain from reporting age in addition to HIV status, so this option is pruned.