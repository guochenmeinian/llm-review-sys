# DeformableTST: Transformer for Time Series Forecasting without Over-reliance on Patching

Donghao Luo, Xue Wang

Department of Precision Instrument, Tsinghua University, Beijing 100084, China

ldh21@mails.tsinghua.edu.cn, wangxue@mail.tsinghua.edu.cn

###### Abstract

With the proposal of patching technique in time series forecasting, Transformer-based models have achieved compelling performance and gained great interest from the time series community. But at the same time, we observe a new problem that the recent Transformer-based models are overly reliant on patching to achieve ideal performance, which limits their applicability to some forecasting tasks unsuitable for patching. In this paper, we intent to handle this emerging issue. Through diving into the relationship between patching and full attention (the core mechanism in Transformer-based models), we further find out the reason behind this issue is that full attention relies overly on the guidance of patching to focus on the important time points and learn non-trivial temporal representation. Based on this finding, we propose **DeformableTST** as an effective solution to this emerging issue. Specifically, we propose deformable attention, a sparse attention mechanism that can better focus on the important time points by itself, to get rid of the need of patching. And we also adopt a hierarchical structure to alleviate the efficiency issue caused by the removal of patching. Experimentally, our DeformableTST achieves the consistent state-of-the-art performance in a broader range of time series tasks, especially achieving promising performance in forecasting tasks unsuitable for patching, therefore successfully reducing the reliance on patching and broadening the applicability of Transformer-based models. Code is available at this repository: https://github.com/luodhhh/DeformableTST.

## 1 Introduction

Time series forecasting is widely used in real-world applications, such as transportation management , economic planning , energy planning  and weather forecasting . Because of the immense practical value, time series forecasting has received great attention and has grown tremendously in recent years .

But looking back at the development of time series forecasting, **Transformer-based models, who have sparked the boom of time series forecasting**, **are constantly being challenged**. In particular, some recent studies  have questioned that attention mechanism is not suitable for modeling the temporal dependency in time series. As the early strike back of Transformer-based models, PatchTST  proposes that attention mechanism can work better in temporal modeling with the help of large size patching technique. Afterwards, equipped with the growing patch size and increasing input length, the advanced Transformer-based models  gain great performance improvement and successfully win back the championship in time series forecasting.

However, with large size patching becoming a must-have technique for the following Transformer-based models, a new problem occurs: **patched-based Transformers have to work with a very long input length and a very large patch size to achieve ideal performance**. But large size patching cannot be apply to all kinds of time series forecasting tasks. For example, some forecastingtasks are with limited input lengths [29; 30; 31], which are not sufficient to be divided into patches. In such condition, the advanced Transformer-based models suffer from severe performance degradation due to the lack of patching [58; 51], limiting their applicability to a wider range of forecasting tasks.

**To broaden the applicability of the Transformer-based model, we need to design an attention mechanism that is less reliant on patching (e.g., can work well with a small patch size or can work well even without patching)**. To this end, we first analyze exactly why attention must work with patching and why patching can help attention better model the temporal dependency in time series forecasting? We visualize the effective receptive fields (ERFs) of PatchTST  in Figure 1. And the ERFs can indicate which parts of the time points in input series are focused by the model when extracting temporal representations. A surprising finding is shown in Figure 1 (left). If without patching, nearly all time points in input series are equally focused by the model and the model performs worse (MSE 0.385), exposing the problem of distracted attention. This finding means that attention has not learned to distinguish the importance of each time point in input series, leading to trivial representation. Note that the time points in a time series are very redundant or even noisy [35; 56; 55; 7; 53], focusing on the trivial part of them will influence the predictions. Thus, an ideal time series forecaster should mainly focus on a small number of important time points which make contribution to better performance and reflect the property of time series. In Figure 1 (right), when using patching, the model focuses on some selected time points and achieve better performance (MSE 0.367), indicating that the model has successfully focused on the important time points. And in terms of why patching can guide the model to learn a non-trivial representation, we find that the pattern of ERF is also divided by patches, which means that patching can force the model to only focus on a small number of important time points based on the patch partition. As a conclusion of above discussion on Figure 1, since **full attention is unable to focus on the important time points by itself, it highly relies on the guidance of patching to focus on the important time points and learn non-trivial representation. This is the reason why full attention must work with patching to achieve ideal performance**.

Therefore, if we can find another way to help attention focus on the important time points, we can get rid of over-reliance on patching. Since full attention is hard to focus due to the redundancy in time series data [35; 56; 55; 7; 53], replacing it with sparse attention can be a natural idea. There are some previous prior-based sparse attentions in time series community [55; 47; 57]. But due to the diverse pattern in different time series, their priors are hard to match all kinds of inputs, resulting in their inferior performance. Different from them, we introduce a data-driven sparse attention called deformable attention under the inspiration of deformable operations [8; 60; 48]. It can sample a subset of important time points from the input series based on the learnable offsets and only calculate attention with these selected important time points. These learnable offsets are learned from each input sample, therefore being more flexible to the diverse property in different time series.

Based on the above motivations, we intend to broaden the applicability of Transformer-based models. To accomplish this goal, **we propose DeformableTST, a Transformer-based model that is less reliant on patching**. Technically, the patching process in our method is optional. We remove the patching process in most cases. Only when the input length is very long, we will use a small size patching for better efficiency. Since the removal of patching will cause severe memory usage in previous plain architecture, we adopt a hierarchical architecture to alleviate this efficiency issue. And we further introduce deformable attention, a data-driven sparse attention that can better focus on the important time points by itself, to achieve excellent performance without patching. Experimentally, DeformableTST achieves the consistent state-of-the-art performance in a wider range of time series tasks, especially in tasks unsuitable for patching, thus successfully reducing the reliance on patching and broadening the applicability of Transformer-based models. **Our contribution are as follows**:

* We dive into the relationship between patching and attention. We point out a new problem that recent advanced Transformer-based models are too reliant on patching. And we further

Figure 1: The Effective Receptive Field (ERF) of PatchTST. A brighter area means that these time points are focused by the model when extracting temporal representation. The results show that PatchTST highly relies on the guidance of patching to focus on the important time points. This phenomenon is also present in multiple advanced patch-based Transformer forecasters (Appendix E).

find out the reason behind this problem is that full attention relies overly on the guidance of patching to focus on important time points and learn non-trivial temporal representation.
* To get rid of the over-reliance on patching, we propose DeformableTST and achieve the consistent state-of-the-art performance in a wider range of time series forecasting tasks. Experimental results show that our deformable attention can better model the temporal dependency in time series without reliance on patching.
* We successfully broaden the applicability of Transformer-based models in time series tasks. Our DeformableTST can flexibly adapt to multiple input lengths and achieve excellent performance in tasks unsuitable for patching, which is a great improvement than previous Transformer-based models.

## 2 Related Work

### Tranformers for Time Series Forecasting

Transformer-based models mainly use attention mechanism to model the temporal dependency in time series [55; 47; 57]. In 2020s, they achieve excellent performance in time series forecasting for the first time and bring great attention to time series forecasting tasks [26; 9; 56; 20; 21; 7]. But their validity is questioned by [52; 18] with the finding that a simple linear layer can outperform complicated attention mechanisms. It's until the proposal of patching that Transformer-based models win back the championship in time series forecasting . Based on patching technique, Pathformer  adopts a multi-scale patches structure. Crossformer  and CARD  further propose to additionally apply attention on variate and feature dimensions rather than only on temporal dimension. Sageformer  combines the graph methods with patch-based Transformer forecasters. And GPT4TS  also transfers pre-trained large language models to time series with the help of patching. But the question of whether attention is suitable for modeling the temporal dependency in time series still remains. For example, although adopting a Transformer architecture, iTransformer  still suggests that linear layers are more appropriate for temporal modeling. Meanwhile, the proposal of patching also comes with a new question that advanced Transformer-based models are too reliant on patching. Therefore, further research about Transformer-based forecasters are still needed, especially on the question of how to better use attention in temporal modeling without over-reliance on patching.

### Sparse Attention

Sparse attention used to be popular in time series forecasting. Early Transformer-based models usually adopt prior-based sparse attention mechanisms. Informer  adopt ProbSparse attention to model the temporal dependency. Autoformer and FEDformer [47; 57] further combine the signal processing technique with the attention mechanisms and select the top-k sparse representation in time domain or frequency domain respectively. But due to the diverse pattern in different time series, these priors are hard to match all kind of inputs, resulting in their inferior performance. As a comparison, data-driven sparse attention, also called deformable attention, is more flexible to diverse inputs. Similar idea has been explored in Computer Vision (CV). Inspired by deformable convolution [8; 59], deformable DERT  proposes multi-scale deformable attention for object detection tasks. And [48; 49] further improve it and make it suitable for general CV tasks. In this work, we propose a deformable attention for time series forecasting to break through the bottleneck faced by previous attention mechanism in modeling temporal dependency.

## 3 DeformableTST

Given an observed multivariate or univariate time series as input, time series forecasting aims to predict the length-\(T\) future series based on the length-\(I\) input series. In real-world scenarios, the input length \(I\) varies from a wide range and is not always sufficient for patching technique, leading to the limited applicability of previous patch-based Transformer forecasters. To tackle this problem, we propose DeformableTST. And we introduce details of DeformableTST in following subsections.

### Structure Overview

As shown in Figure 2, our DeformableTST adopts the encoder-only architecture of Transformer , including the input embedding layer, hierarchical Transformer backbone and prediction head. And following the recent Transformer-based models, we adopt RevIN  to mitigate the distribution shift between the training and testing data.

Input Embedding LayerDenoted \(_{in}^{M I}\) as the \(M\) variates input time series of length \(I\), it will be divided into \(N_{0}\) non-overlapping patches and then embedded variate-independently into \(D_{0}\)-dimensional embeddings:

\[_{0}=(_{in})\] (1)

\(_{0}^{M D_{0} N_{0}}\) is the input embedding. It is worth noting that DeformableTST is less reliant on patching and thus the patching process is optional. We only adopt patching when the input length is very long for efficiency reasons. And we also adopt a much smaller patch size than recent Transformer-based models, making it more adaptable to diverse input lengths.

Hierarchical Transformer BackboneThe backbone is stacked by \(L\) Transformer blocks and utilizes a hierarchical structure. The forward process in the \(i\)-th block is simply formulated as follows:

\[_{i}^{local} =(_{i-1})\] (2) \[_{i}^{global} =(_{i}^{local}+(_{i}^{local}))\] (3) \[_{i} =(_{i}^{global}+(_{i}^{global}))\] (4)

\(_{i}^{M D_{i} N_{i}}\) is the output feature series of the \(i\)-th block, \(i\{1,...,L\}\). And \(D_{i}\) and \(N_{i}\) are the sizes of its feature and temporal dimensions. DeformableAttention is the core component to better capture the global temporal dependency, which will be introduced in Section3.2. \(\) and \(\) are local enhancement modules (Figure 2 (b) and (d)). \(\) is the local perception unit, a depth-wise convolution with residual connection . And \(\) is a feed-forward network injected with a depth-wise convolution . These two modules are adopted to improve the local temporal modeling ability. And a GELU activation  is adopted in \(\) to provide nonlinearity when learning the new feature representation. Meanwhile, to construct a hierarchical structure, a downsampling convolution layer  with kernel size 2 and stride 2 is adopted between two blocks, which will halve the series' temporal dimension and double the feature dimension.

Figure 2: Structure overview of DeformableTST. (a) The input time series is embedded variate-independently. (b) The local perception unit (\(\)) is used to learn the local temporal information. (c) The proposed deformable attention is adopted to learn the global temporal information. (d) The feed-forward network injected with a depth-wise convolution (\(\)) is used to learn the local temporal information and the new feature representation.

Prediction HeadWe first flatten the final representation from the backbone \(_{L}^{M D_{L} N_{L}}\) into \(^{M(D_{L} N_{L})}\). Then we obtain the prediction through a linear projection layer:

\[}=(_{L})\] (5)

Where \(}^{M T}\) is the prediction of length \(T\) with \(M\) variates.

### Deformable Attention

Figure 3 introduces the detailed process of our deformable attention. In each attention module, it first samples a few important time points from the input feature series \(\) based on the learnable offsets. Then the sampled important time points are fed to the key and value projections to get the sampled key and value tokens \(}\), \(}\). Meanwhile, the input feature series \(}\) is also projected into queries \(\). Finally, standard multi-head attention  is applied to \(\), \(}\), \(}\) to obtain the attention output \(\).

Sample the Important Time PointsAs shown in Figure 3 (a), we sample the important time points based on a set of learnable coordinates called sampling points. Specifically, the sampling points are calculated by a set of uniformly sparse reference points and their learnable offsets.

Given a length-\(N\) feature series \(^{M D N}\), we first generate the sparse reference points \(_{ref}^{M 1 N_{samp}}\) from a 1D uniform grid. The grid size \(N_{samp}=N/r\) is downsampled from the input series length \(N\) with a downsampling factor \(r\) to provide sparsity. The reference points indicate the 1D coordinates of some time points uniformly distributed in the feature series \(\) with interval \(r\). These coordinate values are normalized to \([-1,+1]\), where \(-1\) indicates the start of the series and \(+1\) means the end of the series. And these reference points serve as the initial coordinates for the following deforming process.

Then we obtain the offsets for each reference point by offset sub-network (Figure 3 (b)). It contains two convolution layers. The first layer is a depth-wise convolution, which can take the local neighbors into consideration when generating the offsets . It takes the query tokens \(\) as input, where \(\) is the linear projection of the feature series \(\). After a nonlinear activation, the output from the first layer is passed into a point-wise convolution layer to generate the offsets \(^{M 1 N_{samp}}\).

Adding up the reference points with the learnable offsets, we obtain \(N_{samp}\) sampling points, which can serve as the final coordinates to sample the important time points from the feature series \(\). In practice, we follow  and calculate the values of these important time points by linear interpolation \((;)\) to make this sampling process differentiable. The overall process is as follows:

\[ =()\] (6) \[_{samp} =_{ref}+\] (7) \[} =(;_{samp})\] (8)

Figure 3: Deformable Attention. (a) The process of deformable attention from the tensor view and coordinate view. (b) The structure of the offset network, marked with the size of feature series.

where \(}^{M D N_{samp}}\) is the sampled feature series consisting of the important time points. And the implementation of linear interpolation \((;)\) is in Appendix I.1. And we clip \(_{samp}\) by \(-1\) and \(+1\) to avoid sampling outside the feature series.

Calculate Attention OutputIn above sampling process, we have got the query tokens \(\). After the sampling process, we can get the sampled key and value tokens \(}\), \(}\) after two linear projections of the sampled feature series \(}\). Then we calculate the multi-head self-attention with \(H\) heads as:

\[^{(h)}= (^{(h)}}^{ (h)}/+)}^{(h)},h\!=\!1,,H\] (9) \[=( (^{(1)},,^{(H)}))\] (10)

where \(d\!=\!D/H\) is the dimension of each head. The upper index \({}^{(h)}\) denotes the \(h\)-th attention head. After concatenating the output embedding from each attention head \(^{(h)}\) together, we obtain the output of the \(\) module \(^{M D N}\) through a linear projection. \(\) is the deformable relative position bias to provide the positional information into the attention map and its implementation is introduced in Appendix I.2.

To conclude, this subsection introduces the detailed process of \(\) (Eq.(3)). And for the \(i\)-th block, \(\) in this subsection corresponds to \(_{i}^{local}\) in Eq.(3) and \(\) corresponds to \(_{i}^{global}\).

## 4 Experiments

We thoroughly evaluate our DeformableTST on a wide range of time series forecasting tasks, including long-term forecasting tasks with various input lengths, as well as multivariate and univariate short-term forecasting tasks that are unsuitable for patching, to verify the performance and applicability of our DeformableTST.

BaselinesWe extensively include the latest and advanced models in time series community as strong baselines, including patch-based Transformer models: Pathformer , CARD , GPT4TS , PatchTST ; non patch-based Transformer models: iTransformer , FEDformer , Autoformer ; other non Transformer-based models: RLinear , TiDE , TimesNet , DLinear  and SCINet . We also include the state-of-the-art models in each specific task as additional baselines for a comprehensive comparison.

Main ResultAs shown in Figure 4, **our DeformableTST achieves consistent state-of-the-art performance in a broader range of time series tasks**. In details, DeformableTST can flexibly adapt to multiple input lengths and especially achieve excellent performance in tasks unsuitable for patching, which is a great improvement than previous Transformer-based models, **proving that our DeformableTST can successfully reduce the reliance on patching and broaden the applicability of Transformer-based models**. Experiment details and result discussions of each task are provided in following subsections. In each table, the best results are in **bold** and the second best are underlined.

Figure 4: Model performance comparison (left) and performance under different input lengths (right).

### Long-term Forecasting

SetupsWe conduct long-term forecasting experiments on 8 popular real-world benchmarks, including Weather , Traffic , ECL , Solar-Energy  and 4 ETT datasets . **In this paper, we refine the evaluation approach for a comprehensive comparision of the models.** Different from previous settings that use a fixed short input length (e.g., 96) [55; 47; 22]. We fix three different input lengths \(\{96,384,768\}\) and calculate the averaged results to adequately reflect model's adaptability to multiple input lengths. These input lengths covers a variety of real-world application scenarios, i.e., shorter than prediction lengths, within the prediction lengths' interval and longer than prediction lengths. Following the previous settings, we set prediction lengths as \(\{96,192,336,720\}\) and calculate the MSE and MAE of multivariate time series forecasting as metrics.

ResultsTable 1 shows the excellent performance of DeformableTST in long-term forecasting. Concretely, DeformableTST gains the best performance in most cases, surpassing extensive state-of-the-art Transformer-based models. As shown in Figure 4 (right), DeformableTST achieves the consistent state-of-the-art performance in all input lengths and gains continuous performance improvement with the increasing input length, validating its adaptability to multiple input lengths and its effectiveness in extracting useful information from longer history. For comparison, the non patch-based Transformer baselines suffer from performance degradation with increasing input length due to the distracted attention on the prolonging input. And the patch-based Transformer baselines can not work well with a short input length (e.g., 96) because leveraging patching on the short time series leads to very few tokens, limiting attention's ability in long-term modeling.

### Short-term Forecasting

Time series community are currently focusing on long-term forecasting tasks, where the input length and prediction length are adequate for patching technique. However, the short-term forecasting tasks, where the input length and prediction length are limited, are also of extensive practical value in real-world applications. The study on short-term forecasting tasks has been stagnant in recent years and the advanced patching technique proposed in long-term forecasting is hard to apply to short-term forecasting due to the limited input length. To validate the applicability of our DeformableTST in short-term forecasting, we extensively conduct experiments in following two kinds of tasks.

Setups of Multivariate Short-term ForecastingWe conduct multivariate short-term forecasting experiments on 8 popular real-world benchmarks, including Exchange , ILI , 2 ETTh  and 4 PEMS datasets . We set prediction lengths as \(\{6,12,18\}\) and set the input length to be 2 times of the prediction length, which precisely meets the definition of limited input lengths in short-term forecasting. We calculate the MSE and MAE of multivariate time series forecasting as metrics.

Setups of Univariate Short-term ForecastingThe study on univariate short-term forecasting tasks used to be popular in the early time series community [29; 31; 37] but has been stagnant in recent years. In this paper, we bring back this classic tasks and conduct experiments on following datasets: M1 , M3 , M4 , Tourism , NN5 , Hospital  and KDD Cup . Following the classic settings [29; 37], we calculate the SMAPE as metric. Specially for M4 datasets, we follow the

    &  &  &  &  &  &  &  &  &  &  \\  & **(Mon.)** &  &  &  &  &  &  &  &  &  &  & \\  Metric & 385 & MAE & MSE & MSE & MAE & MSE & MSE & MSE & MSE & MSE & MSE & MSE & MAE & MSE & MSE & MAE \\  ETTh1 & **0.413** & **0.40** & 0.409 & 0.449 & 0.446 & 0.403 & 0.458 & 0.429 & 0.459 & 0.488 & 0.444 & 0.461 & 0.463 & 0.505 & 0.483 & 0.483 & 0.485 & 0.482 & 0.455 & 0.429 & 0.434 & 0

[MISSING_PAGE_FAIL:8]

## 5 Model Analysis

### Ablation Study

In order to validate the effectiveness of our designs, we start from a PatchTST  and gradually update it into our DeformableTST by adding our designs step by step. And we provide the trajectory going from a PatchTST to a DeformableTST in Figure 5. To get rid of the over-reliance on patching, we remove the patching design in PatchTST. After this step, PatchTST suffers from degradation in both performance and efficiency. To address these issues, more designs are adopted in DeformableTST.

First, the removal of patching leads to a larger number of tokens processed in the attention computation, resulting in heavier memory usage. Responding to the issue, we adopt a hierarchical structure to gradually reduce the number of tokens, therefore alleviating the efficiency problem. Secondly, full attention is hard to focus on the important time points after removing the patching design, leading to trivial temporal representation and performance degradation. To help attention better focus without patching, we propose deformable attention as a complementary. Thanks to the better focusing ability in deformable attention, we observe great performance improvement after adding this design. Meanwhile, we also adopt some local enhancement modules in our design since locality is also important in time series . And this step also brings performance improvement. After equipped with all our designs, DeformableTST shows great performance and efficiency superiority than the baseline PatchTST, which proves the necessity and effectiveness of our designs.

### Compared Deformable Attention with Prior-based Sparse Attention

In Section 1, we propose that sparse attention can help attention to better foucs without patching and further argue that data-driven sparse attention is more appropriate than prior-based ones. To validate the necessity and effectiveness of using data-driven sparse attention, we compare our deformable attention with some prior-based sparse attentions in time series community, that is ProbSparse Attention in Informer , AutoCorrelation in Autoformer  and FourierAttention in FEDformer . We also include the local window attention in Swin Transformer  and the vanilla full attention  adopted in most baselines [22; 53; 35] for a comprehensive comparison.

As shown in Table 4, our deformable attention surpasses other prior-based competitors in all benchmarks. This is because the priors are hard to match all kind of inputs due to the diverse pattern in different time series, resulting in the inferior performance of prior-based sparse attentions. Different from them, our deformable attention is a data-driven sparse attention that can learn from the input time series, therefore is more flexible to the diverse property in different time series.

Our deformable attention also surpasses the full attention by a large margin for it can better focus on the important time points to learn non-trivial temporal representation, while the full attention suffers from the distracted attention and trivial temporal representation due to the lack of patching. Although window attention can help attention avoid being distracted in a global range by limiting the attention computation into local windows, its performance still decreases due to the lack of long-term modeling ability, which is an important ability a time series forecaster should have.

Figure 5: Ablation study in long-term forecasting tasks. From the top to the bottom, each row means one design that we add on PatchTST to modify it into our DeformableTST. We report the averaged MSE of four prediction lengths. The memory usage is recorded under input-96-predict-96 setting with the same batch size. A lower MSE (a shorter blue bar) and a smaller memory usage (a green star closer to the vertical axis) means better performance and efficiency. More results are in Appendix F.1.

## 6 Conclusion and Future Work

In this paper, we expose an emerging issue faced by advanced Transformer-based models that they have limited applicability in time series forecasting tasks due to their over-reliance on patching. And we further find out the reason behind this problem is that full attention relies overly on the guidance of patching to focus on the important time points and learn non-trivial temporal representation. To tackle this problem, we propose DeformableTST as an effective solution, which equips with deformable attention that can better focus on the important time points by itself to get rid of the over-reliance on patching. Experimentally, DeformableTST achieves the consistent state-of-the-art performance in a broader range of time series forecasting tasks, especially achieving promising performance in tasks unsuitable for patching, therefore successfully reducing the reliance on patching and broadening the applicability of Transformer-based models. And we hope our findings can prompt people to rethink the relationship between Transformer-based models and patching technique, thereby designing more powerful Transformer-based forecasters with a wider range of applicability.