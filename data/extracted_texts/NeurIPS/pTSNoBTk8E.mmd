# DynaDojo: An Extensible Benchmarking Platform

for Scalable Dynamical System Identification

 Logan Mondal Bhamidipaty

Stanford University

loganmb@cs.stanford.edu

&Tommy Bruzzese1

Stanford University

tbru@cs.stanford.edu

&Caryn Tran1

Northwestern University

caryn@u.northwestern.edu

&Rami Mrad

UC Berkeley

ramiratlmrad@berkeley.edu

&Max Kanwal2

Stanford University

kanwal@stanford.edu

###### Abstract

Modeling complex dynamical systems poses significant challenges, with traditional methods struggling to work across a variety of systems and scale to high-dimensional dynamics. In response, we present DynaDojo, a novel benchmarking platform designed for data-driven dynamical system identification. DynaDojo enables comprehensive evaluation of how an algorithm's performance scales across three key dimensions: (1) the number of training samples provided, (2) the complexity of the dynamical system being modeled, and (3) the training samples required to achieve a target error threshold. Furthermore, DynaDojo enables studying out-of-distribution generalization (by providing multiple test conditions for each system) and active learning (by supporting closed-loop control). Through its user-friendly and easily extensible API, DynaDojo accommodates a wide range of user-defined Algorithms, Systems, and Challenges (scaling metrics). The platform also prioritizes resource-efficient training for running on a cluster. To showcase its utility, in DynaDojo 0.9, we include implementations of 7 baseline algorithms and 20 dynamical systems, along with many demo notebooks. This work aspires to make DynaDojo a unifying benchmarking platform for system identification, paralleling the role of OpenAI's Gym in reinforcement learning.3

## 1 Introduction

Dynamical systems, fundamental to disciplines like physics, engineering, economics, and neuroscience, are difficult to predict and control when nonlinear and high-dimensional. Traditional methods that rely on a known underlying model structure fall short when faced with modern problems like stock market forecasting or modeling human social interactions, where the structure is either unknown or non-existent. This has prompted a shift toward data-driven modeling (_system identification_), and especially model-free methods, bypassing the need for predefined equations . To benefit from these data-driven approaches, however, researchers and practitioners need tailored benchmarks to easily evaluate and compare methods for system identification in their area of study.

In this work, we present DynaDojo, an open, extensible benchmarking platform to standardize the process of benchmarking _any_ learning algorithm on _any_ dynamical system. Modeled after OpenAI's Gym  and Procgen , DynaDojo introduces abstractions over algorithms, systems, and challengesto promote ease of use, modularity, and broad compatibility. It includes a growing library of reference algorithms and tunable dynamical systems. Uniquely, DynaDojo facilitates benchmarking scalability via procedural generation and customizable challenges that systematically vary training set size, system complexity, and target error rates. Challenges also allow for assessing out-of-distribution generalization, active learning, algorithm comparison, and cross-system performance. Through its simple yet flexible API, parallelized execution, and interactive demos, DynaDojo aims to serve as an accessible yet rigorous benchmarking platform for the system identification community.

## 2 Related work

Because of the shared contexts in the fields of system identification and reinforcement learning (RL), we draw upon and connect literature from both areas to motivate the development of our benchmarking platform. While system identification focuses on accurately modeling system dynamics, RL aims to optimize inputted actions to a system to maximize reward. Despite their distinct objectives, they both seek to model and interact with complex environments and can often be used to solve similar or overlapping problems. For a summary comparison of DynaDojo against existing benchmarks, see Table 1.

### Benchmarks and benchmarking platforms

Single-System BenchmarksIn the field of system identification, benchmarks have been created that use data on one specific dynamical system from a physical phenomenon of interest [4; 5]. These single-system benchmarks have been used to evaluate specific learning algorithms  or to compare different approaches . These narrowly focused benchmarks do not facilitate the evaluation of an algorithm's generalization across a diverse set of systems, which is a central aim of our work.

Benchmark SuitesBenchmark suites offer a broader scope for evaluating system identification algorithms by covering a larger subset of system classes. These suites, however, are restricted to specific types of systems or representations, such as chaotic systems [8; 9], physical systems , or partial differential equations [11; 12]. DynaDojo is agnostic to the type of dynamical system to allow

Figure 1: Pipeline for how to use DynaDojo. Select an algorithm (Step 1) and a system (Step 2), then instantiate a challenge (Step 3), and evaluate to get results (Step 4). It is easy to run repeated DynaDojo challenges to compare performances on in- vs. out-of-distribution data, with vs. without active learning, between algorithms, and across systems (Step 5).

for a diverse range of systems to be included. We implement 20 such systems (Figure 2) and provide a simple wrapper interface to use to add more.

OpenAI GymTo address the fragmentation and lack of standardization in system identification benchmarks, we adopt a similar approach to OpenAI's Gym environment . OpenAI Gym offers a common interface for RL benchmarking tasks and, with over 6,500 citations, has become a standard benchmark framework in RL. Our work creates an extensible, standardized gym-like platform to unify system identification benchmarks. Contrary to OpenAI's focus on environments and _not_ agents , we provide abstractions over both entities (systems and algorithms, respectively) and additionally implement challenges that orchestrate benchmark evaluation while scaling parameters such as system complexity and training set size. In Subsection 2.2 we motivate our focus on scaling system complexity and training samples. And in Section 3.4, we explain why we implement challenges for benchmark orchestration.

### Generalization and scaling

In designing DynaDojo, we drew upon a variety of literature to determine desirable features and evaluation metrics to implement in our platform.

Out-of-Distribution GeneralizationIn system identification, there is interest in understanding how well models can generalize beyond the training distribution.  probes how deep learning models generalize to trajectories from out-of-distribution initial conditions for dynamical systems.  investigates whether deep neural networks learning cellular automata show out-of-distribution generalization for unseen initial configurations with different rule sets. Recent RL benchmarks have sought to split train and test data to draw from different gaming environments in response to problems of overfitting on training environments [15; 16]. Motivated by this work, DynaDojo enables evaluating algorithms on both in-distribution and out-of-distribution data.

Cross-System GeneralizationWhile we are unaware of any work in system identification that explores generalization across different systems, the concept has been explored in RL. Generalization in RL has expanded from considering unseen states to learning across different domains, as exemplified by AlphaZero's ability to learn Go, Chess, and Shogi , compared to AlphaGo's specialization in Go . It is also similar the field of multi-task learning in which one general policy might train on several different environments . This trend is reflected in RL benchmarks and toolkits that measure performance across a diverse variety of environments [2; 3]. Our work takes an analogous approach by facilitating easy evaluation of an algorithm's performance across different classes of dynamical systems, thereby capturing the algorithm's cross-system generalization capabilities.

Scaling ComplexityThere is a desire to understand how algorithms perform on systems of varied complexity. For example, within system identification,  and  test deep neural networks on their generalization capabilities for learning cellular automata with varied neighborhood sizes.

In RL, benchmarks have been designed to train and test algorithms on game environments with varying difficulty levels . This work on games, however, uses imprecise notions of difficulty which

    & Systems & Algorithms & Extensible & OOD &  Active \\ Learning \\  &  Sample \\ Efficiency \\  & 
 Complexity \\ Measure \\  \\  Single-System [4; 5] & 1 & & & & & & & Fixed \\ Chaos  & 131 & & & & & & & Fixed \\ PDEArena  & 5 & 13 & \(\) & & & & & \\ PDEBench  & 11 & 3 & \(\) & \(\) & & & & \\ nm-benchmark  & 4 & & \(\) & \(\) & & & & \\ Procegn* & 16 & & \(\) & \(\) & \(\) & \(\) & Binary \\
**DynaDojo** & 20 & 7 & \(\) & \(\) & \(\) & \(\) & Tunable \\   

Table 1: Comparison with related system identification benchmarks. DynaDojo, to the best of our knowledge, is the first extensible dynamical systems benchmark that evaluates out-of-distribution (OOD) trajectories, supports active learning, benchmarks sample efficiency, and implements dynamical systems with tunable complexity measures. \({}^{*}\)Note: Procegn is an RL benchmark.

Figure 2: The 20 dynamical systems currently packaged in DynaDojo. Systems are annotated with the system type (discrete, continuous, or hybrid) and their measure of complexity. Users can adjust nearly all DynaDojo parameters on these systems, including how trajectories are initialized, simulated, and controlled, as well as system complexity and evaluation.

are calibrated roughly on the training speeds of baseline algorithms, but this definition is inconvenient because it is hard to generalize and test.

In system identification,  provides a suite of over 100 benchmark datasets of known chaotic dynamical systems. Each system is annotated with mathematical properties reflecting the complexity of the system. These annotations facilitate comparison of learning methods across dynamical systems of varying complexity; our work is similarly motivated. In DynaDojo, instead of providing fixed datasets corresponding to different complexity levels, dynamical system classes are defined to allow their complexity levels to be programmatically scaled. This mirrors the approach in  which evaluates physics-informed neural networks on a pair of dynamical systems with tunable parameters that control complexity.

Active LearningIn RL, active learning--where the algorithm selects control inputs to intelligently explore state-space--enhances sample efficiency. While dynamical systems can accept control inputs and therefore can support active learning, this property is often underutilized in system identification benchmarks that predominantly use static datasets [4; 5; 8; 10; 12]. In our work, we enable algorithms to provide control inputs to dynamical systems, thereby facilitating active learning approaches and interactive data generation.

Sample EfficiencyThe ability to learn effectively from a limited number of training samples is particularly relevant as it directly influences an algorithm's performance in real-world scenarios where data may be scarce, expensive to obtain, or hard to simulate . In DynaDojo, we've designed metrics that measure how an algorithm's performance scales with changes in system complexity and training dataset size. This feature enables users to assess an algorithm's sample efficiency, particularly as it handles increasing complexity.

## 3 Overview of DynaDojo

DynaDojo operates on three core objects: _Algorithms_, _Systems_, and _Challenges_. Run a Challenge with any given Algorithm and System to evaluate how a learning algorithm's performance scales (Figure 1). DynaDojo currently provides a suite of 7 Algorithms, 20 Systems, and 3 Challenges to be used. Additionally, DynaDojo provides abstract interfaces for Algorithms, Systems, and Challenges that can be extended to support custom implementations (Figure 3).

### Algorithms

_Algorithms_ are subclasses of AbstractAlgorithm, an interface designed to wrap any learning algorithm that can be used for system identification. At initialization, algorithms are provided the dimensionality of the data which can be used to define the appropriate parameters for the algorithm,

Figure 3: Users can extend DynaDojo by implementing their own Algorithm, System, or Challenge.

for example, the number of layers in a neural network or degree of a polynomial in a polynomial regression. The AbstractAlgorithm interface further abstracts the details of the algorithm under fit, predict, and act methods for ease of use and simplicity. See the online documentation for additional details on the 7 algorithms included in DynaDojo.

### Systems

_Systems_ are subclasses of AbstractSystem, an interface to wrap any procedurally generated dynamical systems. Systems are initialized with a latent dimension and embedding dimension which determine the complexity of the system and its generated trajectories. The AbstractSystem interface abstracts the details of simulating the system under the make_init_conds and make_data methods. Initial conditions can be produced in-distribution or out-of-distribution. Data can be generated with noise or optional control inputs. The AbstractSystem interface also abstracts the evaluation metric for any particular dynamical system with the calc_error method. For example, continuous systems, such as linear dynamical systems, might implement mean-squared error to evaluate the accuracy of predicted trajectories, whereas a binary system, such as cellular automata, might use Hamming distance instead. Other system-specific metrics, such as achieving stability with control, can likewise be defined in calc_error. See Figure 2 for the systems that we package with DynaDojo and the adjustable parameters available for systems.

### Challenges

_Challenges_ are subclasses of AbstractChallenge, an interface to orchestrate the evaluation of algorithms on systems. Challenges simplify and parallelize repeated trials of training and testing algorithms on systems while a parameter (of the system, algorithm, or training process) scales. A challenge is run via the evaluate method: This method handles parameter scaling, seed generation (for reproducible execution), and job parallelization. evaluate calls on execute_job, which is where one defines the protocol for algorithm and system instantiation, data generation, and testing for a single trial. The plot method visualizes the results from the challenge evaluation.

In DynaDojo 0.9, we implement three challenges to evaluate scaling: FixedComplexity, FixedTrainSize, and FixedError. In FixedComplexity, we repeatedly train and test an algorithm on a system of fixed complexity while scaling the number of training samples. In FixedTrainSize, we repeatedly train and test an algorithm on systems of increasing complexity while fixing the number of training samples. In FixedError, we search for the number of training samples necessary for an algorithm to achieve a target error rate on systems of increasing complexity. In Figure 4, we visualize the relationship between these three scaling challenges.

Figure 4: DynaDojo Challenges provide a snapshot of an Algorithm’s scaling behavior along one slice of a “performance landscape” relating three dimensions: system complexity (C), prediction error (E), and number of training samples (N). Each Challenge varies parameters along one dimension while holding a second one constant, to measure the algorithm’s performance on the third.

### Design

Unified but decoupled.Rather than providing a single or suite of benchmarks, DynaDojo is a benchmarking _platform_. We designed abstractions over algorithms _and_ systems to unify system identification benchmarking under one platform. This allowed us to package 20 Systems and 7 Algorithms in DynaDojo and standardize comparing them via challenges for better consistency. While each system and algorithm work seamlessly together, they are not locked into the platform. To ameliorate the inflexibility of requiring interfaces over both algorithms and systems, we designed them to be decoupled from another and thus independently usable outside of DynaDojo (see Section 4.1).

Focused on scaling, not error.Often, algorithms are evaluated by their performance on a single task. On the contrary, we are focused on how that performance scales as the task gets harder or more data is provided (see Figure 4). Specifically in system identification, we saw an opportunity to numerically define task difficulty via measures of complexity. To evaluate scaling, many rounds of instantiating algorithms and systems, training, and testing must be repeated as parameters scale. This can lead to embarrassingly parallel resource-intensive workloads. Thus, we conceived of Challenges to manage job parallelization across cores and compute nodes. We also packaged a DynaDojo Docker container for easy use on cluster environments.

Procedurally generated, not static.Rather than a static dataset, dynamical systems in DynaDojo procedurally generate data at train and test time in order to support active learning, where system trajectories are altered by control inputs provided from an algorithm. Additionally, we require a tunable, continuous measure of complexity for each system to support scaling metrics. This requires that dimensionality of system trajectories must be dependent on the system complexity-another reason for procedural data generation. Compared to pre-computed datasets, a limitation of our work is that procedural generation of data can be costly, especially for systems of high complexity.

Simple by default.A key challenge in system identification is being able to compare one algorithm against others across many systems. This is difficult because various existing algorithms and systems either lack a simple API or all have different APIs. Wrestling together APIs and setting the right parameters is a major barrier to benchmarking in system identification. We designed DynaDojo with a focus on simplicity of the API to ensure that different algorithms can painlessly run on any system. To enable this, all systems and algorithms must come with default presets for all parameters, with optional overrides. This requires developers to do work upfront to determine reasonable or adaptive settings for the algorithms or systems they contribute.

Flexible and versatile.We support a variety of settings to enable features such as OOD data generation and control inputs in order to ensure DynaDojo covers broad use-cases (Figure 2). Our extensible interfaces for algorithms, systems, and challenges to ensure that DynaDojo can be adapted to use-cases we have not yet covered (Figure 3). To show the versatility of DynaDojo as a benchmarking platform, we provide numerous example Jupyter notebooks for implemented algorithms, systems, and challenges, available in our GitHub repository.

## 4 Example Usage

### Running a single algorithm on a single system

DynaDojo algorithms and systems can be used independently of challenges. To train and test a single algorithm instance on a single system instance, first instantiate the system and create the training and test data (which, in this example, is OOD).

```
1latent_dim,embed_dim,train_size,test_size,timesteps=(3,3,50,10,50)
2lorenz=LorenzSystem(latent_dim,embed_dim,seed=100)
3x0=lorenz.make_init_conds(train_size)
4y0=lorenz.make_init_conds(test_size,in_dist=False)
5x=lorennz.make_data(x0,timesteps=timesteps)
6y=lorenz.make_data(y0,timesteps=timesteps,noisy-True) ```

Then instantiate the algorithm, fit on the training data, predict, and calculate error. Plotting utilities, demos, and examples are provided in our GitHub repository.

```
1sindy=SINDy(embed_dim,timesteps,seed=100)
2sindy.fit(x)
3
4#predicttrajectories
5y_pred=sindy.predict(y[:,0],timestimesteps)
6error=lorenz.calc_error(y,y_pred)
7
8fig,ax=dynadojo.utils.plot([y_pred,y],target_dim=min(3,latent_dim),labels-["pred","truth"]) ```

(a) Code for fitting and testing SINDy on a LorenzSystem using DynaDojo. Plotting utilities are provided to visualize high dimensional systems.

### Running a challenge

To evaluate, for example, how linear regression generalizes to linear dynamical systems of increasing complexity, run FixedTrainSize. First, decide on the complexities (latent dimensions) to scale across, the number of training samples, the number of trials, and the training and testing conditions. Supply these arguments to instantiate a FixedTrainSize challenge. Then, evaluate the challenge with the algorithm class and plot the results. By default, challenges are run without parallelization; however, code examples showing parallelization across cores and computers are provided on GitHub.

```
1challenge=FixedTrainSize(
2Latent_dims=np.logspace(1,3,...20,include_end-True),train_size=100,timesteps=50,control_horizons=0,max_control_cost_per_dim=0,system_cls=LDSystem,trials=100,test_examples=50,test_timesteps=50)
12data=challenge.evaluate(
13LinearRegression,noisy-True,ood-True)
14
15FixedTrainSize.plot(data) ```

(a) Code for running FixedTrain challenge with on a training set size of 100 for LinearRegression on LDSystem with latent dimensions from 10 to 1000.

#### 4.2.1 Analyses

With a suite of baseline algorithms and dynamical systems, DynaDojo is designed to support running repeated challenges to analyze algorithms across systems, schematically depicted in Figure 1.

[MISSING_PAGE_FAIL:9]

## 5 Future work

DynaDojo is still a work in progress. As of our most recent release, our platform has certain limitations worth noting: While we implement parallel computing across all Challenges, the FixedError challenge is still especially resource-intensive and susceptible to noise. We aim to implement more sophisticated root-finding search algorithms to replace our exponential search.

To improve the rigor of our scaling results, we would like to move to a more objective measure of system complexity (e.g., the intrinsic dimension of the objective landscape ). We also seek to develop scaling metrics that summarize the results plotted by DynaDojo. Furthermore, we would like to define a unified generalization metric that captures an algorithm's capacity to work on OOD test data, across scales of complexity, and on different dynamical systems altogether.

We will, of course, always look for ways to include more state-of-the-art algorithms and dynamical systems of interest. In particular, we would like to develop simple wrappers for OpenAI Gym environments and algorithms to immediately accommodate their vast library, and vice-versa, wrapping DynaDojo Systems and Algorithms for OpenAI Gym.

To further broaden the scope and applicability of DynaDojo, we plan on introducing new Challenges of interest to the community. For example, we aim to incorporate an optimal control challenge involving stabilizing a system around a target trajectory, a transfer learning challenge focusing on fine-tuning to new system data, a multi-task learning challenge around maintaining consistent performance across multiple dynamical systems, and a curriculum learning challenge focusing on leveling up system complexity without retraining an algorithm from scratch. All of the Challenges can also be extended to measure prediction/control error on multiple timescales.

Lastly, in light of the emerging importance of scaling laws in deep learning, we hope to incorporate new scaling dimensions. These will include the number of model parameters, computational cost of training, and activation sparsity, on top of the three existing scaling dimensions. By including these dimensions, we aim to offer a more comprehensive view of how algorithms scale.

Figure 10: Cross-system generalization: SINDy is evaluated on Lorenz Systems (left) and linear dynamical systems (LDS) (right) in a FixedTrainSize challenge. Error (\(y\)-axis) decreases with complexity (\(x\)-axis) for Lorenz, but increases and plateaus with complexity for LDS. Also, OOD (orange) and in-distribution (blue) error are matched, showing OOD generalization.

## Author contributions

Using the Contributor Roles Taxonomy (CRediT)2:

* LMB contributed to the software (lead on design, documentation; support on algorithms/systems), data curation (equal on demo notebooks).
* TB contributed to the visualization (lead on creative direction; co-lead on figures), software (lead on algorithms/systems; support on documentation), data curation (equal on demo notebooks), writing (editing).
* CT contributed to the writing (co-lead), software (lead on reproducibility, parallelization, testing; support on design), visualization (co-lead on figures), and led the investigation, validation.
* RM contributed to the software (support on algorithms/systems).
* MK led the conceptualization, supervision, project administration, methodology, and contributed to the writing (co-lead), software (all-around support).