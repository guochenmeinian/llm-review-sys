# An engine not a camera:

Measuring performative power of online search

Celestine Mendler-Dunner\({}^{*,@sectionsign}\)Gabriele Carovano\({}^{}\)Moritz Hardt\({}^{@sectionsign}\)

\({}^{*}\)ELLIS Institute Tubingen

\({}^{@sectionsign}\)Max-Planck Institute for Intelligent Systems, Tubingen and Tubingen AI Center

\({}^{}\)Italian Competition Authority

###### Abstract

The power of digital platforms is at the center of major ongoing policy and regulatory efforts. To advance existing debates, we designed and executed an experiment to measure the performative power of online search providers. Instantiated in our setting, performative power quantifies the ability of a search engine to steer web traffic by rearranging results. To operationalize this definition we developed a browser extension that performs unassuming randomized experiments in the background. These randomized experiments emulate updates to the search algorithm and identify the causal effect of different content arrangements on clicks. Analyzing tens of thousands of clicks, we discuss what our robust quantitative findings say about the power of online search engines, using the Google Shopping antitrust investigation as a case study. More broadly, we envision our work to serve as a blueprint for how the recent definition of performative power can help integrate quantitative insights from online experiments with future investigations into the economic power of digital platforms.

## 1 Introduction

At the heart of one of Europe's most prominent antitrust case is a seemingly mundane question: How much can a search engine redirect traffic through content positioning? In 2017, the European Commission alleged that Google favored its own comparison shopping service by steering clicks away from search results towards Google's own product comparison service. The technical centerpiece of the case was an ad-hoc data analysis about the position and display biases of Google search results. Google appealed the European Commission's charges, pointing to, among other arguments, methodological errors.1

The case is emblematic of a broader problem. Although urgently needed, there is currently no accepted technical framework for answering basic questions about the economic power of digital platforms. Lawyers, economists, and policy makers agree that traditional antitrust tools struggle with multi-sided platforms . Against this backdrop, a recently developed concept from the machine learning literature, called performative power , suggests a way to augment existing antitrust enforcement tools and mitigate some of their limitations. Performative power measures how much a platform can causally influence platform users through its algorithmic actions. By directly relating power to a causal effect, it sidesteps the complexities underlying conventional market definitions and offers a promising framework to integrate data and experimental methods with digital market investigations. Although the definition of performative power enjoys appealing theoretical properties, a proof of its practical applicability was still missing.

Our contributions.We present a first proof of concept showing how to use performative power as an investigative tool in practice. The instantiation of performative power we consider is motivated by the recent Google Shopping antitrust investigation ran by the European Commission against Alphabet Inc. It concerns the ability of a search engine to impact web traffic through decisions of how to arrange content.

Our core contribution is to design and implement an online experiment to establish a lower bound on performative power for the two most widely used search engines, Google Search and Bing, by providing quantitative insights into the causal effect of algorithmic updates on clicks. Our experiment is based on a browser extension, called Powermeter, that emulates updates to the platform's algorithm by modifying how search results are displayed to users. The arrangement to which a user is exposed is chosen at random every time they perform a search. We implement different counterfactual arrangements to inspect the effect of re-ranking and favored positioning (e.g., Ads or Shopping boxes) on clicks, both in isolation and jointly. We discuss several technical steps we implemented to take care of the internal validity of our experimental design.

Using Powermeter we collected data of about 57,000 search queries from more than 80 different subjects, over the period of 5 months. Our experiment is designed to measure the causal effects of arrangement under natural interactions of users with the platform and the queries for any given user follow the distribution of queries under their every-day use of online search. Figure 1 provides a first glimpse into the observed effects. In summary, we find that consistently down-ranking the first element by one position causes an average reduction in clicks of \(42\%\) for the respective element on Google search. Down-ranking the same element by two positions yields a reduction of more than \(50\%\). For Bing we find an even larger effect of ranking, although with less tight confidence intervals due to the small number of Bing queries performed by our participants. When combining down-ranking with the addition of Ads or Shopping boxes, the effect of arrangement is even more pronounced, showing a distortion in clicks for the first result of \(66\%\) averaged across queries where such elements are naturally present on Google search. Inspecting different subsets of queries we find that the effect of position is larger for queries with a high number of candidate search results. To the best of our knowledge, we are the first to offer independent quantitative experimental insights into display effects on Google search and Bing.

Finally, we outline how to formally relate our quantitative piece of evidence to questions about self-preferencing relevant in the context of the Google Shopping case. Together, we hope our empirical and theoretical results can serve as a first blueprint for what future antitrust investigations of digital platforms' market power based on performative power might look like.

## 2 Preliminaries and related work

The market power of digital platforms is the subject of a robust debate in policy, legal, and technical circles. See, for example, Newman , Cremer et al. , Stigler Committee , Furman , Cabral et al. . Conventional antitrust enforcement tools have been put into question  and new concepts of market power have been proposed to deal with the complexities of digital markets . These account for the multi-sided nature of the markets as well as the role of behavioral weaknesses of consumers--albeit with limited success. We refer to a comprehensive literature survey about behavioral aspects in online market by the UK Competition and Market Authority .

Figure 1: The ability to influence web traffic through content arrangement. Blue bars show average click probability observed for generic search results in position 1 to 6 on Google search under different counterfactual arrangements; default arrangement (left), swapping results 1 and 2 (middle), swapping results 1 and 3 (right). We provide a detailed discussion in Section 5 where we also explore arrangement changes beyond reranking.

Performance power.The concept of performative power is inspired by recent developments in performative prediction  from the computer science literature. We refer the reader to Hardt and Mendler-Dunner  for a recent survey on the topic. A robust insight from performative prediction is that beyond learning patterns in data, the ability to _steer_ the data-generating distribution similarly factors into a predictive system's performance. Performative power  recognizes that the ability to steer depends on _power_--in terms of reach and scale--of the platform making the predictions. Thus, the core idea behind the new notion of power is to measure the degree to which predictions are performative to obtain an estimate of the power of a platform. Formally, performative power relates the ability of a platform to steer the population of participants, to the causal effect of algorithmic actions.

**Definition 1** (Performative power ).: Given the algorithmic action \(a_{0}\) and a set of alternative conducts \(\), a population \(\) and an outcome variable \(z\). Performative power is defined as

\[:=_{a}\ |}_{q }\|z_{a_{0}}(q)-z_{a}(q)\|_{1}, \]

where \(z_{a_{0}}(q)\) denotes the outcome for unit \(q\) under \(a_{0}\) and \(z_{a}(q)\) denotes the counterfactual outcome, would the platform implement \(a\) instead. Expectations are taken with respect to the randomness in the potential outcome.

Performative power is a measure of influence that predictive systems can have over their participants. It offers a family of definitions that can be instantiated flexibly in a given context. The specific meaning is determined by each instantiation. Performative power can be applied forward-looking to understand whether a platform has the ability to plausibly cause a specific change, as well as in retrospect to measure the effect of an observed conduct. In this work we use performative power to quantify the effect of an algorithmic update \(a^{*}\) central to a recent antitrust investigation against Alphabet Inc. ran by the European Commission.2 In practical terms, we instantiate \(\) with a set of conservative and implementable counterfactuals such as to provide a plausible lower bound on the effect of \(a^{*}\).

The Google Shopping case.In 2017 the European Commission imposed a fine of 2.42 billion EUR on Alphabet Inc. for "abusing its dominance as a search engine by favouring its comparison shopping service.".3 The General Court dismissed Google's action against the decision in 2021 and the Court of Justice of the European Union upheld the Court's ruling in 2024. It represents a landmark in EU competition law. The conduct under investigation concerned a specific update to the Google search algorithm. The update **a)** demoted rival comparison shopping services among the general search results, often by multiple positions, and, at the same time, **b)** systematically gave prominent placement to Google's own comparison shopping service by triggering visually appealing boxes for shopping queries, reserved for Google's own service. The goal of this work is to provide quantitative insights into the effect of this conduct on web traffic by means of online experiments.

Display effects.Consumer choices on digital platforms are critically mediated by how platforms present content to users. Choice architecture designs , presentation bias , position bias , and trust bias  are known to play an important role. There is a rich literature in machine learning aiming to mechanistically understand such biases for debiasing click data , building better ranking models and auctions , and interpreting user feedback in recommender systems , to name a few. Unfortunately behavioral aspects often resist a clean mathematical specification. By focusing on measuring a directly observable statistic, performative power circumvents the challenges of modeling behavioral biases for monitoring, auditing and measuring digital economies.

Measuring the effect of algorithmic updates.Several works have been interested in measuring the effects of potential arrangement changes of online platforms. For example, Ursu  rely on public data collected under randomized result ordering to investigate the role of positioning on Expedia. Narayanan and Kalyanam  investigated position bias in search advertising using a regression discontinuity design. Also focusing on online advertising, Agarwal et al.  investigate position bias by experimentally randomizing bids to indirectly influence the ranking. Similarly in information retrieval researchers have studied active interventions in the form of order randomization , or relied on harvesting click data collected under multiple historical rankings . In our work we collect experimental data ourselves. We use a browser extension to emulate the algorithmic updates of interest _without_ requiring control over the platform's algorithm.

Browser extensions have previously been used as a tool for automatically collecting data to audit systems. Robertson et al.  audit Google search for polarization on politically-related searches. Gleason et al.  collect data via an extension to directly investigate the effect of search result components on clicks. Also the ongoing National Internet Observatory  relies on a browser extension to collect web traffic data. While prior works focus on collecting observational data for monitoring systems, we use the extension to conduct online experiments.

## 3 Performativity in online search

We start by formalizing the causal question under investigation. We model an online search platform as a distribution over _events_. An event is a triplet of a user query \(Q\), content arrangement \(A\) and click outcome \(C\). A user query corresponds to a person visiting the search page and entering a search query in the search bar. The query is processed by the platform and results in an arrangement of content on the website. The mapping is typically defined by a proprietary pipeline involving a ranking algorithm that determines the order in which search results are ranked and displayed, including the positioning of components such as Ads or featured elements. Then, mediated by the arrangement, the user query leads to a click outcome \(C\). The categorical random variable \(C\) indexes the element clicked over by the user. It is a function of the user query and the arrangement.

### From the causal effect of arrangement to performative power

Assume the platforms were to change their algorithm that determines the content arrangement. We seek to answer the following causal questions: _How much does a change to the arrangement impact clicks of a content element on the search page_?

If clicks were solely determined by stable preferences, then we would see no effect. Performativity is the reason why we see an effect. Display baises, the limited ability to process large amounts of data, and trust in the platform can be a source for performativity. The more performative the arrangement is, the stronger the effect. We use \(a_{0}\) to refer to the reference arrangement of results on Google search. For a given user query \(q\) we define the potential outcome of a click event under the arrangement \(a\) as \(C_{q}(a)\). The variable \(C\) takes on categorical values, indexing the elements on the page. Let \(\{c_{1},...c_{K}\}\) denote the top \(K\) general search results indexed in the order they appear under \(a_{0}\).

**Definition 2** (Performativity gap).: Given a counterfactual arrangement \(a^{}\), we define the _performativity gap_ at position \(i\) with respect to a population of queries \(\) as

\[^{i}(a^{})=[1\{C_{q}(a^{})=c_{i}\} ]-[1\{C_{q}(a_{0})=c_{i}\}],\]

where expectations are taken over queries \(q\) and the randomness in the potential outcome.

The performativity gap quantifies how much the click through rate of search item \(c_{i}\) changed, in expectation across queries \(\), had the platform deployed arrangement \(a^{}\) instead of arrangement \(a_{0}\). The following result generalizes Theorem 8 in Hardt et al. :

**Theorem 1** (Lowerbound on performative power).: _Let \(\) be the performative power of a search platform defined with respect to a set of arrangements \(\), a population of search queries \(\) performed on the platform, and the outcome variable \(z_{a}(q)=1\{C_{q}(a)=c_{1}\}\). Then, performative power is lower bounded by the performativity gap as \(_{a}^{1}(a)\)._

Note that the instantiation of performative power in Theorem 1 to which we relate the performativity gap measures a platform's ability to steer _outgoing_ traffic from its online search website. We will discuss how to relate this notion to a broader discussion of the power of online search in vertically integrated markets in Section 6.

Algorithmic distortion.Often it can be useful to express the performativity gap relative to the base click rate. Thus, we define the _algorithmic distortion factor_ as the smallest factor \(>0\) such that

\[^{i}(a^{})[1\{C_{q}(a_{0})=c_{i}\} ]. \]

This quantity serves as a way to denote the fraction of clicks taken away from content item \(c_{i}\) as a result of the update \(a_{0} a^{}\). Also, as we will see, it helps to express performative power relative to a base click through rate which offers a more interpretable quantity for investigators.

### Estimating the performativity gap using an RCT

To estimate the performativity gap for the different arrangements, we rely on a randomized controlled trial (RCT), the gold standard methodology to estimate causal effects [35; 36; 37]. As we can not observe a search query simultaneously exposed to different arrangements, the idea of an RCT is to randomly select, for each query \(q\), the arrangement they are exposed to. We write \(Q_{a}\) for the subset of queries that are exposed to treatment \(A=a\). We also refer to these subsets as treatment groups. Comparing the click events across groups allows us to obtain an estimate of the performativity gap as

\[^{i}(a^{})=^{i}(a^{})-^{i}(a_{ 0})^{i}(a)=|}_{q Q_{a}} 1\{C_{q}(a)=c_{i}\}.\]

For \(^{i}(a^{})\) to provide an unbiased estimate of \(^{i}(a^{})\), we rely on an application of the stable unit treatment value assumption (SUTVA) , referred to as isolation assumption by Bottou et al. :

**Assumption 1** (Independence across queries).: User behavior in response to query \(q\) is not affected by the treatment status of other queries, i.e., for all \(q Q\) we have \(C_{q}(A_{q})\!\!\! A_{q^{}}\  q^{} q\) where \(A_{q}\) denotes the random variable assigning query \(q\) to a treatment group.

This assumption justifies why we can interleave the measurement of different interventions. It requires that the intervention performed on one query does not change individuals' browsing behavior in response to subsequent queries. Crucially, this can only be satisfied, if individual interventions under investigation do not impede user experience in a lasting manner. In the following section we discuss steps we take in our experimental design towards justifying Assumption 1.

More broadly, the key advantage of using an experimental approach to measure the performativity gap is that, while the mechanism mapping user queries to clicks can be arbitrarily complex, this complexity does not affect the experiment. Aspects such as users' preference for clicking links on the left side of the screen , the effect of visually appealing elements , users' trust in the platform , or the relevance gap between search results will naturally enter our measurement.

## 4 Powermeter: Experiment in the wild

We designed an online experiment to measure the performative power of two popular online search platforms operated by Google and Bing. The experiment is built around a Chrome browser extension that modifies the arrangement of search result pages and records user click statistics in a privacy-preserving fashion. The extension allows us to observe an organic set of user queries and click outcomes under different arrangements without having control over the platforms' algorithm.

### Browser extension

Browser extensions can add functionalities to the web-browser and change how a website is displayed to the user. Powermeter makes use of these functionalities to emulate algorithmic updates by implementing different counterfactual arrangements on Google search and Bing search. We emphasize that Powermeter only hides or reorders, but never modifies or adds any content on the search page.

Technical details.Once activated, the extension triggers the experiment whenever the user enters a search query on either Google search or Bing search. This can be identified by monitoring the url string of the current tab. Before search results are loaded the extension immediately hides the content of the website, inspects the html document, randomly assigns the user to one of the experimental groups and then implements the respective changes to the website before making the page visible. The implementation of the counterfactuals is done by identifying the relevant items to hide or swap by their html class names or ids. We also add custom tags and event listeners to the identified elements that we can fall back on at a later stage. The entire setup of the experiment usually takes around 40 milliseconds. This delay is far below what was found to be noticeable to users [42; 43]. Hiding the html body of the website with the first possible Chrome event is crucial to avoid glitches in case of bad internet connection and make sure the control arrangement is not revealed to the participant. To ensure internal validity of our experiment, we also have to ensure a participant is never reassigned to a new experimental group when reloading a page, navigating between tabs or repeatedly entering the same search query. This is done by storing a hash of user ID and search query together with the assigned experimental group in the browser cache.

Backend and data collection.Every participant is assigned a unique random number that serves as anonymous user ID upon installation of the extension. This user ID persists throughout the experiment. Every time a click event on an element on the search page is registered, the click data is aggregated into a json object and sent to a database server hosted locally at out institution via a post request using the encrypted https protocol. This concerns information about the index of the clicked search result, the click element type, the page index, and the experimental group. In addition, statistics about the website such as the number of search results, the presence of ads and boxes, the number of candidate results, and the position of specialized search results are extracted from the website are recorded. The database server is built using the Microsoft.Net core framework and deployed within a docker container. The database access is rate limited and the Get endpoint of the database is key protected. We use a SQLite database that is mapped to persistent memory.

Privacy considerations.The information that is stored with every click does not contain any personally identifiable information. While we record the position of the clicked element on the search page, we never store search queries or any information about the websites visited by the user. This is an intentional choice to preserve user privacy, and to demonstrate that valuable insights can be gathered without privacy invasive data collection. The experiment went through an internal approval procedure and the privacy policy can be found on our website.3

### Experimental groups

We implement six different counterfactual arrangements, summarized in Table 1, each defining a treatment group. We refer to Figure 2 for the terminology used to refer to individual elements on the general search page. It equivalently applies to both, Google search and Bing. Arrangements \(a_{1}-a_{6}\) are designed to emulate conservative variants of the conduct \(a^{*}\) of interest, to inform a plausible lower bound on performative power. The first three arrangements \(a_{1}\)-\(a_{3}\) concern the reordering of organic search results, leaving the other elements on the website untouched. The arrangements

    & Arrangement & Description \\  \(a_{0}\) & control & Search results are displayed without any modification. \\ \(a_{1}\) & swap 1-2 & The position of the first and the second generic search result are swapped. \\ \(a_{2}\) & swap 1-3 & The position of the first and the third generic search result are swapped. \\ \(a_{3}\) & swap 2-3 & The position of the second and the third generic search result are swapped. \\ \(a_{4}\) & hide Ads/Box & Top Ads and Shopping boxes are hidden. \\ \(a_{5}\) & hide + swap & Combines the latter modification (\(a_{4}\)) with swap 1-2 (\(a_{1}\)). \\ \(a_{6}\) & hide Box & The shopping boxes are hidden. \\   

Table 1: Counterfactual content arrangements implemented by Powermeter as part of the RCT.

Figure 2: Illustration of different elements on the Google search website.

\(a_{4}\) and \(a_{6}\) perform modifications not directly concerning organic search results: Arrangement \(a_{6}\) hides a specific element, called the Shopping box, appearing either in the right side panel or on top of the search results page. Arrangement \(a_{4}\) hides the box together with all the Ads. Finally, Arrangement \(a_{5}\) combines the latter change with a change in search result order. For Bing we only implemented the counterfactual \(a_{1}\) to ensure statistical power despite data scarcity. A practical reason not to implement larger modifications is also users' sensitivity to the resulting deterioration of quality towards ensuring Assumption 1. The Bing experiment of the European Commission's investigation had to be discontinued after one week for that exact reason.4 We made sure to avoid a similar failure point. Based on user feedback collected during an initial test round there was no indication that the modifications were even noticeable to users.

### Onboarding

Participants were provided the link to the project website as an entry point. The website contains information about the experiment, the purpose of the study, an onboarding video, as well as the privacy policy of the extension. The extension itself is distributed through the official Chrome webstore and there is a button directly navigating the user to the item in the store. We did not list the extension publicly to ensure participants are informed about the purpose of the study, and protect the integrity of our data. The installation follows the standard procedure of adding a browser extension to Chrome. The user has to give consent to access Google and Bing websites, as well as to use the storage API. The extension remains active until participants remove it from their web-browser, or until the experiment is stopped. The study participants are trusted individuals of different age groups and backgrounds, recruited by reaching out personally or via email. We provide demographic statistics over our pool of participants in Figure 9 in the appendix.

Data preprocessing.For each participant we ignored the clicks collected during the first four days after onboarding, as suggested by Keusch et al.  in order to avoid potential confounding due to participation awareness. We also removed clicks where the search elements could not be identified reliably for implementing the RCT to avoid selection bias towards the control group.

## 5 Empirical results

Using our Powermeter browser extension we collected click data from \(85\) participants over the course of 5 months, from September 2023 until January 2024. This resulted in \(56,971\) click events, and a total of \(45,625\) clicks after preprocessing. Out of the clicks \(98.9\%\) were registered on Google, and \(1.1\%\) on Bing. Figure 8 in the appendix visualizes some aggregate statistics over the clicks collected. We will consider several subsets of these events for which we measure the performativity gap and algorithmic distortion. In the following we discuss the main insights from the collected data. For all plots, we provide bootstrap confidence bounds over 200 resamples.

### Reordering search results

We first inspect the three counterfactual arrangements \(a_{1}\), \(a_{2}\), \(a_{3}\) concerning reranking. Recall that \(c_{i}\) indexes search results in the order in which they appears under \(a_{0}\). In Figure 3 we visualize the event probabilities \(C=c_{i}\) for each search result \(i=1,2,...,6\) under the control group (blue bars) and compare it to the respective probabilities under the three counterfactuals (orange bars). The figures on the left show the results across Google search queries. Here the counterfactuals correspond to swapping the position of the first two results (left), the first and the third (middle) and the second and the third (right). The right figure shows the results evaluated on Bing search queries when the first two results are swapped. The lower figure visualizes the corresponding performativity gap \(^{i}(a)\), corresponding to the change in clicks to item \(i\) caused by the respective arrangement change.

We observe a consistently large effect of arrangement on clicks. Being down ranked by one position on Google search decreases average click through rate of \(c_{1}\) from \(43\%\) to \(24\%\), resulting in \(^{1}(a_{1})=-0.19\) and an algorithmic distortion of \(=0.44\). Being down-ranked by 2 positions results in \(^{1}(a_{2})=-0.27\) and an average loss of more than \(50\%\) of traffic. Note that a similar effect size has been reported in the case decision for the UK market for a two position shift [45, para 460], indicatingthat the estimate is robust across different user populations. Finally, by down-ranking the second content element by one position we still observe a significant traffic reduction, corresponding to an algorithmic distortion of \(=0.39\).

An interesting observation is also that for every counterfactual arrangement, the element shown first ends up getting most clicks on average. Implying that all the rankings are close to performatively stable  with respect to the non-personalized reranking strategies considered. However, there are several indications of Google's ranking \(a_{0}\) reflecting relevance of search results better then the other arrangements. Namely, \(c_{1}\) gets more clicks when displayed first, compared to other results displayed in the same position (\(c_{2}\) corresponds to first result under \(a_{2}\) or and \(c_{3}\) to the first result under \(a_{3}\)). A second indicator is that \(c_{2}\) benefits from arrangement \(a_{3}\); under the hypothesis that users consider search results in order this indicates that content item \(c_{3}\) absorbs less clicks than \(c_{1}\).

For Bing the position effect seems to be even more pronounced, although confidence intervals are significantly larger in this case. We conjecture that this could be due to the average number of specialized search results and Ads present on the search page being larger on Bing. This results in a larger spacial separation of search results and potentially larger display effects. Statistics about the type of elements present on the the search pages are reported in Figure 8 in the Appendix.

### Indirect effect of visually appealing elements

Next, we consider the counterfactuals \(a_{4}\) and \(a_{6}\) that leave generic search results untouched and hide certain elements on the website. We first inspect the number of clicks these elements absorb if present on the page. We focus on Google search. In Figure 4 we compare the fraction of clicks going to generic search results, Ads, and Boxes for \(a_{0}\), \(a_{4}\), \(a_{6}\). We plot the statistics across the aggregate queries (left panel), the subset of queries where Ads are present on the page under \(a_{0}\) (middle panel) and the subset of queries where the box is present on the page under \(a_{0}\) (right panel). We find the addition of boxes absorbs \(22.4\%\) of the clicks on average across queries where it is present under \(a_{0}\) and these clicks are mostly taken away form the generic search results. Similarly, Ads absorb close to \(30\%\) of the clicks on average for queries where they are present. However, considering the overall number of clicks the effect is smaller since a large fraction of queries contains neither Ads nor Boxes.

Combined conduct.We now consider the combined conduct of adding the box and down-ranking an element. We constrain our focus on queries where the Shopping box is present under \(a_{0}\), either in the center column or in the right sidebar. These are \(3.2\%\) of all the events. We show the corresponding click probabilities for the three first search results in Figure 5. In both figures the blue bars correspond to the control group \(a_{0}\), and the red bars correspond to \(a_{1}\). For these groups boxes are present on the page. In the left figure we investigate the effect of hiding boxes only and the orange bars correspond to arrangement \(a_{4}\). In the right figure we investigate the effect of also hiding Ads, here the orange bars correspond to \(a_{6}\). We find that when adding Boxes, all three content items loose a significant fraction of clicks, whereas Ads mostly take away from \(c_{1}\). In the right figure we additionally show \(a_{5}\) using the hatched bars (i.e., down-ranking the first element by one position if box is hidden). For \(c_{1}\) the combined effect of adding Ads and Boxes on the click through rate is almost as large as the effect

Figure 3: Click through rate and performativity gap for general search results \(c_{1}\) to \(c_{6}\) under the counterfactual arrangements \(a_{1}\), \(a_{2}\), \(a_{3}\) for Google and the counterfactual arrangement \(a_{1}\) for Bing, compared to the control arrangement \(a_{0}\) (in blue).

of down ranking the same item by one position. What we can see consistently is that combining the conduct of adding visually appealing elements on top of the page, and down-ranking a content item, has a combined effect that is larger than the effect of the two individual modifications alone. For element \(c_{1}\) the measured distortion is reported in the table. The combined effect leads to a reduction of \(25\%\) in clicks and an algorithmic distortion of \(66\%\) when considering the effect of Ads/Boxes, and \(53\%\) when only considering Boxes (comparing orange and red bar). We believe this to be the first time that quantitative insights into this combined conduct are made public.

### Factors that impact performative power

We perform additional investigations into what factors have a reinforcing effect on the performativity of ranking. To this end, we inspect different subsets of Google queries and measure the performativity gap as well as algorithmic distortion for \(c_{1}\) under the counterfactual \(a_{1}\). First, we split the data across two different axes depending on whether Ads or boxes are present, and whether Specialized Search results (SSR) are present in between the first two generic search results. The respective comparisons are visualized in the left and middle panel of Figure 6. We observe that the performativity gap in the presence of Ads and boxes is smaller, and about the same whether special search results are present in between search results. However, if we plot distortion we get a different picture, since the base click probability of content item \(c_{1}\) across different splits is different for the three cases. We find that while Ads and Boxes have little effect on distortion, the presence of a specialized search result in between the swapped results tends to increase the effect of down-ranking \(c_{1}\).

For the second investigation we group the queries by the number of candidate search results available on Google. This number was extracted from the website where it appeared as a string on top of the page in the form 'About 323'000'000 results (0.65 second)'. The right panel of Figure 6 shows algorithmic distortion for each percentile of the data. We see a clear trend that the performativity gap increases with the number of candidate results. We suspect this to be connected to the smaller relevance gap across results for queries with more potential results, leading to a higher influence of the arrangement on clicks. However, note that findings in this subsection are no causal claims.

Figure 4: Effect of arrangement on the click distribution across different element types (generic search results, Ads, boxes), visualized for three different subsets of Google queries.

Figure 5: Effect of hiding box and swapping elements on the click probability of generic search results. Statistics are evaluated for the subset of queries for which box is naturally present. The hashed bar shows the click probability under \(a_{5}\) when top content is hidden and the first two elements are swapped. The right table reports the empirical measure of algorithmic distortion for different conducts, extracted form the results in the left figure.

## 6 Discussion

We presented a flexible experimental design, based on a browser extension, to investigate the effect of search algorithms on user clicks. The browser extension performs interventions at the level of display to emulate algorithmic updates, without access to the platform algorithm. We implement different counterfactuals relevant for the Google Shopping antitrust investigation, and provide quantitative insights into the causal effect on clicks. Theorem 1 formally relates our quantitative findings to an instantiation of performative power, measuring the platform's ability to steer _outgoing_ traffic from search.

In a final step, we describe how our findings could fit with a broader anti-trust investigation potentially concerning effects spanning different markets. Take the Google Shopping case as an example. It is concerned with the ability of Google search to distort _incoming_ traffic to a business operating in the market of comparison shopping services by changing where it appears on Google search relative to its competitors. Establishing the causal link between arrangements on search and their effect on incoming traffic to a third party website composes into two steps: a) establish Google's ability to steer outgoing traffic, b) quantify how much of the incoming traffic is mediated by Google search. The first step is a notion of power that experiments like ours operationalize, the second is a number that can readily be obtained from web traffic data. The overall performative power will be the product of the two factors. For putting this together, let's work though the following thought experiment:

_Suppose, \(80\%\) of the referrals to the competitor's website come from Google Search.5 Further, assume that \(70\%\) of the referrals from Google happen while the service is ranked among the top two generic search results. Our estimates suggest that distortion of traffic at the first position can be as large as \(66\%\) for small arrangement changes. Assuming for the second position the effect is \(20\%\) smaller, giving a conservative average effect size of \(=0.8 0.66\). Multiplying the effect size by the fraction of incoming clicks it concerns, we get \(0.8 0.7 30\%\). This is the fraction of traffic to the site Google can redirect._

Turning this number into a conservative lower bound on performative power, it offers an interpretable measure of power for an investigator to judge whether the algorithmic lever of self-preferencing through arrangement changes should be a concern for competition in the down-stream market or not. We can use the same logic to compare search engines, and assess the effectiveness of remedies.

More broadly, we hope our work can serve as a blue print for how performative power can be used to integrate experiments with future digital market investigations, and how tools from computer science, causality, and performative prediction can inform ongoing legal debates related to the power of digital platforms. Our work is situated within a growing scholarship [c.f., 46-48] that takes advantage of the accessibility of digital markets for monitoring and regulating digital platforms. Beyond data, we demonstrate how experimental methods can offer an additional tool for power assessments.

From the perspective of computer science our work offers measurements of performativity in the context of online search, contributing quantitative and empirical support to the study of performativity on digital platforms. As its name suggests a search engine is performative, it acts as an _engine_ steering consumption through its ranking algorithm, rather than a _camera_ merely picturing candidate results--we borrow this analogy from MacKenzie .

Figure 6: Performativity gap and algorithmic distortion for content item \(c_{1}\) under the counterfactual arrangement \(a_{1}\) measure across different subsets of Google search queries.