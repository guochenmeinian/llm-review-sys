# FAMO: Fast Adaptive Multitask Optimization

Bo Liu, Yihao Feng, Peter Stone, Qiang Liu

The University of Texas at Austin, Salesforce AI Research, Sony AI

{bliu, pstone, lqiang}@cs.utexas.edu, yihaof@salesforce.com

###### Abstract

One of the grand enduring goals of AI is to create generalist agents that can learn multiple different tasks from diverse data via multitask learning (MTL). However, in practice, applying gradient descent (GD) on the average loss across all tasks may yield poor multitask performance due to severe under-optimization of certain tasks. Previous approaches that manipulate task gradients for a more balanced loss decrease require storing and computing all task gradients (\((k)\) space and time where \(k\) is the number of tasks), limiting their use in large-scale scenarios. In this work, we introduce Fast Adaptive Multitask Optimization (FAMO), a dynamic weighting method that decreases task losses in a balanced way using \((1)\) space and time. We conduct an extensive set of experiments covering multi-task supervised and reinforcement learning problems. Our results indicate that FAMO achieves comparable or superior performance to state-of-the-art gradient manipulation techniques while offering significant improvements in space and computational efficiency. Code is available at https://github.com/Cranial-XIX/FAMO.

## 1 Introduction

Large models trained on diverse data have advanced both computer vision  and natural language processing , paving the way for generalist agents capable of multitask learning (MTL) . Given the substantial size of these models, it is crucial to design MTL methods that are _effective_ in terms of task performance and _efficient_ in terms of space and time complexities for managing training costs and environmental impacts. This work explores such methods through the lens of optimization.

Perhaps the most intuitive way of solving an MTL problem is to optimize the average loss across all tasks. However, in practice, doing so can lead to models with poor multitask performance: a subset of tasks are _severely under-optimized_. A major reason behind such optimization failure is that a subset of tasks are under-optimized because the average gradient constantly results in small (or even negative) progress on these tasks (see details in Section 2).

To mitigate this problem, gradient manipulation methods  compute a new update vector in place of the gradient to the average loss, such that all task losses decrease in a more balanced way. The new update vector is often determined by solving an additional optimization problem that involves all task gradients. While these approaches exhibit improved performance, they become computationally expensive when the number of tasks and the model size are large . This is because they require computing and storing all task gradients at each iteration, thus demanding \((k)\) space and time complexities, not to mention the overhead introduced by solving the additional optimization problem. In contrast, the average gradient can be efficiently computed in \((1)\) space and time per iteration because one can first average the task losses and then take the gradient of the average loss.1 To this end, we ask the following question:

* _Is it possible to design a multi-task learning optimizer that ensures a balanced reduction in losses across all tasks while utilizing_ \((1)\) _space and time per iteration?_

In this work, we present Fast Adaptive Multitask Optimization (FAMO), a simple yet effective adaptive task weighting method to address the above question. On the one hand, FAMO is designed to ensure that all tasks are optimized with approximately similar progress. On the other hand, FAMO leverages the loss history to update the task weighting, hence bypassing the necessity of computing all task gradients. To summarize, our contributions are:

1. We introduce FAMO, an MTL optimizer that decreases task losses approximately at _equal rates_ while using only \((1)\) space and time per iteration.
2. We demonstrate that FAMO performs comparably to or better than existing gradient manipulation methods on a wide range of standard MTL benchmarks, in terms of standard MTL metrics, while being significantly computationally cheaper.

## 2 Background

In this section, we provide the formal definition of multitask learning, then discuss its optimization challenge, and provide a brief overview of the gradient manipulation methods.

Multitask Learning (MTL)MTL considers optimizing a _single_ model with parameter \(^{m}\) that can perform \(k 2\) tasks well, where each task is associated with a loss function \(_{i}():^{m}_{ 0}\).2 Then, it is common to optimize the average loss across all tasks:

\[_{^{m}}\{_{0}():=_{i=1} ^{k}_{i}()\}.\] (1)

Optimization ChallengeDirectly optimizing (1) can result in severe under-optimization of a subset of tasks. A major reason behind this optimization challenge is the "generalized" conflicting gradient phenomenon, which we explain in the following. At any time step \(t\), assume one updates the model

Figure 1: **Top left:** The loss landscape, and individual task losses of a toy 2-task learning problem (\(\) represents the minimum of task losses). **Top right:** the runtime of different MTL methods for 50000 steps. **Bottom:** the loss trajectories of different MTL methods. Adam fails in 1 out of 5 runs to reach the Pareto front due to CG. FAMO decreases task losses in a balanced way and is the only method matching the \((1)\) space/time complexity of Adam. Experimental details and analysis are provided in Section 5.1.

parameter using a gradient descent style iterative update: \(_{t+1}=_{t}- d_{t}\) where \(\) is the step size and \(d_{t}\) is the update at time \(t\). Then, we say that conflicting gradients (CG) [24; 43] happens if

\[ i,\ \ _{i}(_{t+1})-_{i}(_{t})- _{i}(_{t})^{}d_{t}>0.\]

In other words, certain task's loss is increasing. CG often occurs during optimization and is not inherently detrimental. However, it becomes undesirable when a subset of tasks persistently undergoes under-optimization due to CG. In a more general sense, it is not desirable if a subset of tasks has much slower learning progress compared to the rest of the tasks (even if all task losses are decreasing). This very phenomenon, which we call the "generalized" conflicting gradient, has spurred previous research to mitigate it at each optimization stage .

Gradient Manipulation MethodsGradient manipulation methods aim to decrease all task losses in a more balanced way by finding a new update \(d_{t}\) at each step. \(d_{t}\) is usually a convex combination of task gradients, and therefore the name gradient manipulation (denote \(_{i,t}=_{}_{i}(_{t})\) for short):

\[d_{t}=_{1,t}^{}\\ \\ _{k,t}^{}^{}w_{t},\ \ \ \ \ w_{t}= w_{1,t}\\ \\ w_{k,t}=f_{1,t},,_{k,t} _{k}.\] (2)

Here, \(_{k}=\{w_{ 0}^{k} w^{}=1\}\) is the probabilistic simplex, and \(_{t}\) is the task weighting across all tasks. Please refer to Appendix A for details of five state-of-the-art gradient manipulation methods (MGDA, PCGrad, CAGGrad, IMTL-G, NashMTL) and their corresponding \(f\). Note that existing gradient manipulation methods require computing and storing \(k\) task gradients before applying \(f\) to compute \(d_{t}\), which often involves solving an additional optimization problem. As a result, we say these methods require at least \((k)\) space and time complexity, which makes them slow and memory inefficient when \(k\) and model size \(m\) are large.

## 3 Fast Adaptive Multitask Optimization (FAMO)

In this section, we introduce FAMO that addresses question \(Q\), which involves two main ideas:

1. At each step, decrease all task losses at _an equal rate_ as much as possible (Section 3.1).
2. Amortize the computation in 1. over time (Section 3.2).

### Balanced Rate of Loss Improvement

At time \(t\), assume we perform the update \(_{t+1}=_{t}- d_{t}\), we define the rate of improvement for task \(i\) as

\[r_{i}(,d_{t})=-_{i,t+1}}{_{i,t}}.@note{ footnote}{To avoid division by zero, in practice we add a small constant (e.g., $1e-8$) to all losses. For the ease of notation (e.g., $_{i}()_{i}()+1e-8$, we omit it throughout the paper.}\] (3)

FAMO then seeks an update \(d_{t}\) that results in the largest _worst-case improvement rate_ across all tasks (\(d_{t}\) is subtracted to prevent an under-specified optimization problem where the objective can be infinitely large):

\[_{d_{t}^{m}}_{i\{k\}}r_{i}(,d_{t })-d_{t}^{2}.\] (4)

When the step size \(\) is small, using Taylor approximation, the problem (4) can be approximated by

\[_{d_{t}^{m}}_{i\{K\}}^{}d_{ t}}{_{i,t}}-d_{t}^{2}=_{i,t} ^{}d_{t}-d_{t}^{2}.\] (5)

Instead of solving the primal problem in (5) where \(d^{m}\) (\(m\) can be millions if \(\) is the parameter of a neural network), we consider its dual problem:

**Proposition 3.1**.: _The dual objective of (5) is_

\[z_{t}^{*}*{arg\,min}_{z_{k}}J _{t}z^{2},\ \ \ \ J_{t}=_{1,t}^{}\\ \\ _{k,t}^{},\] (6)

_where \(z_{t}^{*}=z_{t,i}^{*}\) is the optimal combination weights of the gradients, and the optimal update direction is \(d_{t}^{*}=J_{t}z_{t}^{*}\)._

Proof.: \[_{d^{m}}_{i\{k\}}_ {i,t}^{}d-d^{2}\] \[=_{d^{m}}_{z_{k}}_{i =1}^{k}z_{i}_{i,t}^{}d-d^{2}\] \[=_{z_{k}}_{d^{m}}_{i =1}^{k}z_{i}_{i,t}^{}d-d^{2 }\]

Write \(g(d,z)=_{i=1}^{k}z_{i}_{i,t}^{}d- {2}d^{2}\), then by setting

\[=0 d^{*}=_{i=1}^{k} z_{i}_{i,t}.\]

Plugging in \(d^{*}\) back, we have

\[_{d^{m}}_{i\{k\}}_{i,t}^ {}d-d^{2}=_{z_{k}} _{i=1}^{k}z_{i}_{i,t}^{2}=_{z _{k}}J_{t}z^{2}.\]

At the optimum, we have \(d_{t}^{*}=J_{t}z_{t}^{*}\). 

The dual problem in (6) can be viewed as optimizing the log objective of the multiple gradient descent algorithm (MGDA) . Similar to MGDA, (6) only involves a decision variable of dimension \(k m\). Furthermore, if the optimal combination weights \(z_{t}^{*}\) is an interior point of \(_{k}\), then the improvement rates \(r_{i}(,d_{t}^{*})\) of the different tasks \(i\) equal, as we show in the following result.

**Proposition 3.2**.: _Assume \(\{_{i}\}_{i=1}^{k}\) are smooth and the optimal weights \(z_{t}^{*}\) in (6) is an interior point of \(_{k}\), then_

\[\ i j[k], r_{i}^{*}(d_{t}^{*})=r_{j}^{*}(d_{t}^{*}),\]

_where \(r_{i}^{*}(d_{t}^{*})=_{ 0}r_{i}(,d_{t}^{*})\)._

Proof.: Consider the Lagrangian form of (6)

\[(z,,)=_{i=1}^{k}z_{i} _{i,t}^{2}+_{i=1}^{k}z_{i}-1-_{i= 1}^{k}_{i}z_{i},\ \ \  i,_{i} 0.\] (7)

When \(z^{*}\) reaches the optimum, we have \((z,,)/ z=0\), recall that \(d_{t}^{*}=J_{t}z_{t}^{*}\), then

\[J_{t}^{}J_{t}z^{*}=--,\ \ \ \ \ J_{t}= _{1,t}^{}\\ \\ _{k,t}^{}\ \ \ \ \ J_{t}^{}d_{t}^{*}=-(+).\]

When \(z_{t}^{*}\) is an interior point of \(_{k}\), we know that \(=0\). Hence \(J_{t}^{}d_{t}^{*}=-\). This means,

\[ i j,_{ 0}r_{i}(,d_{t}^{*} )=_{i,t}^{}d_{t}^{*}=_{j,t}^{}d_{t}^{*}= _{ 0}r_{j}(,d_{t}^{*}).\]

### Fast Approximation by Amortizing over Time

Instead of fully solving (6) at each optimization step, FAMO performs a single-step gradient descent on \(z\), which amortizes the computation over the optimization trajectory:

\[z_{t+1}=z_{t}-_{z},\ \ =_{z}_{i=1}^{k}z_{i,t} _{i,t}^{2}=J_{t}^{}J_{t}z_{t}.\] (8)

But then, note that

\[_{1,t}-_{1,t+1}\\ \\ _{k,t}-_{k,t+1} J_{t}^{}d_{t}=J_{t}^{ }J_{t}z_{t},\] (9)

so we can use the change in log losses to approximate the gradient.

In practice, to ensure that \(z\) always stays in \(_{k}\), we re-parameterize \(z\) by \(\) and let \(z_{t}=(_{t})\), where \(_{t}^{K}\) are the unconstrained softmax logits. Consequently, we have the following approximate update on \(\) from (8):

\[_{t+1}=_{t}-,\ \ = ^{}z_{1,t}()\\ \\ ^{}z_{k,t}()^{}_{1,t}- _{1,t+1}\\ \\ _{k,t}-_{k,t+1}.\] (10)

**Remark:** While it is possible to perform gradient descent on \(z\) for other gradient manipulation methods in principle, we will demonstrate in Appendix B that not all such updates can be easily approximated using the change in losses.

### Practical Implementation

To facilitate practical implementation, we present two modifications to the update in (10).

Re-normalizationThe suggested update above is a convex combination of the gradients of the log loss, e.g.,

\[d^{*}=_{i=1}^{k}z_{i,t}_{i,t}=_{i=1}^{k}}{_{i,t}}}_{i,t}.\]

When \(_{i,t}\) is small, the multiplicative coefficient \(}{_{i,t}}\) can be quite large and result in unstable optimization. Therefore, we propose to multiply \(d^{*}\) by a constant \(c_{t}\), such that \(c_{t}d^{*}\) can be written as a convex combination of the task gradients just as in other gradient manipulation algorithms (see (2) and we provide the corresponding definition of \(w\) in the following):

\[c_{t}=_{i=1}^{k}}{_{i,t}}^{-1} d_{t}=c_{t}d^{*}=_{i=1}^{k}w_{i}_{i,t},\ \ \ \ w_{i}=c_{t}}{_{i,t}}.\] (11)RegularizationAs we are amortizing the computation over time and the loss objective \(\{_{i}()\}\)s are changing dynamically, it makes sense to focus more on the recent updates of \(\). To this end, we put a decay term on \(w\) such that the resulting \(_{t}\) is an exponential moving average of its gradient updates:

\[_{t+1}=_{t}-(_{t}+_{t})=-_{t}+(1- )_{t-1}+(1-)^{2}_{t-2}+.\] (12)

We provide the complete FAMO algorithm in Algorithm 1 and its pseudocode in Appendix C.

### The Continuous Limit of FAMO

One way to characterize FAMO's behavior is to understand the stationary points of the continuous-time limit of FAMO (i.e. when step sizes \((,)\) shrink to zero). From Algorithm 1, one can derive the following non-autonomous dynamical system (assuming \(\{_{i}\}\) are all smooth):

\[[\\ ]=-c_{t}[J_{t}z_{t}\\ A_{t}J_{t}^{}J_{t}z_{t}+}_{t}],\;\; \;A_{t}=^{}z_{1,t}(_{t})\\ \\ ^{}z_{k,t}(_{t}).\] (13)

(13) reaches its stationary points (or fixed points) when (note that \(c_{t}>0\))

\[[\\ ]=0\;\;\;\;J_{t}z_{t}=0\;\; \;\;_{t}=0\;\;\;\;_{i=1}^{k}_{i,t}=0.\] (14)

Therefore, the minimum points of \(_{i=1}^{k}_{i}()\) are all stationary points of (13).

## 4 Related Work

In this section, we summarize existing methods that tackle learning challenges in multitask learning (MTL). The general idea of most existing works is to encourage positive knowledge transfer by sharing parameters while decreasing any potential negative knowledge transfer (a.k.a, interference) during learning. There are three major ways of doing so: task grouping, designing network architectures specifically for MTL, and designing multitask optimization methods.

Task GroupingTask grouping refers to grouping \(K\) tasks into \(N<K\) clusters and learning \(N\) models for each cluster. The key is estimating the amount of positive knowledge transfer incurred by grouping certain tasks together and then identifying which tasks should be grouped [39; 45; 38; 36; 11].

Multitask ArchitectureNovel neural architectures for MTL include _hard-parameter-sharing_ methods, which decompose a neural network into task-specific modules and a shared feature extractor using manually designed heuristics [21; 29; 2], and _soft-parameter-sharing_ methods, which learn which parameters to share [30; 34; 12; 27]. Recent studies extend neural architecture search for MTL by learning where to branch a network to have task-specific modules [14; 3].

Multitask OptimizationThe most relevant approach to our method is MTL optimization via task balancing. These methods dynamically re-weight all task losses to mitigate the conflicting gradient issue [40; 43]. The simplest form of gradient manipulation is to re-weight the task losses based on manually designed criteria [6; 13; 18], but these methods are often heuristic and lack theoretical support. Gradient manipulation methods [35; 43; 25; 7; 16; 24; 32; 26; 47] propose to form a new update vector at each optimization by linearly combining task gradients. The local improvements across all tasks using the new update can often be explicitly analyzed, making these methods better understood in terms of convergence. However, it has been observed that gradient manipulation methods are often slow in practice, which may outweigh their performance benefits . By contrast, FAMO is designed to match the performance of these methods while remaining efficient in terms of memory and computation. Another recent work proposes to sample random task weights at each optimization step for MTL , which is also computationally efficient. However, we will demonstrate empirically that FAMO performs better than this method.

## 5 Empirical Results

We conduct experiments to answer the following question:

_How does_ FAMO _perform in terms of space/time complexities and standard MTL metrics against prior MTL optimizers on standard benchmarks (e.g., supervised and reinforcement MTL problems)?_

In the following, we first use a toy 2-task problem to demonstrate how FAMO mitigates CG while being efficient. Then we show that FAMO performs comparably or even better than state-of-the-art gradient manipulation methods on standard multitask supervised and reinforcement learning benchmarks. In addition, FAMO requires significantly lower computation time when \(K\) is large compared to other methods. Lastly, we conduct an ablation study on how robust FAMO is to \(\). Each subsection first details the experimental setup and then analyzes the results.

### A Toy 2-Task Example

To better understand the optimization trajectory of FAMO, we adopt the same 2D multitask optimization problem from NashMTL  to visualize how FAMO balances different loss objectives. The model parameter \(=(_{1},_{2})^{2}\). The two tasks' objectives and their surface plots are provided in Appendix D and Figure 2. We compare FAMO against Adam, MGDA , PCGrad, CAGrad, and NashMTL . We then pick 5 initial points \(_{}\{(-8.5,7.5),(-8.5,5),(0,0),(9,9),(10,-8)\}\) and plot the corresponding optimization trajectories with different methods in Figure 1. Note that the toy example is constructed such that naively applying Adam on the average loss can cause the failure of optimization for task 1.

**Findings:** From Figure 1, we observe that FAMO, like all other gradient manipulation methods, mitigates the CG and reaches the Pareto front for all five runs. In the meantime, FAMO performs similarly to NashMTL and achieves a balanced loss decrease even when the two task losses are improperly scaled. Finally, as shown in the top-right of the plot, FAMO behaves similarly to Adam in terms of the training time, which is 25\(\) faster than NashMTL.

### MTL Performance

Multitask Supervised Learning.We consider four supervised benchmarks commonly used in prior MTL research [24; 27; 32; 33]: NYU-v2  (3 tasks), CityScapes  (2 tasks), QM-9  (11 tasks), and CelebA  (40 tasks). Specifically, NYU-v2 is an indoor scene dataset consisting of 1449 RGBD images and dense per-pixel labeling with 13 classes. The learning objectives include image segmentation, depth prediction, and surface normal prediction based on any scene image. CityScapes dataset is similar to NYU-v2 but contains 5000 street-view RGBD images with per-pixel annotations. QM-9 dataset is a widely used benchmark in graph neural network learning. It consists of >130K molecules represented as graphs annotated with node and edge features. We follow the same experimental setting used in NashMTL , where the learning objective is to predict 11 properties of molecules. We use 110K molecules from the QM9 example in PyTorch Geometric , 10K molecules for validation, and the rest of 10K molecules for testing. The characteristic of this dataset is that the 11 properties are at different scales, posing a challenge for task balancing in MTL. Lastly, CelebA dataset contains 200K face images of 10K different celebrities, and each face image is provided with 40 facial binary attributes. Therefore, CelebA can be viewed as a 40-task MTL problem. Different from NYU-v2, CityScapes, and QM-9, the number of tasks (\(K\)) in CelebA is much larger, hence posing a challenge to learning efficiency.

Figure 2: The average loss \(L^{0}\) and the two task losses \(L^{1}\) and \(L^{2}\) for the toy example.

We compare FAMO against 11 MTL optimization methods and a single-task learning baseline: **(1)** Single task learning (STL), training an independent model (\(\) for each task; **(2)** Linear scalarization (LS) baseline that minimizes \(L^{0}\); **(3)** Scale-invariant (SI) baseline that minimizes \(_{k} L^{k}()\), as SI is invariant to any scalar multiplication of task losses; **(4)** Dynamic Weight Average (DWA) , a heuristic for adjusting task weights based on rates of loss changes; **(5)** Uncertainty Weighting (UW)  uses task uncertainty as a proxy to adjust task weights; **(6)** Random Loss Weighting (RLW)  that samples task weighting whose log-probabilities follow the normal distribution; **(7)** MGDA  that finds the equal descent direction for each task; **(8)** PCGrad proposes to project each task gradient to the normal plan of that of other tasks and combining them together in the end; **(9)** CAGrad optimizes the average loss while explicitly controls the minimum decrease across tasks; **(10)** IMTL-G  finds the update direction with equal projections on task gradients; **(11)** GradDrop that randomly dropout certain dimensions of the task gradients based on how much they conflict; **(12)** NashMTL  formulates MTL as a bargaining game and finds the solution to the game that benefits all tasks. For FAMO, we choose the best hyperparameter \(\{0.0001,0.001,0.01\}\) based on the validation loss. Specifically, we choose \(\) equals \(0.01\) for the CityScapes dataset and \(0.001\) for the rest of the datasets. See Appendix E for results with error bars.

Evaluations: We consider two metrics  for MTL: **1) \(\%\)**, the average per-task performance drop of a method \(m\) relative to the STL baseline denoted as \(b\): \(\%=_{k=1}^{K}(-1)^{_{k}}M_{m,k}-M_{b, k}/M_{b,k} 100\), where \(M_{b,k}\) and \(M_{m,k}\) are the STL and \(m\)'s value for metric \(M_{k}\). \(_{k}=1\) (or \(0\)) if the \(M_{k}\) is higher (or lower) the better. **2) Mean Rank (MR)**: the average rank of each method across tasks. For instance, if a method ranks first for every task, **MR** will be 1.

Findings: Results on the four benchmark datasets are provided in Table 1, 2 and 3. We observe that FAMO performs consistently well across different supervised learning MTL benchmarks compared

    &  &  &  \\ 
**Method** & mIoU \(\) & Pix Acc \(\) & Abs Err \(\) & Rel Err \(\) &  & \)} &  & \(\%\) \\   & & & & & Mean & Median & 11.25 & 22.5 & 30 & \\  STL & 38.30 & 63.76 & 0.6754 & 0.2780 & 25.01 & 19.21 & 30.14 & 57.20 & 69.15 & \\  LS & 39.29 & 65.33 & 0.5493 & 0.2263 & 28.15 & 23.96 & 22.09 & 47.50 & 61.08 & 8.89 & 5.59 \\ SI & 38.45 & 64.27 & 0.5354 & 0.2201 & 27.60 & 23.37 & 22.53 & 48.57 & 62.32 & 7.89 & 4.39 \\ RLW & 37.17 & 63.77 & 0.5759 & 0.2410 & 28.27 & 24.18 & 22.26 & 47.05 & 60.62 & 11.22 & 7.78 \\ DWA & 39.11 & 65.31 & 0.5510 & 0.2285 & 27.61 & 23.18 & 24.17 & 50.18 & 62.39 & 7.67 & 3.57 \\ UW & 36.87 & 63.17 & 0.5446 & 0.2260 & 27.04 & 22.61 & 23.54 & 49.05 & 63.65 & 7.44 & 4.05 \\ MGDA & 30.47 & 59.90 & 0.6070 & 0.2555 & **24.88** & **19.45** & **29.18** & **56.88** & **69.36** & 6.00 & 1.38 \\ PCGrad & 38.06 & 64.64 & 0.5550 & 0.2325 & 27.41 & 22.80 & 23.86 & 49.83 & 63.14 & 8.00 & 3.97 \\ GradDrop & 39.39 & 65.12 & 0.5455 & 0.2279 & 27.48 & 22.96 & 23.38 & 49.44 & 62.87 & 7.00 & 3.58 \\ CAGrad & 39.79 & 65.49 & 0.5486 & 0.2250 & 26.31 & 21.58 & 25.61 & 52.36 & 65.58 & 4.56 & 0.20 \\ IMTL-G & 39.35 & 65.00 & 0.5426 & 0.2256 & 26.02 & 21.19 & 26.20 & 53.13 & 66.24 & 3.78 & -0.76 \\ NashMTL & **40.13** & 65.93 & 0.5261 & 0.2171 & 25.26 & 20.08 & 28.40 & 55.47 & 68.15 & 2.11 & -4.04 \\  FAMO & 38.88 & 64.90 & 0.5474 & 0.2194 & 25.06 & 19.57 & 29.21 & 56.61 & 68.98 & 3.44 & **-4.10** \\   

Table 1: Results on NYU-v2 dataset (3 tasks). Each experiment is repeated over 3 random seeds and the mean is reported. The best average result is marked in bold. **MR** and \(\%\) are the main metrics for MTL performance.

  
**Method** & \(\) & \(\) & \(_{}\) & \(_{}\) & \((R^{2})\) & ZPVE & \(U_{0}\) & \(U\) & \(H\) & \(G\) & \(c_{v}\) & **MR \(\)** & \(\%\) \\   & & & & & MAE \(\) & & & & & & & \\  STL & 0.07 & 0.18 & 60.6 & 53.9 & 0.50 & 4.53 & 58.8 & 64.2 & 63.8 & 66.2 & 0.07 & & \\  LS & 0.11 & 0.33 & 73.6 & 89.7 & 5.20 & 14.06 & 143.4 & 144.2 & 144.6 & 140.3 & 0.13 & 6.45 & 177.6 \\ SI & 0.31 & 0.35 & 149to other gradient manipulation methods. In particular, it achieves state-of-the-art results in terms of \(\%\) on the NYU-v2 and QM-9 datasets.

Multitask Reinforcement Learning.We further apply FAMO to multitask reinforcement learning (MTRL) problems as MTRL often suffers more from conflicting gradients due to the stochastic nature of reinforcement learning . Following CAGrad, we apply FAMO on the MetaWorld  MT10 benchmark, which consists of 10 robot manipulation tasks with different reward functions. Following , we use Soft Actor-Critic (SAC)  as the underlying RL algorithm, and compare against baseline methods including LS (SAC with a shared model) , Soft Modularization  (an MTL network that routes different modules in a shared model to form different policies), PCGrad, CAGrad and NashMTL . The experimental setting and hyperparameters all match exactly with those in CAGrad. For NashMTL, we report the results of applying the NashMTL update once per \(\{1,50,100\}\) iterations.4 The results for all methods are provided in Table 4.

  
**Method** & Success \(\) \\  & (mean \(\) stderr) \\  LS (lower bound) & 0.49 \(\)0.07 \\ STL (proxy for upper bound) & 0.90 \(\)0.03 \\  PCGrad & 0.72 \(\)0.02 \\ Soft Modularization & 0.73 \(\)0.04 \\ CAGrad & 0.83 \(\)0.05 \\ NashMTL  (every 1) & 0.91 \(\)0.03 \\ NashMTL  (every 50) & 0.85 \(\)0.02 \\ NashMTL  (every 100) & 0.87 \(\)0.03 \\  NashMTL (ours) (every 1) & 0.80 \(\)0.13 \\ NashMTL (ours) (every 50) & 0.76 \(\)0.10 \\ NashMTL (ours) (every 100) & 0.80 \(\)0.12 \\ UW & 0.77 \(\)0.05 \\  FAMO (ours) & 0.83 \(\)0.05 \\   

Table 4: MTRL results (averaged over 10 runs) on the Metaworld-10 benchmark.

    &  &  \\ 
**Method** &  &  &  & \%\)**} &  & \%\)**} \\   & mIoU \(\) & Pix Acc \(\) & Abs Err \(\) & Rel Err \(\) & & & & \\  STL & 74.01 & 93.16 & 0.0125 & 27.77 & & & & \\  LS & 70.95 & 91.73 & 0.0161 & 33.83 & 6.50 & 14.11 & 4.15 & 6.28 \\ SI & 70.95 & 91.73 & 0.0161 & 33.83 & 9.25 & 14.11 & 7.20 & 7.83 \\ RLW & 74.57 & 93.41 & 0.0158 & 47.79 & 9.25 & 24.38 & 1.46 & 5.22 \\ DWA & 75.24 & 93.52 & 0.0160 & 44.37 & 6.50 & 21.45 & 3.20 & 6.95 \\ UW & 72.02 & 92.85 & 0.0140 & 30.13 & 6.00 & 5.89 & 3.23 & 5.78 \\ MGDA & 68.84 & 91.54 & 0.0309 & 33.50 & 9.75 & 44.14 & 14.85 & 10.93 \\ PCGrad & 75.13 & 93.48 & 0.0154 & 42.07 & 6.75 & 18.29 & 3.17 & 6.65 \\ GradDrop & 75.27 & 93.53 & 0.0157 & 47.54 & 6.00 & 23.73 & 3.29 & 7.80 \\ CAGrad & 75.16 & 93.48 & 0.0141 & 37.60 & 5.75 & 11.64 & 2.48 & 6.20 \\ IMTL-G & 75.33 & 93.49 & 0.0135 & 38.41 & 4.00 & 11.10 & 0.84 & 4.67 \\ NashMTL & 75.41 & 93.66 & 0.0129 & 35.02 & 2.00 & 6.82 & 2.84 & 4.97 \\  FAMO & 74.54 & 93.29 & 0.0145 & 32.59 & 6.25 & 8.13 & 1.21 & 4.72 \\   

Table 3: Results on CityScapes (2 tasks) and CelebA (40 tasks) datasets. Each experiment is repeated over 3 random seeds and the mean is reported. The best average result is marked in bold. **MR** and \(\%\) are the main metrics for MTL performance.

Figure 3: Training Success Rate and Time.

Findings: From Table 5.2, we observe that FAMO performs comparably to CAGrad and outperforms PCGrad and the average gradient descent baselines by a large margin. FAMO also outperforms NashMTL based on our implementation. Moreover, FAMO is significantly faster than NashMTL, even when it is applied once every 100 steps.

### MTL Efficiency (Training Time Comparison)

Figure 4 provides the FAMO's average training time per epoch against that of the baseline methods.

**Findings:** From the figure, we observe that FAMO introduces negligible overhead across all benchmark datasets compared to the LS method, which is, in theory, the lower bound for computation time. In contrast, methods like NashMTL have much longer training time compared to FAMO. More importantly, the computation cost of these methods scales with the number of tasks. In addition, note that these methods also take at least \((K)\) space to store the task gradients, which is implausible for large models in the many-task setting (i.e., when \(m=||\) and \(K\) are large).

### Ablation on \(\)

In this section, we provide the ablation study on the regularization coefficient \(\) in Figure 5.

**Findings:** From Figure 5, we can observe that choosing the right regularization coefficient can be crucial. But except for CityScapes, FAMO performs reasonably well using all different \(\)s. The problem with CityScapes is that one of the task losses is close to \(0\) at the very beginning, hence small changes in task weighting can result in very different loss improvement. Therefore we conjecture that using a larger \(\), in this case, can help stabilize MTL.

## 6 Conclusion and Limitations

In this work, we introduce FAMO, a fast optimization method for multitask learning (MTL) that mitigates the conflicting gradients using \((1)\) space and time. As multitasking large models gain more attention, we believe designing efficient but effective optimizers like FAMO for MTL is crucial. FAMO balances task losses by ensuring each task's loss decreases approximately at an equal rate. Empirically, we observe that FAMO can achieve competitive performance against the state-of-the-art MTL gradient manipulation methods. One limitation of FAMO is its dependency on the regularization parameter \(\), which is introduced due to the stochastic update of the task weighting logits \(\). Future work can investigate a more principled way of determining \(\).

Figure 4: Average training time per epoch for different MTL optimization methods. We report the relative training time of a method to that of the linear scalarization (LS) method (which uses the average gradient).

Figure 5: Ablation over \(\): we plot the performance of FAMO (in terms of \(\%\) using different values of \(\) from \(\{0.0001,0.001,0.01\}\) on the four supervised MTL benchmarks.