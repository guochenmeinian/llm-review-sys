# WildVision: Evaluating Vision-Language Models

in the Wild with Human Preferences

 Yujie Lu\({}^{}\) Dongfu Jiang\({}^{}\)

&Wenhu Chen\({}^{}\) William Yang Wang\({}^{}\) Yejin Choi\({}^{}\) Bill Yuchen Lin\({}^{}\)

\({}^{}\)Allen Institute of AI University of Washington

\({}^{}\)University of California, Santa Barbara \({}^{}\)University of Waterloo

yujielu@ucsb.edu, yuchenl@allenai.org

###### Abstract

Recent breakthroughs in vision-language models (VLMs) emphasize the necessity of benchmarking human preferences in real-world multimodal interactions. To address this gap, we launched WildVision-Arena (WV-Arena), an online platform that collects human preferences to evaluate VLMs. We curated WV-Bench by selecting 500 high-quality samples from 8,000 user submissions in WV-Arena. WV-Bench uses GPT-4 as the judge to compare each VLM with Claude-3-Sonnet, achieving a Spearman correlation of 0.94 with the WV-Arena Elo. This significantly outperforms other benchmarks like MMVet, MMMU, and MMStar. Our comprehensive analysis of 20K real-world interactions reveals important insights into the failure cases of top-performing VLMs. For example, we find that although GPT-4V surpasses many other models like Reka-Flash, Opus, and Yi-VL-Plus in simple visual recognition and reasoning tasks, it still faces challenges with subtle contextual cues, spatial reasoning, visual imagination, and expert domain knowledge. Additionally, current VLMs exhibit issues with hallucinations and safety when intentionally provoked. We are releasing our chat and feedback data to further advance research in the field of VLMs.

Figure 1: WildVision-Arena (WV-Arena) supports multi-round multimodal chats with \(20+\) models, enabling the comparison of VLMs in real-world scenarios. We curate WildVision-Bench (WV-Bench) by selecting 500 samples from \(20k+\) in-the-wild chats and \(8k+\) user ratings. Automatic model scorings on WV-Bench closely correlate with the Elo ratings on WV-Arena.

Introduction

Vision-language models (VLMs) [68; 82; 69; 49; 14; 113; 3; 5] have shown groundbreaking performance across various applications, necessitating enhanced evaluation approaches [87; 24; 107; 106] to keep up with their rapid advancements. Current evaluation benchmarks, however, are constrained by simplicity [53; 102] and practicality [101; 50]. Meanwhile, evaluation metrics for vision and language tasks are predominantly reference-based, focusing on exact matches or model-based scores [87; 7]. The success of the CLIP model  has enabled reference-free evaluation , reducing the need for reference curation while maintaining alignment with human annotators. More recent evaluation methods [56; 107; 35] leverage the instruction-following capability of LLMs and the expertise of vision models [15; 91; 34], making the automatic evaluation of VLMs more fine-grained and interpretable. Despite these advancements, a gap remains between these metrics and human preferences when comparing a large number of models' capabilities in real-world multimodal interactions.

In this paper, we introduce WildVision-Arena and WildVision-Bench to address the need for tracking human preferences regarding models' capabilities in the wild. Our WildVision-Arena is a chatbot-style [110; 12] platform that facilitates easy comparison among VLMs, utilizing the Elo Rating system as the primary ranking metric. With the support of over \(20\) models (GPT-4o , GPT-4V , Gemini-Pro , Gemini-1.5 , Reka , Claude-3 , LLaVA-NEXT , etc), alongside a side-by-side chatting interface over images, we have crowdsourced over \(20,000\) multi-round human-AI chat interactions, including over \(8,000\) votes and fine-grained feedback. We then sample diversified and safe data as our WildVision-Bench and adapt AlpacalEval  to visual context. Specifically, we use the latest released GPT-4o  as a judge model to vote between each VLM and the reference model Claude-3-Sonnet . The statistically estimated model scores on WV-Bench achieve a Spearman's Correlation of \(0.94\) with Elo ratings in WildVision-Arena.

Our comprehensive analysis of these in-the-wild chats identifies areas for improvement in recognizing visual context, spatial reasoning and imagination, and expert domain knowledge. Additionally, lower-performing VLMs struggle with discerning fine visual details in images, hindered by resolution and contextual limitations. Across the board, these models also face challenges with hallucination and safety concerns. Our main contributions can be summarized as:

* We develop WildVision-Arena, an interactive evaluation platform that hosts over 20 VLMs and a live leaderboard reflecting crowdsourced user preferences on real-world chats.
* We curate WildVision-Bench from WildVision-Arena, a fast-evaluation benchmark that closely aligned with human preferences at \(0.94\) Spearman's Correlation.
* We comprehensively analyze \(20,000+\) multimodal conversations and \(8,000+\) votes, and we will release this data to advance future research in VLMs.

## 2 WildVision-Arena: Ranking VLMs with Human Preference

In this section, we introduce WildVision-Arena and present statistics of in-the-wild chat data, along with a deep analysis of human preferences that formulate our online VLMs leaderboard.

### Overview Design of WildVision-Arena

Users conduct multi-round chats over uploaded images, during which two models from the pool or third-party APIs are sampled. Users vote for the better response, with the model's identity revealed afterward, and can provide reasons for their choices. Votes contribute to a live leaderboard, which is updated every few hours to rank the models. Appendix A shows a screenshot of our user interface. In WildVision-Arena, we currently support \(20+\) VLMs as shown in the leaderboard on the right part of Figure 1. The generation hyperparameters are set the same when comparing these models, and users can change the temperature, top-p and max output tokens per their use cases.

### Statistics of Chat Data with Votings

Each chat data point that has human voting is classified into a category-subcategory and domain-subdomain using GPT-4v. The prompt template details are provided in Appendix E.1. Key statistics of user voting in WildVision-Arena are presented in Table 1. The number of tokens is estimated with tiktoken tokenizer corresponding to model 'gpt-3.5-turbo'. Figure 2 and Figure 3 visualize the distribution of these voting data in terms of question categories and image domains, respectively. In addition to the three dominant question categories (Recognition, Descriptive, Analytical), the Interactive, Instructive, and Creative categories are also receiving increasing interest. Users are mostly interested in chat about images tagged with the _Entertainment_ domain (most of which are related to games and movies/TV shows), as well as the Urban, Expert, and People domains.

### Crowdsourced Human Preference on VLMs in the Wild

Pairwise ComparisonWe visualize the heatmap of battle counts and win fractions of seven models out of the 20+ models supported in the WildVision-Arena in Figure 4. The battle count heatmap highlights the frequency of direct comparisons, with models like GPT-4V vs. Gemini-Pro (\(252\) voted battles) being tested more rigorously. GPT-4o consistently outperforms the others by a large margin, winning \(77\%\) of its battles against the second-best model, GPT-4V, which ranks as the second best. Reka-Flash follows closely behind GPT-4V, winning \(42\%\) of its battles, while other models demonstrate lower winning rates. Among the open-source models, LLAVA-NEXT leads, though there remains a significant gap between it and both GPT-4V and GPT-4o.

Expert Agreement with User VotingTo assess the quality of crowdsourced user voting data on our platform, we evaluated inter-annotator agreement by comparing the annotations of our experts

Figure 4: Battle Count Heatmap (Left): the number of voted comparisons between models. Win Fraction Heatmap (Right): the winning rate of Model A over Model B in voted comparisons.

with those from users of the WildVision-Arena. This analysis was conducted on a set of 100 samples. Our findings indicate a substantial level of agreement with the two experts, with an average percentage agreement of \(72.5\%\). Furthermore, the calculated Cohen's Kappa coefficient was \(0.59\), suggesting a moderate to high degree of reliability in the annotations across different annotators.

### Model Ranking with Elo Rating in WildVision-Arena

Following Chatbot Arena , we adapt Elo Rating System  to provide a dynamic evaluation platform for ranking VLMs by statistical modeling based on our collected direct pairwise comparisons. We briefly introduce the Online Elo Rating and the statistical estimation method.

Online Elo RatingElo rating focuses on modeling the probability of player \(i\) winning against player \(j\) given their existing ratings \(R_{i}\) and \(R_{j}\) respectively, where \(i,j N\). We define a binary outcome \(Y_{ij}\) for each comparison between player \(i\) and player \(j\), where \(Y_{ij}=1\) if player \(i\) wins against player \(j\), and \(Y_{ij}=0\) otherwise. Then the logistic probability is formulated as:

\[P(Y_{ij}=1)=-R_{i})/}},\] (1)

where \(=400\) for Elo rating computation. After a match, each player's rating is updated by the formula: \(R^{}_{i}=R_{i}+K(S(i|j)-E(i|j))\), where \(S(i|j)\) is the actual match outcome (1 for a win, 0.5 for a tie, and 0 for a loss), and \(E(i|j)=P(Y_{ij}=1)\). The higher-rated player will win fewer points if they win but lose more if they lose, while the lower-rated player will experience the opposite. The computation of the online Elo rating is correlated with the comparison order. Therefore, we follow Chatbot Arena to adopt the Bradley-Terry model  for a stable statistical estimation.

The probability of player \(i\) winning against player \(j\) given their existing ratings \(R_{i}\) and \(R_{j}\) respectively.

Statistical EstimationThe Bradley-Terry model  estimates the Elo rating using a logistic regression model and maximum likelihood estimation (MLE). Let's say there are \(N\) players, and we have a series of pairwise comparisons, where \(W_{ij}\) is the number of times player \(i\) wins against player \(j\). The log-likelihood function for all pairwise comparisons can be written as:

\[()=_{i,j N,i j}(W_{ij}Y_{ij} P(Y_{ij }=1)),\] (2)

   &  &  &  &  &  &  \\   & & & & & & & & Analy. & Descri. & Recogn. & Enter. & Objects & Expert \\  GPT-40  & – & **1235** & \(434\) & **62.8** & **1290** & **1250** & **1236** & **1362** & **1203** & **1293** \\ GPT-4-Vision  & – & 1132 & \(2288\) & 56.8 & 1154 & 1169 & 1099 & 1177 & 1109 & 1178 \\ Reka-Flash  & – & 1107 & 513 & 56.3 & 1093 & 1141 & 1067 & 1069 & 1101 & 1191 \\ Claude-3-OPUS  & – & 1100 & 908 & 59.4 & 1117 & 1096 & 1092 & 1111 & 1127 & 1128 \\ Gemini-Pro-Vision  & – & 1061 & 2229 & 47.9 & 1099 & 1041 & 1090 & 1088 & 1077 & 1041 \\ Yi-VL-PLUS  & – & 1061 & 283 & – & 1084 & 1040 & 1078 & 1001 & 1119 & 1101 \\ LLaVA-NEXT  & \(34B\) & 1059 & 1826 & 51.1 & 1068 & 1104 & 1021 & 1074 & 1015 & 1052 \\ Gemini-1-5-Flash  & – & 1055 & 132 & – & 1090 & 1018 & 1085 & 1190 & 990 & 1127 \\ Claude-3-Bonnet  & – & 1041 & 496 & 53.1 & 1063 & 1056 & 1041 & 1033 & 1023 & 1119 \\ CogVLM-Chat-HF  & \(13B\) & 1016 & 1024 & 32.1 & 950 & 947 & 1006 & 955 & 930 & 950 \\ Claude-3-Haiku  & – & 1002 & 419 & 50.2 & 964 & 1008 & 996 & 1033 & 1014 & 1005 \\ LLAVA-NEXT  & \(7B\) & 992 & 1367 & 35.1 & 963 & 1032 & 977 & 992 & 1023 & 1001 \\ DeepSeek-VL  & \(7B\) & 979 & 646 & 36.6 & 988 & 984 & 953 & 956 & 1026 & 962 \\ Idefies  & \(8B\) & 965 & 100 & 36.6 & 818 & 1003 & 1011 & 909 & 1071 & 1020 \\ LLAVA-NEXT  & \(13B\) & 956 & 201 & 35.9 & 965 & 974 & 1006 & 975 & 971 & 987 \\ Qwen-VL-Chat  & \(10B\) & 930 & 1328 & 35.9 & 898 & 937 & 940 & 923 & 942 & 902 \\ Bunny-V1  & \(3B\) & 921 & 389 & 38.2 & 897 & 922 & 878 & 884 & 823 & 823 \\ MiniCPM-V  & \(3B\) & 910 & 1349 & 34.7 & 895 & 911 & 925 & 888 & 890 & 840 \\ LLaVA-V1-5  & \(13B\) & 891 & 299 & 36.4 & 952 & 838 & 920 & 887 & 827 & 914 \\ Tiny-LLaVA-v1-HF  & \(3B\) & 879 & 288 & 33.1 & 901 & 828 & 821 & 808 & 853 & 894 \\ InstrueBLIP  & \(7B\) & 862 & 807 & 30.6 & 834 & 856 & 891 & 840 & 902 & 763 \\ UFORM-Gen2-Qwen  & \(500M\) & 827 & 452 & – & 911 & 785 & 853 & 768 & 937 & 830 \\  

Table 2: WildVision-Arena Leaderboard. We show the full elo score and within three question categories (Analytical, Descriptive, Recognition) and three image domains (Entertainment, Objects, Expert) of 22 models with a time cutoff at May 29, 2024. **Best** Second Best Best among proprietary models Best among open-source models.

where \(=\{R1,...,R_{N}\}\) is the Elo rating variable of each player. Since this modeling does not consider ties, in practice, we duplicate all the votes and force half of the tie votes to be counted as left model \(i\) winning (\(Y_{ij}=1\)) and the other half as right model \(j\) winning (\(Y_{ij}=0\)).

### WildVision-Arena Leaderboard

We report the leaderboard results in Table 2, including the full Elo ratings and the total number of battles for each model, with a time cutoff on May 29, 2024. Additionally, we provide the Elo ratings for three main question categories (Analytical, Descriptive, Recognition) and three main image domains (Entertainment, Natural, Expert) to better understand the specialties of each model. GPT-4o quickly dominates the leaderboard after its release, surpassing the previous state-of-the-art GPT-4V by a significant margin, followed by Reka-Flash, Claude-3-OPUS. Yi-VL-PLUS and LLaVA-NEXT-34B achieve the same rank, reflecting that both models are based on the Yi . Among open-source models, LLaVA-NEXT-34B ranks first, even surpassing Gemini-1.5-Flash and Claude-3-Sonnet, Claude-3-Haiku, indicating a strong baseline for research purposes. To compare models under each question category and image domain, we present the top six models ranked in the WildVision-Arena leaderboard in terms of Elo ratings for each question category and image domain in Figure 5. GPT-4o consistently outperforms all other models except for the images tagged with Natural, where varying specialties are more commonly observed among the other models.

## 3 WildVision-Bench: In-the-Wild Testbed for VLMs

Recent VLMs reveal a closing gap with GPT-4V on various benchmarks, but this improvement is not always reflected in users' daily experiences. This discrepancy arises from current models' limited generalizability compared to proprietary ones, which fixed benchmarks fail to capture. To address this, we propose creating WildVision-Bench, a challenging and natural benchmark for VLMs that reflects real-world human use cases, with models' rankings aligning closely with the WildVision-Arena leaderboard contributed by diverse crowdsourced user votes.

Figure 5: Elo ratings of six models across question categories (Top) and image domains (Bottom).

### Data Curation Pipeline

Starting with in-the-wild multimodal conversation data from WildVision-Arena's users, we apply the NSFW detector  on the images to filter out unsafe content. We then perform deduplication on the images and apply diversity sampling to formulate a public set of 500 data samples for WildVision-Bench. Our experts manually annotate 50 samples as a preview of a hidden set, which will be updated dynamically to avoid contamination. We showcase the model performance on two cases from expert annotations in Table 3.

### Automatic Evaluation on WildVision-Bench

VLMs as a Local EvaluatorPrevious work [107; 35] shows alignment between GPT-4V and humans when evaluating the performance of VLMs. We further validate the agreement of GPT-4V with crowdsourced human preferences in WildVision-Arena to ensure its efficacy in the wild. Specifically, we feed a pair of multimodal conversations along with the votes into GPT-4V to select among four choices: 1) left/right vote: the left/right model response is better, 2) tie/bad vote: both models are equally good/bad. In Appendix E.3, we provide the detailed prompt template for GPT-4V. We show the GPT-4V vs Arena Human alignment in Figure 6. We observe that GPT-4V has relatively low agreement with humans on tie votes but shows high agreement with humans when both models

    &  \\   & **4-way** & **3-way** & **Binary** \\  F1 Score (Macro) & 0.4245 & 0.5143 & 0.7792 \\ F1 Score (Micro) & 0.5747 & 0.5842 & 0.7796 \\ F1 Score (Weighted) & 0.5407 & 0.5536 & 0.7798 \\ Cohen’s Kappa Score & 0.3404 & 0.3442 & 0.5585 \\ Pearson Correlation & 0.2906 & 0.2880 & 0.5587 \\   

Table 3: VLMs’ responses on two cases from WildVision-Bench expert annotated samples. The example \(\#61\) is a hard case that all models fall short at.

Figure 6: Left: GPT-4V vs. Arena Human Voting. Right: Agreement; 4-way: left/right/tie/bad vote. 3-way: left/right/other. Binary: left/right voteexhibit distinguishable differences. However, predicting when both models are bad is challenging as GPT-4V sometimes falls short in these examples as well.

WildVision-Bench Alignment with Human Preferences in WildVision-ArenaInspired by Alpaca Eval , we adopt a similar approach to rank VLMs on our WildVision-Bench automatically. Specifically, we use GPT-4o as the judgment model and Claude-3-Sonnet as our reference model. We compare each model's answers on the WildVision-Bench public set with Claude-3-Sonnet and then use GPT-4o, which shows better alignment with humans in our cases, to give a vote. The template in Table E.3 is used for the prompt of the judge, where 5 levels of comparison results are defined, which are "Better+", "Better", "Tie", "Worse", and "Worse+" respectively. We report the score results of these models in Table 4. This achieves a \(0.94\) Spearman correlation with the WildVision-Arena leaderboard.

Benchmark Correlation HeatmapWe visualize the Spearman correlation heatmap among various multimodal benchmarks in Figure 7. The MMBench-series  (CCBench, MMBench EN, MMBench CN) considers fine-grained perception and reasoning tasks in multiple choice questions. MMVet  evaluates integrated capabilities in visual question answering. MMStar  alleviates misjudgment issues with high-quality multiple choice questions. HallucionBench  focus on investigating hallucination issues, while MMMU  and MathVista  focus on college-level subject knowledge and mathematical reasoning in visual contexts, respectively. WildVision Elo represents the arena leaderboard, reflecting human preferences using Elo ratings from pairwise comparisons. WildVision Bench represents ranking model using estimated model score on our WildVision-Bench. This achieves the highest correlation with WildVision Elo, indicating its crucial role in simulating human preferences on these VLMs in the real world. The runner-up in alignment with human preferences is MMVet, followed by MMMU and MMStar.

  
**Model** & **Score** & **95\% CI** & **Win Rate** & **Reward** & **Much Better** & **Better** & **Tie** & **Worse** & **Much Worse** & **Avg Tokens** \\  GPT-4o  & \(89.41\) & \((-1.7,2.0)\) & \(80.6\) & \(56.4\) & \(255.0\) & \(148.0\) & \(14.0\) & \(72.0\) & \(11.0\) & \(157\) \\ GPT-4Vision  & \(80.01\) & \((-19.28)\) & \(71.8\) & \(39.4\) & \(18.2\) & \(177.0\) & \(22.0\) & \(91.0\) & \(28.0\) & \(140\) \\ Relax-Flash  & \(64.79\) & \((-2.9,3.0)\) & \(58.8\) & \(18.9\) & \(135.0\) & \(15.0\) & \(15.0\) & \(28.0\) & \(116.0\) & \(62.0\) & \(181\) \\ Claude-3-Opus  & \(26.25\) & \((-2.8,3.4)\) & \(53.0\) & \(13.5\) & \(103.0\) & \(162.0\) & \(48.0\) & \(141.0\) & \(64.0\) & \(120\) \\ Yi-VL-PLUS  & \(55.09\) & \((-2.9,3.0)\) & \(52.8\) & \(7.2\) & \(98.0\) & \(160.0\) & \(29.0\) & \(124.0\) & \(83.0\) & \(150\) \\ LiLAv-NEXT-348  & \(51.91\) & \((-31.2,4)\) & \(49.2\) & \(2.5\) & \(90.0\) & \(156.0\) & \(26.0\) & \(145.0\) & \(83.0\) & \(165\) \\ Claude-3-Sonnet  & \(50.00\) & \((-2.9,3.0)\) & \((-2.9,3.0)\) & \((-2.9,3.0)\) & \((-2.9,3.0)\) & \((-2.9,3.0)\) & \((-2.9,3.0)\) & \((-2.9,3.0)\) & \((-2.9,3.0)\) & \((-2.9,3.0)\) & \((-2.9,3.0)\) \\ Claude-3-Biasu  & \(37.07\) & \((-32.4,2.2)\) & \(30.6\) & \(-12.6\) & \(5.0\) & \(99.0\) & \(47.0\) & \(-228.0\) & \(72.0\) & \(97\) \\ Gemini-P-vision  & \(35.45\) & \((-2.6,3.2)\) & \(32.6\) & \(-21.0\) & \(80.0\) & \(83.0\) & \(27.0\) & \(167.0\) & \(143.0\) & \(66\) \\ LLAv-NEXT-138  & \(33.69\) & \((-3.8,2.7)\) & \(33.85\) & \(-21.4\) & \(62.0\) & \(107.0\) & \(25.0\) & \(167.0\) & \(139.0\) & \(138\) \\ DeepSF-VL-7B  & \(33.48\) & \((-2.2,3.0)\) & \(35.6\) & \(-21.2\) & \(59.0\) & \(119.0\) & \(17.0\) & \(161.0\) & \(144.0\) & \(119\) \\ CoSyT-ML-49N  & \(38.48\) & \((-2.7,2.4)\) & \(30.05\) & \(-26.4\) & \(75.0\) & \(78.0\) & \(15.0\) & \(172.0\) & \(160.0\) & \(63\) \\ LiLAv-NEXT-7B  & \(26.15\) & \((-2.7,2.3)\) & \(27.05\) & \(-31.4\) & \(45.0\) & \(90.0\) & \(36.0\) & \(164.0\) & \(165.0\) & \(139\) \\ Hefe(2-37)  & \(27.1\) & \((-2.4,2.5)\) & \(26.4\) & \(-35.8\) & \(44.0\) & \(88.0\) & \(19.0\) & \(164.0\) & \(185.0\) & \(128\) \\ Owen-VL-Chat  & \(17.87\) & \((-2.6,2.2)\) & \(19.6\) & \(45.7\) & \(47.9\) & \(42.0\) & \(56.0\) & \(15.0\) & \(155.0\) & \(232.0\) & \(70\) \\ LiLAv-NEXT-5138  & \(14.15\) & \((-2.2,2.2)\) & \(16.8\) & \(52.5\) & \(28.0\) & \(56.0\) & \(19.0\) & \(157.0\) & \(240.0\) & \(87\) \\ Bumy-38  & \(12.70\) & \((-1.8,1.9)\) & \(16.6\) & \(-54.4\) & \(23.0\) & \(60.0\) & \(10.0\) & \(164.0\) & \(243.0\) & \(76\) \\ MinTM-CV  & \(11.66\) & \((-1.8,1.2)\) & \(13.6\) & \(57.5\) & \(25.0\) & \(43.0\) & \(16.0\) & \(164.0\) & \(252.0\) & \(89\) \\ Tiny-LLVA  & \(8.01\) & \((-1.4,1.4)\) & \(11.0\) & \(<66.2\) & \(16.0\) & \(39.0\) & \(15.0\) & \(127.0\) & \(303.0\) & \(74\) \\ UFORM-Gen2-Owen  & \(7.55\) & \((-1.6,1.1)\) & \(10.8\) & \(<68.5\) & \(16.0\) & \(38.0\) & \(11.0\) & \(115.0\) & \(320.0\) & \(92\) \\ Intruc(BLP-TB)  & \(5.54\) & \((-1.3,1.5)\) & \(7.8\) & \(7\) & \(-72.5\) & \(11.0\) & \(28.0\) & \(15.0\) & \(117.0\) & \(329.0\) & \(47\) \\   

Table 4: Estimated model scores of VLMs on WildVision-Bench

Figure 7: WildVision-Bench achieves the highest correlation with WildVision-Arena, with a Spearman’s correlation of 0.94.

## 4 Analysis

In-the-wild Multimodal ChatIn contrast to public benchmark, in-the-wild multimodal conversations involve images and instructions from a diverse range of sources and receive vote data from a varied group of users. This better helps us understand how current VLMs can benefit real-world scenarios and reveal improvement directions for researchers in the field. In Appendix B, we present more cases under each image domain and question category. We will release both multimodal chat and crowdsourced voting data for future research.

Failure CasesIn Table 5, we present two distinct failure instances that are documented in the WildVision-Arena platform. This analysis reveals that GPT-4V's limitations primarily stem from insufficient background knowledge, whereas Gemini-Pro-Vision often fails to discern and process subtle details crucial for deriving correct answers. Additional details on these failure cases are provided in Appendix Our categorization of common failures includes six types: Visual Recognition, Visual Reasoning, Spatial Imagination, Contextual Understanding, Expert Domain Knowledge, Hallucination, and Safety. Although not all failure cases can be included in this paper, we plan to periodically release additional cases on our live platform to aid ongoing research and development.

Model Comparison on WildVision-BenchTable 3 compares the responses of GPT-4V, LLaVA-NEXT-34B, and Gemini-Pro-Vision on a validation sample from WildVision-Bench. GPT-4V generally outperforms the other models, confirming expectations of its superior capabilities. Nevertheless, all models occasionally fail to deliver correct responses, notably in scenarios requiring compositional reasoning, regardless of the simplicity of the text or the image involved. We also observe that recognizing and interpreting subtle visual details within images is still challenging for less capable models.

Broader ImpactFor the first version of data release, we plan to release over 20,000 crowdsourced multi-turn conversation data and more than 8,000 human votings with reasons, providing a valuable resource for understanding human preferences in VLMs interactions and developing models that align more closely with human standards in real-world scenarios. We will also present a live leaderboard together with useful failure case analysis to keep track of recent advancements in this field. Additionally, by open-sourcing the WildVision-Arena code, we enable researchers and developers to adapt our methods to other domains. We will also support fast evaluation of our WildVision-Bench for quick and human-aligned evaluation, which aligns with the human preferences in VLMs in real-world scenarios.

 p{142.3pt}}  
**GPT-4V:** The image shows a screenshot of Windows 3x, which is a series of graphical operating environments produced by Microsoft for use on personal computers, prior to the Windows 95 operating system... \\   

Table 5: Failure cases of GPT-4V and Gemini-Pro-Vision sampled from WildVision-Arena.

Modality, Resolution, Long Context, Resource-EfficientMany work have extended vision-language models (VLMs) beyond image-text modalities, including video [105; 57; 109], audio , and even applied to embodied agent . Future work may consider improving all-in-one models [63; 92; 82; 112; 19] by discovering better methods to integrate these modality data. Recent works have enabled high-resolution [48; 96] and text reading [108; 25] capabilities in VLMs, although many failure cases are still induced by low resolution or poor OCR capability. Other work advances multi-image and long-context capabilities in VLMs [61; 37; 29; 79; 54]. We expect future research to discover the best mechanisms for balancing compact and effective approaches to convey multimodal information, such as recent progress of text representation in pixel space [75; 18; 55]. This is essential to closing the gap between open-source multimodal agents [99; 104] and proprietary ones [97; 69]. Although many works [26; 111] have made VLMs more compact, their performance is still not satisfying. Future work may further improve the performance of smaller models with less training data and higher throughput inference.

World Knowledge and Safety in VLMsThe challenge of embedding extensive world knowledge within VLMs is significant, particularly given their current limitations in understanding physical principles and interacting with real-world environments. These models' ability to dynamically expand their knowledge base through activities like browsing the internet, reading books, or watching videos is an exciting potential advancement. Key concerns in LLMs include security [94; 64; 90; 98], privacy [31; 38], and the propagation of truthfulness [30; 77; 45] and prevention of misinformation [80; 72; 103]. For VLMs, they face unique safety challenges: 1) incorrect alignment of multimodal data can lead to harmful outputs, 2) images may contain sensitive information, necessitating careful handling, and 3) VLMs are vulnerable to attacks manipulating both text and images.

## 5 Related Work

Live Benchmarking for vision-language modelsVision-and-language pre-training starts from models [42; 43] adapting objectives in BERT , to models  adopting contrastive learning, and to unified frameworks [52; 88; 41; 40] without task-specific head. With recent advancements of Large Language Models [67; 20; 4; 84; 85], their multi-modal counterparts [68; 82; 14; 49; 47; 5; 28; 37] are dominating vision and language tasks. Beyond previous task-specific caption [11; 78], visual question answer [62; 59; 27; 21; 60], grounding [46; 100; 66; 58; 71], more benchmarks [101; 50; 39; 32] are proposed to capture VLMs capabilities. When building such benchmarks, there is an urge need to consider alleviating data contamination [76; 6] during eval, assuring robustness  and difficulty , and incorporating real-world scenarios [8; 93]. We build WildVision-Arena to support diversified, difficult, in-the-wild, live benchmarking [12; 95] of VLMs.

Human-Aligned Evaluation for vision-language modelsEvaluation for open-ended vision and language tasks [8; 93; 70] are usually challenging, and recent techniques improve human alignment by mapping free-form predictions to pre-defined choices , using larger models as the evaluator [56; 107]. In the domain of evaluating LLMs, a certain approaches [110; 16] prove their effectiveness in aligning with real-world annotators on the Chatbot Arena . This inspires our efforts in curating in-the-wild small-scale WildVision-Bench, that can support fast evaluation by pair-wise comparison with reference model (such as Claude-3-Sonnet ), and achieve alignment with crowdsourced human ratrors on WildVision-Arena.

## 6 Conclusion

We first introduce WildVision-Arena, a dynamic evaluation platform for comparing vision-language models (VLMs) in the wild. We conduct comparative insights across over 20 models by utilizing an extensive dataset of 20,000+ multimodal conversations and 8,000+ votes, allowing for continuous refinement of VLMs performance. From these in-the-wild chats, we then sample safe and diversified data for WildVision-Bench and apply automatic evaluation that closely aligns with crowdsourced human preferences from WildVision-Arena. Our comprehensive analysis on these in-the-wild chats indicates future directions for advancing VLMs.