# CLIPLoss and Norm-Based Data Selection Methods

for Multimodal Contrastive Learning

 Yiping Wang

University of Washington

&Yifang Chen

University of Washington

&Wendan Yan

University of Washington

&Alex Fang

University of Washington

&Wenjing Zhou

University of Michigan

&Kevin Jamieson

University of Washington

&Simon Shaolei Du

University of Washington

Equal contribution. Correspondence to ypwang61@cs.washington.edu. Codes are available at https://github.com/ypwang61/negCLIPLoss_NormSim.

###### Abstract

Data selection has emerged as a core issue for large-scale visual-language model pretraining (e.g., CLIP), particularly with noisy web-curated datasets. Three main data selection approaches are: (1) leveraging external non-CLIP models to aid data selection, (2) training new CLIP-style embedding models that are more effective at selecting high-quality data than the original OpenAI CLIP model, and (3) designing better metrics or strategies universally applicable to any CLIP embedding without requiring specific model properties (e.g., CLIPScore is one popular metric). While the first two approaches have been extensively studied, the third remains under-explored. In this paper, we advance the third approach by proposing two new methods. Firstly, instead of classical CLIP scores that only consider the alignment between two modalities from a single sample, we introduce **negCLIPLoss**, a method inspired by CLIP training loss that adds the alignment between one sample and its contrastive pairs as an extra normalization term to CLIPScore for better quality measurement. Secondly, when downstream tasks are known, we propose a new norm-based metric, **NormSim**, to measure the similarity between pretraining data and target data. We test our methods on the data selection benchmark, DataComp . Compared to the best baseline using only OpenAI's CLIP-L/14, our methods achieve a 5.3% improvement on ImageNet-1k and a 2.8% improvement on 38 downstream evaluation tasks. Moreover, both **negCLIPLoss** and **NormSim** are compatible with existing techniques. By combining our methods with the current best methods DFN  and HYPE , we can boost average performance on downstream tasks by 0.9%, achieving a new state-of-the-art on the DataComp-medium benchmark2.

## 1 Introduction

Curating large-scale visual-language datasets from web-sourced data has become common for pretraining multi-modal models. However, the quality of these web-curated data pairs remains a critical bottleneck. Research has shown that the choice of dataset significantly impacts model performance, irrespective of the models and training techniques employed , and this motivatesthe development of various data selection strategies. This paper focuses on optimizing subset selection from a fixed data pool to train a CLIP model  that achieves superior performance on zero-shot downstream tasks.

Classical methods _rely solely on OpenAI's (OAI) pretrained CLIP model_ (i.e., a teacher model) and focus on better utilizing the embeddings. The most commonly used one is calculating CLIPScore, which measures the cosine similarity between the visual and language embeddings of the CLIP model for the same sample, to eliminate low-quality data with mismatches between text and image. Other works also leverage heuristic distribution alignment techniques to select samples relevant to downstream tasks, such as image-based filtering . These approaches are generally viewed as providing only limited enhancements. However, we argue that the potential of those embeddings has been heavily under-explored. This work seeks a universal method to better employ any given embeddings, not only from OAI CLIP, but also from other CLIP-style models.

On the other hand, recent leading data filtering methods, instead of focusing on improving embedding utilization stagey itself, mainly follow the other two directions, both employing external resources. They either (1) use _external non-CLIP models_ that aid in data selection, (2) or use _external high-quality multi-modal data_ to train a _better CLIP-style embedding model_ than the original OAI CLIP to filter out low-quality data. Specifically, in the first line of works, HYPE  leverages embeddings from hyperbolic models instead of the classical Euclidean-based CLIP to measure how each data point has semantically overlaps with other data points and filters out data with low specificity. T-MARS  removes images where the text is the only feature correlated with the caption using FAST , an off-the-shelf OCR text detection model. Devil  applies fasttext  to remove non-English texts and use BLIP-2  model for digit recognition to keep useful images with digits. The second direction, represented by Data Filtering Network (DFN) , involves training a new CLIP-style teacher model that uses high-quality datasets like HQITP-350M. Although the embeddings extracted from this model perform worse than the OAI CLIP in downstream tasks, it is particularly good at filtering out low-quality data. Notably, some of these methods can be combined and indeed, merging the selected data from DFN and HYPE achieves current state-of-art as shown in HYPE .

Previous works mainly focus on improving the CLIP embedding quality or utilizing an external model to do filtering but employ the CLIP embedding in a suboptimal way by only using classical methods like CLIPScore. In contrast, in this work, we focus on improving the filtering methods themselves for any given CLIP embedding. We show that there are universal and more effective strategies for utilizing any CLIP teacher model, regardless of its architecture (e.g., B/32 or L/14) or the dataset it was trained on (e.g., OpenAI-WIT-400M or DFN's high-quality dataset). These strategies should always be orthogonal to the use of any newly trained CLIP-style models like DFN and might also be compatible with methods using external models like FAST and BLIP-2.

**Our Contributions.** We propose an alternative to CLIPScores that we call **negCLIPLoss** that more accurately characterizes data quality. We also introduce a new distribution metric we call the p-Norm Similarity Score (**NormSim**) when knowledge about downstream tasks is available. Two major observations directly inform our proposals:

* [leftmargin=*,noitemsep,topsep=0pt]
* Firstly, we observe that classical methods measure the quality of a multi-modal sample by computing the cosine similarity between its visual and language embeddings, believing that lower similarity indicates that the text does not match its image part well. However, we find that some less informative samples may have a systematic bias, which leads to higher CLIPScores. For example, the language part containing the word "image" can result in higher similarity with any visual part, even when the text does not accurately describe its image content. Our proposed method **negCLIPLoss**, inspired by the standard CLIPLoss, normalizes the original CLIPScore by the similarity between a sample and its contrastive pairs. For example, the high score caused by the word "image" is typically consistent across its contrastive pairs, so our adjustment reduces this bias. As we have highlighted, such replacement can be universally applied across different embedding models. See Fig. 2 for illustrations.
* Secondly, if one has access to examples drawn from the same distribution as the target task, it is natural to assume that this extra knowledge could be leveraged to inform the data filtering process. We propose the **NormSim** metric to measure the vision similarity between a training sample \(x\) and the target task dataset \(X^{v}_{}^{n D}\) defined as \(\|f_{v}(X^{v}_{})f_{v}(x^{v})\|_{p}\), where \(f_{v}:^{D}^{d}\) is the vision encoder of teacher model so that \(f_{v}(X^{v}_{})^{n d}\), \(f_{v}(x^{v})^{d}\), and \(f_{v}(X^{v}_{})f_{v}(x^{v})^{n}\), and \(\|\|_{p}\) is the \(p\) norm; effective choices are \(p=2\) or \(\). Notably, unlike previous ImageNet-based filtering , which tries to keep the training set as diverse as downstream tasks by clustering the training set and finding the nearest neighbor group for _every target sample_, our method does not explicitly consider the diversity but select examples as long as it is close to _any target sample_ (i.e. select high NormSim score). Notably, **negCLIPLoss** and **NormSim** enjoy complementary effect in data selection. See Fig. 3.

To illustrate the effectiveness of our methods, we use a widely used benchmark DataComp  as our primary method of evaluating the datasets created by our data filtering methods. We show that, by simply replacing the CLIPScores with **negCLIPLoss** and utilizing **NormSim** we are able to exceed the best OAI-CLIP(L/14)-based baseline by 5.3% on ImageNet-1k and 2.8% on average across 38 downstream tasks, which is similar or even better than the performance achieved by many external-resources-based methods. Notably, even if the target downstream tasks are not available, using NormSim on a proxy downstream task constructed from the training set, called **NormSim\({}_{2}\)-D**, combined with negCLIPLoss, can also gain a 1.9% improvement on 38 downstream evaluation.

Moreover, the improvements achieved by our methods are not limited to OAI CLIP-based methods but can also be obtained by combining our methods with advanced models that require external resources. _By merging the subset selected by **negCLIPLoss** and **NormSim** with the subset selected by current state-of-the-art method "HYPE \(\) DFN", we can further improve it by 0.9% on both ImageNet-1k and on average 38 downstream tasks. Besides, we can also achieve a 0.8% improvement on average 38 tasks over "HYPE \(\) DFN" using only the data selected by DFN and our strategies._ More importantly, we demonstrate that negCLIPLoss, as a replacement for CLIPScore, can be applied to any other embedding models like OAI-L/14, OAI-B/32, and DFN-B/32, universally boosting performance from 0.4% to 3.0% on an average of 38 tasks. This result is not only technically insightful for understanding the information available in embeddings but also practically significant. Compared to existing methods, our approach saves a significant amount of computational time on both reprocessing and new embedding retraining as shown in Table 5.

## 2 Problem Setup

**Data Filtering on Multimodal Dataset.** We are given a training dataset \(D_{}=\{x^{v},x^{l}\}\), where \((x^{v},x^{l})^{D}\) is the image-text (vision-language) training pair. For convenience, we will let superscript \(vl\) denote either modality so that, for example, \(x^{vl} x^{v},x^{l}\). Our goal is to identify a subset \(S D_{}\) that maximizes the zero-shot accuracy of the CLIP model on some downstream tasks when \(S\) is used to train the CLIP model.

**CLIP score and embedding.** Recent efforts, such as LAION  and DataComp , use OpenAI's CLIP ViT-L/14 model  as a teacher model to obtain quality score. Here we denote this vanilla CLIP model as \(_{vl}\). For any pair \(x^{vl}\), the model outputs a normalized unit-vector \(_{vl}(x^{vl})\). If \(X^{vl}:=\{x^{vl}_{1},,x^{vl}_{m}\}\) denotes a dataset containing \(m\) samples, then we define \(_{vl}(X^{vl})=[_{vl}(x^{vl}_{1}),,_{vl}(x^{vl}_{m}) ]^{}^{m d}\) as the embedding matrix. The popular filtering metric "CLIPScore" is defined as \(_{v}(x^{v}),_{l}(x^{l})[-1,1]\).

**Dataset and model.** Here we follow the pipeline of Datacomp  to standardize the training and evaluation process. This is a testbed for dataset experiments aiming to open-source and further improve the vanilla CLIP model and is widely adopted in previous data selection papers [17; 18; 12; 2; 19; 7]. We will give more details in Sec. 4.

## 3 Data Filtering Strategy

### negCLIPLoss: A Better Metric than CLIPScore

In this section, we introduce a better and statistically interpretable quality metric called negCLIPLoss, which directly replaces the common metric CLIPScore. Fig. 1 illustrates how negCLIPLoss works. This new metric only requires negligible extra computational costs and no additional external data collection costs. As the name suggested, this metric is inspired by the standard CLIP loss used in the actual training process of the teacher CLIP model, which is defined as

\[_{B^{*}}(x^{vl}_{i})=-[(_{v}(x ^{v}_{i})^{}_{l}(x^{l}_{i})/)}{_{j B^{*}}( {f}_{v}(x^{v}_{i})^{}_{l}(x^{l}_{j})/)}+( _{v}(x^{v}_{i})^{}_{l}(x^{l}_{i}))/}{_{j B^{*}} (_{v}(x^{v}_{j})^{}_{l}(x^{l}_{i})/)}]\] (1)

Here \(B^{*}\) is the random batch where \(i\)-th sample belongs during a particular training step, and \(\) is the learnable temperate parameter. Notably, the teacher loss differs from CLIPScore primarily by a normalization term \(^{*}\) as follows:

\[-_{B^{*}}(x_{i}^{vl})=_{v}(x_{i}^{v})^{}_{l}(x_{i}^{l})}_{}-[_{j B^{*}} (_{v}(x_{i}^{v})^{}_{l}(x_{j}^{l})}{})+ _{j B^{*}}(_{v}(x_{j}^{v})^{}_{l}(x_{i}^{l})}{ })]}_{^{*}}\]

In practice, since the training dataset of teacher CLIP models, like OAI-WIT400M , and the actual batch divisions \(B^{*}\) is inaccessible, we randomly select \(K\) batches from the student model's training data and use the averaged results from \(\{B_{k}\}_{i=1}^{K}\) to estimate the normalization term \(^{*}\) on \(B^{*}\):

\[(x_{i}^{vl}):=-_{k=1}^{K}_{B_{k}}(x_{i }^{vl})\;\;(x_{i}^{vl})-^{*}\] (2)

Here \(\{B_{k}\}_{i=1}^{K}\) are some batches randomly selected from the student model's training data and \(x_{i} B_{k}, k\). We choose \(K=10\) in our experiments, but any sample size larger than 5 is sufficiently stable for estimating the original CLIPLoss (Details in Appendix D.1). Besides, in Sec. 4.3.3 we also show that the computational cost introduced by \(\) remains negligible compared to other baselines. The temperature \(\) and batch size \(|B^{*}|\) can be directly obtained from the parameters of the pretrained teacher model. More details of negCLIPLoss are in Appendix, including the concentration analysis of \(\) (Appendix A.1), pseudocode (Algorithm 1), and the ablation study of \(\) and \(|B|\) (Appendix C.2).

**Motivation behind negCLIPLoss.** Other existing works also use loss-guided data selection, such as LESS  in NLP, CoDis  in CV, and RHO  in general data scheduling scenarios. However, it is still unclear whether selecting based on teacher loss is suitable for multi-modal contrastive learning. Here we give an affirmative answer as shown in Fig. 2, where we can see negCLIPLoss performs better than or on par with CLIPScore consistently.

To illustrate how teacher loss helps our selection, we demonstrate that the normalization term provided by negCLIPLoss is crucial for correcting the overestimation or underestimation inherent in CLIPScore. A high normalization term implies that either the image embedding, text embedding, or both can easily match multiple contrastive pairs beyond their

Figure 1: Illustration of negCLIPLoss. CLIPScore may underestimate (bottom left, where the data quality is high but CLIPScore is low) or overestimate (bottom right, where the data quality is low but CLIPScore is high) the quality of image-text pairs. However, this issue can be mitigated by simply subtracting a normalization term \(\). negCLIPLoss employs the teacher model to calculate the negative CLIP loss on training data and serves as a more accurate metric. Here, “Top X%” denotes that the score represents the top X% _high_ values within the entire dataset (i.e., the (100-X)% percentile among all the values). For example, “\(:\) Top \(100\%\)” means this data has almost the smallest \(\) among the whole dataset, which represents that it contains highly specific elements in both images and texts.

Figure 2: Comparison of negCLIPLoss and CLIPScore across different downsampling ratios on DataComp-medium.

corresponding counterparts. For example, in the bottom right of Fig. 1, the text containing "Image" or "Photo" can be easily matched with any visual content. Similarly, the image of "verloopring" only contains very simple features and can be matched with many words like "white", "empty" or "circle", etc. Consequently, despite a high absolute CLIPScore, the relative negCLIPLoss within its batch can be lower. In contrast, the bottom left features highly specific elements in both text and images, such as "Islands Harbor," "American football", and "sheep at green". These elements are specific and less likely to match with contrastive pairs, resulting in a higher relative negCLIPLoss.

### NormSim: A New Training-Target Similarity Metric

Our proposed negCLIPLoss is a universal approach to improve filtering performance by estimating quality better, and it does not rely on any downstream task. Now, if we can access some knowledge of the downstream tasks, we could further improve the performance by using a vision-only _\(p\)-norm similarity to target data_ metric to measure the relationship between each training sample and the downstream target data. We will discuss the reason to use vision-only embedding later in this section.

Specifically, we assume access to the target set of downstream tasks and denote them as \(X_{}=\{x_{,(1)},,x_{,(m)}\}\), where each \(x_{,(i)}^{d}\) is _i.i.d._-sampled from the target downstream distribution \(_{}\)3, but without overlapping with the test set. Then, for each training sample \(x^{vl}\) and the corresponding target set \(X_{}\), the NormSim is defined as:

\[_{p}(X_{},x):=\|_{v}(X_{}^{v}) _{v}(x^{v})\|_{p}=(_{x_{t} X_{}}| {f}_{v}(x_{t}^{v}),_{v}(x^{v})|^{p})^{1/p}\] (3)

We select the subset \(S\) by choosing the samples with top-\(N\) highest NormSim scores. The choice of the norm type \(p\) can be based on the data distribution and training process. In this paper, we consider two instantiations of \(p\):

When \(p=2\), our data selection method can be regarded as the following equation. It's equivalent to selecting a subset that aligns with the principal components of the target set variance (Appendix C.6.1).

Figure 3: Illustration of NormSim on DataComp. \(X_{}\) is the target prior data. “Top X%” denotes that the score represents the top X% high values within the entire dataset. (a) Visualization of data with different NormSim and negCLIPLoss. Here we use \(_{2}\)(ImageNet-1k) as an example. Although both Type 2 and Type 4 data have high negCLIPLoss and thus high quality, data with low \(_{2}\) (Type 4) are more irrelevant to downstream tasks like ImageNet, VTAB, and MSCOCO. For example, they contain many images dominated by OCR content and make little contribution to improving downstream performance. (b) Illustration of a rough comparison of sampling data for different filtering methods. Using “negCLIPLoss \(\) NormSim” filtering can balance the quality and relevance to downstream tasks, thus increasing the proportion of Type 2 data. (Refer to Appendix E for more visualization.)

\[S=_{|S|=N}_{i S}_{2}(x_{t},x_{i}),_{2}(x_{t},x_{i})=(_{x_{t} X_{}}|_{v}(x _{t}^{v})^{}_{v}(x^{v})|^{2})^{1/2}\] (4)

When \(p=\), the distance metric can be regarded as an even more optimistic measure, such that a training sample will be selected if it has high similarity to _any target sample_. Note that this is different from nearest-neighbor-based method used in image-based filtering , where they are trying to find the nearest training sample of _every target sample_. In this case, it can be regarded as:

\[S=_{|S|=N}_{i S}_{}(x_{t},x_{i}), _{}(x_{t},x_{i})=_{x_{i} X_{}}_ {v}(x_{t}^{v})^{}_{v}(x_{i}^{v})\] (5)

In Appendix D.3, we also show that our NormSim\({}_{}\) can outperform the nearest neighbor selection on the downstream target tasks. Here, we show an example selected via the NormSim\({}_{2}\)(ImageNet-1k) in Fig. 3, showing that this vision-target-aware method is complementary to the quality-based one.

**Choice of Target Data.** In the experiment parts, we try two kinds of target data: training data from ImageNet-1k (1.3M) or training data from all 24 accessible downstream tasks (2.1M)4. We denote them as \(_{p}\)(IN-1k) and \(_{p}\)(Target), respectively.

**Necessity of using vision-only information** We use only the visual information \(x^{v}\) instead of multimodal information \(x^{v^{l}}\) for measuring similarity. This is because common crawled text often has brief captions, making the OAI CLIP language embedding weaker than its visual embedding model . Consequently, the language part cannot characterize the pre-training and downstream task distribution as well as the visual part. This phenomenon is also observed in Gadre et al. , where image-based filtering (select data whose image embeddings are similar to that from ImageNet-1k) outperforms text-based filtering (select data whose captions contain words from ImageNet-21k). More ablation studies are provided in Appendix D.4.

**Generality of NormSim in choosing teacher model.** Notably, since we just use image embeddings in the NormSim metric, we believe it unnecessary to use CLIP model to obtain NormSim. NormSim can be a general metric for selecting target-related image/image-text data if any good image representations are given, like the representations obtained from pretrained ResNet-50.

**Theoretical justification.** Unlike many existing methods that force diversity by selecting training samples around each \(_{}\), our strategy maximizes similarity without directly considering data diversity. For the \(p=2\) case, we demonstrate that maximizing \(_{2}\) is optimal under a linear model \(_{v}\), as shown in Appendix A.2. Our theorem also provides error guarantees for noisy embeddings and explains when vision-only embeddings outperform combined vision and language embeddings. Recent work by Joshi et al.  provides a similar analysis but focuses on high-quality data and cross-variance between images and texts. This approach is less effective than image-only methods for filtering noisy datasets, as discussed above.

**Using proxy when downstream \(_{}\) is inaccessible.** Surprisingly, we show that the 2-norm can also be used when only the pre-training set is available. In this case, we construct a proxy "target" set from the pre-training set itself. Specifically, let \(S_{i}\) be the selected subset at step \(i\), then we treat the current \(S_{i}\) as the proxy "target" set. To construct the next smaller set, we select the next data batch \(S_{i+1}\) satisfying \(_{S_{i+1} S_{i}}_{x S}_{2}(S_{i},x)\), until reaching an N size subset. We call this approach \(_{2}\)-D (Dynamic) and will specify the algorithm details in Appendix C.3.

## 4 Experimental Results

In this section, we evaluate the performance of negCLIPLoss and NormSim, aiming to address the following questions: **Q1:** Given a fixed CLIP teacher model, can our methods more effectively utilize CLIP embeddings for data filtering? **Q2:** Are our methods applicable to diverse CLIP teacher models with varying architectures or different pretrained datasets? **Q3:** How does our method compare to other leading approaches that utilize external models or multimodal datasets? Additionally, could our method be compatible with these methods and enhance their effectiveness?

### Setup

We adhere to the standardized training and evaluation protocols of the DataComp benchmark . **Training configuration.** We employ the medium-scale training configuration of DataComp (DataComp-medium). It provides a substantial dataset comprising 128 million low-quality, web-curated image-text pairs to be filtered. Once the data subset is obtained by some data filtering strategy, it will be used to train a fixed CLIP-B/32 model in a fixed training budget that allows the model to pass 128 million data points an epoch. Therefore, smaller subsets will be repeated more frequently, ensuring a fair comparison. We note that the size of the DataComp dataset becomes smaller over time since some URLs of images become invalid5, and we only successfully downloaded about 110M data. Therefore, the results of baselines on the leaderboard do not apply to our datasets, and we reproduce all the top baselines on the leaderboard with their public UIDs of the selected data.

**Evaluation.** We measured the model performance on 38 downstream datasets including image classification and retrieval tasks followed by DataComp. The image classification tasks contain ImageNet-1k , ImageNet distribution shifts , 11 datasets from the Visual Task Adaptation Benchmark (VTAB)  and 3 datasets from WILDS . Retrieval datasets contain Flickr30k , MSCOCO  and WinoGAViL .

**Teacher model architecture.** Our experiments utilize two architectures for OpenAI's CLIP teacher models: ViT-L/14 and ViT-B/32. Additionally, we use the public version of DFN (DFN-P) proposed by Fang et al.  as a teacher model, and its architecture is also ViT-B/32.

### Baselines

We restate the three current research directions mentioned before based on how much external resources are employed: (D1) using OAI CLIP alone while optimizing embedding employment strategies, (D2) training and using a more advanced CLIP embedding model based on external data, and (D3) utilizing non-CLIP external models to aid data selection. It is important to note that D2 and D3 may also incorporate strategies from D1. For example, CLIPScore (D1) has been used in almost all the top methods. Therefore, we categorize baselines by the largest possible category they encompass. According to the above categorization, we summarize the baselines we used in our experiments as follows. Please refer to Fig. 4 and Appendix C.4 for more details.

**D1: OAI CLIP embedding only.** The learner can only access the pretraining dataset (like DataComp-medium), the original OAI CLIP teacher model that is used to extract embeddings, and some target data of the downstream tasks which is much smaller than the pretraining dataset (like ImageNet-1k). In this category, we don't use any existing external non-CLIP models or any newly trained CLIP model based on external multi-modal dataset. In detail, This category includes (1) **CLIPScore**, which only uses CLIPScore for filtering as we mentioned before. (2) **Image-based filtering**, which uses ImageNet-1K training data as the downstream target data for data filtering. It applies k-means clustering to the _image_ embeddings of training data and selects clusters closest to the ImageNet-1K embeddings. Gadre et al.  also try to combine image-based filtering and CLIPScore together. (3) \(^{2}\) **Pruning**, which represents the dataset as an undirected graph and selects the data by combining difficulty and diversity. They use the CLIP score to initialize their graph.

**D2, D3: Accessible external model and multi-modal data.** All the current top baselines enable the learner to utilize external resources, either to train a better CLIP teacher model or to help filtering using existing models' properties. In detail, (1) **DFN** trains another CLIP data filtering network via external high-quality data. Their currently public model (**DFN-P**) is trained on CC12M  + CC3M  + SS15M , while the best DFN is trained on nonpublic HQITP-350M , which is even larger than DataComp-medium. (2) **HYPE** leverages hyperbolic embeddings (different from CLIP embedding) and the concept of entailment cones to filter out samples with meaningless or underspecified semantics, enhancing the specificity of each sample. (3) **HYPE**\(\)**DFN** proposed by  samples subset separately for each method and then merge them. This is the state-of-the-art method on the DataComp benchmark for medium size. (4) Other methods including **T-MARS**, **Devils**, **MLM**, which leverage external models such as text detection model FAST , BLIP-2  and LLAVA-1.5  to heuristically select data. See details in Appendix C.4.

**Cross-setting comparison.** We make these separations for fair comparison. Intuitively, performance should be ranked as **D2, D3 > D1**. However, our results show that cross-setting comparisons are possible and our D1 methods can perform similar or even better than most of D3 methods.

[MISSING_PAGE_FAIL:8]

#### 4.3.2 Try Other Teacher Models (Q2)

To evaluate whether our method applies to other CLIP teacher models, we replaced OAI CLIP-L/14 with OAI CLIP-B/32 and DFN-P as embedding models. We compare the best baseline "CLIPScore" with our "negCLIPLoss" and best strategy "negCLIPLoss \(\) NormSim\({}_{}\)(Target)" as shown in Table 1 and Appendix D.2. Note that the original DFN paper selects a subset comprising 19.2M data points, which accounts for approximately \(17.5\%\) of our dataset and \(15\%\) of their dataset, we incorporate these sampling ratios into our comparison.

**negCLIPLoss can be applied to different CLIP embedding models.** Our proposed negCLIPLoss, as a replacement of CLIPScore, not only leads to better performance compared to all the other baselines using OAI CLIP-L/14 as shown in Table 2, but also achieves universal improvement on the other two CLIP embedding models, OAI CLIP-B/32 and DFN-P as shown in Table 1. Our methods can consistently outperform all downstream tasks for different filtering ratios and models, like a 0.5%-5.4% increase on ImageNet-1k.

**Embedding required by NormSim should have good downstream performance.** When combining negCLIPLoss with NormSim\({}_{}\), OAI CLIP-B/32 and DFN-P exhibit completely different behaviors. The former obtains results even better than those in Table 2, which uses OAI CLIP-L/14 as the teacher model, while DFN-P achieves results even worse than using negCLIPLoss alone6. The reason is that, unlike OAI CLIP-B/32, DFN-P is specially designed for data filtering _at the expense of downstream task performance_, as claimed by its authors. For example, the ImageNet-1k accuracy for DFN-P, OAI CLIP-B/32, and OAI CLIP-L/14 are 45%, 63%, and 75%, respectively. This indicates that the embeddings obtained from DFN on target data might be highly unreliable, leading to inaccurate similarity calculations between training and target data. To support this, if we use DFN-P to evaluate negCLIPLoss but utilize OAI CLIP-B/32 for calculating NormSim, as shown in "negCLIPLoss (17.5%) \(\) NormSim\({}_{}^{}\)(Target)", we can further improve the results compared to using negCLIPLoss alone. Its average performance on 38 tasks is even higher than utilizing the best DFN (trained on HQITP-350M) with CLIPScore, as shown in Table 3.

#### 4.3.3 Comparison with D2 & D3 Categories (Q3)

In this part, we compare all the D2 & D3 baselines mentioned in Sec. 4.2 together with our best strategy in Table 3. Here we reproduce all the baselines if their official UIDs are available. For "A \(\) B" mentioned in Table 3, we follow the way of "HYPE \(\) DFN" in Kim et al.  to merge the data, which generates the sampling subset separately for each method and then merge them. This will result in oversampling the shared data, which is intuitively more important.7 We also show the best result

    &  & **Dataset Size** & **IN-1k** & **IN Dist. Shift** & **VTAB** & **Retrieval** & **Avg.** \\  & & **Size** & (1) & (5) & (11) & (3) & (38) \\  D3 & T-MARS  & 22M & 30.8 & 26.3 & 34.8 & 25.4 & 34.1 \\ D3 & Devil  & 20M & 31.0 & 26.7 & 35.9 & 24.7 & 34.5 \\ D3 & MLM  & 38M & 30.3 & 25.6 & 36.0 & **29.0** & 34.5 \\ D3 & HYPE  & 10M & 30.3 & 25.8 & 34.3 & 22.2 & 31.9 \\ D2 & DFN  & 16M & 36.0 & 30.1 & 36.2 & 27.0 & 35.4 \\ D3 & DFN \(\) HYPE  & 20M & 36.4 & 30.8 & 38.5 & 28.0 & 36.8 \\  D1 & **Ours (20\%)** & 22M & 32.4 & 27.4 & 35.9 & 26.3 & 35.2 \\ D3 & DFN \(\)**Ours (20\%)* & 23M & 36.4 & 30.9 & **38.6** & 28.1 & 37.6 \\ D3 & DFN \(\) HYPE \(\)**Ours (10\%)* & 22M & **37.3** & **31.4** & 38.5 & 27.6 & **37.7** \\   

Table 3: Results of all D1&D2&D3 top methods on DataComp-medium. The results of MLM  are from their paper, while all other baselines are reproduced on our downloaded dataset using their official UIDs. “Ours (20%)” refers to use “negCLIPLoss (30%) \(\) NormSim\({}_{}\)(Target)” to get 20% of original data, while “Ours (10%)” denotes applying “negCLIPLoss (20%) \(\) NormSim\({}_{}\)(Target)” to get 10%. And we use “*” to indicate the case where we choose the intersection of the data selected by using OAI CLIP-B/32 and OAI CLIP-L/14 separately, which results in about 15M data for “Ours (20%)*” and 7.4M data for “Ours (10%)*”.

we obtain by combining our method with DFN  and HYPE  on the full DataComp-medium dataset in Table 4, where the baselines are from DataComp benchmark.

**Our methods can outperform most of the D3 methods.** In Table 3, we show that without using any external models or data, our best combination, i.e., using OAI CLIP-B/32 for "negCLIPLoss (30%) \(\) NormSim\({}_{}\)(Target)" (**Ours (20%)**), still outperforms all methods except DFN and "DFN \(\) HYPE". This answers the first part of Q3 and further indicates that some external models may be redundant since CLIP embeddings already contain necessary information.

**We can further improve the SOTA method.** In Table 3, we show that our model can further boost the performance of the current SOTA method "HYPE \(\) DFN" by 0.9% on both ImageNet-1k and on average 38 downstream tasks, and close results can be achieved even without combining HYPE which utilizes the external embedding model MERU . And we update the SOTA performance of the DataComp-medium (full dataset) benchmark as shown in Table 4. Here we use the data selected by both OAI CLIP-B/32 and L/14, which we found is more robust than using one of them alone. Our better results answer the second part of Q3, that is, our methods can be compatible with other D2&D3 methods.

## 5 Conclusion and Limitation

In this paper, we introduce two metrics, negCLIPLoss and NormSim, to enhance data selection in multimodal contrastive learning without relying on external resources. negCLIPLoss provides a more accurate quality metric compared to the commonly used CLIPScore, while NormSim measures the similarity between pretraining data and target data for known downstream tasks. Experiments show that our methods achieve results that are competitive with or even better to approaches using external models or datasets. Additionally, negCLIPLoss and NormSim are compatible with existing top techniques, allowing us to achieve a new state-of-the-art by combining them.

A notable limitation of our study is the exclusion of larger pretraining datasets, such as the large and xlarge scales of DataComp. However, DataComp-medium is the most commonly used benchmark for data selection in CLIP pretraining, and our method has demonstrated both effectiveness (Table 2-3) and efficiency (Table 5) on it. Future directions include exploring better ways to merge data selected by different methods and incorporating our methods into data scheduling scenarios.

## 6 Acknowledgement

We thank Tong Chen, Pang Wei Koh, Xiaochuang Han, Rui Xin, Luyao Ma, Lei Chen, and other members in the UW ML Group for many insightful discussions and helpful feedback. The research of Kevin Jamieson and Yifang Chen are partially supported by the NSF through the University of Washington Materials Research Science and Engineering Center, DMR-2308979, and awards CCF 2007036. SSD acknowledges the support of NSF IIS 2110170, NSF DMS 2134106, NSF CCF 2212261, NSF IIS 2143493, NSF CCF 2019844, and NSF IIS 2229881.

  
**Strategy** & **IN-1k** & **Avg.** \\  No filtering & 17.6 & 25.8 \\ CLIPScore  & 27.3 & 32.8 \\ T-MARS  & 33.0 & 36.1 \\ Devils  & 32.0 & 37.1 \\ DFN  & 37.1 & 37.3 \\ DFN \(\) HYPE  & **38.2** & 37.9 \\  DFN \(\)**Ours (20\%)** & 37.5 & 38.6 \\ DFN \(\) HYPE \(\)**Ours (10\%)** & **38.2** & **38.8** \\   

Table 4: Results of the top methods on the full DataComp-medium dataset (128M data).