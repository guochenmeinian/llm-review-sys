# Revisiting Motion Information for RGB-Event

Tracking with MOT Philosophy

 Tianlu Zhang

EMIM

Xidian University

tlzhang96@outlook.com &Kurt Debattista

Warwick Manufacturing Group

University of Warwick

K.Debattista@warwick.ac.uk &Qiang Zhang*

EMIM

Xidian University

qzhang@xidian.edu.cn &Guiguang Ding

School of Software

Tsinghua University

dinggg@tsinghua.edu.cn &Jungong Han*

Department of Automation

Tsinghua University

jungonghan77@gmail.com

Corresponding author.

###### Abstract

RGB-Event single object tracking (SOT) aims to leverage the merits of RGB and event data to achieve higher performance. However, existing frameworks focus on exploring complementary appearance information within multi-modal data, and struggle to address the association problem of targets and distractors in the temporal domain using motion information from the event stream. In this paper, we introduce the Multi-Object Tracking (MOT) philosophy into RGB-E SOT to keep track of targets as well as distractors by using both RGB and event data, thereby improving the robustness of the tracker. Specifically, an appearance model is employed to predict the initial candidates. Subsequently, the initially predicted tracking results, in combination with the RGB-E features, are encoded into appearance and motion embeddings, respectively. Furthermore, a Spatial-Temporal Transformer Encoder is proposed to model the spatial-temporal relationships and learn discriminative features for each candidate through guidance of the appearance-motion embeddings. Simultaneously, a Dual-Branch Transformer Decoder is designed to adopt such motion and appearance information for candidate matching, thus distinguishing between targets and distractors. The proposed method is evaluated on multiple benchmark datasets and achieves state-of-the-art performance on all the datasets tested.

## 1 Introduction

Single object tracking (SOT) aims to predict the position of a target in videos, by being given only the position of the target in the initial frame. While traditional RGB-based trackers  can effectively capture comprehensive scene representations, including color and semantic information, they face significant performance degradation in challenging conditions like fast motion, low illumination and distractions from similar objects.

To address such challenges associated with frame-based cameras, some researchers have taken advantage of event cameras , which are characterized by high temporal resolution and high dynamic range, to augment the RGB data for reliable object tracking. In the past few years, various methods have been proposed for RGB-Event (RGB-E) object tracking. Existing RGB-E trackers primarily concentrate on exploring complementary appearance information within RGB and eventdata to enhance tracking performance with three typical approaches i.e., early fusion method , middle fusion method [37; 38] and one-stream method [24; 44]. Despite achieving commendable improvements, mainstream RGB-E tracking algorithms still cannot solve the association problem of the targets and distractor objects in the temporal domain, as shown in Fig. 1 (a).

Alternatively, some RGB trackers [2; 3; 19] propagate valuable scene information through the sequence to improve their discriminative ability, as shown in Fig. 1 (b). These methods mine the information in two main ways. The first one relies on implicitly transforming the scene information to locate the targets, and the scene information is generally represented using a set of embeddings [2; 3]. To ensure effective transformation and avoid introducing noisy information, these methods usually require careful design of the encoding strategy to obtain effective scene information embeddings. The second approach explicitly explores the scene information by simultaneously keeping track of both targets and distractors [19; 41]. However, these methods are susceptible to environmental interference, and their matching strategies relying on appearance information may miss the target when the target and distractor trajectories are close.

In fact, event data can not only provide the edge information to improve the RGB feature representations but also contains abundant motion cues to reflect the motion state of the objects, which is meaningful to differentiate between targets and distractors, even if they may look similar. Motivated by these observations, we propose an Appearance-Motion Modeling RGB-E tracking framework with a Cascade Structure, referred to as CSAM, that goes beyond leveraging complementary appearance information within RGB-E data. As shown in Fig. 1 (c), the proposed CSAM framework employs an appearance model to initially determine the candidates with similar appearance to the targets, and then designs a candidate matching network with encoder-decoder structure to dynamically incorporate motion information contained in the RGB-E videos to track all the candidates with the Multi-Object Tracking (MOT) philosophy. The candidate that matches the historical target tracklet will be regarded as the final tracking result.

Specifically, the candidate matching network consists of a Candidate Encoding Module (CEM), a Spatial-Temporal Transformer Encoder (STTE), and a Dual-branch Transformer Decoder (DBTD). Recognizing the critical roles of the two types of information in candidate association, our proposed CEM is used to encode both appearance cues and motion representations for each candidate. Subsequently, an STTE block, comprising a Spatial Encoder and a Temporal Encoder, is introduced to model spatial-temporal relationships among candidates by synergistically utilizing such appearance and motion embeddings. Finally, a DBTD block, comprising a Spatial-temporal Decoder and a Motion Decoder, is presented to match candidates with historical tracklets by using both appearance and motion information.

**Contributions:** In summary, our contributions are: **(i)** We propose a novel RGB-E tracking framework, i.e., CSAM, which first predicts the candidates by using an appearance model and then keeps track of both targets and distractors with an MOT philosophy. To the best of our knowledge, we are

Figure 1: Architectures of different RGB-E tracking frameworks. (a) RGB-E tracker based on appearance information. (b) RGB tracker based on scene information propagation. (c) Our proposed CSAM framework.

the first to introduce the MOT philosophy for the SOT task using RGB-E data. **(ii)** We propose three effective modules: a Candidate Encoding Module, a Spatial-Temporal Transformer Encoder and a Dual-branch Transformer Decoder. The appearance information as well as the motion cues within the RGB-E data can be fully exploited by the proposed modules for accurate candidate association. **(iii)** We show significantly improved state-of-the-art results of our proposed method on multiple RGB-E tracking benchmarks.

## 2 Related Work

**Visual object tracking:** The current prevalent tracking pipelines can be categorized into three groups: CNN-based trackers, CNN-Transformer trackers and Transformer-based trackers. CNN-based trackers utilize a Siamese network [17; 15] or Discriminative Correlation Filter (DCF) [8; 1] to address tracking tasks by matching templates and search regions. However, the inherent properties of CNNs limit their ability for global information exploration and interaction, thereby constraining the advancement of CNN-based trackers. In response, some CNN-Transformer trackers [33; 5; 28] employ attention mechanisms to establish global dependencies between template features and search features. But these hybrid CNN-Transformer trackers still independently extract features from templates and search regions using CNN networks, resulting in extracted features being unaware of the tracking target. To address this issue, several pure Transformer-based trackers [35; 32; 40] overcome the challenge by unifying feature extraction and feature relation modeling through a single Transformer backbone, leading to state-of-the-art tracking performance.

**RGB-E object tracking:** In recent years, there has been a growing interest among researchers in merging RGB frames and event streams for object tracking. Some researchers focus on exploring complementary information within RGB-E data via specially designed cross-modal interaction strategies [29; 24; 44]. For instance, Zhang et al.  proposed a cross-domain feature integrator to dynamically fuse visual cues from both the frame and event domains. Alternatively, CEUTtrack  proposed a one-stream framework based on the Transformer, which simultaneously addresses feature extraction, template-search relation modeling, and cross-modal interaction. Recently, some methods [13; 42; 31] aim to adapt the RGB tracking model to RGB-E tracking in the prompt learning manner. _However, existing RGB-E tracking frameworks cannot fully explore the abundant motion cues within the event stream, consequently limiting tracking performance in the presence of distractors._

**Multi object tracking:** Multi-object tracking (MOT) aims to track multiple objects in a video sequence. Currently, the tracking-by-detection paradigm [7; 6], where an object detector is initially employed to locate all proposals, followed by an association network to match all of these objects, is gaining popularity for the MOT task. Additionally, some researchers have explored the joint-detection-and-tracking pipeline , aiming to achieve detection and tracking simultaneously in a

Figure 2: Overview of the proposed RGB-E tracking pipeline.

single stage. There are some approaches that aim to enhance the tracking performance of Single Object Tracking (SOT) via the use of an MOT philosophy. For instance, DMTrack  designed a lightweight detector and an explicit object association module to track both targets and distractors. KeepTrack  proposed a learnable candidate matching network and designed several mechanisms, including partial supervision, self-supervised learning and sample-mining, to address the problem of incomplete annotation in SOT training data. _However, these methods only use the RGB modality and overlook the importance of spatial-temporal relationships among candidates for matching candidates and tracklets._

## 3 Method

As shown in Fig. 2, our framework first employs an appearance model to generate the potential proposals. Subsequently, several modules are proposed to identify targets and maintain tracking of all candidates to prevent tracking drift. Specifically, the appearance model predicts the target scores and bounding boxes for \(\) candidates of the \(\) current frame and \(\) candidates for each previous frame (see Sec.3.1). Secondly, a set of features are extracted for each candidate from \(\) previous frames, including target classification scores, appearance features, event embeddings and candidate locations. These features are then aggregated into appearance embeddings and motion embeddings for each proposal (see Sec.3.2). Thirdly, the STTE is employed to jointly model spatial-temporal relationships for each tracklet (see Sec.3.3). Fourthly, utilizing the DBTD, an \(\) assignment matrix is calculated for matching tracklets from previous frames with candidates from the current frame (see Sec.3.4). It should be noted that not every tracklet has all the candidates in the previous \(\) frames due to occlusion, missing detection, etc. For illustrative purposes, we consider the situation that there are no missing candidates for each tracklet. Our method solves the cases with missing tracklets similarly to the typical multi-object tracking method  (see supplementary material Sec.B). In the following, we will describe the proposed tracking framework in detail.

### Appearance Model

Here, we employ CEUTrack  as our appearance model. Specifically, the event streams are initially transformed into voxel representations through a voxelization operation . Subsequently, given the initial locations and tracking results, we crop the template patch, the template voxel, the search patch and the search voxel, respectively. After that, the projection layers are adopted to transform the four inputs into token representations, which are then fed into the vanilla ViT  for joint feature extraction, cross-modal interaction and search-template matching. Finally, the tracking head, employing the same structure as that in OStrack , takes the concatenated RGB and event search region features from the backbone as input to predict the appearance tracking results. Please refer to the supplementary material (Sec.A.1) for additional details about the appearance model.

### Candidate Embedding Module

The goal of CEM is to first select initial candidates similar to the target and filter out the most simple negative backgrounds, and then obtain the appearance embeddings as well as the motion embeddings. With the classification scores and regression offsets outputted by the appearance model, we generate \(\) candidates similar to the target for each previous frame by using Non-maximum Suppression (NMS) . The appearance features of each candidate can be obtained by performing the PRoIAligh  on the RGB backbone features based on its corresponding location. We can represent the appearance features of each candidate in the \(\) frame as \(F^{}=\{f^{}_{}^{}\}^{ }_{n=1}\) and represent their corresponding classification scores as \(C^{}=\{c^{}_{}^{}\}^{ }_{n=1}\). Both the appearance features and the classification scores convey essential appearance cues for each candidate. To integrate these two types of information, we propose an Appearance Feature Encoding (AFE) layer. Specifically, AFE processes the backbone features \(f^{}_{}\) via a single convolution layer to obtain more discriminative features \(fe^{}_{}^{}\) and employs several MLP layers on \(c^{}_{}\) to generate the classification embeddings \(ce^{}_{}^{}\). These features are then combined as: \(a^{}_{}=fe^{}_{}+ce^{}_ {}\). The appearance feature embeddings for the \(\) frame are thus represented by \(A^{}=\{a^{}_{}\}^{}_{n=1}\).

Additionally, we obtain the event features of each candidate in the \(()-\) frame as \(E^{}=\{_{}^{}^{}\}_{n=1}^{}\) and represent their locations as \(P^{}=\{p_{}^{}^{}\}_{n=1}^ {}\), where \(p_{}^{}=\{x_{}^{},y_{}^ {},w_{}^{},h_{}^{}\}\) denotes the normalized bounding box coordinates. Both the event stream and the location set contain rich motion information about those candidates. To fuse these two types of features, a Motion Feature Encoding (MFE) layer, which has a similar structure of AFE, is first used to obtain the enhanced event representations \(}}}}}} }}}}^{} ^{}\) and location embeddings \(p_{}^{}^{}\). Then these features are fused as: \(m_{}^{}=e_{}^{}+p_{}^{ }\). The motion feature embeddings of each candidate in the \(()-\) frame can be thus represented by \(M^{}=\{m_{}^{}\}_{n=1}^{}\).

### Spatial-Temporal Transformer Encoder

The proposed STTE aims at learning more discriminative feature representations for each tracklet and establishing effective relationships among objects in both spatial and temporal domains. The inputs for STTE include \(\) sets of appearance embeddings \(\{A^{},...,A^{}\}\) and \(\) sets of motion representations \(\{M^{},...,M^{}\}\). All of these embeddings will be first processed via a Spatial Encoder to construct the spatial relationships among candidates in each frame. Subsequently, these spatially encoded features are re-arranged to construct \(\) tracklets across \(\) frames, and their temporal relationships are established via the proposed Temporal Encoder.

**Spatial Transformer Encoder:** The proposed Spatial Encoder independently processes each frame to construct spatial correlations, and we take the \(()-\) frame as an example for illustration. It is difficult to establish distinctive spatial relationships by only using the appearance information. Differently, the motion information is more suitable to establish meaningful spatial affinities. Inspired by the graph attention networks , each candidate's appearance embedding is regarded as a node, and the edge weight between each nodes is defined by the motion information.

Specially, at the \(()-\) frame, we consider each candidate's appearance and motion representations in \(A^{}\) and \(M^{}\) as two node sets \(V_{}^{}\), respectively. As shown in Fig. 3 (a), we use a complete bipartite graph \(G_{m}^{}=(V_{m}^{},E_{m}^{})\) to model the object-level relations between these candidates. Here, \(E_{m}^{}=\{(u,v)| u,v V_{}^{}\}\). Then, the edge weight between node \(i\) and node \(j\) in \(V_{m}^{}\) will be denoted as \(e_{ij}\), which can be calculated through the inner product operation. More specifically, as that in a typical Transformer block, some normalization layers and linear transformations are first applied on these motion nodes, followed by an inner product calculation, to achieve \(e_{ij}\). Formally,

\[e_{ij}=(W_{}m_{i}^{})^{}(W_{}m_{j}^{ }),\] (1)

where \(W_{}\) and \(W_{}\) are the linear transformations and \(e_{ij}\) will be further normalized by the softmax function, obtaining

\[ew_{ij}=)}{_{k V_{}^{}}exp(e_{ik} )}.\] (2)

Figure 4: Architectures of proposed Graph Multi-head Attention Block.

Figure 3: Architectures of the proposed Spatial-Temporal Transformer Encoder.

With the edge weights passed from all nodes in \(V_{}^{t-1}\) to the \(i-\) node in \(V_{}^{t-1}\), the aggregated representation of the \(i-\) node in \(V_{}^{t-1}\) can be transformed by:

\[me_{i}^{t-1}=_{j V_{}^{t-1}}ew_{ij}W_{}m_{j}^{t-1}.\] (3)

where \(W_{}\) is a matrix for linear transformation. It should be noted that we adopt the multi-head attention structure to improve the discriminability of graph attention learning.

Finally, we fuse the aggregated features \(me_{i}^{t-1}\) with the appearance features \(a_{i}^{t-1}\) to obtain a more powerful feature representation:

\[_{i}^{t-1}=FFN(cat(me_{i}^{t-1},a_{i}^{t-1})),\] (4)

where \(cat()\) represents vector concatenation, \(FFN()\) denotes the feedforward neural network. The final spatial encoded features \(ASE^{t-1}=\{s_{1}^{t-1},...,s_{}^{t-1}\}\) are obtained by further employing the residual connections and two FFN layers as that in a typical Transformer block. These spatial encoded features are re-arranged to \(\) tracklet sets \(\{_{1},...,_{}\}\) of all candidates through the \(\) frames, where \(_{}=\{s_{}^{t-},...,s_{}^{t-1}\}\). Meanwhile, the motion embeddings are also re-arranged to \(\) tracklet sets \(\{_{1},...,_{}\}\), where \(_{}=\{m_{}^{t-},...,m_{}^{t-1}\}\).

**Temporal Transformer Encoder:** These \(\) tracklet sets are further encoded by a Temporal Transformer Encoder. Here, we take \(-\) tracklet as an example for illustration. As shown in Fig. 3, we first fuse the spatial encoded features \(_{}\) with the motion feature set \(_{}\) by the element-wise addition operation, and then employ the multi-head attention to calculate the attention weights \(A_{}^{}\), thus generating the attention-weighted features. After that, these weighted features are processed through two FFN layers and residual connections to obtain the final output \( E_{}\), which consists of the \(-\) tracklet's feature representations \(\{t_{}^{t-},...,t_{}^{t-1}\}\). The outputs of the Temporal Transformer Encoder \(\{ E_{1},..., E_{}\}\) are re-arranged to \(^{}^{}\).

### Dual-branch Transformer Decoder

The proposed DBTD generates the assignment matrix \(A\) by using the output of the Transformer encoder \(^{}\) and the features of \(\) candidates in the current frame. Initially, we generate the spatial-encoded feature set \(\{s_{1}^{t},...,s_{}^{t}\}\) and re-arrange it to \(^{}_{}^{}\) as that in the Sec.3.3. Then, \(^{}_{}\) is duplicated \(\) times, resulting in \(^{}_{}^{}_{} ^{}\).

After that, the proposed Spatial-temporal Decoder follows the standard Transformer framework, which takes the spatial-encoded features of the current frame \(^{}_{}\) as the query, and uses the encoded features of the previous frame \(^{}\) as the key and value. The Multi-Head Attention mechanism  is calculated for \(^{}_{}\) and \(^{}\) to generate attention weights. The output passes through two FFN layers and residual connections, generating the output tensor \(^{}\). The output of the appearance decoder can be processed through an FFN and a softmax layer to generate the assignment matrix \(A^{}^{}\).

Moreover, to better match each tracklet with the candidates in the current frame, the motion information of the \((-1)-\) frame is utilized as the motion information for the tracklet. The Motion Decoder employs the same structure as that in the appearance decoder to obtain the assignment matrix \(A^{}^{}\). The final assignment matrix \(A\) can be obtained by \(A=A^{}+A^{}\).

Figure 5: Architectures of the proposed Dual-branch Transformer Decoder.

### Object Association

During inference, we first employ the appearance model to generate candidates of each frame. If only one target with a high score is present in both the previous and current frames, this candidate is selected as the target, and the candidate matching model is omitted to reduce computational costs and accelerate inference. In contrast, when multiple candidates exist during tracking, the assignment matrix \(A\) is calculated using the proposed candidate matching model. After that, a threshold \(_{}\) is adopted on \(A\) to remove the ambiguous correspondence. Finally, we match the predicted boxes and the candidate boxes using the Hungarian algorithm . The candidates that do not match any tracklet will be assigned a new ID, and the tracklets that do not match any of the detections in the past consecutive \(\) frames will be terminated. \(\) is experimentally set to 15. Please refer to the supplementary material (Sec.B.3) for more details.

## 4 Experiment

### Implementation details

Our proposed CSAM is implemented in Python 3.8 using PyTorch 1.7.1. The CSAM training is conducted on two Nvidia RTX 3090 GPUs. For inference, we test our tracker on a single Nvidia RTX 3090 GPU. The search region is \(4^{2}\) times the target object area and resized to a resolution of 256x256 pixels, whilst the template is \(2^{2}\) times the target object area and resized to 128 x 128 pixels.

**Architectures:** We instantiate two models of CSAM: CSAM-T and CSAM-B, by varying the backbone network in the appearance model, i.e., ViT-Tiny and ViT-Base. We initialize ViT-Tiny using the weights from DeiT-tiny, and the backbone weights ViT-B are initialized with corresponding MAE encoders. In the candidate matching network, both the proposed STTE and DBTD apply one individual layer. Please refer to the supplementary material (Sec.A.1 and Sec.B.1) for more details.

**Training:** The training of our CSAM comprises three parts. In the first part, the following three loss functions are adopted: the focal loss for classification, and L1 loss and GIOU loss for bounding box regression. We employ the same training setting as that in OSTrack to train an RGB tracker. Secondly, we employ the same training setting as that in HRCEUTrack to train the

    &  &  &  &  &  \\  & & &  &  &  &  &  &  &  &  \\  STNet & CVPR22 & - & Event & 58.5 & 89.6 & 49.2 & - & 35.2 & - & - & - \\ MonTrack & NeurIPS22 & - & Event & 63.3 & 90.7 & - & - & - & - & - & - \\ DANet & TIP23 & Res18 & Event & 56.7 & 89.2 & 54.5 & - & 39.8 & - & - & - \\ HDETrack & CVPR24 & ViT-B & Event & 59.8 & 92.2 & - & - & - & 59.0 & 59.0 & 52.3 \\  DiMP* & ICCV19 & Res50 & RGB-E & 57.1 & 85.1 & 67.0 & 58.1 & 47.8 & 67.1 & 65.9 & 58.9 \\ PrDiMP* & CVPR20 & Res50 & RGB-E & 55.2 & 86.8 & 65.3 & 57.7 & 47.6 & 65.0 & 64.0 & 57.9 \\ SiamRCNN* & CVPR20 & Res101 & RGB-E & - & - & 68.0 & 62.6 & 52.7 & 67.5 & 66.3 & 60.9 \\ TDiMP* & CVPR21 & Res50 & RGB-E & 60.3 & 91.2 & - & - & - & 66.9 & 65.8 & 60.1 \\ TransT* & CVPR21 & Res50 & RGB-E & 63.9 & 93.0 & - & - & - & 67.9 & 66.6 & 60.5 \\ TQMP* & CVPR22 & Res101 & RGB-E & 61.8 & 91.1 & - & - & - & 67.2 & 66.0 & 59.9 \\ FENet & ICCV21 & Res18 & RGB-E & 63.1 & 91.8 & - & - & - & - & - & - \\ CEUTrack & ArXiV22 & ViT-B & RGB-E & 55.6 & 84.5 & 71.8 & 66.4 & 53.5 & 70.5 & 69.0 & 62.0 \\ HRCEUTrack-B & ICCV23 & ViT-B & RGB-E & - & - & - & - & - & 71.9 & 70.2 & 63.2 \\ HRCEUTrack-L4 & ICCV23 & ViT-T & RGB-E & - & - & - & - & - & 73.8 & 71.9 & 65.0 \\ HRMonTrack-T & ICCV23 & ViT-B & RGB-E & 66.3 & 95.3 & - & - & - & - & - \\ HRMonTrack-B & ICCV23 & ViT-T & RGB-E & 68.5 & 96.2 & - & - & - & - & - \\ AFNet & CVPR23 & Res18 & RGB-E & - & - & - & 67.8 & - & 59.2 \\ ViPT\(\) & CVPR23 & ViT-B & RGB-E & - & - & 76.6 & 73.0 & 60.8 & 73.9 & 72.2 & 65.7 \\ SDSTrack & CVPR24 & ViT-B & RGB-E & - & - & 79.3 & 75.5 & 62.6 & - & - & - \\ OneTrack & CVPR24 & ViT-B & RGB-E & - & - & 78.1 & 75.6 & 63.2 & - & - & - \\ SeqTrackv2-B256 & ArXiV24 & ViT-B & RGB-E & - & - & 79.9 & 75.6 & 63.7 & - & - \\ SeqTrackv2-L256f & ArXiV24 & ViT-L & RGB-E & - & - & 80.6 & 77.8 & 65.2 & - & - & - \\  CSAM-T\(\) & 2024 & ViT-T & RGB-E & 66.7 & 95.5 & 76.1 & 72.4 & 61.5 & 73.3 & 70.5 & 63.6 \\ CSAM-B\(\) & 2024 & ViT-B & RGB-E & 70.5 & 97.1 & 81.6 & 78.6 & 65.9 & 76.7 & 74.8 & 68.1 \\  

Table 1: Comparison with state-of-the-art trackers on COESOT , FE108  and VisEvent . The numbers with red and blue colors indicate the best and the second best results, respectively.

appearance model. In the third part, the parameters of the appearance model are fixed and other parameters in our proposed framework are set to be trainable. Since only the targets' locations are provided in the existing RGB-E tracking dataset, the partial supervision loss and self-supervised loss in KeepTrack  are employed to supervise the assignment matrix \(A\) generated by our proposed model. Please refer to the supplementary material (Sec.A.2 and Sec.B.2) for additional details about the implementation details.

### Evaluation datasets and metrics

**Dataset:** We evaluate the performance of our proposed CSAM on three large-scale RGB-E single object tracking datasets: VisEvent  FE108  and COESOT . These three datasets were captured using DAVIS346, with a spatial resolution of \(346 260\), a dynamic range of 120 dB and the minimum latency of 20 \(\)s. The COESOT  dataset comprises 578K RGB-E pairs, divided into 827 and 527 sequences for training and testing, respectively. These sequences were collected from both indoor and outdoor scenarios, covering a range of 90 classes and 17 challenging attributes. The FE108  dataset contains 108 RGB-E sequences, which capture 21 different types of objects. It is divided into 76 and 32 sequences for training and testing, respectively. VisEvent  dataset collects 820 RGB-E video pairs, divided into 500 and 320 sequences for training and testing, respectively. Following , after removing sequences that miss event data or have misaligned timestamps, the VisEvent dataset includes 377 sequences for training and 172 for testing.

**Metrics:** In FE108 , we use representative success rate (RSR) and representative precision rate (RPR) to evaluate all trackers. In COESOT  and VisEvent , we use success rate (SR), precision rate (PR) and normalized precision rate (NPR) for evaluation.

### Comparisons with State-of-the-art Methods

To show the effectiveness of the proposed method, we evaluate and compare our CSAM with several state-of-the-art trackers, including 4 Event trackers and 17 RGB-E trackers. As shown in Table. 1. * denotes that the RGB trackers are extended to RGB-E trackers via the early fusion approach. \(\) and * denotes that the model is pre-trained on RGB tracking datasets.

**Results on FE108:** As shown in Table. 1, our proposed CSAM-B outperforms other top-performing trackers, such as HRMonTrack-B , HDETrack  and TransT , with a clear margin, and achieves the best performance with an RSR score of 70.5%. and an RPR score of 97.1%. Even when compared to HRMonTrack-B, which has already obtained impressive tracking performance, our approach demonstrates notable improvements, with a 2.0% increase in RSR and a 0.9% increase in RPR. These comparisons fully demonstrate the effectiveness of tracking multiple candidates for robust tracking.

**Results on VisEvent:** From Table. 1, we find that our method sets a new state-of-the-art score on VisEvent. First, our proposed framework outperforms the Event trackers, e.g., STNet  and DANet , by a clear margin. Secondly, compared with appearance trackers SDSTrack  and ViPT , our model further improves the PR score by 4.1% and 1.6% in NPR scores, respectively. This enhancement is attributed to our model's comprehensive utilization of both appearance and motion information, enabling effective tracking of targets and distractors. Thirdly, our tracker surpasses the previous best tracker SeqTrackv2-L , which demonstrates that our method has a stronger capability in handling various challenges.

**Results on COESOT:** As shown in Table. 1, our proposed CSAM-B achieves a PR score of 76.7% and a SR score of 68.1%, surpassing recent state-of-the-art trackers. Compared with the most competitive RGB-E tracker ViPT , our CSAM-B achieves performance gains of 2.4% in NPR score. These results meet our expectation that the exploration of spatial-temporal relationships from the appearance cues as well as the motion cues can effectively match the candidates and tracklets for SOT task.

**Speed Analysis:** As shown in Table 2, despite increased computational costs and parameters for simultaneous target and distractor tracking, CSAM maintains real-time performance on the RTX 3090 GPU. It strikes a good balance between resource consumption and efficacy compared to competitors. Compared with the appearance tracker, our CSAM introduces limited computation costs, while significantly improving the tracking performance. Consequently, CSAM-B achieves an average running speed of 53 frames per second (FPS). We also notice CSAM's superior performance compared with the second best RGB-E tracker SeqTrackv2-L256.

### Ablation Study

To verify the effectiveness of our designed framework, we perform ablation analysis to evaluate different components in our method by using the COESOT test set . Please refer to the supplementary material (Sec.C) for more ablation experiments.

**Effectiveness of the proposed CEM:** To investigate the impact of our proposed CEM, several versions of our proposed method are provided, including 1: Removing the AFE and MFE sub-modules in CEM block. 2: Removing the AFE sub-module in CEM block. 3: Removing the MFE sub-module in CEM block. As can be seen in Table 3, the tracking performance degrades after removing AFE or MFE sub-modules, which demonstrates the necessity of embedding appearance and motion information from the classification scores and bounding box coordinates.

**Effectiveness of the proposed STTE:** To further verify the effectiveness of the proposed STTE, several variants are proposed, including 1: Removing the spatial encoder (SE) and temporal encoder (TE). 3: Replacing the spatial encoder by the original Transformer Encoder (OTE) block. 4: Replacing the spatial encoder by SuperGlue. 5: Removing the temporal encoder in STTE block. 6: Removing the spatial encoder in STTE block. As can be seen in Table 3, the tracking performance experiences a significant decline upon the removal of temporal encoder or spatial encoder, which confirms the necessity of spatial-temporal relationships in enhancing feature representations of the candidates. Furthermore, compared with several existing methods, e.g., OTE and SuperGlue, the proposed spatial encoder takes full advantage of the appearance information as well as motion information in constructing robust spatial correlations, thus achieving better results.

**Effectiveness of the proposed DBTD:** To further verify the effectiveness of the proposed DBTD, several variants are proposed, including 1: Replacing the spatial-temporal decoder (STD) by the original Transformer Decoder (OTD) block. 2: Removing the motion decoder (MD) in DBTD block. 3: Removing the spatial-temporal decoder in DBTD block. As can be seen in Table 3, not using spatial-temporal decoder or motion decoder substantially deteriorates the performance. This demonstrates that the exploration of both appearance and motion information is crucial for ensuring robust tracking.

   & ViPT & SeqTrackv2-R256 & SeqTrackv2-L256 & Appearance Tracker & CSAM-B \\  PR/SR & 76.6/60.8 & 79.9/63.7 & 80.6/65.2 & 75.3/60.6 & 81.6/65.9 \\ FPS & 75 & 40 & 15 & 75 & 53 \\ Model size (M) & 93.3 & 89 & 309 & 92.5 & 106.9 \\ FLOPS (G) & 52.1 & 66 & 232 & 62.7 & 83.2 \\  

Table 2: CSAM-B’s efficiency analysis on VisEvent with a fixed candidate count of 4 for FLOPS calculation.

   & AppModel & AFE & MFE & OTE & SuperGlue & SE & TE & OTD & STD & MD & SR & PR \\   & ✓ & & & & & & & & & & 65.5 & 74.8 \\  CEM & ✓ & & & & & & ✓ & & ✓ & ✓ & 67.1 & 75.9 \\  & ✓ & ✓ & & & & ✓ & ✓ & ✓ & ✓ & 67.3 & 76.4 \\  & ✓ & & ✓ & & & ✓ & ✓ & ✓ & ✓ & 67.5 & 76.3 \\  STTE & ✓ & ✓ & ✓ & & & & & ✓ & ✓ & 66.2 & 75.3 \\  & ✓ & ✓ & ✓ & ✓ & & & ✓ & ✓ & ✓ & 66.8 & 75.8 \\  & ✓ & ✓ & ✓ & & ✓ & & ✓ & ✓ & ✓ & 67.1 & 75.9 \\  & ✓ & ✓ & ✓ & & & & ✓ & ✓ & ✓ & 67.4 & 76.2 \\  & ✓ & ✓ & ✓ & & & & ✓ & ✓ & ✓ & 66.6 & 75.5 \\  DBTD & ✓ & ✓ & ✓ & & & ✓ & ✓ & ✓ & & 66.7 & 75.3 \\  & ✓ & ✓ & ✓ & & & ✓ & ✓ & & ✓ & 67.7 & 76.3 \\  & ✓ & ✓ & ✓ & & & ✓ & ✓ & & ✓ & 66.4 & 75.5 \\  CSAM & ✓ & ✓ & ✓ & & & ✓ & ✓ & ✓ & ✓ & 68.1 & 76.7 \\  

Table 3: Experiment results of different variants for Candidate Encoding Module (CEM), Spatial-Temporal Transformer Encoder (STTE) and Dual-Branch Transformer Decoder (DBTD). Here, ’AppModel’, ’SE’, ’TE’, ’STD’ and ’MD’ denote the appearance model, Spatial encoder, Temporal encoder, Spatial-temporal decoder and Motion decoder, respectively.

Visualizations of the MOT philosophyBased on the proposed candidate matching network, CSAM-B can track both the targets and distractors. As shown in Fig. 6, (\(\), \(\), \(\)) denote the target and the candidates in each frame. An object disappears from the scene if none of the current candidates are associated with it. The MOT philosophy in CSAM-B can effectively suppress the negative influence of distractors for tracking.

## 5 Conclusion

In this paper, a novel RGB-E tracking framework with MOT philosophy has been proposed in order to keep track of both targets and distractors to robustly track a single object. Specifically, a Spatial-Temporal Transformer Encoder is proposed to establish a rich temporal-spatial relationships by using appearance information in combination with motion information. Furthermore, by formulating the tracklets and candidates with the appearance features and motion embeddings, the affinities of the tracklets and candidates are explicitly modeled and leveraged. We conduct comprehensive experimental validation and analysis of our approach on three RGB-E object tracking benchmarks and produce new state-of-the-art results.

**Limitation:** The current method is dedicated to constructing an effective framework for RGB-E tracking with MOT philosophy, but it pays less effort to improve the efficiency of the supervision signals, which we will consider in future work.

## 6 Acknowledgments

This work was supported in part by the State Key Laboratory of Reliability and Intelligence of Electrical Equipment under Grant EERI KF2022005, in part by the Hebei University of Technology, in part by the National Natural Science Foundation of China under Grant 61803290 and Grant 61773301, in part by China Postdoctoral Science Foundation under Grant 2023M742745, and in part by the Natural Science Foundation of Shaanxi Province under Grant 2019JQ-312.