# Balance Risk and Reward: A Batched-Bandit Strategy for Automated Phased Release

Yufan Li\({}^{1}\),  Jialiang Mao\({}^{2}\),  Iavor Bojinov\({}^{3}\)

\({}^{1}\)Harvard University \({}^{2}\)LinkedIn Corporation \({}^{3}\)Harvard Business School

yufan_li@g.harvard.edu, jimao@linkedin.com, ibojinov@hbs.com

###### Abstract

Phased releases are a common strategy in the technology industry for gradually releasing new products or updates through a sequence of A/B tests in which the number of treated units gradually grows until full deployment or deprecation. Performing phased releases in a principled way requires selecting the proportion of units assigned to the new release in a way that balances the risk of an adverse effect with the need to iterate and learn from the experiment rapidly. In this paper, we formalize this problem and propose an algorithm that automatically determines the release percentage at each stage in the schedule, balancing the need to control risk while maximizing ramp-up speed. Our framework models the challenge as a constrained batched bandit problem that ensures that our pre-specified experimental budget is not depleted with high probability. Our proposed algorithm leverages an adaptive Bayesian approach in which the maximal number of units assigned to the treatment is determined by the posterior distribution, ensuring that the probability of depleting the remaining budget is low. Notably, our approach analytically solves the ramp sizes by inverting probability bounds, eliminating the need for challenging rare-event Monte Carlo simulation. It only requires computing means and variances of outcome subsets, making it highly efficient and parallelizable.

## 1 Introduction

Phased release, also known as staged rollout, is a widely used strategy in the technology industry that involves gradually releasing a new product or update to larger audiences over time . For example, Apple's App Store offers a phased release option where application updates are released over a 7-day period on a fixed schedule . Google Play Console provides a similar feature with more flexibility in the release schedule . Typically, the audiences are randomly selected at each stage from the set of all customers, and so phased releases can be thought of as a sequence of A/B tests (or randomized experiments) in which the proportion of units assigned to the treatment group changes until either the product or update is fully launched or deprecated . The process of combining phased releases with A/B tests is often called controlled rollout or iterative experiments and provides companies with an important mechanism to gather feedback on early product versions .

The key advantage of phased release is its ability to mitigate risks associated with launching a new product or update directly to all users. The potential impact of faulty features is limited by releasing the update first to a small percentage of the users (i.e., the treatment group). However, this risk-averse approach introduces an opportunity cost for slowly launching beneficial features, which quickly adds up for companies that release thousands of features yearly . Therefore, when designing a phased release schedule, it is important to determine the release percentage (known as ramp schedule) at each stage that balances the need to control risk while maximizing the speed of ramp-up. \({}^{*}\) This paper proposes an algorithm to address this challenge by automatically determining the releasepercentage for the next phase based on observations from previous stages. Specifically, we frame the challenge as a budget-constrained batched bandit problem. For each batch, we aim to determine the assignment probabilities of newly arrived users while keeping the probability of depleting a pre-specified experimental budget, where the experiment's cost is the cumulative treatment effect that are not directly observed. Formally, we derive recursive relations that decompose the risk of ruin (depleting the budget) of a phased release to the individual stages in the sense that the risk of ruin of the entire experiment is controlled if stage-wise ruin probabilities are controlled. Our algorithm is Bayesian in the sense that it learns from past observations by computing the posteriors of a conjugate Gaussian model and uses these parameters to infer the remaining budget and other cost-related quantities. However, the algorithm is robust to misspecifications and works well even when underlying outcomes are far from Gaussian by law of large number and central limit theorem; nevertheless, in Appendix E, we provide an extension to non-Gaussian outcomes. Finally, the next stage's assignment probabilities are derived from the posterior distribution and the stage-wise risk tolerances. Notably, our approach solves ramp sizes analytically from inverting the ruin probability upper bounds, avoiding challenging rare-event Monte-Carlo simulation for budget depletion events and data imputation procedures for unobserved counterfactual outcomes.

### Literature review

While many firms have guidelines on how to conduct a phased release process, these guidelines are often ad-hoc and qualitative, making it difficult to create executable ramp schedules. The SQR framework in  is the first attempt to address this problem by providing quantitative guidance. Our work differs significantly from SQR. Our algorithm adopts a fully Bayesian approach, enabling us to incorporate prior information on the risk of a feature in a probabilistic manner when initiating a ramp. Additionally, unlike SQR, our approach introduces a "shared budget" over the entire phased release, allowing the budget to be sequentially adjusted based on the observations from prior iterations. Finally, our algorithm is robust to modifications made to the treatment during experiments and different outcome models.

Our work is notably distinct from the risk-averse multiarmed bandit approaches considered in previous research [19; 14; 35; 28; 23; 10; 8]. In these approaches, the agent considers the expected variability in expected rewards to identify and avoid less predictable (and therefore risky) actions, without considering a budget constraint. A related literature focuses on batched and Bayesian variants of these methods in multi-stage clinical trials [4; 27; 21; 24; 2]. While this literature also aims to determine treatment assignment for each stage of the experiment, it differs from our setting in two key aspects: (i) the objective is to maximize treatment effect while balancing exploration of treatment arms, rather than rapidly ramping up experiments, and (ii) to the best of our knowledge, no clinical trials paper has addressed the imposition of a budget for potential adverse treatment effects. Hence, bandit approaches developed for clinical trials cannot be directly applied to our setting. To illustrate the difference, we present a numerical simulation of a Thompson sampling-based Bayesian bandit from  and highlight that budget spent and the aggressiveness of the ramp-up schedule depends on model tuning in a very unpredictable way, making the ramp-up schedule far from ideal. Another related literature is budgeted multiarmed bandits [32; 31; 9; 29; 12]. However, most budgeted bandit algorithms are developed for settings very different from ours and do not consider risk-of-ruin control or handle unobserved costs. Therefore, these algorithms cannot be directly applied to our specific scenario.

Notation.Let \(\) be the set of non-negative integers and \(_{+}:=\{0\}\). Let \(\), \(_{+}\) denote the set of real numbers and positive real numbers respectively. \([N]:=1,,N\) for \(N_{+}\). \(()\) is the generated \(\)-algebra. \(X\) if random variable \(X\) is measurable to \(\). \([X]\) denotes a random variable with distribution \((X)\) for a random variable \(X\) and \(\)-algebra \(\).

## 2 Risk-of-ruin-constrained experiment and strategy overview

### Risk-of-ruin-constrained experiment

Consider a scenario in which a single feature is released to a sequence of subpopulations \(_{t}\) consisting of \(N_{t}\) units at stages \(t=1,...,T\), where \(T\) is not necessarily fixed. At each stage, we randomly assign treatment to a group of units denoting the indexing set \(_{t}\), while the control group with indexing set \(_{t}\). The size of the treatment group at stage \(t\) is \(|_{t}|=m_{t}\) and the size of the control group is \(|_{t}|=N_{T}-m_{t}\).

Our paper adopts the Neyman-Rubin framework for causal inference [22; 25; 7], where the potential outcome of each unit \(i_{t}\) during experiment stage \(t\) under control and treatment are denoted by \(Y_{i,t}(0)\) and \(Y_{i,t}(1)\), respectively1. Appendix E provides the extension to multivariate outcomes. The treatment assignment of unit \(i_{t}\) at stage \(t\) is denoted by \(W_{i,t}\). Since each unit only receives a single treatment at each stage, we only observe \(Y_{i,t}(W_{i,t})\) during the experiment, not the counterfactual \(Y_{i,t}(1-W_{i,t})\) (we are explicitly assuming that there is full compliance).

Let \(_{t}=Y_{_{k},k}(1)_{k[t]}, (Y_{_{k},k}(0))_{k[t]},(W_{i,k})_{i ,k[t]}\) be the \(\)-algebra generated by the treatment assignment and the observed experiment outcome in the first \(t\) stages, with \(_{0}\) representing the trivial \(\)-algebra. In our setting, the experimenter aims to ramp up the experiment to the "max-power stage" (50% of the population placed in treatment) as quickly as possible while avoiding the risk of a large negative business impact (or cost). We define the cost of the experiment as the treatment effect on the treated. See generalized cost in Appendix E.

**Definition 2.1** (Experiment cost).: The cost of the experiment from stage \(t[T]\) is \(r_{t}:=_{i_{t}}Y_{i,t}(1)-Y_{i,t}(0)\), where \(r_{t}=0\) if \(_{t}=\). The cumulative cost is \(R_{t}:=_{k[t]}r_{k}\).

Throughout, \(r_{t}<0\) corresponds to a negative business impact; our goal is to control the experiment cost by setting a budget \(B<0\) and imposing the cost constraint \(R_{T}>B\). Since the outcomes are stochastic, we require this cost constraint to be satisfied with probability at least \(1-\) for some \([0,1)\) set before the experiment. Our goal is then to adaptively determine the size of the treatment group based on the observed data while satisfying our risk constraint. We refer to such an experiment as a risk-of-ruin-constrained (RRC) experiment.

**Definition 2.2** (RRC experiment).: Fix any \(B<0,[0,1)\). A \((,B)\)-RRC experiment running for \(T\) stages selects the size of \(_{t}\) before \(t\)-th stage of the experiment such that \((R_{T}>B) 1-\).

### Strategy overview

Our goal is to determine the number of users to assign to the treatment \(m_{t}[0,N_{t}/2]\), such that \(_{i=1}^{T}m_{t}\) is maximized while ensuring that the experiment is \((B,)\)-RRC.

Our strategy is to decompose the overall constraint (_i.e._, that the experiment is \((B,)\)-RRC) into a sequence of stage-wise adaptive constraints using Theorem 3.1, below. We then sequentially maximize \(m_{t}\) under the stage-wise constraint for the \(t\)-th stage. Under the Gaussian model, the stage-wise constraints can be directly solved using a simple quadratic equation to obtain the maximum \(m_{t}\).

Although our algorithm solves a relaxed version of the original optimization problem, our solution has crucial practical implications. Theorem 3.1 decomposes the \((B,)\)-RRC constraints in an adaptive fashion, _i.e._, if the feature turns out to be safe, the stage-wise constraints will relax in response, and the experiment will ramp up quickly.

## 3 Model and the algorithm

### Decompose the risk of ruin

Our experimental design is based on the following theorem, which identifies a sequence of sufficient conditions for a sequential experiment to be \((,B)\)-RRC. We defer its proof to Appendix A.

**Theorem 3.1**.: _Fix \(B<0\) and \([0,1)\). For any stopping time \(T 1\), let \((b_{t})_{t[T]}\) be a budget sequence, such that \(b_{t}}{{}}B, t[T]\), and \((_{t})_{t[T]}\) be a risk tolerance sequence, such that \(_{t}[0,1), t[T]\) and \(1-_{t=1}^{T}(1-_{t})}{{}}\). Then, if \((_{t})_{t[T]}\) is chosen such that for \(t=1\),_

\[r_{1} b_{1}}{{ }}_{1} \]

_and for any \(t=2,...,T\), almost surely,_

\[_{t}=,&(R_{t-1}>B _{t-1})=0\\ (R_{t} b_{t} R_{t-1}>B,_{t-1})}{{}}_{t},& \]

_then \((R_{T}>B) 1-\). This inequality is tight when \((i)\)-\((iv)\) are all equalities and \(r_{t} 0, t[T]\) almost surely. Furthermore, if we set \(_{t}\), (1), (2) always hold._

Recall, \(B<0\) denotes the budget and \([0,1)\) is our risk tolerance that controls the risk of ruin (_i.e.,_ the probability of exceeding the budget); both need to be fixed a _priori_. In general, smaller \(\) leads to more conservative experimentation and slower releases. The sequence \((b_{t})_{t 1}\) "rations" the budget: setting \(b_{t}<B\) at stage \(t\) reserves \(B-b_{t}\) budget for later stages, which may be beneficial when the released feature is expected to undergo modifications during the experiment . To quickly scale the experiment, we can set \(b_{t}=B, t\). The sequence \((_{t})_{t 1}\) distributes the overall tolerance \(\) to individual stages by \(=1-_{i-t}^{T}(1-_{t})\) allowing us to customize the tolerance for individual stages. If \(T\) is fixed a priori, we can uniformly distribute the tolerance by setting \(_{t}=1-(1-)^{1/T}, t[T]\).

Theorem 3.1 breaks the risk constraint \((R_{T} B)\) into stage-wise constraints in the form of (1),(2). The idea is to control current-stage cumulative experiment cost \(R_{t}\) given past observations \(_{t}\) and determine the treatment assignment based on posterior inference of the remaining budget. In our setting, the goal is to maximize \(m_{t}=|_{t}|\) subject to (1),(2). Note that the first line in (2) stops the experiment when the model estimates the budget is exhausted, while the second line sets the stage cost \(r_{t}\) below the remaining budget \(b_{t}-R_{t-1}\) with high probability. We require assumptions on the data-generating model to derive an explicit algorithm, which we present in the next sections.

Generally, the total number of stages \(T\), stage-wise budget and tolerance \((b_{t})_{t 1}\), \((_{t})_{t 1}\) can be determined dynamically during the process. That is, we can define \(_{t}\) and \(b_{t}\) just before stage \(t\) as long as \(_{r=1}^{t}(1-_{t}) 1-\) and \(b_{t} B\). If we plan to terminate the experiment after stage \(t\), we can define \(_{t}\) such that \(_{r=1}^{t}(1-_{r})=1-\) and \(b_{t} B\) is attained in which case \(T=t\). If is also possible to have \(T=+\) and \(_{t=1}^{}(1-_{t})=1-\). For example, choosing \(_{t}=(_{}/t)^{2}\) where \(_{}\) is the unique solution of \((_{})=1-\) on \(\) (cf. [13, Eq. (1)]) satisfy our condition.

Note that the decomposition scheme in Theorem 3.1 is formulated such that \((R_{T} B)\) is tight if inequalities (i)--(iv) are tight. Practically, this means that the ramp-up schedules obtained through this approach are typically not overly conservative, unlike approaches that leverage a union-bound for risk decomposition. Finally, Theorem 3.1 holds for a general definition of the cost \(r_{t}:=r_{t}((Y_{i,t})_{i_{t}},_{t})\) such that \(r_{t}=0\) if \(_{t}=\), making it useful in other budgeted online problems beyond our setting.

### Gaussian outcome model

For this subsection, make the following model assumptions on the outcomes distribution; appendix E provides the extension to general outcome models.

**Definition 3.1** (Conjugate Gaussian outcomes).: Let the unknown model parameters \(_{}(0),_{}(1)\) satisfy the prior \(_{}(w) N_{0}(w),_{0}{(w )}^{2}\) for \(w=0,1\) independently, where \(_{0}(0),_{0}(1),_{0}{(0)}^{2},_{0}{ (1)}^{2}_{+}\) are hyperparameters. The experiment outcome of unit \(i\) at stage \(t\) are distributed independently and identically as

\[(Y_{i,t}(0)\\ Y_{i,t}(1))}}{{}}N ((_{}(0)\\ _{}(1)),( ^{2}&0\\ 0&^{2})) \]

where \(^{2},^{2}_{+}\) are hyperparameters.

The unknown parameters \(_{}(0)\) and \(_{}(1)\) represent the intrinsic quality of the feature before and after the update, as measured by a specific metric. If \(_{}(1)-_{}(0)<0\), the feature update is likely to have a negative business impact, _i.e._, \(Y_{i,t}(1)-Y_{i,t}(0)<0\).

To derive the posterior distribution we need the following statistics. For \(t=1,w=0,1\)

\[& S_{0}^{}(w)=s_{0}^{}(w)=S_{0}^{ }(w)=s_{0}^{}(w)=M_{0}(w)=0\\ &_{p,t=1}(w)=_{0}(w),_{p,t=1}(w)^{2}=_{0}( w)^{2}. \]

and for \(t 2,w=0,1\)

\[& s_{t}^{}(w):=_{i _{t}}Y_{i,t}(w),\;S_{t}^{}(w):=_{r[t]}s_{r}^{}(w),\;s_{t}^{}(w):=_{i_{t}}Y_{i,t}(w),\;S_{t}^{ }(w):=_{r[t]}s_{r}^{}(w) \] \[_{p,t}(w):=(w)^{2}}+ (w)}{(w)^{2}}}(w)}{_{0}(w)^{2}}+(w=0)S_{t-1}^{}(w)+(w=1)S_{t-1}^{}(w )}{(w)^{2}}\] (5b) \[_{p,t}(w)^{2}=_{0}(w)^{-2}+M_{t-1}^{(w)}(w)^{-2} ^{-1}\,, \]

where \(M_{t}^{(1)}:=_{r[t]}m_{r}\) and \(M_{t}^{(0)}:=_{r[t]}N_{r}-m_{r}\) are the cumulative number of users in the treatment and control groups up to stage \(t\), respectively. In (5a), \(S_{t}^{}(w)\) and \(S_{t}^{}(w)\) represent the cumulative sum of outcomes for \(w=0,1\) in the treatment and control groups up to stage \(t\), while \(s_{t}^{}(w)\) and \(s_{t}^{}(w)\) represent the sum of outcomes at stage \(t\). In equation (5b), \(_{p,t}(w)\) represents the posterior mean of \(_{}(w)\), while in equation (5c), \(_{p,t}(w)^{2}\) represents the posterior variance of \(_{}(w)\), for \(w=0,1\). When lacking prior information, we suggest using a non-informative priors by setting \(_{0}(0)=_{0}(1)=0\) and \(_{0}(0)^{2},_{0}(1)^{2}\) sufficiently large.

The model parameters \((0)^{2}\) and \((1)^{2}\) at stage \(t 2\) can be estimated using unbiased and consistent estimators. For \(w=0,1\) let

\[(w)^{2}_{i _{r}}Y_{i,r}(w)-(w)}(w=0 )S_{t-1}^{}(w)+(w=1)S_{t-1}^{}(w) ^{2}}{M_{t-1}(w)-1}. \]

For \(t=1\), some prior estimate can be used, either from a similar experiment or from a small-scale pretrial run.

### An algorithm for the sample size in an RRC experiment

We now derive an explicit algorithm from Theorem 3.1 that to outputs \((m_{t})_{t 1}\), the treatment group size at stage \(t\), such that, the experiment is \((,B)\)-RRC. Recall that for an experiment to be \((,B)\)-RRC, it suffices that (1), (2) holds for each \(t 1\). Under Definition 3.1, we have that (i) \((Y_{i,t})_{i,t}\) are exchangeable random variables (ii) for any \(t 2\), \((S_{t-1}^{}(0)<S_{t-1}^{}(1)-B|_{t- 1})>0\) almost surely for any choice of \(m_{[t-1]}\). Combining these observations, we get that (1), (2) hold if for each \(t 1\),

\[s_{t}^{}(1)-S_{t}^{}(0) b_{t}-S_{t- 1}^{}(1)S_{t-1}^{}(0)<S_{t-1}^{}(1)-B,_{t-1}_{t}. \]

Lemma 3.2 provides an upper bound of the left hand size of (7); the proof is in Appendix B.

**Lemma 3.2** (Stochastic domination).: _Assume the outcomes \((Y_{i,t}(0),Y_{i,t}(1))_{i,t}\) are generated as in Definition 3.1. For any \( 1\), almost surely,_

\[&s_{t}^{}(1)-S_{t}^{ }(0) b_{t}-S_{t-1}^{}(1)S_{t-1}^{} (0)<S_{t-1}^{}(1)-B,_{t-1}\\ &s_{t}^{}(1)-S_{t}^{ }(0) b_{t}-S_{t-1}^{}(1)_{t-1} . \]Using Lemma 3.2, for (1) and (2) to hold, it suffices to choose any \(m_{t}\) such that

\[s_{t}^{}(1)-S_{t}^{}(0) b_{t}-S_{t-1 }^{}(1)_{t-1}_{t} \]

and set \(m_{t}=0\) if such \(m_{t}\) does not exist. From posterior-predictive formulas for the conjugate Gaussian model in Definition 3.1 (see (15e), (15f) in Appendix C), we have \(s_{t}^{}(1)-S_{t}^{}(0)_{t-1 } N(_{t}(m_{t}),_{t}^{2}(m_{t}))\), where

\[_{t}(m) :=_{p,t}(1) m-_{p,t}(0)m+M_{t-1}^{(1)}  \] \[_{t}^{2}(m) :=m^{2}_{p,t}(1)^{2}+m(1)^{2}+m+M_{t -1}^{(1)}^{2}_{p,t}(0)^{2}+m+M_{t-1}^{(1)} (0)^{2}. \]

Combining the above with (9) yields the following Lemma.

**Lemma 3.3**.: _Assume the outcomes \((Y_{i,t}(0),Y_{i,t}(1))_{i,t}\) are generated as in Definition 3.1. For each \(t 1\), the inequality (9) holds if and only if_

\[-S_{t-1}^{}(1)-_{t}(m_{t})}{ {}_{t}(m_{t})} q_{t}:=^{-1}(_{t}) \]

_where \(^{-1}\) denotes inverse CDF of the standard normal distribution._

Replace the inequality in (11) with equality and square both sides gives us the quadratic equation \(A_{t} m_{t}^{2}+B_{t} m_{t}+C_{t}=0\) where

\[A_{t} :=q_{t}^{2}_{p,t}(1)^{2}+_{p,t}(0)^{2}- (_{p,t}(1)-_{p,t}(0))^{2} \] \[B_{t} :=q_{t}^{2}(1)^{2}+(0)^{2}+2_{p,t}(0)^{2 }M_{t-1}^{(1)}\] \[+2(b_{t}-S_{t-1}^{}(1)+_{p,t}(0)M_{ t-1}^{(1)})(_{p,t}(1)-_{p,t}(0))\] \[C_{t} :=q_{t}^{2}_{p,t}(0)^{2}(M_{t-1}^{(1)})^{2}+q_{t} ^{2}(0)^{2}M_{t-1}^{(1)}-b_{t}-S_{t-1}^{}(1)+_{p,t} (0)M_{t-1}^{(1)}^{2}\,.\]

Algorithm 1 finds the floor transform of the solutions of this equation and chooses \(m_{t}\) as the largest, positive integer that satisfies (11). If such a solution cannot be found, then either we do not have enough budget or the cost of the experiment is negligible (this accrues when \(_{}(1)-_{}(0) 0\)), and the inequality in (11) will be strict for any choice of \(m_{t}\). In the former case, Algorithm 1 sets \(m_{t}=0\); in the latter case, it sets \(m_{t}= N_{t}/2\). Therefore, by construction, the sequence \((m_{t})_{t 1}\) output by Algorithm 1 guarantees that (1) and (2) hold, thereby defining a \((,B)\)-RRC experiment. Note that this approach directly solves for \(m_{t}\) from the quadratic equation \(A_{t}m_{t}^{2}+B_{t}m_{t}+C_{t}=0\), bypassing the challenging task of estimating tail probabilities for potential choices of \(m_{t}\) through Monte-Carlo methods. By Algorithm 1, we can also conduct posterior inference on treatment effect after stage \(t\) using \(_{p,t+1}(w),_{p,t+1}(w),w=0,1\) and estimate the remaining budget by \(B-_{r=1}^{t}m_{r}(_{p,r+1}(1)-_{p,r+1}(0))\).

**Theorem 3.2**.: _Assume the outcomes \((Y_{i,t}(0),Y_{i,t}(1))_{i,t}\) are generated as in Definition 3.1. The experiment by Algorithm 1 is \((,B)\)-RRC._

Even though our algorithms is derived from the conjugate Gaussian model we have found that it remains effective for broader outcome models. This is because the learning occurs essentially through computation of the first and second moments of past outcomes as in (5), (6), and Algorithm 1 tends to be successful so long as they are predictive of the outcome moments in future stages. The risk of ruin control remains approximately valid due to the law of large numbers and standard central limit theorem under specific conditions; see next section.

Finally, in Algorithm 1, the assumption is made that the population size \(N_{t}\) for the next stage is known to ensure that \(m_{t}\) does not exceed \( N_{t}/2\). In practice, we recommend estimating \(N_{t}\) and using the model output \(m_{t}\) to calculate the assignment probability \(p_{t}=m_{t}/N_{t}\), the allows the experimenter to assign each incoming user to the treatment group with a probability of \(p_{t}\).

```
0:\(B<0\), \([0,1)\)
1:Initialize\(t 1\), \(_{r=1}^{0}(1-_{r}) 1\)
2:while\(_{r=1}^{t-1}(1-_{r})>1-\)do
3: choose\(_{t}[0,^{t-1}(1-_{r})} -1],b_{t} B\)
4: estimate data variance \((w)^{2},w=0,1\) (if unknown) using (6)
5: compute\(_{p,t}(w),_{p,t}^{2}(w),w=0,1\) by (5b), (5c), (4) and \(q_{t},A_{t},B_{t},C_{t}\) by (11), (12)
6:if\(-S_{t-1}^{T}(1)-_{t}( N_{t}/2 )}{^{2}( N_{t}/2)}}  q_{t}\)then\(m_{t} N_{t}/2\)
7:elseif\(B_{t}^{2}-4A_{t}C_{t}<0\)then\(m_{t} 0\)
8:else
9:\(_{t}\{+^{2}-4A_{t }C_{t}}}{2A_{t}},-^{2}-4A_{t}C _{t}}}{2A_{t}}\}\)
10:\(_{t}\{m_{t}[0,}{2} ]:-S_{t-1}^{T}(1)-_{t}(m)}{_{t}(m)}  q_{t}\}\)
11:if\(_{t}\)then
12:\(m_{t}_{t}\)
13:else
14:\(m_{t} 0\)
15:endif
16:endif
17:Output\(m_{t}\), conduct stage \(t\)-experiment and observe outcomes \(s_{t}^{}(1),s_{t}^{}(0)\)
18:compute\(M_{t}(0),M_{t}(1),S_{t}^{}(1),S_{t}^{}(0)\) by (5)
19:update\(t t+1\)
20:endwhile
```

**Algorithm 1** Output ramp size adaptively

### Robustness to non-identically distributed and non-Gaussian outcomes

We now derive conditions for the validity of Algorithm 1 under the assumption that experiment outcomes are independent.

**Definition 3.4**.: The experiment outcomes \((Y_{i,t}(0),Y_{i,t}(1))\) are independent across different units \(i\) and experiment stage \(t\).

Definition 3.4 allows \(Y_{i,t}(0)\) and \(Y_{i,t}(1)\) to be dependent and/or discrete-valued (_e.g._, binary outcomes). In addition, the outcome distribution \((Y_{i,t}(0),Y_{i,t}(1))\) can differ across \(i,t\); for instance, treatment effect may be non-stationary. The validity of Algorithm 1 under Definition 3.4 is now given in Theorem 3.3; we defer the proof to Appendix D.

**Theorem 3.3**.: _Assume the outcomes \((Y_{i,t}(0),Y_{i,t}(1))_{i,t}\) satisfy Definition 3.4. The experiment by Algorithm 1 is \((,B)\)-RRC if, for each stage \(t 1\) where \(m_{t} 0\), the following conditions hold_

\[(^{}(1)-S_{t}^{} (0)-_{t}}{_{t}} z_{t} _{t-1})(z_{t}), z_{t}_{t}- _{t}}{_{t}-_{t}}, \]

_where_

\[_{t}:=[s_{t}^{}(1)-S_{t}^{ }(0)_{t-1}],\;_{t}^{2 }:=[s_{t}^{}(1)-S_{t}^{}( 0)_{t-1}],\;z_{t}:=-S_{t-1}^{ }(1)-_{t}}{_{t}},\]

_and \(_{t}:=_{t}(m_{t}),_{t}:=_{t} (m_{t})\) are defined by (10a), (10b), and \(s_{t}^{}(1),S_{t}^{}(0)\) are defined in (5)._

We expect the first condition in (13) to hold as a consequence of central limit theorem for independent but non-identical random variables. Suppose \(_{t} 0.5, t\) (_i.e., \(z_{t} 0\)_). By law of large number for independent but non-identical random variables, the second condition in (_13_) holds if (i) we have chosen prior and model parameters conservatively such that

\[_{0}(1)-_{0}(0)}_{i_{1}} (Y_{i,1}(1)-Y_{i,1}(0))\] \[(0)^{2}+(1)^{2}+m_{t}(_{0}(1)^{2}+ _{0}(0)^{2})}_{i_{t}}(Y_{i,t}(1)-Y_{i,t}(0))\]and (ii) if the treatment effects increase or stay roughly constant throughout the experiments

\[}_{i_{t}}(Y_{i,t}(1)-Y_{i,t}(0) )^{(1)}}_{r[t-1]}_{i_{r}} [Y_{i,t}(1)-Y_{i,t}(0)]\]

and our variance estimates \(^{2},^{2}\) are accurate or conservative in the sense that

\[^{2}^{(1)}}_{r[t-1]}_{i_{r}}[Y_{i,t}(0)],\ \ ^{2}+^{2} }_{i_{t}}(Y_{i,t}(1)-Y_{i,t}(0 )).\]

In summary, under Definition 3.4, the validity of Algorithm 1 depends on the accuracy and conservatism of the model's estimates based on past stages for the true treatment effect and volatility in the next stage. The algorithm's effectiveness may be compromised when there is a sudden decrease in treatment effect or a surge in outcome volatility in the next stage; see discussion in Appendix D.

## 4 Numerical and empirical experiments

Simulated ramp scheduleWe now examine the following three experimental scenarios, for each \((Y_{i,t}(1),Y_{i,t}(0))_{i,t}\) are iid sampled from (3) with variance \(^{2}=^{2}=10\) and means given below:

* PTE: Positive treatment effect, with \(_{}(0)=0\), \(_{}(1)=1\);
* NTE: Negative treatment effect, with \(_{}(0)=1\) and \(_{}(1)=0\);
* PNTE: Negative to positive treatment effect, with \(_{}(1)(t)=(-2+0.5(t-1),2)\).

For each scenario, we set \(T=10\) with \(N_{t}=500, t\) and we choose non-informative prior \(_{0}(w)=0,_{0}(w)^{2}=100,w=0,1\). We assume model variance is known; however, using (6) to estimate the variance gives similar results. We repeat each scenario 500 times. Figure 1 (a)--(c) show median, 25% and 75% quantile of the simulated ramp schedules \((m_{t})_{t[T]}\) and (g),(h) show the budget surpluses \(_{r[t]}_{i_{r}}(Y_{i,t}(1)-Y_{i,t}(0))-B\) produced given different choices of \(B,,(b_{t})_{t[T]},()_{t[T]}\).

Across the various scenarios, our model gives a reasonable ramp schedule. Large \(B,\) typically leads to more treated units and faster ramp-up. For the NPTE scenario, inadequate budget and low ruin tolerance can result in a failure to ramp up to 50% (\(m_{t}=250\)). We also found that reserving the budget for later stages by decreasing \(b_{t}\) or \(_{t}\) in the initial stages leads to a faster ramp-up because more budget is available to support a swift increase when the treatment effect turns positive. This suggests that the experimenter may want to consider reserving some budget for later stages if the treatment effect \(_{p,t}(1)-_{p,t}(0)\) has not stabilized.

For PNTE scenario, we compare our method to a Thompson-sampling bandit with tuning parameters \(c\) and prior \(_{0}(1)=-2,_{0}(0)=0,_{0}{(0)}^{2}=_{0}{(1)}^{2}=0.05\) (see  and Appendix G for details). The prior is chosen so that the bandit can initialize conservatively depending on \(c\). It can be seen in Figure 1 (e),(i) that the ramp schedule generated is rather sub-optimal and does not respect the budget. It also follows a rigid pattern where with small \(c\), the ramp-up initializes too aggressively, and for large \(c\), the ramp-up proceeds too conservatively. These results demonstrate that our approach significantly outperforms the main existing alternative.

Semi-real LinkedIn ramp schedule comparisonAppendix F gives group-level statistics from a 6-stage phased release run at LinkedIn. Due to privacy constraints, the individual-level data is not available and is simulated from (4) using stage-wise \(_{}(w),^{2},w=0,1\) (both unobserved). The ramp-up schedules for different tuning parameters are shown in Figure 1,(d). It is noteworthy that the ramp-up schedule employed by LinkedIn's data scientists, which was chosen without considering a specific budget, is roughly consistent with the budget-rationing schedule denoted as "ration-budget LinkedIn" in the caption. Our results suggest that deducing the budget and risk tolerance associated with an experiment retroactively using our method is possible. We also run the experiment using Thompson sampling Bayesian bandit with the same prior as for NPTE above. In Figure 1(f), we again observe the rigidity issue: with small \(c\), the ramp-up initializes too aggressively, and for large \(c\), the ramp-up proceeds too conservatively.

Figure 1: Line plots (a)—(i) show the median, 25%, 75% quantiles of either the treatment group sizes or the budget surplus for the 500 simulations of the different experiment setups (PTE, NTE, NPTE, Linkedln) using our model and Thompson-sampling Bayesian bandit. Under the legends “\((B,)\)”, we set \(b_{t}=B,_{t}=1-(1-)^{1/T}, t\). We also use (i) “ration budget” to denote \((B,)=(-500,0.01),b_{t}=-400, t 5,b_{t}=-500, t>5\) and \(_{t}=1-(1-)^{1/T}, t\); (ii) “ration tolerance” to denote \((B,)=(-500,0.01),b_{t}=-500, t\) and \(_{t}=0.0001, t 5,_{t}=0.0019, t>5\) (iii) “actual Linkedln” to denote the actual ramp up schedule used by Linkedln data scientists (iv) “ration-budget Linkedln” to denote \((B,)=(-1500,0.01),b_{t}=-400,t 4,b_{t}=-1500,t>4,_{t}=1-(1- )^{1/T}\). Particularly, (e), (f), (i) are results using Thompson-sampling Bayesian bandit with different values of tuning parameter \(c\), denoted by “TOM” in sub-caption, for experiment NPTE, Linkedln. We set \(B=-500\) to produce (h), although the model is not budget-aware. Histograms (j)—(n) show distribution of the budget used over 5000 simulations. The red dashed line marks the budget available \(B=-500\). “\(x\)% \(/\)\(5\%\)” in the sub-captions denotes that the actual risk of ruin is \(x\%\) and the ruin tolerance is \(=5\%\).

Budget-spent distributionTo explore how our algorithms controls the risk of ruin and budget spending, we simulate following experiments for 5,000 times and plot distribution of the budget spent \(_{r[t]}_{i_{r}}(Y_{i,t}(1)-Y_{i,t}(0))\) in Figure 1 (j)--(n): \((Y_{i,t}(1),Y_{i,t}(0))_{i,t}\) are sampled iid from (3) with \(_{}(0)=0,_{}(1)=1,(0)^ {2}=(1)^{2}=10\); (ii) corr: same as norm except that for each \(i,t\), \(Y_{i,t}(1)\) is correlated with \(Y_{i,t}(0)\) with correlation coefficient 0.8 (iii)) bern: \(Y_{i,t}(0)}{}6.4(p=0.5786)\) and \(Y_{i,t}(1)}{}6.4(p=0.4224)\); (iv) fat: \(Y_{i,t}(0)}{}1+_{4}\), and \(Y_{i,t}(1)}{}_{4}\)2 (v) detc: same as norm except \(_{}(1)(t)=-(t-1)\). Note that (iii), (iv) is configured so that \([Y_{i,t}(1)-Y_{i,t}(0)]=1,(Y_{i,t}(0))=(Y_{i,t}(1))=10\). For all the above experiments, we run \(T=10\) stages with \(N_{t}=500, t\) and we use non-informative prior \(_{0}(w)=0,_{0}(w)^{2}=100,w=0,1\).

As shown in Figure 1 (j)--(m) the model successfully controls risk of ruin for (i)--(iv). The actual ruin risk is at a reasonable level (\( 1.2\)%) compared to the ruin tolerance given (\(5\)%). Note that the actual ruin risk are close for different outcome distribution. This is a consequence of central limit theorem and law of large numbers as discussed in Section 3.4. The model fails to control risk of ruin for (v) as expected since the treatment effect keeps decreasing and the model assigns treatment based on past stages which leads to higher-than-expected costs (cf. Section 3.4).