# MSPE: Multi-Scale Patch Embedding Prompts Vision Transformers to Any Resolution

Wenzhuo Liu\({}^{1,2}\), Fei Zhu\({}^{3}\), Shijie Ma\({}^{1,2}\), Cheng-Lin Liu\({}^{1,2}\)

\({}^{1}\)School of Artificial Intelligence, UCAS

\({}^{2}\)State Key Laboratory of Multimodal Artificial Intelligence Systems, CASIA

\({}^{3}\)Centre for Artificial Intelligence and Robotics, HKISI-CAS

{liuwenzhuo2020, zhufei2018, mashijie2021}@ia.ac.cn, liucl@nlpr.ia.ac.cn

Corresponding author.

###### Abstract

Although Vision Transformers (ViTs) have recently advanced computer vision tasks significantly, an important real-world problem was overlooked: adapting to variable input resolutions. Typically, images are resized to a fixed resolution, such as 224x224, for efficiency during training and inference. However, uniform input size conflicts with real-world scenarios where images naturally vary in resolution. Modifying the preset resolution of a model may severely degrade the performance. In this work, we propose to enhance the model adaptability to resolution variation by optimizing the patch embedding. The proposed method, called Multi-Scale Patch Embedding (MSPE), substitutes the standard patch embedding with multiple variable-sized patch kernels and selects the best parameters for different resolutions, eliminating the need to resize the original image. Our method does not require high-cost training or modifications to other parts, making it easy to apply to most ViT models. Experiments in image classification, segmentation, and detection tasks demonstrate the effectiveness of MSPE, yielding superior performance on low-resolution inputs and performing comparably on high-resolution inputs with existing methods. Code is available at https://github.com/SmallPigPeppa/MSPE.

## 1 Introduction

Vision Transformers (ViTs) [1; 2; 3; 4] have achieved significant success in various computer vision tasks, becoming a viable alternative to traditional convolutional neural networks [5; 6; 7; 8]. ViT divides an image into multiple patches, converts these patches into tokens via the patch embedding layer, and feeds them into the Transformer model [9; 10]. The token representations are usually obtained using a convolutional neural network (CNN). Early CNN architectures like AlexNet  were designed for fixed-size images (e.g., 224x224). For easy comparison, this setting has been maintained by subsequent image recognition models, including ViT . For fitting neural network input layer size, ViTs typically resize the input image to a fixed resolution 2 and divide it into a specific number of patches [12; 13; 14]. This practice restricts ViT models to processing single-resolution inputs. However, fixed input sizes conflict with real-world scenarios where image resolution varies due to camera devices/parameters, object size, and distance. This discrepancy can significantly degrade ViT's performance on different-resolution images. Changing the preset size requires retraining, and every resolution necessitates an individual model. Therefore, a natural question arises: _Is it possible for a single ViT model to process different resolutions directly?_A few recent methods have been proposed for this problem: FlexiViT  uses a novel resizing method allowing flexible patch size and token length in ViT. ResFormer  enhances resolution adaptability through multi-resolution training and a global-local positional embedding strategy. NaViT  leverages example packing  to adjust token lengths during training, boosting efficiency and performance. However, these methods have two limitations. **First**, some of them are incompatible with existing transformer models, preventing the effective use of pre-trained ViTs, and thus lead to high re-training costs when applied. For example, models like NaViT and ResFormer require training the entire model to achieve multi-resolution performance. **Second**, their multi-resolution performance is insufficient. For example, FlexiViT demonstrates only slight performance improvement when the input images are smaller than the preset size.

We intuitively believe that an ideal solution should be compatible with most existing ViT models and perform well at any size and aspect ratio. To this end, we propose a method called **M**ulti-**S**cale **P**atch **E**mbedding (MSPE). It replaces the patch embedding layer in standard ViT models without altering the other parts of the model. This design makes MSPE compatible with most ViT models and allows for low-cost, direct application. Specifically, our method uses a set of learnable adaptive convolution kernels instead of fixed ones, which can automatically adjust the size and aspect ratio of the kernels based on the input resolution. It directly converts images into patch embeddings without modifying the input size or aspect ratio. The results in Figure 1 show that this simple method significantly improves performance, for example, increasing ImageNet-1K accuracy up to 47.9% across various resolutions. Our contributions are as follows:

* By analyzing resolution adaptability in ViT models, we identify the patch embedding layer as the crucial component and provide a low-cost solution.
* We propose Multi-Scale Patch Embedding (MSPE), which enhances ViT models by substituting the standard patch embedding layer with learnable, adaptive convolution kernels, enabling ViTs to be applied on any input resolution.
* Experiments demonstrate that with minimal training (only five epochs), MSPE significantly enhances performance across different resolutions on classification, segmentation, and detection tasks.

## 2 Preliminaries and Analysis

### Vision Transformer Models

Vision Transformers (ViTs) leverage the capabilities of the transformer model [20; 21], initially developed for natural language processing, to address vision tasks [22; 23; 24]. ViTs primarily consist of the patch embedding layer and Transformer encoder.

**Patch Embedding** converts an image \(^{h w c}\) in the input space \(\) into a sequence of tokens \(\{_{i}\}_{i=1}^{N}\), where \(_{i}^{d}\) is a vector in patchification space \(\). This transformation is achieved via patch embedding layer \(g_{}:\), parameterized by \(\). Specifically, the function \(g_{}\) is implemented as convolution \(_{}\), using kernel \(_{}^{h_{k} w_{k} d}\) and bias \(_{}^{d}\). The kernel size is \((h_{k},w_{k})\) and the stride is \((h_{s},w_{s})\). In existing ViTs, patch embedding methods are categorized into non-overlapping and overlapping types:

Figure 1: **MSPE results on ImageNet-1K. We loaded a ViT-B model pre-trained on ImageNet-21K from  and evaluated: (a) Height equals width, ranging from 28\(\)28 to 896\(\)896, and (b) Fixed height=128, width ranging from 28 to 896. Vanilla ViT performance drops with size/aspect ratio changes; FlexiViT  significantly improves performance, and our method surpasses FlexiViT.**

* **Non-overlapping patch embedding:** In standard Vision Transformers (e.g., ViT ), the stride of \(_{}\) matches the kernel size, _i.e._, \(h_{s}=h_{k}\) and \(w_{s}=w_{k}\). The number of tokens \(N\) is calculated by \(}}\).
* **Overlapping patch embedding:** In models like PVT  and MViT , the stride is smaller than the kernel size, _i.e._, \(h_{s}<h_{k}\), \(w_{s}<w_{k}\). The number of tokens \(N\) is calculated by \(}{h_{s}}+p} {w_{s}}+p\), where \(p\) is the padding size.

**Transformer Encoder** adds position encodings \(_{i}\) to the token sequence, modifying it to \(\{_{i}^{}\}_{i=1}^{N}=\{_{i}+_{i}\}_{i=1}^{N}\). A class token \(_{}\) for global semantics is added to the front, _i.e._, \(\{_{},_{1}^{},,_{N}^{}\}\). This sequence feeds into the Transformer Encoder \(_{}()\), parameterized by \(\). For classification, only the class token \(_{}\) is used to produce a probability distribution, and other tokens \(\{_{1}^{},...,_{N}^{}\}\) are discarded.

### Problem Formulation

In real-world scenarios, images have variable resolutions due to variations in camera devices, parameters, object scale, and imaging distance. Our task is to ensure the model adapts to different resolutions. To this end, we resize the same image to multiple resolutions and optimize the model for these diverse inputs. Here is the formal definition of this task:

Firstly, the process of resizing (e.g. bilinear reize) is formally defined as a linear transformation:

\[_{r}^{}()=B_{r}^{}().\] (1)

where \(^{h w}\) is any input, the resolution \(r\) of \((h,w)\) transforms to \(r\)\(\) of \((h\)\(\)\(,w\)\(\)\()\) after resizing, using the transformation matrix \(B_{r}^{}^{h w hw}\). Each channel of \(\) is resized independently.

For input \((,y)\) in dataset \(\), the learning objective is to minimize the loss function \(\) (e.g., cross-entropy loss) across a series of resolutions \(\{r_{i}\}_{i=1}^{M}\), optimizing performance at each resolution from \(r_{1}\) to \(r_{M}\):

\[&_{,}_{(,y) }[(_{}(g_{}(B_{r}^ {r_{i}})),y] r_{i}\{r_{i}\}_{i=1}^{M}\\ &\ _{(,y)}[(_{ }(g_{}()),y)]_{(,y)}[(_{}(g_{}()),y)]+ , 0,\] (2)

where \(Enc_{}()\) and \(g_{}()\) are Transformer encoder and patch embedding layer, respectively. The slack variable \(\) allows minor loss increments in the well-trained model \(_{}(g_{}())\).

The central challenge is maintaining acceptable performance across different resolutions from \(r_{1}\) to \(r_{M}\) but keeping the performance of original resolution \(r\), which means adjustments of \(\) and \(\) must be careful. In this work, we confirm the key role of patch embedding and only optimizing \(g_{}\):

\[}_{}_{(,y) }[(_{}(g_{}(B_{r}^{r_{i}})),y] r_{i}\{r_{i}\}_{i=1}^{M}.\] (3)

### Pseudo-inverse Resize

To solve the optimization problems stated in Eq. (2) and (3), an intuitive solution appears to ensure that embedding layer \(g_{}\) produces consistent features on different resolutions. For this purpose, FlexiViT  proposed PI-resize, which adjusts the embedding kernel to ensure uniform outputs:

\[\{}},}}\}=}_{}}_{ }[(g_{}(B_{r}^{r_{i}})-g_{}())^{2}] r_{i}\{r_{i}\}_{i=1}^{M},\\ }}_{}}}_{}[(,})-  B_{r}^{r_{i}},}})^{2}]. \] (4)

When upscaling with \(r_{i}>r\), the analytic solution for Eq. (4) is \(}}=B_{r}^{r_{i}}(B_{r}^{r_{i}T}B_{r}^{r_{i}})^{-1} }\), denote as \((B_{r}^{r_{i}T})^{+}}\):

\[ B_{r}^{r_{i}},}}=^{T}B_{r }^{r_{i}T}B_{r}^{r_{i}}(B_{r}^{r_{i}T}B_{r}^{r_{i}})^{-1}}=^{T}}=,}.\] (5)

When downscaling with \(r_{i}<r\), the matrix \(B_{r}^{r_{i}T}B_{r}^{r_{i}}\) is non-invertible. Under the assumption \(=(0,1)\), it is proven that \((B_{r}^{r_{i}T})^{+}}\) is the optimal solution. In summary, Pseudo-inverse resize (PI-resize) is defined as follows:

\[_{r}^{}()=(B_{r}^{r_{i}T})^{+}().\] (6)

### Motivation

However, is this optimization target in FlexiViT  truly appropriate? We suggest that there are two problems with Equation 4: **First**, it is a stricter sufficient condition of Eq. (3), but it ignores the impact of the encoder \(_{}()\) and the objective function \(\). **Second**, at lower resolutions, _i.e._\(r_{i}<r\), this goal has no analytical solution; PI-resize is just an approximation assuming \((0,1)\), resulting in significant performance degradation.

Moreover, we intuitively suspect that similarity in patch embeddings does not ensure the best performance. As Figure 2(a) illustrates, features derived from patch embeddings are transformed into the classification feature space, namely the class token space, through the Transformer encoder \(_{}\). The gradient directions of these two feature spaces may not align, resulting in image features that are close to optimal in patch embedding being far from the optimal class token after encoder processing.

A more effective method is directly adjusting the weights \(w_{}\) of embedding layer using the objective function \(\) (Eq. (3)). To verify this idea and our assumptions, we evaluate a well-trained ViT-B/16 model  (pre-trained on ImageNet-21K, 224x224, 85.10% accuracy). We measure the cosine similarity between patch embeddings \(\{_{i}\}_{i=1}^{N}\) and the class token \(_{cls}\) at resolutions of 56x56 and 224x224. The results in Figures 2 (b) and (c) show that FlexiViT has higher patch embedding similarity and classification accuracy compared to the vanilla model; however, our method significantly outperforms FlexiViT with even lower patch embedding similarity. These results confirm that our analysis is reasonable.

## 3 Method

As discussed in Section 2.4, optimizing patch embedding layers through objective function is simple but effective. Motivated by this, we propose **M**ulti-**S**cale **P**atch **E**mbedding (MSPE). It divides the resolution domain into different ranges \(\{r_{i}\}_{i=1}^{M}\), each using a shared-weight patch embedding layer that adjusts size through PI-resize. MSPE can replace the patch embedding layers in most ViT models, including overlapping and non-overlapping types. Our method is illustrated in Figure 3 and presented below.

### Architecture of MSPE

MSPE only changes the patch embedding layer of the ViT model, making it directly applicable to a well-trained ViT model. As demonstrated in Figure 3, we introduce the following architectural modifications.

**Multiple patching kernels.** The typical patch embedding layer \(g_{}\) employs a single convolution size with parameters \((},})\), making it unsuitable for varying image resolutions. To overcome this, MSPE incorporates \(K\) convolutions with differenet kernel sizes \(\{g_{}^{i}\},,g_{}^{K}\}\), where each \(g_{}^{i}\) is parameterized by \((^{i}},^{i}})\), to support a broader range of input image sizes.

**Adaptive patching kernels.** Although \(K\) convolutions of different kernel sizes improve adaptability, they cannot cover all possible resolutions. Setting a unique size convolution kernel for each resolution is unrealistic. In MSPE, the size and ratio of the kernel \((^{i}},^{i}})\) are adjustable rather than fixed. Specifically, for input \(\) of any resolution \((h,w)\), the corresponding kernel size \((h_{k},w_{k})\) is

Figure 2: Similarity in patch embeddings does not guarantee optimal performance (a). We confirm this by evaluating the accuracy and cosine similarity of: (b) patch embeddings \(\{_{i}\}_{i=1}^{N}\)from 56x56 and 224x224 images, and (c) class tokens \(_{}\) from 56x56 and 224x224 images.

\(( h/N, w/N)\). Using Eq. (6), we adjust \(_{}^{i}\) to the corresponding size \(}_{}^{i}^{h_{k} w_{k}}\). As shown in Figure 3 (b), it can be directly convolved with the image patch.

Another module requiring careful design is the position embedding, which must correspond to the feature map size after patch embedding. In ViT  and DeiT , the position embedding is bilinearly interpolated to match the feature map size, a method known as _resample absolute pos embed_, and it has proven effective during fine-tuning . In MSPE, we discover that bilinear interpolation of position embedding satisfies our requirements. To simplify the architecture and minimize changes to the vanilla model, we follow this dynamic positional embedding method.

### Learning Objectives

MSPE optimizes the patch embedding layer \(g_{}\) through the objective function but does not explicitly constrain the embedded tokens \(\{_{i}\}_{i=1}^{N}\) across different resolutions. Specifically, MSPE is based on multi-resolution training (mixed resolution training), similar to methods like ResFormer and NaViT. The training process is as follows.

Firstly, the mixed resolution \(\{r_{i}\}_{i=1}^{M}\) is divided into \(K\) subsets \(\{S_{k}\}_{k=1}^{K}\), i.e. \(_{k=1}^{K}S_{k}=\{r_{i}\}_{i=1}^{M}\), and \(r_{k}\) is randomly sampled from \(S_{k}\). The patching kernel weights \(_{k}\) are shared within \(S_{k}\) and transformed into the corresponding weights \(}_{k}\) for each \(r_{i}\) according to Eq. (6), The loss function is defined as:

\[_{}(,y)=_{i=1}^{K}[(_{}(g_{ }_{i}}(B_{r}^{r_{i}})),y]+[(_{}(g_{}()),y],\] (7)

where \(\) is the task loss function (e.g., cross-entropy loss), and \(\) is a hyperparameter to prevent performance degradation. We optimize only the patch embedding parameters \(\) during training, setting the learning rate of \(\) to zero. Algorithm 1 in Appendix E.2 details the training procedure of MSPE and PyTorch-style implementation.

### Inference on Any Resolution

For an input \(\) with any resolution \(r^{*}\), previous models resize the image to a fixed resolution \(r\). The inference procedure is:

\[=_{}(g_{}(B_{r^{*}}^{r})),\] (8)

With MSPE, the model can infer directly from the original image without resizing or altering its aspect ratio. The process is:

\[=_{}(g_{^{*}}()),\] (9)

Figure 3: Illustration of the ViT model [2; 3] with MSPE. MSPE only replaces the patch embedding layer in the vanilla model, making well-trained ViT models to be directly applied to any size and aspect ratio. In our method, the patch embedding layer has several variable-sized kernels. The Transformer encoder is shared and frozen.

where \(^{*}\) is the patching weight that matches the resolution \(r^{*}\). Details on the computation of \(^{*}\) are provided in Appendix E.1 and Eq.(10).

## 4 Experiments

**Datasets.** We conduct experiments on 4 benchmark datasets: ImageNet-1K  for classification tasks, ADE20K  and Cityscapes  for semantic segmentation, and COCO2017  for object detection.

**Backbone networks.** Our method is assessed on three ViT models, including ViT-B  and DeiT-B III  (non-overlapping patches); PVT v2-B3  (overlapping patches). ViT and DeiT III are pre-trained using the ImageNet-21K and ImageNet-22K datasets.

**Implementation details.** MSPE is trained using SGD optimizer for _five epochs_, with a learning rate of 0.001, momentum of 0.9, weight decay of 0.0005, and batch size of 64 per GPU. To validate our model, we implement ViT  and other networks [30; 4; 25] via open-sourced timm library for classification, ViTDet  via MMDetection  for object detection, and SETR  via MMSegmentation  for segmentation; additional details are available in Appendix B.

### Image Classification

**Comparison with FlexiViT.** As shown in Table 1 and Figure 4, ViT models using non-overlapping (like ViT and DeiT III) and overlapping patch embeddings (like PVTv2 and MViTv2) significantly lose accuracy when the input resolution varies. This is consistent with the results from [15; 16; 17], demonstrating that the vanilla ViT models are not adaptable to input resolution changes. This issue primarily arises from their patch embedding layers failing to adjust to varying resolutions. This leads to high-level features shifting after the patch tokens are fed into the Transformer encoder and significantly degrading performance. FlexiViT shows remarkably stable performance at upscaled resolutions (e.g., 448x448) by ensuring consistency of patch tokens across different resolutions, outperforming vanilla models. However, as Section 3 analyzes, FlexiViT still struggles with downscaling.

Our method significantly boosts accuracy with targeted optimization goals, outperforming FlexiViT across various resolutions and aspect ratios.

**Comparison with ResFormer and NaViT.** ResFormer improves performance by multi-resolution training across 128x128, 160x160, and 224x224 resolutions. However, the modified ViT architecture does not suit networks like PVT and MiViT that use overlap patch embedding. Moreover, ResFormer trains for 200 epochs, but our method requires only five epochs. NaViT keeps the original aspect ratio and trains with mixed resolutions from 64x64 to 512x512, leveraging a larger JFT pre-training dataset and longer training cycles (up to 920,000 steps). Table 1 and Figure 5 show that these state-of-the-art methods improve the vanilla model remarkably. Compared to ResFormer and NaViT, MSPE achieves superior multi-resolution performance with far fewer training resources, which proves the essential role of optimized patch embedding layers.

### Semantic Segmentation

To validate the effectiveness of MSPE in semantic segmentation, we test the SETR  model on ADE20K  and Cityscapes  datasets, with the vanilla model trained at 512x512 and 768x768 resolutions, respectively. Results in Table 2 show that FlexiViT significantly enhances the vanilla model's performance in semantic segmentation (e.g., 59.78 vs. 13.33 on the mIOU metric). This confirms that adjusting the patch embedding layer is effective for pixel-level tasks, demonstrating its critical role in enhancing multi-resolution robustness. Moreover, MSPE consistently outperforms FlexiViT across various resolutions, proving our method is ready for pixel-dense tasks in real-world scenarios.

### Object Detection

In our experiments on the COCO2017 dataset for object detection and instance segmentation, we utilize the ViTDeT  model with ViT-B (pre-trained on ImageNet-1K via MAE ) and

    &  &  &  &  \\  & & mIOU & mACC & F1-score & mIOU & mACC & F1-score & mIOU & mACC & F1-score \\   & Vanilla & 2.89 & 4.91 & 4.64 & 7.48 & 10.81 & 15.88 & 45.39 & 55.70 & 59.71 \\  & FlexiViT & 31.44 & 38.86 & 44.13 & 42.97 & 52.48 & 57.21 & 45.39 & 55.70 & 59.71 \\  & MSPE & **39.65** & **49.01** & **51.37** & **44.39** & **53.73** & **58.90** & **45.39** & **55.70** & **59.71** \\   & & & **192x192** & & & **384x384** & & **768x768** & \\  & & mIOU & mACC & F1-score & mIOU & mACC & F1-score & mIOU & mACC & F1-score \\   & Vanilla & 13.33 & 19.49 & 24.62 & 20.10 & 27.34 & 26.84 & 77.58 & 84.94 & 86.72 \\  & FlexiViT & 59.78 & 68.65 & 72.74 & 74.60 & 82.81 & 84.58 & 77.58 & 84.94 & 86.72 \\  & MSPE & **65.90** & **74.84** & **77.21** & **75.79** & **83.40** & **85.35** & **77.58** & **84.94** & **86.72** \\   

Table 2: Comparative results of semantic segmentation on ADE20K and Cityscapes, using well-trained SETR Naive  as the segmentation model (ViT-L backbone), evaluated by mIOU, mACC, and F1-score.

Figure 5: Comparison of MSPE, Vanilla, and NaViT: only NaViT was pre-trained on the JFT dataset, baseline results come from .

    & **Method** & \(}\) & \(_{0}}\) & \(_{}}\) & \(_{}}\) & \(_{}}\) & \(_{1}}\) & \(}\) & \(_{}}\) & \(_{}}\) & \(_{}}\) & \(_{}}\) & \(_{}}\) & \(_{}}\) \\   & Vanilla & 0.52 & 0.72 & 0.57 & 0.35 & 0.56 & 0.66 & 0.46 & 0.69 & 0.50 & 0.27 & 0.49 & 0.64 \\   & Vanilla & 0.34 & 0.50 & 0.36 & 0.19 & 0.37 & 0.45 & 0.29 & 0.47 & 0.31 & 0.14 & 0.31 & 0.44 \\  & FlexiViT & 0.44 & 0.63 & 0.47 & 0.27 & 0.47 & 0.57 & 0.39 & 0.61 & 0.41 & 0.21 & 0.42 & 0.56 \\  & MSPE & **0.47** & **0.68** & **0.49** & **0.29** & **0.50** & **0.62** & **0.42** & **0.64** & **0.43** & **0.22** & **0.44** & **0.61** \\   & Vanilla & 0.03 & 0.05 & 0.03 & 0.01 & 0.04 & 0.05 & 0.03 & 0.05 & 0.03 & 0.01 & 0.04 & 0.05 \\   & FlexiViT & 0.19 & 0.31 & 0.20 & 0.10 & 0.23 & 0.29 & 0.17 & 0.29 & 0.17 & 0.07 & 0.19 & 0.28 \\   & MSPE & **0.30** & **0.42** & **0.29** & **0.16** & **0.34** & **0.44** & **0.27** & **0.39** & **0.27** & **0.12** & **0.30** & **0.43** \\   

Table 3: Comparative results of object detection and instance segmentation on COCO2017, employing well-trained ViTDeT  as the detection model (ViT-B backbone), pre-trained on ImageNet-1K via MAE .

employed Mask R-CNN  as the detection head. During the evaluation, we replace the ViT's patch embedding layer with MSPE or FlexiViT, keeping the rest of the architecture unchanged ( aligned with classification and segmentation ). As shown in Table 3, MSPE significantly improves the multi-resolution performance of well-trained detection models. This result is consistent with those of classification and segmentation tasks, demonstrating the effectiveness of our method across different visual tasks.

### Ablation Study and Analysis

**Training epochs.** Figure 6 (a) presents the performance of MSPE on different training epochs \((1,3,5)\). It is observed that MSPE significantly enhances performance with only a few training epochs. The models trained for 3 and 5 epochs show similar performance, with no significant improvement from additional epochs. Thus, we train MSPE for 5 epochs in our experiments.

**Model size.** For evaluating the impact of model size on MSPE, we test on ImageNet-1K across different sizes of DeiT III models (Small (S), Base (B), Large (L)), all pre-trained on ImageNet-22K. As shown in Figure 6 (b), the results of the larger model DeiT-L and the smaller model DeiT-S align with the main experiment; the larger model yields higher accuracy at different resolutions. This result demonstrates the effectiveness of our method across models of different sizes.

**Hyperparameters.** We conduct ablation studies of hyperparameters \(\) in Figure 7 (a). When lambda is set to 0, the learning of patch embedding is too flexible, leading to inadequate alignment with the original parameter space. Lambda values of 1 and 2 lead to similar performances; this hyperparameter is set to 1 in our experiments.

**Kernel count \(K\).** Figure 7 (b) shows the impact of different kernel quantities on model performance. In MSPE, the resolution \(r_{i}\) is divided into \(K\) subsets, with each subset sharing the patch embedding layer weights. Therefore, the number of subsets \(K\) equals the number of patchify kernels in MSPE. The results indicate that slightly increasing \(K\) can improve performance. Specifically, when \(K=3\) and \(K=4\), the model performance is nearly identical, and additional kernels provide little improvement. This suggests that patch embedding parameters can be shared across different resolutions. In our method, \(K\) is set to 4.

**Resizing method.** In our method, the patch embedding weights are dynamically resized for images with different sizes and ratios, denoted as adaptive kernels in Section 3.1. To evaluate the effect of resizing methods on MSPE, we load an ImageNet-21K pre-trained ViT-B model from  and train MSPE using different resizing methods, such as standard linear resizing. As shown in Figure 8, the

  
**Model** & **Method** & **Params(\(g\))** & **FLOPs(\(g\))** & **Params(all)** \\   & Vanilla & **590.59K** & 0.92 & **86.57M** \\  & MSPE & 1108.99K & **0.01-0.92** & 87.09M \\   & Vanilla & **10.50K** & 2.83 & **45.25M** \\  & MSPE & 29.70K & **0.13-2.83** & 45.27M \\   

Table 4: Parameter and computational cost of the patch embedding layer \(g_{}\) in MSPE and Vanilla models, the parameter count of the entire model remains nearly unchanged.

Figure 8: Comparison results of different resizing methods in MSPE. PI-resize shows the best performance and robustness.

Figure 6: Comparison results: (a) different training epochs; (b) model sizes of S, B, and L.

Figure 7: Comparison results: (a) hyperparameter \(\); (b) differenet kernel count \(K\).

results indicate that PI-resize consistently outperforms other common resizing methods, aligning with findings from the FlexiViT .

**Parameters and computation overhead.** As shown in Table 4, we analyze the impact of MSPE on the parameter and computational cost of the vanilla VIT model. MSPE modifies only the patch embedding layer \(g_{}\), increasing its parameter count by 2x\(\)3x. However, MSPE provides a more flexible approach to calculating patch embeddings, significantly reducing computational costs compared to the original method. The total parameter count of the model remains nearly unchanged because the patch embedding layer is an extremely small component.

**Image resizing v.s. MSPE.** Table 5 shows the comparison results of image resizing (IMG-resize) and MSPE on ImageNet-1K. IMG-resize adjusts images of different resolutions to a preset resolution (e.g., 224x224) during testing, whereas our method keeps the original image size and aspect ratio unchanged. In real-world scenarios, resizing small-size images to a larger size incurs digitization noise and increased computation costs, and model performance is not optimal at different resolutions. The results indicate that our method comprehensively outperforms IMG-resize, suggesting that model inference on the original images is a viable option.

## 5 Related Work

Most relevant to our work is FlexiViT , which proposed to resize the patch embedding weights, enabling flexible patch sizes and token lengths in ViT models. Pix2struct  supports variable aspect ratios by a novel positional embedding method, enhancing efficiency and performance in chart and document understanding. ResFormer  enables resolution adaptability through multi-resolution training and a global-local positional embedding strategy. NaViT  enhances efficiency and performance by example packing  to adjust token lengths for different resolutions during training. Compared to these methods, MSPE only changes the patch embedding layer and achieves better performance. This justifies the patch embedding layer's key role in adapting ViT models for different resolutions.

In CNN-based models, Mind the Pooling  addresses overfitting on resolution by introducing SBPooling to replace max-pooling, enabling CNNs to process different resolutions. Learn to Resize  uses a learnable resizing layer to replace bilinear interpolation. Another study  examined the relationship between training and testing resolutions, showing that training at slightly lower resolutions than testing can improve performance. Networks like Resolution Adaptive Networks (RANet)  and Dynamic Resolution Networks (DRNet)  use multiple sub-models to choose the appropriate resolution and model based on task difficulty, thus enhancing model efficiency and resolution adaptability.

## 6 Conclusion

To make ViT models compatible with images of different sizes and aspect ratios, we propose MSPE to replace the traditional patch embedding layer for accommodating variable image resolutions. MSPE uses multiple variable-sized patch kernels and selects the best parameters for different resolutions, eliminating the need to resize the original image. Extensive experiments demonstrate that MSPE performs well in various visual tasks (image classification, segmentation, and detection). Particularly, MSPE yields superior performance on low-resolution inputs and performs comparably on high-resolution inputs with previous methods. Our method has the potential for application in various vision tasks, and can be extended by optimizing the embedding layer and transformer encoder jointly.

    &  &  \\  & & 28 & 42 & 56 & 70 & 84 & 98 & 112 & 126 & 140 & 168 & 224 & 448 \\   & IMG-resize & 52.99 & 67.51 & 74.06 & 76.86 & 79.16 & 80.25 & 80.88 & 82.04 & 82.68 & 83.99 & 85.10 & 84.92 \\  & MSPE & **56.41** & **71.02** & **77.94** & **79.54** & **81.63** & **82.51** & **83.75** & **83.81** & **83.94** & **84.70** & **85.10** & **85.11** \\   & IMG-resize & 39.97 & 58.40 & 66.60 & 71.16 & 74.20 & 76.37 & 78.15 & 79.67 & 80.39 & 82.01 & 83.39 & 82.46 \\  & MSPE & **45.01** & **62.54** & **71.92** & **73.72** & **77.09** & **78.18** & **80.29** & **79.94** & **80.58** & **82.14** & **83.39** & **83.39** \\   & IMG-resize & 32.09 & 52.12 & 61.54 & 67.36 & 71.31 & 74.05 & 76.32 & 78.14 & 79.45 & 80.95 & 83.12 & 82.34 \\   & MSPE & **34.20** & **60.76** & **72.09** & **75.39** & **77.39** & **78.08** & **80.36** & **81.12** & **81.59** & **82.14** & **83.12** & **83.08** \\   

Table 5: Comparison results of image resizing and MSPE on ImageNet-1K Top-1 accuracy.

Acknowledgments and Disclosure of Funding

This work has been supported by the National Natural Science Foundation of China (NSFC) grant U20A20223 and the InnoHK program.