# On the Ability of Graph Neural Networks to

Model Interactions Between Vertices

 Noam Razin, Tom Verbin, Nadav Cohen

Tel Aviv University

{noamrazin,tomverbin}@mail.tau.ac.il, cohennadav@cs.tau.ac.il

###### Abstract

Graph neural networks (GNNs) are widely used for modeling complex interactions between entities represented as vertices of a graph. Despite recent efforts to theoretically analyze the expressive power of GNNs, a formal characterization of their ability to model interactions is lacking. The current paper aims to address this gap. Formalizing strength of interactions through an established measure known as _separation rank_, we quantify the ability of certain GNNs to model interaction between a given subset of vertices and its complement, _i.e._ between the sides of a given partition of input vertices. Our results reveal that the ability to model interaction is primarily determined by the partition's _walk index_ -- a graph-theoretical characteristic defined by the number of walks originating from the boundary of the partition. Experiments with common GNN architectures corroborate this finding. As a practical application of our theory, we design an edge sparsification algorithm named _Walk Index Sparsification_ (_WIS_), which preserves the ability of a GNN to model interactions when input edges are removed. WIS is simple, computationally efficient, and in our experiments has markedly outperformed alternative methods in terms of induced prediction accuracy.1 More broadly, it showcases the potential of improving GNNs by theoretically analyzing the interactions they can model.

## 1 Introduction

_Graph neural networks_ (_GNNs_) are a family of deep learning architectures, designed to model complex interactions between entities represented as vertices of a graph. In recent years, GNNs have been successfully applied across a wide range of domains, including social networks, biochemistry, and recommender systems (see, _e.g._, ). Consequently, significant interest in developing a mathematical theory behind GNNs has arisen.

One of the fundamental questions a theory of GNNs should address is _expressivity_, which concerns the class of functions a given architecture can realize. Existing studies of expressivity largely fall into three categories. First, and most prominent, are characterizations of ability to distinguish non-isomorphic graphs , as measured by equivalence to classical Weisfeiler-Leman graph isomorphism tests . Second, are proofs for universal approximation of continuous permutation invariant or equivariant functions, possibly up to limitations in distinguishing some classes of graphs . Last, are works examining specific properties of GNNs such as frequency response  or computability of certain graph attributes, _e.g._ moments, shortest paths, and substructure multiplicity .

A major drawback of many existing approaches -- in particular proofs of equivalence to Weisfeiler-Leman tests and those of universality -- is that they operate in asymptotic regimes of unboundednetwork width or depth. Moreover, to the best of our knowledge, none of the existing approaches formally characterize the strength of interactions GNNs can model between vertices, and how that depends on the structure of the input graph and the architecture of the neural network.

The current paper addresses the foregoing gaps. Namely, it theoretically quantifies the ability of fixed-size GNNs to model interactions between vertices, delineating the impact of the input graph structure and the neural network architecture (width and depth). Strength of modeled interactions is formalized via _separation rank_ -- a commonly used measure for the interaction a function models between a subset of input variables and its complement (the rest of the input variables). Given a function and a partition of its input variables, the higher the separation rank, the more interaction the function models between the sides of the partition. Separation rank is prevalent in quantum mechanics, where it can be viewed as a measure of entanglement . It was previously used for analyzing variants of convolutional, recurrent, and self-attention neural networks, yielding both theoretical insights and practical tools [30; 33; 61; 62; 64; 100; 65; 85]. We employ it for studying GNNs.

Key to our theory is a widely studied correspondence between neural networks with polynomial nonlinearity and _tensor networks_2[32; 29; 30; 34; 90; 61; 62; 7; 56; 57; 63; 64; 83; 100; 84; 85; 65]. We extend this correspondence, and use it to analyze message-passing GNNs with product aggregation. We treat both graph prediction, where a single output is produced for an entire input graph, and vertex prediction, in which the network produces an output for every vertex. For graph prediction, we prove that the separation rank of a depth \(L\) GNN with respect to a partition of vertices is primarily determined by the partition's \((L-1)\)_-walk index_ -- a graph-theoretical characteristic defined to be the number of length \(L-1\) walks originating from vertices with an edge crossing the partition. The same holds for vertex prediction, except that there walk index is defined while only considering walks ending at the target vertex. Our result, illustrated in Figure 1, implies that for a given input graph, the ability of GNNs to model interaction between a subset of vertices \(\) and its complement \(^{c}\), predominantly depends on the number of walks originating from the boundary between \(\) and \(^{c}\). We corroborate this proposition through experiments with standard GNN architectures, such as Graph Convolutional Network (GCN)  and Graph Isomorphism Network (GIN) .

Our theory formalizes conventional wisdom by which GNNs can model stronger interaction between regions of the input graph that are more interconnected. More importantly, we show that it facilitates an _edge sparsification_ algorithm that preserves the expressive power of GNNs (in terms of ability to model interactions). Edge sparsification concerns removal of edges from a graph for reducing computational and/or memory costs, while attempting to maintain selected properties of the graph (_cf._[11; 93; 48; 20; 86; 98; 67; 24]). In the context of GNNs, our interest lies in maintaining prediction accuracy as the number of edges removed from the input graph increases. We propose an algorithm for removing edges, guided by our separation rank characterization. The algorithm, named _Walk Index Sparsification_ (_WIS_), is demonstrated to yield high predictive performance for GNNs (_e.g._ GCN and GIN) over standard benchmarks of various scales, even when removing a significant portion of

Figure 1: Illustration of our main theoretical contribution: quantifying the ability of GNNs to model interactions between vertices of an input graph. Consider a partition of vertices \((,^{c})\), illustrated on the left, and a depth \(L\) GNN with product aggregation (Section 3). For graph prediction, as illustrated on the right, the strength of interaction the GNN can model between \(\) and \(^{c}\), measured via separation rank (Section 2.2), is primarily determined by the partition’s \((L-1)\)_-walk index_ — the number of length \(L-1\) walks emanating from \(_{}\), which is the set of vertices with an edge crossing the partition. The same holds for vertex prediction, except that there walk index is defined while only considering walks ending at the target vertex.

edges. WIS is simple, computationally efficient, and in our experiments has markedly outperformed alternative methods in terms of induced prediction accuracies across edge sparsity levels. More broadly, WIS showcases the potential of improving GNNs by theoretically analyzing the interactions they can model, and we believe its further empirical investigation is a promising direction for future research.

The remainder of the paper is organized as follows. Section 2 introduces notation and the concept of separation rank. Section 3 presents the theoretically analyzed GNN architecture. Section 4 theoretically quantifies (via separation rank) its ability to model interactions between vertices of an input graph. Section 5 proposes and evaluates WIS -- an edge sparsification algorithm for arbitrary GNNs, born from our theory. Lastly, Section 6 concludes. Related work is discussed throughout, and for the reader's convenience, is recapitulated in Appendix B.

## 2 Preliminaries

### Notation

For \(N\), let \([N]:=\{1,,N\}\). We consider an undirected input graph \(=(,)\) with vertices \(=[||]\) and edges \(\{\{i,j\}:i,j\}\). Vertices are equipped with features \(:=(^{(1)},,^{(||)}) ^{D_{x}||}\) -- one \(D_{x}\)-dimensional feature vector per vertex (\(D_{x}\)). For \(i\), we use \((i):=\{j:\{i,j\}\}\) to denote its set of neighbors, and, as customary in the context of GNNs, assume the existence of all self-loops, _i.e._\(i(i)\) for all \(i\) (_cf._[59; 50]). Furthermore, for \(\) we let \(():=_{i}(i)\) be the neighbors of vertices in \(\), and \(^{c}:=\) be the complement of \(\). We use \(_{}\) to denote the boundary of the partition \((,^{c})\), _i.e._ the set of vertices with an edge crossing the partition, defined by \(_{}:=\{i:(i)^{c} \}\{j^{c}:(j) \}\).3 Lastly, we denote the number of length \(l_{ 0}\) walks from any vertex in \(\) to any vertex in \(\) by \(_{l}(,)\).4 In particular, \(_{l}(,)=_{i,j}_ {l}(\{i\},\{j\})\).

Note that we focus on undirected graphs for simplicity of presentation. As discussed in Section 4, our results are extended to directed graphs in Appendix D.

### Separation Rank: A Measure of Modeled Interaction

A prominent measure quantifying the interaction a multivariate function models between a subset of input variables and its complement (_i.e._ all other variables) is known as _separation rank_. The separation rank was introduced in , and has since been employed for various applications [51; 47; 13]. It is also a common measure of _entanglement_, a profound concept in quantum physics quantifying interaction between particles . In the context of deep learning, it enabled analyses of expressivity and generalization in certain convolutional, recurrent, and self-attention neural networks, resulting in theoretical insights and practical methods (guidelines for neural architecture design, pretraining schemes, and regularizers -- see [30; 33; 61; 62; 64; 100; 65; 85]).

Given a multivariate function \(f:(^{D_{x}})^{N}\), its separation rank with respect to a subset of input variables \([N]\) is the minimal number of summands required to express it, where each summand is a product of two functions -- one that operates over variables indexed by \(\), and another that operates over the remaining variables. Formally:

**Definition 1**.: The _separation rank_ of \(f:(^{D_{x}})^{N}\) with respect to \([N]\) is:

\[(f;):=R _{ 0}:&\;g^{(1)},,g^{(R)}:(^{D_{x}})^{| |},\;^{(1)},,^{(R)}:(^{D _{x}})^{|^{c}|}\\ &f()=_{r=1}^{R}g^{(r)}(_{ })^{(r)}(_{^{c}})}\,,\] (1)

where \(:=(^{(1)},,^{(N)})\), \(_{}:=(^{(i)})_{i_{}}\), and \(_{^{c}}:=(^{(j)})_{j^{c}}\). By convention, if \(f\) is identically zero then \((f;)=0\), and if the set on the right hand side of Equation (1) is empty then \((f;)=\).

InterpretationIf \((f;)=1\), the function is separable, meaning it does not model any interaction between \(_{}\) and \(_{^{c}}\), _i.e._ between the sides of the partition \((,^{c})\). Specifically, it can be represented as \(f()=g(_{})(_{^{c}})\) for some functions \(g\) and \(\). In a statistical setting, where \(f\) is a probability density function, this would mean that \(_{}\) and \(_{^{c}}\) are statistically independent. The higher \((f;)\) is, the farther \(f\) is from separability, implying stronger modeling of interaction between \(_{}\) and \(_{^{c}}\).

## 3 Graph Neural Networks

Modern GNNs predominantly follow the message-passing paradigm [45; 50], whereby each vertex is associated with a hidden embedding that is updated according to its neighbors. The initial embedding of \(i\) is taken to be its input features: \(^{(0,i)}:=^{(i)}^{D_{x}}\). Then, in a depth \(L\) message-passing GNN, a common update scheme for the hidden embedding of \(i\) at layer \(l[L]\) is:

\[^{(l,i)}=\!\!\!} ^{(l)}^{(l-1,j)}:j(i)\!}\!\,,\] (2)

where \(\{\!\{.\}\!\}\) denotes a multiset, \(^{(1)}^{D_{h} D_{x}},^{(2)}^{D_{h} D_{h}},,^{(L)}^{D_{h} D_{h}}\) are learnable weight matrices, with \(D_{h}\) being the network's width (_i.e._ hidden dimension), and aggregate is a function combining multiple input vectors into a single vector. A notable special case is GCN , in which aggregate performs a weighted average followed by a non-linear activation function (_e.g._ ReLU).5 Other aggregation operators are also viable, _e.g._ element-wise sum, max, or product (_cf._[49; 53]). We note that distinguishing self-loops from other edges, and more generally, treating multiple edge types, is possible through the use of different weight matrices for different edge types [49; 88]. For conciseness, we hereinafter focus on the case of a single edge type, and treat multiple edge types in Appendix D.

After \(L\) layers, the GNN generates hidden embeddings \(^{(L,1)},,^{(L,||)}^{D_{h}}\). For graph prediction, where a single output is produced for the whole graph, the hidden embeddings are usually combined into a single vector through the aggregate function. A final linear layer with weights \(^{(o)}^{1 D_{h}}\) is then applied to the resulting vector.6 Overall, the function realized by a depth \(L\) graph prediction GNN receives an input graph \(\) with vertex features \(:=(^{(1)},,^{(||)})^{D_{x}||}\), and returns:

\[ f^{(,)}():= ^{(o)}\!\!\!} ^{(L,i)}:i\!}\!\,,\] (3)

with \(:=(^{(1)},,^{(L)},^{(o)})\) denoting the network's learnable weights. For vertex prediction tasks, where the network produces an output for every \(t\), the final linear layer is applied to each \(^{(L,t)}\) separately. That is, for a target vertex \(t\), the function realized by a depth \(L\) vertex prediction GNN is given by:

\[ f^{(,,t)}():= ^{(o)}^{(L,t)}\,.\] (4)

Our aim is to investigate the ability of GNNs to model interactions between vertices. Prior studies of interactions modeled by different deep learning architectures have focused on neural networks with polynomial non-linearity, building on their representation as tensor networks [32; 30; 34; 90; 61; 62; 7; 56; 63; 64; 83; 100; 84; 85; 65]. Although neural networks with polynomial non-linearity are less common in practice, they have demonstrated competitive performance [28; 31; 91; 94; 27; 37; 53], and hold promise due to their compatibility with quantum computation [46; 14] and fully homomorphic encryption . More importantly, their analyses brought forth numerous insights that were demonstrated empirically and led to development of practical tools for widespread deep learning models (with non-linearities such as ReLU).

Following the above, in our theoretical analysis (Section 4) we consider GNNs with (element-wise) product aggregation, which are polynomial functions of their inputs. Namely, the aggregate operator from Equations (2) and (3) is taken to be:

\[():=_{}\,,\] (5)where \(\) stands for the Hadamard product and \(\) is a multiset of vectors. The resulting architecture can be viewed as a variant of the GNN proposed in , where it was shown to achieve competitive performance in practice. Central to our proofs are tensor network representations of GNNs with product aggregation (formally established in Appendix E), analogous to those used for analyzing other types of neural networks. We empirically demonstrate our theoretical findings on popular GNNs (Section 4.2), such as GCN and GIN with ReLU non-linearity, and use them to derive a practical edge sparsification algorithm (Section 5).

We note that some of the aforementioned analyses of neural networks with polynomial non-linearity were extended to account for additional non-linearities, including ReLU, through constructs known as _generalized tensor networks_. We thus believe our theory may be similarly extended, and regard this as an interesting direction for future work.

Theoretical Analysis: The Effect of Input Graph Structure and Neural Network Architecture on Modeled Interactions

In this section, we employ separation rank (Definition 1) to theoretically quantify how the input graph structure and network architecture (width and depth) affect the ability of a GNN with product aggregation to model interactions between input vertices. We overview the main results and their implications in Section 4.1, while deferring the formal analysis to Appendix A due to lack of space. Section 4.2 provides experiments demonstrating our theory's implications on common GNNs, such as GCN and GIN with ReLU non-linearity.

### Overview and Implications

Consider a depth \(L\) GNN with width \(D_{h}\) and product aggregation (Section 3). Given a graph \(\), any assignment to the weights of the network \(\) induces a multivariate function -- \(f^{(,)}\) for graph prediction (Equation (3)) and \(f^{(,,t)}\) for prediction over a given vertex \(t\) (Equation (4)) -- whose variables correspond to feature vectors of input vertices. The separation rank of this function with respect to \(\) thus measures the interaction modeled across the partition \((,^{c})\), _i.e._ between the vertices in \(\) and those in \(^{c}\). The higher the separation rank is, the stronger the modeled interaction.

Key to our analysis are the following notions of _walk index_, defined by the number of walks emanating from the boundary of the partition \((,^{c})\), _i.e._ from vertices with an edge crossing the partition induced by \(\) (see Figure 1 for an illustration).

**Definition 2**.: Let \(\). Denote by \(_{}\) the set of vertices with an edge crossing the partition \((,^{c})\), _i.e._\(_{}:=\{i:(i)^{c} \}\{j^{c}:(j) \}\), and recall that \(_{l}(_{},)\) denotes the number of length \(l_{ 0}\) walks from any vertex in \(_{}\) to any vertex in \(\). For \(L\):

* (graph prediction) we define the \((L-1)\)-_walk index_ of \(\), denoted \(_{L-1}()\), to be the number of length \(L-1\) walks originating from \(_{}\), _i.e._\(_{L-1}():=_{L-1}(_{},)\); and
* (vertex prediction) for \(t\) we define the \((L-1,t)\)-_walk index_ of \(\), denoted \(_{L-1,t}()\), to be the number of length \(L-1\) walks from \(_{}\) that end at \(t\), _i.e._\(_{L-1,t}():=_{L-1}(_{},\{t\})\).

As our main theoretical contribution, we prove:

**Theorem 1** (informally stated).: _For all weight assignments \(\) and \(t\):_

\[f^{( ,)};=(D_{h}) _{L-1}()\,,\] \[f^{( ,,t)};=(D_{h}) _{L-1,t}()\,.\]

_Moreover, nearly matching lower bounds hold for almost all weight assignments.7_

The upper and lower bounds are formally established by Theorems 2 and 3 in Appendix A, respectively, and are generalized to input graphs with directed edges and multiple edge types in Appendix D. Theorem 1 implies that, the \((L-1)\)-walk index of \(\) in graph prediction and its \((L-1,t)\)-walk index in vertex prediction control the separation rank with respect to \(\), and are thus paramount for modeling interaction between \(\) and \(^{c}\) -- see Figure 2 for an illustration. It thereby formalizes the conventional wisdom by which GNNs can model stronger interaction between areas of the input graph that are more interconnected. We support this finding empirically with common GNN architectures (_e.g._ GCN and GIN with ReLU non-linearity) in Section 4.2.

One may interpret Theorem 1 as encouraging addition of edges to an input graph. Indeed, the theorem states that such addition can enhance the GNN's ability to model interactions between input vertices. This accords with existing evidence by which increasing connectivity can improve the performance of GNNs in practice (see, _e.g._, [40; 1]). However, special care needs to be taken when adding edges: it may distort the semantic meaning of the input graph, and may lead to flights known as over-smoothing and over-squashing [68; 78; 22; 1; 8]. Rather than employing Theorem 1 for adding edges, we use it to select which edges to preserve in a setting where some must be removed. That is, we employ it for designing an edge sparsification algorithm. The algorithm, named _Walk Index Sparsification_ (_WIS_), is simple, computationally efficient, and in our experiments has markedly outperformed alternative methods in terms of induced prediction accuracy. We present and evaluate it in Section 5.

### Empirical Demonstration

Our theoretical analysis establishes that, the strength of interaction GNNs can model across a partition of input vertices is primarily determined by the partition's walk index -- a graph-theoretical characteristic defined by the number of walks originating from the boundary of the partition (see Definition 2). The analysis formally applies to GNNs with product aggregation (see Section 3), yet we empirically demonstrate that its conclusions carry over to various other message-passing GNN architectures, namely GCN , GAT , and GIN  (with ReLU non-linearity). Specifically, through controlled experiments, we show that such models perform better on tasks in which the partitions that require strong interaction are ones with higher walk index, given that all other aspects of the tasks are the same. A description of these experiments follows. For brevity, we defer some implementation details to Appendix H.2.

We constructed two graph prediction datasets, in which the vertex features of each input graph are patches of pixels from two randomly sampled Fashion-MNIST  images, and the goal is to predict whether the two images are of the same class.8 In both datasets, all input graphs have the same structure: two separate cliques with \(16\) vertices each, connected by a single edge. The datasets differ in how the image patches are distributed among the vertices: in the first dataset each clique holds all the patches of a single image, whereas in the second dataset each clique holds half of the patches from the first image and half of the patches from the second image. Figure 2 illustrates how

Figure 2: Depth \(L\) GNNs can model stronger interactions between sides of partitions that have a higher walk index (Definition 2). The partition \((_{1},_{1}^{c})\) (left) divides the vertices into two separate cliques, connected by a single edge. Only two vertices reside in \(_{_{1}}\) — the set of vertices with an edge crossing the partition. Taking for example depth \(L=3\), the \(2\)-walk index of \(_{1}\) is \((||^{2})\) and its \((2,t)\)-walk index is \((||)\), for \(t\). In contrast, the partition \((_{2},_{2}^{c})\) (right) equally divides the vertices in each clique to different sides. All vertices reside in \(_{_{2}}\), meaning the \(2\)-walk index of \(_{2}\) is \((||^{3})\) and its \((2,t)\)-walk index is \((||^{2})\), for \(t\). Hence, in both graph and vertex prediction scenarios, the walk index of \(_{1}\) is relatively low compared to that of \(_{2}\). Our analysis (Section 4.1 and Appendix A) states that a higher separation rank can be attained with respect to \(_{2}\), meaning stronger interaction can be modeled across \((_{2},_{2}^{c})\) than across \((_{1},_{1}^{c})\). We empirically confirm this prospect in Section 4.2.

image patches are distributed in the first (left hand side of the figure) and second (right hand side of the figure) datasets, with blue and red marking assignment of vertices to images.

Each dataset requires modeling strong interaction across the partition separating the two images, referred to as the _essential partition_ of the dataset. In the first dataset the essential partition separates the two cliques, thus it has low walk index. In the second dataset each side of the essential partition contains half of the vertices from the first clique and half of the vertices from the second clique, thus the partition has high walk index. For an example illustrating the gap between these walk indices see Figure 2.

Table 1 reports train and test accuracies achieved by GCN, GAT, and GIN (with ReLU non-linearity) over both datasets. In compliance with our theory, the GNNs fit the dataset whose essential partition has high walk index significantly better than they fit the dataset whose essential partition has low walk index. Furthermore, the improved train accuracy translates to improvements in test accuracy.

## 5 Practical Application: Expressivity Preserving Edge Sparsification

Section 4 theoretically characterizes the ability of a GNN to model interactions between input vertices. It reveals that this ability is controlled by a graph-theoretical property we call walk index (Definition 2). The current section derives a practical application of our theory, specifically, an _edge sparsification_ algorithm named _Walk Index Sparsification_ (_WIS_), which preserves the ability of a GNN to model interactions when input edges are removed. We present WIS, and show that it yields high predictive performance for GNNs over standard vertex prediction benchmarks of various scales, even when removing a significant portion of edges. In particular, we evaluate WIS using GCN , GIN , and ResGCN  over multiple datasets, including: Cora , which contains thousands of edges, DBLP , which contains tens of thousands of edges, and OGBN-ArXiv , which contains more than a million edges. WIS is simple, computationally efficient, and in our experiments has markedly outperformed alternative methods in terms of prediction accuracy across edge sparsity levels. We believe its further empirical investigation is a promising direction for future research.

### Walk Index Sparsification (WIS)

Running GNNs over large-scale graphs can be prohibitively expensive in terms of runtime and memory. A natural way to tackle this problem is edge sparsification -- removing edges from an input graph while attempting to maintain prediction accuracy (_cf._[67; 24]).9\({}^{,}\)10

    & &  \\   & & Low & High \\   & Train Acc. (\%) & \(70.4 1.7\) & \(\) \\  & Test Acc. (\%) & \(52.7 1.9\) & \(\) \\   & Train Acc. (\%) & \(82.8 2.6\) & \(\) \\  & Test Acc. (\%) & \(69.6 0.6\) & \(\) \\   & Train Acc. (\%) & \(83.2 0.8\) & \(\) \\  & Test Acc. (\%) & \(53.7 1.8\) & \(\) \\   

Table 1: In accordance with our theory (Section 4.1 and Appendix A), GNNs can better fit datasets in which the partitions (of input vertices) that require strong interaction are ones with higher walk index (Definition 2). Table reports means and standard deviations, taken over five runs, of train and test accuracies obtained by GNNs of depth \(3\) and width \(16\) on two datasets: one in which the essential partition — _i.e._ the main partition requiring strong interaction — has low walk index, and another in which it has high walk index (see Section 4.2 for a detailed description of the datasets). For all GNNs, the train accuracy attained over the second dataset is considerably higher than that attained over the first dataset. Moreover, the better train accuracy translates to better test accuracy. See Appendix H.2 for further implementation details.

``` Input:\(\) -- graph, \(N\) -- number of edges to remove Result: Sparsified graph obtained by removing \(N\) edges from \(\) ``` for\(n=1,,N\)do  # per edge, compute walk indices of partitions induced by \(\{t\}\), for \(t\), after its removal for\(e\) (excluding self-loops) do  initialize \(^{(e)}=(0,,0)^{||}\)  remove \(e\) from \(\) (temporarily)  for every \(t\), set \(_{t}^{(e)}=_{L-1,t}(\{t\})\) # = number of length \(L-1\) walks from \(_{\{t\}}\) to \(t\)  add \(e\) back to \(\) endfor  # prune edge whose removal harms walk indices the least according to an order over \((^{(e)})_{e}\)  for \(e\), sort the entries of \(^{(e)}\) in ascending order  let \(e^{}*{argmax}_{e}^{(e)}\) according to lexicographic order over tuples remove \(e^{}\) from \(\) (permanently) endfor ```

**Algorithm 1**\((L-1)\)-Walk Index Sparsification (WIS)

``` Input:\(\) -- graph, \(N\) -- number of edges to remove Result: Sparsified graph obtained by removing \(N\) edges from \(\) ``` for\(n=1,,N\)do for\(\{i,j\}\) (excluding self-loops) do  let \(_{min}(i,j):=\{|(i)|,|(j)|\}\)  let \(_{max}(i,j):=\{|(i)|,|(j)|\}\) endfor  # prune edge \(\{i,j\}\) with maximal \(_{min}(i,j)\), breaking ties using \(_{max}(i,j)\)  let \(e^{}*{argmax}_{\{i,j\}} _{min}(i,j),_{max}(i,j)\) according to lexicographic order over pairs remove \(e^{}\) from \(\) endfor ```

**Algorithm 2**\(1\)-Walk Index Sparsification (WIS) (efficient version of Algorithm 1 for \(L=2\))

Our theory (Section 4) establishes that, the strength of interaction a depth \(L\) GNN can model across a partition of input vertices is determined by the partition's walk index, a quantity defined by the number of length \(L-1\) walks originating from the partition's boundary. This brings forth a recipe for pruning edges. First, choose partitions across which the ability to model interactions is to be preserved. Then, for every input edge (excluding self-loops), compute a tuple holding what the walk indices of the chosen partitions will be if the edge is to be removed. Lastly, remove the edge whose tuple is maximal according to a preselected order over tuples (_e.g._ an order based on the sum, min, or max of a tuple's entries). This process repeats until the desired number of edges are removed. The idea behind the above-described recipe, which we call _General Walk Index Sparsification_, is that each iteration greedily prunes the edge whose removal takes the smallest toll in terms of ability to model interactions across chosen partitions -- see Algorithm 3 in Appendix F for a formal outline. Below we describe a specific instantiation of the recipe for vertex prediction tasks, which are particularly relevant with large-scale graphs, yielding our proposed algorithm -- Walk Index Sparsification (WIS). Exploration of other instantiations is regarded as a promising avenue for future work.

In vertex prediction tasks, the interaction between an input vertex and the remainder of the input graph is of central importance. Thus, it is natural to choose the partitions induced by singletons (_i.e._ the partitions \((\{t\},\{t\})\), where \(t\)) as those across which the ability to model interactions is to be preserved. We would like to remove edges while avoiding a significant deterioration in the ability to model interaction under any of the chosen partitions. To that end, we compare walk index tuples according to their minimal entries, breaking ties using the second smallest entries, and so forth. This is equivalent to sorting (in ascending order) the entries of each tuple separately, and then ordering the tuples lexicographically.

Algorithm 1 provides a self-contained description of the method attained by the foregoing choices. We refer to this method as \((L-1)\)-Walk Index Sparsification (WIS), where the "\((L-1)\)" indicates that only walks of length \(L-1\) take part in the walk indices. Since \((L-1)\)-walk indices can be computed by taking the \((L-1)\)'th power of the graph's adjacency matrix, \((L-1)\)-WIS runs in \((N||||^{3}(L))\) time and requires \((||||+||^{2})\) memory, where \(N\) is the number of edges to be removed. For large graphs a runtime cubic in the number of vertices can be restrictive. Fortunately, \(1\)-WIS, which can be viewed as an approximation for \((L-1)\)-WIS with \(L>2\), facilitates a particularly simple and efficient implementation based solely on vertex degrees, requiring only linear time and memory -- see Algorithm 2 (whose equivalence to \(1\)-WIS is explained in Appendix G). Specifically, \(1\)-WIS runs in \((N||+||)\) time and requires \((||+||)\) memory.

### Empirical Evaluation

Below is an empirical evaluation of WIS. For brevity, we defer to Appendix H some implementation details, as well as experiments with additional GNN architectures (GIN and ResGCN) and datasets (Chameleon , Squirrel , and Amazon Computers ).

Using depth \(L=3\) GNNs (with ReLU non-linearity), we evaluate over the Cora dataset both \(2\)-WIS, which is compatible with the GNNs' depth, and \(1\)-WIS, which can be viewed as an efficient approximation. Over the DBLP and OGBN-ArXiv datasets, due to their larger scale only \(1\)-WIS is evaluated. Figure 3 (and Figure 8 in Appendix H) shows that WIS significantly outperforms the following alternative methods in terms of induced prediction accuracy: _(i)_ a baseline in which edges are removed uniformly at random; _(ii)_ a well-known spectral algorithm  designed to preserve the spectrum of the sparsified graph's Laplacian; and _(iii)_ an adaptation of UGS  -- a recent supervised approach for learning to prune edges.11 Both \(2\)-WIS and \(1\)-WIS lead to higher test accuracies, while (as opposed to UGS) avoiding the need for labels, and for training a GNN over the original (non-sparsified) graph -- a procedure which in some settings is prohibitively expensive in terms of runtime and memory. Interestingly, \(1\)-WIS performs similarly to \(2\)-WIS, indicating that the efficiency it brings does not come at a sizable cost in performance.

Figure 3: Comparison of GNN accuracies following sparsification of input edges — WIS, the edge sparsification algorithm brought forth by our theory (Algorithm 1), markedly outperforms alternative methods. Plots present test accuracies achieved by a depth \(L=3\) GCN of width \(64\) over the Cora (left), DBLP (middle), and OGBN-ArXiv (right) vertex prediction datasets, with increasing percentage of removed edges (for each combination of dataset, edge sparsification algorithm, and percentage of removed edges, a separate GCN was trained and evaluated). WIS, designed to maintain the ability of a GNN to model interactions between input vertices, is compared against: _(i)_ removing edges uniformly at random, _(ii)_ a spectral sparsification method ; and _(iii)_ an adaptation of UGS . For Cora, we run both \(2\)-WIS, which is compatible with the GNN’s depth, and \(1\)-WIS, which can be viewed as an approximation that admits a particularly efficient implementation (Algorithm 2). For DBLP and OGBN-ArXiv, due to their larger scale only \(1\)-WIS is evaluated. Markers and error bars report means and standard deviations, respectively, taken over ten runs per configuration. Note that \(1\)-WIS achieves results similar to \(2\)-WIS, suggesting that the efficiency it brings does not come at a significant cost in performance. Appendix H provides further implementation details and experiments with additional GNN architectures (GIN and ResGCN) and datasets (Chameleon, Squirrel, and Amazon Computers). Code for reproducing the experiment is available at https://github.com/noamrazin/gnn_interactions.

Conclusion

### Summary

GNNs are designed to model complex interactions between entities represented as vertices of a graph. The current paper provides the first theoretical analysis for their ability to do so. We proved that, given a partition of input vertices, the strength of interaction that can be modeled between its sides is controlled by the _walk index_ -- a graph-theoretical characteristic defined by the number of walks originating from the boundary of the partition. Experiments with common GNN architectures, _e.g._ GCN  and GIN , corroborated this result.

Our theory formalizes conventional wisdom by which GNNs can model stronger interaction between regions of the input graph that are more interconnected. More importantly, we showed that it facilitates a novel edge sparsification algorithm which preserves the ability of a GNN to model interactions when edges are removed. Our algorithm, named _Walk Index Sparsification_ (_WIS_), is simple, computationally efficient, and in our experiments has markedly outperformed alternative methods in terms of induced prediction accuracy. More broadly, WIS showcases the potential of improving GNNs by theoretically analyzing the interactions they can model.

### Limitations and Future Work

The theoretical analysis considers GNNs with product aggregation, which are less common in practice (_cf._ Section 3). We empirically demonstrated that its conclusions apply to more popular GNNs (Section 4.2), and derived a practical edge sparsification algorithm based on the theory (Section 5). Nonetheless, extending our analysis to additional aggregation functions is a worthy avenue to explore.

Our work also raises several interesting directions concerning WIS. A naive implementation of \((L-1)\)-WIS has runtime cubic in the number of vertices (_cf._ Section 5.1). Since this can be restrictive for large-scale graphs, the evaluation in Section 5.2 mostly focused on \(1\)-WIS, which can be viewed as an efficient approximation of \((L-1)\)-WIS (its runtime and memory requirements are linear -- see Section 5.1). Future work can develop efficient exact implementations of \((L-1)\)-WIS (_e.g._ using parallelization) and investigate regimes where it outperforms \(1\)-WIS in terms of induced prediction accuracy. Additionally, \((L-1)\)-WIS is a specific instantiation of the general WIS scheme (given in Appendix F), tailored for preserving the ability to model interactions across certain partitions. Exploring other instantiations, as well as methods for automatically choosing the partitions across which the ability to model interactions is preserved, are valuable directions for further research.