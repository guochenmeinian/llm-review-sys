# AGILE: A Novel Reinforcement Learning Framework

of LLM Agents

 Peiyuan Feng\({}^{*}\)1 Yichen He\({}^{*}\)1 Guanhua Huang\({}^{*}\)1 Yuan Lin\({}^{*}\)1

**Hanchong Zhang\({}^{*}\)1\({}^{3}\) Yuchen Zhang\({}^{*}\)1 Hang Li\({}^{1}\)**

\({}^{1}\)ByteDance Research \({}^{2}\)University of Science and Technology of China

\({}^{3}\)Shanghai Jiao Tong University

{fpy,hyc,linyuan.0,zhangyuchen.zyc,lihang.lh}@bytedance.com,

guanhuahuang@mail.ustc.edu.cn, zhanghanchong@sjtu.edu.cn

Equal contribution. Alphabet order.Work done during ByteDance Research internship.

###### Abstract

We introduce a novel reinforcement learning framework of LLM agents named AGILE (**AG**ent that **I**nteracts and **L**earns from **E**nvironments) designed to perform complex conversational tasks with users, leveraging LLMs, memory, tools, and interactions with experts. The agent possesses capabilities beyond conversation, including reflection, tool usage, and expert consultation. We formulate the construction of such an LLM agent as a reinforcement learning (RL) problem, in which the LLM serves as the policy model. We fine-tune the LLM using labeled data of actions and the PPO algorithm. We focus on question answering and release a dataset for agents called ProductQA, comprising challenging questions in online shopping. Our extensive experiments on ProductQA, MedMCQA and HotPotQA show that AGILE agents based on 7B and 13B LLMs trained with PPO can outperform GPT-4 agents. Our ablation study highlights the indispability of memory, tools, consultation, reflection, and reinforcement learning in achieving the agent's strong performance. Datasets and code are available at https://github.com/bytarnish/AGILE.

## 1 Introduction

Large Language Models (LLMs) have exhibited remarkable capabilities such as instruction following, reasoning, and zero-shot learning , which have greatly catalyzed the development of autonomous agents based on LLMs , also known as LLM agents. Recent works propose several essential components or workflows to enhance the abilities of LLM agents, such as planning , reflection , tool-use  and life-long learning . However, it remains unclear how to integrate all components into a unified framework and optimize them end-to-end.

In this paper, we introduce a novel reinforcement learning framework for LLM agents to unify various components and streamline their learning and operation processes. As shown in Figure 1(a), the architecture of the agent system, named AGILE, comprises four modules: LLM, memory, tools, and executor. Furthermore, the agent can interact with both users and experts. The LLM, functioning as the predictor of all actions, generates instructions and processes responses. The executor, working as the controller of all actions, interprets the LLM instructions to activate the corresponding modules and collects their responses for the LLM. For example, the executor can fetch a text from the memory and append it to the context of LLM, or extract an excerpt from the context and append it to the memory.

The executor can also follow instructions of the LLM to utilize a search tool. In addition to skills such as reasoning, planning, and reflection, we propose a new ability called _seeking advice_, which means that the agent proactively results human experts when it encounters a problem unsolvable. The agent can reflect on the expert feedback and memorize it for future use. Furthermore, we propose a training method based on reinforcement learning (RL), which simultaneously trains the policy of invoking different modules and the reasoning, planning, reflection, and seeking advice abilities of the LLM agent in an end-to-end fashion.

While the proposed agent framework is general, in this paper, we evaluate it in complex question answering (QA). It is a task an LLM agent has the potential of outperforming existing solutions such as the use of an LLM alone. However, existing QA benchmarks  are designed for specific subsets of capabilities (e.g., reflection, memory retrieve, etc.) which cannot simultaneously investigate the ability to combine all modules and capabilities of the agent.

To address this, we have developed a new benchmark called ProductQA. ProductQA comprises 88,229 question-answer pairs in customer service divided into 26 QA tasks, each corresponding to a distinct Amazon product category. This benchmark is based on real Amazon user queries and includes fact-based questions, reasoning questions, and product recommendation queries. It comprehensively evaluates agents' abilities to handle historical information and accumulated knowledge, leverage tools, interact with humans, perform self-evaluation, and conduct reflection. Additionally, the training and testing tasks are made disjoint to assess the agent's ability to adapt to new product categories.

We evaluate our agent framework on three tasks, ProductQA, MedMCQA  and HotPotQA . For ProductQA, we use a two-stage training method based on Vicuna-13b . In the first stage, imitation learning is employed to create agile-vic13b-sft. In the second stage, the policy gradient algorithm of PPO  produces agile-vic13b-ppo. Experimental results show that agile-vic13b-ppo improves the relative total performance score by 9.2% over GPT-4 and by 90.8% over GPT-3.5. Ablation studies confirm that all modules in Figure 1 are indispensable. Specifically, removing tools or memory usage negatively impacts the agent's performance, leading to a 25.9% or 17.4% increase in seeking advice, respectively, or a 9.3% or 4.0% relative decrease in the total score, respectively. Disabling the seeking advice function results in a 10.7% decrease in accuracy. Finally, agile-vic13b-ppo achieves a 2.3% relative increase in total score compared to agile-vic13b-sft, demonstrating the necessity of PPO training. On MedMCQA, we train an agile-mek7b-ppo agent, initialized from Meerkat-7b , following the same two-stage procedure.

Figure 1: (a) Architecture of our agent system, including LLM, memory, tools, and executor. (b) A running example of AGILE in a customer service QA environment. The tokens (actions) generated by the LLM are in orange color and the tokens appended by the executor are in blue color.

Our agent improves the base LLM's accuracy from 53.4% to 85.2% by seeking advice on 31.6% instances. This accuracy surpasses the SOTA accuracy of 79.1% by GPT4-MedPrompt . When all agents are able to seek advice, our agent also outperforms the GPT-4 agent in terms of the total score. For HotPotQA, we use the same two-stage method to train agile-vic13b-ppo from Vicuna-13b. Our agent achieves 67.5% accuracy, surpassing the strongest baseline of 48.2%, by seeking advice on 15.6% of instances. When advice-seeking is enabled for all agents, our agent outperforms GPT-4 by 10.8% in total score.

The main contributions of this paper are summarized as follows:

* We propose a novel reinforcement learning framework of LLM agents. It facilitates end-to-end learning of agents. Notably, this framework enables the agent to proactively seek advice from human experts, providing two advantages: 1) It ensures high-level accuracy when dealing with complex and challenging questions, and 2) it fosters learning from humans, thereby enhancing its abilities to adapt to new tasks.
* We develop a benchmark, ProductQA, to comprehensively evaluate the agent's capabilities in complex question answering.
* We perform experiments on multiple tasks to verify our framework and show that AGILE agents based on 13B and 7B LLMs trained with PPO can surpass GPT-4 agents.

## 2 Methods

### RL formulation of agent

Our agent framework comprises four elements: LLM, memory, tools, and executor, see Figure 1(a). The LLM possesses a _context_, defined as the sequence of tokens it utilizes to generate the next token. In RL terminology, the agent conducts a token-level Markov decision process (MDP). The action space \(\) corresponds to the LLM's vocabulary, with each token representing an action. Hence, the LLM serves as the _policy model_. The agent's state consists of the (context, memory) pair. Upon predicting a new action \(a_{t}\) (i.e., a new token), the LLM transfers control to the executor. The executor applies predefined logic to transition from the current state \(s_{t}\) to the next state \(s_{t+1}\), implementing the state transition function \(\) in RL, and then returns control to the LLM to predict the next action. Concurrently, the environment issues a reward \(r(s_{t},a_{t})\).

Let us examine the state transition more closely. For each action, the executor's first operation is to append the token to the context, preparing the LLM for generating the next token. Then, the executor checks a registered list of _functions_. Each function is designed to execute a set of operations, including memory I/O, tool usage, and interaction with the environment. If the action (i.e., the token) matches a function name, the executor will execute the associated function implementation, further mutating the agent state. For instance, if the token is [GetQuestion], the executor will prompt the user for a new question and append it to the context; if the token is [UpdateMemory], the executor will write a specific segment of the context into the memory; if the token is [ClearContext], the executor will reset the context to [BOS]. In summary, the LLM interacts with the memory and tools by predicting function names, relying on the executor to execute these functions. See Table 1 for a full list of functions defined for a QA agent and see Figure 1(b) for a running example.

### Policy learning

We frame the policy learning problem as a task of training a language model. Consider an agent trajectory \(=(s_{1},a_{1},...,s_{n},a_{n})\), we derive a _training sequence_ denoted as \((e_{1},...,e_{n})\), where \(e_{i}\) represents the tokens that the executor appends to the context at step \(i\). If \(a_{i}\) is a function name token, then \(e_{i}\) is the concatenation of \(a_{i}\) and extra tokens appended by the function execution; otherwise, \(e_{i}=a_{i}\). In this sequence, \(\{a_{1},...,a_{n}\}\) (the first token of each \(e_{i}\)) are referred to as action tokens. The LLM context at step \(i\), denoted by \(c_{i}\), is a subsequence of the prefix \((e_{1},...,e_{i-1})\); \(c_{i}\) may be shorter than \((e_{1},...,e_{i-1})\) because the executor can delete context tokens.

In Imitation Learning (IL), we generate trajectories by observing human experts or more proficient agents, then we derive the training sequences to fine-tune the LLM. It is important to point out that (1) the loss is calculated on the action tokens only, and (2) \(c_{i}\) should serve as the attention mask for tokens in \(e_{i}\), as it reflects the true context perceived by the LLM at the time of action prediction. Inreinforcement learning (RL), we treat the LLM as the policy model, from which training sequences can be sampled and individual action tokens are assigned rewards. Consequently, the LLM can be optimized using policy gradient methods, such as PPO . Analogous to the IL setup, we apply policy gradient updates exclusively to the action tokens and employ \(c_{i}\) as the attention mask.

In some situations, an agent may produce very long trajectories, potentially yielding training sequences that span millions of tokens and are impractical for training. We can leverage the structure of the trajectory to partition it into smaller segments. For instance, if the agent resets its LLM context at the beginning of every QA session, then we can partition by the session boundary. Nevertheless, these sessions are not entirely independent; actions taken in earlier sessions can influence memory, creating lasting effects on subsequent sessions. To tackle this challenge of long-range dependencies, we propose a training algorithm detailed in Appendix A.

### Interaction with human experts

Our agent framework enables the agent to proactively seek advice from human experts. For example, the agent can invoke a [SeekAdvice] function to request expert advice. This approach helps in two ways. Firstly, the agent can request the correct answer when its confidence is low, ensuring sufficient accuracy for the application. Secondly, the agent can use [Reflection] to distill general knowledge from the expert advice before storing it in memory. This accumulation of knowledge allows the agent to adapt to new tasks that it has not encountered during training.

Seeking advice involves complex decision-making. The agent must estimate its own confidence in the current session, predict the potential value of the advice for future sessions, and consider the cost of human resources. The optimal trade-off is difficult to annotate manually but aligns well with our RL framework. Specifically, the present risk, future value, and cost of action can all be represented as RL rewards, allowing this skill to be trained as part of the policy model on an end-to-end basis.

## 3 The ProductQA dataset

We believe that product question answering in a real online shopping environment offers a comprehensive challenge for evaluating LLM agents. First, it demands expert knowledge about millions of products, including their technical specifications, usage in particular scenarios, and compatibility with other products. Second, answering some questions requires the use of tools, such as a product search tool. Third, the continuous emergence of new products necessitates the adaptability of the agent. This has motivated the creation of the ProductQA dataset. Unlike existing online shopping QA datasets [38; 8], which primarily focus on questions about product metadata or page information, ProductQA features more complex queries involving reasoning, expert knowledge, and tool usage (e.g., SQL), providing a comprehensive assessment of an agent's capabilities.

The ProductQA dataset consists of 26 QA tasks, each representing a distinct group of products within a specific category. Each group encompasses 17-20 products. We collected 20 groups for training and 6 for testing, allowing for assessing the agent's adaptability to new tasks. We collected an average of 3,393 question-answer pairs for each product group. The questions within the same

  
**Function name** & **Function implementation** \\ 
[GetQuestion] & Prompt the user for a question and append it to the context. \\
[RetrieveMemory] & Retrieve relevant entries from the memory and append them to the context. \\
[SeekAdvice] & Ask human experts for advice and append it to the context. \\
[Reflection] & \(\) \\
[UpdateMemory] & Write a specific segment of the context into the memory. \\
[SearchProduct] & Extract a search query from the context, then invoke the search tool and append results to the context. \\
[PredictAnswer] & \(\) \\
[SubmitAnswer] & Extract a predicted answer from the context and submit it to the user. \\
[ClearContext] & Reset the context to a single token [BOS]. \\   

Table 1: Functions for an exemplary customer service QA agent. Among them, [Reflection] and [PredictAnswer] are trivial functions, as the executor passes control immediately back to the LLM to start generating result tokens.

group are correlated, as knowledge from one answer may aid in addressing other questions. The dataset statistics are presented in Table 12.

The dataset is annotated by 20 professional annotators, each with at least a college degree, employed by a commercial data annotation company. We pay the company at market rates for professional annotation. See annotation guidelines in Appendix F.2. In addition, we will release the code for the data pre-processing before human annotation.

### Product collection

We gather products from the Amazon Review Data , which includes product metadata as well as reviews. We initially filter the Amazon Review Data to retain only popular products with at least 100 reviews, then cluster them by category tags. From these clusters, we select 26 based on the size of the cluster, each defined as a _product group_. Subsequently, we sample products from each product group. See Appendix F.1 for more details about product group and product selection.

After the products are collected, annotators compile an information table for each product group. An example of such a table is presented in Table 2. To enhance the efficiency of the annotation process, we employ GPT-4 to extract as many product features as possible from the reviews. These features, together with the product metadata, are provided to the annotators for table creation.

### QA collection

We identify three predominant types of questions in online shopping contexts: 1) **Fact-QA**: questions concerning specific product details; 2) **Search-QA**: searches for product recommendations tailored to user preferences; 3) **Reasoning-QA**: questions whose answers require domain-specific reasoning, such as the implications of a product feature. Accordingly, we annotate question-answer pairs for these types. Each question is annotated with both a detailed paragraph-long answer and a concise short answer. The long answer should resemble a response from human customer service, while the short answer consists of a few words. We train the model to predict both answer types. The accuracy of the long answers is evaluated using GPT-4 (see Appendix J for the prompt); the short answers are assessed by exact match and are used for defining rewards for RL training.

Fact-QAFact-QAs are constructed from product reviews. For each product, we provide GPT-4 with a batch of 30 reviews, prompting it to generate 20 questions and their corresponding answers

  
**Product ID** & **Title** & **Price** & **Brand** &  **Headphone** \\ **Type** \\  & **Cable Type** &  **Audio** \\ **Transmission** \\  & 
 **Audio** \\ **Output Mode** \\  & **...** \\  B00WSL2FTK & Sennheiser RS 170 & 511.03 & Sennheiser & over-ear & bluetooth & kleer & stereo &... \\  B003ALL2HE & JVC HAEB75B & 59.99 & JVC & earbud & 3.5mm Jack & analog & bass boost &... \\  B01C22IJV0 & Phaiser BHs-530 & 56.04 & Phaiser & earbud & bluetooth & bluetooth & stereo &... \\  B0013OWPV4 & JVC HARX700 & 52.00 & JVC & over-ear & 3.5mm Jack & analog & stereo &... \\... &... &... &... &... &... &... &... &... \\   

Table 2: An example of an information table for the headphones group.

  
**Type** & **Question** & **Long Answer** & **Short Answer** \\   & What is the size of the & The JVC HA-EB75 headphones contain a 13.5 \\  & &  \ driver used in the \\ JVC HA-EB75 headphones? \\  & 13.5 mm \\   &  \(\)m an audophile always on the \\ move, so I need my music non- \\ stop. Tell me, what’s the headphone with the longest playtime \\ you have, either on-ear or in-ear? \\  & I found a product that matches your criteria. \\  & &  \\ 'ABChopUSA Wireless Earbuds True’ with asin: \\ B00LJT2EPK \\  & B00LJT2EPK \\   &  Will these headphones deliver \\ comparable sound quality to \\ wired alternatives when I am \\ editing music? \\  & No, these headphones may not suit your needs for \\  & & music editing since they are wireless and can introduce audio compression and slight latency. Such \\   & & 
 no \\ use audio compression and slight latency. Such \\ issues can impact the precise listening experience \\ crucial for professional audio editing tasks. \\  & no \\   

Table 3: Examples of Fact-QA, Search-QA and Reasoning-QA in ProductQA.

before moving on to the next batch. We encourage GPT-4 to create diverse questions. The results are then given to annotators to refine and finalize the question-answer pairs.

Search-QAStarting with an information table for a given product group, we generate random SQL expressions using a set of predefined rules. These expressions are then translated into natural language questions by GPT-4. The answers are obtained by executing the SQL queries. Subsequently, human annotators thoroughly revise the QA pairs.

Reasoning-QAAs the first step, we collect professional knowledge for each product group. To enhance efficiency, we utilize GPT-4 to generate candidate knowledge entries based on the technical specifications from the information table. These entries are then curated and refined by human annotators. Here is an example of a knowledge entry: _Motherboards with the ATX form factor are ideally suited for high-performance computing tasks and gaming, due to their ample expansion slots for graphics cards and other peripherals that boost computing capabilities._ Finally, annotators develop question-answer pairs from these knowledge entries.

## 4 Experiments

### Experimental setting

DatasetWe evaluate our agent on three complex QA tasks: ProductQA, MedMCQA and HotPotQA. MedMCQA  is a dataset for multiple-choice QA. It consists of questions from medical school entrance examinations. HotPotQA  features natural, multi-hop questions, which challenge an agent's ability to perform reasoning and utilize search tools. For both MedMCQA and HotPotQA, we report results on their respective full dev sets.

Agent definitionOur agent can invoke functions defined in Table 1. In a typical workflow, the agent prompts the user for a new question at the session start. It can then retrieve memory to get relevant information. The memory can be initialized as empty (ProductQA) or with domain knowledge (QA pairs from MedMCQA training dataset). The agent has the option to use external tools, such as product search in ProductQA and article search in HotPotQA), to gather more information. At last, the agent decides whether to predict an answer directly or seek human advice. If the agent seeks advice, it obtains a human answer (ground-truth answer in our setting). The agent can then optionally use a reflection round to extract general knowledge from the human answer, writing both the human answer and the reflected knowledge to its memory. Finally, the agent submits an answer to the user. In our setting, submitting a correct answer incurs a \(+1\) reward, while submitting a wrong answer incurs a \(0\) reward. Seeking human advice has a fixed \(-c\) reward, where \(c\) represents _seeking advice cost_. Assuming the human advice always contains a correct answer, then the possible total rewards are \(\{0,1,1-c\}\).

TrainingThe training consists of two stages. First, we construct trajectories from the training data and employ imitation learning to train the agent. Then we apply Algorithm 1 for further optimization by reinforcement learning. See Appendix B for implementation details. For ProductQA and HotPotQA, the agent's LLM is initialized from Vicuna-13b-1.5. For MedMCQA, we use Meerkat-7b , a medical LLM trained with high-quality CoT reasoning paths from 18 medical textbooks and diverse instruction-following datasets. We fine-tune the model for 2 epochs with a learning rate of 1e-5 and a batch size of 64. We implement PPO for 1 epoch with a learning rate of 1e-6 and a batch size of 64. The training runs on NVIDIA-H800. Training times and the number of GPUs for each experiment are reported in Table 13. The LLM is fully trained without using LoRA.

Evaluation and baselinesWe report three metrics for the agent: (a) Advice rate: the rate of seeking human advice; (b) Accuracy: the rate of predicting the correct answer; (c) Total score: the average reward across all sessions, taking the advice rate and the accuracy both into account.

We compare our agent against two types of baselines: 1) Prompting GPT-3.5 (gpt-3.5-turbo-0301) and GPT-4 (gpt-4-0613)  to directly answer the question, without working in an agent manner, noted as gpt3.5-prompt and gpt4-prompt. 2) Prompting GPT-3.5 and GPT-4 within the AGILE framework, noted as agile-gpt3.5-prompt and agile-gpt4-prompt. We carefully designed prompts for all baselines and they are shown in Appendix J.

### Results on ProductQA

As Table 4 shows, our AGILE agent outperforms all baselines on ProductQA. Notably, the average total score of agile-vic13b-ppo across six test groups shows a relative improvement of 9.2% in short answers and 5.0% in long answers to agile-gpt4-prompt where the seeking advice cost is added into the prompt. Concretely, agile-vic13b-ppo uses a comparable number of seeking advice to achieve 7.4% higher accuracy in short answers than agile-gpt4-prompt, and as Figure 2 shows, this accuracy improvement is consistent across the whole trajectory. Our agile-vic7b-ppo agent also outperforms agile-gpt4-prompt in average total scores. Note that the GPT-4 agent knows the seeking advice cost from its prompt (see Figure 7).

We investigate the impact of varying the seeking advice cost. As shown in Figure 3, when the cost decreases, both the advice rate and the accuracy increase, indicating greater utilization of human assistance. Specifically, with a high cost of 0.5, the advice rate is close to 0, and at a low cost of 0.1, the accuracy is close to 1. This result demonstrates that by adjusting the cost and through RL training, we can effectively manage the trade-off between accuracy and human cost. For instance, the agent can achieve 94.1% accuracy on the Motherboards task with a seeking advice cost of \(c=0.1\) (refer to Table 16). This capability is especially important in realistic scenarios that demand high accuracy levels. In most experiments, we set the cost at a medium level with \(c=0.3\).

To validate the accuracy of GPT-4 evaluator in assessing the long answer results, we randomly select 100 triplets (questions, reference long answer, model-predicted long answer) and manually labeled the correctness. The results show a 94% agreement rate between the GPT-4 evaluator and the author.

Ablation studyWe present ablation studies in Table 5 to assess the contributions of individual agent components and the effects of RL training. The table indicates that disabling the option to seek advice (w/o Advice) leads to a 10.7% drop in accuracy and a 5.0% relative reduction in total score. Forcing the agent to seek advice at the initial part of the trajectory (Non-adapt Advice) causes a 4.2% decrease in accuracy, underscoring the value of adaptive decision-making. Removing reflection and memory capabilities (w/o Memory and w/o Reflection) both increase the frequency of advice-seeking, as the agent struggles to accumulate or leverage valuable knowledge, consequently decreasing the

    &  **Advice** \\ **Rate** \\  } &  &  \\   & & **Short** & **Long** & **Short** & **Long** \\  gpt3.5-prompt & - & 0.202 & 0.322 & - & - \\ gpt4-prompt & - & 0.464 & 0.571 & - & - \\ agile-vic13b-prompt & 0.174 & 0.174 & 0.294 & 0.122 & 0.242 \\ agile-gpt3.5-prompt & 0.323 & 0.508 & 0.644 & 0.411 & 0.547 \\ agile-gpt4-prompt & 0.208 & 0.780 & 0.809 & 0.718 & 0.747 \\  agile-vic7b-ppo (ours) & 0.179 & 0.818 & 0.800 & 0.764 & 0.746 \\ agile-vic13b-ppo (ours) & 0.233 & **0.854** & **0.854** & **0.784** & **0.784** \\   

Table 4: Results on ProductQA. Here, X-prompt represents directly prompting model X; agile-X-Y incorporates model X within the AGILE framework, while Y represents prompting or PPO training. We report results on short and long answers, respectively. The seeking advice cost is \(c=0.3\). Results are averaged over six test tasks. See Table 14 for individual product group performance.

total score. Furthermore, disabling tool use (w/o Tool-Use) causes a substantial 25.9% increase in the advice-seeking rate because the agent's capabilities are diminished, making it more reliant on external advice. Lastly, RL training improves the relative total score by 2.3%, lowers the advice-seeking rate, and boosts accuracy, demonstrating that RL training effectively optimizes the policy. Additional results on RL training can be found in Appendix C.

In Appendix E, we present detailed examples of agile-vic13b-ppo illustrating how memory, tools, seeking advice, and reflection enhance the agent workflow.

Trend of advice rateFigure 4 demonstrates a consistent decrease in the advice rate of agile-vic13b-ppo as more sessions are added to the trajectory. This decline can be attributed to the agent progressively accumulating knowledge and becoming more independent. Additionally, the figure illustrates that disabling RL training or reflection leads to a significant increase in the advice rate, underscoring the importance of RL training and reflection in reducing human costs.

### Results on MedMCQA

Our agile-mek7b-ppo agent, based on the smaller Meerkat-7b  model, reaches an accuracy of 85.2% with an advice rate of 31.6%. As Table 6 shows, this represents a 31.8% accuracy increase over the base model Meerkat-7b-prompt and a 6.1% increase over the state-of-the-art gpt4-Medprompt. Table 6 also shows that the ability to seek advice alone contributes a 23.2% accuracy gain, meaning that each instance of seeking advice corrects an average of 0.73 prediction errors. This indicates that PPO training effectively helps the agent identify its mistakes. For a fair comparison, we also evaluate agile-gpt3.5-prompt and agile-gpt4-prompt, which incorporate GPT-3.5

  
**Method** & **Advice Rate \(\)** & **Accuracy \(\)** & **Total Score \(\)** \\  Meerkat-7b-prompt & - & 0.534 & - \\ gpt3.5-prompt & - & 0.501 & - \\ gpt4-prompt & - & 0.695 & - \\ gpt4-Medprompt & - & 0.791 & - \\  agile-gpt3.5-prompt & 0.194 & 0.697 & 0.619 \\ agile-gpt4-prompt & 0.421 & **0.884** & 0.721 \\  agile-mek7b-w/o Reflection & 0.368 & 0.790 & 0.643 \\ agile-mek7b-w/o Memory & 0.506 & 0.741 & 0.539 \\ agile-mek7b-w/o Advice & 0.000 & 0.620 & 0.620 \\ agile-mek7b-w/o RL & 0.322 & 0.837 & 0.708 \\  agile-mek7b-ppo (ours) & 0.316 & 0.852 & **0.726** \\   

Table 6: Results on the MedMCQA dev dataset. X-prompt represents directly prompting the model X; agile-X-Y represents incorporating the model X within the AGILE framework, while Y represents prompting, ablation studies or standard PPO training. The seeking advice cost is \(c=0.4\).

Figure 4: Advice rate over the following 200 sessions on ProductQA (\(c=0.3\)).

  
**Method** & **Advice Rate \(\)** & **Accuracy \(\)** & **Total Score \(\)** \\  w/o Reflection & 0.270 & 0.852 & 0.771(-1.79) \\ w/o Memory & 0.407 & 0.876 & 0.754(-4.09) \\ w/o Advice & 0.000 & 0.747 & 0.747(-5.09) \\ non-adapt-advice & 0.233 & 0.812 & 0.742(-5.79) \\ w/o Tool-Use & 0.492 & 0.864 & 0.717(-3.93) \\ w/o RL & 0.256 & 0.843 & 0.766(-2.93) \\  agile-vic13b-ppo (ours) & 0.233 & **0.854** & **0.784** \\   

Table 5: Ablation studies for disabling reflection, memory, seeking advice, tool use, or RL training. Here, non-adapt-advice means that seeking advice is invoked for the first \(K\) sessions of the trajectory, where \(K\) equals to the number of [SeekAdvice] performed by agile-vic13b-ppo. See Table 15 for ablation results on individual product groups.

and GPT-4 within our AGILE framework. These agents also leverage advice-seeking to enhance accuracy, but without RL training, their total scores are lower than agile-mek7b-ppo. Finally, through ablation studies, we confirmed the essential roles of memory, reflection, seeking advice, and RL training in achieving high performance. Removing these components leads to a significant drop in total scores, detailed in Table 6.

### Results on HotPotQA

We compare our method against several baselines. Specifically, we found the original ReAct baseline implementation in  to be suboptimal. By reproducing their results with GPT-4 (ReAct-gpt4-prompt), we observed improved performance. As shown in Table 7, our agile agent outperforms all baselines in accuracy, achieving a 40.0% relative improvement over ReAct-gpt4-prompt, which is the strongest baseline. Additionally, compared to agile-gpt4-prompt, the trained agile-vic13b-ppo demonstrates both higher accuracy and a lower advice rate, leading to a 10.8% relative increase in total score. Ablation studies confirm that removing either seeking-advice or PPO training results in a significant decrease in the total score.

## 5 Related work

LLM agentsLarge Language Models (LLMs) have demonstrated substantial capabilities in following instructions, reasoning, and planning. Numerous research works, as shown in Table 8, utilizing prompt engineering, have constructed remarkable LLM agents capable of autonomously resolving complex tasks across various environments [28; 44; 2; 30; 4]. Furthermore, extensive works identify key components in the design of LLM agents, including planning [22; 39; 10; 32; 51; 35], tool-use [19; 29; 48; 36], and reflection [40; 21]. In this work, we enable the agent to utilize memory, tools and proactively learn from the environment. We then formulate the entire process within an RL framework so that all agent skills can be jointly optimized end-to-end.

    &  &  &  &  \\  & & **(Exact Match)** & **(GPT-4 Evaluator)** & **(Exact Match)** \\  ReAct  & - & 0.351 & - & - \\ ReAct-gpt4-prompt & - & 0.482 & - & - \\ CRITIC  & - & 0.443 & - & - \\ Expel  & - & 0.390 & - & - \\ AutoAct  & - & 0.384 & - & - \\  agile-gpt4-prompt & 0.194 & 0.664 & 0.842 & 0.567 \\  agile-vic13b-w/o Advice & 0.000 & 0.553 & 0.751 & 0.553 \\ agile-vic13b-w/o RL & 0.171 & 0.668 & 0.857 & 0.617 \\  agile-vic13b-ppo (ours) & 0.156 & **0.675** & **0.858** & **0.628** \\   

Table 7: Results on the HotPotQA full dev dataset. X-prompt represents directly prompting the model X; agile-X-Y represents incorporating the model X within the AGILE framework, while Y represents prompting, ablation studies or standard PPO training. The seeking advice cost is \(c=0.3\).

    &  &  &  &  &  &  &  \\  & & & & & & & **Human-agent** \\  WebGPT  & GPT-3 & & ✓ & ✓ & ✗ & ✓ & ✗ & ✗ \\ ReAct  & PaLM-540b & & ✓ & ✗ & ✓ & ✓ & ✗ & ✗ \\ Reflexion  & GPT-3/3,5/4 & ✗ & ✗ & ✓ & ✓ & ✓ & ✗ \\ ChatDev  & ChatGPT-turbo-16k & ✗ & ✗ & ✓ & ✓ & ✓ & ✗ \\ RAP  & LLAMA-33b & ✗ & ✓ & ✗ & ✗ & ✗ & ✗ \\ AutoAct  & LLAMA2-70b & ✓ & ✗ & ✓ & ✓ & ✓ & ✗ \\ TPTU  & ChatGPT/IntermLM & ✗ & ✗ & ✓ & ✓ & ✓ & ✗ \\  AGILE (Ours) & Vicuna-13b/Meerkat-7b & ✓ & ✓ & ✓ & ✓ & ✓ & ✓ \\   

Table 8: Related work on LLM agents. AGILE stands out as the pioneering work that trains the entire agent using reinforcement learning, incorporating proactive human advice-seeking.

Human-agent interactionAlthough LLMs face practical challenges, such as hallucination  and a lack of long-tail knowledge , consulting human experts can help mitigate these issues. Several studies [52; 46] have incorporated human experts into agent workflows relying on passive feedback or predefined rules. However, these approaches do not involve proactively seeking advice, which requires more complex decision-making. While [5; 31] train models to ask questions using behavior cloning, they ignore the fact that the decision to seek advice must be based on the LLM's own knowledge and capabilities [55; 18; 13].  use a calibrated version of an LLM's token probabilities as a confidence measure, yet token probabilities tend to be overconfident , and existing calibration methods don't generalize well to our agent setting when the LLM makes multiple decisions in sequence. Ultimately, the challenge of seeking advice is tied to the LLM's self-evaluation, which is difficult to ground truth or optimize through SFT. In our RL framework, the value and cost of seeking advice can be directly represented as RL rewards, enabling the proactive skill of seeking advice to be optimized as part of the policy model on end-to-end RL training.

LLM agent benchmarksSeveral benchmarks have been designed to assess the capabilities of agents. For instance, the Webshop  and Mind2Web  datasets evaluate agents' tool usage and planning abilities within a web environment. HotPotQA  and TriviaQA  focus on agents' reasoning and tool usage for question answering. ALFWorld  examines planning and navigation skills, while ScienceWorld  provides an interactive text-based environment to evaluate agents' scientific aptitude. As illustrated in Table 9, despite these existing benchmarks, none comprehensively addresses all the core challenges of real-world agent applications, such as handling long-tail knowledge, human-agent interaction, long-term memory usage, tool usage, self-evaluation, and reflection. This motivated us to develop ProductQA.

## 6 Conclusion and future work

In this work, we introduce a novel reinforcement learning framework of LLM agents, called AGILE. First, the whole system of AGILE is trained end-to-end by reinforcement learning. Second, AGILE has the ability of seeking advice from external human experts. In addition, we develop a challenging dataset of complex QA, ProductQA, for comprehensive evaluation of an agent's capabilities. Extensive experiments demonstrate that within our framework, an agent based on a smaller model after RL training can outperform GPT-4.

AGILE is a general agent framework and we can certainly consider multiple extensions of it. An agent can be equipped with more tools, such as multimodal perception, manipulations in physical environments, logical reasoning, among others. We posit that AGILE's activities can be categorized into two distinct types: utilizing its LLM alone, and integrating the LLM with other tools. These two approaches conceptually align with the human cognitive processes known as System 1 and System 2 [15; 1]. Furthermore, AGILE's memory serves as a repository for the accumulation of experiences and knowledge, which is crucial for self-improvement. Consequently, AGILE offers an architecture for an very powerful agent that has the potential to attain human-level intelligence.

AGILE also includes interactions between the agent and external human experts. The framework can be extended to allow interactions with humans or machine agents in various roles such as students or teachers, and in different formats such as debates or coordination. Furthermore, AGILE can be employed in multi-agent systems.

  
**Datasets** & **Type** & **Fields** & **Size** &  **Long** \\ **Trajectory** \\  &  **Tool** \\ **Usage** \\  &  **Long-term** \\ **Knowledge** \\  & 
 **Cross** \\ **Task** \\  \\  Webshop  & Simulator & Web & 12,087 & ✗ & ✗ & ✗ & ✗ \\ Mind2Web  & Simulator & Web & 2,350 & ✗ & ✗ & ✗ & ✓ \\ ALFWorld  & Simulator & Navigation & 3,827 & ✗ & ✗ & ✗ & ✓ \\ ScienceWorld  & Simulator & Science & 7,207 & ✗ & ✗ & ✗ & ✗ \\ HotPotQA  & QA & Wikipedia & 112,779 & ✗ & ✓ & ✗ & ✗ \\ TriviaQA  & QA & Web & 95,956 & ✗ & ✓ & ✓ & ✗ \\  ProductQA (ours) & QA & E-commerce & 88,229 & ✓ & ✓ & ✓ & ✓ \\   

Table 9: Benchmarks for evaluating LLM agents. ProductQA features long trajectories, tool use, long-term knowledge accumulation, and cross-task capabilities.