# Revisiting Self-Supervised Heterogeneous Graph Learning from Spectral Clustering Perspective

Yujie Mo\({}^{1,2}\)   Zhihe Lu\({}^{2}\)   Runpeng Yu\({}^{2}\)   Xiaofeng Zhu\({}^{1}\)1   Xinchao Wang\({}^{2}\)1

\({}^{1}\)School of Computer Science and Engineering,

University of Electronic Science and Technology of China

\({}^{2}\)National University of Singapore

###### Abstract

Self-supervised heterogeneous graph learning (SHGL) has shown promising potential in diverse scenarios. However, while existing SHGL methods share a similar essential with clustering approaches, they encounter two significant limitations: (i) noise in graph structures is often introduced during the message-passing process to weaken node representations, and (ii) cluster-level information may be inadequately captured and leveraged, diminishing the performance in downstream tasks. In this paper, we address these limitations by theoretically revisiting SHGL from the spectral clustering perspective and introducing a novel framework enhanced by rank and dual consistency constraints. Specifically, our framework incorporates a rank-constrained spectral clustering method that refines the affinity matrix to exclude noise effectively. Additionally, we integrate node-level and cluster-level consistency constraints that concurrently capture invariant and clustering information to facilitate learning in downstream tasks. We theoretically demonstrate that the learned representations are divided into distinct partitions based on the number of classes and exhibit enhanced generalization ability across tasks. Experimental results affirm the superiority of our method, showcasing remarkable improvements in several downstream tasks compared to existing methods.

## 1 Introduction

Self-supervised heterogeneous graph learning (SHGL) aims to effectively process diverse types of nodes and edges in the heterogeneous graph, producing low-dimensional representations without the need for human annotations [72; 71; 25]. Thanks to its remarkable capabilities, SHGL has attracted significant interest and has been utilized in a broad array of applications, including recommendation systems [44; 12], social network analysis [45; 9], and molecule design [68; 59].

Existing SHGL methods can be broadly classified into two groups, _i.e.,_ meta-path-based methods and adaptive-graph-based methods. Meta-path-based methods typically utilize pre-defined meta-paths to explore relationships among nodes that may share the same label in the heterogeneous graph [18; 74]. However, building meta-paths requires extensive prior knowledge and incurs additional computation costs . To address these drawbacks, adaptive-graph-based methods dynamically assign significant weights to node pairs likely to share the same label, using the adaptive graph structure rather than traditional meta-paths . Both groups of SHGL methods facilitate message-passing among nodes within the same class, either through meta-path-based graphs or adaptive graph structures. As a result, this process minimizes intra-class differences and promotes a clustered pattern in the learned representations, aligning these methods closely with conventional clustering techniques.

Despite the effectiveness of previous SHGL methods, they encounter two limitations. First, previous methods conduct message-passing relying on meta-path-based graphs and adaptive graph structures, which inevitably include noise, _i.e.,_ connections among nodes from different classes . Consequently, such noise compromises the identifiability of node representations after the message-passing process. Second, while previous methods exhibit clustering characteristics, they typically emphasize the node-level consistency only, neglecting to capture and leverage the cluster-level information effectively . This may not fully exploit the potential benefits of clustering for representation learning, thereby diminishing the performance of downstream tasks.

Based on the above analysis, it is feasible to analyze previous SHGL methods from a clustering perspective thanks to their close connection to clustering techniques and further optimize the graph structures to mitigate noisy connections as well as harness the cluster-level information to enhance previous SHGL. To achieve this, there are three key challenges, _i.e.,_ (i) How to formally understand previous SHGL methods from the clustering perspective? (ii) With insights from the clustering analysis, how to learn an adaptive graph structure that effectively captures intra-class connections while filtering out inter-class noise? (iii) How to enable the effective incorporation of cluster-level information in the heterogeneous graph to boost the performance of downstream tasks?

In this paper, we address the outlined challenges by first theoretically revisiting previous SHGL methods from a clustering perspective and then introducing a novel framework, termed Spectral Clustering-inspired HeterOgeneOus graph Learning (SCHOOL for short), that incorporates rank-constrained spectral clustering and dual consistency constraints, as depicted in Figure 1. Specifically, we start by proving that existing SHGL can be reformulated as spectral clustering with an additional regularization term under the assumption of orthogonality, thus addressing **challenge (i)** and laying the foundational theory for our approach. Next, to tackle **challenge (ii)**, we propose an efficient spectral clustering technique that includes a rank constraint on the affinity matrix, aiming to effectively mitigate noisy connections among different classes. Furthermore, to resolve **challenge (iii)**, we design dual consistency constraints at both node and cluster levels to capture invariant and clustering information, respectively, which reduces the intra-cluster differences and enhances the performance of downstream tasks. Finally, theoretical analysis indicates that the learned representations are divided into distinct partitions corresponding to the number of classes, and are demonstrated superior generalization ability compared to those derived from previous SHGL methods.

Figure 1: The flowchart of SCHOOL, which first employs the Multi-Layer Perception \(g_{}\) to derive semantic representations \(\), followed by obtaining orthogonal cluster assignment matrix \(\) and orthogonal \(\). Subsequently, SCHOOL filters noisy connections by deriving the rank-constrained affinity matrix \(\), which is further used to multiply with \(\) and then obtain node representations \(\). Meanwhile, SCHOOL employs a heterogeneous encoder \(f_{}\) to aggregate information across node types, yielding heterogeneous representations \(}\). Finally, SCHOOL incorporates spectral loss \(_{sp}\) to optimize \(\) to fit eigenvectors of the Laplacian matrix of \(\). Moreover, SCHOOL designs node-level (_i.e.,_\(_{nc}\)) and cluster-level (_i.e.,_\(_{cc}\)) consistency constraints on projected representations (_i.e.,_\(\) and \(}\)) and cluster representations \(}\) to capture the invariant and clustering information, respectively.

Compared to previous SHGL works, our contributions can be summarized as follows:

* To the best of our knowledge, we make the first attempt to theoretically revisit previous SHGL methods from the spectral clustering perspective in a unified manner.
* We adaptively learn a rank-constrained affinity matrix to mitigate noisy connections inherent in previous SHGL methods. Moreover, we introduce dual consistency constraints to capture both invariant and clustering information to enhance the effectiveness of our method.
* We theoretically demonstrate that the proposed method divides the learned representations into distinct partitions based on the number of classes, instead of dimensions in previous SHGL methods. Furthermore, the representations obtained by this method exhibit enhanced generalization ability compared to those derived from previous SHGL methods.
* We experimentally manifest the effectiveness of the proposed method across a variety of downstream tasks, using both heterogeneous and homogeneous graph datasets, compared to numerous state-of-the-art methods.

## 2 Method

**Notations.** Let \(=(,,,,)\) represent a heterogeneous graph, where \(\) and \(\) indicate set of nodes and set of edges, respectively. \(=\{_{i}\}_{i=1}^{n}\) denotes the matrix of node features, where \(n\) indicates the number of nodes. Moreover, \(\) and \(\) indicate set of node types and set of edge types, respectively. Given the heterogeneous graph \(\), most existing SHGL methods utilize meta-paths or adaptive graph structures to explore connections among nodes within the same class, thus exhibiting the characteristics of clustering and obtaining discriminative representations. To gain a deeper insight of previous SHGL methods, we first propose to revisit them from a clustering perspective as follows.

### Revisiting Previous SHGL Methods from Spectral Clustering

As mentioned above, previous SHGL methods tend to conduct clustering implicitly, relying on meta-path-based graphs or adaptive graph structures. For example, given an academic heterogeneous graph with several node types (_i.e.,_ paper, author, and subject), for the meta-path-based methods, if two papers belong to the same class, there may exist a meta-path "paper-subject-paper" to connect them (_i.e.,_ two papers are grouped into the same subject). Similarly, for the adaptive-graph-based methods, when two papers belong to the same class, the adaptive graph structures likely assign large weights to connect them. Therefore, representations of nodes within the same class will be close to each other after the message-passing process, thus implicitly presenting a clustered pattern.

Based on the above observation, actually, we can further theoretically understand previous SHGL methods from the clustering perspective. To do this, we first give the following definition.

**Definition 2.1**.: _(Spectral Clustering) Given the Laplacian matrix \(\), the optimization problem of the spectral clustering can be described as follows:_

\[_{}(^{T}) { s.t., }^{T}=,\] (1)

_where \(=-\), \(^{n n}\) is a data similarity matrix, \(\) is a diagonal matrix whose entries are column sums of \(\), \(^{n d}\) is a representations matrix, \(()\) indicates the matrix trace, and \(\) indicates the identity matrix._

According to Definition 2.1, for both meta-path-based methods [31; 57; 58] and adaptive-graph-based SHGL methods , we then have Theorem 2.2, whose proof can be found in Appendix C.1.

**Theorem 2.2**.: _Assume the learned representations \(\) are orthogonal, optimizing previous meta-path-based and adaptive-graph-based SHGL methods is equivalent to performing spectral clustering with additional regularization, i.e.,_

\[_{}_{SHGL}_{}( ^{T}})+R()^{T}=,\] (2)

_where \(R()\) indicates the regularization term, \(}\) indicates the Laplacian matrix of the meta-path-based graph or the adaptive graph structure._Theorem 2.2 reveals the connection between previous SHGL and the spectral clustering as well as indicates that previous SHGL heavily relies on the Laplacian matrix of meta-path-based graph or adaptive graph structure. Moreover, based on Theorem 2.2, we can further bridge previous SHGL and the graph-cut algorithm  as follows, whose proof can be found in Appendix C.2.

**Theorem 2.3**.: _Under the same assumption in Theorem 2.2, optimizing previous meta-path-based and adaptive-graph-based SHGL methods is approximate to performing the \((V_{1},,V_{d})\) algorithm that divides the learned representations into \(d\) partitions \(\{V_{1},,V_{d}\}\), i.e.,_

\[_{}_{SHGL}_{} (V_{1},,V_{d}),\] (3)

_where \(d\) indicates the dimension of representations \(\)._

Theorem 2.3 further indicates that previous SHGL methods divide the learned representations into \(d\) partitions, where \(d\) is generally much larger than the number of classes. Therefore, Theorem 2.3 connects the traditional graph-cut algorithm with existing SHGL methods, which requires custom analysis. As a result, we theoretically revisit previous SHGL methods from spectral clustering as well as graph-cut perspectives and build the connections between them, thus solving the challenge (i).

### Rank-Constrained Spectral Clustering

Based on the connections between previous SHGL methods and the spectral clustering as well as the graph-cut algorithm, we have the observations as follows. First, according to Theorem 2.2, previous SHGL methods conduct spectral clustering based on the Laplacian matrix of meta-path-based graph or adaptive graph structure, which may not guarantee optimality and could potentially contain noisy connections, thus affecting the spectral clustering. Second, according to Theorem 2.3, previous SHGL methods conduct the graph-cut to divide the learned representations into \(d\) partitions, which are generally not equal to the number of classes \(c\). As a result, optimizing previous SHGL methods becomes a hard or even error problem, and the learned representations can not be clustered well. Therefore, it is intuitive to mitigate noise in the adaptive graph structure as well as divide the learned representations into exactly \(c\) partitions to improve existing SHGL methods.

Specifically, in this paper, we propose to learn an adaptive affinity matrix with the rank constraint to mitigate noisy connections as much as possible. To do this, we first employ the Multi-Layer Perceptron (MLP) as the encoder \(g_{}^{f d_{1}}\) to obtain the semantic representations \(\) by:

\[=(g_{}()),\] (4)

where \(f\) and \(d_{1}\) are the dimensions of node features and representations, respectively, and \(\) is the activation function. After that, we propose to learn an adaptive affinity matrix \(^{n n}\) based on the semantic representations. Intuitively, in a uncorrelated representations subspace, a smaller distance \(\|_{i}-_{j}\|_{2}^{2}\) between semantic representations should be assigned a larger probability \(_{ij}\). Therefore, it is a natural approach to learn the affinity matrix \(\) by:

\[_{i,j=1}^{n}(\|_{i}-_{j}\|_{2}^{2} _{ij}+_{ij}^{2})\ \  i,_{i}^{T}=1,0 _{i} 1,\] (5)

where \(\) is a non-negative parameter. In Eq. (5), the first term encourages the affinity matrix to assign large weights to node pairs with small distances. Moreover, the second term avoids the trivial solution that only the nearest node can be the neighbor of \(v_{i}\) with probability 1. However, similar to previous SHGL methods, usually the affinity matrix learned by Eq. (5) can not reach the ideal case (_i.e.,_ having no noisy connections among different classes and containing exactly \(c\) connected components). As a result, the noisy connections in the affinity matrix may induce a negative interference during the message-passing process. To solve this issue, we first introduce the following lemma in .

**Lemma 2.4**.: _The multiplicity \(c\) of the eigenvalue 0 of the Laplacian matrix \(}\) is equal to the number of connected components in the affinity matrix \(\)._

Lemma 2.4 indicates that if the rank of \(}\) equals to \(n-c\), then the affinity matrix \(\) contains exactly \(c\) connected components to achieve the ideal scenario, where \(}=-+^{T}}{2}\), and \(\) is the degree matrix of \(+^{T}}{2}\). Based on Lemma 2.4, we can solve the above issue by adding the rank constraint on the affinity matrix, _i.e.,_ enforcing the smallest \(c\) eigenvalues of \(}\) to be 0:

\[(})=n-c_{i=1}^{c}_{i}( }),\] (6)where \(_{i}(})\) is the \(i\)-th smallest eigenvalue of \(}\) and \(_{i}(}) 0\) because \(}\) is positive semi-definite. Moreover, according to Ky Fan's Theorem , the constraint in Eq. (6) can be rewritten in the spectral clustering form as follows (derivation listed in Appendix C.5).

\[_{i=1}^{c}_{i}(})=_{^{T}=}(^{T}})=_{ ^{T}=}_{i,j}_{ij} \|_{i}-_{j}\|_{2}^{2},\] (7)

where \(^{n c}\) is formed by part eigenvectors of \(}\) after the eigendecomposition. Therefore, based on Eq. (7), we can add the rank constraint on the affinity matrix and reformulate Eq. (5) as:

\[_{,}_{i,j=1}^{n}(\| _{i}-_{j}\|_{2}^{2}_{ij}+ _{ij}^{2}+\|_{i}-_{j}\|_{2}^{2}_{ij})\\ , i,_{i}^{T}=1,0_{i} 1,^{T}=,\] (8)

where \(\) is a non-negative parameter. Eq. (8) can be solved by applying the alternating optimization approach. Specifically, when \(\) is fixed, Eq. (8) becomes \(_{^{T}=}_{i,j=1}^{n}\|_{i }-_{j}\|_{2}^{2}_{ij}=2_{^{T}=}(^{T}})\), whose optimal solution \(\) is the eigenvectors of \(}\) corresponding to the \(c\) smallest eigenvalues. When \(\) is fixed, denote \(_{ij}=\|_{i}-_{j}\|_{2}^{2}+ \|_{i}-_{j}\|_{2}^{2}\), Eq. (8) can be written in the vector form, _i.e.,_

\[_{_{i}^{T}=1,0_{i} 1},\| _{i}+_{i}\|_{2}^{2}.\] (9)

According to the Lagrangian function of Eq. (9) and the KKT condition , we can obtain the closed-form solution of the element \(_{ij}\) in the affinity matrix, _i.e.,_

\[_{ij}=(-_{ij}+)_{+},\] (10)

where \(\) is a non-negative parameter, and \(()_{+}\) indicates \(\{,0\}\). In practice, a sparse affinity matrix usually obtains better performance and reduces computation costs. Therefore, we only calculate \(_{ij}\) between node \(v_{i}\) and its \(k\) nearest neighbors. Then parameters \(\) and \(\) can be further determined, details listed in Appendix C.6.

However, when fixing \(\) and optimizing \(\) in Eq. (8), computation costs of obtaining eigenvectors \(\) remain prohibitive due to the cubic time complexity of the eigendecomposition. To address this issue, we propose to replace the eigendecomposition with a projection head and orthogonal layer . Specifically, we first employ the projection head \(p_{}^{d_{1} c}\) to map semantic representations to the cluster assignment space \(^{n c}\), _i.e.,_

\[=(p_{}()),\] (11)

where \(\) is the activation function. After that, we employ an orthogonal layer to derive the orthogonal cluster assignment matrix \(^{n c}\) (orthogonal derivation listed in Appendix C.7), _i.e.,_

\[=(^{-1}),\] (12)

where \(\) is an upper triangular matrix obtained from the QR decomposition  (_i.e.,_\(=\) and \(^{T}=\)) on the full rank \(\). Similarly, we can also implement the orthogonal layer to achieve the uncorrelation (_i.e.,_\(^{T}=\)) on the representations subspace.

As a result, the projection head and the orthogonal layer act as a linear transformation to achieve the orthogonality constraint on \(\). To replace the eigendecomposition, the cluster assignment matrix \(\) should further fit the eigenvectors \(\) in Eq. (8). To do this, we design a spectral loss \(_{sp}\) to optimize the parameters in \(p_{}\) to simulate the spectral clustering of the third term in Eq. (8), _i.e.,_

\[_{sp}=}_{i,j=1}^{n}_{ij}\|_{i}-_{j}\|_{2}^{2}- H(),\] (13)

where \(\) is a non-negative parameter, \(H()=-_{i=1}^{c}P(^{i})P(^{i})\) is the entropy of cluster assignment probabilities \(P(^{i})=_{j=1}^{n}_{j}^{i}\), and \(_{j}^{i}\) indicates the \(i\)-th column and \(j\)-th row of \(\). According to Eq. (7), the first term in Eq. (13) simulates the spectral clustering to enforce the learned cluster assignment matrix \(\) to approximate eigenvectors \(\), and the second term is a widely used regularization term [1; 14] to avoid the trivial solution that most nodes are assigned to the same cluster. Therefore, when \(\) is fixed, we optimizing \(\) to approach the optimal \(\) by achieving orthogonality with Eq. (12), and fitting eigenvectors \(\) with Eq. (13). As a result, this reduces the cubic time complexity of eigendecomposition to \((nd^{2}+nc^{2}+nkd+nkc+c^{3}+d^{3})\), where \(d^{2},c^{2}<n\), thus is linear to the sample size, details are shown in Appendix B.2. Finally, the proposed method still optimizes the affinity matrix \(\) and eigenvectors \(\) in Eq. (8) in an alternating approach, _i.e.,_ fix \(\) and then obtain the closed-form solution of \(\), and fix \(\) and then optimize parameters (_i.e.,_\(g_{}\) and \(p_{}\)) to update \(\) to approach the optimal \(\).

Therefore, the proposed method obtains the affinity matrix with exactly \(c\) connected components to mitigate noisy connections in an effective and efficient way. Then we can obtain the node representations \(=\), which is expected to conduct message-passing among nodes within the same class. Moreover, we can bridge the connections between the proposed method and the spectral clustering as well as the graph-cut algorithm as follows, whose proof can be found in Appendix C.3.

**Theorem 2.5**.: _Optimizing the spectral loss \(_{sp}\) leads to performing the spectral clustering based on the affinity matrix \(\) with \(c\) connected components and conducting RatioCut (\(V_{1},,V_{c}\)) algorithm to divide the learned representations into \(c\) partitions, i.e.,_

\[_{sp}(^{T}})(V_{1},,V_{c}).\] (14)

Theorem 2.5 indicates that the proposed method conducts the spectral clustering as previous SHGL methods, but is performed on an affinity matrix with exactly \(c\) connected components (verified in Section 3.2.3), thus mitigating noisy connections from different classes and solving the challenge (ii). Moreover, the proposed method divides the learned representations into \(c\) partitions, which is a better optimization goal than previous SHGL methods to obtain discriminative representations.

### Dual Consistency Constraints

The message-passing among nodes within the same class reduces intra-class differences and enhances node representations \(\). Meanwhile, the message-passing among nodes from different types also contributes to obtaining task-related contents and benefits downstream tasks . To do this, we propose to aggregate the information of nodes from different types in the heterogeneous graph with a heterogeneous encoder \(f_{}^{f d_{1}}\).

Specifically, for the node \(v_{i}\), we concatenate the information of itself and its relevant one-hop neighbors (_i.e.,_ nodes of other types) based on edge types in \(\), and then derive the heterogeneous representations \(}\) by:

\[}_{i}=|}_{r}(f _{}(_{i})||_{v_{j}_{i,r}}f_{}( _{j})),\] (15)

where \(\) is the activation function, \(||\) indicates the number of edge types, \(_{i,r}\) indicates the set of one-hop neighbors of node \(v_{i}\) based on the edge type \(r\), \(f_{}()\) indicates the linear transformation, and \(||\): indicates the concatenation operation. Therefore, the heterogeneous representations \(}\) aggregate the information of nodes from different types to introduce more task-related contents.

Given node representations \(\) and heterogeneous representations \(}\), most previous SHGL methods utilize the node-level consistency constraint (_e.g.,_ Info-NCE loss ) to capture the invariant information between them and enhance the effectiveness [57; 30]. In addition, according to Theorem 2.2, previous SHGL methods actually perform spectral clustering to learn node representations. However, previous SHGL methods fail to utilize the cluster-level information outputted by the spectral clustering, thus weakening the downstream task performance. To solve this issue, we design dual consistency constraints to capture the invariant information as well as the clustering information between \(\) and \(}\).

Specifically, we first employ a projection head \(q_{}^{d_{1} d_{2}}\) to map both \(\) and \(}\) into the same latent space, _i.e.,_\(=q_{}()\) and \(}=q_{}(})\), where \(d_{2}\) is the projected dimension. Then we follow previous works [57; 58] to design a node-level consistency constraint to capture the invariant information between \(\) and \(}\), _i.e.,_

\[_{nc}=\|-}\|_{F}^{2}+_{i,j=1} ^{d}e^{_{ij}},\] (16)where \(=^{T}+}^{T}}\), and \(\) is a non-negative parameter. Similar to previous works, the first term in Eq. (16) enforces representations in \(}\) agree with the corresponding representations in \(\), thus capturing the invariant information between them. The second term enables different dimensions of \(\) and \(}\) to uniformly distribute over the latent space to avoid collapse.

In addition to the node-level consistency constraint, we further design the cluster-level consistency constraint to capture the clustering information from the cluster assignment matrix \(\). To do this, we first obtain the cluster indicator matrix \(}\) based on the cluster assignment matrix \(\), _i.e.,_\(}=*{argmax}()\). After that, we conduct average pooling on node representations that possess the same cluster indicator to obtain the \(j\)-th cluster representation \(}_{j}\), _i.e.,_

\[}_{j}=|}_{v_{i} V_{j}}_{i},\] (17)

where \(V_{j}\) indicates the set of nodes whose cluster indicators equal to \(j\), and \(|V_{j}|\) indicates the number of nodes in \(V_{j}\). Then we design a cluster-level consistency constraint on cluster representations \(}\) and projected representations \(}\) to capture the clustering information, _i.e.,_

\[_{cc}=_{i=1}^{n}\|}_{i}-}_{y_{i }}\|_{2}^{2},\] (18)

where \(}_{y_{i}}\) indicates the cluster representation whose label equals to \(_{i}\). Eq. (18) enables the projected representation \(}_{i}\) and the cluster representation \(}_{y_{i}}\) to align each other. As a result, representations capture the clustering information based on cluster indicators and reduce intra-cluster differences to improve the performance of downstream tasks, thus solving challenge (iii).

We integrate the spectral loss in Eq. (13), the node-level consistency constraint in Eq. (16), with the cluster-level consistency constraint in Eq. (18) to have the objective function as:

\[=_{sp}+_{nc}+_{cc},\] (19)

where \(\) and \(\) are non-negative parameters. Finally, we concatenate node representations \(\) with heterogeneous representations \(}\) to obtain representations for downstream tasks. Actually, for the learned representations, we have the following Theorem, whose proof can be found in Appendix C.4.

**Theorem 2.6**.: _The proposed method with dual consistency constraints achieves a lower boundary of the model complexity \(C\) and a higher generalization ability boundary \(G\) than previous SHGL with the node-level consistency constraint only, i.e.,_

\[(C_{SCHOOL})<(C_{SHGL}),(G_{SCHOOL})>(G_{SHGL}),\] (20)

_where \(()\) and \(()\) indicates lower bound and upper bound, respectively._

Theorem 2.6 indicates that the representations learned by the dual consistency constraints can be theoretically proved to exhibit superior generalization ability than the representations learned by previous SHGL methods with the node-level consistency constraint only, thus are expected to perform better in different downstream tasks (verified in Section 3.2).

## 3 Experiments

In this section, we conduct experiments on both heterogeneous and homogeneous graph datasets to evaluate the proposed method in terms of different downstream tasks (_i.e.,_ node classification and node clustering), compared to both heterogeneous and homogeneous graph methods. Detailed settings are shown in Appendix D, and additional results are shown in Appendix E.

### Experimental Setup

#### 3.1.1 Datasets

The used datasets include four heterogeneous graph datasets and two homogeneous graph datasets. Heterogeneous graph datasets include three academic datasets (_i.e.,_ ACM , DBLP , and Aminer ), and one business dataset (_i.e.,_ Yelp ). Homogeneous graph datasets include two sale datasets (_i.e.,_ Photo and Computers ).

#### 3.1.2 Comparison Methods

The comparison methods include eleven heterogeneous graph methods and twelve homogeneous graph methods. The former includes two semi-supervised methods (_i.e.,_ HAN  and HGT ), one traditional unsupervised method (_i.e.,_ Mp2vec ), and eight self-supervised methods (_i.e.,_ DMGI , DMGIattn , HDMI , HeCo , HGCML , CPIM , HGMAE , and HERO ). The latter includes two semi-supervised methods (_i.e.,_ GCN  and GAT ), one traditional unsupervised method (_i.e.,_ DeepWalk ), and nine self-supervised methods, (_i.e.,_ DGI , GMI , MVGRL , GRACE , GCA , G-BT , COSTA , DSSL , and LRD ).

For a fair comparison, we follow  to select meta-paths for previous meta-path-based SHGL methods. Moreover, we follow  to implement homogeneous graph methods on heterogeneous graph datasets by separately learning the representations of each meta-path-based graph and further concatenating them for downstream tasks. In addition, we replace the heterogeneous encoder \(f_{}\) with GCN to implement the proposed method on homogeneous graph datasets because there is only one node type in the homogeneous graph. Moreover, we follow previous works  to generate two different views for the homogeneous graph by removing edges and masking features. The code of the proposed method is released at https://github.com/YujieMo/SCHOOL.

### Results Analysis

#### 3.2.1 Effectiveness on Heterogeneous and Homogeneous Graph

We first evaluate the effectiveness of the proposed method on the heterogeneous graph datasets and report the results of node classification and node clustering in Table 1 and Appendix E, respectively. Obviously, the proposed method obtains better performance on both node classification and node clustering tasks than comparison methods.

Specifically, first, for the node classification task, the proposed method consistently outperforms the comparison methods by large margins. For example, the proposed method on average, improves by 1.1%, compared to the best SHGL method (_i.e.,_ HERO), on four heterogeneous graph datasets. The reason can be attributed to the fact that the proposed method adaptively learns a rank-constrained affinity matrix to mitigate noisy connections among different classes, thus reducing intra-class differences. Second, for the node clustering task, the proposed method also obtains promising improvements. For example, the proposed method on average, improves by 3.1%, compared to the best SHGL method (_i.e.,_ HGMAE), on four heterogeneous graph datasets. This demonstrates the superiority of the proposed method, which simulates the spectral clustering with the spectral loss and conducts the cluster-level consistency constraint to further utilize the clustering information. As a result, the effectiveness of the proposed method is verified on different downstream tasks.

    &  &  &  &  \\   & Macro-F1 & Micro-F1 & Macro-F1 & Micro-F1 & Macro-F1 & Micro-F1 & Macro-F1 & Micro-F1 \\  DeepWalk & 73.9\(\)0.3 & 74.1\(\)0.1 & 68.7\(\)1.1 & 73.2\(\)0.9 & 88.1\(\)0.2 & 89.5\(\)0.3 & 54.7\(\)0.8 & 59.7\(\)0.7 \\ GCN & 86.9\(\)0.2 & 87.0\(\)0.3 & 85.0\(\)0.6 & 87.4\(\)0.8 & 90.2\(\)0.2 & 90.9\(\)0.5 & 64.5\(\)0.7 & 71.5\(\)0.9 \\ GAT & 85.0\(\)0.4 & 84.9\(\)0.3 & 86.4\(\)0.5 & 88.2\(\)0.7 & 91.0\(\)0.4 & 92.1\(\)0.2 & 63.8\(\)0.4 & 70.6\(\)0.7 \\  Mp2vec & 87.6\(\)0.5 & 88.1\(\)0.3 & 78.2\(\)0.8 & 83.6\(\)0.9 & 85.7\(\)0.3 & 87.6\(\)0.6 & 58.7\(\)0.5 & 65.3\(\)0.6 \\ HAN & 89.4\(\)0.2 & 89.2\(\)0.2 & 90.5\(\)1.2 & 90.7\(\)1.4 & 91.2\(\)0.4 & 92.0\(\)0.5 & 65.3\(\)0.7 & 72.8\(\)0.4 \\ HGT & 91.5\(\)0.7 & 91.6\(\)0.6 & 89.9\(\)0.5 & 90.2\(\)0.6 & 90.9\(\)0.6 & 91.7\(\)0.8 & 64.5\(\)0.5 & 71.0\(\)0.7 \\ DMGI & 89.8\(\)0.1 & 89.8\(\)0.1 & 82.9\(\)0.8 & 85.8\(\)0.9 & 92.1\(\)0.2 & 92.9\(\)0.3 & 63.8\(\)0.4 & 67.6\(\)0.5 \\ DMGIattn & 88.7\(\)0.3 & 88.7\(\)0.5 & 82.8\(\)0.7 & 85.4\(\)0.5 & 90.9\(\)0.2 & 91.8\(\)0.3 & 62.4\(\)0.9 & 66.8\(\)0.8 \\ HDMI & 90.1\(\)0.3 & 90.1\(\)0.3 & 80.7\(\)0.6 & 84.0\(\)0.9 & 91.3\(\)0.2 & 92.2\(\)0.5 & 65.9\(\)0.4 & 71.7\(\)0.6 \\ HeCo & 88.3\(\)0.3 & 88.2\(\)0.2 & 85.3\(\)0.7 & 87.9\(\)0.6 & 91.0\(\)0.3 & 91.6\(\)0.2 & 71.8\(\)0.9 & 78.6\(\)0.7 \\ HGCML & 90.6\(\)0.7 & 90.7\(\)0.5 & 90.7\(\)0.8 & 91.0\(\)0.7 & 91.9\(\)0.8 & 93.2\(\)0.7 & 70.5\(\)0.4 & 76.3\(\)0.6 \\ CPIM & 91.4\(\)0.3 & 91.3\(\)0.2 & 90.2\(\)0.5 & 90.3\(\)0.4 & 93.2\(\)0.6 & 93.8\(\)0.8 & 70.1\(\)0.9 & 75.8\(\)1.1 \\ HGMAE & 90.5\(\)0.5 & 90.6\(\)0.7 & 90.5\(\)0.7 & 90.7\(\)0.5 & 92.9\(\)0.5 & 93.4\(\)0.6 & 72.3\(\)0.9 & 80.3\(\)1.2 \\ HERO & 92.2\(\)0.5 & 92.1\(\)0.7 & 92.4\(\)0.7 & 92.3\(\)0.6 & 93.8\(\)0.6 & 94.4\(\)0.4 & 75.1\(\)0.7 & 84.5\(\)0.9 \\ SCHOOL & **92.7\(\)0.6** & **92.6\(\)0.5** & **93.0\(\)0.7** & **92.8\(\)0.4** & **94.0\(\)0.3** & **94.7\(\)0.4** & **77.5\(\)0.9** & **86.8\(\)0.7** \\   

Table 1: Classification performance (_i.e.,_ Macro-F1 and Micro-F1) on heterogeneous graph datasets.

We further evaluate the effectiveness of the proposed method on homogeneous graph datasets and report the results of node classification in Appendix E. We can observe that the proposed method also achieves competitive results on the homogeneous graph datasets compared to other homogeneous graph methods. For example, the proposed method outperforms the best self-supervised method (_i.e.,_ LRD), on almost all homogeneous graph datasets. This indicates that the proposed method is also available to learn the noise-free affinity matrix for homogeneous graphs as well as capture invariant and clustering information to benefit downstream tasks. Therefore, the effectiveness of the proposed method is verified on both heterogeneous and homogeneous graph datasets.

#### 3.2.2 Ablation Study

The proposed method investigates the objective function \(\) to learn the rank-constrained affinity matrix, as well as capture invariant and clustering information. To verify the effectiveness of each component of \(\) (_i.e.,_\(_{sp}\), \(_{nc}\), and \(_{cc}\)), we investigate the performance of all variants on the node classification task and report the results in Table 2.

From Table 2, we have the observations as follows. First, the proposed method with the complete objective function obtains the best performance. For example, the proposed method on average improves by 1.8%, compared to the best variant (_i.e.,_ without \(_{nc}\)), indicating that all components in the objective function are necessary for the proposed method. This is consistent with our claims, _i.e.,_ it is essential to optimize the adaptive graph structure to mitigate noisy connections as well as utilize the cluster-level information to benefit downstream tasks. Second, the variant without \(_{sp}\) achieves inferior results to the other two variants (_i.e.,_ without \(_{nc}\) and without \(_{cc}\), respectively). This can be attributed to the fact that the spectral loss \(_{sp}\) enforces the cluster assignment matrix to fit the eigenvectors, which is necessary for the closed-form solution of the affinity matrix.

#### 3.2.3 Visualization

To verify the effectiveness of the learned affinity matrix and the representations for downstream tasks, we visualize the affinity matrix in the heatmap and visualize the representations with t-SNE  on DBLP and Aminer datasets and report the results in Figure 2.

Specifically, we randomly sample 50 nodes in each class and then visualize elements of the affinity matrix \(\) among sampled nodes with the heatmap, where rows and columns are reordered by node labels. In the correlation map, the darker a pixel, the larger the value of the element of \(\). In Figures 2(a) and 2(c), the heatmaps exhibit that there are nearly \(c\) (_i.e.,_ the number of classes) components in the affinity matrix, and almost all elements with large values fall in the block diagonal structure. This indicates that the affinity matrix indeed contains \(c\) connected components to mitigate noisy connections among different classes. Moreover, the t-SNE visualization in Figures 2(b) and 2(d) further indicate that the learned representations can be well divided into \(c\) partitions. This is consistent with the observation in Theorem 2.5 and verifies the effectiveness of the learned representations.

## 4 Conclusion

In this paper, we revisited previous SHGL methods from the perspective of spectral clustering and then introduced a novel framework to alleviate existing issues. Specifically, we first proved that optimizing

   _{sp}\)} & _{nc}\)} & _{cc}\)} &  &  &  &  \\   & & & Macro-F1 & Micro-F1 & Macro-F1 & Micro-F1 & Macro-F1 & Micro-F1 & Macro-F1 & Micro-F1 \\  \(-\) & \(-\) & \(\) & 85.9\(\)0.3 & 85.8\(\)0.6 & 91.8\(\)0.6 & 91.3\(\)0.5 & 91.0\(\)0.2 & 92.1\(\)0.4 & 66.8\(\)0.7 & 75.5\(\)0.9 \\ \(-\) & \(\) & \(-\) & 88.8\(\)0.6 & 88.6\(\)0.7 & 92.5\(\)0.7 & 92.1\(\)0.4 & 91.7\(\)0.4 & 92.7\(\)0.5 & 72.4\(\)0.5 & 80.3\(\)0.7 \\ \(\) & \(-\) & \(-\) & 87.6\(\)0.3 & 87.5\(\)0.5 & 92.3\(\)0.8 & 92.0\(\)0.6 & 90.7\(\)0.6 & 91.7\(\)0.6 & 67.3\(\)0.6 & 74.7\(\)0.5 \\ \(-\) & \(\) & 86.9\(\)0.7 & 86.7\(\)0.5 & 92.1\(\)0.3 & 91.5\(\)0.4 & 93.4\(\)0.8 & 94.2\(\)0.6 & 75.2\(\)0.4 & 83.9\(\)0.7 \\ \(\) & \(-\) & \(\) & 89.0\(\)0.5 & 88.9\(\)0.4 & 92.4\(\)0.5 & 92.0\(\)0.3 & 93.5\(\)0.6 & 94.2\(\)0.4 & 76.2\(\)0.5 & 85.2\(\)0.8 \\ \(\) & \(\) & \(-\) & 88.9\(\)0.7 & 88.8\(\)0.6 & 92.6\(\)0.6 & 92.3\(\)0.5 & 91.9\(\)0.7 & 92.8\(\)0.8 & 77.1\(\)0.8 & 86.0\(\)0.6 \\ \(\) & \(\) & \(\) & **92.7\(\)0.6** & **92.6\(\)0.5** & **93.0\(\)0.7** & **92.8\(\)0.4** & **94.0\(\)0.3** & **94.7\(\)0.4** & **77.5\(\)0.9** & **86.8\(\)0.7** \\   

Table 2: Classification performance (_i.e.,_ Macro-F1 and Micro-F1) of each component in the objective function \(\) on all heterogeneous graph datasets.

previous SHGL methods is equivalent to performing spectral clustering with additional regularization under the orthogonalization assumption. Then we proposed an efficient spectral clustering method with the rank constraint to learn an adaptive affinity matrix and mitigate noisy connections in previous methods. Moreover, we designed node-level and cluster-level consistency constraints to capture invariant and clustering information, thus benefiting the performance of downstream tasks. Theoretical analysis indicates that the learned representations are divided into distinct partitions based on the number of classes, and are expected to achieve better generalization ability than representations of previous SHGL methods. Comprehensive experiments verify the effectiveness of the proposed method on both homogeneous and heterogeneous graph datasets on different downstream tasks.

**Potential limitations and broader impact.** Our potential limitation is that this work is designed based on node features. However, in heterogeneous graphs, instances arise where nodes are devoid of features. While one-hot vectors or structural embeddings can be designated as node features to tackle this problem, we recognize the necessity of devising dedicated techniques tailored for heterogeneous graphs with missing node features. In addition, the proposed method can also be used to deal with the homophily problem, which aims to explore the connections within the same class. We consider these aspects as potential directions for future research. Despite the great development of SHGL, some theoretical foundations are still lacking. Our work theoretically connects existing SHGL methods and spectral clustering and may open a new path to understanding and designing SHGL. Besides that, we do not foresee any direct negative impacts on the society.