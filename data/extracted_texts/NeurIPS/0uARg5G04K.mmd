# The Adversarial Consistency of Surrogate Risks for Binary Classification

Natalie S. Frank

Courant Institute

New York University

New York, NY 10012

nf1066@nyu.edu &Jonathan Niles-Weed

Courant Institute

New York University

New York, NY 10012

jnw@cims.nyu.edu

###### Abstract

We study the consistency of surrogate risks for robust binary classification. It is common to learn robust classifiers by adversarial training, which seeks to minimize the expected \(0\)-\(1\) loss when each example can be maliciously corrupted within a small ball. We give a simple and complete characterization of the set of surrogate loss functions that are _consistent_, i.e., that can replace the \(0\)-\(1\) loss without affecting the minimizing sequences of the original adversarial risk, for any data distribution. We also prove a quantitative version of adversarial consistency for the \(\)-margin loss. Our results reveal that the class of adversarially consistent surrogates is substantially smaller than in the standard setting, where many common surrogates are known to be consistent.

## 1 Introduction

A central issue in the study of neural nets is their susceptibility to adversarial perturbations--perturbations imperceptible to the human eye can cause a neural net to misclassify an image (Szegedy et al., 2013; Biggio et al., 2013). The same phenomenon appears in other types of data such as speech and text. As deep nets are used in applications such as self-driving cars and medical imaging (Paschali et al., 2018; Li et al., 2021), training classifiers robust to adversarial perturbations is a central question in machine learning.

The foundational theory of surrogates for classification in well understood. In the standard classification setting, one seeks to minimize the _classification_ risk-- the proportion of incorrectly classified data. Since minimizing the classification risk is typically computationally intractable (Ben-David et al., 2003), a common approach is to instead minimize a better-behaved alternative called the _surrogate risk_. However, one must verify that classifiers with low surrogate risk also achieve low classification risk. If for every data distribution, a sequence of functions minimizing the surrogate also minimizes the classification risk, the surrogate risk is called _consistent_. Many classic papers study the consistency of surrogate risks in the standard classification setting (Bartlett et al., 2006; Lin, 2004; Steinwart, 2007; Philip M. Long, 2013; Mingyuan Zhang, 2020).

Unlike the standard case, however, little is known about the consistency of surrogate risks in the context of adversarial training, which involves risks that compute the supremum of a surrogate loss function over an \(\)-ball. Though this question has been partially studied in the literature (Awasthi et al., 2021; Awasthi et al., 2021; Meunier et al., 2022), a general theory is lacking. Existing results reveal, however, that the situation is substantially different from the standard case: for instance, (Meunier et al., 2022) show that no _convex_ surrogate can be adversarially consistent. To our knowledge, no adversarially consistent risks are known.

In this work, we give a complete characterization of adversarial consistency for surrogate losses.

#### Our Contributions:

* In Section 4 we give a surprisingly simple necessary and sufficient condition for adversarial consistency: **Informal Theorem**.: _Under reasonable assumptions on the surrogate loss \(\), the supremum-based \(\)-risk is adversarially consistent if and only if \(_{}()/2+(-)/2<(0)\). In particular, this result proves consistency for any loss function that is _not_ midpoint convex at the origin.
* In Section 5, we specialize to the case of the \(\)-margin loss, where we obtain a quantitative proof of adversarial consistency by explicitly bounding the excess adversarial risk.

To the best of the authors' knowledge, this paper is the first to prove that a loss-based learning procedure is consistent for a wide range of distributions in the adversarial setting. As mentioned above, the \(\)-margin loss \(_{}()=(1,(1-/,0))\) satisfies the conditions of Informal Theorem above, as does the shifted sigmoid loss \(_{}()=1/(1+(-))\) with \(>0\), which confirms a conjecture of Meunier et al. (2022). By contrast, all convex losses satisfy \(_{}()/2+(-)/2=(0),\) and are therefore not adversarially consistent.

In addition to consistency, one would hope to obtain a quantitative comparison between the adversarial surrogate risk and the adversarial classification risk. Our bound in Section 5 shows that the excess error of the adversarial \(\)-margin loss is a linear upper bound on the adversarial classification error, which implies that minimizing the adversarial \(\)-margin loss is an effective procedure for minimizing the adversarial classification error. Extending the bound in Section 5 to further losses remains an open question.

## 2 Related Works

Many previous works have studied the consistency of surrogate risks (Bartlett et al., 2006; Lin, 2004; Steinwart, 2007; Philip M. Long, 2013; Mingyuan Zhang, 2020). The classic papers by (Bartlett et al., 2006; Lin, 2004; Zhang, 2004) explore the consistency of surrogate risks over all measurable functions. The works (Philip M. Long, 2013; Mingyuan Zhang, 2020; Awasthi et al., 2022) study \(\)-consistency, which is consistency restricted to a smaller set of functions. Steinwart (2007) generalizes some of these results into a framework referred to as _calibration_. Awasthi et al. (2021); Bao et al. (2021); Awasthi et al. (2021); Meunier et al. (2022) then use this framework to analyze the calibration of adversarial surrogate losses. Furthermore Meunier et al. (2022) relate calibration to consistency for adversarial losses in certain cases -- they show that no convex loss is adversarially consistent. They also conjecture that a class of surrogate losses called the _odd shifted_ losses are adversarially consistent. Meunier et al. (2022) also show that in a restricted setting, surrogates are consistent for 'optimal attacks'. The proof of our result formalizes this intuition. Simultaneous work (Mao et al., 2023) shows that the \(\)-margin loss is adversarially \(\)-consistent for typical function classes. Lastly, Bhattacharjee and Chaudhuri (2020, 2021) use a different set of techniques to study the consistency of non-parametric methods in adversarial scenarios.

Our results rely on recent works establishing the properties of minimizers to surrogate adversarial risks. (Awasthi et al., 2021; Pydi and Jog, 2021; Bungert et al., 2021) all proved the existence of minimizers to the adversarial risk and (Pydi and Jog, 2021) proved a minimax theorem for the zero-one loss. Building on the work of (Pydi and Jog, 2021), (Frank and Niles-Weed, 2023) later proved similar existence and minimax statements for arbitrary surrogate losses. Trillos et al. (2022, 2023) extend some of these results to the multiclass case. Lastly, (Trillos and Murray, 2020) study further properties of the minimizers to the adversarial classification loss.

## 3 Problem Setup

This section contains the necessary background for our results. Section 3.1 gives precise definitions for the main concepts, and Section 3.2 describes the minimax theorems that are at the heart of our proof.

### Surrogate Risks

This paper studies binary classification on \(^{d}\). Explicitly, labels are \(\{-1,+1\}\) and the data is distributed according to a distribution \(\) on the set \(^{d}\{-1,+1\}\). The measures \(_{1}\), \(_{0}\) define the relative probabilities of finding points with a given label in a region of \(^{d}\). Formally, define measures on \(^{d}\) by

\[_{1}(A)=(A\{+1\}),_{0}(A)=(A \{-1\}).\]

The _classification risk_\(R(f)\) is then the probability of misclassifying a point under \(\):

\[R(f)=_{f() 0}d_{1}+_{f( )>0}d_{0}.\] (1)

The surrogate to \(R\) is

\[R_{}(f)=(f)d_{1}+(-f)d_{0}\,.\] (2)

A classifier can be obtained by minimizing either \(R\) or \(R_{}\) over the set of all measurable functions. A point \(\) is then classified according to \(\,f\). There are many possible choices for \(\)--typically one chooses a loss that is easy to optimize. In this paper, we assume that

**Assumption 1**.: \(\) _is non-increasing, non-negative, continuous, and \(_{}()=0\)._

Most surrogate losses in machine learning satisfy this assumption. Learning algorithms typically optimize the risk in (2) using an iterative procedure, which produces a sequence of functions that minimizes \(R_{}\). We call \(R_{}\) a _consistent risk_ and \(\) a _consistent loss_ if for all distributions, every minimizing sequence of \(R_{}\) is also a minimizing sequence of \(R\).1 Alternatively, the risks \(R\), \(R_{}\) can be expressed in terms of the quantities \(=_{0}+_{1}\) and \(=d_{1}/d\). For all \(\), define

\[C(,) =_{ 0}+(1-)_{>0}, C ^{*}()=_{}C(,),\] (3) \[C_{}(,) =()+(1-)(-), C_{}^{*}() =_{}C_{}(,)\] (4)

For more on the definitions of \(R,R_{},C,C_{}\), see  or Sections 3.1 and 3.2 of . Using these definitions, \(R(f)= C((),f())d\) and

\[R_{}(f)= C_{}((),f())d\] (5)

This alternative view of the risks \(R\) and \(R_{}\) provides a 'pointwise' criterion for consistency-- if the function \(f()\) minimizes \(C_{}((),)\) at each point, then it also minimizes \(R_{}\). However, minimizers to \(C_{}(,)\) over \(\) do not always exist-- consider for instance \(=1\) for the exponential loss \(()=e^{-}\). In general, for minimizers of \(C_{}(,)\) to exist, one must work over the extended real numbers \(}=\{-,+\}\). The following proposition proved in Appendix A implies that 'pointwise' considerations also extends to minimizing sequences of functions.

**Proposition 1**.: _The following are equivalent:_

1. \(\) _is consistent_
2. _Every minimizing sequence of_ \(C_{}(,)\) _is also a minimizing sequence of_ \(C(,)\)__
3. _Every_ \(}\)_-valued minimizer of_ \(R_{}\) _is a minimizer of_ \(R\)__

This result is well-known in prior literature; in particular the equivalence between 2) and 3) is closely related to the equivalence between calibration and consistency in the non-adversarial setting . Most importantly, the equivalence between 1) and 3) reduces studying minimizing sequences of functionals to studying minimizers of functions. We will show that the equivalencebetween 1) and 2) has an analog in the adversarial scenario, but the equivalence between 1) and 3) does not.

In the adversarial classification setting, every \(x\)-value is perturbed by a malicious adversary before undergoing classification by \(f\). We assume that these perturbations are bounded by \(\) in some norm \(\|\|\) and furthermore, the adversary knows both our classifier \(f\) and the true label of the point \(\). In other words, \(f\) misclassifies \((,y)\) when there is a point \(^{}()}\) for which \(_{f(^{}) 0}=1\) for \(y=+1\) and \(_{f(^{})>0}=1\) for \(y=-1\). Conveniently, this criterion can be expressed in terms of suprema. For any function \(g\), we define

\[S_{}(g)()=_{\|\|}g(+ )\]

A point \(\) with label \(+1\) is misclassified when \(S_{}(_{f 0})()=1\) and a point \(\) with label \(-1\) is misclassified when \(S_{}(_{f>0})()=1\). Hence the expected fraction of errors under the adversarial attack is

\[R^{}(f)= S_{}(_{f 0})d_{1}+ S _{}(_{f>0})d_{0},\] (6)

which is called the _adversarial classification risk_2. Again, optimizing the empirical version of (6) is computationally intractable so instead one minimizes a surrogate of the form

\[R^{}_{}(f)= S_{}( f)d_{1}+ S_{ }(-f)d_{0}\] (7)

Due to the supremum in this expression, we refer to such a risk as a _supremum-based surrogate_. We define adversarial consistency as

**Definition 1**.: _The risk \(R^{}_{}\) is adversarially consistent if for every data distribution, every sequence \(f_{n}\) which minimizes \(R^{}_{}\) over all Borel measurable functions also minimizes \(R^{}\). We say that the loss \(\) is adversarially consistent if the risk \(R^{}_{}\) is adversarially consistent._

Many convex and non-convex losses are consistent in standard classification . By contrast, adversarial consistency often fails. For instance, Meunier et al.  show that convex losses are not adversarially consistent. Furthermore, their example shows that the equivalence between 1) and 3) in Proposition 1 does _not_ hold in the adversarial context. Thus, to understand adversarial consistency, it does not suffice to compare minimizers of \(R^{}_{}\) and \(R^{}\). To illustrate this distinction, we show the following result, adapted from .

**Proposition 2**.: _Assume that \(_{}()/2+(-)/2=(0)\). Then \(\) is not adversarially consistent._

Proof.: Let \(_{0}=_{1}\) be the the uniform distribution on the ball \(()}\) and let \(=2R\). Let \(\) be a loss function for which \(_{}()/2+(-)/2=C^{*}_{}(1/2)=(0)\). Notice that \(_{f}R^{}(f)_{f}R(f)\) and \(_{f}R^{}_{}(f)_{f}R_{}(f)\). Since \(_{0}=_{1}\), the optimal non-adversarial risk is \(_{f}R(f)=1/2\). Moreover, as \(C^{*}_{}(1/2)=(0)\), the optimal non-adversarial surrogate risk is \(_{f}R_{}(f)=C^{*}_{}(1/2)=(0)\). Thus, for the function \(f^{*} 0\), \(R^{}(f^{*})=_{f}R(f)=1/2\) and \(R^{}_{}(f^{*})=_{f}R_{}(f)=(0)\). Therefore \(f^{*}\) minimizes both \(R^{}_{}\) and \(R^{}\). Now consider the sequence of functions

\[f_{n}()=&=0\\ -& 0\]

Because \(=2R\), every point in the support of the distribution can be perturbed to every other point. Thus \(S_{}( f_{n})()=(-1/n)\) and \(S_{}(-f_{n})()=(-1/n)\). However, \(S_{}(_{f 0})=1\) and \(S_{}(_{f>0})=1\). Therefore, \(R^{}_{}(f_{n})=(-1/n)\) while \(R^{}(f_{n})=1\) for all \(n\). As \(\) is continuous, \(_{n}R^{}_{}(f_{n})=(0)\). Thus \(f_{n}\) is a minimizing sequence of \(R^{}_{}\) but not of \(R^{}\), so \(\) is not adversarially consistent.

This example shows that if \(C^{*}_{}(1/2)=(0)\), then \(\) is not adversarially consistent. The main result of this paper is that this is the _only_ obstruction to adversarial consistency: \(\) is adversarially consistent if and only if \(C^{*}_{}(1/2)<(0)\).

We begin by showing that this condition suffices for consistency in the _non-adversarial_ setting. Surprisingly, despite the wealth of work on this topic, this condition does not appear to be known.

**Proposition 3**.: _If \(C^{*}_{}(1/2)<(0)\), then \(\) is consistent._

See Appendix C for a proof.

Again, some losses that satisfy this property are the \(\)-margin loss \(_{}()=(1,(1-/,0))\) and the the shifted sigmoid loss proposed by Meunier et al. (2022), \(()=1/(1+(-))\), \(>0\). (In fact, one can show that the class of shifted odd losses proposed by Meunier et al. (2022) satisfy \(C^{*}_{}(1/2)<(0)\).)

Notice that all convex losses satisfy \(C^{*}_{}(1/2)=(0)\):

\[C^{*}_{}(1/2)=_{}()+(- )(0)\]

The opposite inequality follows from the observation that \(C^{*}_{}(1/2) C_{}(1/2,0)=(0)\). In contrast, recall that a convex loss \(\) with \(^{}(0)<0\) is consistent (Bartlett et al., 2006).

As conjectured by prior work Bao et al. (2021); Meunier et al. (2022), the fundamental reason losses with \(C^{*}_{}(1/2)<(0)\) are adversarially consistent is that minimizers of \(C_{}(,)\) are uniformly bounded away from 0 for all \(\):

**Lemma 1**.: _The loss \(\) satisfies \(C^{*}_{}(1/2)<(0)\) iff there is an \(a>0\) for which any minimizer \(^{*}\) of \(C_{}(,)\) satisfies \(|| a\)._

See C for a proof. Concretely, one can show that for the \(\)-margin loss \(_{}\), a minimizer \(^{*}\) of \(C_{_{}}(,)\) must satisfy \(|^{*}|\). Similarly, a minimizer \(^{*}\) of \(C_{_{}}(,)\) of the shifted sigmoid loss \(_{}=1/(1+(-))\), \(>0\) is always either \(-\) or \(+\). In 4, we use this property to show that minimizing sequences of \(R^{}_{}\) must be uniformly bounded away from zero, thus ruling out the counterexample presented in Proposition 2.

### Minimax Theorems for Adversarial Risks

We study the consistency of \(\) by by comparing minimizing sequences of \(R^{}_{}\) with those of \(R^{}\). In the next section, in order to compare these minimizing sequences, we will attempt to re-write the adversarial loss in a 'pointwise' manner similar to Proposition 1. In order to achieve this representation of the adversarial loss, we apply minimax and complimentary slackness theorems from (Pydi and Jog, 2021; Frank and Niles-Weed, 2023).

Before presenting these results, we introduce the \(\)-Wasserstein metric from optimal transport. For two finite probability measures \(,^{}\) satisfying \((^{d})=^{}(^{d})\), let \((,^{})\) be the set of _couplings_ between \(\) and \(^{}\):

\[(,^{})=\{:^{d} ^{d}(A^{d})=(A),(^{d} A) =^{}(A)\}\]

The distance between \(^{}\) and \(\) in the Wasserstein \(\)-metric \(W_{}\) is defined as

\[W_{}(,^{})=_{(, ^{})}*{ess\,sup}_{(,) }\|-\|.\]

The \(W_{}\) distance is in fact a metric on the space of measures. We denote the \(\)-Wasserstein ball around a measure \(\) by

\[^{}_{}()=\{^{}: ^{}, W_{}(,^{})\}\]

Informally, the measure \(^{}\) is in \(^{}_{}()\) if perturbing points by at most \(\) under the measure \(\) can produce \(^{}\). As a result, Wasserstein \(\)-balls are fairly useful for modeling adversarial attacks. Specifically, one can show:

**Lemma 2**.: _For any function \(g\) and measures \(^{}\), \(\) with \(W_{}(^{},)\), the inequality \( S_{}(g)d gd^{}\) holds._

See Appendix D for a proof.

Minimax theorems from prior work use this framework to introduce dual problems to the adversarial classification risks (6) and (7). Let \(^{}_{0},^{}_{1}\) be finite Borel measures and define

\[(^{}_{0},^{}_{1})= C^{*}( ^{}_{1}}{d(^{}_{0}+^{ }_{1})})d(^{}_{0}+^{}_{1})\] (8)

where \(C^{*}\) is defined by (3). The next theorem states that maximizing \(\) over \(W_{}\) balls is in fact a dual problem to minimizing \(R^{}\).

**Theorem 1**.: _Let \(\) be defined by (8)._

\[_{f\\ }R^{}(f)=_{^{}_{0}^{}_{}(_{0})\\ ^{}_{1}^{}_{}(_{1})} (^{}_{0},^{}_{1})\] (9)

_and furthermore equality is attained for some Borel measurable \(\) and \(}_{1},}_{0}\) with \(W_{}(}_{0},_{0})\) and \(W_{}(}_{1},_{1})\)._

The first to show such a theorem was Pydi and Jog (2021). In comparison to their Theorem 8, Theorem 1 removes the assumption that \(_{0},_{1}\) are absolutely continuous with respect to Lebesgue measure and shows that the minimizer \(\) is in fact Borel. We prove this theorem in Appendix E. Frank and Niles-Weed (2023) prove a similar statement for the surrogate risk \(R^{}_{}\). This time, the dual objective is

\[_{}(^{}_{0},^{}_{1})= C^{*}_{ }(^{}_{1}}{d(^{}_{0}+^{}_{1})})d(^{}_{0}+^{}_{1})\] (10)

with \(C^{*}_{}\) defined by (4).

**Theorem 2**.: _Assume that Assumption 1 holds, and define \(_{}\) by (10). Then_

\[_{f\\ f}}R^{}_{}(f)=_{ ^{}_{0}^{}_{}( _{0})\\ ^{}_{1}^{}_{}(_{1})} _{}(^{}_{0},^{}_{1})\] (11)

_and furthermore equality in the dual problem is attained for some \(^{*}_{1},^{*}_{0}\) with \(W_{}(^{*}_{0},_{0})\) and \(W_{}(^{*}_{1},_{1})\)._

Frank and Niles-Weed (2023) proved this statement in Theorem 6 but with the infimum taken over \(}\)-valued functions. To extend the result to \(\)-valued functions as in Theorem 2, we show that \(_{ff}}}R^{}_{}(f)=_{f f}}R^{}_{}(f)\) in Appendix B.

## 4 Adversarially Consistent Losses

This section contains our main results on adversarial consistency. In light of Proposition 2, our main task is to show that a loss satisfying \(C^{*}_{}(1/2)<(0)\) is adversarially consistent.

At a high level, we will show that every minimizing sequence of \(R^{}_{}\) must also minimize \(R^{}\). However, directly analyzing minimizing sequences \(\{f_{n}\}\) of \(R^{}_{}\) and \(R^{}\) is challenging due to the supremums in the definitions of the adversarial risks. We therefore develop alternate characterizations of minimizing sequences to both functionals, based on complimentary slackness conditions derived from the convex duality results of Section 3.2. However, unlike standard complementary slackness conditions well known from convex optimization, these theorems allow us to characterize minimizing sequences as well as minimizers.

### Approximate Complimentary Slackness

We first state this slackness result for the surrogate case, due to Frank and Niles-Weed (2023, Lemmas 16 and 26) and Theorem 2.

**Proposition 4**.: _Let \((^{*}_{0},^{*}_{1})\) be any maximizers of \(R_{}\) over \(^{}_{}(_{i})\). Define \(^{*}=^{*}_{0}+^{*}_{1}\), \(^{*}=d^{*}_{1}/d^{*}\). If \(f_{n}\) is a minimizing sequence for \(R^{}_{}\), then the following hold:_

\[_{n} C_{}(^{*},f_{n})d^{*}= C ^{*}_{}(^{*})d^{*}.\] (12) \[_{n} S_{}( f_{n})d_{1 }- f_{n}d^{*}_{1}=0,_{n} S_{ }(-f_{n})d_{0}--f_{n}d^{*}_ {0}=0\] (13)

Proof.: Let \(R^{}_{,*}\) be the minimal value of \(R^{}_{}\) and choose a \(>0\). Then for sufficiently large \(N\), \(n N\) implies that \(R^{}_{}(f_{n}) R^{}_{,*}+\). Lemma 2 and the definition of \(C^{*}_{}\) in (4) further imply that

\[R^{}_{,*}+ S_{}( f_{n})d _{1}+ S_{}(-f_{n})d_{0} f_{n}d ^{*}_{1}+-f_{n}d^{*}_{0} R^{}_{ ,*}\] (14)

As \(R^{}_{,*}= C^{*}_{}(^{*})d^{*}\), this relation immediately implies (12).

Next, Lemma 2 again implies that

\[ S_{}( f_{n})d_{1} f_{n}d ^{*}_{1} S_{}(-f_{n})d _{0}-f_{n}d^{*}_{0}\] (15)

while (14) implies that

\[R^{}_{,*}- f_{n}d^{*}_{1}+-f_ {n}d^{*}_{0} 0.\]

Therefore, subtracting \( f_{n}d^{*}_{1}+-f_{n}d^{*}_{0}\) from (14) results in

\[( S_{}( f_{n})d_{1}-  f_{n}d^{*}_{1})+( S_{}(-f_{n} )d_{0}--f_{n}d^{*}_{0}) 0.\] (16)

Again, (15) implies that the quantities on parentheses are both positive which implies (13).

Proposition 4 shows that minimizing sequences of \(R^{}_{}\) satisfy two properties: 1) The sequence \(\{f_{n}\}\) must minimize the _standard_\(\)-risk \(R_{}\) with measures \(^{*}_{0}\), \(^{*}_{1}\) in place of \(_{0},_{1}\), 2) At the limit, the measures \(^{*}_{0},^{*}_{1}\) are best adversarial attacks on \( f_{n},-f_{n}\). In fact, one can show that \(\{f_{n}\}\) is a minimizing sequence of \(R^{}_{}\)_if and only if_ it satisfies these properties. Crucially, a very similar characterization holds for minimizers of the adversarial classification loss. We state and prove the 'only if' direction of this characterization in Proposition 5.

**Proposition 5**.: _Let \(f_{n}\) be a sequence and let \(^{*}_{0}\), \(^{*}_{1}\) be measures in \(^{}_{}(_{i})\). Define \(^{*}=^{*}_{0}+^{*}_{1}\), \(^{*}=d^{*}_{1}/d^{*}\). If the following two conditions hold:_

\[_{n} C(^{*},f_{n})d^{*}= C^{*}(^{*})d ^{*}\] (17)

\[_{n} S_{}(_{f_{n} 0})d_{1}- _{f_{n} 0}d^{*}_{1}=0,_{n} S_{ }(_{f_{n}>0})d_{0}-_{f_{n}>0}d^{*}_{0}=0,\] (18)

_then \(f_{n}\) is a minimizing sequence of \(R^{}\)._

Proof.: Equation 17 implies that the limit \(_{n}C(^{*},f_{n})d^{*}\) exists. Thus (17) and (18) imply that

\[_{n}R^{}(f_{n}) =_{n} S_{}(_{f_{n} 0})d _{1}+ S_{}(_{f_{n}>0})d_{0}=_{n }_{f_{n} 0}d^{*}_{1}+_{f_{n}>0}d^{*}_{0}\] \[=_{n} C(^{*},f_{n})d^{*}= C^{*}( ^{*})d^{*}=(^{*}_{0},^{*}_{1})\,.\]

Therefore, Strong duality (Theorem 1) then implies that

\[_{n}R^{}(f_{n})_{^{ }_{0}^{}_{}(_{0})\\ ^{}_{1}^{}_{}(_{1})} (^{}_{0},^{}_{1})=_{ \\ }R^{}(f)\]

and therefore, \(f_{n}\) is a minimizing sequence.

We end this section by comparing the different criteria for consistency presented in Proposition 1 with Propositions 4 and 5. Together, Propositions 4 and 5 will allow us to compare minimizing sequences of \(R^{}_{}\) to those of \(R^{}\) by showing that any sequence satisfying (12)-(13) must also satisfy (17)-(18). This statement is the analog to 2) of Proposition 1. Indeed, because \(C_{}(^{*},f_{n}) C^{*}_{}(^{*})\), (12) is actually equivalent to to \(C_{}(^{*},f_{n}) C^{*}_{}(^{*})\) in \(L^{1}(^{*})\). However, the extra criterion (18) implies an additional constraint on the structure of the minimizing sequence. This additional constraint is the reason 3) of Proposition 1 is false in the adversarial setting. In the restricted situation where \(_{}=\), Meunier et al. (2022) show that (12) implies (17) (Proposition 4.2). However, this observation does not suffice to conclude consistency.

### Adversarial Consistency

We are now in a position to prove consistency. Before presenting the full proof, we pause to discuss the overall strategy. Consistency will follow from three considerations. First, every minimizing sequence of \(R^{}_{}\) satisfies conditions (12) and (13). Second, conditions (12) and (13) imply the very similar conditions (17) and (18). Finally, any function sequence satisfying (17) and (18) must be a minimizing sequence to \(R^{}\). The first and last steps are the content of Propositions 4 and 5, so it remains to justify the middle step.

Verifying that (12) implies (17) is straightforward. The relation (12) actually states that \(f_{n}\) minimizes the _standard_ surrogate risk with respect to the distribution given by \(^{*}_{0}\), \(^{*}_{1}\). Therefore (12) implies (17) so long as \(\) is consistent.

The main difficulty is verifying (18), due to the discontinuity of \(_{<0}\), \(_{ 0}\) at \(0\). Due to this discontinuity, one cannot directly argue that (13) implies (18): to simplify the discussion, assume that \(\) is strictly decreasing on a neighborhood of the origin, in which case \(_{<0}=_{()>(0)}\) and \(_{ 0}=_{(-)(0)}\). Recall that according to (13), in the limit \(n\), \(^{*}_{0},^{*}_{1}\) are the strongest attack in \(^{}_{}(_{0})^{}_{ }(_{1})\), or informally, \(S_{}( f_{n})()\) approaches \((f_{n}(^{}))\) for an optimal perturbation \(^{}\) w.h.p., with a similar condition for \(-f_{n}\). However, due to the discontinuity of \(_{(-)(0)}\) at \((0)\), if \(f_{n}(^{}) 0\) as \(n\), this relation does not imply that \(_{S_{}(-f_{n})()(0)}\) approaches \(_{-f_{n}(^{}) 0}\).

Lemma 1 says that if \(C^{*}_{}(1/2)<(0)\), minimizers of \(C_{}(,)\) are uniformly bounded away from \(0\). This fact suggests that minimizing sequences will also be bounded away from the origin, which will allow us to avoid the discontinuity there. Concretely, we show:

**Lemma 3**.: _Let \(C^{*}_{}(1/2)<(0)\). Then there is a \(>0\) and a \(c>0\) with \((c)<(0)\) for which \([-c,c]\) implies \(C_{}(,) C^{*}_{}()+\), uniformly in \(\). Furthermore, for this value of \(c\), if \(>c\) then \(()<(c)\)._

We prove this lemma in Appendix C. Because \(C_{}(^{*},f_{n}) C^{*}_{}(^{*})\) in \(L^{1}(^{*})\), Lemma 3 implies that

\[_{n}^{*}(f_{n}[-c,c])=0.\] (19)

This relation is the key fact that allows us to show that (13) implies (18). The condition \(C^{*}_{}(1/2)<(0)\) is essential for this step of the argument.

Lastly, Lemma 2 implies that \( S_{}(_{f_{n} 0})d_{1}_{f_{n}  0}d^{*}_{1}\) and thus to validate (18), it suffices to verify the opposite inequality in the limit \(n\).

**Lemma 4**.: _Let \(f_{n}\) be a sequence of functions and let \(^{*}_{0}^{}_{}(_{0})\), \(^{*}_{1}^{}_{}(_{1})\). The equation_

\[_{n} S_{}(_{f_{n} 0})d_{1} _{n}_{f_{n} 0}d^{*}_{1}\] (20)

_implies the first relation of (18) and_

\[_{n} S_{}(_{f_{n}>0})d_{0} _{n}_{f_{n}>0}d^{*}_{0}\] (21)

_implies the second relation of (18)._

See Appendix F for a proof. These considerations suffice to prove the main result of this paper:

**Theorem 3**.: _The loss \(\) is adversarially consistent if and only if \(C^{*}_{}(1/2)<(0)\)._

Proof.: The 'only if' portion of the statement is Proposition 2.

To show the 'if' statement, recall the standard analysis fact: \(_{n}a_{n}=a\) iff for all subsequences \(\{a_{n_{j}}\}\) of \(\{a_{n}\}\), there is a further subsequence \(a_{n_{j_{k}}}\) for which \(_{k}a_{n_{j_{k}}}=a\). This result implies that to prove \(R^{}_{}\) is consistent, it suffices to show that every minimizing sequence \(f_{n}\) of \(R^{}_{}\) has a subsequence \(f_{n_{j}}\) that minimizes \(R^{}\).

Let \(f_{n}\) be a minimizing sequence of \(R^{}_{}\). For convenience, pick a subsequence \(f_{n_{j}}\) for which the limits \(_{j} S_{}(_{f_{n_{j}}<0})d_{0}\), \(_{j} S_{}(_{f_{n_{j}} 0})d_{1}\) both exist. For notational clarity, we drop the \({}_{j}\) subscript and denote this sequence as \(f_{n}\).

By Proposition 4, the equations (12) and (13) hold. We will argue that \(f_{n}\) is in fact a minimizing sequence of \(R^{}\) by verifying the conditions of Proposition 5.

First, the relation (12) states that the sequence \(f_{n}\) minimizes the _standard_\(\)-risk for the distribution given by \(_{0}^{*}\) and \(_{1}^{*}\). As the loss \(\) is consistent by Proposition 3, the sequence \(f_{n}\) must minimize the standard classification risk for the distribution \(_{0}^{*},_{1}^{*}\). This statement implies (17). Next we will argue that (18) holds.

Let \(c,\) be as in Lemma 3. Because \(C_{}(^{*},f_{n}) C^{*}_{}(^{*})\), (12) implies that \(C_{}(^{*},f_{n})\) converges to \(C^{*}_{}(^{*})\) in \(L^{1}\). However, \(L^{1}\) convergence implies convergence in measure (see for instance Proposition 2.29 of ), and therefore \(_{n}^{*}C_{}(^{*},f_{n})>C^{*}_{}( ^{*})+=0\). Lemma 3 then implies that for \(i=0,1\)

\[_{n}_{i}^{*}(f_{n}[-c,c])=0.\] (22)

Next, because \(\) is non-increasing, \(f 0\) implies \((f)(0)\) and thus \(_{f 0}_{ f(0)}\). Furthermore, as the function \(_{ 0}\) is monotone and upper semi-continuous,

\[ S_{}(_{f_{n} 0})d_{1} S_{ }(_{ f_{n}(0)})d_{1} _{S_{}( f_{n})(0)}d_{1}.\] (23)

Let \(_{i}\) be a coupling between \(_{i}\) and \(_{i}^{*}\) for which \(*{ess\,sup}_{(,)_{i}}\|- \|\). Then the measure \(_{i}\) is supported on \(_{}=\{(,)\|-\| \}\). Furthermore, as \(S_{}( f_{n})() f_{n}(^{ })\) everywhere on \(_{}\), the relation \(S_{}( f_{n})() f_{n}(^{ })\) actually holds \(_{1}\)-a.e. Therefore, (13) actually implies that \(S_{}( f_{n})()- f_{n}(^{})\) converges in \(_{1}\)-measure to \(0\). In particular, since \((c)<(0)\), \(_{n}_{1}S_{}( f_{n})()- (f_{n}(^{}))(0)-(c)=0\) and thus \(_{n}_{1}(S_{}( f_{n})()( 0) f_{n}(^{})<(c))=0\). Therefore,

\[_{n}_{1}(S_{}( f_{n})( )(0))=_{n}_{1}(S_{}( f _{n})()(0) f_{n}(^{})( c))\] \[_{n}_{1}( f_{n}(^{ })(c))=_{n}_{1}^{*}( f_{n}( ^{})(c))\]

This calculation implies

\[_{n}_{S_{}( f_{n})() (0)}d_{1}_{n}_{ f _{n}(^{})(c)}d_{1}^{*}_{n }_{f_{n} c}d_{1}^{*}\] (24)

The last inequality follows because Lemma 3 states that \(>c\) implies \(()<(c)\) and therefore \(_{ f_{n}(c)}_{f_{n} c}\). Equation 22 then implies

\[_{n}_{ f_{n}(0)}d_{1}^{*} _{n}_{f_{n} c}d_{1}^{*}= _{n}_{f_{n}-c}d_{1}^{*}.\] (25)

Recall that the sequence \(f_{n}\) was chosen so that the limit \(_{n} S_{}(_{f_{n} 0})d_{1}\) exists. Combining this fact with (23), (24), and (25) results in

\[_{n} S_{}(_{f_{n} 0})d_{1} _{n}_{f_{n}-c}d_{1}^{*}_{n }_{f_{n} 0}d_{1}^{*}\] (26)

The first relation of (18) then follows from (26) together with Lemma 4.

A similar argument implies the second relation of (18). Because \(_{f>0}=_{-f<0}_{-f 0}\), the same chain of inequalities as (23), (24), and (25) implies that

\[_{n} S_{}(_{f_{n}>0})d_{0} _{n} S_{}(_{-f_{n} 0})d_{0} _{n}_{-f_{n}-c}d_{0}^{*}= _{n}_{f_{n} c}d_{0}^{*}\]

As \(c>0\), it follows that \(_{n} S_{}(_{f_{n}>0})d_{0} _{n}_{f_{n}>0}d_{0}^{*}\). Once again, the second expression of (18) follows from this relation and Lemma 4. 

## 5 Quantitative Bounds for the \(\)-Margin Loss

As discussed in the introduction, statistical consistency is not the only property one would want from a surrogate. Hopefully, minimizing a surrogate will also efficiently minimize the classification loss. Bartlett et al. (2006); Steinwart (2007); Reid and Williamson (2009) prove bounds of the form \(R(f)-R_{*} G_{}(R_{}^{*}(f)-R_{,*})\) for a function \(G_{}\) and \(R_{*}=_{f}R(f)\), \(R_{,*}=_{f}R_{}(f)\). The function \(G_{}\) is an upper bound on the rate of convergence of the classification risk in terms of the rate of convergence of the surrogate risk. One would hope that \(G_{}\) is not logarithmic, as such a bound could imply that reducing \(R(f)-R_{*}\) by a quantity \(\) could require an exponential change of \(e^{}\) in \(R_{}(f)-R_{,*}\). Bartlett et al. (2006) compute such \(G_{}\) for several popular losses in the standard classification setting. For example, they show the bounds \(G_{}()=\) for the hinge loss \(()=(1-)_{+}\) and \(G_{}()=\) for the squared hinge loss \(()=(1-)_{+}^{2}\). On can prove an analogous bound for the \(\)-margin loss in the adversarial setting:

**Theorem 4**.: _Let \(_{}=(1,(1-/,0))\) be the \(\)-margin loss, \(R_{*}^{}=_{f}R^{}(f)\), and \(R_{_{},*}^{}(f)=_{f}R_{_{}}^{}(f)\). Then_

\[R^{}(f)-R_{*}^{} R_{_{}}^{}(f)-R_{_{ },*}^{}.\]

Notice that this theorem immediately implies that the \(\)-margin loss is in fact adversarially consistent. The proof below is completely independent of the argument in Section 4.

Proof.: Notice that for the \(\)-margin loss, \(C_{_{}}^{*}=C^{*}\) and therefore, the optimal \(_{}\)-risk \(R_{_{},*}^{}\) equals the optimal adversarial classification risk \(R_{*}^{}\). However, since \(_{}()_{ 0}\) and \(_{}(-)_{>0}\) for any \(\), one can conclude that \(R^{}(f) R_{_{}}^{}(f)\). Therefore,

\[R^{}(f)-R_{*}^{}=R^{}(f)-R_{_{},*}^{}  R_{_{}}^{}(f)-R_{_{},*}^{}\]

This bound implies that reducing the excess adversarial \(\)-margin loss by \(\) also reduces an upper bound on the excess adversarial classification loss by \(\). Thus, one would expect that minimizing the adversarial \(\)-margin risk would be an effective procedure for minimizing the adversarial classification risk.

Extending Theorem 4 to other losses remains an open problem. In the non-adversarial scenario, many prior works develop techniques for computing such bounds. These include the \(\)-transform of Bartlett et al. (2006), calibration analysis in Steinwart (2007), and special techniques for proper losses in Reid and Williamson (2009).

Contemporary work Mao et al. (2023) derives an \(\)-consistency surrogate risk bound for a variant of the adversarial \(\)-margin loss.

## 6 Conclusion

In conclusion, we proved that the adversarial training procedure is consistent for perturbations in an \(\)-ball if an only if \(C_{}^{*}(1/2)<(0)\). The technique that proved consistency extends to perturbation sets which satisfy existence and minimax theorems analogous to Theorems 1 and 2. Furthermore, we showed a quantitative excess risk bound for the adversarial \(\)-margin loss. Finding such bounds for other losses remains an open problem. We hope that insights to consistency and the structure of adversarial learning will lead to the design of better adversarial learning algorithms.