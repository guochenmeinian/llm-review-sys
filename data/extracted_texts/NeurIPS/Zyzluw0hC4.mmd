# 3D molecule generation by denoising voxel grids

Pedro O. Pinheiro, Joshua Rackers, Joseph Kleinhenz, Michael Maser, Omar Mahmood, Andrew Martin Watkins, Stephen Ra, Vishnu Sresht, Saeed Saremi

Prescient Design, Genentech

###### Abstract

We propose a new score-based approach to generate 3D molecules represented as atomic densities on regular grids. First, we train a denoising neural network that learns to map from a smooth distribution of noisy molecules to the distribution of real molecules. Then, we follow the _neural empirical Bayes_ framework  and generate molecules in two steps: (i) sample noisy density grids from a smooth distribution via underdamped Langevin Markov chain Monte Carlo, and (ii) recover the "clean" molecule by denoising the noisy grid with a single step. Our method, _VoxMol_, generates molecules in a fundamentally different way than the current state of the art (_i.e._, diffusion models applied to atom point clouds). It differs in terms of the data representation, the noise model, the network architecture and the generative modeling algorithm. Our experiments show that VoxMol captures the distribution of drug-like molecules better than state of the art, while being faster to generate samples.

## 1 Introduction

Finding novel molecules with desired properties is an important problem in chemistry with applications to many scientific domains. In drug discovery in particular, standard computational approaches perform some sort of local search--by scoring and ranking molecules--around a region of the molecular space (chosen based on some prior domain knowledge). The space of possible drug-like molecules is prohibitively large (it scales exponentially with the molecular size [2; 3], estimated to be around \(10^{60}\)), therefore search in this space is very hard. Search-based approaches achieve some successes in practice, but have some severe limitations: we can only explore very small portions of the molecular space (on the order of billions to trillions molecules) and these approaches cannot propose new molecules conditioned on some desiderata.

Generative models for molecules have been proposed to overcome these limitations and explore the molecular space more efficiently . These approaches often consider one of the following types of molecule representations: (i) one-dimensional sequences such as SMILES  or SELFIES  (_e.g._, [8; 9; 10]), (ii) two-dimensional molecular graphs, where nodes represent atoms or molecular substructures and edges represent bonds between them (_e.g._, [11; 12; 13; 14]), or (iii) atoms as three-dimensional points in space. Molecules are entities laying on three-dimensional space, therefore 3D representations are arguably the most complete ones--they contain information about atom types, their bonds and the molecular conformation.

Recent generative models consider molecules as a set of points in 3D Euclidean space and apply diffusion models on them [15; 16; 17; 18; 19; 20]. Point-cloud representations allow us to use equivariant graph neural networks [21; 22; 23; 24; 25]--known to be very effective in molecular discriminative tasks--as the diffusion model's denoising network. However, point-based diffusion approaches have some limitations when it comes to generative modeling. First, the number of atoms in the molecule (_i.e._, nodes on the 3D graph) to be diffused need to be known beforehand. Second, atom types and their coordinates have very different distributions (categorical and continuous variables, respectively) and are treated separately. Because a score function is undefined on discrete distributions,some workaround is necessary. Finally, graph networks operate only on nodes and edges (single and pairwise iterations, respectively). Therefore, capturing long-range dependencies over multiple atoms (nodes) can become difficult as the number of atoms increases. This is related to the limitations of the message-passing formalism in graph neural networks . Higher-order message passing can alleviate this problem to a degree [27; 28], but they come at a significant computational cost and they have been limited to third-order models  (see next section for more discussions on the tradeoffs between model expressivity and built-in equivariance).1

In this work we introduce _VoxMol_, a new score-based method to generate 3D molecules. Similar to , and unlike most recent approaches, we represent atoms as continuous (Gaussian-like) densities and molecules as a discretization of 3D space on voxel (_i.e._, a discrete unit of volume) grids. Voxelized representations allow us to use the same type of denoising architectures used in computer vision. These neural networks--the workhorse behind the success of score-based generative models on images, _e.g._[34; 35; 36]--are very effective and scale very well with data.

We start by training a neural network to denoise noisy voxelized molecules. Noisy samples are created simply by adding Gaussian noise (with a fixed identity covariance matrix scaled by a large noise level) to each voxel in the molecular grid. This denoising network also parametrizes the score function of the smooth/noisy distribution. Note that in contrast to diffusion models, the noise process we use here does not displace atoms. Then, we leverage the (learned) denoising network and generate molecules in two steps : (i) _(walk)_ sample noisy density grids from the smooth distribution via Langevin Markov chain Monte Carlo (MCMC), and (ii) _(jump)_ recover "clean" molecules by denoising the noisy grid. This sampling scheme, referred to as walk-jump sampling in , has been successfully applied before to 2D natural images [37; 38] and 1D amino acid sequences .

Compared to point-cloud diffusion models, VoxMol is simpler to train, it does not require knowing the number of atoms beforehand, and it does not treat features as different distributions (continuous, categorical and ordinal for coordinates, atom types and formal charge)--we only use the "raw" voxelized molecule. Moreover, due to its expressive network architecture, our method scales better to large, drug-sized molecules. Figure 1 (and Figures 8, 9 on appendix) illustrates voxelized molecules and their corresponding molecular graphs generated by our model, trained on two different datasets. These samples show visually that our model learns valences of atoms and symmetries of molecules.

The main contributions of this work can be summarized as follows. We present VoxMol, a new score-based method for 3D molecule generation. The proposed method differs from current approaches--usually diffusion models on point clouds--in terms of the data representation, the noise model, the network architecture, and the generative modeling algorithm. We show in experiments that VoxMol performs slightly worse than state of the art on a small dataset (QM9 ), while outperforms it (by a large margin) on a challenging, more realistic drug-like molecules dataset (GEOM-drugs ).

Figure 1: Voxelized molecules generated by our model and their corresponding molecular graphs. Left, samples from a model trained on QM9 dataset (\(32^{3}\) voxels). Right, samples from a model trained on GEOM-drugs (\(64^{3}\) voxels). In both cases, each voxel is a cubic grid with side length of \(.25\)Ã…. Each color represents a different atom (and a different channel on the voxel grid). Best seen in digital version. See appendix for more generated samples.

Related Work

Voxel-based unconditional 3D molecule generation.Skalic _et al._ and Ragoza _et al._ map atomic densities on 3D regular grids and train VAEs  using 3D convolutional networks to generated voxelized molecules. To recover atomic coordinates from the generated voxel grids2,  introduces a simple optimization-based solution, while  trains another model that "translates" voxel structures into SMILES strings. Voxel representations are flexible and can trivially be applied to related problems with different data modalities. For instance,  proposes a GAN  on voxelized electron densities, while  leverages voxelized 3D pharmacophore features to train a pocket-conditional model. Similar to these works, our model also relies on discretization of 3D space. Like , we use a simple peak detection algorithm to extract atomic coordinates from the generated voxel grids. However, our method differs on the underlying generative modeling, architecture, datasets, input representations and evaluations.

Point cloud-based unconditional generation.Most recent models treat molecules as sets of points, where each node is associated with a particular atom type, its coordinates and potentially extra information like formal charge. Different modeling approaches have been proposed, _e.g._, [47; 48; 49] utilize autoregressive models to iteratively sample atoms, and [50; 51] use normalizing flows . Hoogeboom _et al._ proposes E(3) Equivariant Diffusion Models (EDM), a diffusion -based approach that performs considerably better than previous models on this task. EDMs learn to denoise a diffusion process (operating on both continuous and categorical data) and generate molecules by iteratively applying the denoising network on an initial noise. Several works have been proposed on the top of EDM [54; 55; 20; 56]. For instance, Xu _et al._ improves EDM by applying diffusion on a latent space instead of the atomic coordinates, while MiDi  shows that EDM results can be improved by jointly generating the 3D conformation and the connectivity graph of molecules (in this setting, the model has access to both the 3D structure and the 2D connectivity graph).

Conditional 3D molecule generation.A related body of work is concerned with _conditional_ generation. In many cases, conditional generation is built on the top of unconditional generation methods. Some authors propose to predict the 3D structure of the molecules given a molecular graph (this is called the conformer generation task): VAEs [57; 58], normalizing flows , reinforcement learning , optimal transport , autoregressive models  and diffusion models [63; 16; 64] have been proposed to this task. Some work [65; 66] condition 3D generation on shape while other works condition molecule generation on other structures. For instance, [17; 18; 19; 67] adapt (unconditional) diffusion models to condition on protein pockets, while  adapt their previous work  to condition voxelized structures to protein targets. Finally,  proposes a hybrid conditional generation model by modeling fragments/scaffolds with point cloud representation and the 3D target structures and pharmacophores features  with voxel grids.

Comparison between voxel and point-cloud representations.Voxels have some advantages and disadvantages compared to point cloud representations. First, voxels are straightforward generalizations of 2D pixels to 3D space, therefore we can leverage similar machinery used in score-based generative modeling for images. These models are known to perform well and scale nicely with data. Second, message passing on graphs operate on single and pairwise interactions while convolution filters (and potentially transformer layers applied to regular grids) can capture multiple local interactions by construction (see  for a discussion on the _many-body representation_ hypothesis). Third, voxel representations have a higher memory footprint and lower random memory accesses than point cloud representations . We note however, that developing models on drug-sized molecules (that is, molecules with size close to those on GEOM-drugs ) with reasonable resolution (.1-.2A) is possible on current GPU hardware. Fourth, recovering point coordinates from a discrete grid has no analytical solution, therefore voxel-based models require an extra step to retrieve atomic coordinates. We show empirically that this is not a problem in practice as we can achieve competitive results, even with a very simple peak detection algorithm.

Finally, graph networks are less expressive due to message passing formalism [26; 27], but are a better fit for built-in SE(3)-equivariance architectures (_e.g._[21; 22; 23; 24; 25]). Rotation-equivariant 3D con volutional network have been proposed [72; 73; 74]3, but current models do not scale as well as standard convnets, and it would be a challenge to apply them to drug-sized molecules. Built-in rotation equivariance is a good property to have, however equivariance can also be learned with strong data augmentation/larger datasets [75; 76; 32]. In fact, concurrently to this work,  also show that built-in SE(3)-equivariant architecture is not necessary to generate molecules. Our experiments show that an expressive denoiser scales up better, allowing VoxMol to outperform current state of the art on GEOM-drugs. However, we hope our results motivate exploration of more efficient SE(3)-equivariant convnet architectures.

## 3 Method

We follow previous work (_e.g._, [78; 33; 70; 79]) and represent atoms as continuous Gaussian-like atomic densities in 3D space, centered around their atomic coordinates. Molecules are generated by discretizing the 3D space around the atoms into voxel grids, where each atom type (element) is represented by a different grid channel. See appendix for more information on how we discretize molecules. This discretization process gives us a dataset with \(n\)_voxelized molecules_\(\{x_{i}\}_{i=1}^{n},x_{i}^{d},d=c l^{3}\), where \(l\) is the length of each grid edge and \(c\) is the number of atom channels in the dataset. Each voxel in the grid can take values between 0 (far from all atoms) and 1 (at the center of atoms). Throughout our experiments, we consider a fixed resolution of.25A (we found it to be a good trade-off between accuracy and computation). Therefore, voxel grids occupy a volume of \((l/4)^{3}\) cubic Angstroms.

### Background: neural empirical Bayes

Let \(p(x)\) be an unknown distribution of voxelized molecules and \(p(y)\) a smoother version of it obtained by convolving \(p(x)\) with an isotropic Gaussian kernel with a known covariance \(^{2}I_{d}\)4. Equivalently, \(Y=X+N\), where \(X p(x)\), \(N(0,^{2}I_{d})\). Therefore \(Y\) is sampled from:

\[p(y)=_{^{d}}\!)^{d/2}}\! (-}{2^{2}})p(x)dx\,.\]

This transformation will smooth the density of \(X\) while still preserving some of the structure information of the original voxel signals. Robbins  showed that if we observe \(Y=y\), then the least-square estimator of \(X\)_is_ the Bayes estimator, _i.e._, \((y)=[X|Y=y]\). Built on this result, Miyasawa  showed that, if the noising process is Gaussian (as in our case), then the least-square estimator \((y)\) can be obtained purely from the (unnormalized) smoothed density \(p(y)\):

\[(y)=y+^{2}g(y)\,,\] (1)

where \(g(y)=_{y}\) log \(p(y)\) is the score function  of \(p(y)\). This interesting equation tells us that, _if_ we know \(p(y)\) up to a normalizing constant (and therefore the score function associated with it), we can estimate the original signal \(x\) only by observing its noisy version \(y\). Equivalently, if we have access to the estimator \((y)\), we can compute the score function of \(p(y)\) via (1).

Our generative model is based on the _neural empirical Bayes (NEB)_ formalism : we are interested in learning the score function of the smoothed density \(p(y)\) and the least-square estimator \((y)\) from a dataset of voxelized molecules \(\{x_{i}\}_{i=1}^{n}\), sampled from unknown \(p(x)\). We leverage the (learned) estimator and score function to generate voxelized molecules in two steps: (i) sample \(y_{k} p(y)\) with Langevin MCMC , and (ii) generate clean samples with the least-square estimator. The intuition is that it is much easier to sample from the smooth density than the original distribution. See Saremi and Hyvarinen  for more details.

### Denoising voxelized molecules

We parametrize the Bayes estimator of \(X\) using a neural network with parameters \(\) denoted by \(_{}:^{d}^{d}\). Since the Bayes estimator is the least-squares estimator, the learning becomes a least-squares _denoising objective_ as follows:

\[()=_{x p(x),y(x,^{2}I_{d} )}\,||x-_{}(y)||^{2}\,.\] (2)Using (1), we have the following expression for the smoothed score function in terms of the denoising network5 :

\[g_{}(y)\!=\!}(_{}(y)\!-\!y)\,.\] (3)

By minimizing the learning objective (2) we learn the optimal \(_{}\) and by using (3) we can compute the score function \(g_{}(y)\!\!_{y}p(y)\).

We model the denoising network \(_{}\) with an encoder-decoder 3D convolutional network that maps every noised voxel on the grid to a clean version of it. Figure 2(a) shows a general overview of the denoising model. The noise level, \(\), is kept constant during training and is a key hyperparameter of the model. Note that in the empirical Bayes formalism, \(\) can be any (large) value.

Compared to diffusion models, this training scheme is simpler as the noise level is fixed during training. VoxMol does not require noise scheduling nor temporal embedding on the network layers. We observe empirically that single-step denoising is sufficient to reconstruct voxelized molecules (within the noise levels considered in this paper). Our hypothesis is that this is due to the nature of the voxel signals, which contain much more "structure" than "texture" information, in comparison to natural images.

### Sampling voxelized molecules

We use the learned score function \(g_{}\) and the estimator \(_{}\) to sample. We follow the _walk-jump sampling_ scheme  to generate voxelized molecules \(x_{k}\):

1. _(walk step)_ For sampling noisy voxels from \(p(y)\), we consider Langevin MCMC algorithms that are based on discretizing the underdamped Langevin diffusion : \[dv_{t} =- v_{t}dt-ug_{}(y_{t})dt+()dB_{t}\] (4) \[dy_{t} =v_{t}dt\,,\] where \(B_{t}\) is the standard Brownian motion in \(^{d}\), \(\) and \(u\) are hyperparameters to tune (friction and inverse mass, respectively). We use the discretization algorithm proposed by Sachs _et al._ to generate samples \(y_{k}\), which requires a discretization step \(\). See appendix for a description of the algorithm.
2. _(jump step)_ At an arbitrary time step \(k\), clean samples can be generated by estimating \(X\) from \(y_{k}\) with the denoising network, _i.e._, computing \(x_{k}\!=\!_{}(y_{k})\).

This approach allows us to approximately sample molecules from \(p(x)\) without the need to compute (or approximate) \(_{x}\,p(x)\). In fact, we do MCMC on the smooth density \(p(y)\), which is known to be easier to sample and mixes faster than the original density \(p(x)\). Figure 2(b) shows a schematic representation of the generation process. Following , we initialize the chains at by adding uniform noise to Gaussian noise (with the same \(\) used during training), _i.e._, \(y_{0}=N+U\), \(N\!\!(0,\!^{2}I_{d})\), \(U\!\!_{d}(0,\!1)\) (this was observed to mix faster in practice).

Figure 2: (a) A representation of our denoising training procedure. Each training sample (_i.e._, a voxelized molecule) is corrupted with isotropic Gaussian noise with a fixed noise level \(\). The model is trained to recover clean voxel grids from the noisy version. To facilitate visualization, we threshold the grid values, \(\!=\!1_{ 1}()\). (b) Graphical model representation of the walk-jump sampling scheme. The dashed arrows represent the walk, a MCMC chain to draw noisy samples from \(p(y)\). The solid arrow represents the jump. Both walks and jumps leverage the trained denoising network.

The noise level plays a key role in this sampling framework. If the noise is low, denoising (jump step) becomes easier, with lower variance, while sampling a "less smooth" \(p(y)\) (walk step) becomes harder. If the noise is high, the opposite is true.

Figure 3 illustrates an example of a walk-jump sampling chain, where generated molecules change gradually as we walk through the chain (the clean samples are shown every ten steps, \( k=10\)). This figure is a demonstration of the fast-mixing properties of our sampling scheme in generating 3D molecules. For instance, some atoms (or other structures like rings) might appear/disappear/change as we move through the chain. Interestingly, this behavior happened on most chains we looked into explicitly.

### Recovering atomic coordinates from voxelized molecules

It is often useful to extract atomic coordinates from generated voxelized molecules (_e.g._, to validate atomic valences and bond types or compare with other models). We use a very simple algorithm (a simplified version of the approach used in ) to recover the set of atomic coordinates from generated voxel grids: first we set to 0 all voxels with value less than.1, _i.e._, \(x_{k}=1_{.1}(x_{k})\). Then we run a simple peak detection to locate the voxel on the center of each Gaussian blob (corresponding to the center of each atom). Finally we run a simple gradient descent coordinate optimization algorithm to find the set of points that best create the generated voxelized molecule. Once we have obtained the optimized atomic coordinates, we follow previous work [33; 18; 17; 20] and use standard cheminformatics software to determine the molecule's atomic bonds. Figure 4 shows our pipeline to recover atomic coordinates and molecular graphs from generated voxelized molecules. See appendix for more details.

## 4 Experiments

In this section, we evaluate the performance of our model on the task of unconditional 3D molecule generation. Our approach is the first of its kind and therefore the objective of our experiments is to show that (i) VoxMol is a feasible approach for unconditional generation (this is non-trivial) and (ii) it scales well with data, beating a established model on a large, drug-like dataset. In principle, VoxMol can be used for guided (or conditional) generation, an arguably more useful application for molecular sciences (see appendix for a discussion on how guidance can be used on generation).

We start with a description of our experimental setup, followed by results on two popular datasets for this problem. We then show ablation studies performed on different components of the model.

### Experimental setup

Architecture.The denoising network \(_{}\) is used in both the walk and jump steps described above. Therefore, its parametrization is very important to the performance of this approach. We use a 3D U-Net  architecture for our denoising network. We follow the same architecture recipe as DDPM , with two differences: we use 3D convnets instead of 2D and we use fewer channels on all layers. The model has 4 levels of resolution and we use self-attention on the two lowest resolutions. We augment our dataset during training by applying random rotation and translation to every training sample. Our models are trained with noise level \(=.9\), unless stated otherwise. We train our models with batch size of 128 and 64 (for QM9 and GEOM-drugs, respectively) and we use AdamW 

Figure 3: Illustration of walk-jump sampling chain. We do Langevin MCMC on the noisy distribution (walk) and estimate clean samples with the denoising network at arbitrary time (jump).

(learning rate \(2 10^{-5}\), weight decay \(10^{-2}\)) to optimize the weights. The weights are updated with exponential moving average with a decay of.999. We use \(=1.0\), \(u=1.0\) and \(=.5\) for all our MCMC samplings. See appendix for more details on the architecture, training and sampling.

Datasets.We consider two popular datasets for this task: _QM9_ and _GEOM-drugs_. QM9 contains small molecules with up to 9 heavy atoms (29 if we consider hydrogen atoms). GEOM-drugs contains multiple conformations for 430k drug-sized molecules and its molecules have 44 atoms on average (up to 181 atoms and over 99% are under 80 atoms). We use grids of dimension \(32^{3}\) and \(64^{3}\) for QM9 and GEOM-drugs respectively. These volumes are able to cover over 99.8% of all points on both datasets. All our models model hydrogens explicitly. For QM9, we consider all 5 chemical elements (C, H, O, N and F) present on the dataset. For GEOM-drugs, we consider 8 elements (C, H, O, N, F, S, Cl and Br). We ignore P, I and B elements as they appear in less than.1% of the molecules in the dataset. Finally, the input voxel grids are of dimension \(^{5 32 32 32}\) and \(^{8 64 64}\) for QM9 and GEOM-drugs, respectively. We perform the same pre-processing and dataset split as  and end up with 100K/20K/13K molecules for QM9 and 1.1M/146K/146K for GEOM-drugs (train, validation, test splits respectively).

Baselines.We compare our method with two state-of-the-art approaches: _GSchNet_, a point-cloud autoregressive model and _EDM_, a point-cloud diffusion-based model. We note that both methods rely on equivariant networks, while ours does not. Our results could potentially be improved by successfully exploiting equivariant 3D convolutional networks. We also show results of VoxMol\({}_{}\) in our main results, where we assume we have access to real samples from the noisy distribution. Instead of performing MCMC to sample \(y_{k}\), we sample molecules from the validation set and add noise to them. This baseline assumes we would have perfect sampling of noisy samples (walk step) and let us assess the quality of our model to recover clean samples. It serves as an upper bound for our model and allows us to disentangle the quality of the walk (sampling noisy samples) and jump (estimating clean molecules) steps.

All methods generate molecules as a set of atom types and their coordinates (in the case of voxelized molecules, we use the post-processing described above to get the atomic coordinates). We follow previous work  and use standard cheminformatics software to determine the molecule's atomic bonds given the atomic coordinates 6. Using the same post-processing for all methods allows a more apples-to-apples comparison of the models.

Metrics.Most metrics we use to benchmark our model come from 7. We draw 10,000 samples from each method and measure performance with the following metrics: _stable mol_ and _stable atom_, the percentage of stable molecules and atoms, respectively, as defined in ; _validity_, the percentage of generated molecules that passes RDKit 's sanitization filter; _uniqueness_, the proportion of valid molecules that have different canonical SMILES; _valency_\(W_{1}\), the Wasserstein distance between the distribution of valencies in the generated and test set; _atoms TV_ and _bonds TV_, the total variation

Figure 4: Pipeline for recovering atomic coordinates from voxel grids: (i) VoxMol generates voxelized molecules, (ii) atomic coordinates are extracted from voxel grid with simple peak detection algorithm, (iii) we use cheminformatics software to add atomic bonds and extract SMILES strings, molecular graphs, etc.

between the distribution of atom types and bond types, respectively; _bond length_\(W_{1}\) and _bond angle_\(W_{1}\), the Wasserstein distance between the distribution of bond and lengths, respectively. Finally, we also report the _strain energy_ metric proposed in . This metric is defined as the difference between the internal energy of the generated molecule's pose and a relaxed pose of the molecule. The relaxation and the energy are computed using the Universal Force Field (UFF)  within RDKit. See appendix for more details about the metrics.

### Experimental results

Table 1 and Table 2 show results on QM9 and GEOM-drugs respectively. We report results for models trained with and without data augmentation (VoxMol and VoxMol\({}_{}\), respectively) and generate 10,000 samples with multiple MCMC chains. Each chain is initialized with 1,000 warm-up steps, as we observed empirically that it slightly improves the quality of generated samples. Then, samples are generated after each 500 walk steps (each chain having a maximum of 1,000 steps after the warm-up steps). Results for our models are shown with mean/standard deviation among three runs. The row _data_ on both tables are randomly sampled molecules from the training set.

On QM9, VoxMol performs similar to EDM in some metrics while performing worse in others (specially stable molecule, uniqueness and angle lengths). On GEOM-drugs, a more challenging and realistic drug-like dataset, the results are very different: _VoxMol outperforms EDM in eight out of nine metrics, often by a considerably large margin_.

Figure 5(a,b) shows the cumulative distribution function (CDF) of strain energies for the generated molecules of different models on QM9 and GEOM-drugs, respectively. The closer the CDF of generated molecules from a model is to that of _data_ (samples from training set), the lower is the strain energy of generated molecules. The ground truth data has median strain energy of 43.87 and 54.95 kcal/mol for QM9 and GEOM-drugs, respectively. On QM9, all models have median strain energy around the same ballpark: 52.58, 66.32 and 56.54 kcal/mol for EDM, VoxMol\({}_{}\) and VoxMol, respectively. On GEOM-drugs, the molecules generated by _VoxMol have considerably lower median strain energy than EDM_: 951.23 kcal/mol for EDM versus 286.06 and 171.57 for VoxMol\({}_{}\) and VoxMol.

We observe, as expected, that augmenting the training data with random rotations and translations improves the performance of the model. The improvement is bigger on QM9 (smaller dataset) than on GEOM-drugs. In particular, the augmentations help to capture the distribution of bonds and angles between atoms and to generate more unique molecules. We note that, unlike EDM, our model does not require knowledge of the number of atoms beforehand (neither for training nor sampling). In fact, Figure 6 show that our model learns the approximate distribution of the number of atoms per molecule on both datasets. Implicitly learning this distribution can be particularly useful in applications related to in-painting (_e.g._, pocket conditioning, linking, scaffold conditioning). Finally, our method generates drug-like molecules in fewer iterations and is faster than EDM on average (see Table 3). EDM sampling time scales quadratically with the number of atoms, while ours has constant time (but scales cubically with grid dimensions).

These results clearly show one of the main advantages of our approach: a more expressive model _scales better_ with data. Architecture inductive biases (such as built-in SE(3) equivariance) are helpful in the setting of small dataset and small molecules. However, on the large-scale regime, a more expressive model is more advantageous in capturing the modes of the distribution we want to model. Compared

  & stable & stable & valid & unique & valency & atom & bond & bond & bond \\  & mol \%\({}_{}\) & atom \%\({}_{}\) & \%\({}_{}\) & \%\({}_{}\) & W\({}_{}\) & TV\({}_{}\) & len \(_{}\) & ang \(_{}\) \\  _data_ & 98.7 & 99.8 & 98.9 & 99.9 &.001 &.003 &.000 &.000 &.120 \\  GSchNet & 92.0 & 98.7 & 98.1 & 94.5 &.049 &.042 &.041 &.005 & 1.68 \\ EDM & 97.9 & 99.8 & 99.0 & 98.5 &.011 &.021 &.002 &.001 & 0.44 \\ VoxMol\({}_{}\) & 84.2 & 98.2 & 98.1 & 77.2 &.043 &.171 &.050 &.007 & 3.80 \\  & \(( 1.6)\\  6\) & \(( 3)\\  9.2\) & \(( 4)\\  9.2\) & \(( 1.7)\\  9.2\) & \(( 0.1)\\  0.09\) & \(( 0.0)\\  0.03\) & \(( 0.0)\\  0.02\) & \(( 0.0)\\  0.04\) \\ VoxMol & 89.3 & 99.2 & 98.7 & 92.1 &.023 &.029 &.009 &.003 & 1.96 \\  & \(( 6)\\  6\) & \(( 1)\\  1\) & \(( 1)\\  3\) & \(( 3)\\  0.02\) & \(( 0.009)\\  0.009\)to VoxMol\({}_{}\) results, we see that VoxMol can still be vastly improved. We can potentially close this gap by improving the quality of the denoising network (_e.g._, by improving the architecture, train on more data, efficient built-in SE(3)-equivariance CNNs, etc).

### Ablation studies

Noise level \(\).Unlike diffusion models, the noise level is considered fixed during training and sampling. It is an important hyperparameter as it poses a trade-off between the quality of the walk step (Langevin MCMC) and the jump step (empirical Bayes). The ideal noise level is the highest possible value such that the network can still learn how to denoise. We train models on QM9 with \(\) in \(\{.6,.7,.1,.2\}\), while keeping all other hyperparameters the same. Figure 7(a,b,c) shows how noise level \(\) influences the performance on the validation set. While most metrics get better as the noise level increases, others (like stable molecules and valency W1) get worse after a value. We observe empirically that \(=.9\) is the sweet spot level that achieves better overall performance on the validation set of QM9.

Number of steps \( k\).Table 3 shows how VoxMol's performance on GEOM-drugs change with the number of walk steps \( k\) on the Langevin MCMC sampling. In this experiment, we use the same trained model and only change the number of steps during sampling. Results of EDM are also shown for comparison (it always requires 1,000 diffusion steps for generation). We see that some metrics barely change, while others improve as \( k\) increases. The average time (in seconds) to generate a

  & stable & stable & valid & unique & valency & atom & bond & bond & bond \\  & mol \%\({}_{}\) & atom \%\({}_{}\) & \%\({}_{}\) & \%\({}_{}\) & W\({}_{1}\) & TV\({}_{}\) & TV\({}_{}\) & len W\({}_{1}\) & ang W\({}_{1}\) \\  _data_ & 99.9 & 99.9 & 99.8 & 100. &.001 &.001 &.025 &.000 &.050 \\  EDM & 40.3 & 97.8 & 87.8 & 99.9 &.285 &.212 &.048 &.002 & 6.42 \\ VoxMol\({}_{}\) & 44.4 & 96.6 & 89.7 & 99.9 &.238 &.025 &.024 &.004 & 2.14 \\  & \({}_{( 1)}\) & \({}_{( 1)}\) & \({}_{( 2)}\) & \({}_{( 0)}\) & \({}_{( 001)}\) & \({}_{( 001)}\) & \({}_{( 001)}\) & \({}_{( 000)}\) & \({}_{( 02)}\) \\ VoxMol & 75.0 & 98.1 & 93.4 & 99.1 &.254 &.033 &.036 &.002 & 0.64 \\  & \({}_{( 1)}\) & \({}_{( 3)}\) & \({}_{( 5)}\) & \({}_{( 2)}\) & \({}_{( 003)}\) & \({}_{( 041)}\) & \({}_{( 006)}\) & \({}_{( 001)}\) & \({}_{( 13)}\) \\  VoxMol\({}_{}\) & 81.9 & 99.0 & 94.7 & 97.4 &.253 &.002 &.024 &.001 & 0.31 \\ 

Table 2: Results on GEOM-drugs. We use 10,000 samples from each method. Our results are shown with mean/standard deviation across 3 runs.

Figure 5: The cumulative distribution function of strain energy of generated molecules on (a) QM9 and (b) GEOM-drugs. For each method, we use 10,000 molecules.

Figure 6: Empirical distribution of number of atoms per molecule on QM9 (left) and GEOM-drugs (right). We sample 10,000 molecules from train set and generate the same number of VoxMol samples.

molecule increases linearly with the number of steps, as expected. We observe that even using 500 steps, our model is still faster than EDM on average, while achieving better performance in these metrics. Remarkably, with only 50 steps, VoxMol already outperforms EDM in most metrics, while being _an order of magnitude_ faster on average.

Atomic density radii.We also assess how the performance of the model changes with respect to the size of atomic radii chosen during the voxelization step (while always keeping the resolution of the grid fixed at.25A). See appendix for how this is done. We tried four different values for the radii (same for all elements):.25,.5,.75 and 1.0. We observe--throughout different versions of the model, with different hyperparameters--that using a fixed radius of.5 consistently outperform other values. Training does not converge with radius.25 and quality of generated samples degrades as we increase the radius. We also tried to use Van der Waals radii (where each atom type would have their own radius), but results were also not improved.

## 5 Conclusion

We introduce VoxMol, a novel score-based method for 3D molecule generation. This method generates molecules in a fundamentally different way than the current state of the art (_i.e._, diffusion models applied to atoms). The noise model used is also novel in the class of score-based generative models for molecules. We represent molecules on regular voxel grids and VoxMol is trained to predict "clean" molecules from its noised counterpart. The denoising model (which approximates the score function of the smoothed density) is used to sample voxelized molecules with walk-jump sampling strategy. Finally atomic coordinates are retrieved by extracting the peaks from the generated voxel grids. Our experiments show that VoxMol scales better with data and outperforms (by a large margin) a representative state of the art point cloud-based diffusion model on GEOM-drugs, while being faster to generate samples.

Broader impact.Generating molecules conditioned on some desiderata can have huge impacts in many different domains, such as, drug discovery, biology, materials, agriculture, climate, etc. This work deals with unconditional 3D molecule generation (in a pure algorithmic way): a problem that can be seen as an initial stepping stone (out of many) to this long-term objective. We, as a society, need to find solutions to use these technologies in ways that are safe, ethical, accountable and exclusively beneficial to society. These are important concerns and they need to be thought of at the same time we design machine learning algorithms.

Acknowledgements.The authors would like to thank the whole Prescient Design team for helpful discussions and Genentech's HPC team for providing a reliable environment to train/analyse models.

 \( k\) & stable & stable & valid & unique & valency & atom & bond & bond & bond & avg. t \\ (n steps) & mol \%\({}_{}\) & atom \%\({}_{}\) & \%\({}_{}\) & \%\({}_{}\) & W\({}_{1}\) & TV\({}_{}\) & TV\({}_{}\) & len \(_{1}\) & ang \(_{1}\) & s/mol\({}_{}\) \\ 
50 & 78.9 & 98.7 & 96.3 & 87.8 &.250 &.073 &.102 &.002 & 1.18 & 0.90 \\
100 & 78.6 & 98.6 & 95.5 & 94.3 &.256 &.050 &.101 &.002 & 1.62 & 1.64 \\
200 & 77.9 & 98.4 & 94.4 & 98.6 &.253 &.037 &.104 &.002 & 1.02 & 3.17 \\
500 & 76.7 & 98.2 & 93.8 & 99.2 &.252 &.043 &.042 &.002 & 0.56 & 7.55 \\
1,000 & 75.5 & 98.4 & 93.4 & 99.8 &.257 &.029 &.050 &.002 & 0.79 & 14.9 \\  EDM & 40.3 & 97.8 & 87.8 & 99.9 &.285 &.212 &.048 &.002 & 6.42 & 9.35 \\ 

Table 3: Effect of number of walk steps \( k\) on generation quality on GEOM-drugs (2,000 samples). EDM results are shown for comparison.

Figure 7: Effect of noise level \(\) on generation quality. Models are trained on QM9 with a different noise level. Each plot shows two metrics: (a) molecule stability and uniqueness, (b) atom and bond TV, (c) valency and angle lengths W1.