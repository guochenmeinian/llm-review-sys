# Disentangled Unsupervised Skill Discovery

for Efficient Hierarchical Reinforcement Learning

 Jiaheng Hu

University of Texas at Austin

jiahengh@utexas.edu

&Zizhao Wang

University of Texas at Austin

zizhao.wang@utexas.edu

&Peter Stone

University of Texas at Austin, Sony AI

pstone@cs.utexas.edu

&Roberto Martin-Martin

University of Texas at Austin

robertomm@cs.utexas.edu

Equal supervision.

###### Abstract

A hallmark of intelligent agents is the ability to learn reusable skills purely from unsupervised interaction with the environment. However, existing unsupervised skill discovery methods often learn _entangled_ skills where one skill variable simultaneously influences many entities in the environment, making downstream skill chaining extremely challenging. We propose **Disentangled Unsupervised Skill Discovery (DUSDi)**, a method for learning _disentangled skills_ that can be efficiently reused to solve downstream tasks. DUSDi decomposes skills into disentangled components, where each skill component only affects one factor of the state space. Importantly, these skill components can be **concurrently** composed to generate low-level actions, and efficiently chained to tackle downstream tasks through hierarchical Reinforcement Learning. DUSDi defines a novel mutual-information-based objective to enforce disentanglement between the influences of different skill components, and utilizes value factorization to optimize this objective efficiently. Evaluated in a set of challenging environments, DUSDi successfully learns disentangled skills, and significantly outperforms previous skill discovery methods when it comes to applying the learned skills to solve downstream tasks. Code and skills visualization at jiahenghu.github.io/DUSDi-site/.

## 1 Introduction

Reinforcement learning (RL) algorithms have achieved many successes in challenging tasks, including magnetic plasma control , automobile racing , and robotics . However, applying existing RL algorithms to every new task in a _tabula rasa_ manner often results in low sample efficiency that limits RL's broader applicability . Unsupervised skill discovery holds the promise of improving the sample efficiency of Reinforcement Learning, by learning a set of reusable skills through reward-free interaction with the environment that can be later recombined to tackle multiple downstream tasks more efficiently. In practice, prior unsupervised RL skills are represented as a policy that conditions on a skill variable to generate diverse behaviors, and have led to successful and efficient learning of downstream tasks when combined with skill fine-tuning or hierarchical RL skill selection .

Despite prior successes, a common limitation of the skills learned by existing unsupervised RL methods is that they are _entangled_: any change in the skill variable causes the agent to induce changes in _multiple dimensions_ of the state space simultaneously. Learning to use and recombine these _entangled_ skills can be extremely hard for an agent trying to solve downstream tasks, especially incomplex domains like multi-agent systems or household humanoid robots, where the agent needs to concurrently change multiple independent dimensions of the state to complete the task. For example, consider an agent learning to operate a car: if a single skill variable simultaneously changes the speed, steering, and headlights of the car, it will be extremely challenging for the agent to learn how to turn on/off the headlights while keeping the car at the right speed and direction. In contrast, humans naturally have the ability to concurrently and independently adjust the car's acceleration, steering, and headlights based on the car's current speed, surroundings, and lighting conditions. In other words, humans naturally obtain _disentangled_ skill components where each component only affects one or few state variables, and can easily recombine these skill components into _compositional skills_ to control multiple factors simultaneously.

In this work, we aim to create such a mechanism for artificial agents to learn disentangled skills that facilitate solving downstream tasks. We introduce Disentangled Unsupervised Skill Discovery (DUSDi), a novel method for unsupervised discovery of disentangled skills. A key insight of DUSDi is to take advantage of state factorization that is naturally available in unsupervised RL environments [13; 35; 17] (e.g. speed, direction, and lighting conditions of the car in the driving example; the state of different objects in a household environment). These factored state spaces provide a natural inductive bias we leverage for disentanglement: DUSDi decomposes skills into disentangled components, and encourages each skill component to affect only one state factor while discouraging it from affecting any other factors. To that end, DUSDi designs a novel intrinsic reward for unsupervised skill learning based on mutual information (MI) between disentangled skills and state factors: the learning agent receives high rewards for 1) increasing the MI between a state factor and the skill component assigned to change it, and 2) for decreasing the MI between that skill component and all other state factors.

DUSDi introduces a set of technical innovations to tractably and efficiently optimize the proposed mutual information objective. Once the DUSDi skills are learned, they can be used as the low-level policy in a hierarchical reinforcement learning (HRL) setting to tackle downstream tasks. Compared to using entangled skills, a key benefit of using the disentangled DUSDi skills is that they guarantee more efficient exploration during downstream task learning and therefore often lead to significantly better performance. Furthermore, the structured skill space of DUSDi opens up additional possibilities to inject domain knowledge into the learning process to further improve the efficiency of both skill learning and downstream task learning.

DUSDi is easy to implement and can be integrated into any MI-based unsupervised skill discovery approach. In our experiments, we integrate DUSDi with DIAYN  and evaluate the performance on four domains: a 2D agent navigation domain, a DMC walker domain, a large-scale multi-agent particle domain, and a 3D realistic simulated robotics domain. Our experiments indicate that DUSDi can indeed learn disentangled skills, and significantly outperforms other Unsupervised Reinforcement Learning methods on solving complex downstream tasks with HRL.

Figure 1: Consider an agent practicing driving skills by learning to control a car’s speed (length of orange arrow), steering (curvature of orange arrow), and headlights (blue symbol), **(Left)** previous unsupervised skill discovery methods learn _entangled_ skills, where a change in the skill variable can cause all three environment factors to change **(Right)** DUSDi learns _disentangled skills_ with concurrent components, where each skill component only affects one factor of the state space, enabling efficient downstream task learning with hierarchical RL.

## 2 Preliminaries

Factored Markov Decision Process (f-MDP)In this work, we consider unsupervised skill discovery in a reward-free Factored Markov Decision Process. Following Osband and Van Roy , Mohan et al. , we define a Factored Markov Decision Process by the tuple \(=(,\,,\,)\), where \(=^{1}^{N}\) is a factored state space with \(N\) factors such that each state \(s\) consists of \(N\) state factors: \(s=(s^{1},\,,s^{N}),s^{i}^{i}\). \(\) is the action space, and \(\) is an unknown Markovian transition model, \(\). Notice that a factored state space is often naturally available in domains used by prior works [13; 24; 35; 17; 9; 49] as it can naturally represent environments with separate elements (e.g., objects) that can be changed independently. DUSDi leverages the property that factors often have sparse dynamics dependencies, which opens up the possibility of learning disentangled skills to control the state of each factor. Moreover, many downstream tasks are defined by specific changes in one or a few factors (e.g., changing the state of a single object and not others), which are easier to learn with disentangled skills. In domains with only image-based (unfactored) observations, a factored state space can be extracted using disentangled representation learning or object-centric representation learning methods [31; 20], which we empirically evaluated in Sec. 4.5.

Mutual-Information-Based Skill DiscoveryMutual-information-based skill discovery methods, such as the paradigmatic DIAYN , specify the skills with a latent variable \(z\), and learns a skill-conditioned policy \((a|s,z)\). The optimization objective these methods use to learn the skills is to maximize the mutual information (MI) between the state, \(s\), and the skill latent variable, \(z\): \(I(;)\), which incentivizes the agent to reach diverse and distinguishable states. One popular way to determine the MI, \(I(;)\), is to decompose it as \(I(;)=H()-H(\,|\,)\), where \(H\) denotes entropy. Since the skill variable is typically sampled from a fixed distribution, \(H()\) can be assumed constant: maximizing \(I(;)\) is thus equivalent to minimizing \(H(\,|\,)\). Following the definition of conditional entropy, \(-H(\,|\,)=_{s,z}[ p(z|s)]\), DIAYN proposes to approximate \(p(z|s)\) with a learned _discriminator_\(q(z|s)\) that predicts the skill latent, \(z\), given the state, \(s\).

After discovering the skills, mutual-information-based methods apply them to learn downstream reward-supervised tasks. Many methods (e.g., DIAYN) adopt a hierarchical RL structure for this second phase, where the skill policy is used as a low-level "frozen" element, and a high-level policy \(_{}(z|s)\) learns to sequentially activate skill \(z\) based on observations. The high-level policy is trained to maximize the provided task reward, \(\), with \(\) as the action space.

## 3 Learning Disentangled Skills with DUSDi

Similar to prior works in unsupervised skill discovery, DUSDi implements a two-stage learning procedure for the agents: in the first phase, DUSDi develops a library of skills without external reward (Sec. 3.1). The key to DUSDi's success is to encourage disentanglement between different skill components through a novel learning objective that restricts the effect of each disentangled skill

Figure 2: Two learning stages of DUSDi: **(a)** in _disentangled_ skill learning stage, DUSDi creates a one-to-one mapping between state factors and skill components — each disentangled skill component \(z^{i}\) only influences state factor \(s^{i}\). DUSDi designs a novel mutual-information-based intrinsic reward to enforce disentanglement and utilize \(Q\)-value decomposition to learn the skill policy \(_{}\) efficiently. **(b)** in the task learning stage, the skill policy is used as a frozen low-level policy and a high-level policy \(_{}\) is learned to select skill \(z\) for every \(L\) steps, by maximizing the task reward \(r^{}\).

component to independent factors. In the second phase, \(\) leverages the learned skills to solve downstream tasks through Hierarchical Reinforcement Learning, achieving higher returns \(_{}\) than methods with entangled skills (Sec. 3.3). In practice, learning disentangled skills in environments with many factors can be challenging. To address this challenge, we introduce improvements to \(\)'s first phase based on Q-function decomposition (Sec. 3.2). We present the entire \(\) pipeline in Fig. 2, and the pseudo-code in Alg. 1.

```
1:Initialize skill policy \(_{}\), discriminators \(q^{i}_{}\), \(q^{i}_{}\) and value function \(Q^{i}\) for each state factor \(^{i}\).
2:for each skill training episode do
3: Sample skill \(z p(z)\).
4: Collect state transitions with actions from \(_{}(a|s,z)\).
5: Sample a batch of \((s,a,z)\) from the replay buffer.
6:for\(i=1,,N\)do
7: Update \(q^{i}_{}(z^{i}|s^{i})\) and \(q^{i}_{}(z^{i}|s^{ i})\) with discrimination losses.
8: Calculate \(r^{i}\) based on Eq. 4
9: Update \(Q^{i}(s,a,z)\) with reward \(r^{i}\) using SAC.
10:endfor
11: Update \(_{}\) with \(Q=_{i=1}^{N}Q^{i}\) using SAC.
12:endfor ```

**Algorithm 1**\(\) Skill Learning

### Disentangled Skill Spaces and Learning Objective

\(\) aims to create disentangled skill components that can be easily recombined to solve downstream tasks. To that end, \(\) proposes a novel factorization of the latent skill conditioning variable, \(z\), into \(N\) independent disentangled components such that the latent space \(\) becomes \(=^{1}^{N}\). We equate \(N\) to the number of state factors and consider \(z^{i}^{i}\) the disentangled skill component that affects state factor \(i\). The skill policy \((a|s,z)\) takes in \(z\), which is a composition of the skill components.

While the factored latent space \(\) could be discrete or continuous, we consider discrete skill space in this paper, and discuss how \(\) can be applied to continuous skills in Appendix A. We can then assume that each disentangled component \(z^{i}\) takes the form of an integer, \(z^{i}[1,k]\), resulting in a compositional skill, \(z\), with the form of a \(N\)-dimensional multi-categorical vector with \(k^{N}\) possible values. During skill learning, we independently sample each disentangled component \(z^{i}\) from a fixed uniform distribution \(p(z^{i})\), similar to previous works [13; 35].

Given this factored skill space, our goal is to learn a skill policy network, \(_{}:\), such that each disentangled component \(^{i}\) affects and only affects the value of a state factor, \(^{i}\). For each disentangled component and state factor pair \((^{i},^{i})\), we encourage diverse and distinguishable behaviors by maximizing their mutual information \(I(^{i};^{i})\). While this objective enables a disentanglement skill component to affect the corresponding factor, it does not restrict the component from affecting other factors. This is undesirable since the resulting skill components would still be entangled in their effects. To prevent that, we propose to ensure that each skill component, \(^{i}\), minimally affects the rest of the state factors, \(^{ i}\), where \(^{ i}\) denotes the subspace formed by all other state factor spaces except \(^{i}\): \(^{1},^{i-1}^{i+1} ^{N}\). Specifically, we incorporate an entanglement penalty to minimize, \(I(^{ i};^{i})\), which corresponds to the mutual information between a skill component and all other state factors that it should not affect.

Formally, the skill policy aims to maximize the following objective:

\[()=_{i=1}^{N}I(^{i};^{i})- I (^{ i};^{i}),\] (1)

where \(<1\) is a hyperparameter that controls the importance of the entanglement penalty relative to the skill-factor association. We restrict \(\) to be smaller than one for the following reason: in some environments, due to intrinsic dynamical dependencies between state factors themselves, controllinga state factor, \(^{i}\), has to introduce some association between \(^{i}\) and other factors in \(^{ i}\), e.g., when controlling an object whose manipulation requires the agent to use other objects as tools. In these cases, as the policy learns to maximize the MI between a skill and a factor, \(I(^{i},^{i})\), the MI with other factors, \(I(^{ i};^{i})\), may also increase. For these cases, the use of \(<1\) will ensure that the entanglement penalty does not overpower the association reward, and the policy is still incentivized to learn disentangled skill components that change \(S^{i}\) distinguishably while introducing minimal changes on other factors. In practice, we simply set \(=0.1\) in all our experiments.

**Optimizing DUSDi's Objective:** Directly maximizing the objective in Eq. 1 is intractable. Alternatively, we propose to approximate the objective using a variational lower bound of the mutual information :

\[I(^{i};^{i})=H(^{i})-H(^{i}| ^{i}) C+_{z,s} q^{i}_{}(z^{i}|s^{i}),\] (2)

where \(C\) represents the constant value of \(H(^{i})\), the entropy of the prior distribution over the skill latent variable, which does not change during training, and \(q^{i}_{}\) is a variational distribution.

Similarly, we can approximate the MI in the entanglement penalty by:

\[I(^{ i};^{i}) C+_{z,s} q^{i}_{ }(z^{i}|s^{ i}),\] (3)

where \(q^{i}_{}\) is another variational distribution3. Importantly, when these \(q\) approximations perfectly recover the posterior distribution of \(z^{i}\), we obtain equality in Eq. 2 and Eq. 3. In DUSDi, we implement the variational distributions, \(q_{}\) and \(q_{}\), as neural network discriminators mapping input state factor(s) to the predicted disentangled component values, \(z^{i}\).

To optimize \(()\), we alternate between two steps: 1) performing variational inference to train the discriminators \(q^{i}_{}\) and \(q^{i}_{}\) through gradient ascent, and 2) using \(q^{i}_{}\) and \(q^{i}_{}\) to learn a disentangled skill policy \(_{}\) through RL by maximizing the following intrinsic reward approximating Eq. 1:

\[r_{z}(s,a)_{i=1}^{N}q^{i}_{}(z^{i}|s^{i})- q^{i}_{ }(z^{i}|s^{ i})\] (4)

Interestingly, the decomposed nature of our intrinsic reward allows a convenient avenue for shaping skill behaviors based on domain knowledge. In particular, we can restrict a state factor \(s^{i}\) to only take certain values by constraining \(q^{i}_{}(z^{i}|s^{i})\) accordingly. While not the main focus of this work, we briefly explore this further optimization enabled by DUSDi in Appendix H.

### Accelerating Skill Learning through Q Decomposition

When using reinforcement learning (RL) to optimize the intrinsic reward function defined in Eq. 4, standard RL algorithms treat the reward function as a black box and learn a single value function from the mixture of intrinsic reward terms. While this approach may be sufficient for environments with few state factors, doing so for complex environments with many state factors (large \(N\)) often leads to suboptimal solutions. A key reason is that the mixture of \(2N\) reward terms leads inevitably to high variance in the reward, making the value of the Q function oscillate. Furthermore, the sum of reward terms obscures information about each term's value, which hinders credit assignment.

DUSDi overcomes this issue by leveraging the fact that the intrinsic reward function in Eq. 4 is a linear sum over terms associated with each disentangled component. Thanks to the linearity of expectation, we can decompose the Q function into \(N\) disentangled Q functions as follows:

\[Q_{}(s,a,z) =_{}[_{t=0}^{}^{t}r_{t}]\] \[=_{}[_{t=0}^{}^{t}_{i=1}^{N}q_ {}^{i}(z^{i}|s^{i})- q_{}^{i}(z^{i}|s^{-i})]\] \[=_{i=1}^{N}_{}[_{t=0}^{}^{t}( q_{}^{i}(z^{i}|s^{i})- q_{}^{i}(z^{i}|s^{-i}))]\] \[=_{i=1}^{N}Q^{i}(s,a,z)\] (5)

where \(Q^{i}\) represents each disentangled Q function, one for each disentangled component. The disentangled Q functions can be then updated only with their corresponding intrinsic reward terms, \(r^{i} q_{}^{i}(z^{i}|s^{i})- q_{}^{i}(z^{i}|s^{-i})\). During policy learning, we sum all disentangled Q functions together to recover the global critic, \(Q_{}\), as shown in Fig. 2 (a), top. Compared to learning \(Q_{}\) directly from all \(2N\) reward terms, learning disentangled Q functions significantly reduces reward variance, allowing \(Q_{}\) to converge faster and more stably.

### Downstream Task Learning

Similar to Eysenbach et al. , in DUSDi we utilize hierarchical RL to solve reward-supervised downstream tasks with the discovered skills, as depicted in Fig.2 (b). The skill policy, \(_{}:\), acts as the low-level policy and is kept constant while a high-level policy, \(_{}:\), learns to select which skill to execute for \(L\) steps using the skill latent variable, \(z\). Thus, the skill latent conditioning space, \(\), acts as the action space of the high-level policy, \(_{}\). As extensively evaluated in our experiments, without any additional "ingredient", performing downstream task learning in the action space formed by DUSDi skills often results in significantly super performance compared to an action space formed by entangled skills. We show that the superior performance of DUSDi can be explained by **more efficient exploration** when using the DUSDi skills for hierarchical RL, which we elaborate on in Appendix B, through analyzing the benefits and search complexity of DUSDi's skill space over DIAYN's.

Depending on the nature of the downstream tasks, we can often take further advantage of the disentangled skills learned by DUSDi through leveraging its structure. One such scenario is when the downstream task has a composite reward function consisting of multiple terms. Previous works [16; 46] have shown that when the causal dependencies from action dimensions to reward terms are available (e.g., the reward for speed only depends on actions that affect speed), one can use Causal Policy Gradient (CPG) to decompose the policy update (e.g., only the "speed actions" get updated by the speed reward) and greatly improve sample efficiency, especially when the dependencies are sparse. In downstream task learning, with an action space (of the high-level policy) consisting of the skills learned by DUSDi, we have a convenient way of applying causal policy gradient, where the causal dependencies between the action dimensions (i.e., skill components) and reward terms are often sparse and can be easily obtained by examining the state factor that a skill component is associated with, which we evaluate empirically in Sec. 4.6.

## 4 Experimental Evaluation

In the evaluation of DUSDi, we aim to answer the following questions: **Q1**: Are skills learned by DUSDi truly disentangled (Sec. 4.2)? **Q2**: Can Q-decomposition improve skill learning efficiency (Sec. 4.3)? **Q3**: Do our disentangled skills perform better when solving downstream tasks compared to other unsupervised reinforcement learning methods (Sec. 4.4)? **Q4**: Can DUSDi be extended to image observation environments (Sec.4.5)? **Q5**: Can we leverage the structured skill space of DUSDi to further improve downstream task learning efficiency (Sec.4.6)?

### Evaluation Environments

Previous works [13; 34; 35; 44; 23] extensively rely on standard RL environments such as DMC  and OpenAI Fetch  to evaluate unsupervised RL methods. However, unlike previous unsupervised skill discovery methods, DUSDi focuses on learning a set of disentangled skill components that can be concurrently executed and re-combined to complete downstream tasks. As such, it only makes sense to examine the performance of DUSDi in challenging tasks that require concurrent control of many environment entities (e.g. multi-agent systems, complex household robots). Previous environments lack this property: in DMC for example, while the state and action space can be very complex, the predominant downstream tasks are just to move the center-of-mass of the agent to different places. In such cases, there is no need for concurrent skill components, and therefore we do not expect large gains from using DUSDi's disentangled skills. Nevertheless, we include an evaluation on the **DMC-Walker** environment to demonstrate that our method is also applicable to those environments, but focus the majority of our evaluation on environments that DUSDi is designed for, including **2D Gunner**, **Multi-Particle**, and **iGibson**.

The 2D gunner is a relatively simple domain, where a point agent can navigate inside a continuous 2D plane, collecting ammo and shooting at targets. Multi-Particle is a multi-agent domain modified based on . In this domain, a centralized controller simultaneously controls 10 heterogenous point-mass agents to interact with 10 stations, where each agent can only interact with a specific station. We evaluate in this domain to test the scalability of our methods to a large number of state factors. iGibson  is a challenging simulated robotics domain with the same action space and complexity as real-world robots, where a mobile manipulator can navigate in a room, inspect the room using its head camera, and interact with electric appliances in the room by pointing a remote control to them and switching them on/off. We evaluate in this domain to examine whether our method can handle home-like environments with complex dynamics. We provide visualizations and additional information about each of the environments in Appendix C.

### Evaluating Skill Disentanglement

First, we examine whether the skills learned by DUSDi are truly disentangled (**Q1**) using the DCI metric proposed by Eastwood and Williams . The DCI metric consists of three terms, namely **disentanglement**, **completeness**, and **informativeness**, explained in detail in Appendix F. In the original work, measuring DCI requires knowing the ground truth generative factors. In our case, the generative factors are simply the state factors, and we only need to discretize the value of each state factor to make it compatible for evaluation. For each method on each domain, we collect \(100K\) rollout steps using the learned skill policy, \((s,z)\), where the skill is (re)sampled from the uniform prior distribution, \(p(z)\), every 50 steps. These (state, skill) pairs are then used to calculate DCI.

We compare against **DIAYN-MC** (Multi-channel DIAYN) that uses the same skill representation as DUSDi but optimizes the DIAYN objective of \(I(;)\), and show results in Table 1. Unsurprisingly, DUSDi significantly outperforms DIAYN-MC, especially on Disentanglement and Completeness, across all three environments. These results indicate that DUSDi learns truly disentangled skills, enabling efficient downstream task learning, as we will show in Sec. 4.4. We encourage the readers to visit our project website for a qualitative visualization of the learned skills.

### Evaluating Skill Learning Efficiency with Q-decomposition

To examine the importance of Q-decomposition (**Q2**), we measure the performance of optimizing the DUSDi objective during skill learning with and without a decomposed Q network. We compare the classification accuracy of the skill discriminators \(q_{}^{i}(z^{i}|s^{i})\), averaged over all skill channels, which

    &  &  &  \\  & DUSDi (ours) & DIAYN-MC & DUSDi (ours) & DIAYN-MC & DUSDi (ours) & DIAYN-MC \\  Disentanglement (\(\)) & **0.864**\(\) 0.018 & 0.016 \(\) 0.002 & **0.705**\(\) 0.037 & 0.002 \(\) 0.000 & **0.833**\(\) 0.022 & 0.017 \(\) 0.006 \\ Completeness (\(\)) & **0.864**\(\) 0.017 & 0.024 \(\) 0.004 & **0.750**\(\) 0.041 & 0.003 \(\) 0.000 & **0.834**\(\) 0.021 & 0.019 \(\) 0.005 \\ Informativeness (\(\)) & **0.897**\(\) 0.012 & 0.821 \(\) 0.010 & **0.849**\(\) 0.052 & 0.791 \(\) 0.032 & **0.854**\(\) 0.006 & 0.752 \(\) 0.015 \\   

Table 1: Evaluation of skill disentanglement based on the DCI metric, shown as mean and standard deviation across skill policies trained with 3 random seeds.

indicates progress towards discovering diverse and distinguishable skills, with higher accuracy being better. We depict our results in Fig. 3. We observe that Q-decomposition has a similar performance to the regular Q network in the simplest 2D gunner domain, but significantly outperforms the regular Q network in domains with more state factors (Multi-Particle) and more complex dynamics (iGibson), suggesting that Q-decomposition is necessary for scaling towards complex domains.

### Evaluating Downstream Task Learning

The promise of DUSDi is to incorporate disentanglement into skills so that the skills can be effectively used in downstream task learning. Therefore, the most critical evaluation of our work focuses on comparing the performance of different unsupervised RL methods on task learning (**Q3**). We compare against existing state-of-the-art unsupervised reinforcerment learning algorithms, including **DIAYN**, **CIC**, **CSD**, **METRA**, **ICM**, **RND**, **ELDEN**, and **Vanilla RL**, where these baselines are further explained in Appendix E.

Similar to the evaluation setting in the URLB benchmark , we allow each method to train for 4 million steps without access to reward (i.e., pretraining phase) before the reward is revealed to the agent and the downstream learning takes place. During the pre-training phase, all methods use soft actor-critic (SAC)  to optimize the intrinsic reward. For all skill discovery methods (i.e., DUSDi, DIAYN, CIC, CSD, METRA), a skill-conditioned policy, \(_{}(a|s,z)\), is learned during the pretraining phase. During downstream learning, the skill network is fixed, whereas an upper policy, \(_{}(z|s)\), is trained using proximal policy optimization (PPO)  to optimize the task reward. Similar to previous works [13; 44], we omit proprioceptive states from the MI optimization for all skill discovery methods to facilitate more meaningful explorations. For exploration methods (i.e., RND, ICM, ELDEN), a policy \(_{}(a|s)\) is learned during the pretraining phase on intrinsic reward and fine-tuned using the task reward during the downstream learning phase. The hyperparameters are specified in Appendix G.

We evaluate all methods in four environments and 13 downstream tasks, detailed in Appendix D. The results are depicted in Fig. 4. As expected, DUSDi performs similarly to previous unsupervised RL methods in the DMC walker environment due to the simplicity in terms of its downstream objectives (all related to center-of-mass locomotion), but significantly outperforms all previous methods on domains where downstream tasks require coordinative control of multiple state factors. The most crucial comparison is between DUSDi and DIAYN. DIAYN is a special case of DUSDi where there is only one state factor (consisting of the entire state) and one skill component. Therefore comparing against DIAYN offers a straightforward examination of the effect of disentangled skills for downstream task learning. DUSDi significantly outperforms DIAYN in all downstream tasks, demonstrating the effectiveness of using disentangled skills. In general, we found exploration-based methods to be less capable than skill discovery methods, possibly due to their lack of temporal abstraction. CIC performs very poorly, likely because the CIC objective does not explicitly encourage distinguishable skills and instead generates the intrinsic reward solely based on state entropy, making it very hard for the upper policy to select the right skill. This result again shows the importance of having a proper skill representation. DUSDi also outperforms CSD and METRA on most downstream tasks, especially on the more complex and high-dimensional domains, like Multi-Particle. This superiority is perhaps surprising considering that in our experiments, DUSDi only relies on the simple DIAYN-style intrinsic reward for skill discovery, but further demonstrates the importance of learning a disentangled skill space. It is important to notice that many techniques proposed to improve skill discovery quality

Figure 3: Evaluation of the effect of Q-decomposition in skill learning. The plots depict the mean and standard deviation of accuracy (\(\)) when predicting the skill component \(z^{i}\) based on the state factor \(s^{i}\), computed across 3 training processes. The higher prediction accuracy indicates that the policy learns to control more state factors in more distinguishable ways, leading to more efficient downstream task learning.

(e.g., Baumli et al. , Zhao et al. ), can be seamlessly incorporated into DUSDi. Therefore, we expect our method to perform even better as new advances are made in unsupervised skill discovery.

### Extending DUSDi to Image Space

Although this paper primarily focuses on applying DUSDi to factored state space, we can straightforwardly extend it to image space through existing works in factored / object-centric representation learning [29; 20; 53; 28; 55] (**Q4**). We empirically illustrate this capability in the Multi-Particle environment, where we replace the low-dimensional state observation with \(64 64\) image observations. Specifically, we first pretrain an object-centric encoder following Yang et al. , and then use our method on top of the extracted representation to learn disentangled skills. Hence, essentially, the skill policy uses images as observation. As shown in Fig. 5, when learning from image observation, DUSDi achieves similar performance to learning from state space, whereas the baseline methods are unable to learn these two tasks even when learning from the low-dimensional state space as in Fig. 4.

### Leveraging Structure of DUSDi Skills

While DUSDi can already learn downstream tasks quite efficiently, it is possible to further improve the sample efficiency of downstream task learning through leveraging the structured skill space of DUSDi (**Q5**), as described in the second paragraph of Sec.3.3. Specifically, we apply Causal Policy Gradient  to the Multi-Particle domain, where the causal dependencies between state factors and reward terms are easy to identify. We present our results in Fig. 6, where the sample efficiency of downstream task learning is greatly improved thanks to the structured skill space of DUSDi.

## 5 Related Work

**Unsupervised Skill Discovery** In unsupervised skill discovery, the goal of an agent is to learn task-agnostic skills without external rewards. To learn such skills, previous methods propose various forms of intrinsic reward: (1) maximizing the mutual information between visited states and the skill variables [13; 44; 6; 24], (2) maximizing the traveled distance along the direction specified by the skill variables [34; 35; 36], (3) learning to reach a diverse set of goals [52; 40; 38]. These skills can be used to boost the sample efficiency of downstream task learning, for example, (1) using hierarchical RL where a high-level policy learns to select which skill to execute , or (2) using the skill policy to initialize the task solving policy and then fine-tuning it .

Figure 4: Training curves of DUSDi and baselines on multiple downstream tasks (reward supervised second phase). The plots depict the mean and standard deviation of the return of each method over 3 random seeds. DUSDi outperforms all baselines that learn entangled skills, converging faster and to higher returns.

**State Space Factorization in RL** In RL, there is a long history of leveraging state factorization, including learning a world model between state factors for planning [22; 50], augmenting data , and providing intrinsic rewards [42; 17; 9]. Relevant to our work are skill discovery methods that learn to either reach a goal for each controllable object [19; 9] or achieve interactions between a pair of specified objects . Though these methods achieve disentanglement by influencing one or a pair of objects during a skill, they do not apply to tasks that require controlling multiple objects simultaneously, like driving where we need to control the car's speed and heading directions at the same time. In contrast, our method can combine disentangled skill components into concurrent skills  to solve a wide range of tasks.

**Disentanglement in Skill Learning** Inspired by the benefits of compositionality, disentanglement has been extensively studied, mainly in learning image representations . There are a few works investigating disentanglement in unsupervised skill discovery. Lee et al.  consider a special case of disentangled skills -- for a multi-arm robot, learning independent skills for each arm. However, they rely on manually factored action spaces which is an assumption that often limits the behavior of the agent. Kim et al.  encourage the disentanglement between different dimensions of the skill variable by regularizing it with \(\)-VAE objective , but Locatello et al.  point out that such regularization is impossible to achieve disentanglement. To learn disentangled skills, Song et al.  learns a decoder from skill variables to state trajectories and their generation factors, which is then used to train the skill policy through imitation learning. However, their training of the decoder requires pre-collected trajectories and corresponding generation factors, whereas our method is fully unsupervised with no expert data.

## 6 Conclusion

We present DUSDi, an unsupervised skill discovery method for learning disentangled skills by leveraging the factorization of the state space. DUSDi designs a skill space that exploits the factorization of the state space and learns a skill-conditioned policy where each sub-skill affects only one state factor. DUSDi enforces disentanglement through an intrinsic reward based on mutual information, and shows superior performance on a set of downstream tasks with naturally factored state spaces compared to baselines and state-of-the-art unsupervised RL methods.

One limitation of DUSDi is the assumption of access to a factored state space. While a factored state space is naturally available in many existing RL environments, and can be extracted from images as we have shown in our experiment (Sec. 4.5), we believe that future advances in disentangled representation learning will greatly broaden the applicability of DUSDi towards partially observable, pixel-based environments. Secondly, DUSDi primarily focuses on learning a structured skill space for more efficient downstream learning, and its exploration capability during skill learning is largely determined by the specific algorithm used to optimize for our mutual information objective. While we used DIAYN  in this work due to its simplicity, it would be interesting to examine extending the idea of learning disentangled skills to other skill discovery methods, e.g., Zhao et al. , Laskin et al. , including those that are not based on mutual information [35; 56].

Figure 5: Performance of DUSDi with image observations on two multi-particle downstream tasks over three random seeds. With the help of disentangled representation learning, DUSDi effectively learns skills based only on image observations and leverages the skills to solve challenging downstream tasks where baseline methods fail.

Figure 6: Performance of DUSDi in two multi-particle downstream tasks when combined with Causal Policy Gradient (CPG, orange). The disentangled skills of DUSDi provide opportunities for leverage structure and speed up downstream task learning, greatly improving the sample efficiency when learning downstream tasks.

AcknowledgementsThis work took place at the Learning Agents Research Group (LARG) and the Robot Interactive Intelligence Lab (RobIn) at UT Austin. RobIn is supported in part by DARPA TIAMAT program (HR0011-24-9-0428). LARG research is supported in part by NSF (FAIN-2019844, NRT-2125858), ONR (N00014-18-2243), ARO (W911NF-23-2-0004), Lockheed Martin, and UT Austin's Good Systems grand challenge. Peter Stone serves as the Executive Director of Sony AI America and receives financial compensation for this work. The terms of this arrangement have been reviewed and approved by the University of Texas at Austin in accordance with its policy on objectivity in research.