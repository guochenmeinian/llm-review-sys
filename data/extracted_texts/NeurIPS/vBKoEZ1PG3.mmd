# Hawk:

Learning to Understand Open-World Video Anomalies

 Jiaqi Tang\({}^{1,2,3}\)  Hao Lu\({}^{1,2}\)  Ruizheng Wu\({}^{4}\)  Xiaogang Xu\({}^{5,6}\)  Ke Ma\({}^{7}\)  Cheng Fang\({}^{7}\)  Bin Guo\({}^{7}\)  Jiangbo Lu\({}^{3,4}\)  Qifeng Chen\({}^{2}\)  Ying-Cong Chen\({}^{1,2,3}\)

\({}^{1}\)The Hong Kong University of Science and Technology (Guangzhou)

\({}^{2}\)The Hong Kong University of Science and Technology \({}^{3}\)HKUST(GZ) - SmartMore Joint Lab

\({}^{4}\)SmartMore Corporation \({}^{5}\)The Chinese University of Hong Kong \({}^{6}\)Zhejiang University

\({}^{7}\)Northwestern Polytechnical University

{jtang092, hlu585}@connect.hkust-gz.edu.cn

{ruizheng.wu, jiangbo}@smartmore.com xiaogangxu00@gmail.com

{2544552413, sura}@mail.nwpu.edu.cn guob@nwpu.edu.cn

cqf@ust.hk yingcongchen@hkust-gz.edu.cn

Equal contribution.Corresponding author.

###### Abstract

Video Anomaly Detection (VAD) systems can autonomously monitor and identify disturbances, reducing the need for manual labor and associated costs. However, current VAD systems are often limited by their superficial semantic understanding of scenes and minimal user interaction. Additionally, the prevalent data scarcity in existing datasets restricts their applicability in open-world scenarios. In this paper, we introduce **Hawk**, a novel framework that leverages interactive large Visual Language Models (VLM) to interpret video anomalies precisely. Recognizing the difference in motion information between abnormal and normal videos, **Hawk** explicitly integrates motion modality to enhance anomaly identification. To reinforce motion attention, we construct an auxiliary consistency loss within the motion and video space, guiding the video branch to focus on the motion modality. Moreover, to improve the interpretation of motion-to-language, we establish a clear supervisory relationship between motion and its linguistic representation. Furthermore, we have annotated over 8,000 anomaly videos with language descriptions, enabling effective training across diverse open-world scenarios, and also created 8,000 question-answering pairs for users' open-world questions. The final results demonstrate that **Hawk** achieves SOTA performance, surpassing existing baselines in both video description generation and question-answering. Our codes/dataset/demo will be released at https://github.com/jqtangust/hawk.

## 1 Introduction

_"Have eyes like a_**Hawk**_!" - Longman Dictionary_

In recent years, the deployment of Video Anomaly Detection (VAD) systems has seen a significant uptick across a diverse array of domains, including but not limited to, autonomous driving , surveillance , and crime scene analysis . The inherent capability of these systems to autonomously monitor and identify disturbances within a scene has markedly diminished the reliance on manual labor, thereby streamlining operational efficiency and reducing associated costs.

Despite the extensive focus on anomaly detection in most existing VAD systems  (as shown in Fig. 1 (A)), there is often a lack of deeper semantic understanding of the scenes and insufficient interaction with users. While Pu et al.  and Wu et al.  incorporated semantic information for video anomaly detection, their frameworks are limited as multiple-class classifiers (as displayed in Fig. 1 (B)). Consequently, the functionality of these systems is confined to the detection of anomalous frames, necessitating further manual analysis by users to analyze the detected anomalies comprehensively. Although Lv et al.  has pioneered the development of a large language model for the video anomaly explanation, their approach primarily relies on _pseudo labels_ for training. The lack of robust training data severely constrains its practical applicability. Besides, such a method focuses more on acquiring long-range context information rather than anomaly-related features on anomaly understanding (as exhibited in Fig. 1 (C)).

To solve the above challenges, we propose an interactive large visual-language model , **Hawk**, for precisely understanding video anomalies (as illustrated in Fig. 1 (D)). Considering that the motion in normal and abnormal videos is significantly different , we explicitly integrate motion modality by a dual-branch framework in **Hawk** to enhance the understanding of anomalies (Section 4.1). Besides, to reinforce motion attention, we construct an auxiliary consistency loss based on the mutual information between the original video (appearance feature) and its motion in tight space (Section 4.2), to implicitly guide the video branch to focus on motion-related features. However, the interpretation of motion to the corresponding language remains unclear. Therefore, we extract the motion-related language (verbs and their entities) from the original description to directly supervise the visual and linguistic representations of motion, for accurately enhancing the interpretation of video anomaly in **Hawk** (Section 4.3).

Furthermore, we also collect seven video anomaly datasets from various scenarios and generate language descriptions for each video. Besides, to address the open-ended questions raised by users, we utilize language descriptions of the videos to generate potential question-answer pairs for training. Since these datasets cover a range of scenarios (Section 3), including crime (UCF-Crime ), campus environments (ShanghaiTech  and CUHK Avenue ), pedestrian walkways (UCSD Ped1  and Ped2 ), traffic situations (DoTA ), and human behavior (UBnormal ), and finally, the model tends to generalize to open-world scenarios.

To train our framework, we initially pre-train it on WebVid  to equip it with the capability to understand general videos. Then, we fine-tuned it on our proposed video anomaly dataset to enhance its understanding of video anomalies across multiple scenarios. Compared to other baselines, our

Figure 1: Different framework in video anomaly detection. (A) shows traditional video anomaly detection methods, which use binary classifiers to detect anomalies. (B), following (A), introduces a multi-class classifier for integrating semantic information, allowing users to obtain different types of anomaly information. Neither (A) nor (B) can interact with users. (C) is a previous video understanding framework that can interactively provide richer semantic information for users, but cannot specifically locate video anomalies. Our framework (D) enhances the anomaly understanding capability and provides annotated labels with rich semantic information.

model achieves SOTA performance in both Text-Level and GPT-Guided Metrics. Our contributions are summarized as follows:

* We propose a novel video-language framework, **Hawk**, aiming at understanding video anomalies, which incorporates motion modality to enhance its capability.
* We generate rich language descriptions for seven different video anomaly datasets. Meanwhile, considering the diversity of open-world problems, we also generate question-answer pairs to tackle potential user inquiries.
* Compared to other large video models, our framework demonstrates SOTA performance for video anomaly understanding and question-answering across multiple scenarios, which will help open-world anomaly understanding in the future.

## 2 Related Work

Video Anomaly DetectionVideo Anomaly Detection (VAD) usually focuses on identifying unexpected events from the video and it has been widely applied in various fields, including autonomous driving , public surveillance [6; 23], and crime scene analysis  etc. Previous VAD methods [27; 33; 23; 44; 9; 13; 19; 34; 40; 48; 52] are designed in numerous pathways. Lu et al.  designed to learn video features only from normal videos, and hand-craft features or deep-learning-based features are leveraged. Sultani et al.  proposed multiple instance learning (MIL), which is the main paradigm for many weakly-supervised learning methods. Recently, Lv et al.  first proposed video-based large language models in the framework of VAD.

However, these methods lack sufficient semantic comprehension of scenes and offer inadequate user interaction. Several approaches [31; 42] have introduced multi-class classifiers to integrate semantic information with various types of anomaly information. Nevertheless, their output is still limited. In contrast, our framework not only integrates more comprehensive semantic information as a general video understanding system but also provides advanced interaction capabilities for users.

Large Model in Video UnderstandingRecent studies have demonstrated the reliable capabilities of large models in video understanding. Beyond powerful vision-language models [16; 51; 21; 24], recent research has increasingly explored more modalities [27; 18; 28; 46; 26]. Bain et al. introduced a large-scale dataset with general video content descriptions. Several LLM-based works[18; 28; 46; 26] aim to comprehend visual content. Additionally, Video-LLAMa  extends comprehension to both auditory and visual information, while Su et al. utilize multi-modal encoders to understand across six modalities. Recently, Lv et al. proposed video-based large language models for VAD tasks in a weakly supervised framework. In this paper, we introduce the _motion modality_ in our proposed vision-language model, which enhances the model's ability to locate anomalies by prioritizing relevant video content.

## 3 Data Engineering

Previous datasets are inadequate for addressing our problem. Most existing VAD datasets, such as UBnormal  and DoTA , only contain **simple video category labels** and lack detailed language descriptions. This results in video understanding models lacking accurate and comprehensive supervision, creating a significant obstacle to identifying anomalies in videos. Recently, Lv et al. attempted to create **pseudo language descriptions** for anomaly videos. However, these descriptions are naive combinations of labels and fixed text, relying on a rigid format that offers only limited information. Other datasets, like WebVid, include only **general descriptions** of video content, which may not direct the model's focus on anomalies.

Our PrincipleTo tackle the above problems, we annotate detailed language descriptions specifically for anomaly scenes in seven different existing <Video> datasets. These seven datasets include a variety of anomalous scenarios such as crime (UCF-Cirme ), campus (ShanghaiTech  and CUHK Avenue ), pedestrian walkways (UCSD Ped1  and Ped2 ), traffic (DoTA ), and human behavior (UBnormal ). With the support of these visual scenarios, we can perform comprehensive fine-tuning for various abnormal scenarios, being closer to open-world scenarios.

Moreover, to better account for real-world user situations, we believe that language descriptions should not only include **descriptions of the video anomalies** themselves, but also address **open questions asked by users**. Therefore, we construct open-ended question-answer pairs for each scenario to further enhance model's practical ability to answer users' varying questions. The procedure for answering users' questions is shown in Fig. 2. The data format of can be described by the Eq. (1),

\[| QA: \}.}\] (1)

**Anomaly Video Description Generation** To construct natural language descriptions <Description> for anomalous video datasets, we refer to previous research such as LLaVa  and VideoChat , and employ GPT-4  as an assistant. We first split the video into dense clips to ensure key information is captured. Following VideoChat , we use perception tools (InternVideo , Tag2Text , or GRiT ) to automatically generate captions for each key clip, obtaining a dense representation of the videos (except for the UCF-Crime dataset, which already has a dense representation built in ). Next, we use GPT-4  to generate anomaly-related descriptions based on the captions for each video. Unlike other general video understanding datasets , we provide prompts for GPT-4 to generate specific descriptions closely related to video anomalies. Finally, due to varying quality of dense captions, some videos may have incorrect annotations. Thus, we manually recheck the final generated video anomaly descriptions to ensure label accuracy.

Human-Centric Question-Answering Pairs GenerationSo far, we have obtained nearly accurate descriptions of anomaly videos. However, our framework may still face challenges with more open-ended questions from users. Therefore, anomaly-related question-answering is a significant practical requirement. Given the diversity of open-world scenes, users may ask questions involving various pronouns. Thus, we mainly consider these two principles: **1** Anomaly-related**, our questions should be strongly related to the anomaly in the video. **2** **5W2H**, we introduce seven different question pronouns (What, Who, Where, When, How, How much, and Why) to simulate various question formats that users may employ. This enables us to address a wide range of open questions related to

Figure 2: Generation pipeline of our dataset. In the first line, we first segment videos into clips and **generate dense captions** for each segment, including a comprehensive description of the video content. Then, we use GPT-4 to guide the **generation of corresponding anomalous video descriptions** based on these descriptions, which are then **manually checked to reduce mistakes**. In the second line, to generate user-centered QA pairs, we first use GPT-4 to **generate open-ended questions** based on the proposed two principles. Then, the questions and video descriptions are jointly input into GPT-4 to **provide possible answers**.

video anomalies. We input these two principles into GPT-4  to generate open questions for anomaly videos. We then manually review and select the 100 most suitable questions, which are randomly assigned to each video. Finally, GPT-4  will generate <Answers> to these <Questions>.

Our data is more practical compared to previous ones: it not only understands multiple anomalies in videos but also supports question-answering in open scenarios (More details in Appendix D).

## 4 Methodology

To construct a practical framework for understanding video anomalies, our goal is to accurately interpret these anomalies into natural language. However, most previous studies [18; 49; 29; 20; 27] focus on enhancing general video understanding capabilities while neglecting video anomalies. This oversight results in equal attention being given to all parts of the video, such as the background and human appearances, often at the expense of key anomaly features, as shown in Fig. 1 (C). Consequently, these approaches are not effective in accurately focusing on anomaly-related features.

Overview of SolutionThe core of our solution is guiding visual instruction to focus on anomalies. Previous studies in video anomaly detection [44; 52] have demonstrated that _motion-related feature_ help identify multiple anomalies. Therefore, in Section 4.1, we first explicitly integrate a motion modality into our proposed framework to target anomaly-related features. Subsequently, in Section 4.2, we maintain mutual information consistency between the appearance and motion modalities within a tight feature space, implicitly guiding the appearance branch to reinforce motion attention. Finally, in Section 4.3, to improve the interpretation of motion-to-language, we extract motion-related language descriptions to directly match the motion and its corresponding motion-related language.

### Explicit Motion Modality Integration

To enhance the capability of interpreting anomalies, we build a framework, **Hawk**, to explicitly integrate motion modality. **Hawk** has a dual-branch architecture, with \(f_{v}\) as the original video understanding network and \(f_{m}\) for motion understanding. Inspired by Video-LLaMA , \(f_{v}\) and \(f_{m}\) share the same architecture but separate parameters in Fig. 3. Eq. (2) denotes our framework as,

\[=([P_{v}(})),P_{m}(f_{m}( }))] f_{t}()\,,\] (2)

where \(}^{T C H W}\) represents the <Video> input for extracting appearance feature, and \(T\) denotes the temporal dimension. \(}=M(})\), with \(M()\) being the motion extractor.

\(f_{v}()\) and \(f_{m}()\) are the frozen pre-trained video encoders from BLIP-2 , which consist of one EVA-CLIP  and one pre-trained Video Q-Former to output embeddings. Then, the output embeddings from \(f_{v}()\) and \(f_{m}()\) are passed through learnable projection networks for video and motion, \(P_{v}()\) and \(P_{m}()\), respectively. These networks aim to project visual (video and motion) embedding into the language feature space for interpreting. \(f_{t}()\) is the frozen text token to embedding projection, that makes textual information can be inputted into LLaMA-2 . \(\) is for combining our input prompt, we define our prompt as: "_Here is the input video embedding:_ <Video_Embedding>_ _and motion embedding_ <Motion_Embedding>_ _in different frames, please help me to_ <Device_Video> _| <Question>_.". <Describe_Video>_ and <Question> are the question classes for video description generation and video question answering respectively (Details see Appendix D). By combining the visual token embedding with the textual embedding, \(f_{t}()\), LLaMA-2 , is employed to generate the final language response, \(\). This framework explicitly integrates the motion modality during visual instruction tuning, significantly targeting anomaly-related features.

### Implicitly Motion Attention Reinforcement

Although we integrate the motion modality to facilitate fine-tuning of **Hawk**, motion and video branches operate independently. Therefore, we cannot expect the original video branch to extract appearance features that focus on the region where the anomaly occurred (i.e., motion). To help **Hawk** focus more on these regions, we observed the containment relationship in mutual information between motion and the original video. We use this relationship to construct an auxiliary consistency loss function, implicitly reinforcing the motion attention (Fig. 4 ).

Extract MotionSpecifically, to obtain motion, we employ a motion describer \(M()\), which generates motion between two successive frames as shown in Eq. (3),

\[_{}^{()}=M^{()}(_{}^{()},_{}^{(-)}),\] (3)

where \(M^{()}()\) is the motion describer at the time step \(\), we currently use Gunnar Farneback's algorithm , and \(_{}^{()},_{}^{(- )}^{1 C H W}\) denote the video frames at time steps \(\) and \(-\). \(_{}^{()}^{2 H W}\) includes two channels motion vector in \(\) (horizontal) and \(\) (vertical) directions. We use the optical flow magnitude from these channels as a **Mask**, normalized to \(\) and multiplied with the original video appearance, to hide other non-motion regions, as Eq. (4),

\[_{}^{()}=(_{}^{()}())^{2}+(_{ }^{()}())^{2}})}_{}_{}^{()},\] (4)

where \(\) is the operator of pixel-wise multiplication. \(_{}^{()},_{}^{()} ^{1 C H W}\) donate the original video and our input motion information at time step \(\), respectively. We usually extract \(T\) frames as motion input \(_{}^{T C H W}\), same as \(_{}\).

Build \(_{MV}\) LossThen, we consider that \(_{}\) only contains key information for anomaly and it is contained in \(_{}\), and feature space from \(_{}\) is more sparse. Therefore, we compact features from \(_{}\) and \(_{}\) into a tight space. At this space, we aim to maintain the mutual information between \(_{}\) and \(_{}\) consistency, and in this way, the appearance feature can be focused on the motion region. Therefore, we construct an auxiliary loss to promote \(_{}\)'s motion attention, as in Eq. (5),

\[_{MV}=1-(_{}^{}, _{}^{})=1-_{}^{ }_{}^{}}{\|_{ }^{}\|\|\|},\] (5)

where \(_{}^{}=C_{v}(f_{v}(_{}))\) and \(_{}^{}=C_{m}(f_{m}(_{}))\) denote the tightly compressed representations of \(_{}\) and \(_{}\), respectively, by the compression functions \(C_{v}\) and \(C_{m}\). \(C_{v}\) and \(C_{m}\) share some initial shallow layer parameters with \(P_{v}\) and \(P_{m}\) (as Fig. 3). Then, following a subsequent tight projection to compresses both \(_{}\) and \(_{}\) into a more compacted space.

Finally, with this auxiliary loss, we can reinforce the motion attention in the appearance feature, and **Hawk**'s feature space will focus on more abnormal related features, which will promote the understanding of anomalies in the whole framework.

Figure 4: Visualization of **Hawk**’s loss. 1 is the original video-to-language loss. 2 is the cosine similarity loss for motion modality adaptation. 3 is the motion-to-language loss.

Figure 3: Overview of **Hawk**. During training (**Black** and \(\) path), we aim to optimize for video-language matching loss, along with Video-Motion Consistency and Motion-Language Matching. During inference (only \(\) path), we generate language descriptions using video, motion, and text.

### Interpreting Motion-to-Language

Although **Hawk** has already accommodated the motion modality in visual input, the corresponding motion from language is still unclear. This limitation hinders **Hawk**'s interpretation in motion modality. Hence, to augment this relationship, we aim to reinforce the correspondence between motion and their linguistic representation.

Extract Motion-related LanguagePrevious studies [5; 36; 43; 15] have proved that the representation of motion in the language is predominantly from **verbs** and their corresponding **entities**. Therefore, to extract linguistic representation, the first step is to do dependency parsing for the original sentences, as Eq. (6),

\[}=D(}),\] (6)

where \(D()\) is the dependency parsing and \(}\) is the ground truth. \(}\) represents the graph of the dependency structure, which symbolizes the syntactic relationships among the words in a sentence.

Based on this graph, we can extract predicates (verbs) \(\), and also entities closely related to these predicates, such as subjects \(\), objects \(\), indirect subjects \(}\), and indirect objects \(}\). These elements are then combined to form short phrases representing motion, as in Eq. (7),

\[^{m}}=\{,,,}, }\}=M_{l}(}),\] (7)

where \(M_{l}()\) is the language motion extraction operator, and \(^{m}}\) is the motion-related language.

Build \(_{ML}\) LossAfter obtaining motion-related language, we can establish strong supervision between motion in both vision and linguistic representation (as Fig. 4 3), significantly enhancing the ability to interpret motion to language in **Hawk**. Consequently, we design a motion-language matching as an auxiliary loss, as Eq. (8),

\[_{ML}^{m}(},^ {m}})&=-_{i=1}^{N}^{m}}^{i}(}^ {i})\\ \ }&=(P_{m}(f_ {m}(})),f_{i}()),\] (8)

where \(_{ML}()\) is the cross-entropy loss, which contains \(N\) words.

Optimization GoalFinally, our total loss \(\) shows as, \(=}_{VL}+} _{MV}+}_{ML}\), where \(_{VL}\) is original video to language loss (as Fig. 4 1), and \(}\), \(}\) and \(}\) is the hyper-parameter.

## 5 Experiments

In this section, we will provide a comprehensive introduction to the experiments, including the processes of training and testing, the establishment of baselines, the methods of evaluations, and the detailed examination of ablation experiments pertaining to **Hawk**.

Training & TestingTo enhance our framework's anomaly understanding capabilities, we've structured our training and testing process into three stages, as Fig. 5. **Stage 1** involves pre-training on the WebVid dataset  to acquire a general understanding of video content. In **Stage 2**, we finetune the model's focus towards video anomaly understanding by employing a specially curated dataset described in Section 1, consisting of over \(8,000\) videos. We use 90% of these videos for training and allocate the remaining 10% for testing purposes. We jointly train on two tasks: video <Description> generation and video <Question>-><Answering>. In **Stage 3**, we evaluate these two tasks independently in the testing set to ensure our model's effectiveness.

BaselinesTo evaluate the anomaly understanding performance of our proposed framework, we conduct comparisons with SOTA video understanding baselines. We select five baselines: Video-ChatGPT , VideoChat , Video-LLaMA , LLaMA-Adapter , and Video-LLaVA . Our comparison aims to determine whether these baselines can fully understand and interpret video

Figure 5: Training & Testing.

anomalies. To ensure the fairness of our experiments, we employed the baselines with the same size (7B parameters) as the backbone.

Evaluation MetricsTo accurately evaluate our model's performance in understanding video anomalies, we firstly adopt four **Text-Level** metrics, from BLEU (Bilingual Evaluation Understudy) -1 to BLEU-4 to measure word overlap between the model-generated text and the ground truth. This approach enables us to objectively assess the similarity and also take into account various levels of granularity at the text-level, thus providing a clear indicator of how well the model understands and describes anomalies.

Besides, we expand our evaluation framework by incorporating insights from recent research in LLaVa  or Video-ChatGPT , utilizing **GPT-Guided** methods to assess the quality of the generated text. GPT  serves as a critical evaluator, generating scores for three key aspects of the language produced, with each aspect scored on a scale from \(0\) to \(1\). These three aspects are as,

* **Reasonability:** evaluates the logical reasoning and coherence of the generated language.
* **Detail:** assesses the level of detail and specificity of the generated language.
* **Consistency:** evaluates the coherence and consistency of the generated language.

By leveraging GPT  as an evaluative tool, we aim to provide a nuanced understanding of the text's quality, focusing on aspects that traditional metrics may overlook.

Quantitative EvaluationTable 1 (A) and (B) demonstrate the effectiveness of our model to describe abnormal phenomena. Our proposed model significantly outperforms the previous baselines, achieving SOTA performance in every metric for both Text-level and GPT-guided metrics, thus it can generate text that more closely aligns with actual scenarios.

Qualitative EvaluationTable 2 (A) and (B) demonstrate that our proposed framework achieves optimal qualitative performance in video description generation and question-answering, respectively. Compared with other baselines, **Hawk** can accurately understand and focus on video anomalies. For example, in Table 2 (A) - Video-LLaMa , it pays more attention to the clothing information from the people (_wearing blue and red jacket_), while ignoring the motion-related anomaly (_slipping_). In Table 2 (B) - Video-ChatGPT, it may produce hallucinations (_two people... who were hit by the car_), which differ from the original video anomaly (_car suddenly braking_). In contrast, **Hawk** generates descriptions that are close to the real semantics (_driver losing control_).

    &  &  &  \\   & & BLEU-1 & BLEU-2 & BLEU-3 & BLEU-4 & Reasonability & Detail & Consistency \\  Video-ChatGPT  & LLaMA 7B & 0.107 & 0.046 & 0.017 & 0.008 & 0.084 & 0.108 & 0.055 \\ VideoChat  & Vicuna 7B & 0.053 & 0.023 & 0.008 & 0.003 & 0.107 & 0.205 & 0.054 \\ Video-LLaMA  & LLaMA-2 7B & 0.062 & 0.025 & 0.009 & 0.004 & 0.120 & 0.217 & 0.066 \\ LLaMA-Adapter  & LLaMA-2 7B & 0.132 & 0.052 & 0.018 & 0.008 & 0.060 & 0.091 & 0.038 \\ Video-LLaVA  & Vicuna 7B & 0.071 & 0.030 & 0.012 & 0.005 & 0.077 & 0.115 & 0.038 \\  Ours & LLaMA-2 7B & 0.270 & 0.139 & 0.074 & 0.043 & 0.283 & 0.320 & 0.218 \\   

Table 1: Quantitative performance of (A) anomaly video description generation and (B) video question-answering. Red indicates the best performance, while blue denotes the second best.

Ablation StudyWe conducted ablation experiments on three key structures proposed in this paper and analyzed their impact on the overall performance in Table 3 (A) and (B).

* **Effectiveness of Motion Information:** We ablate all the motion components, including \(f_{m}\), \(P_{m}\) and the motion input \(}\) for proving the effectiveness of introducing motion modality.

 p{142.3pt}}  
**Video-Chat GPT :** & **NoT FOCUS ON ANOMAT** & **The most interesting part of the video is a person walking on the snow-covered sidewalk, which is not a common sight on a snowy street. \\ 
**VideoChat :** & The video captures a pedestrian crossing in a snowy town at night. The crossing features a white and red crossing sign, and there are some street lights on the road. **NoT FOCUS ON ANOMAL** & **A few people are walking on the sidewalk, while others are crossing the street. \\ 
**Video-LLAMA :** & First, the video shows a city street with cars parked on both sides. There are also people walking on the sidewalk. Next, the view changes to **FOCUS ON PEOPLE’S CLUCHING** & **a man and a woman standing on a crosswalk, wearing blue and red jackets respectively. \\ 
**LLAMA-Adapter :** & There is an anomaly in the video: a person appears to be walking on the street, but they are not visible in the blurred image. This person seems to be walking in the midst of the snowstorm, **HALUCINATION** & **The video shows a man walking across a street while a woman is lying on the ground. **UNCLEAR** & **The woman lying on the ground is not moving, and it is unclear what happened to her. \\ 
**Ours:** & In this video, the man and woman crossing the street at a crosswalk might be impacted by the anomaly. The woman could slip on the snowy sidewalk and be injured, which would be the anomaly in the video as they should be midmid of their surroundings. \\ 
**Ground Truth:** & In this anomaly situation, one of the people walking across the snowy crosswalk suddenly falls down probably due to a medical emergency, possibly suffering an injury. This could also lead to a chain reaction, with other pedestrians trying to avoid the fallen person and potentially slipping themselves. \\   

Table 2: Qualitative performance on (A) anomaly video description generation, and (B) question-answering. Red texts indicate key semantic inconsistencies, whereas Green texts signify that the generated results are closely aligned with the Ground Truth. **YELLOW** indicates the text problem.

[MISSING_PAGE_FAIL:10]