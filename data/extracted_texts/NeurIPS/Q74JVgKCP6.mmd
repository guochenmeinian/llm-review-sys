# Near-Optimality of Contrastive Divergence Algorithms

Pierre Glaser Kevin Han Huang Arthur Gretton

Gatsby Computational Neuroscience Unit, University College London

pierreglaser@gmail.com, han.huang.20@ucl.ac.uk, arthur.gretton@gmail.com

###### Abstract

We perform a non-asymptotic analysis of the contrastive divergence (CD) algorithm, a training method for unnormalized models. While prior work has established that (for exponential family distributions) the CD iterates asymptotically converge at an \(O(n^{-1/3})\) rate to the true parameter of the data distribution, we show, under some regularity assumptions, that CD can achieve the parametric rate \(O(n^{-1/2})\). Our analysis provides results for various data batching schemes, including the fully online and minibatch ones. We additionally show that CD can be near-optimal, in the sense that its asymptotic variance is close to the Cramer-Rao lower bound.

## 1 Introduction

Describing data using probability distributions is a central task in multiple scientific and industrial disciplines [1; 2; 3]. Since the true distribution of the data is generally unknown, such a task requires finding an estimator of the true distribution among a model class that best describes the available data. An estimator can be characterized at multiple levels of granularity: at the highest level lies _consistency_, a property which states that as the number of available data points increases, a given estimator will converge to the one best describing the data distribution. At a lower level, a consistent estimator can be further characterized by its convergence rate, a quantity upper-bounding its distance to the true distribution as a function of the number of samples. A convergence rate can be either _asymptotic_, e.g. hold only in the limit of an infinite sample size, or _non-asymptotic_, in which case the rate also holds for finite sample sizes. In their simplest form, convergence rates are provided in big-\(O\) notation, discarding finer grained information such as asymptotically dominated quantities as well as multiplicative constants. These constants play a role in the so-called _asymptotic variance_ of the estimator, which is a precise descriptor of an estimator's statistical efficiency. Convergence rates and asymptotic variances have been the subject of extensive research in the statistical literature; in particular, well-known lower bounds exists regarding both the best possible (asymptotic) convergence rate of an estimator and its best possible asymptotic variance. These results set a clear frame of reference to interpret individual convergence rates, and are routinely present in the analysis of modern statistical algorithms such as noise-contrastive estimation [5; 6] or score matching [7; 8; 9].

In this work, we focus on cases where (1) the true data distribution admits a density with respect to some known base measure, and (2) the model class is parametrized by a finite-dimensional parameter. In this setting, provided that the true distribution belongs to the model class, a celebrated result in statistical estimation states that the model maximizing the average log-likelihood both achieves the best possible asymptotic convergence rate (called the _parametric rate_) and the best possible asymptotic variance, called the Cramer-Rao bound (see, e.g. ). While this result shows that Maximum Likelihood Estimators (MLE) are asymptotically optimal, fitting them is complicated by computational hurdles when using models with intractable normalizing constants. Such _unnormalized models_ are common in the Machine Learning literature due to their high flexibility [11; 12]; their weakness however lies in the fact that expectations under these models have no unbiased approximation. For this reason, popular approximation algorithms such as unbiased gradient-basedstochastic optimization of the empirical log-likelihood cannot _a priori_ be used, as the gradient of the normalizing constant is given by an expectation under the model distribution.

The Contrastive Divergence (CD) algorithm  is a popular approach that circumvents this issue by using a Markov Chain Monte Carlo (MCMC) algorithm to approximate the gradient of the log-likelihood. Unnormalized models trained with Contrastive Divergence have been shown to reach competitive performance in high-dimensional tasks such as image [14; 15; 16], text , and protein modeling [18; 19], or neuroscience . A consistency analysis of the Contrastive Divergence algorithm is delicate, however: indeed, the _optimization error_ e.g. the difference between the estimate returned by CD and the MLE, is likely to be non-negligible as compared with _statistical error_ - the distance between the MLE and the true distribution - and thus cannot be discarded, as often done when analyzing estimators that minimize tractable objectives [8; 5]. Recent work  elegantly established asymptotic \(O(n^{-1/3})\)-consistency of the CD estimator for unnormalized exponential families when using only a _finite_ number of MCMC steps. Key to their argument is the fact that the bias of the CD gradient estimate decreases as iterates approach the data distribution. However, as noted by the authors, their work left open the question of whether and under what conditions CD might achieve \(O(n^{-1/2})\)-consistency.

ContributionsIn this work, we answer this question by providing a non-asymptotic analysis of the CD algorithm for unnormalized exponential families. While existing convergence bounds  were derived for the "full batch" setting, where the CD gradient is estimated using the full dataset at each iteration, our analysis covers both the online setting (where data points are processed one at a time without replacement), and the offline setting with multiple data reuse strategies (including full batch).

In the online case (Section 3), we show, under a restricted set of assumptions compared to Jiang et al. , that the CD iterates can converge to the true distribution at the parametric \(O(n^{-1/2})\) rate. Our analysis reveals that CD contains two sources of approximation: a bias term, and a variance term. These sources are almost independent of each other, in the sense that decreasing the bias by increasing the number of MCMC steps will not decrease the variance. The impact of these two sources of approximation transparently propagates in our resulting bounds: in particular, as the bias of the CD algorithm goes to 0, our bounds recover well-known results in online stochastic optimization . Finally, we study the asymptotic variance of an estimator obtained by averaging the CD iterates, a classic acceleration technique in stochastic optimization . We show that provided that the number of steps \(m\) is sufficiently large, the _asymptotic_ variance of this estimator matches (up to a factor 4) the Cramer-Rao bound.

Next, we study the offline setting (Section 4), where the CD gradient is estimated by reusing (potentially random) subsets of a finite dataset. We show that a similar result to the online setup holds, up to an additional correlation term that arises from data reuse, and present several approaches to control this term. We improve over the results of  by showing a non-asymptotic and near-parametric rate at \(O(( n)^{1/2}n^{-1/2})\) under their conditions, and also illustrate how different rates can be obtained under a variety of conditions. Our results also show an interesting tradeoff between the effect of initialization and the statistical error as a function of batch size.

In summary, we establish the near-optimality of a variety of Contrastive Divergence algorithms for unnormalized exponential families in the so called "long-run" regime, where the number of MCMC steps is high enough to ensure that the CD gradient bias is sufficiently offset by the convexity of the negative log-likelihood.

## 2 Contrastive Divergence in Unnormalized Exponential Families

Unnormalized Exponential Families Exponential families (EF) [24; 25] form a well-studied class of probability distributions, given by

\[p_{}(x) e^{^{}(x)- Z()}c(x), Z()_{}e^{^{}(x)}c(x).\] (1)

Here, \( x\) is the _data_ or _sample space_, which we set to be a subset of \(^{d}\) for some \(d^{*}\), although our results are readily extendable to more general measurable spaces. \(c\) is a measure on \(\) called the _base_ or _carrier_ measure. When \(^{d}\), \(c\) is often set to be the corresponding Lebesgue measure. \(^{p}\) is a finite-dimensional parameter called the _natural parameter_, and \(:^{d}^{p}\) is a function called the _sufficient statistics_, which, alongside with the base measure, fully describes an exponential family. Finally, \( Z()\), the _log-normalizing_ (or _cumulant_) function, is a quantity ensuring that \(p_{}\) integrates to \(1\) over \(\). Crucially, we will not assume that \( Z()\) admits a closed form expression for all \(\). The latter fact provides the practitioner with a great deal of flexibility in designing the model class: indeed, the only requirement that should be satisfied prior to performing statistical estimation is to have \(Z()<+\) for all \(\), something that can be readily verified and is often the case in practice. The drawback of unnormalized EFs is the fact that sampling (and thus approximating expectations under the model) cannot usually be performed in an unbiased manner. Instead, inference in unnormalized EFs is often performed using tools from the Bayesian Inference literature, such as MCMC . Unnormalized EFs belong to the larger class of _unnormalized models_, of the form \(e^{-E_{}(x)- Z()}c(x),\ Z()= e^{-E_{}(x)}c( x)\), for some parametrized function \(E_{}:^{d}\) referred to as the _energy_. Unnormalized models thus take the flexibility of unnormalized EFs one step further by allowing the (negative) unnormalized log-density to be an arbitrary function \(E_{}\) of \(x\) and \(\), instead of requiring a linear dependence on \(\) as in Equation 1. We focus in this work on unnormalized EFs due to the multiple computational benefits they provide, as explained in the next section, but we believe that extending our analysis to more general unnormalized models is an interesting avenue for future work.

**Statistical Estimation in Unnormalized Exponential Families using Contrastive Divergence** We now review the Contrastive Divergence algorithm, an algorithm used to fit unnormalized models, and our main object of study in this work. The general setting is the following: we assume access to \(n\) i.i.d. samples \((X_{1},,X_{n})\) drawn from some unknown distribution \(p^{}\), which we assume belongs to \(_{}\), e.g. \(p^{}=p_{^{}}\) for some \(^{}\). Given these samples, we aim to perform _statistical estimation_, e.g. find a parameter \(_{n}\) within \(\) that should approach \(^{}\) as \(n\) grows.

The starting point of the Contrastive Divergence algorithm is the unfortunate realization that Maximum Likelihood Estimation, which corresponds to minimizing the cross-entropy \(()-_{p_{n}}p_{}/c\) between the model \(p_{}\) and the empirical data distribution \(p_{n} 1/n_{i=1}^{n}_{X_{i}}\), cannot be performed using exact (possibly stochastic) gradient-based optimization, as the gradient \(_{}()\) of \(\) with respect to the parameter \(\) contains an expectation under the model distribution \(p_{}\). Indeed, the cross entropy and its gradient are given by

\[()&=-_{i=1}^{n}(X_{i})^{} + Z()\\ _{}()&=-_{i=1}^{n}(X_{i})+_{p_{}}.\] (2)

The second line follows from the well known identity \(_{} Z()_{p_{}}\); we refer to [25, Proposition 3.1] for a proof. The Contrastive Divergence algorithm circumvents this issue by running approximate stochastic gradient descent (SGD) on \(\), where the intractable expectation in \(_{} Z\) is estimated using an MCMC algorithm initialized at the empirical data distribution. In more details, given a number of epochs \(T\), a sequence of data batches \(B_{t,j}\) of size \(B\) (e.g. \(B_{t,j} 1,n^{B},1 t T,1 j N n/B\)), and a family of _Markov kernels_\(\{k_{},\}\) each with invariant distribution \(p_{}\), at the \(j^{th}\) minibatch of epoch \(t\), \(_{} Z(_{t,j-1})\) is approximated by \(_{i B_{t,j}}(_{i}^{m})\), where \(_{i}^{m}\) is produced by running the recursion \(_{i}^{k} k_{_{t}}(_{i}^{k-1},),\ _{i}^{0}=X_{i}\) up to \(k=m\). Throughout the paper, we will refer to the conditional distribution of \(_{i}^{m}\) given \(X_{i}\) as \(k_{}^{m}(X_{i},)\). The resulting gradient estimate arising from combining this approximation with the other (tractable) sum over the data samples present in \(_{}()\), which we refer to as the _CD gradient_ and denote as \(h_{t}\), is thus

\[h_{t,j}_{i B_{t,j}}(X_{i})-_{ i B_{t,j}}(_{i}^{m})=_{i B_{t,j}}((X_{i})- (_{i}^{m})).\] (3)

Key to the behavior and analysis of the CD algorithm is the strategy employed to generate minibatches \(B_{t,j}\). The case where \(T=1\), \(B=1\), and \(B_{1,j}=\{j\}\) will be referred to as _online_ CD, while the variant where \(T>1\), and each batch \(B_{t,j}\) draws \(B\) indices (with or without replacement) from \( 1,n\) will be referred to as _offline_ CD. In online CD, each data point is present in one and one batch only, while in offline CD, data points are reused across batches. From a statistical perspective, we will see that online CD can be analyzed in a remarkably simple way, while offline CD introduces additional correlations that require care to be controlled. Both settings come with their advantages and drawbacks, as we will see in the next section. The CD algorithms we study will employ decreasing step size schedules \((_{t})_{t 0}\) of the form \(_{t}=Ct^{-}\), where \(C>0\) is the initial leaning rate and \(\). We lay out online CD and offline CD in Algorithms 1 and 2. Note that our algorithms include a projection step on the parameter space \(\) to account for the case where \(\) is compact. In the case \(=^{p}\), this step can be omitted. Next we depart from the setting of  and start by analyzing online CD.

## 3 Non-asymptotic analysis of Online CD

### Preliminaries and Assumptions

Recall that the chi-squared divergence between two probability measures \(p\) and \(q\) is defined as: \(^{2}(p,q)(p}{q}(x)-1)^{2}q(x)\) if \(p q\), and \(+\) otherwise. Here, \(p q\) denotes that \(p\) is absolutely continuous with respect to \(q\) and \(p/q\) is the Radon-Nikodym derivative  of \(p\) with respect to \(q\). Let \(L^{2}(p_{})\) be the space of square-integrable functions with respect to \(p_{}\). For a function \(f L^{2}(p_{})\), we define

\[(f,)=_{p_{}}f)(y)\,k_{}(x, y))^{2}p_{}(x))^{1/2}}{((f-_{ p_{}}f)(x)^{2}p_{}(x))^{1/2}}\] (4)

which is a measure of how quick a Markov chain with kernel \(k_{}\) mixes, relative to the function \(f\). With these definitions in hand, we now state the assumptions required by our analysis of online CD. These assumptions form a strict subset of the assumptions considered in prior work , which required additional regularity and tail conditions on the Markov kernels \(k_{}\).

**Assumption A1**.: \(_{}\) is a subset of a _regular_ and _minimal_[25, Section 3.2] exponential family with natural parameter domain \(^{p}\), \(\) is a _convex and compact_ subset of \(\), and \(^{*}\) lies in the interior of \(\).

**Assumption A2**.: There exists a constant \(C_{}>0\) such that \(^{2}(p_{^{*}},p_{}) C_{}^{2}\|-^{*}\|^{2}\)

**Assumption A3**.: \(\{(f,),\,f\{_{i}\}_{i=1}^{p}\{_{i }_{j}\}_{i,j=1}^{p},\,\}<1\), where \(_{i}\) is the \(i\)-th component of the function \(\), and \(_{i}^{2}\) is the \(i\)-th component of the function \(x(x)^{2}\).

A well known property of EFs [25, Proposition 3.1] is that their negative cross-entropy (against any other measure) is \(C^{}\), convex, and strictly so if the exponential family is minimal (meaning that the set of sufficient statistic functions \(_{i}\) are not linearly dependent). Leaving aside the issue of intractable expectations, this convexity suggests that \(\) can be efficiently minimized using stochastic approximation algorithms [31; 22]. The compactness of \(\) provided by Assumption A1 thus ensures, by the extreme value theorem , the existence of finite positive constants \(\) and \(L\) defined as:

\[_{}_{}(_{}^{2}()), L_{}_{}(_{ }^{2}()),\] (5)

where \(_{}^{2}\) is the Hessian of \(\) with respect to \(\). \(\) (called the _strong convexity_ constant) and \(L\) (a bound controlling the smoothness of the problem) play a critical role in the analysis of convex optimization algorithms . While it is possible to obtain convergence rates in non-smooth or non-strongly-convex settings, our analysis follows the spirit of  by leveraging the strong convexity of the problem to compensate for the bias introduced by using CD gradients instead of unbiased stochastic gradients.

Assumption A2 allows link variations in distribution space to variations in parameter space, and will be instrumental to control the bias of the CD gradient. Note that since

[MISSING_PAGE_FAIL:5]

While we precisely investigate the impact of the residual variance term in the next section, we now unify \(_{}\) and \(_{t}\) by introducing

\[_{}(_{p_{}}\|-_{p_{ }}\|^{2})^{1/2}.\] (8)

\(\) is an upper bound on the noise induced _both_ by the CD gradient and by the online setup, and was used in prior work . Note that by the properties of \( Z\), \(^{2}\) also equals \(_{}(^{2}_{}())\), where \((A)\) is the trace of \(A^{p p}\), and thus finite by the extreme value theorem. The following theorem is obtained by invoking standard unrolling arguments in the convex optimization literature. In the next result, we use the function \(_{}(t)\), defined as \(_{}(t)=}{}\) if \( 0\), and \( t\) if \(=0\).

**Theorem 3.2**.: _Fix \(n 1\). Let \((_{t})_{0 t n}\) be the iterates produced by Algorithm 1, and define \(_{t}\|_{t}-^{}\|^{2}\). Moreover, assume that \(m>/)}{||}\), i.e. \(_{m}-^{m} C_{}>0\). Then under Assumptions A1, A2 and A3, for \(_{t}=Ct^{-}\) with \(C>0\), we have:_

\[_{n}2(4C^{2}_{1-2}(n) )(-_{m}C}{4}n^{1-})(_{0 }+_{m}^{2}}{L^{2}})+_{m}^{2} }{_{m}n^{}},&0<1\\ ^{2}C^{2})}{n^{ C}}(_{0}+ {_{m}^{2}}{L^{2}})+2_{m}^{2}C^{2}_{m}C/2-1}(n)}{n^{_{m}C/2}},&=1\;,\]

_where \(_{m}=^{2}(2+2^{2m})+^{m/2}\| Z\| _{3,}^{2}C_{}^{2}\) and \(=(L^{2}+^{m/2})^{1/2}\). Consequently, if \(_{n}=\) with an initial learning rate \(C>2_{m}^{-1}\), we have \(} 2_{m}C_{m}C}{_{m}C-2}}}+o(})\)._

Theorem 3.2 is proved in Appendix D.3. It shows that the iterates produced by online CD will converge to the true parameter \(^{}\) at the rate \(O(n^{-1/2})\) provided that the number of steps \(m\) is sufficiently large, improving over the asymptotic \(O(n^{-1/3})\) rate of , while imposing slightly weaker conditions on the number of steps \(m\) (see (21, Theorem 2.1)). This proves that online CD can be asymptotically competitive with other methods for training unnormalized models, such as Noise Contrastive Estimation , or Score Matching . However, the asymptotic variance of \(_{t}\) (e.g. the multiplicative factor in front of the \(O(n^{-1/2})\) term) is likely to be suboptimal, e.g. much larger than the Cramer-Rao bound, given by the trace of the inverse of the Fisher information matrix . Given the statistical optimality of MLE, and the fact that CD in an approximate MLE method, this motivates the further goal or obtaining a CD estimator with near-optimal statistical properties. In the next section, we achieve this goal by showing that averaging the iterates \(_{t}\) will produce a near statistically-optimal estimator, in a sense that we will make precise.

#### 3.2.2 Towards statistical optimality with averaging

Polyak-Ruppert averaging  is a simple yet surprisingly effective way to construct an asymptotically optimal estimator \(_{n}_{t=1}^{n}_{i}\) from a sequence of iterates \((_{t})_{0 t n}\) obtained by running a standard online SGD algorithm . As shown in , when the objective is the cross-entropy of a model, and assuming the unbiased stochastic gradients are available, averaging yields an estimator \(\) with the asymptotic variance \(((^{})^{-1})/n\), where \(()_{p_{}}\), \(\) is the Fisher information matrix of the data distribution \(p_{^{}}\). \((^{})^{-1}\) being the Cramer-Rao _lower bound_ on asymptotic variances of statistical estimators , this estimator \(_{n}\) is asymptotically optimal. The following theorem shows conditions under which averaging CD iterates can give rise to a near-optimal estimator.

**Theorem 3.3** (Contrastive Divergence with Polyak-Ruppert averaging).: _Let \((_{t})_{t 0}\) the sequence of iterates obtained by running the CD algorithm with a learning rate \(_{t}=Ct^{-}\) for \((,1)\). Define \(_{n}_{i=1}^{n}_{i}\). Then, under the same assumptions as Theorem 3.2, and assuming additionally that \(m m(n)>\), we have, for all \(n 1\),_

\[(\|_{n}-^{}\|^{2})^{1/ 2}\;\;2((^{})^{-1})}{n}}+o (n^{-1/2})\]

_Consequently, we have that \(_{n}n(\|_{n}-^{}\|^ {2}) 4((^{})^{-1})\)._

Theorem 3.3, alongside with a statement which includes the asymptotic order of the residual term, is proved in Appendix D.4. It shows that at the cost of an increase in _computational_ complexity of the entire algorithm from \(O(n)\) to \(O(n n)\), \(_{n}\) will be a near-optimal statistical estimator of \(^{}\)While this increase in complexity emerges from the bias of CD, the additional variance of CD results in an asymptotic variance inflated by a factor of \(4\) compared to the Cramer-Rao bound.

Theorem 3.3 concludes our analysis of online CD. Despite their asymptotic near-optimality, the bounds provided for online CD and its averaged version have weaknesses: the online CD iterates are not robust to choices of C. On the other hand, as shown in Appendix D.4, the bound of the averaged iterates contain higher-order terms that could be large in intermediate sample regimes. Next, we show that offline CD, which processes data points multiple times, can alleviate these issues.

## 4 Non-asymptotic analysis of offline CD

In practice, CD gradient approximation schemes are commonly used within an offline stochastic gradient descent (SGD) algorithm, where one is given the full size-\(n\) dataset upfront and each update uses some stochastic subset of the data. We study CD under offline SGD with replacement (SGDw), i.e. Algorithm 2 with batches \(B_{t,j}\) being i.i.d. uniform draws of size-\(B\) subsets of \([n]\), and include SGD without replacement in Appendix B.2. To do so, we follow the setting of prior work on offline CD , which established its asymptotic \(O(n^{-})\) consistency. We show that by slightly strengthening a moment assumption used in , the offline CD iterates converge to the true parameter at a near-parametric \(O(( n)^{}n^{-})\) rate. Our proof proceeds by controlling a "tail probability" term specific to the offline setting which characterizes the strength of the correlations between the offline CD iterates and the training data. While, as we show, the assumptions of  provide a tail control sufficient to obtain a near-parametric rate, other strategies are possible to obtain convergence guarantees. In particular, we show that non-asymptotic convergence can be obtained by either (1) relaxing assumptions on the Markov kernel required by prior work, or (2) making a specific mixing assumption the Markov chain.

### Background: Asymptotic consistency of offline CD in subexponential settings

Prior work  has established _asymptotic_\(O(n^{-})\) consistency of the (averaged) offline CD iterates in the full-batch case. We summarize their results and assumptions below.

**Assumption A4**.: There exists \( 2\) s.t. for all \(m\), there is \(_{;m}<\) s.t.

\[_{x}\,_{}\,((K_{ }^{m}(x))-[(K_{}^{m}(x))]^{})^{1/} \;\;_{;m}\;.\]

**Assumption A5**.: There exists some \(C_{m}>0\) such that, for all \(_{1},_{2},_{x}\|[(K_{_{1}}^ {m}(x))]-[(K_{_{2}}^{m}(x))]\| C_{m}\|_{1}-_{2}\|\).

**Assumption A6**.: There exist some \(_{m},_{m}>0\) such that, for any \(z^{p}\) with \(\|z\|_{m}\), \([e^{z^{}((K_{}^{m}(X_{}))-[(K_{_{ }^{m}(X_{})}^{m}(X_{}))])}] e^{_{m}^{2}\|z\|^{2}/2}\).

**Theorem 4.1** (Theorem 2.1 of ).: _Assume assumptions A1,A2, A3, A4 (for \(=2\)), A5 and A6. Let \(_{t,1}\) be the \(t\)-th iterate of offline CD with full-batch gradient descent and constant stepsize \(_{t}=C\), i.e the iterates produced by Algorithm 2 using \(B_{t,1}= 1,n\). Then for any learning rate \(C\) and number of Markov kernel steps \(m\) satisfying \(-^{m} C_{}-(L+^{m} C_{})^{2}>0\), we have, for some \(A_{m}>0\),_

\[_{n}(_{T}\|_{ t=1}^{T}_{t,1}^{}-^{}\|>A_{m}n^{-} )=0\]

This result shows convergence of the _averaged_ full-batch CD iterates to the true parameter in the large \(n\) and \(T\) limit. As discussed, this result is asymptotic both in \(n\) and \(T\): the probability of the error exceeding \(A_{m}n^{-}\) goes to 0 as \(n\) and \(T\), but at an unknown rate. Moreover, the \(O(n^{-})\) does not match the optimal \(O(n^{-})\) rate.

### Sharpening offline CD bounds in subexponential settings

#### 4.2.1 Non-asymptotic \((n^{-1/2})\)-consistency

As a first result, we show that under the assumptions of  (except for a slightly stronger \(>2\) moment assumption in A4), \(_{T,N}^{}\) in fact achieves a near-parametric rate. The most general version of our result holds for any learning rate schedule of the form \(Ct^{-},\), and for offline SGDwith arbitrary batch sizes \(B\), with data drawn either with or without replacement across batches. For simplicity, we first present our result assuming full batch (\(B=n,N=1,_{t,j}^{}=_{t,1}^{}\) for \(t 1\)) SGD with constant step sizes \(_{t}=C\), which is the setting of . Analogue bounds holding for the other mentioned batching and step sizes schedules can be found in Appendix B.

**Theorem 4.2**.: _Assume the setup of Theorem 4.1, except that Assumption A4 holds for some \(>2\), and that \(_{m}\ =\ -^{m} C_{}\ >\ 4CL^{2}\). Let \(_{t,j}^{}\|_{t,j}^{}- ^{*}\|^{2}\). Then, we have:_

\[^{}} E_{1}^{T,1}^{ }}\ +\ C^{}(p,,m,)}{}+}_{m}C}{2}}}{_{m}C }+^{T,1}}{L^{2}C^{2}}\,\] (9)

_where \(E_{1}^{T,1}\), \(E_{2}^{T,1}\) are functions decreasing exponentially in \(T\), and \(C^{}(p,,m,)\) is a constant in \(n,T\). Consequently,_

\[_{T}^{}}_{m}C}{2}}}{_{m}C}C^{}(p,,m,,)}{}+}\.\]

The precise values of all the constants can be found in Theorem B.1 (for \(E_{1}^{T,1}\), \(E_{2}^{T,1}\)) and Lemma B.3 (for \(C^{}(p,,m,,)\)), including their expressions for \(N>1\) and \(\). We comment on the main differences between our result and the one of . First our bound holds for _any_ epoch \(T\) and number of samples \(n\). Second, fixing \(n\) but taking \(T\), the final bound matches the parametric \(O()\) up to a \(\) factor, a significant improvement over the \(O(n^{-})\) rate of . Finally, we control an \(L_{2}\) error, which is a stronger control than a high probability bound by Markov's inequality; we hypothesize this is the reason why a slightly stronger moment assumption is required for our setup, compared to the one used for the high probability bound in .

Inspecting Equation 9, we notice the presence of two _transient_ terms, and a _stationary term_, reminiscent of the structure of upper bound of Theorem 3.2. The transient terms (i.e. the ones containing \(E_{1}^{T,1}\) and \(E_{2}^{T,1}\)) vanish exponentially fast in the total number of CD updates \(T\). However, unlike in online CD where the number of updates and the number of samples are tied (e.g. \(T=n\)), these two values are now _decoupled_, and these terms can be made arbitrarily small by increasing the number of gradient steps \(T\) without having to collect more samples \(n\). The stationary term, which is the only one remaining in the limit of \(T\), decreases with \(n\) at a rate that is independent of hyperparameters like the step size \(C\) or the learning rate schedule \(\) (see Lemma B.3). In that sense, offline CD compares favorably to online CD, whose rate is sensitive to \(\) and \(C\), and averaged online CD, whose bound contains higher-order (in \(n\)) terms which can be large in the moderate \(n\) regime. On the other hand, the stationary term in offline CD is asymptotically suboptimal: its rate is larger (while only up to a log factor) than the best-case \(O()\) one achieved by online CD algorithms, and the leading constant does not match the optimal one.

#### 4.2.2 Proof of Theorem 4.2

The high-level proof of Theorem 4.2 follows a similar strategy as the online one: first, derive a recursion for the quantity \(_{t,1}^{}\|_{t,1}^{}- ^{*}\|^{2}\), then unroll it explicitly to obtain a final bound on \(_{T,1}^{}\). The main difference to online CD is the presence of an additional offline-specific correlation between the iterates and the data. We thus break down the proof into three steps: (1) deriving a controllable, uniform-in-time upper bound of the data-iterate correlations, (2) deriving and unrolling a recursion on \(_{t,1}^{}\) containing this new term, and (3) controlling that term to obtain a final bound on \(_{T,1}^{}\).

Step 1:characterizing the data-iterate correlations in offline CDIn offline CD, at each epoch \(t 1\), the iterate \(_{t-1,1}^{}\) and the data samples \(X_{i}\) are correlated: this is because these samples may have been used in previous epochs \(t^{}<t-1\) to obtain the \(_{t^{},1}^{}\), which themselves influenced \(_{t-1,1}\). With such correlations, we now have \((X_{i}|_{t-1,1}^{})(X_{i})\), preventing us from obtaining an unrollable recursion on \(_{t,1}^{}\) by first marginalizing \(X_{i}\) out to obtain an upper bound of \([_{t,1}^{}-^{*}^{2}|_{t -1,1}^{}]\) that only depends on \(_{t-1,1}^{}-^{*}\), and then marginalizing over \(_{t-1,1}^{}\) to obtain a recursion as in Lemma 3.1. As this problem would not have occurred had we used "fresh samples" (e.g. i.i.d copies of \(X_{i}\) not present in the training data) to perform our update, the core of the proof lies in controlling the following quantity:

\[(_{t,1}^{})\!\!\|\,_{i n }(K_{i;_{t,1}^{}}^{m}(X_{i}) _{t,1}^{},X_{i}- K_{i;_{t,1}^{}}^{m}(X_{1}^{})\, _{t,1}^{}\|\]

where \(X_{1}^{}\) is an i.i.d. copy of \(X_{1}\). \((_{t,1}^{})\) is the expected (over the data and iterates) error between a quantity that allows to obtain a recursion (the rightmost term) and the one actually used by offline CD (the leftmost term). To control it, we upper-bound it using a tail decomposition:

\[[(_{t,1}^{})^{2}]^{2}+(_{t} [(_{t,1}^{})^{}])^{2/}_{t} ((_{t,1}^{})>)^{}_{n,m,T;}^{}()^{2}\] (10)

We invoke an additional assumption to ensure that \(([(_{t,1}^{})^{}])^{2/}\) is finite; in the results of , this is automatically implied by assumptions A4 and A6. For simplicity we assume the same bounding constant \(_{;m}\).

**Assumption A7**.: There exists \( 2\) s.t. for all \(m\), \(_{;m}\) from A4 moreover verifies

\[_{}\,((K_{}^{m}(X_{1}))-[(K_{}^{m}(X_{1}))]^{})^{1/}\;\;_{;m}\;.\]

Note the similarity of this assumption with assumption A4: the only difference is that \(X_{1}\) is now a random training point instead of an deterministic (arbitrary) one. Ensuring assumption A7 in addition to assumption A4 thus requires controlling a \(\)-th order moment, instead of all moments as implied by assumption A6.

Step 2: Deriving and unrolling the recursion on \(_{t,1}^{}\)The right-hand side of Equation (10) does not depend on \(t\), allowing for the derivation of an "unrollable" recursion on \(_{t,1}^{}\) and its subsequent unrolling, which is performed in the following theorem. For simplicity, we again assume \(=0\) and \(N=1\) and defer the general case to Theorem B.1 in appendix.

**Theorem 4.3** (Convergence up to a tail control).: _Assume A1, A2, A3, A4 and A7. Let \(_{t}=C\) for some \(C>0\), and assume that \(_{m}\;=\;-^{m} C_{}\;>\;4CL^{2}\). Then for any \(>0\),_

\[^{}} E_{1}^{T,1}^{ }}\;+\;C(_{n,m,T;}^{}() +}{})(_{m}}{2}C}}{_{m}C}+^{T,1}}{L^{2}C^{2}}\,)\]

_where \(_{n,m,T;}^{}()\) is defined in Equation (10)._

Note that in the general, non-full batch \(B n\) case, \(}{}\) is replaced by \(}{}\) (see Theorem B.1). Under our bounds, obtaining consistency thus requires setting \(B B(n)+\).

Step 3: Controlling the tail probability termTheorem 4.3 is just one step away from the final bound of Theorem 4.2: it remains to control the tail term \(_{n,m,T;}^{}()\). Under the assumptions of , minimizing \(_{n,m,T;}^{}()\) over \(\) yields the following result:

**Lemma 4.4**.: _Assume the setup of Theorem 4.2. Let \(n\) be sufficiently large s.t. \(<^{2}_{m}^{2}}{p+-2}\). Denote \(r_{}\) as the radius of the smallest sphere in \(^{p}\) that contains \(\), which is finite under A1. Then \(_{>0}\!\!_{;n,m,T}^{}()\)_

\[(}{}+ _{;m}2^{}(r_{})^{} 1+(-2)^{1/2}}{_{m}p^{1/2}((-2)p+2)^{1/2}} ^{})}{}.\]

To obtain this result, we control the moment term \((_{t}[(_{t,1}^{})^{}])\) using A7, and we control the tail probability term \(_{t}((_{t,1}^{})^{}])\) as in [21, Lemma 3.1] using an union bound, a covering argument and A6. Theorem 4.2 then follows by plugging Lemma 4.4 into Theorem 4.3.

### Consistency of offline CD: beyond subexponential tails.

As discussed above, the general unrolling result of Theorem 4.3 holds without the subexponentiality assumption A6; this assumption was only used in Lemma 4.4 to control \(_{n,m,T;}^{}()\). We now discuss two alternative ways to control this quantity without requiring subexponential tails. The first generalizes the idea of Jiang et al. , while the second exploits mixing of the Markov chain \(K_{}^{m}(x)\) as \(m\). As before we only state partial results (full batch, \(=0\)) and defer the full explicit bounds to Appendix B.3.

Control via Markov InequalityOur first alternative uses Markov Inequality to yield the following.

**Theorem 4.5**.: _Assume the setup of Theorem 4.3 and additionally that A5 holds. Then_

\[_{>0}_{;n,m,T}^{}() (p,,m,)\,n^{-+( -2)p)}}\,\] \[_{T}^{}} ^{}(p,,m,)n^{- {2(^{2}+(-2)p)}}+},\]

_where \(\) and \(^{}\) are functions whose explicit expressions are given in Lemma B.4 in the appendix._

In the case \(p=1\) and \(=3\), the sub-optimal error from Theorem 4.5 reads \(O(n^{-3/20})\). Theorems 4.2 and 4.5 reveal that, depending on the tail condition imposed on the noise introduced by the Markov kernel, the convergence rate of offline CD varies: A subexponential tail, as assumed in prior work, in fact leads to near-parametric rate. Meanwhile, consistency can be obtained without assuming subexponentiality, albeit at a sub-optimal rate.

Control via Markov chain mixing.Alternatively, notice that \([(_{t,1}^{})^{2}]\) involves an average of \(K_{_{t,1}^{}}^{m}(X_{i})^{2} X_{i},_{t,1}^{}- K_{_{t,1}^{}}^{m}(X_{1}^{})_{ t,1}^{}\). When \(m\), the effect of initialization vanishes, and one may expect the difference to converge to zero. We defer to Lemma B.5 in the appendix to show that, under a \(\)-discrepancy mixing condition () with a mixing coefficient \([0,1)\),

\[_{>0}_{;n,m,T}^{}()\ =O(_{;m}^{})_{T}^{}}\ =O_{;m}^{}+}{}\.\]

As \(m\), this recovers the parametric rate \(O(n^{-1/2})\). This alternative convergence guarantee comes at the cost of requiring \(m\), the number of Markov chain steps, to grow with the sample size \(n\).

**Remark** (Examples).: In our main results (Theorems 3.2, 3.3 and 4.3) and the tail condition for offline SGD (Theorem 4.2), we employed a weaker set of assumptions than those in  (except for the mild \(>2\) moment assumption in (A4)). Consequently, our results apply to all three examples studied in : A bivariate Gaussian model with unknown mean and random-scan Gibbs sampler, a fully visible Boltzmann machine with random-scan Gibbs sampler, and an exponential-family random graph model with a Metropolis-Hastings sampler.

## 5 Related Work

Central to this paper is the prior work of Jiang et al. , which provided a rigorous theoretical foundation to analyze the convergence of full-batch CD, and which we refine. The study of optimization with biased gradient descent has attracted a lot of attention in recent years [36; 37; 38; 39]. These works, while closely connected to ours, analyze algorithms with different implementation choices than the CD algorithm: i.i.d. noise setup , or setup where a persistent Markov chain is maintained through the iterations [36; 37; 38; 39]. The latter is akin to a variant of the CD algorithm, called the persistent CD . In contrast, our analysis focus on the CD algorithm that restarts a batch of Markov chains from the data distribution at every iteration. Finally, there is a rich body of work on convergence guarantees for offline multi-pass SGD [41; 42; 43; 44; 45; 46]. A notable difference of our analysis is that we are primarily concerned with statistical errors associated with convergence to the true parameter \(^{*}\) in number of samples \(n\), and not the commonly studied convergence rate in number of epochs \(T\). Consequently, most of our work for the offline setup goes into handling the correlations that accumulate by reusing data across epochs.

## 6 Discussion

In this work, we provide a non-asymptotic analysis of the Contrastive Divergence algorithms, showing, in the online setting, their potential to converge at the parametric rate and to have near-optimal asymptotic variance, and proving a near-parametric rates in the offline setting, significantly extending prior results. Our results apply to unnormalized exponential families: despite their flexibility, these models only cover log-densities with linear relationships on the model parameters. We believe that extending our results to more general forms of unnormalized models is an important direction for future work.