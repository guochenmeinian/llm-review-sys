# Make-An-Agent: A Generalizable Policy Network Generator with Behavior-Prompted Diffusion

Yongyuan Liang\({}^{12}\)  Tingqiang Xu\({}^{3}\)  Kaizhe Hu\({}^{3}\)  Guangqi Jiang\({}^{4}\)

**Furong Huang\({}^{2}\)  Huazhe Xu\({}^{13}\)**

\({}^{1}\) Shanghai Qi Zhi Institute \({}^{2}\) University of Maryland, College Park

\({}^{3}\) Tsinghua University \({}^{4}\) University of California, San Diego

###### Abstract

Can we generate a control policy for an agent using just one demonstration of desired behaviors as a prompt, as effortlessly as creating an image from a textual description? In this paper, we present **Make-An-Agent**, a novel policy parameter generator that leverages the power of conditional diffusion models for behavior-to-policy generation. Guided by behavior embeddings that encode trajectory information, our policy generator synthesizes latent parameter representations, which can then be decoded into policy networks. Trained on policy network checkpoints and their corresponding trajectories, our generation model demonstrates remarkable versatility and scalability on multiple tasks and has a strong generalization ability on unseen tasks to output well-performed policies with only few-shot demonstrations as inputs. We showcase its efficacy and efficiency on various domains and tasks, including varying objectives, behaviors, and even across different robot manipulators. Beyond simulation, we directly deploy policies generated by **Make-An-Agent** onto real-world robots on locomotion tasks.

## 1 Introduction

Policy learning traditionally involves using sampled trajectories from a replay buffer or behavior demonstrations to learn policies or trajectory models mapping from state \(s\) to action \(a\), modeling a narrow behavior distribution. In this paper, we consider a shift in paradigm: moving beyond training a policy, can we reversely predict optimal policy network parameters using suboptimal trajectories from offline data? This approach would obviate the need to explicitly model behavior distributions, allowing us to learn the underlying parameter distributions in the parameter space, thus revealing the implicit relationship between agent behaviors for specific tasks and policy parameters.

Using low-dimensional demonstrations (such as agent behavior) to guide the generation of high-dimensional outputs (policy parameters) is a challenging problem. When diffusion models [12; 20] have demonstrated highly competitive performance on various tasks including text-to-image synthesis, we are inspired to approach policy network generation as a conditional denoising diffusion process. By progressively refining noise into structured parameters, the diffusion-based generator can discover various policies that are not only superior in performance but also more robust and efficient than the demonstration in the policy parameter space.

While prior works on hypernetworks [10; 1; 18] explore the concept of training a hypernetwork to generate weights for another neural network, they primarily use hypernetworks as an initialization network of meta-learning  and then adapt to specific task settings. Our approach diverges from this paradigm by leveraging agent behaviors as direct prompts or to generate optimal policies within theparameter space, without the need for any downstream policy fine-tuning or adaptation with gradient updates. Since behaviors - as the observable manifestation of deployed policies - from different tasks often share underlying skills or environmental information, our policy generator can exploit these potential correlations in the parameter space, such as shared parameters for similar motion patterns, which leads to enhanced cross-task one-shot generalizability. What we need is an end-to-end behavior-to-policy generator, not a shared base policy.

To achieve this, we introduce **Make-An-Agent**, featuring three key technical contributions: (1) We propose an autoencoder that encodes policy networks into compact latent representations based on their network layers, which can also effectively reconstruct the original policy from its latent representation. (2) We leverage contrastive learning to capture the mutual information between long-term trajectories and their success or future states. This approach yields a novel and efficient behavior embedding. (3) We utilize a simple yet effective diffusion model conditioned on the learned behavior embeddings, to generate policy parameter representations, which are then decoded into deployable policies using the pretrained decoder. (4) We construct a pretrained dataset of policy network parameters and corresponding deployed trajectories to train our proposed methodology.

To investigate the generation performance of **Make-An-Agent**, we evaluate our approach in three continuous control domains including diverse tabletop manipulation and real-world locomotion tasks. During test time, we generate policies using trajectories from the replay buffer of partially-trained RL agents. The policies generated by our method demonstrate superior performance compared to policies produced by multi-task [28; 23] or meta learning [7; 27] and other hypernetwork-based generation methods . Our generator offers several key advantages:

* **Versatility**: **Make-An-Agent** excels in generating effective policies for a wide range of tasks by conditioning on agent behavior embeddings. Since we train the parameter generator for latent parameter representations, it can generate policy networks of varying sizes within the latent space, demonstrating scalability.
* **Generalizability:** Our diffusion-based generator demonstrates robust generalization, yielding proficient policies even for unseen behaviors or unseen embodiments in unfamiliar tasks.
* **Robustness**: Our method can generate diverse policy parameters, exhibiting resilient performance under environmental randomness from simulators and real-world environments. Notably, **Make-An-Agent** can synthesize high-performing policies when fed with noisy trajectories, highlighting the robustness of our model.

## 2 Backgrounds

Policy Learning.Reinforcement Learning (RL) is structured within the formation of Markov Decision Processes (MDPs) , which is defined by the tuple \(M=,,P,,\). Here, \(\) signifies the state space, \(\) the action space, \(P\) the transition probabilities, \(\) the reward function and \(\) the discount factor. RL aims to optimize an agent's policy \(:\), which outputs action \(a_{t}\) based on state \(s_{t}\) at each timestep, to maximize cumulative rewards. The optimal policy can be expressed as:

\[^{*}=_{}_{z}[_{t=0}^{}^{t }r_{t}],\] (1)

where \(z\) represents a trajectory generated by following policy \(\). In deep RL, policies \(\) are represented using neural network function approximations , parameterized by \(_{}\), facilitating the learning of intricate behaviors across high-dimensional state and action spaces.

Diffusion Models.Denoising Diffusion Probabilistic Models (DDPMs)  are generative models that frame data generation through a structured diffusion process, which involves iteratively adding noise to the data and then denoising it to recover the original signal. Given a sample \(x_{0}\), the forward diffusion process to obtain \(x_{1},x_{2},...,x_{T}\) of increasing noise intensity is typically denoted by:

\[q(x_{t} x_{t-1})=(x_{t},}x_{t-1},_{t}I),\] (2)

where \(q\) is the forward process, \(\) is Gaussian noise, and \(_{t}(0,1)\) is is the noise variance.

The denoising process, which is the reverse of the forward diffusion, can be formulated as:

\[p_{}(x_{t-1} x_{t})=(x_{t-1}_{} (x_{t},t),_{}),\] (3)

where \(p_{}\) denotes the reverse process, \(_{}\) and \(_{}\) are the mean and variance of the Gaussian distribution respectively, which can be approximated by a noise prediction neural network parameterized by \(\).

Diffusion models aim to learn reverse transitions that maximize the likelihood of the forward transitions at each time step \(t\). The noise prediction network \(\) is optimized using the following objective, as the function mapping from \(_{}(x_{t},t)\) to \(_{}(x_{t},t)\) is a closed-form expression:

\[_{}():=_{x_{0} q, (0,1),t}[||-_{}(_{t}}x_{0}+ _{t}},t)||^{2}],\] (4)

Here, \((,)\), is the target Gaussian noise, \(_{t}:=_{s=1}^{t}1-_{s}\), and \(_{t}}x_{0}+_{t}}\) is the estimated distribution of \(x_{t}\) from the closed-form relation.

Although diffusion models are typically used for image generation through the reverse process, the variable \(x\) can be generalized to represent diverse entities for generation. In this paper, we adapt \(x\) to represent the parameters \(_{}\) of the policy network in policy learning.

## 3 Methodology

Overview.An overview of our proposed methodology is illustrated in Figure 1. To achieve this, we address several key challenges: (1) Developing latent representations of high-dimensional policy parameters that can be effectively reconstructed into well-functioned policies. (2) Learning an embedding of behavior demonstrations that serves as an effective diffusion condition. (3) Training a conditional diffusion model specifically for policy parameter generation.

Parameter representation.We use an MLP with \(m\) layers as the common policy approximator. Consequently, when the full parameters of a policy are flattened, they form a high-dimensional vector.

To enable generation with limited computational resources while retaining efficacy, and to support the generation of policies for different domains with varying state and action dimensions, we compress the policy network parameters into a latent space.

Based on the policy network architecture, we unfold the parameters following the architecture of the policy network, represented as \(x=[x_{0},x_{1},,x_{m-1}]\), where \(x_{i}\) denotes the flattened parameters from each layer. The encoder \(\) encodes each \(x_{i}\) as \(z_{i}\), resulting in a parameter latent representation denoted as \(z=[z_{0},z_{1},,z_{m-1}]\), where each \(z_{i}\) in the latent space has the same dimension, while the decoder \(\) can decode \(z\) into \(x\). To improve the robustness of this procedure, we introduce random noise augmentation in both encoding and decoding during training. Given each vectorized parameter as \(x\), we minimize the objective as,

\[=(x,((x+_{})+_{ })),\] (5)

where \(z=(x+_{})\), and \(_{}\) and \(_{}\) represent the augmented noise. The architecture of the autoencoder is shown in Figure 2.

For each domain, the autoencoder for parameter representation only needs to be trained once before parameter generation, which can handle policy parameters from different tasks. To facilitate the generalizability of the policy generator across domains, we design the latent parameter representations to have the same dimensions for different domains.

Behavior embedding.Since our goal in learning behavior embeddings is not to model the distribution of states and actions, but to provide conditional information for policy parameter generation, we aim for them to encapsulate both crucial environmental dynamics and the key information of the task goal. The principle behind our behavior embeddings is to learn the mutual information between preceding \(n\) step trajectories and subsequent states with success signals.

\[=(s_{success};\{s_{i},a_{i}\}_{i=0}^{n})\] (6)

We propose a novel contrastive method to train behavior embeddings. In Figure 3, we present a design demonstration of our contrastive loss. For a long trajectory \(\), we decouple it as the \(n\) initial state-action pairs \(^{n}=(s_{0},a_{0},s_{1},a_{1},,s_{n},a_{n})\) and the \(m\) states after the first success time \(K\) as \(=(s_{K},s_{K+1},,s_{K+m})\). Given a batch of trajectory sequences \(\{_{i}\}_{i=1}^{N}\) which can be presented as \(\{_{i}^{n},_{i}\}_{i=1}^{N}\), we optimize the contrastive objective  as:

\[(_{},_{},W)=-_{i=1}^{N} ^{}Wv_{i}}{_{j=1}^{N}h_{i}^{}Wv_{j}}\] (7)

where \(h_{i}=_{}(_{i}^{n})\) and \(v_{i}=_{}(_{i})\) are embeddings from different parts of the long trajectory \(_{i}\) and \(W\) is a learnable metric that measures the similarity between embeddings \(h_{i}\) and \(v_{i}\).

For each trajectory \(\), we obtain a set of embeddings \(_{e}=\{h_{i},v_{i}\}\). In practice, the choice of specific embeddings can be tailored to the characteristics of different tasks and trajectories. We use \((h_{i},v_{i})\) as the conditional input in our experiments.

Flexibility.With the consideration that in many scenarios, rewards are often sparse or non-existent, whereas success signals serve as a more direct indicator of whether a policy has achieved its objective. We therefore use original trajectories that exclude reward information but include success information.

For tasks without explicit success signals, such as locomotion, we segment long trajectories into multiple shorter trajectories. For each segment, we use the last \(m\) states as \(\) and the \(0-n\) state-action pairs as \(^{n}\). The informative behavior embeddings of a long trajectory are concatenated from the embeddings of all the trajectory segments.

This embedding approach strives to capture the essential information for generating behavior-specific policy parameters, including environmental dynamics and task goals, using the most concise representation possible from long trajectories and prioritizing flexibility and efficiency.

Conditional policy generator.After training the parameter autoencoder and behavior embeddings, for policy parameter \(x\) and the corresponding trajectory \(g\) deployed by policy \(x\), we can transfer \(x\) as latent parameter representation \(z\) with the autoencoder \(\) and trajectory \(\) as behavior embedding \(_{e}\). The conditional diffusion generator is trained on latent representation \(z\), conditioning on \(_{e}\). We optimize the conditional latent diffusion model via the following loss function:

\[_{}():=_{z,e(0,1),t}[ \|-_{}(z_{t},_{e},t)\|_{2}^{2}],\] (8)where the neural backbone \(_{}(z_{t},_{e},t)\) is implemented as a 1D convolutional UNet  parameterized by \(\) and t is uniformly sampled from \(\{1,,T\}\). The outputs of our parameter generator can be encoded by \(\) as deployable policies. During training the diffusion model, both the parameter autoencoder and behavior embedding layers are frozen, which ensures the training stability and efficiency.

Dataset.We build a dataset containing tens of thousands of policy parameters and trajectories from deploying these policies. The dataset is obtained from multiple RL training across a range of tasks. We utilized the dataset to train both the autoencoder and behavior embedding models. Then we use the encoded parameter representations and behavior embeddings derived from the collected trajectory to train the conditional diffusion model for policy parameter generation.

## 4 Experiments

We conduct extensive experiments to evaluate **Make-An-Agent**, answering the following problems:

* How does our method compare with other multi-task or learning-to-learn approaches for policy learning, in terms of performance on seen tasks and generalization to unseen tasks?
* How scalable is our method, and can it be fine-tuned across different domains?
* Does our method merely memorize policy parameters and trajectories of each task, or can it generate diverse and new behaviors?

Benchmarks.We include two manipulation benchmarks for simulated experiments and real-world robot tasks to show the performance and capabilities of our method as visualized in Figure 4.

MetaWorld.MetaWorld  is a benchmark suite for robotic tabletop manipulation, consisting of a diverse set of motion patterns for the Sawyer robotic arm and interactions with different objects. We selected 10 tasks for training as **seen** tasks and 8 for evaluation as **unseen** downstream tasks. Detailed descriptions of these tasks can be found in Appendix C.1. The state space of MetaWorld consists of 39 dimensions and the action space has 4 dimensions. The policy network architecture used for MetaWorld is a 4-layer MLP with 128 hidden units, containing a total of 22,664 parameters.

Robosuite.Robosuite , a simulation benchmark designed for robotic manipulation, supports various robots such as the Sawyer and Panda arms. We train models on three manipulation tasks: Block Lifting, Door Opening and Nut Assembly, using the single-arm Panda robot. Evaluations are conducted on the same tasks using the Sawyer robot. This experimental design aims to validate the practicality of our approach by assessing whether the generated policy can be effectively utilized on different robots. In the Robosuite environment, the state space comprises 41 dimensions, and the action space consists of 8 dimensions. The policy network employed for this domain contains 23,952 parameters.

Quadrupedal locomotion.To evaluate the policies generated by **Make-An-Agent** in the real world, we utilize walk-these-ways  to train policies on IsaacGym and use our method to generate actor networks conditioning on trajectories from IsaacGym simulation with the pretrained adaptation modules. Then, we deploy the generated policy on real robots in environments differ from simulations. The policies generated for real-world locomotion deployment comprise 50,956 parameters.

Dataset.We collect 1500 policy networks for each task in MetaWorld and Robosuite. These networks are sourced from policy checkpoints during SAC  training. The checkpoints are saved every

Figure 4: Visualization of MetaWorld, Robosuite, and real quadrupedal locomotion.

5000 training steps once the test success rate reaches 1. During the training stage, we fix the initial locations of objects and goals and train the policies using different random seeds. For each task, we require an average of 8 SAC training runs, approximately 30 GPU hours.

For evaluation, the trajectories used as generation conditions are sampled from the SAC training buffer within the first 0.5 million timesteps, which can be highly sub-optimal, under the same environmental initialization. During testing, the generated policies are evaluated in 5 random initial configurations, thoroughly assessing the robustness of policies generated using trajectories from the fixed environment settings.

In RoboSuite experiments, due to the inconsistency in policy networks, we retrain the autoencoder and finetune the diffusion generator trained on MetaWorld data. The experimental setup for RoboSuite is almost identical to that of MetaWorld, with the only difference being the robot used during testing.

For real-world locomotion tasks, we save 10,000 policy network checkpoints using walk-these-ways (WTW)  trained on IsaacGym, requiring a total of 200 GPU hours. The 100 trajectories used as generation conditions are sourced from the first 10,000 training iterations of WTW.

Baselines.We compare **Make-An-Agent** with four baselines, including multi-task imitation learning (IL), multi-task reinforcement learning (RL), meta-RL with hypernetworks, and meta-IL with transformers. These represent state-of-the-art methods for multi-task policy learning and adaptation. For a fair comparison, each baseline uses the same testing trajectory data for downstream adaptation.

**Multi-task BC**: We train a multi-task behavior cloning policy using trajectories in our training dataset, and then finetune it with test trajectories to adapt in specific tasks.

**Multi-task RL, CARE**: We train an mixture of encoders for 2 million steps (for each task). For RL training, we train the algorithm in dense reward environments and finetune the model using test trajectories with sparse rewards, where feedback is only provided at the end of a trajectory.

**Meta-RL with hypernetworks**: We train a hypernetwork with our training data with dense rewards, which can adapt to different task-specific policies during testing with test trajectories.

**Meta Imitation Learning with decision transformer(DT)** We train the pre-trained DT model using the training trajectories in our dataset, then use the test trajectories from replay to adapt it to test tasks.

### Performance Analysis

By using test trajectories as conditions, our policy generator can produce an equivalent number of policy parameters. Compared with baselines, we report both the best result among the generated policies and the average performance of the top 5 policies. All algorithms use the same task-specific replay trajectories. The difference is that we use them as generation conditions, whereas other methods use them for adaptation.

We define policies achieving a 100% success rate during evaluation as qualified policies. The analysis of qualification rates for policies generated by our model is presented in Appendix C.2.

Figure 5: **Evaluation of seen tasks with 5 random initializations on MetaWorld and Robosuite. Our method generate policies using 5/10/50/100 test trajectories. Baselines are finetuned/adapted by the same test trajectories. Results are averaged over training with 4 seeds.**

**Adaptability to environmental randomness on seen tasks.** Figure 5 demonstrates the significant advantage of our algorithm over other methods on seen tasks. This is attributed to the fact that, despite test trajectories originating from the same environment initialization, the generated policy parameters are more diverse, thus possessing a strong ability to adapt to environmental randomness. In contrast, other algorithms, when adapted using such singular trajectories, exhibit more limited adaptability in these scenarios. Our experimental design aligns with practical requirements, as real-world randomness is inherently more complex.

**Generalizability to unseen tasks.** Figure 6 showcases the superior performance of our algorithm on unseen tasks. Test trajectories originate from the same environment setting for each task, while evaluation occurs in randomly initialized environments. Our policy generator, without fine-tuning, directly utilizes test trajectories as input, demonstrating a remarkable ability to generate parameters that work on unseen tasks. The agent's behavior in unseen tasks exhibits similarities to seen task behaviors, such as arm dynamics and the path to goals. By effectively combining parameter representations related to these features, the generative model successfully generates effective policies. In contrast, baseline methods struggle to adapt in environmental randomness.

These results strongly suggest that our algorithm, compared to other policy adaptation methods, may offer a superior solution for unseen scenarios. To further investigate robustness in generalization, we added Gaussian noise with a standard deviation of 0.1 to actions in test trajectories used for policy generation or adaptation on unseen tasks. Figure 7 demonstrates that our method remains resilient to noisy inputs, while the performance of the baselines is significantly impacted. We believe this is because our behavior embeddings only need to capture key dynamic information as conditions to generate policies, without directly learning state-action relationships from trajectories, resulting in better robustness.

**Trajectory difference.** To compare the difference between using test trajectories as conditions and the trajectories obtained by deploying the generated policies, we visualize the trajectories during unseen task evaluations. As shown in Figure 9, our diffusion generator can synthesize various policies, which is significantly different from policy learning methods that learn to predict actions or states from trajectory data. We believe that this phenomenon fully illustrates the value of our proposed policy parameter generation paradigm.

**Parameter distinction.** Beyond trajectory differences, we also investigate the distinction between synthesized parameters and RL policy parameters. We calculate the cosine similarity between the RL policies used to obtain the test trajectories and the parameters generated from these trajectories. As a benchmark, we include the RL policies after 100 steps of finetuning with the test data. For tasks seen during training, the parameters generated by our approach demonstrate significantly greater diversity compared to the RL parameters after fine-tuning, indicating that our generator does not simply memorize training data. On unseen tasks, the similarity between our generated parameters and those learned by RL is almost negligible, with most similarities falling below 0.2. This further highlights the diversity and novelty of the policy parameters generated by our method.

**Real-world Evaluation** We further deploy policies generated from simulation trajectories onto a quadruped robot, instructing it to complete tasks as illustrated in Figure 10. Our synthesized policies

Figure 6: **Evaluation of 8 unseen tasks with 5 random initializations on MetaWorld and Robosuite. Our method generates policies using 50/100 test trajectories without any finetuning. Baselines are adapted using the same test trajectories. Average results are from training with 4 seeds.**exhibit smooth and effective responses when faced with these challenging tasks, which highlights the stability of the generated policies under the dynamics randomness of real-world environments.ii

Footnote ii: We thank Kun Lei and Qingwei Ben for their help and support in real-robot applications.

### Ablation Studies

To better investigate the impact of each design choice in our method on the final results, we conduct a series of comprehensive ablation studies. All ablation studies report average results of the Top 5 generation models on MetaWorld.

Choice of behavior embeddings.Regarding the choice of conditional embeddings, as illustrated in Figure 3, we concatenate \(h\) and \(v\) as generation conditions to maximally preserve trajectory information. Figure 8 shows that utilizing either embedding individually also achieves comparable performance due to our contrastive loss, ensuring efficient capture of dynamics information. Our contrastive behavior embeddings significantly outperform a baseline that adds an embedding layer in the diffusion model to encode trajectories as input. These ablation results underscore the effectiveness of our behavior embeddings.

Choice of trajectory length.The trajectory length \(n\) used in behavior embeddings can also impact experimental results. Figure 11(a) demonstrates that overly short trajectories lead to performance degradation, probably due to the absence of crucial behavior information. However, beyond 40 steps, trajectory length minimally impacts policy generation, indicating that our method is not sensitive to the length of trajectories.

Impact of policy network size.The impact of policy network size on generated parameters is also worth discussing, as the network's hidden size influences the dimensionality of parameters to be generated. Figure 11(b) suggests that a hidden size of 128 is a suitable choice. Smaller networks may hinder policy performance, while larger ones increase parameter reconstruction complexity.

Impact of parameter number used in training.We study the impact of the number of policy checkpoints included per task in the training dataset, as shown in Figure 11(c). Insufficient training data (\(\)=1000) leads to a significant performance decline across all tasks. With more than 1000 parameters, there is no notable improvement in performance.

Impact of latent representation size.Additionally, Figure 11(d) illustrates the impact of varying the size of the latent parameter representation. Larger latent representations can negatively affect the performance of the generative model. Conversely, when the size of parameter representations is too small, it may hinder the autoencoder's capacity to decode representations to deployable policies. This underscores the influence of the parameter autoencoder on the overall effectiveness of the policy network generator.

## 5 Related Works

Parameter Generation.Learning to generate neural networks has long been a compelling area of research. Since the introduction of Hypernetworks  and the subsequent extensions , several studies have explored neural network weight prediction. Hypertransformer  utilizes Transformers to generate weights for each layer of convolutional neural networks (CNN) using task samples for supervised and semi-supervised learning. Additionally, previous work  employs self-supervised learning to learn hyper representations of neural network weights. In the context of using diffusion models for parameter generation, G.pt  trains a diffusion transformer to generate parameters conditioned on learning metrics such as test losses and prediction errors, enabling the optimization of unseen parameters with a single update. Similarly, p-diff  propose a diffusion-based method to generate the last two normalization layers without any conditions for classification tasks. In contrast to these prior works, our focus is on policy learning problems. We develop a latent diffusion parameter generator that is more generalizable and scalable, based on agents' behaviors as prompts.

Learning to Learn for Policy Learning.When discussing learning to learn in policy learning, the concept of meta-learning  has been widely explored. The goal of meta-RL [7; 6; 9; 15] is to learn a policy that can adapt to any new task from a given task distribution. During the meta-training or meta-testing process, prior meta-RL methods require rewards as supervision for policy adaptation. Meta-imitation learning [8; 5; 27] addresses a similar problem but assumes the availability of expert demonstrations. Diffusion models have also been used in meta-learning. Metadiff  models the gradient descent process for task-specific adaptation as a diffusion process to propose a diffusion-based meta-learning method. Our work departs from these learning-to-learn works. Instead, we shift the focus away from data distributions across tasks and simply leverage behavior embeddings as conditional inputs for policy synthesis in the parameter space.

## 6 Conclusion

In this paper, we introduced a novel policy generation method based on conditional diffusion models. Targeting the generation of policies in high-dimensional parameter spaces, we employ an autoencoder to encode and reconstruct parameters, incorporating a contrastive loss to learn efficient behavior

Figure 12: Ablation studies of our technical designs on MetaWorld with 50 test trajectories (Top 5 models).

embeddings. By prompting with these behavior embeddings, our policy generator can effectively produce diverse and well-performing policies. Extensive empirical results across various domains demonstrate the versatility of our approach in multi-task settings, the generalization ability on unseen tasks, and the resilience to environmental randomness. Our work not only introduces a fresh perspective on policy learning, but also establishes a new paradigm that delves into the latent connections between agent behaviors and policy parameters.

Limitation.Due to the vast number of parameters involved, we have not yet explored larger and more diverse policy networks. Additionally, the capabilities of the parameter diffusion generator are limited by the parameter autoencoder. We believe there is substantial room for future research to explore more flexible parameter generation methods. It would also be interesting to apply our proposed generation framework to generate other structures, further facilitating exploration in policy learning within the parameter space.