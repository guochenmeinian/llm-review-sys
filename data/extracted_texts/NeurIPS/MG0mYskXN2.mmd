# Should Under-parameterized Student Networks

Copy or Average Teacher Weights?

Berfin Simsek

NYU\({}^{*}\)

bs3736@nyu.edu

&Amire Bendjeddou

EPFL

amire.bendjeddou@epfl.ch

&Wulfram Gerstner

EPFL

wulfram.gerstner@epfl.ch

&Johanni Brea

EPFL

johanni.brea@epfl.ch

Previous address: EPFL.

###### Abstract

Any continuous function \(f^{*}\) can be approximated arbitrarily well by a neural network with sufficiently many neurons \(k\). We consider the case when \(f^{*}\) itself is a neural network with one hidden layer and \(k\) neurons. Approximating \(f^{*}\) with a neural network with \(n<k\) neurons can thus be seen as fitting an under-parameterized "student" network with \(n\) neurons to a "teacher" network with \(k\) neurons. As the student has fewer neurons than the teacher, it is unclear, whether each of the \(n\) student neurons should copy one of the teacher neurons or rather average a group of teacher neurons. For shallow neural networks with erf activation function and for the standard Gaussian input distribution, we prove that "copy-average" configurations are critical points if the teacher's incoming vectors are orthonormal and its outgoing weights are unitary. Moreover, the optimum among such configurations is reached when \(n-1\) student neurons each copy one teacher neuron and the \(n\)-th student neuron averages the remaining \(k-n+1\) teacher neurons. For the student network with \(n=1\) neuron, we provide additionally a closed-form solution of the non-trivial critical point(s) for commonly used activation functions through solving an equivalent constrained optimization problem. Empirically, we find for the erf activation function that gradient flow converges either to the optimal copy-average critical point or to another point where each student neuron approximately copies a different teacher neuron. Finally, we find similar results for the ReLU activation function, suggesting that the optimal solution of underparameterized networks has a universal structure.

## 1 Introduction

A shallow neural network with a single hidden layer of a large number \(k\) of neurons can approximate any continuous function \(f^{*}\) arbitrarily well on a compact subset of the input space . We consider a related problem, where the function \(f^{*}\) itself is a neural network with a large number \(k\) of neurons, and its approximation is a smaller network with \(n<k\) neurons. In other words, we fit an under-parameterized "student" network with \(n\) neurons to a "teacher" network with \(k\) neurons. As the student has fewer neurons than the teacher, it cannot perfectly match the teacher. In the configuration with the lowest loss, where the approximation error is smallest, one may expect that the incoming and outgoing weights of a student neuron are either identical to those of a teacher neuron or that they are aligned with the weights of a group of teacher neurons, but it is unclear what the optimal configuration is.

To answer the question of whether student neurons should "copy" or "average" teacher neurons, and more generally to shed light on the loss landscape of under-parameterized neural networks, we study the theoretically tractable setup with standard Gaussian input data and teacher networks with orthogonal incoming vectors. First, we re-parameterize the loss in terms of interactions between pairs of neurons, similar to , and we re-formulate the original optimization problem as a constrained optimization problem. The interactions between neurons can be written as a function expressed in terms of the standard deviation and correlation of two Gaussian random variables, with explicit formulas for the erf and ReLU activation functions . Next, we prove several properties of the most extremely under-parameterized student network with a single neuron \(n=1\), extending thus the important work of . For many commonly used activation functions, we prove for the network with a single hidden neuron that the optimal solution is the only non-trivial critical point of the loss function up to symmetries and is achieved when the incoming vector of the one-neuron student reaches a configuration that can be interpreted as a damped average of all incoming teacher weights.

The proof relies on identifying the critical points of the constrained optimization problem and showing that the common activation functions satisfy the assumptions. We rely in particular on the derivative rule of the interaction function which comes as a pleasant consequence of Stein's Lemma  instead of the Hermite basis expansion which is a commonly used technique . For the erf and ReLU activation functions we derive additionally a closed-form solution of the optimization problem for \(n=1\). Next, we investigate "copy-average" configurations of students with \(n>1\) neurons, where some student neurons copy teacher neurons and other student neurons average sub-groups of teacher neurons, in the sense that they are at the optimal one-neuron solution for the given sub-group of teacher neurons. Our particular contributions are:

* We propose a constrained optimization formulation of the standard minimization problem in the weight-space in terms of the _interaction function_ (Section 3). The interaction function is a natural generalization of the dual activation .
* Applying the constrained optimization formulation for \(n=1\), we prove that the incoming vector of the student lies in the span of the incoming vectors of an orthogonal teacher network (Proposition 4.1). For a broad class of activation functions, we prove that the incoming vector aligns with the average of the teacher's incoming vectors for the "unit-orthonormal" teacher network (Theorem 5.1). Using the derivative rule of the interaction function (Lemma F.1), we show that common activation functions such as erf, softplus, tanh, and ReLU satisfy this property (Lemma F.2 and Corollary G.5).
* Assuming a unit-orthonormal teacher network and erf activation function, we prove that the concatenation of critical points of single neurons (of the student network) each approximating a teacher subnetwork is a _copy-average_ critical point (Theorem 4.2).
* Assuming a unit-orthonormal teacher network and erf activation function, we prove that the optimal copy-average (CA) configuration is such that \(n-1\) student neurons each copy a teacher neuron and the \(n\)-th student neuron approximates optimally the sum of the remaining teacher neurons (Theorem 5.5; see also Fig. 1, top row). Empirically, we find that the gradient flow converges to an optimal-CA point for all seeds when \(n<_{1}k\) with a fixed \(_{1}\) near \(0.46\) (Figure 4).
* Surprisingly, we find empirically three regimes of training via gradient flow (GF)2for under-parameterized networks (Figure 4): (i) for \(n<_{1}k\), GF converges to an optimal-CA point for all seeds, (ii) for \(n>_{2}k\) with a fixed \(_{2}\) near \(0.6\), GF converges to a point that we call _perturbed-\(n\)-copy_ for all seeds, (iii) for \(_{1}k<n<_{2}k\), GF converges to either an optimal-CA point or a perturbed-\(n\)-copy point. Therefore, as the under-parameterized network grows larger, the solution found with gradient flow where the weights are initialized randomly with a fixed standard deviation changes qualitatively. The code to reproduce these findings is available on GitHub, and we refer to Appendix C for details. 
### Related Work

The teacher-student setup has been extensively used in the literature to study the evolution of gradient flow trajectories and of the generalization error . This series of work gives insight into the solution found at convergence, however, they rely on numerically integrating the equations of dynamics. Tian  gives convergence guarantees for ReLU activation function, however, their method only works for _one_ student and _one_ teacher neuron. Xu and Du  recently gave the convergence rates for _multiple_ student neurons for the case of _one_ teacher neuron as a prototypical setup for overparameterization. These convergence guarantees were extended to broad input distributions  and finite training data . We give the analytical formula of the optimal solution and its generalization error for one student neuron and unit-orthonormal teacher network with _multiple_ neurons for erf and ReLU activation functions and a partial characterization for a broader class of activation functions without relying on the analytic formula of the loss.

The studies cited above showed positive results for a single-neuron teacher or a unit-orthonormal teacher. However, even for settings where the teacher has only a few neurons, hard teachers can be constructed in the sense that the student fails to find a zero-loss solution for a certain fraction of random initializations . Moreover, for medium-scale problems, gradient flow often converges to 'non-zero loss' solutions . Arjevani and Field  characterized some families of local minima using symmetries, for the ReLU activation function and unit-orthonormal teacher network. In this paper, we similarly characterize, for the case that the student has a smaller size than the teacher, a large family of 'copy-average' critical points, but for the erf activation function. Our approach focuses on the important regime of under-parameterized networks which is relevant for the superposition of features  and for the distillation of large networks into smaller ones .

There is a large history of approximation theory of neural networks that give universal guarantees on the approximation error, e.g. . However, these works focus on rates of convergence and provide neither a formula nor an approximation for the error. In this paper, we make a conjecture for the _exact formula_ of the approximation error of under-parameterized student networks which we support both theoretically and numerically.

Figure 1: _The gradient flow converges to the copy-average optimum point for erf activation (top), or nearby for ReLU activation (bottom): the first \(n-1\) neurons copy one teacher neuron each; the \(n\)-th neuron takes an average of the remaining teacher neurons._ The teacher network is unit-orthonormal, i.e. \(f^{*}(x)\!=\!_{j=1}^{k}\!(v_{j}\!\!x)\) where \(v_{j}\!\!^{d}\)’s are orthonormal, and \(d\!=\!k\!+\!1\). **A1** The gradient flow trajectory is shown in the weight space for \(n\!=\!2,k\!=\!3\): the positions of the circles (red and green) represent incoming vector \(w_{i}\) projected down to the span of \(v_{1},v_{2},v_{3}\) and the sizes of the circles represent outgoing weights \(a_{i}\). The blue circle represents the one-neuron solution (the position shows \(w^{*}\), the size shows \(a^{*}\)). **A2** Same setting, the weight-space parameters at convergence are mapped to the order-parameter space; \(u_{i}=(u_{i1},...,u_{ik})\) where \(u_{ij}\) represents the normalized dot product between \(w_{i}\) and \(v_{j}\) and \(r_{i}=\|w_{i}\|\). **B** Order parameters shown at convergence for \(n\!=\!4,k\!=\!8\). For erf (top) the point at convergence is exactly an \((n-1)\)-copy-\(1\)-average point, whereas for ReLU, it is perturbed away from this configuration. Neurons are reordered for clarity.

## 2 Setup

**Neural network:** Consider a two-layer (student) network function \(f:^{d}\) with \(n\) neurons

\[f(x)=_{i=1}^{n}a_{i}(w_{i} x)\] (1)

where \(w_{i}^{d}\) is the incoming vector, \(a_{i}\) is the outgoing weight of neuron \(i\), and the activation function \(\) is twice differentiable unless it is specified to be ReLU, i.e. \(_{}(x)=(0,x)\), and the dot marks the scalar product. \(P\!=\!n(d+1)\) is the number of parameters.

**Parameter vector:** The parameter vector is represented as

\[=(w_{1},a_{1})...(w_{n},a_{n})^{P}\] (2)

where \(\) denotes the concatenation of two vectors into one vector. We use the notation \(\), since the network function can be seen as a sum of its hidden neurons. Sometimes \(\) is written explicitly in the network function \(f(x|)\!=\!f(x)\).

**Loss function:** We assume that the input distribution is a standard \(d\)-dimensional Gaussian \(=(0,I_{d})\). The target function is denoted by \(f^{*}:^{d}\). Using the square cost, the loss function \(L:^{P}\) (also known as the risk or the generalization error) is defined as

\[L()=_{x}[(f(x|)-f^{*}(x))^{2}].\] (3)

**Orthogonal teacher network:** We assume that the target function is a neural network (also known as the teacher network or a multi-index model)

\[f^{*}(x)=_{j=1}^{k}b_{j}(v_{j} x)\] (4)

where its outgoing weights are non-zero and its incoming vectors \(v_{1},,v_{k}^{d}\) are orthogonal to each other, that is, \(v_{i} v_{j}=0\) for \(i j\). This implies that the input dimension satisfies \(d\!\!k\). Following [27; 30; 37], we particularly focus on the _unit-orthonormal_ teacher network where the outgoing weights are all one, that is \(b_{j}=1\), and the incoming vectors have unit norm, i.e. \(v_{i} v_{j}=_{ij}\).

**Optimal loss:** We study the optimal solution(s) of the following non-convex optimization problem

\[L^{n,k}(_{i=1}^{n}(a_{i},w_{i}))=_{x}[ (_{i=1}^{n}a_{i}(w_{i} x)-_{j=1}^{k}b_{j}(v_{j}  x))^{2}].\] (5)

for under-parameterized (student) networks, i.e. \(n<k\), and orthogonal teachers. For \(n k\) neurons, the network can copy all teacher neurons and set the outgoing weights of the remaining neurons to zero, therefore the optimal loss is trivially zero. If the teacher is unit-orthonormal, then all of its neurons contribute equally; hence the optimal loss is determined by \(n\) and \(k\) only and denoted by \(L^{*}(n,k)\). If the student neural network has one neuron we use the notation \(L^{*}(k):=L^{*}(1,k)\).

## 3 Foundations & Constrained Optimization Formulation

In this section, we introduce a constrained optimization problem that is a reformulation of the minimization problem in Eq. 5. This formulation allows us to show that the incoming vector of any non-trivial critical point of the one-neuron network is in the span of the teacher's \(k\) orthogonal (or potentially even non-orthogonal, see Appendix Remark D.1) incoming vectors (see Proposition 4.1). We give the exact solution in the case of a unit-orthonormal teacher (see Corollary 5.2 and Corollary G.5).

Using the linearity of expectation, the loss function in Eq. 5 can be expanded as a weighted sum of the following Gaussian integral terms

\[_{x}[(V_{1} x)(V_{2} x)]\]where \(V_{1}\) and \(V_{2}\) represent two arbitrary vectors of student and teacher networks such as \((w_{i},w_{j})\) or \((w_{i},v_{j})\). As both \(V_{1} x\) and \(V_{2} x\) are centered Gaussian random variables, the above expectation can be expressed in terms of the covariance of the two-dimensional Gaussian

\[_{x}(V_{1} x)^{2}&(V_{1} x)( V_{2} x)\\ (V_{1} x)(V_{2} x)&(V_{2} x)^{2}=r_{ 1}^{2}&r_{1}r_{2}u\\ r_{1}r_{2}u&r_{2}^{2}\]

where \(r_{i}:=\|V_{i}\|\) for \(i=1,2\) is the \(_{2}\)-norm and, assuming \(r_{i}>0\), \(u:=V_{1} V_{2}/(r_{1}r_{2})\) is the correlation. The covariance entries \(Q_{ii}=r_{i}^{2},Q_{12}=r_{1}r_{2}u\) have been used to study the gradient flow trajectories . We prefer the parametrization with \(r_{i}\) and \(u\) as it enables us to make the positive definiteness constraint explicit, i.e.

\[|u|= V_{2}|}{r_{1}r_{2}} 1,\] (6)

due to the Cauchy-Schwarz inequality. We introduce the _interaction function_\(g_{}:^{2}_{ 0}[-1,1]\)

\[g_{}(r_{1},r_{2},u)=_{(x_{1},x_{2})(0,)}[ (r_{1}x_{1})(r_{2}x_{2})]\;\;= 1&u\\ u&1,\;\;r_{1},r_{2}>0,\] (7)

to express the Gaussian integral terms. Note that \(u\) is not well-defined if one of the norms is zero. Extending the formula above, for the case w.l.o.g. \(r_{2}=0\), we define

\[g_{}(r_{1},0,u):=_{x(0,1)}[(r_{1}x)] (0)\]

for all \(u[-1,1]\). In this paper, we consider the activation functions satisfying the following.

**Assumption 3.1**.: _For all \(r_{1},r_{2}>0\) and \(u(-1,1)\), we assume that the interaction function \(g_{}\) satisfies either the first or both of the following properties_

\[\;g_{}(r_{1},r_{2},u)>0,\;}{du^{2}}g_{}(r_{1},1,u)u<g_{}(r_{1},1,u).\] (8)

To check whether a specific activation function satisfies the above properties, we mainly rely on Lemma F.1 which gives us the rule for the partial derivative of \(g_{}\) with respect to the correlation

\[g_{}(r_{1},r_{2},u)=r_{1}r_{2}[^{}(r_ {1}x)^{}(r_{2}y)].\] (9)

Hence, if \(\) is monotonic (increasing or decreasing)3, the integrand on the right-hand side is positive; satisfying Assumption 3.1 (i). The ReLU activation function \(_{}(x)=(0,x)\) also satisfies it because of the known analytical expression of the interaction 

\[g_{}(r_{1},r_{2},u)=r_{1}r_{2}h(u)\;\;h(u)= {1}{2}(}+(-(u))u).\]

Checking Assumption 3.1 (ii) for a given activation function is delicate. We rely on it in Section 5.

Using the interaction function, the loss function can be expressed in terms of the _order parameters:_

* norms of the incoming vectors of the student \(r_{i}=\|w_{i}\|\),
* correlations between the incoming vectors of the student and teacher \(u_{ij}=w_{i} v_{j}/(r_{i}\|v_{j}\|)\),
* correlations between the incoming vectors of the student \(_{ii^{}}=w_{i} w_{i^{}}/(r_{i}r_{i^{}})\);

where we assumed \(r_{i}>0\) for all \(i[n]\). The constrained optimization formulation is possible for general non-orthogonal teacher networks (see Remark D.1 in the Appendix). For the sake of simplicity, we formulate here the constrained optimization problem for the case of orthogonal teachers and reformulate the objective in Eq. 5 as

minimize \[_{i=1}^{n}a_{i}^{2}g_{}(r_{i},r_{i},1)+2_{i i^ {}}a_{i}a_{i^{}}g_{}(r_{i},r_{i^{}},_{ii^{}} )-2_{i=1}^{n}_{j=1}^{k}a_{i}b_{j}g_{}(r_{i},\|v_{j}\|,u_{ij})+C\] subject to \[\|u_{i}\| 1,\;\;r_{i} 0,\;\;i[n],\] \[|_{ii^{}}-u_{i} u_{i^{}}| \|^{2}}}\|^{2}},\;\;i i^{}[n];\] (10)where \(u_{i}=(u_{i1},...,u_{ik})\) and \(C=_{x}[f^{*}(x)^{2}]\). The constraints in Eq. 10 give tighter bounds than simply bounding correlations with the help of Eq. 6. See Appendix D for the derivation of the constraints and Fig. 2 for a schematic.

The objective above is _exact_ for \(n=1,2\), in the sense that its optimal solution is equivalent to the optimal solution in the weight space, since the mapping from the weight-space to the order space is invertible. However, it is a _relaxation_ for \(n 3\), since there are order-parameter configurations in the domain (see Figure 2) that do not correspond to any weight-space configuration (see Appendix D.3 for a construction). It seems possible to overcome this gap by considering the geometry of the angles between \(n 3\) incoming vectors to tighten the constraints between student-student correlations.

## 4 Copy-Average Critical Points

In this section, we identify a new family of critical points by 'combining' critical points of one-neuron networks for the unit-orthonormal teacher and the erf activation function. We first show that in a network with \(n=1\) student neuron, for any "non-trivial" critical point, that is \(w^{*} 0\) and \(a^{*} 0\), the incoming vector \(w^{*}\) is in the span of the teacher's incoming vectors (Proposition 4.1). Applying this proposition to the special case of the erf activation function, we show that the concatenation of such critical points is also a critical point for multi-neuron networks (Theorem 4.2).

**Proposition 4.1**.: _Assume that \(f^{*}\) is an orthogonal teacher network (Eq. 4) of width \(k\). If the activation function satisfies Assumption 3.1 (i), any non-trivial critical point \(^{*}=(w^{*},a^{*})\), i.e. \( L^{1,k}(^{*})=0\), \(\|w^{*}\| 0\), \(a^{*} 0\), satisfies that \(w^{*}\) is in the span of the teacher's incoming vectors._

The proof uses the constrained optimization formulation for \(n=1\) (see Appendix G.1). In short, a critical point mapped to the order parameter space satisfies either \(\|u_{1}\|=1\) or \(_{u}g_{}(r,\|v_{j}\|,u_{1j})=0\) for all \(j[k]\). Under the Assumption 3.1 (i) these partial derivatives are non-zero, hence \(\|u_{1}\|=1\). In a recent work , the incoming vectors of the student network are also shown to converge to the span of the vectors of the multi-index model using weight-decay.

Finding the optimal solution for the multi-neuron network is challenging. Natural candidates are concatenation of student neurons where each one of them is a critical point of the loss function \(L^{1,_{i}}\) where \(_{i}\) is the number of the subgroup of teacher neurons. More precisely, let us pick a partition \(_{1}+...+_{n} k\) such that \(_{i} 1\), and define \(s_{m}=_{i=1}^{m}_{i}\) for \(m n\) and \(s_{0}=0\). We denote a one-neuron critical point by \(_{i}^{*}=(w_{i}^{*},a_{i}^{*})\), when learning from a part of the teacher network

\[f_{i}^{*}(x)=_{j=s_{i-1}+1}^{s_{i}}(v_{j} x).\] (11)

Since \(f_{i}^{*}\) is a unit-orthonormal teacher, \(w_{i}^{*}\) is in the span of \(v_{s_{i-1}+1},...,v_{s_{i}}\) due to Proposition 4.1.

Figure 2: _Cartoon representation of the mapping of a student with three neurons from the weight space **A**\(^{nd}\) to order parameter space **B1-B2**. The mapping between the outgoing weights is an identity mapping hence not shown. **A** Each axis shows the direction of weights \(v_{i}\) of one teacher neuron (\(k 3\)). **B1** Each incoming vector \(w_{i}^{d}\) is first transformed into \((r_{i},w_{i}/r_{i})\) and then \(w_{i}/r_{i}\) is projected onto the span of the teacher’s incoming vectors, yielding the student-teacher correlation vector \(u_{i}=(u_{i1},...,u_{ik})\). **B2** The student-student correlations \(_{ii^{}}\) are in general free parameters bounded in between \(u_{i} u_{j}\|^{2}}}\|^{2}}\) hence the box constraint. An activated constraint, w.l.o.g. \(u_{1}^{k-1}\), gives a vanishing \(\) term for the interval of correlation \(_{1i}\) for all \(i 1\), hence they are no longer free (shown in red). In the case \(d=k\), all \(u_{i}\) are on the hypersphere due to the problem geometry, hence the correlations \(_{ii^{}}\) are fixed and not free (see Appendix D.1).

We use the term _copy-average_ (CA) point or configuration to refer to the concatenation of such one-neuron critical points in the student network with \(n\) neurons: if \(_{i}=1\), the student neuron copies one of the teacher neurons \((v_{j},1)\); if \(_{i}>1\), it averages a group of teacher neurons in the sense of approximating their sum with one neuron. For odd activation functions, the one-neuron network problems decouple from each other, as the cross-terms \([(w_{1} x)(w_{2} x)]\) vanish for \(w_{1} w_{2}\). For the specific case of erf, we prove that all the copy-average configurations are critical points.

**Theorem 4.2**.: _Assume that \((x)=_{\!}(x)=}_{0}^{}}e^{-t^{2}}dt\). We pick a copy-average parameter_

\[^{*}=(w_{1}^{*},a_{1}^{*})...(w_{n}^{*},a_{n}^{*})\] (12)

_where \((w_{i}^{*},a_{i}^{*})\) is a non-trivial critical point when learning from a unit-orthonormal teacher \(f_{i}^{*}\) with the incoming vectors \(v_{s_{i-1}+1},...,v_{s_{i}}\) shown in Eq. 11. Then \(^{*}\) is a critical point of the loss function \(L^{n,k}\) where the target function is \(f^{*}(x)=_{j=1}^{k}(v_{j} x)\)._

In particular, all neurons are equivalent to each other in a unit-orthonormal teacher network. Therefore, the copy-average configurations where \(n-1\) student neurons each copy a distinct teacher neuron and the \(n\)-th student neuron takes an average are also equivalent and called \((n-1)\)-copy-\(1\)-average, or \((n-1)\)-C-1-A in short. Another interesting configuration is where \(n\) student neurons each copy a distinct teacher neuron, which is called \(n\)-copy, or \(n\)-C in short.

For general activation functions the copy-average parameter vectors are not critical points (see Eq. 56). Nevertheless, we numerically find that the gradient flow converges to similar configurations for the ReLU activation function (see Figure 1, see Appendix C.3 for more experiments).

## 5 Approximation Error of Underparameterized Networks

The target function is assumed to be a unit-orthonormal teacher network in this section. In Subsection 5.1, we show for the one-neuron network that there is a unique non-trivial critical point up to symmetries, which is necessarily the global minimum (Theorem 5.1). Furthermore, we give the analytic expression of the optimal solution and its loss for erf (Corollary 5.2) and ReLU (Corollary G.5) activation functions. In Subsection 5.2, we provide for the under-parameterized student with \(n>1\) neurons the exact loss of copy-average critical points for the erf activation function and show that the \((n-1)\)-copy-\(1\)-average configurations reach the lowest loss among CA-critical points (see also Appendix E.1 for the combinatorial number of the equivalent copy-average configurations related to the landscape complexity calculations ).

### One-Neuron Network

Using the constrained optimization formulation in 10, we first prove that at any non-trivial critical point of the one-neuron network, the incoming vector aligns equally with all teacher's incoming vectors for unit-orthonormal teachers for activation functions satisfying Assumption 3.1 (see Theorem 5.1). This is related to the symmetric solution visited during the learning plateaus studied in Saad and Solla  for erf activation and in Tian  for ReLU activation (see Appendix B for a detailed comparison). Our proof works for a broad class of activation functions and does not use the analytic expression of the interaction function.

**Theorem 5.1**.: _Assume that the activation function satisfies Assumptions 3.1 (i) and (ii). At any non-trivial critical point \((w^{*},a^{*})\) of the loss \(L^{1,k}\) for the unit-orthonormal teacher network, the incoming vector satisfies_

\[}{\|w^{*}\|}=u^{*}_{j=1}^{k}v_{j}\] (13)

_where \(u^{*}\) is either \(}\) or \(-}\)._

Proof Sketch.: There is no student-student interaction term since we have a single neuron; therefore we write \(u_{j}\) instead of \(u_{1j}\) and the constrained optimization problem in 10 simplifies to

\[a^{2}g_{}(r,r,1)-2a_{j=1}^{k}g_{}(r,1,u_{j} )+,\|u\| 1,r 0.\] (14)From Proposition 4.1, we have that \(\|u\|=1\) for any non-trivial critical point. Therefore, the constraint of 14 on the correlations \(u=(u_{1},...,u_{k})\) is satisfied. The mapping of any non-trivial critical point to the order-parameter space is a critical point of the Lagrangian loss (see Appendix Lemma G.2). Hence every \(u_{j}\) satisfies

\[-2a}g_{}(r,1,u_{j})+2 u_{j}=0\] (15)

for fixed \((r,a)\). Assumption 3.1-(ii) implies that \(_{u}g_{}(r,1,u)\) is one-to-one hence all \(u_{j}\) are equal. _End of Proof Sketch._

We show in Lemma F.2 that the interactions of the common activation functions such as erf, tanh, sigmoid, and softplus (respectively)

\[_{}(x)=}\!_{0}^{}}e^{-t^{2}}dt,\;_{}(x)=}{1+e^{-x}},\; _{}(x)=},\;_{}^{}(x)= (e^{ x}+1),\]

with \((0,2]\) satisfy Assumption 3.1-(ii). The interaction of the ReLU activation function, i.e. \(_{}(x)=(0,x)\) also satisfies Assumptions 3.1 (with a slight modification in the domain for (ii); see the proof of Corollary G.5).

Thanks to Theorem 5.1, the loss in Eq. 14 can be reduced to a two-dimensional loss in \(a\) and \(r\), which can be solved explicitly for ReLU (Corollary G.5) and erf.

**Corollary 5.2**.: _Assume that the activation function is \(_{}\). The optimal solution \((w^{*},a^{*})\) is given by_

\[\|w^{*}\|=}, a^{*}=k,}{ \|w^{*}\|}=}_{i=1}^{k}v_{i},\]

_or, equivalently, by \((-w^{*},-a^{*})\). The optimal loss is then given by_

\[L^{*}_{}(k)=k\! {1}{2}-k^{2}\!.\] (16)

The proof of Corollary 5.2 is presented in Section G.3. For general activation functions, the two-dimensional loss does not admit an analytical expression. For this case, from the partial derivatives, we obtain a fixed-point equation in \(r\) which we solve numerically for the activation functions listed above (see Appendix Section G.2.2). For softplus, specifically, we prove in addition the following

**Theorem 5.3**.: _Assume that the activation function is \(_{}^{}(x)\) with \( 2\). The optimal solution \((w^{*},a^{*})\) satisfies \(\|w^{*}\|}{{}}\) and \(a^{*} k\)._

We use the FKG inequality to prove Theorem 5.3 (see Appendix G.5). Although the proof requires specific properties of softplus, we show that the above bounds hold also for tanh and sigmoid by numerically solving the fixed point equation (Figure 10, also Figure 5). Note that the incoming vector norm is bounded above by \(1/\), hence \(w^{*}\) is a _damped_ average of the teacher incoming vectors.

Figure 3: _One-neuron network solutions._ **A** Network output (color coded) as a function of input in \(d=2\) for (left) a unit-orthonormal network with \(k=2\) neurons (incoming vectors \(v_{1}\) and \(v_{2}\) are shown as black dots) and (right) the student network function generated by the optimal solution (incoming vector shown in red) for the erf activation function. **B** Same for the softplus activation function. **C** Approximation error of a student with \(n=1\) neurons as a function of the number of \(k\) teacher neurons. For large \(k\), the approximation error for \(n=1\) grows near-linearly for the differentiable activation functions studied in this paper (erf, sigmoid, tanh, and softplus with \(=1\)); however the growth is quadratic for ReLU (see Appendix Corollary G.5).

**Remark 5.4**.: _We do not impose either of the two reductions that are common in literature: (i) incoming vector \(w\) is constrained to be on the unit sphere , (ii) the outgoing weight \(a\) is constrained to be one . An important step in our analysis is related to the norm \(r\) of the incoming vector which we discuss in Appendix Section G.2.2._

### Multi-Neuron Network

In this subsection, we assume that the activation function is erf such that CA-configurations are critical points (see Theorem 4.2). For a student network with \(n=2\) and a partition \((_{1},_{2})\) we can decompose the loss of a CA critical point as

\[L_{}^{*}(_{1})+L_{}^{*}(_{2})+L_{}^{*}(0, k-(_{1}+_{2}))\] (17)

where \(L_{}^{*}(0,_{0}):=_{x}[f_{_{0}}^{*} (x)^{2}]\) is the error made by a student with vanishing output when representing a reduced unit-orthonormal teacher network with \(_{0}\) neurons. This decomposition is possible because the cross-terms between orthogonal vectors are zero for odd activation functions. Furthermore, for the erf activation function, we show that

\[L_{}^{*}(_{1})+L_{}^{*}(0,_{0})>L_{}^{*} (_{1}+_{0})\] (18)

(see the proof of Lemma E.2). Therefore, we should search for the minimum loss configuration among the partitions with \(_{1}+_{2}=k\). Among such partitions, Lemma E.2 shows that the optimum CA-point has the partition \((1,k-1)\). In words, the optimum is a \(1\)-copy-\(1\)-average point.

For general \(n\), using Lemma E.2 and a small trick, we prove the following.

**Theorem 5.5**.: _Consider a unit-orthonormal teacher network \(f^{*}(x)=_{j=1}^{k}(v_{j} x)\) and the erf activation function. For an under-parameterized student network with \(n\) neurons, the minimum-loss copy-average configuration up to permutations (of the student and teacher neurons) is_

\[=(_{1}v_{1},_{1})...(_{n-1}v_{n-1}, _{n-1})(_{n}w_{n}^{*},_{n}a_{n}^{*})\] (19)

_where \(_{i}\{ 1\}\) and \((w_{n}^{*},a_{n}^{*})\) is given by Corollary 5.2 after substituting \(k\) with \(k-n+1\)._

Figure 4: _Under-parameterized student networks of width \(n\) with erf activation function learning (via gradient flow) from a unit-orthonormal teacher network of width \(k\)._ **A** Each dot is the mean error at convergence for \(20\) seeds of random initializations; black-dashed lines are the theory predictions \(L_{}^{*}(k-n+1)\), see Eq. 20. Standard deviations do not show on the figure as they are too small. We identify four regimes indicated by colors (green-gray-blue-red) depending on the type of solution found by gradient flow (GF). In the green regime, GF converges to an optimal \((n-1)\)-C-\(1\)-A solution for all \(20\) initializations (Fig. 4-B1). In the gray regime, GF converges either to \((n-1)\)-C-\(1\)-A solution or to a “Perturbation of the all-copy solution” that we call P-\(n\)-C (Fig. 4-B2). In the blue and red regimes, for \(n>_{2}k\) where \(n=8,12,16\) the gradient flow converges to a P-\(n\)-C solution from all seeds (Fig. 4-B3). Moreover, in the red regime, for \(n>_{3}k\) where \(n=8,12,16\) and \(_{3}\) is near \(0.75\), the P-\(n\)-C solutions achieve lower loss than the \((n-1)\)-C-\(1\)-A solutions (Fig. 4-B4). **B1-B4** Examples of loss at convergence (vertical axis) for all \(20\) different initialization seeds (horizontal axis); theory is shown by the red-dashed horizontal line. Insets show examples of correlation matrices \(u_{ij}\) (\(k\) lines, \(n\) columns) between student and teacher incoming vectors at convergence after reordering neurons. In the gray regime (for ex. B2) the gradient flow converges to either one of the two types of minima with correlations shown in the inset; in the other regimes, it consistently converges to the same minimum up to permutations.

See Appendix E.3 for the proof. Because copy-average critical points are not necessarily the only critical points of the loss function for students with \(n>1\), we investigate in simulations, if they are found by gradient flow where the weights are initialized as Gaussian with a fixed standard deviation (see Fig. 4).

Interestingly, gradient flow converges to the CA-optimal solution for all random seeds in a broad regime of under-parameterization (green in Fig. 4). Only when \(n>_{1}k\) for \(n=8,12,16\) and \(_{1} 0.46\), gradient flow converged in some seeds to points close to the \(n\)-copy critical point. We call these newly found points "perturbed \(n\)-copy" (P-\(n\)-C) points. In gray-blue regimes, the P-\(n\)-C points have higher loss than the optimal CA critical point (Fig. 4).

However, this is not always the case: when the student width is close to the teacher width (low compression regime), the P-\(n\)-C point has a slightly lower loss the lowest amongst the CA critical points (red in Fig. 4). When \(k-n\) is fixed, we found that the \((n-1)\)-C-\(1\)-A solution turns from a minimum for small \(n\) to a saddle for large \(n\) (see App. Fig. 6); which explains why the gradient flow escapes it in this regime and converges to another minimum at a lower loss.

Finally, based on our theory and experiments, we conjecture that there exists a \(_{0}(0,_{3})\) such that when \(n<_{0}k\) and when the activation function is erf, the global optimum of the non-convex loss in Eq. 5 is a \((n-1)\)-C-\(1\)-A configuration. Therefore, if our conjecture holds, the _exact_ approximation error, i.e. the optimal loss, is identical to that of a one-neuron network approximating a teacher with \(k-n+1\) neurons and is given by

\[L^{*}_{}(n,k)=L^{*}_{}(k\!-\!n\!+\!1).\] (20)

## 6 Conclusion & Future Directions

We studied the learning of under-parameterized student networks from orthogonal teacher networks for standard Gaussian input data and vanishing thresholds. For erf activation function, we introduced a new family of critical points that arise from the decoupling of the problem into one-neuron networks that can be solved separately. Moreover, the exact parameters of copy-average (CA) critical points are given which can be used to study escape behavior near saddles and to determine convergence of first and second-order optimization algorithms [16; 41].

Furthermore, we showed that the optimal CA point is that \(n-1\) neurons copy teacher neurons and the \(n\)-th neuron averages the remaining \(k-n+1\) neurons. In simulations, gradient flow converges to a CA-optimal solution for \(n<_{1}k\) where \(_{1}\) is near \(0.46\). However, for \(n>_{2}k\) where \(_{2}\) is near \(0.6\), we observe another phase where the gradient flow finds a perturbed copy solution. For the ReLU activation function and the onset of under-parameterization, gradient flow converges to qualitatively similar solutions; however, at the crossing point from under-parameterization to over-parameterization (i.e. \(n=k\)), gradient flow is known to get stuck in spurious local minima . On another note, determining the CA-optimal solution of the two-neuron network plays a critical role in our analysis. Still, there is only little literature on two-neuron networks [5; 42] compared to the well-studied one-neuron case [5; 6; 7; 22; 23]. The two-neuron network is possibly the simplest model with interactions between neurons, hence it is important to understand the global minimum and gradient flow dynamics of this challenging problem.

On the practical side, our analysis of under-parameterized networks gives a recipe for how to warm-start smaller neural networks for distilling unit-orthonormal teacher networks. If one desires low compression (\(n>_{3}k\)), then we recommend initializing the student network in a configuration where each neuron copies a different teacher neuron, to be close to a P-\(n\)-C point. However, for higher compression, we recommend initializing the student network in a configuration where \(n-1\) neurons are each copied and the \(n\)-th neuron is initialized as an average neuron to be close to a \((n-1)\)-copy-\(1\)-average point. It remains an open question whether this recipe applies to non-idealized scenarios such as non-isotropic input distribution, teacher networks with non-orthogonal incoming vectors, or non-unit outgoing weights. More generally, it is natural to expect that the optimal distillation strategy changes from low compression levels to high compression levels. How exactly and where this change happens is a very intriguing question of theory and practice.