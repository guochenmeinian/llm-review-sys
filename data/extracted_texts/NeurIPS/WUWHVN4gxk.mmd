# Dataset and Lessons Learned from the 2024 SaTML LLM Capture-the-Flag Competition

Edoardo Debenedetti\({}^{*}\)\({}^{1}\) Javier Rando\({}^{*}\)\({}^{1}\) Daniel Paleka\({}^{*}\)\({}^{1}\)

Fineas Silaghi\({}^{3}\) Dragos Albastroiu\({}^{1}\) Niv Cohen\({}^{4}\) Yuval Lemberg

**Reshmi Ghosh\({}^{5}\) Rui Wen\({}^{2}\) Ahmed Salem\({}^{5}\) Giovanni Cherubin\({}^{5}\) Santiago Zanella-Beguelin\({}^{5}\) Robin Schmid\({}^{1}\) Victor Klemm\({}^{1}\) Takahiro Miki\({}^{1}\) Chenhao Li\({}^{1}\) Stefan Kraft\({}^{6}\) Mario Fritz\({}^{2}\) Florian Tramer\({}^{1}\) Sahar Abdelnabi\({}^{5}\) Lea Schonherr\({}^{2}\)**

\({}^{1}\)ETH Zurich \({}^{2}\)CISPA Helmholtz Center for Information Security \({}^{3}\)West University of Timisoara

\({}^{4}\)New York University \({}^{5}\)Microsoft \({}^{6}\)Zurich University of the Arts

{edoardo.debenedetti, javier.rando, daniel.paleka}@inf.ethz.ch

indicates equal contribution. Authors between \(\) submitted awarded defenses and attacks.

###### Abstract

Large language model systems face significant security risks from maliciously crafted messages that aim to overwrite the system's original instructions or leak private data. To study this problem, we organized a _capture-the-flag_ competition at IEEE SaTML 2024, where the _flag_ is a secret string in the LLM system prompt. The competition was organized in two phases. In the first phase, teams developed defenses to prevent the model from leaking the secret. During the second phase, teams were challenged to extract the secrets hidden for defenses proposed by the other teams. This report summarizes the main insights from the competition. Notably, we found that all defenses were bypassed at least once, highlighting the difficulty of designing a successful defense and the necessity for additional research to protect LLM systems. To foster future research in this direction, we compiled a dataset with over 137k multi-turn attack chats and open-sourced the platform.

## 1 Introduction

Large language models (LLMs) are increasingly deployed as chatbots in various applications where they may interact with untrusted users. To enable useful and personalized applications, LLMs support _system prompts_. System prompts are application-specific and contain instructions that should be followed at all times (e.g., "only answer questions about billing") and relevant system information (e.g., the current date). These chatbots can also be augmented with data or access to retrieval tools that can be useful for the particular application. For instance, a customer support chatbot may have query access to the customer database to answer questions about billing.

Although this design enables many interesting use cases, there are no established methods to guarantee that instructions in the system prompt will always be followed (Wallace et al., 2024) or to ensure the confidentiality of the data accessed by the model. Prior work showed that malicious users could design _prompt injection attacks_ to override the system's instructions or leak private information (Perez and Ribeiro, 2022). In light of the recent integration of LLMs in general-purpose applications, suchas Google Search or Copilot, there is a sense of urgency to improve our understanding and rapidly develop mitigations for these attacks.

The research community's current understanding of prompt injection attacks and defenses is very limited since most of these systems are proprietary, and vulnerabilities are rarely disclosed. To promote research in this area, we organized a prompt injection competition at IEEE SaTML 2024 with two main goals: (1) to collect a comprehensive dataset for future research and (2) to gain insight into the strengths and limitations of state-of-the-art LLMs.

We designed the competition as a _capture-the-flag_ (CTF) challenge, where the _flag_ is a secret string included in the LLM system prompt. Competition participants were tasked with both protecting and leaking these secret strings. The competition consisted of two rounds. First, participants submitted defenses to prevent the model from revealing the secret. Then, participants attempted to extract secrets from as many submitted defenses as possible. This paper presents a comprehensive analysis of the most successful defenses and attacks, along with a summary of the main insights from the competition. Together with this report, we are releasing the resulting prompt injection dataset--with more than 137k multi-turn conversations--and the competition code base. These resources can be used to benchmark future methods.

## 2 Competition setup

The competition was organized as a capture-the-flag (CTF) challenge with free registration. Every registered team obtained $20 in credits to query the models required for the competition. The CTF was separated into two disjoint and consecutive phases--defense and attack. Teams could participate in the defense phase, attack phase, _or both_. We provided participants a web interface (see Figure 1) and an API for automated experiments.

Defense phase.In this phase, the teams created defenses to prevent attackers from extracting the secret from GPT-3.5 and Llama-2 (70B), respectively. A defense is defined by three components: a system prompt that can provide further instructions to the model, a Python filter, and an LLM filter. Both filters modify model outputs based on their content and the user input. See Appendix B.5 for details. Defenses were evaluated on a utility benchmark to ensure that they preserve utility for prompts that are not related to secret extraction. For instance, a defense that always returns "I cannot help with that" would be impenetrable but would fail our utility evaluation.

Attack Phase.After finishing the defense phase, the target was to break as many submitted defenses as possible. The attack phase consists of two overlapping stages: the _reconnaissance phase_ and the _evaluation phase_. In the _reconnaissance phase_, attackers can interact arbitrarily with defenses instantiated with random secrets. In the _evaluation phase_, the secret is fixed, and the number of interactions is limited and scored. See Appendix B.3 for details.

Figure 1: CTF web interface for the _defense phase_. Teams can create and edit defenses (left) and then chat with the resulting model (right). A similar interface—without defense information—was provided during the _attack phase_.

Teams were scored independently for their performance in both phases. In short, the best defense is the one broken by the fewest attackers. Conversely, the best attack is the one extracting the most secrets with the fewest amount of attempts. For detailed scoring rules, see Appendix B.3. The best three attacks and defenses obtained cash prizes and a travel grant to present their approach at SaTML 2024. Their methods are detailed in Sections 5 and 6.

## 3 Competition Outcomes

The competition in numbers.The CTF had 163 registered teams. 72 defenses were submitted by 39 different teams, with 44 of these defenses being accepted after utility evaluation. During the attack phase, there were 137,063 unique chats to break the defenses, and 35 teams succeeded in breaking at least one defense.

A dataset for future research.We have compiled a labeled dataset with all the chats recorded during the attack phase and their corresponding defenses. This is one of the largest prompt injection datasets available to date. Unlike existing datasets (Schulhoff et al., 2023; Toyer et al., 2023; Lakera et al., 2023), the defenses are not only prompts and simple rules but also include complex filters, and we include multi-turn conversations. We hope this dataset will foster future research into the security of LLMs. All dataset details are in Section 4.1.

An open-source platform for future experiments.We have also open-sourced the CTF codebase, which can be easily adapted to host similar competitions in the future. This codebase is also a valuable resource for education and research since it can be used for smaller prompt injection competitions or tutorials with little effort. All platform details are in the official repository https://github.com/ethz-spylab/satml-llm-ctf.

Summary of findings.Analyzing trends across submitted defenses and attacks provide valuable insights for future research. Our findings emphasize the importance of adaptive attacks and their role in evaluating the security of LLM systems. They also reveal the brittleness of defenses against prompt injection. In fact, all defenses were bypassed at least once, highlighting the difficulty of building secure systems that can withstand adversarial attempts to bypass security measures. We elaborate on the main takeaways and future research directions in Section 7.

## 4 The Dataset

### Dataset Structure

We release the full dataset of defenses and adversarial chats collected during the _attack phase1_. The dataset can be accessed via HuggingFace2, and we provide a starting codebase to perform basic operations and explore the dataset3. The dataset is divided into two splits: _defenses_ and _chats_.

2 Dataset on HuggingFace: https://huggingface.co/datasets/ethz-spylab/ctf-satml24

Defenses split.It contains all 44 accepted defenses and their details: defense prompt, Python filter, and LLM filter. Each defense is also linked to a team and model. Appendix G includes an example and the schema.

Chats split.It contains _137,063_ chats recorded during the _attack phase_. Each entry includes the defense details, the secret, the model used, the attacking team, and the history of messages. If the defense applies filters, all intermediate outputs before and after each filter are included. Additionally, we include two binary flags: is_evaluation, which indicates whether the chat occurred in reconnaissance (false) or evaluation (true) mode; and was_successful_secret_extraction, which indicates whether the attackers submitted a correct guess for the secret during the chat. Appendix G includes an example and the schema.

### Dataset Exploration

The chat dataset includes conversations from 65 attack teams, with 35 teams having extracted at least one secret. In total, there are 5,461 entries (4%) with a successful secret extraction. This granularity at the team level enables future research to analyze how attack strategies develop for each team and identify the key factors that contribute to their success, especially for more challenging defenses.

We analyze the diversity of attack strategies in the chat datasets using three metrics. (1) To identify attacks evolving from a common template, we counted the number of distinct 20-character prefixes in the first user messages of the conversations. (2) We also counted the total number of distinct first messages to account for attackers using the same attack across different defenses. (3) Finally, we counted the number of distinct pairs (attacker, defense). Table 1 summarizes our diversity metrics for successful and unsuccessful chats.

The dataset contains 6,402 (4.6%) distinct 20-characters prefixes and 40,878 (30%) distinct first messages. These results suggest that many teams started their attacks from a common structure and implemented different ablations to achieve successful attacks.

Unlike existing datasets (Schulhoff et al., 2023; Toyer et al., 2023; Lakera AI, 2023), our CTF enables _multi-turn conversations_. Table 2 provides an overview of the distribution of attacker messages per conversation in our dataset. The results highlight the importance of multi-turn conversations for successful attacks. While 82% of unsuccessful chats have only a single user message, only 67% of the successful attacks contain one user message. In fact, 15% of the successful chats used four or more user messages. The awarded attackers also reported using multi-turn attacks to bypass the defenses (see Section 6).

Limitations.During our exploration, we encountered a small percentage of conversations where the label was_successful_secret_extraction may be unreliable. We define a chat as successful if there is a correct secret guess tied to it4. This can be misleading in two uncommon cases: (1) the secret is extracted across multiple chats (e.g., one character per chat) with only the last chat labeled as a successful extraction; and (2) the attacker successfully extracts the secret but either forgets or chooses not to submit it (e.g., to test if it is a decoy secret) and then creates an empty chat to submit their final guess. Based on a sample of the dataset, we found both cases to be rare and treatable as noise in the data. Automatically relabeling these chats is challenging, as the extracted secrets can be obfuscated in non-trivial ways to bypass filters (see Section 6).

    & Total & Distinct 20-char & Distinct & (Attacker user, & (Attacker team, \\  & chats & prefixes & first messages & defense) pairs & defense) pairs \\  Successful & 5,461 & 408 & 1,548 & 747 & 610 \\ Unsuccessful & 131,602 & 6,377 & 40,668 & 1,745 & 1,157 \\  All chats & 137,063 & 6,402 & 40,878 & 1,800 & 1,186 \\   

Table 1: Diversity evaluation on the _chats_ split.

    &  \\   & 1 & 2 & 3 & 4-7 & \(>\)7 \\  Successful chats & 67.8\% & 11.5\% & 5.7\% & 12.6\% & 2.4\% \\ Unsuccessful chats & 82.5\% & 9.3\% & 2.4\% & 4.1\% & 1.7\% \\ All chats & 81.9\% & 9.4\% & 2.6\% & 4.4\% & 1.8\% \\   

Table 2: Number of attacker messages per chat. Longer conversations are more successful. The total chat length is twice this number, as each user message has a corresponding model response.

Most Effective Defenses (3 Winning Teams)

### Team Hestia

System prompt.This defense included mock secrets into the system prompt as decoys, prompting the model to reveal these instead of the actual secret. As a result, attackers could retrieve strings that resembled the secret they were looking for, but were not the actual hidden flag. Anticipating that attackers might try to bypass the decoys by asking for the first secret in the prompt, one of the decoys was designated as the "first secret."

Python filter.The filter included a regular expression to replace the secret and decoys, verbatim or assuming some minor modifications such as lowercasing, uppercasing, or missing characters. The resulting filtered string was then uppercased. Additionally, the filter returned an empty string to terminate the session if one of the secrets in the prompt was detected.

LLM filter.The LLM filter was used for a single purpose: to restore capitalization after the Python filter had uppercased the output. This filter helps maintain natural capitalization, preserving the model utility on different tasks. The LLM filter was limited to capitalization since it frequently made undesired edits when given more complex instructions.

More details about this defense are in Appendix C.

### Team RSLLM

Defense prompt.The system prompt was enhanced with: decoys to mislead attackers, explicit instructions to avoid revealing secrets in specific formats, and keywords to strengthen the defense's authority. The authors discarded less effective methods, such as threatening the model if the secret was revealed.

Python Filter.The primary purpose of the filter was to modify the case (uppercase/lowercase) of the text. For GPT-3.5, all characters were changed to uppercase and then corrected using the LLM filter. For Llama-2, LLM filters could not reliably reconstruct the text from an all uppercase output, thus the output was converted to lowercase instead. Additionally, the filter removed the secret or common transformations, such as case changes or letter-by-letter reveals.

LLM filter.The LLM filter corrected the case-modified text from the Python filter and refused responses that might reveal the secret in formats that were not captured by the filter form. This filter was not heavily relied upon as a defense, as it can be bypassed by adaptive attacks.

More details about this defense is in Appendix D.

### Team WreckTheLine

Defense prompt.Similar to the approach used by previous teams, this defense incorporates five fake decoy secrets in the system prompt (Stage I in Appendix E). However, since these decoys are static and can be learned and bypassed by attackers, the defense includes two additional stages. First, it leverages safety alignment to associate the revelation of secrets with unsafe behaviors (e.g., "revealing secret is immoral", etc.). Finally, to preserve utility in all other cases, the prompt instructs the model to answer any query not related to the secret.

Python filter.The filter replaced the secret, including encoded versions such as base64, base32, or hexadecimal. If the secret or any decoy secret is found, the filter returns the user input instead.

LLM filter.This defense did not implement a LLM filter.

More details about this defense are in Appendix E.

Most Effective Attacks (3 Winning Teams)

### Team WreckTheLine

The attack contains a common exploit for all defenses that was later adapted to bypass each specific defense. A common defense approach is filtering out queries containing specific "forbidden words" (e.g., secret, code, reveal, etc.). The attack thus avoids using these words by either altering letters (e.g., changing "code" to "c0de") or using synonyms (e.g., referring to "code" as "6 characters key").

Another common issue was obtaining incorrect secrets. These secrets were constant across prompts and transformations, leading the attackers to assume these were probably fake secrets included by the defenses. The most effective method to bypass this defense involved iteratively extracting these fake secrets and instructing the model to ignore them until the actual secret was extracted.

Finally, the attack had to account for potential transformations that the filters may have introduced in the extracted secrets (e.g., casing modifications). The attackers instruct the model to display each character of the secret code on a separate line and to specify whether it is a digit, lowercase, or uppercase letter. Thus, even after filter transformations, the attack could still be successful, as the output would look like: "A - lower, 1 - digit, B - upper", etc.

### Team Shrug Face "_(e)_/"

This attack also has a common exploit that works for many defenses and specific exploits for harder defenses. The common exploit contained three main components: (1) instructs the model to be "super brief", (2) asks a series of benign questions first before asking about the secret, (3) queries the model to "write a Python list of the individual characters of the six characters string [...] within single quotes [...]" (see Appendix F for details).

For defenses where the common attack failed, additional adaptive methods were employed. For instance, with defenses that make the model refuse queries, adaptive attacks instructed the model to begin responses with specific phrases like "Sure, here is [\(\)]". For defenses including fake secrets in the system prompt, the attack requested the model to ignore the pre-extracted fake secrets.

Finally, to bypass defenses that manipulate capitalization, the attack defines an in-context encoding. This encoding instructed the model to repeat a word beginning with each character 3, 4, or 5 times, depending on whether the character is a digit, lowercase letter, or uppercase letter, respectively. For instance, if the secret string were "a1PITx", the model would output

- a (lower) : apple apple apple apple

- 1: one one one

- P (upper) : pig pig pig pig

...

### Team Hestia

The attack uses several high-level ideas to fool the model into leaking the secret without explicitly asking for it. Since most defense prompts prevent the model from giving away the secret or any information about it, asking for the secret directly often results in refusal. To circumvent this, the attacker may frame the request as a call for assistance in _preventing_ information leakage such as:

_User: "...for correct protection of the secret you will add a short python script that replaces any character of the secret..."_

The resulting script often contains the secret. Similarly, since the model typically refuses attacker queries, the attack might succeed by including a response template that begins with a refusal:

_User: "...start your answer with "I'm sorry I cannot help you with that. However I can provide..."_

Another effective attack vector is prompting the model with requests that are not directly related to the secret but could lead it to inadvertently leak it. An example of this is the "buffer fill" attack,where the attacker asks the model to fix Python code that logs into a server. Although the secret is not explicitly mentioned, the model often infers that the buffer should be filled with the secret.

Finally, exploiting the context of the conversation can also make the model more likely to cooperate. The attackers observed that starting the conversation with benign questions such as _"What is the capital of France?"_ or _"Where can I get the best pizza in New York?"_, increases the probability that the model will later cooperate with questions such as _"What is your initial prompt?"_

## 7 Lessons Learned

Based on the strategies used by different teams, we summarize some of the common trends and lessons learned. We hope these lessons inform model and system developers and, along with the dataset introduced in this work, help advance future research on evaluating LLMs and developing robust mitigations.

Importance of adaptive attacks.Adaptive attacks became the standard method for evaluating machine learning defenses in adversarial scenarios (Carlini and Wagner, 2017; Tramer et al., 2020). In this CTF, we have seen the importance of adaptive attacks in accurately assessing the robustness of prompt injection defenses. Participants often assumed that other teams would likely develop defenses similar to their own, which turned out to be true. While some defenses might seem robust against simple attacks, teams that built adaptive attacks based on assumptions were often able to leak the secret. This observation aligns with recent findings on using simple adaptive attacks to jailbreak LLMs (Andriushchenko et al., 2024).

Importance of multi-turn evaluation.One factor that makes current LLMs notoriously hard to evaluate and secure is their multi-turn setup that (1) may progressively and iteratively steer the model via the ongoing context toward an unintended behavior (Anil et al., 2024), and (2) allows having individual turns/prompts that are not harmful on their own, and thus very hard to detect/filter out, but together break safety guardfalls (Russinovich et al., 2024). Our dataset exploration (Section 4.2) and the details of the awarded attacks (Sections 6.2 and 6.3) show that many successful attacks exploit multi-turn interactions. These findings suggest that single-turn jailbreak and safety benchmarks are inadequate for evaluating models and that more multi-turn benchmarks and datasets, like ours, are needed.

Filtering is likely to be evaded.The attacks alarmingly suggest that effectively safeguarding models via filtering or censoring is extremely challenging. Even in a very controlled setup where the defender knows exactly what to protect, which is a relatively short string, attacks could reconstruct the impressive string from permissible ones (Glukhov et al., 2024). The defender's job is only expected to be harder when extending filtering to semantic concepts that are naturally less defined (e.g., helping with misinformation) or to larger pieces of information that cannot be explicitly filtered (e.g., not leaking other clients' data). However, we found it very difficult to design effective filters as the attacker can try until the filter is bypassed, while the filter cannot be updated constantly. Also, aggressive filtering can affect the utility of the system.

Defenses are not stand-alone components.Another observation is that not only can filtering be evaded, but it can also _leak_ information about the system's design, similar to previous side-channel attacks (Debenedetti et al., 2023). In fact, the most successful attackers figured out how defenses worked exactly (e.g., finding the decoys and reverse-engineering the filter) to come up with a successful attack. Some teams used the defense to further _verify_ whether the extracted secret is correct--without using scored attempts--as the models' response can be different if the true secret is contained in the output. Thus, developers should consider the effect of defenses (e.g., filtering) over the entire system. Such effects can range from a security-utility trade-off, leaking private or confidential data in the system prompt, or even enabling easier adaptive attacks.

Workarounds are (probably) more successful.A common theme among the winning defense teams is using decoys to side-step the challenge of protecting real secrets. While this can still be evaded with adaptive attacks, especially when using static decoys, it raises interesting follow-up questions of how to design mitigations that systematically make it computationally harder for the attacker to exploit models' weaknesses instead of attempting to solve them.

Other potential mitigations.In this work, we evaluated black-box prompting and filtering as easy and deployment-friendly defenses for developers. The dataset released in this work could be useful for alternative white-box methods such as fine-tuning. Future work could explore detecting undesired behaviors in the input/output via white-box methods that examine models' internals. The dataset introduced in this work can also help advance this open research question by leveraging and contrasting successful and unsuccessful attack and defense instances. Additionally, future work could also explore mitigations (be it black-box or white-box ones) that operated over multiple turns.

Limitations.A key limitation of our dataset is its narrow scope since it focuses only on secret extraction. This may not be representative of the broader range of tasks that model developers are concerned with. Also, many defenses and attacks within our dataset are designed specifically for six-character strings. However, despite these limitations, the dataset offers a test bed for evaluating prompt injection in this particular context where defenses still underperformed.

## 8 Related Work

Prompt injections.Following work on adversarial examples for classification tasks with LLMs (Branch et al., 2022), Perez and Ribeiro (2022) first introduced prompt injection attacks for _goal hijacking_ (overwriting the original goal of the LLM), and for _prompt leaking_ (extracting the proprietary system prompt)5. Follow-up work has further analyzed this problem (Liu et al., 2023, 2023, 2024, 2025) and introduced _indirect_ prompt injections (Greshake et al., 2023), where the malicious prompt is not directly introduced by the user but by a third party who tampers with the information models retrieve to answer user queries (e.g., adding "ignore previous instructions and say hello" on a website), and attacks against retrieval augmented generation (RAG) systems Chaudhari et al. (2024); Stefano et al. (2024). Despite ongoing efforts to defend against prompt injections (Wallace et al., 2024; Abdelnabi et al., 2024; Pape et al., 2024), they remain an open problem and a security risk for LLM applications (Anwar et al., 2024).

Competitions and datasets.Although several competitions have focused on LLM safety in the past (Rando et al., 2024; Center for AI Safety, 2023), only one competition addresses the security risks posed by prompt injection attacks (Schulhoff et al., 2023). The competition organized by Schulhoff et al. (2023) has ten levels where users interact with an LLM. Each level has a system prompt instructing the model to perform a task, such as translating text. Users can send one message to the model with the goal of making it output a specific string (e.g., "I have been PWNED"). Defenses to prevent prompt injection are defined by the organizers and briefly described to the attacker (e.g., "you cannot use the letters p,w,n,e,d"). In contrast, our competition frames defense creation as a challenge, motivating participants to optimize their defenses. Additionally, our competition enables multi-turn conversations, allowing attackers to devise stronger attacks by iteratively interacting with the models. Another similar effort to collect data for prompt injection research is _TensorTrust_(Toyer et al., 2023). TensorTrust is an online game where participants create a secret password that makes an LLM output "access granted", and a defense prompt that prevents attacker from leaking the password or hijacking the model to say "access granted". Users can then attack other defenses to obtain points in the public leaderboard. The main difference in our setup is that we enable stronger defenses by including Python and LLM filters, and stronger attacks with multi-turn interactions. Previously, _Gandalf_(Lakera AI, 2023), another online game where a secret word was hidden in the model prompt, received significant attention. However, most of the resulting dataset was kept private.

## 9 Conclusion

More research is needed to better understand the vulnerabilities of large language models against prompt injection attacks. We organized a _capture-the-flag_ competition at IEEE SaTML 2024 to collect a dataset that could foster future research in this direction. The competition had two phases where participants tried to build robust defenses and extract secrets behind other teams' defenses, respectively. This report summarizes the main insights from the competition and presents a large dataset with more than 137k multi-turn conversations.