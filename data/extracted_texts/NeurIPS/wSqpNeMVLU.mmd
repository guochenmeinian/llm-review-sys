# A Theoretical Perspective for Speculative Decoding Algorithm

Ming Yin

Princeton University

my0049@princeton.edu

&Minshuo Chen

Northwestern University

minshuo.chen@northwestern.edu

Kaixuan Huang

Princeton University

kaixuanh@princeton.edu

&Mengdi Wang

Princeton University

mengdiw@princeton.edu

Correspondence to: my0049@princeton.edu, mengdiw@princeton.edu.

###### Abstract

Transformer-based autoregressive sampling has been the major bottleneck for slowing down large language model inferences. One effective way to accelerate inference is _Speculative Decoding_, which employs a small model to sample a sequence of draft tokens and a large model to validate. Given its empirical effectiveness, the theoretical understanding of Speculative Decoding is falling behind. This paper tackles this gap by conceptualizing the decoding problem via markov chain abstraction and studying the key properties, _output quality and inference acceleration_, from a theoretical perspective. Our analysis covers the theoretical limits of speculative decoding, batch algorithms, and output quality-inference acceleration tradeoffs. Our results reveal the fundamental connections between different components of LLMs via total variation distances and show how they jointly affect the efficiency of decoding algorithms.

## 1 Introduction

The recent surge of scaling Transformer models has led to the flourishing of AI, where success has been witnessed in wide areas such as natural language , computer vision , video generations , and robotics . In the meantime, the decoding process also becomes more and more time-consuming as the model size scales up. This is mainly due to the autoregressive nature of Transformers, where each generated token also serves as the input for future generations. As a result, decoding \(T\) tokens would take \(T\) forward passes of the full model.

A recent effort to tackle this challenge is _speculative decoding_ (SD) , where the autoregressive sampling is performed on a small draft model and the large language model verifies tokens generated by draft model to decide whether it should be accepted/rejected. Once a token is rejected, the generation process will start from the most recently accepted token, until a full response is completed. Speculative decoding achieves 2-2.5\(\) LLM inference speedup empirically, while preserving the quality of generation.

Subsequently, numerous studies  have expanded this methodology, enabling further inference acceleration. Intuitively, for speculative decoding, when the generation distribution of small model \(p\) and large model \(q\) are close to each other, decoding is faster (since less rejection occurs), and when the distribution overlap between \(p\) and \(q\) is small, the opposite happens. However, a precise understanding of inference accelerating given the small model \(p\) and large model \(q\) remains elusive. This motivates us to ask the following question:_What is the fundamental limit for inference acceleration via speculative decoding? In addition, what is the best trade-off between inference acceleration and output quality for speculative decoding?_

In this paper, we answer these questions from the theoretical lens. Our contributions are summarized as follows.

We formalize the decoding problem through the Markov Chain abstraction that establishes the theoretical setup. We draw the connection \(=\)**rejections** and use it to measure efficiency. We derive the exact formula, fully characterized by distribution \(p\) and \(q\), for the expected rejections \([N_{}]\) for Speculative Decoding (Theorem 1). This renders a theoretical reference for understanding the acceleration rate \(T/[N_{}]\).

Next, to understand whether Speculative Decoding can be further improved, we generalize it to a class of rejection-based algorithms 2 where probability \(b_{t}\) and distribution \(_{t}\) can be customized. We prove in Theorem 2 that any unbiased algorithm cannot have fewer rejections than Speculative Decoding. This indicates its optimality among the class, and having fewer rejections needs to suffer quality loss or requires extra information.

Furthermore, we consider a batch version of Speculative Decoding (Algorithm 4) that utilizes multiple draft sequences. We show our batch algorithm is unbiased. We derive the expected rejections that is fully expressed by \(p\) and \(q\) and exhibit the improvement over non-batch version in Theorem 3. We provide examples and detailed discussion to explain how our theory characterize the improvement.

In section 4, we shift from unbiased algorithms and study the tradeoff between _inference cost_ and _quality degradation_. We formulate this into an optimization model (1). Theorem 5 established a linear Pareto front that characterizes the tradeoff between inference cost and quality degradation (Figure 4). A simple experiment in Section 4.2 is also consistent with our theoretical finding.

Last but not least, our technical results involve novel analysis, for instance, the design of \(_{+},_{-}\) in the lower bound proof C and the iterative computation for \(f\) in D.1. They are the first of its kind and consist of our technical contributions. We provide a proof sketch section in Appendix A.

### Related works

**Speculative Decoding and its applications.** Speculative execution, where performing speculative work can expedite computation on parallel machines by allowing certain tasks to commence before their necessity is confirmed, can date back to [9; 15]. Recently, [10; 24] formalize this idea with rejection sampling based design for LLM Decoding and achieve multiple-time inference acceleration compared to vanilla auto-regressive decoding. There are fruitful studies [41; 26; 25; 37; 23; 46; 32; 27; 18; 4; 34; 2; 30; 43; 11; 42; 45; 29; 35; 5; 44; 40; 20] since then, and they improve speculative decoding from different angles such as online updating , multiple candidates , retrieval technique , Multimodality  or even decoding without draft models [14; 6].

**Theoretical endeavor for Speculative Decoding.** There are also works that study the theoretical properties for speculative decoding (SD). In particular,  considers speculative decoding from the optimal transport perspective and show it is optimal in the single token regime. It further extends to the \(k\) multiple draft token setting and formulates the optimal transport solution via linear programming

Figure 1: Left: Standard Auto-Regressive Decoding (Algorithm 3) v.s. Right: Speculative Decoding (Algorithm 1), where a large model is used to validate the responses of the small model.

with exponential in \(k\) computation time. An approximate sequential selection algorithm is also proposed with linear time.  further proposes the improved plan, and  extends  to the block-level optimal transport for SD.  investigates the synergy between draft length and batch size for Speculative Decoding and formulate the optimal speculation length as the root of a polynomial equation.  proposes the SpecInfer, a batch algorithm that uses small speculative models to form the token tree, and proves its output matches the distribution of the large model.  considers batch speculative decoding without replacement to avoid repeatedly sampling rejected tokens and proves it keeps the decoding quality. Nevertheless, the findings for inference acceleration of these works are mostly empirical, lacking theoretical guarantees.

```
1:Input: Set probability \(b_{t}=\{1,}{p_{t}}\}\) and the distribution \(_{t}=[q_{t}-p_{t}]_{+}\) in Algorithm 2.
2:Require:\(p_{t}():=p_{t}(|x_{1:n-1},_{n:t-1})\), \(q_{t}():=q_{t}(|x_{1:n-1},_{n:t-1})\). \( t n\).
3:for\(t=n:T\)do
4: Sample \(r\).
5:if\(r\{1,(_{t})}{p_{t}(_{t})}\}\)then
6: Accept with \(x_{n}=_{t}\). \(n n+1\).
7:else
8: Sample \(x_{n}[q_{t}-p_{t}]_{+}()\).
9:\(n n+1\). Break.
10: // Recall \([q_{t}-p_{t}]_{+}\) in Section 2.1
11:endif
12:endfor ```

**Algorithm 1** Speculative Decoding [10; 24]

## 2 Preliminaries

### Background for decoding problems

In this section, we provide the mathematical formulation for decoding problems using Markov Chains, and we explain auto-regressive models and speculative decoding algorithm based upon that.

**A Markov Chain Model for Decoding.** We denote \(x_{0}\) as the prompt.2\(x_{n}\) is the \(n\)-th token output and \(x_{1:T}\) is the trajectory output by the algorithm with \(T\) to be the fixed decoding horizon.3. Then any decoding algorithm can be characterized by a Markov Chain: state at \(t\) is described by history \(x_{0:t}\), the initial state is \(x_{0}\); the state transition \(P_{t}\) maps state \(x_{0:t}\) to state \(x_{0:t+1}\). In the context of decoding problem, the transition matrix is defined as \(P(x_{0:t+1}|x_{0:t}):=p(x_{t+1}|x_{1:t})\). In particular, we use \(p\) to denote the (conditional) distribution for the _draft model_ that resembles small speculative model, and \(q\) for the _target model_ that represents large language model. We use \([q-p]_{+}\) to denote the normalized distribution for \(\{0,q(x)-p(x)\}, x\) with \(\) being the token vocabulary. We also denote \(}_{x T}[g]:=_{x}f(x)g(x)\).

**Auto-Regressive Decoding.** Consider sampling a trajectory \(x_{1:T}\) from an auto-regressive model, where for the given \(x_{1:n-1}\), the next token \(x_{n}\) follows the conditional distribution \(q\). This decoding mechanism (Algorithm 3) is the prototype for Transformer-based LLMs (GPT-4). As mentioned in , the accuracy performance of Transformer-based LLMs has been shown to scale with model size, with larger models demonstrating improved capabilities . However, this improvement comes at the cost of higher latency during inference and increased computational requirements.

**Speculative Decoding.** Different from large models, small models are usually much faster at the inference stage. Consequently, we can use a small model to perform auto-regressive sampling and assign large model as a verifier, where the goal of the large model is to check whether the token sampled by the small model should be accepted/rejected. Concretely, this procedure can be summarized into the following three steps:

* _Draft sampling_: given the verified tokens \(x_{1:n-1}\), the draft model obtains \(K\) sequential candidate tokens \(_{n:n+K-1}\) via sampling from \(p(|x_{1:n-1},_{n:n+i})\), \(i[K-1]\);
* _Conditional score computation_: given \(_{n:n+K-1}\), computing the logits of the \(K\) tokens \(q(|x_{1:n-1},_{n:n+i})\), \(i[K-1]\)_in parallel_;
* _Token validation_: Accept the candidate token \(_{t}\) (as \(x_{n}\)) with some probability \(0 b_{t} 1\). If accepted, continue to validate the next candidate \(_{t+1}\), otherwise reject and sample \(x_{n}\) from some designed distribution \(_{t}\).

The above process repeats until \(x_{1:T}\) are generated, and the whole algorithm is summarized as the general rejection-based decoding \(2\). Specifically, _Speculative Decoding_[10; 24] designs \(b_{t}(_{t}):=\{1,(_{t})}{p_{t}(_{t})}\}\) and distribution \(_{t}():=[q_{t}-p_{t}]_{+}()\) (see Algorithm 1 and Figure 1).

### Problem Setup

Speculative Decoding has two key merits. First, it maintains **output quality**, meaning the output distribution by the algorithm is identical to the output distribution of the large model \(q\), and we term it _distribution unbiasedness_. Second, it has **fast inference** property since the auto-regressive sampling is performed on the small model and the target model only verifies. With (possibly) multiple draft tokens being accepted very round, Speculative Decoding can be much faster than direct decoding on large models, and its inference is bottlenecked by the parallel score computation \(q(|x_{1:n-1},_{n:n+i})\), \(i[T-n]\). To highlight this, we defined it as the oracle call and make an assumption based on that.

**Definition 1**.: _We define one trigger of obtaining logits for \(_{n:T}\) in Step 7 of 2 as one **Oracle call**._

**Assumption 1**.: _We assume that: (1) compared to the large model, the computational cost of the draft/small model is negligible. (2) each oracle call has runtime \(O(1)\)._

**Remark 1**.: _We assume the above only for the theoretical cleanliness. With negligible draft model, the lookhead \(K=T\) in Algorithm 2. In other words, given \(x_{1:n-1}\), instead of sampling the next \(K\) draft tokens \(_{n:n+K-1}\), we are allowed to sample until the end, i.e. \(_{n:T}\). In practice, Assumption 1(1) also holds true in many cases. One example of negligible-cost model \(c 0\) is n-gram models, and the empirical evidence in [24; 28] shows n-gram draft model speeds up inference pretty well. In addition, for summarization tasks where long sequences are likely to repeat, any draft model that reuses tokens from the context with a matching prefix, is also cost negligible. Assumption 1(2) is also a standard abstraction, since parallelization consumes (roughly) equal computation as for a single logit. It will consume more memory, but such aspect is beyond the scope of our theoretical study._

**Metric for measuring efficiency.** Desired algorithms should preserve the output quality and be efficient for decoding. To formalize, our theory aims to study the following two quantities:

* _Inference acceleration_: the ratio between the inference time of decoding large model \(q\) and the inference time of decoding the algorithm;
* _Output quality_: the distribution bias between the algorithm and the large model distribution \(q\). An algorithm maintains the output quality if it is distribution unbiased.

**Rejections, and why consider it?** A third metric for decoding is the number of draft tokens being rejected. With more draft tokens being accepted, the fewer rejections would occur. Therefore, algorithms with large number of rejections are slower than those with fewer rejections. This means _Rejections_ serves as an alternative metric for the inference speedups. Throughout the paper, we use _number of rejections_ for measuring _inference acceleration_.

To further motivate why choosing Rejections is appropriate, we draw its connection to inference runtime. For Speculative Decoding, the runtime is dominated by the number of oracle calls (Definition 1). After each rejection (Step 10 of 2 or Step 7 of 1), there is one oracle call, thus we obtain

\[==.\]

Notice the runtime for auto-regressive decoding (3) is \(T\), therefore we have the relation: **inference acceleration\(=\ T/\)**. Based on this setup, we present our main results in next sections.

Analysis on Efficiency and Optimality for Speculative Decoding

We start with the following theorem at the beginning of the section. It covers output quality, which is measured by distribution bias, and expected number of rejections for speculative decoding. Its proof is deferred to Appendix B.

**Theorem 1**.: _We have the following two results for Speculative Decoding._

_(1) We define random variables \(R_{n}\{0,1\}\) that indicates whether the \(n\)-th token is rejected (with \(1\) being rejected). Here rejection means Line 7 of Algorithm 1 is executed. Then, the total number of rejections \(N_{}=_{n=1}^{T}R_{n}\). For Speculative Decoding (here \(\) denote the TV distance):_

\[[N_{}]=_{n=1}^{T}_{x_{1:n-1} q}[(p_{n}(|x_{1:n-1}),q_{n}(|x_{1:n-1}))].\]

_(2) The output distributions of Algorithm 1 and the target model \(q\) are identical, i.e. for any output sequence \(x_{1:T}^{T}\), the joint the distributions over \(x_{1:T}\) satisfies: \(^{}(x_{1:T})=q(x_{1:T})\)._

The first part of Theorem 1, to our knowledge, is the first result that characterizes the expected rejection for speculative decoding a sequence of length \(T\) using \(p\) and \(q\). The second part of Theorem 1 shows the distribution unbiasedness for SD, which has been presented in . There are three interesting indications:

* If \([(p_{n},q_{n})(|x_{1:n-1})]=0\) for all \(n\), all tokens are accepted and the accelerate rate \(=T\);
* If \([(p_{n},q_{n})(|x_{1:n-1})]=1\) for all \(n\), all tokens are rejected and the accelerate rate \(=1\), _i.e._ all \(T\) tokens are sampled from the large model \(q\);
* In general, the accelerate rate for SD is \(T/_{n=1}^{T}[(p_{n},q_{n})(|x_{1:n-1})]\).

**Remark 2**.: _[_24_]_ _derive the expected number of token generated per run of Speculative Decoding as \(1/[(p,q)]\) for \(K=\).4 Their result equals \(T/_{n}^{T}[(p_{n},q_{n})(|x_{1:n-1})]\) when \([(p_{n},q_{n})(|x_{1:n-1})]\) is identical for all \(n\), and this is due to their assumption that the acceptance rates \(\) are i.i.d.. In contrast, our guarantee holds for the case that allows the sequential dependence between different decoding steps._

**Simulation.** We also provide a simulation of Speculative Decoding and compare it with our Theorem 1 in the left panel of Figure 2(a) with horizon \(T=50\), \(p_{n},q_{n},n=1,,50\) are nonstationary Markov Chains. The green line is the empirical average rejections among \(100 N\) runs of Algorithm 1 and the orange line the theoretical value computed via Theorem 1. From the simulation, after \(5000\) runs the empirical average rejections converge to our theoretical value \(16.41\). In this example, the acceleration rate is \(50/16.41=3.05\). The specifications of the simulation is included in Appendix F.

### Optimality of Speculative Decoding

Now we have analyzed the efficiency of speculated decoding and provided mathematical characterization of the number of expected rejections, which is depicted by the TV distance and scales linearly with the inference time. A natural next-step question to ask is: _Is there any other rejection-based algorithm 2 that can do better?_ We answer this question in the next theorem.

**Theorem 2** (Instance-dependent Rejection Lower Bound).: _For an instance \(:=(p,q)\), where \(p,q\) stand for the distributions of the draft model and the target model respectively, defining the family of algorithms as \(:=\{\,:\,\,\,\,^{}_{+}=q_{t}\,\, t \,\,()\}\). For an algorithm \(\), denote \(N_{}\) as the number of rejections. Then we have the lower bound_

\[_{}^{}_{p}[N_{}]_{n=1}^{T}_{x_{1:n-1} q}[(p_{n},q_{n})(|x_{1:n-1})].\]

**Takeaways.** Via Theorem 2, the answer to the key question is: _No rejection improvement can be made in Algorithm 2 by changing the acceptance probability \(b_{t}\) and the distribution \(_{t}\) if we want to keep the distribution unbiasedness._ We point out that lower bound of Theorem 2 matches the complexity upper bound of Speculative Decoding given by Theorem 1. This result confirms that speculative decoding is optimal in the class of all rejection-based methods.

The practical implication is that there is no need to tweak acceptance probability, as it will not make performance better. In the next Section 3.2, we will see that Speculative Decoding can be provably improved, as long as one can decode and verify multiple sequence of tokens in parallel.

**Connection to optimal transport.** We mention  nicely consider maximizing the acceptance probability from the _optimal transport_ perspective. For a single token, the optimal transport cost is \(_{x}(p(x),q(x))\), which corresponds to the optimal expected rejection \((p,q)\). However, for the sequential \(T\) tokens, their Section 5,6 does not provide a explicit formulation for optimal acceptance/rejections. In this sense, their optimality result can be cast as a special case of ours. However, we do emphasis there are differences in the settings, where  consider the optimal transport and we study the class \(\).

### Analysis for Batch Speculative Decoding

To further improve the provable efficiency, we consider batch algorithms that extend the speculative decoding with multiple candidate draft sequences. Most of the existing works  formulates batch sequences via a speculation tree structure. In particular, the representative work  propose the merged token tree structure that combines sequences with the same prefix. A _depth-first search_ is designed for speculation to ensure the unbiasedness of the algorithm. In addition,  devise the parallel decoding structure that speculate the token of each responses given its previous tokens are accepted. Motivated by these works, we consider a batch version of speculative decoding algorithm using a simpler parallel structure (Left of Figure 3). Our Algorithm 4 can be viewed as a simplified approximation to those batch algorithms. There are several differences that distinguish batch algorithm from the non-batch version. We highlighting them as follows.

**Difference1: Oracle call.** At the beginning of batch speculation, \(M\) draft sequences are generated in parallel as well as the corresponding logits \(q\). This corresponds to Step 4-9 of Alg 4 and is defined as one _oracle call_ which we assume to have unit computation \(O(1)\):5

**Difference2: Speculation procedure.** It follows the _DFS_ principle: If the first token of a response is accepted, only that response will be speculated until rejection. For instance in the Left panel of Figure 3, if the token 'deep' is accepted, the algorithm will keep verifying token 'learn', 'ing' until rejection and the rest of sequences won't be examined; if 'deep' is not verified, then the algorithm will keep examing'rein'. In this case, rejection happens only if 'deep','rein' and 'atten' are all

Figure 2: The numeric instance in this figure chooses \(p,q\) to be nonstationary Markov Chains with horizon \(T=50\). Left \((a)\): A simulation of Speculative Decoding. The green line is the empirical average rejections among \(100N\) runs and the orange line the theoretical value computed via Theorem 1. Middle \((b)\): Batch Speculative Decoding simulations with batch \(M=4,5\). The green/purple lines are the empirical average rejections among \(100N\) runs and the orange/pink lines are the theoretical values computed via Theorem 3. Right \((c)\): The scaling law of expected rejections for Batch SD as a function of \(M\). It converges to a limit as \(M\).

rejected. Once rejection happens, the process will restart. By this design, it still holds true that: _inference time \(=\) # oracle calls \(=\) # rejections_.

Again, we measure the output quality and rejections for the batch algorithm. The following main result (Theorem 3) provides the explicit characterization for rejections and batch improvement. Detailed proofs and discussions can be found in D.

**Theorem 3** (Unbiasedness and efficiency of batch SD).: _Recall the definition of \(R_{n}\) and \(N_{}\) in Thm 1, and iteratively define: \(q^{m+1}=[q^{m}-p]_{+}\), \( m[M]\) with \(q^{1}=q\) being the target distribution. Then, for Batch Speculative Decoding 4, \(^{}(x_{1:T})=q(x_{1:T})\), \( x_{1:T}^{T}\). Moreover,_

\[[N_{}]=_{n=1}^{T}_{x_{1:n-1} q}[ [q,p](|x_{1:n-1})]-^{T}_{x_{ 1:n-1} f}[(q,p)(x_{1:n-1})-[_{m=1}^{M}(q^{m},p) (x_{1:n-1})]]}_{}\]

_where \(f(x_{1:n}):=(x_{1:n}\{n\})\). \(f\) can be iteratively computed via \(p,q\)._

**Takeaways.** The expected rejection of Batch Decoding composes of two parts: _(i)_\(_{n}^{T}_{q}[[q,p]]\) which is the rejection that matches the non-batch speculative decoding; _(ii)_ the batch improvement (BI) which comes from our design and is always non-negative. To better understand how the batch improvement scale with \(M\), we instantiate the single token \((q,p):=[q,p]-_{m=1}^{M}[q^{m},p]\) with two simple examples.

**Uniform Distribution.** For the whole space \(\), let \(p\) be a uniform distribution with support \(\) and \(q\) be a uniform distribution over a subset of \(\) with size \(V^{}<V\). Let \(V/V^{}=r\), in this case \(((V^{}),(V))=(1-)-(1- {r})^{M},\ r=V/V^{}\). Its pattern is visualized in the lower right panel of Figure 3. The improvement converges to \(1-1/r\) and is always positive as long as batch size is at least \(2\). Also, when the vocabulary size \(V\) scales up, _i.e._\(r\) goes up, the improvement is going to zero. This is not surprising, since the probability of draft sample to fall within in the support of target distribution is very small.

**Bernoulli Distribution.** Suppose \(u v\) and let \(p(u)\), \(q(v)\). Then \(((v),(u))=|u-v|(1-u^{M-1})\). Its pattern is exhibited in the upper right panel of Figure 3. Notice for both cases, the limiting batch improvement is \((p,q)\), which is only significant when \(p\) deviates from \(q\). This provides the practical design heuristic: _batch design can significantly reduce the number of rejections when the draft distribution \(p\) and target distribution \(q\) are different from each other and won't make too much improvement when \(p\) and \(q\) are close._

**Batch Simulation.** The Middle panel of Figure 2(b) shows Batch Speculative Decoding simulations with batch \(M=4,5\). The green/purple lines are the empirical average rejections simulated via Algorithm 4 and the orange/pink lines are the theoretical values computed via Theorem 3. The right panel of Figure 2(c) plots the rejection vs. batch size. In particular, the the black dot with coordinate \((1,16.41)\) represents the Speculative Decoding 1. The numeric example shows when \(M\), \([N_{}] 0\). Intuitively, this is partial due to, if the distribution mismatch between \(p\) and \(q\) is large,

Figure 3: Left: Batch Speculative Decoding. Right: Batch Improvement vs. Batch size \(M\). Upper: Bernoulli distributions with \(q=Ber(0.5)\). Lower: \(p(V)\), \(q(V^{})\) with \(r=V/V^{}\).

rejection is unavoidable no matter how many draft responses are sampled from \(p\). We also formally prove this in Proposition 1. This theoretical discovery indicates that having a very large draft batches does not necessarily result in significant inference speedups compared to small batches.

## 4 Analysis on the Optimal Rejection-Distribution Bias Tradeoff

In earlier sections, we focus on analyzing SD or Batch SD which are distribution unbiased. To further reduce rejections, the algorithm may have to compromise on output quality. Nevertheless, it is usually acceptable for many real-world applications. For instance, for the family of Gemini models , Google may deploy the small model _Gemini Nano_ as the draft model \(p\) and _Gemini Ultra_ as the target model \(q\) and tune the acceptance probability \(b\) (in Algorithm 2) higher such that _Gemini Nano_ is applied more often for the purpose of less expenditure. Therefore, an intriguing question to ask is: _For biased algorithms, what is the optimal tradeoff between rejection and distribution bias?_

### An Optimization Formulation and Pareto-optimal Characterization

We measure the distribution bias between \(^{}\) and \(q\) via \(\) distance.6 Concretely, for any algorithm \(:=(b,)\) in 2 with acceptance probability \(b\) and rejection distribution \(\), we fix \(b\) and aim to optimize the following objective

\[^{}_{}(b):=_{}[ ^{},q], where\ :=(b,).\] (1)

We wish to solve the above since it characterizes the minimal distribution bias for any design \(b\). We present our solution in the next.

**Theorem 4** (Optimal solution to optimization (1)).: _We can show the objective (1) is equivalent to_

\[^{}_{}(b):=_{}\ _{x}|q(x)-b(x)p(x)-(x)_{}[1-b()]p()|\ s.t.\ _{x}(x)=1,\ (x) 0,\  x.\]

_Suppose \(_{x}[1-b(x)]p(x)>0\). Define \(A(x):=}[1-b()]p()},\) and the sets \(A_{+}=\{x:A(x) 0\}\), \(A_{-}=\{x:A(x)<0\}\). Then the set of optimal distributions of objective (1) is characterized as \(\{^{*}:^{*}|_{A_{-}}()=0;0^{*}|_{A_{ +}}() A()\},\) and the optimal value is \(^{}_{}(b)=_{x}|q(x)-b(x)p(x)|- _{x}(1-b(x))p(x) 0.\)_

Theorem 4 is a universal characterization that contains both biased and unbiased situations. 1. If \(b\) is less than Speculative Decoding threshold, _i.e._\(b\{1,q/p\}\) then \(A\) becomes a probability distribution and the optimal distribution \(^{*}\) equals \(A\) which is also unbiased (\(^{}_{}(b)=0\)); 2. If \(b\) exceeds the Speculative Decoding threshold, then \(A\) is no longer a distribution and there are multiple optimal distributions \(^{*}\). In this case, the optimal distribution bias \(^{}_{}(b)>0\) for Algorithm 2.

Figure 4: Left \((a)\): The Pareto Front between _Rejection Probability_\(^{}()\) vs. _Distribution bias_\([^{},q]\). For a given rejection probability, the black line denotes the optimal deviation \(^{}_{}\). Middle \((b)\) and Right \((c)\): A numeric example. In the plot, the over acceptance \(\)â€™s are set as positive constants that define \(b(x)=\{1,\}\).

**Main Takeaways.** Via Theorem 4, we derive the pareto front (the optimal tradeoff) between rejection probability7\(()\) vs. distribution distance \([^{},q]\) (Left panel of Figure 4). The blue region can be realized by some algorithm 2, and the red region cannot. \((0,0)\) is the "perfect algorithm" (no rejection, no bias) which does not exists, and, in particular, \((0,[p,q])\) stands for Speculative Decoding. Surprisingly, the pareto front is a straight line connecting \((0,[p,q])\) and \(([p,q],0)\), which represents a linear relationship between the rejection probability and the optimal \(\) deviation. This is guaranteed by the following theorem.

**Theorem 5** (Pareto front).: _For any Algorithm \(\) in the class 2 that satisfies \(\{1,\} b(x) 1,\  x\). Then_

\[^{}()+^{*}_{}(b)= [p,q].\]

_Here \(^{}()=1-_{x}b(x)p(x)\) and \(^{*}_{}(b):=_{}[^{ },q]\)._

We plot the numerical example using two Markov Chains in the middle and right panel of Figure 4 that coincides with our theoretical finding. In the figure, the acceptance probability is set to be \(b(x)=\{1,\}\). The orange line in \((c)\) and the green boundary in \((b)\) are computed via \(^{*}_{}(b)\) from Theorem 4. The complete proofs for Theorem 5 and Theorem 4 are deferred to Appendix E. For the clearness of illustration, we focus on the pareto front between rejection vs. the minimal \(\) deviation for a single token.

### Experiment

We provide an additional simple experiment to show the effectiveness of our Pareto-optimal solution in Theorem 4. Consider objective (1). For the given acceptance probability \(b>\{1,q/p\}\), we have two options for \(\): 1. Baseline: select \(\) to be suboptimal to (1), _i.e._ simply set \(:=q\), which is the target distribution; 2. Set \(:=^{*}\) be the optimal distribution of Theorem 4. We call the first method Decoding-UNO and the latter one Decoding-OPT. Instead of TV distance, we measure the quality via WinRate in our experiment. Concretely, for each prompt, we let Decoding-UNO and Decoding-OPT to generate responses independently, and use score models to compare whose generation has higher quality. We specify draft model \(p\) as pythia-70m and target model \(q\) as pythia-2.8b from EleutherAI . We apply the score model to be RM-Mistral-7B or GPT-4. We test 200 prompts from Alpaca-Farm-EvaI Dataset  with 500 responses/comparisons per prompt. Table 1 shows that Decoding-OPT achieves better performance than Decoding-UNO across different choice of \(\)'s. Due to space constraint, the missing details are deflected to Appendix G.

## 5 Discussions

**On the optimality for batch algorithms.** Unlike their non-batch counterparts, our batch algorithm studies do not come with optimality guarantees. This is largely due to the diverse and arbitrary nature of batch algorithm designs, making it challenging to define a comprehensive class that encompasses a wide range of batch algorithms. While  investigate optimal batch algorithms through an optimal transport lens, their work does not extend to calculating optimal rejection rates or developing an efficient algorithm to achieve this (they only propose an approximate solution). Consequently, the pursuit of batch optimality remains an open field. Identifying the optimal batch algorithm could yield valuable insights for enhancing practical applications in real-world scenarios.

    &  &  \\   & \(=0.1\) & \(=0.2\) & \(=0.4\) & \(=0.8\) & \(=0.1\) & \(=0.2\) & \(=0.4\) & \(=0.8\) \\  Decoding-OPT & 53\% & 53.5\% & 57.5\% & 52.5\% & 54.5\% & 53\% & 53.5\% & 55\% \\ Decoding-UNO & 47\% & 46.5\% & 42.5\% & 47.5\% & 45.5\% & 47\% & 46.5\% & 45\% \\   

Table 1: WinRate for Decoding-OPT vs Decoding-UNO with different over-acceptance threshold \(\). The acceptance probability \(b(x)=\{1,\}\).

**Extending Speculative Decoding to other studies.** Speculative Decoding is a generic sampling approach that extends beyond mere decoding tasks. It holds potential for wider applications such as search engines and recommendation systems, where it can be employed to quickly generate and refine search outcomes or content suggestions, enhancing the overall efficiency and user experience of these systems. We leave these as future works.