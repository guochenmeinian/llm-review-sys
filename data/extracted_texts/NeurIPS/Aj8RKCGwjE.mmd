# AROMA: Preserving Spatial Structure for Latent PDE Modeling with Local Neural Fields

Louis Serrano\({}^{1}\)  Thomas X Wang\({}^{1}\)  Etienne Le Naour\({}^{1,2}\)

Jean-Noel Vittaut\({}^{3}\)  Patrick Gallinari\({}^{1,4}\)

\({}^{1}\)Sorbonne Universite, CNRS, ISIR, 75005 Paris, France

\({}^{2}\)EDF R&D, Palaiseau, France

\({}^{3}\)Sorbonne Universite, CNRS, LIP6, 75005 Paris, France

\({}^{4}\)Criteo AI Lab, Paris, France

Corresponding author: louis.serrano@isir.upmc.fr

###### Abstract

We present AROMA (Attentive Reduced Order Model with Attention), a framework designed to enhance the modeling of partial differential equations (PDEs) using local neural fields. Our flexible encoder-decoder architecture can obtain smooth latent representations of spatial physical fields from a variety of data types, including irregular-grid inputs and point clouds. This versatility eliminates the need for patching and allows efficient processing of diverse geometries. The sequential nature of our latent representation can be interpreted spatially and permits the use of a conditional transformer for modeling the temporal dynamics of PDEs. By employing a diffusion-based formulation, we achieve greater stability and enable longer rollouts compared to conventional MSE training. AROMA's superior performance in simulating 1D and 2D equations underscores the efficacy of our approach in capturing complex dynamical behaviors. _Github page_: https://github.com/LouisSerrano/aroma

## 1 Introduction

In recent years, many deep learning (DL) surrogate models have been introduced to approximate solutions to partial differential equations (PDEs) (Lu et al., 2021; Li et al., 2021; Brandstetter et al., 2022; Stachenfeld et al., 2022). Among these, the family of neural operators has been extensively adopted and tested across various scientific domains, demonstrating the potential of data-centric DL models in science (Pathak et al., 2022; Vinuesa & Brunton, 2022).

Neural Operators were initially constrained by discretization and domain geometry limitations. Recent advancements, such as neural fields (Yin et al., 2022; Serrano et al., 2023) and transformer architectures (Li et al., 2023; Hao et al., 2023), have partially addressed these issues, improving both dynamic modeling and steady-state settings. However, Neural Fields struggle to model spatial information and local dynamics effectively, and existing transformer architectures, while being flexible, are computationally expensive due to their operation in the original physical space and require large training datasets.

Our hypothesis is that considering spatiality is essential in modeling spatio-temporal phenomena, yet applying attention mechanisms directly is computationally expensive. We propose a new framework that models the dynamics in a reduced latent space, encoding spatial information compactly, by one or two orders of magnitude relative to the original space. This approach addresses both the complexity issues of transformer architectures and the spatiality challenges of Neural Fields.

Our novel framework leverages attention blocks and neural fields, resulting in a model that is easy to train and achieves state-of-the-art results on most datasets, particularly for complex geometries, without requiring prior feature engineering. To the best of our knowledge, we are the first to propose a fully attention-based architecture for processing domain geometries and unrolling dynamics. Compared to existing transformer architectures for PDEs, our framework first encapsulates the domain geometry and observation values in a compact latent representation, efficiently forecasting the dynamics at a lower computational cost. Transformer-based methods such as (Li et al., 2023; Hao et al., 2023) unroll the dynamics in the original space, leading to high complexity.

Our contributions are summarized as follows:

* A principled and versatile encode-process-decode framework for solving PDEs that operate on general input geometries, including point sets, grids, or meshes, and can be queried at any location within the spatial domain.
* A new spatial encode / process / decode approach: Variable-size inputs are mapped onto a fixed-size compact latent token space that encodes local spatial information. This latent representation is further processed by a transformer architecture that models the dynamics while exploiting spatial relations both at the local token level and globally across tokens. The decoding exploits a conditional neural field, allowing us to query forecast values at any point in the spatial domain of the equation.
* We include stochastic components at the encoding and processing levels to enhance stability and forecasting accuracy.
* Experiments performed on representative spatio-temporal forecasting problems demonstrate that AROMA is on par with or outperforms state-of-the-art baselines in terms of both accuracy and complexity.

## 2 Problem setting

In this paper, we focus on time-dependent PDEs defined over a spatial domain \(\) (with boundary \(\)) and temporal domain \([0,T]\). In the general form, their solutions \((x,t)\) satisfy the following constraints :

\[}{ t} =F(,t,x,,}{ x},}{ x^{2}},), x,  t(0,T]\] (1) \[()(t,x) =0 x, t(0,T]\] (2) \[(0,x) =^{0} x\] (3)

where \(\) represents a set of PDE coefficients, Equations (2) and (3) represent the constraints with respect to the boundary and initial conditions. We aim to learn, using solutions data obtained with classical solvers, the evolution operator \(\) that predicts the state of the system at the next time step: \(^{t+ t}=(^{t})\). We have access to training trajectories obtained with different initial conditions, and we want to generate accurate trajectory rollouts for new initial conditions at test time. A rollout is obtained by the iterative application of the evolution operator \(^{m t}=^{m}(^{0})\).

## 3 Model Description

### Model overview

We provide below an overview of the global framework and each component is described in a subsequent section. The model comprises three key components, as detailed in Figure 1.

* **Encoder**\(_{w}:^{t}_{}^{t}\). The encoder takes input values \(^{t}_{}\) sampled over the domain \(\) at time \(t\), where \(\) denotes the discrete sample space and could be a grid, an irregular mesh or a point set. \(^{t}_{}\) is observed at locations \(\) = (\(x_{1}\),...\(x_{N}\)), with values \(^{t}=(^{t}(_{1}),,^{t}(_{N}))\). \(N\) is the number of observations and can vary across samples. \(^{t}_{}\) is projected through a cross attention mechanism onto a set of \(M\) tokens \(^{t}=(^{t}_{1},,^{t}_{M})\) with \(M\) a fixed parameter. This allowsmapping any discretized input \(_{}^{t}\) onto a fixed dimensional latent representation \(^{t}\) encoding implicit local spatial information from the input domain. The encoder is trained as a VAE and \(^{t}\) is sampled from a multivariate normal statistics as detailed in Section 3.2.
* **Latent time-marching refiner \(_{}:^{t}}^{t+ t}\).** We model the dynamics in the latent space through a transformer. The dynamics can be unrolled auto-regressively in the latent space for any time horizon without requiring to project back in the original domain \(\). Self-attention operates on the latent tokens, which allows modeling global spatial relations between the local token representations. The transformer is enriched with a conditional diffusion mechanism operating between two successive time steps of the transformer. We experimentally observed that this probabilistic model was more robust than a baseline deterministic transformer for temporal extrapolation.
* **Decoder \(_{}:}^{t+ t}}^{t+ t}\).** The decoder uses the latent tokens \(}^{t+ t}\) to approximate the function value \(}^{t+ t}(x)=_{}(x,}^{t+ t})\) for any query coordinate \(x\). We therefore denote \(}^{t+ t}=_{}(^{t+ t})\) the predicted function.

InferenceWe encode the initial condition and unroll the dynamics in the latent space by successive denoisings: \(}^{m t}=_{}_{}^{m} _{w}(^{0})\). We then decode along the trajectory to get the reconstructions. We outline the full inference pipeline in Figure 1 and detail its complexity analysis in Appendix C.1.

TrainingWe perform a two-stage training: we first train the encoder and decoder, secondly train the refiner. This is more stable than end-to-end training.

### Encoder-decoder description

The encoder-decoder components are jointly trained using a VAE setting. The encoder is specifically designed to capture local input observation from any sampled point set in the spatial domain and encodes this information into a fixed number of tokens. The decoder can be queried at any position in the spatial domain, irrespective of the input sample.

EncoderThe encoder maps an arbitrary number \(N\) of observations \((,()):=((x_{1},(x_{1})),,(x_{N},(x_{N}))\) onto a latent representation \(\) of fixed size \(M\) through the following series of transformations:

Figure 1: **AROMA inference**: The discretization-free encoder compresses the information of a set of \(N\) input values to a sequence of \(M\) latent tokens, where \(M<N\). The conditional diffusion transformer is used to model the dynamics, acting as a latent refiner. The continuous decoder leverages self-attentions (SA), cross-attention (CA) and a local INR to map back to the physical space. Learnable tokens are shared and encode spatial relations. Latent token \(Z^{t}\) represents \(u_{t}\) and \(Z^{t+ t}\) is the prediction corresponding to \(u_{t+ t}\).

where \(((),())=(((x_{1}),v(x_{1})),,((x_{N }),v(x_{N})))\), and \(h d\).

**(i) Embed positions and observations**: Given an input sequence of coordinate-value pairs \((x_{1},(x_{1})),,(x_{N},(x_{N}))\), we construct sequences of positional embeddings \(=((x_{1}),,(x_{N}))\) and value embeddings \(=(v(x_{1}),,v(x_{N}))\), where \((x)=(x;)\) and \(v(x)=((x))\), with \(\) a fixed set of frequencies. These embeddings are aggregated onto a smaller set of learnable query tokens \(=(T_{1},,T_{M})\) and then \(^{}=(T_{1}^{},,T_{M}^{})\) with \(M\) fixed, to compress the information and encode the geometry and spatial latent representations.

**(ii) Encode geometry**: Geometry-aware tokens \(\) are obtained with a multihead cross-attention layer and a feedforward network (FFN), expressed as \(^{}=+(( =_{Q},=_{K}, =_{V}))\). This step does not include information on the observations, ensuring that similar geometries yield similar query tokens \(^{}\) irrespective of the \(\) values.

**(iii) Encode observations**: The \(^{}\) tokens are then used to aggregate the observation values via a cross-attention mechanism: \(^{}=^{}+(( =_{Q}^{}^{},=_{K}^{},=_{V}^{}))\). Here, the values contain information on the observation values, and the keys contain information on the observation locations.

**(iv) Reduce channel dimension and sample \(\)**: The information in the channel dimension of \(^{}\) is compressed using a bottleneck linear layer. To avoid exploding variance in this compressed latent space, we regularize it with a penalty on the \(L_{2}\) norm of the latent code \(\|\|^{2}\). Introducing stochasticity through a variational formulation further helps to regularize the auto-encoding and obtain smoother representations for the forecasting step. For this, we learn the components of a Gaussian multivariate distribution \(=(^{})\) and \(()=(^{})\) from which the final token embedding \(\) is sampled.

DecoderThe decoder's role is to reconstruct \(}^{t+ t}\) from \(}^{t+ t}\), see Figure 1. Since training is performed in two steps ("encode-decode" first and then "process"), the decoder is trained to reconstruct \(}^{t}\) for input \(^{t}\). One proceeds as follows. **(i) Increase channel dimensions and apply self-attention**: The decoder first lifts the latent tokens \(\) to a higher channel dimension (this is the reverse operation of the one performed by the encoder) and then apply several layers of self-attention to get tokens \(^{{}^{}}\). **(ii) Cross-attend**: The decoder applies cross-attention to obtain feature vectors that depend on the query coordinate \(x\), \((_{q}^{u}(x))=(=_{Q}( _{q}(x)),=_{K}^{{}^{}},= _{V}^{{}^{}})\), where \(_{q}\) is a Fourier features embedding of bandwidth \(_{q}\). **(iii) Decode with MLP**: Finally, we use a small MLP to decode this feature vector and obtain the reconstruction \(}(x)=(_{q}^{u}(x))\). In contrast with existing neural field methods for dynamics modeling, the feature vector here is local. In practice, one uses multiple cross attentions to get feature vectors with different frequencies (see Appendix Figures 7 and 8 for further details).

TrainingThe encoder and decoder are jointly optimized as a variational autoencoder (VAE) (Kingma & Welling, 2013) to minimize the following objective : \(=_{}+_{KL}\); where \(_{}=(u_{}^{t},_{} ^{t})\) is the reconstruction loss between the input and the reconstruction \(_{}(^{t},)\) on the grid \(\), with \(^{t}(^{t},(^{t})^{2})\) and \(^{t},^{t}=_{w}(u_{}^{t})\). The KL divergence loss \(_{}=D_{}((^{t},(^{t}) ^{2})\,||(0,I))\) helps regularize the network and prevents overfitting. We found that using a variational formulation was essential to obtain smooth latent representations while training the encoder-decoder.

### Transformer-based diffusion

Modeling the dynamics is performed in the latent \(\) space. This space encodes spatial information present in the original space while being a condensed, smaller-sized representation, allowing for reduced complexity dynamics modeling. As indicated, the dynamics can be unrolled auto-regressively in this space for any time horizon without the need to map back to the original space. We use absolute positional embeddings \(E_{}\) and a linear layer to project onto a higher dimensional space: \(_{}=()+E_{}\). The backbone then applies several self-attention blocks, which process tokens as follows:

\[_{[l+1]}_{[l]}+((_{[l]}))\] (4) \[_{[l+1]}_{[l+1]}+((_{[l+1]})\] (5)

We found out that adding a diffusion component to the transformer helped enhance the stability and allowed longer forecasts. Diffusion steps are inserted between two time steps \(t\) and \(t+ t\) of the time-marching process transformer. The diffusion steps are denoted by \(k\) and are different from the ones of the time-marching process (several diffusion steps \(k\) are performed between two time-marching steps \(t\) and \(t+ t\)).

We then use a conditional diffusion transformer architecture close to Peebles and Xie (2023) for \(_{}\), where we detail the main block in Appendix B. At diffusion step \(k\), the input to the network is a sequence stacking the tokens at time \(t\) and the current noisy targets estimate \((^{t},}_{k}^{t+ t})\). See Appendix B, Figure 4 and Figure 5 for more details. To train the diffusion transformer \(_{}\), we freeze the encoder and decoder, and use the encoder to sample pairs of successive latent tokens \((^{t},^{t+ t})\). We employ the "v-predict" formulation of DDPM (Salimans and Ho, 2022) for training and sampling.

## 4 Experiments

In this section, we systematically evaluate the performance of our proposed model across various experimental settings, focusing on its ability to handle dynamics on both regular and irregular grids. First, we investigate the dynamics on regular grids, where we benchmark our model against state-of-the-art neural operators, including Fourier Neural Operators (FNO), ResNet, Neural Fields, and Transformers. This comparison highlights the efficacy of our approach in capturing complex spatio-temporal patterns on structured domains. Second, we extend our analysis to dynamics on irregular grids and shared geometries, emphasizing the model's extrapolation capabilities in data-constrained regimes. Here, we compare our results with Neural Fields and Transformers, demonstrating the robustness of our model in handling less structured and more complex spatial configurations. Lastly, we assess the model's capacity to process diverse geometries and underlying spatial representations by comparing its performance on irregular grids and different geometries. This evaluation highlights the flexibility and generalization ability of our model in encoding and learning from varied spatial domains, showcasing its potential in accurately representing and predicting dynamics across a wide range of geometric settings. We include additional results from ablation studies in Appendix C.6.

Figure 2: Spatial interpretation of the tokens through cross attention between \(T^{geo}\) and \((x)\) for each \(x\) in the domain. Here we visualize the cross-attention of three different tokens for a given head. The cross attentions can have varying receptive fields depending on the geometries.

### Dynamics on regular grids

We begin our analysis with dynamics modeling on regular grid settings. Though our model is targeted for complex geometries, we believe this scenario remains an important benchmark to assess the efficiency of surrogate models.

Datasets\(\)**1D Burgers' Equation** (_Burgers_): Models shock waves, using a dataset with periodic initial conditions and forcing term as in Brandstetter et al. (2022). It includes 2048 training and 128 test trajectories, at resolutions of \((250,100)\). We create sub-trajectories of 50 timestamps and treat them independently. \(\)**2D Navier Stokes Equation**: for a viscous and incompressible fluid. We use the data from Li et al. (2021). The equation is expressed with the vorticity form on the unit torus: \(+u w= w+f\), \( u=0\) for \(x,t>0\), where \(\) is the viscosity coefficient. We consider two different versions \(=10^{-4}\) (_Navier-Stokes_\(1 10^{-4}\)) and \(=10^{-5}\) (_Navier-Stokes_\(1 10^{-5}\)), and use train and test sets of \(1000\) and \(200\) trajectories with a base spatial resolution of size \(64 64\). We consider a horizon of \(T=30\) for \(=10^{-4}\) and \(T=20\) for \(=10^{-5}\) since the phenomenon is more turbulent. At test time, we use the vorticity at \(t_{0}=10\) as the initial condition.

SettingWe train all the models with supervision on the next state prediction to learn to approximate the time-stepping operator \(^{t+ t}=(^{t})\). At test time, we unroll the dynamics auto-regressively with each model and evaluate the prediction with a relative \(L_{2}\) error defined as \(L_{2}^{}=}}_{j}^{}_{j}-^{}_{j}||_{2}}{||^ {}_{j}||_{2}}\).

BaselinesWe use a diverse panel of baselines including state of the art regular-grid methods such as FNO (Li et al., 2021) and ResNet (He et al., 2016; Lippe et al., 2023), flexible transformer architectures such as OFormer (Li et al., 2023), and GNOT (Hao et al., 2023), and finally neural-field based methods with DINO (Yin et al., 2022) and CORAL (Serrano et al., 2023).

ResultsTable 1 presents a comparison of model performance on the _Burgers_, _Navier-Stokes1e-4_, and _Navier-Stokes1e-5_ datasets, with metrics reported in Relative \(L_{2}\). Our method, AROMA, demonstrates excellent performance across the board, highlighting its ability to capture the dynamics of turbulent phenomena, as reflected in the _Navier-Stokes_ datasets.

In contrast, DINO and CORAL, both global neural field models, perform poorly in capturing turbulent phenomena, exhibiting significantly higher errors compared to other models. This indicates their limitations in handling complex fluid dynamics. On the other hand, AROMA outperforms GNOT on all datasets, though it performs reasonably well compared to the neural field based method.

Regarding the regular-grid methods, ResNet shows suboptimal performance in the pure teacher forcing setting, rapidly accumulating errors over time during inference. FNO stands out as the best baseline, demonstrating competitive performance on all datasets. We hypothesize that FNO's robustness to error accumulation during the rollout can be attributed to its Fourier block, which effectively cuts off high-frequency components. Overall, the results underscore AROMA's effectiveness and highlight the challenges Neural Field-based models face in accurately modeling complex phenomena.

### Dynamics on irregular grids with shared geometries

We continue our experimental analysis with dynamics on unstructured grids, where we observe trajectories only through sparse spatial observations over time. We adopt a data-constrained regime and show that our model can still be competitive with existing Neural Fields in this scenario.

DatasetsTo evaluate our framework, we utilize two fluid dynamics datasets commonly used as a benchmark for this task (Yin et al., 2022; Serrano et al., 2023) with unique initial conditions for each trajectory: \(\)**2D Navier-Stokes Equation** (_Navier-Stokes_\(1 10^{-3}\)): We use the same equation as

   Model & _Burgers_ & _Navier-Stokes_ & _Navier-Stokes_ \\  & & \(1 10^{-4}\) & \(1 10^{-5}\) \\  FNO & \(5.00 10^{-2}\) & \(1.53 10^{-1}\) & \(}\) \\ ResNet & \(8.50 10^{-2}\) & \(3.77 10^{-1}\) & \(2.56 10^{-1}\) \\  DINO & \(4.57 10^{-1}\) & \(7.25 10^{-1}\) & \(3.72 10^{-1}\) \\ CORAL & \(6.20 10^{-2}\) & \(3.77 10^{-1}\) & \(3.11 10^{-1}\) \\  GNOT & \(1.28 10^{-1}\) & \(1.85 10^{-1}\) & \(1.65 10^{-1}\) \\ OFormer & \(4.92 10^{-2}\) & \(1.36 10^{-1}\) & \(2.40 10^{-1}\) \\  AROMA & \(}\) & \(}\) & \(}\) \\   

Table 1: **Model Performance Comparison** - Test results. Metrics in Relative \(L_{2}\).

in Section 4.1 but with a higher viscosity coefficient \(=1e-3\). We have 256 trajectories of size 40 for training and 32 for testing. We used a standard resolution of 64x64. \(\)**3D Shallow-Water Equation** (_Shallow-Water_): This equation approximates fluid flow on the Earth's surface. The data includes the vorticity \(w\) and height \(h\) of the fluid. The training set comprises 64 trajectories of size 40, and the test set comprises 8 trajectories with 40 timestamps. We use a standard spatial resolution of \(64 128\).

**Setting \(\) Temporal Extrapolation**: For both datasets, we split trajectories into two equal parts of 20 timestamps each. The first half is denoted as _In-t_ and the second half as _Out-t_. The training set consists of _In-t_. During training, we supervise with the next state only. During testing, the model unrolls the dynamics from a new initial condition (IC) up to the end of _Out-t_, i.e. for 39 steps. Evaluation within the _In-t_ horizon assesses the model's ability to forecast within the training regime. The _Out-t_ evaluation tests the model's extrapolation capabilities beyond the training horizon. \(\)**Sparse observations**: For the train and test set we randomly select \(\) percent of the available regular mesh to create a unique grid for each trajectory, both in the train and in the test. The grid is kept fixed along a given trajectory. While each grid is different, they maintain the same level of sparsity across trajectories. In our case, \(=100\%\) amounts to the fully observable case, while in \(=25\%\) each grid contains around 1020 points for _Navier-Stokes_\(1 10^{-3}\) and 2040 points for _Shallow-Water_.

BaselinesWe compare our model to OFormer (Li et al., 2023), GNOT (Hao et al., 2023), and choose DINO (Yin et al., 2022) and CORAL (Serrano et al., 2023) as the neural field baselines.

Training and evaluationDuring training, we only use the data from the training horizon (_In-t_). At test time, we evaluate the models to unroll the dynamics for new initial conditions in the training horizon (_In-t_) and for temporal extrapolation (_Out-t_).

ResultsTable 2 demonstrates that AROMA consistently achieves low MSE across all levels of observation sparsity and evaluation horizons for both datasets. Overall, our method performs best with some exceptions. On _Shallow-Water_ our model is slightly outperformed by CORAL in the fully observed regime, potentially because of a lack of data. Similarly, on _Navier-Stokes_\(1 10^{-3}\) CORAL has slightly better scores in the very sparse regime \(=5\%\). Overall, this is not surprising as meta-learning models excel in data-constrained regimes. We believe our geometry-encoding block is crucial for obtaining good representations of the observed values in the sparse regimes, potentially explaining the performance gap with GNOT and OFormer.

### Dynamics on different geometries

Finally, we extend our analysis to learning dynamics over varying geometries.

   _{tr}_{te}\)} &  & _Navier-Stokes_\(1 10^{-3}\) &  \\   & & _In-t_ & _Out-t_ & _In-t_ & _Out-t_ \\   & DINO & \(2.51 10^{-2}\) & \(9.91 10^{-2}\) & \(4.15 10^{-4}\) & \(3.55 10^{-3}\) \\  & CORAL & \(5.76 10^{-4}\) & \(3.00 10^{-3}\) & \(}\) & \(}\) \\  & OFormer & \(7.76 10^{-3}\) & \(6.39 10^{-2}\) & \(1.00 10^{-2}\) & \(2.23 10^{-2}\) \\  & GNOT & \(3.21 10^{-4}\) & \(2.33 10^{-3}\) & \(2.48 10^{-4}\) & \(2.17 10^{-3}\) \\  & AROMA & \(}\) & \(}\) & \(3.10 10^{-5}\) & \(8.75 10^{-4}\) \\   & DINO & \(3.27 10^{-2}\) & \(1.40 10^{-1}\) & \(4.12 10^{-4}\) & \(3.26 10^{-3}\) \\  & CORAL & \(1.54 10^{-3}\) & \(1.07 10^{-2}\) & \(3.77 10^{-4}\) & \(1.44 10^{-3}\) \\   & OFormer & \(3.73 10^{-2}\) & \(1.60 10^{-1}\) & \(6.19 10^{-3}\) & \(1.40 10^{-2}\) \\   & GNOT & \(2.07 10^{-2}\) & \(6.24 10^{-2}\) & \(8.91 10^{-4}\) & \(4.66 10^{-3}\) \\   & AROMA & \(}\) & \(}\) & \(}\) & \(}\) \\   & DINO & \(3.63 10^{-2}\) & \(1.35 10^{-1}\) & \(4.47 10^{-3}\) & \(9.88 10^{-3}\) \\  & CORAL & \(}\) & \(}\) & \(2.72 10^{-3}\) & \(6.58 10^{-3}\) \\  irregular grid & OFormer & \(3.23 10^{-2}\) & \(1.12 10^{-1}\) & \(8.67 10^{-3}\) & \(1.72 10^{-2}\) \\   & GNOT & \(7.43 10^{-2}\) & \(1.89 10^{-1}\) & \(5.05 10^{-3}\) & \(1.49 10^{-2}\) \\   & AROMA & \(4.73 10^{-3}\) & \(2.01 10^{-2}\) & \(}\) & \(}\) \\   

Table 2: **Temporal Extrapolation** - Test results. Metrics in MSE.

DatasetsWe evaluate our model on two problems involving non-convex domains, as described by Pfaff et al. (2021). Both scenarios involve fluid dynamics in a domain with an obstacle, where the area near the boundary conditions (BC) is more finely discretized. The boundary conditions are specified by the mesh, and the models are trained with various obstacles and tested on different, yet similar, obstacles. \(\)**Cylinder (_CylinderFlow_)**: This dataset simulates water flow around a cylinder using a fixed 2D Eulerian mesh, representing _incompressible_ fluids. For each node \(j\) in the mesh \(\), we have data on the node position \(x^{(j)}\), momentum \(w(x^{(j)})\), and pressure \(p(x^{(j)})\). Our task is to learn the mapping from \((w_{t}(x),p_{t}(x))_{x}\) to \((w_{t+ t}(x),p_{t+ t}(x))_{x}\) for a fixed \( t\). \(\)**Airfoil** (_AirfoilFlow_)**: This dataset simulates the aerodynamics around an airfoil, relevant for _compressible_ fluids. In addition to the data available in the Cylinder dataset, we also have the fluid density \((x^{(j)})\) for each node \(j\). Our goal is to learn the mapping from \((w_{t}(x),p_{t}(x),_{t}(x))_{x}\) to \((w_{t+ t}(x),p_{t+ t}(x),_{t+ t}(x))_{x}\). Each example in the dataset corresponds to a unique mesh. On average, there are 5233 nodes per mesh for _AirfoilFlow_ and 1885 for _CylinderFlow_. We temporally subsample the original trajectories by taking one timestamp out of 10, forming trajectories of 60 timestamps. We use the first 40 timestamps for training (_In-t_) and keep the last 20 timestamps for evaluation (_Out-t_).

SettingWe train all the models with supervision on the next state prediction. At test time, we unroll the dynamics auto-regressively with each model and evaluate the prediction with a mean squared error (MSE) both in the training horizon _(In-t)_ and beyond the training horizon _(Out-t)_.

ResultsThe results in Table 3 show that AROMA outperforms other models in predicting flow dynamics on both _CylinderFlow_ and _AirfoilFlow_ geometries, achieving the lowest MSE values across all tests. This indicates AROMA's superior ability to encode geometric features accurately. Additionally, AROMA maintains stability over extended prediction horizons, as evidenced by its consistently low _Out-t_ MSE values.

### Long rollouts and uncertainty quantification

After training different models on _Burgers_, we compare them on long trajectory rollouts. We start from \(t_{0}=50\) (i.e. use a numerical solver for 50 steps), and unroll our dynamics auto-regressively for 200 steps. Note that all the models were only trained to predict the next state. We plot the correlation over rollout steps of different methods, including our model without the diffusion process, in Figure 3. We can clearly see the gain in stability in using the diffusion for long rollouts. Still, the predictions will eventually become uncorrelated over time as the solver accumulates errors compared with the numerical solution. As we employ a generative model, we can generate several rollouts and estimate the uncertainty of the solver with standard deviations. We can see in Appendix Figure 11 that this uncertainty increases over time. This uncertainty is not a guarantee that the solution lies within the bounds, but is an indication that the model is not confident in its predictions.

Figure 3: Correlation over time for long rollouts with different methods on _Burgers_

   Model &  &  \\   & _In-t_ & _Out-t_ & _In-t_ & _Out-t_ \\  CORAL & \(4.458 10^{-2}\) & \(8.695 10^{-2}\) & \(1.690 10^{-1}\) & \(3.420 10^{-1}\) \\ DINO & \(1.349 10^{-1}\) & \(1.576 10^{-1}\) & \(3.770 10^{-1}\) & \(4.740 10^{-1}\) \\ OFormer & \(5.020 10^{-1}\) & \(1.080 10^{0}\) & \(5.620 10^{-1}\) & \(7.620 10^{-1}\) \\ AROMA & \(}\) & \(}\) & \(}\) & \(}\) \\   

Table 3: **Dynamics on different geometries** - Test results. MSE on normalized data.

Related Work

Our model differs from existing models in the field of operator learning and more broadly from existing neural field architectures. The works most related to ours are the following.

Neural Fields for PDENeural Fields have recently emerged as powerful tools to model dynamical systems. DINO (Yin et al., 2022) is a space-time continuous architecture based on a modulated multiplicative filter network (Fathony et al., 2021) and a NeuralODE (Chen and Zhang, 2019) for modeling the dynamics. DINO is capable of encoding and decoding physical states on irregular grids thanks to the spatial continuity of the INR and through auto-decoding (Park et al., 2019). CORAL is another neural-field based architecture, which tackles the broader scope of operator learning, also builds on meta-learning (Zintgraf et al., 2019; Dupont et al., 2022) to freely process irregular grids. CORAL and DINO are the most similar works to ours, as they are both auto-regressive and capable of processing irregular grids. On the other hand Chen et al. (2022) and Haghberger et al. (2024) make use of spatio-temporal Neural Fields, for obtaining smooth and compact latent representations in the first or to directly predict trajectory solutions within a temporal horizon in the latter. Moreover, they either use a CNN or rely on patches for encoding the observations and are therefore not equipped for the type of tasks AROMA is designed for.

Transformers for PDESeveral PDE solvers leverage transformers and cross-attention as a backbone for modeling PDEs. Transformers, which operate on token sequences, provide a natural solution for handling irregular meshes and point sets. Li et al. (2023) and Hao et al. (2023) introduced transformer architectures tailored for operator learning. Hao et al. (2023) incorporated an attention mechanism and employed a mixture of experts strategy to address multi-scale challenges. However, their architecture relies on linear attention without reducing spatial dimensions, resulting in linear complexity in sequence size, but quadratic in the hidden dimensions, which can be prohibitive for deep networks and large networks. Similarly, Li et al. (2023) utilized cross-attention to embed both regular and irregular meshes into a latent space and applied a recurrent network for time-marching in this latent space. Nonetheless, like GNOT, their method operates point-wise on the latent space. Transolver (Wu et al., 2024) decomposes a discrete input function into a mixture of "slices," each corresponding to a prototype in a mixture model, with attention operating in this latent space. This approach, akin to our model, reduces complexity. However, it has not been designed for temporal problems. (Alkin et al., 2024) recently proposed a versatile model capable of operating on Eulerian and Lagrangian (particles) representations. They reduce input dimensionality by aggregating information from input values onto "supernodes" selected from the input mesh via message passing while decoding is performed with a Perceiver-like architecture. In contrast, AROMA performs implicit spatial encoding with cross-attention to encode the geometry and aggregate obsevation values. Finally, their training involves complex end-to-end optimization, whereas we favor two simple training steps that are easier to implement.

## 6 Conclusion and Limitations

AROMA offers a novel and flexible neural operator approach for modeling the spatio-temporal evolution of physical processes. It is able to deal with general geometries and to forecast at any position of the spatial domain. It incorporates in an encode-process-decode framework attention mechanisms, a latent diffusion transformer for spatio-temporal dynamics and neural fields for decoding. Thanks to a very compact spatial encoding, its complexity is lower than most SOTA models. Experiments with small-size datasets demonstrate its effectiveness. Its reduced complexity holds potential for effective scaling to larger datasets. As for the limitations, the performance of AROMA are still to be demonstrated on larger and real world examples. Moreover, like all dynamical models that operate over a latent space, the reconstruction capabilities of the decoder is a bottleneck for the rollout accuracy. Since the encoder and decoder are learning spatial relationships from scratch, conferring the framework a high flexibility, the training efficiency does not match that of CNN-based auto-encoders on regular grids. We therefore believe there could be further improvements to be made to achieve a similar performance while keeping the same level of flexibility. Finally, even though our model has some potential for uncertainty modeling, this aspect has still to be further explored and analyzed.