# Instance-Specific Asymmetric Sensitivity in Differential Privacy

David Durfee

Mozilla Anonym

ddurfee@mozilla.com

###### Abstract

We provide a new algorithmic framework for differentially private estimation of general functions that adapts to the hardness of the underlying dataset. We build upon previous work that gives a paradigm for selecting an output through the exponential mechanism based upon closeness of the inverse to the underlying dataset, termed the inverse sensitivity mechanism. Our framework will slightly modify the closeness metric and instead give a simple and efficient application of the sparse vector technique. While the inverse sensitivity mechanism was shown to be instance optimal, it was only with respect to a class of unbiased mechanisms such that the most likely outcome matches the underlying data. We break this assumption in order to more naturally navigate the bias-variance tradeoff, which will also critically allow for extending our method to unbounded data. In consideration of this tradeoff, we provide theoretical guarantees and empirical validation that our technique will be particularly effective when the distances to the underlying dataset are asymmetric. This asymmetry is inherent to a range of important problems including fundamental statistics such as variance, as well as commonly used machine learning performance metrics for both classification and regression tasks. We efficiently instantiate our method in \(O(n)\) time for these problems and empirically show that our techniques will give substantially improved differentially private estimations.

## 1 Introduction

We consider the general problem of estimating aggregate functions or statistics of a dataset with differential privacy. The massive increase in data collection to improve analytics and modelling across industries has made such data computations invaluable, but can also leak sensitive individual information. Rigorously measuring such leakage can be achieved through differential privacy, which quantifies the extent that one individual's data can affect the output. Much of the focus within the field of differential privacy is upon constructing algorithms that give both accurate output and privacy guarantees by injecting specific types of randomness. One of the most canonical mechanisms for achieving this considers the maximum effect one individual's data could have upon the output of a given function, referred to as the _sensitivity_ of the function, and adds proportional noise to the function output. In general, the notion of sensitivity plays a central role in many differentially private algorithms, directly affecting the accuracy of the output.

While using the worst-case sensitivity across all potential datasets will ensure privacy guarantees, the utility can be improved by using variants of sensitivity that are specific to the underlying dataset. This notion was initially considered in Nissim et al. (2007), introducing _smooth sensitivity_, an interpolation between worst-case sensitivity and _local sensitivity_ of the underlying data, by which noise could be added proportionally. The smooth sensitivity adapts well to the underlying data andwas further extended to other commonly used variants of the original privacy definition Bun & Steinke (2019).

More aggressive methods were also considered with a data-independent conjectured sensitivity parameter and more accurate results provided when the underlying data complies with the parameter. The propose-test-release methods check that all datasets close to the underlying data have sensitivity below a parameter and add noise proportional when the criteria is met and fail otherwise Dwork & Lei (2009); Thakurta & Smith (2013). Preprocessing methods provide an approximation of the function with sensitivity below a given parameter by which noise can be added proportionally and the approximation is accurate for underlying data with low sensitivity Chen & Zhou (2013); Blocki et al. (2013); Kasiviswanathan et al. (2013); Cummings & Durfee (2020). Clipping techniques, commonly seen in differentially private stochastic gradient descent Abadi et al. (2016), are also a more rudimentary and efficient preprocessing method for ensuring sufficiently small sensitivity. The primary challenge with these approaches is the sensitivity parameter must be specified _a priori_ and can add significant bias if the underlying data does not comply with the parameter.

In contrast, the inverse sensitivity mechanism directly improves upon the smooth sensitivity technique adapting even better to the underlying data. While several instantiations had been previously known in the literature, it was introduced in it's full generality in Asi & Duchi (2020b). Specifically, this framework considers all potential outputs based upon the closeness of their inverse to the underlying data and applies the exponential mechanism to select a point accordingly. This exact methodology can even improve upon adding noise proportional to the local sensitivity of the underlying data, which generally violates differential privacy. Follow-up work gave approximations of this method that allow for efficient implementations of more complex instantiations Asi & Duchi (2020a). For both the exact and approximate versions, the inverse sensitivity mechanism is instance optimal and nearly instance optimal, respectively, under certain assumptions Asi & Duchi (2020a,b).

### Our techniques

We build upon the inverse sensitivity mechanism, particularly within the class of functions for which it was shown to be optimal. However, those guarantees only held for a class of unbiased mechanisms such that the most likely outcome matches the underlying data. The inverse sensitivity mechanism and smooth sensitivity techniques fit this characterization of unbiased. The methods that specify a data-independent sensitivity parameter break this assumption by essentially fixing the variance through this parameter but adding significant bias if the variance parameter is set too low. Our method will also break the unbiased assumption but still adapt well to the underlying data to more naturally navigate the bias-variance tradeoff.

In particular, we similarly consider the distance from the underlying data to the inverse of each possible output, which can be considered the inverse sensitivity. However, we instead invoke the well-known sparse vector technique, originally introduced in Dwork et al. (2009), to select an output close to the underlying data. The iterative nature of sparse vector technique will create a slight bias, while still adapting well to the underlying data. By utilizing this iterative technique, we can also better take advantage when the sensitivities are asymmetric that allows us to reduce the variance, and we thusly term our method the _asymmetric sensitivity mechanism_. In fact, the local sensitivity can be infinite with unbounded data and our technique can still naturally handle this setting for a wide variety of functions including our instantiations. We support this with theoretical utility guarantees that are asymptotically superior to previous work under these conditions.

Our notion of asymmetric sensitivities is inherent to a range of problems, and we first instantiate our method upon variance, a fundamental property of a dataset that is widely used in statistical analysis. Likewise, this property will also apply to commonly used machine learning performance metrics: cross-entropy loss, mean squared error (MSE), and mean absolute error (MAE). Model performance evaluation is an essential part of a machine learning pipeline, particularly for iterative improvement, so accurate and private evaluation is critical. We instantiate our method upon these functions as well, and give an extensive empirical study for each instantiation across a variety of datasets and privacy parameters. We show that our method significantly improves performance of private estimation for these important problems. We further complement our results with an approximate method that allows for more efficient implementations of general functions while still preserving the asymmetry that we exploit for improved estimations. This will allow us to give \(O(n)\) time implementations for each invocation of our method.

### Additional related works

While the most closely related literature was discussed in more detail previously, we provide additional related works here. Recent work considered instance-optimality but for estimators population quantities McMillan et al. (2022), which differ from the empirical quantities studied here and in the other previously mentioned work. Additional work formally considered the bias-variance-privacy trade-off particularly for mean estimation Kamath et al. (2023), but considers bias in the more classical sense. Interestingly, it's also been seen in the work for obtaining (asymptotically) optimal mean estimation for subgaussian distributions Karwa and Vadhan (2018); Bun and Steinke (2019) and distributions satisfying bounded moment conditions Barber and Duchi (2014); Kamath et al. (2020), that adding bias was necessary. This fits with our results where bias, albeit a different type, was needed to improve instance-specific differential privacy.

### Our contributions

We summarize our primary contributions as the following:

1. We introduce a new algorithmic framework for private estimation of general functions, which we refer to as the asymmetric sensitivity mechanism, along with a more computationally efficient approximate variant (see Section 3).
2. We provide theoretical utility guarantees that asymptotically confirm our method's advantage when the sensitivities are asymmetric and further give intuition and empirical support of this asymmetric advantage (see Section 4)
3. We efficiently instantiate our method for private variance estimation, and provide an extensive empirical study showing significantly improved accuracy (see Section 5).
4. We further invoke our method upon model evaluation for both classification and regression tasks with corresponding efficient implementations and empirical studies showing improved estimations (see Section 6).

Additional and supplemental analysis, results and empirical studies are pushed to the appendix.

## 2 Preliminaries

For simplicity and ease of comparison we borrow much of the notation from Asi and Duchi (2020, 2020).

**Definition 2.1**.: Let \(,^{}\) be datasets of our data universe \(^{n}\). We define \(d_{}(,^{})=|\{i\,:\,_{i}_{i}^ {}\}|\) to be the Hamming distance between datasets. If \(d_{}(,^{}) 1\) then \(,^{}\) are _neighboring_ datasets.

Note that we assume the _swap_ definition of neighboring datasets but will also discuss how our results apply to the _add-subtract_ definition in Appendix B.3. We further define the (global) sensitivity.

**Definition 2.2**.: \(f\,:\,^{n}\) has sensitivity \(\) if for any neighboring datasets \(|f()-f(^{})|\)

We will be using the classical (pure) differential privacy definition, but will also discuss how our methods apply to other definitions with improved guarantees.

**Definition 2.3**.: Dwork et al. (2006, 2020) A mechanism \(M:\,^{n}\) is \((,)\)-differentially-private (DP) if for any neighboring datasets \(,^{}\) and measurable \(S\):

\[M() S^{}M( ^{}) S+.\]

If \(=0\) then \(M\) is \(\)-DP.

### Sparse vector technique

We define the fundamental sparse vector technique introduced in Dwork et al. (2009) and often considered to apply Laplacian noise Lyu et al. (2017). However, recent work showed the noise can instead be added from the exponential distribution at the same parameter for improved utility Durfee (2023). Let \((b)\) denote a draw from the exponential distribution with scale parameter \(b\). The sparse vector technique iteratively calls the following algorithm.

This technique can further see improvement when the queries are monotonic which will apply to most of our instantiations of our method.

**Definition 2.4**.: We say that stream of queries \(\{f_{i}\,:\,^{n}\}\) with sensitivity \(\) is _monotonic_ if for any neighboring \(,^{}^{n}\) we have either \(f_{i}() f_{i}(^{})\) for all \(i\) or \(f_{i}() f_{i}(^{})\) for all \(i\).

This allows for the following differential privacy guarantees from Durfee (2023).

**Proposition 2.5**.: _Algorithm 1 is \((_{1}+2_{2})\)-DP in general and \((_{1}+_{2})\)-DP for monotonic queries_

### Inverse sensitivity mechanism

The inverse sensitivity mechanism had seen several previous instantiations but was introduced in it's full generality in Asi & Duchi (2020b). We first introduce the exponential mechanism.

**Definition 2.6**.: McSherry & Talwar (2007) The Exponential Mechanism is a randomized mapping \(M\,:\,^{n}\) such that

\[[M()=t](,t)}{ 2})\]

where \(q\,:\,^{n}\) has sensitivity \(\).

**Proposition 2.7**.: McSherry & Talwar _(_2007_)_ _The exponential mechanism is \(\)-DP_

We then define the distance of a potential output from the underlying dataset to be the Hamming distance required to change the data such that the new data matches the output.

**Definition 2.8**.: For a function \(f\,:\,^{n}\) and \(^{n}\), let the _inverse sensitivity_ of \(t\) be

\[_{f}(;t)}}{{=}}_{ ^{}}\{d_{}(,^{})|f(^{})=t\}\]

By construction this distance metric for any output cannot change by more than one between neighboring datasets due to the triangle inequality for Hamming distance.

**Corollary 2.9**.: _For any neighboring datasets \(,^{}^{n}\) and \(t(f)\) where \((f)\) is the image of the function, we have \(|_{f}(;t)-_{f}(^{};t)| 1\)_

The _inverse sensitivity mechanism_ then draws from the exponential mechanism instantiated upon the distance metric giving the density function

\[_{M_{in}()}(t)=_{f}()/2}}{ _{}e^{-_{f}()/2}ds}\] (M.1)

and mechanism M.1 is \(\)-DP by Proposition 2.7 and Corollary 2.9.

## 3 Asymmetric Sensitivity Mechanism

In this section we introduce our general methodology for instance-specific differentially private estimation, which we term the asymmetric sensitivity mechanism. We first give the exact formulation which will fit an extensive class of functions focused upon in Asi & Duchi (2020b). Nextwe provide a simple framework by which our method can be implemented. The efficiency of this implementation is highly dependent upon the function of interest, but we supplement these results with an approximate method. This can allow for broader efficient implementations and also extends our methodology to general functions. While this section will set up and provide the necessary rigor for our techniques, we also point the reader to Appendix C for a more intuitive explanation of our approach compared to the inverse sensitivity mechanism.

### Exact asymmetric sensitivity mechanism

Our method will similarly consider the distance for each output from the underlying data with the goal being to select an output with distance close to zero. The inverse sensitivity mechanism does this through the exponential mechanism, but we will instead apply the sparse vector technique. In order to better apply the sparse vector technique, we will first modify the inverse sensitivity such that it is negative for outputs that are less than output from the underlying data.

**Definition 3.1**.: For \(f\,:\,^{n}\) and \(^{n}\), let the _reflective inverse sensitivity_ of \(t\) be

\[_{f}(;t)}}{{=}} {sgn}(t-f())(_{f}(;t)-)\]

Intuitively, the goal of applying the sparse vector technique will be to identify when the reflective inverse sensitivity crosses the threshold from negative to positive. While there are reasonable methods of extending our approach to higher dimensions, it will both become computationally inefficient and the notion of asymmetry, which gives our method the most significant improvement, is less inherent in higher dimensions. Initially, we focus upon a general class of functions considered in Asi and Duchi (2020) that was shown to include all continuous functions from a convex domain.

**Definition 3.2** (Definition 4.1 in Asi and Duchi (2020)).: Let \(f\,:\,^{n}\). Then \(f\) is _sample-monotone_ if for every \(^{n}\) and \(s,t\) satisfying \(f() s t\) or \(t s f()\), we have \(_{f}(;s)_{f}(;t)\)

For this class of functions, we show that the reflective inverse sensitivity of an output maintains closeness between neighboring datasets. Accordingly, we can apply the sparse vector technique to a stream of potential outputs in order to (noisily) identify when the reflective inverse sensitivity crosses the threshold from negative to positive. This gives the _asymmetric sensitivity mechanism_ for a stream of potential outputs \(\{t_{i}\}\) that calls \(\) and returns \(t_{k}\) when

\[(,\{_{f}(;t_{i})\},T=0)=\{^{k- 1},\}\] (M.2)

To be effective, this stream of potential outputs should be increasing (or decreasing if we flip the sign of \(_{f}\)) but will still achieve the desired privacy guarantees regardless which is shown in Appendix A.2.

**Theorem 3.3**.: _Given sample-monotone \(f\,:\,^{n}\) and any stream of potential outputs \(\{t_{i}\}\), we have that mechanism M.2 is \((_{1}+2_{2})\)-DP in general and \((_{1}+_{2})\)-DP if \(_{f}\) is monotonic._

We further detail in Appendix B a simple, general, and robust strategy for selecting potential outputs that provides a reasonable limit on the number of queries.

### Implementation framework

The primary bottleneck in efficiently implementing both our asymmetric sensitivity mechanism and the inverse sensitivity mechanism is the computation of the inverse sensitivity. In particular, it will require computing upper and lower output bounds for different Hamming distances from our underlying data.

**Definition 3.4**.: For a function \(f\,:\,^{n}\), we define the upper and lower output bounds for Hamming distance \(\) as

\[U^{}_{f}()}}{{=}}_{^{ }}\{f(^{})\,:\,d_{}(,^{}) \}\]

and

\[L^{}_{f}()}}{{=}}_{^{ }}\{f(^{})\,:\,d_{}(,^{}) \}\]The complexity of computing these depends upon the function, but we can use the upper and lower output bounds to get the inverse sensitivity with the following lemma proven in Appendix A.2.

**Lemma 3.5**.: _If \(f\) is sample-monotone then \(_{f}(;t)=\{t\,:\,L^{t}_{f}() t U^{t}_{f}( )\}\) for all \(t\)_

If we then assume access to the array \(L^{n}_{f}(),...,L^{1}_{f}(),f(),U^{1}_{f}(),...,U^{n}_ {f}()\), for any potential output \(t_{i}\) we can compute \(_{f}(;t_{i})\) in \(O((n))\) time with a simple binary search. Alternatively, we could also take \(O(n)\) amortized time by maintaining a pointer and iteratively increasing the index for each new potential output if we assume the stream of potential outputs are non-decreasing. This gives the general implementation framework:

1. Compute upper and lower output bounds \(U^{t}_{f}()\) and \(L^{t}_{f}()\) for all \(t[n]\)
2. Use the output bounds to efficiently \((,\{_{f}(;t)\},T=0)\)

### Approximate asymmetric sensitivity mechanism

In Section A, we show how we can extend our asymmetric sensitivity mechanism to general functions \(f\,:\,^{n}\) and provide more efficient implementations. It will follow closely with our exact version above.

## 4 Asymmetric Sensitivity Advantage

In this section, we first connect our definitions with the corresponding definitions in the previous work, by which utility guarantees are provided. Then we discuss the notion of asymmetric sensitivities and provide our utility guarantees that exploit this asymmetry to asymptotically improve upon the previous work under those conditions.

### Connection to previous work

An essential quantity for our method and both inverse and smooth sensitivity mechanisms is the amount a function output can change if \(k\) individuals change their data. This is quantified in Equation 3 from Asi & Duchi (2020) which can be translated to our definitions (in the case when \(=\)) as

\[_{f}(;k)}{=}\{|f()-L^{k}_{f}() |,|f()-U^{k}_{f}()|\}\]

Note that if \(k=1\) then this is the _local sensitivity_ of the function. It's then shown in Asi & Duchi (2020) (Corollary 4.2 and Equation 13, respectively) that the general utility guarantees of both inverse sensitivity mechanism and smooth sensitivity mechanism are bounded with respect to this quantity. More simply, the accuracy guarantees degrade as the local sensitivity increases and there is no utility bound if local sensitivity is infinite.

### Asymmetric accuracy guarantees

To understand the advantages of our method, we will consider the sensitivities to be asymmetric if \(|f()-U^{k}_{f}()|>>|f()-L^{k}_{f}()|\) for most \(k\) (or vice versa), which is to say that changing an individual's data can generally increase the function more than decrease it. In general, we expect any lower bounded function to inherently limit the amount changing one individual's data can decrease the function compared to increasing the function. Each instantiation in our empirical study is a non-negative function which then fits this characterization. For simplicity, we will restrict our consideration to non-negative functions for our utility guarantees, but can easily apply these to other settings.

The goal for our method is to exploit the asymmetric sensitivities by instead applying the sparse vector technique. The iterative nature of this technique biases the output towards being less than \(f()\), but more importantly the \(U^{k}_{f}()\) values will have little effect upon the accuracy. Specifically, once the threshold is crossed it becomes increasingly unlikely that the sparse vector technique will proceed. Explicitly connecting this with our mechanism, even if \(U^{k}_{1}()=\) and so the local 

[MISSING_PAGE_FAIL:7]

While the formula above immediately suggests an \(O(n^{2})\) time computation of all the lower output bounds, we will further prove in Appendix D that we can use our approximation to get a more efficient implementation. In particular, we consider a data independent fixed distance and only compute the exact bounds outputs closer to the underlying data to still ensure accurate estimations.

**Lemma 5.3**.: _Given \(^{n}\) and \(c 0\), we can compute all approximate output bounds \(^{}_{}()=L^{}_{}()\) for \( c\) and \(^{}_{}()=0\) for \(>c\) in \(O(n+c^{2})\) time._

Next we consider the upper output bounds, but if the data is unbounded then we must have \(U^{i}_{}()=\). It is precisely for this reason that asymmetric sensitivities are inherent for variance. Our method can naturally handle this setting and we show in Appendix B.1 that unbounded upper output bounds barely affects our accuracy. However, applying the inverse sensitivity mechanism requires reasonable bounds upon each data point that should be data-independent and also sufficiently loose to not add bias from clipping data points.

**Lemma 5.4**.: _If we restrict all values to the interval \([a,b]\) then given \([a,b]^{n}\) we can give approximate upper output bounds_

\[^{}_{}()=[]+}{n}\]

To our knowledge, there is no efficient method for computing the exact upper output bounds for general data (contained in a range), so to maintain practicality we provide approximate bounds, proven in Appendix D.

```
0: Input dataset \(\), and parameter \(>1\)
1: Compute all \(^{}_{}()\) with \(c=100\) (Lemma 5.3)
2: Compute all \(^{}_{}()\) if domain is restricted to \([a,b]^{n}\) (Lemma 5.4)
3:\(\{^{k-1},\}(,\{ _{f}}(;^{i}-1)\},T=0)\)
0:\(^{k}-1\) ```

**Algorithm 2** Variance instantiation of asymmetric sensitivity mechanism

**Theorem 5.5**.: _Algorithm 2 is \((_{1}+2_{2})\)-DP and has a runtime of \(O(n+q)\) where \(q\) is the number of queries in \(\)_

Proof.: If we assume the domain is restricted to \([a,b]^{n}\) then the privacy guarantees follow from Lemma 5.3 and Lemma 5.4 applied to Theorem A.4. If not then we apply Lemma 5.3 and \(^{1}_{}()=\) to Theorem A.4 to get our privacy guarantees.

For the runtime, computing all \(^{}_{}()\) is \(O(n)\) time by Lemma 5.3 and fixing \(c=100\), and computing all \(^{}_{}()\) is \(O(n)\) time. Finally, we can run \(\) in \(O(n+q)\) time as seen in Section 3.2 

In our implementations we'll more reasonably assume \( 1.001\), so \(^{50000}>10^{21}\) and we'll simply terminate \(\) after at most \(50,000\) queries for all datasets without affecting the privacy guarantees. This then gives a runtime of \(O(n)\).

### Empirical study of variance estimation

For our instantiations of machine learning model evaluation we will be using the following datasets for regression tasks: Diamonds dataset containing diamond prices and related features Wickham (2016); Abalone dataset containing age of abalone and related features Nash et al. (1995); and Bike dataset containing number of bike rentals and related features Fanaee-T (2013). We will also use the labels from these datasets to test our variance invocation. We also use the Adult dataset, Becker & Kohavi (1996), for model evaluation of classification tasks so we will borrow two of the features, age and hours worked per week, to test our variance invocation.

While our method does not require any bounds on the data to still maintain high accuracy (see Appendix B.1), it is necessary for the other mechanisms. All of our data is inherently non-negativeand some data has innate upper bounds. If not, we use reasonable upper bounds, which should be data independent, to avoid adding bias from clipping the data. We use the following bounds:  for diamond prices;  for abalone ages;  for city bike rentals;  for hours; and  for age. Our algorithm (Algorithm 2 in Appendix D) will have a parameter \(\) which we fix \(=1.005\) and will maintain this consistency across all experiments. We further show in Appendix B.2 that our method is robustly accurate across reasonable \(\) choices.

For each privacy parameter, we repeat 100 times: sample 1,000 datapoints from the dataset and call each mechanism on the sampled data for estimates. We plot the average absolute error for each method along with confidence intervals of 0.9 in Figure 2. As expected, we see that our approach of variance estimation sees substantially less error across privacy parameters and datasets.

## 6 Private Machine Learning Model Evaluation

In this section, we invoke our asymmetric sensitivity mechanism upon commonly used metrics for machine learning model evaluation for both classification and regression tasks. In particular, we consider cross-entropy loss for classification tasks, and mean squared error (MSE) and mean absolute error (MAE) for regression tasks. Note that combining our improved estimation for variance in Section 5 with the improved estimation for MSE also implies an improved estimation of the coefficient of determination, \(R^{2}\), also commonly used for evaluating regression performance.

We provide full definitions along with technical analysis in Section E.

### Empirical study of model evaluation for classification

For our empirical study of model evaluation for classification tasks we will consider two tabular datasets with binary labels, the Adult dataset Becker & Kohavi (1996) and Diabetes dataset Efron et al. (2004), along with two computer vision tasks with 10 classes, the mnist dataset LeCun et al. (2010) and cifar10 dataset Krizhevsky et al. (2009). Our focus here is upon the accuracy of our evaluation, not optimizing the quality of the model itself. As such, we will be using reasonable choices for models for simplicity but certainly not state-of-the-art models.

For the tabular data, we partition into train and test with an 80/20 split and train with an xgboost classifier with the default parameters. For the mnist data, which is already partitioned, we use a simple MLP with one inner dense layer of 128 neurons and relu activation, and the final layer of 10 neurons has a softmax activation. We train this model for 5 epochs. For the cifar10 data, which is already partitioned, we use a relatively small CNN with several pooling and convolutional layers,

Figure 1: Plots comparing each method for estimating variance. For each privacy parameter we sample 1,000 datapoints from the dataset and call each mechanism 100 times, plotting the average absolute error with 0.9 confidence intervals.

and several dense layers at the end with relu activation and final layer with softmax activation. We train this model for 10 epochs.

Again our method does not require any bounds on the data to maintain high accuracy, but it is necessary for the inverse sensitivity mechanism. Given the softmax activation for our models, the outputs are unbounded, but we will provide reasonable bounds. We will use the bounds [-10,10] of the model output for binary classification tabular data, and bounds \([-25,25]^{10}\) of the model output for the multi-classification vision data. Once again, we fix our parameter \(=1.005\).

### Empirical study of model evaluation for regression

As discussed in Section 5, our machine learning model evaluations for regression will use the following datasets: Diamonds dataset containing diamond prices and related features Wickham (2016); Abalone dataset containing age of abalone and related feature Nash et al. (1995); and Bike dataset containing number of bike rentals and related feature Fanaee-T (2013). We also use the same parameters from Section 5 for these datasets. Once again, our goal here is to accurately assess the quality of the model and not optimize performance. As such we simply train with xgboost regressor under default parameters after partitioning each dataset into train and test with an 80/20 split.

We first consider mean squared error estimation and repeat 100 times for each privacy parameter: draw 1,000 datapoints from the test set and call each mechanism 100 times for estimates. We then plot the average absolute error along with confidence intervals of 0.9. We repeat this process for mean absolute error.

Figure 2: Plots comparing each method for estimating cross entropy loss. For each privacy parameter we both sample 1,000 datapoints from the test set and call each mechanism 100 times, plotting the average absolute error with 0.9 confidence intervals.