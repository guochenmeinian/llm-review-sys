# VoxDet: Voxel Learning for Novel Instance Detection

Bowen Li\({}^{1}\) &Jiashun Wang\({}^{1}\) &Yaoyu Hu\({}^{1}\) &Chen Wang\({}^{2}\) &Sebastian Scherer\({}^{1}\)

\({}^{1}\) Carnegie Mellon University \({}^{2}\) State University of New York at Buffalo

###### Abstract

Detecting unseen instances based on multi-view templates is a challenging problem due to its open-world nature. Traditional methodologies, which primarily rely on 2D representations and matching techniques, are often inadequate in handling pose variations and occlusions. To solve this, we introduce VoxDet, a pioneer 3D geometry-aware framework that fully utilizes the strong 3D voxel representation and reliable voxel matching mechanism. VoxDet first ingeniously proposes template voxel aggregation (TVA) module, effectively transforming multi-view 2D images into 3D voxel features. By leveraging associated camera poses, these features are aggregated into a compact 3D template voxel. In novel instance detection, this voxel representation demonstrates heightened resilience to occlusion and pose variations. We also discover that a 3D reconstruction objective helps to pre-train the 2D-3D mapping in TVA. Second, to quickly align with the template voxel, VoxDet incorporates a Query Voxel Matching (QVM) module. The 2D queries are first converted into their voxel representation with the learned 2D-3D mapping. We find that since the 3D voxel representations encode the geometry, we can first estimate the relative rotation and then compare the aligned voxels, leading to improved accuracy and efficiency. In addition to method, we also introduce the first instance detection benchmark, RoboTools, where 20 unique instances are video-recorded with camera extrinsic. RoboTools also provides 24 challenging cluttered scenarios with more than 9k box annotations. Exhaustive experiments are conducted on the demanding LineMod-Occlusion, YCB-video, and RoboTools benchmarks, where VoxDet outperforms various 2D baselines remarkably with faster speed. To the best of our knowledge, VoxDet is the first to incorporate implicit 3D knowledge for 2D novel instance detection tasks. Our code, data, raw results, and pre-trained models are public at https://github.com/Jaraxxus-Me/VoxDet.

## 1 Introduction

Consider the common scenarios of locating the second sock of a pair in a pile of laundry or identifying logage amid hundreds of similar suitcases at an airport. These activities illustrate the remarkable capability of human cognition to swiftly and accurately identify a specific _instance_ among other similar objects. Humans can rapidly create a mental picture of a novel _instance_ with a few glances even if they see such an _instance_ for the first time or have never seen _instances_ of the same type. Searching for instances using mental pictures is a fundamental ability for humans, however, even the latest object detectors  still cannot achieve this task.

We formulate the above tasks as novel instance detection, that is identification of an unseen instance in a cluttered query image, utilizing its multi-view support references. Previous attempts mainly work in 2D space, such as correlation , attention mechanisms , or similarity matching , thereby localizing and categorizing the desired instance, as depicted in Fig. 1 gray part. However, these techniques struggle to maintain their robustness when faced with significant disparities between the query and templates. In comparison to novel instance detection, there is a vast amount of work centered around few-shot category-level object detection . Yet, these class-level matching techniques prove insufficient when it comes to discerning specific instance-level features.

Humans exhibit the remarkable capability to swiftly formulate a mental model of an unfamiliar instance, facilitated by a rapid comprehension of its 3D geometric structure [10; 11; 12]. Leveraging such a mental representation, once presented with a single query image, a human can probably search and identify the same instance despite alterations in distance, occlusion, and even approximate the instance's orientation. Motivated by this, we propose VoxDet, a pioneer 3D geometry-aware instance detection framework as shown in Fig. 1 bottom. In contrast to state-of-the-art methods [7; 5; 6; 9; 13], VoxDet adapts two novel designs: (1) a compact 3D voxel representation that is robust to occlusion and pose variations and (2) an effective voxel matching algorithm for identifying instances.

VoxDet consists of three main modules: a template voxel aggregation (TVA) module, an open-world detection module, and a query voxel matching (QVM) module. Initially, the TVA module transforms multi-view 2D features of an instance into individual 3D template voxels . These template voxels are then accumulated using relative rotations, thus incorporating both geometry and appearance into a condensed template voxel. As VoxDet learns this 2D-3D mapping via a reconstruction objective, TVA effectively encapsulates both the geometry and appearance of any instance into a compact template voxel. When presented with a query image, VoxDet employs an open-world detector  that universally identifies potential objects within the image as 2D proposals. These proposals are then converted to query voxels via the learned 2D-3D mapping and compared with the template voxel by the QVM module. QVM initiates this comparison process by first estimating the relative rotation between a query voxel and the template, which is then used to align the two voxels. Finally, the comparison between aligned voxels is delivered by a carefully designed voxel relation module.

Besides methodology, we also construct a large-scale synthetic training dataset, Open-World Instance Detection (OWID). OWID comprises 10k instances sourced from the ShapeNet  and Amazon Berkeley Objects  datasets, culminating in 55k scenes and 180k query bounding boxes. Trained on OWID, VoxDet demonstrates strong generalization ability on novel instances, which we attribute to the meticulously designed voxel-based framework and the large-scale OWID training set.

To validate VoxDet, we further build RoboTools, a new instance detection benchmark compiled from a diverse range of real-world cluttered environments. RoboTools consists of 20 unique instances, 24 test scenes, and over 9,000 annotated bounding boxes. As shown in Fig. 1 right, in the demanding RoboTools benchmark, VoxDet can robustly detect the novel instances under severe occlusion or varied orientation. Evaluations are also performed on the authoritative Linemod-Occlusion  and YCB-video  for more compelling results. The exhaustive experiments on these three benchmarks demonstrate that our 3D geometry-aware VoxDet not only outperforms various previous works [5; 6; 7] and different 2D baselines [19; 9] but also achieves faster inference speed.

## 2 Related Works

**Typical object detection**[20; 21; 22; 23; 24; 25; 26] thrive in category-level tasks, where all the instances belonging to a pre-defined class are detected. Typical object detection can be divided into two-stage approaches and

Figure 1: Architecture comparison between previous 2D methods (_gray_) and proposed VoxDet (_black_). Previous methods resorts to pure 2D correlation/attention/matching for novel instance detection. In contrast, VoxDet is 3D-inspired, leveraging reconstruction objective to learn the geometry-aware voxel representation, which enables more effective and accurate voxel-based instance detection. In the challenging newly built RoboTools benchmark shown on the right, VoxDet exhibits surprising robustness to severe occlusion and orientation variation.

one-stage approaches. For the former one, RCNN  and its variants [21; 22] serves as foundations, where the regions of interest (ROI) are first obtained by the region proposal network. Then the detection heads classify the labels of each ROI and regress the box coordinates. On the other hand, the YOLO series [23; 24; 25] and recent transformer-based methods [4; 3] are developing promisingly as the latter stream, where the detection task is tackled as an end-to-end regression problem.

**Few-shot/One-shot object detection**[1; 27; 28; 2; 29; 7] can work for unseen classes with only a few labeled support samples, which are closer to our task. One stream focuses on transfer-learning techniques [28; 27], where the fine-tuning stage is carefully designed to make the model quickly generalize to unseen classes. While the other resorts to meta-learning strategies [1; 7; 2; 29], where various kinds of relations between supports and queries are discovered and leveraged. Since the above methods are category-level, they assume more than one desired instances exist in an image, so the classification/matching designs are usually tailored for Top-100 precision, which is not a very strict metric. However, they can easily fail in our problem, where the _Top-1_ accuracy is more important.

**Open-world/Zero-shot object detection**[30; 31; 32; 14] finds any objects on an image, which is class-agnostic and universal. Some of them learn objectiveness [30; 14] and others  rely on large-scale high-quality training sets. These methods can serve as the first module in our pipeline, which generates object proposals for comparison with the templates. Among them, we adopt  with its simple structure and promising performance.

**Instance detection** requires the algorithm to find an unseen instance in the test image with some corresponding templates. Previous methods [6; 5; 8] usually utilize pure 2D representations and 2D matching/relation techniques. For example, DTOID  proposed global object attention and a local pose-specific branch to predict the template-guided heatmap for detection. However, they easily fall short when the 2D appearance variates due to occlusion or pose variation. Differently, VoxDet leverages the explicit 3D knowledge in the multi-view templates to represent and match instances, which is geometry-invariant.

**Multi-view 3D representations** Representing 3D scenes/instances from multi-view images is a long-standing problem in computer vision. Traditional methods resort to multi-view geometry, where structure from motion (SfM)  pipeline has enabled joint optimization of the camera pose and 3D structure. Modern methods usually adopts neural 3D representations [34; 11; 35; 36; 12; 10], including deep voxels [35; 12; 10; 38] and implicit functions [36; 37], which have yielded great success in 3D reconstruction or novel view synthesis. Our framework is mainly inspired by Video Autoencoder , which encodes a video by separately learning the deep implicit 3D structure and the camera trajectory. One biggest advantage of  is that the learned Autoencoder can encode and synthesize test scenes without further tuning or optimization, which greatly satisfies the efficiency requirement of our instance detection task.

## 3 Methodology

### Problem Formulation

Given a training instance set \(_{}\) and an unseen test instance set \(_{}\), where \(_{}_{}=\), the task of novel instance detection (open-world detection) is to find an instance detector trained on \(_{}\) and then detect new instances in \(_{}\) with no further training or finetuning. Specifically, for each instance, the input to the detector is a query image \(^{}^{3 H}\) and a group of \(M\) support templates \(^{}^{M 3 W H}\) of the target instance. The detector is expected to output the bounding box \(^{4}\) of an instance on the query image. We assume there exists exactly one such instance in the query image and the instance is located near the center of the support images.

### Architecture

The architecture of VoxDet is shown in Fig. 2, which consists of an open-world detector, a template voxel aggregation (TVA) module, and a query voxel matching (QVM) module. Given the query image, the open-world detector aims to generate universal proposals covering all possible objects. TVA aggregates multi-view supports into a compact template voxel via the relative camera pose between frames. QVM lifts 2D proposal features onto 3D voxel space, which is then aligned and matched with the template voxel. In order to empower the voxel representation with 3D geometry, we first resort to a reconstruction objective in the first stage. The pre-trained models serve as the initial weights for the second instance detection training stage.

#### 3.2.1 Open-World Detection

Since the desired instance is unseen during training, directly regressing its location and scale is non-trivial. To solve this, we first use an open-world detector  to generate the most possible candidates. Different from standard detection that only finds out pre-defined classes, an open-world detector locates _all_ possible objects in an image, which is class-agnostic.

As shown in Fig. 2, given a query image \(^{}\), a 2D feature map \(^{}\) is extracted by a backbone network \(()\). To classify each pre-defined anchor as foreground (objects) or background, the region proposal network (RPN)  is adopted. Concurrently, the boundaries of each anchor are also roughly regressed. The resulting anchors with high classification scores are termed region proposals \(=[_{1},_{2},,_{N}] ^{N 4}\), where \(N\) is the number of proposals. Next, to obtain the features \(^{}\) for these candidates, we use region of interest pooling (ROIAlign) , \(^{}=(,^{}) ^{N C w w}\), where \(C\) denotes channel dimensions and \(w\) is the spatial size of proposal features. Finally, we obtain the final classification result and bounding box by two parallel multi-layer perceptrons (MLP), known as the detection head, which takes the proposal features \(^{}\) as input, and outputs the binary classification scores and the box regression targets. The training loss is comprised of RPN classification loss \(^{}_{}\), RPN regression loss \(^{}_{}\), head classification loss \(^{}_{}\), and head regression loss \(^{}_{}\).

To make the detector work for open-world objects, the classification branches (in RPN and head) are guided by _objectiveness regression_. Specifically, the classification score is defined (supervised) by Intersection over Union (IoU), which showed a high recall rate over the objects in test images, even those unseen during training. Since they have learned the class-agnostic "objectiveness", we assume the open-world proposals probably cover the desired novel instance. Therefore, we take the top-ranking candidates and their features as the input of the subsequent matching module.

#### 3.2.2 Template Voxel Aggregation

To learn geometry-invariant representations, the Template Voxel Aggregation (TVA) module compresses multi-view 2D templates into a compact deep voxel. Inspired by previous technique  developed for unsupervised video encoding, we propose to encode our instance templates via their relative orientation in the physical 3D world. To this end, we first generate the 2D feature maps \(^{}=(^{})^{M C  w w}\) using a shared backbone network \(()\) used in the query branch and then map the 2D features to 3D voxels for multi-view aggregation.

**2D-3D mapping:** To map these 2D features onto a shared 3D space for subsequent orientation-based aggregation, we utilize an implicit mapping function \(()\). This function translates the 2D features to 3D voxel features, denoted by \(=(^{})^{M C_{v}  D L L}\), where \(\) is the 3D voxel feature

Figure 2: Architecture of VoxDet. VoxDet mainly consists of three modules, namely, open-world detection, template voxel aggregation (TVA), and query voxel matching (QVM). We first train TVA via the reconstruction stage, where the 2D-3D mapping learns to encode instance geometry. Then the pre-trained mapping serves as initial weights in the TVA and QVM modules for detection training.

from the 2D feature, \(C_{ v}\) is the feature dimension, and \(D,L\) indicate voxel spatial size. Specifically, we first reshape the feature maps to \(^{ S}^{M(C/d) d w w}\), where \(d\) is the pre-defined implicit depth, then we apply 3D inverse convolution to obtain the feature voxel.

Note that with multi-view images, we can calculate the relative camera rotation easily via Structure from Motion (SfM)  or visual odometry . Given that the images are object-centered and the object stays static in the scene, these relative rotations in fact represent the relative rotations between the object orientations defined in the same camera coordination system. Different from previous work  that implicitly learns the camera extrinsic for unsupervised encoding, we aim to explicitly embed such geometric information. Specifically, our goal is to first transform every template into the same coordinate system using their relative rotation, which is then aggregated:

\[^{ S}=_{i=1}^{M}(( _{i},_{i}^{}))\;,\] (1)

where \(_{i}^{C_{ v} D L L}\) is the previously mapped \(i\)-th independent voxel feature, \(_{i}^{}\) denotes the relative camera rotation between the \(i\)-th support frame and the first frame. \((,)\) is the 3D transform used in , which first wraps a unit voxel to the new coordination system using \(_{i}^{}\) and then samples from the feature voxel \(_{i}\) with the transformed unit voxel grid. Therefore, all the \(M\) voxels are transformed into the same coordinate system defined in the first camera frame. These are then aggregated through average pooling to produce the compact template voxel \(^{ S}\).

By explicitly embedding the 3D rotations into individual reference features, TVA achieves a geometry-aware compact representation, which is more robust to occlusion and pose variation.

#### 3.2.3 Query Voxel Matching

Given the proposal features \(^{ Q}\) from query image \(^{ Q}\) and the template voxel \(^{ S}\) from supports \(^{ S}\), the task of the query voxel matching (QVM) module is to classify each proposal as foreground (the reference instance) or background. As shown in Fig. 2, in order to empower the 2D features with 3D geometry, we first use the same mapping to get query voxels, \(^{ Q}=(^{ Q})^{N C_{  v} D L L}\). VoxDet next accomplishes matching \(^{ S}\) and \(^{ Q}\) through two steps. First, we need to estimate the relative rotation between query and support, so that \(^{ Q}\) can be aligned in the same coordinate system as \(^{ S}\). Second, we need to learn a function that measures the distance between the aligned two voxels. To achieve this, we define a voxel relation operator \(_{ v}(,)\):

**Voxel Relation** Given two voxels \(_{1},_{2}^{c a a a}\), where \(c\) is the channel and \(a\) is the spatial dimension, this function seeks to discover their relations in every semantic channel. To achieve this, we first interleave the voxels along channels as \((_{1},_{2})=[_{1}^{1},_{2} ^{1},_{1}^{2},_{2}^{2},,_{1}^{c}, _{2}^{c}]^{2c a a a}\), where \(_{1}^{k},_{2}^{k}\) is the voxel feature in the \(k\)-th channel. Then, we apply grouped convolution as \(_{ v}(_{1},_{2})=( (_{1},_{2}),=c)\). In the experiments, we found that such a design makes relation learning easier since each convolution kernel is forced to learn the two feature voxels from the same channel. With this voxel relation, we can then roughly estimate the rotation matrix \(}^{Q}^{N 3 3}\) of each query voxel relative to the template as:

\[}^{Q}=(_{ v}(^{ S}, ^{ Q}))\;,\] (2)

where \(^{ S}\) is copied \(N\) times to get \(^{ S}\). In practice, we first predict 6D continuous vector  as the network outputs and then convert the vector to a rotation matrix. Next, we can define the classification head with the Voxel Relation as:

\[=(_{ v}(^{ S}, (^{ Q},}^{Q}))),\] (3)

where \((^{ Q},}^{Q})\) rotates the queries to the support coordination system to allow for reasonable matching. In practice, we additionally introduced a global relation branch for the final score, so that the lost semantic information in implicit mapping can be retrieved. More details are available in the supplementary material. During inference, we rank the proposals \(\) according to their matching score and take the Top-k candidates as the predicted box \(}\).

### Training Objectives

As illustrated in Fig. 2, VoxDet contains two training stages: reconstruction and instance detection.

**Reconstruction** To learn the 3D geometry relationships, specifically 3D rotation between instance templates, we pre-train the implicit mapping function \(()\) using a reconstruction objective. Wedivide \(M\) multi-view templates \(^{}\) into input images \(^{}_{}^{(M-K) 3 W H}\) and outputs \(^{}_{}^{K 3 W H}\). Next, we construct the voxel representation \(^{}\) using \(^{}_{}\) via the TVA module and adopt a decoder network \(\) to reconstruct the output images through the relative rotations:

\[}^{}_{}=(( ^{},^{}_{j}))\,j\{1,2,,K\}\,\] (4)

where \(}^{}_{}\) denotes the \(j\)-th reconstructed (fake) output images and \(_{j}\) is the relative rotation matrix between the 1-st to \(j\)-th camera frame. We finally define the reconstruction loss as:

\[_{}=w_{}_{}+w_{ }_{}+w_{}_{ }\,\] (5)

where \(_{}\) denotes the reconstruction loss, _i.e._, the L1 distance between \(^{}_{}\) and \(}^{}_{}\). \(_{}\) is the generative adversarial network (GAN) loss, where we additionally train a discriminator to classify \(^{}_{}\) and \(}^{}_{}\). \(_{}\) means the perceptual loss, which is the L1 distance between the feature maps of \(^{}_{}\) and \(}^{}_{}\) in each level of VGGNet . Even though the reconstruction is only supervised on training instances, we observe that it can roughly reconstruct novel views for unseen instances. We thus reason that the pre-trained voxel mapping can roughly encode the geometry of an instance.

**Detection base training** : In order to empower \(()\) with geometry encoding capability, we initialize it with the reconstruction pre-trained weights and conduct the instance detection training stage. In addition to the open-world detection loss , we introduce the instance classification loss \(^{}_{}\) and rotation estimation loss \(^{}_{}\) to superrise our VoxDet.

We define \(^{}_{}\) as the binary cross entropy loss between the true labels \(\{0,1\}^{N}\) and the predicted scores \(}^{N 2}\) from the QVM module. The rotation estimation loss is defined as:

\[^{}_{}=\|}^{Q}^{ Q}-\|\,\] (6)

Figure 3: The instances and test scenes in the newly built RoboTools benchmark. The 20 unique instances are recorded as multi-view videos, where the relative camera poses between frames are provided. RoboTools consists of various challenging scenarios, where the desired instance could be under severe occlusion or in different orientation.

where \(^{Q}\) is the ground-truth rotation matrix of the query voxel. Note that here we only supervise the positive samples. Together, our instance detection loss is defined as:

\[_{}=w_{1}_{}^{}+w_{2} _{}^{}+w_{3}_{}^{ }+w_{4}_{}^{}+w_{5} _{}^{}+w_{6}_{}^{}\,,\] (7)

**Remark 1**: In both training stages, we only use the training objects, \(_{}\). During inference, VoxDet doesn't need any further fine-tuning or optimization for \(_{}\).

## 4 Experiments

### Implementation Details

Our research employs datasets composed of distinct training and test sets, adhering to \(=\) to ensure no overlap between semantic classes of \(\) and \(\).

**Synthetic Training set:** In response to the scarcity of instance detection traing sets, we've compiled a comprehensive synthetic dataset using 9,901 objects from ShapeNet  and ABO . Each instance is rendered into a 40-frame, object-centric \(360^{}\) video via Blenderproc . We then generate a query scene using 8 to 15 randomly selected objects from the entire instance pool, each initialized with a random orientation. This process yielded 55,000 scenes with 180,000 boxes for training and an additional 500 images for evaluation, amounting to 9,800 and 101 instances respectively. We've termed this expansive training set "open-world instance detection" (OWID-10k), signifying our model's capacity to handle unseen instances. To our knowledge, this is the first of its kind.

**Synthetic-Real Test set:** We utilize two authoritative benchmarks for testing. LineModOcclusion  (LM-O) features 8 texture-less instances and 1,514 box annotations, with the primary difficulty being heavy object occlusion. The YCB-Video  (YCB-V) contains 21 instances and 4,125 target boxes, where the main challenge lies in the variance in instance pose. These datasets provide real test images while lacks the reference videos, we thus render synthetic videos using the CAD models in Blender.

**Fully-Real Test set:** To test the sim-to-real transfer capability of VoxDet, we introduced a more complex fully real-world benchmark, RoboTools, consisting of 20 instances, 9,109 annotations,

    &  &  &  \\ Method &  & Train &  & _{50}\)} & _{55}\)} &  & _{50}\)} & _{75}\)} & _{95}\)} &  & _{50}\)} & _{75}\)} &  \\ 
**VoxDet** & OLN* & OWID & **29.2** & **43.1** & **33.3** & **0.8** & **31.5** & **51.3** & **33.4** & **1.7** & **30.4** & **47.2** & **33.4** & **6.5** \\ OLN\({}_{}\) & OLN* & OWID & 22.3 & 34.4 & 24.7 & 0.5 & 24.8 & 41.1 & 26.1 & 0.7 & 23.6 & 37.8 & 25.4 & 5.5 \\ DTOD  & N/A & OWID & 9.8 & 28.9 & 3.7 & +0.1 & 16.3 & 48.8 & 4.2 & +0.1 & 13.1 & 38.9 & 4.0 & 2.8 \\ OSED  & N/A & OWID & 0.2 & 0.7 & 0.1 & -0.1 & 5.2 & 18.3 & 1.9 & -0.1 & 2.7 & 9.5 & 1.0 & 5.3 \\  OLN\({}_{}\) & OLN & OWID* & 16.2 & 32.1 & 15.3 & 0.5 & 10.7 & 25.4 & 7.3 & 0.2 & 13.5 & 28.8 & 11.3 & 2.8 \\ OLN\({}_{}\) & OLN & OWID* & 23.6 & 41.6 & 24.8 & 0.6 & 25.6 & 53.0 & 21.1 & 0.8 & 24.6 & 47.3 & 23.0 & 2.8 \\ Gen6D  & N/A & OWID* & 12.0 & 29.8 & 6.6 & -0.1 & 12.1 & 37.1 & 5.2 & -0.1 & 12.1 & 33.5 & 5.9 & 1.3 \\ BHRL  & N/A & COCO & 14.1 & 21.0 & 15.7 & 0.5 & 31.8 & 47.0 & 34.8 & 1.4 & 23.0 & 34.0 & 25.3 & N/A \\   

Table 1: Overall performance comparison on synthetic-real datasets LM-O  and YCB-V . Compared with various 2D methods, including correlation , attention , and feature matching [9; 19], our VoxDet holds superiority in both accuracy and efficiency. OLN* means the open-world object detector (OW Det.)  is jointly trained with the matching head while OLN denotes using fixed modules. \({}^{}\) the model is trained on both synthetic dataset OWID and real images.

   Method/Module & Open-World Det. & Matching & ToTal \\ 
**VoxDet** & & **0.032** & **0.154** \\ OLN\({}_{}\) & & 0.248 & 0.370 \\ OLN\({}_{}\) & 0.122 & 0.235 & 0.357 \\ OLN\({}_{}\) & & 0.060 & 0.182 \\   

Table 3: Per module efficiency comparison. All the four methods share the same open-world detector . Compared with 2D baselines that adopt cosine similarity [9; 19] or learnable correlation , our Voxel matching is more efficient, which shows \(\) 2\(\) faster speed. The numbers presented below are measured in seconds.

and 24 challenging scenarios. The instances and scenes are presented in Fig. 3. Compared with existing benchmarks [17; 18], RoboTools is much more challenging with more cluttered backgrounds and more severe pose variation. Besides, the reference videos of RoboTools are also real-images, including real lighting conditions like shadows. We also provide the ground-truth camera extrinsic.

**Baselines:** Our baselines comprise template-driven instance detection methods, such as correlation  and attention-based approaches . However, these methods falter in cluttered scenes, like those in LM-O, YCB-V, and RoboTools. Therefore, we've self-constructed several 2D baselines, namely, \(_{}\), \(_{}\), and \(_{}\). In these models, we initially obtain open-world 2D proposals via our open-world detection module . We then employ different 2D matching methods to identify the proposal with the highest score. In \(_{}\) and \(_{}\), we leverage robust features from pre-trained backbones [9; 19] and use cosine similarity for matching. 1 For \(_{}\), we designed a 2D matching head using correlation as suggested in . These open-world detection based 2D baselines significantly outperform previous methods [5; 6]. In addition to these instance-specific methods, we also include a class-level one-shot detector, OS2D  and BHRL  for comparison.

**Hardware and configurations:** The reconstruction stage of VoxDet was trained on a single Nvidia V100 GPU over a period of 6 hours, while the detection training phase utilized four Nvidia V100 GPUs for a span of \(\)40 hours. For the sake of fairness, we trained the methods referenced [5; 6; 7; 14; 19; 9] mainly on the OWID dataset, adhering to their official configuration. Inferences were conducted on a single V100 GPU to ensure fair efficiency comparison. During testing, we supplied each model with the same set of \(M=10\) template images per instance, and all methods employed the top \(N=500\) ranking proposals for matching. In the initial reconstruction training stage, VoxDet used 98% of all 9,901 instances in the OWID dataset. For each instance, a random set of \(K=4\) images were designated as output \(_{}^{}\), while the remaining \(M-K=6\) images constituted the inputs \(_{}^{}\). For additional configurations of VoxDet, please refer to Appendix A and our code.

**Metrics:** Given our assumption that only one desired instance is present in the query image, we default to selecting the Top-1 proposal as the predicted result. We report the average recall (AR) rate  across different IoU, such as mAR (\( 0.5 0.95\)), AR\({}_{50}\) (\(\;0.5\)), AR\({}_{75}\) (\(\;0.75\)), and AR\({}_{95}\) (\(\;0.95\)). Note that the AR is equivalent to the average precision (AP) in our case.

### Quantitative Results

**Overall Performance Comparison:** On the synthetic real datasets, we comprehensively compare with all the potential baselines, the results are detailed in Table 1, demonstrating that VoxDet consistently delivers superior performance across most settings. Notably, VoxDet surpasses the

Figure 4: Number of templates analysis of VoxDet and 2D baseline, \(_{}\)[14; 9] on YCB-V benchmark. Thanks to the learned geometry-aware 2D-3D mapping, VoxDet can work well with very few reference images, while 2D method suffers from such setting, dropping up to \(\%\).

Figure 5: Top-K analysis of VoxDet and One-shot object detector . By virtue of the instance-level matching method, QVM, VoxDet can better classify the proposals, so that \(90\%\) of the true positives lie in Top-10, while for OS2D, this ratio is only \(60\%\).

next best baseline, \(_{}\), by an impressive margin of up to **20\(\%\)** in terms of average mAR. Furthermore, due to its compact voxel representation, VoxDet is observed to be markedly more efficient. On the newly built fully real dataset, RoboTools, we only compare methods trained on the same synthetic dataset for fairness. As shown in Table 2, VoxDet demonstrates better sim2real transfering capability compared with the 2D methods due to its 3D voxel representation. We present the results comparison with the real-image trained models in Appendix D.

**Efficiency Comparison:** As QVM has a lower model complexity than \(_{}\) and \(_{}\), it achieves faster inference speeds, as detailed in Table 3. Compared to correlation-based matching , VoxDet leverages the aggregation of multi-view templates into a single compact voxel, thereby eliminating the need for exhaustive 2D correlation and achieving **2\(\)** faster speed.

In addition to inference speed, VoxDet also demonstrates greater efficiency regarding the number of templates. We tested the methods on the YCB-V dataset  using fewer templates than the default. As illustrated in Fig. 4, we found that the 2D baseline is highly sensitive to the number of provided references, which may plummet by **87\(\%\)** when the number of templates is reduced from 10 to 2. However, such a degradation rate for VoxDet is **2\(\)** less. We attribute this capability to the learned 2D-3D mapping, which can effectively incorporate 3D geometry with very few views.

**Top-K Analysis:** Compared to the category-level method , VoxDet produces considerably fewer false positives among its Top-10 candidates. As depicted in Fig. 5, we considered Top-\(K=1,5,10,20,30,50,100\) proposals and compared the corresponding AR between VoxDet and OS2D . VoxDet's AR only declines by \(5 10\%\) when \(K\) decreases from 100 to 10, whereas OS2D's AR suffers a drop of up to \(38\%\). This suggests that over \(90\%\) of VoxDet's true positives are found among its Top-10 candidates, whereas this ratio is only around \(60\%\) for OS2D.

**Ablation Studies:**

The results of our ablation studies are presented in Table 4. Initially, we attempted to utilize the 3D depth-wise convolution for matching (see the fourth row). However, this proved to be inferior to our proposed instance-level voxel relation. Reconstruction pre-training is crucial for VoxDet's ability to learn to encode the geometry of an instance (see the last row). Additionally, we conducted an ablation on the rotation measurement module (R) in the QVM, and also tried not supervising the predicted rotation. Both are inferior to our default settings.

   Recon. & R & w/ sup. & Voxel Rel. & mAR & AR\({}_{50}\) & AR\({}_{75}\) \\  ✓ & ✓ & ✓ & ✓ & **18.7** & **23.6** & **20.5** \\ ✓ & ✓ & ✗ & ✓ & 18.2 & 23.2 & 20.0 \\ ✓ & ✗ & ✗ & ✓ & 15.6 & 21.9 & 17.0 \\ ✓ & ✓ & ✓ & ✗ & 15.1 & 19.4 & 16.2 \\ ✗ & ✓ & ✓ & ✓ & 14.2 & 18.3 & 15.7 \\   

Table 4: Ablation study for VoxDet in RoboTools benchmark. All the three critical modules are helpful in our design. Supervising the estimated rotation achieves slightly better results. Comparison with more matching module see Appendix B.

Figure 6: Detection qualitative results comparison between VoxDet and 2D baselines on the three benchmarks. VoxDet shows better robustness under pose variance (_e.g._ Obj. 5@LM-O first and second columns) and occlusion (_e.g._ Obj. 13@YCB-V second column and Obj. 9@RoboTools).

### Qualitative Results

#### 4.3.1 Detection Visualization

The qualitative comparison is depicted in Fig. 6, where we compare VoxDet with the two most robust baselines, \(\) and \(\).. We notice that 2D methods can easily falter if the pose of an instance is not seen in the reference, e.g., 2-nd query image in the 1-st row, while VoxDet still accurately identifies it. Furthermore, 2D matching exhibits less robustness under occlusion, where the instance's appearance could significantly differ. VoxDet can effectively overcome these challenges thanks to its learned 3D geometry. More visualizations and qualitative comparisons see Appendix C.

#### 4.3.2 Deep Voxels Visualization

To better validate the geometry-awareness of our learned voxel representation, we present the deep visualization in Fig. 7. The gradient of the matching score is backpropagated to the template voxel and we visualze the activation value of each grid. Surprisingly, we discover that as the orientation of the query instance changes, the activated regions within our voxel representations accurately mirror the true rotation. This demonstrates that the voxel representation in VoxDet is aware of the orientation of the instance.

#### 4.3.3 Reconstruction Visualization

The voxel representation in VoxDet can be decoded to synthesize novel views, even for unseen instances, which is demonstrated in Fig. 8. The voxel, pre-trained on 9500 instances, is capable of approximately reconstructing the geometry of unseen instances.

## 5 Discussions

#### 5.0.1 Conclusion:

This work introduces VoxDet, a novel approach to detect novel instances using multi-view reference images. VoxDet is a pioneering 3D-aware framework that exhibits robustness to occlusions and pose variations. VoxDet's crucial contribution and insight stem from its geometry-aware Template Voxel Aggregation (TVA) module and an exhaustive Query Voxel Matching (QVM) specifically tailored for instances. Owing to the learned instance geometry in TVA and the meticulously designed matching in QVM, VoxDet significantly outperforms various 2D baselines and offers faster inference speed. Beyond methodological contributions, we also introduce the first instance detection training set, OWID, and a challenging RoboTools benchmark for future research.

#### 5.0.2 Limitations:

Despite its strengths, VoxDet has two potential limitations. Firstly, the model trained on the synthetic OWID dataset may exhibit a domain gap when applied to real-world scenarios, we present details in Appendix D. Secondly, we assume that the relative rotation matrixes and instance masks (box) for the reference images are known, which may not be straightforward to calculate. However, the TVA module in VoxDet doesn't require an extremely accurate rotation and 2D appearance. We present further experiments addressing these issues in Appendix E.

Figure 8: Reconstruct results of VoxDet on unseen instances. The voxel representation in VoxDet can be decoded with a relative rotation and synthesize novel views, which demonstrate the geometry embedded in our learned voxels.

Figure 7: Visualization of the high activation grids during matching. As query instance rotates along a certain axis, the location of the high-activated grids roughly rotates in the corresponding direction.