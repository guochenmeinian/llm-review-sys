# Calibrating

"Cheap Signals" in Peer Review without a Prior

 Yuxuan Lu

Center on Frontiers of Computing Studies

School of Computer Science

Peking University

Beijing, China

yx_lu@pku.edu.cn

&Yuqing Kong

Center on Frontiers of Computing Studies

School of Computer Science

Peking University

Beijing, China

yuqing.kong@pku.edu.cn

Corresponding author.

###### Abstract

Peer review lies at the core of the academic process, but even well-intentioned reviewers can still provide noisy ratings. While ranking papers by average ratings may reduce noise, varying noise levels and systematic biases stemming from "cheap" signals (e.g. author identity, proof length) can lead to unfairness. Detecting and correcting bias is challenging, as ratings are subjective and unverifiable. Unlike previous works relying on prior knowledge or historical data, we propose a one-shot noise calibration process without any prior information. We ask reviewers to predict others' scores and use these predictions for calibration. Assuming reviewers adjust their predictions according to the noise, we demonstrate that the calibrated score results in a more robust ranking compared to average ratings, even with varying noise levels and biases. In detail, we show that the error probability of the calibrated score approaches zero as the number of reviewers increases and is significantly lower compared to average ratings when the number of reviewers is small.

## 1 Introduction

Peer review is fundamental to the scientific process. However, as noted in Rennie :

_But it is a human system. Everybody involved brings prejudices, misunderstandings, and gaps in knowledge, so no one should be surprised that peer review is often **biased** and inefficient... even with **the best of intentions**..._

Let us consider the following scenario.

**Example 1** (Peer review).: _We have two papers and each paper is reviewed by 5 reviewers. We can only accept one paper. The first paper receives 4 "accept" and 1 "reject", the second paper receives 3 "accept"and 2 "reject"._

A typical choice is to accept the first paper since it receives more fraction of "accept". Ideally, we should accept the paper which receives more fraction of "accept" in expectation from reviewers who have full expertise and invest full efforts, called the clean setting. However, in practice, the ratings can be noisy due to the lack of expertise or effort. If the noise is consistent for the two papers, the better paper should still receive more fraction of "accept" in expectation. Nevertheless, the noise can be different for different papers.

**Example 2** (Hot vs. cold topic).: _The topic of the first paper is more popular than the second paper. In this case, the first paper is easier to obtain reviewers with high expertise. Thus, in the noisy setting, the first paper has less noisy ratings than the second paper._It is possible that in the clean setting, the first paper receives 80% of "accept" in expectation and the second paper receives 90%. However, in the noisy setting, the first paper may still obtain 80% "accept" in expectation while the second paper may only obtain 60% "accept" in expectation because it receives more noisy ratings. In addition to the noise level, the noise can be systematically biased due to the existence of "cheap signals".

**Example 3** (Long proof vs. short proof).: _The first paper has a complicated proof. The second paper has a short proof. In the noisy setting, each reviewer (unconsciously) intends to vote for "accept" for paper with a long proof, even if the length of the proof is a "cheap signal"._

It may occur in practice that the second paper is better in the clean setting, but the first paper receives more fraction of "accept" in expectation in the noisy setting, due to the **systematically biased** noise caused by cheap signals. Cheap signals can also be the authors' reputation, institutions, or writing skills. One may argue that the cheap signals may correlate with expensive signals, which represent the true quality of the paper. However, these cheap signals like proof length can be easily manipulated.

Prior knowledge and historical data may help to calibrate the noise. However, the identification of bias can be subjective and ad hoc. It will be more applicable if we do not need prior knowledge and design a general **one-shot** process. A fundamental question arises:

_*Can we design a one-shot scoring process that leads to a noise-robust rank without any prior knowledge? Formally, we want the paper with a higher expected score in the clean setting also have a higher score in the noisy setting with high probability, even if different papers' reviews have different noise levels and biases._

A key challenge is that the ratings are subjective and unverifiable. As an outsider without any prior knowledge, we do not even know whether the bias exists or not. Therefore, it is impossible to address the above question without eliciting any additional information from the reviewers. Prelec  propose an innovative method, Surprisingly Popular (SP), to aggregate the information despite agents' systematic biases without any prior. Here each agent is asked to report both her signal (e.g. accept or reject) and her prediction (e.g. 90% reject, 10% accept) for a random peer's report. The elicited predictions are used to construct the prior. Instead of a majority vote, the aggregation is the signal which is more popular compared to the prior2. By assuming that the ground truth is positively correlated to each agent's signal3, the aggregation is correct when the size of the crowds is sufficiently large.

We adopt the signal-prediction framework of SP, i.e., asking each reviewer to additionally provide her prediction for a randomly selected reviewer's report. Besides, we follow the literature of information elicitation  and information aggregation  to assume that agents are perfect Bayesian, which establishes a start for theoretical analysis:

**Assumption 1** (Bias generated from "cheap signals" can be predicted).: _(Informal) We assume agents are perfect Bayesian. Besides, when their ratings are noisy and have systematic biases, they will adjust their predictions based on the biased noise._

The above assumption essentially provides a general definition of the bias. With this assumption, we aim to design a one-shot scoring process with the reviewers' predictions as calibrations.

We cannot directly apply SP to this scenario, not only because it requires a large number of agents, but also because it is designed to solve a problem with objective ground truth. In the peer review setting, each paper's ratings are subjective and do not need to be the same even in the clean setting. We may ask the reviewer to compare two papers and apply SP to aggregate the comparisons. However, in practice, papers are reviewed independently by a possibly different set of reviewers.

Moreover, it is possible that both two papers' surprisingly popular ratings are "accept". To rank them, one idea is to compare the "amount" of surprise. For example, SP calibrates the ratings by dividing the prior, where the prior is constructed by the predictions. Subtracting the prior is another option. However, neither of the two calibration options is robust to the noise. In Example 2, when the prior is 50% / 50% for both the clean setting and the noisy setting, both of the two papers' surprisingly popular ratings are "accept". However, the second paper's "accept" is less popular compared to the prior because its ratings have more noise, while the second paper is better in the clean setting.

We formulate the above problem into a formal mathematical model, defining a paper's true quality as its expected rating in the clean setting without noise, and assuming that the noise is non-degenerate and each reviewer's received noisy signal positively correlates with her clean signal4.

Our ResultsWithin the model and assumptions, we propose a one-shot scoring process. When the number of reviewers is infinite, a paper with a higher score in the noisy setting will also have a higher true quality even if the papers have different biased noises. When the number of reviewers is finite, we provide a bound for the error probability and numerically show that our method beats the baseline, the average rating, in various situations. We also extend the results to non-binary setting naturally, with proper restrictions on the noise structures. Of an independent interest, we also provide surprisal measures that are invariant to noises.

High-level IdeasThe baseline score is the average rating, e.g., the fraction of "accept" votes. In this paper, we define the Surprisal-based Score in the form of \(}{}\), where the correlation is measured by a power of the determinant of the joint distribution matrix between two reviewers' ratings. Intuitively, the subtraction of the prior will cancel the effect of the systematic bias, and the division of the correlation will compensate for varying noise levels across contexts. For example, when faced with noisy reviewers providing weak signals beyond the prior, the average rating and the prior will closely align, resulting in a small correlation between ratings. By normalizing using this small correlation, the surprisal score is scaled up to compensate for the strong noise.

Three Reviewers, Binary RatingsTo illustrate our scoring process, we provide an example for the case of three reviewers, binary ratings.

**Example 4** (Three reviewers, binary ratings).: _In the setting of binary ratings, the signal of reviewers is either "accept" (1) or "reject" (0). We call the reviewer who votes for "accept" as positive reviewer and the reviewer who votes for "reject" as negative reviewer. We map 3 "accept" signals to the highest score, and 3 "reject" signals to the lowest score. When there are at least an "accept" and a "reject", we construct a matrix \(}\), where \(_{s,t}\) is the average prediction, from the reviewers who report signal \(s\), for the probability that a random reviewer votes for signal \(t\). For example, \(_{0,1}\) is the average of the negative reviewers' predictions for the probability that a random reviewer votes for "accept"._

_Specifically, the score is defined as_

\[()\!=\!(\!-\!_{1})_{1}_{0}(_{1,1}\!-\!_{0,1})^{-}&\\ (\!-\!_{1})_{1}_{0}(_{1,1}\!-\! _{0,1})^{-}&\\ -&\]

_where \(_{1}\!=\!_{1,1}}{_{0,1}\!+\!_{1,0}}\) and \(_{0}\!=\!_{1,0}}{_{0,1}\!+\!_{1,0}}\)._

_Figure 1 illustrates how the score depends on the reviewers' predictions. At a high level, the score will be higher when the reviewers underestimate the probability that a random reviewer rates "accept" (the lower left corner)5._

To the best of our knowledge, we are the pioneer to rigorously formulate the noise-robust comparison problem with a one-shot scoring process, when different rating tasks have different noises. We believe our work provides a start in the peer review setting for calibrating the bias without any prior.

### Related Work

Bias in Peer ReviewStudies on peer review originated with Mahoney  and Hess . Since then, numerous studies have emerged to identify sources and impacts of bias in peer review . In addition to exploring sources and impacts, recent studieshave attempted to detect or calibrate biases automatically, employing various methods such as optimization, nonparametric estimation, and mechanism design [14; 15; 16; 17; 18; 19; 20]. However, these methods rely on historical data or a reviewer's votes for multiple papers.

Mechanism Design via Second-order InformationPrelec  introduced the signal-prediction framework where agents are asked to provide both their answers and predictions. By applying the signal-prediction framework, researchers have discovered methods to aggregate people's answers to the surprisingly popular option , reduce bias by agents' predictions about ranking , incentivise truthful agents[3; 22; 23], and find more informative answers [24; 25]. In addition to the discrete setting, recently, a growing literature [26; 27; 28; 29; 30; 31] has extended the signal-prediction framework to forecast aggregation settings. In this work, we employ second-order information for noise-robust comparison in a different setting.

## 2 Background and Problem Statement

### Background: Single-task Information Elicitation

In this section, we describe how agents receive private signals and generate predictions, following the standard signal-prediction framework [2; 3].

ModelWe regard each paper as a task with a _state_\(\!\!^{1||}\), where \(\) is the set of all possible _signals_. There are \(n\)_agents_ (reviewers) assigned to the task6, labeled from \(1\) to \(n\). Each agent \(i\!\![n]\) receives private signal \(X_{i}\!=\!s\) with probability \(w_{s}\). The notation \(x_{i}\) is the realization of \(X_{i}\), representing the signal actually received by agent \(i\). In this paper, we use the notation \(s\) or \(t\) to refer to a particular signal and the notation \(i\) or \(j\) to refer to a particular agent. There is a prior distribution \(Q\) over the states and \(W\) is a random state drawn from the prior distribution \(Q\). We use a row vector \(\!\![0,\!1]^{1||}\) to denote the prior probability that an agent will receive each signal, i.e., for all \(s\!\!\), \(q_{s}\!=\!_{Q}[X_{i}\!=\!s]\!=\!_{}\!_{Q}[W\!=\! ]w_{s}\). Analogously, \(\!\![0,\!1]^{||||}\) denotes the prediction matrix such that \(P_{s,t}\!=\!_{Q}[X_{j}\!=\!t|X_{i}\!=\!s],i\!\!j\) is the probability that agent \(j\) will receive the signal \(t\) conditioning on another agent \(i\) receives the signal \(s\). The joint distribution matrix \(\!\![0,\!1]^{||||}\) is the matrix where \( s,t\!\!,\!U_{s,t}\!=\!q_{s}P_{s,t}\!=\!_{Q}[X_{i}\!=\!s,X_ {j}\!=\!t]^{7}\). Because agents' identities do not matter, we omit \(i,j\!\![n]\) in the definition. To avoid degeneration, we assume the determinant of \(\) is non-zero. In the task, each agent \(i\) is asked to report her signal \(s_{i}\) (e.g. accept) and her prediction \(_{i}\) for a random agent's signal (e.g. 40% accept, 60% reject) without any communication to other agents.

The above model describes how agents receive signals **without noise**, which we call the clean setting. Figure 2 illustrates the model briefly and we offer a detailed example in Appendix A. We will introduce the noise model later.

Figure 1: Contours of \(S\)(ratings,predictions): We fix ratings (left: 1 accept, 2 reject; right: 1 reject, 2 accept) and illustrate the relationship between the score and the reviewers’ predictions for the probability that a random reviewer votes for “accept”.

Model InferenceFor each agent \(i\!\![n]\), given her private signal \(s_{i}\), we assume that her prediction \(_{i}\) is the Bayesian posterior, i.e., \(\!t\!\!,_{i}(t)\!=\!P_{s_{i},t}\). If for all signal \(s\!\!\), there exists an agent who reports \(s\), then we can obtain the prediction matrix \(\) according to the agents' predictions. We can use \(\) to induce the joint distribution matrix \(\) where \(\!s,\!t\!\!\),\(U_{s,t}\!=\!q_{s}P_{s,t}\!=\!_{Q}[X_{i}\!=\!s,\!X_{j}\!=\!t]\).

**Claim 1** (Prelec et al. ).: _We can construct the vector \(\) and the joint distribution matrix \(\) from the prediction matrix \(\): \(\!s,\!t\!\!\),\(q_{s}\!=\!(_{t}}{P_{t,s}})^{-1}(0/0\!=\!0)\),\(U_{s,t}\!=\!q_{s}P_{s,t}\)._

In this paper, we will assume the reviewers have good intentions and will tell the truth. That is, if she receives an "accept" signal for a paper, she will vote for "accept" as well. Regarding incentive design, prediction reports can be incentivized by proper scoring rules . For the signal report, Prelec  develops a payment method called Bayesian Truth Serum (BTS) to incentivize agents to tell the truth when the number of agents is infinite. By assuming that agents perform the same strategy, Kong et al.  propose a method that only requires the number of agents greater or equal \(6\).

### Problem Statement

Noise ModelIn the noisy setting, instead of observing the original signal \(s\), which we call the _clean signal_, each agent can only observe its noisy version \(M(s)\), which we call the _noisy signal_, where \(M\!:\!\!\!\) is a random operator. Denote \(\) as the matrix where \(M_{s,t}\) is the probability of observing noisy signal \(t\) given clean signal \(s\). We only consider the non-degenerate noise whose noise matrix \(\) is invertible. Additionally, we focus on the homogeneous noise which corresponds to systematic biases. Homogeneous noise means that the noise is the same for all reviewers of a specific paper.

In the noisy setting, we use the notation \(\) to denote the prior distribution over the noisy states. Similar to the clean setting, \(_{i}\) denotes the random variable of agent \(i\)'s noisy signal and \(_{i}\) is its realization; \(}\) denotes the joint distribution matrix where \(_{s,t}\!=\!_{}[_{i}\!=\!s,_{j}\!=\!t]\); \(}\) denotes the noisy state; \(}\) denotes the noisy prior over signals and \(}\) denotes the noisy prediction matrix. Note that the relationship between \(}\), \(}\) and \(}\) remains the same. By abusing notation a little bit, we sometimes write \(\!=\!M(Q),}\!=\!M(),}\!=\!M( )\). Figure 3 briefly introduces how noise affects reviewers' ratings, and we offer a detailed example of noise model in Appendix A.

**Assumption 2**.: _In the noisy setting, we assume that for each agent \(i\), given her private noisy signal \(s_{i}\), her prediction \(}_{i}\) is still the Bayesian posterior where \(\!t\!\!\),\(}_{i}(t)\!=\!_{}[X_{j}\!=\!t|X_{i}\!=\!s_{i}]\!=\! _{s_{i},t}\)._

Ideally, we want to reconstruct the original state \(\) from the noisy \(}\) and \(}\). However, we do not have a sufficient amount of information. Instead, we aim to solve the following problem.

Noise-Robust ComparisonWe have two rating tasks, say reviews for paper A and B, whose clean states are \(_{A}\) and \(_{B}\) respectively. Both \(_{A}\) and \(_{B}\) follow the distribution \(Q\). There is a known one-to-one mapping \(\!:\!\!\!\) between signals in \(\) and paper scores. For example, when ratings are binary, the mapping \(\) is \(\{\!\!0,\,\!\!1\}\).

Figure 2: **Model without noise**: \(Q\) is the prior distribution over the papers’ states. In the figure, there are three types of states, each of which shows up with probability \(\). A random paper’s state follows the distribution \(Q\). When the paper’s state realizes as \(\!=\![0.2,0.8]\), a random reviewer of this paper will vote for “reject” with probability \(0.2\) and vote for “accept” with probability \(0.8\).

Therefore, for simplicity, we sometimes write signal \(s\) as a score of real number and \(\) as a set of scores. For example, in the binary case, \(\!=\!\{0(),\!1()\}\). We care about the _true quality_ which is defined as the expected score in the clean state, i.e., \(_{}\!\!=\!_{s}\!(s)w_{s}\). For example, in the binary case, the quality is \(_{}\!=\!w_{1}\), i.e., the probability of a random reviewer voting for "accept".

Paper A has \(n_{A}\) reviewers and paper B has \(n_{B}\) reviewers. They do not need to have the same number of reviewers. The pair \((_{i}^{A},}_{i}^{A})_{i[n_{A}]}\) denotes the reviewers' actual ratings and predictions for paper A, and \((_{j}^{B},}_{j}^{B})_{j[n_{B}]}\) analogously. Let \(S()\) be a function that takes reviewers' noisy ratings and predictions of a single paper as input, and outputs a calibrated score for the paper. For a pair of papers, we will rank the paper according to their calibrated scores8. Given a set of noises \(\), we aim to design \(S()\) such that for all \(M_{A},M_{B}\), the _error probability_\([S()\) ranks A higher than B\(\)B's quality is better than A's\(\) is upper-bounded and the upper bound goes to \(0\) when both \(n_{A}\) and \(n_{B}\) go to infinity.

## 3 Invariant Surprisal Vector

This section will introduce the key ingredient of our scoring process, a surprisal vector which is invariant to a natural family of noises. We first introduce this family of noises. Here each individual receives the clean signal with probability \(1\!-\!\!>\!0\). With probability \(\), which we call noise level, each individual receives a signal according to a distribution vector \(\!\!_{}\), which we call bias. We state the noise formally in the following definition.

**Definition 1** (A Family of Noises \(^{*}\)).: _We consider a family of noises with two parameters, a noise level \(\!\![0,\!1)\) and a bias vector \(\!\!_{}\), where \(_{}\) is the probability simplex on the signal set \(\). Let \(\) denote the matrix whose rows are all \(\). The noise is defined as \(_{,}\!=\!(1\!-\!)\!+\! \)._

**Claim 2**.: _The noisy state \(}\!=\!(1\!-\!)\!+\!\) is a convex combination of the clean state \(\) and the bias vector \(\). In the binary case (\(\!=\!\{0(),\!1()\}\)), \(^{*}\) is the set of all non-degenerate noises where \(M_{1,1}\!>\!M_{0,1}\)._

Then, we define a surprisal vector that is invariant to the above noises.

**Definition 2** (\(^{*}\)-Invariant Surprisal Vector).: _Given paper's state \(\) and joint distribution \(\), we define the \(^{*}\)-invariant surprisal vector as_

\[^{*}(,)\!=\!()^{- {2(2\!-\!1)}}(\!-\!),\]

_where \(\!\![0,\!1]^{1||}\) denotes the marginal distribution of \(\)._

Figure 3: **Model with noise**

**Claim 3**.: _In the binary case (\(=\{0(),1()\}\)), the \(^{*}\)-invariant surprisal vector can be simplified as_

\[^{*}(,)=-}{ q_{1}(P_{1,1}\!-\!P_{0,1})}}.\]

**Theorem 1** (Invariance).: _The \(^{*}\)-invariant surprisal is non-negative, vanishes when the state \(\) equals the prior \(\), and is invariant to \(_{,}\) for all \(\!\![0,1)\), \(\!\!_{}\). The implication of invariance is that for any noise with \(\!\![0,1)\), \(^{*}(,)\!=\!^{*}( },})\)._

The proofs of Claim 2, Claim 3 and Theorem 1 are deferred to Appendix C. For general noise, the direction of \(\!-\!\) may not be invariant. For example, one possible noise is flipping the ratings, i.e., rating "accept" when "reject" and rating "reject" when "accept". In this case, the direction of \(\!-\!\) will be flipped as well. However, if we only care about the amount of surprisal, we successfully design a measure that is invariant to all possible non-degenerate noise and show that it can be used to identify the true state \(\) in some scenarios (see Appendix B).

## 4 Surprisal-based Score

This section introduces our scoring process based on the surprisal vector introduced in Section 3. We first introduce the score based on "wishful thinking", i.e., when we have the state (or the noisy state). We then resolve this "wishful thinking" by providing an estimation of the state.

**Definition 3** (Surprisal-based Score).: _Given \(\) and \(\), we define the \(^{*}\)-invariant Surprisal-based Score as_

\[^{*}(,)=(_{}- _{})()^{-}.\]

_According to the simplification in the binary case (Claim 3), the \(^{*}\)-invariant surprisal score in the binary case can be simplified to_

\[^{*}(,)=\!-\!q_{1}}{q_{1}(P_{1,1}\!-\!P_{0,1})}}.\]

Theorem 1 directly implies the following results: given the noisy states, we can compare their Surprisal-based Scores \(^{*}(,)\) and the comparison is invariant to the noise in \(^{*}\) and consistent to the comparison of their true qualities.

 
**Notation** & **Description** \\  \(Q,\) & \(Q\) denotes the distribution over the clean states. \(\) denotes the distribution over the noisy states. \\  \(,}\) & \(\) denotes the paper’s clean state. \(}\) denotes the paper’s noisy state. \\  \(X_{i},_{i}\) & \(X_{i}\) denotes a random variable indicating reviewer \(i\)’s clean signal for a paper which state follows \(Q\). \(X_{i},_{i}\) are their realizations respectively. \\  \(,}\) & \(\) denotes the prior joint distribution over two reviewers’ ratings, where \(U_{s,t}=_{Q}[X_{i}=s,X_{j}=t]\). \(}\) denotes its noisy version that \(_{s,t}=_{}[X_{i}=s,X_{j}=t]\). \\  \(,}\) & \(\) denotes the prior distribution vector over a reviewer’s vote, where \(q_{s}=_{Q}[X_{i}=s]\). \(}\) denotes its noisy version that \(_{s}=_{}[X_{i}=s]\). \(}\) denotes the prediction matrix, where \(P_{s,t}=_{Q}[X_{j}=t|X_{i}=s],i j\). \(}\) denotes its noisy version where \(_{s,t}=_{}[X_{j}=t|X_{i}=s],i j\). \\  \(n\) & \(n\) denotes the number of reviewers that are assigned for a paper. \\  \(\) & \(\) is the noise matrix defined by \(=(1\!-\!)+^{}\), where \(\) is the noise level and \(\) is the bias vector. \\  \(,}\) & The frequency vector of the ratings, where \(v_{s}=_{i}[_{i}=s]\). \(}\) denotes its noisy version that \\  \(^{*}()\) & \(^{*}()\) is our Surprisal-based Score calculated from \(\) and \(\). \\  \(()\) & \(()\) is our Empirical Surprisal-based Score calculated from \(n\) agents’ ratings and predictions. \\  

Table 1: Notation table

**Corollary 1** (Noise-invariant comparison by \(^{*}()\)).: _For any two states \(_{A},_{B}\) which follow distribution \(Q\), and any two noises \(M_{A},M_{B}\!\!^{*}\),_

\[^{*}(M_{A}(_{A}),\!M_{A}())\!-\!^{*}(M_{B}(_{B}),\!M_{B}())\] \[= ^{*}(_{A},)\!-\!^{*}( _{B},\!)\] \[ _{_{A}}\!-\!_{_{B }}.\]

When there are only a small number of reviewers, we will use an estimation of the Surprisal-based Score, which we call the Empirical Surprisal-based Score. We first focus on the binary case. We have already proved that for any invertible noise matrix \(\) where \(M_{1,1}\!>\!M_{0,1}\), the metric \(\!-\!q_{1}}{_{1}(P_{1,1}\!-\!P_{0,1}))}}\) can be used to implement a noise-invariant comparison (Claim 2, Corollary 1). We will use the reviewers' ratings and predictions to estimate the score. We employ a _frequency vector_\(}\!\!^{1||}\) to denote the frequency of the reviewers' reported signals. Formally, for all \(s\!\!\), \(_{s}\!=\!_{i=1}^{n}\![_{i}\!=\!s]\), where \(_{i}\) is the reported noisy signal of reviewer \(i\). For example, in the binary case, \(}\!=\!(_{0},_{1})\) where \(_{1}\) is the fraction of "accept" and \(_{0}\) is the fraction of "reject".

Empirical Surprisal-based Score (Binary)When there exists at least one "accept" and one "reject", we construct a \(2\!\!2\) prediction matrix \(}\) based on the reviewers' ratings and predictions. Each element of the matrix, \(_{s,t}\), is the average prediction from the reviewers who report the signal \(s\), for the probability that a random reviewer reports the signal \(t\). For example, \(_{0,1}\) is the average of the negative reviewers' prediction for the probability a random reviewer votes for "accept". When reviewers' ratings are the same, the matrix \(\) cannot be constructed. In this case, we set \(+\) score for the papers that all reviewers vote for "accept" and \(-\) for the papers that all reviewers vote for "reject". Formally,

\[((_{i},}_{i})_{i[n]})\!=\! -&_{1}\!=\!0\\ _{1}\!-\!_{1}}{_{0}_{1}(_{1,1}- _{0,1})}}&_{1}\!\!(0,\!1)\,,\\ +&_{1}\!=\!1\]

where \(_{1}\!=\!_{1,1}}{_{0,1}\!+\!_{1,0}}\) and \(_{0}\!=\!_{1,0}}{_{0,1}\!+\!_{1,0}}\)(See Claim 1 for the calculation of \(}\) from \(}\)).

We naturally extend the process to non-binary settings. For example, review signals contain multiple grades (\(\!=\!\{-2(),\!-\!1(),\!1(),\!2()\}\)).

Empirical Surprisal-based Score (General)When \(_{s}\!>\!0\; s\!\!\), we construct the prediction matrix \(}\). The calibrated score is defined as

\[((_{i},}_{i})_{i[n]})\!=\!( _{\!}\!-\!_{}})(})^{-}\,,\]

where \( s\!\!\),\(_{s}\!=\!(_{t}\!_{s,t}}{_{s,t}})^{-1}\) (\(0/\!0\!\!0\)). When \((})\!<\!0\), \(()\) remains undefined. This "bad event" arises when a reviewer favors a paper, her belief in the likelihood of another reviewer also favoring it decreases. This unusual scenario alerts the Program Committee that this particular paper warrants further discussion and careful consideration before making a final decision. When comparing tasks with undefined scores, our method degenerates to the baseline, i.e., comparing the value of \(_{}}\).

## 5 Theoretical Guarantee

In this section, we theoretically analyse the performance of our Empirical Surprisal-based Score. We use the _error probability_ to measure the performance. Recall that we use the expected score of a paper in the clean state to measure its true quality. In the binary case, the true quality of a paper, whose clean state is \(\), is \(w_{1}\), i.e., the fraction of "accept" ratings in the clean setting. The error probability is the probability that the score ranks paper A higher than paper B while B's quality is better than A's.

Theorem 2 (binary) and Theorem 3 (general) show the theoretical upper bound of the error probability of our method. When the number of reviewers goes to infinity, the error probability goes to zero. The analysis follows from the results of Corollary 1 and a standard concentration bound analysis. We defer the proofs of these two theorems to Appendix C.

**Theorem 2** (Error probability, binary case).: _For any pair of papers \(A,B\) whose clean states are \(^{A},^{B}\) correspondingly (without loss of generality, let \(w_{1}^{A}\!<\!w_{1}^{B}\)), given their noise matrices\(^{A},^{B}^{*}\) correspondingly, the error probability \([(A)>(B)|w_{1}^{A},w_{1}^{B}]+[(A )=(B)|w_{1}^{A},w_{1}^{B}]\) goes to \(0\) when the number of reviewers \(n_{A}\),\(n_{B}\) goes to infinity, and is bounded by_

\[_{1}^{A}^{n_{A}}+1-_{1}^{B}^{n_{B}}- _{1}^{A}^{n_{A}}_{1}^{B} ^{n_{B}}+1-_{1}^{A}^{n_{A}}1-_{1}^{B }^{n_{B}}+\{-^{B}-w_{1}^{A})^{2}}{ {n_{A}(1-_{A})^{2}}+(1-_{B})^{2}}}\},\]

_where \(_{A}\) is the noise level of paper A and \(_{B}\) is the noise level of paper B9._

The error bound has three terms. The first two terms bound the error probability conditioning on at least one of the papers having an infinite score, i.e., every reviewer votes for "accept" ("reject"). The last term bounds the error probability conditioning on both papers has at least an "accept" and a "reject" vote. This theorem shows that we can achieve a better error bound when the rating difference (\(w_{1}^{B}-w_{1}^{A}\)) between two papers is larger, when there are more reviewers (\(n_{A}\) and \(n_{B}\)), and when the noise levels (\(_{A}\) and \(_{B}\)) are lower.

**Theorem 3** (Error probability, general case).: _For any pair of tasks \(A,B\) whose clean states are \(^{A},^{B}\) correspondingly (without loss of generality, let \(_{^{A}}<_{^{B}}\)), given their noise matrices \(^{A},^{B}^{*}\) correspondingly, the error probability \([(A)>(B)|^{A},^{B}]+ [(A)=(B)|^{A},^{B}]\) goes to \(0\) when the number of reviewers \(n_{A}\),\(n_{B}\) goes to infinity, and is bounded by10_ 

## 6 Numerical Experiments

We perform numerical experiments to compare the performance of our Surprisal-based Score and the baseline score in the binary setting (\(=\{0,1\}\)). Recall that the baseline is the proportion of the "accept" ratings. Here we describe the parameters we select in numerical experiments.

1. **The number of agents \(n\)**: We perform the experiments in the settings of \(n=3\) and \(n=5\).
2. **The prior distribution of states \(Q\)**: In the binary case, there exists a one-to-one mapping between the state \(=(w_{0}=1-w_{1},w_{1})\) and \(w_{1}\), which is the probability that a random reviewer votes for "accept" in the clean setting. The prior over the states can be described by a prior distribution over \(w_{1}\). We use three different priors for \(w_{1}\), which are Beta\((,)\)(most papers' quality is either high or low), Beta\((1,1)\)(papers' quality is distributed uniformly), and Beta\((3,3)\)(most papers have a medium quality).
3. **The bias vector \(\)**: Since the numerical experiment is to compare the scores of two papers, we consider the opposite and same biases between two papers: * **opposite**: Paper A has the positive bias vector \(_{A}=\) where the reviewers intend to vote for "accept" without careful review. Paper B has an negative bias vector \(_{B}=\). This simulates a situation where one paper has a negative cheap signal and another paper has a positive cheap signal. * **same**: Both paper A and B have the same bias vector \(_{A}=_{B}=\)12. This simulates a situation where both papers have positive (negative) cheap signals. 
We evaluate the performance by accuracy, which is \(1\) minus the expected error probability, and perform the experiments when two papers have opposite biases (Figure 4) and the same bias (Figure 5). In each setting, we consider three scenarios with varying noise levels for paper A (\(_{A}=0,0.3,0.6\)). In each scenario, we vary the noise level of paper B and compare our Surprisal-based Score and baseline under different priors and the number of reviewers. The x axis is paper B's noise level and the y axis is the accuracy of Surprisal-based Score (red lines) and baseline (green lines)13. According to the results, our method outperforms or equals the baseline in accuracy in all cases. We also notice that our method has more advantages under the opposite bias setting and when the number of reviewers is higher.

In addition, we propose another benchmark for comparison, named the SP-inspired score. This benchmark slightly modifies the original Surprisingly Popular method  to fit our setting. This benchmark is not invariant to noise but usually has a lower variance. We include the details and the numerical experiments comparing our method with the SP-inspired score in Appendix D.

## 7 Conclusion and Discussion

When papers receive ratings of different noises due to heterogeneity of paper topics and the existence of cheap signals, we propose a scoring process that leads to a noise-robust rank among papers. The implementation of the process requires the reviewers to report their predictions for other ratings but does not require any prior knowledge. We provide theoretical and numerical justification for the process.

Our theory assumes that the reviewers are perfect Bayesian. However, this may not be the case in practice. Non-Bayesian reviewers may exhibit reporting bias and prediction bias. To some extent, prediction bias is inevitable even with a rational crowd, since it is hard for reviewers to fully understand the prior. The good news is, if the prediction bias is identically distributed among reviewers, then it can be regarded as part of the noisy prior, making the solution we propose still feasible. In contrast, dealing with reporting bias in the one-shot setting appears to be more challenging, since it can violet the basic assumption of homogeneous reviewers. For instance, some reviewers may be more lenient, while others are more strict. Many related works have considered this situation and proposed solutions to mitigate or calibrate the reporting error. We can employ any of these report debiasing schemes based on historical information or multi-tasking, and then use our Surprisal-based Score on the processed reports to measure the paper quality.

## 8 Acknowledgements

This research is supported by National Key R&D Program of China (2022ZD0114900).

Figure 4: **Performance evaluation (opposite biases)**

Figure 5: **Performance evaluation (same bias)**