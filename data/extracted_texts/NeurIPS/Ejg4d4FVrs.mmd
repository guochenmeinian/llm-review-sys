# Elliptical Attention

Stefan K. Nielsen

FPT Software AI Center

Ha Noi, Vietnam

stefannvkp@fpt.com &Laziz U. Abdullaev

Department of Mathematics

National University of Singapore

Singapore 119077, Singapore

laziz.abdullaev@u.nus.edu &Rachel S.Y. Teo

Department of Mathematics

National University of Singapore

Singapore 119077, Singapore

rachel.teo@u.nus.edu &Tan M. Nguyen

Department of Mathematics

National University of Singapore

Singapore 119077, Singapore

tannm@nus.edu.sg

Equal contribution. Correspondence to stefannvkp@fpt.com

###### Abstract

Pairwise dot-product self-attention is key to the success of transformers that achieve state-of-the-art performance across a variety of applications in language and vision. This dot-product self-attention computes attention weights among the input tokens using Euclidean distance, which makes the model prone to representation collapse and vulnerable to contaminated samples. In this paper, we propose using a Mahalanobis distance metric for computing the attention weights to stretch the underlying feature space in directions of high contextual relevance. In particular, we define a hyper-ellipsoidal neighborhood around each query to increase the attention weights of the tokens lying in the contextually important directions. We term this novel class of attention Elliptical Attention. Our Elliptical Attention provides two benefits: 1) reducing representation collapse, and 2) enhancing the model's robustness as Elliptical Attention pays more attention to contextually relevant information, rather than focusing on some small subset of informative features. We empirically demonstrate the advantages of Elliptical Attention over the baseline dot-product attention and state-of-the-art attention methods on various practical tasks, including object classification, image segmentation, and language modeling across different data modalities. The code is publicly available at https://github.com/stefvk/Elliptical-Attention.

## 1 Introduction

Attention mechanisms and transformers  have achieved state of the art performance across a wide variety of tasks in machine learning  and, in particular, within natural language processing , computer vision , and reinforcement learning . They have also demonstrated strong performance in knowledge transfer from pretraining tasks to various downstream tasks with weak or no supervision . At the core of these models is the dot-product self-attention mechanism, which learns self-alignment between tokens in an input sequence by estimating the relative importance of each token with respect to all others. The mechanism then transforms each token into a weighted average of the feature representations of the other tokens with weights proportional to the learned importance scores. The relative importance scores capture contextual information among tokens and are key to the success of the transformer architecture .

Recent work has begun exploring the connections between self-attention and non-parametric kernel regression [54; 23]. Under this interpretation, there is an unknown, underlying function \(f\) mapping the tokens in the input sequence to the output sequence. The self-attention mechanism estimates \(f\) by performing Nadaraya-Watson (NW) regression with isotropic Gaussian kernels. Our work leverages this perspective on self-attention, where we notice that Gaussian isotropic kernels are spherically invariant. This has the drawback of assuming all dimensions of the feature space are equal in terms of importance, meaning nearby tokens are assigned contextual relevance weights dependant only on their Euclidean distance from a query, regardless of direction. From the non-parametric regression perspective, we show that spherical invariance in the kernel causes the estimator to suffer provably higher variance. This causes two connected disadvantages in the self-attention setting. First, high variance in the estimator impairs robustness as small contaminations in the input cause large, erroneous changes in the self-attention output. Second, the high variance of the estimator reduces the capacity of the self-attention mechanism as hidden representations passing through the model are increasingly composed of uninformative noise.

**Contribution.** In this work, we propose Elliptical Attention, a new class of self-attention that constructs hyper-ellipsoidal, rather than hyper-spherical, neighborhoods around the attention queries. The key idea is to stretch the neighborhoods around the queries to upweight keys in directions of high importance. We achieve this by computing a Mahalanobis transformation that stretches the axes of the underlying feature space according to a learned measure of coordinate-wise relevance. Constructing hyper-ellipsoidal neighborhoods following this scheme allows the self-attention mechanism to learn higher-quality contextual representations that prevent representation collapse while simultaneously exhibiting stronger robustness. We additionally propose an estimator of coordinate-wise relevance in the self-attention mechanism that can be computed highly efficiently and with no learnable parameters. We theoretically prove that our estimator accurately estimates the relative coordinate-wise relevance in the feature space. Finally, our approach of constructing hyper-ellipsoidal neighborhoods is linked to theoretical improvements in the mean squared error (MSE) of non-parametric estimators by reducing variance without introducing bias. We demonstrate that this provable reduction in variance is related to both representation collapse and robustness, proposing a unifying framework for both phenomena. This framework is based on the geometry of the predictive neighborhood around queries in the attention mechanism. In summary, our contributions are three-fold:

1. We develop the novel Elliptical Attention, which learns better contextual representations by constructing hyper-ellipsoidal neighborhoods around queries.
2. We propose an efficient estimator of the coordinate-wise relevance in the self-attention mechanism, which requires no learnable parameters, and provide theoretical guarantees for this estimator.
3. We derive a theoretical framework unifying representation collapse and robustness in transformers based only on the implicit geometry of the attention mechanism.

We empirically demonstrate that 1) Elliptical Attention outperforms baseline self-attention models in terms of accuracy and robustness on a variety of practical benchmarks, including WikiText-103 language modelling, ImageNet-1K object classification, LRA long sequence modeling, and ADE20K image segmentation, 2) Elliptical Attention attains robust improvements with lower memory requirements and faster computational speed than baseline robust transformers, and 3) Elliptical Attention can be combined with state-of-the-art robust transformers to further boost robust performance in ImageNet-1K under adversarial attack.

Figure 1: Comparison of Attention Heatmaps. Elliptical pays attention to more relevant information. DeiT focuses on just a subset of informative features while Elliptical considers a wider set of contextually relevant information, helping to produce more accurate and robust predictions. Attention scores are min-max scaled for visualization purposes.

**Organization.** We structure this paper as follows: In Section 2, we present preliminaries on self-attention and non-parametric kernel regression. In Section 3, we illustrate the theoretical benefits of hyper-ellipsoidal neighborhoods, demonstrate how we build the required transformation, and provide the full technical formulation of Elliptical Attention. We empirically validate the advantages of the Elliptical Attention in Section 4. Related work is discussed in Section 5 before presenting concluding remarks in Section 6. Proofs, technical details, and further experiments are provided in the Appendix.

## 2 Background: Self-Attention and Non-Parametric Regression

We first provide preliminaries on the self-attention mechanism followed by background on its connection to the Nadaraya-Watson (NW) estimator in non-parametric regression .

### Self-Attention Mechanism

Given an input sequence \(=[_{1},,_{N}]^{}^{N D_{x}}\) of \(N\) feature vectors, the self-attention mechanism transforms the input to \(:=[_{1},,_{N}]^{}^{N D_{x}}\) as follows:

\[_{i}=_{j[N]}(_{i}^{}_ {j}}{})_{j},i=1,,N.\] (1)

The vectors \(_{i},_{j},\) and \(_{j}\) are the query, key, and value vectors, respectively. They are computed as \([_{1},,_{N}]^{}:==_{Q}^{} ^{N D}\), \([_{1},,_{N}]^{}:==_{K}^{} ^{N D}\), and \([_{1},,_{N}]^{}:==_{V}^{} ^{N D_{v}}\) where \(_{Q},_{K}^{D D_{x}},_{V}^{D _{v} D_{x}}\) are the weight matrices. Eqn. 1 can be expressed in matrix form as:

\[=(^{}}{}),\] (2)

where the softmax function is applied row-wise to the matrix \(^{}/\). We refer to transformers built with Eqn. 2 as standard transformers or just transformers.

### A Non-Parametric Regression Perspective of Self-Attention

We now present the connection between self-attention as described in Eqn. 1 and non-parametric regression. We first assume key and value vectors \(\{_{j},_{j}\}_{j[N]}\) are obtained from the following data generating process:

\[=f()+,\] (3)

where \(\) is random zero-mean noise \([]=0\), and \(f\) is the unknown function to be estimated. We consider the random design setting where the keys \(\{_{j}\}_{j[N]}\) are i.i.d samples drawn from the marginal distribution \(p()\). We use \(p(,)\) to denote the joint distribution of pairs \((,)\) as obtained according to Eqn. 3. At any given new query \(\), we aim to estimate the unknown function \(f()\).

The NW estimator is a non-parametric estimator of the unknown \(f\) described by

\[f()=[|]=_{^{D}} p( |)d=_{^{D}} p(,)}{p( )}d,\] (4)

Figure 2: **Left:** The function does not vary in the \(x_{2}\) axis so we stretch the neighborhood in that direction. **Right:** The stretched ellipsoidal neighborhood includes 4 more keys.

where we apply zero-mean noise for the first equality and the definitions of conditional expectation and density for the second and final. Then, it can be shown that by estimating the joint density \(p(,)\) and marginal density \(p()\) using isotropic Gaussian kernels with bandwidth \(\) and evaluating the NW estimator at a new query \(_{i}\), we obtain

\[_{}(_{i}) =_{j}(-\|_{i}-_{j} \|^{2}/2^{2})}{_{j[N]}(-\|_{i}-_{j}\|^ {2}/2^{2})}\] (5) \[=_{j}(_{i}^{}_{j}/ ^{2})}{_{j[N]}(_{i}^{}_{j}/^{2})}=_{ j[N]}(_{i}^{}_{j}/^{2})_{j},\] (6)

where choosing \(^{2}=\) as the isotropic variance recovers the full attention mechanism. We present the full derivation in Appendix A.

**Limitation of self-attention.** We see in Eqn. 5 that standard self-attention computes the relative importance scores between queries and keys via Euclidean distance. Euclidean distances are spherically invariant and therefore fail to consider coordinate-wise significance in the feature space, meaning the proximity of \(_{j}\) from \(_{i}\) influences its contextual relevance equally regardless of direction.

## 3 Elliptical Attention: Leveraging Hyper-Ellipsoids to Pay More Attention Without Losing Focus

In this section, we first present how NW regression obtains a lower MSE by taking hyper-ellipsoidal neighborhoods around queries. We then construct the required hyper-ellipsoidal transformation via a Mahalanobis metric. We present the framework relating robustness and representation collapse to the geometry of the query neighborhoods and show how our proposed scheme offers improvements in both areas. We then provide an efficient estimator of the coordinate-wise relevance before finally giving the full technical formulation of Elliptical Attention. Technical details on the implementation procedure are in Appendix E.

### Improving NW Regression with Hyper-Ellipsoids

Distance-based estimators, such as the NW estimator, can obtain a lower MSE by taking hyper-ellipsoidal neighborhoods around queries [29; 30]. The key idea is that we wish to stretch the axes of the underlying space in directions for which the true \(f\) in Eqn. 3 varies least.

Figure 2 shows a situation in which \(f\) does not vary equally in all directions. This is actually a limiting case in which the function is sparse in the \(x_{2}\) direction. In the left sub-figure, we show the result of stretching the Euclidean circular neighborhoods around each query in the \(x_{2}\) direction for which the function does not vary. The right sub-figure then shows how the resulting ellipse in the \(x_{2}\) direction can include additional data points without adding additional bias into the model. It is a well-established result that the variance of non-parametric estimates at a point is inversely proportional to the number of samples in that point's neighborhood, as the additional samples smooth out the effect of noise. As a result, stretching the neighborhood, as shown in the right sub-figure, decreases the variance. Crucially, including these additional samples does not cause the estimate to miss the true variation in the function, as there is no variation in the \(x_{2}\) direction. By including points in this direction, we do not introduce bias into the estimate. Hence, we lower variance without the introduction of bias, obtaining a lower MSE estimator. This intuition is formalized in Theorem 1 in Appendix C, which shows that the best achievable rate of convergence for estimators of non-sparse Lipschitz functions is of the order \((n^{-2/(2+d)})\) for a \(d\) dimensional feature space. However, when the function only depends on \(R[d]\) coordinates, the rate improves to \((n^{-2/(2+|R|)})\). In the case of approximate sparsity, when coordinate directions exhibit differing variability, the same intuition carries over as shown by the improvement in convergence rates in Theorem 2 in Appendix C.

We leverage this analysis from non-parametric regression to motivate our Elliptical Attention. From the regression perspective, the self-attention mechanism, which performs NW regression, is able

Figure 3: Representation Collapse on WikiText-103. Elliptical Attention learns more diverse representations.

to learn a lower MSE estimator of the true underlying \(f\) by reducing the variance of the estimator without (or with minimal) introduction of bias. From the attention perspective, this means queries pay higher attention to more relevant keys, producing more contextually meaningful attention scores and better, more robust learned representations.

### Capturing Coordinate-wise Variability and Building the Mahalanobis Transformation

We measure the variation in \(f\) in the \(i^{th}\) coordinate direction by the expectation of the \(_{1}\) norm of the \(i^{th}\) directional derivative taken over all \(k_{k}\), where \(_{k}^{D}\) denotes the feature space. Roughly speaking, this quantity corresponds to the average absolute gradient of \(f\) in the \(i^{th}\) direction throughout the space. Formally, this quantity is defined as

**Definition 1** (Coordinate-wise Variability of \(f:^{D}^{D_{v}}\)): _The coordinate-wise variability of \(f:^{D}^{D_{v}}\) with Jacobian matrix \(_{f}^{D_{v} D}\) in the \(i^{th}\) direction is given by the quantity \(\|f^{}_{i}\|_{1,}:=_{}\|_{f}()_{i}\|_{1},i[D]\), where \(e_{i}\) is an all-zero vector with a single 1 in the \(i^{th}\) coordinate and \(\) is the marginal distribution of \(k\) over support \(_{k}\)._

**Remark 1**: _This definition is one of many possible. One could also take the supremum rather than the expectation or consider second derivatives. We select this definition as averages over first derivatives are more easily estimated and the definition still captures the intuitive properties of variability._

Denoting estimates of the coordinate-wise variability \(\|f^{}_{i}\|_{1,}\) by \(m_{i}\), we can then incorporate these quantities into a distance function of the form

\[d(,):=-)^{}(-)},\] (7)

where \(=(m_{1},m_{2},,m_{D})\) is a diagonal matrix whose diagonal elements are the estimates of \(\|f^{}_{i}\|_{1,}\) for \(i[D]\).

**Remark 2**: _The metric described in Eqn. 7 is a form of Mahalanobis distance metric, which can be interpreted as first applying a transformation to the underlying space in which we stretch the coordinate axes by the diagonal elements of \(\). Therefore using this metric within the self-attention computation produces the desired hyper-ellipsoidal neighborhoods around queries._

**Remark 3**: _In practice, we maxscale the estimates to obtain \(m_{i} m_{i}/m_{max}\) where \(m_{max} m_{i}\) for all \(i[D]\). This is because we care about the relative magnitudes of the direction-wise variability as opposed to the absolute magnitudes. Under this interpretation, we identify the most variable dimension and stretch all others relative to this direction._

### Promoting Robustness and Avoiding Representation Collapse

Before providing the technical procedure for estimating \(\) and the full technical formulation of Elliptical Attention in Section 3.5, we first theoretically analyze in Propositions 1 and 2 how the hyper-ellipsoidal transformation in Eqn.7 improves robustness and alleviates representation collapse.

**Dimension-wise input sensitivity of Elliptical Attention and robustness.** In Lemma 1, we show that when each input component is weighted according to the Mahalanobis transformation in Eqn. 7, the impact of perturbing the \(i^{th}\) input coordinate on any coordinate of the output is proportional to the corresponding weighting parameter with proportionality coefficient depending on the indices \(i\) and \(j\).

**Lemma 1**: _Let \(:^{D}^{N}\) denote the transformed Elliptical \(\) operator for a given set of keys as \(():=(^{}_{j}) }[(^{}_{1}),(^{}_{2}),,(^{}_{N})]^{}\) for weight matrix \(\) as in Eqn. 7. Then, the achievable rate of change of \(()\) in \(i^{th}\) input dimension is proportional to \(m_{i}\), that is, \(_{}|_{}()_{ji}| m_{i},\) for all \(i[D]\) and \(j[N]\) where \(_{}\) is the Jacobian matrix of \(\)._

By virtue of Lemma 1, which is proven in Appendix B.1, we show in Proposition 1 that choosing the weights as properly scaled estimates of the underlying function variability, as in Elliptical Attention, the output vectors become less prone to large errors caused by noisy input while simultaneously respecting the dimension-wise variability pattern of the true self-attention function.

**Proposition 1** (Robustness of Elliptical Attention): _Let \(f:^{D}^{D_{v}}\) be the true self-attention function, \(_{d}\) be the Elliptical Attention estimator with metric \(d\) as described in Eqn. 7. Then for any index \(i[N]\) and noise \(^{D}\), the following error bound holds_

\[\|_{d}(_{i})-_{d}(_{i}+)\|( _{j[N]}(_{j}^{2}^{2})}\|_{j}\| )\|\|\,,\] (8)

_where \(\{_{j}\}_{j[N]}\) are constant diagonal matrices that depend only on the key vectors._

Note that when the estimates are maxscaled so that \(m_{i} 1\), the achievable output error of Elliptical Attention is lower than that of standard self-attention where \(m_{i}=1\) for all \(i[D]\). Besides, when the true function exhibits approximate sparsity in some number of dimensions (i.e. \(m_{i} 0^{+}\) for majority of indices), the error bound in Eqn. 8 becomes significantly tighter for Elliptical Attention. The proof of Proposition 1 is provided in Appendix B.2.

**Input smoothing and representation collapse.** In each layer, the standard self-attention mechanism fits a noisy estimate of the true function \(f\), which is then fed into subsequent layers and iteratively refit. The input to each attention layer is then partially composed of noise, which is equivalently the common regularization method of random input smoothing. We show that by reducing the noise component in each layer, Elliptical Attention maintains expressive power and resists representation collapse. This is formalized in the following proposition:

**Proposition 2** (Elliptical Attention maintains expressive power by reducing noise): _Let \(_{d}^{}\) denote the output of a transformer using Elliptical Attention with metric \(d\) as described in Eqn. 7 and \(^{}\) denote the output of a transformer using standard self-attention at layer \(\). Let \(\) be the sampling distribution of the data and let \(^{D}\). Then, for any \(,_{d}\) and layer \(\), in expectation a standard self-attention transformer attenuates towards \(\) faster than Elliptical Attention. Formally, we have:_

\[_{}\|_{d}^{}-\|_{}\|^{}-\|.\] (9)

Proof is provided in Appendix B.3. Proposition 2 shows Elliptical Attention maintains better expressive power than standard self-attention. We find this empirically supported as shown in Fig 3.

### An Efficient Estimator of the Coordinate-wise Variability

We propose a simple difference-based estimator that effectively captures the coordinate-wise variability of the underlying function. Our estimator is easily and efficiently computed. It requires no additional learnable parameters and demands negligible additional memory. Let \(_{n}\) denote empirical mean over \(n\) samples, \(^{}(i)\) denote the \(i^{th}\) component of the vector \(\) at the \(^{th}\) layer, and \(_{v}^{,+1}=\{(^{+1},^{}):^{}= f(^{})+\}\) be the value feature space at neighboring layers \(\) and \(+1\) where values are generated according to the process described in Eqn. 3. Then, our approach to estimating the \(i^{th}\) coordinate-wise variability is described in the following proposition.

**Proposition 3** (Coordinate-wise Variability Estimator): _Given a function \(f:^{D}^{D_{v}}\) with \(i^{th}\) directional variation \(\|f_{i}^{}\|_{1,},i[D]\) and some \(>0\), the directional variation can be estimated by the quantity_

\[m_{i}:=^{},^{+1})_{v}^{, +1}}{_{n}}^{+1}(i)-^{}(i)|}{}.\] (10)

**Remark 4**: _For the purposes of improving the performance of transformers by stretching the feature space according to the direction-wise variability of \(f\), we note that consistent estimators of \(\|f_{i}^{}\|_{1,}\) for all \(i[D]\) are sufficient but not necessary. Instead, we require only the weaker objective of accurately estimating the relative magnitudes of the direction-wise variability. That is, if \(\|f_{i}^{}\|_{1,}\|f_{j}^{}\|_{1,}\), we need only that \(m_{i} m_{j}\). This is because the theory requires us only to identify coordinate directions of more or less variability and shrink or stretch the space accordingly._

The intuition behind our estimator in Eqn. 10 lies in prior lines of research studying transformers as an Euler discretization of a continuous-time dynamic, usually as a system of first-order ordinary differential equations (ODEs) . In fact, our estimator resembles the absolute value of a forward Euler discretization of the variability of the \(i^{th}\) component of a value vector over time \((i,t)/ t\), where the layers \(\) and \(+1\) represent consecutive time points in an interval partition with the step size \(\). We prove that our estimator in Eqn. 10 effectively estimates the relative magnitudes of the coordinate-wise variability of \(f\) in Appendix B.5.

### Full Technical Formulation of Elliptical Attention

We now present the full formulation of Elliptical Attention. Given the distance function \(d(,)\) as in Eqn. 7, where \(=(m_{1},,m_{D})\) is a diagonal matrix with elements \(m_{i}\) as in Prop. 3, the \(\)-norm can be defined as \(\|\|_{}:=^{T}}\), which produces hyper-ellipsoidal stretching in the feature space. Then, Elliptical Attention is defined as follows.

**Definition 2** (Elliptical Attention Computation): _Let \(_{d,}:^{D}\) denote the Gaussian density kernel with variance \(^{2}\) equipped with the \(\)-norm as defined above. Then the corresponding NW estimator at \(_{i}\) becomes_

\[_{d,D}(_{i}):=_{j}(-\|_{i }-_{j}\|_{}^{2}/2^{2})}{_{j[N]}(-\|_{i}-_{j}\|_{}^{2}/2^{2})}.\] (11)

_Then, the Elliptical Attention output for the \(i^{th}\) query \(_{i}\) given keys \(\{_{i}\}_{i=1}^{N}\) and values \(\{_{i}\}_{i=1}^{N}\) corresponding to the NW estimator (11) with \(^{2}=\) is given by_

\[_{i}=_{j[N]}_{i}^{}_{j}/ _{j}}{_{j[N]}_{i}^{} _{j}/}=_{j[N]}(_{i}^{} _{j}/)_{j},\] (12)

_where \(=(m_{1},,m_{D})\) with \(m_{i}\) defined as Eqn. 10 for all \(i[D]\)._

Eqn. 12 is equivalently expressed in matrix form as

\[=(^{}}{}) .\] (13)

**Remark 5**: _We see from the form of Eqns. 12, 13 that standard self-attention is recoverable by setting \(=_{D}\). Under our framework, this implies that standard self-attention assumes the underlying regression function to have exactly equal variability in all coordinate directions._

Pseudocode for the Elliptical Attention computation is provided in Appendix F.12.

## 4 Experimental Results

In this section, we empirically justify the advantage of Elliptical Attention over baseline transformers that take hyper-spheres around queries. We evaluate our method on robust Wikitext-103 modeling under Word Swap contamination , ImageNet classification under a wide range of attacks [14; 67], the LRA benchmark , and ADE20K image segmentation . We compare Elliptical Attention with state-of-the-art (SOTA) clean and robust models, including Performer , FourierFormer , Robust Vision Transformer , Fully Attentional Network (FAN) , Mixture of Gaussian Keys (MGK) , Mixture-of-Expert (MoE) based transformers, such as Switch transformer  and Generalist Language Model (GLAM) , and robust kernel density estimation (KDE) based transformers, such as Median of Means (MoM) and Scaled Projected KDE (SPKDE) . We aim to show that i) Elliptical Attention offers substantive improvements over baseline models across tasks on both clean and contaminated data; ii) Elliptical Attention attains these improvements on contaminated data while reducing memory requirements and increasing computational speed compared to comparative robust models; iii) Elliptical Attention can be combined with SOTA robust transformers to further improve robustness with negligible increase in computational overhead. We compare Elliptical Attention with baselines of the same configuration. Results are averaged over 5 runs with different seeds. Additional results and full details on experimental setup are in Appendix F.

### Robust Language Modelling

**Experimental setup.** We adopt the experimental setup in [54; 23]. We pretrain and evaluate our models on the WikiText-103 benchmark in comparison with the standard baseline Transformer , Performer , Transformer-MGK , FourierFormer , and the robust kernel density estimation-based Transformers including Transformer-SPKDE and Transformer-MoM . All models use the 44M-parameter Transformer backbone. We pretrain all models on clean data for 125 epochs before attacking only the test set using a Word Swap Attack, which substitues random words with a generic 'AAA' token at a 2.5% swap rate. We report test perplexity (PPL) as the performance metric.

**Results.** Table 1 shows our Elliptical Transformer (_Elliptical_) achieves top test perplexity in clean data while also achieving second top test perplexity under data contamination by Word Swap , illustrating that the Elliptical Attention is highly robust and offers substantial advantages on clean data as well.

### Image Classification under Adversarial Attack

**Experimental setup.** We adopt the experimental setup in . We train and evaluate _Elliptical_ on ImageNet-1K against standard vision transformers, including DeiT  and Distill , as well as the FourierFormer . We also compare _Elliptical_ with robust vision transformers, including DeiT-KDE , DeiT-MoM , RVT , and FAN . The DeiT backbone is the tiny configuration of 5.7M parameters. We train all models on clean ImageNet-1K for 300 epochs before evaluating their top-1 and top-5 accuracy on the test dataset under fast gradient sign method (FGSM) , projected gradient descent (PGD) , and simultaneous perturbation stochastic approximation (SPSA) . We also present results for performance against Auto Attack , which is an ensemble of auto PGD-Cross Entropy (APGD-CE), auto PGD-targeted (APGD-T), fast adaptive boundary-targeted (FAB-T), and Square. We display results for attacks individually and in default sequential mode.

**Results.** Table 2 shows _Elliptical_ attains top robustness in PGD and SPSA and second top in FGSM while achieving highly competitive clean accuracy. _DeiT-Elliptical_ is particularly impressive under black box attack SPSA, improving over the next best model, _RVT_, , by 10%. Table 4 shows results on Auto Attack , where we see _DeiT-Elliptical_ substantially outperforms standard _DeiT_ in each attack individually and sequentially. We again see strong performance against black box attack Square with an 8.5% improvement. When combining with SOTA robust transformer, _FAN_, Elliptical Attention improves robustness to sequential Auto Attack and all individual attacks except FAB-T, for which it still remains highly competitive. This shows Elliptical Attention can further boost robustness when combined with SOTA robust models.

### Long Sequence Modelling on the LRA Benchmark

**Experimental setup.** We adopt the setup in . For each of the 5 tasks, equation calculation (ListOps) , review classification (Text) , document retrieval (Retrieval) , image classification (Image) , and image spatial dependencies (Pathfinder) , we compare _Elliptical_ with standard Transformer , Linformer , Reformer , Performer , and Longformer .

**Results.** Elliptical Attention achieves top or second top test accuracy in every task and top overall performance. This shows Elliptical Attention learns superior representations across a wide range of modalities in long-range contexts.

   Model & Clean Test PPL (\(\)) & Contaminated Test PPL (\(\)) \\   _Transformer_ & 34.29 & 74.56 \\ Performer  & 33.49 & 73.48 \\ Transformer-MGK  & 33.21 & 71.03 \\ FourierFormer  & 32.85 & 68.33 \\ Transformer-SPKDE  & 32.18 & 54.97 \\ Transformer-Mod  & 34.68 & **52.14** \\  Transformer-Elliptical & **32.00** & 52.59 \\   

Table 1: Perplexity (PPL) on WikiText-103 under Word Swap contamination. Elliptical achieves top PPL in clean data and second best in contaminated. Best result in bold and second best underlined.

    &  &  &  &  \\  & Top 1 & Top 5 & Top 1 & Top 5 & Top 1 & Top 5 & Top 1 & Top 5 \\   _DeiT_ & 72.23 & 91.13 & 52.61 & 82.26 & 41.84 & 76.49 & 48.34 & 79.36 \\ Distill  & 74.32 & 93.72 & 53.24 & 84.07 & 41.72 & 76.43 & 49.56 & 80.14 \\ FourierFormer  & 73.25 & 91.66 & 53.08 & 83.95 & 41.34 & 76.19 & 48.79 & 79.57 \\ RVT  & **74.37** & **93.89** & 53.67 & 84.11 & 43.39 & 77.26 & 51.43 & 80.98 \\ DeiT-KDE  & 72.58 & 91.34 & 52.25 & 81.52 & 41.38 & 76.41 & 48.61 & 79.68 \\ DeiT-MoM  & 71.94 & 91.08 & **55.76** & **85.23** & 43.78 & 78.85 & 49.38 & 80.02 \\  DeiT-Elliptical & 72.36 & 91.33 & 54.64 & 85.18 & **44.96** & **79.35** & **56.55** & **87.26** \\   

Table 2: Top-1 and Top-5 Test accuracy on ImageNet under adversarial attacks PGD, FGSM, and SPSA with perturbation budget 1/255. Best result shown in bold and second best shown underlined.

### Image Segmentation on ADE20K

**Experimental setup.** We adopt the setup in . The encoder is pretrained on ImageNet-1K following the same specification described in 4.2. In particular, the encoder is a DeiT-tiny backbone of 5.7M parameters pretrained for 300 epochs. After pretraining, we then attach a decoder that contains 2-layer masked transformer and finetune the full encoder-decoder model for 64 epochs on the ADE20K  image segmentation dataset.

**Results.** Table 7 reports pixel accuracy, mean accuracy, and mean intersection over union (IOU). Elliptical Attention boosts performance across all metrics, with intersection over union, in particular, improving by a substantive 4.7%.

### Further Clean Data Language Modelling

**Experimental setup.** For experiments using Switch Transformer  and GLaM  backbones, we adopt the setup in . In particular, we integrate _Elliptical_ into small (70M parameters) and medium (220M parameters) GlaM backbones and train the models on WikiText-103 for 80 and 120 epochs, respectively. We consider Switch backbones at medium (220M parameters) and large (388M parameters) configurations, both trained for 80 epochs. All models use top-2 expert routing. For the standard transformer experiments, we continue with the setup of  and additionally present results for _Elliptical_ in a medium configuration with 90M parameters trained for 100 epochs.

**Results.** We present in Tables 5 and 6 the performance of _Elliptical_ in MoE backbones. We see moving from smaller to larger configurations, _Elliptical_ maintains strong, consistent improvements in test PPL. We note particularly substantive improvements with scale in the GLaM backbone, where at the small configuration _Elliptical_ attains a 2.7% improvement, but at the medium configuration this performance improvement almost doubles to 5.0%. Table 8 further shows that in the standard transformer backbone, _Elliptical_ maintains its substantive 6.8% improvement when scaling up to a larger configuration. These results show that Elliptical Attention scales well with model size.

   Model & Test PPL (\(\)) \\   _Switch Transformer-medium_ & 58.27 \\ Switch Elliptical-medium & **56.69** \\  _GLaM-medium_ & 38.27 \\ GLaM-Elliptical-medium & **36.34** \\   

Table 6: GLaM Language Modeling

   Dataset (seq. length) & _Trans._ & _Lin._ & _Re._ & _Per._ & _Long._ & _Elliptical_ \\   ListOps (2K) & 37.1 & 37.3 & 19.1 & 18.8 & 37.2 & **37.8** \\ Text (4K) & 65.0 & 55.9 & 64.9 & 63.8 & 64.6 & **65.6** \\ Retrieval (4K) & 79.4 & 79.4 & 78.6 & 78.6 & **81.0** & 80.3 \\ Image (1K) & 38.2 & 37.8 & **43.3** & 37.1 & 39.1 & 40.2 \\ Pathfinder (1K) & **74.2** & 67.6 & 69.4 & 69.9 & 73.0 & 73.2 \\  Average Accuracy & 58.5 & 55.6 & 55.1 & 53.6 & 59.0 & **59.4** \\   

Table 3: Test accuracy on long range tasks: ListOps, Text, Retrieval, Image, and Pathfinder. Best result in bold and second best underlined.

   Method &  &  &  &  \\  & Top 1 & Top 5 & Top 1 & Top 5 & Top 1 & Top 5 & Top 1 & Top 5 \\   Clean Data & 72.23 & 91.13 & **72.36** & **91.33** & 76.31 & 93.42 & **76.38** & **93.53** \\  APGD-CE & 27.75 & 66.48 & **31.27** & **68.28** & 35.05 & 74.56 & **36.13** & **75.69** \\ APGD-T & 27.74 & 73.37 & **29.69** & **74.39** & 35.02 & 80.46 & **36.25** & **81.30** \\ FAB-T & 71.61 & 90.54 & **71.74** & **90.81** & **76.35** & **93.65** & 76.16 & 93.45 \\ Square & 43.55 & 80.96 & **47.25** & **81.65** & 56.75 & 88.05 & **58.38** & **88.20** \\  Average & 42.66 & 77.84 & **45.00** & **78.78** & 50.79 & 84.18 & **51.73** & **84.66** \\ Sequential Attack & 26.08 & 64.18 & **27.45** & **67.77** & 33.29 & 74.52 & **34.54** & **75.67** \\   

Table 4: Top-1 and Top-5 Test accuracy on ImageNet under Auto Attack applied both individually and sequentially with perturbation budget 1/255. Best result is shown in bold.

   Model & Test PPL (\(\)) \\   _Switch Transformer-medium_ & 35.33 \\ Switch Elliptical-medium & **34.67** \\  _Switch Transformer-large_ & 31.18 \\ Switch Elliptical-large & **30.56** \\   

Table 5: Switch Transformer Language Modeling

## 5 Related Work

**Theoretical Frameworks for Attention.** Attention mechanisms have been studied from a range of perspectives.  shows that self-attention can be derived from kernel similarity functions, and  points out that self-attention projects its query vectors onto the principal component axes of its key matrix in a feature space.  formulates self-attention as the support vector expansion derived from a support vector regression problem, while  explains attention through nonlinear singular value decomposition of asymmetric kernels. Attention has also been explained through ordinary/partial differential equations, Gaussian mixture models, and graph-structured learning [40; 68; 51; 72; 20; 52; 31; 86]. [54; 23] show that self-attention performs Nadaraya-Watson regression with Gaussian isotropic kernels. This paper leverages this viewpoint and proposes modifying the Gaussian isotropic kernels to include a Mahalanobis metric which can be interpreted as stretching the hyper-spherical neighborhoods of the kernel to hyper-ellipsoids.

**Robust Transformers.** In vision,  proposes an ensemble defense strategy to white-box attacks while  proposes position-aware attention scaling and patch-wise augmentation. Recently,  proposes a fully-attentional network to attain state-of-the-art accuracy on corrupted image data. In language,  proposes structurally aware table-text encoding,  proposes a robust end-to-end transformer for crisis detection, and  proposes duration-based hard attention. [6; 4] integrate a Gaussian process into attention for out-of-distribution detection, and  develops equivariant neural functional networks for transformers. These methodologies are motivated by their respective domain and tend to have limited generalizability to differing domains. Our approach, by contrast, proposes a general framework that makes no assumption on the downstream task and requires no additional parameters and negligible computational overhead.

**Mahalanobis Metrics.** Mahalanobis metrics have been used predominantly in classical machine learning algorithms. In nearest-neighbor (NN) classification and regression, [84; 49] learn the metric through backpropagation. In NN KL divergence estimation,  learns a Mahalanobis metric from density approximation. In kernel regression,  takes eigenvalues of the estimated Jacobian while [29; 30] estimate coordinate-wise variability of the true function. Our model similarly uses coordinate-wise variability of the unknown function to form the Mahalanobis transformation but instead uses a more efficient estimator that does not require materializing the prediction function and accommodates the self-attention setting. In general, our method is among the early work in incorporating Mahalanobis metrics into the self-attention mechanism.

## 6 Conclusion and Future Work

In this paper, we present Elliptical Attention, a novel variant of attention that computes a Mahalanobis transformation to stretch the underlying feature space in directions of high contextual relevance. This transformation can be interpreted as modifying the hyper-spherical neighborhoods around queries to hyper-ellipsoids which upweight the attention paid to keys lying in important directions, enabling the transformer to learn better and more robust representations. This approach makes no assumptions on the downstream task, requires no learnable parameters, and can be applied to any transformer to boost clean and robust performance. A limitation of our work is that we use the values over layers to estimate the average direction-wise gradient of the true self-attention function, which makes the estimate prone to noise. For ongoing work, we are exploring more precise estimation methods with provable convergence guarantees that do not compromise efficiency.

   Model & Pixel Acc. & Avg Acc. & Avg IOU \\   _DeiT_ & 77.93 & 46.30 & 35.44 \\ Elliptical & **78.46** & **48.04** & **37.09** \\   

Table 7: Image Segmentation Results

   Model & Test PPL (\(\)) \\   _Transformer-small_ & 34.29 \\ Elliptical-small & **32.00** \\  _Transformer-medium_ & 29.60 \\ Elliptical-medium & **27.60** \\   

Table 8: Wikitext-103 Results

Figure 4: ImageNet Efficiency: Comparison of throughput and max memory allocated for DeiT, Elliptical, RVT, RKDE, MoM on Tiny, Small, and Base sizes. Elliptical is the most efficient robust model. Numerical analysis in Table 12 of Appendix F.