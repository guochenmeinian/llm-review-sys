# Bridging Discrete and Backpropagation:

Straight-Through and Beyond

 Liyuan Liu Chengyu Dong Xiaodong Liu Bin Yu Jianfeng Gao

Microsoft Research

{lucliu, v-chedong, xiaodl, v-ybi, jfgao}@microsoft.com

###### Abstract

Backpropagation, the cornerstone of deep learning, is limited to computing gradients for continuous variables. This limitation poses challenges for problems involving discrete latent variables. To address this issue, we propose a novel approach to approximate the gradient of parameters involved in generating discrete latent variables. First, we examine the widely used Straight-Through (ST) heuristic and demonstrate that it works as a first-order approximation of the gradient. Guided by our findings, we propose ReinMax, which achieves second-order accuracy by integrating Heun's method, a second-order numerical method for solving ODEs. ReinMax does not require Hessian or other second-order derivatives, thus having negligible computation overheads. Extensive experimental results on various tasks demonstrate the superiority of ReinMax over the state of the art.

## 1 Introduction

There has been a persistent pursuit to build neural network models with discrete or sparse variables (Neal, 1992). However, backpropagation (Rumelhari et al., 1986), the cornerstone of deep learning, is restricted to computing gradients for continuous variables. Correspondingly, many attempts have been made to approximate the gradient of parameters that are used to generate discrete variables, and most of them are based on the Straight-Through (ST) technique (Bengio et al., 2013).

The development of ST is based on the simple intuition that non-differentiable functions (e.g., sampling of discrete latent variables) can be approximated with the identity function in the backpropagation (Rosenblatt, 1957; Bengio et al., 2013). Due to the lack of theoretical underpinnings, there is neither guarantee that ST can be viewed as an approximation of the gradient, nor guidance on hyper-parameter configurations or future algorithm development. Thus, researchers have to develop different ST variants for different applications in a trial-and-error manner, which is laborious and time-consuming (van den Oord et al., 2017; Liu et al., 2019; Fedus et al., 2021). To address these limitations, we aim to explore _how ST approximates the gradient and how it can be improved_.

First, we adopt a novel perspective to examine ST and show that it works as a special case of the forward Euler method, approximating the gradient with first-order accuracy. Besides confirming that ST is indeed an approximation of the gradient, our finding provides guidance on how to optimize hyper-parameters of ST and its variants, i.e., ST prefers to set the temperature \( 1\), and Straight-Through Gumbel-Softmax (STGS; Jang et al., 2017) prefers to set the temperature \( 1\).

Our analyses not only shed insights on the underlying mechanism of ST but also lead us to develop a novel gradient estimation method called ReinMax. ReinMax integrates Heun's Method and achieves second-order accuracy, i.e., its approximation matches the Taylor expansion of the gradient to the second order, without requiring the Hessian matrix or other second-order derivatives.

We conduct extensive experiments on polynomial programming Tucker et al. (2017); Grathwohl et al. (2018); Pervez et al. (2020); Paulus et al. (2021), unsupervised generative modeling (Kingma and Welling, 2013), structured output prediction (Nangia and Bowman, 2018), and differentiable neural architecture search (Dong et al., 2020) to demonstrate that ReinMax brings consistent improvements over the state of the art1.

Our contributions are two-fold:

* We formally establish that ST works as a first-order approximation to the gradient in the general multinomial case, which provides valuable guidance for future research and applications.
* We propose a novel and sound gradient estimation method ReinMax that achieves second-order accuracy without requiring the Hessian matrix or other second-order derivatives. ReinMax is shown to outperform the previous state-of-the-art methods in extensive experiments.

## 2 Related Work and Preliminary

``` Input:\(\): softmax input, \(\): temperature. Output:\(\): one-hot samples.
1\(_{0}()\)\((_{0})\)\(_{1}_{}()\)/* stop_gradient\(()\) duplicates its input and detaches it from backpropagation.
2\(_{1}-(_{1})+\) return\(\) ```

**Algorithm 1**ST.

**Discrete Latent Variables and Gradient Computation.**  The idea of incorporating discrete latent variables and neural networks dates back to sigmoid belief network and Helmholtz machines (Williams, 1992; Dayan et al., 1995). To keep things straightforward, we will focus on a simplified scenario. We refer to the tempered softmax as \(_{}()_{i}=_{i}/)}{ _{j=1}^{n}(_{j}/)}\), where \(n\) is the number of possible outcomes, \(^{n 1}\) is the parameter, and \(\) is the temperature2. For \(i[1,,n]\), we mark its one-hot representation as \(_{i}^{n 1}\), whose element equals \(1\) if it is the \(i\)-th element or equals \(0\) otherwise. Let \(\) be a discrete random variable and \(\{_{1},,_{n}\}\), we assume the distribution of \(\) is parameterized as: \(p(=_{i})=_{i}=()_{i}\), and mark \(_{}()\) as \(^{()}\). Given a differentiable function \(f:^{n}\), we aim to minimize (note that temperature scaling is not used in the generation of \(\)):

\[_{}(),\ \ \ ()=E_{()}[f()].\] (1)

Here, we mark the gradient of \(\) as \(\):

\[:=()}{}=_{i }f(_{i})_{i}}{d\,}.\] (2)In many applications, it is usually too costly to compute \(\), since it requires the computation of \(\{f(_{1}),,f(_{n})\}\) and evaluating \(f(_{i})\) is costly for typical deep learning applications. Correspondingly, many efforts have been made to estimate \(\) efficiently.

The \(_{}\)(Williams, 1992) is unbiased (i.e., \(E[_{}]=\)) and only requires the distribution of the discrete variable to be differentiable (i.e., no backpropagation through \(f\)):

\[_{}:=f())}{d\,}.\] (3)

Despite the REINFORCE estimator being unbiased, it tends to have prohibitively high variance, especially for networks that have other sources of randomness (i.e., dropout or other independent random variables). Recently, attempts have been made to reduce the variance of REINFORCE (Gu et al., 2016; Tucker et al., 2017; Grathwohl et al., 2018; Shi et al., 2022). Still, it has been found that the REINFORCE-style estimators fail to work well in many real-world applications. Empirical comparisons between ReinMax and REINFORCE-style methods are elaborated in Section 6.5.

Efficient Gradient Approximation.In practice, a popular family of estimators is Straight-Through (ST) estimators. They compute the backpropagation "through" a surrogate that treats the non-differentiable function (e.g., the sampling of \(\)) as an identity function. The idea of ST originates from the perceptron algorithm (Rosenblatt, 1957; Mullin & Rosenblatt, 1962), which leverages a modified chain rule and utilizes the identity function as the proxy of the original derivative of a binary output function. Bengio et al. (2013) improves this method by using non-linear functions like sigmoid or softmax, and Jang et al. (2017) further incorporates the Gumbel reparameterization. Here, we briefly describe Straight-Through (ST) and Straight-Through Gumbel-Softmax (STGS).

In the general multinomial distribution case, as in Algorithm 1, the ST estimator treats the sampling process of \(\) as an identity function during the backpropagation3:

\[_{}:=)}{} }{d\,}.\] (4)

In practice, \(_{}\) is usually implemented with the tempered softmax, under the hope that the temperature hyper-parameter \(\) may be able to reduce the bias introduced by \(_{}\)(Chung et al., 2017).

The STGS estimator is built upon the Gumbel re-parameterization trick (Maddison et al., 2014; Jang et al., 2017). It is observed that the sampling of \(\) can be reparameterized using Gumbel random variables at the zero-temperature limit of the tempered softmax (Gumbel, 1954):

\[=_{ 0}_{}(+)\ \ \ _{i}\ \ _{i}(0,1).\]

STGS treats the zero-temperature limit as identity function during the backpropagation:

\[_{}:=)}{} _{}(+)}{d\,}.\] (5)

Both \(_{}\) and \(_{}\) are clearly biased. However, since the mechanism of ST is unclear, it remains unanswered what the form of their biases are, how to configure their hyper-parameters for optimal performance, or even whether \(E[_{}]\) or \(E[_{}]\) can be viewed as an approximation of \(\). Thus, we aim to answer the following questions: _How \(_{}\) approximates \(\) and how it can be improved?_

## 3 Discrete Variable Gradient Approximation: a Numerical ODE Perspective

In numerical analysis, extensive studies have been conducted to develop numerical methods for solving ordinary differential equations. In this study, we leverage these methods to approximate \(\) with the gradient of \(f\). To begin, we demonstrate that ST works as a first-order approximation of \(\). Then, we propose ReinMax, which integrates Heun's method for a better gradient approximation and achieves second-order accuracy.

### Straight-Through as a First-order Approximation

We start by defining a first-order approximation of \(\) as \(_{}\).

**Definition 3.1**.: _One first-order approximation of \(\) is \(_{}:=_{i}_{j}_{j}^{ _{j})}{_{j}}}( {I}_{i}-_{j})^{_{i}}\)._

To understand why \(_{}\) is a first-order approximation, we rewrite \(\) in Equation 2 as4:

\[=_{i}(f(_{i})-E[f()])_{i}}{d\,}+_{i}E[f()] _{i}}{d\,}=_{i}_{j} _{j}(f(_{i})-f(_{j}))_{i}}{d\,}.\] (6)

Comparing \(_{}\) and Equation 6, it is easy to notice that \(_{}\) approximates \(f(_{i})-f(_{j})\) as \(_{j})}{_{j}}( {I}_{i}-_{j})\). In numerical analyses, this approximation is known as the forward Euler method, which has first-order accuracy (we provide a brief introduction to the forward Euler method in Appendix E). Correspondingly, we know that \(_{}\) is a first-order approximation of \(\).

Now, we proceed to show \(_{}\) works as a first-order approximation. Note that our analyses only apply to \(_{}\) as defined in Equation 4 and may not apply to its other variants.

**Theorem 3.1**.: \[E[_{}]=_{}.\]

The proof of Theorem 3.1 is provided in Appendix A.

It is worth mentioning that Tokui & Sato (2017) discussed this connection for the special case of \(\) being a Bernoulli variable. However, their study is built upon a Bernoulli variable property (i.e., \(=(f(_{2})-f(_{1}))^{_{1}}{d\,}}=(f(_{1})-f(_{2}))^ {_{2}}{d\,}^{2}}\)), making their analyses not applicable to multinomial variables. Alternatively, the analyses in Gregor et al. (2014) and Pervez et al. (2020) are applicable to multinomial variables but resort to modify \(_{}\) as \(_{D}}_{}\), in order to position it as a first-order approximation. We suggest that this modification would lead to unwanted instability and provide more discussions in Section 4.1 and Section 6.6. Here, our study is the first to formally established \(_{}\) works as a first-order approximation in the general multinomial case.

Besides revealing the mechanism of the Straight-Through estimator, our finding also shows that the bias of \(_{}\) comes from using the first-order approximation (i.e., the forward Euler method). Accordingly, we propose to integrate a better approximation for \(f(_{i})-f(_{j})\).

### Towards Second-order Accuracy: ReinMax

The literature on numerical methods for differential equations shows that it is possible to achieve higher-order accuracy _without computing higher-order derivatives_. Correspondingly, we propose to integrate a second-order approximation to reduce the bias of the gradient estimator.

**Definition 3.2**.: _One second-order approximation of \(\) is_

\[_{}:=_{i}_{j}_{ j}}{2}(_{j})}{_{j}}+ _{i})}{_{i}})( {I}_{i}-_{j})_{i}}{d\,}.\]

Comparing \(_{}\) and Equation 6, we can observe that, \(_{}\) approximates \(f(_{i})-f(_{j})\) as \((_{i})}{_{i}} +_{j})}{_{j}})( {I}_{i}-_{j})\). This approximation is known as the Heun's Method and has second-order accuracy (we provide a brief introduction to Heun's method in Appendix E). Correspondingly, we know that \(_{}\) is a second-order approximation of \(\).

Based on this approximation, we propose the ReinMax operator as (\(}\) refers to \(+}{2}\), \(}\) refers to the identity matrix, and \(\) refers to the element-wise product):

\[_{}:=2^{+}{2}}-_{},^{+}{2}}=)}{}((} 1^{T})}- }}^{T})\] (7)Then, we show that \(_{}\) approximates \(\) to the second order. Or, formally we have:

**Theorem 3.2**.: \[E[_{}]=_{}.\]

The proof of Theorem 3.2 is provided in Appendix B.

Computation Efficiency of ReinMax.Instead of requiring Hessian or other second-order derivatives, \(_{}\) achieves second-order accuracy with two first-order derivatives (i.e., \(_{i})}{_{j}}\) and \(_{i})}{_{i}}\)). As observed in our empirical efficiency comparisons in Section 6, the computation overhead of \(_{}\) is negligible. At the same time, similar to \(_{}\) (as in Algorithm 1), our proposed algorithm can be easily integrated with existing automatic differentiation toolkits like PyTorch (a simple implementation of ReinMax is provided in Algorithm 2), making it easy to be integrated with existing algorithms.

Applicability of Higher-order ODE solvers.Although it's possible to apply higher-order ODE solvers, they require more gradient evaluations, leading to undesirable computational overhead. To illustrate this point: The approximation used by ReinMax requires n gradient evaluations, i.e., \(\{_{i})}{_{i}}\}\). In contrast, the approximation derived by RK4 needs \(n^{2}+n\) gradient evaluations, i.e., \(\{_{i})}{_{i}}\}\) and \(\{_{i_{j}})}{_{j}}\}\), where \(_{ij}=_{i}+_{j}}{2}\). Therefore, while higher-order solvers are applicable, they may not be suitable in our case.

## 4 ReinMax and Baseline Subtraction

Equation 6 plays a crucial role in positioning ST as a first-order approximation of the gradient and deriving our proposed method, ReinMax. This equation is commonly referred to as baseline subtraction, a common technique for reducing the variance of REINFORCE.

In this section, we first discuss the reason for choosing \(E[f()]\) as the baseline, and then reveal that the derivation of ReinMax is independent to baseline subtraction.

### Benefits of Choosing \(E[f()]\) as the Baseline

The choice of baseline in reinforcement learning has been the subject of numerous discussions (Weaver & Tao, 2001; Rennie et al., 2016; Shi et al., 2022). Similarly, in our study, different baselines lead to different gradient approximations.

Here, we discuss the rationale for choosing \(E[f()]\) as the baseline. Considering \(_{i}_{i}f(_{i})\) as the general form of the baseline (\(_{i}\) is a distribution over \(\{_{1},,_{n}\}\), i.e., \(_{i}_{i}=1\)), we have:

**Remark 4.1**.: _When \(_{i}_{i}f(_{i})\) is used as the baseline and \(f(_{i})-f(_{j})\) is approximated as \(_{j})}{_{j}}(_{i}-_{j})\), we mark the resulting first-order approximation of \(\) as \(_{}\). Then, we have \(E[_{}}{_{}}_{}]= _{}\)._

The derivations of Remark 4.1 are provided in Appendix C. Intuitively, since \(_{}\) is the output of the softmax function, it could have very small values, which makes \(_{}}{_{}}\) to be unreasonably large and leads to undesired instability. Therefore, we suggest that \(E[f()]\) is a better choice of baseline when it comes to gradient approximation, since its corresponding gradient approximation is free of the instability \(_{}}{_{}}\) brought.

It is worth mentioning that, when setting \(\) as \(\), the result of Remark 4.1 echoes some existing studies. Specifically, both Gregor et al. (2014) and Pervez et al. (2020) propose to approximate \(\) as \(_{}}_{}\), which matches the result of Remark 4.1 by setting \(=\).

In Section 6, we compared the corresponding second-order approximation when treating \(E[f()]\) and \(_{i}f(_{i})\) as the baseline, respectively. We observed that gradient estimators that use \(E[f()]\) as the baseline consistently outperform gradient estimators that use \(_{i}f(_{i})\) as the baseline, which verifies our intuition and demonstrates the importance of the baseline selection.

### Independence of ReinMax over Baseline Subtraction

To better understand the effectiveness of ReinMax, we further provide an alternative derivation that does not rely on the selection of the baseline. For simplicity, we only discuss \(}{_{k}}\) and mark it as \(_{k}\). Similar to Equation 2, we have:

\[_{k}:=}{_{k}}=_{i}f(_{i})_{i}}{d\,_{k}}=_{k}_{i}_{i}(f( _{k})-f(_{i})).\] (8)

It is worth mentioning that the derivation of Equation 8 leverages the derivative of the softmax function (i.e., for \(=()\), we have \(_{i}/_{k}=_{k}(_{ik}-_{i})\)) and does not involve the baseline subtraction technology.

**Remark 4.2**.: _In Equation 8, we approximate \(f(_{k})-f(_{i})\) as \((_{i})}{_{i}}+_{k})}{_{k}})(_{k}-_{i})\), and mark the resulting second-order approximation of \(_{k}\) as \(\)2nd-order-wo-baseline\({}_{,k}=_{k}_{i}_{i}(_{i})}{ _{i}}+_{k})}{_{k}})( _{k}-_{i})\), Then, we have \(E[_{}]=\)2nd-order-wo-baseline_

The proof of Remark 4.2 is provided in Appendix D.

As in Remark 4.2, applying the Heun's method on Equation 8 and Equation 6 lead to the same gradient estimator, which implies another benefit of using \(E[f()]\) as the baseline: the resulting gradient estimator does not rely on additional prior (i.e., its derivation can be free of baseline subtraction).

## 5 Temperature Scaling for Gradient Estimators

Here, we discuss how to apply temperature scaling, a technique widely used in gradient estimators, to our proposed method, ReinMax. While the typical practice is to set the temperature \(\) to small values for STGS, we show that ST and ReinMax need a different strategy.

**Temperature Scaling for \(_{}\).**  As introduced in Section 2, \(_{}\) conduct a two-step approximation: (1) it approximates \(_{}E[f()]\) as \(_{}E[f(_{}(+)))]\); (2) it approximates \(_{}(+))}{_{}(+)}\) as \()}{}\). Since the bias introduced in both steps can be controlled by \(\), \(_{}\) prefers to set \(\) as a relatively small value.

**Temperature Scaling for \(_{}\) and \(_{}\).**  As in Section 4, it does not involve temperature scaling to show \(_{}\) and \(_{}\) work as the first-order and the second-order approximation to the gradient. Correspondingly, temperature scaling technology cannot help to reduce the bias for \(_{}\) in the same way it does for \(_{}\). As in Figure 2, STGS, GR-MCK, and GST-1.0 work better when setting the temperature \( 1\). ST and ReinMax work better when setting the temperature \( 1\).

Thus, we incorporate temperature scaling to smooth the gradient approximation (\(_{}=_{}()\)) as \(_{}=2^{}- _{}\). It is worth emphasizing that \(\) in \(_{}\) is used to stabilize the gradient approximation (instead of reducing bias) at the cost of accuracy. Therefore, the value of \(\) should be larger or equal to \(1\).

Figure 2: Training \(-\)ELBO on MNIST-VAE (lighter color indicates better performance). STGS, GST-1.0, and GR-MCK prefer to set the temperature \( 1\). ST and ReinMax prefer to set \( 1\).

## 6 Experiments

Here, we conduct experiments on polynomial programming, unsupervised generative modeling, and structured output prediction. In all experiments, we consider four major baselines: Straight-Through (ST), Straight-Through Gumbel-Softmax (STGS), Gumbel-Rao Monte Carlo (GR-MCK), and Gapped Straight-Through (GST-1.0). For a more comprehensive comparison, we run a complete grid search on the training hyper-parameters for all methods. Also, we would reference results from the literature when their setting is comparable with ours. More details are elaborated in Appendix F.

### Polynomial Programming

Following previous studies (Tucker et al., 2017; Grathwohl et al., 2018; Pervez et al., 2020; Paulus et al., 2021), we start with a simple problem. Consider \(L\) i.i.d. latent binary variables \(_{1},,_{L}\{0,1\}\) and a constant vector \(^{L 1}\), we parameterize the distributions of \(\{_{1},,_{L}\}\) with \(L\) softmax functions, i.e., \(_{i}}}{{}}( (_{i}))\) and \(_{i}^{2}\). Following previous studies, we set every dimension of \(\) as \(0.45\), i.e., \( i\), \(_{i}=0.45\), and use \(_{}E_{}[-\|_{}^{p}}{L}]\) as the objective.

**Training Curve with Various \(p\).** We first set the number of latent variables (i.e., \(L\)) as 128 and batch size as 256. The training curve is visualized in Figure 1 for \(p=1.5\), \(2\), and \(3\). In all cases, ReinMax achieved near-optimal performance and the best convergence speed. Meanwhile, we can observe that ST and GST-1.0 do not perform well in all three cases. Although the final performance of STGS and GR-MCK is close to ReinMax, ReinMax has a faster convergence speed.

### ListOps

We conducted unsupervised parsing on ListOps (Nangia and Bowman, 2018) and summarized the average accuracy and the standard derivation in Table 1. We also visualized the accuracy and loss on the valid set in Figure 3. Although the ST algorithm performs poorly on polynomial programming, it achieves a reasonable performance on this task. Also, while all baseline methods perform similarly, our proposed method stands out and brings consistent improvements. This further demonstrates the benefits of achieving second-order accuracy and the effectiveness of our proposed method.

    & AVG & \(8 4\) & \(4 24\) & \(8 16\) & \(16 12\) & \(64 8\) & \(10 30\) \\  STGS & 105.20 & 126.85\(\)0.85 & 101.32\(\)0.43 & 99.32\(\)0.33 & 100.09\(\)0.32 & 104\(\)0.41 & 99.63\(\)0.63 \\ GR-MCK & 107.06 & 125.94\(\)0.71 & 99.96\(\)0.25 & 99.58\(\)0.31 & 102.54\(\)0.48 & 112.34\(\)0.48 & 102.02\(\)0.18 \\ GST-1.0 & 104.25 & 126.35\(\)1.24 & 101.49\(\)0.44 & 98.29\(\)0.66 & 98.12\(\)0.57 & 102.53\(\)0.57 & 98.64\(\)0.33 \\  ST & 116.72 & 135.53\(\)0.31 & 112.03\(\)0.03 & 112.94\(\)0.32 & 113.31\(\)0.43 & 113.90\(\)0.28 & 112.63\(\)0.34 \\ ReinMax & **103.21** & **124.66\(\)0.88** & **99.77\(\)0.45** & **97.70\(\)0.39** & **98.06\(\)0.53** & **100.71\(\)0.70** & **98.37\(\)0.44** \\   

Table 1: Performance on ListOps.

Figure 3: The accuracy (left) and loss (right) on the valid set of ListOps.

### Mnist-Vae

We benchmark the performance by training variational auto-encoders (VAE) with _categorical_ latent variables on MNIST (LeCun et al., 1998). As we aim to compare gradient estimators, we focus our discussions on training ELBO. We find that training performance largely mirrors test performance (Dong et al., 2020, 2021; Fan et al., 2022) and briefly discussed test ELBO in Appendix F.

**Biases of the Approximated Gradient.** With 4 latent dimensions and 8 categorical dimensions, we iterate through the whole latent space (the size of the latent space is only \(4096\)), compute the gradient as in Equation 2, and measured the cosine similarity between the gradient of latent variables and various approximations. As visualized in Figure 4, ReinMax achieves consistently more accurate gradient approximation across the training and, accordingly, faster convergence. Also, we can observe that, besides faster convergence, the performance of ReinMax is more stable.

**Experiment with Larger Latent Spaces.** Let us proceed to larger latent spaces. First, we consider 4 settings with the latent space of \(2^{48}\). Then, following Fan et al. (2022), we also conduct experiments with 10 latent dimensions and 30 categorical dimensions (the size of the latent space is \(10^{30}\)). As summarized in Table 2, ReinMax achieves the best performance on all configurations.

**GST-1.0 Performance on Different Problems.** It is worth mentioning that, despite GST-1.0 achieving good performance on most settings of MNIST-VAE, it fails to maintain this performance on polynomial programming and unsupervised parsing, as discussed before. Upon discussing with Fan et al. (2022), we suggest that this phenomenon is caused by the characteristic of GST-1.0, which behaves similarly to ST on problems with a near one-hot optimal distribution. In other words, GST-1.0 has an implicit prior and prefers distributions that are not one-hot. At the same time, a different variant of GST (i.e., GST-p) would behave similarly to STGS on problems with a near one-hot optimal distribution, which achieves a significant performance boost over GST-1.0 on polynomial programming. However, on MNIST-VAE and ListOps, GST-p achieves an inferior performance.

This observation verifies our intuition that, without understanding the mechanism of ST, different applications have different preferences on its configurations. Meanwhile, ReinMax achieves consistent improvements in all settings, which greatly simplifies future algorithms developments.

    &  &  &  \\   & validation & test & validation & test & validation & test \\  GDAS + STGS\({}^{*}\) & 89.68\(\)0.72 & 93.23\(\)0.58 & 68.35\(\)2.71 & 68.17\(\)2.50 & 39.55\(\)0.00 & 39.40\(\)0.00 \\  GDAS + ReinMax & **90.01\(\)0.12** & **93.44\(\)0.23** & **69.29\(\)2.34** & **69.41\(\)2.24** & **41.47\(\)0.79** & **42.03\(\)0.41** \\   

Table 4: Performance on NATS-Bench. \({}^{*}\) Baseline results are referenced from Dong et al. (2020).

Figure 4: The training \(-\)ELBO (left) and the cos similarity between the gradient and its approximations (right) on MNIST-VAE (with 4 latent dimensions and 8 categorical dimensions).

    & ReinMax & ST & STGS & GST-1.0 & GR-MCK\({}_{100}\) & GR-MCK\({}_{300}\) & GR-MCK\({}_{1000}\) \\  QP & 0.2s / 6.5Mb & 0.2s / 5.0Mb & 0.2s / 5.3Mb & 0.2s / 8.0Mb & 0.8s / 0.3Gb & 2.2s / 1Gb & 6.6s / 3Gb \\  MNIST-VAE & 5.2s / 13Mb & 5.2s / 13Mb & 5.2s / 13Mb & 5.2s / 13Mb & 5.2s / 76Mb & 5.2s / 0.2Gb & 5.4s / 0.6Gb \\   

Table 3: Average time cost (per epoch) / peak memory consumption on quadratic programming (QP) and MNIST-VAE. QP is configured to have 128 binary latent variables and 512 samples per batch. MNIST-VAE is configured to have 10 categorical dimensions and 30 latent dimensions.

### Applying ReinMax to Differentiable Neural Architecture Search

To demonstrate the applicability of ReinMax as a drop-in replacement, we conduct experiments following the topology search setting in the NATS-Bench benchmark (Dong et al., 2020), and summarize the results in Table 4. GDAS is an algorithm that employs STGS to estimate the gradient of neural architecture parameters (Dong and Yang, 2019). We replaced STGS with ReinMax as the gradient estimator (configurations elaborated in Appendix F). ReinMax brings consistent performance improvements across all three datasets, demonstrating the great potential of ReinMax.

### Comparisons with REINFORCE-style Methods

Here, we conduct experiments to discuss the difference between ReinMax and REINFORCE-style methods. First, following Fan et al. (2022), we conduct experiments on the setting with a larger batch size (i.e., 200), longer training (i.e., \(5 10^{5}\) steps), 32 latent dimensions, and 64 categorical dimensions (details are elaborated in Appendix F). As in Table 5, ReinMax outperforms all baselines, including two REINFORCE-based methods (Dong et al., 2020, 2021).

We further conduct experiments to compare with the state of the art. Specifically we apply ReinMax to Bernoulli VAEs on MNIST, Fashion-MNIST (Xiao et al., 2017), and Omniglot(Lake et al., 2015), adhering closely to the experimental settings of Shi et al. (2022), including pre-processing, model architecture, batch size, and training epochs. As in Tables 6 and Figure 5, ReinMax consistently outperforms RODEO across all settings. To better understand the difference between RODEO and ReinMax, we conduct more experiments on polynomial programming (as elaborated in Appendix F.6).

    & ^{*}\)} & ^{*}\)} & ^{*}\)} & ^{*}\)} & ^{*}\)} &  \\   & MNIST & 101.99\(\)0.04 & 100.84\(\)0.14 & / & 100.94\(\)0.01 & 100.46\(\)0.13 & **97.83\(\)0.36** \\  & Fashion-MNIST & 237.74\(\)0.12 & 237.05\(\)0.12 & / & 237.40\(\)0.11 & 236.88\(\)0.12 & **234.53\(\)0.42** \\  & Omniglot & 115.70\(\)0.08 & 115.32\(\)0.07 & / & 115.06\(\)0.12 & 115.01\(\)0.05 & **107.51\(\)0.42** \\   & MNIST & / & / & 102.75\(\)0.08 & 102.14\(\)0.06 & 101.89\(\)0.17 & **98.17\(\)0.29** \\  & Fashion-MNIST & / & / & 237.68\(\)0.13 & 237.55\(\)0.16 & 237.44\(\)0.09 & **234.89\(\)0.21** \\   & Omniglot & / & / & 116.50\(\)0.04 & 116.39\(\)0.10 & 115.93\(\)0.06 & **107.79\(\)0.27** \\   

Table 6: Train \(-\)ELBO of \(2 200\) VAE on MNIST, Fashion-MNIST, and Omniglot. \({}^{*}\) Baseline results are referenced from Shi et al. (2022). K refers to the number of evaluations.

Figure 5: \(2\)200 VAE training curves on MNIST, Omniglot, and Fashion-MNIST when K=2 or 3.

Overall, ReinMax achieves better performance in more challenging scenarios, i.e., smaller batch size, more latent variables, or more complicated problems. Meanwhile, REINFORCE and RODEO achieve better performance on simpler problem settings, i.e., larger batch size, fewer latent variables, or simpler problems. This observation matches our intuition:

* REIFORCE-style algorithms excel as they provide unbiased gradient estimation but may fall short in complex scenarios, since they only utilize the zero-order information (i.e., a scalar \(f()\)).
* ReinMax, using more information (i.e., a vector \()}{}\)), handles challenging scenarios better. Meanwhile, as a consequence of its estimation bias, ReinMax leads to slower convergence in some simple scenarios.

### Discussions

Choice of Baseline.As introduced in Section 4.1, the choice of subtraction baseline has a huge impact on the performance. Here, we demonstrate this empirically.

We use \(_{i}f(_{i})\) as the baseline and compare the resulting gradient approximation with ReinMax. As visualized in Figure 6, ReinMax, which uses \(E[f()]\) as the baseline, significantly outperforms the one that uses \(_{i}f(_{i})\) as the baseline. We suspect that the gradient approximation using \(_{i}f(_{i})\) as the baseline is very unstable as it contains the \()}\) term.

Temperature Scaling.On MNIST-VAE (four settings with the \(2^{48}\) latent space), we utilize heatmaps to visualize the final performance of all five methods under different temperatures, i.e., \(\{0.1,0.3,0.5,0.7,1,2,3,4,5\}\). As in Figure 2, these methods have different preferences for the temperature configuration. Specifically, STGS, GST-1.0, and GR-MCK prefer to set the temperature \( 1\). Differently, ST and ReinMax prefer to set the temperature \( 1\). These observations match our analyses in Section 5 that a small \(\) can help reduce the bias introduced by STGS-style methods. Also, it verifies that ST and ReinMax work differently from STGS, GST-1.0, and GR-MCK.

Efficiency.As summarized in Table 3, we can observe that, since GR-MCK uses the Monte Carlo method to reduce the variance, it has larger time and memory consumption, which becomes less significant with fewer Monte Carlo samples (we use GR-MCK\({}_{s}\) to indicate GR-MCK with \(s\) Monte Carlo samples). Meanwhile, all remaining methods have roughly the same time and memory consumption. This shows that the computation overheads of ReinMax are negligible.

## 7 Conclusion and Future Work

In this study, we seek the underlying principle of the Straight-Through (ST) gradient estimator. We formally establish that ST works as a first-order approximation of the gradient and propose a novel method, ReinMax, which incorporates Heun's Method and achieves second-order accuracy without requiring second-order derivatives. We conduct extensive experiments on polynomial programming, unsupervised generative modeling, and structured output prediction. ReinMax brings consistent improvements over the state-of-the-art methods.

It is worth mentioning that analyses in this study further guided us to empower Mixture-of-Expert training (Liu et al., 2023). Specifically, for gradient approximation of sparse expert routing, while ReinMax requires the network to be fully activated, Liu et al. (2023) uses \(f()\) as the baseline and only requires the network to be partially activated. In the future, we plan to conduct further analyses on the truncation error to stabilize and improve the gradient estimation.