# _Data Mixture Inference:_ What do BPE Tokenizers Reveal about their Training Data?

*Jonathan Hayase\({}^{}\) &Alisa Liu\({}^{}\)  Yejin Choi\({}^{}\)  Sewoong Oh\({}^{}\)  Noah A. Smith\({}^{}\)

\({}^{}\)University of Washington \({}^{}\)Allen Institute for AI

{jhayase,alisaliu}@cs.washington.edu

Equal contribution, ordered alphabetically.

###### Abstract

The pretraining data of today's strongest language models is opaque; in particular, little is known about the proportions of various domains or languages represented. In this work, we tackle a task which we call _data mixture inference_, which aims to uncover the distributional make-up of training data. We introduce a novel attack based on a previously overlooked source of information: byte-pair encoding (BPE) tokenizers, used by the vast majority of modern language models. Our key insight is that the ordered list of merge rules learned by a BPE tokenizer naturally reveals information about the token frequencies in its training data. Given a tokenizer's merge list along with example data for each category of interest, we formulate a linear program that solves for the proportion of each category in the tokenizer's training set. In controlled experiments, we show that our attack recovers mixture ratios with high precision for tokenizers trained on known mixtures of natural languages, programming languages, and data sources. We then apply our approach to off-the-shelf tokenizers released with recent LMs. We confirm much publicly disclosed information about these models, and also make several new inferences: Gpt-40 and Mistral NeMo's tokenizers are much more multilingual than their predecessors, training on 39% and 47% non-English language data, respectively; Llama 3 extends Gpt-3.5's tokenizer primarily for multilingual (48%) use; Gpt-3.5's and Claude's tokenizers are trained on predominantly code (\( 60\)%). We hope our work sheds light on current design practices for pretraining data, and inspires continued research into data mixture inference for LMs.1

## 1 Introduction

Pretraining data is at the heart of language model development, yet it remains a trade secret for today's strongest models. While it has become more common for model-producing organizations to release model parameters, they rarely share the pretraining data or important details about its construction. In particular, little is known about the proportion of different languages, code, or data sources present in the data; these design decisions require extensive experimentation that few organizations have the resources to perform, and have a significant impact on the resulting LM .

While a long line of membership inference attacks  aim to reveal information about the model's pretraining data, they typically focus on testing whether particular instances or authors contributed to the data. In this work, we tackle a different task we call _data mixture inference_ which, given a set of disjoint categories that cover the training data (e.g., the set of natural and programming languages), aims to uncover each of their proportions.

To this end, we identify a previously overlooked source of information: trained byte-pair encoding tokenizers (BPE; ), which are the near-universal choice for modern language models. Our key insight is that the _ordered merge rules_ learned by a BPE tokenizer _naturally reveal information about the frequency of tokens_ in the tokenizer's training data. During training, the BPE algorithm iteratively finds the ordered pair of tokens with the highest frequency, adds it to the merge list, and applies the merge to the dataset. Therefore, if the pair \((,)\) was merged in the 52nd step (as is the case for Gpt-4o), then it must be the most frequent pair in the data after applying the preceding 51 merges; in this case, it is a signature of substantial code data. Note that at inference-time, new text is tokenized by applying the learned merge rules in-order. Open-source models require open tokenizers; even closed models often have open tokenizers for the purpose of estimating query cost.

Our method builds a linear program where the constraints are derived from the true most-frequent merge at every step in the merge list, and solves for the proportions of each category. We first demonstrate its effectiveness in controlled experiments where we train tokenizers on known mixtures of data. We consider three kinds of data mixtures: natural languages, programming languages, and data sources. Our method is highly effective, achieving accuracy between two and five _orders of magnitude_ better than baselines based on tokenizer efficiency or inspection of the vocabulary.

Then, we apply our method to infer previously unknown distributional information about off-the-shelf, commercial tokenizers (the top of these merge lists are shown in SSC.2 for qualitative inspection). We consider all tokenizers released with Gpt, Llama, and Mistral model families, as well as Gpt-NeoX, Gemma, and Claude, which we will refer to later using their associated model names. We corroborate reported information and public intuition about these tokenizers with exact numbers -- Gpt-2 is trained on predominantly English (99%), Gpt-3.5 is the first in the Gpt family to be trained extensively on code (63%), and Llama trains on only languages that use Latin or Cyrillic scripts. We also make several new inferences: Gpt-4o is much more multilingual than its predecessors, training on 39% non-English text, with 68 languages that make up at least 0.1% of the data. Llama 3 extends Gpt-3.5's tokenizer primarily for multilingual use, using 48% non-English text (and 30% code data). Finally, we surprisingly infer that all tokenizers we study are trained on 7% - 26% book data, potentially because books use a more standard vocabulary compared to the web.

Inferring tokenizer training data mixtures has several important implications. Ideally, the tokenizer training data is representative of the LM's pretraining data ; disconnects can lead to poor encoding

Figure 1: **Illustration of our problem statement on a simple example where two tokenizers are trained on different mixtures of English and Python data. During training, the BPE algorithm iteratively finds the pair of tokens with the highest frequency in the training data, adds it to the merge list, then applies it to the dataset before finding the next highest-frequency pair. To encode text at inference time, the learned merge rules are applied in order. The resulting order of merge rules is extremely sensitive to the proportion of different data categories present. Our goal is to solve for these proportions, a task which we call _data mixture inference_.**

of the pretraining text  and potential for "glitch tokens" that trigger degenerate model behavior [48; 30; 33]. Additionally, the tokenizer can be seen as a leading indicator of the model developer's priorities, as it is often designed to accommodate future models. Tokenizer training mixtures may also be used to accelerate model-based attacks, for instance by suggesting data categories to prioritize for membership inference. Finally, it can enable external auditing of training data for biases, by identifying under-represented languages or data sources.

## 2 Background: BPE tokenizers

Byte-pair encoding (BPE), introduced by Sennrich et al.  for NLP,2 is a tokenization algorithm that learns subword-based encodings from training data. At a high level, the algorithm iteratively merges frequently co-occurring pairs of tokens until the desired vocabulary size is reached.

More precisely, the training text is first _pretokenized_ by splitting it into "words" that limit the extent of tokenization. Merges cannot bridge these words, and thus the final learned tokens will be _parts_ of these words. Pretokenization can be as simple as splitting on whitespace, so that common sequences of words (e.g., "_it is_") do not become a single token.

After pretokenization, the words are split into bytes, which form the starting vocabulary. Then, the BPE algorithm iteratively counts the frequency of each neighboring pair of tokens and picks the most frequent to be merged next. This merge is added to the merge list and applied to the entire text, and the merged token is added to the vocabulary. For instance, if the merge is (th, e), then all instances of the token sequence th, e will be replaced with the, which is added to the vocabulary. BPE then updates the frequencies of all pairs, and identifies the next most frequent. This continues until the desired vocabulary size is reached. At the end of training, the algorithm has learned an ordered list of merge rules \(m^{(1)},,m^{(M)}\).

To tokenize new text, the tokenizer splits the text into bytes and applies the learned merge rules, in order. As we will see, the merge list reflects rich distributional information about the training data.

## 3 Data mixture inference attack

Suppose we have a set of \(n\) data categories of interest, and data distributions \(\{_{i}\}_{i=1}^{n}\) for each one. Then suppose we receive a BPE tokenizer, which was trained on a large sample of text from the mixture \(_{i=1}^{n}_{i}^{*}_{i}\) with non-negative weights \(^{*}^{n}\) satisfying \(_{i=1}^{n}_{i}^{*}=1\). Given corpora

Figure 2: **Training data mixture predictions for several commercial tokenizers.** Complete results over 112 languages and 5 domains are given in SSC; categories are grouped here for readability. We confirm that Gpt-2 was trained overwhelmingly on English (99%), while Gpt-3.5 is the first model in the Gpt series to train on substantial code data (63%). Gpt-4o is much more multilingual than its predecessors, with 39% of its corpus being non-English text. Llama is also multilingual, but focuses on languages using Latin or Cyrillic scripts (note this category in the figure excludes English). Llama 3\({}^{*}\) results are only based on the last 27,744 merges (the first 100K are copied from Gpt-3.5), which we observe was primarily for multilingual adaptation.

\(\{D_{i}\}_{i=1}^{n}\) sampled from each of the \(_{i}\) respectively, the goal of _data mixture inference_ is to produce a good estimate \(\) of \(^{*}\).

Now we describe how to set up the set of constraints that make up a linear program whose solution is this estimate (SS3.1), reduce the storage requirements (SS3.2), and improve efficiency (SS3.3, SS3.4).

### Data mixture inference via linear programming

We build a linear program (LP) with variables \(\) and constraints derived using information from the tokenizer and our sample corpora. The given tokenizer can be represented by an ordered list of merge rules \(m^{(1)},,m^{(M)}\). For each time step \(t[M]\), we apply all preceding merge rules \(m^{(1)},,m^{(t-1)}\) to our corpora \(D_{i}\) and use \(c_{i,p}^{(t)}\) to denote how many times the token pair \(p\) occurred in the partially merged text. We know that when the tokenizer was trained, the pair \(m^{(t)}\) was more frequent at time \(t\) than any other pair. In other words,

\[_{i=1}^{n}_{i}c_{i,m^{(t)}}^{(t)}_{i=1}^{n}_{i}c_{i,p} ^{(t)}p m^{(t)}.\]

Collecting these constraints for all \(t\) and \(p\) defines a set of possible \(\)'s.

Of course, because we only have samples from the category distributions and not the exact data the tokenizer was trained on, the above linear program may not be feasible, as the counts will include some sampling noise. To address this, we relax the constraints by introducing new non-negative variables \(v^{(t)}\) for all \(t[M]\), and \(v_{p}\) for all pairs \(p\), which represent the degree of constraint violation for each merge and pair, respectively. We replace our constraints with new ones of the form

\[v^{(t)}+v_{p}+_{i=1}^{n}_{i}c_{i,m^{(t)}}^{(t)}_{i=1}^{n} _{i}c_{i,p}^{(t)}p m^{(t)}.\]

In general, we expect \(v^{(t)}\) to be large when \(m^{(t)}\) is over-represented in the tokenizer training data and \(v_{p}\) to be large when \(p\) is over-represented in the mixture defined by \(\). This new system of constraints is guaranteed to be feasible, as the \(v\)'s can be made arbitrarily large. To produce the best possible estimate, our objective is to minimize the total constraint violation \(_{t=1}^{M}v^{(t)}+_{p}v_{p}\). We call the resulting linear program LP1. To estimate \(\), we solve LP1 and report the optimal value of \(\) as \(\).

As written, LP1 can be prohibitively large. If our vocabulary has size \(V\), the total number of constraints scales like \(O(V^{3})\) since there are \(O(V)\) time steps \(t\) to consider and \(O(V^{2})\) competing byte pairs \(p m^{(t)}\). Additionally, there are \(O(V^{2})\) variables \(v_{p}\). The first step to reduce the size is to limit \(t\) to the first \(T\) merges. We will call this truncated program LP1\({}_{T}\). However, even for modest choices of \(T\), LP1\({}_{T}\) can still have millions of variables and tens of billions of constraints. In the

Figure 3: **Illustration of our method on a simple example. We know that after applying in the first \(t-1\) merges to the training data, the \(t^{}\) merge must be the most common pair. More explicitly, this means that \(_{i}\) should give a vector in which the value corresponding to the true next merge is the maximum. Our attack collects these inequalities at every time step to construct the linear program.3**

following sections, we will describe how to efficiently solve \(_{T}\) using simultaneous delayed row (constraint) and column (variable) generation [12; 23].

### Efficient storage of pair counts

First, as a preprocessing step, we apply the target tokenizer to each language corpus \(D_{i}\), recording the pair counts \(c^{(t)}_{i,p}\) after each merge is applied for later use. Naively, this would require a large amount of space, since the number of possible pairs \(p\) scales like \(O(V^{2})\). However, note that \(c^{(t)}_{i,p} c^{(t+1)}_{i,p}\) only when \(p\) overlaps with \(m^{(t)}\). In other words, pairs with no overlap with the most recent merge will have unchanged counts. Thus, there are only \(O(V)\) differences between \(c^{(t)}_{i.}\) and \(c^{(t+1)}_{i.}\). In practice, the number of changes caused by a single merge is usually a few hundred at most. By saving only the incremental changes from each set of pair counts to the next, we can efficiently record the pair counts at every iteration of the tokenization process.

### Efficient constraint violation detection

Our plan is to solve \(_{T}\) using only a subset of its constraints, giving a potential solution \((,v)\). We then check whether \((,v)\) violates any of the constraints of \(_{T}\) and add any such constraints to the subset. This requires an efficient method to detect violated constraints, which we describe below.

For convenience, let \(s^{(t)}_{p}_{i=1}^{n}_{i}c^{(t)}_{i,p}\) and recall that, for a given time step \(t\), we want to check whether \(v^{(t)}+s^{(t)}_{m^{(t)}}_{p}(s^{(t)}_{p}-v_{p})\). Naively, we would do so by iterating over all possible \(p m^{(t)}\) to see if the constraint is violated, which can be quite costly. Moreover, we must do this for all \(t T\). However, by taking advantage of the structure of the \(s^{(t)}_{p}\) as \(t\) varies, we can reduce our work substantially.

The first step is to take each initial pair \(p\) and add it to a priority queue with priority \(s^{(0)}_{p}-v_{p}\). This can be done in aggregate in \(O(V^{2})\) time using a fast heap building algorithm. Now, we can iterate through the pairs in descending \(s^{(0)}_{p}-v_{p}\) order using the queue's delete-min operation. For each pair \(p\), we can check whether \(v^{(0)}+s^{(0)}_{m^{(0)}}>s^{(0)}_{p}-v_{p}\) and if not, we mark the corresponding constraint as violated. Once we find a \(p_{}\) satisfying the constraint, we stop, since all pairs remaining in the queue must satisfy their constraints. If there were \(k\) pairs before \(p_{}\) in the queue, then the total time taken is \(O(k V)\).

Crucially, we can quickly update the priority queue to reflect the state at \(t=1\). Since we precomputed all count changes from \(c^{(0)}_{i,p}\) to \(c^{(1)}_{i,p}\), we know what queue entries need to be updated or inserted. If \(k_{}\) new pairs were created and \(k_{}\) pairs had their counts changed when pair \(m^{(0)}\) was merged, then we can update the priority queue using \(k_{}\) insert operations and \(k_{}\) decrease-priority operations, which can be done in \(O((k_{}+k_{}) V)\) time. Now that we have updated the priority queue, we can repeat the above procedure to check for any constraint violations for \(t=1\). By iterating this process, we can quickly check for violated constraints for all \(t T\).

### Lazy variable and constraint generation

Now we are ready to efficiently solve \(_{T}\). We begin by guessing uniform proportions for \(\), and \(v^{(t)}=v_{p}=0\) for all \(t,p\). Then we use our constraint checker to identify violated constraints of \(_{T}\) and construct a lazy version of \(_{T}\), which we denote \(_{T}\), using only those constraints and the variables they contain. We then solve \(_{T}\), which gives us a new guess for \(\). We repeat the above steps, adding progressively more constraints (along with their variables) to \(_{T}\) until we find a solution that is also feasible for \(_{T}\). It follows that this solution is optimal for \(_{T}\) since the two programs share the same objective. This is guaranteed to happen eventually because there are a finite number of constraints and variables to add.

In practice, the constraint violation detection can typically check \(T=30000\) merges in less than 10 seconds. On difficult instances, such as those for commercial tokenizers in SS5, the full solve can take up to a day to complete. Easier instances like those in Table 1 can be solved in a few minutes.

## 4 Experiments

In our initial experiments, we train tokenizers on known data mixtures and measure the accuracy of our attack's prediction. We consider mixtures of natural languages, programming languages, and data sources (which we also refer to as domains).

### Setup

Because BPE tokenizers operate on bytes, we measure the proportion of each category in terms of bytes. Each tokenizer is trained on a mixture of \(n\) categories, where \(n\) varies from 5 and 112. We randomly sample the \(n\) categories and their weights from the unit simplex (using the algorithm from ), and train 100 tokenizers on 10 GB of data. The data for each category is sampled from the corresponding corpus; if there is not enough data for any category (e.g., we have many low-resource languages), we duplicate the data until the necessary amount is achieved, to preserve the desired mixture ratio. We train tokenizers using the HuggingFace tokenizers library with a maximum vocabulary size of 30,000, and apply a minimal set of common pretokenization operations: we split on whitespace and only allow digits to be merged with other contiguous digits.

After training the tokenizers, we apply our attack. We estimate merge frequencies for each category by sampling 1 GB of data per category, or less if there is not that much data. Note that the data used for training the tokenizer and estimating pair frequencies are sampled from the same distribution, but are not necessarily the same data. We use **mean squared error** to evaluate the estimated proportions, \(_{i=1}^{n}(_{i}-_{i}^{*} )^{2}\). In practice, we report \(_{10}()\).

In SSB.4, we analyze how our attack's performance varies with the amount of data used and the number of merges \(T\) considered from the merge list.

Natural Language MixturesWe use the Oscar v23.01 corpus , which is based on the Nov/Dec 2022 dump from Common Crawl. We consider the 112 languages with at least 1 MB of data.

   \(n\) & **Method** & **Languages** & **Code** & **Domains** \\   & Random & \(-1.39_{ 0.36}\) & \(-1.39_{ 0.36}\) & \(-1.39_{ 0.36}\) \\  & TEE & \(-2.02_{ 0.41}\) & \(-2.54_{ 0.42}\) & \(-1.69_{ 0.29}\) \\  & TC & \(-2.12_{ 0.49}\) & \(-1.92_{ 0.36}\) & \(-1.64_{ 0.35}\) \\  & Ours & \(_{ 1.31}\) & \(_{ 0.79}\) & \(_{ 0.94}\) \\   & Random & \(-1.84_{ 0.23}\) & \(-1.84_{ 0.23}\) & \(-\) \\  & TEE & \(-2.29_{ 0.26}\) & \(-2.59_{ 0.24}\) & \(-\) \\  & TC & \(-2.55_{ 0.36}\) & \(-2.38_{ 0.20}\) & \(-\) \\  & Ours & \(_{ 1.04}\) & \(_{ 0.64}\) & \(-\) \\   & Random & \(-2.70_{ 0.13}\) & \(-2.70_{ 0.13}\) & \(-\) \\  & TEE & \(-3.07_{ 0.16}\) & \(-3.15_{ 0.13}\) & \(-\) \\  & TC & \(-3.42_{ 0.23}\) & \(-2.38_{ 0.20}\) & \(-\) \\  & Ours & \(_{ 1.12}\) & \(_{ 1.11}\) & \(-\) \\   & Random & \(-3.82_{ 0.07}\) & \(-\) & \(-\) \\  & TEE & \(-4.15_{ 0.08}\) & \(-\) & \(-\) \\   & TC & \(-4.46_{ 0.12}\) & \(-\) & \(-\) \\   & Ours & \(_{ 1.28}\) & \(-\) & \(-\) \\   

Table 1: **Experimental results for controlled experiments.** The settings we consider are mixtures of natural languages, mixtures of programming languages, and mixtures of domains. \(n\) denotes the number of categories in the mixture, which are drawn from 112 natural languages, 37 programming languages, or 5 domains. In each cell, we report the mean and standard deviation of \(_{10}()\) over 100 trials; note that a decrease by 1 corresponds to a 10\(\) improvement in the MSE. In addition to a **Random**-guessing baseline, we implement two alternative approaches to the problem: **TEE** (Tokenizer Encoding Efficiency) uses the tokenizer’s encoding efficiency on each data category, and **TC** (Token Classification) assigns each token in the vocabulary to a data category based on frequency.

Programming Language MixturesWe use the GitHub split of RedPajama . To determine the programming language for each record, we map the file extension to its associated language (e.g.,.py \(\)Python). This leads to a total of 37 programming languages.

Domain MixturesWe consider the following five English domains (adapted from ), instantiated by data from the RedPajama dataset: **Wikipedia**, containing English Wikipedia dumps from Jun-Aug 2022, **Web**, Common Crawl data that was de-duplicated and filtered for English, **Books** from the Gutenberg Project and Books3 of The Pile, **Code** from GitHub, and **Academic**, which contains LaTeX files of scientific papers on ArXiv.

For dataset details, e.g., the full list of categories and data sizes, please see SSB.

### Baselines

We construct two intuitive baselines: one based on tokenizer encoding efficiency and one based on analysis of tokens in the vocabulary. Neither takes the BPE training algorithm into account.

Baseline based on tokenizer encoding efficiency (TEE)Intuitively, we expect data categories with greater representation in the training data to be encoded more efficiently by the resulting tokenizer. To capture this, we calculate a given tokenizer's byte-to-token ratio on each category (a more efficient tokenizer will encode more bytes per token), then normalize it by that of a reference tokenizer trained on _only_ that category, to control for different categories being inherently easier or harder to encode. Then we learn a log-log linear model to predict each category's true proportion given the encoding efficiency. To ensure correctness, we normalize the resulting set of predictions into a probability distribution.

Baseline based on token classification (TC)We also consider a baseline that assigns each token in the vocabulary to a data category based on its empirical frequency in the sample data. Intuitively, we expect that if there is a large proportion of e.g., English data in the training data, then there will be more "English" tokens. For each token in the vocabulary, we count its occurrences in data sampled from each category, and assign it to the one in which it is most frequent (we find that hard assignment outperforms all variations of soft assignment). Then we count the number of tokens assigned to each category, and normalize the counts to produce an estimate of the data mixture.

### Results

Shown in Table 1, our attack is highly effective. Over all mixture types and values of \(n\), we achieve mean MSE two and six _orders of magnitude_ better than random guessing. In contrast, the baselines based on tokenizer efficiency (TEE) and token classification (VC) do not come close to the kind of precision possible with our attack, achieving at best one order of magnitude better than random.

We observe that the setting with the highest attack success is mixed languages, whereas the most challenging is mixed English domains. This is perhaps unsurprising when considering the source of signal for our attack, which is the different token pair frequencies in different data categories. Intuitively, we would expect these to be very different for different natural languages, which have distinct vocabularies. In contrast, programming languages can share many syntactic features, such as using indents, curly brackets {}, and English variable names. Even more so, English data from different domains (e.g., books vs. Wikipedia) will largely share the same vocabulary but have subtle differences in token frequencies due to style, topic, and formatting. Nonetheless, even in this most challenging setting, we achieve accuracy \(100\) better than either baseline.

## 5 Attacking commercial tokenizers

After validating our attack in synthetic experiments (SS4), we apply it to infer training data mixtures of off-the-shelf commercial tokenizers. We refer to tokenizers by the name of the model they were first released with, whose pretraining data they most likely reflect. We consider Gpt-2 , Gpt-3.5 , Gpt-4o , Llama , Llama 3 , Mistral , Mistral-NeMo , Gpt-NeoX , Claude , and Gemma. While many of these are closed models, their tokenizers are publicly available so that customers can estimate the cost of queries ahead of time. We note that the Llama, Gemma, and Mistral tokenizers use _characters_ instead of bytes as the base vocabulary for the BPE algorithm; this does not affect our attack, but we discuss the distinction in SSC.6.

In these experiments, we aim to infer the proportion of different natural languages, code, and English domains, for a total of 116 categories. We consider code as a single category (not split into separate programming languages) because some languages like Markdown and Pod6 are almost entirely English, and we do not expect the distribution of programming languages in pretraining to differ substantially from that of GitHub, the largest public code hosting platform. To infer the distribution of English domains, we replace the English category with the four English domains from SS4 (web, books, Wikipedia, and academic), which we expect to approximately cover the English data.

Our predictions are shown in Figure 2, with specific numbers in SSC.1. Below, we discuss our findings in comparison with publicly disclosed information about these models.

### Gpt models

All tokenizers accompanying Gpt models are open-source on tiktoken.4 There are three such tokenizers, released with Gpt-2, Gpt-3.5, and the very recent Gpt-4o.5

Gpt-2Gpt-2 was trained on WebText, consisting of text scraped from outbound links from Reddit, and filtered to be English-only. The Gpt-2 tokenizer was reused for Gpt-3. Indeed, we confirm the training data consists of 99.1% English. However, we surprisingly estimate that only 83.6% of the data was web, with another 15.4% being books, which were not explicitly included in WebText. In a data contamination analysis, the authors indeed report that they find books in WebText, but our estimate suggests the contamination may be deeper. We note that books were a popular source of pretraining data for early Transformer LMs, with Gpt-1 being trained entirely on BooksCorpus .

Gpt-3.5The Gpt-3.5 family of models is known to depart from its predecessors by training on large amounts of code: the first model in this family was code-davinci-002, trained on text and code. In fact, some evidence suggests that Gpt-3.5's large leap in reasoning abilities comes from this code data, which intuitively requires similar procedural skills . The Gpt-3.5 tokenizer was reused for Gpt-4.

Indeed, we estimate that Gpt-3.5 is trained on 62.6% code. In the domain breakdown, 27.3% is of the data is web, 6.8% books, and 0.2% academic articles. The substantial representation of books (though lower than Gpt-2) is consistent with findings that this model has memorized a wide collection of copyrighted books .

Gpt-4oGpt-4o is a multimodal model announced as more multilingual than its predecessors; its tokenizer achieves a better compression rate on non-English languages, and the model has notably better non-English performance.

Our findings support this. Gpt-4o is trained on 39.0% non-English text, compared to only 3.2% for Gpt-3.5. The language distribution has a thick non-English tail, with 68 languages that make up at least 0.1% of the data: the most common are French (2.9%), Russian (2.8%), Spanish (2.8%), Portuguese (2.3%), Dutch (2.0%), German (1.8%), Arabic (1.6%), and Hindi (1.4%). Additionally, Gpt-4o was trained on 7.4% books.

### Llama models

LlamaThe training data for Llama is known to be primarily English, though the Wikipedia split "_covers 20 languages which use either the Latin or Cyrillic scripts:_ big, ca, cs, da, de, en, es, fr, hr, hu, it, nl, pl, pt, ro, ru, sl, sr, sv, uk". The training data is reportedly sourced from Common Crawl (67% of examples), C4 (15.0%), Github (4.5%), Wikipedia (4.5%), Books (4.5%), ArXiv (2.5%), and StackExchange (2.0%). The Llama tokenizer was reused for Llama2.

We corroborate that Llama is indeed primarily made up of the stated languages; when combined with code, this sums to 95.7% of the training corpus. Indeed, other generally high-resource languages, such as Chinese, Arabic, and Hindi, have 0.0% representation. However, we predict a very different domain distribution compared to what is reported in the paper for Llama's pretraining data. We predict that the tokenizer is trained on 23.1% books, 11.3% web, 6.7% Wikipedia, and 8% ArXiv. The high representation of books is surprising - we hypothesize that the tokenizer was trained on a different distribution than the LM, primarily focusing on books which uses a more standard vocabulary compared to web data.

Llama3We observe that Llama 3, rather than training a new tokenizer, extends Gpt-3.5's merge list (of 100,000 merges) with an extra 27,744 merges. Thus, we apply our attack to these new merges to infer what data was used to _extending_ the Llama 3 tokenizer. It is reported that Llama 3 is trained on more than 5% of "_high-quality non-English text that covers over 30 languages._" We find that Llama 3 is extended with primarily non-English text (48.5%) and code (30.2%), indicating that the goal of extending Gpt-3.5's tokenizer was primarily for multilingual use.

### Mistral

Mistraldistral models "_handle English, French, Italian, German and Spanish_" ; indeed, we find that these are the top five languages in the training data. There is a long tail of other languages, but they predominantly (97%) use either the Latin or Cyrillic script.

Mistral NemoIn contrast, Mistral Nemo was "_designed for global, multilingual applications, bringing frontier AI models to... all languages._" It introduces a new tokenizer (based on tiktoken instead of sentencepiece), which is the most multilingual of tokenizers we study, training on 46.6% non-English text. French (6.3%) and Arabic (4.8%) are the most common non-English languages.

### Gpt-NeoX

The tokenizer of Gpt-NeoX  was trained on the Pile  with "_certain components... upsampled._" It is popularly re-used in open-source model development, including by OLMo , Pythia, and Dclm . Though our domains do not map neatly onto the constituent datasets of the Pile, our inference is generally consistent, with a prediction of 43.7% web, 26.3% books, 12.1% academic, 15.2% code, and 2.7% non-English text.

### Gemma

Gemma is reported as training on "_primarily-English data from web documents, mathematics, and code._" German uses a subset of the Gemini tokenizer; we believe it is likely that this subset was obtained by truncation, which would make our inferences valid for Gemini as well. We predict that Gemma is trained on 45.7% English, which comes from 25.6% web, 12.8% books, 4.3% academic, and 3.0% Wikipedia. It is also trained on 28.4% non-English text, which explains its large multilingual vocabulary of 256,000 tokens. However, compared to Gpt-4o, the multilingual representation is more skewed toward languages that use Latin or Cyrillic scripts.

### Claude

Very little is known about models from the Claude family, but a remark in the Anthropic SDK suggests that Claude 1  and 2  share the same tokenizer, which is open-source, while Claude 3  uses a different (closed) tokenizer. Our attack predicts that Claude was trained on 57.5% code, 38.8% English, and 3.7% other languages. Moreover, half of its data (17.4% overall) comes from books, with substantial contribution from Wikipedia (3.7%) and academic text (5.1%) as well. The lack of multilingual training data likely explains why a new tokenizer was trained for Claude 3, which boasts "_increased capabilities... in non-English languages_" .

Robustness analysis

### Is the attack robust to distribution shift?

We measure the impact of using out-of-distribution data instead of data from the tokenizer's training distribution to count merge frequencies. Note that in the main experiments, we show that the attack is effective at separating English domains, so performance degradation under distribution shift is expected. To empirically measure this, we train tokenizers on mixtures of \(n=10\) languages using web data from Oscar, but estimate merge frequencies using corresponding language splits of Wikipedia, a substantially different domain. Using the same settings as the main experiments (SS4), we achieve \(\) MSE of \(-3.53\), compared to \(-7.66\) with no shift. Thus, the attack performance drops considerably under this extreme distribution shift, while remaining 100\(\) better than random.

### Is the attack robust to unaccounted-for categories?

In a typical attack setting, the attacker may not have explicitly accounted for every source of data used to train the tokenizer. To show that our approach is robust in the presence of such data, we modify our \(n=112\) languages experiment from our main experiments (SS4). We randomly withhold a random subset of 1 to 50 languages from the solver and measure the resulting prediction error on the remaining languages. Results are shown in Figure 4. Although the performance of our method does worsen as the amount of unknown data increases, the predictions remain substantially better than random.

## 7 Related work

Attacks on LMsMembership inference, the task of inferring whether a particular example was part of the training data, has been studied in great depth for language models . Attacks commonly use model-assigned probabilities, potentially with another reference model, to make an inference. The problem remains extremely difficult, with a recent survey finding that many attacks perform near random when pretraining data is deduplicated, as in common practice .

Closely related to this, some memorization attacks aim to extract memorized examples from the pretraining data via prompting attacks . Recently, there has even been progress in recovering the parameters of black-box LMs through model stealing attacks .

Distribution inferenceIn contrast to membership inference, _distribution inference_ is concerned with inferring global properties of a model's training data, but has not previously been studied for LM pretraining data. In early works, these attacks were successful against machine learning classifiers , CNNs , and GANs . More recently, some works show that multi-party machine learning can leak property information such as which authors contributed to the data . All previous distribution inference attacks take a meta-classifier approach, where models are trained on datasets with different properties, then a meta-classifier is trained using those models.

## 8 Conclusion

In this work, we present a data mixture inference attack that solves for the distributional make-up of a tokenizer's training data, which is commonly representative of the language model's pretraining data. Beyond the properties we study, we believe there is still a wealth of information hidden in tokenizer merge lists. This can shed light on the secretive and often contentious design decisions surrounding pretraining data today, potentially enabling external auditing for safety, copyright issues, and distributional biases. We hope our work will inspire continued research into inferring more global properties of training data, for tokenizers and language models more generally.

Figure 4: Performance remains much better than random even with large amounts of unknown data.