# Transductive Active Learning:

Theory and Applications

 Jonas Hubotter

Department of Computer Science

ETH Zurich, Switzerland

&Bhavya Sukhija

Department of Computer Science

ETH Zurich, Switzerland

&Lenart Treven

Department of Computer Science

ETH Zurich, Switzerland

&Yarden As

Department of Computer Science

ETH Zurich, Switzerland

&Andreas Krause

Department of Computer Science

ETH Zurich, Switzerland

Correspondence to jonas.huebotter@inf.ethz.ch

###### Abstract

We study a generalization of classical active learning to real-world settings with concrete prediction targets where sampling is restricted to an accessible region of the domain, while prediction targets may lie outside this region. We analyze a family of decision rules that sample adaptively to minimize uncertainty about prediction targets. We are the first to show, under general regularity assumptions, that such decision rules converge uniformly to the smallest possible uncertainty obtainable from the accessible data. We demonstrate their strong sample efficiency in two key applications: active fine-tuning of large neural networks and safe Bayesian optimization, where they achieve state-of-the-art performance.

## 1 Introduction

Machine learning, at its core, is about designing systems that can extract knowledge or patterns from data. One part of this challenge is determining not just how to learn given observed data but deciding what data to obtain next, given the information already available. More formally, given an unknown and sufficiently regular function \(f\) over a domain \(\): _How can we learn \(f\) sample-efficiently from (noisy) observations?_ This problem is widely studied in _active learning_ and _experimental design_(Chaloner & Verdinelli, 1995; Settles, 2009).

Active learning methods commonly aim to learn \(f\) globally, i.e., across the entire domain \(\). However, in many real-world problems, **(i)** the domain is so large that learning \(f\) globally is hopeless or **(ii)** agents have limited information and cannot access the entire domain (e.g., due to restricted access or to act safely). Thus, global learning is often not desirable or even possible. Instead, intelligent systems are typically required to act in a more _directed_ manner and _extrapolate_ beyond their limited information. This work formalizes the above two aspects of active learning, which have remained largely unaddressed by prior work. We provide a comprehensive overview of related work in Section 6.

"Directed" transductive active learningWe consider the generalized problem of _transductive active learning_, where given two arbitrary subsets of the domain \(\); a _target space_\(\), and a _sample space_\(\), we study the question:

_How can we learn \(f\) within \(\) by actively sampling observations within \(\)?_This problem is ubiquitous in real-world applications such as safe Bayesian optimization, where \(\) is a set of safe parameters and \(\) might represent parameters outside \(\) whose safety we want to infer. Active fine-tuning of neural networks is another example, where the target space \(\) represents the test set over which we want to minimize risk, and the sample space \(\) represents the dataset from which we can retrieve data points to fine-tune our model to \(\). Figure 1 visualizes some instances of transductive active learning.

Whereas most prior work has focused on the "global" inductive instance \(==\), MacKay (1992) was the first to consider specific target spaces \(\) and proposed the principle of selecting points in \(\) to minimize the "posterior uncertainty" about points in \(\). Since then, several works have studied this principle empirically (e.g., Seo et al., 2000; Yu et al., 2006; Bogunovic et al., 2016; Wang et al., 2021; Kothawade et al., 2021; Bickford Smith et al., 2023). In this work, we model \(f\) as a Gaussian process or (equivalently) as a function in a reproducing kernel Hilbert space, for which the above principle is analytically and computationally tractable. Our contributions are:

* **Theory (Section 3):** We are the first to give rates for the uniform convergence of uncertainty over the target space \(\) to the smallest attainable value, given samples from the sample space \(\) (Theorems 3.2 and 3.3), Our results provide a theoretical justification for the principle of minimizing posterior uncertainty in transductive active learning, and indicate that transductive active learning can be more sample efficient than inductive active learning.
* **Applications:** We show that transductive active learning improves upon the state-of-the-art in the batch-wise _active fine-tuning_ of neural networks for image classification (Section 4) and in _safe Bayesian optimization_ (Section 5).

## 2 Problem Setting

We assume for now that the target space \(\) and sample space \(\) are finite, and relax these assumptions in the appendices. We model \(f\) as a stochastic process and denote the marginal random variables \(f()\) by \(f_{}\), and joint random vectors \(\{f_{}\}_{ X}\) for some \(X,|X|<\) by \(_{X}\). Let \(_{X}\) denote the noisy observations of \(_{}\), \(\{y_{}=f_{}+_{}\}_{ X}\), where \(_{}\) is independent noise.2 We study the "adaptive" setting, where in round \(n\) the agent selects a point \(_{n}\) and observes \(y_{n}=y_{_{n}}\). The agent's choice of \(_{n}\) may depend on the outcome of prior observations \(_{n-1}}}{{=}}\{(_{i},y_{i })\}_{i<n}\).

Background on information theoryWe briefly recap several important concepts from information theory of which we provide formal definitions in Appendix B. The (differential) entropy \([]\) is one possible measure of uncertainty about \(\) and the conditional entropy \([]\) is the (expected) posterior uncertainty about \(\) after observing \(\). The information gain \((;)=[]-[]\) measures the (expected) reduction in uncertainty about \(\) due to \(\). We denote the information gain about \(\) from observing \(X\) by \((_{};_{X})\). The maximum information gain about \(\) from \(n\) observations within \(\) is

\[_{,}(n)}}{{=}} _{X\\ |X| n}(_{};_{X}).\]

This "information capacity" measures the information about \(_{}\) that is accessible from within \(\), and has been used previously (e.g., by Srinivas et al., 2009; Chowdhury and Gopalan, 2017; Vakili et al., 2021) in the setting where \(==\), taking the form of \(_{n}}}{{=}}_{}(n) }}{{=}}_{,}(n)\). We remark that \(_{,}(n)_{}(n)\) holds uniformly for all \(\), \(\), and \(n\) due to the data processing inequality. Generally, \(_{,}(n)\) can be substantially smaller if the target space is a sparse subset of the sample space.

Figure 1: Instances of transductive active learning with target space \(\) shown in blue and sample space \(\) shown in gray. The points denote plausible observations within \(\) to “learn” \(\). In **(A)**, the target space contains “everything” within \(\) as well as points _outside_\(\). In **(B, C, D)**, one makes observations _directed_ towards learning about a particular target. Prior work on inductive active learning has focused on the instance \(=\).

Main Results

We analyze the following principle for transductive active learning:

_Select samples to minimize the posterior "uncertainty" about \(f\) within \(\)._ (\(\))

This principle yields a family of simple and natural decision rules which depend on the chosen measure of "uncertainty". Two natural measures of uncertainty are (1) the entropy of prediction targets, \([}}]\), and (2) their total variance, \(_{^{}}[f_{^{}}]\). The corresponding decision rules are

\[\] \[\] \[\]

with an implicit expectation over the feedback \(y_{}\). That is, \(\) (short for _Information-based Transductive Learning_) and \(\) (_Variance-based TL_) select \(_{n}\) so as to minimize the uncertainty about the prediction targets \(}}\) (in expectation) after having received the feedback \(y_{n}\). Unlike \(\), \(\) takes into account the mutual dependence between points in \(\). These decision rules were suggested previously (MacKay, 1992; Seo et al., 2000; Yu et al., 2006) without deriving theoretical guarantees; and they generalize several widely used algorithms which we discuss in more detail in Section 6. Most prominently, in the inductive setting where \(\), \(\) reduces to \(_{n}=_{}(f_{};y_{} _{n-1})\), i.e., is "undirected" and reduces to standard uncertainty-based active learning strategies (cf. Appendix C.1). The convergence properties for the special instance of \(\) with \(=\) have been studied extensively. To the best of our knowledge, we are the first to extend these guarantees to the more general setting of transductive active learning.

In our presented results, we make the following assumption.

**Assumption 3.1**.: In the case of \(\), the information gain \(_{}(X)=(}};})\) is submodular. In the case of \(\), the variance reduction \(_{}(X)=\ [}}]- \ [}}}]\) is submodular.

Under this assumption, \(_{}(_{1:n})\) is a constant factor approximation of \(_{X,|X| n}_{}(X)\) due to the seminal result on submodular function maximization by Nemhauser et al. (1978). Similar assumptions have been made, e.g., by Bogunovic et al. (2016) and Kothawade et al. (2021). Assumption 3.1 is satisfied exactly for \(\) when \(\) and \(f\) is a Gaussian process (cf. Lemma C.9), and we provide an extensive discussion of our results in Appendix C.4 for instances where Assumption 3.1 is satisfied approximately, relying on the notion of weak submodularity (Das and Kempe, 2018).

### Gaussian Process Setting

When \(f(,k)\) is a Gaussian process (GP, Williams and Rasmussen, 2006) with known mean function \(\) and kernel \(k\), and the noise \(_{}\) is mutually independent and zero-mean Gaussian with known variance, the \(\) and \(\) objectives have a closed form expression (cf. Appendix F) and can be optimized efficiently. Further, the information capacity \(_{n}\) is sublinear in \(n\) for a rich class of GPs (Srinivas et al., 2009; Vakili et al., 2021), with rates summarized in Table 3 of the appendix.

**Convergence to irreducible uncertainty** So far, our discussion was centered around the role of the target space \(\) in facilitating _directed_ learning. An orthogonal contribution of this work is to study _extrapolation_ from the sample space \(\) to points \(\). To this end, we derive bounds on the marginal posterior variance \(_{n}^{2}()}}{{=}}[f _{}_{n}]\) for points in \(\). These bounds depend on the instance of transductive active learning (i.e., \(\) and \(\)) and might be of independent interest for active learning. For \(\) and \(\), they imply uniform convergence of the variance for a rich class of GPs. To the best of our knowledge, this work is the first to present such bounds.

We define the _irreducible uncertainty_ as the variance of \(f()\) provided complete knowledge of \(f\) in \(\):

\[_{}^{2}()}}{{=}} [f_{}}}].\]

As the name suggests, \(_{}^{2}()\) represents the smallest uncertainty one can hope to achieve from observing only within \(\). For all \(\), it is easy to see that \(_{}^{2}()=0\). However, the irreducible uncertainty of \(\) may be (and typically is!) strictly positive.

**Theorem 3.2** (Bound on marginal variance for \(\) and \(\)).: _Let Assumption 3.1 hold and the data be selected by either \(\) or \(\).. Assume that \(f(,k)\) with known mean function \(\)_and kernel \(k\), the noise \(_{}\) is mutually independent and zero-mean Gaussian with known variance, and \(_{n}\) is sublinear in \(n\). Then there exists a constant \(C\) such that for any \(n 1\) and \(\),_

\[_{n}^{2}()}^{2}()}_{}+,}(n)}{}}_{}.\] (1)

_Moreover, if \(\), there exists a constant \(C^{}\) such that_

\[_{n}^{2}() C^{},}( n)}{n}.\] (2)

Intuitively, Equation (1) of Theorem 3.2 can be understood as bounding an epistemic "generalization gap" (Wainwright, 2019) of the learner. The reducible uncertainty converges to zero at all prediction targets \(\), e.g., for linear, Gaussian, and smooth Matern kernels. As to be expected, a smaller target space (i.e., more targeted sampling) leads to faster convergence due to a smaller information capacity \(_{,}(n)_{n}\). Equation (2) matches prior results for the setting \(=\). We provide a formal proof of Theorem 3.2 in Appendix C.6.

### Agnostic Setting

The result from the GP setting translates also to the agnostic setting, where the "ground truth" \(f^{}\) may be any sufficiently regular fixed function on \(\).3 In this case, we use the model \(f\) from Section 3.1 as a (misspecified) model of \(f^{}\), with some kernel \(k\) and zero mean function \(()=0\). We denote by \(_{n}()=[f()_{n}]\) the posterior mean of \(f\). W.l.o.g. we assume in the following result that the prior variance is bounded, i.e., \([f()] 1\).

**Theorem 3.3** (Bound on approximation error for ITL and VTL, following Abbasi-Yadkori (2013); Chowdhury and Gopalan (2017)).: _Let Assumption 3.1 hold and the data be selected by either ITL or VTL. Pick any \((0,1)\). Assume that \(f^{}\) lies in the reproducing kernel Hilbert space \(_{k}()\) of the kernel \(k\) with norm \( f^{}_{k}<\), the noise \(_{n}\) is conditionally \(\)-sub-Gaussian, and \(_{n}\) is sublinear in \(n\). Let \(_{n}()= f^{}_{k}++1+(1/))}\). Then for any \(n 1\) and \(\), jointly with probability at least \(1-\),_

\[ f^{}()-_{n}()_{n}() }()}_{}+ ,}(n)}_{}\]

_where \(_{,}^{2}(n)\) denotes the reducible part of Equation (1)._

We provide a formal proof of Theorem 3.3 in Appendix C.7. Theorem 3.3 generalizes approximation error bounds of prior works to the extrapolation setting, where some prediction targets \(\) lie outside the sample space \(\). For prediction targets \(\), the irreducible uncertainty vanishes, and we recover previous results from the setting \(=\).

Theorems 3.2 and 3.3 show that ITL and VTL efficiently learn \(f\) at the prediction targets \(\) for large classes of "sufficiently regular" functions \(f\). In the following, we validate these results experimentally by showing that ITL and VTL exhibit strong empirical performance in a broad range of applications.

### Experiments in the Gaussian Process Setting

Before demonstrating ITL and VTL on GPs to develop more intuition, we introduce a natural correlation-based baseline, which will later uncover connections to existing approaches:

\[_{n}=*{arg\,max}_{}_{^{ }}[f_{},f_{^{}} _{n-1}].\] (CTL)

How does the smoothness of \(f\) affect ITL?We contrast two "extreme" kernels: the _Gaussian kernel_\(k(,^{})=(--^{} _{2}^{2}/2)\) and the _Laplace kernel_\(k(,^{})=(--^{} _{1})\). In the mean-squared sense, the Gaussian kernel yields a smooth process \(f\) whereas the Laplace kernel yields a continuous but non-differentiable \(f\)(Williams and Rasmussen, 2006). Figure 2 showshow ITL adapts to the smoothness of \(f\): Under the "smooth" Gaussian kernel, points outside \(\) provide higher-order information. In contrast, under the "rough" Laplace kernel and if \(\), points outside \(\) do not provide any additional information, and therefore are not sampled by ITL. If, however, \(\), information "leaks" \(\) even under a Laplace kernel prior. That is, even for non-smooth functions, the point with most information need not be in \(\).

Does ITL outperform uncertainty sampling?Uncertainty sampling (UnSa, Lewis and Catlett, 1994) is one of the most popular active learning methods. UnSa selects points \(\) with high _prior_ uncertainty: \(_{n}=*{arg\,max}_{}_{n-1}^{2}( )\). This is in stark contrast to ITL and VTL which select points \(\) that minimize _posterior_ (epistemic) uncertainty about \(\). It can be seen that UnSA is the special "undirected" case of ITL when \(\) and observation noise is homoscedastic (cf. Appendix C.1).

We compare UnSa to ITL, VTL, and CTL in Figure 3. We observe that ITL and VTL outperform UnSa which also samples points that are not informative about \(\). Further, ITL and VTL outperform "local" UnSa (i.e., UnSa constrained to \(\)) which neglects all information provided by points outside \(\).4 As one would expect, VTL has an advantage with respect to reducing the total variance of \(_{}\), whereas ITL reduces the entropy of \(_{}\) faster. We include ablations in Appendix H where we, in particular, observe that the advantage of ITL and VTL over UnSA increases as the volume of prediction targets shrinks in comparison to the size of domain.

## 4 Active Fine-Tuning of Neural Networks

Fine-tuning a large pre-trained model is a cost- and computation-effective approach to improve performance on a given target domain (Lee et al., 2022). While previous work has studied the effectiveness of various training procedures for fine-tuning (e.g., Eustratiadis et al., 2024), we ask: _How can we select the right data for fine-tuning to a specific task?_ This _active_ fine-tuning problem is an instance of the introduced "directed" transductive learning problem: Concretely, consider a supervised setting, where the function \(f\) maps inputs \(\) to outputs \(y\). We have access

Figure 3: Entropy of \(_{}\) ranging from \(-3850\) to \(-3725\) and the mean marginal standard deviations of \(_{}\) ranging from \(0\) to \(0.15\). Experiment is using the Gaussian kernel of the left instance (\(\)) from Figure 2. It can be seen that ITL and VTL outperform UnSa and Random. Uncertainty bands correspond to one standard error over \(10\) random seeds.

Figure 2: Initial \(25\) samples of ITL under a Gaussian kernel with lengthscale \(1\) (left) and a Laplace kernel with lengthscale \(10\) (right). Shown in gray is the sample space \(\) and shown in blue is the target space \(\). In three of the four examples, points outside the target space provide useful information.

to noisy samples from a training set \(\) on \(\), and we would like to learn \(f\) such that our estimate minimizes a given risk measure, such as classification error, with respect to a test distribution \(_{}\) on \(\). The goal is to actively and efficiently sample from \(\) to minimize risk with respect to \(_{}\).5 We show in this section that ITL and VTL can learn \(f\) from only _few examples_ from \(\).

How can we leverage the latent structure learned by the pre-trained model?As common in related work, we approximate the (pre-trained) neural network (NN) \(f(;)\) as a linear function in a latent embedding space, \(f(;)^{}_{}()\), with weights \(^{p}\) and embeddings \(_{}:^{p}\). Common choices of embeddings include last-layer embeddings (Devlin et al., 2019; Holzmuller et al., 2023), neural tangent embeddings arising from neural tangent kernels (Jaco et al., 2018) which are motivated by their relationship to the training and fine-tuning of ultra-wide NNs (Arora et al., 2019; Lee et al., 2019; Khan et al., 2019; He et al., 2020; Malladi et al., 2023), and loss gradient embeddings (Ash et al., 2020). We provide a comprehensive overview of embeddings in Appendix J.2. Now, supposing the prior \((,)\), often with \(=\)(Khan et al., 2019; He et al., 2020; Antoran et al., 2022; Wei et al., 2022), this approximation of \(f\) is a Gaussian process with kernel \(k(,})=_{}()^{} _{}(})\) which quantifies the similarity between points in terms of their alignment in the learned latent space. Note that the correlation \(k(,})/,)k(},})}\) between two points \(,}\) is equal to the cosine similarity of their embeddings.

In this context, Theorem 3.2 bounds the epistemic posterior uncertainty about a prediction using the approximation \(^{}_{}()\), given that the model is trained using data selected by ITL or VTL. Theorem 3.3 bounds the generalization error when using the posterior mean of \(\) for prediction. This extends recent work which has studied estimators of this generalization error (Wei et al., 2022).

Batch selection: Diversity via conditional embeddingsEfficient labeling and training necessitates a batch-wise selection of inputs. The selection of a batch of size \(b>1\) can be seen as an individual _non-adaptive_ active learning problem, and significant recent work has shown that batch diversity is crucial in this setting (Ash et al., 2020; Zanette et al., 2021; Holzmuller et al., 2023; Pacchiano et al., 2024). An information-based batch-wise selection strategy is formalized by the following non-adaptive transductive active learning problem (Chen and Krause, 2013) and the greedy approximation of \(B_{n}\) by ITL which selects elements \(_{n,i}\) of the \(n\)-th batch iteratively based on \(_{n,1:i-1}\):

\[B_{n}=*{arg\,max}_{B,|B|=b}( _{};_{B}_{n-1});_{n,i}=* {arg\,max}_{}(_{};_{}_{n-1},_{_{n,1:i-1}}).\] (3)

The batch \(B_{n}\) is diverse and informative by design. We show that under Assumption 3.1, \(B_{n}^{}=_{n,1:b}\) yields a constant-factor approximation of \(B_{n}\) (cf. Appendix C.3).

### Experiments on Active Fine-Tuning

Our empirical evaluation is motivated by the following practical example: We deploy a pre-trained image classifier to user's phones who use it within their local environment. We would like to locally fine-tune a user's model to their environment. Since the users' images \(\) are unlabeled, this requires selecting a small number of relevant and diverse images from the set of labeled images \(\). As such, we will focus here on the setting where the points in our test set do not lie in our training set (i.e., \(=\)), and discuss alternative instances such as active domain adaptation in Appendix I.

Testbeds & architecturesWe use the MNIST (LeCun et al., 1998) and CIFAR-100 (Krizhevsky et al., 2009) datasets as testbeds. In both cases, we take \(\) to be the training set, and we consider the task of learning the digits \(3\), \(6\), and \(9\) (MNIST) or the first \(10\) categories of CIFAR-100.6 For MNIST, we train a simple convolutional neural network with ReLU activations, three convolutional layers with max-pooling, and two fully-connected layers. For CIFAR-100, we fine-tune an EfficientNet-B0 (Tan and Le, 2019) pre-trained on ImageNet (Deng et al., 2009), augmented by a final fully-connected layer. We train the NNs using the cross-entropy loss and the ADAM optimizer (Kingma and Ba, 2014).

ResultsIn Figure 4, We compare against **(i)** active learning methods which largely aim for sample diversity but which are not directed towards the target distribution \(_{}\) (e.g., BADGE; Ash et al., 2020), and **(ii)** search methods that aim to retrieve the most relevant samples from \(\) with respect to the targets \(_{}\) (e.g., maximizing cosine similarity to target embeddings as is common in vector databases;Settles & Craven, 2008; Johnson et al., 2019). InformationDensity (ID, Settles & Craven, 2008) is a heuristic approach aiming to combine **(i)** diversity and **(ii)** relevance. In Appendix J.5, we also compare against a wide range of additional baselines (e.g., CoreSet(Sener & Savarese, 2017), TypiClust(Hacohen et al., 2022), ProbCover(Yehuda et al., 2022), etc.) that fall into one of the categories **(i)** and **(ii)**, and which perform similar to the baselines listed here.

We observe that ITL, VTL, and CTL consistently and significantly outperform random sampling from \(\) as well as all baselines. We see that relevance-based methods such as CosineSimilarity have an initial advantage over Random but for batch sizes larger than \(1\) they quickly fall behind due to diminishing informativeness of the selected data. In contrast, diversity-based methods such as BADGE are more competitive with Random but do not explicitly aim to retrieve relevant samples.

Remarkably, transductive active learning outperforms random data selection even in the MNIST experiment where the model is randomly initialized. This suggests that the learned embeddings can be informative for data selection even in the early stages of training, bootstrapping the learning progress.

Balancing sample relevance and diversityOur proposed methods unify approaches to coverage (promoting _diverse_ samples) and search (aiming for _relevant_ samples with respect to a given query \(\)) which leads to the significant improvement upon the state-of-the-art in Figure 4. Notably, for a batch size and query size of \(1\) and if correlations are non-negative, ITL, VTL, CTL, and the canonical cosine similarity are equivalent. CTL can be seen as a direct generalization of cosine similarity-based retrieval to batch and query sizes larger than one. In contrast to CTL, ITL and VTL may also sample points which exhibit a strong negative correlation (which is also informative).

We observe empirically that ITL obtains samples from \(_{}\) at more than twice the rate of CosineSimilarity, which translates to a significant improvement in accuracy in more difficult learning tasks, while requiring fewer (labeled) samples from \(\). This phenomenon manifests for both MNIST and CIFAR-100, as well as imbalanced datasets \(\) or imbalanced reference samples from \(_{}\) (cf. Appendix J.6). The improvement in accuracy appears to increase in the large-data regime, where the learning tasks become more difficult. Akin to a previously identified scaling trend with size of the pre-training dataset (Tamkin et al., 2022), this suggests a potential scaling trend where the improvement of ITL over random batch selection grows as models are fine-tuned on a larger pool of data.

Figure 4: Active fine-tuning on MNIST (left) and CIFAR-100 (right). Random selects each observation uniformly at random from \(\). The batch size is \(1\) for MNIST and \(10\) for CIFAR-100. Uncertainty bands correspond to one standard error over \(10\) random seeds. We see that transductive active learning with ITL and VTL significantly outperforms competing methods, and in particular, retrieves substantially more samples from the support of \(_{}\). See Appendix J for details and ablations.

Towards task-driven few-shot learningBeing able to efficiently and automatically select data may allow dynamic few-shot fine-tuning to individual tasks (Vinyals et al., 2016; Hardt and Sun, 2024), e.g., fine-tuning the model to each test point / query / prompt. Such task-driven few-shot learning can be seen as a form of "memory recall" akin to associative memory (Hopfield, 1982). Our results are a first indication that task-driven learning can lead to substantial performance gains, and we believe that this is a promising direction for future studies.

## 5 Safe Bayesian Optimization

Another practical problem that can be cast as "directed" learning is safe Bayesian optimization (Safe BO, Sui et al., 2015; Berkenkamp et al., 2021) which has applications in natural science (Cooper and Netoff, 2022) and robotics (Wischnewski et al., 2019; Sukhija et al., 2023; Widmer et al., 2023). Safe BO solves the following optimization problem

\[_{^{}}f^{}()^{}=\{ g^{}() 0\}\] (4)

which can be generalized to multiple constraints. The functions \(f^{}\) and \(g^{}\), and hence also the "safe set" \(^{}\), are unknown and have to be actively learned from data. However, it is crucial that the data collection does not violate the constraint, i.e., \(_{n}^{}, n 1\).

Safe Bayesian optimization as Transductive Active LearningIn the agnostic setting from Section 3.2, GPs \(f\) and \(g\) can be used as well-calibrated models of the ground truths \(f^{}\) and \(g^{}\), and we denote lower- and upper-confidence bounds by \(l_{n}^{f}(),l_{n}^{g}()\) and \(u_{n}^{f}(),u_{n}^{g}()\), respectively. These confidence bounds induce a _pessimistic_ safe set \(_{n}=\{ l_{n}^{g}() 0\}\) and an _optimistic_ safe set \(}_{n}=\{ u_{n}^{g}() 0\}\) which satisfy \(_{n}^{}}_{n}\) with high probability at all times. Similarly, the set of _potential maximizers_

\[_{n}}{=}\{}_{n}  u_{n}^{f}()_{^{}_{n}}l_{n}^{f}( ^{})\}\] (5)

contains the solution to Equation (4) at all times with high probability.

The (simple) regret \(r_{n}()}{=}_{}f^{ }()-f^{}(}_{n})\) with \(}_{n}}{=}*{arg\,max}_{ _{n}}l_{n}^{f}()\) measures the worst-case performance of a decision rule. To achieve small regret, one faces an _exploration-expansion_ dilemma wherein one needs to explore points that are known-to-be-safe, i.e., lie in the estimated safe set \(_{n}\), and might be optimal, while at the same time discovering new safe points by "expanding" \(_{n}\). Accordingly, a natural choice for the target space of Safe BO is \(_{n}\) since it captures both exploration and expansion _simultaneously_.7 To prevent constraint violation, the sample space is restricted to the pessimistic safe set \(_{n}\). In Safe BO, both the target space and sample space change with each round \(n\), and we generalize our theoretical results from Section 3 in Appendix C to this setting.

**Theorem 5.1** (Convergence to safe optimum).: _Pick any \(>0\), \((0,1)\). Assume that \(f^{}\), \(g^{}\) lie in the reproducing kernel Hilbert space \(_{k}()\) of the kernel \(k\), and that the noise \(_{n}\) is conditionally \(\)-sub-Gaussian. Then, we have with probability at least \(1-\),_

\[n 1_{n}^{}.\]

_Moreover, assume \(_{0}\) and denote with \(\) the largest reachable safe set starting from \(_{0}\). Then, the convergence of reducible uncertainty implies that there exists \(n^{}>0\) such that with probability at least \(1-\),_

\[n n^{} r_{n}().\]

We provide a formal proof in Appendix C.8. Central to the proof is the application of Theorem 3.3 to show that the safety of parameters _outside_ the safe set \(_{n}\) can be inferred efficiently. In Section 3, we outline settings where the reducible uncertainty converges which is the case for a very general class of functions, and for such instances Theorem 5.1 guarantees optimality in the largest reachable safe set \(\). \(\) represents the largest set any safe learning algorithm can explore without violating the safety constraints (with high probability) during learning (cf. Definition C.29). Our guarantees are similar to those of other Safe BO algorithms (Berkenkamp et al., 2021) but require fewer assumptions and generalize to continuous domains. We obtain Theorem 5.1 from a more general result (Theorem C.34) which can be specialized to yield "free" novel convergence guarantees for problems other than Bayesian optimization, such as level set estimation, by choosing an appropriate target space.

### Experiments on Safe Bayesian Optimization

We evaluate two synthetic experiments for a 1d and 2d parameter space, respectively (cf. Appendix K.4 for details), which demonstrate the various shortcomings of existing Safe BO baselines. Additionally, as third experiment, we safely tune the controller of a quadcopter.

Safe controller tuning for a quadcopterWe consider a quadcopter with unknown dynamics; \(_{t+1}=(_{t},_{t})\) where \(_{t}^{d_{u}}\) is the control signal and \(_{t}^{d_{s}}\) is the state at time \(t\). The inputs \(_{t}\) are calculated through a deterministic function of the state \(:\) which we call the policy. The policy is parameterized via parameters \(\), e.g., PID controller gains, such that \(_{t}=_{}(_{t})\). The goal is to find the optimal parameters with respect to an unknown objective \(f^{}\) while satisfying some unknown constraint(s) \(g^{}() 0\), e.g., the quadcopter does not fall on the ground. This is a typical Safe BO problem which is widely applied for safe controller learning in robotics (Berkenkamp et al., 2021; Baumann et al., 2021; Widmer et al., 2023).

ResultsWe compare ITL and VTL to SafeOpt(Berkenkamp et al., 2021), which is undirected, i.e., expands in all directions including ones that are known-to-be suboptimal, and ISE (Bottero et al., 2022), which is solely expansionist -- does not trade-off expansion-exploration. We provide a detailed discussion of baselines in Appendix K.2. In all our experiments, summarized in Figure 5, we observe that ITL and VTL systematically perform well, i.e., better or on par with the state-of-the-art. We attribute this to its directed exploration and less conservative expansion over Safeopt (cf. 1d task and quadcopter experiment), and natural trade-off between expansion and exploration as opposed to ISE (see 2d task). Generally, VTL has a slight advantage over ITL, which is because VTL minimizes marginal variances (as opposed to entropy), which are decisive for expanding the safe set. While ITL and VTL do not violate constraints, we observe that other methods that do not explicitly enforce safety such as EIC (Gardner et al., 2014) lead to constraint violation (cf. Appendix K.4.2).

## 6 Related Work

(Inductive) active learningThe special case of transductive active learning where \(==\) has been widely studied. We refer to this special instance as _inductive_ active learning, since the goal is to extract as much information as possible as opposed to making predictions on a specific target set.

Several works have previously found entropy-based decision rules to be useful for inductive active learning (Krause and Guestrin, 2007; Guo and Greiner, 2007; Krause et al., 2008) and semi-supervised learning (Grandvalet and Bengio, 2004). The variance-based VTL has previously been proposed by Cohn (1993) in the special case of inductive active learning without proving theoretical guarantees. VTL was then recently re-derived by Shoham and Avron (2023) along other experimental design

Figure 5: We compare ITL and VTL to Oracle SafeOpt, which has oracle knowledge of the Lipschitz constants, SafeOpt, where the Lipschitz constants are estimated from the GP, as well as Heuristic SafeOpt and ISE, and observe that ITL and VTL systematically perform well. We compare against additional baselines in Appendix K.1. The regret is evaluated with respect to the ground truth objective \(f^{}\) and constraint \(g^{}\), and averaged over 10 (in synthetic experiments) and 25 (in the quadcopter experiment) random seeds. Additional details can be found in Appendix K.4.

criteria under the lens of minimizing risk for inductive one-shot learning in overparameterized models. Substantial work on active learning has studied entropy-based criteria in _parameter-space_, most notably BALD (MacKay, 1992; Houlsby et al., 2011; Gal et al., 2017; Kirsch et al., 2019), which selects \(_{n}=_{}(;_{} _{n-1})\), where \(\) is the random parameter vector of a parametric model (e.g., obtained via Bayesian deep learning). Such methods are inherently inductive in the sense that they do not facilitate learning on specific prediction targets.

Transductive active learningIn contrast, ITL operates in _output-space_ where it is straightforward to specify prediction targets, and which is computationally easier. Special cases of ITL when \(=\) and \(||=1\) have been proposed in the foundational work of MacKay (1992) on "directed" output-space active learning. As generalization to larger target spaces, MacKay (1992) proposed mean-marginal ITL,

\[_{n}=*{arg\,max}_{}_{^{ }}(f_{^{}};y_{}_{n-1})\,,\] (MM-ITL)

for which we derive analogous versions of Theorems 3.2 and 3.3 in Appendix D.3. We note that similarly to VTL, MM-ITL disregards the mutual dependence of points in the target space \(\) and differs from VTL only in a different weighting of the posterior marginal variances of the prediction targets (cf. Appendix D.3). Recently, Bickford Smith et al. (2023) generalized MM-ITL by treating the prediction target as a random variable, and Kothawade et al. (2021) and Bickford Smith et al. (2024) demonstrated the use of output-space decision rules for image classification tasks in a pre-training context.

Influence functionsmeasure the change in a model's prediction when a single data point is removed from the training data (Cook, 1977; Koh and Liang, 2017; Pruthi et al., 2019). Influence functions have been used for data selection in settings closely related to the transductive active fine-tuning of neural networks proposed in this work (Xia et al., 2024). They select data that reduces a first-order Taylor approximation to the test loss after fine-tuning a neural network, which corresponds to maximizing cosine similarity to the prediction targets in a loss-gradient embedding space. We show in our experiments that transductive active learning can substantially outperform CosineSimilarity. We attribute this primarily to influence functions implicitly assuming that the influence of selected data adds linearly (i.e., two equally scored data points are expected to doubly improve the model performance, Xu and Kazantsev, 2019, Section 3.2). This assumption does not hold in practice as seen, e.g., by simply duplicating data. The same limitation applies to the related approach of datamodels (Ilyas et al., 2022).

Other work on directed active learningDirected active learning methods have been proposed for the problem of determining the optimum of an unknown function, also known as best-arm identification (Audibert et al., 2010) or pure exploration bandits (Bubeck et al., 2009). Entropy search methods (Hennig and Schuler, 2012; Hernandez-Lobato et al., 2014) are widely used and select \(_{n}=_{}(^{*};y_{} _{n-1})\) in _input-space_ where \(^{*}=_{}f_{}\). Similarly to ITL, _output-space_ entropy search methods (Hoffman and Ghahramani, 2015; Wang and Jegelka, 2017), which select \(_{n}=_{}(f^{*};y_{} _{n-1})\) with \(f^{*}=_{}f_{}\), are more computationally tractable. In fact, output-space entropy search is a special case of ITL with a stochastic target space (cf. Equation (47) in Appendix K.1). Bogunovic et al. (2016) analyze TruVar in the context of Bayesian optimization and level set estimation. TruVar is akin to VTL with a similar notion of "target space", but their algorithm and analysis rely on a threshold scheme which requires that \(\). Fiez et al. (2019) introduce the _transductive linear bandit_ problem, which is a special case of transductive active learning limited to a linear function class and with the objective of determining the maximum within an initial candidate set.8 We mention additional more loosely related works in Appendix A.

## 7 Conclusion

We investigated the generalization of active learning to settings with concrete prediction targets and/or with limited information due to constrained sample spaces. This provides a flexible framework, applicable also to other domains than were discussed (such as recommender systems, molecular design, robotics, etc.) by varying the choice of target space and sample space. Further, we proved novel generalization bounds which may be of independent interest for active learning. Finally, we demonstrated across broad applications that sampling _relevant and diverse_ points (as opposed to only one of the two) leads to a substantial improvement upon the state-of-the-art.