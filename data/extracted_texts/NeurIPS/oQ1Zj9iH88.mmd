# Penalty-based Methods for Simple Bilevel Optimization under Holderian Error Bounds

Pengyu Chen

School of Data Science

Fudan University

pychen22@m.fudan.edu.cn

&Xu Shi

School of Data Science

Fudan University

xshi22@m.fudan.edu.cn

&Rujun Jiang

School of Data Science

Fudan University

rjjiang@fudan.edu.cn

Equal contribution

Corresponding author

Jiulin Wang

School of Data Science

Fudan University

wangjiulin@fudan.edu.cn

###### Abstract

This paper investigates simple bilevel optimization problems where we minimize an upper-level objective over the optimal solution set of a convex lower-level objective. Existing methods for such problems either only guarantee asymptotic convergence, have slow sublinear rates, or require strong assumptions. To address these challenges, we propose a penalization framework that delineates the relationship between approximate solutions of the original problem and its reformulated counterparts. This framework accommodates varying assumptions regarding smoothness and convexity, enabling the application of specific methods with different complexity results. Specifically, when both upper- and lower-level objectives are composite convex functions, under an \(\)-Holderian error bound condition and certain mild assumptions, our algorithm attains an \((,^{})\)-optimal solution of the original problem for any \(>0\) within \((}})\) iterations. The result can be improved further if the smooth part of the upper-level objective is strongly convex. We also establish complexity results when the upper- and lower-level objectives are general nonsmooth functions. Numerical experiments demonstrate the effectiveness of our algorithms.

## 1 Introduction

Bilevel optimization involves embedding one optimization problem within another, creating a hierarchical structure where the upper-level problem's feasible set is influenced by the lower-level problem. This framework frequently occurs in various real-world scenarios, such as meta-learning (Bertinetto et al., 2018; Rajeswaran et al., 2019), hyper-parameter optimization (Chen et al., 2024; Franceschi et al., 2018; Shaban et al., 2019), reinforcement learning (Mingyi et al., 2020) and adversarial learning (Bishop et al., 2020; Wang et al., 2021, 2022). In this paper, we concentrate on a subset of bilevel optimization known as simple bilevel optimization (SBO), which has garnered significant interest in the machine learning community due to its relevance in dictionary learning (Beck and Sabach, 2014; Jiang et al., 2023), lexicographic optimization (Kissel et al., 2020; Gong et al., 2021), lifelong learning (Malitsky, 2017; Jiang et al., 2023); see more details in Appendix A.

SBO aims to find an optimal solution that minimizes the upper-level objective over the solution set of the lower-level problem. In other words, we are interested in solving the following problem:

\[_{^{n}}F()\ \  *{arg\,min}_{^{n}}G().\] (P)

Here \(F,G:^{n}\{\}\) are proper, convex, and lower semi-continuous functions. We also assume that the optimal solution set of the lower-level problem, denoted as \(X_{}\), is nonempty. Moreover, since \(G\) is convex and lower semi-continuous, it holds that \(X_{}\) is closed and convex (Bertsekas et al., 2003, Proposition 1.2.2 and Page 49).

In this paper, we first reformulate problem (P) into the constrained form:

\[_{^{n}}F()\ \ G()-G^{*} 0,\] (P \[{}_{}\] )

where \(G^{*}\) represents the optimal value of the unconstrained lower-level problem.

Based on this reformulation, we consider the following penalization of (P\({}_{}\)),

\[_{^{n}}_{}()=F()+  p(),\] (P \[{}_{}\] )

where \(p():=G()-G^{*}\) is the so-called residual function and \(>0\) is the penalized parameter. Obviously, we have \(p() 0\), and \(p()=0\) if and only if \( X_{}\).

Denote \(F^{*}\) and \(G^{*}\) as the optimal values of problem (P) and the lower-level problem \(_{^{n}}G()\), respectively. We aim to find an \((_{F},_{G})\)-optimal solution \(}^{*}\) of problem (P), which satisfies

\[F(}^{*})-F^{*}_{F}, G(}^{*} )-G^{*}_{G}.\] (1)

Moreover, a point \(}^{*}_{}\) is said to be an \(\)-optimal solution of problem (P\({}_{}\)) if

\[_{}(}^{*}_{})-^{*}_{},\]

where \(^{*}_{}\) is the optimal value of problem (P\({}_{}\)).

### Related work

Various approaches have been developed to solve problem (P) (Cabot, 2005; Solodov, 2007; Sabach and Shtern, 2017; Dutta and Pandit, 2020; Gong et al., 2021). Among those, one category that is the most related to penalization formulation (P\({}_{}\)) is the regularization method, which integrates the upper- and lower-level objectives through Tikhonov regularization (Tikhonov and Arsenin, 1977)

\[_{^{n}}():= F()+G( ),\] (P \[{}_{}\] )

where \(\) is the so-called regularization parameter. When \(F\) is strongly convex and its domain is compact, Amini and Yousefian (2019) extended the IR-PG method from Solodov (2007), which achieved a asymptotic convergence rate for the upper-level problem and a convergence rate of \((1/K^{0.5-b})\) for the lower-level problem, where \(b(0,0.5)\). Malitsky (2017) studied a version of Tseng's accelerated gradient method and showed a convergence rate of \((1/K)\) for the lower-level problem, while the convergence rate for the upper-level objective is not explicitly provided. Kaushik and Yousefian (2021) proposed an iteratively regularized gradient (a-IRG) method which obtains a complexity of \((1/K^{0.5-b})\) and \((1/K^{b})\) for the upper- and lower-level objective, respectively, where \(b(0,0.5)\). Inspired by this research, and under a quasi-Lipschitz assumption for \(F\), Merchav and Sabach (2023) introduced a bi-subgradient (Bi-SG) method. This method demonstrates convergence rates of \((1/K^{b})\) and \((1/K^{1-b})\) for the lower- and upper-level objectives, respectively, where \(b(0.5,1)\). In their framework, the convergence rate of the upper-level objective can be improved to be linear when \(F\) is strongly convex. Recently, under the weak-sharp minima assumption of the lower-level problem, Samadi et al. (2023) proposed a regularized accelerated proximal method (R-APM), showing a convergence rate of \((1/K^{2})\) for both upper- and lower-level objectives. When the domain is compact and \(F,G\) are both smooth, Giang-Tran et al. (2023) proposed an iteratively regularized conditional gradient (IR-CG) method, which ensures convergence rates of \((1/K^{p})\) and \((1/K^{1-p})\) for upper- and lower-level objectives, respectively, where \(p(0,1)\).

Despite the abundance of existing methodologies yielding non-asymptotic convergence outcomes, their efficacy is frequently contingent upon additional assumptions. Denote \(L_{f_{1}}\) and \(L_{g_{1}}\) as the Lipschitz constants for the gradients of the smooth components in the upper- and lower-level objectives,respectively. Specifically, when \(F\) is strongly convex and \(G\) is smooth, Beck and Sabach (2014) presented the Minimal Norm Gradient (MNG) method and provided the asymptotic convergence to the optimal solution set and a convergence rate of \((L_{g_{1}}^{2}/^{2})\) for the lower-level problem. When \(F\) is assumed to be smooth, Jiang et al. (2023) introduced a conditional gradient-based bilevel optimization (CG-BiO) method, which invokes at most \((\{L_{f_{1}}/_{F},L_{g_{1}}/_{G}\})\) of linear optimization oracles to achieve an \((_{F},_{G})\)-optimal solution. Shen et al. (2023) combined an online framework with the mirror descent algorithm and established a convergence rate of \((1/^{3})\) for both upper- and lower-level objectives, assuming a compact domain and boundedness of the functions and gradients at both levels. Furthermore, they showed that the convergence rate can be improved to \((1/^{2})\) under additional structural assumptions. For a concise overview of overall methodologies, including their assumptions and convergence outcomes, refer to Table 1 in Appendix B.

For general bilevel optimization problems, there have been recent results on convergent guarantees (Shen and Chen, 2023; Sow et al., 2022; Chen et al., 2023; Huang, 2023). Among those, the one that is the most related to ours is (Shen and Chen, 2023). It investigates the case when the upper-level objective is nonconvex and gives convergence results under additional assumptions (Shen and Chen, 2023, Theorem 3 and 4). However, as the general bilevel optimization problem is nonconvex, the algorithms in the literature often converge to weak stationary points, while our method for SBO converges to global optimal solution.

### Our approach

Our approach is straightforward. Firstly, we introduce a penalization framework delineating the connection between approximate solutions of problems (P) and (P\({}_{}\)). This framework enables the attainment of an \((_{F},_{G})\)-optimal solution by solving problem (P\({}_{}\)) approximately. Subsequently, our focus shifts solely to resolving the unconstrained problem (P\({}_{}\)). Depending on varying assumptions regarding smoothness and convexity, we can employ different methods such as the accelerated proximal gradient (APG) methods (Beck and Teboulle, 2009; Nesterov, 2013; Lin and Xiao, 2014) to solve problem (P\({}_{}\)). We summarize our main contributions as follows.

* We propose a framework that explicitly examines the relationship between an \(\)-optimal solution of penalty formulation (P\({}_{}\)) and an \((_{F},_{G})\)-optimal solution of problem (P). We also provide a lower bound for the metric \(F()-F^{*}\).
* When \(F\) and \(G\) are both composite convex functions, we provide a penalty-based APG algorithm that attains an \((,e^{})\)-optimal solution of problem (P) within \((}})\) iterations. If the upper-level objective is strongly convex, the complexity can be improved to \((}})\). We also apply our method for the scenario where both the upper- and lower-level objectives are generalized nonsmooth convex functions.
* We present adaptive versions of PB-APG and PB-APG-sc with warm-start, which dynamically adjust the penalty parameters, and solve the associated penalized problem with adaptive accuracy. The adaptive ones have similar complexity results as their primal counterparts but can achieve superior performance in some experiments.

Utilizing the penalization method to address the original SBO problem is a novel approach. While Tikhonov regularization may seem similar to our framework, its principles differ. Implementing Tikhonov regularization necessitates the "slow condition" (\(_{k}_{k}=0,_{k=0}^{}_{k}=+\)), which requires iterative solutions for each iteration. In contrast, our method simply involves solving a single optimization problem (P\({}_{}\)) for a given \(\). Furthermore, we establish a relationship between the approximate solutions of the original bilevel problem and those of the reformulated single-level problem (P\({}_{}\)) for a specific \(\). This is the first theoretical result connecting the original bilevel problem to the penalization problem, accompanied by an optimal non-asymptotic complexity result.

## 2 The penalization framework

We begin by outlining specific assumptions for \(F\) and \(G\), as detailed below.

**Assumption 2.1**.: The set \(S:=_{ X_{}} F()\) is bounded with a diameter \(l_{F}:=_{ S}\|\|\).

Note that the type of subdifferential \( F\) used here is the most general form for a convex function, as detailed in (Bertsekas et al., 2003, Section 4.2). When the upper-level objective \(F\) is non-convex,we replace the assumption with the condition that the upper-level objective is Lipschitz continuous (cf. Theorems 2.7 and 2.8).

**Assumption 2.2** (Holderian error bound).: The function \(p():=G()-G^{*}\) satisfies the Holderian error bound with exponent \( 1\) and \(>0\). Namely,

\[(,X_{})^{} p( ),(G),\]

where \((,X_{}):=_{ X_{ }}\|-\|\).

We remark that Holderian error bounds are satisfied by many practical problems and widely used in optimization literature (Pang, 1997; Bolte et al., 2017; Zhou and So, 2017; Roulet and d'Aspremont, 2020; Jiang and Li, 2022). There are two notable special cases: (i) when \(=1\), we often refer to \(X_{}\) as a set of weak sharp minima of \(G\)(Burke and Ferris, 1993; Studniarski and Ward, 1999; Burke and Deng, 2005; Samadi et al., 2023); (ii) when \(=2\), Assumption 2.2 is known as the quadratic growth condition (Drusvyatskiy and Lewis, 2018a). Additional examples of functions exhibiting Holderian error bound, along with their corresponding parameters, are presented in Appendix C.

We are now ready to establish the connection between approximate solutions of problems (P) and (P\({}_{}\)). The subsequent two lemmas build upon the work of Shen and Chen (2023) for (general) bilevel optimization. Compared with their work, we generalize the exponent \(\) from \(2\) to \( 1\), providing a more general result. Furthermore, we also derive a lower bound for the penalized parameter for all \( 1\) and present a theoretical framework for these scenarios.

**Lemma 2.3**.: _Suppose that Assumptions 2.1 and 2.2 hold with \(>1\). Then, for any \(>0\), an optimal solution of problem (P) is an \(\)-optimal solution of problem (P\({}_{}\)) when \( l_{F}^{}(-1)^{-}^{1-}\)._

Lemma 2.3 establishes the relationship between an optimal solution of problem (P) and an \(\)-optimal solution of problem (P\({}_{}\)) when \(>1\). It also provides a lower bound for \(\), which plays a pivotal role in the complexity results. The proofs of this paper are deferred to Appendix E.

The lemma presented below yields a more favorable outcome when \(=1\), which is referred to as exact penalization. Notably, this specific result is not discussed in Shen and Chen (2023).

**Lemma 2.4**.: _Suppose that Assumptions 2.1 and 2.2 hold with \(=1\). Then an optimal solution of problem (P) is also an optimal solution of problem (P\({}_{}\)) if \( l_{F}\), and vice versa if \(> l_{F}\). In this case, we say that there is an exact penalization between problems (P) and (P\({}_{}\))._

For simplicity, we define

\[^{*}=\{& l_{F}^{}(-1)^{- }^{1-}&>1\\ & l_{F}&=1..\] (2)

Based on Lemmas 2.3 and 2.4, we give an overall relationship of approximate solutions between problems (P\({}_{}\)) and (P).

**Theorem 2.5**.: _Suppose that Assumptions 2.1 and 2.2 hold. For any given \(>0\) and \(>0\), let_

\[=^{*}+\{&2l_{F}^{}^{1-} &>1,\\ &l_{F}^{}^{1-}&=1,.\]

_with \(^{*}\) defined in (2). If \(}_{}^{*}\) is an \(\)-optimal solution of problem (P\({}_{}\)), then \(}_{}^{*}\) is an \((,l_{F}^{-}^{})\)-optimal solution of problem (P)._

Particularly, we are also able to establish a lower bound for \(F(}_{}^{*})-F^{*}\) under the same conditions outlined in Theorem 2.5.

**Theorem 2.6**.: _Suppose that the conditions in Theorem 2.5 hold. Then, \(}_{}^{*}\) satisfies the following suboptimality lower bound,_

\[F(}_{}^{*})-F^{*}-l_{F}( l_{F}^{-} ^{})^{}.\]

By setting \(=\), we obtain \(F(}_{}^{*})-F^{*}-^{}\). which along with Theorem 2.5 gives

\[|F(}_{}^{*})-F^{*}|\{,^{}\}.\]

We emphasize that the lower bound established in Theorem 2.6 is an intrinsic property of problem (P) under Assumptions 2.1 and 2.2. This property is independent of the algorithms we present.

### The upper-level function is non-convex

Note that the upper-level objective \(F\) is required to be convex in the above context (cf. Theorem 2.5). This raises a question: while Theorem 2.5 establishes the relationship between approximate solutions of problems (P) and (P\({}_{}\)), the distinction between the global or local optimal solutions of problem (P) and (P\({}_{}\)) remains unclear when \(F\) is non-convex.

We first establish the relationship between global optimal solutions of problems (P) and (P\({}_{}\)) when \(F\) is non-convex, which is similar to Theorem 2.5.

**Theorem 2.7**.: _Suppose that Assumption 2.2 holds, \(G\) is convex, and \(F\) is \(l\)-Lipschitz continuous on \((F)\). For any given \(>0\) and \(>0\), let_

\[=^{*}+\{&2l^{}^{1-}& >1,\\ &l^{}^{1-}&=1,.\] (3)

_where \(^{*}\) is given by (2). If \(}_{}^{*}\) is an \(\)-global optimal solution of problem (P\({}_{}\)), then \(}_{}^{*}\) is an \((,l^{-}^{})\)-global optimal solution of problem (P)._

Theorem 2.7 provides the relationship between the global optimal solutions of problems (P\({}_{}\)) and (P). However, the relationship between local optimal solutions of these problems is more intricate than those of the global ones (Shen and Chen, 2023). Given \(r>0\) and \(^{n}\), define \((,r):=\{^{n}:\|-\| r\}\). We present the following theorem, which demonstrates that local optimal solutions of problem (P\({}_{}\)) can serve as approximate local optimal solutions of problem (P).

**Theorem 2.8**.: _Suppose that Assumption 2.2 holds and \(G\) is convex. Let \(_{}^{*}\) be a local optimal solution of problem (P\({}_{}\)) on \((_{}^{*},r)\). Assume \(F\) is \(l\)-Lipschitz continuous on \((_{}^{*},r)\). Then \(_{}^{*}\) is an approximate local optimal solution of problem (P) that satisfies \(F(_{}^{*})-F_{}^{*} 0\) and \(G(_{}^{*})-G^{*}\) when \(>1\) and \((}{^{-1}})^{}\), where \(F_{}^{*}\) is the optimal value of problem (P) on \((_{}^{*},r) X_{}\). Furthermore, \(_{}^{*}\) is a local optimal solution of problem (P) when \(=1\) and \(> l\)._

Indeed, the relationship between approximate local optimal solutions of problems (P\({}_{}\)) and (P) is more intricate than the connection among global solutions presented in Theorem 2.5. These interactions will be the focus of our future work. The proofs of Theorems 2.7 and 2.8 are presented in Appendixes E.5 and E.6.

## 3 Main algorithms

In this section, we concentrate on addressing problem (P), making various assumptions, and offering distinct convergence outcomes.

### Both objectives are convex composite functions

In this scenario, we address problem (P) where \(F\) and \(G\) are both composite functions, i.e., \(F=f_{1}+f_{2}\) and \(G=g_{1}+g_{2}\).

**Assumption 3.1**.: \(F\) and \(G\) satisfy the following assumptions.

(1) The gradient of \(f_{1}()\), denoted as \( f_{1}\), is \(L_{f_{1}}\)-Lipschitz continuous on \((F)\);

(2) The gradient of \(g_{1}()\), denoted as \( g_{1}\), is \(L_{g_{1}}\)-Lipschitz continuous on \((G)\);

(3) \(f_{2}\) and \(g_{2}\) are proper, convex, lower semicontinuous, and possibly non-smooth.

We remark that Assumption 3.1(1)(3) is more general than many existing papers in the literature. Specifically, while previous works such as Beck and Sabach (2014), Amini and Yousefian (2019), Jiang et al. (2023), Giang-Tran et al. (2023) require the upper-level objective to be smooth or strongly convex, we simply assume that \(F\) is a composite function composed of a smooth convex function and a possibly non-smooth convex function. For the lower-level objective, previous works such as Beck and Sabach (2014), Amini and Yousefian (2019), Jiang et al. (2023), Giang-Tran et al. (2023) impose smoothness assumptions and, in some cases, convexity and compactness constraints on the domain; while our approach does not require these additional constraints, allowing for more flexibility and generality as presented in Assumption 3.1(2)(3).

We are now prepared to introduce two algorithms: the penalty-based accelerated proximal gradient (PB-APG) algorithm and its adaptive counterpart, the aPB-APG to solve problem (P\({}_{}\)) and, subsequently, to obtain an \((_{F},_{G})\)-optimal solution of problem (P).

To simplify notations, we omit the constant term \(- G^{*}\), and rewrite problem (P\({}_{}\)) as follows,

\[_{^{n}}_{}():=_{}( )+_{}(),\] (P \[{}_{}\] )

where \(_{}()=f_{1}()+ g_{1}()\) and \(_{}()=f_{2}()+ g_{2}()\) represent the smooth and nonsmooth parts, respectively. Then, it follows that the gradient of \(_{}()\) is \(L_{}\)-Lipschitz continuous with \(L_{}=L_{f_{1}}+ L_{g_{1}}\).

To implement the APG methods, we need another assumption concerning \(_{}()\).

**Assumption 3.2**.: For any \(>0\), the function \(_{}()\) is prox-friendly, i.e., the proximal mapping

\[_{t_{}}():=^ {n}}{}\{_{}()+\|- \|^{2}\},\]

is easy to compute for any \(t>0\).

The function \(_{}()\) represents the sum of two non-smooth functions, and proximal mapping for such function sums is widely studied and used in the literature (Yu, 2013; Pustelnik and Condat, 2017; Adly et al., 2019; Boob et al., 2023; Latafat et al., 2023). This assumption is also a more general requirement compared to many existing algorithms (Sabach and Shtern, 2017; Giang-Tran et al., 2023). It is important to note that our assumption is more general than existing literature. In the simple bilevel literature, when employing proximal mappings, researchers often consider the scenario where only one level contains a nonsmooth term (see, e.g., (Jiang et al., 2023; Doron and Shtern, 2023; Samadi et al., 2023; Merchav and Sabach, 2023)). In this case, the proximal mapping of the sum \(f_{2}+ g_{2}\) is then reduced to the proximal mapping of either \(f_{2}\) or \(g_{2}\), which is a more easily satisfied condition.

#### 3.1.1 Accelerated proximal gradient-based algorithm

We apply the APG algorithm (Beck and Teboulle, 2009; Lin and Xiao, 2014; Nesterov, 2013) to solve problem (P\({}_{}\)), as outlined in Algorithm 1. Moreover, if the Lipschitz constant \(L_{}\) is unknown or computationally infeasible, line search (Beck and Teboulle, 2009) can be adopted and will yield almost the same complexity bound. For brevity, we denote Algorithm 1 as \(}=(_{},_{},L_{f_{1}},L_{g_{1}}, _{0},)\), where \(}\) represents an \(\)-optimal solution of (P\({}_{}\)).

```
1:Input:\(,L_{}=L_{f_{1}}+ L_{g_{1}},_{-1}=_{0} ^{n},R>0\), \(t_{-1}=t_{0}=1,k=0,>0\) and \(\{t_{k}\}\).
2:for\(k 0\)do
3:\(_{k}=_{k}+t_{k}(t_{k-1}^{-1}-1)(_{k} -_{k-1})\)
4:\(_{k+1}=_{L_{}^{-1}_{}}(_{k}-L_ {}^{-1}_{}(_{k}))\)
5:endfor ```

**Algorithm 1** Penalty-based APG (PB-APG)

In Algorithm 1, we stop the loop of Line. 3 - 4 if the number of iterations satisfies that:

\[+ L_{g})R^{2}}{(k+1)^{2}},\]

where \(R\) is a constant that satisfies \(\|_{0}-^{*}\| R\).

Combining Theorem 2.5 and (Tseng, 2008, Corollary 2), we establish the following complexity result for problem (P).

**Theorem 3.3**.: _Suppose that Assumptions 2.1, 2.2, 3.1 and 3.2 hold and the sequence \(\{t_{k}\}\) in Algorithm 1 satisfies \(}{t_{k+1}^{2}}^{2}}\). Let \(\) be given as in Theorem 2.5. Algorithm 1 generates an \((,l_{F}^{-}^{})\)-optimal solution of problem (P) after at most \(K\) iterations, where_

\[K=(}}{}}+^{( ,)}L_{g_{1}}}{^{(,)}}}).\]Note that Theorem 3.3 encompasses all possible relationships between the magnitudes of \(_{F}\) and \(_{G}\) in (1), as \( 1\) and \(>0\) are arbitrary. Specially, if \(=1\) and \(\), the number of iterations is \(K=(}+l_{F}L_{g_{1}})/})\). This result matches the lower bound complexity for unconstrained smooth or convex composite optimization (Nemirovsky and Yudin, 1983; Woodworth and Srebro, 2016). Additionally, if \(g_{1} 0\), the number of iterations for obtaining an \((,^{})\)-optimal solution of problems (P) is independent of \(\), which can be improved to \(K=(}/})\).

**Remark 3.4**.: It is noteworthy that Theorem 1 in a previous paper Samadi et al. (2023) provides the first method that needs \((}+l_{F}L_{g_{1}})/})\) iterations to achieve an \((,)\) solution if \(=1\) and \(F\) is smooth. Nevertheless, our methodology diverges in various respects. First, our approach is rooted in the penalization formulation of problem (P\({}_{}\)), while the approach proposed by Samadi et al. (2023) is based on the Tikhonov regularization (Tikhonov and Arsenin, 1977). Second, we provide a theoretical framework that clearly delineates the relationship between approximate solutions of problems (P) and (P\({}_{}\)) for all cases of \( 1\) and \(F\) is non-convex, as indicated in Lemmas 2.3, 2.4 and Theorems 2.5, 2.7, 2.8. Therefore, we can first shift our focus from (P) to (P\({}_{}\)) based on the penalization framework and then use various methods to solve (P\({}_{}\)), not limited to using the APG methods. Besides, the association between approximate solutions of problem (P) and (P\({}_{}\)) differs significantly based on whether \(>1\) or \(=1\). For the case of \(>1\), the lower bound comprehensively integrates the accuracy parameter \(\), which results in a more sophisticated analysis of the convergence result, while Samadi et al. (2023) did not consider the situation when \(>1\). Third, our method applies to the case that \(F\) is composite, while Samadi et al. (2023) requires \(F\) to be smooth. Finally, we also propose an adaptive version of our algorithm (see Algorithm 2) that does not require an estimate of \(\).

#### 3.1.2 Adaptive version with warm-start mechanism

In practice, the penalty parameter \(\) might be difficult to determine. This motivates us to propose Algorithm 2, which adaptively updates \(\) and invokes PB-APG with dynamic \(\) and solution accuracies.

```
1:Input:\(_{0}^{n}\), \(_{0}=_{1}>0\), \(L_{f_{1}},L_{g_{1}}\), \(>1,>1\), \(_{0}>0\).
2:for\(k 0\)do
3:\(_{k}()=f_{1}()+_{k}g_{1}()\)
4:\(_{k}()=f_{2}()+_{k}g_{2}()\)
5: Invoke \(_{k}=(_{k},_{k},L_{f_{1}},L_{g_{1}},_ {k-1},_{k})\)
6:\(_{k+1}=_{k}/\)
7:\(_{k+1}=_{k}\)
8:endfor ```

**Algorithm 2** Adaptive PB-APG method (aPB-APG)

In Algorithm 2, we adaptively update the penalty parameter \(_{k}\), and invoke the PB-APG to generate an approximate solution for (P\({}_{}\)) with accuracy \(=_{k}\). Meanwhile, a warm-start mechanism is employed, meaning that the initial point for each subproblem is the output of the preceding subproblem. The convergence result of Algorithm 2 is as follows.

**Theorem 3.5**.: _Suppose that Assumptions 2.1, 2.2, 3.1, and 3.2 hold. Also assume that for every outcome of inner loop in Algorithm 2, \(\|_{k}-_{k}^{*}\| R\). Let \(_{0}>0\) be given._

* _When_ \(>1\)_, set_ \(>^{-1}\)_, and define_ \(N:=_{^{1-}}( L_{F}^{}(-1)^{-1} ^{-}_{0}^{1-}/_{0})_{+}\) _and_ \(_{k}^{*}:= L_{F}^{}(-1)^{-1}^{-} _{0}^{1-}^{k(-1)}\)_._
* _When_ \(=1\)_, set_ \(>1\)_, and define_ \(N:=_{}( L_{F}/_{0})_{+}\) _and_ \(_{k}^{*}:= L_{F}\)_._

_Then, for any \(k N\), Algorithm 2 generates an \((}{^{k}},}{^{k}(_{0}^{k} -_{k}^{*})})\)-optimal solution of problem (P) after at most \(K\) iterations, where \(K\) satisfies_

\[K=(}^{k}}{_{0}}}+}_{0}()^{k}}{_{0}}}).\]

Theorem 3.5 shows that for any given initial accuracy \(_{0}>0\), Algorithm 2 can produce an approximate solution of problem (P) with the desired accuracy.

**Remark 3.6**.: From Theorem 3.5, one can obtain an \((,^{k}-_{k}^{*}})\)-optimal solution of problem (P) within \((}}{}}+}}{ ^{}}})\) iterations when \(/_{0}/^{k}\), which is similar to the complexity results in Theorem 3.3.

#### 3.1.3 The upper-level objective is strongly convex

We investigate the convergence outcomes when the smooth part of the upper-level objective exhibits strong convexity.

**Assumption 3.7**.: \(f_{1}()\) is \(\)-strongly convex on \((F)\) with \(>0\).

Assumption 3.7 is another widely adopted setting in the existing SBO literature (Beck and Sabach, 2014; Sabach and Shtern, 2017; Amini and Yousefian, 2019; Merchav and Sabach, 2023). Here, we propose a variant of PB-APG that can provide better complexity results than existing methods. Our main integration is an APG-based algorithm, which has been studied in the existing literature (Nesterov, 2013; Lin and Xiao, 2014; Xu, 2022). In this paper, we adopt the algorithm proposed in Lin and Xiao (2014) and modify it with a constant step-size for simplicity as in Algorithm 3. Similar to Algorithm 1, we denote Algorithm 3 by \(}=(_{},_{},,L_{f_{1}},L_ {g_{1}},_{0},)\).

```
1:Input:\(\), \(\), \(L_{}=L_{f_{1}}+ L_{g_{1}}\), \(_{-1},_{0}^{n}\).
2:\(}=_{0}-L_{}^{-1}_{}( _{-1})\)
3:\(}=_{L_{1}^{-1}_{}}(}- L_{}^{-1}_{}(}))\)
4:Initialization: Let \(_{-1}=_{0}=}\), \(k=0\)
5:for\(k 0\)do
6:\(_{k}=_{k}+-}}{+}}(_{k}-_{k-1})\)
7:\(_{k+1}=_{L_{1}^{-1}_{}}(_{k}-L_{ }^{-1}_{}(_{k}))\)
8:endfor ```

**Algorithm 3** PB-APG method for Strong Convexity Case (PB-APG-sc)

The convergence analysis of Algorithm 3 is in the existing literature (Nesterov, 2013; Lin and Xiao, 2014). Combining (Lin and Xiao, 2014, Theorem 1) and Theorem 2.5, we have the following complexity result.

**Theorem 3.8**.: _Suppose that Assumptions 2.1, 2.2, 3.1, 3.2, and 3.7 hold. Algorithm 3 can produce an \((,l_{F}^{-}^{})\)-optimal solution of problem (P) after at most \(K\) iterations, where \(K\) satisfies_

\[K=(}}{}}+^{\{,\}}L_{g_{1}}}{^{\{-1,-1 \}}}}).\]

Theorem 3.8 improves the complexity results of Theorem 3.3 significantly. Specifically, when \(0<=1\), the convergence rate can be improved to be linear, i.e., \(K=(}/})\).

Additionally, we present an adaptive variant of PB-APG-sc, termed aPB-APG-sc, which adaptively executes \(_{k}=(_{k},_{k},,L_{f_{1}},L_{g_{1}}, _{k-1},_{k})\) and enjoys the similar complexity results of Algorithm 3, as delineated in Algorithm 4 within Appendix D.1.

### Both objectives are non-smooth

In this section, we focus on the scenario where both the upper- and lower-level objectives are non-smooth, namely, \(f_{1}=g_{1} 0\). Additionally, we assume that there is a point \(x C\) in the lower level problem, where \(C\) is either \(^{n}\) (the unconstrained case) or a nonempty closed and convex set satisfying \(C((F)(G))\).

It is worth noting that in the case where both \(F\) and \(G\) are non-smooth, the convergence result may not be as favorable as those in the previous scenarios. This is primarily due to the limited availability of information and unfavorable properties concerning \(F\) and \(G\). In this case, we employ a subgradient method to solve problem (P\({}_{}\)), which has been extensively studied in the existing literature (Shor, 2012; Bubeck et al., 2015; Beck, 2017; Nesterov, 2018). Specifically, we update

\[_{k+1}=_{C}(_{k}-_{k}_{k}),\] (4)

where \(_{k}_{}(_{k})\) is an subgradient of \(_{}(_{k})\), and \(_{C}()\) is the projection of \(\) onto \(C\).

Let \(_{}^{*}\) be an optimal solution of problem (P\({}_{}\)) and suppose that there exists a constant \(R\) such that \(\|_{0}-_{}^{*}\| R\). Motivated by Theorem 8.28 in Beck (2017), we establish the subsequent complexity result for problem (P).

**Theorem 3.9**.: _Suppose that Assumption 3.1(3) holds, \(f_{2}\) and \(g_{2}\) are \(l_{f_{2}}\)- and \(l_{g_{2}}\)-Lipschitz continuous, respectively. Set step-size \(_{k}=}\) in (4). Then, the subgradient method produces an \((,l_{f_{2}}^{-}^{})\)-optimalsolution of problem (P) after at most \(K\) iterations, where \(K\) satisfies_

\[K=(}^{2}}{^{2}}+}^{\{2 ,2\}}l_{g_{2}}^{2}}{^{\{2,2\}}}).\]

For non-smooth SBO problems, our method has lower complexity compared to existing approaches. Specifically, under a bounded domain assumption, Helou and Simoes (2017) simply proposed an \(\)-subgradient method with an asymptotic rate towards the optimal solution set. The a-IRG method in Kaushik and Yousefian (2021) achieved convergence rates of \((1/^{})\) and \((1/^{})\) for the upper- and lower-level objectives, respectively, where \(b(0,0.5)\). Setting \(b=0.25\) yields the convergence rates of \((1/^{4})\) for both upper- and lower-level objectives, which indicates that our complexity is more efficient than theirs when \(<2\) and \(\). Furthermore, the online framework proposed in Shen et al. (2023) performed a complexity of \((1/^{4})\) for both upper- and lower-level objectives. Similarly, our approach prevails over theirs when \(<1.5\) and \(\).

Strongly convex upper-level objective.Based on Theorem 8.31 in Beck (2017), we next explore the improved complexity result for problem (P) when \(f_{2}\) is additionally strongly convex.

**Theorem 3.10**.: _Suppose that Assumption 3.1(3) holds, \(C((F)(G))\). \(f_{2}\) is \(l_{f_{2}}\)-Lipschitz continuous and \(_{f_{2}}\)-strongly convex3, and \(g_{2}\) is \(l_{g_{2}}\)-Lipschitz continuous. Choose step-size \(_{k}=}(k+1)}\) in (4). Then, the subgradient method produces an \((,l_{f_{2}}^{-}^{})\)-optimal solution of problem (P) after at most \(K\) iterations, where \(K\) satisfies_

\[K=(}^{2}}{_{f_{2}}}+}^{ \{2,2\}}l_{g_{2}}^{2}}{_{f_{2}}^{\{2-1,2 -1\}}}).\]

To our knowledge, within the context of Theorem 3.10, current findings fail to exploit strong convexity to enhance results. However, our approach capitalizes on distinct structural characteristics that yield superior complexity outcomes relative to Theorem 3.9 in cases where \(<2\) and \(\).

## 4 Numerical experiments

We apply our Algorithms 1, 2, 3 and 4 to two simple bilevel optimization problems from the motivating examples in Appendix A. The performances of our methods are compared with several existing methods: MNG (Beck and Sabach, 2014), BiG-SAM (Sabach and Shtern, 2017), DBGD (Gong et al., 2021), a-IRG (Kaushik and Yousefian, 2021), CG-BiO (Jiang et al., 2023), Bi-SG (Merchav and Sabach, 2023) and R-APM (Samadi et al., 2023). For practical efficiency, we use the Greedy FISTA algorithm proposed in Liang et al. (2022) as the APG method in our approach. Detailed settings and additional experimental results are presented in Appendix F.

### Logistic regression problem (LRP)

The LRP reads

\[_{^{n}}\|\|^{2} \ \ *{arg\,min}_{^{n}} _{i=1}^{m}(1+(-_{i}^{}b_{i}))+I_{C} (),\] (5)

where \(I_{C}()\) is the indicator function of the set \(C=\{^{n}:\|\|_{1}\}\) with \(=10\). Our goal is to find a solution to the lower-level problem with the smallest Euclidean norm. The upper-level objective only consists of the smooth part, which is \(1\)-strongly convex and \(1\)-smooth; meanwhile, the lower-level objective is a composite function, where the smooth part is \(_{}(A^{}A)\)-smooth, and the nonsmooth part is prox-friendly (Duchi et al., 2008).

In this experiment, we compare our methods with MNG, BiG-SAM, DBGD, a-IRG, CG-BiO, and Bi-SG. We plot the values of residuals of the lower-level objective \(G(_{k})-G^{*}\) and the upper-level objective over time in Figure 1.

As shown in Figure 1, the PB-APG, aPB-APG, PB-APG-sc, and aPB-APG-sc algorithms exhibit significantly faster convergence performance than the other methods for both lower- and upper-level objectives, although R-APM attains similar outcomes, our PB-APG and PB-APG-sc ensure a more rapid decline than it, as shown in the first subfigure of Figure 1. This is because our methods achieve lower optimal gaps and desired function values of the lower- and upper-level objectives with less execution time. This observation confirms the improved complexity results stated in the theorems above. Although the high exactness of our methods for the lower-level problem leads to larger upper-level objectives, Table 3 in Appendix F.1 shows that our methods are much closer to the optimal value. This is reasonable because the other methods exhibit lower accuracy at the lower-level problem, resulting in larger feasible sets compared to the lower-level optimal solution set \(X_{}\). In addition, Figure 1 demonstrates that aPB-APG and aPB-APG-sc outperform PB-APG and PB-APG-sc in terms of convergence rate. This improvement can be attributed to the adaptiveness incorporated in Algorithms 2 and 4.

### Least squares regression problem (LSRP)

The LSRP has the following form:

\[_{^{n}}\|\|^{2}+\|\|_{1}\ *{arg\,min}_{^{n}}\|A-b\|^{2},\] (6)

where \(=0.02\) regulates the trade-off between \(_{1}\) and \(_{2}\) norms. We aim to find a sparse solution for the lower-level problem. The upper-level objective is formulated as a composite function, which consists of a \(\)-strongly convex and \(\)-smooth component, along with a proximal-friendly non-smooth component (Beck, 2017). The lower-level objective is a smooth function with a smoothness parameter of \(_{}(A^{}A)\).

In this experiment, we compare the performances of our methods with a-IRG, BiG-SAM, and Bi-SG. We plot the values of residuals of lower-level objective \(G(_{k})-G^{*}\) and the upper-level objective over time in Figure 2.

Figure 2 shows that the proposed PB-APG, aPB-APG, PB-APG-sc, and aPB-APG-sc converge faster than the compared methods for both the lower- and upper-level objectives, as well. For the upper-level objective, our methods achieve larger function values than other methods, except BiG-SAM (\(=0.01\)). This is because our methods attain higher accuracy for the lower-level objective than other methods. We have similar observations in Section 4.1. Furthermore, Figure 1 also demonstrates that the adaptive mechanism produces staircase-shaped curves for aPB-APG and aPB-APG-sc, which might prevent undesirable fluctuations in PB-APG and PB-APG-sc.

## 5 Conclusion

This paper proposes a penalization framework that effectively addresses the challenges inherent in simple bilevel optimization problems. By delineating the relationship between approximate solutions of the original problem and its penalized reformulation, we enable the application of specific methods under varying assumptions for the original problem. Under the Holderian error bound condition, our methods achieve superior complexity results compared to the existing methods. The performance is further improved when the smooth component of the upper-level objective is strongly convex. Additionally, we extend our framework to scenarios involving general nonsmooth objectives. Numerical experiments also validate the effectiveness of our algorithms.