# Global Convergence in Training Large-Scale Transformers

Cheng Gao\({}^{1}\)1 Yuan Cao\({}^{2}\)1 Zihao Li\({}^{1}\) Yihan He\({}^{1}\) Mengdi Wang\({}^{1}\)

Han Liu\({}^{3}\) Jason M. Klusowski\({}^{1}\)2 Jianqing Fan\({}^{1}\)2

\({}^{1}\)Princeton University \({}^{2}\)The University of Hong Kong \({}^{3}\)Northwestern University

{chenggao,zihaoli,yihan.he,mengdiw,jason.klusowski,jqfan}@princeton.edu

yuancao@hku.hk hanliu@northwestern.edu

Equal Contribution.

###### Abstract

Despite the widespread success of Transformers across various domains, their optimization guarantees in large-scale model settings are not well-understood. This paper rigorously analyzes the convergence properties of gradient flow in training Transformers with weight decay regularization. First, we construct the mean-field limit of large-scale Transformers, showing that as the model width and depth go to infinity, gradient flow converges to the Wasserstein gradient flow, which is represented by a partial differential equation. Then, we demonstrate that the gradient flow reaches a global minimum consistent with the PDE solution when the weight decay regularization parameter is sufficiently small. Our analysis is based on a series of novel mean-field techniques that adapt to Transformers. Compared with existing tools for deep networks  that demand homogeneity and global Lipschitz smoothness, we utilize a refined analysis assuming only _partial homogeneity_ and _local Lipschitz smoothness_. These new techniques are of independent interest.

## 1 Introduction

Transformers have revolutionized the field of deep learning since their introduction in . These models are distinguished by their immense scales, often comprising billions of parameters to achieve state-of-the-art performance. Notably, this massive parameterization enables them to excel in a variety of domains, notably in natural language processing  and vision tasks , where they have significantly advanced the frontiers of machine learning.

Despite the widespread adoption of Transformer models, our understanding of their optimization guarantees is still in its early stages. One particularly intriguing phenomenon is that as the size of model increases, training algorithms typically converges globally despite the highly nonconvex landscape of the training objective function. Remarkably, it remains somewhat enigmatic how gradient-based approaches can consistently succeed when training large-scale Transformers.

Notably, there have been several recent works showing the global convergence of training overparameterized neural networks . In particular, several works  studied the setting with deep neural networks with skip connections. By studying the connections between the network with discretization in the parameter space and a corresponding ordinary differential equation system , these works demonstrated global convergence guarantees of wide and deep neural networks based on a mean-field analysis. However, these results are established based on certain homogeneity and/or global Lipschitz smoothness properties of theneural network, which are not applicable to Transformer models. Therefore, it remains an open question how gradient-based methods can effectively train large-scale Transformers.

### Our contribution

In this work, we bridge the gap between Transformer theory and practice by demonstrating the global convergence of Transformer training optimization via gradient flow in a large-scale model regime. We analyze the mean-field limit of the Transformer model, which is characterized by the _distribution_ of model parameters, shifting the focus from parameter space to distributional dynamics in the Wasserstein metric . This approach yields two key theorems:

1. We show the closeness between practical discrete Transformers trained by gradient flow and continuous Transformers whose parameter distribution follows a partial differential equation of the Wasserstein gradient flow (Theorem 3.1). Our result demonstrates that large-scale discrete Transformers can be approximated by its mean-field limit and the approximation error can be expressed in terms of the width and depth of the Transformer models.
2. This approximation facilitates our analysis of the global convergence (Theorem 4.1) of discrete Transformer models. By leveraging the universal approximation capabilities of either the self-attention or feed-forward layers, we demonstrate that a basic gradient flow method can reliably find a global optimum, despite the highly non-convex landscape of the training objective.

We also highlight our novel contributions to Transformer theory through the development of these two core results:

1. The assumption on activation regularity conditions (Assumption 2) is less stringent compared to those usually found in studies of two-layer neural networks [51; 16; 28; 14] or deep ResNet networks [22; 23; 47]. In particular, many existing approximation guarantees reply on a Lipschitz continuity property of the network gradients, which limits the mean-filed study to neural networks with smooth activation functions. In comparison, our analysis relaxes this assumption and only requires local Lipschitz continuity of the gradient in expectation. This relaxation broadens the applicability of our approach and ensures that our result can cover more practical Transformer architectures.
2. Our model differs from the ResNet models in [47; 22; 23; 13], as those models incorporate only a single identical encoder within each evolutionary block. Unlike the typical theoretical configurations, our model employs two distinct encoders \(f\) and \(h\) that alternate throughout the network's depth. More importantly, despite the distinct encoders used, the continuous limit of our model uniformly interprets the encoder as an average of \(f\) and \(h\), providing a rigorous validation of concepts proposed in  and  from a new perspective.
3. Our global convergence guarantee for training Transformer models is also broadly applicable: our assumption (Assumption 4) ensures global convergence by relying on the universal approximation capabilities of _either_ the self-attention or the feed-forward encoder. Additionally, we incorporate a more flexible framework by adopting partial \(1\)-homogeneity for only a subset of the parameters, in contrast to the full parameter homogeneity required in studies such as . This modification enables the use of softmax and sigmoid activation, expanding beyond the hardmax and ReLU restricted by full homogeneity.

Additional related works.See Appendix B for a detailed discussion.

Notations.For any \(^{d}\), \(()\) refers to its dimension \(d\). For any \(B^{d d}\), its trace is denoted by \((B)\). For any positive integer \(n\), Let \([n]=\{1,2,,n\}\). Let \(0_{d}\) denote the \(d\)-dimension vector of all zeros. Let \(W_{p}(,)\) denote the Wasserstein-\(p\) distance between two probability measures \(,(^{d})\) for \(p 1\). For a matrix \(A=(a_{1},a_{2},,a_{n})\), define its vectorization version as \([A]:=(a_{1}^{},a_{2}^{},,a_{n}^{})^{}\). Let \(()\) denote the Dirac mass and \(\{\}\) be the indicator function. Let \(()\) denote the support of any distribution. Let \(\|\|=\|\|_{2}\) denote the \(l_{2}\) norm and \(\|\|_{}\) denote the maximum norm. For any subsets \(D_{1},D_{2}\) in Euclidean space, define \((D_{1},D_{2})\) as the collection of functions that map \(D_{1}\) to \(D_{2}\) and are continuous over \(D_{1}\). Define the Bounded Lipschitz norm for any measure \((^{d})\) as \(\|\|_{}:=\{ fd:f:^{d}, \ |f| 1,\ f\ \ 1-\}\).

Transformer model

In this section, we describe our deep Transformer model with each data input as a sequence, and the gradient flow algorithm used for training.

### Data setting

In our paper, the data input is both general and straightforward: an input sequence \(H^{D(N+1)}\) consisting of \(N+1\) tokens, each with dimension \(D\). We consider the setting where each input sequence \(H\) is associated with a label \(y(H)\), where \(y(H)\) is the target function we aim to learn. Furthermore, we assume that each instance \(H\) is i.i.d. drawn from a population distribution \(\).

Relation to in-context learning (ICL)Our data setting is versatile and applicable to any task involving sequential input. It particularly suits the in-context learning (ICL) scenario [6; 10; 75], where models are capable of making accurate predictions on new data when prompted with training examples from the same pool. For clarity, consider the input sequence \(H^{D(N+1)}\) formatted as follows:

\[H=[h_{1},h_{2},,h_{N+1}]=x_{1}&x_{2}&&x_{N}&x_{N+1} \\ y_{1}&y_{2}&&y_{N}&0\\ p_{1}&p_{2}&&p_{N}&p_{N+1}, y_ {N+1}=y(H).\]

Here, \(\{x_{i}\}_{i[N]}\) are the input vectors, each associated with a corresponding label \(\{y_{i}\}_{i[N]}\). The last token, \(x_{N+1}\) is the test input for which a prediction is made. The third row contains the customized and fixed positional encoding vectors \(\{p_{i}\}_{i[N]}\), which typically include ones, zeros, and indicators denoting the token for prediction. The label for the query point \(x_{N+1}\) is then given by \(y_{N+1}=y(H)\) in our terminology. ICL operates in a zero-shot fashion, without any updates to the model's parameters, highlighting a unique and powerful capability of these systems to adapt and generalize based on the provided context alone. In , the authors demonstrate that fixed Transformers can approximate in-context penalized generalized linear regression to any desired degree.

### Model

We follow a common configuration of Transformer architectures [6; 38; 40; 48; 73] where each Transformer block consists of two distinct layers: a self-attention mechanism layer and a token-wise feed-forward neural network layer, both equipped with skip connections. We assume that both layers consist of the average of \(M\) heads, treated uniformly as the _width_ across all blocks for simplicity. The formulation for a matrix input \(Z^{D(N+1)}\) and a given residual step size \(>0\) is as follows: Each residual self-attention layer is represented by

\[_{_{1},_{2},,_{M}}(Z,)=Z+ M^{-1} _{j=1}^{M}f(Z,_{j}),\] (2.1)

and each residual feed-forward neural network layer is defined by

\[_{w_{1},w_{2},,w_{M}}(Z,)=Z+ M^{-1}_{j=1}^{M}h (Z,w_{j})\] (2.2)

for parameter vectors \(\) and \(w\) in the Euclidean space. The encoders for the self-attention and feed-forward layers are denoted as \(f:^{D(N+1)}^{D(N+1)}\) and \(h:^{D(N+1)}^{D(N+1)}\), respectively. The self-attention encoder \(f\) formulation, commonly adopting a _multiplicative_ or _dot-product_ approach as detailed in [8; 38; 48; 64; 66; 73], can be exemplified by

\[f(Z,)=W_{O}W_{V}Z_{}(W_{K}Z)^{}W_{Q}Z,\]

where \(W_{V},W_{K},W_{Q}^{s D}\), and \(W_{O}^{D s}\). This formulation can be reparametrized to

\[f(Z,)=VZ_{}Z^{}WZ,\] (2.3)where \(V,W^{D D}\), \(=[V,W]\). The activation \(_{}\) typically uses column-wise softmax, but component-wise ReLU is also viable, as in . For the feed-forward layer, an example of the encoder is \(h(Z,w)=W_{2}_{}(W_{1}Z)\), as detailed in [6; 38; 73], where \(w=[W_{1},W_{2}]\) and the activation \(_{}\) is component-wise ReLU. Alternatively, setting \(h 0\) results in a Transformer block that comprises only the self-attention layer, referred to as "attention-only" Transformers, as discussed in [6; 46; 49; 66].

Next, we analyze a Transformer network composed of \(L\) Transformer blocks, referring to \(L\) as the _depth_ of the model. In this paper, we introduce an additional term, \(\), in (2.1) and (2.2) to simulate the model's evolution in a residual manner. We set the step size \(\) as \( t/2\), where \( t=1/L\). As \(L\) increases, \( t\) approaches zero, allowing Transformer blocks to incrementally contribute to the model's overall progression. The structure of the network is then defined as follows:

\[_{}(H,t+ t/2)&=_{_{t,1},,_{t,M}}(_{}(H,t), t/2)\\ _{}(H,t+ t)&=_{w_{t,1},,w_{t,M}}( _{}(H,t+ t/2), t/2)\] (2.4)

for each \(t=0,\  t,,\ (L-1) t\) with \(_{}(H,0)=H\). We abbreviate the subscript \(t=0, t,,(L-1) t\) by \(t\) and \(j=1,2,,M\) by \(j\) for simplicity. Here, \(=\{_{t,j},w_{t,j}\}_{t,j}\) denotes all parameters in the Transformer model.

Throughout this paper, we treat \(D\) and \(N\) as bounded finite values, while \(M\) and \(L\) are treated as diverging, aligning with the setting of large-scale Transformers.

### Gradient flow

For the \(l_{2}\) regularization with \(>0\), we consider training the constructed Transformer model using the following \(\)-regularized risk objective:

\[()=()+_{t}_{j=1} ^{M}(\|_{t,j}\|_{2}^{2}+\|w_{t,j}\|_{2}^{2}),\] (2.5)

with the population squared risk function defined as

\[()=_{}[ _{}(H,1)]-y(H)^{2}.\]

In Section 3.2, we will show that \(l_{2}\)-regularization on the parameter norms is essential for the well-posedness of the (Wasserstein) gradient flow to control parameter growth under our mild assumptions, even with a very small \(>0\). Similar strategies that consider necessary \(l_{2}\) regularization are employed in  and . Then, drawing on the methodologies in [6; 32; 46], our model processes the final output through a simple _read-out_ function, \([]\), extracting the \((d+1,N+1)\)-th entry of its input. We propose that this read-out layer can be expanded to any linear mapping with bounded parameter norm without affecting the validity of our theoretical results.

To minimize the objective function (2.5), we implement the _standard gradient flow method_ as follows:

1. Initially, for each \(t=0, t,,(L-1) t\), we sample \(M\) particles \(_{t,j}^{(0)},w_{t,j}^{(0)}\) with \(j[M]\) independently from \(_{0}(,w|t)\), where \(_{0}\) is a pre-defined distribution with bounded support.
2. Then, we update all parameters \(_{t,j}^{()},w_{t,j}^{()}\) in the set \(^{()}=\{_{t,j}^{()},w_{t,j}^{()}\}_{t,j}\) using gradient flow (scaled by \(ML\)), which is defined as follows: \[^{()}}{d}=-ML_{_{t,j}}[( ^{()})],^{()}}{d}=-ML_{w_{t,j}}[ (^{()})].\] (2.6)

Define the function \((H;)=([_{}(H,1)]-y(H ))^{2},\) and the partial derivative \(_{}(H,t)=(H;)/_{ }(H,t)^{}\) for each \(t=0, t/2, t,,(L-1) t,(L-1/2) t\). Refer to Appendix C.4 for the explicit formula of \(_{}(H,t)\). Using the chain rule, we derive the explicit form of the gradient flow as follows:

\[^{()}}{d}=-_{f}(_{t,j}^{()}, ^{()},t),^{()}}{d}=-_{h}(w_{t,j}^{()},^{()},t).\] (2.7)where

\[_{f}(,,t)=_{} _{}f(_{}(H,t),)^{} _{}(H,t+ t/2)+,\] \[_{h}(w,,t)=_{} _{w}h(_{}(H,t+/2),w)^{} _{}(H,t+ t)+ w\]

for \(t=0, t,,(L-1) t\).

## 3 Approximation by the mean-field limit

In this section, we present a rigorous approximation result that bridges Transformer models in (2.4) with their mean-field limit as continuous Transformers. Thus, the width \(M\) and depth \(L\) in our proposed model are treated as discretization of this continuous limit in the parameter space.

### Assumptions

In addition, we introduce the norm \(\|\|_{2-}\) as the maximum \(l_{2}\) norm across all columns of a matrix. We proceed under several mild assumptions related to the data distribution and the encoders \(f\) and \(h\).

**Assumption 1** (Data regularity).: _There exists some universal constant \(B>0\) such that, for any \(H()\), we have \(\{\|H\|_{2-},y(H)\} B\). In addition, a universal constant \(K_{y}>0\) ensures that \(y(H)\) is \(K_{y}\)-Lipschitz continuous for \(\|\|_{F}\) over \(H()\)._

RemarkAssumption 1 is irrelevant to the Transformer model, and is only a fairly mild assumption on the data.

**Assumption 2** (Transformer particle growth bound).: _We assume that the gradient of \(f(T,)\) and \(h(T,w)\) exists. Furthermore, we have_

1. \(\|f(T,)\|_{2-} K\|T\|_{2-}(1+\|\|+\| \|^{2})\)_._
2. _For every_ \(i[N+1]\)_, we have_ \(\|_{}f(T,)_{:,i}\|_{2}_{P}(\|T\|_{2-})( 1+\|\|)\)_._
3. \(\|_{[T]}[f(T,)]\|_{2}_{T}(N,D,\| T\|_{F})(1+\|\|+\|\|^{2})\)_._

_for some continuous, monotonically increasing functions \(_{P},_{T}\) for every coordinate, and a universal constant \(K>0\). Similarly, if we replace \(f\) with \(h\) and \(\) with \(w\), the same conditions apply._

RemarkThere are three key observations for Assumption 2. Firstly, it incorporates the \(\|\|_{2-}\) norm, which is particularly useful for handling sequential inputs where each column represents a token. Secondly, as we consider higher-order multiplications between data and parameters, this assumption accommodates a broader range of self-attention encoders, such as the one in (2.3) with softmax or ReLU activation (where the derivative is defined as \(^{}(x)=\{x>0\}\)). Lastly, a particularly interesting and frontier question is identifying the function \(_{T}\), and we have listed related literature in Appendix B.

**Assumption 3** (Locally Lipschitz continuous gradient in expectation).: _Besides Assumption 2, for any \(L_{T}>0\) and any \(L_{T}\)-Lipschitz continuous functions \(T_{1}=T_{1}(H)\) and \(T_{2}=T_{2}(H)\), for every \(i[N+1]\), we have_

\[i.\ _{}\|_{}f(T_{1},)_{:,i}- _{}f(T_{2},)_{:,i}\|_{2}_{PT}(\|\|,K_{T},L_{T}) _{H}\|T_{1}-T_{2}\|_{2-},\] \[ii.\ _{}\|_{[T]}[f(T_{1}, )]-_{[T]}[f(T_{1},^{})]\|_{2} _{TP}(N,D,_{H}\|T_{1}\|_{F},K_{P},L_{T})\|-^{}\|\] \[iii.\ _{}\|_{}f(T_{1},)_{:,i}- _{}f(T_{1},^{})_{:,i}\|_{2}_{PP}(K_{P},_{H}\|T_{ 1}\|_{2-},L_{T})\|-^{}\|,\] \[iv.\ _{}\|_{[T]}[f(T_{1}, )]-_{[T]}[f(T_{2},)]\|_{2}_{ TT}(N,D,K_{T},\|\|,L_{T})_{H}\|T_{1}-T_{2}\|_{F}\]

_for \(K_{T}=\{_{H}\|T_{1}\|_{2-},_{H}\|T_{2}\|_{2-}\},K_{P}=\{\|\|,\|^{}\|\}\), and some continuous functions \(_{PT},_{TP},_{PP},_{TT}\) that are monotonically increasing for every coordinate. Similarly, if we replace \(f\) with \(h\) and \(\) with \(w\), the same conditions apply._RemarkAssumption 3 states that functions are locally Lipschitz continuous in expectation, suitable for encoders that utilize ReLU functions and have second-order derivatives almost everywhere. This assumption is naturally satisfied if the activation has a locally Lipschitz continuous gradient.

Define \(^{2}\) as the set of probability measures endowed with the Wasserstein-\(2\) distance, where the Lipschitz continuity with respect to the depth holds, i.e. there exists some universal constant \(C_{}>0\) such that \(\|(,t)-(,t^{})\|_{} C_{}|t-t^{ }|\) for any \(t,t^{}\).

Choice of \(_{0}\)Suppose \(_{0}^{2}\) satisfies that for any \(t\), the support of \(_{0}(,,t)\) is contained within the set \(\{(,w):\|\|^{2}+\|w\|^{2} R^{2}\}\) for a universal constant \(R\). Additionally, for each \(t\), it holds that \(_{,w}_{0}(,w,t)d(,w)=1\). This condition suits common bounded support distributions, and a natural choice is a uniform distribution across a disk with radius \(R\) for each \(t\).

We would like to clarify that verifying Assumptions 2 and 3 for concrete examples of Transformer architectures with smooth activation functions is fairly intuitive, and the proof is mainly based on a series of tedious calculations. We give a concrete proposition with its brief proof in Appendix G.

### Continuous Transformer and Wasserstein gradient flow

Drawing inspiration from  and , which suggest that deep residual networks behave like ensembles of residual networks locally, we apply a similar manipulation to formulate the continuous version of (2.4). Consider the following continuous version \(T_{}(H,t)^{D(N+1)}\), governed by the following continuous ODE that _averages the two encoders_:

\[_{}(H,t)=_{,w}(H,t),)+h(T_{}(H, t),w)}{2}(,w,t)d(,w), T_{}(H,0)=H\] (3.1)

In (3.1), each encoder \(f\) or \(h\) is conceptualized as a particle, and we consider the distribution of these particles denoted as \((,w,t)\). For any \(^{2}\) that have a bounded support, the well-posedness of \(T_{}(H,t)\) that satisfies the Transformer ODE (3.1) is shown in Proposition C.1. Transitioning to the framework with continuous Transformers, our objective shifts to minimizing the \(l_{2}\) risk function with regularization on the second moment of \(\) as follows:

\[Q()=R()+_{0}^{1}_{,w}(\|\|_{2}^{ 2}+\|w\|_{2}^{2})(,w,t)d(,w)dt,\] (3.2)

with

\[R()=_{}[T_{}(H,1)] -y(H)^{2}.\] (3.3)

Define \(p_{}(H,t)^{D(N+1)}\), the partial derivative of \(R()\) relative to \(T_{}(H,t)\) at a local query point \(H\), as the solution derived in Appendix C.4 using the classical adjoint sensitivity method :

\[[p_{}(H,t)]^{}=[T_{}(H,1)]-y(H) _{t}^{1}_{}_{[T]}[g(T_{}(H,t),)(,t)d dt]_{DN+d+1,:}.\]

Using this, we can compute the functional derivative to \(\) as follows:

\[(,w,t)=_{} (H,t),)+h(T_{}(H,t),w)}{2}^{ }p_{}(H,t)+(\|\|_{2}^{2}+\|w\|_{ 2}^{2}).\] (3.4)

The following Proposition claims that \(\) is indeed the derivative with respect to \(\) (specifically, the Frechet derivative ) for the functional \(Q()\).

**Proposition 3.1** (Functional derivative to \(\)).: _Under Assumptions 1 and 2, for any pair \(,^{2}\) that have bounded supports, we have_

\[Q(+(-))=Q()+,- +o(),\]

_where \(\) is defined in (3.4), and \((,-)=_{0}^{1}_{(,w)}(-)d(,w)dt\)._Now, we are in a position to display the gradient flow of \(\) in the Wasserstein metric , given by a McKean-Vlasov type equation [4; 37; 54; 56]. Specifically, we study the following partial differential equation of the distribution \(^{()}(,w,t)\):

\[(,w,t)}{d}& =_{(,w)}^{()}_{( ,w)}_{=^{()}}\\ &=_{}^{()}G_{f}(, ^{()},t)+_{w}^{()}G_{h}(w, ^{()},t),\] (3.5)

where \(^{(0)}=_{0}\), \(\) is the divergence operator, and the gradient functions are defined as

\[ G_{f}(,,t)&= _{}_{}f(T_{}(H,t), )^{}p_{}(H,t)+,\\ G_{h}(w,,t)&=_{} _{w}h(T_{}(H,t),w)^{}p_{}(H,t) + w.\]

Propositions D.1 and 3.2 provide the well-posedness of both gradient flow and Wasserstein gradient flow respectively. In both propositions, a \(>0\) is essential to stabilize the optimization process by controlling both the maximum and average norms across all parameters. If \(\) is set to \(0\), it is only possible to establish the well-posedness of (3.5) over a finite maximal interval . Similar adjustments to regularize the risk function are also noted in .

**Proposition 3.2** (Existence and uniqueness of Wasserstein gradient flow).: _Under Assumptions 1 and 2, there exists a unique solution \((^{()})_{ 0}^{2}\) with \(^{(0)}=_{0}\) for (3.5). Additionally, for any \( 0\), we have_

_i. \(^{()}\) has a bounded support \(\{,w:\|\|^{2}+\|w\|^{2} R_{}\}\), where \(R_{}=(R+1)(C_{R})-1\) for some constant \(C_{R}\) that only depends on \(N,D,\) and the parameters of the assumptions._

_ii. \(_{0}^{1}(\|\|^{2}+\|w\|^{2})^{()}(,w,t)d(,w)dt  A_{0}^{2}\), where \(A_{0}:=R^{2}+^{-1}(2B^{2}+2B^{2}(K(1+R+R^{2}))^{2})\)._

_iii. \(_{(,w)}^{()}(,w,t)d(,w)=1\) for any \(t\)._

### Approximation of large-scale Transformer

In this section, we discuss the general results associated with approximating our discrete Transformer model to its mean-field limit. First, we highlight that the minimization of the risk function with discretization, whether or not regularization is included, closely approximates the minimal risk achievable by continuous models.

**Proposition 3.3** (Global minimum approximation of discretization).: _Under Assumptions 1 and 2, we define \(^{2,r}\) as the set of distributions in \(^{2}\) concentrated on \(\{(,w):\|\|^{2}+\|w\|^{2} r^{2}\}\), for any \(r>0\). Then there exists a constant \(C\) dependent on \(N,D,r\) and the parameters of the assumptions such that_

\[_{}()& _{^{2,r}}R()+CL^{-1}+},\\ _{}()&_{ ^{2,r}}Q()+C(1+)L^{-1}+}.\]

Proposition 3.3 specifies that the distributions under consideration must have bounded support. While it is typically challenging to confirm whether the minimal risk is indeed achieved on a distribution with bounded support, this assumption is justified as \(\) regulates parameter norms, implicitly encourages solutions residing in a compact region of the parameter space.

We now present the main theorem concerning the convergence of the gradient flow process to the Wasserstein gradient flow as outlined in (3.5). The proof with detailed explanation of the techniques used in Theorem 3.1 is provided in Appendix D.

**Theorem 3.1** (Gradient flow approximation of discretization).: _Define the empirical distribution as \(^{()}:=_{t}_{j=1}^{M}(_{t,j}^{( )},w_{t,j}^{()},t)\) for any \( 0\). Under Assumptions 1-3, we _have that \((^{()})_{ 0}\) weakly converges to \((^{()})_{ 0}\) almost surely along any sequence such that \(L,M/ L\). Moreover, for any fixed \(>0\) and any \(>0\), with probability at least \(1-3(-)\) with respect to the parameter initialization \(^{(0)}\), we have_

1. \(_{s[0,]}[_{^{(s)}}(H,t)] -[T_{^{(s)}}(H,t)] CL^{-1}+}\)__
2. \(_{s[0,]}(^{(s)})-R(^{(s)}) C L^{-1}+}\)__
3. \(_{s[0,]}(^{(s)})-Q(^{(s)}) C L^{-1}+}\)__

_for some constant \(C\) that depends on on \(N,D,,\) and the parameters of the assumptions._

Theorem 3.1 significantly advances our understanding by controlling the difference regarding both the Transformer output, the risk function, and the regularized risk function. It's noted that the difference bound in the model's approximation may increase, possibly exponentially [22; 23; 51], as the time horizon extends. As argued in , such behavior may be inherent to the systems being modeled.

Additionally, the technical uniqueness and innovation of this theorem contrast sharply with previous results from overparametrized ResNet models. Our analysis distinguishes itself in two ways. First, our discrete Transformer model (2.4) uniquely splits the averaged encoder \((f+h)/2\) into two distinct blocks with encoders \(f\) and \(h\). Second, we demonstrate uniform error control over any finite time interval \([0,]\), enabling continuous monitoring of maximum error across the gradient flow's trajectory. In contrast, models in prior studies such as [22; 23] restricts the error analysis to a specific \(s[0,]\).

## 4 Global convergence of gradient flow

In this section, we explore the optimization problem for gradient flow in the context of the discrete Transformer model, focusing on our general global convergence results.

### An additional assumption

To ensure the global convergence of gradient flow for our discrete Transformer model, we introduce the following assumption. While influenced by the work in [16; 22; 23; 47], our assumption is uniquely tailored to the context of Transformers:

**Assumption 4**.: _There exists a pair \((g,)\{(f,),(h,w)\}\) with a partition \(=(_{1},_{2})\) such that_

1. _(Partial_ \(1\)_-homogeneity) for any_ \(T^{D(N+1)}\) _and_ \(c\)_, we have_ \(g(T,c_{1},_{2})=cf(T,_{1},_{2})\)_._
2. _(Universal kernel) a compact set_ \(^{(_{2})}\) _ensures that the span of_ \(g(,):^{(_{1})}}\) _is dense in_ \(( T_{2-} B,^{D(N+1)})\) _for any_ \(B>0\)_._

We emphasize that the universal kernel property, as discussed in , closely relates to the universal approximation abilities. Under our assumption, we require the universal approximation capabilities of _either_ the self-attention encoder or the feed-forward encoder. In Appendix G, we provide a concrete example of Transformer architectures and verify the validity of Assumption 4.

The universal kernel property of the feed-forward layer encoder \(h\) is well-established, particularly in two-layer neural network contexts . Conversely, the universal approximation abilities of self-attention layers is a frontier research area, which, while not extensively covered in this paper, holds significant potential. Often labeled as "memorization capacity", this area is recently explored across multiple studies [27; 31; 38; 39; 49; 63; 73]. The interconnection between approximation abilities and memorization capacities is established in . Notably,  investigated the expressive capabilities of one single multi-head softmax self-attention layer, thereby potentially validating our assumptions.

Finally, we posit that the universal kernel applies to \(_{2}\) within a compact set, as the function's scale can be moderated by the homogeneous part \(_{1}\). In scenarios where \(_{2}\) and \(\) are absent, our assumption simplifies to that in , characterized by complete homogeneity. Conversely, in the absence of the \(_{1}\) component, our framework aligns with  which necessitates a more stringent support condition for \(\), as detailed later in Theorem 4.1.

### Global convergence result

In this section, we establish the convergence properties of the optimization task for discrete Transformers through gradient flow dynamics.

**Theorem 4.1** (Global convergence up to \(\)).: _Suppose that Assumptions 1-4 hold, and the Wasserstein gradient flow \((^{()})_{ 0}\) weakly converges to some \(_{}^{2}\). If for some universal constant \(R_{}>1\), the following two conditions hold:_

1. \((^{()})_{ 0}\) _is concentrated on_ \(\{,w:\|\|^{2}+\|w\|^{2} R_{}^{2}\}\) _when_ \(\) _is sufficiently large._
2. _If Assumption_ 4 _holds with_ \((g,)=(f,)\)_, we assume there exists a_ \(t^{*}\) _such that the connected set_ \((_{}(,t^{*})) \{w_{0}\}\)_, for some_ \(w_{0}^{(w)}\) _and_ \(^{(_{1})}\) _that separates_ \(\{_{1}:\|_{1}\|=1/R_{}\}\) _and_ \(\{_{1}:\|_{1}\|=R_{}\}\)_._
3. _If Assumption_ 4 _holds with_ \((g,)=(h,w)\)_, we assume there exists a_ \(t^{*}\) _such that the connected set_ \((_{}(,t^{*}))\{_{0}\} \)_, for some_ \(_{0}^{()}\) _and_ \(^{(w_{1})}\) _that separates_ \(\{w_{1}:\|w_{1}\|=1/R_{}\}\) _and_ \(\{w_{1}:\|w_{1}\|=R_{}\}\)_._

_Then, for any \(>0\), there exists some \(_{0}>0\) such that_

\[_{_{0}}(^{()})+C_{1}L ^{-1}+}+C_{2}\]

_with probability at least \(1-3(-)\) with respect to the parameter initialization \(^{(0)}\) for any \(>0\). Here, \(C_{1}\) is some constant dependent only on \(N,D,_{0},\) and the parameters of the assumptions, while \(C_{2}\) depends only on \(N,D,R_{}\) and the parameters of the assumptions._

Theorem 4.1 depicts the behavior of the risk function \((^{()})\) as the training duration \(\) is sufficiently large. Specifically, \((^{()})\) asymptotically approaches zero as both \(L\) and \(M/ L\), with an additional term that scales with \(\). This additional term attributes to the incorporation of a \(\)-weighted penalty on the norm of the parameters in our training objective \(\). Consequently, by selecting an appropriately small \(>0\), the risk approximates zero, demonstrating global convergence to the minimum of \(\).

In addition, Theorem 4.1 posits some additional assumptions: the weak convergence of \(^{()}\), the long-time uniform boundedness, and the separation property for \(_{1}\) with the support expansion of \(_{2}\) to \(\). Similar assumptions are made in the literature of deep model optimization theory . While these types of assumptions are typically challenging to justify, we provide high-level justifications for them in Appendix C.5, deferring detailed verification to future research.

We then present a corollary that directly follows from Theorem 4.1:

**Corollary 4.1**.: _Continuing with the notations and assumptions from Theorem 4.1, suppose \( C_{}\) for some universal constant \(C_{}>0\). Then, for any \(>0\), constants \(_{0},L_{0},K_{0}>0\) can be found such that:_

\[_{_{0},L>L_{0},M/ L>K_{0}}(^{()}) (1/2+C_{2}C_{})\] (4.1)

_with probability at least \(1-3(-)\) with respect to the parameter initialization \(^{(0)}\). Notably, if \(C_{}(2C_{2})^{-}\), the upper bound in (4.1) is less than or equal to \(\)._

Corollary 4.1 claims that with a fixed \(>0\), for any \(>0\), we can achieve an order of \(\)-close approximation with sufficiently large \(L\) and \(M\). Though our result is asymptotic and does not involve an explicit rate, it is the first of its kind and lays the groundwork for future theoretical optimization guarantees for Transformers.

Proof ideas of main theorems

Given the technical nature of this paper, this section presents the key ideas behind the proof of our main novel results, along with an outline of the proof preparation.

Idea for Theorem 3.1This convergence is described in two parts. First, the finite-time result (points (i)-(iii)) uses _propagation of chaos_ to analyze how differences evolve over time, comparing the evolution of parameter particles in discrete and continuous dynamics. The approximation bound is derived using a third auxiliary dynamic ("nonlinear dynamics"), involving the triangle inequality and Gronwall's inequality, which allows us to bound output differences over time.

Second, weak convergence of the empirical distribution process relies on optimal transport theory and stability results for Wasserstein gradient flows , focusing on the convergence of _momentum fields_. This also requires bounding the gradient differences between discrete and continuous Transformers as they approach the mean-field limit. See Appendix D for a detailed illustration, including a description of each main step.

Idea for Theorem 4.1We first establish the continuity of the functional gradient \(_{_{}}\), ensuring it remains constant if the derivative with respect to \(\) is constant over a region. Next, we derive the key bound for \(Q(_{})\), which is proportional to \(\), by analyzing the functional energy \(Q\)'s landscape through its derivatives.

Finally, we show that the finite-time risk can approach this bound. Achieving \(\)-level loss requires \(Q(^{(_{0})})\) for some large \(_{0}\). Applying Theorem 3.1, we show \((_{0})\) becomes sufficiently small, and since \((^{()})\) is non-increasing, it remains small for \(_{0}\). See Appendix E for a detailed illustration, including a description of each main step.

Proof preparation for the main theoremsAppendix C.3 lists several useful lemmas essential to the main results. Specifically, Lemmas C.1-C.6 ensure the boundedness of key components and bound the output differences between discrete and continuous Transformers under different parameter settings. This boundedness is non-trivial due to the mild Assumptions 2 and 3 that fit the Transformer architecture. The technical lemmas in Appendix C.3 form the foundation for all subsequent proofs. Before introducing the nonlinear dynamics used to bound parameter differences under non-i.i.d. settings, these lemmas first establish an important oracle approximation bound result (Lemma D.6) with i.i.d. parameter settings. Additionally, they serve as key tools for bounding the (functional) gradient differences between Transformer dynamics, as shown in Lemmas D.1-D.3, which are essential for proving the approximation bound in Theorem 3.1.

In Theorem 4.1, Lemma E.1 plays a key role by demonstrating that for any \(\), there is always a nearly descent direction around \(\) for \(Q()\), implying that all local minima are nearly global. This motivates further landscape analysis for bounding \(_{_{}}\) in the main theorem.

## 6 Conclusion

We conclude by summarizing our key contributions and suggesting future research directions. This paper establishes the global convergence of large-scale Transformer models through gradient flow dynamics, providing a thorough theoretical foundation. Our analysis, focused on the mean-field limit with infinite width and depth, shifts optimization from parameter space to distributional probability measures. We present two main theorems: one confirming the close approximation between discrete and continuous gradient flows, and another demonstrating global convergence, highlighting that basic optimization methods can successfully navigate complex landscapes to find optimal solutions. The techniques and results from this study lay the groundwork for further exploration into Transformer optimization. Future work could explore direct gradient descent with specific focus on step sizes, and expand on the in-context learning approximation capabilities of Transformers, as initiated by . Additionally, it's crucial to rigorously assess under what conditions can self-attention layers serve as universal kernels to enhance our theoretical understanding, and to determine the generalization error bounds of Transformers trained on finite samples. These directions promise to deepen the theoretical and practical insights into Transformer models.