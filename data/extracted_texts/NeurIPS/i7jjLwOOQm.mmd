# Learning Temporal Higher-order Patterns to Detect Anomalous Brain Activity

Ali Behrouz

Cornell University

Ithaca, NY

ab2947@cornell.edu &Farmoosh Hashemi

Cornell University

Ithaca, NY

sh2574@cornell.edu

###### Abstract

Due to recent advances in machine learning on graphs, representing the connections of the human brain as a network has become one of the most pervasive analytical paradigms. However, most existing graph machine learning-based methods suffer from a subset of five critical limitations: They are (1) designed for simple pairwise interactions while recent studies on the human brain show the existence of higher-order dependencies of brain regions, (2) designed to perform on pre-constructed networks from time-series data, which limits their generalizability, (3) designed for classifying brain networks, limiting their ability to reveal underlying patterns that might cause the symptoms of a disease or disorder, (4) designed for learning of static patterns, missing the dynamics of human brain activity, and (5) designed in supervised setting, relying their performance on the existence of labeled data. To address these limitations, we present HADiB, an end-to-end anomaly detection model that automatically learns the structure of the hypergraph representation of the brain from neuroimage data. HADiB uses a tetra-stage message-passing mechanism along with an attention mechanism that learns the importance of higher-order dependencies of brain regions. We further present a new adaptive hypergraph pooling to obtain brain-level representation, enabling HADiB to detect the neuroimage of people living with a specific disease or disorder. Our experiments on Parkinson's Disease, Attention Deficit Hyperactivity Disorder, and Autism Spectrum Disorder show the efficiency and effectiveness of our approaches in detecting anomalous brain activity.

## 1 Introduction

Recent advancements in neuroscience and neuroimaging have led researchers to shift from examining isolated brain regions to exploring network models . This shift is largely attributed to the rapid progress in technologies like functional Magnetic Resonance Imaging (fMRI) and structural Magnetic Resonance Imaging (sMRI) , which can provide large-scale neuroimaging data with better quality. In network models of the brain, brain regions of interest (ROIs) are represented as nodes, and the similarities between these regions form edges . Brain network models have demonstrated their effectiveness in enhancing our understanding of brain diseases and disorders . As a result, empirical data on brain networks has substantially increased in size and complexity, leading to a strong demand for appropriate tools and methods to model and analyze this data 

Simultaneously, machine learning techniques for analyzing graph-structured data have gained attention across various fields, including drug discovery, neuroscience, and biology . Although numerous studies have confirmed the effectiveness of machine learning for human brain network analysis, most have concentrated on graph or node classification tasks . These tasks usually aim at disease detection , biological feature prediction , or functional system identification .

However, the identification of abnormal brain activity, especially in those with neurological disorders, remains a critical focus for researchers. This is essential for understanding the mechanisms behind symptoms, enabling early detection, and facilitating the development of medical treatments. Most existing studies consider pairwise interaction among brain regions, neglecting the effect of non-pairwise interactions on the emerging dynamics. Recently, several studies have discussed the importance of higher-order correlation of brain regions. Rosenthal et al.  show that the network context might not be directly accessible at the level of individual regions, and Santoro et al.  show the signatures of higher-order patterns in brain functional activity. Temporal hypergraph representation of the brain from neuroimaging data can address this limitation by capturing both higher-order interactions of brain regions as well as higher-order patterns that are correlated to emerging dynamics of brain activity. Hypergraphs are powerful paradigms to model higher-order interactions, where each connection, also known as hyperedge, can connect a group of nodes at once.

**Limitation of Previous Methods.** The process of brain network analysis generally unfolds in two sequential steps. Initially, brain networks are derived from individual neuroimage data (e.g., fMRI). This typically involves choosing a brain atlas, which identifies specific ROIs to serve as nodes, and edges that show association between ROIs. For example in fMRI data, from each designated brain region, fMRI blood-oxygen-level-dependent (BOLD) signal sequences are then extracted, and the subsequent phase of edge generation involves calculating pairwise connectivity between these nodes, often based on Pearson correlation and/or mutual information. Finally, the connectivity measures established between node pairs are utilized for the downstream tasks. Although this process along with diverse machine learning methods is widely used in the existing literature , both the pipeline and existing methods suffer from a subset of the following limitations, which makes directly applying them in real-world scenarios challenging and/or impractical:

1. Capturing linear correlation and ignoring temporal order: Most existing studies assume that the true dependency structure between brain regions is known prior to model training. That is, existing methods often use Pearson correlation and/or mutual information between the signals of brain regions (e.g, BOLD signals), while there is no statistical measure of dependency for truly capturing functional connectivity . Moreover, these statistical correlations focus on capturing linear correlation and ignore temporal order, which means shuffling the time in each time window does not change the results. In order to mitigate the above limitation and to ensure that the model can effectively learn meaningful network representations for use in downstream tasks it is crucial to establish an approach for constructing the dependency structure of the network that accurately reflects the underlying neuroimage data.

2. Missing higher-order dependencies: Existing studies assume that the dependencies of brain regions can be captured by pair-wise interactions, while the signatures of higher-order patterns in brain functional activity have been seen in recent studies . Moreover, the network context might not be directly accessible at the level of individual regions and requires considering higher-order interactions between a group of regions . Accordingly, ignoring the higher-order dependencies of ROIs might result in suboptimal performance.

3. Missing hierarchical structure of the brain: The human brain is comprised of functional systems (FS) , which are groups of ROIs that perform similar functions as an integrated network . These communities are essential in understanding the functional organization of the brain  as ROIs in the same functional modules tend to have similar behaviors and clustered representations . Most existing studies neglect the hierarchical structure of the brain, leading to missing the functional dependencies of ROIs in the same FS.

4. Missing the dynamics of the network: Some existing studies neglect the fact that the functional connectivity of the human brain dynamically changes over time, even in resting-state neuroimaging data . In task-dependent neuroimage data, subjects are asked to perform different tasks in different time windows, and the dynamics of the brain activity during these tasks play an important role in understanding neurological disease/disorder . Moreover, Liegeois et al.  shows that brain dynamics at different timescales capture distinct aspects. Accordingly, mitigating the above limitation requires a model that not only captures the dynamics of the brain activity but it also can capture its dynamics at different timescales.

5. Designed for classification tasks: Most existing studies have focused on semi-supervised/supervised classification tasks, e.g., detecting diseases , predicting biological features , or identifying functional systems . This setting not only relies their performance on the existence of labeled data, but it also limits their ability to reveal underlying patterns that might cause the symptoms of a disease or disorder. That is, a black-box classification method only predicts the label for a given brain network while understanding the cause of the brain disease/disorder (i.e., detecting abnormal brain activity) is a crucial step in facilitating early detection, and developing medical treatments.

**Contributions.** To address all the above limitations, we design HADiB (**H**ypergraph **A**nomaly **D**etection **i** in **B**rain), an end-to-end unsupervised anomaly detection method that can detect anomalous patterns at different level of granularity. HADiB first uses a novel hierarchical multivariate time-series encoder that can capture both cross-time and cross-ROI dependencies of ROIs' signals at different time scales. Next, to mitigate **I** and **2**, HADiB employs a temporal hypergraph constructor that learns the temporal higher-order dependencies between ROIs' signals. To address the **3** and **4**, HADiB uses a tetra-stage message passing at different levels of granularity to learn the ROI, functional system, and brain encodings, taking advantage of the hierarchical structure of the brain. Experimental evaluation supports the need for considering higher-order patterns and shows the superior performance of HADiB over baselines, as well as the importance of HADiB's critical components. Finally, using real-world case studies we show how HADiB can be used to detect abnormal brain regions or functional systems in a control group with brain disease or disorder.

## 2 Related Work

**Anomaly Detection in Hypergraphs and Time Series.** The problem of anomaly detection, which aims to detect abnormal nodes, edges, or subgraphs within a graph, has been extensively studied for both static and temporal graphs [25; 26]. Surprisingly, hypergraph anomaly detection is relatively unexplored. Park et al.  uses scan statistics to detect anomalous nodes. Leontjeva et al.  suggest taking advantage of the structural features of the nodes to detect anomalies. Recently, Lee et al.  designed a fast and effective algorithm based on a proximity matrix to detect abnormal nodes. Moreover, several studies use hypergraphs to learn higher-order patterns in time series forecasting [27; 30]. However, all these methods **I** are designed for general applications and cannot take advantage of special properties of the brain, **2** cannot detect anomalies at different levels of granularity (e.g., hyperedge, node, etc.).

**Machine Learning and Anomaly Detection in Brain Networks.** Several studies have recently analyzed brain networks to differentiate between healthy and diseased human brains [31; 32; 33]. With the success of graph neural networks in graph data analysis, deep learning models have been developed to predict brain diseases by studying brain network structures [34; 15; 35; 36; 11; 10]. However, these models focus on graph or node classification and aren't directly suited for anomaly detection. There are also methods aimed at detecting anomalies in brain regions or subgraphs, which could signal disease presence [4; 37; 38]. These studies adopt non-learning approaches and rely on pre-defined patterns or rules for detecting anomalies. Due to the complex nature of brain activity, it is unrealistic to assume that all abnormal brain activities follow the same pre-define pattern or rule. To address this limitation and to learn the abnormal patterns in the human brain from data, recently, Behrouz and Seltzer  proposed ADMire, an unsupervised anomaly detection method that uses multiplex random walks to extract networks motif and to detect anomalous patterns in the brains of people living with a disease or disorder, accordingly.

Figure 1: **Schematic of the HADiB. HADiB consists of three stages: (1) Feature Extraction, (2) Hypergraph Construction, and (3) Hypergraph Learning.**

All these methods target anomaly detection in brain networks with pairwise interactions, missing temporal higher-order patterns that play important roles in understanding the network context and its dynamics over time [13; 14]. Moreover, they do not take advantage of the hierarchical structure of the brain, missing the dependencies of ROIs' activity in the same functional systems [20; 21].

## 3 Methods

Each neuroimage data (e.g., fMRI or EEG) can be represented as a multivariate timeseries \(_{1:T}=(_{1},,_{T})^{n T}\), where \(n\) is the number of ROIs. Next, we formally define temporal hypergraphs:

**Definition 1** (Temporal Hypergraphs).: _A temporal hypergraph \(=(,,)\), can be represented as a sequence of snapshots \(=\{^{(t)}\}_{t=1}^{T}\) that arrive over time, where \(^{(t)}=(,^{(t)},^{(t)})\) is the \(t\)-th snapshot, \(\) is the set of nodes, \(e_{i}^{(t)} 2^{}\) are hyperedges, and \(^{(t)}^{|| f}\) is a matrix that encodes node attribute information for nodes in \(\). Note that we treat each hyperedge \(e_{i}\) as the set of all vertices connected by \(e_{i}\)._

Given a hypergraph \(=(,,)\), we represent the set of hyperedges attached to a node \(u\) before time \(t\) as \(^{(t)}(u)=\{(e,t^{})|t^{}<t,u e\}\). We say two hyperedges \(e\) and \(e^{}\) are adjacent if \(e e^{}\) and use \(^{t}(e)=\{(e^{},t^{})|t^{}<t,e^{} e\}\) to represent the set of hyperedges adjacent to \(e\) before time \(t\). Each hypergraph snapshot \(^{(t)}\) can be represented by an adjacency matrix \(^{(t)}\{0,1\}^{|||^{(t)}|}\) such that \(^{(t)}_{u,e}=1\) if \(u e\) and \(^{(t)}_{u,e}=0\) otherwise.

We focus on the problem of anomaly detection in the human brain at three levels of granularity: 1 Hyperedge anomaly detection, 2 ROI anomaly detection, and 3 FS/Brain anomaly detection.

### HADiB: An Unsupervised Anomaly Detection Method in Human Brain

In this section, we design HADiB, an unsupervised anomaly detection method for the human brain, and discuss each of its components in detail. Figure 1 illustrates the overview of HADiB architecture.

**Hierarchical Feature Extraction from Time Series.** The Time Mixer module aims to encode the multivariate time series data at different time scales. To this end, we first map the input sequence \(_{1:T}\) into a hierarchical series \(=\{^{(s)}\}_{s}\) such that \(_{1:T_{s}}^{(s)}=(_{1}^{(s)},,_{T_{s}}^{(s )})^{n T_{s}}\), \(\) is the set of all scales, and each \(s\) is a time window of \(^{(s)}\) (i.e., \(T_{s}=\)). At each time scale \(s\), we need to capture both cross-ROI and cross-time dependencies. Accordingly, inspired by Mlp-Mixer, we use a Time-Mixer and ROI-Mixer modules to capture cross-time and cross-ROI dependencies, respectively. In the Time-Mixer module, we have:

\[_{}^{(s)}=^{(s)}+_{}^{(1)^ {(s)}}(_{}^{2^{(s)}}( ^{(s)})),\]

where \(_{}^{(1)^{(s)}}\) and \(_{}^{(2)^{(s)}}\) are learnable metrics, \((.)\) is layer-normalization , and \((.)\) is a non-linearity (e.g., GeLU ). To capture cross-ROI dependencies, we use ROI-Mixer as follows:

\[_{}^{(s)} =_{}^{(s)}\] \[+((_{}^{(s)} )_{}^{(1)^{(s)}})_{}^{(2)^{ (s)}}, \]

where \(_{}^{(1)^{(s)}}\) and \(_{}^{(2)^{(s)}}\) are learnable metrics. Note that, while the feature extractor consists of simple all-MLP Time- and ROI-Mixer modules, its architecture (i.e., capturing cross-time and cross-ROI dependencies) make it a powerful model (see SS 4).

Finally, we project \(_{}^{(s)}\) to have the same size encoding for different time scales:

\[_{}^{(s)}=(_{}^{(s)} ). \]

The above procedure encodes the signals at different time scales separately. Accordingly, it does not take advantage the complementary information provided by different time scales . To address this challenge, we concatenate the \(_{}^{(s)}\) for different \(s\) in the order of the time scale, and use an Mlp-Mixer module  to combine the encodings:

\[_{}=(_{s}_{}^{(s)}), \]

where \(\) denotes concatination along the ROI dimension (i.e., \(_{}^{n d}\)).

**Temporal Hypergraph Construction.** To capture dependencies between brain regions, while considering the extracted temporal features, we use a self-attention mechanism to learn temporal hypergraph representation of the brain, where each node is a ROI and each hyperedge represents the association of a group of nodes. As discussed in SS1, the connectivity of the human brain dynamically changes over time . For example in task-dependent fMRI, subjects are asked to perform a task in each time window. To this end, we use \(s_{}=\{s\}\), as the time window to construct different snapshots of the network. Accordingly, using zero-padding, we use the above feature extraction procedure for \(_{1:(t+1)s_{}}\) to obtain \(_{}^{(t)}\) for all \(0 t}\). Next, to construct the \(t\)-th snapshot of the hypergraph, we use modified self-attention mechanism  as follows:

\[^{(t)}=(K^{(t)}}{}}}), \]

where \(Q^{(t)}=_{}^{(t)}_{}^{(1)}\), \(K^{(t)}=^{(t)}_{}^{}_{}^{(2)}\), and \(_{}^{(1)}\) and \(_{}^{(2)}\) are learnable matrices. We interpret \(^{(t)}\) as the adjacency matrix of the hypergraph representation of the brain in the \(t\)-th snapshot. Inspired by results reported by Said et al. , we use \(^{(t)}=_{}^{(t)}\) as the node features.

**Hypergraph Message-Passing.** After constructing the temporal hypergraph, to learn the dependencies between temporal patterns of different scales, we propose a tetra-stage message passing mechanism, which contains 1 ROI-ROI, 2 ROI-Hyperedge, 3 Hyperedge-FS, and 4 FS-FS.

1 ROI-ROI phase: To learn the local dependencies of ROIs, we iteratively aggregate messages from the local neighborhood of ROIs. Given \(t\)-th snapshot, if \(u\) and \(v\) are in a hyperedge \(e^{(t)}\), then we update ROI encodings in \(\)-th layer of neural network as follows:

\[m_{u v}^{()} =_{}^{()}(}_{u}^{(t)^{(-1)}},}_{v}^{(t)^{(-1)}},_{e}^{(t)^{( -1)}}),\] \[}_{u}^{(t)^{()}} =(\{m_{v u}^{()}|v^{(t)}(u) \})+}_{u}^{(t)^{(-1)}}, \]

where \(_{}^{()}\) is a learnable matrix, \(_{e}^{(t)^{(-1)}}\) is the encoding (strength) of hyperedge \(e\) (we discuss and compute it in 2), and \(^{(t)}(u)\) is the set of nodes that are connected to \(u\) by at least a hyperedge in \(t\)-th snapshot. We initialize the \(}_{u}^{(t)^{(0)}}=_{F}^{(t)}(u)\), where \(_{F}^{(t)}(u)\) is the corresponding row of matrix \(_{F}^{(t)}\) for \(u\).

3 ROI-Hyperedge phase: To obtain the encoding of a hyperedge, one can simply consider the summation of all ROI's encoding connected by the hyperedge. However, the strength of each dependencies for each ROI is different. Accordingly, we use the attention mechanism in Equation 4 to learn the importance of each hyperedge for each node (ROI) in the hypergraph. Let \(_{e}^{(t)}\) be the encoding of hyperedge \(e^{(t)}\):

\[_{e}^{(t)^{()}}=_{u e}_{u,e}^{(t)}}_ {u}^{(t)^{()}}, \]

where \(_{u,e}^{(t)}\) is computed by Equation 4.

4 Hyperedge-FS phase: We consider each FS as a set of ROIs. Accordingly, to encode an FS from its ROIs' encoding one might suggest aggregating the encoding of its ROIs. However, this aggregation misses the dependencies of ROIs in each FS. To this end, we propose a pooling function \((.)\) that aggregates the encodings of hyperedge within each FS. This approach not only incorporates the information from different ROIs, but also considers their dependencies and their roles in the dynamics of the system. Let \(\) be the set of all functional systems, for \(f\) we use \(_{f}\) to denote the set of hyperedges within the \(f\) and matrix \(_{f}^{(t)^{(t)}}=_{e_{f}}\ \ _{e}^{(t)^{(t)}}\). We compute the FS encoding as:

\[}_{f}^{(t)^{(t)}}=(_{f}^{(t)^{(t)}}, _{f}^{(t-1)^{(t)}})\]

\[}_{f}^{(t)^{(t)}}\!\!=\!}_{f}^{(t)^{(t)}}\!\! +(((}_{f }^{(t)^{(t)}})^{\!}))^{}\]

\[_{f}^{(t)^{(t)}}=(}_{f}^{(t)^{(t)}}+ ((}_{f}^{(t)^{(t)}}) _{P}^{(1)})_{P}^{(2)})\]

where \((.)\) is mean function along the hyperedge dimension. The above pooling method uses a similar architecture as the feature extractor to capture both cross-hyperedge and cross-feature dependencies. However, there are two main differences: First, to reduce the number of parameters, we bind features in a non-parametric manner using a \((.)\) function. Second, the first line, the Gru cell , is used to update the encodings over time, capturing the dynamics of the functional systems.

4) FS-FS phase: Finally, we perform structural learning at the level of functional systems to capture the dependencies of brain activity at a higher level and use it to obtain brain-level encoding. To this end, we assume that all functional systems are connected and use a self-attention mechanism  to learn the strength of the dependencies of functional systems. Given \(_{f}^{(t)^{(t)}}\) for each \(f\), let \(^{(t)^{(t)}}^{|| d}\) be the matrix whose rows are \(_{f}^{(t)^{(t)}}\). We use

\[_{}^{(t)}=(}^{( t)}K_{}^{(t)}}{}^{(T)^{}}}}), \]

where \(Q_{}^{(t)}=^{(t)^{(t)}}_{}^{(1)}\), \(K_{}^{(t)}=^{(t)^{(t)}}_{}^{(2)}\), and \(_{}^{(1)}\) and \(_{}^{(2)}\) are learnable matrices. Accordingly, we can obtain the brain-level encoding as the weighted aggregation of functional system encodings \(^{(t)^{(t)}}=_{f}_{f,f}^{(t)}_{f}^{(t )^{(t)}}\).

**Joint Training.** In the training phase, we want the model to learn to detect abnormal brain activities at the level of higher-order dependencies (i.e., hyperedges), ROIs (i.e., nodes), and function system or Brain. Accordingly, we use contrastive learning to train the model in an unsupervised manner. We generate negative samples at the level of hyperedges and nodes in the hypergraph. We adopt the commonly used negative sample generation method [25; 26] to generate negative hyperedges. That is, for each hyperedge \(e=\{u_{1},u_{2},,u_{k}\}\) we choose \(\)% of nodes connected by \(e\) and change them to a randomly selected set of vertices. We use binary cross-entropy loss, \(_{}\), to learn hyperedge encodings. To learn the time series encoding, we follow existing studies [46; 47] and replace a brain signal in the time window with another signal that is randomly selected from the batch. We also follow these studies and use the contrastive loss, \(_{}\) proposed by Woo et al. .

**Loss Function.** As we discussed earlier, ROIs in the same functional systems have similar patterns, and accordingly, it is expected to have similar encodings. To this end, inspired by DGI , we maximize the mutual information between ROIs encodings and their corresponding functional system encoding. We refer to this loss function as \(_{}\). Accordingly, we aim to minimize \(=_{1}_{}+_{2}_{ }-(1-_{1}-_{2})_{}\).

## 4 Experiments

**Datasets.** We use five real-world datasets: 1 PD  consists of the functional MRI images of 25 participants with and 21 participants without PD, who do the ANT task . 2 ADHD  contains data for 100 subjects in the ADHD group and 100 subjects in the typically developed (TD) control group. 3 The Seizure detection TUH-EEG dataset  consists of EEG data (31 channels) of 642 subjects. 4 ASD  contains data for 45 subjects in the ASD group and 45 subjects in the TD control group. 5 ABIDE  consists of resting-state functional MRI of 1009 subjects (516 with ASD) parcellated by Craddock 200 atlas. For the first part of the experiment, we follow the methodology used in existing studies [26; 55; 39] and inject 1% and 5% anomalous edges into the brain networks in the control group.

**Baselines.** We compare our HADiB with state-of-the-art methods. In all tasks, we use 

1 Graph-based methods: GOutlier , NetWalk, BrainGnn, BrainNetCnn, AD-Mire, and BNTransformer.

3 Hypergraph-based methods: HyperSAGCN, NHP, HashNWalk.

4 Time-series-based methods: Usad and Mvts. We may exclude some baselines in some tasks as they cannot be applied in that setting.

**Quantitative Results.** In the first experiment, we compare the performance of HADiB with baselines in hyperedge, ROI, and graph anomaly detection tasks. Table 1 reports the the AUC of HADiB and baselines. HADiB outperforms all the baselines in all three tasks. The reason is fourfold:

1 Graph-based methods can only capture the pair-wise dependencies of ROIs, missing higher-order dependencies.

2 Hypergraph-based methods also miss the temporal properties as well as the dynamics of the functional connectivity.

3 HADiB's training strategy and encoding of the brain at different level of granularity can provide complementary information.

4 HADiB is an end-to-end model that simultaneously learns the hypergraph structure and its element (e.g., node, hyperedge)

    &  &  &  &  &  \\   & Anomaly \% & 1\% & 5 \% & 1\% & 5 \% & 1\% & 5 \% & 1\% & 5 \% & 1\% & 5 \% \\    &  &  &  &  &  &  &  &  &  &  &  &  &  \\   & GOutlier & 61.24\({}_{1,41}\) & 59.98\({}_{8,23}\) & 64.08\({}_{1,33}\) & 63.23\({}_{1,41}\) & 63.23\({}_{1,41}\) & 63.27\({}_{1,23}\) & 64.10\({}_{2,42}\) & 65.63\({}_{1,41,33}\) & 64.12\({}_{2,44,33}\) & 60.80\({}

encodings. However, all the baselines use pre-computed functional connectivity, which limits their generalizability.

**Ablation Study.** We conduct ablation studies to validate the effectiveness of HADiB's critical components. Table 2 shows the AUC for the hyperedge anomaly detection task. The first row reports the performance of the complete HADiB implementation. Each subsequent row shows results for HADiB with one module modification: row 1 replaces feature extraction with Pearson's correlation, row 2 considers only one time scale, row 3 removes the self-attention mechanism, row 4 removes Gru cell in the pooling, rows 5-7 removes one of the loss functions at each time, rows 8 replaces the tetra stage messaging with a simple message passing, and finally, the last row converts the hypergraph to a graph. These results show that each component is critical for achieving HADiB's superior performance. The greatest contribution comes from the training process and loss functions, followed by Feature Extraction modules.

## 5 Case Studies

In the following case studies, we train our model on the healthy control group and then test it on the condition group. To this end, we report how anomalous brain activities found by HADiB are distributed in the brains of people living with PD/ADHD/ASD.

**Parkinson's Disease.** Figure 2 reports the distribution of anomalous ROIs within the PD group. A majority (95%) of the identified anomalies by HADiB involve ROIs located in one of the Left Thalamus, Supramarginal Gyrus, Superior Parietal, Medial Orbitofrontal, or Pars Opercularis. Interestingly, these ROIs are correlated with some PD symptoms (e.g., affected motor skills). Also, these results are consistent with previous studies using resting-state fMRI .

**Attention Deficit Hyperactivity Disorder.** Figure 2 reports the distribution of anomalous ROIs within the brain networks of the ADHD group. Most found abnormal ROIs (80% of all found anomalies) by HADiB are located in the Left Temporal Pole, Frontal Pole, Left Lateral Occipital, and Lingual Gyrus. These findings are consistent with previous studies on ADHD by using diffusion tensor imaging  and Forman-Ricci curvature changes .

**Autism Spectrum Disorder.** The distribution of anomalous ROIs within the brain networks of the ASD group is visualized in Figure 2. More than 80% of all found anomalies by HADiB involve ROIs located in the Right Cerebellum Cortex, Left Cerebellum Cortex, Right Cerebellum Cortex, Frontal Pole, Left Lateral Occipital, and Right Superior Temporal Gyrus. Our results on findings of abnormal activity in the cerebellum cortex are consistent with previous studies .

## 6 Conclusion

We present HADiB, an end-to-end unsupervised learning method on brain time series data to detect abnormal brain activity at different levels of granularity that might suggest a brain disease or disorder. HADiB uses a hierarchical multivariate time-series encoder to capture both cross-time and cross-ROI dependencies of ROIs' signals at different time scales and employs a temporal hypergraph constructor module on top of that to learn the temporal higher-order dependencies between ROIs. Using a novel hierarchical tetra-stage message passing on the constructed hypergraph, HADiB first learns ROI-level, hyperedge-level, and brain-level encodings and then leverages them to detect abnormal brain activity at different levels. Our experimental results show the superior performance of HADiB against baselines and the potential of HADiB in detecting abnormal brain activity.

Figure 2: The distribution of found anomalies by HADiB in condition groups.