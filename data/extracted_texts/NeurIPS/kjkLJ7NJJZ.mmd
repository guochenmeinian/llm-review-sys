# Offline Minimax Soft-Q-learning Under Realizability and Partial Coverage

Masatoshi Uehara

Genentech

uehara.masatoshi@gene.com

&Nathan Kallus

Cornell University

kallus@cornell.edu

&Jason D. Lee

Princeton University

jasonlee@princeton.edu

&Wen Sun

Cornell University

ws455@cornell.edu

This work was done at Cornell University

###### Abstract

In offline RL, we have no opportunity to explore so we must make assumptions that the data is sufficient to guide picking a good policy, and we want to make these assumptions as harmless as possible. In this work, we propose value-based algorithms for offline RL with PAC guarantees under just partial coverage, specifically, coverage of just a single comparator policy, and realizability of the soft (entropy-regularized) Q-function of the single policy and a related function defined as a saddle point of certain minimax optimization problem. This offers refined and generally more lax conditions for offline RL. We further show an analogous result for vanilla Q-functions under a soft margin condition. To attain these guarantees, we leverage novel minimax learning algorithms and analyses to accurately estimate either soft or vanilla Q-functions with strong \(L^{2}\)-convergence guarantees. Our algorithms' loss functions arise from casting the estimation problems as nonlinear convex optimization problems and Lagrangifying. Surprisingly we handle partial coverage even without explicitly enforcing pessimism.

## 1 Introduction

In offline Reinforcement Learning (RL), we must learn exclusively from offline data and are unable to actively interact with the environment (Levine et al., 2020). Offline RL has garnered considerable interest in a range of applications where experimentation may be prohibitively costly or risky.

Offline RL is generally based on two types of assumptions: sufficient coverage in the offline data and sufficient function approximation. For instance, classical Fitted-Q-iteration (Antos et al., 2008; Chen and Jiang, 2019) requires (a) full coverage in the offline data, \(_{(s,a)}d_{,_{0}}(s,a)/P_{_{b}}(s,a)<\) for any policy \(\) where \(P_{_{b}}(s,a)\) is the offline data's distribution on the states and actions and \(d_{,_{0}}(s,a)\) is the state-action occupancy distribution under a policy \(\) and initial-state distribution \(_{0}(s)\); (b) realizability of the \(Q^{*}\)-function in a hypothesis class; and (c) Bellman completeness, _i.e._, the Bellman operator applied to any function in the hypothesis class remains in the class. Full coverage (a) and Bellman completeness (c) can be particularly stringent because offline data is often insufficiently exploratory and Bellman completeness significantly restricts transition dynamics.

To overcome these challenges, we here propose algorithms with guarantees under realizability of single functions and refined partial coverage of single policies, and without Bellman completeness. We tackle this by introducing two novel value-based algorithms. The first algorithm, MSQP (mimimax soft-Q-learning with penalization), comprises of two steps: learning soft Q-functions (a.k.a., entropyregularized Q-functions, as defined in Fox et al., 2015; Schulman et al., 2017) from offline data, and using the softmax policies of the learned soft Q-functions. The second algorithm, MQP (minimax Q-learning with penalization), consists of two steps: learning standard Q-functions from offline data and employing the greedy policy of the learned Q function on the offline data.

Using the above-mentioned two algorithms, we attain PAC guarantees under partial coverage and realizability, yet without Bellman completeness. In particular, in MSQP using soft Q-functions, we ensure strong performance under the realizability of \(q_{}^{}\), \(l_{}^{}\) and the (density-ratio-based) partial coverage \(_{(s,a)}d_{^{}_{},_{0}}(s,a)/P_{b}(s,a)<\). Here \(q_{}^{}\) is a soft Q-function, \(l_{}^{}\) is a function that possesses a certain dual relation to \(q_{}^{}\), \(_{}^{}\) is the soft-max optimal policy, and \(\) is the temperature parameter for the entropy-regularization. Notably, \(_{(s,a)}d_{^{}_{},_{0}}(s,a)/P_{b}(s,a)<\) is significantly less stringent than the uniform coverage in that the coverage is only imposed against a policy \(_{}^{}\). In MQP using Q-functions, we similarly ensure strong performance under a soft margin, the realizability of \(q^{}\), \(l^{}\), and the partial coverage \(_{(s,a)}d_{^{},_{0}}(s,a)/P_{b}(s,a)<\). Here \(q^{}\) is the vanilla Q-function and \(l^{}\) is a function that possesses a certain dual relation to \(q^{}\), and \(^{}\) is the usual optimal policy. Note the soft margin is introduced to allow realizability on standard Q-functions rather than soft Q-functions. However, the conditions \(_{(s,a)}d_{^{}_{},_{0}}(s,a)/P_{b}(s,a)<\) or \(_{(s,a)}d_{^{},_{0}}(s,a)/P_{b}(s,a)<\) may still be strong as these marginal density ratios may not exist in large-scale MDPs. For example, this condition is easily violated when the initial distribution \(_{0}\) is not covered by \(P_{b}\) (_i.e._, \(_{s}_{0}(s)/P_{b}(s)=\) where \(P_{b}(s)_{a}P_{b}(s,a)\)). Therefore, as an additional innovation, in our algorithms we can further relax these density-ratio-based partial coverage conditions. Specifically, we can demonstrate results under a _refined partial coverage_, which is adaptive to Q-function classes, even when the initial distribution \(_{0}\) is not covered by \(P_{b}\).2

The primary challenge lies in the design of loss functions for effectively learning soft Q-functions and vanilla Q-functions from offline data _without Bellman completness_. To tackle this, we devise new minimax loss functions with certain regularization terms to achieve favorable \(L^{2}\)-convergence rates on the offline data (_i.e._, in terms of \(_{(s,a) P_{b}}[\{_{}-q\}^{2}(s,a)]\) given an estimator \(\)). This result serves as the key building block for obtaining refined partial coverage under realizability and is of independent interest in its own right. Existing results are often constrained to specific models, such as linear models (Shi et al., 2022), or they require Bellman completeness (Antos et al., 2008; Chen and Qi, 2022; Chen and Jiang, 2019). In contrast, our guarantee is applicable to any function

    \\  Jiang and Huang (2020) & \(w^{},\;q_{}\;\) \\ Xie et al. (2021) & \(q_{},\;^{}\; \) \\ Zhan et al. (2022) & \(_{}^{},\;w_{}^{}\) \\  MSQP & \(q_{}^{},\;l_{}^{}\) \\ Chen and Jiang (2022) & Hard margin, \(w^{},\;q^{}\) \\ MQP & Soft margin, \(q^{},\;l^{}\) \\   

Table 1: Summary of partial-coverage-type guarantees with model-free general function approximation. Here, \(w^{} d_{^{},_{0}}/P_{b}\) where \(d_{^{},_{0}}\) is the occupancy distribution under the optimal policy \(^{}\) starting from \(_{0}\) and \(P_{b}\) is the distribution over the offline data. A function \(_{}^{}\) is a regularized marginal density ratio that satisfies \(_{0}^{}=w^{}\). Functions \(q^{},q_{}^{},q_{}\) are the optimal \(Q^{}\)-function, the soft Q-function, and the Q-function under a policy \(\), respectively. Functions \(v_{}^{},l_{}^{}\) are Lagrange multipliers of specific minimax optimization problems. The operator \(^{}\) is a Bellman operator under a policy \(\). Function classes \(,,,\) consist of functions that map states (and actions) to real numbers. Note the guarantees provided by Jiang and Huang (2020); Xie et al. (2021) are more general than the below in that the output policy can compete with any policy in the policy class \(\). For simplicity, we set the comparator policy to be the optimal policy \(^{}\) in this table. Note that other studies (Ozdaglar et al., 2023; Rashidinejad et al., 2022; Zhu et al., 2023) proposing model-free general function approximation under partial coverage rely on the completeness-type assumption as in (Xie et al., 2021) or realizability for any \(\) as in Jiang and Huang (2020).

approximation method, without the need for Bellman completeness. To the best of our knowledge, this is the first guarantee of its kind.

Our work exhibits marked improvements over two closely related studies (Zhan et al., 2022; Chen and Jiang, 2022). Similar to our work, they propose algorithms that operate under the realizability of specific functions and partial coverage, yet without Bellman completeness. Zhan et al. (2022) ensures a PAC guarantee under (a') partial coverage in the offline data \(_{(s,a)}d_{_{}^{*},_{0}}(s,a)/P_{b}(s,a)<\) where \(_{}^{*}\) is a specific near-optimal policy under the regularization, which differs from the soft optimal policy, and (b') realizability of \(d_{_{}^{*},_{0}}/P_{b}\) and the regularized value function. However, unlike MSQP, it is unclear how to refine the abovementioned coverage, _i.e._, the guarantee could be vacuous when the initial distribution is not covered by offline data. A similar guarantee, but without regularization, is obtained under the additional hard margin (a.k.a., gap) condition in Chen and Jiang (2022). Our soft margin is a strict relaxation of the hard margin, which is important because, unlike the soft margin, the hard margin generally does not hold in continuous state spaces and involves very large constants in discrete state spaces. Lastly, although Chen and Jiang (2022); Zhan et al. (2022) use completely different algorithms and attain guarantees for regularized value-functions and non-regularized value functions, respectively, our guarantee can afford guarantees for regularized and non-regularized value-functions in a _unified_ manner since MQP can be seen as a limit of MSQP when \(\) goes to \(0\).

Our contributions are summarized below and in Table 1.

1. We establish that the optimal policy can be learned under partial coverage and realizability of the optimal soft Q-function and its dual. Notably, we abstain from the use of possibly stronger conditions in offline RL, such as full coverage, Bellman completeness, and uniform realizability over the policy class (such as \(q_{}\) for any \(\) as in Jiang and Huang, 2020). In particular, while a similar guarantee is provided in Zhan et al. (2022), our partial coverage guarantee has an advantage in that we are able to potentially accommodate scenarios where the initial distribution is not covered by \(P_{b}\). This is feasible because our algorithm is value-based in nature, which allows us to leverage the structure of the Q-function classes and refine the coverage condition.
2. We demonstrate that the optimal policy can be learned under partial coverage, realizability of the Q-function and its dual, and a soft margin. While a similar guarantee is obtained in Chen and Jiang (2022), our guarantee has the advantage that the soft margin is significantly less stringent than the hard margin required therein.

### Related Works

We summarize related works as follows. Further related works is discussed in Section A.

Offline RL under partial coverage.There is a growing number of results under partial coverage following the principle of pessimism in offline RL (Yu et al., 2020; Kidambi et al., 2020). In comparison to works that focus on tabular (Rashidinejad et al., 2021; Li et al., 2022; Shi et al., 2022; Yin and Wang, 2021) or linear models (Jin et al., 2020; Chang et al., 2021; Zhang et al., 2022; Nguyen-Tang et al., 2022; Bai et al., 2022), our emphasis is on general function approximation (Jiang and Huang, 2020; Uehara and Sun, 2021; Xie et al., 2021; Zhan et al., 2022; Zhu et al., 2023; Rashidinejad et al., 2022; Zanette and Wainwright, 2022; Ozdaglar et al., 2023). Among them, we specifically focus on model-free methods. The representative work is summarized in Table 1.

Soft (entropy-regularized) Q-functions.Soft Q-functions are utilized in various contexts in RL (Geist et al., 2019; Neu et al., 2017). They have been shown to improve performance in online RL settings, as demonstrated in Soft Q-Learning (Fox et al., 2015; Schulman et al., 2017) and Soft Actor critic (Haarnoja et al., 2018). In the field of imitation learning, they play a crucial role in Maximum Entropy IRL (Ziebart et al., 2008, 2010). Furthermore, within the realm of offline RL, these soft Q-functions are utilized to make the learned policy and behavior policy sufficiently similar (Wu et al., 2019; Fakoor et al., 2021). However, to the best of the authors' knowledge, none of these proposals in the context of offline RL have provided sample complexity results under partial coverage.

Lagrangian view of offline RL.In the realm of offline policy evaluation (OPE), Nachum and Dai (2020); Yang et al. (2020); Huang and Jiang (2022) have formulated the problem as a constrained linear optimization problem. Notably, within the context of policy optimization, Zhan et al. (2022) have proposed estimators for regularized density ratios with \(L^{2}\)-convergence guarantees, which is a crucial step in obtaining a near-optimal policy. Our work is similarly motivated, but with a key distinction: our target functions are the soft Q-function and Q-function, rather than the regularized density ratio, which presents additional analytical challenges due to the nonlinear constraint.

## 2 Preliminaries

We consider an infinite-horizon discounted MDP \(=,,P,r,,_{0}\) where \(\) is the state space, \(\) is the finite action space, \([0,1)\) is the discount factor, reward \(r\) is a random variable following \(P_{r}( s,a)\) on \([R_{},R_{}]\) (\(R_{} 0\)), \(_{0}\) is the initial distribution. A policy \(:()\) is a map from the state to the distribution over actions. We denote the discounted state-action occupancy distribution under a policy \(\) starting from an initial distribution \(_{0}\) by \(d_{,_{0}}(s,a)\). With slight abuse of notation, we denote \(d_{,_{0}}(s)=_{a}d_{,_{0}}(s,a)\). We define the value under \(\) as \(J()_{}[_{t=0}^{}^{t}(s_{t},a_{t})]\) where the expectation is taken under \(\). We denote the optimal policy \(_{}J()\) by \(^{}\), and its Q-function \(_{^{}}[_{t}^{t}(s_{t},a_{t}) s_{0}=s,a_{0}=a]\) by \(q^{}(s,a)\).

In offline RL, using offline data \(=\{(s_{i},a_{i},r_{i},s^{}_{i}):i=1,,n\}\), we search for the policy \(^{}\) that maximizes the policy value. We suppose each \((s_{i},a_{i},r_{i},s^{}_{i})\) is sampled i.i.d. from \(s_{i} P_{b},a_{i}_{b}( s),r_{i} P_{r}( s_{i}, a_{i}),s^{}_{i} P( s_{i},a_{i})\). We denote the sample average of \(f\) by \(_{n}[f(s,a,r,s^{})]=_{i=1}^{n}f(s_{i},a_{i},r_{i },s^{}_{i})\), and the expectation of \(f\) with respect to the offline data distribution by \([f(s,a,r,s^{})]\) (without any scripts). The policy \(_{b}\) used to collect data is typically referred to as a behavior policy. With slight abuse of notation, we denote \(P_{b}(s,a)=P_{b}(s)_{b}(a s)\).

**Notation.** We denote the support of \(P_{b}()\) by \(()_{b}\), and the \(L^{}\)-norm on \(()_{b}\) by \(\|\|_{,b}\). The \(L^{}\)-norm on \(()\) is denoted by \(\|\|_{}\). We define \(w_{}(s,a)=d_{,_{0}}(s,a)/P_{b}(s,a)\) (if it exists). We define \((h)=(h(s,a))}\) and \(\|h\|_{2}=_{(s,a) P_{b}}[h^{2}(s,a)]^{1/2}\) for \(h:\). We denote universal constants by \(c_{1},c_{2},\). We use the convention \(a/0=\) when \(a 0\) and \(0/0=0\).

## 3 Algorithms

In this section, we present two algorithms. The first algorithm aims to estimate the soft optimal policy by first estimating a soft Q-function. The second algorithm estimates the optimal policy after estimating the Q-function.

### Minimax Soft-Q-learning with Penalization

Our ultimate aim is to mimic the optimal policy \(^{}\). As a first step, we begin by finding a policy that maximizes the following regularized objective: \(_{}J_{}()\) where for \(>0\) we define

\[J_{}()=(1-)^{-1}_{(s,a) d_{,_{0}},r P_ {r}( s,a)}[r-(a s)\} }_{$)}}]\]

This objective function is used in a variety of contexts in RL as mentioned in Section 1.1. The optimal policy that maximizes \(J_{}()\) with respect to \(\) is

\[^{}_{}=(q^{}_{}/+_{b}),\] (1)

where \(q^{}_{}:\) is the soft Q-function uniquely characterized by the soft Bellman equation:

\[(s,a);_{s^{} P( s,a)}[_{ ,_{b}}(q^{}_{})(s^{})+r-q^{}_{}(s,a)  s,a]=0,\]

where \(_{,_{b}}:[] []\) has \(_{,_{b}}(q)(s)=_{a}\{(q(s,a^{})/ )_{b}(a^{} s)\}\). As opposed to the standard objective function with \(=0\), the KL penalty term serves as a regularization term that renders \(^{}_{}\) sufficiently proximate to \(_{b}\). As \(\) approaches \(\), the optimal policy \(^{}_{}\) approaches \(_{b}\). On the other hand, when \(=0\), \(^{}_{}\) is \(^{}\). Thus, in order to compete with \(^{}\), it is necessary to keep \(\) sufficiently small. We elaborate on this selection procedure in Section 5.

The natural method for offline RL using this formulation involves learning \(q^{}_{}\) from the offline data and plugging it into (1). The question that remains is how to accurately learn \(q^{}_{}\) from the offline data. We consider the following optimization problem:

\[_{q^{}}0.5_{(s,a) P_{b}}[q^{2}(s,a)]\] (2)where \(^{}\) consists of all functions \(q:\) satisfying

\[_{s^{} P(|s,a)}[_{,_{b}}(q)(s^{ })+r-q(s,a) s,a]=0\ \ \ (s,a)()_{b}.\] (3)

Here, because of the constraint (3), the solution is \(q_{}^{}\). Furthermore, we use \(q^{2}(s,a)\) in (2) because this choice relaxes the equality in (3) to an inequality \( 0\) as we will demonstrate in Section B. Consequently, the entire optimization problem outlined in (2) and (3) transforms into a convex optimization problem.

Then, using the method of Lagrange multipliers, (2) is transformed into

\[_{q}_{l}L_{}(q,l), L_{}(q,l):=[q^{2}(s,a)/2+\{_{,_{b}}(q)(s^{})+r-q(s,a)\}l(s,a)].\] (4)

Being motivated by the above formulation, our MSQP algorithm, specified in Algorithm 1, approximates this formulation by replacing expectations with sample averages and restricting optimization to function classes with bounded complexity.

**Remark 1** (Computation).: _Although minimax optimization is generally difficult to solve, it is computationally feasible when we choose RKHS or linear function classes for \(\). In this case, we can solve the inner maximization problem analytically in closed form, as the objective function is linear in \(l\). As a result, the minimax optimization problem reduces to empirical risk minimization._

```
1:Require: Parameter \(^{+}\), Models \(,[^{+}]\).
2:Estimate \(q^{}\) as follows: \[_{}*{arg\,min}_{q}_{l }_{n}[q^{2}(s,a)/2+\{_{,_{b}}(q)(s^{ })+r-q(s,a)\}l(s,a)].\] (5)
3:Estimate the soft optimal policy: \(_{}=*{softmax}(_{}/+_{b})\). ```

**Algorithm 1** MSQP (Minimax Soft-Q-learning with Penalization)

### Minimax \(Q^{}\)-learning with Penalization

Next, we examine a policy learning algorithm utilizing \(Q^{}\)-functions. To learn \(Q^{}\), our objective function is derived from the constrained optimization problem:

\[*{arg\,min}_{q^{}}\,0.5_{(s,a) P _{b}}[q^{2}(s,a)]\] (6)

where \(^{^{}}\) consists of all functions \(q:\) satisfying

\[(s,a)()_{b};_{s^{} P (|s,a)}[_{a^{}}q(s^{},a^{})+r- q(s,a) s,a]=0.\]

Next, again using the method of Lagrange multipliers, (6) is transformed into

\[_{q}_{l}L_{0}(q,l),\,L_{0}(q,l):=[q^{2}(s,a)/2+\{ _{a^{}}q(s^{},a^{})+r-q(s,a)\}l(s,a)].\] (7)

Note \(L_{0}\) is the limit of \(L_{}\) as \( 0\).

Our MQP algorithm, specified in Algorithm 2, similarly approximates this formulation by replacing expectations with sample averages and restricting optimization to function classes with bounded complexity. Our final policy is greedy with respect to the learned Q-function but restricting to the support of the offline data in order to avoid exploiting regions not covered by the offline data.

**Remark 2** (Prominent differences).: _There exist several other minimax estimators for \(Q^{}\) including BRM (Antos et al., 2008) and MABO (Xie and Jiang, 2020). Although these ensure convergence guarantees in terms of Bellman residual errors, they do not ensure the guarantee in terms of \(L^{2}\)-errors, which is our focus. Our minimax objective function differs significantly from that of the aforementioned approaches, and its unique design plays a pivotal role in enabling \(L^{2}\)-rates._

## 4 \(L^{2}\)-convergence Rates for Soft \(Q\)-functions and \(Q^{*}\)-functions

To analyze our Q-estimators we first establish conditions that ensure \(q^{*}_{}=_{q}_{l}L_{}(q,l)\) on the support \(()_{b}\). Building on this, we prove \(L^{2}\)-convergence rates for \(_{}\) and \(_{0}\). These \(L^{2}\)-convergence guarantees are subsequently translated into performance guarantees of the policies we output in Section5.

### Identification of Soft Q-functions

Consider an \(L^{2}\)-space \(\) where the inner product is define as \( h_{1},h_{2}=_{(s,a) P_{b}}[h_{1}(s,a)h_{2}(s,a)]\). Then we define two operators and a key function:3

\[P^{*}_{}: f_{s^{}  P(s,a),a^{}^{*}_{}}[f(s^{},a^{})(s,a) =],\] \[\{P^{*}_{}\}^{}: f P(  s,a)^{*}_{}()f(s,a)(s,a),\] \[l^{*}_{}(s,a)_ {}\}^{})^{-1}(P_{b}(s,a)q^{*}_{}(s,a))}{P_{b}(s,a)}&(s,a)( )_{b},\\ 0&(s,a)()_{b}.\]

These satisfy a key adjoint property, which we leverage to show \((q^{*}_{},l^{*}_{})\) is a saddle point of \(L_{}(q,l)\).

**Lemma 1**.: \( q\)_, we have \( l^{*}_{},(I- P^{*}_{})q_{}= q ^{*}_{},q_{}\)._

Our first assumption ensures that \(l^{*}_{}\) exists.

**Assumption 1**.: _Suppose \(\|d_{^{*}_{},P_{b}}/P_{b}\|_{}<\). Note the infinity norm \(\|\|_{}\) is over \(\)._

**Proposition 1**.: _Under Assumption 1, we have \(\|l^{*}_{}\|_{}<\)._

Proposition 1 is immediate noting that \((I-\{P^{*}_{}\}^{})^{-1}(P_{b}()q^{*}_{}())= _{t=0}^{}^{t}(\{P^{*}_{}\}^{})^{t}(P_{b}q^{*}_{})\) and recalling the discounted occupancy measure under \(^{*}_{}\) with initial distribution \(_{0}\) is written as \(d_{^{*}_{},_{0}}=(1-)(I-\{P^{*}_{}\}^{})^{-1 }(_{0})\). Hence, \(\|l^{*}_{}\|_{}(1-)^{-1}R_{}\|d_{^{*}_{},P_ {b}}/P_{b}\|_{}\). Note that \(\|d_{^{*}_{},P_{b}}/P_{b}\|_{}\) crucially differs with the standard density-ratio-based concentrability coefficient \(\|d_{^{*}_{},_{0}}/P_{b}\|_{}\) in offline RL. Unlike \(\|d_{^{*}_{},P_{b}}/P_{b}\|_{}\), the value of \(\|d_{^{*}_{},_{0}}/P_{b}\|_{}\) can be infinite when the initial distribution \(_{0}\) is not covered by offline data \(P_{b}\) as the practical motivating example is explained in the footnote in Section1 and Example2.

Our next assumption ensures \(q^{*}_{} 0\), which also guarantees that \(l^{*}_{} 0\).

**Assumption 2**.: _Suppose \(\|^{*}_{}/_{b}\|_{} R_{}\)._

Assumption 2 can be satisfied by rescaling reward (_i.e._, rescaling \(R_{}\)) as long as \(\|^{*}_{}/_{b}\|_{}\) is finite. Hence, it is very mild. Putting Lemma1 together with our assumptions we have the following.

**Lemma 2**.: _Suppose Assumptions 1 and 2 hold. Then, \((q^{*}_{},l^{*}_{})\) is a saddle point of \(L_{}(q,l)\) over \(q,l\), i.e., \(L_{}(q,l^{*}_{}) L_{}(q^{*}_{},l^{*}_{}) L _{}(q^{*}_{},l)\; q, l\)._

Recall that a point \((,)\) is a saddle point if and only if the strong duality holds, and \(_{q}_{l}L_{}(q,l), _{l}_{q}L_{}(q,l)\) using the general characterization (Bertsekas, 2009). Hence, Lemma2 ensures \(q^{*}_{}_{q}_{l}L_{}(q,l)\).

Next, we consider the constrained optimization problem when we use function classes \(,\). As long as the saddle point is included in \((,)\), we can prove that \(q^{*}_{}\) is a unique minimaxer.

**Lemma 3**.: _Suppose Assumptions 1 and 2 hold, \(q^{*}_{}\), and \(l^{*}_{}\). Then, we have that \(q^{*}_{}=_{q}_{l}L_{}(q,l)\) on the support \(()_{b}\)._

This establishes that realizability (\(q^{*}_{}\), \(l^{*}_{}\)) is sufficient to identify \(q^{*}_{}\) on the offline data distribution. At a high level, \(q^{*}_{}_{q}_{l}L_{}(q,l)\) is established through the invariance of saddle points, _i.e._, saddle points over original sets remain saddle points over restricted sets. Its uniqueness is verified by the strong convexity in \(q\) of \(L_{}(q,l)\) induced by \(_{(s,a) P_{b}}[q^{2}(s,a)]\).

### \(L^{2}\)-convergence Rate for Soft Q-estimators

Based on the population-level results in Section4.1, we give a finite-sample error analysis of \(_{}\)

**Assumption 3** (Realizability of soft Q-function).: _Suppose \(q^{*}_{}\) and \(\|q\|_{} B_{}\, q\)._

**Assumption 4** (Realizability of Lagrange multiplier).: _Suppose \(l^{}_{}\) and \(\|l\|_{} B_{}\, l\)._

It is natural to set \(B_{}=(1-)^{-1}R_{}\) and \(B_{}=(1-)^{-1}R_{}\|d_{^{*}_{},P_{b}}/P_{b}\|_{}\), but letting these be arbitrary offers further flexibility to our results.

**Theorem 1** (\(L^{2}\)-convergence of soft Q-estimators).: _Suppose Assumptions 1, 2, 3, and 4 hold. Then, with probability \(1-\), the \(L^{2}\)-error \(\|_{}-q^{*}_{}\|_{2}\) is upper-bounded by_

\[c(_{}^{2}+_{}_{ }\{+(||)\})^{1/2}((||| |/)/n)^{1/4}.\]

Our result is significant as it relies on realizability-type conditions rather than Bellman closedness. Since the majority of existing works focus on non-regularized Q-functions, we postpone the comparison to these existing works to the next section. Note when \(\) and \(\) are infinite, we can easily replace \(||,||\) with their \(L^{}\)-covering numbers following Uehara et al. (2021). Details are given in the appendix.

### \(L^{2}\)-convergence Rate for \(Q^{}\)-functions

Next, we give analogous finite-sample error analysis of \(_{0}\) leveraging the same reasoning.

**Assumption 5** (Realizability of \(Q^{*}\)-functions).: _Suppose \(q^{*}\) and \(\|q\|_{} B_{}\, q\)._

Next, we define the Lagrange multiplier:

\[\{P^{}\}^{}: f P( s,a)^{}( )f(s,a)(s,a),\]

\[l^{}\{(I-\{P^{}\}^{})^{-1}(q^{*}P_{_{b}})\}/ P_{_{b}}.\]

While \(l^{}\) involves the density ratio, this is always well-defined as long as \(\|d_{^{*},P_{b}}/P_{b}\|_{}<\).

Then, it can be similarly established that \((q^{*},l^{})\) is a saddle point of \(L_{0}(q,l)\) over \(q,l\) as we show in Lemma2. We lastly require its realizability.

**Assumption 6** (Realizability of Lagrange multiplier).: _Suppose \(\|d_{^{*},P_{b}}/P_{b}\|_{}<\) and \(l^{}\). Further suppose \(\|l\|_{} B_{}\, l\)._

**Theorem 2** (\(L^{2}\)-convergence of Q-estimators).: _Suppose Assumptions 5 and 6 hold. Then, with probability \(1-\), the \(L^{2}\)-error \(\|_{0}-q^{}\|_{2}\) is upper-bounded by_

\[c(_{}^{2}+_{}_{ })^{1/2}((||||/)/n)^{1/4}.\]

To the best of our knowledge, this is the first guarantee on \(L^{2}\) errors for learning \(q^{*}\) using _general function approximation without relying on Bellman completeness_. This is highly nontrivial, and we have carefully crafted our algorithm to obtain this guarantee. Existing results are often specific to particular models, such as linear models (Shi et al., 2022), or they require Bellman completeness (Chen and Jiang, 2019; Chen and Qi, 2022), or they are limited to offline policy evaluation scenarios (Huang and Jiang, 2022) (_i.e._, cases involving linear Bellman operators, but nonlinear Bellman operators). Actually, it seems that even under the assumption of Bellman completeness, obtaining an L2 guarantee _without strong coverage assumptions_ remains unclear. A detailed comparison among these different approaches is presented in SectionA.

## 5 Finite Sample Guarantee of MSQP

In this section, we present our primary sample complexity guarantee for our MSQP algorithm under the assumptions of realizability of \(q^{*}_{}\) and \(l^{}_{}\) and partial coverage. We first show the learned policy \(_{}\) can compete with \(^{*}_{}\). Finally we show \(_{}\) can compete with \(^{*}\) by selecting \(\) properly.

We first introduce the flattened behavior policy \(^{}_{b}\), which is uniform on the support of \(_{b}\). We use it as a technical device to define a model-free concentrability coefficient following Xie et al. (2021).

**Definition 1** (Model-free concentrability coefficient).: _Define_

\[C_{,d_{^{*}_{},_{0}}}_{q}_{s d_{^{*}_{},_{0}},a^{*}_{}(a|s)}[\|q(s,a)-q^{*}_{}(s,a)\|^{2}_{2}]}{_{(s,a) P_{0}}[\|q(s,a)-q^{*}_ {}(s,a)\|^{2}_{2}]}\]

_where \(^{}_{b}( s)=0&_{b}( s)=0\\ 1/|\{a_{b}(a s)>0\}|&_{b}( s)>0\) is the flattened behavior policy._

Clearly, \(C_{,d_{^{*}_{},_{0}}}\) is smaller than density-ratio-based concentrability coefficient, in other words,

\[C_{,d_{^{*}_{},_{0}}}_{(s,a)}_ {},_{0}}(s)^{}_{b}(a s)}{P_{b}(s)_{b}(a s)}.\]

Here, we always have \(\|^{}_{b}/_{b}\|<\) even if \(_{b}(a s)\) is \(0\) for some \((s,a)\). In the special case where \(_{b}(a s) 1/C^{}\) for any \((s,a)\), we have \(C_{,d_{^{*}_{},_{0}}} C^{}\|d_{^{*}_{ },_{0}}/P_{b}\|_{}\). The coefficient \(C_{,d_{^{*}_{},_{0}}}\) is is a refined concentrability coefficient, which adapts to a function class \(\). For example, in linear MDPs, it reduces to a relative condition number as follows. Similar properties are obtained in related works (Xie et al., 2021; Uehara and Sun, 2021).

**Example 1** (Linear MDPs).: _A linear MDP is one such that, for a known feature vector \(:^{d}\), the true density satisfies \(P(s^{} s,a)=^{}(s^{}),(s,a)\) for some \(^{}:^{d}\) and the reward function satisfies \([r s,a]=_{r},(s,a)\) for some \(_{r}^{d}\)._

_In linear MDPs, \(q^{}_{}\) is clearly linear in \((s,a)\). Hence, the natural function class is \(=\{,(s,a)\|\| B\}\) for a certain \(B^{+}\). Then, we have_

\[C_{,d_{^{*}_{},_{0}}}=_{x 0} _{s d_{^{*}_{},_{0}},a^{*}_{}(a|s)}[ (s,a)(s,a)^{}]x}{x^{}_{(s,a) P_{0}}[(s,a) (s,a)^{}]x}.\]

We are now prepared to present our main result, which states that given the realizability of the soft Q-function \(q^{}_{}\) and Lagrange multiplier \(l^{}_{}\), it is possible to compete with \(^{}_{}\) under the coverage condition \(C_{,d_{^{*}_{},_{0}}}<,\|d_{^{*}_{},P_{0 }}/P_{b}\|_{}<\).

**Theorem 3** (\(_{}\) can compete with \(^{}_{}\)).: _Fix \(>0\). Suppose Assumptions 1, 2, 3, and 4 hold. With probability \(1-\), the regret \(J(^{}_{})-J(_{})\) is upper-bounded by_

\[n^{-1/4}(||,_{},_{},C_{,d_{^{*}_{},_{0}}},(|||}{}),,R_{})\]

The proof mainly consists of two steps: (1) obtaining \(L^{2}\)-errors of \(_{}\) as previously demonstrated in Theorem 1, (2) translating this error into the error of \(_{}\). In the second step, the Lipshitz continuity of the softmax function plays a crucial role. If there is no regularization (\(=0\)) and the greedy policy of \(q^{}_{0}\) is utilized, the second step does not proceed (without any further additional assumptions).

Our ultimate goal is to compete with \(^{}\). Theorem 3 serves as the primary foundation for this goal. The remaining task is to analyze the approximation error \(J(^{})-J(^{}_{})\). Fortunately, this term can be controlled through \(\) and the density ratio between \(^{}\) and \(_{b}\). Then, by properly controlling \(\), we can obtain the following sample complexity result.

**Theorem 4** (PAC guarantee of \(_{}\)).: _Fix any \(>0\). Suppose Assumptions 1, 2, 3, and 4 hold for \(=c/n^{1/8}\) and \(\|^{}_{0}/_{b}\|_{} C_{0}\), \(C_{,d_{^{*}_{},_{0}}}<\). Then, if \(n\) is at least_

\[^{-8}(||,_{},_ {},C_{,d_{^{*}_{},_{0}}},(||| |/),(1-)^{-1},(C_{0}),R_{}),\]

_with probability at least \(1-\), we can ensure \(J(^{})-J(_{})\)._

In summary, the realizability of \(q^{}_{},l^{}_{}\), per-step coverage \(\|^{}_{0}/_{b}\|_{}<\) and partial coverage \(C_{,d_{^{*}_{},_{0}}}<\), \(\|d_{^{*}_{},P_{b}}/P_{b}\|_{}<\) are sufficient to compete with \(^{}\). This is a novel and attractive result. Firstly, if we solely use the naive FQI or Bellman residual minimization, existing PAC results require the global coverage \(\|d_{,_{0}}/P_{b}\|_{}<\) for any possible policy \(\)(Munos and Szepesvari, 2008; Antos et al., 2008). Our result only requires coverage under a single policy \(^{}_{}\) (near-optimal policy). Secondly, we only require the realizability of two functions, and we do not necessitaterealizability-type conditions for all policies in the policy class or Bellman completeness, unlike existing works with partial coverage (Xie et al., 2021; Jiang and Huang, 2020).

The most similar result is Zhan et al. (2022). However, our guarantee possesses a certain advantage over their guarantee as follows. They demonstrate the realizability of certain functions \(_{}^{},v_{}^{}\) and partial coverage \(\|d_{_{},_{0}}/P_{b}\|<\) are sufficient conditions in offline RL, where \(_{}^{}=d_{_{},_{0}}/P_{b}\) (\(_{}\) is a certain regularized optimal policy, but fundamentally distinct from \(_{}^{}\)) and \(v_{}^{}\) is a near-optimal regularized value function parameterized by \(\). Here, we have \(_{0}^{}=w^{},v_{0}^{}=v^{}\). Our guarantee has a similar flavor in the sense that it roughly illustrates realizability and partial coverage are sufficiently different conditions. However, the meanings of realizability and partial coverage are significantly different. In particular, by employing our algorithm, we can ensure PAC guarantees under the boundedness of the refined concentrability coefficient \(C_{,d_{_{}^{},_{0}}}<\) (and \(\|d_{_{}^{},P_{b}}/P_{b}\|_{}\) through \(_{}\)). As a result, the \(L^{}\)-norm of the density-ratio-based concentrability coefficient \(\|d_{_{}^{},_{0}}/P_{b}\|_{}\) can even be infinite. More specifically, we can permit situations where \(_{s}_{0}(s)/P_{b}(s)=\) as we will see the practical example soon. Conversely, Zhan et al. (2022) excludes this possibility since the algorithm explicitly estimates the density ratio \(_{}^{}\).

**Example 2** (Contextual bandit under external validity).: _We consider the contextual bandit setting where we want to optimize \(J()=_{s_{0},a(s),r P_{s}(s,a)}[r]\) using offline data \(s P_{b},a_{b}(s),r P(s,a)\). This is the simplest RL setting with \(=0\). Here, note \(_{0}\) could be different from \(P_{b}\). This case often happens in practice as discussed in the literature on causal inference related to external validity (Pearl and Bareinboim, 2014; Dahabreh et al., 2019; Uehara et al., 2020), which refers to the shift between the target population and the offline data. Here, our PAC guarantee does not require that \(_{0}(s)\) is covered by \(P_{b}(s)\) in terms of the density ratio as long as the relative condition number is upper-bounded when we use linear models. On the other hand, Zhan et al. (2022) excludes this possibility._

Despite the aforementioned advantage of our approach, unfortunately, our sample complexity of \(O(1/^{8})\) is slower compared to that of \(O(1/^{6})\) in Zhan et al. (2022). In the following, we demonstrate that MQP, which is a special version of MSQP when \( 0\), can achieve a faster rate of \(O(1/^{2})\).

## 6 Finite Sample Guarantee of MQP

In this section, building upon the convergence result of \(_{0}\), we demonstrate the finite sample guarantee of our MQP algorithm under partial coverage. We first introduce the soft margin.

**Assumption 7** (Soft margin).: _For any \(a^{}\), there exists \(t_{0}^{+},(0,]\) such that_

\[_{s d_{^{},_{0}}}(0<|q^{}(s,^{}(s))-q^{ }(s,a^{})|<t)(t/t_{0})^{}\]

_for any \(t>0\). Here, we use the convention \(x^{}=0\) if \(0<x<1\) and \(x^{}=\) if \(x>1\)._

In the extreme case, if there exists a gap in \(q^{}\) (also known as a hard margin) so that the best action is always better than the second-best by some lower bounded amount, then the soft margin is satisfied with \(=\). Thus, the soft margin is more general than the gap condition used in Simchowitz and Jamieson (2019); Wu et al. (2022). Crucially, a gap generally does _not_ exist in continuous state spaces unless Q-functions are discontinuous or one action is always option, or a gap involves a large \(t_{0}\) constant in discrete state spaces with bad dependence on the number of states. In contrast, a soft margin with some \(>0\) generally holds (see, _e.g._, lemma 4 in Hu et al., 2021). The soft margin is widely used in the literature on classification, decision making, and RL (Audibert and Tsybakov, 2007; Perchet and Rigollet, 2013; Lucetke and Chambaz, 2020; Hu et al., 2021, 2022).

**Theorem 5** (PAC guarantee of \(_{0}\)).: _Suppose Assumptions 5, 6, and 7 hold and \(\|^{}/_{b}\|_{} C_{0}\). Fix any \(>0\). Then, if \(n\) is at least_

\[\{|}{}\}^{}(t_{ 0}^{-1},||,_{},_{},C_{ ,d_{^{},_{0}}},(|||}{ }),(1-)^{-1},(C_{0}),R_{})\]

_with probability at least \(1-\), we can ensure \(J(^{})-J(_{0})\)._

The proof mainly consists of two steps: (1) obtaining \(L^{2}\)-errors of \(_{0}\) as demonstrated in Theorem 2, (2) translating this error into the error of \(_{0}\). In the second step, the soft margin plays a crucial role.

These theorems indicate that the realizability of the \(Q\)-function \(q^{}\) and Lagrange multiplier \(l^{}\), and the soft margin are sufficient for the PAC guarantee under partial coverage \(C_{,d_{^{},_{0}}}<\),\(\|d_{^{*},P_{b}}/P_{b}\|_{}<\). Our algorithm is _agnostic_ to \(\) and operates under any value of \(\). In particular, when there is a gap (\(=\)), we can achieve sample complexity of \(O(1/^{2})\)4. In comparison to Theorem 3, although we additionally use the soft margin, the realizability in Theorem 5 is more appealing since it is imposed on the standard Q-function \(q^{*}\). The closest guarantee to our work can be found in Chen and Jiang (2022), which demonstrates that the existence of the gap in \(q^{*}\), the realizability of \(q^{*},w^{*}(:=d_{^{*},_{0}}/P_{_{b}})\), and partial coverage \(\|w^{*}\|_{}<\) are sufficient conditions. A similar comparison is made in Ozdaglar et al. (2023). In comparison to their work, we use the soft margin, which is significantly less stringent.

## 7 Conclusions

We propose two value-based algorithms, MSQP and MQP, that operate under realizability of certain functions and partial coverage (_i.e._, single-policy-coverage). Notably, our guarantee does not require Bellman completeness and uniform-type realizability over the policy class. While guarantees with similar flavors are obtained in Zhan et al. (2022); Chen and Jiang (2022), MSQP can potentially relax the density-ratio-based partial coverage regarding the initial distribution as opposed to Zhan et al. (2022), and MQP can operate under the soft-margin, which is less stringent than the hard margin imposed in Chen and Jiang (2022). Moreover, both algorithms work on Q-functions, which are more commonly used in practice.