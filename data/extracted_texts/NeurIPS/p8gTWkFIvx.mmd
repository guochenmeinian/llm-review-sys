# Modality-Independent Teachers Meet

Weakly-Supervised Audio-Visual Event Parser

Yung-Hsuan Lai,1 Yen-Chun Chen,2 Yu-Chiang Frank Wang1,3

1National Taiwan University 2Microsoft 3NVIDIA

r10942097@ntu.edu.tw, chen.yenchun.tw@gmail.com, frankwang@nvidia.com

###### Abstract

Audio-visual learning has been a major pillar of multi-modal machine learning, where the community mostly focused on its _modality-aligned_ setting, _i.e_., the audio and visual modality are _both_ assumed to signal the prediction target. With the Look, Listen, and Parse dataset (LLP), we investigate the under-explored _unaligned_ setting, where the goal is to recognize audio and visual events in a video with only weak labels observed. Such weak video-level labels only tell what events happen without knowing the modality they are perceived (audio, visual, or both). To enhance learning in this challenging setting, we incorporate large-scale contrastively pre-trained models as the modality teachers. A simple, effective, and generic method, termed **V**isual-**A**udio **L**abel **E**l**l**aboration (Valor), is innovated to harvest modality labels for the training events. Empirical studies show that the harvested labels significantly improve an attentional baseline by **8.0** in average F-score (Type@AV). Surprisingly, we found that modality-independent teachers outperform their modality-fused counterparts since they are noise-proof from the other potentially unaligned modality. Moreover, our best model achieves the new state-of-the-art on all metrics of LLP by a substantial margin (**+5.4** F-score for Type@AV). Valor is further generalized to Audio-Visual Event Localization and achieves the new state-of-the-art as well.1

## 1 Introduction

Multi-modal learning has become a pivotal topic in modern machine learning research. Audio-visual learning is undoubtedly one of the primary focuses, as human frequently uses both hearing and vision to perceive the surrounding environment. Countless researchers have devoted to its _modality-aligned_ setting with a strong assumption that the audio and visual modality _both_ contain learnable clues to the desired prediction target. Numerous audio-visual tasks and algorithms have then been proposed, such as audio-visual speech recognition , audio-visual action recognition , sound generation from visual data , audio-visual question answering , and many more. However, almost all real-world events can be audible while invisible, and _vice versa_, depending on how they are perceived. For example, a mother doing dishes in the kitchen might hear a baby crying from the living room, but be unable to directly see what is happening to the baby.

Having observed this potential modality mismatch in generic videos, Tian et al.  proposed the Audio-Visual Video Parsing (AVVP) task, which aims to recognize events in videos independently of the audio and visual modalities and also temporally localize these events. AVVP presents an _unaligned_ setting of audio-visual learning since all 25 event types considered can be audio-only, visual-only, or audio-visual. Unfortunately, due to the laborious labeling process, Tian et al.  created this dataset (Look, Listen, and Parse; LLP) in a weakly-supervised setting.2 More specifically,only video-level event annotations are available at training. In other words, the modality (audio, visual, or both) and the timestamp of which an event occurs are not given to the learning model.

The AVVP task poses significant challenges from three different perspectives. First, an event is typically modality independent, _i.e._, knowing an event occurs in one modality says nothing about the other modality. As illustrated in Fig. 1, a sleeping dog is seen but may not be heard; conversely, a violin being played could sometimes go out of the camera view. Second, existing works heavily rely on the Multi-modal Multiple Instance Learning (MMIL) loss  to soft-select the modality (and timestamp), given only weak modality-less labels. This would be challenging for models to learn the correct event modality without observing a large amount of data. The uni-modal guided loss via label smoothing is also used to introduce uncertainty to the weak labels and thus regularize modality recognition. However, we hypothesize this improvement could be sub-optimal because no explicit modality information is introduced. Finally, AVVP requires models to predict events for all 1-second segments in a given video. Learning from weak video-level labels without timestamps makes it challenging for models to predict on a per-segment basis.

To address the above challenges in AVVP, we propose to incorporate large-scale pre-trained open-vocabulary models, namely CLIP  and CLAP , to enhance learning with weak labels. Pre-trained on pixels and waveforms (and contrastively pre-trained with natural language), these models are inherently isolated from potential spurious noise from the other modality. Another benefit is the applicability of their prediction in an open-vocabulary fashion. Therefore, to benefit from CLIP and CLAP, we aim to harvest explicit modality learning signals from them. Moreover, we aim to inference these models per video segment, yielding fine-grained temporal annotations.

While it might be tempting to naively treat these pre-trained models as teachers and then applies knowledge distillation (KD) , this could be sub-optimal as some events are difficult to distinguish from a single modality, even for humans. For example, cars, motorcycles, and lawn mowers all produce similar sounds. To better utilize CLIP and CLAP, we introduce **V**isual-**A**udio **L**abel **E**laboration (Valor), to harvest modality and timestamp labels in LLP. We prompt CLIP/CLAP with natural language description of all visual/audio event types for each video segment-by-segment and then extract labels when a threshold is met. Additionally, implausible events are filtered out using the original weak labels accompanied with the video to mitigate the above indistinguishable problem. Valor constructs fine-grained temporal labels in both modalities so that models have access to explicit training signals.

In addition to achieving the promising performance of AVVP, we observe that modality-independent teachers, CLIP and CLAP, generate more reliable labels than a modality-fused one, a cross-modal transformer. We also showcase the generalization capability of Valor via the Audio-Visual Event Localization (AVE) task, in which our method also achieves the new state-of-the-art. Our contributions are summarized as follows:

* A simple and effective AVVP framework, Valor, is proposed to harvest modality and temporal labels directly from video-level annotations, with an absolute improvement of **+8.0** F-score.
* We are the first to point out that modality independence could be crucial for audio-visual learning in the _unaligned_ and weakly-supervised setup.
* Our Valor achieves new state-of-the-art results with significant improvements on AVVP (**+5.4** F-score), with generalization to AVE (**+4.4** accuracy) jointly verified.

Figure 1: **Modality-unaligned samples from LLP. Note that recent AVVP approaches like HAN  are vulnerable to unaligned data modality and produce incorrect predictions.**

Preliminaries

Audio-Visual Video Parsing (AVVP)The AVVP  task is to recognize events of interest in a video in both visual and audio modalities and to temporally identify the associated frames. For the benchmark dataset of Look, Listen, and Parse (LLP), a \(T\)-second video is split into \(T\) non-overlapping segments. Each video segment is paired with a set of multi-class event labels \((_{t}^{v},_{t}^{a})\{0,1\}^{C}\) (\(_{t}^{v}\): visual events, \(_{t}^{a}\): audio events, \(C\): number of event types). However, in the training split, the dense'segment-level' labels \((_{t}^{v},_{t}^{a})\) are _not_ available. Instead, only the global modality-less 'video-level' labels \(:=_{t}\{_{t}^{v}_{t}^{a}\}_{t=1}^{T}\) are provided (\(\): element-wise 'logical and'). In other words, AVVP models need to be learned in a weakly-supervised setting.

Baseline ModelWe now briefly review the model of Hybrid Attention Network (HAN) , which is a common baseline for AVVP. In HAN, ResNet-152  and R(2+1)D  are employed to extract 2D and 3D visual features. Subsequently, they are concatenated and projected into segment-level features \(^{v}=\{_{t}^{v}\}_{t=1}^{T}^{T d}\) (\(d\): hidden dimension). Segment-level audio features \(^{a}=\{_{t}^{a}\}_{t=1}^{T}^{T d}\) are extracted using VGGish  and projected to the same dimension. HAN takes these features and aggregates the intra-modal and cross-modal information through self-attention and cross-attention:

\[}_{t}^{a}=_{t}^{a}+(_{t}^{a},^{a}, ^{a})+(_{t}^{a},^{v},^{v})\] (1)

\[}_{t}^{v}=_{t}^{v}+(_{t}^{v},^{v}, {F}^{v})+(_{t}^{v},^{a},^{a}),\] (2)

where \((,,)\) denotes multi-head attention . Following Transformer's practice, the outputs are further fed through LayerNorms  and a 2-layer FFN to yield \(}_{t}^{a},}_{t}^{v}\). With another linear layer, the hidden features are transformed into categorical logits \(_{t}^{v}\), \(_{t}^{a}\) for visual and audio events, respectively. Finally, the segment-level audio and visual event categorical probabilities, \(_{t}^{a}\) and \(_{t}^{v}\) (\(^{C}\)), are obtained by applying Sigmoid activation.

As a key module in Tian et al. , Multi-modal Multiple Instance Learning pooling (MMIL) is applied to address the above weakly-supervised learning task, which predicts the audio and visual event probabilities (\(^{m},\,m\{a,v\}\), audio and visual modalities) as:

\[^{m}=\{_{t}^{m}\}_{t}=_{t}(}^{m} ^{m}),^{m}=_{t}_{t}^{m} _{t}^{m},\] (3)

where trainable parameters \(^{m}^{d C}\) are implemented as linear layers (\(\): element-wise product). For video-level event probability \(\):

\[}=\{\{_{t}^{m}\}_{t}\}_{m}=_{m}(}),=_{m}_{t}_{t}^ {m}_{t}^{m}_{t}^{m},\] (4)

where \(}=\{}^{m}\}_{m}^{2 T d}\) and \(\) as a trainable linear layer. Moreover, modality training targets are obtained via label smoothing (LS) : \(}^{m}=()\). Finally, the model is trained with binary cross entropy (BCE) as the loss function:

\[_{}=_{}+_{}^{a}+_{}^{v},_{}=(,),_{}^{m}=(^{m},}^{m}).\] (5)

In summary, by the attention mechanisms introduced in HAN, MMIL pooling assigns event labels for each modality across time segments with only video-level event labels observed during training.

## 3 Method

With only video-level event labels observed during training, we address three major challenges of AVVP: 1) modality independence of events' occurrence, 2) reliance on MMIL pooling for event label assignment under insufficient data, and 3) demand for dense temporal predictions. To address these challenges, we propose to leverage large-scale pre-trained contrastive models, CLIP and CLAP, to extract modality-aware, temporally dense training signals to guide model learning.

### Zero-Shot Transfer of Contrastive Pre-trained Models

Radford et al.  proposed Contrastive Language-Image Pre-training (CLIP) to utilize web-scale image-text pairs to train a strong image encoder. As a result, CLIP overthrows the limitation of predicting predefined categories. Due to its large training data size (400M), CLIP has demonstrated remarkable zero-shot performance on a wide range of visual recognition tasks. All the above motivates us to incorporate CLIP to improve visual event recognition in AVVP.

In our work, CLIP's visual understanding of AVVP is extracted as the following. We extract \(T\) evenly spaced video frames and pass them into CLIP's image encoder to obtain the visual features \(\{_{t}^{}\}_{t=1}^{T}^{T d_{2}}\) (\(d_{2}\): the dimension of CLIP's feature). For simplicity and readability, we will In light of the notable success of CLIP , several studies have devoted to research on learning representative audio embeddings and text embeddings through Contrastive Language-Audio Pre-training (CLAP) [13; 15; 25; 49; 79]. In the same way as images and text are encoded in CLIP, web-scale audios and text are pre-trained with a contrastive objective in CLAP. Symmetrically, we obtain CLAP's understanding of AVVP audios as the following. From the audio channel, the raw waveform is extracted and split into \(T\) segments with the same lengths and then fed into CLAP, yielding segment-level audio features \(\{_{t}^{}\}_{t}^{T d_{3}}\) (\(d_{3}\): the dimension of CLAP's feature). On the other hand, an audio event caption is constructed by adding the prefix "This is a sound of" to each AVVP event's name. Processed by the CLAP text encoder, we obtain \(^{}=\{_{c}^{}\}_{c}^{C d _{3}}\). Segment-level audio event logits \(^{}^{C}\) are obtained by the inner products:

\[^{}=^{}^{^{}}.\] (7)

We note that Eqn. (6) and (7) can be viewed as CLIP's and CLAP's understanding of the associate video frame in the event space of AVVP.

### Harvesting Training Signals

Given the logits \(^{}\) and \(^{}\), we aim to convert them to useful training signals for the AVVP task. An intuitive idea is to teach our model via knowledge distillation (KD) . To deploy KD in training, segment-level normalized probabilities are first computed: \(^{P}=_{c}(^{P})\), \(^{m}=_{c}(^{m})\), where \((m,P)\{(v,),(a,)\}\) denotes data modality (audio/visual) and **pre**-trained model (CLIP/CLAP) pair. Next, KL-divergence for all segments is calculated: \(_{}^{m}=_{t}(_{t}^{P},_{t}^{m})\). Finally, KD training is done by optimizing the loss function:

\[_{}=_{}+_{}^{a }+_{}^{v}.\] (8)

However, as we find out empirically (shown in Table 4), this is _not_ the optimal usage of CLIP and CLAP. We hypothesize that some events are hard to distinguish from a single modality, _e.g._ cars, motorcycles, and lawn movers produce the sound of an engine. Therefore, we design Valor, utilizing video-level labels to filter out the impossible events, hence mitigating the confusion.

Visual-Audio Label Elaboration (Valor)To better exploit CLIP and CLAP, we design a simple yet effective method, Valor, to harvest dense labels in both modalities. In particular, we first define class-dependent thresholds \(^{P}^{C}\) for each modality to obtain segment-level labels from logits. Next, the impossible events are excluded using the given video-level labels, done via logical AND. Formally, this process can be written as: \(}_{t}^{m}=\{_{t}^{P}>^{P}\}\), with the overall loss function:

\[_{}=_{}+_{}^{a}+_{}^{v},_{}^{m}= _{t}(_{t}^{m},}_{t}^{m}).\] (9)

Figure 2: **Valor framework. With modality-independent label elaboration via CLIP and CLAP, the harvested temporally dense labels serve as additional modality- and time-aware cues.**

To summarize, we design a simple yet effective method, Valor, to utilize large-scale pre-trained contrastive models, CLIP and CLAP, to generate segment-level labels in both modalities. Due to the nature of immunity to spurious noise from the other modality, the contrastive pre-training methods, and the large pre-training dataset size, CLIP and CLAP are able to provide reliable labels in visual and audio modality, respectively. In addition, they are able to provide temporally dense labels to explicitly guide the model in learning events in each segment.

## 4 Related Work

### Audio-Visual Video Parsing with Look, Listen, and Parse

For AVVP, research flourishes along two orthogonal directions: enhancing the model architecture and label refinement. Architectural improvements include cross-modal co-occurrence module , class-aware uni-modal features and cross-modal grouping , and Multi-Modal Pyramid attention . On the other hand, label refinement shares a similar spirit with ours. MA  corrupted the data by swapping the audio channel of two videos with disjoint video-level event sets. The model's likelihood of the corrupted data was then used to determine the modality label. More recently, JoMoLD  utilized a two-stage approach. First, an AVVP model was trained as usual. Next, another model was trained while denoising the weak labels with prior belief from the first model. Both MA and JoMoLD produced global modality labels without timestamps. Concurrent to ours, VPLAN  and LSLD  generate dense temporal visual annotations with CLIP; however, the audio labels remain absent. Our Valor represents a _unified_ framework to elaborate the weak labels, along modality _and_ temporal dimension, via zero-shot transfer of pre-trained models. We further emphasize the importance of modality independence when synthesizing modality supervision.

### More Audio-Visual Learning

Audio-Visual Event Localization (AVE)Tian et al.  proposed AVE to recognize the audio-visual event in a video while localizing its temporal boundaries. Numerous studies have been conducted, including Lin et al.  with seq2seq models, Lin and Wang  using intra&inter frame Transformers, Wu et al.  via dual attention matching, audio-spatial channel-attention by Xu et al. , positive sample propagation from Zhou et al. , and Xia and Zhao  employing background suppression. We generalize Valor to AVE's weakly supervised setting.

Audio-Visual AssistanceWhile significant advancements have been made in speech recognition, speech enhancement, and action recognition, noise or bias residing in the uni-modal data is still problematic. An effective solution could involve integrating data from an additional modality. This research direction encompasses various areas including speech recognition , speaker recognition , action recognition , speech enhancement or separation , and object sound separation .

Audio-Visual Correspondence and UnderstandingHumans possess an impressive capacity to deduce occurrences in one sensory modality using information solely from another. This fascinating human ability to perceive across modalities has inspired researchers to delve into sound generation from visual data , video generation from audio , and audio-visual retrieval . In the pursuit of understanding how humans process audio-visual events, numerous studies have been undertaken on audio-visual understanding tasks such as sound localization in videos , audio-visual navigation , and audio-visual question answering .

## 5 Experiments

### Experimental Setup

Dataset and MetricsThe LLP dataset is composed of \(11849\) 10-second Youtube video clips covering 25 event categories, such as human activities, musical instruments, vehicles, and animals. The dataset is divided into training, validation, and testing splits, containing \(10,000\), \(649\), and \(1200\) clips, respectively. The official evaluation uses F-score to evaluate audio (A), visual (V), and audio-visual (AV) events separately. Type@AV (Type) is the averaged F-scores of A, V, and AV. Event@AV (Event) measures the ability to detect events in both modalities by combining audio and visual event detection results. Different from segment-level metrics, the event-level metrics treat consecutive positive segments as a whole, and mIoU of \(0.5\) is applied to calculate F-scores.

Implementation DetailsUnless otherwise specified, Valor uses HAN under a fair setting w.r.t. previous works with same data pre-processing. For the visual feature extraction, video frames are sampled at \(8\) frames per second. Additionally, we conduct experiments using CLIP and CLAP as feature extractors. The pre-trained ViT-L CLIP and HTSAT-RoBERTa-fusion CLAP are used to generate labels and extract features. Note that for all experiments with CLAP, we use the implementation from Wu et al.  pre-trained on LAION-Audio-630K. We do not use the version pre-trained on AudioSet (a larger pre-training corpus) since it overlaps with the AVVP validation and testing videos.

### Unified Label Elaboration for State-of-the-Art Audio-Visual Video Parsing

To demonstrate the effectiveness of Valor, we evaluate our method on the AVVP benchmark. Existing works include: 1) weakly-supervised audio-visual event localization methods AVE and AVSDN, 2) HAN and its network architecture advancements MM-Pyramid, MGN, CVCMS, and DHHN, and 3) different label refinement methods MA, JoMoLD, and VPLAN. We report the results on the test split of the LLP dataset in Table 1.

We achieve the new state-of-the-art (SOTA) on all metrics consistently with a large margin. Our method Valor significantly improves the baseline (HAN) by **8.0** in segment-level Type@AV. Compared to previous published SOTA, JoMoLD, Valor scores higher on all metrics, including the **5.4** F-score improvement for segment-level Type@AV, under a fair setting. With light hyper-parameter tuning, Valor+ further achieves a significant \(2.4\) improvement on Type@AV, with a deeper yet thinner HAN while keeping a similar number of trainable parameters. Our improvement on the audio side w.r.t. the concurrent preprint VPLAN is more significant than the visual side, which may be attributed to our effective audio teacher CLAP and label elaboration along the modality axis. We empirically conclude that Valor has successfully unified label refinement along both modality and temporal dimensions. To push to the limits, we further proposed Valor++ by replacing the feature extraction models with CLIP and CLAP, achieving another consistent boost, including \(3.0\) in segment Type@AV. We will release the Valor++ pre-trained checkpoint, features, and harvested labels to boost future AVVP research.

    &  &  \\  & A & V & AV & Type & Event & A & V & AV & Type & Event \\  AVE  & 47.2 & 37.1 & 35.4 & 39.9 & 41.6 & 40.4 & 34.7 & 31.6 & 35.5 & 36.5 \\ AVSDN  & 47.8 & 52.0 & 37.1 & 45.7 & 50.8 & 34.1 & 46.3 & 26.5 & 35.6 & 37.7 \\  HAN  & 60.1 & 52.9 & 48.9 & 54.0 & 55.4 & 51.3 & 48.9 & 43.0 & 47.7 & 48.0 \\ MM-Pyr  & 60.9 & 54.4 & 50.0 & 55.1 & 57.6 & 52.7 & 51.8 & 44.4 & 49.9 & 50.5 \\ MGN  & 60.8 & 55.4 & 50.4 & 55.5 & 57.2 & 51.1 & 52.4 & 44.4 & 49.3 & 49.1 \\ CVCMS  & 59.2 & 59.9 & 53.4 & 57.5 & 58.1 & 51.3 & 55.5 & 46.2 & 51.0 & 49.7 \\ DHHN  & 61.3 & 58.3 & 52.9 & 57.5 & 58.1 & 54.0 & 55.1 & 47.3 & 51.5 & 51.5 \\  MA  & 60.3 & 60.0 & 55.1 & 58.9 & 57.9 & 53.6 & 56.4 & 49.0 & 53.0 & 50.6 \\ JoMoLD  & 61.3 & 63.8 & 57.2 & 60.8 & 59.9 & 53.9 & 59.9 & 49.6 & 54.5 & 52.5 \\ VPLAN\({}^{}\) & 60.5 & 64.8 & 58.3 & 61.2 & 59.4 & 51.4 & 61.5 & 51.2 & 54.7 & 50.8 \\  Valor & 61.8 & 65.9 & 58.4 & 62.0 & 61.5 & 55.4 & 62.6 & 52.2 & 56.7 & 54.2 \\ Valor+ & 62.8 & 66.7 & 60.0 & 63.2 & 62.3 & 57.1 & 63.9 & 54.4 & 58.5 & 55.9 \\ Valor++ & **68.1** & **68.4** & **61.9** & **66.2** & **66.8** & **61.2** & **64.7** & **55.5** & **60.4** & **59.0** \\   

Table 1: **AVVP benchmark. Note that pseudo label denoising is not applied for VPLAN\({}^{}\). Valor+ is trained on a thinner yet deeper HAN of similar size. Valor++ further uses CLIP and CLAP as feature extractors and significantly boosts all metrics. The best numbers are in bold and the second best numbers are underlined.**

### Ablation Studies

The impressive results achieved in Table 1 are based on careful design. In this subsection, we elaborate on why we choose CLIP and CLAP to synthesize dense labels with modality annotations with empirical support. Furthermore, we break down the loss function and modeling components into orthogonal pieces and evaluate their individual effectiveness.

How to choose the labeler?In Table 2, we show the necessity of modality-independent pre-trained models (CLIP and CLAP) over the multi-modal model (HAN) as the labeler (2nd row) and that modality-aware labels beat modality-agnostic labels (3rd row). We aim to demonstrate the necessity and importance of **using large-scale pre-trained uni-modal models** to annotate **modality-aware segment-level labels**. To validate the former, we employ a baseline model (HAN) that has been trained on AVVP to individually annotate segment-level labels within the two modalities. Experimental results show that modality-aware temporal dense labels generated by a multi-modal model (HAN), learned from weak labels, are less effective than those generated by large-scale pre-trained uni-modal models (CLIP and CLAP), thereby underscoring the essentiality of using large-scale pre-trained uni-modal models. Subsequently, to validate the latter, we generate modality-agnostic segment-level labels from CLIP and CLAP, meaning that these labels only reveal the events occurring in each segment but do not disclose the modality of the event. As seen from the third row of Table 2, while such a labeling method increases the F-score for visual events, it dramatically decreases the F-score for audio events. The overall performance (Type F-score) is even worse than that of HAN (the first row), clearly indicating the importance of modality-aware labeling for the model to learn the AVVP task effectively.

How accurate are the elaborated labels?To measure the fidelity of the pseudo labels generated via Valor in audio and visual modalities, we conduct a comparison between the segment-level labels generated from Valor and those from a naive approach where we assume that video-level labels also serve as segment-level labels, _i.e_., we assume that an event occurs in both modalities and all segments if it occurs in the video. We directly evaluate these pseudo labels on the validation split before using them for training. The results, presented in Table 3, clearly demonstrate the superiority of our generated segment-level audio and visual pseudo labels compared to the naive counterparts. Notably, our segment-level visual F-score exceeds the naive approach by nearly \(15\) points while the audio-visual F-score significantly surpasses for more than \(17\) points. These results highlight the reliability of the Valor-generated pseudo labels, which provide more faithful temporal and modal information to facilitate model training.

   Dense & Modality &  &  \\ Labeler & Label & A & V & AV & Type & Event & A & V & AV & Type & Event \\  None & ✔ & 62.0 & 54.5 & 50.2 & 55.6 & 57.1 & 53.5 & 50.5 & 43.6 & 49.2 & 50.3 \\ HAN & ✔ & 62.1 & 56.4 & 52.1 & 56.8 & 57.6 & 53.4 & 52.0 & 45.4 & 50.3 & 50.6 \\ CLIP\& CLAP & ✔ & 41.0 & 59.0 & 34.5 & 44.9 & 52.1 & 33.2 & 56.2 & 28.2 & 39.2 & 43.1 \\ CLIP\& CLAP & ✔ & **62.7** & **66.3** & **61.0** & **63.4** & **61.8** & **55.5** & **62.0** & **54.1** & **57.2** & **53.8** \\   

Table 2: **Selection of modality-independent labeler.** Note that utilizing a cross-modal labeler HAN instead of CLIP and CLAP to generate segment-level labels hardly improves the baseline (HAN). On the other hand, modality-less segment-level labels deteriorates the performance. All results are reported on the **validation** split of LLP.

   Label Generation Methods & Audio & Visual & Audio-Visual \\  Video Labels & 80.08 & 67.21 & 59.45 \\ Valor & **85.07** (+4.99) & **82.14** (+14.93) & **77.07** (+17.62) \\   

Table 3: **Fidelity of the elaborated labels.** We conduct a comparison between the segment-level labels generated from Valor and those from a naive approach where we assume video-level labels also serve as segment-level labels. We directly evaluate these pseudo labels on the validation split before training. The results clearly indicate that Valor-generated labels are more accurate than the naive ones.

How to use the elaborated labels?We conduct an ablation study on utilizing CLIP and CLAP together. The results are presented in Table 4. The replacement of the smoothed video-level event labels \(^{a}\) and \(^{v}\) with their respective refined weak labels \(^{a}\) and \(^{v}\) derived from our method leads to a significant increase in the Type@AV F-score, from \(54.0\) to \(60.8\). This finding underscores the importance of incorporating labels that are proximal to ground truth, albeit weak. Furthermore, we leverage the CLIP and CLAP models to generate segment-level labels for each modality. This approach results in an improvement of \(8.0\) Type@AV F-score over the baseline, indicating that explicitly informing the model of the events occurring in each segment of the audio-visual video facilitates the learning of the Audio-Visual Video Parsing (AVVP) task. In addition, CLIP and CLAP are also used to obtain more representative features. Replacing the ResNet-152 and VGGish features with CLIP and CLAP features yields a Type@AV F-score improvement of \(4.0\).

Whether using video-level labels as filters?Video-level labels are pivotal for generating reliable pseudo labels in our method, where we employ them as filters to eliminate impossible events misclassified by CLIP or CLAP. In Figure 4, we conduct experiments to underscore the necessity of using video-level labels as filters. Notably, without utilizing video-level labels as filters, both audio and visual F-scores plummet, reaching \(47.9\) and \(53.8\), respectively.

How well can the modality non-aligned problem be solved?As we have pointed out that the modality independence of events is one of the crucial challenges in the AVVP task, we assess the extent of the modality non-aligned problem in the LLP dataset and the extent to which the models can solve the problem. First, we define the word "segment-level event" as the cumulative sum of the number of events that occur without modality differences across all segments. In other words, if an audio event and a visual event from the same category occur within a segment, they are counted as a single "segment-level event." In the LLP dataset's validation split, there are \(9126\) segment-level events. Among these, \(4048\) segment-level events are modality non-aligned,, they occur in exactly one modality. To measure how well trained models address the modality non-aligned issue, we conduct experiments involving several models, including our own, to predict both the modality and event of these segments. A successful prediction entails correctly identifying the event and confirming its presence in both modalities. The results, as displayed in Figure 4, reveal that HAN exhibits the poorest performance in predicting modality non-aligned events. Conversely, our methods Valor and Valor++ outperform the prior SOTA, JoMoLD. This highlights the effectiveness of our approach in mitigating the modality non-alignment challenge within the AVVP task.

### Generalize Valor to Audio-Visual Event Localization

In this section, we showcase the additional generalization ability of Valor by applying it to the Audio-Visual Event Localization (AVE) task. We consider the weakly-supervised version of AVE, where segment-level ground truths are not available to the model during training, meaning that no timestamp is provided for the event, motivating us to apply Valor to harvest event labels. Without task-specific modification, we directly apply HAN and Valor to AVE. The only difference is that at inference, we combine the audio and visual prediction to obtain the audio-visual event required in this task. Please refer to the supplementary for more implementation details of the AVE task.

Quantitative ResultsFrom Table 5, we observe that our baseline method performs on par with the previous state-of-the-art method CMBS . When our method is applied to the model, the accuracy leaps from \(75.3\) to \(80.4\), indicating the generalizability of our method. In addition, we surpass CMBS  and have become the new state-of-the-art on the weakly-supervised AVE task with an improvement of **4.4** in accuracy.

### Additional Analyses

Qualitative ComparisonAside from quantitative comparison with previous AVVP works, we perform a qualitative evaluation as well. We qualitatively compare with the baseline method HAN  and the state-of-the-art method JoMoLD . From Figure 5, it can be seen that only our model can correctly predict the "Baby Cry" visual event. HAN not only fails to predict "Baby Cry" correctly but also mistakenly identifies the woman in the video as speaking. In the audio modality, all models correctly predict the presence of "Baby Cry" in the sound, but they also simultaneously misinterprets that someone is talking. Among all models, our model makes the least severe misjudgments."

Class-wise F-score ComparisonWe further evaluate the effectiveness of providing accurate uni-modal segment-level pseudo labels for the model training. We visualize class-wise improvements between our generated segment-level labels for each modality and the naive segment-level labels derived from the video-level labels. In Figure 6, we observe that when our audio pseudo labels are used, most of the audio events improve. In Figure 7, when our visual pseudo labels are used, nearly every event's F-score increases. These results indicate the effectiveness of our method in guiding the model to learn events in each modality. For the inferior performance on the "Speech" event, since CLIP is inept at extracting fine-grained visual information, it is not expected to recognize the "Speech" event well, which requires close attention on mouth movements.

Figure 5: **Qualitative Comparison with Previous AVVP Works. “GT” denotes the ground truth annotations. We compare with HAN  and JoMoLD .**

   Method & Accuracy(\%) \\  VGG-like, VGG-19 features & \\ AVEL  & 66.7 \\ AVSDN  & 67.3 \\ CMAN  & 70.4 \\ ARB  & 68.9 \\ AVIN  & 69.4 \\ AVT  & 70.2 \\ CMRAN  & 72.9 \\ PSP  & 73.5 \\ CMBS  & 74.2 \\  VGG-like, ResNet-15 features & \\ AVEL  & 71.6 \\ AVSDN  & 74.2 \\ CMRAN  & 75.3 \\ CMBS  & 76.0 \\   CLAP, CLIP, R(2+1)D features & \\ HAN & 75.3 \\ Valor & **80.4** \\   

Table 5: Results on the AVE task.

## 6 Conclusion

We propose **V**isual-**A**udio **L**abel **E**laboration (Valor) for weakly-supervised Audio-Visual Video Parsing. By harnessing large-scale pre-trained contrastive models CLIP and CLAP, we generate fine-grained temporal labels in audio and visual modalities, providing explicit supervision to guide the learning of AVVP models. We show that utilizing modality-independent pre-trained models (CLIP and CLAP) and generating modality-aware labels are essential for AVVP. Valor outperforms all the previous works when comparing in a fair setting, demonstrating its effectiveness. In addition, we demonstrate the generalizability of our method in the Audio-Visual Event Localization task, where we improve the baseline greatly and achieve a state-of-the-art result.

LimitationsWhile Valor performs well on AVVP, it is uncertain whether it will maintain this efficacy when the number of events to classify expands. Moreover, because CLIP is far from perfect at capturing fine-grained visual details, it may fail to generate precise labels when the subject of the event is small or when the video quality is poor, potentially confounding the model.

Broader ImpactsAs an event recognition model, Valor could be applied to future intelligent surveillance systems. While may reduce physical crime concerns, it could on the other hand infringe people's privacy and rights. Since the input consists of videos of people, data privacy issues are inevitable, and it is essential to prioritize data protection against unauthorized access.

Figure 6: **Class-wise improvement on audio events.** Using the derived audio segment-level pseudo label is advantageous over the baseline using video-level labels as if they were audio segment-level.

Figure 7: **Class-wise improvement on visual events.** Valor’s visual segment-level labels clearly outperforms the video-level labels. Note that CLIP is applicable to extract global but not fine-grained information from visual inputs. Thus, it is not expected to produce proper visual cues for the “Speech” event, which requires close attention to mouth movements.