# Image Copy Detection for Diffusion Models

Wenhao Wang\({}^{1}\), Yifan Sun\({}^{2}\), Zhentao Tan\({}^{2}\), Yi Yang\({}^{3}\)

\({}^{1}\)University of Technology Sydney \({}^{2}\)Baidu Inc. \({}^{3}\)Zhejiang University

Corresponding Author.

###### Abstract

Images produced by diffusion models are increasingly popular in digital artwork and visual marketing. However, such generated images might replicate content from existing ones and pose the challenge of content originality. Existing Image Copy Detection (ICD) models, though accurate in detecting hand-crafted replicas, overlook the challenge from diffusion models. This motivates us to introduce ICDiff, the first ICD specialized for diffusion models. To this end, we construct a Diffusion-Replication (D-Rep) dataset and correspondingly propose a novel deep embedding method. D-Rep uses a state-of-the-art diffusion model (Stable Diffusion V1.5) to generate \(40,000\) image-replica pairs, which are manually annotated into 6 replication levels ranging from \(0\) (no replication) to \(5\) (total replication). Our method, PDF-Embedding, transforms the replication level of each image-replica pair into a probability density function (PDF) as the supervision signal. The intuition is that the probability of neighboring replication levels should be continuous and smooth. Experimental results show that PDF-Embedding surpasses protocol-driven methods and non-PDF choices on the D-Rep test set. Moreover, by utilizing PDF-Embedding, we find that the replication ratios of well-known diffusion models against an open-source gallery range from \(10\%\) to \(20\%\). The project is publicly available at [https://icdiff.github.io/](https://icdiff.github.io/).

## 1 Introduction

Diffusion models have gained popularity due to their ability to generate high-quality images. A phenomenon accompanying this trend is that these generated images might replicate content from

Figure 1: Some generated images (top) from diffusion models replicates the contents of existing images (bottom). The existing (matched) images are from LAION-Aesthetics . The diffusion models include both commercial and open-source ones.

existing ones. In Fig. 1, we choose six well-known diffusion models  to illustrate this replication phenomenon. The content replication is acceptable for some (fair) use while interest holders may regard others as copyright infringement . This paper leaves this dispute alone, and focuses a scientific problem: _How to identify the content replication brought by diffusion models?_

Image Copy Detection (ICD) provides a general solution to the above demand: it identifies whether an image is copied from a reference gallery after being tampered with. However, the current ICD methods are trained using hand-crafted image transformations (_e.g._, horizontal flips, random rotations, and random crops) and overlook the challenge from diffusion models. Empirically, we find existing ICD methods can be easily confused by diffusion-generated replicas (as detailed in Table 3). We infer it is because the tamper patterns underlying diffusion-generated replicas (Fig. 2 right) are different from hand-crafted ones (Fig. 2 middle), yielding a considerable pattern gap.

In this paper, we introduce ICDiff, the first ICD specialized for diffusion-generated replicas. Our efforts mainly involve building a new ICD dataset and proposing a novel deep embedding method.

\(\)**A Diffusion Replication (D-Rep) dataset.** D-Rep consists of \(40,000\) image-replica pairs, in which each replica is generated by a diffusion model. Specifically, the images are from LAION-Aesthetic V2 , while their replicas are generated by Stable Diffusion V1.5 . To make the replica generation more efficient, we search out the text prompts (from DiffusionDB )that are similar to the titles of LAION-Aesthetic V2 images, input these text prompts into Stable Diffusion V1.5, and generate many redundant candidate replicas. Given these candidate replicas, we employ human annotators to label the replication level of each generated image against a corresponding LAION-Aesthetic image. The annotation results in \(40,000\) image-replica pairs with \(6\) replication levels ranging from \(0\) (no replication) to \(5\) (total replication). We divide D-Rep into a training set with \(90\%\) (\(36,000\)) pairs and a test set with the remaining \(10\%\) (\(4,000\)) pairs.

\(\)**A novel method named PDF-Embedding.** The ICD methods rely on deep embedding learning at their core. In the deep embedding space, the replica should be close to its original image and far away from other images. Compared with popular deep embedding methods, our PDF-Embedding learns a Probability-Density-Function between two images, instead of a similarity score. More concretely, PDF-Embedding transforms the replication level of each image-replica pair into a PDF as the supervision signal. The intuition is that the probability of neighboring replication levels should be continuous and smooth. For instance, if an image-replica pair is annotated as level-3 replication, the probabilities for level-\(2\) and level-\(4\) replications should also not be significantly low.

PDF-Embedding predicts the probability scores on all the replication levels simultaneously in two steps: 1) extracting \(6\) feature vectors in parallel from both the real image and its replica, respectively and 2) calculating \(6\) inner products (between two images) to indicate the probability score at \(6\) corresponding replication levels. The largest-scored entry indicates the predicted replication level. Experimentally, we prove the effectiveness of our method by comparing it with popular deep embedding models and protocol-driven methods trained on our D-Rep. Moreover, we evaluate the replication of six famous diffusion models and provide a comprehensive analysis.

In conclusion, our key contributions are as follows:

Figure 2: The comparison between current ICD with the ICDiff. The current ICD focuses on detecting edited copies generated by transformations like horizontal flips, random rotations, and random crops. In contrast, the ICDiff aims to detect replication generated by diffusion models, such as Stable Diffusion . (Source of the original image: Lawsuit from Getty Images.)

1. We propose a timely and important ICD task, _i.e_, Image Copy Detection for Diffusion Models (ICDiff), designed specifically to identify the replication caused by diffusion models.
2. We build the first ICDiff dataset and introduce PDF-Embedding as a baseline method. PDF-Embedding transforms replication levels into probability density functions (PDFs) and learns a set of representative vectors for each image.
3. Extensive experimental results demonstrate the efficiency of our proposed method. Moreover, we discover that between \(10\%\) to \(20\%\) of images generated by six well-known diffusion models replicate contents of a large-scale image gallery.

## 2 Related Works

### Existing Image Copy Detection Methods

Current ICD methods try to detect replications by learning the invariance of image transformations. For example, ASL  considers the relationship between image transformations and hard negative samples. AnyPattern  and PE-ICD  build benchmarks and propose solutions that focus on novel patterns in real-world scenarios. SSCD  reveals that self-supervised contrastive training inherently relies on image transformations, and thus adapts InfoNCE  by introducing a differential entropy regularization. BoT  incorporates approximately ten types of image transformations, combined with various tricks, to train an ICD model. By increasing the intensity of transformations gradually, CNNCL  successfully detects hard positives using a simple contrastive loss and memory bank. EfNet  ensembles models trained with different image transformations to boost the performance. In this paper, we discover that capturing the invariance of image transformations is ineffective for detecting copies generated by diffusion models. Consequently, we manually label a new dataset and train a specialized ICD model.

### Replication in Diffusion Models

Past research has explored the replication problems associated with diffusion models. The study by  questions if diffusion models generate unique artworks or simply mirror the content from their training data. Research teams from Google, as highlighted in , note that diffusion models reveal their training data during content generation. Other studies prevent generating replications from the perspectives of both training diffusion models [23; 24; 25; 26; 27] and copyright holders [28; 29; 30]. Some experts, such as those in , find that the replication of data within training sets might be a significant factor leading to copying behaviors in diffusion models. To address this,  proposes an algorithmic chain to de-duplicate the training sources like LAION-2B . In contrast to these efforts, our ICDiff offers a unique perspective. Specifically, unlike those that directly using existing image descriptors (such as those from CLIP  and SSCD ), we manually-label a dataset and develop a specialized ICD algorithm. By implementing our method, the analytical tools and preventative strategies proposed in existing studies may achieve greater efficacy.

## 3 Benchmark

This section introduces the proposed ICD for diffusion models (ICDiff), including our dataset (D-Rep) and the corresponding evaluation protocols.

### D-Rep Dataset

Current ICD [33; 34; 35; 14; 21; 20] primarily focuses on the replica challenges brought by hand-crafted transformations. In contrast, our ICDiff aims to address the replication issues caused by diffusion models [3; 4; 5; 6; 7; 8]. To facilitate ICDiff research, we construct D-Rep dataset, which is characterized for diffusion-based replication (See Fig. 3 and the Appendix (Section A) for the examples of diffusion-based replica). The construction process involves generating candidate pairs followed by manual labeling.

**Generating candidate pairs.** It consists of (1) selecting the top \(40,000\) most similar prompts and titles: this selection provides an abundant image-replica pair source. In detail, we use the Sentence Transformer  to encode the \(1.8\) million real-user generated prompts from DiffusionDB  and the \(12\) million image titles from LAION-Aesthetics V2 6+ , and then utilize the computed cosine similarities to compare; (2) obtaining the candidate pairs: the generated images are produced using the prompts with Stable Diffusion V1.5 , and the real images are fetched based on the titles.

**Manual labeling.** We generally follow the definition of replication in  and further define six levels of replication (0 to 5). A higher level indicates a greater degree that the generated image replicates the real image. Due to the complex nature of diffusion-generated images, we use multiple levels instead of the binary levels used in , which employed manual-synthetic datasets as shown in their Fig. 2. We then train ten professional labelers to assign these levels to the \(40,000\) candidate pairs: Initially, we assign \(4,000\) image pairs to each labeler. If labelers are confident in their judgment of an image pair, they will directly assign a label. Otherwise, they will place the image pair in an undecided pool. On average, each labeler has about \(600\) undecided pairs. Finally, for each undecided pair, we vote to reach a final decision. For example, if the votes for an undecided pair are \(2\), \(2\), \(2\), \(3\), \(3\), \(3\), \(3\), \(3\), \(4\), the final label assigned is \(3\). Given the complexity of this labeling task, it took both the labelers and our team one month to finish the process. To maintain authenticity, we did not pre-determine the proportion of each score. The resulting proportions are on the left side of Fig. 3.

### Evaluation Protocols

To evaluate ICD models on the D-Rep dataset, we divide the dataset into a 90/10 training/test split and design two evaluation protocols: Pearson Correlation Coefficient (PCC) and Relative Deviation (RD).

**Pearson Correlation Coefficient (PCC).** The PCC is a measure used to quantify the linear relationship between two sequences. When PCC is near \(1\) or \(-1\), it indicates a strong positive or negative relationship. If PCC is near \(0\), there's little to no correlation between the sequences. Herein, we consider two sequences, the predicted replication level \(^{p}=(s_{1}^{p},s_{2}^{p},,s_{n}^{p})\) and the ground-truth \(^{l}=(s_{1}^{l},s_{2}^{l},,s_{n}^{l})\) (\(n\) is the number of test pairs). The PCC for ICDiff is defined as:

\[=^{n}(s_{i}^{p}-^{p}}) (s_{i}^{l}-^{l}})}{^{n}(s_{i }^{p}-^{p}})^{2}}^{n}(s_{i} ^{l}-^{l}})^{2}}}, \]

where \(^{p}}\) and \(^{l}}\) are the mean values of \(^{p}\) and \(^{l}\), respectively.

Figure 3: The demonstration of the manual-labeled D-Rep dataset. The percentages on the left show the proportion of images with a particular level.

A limitation of the PCC is its insensitivity to global shifts. If all the predictions differ from their corresponding ground truth with the same shift, the PCC does not reflect such a shift and remains large. To overcome this limitation, we propose a new metric called the Relative Deviation (RD).

**Relative Deviation (RD).** We use RD to quantify the normalized deviation between the predicted and the labeled levels. By normalizing against the maximum possible deviation, RD provides a measure of how close the predictions are to the labeled levels on a scale of \(0\) to \(1\). The RD is calculated by

\[=_{i=1}^{n}(^{p}-s_{i}^{l} |}{(N-s_{i}^{l},s_{i}^{l})}), \]

where \(N\) is the highest replication level in our D-Rep.

**The Preference for RD over Absolute One.** Here we show the preference for employing RD over the absolute one through two illustrative examples. We denote the relative and absolute deviation of the \(i\)th test pair as: \(S_{i}=^{p}-s_{i}^{l}|}{(N-s_{i}^{l},s_{i}^{l} )}\), and \(T_{i}=^{p}-s_{i}^{l}|}{N}\).

(1) For a sample with \(s_{i}^{l}=3\), if \(s_{i}^{p}=3\), both \(S_{i}\) and \(T_{i}\) equal \(0\); however, if \(s_{i}^{p}=0\) (representing the worst prediction), \(S_{i}=1\) and \(T_{i}=0.6\). Here, \(S_{i}\) adjusts the worst prediction to a value of \(1\).

(2) In the first scenario, where \(s_{i}^{l}=3\) and \(s_{i}^{p}=0,S_{i}=1\) and \(T_{i}=0.6\). In the second scenario, where \(s_{i}^{l}=5\) and \(s_{i}^{p}=2,S_{i}=0.6\) and \(T_{i}=0.6\). For both cases, \(T_{i}\) remains the same at \(0.6\), whereas \(S_{i}\) values differ. Nevertheless, the two scenarios are distinct: in the first, the prediction cannot deteriorate further; in the second, it can. The value of \(S_{i}\) accurately captures this distinction, whereas \(T_{i}\) does not.

## 4 Method

This section introduces our proposed PDF-Embedding for ICDiff. PDF-Embedding converts each replication level into a probability density function (PDF). To facilitate learning from these PDFs, we expand the original representative vector into a set of vectors. The demonstration of PDF-Embedding is displayed in Fig. 4.

### Converting Level to Probability Density

Given an replication level \(s^{l}^{l}\), we first normalize it into \(p^{l}=s^{l}/max(^{l})\). Then we transfer \(p^{l}\) into a PDF denoted as \(g(x)\)2, where \(x\) indicates each possible normalized level. The

Figure 4: The demonstration of the proposed PDF-Embedding. Initially, PDF-Embedding converts manually-labeled replication levels into probability density functions (PDFs). To learn from these PDFs, we use a set of vectors as the representation of an image.

intuition is that the probability distribution of neighboring replication levels should be continuous and smooth. The function \(g(x)\) must satisfy the following conditions: (1) \(_{0}^{1}g(x)dx=1\), ensuring that \(g(x)\) is a valid PDF; (2) \(g(x) 0\), indicating the non-negativity of the PDF; and (3) \(g(x) g(p^{l}),\) which implies that the density is maximized at the normalized level \(p^{l}\). We use three different implementations for \(g(x)\):

Gaussian:

\[g(x A,,)=A(-}{2^{2}} ), \]

linear:

\[g(x A,,)=A-|x-|, \]

and exponential:

\[g(x A,,)=A(-|x-|), \]

where: \(A\) is the amplitude, and \(\) is the center; \(\) is the standard deviation of Gaussian function, \(\) is the slope of linear function, and \(\) is the spread of exponential function.

The performance achieved with various converted PDFs is illustrated in the experimental section. For additional details, please see the Appendix (Section B), which includes (1) **the methodology for calculating distribution values**, (2) **the visualization of the learned distributions corresponding to different image pairs**, and (3) **an analysis of the deviation rate from peak values**.

### Representing an Image as a Set of Vectors

To facilitate learning from the converted PDFs, we utilize a Vision Transformer (ViT)  to represent an image as a set of vectors. Let's denote the patch tokens from a real image as \(_{r}^{0}\) and from a generated image as \(_{g}^{0}\), the ViT model as \(f\), and the number of layers in ViT as \(L\). The feed-forward process can be expressed as:

\[[_{r}^{L},_{r}^{L}]& =f([^{0},_{r}^{0}]),\\ [_{g}^{L},_{g}^{L}]&=f ([^{0},_{g}^{0}]), \]

where \(^{0}\) is a set of class tokens; \(_{r}^{L}\) is a set of representative vectors for the real images, consisting of vectors \(c_{0,r}^{L},c_{1,r}^{L},,c_{N,r}^{L}\), and \(_{g}^{L}\) is a set of representative vectors for the generated images, consisting of vectors \(c_{0,g}^{L},c_{1,g}^{L},,c_{N,g}^{L}\); \(N\) is the highest replication level.

Therefore, we can use another PDF \(h(x)\) (\(x\)) to describe the predicted replication between two images by

\[h(x)=_{r}^{L}_{g}^{L}, \]

which expands to:

\[h(x)=[_{0,r}^{L},_{0,g}^{L}, _{1,r}^{L},_{1,g}^{L},, _{N,r}^{L},_{N,g}^{L}], \]

where \(,\) denotes the cosine similarity.

For training, recalling the PDF \(g(x)\) derived from the level, we define the final loss using the Kullback-Leibler (KL) divergence:

\[=D_{KL}(g\|h)=_{0}^{1}g(x)()dx, \]

which serves as a measure of the disparity between two probability distributions. Additionally, in the Appendix (Section C), we demonstrate what the network captures during its learning phase.

During testing, the normalized level between two images is denoted by \(^{l}\), satisfying \(h(x) h(^{l})\). As illustrated in Eqn. 8, \(h(x)\) in practice is discrete within the interval \(\). Consequently, the resulting level is

\[j=*{argmax}h(x), \]

and the normalized level is quantified as \(\).

## 5 Experiments

### Training Details

We implement our PDF-Embedding using PyTorch  and distribute its training over 8 A100 GPUs. The ViT-B/16  serves as the backbone and is pre-trained on the ImageNet dataset  using DeiT , unless specified otherwise. We resize images to a resolution of \(224 224\) before training. A batch size of \(512\) is used, and the total training epochs is \(25\) with a cosine-decreasing learning rate.

### Challenge from the ICDiff Task

This section benchmarks popular public models on our D-Rep test dataset. As Table 3 shows, we conduct experiments extensively on vision-language models, self-supervised models, supervised pre-trained models, and current ICD models. We employ these models as feature extractors and calculate the cosine similarity between pairs of image features (except for GPT-4V Turbo , see Section D in the Appendix for the implementation of it). For the computation of PCC and RD, we adjust the granularity by scaling the computed cosine similarities by a factor of \(N\). In the Appendix (Section E), we further present the concrete similarities predicted by these models and provide corresponding analysis. We observe that: (1) the large multimodal model GPT-4V Turbo  performs best in PCC, while the self-supervised model DINOv2  excels in RD. This can be attributed to their pre-training on a large, curated, and diverse dataset. Nevertheless, their performance remains somewhat limited, achieving only \(47.3\%\) in PCC and \(32.9\%\) in RD. This underscores that even the best publicly available models have yet to effectively address the ICDiff task. (2) Current ICD models, like SSCD , which are referenced in analysis papers [10; 24] discussing the replication issues of diffusion models, indeed show poor performance. For instance, SSCD  registers only \(29.1\%\) in PCC and \(62.3\%\) in RD. Even the more advanced model, BoT , only manages \(35.6\%\) in PCC and \(53.8\%\) in RD. These results underscore the need for a specialized ICD method for diffusion models. Adopting our specialized ICD approach will make their subsequent analysis more accurate and convincing. (3) Beyond these models, we also observe that others underperform on the ICDiff task. This further emphasizes the necessity of training a specialized ICDiff model.

### The Effectiveness of PDF-Embedding

This section demonstrates the effectiveness of our proposed PDF-Embedding by (1) contrasting it against protocol-driven methods and non-PDF choices on the D-Rep dataset, (2) comparing between different distributions, and (3) comparing with other models in generalization settings.

   Class & Method & PCC (\(\%\)) \(\) & RD (\(\%\)) \(\) \\   Vision- \\ language \\  } & SLIP  & \(31.8\) & \(49.7\) \\  & BLIP  & \(34.8\) & \(41.6\) \\  & ZeroVL  & \(36.3\) & \(36.5\) \\  & CLIP  & \(36.8\) & \(35.8\) \\  & GPT-4V  & \(47.3\) & \(38.7\) \\   Self- \\ supervised \\ Learning \\  } & SimCLR  & \(7.2\) & \(49.4\) \\  & MAE  & \(20.7\) & \(67.6\) \\  & SimSiam  & \(33.5\) & \(45.4\) \\  & MoCov3  & \(35.7\) & \(40.3\) \\  & DINOv2  & \(39.0\) & \(32.9\) \\   Supervised \\ Pre-trained \\  } & EfficientNet  & \(24.0\) & \(59.3\) \\  & Swin-B  & \(32.5\) & \(38.4\) \\  & ConvNeXt  & \(33.8\) & \(36.0\) \\  & DeiT-B  & \(35.3\) & \(41.7\) \\  & ResNet-50  & \(37.5\) & \(34.5\) \\   Current \\ ICD \\ Models \\  } & ASL  & \(5.6\) & \(78.1\) \\  & CNNCL  & \(19.1\) & \(51.7\) \\  & SSCD  & \(29.1\) & \(62.3\) \\  & EfNet  & \(30.5\) & \(62.8\) \\  & BoT  & \(35.6\) & \(53.8\) \\   

Table 1: The performance of publicly available models and our PDF-Embedding on the D-Rep. For qualitative results, please refer to Section E in the Appendix.

**Comparison against protocol-driven methods.** Since we employ PCC and RD as the evaluation protocols, a natural embedding learning would be directly using these protocols as the optimization objective, _i.e._, enlarging PCC and reducing RD. Moreover, we add another variant of "reducing RD", _i.e._, reducing the absolute deviation \(|s_{i}^{p}-s_{i}^{l}|\) in a regression manner. The comparisons are summarized in Table 2, from which we draw three observations as below: (1) Training on D-Rep with the protocol-driven method achieves good results on their specified protocol but performs bad for the other. While "Enlarging PCC" attains a commendable PCC, its RD of \(40.1\%\) indicates large deviation from the ground truth. "Reducing RD" or "Reducing Deviation" shows a relatively good RD (\(28.1\%\)); however, they exhibit small PCC values that indicate low linear consistency. (2) Our proposed PDF-Embedding surpasses these protocol-driven methods in both PCC and RD. Compared against "Enlarging PCC", our method improves PCC by \(1.6\%\) and decreases RD by \(16.1\%\). Besides, our method achieves \(+16.0\%\) PCC and \(-4.1\%\) RD compared against "Reducing RD" and "Reducing Deviation". (3) The computational overhead introduced by our method is negligible. First, compared to other options, our method only increases the training time by \(5.8\%\). Second, our method introduces minimal additional inference time. Third, while our method requires a longer matching time, its magnitude is close to \(10^{-9}\), which is negligible when compared to the inference time's magnitude of \(10^{-3}\). Further discussions on the matching time in real-world scenarios can be found in Section 5.4.

**Comparison against two non-PDF methods.** In Table 2, we also show the experimental results of our method under two standard supervising signals, _i.e._, "One-hot Label" and "Label Smoothing (\(=0.5\))". In comparison, our PDF-Embedding using PDFs gains significant superiority, _e.g._, using exponential PDF is better than label smoothing by \(+21.3\%\) PCC and \(-10.5\%\) RD. This superiority validates our intuition that neighboring replication levels should be continuous and smooth.

**Comparison between different PDF implementations.** We compare between three different PDF implementations for the proposed PDF-Embedding in Fig. 5. We observe that: (1) The exponential (convex) function benefits the PCC metric, whereas the Gaussian (concave) function favors the RD metric. The performance of the linear function, which lacks curvature, falls between that of the convex and concave functions. (2) Our method demonstrates robust performance across various distributions, reducing the challenge of selecting an optimal parameter. For example, when using the exponential function, the performance remains high when \(A\) ranges from \(0.6\) to \(1.8\). (3) A model

  Method & PCC (\(\%\)) \(\) & RD (\(\%\)) \(\) & Train (\(s/iter\)) \(\) & Infer (\(s/img\)) \(\) & Match (\(s/pair\)) \(\) \\  Enlarging PCC & \(54.4\) & \(40.1\) & \(0.293\) & & \\ Reducing RD & \(15.1\) & \(29.9\) & \(0.294\) & \(2.02 10^{-}\) & \(1.02 10^{-}\) \\ Regression & \(40.3\) & \(28.1\) & \(0.292\) & & \\  One-hot Label & \(37.6\) & \(43.3\) & & & \\ Label Smoothing & \(35.0\) & \(36.1\) & & & \\  Ours (Gaussian) & \(53.7\) & \(\) & & & \\ Ours (Linear) & \(54.0\) & \(24.6\) & \(0.310\) & \(2.07 10^{-}\) & \(6.97 10^{-}\) \\ Ours (Exp.) & \(\) & \(25.6\) & & & \\  

Table 2: Our method demonstrates performance superiority over others.

Figure 5: The comparison of different PDFs: Gaussian (left), linear (middle), and exponential (right). “\(A\)” is the amplitude in each PDF function (Eqn. 3 to Eqn. 5).

supervised by a smooth PDF outperforms that supervised by a steeper one (also see the corresponding distributions in Fig. 15 of the Appendix). That consists with our intuition again.

**Our model has good generalizability compared to all other methods.** Because the collection process of the images from some diffusion models (see Appendix F) differs from the process used to build the test set of our D-Rep dataset, it is difficult to label 6 levels for them and the proposed PCC and RD are not suitable. In the Table 3, we consider a quantitative evaluation protocol that measures the average similarity predicted by a model for given \(N\) image pairs, which are manually labeled with the highest level. When normalized to a range of \(0\) to \(1\), a larger value implies better predictions. This setting is practical because, in the real world, most people's concerns focus on where replication indeed occurs. We manually confirm \(100\) such pairs for each diffusion model. We draw three conclusions: (1) Our PDF-Embedding is more generalizable compared to all zero-shot solutions, such as CLIP, GPT4-V, and DINov2; (2) Our PDF-Embedding still surpasses all other plausible methods trained on the D-Rep dataset in the generalization setting; (3) Compared against testing on SD1.5 (same domain), for the proposed PDF-Embedding, there is no significant performance drop on the generalization setting.

### Simulated Evaluation of Diffusion Models

In this section, we simulate a scenario using our trained PDF-Embedding to evaluate popular diffusion models. We select 6 famous diffusion models, of which three are commercial, and another three are open source (See Section F in the Appendix for more details). We use the LAION-Aesthetics V2 6+ dataset  as the gallery and investigate whether popular diffusion models replicate it. When assessing the replication ratio of diffusion models, we consider image pairs rated at Level 4 and Level 5 to be replications.

    &  &  & Midjo- &  &  &  & } &  \\  & & &  &  &  &  &  &  &  \\   Vision- \\ language \\ Models \\  } & SLIP  & 0.685 & 0.680 & 0.668 & 0.710 & 0.688 & 0.718 & 0.699 \\  & BLDP  & 0.703 & 0.674 & 0.673 & 0.696 & 0.696 & 0.717 & 0.689 \\  & ZeroVL  & 0.578 & 0.581 & 0.585 & 0.681 & 0.589 & 0.677 & 0.707 \\  & CLIP  & 0.646 & 0.665 & 0.694 & 0.728 & 0.695 & 0.735 & 0.727 \\  & GPT-4V  & 0.661 & 0.655 & 0.705 & 0.731 & 0.732 & 0.747 & 0.744 \\   Self- \\ supervised \\ Learning \\ Models \\  } & SimCLR  & 0.633 & 0.640 & 0.644 & 0.656 & 0.649 & 0.651 & 0.655 \\  & MAE  & 0.489 & 0.488 & 0.487 & 0.492 & 0.487 & 0.489 & 0.490 \\  & SimSiam  & 0.572 & 0.611 & 0.619 & 0.684 & 0.620 & 0.645 & 0.683 \\  & MoCov3  & 0.585 & 0.526 & 0.535 & 0.579 & 0.541 & 0.554 & 0.599 \\  & DINov2  & 0.766 & 0.529 & 0.593 & 0.723 & 0.652 & 0.734 & 0.751 \\   Supervised \\ Pre-trained \\ Models \\  } & EfficientNet  & 0.116 & 0.185 & 0.215 & 0.241 & 0.171 & 0.210 & 0.268 \\  & Swin-B  & 0.334 & 0.387 & 0.391 & 0.514 & 0.409 & 0.430 & 0.561 \\ Pre-trained & ConvNeXt  & 0.380 & 0.429 & 0.432 & 0.543 & 0.433 & 0.488 & 0.580 \\  & DeiT-B  & 0.386 & 0.478 & 0.496 & 0.603 & 0.528 & 0.525 & 0.694 \\  & ResNet-50  & 0.362 & 0.436 & 0.465 & 0.564 & 0.450 & 0.522 & 0.540 \\   Current \\ ICD \\  } & ASL  & 0.183 & 0.231 & 0.093 & 0.122 & 0.048 & 0.049 & 0.436 \\  & CNNCL  & 0.201 & 0.311 & 0.270 & 0.347 & 0.279 & 0.358 & 0.349 \\  & SSCD  & 0.116 & 0.181 & 0.180 & 0.303 & 0.166 & 0.239 & 0.266 \\ Models & EFNet  & 0.133 & 0.265 & 0.267 & 0.438 & 0.249 & 0.340 & 0.349 \\  & BoT  & 0.216 & 0.345 & 0.346 & 0.477 & 0.338 & 0.401 & 0.489 \\   Models \\ Trained \\ on D-Rep \\  } & Enlarging PCC & 0.598 & 0.510 & 0.523 & 0.595 & 0.506 & 0.554 & 0.592 \\  & Reducing RD & 0.795 & 0.736 & 0.768 & 0.729 & 0.768 & 0.785 \\  & Regression & 0.750 & 0.694 & 0.705 & 0.739 & 0.704 & 0.721 & 0.744 \\  & One-hot Label & 0.630 & 0.376 & 0.400 & 0.562 & 0.500 & 0.548 & 0.210 \\  & Label Smoothing & 0.712 & 0.568 & 0.636 & 0.628 & 0.680 & 0.676 & 0.548 \\   & Gaussian PDF & 0.787 & 0.754 & 0.784 & 0.774 & 0.774 & 0.780 & 0.776 \\  & Linear PDF & 0.822 & 0.758 & 0.798 & 0.794 & 0.782 & 0.794 & 0.790 \\  & Exponential PDF & **0.831** & **0.814** & **0.826** & **0.804** & **0.802** & **0.818** & **0.794** \\   

Table 3: The experiments for “Generalizability to other datasets or diffusion models”. The gray color indicates training and testing on the images generated by the same diffusion model.

**Evaluation results.** Visualizations of matched examples and the replication ratios are shown in Fig. 6 (Left). For more visualizations, please refer to the Appendix (Section G). We observe that the replication ratios of these diffusion models roughly range between \(10\%\) and \(20\%\). The most "aggressive" model is Midjourney  with a rate of \(20.21\%\), whereas the "conservative" model is SDXL  at \(10.91\%\). We also include an analysis of failure cases in the Appendix (Section H).

**Efficiency analysis.** Efficiency is crucial in real-world scenarios. A replication check might slow down the image generation speed of diffusion models. Our PDF-Embedding requires only \(2.07 10^{-3}\) seconds for inference and an additional \(8.36 10^{-2}\) seconds for matching when comparing a generated image against a reference dataset of \(12\) million images using a standard A100 GPU. This time overhead is negligible compared to the time required for generating (several seconds).

**Intuitive comparison with another ICD model.** In , SSCD  is used as a feature extractor to identify replication, as illustrated in Fig. 6 (Right). In comparison, our PDF-Embedding detects a higher number of challenging cases ("hard positives"). Despite visual discrepancies between the generated and original images, replication has indeed occurred.

## 6 Conclusion

This paper investigates a particular and critical Image Copy Detection (ICD) problem: Image Copy Detection for Diffusion Models (ICDiff). We introduce the first ICDiff dataset and propose a strong baseline called "PDF-Embedding". A distinctive feature of the D-Rep is its use of replication levels. The dataset annotates each replica into 6 different replication levels. The proposed PDF-Embedding first transforms the annotated level into a probability density function (PDF) to smooth the probability. To learn from the PDFs, our PDF-Embedding adopts a set of representative vectors instead of a traditional representative vector. We hope this work serves as a valuable resource for research on replication in diffusion models and encourages further research efforts in this area.

**Disclaimer.** The model described herein may yield false positive or negative predictions. Consequently, the contents of this paper should not be construed as legal advice.

Figure 6: Left: Examples of diffusion-based replication fetched by our PDF-Embedding. The accompanying percentages indicate the replication ratio of each model. Right: Examples filtered by SSCD  in . Compared to them, our results are more diverse: For example, the “Groot” generated by SDXL includes the whole body, whereas the original one features only the face; and the “Moai statuses” created by DeepFloyd IF are positioned differently compared to the original image.