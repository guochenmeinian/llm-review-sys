# Emergence of heavy tails in homogenized stochastic gradient descent

Zhe Jiao

School of Mathematics and Statistics

Northwestern Polytechnical University

Xi'an 710129, China

zjiao@nwpu.edu.cn &Martin Keller-Ressel

Institute of Mathematical Stochastics

Technische Universitat Dresden

01217 Dresden, Germany

martin.keller-ressel@tu-dresden.de

Center for scalable data analytics and artificial intelligence (ScaDS.ai), Leipzig/Dresden, Germany.

###### Abstract

It has repeatedly been observed that loss minimization by stochastic gradient descent (SGD) leads to heavy-tailed distributions of neural network parameters. Here, we analyze a continuous diffusion approximation of SGD, called homogenized stochastic gradient descent, and show in a regularized linear regression framework that it leads to an asymptotically heavy-tailed parameter distribution, even though local gradient noise is Gaussian. We give explicit upper and lower bounds on the tail-index of the resulting parameter distribution and validate these bounds in numerical experiments. Moreover, the explicit form of these bounds enables us to quantify the interplay between optimization hyperparameters and the tail-index. Doing so, we contribute to the ongoing discussion on links between heavy tails and the generalization performance of neural networks as well as the ability of SGD to avoid suboptimal local minima.

## 1 Introduction

Stochastic gradient descent (SGD) is the cornerstone of optimization in modern deep learning (cf. ). In contrast to deterministic methods, it introduces stochasticity to the optimization procedure and therefore has to be analyzed from a probabilistic viewpoint. For instance, it has been observed by Martin and Mahoney (2019); Simsekli et al. (2019); Hodgkinson and Mahoney (2021); Gurbuzbalaban et al. (2021) and others, that the distributions of neural network parameters under loss minimization by SGD are typically _heavy-tailed_. This heavy-tailed behavior has been linked to the generalization performance of neural networks: Simsekli et al. (2019) give evidence that the extreme realizations of heavy-tailed random variables allows SGD to escape local minima of the loss landscape, and Hodgkinson and Mahoney (2021) argue for a negative correlation between the parameter distributions's tail-index and the network's generalization performance.2 For these reasons, it is important to understand the origin and effects of heavy-tailed behavior of neural network parameters in SGD. An important step in this direction has been taken in , where the tail behavior of SGD iterates is characterized in dependence on optimization parameters, dimension and Hessian curvature at the loss minimum. One limitation of  is that this link is described only qualitatively, but not quantitatively. Here, we provide an alternative approach through analyzing homogenized stochastic gradient descent, a diffusion approximation of SGD introduced in . Leveraging Ito calculus for diffusion processes, we are able to provide more precise bounds and estimates of the tail behavior of SGD iterates, which we subsequently validate in numerical experiments.

### Our contribution

Our contribution to the analysis of heavy-tailed phenomena in SGD can be summarized as follows:

* We introduce a new method, namely comparison results in _convex stochastic order_ for homogenized stochastic gradient descent. These comparison results, given in Section 3 allow us to link SGD to the well-studied class of _Pearson Diffusions_ (cf. [Forman and Sorensen, 2008]) and then to obtain bounds for their tail-index.
* Contrary to [Gurbuzbalaban et al., 2021], who describe the tail-index only implicitly (observing phase-transitions between different regimes) our tail-index bounds are fully explicit. Moreover, their explicit form is validated in numerical experiments in Section 4.
* Our results suggest (skew) Student-\(t\)-distributions as surrogate for parameter distributions in neural networks under SGD, in contrast to the earlier work of [Gurbuzbalaban et al., 2021] where \(\)-stable distributions have been suggested. This proposal is validated by numerical experiments and statistical test in Section 4.
* an SDE driven by Brownian motion
- which asymptotically exhibits heavy-tailed behavior with a tail-index that, in experiments, closely matches the empirical tail index of SGD iterates on real data.

## 2 Background

### Empirical risk minimization

The general framework for training deep neural networks is to solve the problem of empirical risk minimization

\[_{x^{d}}\{L(x):=_{i=1}^{n}L_{i}(x)\},\] (ERM)

where \(L_{i}\) denotes the loss induced by the data point \(a_{i}^{d}\) with label/response \(b_{i}\), given the model's parameter vector \(x^{d}\). For our theoretical and numerical analysis of heavy-tailed phenomena we focus on the specific case of regularized linear regression. Hence, as in [Gurbuzbalaban et al., 2021], we assume a quadratic structure of \(L_{i}(x)\), setting

\[L_{i}(x)=(a_{i} x-b_{i})^{2}.\]

Including a regularization term weighted by \( 0\), we arrive at the objective function

\[L^{}(x)=L(x)+|x|^{2}=(_{i=1}^{n }L_{i}(x)+|x|^{2}),\] ( \[\] -ERM)

which is the loss function of _ridge regression_ (cf. [Hastie et al., 2009]). We arrange the training data into a design matrix \(A^{n d}\) and label vector \(b^{n}\), whose \(i\)-th row are given by \(a_{i}\) and \(b_{i}\) respectively, allowing the write (\(\)-ERM) as s

\[L^{}(x)=|Ax-b|^{2}+|x|^{2}\]

with gradient given by \( L^{}(x)=(A^{}(Ax-b)+ x)\).

### Stochastic gradient descent

The standard approach to solve the problem of empirical risk minimization in deep learning is to use stochastic gradient descent (SGD) or any of its generalizations involving momentum, adaptive learning rates, gradient rescaling, etc. (cf. [Goodfellow et al., 2016, Bottou et al., 2018]). As a first step, we consider plain SGD with constant learning rate \(\), which can be written in recursive form as

\[x_{k+1}=x_{k}- L^{}_{_{k}}(x_{k}),\] (SGD)where \( L^{}_{_{k}}(x_{k})=_{i_{k}}L^{ }_{i}(x)\) and \(_{k}\) is a batch of size \(B 1\) sampled uniformly and independently from \(\{1,,n\}\). It will be convenient to rewrite (SGD) as

\[x_{k+1}=x_{k}- L^{}(x_{k})+(x_{k}),\] (1)

where the gradient noise is given by

\[(x_{k})=-[ L_{_{k}}(x_{k})- L(x_{k})].\] (2)

Note that the gradient noise is unbiased (i.e. \((x)=0\)) with covariance matrix given by3

\[C(x):=[(x)(x)^{}]= (_{i=1}^{n} L_{i}(x) L_{i}(x)^{}-} L(x) L(x)^{}).\]

The theoretical properties of SGD can now be either analysed directly through the stochastic recurrence (1) (cf. ) or through a continuous diffusion approximation, known in the general case as _stochastic modified equation_, cf. . This approximation is obtained by recognizing (1) as the Euler-Maruyama approximation (in the small learning-rate regime) of the stochastic differential equation (SDE)

\[dX_{t}=- L^{}(X_{t})dt+)}dW_{t}\] (SME)

driven by a \(d\)-dimensional Brownian motion \((W_{t})_{t 0}\); cf. Thm. 1 in . A common further simplification is to assume that the covariance matrix \(C(x)\) is constant, yielding the Ornstein-Uhlenbeck-approximation (also known as Langevin equation) of SGD, cf. .

### Homogenized Stochastic Gradient Descent

Our analysis of SGD is based on _homogenized stochastic gradient descent_ (hSGD), introduced concurrently in  and , which is another approximation of (SME). In contrast to the Ornstein-Uhlenbeck-approximation where the covariance matrix of gradient noise is assumed constant, hSGD uses the more elaborate 'decoupling approximation'

\[C(x)L(x)^{2}L(x),\]

see  and  for a derivation. Hence, in our notation, hSGD for penalized empirical risk minimization is given by4

\[dX_{t}=- L^{}(X_{t})dt+L(X_{t}) ^{2}L(X_{t})}dW_{t}.\] (hSGD)

In the regime where \(n\) and \(d\) are simultaneously large, and under certain assumptions on the distribution of the data \(A\) and \(b\),  provide approximation guarantees of the following form: For any given \(T>0\) and \(D>0\), there is a \(C>0\), such that

\[(_{0 t T}|(x_{ tn})- (X_{t})|>d^{-/2}) Cd^{-D}\] (3)

for quadratic statistics \(:^{d}\) and when \(n d^{}\) for some \(>0\); cf. Thm. 1.3 in  for details. Further empirical evidence for the approximation quality of hSGD with respect to SGD can is also given in , altogether providing a sufficient basis for analyzing the properties of SGD through hSGD.

Furthermore, the stochastic differential equation (hSGD) can be simplified by using the reduced singular value decomposition (SVD) of the design matrix \(A\). In detail, let \(r=(A) d\), and let \(A=P Q^{}\) be the reduced SVD of \(A\), where \(Q\) is \(d\)-by-\(r\) and satisfies \(Q^{}Q=I_{r}\), \(P\) is \(n\)-by-\(r\) and satisfies \(P^{}P=I_{r}\), and

\[=\{_{j}\},_{1}_{2} _{r}>0\]

is the diagonal matrix of non-zero singular values of \(A\). We distinguish the following two cases of hSGD:* _Underparametrized hSGD_: \(Ax=b\) has no exact solution,
* _Overparametrized hSGD_: \(Ax=b\) has an exact solution,

and impose the following assumption:

**Assumption 2.1**.: In the overparametrized case, we require \(>0\), i.e. the loss function must be regularized.

It is easily verified that \(x_{*}=Q^{-1}P^{}b\) is the unique global minimum of the unregularized loss in the underparametrized case and the global minimum of smallest norm in the overparametrized case. We set \(Y_{t}=(Y_{t}^{i})_{i=1}^{r}=Q^{}X_{t}-Q^{}x_{*}\) and obtain the following system of SDEs5 for the 'centered principal components' \((Y_{t}^{1},,Y_{t}^{r})\) of (hSGD)

\[dY_{t}^{i}=-[(_{i}^{2}+)Y_{t}^{i }-_{i}]dt+}{n}[ _{j=1}^{r}(_{j}Y_{t}^{j})^{2}+]}dB_{t}^{i}\] (4)

with

\[=(_{i})_{i=1}^{r}=-^{-1}P^{}b,=b^{}(I_{n }-PP^{})b 0\]

and \((B_{t})_{t 0}\) a \(r\)-dimensional Brownian motion, obtained as an orthogonal transformation \(B_{t}=Q^{}W_{t}\) of the \(d\)-dimensional Brownian motion \((W_{t})_{t 0}\). Note that \(PP^{}b\) is the projection of \(b\) onto the column space of \(A\). Thus, in the overparametrized case, \(PP^{}b=b\) and hence \(=0\), whereas in the underparametrized case \(PP^{}b b\) and hence \(>0\). Here, our main objective is to use (hSGD) to study the distributional properties, in particular the tail behavior, of SGD iterates.

### Heavy-Tailed Distributions

We collect some relevant definitions related to heavy-tailed distributions and their tail index (cf. (Resnick, 2007; Foss et al., 2011)).

**Definition 2.2** (See Def. 1.1 in Foss et al. (2011)).: A distribution function \(F(z)\) is said to be _heavy-tailed_ (at the right end) if and only if

\[_{z}}=,s>0.\]

A real-valued random variable is said to be heavy-tailed if its distribution function is heavy-tailed.

**Definition 2.3**.: An \(^{d}-\)valued random vector \(X\) is heavy-tailed if \(u^{}X\) is heavy-tailed for some vector \(u^{d-1}:=\{u^{d}:|u|=1\}\).

**Definition 2.4**.: The _tail-index_ of an \(^{d}-\)valued random vector \(X\) is defined as

\[:=\{p 0:[|X|^{p}]<\}[0,].\]

In particular, a finite tail-index \(<\) implies heavy-tailedness of \(X\), and lower values of \(\) signify increased heaviness of tails and more extremal behavior. A tail index of \(<2\), for example, implies infinite variance and \(<1\) implies non-existence of even the mean of \(X\). Examples of heavy-tailed distributions are the lognormal distribution, the Student-\(t\)-distribution, the Pareto (power-law) distribution, and \(\)-stable distributions.

Finally, we introduce a definition related to the asymptotic behavior of stochastic processes.

**Definition 2.5**.: Let \(X=(X_{t})_{t 0}\) be a stochastic process. The _asymptotic tail-index_ of \(X\) is defined as

\[:=\{p 0:_{t}[|X_{t}|^{p}]<\}.\] (5)

### Pearson Diffusions

To analyze its tail behavior, we perform a further rescaling of (4) by setting, for \(i\{1,,r\}\),

\[Z_{t}^{i}=\{_{i}(_{i})Y_{t}^{i}, &=0,\\ }{}Y_{t}^{i},&>0,._{ i}=\{|_{i}|}{_{i}^{2}+ },&=0,\\ _{i}}{(_{i}^{2}+)},&> 0,.=\{0,&=0,\\ 1,&>0,.\] (6)

\[_{i}=(_{i}^{2}+)>0_{i}=^{4}}{2nB(_{i}^{2}+)}>0.\]

This recasts the system (4) to

\[dZ_{t}^{i}=-_{i}(Z_{t}^{i}-_{i})dt+_{i}(|Z_{t}|^ {2}+)}dB_{t}^{i}\] (7)

with \(|Z_{t}|^{2}=_{i=1}^{r}(Z_{t}^{i})^{2}\). These SDEs now have a clear structural resemblance to the system of independent one-dimensional SDEs

\[d_{t}^{i}=-(_{t}^{i}-_{i})dt+_{i}( (_{t}^{i})^{2}+)}dB_{t}^{i}\] (8)

with the only difference given by the coupling of (7) through the \(|Z_{t}|^{2}\)-term in the diffusion coefficient.6 The components of (8) are independent _Pearson diffusions_. Pearson diffusions are a flexible class of SDEs with a unified theory for statistical inference and with stationary distributions known as Pearson distributions (cf. [Forman and Sorensen, 2008]). In more detail, we obtain from [Forman and Sorensen, 2008] the following properties:

_Underparametrized hSGD (\(>0\)):_\(_{t}^{i}\) is \(\)-valued and the stationary distribution of \(_{t}^{i}\) is called Pearson's type IV distribution (or skew Student \(t\)-distribution) and has the unnormalized density

\[p_{i}(u)[1+(}}+_{i})^{2} ]^{-+1}{2}}\{_{i}(_{i}-1)( }}+_{i})\}\] (9)

with \(_{i}=_{i}^{-1}+1\).

_Overparametrized hSGD (\(=0\)):_\(_{t}^{i}\) is \((0,)\)-valued and the stationary distribution of \(_{t}^{i}\) is called Pearson's type V distribution (or inverse Gamma distribution) and has the unnormalized density

\[p_{i}(u) u^{-_{i}-1}(-(_{i}-1)}{u})\] (10)

with \(_{i}=_{i}^{-1}+1\).

In both cases, the stationary distribution is heavy-tailed with tail-index given by \(_{i}\), thus providing a first connection between the SDE-approach and the emergence of heavy-tails. This connection will be quantified and made rigorous in Section 3.

### Comparison to existing literature

We compare our approach to studying the distributional properties of SGD through (hSGD) with other continuous-time approximations: The Ornstein-Uhlenbeck-approximation uses (SME) under the additional assumption that the covariance matrix \(C(x)\) is constant. Thus, gradient noise is approximated by Gaussian noise and the Gaussian noise enters (SME) _additively_. The \(\)-stable Ornstein-Uhlenbeck-approximation of [Gurbuzbalaban et al., 2021] instead presumes (based on a generalized central limit theorem) that gradient noise is non-Gaussian and follows an \(\)-stable law. Moreover, the noise is assumed state-independent, and therefore also enters additively. In (hSGD), gradient noise is locally (i.e., conditionally on the state \(X_{t}\)) Gaussian, but _state-dependent_. The diffusion term in (7) reveals that the noise enters the SDE both multiplicatively (through the \(|Z_{t}|^{2}\)-term) and additively (through the constant \(\)). Moreover, \(=0\) in the overparametrized case, such that we observe a phase transition from a mix of additive and multiplicative noise in the underparametrized case, to purely mulitiplicative noise in the overparametrized case. We note that the importance of multiplicative noise in models of SGD dynamics is discussed in great detail in [Hodgkinson and Mahoney, 2021]. We provide a summary of the comparison of these approaches in Table 1

## 3 Theoretical results

### Moment comparison

Our first result shows that the decoupled Pearson diffusions (8) are lower bounds, in _convex stochastic order_7, to the coupled hSGD process (7). In particular, a comparison result for moments holds.

**Theorem 3.1**.: _For \(i=1,,d\), let \((Z_{t}^{i})_{t 0}\) be the components of the rescaled (hSGD) from (7) and \((_{t}^{i})_{t 0}\) be the independent Pearson diffusion from (8). Then for any \(t 0\) and convex function \(g:\) it holds that_

\[[g(Z_{t}^{i})][g(_{t}^{i})].\] (11)

_In particular this implies the ordering of \(p\)-moments_

\[[|Z_{t}^{i}|^{p}][|_{t}^{i}|^{p}]\] (12)

_for all \(p 1\)._

Note that finiteness of the expectations does not need to be assumed, i.e., the inequalities also hold if one of the expectations takes the value \(+\). Comparison results for SDEs generally require two conditions (cf. [Bergenthum and Ruschendorf, 2007]): An ordering between the drift- and diffusion-coefficients of the two SDEs, and the 'propagation-of-order'-property for one of the processes. Comparing (7) and (8), we see that the drift coefficients are identical, while the diffusion coefficients satisfy the required ordering condition \(2_{i}_{i}(|z|^{2}+) 2_{i}_{i}(z_{i}^{2}+)\) for any \(z^{r}\). The propagation-of-order property of \(\) and the full proof of Theorem 3.1 are provided in Supplement A.3.

### Upper and lower bounds for the asymptotic tail index

Since the process \((Z_{t})_{t 0}\) is a linear transformation of the hSGD process \((X_{t})_{t 0}\), it is clear that the tail behaviour of their marginal distributions - in particular the finiteness of \(p\)-moments - is identical. Hence, an application of Thm. 3.1 provides an upper bound on the asymptotic tail index of (hSGD):

**Theorem 3.2**.: _The asymptotic tail index \(\) of (hSGD) has the upper bound_

\[^{*}:=1+^{2}+)}{_{1}^{4}}.\] (13)

Under conditions on the learning rate \(\), a complementary lower bound can be derived from existing results on moment stability of SDEs, see Thm. 5.2 in [Li et al., 2019] and Supplement A.5 for details:

**Theorem 3.3**.: _Suppose that the learning rate \(\) satisfies_

\[<=:^{2}+)}{_{1}^{2} _{i=1}^{r}_{i}^{2}},\]

_then the asymptotic tail index \(\) of (hSGD) has the lower bound_

\[_{*}:=1+^{2}+)}{_{1}^{4}} -^{r}_{i}^{2}}{_{1}^{2}}.\] (14)

### Wasserstein convergence

Theorems 3.2 and 3.3 are results on the _asymptotic_ tail index, raising the question how fast convergence to the stationary distribution takes place. The next result shows that, under a suitable assumption on the learning rate, convergence takes place exponentially fast in \(2\)-Wasserstein distance:

   Model & local gradient noise & global parameter distribution & tail-index \\  Gaussian OU & Gaussian additive & Gaussian & \(+\) \\ \(\)-stable OU & Non-Gaussian additive & Non-Gaussian (\(\)-stable) & \((0,2)\) \\ homogenized SGD & Gaussian additive/multiplicative & Non-Gaussian (with Student-\(t\) as proxy) & \((1,)\) \\   

Table 1: Comparison of continuous-time models of SGD

**Theorem 3.4**.: _Suppose that the learning rate \(\) satisfies_

\[<^{}=:\{_{i=1}^{r}^{4}}{ _{i}^{2}+}\}^{-1}.\]

_Then the equation_

\[_{i=1}^{r}^{4}}{_{i}^{2}+-n/}= \]

_has a unique positive solution \(_{*}>0\), and the marginal distribution \(_{t}\) of the hSGD process \(X_{t}\) converges in \(2\)-Wasserstein distance \(_{2}\) to its unique invariant distribution \(\). Moreover, there exists \(C>0\), such that_

\[_{2}(_{t},) Ce^{-t_{*}}.\]

We remark that if the conditions of Theorem 3.4 are satisfied, then the asymptotic tail-index \(\) is necessarily greater than two, such that second moments and in particular the \(2\)-Wasserstein distance are well-defined and finite.

### Discussion of theoretical results

We compare our results to Gurbuzbalaban et al. (2021), who analyse the distributional properties of SGD directly through the stochastic recurrence (1) under the assumption of an isotropic Gaussian data distribution. In our setting, the data distribution is arbitrary, since all results are given conditional on the data matrix \(A\). On the other hand, we analyse SGD only through its diffusion approximation (hSGD) rather than directly. However, in contrast to (Gurbuzbalaban et al., 2021), we obtain the _quantitative_ and _explicit_ tail-index bounds (13) and (14), whereas Gurbuzbalaban et al. (2021) only describe the tail index through an _implicit equation_ and derive _qualitative results_ on its behaviour.

_Parameter Dependency._ Some interesting observations can be made when we consider the dependency of \(\) on several meta-parameters of the stochastic gradient descent procedure:

**Corollary 3.5**.: _The upper and lower bounds of the tail-index are increasing in the regularization parameter \(\) and batch size \(B\), and are decreasing in the learning rate \(\) and the first singular value \(_{1}\) of the data matrix \(A\)._

This result agrees with Theorem 4 in (Gurbuzbalaban et al., 2021), obtained under the assumption of an isotropic data distribution \(a_{i} N(0,^{2}I_{d})\), in all aspects, except the dependency on dimension \(d\). While Gurbuzbalaban et al. (2021) report decreasing dependency on \(d\), our tail-index bounds do not explicitly depend on dimension \(d\). Nevertheless, the two results can be reconciled as follows: Under the assumptions in (Gurbuzbalaban et al., 2021), the data matrix \(A=(a_{i})\) is random with \((A^{}A)=^{2}I_{d}\), and the product matrix \(W:=A^{}A\) follows the so-called Wishart ensemble (cf. (Wishart, 1928)). Moreover, from Theorem 1.1 in (Johnstone, 2001) it follows that for large \(d\) the maximum eigenvalue of \(W\) is

\[_{1}^{2}=^{2}[(}+1)^{2}d+r^{ }(}+1)^{}d^{}],\] (15)

where the ratio \(r=<1\) and the distribution function of the random variable \(\) is the well-known Tracy-Widom distribution of order \(1\)(cf. (Craig and Harold, 1996)). From (15), we can calculate the average of \(_{1}^{2}\) as

\[[_{1}^{2}]=^{2}(}+1)^{2}d= ^{2}(+)^{2}\]

and \(_{1}^{2}\) fluctuates around this expectation over a narrow region of width \(O(d^{})\). Substituting \(_{1}^{2}\) by its expectation in (13) and (14) we can now see that \(_{*}\) and \(^{*}\) increase in both variance \(^{2}\) and \(d\), consistent with (Gurbuzbalaban et al., 2021).

_Distributional properties._ From Theorem 3.1 we see that the skew Student-\(t\) distribution provides an asymptotic lower bound in convex order for the marginal distribution of hSGD. Empirically (see Section 4) we see that skewness is negligible and furthermore, that the Student-\(t\)-distribution not only provides a lower bound, but in fact a very good fit to the parameter distribution of SGD in general, surpassing the fit of the \(\)-stable distribution proposed in (Gurbuzbalaban et al., 2021). For this reason, we propose to use the Student-\(t\)-distribution, rather than \(\)-stable distribution, as a proxy for the parameter distribution in SGD.

Experiments

Based on the upper and lower bounds in Theorems 3.2 and 3.3, we present some experiments to illustrate the tail behavior of SGD and the factors influencing the tail index. The procedure of our experiments contains the following steps.

1. Given \([|b]\), we transform the data to be on a similar scale by the linear scaling \[A=-\{\}}{\{\}-\{ \}}.\]
2. Let \(K\) be the iteration number of SGD. We apply (SGD) to solve (ERM). The final state \(x_{K}^{d}\) is a random vector.
3. Repeat the second step \(1000\) times for different initial points and obtain \(1000\) different samples of \(x_{K}\).
4. For further distributional analysis we project \(x_{K}\) via \(=q_{1}^{}x_{K}\) on the dominant direction, given by the first right singular vector \(q_{1}\) of \(A\). Then we utilize the \(1000\) samples to obtain the empirical complementary cumulative distribution function (ccdf) of \(\).

### Datasets

_Synthetic data._ We first validate our results in the same synthetic setup used in (Gurbuzbalaban et al., 2021). All data points are drawn from isotropic Gaussian distributions, precisely, the \(i\)-th row of \(^{n d}\) contains \(_{i}^{d}(0,I_{d})\). Then given \(x^{d}(0,3I_{d})\) we draw the response vector \(b^{n}\) with components \(b_{i}(_{i}x,3)\). We set the number \(n\) of the synthetic data to be \(2000\) through our experiments.

_Real data._ In our second setup we conduct our experiments on the handwritten digits dataset from the Scikit-learn python package (cf. (Pedregosa et al., 2011)) using a random feature model proposed in (Rahimi and Recht, 2007) and a three-layer neural network. The digits dataset contains \(n=1797\) images of handwritten digits in a \(8 8\) pixel format. The pixels are stacked into vectors of length \(n_{0}=8^{2}=64\) resulting in a raw data matrix \(^{n n_{0}}\) and the class label \(b_{i}=\{0,1,,9\}\) is used as response vector. For the random feature model, we choose a dimension \(d\) and draw a random weight matrix \(W^{n_{0} d}\) having standard Gaussian entries. The feature matrix \(W^{n d}\) is given by

\[=(W}{}})^{n d},\]

where \(()\) is a rescaled ReLu activation function. The neural-network model uses 64 neurons in each hidden layer and sigmoid activation functions. The precise parameter values used for the figures are reported in Tables 4 and 5 in the supplement.

### Empirical results

To verify the heavy-tailed behavior of \(\) as well as our tail-index bounds from Theorems 3.2 and 3.3 and the distributional approximation suggested by (9), we use MLE-estimation to fit our centered data as

\[:=-\{\} t(),\]

where \(t()\) denotes a Student-\(t\)-distribution with parameter \(\) and \(\) is a fitted scaling factor.8 The QQ-plots in Figures 1, 2 (a)-(c) show that the Student-\(t\)-distribution provides a very good fit to the empirical data, validating our use of Pearson diffusions to approximate SGD. In comparison, it can be seen in Figure 1, 2 (d)-(f) that the fitted \(\)-stable distribution overestimates the heaviness of tails, in particular for the random feature model on real data. We complement these figures by Kolmogorov-Smirnov tests (cf. Chapter 4.4 in (Corder and Foreman, 2014)) testing for the goodness-of-fit of the Student-\(t\)-distribution and the \(\)-stable distribution respectively; see Tables 2, 3 for detailed results. In all three settings, the hypothesis of a Student-\(t\)-distribution is accepted, while the \(\)-stable distribution is rejected.

Moreover, in Figure 1 (g)-(i) we plot (in doubly logarithmic coordinates) the empirical ccdf of the SGD iterates \(\), together with the ccdf of the Student-\(t\)-distribution parametrized by our lower and upper bound \(_{*}\) and \(^{*}\). It can be seen that the empirical ccdf, including its tail, is nicely sandwiched between upper and lower bound, validating Theorems 3.2 and 3.3. Additionally, we once more confirm the heavy-tailed behavior of SGD iterates as already observed in (Simsekli et al., 2019; Hodgkinson and Mahoney, 2021; Gurbuzbalaban et al., 2021).

   Distribution & Dataset & In Fig. 1 & K-S statistic & \(p\)-value & decision \\  Student-\(t\) & \(\) & (a) & \(0.029\) & \(0.795>0.05\) & accept \(H_{0}\) \\ Student-\(t\) & \(\) & (b) & \(0.039\) & \(0.433>0.05\) & accept \(H_{0}\) \\ Student-\(t\) & \(\) & (c) & \(0.030\) & \(0.759>0.05\) & accept \(H_{0}\) \\  \(\)-stable & \(\) & (d) & \(0.084\) & \(0.002<0.05\) & reject \(H_{0}\) \\ \(\)-stable & \(\) & (e) & \(0.067\) & \(0.022<0.05\) & reject \(H_{0}\) \\ \(\)-stable & \(\) & (f) & \(0.070\) & \(0.015<0.05\) & reject \(H_{0}\) \\   

Table 2: Kolmogorov-Smirnov test of theoretical distributions against observed SGD iterates of the linear regression/random feature model. The null hypothesis \(H_{0}\) is that two distributions are identical, the alternative \(H_{1}\) is that they are not identical.

Figure 1: Results for linear regression/random feature model trained on datasets \(\), \(\), and \(\). (a)-(c) Quantile-Quantile plots of fitted Student-\(t\)-distribution against empirical SGD iterates; (d)-(f) Quantile-Quantile plots of fitted \(\)-stable distribution against empirical SGD iterates; (g)-(i) Comparison between ccdf of empirical data and Student-\(t\)-distribution parameterized by upper tail-index bound \(^{*}\) and lower bound \(_{*}\).

## 5 Conclusion and Limitations

This study delves into the phenomenon of heavy tails emerging in the parameters of homogenized stochastic gradient descent applied to regularized linear regression. By establishing a connection between hSGD and Pearson diffusions, we have been able to derive both explicit upper and lower bounds for the tail index of the parameter distribution. Our results reveal that heavy tails can emerge even in the presence of locally Gaussian gradient noise and provide insights into the influence of optimization hyperparameters on the tail index. However, it is essential to recognize that our analysis relies on the approximation of SGD by hSGD and is limited to the setting of linear regression with quadratic loss. Another limitation (see (14)) is that the tail-index of hSGD is lower-bounded by one, and thus hGSD can not be used to analyse 'ultra-heavy tails' with tail-index \( 1\). Future work will be devoted to extending our results to non-linear models and to providing a tighter connection between the behaviour of hSGD and the discrete-time SGD algorithm.

   Distribution & Dataset & In Fig. 2 & K-S statistic & \(p\)-value & decision \\  Student-\(t\) & \(\) & (a) & \(0.011\) & \(0.208>0.05\) & accept \(H_{0}\) \\ Student-\(t\) & \(\) & (b) & \(0.061\) & \(0.871>0.05\) & accept \(H_{0}\) \\ Student-\(t\) & \(\) & (c) & \(0.060\) & \(0.883>0.05\) & accept \(H_{0}\) \\  \(\)-stable & \(\) & (d) & \(0.078\) & \(0.001<0.05\) & reject \(H_{0}\) \\ \(\)-stable & \(\) & (e) & \(0.084\) & \(0.035<0.05\) & reject \(H_{0}\) \\ \(\)-stable & \(\) & (f) & \(0.070\) & \(0.015<0.05\) & reject \(H_{0}\) \\   

Table 3: Kolmogorov-Smirnov test of theoretical distributions against observed SGD iterates from the second layer of the three-layer neural network model. The null hypothesis \(H_{0}\) is that two distributions are identical, the alternative \(H_{1}\) is that they are not identical.

Figure 2: Results for three-layer neural network model trained on datasets \(\), \(\), and \(\). (a)-(c) Quantile-Quantile plots of fitted Student-\(t\)-distribution against empirical SGD iterates of second layer; (d)-(f) Quantile-Quantile plots of fitted \(\)-stable distribution against empirical SGD iterates of second layer.