# Towards Neuron Attributions in Multimodal

Large Language Models

 Junfeng Fang, Zongze Bi, Ruipeng Wang, Houcheng Jiang, Yuan Gao, Kun Wang

University of Science and Technology of China

{fjf,zacb2018,wrp20@21021,janghc,gaoy,wk520529}@mail.ustc.edu.cn

An Zhang

National University of Singapore

an_zhang@nus.edu.sg

Jie Shi

Huawei

shi.jie1@huawei.com

Xiang Wang

University of Science and Technology of China

xiangwang1223@gmail.com

Corresponding authors.

###### Abstract

As Large Language Models (LLMs) demonstrate impressive capabilities, demystifying their internal mechanisms becomes increasingly vital. Neuron attribution, which attributes LLM outputs to specific neurons to reveal the semantic properties they learn, has emerged as a key interpretability approach. However, while neuron attribution has made significant progress in deciphering text-only LLMs, its application to Multimodal LLMs (MLLMs) remains less explored. To address this gap, we propose a novel Neuron Attribution method tailored for **MLLMs**, termed **NAM**. Specifically, NAM not only reveals the modality-specific semantic knowledge learned by neurons within MLLMs, but also highlights several intriguing properties of neurons, such as cross-modal invariance and semantic sensitivity. These properties collectively elucidate the inner workings mechanism of MLLMs, providing a deeper understanding of how MLLMs process and generate multi-modal content. Through theoretical analysis and empirical validation, we demonstrate the efficacy of NAM and the valuable insights it offers. Furthermore, leveraging NAM, we introduce a multi-modal knowledge editing paradigm, underscoring the practical significance of our approach for downstream applications of MLLMs. Our code is available at https://github.com/littlelitlenine/NAM_1.

## 1 Introduction

As Large Language Models (LLMs) demonstrate impressive capabilities [1; 2; 3; 4], demystifying their internal mechanisms becomes increasingly vital, particularly in applications emphasizing fairness, trust, and ethical decision-making [5; 6; 7]. To interpret LLMs, "neuron attribution" stands out as a pivotal approach. This method involves attributing text outputs to individual model components (_e.g.,_ neurons and hidden layers) to reveal the knowledge and linguistic properties they learn [5; 8; 9; 10; 11; 12]. Such insights not only facilitate tasks like model editing and pruning [13; 14; 15], but also offer a deeper understanding of how LLMs internalize knowledge. For instance, leading neuron-attribution studies [16; 17; 13; 14; 18; 19] suggest that this capability of internalization may predominantly originate from their Feedforward Neural Networks (FFNs).

Recently, the rapid development of multi-modal large language models (MLLMs)  is sparking the interest of interpretability . Here we focus on the MLLMs that can perceive, generate texts and images simultaneously. Scrutinizing prior studies, we can summarize two common components beyond the LLM base: image encoding and generating modules. Specifically, the image encoding module projects the input image into the representation space of the base LLM; hereafter, the image generating module generates image outputs conditioned on the representations given by the base LLM. Take GILL  as an example. As shown in Figure 1 (a), it hires OPT  as the LLM base, CLIP Vit-L  with a cross-modal projector as the image encoding module, and Stable Diffusion  with another projector as the image generating module.

While this expanded capacity endows GILL with versatility suitable for a variety of downstream tasks, it concurrently presents challenges for interpretation, particularly concerning neuron attributions. Specifically, we outline these challenges through three progressive points:

* **Source of Attribution: Semantic Noise.** As shown in Figure 1 (b), for semantics like dog, current methods typically attribute the output to neurons directly in text-only LLMs . However, in MLLMs, attributing the entire generated image to neurons directly might result in inaccuracies. As shown in Figure 1 (c), when GILL is tasked with drawing a dog, the generated image might contain other semantic elements like lawn, introducing noise and distorting attribution.
* **Process of Attribution: Inefficiency.** Leading attribution methods typically rely on gradients  or causal effects , requiring extensive forward/backward propagation processes, which are inherently time-consuming and storage-intensive. The added complexity of encoding and generation modules in MLLMs further exacerbates this challenge.
* **Results of Attribution: Intermingled Neurons.** In text-only LLMs, attributing the concept dog involves identifying neurons crucial for outputting the word "dog", termed **T-neurons**. In contrast, MLLMs also require identifying neurons crucial for image generation, called **I-neurons**. As illustrated in Figure 1 (c), the distribution of T-neurons and I-neurons differs for the same concept, leading to conflicting results that complicate further analysis and applications.

In sight of this, we introduce a new **N**euron **A**ttribution paradigm tailored for **MLLMs, termed **NAM**, to reveal the modality-specific semantic properties learned by neurons within the FFN layers. Specifically, to address the above challenges, NAM undertakes the following efforts:

* **Image Segmentation for Semantic Noise:** As shown in Figure 1 (d), NAM employs the image segmentation model to distinguish regions containing the target semantics from other noisy semantic areas, and attributes these regions to the neurons, rather than the entire image, to ensure accuracy.
* **Activation-based Scores for Inefficiency:** Drawing inspiration from prior studies on neuron activations , NAM introduces a new attribution score that relies on neuron activations, eliminating the need for additional forward/backward propagation or gradient calculations.
* **Modality Decoupling for Intermingled Neurons:** NAM assigns modality-specific attribution scores to neurons to prevent cross-modal disturbances during attributions. This paradigm facilitates the decoupling analysis of T-neurons and I-neurons, as depicted in Figure 1 (d).

Furthermore, based on the empirical results of NAM, we reveal several intriguing neuron properties within MLLMs. These properties collectively elucidate the inner workings of MLLMs, enhancing our

Figure 1: Illustration of neuron attribution methods for interpreting LLMs. (a) The paradigm of GILL; (b) current attribution methods tailored for text-only LLMs; (c) the challenges of extending current attribution methods to MLLMs; (d) the paradigm of our NAM. Best viewed in color.

understanding of their capacity to process and generate multi-modal content. Interestingly, among these insights, a pivotal finding exhibits that when generating multi-model content for the same semantics (_e.g.,_ the word "dog" & an image of a dog), the crucial neurons (_i.e.,_ T-Neurons & I-Neurons) are typically not identical. This distinction underscores the complex nature of neurons within MLLMs, and highlights the necessity of neuron attribution across modalities. Additionally, by applying NAM to enhance image editing tasks, we further underscore the significance and potential applications of our NAM for MLLM community.

## 2 Preliminary

**Transformer-Based LLMs.** An autoregressive transformer language model \(G:\) operates over the vocabulary \(V\). It receives a token sequence \(\) and generates a probability distribution \(=[y_{1},y_{2},...,y_{|V|}]\) to predict the next token [29; 13]. Each token is represented as a series of representations \(^{l}^{H}\) in \(l\)-th layer, where \(^{0}\) is the embedding of the token in \(\). The model's final output, \(=_{}(^{L})\), is derived from the last representation \(^{L}=[h_{1}^{L},h_{2}^{L},...,h_{H}^{L}]^{}\) in layer \(L\) using the unembedding matrix \(_{}\). Figure 2 (a) exhibits a visualization of how \(^{l}\) are computed within layer \(l\). The representation in each layer results from the combination of the global attention \(^{l}\), the local MLP output \(^{l}\), and the representation \(^{l-1}\) from the previous layer. Formally,

\[^{l}=^{l}+^{l-1}+^{l},\ ^{l}= _{}^{l}\,(_{}^{l}\, (^{l}+^{l-1})),\] (1)

where \(_{}^{d H}\) and \(_{}^{H d}\) are the first and second linear layer in FFN with the dimensionality of the FFN's intermediate layer \(d\); \(\) and \(\) are rectifying and normalizing nonlinearity. For further background on transformers, we refer to . Additionally, focusing on the the \(k\)-th neuron \(u_{k}^{l}\) in the \(l\)-th FFN layers, we simplify the definition by considering its activation \(a_{k}^{l}\) as:

\[^{l}=[a_{1},a_{2},...,a_{d}]^{}=(_{}^{l}\,(^{l}+^{l-1}))\] (2)

**MLLMs.** Here, we take GILL as an example to illustrate a common paradigm of MLLMs. As shown in Figure 2 (a), GILL incorporates the following modifications on the above text-only LLMs: (1) In addition to the textual prompt, the input token sequence \(\) also includes the encoding of the input image produced by the image encoding module. (2) The representation \(^{L}\) of the last hidden layer is utilized as input to the image generation module, facilitating conditional image generation.

## 3 Method

This section delineates the implementation of NAM within MLLMs. Specifically, Section 3.1 and 3.2 introduce the attribution process for image and text outputs, respectively; In Section 3.3, we

Figure 2: Illustration of neuron attribution methods for interpreting LLMs. (a) The paradigm of current attribution methods tailored for text-only LLMs. (b) The challenges of extending current attribution methods to MLLMs. (c) The paradigm of our NAM.

explore a significant application of NAM, _i.e.,_ editing images generated by MLLMs. To illustrate these processes, we utilize GILL , a representative MLLM capable of image generation. For a detailed introduction to GILL, please refer to Appendix B.

### Neuron Attribution for Image Generation

We first detail how to attribute the output images to the specific neurons within FFN layers. Retrospecting the challenges highlighted in Introduction, using the MLLM to generate images of a specific concept (_e.g.,_ dog) often results in outputs that include extraneous, noisy elements (_e.g.,_ lawn). To mitigate the negative effects of semantic noise on neuron attribution, we propose a two-step approach to extract I-Neurons. Specifically, (1) the **first step** focuses on attributing the output of the image generation module (_i.e.,_ images) to the input of the image generation module (_i.e.,_ last representation \(^{L}\)), and (2) the **second step** endeavors to attribute the input of the image generation module to the specific neurons. Next, we provide detailed descriptions of these two steps.

#### 3.1.1 STEP1: Attribution From Images to Representation \(^{L}\)

The purpose of this step is to attribute the image to \(^{L}^{H}\). That is, to identify the contribution of each element in \(^{L}\) for image generation. We define these contribution scores as \(^{H}\). As shown in Figure 2 (b), NAM acquires \(\) by sequentially executing the following processes:

* Given an image generated by prompting MLLM with the semantics dog, NAM first employs the leading segmentation model, EVA02 , to identify regions specifically related to dog. This is crucial for minimizing interference from extraneous semantics, such as lawn in the background;
* Subsequently, NAM utilizes the advanced attribution algorithm of the diffusion model, Diffuser-Interpreter , to access the relevance of each dimension in the input of the image generation module to the dog region in the generated image.
* Ultimately, by normalizing these relevance scores to \((0,1)\), we obtain the importance scores \(=[r_{1},r_{2},...,r_{H}]^{}\) of \(^{L}=[h_{1},h_{2},...,h_{H}]^{}\)_w.r.t_ the target semantics in the output image.

Due to the established applications of EVA02 and Diffuser-Interpret, we provide detailed introductions in Appendix B. Furthermore, it is worth mentioning that NAM can be transferred to any other modality by utilizing the (1) semantic segmentation algorithms and (2) attribution algorithms of generation modules tailored for other modalities (_e.g.,_ audio and video). After obtaining \(\) by these advanced modality-specific algorithms, the subsequent attribution steps are universal across all transformer-based MLLMs.

#### 3.1.2 STEP2: Attribution From Representation \(^{L}\) to Neuron \(u_{k}^{l}\)

This step involves attributing the representation \(^{L}\) in the last layer to the specific neuron \(u_{k}^{l}\) within the FFNs of the base LLM. To this end, NAM aims to trace each neuron's contribution to \(^{L}\), and identify the neurons with significant contributions as I-Neurons for the semantic of interest. Hence, a fair and efficient contribution scoring method is crucial.

**Direct Contributions through Residual Stream.** Current methods for scoring contributions often rely on gradients, such as the product of gradients and activations  or the integration of gradients . However, these methods are computationally intensive, particularly for large-scale models with extensive parameters. In sight of this, drawing inspiration from prior studies on neuron activation [18; 19], we first introduce a new attribution score that relies on the neuron activation \(a_{k}^{l}\). Specifically, we first try to disassemble and deduce the generation procedure of \(^{L}\) by expanding \(^{L}\) as follows:

\[^{L}&=^{L}+^{L-1}+^{L}=_{l=1}^{L}^{l}+^{0}+_{l=1}^{ L}^{l}\\ &=_{l=1}^{L}_{}^{l}^{l}+ ^{0}+_{l=1}^{L}^{l}=_{l=1}^{L}_{k=1}^{d}a_{k} ^{l}(_{}^{l})_{k}+^{0}+_{l=1}^{L} ^{l},\] (3)

where \((_{}^{l})_{k}^{H}\) is the \(k\)-th column of the weight matrix \(_{}^{l}\) corresponding to the index of neuron \(u_{k}^{l}\), as shown in Figure 2 (b). Note that the first term of Equation (3) reflects the direct contribution of the neuron \(u_{k}^{l}\) to the last representation \(^{L}\), _i.e.,_ the contribution through the residual stream  of the base LLM.

Hence, we employ \(a_{k}^{l}(_{}^{l})_{k}\) as the indicator for the neuron \(u_{k}^{l}\)'s contribution to \(^{L}\).

Furthermore, by integrating **R** in Section 3.1.1, we can establish a complete attribution pipeline (_i.e.,_ targeted semantic region in generated image \(\) representation \(^{L}\)\(\) neuron \(u_{k}^{l}\)). Recall that **R** has assigned the contribution for each dimension in \(^{L}\), the neuron \(u_{k}^{l}\)'s contribution \(s_{k}^{l}\) can be defined as \(a_{k}^{l}(_{}^{l})_{k}\) weighted by elements in **R**. That is, \(s_{k}^{l}=a_{k}^{l}(_{}^{l})_{k}^{}\), as illustrated in Figure 2 (b).

**Contribution Score Considering Indirect Influence.** While \(s_{k}^{l}\) quantifies the neuron \(u_{k}^{l}\)'s direct contribution to \(^{L}\) through the residual stream, it does not account for all influential factors. Specifically, it overlooks the indirect contributions that neurons make through the attention mechanisms within subsequent FFN layers. Supporting evidence exhibited in Appendix B verifies that this oversight might lead to a bias. To address this issue, and in line with our objective to eschew complex computations like gradient, we implement a heuristic optimization of the current indicator \(s_{k}^{l}\). Specifically, we employ the relative magnitude of neuron activation as another indicator to identify neurons that may have a significant indirect contribution to \(^{L}\) - contribution which \(s_{k}^{l}\) might overlook.

Furthermore, considering the computation of \(s_{k}^{l}\) already incorporates \(a_{k}^{l}\), the contributions reflected by these two indicators may overlap. To prevent redundancy from summing or multiplying these two metrics, we utilize the maximum function in our final score design:

\[_{k}^{l}=\{^{l}}}{_{l=1}^{L}_{k=1}^{d}e ^{s_{k}^{l}}},^{l}}}{_{l=1}^{L}_{k=1}^{d}e^{a_{k}^{l}}}\},\] (4)

where the normalization operation ensures fair competition between \(s_{k}^{l}\) and \(a_{k}^{l}\). Note that our experiments in Section 4 have shown that this combined scoring approach is more effective than utilizing \(s_{k}^{l}\) or \(a_{k}^{l}\) alone, verifying their complementary nature. By computing contributions following Equation (4) for various semantics across all layers, NAM identifies neurons that consistently demonstrate the highest contributions to the generated images. These neurons, distinguished by their significant roles, are designated as I-neurons responsible for targeted semantics in MLLMs.

### Neuron Attribution for Text Generation

We then focus on how to acquire the neuron's contribution to the text outputs. Similar to the derivation process of contribution score for image output, here we first focus on the contribution for output **y** through the residual stream. Specifically, we expand \(^{L}\) as follows:

\[=_{}(^{L}+^{L-1}+^{L})=_{l=1}^{L}_{}_{}^{l} \,^{l}+_{}(^{0}+_{l=1}^{L}^{l}).\] (5)

According to a commonly used assumption for analyzing the internal mechanisms of LLMs, representations at any layer within the language models can be transformed into a distribution over the token vocabulary \(V\) using the output embeddings . Hence, \(_{}_{}^{l}^{|V| H}\) can be considered as the new unembedding matrix at the end of the residual stream, and \(^{l}\) contributes to the model output distribution **y** over the vocabulary through \(_{}_{}^{l}^{l}\), as shown in Figure 2 (c).

**Refined Contribution of Individual Neuron.** We further disassemble Equation (5) to refine individual neuron \(u_{k}^{l}\)'s contribution to the output word. Specifically, denoting \(p\) as the index of the word "dog" in the vocabulary \(V\), we have:

\[y_{p}=(_{})_{p}_{}^{l}^{l}+( _{})_{p}(^{0}+_{l=1}^{L}^{l})= _{k=1}^{d}a_{k}^{l}(_{})_{p}(_{}^{l})_{ k}+(_{})_{p}(^{0}+_{l=1}^{L}^{l}),\] (6)

where \((_{})_{p}^{1 d}\) is the \(p\)-th row of \(_{}\). According to Equation (6), the neuron \(u_{k}^{l}\)'s contribution \(c_{k,p}^{l}\) to the \(p\)-th word "dog" on vocabulary can be obtained by \(c_{k,p}^{l}=a_{k}^{l}(_{})_{p}(_{}^{l})_{ k}\), as illustrated in Figure 2 (c). Furthermore, we would like to encourage the semantic specificity of the identified crucial neurons - that is, only preserving a single semantic concept with the maximum contribution, while discarding other semantics. Formally, for the semantics of \(p\)-th word on vocabulary, the neuron \(u_{k}^{l}\)'s contribution \(s_{k}^{l}\) can be defined as:

\[p^{*}=*{arg\,max}_{p}c_{k,p}^{l},\ s_{k}^{l}=\{[] {cc}c_{k,p}^{l}&p=p^{*},\\ 0&.\] (7)By substituting \(s^{l}_{k}\) to Equation (4), we can acquire the final attribution score of neuron \(u^{l}_{k}\) for text output. The neurons that consistently exhibit the highest contributions are then designated as T-neurons.

### Image Editing Enhanced by NAM

Knowledge editing methods based on neuron attribution have already been explored for text outputs [13; 14; 33; 34]. Here, we focus on how to leverage attribution results to facilitate knowledge editing of images. This objective requires replacing some semantics (_e.g.,_ dog) with another semantics (_e.g.,_ cat). To this end, we leverage the I-Neurons identified by NAM for image editing through a straightforward, training-free approach. Specifically, we first construct the set of I-Neurons, \(\), for the semantics like dog. Then, we collect the positions \((l,k)\) of the neurons \(u^{l}_{k}\) in \(\), and construct the set of these position indices, \(\). For \((l,k)\), we add a perturbation \((^{l}_{})_{k}\) to \((^{l}_{})_{k}\) following:

\[=\{(^{l}_{})_{k}(l,k)\}\] \[(^{l}_{})_{k}=_{}||_{(l,k)}a^{l}_{k}( ^{l}_{})_{k}\,,(}^{L}-^{L})^{ }||_{2}+\,||_{(l,k)}(^{l}_{})_{k}^{}||_{2},\] (8)

where \(^{L}\) and \(}^{L}\) is the last representation of the base LLM when generating the image of dog and cat, respectively; \(\) serves as a trade-off parameter. In Equation (8), the first term aims to facilitate a shift in the image generation module's input from \(^{L}\) to \(}^{L}\), while the second term is the \(_{2}\) norm constraint for preventing drastic edits that might affect images containing other semantics. According to this method, NAM can be utilized to enable simple and efficient images editing, underscoring the significance and potential applications of the NAM for MLLMs2.

## 4 Experiment

In this section, we aim to validate the effectiveness of NAM from three aspects:

* What is the distribution of T/I-Neurons identified by NAM?
* What properties do the T/I-neurons identified by NAM have? How to verify that the T/I-neurons identified by NAM are more critical compared to the neurons identified by baseline methods?
* Can the T/I-Neurons identified by NAM facilitate the image editing within MLLMs?

### Investigation Setup

**Target Models & Datasets.** Our research focuses on GILL  and NEXT-GPT , two representative MLLMs with the capability of image generation. All experiments are conducted on the Common Objects in Context (COCO) , a large-scale object detection, segmentation, and captioning dataset including 80 object categories and five captions per image to conduct our experiments. Due to space limitations, we only present the experimental results on GILL in this section. The remains and the detailed implementations, such as the setting of hyper-parameters, can be found in Appendix B.

**Baselines.** We collect five advanced neuron attribution methods across three categories (gradient-, activation-, and causality-based attribution). Specifically, their abbreviations and the attribution scores they employ are: (1) AcT: neuron activation ; (2) AcU: The product of activation and the unembedding matrix, focused on the dimension corresponding to the output word ; (3) GraD: The gradient of the output dimension corresponding to the output word _w.r.t_ activation. (4) GraT: The product of the gradients and activation ; (5) GraI: The integral of the gradients ; (6) CE: The causal effect of activation on outputs [13; 14]. See detailed description in Appendix B.1. For NAM and baselines stand and their role in rich literature, please refer to Appendix A (_i.e.,_ Related Work).

### RQ1: Distribution of T/I-Neurons

We first focus on the distribution of T/I-neurons identified by NAM. Specifically, we randomly select 1000 images from the COCO dataset. Then, we feed each image to GILL individually and instruct GILL to generate a similar image. The distributions of T/I-neurons are exhibited in Figures 3 (a) and (b), while Figure 3 (c) illustrates the distribution of intersections of T- and I-neurons. These results demonstrate the following observations:

**Observation 1:** Within MLLMs, the crucial neurons for the text and image output containing specific semantics predominantly occur in the middle and high layers of the base LLM. Note that this observation is consistent with the previous works involving neuron attributions within LLMs . Additionally, the similar distribution of T and I neurons suggests that the formation time of semantic concepts across different modalities in MLLMs may be consistent.

**Observation 2:** Figure 3 (c) reveals a partial overlap between T and I neurons. However, it also pronunces distinctions between them. This finding substantiates the claim presented in the Introduction of this paper: even for the same semantics, critical neurons are modality-specific within MLLMs.

### RQ2: Properties of T/I-neurons & Effectiveness of NAM

We then explore the following properties of T/I-neurons through comprehensive quantitative and qualitative experiments: **Semantic Relevance**, **Cross-Sample Invariance**, and **Concept Specificity**. Additionally, by comparing these properties with those of neurons identified by various baselines, we validate the effectiveness of our NAM. Note that we only present the results of the best-performing baseline for each class, and remains are shown in Appendix B.

#### 4.3.1 Semantic Relevance

Following previous studies , we treat the unembedding matrix and the second linear layer matrix in the FFN as a projection from neuron activation to the probability distributions of the vocabulary. Based on this, words with the average highest probability can be regarded as the relevant

  
**Output** & **Segmentation** & **Category** & **Method** & **Location** & **Semantics** \\   &  & Grad & L28.04786 & \{'dog’,'shark’, 'cat’, 'bird’\} \\  & & & Act & L30.13868 & \{'animals’, 'animal’, 'animal’, 'Animals’\} \\  & & & CE & L27.014262 & \{'vehicle’, 'trucks’, 'ears’, 'boats’\} \\  & & & **NAM** & **L23.05318** & \{'horses’, 'horse’, 'Horses’\} \\   & & & Grad & L29.014374 & \{'laming’, 'farm’, 'farms’, 'ag’\} \\  & & & Act & L26.019257 & \{'animal’, 'amimals’,'vetrain’,'vetrain’\} \\  & & & CE & L28.01208 & \{'Kinkuism’, 'cliff’, 'Nano’, 'Vaults’\} \\  & & & **NAM** & **L23.05318** & \{'horses’, 'horse’, 'Horses’\} \\   &  & Grad & L28.012056 & \{'child’, 'child’, 'children’,'male’\} \\  & & & Act & L42.012845 & \{'dogs’, 'dog’, 'dog’, 'canine’\} \\  & & & CE & L25.03655 & \{'those’, 'Those’, 'that’, 'this’\} \\  & & & **NAM** & **L24.101701** & \{'dogs’, 'Dog’, 'Dogs’, 'pets’\} \\   & & & Grad & L26.01135 & \{'dagott’, 'ped’, 'adopting’, 'adopting’\} \\  & & & Act & L30.113868 & \{'animal’, 'animal’, 'animal’, 'Animals’\} \\  & & & CE & L31.11315 & \{'wecks’, 'chickens’, 'compos’, 'trash’\} \\  & & & **NAM** & **L24.012845** & \{'dogs’, 'dog’, 'Dog’, 'canine’\} \\   &  & Grad & L26.013972 & \{'d’ving’, 'dagott’, 'dalling’\} \\  & & Act & L28.011438 & \{'boa’, 'car’, 'phone’,'vehicle’\} \\  & & & L26.102335 & \{'jna’,'sat’,'ser’, 'canine’\} \\  & & & **NAM** & **L26.02903** & \{'ship’,'ships’,'ship’\} \\   &  & Grad & L27.18984 & \{'bush’, 'tree’, 'brush’,'shr’\} \\  & & Act & L25.02539 & \{'swim’,'swim’,'swim’,'medv’\’\} \\   & & CE & L25.05113 & \{'borne’,'sign’, 'box’, 'SIGN’\} \\   & & **NAM** & **L28.010626** & \{'ship’,'ships’,'sea’, 'ocean’\} \\   

Table 1: The semantics of T/I-neurons with the highest attribution scores identified by different attribution methods, when the output of MLLMs containing the targeted semantics. (L/U\(k\)) denotes the \(k\)-th neuron at layer \(l\). For each method, we report semantics with top-4 probabilities.

Figure 3: Distribution of (a) I-neurons, (b) T-neurons, and (c) intersection and subset of I-neurons and T-neurons per layer identified by NAM, chosen by different number of neurons with top scores on average. Best viewed in color.

semantics of neurons, as shown in Table 1. Note that the results of AcU are not exhibited since its attribution score is exactly this probability. Furthermore, to explore the semantic relevance of the neurons quantitatively, we calculate the consistency between the semantics of neurons and the input/output images employing CLIPScore . BERTScore , MoverScore , and BLEURT  are also employed to quantify their consistency with (1) input image's caption provided by the dataset and (2) output image's caption given by GPT . Table 2 exhibits the average quantified results. According to Table 1 and 2, we have the following observation:

**Observation 3:** The semantics of T/I-neurons identified by NAM align more closely with the input/output images and their captions, while the other attribution methods typically identify the neurons that are hardly correlated with the targeted semantics. The quantitative results share a similar tendency, confirming the high semantic relevance of T/I-neurons and the effectiveness of our NAM.

#### 4.3.2 Cross-sample Invariance

For different text/image outputs containing the same semantics, the T/I-neurons identified by the attribution methods shall be consistent. To quantify this consistency, we instruct GILL to describe and generate images for the same semantics ten times, and collect the set of T/I-neurons each time. We then calculate the proportion of neurons that appeared in all ten sets as the quantification of

    &  &  \\ 
**Method** & **Grad.** & **Act.** & **Ca.** & \({}^{}\)**CLipScore** & \({}^{}\)**CLipScore** & **BERTScore** & **MoveScore** & **BLEURT** \\  CE & & & ✓ & \(0.264_{ 0.015}\) & \(0.251_{ 0.022}\) & \(0.273_{ 0.029}\) & \(0.257_{ 0.019}\) & \(0.040_{ 0.005}\) \\ GraI & ✓ & ✓ & & \(0.239_{ 0.018}\) & \(0.244_{ 0.020}\) & \(0.276_{ 0.032}\) & \(0.296_{ 0.030}\) & \(0.039_{ 0.005}\) \\ GraD & ✓ & & & \(0.378_{ 0.047}\) & \(0.396_{ 0.032}\) & \(0.457_{ 0.027}\) & \(0.436_{ 0.030}\) & \(0.064_{ 0.008}\) \\ GraT & ✓ & ✓ & & \(0.425_{ 0.010}\) & \(0.422_{ 0.029}\) & \(0.486_{ 0.018}\) & \(0.477_{ 0.036}\) & \(0.072_{ 0.009}\) \\ AcT & & ✓ & & \(0.556_{ 0.037}\) & \(0.594_{ 0.016}\) & \(0.624_{ 0.057}\) & \(0.653_{ 0.054}\) & \(0.139_{ 0.013}\) \\ AcU & & ✓ & & \(0.543_{ 0.051}\) & \(0.624_{ 0.057}\) & \(0.618_{ 0.054}\) & \(0.609_{ 0.038}\) & \(0.135_{ 0.014}\) \\  NAM & & ✓ & & \(0.562_{ 0.054}\) & \(0.637_{ 0.047}\) & \(0.640_{ 0.039}\) & \(0.657_{ 0.048}\) & \(0.148_{ 0.013}\) \\    &  \\ 
**Method** & **Grad.** & **Act.** & **Ca.** & \({}^{}\)**CLipScore** & \({}^{}\)**CLipScore** & **BERTScore** & **MoveScore** & **BLEURT** \\  CE & & & ✓ & \(0.228_{ 0.017}\) & \(0.219_{ 0.026}\) & \(0.245_{ 0.033}\) & \(0.250_{ 0.021}\) & \(0.044_{ 0.003}\) \\ GraI & ✓ & ✓ & & \(0.230_{ 0.021}\) & \(0.235_{ 0.021}\) & \(0.259_{ 0.027}\) & \(0.278_{ 0.037}\) & \(0.035_{ 0.004}\) \\ GraD & ✓ & & & \(0.370_{ 0.012}\) & \(0.377_{ 0.026}\) & \(0.432_{ 0.040}\) & \(0.409_{ 0.041}\) & \(0.058_{ 0.006}\) \\ GraT & ✓ & ✓ & & \(0.432_{ 0.038}\) & \(0.394_{ 0.032}\) & \(0.453_{ 0.042}\) & \(0.437_{ 0.038}\) & \(0.068_{ 0.008}\) \\ AcT & & ✓ & & \(0.547_{ 0.043}\) & \(0.580_{ 0.051}\) & \(0.597_{ 0.056}\) & \(0.623_{ 0.061}\) & \(0.128_{ 0.013}\) \\ AcU & & ✓ & & \(0.501_{ 0.061}\) & \(0.601_{ 0.054}\) & \(0.559_{ 0.039}\) & \(0.548_{ 0.052}\) & \(0.137_{ 0.013}\) \\  NAM & & ✓ & \(0.558_{ 0.053}\) & \(0.613_{ 0.052}\) & \(0.611_{ 0.048}\) & \(0.630_{ 0.056}\) & \(0.144_{ 0.014}\) \\   

Table 2: Consistency between the neuron’s semantics and the images/captions. _Grad._, _Act._ and _Ca._ denote gradient-, activation- and causality-based methods, respectively. \({}^{}\) and \(\) represent the CLIPScore _w.r.t_ input and output images. We use background \(\) highlights the best performance.

Figure 4: Cross-sample invariance and semantic specificity of T/I-neurons. (a) exhibits invariance by calculating the average ratio between T/I-neurons’ subset and intersection across different text/image output; (b) quantifies specificity by showing: 1. the number of neurons crucial for specific semantics solely and 2. the average number of neurons whose probability of being crucial to other semantics is lower than a certain value. Best viewed in color.

cross-sample invariance. The average invariance Figure 4(a) presents the average invariance for different concepts. Specifically,

**Observation 4:** NAM outperforms all baselines by an average of 16.83% _w.r.t_ cross-sample invariance across all semantics. This demonstrates that NAM extracts the critical neurons for the targeted semantics across samples, effectively filtering out the neurons sensitive to sample-specific noise.

#### 4.3.3 Semantic Specificity

Neurons that are crucial for specific semantics should not be indiscriminately crucial across others. Therefore, we study the neuron's specificity in this part. We identify the Top-500 T/I-neurons for the specific semantics. Then, we show (1) the number of neurons that are crucial for specific semantics solely and (2) the average number of neurons whose probability of being crucial to other semantics is lower than \(\{10\%,15\%,,45\%\}\) in Figure 4 (b). The results of the three best-performing methods are exhibited here. These results highlight that:

**Observation 5:** The T/I-neurons identified by our NAM are specialized and not commonly sensitive across different semantics, verifying their specificity across the semantics.

### RQ3: Image Editing Enhanced by NAM

Lastly, we aim to edit the images generated by MLLMs through perturbing I-neurons identified by NAM, as outlined in Section 3.3. Table 3 exhibits the pre- and post-editing semantics, the selected images for collecting \(^{L}\) and \(}^{L}\), and the magnitude of the perturbations. Furthermore, for fair comparisons, we also perturb I-neurons identified by baselines to achieve similar editing results. According to Table 3 we can find that:

**Observation 6:** NAM-enhanced editing methods can not only replace the original semantics with the target semantics precisely within the outputs of MLLMs, but also necessitate minimal perturbations. Specifically, the perturbation it added is 40.2% less than the baselines on average, and nearly 15% less than the best baseline, underscoring its significance and potential applications for MLLMs.

## 5 Limitations & Future Work

This study provides new insights into interpreting MLLMs, enhancing the understanding of their inner working mechanize. However, while our experiments thoroughly investigated the neuron properties within GILL and NExTGPT, they did not extend to a broader range of models. Additionally, although the proposed attribution method can be transferred to any other modality, as demonstrated in Section 3.1.1, our experiments focused on text and image outputs solely. Looking forward, we plan to incorporate more MLLMs and modalities into our research, and streamline our attribution method to eliminate the reliance on external interpreters. By expanding the scope of our study and refining our method, we aim to uncover more valuable insights that will benefit the MLLM community.

## 6 Conclusion

We propose NAM, a novel neuron attribution method tailored for MLLMs. Specifically, NAM is tailored for multi-modal attribution, revealing the modality-specific semantic properties learned by neurons within the FFN layers.To address the challenges of extending attribution methods from text

  
**Pre-editing** & **Post-editing** & **Method** & **Value (\(\))** \\   & _“girl”_ & CE & 0.726 \\  & & Gral & 0.625 \\  & & Gral & 0.471 \\  & & GraT & 0.502 \\  & & AcT & 0.390 \\  & & AcU & 0.362 \\  & & **NAM** & **0.350** \\   & _“cat”_ & CE & 0.601 \\  & & Gral & 0.540 \\  & & Gral & 0.458 \\  & & GraT & 0.321 \\  & & AcT & 0.259 \\  & & & **NAM** & **0.255** \\  & & & **NAM** & **0.350** \\   & _“apple”_ & CE & 0.493 \\  & & Gral & 0.360 \\   & & Gral & 0.357 \\   & & GraT & 0.401 \\   & & AcT & 0.328 \\   & & & **NAM** & **0.316** \\   

Table 3: Results of image editing. The Value represents the \(_{2}\) norm of the perturbation added to the I-neurons, demonstrating that NAM necessitates minimal perturbations for the editing.

only LLMs to MLLMs, NAM first employs a leading image segmentation model to remove the noisy semantics, then proposes a new attribution score to eliminate the need for additional forward/backward propagation or gradient calculations. Based on NAM, we highlights several intriguing properties of neurons, elucidating the inner workings mechanism of MLLMs.