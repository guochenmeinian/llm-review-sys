# Entropy testing and its application to testing Bayesian networks

Clement L. Canonne

University of Sydney

clement.canonne@sydney.edu.au &Joy Qiping Yang

University of Sydney

qyan6238@uni.sydney.edu.au

###### Abstract

This paper studies the problem of _entropy identity testing_: given sample access to a distribution \(p\) and a fully described distribution \(q\) (both discrete distributions over a domain of size \(k\)), and the promise that either \(p=q\) or \(|H(p)-H(q)|\), where \(H()\) denotes the Shannon entropy, a tester needs to distinguish between the two cases with high probability. We establish a near-optimal sample complexity bound of \((/+1/^{2})\) for this problem, and show how to apply it to the problem of identity testing for in-degree-\(d\)\(n\)-dimensional Bayesian networks, obtaining an upper bound of \((2^{d/2}n^{3/2}/^{2}+n^{2}/^{4})\). This improves on the sample complexity bound of \((2^{d/2}n^{2}/^{4})\) from , which required an additional assumption on the structure of the (unknown) Bayesian network.

## 1 Introduction

Entropy is a fundamental information theory notion, which quantifies the amount of "uncertainty" a given random variable carries. Since its introduction by Shannon, this notion has found myriads of applications, and is central - among others - to compression and coding, probability, electrical engineering, and learning theory.

As a result, the task of _estimating_ the Shannon entropy of a discrete random variable (or, equivalently, its probability distribution) from samples has naturally emerged, starting (in Computer Science) with the work of  which considered _multiplicative_ approximations. _Additive_ approximation of the entropy (within \(\)) was then considered in a series of papers , culminating with the work of , which establishes the optimal sample complexity, \(+k}{^{2}} \), where \(k 1\) is the domain size.

While the resulting sample complexity is _sublinear_ in the domain size \(k\), it is only so by a mere logarithmic factor. In some settings, paying this near-linear dependence in the amount of data necessary is impractical, typically in the large-domain regime (e.g., for high-dimensional data, where \(k\) is exponential in the dimension); moreover, it may even be _unnecessary_. Specifically, one may not be concerned so much about the (approximate) value of the entropy of a distribution, but rather about whether it is above a threshold, or differs from that of a given purported model.

It is this latter task we introduce and consider in our work, which can be seen as a variant of the standard _identity testing_ question from distribution testing: given a reference known hypothesis distribution \(q\) over a domain of size \(k\), and i.i.d. samples from an unknown distribution \(p\), what is the sample complexity of testing whether \(p\) is equal to \(q\), or their entropies differ significantly? And, crucially, _is this testing task more sample-efficient than that of estimating \(H(p)\)?_

**Entropy Identity testing:** Given a reference distribution \(q\), parameter \(>0\), and samples from an unknown \(p\), what is the cost of deciding (with high probability) whether \(p=q\) vs. \(|H(p)-H(q)|>\), with correct probability at least \(2/3\)?

Note that in the case where \(q\) is the uniform distribution over the domain, this task is equivalent to distinguishing between \(H(p)= k\) and \(H(p)< k-\).

Our main contribution is to show that the testing question can indeed be performed much more efficiently than the estimation one, at least for most parameter regimes. Specifically, we establish the following theorem:

**Theorem 1.1**.: _The sample complexity of entropy identity testing is \(O(/+^{2}(k)/^{2})\). Moreover, this is nearly tight: \((/+^{2}k/^{2})\) samples are necessary in the worst case._

Interestingly, this differs both from the _estimation_ task (which, as discussed before, has a near-linear dependence on the domain size \(k\)) but also from identity testing _in total variation distance_, which has sample complexity \((/^{2})\) (see Section 1.1).

**Application: Identity testing for Bayesian networks.** As an application of Theorem 1.1, we derive an efficient algorithm for identity testing (in total variation distance) for maximum in-degree \(d\) Bayesian networks (shorten as degree-\(d\) Bayes net in the remaining of the paper):1

**Theorem 1.2** (Informal; see Theorem 3.1).: _There is an algorithm which, given sample access to a degree-\(d\) Bayes net \(p\) and the full description of a reference degree-\(d\) Bayes net \(q\) (both over \(\{0,1\}^{n}\)), takes \((n^{3/2}}{^{2}}+}{^ {4}})\) samples from \(p\), and distinguishes between \(p=q\) and \(d_{}(p,q)\)._

Prior to this, the best known sample complexity upper bound for this task  was quadratically worse in both \(n\) and \(\), and further required an assumption on the underlying graph structure of both \(p\) and \(q\). We emphasize that (1) our result improves on the sample complexity of the learning baseline for \(d(n/)\), and on its computational efficiency; and (2) compared to the previous testing results, removes strong structural assumptions which considerably limited their applicability. We elaborate on this in the next section.

### Related work

As previously discussed, entropy estimation has received a considerable amount of interest from computer scientists, information theorists and statisticians . Entropy is also a key example of _symmetric property_ (invariant to relabeling of the domain) , and has been considered in other settings as well, e.g., the quantum case  and the memory-limited setting . Estimation of some generalizations of Shannon entropy, such as the family of Renyi entropies, also have been studied .

Over the years, sample complexity of identity testing for discrete distribution has been intensively studied and essentially settled . In high dimensions, however, the square root dependence of the sample complexity on the domain size means that most identity testing tasks of interest require sample complexity exponential in the dimension. Moreover, this curse of dimensionality extends to a large range of distribution testing problems [1, Theorem B.1]. As such, many turn to the study of testing distributions under additional natural structural assumptions, such as graphical models:  look at identity testing for product distributions (degree-\(0\) Bayes nets) and give the optimal bound of \((})\), where \(||\) is the alphabet size of each variable (rather than binary alphabet studied in our paper).  study testing Ising models, obtaining sample complexity bounds that are \((n/)\); ,  give tight results to identity testing and closeness testing for a variety of constant in-degree Bayes nets, which also gives polynomial sample complexity bounds.

However, the testing algorithms provided in  and  are not fully satisfactory, as they require some strong assumptions on Bayes nets. Specifically, [1, Theorem 21] assumes thatthe topological ordering of the two Bayes nets are the same, and shows that under this assumption \(O(2^{d/2}n^{2}/^{4})\) samples are sufficient.2[3, Theorem 17] makes the further stringent restriction that the reference Bayes net has to be _balanced_, i.e., that the conditional probabilities are all bounded away from 0 and 1; moreover, it also requires every parental configuration to be bounded from 0, and that the structure of the unknown Bayes net be a subset of that of the reference one. The result of [3, Theorem 4.2] combined with the Hellinger tester from [13, Theorem 1] implies that, under the assumption that \(p\) and \(q\) share the same factorization structure (i.e., their associated DAGs are the same or one is a subgraph of the other), then this problem is solvable in \((2^{d/2}n/^{2})\) samples. While this latter sample complexity is near-optimal (in some regime3), in view of the \((2^{d/2}n/^{2})\) lower bound obtained in [1, Theorem 4.1], the factorization structure requirement considerably limits the applicability of the algorithm.

One can also compare our result to the _learning_ results on Bayesian networks, as any learning algorithms enables testing as well (the "testing-by-learning" baseline). It is known  that learning degree-\(d\) Bayes nets can be done with \((2^{d}n/^{2})\) samples, without any structural assumptions. Our testing result improves on this sample complexity as long as \(n^{2}/^{4} 2^{d}n/^{2}\) and \(2^{d/2}n^{3/2} 2^{d}n\), i.e., for \(d(n/)\); moreover, it is worth noting that the known learning algorithms are computationally inefficient (running in time \(n^{O(dn)}\) via an enumeration of all possible underlying graph structures ), and this is believed to be inherent . In contrast, our algorithm runs in time \((n^{d},1/)\).

### Techniques overview

**Testing in entropy.** A first idea is to use the conversion between total variation (TV) distance and entropy difference to reduce this problem to identity testing in \(\): When \(d_{}(p,q) 1/2\), then \(|H(p)-H(q)| d_{}(p,q)}(p,q)}\)[3, Lemma 2.7].4 This gives an upper bound of \(O(^{2}(k/)}{^{2}})\), which is already better than the sample complexity of estimation: \(O(+k}{^{2}})\) for the parameter \(k\). However, it is not clear whether the quadratic dependence on \(\) is necessary: indeed, the "hard instances" for TV testing (the Paninski construction ), small perturbations around the uniform distribution which have TV distance \(\) from uniform, actually only have entropy \( k-(^{2})\). The \((/^{2})\) uniformity testing lower bound from these hard instances thus only implies an \((/)\) entropy identity testing lower bound!

A next natural idea is to strengthen the lower bound. However, it then becomes clear that the Paninski  construction cannot be improved: as just mentioned, when its \(\) distance to the uniform distribution is around \(()\) its entropy difference to it is only \(()\) (giving an \((/)\) lower bound). Moreover, this is not a coincidence: when the reference distribution \(q\) is uniform, we are able to get a matching upper bound using [13, Algorithm 1], upon noticing that

\[H(p)= k-d_{}(p\|u_{k}),\] (1)

which implies \(d_{}(p\|u_{k})= k-H(p)\), where \(u_{k}\) is the uniform distribution on \([k]\) and \(d_{}\) denotes the Kullback-Leibler divergence. Interestingly, a completely different hard instance, against a very much non-uniform reference distribution, does yield the second term of our lower bound, \((^{2}k/^{2})\).

Inspired by these two different lower bounds, we can generalize (1) by defining \(\) as the set of "not too small probability elements under \(q\)", and then observing (looking ahead, using the inequality (7)) that

\[|H(p_{})-H(q_{})||d_{}(p_{}\|q_{})|+|_{i}(p_{i}-q_{i})}|\] (2)

where \(H(p_{})\) is the "entropy" of the sub-distribution restricted to the set \(\). In particular, this hints that one could solve the general problem by testing if either of the two terms on the right-hand-sideis large. The name of the game now is to (i) choose the threshold for \(\) (i.e., what does it mean for an element to have "not too small probability under \(q\)"), and (ii) have algorithms to test whether these two quantities are noticeably large.

Let us focus on how to test the first term of (2). If \(_{i}q_{i}()\), we can adapt and use an algorithm of  to efficiently test \(d_{}(p\|q)\) vs. \(p=q\). In addition, if \((1/q_{i})\) is bounded, then in fact, estimating the second term to \(O()\) is possible as well. Thus it is natural to wonder if we can afford to neglect the region where \(q_{i}\). Indeed, the impact on entropy is at most \(O((k/))\) if we are to remove regions with at most \(O()\) as mass. Thus, by adjusting the appropriate threshold, we can still detect difference in entropy even if we only test on elements with greater than \(/k\) masses, where \(=\).

The problem then becomes to check if \(p\) puts more than \(100\) mass in \(}=\{i[k]:q_{i}</k\}\), which costs \(O(1/)=O((k/)/)\) samples. If it does, then it cannot be the case that \(p=q\); we can reject. After this stage, both \(p(}),q(}) O()\). To move forward, we need to check the influence on entropy: \(H(p)\) and \(H(q)\). By Jensen's inequality and monotonicity of \(f(x)=x\) when \(x<\), we have

\[_{i}}p_{i}} p(})})}.\]

Therefore, the impact on entropy will be at most \(O()\). Setting \(=\), this becomes \(O()\), which gives us the room to check if \(|H(p_{})-H(p_{})| 100\) or \(p_{}=q_{}\).

Testing Bayesian networks.Similar to [1, Theorem 4.2],5 the identity testing algorithm is straight-forward: check all every \(i[n]\), if \(p_{X_{i},_{i}^{G}}=q_{X_{i},_{i}^{G}}\) or is one of them is far apart, where \(q\) is Markov with respect to \(G\) (\(q\) factorizes according the DAG \(G\)). The main technical part is to show that the distance is "subaddiitve" when \(p\) and \(q\) share no common structure, but are close to sharing a common factorization structure (this can be thought of as a relaxation of [1, Theorem 4.2]; refer to Lemma 3.3 for details). As a consequence of "subadditivity", if \(p\) and \(q\) are far in distributional distance (differed from , our work opted to test in KL divergence restricted on subsets with large enough density), then it would imply that one of the local distance between \(p\) and \(q\) is sufficiently large. This allows us to reduce from global testing to local testing.

Another key aspect is checking whether \(p\) and \(q\) are close to sharing common structure. More specifically, whether \(d_{}(p\|p_{G})\) is small, where \(p_{G}\) is the projection of \(p\) unto \(q\)'s DAG \(G\). Here, we establish a connection between entropy closeness and structure closeness. In particular, we show that if every local entropy (involving subsets of size \(d+1\), where \(d\) is the bound on maximum in-degree) between \(p\) and \(q\) is close, then this means that they must approximately share the same structure (see Lemma 3.4). The intuition behind the connection is that if all tests pass, then we can conclude that \(p\) and \(q\) are close in local entropy and thereafter, we can utilize entropy of \(q\) to learn the graphical structure  of \(p\) (which uses no additional samples).

At a high level, our algorithm first check if \(p\) and \(q\) roughly share the same structure via a proxy check of local entropy tests. If all local entropy tests pass, then we can show that there exists \(i[n]\) such that local KL restricted on subset with large enough mass is greater than \((}{n})\). A subsequent identity test with \(^{2}\)-test  suffice.

Preliminaries and notation.The (Shannon) entropy \(H\) of a discrete distribution \(p\) supported on \([k]\) is given by:

\[H(p)=-_{i[k]}p_{i} p_{i}.\]

The conditional entropy \(H(p_{X} p_{Y})\) for \(X\) supported on \(\), and \(Y\) on \(\), defined by the joint distribution \(p_{X,Y}\), can be written as

\[H(p_{X} p_{Y})=-_{x,y}p(x,y)=H(p_{X,Y})-H(p_{Y}).\] (3)We adopt the entropy notation for a sub-probability vector \(H(q_{})=_{i}q_{i}}\). Throughout this paper, we will use \(e\) as base of the log and of the entropy. We will use \(\) for variable assignment. We adopt the standard \(O()\), \(()\) and \(()\) asymptotic notation and use \(\) to hide any polylogarithmic factors in the argument. We will use various metrics or divergences on probability distributions: Kullback-Leibler (\(d_{}\)), Hellinger (\(d_{}\)), chi-squared (\(d_{^{2}}\)), and total variation (\(d_{}\)). We denote \(p_{}\) as restricting \(p\) onto the elements in \(\), and we denote distributional distances restricting on \(\) as follows: \(d_{}(p_{},q_{})=_{i}p_{i} }{q_{i}}\). \(_{}(p_{},q_{})=}(}-})^{2}}}\). For a set \(\), we write \(p()=_{i}p_{i}\). We also have the following inequality [13, Proposition 1]:

\[d_{}(p_{},q_{})\,_{}(p_{},q_{})}(q_{i}-p_{i})+d_{}(p_{},q_{})} }(p_{},q_{})}.\] (4)

A distribution \(p\) supported over the hypercube \(\{0,1\}^{n}\) is a Bayesian network if its probability mass function satisfies the factorization associated with \(G\), a directed acyclic graph (DAG):

\[p(x_{1},,x_{n})=_{i=1}^{n}p(x_{i}|_{i}),\] (5)

and \(_{i}\) is the set of parents of \(X_{i}\) in \(G\); and we say that \(p\) is Markov with respect to DAG \(G\). In section 3, slightly abusing notation, we use \(p_{G}\) to denote a projection of a Bayes net \(p\) to a DAG \(G\) (which it may or may not be Markov with respect to; see Definition 3.2). We work in the Poissonized setting (see, e.g., [1, Appendix C]) - instead of drawing \(N\) samples directly from \(p\), we draw \(Y(N)\) samples from \(p\), where \((N)\) denotes the random variable distributed as the Poisson distribution with parameter \(N\). The Poissonized and usual sampling settings are equivalent for constant probability of failure, up to a (small) multiplicative factor in the sample complexity.

## 2 Near-optimal entropy testing

We prove Theorem 1.1, establishing the sample complexity upper and lower bounds separately.

An \(O}{}+(k)}{ ^{2}}\) upper bound

We will prove the following theorem:

**Theorem 2.1**.: _There is an algorithm (Algorithm 1) which, given \(n\) samples from a discrete distribution \(p\), the full description of a reference distribution \(q\), both over \([k]\), and parameter \(>0\), distinguishes between \(p=q\) and \(|H(p)-H(q)|\) with probability at least \(2/3\), as long as_

\[n c_{1}(}{}+(k)}{^{2}})\]

_and \(c_{2} k\), for some absolute constants \(c_{1},c_{2}>0\). Moreover, the algorithm runs in time linear in the number of samples \(n\) and the domain size \(k\)._

The proof will rely on the two following claims and Lemma 2.4, which is a straightforward extension of [13, Lemma 2]. Their proofs are deferred to Appendix B. Throughout, we let \(:=\), and \(:=i[k] q_{i}}\), as in Algorithm 1.

**Claim 2.2**.: _Let \(\) be any set such that \(p(})</2\). Then, if \(|H(p_{})-H(q_{})|\), we must have (i) \(d_{}(p_{}\|q_{})\) or (ii) \(|_{i}(p_{i}-q_{i})(})|\)._

**Claim 2.3**.: _Let \(\) be the empirical estimator for an unknown discrete distribution \(p\) supported on \([k]\), based on \((m)\) samples, where \(m=(k)}{^{2}}\); assume that \(d_{^{2}}(p_{},q_{})/8\) and \(p(})+q(}) 4=\),6 then_

\[[|_{i}(p_{i}-_{i})} |].\]

**Lemma 2.4**.: _Let \(:=\{i[k] q_{i}\}\). Let \(m_{2} 16384\{},}{ }\}\) be the number of samples used to compute \(Z_{2}\). Then \([Z_{2}]=m_{2}d_{^{2}}(p_{},q_{})\). Moreover, if \(d_{^{2}}(p_{},q_{})\), then \([Z_{2}](m_{2})^{2}\). If \(d_{^{2}}(p_{},q_{})\), then \([Z_{2}] O([Z_{2}]^{2})\)._

Proof of Theorem 2.1.: We prove the statement by analyzing Algorithm 1. First, note that excluding the set of \(}\) (elements with small mass), can change the value of \(H(q)\) by at most \(/8\): indeed, by Jensen's inequality (\(f(x)= x\) is concave) and \(x\) being monotonically increasing in \((0,1/e)\),

\[H(q_{}})=_{i}q_{i}}  q(})}|}{q(})}= (),\]

when \( 1/e\). Similarly, if \(p(}) 3\), we have that \(H(p_{}})\). Therefore,

\[|H(p)-H(q)|  |H(p_{})-H(q_{})|+|H(p_{}} )-H(q_{}})|\] \[ |H(p_{})-H(q_{})|+|H(p_{}} )|+|H(q_{}})|\] \[ |H(p_{})-H(q_{})|+.\]

For Line 4, we prove the following: with probability at least \(99/100\), if \(Z_{1} 2\), then \(p(})\); and if \(Z_{1}<2\), then \(p(})<3\) (this is a standard technique; see e.g., [1, Fact 2.2].) For the sake of completeness we include the full derivation in the Appendix A.

**After Line 4 of Algorithm 1.** We conclude from the above that

1. \(\) still has sufficient entropy gap to test on: \(|H(p_{})-H(q_{})|\).
2. With probability at least \(99/100\), when \(p=q\), it will not be rejected in Algorithm 4 of Line 4; and once it is pass through this stage, we have \(p(}) 3\).

**Completeness: when \(p=q\).**

* We have that \(d_{^{2}}(p_{},q_{})=0\), and via Lemma 2.4, we know that \([Z_{2}]=0\) and \([Z_{2}]}m_{2}^{2}^{2}\). By Chebyshev's inequality, \[[|Z_{2}-[Z_{2}]| 2[Z_{2}]}],[Z_{2} 2m_{2}+[Z_{2}]] ;\] and we have \([Z_{2}m_{2}]\).

* On the other hand, by Claim 2.3, setting \(m_{3}=(k)}{^{2}}\), we have that with probability at least \(99/100\), \[Z_{3}=|_{i}(_{i}-q_{i})}| =|_{i}(_{i}-p_{i})}| .\] Therefore, with probability at least \(1--=>\), the tester will accept.

**Soundness: when \(|H(p)-H(q)|\).** If \(p(}) 3\) then \((}) 2\) with probability \(99/100\), and the algorithm will output Reject. We proceed assuming \(p(}) 3\) and recall Item ii. from before, we have \(|H(p_{})-H(q_{})|\). By Claim 2.2, we have that either \(d_{}(p_{},q_{})\) or \(|_{i}(p_{i}-q_{i})(1/q_{i})| \). We apply Lemma 2.4, setting \(=/k\) and \(m_{2} 65536/\).

* If \(d_{}(p_{}\|q_{})\), with (4) and \((3/2) k/\), we have \[-3+d_{}(p_{},q_{})_{i}(q_{i}-p_{i})+d_{}(p_{ },q_{}) d_{^{2}}(p_{},q_{ }),\] which by Lemma 2.4, and our setting of \(m_{2}\) and \(\), implies \([Z_{2}]([Z_{2}])^{2}\) and \([Z_{2}]=m_{2} d_{^{2}}(p_{},q_{}) m_{2}\). By Chebyshev, \[[|Z_{2}-[Z_{2}]| 2[Z_{2}]} ][Z_{2}m_{2} ].\]
* On the other hand, if it is the case that \(|_{i}(p_{i}-q_{i})(1/q_{i})| \), by Claim 2.3, setting \(m_{3}=140800^{2}(k)/^{2}\), with probability at least \(~{}99/100\), \[  |_{i}p_{i}}-q_{i}}|\] \[ |_{i}(p_{i}-_{i})}|+ |_{i}(_{i}-q_{i})}|\] \[ +|_{i}(_{i}-q_{i}) {1}{q_{i}}|.\] We have that \(Z_{3}=|_{i}(_{i}-q_{i})}| \) and thus with probability at least \(1--=\), the following will happen, the tester will reject: either \(p(}) 3\), and it is rejected at Line 4 of Algorithm 4, or it passes and \(p(}) 3\) and \[Z_{2}m_{2}Z_{3},\] and will be rejected. This concludes the proof. 

**Remark 2.5**.: _We note that we can slightly improve the sample complexity of Theorem 1.1 (specifically, improving on the \(\) term), at the price of a more complicated algorithm, by adding thresholds \(^{}=\), \(^{}=\), and considering separately the elements in \(^{}=\{i:q_{i}(/k,^{}/k]\}\), \(^{}=\{i:q_{i}(^{}/k,^{}/k]\}\); specifically, by grouping them in groups, and "merging" each group to get a "new" element with larger probability. For the sake of clarity, we defer this improvement to Appendix D._

### An \((/+^{2}k/^{2})\) lower bound

The \((/+^{2}k/^{2})\) lower bound comes from the combination of Lemma 2.6 and Lemma 2.7. We obtain Lemma 2.6 through the classical hard instance used for uniformity testing  and a simple conversion between TV distance and entropy difference gives the result. We note that distributions close to uniform distribution actually have smaller entropy difference (uniform distribution is quite special: having the highest entropy of \( k\)). Indeed, replacing the uniform distribution with a slightly biased distribution, we obtain another hard instance for Lemma 2.7, using the classical Le Cam's two-point method.

**Lemma 2.6**.: _With fewer than \(c_{3}/\) samples from \(p\), no tester can distinguish between \(p=q\) and \(|H(p)-H(q)|\) with probability higher than \(2/3\), where \(c_{3}>0\) is an absolute constant._

**Lemma 2.7**.: _With fewer than \(c_{4}^{2}k/^{2}\) samples from \(p\), no tester can distinguish between \(p=q\) and \(|H(p)-H(q)|\) with probability higher than \(2/3\), where \(c_{4}>0\) is an absolute constant._

## 3 Application to identity testing for Bayes nets

We now provide an application of our main entropy identity testing theorem, to obtain an improved "standard" identity testing algorithm for Bayesian networks:

**Theorem 3.1**.: _Given sample access to an in-degree \(d\) Bayes net \(p\) and full description of in-degree \(d\) Bayes net \(q\), Algorithm 2 takes_

\[C(nd^{3/2} n}{ ^{2}}+n^{2} n}{^{4}}+n ^{3/2} n}{^{2}})\]

_samples to test between \(p=q\) or \(_{}(p,q)()\), where \(C>0\) is an absolute constant. Moreover, the algorithm runs in time polynomial in \(n^{d}\) and \(1/\)._

Before proceeding to the analysis of our algorithm, we require the following definitions.

**Definition 3.2**.: _A projection of a Bayes net \(p\) on \(\{0,1\}^{n}\) into a DAG \(G\) is denoted \(p_{G}\), and is defined by its probability mass function (PMF) as follows:_

\[p_{G}(X_{1},,X_{n})=_{i=1}^{n}p(X_{i}_{i}^{G}),\]

_where \(_{i}^{G}\) is the set of parents of \(X_{i}\) in \(G\). Abusing the notation in the context of Bayesian networks, we refer to \(p_{X_{i},_{i}}\) or \(p_{X_{i},_{i}}(x_{i},_{i})\) as the marginal distribution of \(p\) on the subset \(\{X_{i},_{i}\}\)._

Denote \(:=_{i=1}^{n}_{i}\), where \(_{i}:=\{x\{0,1\}^{n}:q_{X_{i},_{i}^{G}}(x_{i}(x),_{i} ^{G}(x))(}{2^{d+1}n^{2}(n/ )})\}\). This gives us the property that marginalization over \(X_{i}=x_{i},_{i}^{G}=_{i}^{G}\) works nicely as we include elements only based on its local property (as long as \(q_{X_{i},_{i}}\) is large enough). And \(q\) is Markov w.r.t. \(G\). We use \((x_{i},_{i})_{i}^{}\), where \(_{i}^{}=\{x^{}\{0,1\}^{|_{i}^{G}|+1}:q_{X_{ i},_{i}^{G}}(x^{})(}{2^{d+1}n^{2} (n/)})\}\). Let \((a,B)_{i}^{}\), we have that as long as \((x_{i}(x),_{i}(x))=(a,B)\), then \(x_{i}\) and vice versa, which means that

\[=_{i=1}^{n}_{i}=_{i=1}^{n}\{x\{0,1\}^ {n}:(x_{i}(x),_{i}^{G}(x))_{i}^{}\}.\]We will check if \(p_{X_{i},_{i}^{G}}(}^{}_{i})(^{2} /(n^{2}(n/)))\) and reject early if true; this takes \(O((n/) d(n)}{^{2}})\) samples for all tests to be correct via a union bound. After passing this test, we can conclude that

\[p(})=_{x_{i=1}^{n}}_{i}}p(x) _{x}_{1}}p(x)=_{x^{}}_{1}^{}}p_{X_{1},_{1}^{G}}(x^{})=p_{X_{1},_{1}^ {G}}(}^{}_{1}) O(}{n^{2} (n/)}),\]

where we marginalize over everything other than \((X_{1},_{1}^{G})\) in the third step.

Similarly, we can upper bound \(q(}) q_{X_{i},_{1}^{G}}(}^{ }_{i}) O(^{2}/(n^{2}(n/)))\). Abusing the notation slightly, we denote \(p_{G;}\) as the distribution obtained by projecting \(p\) onto \(G\) (which gives \(p_{G}\)) and then restricting the distribution \(p_{G}\) to take elements in \(\).

We will need the following Lemma 3.3 whose proof is deferred to Appendix E.

**Lemma 3.3**.: _Suppose \(d_{H}^{2}(p,q)(^{2})\); \(d_{}(p\|p_{G}) O(^{2})\); \(p(})}{n(n/)}\) ; \( i[n],p(}^{}_{i}) }{n^{2}(n/)}\), where \(^{}_{i}\) is defined above, and \(q\) is Markov with respect to \(G\), then we have_

\[_{i=1}^{n}d_{}(p_{X_{i},_{i}^{G};^{}_{i}} \|q_{X_{i},_{i}^{G};^{}_{i}})(^ {2}).\]

_Therefore testing \(d_{}(p_{X_{i},_{1}^{G};^{}_{i}}\|q_{X_{i},_{ 1}^{G};^{}_{i}})}{n}\) over all \(i\) suffices to detect this case._

In addition, to connect entropy testing to Bayes net testing, we will need the following Lemma 3.4, whose proof will be deferred to Appendix E. And so if we can show that all local entropies between Bayes nets \(p\) and \(q\) are sufficiently close, then this implies that \(p\) must be close to \(q\)'s \(\ G\) and \(q\) must also be close to \(p\)'s \(\ G^{}\).

**Lemma 3.4**.: _Let \(p\) and \(q\) be two max in-degree-\(d\) Bayes nets supported on \(\{0,1\}^{n}\) such that for every subset \(L\{X_{1},,X_{n}\}\) of size \(d+1\), the following holds:_

\[|H(p_{L})-H(q_{L})| O(}{n}).\]

_Suppose \(p\) is Markov w.r.t. \(G^{}\) and \(q\) Markov w.r.t. \(G\). Then we have that_

\[d_{}(p\|p_{G}) O(^{2})d_{}(q\|q_{G^{ }}) O(^{2}).\]

Proof of Theorem 3.1.: We show the result by analyzing Algorithm 2. By Theorem 1.1, the sample complexity for entropy testing on any subset \(L\) of size (dimension) \(d\) or \(d+1\), is

\[O(2^{d/2}n/^{2}+d^{2}n^{2}/ ^{4})\,.\]

To guarantee the success of every tests employed in the algorithm, we increase the sample complexity of each test by an extra \(O((n^{d+1}))=O(d n)\) factor to boost their success probability to \(1-}\) (via a standard majority vote technique), which will allow us to use a union bound over all tests as there are at most \(n^{d+1}\) subsets with size \(d+1\). For this step, the sample complexity will be

\[O((n}{^{2}}+ {d^{2}n^{2}}{^{4}})d n).\]

With this in hand, we will proceed with the analysis under the event that every entropy test performed is correct (which by the above argument happens with high probability). If distribution \(p\) manages to pass all the entropy tests, it must satisfy the following:

\[|H(p_{L})-H(q_{L})|}{n},\] (6)

for every subset \(L\) of size \(d+1\) for the latter, and every subset \(L\) of size \(d\) or \(d+1\) for the former. From here, in principle, we can perform structural learning of \(p\) through \(H(q_{L})\), which then gives us an approximated DAG \(\) of \(p\) and we can check \(_{}(p_{},q_{})\). Unfortunately, structural learning of Bayes nets is known to be computationally hard in many settings , and so this would lead to a computationally inefficient algorithm.

Instead, we argue that this (learning) step can be bypassed entirely: the intuition of the argument is to view structure learning for Bayes net as an optimization problem; and any assignment \(x\) to the two optimization problems (structure learning of \(p\) and \(q\)) satisfy \(f_{1}(x)=f_{2}(x) O(^{2})\) due to their local entropy being close7 - this means that an optima \(x_{1}\) for \(f_{1}\) satisfies \(_{x}f_{1}(x)=f_{1}(x_{1}) f_{2}(x_{1})-^{2} _{x}f_{2}(x)-^{2}\) and vice versa (optima \(x_{2}\) for \(f_{2}\)).

Applying Lemma 3.4, we have that \(d_{}(p\|p_{G}) O(^{2})\) where \(G\) is the DAG \(q\) is Markov with respect to. With this at hand, we continue onto the KL testing part. The algorithm will check if \(p_{X_{i},_{i}^{G}}(}_{i}^{})( ^{2}/(n,(1/)))\) and reject early if it is true (this costs \(O(})\) samples) and then check for every \(i[n]\),

\[d_{}(p_{X_{i},_{i}^{G};_{i}^{}}\|q_{X_{i},_{ i}^{G};_{i}^{}})}{n}\;\;\;\;d_{}(p_{X_{i},_{i}^{G};_{i}^{}}\|q_{X_{i}, _{i}^{G};_{i}^{}})=0.\]

Recalling (4), if the former case holds, we have

\[d_{^{2}}(p_{X_{i},_{i}^{G};_{i}^{}}\|q_{X_{i},_{i}^ {G};_{i}^{}}),_{i}^{G}}( {}_{i}^{})-q_{X_{i},_{i}^{G}}(}_{i}^{ })}_{ O(^{2}/n)}+d_{}(p_{X_{i},_{i}^{G };_{i}^{}}\|q_{X_{i},_{i}^{G};_{i}^{}})= (}{n})\]

the bound on the first term following from the algorithm's check on Line 10. Using Lemma 2.4, we can perform the corresponding check for \(i[n]\), and after a union bound over \(n\) tests, noting that \(q(x_{i},_{i}^{G})}{2^{d+1}n^{2}(n/ )}\) when restricted on \(_{i}^{}\), the sample complexity is

\[O((}{2^{d+1}n^{2}(n/ )}}{n})}+} n }{^{2}}) n)=O(n^{3/2}}{ ^{2}} n).\]

Following this, we look at the two cases:

* If \(p=q\), then with high probability, \(p\) will pass all entropy tests, all \(\) local tests and the tester will accept.
* If \(}(p,q)\), either it fails one of the entropy tests. If it does pass the entropy test, then we must have that \(d_{}(p\|p_{G}) O(^{2})\) by (26). Then following Lemma 3.3 and Lemma 2.4, the tester will reject.

In total, the sample complexity is:

\[O((n}{^{2}}+ n^{2}}{^{4}})d n+n^{3/2}}{ ^{2}} n).\]

This concludes the proof of the theorem. 

## 4 Conclusion and open problems

In this paper, we study a variant of distribution testing problem in terms of entropy difference; we give nearly tight upper and lower sample complexity bounds for the problem. We subsequently apply our entropy testing algorithm to identity testing of Bayes nets, which unlike prior works, makes merely the necessary assumptions (the bound on the in-degree of the Bayes nets).

**Future directions.** We believe the _closeness_ (two-sample) testing variant of the problem (testing if two unknown distribution \(p\) and \(q\) are the same or far in terms of entropy difference) could also be interesting; and, notably, has connections to other distribution testing problems: first, it should lead to a natural solution to closeness testing of Bayes nets via ideas in this paper. Second, solving the closeness entropy testing problem give another path to testing independence in terms of mutual information (studied in  and also covered in ), a notion closely related to entropy.