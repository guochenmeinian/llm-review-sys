# The High Line: Exact Risk and Learning Rate Curves of Stochastic Adaptive Learning Rate Algorithms

Elizabeth Collins-Woodfin

McGill University

elizabeth.collins-woodfin@mail.mcgill.ca&Inbar Seroussi

Tel-Aviv University

inbarser@tauex.tau.ac.il&Begona Garcia Malaxechebarria

University of Washington

begogar9@uw.edu&Andrew W. Mackenzie

McGill University

andrew.mackenzie@mail.mcgill.ca Elliot Paquette

McGill University & Google DeepMind

courtney.paquette@mcgill.ca

Corresponding author

###### Abstract

We develop a framework for analyzing the training and learning rate dynamics on a large class of high-dimensional optimization problems, which we call the high line, trained using one-pass stochastic gradient descent (SGD) with adaptive learning rates. We give exact expressions for the risk and learning rate curves in terms of a deterministic solution to a system of ODEs. We then investigate in detail two adaptive learning rates - an idealized exact line search and AdaGrad-Norm - on the least squares problem. When the data covariance matrix has strictly positive eigenvalues, this idealized exact line search strategy can exhibit arbitrarily slower convergence when compared to the optimal fixed learning rate with SGD. Moreover we exactly characterize the limiting learning rate (as time goes to infinity) for line search in the setting where the data covariance has only two distinct eigenvalues. For noiseless targets, we further demonstrate that the AdaGrad-Norm learning rate converges to a deterministic constant inversely proportional to the average eigenvalue of the data covariance matrix, and identify a phase transition when the covariance density of eigenvalues follows a power law distribution. We provide our code for evaluation at [https://github.com/amackenzie1/highline2024](https://github.com/amackenzie1/highline2024).

## 1 Introduction

In deterministic optimization, adaptive stepsize strategies, such as line search (see , therein), AdaGrad-Norm , Polyak stepsize , and others were developed to provide stability and improve efficiency and adaptivity to unknown parameters. While the practical benefits for deterministic optimization problems are well-documented, much of our understanding of adaptive learning rate strategies for stochastic algorithms are still in their infancy.

There are many adaptive learning rate strategies used in machine learning with many design goals. Some are known to adapt to stochastic gradient descent (SGD) gradient noise while others are robust to hyper-parameters (e.g., ). Theoretical results for adaptive algorithms tend to focus on guaranteeing minimax-optimal rates, but this theory is not engineered to provide realistic performancecomparisons; indeed many adaptive algorithms are minimax-optimal, and so more precise statements are needed to distinguish them. For instance, the exact learning rates (or rate schedules) to which these strategies converge are unknown, nor their dependence on the geometry of the problem. Moreover, we often do not know how these adaptive stepsizes compare with well-tuned constant or decaying fixed learning rate SGD, which can be viewed as a cost associated with selecting the adaptive strategy in comparison to tuning by hand.

In this work, we develop a framework for analyzing the exact dynamics of the risk and adaptive learning rate strategies for a wide class of optimization problems that we call _high-dimensional linear (high line) composite functions_. In this class, the objective function takes the form of an expected risk \(\ :^{d}\) over high-dimensional data \((a,)^{d}\) of a function \(f\,:\,^{3}\) composed with the linear functions \( X,a\), \( X^{*},a\). That is, we seek to solve

\[_{X^{d}}(X)}}{{=}}\,_{a,}[f( a,X, a,X^{*}, )](a,),X^{*}^ {d}}. \]

We suppose \(a(0,K)\) where \(K^{d d}\) is the covariance matrix. We train (1) using (one-pass) stochastic gradient descent with adaptive learning rates, \(_{k}\) (SGD+AL). Our main goal is to give a framework for better2 performance analysis of these adaptive methods. We then illustrate this framework by considering two adaptive learning rate algorithms on the least squares problem3, the results of which appear in Table 1: exact line-search (idealistic) (Sec. 3) and AdaGrad-Norm (Sec. 4). We expect other losses and adaptive learning rates can be studied using this approach.

Main contributions._Performance analysis framework._ We provide an equivalence of \((X_{k})\) and learning rate \(_{k}\) under SGD+AL to deterministic functions \((t)\) and \(_{t}\) via solving a _deterministic_ system of ODEs (see Section 2), which we then analyze to show how the covariance spectrum influences the optimization. See Figure 1. As the dimension \(d\) of the problem grows, the learning curves of \((X_{k})\) become closer to \((t)\) and the curves concentrate around \((t)\) with probability better than any inverse power of \(d\) (See Theorem 2.1).

_Greed can be arbitrarily bad in the presence of strong anisotropy (that is, \((K)/d(K^{2})/d\))._ Our analysis reveals that exact line search, which is to say optimally decreasing the risk at each step, can run arbitrarily slower than the best fixed learning rate for SGD on a least squares problem when \(_{}}}{{=}}_{}(K)>C>0\). The best fixed stepsize (least squares problem) is \(((K)/d)^{-1}\) or the inverse of the average eigenvalue, see Polyak stepsize . Line search, on the other hand, converges to a fixed stepsize of order \(_{}/((K^{2})/d)\). It can be that \(_{}/((K^{2})/d)((K)/d)^{-1}\) making exact line search substantially underperform Polyak stepsize. We further explore this and, in the case where \(d\)-eigenvalues of \(K\) take only two values \(_{1}>_{2}>0\), we give an exact expression as a function of \(_{1}\) and \(_{2}\) for the limiting behavior of \(_{t}\) as \(t\) (See Fig. 5).

Figure 1: **Concentration of learning rate and risk for AdaGrad-Norm on least squares with label noise \(=1\) (left) and logistic regression with no noise (right). As dimension increases, both risk and learning rate concentrate around a deterministic limit (red) described by our ODE in Theorem 2.1. The initial risk increase (left) suggests the learning rate started too high, but AdaGrad-Norm adapts. Our ODEs predict this behavior. See Sec. H for simulation details.**

_AdaGrad-Norm selects the optimal step-size, provided it has a warm start._ In the absence of label noise and when the smallest eigenvalue of \(K\) satisfies \(_{}>C>0\), the learning rate converges to a deterministic constant that depends on the average condition number (like in Polyak) and scales inversely with \((K)}{d}\|X_{0}-X^{}\|^{2}\). Therefore it attains automatically the optimal fixed stepsize in terms of the covariance _without_ knowledge of \((K)\), but pays a penalty in the constant, namely \(\|X_{0}-X^{}\|^{2}\). If one knew \(\|X_{0}-X^{}\|^{2}\) then by tuning the parameters of AdaGrad-Norm one might achieve performance consistent with Polyak; this also motivates more sophisticated adaptive algorithms such as DoG  and D-Adaptation , which adaptively compensate and/or estimate \(\|X_{0}-X^{}\|^{2}\).

_AdaGrad-Norm can use overly pessimistic decaying schedules on hard problems._ Consider power law behavior for the spectrum of \(K\) and the signal \(X^{}\). This is a natural setting as power law distributions have been observed in many datasets . Here the learning rate and asymptotic convergence of \(K\) undergo a _phase transition_. For power laws corresponding to easier optimization problems, the learning rate goes to a constant and the risk decays at \(t^{-_{1}}\). For harder problems, the learning rate decays like \(t^{-_{1}}\) and the risk decays at a different sublinear rate \(t^{-_{2}}\). See Table 1 and Sec. 4 for details.

**Notation.** Define \(_{+}=[0,)\). We say an event holds _with overwhelming probability, w.o.p.,_ if there is a function \(:\) with \((d)/ d\) so that the event holds with probability at least \(1-e^{-(d)}\). We let \(_{}(x)\) be the indicator function of the set \(A\) where it is \(1\) if \(x A\) and \(0\) otherwise. For a matrix \(A^{m d}\), we use \(\|A\|\) to denote the Frobenius norm and \(\|A\|_{}\) to denote the operator-2 norm. If unspecified, we assume that the norm is the Frobenius norm. For normed vector spaces \(\), \(\) with norms \(\|\|_{}\) and \(\|\|_{}\), respectively, and for \( 0\), we say a function \(F:\,\) is \(\)_-pseudo-Lipschitz_ with constant \(L\) if for any \(A,\), we have

\[\|F(A)-F()\|_{} L\|A-\|_{}(1+\|A\|_{ }^{}+\|\|_{}^{}).\]

We write \(f(t) g(t)\) if there exist _absolute_ constants \(C,c>0\) such that \(c g(t) f(t) C g(t)\) for all \(t\). If the constants depend on parameters, e.g., \(\), then we write \(_{}\).

Related work.Some notable adaptive learning rates in the literature are AdaGrad-Norm , RMSprop , stochastic line search, stochastic Polyak stepsize , and more recently DoG  and D-Adaptation . In this work, we introduce a framework for analyzing these algorithms, and we strongly believe it can be used to analyze many more of these adaptive algorithms. We highlight below a nonexhaustive list of related work.

  
**Learning rate** & **K assumption** & **Limiting \(_{}\)** & **Convergence rate** \\   AdaGrad-Norm\((b,)\) \\ (see Sec. 4) \\  & \(_{}>C\) & \(_{t}}{b}+(K)\|X_{0}-X^{ }\|^{2}}{()^{}}-_{}_{}t\) \\   AdaGrad-Norm\((b,)\) \\ Power law \\ (see Sec. 4) \\  & \(+<1\) & \(_{t}_{,}1\) & \((t)_{,}t^{+-2}\) \\   & \(+=1\) & \(_{t}_{,}\) & \((t)_{,}()^{-1}\) \\   & \(1<+<2\) & \(_{t}_{,}t^{-1+}\) & \((t)_{,}t^{-+1}\) \\   Exact line search, \\ idealized \\ (see Sec. 3) \\  & \(_{}>C\) & \(_{t}}{(K^{2})/d}\) & \(()-_{}_{}t\) \\  
 Polyak stepsize \\ (see Sec. 3) \\  & \(_{}>C\) & \(_{t}=(K)/d}\) & \(()-_{}_{}t\) \\   

Table 1: **Summary of adaptive learning rates results on the least squares problem.** We summarize our results for line search and AdaGrad-Norm under various assumptions on the covariance matrix \(K\). We denote \(_{}\) the smallest non-zero eigenvalue of \(K\) and \((K)}{d}\) the average eigenvalue. Power law\((,)\) assumes the eigenvalues of \(K\), \(\{_{i}\}_{i=1}^{d}\), follow a power law distribution, that is, for \(0<<1\), \(_{i}(1-)^{-}_{(0,1)}\) for all \(1 i d\) and \( X_{0}-X^{},_{i}^{2}_{i}^{-}\) where \(\{_{i}\}_{i=1}^{d}\) are eigenvectors of \(K\) (see Prop 4.4). For \({}^{*}\) (see Prop. 4.2), requires a good initialization on \(b\), \(\).

AdaGrad-Norm.AdaGrad, introduced by , updates the learning rate at each iteration using the stochastic gradient information. The single stepsize version , that depends on the norm of the gradient, (see Table 2 for the updates), has been shown to be robust to input parameters . Several works have shown worst-case convergence guarantees . A linear rate of \(O((- T))\) is possible for \(\)-strongly convex, \(L\)-smooth functions (\(\) is the condition number \(/L\)). In  (similar idea in ), the authors show for strongly convex, smooth stochastic objectives (with additional assumptions) that the AdaGrad-Norm learning rate exhibits a two stage behavior - a burn in phase and then when it reaches the smoothness constant it self-stablizes.

Stochastic line search and Polyak stepsizes.Recently there has been renewed interest in studying stochastic line search  and stochastic Polyak stepsize (and their variants) . Much of this research focuses on worst-case convergence guarantees for strongly convex and smooth functions (see e.g., ) and designing practical algorithms. In , the authors provide a bound on the learning rate for Armijo line search in the finite sum setting with a rate of \(L_{}/\) where avg. \(\) is the avg. strong convexity and \(L_{}\) is the max. Lipschitz constant of the individual functions. In this work, we consider a slightly different problem. We work with the population loss and we note that the analogue to \(L_{}\) for us would require that the samples \(a\) satisfy \(\|aa^{T}\|_{} L_{}\)_for all \(a\)_; this fails to hold for \(a(0,K)\). Moreover, \(L_{}\) could be much worse than \([\|aa^{T}\|_{}]\).

Deterministic dynamics of stochastic algorithms in high-dimensions.The literature on deterministic dynamics for isotropic Gaussian data has a long history . These results have been rigorously proven and extended to other models under the isotropic Gaussian assumption . Extensions to multi-pass SGD with small mini-batches  as well as momentum  have also been studied. Other high-dimensional limits leading to a different class of dynamics also exist . Recently, significant contributions have been made in understanding the effects of a non-identity data covariance matrix on the training dynamics . The non-identity covariance modifies the optimization landscape and affects convergence properties, as discussed in . This work extends the findings of  to stochastic adaptive algorithms, exploring the effect of non-identity covariance within these algorithms. Notably, Theorem 1.1 from  is restricted to deterministic learning rate schedules, limiting its applicability in many practical scenarios. In contrast, our Theorem 2.1 accommodates stochastic adaptive learning rates, aligning with widely used algorithms in practice.

### Model Set-up

We suppose that a sequence of independent samples \(\{(a_{k},y_{k})\}\) drawn from a distribution \(^{d}\) is provided where \(y_{k}\) is the target. The target \(y_{k}\) is a function of some random label noise \(_{k}\) and the input feature \(a_{k}\) dotted with a ground truth signal \(X^{}^{d}\), \( a_{k},X^{}\). Therefore, the distribution of the data is only determined by the input feature and the noise, i.e., the pair \((a,)\). In particular, we assume \((a,)\) follows a distributional assumption.

**Assumption 1** (Data and label noise).: _The samples \((a,)\) are normally distributed: \((0,^{2})\) where \(\), and \(a(0,K)\), with a covariance matrix \(K^{d d}\) that is bounded in operator norm independent of \(d\); i.e., \(\|K\|_{} C\). Furthermore, \(a\) and \(\) are independent._

For \(a,X,X^{}^{d}\), \(\), and a function \(f:^{3}\), we seek to minimize an expected risk function \(:^{d}\), which we refer to as the _high-dimensional linear composite4_, of the form

\[(X)}}{{=}}\,_{a, }[(X;a,)](a,),(X;a,)=f( a,X, a,X^{ },). \]

In what follows, we use the matrix \(W=[X|X^{}]^{d 2}\) that concatenates \(X\) and \(X^{}\), and we shall let \(B=B(W)=W^{T}KW\) be the covariance matrix of the Gaussian vector \(( a,X, a,X^{})\).

**Assumption 2** (Pseudo-lipschitz \(f\)).: _The function \(f:^{3}\) is \(\)-pseudo-Lipschitz with \( 1\)._

By assumption, \((X)\) involves an expectation over the correlated Gaussians \( a,X\) and \( a,X^{}\). We can express this as \((X)}}{{=}}h(B)\) for some well-behaved function \(h\,:\,^{2 2}\).

**Assumption 3** (Risk representation).: _There exists a function \(h:^{2 2}\) such that \(h(B)=(X)\) is differentiable and satisfies_

\[_{X}(X)=_{a,}_{X}(X;a,).\]

_Furthermore, \(h\) is continuously differentiable and its derivative \( h\) is \(\)-pseudo-Lipschitz for some \(0 1\), with constant \(L( h)\)._

The final assumption is the well-behavior of the Fisher information matrix of the gradients. The first coordinate of \(f\) is special, as the optimizer must be able to differentiate it. Thus, we treat \(f(x,x^{},)\) as a function of a single variable with two parameters: \(f(x,x^{},)=f(x;x^{},)\) and denote the (almost everywhere) derivative with respect to the first variable as \(f^{}\).

**Assumption 4** (Fisher matrix).: _Define \(I(B)}}{{=}}_{a,}[(f^{ }( a,X; a,X^{},))^{2}]\) where the function \(I:\,^{2 2}\). Furthermore, \(I\) is \(\)-pseudo-Lipschitz with constant \(L(I)\) for some \( 1\)._

A large class of natural regression problems fit within this framework, such as logistic regression and least squares (see [15, Appendix B]). We also note that Assumptions 3 and 4 are nearly satisfied for \(L\)-smooth objectives \(f\) (see Lemma B.1), and a version of the main theorem holds under just this assumption (albeit with a weaker conclusion).

### Algorithmic set-up

We apply _one-pass_ or _streaming_ SGD with an adaptive learning rate \(_{k}\) (SGD+AL) to solve \(_{X^{d}}(X)\), (2). Let \(X_{0}^{d}\) be an initial vector (random or non-random). Then SGD+AL iterates by selecting a _new_ data point \((a_{k+1},_{k+1})\) such that \(a_{k+1}(0,K)\) and \(_{k+1}(0,^{2})\) and makes the update

\[X_{k+1}=X_{k}-_{k}}{d}_{X}(X_{k};a_{k+1}, _{k+1})=X_{k}-_{k}}{d}f^{}( a_{k+1},X_ {k}; a_{k+1},X^{},_{k+1})a_{k+1}, \]

where \(_{k}>0\) is a learning rate (see assumptions below)5. To perform our analysis, we place the following assumption on the initialization \(X_{0}\) and signal \(X^{}\).

**Assumption 5** (Initialization and signal).: _The initialization point \(X_{0}\) and the signal \(X^{}\) are bounded independent of \(d\), that is, \(\{ X_{0}, X^{}\} C\) for some \(C\) independent of \(d\)._

Adaptive learning rate.Our analysis requires some mild assumptions on the learning rate. To this end, we define a learning rate function \(:\,_{+} D([0,)) D([0,)) D([0, ))_{+}\) by6

\[_{k}}}{{=}}(k,N_{k}(d ),G_{k}(d),Q_{k}(d)),\,\,k ,\,\,t 0, \]

\[(N_{k}(t),G_{k}(t),Q_{k}(t))}}{{=}}_{\{t<k\}}(W_{t})^{T}W_{t},_{X}(X_{t};a_ {t+1},_{t+1})^{2},(X_{t}).\]

  
**Algorithm** & **General update** & **Least squares** \\    } & _{k}\)} & \(b_{k}^{2}=b_{k-1}^{2}+(X_{k-1})^{2}\); &  \\  & & \(_{k-1}=d}{b_{k}}\) \\   & & \(_{t}\) & \(+_{0}^{t}((s))\, s}}\) \\   & & \((X_{k})^{2}\) & \((X_{k})^{2}}{ (X_{k})^{2}}{(X_{k})^{2}}{(X_{k})^{2}}{(X_{k}) ^{2}}{(X_{k})^{2}}{(X_{k})^{2}}{(X_{k})^{2}}{ (X_{k})^{2}}{\)), gradients \((G_{k})\), and risk \((Q_{k})\) via this definition.

We also define a conditional expectation version of \(G_{k}\) where the filtration \(_{k}=(X^{},X_{0},,X_{k})\):

\[_{k}(t)}}{{=}}_{\{t<k \}}()[\|_{X}(X_{t};a_{t+1},_{t+1}) \|^{2}|_{t}].\]

With this, we impose the following learning rate condition.

**Assumption 6** (Learning rate).: _The learning rate function \(\,:\,_{+} D([0,)) D([0,)) D([0, ))\) is \(\)-pseudo-Lipschitz with constant \(L()\) (independent of \(d\)) in \(D([0,)) D([0,)) D([0,))\). Moreover, for some constant \(C=C()>0\) independent of \(d\) and \(>0\),_

\[\,[|(k,f,G_{k}(d),q)-(k,f,_{ k}(d),q)||_{k}| Cd^{-}(1+\|f\|_{ }^{}+\|q\|_{}^{}). \]

_Finally, \(\) is bounded, i.e., there exists a constant \(=()>0\) independent of \(d\) so that_

\[(k,f,g,q)(1+\|f\|_{}^{}+\|q\|_{}^{}+ \|g\|_{}^{}). \]

The inequality (5) ensures that the learning rate concentrates around the mean behavior of the stochastic gradients. Many well-known adaptive stepsizes satisfy (4) and Assumption 6 including AdaGrad-Norm, DoG, D-Adaptation, and RMSProp (see Table 2, Sec. A, and Sec. C.3).

## 2 Deterministic dynamics for SGD with adaptive learning rates

Intuition for deriving dynamics:The risk \((X)\) and Fisher matrix can be evaluated solely in terms of the covariance matrix \(B\). Thus, to know the evolution of the risk over time, it would suffice to know the evolution of \(B\). Alas, except in the isotropic case where \(K\) is a multiple of the identity, the evolution of \(B\) is not autonomous (i.e., its time evolution depends on other unknown variables). However, if we let \((_{i},_{i})\) be the eigenvalues and corresponding orthonormal eigenvectors of \(K\), we can consider projections \(V_{i}(X_{k})=d W_{k}^{T}_{i}_{i}^{T}W_{k}\), and it turns out that these behave autonomously.

Example: Least Squares.One canonical example of (2) is least squares, where we aim to recover the target \(X^{}\) given noisy observations \( a,X^{}+\). In this case, the _least squares problem_ is

\[_{X^{d}}(X)=\,_{a, }[( a,X-X^{}-)^{2}]=^{2} +(X-X^{})^{T}K(X-X^{})}. \]

The pair of functions \(h\) (Assumption 3) and \(I\) (Assumption 4) can be evaluated simply:

\[h(B(W))=I(B(W))=(X-X^{})^{T}K(X-X^{})+ {1}{2}^{2}.\]

The deterministic dynamics for the risk \((t)\) in this case can be simplified to:

\[(t)=(X_{0}-X^{})^{T}Ke^{-2K_{0}^{t}_{s} \,\,s}(X_{0}-X^{})+^{2}+_{0}^ {t}_{s}^{2}(K^{2}e^{-2K_{s}^{t}_{s}\,\, })(s)\,\,s.\]

This is a convolution Volterra equation with a convergence threshold of \(_{t}<K}\).

In the noiseless label case (i.e., \(=0\)), the risk is given by \((t)=_{i=1}^{d}_{i}_{i}^{2}(t)\). Using the ODEs in (9), we get the following deterministic equivalent ODE for the \(_{i}^{2}\)s:

\[}{\,t}_{i}^{2}(t)=-2_{t}_ {i}_{i}^{2}(t)+2_{t}^{2}_{i}(t). \]

We will perform a deep analysis of the dynamics of the learning rate on least squares (7), which will generalize to settings where the outer function \(f\) is strongly convex (see D.1).

Deterministic dynamics.To derive deterministic dynamics, we make the following change to continuous time by setting

\[k= td,$ is the continuous time parameter}.\]

This time change is necessary, as when we scale the size of the problem, more time is needed to solve the underlying problem. This scaling law scales SGD so all training dynamics live on the same space. One can solve a smaller \(d\) problem and scale it to recover the training dynamics of the larger problem.7

We now introduce a coupled system of differential equations, which will allow us to model the behaviour of our learning algorithms. For the \(i\)th \((_{i},_{i})\)-eigenvalue/eigenvector of \(K\), set

\[_{i}(t)}}}{{=}} _{11,i}(t)&_{12,i}(t)\\ _{12,i}(t)&_{22,i}(t)i,\,(t)}}}{{=}}_{i=1}^{d}_{i}_{i}(t).\]

The \(_{i}(t)\) and \((t)\) are deterministic continuous analogues of \(V_{i}(X_{td})\) and \(B(X_{td})\) respectively. Define the following continuous analogues

\[ h((t)) }}}{{=}}H_{1,t }&H_{2,t}\\ H_{2,t}&H_{3,t},\,\,(t)}}}{{=}}_{i=1}^{d}_{i}(t),\,\,(t )}}}{{=}}h((t)),\,\,\,\, (t)}}}{{=}}I((t)),\] \[_{t}}}}{{=}}(t,1_{\{ t\}}(),(K)}{d}1_{\{ t\}}(),1_{\{ t\}} ()).\]

We now introduce a system of coupled ODEs for each \((_{i},_{i})\)-eigenvalue/eigenvector pair of \(K\)

\[&_{11,i}(t)=-2_{i} _{t}(_{11,i}(t)H_{1,t}+H_{1,t}_{11,i}(t)+ _{12,i}(t)H_{2,t}+H_{2,t}_{12,i}(t))+_{i} _{t}^{2}(t),\\ &_{12,i}(t)=-2_{i}_{t}(H_ {1,t}_{12,i}(t)+H_{2,t}_{22,i}(t)) \]

with the initialization of \(_{i}(0)\) given by \(V_{i}(X_{0})\). We finally state the deterministic dynamics for the risk and learning rate.

**Theorem 2.1**.: _Under Assumptions 1, 2, 3, 4, 5, 6, then for any \((0,)\) and any \(T>0\)_

\[_{0 t T}\|(X_{[td]})\\ _{[td]}-(t)\\ _{t}\|<d^{-}, \]

_The same statements hold comparing \(W_{td}^{T}W_{td}\) to \((t)\) and \(W_{td}^{T}KW_{td}\) to \((t)\)._

In fact, we can derive deterministic dynamics for a large class of statistics which are linear combinations of \((t)\) and functions thereof (See Theorem B.1, and Corollary B.1).

One important corollary is a deterministic limit for the distance to optimality, \(D^{2}(X_{k})=\|X_{k}-X^{}\|^{2}\), which is a quadratic form of \(W_{k}^{T}W_{k}\) and hence covered by Thm. 2.1. The equivalent deterministic dynamics are

\[^{2}(t)=_{i=1}^{d}_{i}^{2}(t)=_{i=1}^{d}(_{11,i}(t)-2_{12,i}(t)+_{22, i}(t)), \]

where \(_{i}^{2}(t)\) corresponds \(D_{i}^{2}(X_{k})}}}{{=}}d((X_{k} -X^{},_{i}))^{2}\).

## 3 Idealized Exact Line Search and Polyak Stepsize

In this section, we consider two classical idealized algorithms - _exact line search_ and _Polyak stepsize_. In deterministic optimization, these learning rate strategies are chosen so that the function value (exact line search) or distance to optimality (Polyak) produces the largest decrease in function value (resp. distance to optimality) at the next iteration. For stochastic algorithms, we can ask this to hold for the deterministic equivalent to the risk \((t)\) (resp. distance to optimality, \((t)\)) since we know that SGD is close to these deterministic equivalents. Thus, the question is: what choice of learning rate decreases the \((t)\) (_exact line search_) and/or \((t)\) (_Polyak stepsize_)? We will restrict to least squares in this section - see Appendix F.1 and F.2 for general functions as well as proofs for least squares. These are idealized algorithms because we can not implement them as they require distributional knowledge of \(a\) or \(X^{}\). Despite this, they provide a basis for more practical algorithms.

Polyak Stepsize.A natural threshold to consider is the largest learning rate such that \((t)<0\), which we denote by \(_{t}^{}\). Using the least squares ODE (8), this is precisely

\[_{t}^{}=(t)-^{2})}{(K)}{d}(t)}}_{k}^{}= (X_{k})-^{2})}{(K)}{d}(X_ {k})}. \]

Without label noise, (12) simplifies to \(_{t}^{}=}_{k}^{}=(K)/d}\), the exact threshold for convergence of least squares.

A greedy stepsize strategy would maximize the decrease in the distance to optimality at each iteration, denoted by us as _Polyak stepsize_, \(_{t}^{}_{}\,(t)\). In the case of least squares, this is

\[_{t}^{}=_{t}^{} _{k}^{}= }_{k}^{}.\]

The latter yields the optimal fixed learning rate (up to absolute constant factors) for a noiseless target on a least squares problem ).8

Exact Line Search.In the context of risk, using (8) and noting that \((t)=_{i=1}^{d}_{i}_{i}^{2}(t)\), we can find \(_{t}^{}\,(t)\); i.e., the greedy learning rate that decreases the risk the most in the next iteration. We call this _exact line search_. Expressions for the learning rates are given in Table 2, (c.f. Appendix F.1 for general losses). Because these come from ODEs, we can use ODE theory to give exact limiting values for the deterministic equivalent of \(_{k}^{}\).

**Proposition 3.1**.: _[Limiting learning rate; line search on noiseless least squares] Consider the noiseless (\(=0\)) least squares problem (7). Then the learning rate is always lower bounded by_

\[(K)}{\,(K^{2})}_{t}^{ }t 0.\]

_Moreover, suppose \(K\) has only two distinct eigenvalues \(_{1}>_{2}>0\), i.e., \(K\) has \(d/2\) eigenvalues equal to \(_{1}\) eigenvalues and \(d/2\) eigenvalues equal to \(_{2}\). Then_

\[(K)}{\,(K^{2})}_{t} _{t}^{}(K)}{\,(K^{2})}. \]

For a proof and explicit formula for \(_{t}_{t}^{}\), see Section F.2. Hence, being greedy for the risk in a sufficiently anisotropic setting will badly underperform Polyak stepsize (see Fig. 2).

Figure 2: **Comparison for Exact Line Search and Polyak Stepsize on a noiseless least squares problem. The left plot illustrates the convergence of the risk function, while the right plot depicts the convergence of the quotient \(_{t}/(K)}{\,(K^{2})}\) for Polyak stepsize and exact line search. Both plots highlight the implication of equation (13) in high-dimensional settings, where a broader spectrum of \(K\) results in \((K)}{(K^{2})} (K)}\), indicating slower risk convergence and poorer performance of exact line search (unmarked) as it deviates from the Polyak stepsize (circle markers). The gray shaded region demonstrates that equation (13) is satisfied. See Appendix H for simulation details.**

## 4 AdaGrad-Norm analysis

In this section, we analyze the behavior of AdaGrad-Norm learning rate in the least squares setting (see Sec. D for general strongly convex functions). In the presence of additive noise, the AdaGrad-Norm learning rate decays like \(t^{-1/2}\), regardless of the data covariance \(K\). In contrast, the model with no noise exhibits a learning rate that depends on the spectrum of \(K\), as illustrated in Figure 3. The learning rate is bounded below by a constant when \(_{}(K)>0\) is fixed as \(d\), and we quantify this lower bound. If the limiting spectral measure of \(K\) has unbounded density near 0 (e.g. power law spectrum), then the learning rate can approach zero and we quantify the rate of this convergence in the least squares setting as a function of spectral parameters.

For least squares with additive noise, the learning rate asymptotic \(_{t}/(b^{2}+}{d}(K)t)^{(1/2)}\) is the fastest decay that AdaGrad-Norm can exhibit. In contrast, the propositions below concern the noiseless case where, for various covariance examples, the decay rate of \(_{t}\) changes. This is tightly connected to whether the risk is integrable or not. In the simple case of identity covariance, we obtain a closed formula for the trajectory of the integral of the risk and therefore also the learning rate.

**Proposition 4.1**.: _In the case of identity covariance (\(K=I_{d}\)), the risk solves the differential equation_

\[}{t}(t)=(t)}{b ^{2}+2_{0}^{t}(s)\,s}-(t)}{ +2_{0}^{t}(s)\,s}}, \]

The solution \(_{0}^{t}(s)\,s\) approaches (from below) a positive constant which yields a computable lower bound to which \(_{t}\) will converge. Generalizing this to a broader class of covariance matrices, we get the next proposition, which captures the dependence of \(_{t}\) on \((K)\).

**Proposition 4.2**.: _Suppose \(\,(K) b/\), and that \(_{0}^{}(s)_{s}\,s<\) with \(_{s}\) as in Table 2 (AdaGrad-Norm for least squares), then \(_{t}+}{42}\,(K) ^{2}(0)}.\)_

An analog of Proposition 4.2 for the strongly convex setting appears in Sec. D (see Prop. D.1). We now consider two cases in which, as \(d\), there are eigenvalues of \(K\) arbitrarily close to 0.

**Proposition 4.3**.: _Assume that, for some \(C>0\), the number of eigenvalues of \(K\) below \(C\) is \(o(d)\), and that \( X^{*},_{i}=O(d^{-1/2})\) for all \(i\), (i.e. \(X^{*}\) is not concentrated in any eigenvector direction). Then, with the initialization \(X_{0}=0\), there exists some \(>0\) such that \(_{t}>\) for all \(t>0\)._

**Proposition 4.4**.: _Let \(K\) have a spectrum that converges as \(d\) to the power law measure \(()=(1-)^{-}_{(0,1)}\), for some \(<1\)9, and suppose that \(_{1}^{2}(0)_{i}^{-}\) for \( 0\). Then:_

* _For_ \(1>+,\) _there exists_ \(\) _such that_ \(_{t}\)_, and_ \((t)_{,}t^{+-2}\) _for all_ \(t 1\)_._
* _For_ \(1<+<2\)_,_ \(_{t}_{,}t^{-1+}{42}}\)_, and_ \((t)_{,}t^{-+1}\) _for all_ \(t 1\)_._

Figure 3: **Quantities effecting AdaGrad-Norm learning rate.**_(left):_ Effect of noise (\(=1.0\)) on risk (left axis) and learning rate (right axis). Depicted is \(}{}\) so it approaches \(1\). _(Center, right)_: Noiseless least squares (\(=0\)). As predicted in Prop. 4.2, \(_{t}_{t}\) depends on avg. eig. of \(K\) (\((K)/d\)) and \(\|X_{0}-X^{*}\|^{2}\) but not \(=_{}/_{}\). See Appendix H for simulation details.

* _For_ \(1=+\)_,_ \(_{t}_{,}\)_, and_ \((t)_{,}()^{-1}\) _for all,_ \(t 1\)_._

This proposition shows non-trivial decay of the learning rate is dictated by the residuals (distance to optimality at initialization) and the spectrum of \(K\). We note that \(=0\) corresponds to uniform contribution of each mode (e.g. \(X_{0}\) normally distributed). As the eigenmodes of the residuals become more localized, the decay of the learning rate is closer to the behaviour in the presence of additive noise. Furthermore, the scaling behaviour of the loss is affected by the structure of the AdaGrad-Norm algorithm (see Fig. 4). Lastly, constant stepsize SGD yields \((t) t^{+-2}\), with no transition occurring at \(+=1\).

Proofs of the above propositions, in a slightly more general setting, are deferred to Sec. D.

## 5 Conclusions and Limitations

This work studies stochastic adaptive optimization algorithms when data size and parameter size are large, allowing for nonconvex and nonlinear risk functions, as well as data with general covariance structure. The theory shows a concentration of the risk, the learning rate and other key functions to a deterministic limit, which is described by a set of ODEs. The theory is then used to derive the asymptotic behavior of the AdaGrad-Norm and idealized exact line search on strongly convex and least square problems, revealing the influence of the covariance matrix structure on the optimization. A potential extension of this work would be to study other adaptive algorithms such as D-adaptation, DOG, and RMSprop which are covered by the theory. Studying the asymptotic behavior of the risk and the learning rate may improve our understanding of the performance and scalability of these algorithms on more realistic data. Another important application of the theory would be to analyze the ODEs presented here on nonconvex problems.

The current form of the theory is limited to Gaussian data, though many parts of the proof can be extended easily beyond Gaussian data. The main ODE comparison theorem is also only tuned for analyzing problem setups where the trace of the covariance is on the order of the ambient dimension; when the trace of the covariance is much smaller than ambient dimension, other stepsize scalings of SGD are needed. In addition, the analysis is limited to the streaming stochastic adaptive methods. We conjecture that a similar deterministic equivalent holds also for multi-pass algorithms at least for convex problems. This has already been shown in the least square problem for SGD with a fixed deterministic learning rate . Lastly, numerical simulations on real datasets (e.g., CIFAR-5m) suggests that the predicted risk derived by our theory matches the empirical risk of multipass SGD beyond Gaussian data (see for example Figure 6).

Figure 4: **Power law covariance in AdaGrad Norm** on a least squares problem. Ran exact predictions (ODE) for the risk and learning rate (solid lines). Dashed lines give the predictions from Prop. 4.4 which _match experimental results exactly_. **Phase transition as \(+\) varies.** When \(+<1\) (green), the learning rate _(right)_ is constant as \(t\). In contrast, when \(2>+>1\) (purple), the learning rate decreases at a rate \(t^{-1+1/(+)}\) with \(+=1\) (white) where the change occurs. Same phase transition occurs in the sublinear rate of the risk decay _(left)_ (see Prop. 4.4).