# SustainDC: Benchmarking for Sustainable Data Center Control

Avisek Naug\({}^{}\), Antonio Guillen\({}^{}\), Ricardo Luna\({}^{}\), Vineet Gundecha\({}^{}\), Cullen Bash,

**Sahand Ghorbanpour, Sajad Mousavi, Ashwin Ramesh Babu, Dejan Markovikj, Lekhapriya D Kashyap, Desik Rengarajan, Soumyendu Sarkar\({}^{}\)**

Corresponding author. \(\)These authors contributed equally.

###### Abstract

Machine learning has driven an exponential increase in computational demand, leading to massive data centers that consume significant energy and contribute to climate change. This makes sustainable data center control a priority. In this paper, we introduce SustainDC, a set of Python environments for benchmarking multi-agent reinforcement learning (MARL) algorithms for data centers (DC). SustainDC supports custom DC configurations and tasks such as workload scheduling, cooling optimization, and auxiliary battery management, with multiple agents managing these operations while accounting for the effects of each other. We evaluate various MARL algorithms on SustainDC, showing their performance across diverse DC designs, locations, weather conditions, grid carbon intensity, and workload requirements. Our results highlight significant opportunities to improve data center operations using MARL algorithms. Given the increasing use of DC due to AI, SustainDC provides a crucial platform for developing and benchmarking advanced algorithms essential for achieving sustainable computing and addressing other heterogeneous real-world challenges.

## 1 Introduction

One of the growing areas of energy and carbon footprint (\(CFP\)) can be traced to cloud data centers (DCs). The increased use of cloud resources for batch workloads related to AI model training, multimodal data storage and processing, or interactive workloads like streaming services, hosting websites have prompted enterprise clients to construct numerous data centers. Governments and regulatory bodies are increasingly focusing on environmental sustainability and imposing stricter regulations to reduce carbon emissions. This has prompted industry-wide initiatives to adopt more intelligent DC control approaches. This paper presents SustainDC, a sustainable DC Multi-Agent Reinforcement Learning (MARL) set of environments. SustainDC helps promote and prioritize sustainability, and it serves as a platform that facilitates collaboration among AI researchers, enabling them to contribute to a more environmentally responsible DC.

The main contributions of this paper are the following:

* A highly customizable suite of environments focused on Data Center (DC) operations, designed to benchmark energy consumption and carbon footprint across various DC configurations. The framework supports the subclassing of models for different DC components ranging from workloads and individual server specifications to cooling systems, enabling users to test fine-grained design choices.
* The environments are implemented using the Gymnasium _Env_ class, facilitating the benchmarking of various control strategies to optimize energy use, reduce carbon footprint, and evaluate related performance metrics.
* Supports both homogeneous and heterogeneous multi-agent reinforcement learning (MARL) controllers and traditional non-ML controllers. Extensive studies within these environments demonstrate the advantages and limitations of various multi-agent approaches.
* SustainDC enables reward shaping, allowing users to conduct ablation studies on specific DC components to optimize performance in targeted areas.
* SustainDC serves as a comprehensive benchmark environment for heterogeneous, multi-agent, and multi-objective reinforcement learning algorithms, featuring diverse agent interactions, customizable reward structures, high-dimensional observations, and reproducibility.

Code, licenses, and setup instructions for SustainDC are available at GitHub2. The documentation can be accessed at 3.

## 2 Related Work

Recent advancements in Reinforcement Learning (RL) have led to an increased focus on optimizing energy consumption in areas such as building and DC management. This has resulted in the development of several environments for RL applications. _CityLear_ (1) is an open-source platform that supports single and MARL strategies for energy coordination and demand response in urban environments. _Energyym_ (2), _RL-Testbed_ (3) and _Sinergym_ (4) were developed as RL wrappers that facilitate communication between Python and EnergyPlus, enabling RL evaluation on the collection of buildings modeled in EnergyPlus. _SustainGym_ (5) is one of the latest suite of general purpose RL tasks for evaluation of sustainability, simulating electric vehicle charging scheduling and battery storage bid, which lends itself to benchmarking different control strategies for optimizing energy, carbon footprint, and related metrics in electricity markets.

Most of the above-mentioned works use _EnergyPlus_ (6) or, _Modelica_ (7), (8) which were primarily designed for modeling thermo-fluid interactions with traditional analytic control with little focus on Deep Learning applications. The APIs provided in these works only allow sampling actions in a model free manner, lacking a straightforward approach to customization or re-parameterization of system behavior. This is because most of the works have a set of pre-compiled binaries (e.g. FMUs in Modelica) or fine-tuned spline functions (in EnergyPlus) to simulate nominal behavior. Furthermore, there is a significant bottleneck in using these precompiled environments from Energyplus or Modelica for Python based RL applications due to latency associated with cross-platform interactions, versioning issues in traditional compilers for EnergyPlus and Modelica, unavailability of open source compilers and libraries for executing certain applications.

SustainDC allows users to simulate the electrical and thermo-fluid behavior of large DCs directly in Python. Unlike other environments that rely on precompiled binaries or external tools, SustainDC is easily end-user customizable and fast It enables the design, configuration, and control benchmarking of DCs with a focus on sustainability. This provides the ML community with a new benchmark environment specifically for Heterogeneous MARL in the context of DC operations, allowing for extensive goal-oriented customization of the MDP transition function, state space, actions space, and rewards.

## 3 Data Center Operational Model

Figure 1 illustrates the typical components of a DC operation as modeled in SustainDC. _Workloads_ are uploaded to the DC from a proxy client. For non-interactive batch workloads, some of these jobs can be scheduled flexibly, allowing delays to different periods during the day for optimization. This

creates a scheduling challenge of postponing workloads to times when _Grid Carbon Intensity_ (_CI_), energy consumption, or energy pricing is lower.

As the servers (IT systems) in the DC process these workloads, they generate heat that must be removed. A complex HVAC system with multiple components is used to cool the IT system. As shown in Figure 2, warm air rises from the servers via convection. Driven by the HVAC fan's forced draft, this warm air enters the _Computer Room Air Handler_ (CRAH) (depicted by red arrows), where it is cooled to an optimal setpoint by a heat exchange process using a "primary" chilled water loop. The chilled air is then returned to the IT room through a plenum located beneath the DC (shown by blue arrows). The warmed water from this loop returns to the _Chiller_, where another heat exchange process transfers heat to a "secondary" chilled water loop, which carries the heat to a _Cooling Tower_. The cooling tower fan, operating at variable speeds, rejects this heat to the external environment, with fan speed and energy consumption determined by factors such as the secondary loop's inlet temperature at the cooling tower, the desired outlet temperature setpoint, and external air temperature and humidity. Depending on the external _Weather_ and processed _Workload_, the IT and cooling systems consume _Grid Energy_. Selecting the optimal cooling setpoint for the CRAH can reduce the DC's carbon footprint and also impacts the servers' energy efficiency (9).

Larger DCs may include onsite _Battery Banks_ that charge from the grid during low _CI_ periods and may optionally provide auxiliary energy during high _CI_ periods. This introduces a decision-making sustainability challenge to determine the optimal charging and discharging intervals for the batteries.

These three control problems are interrelated, motivating the development of testbeds and environments for evaluating multi-agent control approaches that collectively aim to minimize carbon footprint, energy and water usage, energy cost, and other sustainability metrics of interest.

## 4 SustainDC environment overview

A high-level overview of SustainDC is provided in Figure 3, outlining the three main environments developed in _Python_ along with their individual components, customization options, and associated control challenges.

Figure 1: Operational Model of a SustainDC Data Center

Figure 2: Model of the data center. The configuration allows customization of the number of cabinets per row, the number of rows, and the number of servers per cabinet. The cooling system, comprising the CRAH, chiller, and cooling tower, manages the heat generated by the IT system.

The _Workload Environment_ models and controls the execution and scheduling of delay-tolerant workloads within the DC.

In the _Data Center Environment_, servers housed in IT room cabinets process these workloads. This environment simulates both electrical and thermo-fluid dynamics, modeling heat generated by the workload processing and its transfer to the external environment through HVAC cooling components.

The _Battery Environment_ simulates grid charging during off-peak hours and supplies auxiliary energy to the DC during periods of high grid carbon intensity, offering a solution to manage energy demand sustainably.

Detailed physics-based implementations for each environment are available in the supplementary document. Customization parameters for all aspects of the DC environment design in SustainDC can be fully specified through _dc_config.json_, a universal configuration file.

Figure 2(a) further illustrates SustainDC, showing the _Workload Environment_, _Data Center Environment_, and _Battery Environment_ along with their customizable parameters. Figure 2(b) depicts the RL loop in SustainDC, illustrating how agents' actions and states optimize DC operations, considering external variables like grid CI, workload, and weather.

### Workload Environment

The _Workload Environment_ (\(Env_{LS}\)) manages the execution and scheduling of delay tolerant workloads within the DC by streaming workload traces (measured in FLOPs) over a specified time period. SustainDC includes a set of open-source workload traces from _Alibaba_ (10) and _Google_ (11) data centers. Users can customize this component by adding new workload traces to the _data/Workload_ folder or by specifying a path to existing traces in the _dc_config.json_ file.

Some workloads are flexible, meaning they can be rescheduled within an allowable time horizon. Tasks such as updates or backups do not need immediate execution and can be delayed based on urgency or Service-Level Agreements (SLA). This flexibility allows workloads to be shifted to periods of lower grid carbon intensity (CI), thereby reducing the DC's overall carbon footprint (\(CFP\)).

Users can also customize the CI data. By default, we provide a one-year CI dataset for the following states: Arizona, California, Georgia, Illinois, New York, Texas, Virginia, and Washington, locations selected due to their high data center density. The carbon intensity data files, sourced from eia.gov (https://api.eia.gov/bulk/EBA.zip), are located in the _data/CarbonIntensity_ folder.

Let \(B_{t}\) be the instantaneous DC workload trace at time \(t\), with \(X\%\) of the load reschedulable up to \(N\) simulation steps into the future. The objective of an RL agent (\(Agent_{LS}\)) is to observe the current time of day (\(SC_{t}\)), the current and forecast grid CI data (\(CI_{t...t+L}\)), and the remaining amount of

Figure 3: SustainDC overview and RL loop

reschedulable workload (\(D_{t}\)). Based on these observations, the agent chooses an action \(A_{ls,t}\) (as shown in Table 1) to reschedule the flexible portion of \(B_{t}\), to minimize the net \(CFP\) over \(N\) steps.

### Data Center Environment

The _Data Center_ environment (\(Env_{DC}\)) provides a comprehensive set of configurable models and specifications. For IT-level design, SustainDC enables users to define IT Room dimensions, server cabinet arrangements (including the number of _rows_ and _cabinets_ per row), and both _approach_ and _return_ temperatures. Additionally, users can specify server and fan power characteristics, such as _idle power_, _rated full load power_, and _rated full load frequency_.

On the cooling side, SustainDC allows customization of the _chiller reference power_, _cooling fan reference power_, and the supply air _setpoint_ temperature for IT Room cooling. It also includes specifications for the pump and cooling tower, such as _rated full load power_ and _rated full load frequency_. All these parameters can be configured in the _dc_config,json_ file.

One of SustainDC's key features is its ability to automatically adjust HVAC cooling capacities based on workload demands and IT room configurations, a process known as "sizing." This ensures that the data center remains adequately cooled without unnecessary energy expenditure. In contrast, previous environments often neglected this capability, resulting in inaccurate outcomes. For example, changing IT room configurations in other environments typically impacted only IT energy consumption without considering the overall cooling requirements, leading to inconsistent RL-based control results, as seen in _RL-Testbed_ in (3). SustainDC addresses this by integrating custom supply and approach temperatures derived from Computational Fluid Dynamics (CFD) simulations, simplifying the complex calculations of temperature changes between the IT Room HVAC and the IT Cabinets (9).

In addition, SustainDC includes weather data (in _data/Weather_) in the.epw format for the same locations as the CI data. This data, sourced from https://energyplus.net/weather, represents typical weather conditions for these regions. Users can also specify their own weather files if needed.

Given \(_{t}\) as the adjusted workload from the _Workload Environment_, the goal of the RL agent (\(Agent_{DC}\)) is to select an optimal cooling setpoint \(A_{dc,t}\) (Table 1) to minimize the net carbon footprint \(CFP\) from combined cooling (\(E_{hvac}\)) and IT (\(E_{it}\)) energy consumption over an \(N\)-step horizon. In SustainDC, the agent's default state space includes the time of day and year (\(SC_{t}\)), ambient weather (\(t_{db}\)), IT Room temperature (\(t_{room}\)), previous step cooling (\(E_{hvac}\)) and IT (\(E_{it}\)) energy usage, and forecasted grid CI data (\(CI_{t...t+L}\)).

### Battery Environment

The _Battery Environment_ (\(Env_{BAT}\)) is based on battery charging and discharging models, such as \(f_{charging}(BatSoc,)\) from (12). Parameters for these components, including battery capacity, can be configured in the _dc_config,json_ file.

The objective of the RL agent (\(Agent_{BAT}\)) is to optimally manage the battery's state of charge (\(BatSoc_{t}\)). Using inputs such as the net energy consumption (\(E_{hvac}+E_{it}\)) from the _Data Center_ environment, the time of day (\(SC_{t}\)), the current battery state of charge (\(BatSoc_{t}\)), and forecasted grid CI data (\(CI_{t...t+L}\)), the agent determines an action \(A_{bat,t}\) (as outlined in Table 1). Actions include charging the battery from the grid, taking no action, or discharging to provide auxiliary energy to the data center, all aimed at minimizing the overall carbon footprint, energy consumption, etc.

### Heterogeneous Multi Agent Control Problem

While SustainDC enables users to tackle the individual control problems for each of the three environments, the primary goal of this paper is to establish a multi-agent control benchmark that facilitates joint optimization of the \(CFP\) by considering the coordinated actions of all three agents (\(Agent_{LS}\), \(Agent_{DC}\), and \(Agent_{BAT}\)). The sequence of operations for the joint multi-agent and multi-environment functions can be represented as follows:\[Agent_{LS} :(SC_{t} CI_{t} D_{t} B_{t}) A_{ls,t}\] (1) \[Agent_{DC} :(SC_{t} t_{db} t_{room} E_{hvac} E_{it}  CI_{t}) A_{dc,t}\] (2) \[Agent_{BAT} :(SC_{t} Bat\_SoC CI_{t}) A_{bat,t}\] (3) \[Env_{LS} :(B_{t} A_{ls,t})_{t}\] (4) \[Env_{DC} :(_{t} t_{db} t_{room} A_{dc,t})( E_{hvac},E_{it})\] (5) \[Env_{BAT} :(Bat\_SoC A_{bat,t})(Bat\_SoC,E_{bat})\] (6) \[CFP_{t}= (E_{hvac}+E_{it}+E_{bat}) CI_{t}\] (7)

where \(E_{bat}\) represents the net discharge from the battery based on the change in battery state of charge (\(Bat\_SoC\)), which can be positive or negative depending on the action \(A_{bat,t}\). If the battery provides auxiliary energy, \(E_{bat}\) is negative; if it charges from the grid, \(E_{bat}\) is positive.

The objective of the multi-agent problem is to determine \(_{LS}\), \(_{DC}\), and \(_{BAT}\), which parameterize the policies for \(Agent_{LS}\), \(Agent_{DC}\), and \(Agent_{BAT}\), respectively, such that the total \(CFP\) is minimized over a specified horizon \(N\). For this study, we set \(N=31 24 4\), representing a 31-day horizon with a step duration of 15 minutes.

\[_{LS},_{DC},_{BAT}=argmin_{t=0}^{t= N}CFP_{t}\] (8)

### Rewards

While \(CFP\) reduction is the default objective in SustainDC, the reward formulation is highly customizable, allowing users to incorporate alternative objectives such as total energy consumption, operating costs across all DC components, and water usage.

We primarily consider the following default rewards for the three environments (\(Env_{LS}\), \(Env_{DC}\), \(Env_{BAT}\)):

\[(r_{LS},r_{DC},r_{BAT})=-(CFP_{t}+LS_{Penalty}),-(E_{hvac,t}+E_{it,t}), -(CFP_{t})\]

Here, \(LS_{Penalty}\) is a penalty applied to the Load Shifting Agent (\(Agent_{LS}\)) in the Workload Environment (\(Env_{LS}\)) if it fails to reschedule flexible workloads within the designated time horizon \(N\). Specifically, if \(D_{t}\) is positive at the end of a horizon \(N\), \(LS_{Penalty}\) is assigned. Details on calculating

 p{113.8pt} p{113.8pt} p{113.8pt}} 
**Agent** & **Control Knob** & **Actions** & **Optimization Strategy** & **Figure** \\  }\)} & Delay-tolerant workload & 0\\ 1\) Compute All Immediate Tasks \(2\) Maximize Throughput} & Shift tasks to periods of lower CI/lower external temperature/other variables to reduce the \(CFP\). & Workload \\  }\)} & Cooling & 0\\ 1\) Determine All Immediate Tasks \(2\) Maximize Throughput} & Optimize cooling by adjusting cooling setpoints based on workload, external temperature, and CI. & Workload \\  }\)} & Battery & 0\\ 1\) Change Battery \(1\) Hold Energy \(2\) Discharge Battery} & Store energy when CI/temperature/workload/other is low and use stored energy when is high to reduce \(CFP\). & Change energy when CI/temperature/workload/other is low and use stored energy when is high to reduce \(CFP\). & Change \\    & & & \\   

Table 1: Overview of control choices in SustainDC: the tunable knobs, the respective action choices, optimization strategies, and visual representations.

\(LS_{Penalty}\) are provided in the supplemental document. Users can opt for custom reward formulations by subclassing the base reward class in _utils/reward_creator.py_.

Based on these individual rewards, we can formulate an independent or collaborative reward structure, where each agent receives partial feedback in the form of rewards from the other agent-environment pairs. The collaborative feedback reward formulation for each agent is formulated as:

\[R_{LS} =*r_{LS}+(1-)/2*r_{DC}+(1-)/2*r_{BAT}\] \[R_{DC} =(1-)/2*r_{LS}+*r_{DC}+(1-)/2*r_{BAT}\] \[R_{BAT} =(1-)/2*r_{LS}+(1-)/2*r_{DC}+*r_{BAT}\]

Here, \(\) is the weighting parameter. This reward-sharing mechanism enables agents to incorporate feedback from their actions across environments, making it suitable for independent critic multi-agent RL algorithms, such as IPPO (13). For instance, the adjusted CPU load \(_{t}\) influences data center energy demand (\(E_{cool}+E_{it}\)), which subsequently affects the battery optimizer's charge-discharge decisions and ultimately impacts the net \(CO_{2}\) footprint. Consequently, we explore a collaborative reward structure and conduct ablation experiments with varying \(\) values to assess the effectiveness of reward sharing.

### Extendable plug-n-play Data Center Simulation Platform

Figure 4 illustrates the extendable and plug-and-play design of SustainDC framework for data center control to address the multi-agent optimization of data centers for resolving multiple internal and external dependencies of agents in real-time. The three different controllers for **Cooling Optimizer, Flexible Load Shifter** and **Battery Controller** can be substituted with **RL** or **non-RL controllers**. Similarly, the underlying models performing the simulation can be substituted easily using the **Modules and Extendable Functions** block. In the future, we plan to include the models for next generation of fanless direct liquid cooling for AI servers (14) for Energy HVAC Model Plug-in.

## 5 Evaluation Metrics and Experimental Settings

We consider five metrics to evaluate various RL approaches on SustainDC. The \(CO_{2}\)_footprint_ (\(CFP\)) represents the cumulative carbon emissions associated with DC operations over the evaluation period. _HVAC Energy_ refers to the energy consumed by cooling components, including the chiller, pumps, and cooling tower. _IT Energy_ refers to the energy consumed by the servers within the DC. _Water Usage_, the volume of chilled water recirculated through the cooling system, is a critical metric in DCs where chilled water supply from a central plant is constrained, and efficient use of this resource helps minimize the DC's water footprint. Finally, _Task Queue_ tracks the accumulation of compute FLOPs from workloads that are deferred for rescheduling under lower CI periods. Higher Task Queue values indicate poorer SLA performance within the DC.

Experiments were conducted on an Intel(r) Xeon(r) Platinum 8470 server with 104 CPUs, utilizing 4 threads per training agent. All hyperparameter configurations for benchmark experiments are detailed in the supplemental document. The codebase and documentation are linked to the paper.

Figure 4: Extendable and plug-and-play design of SustainDC for data center control to address the multi-agent holistic optimization of data centers for resolving multiple dependencies in real-time.

Benchmarking Algorithms on SustainDC

The purpose of SustainDC is to explore the benefits of jointly optimizing the _Workload_, _Data Center_, and _Battery Environments_ to reduce the operating \(CFP\) of a DC. To investigate this, we can perform ablation studies in which we evaluate net operating \(CFP\) by running trained RL agents in only one or two of the SustainDC environments while employing baseline methods (\(B_{*}\)) in the other environments. For the _Workload Environment_ (\(Env_{LS}\)), the baseline (\(B_{LS}\)) assumes no workload shifting over the horizon, which aligns with current standard practices in most data centers. For the _Data Center Environment_ (\(Env_{DC}\)), we use the industry-standard ASHRAE Guideline 36 as the baseline (\(B_{DC}\)) (15). In the _Battery Environment_ (\(Env_{BAT}\)), we adapt the method from (12) for real-time operation, reducing the original optimization horizon from 24 hours to 3 hours as our baseline (\(B_{BAT}\)). Future work will include further baseline comparisons using Model Predictive Control (MPC) and other non-ML control algorithms.

Next, we perform ablations on the collaborative reward parameter \(\), followed by benchmarking various multi-agent RL approaches. This includes multi-agent PPO (16) with an independent critic for each actor (IPPO) (13) and a centralized critic with access to states and actions from other MDPs (MAPPO) (17). Given the heterogeneous nature of action and observation spaces in SustainDC, we also benchmark several heterogeneous multi-agent RL (HARL) methods (18), including HAPPO (Heterogeneous Agent PPO), HAA2C (Heterogeneous Agent Advantage Actor Critic), HAD3QN (Heterogeneous Agent Dueling Double Deep Q Network), and HASAC (Heterogeneous Agent Soft Actor Critic). MARL agents were trained on one location and evaluated across different locations.

In Figure 5, we compare the relative performance of different RL algorithms using a radar chart based on the evaluation metrics in Section 5. Since reporting absolute values may lack context, we instead plot relative performance differences, offering insights into the _pros_ and _cons_ of each approach. (Absolute values for these benchmark experiments are provided in the supplementary document in tabular format.) Metrics are normalized by their mean and standard deviation, with lower values positioned at the radar chart periphery and higher values toward the center. Hence, the larger the area for an approach on the radar chart, the better its performance across the evaluated metrics.

### Single vs multi-agent Benchmarks

Figure 4(a) compares the relative performance of a single RL agent versus multi-agent RL benchmarks, highlighting the advantages of a MARL approach for sustainable DC operations. Among single RL agent approaches, the workload manager RL agent (Experiment 1) and the battery agent (Experiment 3) perform similarly in reducing water usage. The standalone DC (cooling) RL agent (Experiment 2) demonstrates strong performance in both energy and \(CFP\) reduction. Note that for Experiments 1 and 3, the Lowest Task Queue metric should be disregarded, as the baseline workload manager does not shift workloads and thus inherently has the lowest task queue.

When we evaluate pairs of RL agents working simultaneously, the absence of a cooling optimization agent (e.g., Experiment 5) results in performance similar to single RL agent implementations (Experiments 1 and 3), where only \(A_{LS}\) or \(A_{BAT}\) are used with baseline agents. This indicates that the RL-based cooling optimizer significantly improves overall performance compared to the rule-based ASHRAE Guideline 36 controller (as seen in Experiments 2 and 4). Finally, when all three RL agents operate simultaneously without a shared critic (Experiment 7 using IPPO), they achieve better outcomes in energy consumption, water usage, and task queue management, with a \(CFP\) relatively similar to other experiments. The combined performance across all three agents highlights the benefits of a MARL approach for DC optimization.

### Reward Ablation on \(\)

Figure 4(b), shows the relative differences in performance when considering collaborative reward components. We considered 2 values of \(\) at the extremes to indicate no collaboration (\(=1.0\)) and relying only on the rewards of other agents (\(=0.1\)). An intermediate value of \(=0.8\) was chosen based on similar work on reward-based collaborative approach in (19; 20). The improvement in setting \(=0.8\) shows that considering rewards from other agents can improve performance w.r.t. no collaboration (\(=1.0\)) especially in a partially observable MDP.

Figure 5: Benchmarking RL Algorithms on the Sustain DC environment

### Multiagent Benchmarks

We evaluated and compared the relative performances of various MARL approaches, including PPO with independent actor-critics (IPPO, \(=0.8\)), centralized critic PPO (MAPPO), heterogeneous multi-agent PPO (HAPPO), HAA2C, HAD3QN, and HASAC. Figures 4(c), 4(d), 4(e), and 4(f) illustrate the relative performance of these methods for DCs located in New York, Georgia, California, and Arizona. Our results reveal a consistent trend where PPO-based shared actor-critic methods (MAPPO, HAPPO) outperform the independent agent counterpart, IPPO. On further analysis, we observed that while IPPO effectively reduces HVAC and IT energy, the battery agent struggles to optimally schedule charging and discharging from the grid to meet data center demand. Among MAPPO, HAPPO, and HAA2C, HAPPO consistently performs best (except in Georgia).

For the off-policy methods (HAD3QN and HASAC), performance varies significantly across regions, with HASAC achieving the highest performance in Arizona. The reasons for these regional performance variations are not fully understood and may be partially due to differences in weather and carbon intensity. We plan to further investigate these variations in future work.

## 7 Limitations

The absence of an oracle that already knows the best results possible for the different environments makes it difficult to quantify the threshold for performance compared to simpler environments. For computational speed in RL, we used reduced order models for certain components like pumps and cooling towers. We could not exhaustably tune the hyperparameters for all the networks.

## 8 Next Steps

We are planning to deploy the trained agents to real data centers and are working towards domain adaptation for deployment with safeguards. We will augment the codebase with these updates. In order to have a smooth integration with current systems where HVAC runs in isolation, we plan a phased deployment with recommendation to the data center operative followed by direct integration of the control agents with the HVAC system with safeguards. For real-world deployment, a trained model should be run on a production server using appropriate checkpoints within a containerized platform with necessary dependencies. Security measures must restrict the software to only read essential data, generate decision variables, and write them with limited access to secure memory for periodic reading by the data center's HVAC management system. To ensure robustness against communication loss, a backup mechanism for generating decision variables is essential.

## 9 Conclusion

This paper introduced SustainDC, a fully Python-based benchmarking environment for multi-agent reinforcement learning (MARL) in sustainable, cost-effective, and energy-efficient data center operations. SustainDC provides comprehensive customization options for modeling multiple aspects of data centers, including a flexible RL reward design, an area we invite other researchers to explore further. We benchmarked an extensive collection of single-agent and multi-agent RL algorithms in SustainDC across multiple geographical locations, comparing their performance to guide researchers in sustainable data center management with reinforcement learning.

Additionally, we are collaborating with consortiums like ExaDigiT, which focuses on high-performance computing (HPC) and supercomputing, as well as with industry partners, to implement some of these approaches in real-world scenarios. SustainDC's complexity and constraints, rooted in realistic systems, make it a suitable platform for benchmarking hierarchical RL algorithms. We plan to implement continual reinforcement learning to accommodate dynamic data center environments and prevent out-of-distribution errors during equipment upgrades and accessory changes.

Moreover, SustainDC features an extendable, plug-and-play architecture of data center modeling compatible with digital twin frameworks, supporting research into other aspects of data center optimization for joint and multi-objective goals.