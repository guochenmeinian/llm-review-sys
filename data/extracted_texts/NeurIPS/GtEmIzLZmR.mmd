# Achievable Fairness on Your Data With Utility Guarantees

Muhammad Faaiz Taufiq

ByteDance Research

faaiz.taufiq@bytedance.com &Jean-Francois Ton

ByteDance Research

jeanfrancois@bytedance.com &Yang Liu

University of California Santa Cruz

yangliu@ucsc.edu

Corresponding authors: faaiz.taufiq@bytedance.com and jeanfrancois@bytedance.com

###### Abstract

In machine learning fairness, training models that minimize disparity across different sensitive groups often leads to diminished accuracy, a phenomenon known as the fairness-accuracy trade-off. The severity of this trade-off inherently depends on dataset characteristics such as dataset imbalances or biases and therefore, using a uniform fairness requirement across diverse datasets remains questionable. To address this, we present a computationally efficient approach to approximate the fairness-accuracy trade-off curve tailored to individual datasets, backed by rigorous statistical guarantees. By utilizing the You-Only-Train-Once (YOTO) framework, our approach mitigates the computational burden of having to train multiple models when approximating the trade-off curve. Crucially, we introduce a novel methodology for quantifying uncertainty in our estimates, thereby providing practitioners with a robust framework for auditing model fairness while avoiding false conclusions due to estimation errors. Our experiments spanning tabular (e.g., Adult), image (CelebA), and language (Jigsaw) datasets underscore that our approach not only reliably quantifies the optimum achievable trade-offs across various data modalities but also helps detect suboptimality in SOTA fairness methods.

## 1 Introduction

A key challenge in fairness for machine learning is to train models that minimize disparity across various sensitive groups such as race or gender [9; 35; 10]. This often comes at the cost of reduced model accuracy, a phenomenon termed accuracy-fairness trade-off [36; 32]. This trade-off can differ significantly across datasets, depending on factors such as dataset biases, imbalances etc. [1; 8; 11].

To demonstrate how these trade-offs are inherently dataset-dependent, we consider a simple example involving two distinct crime datasets. Dataset A has records from a community where crime rates are uniformly distributed across all racial groups, whereas Dataset B comes from a community where historical factors have resulted in a disproportionate crime rate among a specific racial group. Intuitively, training models which are racially agnostic is more challenging for Dataset B, due to the unequal distribution of crime rates across racial groups, and will result in a greater loss in model accuracy as compared to Dataset A.

This example underscores that setting a uniform fairness requirement across diverse datasets (such as requiring the fairness violation metric to be below 10% for both datasets), while also adhering to essential accuracy benchmarks is impractical. Therefore, choosing fairness guidelines for anydataset necessitates careful consideration of its individual characteristics and underlying biases. In this work, we advocate against the use of one-size-fits-all fairness mandates by proposing a nuanced, dataset-specific framework for quantifying acceptable range of accuracy-fairness trade-offs. To put it concretely, the question we consider is:

_Given a dataset, what is the range of permissible fairness violations corresponding to each accuracy threshold for models in a given class \(\)?_

This question can be addressed by considering the optimum accuracy-fairness trade-off, which shows the minimum fairness violation achievable for each level of accuracy. Unfortunately, this curve is typically unavailable and hence, various optimization techniques have been proposed to approximate this curve, ranging from regularization [8; 33] to adversarial learning [45; 40].

However, approximating the trade-off curve using these aforementioned methods has some serious limitations. Firstly, these methods require retraining hundreds if not thousands of models to obtain a good approximation of the trade-off curve, making them computationally infeasible for large datasets or models. Secondly, these works do not account for finite-sampling errors in the obtained curve. This is problematic since the empirical trade-off evaluated over a finite dataset may not match the exact trade-off over the full data distribution.

We illustrate this phenomenon in Figure 1 where the black and red trade-off curves are obtained using the same model but evaluated over two different test data draws. Here, relying solely on the estimated curves without accounting for the uncertainty could lead us to make the incorrect conclusion that the methodology used to obtain the black trade-off curve is sub-optimal (compared to the red curve) as it achieves a higher fairness violation for accuracies in the range \([0.62,0.66]\). However, this discrepancy arises solely due to finite-sampling errors.

In this paper, we address these challenges by introducing a computationally efficient method of approximating the optimal accuracy-fairness trade-off curve, supported by rigorous statistical guarantees. Our methodology not only circumvents the need to train multiple models (leading to at least a 10-fold reduction in computational cost) but is also the first to quantify the uncertainty in the estimated curve, arising from both, finite-sampling error as well as estimation error. To achieve this, our approach adopts a novel probabilistic perspective and provides guarantees that remain valid across all finite-sample draws. This also allows practitioners to distinguish if an apparent suboptimality in a baseline could be explained by finite-sampling errors (as in the black curve in Figure 1), or if it stems from genuine deficiencies in the fairness interventions applied (as in the blue curve).

The contributions of this paper are three-fold:

Figure 1: Accuracy-fairness trade-offs for COMPAS dataset (on held-out data). The black and red curves are obtained using the same optimally trained model evaluated on different splits. The blue curve is obtained using a suboptimally trained model. The green area depicts the range of permissible fairness violations for each accuracy, pink area shows suboptimal accuracy-fairness trade-offs, and blue area shows unlikely-to-be-achieved ones. (Details in Appendix F.5)

* We present a computationally efficient methodology for approximating the accuracy-fairness trade-off curve by training only a single model. This is achieved by adapting a technique from  called You-Only-Train-Once (YOTO) to the fairness setting.
* To account for the approximation and finite-sampling errors, we introduce a novel technical framework to construct confidence intervals (using the trained YOTO model) which contain the optimal accuracy-fairness trade-off curve with statistical guarantees. For any accuracy threshold \(\) chosen at _inference time_, this gives us a statistically backed range of permissible fairness violations \([l(),u()]\), allowing us to answer our previously posed question: _Given a dataset, the permissible range of fairness violations corresponding to an accuracy threshold of \(\) is \([l(),u()]\) for models in a given class \(\)._
* Lastly, we showcase the vast applicability of our method empirically across various data modalities including tabular, image and text datasets. We evaluate our framework on a suite of SOTA fairness methods and show that our intervals are both reliable and informative.

## 2 Preliminaries

**Notation** Throughout this paper, we consider a binary classification task, where each training sample is composed of triples, \((X,A,Y)\). \(X\) denotes a vector of features, \(A\) indicates a discrete sensitive attribute, and \(Y\{0,1\}\) represents a label. To make this more concrete, if we take loan default prediction as the classification task, \(X\) represents individuals' features such as their income level and loan amount; \(A\) represents their racial identity; and \(Y\) represents their loan default status. Having established the notation, for completeness, we provide some commonly used fairness violations \(_{}(h)\) for a classifier model \(h:\) when \(=\{0,1\}\):

**Demographic Parity (DP)** The DP condition states that the selection rates for all sensitive groups are equal, i.e. \((h(X)=1 A=a)=(h(X)=1)\) for any \(a\). The absolute DP violation is:

\[_{}(h)|(h(X)=1 A=1)-(h(X)=1 A =0)|.\]

**Equalized Opportunity (EOP)** The EOP condition states that the true positive rates for all sensitive groups are equal, i.e. \((h(X)=1 A=a,Y=1)=(h(X)=1 Y=1)\) for any \(a\). The absolute EOP violation is:

\[_{}(h) |(h(X)=1 A=1,Y=1)-(h(X)=1 A=0,Y=1)|.\]

### Problem setup

Next, we formalise the notion of _accuracy-fairness trade-off_, which is the main quantity of interest in our work. For a model class \(\) (e.g., neural networks) and a given accuracy threshold \(\), we define the optimal accuracy-fairness trade-off \(^{*}_{}()\) as,

\[^{*}_{}()_{h}_{} (h)(h).\] (1)

Here, \(_{}(h)\) and \((h)\) denote the fairness violation and accuracy of \(h\) over the _full data distribution_. For an accuracy \(^{}\) which is unattainable, we define \(^{*}_{}(^{})=1\) and we focus on models \(\) trained using gradient-based methods. Crucially, our goal is not to estimate the trade-off at a fixed accuracy level, but instead to reliably and efficiently estimate the _entire_ trade-off curve \(^{*}_{}\). In contrast, previous works [1; 11] impose an apriori fairness constraint during training and therefore each trained model only recovers one point on the trade-off curve corresponding to this pre-specified constraint.

If available, this trade-off curve would allow practitioners to characterise exactly how, for a given dataset, the minimum fairness violation varies as model accuracy increases. This not only provides a principled way of selecting data-specific fairness requirements, but also serves as a tool to audit if a model meets acceptable fairness standards by checking if its accuracy-fairness trade-off lies on this curve. Nevertheless, obtaining this ground-truth trade-off curve exactly is impossible within the confines of a finite-sample regime (owing to finite-sampling errors). This means that even if baseline A's empirical trade-off evaluated on a finite dataset is suboptimal compared to the empirical trade-off of baseline B, this does not necessarily imply suboptimality on the full data distribution.

We illustrate this in Figure 1 where both red and black trade-off curves are obtained using the same model but evaluated on different test data splits. Here, even though the black curve appears suboptimal compared to the red curve for accuracies in \([0.62,0.66]\), this apparent suboptimality is solely due to finite-sampling errors (since the discrepancy between the two curves arises only due to different evaluation datasets). If we rely only on comparing empirical trade-offs, we would incorrectly flag the methodology used to obtain the black curve as suboptimal.

To address this, we construct confidence intervals (CIs), shown as the green region in Figure 1, that account for such finite-sampling errors. In this case both trade-offs fall within our CIs which correctly indicates that this apparent suboptimality could stem from finite-sample variability. Conversely, a baseline's trade-off falling above our CIs (as in the blue curve in Figure 1) offers a confident assessment of suboptimality, as this cannot be explained away by finite-sample variability. Therefore, our CIs equip practitioners with a robust auditing tool. They can confidently identify suboptimal baselines while avoiding false conclusions caused by considering empirical trade-offs alone.

High-level road mapTo achieve this, our proposed methodology adopts a two-step approach:

1. Firstly, we propose _loss-conditional fairness training_, a computationally efficient methodology of estimating the entire trade-off curve \(^{*}_{}\) by training a single model, obtained by adapting the YOTO framework  to the fairness setting.
2. Secondly, to account for the approximation and finite-sampling errors in our estimates, we introduce a novel methodology of constructing confidence intervals on the trade-off curve \(^{*}_{}\) using the trained YOTO model. Specifically, given \((0,1)\), we construct confidence intervals \(^{}_{}\) which satisfy guarantees of the form: \[(^{*}_{}()^{}_{}) 1-.\] Here, \(^{}_{}\) and \(\) are random variables obtained using a held-out calibration dataset \(_{}\) (see Section 3.2) and the probability is taken over different draws of \(_{}\).

## 3 Methodology

First, we demonstrate how our 2-step approach offers a practical and statistically sound method for estimating \(^{*}_{}()\). Figure 1 provides an illustration of our proposed confidence intervals (CIs) \(^{}_{}\) and shows how they can be interpreted as a range of 'permissible' values of accuracy-fairness trade-offs (the green region). Specifically, if for a classifier \(h_{0}\), the accuracy-fairness pair \(((h_{0}),_{}(h_{0}))\) lies above the CIs \(^{}_{}\) (i.e., the pink region in Figure 1), then \(h_{0}\) is likely to be suboptimal in terms of the fairness violation, i.e., there likely exists \(h^{}\) with \((h^{})(h_{0})\) and \(_{}(h^{})_{}(h_{0})\). On the other hand, it is unlikely for any model \(h^{}\) to achieve a trade-off below the CIs \(^{}_{}\) (the blue region in Figure 1). Next, we outline how to construct such intervals.

### Step 1: Efficient estimation of trade-off curve

The first step of constructing the intervals is to approximate the trade-off curve by recasting the problem into a constrained optimization objective. The optimization problem formulated in Eq. (1) is however, often too complex to solve, because the accuracy \((h)\) and fairness violations \(_{}(h)\) are both non-smooth . These constraints make it hard to use standard optimization methods that rely on gradients . To get around this issue, previous works [1; 8] replace the non-smooth constrained optimisation problem with a smooth surrogate loss. Here, we consider parameterized family of classifiers \(=\{h_{}:\,|\,\}\) (such as neural networks) trained using the regularized loss:

\[_{}()=[l_{}(h_{}(X),Y)]+ \,_{}(h_{}).\] (2)

where, \(l_{}\) is the cross-entropy loss for the classifier \(h_{}\) and \(_{}(h_{})\) is a smooth relaxation of the fairness violation \(_{}\)[8; 29]. For example, when the fairness violation is DP,  consider

\[_{}(h_{})=[g(h_{}(X))\;|\;A=1]- [g(h_{}(X))\;|\;A=0],\]

for different choices of \(g(x)\), including the identity and sigmoid functions. We include more examples of such regularizers in Appendix F.3. The parameter \(\) in \(_{}\) modulates the accuracy-fairness trade-off with lower values of \(\) favouring higher accuracy over reduced fairness violation.

Now that we defined the optimization objective, obtaining the trade-off curve becomes straightforward by simply optimizing multiple models over a grid of regularization parameters \(\). However, training multiple models can be computationally expensive, especially when this involves large-scale models (e.g. neural networks). To circumvent this computational challenge, we introduce loss-conditional fairness training obtained by adapting the YOTO framework proposed by .

#### 3.1.1 Loss-conditional fairness training

As we describe above, a popular approach for approximating the accuracy-fairness trade-off \(^{*}_{}()\) involves training multiple models \(h_{^{*}_{}}\) over a discrete grid of \(\) hyperparameters with the regularized loss \(_{}\). To avoid the computational overhead of training multiple models,  propose 'You Only Train Once' (YOTO), a methodology of training one model \(h_{}:\), which takes \(\) as an additional input using Feature-wise Linear Modulation (FiLM)  layers. YOTO is trained such that at inference time \(h_{}(,^{})\) recovers the classifier obtained by minimising \(_{^{}}\) in Eq. (2).

Recall that we are interested in minimising the family of losses \(_{}\), parameterized by \(\) (Eq. (2)). Instead of fixing \(\), YOTO solves an optimisation problem where the parameter \(\) is sampled from a distribution \(P_{}\). As a result, during training the model observes many values of \(\) and learns to optimise the loss \(_{}\) for all of them simultaneously. At inference time, the model can be conditioned on a chosen value \(^{}\) and recovers the model trained to optimise \(_{^{}}\). Hence, once adapted to our setting, the YOTO loss becomes:

\[*{arg\,min}_{h_{}:} _{ P_{}}[[l_{}(h_{}(X,),Y)]+\,_{}(h_{}(,)) ].\]

Having trained a YOTO model, the trade-off curve \(^{*}_{}()\) can be approximated by simply plugging in different values of \(\) at inference time and thus avoiding additional training. From a theoretical point of view, [15, Proposition 1] proves that under the assumption of large enough model capacity, training the loss-conditional YOTO model performs as well as the separately trained models while only requiring a single model. Although the model capacity assumption might be hard to verify in practice, our experimental section has shown that the trade-off curves estimates \(_{}()}\) obtained using YOTO are consistent with the ones obtained using separately trained models.

It should be noted, as is common in optimization problems, that the estimated trade-off curve \(_{}()}\) may not align precisely with the true trade-off curve \(^{*}_{}()\). This discrepancy originates from two key factors. Firstly, the limited size of the training and evaluation datasets introduces errors in the estimation of \(_{}()}\). Secondly, we opt for a computationally tractable loss function instead of the original optimization problem in Eq. (1). This may result in our estimation \(_{}()}\) yielding sub-optimal trade-offs, as can be seen from Figure 1. Therefore, to ensure that our procedure yields statistically sound inferences, we next construct confidence intervals using the YOTO model, designed to contain the true trade-off curve \(^{*}_{}()\) with high probability.

### Step 2: Constructing confidence intervals

As mentioned above, our goal here is to use our trained YOTO model to construct confidence intervals (CIs) for the optimal trade-off curve \(^{*}_{}()\) defined in Eq. (1). Specifically, we assume access to a held-out _calibration_ dataset \(_{}\{(X_{i},A_{i},Y_{i})\}_{i}\) which is disjoint from the training data. Given a level \(\), we construct CIs \(^{}_{}\) using \(_{}\), which provide guarantees of the form:

\[(^{*}_{}()^{}_{}) 1 -.\] (3)

Here, it is important to note that \(\) and \(^{}_{}\) are random variables obtained from the calibration data \(_{}\), and the guarantee in Eq. (3) holds marginally over \(\) and \(^{}_{}\). While our CIs in this section require the availability of the sensitive attributes in \(_{}\), in Appendix D we also extend our methodology to the setting where sensitive attributes are missing. In this section, for notational convenience we use \(h_{}()\) to denote the YOTO model \(h_{}(,)\) for \(\).

The uncertainty in our trade-off estimate arises, in part, from the uncertainty in the accuracy and fairness violations of our trained model. Therefore, our methodology of constructing CIs on \(^{*}_{}\), involves first constructing CIs on test accuracy \((h_{})\) and fairness violation \(_{}(h_{})\) for a given value of \(\) using \(_{}\), denoted as \(C^{}_{}()\) and \(C^{}_{}()\) respectively satisfying,

\[((h_{}) C^{}_{}()) 1 -,(_{}(h_{}) C^{ }_{}()) 1-.\]

One way to construct these CIs involves using assumption-light concentration inequalities such as Hoeffding's inequality. To be more concrete, for the accuracy \((h_{})\):

**Lemma 3.1** (Hoeffding's inequality).: _Given a classifier \(h_{}:\), we have that,_

\[((h_{})[(h_{ })}-,(h_{})}+]) 1 -.\]

_Here, \((h)}_{(X_{i},A_{i},Y_{i})_{ }})=Y_{i})}{|_{}|}\) and \(_{}|}()}\)._Lemma 3.1 illustrates that when can use Hoeffding's inequality to construct confidence interval \(C_{}^{}()=(h_{})}-, (h_{})}+]\) on \((h_{})\) such that the true \((h_{})\) will lie inside the CI with probability \(1-\). Analogously, we also construct CIs for fairness violations, \(_{}(h_{})\), although this is subject to additional nuanced challenges, which we address using a novel sub-sampling based methodology in Appendix B. Once we have CIs over \((h_{})\) and \(_{}(h_{})\) for a model \(h_{}\), we next outline how to use these to derive CIs for the minimum achievable fairness \(_{}^{*}\), satisfying Eq. (3). We proceed by explaining how to construct the upper and lower CIs separately, as the latter requires additional considerations regarding the trade-off achieved by YOTO.

#### 3.2.1 Upper confidence intervals

We first outline how to obtain one-sided upper confidence intervals on the optimum accuracy-fairness trade-off \(_{}^{*}()\) of the form \(_{}^{}=[0,U_{}^{}]\), which satisfies the probabilistic guarantee in Eq. (3). To this end, given a classifier \(h_{}\), our methodology involves constructing one-sided lower CI on the accuracy \((h_{})\) and upper CI on the fairness violation \(_{}(h_{})\). We make this concrete below:

**Proposition 3.2**.: _Given \(h_{}\), let \(L_{}^{},U_{}^{}\) be lower and upper CIs on \((h_{})\) and \(_{}(h_{})\), i.e._

\[((h_{}) L_{}^{})  1-/2,(_{}(h_{ }) U_{}^{}) 1-/2.\]

_Then, \((_{}^{*}(L_{}^{}) U_{ {fair}}^{}) 1-\)._

Proposition 3.2 shows that for any model \(h_{}\), the upper CI on model fairness, \(U_{}^{}\), provides a valid upper CI for the trade-off value at \(L_{}^{}\), i.e. \(_{}^{*}(L_{}^{})\). This can be used to construct upper CIs on \(_{}^{*}()\) for a given accuracy level \(\). To understand how this can be achieved, we first find \(\) such that the lower CI on the accuracy of model \(h_{}\), \(L_{}^{}\), satisfies \(L_{}^{}\). Then, since by definition \(_{}^{*}\) is a monotonically increasing function, we know that \(_{}^{*}(L_{}^{})_{}^{*}()\). Since Proposition 3.2 tells us that \(U_{}^{}\) is an upper CI for \(_{}^{*}(L_{}^{})\), it follows that \(U_{}^{}\) is also a valid upper CI for \(_{}^{*}()\).

Intuitively, Proposition 3.2 provides the 'worst-case' optimal trade-off, accounting for finite-sample uncertainty. It is important to note that this result does not rely on any assumptions regarding the optimality of the trained classifiers. This means that the upper CIs will remain valid even if the YOTO classifier \(h_{}\) is not trained well (and hence achieves sub-optimal accuracy-fairness trade-offs), although in such cases the CI may be conservative.

Having explained how to construct upper CIs on \(_{}^{*}()\), we next move on to the lower CIs.

#### 3.2.2 Lower confidence intervals

Obtaining lower confidence intervals on \(_{}^{*}()\) is more challenging than obtaining upper confidence intervals. We begin by explaining at an intuitive level why this is the case.

Suppose that \(h_{}\) is such that \((h_{})=\), then since \(_{}^{*}\) denotes the minimum attainable fairness violation (Eq. (1)), we have that \(_{}^{*}()_{}(h_{})\). Therefore, any valid upper confidence interval on \(_{}(h_{})\) will also be valid for \(_{}^{*}()\). However, a lower bound on \(_{}(h_{})\) cannot be used as a lower bound for the minimum achievable fairness \(_{}^{*}()\) in general. A valid lower CI for \(_{}^{*}()\) will therefore depend on the gap between fairness violation achieved by \(h_{}\), \(_{}(h_{})\), and minimum achievable fairness violation \(_{}^{*}()\) (i.e., \((h_{})\) term in Figure 1(a)). We make this concrete by constructing lower CIs depending on \((h_{})\) explicitly.

**Proposition 3.3**.: _Given \(h_{}\), let \(U_{}^{},L_{}^{}\) be upper and lower CIs on \((h_{})\) and \(_{}(h_{})\), i.e._

\[((h_{}) U_{}^{}) 1-/2, (_{}(h_{}) L_{}^{}) 1-/2.\]

_Then, \((_{}^{*}(U_{}^{}) L_{}^{}-(h_{})) 1-\), where \((h_{})_{}(h_{})-_{}^{*}( (h_{})) 0\)._

Proposition 3.3 can be used to derive lower CIs on \(_{}^{*}()\) at a specified accuracy level \(\), using a methodology analogous to that described in Section 3.2.1. Intuitively, this result provides the 'best-case' optimal trade-off, accounting for finite-sample uncertainty. However, unlike the upper CI, the lower CI includes the \((h_{})\) term, which is typically unknown. To circumvent this, we propose a strategy for obtaining plausible approximations for \((h_{})\) in practice in the following section.

#### 3.2.3 Sensitivity analysis for \((h_{})\)

Recall that \((h_{})\) quantifies the difference between the fairness loss of classifier \(h_{}\) and the minimum attainable fairness loss \(^{*}_{}((h_{}))\), and is an unknown quantity in general (see Figure 1(a)). Here, we propose a practical strategy for positing values for \((h_{})\) which encode our belief on how close the fairness loss \(_{}(h_{})\) is to \(^{*}_{}((h_{}))\). This allows us to construct CIs which not only incorporate finite-sampling uncertainty from calibration data, but also account for the possible sub-optimality in the trade-offs achieved by \(h_{}\). The main idea behind our approach is to calibrate \((h_{})\) using additional separately trained standard models without imposing significant computational overhead.

DetailsOur sensitivity analysis uses \(k\) additional models \(\{h^{(1)},h^{(2)},,h^{(k)}\}\) trained separately using the standard regularized loss \(_{^{}}\) (Eq. (2)) for some randomly chosen values of \(^{}\). Let \(_{0}\) denote the models which achieve a better empirical trade-off than the YOTO model on \(_{}\), i.e. the empirical trade-offs for models in \(_{0}\) lie below the YOTO trade-off curve (see Figure 1(b)). We choose \((h_{})\) for our YOTO model to be the maximum gap between empirical trade-offs of these separately trained models in \(_{0}\) and the YOTO model. It can be seen from Proposition 3.3 that, in practice, this will result in a downward shift in the lower CI until all the separately trained models in \(\) lie above the lower CI. As a result, our methodology yields increasingly conservative lower CIs as the number of additional models \(||\) increases.

Even though the procedure above requires training additional models \(\), it does not impose the same computational overhead as training models over the full range of \(\) values. We show empirically in Section 5 that in practice 2 models are usually sufficient to obtain informative and reliable intervals. Additionally, we also show that when YOTO achieves the optimal trade-off (i.e., \((h_{})=0\)), our sensitivity analysis leaves the CIs unchanged, thereby preventing unnecessary conservatism.

Asymptotic analysis of \((h_{})\)While the procedure described above provides a practical solution for obtaining plausible approximations for \((h_{})\), we next present a theoretical result which provides reassurance that this gap should become negligible as the number of training data \(_{}\) increases.

**Theorem 3.4**.: _Let \(}(h^{})},(h^{ })}\) denote the fairness violation and accuracy for \(h^{}\) evaluated on training data \(_{}\) and let_

\[h_{h^{}}}(h^{ })}(h^{})}.\] (4)

_Then, given \((0,1)\), under standard regularity assumptions, we have that with probability at least \(1-\), \((h)(|_{}|^{-}),\) for some \((0,1/2]\) where \(()\) suppresses dependence on \(\)._

Theorem 3.4 shows that as the training data size \(|_{}|\) increases, the error term \((h_{})\) will become negligible with a high probability for any model \(h_{}\) which minimises the empirical training loss in Eq. (4). In this case, \((h_{})\) should not have a significant impact on the lower CIs in Proposition 3.3 and the CIs will reflect the uncertainty in \(^{*}_{}\) arising mostly due to finite calibration data. We also verify this empirically in Appendix F.7. It is worth noting that Theorem 3.4 relies on the same assumptions as used in Theorem 2 in , which have been provided in Appendix A.2.

Figure 2: Visual illustrations for \((h)\) (Figure 1(a)) and our sensitivity analysis procedure (Figure 1(b)).

## 4 Related works

Many previous fairness methods in the literature, termed in-processing methods, introduce constraints or regularization terms to the optimization objective. For instance, [1; 10] impose a priori uniform constraints on model fairness at training time. However, given the data-dependent nature of accuracy-fairness trade-offs, setting a uniform fairness threshold may not be suitable. Other in-processing methods [37; 24] consider information-theoretic bounds on the optimal trade-off in infinite data limit, independent of a specific model class. While these works offer valuable theoretical insights, there is no guarantee that these frontiers are attainable by models within a given model class \(\). We verify this empirically in Appendix F.6 by showing that, for the Adult dataset, the frontiers proposed in  are not achieved by any SOTA method we considered. In contrast, our method provides guarantees on the _achievable_ trade-off curve within realistic constraints of model class and data availability.

Various other regularization approaches [39; 33; 8; 14; 41; 42; 43] have also been proposed, but these often necessitate training multiple models, making them computationally intensive. Alternative strategies include learning 'fair' representations [44; 30; 31], or re-weighting data based on sensitive attributes [18; 22]. These, however, provide limited control over accuracy-fairness trade-offs.

Besides this, post-processing methods [19; 38] enforce fairness after training but can lead to other forms of unfairness such as disparate treatment of similar individuals . Moreover, many post-hoc approaches such as [3; 2] still require solving different optimisation problems for different fairness thresholds. Other methods such as [46; 27] involve learning a post-hoc module in addition to the base classifier. As a result, the computational cost of training the YOTO model is similar to (and in many cases lower than) the combined cost of training a base model and subsequently applying a post-processing intervention to this pre-trained classifier. We confirm this empirically in Section 5.

## 5 Experiments

In this section, we empirically validate our methodology of constructing confidence intervals on the fairness trade-off curve, across diverse datasets with neural networks as model class \(\). These datasets range from tabular (Adult and COMPAS ), to image-based (CelebA), and natural language processing datasets (Jigsaw). Recall that our approach involves two steps: initial estimation of the trade-off via the YOTO model, followed by the construction of CIs using calibration data \(_{}\).

Figure 3: Results on four real-world datasets where \(_{}\) is a 10% data split. Here, \(=0.05\) and we use \(||=2\) separately trained models for sensitivity analysis.

To evaluate our methodology, we implement a suite of baseline algorithms including SOTA in-processing techniques such as regularization-based approaches , a SOTA kernel-density based method  (denoted as 'KDE-fair'), as well as the reductions method . Additionally, we also compare against adversarial fairness techniques  and a post-processing approach (denoted as 'RTO')  and consider the three most prominent fairness metrics: Demographic Parity (DP), Equalized Odds (EO), and Equalized Opportunity (EOP). We provide additional details and results in Appendix F, where we also consider a synthetic setup with tractable \(^{*}_{}\). The code to reproduce our experiments is provided at github.com/faairf/DatasetFairness.

### Results

Figure 3 shows the results for different datasets and fairness violations, obtained using a 10% data split as calibration dataset \(_{}\). For each dataset, we construct 4 CIs that serve as the upper and lower bounds on the optimal accuracy-fairness trade-off curve. These intervals are computed at a 95% confidence level using various methodologies, including 1) Hoeffding's, 2) Bernstein's inequalities which both offer finite sample guarantees as well as, 3) bootstrapping , and 4) asymptotic intervals based on the Central Limit Theorem  which are valid asymptotically. There are 4 key takeaways:

**Takeaway 1: Trade-off curves are data dependent.** The results in Figure 3 confirm that the accuracy-fairness trade-offs can vary significantly across the datasets. For example, achieving near-perfect fairness (i.e. \(_{}(h) 0\)) seems significantly easier for the Jigsaw dataset than the COMPAS dataset, even as the accuracy increases. Likewise, for Adult and COMPAS, the DP increases gradually with increasing accuracy, whereas for CelebA, the increase is sharp once the accuracy increases above 90%. Therefore, using a uniform fairness threshold across datasets [as in 1] may be too restrictive, and our methodology provides more dataset-specific insights about the entire trade-off curve instead.

**Takeaway 2: Our CIs are both reliable and informative.** Recall that, any trade-off which lies above our upper CIs is guaranteed to be sub-optimal with probability \(1-\), thereby enabling practitioners to effectively distinguish between genuine sub-optimalities and those due to finite-sample errors. Table 1 lists the proportion of sub-optimal empirical trade-offs for each baseline and provides a principled comparison of the baselines. For example, the adversarial, RTO and logsig baselines have a significantly higher proportion of sub-optimal trade-offs than the KDE-fair and separate baselines.

On the other hand, the validity of our lower CIs depends on the optimality of our YOTO model and the lower CIs may be too tight if YOTO is sub-optimal. Therefore, for the lower CIs to be reliable, it must be unlikely for any baseline to achieve a trade-off below the lower CIs. Table 1 confirms this empirically, as the proportion of models which lie below the lower CIs is negligible. In Appendix E, we also account for the uncertainty in baseline trade-offs when assessing the optimality, hence yielding more robust inferences. The results remain similar to those in Table 1.

**Takeaway 3: YOTO trade-offs are consistent with SOTA.** We observe that the YOTO trade-offs align well with most of the SOTA baselines considered while reducing the computational cost by approximately 40-fold (see the final column of Table 1). In some cases, YOTO even achieves a better trade-off than the baselines considered. See, e.g., the Jigsaw dataset results (especially for EOP). Moreover, we observe that the baselines yield empirical trade-offs which have a high variance as

   Category & Baseline & Unlikely & Permissable & Sub-optimal & \(\) Training time \\   & adversary  & \(0.03 0.03\) & \(0.51 0.07\) & \(0.45 0.07\) & \(100\) min\( 40\) \\  & logging  & \(0.0 0.0\) & \(0.66 0.1\) & \(0.33 0.1\) & \(100\) min\( 40\) \\  & reductions  & \(0.0 0.0\) & \(0.79 0.1\) & \(0.21 0.1\) & \(90\) min\( 40\) \\ In-processing & linear  & \(0.01 0.0\) & \(0.85 0.05\) & \(0.14 0.06\) & \(100\) min\( 40\) \\  & KDE-fair  & \(0.0 0.0\) & \(0.97 0.05\) & \(0.03 0.06\) & \(85\) min\( 40\) \\  & separate & \(0.0 0.0\) & \(0.98 0.01\) & \(0.02 0.02\) & \(100\) min\( 40\) \\  & YOTO (Ours) & \(0.0 0.0\) & \(1.0 0.0\) & \(0.0 0.0\) & **105 min\( 1\)** \\  Post-processing & RTO  & \(0.0 0.0\) & \(0.65 0.2\) & \(0.35 0.05\) & 
 95 min (training base classifier) \\ + 10min\( 40\) (post-hoc optimisations) \\  \\   

Table 1: Proportion of empirical trade-offs for each baseline in the three trade-off regions, aggregated across all datasets and fairness metrics (using Bernstein’s CIs). ‘Unlikely’, ‘Permissible’ and ‘Sub-optimal’ correspond to the blue, green and pink regions in Figure 1 respectively. The last column shows the rough average training time per model across experiments \(\) no. of models per experiment.

accuracy increases (see Jigsaw results in Figure 3, for example). This behaviour starkly contrasts the smooth variations exhibited by our YOTO-generated trade-off curves along the accuracy axis.

**Takeaway 4: Sensitivity analysis does not cause unnecessary conservatism.** We use 2 randomly chosen separately trained models to perform our sensitivity analysis for Figure 3. We find that this only causes a shift in lower CIs for 2 out of the 12 trade-off curves presented (i.e. for DP and EO trade-offs on the Adult dataset), leaving the rest of the CIs unchanged. Therefore, in practice sensitivity analysis does not impose significant computational overhead, and only changes the CIs when YOTO achieves a suboptimal trade-off. Additional results have been included in Appendix C.

## 6 Discussion and Limitations

In this work, we propose a computationally efficient approach to capture the accuracy-fairness trade-offs inherent to individual datasets, backed by sound statistical guarantees. Our proposed methodology enables a nuanced and dataset-specific understanding of the accuracy-fairness trade-offs. It does so by obtaining confidence intervals on the accuracy-fairness trade-off, leveraging the computational benefits of the You-Only-Train-Once (YOTO) framework . This empowers practitioners with the ability to, at inference time, specify desired accuracy levels and promptly receive corresponding permissible fairness ranges. By eliminating the need for repetitive model training, we significantly streamline the process of obtaining accuracy-fairness trade-offs tailored to individual datasets.

**Limitations** Despite the evident merits of our approach, it also has some limitations. Firstly, our methodology requires distinct datasets for training and calibration, posing difficulties when data is limited. Under such constraints, the YOTO model might not capture the optimal accuracy-fairness trade-off, and moreover, the resulting confidence intervals could be overly conservative. Secondly, our lower CIs incorporate an unknown term \((h_{})\). While we propose sensitivity analysis for approximating this term and prove that it is asymptotically negligible under certain mild assumptions in Section 3.2.3, a more exhaustive understanding remains an open question. Exploring informative upper bounds for \((h_{})\) under weaker conditions is a promising avenue for future investigations.