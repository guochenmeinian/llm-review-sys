# Differentiable and Stable Long-Range Tracking of Multiple Posterior Modes

Ali Younis and Erik B. Sudderth

{ayounis, sudderth}@uci.edu

Department of Computer Science, University of California, Irvine

###### Abstract

Particle filters flexibly represent multiple posterior modes nonparametrically, via a collection of weighted samples, but have classically been applied to tracking problems with known dynamics and observation likelihoods. Such generative models may be inaccurate or unavailable for high-dimensional observations like images. We instead leverage training data to discriminatively learn particle-based representations of uncertainty in latent object states, conditioned on arbitrary observations via deep neural network encoders. While prior discriminative particle filters have used heuristic relaxations of discrete particle resampling, or biased learning by truncating gradients at resampling steps, we achieve unbiased and low-variance gradient estimates by representing posteriors as continuous mixture densities. Our theory and experiments expose dramatic failures of existing reparameterization-based estimators for mixture gradients, an issue we address via an importance-sampling gradient estimator. Unlike standard recurrent neural networks, our mixture density particle filter represents multimodal uncertainty in continuous latent states, improving accuracy and robustness. On a range of challenging tracking and robot localization problems, our approach achieves dramatic improvements in accuracy, while also showing much greater stability across multiple training runs.

## 1 Introduction

A _particle filter_ (PF)  uses weighted samples to provide a flexible, nonparametric representation of uncertainty in continuous latent states. Classical PFs are generative models that typically require human experts to specify the latent state dynamics and measurement models. With the growing popularity of deep learning and abundance of time-series data, recent work has instead sought to discriminatively learn PF variants that (unlike conventional recurrent neural networks) capture uncertainty in latent states . These approaches are especially promising for high-dimensional observations like images, where learning accurate generative models is extremely challenging.

The key challenge in learning discriminative PFs is the non-differentiable particle resampling step, which is key to robustly maintaining diverse particle representations, but inhibits end-to-end learning by preventing gradient propagation. Prior discriminative PFs have typically used heuristic relaxations of discrete particle resampling, or biased learning by truncating gradients. These methods severely compromise the effectiveness of PF training; prior work has often assumed exact dynamical models are known, or used very large numbers of particles for test data, due to these limitations.

After reviewing prior work on generative (Sec. 2) and discriminative (Sec. 3) training of PFs, Sec. 4 demonstrates dramatic failures of popular reparameterization-based gradient estimators for mixture models. This motivates our _importance weighted samples gradient_ estimator, which provides the foundation for the _mixture density particle filter_ of Sec. 5. Experiments in Sec. 6 show substantial advances over prior discriminative PF for several challenging tracking and visual localization tasks.

## 2 Sequential State Estimation via Generative Models

Algorithms for sequential state estimation compute or approximate the posterior distribution of the system state \(x_{t}\), given a sequence of observations \(y_{t}\) and (optionally) input actions \(a_{t}\), at discrete times \(t=1,,T\). Sequential state estimation is classically formulated as inference in a generative model like the _hidden Markov model_ (HMM) or state space model of Fig. 1, with dynamics defined by state transition probabilities \(p(x_{t} x_{t-1},a_{t})\), and observations generated via likelihoods \(p(y_{t} x_{t})\).

Given a known Markov prior on latent state sequences, and a corresponding observation sequence, the _filtered_ state posterior \(p(x_{t} y_{t},y_{t-1},,y_{1})\) can in principle be computed via Bayesian inference. When the means of state dynamics and observations likelihoods are linear functions, and noise is Gaussian, exact filtered state posteriors are Gaussian and may be efficiently computed via the _Kalman filter_ (KF) [9; 10; 11]. The non-Gaussian state posteriors of general state space models may be approximated via Gaussians , but estimating the posterior mean and covariance is in general challenging, and cannot faithfully capture the multimodal posteriors produced by missing or ambiguous data.

### Particle Filters

To flexibly approximate general state posteriors, particle filters (PF) [1; 2; 3; 4] represent possible states nonparametrically via a collection of weighted samples or _particles_. The classical PF parameterizes the state posterior at time \(t\) by a set of \(N\) particles \(x_{t}^{(:)}=\{x_{t}^{(1)},,x_{t}^{(N)}\}\) with associated weights \(w_{t}^{(:)}=\{w_{t}^{},,w_{t}^{(N)}\}\). PFs recursively update the state posterior given the latest observation \(y_{t}\) and action \(a_{t}\), and may be flexibly applied to a broad range of models; they only assume that the state dynamics may be simulated, and the observation likelihood may be evaluated.

**Particle Proposal.** To produce particle locations \(x_{t}^{(:)}\) at time \(t\), a model of the state transition dynamics is applied individually to each particle \(x_{t-1}^{(i)}\), conditioned on external actions \(a_{t}\) if available:

\[x_{t}^{(i)} p(x_{t} x_{t-1}=x_{t-1}^{(i)},a_{t}).\] (1)

**Measurement Update.** After a new particle set \(x_{t}^{(:)}\) has been proposed, particle weights must be updated to account for the latest observation \(y_{t}\) via the known likelihood function:

\[w_{t}^{(i)} p(y_{t} x_{t}^{(i)})w_{t-1}^{(i)}.\] (2)

The updated weights are then normalized so that \(_{i=1}^{N}w_{t}^{(i)}=1\). This weight update is motivated by importance sampling principles, because (asymptotically, as \(N\)) it provides an unbiased approximation of the true state posterior \(p(x_{t} y_{t},,y_{1}) p(y_{t} x_{t})p(x_{t} y_{t-1},,y_{1})\).

**Particle Resampling.** Due to the stochastic nature of PFs, over time particles will slowly diverge to regions with low posterior probability, and will thus be given little weight in the measurement step. If all but a few particles have negligible weight, the diversity and effective representational power of the particle set is diminished, resulting in poor estimates of the state posterior.

To address this, particle resampling is used to maintain diversity of the particle set over time, and may be performed at every iteration or more selectively when the "effective" sample size becomes too small . During resampling, a new uniformly-weighted particle set \(_{t}^{(:)}\) is constructed by discrete resampling with replacement. Each resampled particle is a copy of some existing particle, where copies are sampled with probability proportional to the particle weights:

\[_{t}^{(i)}=x_{t}^{(j)}, j(w_{t}^{(1)},, w_{t}^{(N)}).\] (3)

After resampling, assigning uniform weights \(_{t}^{(i)}=\) maintains an unbiased approximate state posterior. This discrete resampling is non-differentiable, preventing gradient estimation via standard reparameterization [14; 15; 16], and inhibiting end-to-end learning of dynamics and measurement models.

Figure 1: Sequential state estimation may be formulated via either generative (_left_) or discriminative (_right_) graphical models. In either case, dynamics of states \(x_{t}\) are influenced by actions \(a_{t}\), and estimated via data \(y_{t}\).

### Regularized Particle Filters

The regularized PF [17; 18] acknowledges that a small set of discrete samples will never _exactly_ align with the true continuous state, and thus estimates a continuous state density \(m(x_{t} x_{t}^{(:)},w_{t}^{(:)},)\) by convolving particles with a continuous kernel function \(K\) with bandwidth \(\):

\[m(x_{t} x_{t}^{(:)},w_{t}^{(:)},)=_{i=1}^{N}w_{t}^{(i)}K(x_{t}-x_{ t}^{(i)};).\] (4)

The kernel function could be a Gaussian, in which case \(\) is a (dimension-specific) standard deviation. The extensive literature on _kernel density estimation_ (KDE)  provides theoretical guidance on the choice of kernel. Quadratic Epanechnikov kernels take \(K(u;)=(1-(u/)^{2})\) if \(|u/| 1\), \(K(u;)=0\) otherwise, and asymptotically minimize the mean-squared-error in the estimation of the underlying continuous density. A number of methods have been proposed for selecting the smoothing bandwidth \(\)[19; 20; 21], but they often have asymptotic justifications, and can be unreliable for small \(N\).

Attractively, regularized PFs resample particles \(_{t}^{(i)} m(x_{t} x_{t}^{(:)},w_{t}^{(:)},)\) from the continuous mixture density rather than the discrete particle set. By resampling from a continuous distribution, regularized PFs ensure that no duplicates exist in the resampled particle set, increasing diversity while still concentrating particles in regions of the state space with high posterior probability. Our proposed _mixture density PF_ generalizes regularized PFs, using bandwidths \(\) that are tuned jointly with discriminative models of the state dynamics and observation likelihoods.

## 3 Conditional State Estimation via Discriminative Particle Filters

While classical inference algorithms like KFs and PFs are effective in many domains, they require faithful generative models. A human expert must typically design most aspects of the state dynamics and observation likelihoods, and algorithms that use PFs to learn generative models often struggle to scale beyond low-dimensional, parametric models . For complex systems with high-dimensional observations like images, learning generative models of the observation likelihoods is often impractical, and arguably more challenging than estimating latent states given an observed data sequence. We instead learn _discriminative_ models of the distribution of the state _conditioned_ on observations (see Fig. 1), and replace manually engineered generative models with deep neural networks learned from training sequences with (potentially sparse) latent state observations.

**Discriminative Kalman Filters.** Discriminative modeling has been used to learn conditional variants of the KF [23; 24] where the posterior is parameterized by a Gaussian state space model, and state transition and observation emission models are produced from data by trained neural networks. Like _conditional random field_ (CRF)  models for discrete data, discriminative KFs do not learn likelihoods of observations, but instead condition on them. Discriminative KFs desirably learn a posterior covariance, and thus do not simply output a state prediction like conventional recurrent neural networks. But, they are limited to unimodal, Gaussian approximations of state uncertainty.

**Discriminative Particle Filters.** It is attractive to integrate similar discriminative learning principles with PFs, but the technical challenges are substantially greater due to the nonparametric particle representation of posterior uncertainty, and the need for particle resampling to maintain diversity.

Like in generative PFs, discriminative PFs update the particle locations using a _dynamics model_ as in Eq. (1). Unlike classic PFs, the particle weights are not updated using a generative likelihood function. Instead, discriminative PFs compute new weights \(w_{t}^{(:)}\) using a _measurement model_ function \((x_{t};y_{t})\) trained to properly account for uncertainty in the discriminative particle posterior:

\[w_{t}^{(i)}(x_{t}^{(i)};y_{t})w_{t-1}^{(i)}.\] (5)

Here \((x_{t};y_{t})\) is a (differentiable) function optimized to improve the accuracy of the discriminative particle posterior, rather than a generative likelihood.

Creating a learnable discriminative PF requires parameterizing the dynamics and measurement models as differentiable functions, such as deep neural networks. This is straightforward for the measurement model \((x_{t}^{(i)};y_{t})\), which may be defined via any feed-forward neural network architecture like those typically used for classification (see Fig. 2). For the dynamics model, the neural network does not simply need to score particles; it must be used for stochastic simulation. Using reparameterization [14; 15; 16], dynamics simulation is decomposed as sampling from a standard Gaussian distribution, and thenransforming that sample using a learned neural network, possibly conditioned on action \(a_{t}\):

\[x_{t}^{(i)}=f(_{t}^{(i)};x_{t-1}^{(i)},a_{t}),_{t}^{(i)} N (0,I).\] (6)

The dynamics model \(f(;x_{t-1},a_{t})\) is a feed-forward neural network that deterministically processes noise \(\), conditioned on \(x_{t-1}\) and \(a_{t}\), to implement the dynamical sampling of Eq. (1). The neural network \(f\) may be flexibly parameterized because discriminative particle filters only require simulation of the dynamical model, not explicit evaluation of the implied conditional state density.

### Prior Work on Discriminative Particle Filters

Specialized, heuristic variants of discriminative PFs have been previously used for tasks like robot localization  and multi-object tracking . Principled end-to-end learning of a discriminative PF requires propagating gradients through the PF algorithm, including the discrete resampling step. Zhu et al.  explore replacing the PF resampling step with a learned (deterministic) particle transform, but find that exploding gradient magnitudes prohibit end-to-end training.

**Truncated-Gradient Particle Filter (TG-PF)**. The first so-called "differentiable" particle filter  actually treated the particle resampling step as a non-differentiable discrete resampling operation, and simply truncated all gradients to zero when backpropagating through this resampling. Though simple, this approach leads to biased gradients, and often produces ineffective models because _backpropagation through time_ (BPTT ) is not possible. Perhaps due to these limitations, experiments in Jonschkowski et al.  assumed a simplified training scenario where the ground-truth dynamics are known, and only the measurement model must be learned.

**Discrete Importance Sampling Particle Filter (DIS-PF)**. Scibior et al.  propose a gradient estimator for generative PFs that may also be adapted to discriminative PFs. They develop their estimator by invoking prior work on score-function gradient estimators , but we provide a simpler derivation via importance sampling principles. Instead of discretely resampling particles as in Eq. (3), a separate set of weights \(v_{t}^{(:)}\) is defined and used to generate samples:

\[_{t}^{(i)}=x_{t}^{(j)}, j(v_{t}^{(1)},,v_ {t}^{(N)}).\] (7)

To account for discrepancies between these resampling weights \(v^{(:)}\) and true weights \(w^{(:)}\), the resampled particle weights \(_{t}^{(i)}\) are defined via importance sampling. The choice of \(v_{t}^{(:)}\) critically impacts the effectiveness of the resampling step. To maintain the standard PF update of Eq. (3), we set \(v_{t}^{(i)}=w_{t}^{(i)}\), yielding the following resampled particle weights and associated gradients:

\[_{t}^{(i)}=^{(i)}}{v_{t}^{(i)}|_{v_{t}^{(i)}=w_{t}^{(i)}}}= 1,_{}_{t}^{(i)}=w_{t}^{(i)}}{ v_{t}^{(i)}|_{v_{t}^{(i)}=w_{t}^{(i)}}}.\] (8)

Intuitively, particle locations are resampled according to the current weights in the forward pass of DIS-PF. Then when computing gradients, perturbations of the observation and dynamics models are accounted for by changes in the associated particle weights.

A drawback of this discrete importance sampling is known as the _ancestor problem_: since the resampled particle set is constructed from a subset of the pre-resampled particles, no direct gradients exist for particles that were not chosen during resampling; they are only indirectly influenced by the weight normalization step. This increases the DIS gradient estimator variance, slowing training.

**Soft Resampling Particle Filter (SR-PF)**. Karkus et al.  propose a _soft-resampling_ (SR) mixing of the particle weights with a discrete uniform distribution before resampling: \(v_{t}^{(i)}=(1-)w_{t}^{(i)}+/N\)

Figure 2: _Left: Our MDPF method showing the various sub-components. Middle: Our A-MDPF method with decoupled measurement models and bandwidths. Right: Dynamics and measurement model structures used in MDPF and A-MDPF. The dynamics and measurement models are composed of several neural networks as well as some fixed transforms, which convert the angular state dimensions of particles into a vector representation._

Viewing particle locations as fixed, SR-PF evaluates the resampled particle weights and gradients as

\[_{t}^{(i)}=^{(i)}}{(1-)w_{t}^{(i)}+/N}, _{}_{t}^{(i)}=_{}^{(i)}}{(1- )w_{t}^{(i)}+/N}.\] (9)

While the SR-PF weight update is differentiable, it does not avoid the ancestor problem. By resampling low-weight particles more frequently, SR-PF will degrade overall PF performance when \(N\) is not large. More subtly, the gradient of Eq. (9) has (potentially substantial) bias: it assumes that perturbations of model parameters influence particle weights, but _not_ the outcome of discrete resampling (7). Indeed in some experiments in , smoothing is actually disabled by setting \(=0\).

**Concrete Particle Filter (C-PF).** The Gumbel-softmax or Concrete distribution [31; 32] approximates discrete sampling by interpolation with a continuous distribution, enabling reparameterized gradient estimation. Each resampled particle is a convex combination of the weighted particle set:

\[_{t}^{(i)}=_{j=1}^{N}_{ij}x_{t}^{(j)},_{ij}= {(((w_{t}^{(j)})+G_{ij})/)}{_{k=1}^{N}(((w_{t}^{(k) })+G_{ik})/)}, G_{ij}.\] (10)

Gradients of (10) are biased with respect to discrete resampling due to the non-learned "temperature" hyperparameter \(>0\). When \(\) is large, the highly-biased relaxation will interpolate between modes, and produce many low-probability particles. Bias decreases as \( 0\), but in this limit the variance is huge. Even with careful tuning of \(\), Concrete relaxations are most effective for small \(N\). Madison et al.  focus on models with binary latent variables, and find that performance degrades even for \(N=8\), far smaller than the \(N\) needed for practical PFs. While we provide C-PF as a baseline, we are unaware of prior work successfully incorporating Concrete relaxations in PFs.

**Optimal Transport Particle Filter (OT-PF).** OT-PF  modifies PFs by replacing stochastic resampling with the optimization-based solution of an entropy-regularized _optimal transport_ (OT) problem. OT seeks a probabilistic correspondence between particles \(x_{t}^{(:)}\) with weights \(w_{t}^{(:)}\) as in (5), and a corresponding set of uniformly weighted particles, by minimizing a Wasserstein metric \(^{2}_{2,}\):

\[_{^{N N}}_{i,j=1}^{N}_{ij} \|x_{t}^{(i)}-x_{t}^{(j)}\|^{2}+_{ij}} {N^{-1}w_{t}^{(j)}},_{j=1}^{N}_{ij}=,_{i=1}^{N} _{ij}=w_{t}^{(j)}.\] (11)

Entropy regularization is required for differentiability, and OT-PF accuracy is sensitive to the non-learned hyperparameter \(>0\). OT-PF uses this entropy-regularized mapping to interpolate between particles: \(_{t}^{(i)}=_{j=1}^{N}N_{ij}x_{t}^{(j)}\). This assignment approximates the results of true stochastic resampling in the limit as \(N\), but lacks accuracy guarantees for moderate \(N\).

OT-PF solves a _regularized_ OT problem which relaxes discrete resampling, producing biased gradients that are reminiscent (but distinct) from C-PF. Minimization of Eq. (11) via the Sinkhorn algorithm  requires \((N^{2})\) operations. This OT problem must be solved at each step of both training and test, and in practice, OT-PF is substantially slower than all competing discriminative PFs (see Fig. 10).

## 4 Stable Gradient Estimation for Mixture Model Resampling

Training our discriminative _mixture density particle filter_ (MDPF) requires unbiased and low-variance estimates of gradients of samples from continuous mixtures. Mixture sampling is classically decomposed into discrete (selecting a mixture component) and continuous (sampling from that component) steps. The Gumbel-softmax or Concrete distribution [31; 32] provides a reparameterizable relaxation of discrete resampling, but may have substantial bias. Some work has also applied OT methods for gradient estimation in restricted families of Gaussian mixtures [35; 36]. We instead develop importance-sampling estimators that are unbiased, computationally efficient, and low variance.

**Implicit Reparameterization Gradients.** Direct reparameterization of samples, as used for the latent Gaussian distributions in variational autoencoders [14; 15; 16], cannot be easily applied to mixture models. Reparameterized sampling requires an invertible standardization function \(S_{}(z)=\) that transforms a sample \(z\), drawn from a distribution parameterized by \(\), to an auxiliary variable \(\) independent of \(\). For mixture distributions, the inverse of \(S_{}(z)\) cannot be easily computed.

The _implicit reparameterization gradients_ (IRG) estimator [37; 38] avoids explicit inversion of the standardization function \(S_{}(z)\). Using implicit differentiation, reparameterization can be achieved so long as a computable (analytically or numerically) and invertible \(S_{}(z)\) exists, without the need to explicitly compute its inverse. IRG gradients are expressed via the Jacobian of \(S_{}(z)\):

\[_{}z=-(_{z}S_{}(z))^{-1}_{}S_{}(z).\] (12)

For univariate distributions, the cumulative distribution function (CDF) \(M_{}(z)\) is a valid standardization function. For higher dimensions, the multivariate distributional transform  is used:

\[S_{}(z)=M_{}(z_{1}),M_{}(z_{2} z_{1}),,M_{}(z_{ D} z_{1},,z_{D-1}).\] (13)

While IRG may be applied to any distribution with continuous CDF, and has been explicitly suggested (without experimental validation) for mixtures , we show that it may have enormous variance.

Importance Weighted Sample Gradient Estimator.We propose a novel alternative method for computing gradients of samples from a continuous mixture distribution. Our _importance weighted sample gradient_ (IWSG) estimator employs importance sampling for unbiased gradient estimation. IWSG is related to the DIS-PF gradient estimator for discrete resampling , but we instead consider continuous mixture distributions with arbitrary component distributions. This resolves the ancestry problem present in Scibior et al. , since particle locations could be sampled from multiple overlapping mixture components, whose parameters will all have non-zero gradients.

Formally, we wish to draw samples \(z^{(i)} m(z)\) from a mixture with parameters \(\). Instead of sampling from \(m(z)\) directly, IWSG samples from some proposal distribution \(z^{(i)} q(z)\). Importance weights \(w^{(i)}\), and associated gradients, for these samples then equal

\[w^{(i)}=)}{q(z^{(i)})},_{}w^{(i) }=m(z^{(i)})}{q(z^{(i)})}.\] (14)

Setting the proposal distribution \(q(z)=m(z_{0})\) to be a mixture model with the "current" parameters \(_{0}\) at which gradients are being evaluated, the IWSG gradient estimator (14) becomes

\[w^{(i)}=)_{=_{0}}}{m(z^{(i)}_{0} )}=1,_{}w^{(i)}=m(z^{(i)}) _{=_{0}}}{m(z^{(i)}_{0})}.\] (15)

While current samples are given weight one because they are exactly sampled from a mixture with the current parameters \(_{0}\), gradients account for how importance weights will change as mixture parameters \(\) deviate from \(_{0}\). Note that gradients are _not_ taken with respect to the proposal \(q(z)=m(z_{0})\) in the denominator of \(w^{(i)}\), since sample locations \(z^{(i)}\) are not altered by gradient updates; changes in the associated importance weights are sufficient for unbiased gradient estimation.

Instability of Implicit Reparameterization Gradients.Reparameterization methods capture changes to a given distribution \(p(z|)\), and thus to its parameters \(\), by shifting the samples drawn from that distribution. When \(p(z|)\) is unimodal, this induces a smooth shift in the samples. When \(p(z|)\) contains multiple non-overlapping modes, these shifts are no longer smooth as samples

Figure 3: _Left:_ Example changes in samples from a 1D mixture model with two Epanechnikov  components. Under IRG, changes in the mixture are shown by dramatic sample shifting, where some samples must change modes to account for the changes in the relative weights of each mixture mode. Explicitly plotting the particle transformation induced by IRG reveals a discontinuity (bottom). In contrast, IWSG smoothly reweights samples. _Right:_ Example changes in samples from a 2D mixture of two Gaussians. IRG again induces large shifts as particles change modes, as demonstrated by the vector field (bottom).

jump from one mode to another; see Fig. 3. For mixtures with finitely supported kernels (like the Epanechnikov) and non-overlapping modes, discontinuities exist in \(S_{}^{-1}()\), and IRG estimates may have infinite variance. These discontinuities correspond to samples shifting from one mode to another, and the resulting non-smooth sample perturbations will destabilize backpropagation algorithms for gradient estimation. While the inverse CDF is always continuous for Gaussian mixtures, it may still have near-infinite slope when modes are widely separated, and induce similar IRG instabilities.

In contrast, our IWSG estimator reweights particles instead of shifting them. This reweighting is smooth and does not suffer from any discontinuities. To illustrate this, we create a simple discriminative PF with linear-Gaussian dynamics (16) and a bimodal observation likelihood (17):

\[p(x_{t} x_{t-1},a_{t}) =(x_{t} Ax_{t-1}+Ba_{t},^{2}),\] (16) \[p(y_{t} x_{t}) =w_{1}(y_{t} C_{1}x_{t}+c_{1},^{2})+w_{2} (y_{t} C_{2}x_{t}+c_{2},^{2}).\] (17)

Exact posterior inference for this model is possible via a mixture KF [11; 40] which represents the state posterior as a Gaussian mixture. The number of mixture components grows exponentially with time, motivating PFs for long-term tracking, but remains tractable over a few time steps.

Using a fixed dataset and gradient descent, we train discriminative PFs using the IRG estimator, our IWSG estimator, as well as with biased gradients that are truncated at each resampling step . We initialize with perturbed parameter values and aim to learn the values of all dynamics and likelihood parameters, except for \(\) and \(\) which are fixed to their true values. See Appendix for details.

Fig. 4 shows the estimates and gradients for a subset of the learned parameters, and several gradient estimators. IRG's unstable gradients prevent convergence of all parameters. For training (generative) marginal PFs  which approximate marginals with mixtures, Lai et al.  found that IRG gradient variance was so large that biased estimators converged faster, but provided no detailed analysis. In contrast, IWSG converges smoothly for all parameters, and more rapidly than truncated gradients.

## 5 Mixture Density Particle Filters

Our _mixture density particle filter_ (MDPF) is a discriminative model inspired by regularized PFs, where resampling uses KDEs centered on the current particles. We apply IWSG to this mixture resampling, achieving unbiased and low-variance gradient estimates for our fully-differentiable PF.

MDPF does not incorporate human-specified dynamics or measurement models; some prior work assumed known dynamics to simplify learning [5; 7]. These models are parameterized as deep neural networks (NNs), and trained via stochastic gradient descent to minimize the negative-log-likelihood of (potentially sparsely labeled) states \(x_{t}\), given observations \(y_{t}\) and (optionally) actions \(a_{t}\). As shown in Fig. 2, dynamics and measurement models are flexible composed from smaller NNs. Our MDPF places no constraints on the functional form of NNs, allowing for modern architectures such as CNNs  and Spatial Transformers  to be used. See Appendix for implementation details.

The MDPF uses KDE mixture distributions as in Eq. (4) to define state posteriors used to evaluate the training loss, as well as resampling. A bandwidth parameter \(\) is thus required for each state dimension. Rather than setting \(\) via the classic heuristics discussed in Sec. 2.2, we make \(\) a learnable parameter, optimizing it using end-to-end learning along with the dynamics and measurement models. This contrasts with prior work including SR-PF  and C-PF and OT-PF , which all include relaxation hyperparameters that must be tuned via multiple (potentially expensive) training trials.

We also extend the MDPF by decoupling the mixtures used for particle resampling and posterior state estimation. Using two separate measurement models, we compute two sets of particle weights

Figure 4: A simple temporal prediction problem where IRG has highly unstable gradient estimates. We use \(N=25\) particles when training IRG-PF, IWSG-PF, and truncated gradients (TG-PF). We show the learned values for parameters \(B\), \(w_{2}\), \(C_{1}\) and their gradients during training. IRG produces unstable gradients which prevent parameters from converging at all, but IWSG allows for smooth convergence of all parameters. IWSG is faster than biased TG, and nearly as effective as (expensive, and for general models intractable) mixture KF.

[MISSING_PAGE_FAIL:8]

Figure 8: Example posteriors for three time steps (rows) of House3D tracking, for several discriminative PFs given the same low-noise initialization. We show the current true state and state history (red arrow and dashes), the posterior distribution of the current state (blue cloud, with darker being higher probability), and the estimated mean state and history (yellow arrow and dashes). Several baselines completely fail to track the robot.

Figure 6: Example trajectories from the Bearings-Only tracking task. The radar station (green circle) produces bearing observations (green line) from the radar station to the car (red arrow). Note that observations are noisy and occasionally totally incorrect (see \(t=18\)). We plot the posterior distribution of the current state (blue cloud), the mean particle (yellow arrow), and the \(N=25\) particles (black dots). A-MDPF is able to capture multiple modes in the state posterior (see \(t=15,17,18,19\)) while successfully tracking the true state of the car. Other models (DIS-PF, OT-PF, C-PF) spread their posterior distribution due to poor tracking of the true state.

Figure 7: Example trajectories (rows), with observations in corner, from the House3D task using A-MDPF. _Top:_ Initialized with \(N=50\) particles set to the true state with moderate noise, the posterior (blue cloud) closely tracks the robot. _Middle:_ Multi-modal initialization with \(N=150\) particles (blue dots). _Bottom:_ Naive initialization with \(N=1000\) particles drawn uniformly. A-MDPF maintains multiple modes until enough observations have been seen to disambiguate the true state.

Each method is evaluated and trained 11 times using \(N=25\) particles. To further highlight the stability of our IWSG estimator, we also train each method (except for IRG-MDPF) with the Epanechnikov kernel replacing the Gaussian in the KDE. Fig. 5 shows statistics of performance across 11 training runs. The very poor performance of IRG-MDPF is due to unstable gradients; without aggressive gradient clipping, IRG-MDPF fails to optimize at all. The benefits of IWSG are highlighted by the inferior performance of TG-MDPF. Stable multi-time-step gradients clearly benefit learning and are enabled by IWSG. Qualitative results are shown in Fig. 6.

**Deepmind-Maze Tracking.** In the Deepmind-Maze tracking task, adapted from , we wish to track the 3D state of a robot as it moves through modified versions of the maze environments from Deepmind Lab  using noisy odometry as actions, and images (from the robot's perspective) as observations. We alter the experimental setup of  by increasing the noise of the actions by five times, and using \(N=25\) particles for training and evaluation. We train and evaluate on trajectories from the 3 unique mazes separately, using 5000 trajectories of length \(T=17\) from each maze for training, and 1000 trajectories of length \(T=99\) for evaluation. We show results over multiple training and evaluation runs in Fig. 5, and include qualitative results in the Appendix.

**House3D Tracking.** This 3D state tracking task is adapted from , where a robot navigates single-level apartment environments from the SUNCG dataset  using the House3D simulator , with noisy odometry as action and RGB images as observations (see Fig. 7). Floor plans are input into measurement model via a modern Spatial Transformation Network architecture . Training data consists of 74800 trajectories (length \(T=24\)) from 199 unique environments, with evaluation data being 820 trajectories (length \(T=100\)) from 47 previously unseen floorplans. We train and evaluate with \(N=50\) particles.

Due to the larger computational demands of House3D, we train each method once, reporting results in Table 1. Interestingly we find that TG-PF, OT-PF, SR-PF, and DIS-PF are unable to learn useful dynamics or measurement models. The LSTM model achieves better performance, but looking deeper we discover that it ignores observations completely, and simply blindly propagates the estimated state using actions with good dynamics (i.e., dead-reckoning). This strategy is effective for noise-free actions, but in reality the actions are noisy and thus the LSTMs estimated state diverges from the true state after a moderate number of time-steps. In contrast, our MDPF methods are able to learn good dynamics and measurement models that generalizes well to the unseen environments.

We also investigate MDPs capacity for multimodal tracking in Fig. 7, by initializing A-MDPF with particles spread throughout the state space. When initialized with random particles or with particles clustered around regions with similar observations, A-MDPF does not have sufficient information to collapse the posterior and thus maintains multiple posterior modes. As the robot moves, more evidence is collected, allowing A-MDPF to collapse the state posterior to fewer distinct modes, before eventually collapsing its estimate into one mode containing the true state. Note that such estimation of multiple posterior modes is impossible for standard RNNs like the LSTM.

**Limitations.** A known limitation of all PFs is there inability to scale to very high-dimensional states. Sparsity increases as the dimension grows, and thus more particles (and consequently computation) are required to maintain the expressiveness and accuracy of the state posterior. Our MDPF is not immune to this issue, but we conjecture that end-to-end training of discriminative PFs will allow particles to be more effectively allocated within large state spaces, scaling better than classic PFs.

## 7 Discussion

We have developed a novel importance-sampling estimator for gradients of samples drawn from a continuous mixture model. Our results highlight fundamental flaws in the application of (implicit) reparameterization gradients to any mixture model with multiple modes. Applying our gradient estimator to the resampling step of a regularized discriminative PF, we obtain a fully end-to-end differentiable MDPF that robustly learns accurate models across multiple training runs, and has much greater accuracy than the biased PFs proposed in prior work. While our experiments have focused on synthetic tracking problems in complex virtual environments, future applications of our MDPF to real-world vision and robotics data are very promising.

   Method & NLL & RMSE \\  LSTM & 12.54 \(\) 3.43 & 48.51 \(\) 27.16 \\ TG-PF & 15.50 \(\) 2.21 & 206.32 \(\) 109.44 \\ OT-PF & 14.87 \(\) 2.31 & 159.57 \(\) 109.65 \\ SR-PF & 14.92 \(\) 2.57 & 191.32 \(\) 129.28 \\ DIS-PF & 15.06 \(\) 2.02 & 212.58 \(\) 133.69 \\ TG-MDPF & 8.38 \(\) 2.53 & 35.77 \(\) 16.67 \\ MDPF & 8.18 \(\) 2.72 & 30.59 \(\) 12.98 \\ A-MDPF & **8.09 \(\) 3.49** & **30.51 \(\) 12.32** \\   

Table 1: Mean \(\) inter-quartile of evaluation metrics for all evaluation trajectories from the House3D task.