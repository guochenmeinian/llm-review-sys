# Exploring Question Decomposition for Zero-Shot VQA

Zaid Khan\({}^{1}\) Vijay Kumar BG\({}^{2}\) Samuel Schulter\({}^{2}\) Manmohan Chandraker\({}^{2,3}\) Yun Fu\({}^{2}\)

\({}^{1}\)Northeastern University \({}^{2}\)NEC Laboratories America \({}^{3}\)UC San Diego

{khan.za,yun.fu}@northeastern.edu

{vijay.kumar, samuel, manu}@nec-labs.com

###### Abstract

Visual question answering (VQA) has traditionally been treated as a single-step task where each question receives the same amount of effort, unlike natural human question-answering strategies. We explore a question decomposition strategy for VQA to overcome this limitation. We probe the ability of recently developed large vision-language models to use human-written decompositions and produce their own decompositions of visual questions, finding they are capable of learning both tasks from demonstrations alone. However, we show that naive application of model-written decompositions can hurt performance. We introduce a model-driven _selective decomposition_ approach for second-guessing predictions and correcting errors, and validate its effectiveness on eight VQA tasks across three domains, showing consistent improvements in accuracy, including improvements of \(>20\%\) on medical VQA datasets and boosting the zero-shot performance of BLIP-2 above chance on a VQA reformulation of the challenging Winoground task. Project Site: https://zaidkhan.me/decomposition-0shot-vqa/

## 1 Introduction

On a question-answering test, humans are able to answer some questions in a single step, while other questions require potential deliberation and second-guessing. Visual question answering (VQA)  has traditionally been treated as a single-step task. Models only get one chance for each question, and each question receives equal amounts of computation. This is incongruent to the natural human approach to such tasks, where simple perceptual questions are quickly answered, while harder reasoning questions are allocated more time and computation.

The emergence of task decomposition techniques for large language models (LLMs)  is a potential solution to this incongruency. Task decomposition techniques _prompt_ a LLM to break down an initial complex task into simpler subtasks that can each be solved independently. However, VQA has not benefited from advances in task decomposition techniques for two reasons. First, many task decomposition techniques  have only been effective in the regime of very large unimodal LLMs with parameters in the 30B+ range, while the LLMs underlying vision-language models are typically much smaller, only recently reaching \( 13\)b parameters for publicly available models. Second, existing methods for prompting vision-language models (VLMs) during VQA tasks focus on other use cases, such as providing more examples of the input task  or more information about the image . Given the recent emergence of multi-billion scale VLMs, our main research question is:

_Can multi-billion scale vision-language models benefit by approaching reasoning-_

_heavy VQA as a two-step rather than a single-step problem using decomposition?_

To this end, we explore a form of task decomposition called _question decomposition_ as a strategy for zero-shot visual question answering with large VLMs. Although question decomposition has been explored for specific unimodal QA, it has not been explored as a strategy for multimodaltasks such as VQA with emerging large VLMs [7; 8; 9; 15; 16], and little is known about the in-context learning ability of emerging large VLMs.

First, we probe the in-context learning ability [17; 18; 19] of both LMs and VLMs to exploit oracular question decompositions written by humans. We design experiments to understand whether models can learn to use decompositions without explicit training, and whether they are merely exploiting keywords and surface statistics when they use decompositions. Second, we conduct a series of experiments, again using in-context learning, to understand how well models can _produce_ decompositions that correct the errors of a fixed VQA model. Last, we propose and study an entirely model-driven closed-loop approach mimicking a simplified form of a classic human second-guessing strategy: second-guess answers based on how confident you are about them. We conduct experiments across three domains (art, natural images, medical), eight datasets, three model families, and model sizes ranging from 80M to 11B parameters. Our contributions can be listed as follows:

1. We experimentally demonstrate that large VLMs based on instruction-tuned LLMs can use decompositions to improve their predictions without any training, and are not merely exploiting changes in word statistics introduced by the decomposition. (Sec. 3)
2. We quantitatively show that generative, instruction-tuned language models are capable of writing effective decompositions zero-shot, without task-specific training. (Sec. 4)
3. We find that applying decomposition naively to every question instance harms performance rather than helps (Fig. 4), and propose _selective decomposition_ (Fig. 3), a modular, model-agnostic, training-free strategy that treats VQA as a two-step task. (Sec. 5)
4. We apply selective decomposition to a testbed of 8 datasets and show that it consistently improves performance (Tabs. 3 and 4), with gains of \(>20\%\) on medical VQA datasets[20; 21; 22], and boosts the performance of BLIP-2 above chance on the Winoground benchmark when formulated as a VQA task. (Sec. 5).

## 2 Background

### Problem Setting

In zero-shot VQA, a model \(f:v,q a\) is given an image \(v\), a question \(q\), and outputs an answer, \(a\). Unlike traditional VQA, the model \(f()\) has never seen \(v,q,a\) triplets. In practice, such a setting often occurs when \(f()\) is a foundation model that contains several billion parameters and has undergone large scale pretraining. It is undesirable to retrain such an \(f()\) on visual question answering pairs specifically, both for reasons of computational convenience and because finetuning can degrade robustness. The most common case is that \(f()\) is an autoregressive, generative language model that can optionally be conditioned on the visual modality. We restrict ourselves to such models, which approximate \(_{k=1}^{N}p(t_{k+1}|t_{1:k},v)\), where \(v\) is an image and \(t_{1:k}\) is a sequence of language tokens. In a zero-shot VQA setting, it is expected that \(f()\) understands that it has been given a question \(q\) and should produce the correct answer \(a\) to the question \(q\) in the context of the image \(v\) by modeling it as \(p(a|v,q)\). This setting is common when evaluating very large frozen models, such as in [10; 11], with the exception that in our case, \(f()\) is a vision-language model rather than a language-only model.

### Question Decomposition

Question decomposition is the task of decomposing a complex main question into one or more simpler subquestions that are logically related to the main question, answering those simpler subquestion(s), and then using the answered subquestion(s) to help in composing a final answer to the complex main question. This is a strategy often used by humans for problem solving. For example, consider a human being confronted by a wild animal they have never seen before. To answer the main question _"does this animal pose a threat to me?"_ a human might decompose it into subquestions such as _"does the animal have sharp canine teeth?"_ and _"does the animal have forward facing eyes typical of a predator?"_ Knowing the answer to even one of these subquestions makes answering the main question much easier.

Adopting the terminology of Sec. 2.1, the task of question decomposition consists of _decomposing_ a main visual question \(v\), \(q\) into one or more subquestions \((s_{1},s_{2},)\), answering those subquestions to obtain the decomposition (\(((q^{}_{1},a^{}_{1}),(q^{}_{2},a^{}_{2}))\), and then using \(v\), \(q\) together with the decomposition (\((q^{}_{1},a^{}_{1}),(q^{}_{2},a^{}_{2})\)) to obtain the final answer \(a\).

### What makes a good subquestion?

In Sec. 2.2, we gave a definition of decompositions that is dependent on notions of "simpler" and "logically related". It is challenging to make these notions precise, and difficult to operationalize them to measure whether a sequence of text really is a valid subquestion according to these notions. To sidestep these difficulties, we adopt a consequentialist view of whether a subquestion is "good", following a common consequentialist tradition in artificial intelligence as a whole . We evaluate the "goodness" of a subquestion by measuring the effect of the subquestion. Concretely, let \(v,q,a\) be a visual question triplet where \(v\) is the image, \(q\) is the question, and \(a\) is the answer. Let \(p_{f}(a|v,q)\) be the probability of the ground-truth answer \(a\) as assessed by a visual question answering system \(f()\). We regard a decomposition of \(v,q\) consisting of series of subquestions and their answers (\((q^{}_{1},a^{}_{1}),(q^{}_{2},a^{}_{2})\)) as "good" if \(p_{f}(a|v,q)<p_{f}(a|v,q,((q^{}_{1},a^{}_{1}),(q^{}_{2},a^{ }_{2}))\)), that is, if seeing the decomposition increases the probability of the ground-truth answer \(a\). In practice, we adopt a simpler criterion that takes the consequentialist definition to the limit. _We regard a decomposition as "good" if seeing the decomposition induces the model to produce the true ground-truth answer \(a\)._

### Scope & Limitations

We only consider in-context learning techniques for zero-shot VQA, and do not explore full model training in this work. The class of model we are interested in are instruction-following vision-language models based on large language models [7; 8; 9]. This excludes previous-generation vision-language models that are not based on multi-billion parameter instruction-tuned language models [26; 27; 28; 29; 30].

Figure 1: Model-produced decompositions and their error correcting effects. The decompositions and before/after answers shown above were produced by prompting BLIP-2 models based on FLAN-T5 to produce a subquestion, answering the subquestion with the model and feeding the question and answered subquestion back to the model: it is correcting itself. Before answers are wrong and After answers are correct.

Not all datasets are suitable for exploring question decomposition, as some primarily test low-level perception skills rather than high-level reasoning skills that would benefit from a decomposition. We thus limit our evaluation to datasets that explicitly test for high-level reasoning / knowledge-based ability. We are few-shot for the task of _visual question decomposition_ but zero-shot for the task of _visual question answering_.

## 3 How well can models use decompositions?

Our goal in this section is to understand the ability of vision-language models based on large language models to _consume_ decompositions. The hypothesis we test is: _When provided with gold-standard decompositions on a VQA task, a model's error rate should be lower than without the gold-standard decompositions._ Evaluating this hypothesis presents a number of challenges. First, how can we obtain a set of decompositions that are apriori "known to be good"? Second, how should the model be fed the decompositions?

To find a source of apriori "good" decompositions, we turn to the literature on internal consistency in visual question answering. To probe consistency in question answering systems, several datasets [31; 32; 33] have been proposed. A particularly relevant case of such a dataset is VQA-Introspect , which probes consistency along a reasoning-perception axis. Selvaraju et al.  annotate each question in the VQAv2 validation set as a high-level "reasoning" question or a low-level "perception" question. For each "reasoning" question, Selvaraju et al.  write 1-3 "perceptual" subquestions which are implied by the reasoning question. For example, given a high-level reasoning question such as "Can I eat this banana?" a model that says "yes" should also reply "yellow" to the low-level perception question "what is the color of the banana?" We propose to use the low-level perception questions and answers written for the high-level reasoning questions as an _oracular_ decomposition for the high-level reasoning question, on the basis that the low-level perception questions are simpler than the high-level reasoning question, entail the answer for the high-level reasoning question, and are written by humans.

    &  &  \\ Decomposition & Overall & Boolean & Number & Other & Overall & Boolean & Number & Other \\  None (Baseline) & 79 & 82.5 & 6.8 & 67.4 & 79.1 & 81.9 & 13.7 & 70.1 \\
**Oracle/Oracle** & **88.6** & 91.4 & **40.2** & **79.4** & **89.8** & **92.6** & **45.3** & **80.4** \\  \(\) w.r.t Baseline & 9.6 & 8.8 & 33.3 & 12.1 & 10.8 & 10.7 & 31.6 & 10.3 \\
**Oracle/Self-Answer** & 84 & 87.3 & 21.4 & 72.8 & 83.9 & 87.1 & 26.5 & 73.2 \\ \(\) w.r.t Baseline & 5 & 4.8 & 14.5 & 5.4 & 4.8 & 5.2 & 12.8 & 3.1 \\
**Oracle/No Answer** & 83.3 & 85.9 & 27.4 & 74.9 & 84.1 & 86.9 & 27.4 & 75.2 \\ \(\) w.r.t Baseline & 4.4 & 3.4 & 20.5 & 7.6 & 5.1 & 5 & 13.7 & 5.1 \\
**Oracle/Oracle (Scrambled)** & 84.9 & 87.9 & 37.6 & 74.8 & 86 & 88.9 & 39.3 & 76.2 \\ \(\) w.r.t Baseline & 5.9 & 5.4 & 30.8 & 7.4 & 6.9 & 7 & 25.6 & 6 \\   &  \\ Decomposition & Overall & Boolean & Number & Other & Overall & Boolean & Number & Other \\  None (Baseline) & 57.4 & 64.4 & 6 & 32.2 & 63.8 & 71.9 & 6.8 & 34.3 \\
**Oracle/Oracle** & **72** & **75.8** & **37.6** & **58.4** & **81.5** & **85.1** & **45.3** & **69** \\  \(\) w.r.t Baseline & 14.5 & 11.4 & 31.6 & 26.2 & 17.8 & 13.2 & 38.5 & 34.7 \\
**Oracle/Self-Answer** & 62.1 & 65.8 & 23.1 & 48.8 & 68 & 72.1 & 20.5 & 53.7 \\ \(\) w.r.t Baseline & 4.6 & 1.4 & 17.1 & 16.7 & 4.3 & 0.2 & 13.7 & 19.4 \\
**Oracle/No Answer** & 64.8 & 68.7 & 21.4 & 50.9 & 75.2 & 79 & 26.5 & 62.2 \\ \(\) w.r.t Baseline & 7.3 & 4.3 & 15.4 & 18.8 & 11.4 & 7.1 & 19.7 & 27.9 \\
**Oracle/Oracle (Scrambled)** & 60.5 & 62.6 & 28.2 & 53.3 & 78.9 & 83.1 & 40.2 & 63.5 \\ \(\) w.r.t Baseline & 3.1 & -1.8 & 22.2 & 21.1 & 15.1 & 11.3 & 33.3 & 29.2 \\   

Table 1: Models are capable of using decompositions written by humans to provide more accurate answers. The gray rows are the baseline performance with no decomposition, and each \(\) is calculated w.r.t to this baseline. Oracle/Oracle rows denoting oracle subquestions/oracle answers, have the highest \(\). “Self-Answer” means the model answered oracular subquestions itself, and “No Answer” indicates the answer was left out entirely. Image+Text indicates a vision-language model (BLIP-2) was tested with multimodal inputs, while Text indicates the corresponding language model inside BLIP-2 (FLAN-T5) was tested with text only inputs. Validation split of VQA Introspect is the dataset (22k reasoning questions with their associated decompositions).

The second challenge lies in using a decomposition consisting of a series of subquestions and answers (\(((q^{}_{1},a^{}_{1}),(q^{}_{2},a^{}_{2}))\) alongside a main visual question \((v,q)\). Recall that we cannot train the model \(f()\) being used for the visual question answering task, and for any arbitrary model, it is unknown whether the model has ever seen the _exact_ task of decomposition-aided visual question answering. Thus, we rely on the in-context learning ability [18; 19] of large language models to learn to perform the tasks we require from a demonstration of the task. We handcraft a simple prompt to contain a main visual question \(v,q\) from the VQAv2 validation set, along with one human-written oracular subquestion and human-written answer \(q^{}\), \(a^{}\) for the main question \(v,q\) extracted from VQA-Introspect. The prompt is simply

``` exemplar="Context:istheskyblue?no.aretherecloudsinthesky? yes.Question:whatweatherislikely?Shortanswer:rain" prompt=exemplar+"Context:{subquestion}?{subanswer}.Question:{question}?Shortanswer:" ```

**Experiments & Discussion** We use BLIP-2 models based on the instruction-tuned FLAN-T5 in 3B and 13B sizes. Experiments are run on a combination of A6000s and TPUv3s, on the VQA-Introspect validation set containing 22K reasoning questions and their associated decompositions. The results are shown in Tab. 1. Compared to the baseline with no oracular decompositions, both the 3B/13B vision-language models and their corresponding language models show a clear ability to benefit from decompositions across a variety of question types, with numerical questions benefiting the most. Next, we seek to gain insight into the mechanism by which decompositions aid inference.

_Is the model merely exploiting changes in surface level statistics?_ If so, we would expect that perturbations that leave the statistics largely unchanged but significantly alter the meaning and logical structure of the oracle decomposition should not result in significantly different performance from the unaltered oracle decompositions. We remove the answers from the decomposition so that it only contains the subquestions, and test the effect of only using the subquestions. Compared to the oracle, there is a significant 50% relative decrease in improvement w.r.t to the baseline. Most of the subquestion answers are boolean, so removing them should not significantly change content words in the prompt, though it changes the meaning of the context significantly. Next, we allow the models to answer the subquestions themselves (Oracle/Self-Answer) rather than using the ground-truth questions. The accuracy of all models again decreases relative to the oracle answers, suggesting the answer and question together contribute to the result. Finally, we take the oracle subquestion+answer and scramble the words before providing them to the models. If the model is merely exploiting surface level statistics, the performance difference between the scrambled oracular decompositions and the original decompositions should be minimal, as the words are all the same. Again, we observe a significant drop compared to the original decompositions, suggesting that the models _are not merely exploiting changes in the surface level statistics_. Furthermore, human-written decompositions help in almost all cases over the no-decomposition baseline. **Note:**_See supplement for complete experimental details for all experiments_.

## 4 Can models produce effective decompositions?

In this section, we conduct experiments to answer the following research questions:

1. Can language models \(\) 13B parameters learn to produce effective decompositions purely through demonstrations?
2. Is question decomposition mostly a linguistic ability, or is being able to see the image important?

Recall that a decomposition of a visual question \(v,q\) is a series of one or more _subquestions_ (\((q^{}_{1},a^{}_{1}),(q^{}_{2},a^{}_{2})\)) and their answers, with the constraint that the subquestions and answers should have the property that \(p_{f}(a|v,q)<p_{f}(a|v,q,((q^{}_{1},a^{}_{1}),(q^{}_{2},a^{ }_{2})))\) where \(p_{f}()\) represents probability assessed by a given vision-language model \(f()\) of the ground-truth answer \(a\). We simplify this task to the task of producing a _single_ subquestion \(q^{}\) given a main visual question \(v,q\), and denote the process of decomposition with an arbitrary autoregressive language model \(g()\) as \(d_{g}(v,q) q^{}\). We hereafter refer to the model \(g()\) that generates the decomposition as the _decomposer_. The subquestion is then answered by the vision-language model \(f(v,q^{})=a^{}\) to produce the subquestion-answer pair \((q^{},a^{})\). We call the question answering model the _recomposer_.

We then measure the effectiveness of the decomposition by measuring the _error correction rate_:

\[_{CR}=^{N}[f(v_{i},q_{i}) a_{i} f(v_{ i},q_{i},(q^{}_{i},a^{}_{i}))=a_{i}]}{_{i=1}^{N}[f(v_{i},q_{i})  a_{i}]}\] (1)

where \((v_{i},q_{i},a_{i})\) represent the \(i\)-th image, question, and ground-truth answer respectively, and \(q^{}_{i},a^{}_{i}\) represent a subquestion generated by the decomposer model and the answer predicted for the subquestion by the recomposer (VQA) model, and \([cond]\) is an indicator function that is equal to 1 when \(cond\) is true and 0 otherwise. Simply put, \(_{CR}\) measures the number of instances on which \(f()\) initially predicted a wrong answer, but switched to the correct answer after seeing the decomposition generated by \(g()\). Alternatively, this can be understood as the effectiveness of a decomposer model at correcting the errors of the recomposer model. The error induction rate \(_{IC}\) is the opposite:

\[_{IC}=^{N}[f(v_{i},q_{i})=a_{i} f(v_{ i},q_{i},(q^{}_{i},a^{}_{i})) a_{i}]}{_{i=1}^{N}[f(v_{ i},q_{i})=a_{i}]}\] (2)

and measures how often the produced decompositions flipped an answer that was initially correct to an incorrect answer. The decomposer can be the same as the recomposer if the model can do both tasks by following different prompts, as in the case of instruction-tuned models .

**Experiments & Discussion** We use BLIP-2  based on the FLAN-T5 as the question answering model (recomposer). For the decomposers, we use FLAN-T5 models ranging in size from 80M parameters to 11B parameters, as well as the BLIP-2 models themselves. We use four VQA

    & &  &  &  &  & \\  VQA Model & Decompeer & E\({}_{CR}\) & P & E\({}_{IC}\) & P & E\({}_{IC}\) & P & E\({}_{IC}\) & P & E\({}_{IC}\) & P & E\({}_{IC}\) & P & E\({}_{IC}\) & P & P & Prams \\   & Text & **12.5** & 28.12 & 50.31 & 7.1 & 42.06 & 83.15 & **9.76** & 31.38 & 63.56 & 14.12 & 35.41 & 66.73 & 80.0M \\  & Text & 10.42 & 53.08 & - & 96.59 & 9.81 & - & 9.45 & 52.47 & - & 12.15 & 49.29 & - & 250.0M \\  & Text & 9.2 & 30.76 & - & **12.22** & 41.12 & - & 8.64 & 29.58 & - & 15.25 & 36.83 & - & 780.0M \\  & Text & 7.99 & 15.11 & - & 6.06 & 21.03 & - & 7.98 & 15.01 & - & 16.38 & 37.68 & - & 3.0B \\  &  & 7.81 & **10.9** & - & **6.36** & **13.08** & - & **7.24** & **12.29** & - & **15.96** & **28.9** & - & 3.0B \\   & Text & 9.9 & 24.43 & - & 8.05 & 30.37 & - & 9.73 & 22.46 & - & **17.09** & 39.94 & - & 11.0B \\   & Text & 11.52 & 33.44 & 46.99 & 9.42 & 52.83 & 11.13 & 34.85 & 60.31 & 19.12 & 30.42 & 77.38 & 80.0M \\  & Text & 8.92 & 60.63 & - & 9.94 & 9.52 & - & 9.56 & 58.16 & - & 18.15 & 48.73 & - & 250.0M \\  & Text & 10.22 & 36.57 & - & **12.12** & 40.65 & - & 11.07 & 35.5 & - & 15.35 & 30.83 & - & 780.0M \\  & Text & 10.78 & **20.59** & - & **8.33** & **19.63** & - & 9.73 & **14.43** & - & 19.85 & 35.42 & - & 3.0B \\  &  & **14.13** & **26.36** & - & 10.61 & **21.03** & - & **13.54** & **25.06** & - & **20.71** & **30.42** & - & 11.0B \\   & Text & 12.45 & 30.64 & - & 8.05 & 28.97 & - & **12.42** & 27.11 & - & 18.51 & 32.5 & - & 11.0B \\   

Table 2: Models of drastically different sizes and multimodal capability can produce effective subquestions, as measured by \(_{CR}\) in Eq. (1), their ability to correct errors in a VQA Model. However, subquestions produced by larger models are less likely mislead the consuming VQA model, as measured by \(_{IC}\) in Eq. (2). “Text” indicates a language-only decomposer, while “Image+Text” indicates a vision-language decomposer. “Params” refers to the parameters of the decomposer. A pink highlight indicates when the decomposer and vqa model are the same (the model is talking to itself).

Figure 2: The procedure we use to generate a decomposition and use it as additional guidance during zero-shot VQA. The recomposer can be any question answering model, and the decomposer can be any generative language model, and some models can perform both roles, leading to self-talk. In experiments, we test if various decomposer candidates can learn to write effective subquestions purely from seeing a demonstration of the task.

datasets from three domains: ArtVQA (art), SLAKE (medical), and A-OKVQA and OKVQA (external knowledge VQA on natural images). We then carry out the procedure illustrated in Fig. 2 for each combination of decomposer, recomposer, and dataset. We handcraft three demonstrations of writing a subquestion for a question, in the form "_Reasoning Question: <question>? Perception Question: <subquestion>?_" For each \(v\), \(q\) pair in a dataset, we prompt the decomposer with the demonstration, followed by the question \(q\) as in Fig. 2, and measure E\({}_{CR}\) as in Eq. (1) and E\({}_{IC}\) as in Eq. (2) for each dataset. We show the results in Tab. 2. We find that _yes, language models \( 13\)_B _parameters can learn to produce effective decompositions just by viewing examples_. Decomposer size correlates positively with E\({}_{CR}\) (\(R^{2}=0.344\)), and negatively with E\({}_{IC}\) (\(R^{2}=.273\)) and the correlations are significant at \(=0.05\) across a larger collection of eight datasets used in Sec. 5. A human examination of the "subquestions" produced by smaller models shows that many of them are gibberish and not properly formed questions at all. Despite this, they surprisingly manage to maintain an \(E_{CR}\) that is sometimes higher than larger models. Finally, **the ability to decompose questions in the evaluated datasets _may_ be a primarily linguistic ability_** in that it is possible to ask effective subquestions about an image without being able to see the image, and the difference in effectiveness between the Image+Text BLIP-2 models and the text-only FLAN-T5 models of a similar size is on average \( 10\%\) of the base error rate (but this may not be true of other VQA datasets).

## 5 Selective Decomposition Works Better Than Naive Decomposition

One problem shows up in Sec. 4, which is that applying decompostions to _every_ question can hurt performance, by flipping answers that were initially correct to be incorrect. If we were able to decompose only wrong answers, we would always see a net gain in performance due to the error correction of decompositions. However, in a realistic setting, we do not know apriori that our answers are wrong, and thus run the risk of flipping an answer that was initially correct to be incorrect by applying a decomposition that is misleading. We call this the _second-guessing_ problem.

To deal with the second-guessing problem, we propose following an intuitive human strategy: stick with your initial answer on questions you are confident about, and only second-guess (apply a decomposition) for questions you are not confident about. Language models can be surprisingly well calibrated, meaning that the probability they assess to an output sequence they produce is often well-correlated with the probability that the produced output sequence is the "correct" one for a given task. We make use of this property to treat visual question answering as a selective prediction  task, using the language models's confidence as a decision score to determine whether we should apply a decomposition to a instance or stick with the original answer.

We describe the algorithm in pseudocode in Fig. 3. The _selective decomposition_ procedure transforms VQA from a single-step task to a two-step task given a decomposer model, a recomposer model, a confidence threshold \(\), and a visual question pair \(v\), \(q\). An initial answer \(\) and confidence \(p()\) is solicited from the recomposer model. If \(p()<\), the decomposition procedure is invoked, and a subquestion and answer pair \((q^{},a^{})\) are generated by the decomposer and recomposer working together. The recomposer model is then allowed to "second-guess" the inital answer \(\) with the decomposition \((q^{},a^{})\) as additional context. The decomposer and recomposer can be the same model or different models. We experiment with both scenarios. This introduces an extra hyperparameter \(\) into the inference procedure.

**Experiments & Discussion** In Fig. 4, we show the effect of different values of \(\) on the accuracy of selective decomposition with several decomposers. Across all datasets and all models, there is a wide range of \(\) (expressed as percentiles) for which selective decomposition improves predictive accuracy. At the same time, we clearly demonstrate the _second guessing_ problem in Fig. 4. Decomposing _every_ question often eventually leads to lower accuracy than decomposing no questions at all, because hallucinations and misleading decompositions can flip an initially correct answer to an incorrect answer.

Figure 3: Pseudocode for selective decomposition.

In Tabs. 3 and 4, we show the highest possible net gain achieved by selective question decomposition on three domains by different decomposers. Selective decomposition consistently improves predictive accuracy regardless of the decomposer and domain. _Net gains are larger on datasets (passes -t-test with \(=0.05\)) containing non-naturalistic images and specialized domains (e.g. medical) than they are on domains containing natural images._ The mean optimal surprisal \(I()\) for second-guessing answers is lower for non-natural domains (\(_{I()}=13.2\) for the natural image datasets vs \(_{I()}=9.0\) for the medical and art datasets, confirmed by t-test at \(=0.5\)). We further visualize this in Fig. 5. _This matches our expectations: you should second guess yourself on domains you understand poorly more than on domains you understand well._ A linear regression fit shows that larger decomposers correlate with larger net gains (\(R^{2}\)=0.365, \(R^{2}\)=0.342 for natural image domains and medical / art domains respectively, t-test with \(\)=0.05).

We reformulate Winoground as a VQA task by turning each caption into a question with a boolean yes / no answer (does "<caption>" describe the image?) on which chance accuracy is 50%. As visible in Tab. 3, all BLIP-2 models perform below random chance, in agreement with previous results on Winoground showing that it is extremely difficult for vision-language models. Surprisingly, after decompositions produced by the relatively FLAN-T5-small/base models (80M/200M parameters), the performance of BLIP-2 (13B) rises to significantly above chance (+18%). Upon inspection, many of the decompositions produced by the model appear to be gibberish, yet remarkably, induce the much larger 13B BLIP-2 model to correct over 30% of its initially wrong answers.

## 6 Literature Review

Task decomposition [5; 41; 42; 6] improves the performance of large language models on zero-shot reasoning tasks. The only work so far to apply similar techniques for VQA is MM-CoT , but it does not explore task decomposition with large vision-language models, choosing to finetune a smaller model instead. The ability to use zero-shot task decompositions may be a property of model scale, emerging at 60-200B parameters , or may be a property of large-scale pretraining on code . Such large vision-language models have only been developed recently due to advances in vision-language alignment. The prevailing paradigm in vision-language pretraining was to build vision-language models atop (relatively) small language models [26; 27; 28; 46; 29] below 1B parameters. Meanwhile, language models were being scaled from 3B-175B parameters [47; 48; 49; 50; 34], with each model family having at least one representative with > 10B parameters. Because vision-language

Figure 4: Selective decomposition mitigates the problem of misleading decompositions. We decompose questions based on model confidence in the initial answer, and show how accuracy initial rises past the baseline as the model mostly second guesses wrong answers, and then drops below the baseline with no decompositions (horizontal line) if too many questions initially answered correctly are second-guessed.

Figure 5: Decomposition is more effective on non-natural image domains, and models are also less confident in these domains. Size of circles is proportional to parameter count.

pretraining typically requires full model training, aligning these multi-billion parameter models to the visual modality was prohibitively computationally expensive. However, recent discoveries  motivated by earlier work with frozen models  have shown that the representation spaces of vision models and large-language models are surprisingly close, and rough alignment can be achieved with adapters  or linear mapping layers while keeping the language model frozen, and more advanced techniques have given rise to vision-LLMs . Our work is closely related to the visual question generation paradigm of . However, we direct our question generation to focus on decompositions rather than general questions.

## 7 Conclusion

We show that question decomposition is already a viable strategy that can be used to implement a more natural approach to VQA. Without any training, instruction-tuned VLMs can learn to produce and use decompositions from a few demonstrations of the task. This approach has many possible future directions. For example, we only consider two-step approaches for visual question answering, where we "hardcode" the depth of the decomposition. A natural next step would be to extend the two-step approach to a multi-step approach, which remains unexplored for large vision-language models in an open-world visual question answering setting. Second, in-context learning has limitations. Would models benefit from being trained to produce and consume decompositions?