# e-COP : Episodic Constrained Optimization of Policies

Akhil Agnihotri

University of Southern California

agnihotri.akhil@gmail.com &Rahul Jain

Google DeepMind and USC

rahulajain@google.com &Deepak Ramachandran

Google DeepMind

ramachandran@google.com &Sahil Singla

Google DeepMind

sasingla@google.com

###### Abstract

In this paper, we present the e-COP algorithm, the first policy optimization algorithm for constrained Reinforcement Learning (RL) in episodic (finite horizon) settings. Such formulations are applicable when there are separate sets of optimization criteria and constraints on a system's behavior. We approach this problem by first establishing a policy difference lemma for the episodic setting, which provides the theoretical foundation for the algorithm. Then, we propose to combine a set of established and novel solution ideas to yield the e-COP algorithm that is easy to implement and numerically stable, and provide a theoretical guarantee on optimality under certain scaling assumptions. Through extensive empirical analysis using benchmarks in the Safety Gym suite, we show that our algorithm has similar or better performance than SoTA (non-episodic) algorithms adapted for the episodic setting. The scalability of the algorithm opens the door to its application in safety-constrained Reinforcement Learning from Human Feedback for Large Language or Diffusion Models.

## 1 Introduction

RL problems may be formulated in order to satisfy multiple simultaneous objectives. These can include performance objectives that we want to maximize, and physical, operational or other objectives that we wish to constrain rather than maximize. For example, in robotics, we often want to optimize a task completion objective while obeying physical safety constraints. Similarly, in generative AI models, we want to optimize for human preferences while ensuring that the output generations remain safe (expressed perhaps as a threshold on an automatic safety score that penalizes violent or other undesirable content). Scalable policy optimization algorithms such as TRPO , PPO , etc have been central to the achievements of RL over the last decade . In particular, these have found utility in generative models, e.g., in the training of Large Language Models (LLMs) to be aligned to human preferences through the RL with Human Feedback (RLHF) paradigm . But these algorithms are designed primarily for the unconstrained infinite-horizon discounted setting: Their use for constrained problems via optimization of the Lagrangian often gives unsatisfactory constraint satisfaction results. This has prompted development of a number of constrained policy optimization algorithms for the infinite-horizon discounted setting , and for the average setting .

However, many RL problems are more accurately formulated as episodic, i.e., having a finite time horizon. For instance, in image diffusion models , the denoising sequence is really a _finite_ step trajectory optimization problem, better suited to be solved via RL algorithms for the episodic setting. When existing algorithms for infinite horizon discounted setting are used for such problem, they exhibit sub-optimal performance or fail to satisfy task-specific constraints by prioritizing short-term constraint satisfaction over episodic goals . Furthermore, the episodic setting allows for objective functions to be time-dependent which the infinite-horizon formulations do not. Even when the objective functions aretime-invariant, there is a key difference: for non-episodic settings, a stationary policy that is optimal exists whereas for episodic settings, the optimal policy is always non-stationary and time-dependent. This necessitates development of policy optimization algorithms specifically for the episodic constrained setting. We note that such policy optimization algorithms do not exist even for the unconstrained episodic setting.

In this paper, we introduce e-COP, a policy optimization algorithm for episodic constrained RL problems. Inspired by PPO, it uses deep learning-based function approximation, a KL divergence-based proximal trust region and gradient clipping, along with several novel ideas specifically for the finite horizon episodic and constrained setting. The algorithm is based on a technical lemma (Lemma 3.1) on the difference between two policies, which leads to a new loss function for the episodic setting. We also introduce other ideas that obviate the need for matrix inversion thus improving scalability and numerical stability. The resulting algorithm improves performance over state-of-the-art baselines (after adapting them for the episodic setting). In sum, the e-COP algorithm has the following advantages: (i) _Solution equivalence:_ We show that the solution set of our e-COP loss function is same as that of the original CMDP problem, leading to precise cost control during training and avoidance of approximation errors (see Theorem 3.3); (ii) _Stable convergence:_ The e-COP algorithm converges tightly to safe optimal policies without the oscillatory behavior seen in other algorithms like PDO  and FOCOPS ; (iii) _Easy integration:_e-COP follows the skeleton structure of PPO-style algorithms using clipping instead of steady state distribution approximation, and hence can be easily integrated into existing RL pipelines; and (iv) _Empirical performance:_ the e-COP algorithm demonstrates superior performance on several benchmark problems for constrained optimal control compared to several state-of-the-art baselines.

**Our Contributions and Novelty.** We introduce the first policy optimization algorithm for episodic RL problems, both with and without constraints (no constraints is a special case). While some of the policy optimization algorithms can be adapted for the constrained setting via a Lagrangian formulation, as we show, they don't work so well empirically in terms of constraint satisfaction and optimality. The algorithm is based on a policy difference (technical) lemma, which is novel. We have gotten rid of Hessian matrix inversion, a common feature of policy optimization algorithms (see, for example, PPO , CPO , P3O , etc.) and replaced it with a quadratic penalty term which also improves numerical stability near the edge of the constraint set boundary - a problem unique to constrained RL problems. We provide an instance-dependent hyperparameter tuning routine that generalizes to various testing scenarios. And finally, our extensive empirical results against an extensive suite of baseline algorithms (e.g., adapted PPO , FOCOPS , CPO , PCPO , and P3O ) show that e-COP performs the best or near-best on a range of Safety Gym  benchmark problems.

**Related Work.** A broad view of planning and model-free RL techniques for Constrained MDPs is provided in  and . The development of SOTA policy optimization started with the TRPO algorithm , which used a trust region to update the current policy, and was further improved in PPO by use of proximal ideas . This led to works like CPO , RCPO , and PCPO  for constrained RL problems in the infinite-horizon discounted setting. ACPO  extended CPO to the infinite-horizon average setting. These methods typically require inversion of a computationally-expensive Fischer information matrix at each update step, thus limiting scalability. Lagrangian-based algorithms [29; 30] showed that you could incorporate constraints but constrained satisfaction remained a concern. Algorithms like PDO  and RCPO  also use Lagrangian duality but to solve risk-constrained RL problems and suffer from computational overhead. Some other notable algorithms include IPO , P3O , APPO , etc. that use penalty terms, and hence do not suffer from computational overhead but have other drawbacks. For example, IPO assumes feasible intermediate iterations, which cannot be fulfilled in practice, P3O requires arbitrarily large penalty factors for feasibility which can lead to significant estimation errors. We note that all the above algorithms are for the infinite-horizon discounted (non-episodic) setting (except ACPO , which is for the average setting). We are not aware of any policy optimization algorithm for the episodic RL problem, with or without constraints.

## 2 Preliminaries

An episodic, or fixed horizon Markov decision process (MDP) is a tuple, \(:=(S,A,r,P,,H)\), where \(S\) is the set of states, \(A\) is the set of actions, \(r:S A S\) is the reward function, \(P:S A S\) is the transition probability function such that \(P(s^{}|s,a)\) is the probability of transitioning to state \(s^{}\) from state \(s\) by taking action \(a\), \(:S\) is the initial state distribution, and \(H\) is the time horizon for each episode (characterized by a terminal state \(s_{H}\)).

A policy \(:S(A)\) is a mapping from states to probability distributions over the actions, with \((a|s)\) denoting the probability of selecting action \(a\) in state \(s\), and \((A)\) is the probability simplex over the action space \(A\). However, due to the temporal nature of episodic RL, the optimal policies are generally not stationary, and we index the policy at time \(h\) by \(_{h}\), and denote \(_{1:H}=(_{h})_{h=1}^{H}\). Then, the total undiscounted reward objective within an episode is defined as

\[J(_{1:H}):=}_{_{1:H}}[_{h=1 }^{H}r(s_{h},a_{h},s_{h+1})]\]

where \(\) refers to the sample trajectory \((s_{1},a_{1},s_{2},a_{2},,s_{H})\) generated when following a policy sequence, i.e., \(a_{h}_{h}(|s_{h}),\ s_{h+1} P(|s_{h},a_{h}),\) and \(s_{1}\).

Let \(R_{h:H}()\) denote the total reward of a trajectory \(\) starting from time \(h\) until episode terminal time \(H\) generated by following the policy sequence \(_{h:H}\). We also define the state-value function of a state \(s\) at step \(h\) as \(V_{h}^{}(s):=}_{}[R_{h:H}( )\,|\,s_{h}=s]\) and the action-value function as \(Q_{h}^{}(s,a):=}_{}[R_{h:H}( )\,|\,s_{h}=s,a_{h}=a]\). The advantage function is \(A_{h}^{}(s,a):=Q_{h}^{}(s,a)-V_{h}^{}(s)\). We also define \(_{h}^{}(s\,|\,s_{1})=_{a}_{h}^{ }(s,a\,|\,s_{1})\), where the term \(_{h}^{}(s,a\,|\,s_{1})\) is the probability of reaching \((s,a)\) at time step \(h\) following \(\) and starting from \(s_{1}\).

**Constrained MDPs.** A constrained Markov decision process (CMDP) is an MDP augmented with constraints that restrict the set of allowable policies for that MDP. Specifically, we have \(m\) cost functions, \(C_{1},,C_{m}\) (with each function \(C_{i}:S A S\) mapping transition tuples to costs, similar to the reward function), and bounds \(d_{1},,d_{m}\). And similar to the value function for the reward objective, we define the expected total cost objective for each cost function \(C_{i}\) (called cost value for the constraint) as

\[J_{C_{i}}(_{1:H}):=}_{_{1:H}}[ _{h=1}^{H}C_{i}(s_{h},a_{h},s_{h+1})].\]

The goal then, in each episode, is to find a policy sequence \(_{1:H}^{}\) such that

\[J(_{1:H}^{}):=_{_{1:H}_{C}}J(_{1:H}), \ \ \ \ _{C}:=\{_{1:H}\ :\ J_{C_{i}}(_{1:H}) d_{i},\ \ \ i[1:m]\}\] (1)

is the set of feasible policies for a CMDP for some given class of policies \(\). Lastly, analogous to \(V_{h}^{}\), \(Q_{h}^{}\), and \(A_{h}^{}\), we can also define quantities for the cost functions \(C_{i}()\) by replacing, and denote them by \(V_{C_{i},h}^{}\), \(Q_{C_{i},h}^{}\), and \(A_{C_{i},h}^{}\). Proofs of theorems and statements, if not given, are available in Appendix A.

**Notation.**\([N]\) denotes \(\{1,,N\}\) for some \(N\). \(_{h}\) refers to the policy at time step \(h\) within an episode. Denote \(_{s:t}:=(_{s},_{s+1},,_{t})\) for some \(s t\) with \(s,t[H]\). We shall only write \(_{k,h}\) when it is necessary to differentiate policies from different episodes but at the same time \(h\). It then naturally follows to define \(_{k,s:t}\) to be the sequence \(_{s:t}\) in episode \(k\). We will denote \(_{k}_{k,1:H}\), and where not needed drop the index for the episode so that \(_{k}\).

## 3 Episodic Constrained Optimization of Policies (\(\))

In this section, we propose a constrained policy optimization algorithm for episodic MDPs. Policy optimization algorithms for MDPs have proven remarkably effective given their ability to computationally scale up to high dimensional continuous state and action spaces [31; 32; 33]. Such algorithms have also been proposed for infinite-horizon constrained MDPs with discounted criterion  as well as the average criterion  but not for the finite horizon (or as it is often called, the episodic) setting.

We note that finite horizon is not simply a special case of infinite-horizon discounted setting since the reward/cost functions in the former can be time-varying while the latter only allows for time-invariant objectives. Furthermore, even with time-invariant objectives, the optimal policy is time-dependent, while for the latter setting there an optimal policy that is stationary exists.

**A Policy Difference Lemma for Episodic MDPs.** Most policy optimization RL algorithms are based on a value or policy difference technical lemma . Unfortunately, the policy difference lemmas that have been derived previously for the infinite-horizon discounted  and average case  are not applicable here and hence, we derive a new policy difference lemma for the episodic setting.

**Lemma 3.1**.: _For an episode of length \(H\) and two policies, \(\) and \(^{}\), the difference in their performance assuming identical initial state distribution \(\) (i.e., \(s_{1}\)) is given by_

\[J()-J(^{})=_{h=1}^{H},a_{h} _{h,h}^{_{h}^{}}(s_{1},)}{} A_{h}^{^{}}(s_{h},a_{h}).\] (2)

The proof can be found in Appendix A.1. A key difference to note between the above and similar results for infinite-horizon settings  is that considering stationary policies (and hence corresponding occupation measures) is not enough for the episodic setting since, in general, such a policy may be far from optimal. This explains why Lemma 3.1 looks so different (e.g., see (2) in , and Lemma 3.1 in ). Indeed, the lemma above indicates that policy updates do not have to recurse backwards from the terminal time as dynamic programming algorithms do for episodic settings, which is somewhat surprising.

A Constrained Policy Optimization Algorithm for Episodic MDPs.Iterative policy optimization algorithms achieve state of the art performance  on RL problems. Most such algorithms maximize the advantage function based on a suitable policy difference lemma, solving an unconstrained RL problem. Some additionally ensure satisfaction of infinite horizon expectation constraints . However, given that our policy lemma for the episodic setting (Lemma 3.1) is significantly different, we need to re-design the algorithm based on it. A first attempt is presented as Algorithm 1, where each iteration corresponds to an update with a full horizon \(H\) episode.

```
1:Input: Initial policy \(_{0}\), number of episodes \(K\), episode horizon \(H\).
2:for\(k=1,2,,K\)do
3: Run \(_{k-1}\) to collect trajectories \(\).
4: Evaluate \(A_{h}^{_{k-1}}\) and \(A_{C_{i},h}^{_{k-1}}\) for \(h[H]\) from \(\).
5:for\(t=H,H-1,,1\)do
6:\(_{k,t}^{}=}{}_{h=t}^{H}_{_{k,t}}}{}[-A_{h}^{_{k-1}}(s,a)]\)\(\)\(J_{C_{i}}(_{k-1})+_{h=t}^{H}_{_{k,t}}}{} A_{C_{i},h}^{_{k-1}}(s,a) d_{i},\  i\)
7:endfor
8: Set \(_{k}_{k,1}^{},_{k,2}^{},\ ,_{k,H}^{}\).
9:endfor ```

**Algorithm 1**Iterative **P**olicy **O**ptimization for **C**onstrained **E**pisodic (IPOCE) RL

The iterative constrained policy optimization algorithm introduced above uses the current iterate of the policy \(_{k}\) to collect a trajectory \(\), and use them to evaluate \(A_{h}^{_{k-1}}\) and \(A_{C_{i},h}^{_{k-1}}\) for \(h[H]\). At the end of the episode, we solve \(H\) optimization problems (one for each \(h[H]\)) that result in a new sequence of policies \(\). As is natural in episodic problems, we do backward iteration in time, i.e., solve the problem in step (6) at \(h=H\), and then go backwards towards \(h=1\).

Note that the expectation of advantage functions in equation (3) is with respect to the policy \(\) (the optimization variable) and its corresponding _time-dependent_ state occupation distribution \(_{_{h}}\). In the infinite-horizon settings, the expectation is with respect to the steady state stationary distribution, but that does not exist in the episodic setting.

Using current policy for action selection.Algorithm 1 represents an exact principled solution tothe constrained episodic MDP, but the intractable optimization performed in (3) makes it impractical (as in the case of infinite horizon policy optimization algorithms ). We proceed to introduce a sequence of ideas that make the algorithm practical (e.g., by avoiding computationally expensive Hessian matrix inversion for use with trust-region methods ). However, getting rid of trust regions leads to large updates on policies, but PPO  and P3O  successfully overcome this problem by clipping the advantage function and adding a ReLU-like penalty term to the minimization objective. Motivated by this, we rewrite the optimization problem in (3) as follows by parameterizing the policy \(_{k,t}\) in episode \(k\) and time step \(t\) by \(_{k,t}\):

\[_{k,t}=}{}_{h=t}^{H}}}{}-(_{h})A_{h}^{_{k-1}}(s,a) +_{i}^{m}_{t,i}\{0,_{h=t}^{H}_{_{k,t}}}{}(_{h})A_{C_{i},h}^{_{k-1}}( s,a)+J_{C_{i}}(_{k-1})-d_{i}\}.\] (4)where \((_{h})=}}{_{_{k-1,h}}}\) is the importance sampling ratio, \(_{t,i}\) is a penalty factor for constraint \(C_{i}\), and \(_{k,_{h}}_{k,h}=_{k,h}\). Note that the ReLU-like penalty term above is different from the traditional first-order and second-order gradient approximations that are employed in trust-region methods . In essence, the penalty is applied when the agent breaches the associated constraint, while the objective remains consistent with standard policy optimization when all constraints are satisfied.

**Introducing quadratic damping penalty.** It has been noted in such iterative policy optimization algorithms that the behaviour of the learning agent when it nears the constraint threshold is quite volatile during training . This is because the penalty term is active only when the constraints are violated which results in sharp behavior change for the agent. To alleviate this problem, we introduce an additional quadratic damping term to the objective above, which provides stable cost control to compliment the lagged Lagrangian multipliers. This has proved effective in physics-based control applications  resulting in improved convergence since the damping term provides stability, while keeping the solution set the same as for the original Problem (3) and the adapted Problem (4) (as we prove later).

For brevity, we denote the constraint term in Problem (4) as

\[_{C_{i},t}(_{k-1},_{k}):=_{h=t}^{H} }_{s_{_{k,h}}\\ a_{k-1,h}}(_{h})A_{C_{i},h}^{_{k -1}}(s,a)+J_{C_{i}}(_{k-1})-d_{i}.\]

Now introduce a slack variable \(x_{t,i} 0\) for each constraint to convert the inequality constraint (\(_{C_{i},t}(,) 0\)) to equality by letting

\[w_{t,i}(_{k}):=_{C_{i},t}(_{k-1},_{k})+x_{t,i}=0.\]

With this notation, we restate Problem (4) as:

\[_{k,t}^{}=_{_{k,t}}\,_{t}(_{k},) :=_{h=t}^{H}}_{s_{_{k,h} }\\ a_{k-1,h}}-(_{h})A_{h}^{_{k-1}}(s,a)+_{i}^{m}_{t,i}\{0,_{C_{i},t}(_{k-1},_{k})\}.\]

Now we introduce the quadratic damping term and the intermediate loss function then takes the form,

\[_{t}(_{k},,, ):=_{h=t}^{H}}_{s_{_{ k,h}}\\ a_{k-1,h}}-(_{h})A_{h}^{_{k-1}}(s,a)+_{i}^{m}_{t,i}w_{t,i}(_{k})+_ {i}^{m}w_{t,i}^{2}(_{k})\\ (_{k,t}^{},_{t}^{},_{t}^{})-}_{ 0}_{_{k,t}, }_{t}(_{k},,,)\;,\] (5)

where \(\) is the damping factor, \(_{t}=(_{t,i})_{i=1}^{m}\), and \(_{t}=(x_{t,i})_{i=1}^{m}\). We can then construct a primal-dual solution to the max-min optimization problem. The need for a slack variable \(\) can be obviated by setting the partial derivative of \(_{t}()\) with respect to \(\) equal to 0. This leads to a ReLU-like solution: \(x_{t,i}^{}=0,-_{C_{i},t}(_{k-1},_{k})- }{}\). The intermediate problem then takes the form as below.

**Proposition 3.2**.: _The inner optimization problem in (5) with respect to \(\) is a convex quadratic program with non-negative constraints, which can be solved to yield the following intermediate problem:_

\[(_{k,t}^{},_{t}^{} )=_{ 0}_{_{k,t}}\,_{t}(_{k},,),\\ _{t}(_{k},,)=_{h=t}^{H} }_{s_{_{k,h}}\\ a_{k-1,h}}-(_{h})A_{h}^{_{k-1}} (s,a)+_{i}^{m}0,_{C_{i},t}( _{k-1},_{k})+}{}}^{2}-^{2}}{^{2}}.\] (6)

The proof can be found in Appendix A.1. One can see that the cost penalty is active when \(_{C_{i},t}(_{k-1},_{k})-}{}\) rather than when \(_{C_{i},t}(_{k-1},_{k}) 0\). This allows the agent to act in a constrained manner even before the constraint is violated. Further, as we show next, the introduction of the damping factor and the RELU-like penalty does not change the solution of the problem (under some suitable assumptions):

**Theorem 3.3**.: _Let \(^{(3)^{}}\) be a solution to Problem (3), and let \(^{(6)^{}},^{(6)^{}}\) be a solution to Problem (6). Then, for sufficiently large \(>\) and \(_{t,i}>\;\;\;i\), \(^{(3)^{}}\) is a solution to Problem (6), and \(^{(6)^{}}\) is a solution to Problem (3)._

We refer the reader to Appendix A.1 for the proof. This theorem implies that we can search for the optimal feasible policies of the CMDP Problem (1) by iteratively solving Problem (6). Next, we make some further modifications to Problem (6) that give us our final tractable algorithm.

**Removing Lagrange multiplier dependency.** Problem (6) requires a primal-dual algorithm that will iteratively solve over the policies and the dual variable \(\). But from the Lagrangian, we can actually take a derivative with respect to \(\), and then solve for it, which yields the following update rule for it:

\[_{t,i}^{(k)}=0,_{t,i}^{(k-1)}+_{C_{i},t}( _{k-1},_{k-1}).\] (7)

This update rule simplifies the optimization problem and updates the Lagrange multipliers in the \(kth\) episode based on the constraint violation in the \((k-1)\)_th_ episode.

**Clipping the advantage functions.** Solving the optimization problem presented in equation (6) is intractable since we do not know \(_{}\) beforehand. Hence, we replace \(_{}\) by the empirical distribution observed with the policy of the previous episode, \(_{k-1}\), i.e., \(_{_{k,h}}_{_{k-1,h}}\;\;\;h\). Similar to  for PPO, we also use _clipped_ surrogate objective functions for both the reward and cost advantage functions. Thus, the final problem combining equation (4) and equation (6) can be constructed as follows.

If we let

\[_{t}()=_{h=t}^{H}}_{s _{_{k-1,h}}}-(_{h})A_{h}^{_{k-1}}(s,a),((_{h}),\,1-,\,1+)A_{h}^ {_{k-1}}(s,a)}\] \[_{C_{i},t}()=_{h=t}^{H}}_ {s_{_{k-1,h}}}-(_{h})A_{C_{i},h}^{ _{k-1}}(s,a),((_{h}),\,1-,\,1+ )A_{C_{i},h}^{_{k-1}}(s,a)}\]

then, the final loss function \(}_{t}()\) of the final problem is:

\[_{k,t}^{}=_{_{k,t}}}_{t}(_{},,):= _{_{k,t}} _{t}()+_{i}^{m}_{t,i}0, _{C_{i},t}()+J_{C_{i}}(_{k-1})-d_{i} }\] (8) \[+_{i}^{m}0,_{ C_{i},t}()+J_{C_{i}}(_{k-1})-d_{i}+}{}}^{2}-^{2}}{^{2}}\]

Usually for experiments, Gaussian policies with means and variances predicted from neural networks are used . We employ the same approach and since we work in the finite horizon setting, the reward and constraint advantage functions can easily be calculated from any trajectory \(\). The surrogate problem in equation (8) then includes the pessimistic bounds on Problem (6), which is unclipped.

**Adaptive parameter selection.** The value of \(\) is required to be larger than the unknown \(\) according to Theorem 3.3, but we also know that too large a \(\) decays the performance (as seen in harmonic oscillator kinetic energy formulations ). To manage this tradeoff, we provide an instance-dependent adaptive way to adjust the damping factor as a hyperparameter. In each episode \(k\), we update the damping parameter whenever a secondary constraint cost value denoted by \((_{k})\) is larger than some threshold \(c_{k}\). Using Proposition 3.2, we provide the following definitions.

\[(_{k}):=_{t=1}^{H}_{i}^{m}J_{C_ {i}}(_{k})-d_{i},-^{(k)}}{}}  c_{k}:=}{}_{t[H]} _{t}^{(k)}_{}\]

```
1:Input: Initial policy \(_{0}:=_{0}:=_{_{0}}\), critic networks \(V^{_{0}}\) and \(V^{_{0}}_{C_{i}}\;\;\;i\), penalty factor \(\), number of episodes \(K\), episode horizon \(H\), learning rate \(\), penalty update factor \(p\).
2:for\(k=1,2,,K\)do
3: Collect a set of trajectories \(_{k-1}\) with policy \(_{k-1}\) and update the critic network.
4: Get updated \(^{(k)}\) using equation (7).
5:for\(t=H,H-1,,1\)do
6: Update the policy \(_{k,t}_{k,t+1}-_{}}_{t}(,^{(k)},)\) using equation (8).
7:endfor
8:if\((_{k}) c_{k}\)then
9:\(=(_{},p)\)
10:endif
11:endfor ```

**Algorithm 2** Episodic Constrained **O**ptimization of **P**olicies (\(\)-\(\))Hence, we increase \(\) by a constant factor \(>1\) after every episode if \((_{k})_{k}\) until a stopping condition is fulfilled, typically a constant \(_{}\). This leads to constraint-satisfying iterations that are more stable, and we show that it enables a fixed \(\) to generalize well across various tasks. The initial \(\) can simply be selected by a quantified line-search to obtain a feasible \(>\)[3; 4].

We note that the final loss function in equation (8) is differentiable almost everywhere, so we could easily solve it via any first-order optimizer . The final practical algorithm, e-COP is given in Algorithm 2.

## 4 Experiments

We conducted extensive experimental evaluation on the relative empirical performance of the e-COP algorithm to arrive at the following conclusions: (i) The e-COP algorithm performs better or nearly as well as all baseline algorithms for infinite-horizon discounted safe RL tasks in maximizing episodic return while satisfying given constraints. (ii) It is more robust to stochastic and complex environments , even where previous methods struggle. (iii) It has stable behavior and more accurate cost control as compared to other baselines near the constraint threshold due to the damping term.

**Environments.** For a comprehensive empirical evaluation, we selected eight scenarios from well-known safe RL benchmark environments - Safe MuJoCo  and Safety Gym , as well as MuJoCo environments. These include: Humanoid, PointCircle, AntCircle, PointReach, AntReach, Grid, Bottleneck, and Navigation. See Figure 1 for an overview of the tasks and scenarios. Note that Navigation is a multi-constraint task and for the Reach environment, we set the reward as a function of the Euclidean distance between agent's position and goal position. In addition, we make it impossible for the agent to reach the goal before the end of the episode. For more information see Appendix A.2.1.

**Baselines.** We compare our e-COP algorithm with the following baseline algorithms: CPO , PCPO , FOCOPS , PPO with Lagrangian relaxation [33; 35], and penalty-based P3O . Since the above state-of-the-art baseline algorithms are already well understood to outperform other algorithms such as PDO , IPO , and CPPO-PID  in prior benchmarking studies, we do not compare against them. Moreover, since PPO does not originally incorporate constraints, for fair comparison, we introduce constraints using a Lagrangian relaxation (called PPO-L). In addition, for each algorithm, we report its performance with the discount factor that achieves the best performance. See Appendix A.3.1 for more details.

**Evaluation Details and Protocol.** For the Circle task, we use a a point-mass with \(S^{9}\), \(A^{2}\) and for the Reach task, an ant robot with \(S^{16}\), \(A^{8}\). The Grid task has \(S^{56}\), \(A^{4}\). We use two hidden layer neural networks to represent Gaussian policies for the tasks. For Circle and Reach, size is (32,32) for both layers, and for Grid and Navigation the layer sizes are (16,16) and (25,25). We set the step size \(\) to \(10^{-4}\), and for each task, we conduct 5 independent runs of \(K=500\) episodes each of horizon \(H=200\). Since there are two objectives (rewards in the objective and costs in the constraints), we show the plots which maximize the reward objective while satisfying the cost constraint.

### Performance Analysis

Table 1 lists the numerical performance of all tested algorithms in seven single constraint scenarios, and one multiple constraint scenario. We find that overall, the e-COP algorithm in most cases outperforms (green) all other baseline algorithms in finding the optimal policy while satisfying the constraints, and in other cases comes a close second (light green).

From Figure 2, we can see how the e-COP algorithm is able to improve the reward objective over the baselines while having approximate constraint satisfaction. We also see that updates of e-COP are faster and smoother than other baselines due to the added damping penalty, which ensures smoother

Figure 1: The Humanoid, Circle, Reach, Grid, Bottleneck, and Navigation tasks. See Appendix A.2.1 for details.

convergence with only a few constraint-violating behaviors during training. In particular, e-COP is the _only_ algorithm that best learns almost-constraint-satisfying maximum reward policies across _all_ tasks: in simple Humanoid and Circle environments, e-COP is able to almost exactly track the cost constraint values to within the given threshold. However, for the high dimensional Grid environment we have more constraint violations due to complexity of the policy behavior, leading to higher variance in episodic rewards as compared to other environments. Regardless, overall in these environments, e-COP still outperforms _all_ other baselines with the least episodic constraint violation. For the multiple constraint Navigation environment, see Figure 3.

### Secondary Evaluation

In this section, we take a deeper dive into the empirical performance of e-COP. We discuss its dependence on various factors, and try to verify its merits.

**Generalizability.** From the discussion above, it's clear that e-COP demonstrates accurate safety satisfaction across tasks of varying difficulty levels. From Figure 4, we further see that e-COP satisfies the

   Task & e-COP & FCOPS  & PPO-L  & PCOP  & P3O  & CPO  & APPO  & IPO  \\   & R & 1625.2 \(\) 13.4 & **1734.1 \(\) 27.4** & 1431.2 \(\) 25.2 & 1602.3 \(\) 10.1 & 1669.4 \(\) 13.7 & 1465.1 \(\) 55.3 & 1488.2 \(\) 29.3 & 1578.6 \(\) 25.2 \\  & C (20.0) & 173.7 \(\) 0.3 & 19.7 \(\) 0.6 & 18.8 \(\) 15.6 & 163.1 \(\) 1.4 & 201.2 \(\) 33.8 & 18.5 \(\) 2.9 & 200.0 \(\) 1.3 & 19.1 \(\) 2.5 \\   & R & **1105.9 \(\) 9.3** & 81.6 \(\) 8.4 & 57.2 \(\) 9.2 & 68.2 \(\) 9.1 & 89.1 \(\) 7.1 & 65.3 \(\) 5.3 & 91.2 \(\) 9.6 & 68.7 \(\) 15.2 \\  & C (10.0) & 9.8 \(\) 0.9 & 10.0 \(\) 0.4 & 9.8 \(\) 0.5 & 9.9 \(\) 0.4 & 9.9 \(\) 0.3 & 9.5 \(\) 0.9 & 10.2 \(\) 0.6 & 9.3 \(\) 0.5 \\   & R & **198.6 \(\) 7.4** & 161.9 \(\) 22.2 & 134.4 \(\) 10.3 & 168.3 \(\) 13.3 & 182.6 \(\) 18.7 & 127.1 \(\) 12.1 & 155.5 \(\) 19.4 & 149.3 \(\) 33.6 \\  & C (10.0) & 9.8 \(\) 0.6 & 9.9 \(\) 0.5 & 9.6 \(\) 1.6 & 9.5 \(\) 0.6 & 9.8 \(\) 0.2 & 10.1 \(\) 0.7 & 10.0 \(\) 0.5 & 9.5 \(\) 1.0 \\   & R & **81.5 \(\) 10.2** & 65.1 \(\) 9.6 & 46.1 \(\) 14.8 & 73.2 \(\) 7.4 & 76.3 \(\) 6.4 & 89.2 \(\) 8.1 & 74.3 \(\) 6.7 & 49.1 \(\) 10.6 \\  & C (25.0) & 24.5 \(\) 6.1 & 24.8 \(\) 7.6 & 25.1 \(\) 6.1 & 24.9 \(\) 5.6 & 26.3 \(\) 6.9 & 33.3 \(\) 10.7 & 26.3 \(\) 8.1 & 24.7 \(\) 11.3 \\   & R & 70.8 \(\) 14.6 & 4.8 \(\) 5.6 & 54.2 \(\) 9.5 & 39.4 \(\) 5.3 & **73.6 \(\) 5.1** & 102.3 \(\) 7.1 & 61.5 \(\) 10.4 & 45.2 \(\) 13.3 \\  & C (25.0) & 24.2 \(\) 8.4 & 25.1 \(\) 11.9 & 21.9 \(\) 10.7 & 27.9 \(\) 12.2 & 24.8 \(\) 7.3 & 35.1 \(\) 10.9 & 24.5 \(\) 6.4 & 24.9 \(\) 9.2 \\   & R & 258.1 \(\) 33.1 & 215.4 \(\) 45.6 & **2763.3 \(\) 57.9** & 226.5 \(\) 29.2 & 201.5 \(\) 39.2 & 178.1 \(\) 23.8 & 184.4 \(\) 21.5 & 229.4 \(\) 32.8 \\  & C (75.0) & 71.3 \(\) 26.9 & 76.6 \(\) 29.8 & 71.8 \(\) 25.1 & 72.6 \(\) 16.5 & 79.3 \(\) 19.3 & 69.3 \(\) 19.8 & 79.5 \(\) 35.8 & 74.2 \(\) 24.6 \\   & R & **345.1 \(\) 52.6** & 251.3 \(\) 59.1 & 298.3 \(\) 71.2 & 264.2 \(\) 43.8 & 291.1 \(\) 26.7 & 388.1 \(\) 36.6 & 220.1 \(\) 30.1 & 279.3 \(\) 43.8 \\  & C (50.0) & 49.7 \(\) 15.1 & 46.6 \(\) 19.8 & 41.4 \(\) 17.6 & 49.8 \(\) 10.5 & 45.3 \(\) 8.2 & 54.3 \(\) 13.5 & 47.4 \(\) 12.3 & 48.2 \(\) 14.6 \\   & R & **217.6 \(\) 11.5** & 175.1 \(\) 3.7 & 153.7 \(\) 25.2 & 135.7 \(\) 19.2 & 164.1 \(\) 12.8 \\  & C (10.0) & 9.6 \(\) 15.5 & n/a & 9.9 \(\) 1.9 & n/a & 9.9 \(\) 1.7 & n/a & 9.9 \(\) 2.1 & 10.0 \(\) 0.5 \\   & **23.7 \(\) 4.1** & & 22.3 \(\) 2.1 & 24.5 \(\) 4.1 & & 23.9 \(\) 3.8 & 24.6 \(\) 3.1 \\   

Table 1: Mean performance with normal 95% confidence interval over 5 independent runs on some tasks.

Figure 2: The cumulative episodic reward and constraint cost function values vs episode learning curves for some algorithm-task pairs. Solid lines in each figure are the means, while the shaded area represents 1 standard deviation, all over 5 runs. The dashed line in constraint plots is the constraint threshold.

constraints in all cases and precisely converges to the specified cost limit. Furthermore, the fluctuation observed in the baseline Lagrangian-based algorithms is shown not to be tied to a specific cost limit.

We also conducted a set of experiments wherein we study how e-COP effectively adapts to different cost thresholds. For this, we use the hyperparameters of a pre-trained e-COP agent, which is trained with a particular cost threshold in an environment, for learning on different cost thresholds within the same environment. Figure 6 in Appendix A.3.4 illustrates the training curves of these pre-trained agents, and we see that while e-COP can generalize well across different cost thresholds, other baseline algorithms may require further tuning to accommodate different constraint thresholds.

**Sensitivity.** The effectiveness and performance of e-COP would not be justified if it was not robust to the damping hyperparameter \(\), which varies across tasks depending on the values of \(C()\) and \(c_{h}\). Since this damping penalty enables e-COP to have stable continuous cost control, we update it adaptively as described in Algorithm 2. As seen in Table 2, damping penalty indeed stabilizes the training process and helps in converging to an optimal safe policy.

## 5 Conclusion

In this paper, we have introduced an easy to implement, scalable policy optimization algorithm for episodic RL problems with constraints due to safety or other considerations. It is based on a policy difference lemma for the episodic setting, which surprisingly has quite a different form than the ones for infinite-horizon discounted or average settings. This provides the theoretical foundation for the algorithm, which is designed by incorporating several time-tested, practical as well as novel ideas. Policy optimization algorithms for Constrained MDPs tend to be numerical unstable and non-scalable due to the need for inverting the Fisher information matrix. We sidestep both of these issues by introducing a quadratic damping penalty term that works remarkably well. The algorithm development is well supported by theory, as well as with extensive empirical analysis on a range of Safety Gym and Safe Muloco benchmark environments against a suite of baseline algorithms adapted from their non-episodic roots.

   & &  &  \\    & & \(=5\), fixed & \(=5\), adaptive & \(=10\), fixed & \(=10\), adaptive & \\   & R & 150.5 \(\) 11.1 & **168.6 \(\) 14.3** & 145.2 \(\) 12.2 & 165.3 \(\) 11.4 & 162.4 \(\) 14.7 \\  & C (20.0) & 17.3 \(\) 1.3 & 19.7 \(\) 0.6 & 18.8 \(\) 1.5 & 18.3 \(\) 1.4 & 19.1 \(\) 3.3 \\   & R & 48.2 \(\) 3.5 & 58.6 \(\) 5.1 & 53.2 \(\) 5.3 & **65.2 \(\) 8.1** & 61.1 \(\) 5.6 \\  & C (20.0) & 19.8 \(\) 5.9 & 20.0 \(\) 4.4 & 20.6 \(\) 4.5 & 19.2 \(\) 6.2 & 18.9 \(\) 7.3 \\  

Table 2: Performance of e-COP for different \(\) settings on two tasks. Values are given with normal 95% confidence interval over 5 independent runs.

Figure 4: Cumulative episodic rewards and costs of baselines in two environments with two constraint thresholds.

Figure 3: Navigation environment with multiple constraints: Episodic Rewards (left), Cost1 (center, for hazards) and Cost2 (right, for pillars) of e-COP. The dashed line in the cost plots is the cost threshold (10 for Cost1 and 25 for Cost2). C1/C2 constrained means only taking Cost1/Cost2 into the e-COP loss function and ignoring the other one.