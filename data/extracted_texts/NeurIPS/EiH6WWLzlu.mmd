# Following the chopping, the man starts to arrange the ingredients on the cutting board, indicating a transition to combining the salad components. At this stage, he holds a large glass bowl filled with brightly colored tomatoes and prepped ingredients, ready for further action. Soon, he continuously tips the glass bowl to ensure the ingredients are evenly mixed.

[MISSING_PAGE_FAIL:1]

###### Abstract

We present the ShareGPT4Video series, aiming to facilitate the video understanding of large video-language models (LVLMs) and the video generation of text-to-video models (T2VMs) via dense and precise captions. To achieve this, taking aside the non-scalable costly human annotators, we find using GPT4V to caption video with a naive multi-frame or frame-concatenation input strategy leads to less detailed and sometimes temporal-confused results. We argue the challenge of designing a high-quality video captioning strategy lies in three aspects: **1) Inter-frame precise temporal change understanding. 2) Intra-frame detailed content description. 3) Frame-number scalability for arbitrary-length videos.** To this end, we meticulously designed a differential video captioning strategy, which is stable, scalable, and efficient for generating captions for videos with arbitrary resolution, aspect ratios, and length. Based on it, we construct ShareGPT4Video, which contains 40K high-quality videos spanning a wide range of categories, and the resulting captions encompass rich world knowledge, object attributes, camera movements, and crucially, detailed and precise temporal descriptions of events. Based on ShareGPT4Video, we further develop ShareCaptioner-Video, a superior captioner capable of efficiently generating high-quality captions for arbitrary videos. We annotated 4.8M aesthetically appealing videos by it and verified their effectiveness on a 10-second text2video generation task. For video understanding, we verified the effectiveness of ShareGPT4Video on several current LVLM architectures and presented our superb new LVLM ShareGPT4Video-8B. All the models, strategies, and annotations will be open-sourced and we hope it can serve as a pivotal resource for advancing both the LVLMs and T2VMs community. We released the full project at https://sharegpt4video.github.io/.

## 1 Introduction

Recent advancements in multi-modal learning, driven by large language models, have led to progress in image-text dialogue and text-to-image generation tasks. This has inspired a shift towards video understanding and generation tasks, allowing for user interaction across video and language modalities. Thus, the detailed and high-fidelity video captions, which bridge the aforementioned modalities, are instrumental in propelling the advancements within the field.

Despite the rich semantic and temporal content of videos, they are often paired with brief captions in existing data. These short descriptions limit the detailed video understanding and the controllability of video generation. While the importance of detailed captions is recognized in image-text dialogue and text-to-image generation tasks, similar efforts are lacking in video understanding and generation.

However, creating large-scale, high-quality video captions is challenging. Detailed captioning for long videos is non-trivial and time-consuming even for humans, hindering large-scale annotation. Current open-source LVLMs lack this capability, and closed-source APIs do not yet support video inputs. On the other hand, if we roughly degrade the input from video to multiple frames, even GPT4V struggles to describe the video with satisfied quality. For example, an intuitive idea is to provide multiple frames with timestamps to the GPT4V and generate the caption, while we find that GPT4V is unstable and sometimes misunderstands the temporal relation between the frames, and its performance further degrades with the increasing of video frames. Others such as concatenating all the frames into a large image are non-helpful to the temporal problem, and the caption loses details as the frame number increases. We also showcase these problems in Figure 11 and 12.

We posit that the challenge of devising an effective video captioning strategy is rooted in three fundamental aspects: _1) Inter-frame precise temporal change understanding_: The temporal dimension distinguishes videos from images. An imprecise temporal description can significantly diminish the quality of the video caption and lead to confusion in the training models. _2) Intra-frame detailed content description_: Detailed descriptions  are crucial for aligning modalities between image and text, which are also important for video-text alignment. _3) Frame-number scalability for arbitrary_length videos_: Videos encountered in the wild can vary greatly in length. An ideal captioning strategy should be resilient to this variability and generate appropriate captions for videos of any length.

To this end, we present the **Differential Sliding-Window Captioning strategy** (DiffSW), which is _stable, scalable, and efficient for generating captions for arbitrary videos_. The central concept of DiffSW is translating the all-frames-to-caption task into a differential description task. Specifically, we generate a detailed caption for the first frame and apply a sliding window of length two to the subsequent frames in chronological order. The powerful image multi-modal model, GPT4V , is tasked with identifying the changes between frames based on three inputs: the previous frame, its differential caption, and the current frame. This encompasses alterations in camera movement, object movement, character actions, and scene transitions. Upon acquiring all differential captions, these are input into GPT4  to construct a comprehensive caption for the entire video. The differential concept allows DiffSW to concentrate on the changes between frames, i.e., the temporal changes. Its sliding design ensures the correctness of temporal order and invariance towards the total number of frames. The constant input frame number guarantees that GPT4V does not overlook details and utilizes the API efficiently, resulting in stable, scalable, and efficient caption quality from DiffSW. Furthermore, the differential design enables the re-caption of any sub-clips of a captioned video by reusing its differential captions.

Based on DiffSW, we construct **ShareGPT4Video**, which contains **40K high-quality video-caption pairs** spanning a wide range of categories, and the resulting captions encompass rich world knowledge, object attributes, camera movements, and crucially, detailed and precise temporal descriptions of events. The videos of ShareGPT4Video are collected from various sources [14; 79; 56; 21; 57; 51], employed with a Semantic-based Data Filtering strategy to mitigate content homogeneity among these videos. A Semantic-aware Key-frame Extraction strategy is then applied to the videos to reduce the temporal redundancy. DiffSW is applied to the keyframes to generate high-quality captions and we further improve its stability and quality with a Hierarchical Prompt Design. Manual quality inspection is employed to ensure the quality of the video captions.

Based on ShareGPT4Video, we present ShareCaptionor-Video, an exceptional video captioner capable of efficiently generating high-quality captions for videos of a wide range of resolution, aspect ratio, and duration. It enables the further scaling of high-quality video caption data with minor cost and satisfactory quality, and we generate high-quality captions for 4.8M aesthetically appealing videos (totaling about 3000 hours) by it.

We conduct extensive experiments in video understanding and generation tasks to demonstrate the value of our high-quality video-caption dataset and our superior video captioner. For video generation, a DiT-based  text-to-video model trained on the 4.8M video-captions pairs performs well in generating 10-second high-resolution videos and achieving fine-grained control over content generation. For video understanding, ShareGPT4Video brings consistent performance gain of multiple current LVLMs over multiple benchmarks by replacing a small proportion of training data. We further present ShareGPT4Video-8B, a simple yet superb LVLM that reached SOTA performance on three advancing and comprehensive video benchmarks. The model, strategy, and annotations will be publicly available and we hope this project can serve as a pivotal resource for advancing both the LVLMs and T2VMs community.

## 2 Related Work

**Large video-language models.** The vision-language community has recently made significant advancements in the field of large image-language models [42; 41; 43; 11; 15; 12; 68; 69; 2; 82; 71; 20; 25; 26; 73; 46; 28; 92; 85; 58; 87; 39]. These models typically employ three core components: a visual encoder for extracting visual features, a connector to bridge the visual and language modalities, and a large language model (LLM) to decode the multimodal context. A milestone in this field is LLaVA-1.5 , which uses CLIP-Large  as the visual encoder, a two-layer MLP as the connector, and Vicuna-v1.5  as the LLM backbone. This model achieves exceptional performance with only affordable data for training. Following this paradigm, the large video-language model field has also seen emerging efforts to encode video into vision tokens suitable for LLMs, enabling understanding and reasoning about video content. For example, VideoChat2  employs a video transformer and a Q-Former  to extract and compress visual tokens. VideoLLaVA  uses a pre-aligned video encoder with image and language, along with an MLP, to extract and transform visual tokens.

LLaMA-VID  introduces the LLAVA-like three-stage training strategy and compresses each frame's vision token into one to support long video understanding. Despite the continuous exploration of training strategies and model architectures, none of these methods have investigated the role of high-quality captions in aligning video and language modalities, as has been done in the large image-language model field [11; 8].

**Text-to-video models.** Recent advancements in the text-to-image field, such as DALL-E , and Stable Diffusion , have significantly propelled the T2I area. Inspired by these achievements, researchers have increasingly begun to explore the potential of generating high-fidelity videos from textual descriptions. For instance, MagicVideo  introduces a 3D U-Net-based architecture and directs temporal attention to ensure efficient, high-fidelity video generation while maintaining temporal consistency and authenticity. PixelDance  and SVD  leverage pixel-based and latent-based techniques to produce high-resolution videos, while Make-A-Video  and Imagen Video  extend text-to-image models and techniques to the text-to-video domain. A milestone in this field is Sora , which ambitiously employs DiT to train a text-to-video model from scratch. Sora is the first model capable of generating minute-long videos based on user instructions. The success of this remarkable feat relies on extensive high-quality video-caption data, meticulously designed architectural optimizations, and substantial computational power. Despite the pivotal role of high-quality video-caption data in T2VMs, this aspect has not received sufficient attention. Therefore, in this work, we aim to construct a high-quality video-caption dataset to advance the development of T2VMs.

## 3 ShareGPT4Video Dataset

This section provides a detailed exposition of how we construct the ShareGPT4Video dataset. We detail the entire process in Figure 2.

### Data Collection

**Selection of Data Sources.** To serve both video understanding and video generation tasks, we consider the aesthetic quality and content complexity of videos during our collection process. We first consider Panda-70M , a high-resolution video dataset sourced from YouTube, featuring clips ranging in one minute. This open-domain source covers diverse areas such as wildlife, cooking, sports, news & TV shows, gaming & 3D rendering. It typically includes complex content and transitions, providing a solid foundation for understanding various real-world scenarios. However, the complexity of these contents and transitions presents a significant challenge for the video generation field. To address this, we also source a large volume of aesthetically appealing videos from some user-uploaded video websites [56; 57; 51]. These videos predominantly consist of scenic views and aesthetically pleasing human activities, involving fewer transitions and simpler events. Finally, we supplement our collection with selected videos from Ego4D  and BDD100K  to fill the gaps in ego-centric human activities and auto-driving scenarios, ensuring our video sources encompass as many real-world scenes as possible.

**Semantic-Based Data Filtering.** Although our captioning method can support videos of extended lengths, our collection primarily focuses on videos shorter than two minutes due to the trade-off of the duration and amount of videos. We initially filter out videos from our selected data sources longer than two minutes, leaving videos in two minutes as the candidates. We then introduce a semantic-based data filtering strategy to mitigate content homogeneity among these candidates and maintain diversity in the final video dataset. This approach aims to select videos with significant thematic differences from the pool of candidates to compose our final video collection. Specifically, we first use the Panda-Student  model to generate a short caption with one sentence for each candidate video, and then maintain a final pool of video candidates. Whenever a new video \(V\) is processed, we encode its corresponding short caption \(S\) using the Bert-Base-Uncased  language model to obtain the \(\) token \(P_{n+1}^{1 D}\), which captures high-level semantic expressions. We then calculate the similarity between this \(\) token \(P_{n+1}\) and the \(\) tokens \(\{P_{1},P_{2},,P_{n}\}\) of videos already in the final candidate pool. A new video will only be added to the pool if its maximum similarity is below a predefined threshold. We provide the pseudo-code in Figure 14.

### Video Processing

Videos are commonly redundant on the temporal dimension, and keyframe sampling is a general idea to represent a video compactly. However, traditional key-frame extraction methods [91; 6] often struggle to ensure semantic coherence, leading to missing key-frames covering crucial changes and transitions. Consequently, we develop a semantic-aware key-frame extraction method that strikes a balance between reducing temporal redundancy in videos and maintaining the semantic coherence of the content.

**Semantic-aware Key-frame Extraction.** We denote \(V^{T H W 3}\) as a \(T\) frame set sampled from a video with fixed 2-second intervals. We calculate the keyframe set \(V^{}^{T^{} H W 3}\) that are sufficiently sparse yet comprehensively cover the evolution of events within the video that \(T^{}<T\). We view the output \(\) token of the CLIP-Large image encoder  as the global semantics of each frame and remove the adjacent frames that have a high semantic similarity. In practice, we initialize the keyframe set \(V^{}\) with the first frame of \(V\). For each frame in \(V\), we calculate its semantic similarity \(d\) with the latest keyframe in \(V^{}\). If \(d\) is lower than the pre-defined threshold, we view the frame as a keyframe and add it to the \(V^{}\). If not, the frame is skipped as redundant. For completeness, the last frame of \(V\) is always added in \(V^{}\). We provide the pseudo-code in Figure 15.

### Captioning Pipeline

As we mentioned in Section 1, we find if we feed all the frames to the GPT4V directly, the GPT4V struggles to stably generate captions with the correct temporal relation between frames, and its performance further worsens with the frame number increasing. On the other hand, if we concatenate all the frames into a large image, the GPT4V loses more details with the increasing frame number, as shown in Figure 11 and 12. To this end, a **stable**, **scalable**, and **efficient** strategy is essential for large-scale annotation of videos with arbitrary length.

Figure 3: **Comprehensive video-caption dataset: (a) The dataset covers a broad spectrum of content, including wildlife, cooking, sports, scenery, ego-centric human activities, auto-driving scenarios, etc. (b) The dataset includes videos ranging from 2 seconds to 2 minutes in length. (c) The captions primarily range from 200 to 400 words, providing rich temporal information that serves video understanding and generation tasks well.**

Figure 2: **Pipeline for generating high-quality video-caption data. We begin by selecting diverse video sources based on aesthetic quality and content complexity. Next, we use semantic-based data filtering to prevent content homogenization. We then apply semantic-aware key-frame extraction for sparse sampling, maintaining significant semantic variations. Finally, we implement a differential sliding-window captioning strategy, utilizing GPT-4V to generate detailed and temporally rich captions.**

**Differential Sliding-window Captioning.** To this end, we develop a differential sliding-window captioning pipeline to generate high-quality captions with detailed temporal descriptions for various videos. Specifically, the input fed to the image multi-modal model each time includes the current key-frame and the previous key-frame along with its differential caption. Then, we introduce the Differential Prompt to guide GPT4V in focusing on the changes between the current and previous frames, such as posture, position, camera angle, etc. Additionally, incorporating the differential caption of the previous frame as supplementary context enhances the response quality and reduces hallucinations. This is because the image embedding and textual caption provide explicit and implicit representations of the image, respectively. The differential caption not only adds extra context but also integrates temporal information from two frames ago, further improving the model's temporal understanding. It's important to note that for the first key-frame, which lacks a preceding frame, its differential caption is replaced directly with the standard caption. Finally, we input all differential captions along with their corresponding timestamps into GPT4. A specific Summary Prompt is designed to instruct the LLM to generate high-quality video captions with precise temporal dynamics and detailed spatial information. In practice, we use GPT-4-Turbo-04-09 for all the annotations.

In the design of the prompts, we discovered that an explicit Hierarchical Prompt Design significantly aids the GPT4V in comprehending its role, its expected format, and its operational boundaries. This approach contributes to the stabilization of the output's format and enhances the overall quality of the results. For more details, please refer to Section A.2

## 4 ShareCaptioner-Video

### Model Design

We fine-tune the IXC2-4KHD  using the collected video caption data, resulting in our ShareCaptioner-Video. For flexible usage, we re-organize the data for the following capabilities:

**1. Fast Captioning** The model employs an image-grid format for direct video captioning, providing rapid generation speeds that are ideal for short videos. In practice, we concatenate all the keyframes of a video into a vertically elongated image and train the model on a caption task.

**2. Sliding Captioning** The model supports streaming captioning in a differential sliding-window format, yielding high-quality captions that are suitable for long videos. Similar to the captioning pipeline used in Section 3.3, we take the two adjacent keyframes alongside the previous differential caption as input, and train the model to describe the events occurring between them.

**3. Clip Summarizing** The model can swiftly summarize any clip from ShareGPT4Video or videos that have undergone the differential sliding-window captioning process, eliminating the need to re-process frames. We use all the differential descriptions as input, and the output is the video caption.

**4. Prompt Re-Captioning:** The model can rephrase prompts input by users who prefer specific video generation areas, ensuring that T2VMs trained on high-quality video-caption data maintain format alignment during inference with their training. In practice, we use GPT-4 to generate Sora-style prompts for our dense captions, and we train the re-captioning task in reverse, _i.e._, by using the generated prompt as input and the dense caption as the training target.

Figure 4: The ShareCaptioner-Video is a Four-in-One exceptional video captioning model with the following capabilities: Fast captioning, Sliding Captioning, Clip Summarizing, and Prompt Re-Captioning.

In practice, we fine-tune the model end-to-end over one epoch. We follow the default high-resolution strategy, using 'HD-55' for fast captioning and 'HD-25' for the others. The learning rate is uniform across all model components and warms up from 0 to within the first 1% of steps. The batch size is set to, and we sample the data uniformly.

### Scaling-up Captions

To validate the effectiveness of our ShareCaptioner-Video in the video captioning task and further support the development of the video generation domain, we utilized it to annotate a large volume of aesthetically appealing videos. Specifically, we meticulously collect and process 4.8 million video clips, totaling approximately 3000 hours, from three sources: MixKit , Pexels , and Pixabay . Subsequently, we employ the sliding captioning mode of ShareCaptioner-Video to generate high-quality captions for these videos. The total captioning process requires approximately 4000 H100 GPU hours. We provide some statistics on generated captions in Figure 8.

## 5 Experiments

### Video Understanding

**Datasets and Benchmarks.** To thoroughly explore the benefits that our high-quality video-caption data bring to LVLMs, we conduct comprehensive evaluations of the model across three multi-modal video benchmarks. VideoBench  curates approximately 15,000 QA pairs spanning 10 evaluation dimensions from 13 existing data sources, such as MSVD-QA , MSRVTT-QA , Activitynet-QA , etc. MVBench  is designed to challenge LVLMs with video tasks that cannot be effectively resolved by single-frame reliance, featuring 4,000 QA pairs derived from 11 public video benchmarks. TempCompass  specifically assesses the nuanced performance of LVLMs across various temporal aspects, such as speed, direction, and attribute changes. It includes 410 videos and 7,540 meticulously collected instructions, emphasizing temporal comprehension and interaction.

**Improving current LVLMs with ShareGPT4Video.** We validate the effectiveness of the high-quality video-caption data collected in ShareGPT4Video to improve the performance of current LVLMs. For fairness and simplicity, we integrate 28K high-quality video-caption data related to complex scenes (Panda-70M , Ego4D , and BDD100K ) of ShareGPT4Video to replace the captions data in the VideoChatGPT-100K  conversation data with an equivalent number. Then we train the VideoLLaVA  and LLaMA-VID  with their default training settings and hyperparameters.

As shown in Table 1, ShareGPT4Video consistently improves the alignment between video and language modalities in different LVLM architectures and scales. Specifically, VideoLLaVA-7B  achieves an average performance gain of 1.1 across three comprehensive multi-modal video benchmarks after integrating high-quality captions, while LLaMA-VID-7B and LLaMA-VID-13B achieve an average gain of 2.0 and 2.3, separately. Our high-quality video-caption data is particularly effective in helping LVLMs achieve significant performance improvements on benchmarks that require complex temporal understanding, such as TempCompass .

**ShareGPT4Video-8B.** To obtain our final ShareGPT4Video-8B model, we start with the LLaVA-Next-8B  image multi-modal model, implemented by the Open-LLaVA-Next codebase . Consistent with previous LVLM approaches [40; 50], we uniformly sample 16 frames from each video and arrange these frames into a 4x4 image grid to form the input for both training and inference, following the IG-VLM  strategy. For training data, we first collect 153K VQA data from various instructional video-to-text datasets to build our baseline. This collection includes 13K conversational

   Model &  & Avg. \\  Video1LaVA-7B  & 34.5 & 43.0 & 50.6 & 42.7 \\ Video1LaVA-7B+Ours & **35.2** & **43.6** & **52.7** & **43.8** \\  LLaMA-VID-7B  & 36.5 & 41.3 & 48.1 & 42.0 \\ LLaMA-VID-7B-Ours & **38.2** & **43.2** & **50.6** & **44.0** \\  LLMaVA-VID-13B  & 48.3 & 43.3 & 51.4 & 47.7 \\ LLaMA-VID-13B+Ours & **52.4** & **44.2** & **53.3** & **50.0** \\   

Table 1: **The gain from high-quality captions is universal among model architectures and scales. We report the baseline based on their public checkpoints. The best results are bold.**

   Caption & Unblock & ViT & VideoBench & MVBench & TempCompass & Avg. \\  – & \(\) & 37.3 & 47.2 & 57.2 & 47.2 \\ short & \(\) & 36.9 & 47.5 & 56.1 & 46.8 \\ short & \(\) & 37.5 & 47.9 & 56.9 & 47.4 \\ detailed & \(\) & 40.7 & 50.3 & 60.7 & 50.6 \\ detailed & \(\) & **41.2** & **51.2** & **61.5** & **51.3** \\   

Table 2: **Combined with VQA data, detailed captions can benefit LVLMs more compared to short captions. The baseline (first row) utilizes only 153K VQA data. The best results are in bold.**

[MISSING_PAGE_FAIL:8]

of high-fidelity 10-second videos. For comparison, we fine-tuned a baseline model with the same quantity of video-short-captions pairs. For more training details, please refer to Section A.1.

**Qualitative analysis.** As illustrated in Figure 5, the T2VM can accurately follow detailed prompts and demonstrate remarkable control over semantic content and camera movement when aided by high-quality, detailed captions generated by ShareCaptioner-Video. The resulting video showcases intricate and lively content. In contrast, when provided with brief captions, the T2VM struggles to adhere to complex generation prompts, leading to subpar results.

## 6 Limitations and Social Impacts

**Limitations.** Although our current pipeline for generating high-quality video captions fully utilizes visual and textual information, it is limited by GPT4V's inability to incorporate audio information

Figure 5: **Example of 10-second text-to-video task. The T2VM trained on the detailed video-caption data can exhibit impressive camera control.**

Figure 6: **Influence of T2VM training caption length. Thanks to the high-quality captions generated by ShareCaptioner-Video, the T2VM trained on the detailed video-caption data exhibits impressive semantic content control (video below), while the T2VM with short captions failed to follow the complex prompt (video above).**

   Model & AS & AR & AA & FA & UA & OE & OS & MO & AL & ST & AC & MC & MA & SC & FP & CO & EN & ER & CI & Avg. \\  Other-7B  & 23.0 & 23.0 & 27.5 & 27.0 & 29.5 & 53.0 & 28.0 & 33.0 & 24.5 & 23.5 & 27.5 & 26.0 & 28.5 & 18.0 & 38.5 & 22.0 & 22.0 & 23.5 & 19.0 & 19.5 & 26.8 \\ mPLUG-Out-7B  & 22.0 & 28.0 & 34.0 & 29.0 & 29.0 & 40.5 & 27.0 & 31.5 & 27.0 & 23.0 & 29.0 & 31.5 & 27.0 & 40.0 & 44.0 & 24.0 & 31.0 & 26.0 & 20.5 & 29.5 \\ LLAMA-Mapter  & 23.0 & 28.0 & 51.0 & 30.0 & 33.0 & 53.5 & 32.5 & 33.5 & 25.5 & 21.5 & 30.5 & 29.0 & 29.2 & 41.5 & 39.5 & 25.0 & 31.5 & 22.5 & 28.0 & 32.0 & 31.7 \\ VideoMMFP-7B  & 23.5 & 26.0 & 20.0 & 22.5 & 56.0 & 20.0 & 22.0 & 23.0 & 20.0 & 31.0 & 30.5 & 25.5 & 39.5 & 48.5 & 29.0 & 33.0 & 29.5 & 26.0 & 35.5 & 32.7 \\ VideoLLAMA-7B  & 27.5 & 25.5 & 51.0 & 29.0 & 39.0 & 48.0 & 40.5 & 38.0 & 22.5 & 24.0 & 34.0 & 22.5 & 34.5 & 48.5 & 22.5 & 48.5 & 32.0 & 40.0 & 32.0 & 21.0 & 37.0 & 34.1 \\ Video-7B  & 23.5 & 26.5 & 56.0 & 33.5 & 43.5 & 40.5 & 40.0 & 35.0 & 25.5 & 27.0 & 48.5 & 35.0 & 24.5 & 46.0 & 26.5 & 41.0 & 23.5 & 33.5 & 36.0 & 35.5 \\ VideoLLAMA-7B  & 46.0 & 42.0 & 48.0 & 24.0 & 39.5 & 53.0 & 53.5 & 38.0 & 40.0 & **29.0** & 21.0 & 38.5 & **48.0** & 26.0 & 20.0 & 43.1 & **33.5** & **41.5** & 27.5 & 38.5 & 31.5 & 43.0 \\ LLAMA-VID-7B  & 45.5 & 40.5 & 58.0 & 39.5 & **55.0** & 53.5 & 40.0 & 35.5 & 18.5 & 27.5 & **87.0** & 41.5 & 23.0 & 45.5 & 41.0 & 27.0 & 40.0 & **34.5** & **41.5** & 31.5 & 41.3 \\ ShareGPT+Video-8B & **40.5** & 39.5 & **79.5** & **40.0** & 54.5 & **82.5** & **54.5** & 32.5 & **50.5** & **41.5** & 84.5 & 35.5 & **62.5** & **75.0** & **51.0** & 25.5 & **46.5** & 28.5 & 39.0 & **51.5** & **51.2** \\   

Table 5: **Comparison with SOTA methods on MVBench. * denotes our evaluation results with the public checkpoints. The best results are bold and the second-best results are underlined.**simultaneously. Audio information is beneficial in conversational scenarios involving daily human activities. We plan to introduce audio information in future work, once GPT4o supports audio input, to enhance the quality of our captions further. Additionally, the sampling interval for initial sparsification of the original videos and the window length setting in DiffSW was empirically set to 2 seconds and adjacent 2 frames based on the majority of videos. We plan to make these hyperparameters adaptive to video content in future work to handle a wider variety of video content effectively.

**Social impacts.** 1) Since the large language model involves the generation process of the large-scale captions, we have not manually verified each caption for socially biased content; 2) Although we utilize video data from existing public datasets, we cannot ensure that the selected videos do not contain human faces. Therefore, while there are no restrictions on the use of our generated captions, users must adhere to the licenses of the original video sources when using the videos. Our models can be manipulated or "jailbroken" to produce outputs that are non-inclusive or disrespectful as many LVLMs do. This vulnerability highlights the importance of continuing to improve the robustness and ethical alignment of LVLMs to prevent misuse and ensure they contribute positively to diverse applications.

## 7 Conclusion

In this study, we aim to address the challenge of lacking high-quality video-caption data for large video-language models (LVLMs) and text-to-video models (T2VMs). We develop ShareGPT4Video, a high-quality video-caption dataset, and ShareCaptioner-Video, an advanced and versatile model in the video-language multi-modal area. By employing a series of strategies and designs, we generate 40K detailed captions from advanced image multi-modal model, GPT4V, and 4.8M high-quality captions from our ShareCaptioner-Video. These captions include rich world knowledge, object attributes, camera movements, and detailed temporal descriptions of events. Our extensive experiments validate the effectiveness of our dataset and captioner in enhancing video understanding and generation tasks. We believe that ShareGPT4Video and ShareCaptioner-Video will serve as essential resources for advancing research in the LVLM and T2VM communities.

## 8 Acknowledgments

This work was supported by the Anhui Provincial Natural Science Foundation under Grant 2108085UD12. We acknowledge the partial support of the GPU cluster built by MCC Lab of Information Science and Technology Institution, USTC. This work was also partially supported by the Shanghai Artificial Intelligence Laboratory, the National Key R&D Program of China (2022ZD0160201), the Centre for Perceptual and Interactive Intelligence (CPII) Ltd under the Innovation and Technology Commission (ITC)'s InnoHK. Dahua Lin is a PI of CPII under the InnoHK.