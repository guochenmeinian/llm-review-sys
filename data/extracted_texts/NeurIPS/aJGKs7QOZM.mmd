# Mechanism design augmented with output advice

George Christodoulou

Aristotle University of Thessaloniki

and Archimedes/RC Athena, Greece

gichristo@csd.auth.gr &Alkmini Sgouritsa

Athens University of Economics and Business

and Archimedes/RC Athena, Greece

alkmini@aueb.gr &Ioannis Vlachos

Athens University of Economics and Business

and Archimedes/RC Athena, Greece

ioa.vlahos@aueb.gr

###### Abstract

Our work revisits the design of mechanisms via the _learning-augmented_ framework. In this model, the algorithm is enhanced with imperfect (machine-learned) information concerning the _input_, usually referred to as prediction. The goal is to design algorithms whose performance degrades gently as a function of the prediction error and, in particular, perform well if the prediction is accurate, but also provide a worst-case guarantee under any possible error. This framework has been successfully applied recently to various mechanism design settings, where in most cases the mechanism is provided with a prediction about the _types_ of the agents.

We adopt a perspective in which the mechanism is provided with an _output recommendation_. We make no assumptions about the quality of the suggested outcome, and the goal is to use the recommendation to design mechanisms with low approximation guarantees whenever the recommended outcome is reasonable, but at the same time to provide worst-case guarantees whenever the recommendation significantly deviates from the optimal one. We propose a generic, universal measure, which we call _quality of recommendation_, to evaluate mechanisms across various information settings. We demonstrate how this new metric can provide refined analysis in existing results.

This model introduces new challenges, as the mechanism receives limited information comparing to settings that use predictions about the types of the agents. We study, through this lens, several well-studied mechanism design paradigms, devising new mechanisms, but also providing refined analysis for existing ones, using as a metric the quality of recommendation. We complement our positive results, by exploring the limitations of known classes of strategyproof mechanisms that can be devised using output recommendation.

## 1 Introduction

Motivated by the occasionally overly pessimistic perspective of worst-case analysis, a recent trend has emerged focusing on the design and analysis of algorithms within the so-called _learning-augmented framework_ (refer to  for an overview). Within this framework, algorithms are enhanced with imperfect information about the input, usually referred to as _predictions_. These predictions can stem from machine learning models, often characterized by high accuracy, leading to exceptional performance. However, their accuracy is not guaranteed, so the predicted input may differ significantly from the actual input. Blindly relying on these predictions can have significant consequences compared to employing a worst-case analysis approach.

The framework aims to integrate the advantages of both approaches. The goal is to use these predictions to design algorithms whose performance degrades gently as a function of the inaccuracy of the prediction, known as the prediction error. In particular, they should perform well whenever the prediction is accurate -a property known as _consistency_- and also provide a worst-case guarantee under any possible error -a property known as _robustness_.

Xu and Lu  and Agrawal et al.  applied the learning-augmented framework in mechanism design settings, where there is incomplete information regarding the preferences (or types) of the participants over a set of alternatives. Traditional mechanism design addresses this information gap by devising strategyproof mechanisms that offer appropriate incentives for agents to report their true types. In the learning-augmented model, it is generally assumed that the mechanism is equipped with predictions about the types of the agents. The aim is to leverage these predicted types to design strategyproof mechanisms that provide consistency and robustness guarantees. Since then, this model has found application in diverse mechanism design settings [8; 11; 27; 25].

**Mechanisms with output advice** In this work, we propose an alternative perspective on mechanism design with predictions. We assume that the mechanism is provided with external advice to _output a specific outcome_, rather being provided with predictions of the agents' types. For example, in a job scheduling problem, the designer may receive a recommended partition of tasks for the machines, rather than a prediction about the machines' processing times. Similarly, in an auction setting, an allocation of goods is provided, rather than a prediction about the agents' valuations.

Following the tradition of the learning-augmented framework, we make no assumptions about the quality of the recommended outcome, which may or may not be a good fit for the specific (unknown) input. The goal is to use the recommendation to design a strategyproof mechanism with good approximation guarantees whenever the recommended outcome is a good fit, but at the same time provide worst-case guarantees whenever the recommendation deviates from the optimal one.

We observe that one can reinterpret previous models within the framework of our model, viewing it as a more constrained version of predictions with _limited information_.1Since we only require limited information regarding the outcome, our model may be better suited to handle cases where historical input data is absent or limited, which may occur for various reasons such as privacy concerns, data protection, challenges in anonymizing, or simply because the information is missing. For instance, historical data in an auction may sometimes only contain information about the winners and perhaps the prices, omitting details about their exact valuation or the values of those who lost. Additionally, our model may be applied in cases where the designer does not need to know the specifics of the algorithm and treats it as a black box, as long as it yields satisfactory allocations, even if the inner workings are not fully understood.

We make no assumption about _how_ the outcome recommendation was produced, which makes it quite general and adaptable to different application domains. For instance, the outcome may represent the optimal allocation with respect to predicted data (as seen in ), or a solution generated by an approximation algorithm or a heuristic. Consequently, the quality of the recommended outcome may be affected by various factors, such as the accuracy of the predicted data or limited computational resources which prevent the computation of optimal solutions, even when the data is accurate.

A beneficial side effect of our model is that an outcome recommendation fits in a plug-and-play fashion with a generic machinery for strategyproofness in multi-dimensional mechanism design, particularly maximal in range VCG mechanisms (or more generally with affine maximizers) in a straightforward manner: we simply add the recommended outcome to the range of the affine maximizer (see Section 5).

**Quality of recommendation** In the learning augmented framework, the performance of an algorithm (or mechanism) is evaluated based on the _prediction error_, which quantifies the disparity between the predicted and actual data. Unfortunately, there is no universal definition for such an error; it is typically domain-specific (e.g., the ratio of processing times for scheduling [27; 8] or (normalized) geometric distance for facility location ). Therefore, if one modifies the information data model for a specific problem--for instance, by assuming that only a fraction or a signal of the predicted data is provided--it becomes necessary to redefine the prediction error.

To address this issue, we propose a generic, universal measure that can be applied to analyze algorithms across various information settings and application domains. We define the _quality of recommendation_ as the approximation ratio between the cost (or welfare) of the recommended outcome and the optimal cost (or welfare) both evaluated w.r.t the actual input. It is worth emphasizing that although the above definition aligns naturally with our information model, as we do not assume the designer is provided with predicted data, it can also be applied to richer information models with partial or even full predicted input.

We argue that it provides a unified metric for settings involving predictions, particularly when the objective is to design mechanisms (or more generally algorithms) with low approximation or competitive ratio. The disparity between predicted and actual data, captured by the predicted error, may not always be relevant and can lead to misleading evaluations; there are cases where this error may be significantly large, but the optimal solution remains largely unchanged. For example, consider the problem of makespan minimization in job scheduling (see also Section 3 for a detailed example in facility location). In , the prediction error used is the maximum ratio of processing times, and it appears in the approximation guarantees. There are simple instances where this ratio is arbitrarily large, but the optimal allocation remains the same. Consequently, when the prediction error is incorporated into the analysis, it may lead to overly pessimistic guarantees for mechanisms that perform much better (see Section 3). Our metric avoids such pathological situations.

### Contributions

We propose studying mechanisms augmented with output advice, a setup that utilizes limited information to provide improved approximation guarantees. Additionally, we introduce a unified metric that can provide more accurate evaluations, even for settings with richer information models. We explore the limitations of the class of strategyproof mechanisms that can be devised using this limited information across various mechanism design settings. Detailed results concerning the house allocation problem can be found in the full version of the paper. Table 1 summarizes our results.

**Facility Location** In the facility location problem, there are \(n\) agents each with a preferred location and the goal is to design a strategyproof mechanism that determines the optimal facility location based on an objective. In Section 3, we derive new approximation bounds for the facility location problem revisiting the Minimum Bounding Box and the Coordinatewise Median mechanisms defined in , as a function of the quality of recommendation. We provide tight bounds, and demonstrate that in some cases they outperform previous analysis with the use of a prediction error.

**Scheduling** In Section 4, we study a scheduling problem with unrelated machines, where each machine has a cost for each job, which corresponds to the processing time of the job on the machine. Each job is assigned to exactly one machine, and the goal is to minimize the makespan having an output allocation as a recommendation. We devise a new strategyproof mechanism (Mechanism 1), that takes also as input a confidence parameter \([1,n]\), reflecting the level of trust in the recommendation. We show that this mechanism is \((+1)\)-consistent and \(}{}\)-robust (Theorem 3). Altogether, we obtain a \(\{(+1),n+,}{}\}\) upper bound on the approximation ratio, where \(\) is the quality of the recommendation, that we show that is asymptotically tight (Theorem 4). We complement this positive result, by showing that, given only the outcome as advice, it is impossible to achieve a better consistency-robustness trade-off in the class of the weighted VCG mechanisms (Theorem 5).

**Combinatorial Auctions** Next, we study combinatorial auctions given a recommended allocation (see Section 5). In the combinatorial auctions setting, there is a set of \(m\) indivisible objects to be sold to \(n\) bidders, who have private values for each possible bundle of items. We observe that our advice model fits nicely with the maximal in range VCG mechanisms or more generally with the affine maximizers, by preserving strategyproofness. These mechanisms provide the best known bounds for the approximation of the maximum social welfare for several classes of valuations . By including the recommended outcome in the range of the affine maximizer, we immediately obtain \(1\)-consistency, while maintaining the robustness guarantees of those mechanisms.

**House Allocation** Finally, we switch to the house allocation problem. In this problem, we aim to assign \(n\) houses to a set of \(n\) agents in a way that ensures strategyproofness and maximizes the social welfare. We use the TTC mechanism  with the recommendation as an initial endowment, and prove that this is \(\{,n\}\)-approximate for unit-range valuations and \(\{,n^{2}\}\)-approximatefor unit-sum valuations, where \(\) is the quality of recommendation. Finally, we prove it is optimal among strategyproof, neutral and nonbossy mechanisms using the characterization of  and the correspondence between serial dictator mechanisms and TTC mechanisms .

### Related Work

Learning-augmented mechanism designRecently, there has been increased interest in leveraging predictions to improve algorithms' worst case guarantees. The influential framework of Lykouris and Vassilvitskii  is applied on caching, formally introducing the notions of consistency and robustness, under minimal assumptions on the machine learned oracle. The learning-augmented framework is naturally brought to the algorithmic mechanism design field by  and  independently. Agrawal et al.  design learning-augmented strategyproof mechanisms for the problem of facility location with strategic agents. Xu and Lu  apply the algorithmic design with predictions framework on revenue-maximizing single-item auctions, frugal path auctions, scheduling, and two-facility location. Another version of the facility location problem, obnoxious facility location, is studied by Istrate and Bonchis . Prasad et al.  develop a new methodology for multidimensional mechanism design that uses side information with the dual objective of generating high social welfare and high revenue. Strategyproof scheduling of unrelated machines is studied in , achieving the best of both worlds using the learning-augmented framework. Revenue maximization is also considered in  in the online setting, while Lu et al.  study competitive auctions with predictions. Caragiannis and Kalantzis  assume that the agent valuations belong to a known interval and study single-item auctions with the objective of extracting a large fraction of the highest agent valuation as revenue. Other settings enhanced with predictions include the work of Gkatzelis et al. , where predictions are applied to network games and the design of decentralized mechanisms in strategic settings. In , the scenario includes a set of candidates and a set of voters, and the objective is to choose a candidate with minimum social cost, given some prediction of the optimal candidate.

Facility LocationFor single facility location on the line, the mechanism that places the facility on the median over all the reported points is strategyproof and optimal for the utilitarian objective, and it achieves a 2-approximation for the egalitarian social cost, which is the best approximation achievable by any deterministic and strategyproof mechanism . In the two-dimensional Euclidean space, the Coordinatewise Median mechanism achieves a \(\)-approximation for the utilitarian objective , and a 2-approximation for the egalitarian objective ; these approximation bounds are both optimal among deterministic and strategyproof mechanisms. In , they consider as a prediction the position of the facility to improve the above results. Concerning the egalitarian social cost and the two-dimensional version of the problem, they achieve perfect consistency, and a robustness of \(1+\). They also prove that their mechanism provides an optimal trade-off between robustness and consistency. Regarding the utilitarian social cost in two dimensions, they propose a deterministic mechanism achieving \(+2}}{1+}\)-consistency, \(+2}}{1-}\)-robustness and optimal trade-off among deterministic, anonymous, and strategyproof mechanisms.

SchedulingChristodoulou et al.  validated the conjecture of Nisan and Ronen, and proved that the best approximation ratio of deterministic strategyproof mechanisms for makespan minimization for \(n\) unrelated machines is \(n\). Even if we allow randomization, the best known approximation guarantee achievable by a randomized strategyproof mechanism is \(O(n)\). Following the prediction framework, Xu and Lu  study the problem with predictions \(_{ij}\) denoting the predicted

 
**Problem** & **Cons** & **Rob** & \(f(,)\)**-approximation** \\  Facility Location (egalitarian) & 1  & 1+\(\) & \(\{,1+\}\) \\  Facility Location (utilitarian) & \(+2}}{1+}\) & \(+2}}{1-}\) & \(\{,+,+2}}{1- }\}\) \\  Scheduling & \(+1\) & \(}{^{*}}\) & \(\{(+1),n+,}{}\}\) \\  Combinatorial Auctions & 1 & \(_{M}\) & \(\{,_{M}\}\) \\  House Allocation & 1 & \(n\) (or \(n^{2}\)) & \(\{,n\) (or \(n^{2}\))\} \\  

Table 1: Contribution Results. Consistency, robustness and approximation results proved for the mechanism design problems augmented with _output_ advice. In the house allocation problem, bounds are shown for unit-range valuations, while the ones in parentheses are for unit-sum valuations. In combinatorial auctions, \(_{M}\) is the approximation ratio guarantee of a maximal in range mechanism.

processing time of job \(j\) by machine \(i\). They propose a deterministic strategyproof mechanism with an approximation ratio of \(O(\{^{2},}{^{2}}\})\), where \([1,m]\) is a configurable consistency parameter and \( 1\) is the prediction error. Balkanski et al.  extend these results by identifying a deterministic strategyproof mechanism that guarantees a constant consistency with a robustness of \(2n\), achieving the best of both worlds.

**Combinatorial Auctions**  An important direction in combinatorial auctions related to our work is the design of strategyproof mechanisms that approximate the optimal social welfare using polynomially many queries, see e.g. [17; 24; 19; 18]. Auctions incorporating predictions have been explored across various settings such as revenue maximization auctions [11; 39], competitive auctions  and the online setting . It is noteworthy that the design of strategyproof, near-optimal auctions using neural networks [20; 36] has been studied extensively for automated mechanism design.

**House Allocation**  Regarding the house allocation problem, Filos-Ratsikas et al.  prove that a randomized mechanism, called the Random Priority Mechanism, has approximation ratio of \(()\), and that this is optimal among all strategyproof mechanisms. There exist lower bounds for all deterministic strategyproof mechanisms which are \((n^{2})\) for unit-sum and \((n)\) for unit-range, respectively. To the best of our knowledge there is no single point of reference, for these bounds, but can follow from known results in the literature, after observing that deterministic strategyproof mechanisms are ordinal, see [14; 4]. A lower bound of \((n^{2})\) on the _Price of Anarchy_ for any deterministic mechanism (not necessarily strategyproof) is proved in . In , a \((n^{2})\) bound is proved for the distortion of all _ordinal_ deterministic mechanisms.

## 2 Model

We consider various mechanism design scenarios that fall into the following abstract mechanism design setting. There is a set of \(n\) agents and a (possibly infinite) set of alternatives \(\). Each agent \(i\{1,,n\}\) can express their preference over the set of alternatives via a valuation function \(t_{i}\) which is private information known only to them (also called the _type_ of agent \(i\)). The set \(_{i}\) of possible types of agent \(i\) consists of all functions \(b_{i}:\). Let also \(=_{i N}_{i}\) denote the space of type profiles.

A mechanism defines for each agent \(i\) a set \(_{i}\) of available strategies the agent can choose from. We consider _direct revelation_ mechanisms, i.e., \(_{i}=_{i}\) for all \(i\), meaning that the agents' strategies are to simply report their types to the mechanism. Each agent \(i\) provides a _bid_\(b_{i}_{i}\), which may not match their true type \(t_{i}\), if this serves their interests. A mechanism \((f,p)\) consists of two parts:

**A selection algorithm:**  The selection algorithm \(f\) selects an alternative based on the agents' inputs (bid vector) \(b=(b_{1},,b_{n})\). We denote by \(f()\) the alternative chosen for the bid vector \(=(b_{1},,b_{n})\).

**A payment scheme:**  The payment scheme \(p=(p_{1},,p_{n})\) determines the payments, which also depend on the bid vector \(\). The functions \(p_{1},,p_{n}\) represent the payments that the mechanism hands to each agent, i.e., \(p_{i}:\).

The _utility_\(u_{i}\) of an agent \(i\) is the _actual_ value they gain from the chosen alternative minus the payment they have to pay, \(u_{i}()=t_{i}(f())-p_{i}()\). We consider _strategyproof_ mechanisms. A mechanism is strategyproof, if for every agent, reporting their true type is a _dominant strategy_. Formally,

\[u_{i}(t_{i},_{-i}) u_{i}(t^{}_{i},_{-i}),  i[n],\ \ t_{i},t^{}_{i}_{i},\ \ _{-i} _{-i},\]

where \(_{-i}\) denotes all parts of \(\) except its \(i\)-th part.

In some of our applications (e.g. facility location and scheduling settings), it is more natural to consider that the agents are cost-minimizers rather than utility-maximizers. Therefore, for convenience we will assume that each agent \(i\) aims to minimize a cost function rather than maximizing a utility function. We stress that some of our applications (e.g. facility location, one-sided matching) fall into mechanism design without money, In those cases we will assume \(p_{i}()=0,\) and \(i[n]\).

**Social objective**  We assume that there is an underlying objective function that needs to be optimized. We consider both _cost minimization_ social objectives (facility location in Section 3, scheduling in Section 4) and _welfare maximization_ (house allocation in Section 2.2, auctions in Section 5). In the context of a cost minimization problem, we assume that we are given a social cost function \(C:_{+}\). If all agents' types were known, then the goal would be to select the outcome \(a\) that minimizes \(C(,a)\).

The quality of a mechanism for a given type vector \(\) is measured by the cost \(()\) achieved by its selection algorithm \(f\), \(()=C(,f())\), which is compared to the optimal cost \(()=_{a}C(,a)\). We denote an optimal alternative for a given bid vector \(\) by \(a^{*}\).

In most application domains, it is well known that only a subset of algorithms can be selection algorithms of strategyproof mechanisms. In particular, no mechanism's selection algorithm is optimal for every \(t\), prompting a natural focus on the approximation ratio of the mechanism's selection algorithm. A mechanism is \(\)_-approximate_, for some \( 1\), if its selection algorithm is \(\)-approximate, that is, if \(()}{()}\) for all possible inputs \(\).

**Mechanisms with advice** We assume that in addition to the input bid \(\), the mechanism is also given as a recommendation/advice, a predicted alternative \(\), but without any guarantee of its quality2. A natural requirement, known as _consistency_, requires that whenever the recommendation is accurate, then the mechanism should achieve low approximation. A mechanism is said to be \(\)-consistent if it is \(\)-approximate when the prediction is accurate, that is, the predicted outcome \(\) is optimal for the given \(\) vector. On the other hand, if the prediction is poor, _robustness_ requires that the mechanism retains some reasonable worst-case guarantee. A mechanism is said to be \(\)-robust if it is \(\)-approximate for all predictions:

\[_{}(,a^{*})}{()}\,;_{,}( ,)}{()}\,.\]

In order to measure the quality of the prediction, we define the _recommendation error_, denoted by \(\), as the approximation ratio of the recommended outcome cost to the optimal one i.e., \(=,)}{()}\).

In some of our applications, the social objective is a welfare maximization problem, where there is an underlying welfare function \(W:_{+}\) that needs to be maximized. We adapt our definitions for approximation and for the prediction error accordingly. In particular, the quality of a mechanism for a given type vector \(\) is measured by the welfare \((,)=W(,f(,))\), which is compared to the optimal welfare \(()=_{a}W(,a)\). A mechanism is \(\)-approximate, if \(()}{()}\) for all possible inputs \(\). Consistency and robustness are defined similarly to the cost minimization version, while the recommendation error is defined as the approximation ratio \(=()}{W(,)}\). Note that for both versions, the quality of recommendation \(\) exceeds \(1\), with \(1\) indicating perfect quality and higher values indicating poorer quality. Additionally, we require a smooth decay of the approximation ratio as a function of the quality of the recommendation as it moves from being perfect to being arbitrarily bad. We say that an algorithm is _smooth_ if its approximation ratio degrades at a rate that is at most linear in \(\)[5; 6; 33].

## 3 Facility Location

In this section, we study mechanisms for the facility location problem in the two-dimensional Euclidean space. There are \(n\) agents each with a preferred (private) location \(z_{i}=(x_{i},y_{i}),1 i n\) in \(^{2}\). The goal of the mechanism is to aggregate the preferences of the agents and determine the optimal facility location at a point \(f()\) in \(^{2}\). Given a facility at point \(a^{2}\), the private cost \(t_{i}(a)\) of each agent is measured by the distance of \(z_{i}\) from \(a\), i.e., \(t_{i}(a)=d(z_{i},a)\), and the private objective of each agent is to minimize their cost. Two different social cost functions have been used to evaluate the quality of a location \(a\); the _egalitarian cost_, which measures the maximum cost incurred by \(a\) among all agents \(C(,a)=_{i}t_{i}(a)\), and the _utilitarian_ cost, which considers the sum of the individual costs i.e., \(C(,a)=_{i}t_{i}(a)\).

We assume that the mechanism is equipped with a recommended point \(^{2}\). This is perceived as a recommendation to place the facility at \(\). For a given \(\) we denote by \(a^{*}()\) the optimal location minimizing the social cost, and by \(()\) the quality of the recommended outcome, which is defined as the approximation ratio \(C(,)/()\) and measures the approximation that would by achieved by placing the facility at \(\). We use the simpler notation \(a^{*}\) and \(\) when \(\) is clear from the context.

We note that for this problem our model coincides with the model studied in  for facility location problems, although our perspective is slightly different. Their paper considers that the missing information is the type of the agents, and they assume that they receive a _signal of the predicted input_\(\), the optimal location w.r.t. the predicted types. Due to this perspective, they defined as _prediction error_ the (normalized) distance of their prediction, comparing to the optimal solution w.r.t the actual types. We perceive \(\) as an output advice. Clearly, one can interpret the output as a signal of some sort of predicted data. However, we treat the advice as a recommendation, with unknown quality, and under this perspective in the context of this paper, it makes more sense to measure it by the approximation ratio w.r.t the actual (but unknown) input.

We showcase this effect in the following example of the facility location problem in the line for the utilitarian social cost, and we further discuss it in Section 3.3. Consider \(2m-1\) agents, see Figure 1, whose preferred locations are clustered in two different points, the one at position \((0,0)\) and the other at position \((1,0)\), where the first point is preferred by \(m\) agents and the other is preferred by \(m-1\) agents. The solution \(a^{*}\) that minimizes the social cost places the facility at point \((0,0)\) (preferred by \(m\) agents) resulting in a total cost of \(()=m-1\). Now, take two different recommendations \(_{1}\) and \(_{2}\) at points \((-1,0)\) and \((1,0)\) respectively. The prediction error is the same for both points and it is equal to \(\). However, any recommendation between \(a^{*}\) and \(_{2}\) is almost optimal for large \(m\), in contrast to \(_{1}\). The quality of the recommendation captures this difference: the social cost for the two recommendations are \(C(_{1})=3m-2\) and \(C(_{2})=m\), and therefore the quality of the recommendation for \(_{1}\) and \(_{2}\) are respectively \(_{1}=\) and \(_{2}=\), which converge to \(3\) and \(1\) respectively as \(m\) grows.

In Section 3.1, we study the egalitarian cost and show that the Minimum Bounding Box Mechanism, defined by Agrawal et al. , achieves an approximation ratio of \(\), which combined with the robustness bound of  gives an overall approximation guarantee of \(\{,+1\}\). In Section 3.2 we focus on the utilitarian cost and show that the Coordinatewise Median Mechanism with predictions, defined in , achieves an approximation ratio of at most \(\) which combined with the robustness bound of  gives an overall approximation guarantee of \(\{,+2}}{1-}\}\), where \([0,1)\) is a parameter that models the confidence of the designer on the recommendation; larger values of \(\), correspond to increased confidence about the advice. Finally, in Section 3.3 we compare the bounds obtained as a function of \(\) to previously known results obtained as a function of the prediction error.

### Egalitarian Cost

The main result of this section is an approximation ratio of \(\) for the egalitarian cost, by analyzing the Minimum Bounding Box mechanism defined in . The robustness result for this mechanism , gives a total approximation ratio of \(\{,+1\}\), which we prove that is tight in the full version.

Intuitively, the Minimum Bounding Box mechanism works as follows3: If the minimum rectangle that contains all the input points \(z_{i},i\{1,,n\}\), contains the recommendation point \(\), then we output \(\). Otherwise, we select the boundary point with the minimum distance from \(\).

**Theorem 1**.: _The Minimum Bounding Box mechanism is \(\{,+1\}\)-approximate._

Proof.: \[(,)=_{i}d(z_{i},f(,)) C (,)=()\]

The inequality holds because, whenever the prediction is outside the minimum bounding box, the mechanism projects the prediction on its boundaries, in a way that improves the egalitarian loss compared to the initial prediction. When the prediction is inside the bounding box, then \(f(,)=\)

Figure 1: Quality of recommendation versus prediction error

and the inequality holds with equality. The term \((+1)\) follows from the robustness guarantee proved in . By selecting the minimum of the two bounds, we get the approximation above. 

**Remark 1**.: _We remark that when \(f(,)=\), the upper bound of \(\) is tight. In practice, this happens whenever the recommendation is inside the minimum bounding box defined by the agents' locations._

### Utilitarian Cost

Next, we show a \(\) upper bound for the utilitarian cost by using the Coordinatewise Median with predictions mechanism defined in . This mechanism specifies a parameter \([0,1)\) which models how much the recommendation is trusted. Intuitively,3 the mechanism works as follows; it creates \( n\) copies of the recommendation \(=(x_{},y_{})\). Then, by treating each coordinate separately, it selects the median point among \(n+ n\) in total points; the \(n\) actual bids \(z_{i}=(x_{i},y_{i})\) and the \( n\) copies of the recommendation. After calculating the medians \(x_{a}\) and \(y_{a}\) for each coordinate, it defines the outcome to be \(f(,)=(x_{a},y_{a})\). In the full version of the paper, we show that our analysis is tight.

**Theorem 2**.: _The Coordinatewise Median with Predictions mechanism is \(\{,+,+2}}{1- }\}\)-approximate._

### Comparison of Error Functions

In this section, we compare the quality of recommendation \(\) to the error \(\) defined in  and find instances for which our bounds are tight while previous known bounds are not. We first establish that \(+1\) holds for both the egalitarian and the utilitarian objective. We then show that for both objectives, there exist instances that our bounds are strictly better than the ones proved in .

**Lemma 1**.: _For the egalitarian social cost, there exists an instance where \(<+1\)._

**Lemma 2**.: _For the utilitarian social cost, there exists an instance where \(<+2}}{1+}+\)_

We give all the proofs in the full version of the paper. Note that for the egalitarian objective, our bound is a refinement of the (tight) bound \(+1\) from . On the other hand, for the utilitarian objective, there exist instances for which the \(+2}}{1+}+\) bound of  is better than ours. For this reason, in the full version of the paper we observe the behaviour of \(\), \(\) in real-world datasets [26; 7; 37; 12; 16; 3].

## 4 Scheduling

In this section, we study strategyproof mechanisms for the _makespan minimization scheduling problem_. In this problem, we have a set \(N\) of \(n\) unrelated machines (the agents) and a set \(M\) of \(m\) jobs. Each machine \(i\) has a (private) cost \(t_{ij}\) for each job \(j\), which corresponds to the processing time of job \(j\) in machine \(i\). Since we consider only strategyproof mechanisms, each machine \(i\) declares their _true_ cost \(t_{ij}\) for each job \(j\); let \(t_{i}=(t_{i1},,t_{im})\). The goal of the mechanism is to process the machines' declarations \(=(t_{1},,t_{n})\) and subsequently determine both an allocation \(a()\) of the jobs to the machines and a payment scheme \(p()=(p_{1}(),p_{n}())\), where \(p_{i}()\) is given to each machine \(i\) for processing their allocated jobs. An allocation is given by a vector \(a=(a_{1},,a_{n})\), where \(a_{i}=(a_{i1},,a_{im})\), and \(a_{ij}\) is set to 1 if job \(j\) is assigned to machine \(i\) and 0 otherwise. An allocation \(a\) is feasible if each job is allocated to exactly one machine, i.e., \(_{i N}a_{ij}=1\), for all \(j M\), and \(_{i N,j M}a_{ij}=m\); we denote by \(\) the set of all feasible allocations.

The cost experienced by each machine \(i\) under an allocation \(a\) is the total cost of all jobs assigned to it: \(t_{i}(a)=t_{i}(a_{i})=_{j M}t_{ij}a_{ij}=t_{i} a_{i}\). The private objective of each machine \(i\) is to maximize their utility \(u_{i}()=p_{i}()-t_{i}(a())\). In the strategyproof mechanisms that we consider here, this happens when each machine declares its true cost. The social cost function that is usually used in this problem in order to evaluate the quality of an allocation \(a\), is the maximum cost among all machines, which is known as the makespan: \(C(,a)=_{i}t_{i}(a)\).

We assume that the mechanism is provided with a recommendation \(\), which can be seen as a suggestion on how to allocate the jobs to the machines. For a given \(\) we denote by \(a^{*}()\) the optimal allocation minimizing the social cost function, i.e., \(a^{*}()_{a}(,a)\), and by \(()\) the minimum social cost, i.e., \(()=C(,a^{*}())\). We measure the quality of the recommended outcome with \(()\), which is defined as the approximation ratio \(C(,)/()\) and measures the approximation that we would achieve if we selected the recommended allocation \(\). In the notation of \(a^{*}\) and \(\), we drop the dependency on \(\) when it is clear from the context.

In the remainder of this section, we introduce a strategyproof mechanism that we call Allocation-ScaledGreedy (Mechanism 1). We prove that, given a confidence parameter \(1 n\), it exhibits \((+1)\)-consistency and \(}{}\)-robustness (Theorem 3). Next, we investigate the smoothness of this mechanism and demonstrate that its approximation ratio is upper bounded by \(\{(+1),n+,}{}\}\), which is asymptotically tight (Theorem 4). Furthermore, we establish that, when provided with the outcome as advice, it is impossible to achieve a better consistency-robustness trade-off than the AllocationScaledGreedy mechanism within the class of weighted VCG mechanisms (Theorem 5).

### AllocationScaledGreedy Mechanism

In this subsection, we introduce a strategyproof mechanism called AllocationScaledGreedy, which achieves a \((+1)\)-consistency (more precisely, \((+1)\)-consistency which converges to \(+1\) for large \(n\)) and a \(}{}\)-robustness, where \(\) is a confidence parameter ranging from \(1\) to \(n\), with \(1\) corresponding to full trust and \(n\) corresponding to mistrust. For \(=n\), which can be interpreted as ignoring the recommendation, the AllocationScaledGreedy mechanism corresponds to the VCG mechanism; in that case, consistency and robustness bounds coincide, giving an \(n\)-approximation (same as VCG). Regarding the smoothness of our mechanism, we prove an asymptotically tight approximation ratio of \(\{(+1),n+,}{}\}\).

AllocationScaledGreedyThe mechanism sets a weight \(r_{ij}\) for every machine \(i\) and every job \(j\) based on the recommendation \(\). \(r_{ij}\) is set to 1 wherever \(_{ij}=1\), and \(\) wherever \(_{ij}=0\), for some \([1,n]\). It then decides the allocation by running the weighted VCG mechanism for each job \(j\) separately, and by using \(r_{ij}\) as the (multiplicative) weight of machine \(i\), i.e., each job \(j\) is allocated to some machine in \(_{i}\{r_{ij}t_{ij}\}\) that we denote by \(i_{j}\).

```
1:instance \(^{n m}\), recommendation \(^{n m}\)
2:\(a\)
3:\(r_{ij} 1\) if \(_{ij}=1\), \(\) otherwise, \(([1,n])\)
4:\(i_{j}_{i}\{r_{ij}t_{ij}\}\)
5:if \(i=i_{j}\) then \(a_{ij}=1\) else \(a_{ij}=0\), for each \((i,j) N M\)
```

**Mechanism 1** The AllocationScaledGreedy mechanism

**Remark 2**.: _We remark that the AllocationScaledGreedy mechanism for \(=1\) is a simplification of the SimpleScaledGreedy mechanism of . In , it is assumed that the mechanism is equipped with predictions of the entire cost matrix \(_{ij}\), for every machine-job pair. The SimpleScaledGreedy mechanism utilizes this information to define weights \(r_{ij}\) that may take values in the range \([1,n]\). In contrast, AllocationScaledGreedy uses weights with values only \(1\) or \(n\), for \(=1\). Notably, despite the limited information available to AllocationScaledGreedy, both mechanisms share the same consistency and robustness, but SimpleScaledGreedy lacks the nice property of being smooth, as for a very small prediction error, the approximation ratio has a large discontinuity gap (see full version for an example) as opposed to AllocationScaledGreedy (Theorem 4). SimpleScaledGreedy served as an intermediate step in  in the design of the more sophisticated mechanism ScaledGreedy, (which again relies heavily on the prediction of the entire cost matrix) which achieves the best of both worlds, constant consistency and \(O(n)\)-robustness. However, for similar reasons, ScaledGreedy is not smooth either._

**Theorem 3**.: _The AllocationScaledGreedy mechanism is \((+1)\)-consistent and \(}{}\)-robust._

In the following theorem, we show the smoothness result for the AllocationScaledGreedy mechanism; we show a tight approximation ratio depending on \(\). We prove this theorem in the lemmas. In the first one, we show that \(\{(+1),n+,}{}\}\) is an upper bound, and in the second one that \(\{,-1}{2},-1}{2}\}\) is a lower bound on the approximation ratio of the AllocationScaledGreedy mechanism. We defer the reader to the full version for the complete proof.

**Theorem 4**.: _The AllocationScaledGreedy mechanism is at most \(\{(+1),n+,}{}\}\)-approximate and this bound is asymptotically tight._

### Mechanism Optimality

In this subsection, we provide general impossibility results for the class of weighted VCG mechanisms4, the most general known class of strategyproof mechanisms for multi-dimensional mechanism design settings, such as the scheduling problem. We prove that it is impossible to improve upon the AllocationScaledGreedy mechanism, given the recommended outcome. More specifically, there is no weighted VCG mechanism with \(\)-consistency that can achieve a robustness better than \((}{})\), highlighting the optimality of AllocationScaledGreedy in this class of mechanisms.

**Theorem 5**.: _Given any recommendation \(\), any weighted VCG mechanism that is \(\)-consistent, must also be \((}{})\)-robust, for any \(2 n\)._

Proof sketch.: We provide a proof sketch of Theorem 5 and refer the reader to the full version for the complete proof. We will consider instances with \(n\) machines and \(n^{2}\) jobs. Let a \(\)-consistent weighted VCG mechanism and a recommendation \(\) that assigns every \(n\) jobs to a distinct machine. Focusing on each machine \(i\), we specify the cost vector \(\), such that the optimal allocation matches \(\). The costs are such that the mechanism must assign each job \(j\) either to machine \(i\) or to machine \(_{j}\) that receives job \(j\) in \(\). Machine \(i\) should not receive many jobs, otherwise \(\)-consistency is violated. Consequently, there are many (approximately \(}{2}\)) weights \(r_{ij}\) with value much higher comparing to the weight \(r_{i,j}\), i.e., \(}{r_{ij}}\).

Since this is true for each machine \(i\), there exists a machine \(\), such that, focusing only on the \(n\) jobs that \(\) receives in \(\), there exist approximately \(}{2}\) jobs with value much higher (comparing to \(\)) among all machines. Then it holds that we can assign approximately \(\) jobs to distinct machines such that those machines have high-valued weight for their assigned job; let \(J\) be the set of those jobs. We finally consider the instance where each of those machines has a cost of \(1\) for their assigned job and sufficiently high cost5 for any other job in \(J\), machine \(\) has a cost slightly less than \(\) for jobs in \(J\), and all other machines have infinite cost for jobs in \(J\). The cost for any other job that does not belong to \(J\) is \(0\) for any machine. In this instance \(\), \(()=1\), but the mechanism allocates all jobs of \(J\) to machine \(\), resulting in \((,)\) being approximately \(}{4}\). Hence, any \(\)-consistent weighted VCG mechanism is \((}{})\)-robust.

## 5 Combinatorial Auctions

In this section, we show how output advice can integrate with truthful maximal in range (MIR) mechanisms where the goal is to optimize the social welfare (or more generally an affine function) over a restricted outcome space. Let \(M\) be a MIR mechanism with an approximation guarantee \(_{M}\). We define a mechanism that compares the outcome of \(M\) with a suggested solution \(\) and selects the one that achieves the highest social welfare. This mechanism remains MIR, as it simply expands the range of possible outcomes to include \(\), ensuring it remains strategyproof, and is \(min\{,_{M}\}\)-approximate. Combining with the results of [34; 17] we obtain strategyproof mechanisms for combinatorial auctions with approximation ratio of \(\{,}{{ m}}\}\) for general valuations, \(\{,}{{ m}}}\}\) for subadditive valuations, and \(\{,2\}\) for multi-unit valuations.