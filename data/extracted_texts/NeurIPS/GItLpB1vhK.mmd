# Estimating Koopman operators with sketching to provably learn large scale dynamical systems

Giacomo Meanti

Istituto Italiano di Tecnologia

giacomo.meanti@iit.it

&Antoine Chatalic

Universita di Genova

antoine.chatalic@dibris.unige.it

&Vladimir R. Kostic

Istituto Italiano di Tecnologia

University of Novi Sad

vladimir.kostic@iit.it

&Pietro Novelli

Istituto Italiano di Tecnologia

pietro.novelli@iit.it

&Massimiliano Pontil

Istituto Italiano di Tecnologia

University College London

massimiliano.pontil@iit.it

&Lorenzo Rosasco

Universita di Genova

Istituto Italiano di Tecnologia

Massachusetts Institute of Technology

lrosasco@mit.edu

Equal contribution

###### Abstract

The theory of Koopman operators allows to deploy non-parametric machine learning algorithms to predict and analyze complex dynamical systems. Estimators such as principal component regression (PCR) or reduced rank regression (RRR) in kernel spaces can be shown to provably learn Koopman operators from finite empirical observations of the system's time evolution. Scaling these approaches to very long trajectories is a challenge and requires introducing suitable approximations to make computations feasible. In this paper, we boost the efficiency of different kernel-based Koopman operator estimators using random projections (sketching). We derive, implement and test the new "sketched" estimators with extensive experiments on synthetic and large-scale molecular dynamics datasets. Further, we establish non asymptotic error bounds giving a sharp characterization of the trade-offs between statistical learning rates and computational efficiency. Our empirical and theoretical analysis shows that the proposed estimators provide a sound and efficient way to learn large scale dynamical systems. In particular our experiments indicate that the proposed estimators retain the same accuracy of PCR or RRR, while being much faster. Code is available at https://github.com/Giodiro/NystromKoopman.

## 1 Introduction

In the physical world, temporally varying phenomena are everywhere, from biological processes in the cell to fluid dynamics to electrical fields. Correspondingly, they generate large amounts of data both through experiments and simulations. This data is often analyzed in the framework of dynamical systems, where the state of a system \(\) is observed at a certain time \(t\), and the dynamics is described by a function \(f\) which captures its evolution in time

\[_{t+1}=f(_{t}).\]The function \(f\) must capture the whole dynamics, and as such it may be non-linear and even stochastic for instance when modeling stochastic differential equations, or simply noisy processes. Applications of this general formulation arise in fields ranging from robotics, atomistic simulations, epidemiology, and many more. Along with a recent increase in the availability of simulated data, data-driven techniques for learning the dynamics underlying physical systems have become commonplace. The typical approach of such techniques is to acquire a dataset of training pairs \((_{t},_{t}=_{t+1})\) sampled in time, and use them to learn a model for \(f\) which minimizes a forecasting error. Since dynamical systems stem from real physical processes, forecasting is not the only goal and the ability to interpret the dynamics is paramount. One particularly important dimension for interpretation is the separation of dynamics into multiple temporal scales: fast fluctuations can e.g. be due to thermodynamical noise or electrical components in the system, while slow dynamics describe important conformational changes in molecules or mechanical effects.

Koopman operator theory  provides an elegant framework in which the potentially non-linear dynamics of the system can be studied via the Koopman operator

\[()()=(f()),\] (1)

which has the main advantage of being linear but is defined on a typically infinite-dimensional set of observable functions. The expectation in (1) is taken with respect to the potential stochasticity of \(f\). Thanks to its linearity, the operator \(\) can e.g. be applied twice to get two-steps-ahead forecasts, and one can compute its spectrum (beware however that \(\) is not self-adjoint, unless the dynamical process is time-reversible). Accurately approximating the Koopman operator and its spectral properties is of high interest for the practical analysis of dynamical systems. However doing so efficiently for long temporal trajectories remains challenging. In this paper we are interested in designing estimators which are both theoretically accurate and computationally efficient.

Related worksLearning the spectral properties of the Koopman operator directly from data has been considered for at least three decades , resulting in a large body of previous work. Among the different approaches proposed over time (see Mezic  for a recent review) it is most common to search for finite dimensional approximations to the operator, from which part of the spectrum and the Koopman modes  can be obtained. Dynamic mode decomposition (DMD) , time-lagged independent component analysis (tICA)  and many subsequent extensions  for example can be seen as minimizers of the forecasting error when \(\) is restricted to be a linear function of the states . Extended DMD (eDMD)  and the variational approach for conformation dynamics (VAC)  instead allow for a (potentially learnable, as in recent deep learning algorithms ) dictionary of non-linear functions \(\). Kernel DMD  and kernel tICA  are further generalizations which again approximate the Koopman operator but using an infinite dimensional space of features \(\), encoded by the feature map of a reproducing kernel. While often slow from a computational point of view, kernel methods are highly expressive and can be analyzed theoretically, to prove convergence and derive learning rates of the resulting estimators . Approximate kernel methods which are much faster to run have been recently used for Koopman operator learning by Baddoo et al.  where an iterative procedure is used to identify the best approximation to the full kernel, but no formal learning rates are demonstrated, and by Ahmad et al.  who derive learning rates in Hilbert-Schmidt norm (while we consider operator norm) for the Nystrom KRR estimator (one of the three considered in this paper).

ContributionsIn this paper we adopt the kernel learning approach. Starting from the problem of approximating the Koopman operator in a reproducing kernel Hilbert space, we derive three different estimators based on different inductive biases: kernel ridge regression (KRR) which comes from Tikhonov regularization, principal component regression (PCR) which is equivalent to DMD and its extensions, and reduced rank regression (RRR) which comes from a constraint on the maximum rank of the estimator . We show how to overcome the computational scalability problems inherent in full kernel methods using an approximation based on random projections which is known as the Nystrom method . The approximate learning algorithms scale very easily to the largest datasets, with a computational complexity which goes from \(O(n^{3})\) for the exact algorithm to \(O(n^{2})\) for the approximate one. We can further show that the Nystrom KRR, PCR and RRR estimators have the same convergence rates as their exact, slow counterparts - which are known to be optimal under our assumptions. We provide learning bounds in operator norm, which are known to translate to bounds for dynamic mode decomposition and are thus of paramount importance for applications. Finally, we thoroughly validate the approximate PCR and RRR estimators on synthetic dynamical systems, comparing efficiency and accuracy against their exact counterparts , as well as recently proposed fast Koopman estimator streaming KAF . To showcase a realistic scenario, we train on a molecular dynamics simulation of the fast-folding Trp-cage protein .

Structure of the paperWe introduce the setting in Section 2, and define our three estimators in Section 3. In Section 4 we provide bounds on the excess risk of our estimators, and extensive experiments on synthetic as well as large-scale molecular dynamics datasets in Section 5.

## 2 Background and related work

NotationWe consider a measurable space \((,)\) where \(\) corresponds to the state space, and denote \(L^{2}_{}:=L^{2}(,,)\) the \(L^{2}\) space of functions on \(\) w.r.t. to a probability measure \(\), and \(L^{}_{}\) the space of measurable functions bounded almost everywhere. We denote \(()\) the space of Hilbert-Schmidt operators on a space \(\).

SettingThe setting we will consider is that of Markovian, time-homogeneous stochastic process \(\{X_{t}\}_{t}\) on \(\). By definition of a Markov process, \(X_{t}\) only depends on \(X_{t-1}\) and not on any previous states. Time-homogeneity ensures that the transition probability \(X_{t+1} B|X_{t}=\) for any measurable set \(B\) does not depend on \(t\), and can be denoted with \(p(,B)\). This implies in particular that the distribution of \((X_{t},X_{t+1})\) does not depend on \(t\), and we denote it \(\) in the following. We further assume the existence of the _invariant_ density \(\) which satisfies \((B)=_{}(x)p(,B)\,\). This classical assumption allows one to study a large class of stochastic dynamical systems, but also deterministic systems on the attractor, see e.g. . The Koopman operator \(_{}:L^{2}_{}() L^{2}_{}()\) is a bounded linear operator, defined by

\[(_{}g)()=_{}p(, )g()\,= g(X_{t+1})|X_{t}=, g L^{2}_{}(), .\] (2)

We are in particular interested in the eigenpairs \((_{i},_{i}) L^{2}_{}\), that satisfy

\[_{}_{i}=_{i}_{i}.\] (3)

Through this decomposition it is possible to interpret the system by separating fast and slow processes, or projecting the states onto fewer dimensions [15; 19; 7]. In particular, the Koopman mode decomposition (KMD) allows to propagate the system state in time. Given an observable \(g:^{d}\) such that \(g\{_{i}|i\}\), the modes allow to reconstruct \(g()\) with a Koopman eigenfunction basis. The modes \(^{g}_{i}^{d}\) are the coefficients of this basis expansion:

\[(_{}g)()=g(X_{t})|X_{0}= =_{i}_{i}_{i}() ^{g}_{i}.\] (4)

This decomposition describes the system's dynamics in terms of a stationary component (the Koopman modes), a temporal component (the eigenvalues \(_{i}\)) and a spatial component (eigenfunctions \(_{i}\)).

Kernel-based learningIn this paper we approximate \(_{}\) with kernel-based algorithms, using operators in reproducing kernel Hilbert spaces (RKHS) \(\) associated with kernel \(k:\) and feature map \(:\). We wish to find an operator \(A:\) which minimizes the risk

\[_{}(A)=_{}(A,(, ))(A,(,)):=\|( )-A()\|^{2}.\] (5)

The adjoint of \(A\), denoted by \(A^{*}\), should thus be understood as an estimator of the Koopman operator \(_{}\) in \(\) as will be clarified in (15). In practice \(\) and \(\) are unknown, and one typically has access to a dataset \(\{(_{i},_{i})\}_{i=1}^{n}\) sampled from \(\), where each pair \((_{i},_{i}=f(_{i}))\) may equivalently come from a single long trajectory or multiple shorter ones concatenated together. We thus use the empirical risk

\[}_{}(A)=_{i=1}^{n}(A,(_{i},_{i}))\] (6)

as a proxy for (5). Since minimizing eq. (6) may require finding the solution to a very badly conditioned linear system, different regularization methods (such as Tikhonov or truncated SVD) can be applied on top of the empirical risk.

**Remark 2.1** (Connections to other learning problems): _The problem of minimizing eqs. (5) and (6) has strong connections to learning conditional mean embeddings [59; 22; 44; 33] where the predictors and targets are embedded in different RKHSs, and to structured prediction [12; 13] which is an even more general framework. On the other hand, the most substantial difference from the usual kernel regression setting  is the embedding of both targets and predictors into a RKHS, instead of just targets._

We denote the input and cross covariance \(C=_{}[()()]\) and \(C_{YX}=_{}[()()]\), and their empirical counterparts as \(=_{i=1}^{n}[(_{i})(_{i})]\) and \(_{YX}=_{i=1}^{n}(_{i})(_{i})]\). We also use the abbreviation \(C_{}:=C I\). Minimizing the empirical risk (6) with Tikhonov regularization  yields the following KRR estimator

\[_{}=*{arg\,min}_{A()} }_{}(A)+\|A\|_{}^{2}=_{YX}( + I)^{-1}.\] (7)

Eq. (7) can be computed by transforming its expression with the kernel trick , to arrive at a form where one must invert the kernel matrix - a \(n n\) matrix whose \(i,j\)-th entry is \(k(_{i},_{j})\). This operation requires \(O(n^{3})\) time and \(O(n^{2})\) memory, severely limiting the scalability of KRR to \(n 100\,000\) points. Improving the scalability of kernel methods is a well-researched topic, with the most important solutions being random features [50; 51; 68; 21] and random projections [58; 65; 21]. In this paper we use the latter approach, whereby the kernel matrix is assumed to be approximately low-rank and is _sketched_ to a lower dimensionality. In particular we will use the Nystrom method to approximate the kernel matrix projecting it onto a small set of inducing points, chosen among the training set. The sketched estimators are much more efficient than the exact ones, increasingly so as the training trajectories become longer. For example, the state of the art complexity for solving (non vector valued) approximate kernel ridge regression is \(O(n)\) time instead of \(O(n^{3})\)[38; 1; 10]. Furthermore, when enough inducing points are used (typically on the order of \(\)), the learning rates of the exact and approximate estimators are the same, and optimal [5; 53]. Hence it is possible - and in this paper we show it for learning the Koopman operator - to obtain large efficiency gains, without losing anything in terms of theoretical guarantees of convergence.

## 3 Nystrom estimators for Koopman operator regression

In this section, we introduce three efficient approximations of the KRR, PCR and RRR estimators of the Koopman operator. Our estimators rely on the Nystrom approximation, i.e. on random projections onto low-dimensional subspaces of \(\) spanned by the feature-embeddings of subsets of the data. We thus consider two sets of \(m n\) inducing points \(\{}_{j}\}_{j=1}^{m}\{_{t}\}_{t=1}^{n}\) and \(\{}_{j}\}_{j=1}^{m}\{_{t}\}_{t=1}^{n}\) sampled respectively from the input and output data. The choice of these inducing points (also sometimes called Nystrom centers) is important to obtain a good approximation. Common choices include uniform sampling, leverage score sampling [17; 55], and iterative procedures such as the one used in  to identify the most relevant centers. In this paper we focus on uniform sampling for simplicity, but we stress that our theoretical results in Section 4 can easily be extended to leverage scores sampling by means of [53; Lemma 7]. To formalize the Nystrom estimators, we define operators \(_{X},_{Y}:^{m}\) as \(_{X}w=_{j=1}^{m}w_{j}(}_{j})\) and \(_{Y}w=_{j=1}^{m}w_{j}(}_{j})\), and denote \(P_{X}\) and \(P_{Y}\) the orthogonal projections onto \(*{span}_{X}\) and \(*{span}_{Y}\) respectively.

In the following paragraphs we apply the projection operators to three estimators corresponding to different choices of regularization. For each of them a specific proposition (proven in Section C) states an efficient way of computing it based on the kernel trick. For this purpose we introduce the kernel matrices \(K_{,X},K_{,Y}^{m n}\) between training set and inducing points with entries \((K_{,X})_{ji}=k(}_{j},_{i})\), \((K_{,Y})_{ji}=k(}_{j},_{i})\), and the kernel matrices of the inducing points \(K_{,},K_{,}^{m m}\) with entries \((K_{,X})_{jk}=k(}_{j},}_{k})\) and \((K_{,X})_{jk}=k(}_{j},}_{k})\).

Kernel Ridge Regression (KRR)The cost of computing \(_{}\) defined in Eq. (7) is \(O(n^{3})\) which is prohibitive for datasets containing long trajectories. However, applying the projection operators to each side of the empirical covariance operators, we obtain an estimator which additionally depends on the \(m\) inducing points:

\[_{m,}^{}:=P_{Y}_{YX}P_{X}(P_{X}P_{X}+  I)^{-1}:.\] (8)If \(\) is infinite dimensional, Eq. (8) cannot be computed directly. Proposition 3.1 (proven in Section C) provides a computable version of the estimator.

**Proposition 3.1** (Nystrom KRR): _The Nystrom KRR estimator (8) can be expressed as_

\[^{}_{m,}=_{Y}K^{}_{,}K_{,Y}K_{X,}(K_{,X}K_{X,}+n K_{, })^{}^{*}_{X}.\] (9)

_The computational bottlenecks are the inversion of an \(m m\) matrix and a large matrix multiplication, which overall need \(O(2m^{3}+2m^{2}n)\) operations. In particular, in Section 4 we will show that \(m\) is sufficient to guarantee optimal rates even with minimal assumptions, leading to a final cost of \(O(n^{2})\). Note that a similar estimator was derived in ._

Please note that the \(O(n^{2})\) cost is for a straightforward implementation, and can indeed be reduced via iterative linear solvers (possibly preconditioned, to further reduce the practical running time), and randomized linear algebra techniques. In particular, we could leverage results from Rudi et al.  to reduce the computational cost to \(O(n)\).

Principal Component Regression (PCR)Typical settings in which Koopman operator theory is used focus on the decomposition of a dynamical system into a small set of components, obtained from the eigendecomposition of the operator itself. For this reason, a good prior on the Koopman estimator is for it to be low rank. The kernel PCR estimator \(^{}=_{YX}[]^{}_{r}\) formalizes this concept [29; 67], where here \([\![]\!]_{r}\) denotes the truncation to the first \(r\) components of the spectrum. Again this is expensive to compute when \(n\) is large, but the estimator can be sketched as follows:

\[^{}_{m}=P_{Y}_{YX}[\![P_{X}P_{X}]\!]^{}_{r}.\] (10)

The next proposition provides an efficiently implementable version of this estimator.

**Proposition 3.2** (Nystrom PCR): _The sketched PCR estimator (10) satisfies_

\[^{}_{m}=_{Y}K^{}_{,}K_{ ,Y}K_{X,}[\![K^{}_{,}K_{,X}K_{X, {X}}]\!]_{r}^{*}_{X}\] (11)

_requiring \(O(2m^{3}+2m^{2}n)\) operations, i.e. optimal rates can again be obtained at a cost of at most \(O(n^{2})\) operations._

Note that with \(m=n\), \(^{}_{m}\) is equivalent to the kernel DMD estimator , also known as kernel analog forecasting (KAF) . The sketched estimator of Proposition 3.2 was also recently derived in , albeit without providing theoretical guarantees.

Reduced Rank Regression (RRR)Another way to promote low-rank estimators is to add an explicit rank constraint when minimizing the empirical risk. Combining such a constraint with Tikhonov regularization corresponds to the reduced rank regression [24; 29] estimator:

\[A^{}_{}=*{arg\,min}_{A(A) r} }_{}(A)+\|A\|^{2}_{}.\] (12)

Minimizing Eq. (12) requires solving a \(n n\) generalized eigenvalue problem. The following proposition introduces the sketched version of this estimator, along with a procedure to compute it which instead requires the solution of a \(m m\) eigenvalue problem. For \(m\), which is enough to guarantee optimal learning rates with minimal assumptions (see Section 4), this represents a reduction from \(O(n^{3})\) to \(O(n)\) time.

**Proposition 3.3** (Nystrom RRR): _The Nystrom RRR estimator can be written as_

\[^{}_{m,}=[\![P_{Y}_{YX}P_{X}(P_{X}P_{X}+  I)^{-1/2}]\!]_{r}(P_{X}P_{X}+ I)^{-1/2}.\] (13)

_To compute it, solve the \(m m\) eigenvalue problem_

\[(K_{,X}K_{X,}+n K_{,})^{}K_{, X}K_{Y,}K^{}_{,}K_{,Y}K_{X,}w_{i}= _{i}^{2}w_{i}\]

_for the first \(r\) eigenvectors \(W_{r}=[w_{1},,w_{r}]\), appropriately normalized. Then denoting \(D_{r}:=K^{}_{,}K_{,Y}K_{X,}W_{r}\) and \(E_{r}:=(K_{,X}K_{X,}+n K_{,})^{}K_{ ,X}K_{Y,}D_{r}\) it holds_

\[^{}_{m,}=_{Y}D_{r}E_{r}^{*}^{*}_{X}.\] (14)Learning bounds in operator norm for the sketched estimators

In this section, we state the main theoretical results showing that optimal rates for operator learning with KRR, PCR and RRR can be reached with Nystrom estimators.

AssumptionsWe first make two assumptions on the space \(\) used for the approximation, via its reproducing kernel \(k\).

**Assumption 4.1** (Bounded kernel): _There exists \(K<\) such that \(_{}() K\)._

Assumption 4.1 ensures that \(\) is compactly embedded in \(L^{2}_{}\)[61, Lemma 2.3], and we denote \(^{*}_{X}: L^{2}_{}\) the embedding operator which maps any function in \(\) to its equivalence class \(\)-almost everywhere in \(L^{2}_{}\).

**Assumption 4.2** (Universal kernel): _The kernel \(k\) is universal, i.e. \(((^{*}_{X}))=L^{2}_{}\)._

We refer the reader to [60, Definition 4.52] for a definition of a universal kernel. The third assumption on the RKHS is related to the embedding property from Fischer and Steinwart , connected to the embedding of interpolation spaces. For a detailed discussion see Section A.3.

**Assumption 4.3** (Embedding property): _There exists \(]0,1]\) and \(c_{}>0\) such that \(_{} C^{-1/2}_{}() ^{2} c_{}^{-}\)._

Next, we make an assumption on the decay of the spectrum of the covariance operator that is of paramount importance for derivation of optimal learning bounds. In the following, \(_{i}(A)\) and \(_{i}(A)\) always denote the eigenvalues and singular values of an operator \(A\) (in decreasing order).

**Assumption 4.4** (Spectral decay): _There exists \(]0,]\) and \(c>0\) such that \(_{i}(C) ci^{-1/}\)._

This assumption is common in the literature, and we will see that the optimal learning rates depend on \(\). It implies the bound \(d_{}():=(C^{-1}_{}C)^{-}\) on the effective dimension, which is a key quantity in the analysis (both statements are actually equivalent, see Section E.2). Note that \(d_{}()=}_{} C^{- 1/2}_{}()_{}  C^{-1/2}_{}()\), and thus it necessarily holds \(\). For a Gaussian kernel, both \(\) and \(\) can be chosen arbitrarily close to zero.

Finally, we make an assumption about the regularity of the problem itself. A common assumption occurring in the literature is that \(}[f(X_{1})\,|\,X_{0}=]\) for every \(f\), meaning that one can define the Koopman operator directly on the space \(\), i.e. the learning problem is _well-specified_. However, this assumption is often too strong. Following [30, D.1] we make a different assumption on the cross-covariance remarking that, irrespectively of the choice of RKHS, it holds true whenever the Koopman operator is self-adjoint (i.e. the dynamics is time-reversible).

**Assumption 4.5** (Regularity of \(_{}\)): _There exists \(a>0\) such that \(C_{XY}C^{*}_{XY} a^{2}C^{2}\)._

RatesThe risk can be decomposed as \(_{}(A)=_{}(A)+_{,0}\) where \(_{,0}\) is a constant and \(_{}(A):=_{}^{*}_{X}-^{*}_{X}A^ {*}_{}^{2}\) corresponds to the excess risk (more details in Section B). Optimal learning bounds for the KRR estimator in the context of CME (i.e. in Hilbert-Schmidt norm) have been developed in  under Assumptions 4.1 to 4.4 in well-specified and misspecified settings. On the other hand, in the context of dynamical systems, Kostic et al. [29, Theorem 1] report the importance of _reduced rank estimators_ that have a small excess risk in operator norm

\[(A):=_{}^{*}_{X}-^{*}_{X}A^{*}_{  L^{2}_{}}^{2}.\] (15)

The rationale behind considering the operator norm is that it allows to control the error of the eigenvalues approximation and thus of the KMD (3), (4) as discussed below. Optimal learning bounds in operator norm for KRR, PCR and RRR are established in . In this work we show that the same optimal rates remain valid for the _Nystrom_ KRR, PCR and RRR estimators. According to  and  these operator norm bounds lead to reliable approximation of the Koopman mode decomposition of Eq. (4).

We now provide our main result.

**Theorem 4.6** (Operator norm error for KRR, i.i.d. data): _Let assumptions 4.1 to 4.5 hold. Let \((_{i},_{i})_{1 i n}\) be i.i.d. samples, and let \(P_{Y}=P_{X}\) be the projection induced by \(m\) Nystrom landmarks drawn uniformly from \((_{i})_{1 i n}\) without replacement. Let \(=c_{}n^{-1/(1+)}\) where \(c_{}\) is a constant given in the proof, and assume \(n(c_{}/K^{2})^{1+}\). Then it holds with probability at least \(1-\)_

\[(_{m,}^{})^{1/2} n^{-} m(1,n^{/(1+)})( }{{}}).\]

The proof is provided in Section E.2, but essentially relies on a decomposition involving the terms \(\|C_{}^{-1/2}(C_{YX}-_{YX})\|\), \(\|C_{}^{-1/2}(C-)\|\), \(\|C_{}^{-1/2}(C-)C_{}^{-1/2}\|\), as well as bounding the quantity \(\|P_{X}^{}C^{1/2}\|\) where \(P_{X}^{}\) denotes the projection on the orthogonal of \((P_{X})\). All these terms are bounded using two variants of the Bernstein inequality. Note that our results can easily be extended to leverage score sampling of the landmarks by bounding term \(\|P_{X}^{}C^{1/2}\|\) by means of [53, Lemma 7]; the same rate could then be obtained using a smaller number \(m\) of Nystrom points.

The rate \(n^{-1/(2(1+))}\) is known to be optimal (up to the log factor) in this setting by assuming an additional lower bound on the decay of the covariance's eigenvalues of the kind \(_{i}(C) i^{-1/}\), see [30, Theorem 7 in D.4]. One can see that without particular assumptions (\(==1\)), we only need the number \(m\) of inducing points to be of the order of \(()\) in order to get an optimal rates. For \(\) fixed, this number increases when \(\) decreases (faster decay of the covariance's spectrum), however note that the optimal rate depends on \(\) and also improves in this case. The dependence in \(\) is particularly interesting, as for instance with a Gaussian kernel it is known that \(\) can be chosen arbitrarily closed to zero . In that case, the number \(m\) of inducing points can be taken on the order of \(( n)\).

Note that a bound for the Nystrom KRR estimator has been derived in Hilbert-Schmidt norm by Ahmad et al. . Using the operator norm however allows to derive bounds on the eigenvalues (see discussion below), which is of paramount importance for practical applications. Moreover, we now provide a bound on the error of PCR and RRR estimators, which are not covered in .

**Lemma 4.7** (**Operator norm error for PCR and RRR, i.i.d. data)**: _Under the assumptions of Theorem 4.6, taking \(=c_{}n^{-1/(1+)}\) with \(c_{}\) as in Theorem 4.6, \(n(c_{}/K^{2})^{1+}\), and provided_

\[m(1,n^{/(1+)})(}{{}}),\]

_it holds with probability at least \(1-\)_

\[(_{m,}^{})^{1/2} c_{ }\,n^{-},\ r_{r+1}(_{X}_{}^{*})<(_{r}(_{X} _{}^{*}),n^{-})\] \[\ \ (_{m}^{})^{1/2} c_{ }\,n^{-},\ r>n^{},\]

_where \(c_{}=(_{r}^{2}(_{X}_{}^{*})-_{r+1} ^{2}(_{X}_{}^{*}))^{-1}\) and \(c_{}=(_{r}(_{X})-_{r+1}(_{X}))^{-1}\) are the problem dependant constants._

Note that when rank of \(_{}\) is \(r\), then there is no restriction on \(r\) for the RRR estimator, while for PCR the choice of \(r\) depends on the spectral decay property of the kernel. In general, if \(r>n^{}\), then \(_{r+1}(_{X}_{}^{*})_{r+1}(_{X}) n ^{-1/(2(1+))},\) which implies that RRR estimator can achieve the same rate of PCR but with smaller rank. Again the rate is sharp (up to the log factor) in this setting .

Koopman mode decompositionAccording to [29, Theorem 1], working in operator norm allows us to bound the error of our estimators for dynamic mode decomposition, as well as to quantify how close the eigenpairs \((_{i},_{i})\) of an estimator \(^{*}\) are to being eigenpairs of the Koopman operator. Namely, recalling that for function \(_{i}\), the corresponding candidate for Koopman eigenfunction in \(L_{}^{2}\) space is \(_{X}^{*}_{i}\), one has \(\|_{}(_{X}^{*}_{i})-_{i}(_{X}^{* }_{i})\|/\|_{X}^{*}_{i}\|( )^{1/2}\|_{i}\|/\|_{X}^{*}_{i}\|\). While eigenvalue and eigenfunction learning rates were studied, under additional assumptions, in , where the operator norm error rates were determinant, here, in Section 5, we empirically show that the proposed estimators accurately learn the Koopman spectrum. We refer the reader to Section D for the details on computation of eigenvalues, eigenfunctions and KMD of an estimator in practice.

Dealing with non-i.i.d. dataThe previous results hold for i.i.d. data, which is not a very realistic assumption when learning from sampled trajectories. Our results can however easily be extended to \(\)-mixing processes by considering random variables \(Z_{i}=_{j=1}^{k}X_{i+j}\) (thus representing portions of the trajectory) sufficiently separated in time to be nearly independent. We now consider a trajectory \(_{1},,_{n+1}\) with \(_{1}\) and \(_{t+1} p(_{t},)\) for \(t[1,n]\), and use Lemma J.8 (re-stated from ) which allows to translate concentration results on the \(Z_{i}\) to concentration on the \(X_{i}\) by means of the \(\)-mixing coefficients defined as \(_{X}(k):=_{B}_{k}(B)-( )(B)\) where \(_{k}\) denotes the joint probability of \((X_{t},X_{t+k})\). Using this result the concentration results provided in appendix can thus be generalized to the \(\)-mixing setting, and apart from logarithmic dependencies we essentially obtain similar results to the i.i.d. setting except that the sample size \(n\) is replaced by \(p n/(2k)\).

## 5 Experimental validation

In this section we show how the estimators proposed in section 3 perform in various scenarios, ranging from synthetic low dimensional ODEs to large-scale molecular dynamics simulations. The code for reproducing all experiments is available online. Our initial aim is to demonstrate the speed of NysPCR and NysRRR, compared to the recently proposed alternative Streaming KAF (sKAF) . Then we show that their favorable scaling properties make it possible to train on large molecular dynamics datasets without any subsampling. In particular we run a metastability analysis of the alanine dipeptide and the Trp-cage protein, showcasing the accuracy of our models' eigenvalue and eigenfunction estimates, as well as their efficiency on massive datasets (\(>500\,000\) points)

**Efficiency Benchmarks on Lorenz '63** The chaotic Lorenz '63 system  consists of 3 ODEs with no measurement noise. With this toy dynamical system we can easily compare the Nystrom estimators to two alternatives: 1. the corresponding _exact_ estimators and 2. the sKAF algorithm which also uses randomized linear algebra to improve the efficiency of PCR. In this setting we sample long trajectories from the system, keeping the first points for training (the number of training points varies for the first experiment, and is fixed to \(10\,000\) for the second, see fig. 2), and the subsequent ones for testing. In fig. 1 we compare the run-time and accuracy with of NysPCR and NysRRR versus their full counterparts. To demonstrate the different scaling regimes we fix the number of inducing points to 250 and increase the number of data points \(n\). The accuracy of the two solvers (as measured with the normalized RMSE metric (nRMSE)  on the first variable) is identical for PCR and close for RRR, but the running time of the approximate solvers increases much slower with \(n\) than that of the exact solvers. Each experiment is repeated 20 times to display error bars over the choice of Nystrom centers. In the second experiment, shown in fig. 2, we reproduce the setting of  by training at increasingly long forecast horizons. Plotting the nRMSE we verify that sKAF and NysPCR converge to very similar accuracy values, although NysPCR is approximately \(10\) times faster. NysRRR instead offers slightly better accuracy, at the expense of a higher running time compared to NysPCR. Error bars are the standard deviation of nRMSE over 5 successive test sets with \(10\,000\) points each. In section K we show additional experiments including a comparison to the Nystrom KRR estimator.

**Molecular dynamics datasets** An important application of Koopman operator theory is in the analysis of molecular dynamics (MD) datasets, where the evolution of a molecule's atomic positions as they evolve over time is modelled. Interesting systems are very high dimensional, with hundreds or thousands of atoms. Furthermore, trajectories are generated at very short time intervals (\(<1\,\)) but

Figure 1: Full and Nyström estimators trained on L63 with increasing \(n\). Error (_left_) and running time (_right_) are plotted to show efficiency gains without accuracy loss with the Nyström approximation. RBF(\(=3.5\)) kernel, \(r=25\) principal components and \(m=250\) inducing points.

Figure 2: Nyström and sKAF estimators trained on L63 for increasing forecast horizons; the error (_left_) and overall running times (_right_) are shown. We used a RBF kernel with \(=3.5\), \(r=50\), \(m=250\) (for Nyström methods) and \( n\) random features (for sKAF).

interesting events (e.g. protein folding/unfolding) occur at timescales on the order of at least \(10\,\), so that huge datasets are needed to have a few samples of the rare events. The top eigenfunctions of the Koopman operator learned on such trajectories can be used to project the high-dimensional state space onto low-dimensional coordinates which capture the long term, slow dynamics.

We take three \(250\,\) long simulations sampled at \(1\,\) of the alanine dipeptide , which is often taken as a model system for molecular dynamics [47; 46]. We use the pairwise distances between heavy atoms as features, yielding a 45-dimensional space. We train a NysRRR model with \(10\,000\) centers on top of the full dataset (\(449\,940\) points are used for training, the rest for validation and testing) with lag time \(100\,\), and recover a 2-dimensional representation which correlates well with the \(,\) backbone dihedral angles of the molecule, known to capture all relevant long-term dynamics. Figure 3a shows the top two eigenfunctions overlaid onto \(,\), the first separates the slowest transition between low and high \(\); the second separates low and high \(\). The implied time-scales from the first two non-trivial eigenvalues are \(1262\,\) and \(69\,\), which are close to the values reported by Niiske et al.  (\(1400\,\) and \(70\,\)) who used a more complex post-processing procedure to identify time-scales. We then train a PCCA+  model on the first three eigenfunctions to obtain three states, as shown in fig. 3b. PCCA+ acts on top of a fine clustering (in our case obtained with k-means, \(k=50\)), to find the set of maximally stable states by analyzing transitions between the fine clusters. The coarse clusters clearly correspond to the two transitions described above.

Finally we take a \(208\,\) long simulation of the fast-folding Trp-cage protein , sampled every \(0.2\,\). Again, the states are the pairwise distances between non-hydrogen atoms belonging to the protein, in \(10\,296\) dimensions. A NysRRR model is trained on \(626\,370\) points, using \(5000\) centers in approximately 10 minutes. Note that without sketching this would be a completely intractable problem. Using a lag-time of \(10\,\) we observe a spectral gap between the third and fourth eigenvalues, hence we train a PCCA+ model on the first 3 eigenfunctions to obtain the states shown in fig. 4. The first non-trivial Koopman eigenvector effectively distinguishes between the folded (state 1) and unfolded states as is evident from the first row of fig. 4. The second one instead can be used to identify a partially folded state of the protein (state 0), as can be seen from the insets in fig. 4.

## 6 Conclusions

We introduced three efficient kernel-based estimators of the Koopman operator relying on random projections, and provided a bound on their excess risk in operator norm - which is of paramount importance to control the accuracy of Koopman mode decomposition. Random projections allow to process efficiently even the longest trajectories, and these gains come for free as our estimators still enjoy optimal theoretical learning rates. We leave for future work the refinement our analysis under e.g. an additional source condition assumption or in the misspecified setting. Another future research direction shall be to devise ways to further reduce the computational complexity of the estimators.

Figure 3: Dynamics of the alanine dipeptide (lag-time 100), Nyström RRR model. On the left the Ramachandran plot of all protein conformations is colored with the value of the first two non-constant eigenfunctions evaluated on the 45-d space. On the right color indicates the discrete state obtained by clustering the dynamics projected onto the first eigenfunctions (using PCCA+ with 3 states). The obtained clusters match the main areas of the Ramachandran plot.

## 7 Acknowledgements

This paper is part of a project that has received funding from the European Research Council (ERC) under the European Union's Horizon 2020 research and innovation programme (grant agreement No. 819789). L. R. acknowledges the financial support of the European Research Council (grant SLING 819789), the AFOSR projects FA9550-18-1-7009, FA9550-17-1-0390 and BAA-AFRL-AFOSR-2016-0007 (European Office of Aerospace Research and Development), the EU H2020-MSCA-RISE project NoMADS - DLV-777826, and the Center for Brains, Minds and Machines (CBMM), funded by NSF STC award CCF-1231216. M. P., V. K. and P. N. acknowledge financial support from PNRR MUR project PE0000013-FAIR and the European Union (Projects 951847 and 101070617).