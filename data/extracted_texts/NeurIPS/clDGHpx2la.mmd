# InversionView: A General-Purpose Method for

Reading Information from Neural Activations

 Xinting Huang

Saarland University

xhuang@lst.uni-saarland.de &Madhur Panwar

EPFL

madhur.panwar@epfl.ch &Navin Goyal

Microsoft Research India

navingo@microsoft.com &Michael Hahn

Saarland University

mhahn@lst.uni-saarland.de

###### Abstract

The inner workings of neural networks can be better understood if we can fully decipher the information encoded in neural activations. In this paper, we argue that this information is embodied by the subset of inputs that give rise to similar activations. We propose InversionView, which allows us to practically inspect this subset by sampling from a trained decoder model conditioned on activations. This helps uncover the information content of activation vectors, and facilitates understanding of the algorithms implemented by transformer models. We present four case studies where we investigate models ranging from small transformers to GPT-2. In these studies, we show that InversionView can reveal clear information contained in activations, including basic information about tokens appearing in the context, as well as more complex information, such as the count of certain tokens, their relative positions, and abstract knowledge about the subject. We also provide causally verified circuits to confirm the decoded information.1

## 1 Introduction

Despite their huge success, neural networks are still widely considered black boxes. One of the most important reasons is that the continuous vector representations in these models pose a significant challenge for interpretation. If we could understand what information is encoded in the activations of a neural model, significant progress might be achieved in fully deciphering the inner workings of neural networks, which would make modern AI systems safer and more controllable. Toward this goal, various methods have been proposed for understanding the inner activations of neural language models. They range from supervised probes  to projecting to model's vocabulary space  to causal intervention  on model's inner states. However, to this date, decoding the information present in neural network activations in human-understandable form remains a major challenge. Supervised probing classifiers require the researcher to decide which specific information to probe for, and does not scale when the space of possible outputs is very large. Projecting to the vocabulary space is restricted in scope, as it only produces individual tokens. Causal interventions uncover information flow, but do not provide direct insight into the information present in activations.

Here, we introduce InversionView as a principled general-purpose method for generating hypotheses about the information present in activations in neural models on language and discrete sequences, which in turn helps us identify how the information flows through the model--crucial for obtaining the algorithm implemented by the model. InversionView aims at providing a direct way of reading outthe information encoded in an activation. The technique starts from the intuition that the information encoded in an activation can be formalized as its _preimage_, the set of inputs giving rise to this particular activation under the given model. In order to explore this preimage, given an activation, we train a decoder to sample from this preimage. Inspection of the preimage, across different inputs, makes it easy to identify which information is passed along, and which information is forgotten. It accounts for the geometry of the representation, and can identify which information is reinforced or downweighted at different model components. InversionView facilitates the interpretation workflow, and provides output that is in principle amenable to automated interpretation via LLMs (we present a proof of concept in Section 4).

We showcase the usefulness of the method in three case studies: a character counting task, Indirect Object Identification, and 3-digit addition. We also present preliminary results on the factual recall task, demonstrating the applicability of our method to larger models. The character counting task illustrates how the method uncovers how information is processed and forgotten in a small transformer. In Indirect Object Identification in GPT2-Small , we use InversionView to easily interpret the information encoded in the components identified by Wang et al. , substantially simplifying the interpretability workflow. For 3-digit addition, we use InversionView to provide for the first time a fully verified circuit. Across the case studies, InversionView allows us to rapidly generate hypotheses about the information encoded in each activation site. Coupled with attention patterns or patching methods, we reverse-engineer the flow of information, which we verify using causal interventions.

## 2 Methodology

Interpretation FrameworkWhat information does an activation in a neural network encode? InversionView answers this in terms of the inputs that give rise to this activation (Figure 1). For instance, if a certain activation encodes solely that "_the subject is John_" and nothing else (Figure 1, right), then it will remain unchanged when other parts in the sentence change while preserving this aspect (e.g., "_John is on leave today_." "_John has a cute dog_."). From another perspective, if all sentences where the subject is John are represented so similarly that the model cannot distinguish them, given one of these representations, the only information is the commonality "_the subject is John_" (assuming sentences are represented differently when it does not hold). Building on this intuition, given an activation, InversionView aims to find those inputs that give rise to the same activation, and examine what's common among them to infer what information it encodes. In realistic networks, different inputs will rarely give rise to exactly the same activation. Rather, different changes to an input will change the activation to different degrees. The sensitivity of an activation to different changes reflects the representational geometry: larger changes make it easier for downstream components to read out information than very small changes. This motivates a threshold-based definition of preimages, where we consider information as present in an activation when the activation is sufficiently sensitive to it. Formally speaking, given a space \(\) of valid inputs, a query input \(^{q}\), a function \(f\) that represents the activation of interest as a function of the input, and a query activation \(^{q}=f(^{q})\), define the _\(\)-preimage_:2

\[B_{^{q},f,}=\{:D(f(), ^{q})\},\] (1)

where \(>0\) is a threshold and \(D(,)\) is a distance metric. Both \(\) and \(D(,)\) are chosen by the researcher based on representation geometry; we will define these later in case studies. In practice,

Figure 1: Illustration of the geometry at two different activation sites, encoding different information about the input. Top: the semantics of being on leave are encoded. Bottom: the information that the subject of the input sentence is John is encoded.

in all our three case studies, we vary \(\) and set it so we can read out coherent concepts from the \(\)-preimage (Appendix A.4). With a threshold-based definition, we consider only those pieces of information that have substantial impact on the activation. See more discussion in Appendix A.1.

Conditional Decoder ModelIn this paper, we study the setting where \(^{q}\) is a sequence. Directly enumerating \(B_{^{q},f,}\) is in general not scalable, as the input space grows exponentially with the sequence length. To efficiently inspect \(B_{^{q},f,}\), we train a conditional decoder model that takes as input the activation \(^{q}\) and generates inputs giving rise to similar activations in the model under investigation. In the following, we refer to the original model that we are interpreting as the _probed model_, the conditional decoder as the _decoder_, the place in the probed model from which we take the activation as the _activation site_ (e.g., the output of \(i\)th layer), the inputs generated by the decoder as _samples_, and the index of a token in the sequence as _position_.

We implement the decoder as an autoregressive language model conditioned on \(^{q}\), decoding input samples \(\) (see Figure 2, and details in Appendix C). As the decoder's training objective corresponds to recovering \(\) exactly, sampling at temperature 1 will typically not cover the full \(\)-preimage. Thus, for generating elements of the \(\)-preimage, we increase diversity by drawing samples at higher temperatures and with noise added to \(^{q}\) (details in Appendix A.2). We then evaluate \(D(f(),^{q})\) at each position in each sample \(\), select the position minimizing \(D\),3 determine membership in \(B_{^{q},f,}\), and subsample in-\(\)-preimage and out-of-\(\)-preimage samples for inspection.

An important question is whether this method, relying on a black-box decoder, produces valid \(\)-preimages. _Correctness_ (are all generated samples in the \(\)-preimage?) is ensured by design, as we evaluate \(D(f(),^{q})\) for each generated sample. The other angle is _completeness_ (are the samples representative of the \(\)-preimage?). If some groups of inputs in \(\)-preimage are systematically missing from the generations, one may overestimate the information contained in activations. But this behavior would be punished by the training objective, since the loss on these examples would be high. We explicitly verify completeness by enumerating inputs in one of our case studies (Appendix B). Another approach is to design counter-examples \(\) not satisfying a hypothesis about the content of \(B_{^{q},f,}\). In our experiments, we found that these examples were always outside of \(B_{^{q},f,}\).

## 3 Discovering the Underlying Algorithm by InversionView

Notation.In the transformer architecture, outputs from each layer are added to their inputs due to residual connection. The representations of each token are only updated by additive updates,

Figure 2: **(a)** The probed model is trained on language modeling objective. **(b)** Given a trained probed model, we first cache the internal activations \(\) together with their corresponding inputs and activation site indices (omitted in the figure for brevity), then use them to train the decoder. The decoder is trained with language modeling objective, while being able to attend to \(\). **(c)** When interpreting a specific query activation \(^{}\), we give it to the decoder, which generates possible inputs auto-regressively. We then evaluate the distances on the original probed model.

forming a _residual stream_. Using notation based on  and , we denote the residual stream as \(x^{i,\{\}}^{N d}\), where \(i\) is the layer (an attention (sub)layer + an MLP (sub)layer) index, \(N\) is the number of input tokens, \(d\) is the model dimension, \(\), \(\), \(\) stand for the residual stream before the attention layer, between attention and MLP layer, and after the MLP layer. For example, \(x^{0,}\) is the sum of token and position embedding, \(x^{0,}\) is the sum of the output of the first attention layer and \(x^{0,}\), and \(x^{0,}\) is the sum of the output of the first MLP layer and \(x^{0,}\). Note that \(x^{i,}=x^{i+1,}\). We use subscript \(t\) to refer to the activation at token position \(t\), e.g., \(x^{i,}_{t}^{d}\). The attention layer output decomposes into outputs of individual heads \(h^{i,j}()\), i.e., \(x^{i,}=x^{i,}+_{j}h^{i,j}((x^{i,}))\), where \(()\) represents layer normalization (GPT style/pre-layer-norm). We denote the attention head's output as \(a^{i,j}\), i.e., \(a^{i,j}=h^{i,j}((x^{i,}))\).

Decoder Architecture.We train a single two-layer transformer decoder across all activation sites of interest. The query activation \(^{q}\) is concatenated with an _activation site embedding_\(\), a learned embedding layer indicating where the activation comes from, passed through multiple MLP layers with residual connections, and then made available to the attention heads in each layer of the decoder, alongside the already present tokens from the input, so that each attention head can also attend to the post-processed query activation in addition to the context tokens. Each training example is a triple consisting of an activation vector \(^{q}^{d}\), the activation site index, and the input, on which the decoder is trained with a language modeling objective. Appendix C has technical details.

### Character Counting

We train a transformer (2 layers, 1 head) on inputs such as "vvzccvczvvzcvcv\(|\)v:8" to predict the last token "8", the frequency of the target character (here, "v") before the separator "\(|\)". For each input, three distinct characters are sampled from the set of lowercase characters, and each character's frequency is sampled uniformly from 1-9. The input length varies between 7 and 31. We created 1.56M instances and applied a 75%-25% train-test split; test set accuracy is 99.53% (Details in

Figure 3: **InversionView on Character Counting Task. The model counts how often the target character (after ‘1’) occurs in the prefix (before ‘1’). B and E denote beginning and end of sequence tokens. The query activation conditions the decoder to generate samples capturing its information content. We show non-cherrypicked samples inside and outside the \(\)-preimage (\(=0.1\)) at three activation sites on the same query input. Distance for each sample is calculated between activations corresponding to the parenthesized characters in the query input and the sample. “True count” indicates the correct count of the target character in the samples (decoder may generate incorrect counts). (**a**) _MLP layer amplifies count information_. Comparing the distances before (left) and after (right) the MLP, we see that samples with diverging counts become much more distant from the query activation. (**b**) In the next layer (“:” exclusively attends to target character – copying information from residual stream of target character to the residual stream of “:”), _the count is retained but the identity of the target character is no longer encoded_ (“c”, “m”, etc. instead of “g”), as it is no longer relevant for the predicting the count. Therefore, observing the generations informs us of the activations’ content and how it changes across activation sites.**

Appendix D). We use \(D(,^{q})=-^{q}\|_{2}}{\| ^{q}\|_{2}}\) (i.e., normalized euclidean distance, as the magnitude of activations varies between layers), where \(\) denotes the aforementioned \(f()\), and set \(=0.1\).

Interpreting via InversionView and attention.In layer 0, the target character consistently attends to the same character in the previous context, suggesting that counting happens here. In Figure 2(a), we show the \(\)-preimage of \(x^{0,}_{tc}\) and \(x^{0,}_{tc}\), where the subscript \(tc\) denotes the target character. We show \( 10\) random samples at a single query input, but our hypotheses are based on--and easily confirmed by--rapid visual inspection of dozens of inputs across different query inputs.4 On the left (before the MLP), the activation encodes the target character, as all samples have "g" as the target character. Count information is not sharply encoded: while the closest activation corresponds to "g" occurring 3 times, two activations corresponding to a count-4 input ("g" occurring 4 times) are also close, even closer than a count-3 input. On the other hand, on the right (after the MLP), only count-3 inputs are inside the \(\)-preimage, and count-4 inputs become much more distant than before. Comparing the \(\)-preimage before and after the MLP in layer 0, we find that the MLP makes the count information more prominent in the representational geometry of the activation. The examples are not cherry-picked; count information is generally reinforced by the MLP across query inputs.

In the next layer, the colon consistently attends to the target character, and InversionView confirms that count information is moved to the colon's residual stream (Figure 2(b)). More importantly, this illustrates how information is abstracted: We previously found that \(x^{0,}_{tc}\) encodes identity and frequency of the target character. However, the colon obtains only an abstracted version of the information, in which count information remains while the target character is largely (though not completely) removed. InversionView makes this process visible, by showing that the target character becomes interchangeable with little change to the activation. See more examples in Appendix D.2. Overall, with InversionView, we have found a simple algorithm by which the model makes the right prediction: _In layer 0, the target character attends to all its occurrences and obtains the counts. In layer 1, the colon moves the results from the target character to its residual stream and then produces the correct prediction._ Accounting for other activation sites, we find that the model implements a somewhat more nuanced algorithm, investigated in Appendix D.4. Overall, InversionView shows how certain information is amplified, but also how information is abstracted or forgotten.

Quantitative verification.We causally verified our hypothesis using activation patching [53; 21] on (position, head output) pairs. As the attention head in layer 1 attends almost entirely to the target character, only head outputs \(a^{0,0}_{tc}\), \(a^{0,0}_{:,}\) and \(a^{1,0}_{:}\) can possibly play a role in routing count information. We patch their outputs with activations from a contrast example flipping a single character before "l". We patch activations cumulatively, starting either at the lowest or highest layer, with some fixed ordering within each layer. For example, we patch \(a^{0,0}_{:}\) and observe how final logits change compared to the clean run, then we patch both \(a^{0,0}_{:}\) and \(a^{0,0}_{tc}\) and do the same, and so forth. By the end of patching, the model prediction will be flipped. When adding an activation to the patched set, we attribute to it the increment in the difference of \(LD\) before and after patching, where \(LD\) denotes the logit difference between original count and the count in the contrast example. Cumulative patching allows us to observe dependencies: For instance, as we hypothesize that \(a^{1,0}_{:}\) is _completely_ dependent on \(a^{0,0}_{tc}\), we expect that, when \(a^{0,0}_{tc}\) is already patched, patching \(a^{1,0}_{:}\) will have no further effect, whereas when \(a^{0,0}_{tc}\) is not patched, patching \(a^{1,0}_{:}\) will have a significant effect. Results (Figure 3(a)) match our prediction: Patching either of the activation in the hypothesized path (\(a^{0,0}_{tc}\) and \(a^{1,0}_{:}\)) is sufficient to absorb the entire effect on logit differences, confirming the hypothesis. See Appendix D.3 for further details and D.4 for further experiments.

### IOI circuit in GPT-2 small

To test the applicability of InversionView to transformers pretrained on real-world data, we apply our method to the activations in the indirect object identification (IOI) circuit in GPT-2 small  discovered by Wang et al. . We apply InversionView to the components of the circuit, read out the information, and compare it with the information or function that Wang et al.  had ingeniously inferred using a variety of tailored methods, such as patching and investigating effects on logits and attention. We show that InversionView unveils the information contained in the attention heads' outputs, with results agreeing with those of Wang et al. .

The IOI task consists of examples such as "When Mary and John went to the store, John gave a drink to", which should be completed with "Mary". We use S for the subject "John" in the main clause, IO for the indirect object "Mary" introduced in the initial subclause, S1 and S2 for the first and second occurrences of the subject, and END for the "to" after which IO should be predicted. To facilitate comparison, we denote attention heads as in Wang et al.  with i.j denoting \(h^{i,j}\). Wang et al.  discover a circuit of 26 attention heads in GPT-2 small and categorize them by their function. In short, GPT-2 small makes correct predictions by copying the name that occurs only once in the previous context. For InversionView, we train the decoder on the IOI examples (See details in E.1). Despite the size of the probed model, we find the same 2-layer decoder architecture as in Section 3.1 to be sufficient. We use \(D(,^{q})=1-^{q}}{\|\|\|^{q}\|}\) (i.e., cosine distance), and \(=0.1\). Euclidean distance leads to similar results, but cosine distance is a better choice for this case (Appendix E.4).

We start with the Name Mover Head 9.9, which Wang et al.  found moves the IO name to the residual stream of END. 4b shows the \(\)-preimage at "to". The samples in the \(\)-preimage share the name "Justin" as the IO. The head also shows similar activity at some other positions (Appendix A.3). Results are consistent across query inputs. Therefore, InversionView agrees with the conclusions of Wang et al.  on head 9.9. Applying the same analysis to other heads (Table 2), we recovered information in high agreement with the information that Wang et al.  had inferred using multiple tailored methods. For example, Wang et al.  found S-Inhibition heads were outputting both token signals (value of S) and position signals (position of S1) by patching these heads' outputs from a series of counterfactual datasets. These datasets are designed to disentangle the two effects, in which token and/or position information are ablated or inverted. These two kinds of information can be directly read out by InversionView (Figure 19 shows an example for an S-Inhibition head that contains position information), and there is no need to guess the possible information to design patching experiments. Overall, among the 26 attention heads that Wang et al.  identified, InversionView indicates a different interpretation in only 3 cases; these (0.1, 0.10, 5.9) were challenging for the methods used before (Appendix E.3). In summary, InversionView scales to larger models.

### 3-Digit Addition

We next applied InversionView to the problem of adding 3-digit numbers, between 100 and 999. Input strings have the form "B362+405=767E" or "B824+692=1516E", and are tokenized at the

Figure 4: **(a) Character Counting. Activation patching results show that \(a_{tc}^{0,0}\) and \(a_{:}^{1,0}\) play crucial roles in prediction, as hypothesized based on Figure 3 and Sec. 3.3. In contrast examples, only one character differs. Top: We patch activations cumulatively from left to right. We can see patching \(a_{tc}^{0,0}\) accounts for the whole effect, and when \(a_{tc}^{0,0}\) is already patched, patching \(a_{:}^{1,0}\) has almost no effect. Bottom: On the other hand, if we patch cumulatively from right to left, \(a_{:}^{1,0}\) accounts for the whole effect while patching \(a_{tc}^{0,0}\) has no effect if \(a_{:}^{1,0}\) has been patched. So we verified that \(a_{:}^{1,0}\) solely relies on \(a_{tc}^{0,0}\) and this path is the one by which the model performs precise counting. The patching effect is averaged across the whole test set. (b) IOI. InversionView applied to Name Mover Head 9.9 at “to”; we fix the compared position to “to”. Throughout the \(\)-preimage, “Justin” appears as the IO, revealing that the head encodes this name. This interpretation is confirmed across query inputs.**

character level. We use F1, F2, F3 to denote the three digits of the first operand and S1, S2, S3 for the digits of the second operand, and A1, A2, A3, A4 (if it exists) for the three or four digits of the answer, and C2, C3 for the carry from tens place and ones place (i.e., C2: whether F2+S2\(\)10, C3: whether F3+S3\(\)10). Unlike , we do not left-pad answers to have all the same length; hence, positional information is insufficient to determine the place value of each digit.

The probed model is a decoder-only transformer (2 layers, 4 attention heads, dimension 32). We set attention dropout to 0. Other aspects are identical to GPT-2. The model is trained for autoregressive next-token prediction on the full input, in analogy to real-world language models. In testing, the model receives the tokens up to and including "=", and greedily generates up to "E". The prediction counts as correct if all generated tokens match the ground truth. The same train-test ratio as in Section 3.1 is used. The test accuracy is 98.01%. For other training details see Appendix F.1.

Interpreting via InversionView and attention.As Section 3.1 we use normalized Euclidean distance for \(D(,)\) and the threshold \(=0.1\). We first trace how the model generates the first answer digit, A1, by understanding the activations at the preceding token, "=". We first examine the attention heads at "=" in the \(0\)-th layer (Figure 5). As for the first head (\(a^{0,0}\)), only F1 and S1 matter in the samples - indeed, changing other digits, or swapping their order, has a negligible effect on the activation (Figure 5). Across different inputs, each of the three heads \(a^{0,0}\), \(a^{0,1}\), \(a^{0,2}\) encode either one or both of F1 and S1 (Figure 26); taken together, they always encode both. This is in agreement with attention focusing on these tokens. The fourth and remaining head in layer 0 (\(a^{0,3}\)) encodes F2 and S2, which provide the carry from the tens place to the hundreds place. Combining the information from these four heads, \(x^{0,}\) consistently encodes F1 and S1; and approximately represents F2, S2--only the carry to A1 (whether F2+S2\(\)10) matters here (Figure 5c). Other examples are in

Figure 5: **InversionView applied to 3-digit addition**: Visually inspecting sample inputs inside and outside the \(\)-preimage of the query allows us to understand what information is contained in an activation. The color on each token in generated samples denotes the difference in the token’s likelihood between a conditional or unconditional decoder (Appendix G). The shade thus denotes how much the generation of the token is caused by the query activation (darker shade means a stronger dependence). In (a–c), the colored tokens are most relevant to the interpretation. We interpret two attention heads (a,b) and the output of the corresponding residual stream after attention (c). In **(a)**, what’s common throughout the \(\)-preimage is that the digits in the hundreds places are 6 and 8. Inputs outside the \(\)-preimage don’t have this property. In **(b)**, what’s common is that the digits in tens places are 1, 6, or numerically close. Hence, we can infer that _the activation sites \(a^{0,0}\) and \(a^{0,3}\) encode hundreds and tens place in the input operands respectively;_ the latter is needed to provide carry to A1. Also, the samples show that the activations encode commutativity since the digits at hundreds and tens place are swapped between the two operands. In **(c)**, the output of the attention layer after residual connection combining information from the sites in (a) and (b) _encodes “6” and “8” in hundreds place, and the carry from tens place_. Note that \(a^{0,1}\) and \(a^{0,2}\) contains similar information as \(a^{0,0}\). These observations are confirmed across inputs. Taken together, InversionView reveals how information is aggregated and passed on by different model components.

Figure 27. We can summarize the function of layer 0 at "=": _Three heads route F1 and S1 to the residual stream of "=" \(x_{=}\). The fourth head routes the carry resulting from F2 and S2_. Layer 1 mainly forwards information already obtained in layer 0, and does not consistently add further information for A1. See more examples in Appendix F.2.

Figure 5(a) shows the circuit predicting A1. InversionView allows us to diagnose an important deficiency of this circuit: Even though the ones place sometimes receives attention in layer 1, the circuit does not consistently provide the carry from the ones place to the hundreds place, which matters on certain inputs--we find that this deficiency in the circuit accounts for _all_ mistakes made by the model (Appendix F.3). Taken together, we have provided a circuit allowing the model to predict A1 while also understanding its occasional failure in doing so correctly. Corresponding findings for A2, A3, and A4 are in Table 3 and Figure 31. From A2 onwards, InversionView allows us to uncover how the model exhibits two different algorithms depending on whether the resulting output will have 3 or 4 digits. In particular, when predicting A3, the layer 0 circuit is the same across both cases, while the layer 1 circuit varies, since this determines whether A3 will be a tens place or ones place. Beyond figures in the Appendix, we encourage readers to verify our claims in our interactive web application.

Quantitative verification.We used causal interventions to verify that information about the digits in hundreds and tens place is routed to the prediction of A1 only through the paths determined in Figure 5(a), and none else. Like before, we cumulatively patch the head output on "=" preceding the target token A1, with an activation produced at the same activation site by a contrast example changing both digits in a certain place. Results shown in Figure 5(b) strongly support our previous conclusions. For example, \(a^{0,3}\) and \(a^{1,2}\) are not relevant to F1 and S1. Important heads detected by activation patching, \(a^{0,0}\), \(a^{0,1}\), \(a^{0,2}\), \(a^{1,1}\), all contain F1 and S1 according to Figure 5(a). Furthermore, we can also confirm that \(a^{1,1}\) relies on the output of layer 0 as depicted in sub-figure (a): When heads in layer 0 are already patched, patching \(a^{1.1}\) has no further effect (value corresponding to \(\) is zero), but it has an effect when patching in the opposite direction. On the contrary, \(a^{1,0}\) shows little dependence on layer 0, consistent with Figure 5(a). On the right of Figure 5(b), we can confirm that \(a^{0,3}\) is important for routing F2 and S2, and the downstream heads in layer 1 rely on it. Findings for other answer digits are similar (See Appendix F.5). Overall, the full algorithm obtained by InversionView is well-supported by causal interventions.

Figure 6: **3-Digit Addition Task: (a)** Information flow diagram for predicting A1 inferred via InversionView. The colors denote which places are routed; alternating colors indicate two places are routed. This is a subfigure of Figure 31. (b) Validation of (a) via activation patching for the prediction of A1. Like Figure 5(a), \(\) (\(\)) means cumulatively patching activation from left to right (right to left) on the horizontal axis. **Left:** Patching with activation containing modified F1 and S1 information. **Right:** Patching with activation containing modified F2 and S2 information. As we can see, components from (a) show a substantial increment if and only if they have a not-yet-patched connection to output (when patching right to left) or input (patching left to right), verifying that (a) causally describes the flow of information. Therefore, InversionView helps us uncover both information flow and content of activations.

InversionView reveals granularity of information.Heads often read from both digits of a place, but only the sum matters for addition. Are the digits represented separately, or only as their sum? Unlike traditional probing, InversionView answers this question without designing tailored probing tasks. In Figure 7 (left), \(a^{0,2}\) exactly represents F2 and S2 (here, 2 and 5). Other inputs where F1+S1=5+2 have high \(D\). In contrast, on the right, F2 and S2 are represented only by their sum: throughout the \(\)-preimage, F2+S2=9. In fact, we find such sum-only encoding only when F2+S2=9--a special case where the ones place of operands affects the hundreds place of the answer via cascading carry. We hypothesize that the model encodes them similarly because these inputs require special treatment. Therefore, even though encoding number pairs by their sum is a good strategy for the addition task from a human perspective, the model only does it as needed. We also observe intermediate cases (Figure 29).

### Factual Recall

To test whether InversionView can be applied to larger language models, we explore how GPT-2 XL (1.5B parameters) performs the task of recalling factual associations. In this case study, our intention is not to provide a full interpretation of the computations performed to solve this task, which we deem out of scope for this paper. Instead, we show that InversionView produces interpretable results on larger models by focusing on a relatively small set of important attention heads in upper layers. The decoder model in this case study is based on GPT-2 Medium, because we expect a more complex inverse mapping from activation to inputs to be learned. We observe the resulting \(\)-preimage can express high-level knowledge (Figure 39-44), and sometimes can predict the failure of the model (Appendix H.6). Using InversionView, we again shed light on the underlying mechanism of the model. We present detailed findings in Appendix H.

## 4 Discussion and Related Work

Comparison with other Interpretation MethodsSupervised **probing classifiers**, assessing how much information about a variable of interest is encoded in an activation site, are arguably the most common method for uncovering information from activations [e.g. 2, 6, 5, 4, 55, 52, 33, 34]. It requires a hypothesis in advance and is thus inherently limited to hypotheses conceived a priori by the researcher. InversionView, on the other hand, helps researchers form hypotheses without any need for prior guesses, and allows fine-grained per-activation interpretation. Inspecting attention patterns [e.g. 12] is a traditional approach to inferring **information flow**, and we have drawn on it in our analyses. More recently, path patching  causally identifies _paths_ along which information flows. While the information flow provides an _upper bound_ on the information passed along by tracing back to the input token, it is insufficient for determining how information is processed and abstracted. For instance, in Section 3.1, occurrences of the target character are causally connected to \(a_{te}^{0,0}\), which then connects to \(a_{:}^{1,0}\) (direct or mediated by MLP layer 0). Without looking at encoded information, we only know that the information in these paths is related to the occurrences of the target character, but not whether it is their identity, positions, count, etc. More generally, when a component reads a component that itself has read from multiple components, connectivity does not tell us which pieces of information are passed on. Similar considerations apply to other intervention methods.

Figure 7: **3-Digit Addition Task**: InversionView uncovers different ways in which digit representation is encoded in activations. **Left:** The digits in the hundreds place are encoded separately and hence generations denote them as separate entities. **Right:** The digits in the tens place are encoded as a sum (9 in this case) and the generations represent different 2-partitions (7+2, 6+3, 1+8, 5+4, etc.) of that sum.

Geva et al.  intervene on attention weights to study information flow. Activation patching, a causal intervention method, can be used to study the causal effect of an activation on the output, and can help localize where information is stored [37; 49], or find alignment between a high-level causal model and inner states of a neural model [21; 22; 57]. Many recent works obtain insights about information content by projecting representations or parameters into the **vocabulary space**[42; 7; 44; 31; 54; 23; 24; 16]. This technique is sometimes referred to as Direct Logit Attribution (DLA). We argue that DLA is only suitable for studying model components that _directly_ affect model's final output. For those components whose effect is mediated by other components, their output information is meant to be read by a downstream component, thus not necessarily visible when projecting to the vocabulary space. We provide further discussion in Appendix I. Generalizing this approach, Ghandeharioun et al.  patch activations into an LLM. Another line of recent research [10; 51; 14] decomposes activations into interpretable features using **sparse autoencoders**.

Some other interpretation methods also generate in input space, but differ from InversionView in goals and methods. This includes feature visualization [43; 41], adversarial or counterfactual example generation [27; 59; 46; 45], and GAN inversion methods . We discuss the similarities and differences of these works compared to InversionView in Appendix I.4.

InversionView offers distinctive advantages and makes analyses feasible that are otherwise very hard to do with other methods. It can also improve the interpretability workflow in coordination with other methods. For example, one may first use methods such as path patching or attribution [50; 18] to localize activity to specific components, and then understand the function of these components using InversionView. In sum, InversionView is worth adding to the toolbox of interpretability research.

Transformer Circuits for ArithmeticRelated to Section 3.3,  interpret the algorithm implemented by a \(1\)-layer \(3\)-head transformer for \(n\)-digit addition (\(n\{5,10,15\}\)), finding that the model implements the usual addition algorithm with restrictions on carry propagation. In their one-layer setup, attention patterns are sufficient for generating hypotheses. Lengths of operands and results are fixed by prepending \(0\). Our results, in contrast, elucidate a more complex algorithm computed by a _two_-layer transformer on a more realistic version without padding, which requires the model to determine which place it is predicting. We also contribute by providing a detailed interpretation, including how digits are represented in activations.

Automated Interpretation for InversionViewRecent work has started using LLMs to generate interpretations [8; 10]. The samples produced by InversionView can be easily fed into LLMs for automated interpretation. We show a proof of concept by using Claude 3 to interpret the model trained for 3-digit addition. See results in Table 5. The LLM-provided interpretation reflects the main information in almost all cases of the addition task. Despite some flaws, the outcome is informative in general, suggesting this as a promising direction for further speeding up hypothesis generation.

LimitationsInversionView relies on a black-box decoder, which needs to be trained using relevant inputs and whose completeness needs to be validated by counter-examples. Also, InversionView, while easing the human's task, is still not automated, and interpretation can be laborious when there are many activation sites. We focus on models up to 1.5B parameters; scaling the technique to large models is an interesting problem for future work, which will likely require advances in localizing behavior to a tractable number of components of interest. Fourth, interpretation uses a metric \(D(,)\). The geometry, however, in general could be nonisotropic and treating each dimension equally could be sub-optimal. We leave the exploration of this to future work.

## 5 Conclusion

We present InversionView, an effective method for decoding information from neural activations. In four case studies--character counting, IOI, 3-digit addition, and factual recall--we showcase how it can reveal various types of information, thus facilitating reverse-engineering of algorithm implemented by neural networks. Moreover, we compare it with other interpretability methods and show its unique advantages. We also show that the results given by InversionView can in principle be interpreted automatically by LLMs, which opens up possibilities for a more automated workflow. This paper only explores a fraction of the opportunities this method offers. Future work could apply it to subspaces of residual stream, to larger models, or to different modalities such as vision.