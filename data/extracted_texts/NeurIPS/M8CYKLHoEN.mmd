# Fast Rank-1 Lattice Targeted Sampling for Black-box Optimization

Yueming LYU

Centre for Frontier AI Research (CFAR)

Institute of High Performance Computing (IHPC)

Agency for Science, Technology and Research (A*STAR)

1 Fusionopolis Way, #16-16 Connexis, Singapore 138632

Lyu_Yueming@cfar.a-star.edu.sg

###### Abstract

Black-box optimization has gained great attention for its success in recent applications. However, scaling up to high-dimensional problems with good query efficiency remains challenging. This paper proposes a novel Rank-1 Lattice Targeted Sampling (RLTS) technique to address this issue. Our RLTS benefits from random rank-1 lattice Quasi-Monte Carlo, which enables us to perform fast local exact Gaussian processes (GP) training and inference with \(O(n n)\) complexity w.r.t. \(n\) batch samples. Furthermore, we developed a fast coordinate searching method with \(O(n n)\) time complexity for fast targeted sampling. The fast computation enables us to plug our RLTS into the sampling phase of stochastic optimization methods. This improves the query efficiency while scaling up to higher dimensional problems than Bayesian optimization. Moreover, to construct rank-1 lattices efficiently, we proposed a closed-form construction. Extensive experiments on challenging benchmark test functions and black-box prompt fine-tuning for large language models demonstrate the query efficiency of our RLTS technique.

## 1 Introduction

Black-box optimization has gained great attention for its success in many recent applications, such as prompt fine-tuning for large language models (Sun et al., 2022; 20; 20), policy search for robot control and reinforcement learning (Choromanski et al., 2019; Lizotte et al., 2007; Barsce et al., 2017; Salimans et al., 2017), automatic hyper-parameters tuning in machine learning problems (Snoek et al., 2012), black-box architecture search in engineering design (Wang and Shan, 2007), drug discovery (Negoescu et al., 2011) and accelerated simulation for scientific discovery (Maddox et al., 2021; Hernandez-Lobato et al., 2017), etc. Many efforts have been made for black-box optimization in the literature, including Bayesian optimization (BO) methods (Srinivas et al., 2010; Gardner et al., 2017; Nayebi et al., 2019), stochastic optimization methods like evolution strategies (ES) (Back et al., 1991; Hansen, 2006; Wierstra et al., 2014; Lyu and Tsang, 2021) and genetic algorithms (Srinivas and Patnaik, 1994; Mirjalili and Mirjalili, 2019).

Bayesian optimization usually builds a global (GP) model as a surrogate and provides queries by optimizing some acquisition functions (Snoek et al., 2012). Although BO achieves good query efficiency for low-dimensional problems, it often fails to handle high-dimensional problems with large sample budgets (Eriksson et al., 2019). The computation of GP with a large number of samples itself is expensive, and the internal optimization of the acquisition functions is challenging. Recently, Muller et al. (2021); Nguyen et al. (2022) builds a GP model for both the function value and the gradient and performs local Bayesian optimization. Although these methods improve the scalability of global BO, they usually cannot scale up to five hundred dimensional complex problems. This maybe because the learned gradient heavily depends on the accuracy of the GP model. However, achieving an accurate GP model is challenging for high-dimensional problems. A slightly misspecified GP model may lead to a wrong estimated gradient due to the highly nonlinear acquisition functions.

On the other line, stochastic optimization methods, e.g., ES (Rechenberg and Eigen, 1973; Nesterov and Spokoiny, 2017), natural evolution strategies (NES) (Wierstra et al., 2014b), CMAES (Hansen, 2006), and implicit natural gradient optimizer (INGO) (Lyu and Tsang, 2021), typically sampling form Gaussian distribution and approximate the (natural) gradient for the update of the Gaussian distribution parameters for continuous optimization. These methods can scale up to higher dimensional problems compared with BO. However, the gradient approximation may have a large variance, especially for high-dimensional problems. Thus, the update direction may not be toward the descent direction, leading to inferior query efficiency.

To address high-dimensional black-box problems with good query efficiency, we propose a novel Rank-1 Lattice Targeted Sampling (RLTS) technique. Our RLTS has a \(O(n n)\) time complexity, which is fast for plugging into the sampling phase of stochastic optimization methods. In this way, our methods can improve the query efficiency of stochastic optimization methods while addressing higher-dimensional problems than BO. Our contributions are summarized as follows:

* We propose a novel Rank-1 Lattice Targeted Sampling (RLTS) technique. Our RLTS builds a local GP with a random rank-1 lattice, which enables fast exact GP training and inference with \(O(n n)\) time complexity w.r.t. \(n\) batch samples. Furthermore, we develop a fast coordinate search that enables target sampling with \(O(n n)\) time complexity.
* We propose a closed-form subgroup rank-1 lattice by considering the dual lattice regarding the integral approximation error of functions in Korobov space. Our rank-1 lattice has a more regular pattern of approximation error terms. Moreover, our subgroup rank-1 lattice capitalizes on constructing a circulant kernel Gram matrix benefit from its group property. This enables efficient \(O(n n)\) computations in GP training/inference and fast candidate searching. In contrast, low-discrepancy QMC sequences, such as Sobol sequences or Halton sequences, lack these capabilities. In addition, our new closed-form rank-1 lattice may have potential applications in downstream tasks beyond black-box optimization.
* We plug our RLTS into the sampling phase at each step of stochastic optimization methods to improve query efficiency. In this way, during the optimization procedure, our RLTS sampling from an updated promising region instead of a fixed one at each step. This approach can scale up to address high-dimensional problems.
* Empirically, extensive experiments on high-dimensional challenging benchmark test functions and practical black-box prompt fine-tuning for large language models demonstrate the effectiveness of our RLTS technique.

## 2 Background

### Black-box Optimization

Given a proper function \(f():^{d}\) such that \(f()>-\), black-box optimization is to minimize \(f()\) by using function queries only. Black-box stochastic optimization methods typically employ a sampling distribution \(p(;)\) and optimizes the parameter of the distribution regarding the relaxed problem: \(J():=_{p(;)}[f()]\).

Evolution Strategies (ES) (Rechenberg and Eigen, 1973; Nesterov and Spokoiny, 2017) employ a Gaussian distribution \((,^{2})\) for sampling. The approximate gradient descent update is given as

\[_{t+1}=_{t}-_{i=1}^{n}_ {i}f(_{t}+_{i}),\] (1)

where \(_{i}(,)\) and \(\) denotes the step-size. The ES method performs the approximate first-order gradient descent update. As a result, the convergence of ES may be slow. Several second-order gradient descent methods have been proposed to improve convergence. Wierstra et al. (2014a) proposed the natural evolution strategies (NES), which perform the approximate natural gradient update. When a Gaussian distribution \((,)\) is employed for sampling. The update rule of NES is given in Eq.(2) and Eq.(3):

\[_{t+1}=_{t}-_{i=1}^{n}f(_{t}+_{t}^{}_{i})(_{t}^{ }_{i}_{i}^{}_{t}^{}-_{t})\] (2) \[_{t+1}=_{t}-_{i=1}^{n}f(_{t}+_{t}^{}_{i})_{t}^{} {}_{i}.\] (3)

where \(_{i}(,)\) and \(^{}=^{}\) and \(^{}^{}=\). The NES takes advantage of second-order gradient information, which improves the convergence of ES.

Lyu and Tsang (2021) proposed an implicit natural gradient optimizer (INGO) for black-box optimization, which provides an alternative way to compute the natural gradient update. The update rule of INGO is given as in Eq.(4) and Eq.(5):

\[_{t+1}^{-1}=_{t}^{-1}+_{i=1}^{n} _{i})-}{n}(_{t}^{-1 }(_{i}-_{t})(_{i}-_{t})^{}_{t}^{-1 })\] (4) \[_{t+1}=_{t}-_{i=1}^{n}_{i })-}{n}(_{i}-_{t}).\] (5)

where \(_{i}(_{t},_{t})\), \(=^{n}f(_{i})}{n}\) and \(\) denotes the standard deviation of \(f(_{i})\). The normalization \(_{i})-}{}\) is employed to reduce the variance.

CMAES (Hansen, 2006) provides a more sophisticated update rule and performs well on a wide range of black-box optimization problems. All the above stochastic optimization methods rely on sampling. Thus, the sampling phase is vitally important. And a better sampling technique is promising to achieve further improvement.

### Rank-1 Lattice

A rank-1 lattice is a particular case of the general lattice with a simple operation for point-set construction. It can be used as Quasi-Monte Carlo for integral approximation (Sloan, 2000; Dick et al., 2013). A rank-1 lattice point set \(=\{_{1},,_{n}\}\) can be constructed as Eq.(6):

\[_{i}:=n}{n},i\{1,,n\},\] (6)

where \(^{d}\) is the so-called generating vector, and mod denotes the modulo operation.

Korobov (1960) proposes a rank-1 lattice with the generating vector having a particular form as Eq.(7)

\[:=[1,k,,k^{d-1}]n,\] (7)

where \(k\) is searching over \(\{1,,n-1\}\) to reduce approximation error.

Sloan and Reztsov (2002) further proposed a component-by-component searching method for the generating vector without assuming the Korobov form in Eq. (7). Recently, Lyu et al. (2020) proposed a simple closed-form subgroup-based rank-1 lattice by considering the Toroidal distance in the primal lattice space. The generating vector is given as Eq.(8)

\[=[g^{0},g^{},g^{},,g^{}]n,\] (8)

where \(g\) denotes the primitive root modulo the prime number \(n\). More details of the lattice rules for numerical integration can be found in the book (Dick et al., 2022).

In this paper, we proposed a closed-form subgroup rank-1 lattice by ensuring the approximation error terms of the dual lattice have a more regular pattern. In contrast, Lyu et al. (2020) construct the rank-1 lattice evenly spaced in the primal lattice space.

## 3 Fast Rank-1 Lattice Targeted Sampling

### Random Rank-1 Lattice Quasi-Monte Carlo Gaussian Sampling

We first show how to construct random rank-1 lattice Quasi-Monte Carlo Gassuain samples. These samples enable us to perform the black-box stochastic optimization listed in section 2.1. More importantly, the nice property of the structure of these samples facilitates a fast targeted sampling.

Given a rank-1 lattice point set \(=\{_{1},,_{n}\}\), we first construct a random shifted rank-1 lattice (Dick et al., 2013) as Eq. (9),

\[}_{i}=_{i}+\ \ 1\ \  i\{1,,n\},\] (9)

where \( Uniform^{d}\), and the mod \(1\) operation denotes a modulo operation that takes the non-negative fractional part of the input number element-wise. Then, we can construct random QMC Gaussian samples as Eq. (10)

\[_{i}=^{-1}(}_{i})\ \  i\{1,,n\},\] (10)

where \(^{-1}()\) computes the inverse cumulative density function of the standard Gaussian distribution w.r.t. the input element-wise. Then, the samples for Gaussian \((,)\) can be constructed as follows:

\[_{i}=+^{}_{i}.\] (11)

An illustration of the random QMC Gaussian samples constructed by our closed-form rank-1 lattice is shown in Figure 1. We can see that our rank-1 lattice QMC Gassuan samples are spaced more evenly w.r.t. the density.

### Fast Exact GP Training and Inference with Rank-1 Lattice

This subsection will show how to perform fast exact GP training and inference using our rank-1 lattice samples with a \(O(n n)\) time complexity w.r.t \(n\) samples.

Let \(_{}\) denotes the kernel Gram matrix, i.e., \(_{}=[k_{}(_{i},_{j})]_{1 i,j n}\), the marginal log-likelihood of a GP model (Williams and Rasmussen, 2006) can be formulated as Eq. (12)

\[(p(|))=-^{}(_{}+ ^{2})^{-1}-(|_{}+^{2}|)-  2.\] (12)

Figure 1: Illustration of the our closed-form Rank-1 Lattice sampling and i.i.d. Gaussian sampling.

The standard GP model needs a \(O(n^{3})\) time complexity to compute the marginal log-likelihood, which is prohibitive for fast training as an inner step for stochastic optimization.

In this paper, we construct the random QMC samples based on rank-1 lattice, which enables us to perform fast GP training. Specifically, we build the GP model with the rank-1 lattice as the training data instead of the Gaussian samples. Define modulo kernel as Eq. (13):

\[k(_{i},_{j}):=k_{}((_{i}-_{j})),\] (13)

where \(k_{}()\) is a shift-invariant kernel, and the function \((_{i}-_{j})\) is given as Eq. (14)

\[(_{i}-_{j})=(_{i}-_{j})1,-(_{i}-_{j})1,\] (14)

where operation \((,)\) outputs the minimum among its two inputs element-wise, and mod \(1\) output the positive fractional parts of its inputs element-wise. The nonnegative fractional part of a real number \(x\) is \(x- x\), where \(\) denotes the floor function.

For a GP model with a modulo kernel defined in Eq.(13), the kernel Gram matrix is a circulant matrix thanks to the properties of rank-1 lattice. To be concrete, for rank-1 lattice data, we have Eq.(15)

\[k(_{i},_{j})=k(_{i+1},_{j+1})=k_{} n}{n},-n}{n}.\] (15)

Then the marginal log-likelihood \((p(|))\) can be computed with a \(O(n n)\) time complexity by Fast Fourier Transform (FFT).

Specifically, note that the kernel Gram matrix \(_{}+^{2}\) is a symmetric circulant matrix generated by vector \(_{}\), where \(_{}\) is a vector with its \(i^{th}\) element given as Eq. (16).

\[k_{ i}=k_{}n}{n},-n}{n}.\] (16)

We know that \(_{}+^{2}\) can be diagonalized as \(_{}+^{2}=F^{*} F\), where the \(j^{th}\) row and \(k^{th}\) column element of \(F\) is \(F_{jk}=e^{-2 j/n}\). And the matrix \(\) is the diagonal eigenvalue matrix that can be computed as \(=(F_{})\). The matrix-vector product \(F_{}\) can be computed via FFT with \(O(n n)\) time complexity. And matrix-vector product \(F^{*}\) for a vector \(\) can be computed via inverse FFT. More details about the properties of circulant matrices and fast computation via FFT can be found in [Gray et al., 2006].

Then, we achieve the fast computation of the terms in log-likelihood as Eq.(17) and Eq.(18):

\[^{}(_{}+^{2})^{-1}=^{}(()/(_{}))\] (17) \[(_{}+^{2})=_{i=1}^{ n}(_{i}+^{2})=^{}(_{ }),\] (18)

where \(()\), \(()\) denotes the inverse FFT and FFT operation, respectively, the operator \(/\) in Eq.(17) performs divide element-wise. And the \(()\) is an element-wise operation. And \(_{i}\) in Eq.(18) denotes the eigenvalue of kernel Gram matrix \(_{}\).

For inference, GP model has closed-form posterior mean and variance [Williams and Rasmussen, 2006] given as Eq.(19) and Eq.(20) :

\[() =_{}()^{}(_{}+^{2}I)^{-1} \] (19) \[^{2}() =k_{}(,)-_{}()^{}(_{ }+^{2}I)^{-1}_{}(),\] (20)

where \(_{}()=[k_{}(,_{1}),...,k_{}(,_{n})]^{}\).

With rank-1 lattice input data, we can perform fast inference by Eq.(21) and Eq.(22):

\[() =_{}()^{}(()/ (_{}))\] (21) \[^{2}() =k_{}(,)-_{}()^{}((_{}())/(_{})).\] (22)

Both the exact GP training and inference benefit from the structure of rank-1 lattice and FFT acceleration, which can be performed with a \(O(n n)\) time complexity. A deep learning toolbox, e.g., Pytorch, can be used to train the parameters of the kernel.

### Fast Coordinate Search for Targeted Sampling

This subsection shows how to perform a fast coordinate search for targeted sampling. A rank-1 lattice with \(n\) points is contained in a grid \(\{0,,,\}^{d}\). We thus perform a coordinate descent search from the index set \(\{0,1,,n-1\}^{d}\) to minimize the GP posterior mean in Eq.(19).

Let \(k(,)=k_{}()\) be a shift-invariant kernel with a decomposition structure as Eq. (23):

\[k(^{*},)=k_{}((^{*}-))=_{q=1}^{d}k_{ }((x_{q}^{*}-x_{q})),\] (23)

where \(x_{q}^{*}\), \(x_{q}\) denotes the \(q^{th}\) element in \(^{*}\), \(\), respectively. We can perform a coordinate search by fixing all the components except the \(q^{th}\) one as the current working component for index searching. Formally, let \(=(_{}+^{2}I)^{-1}\). Then, we have the GP posterior mean function given as Eq. (24):

\[(^{*})=_{}^{q}(x_{q}^{*})}_{}^{q},\] (24)

where \(\) denotes the element-wise product, and \(_{}^{q}(x_{q}^{*})\) denotes a vector with \(i^{th}\) element given as \(_{ i}^{q}=k_{}((x_{q}^{*}-_{qi}))\), and \(_{qi}\) denotes the element in \(q^{th}\)-row and \(i^{th}\)-column of the rank-1 lattice matrix \(=[_{1},,_{n}]\). The vector \(}_{}^{q}\) denotes the remainder vector with its \(i^{th}\)-element given as Eq. (25):

\[}_{ i}^{q}=((x_{q}^{*}-_{qi }))}_{q=1}^{d}k_{}((x_{q}^{*}-_{qi})).\] (25)

To optimize the \(q^{th}\) component \(x_{q}^{*}\) of \(^{*}\), we fix the other components of \(^{*}\) and the corresponding vector \(}_{}^{q}\). We find \(x_{q}^{*}\) by solving the subproblem given in Eq. (26)

\[x_{q}^{*}=*{arg\,min}_{x\{0,,n-1\}}_{}^{q} (x)^{}}_{}^{q}.\] (26)

Directly enumerate computation of the problem (26) needs a \(O(n^{2})\) time complexity. In our paper, we can perform a fast computation with \(O(n n)\) time complexity thanks to the rank-1 lattice \(\). Specially, when \(\) is a rank-1 lattice with the generating vector \(=[z_{1},,z_{d}]\), then the matrix \(_{}^{q}=[_{}^{q}(0),_{}^{q}(  n)}{n},,_{}^{q}( n}{n})]\) forms a circulant matrix, and the problem (26) can be accelerated via FFT by Eq. (27)

\[^{q}=_{}^{q}}_{}^{q} =((_{}^{q}(0))( }_{}^{q})),\] (27)

where \(()\) and \(()\) denote the FFT and inverse FFT operation. Then, we can achieve \(x_{q}^{*}\) by the index \(i^{*}\) of the minimum element in vector \(^{q}=_{}^{q}}_{}^{q} \), and set \(x_{q}^{*}=z_{q} n}{n}\).

We present the algorithm of the fast coordinate search in Algorithm 1. The Algorithm 1 return a targeted sample with a small prediction value in a fast manner. We can use the targeted sample to accelerate the stochastic optimization. Finally, we present our overall stochastic optimization algorithm in the Algorithm 2. We choose INGO (Lyu and Tsang, 2021) as our backbone algorithm because of its simple implementation and fewer hyperparameters. One can plug our RLTS into other stochastic optimization methods to improve query efficiency.

### Closed-form Rank-1 Lattice Construction

This subsection will show how to construct our closed-form rank-1 lattice for fast sampling. For \(,^{d}\) and \(>1\), define a reproducing kernel as Eq. (28)

\[K(,)=_{^{d}}_{}()( 2^{}(-)),\] (28)

where \(^{2}=-1\) and \(_{}()=_{j=1}^{d}_{}(k_{j})\) with \(_{}(k)\) is given as follows:

\[_{}(k)=1&k=0\\ |k|^{-}&k 0.\] (29)

A Korobov space is a reproducing kernel Hilbert space (RKHS) associated with the kernel in Eq.(28), denoted as \(_{k}\).

Our closed form of the generating vector is given as Eq.(30):

\[=[g^{0},g^{},g^{},,g^{}] n,\] (30)

where \(g\) denotes the primitive root modulo the prime number \(n\), and \((2d-1)|(n-1)\). Then, our close-form rank-1 lattice can be achieved by Eq. (6)

Given a point set \(=\{_{1},,_{n}\}\), the square worst case integral approximation error for \(f_{k}\) is defined as Eq.(31):

\[e^{2}(_{k};)=_{f_{k},\|f\|_{_{k}} 1}_{^{d}}f()- _{j=0}^{n-1}f(_{j})^{2}.\] (31)

We further show that our rank-1 lattice constructed by Eq. (30) has a regular worst-case error pattern in Theorem 1. The proof is given in the Appendix.

**Theorem 1**.: _Let \(n\) be a prime number such that \((2d-1)|(n-1)\). Suppose the integrand function \(f_{k},\|f\|_{_{k}} 1\), the square worst-case integral approximation error of rank-1 lattice \(\) constructed by Eq.(30) is given as Eq.(32):_

\[e^{2}(_{k};)=^{}(^{0} ^{2d-2}--(^{1}^{ d-1}-)^{0}(^{-1}^{-(d-1)}- ))+}(,1),\] (32)_where \(\) denotes the element-wise product, symbol \(\) denotes the vector with elements all ones, and \(^{i}=^{i}\) with \(\) as the discrete Fourier matrix, i.e., \(_{jk}=(2)\), and \(^{i}\) denotes the matrix after permutation of the rows of \(\) such that the \(j^{th}\) row of \(^{i}\) equals to the \(^{th}\) row of \(\), where \(=jg^{}\) mod \(n\). And \(=[_{1},,_{n}]^{}\) with \(_{k}=}(,}{n})+( ,}{n})\) for \(k\{1,,n-1\}\) and \(_{n}=1+}(,1)\), where \((,)\) is the Hurwitz zeta function._

**Remarks:** The term \(H=^{0}^{2d-2}-( ^{1}^{d-1}) ^{0}(^{-1}^{-( d-1)})\) has a regular pattern because of \(\{g^{0},g^{},g^{},,g^{},,g^{}\}\) mod \(n\) forms a subgroup of \(\{1,,n-1\}\) mod \(n\). According to the Lagrange's theorem in group theory [Dummit and Foote, 2004], the vector \(^{0}^{2d-2}\) has \(\) different elements.

## 4 Experiments

We replace the i.i.d. Gaussian sampling of the INGO [Lyu and Tsang, 2021] with our RLTS. We evaluate our RLTS by comparing it with the standard INGO and the CMAES [Hansen, 2006]. In all the experiments, we keep the number of batch samples and the initialization the same for RLTS, INGO and CMAES. For all the methods, we initialize the \(=\). For INGO and RLTS, we set the step-size parameter \(=0.2\) in all experiments. For RLTS, we set the parameter \(=1\) in all experiments.

### Evaluation on Benchmark Functions

We first evaluate our RLTS on challenging benchmark test functions: Rosenbrock, Rastrigin, and Nesterov. Rastrigin and Rosenbrock are smooth multi-mode functions, and Nesterov is a non-smooth function. These functions are very challenging benchmarks for black-box optimization. We offset the optimum by setting \(=-5\) of the test functions. This increases the distance between the optimum and the initial point \(=\), which makes the test problems more challenging. We implement INGO by ourselves. For CMAES, we use the publicly available code 2. We initialized \(=\) for all the methods.

Figure 2: Cumulative min objective value v.s. the number of query evaluations on 50-dimensional and 500-dimensional benchmark test functions.

We evaluate RLTS on 50 and 500-dimensional problems. The batchsize of all the methods are set to 200 and 2000 for 50 and 500-dimensional problems, respectively. All the experiments are performed in ten independent runs. The experimental results are shown in Figure 2. From Figure 2, we can observe that RLTS consistently converge faster than INGO on all the test functions on both 50-dimensional and 500-dimensional cases. It shows that our RLTS significantly improves the query efficiency of INGO, which verifies the effectiveness of RLTS. Moreover, we can see that RLTS outperforms CMAES on all the test functions on both 50-dimensional and 500-dimensional cases. In addition, we see that CMAES converge slowly on the 500-dimensional benchmark problems, while RLTS converges faster.

### Evaluation on Black-box Prompt Fine-tuning Tasks

Prompt fine-tuning of large language models is a promising direction to achieve expertise models efficiently for downstream tasks. We evaluate our RLTS on black-box prompt fine-tuning tasks.

We employ the deep model in  with publicly available code 3 as the backbone model for black-box prompt fine-tuning. It has \(24\) layers. For each layer, we set the dimension of the continuous prompt to \(500\). Thus, the total dimension is \(24 500\). We employ the hinge loss of training data as the black-box objective. Six benchmark datasets for different language tasks are employed for evaluation: DBpedia, SS2, SNLI, AG's News, MRPC and RTE. The SST2  dataset is a dataset for the sentiment analysis task. AG's News and DBPedia datasets  are used for topic classification tasks. SNLI  and RTE  are employed for natural language inference. MRPC dataset  is used for the paraphrasing task.

In all the experiments, we keep the number of batch samples and the initialization the same for RLTS, INGO and CMAES. We set the number of batch samples to \(2000\). Specifically, our RLTS employs \(1999\) rank-1 lattice QMC Gaussian samples and one sample from targeted sampling. INGO employs \(1999\) rank-1 lattice QMC Gaussian samples and one Gaussian sample. CMAES employs \(2000\) Gaussian samples. We initialize the \(=\) and \(=0.2\) for all the methods. For INGO and RLTS, we set the step-size parameter \(=0.2\) in all experiments. For RLTS, we set the parameter \(=1\) in all experiments. All the experiments are performed in five independent runs with seeds in \(\{1,2,3,4,5\}\). The layer-wise coordinate descent update approach in  is employed for all the methods.

Figure 3: Hinge loss v.s. the number of query evaluations on different black-box fine-tuning models.

The experimental results of mean objective \(\) std v.s. the number of queries are shown in Figure 3. From Figure 3, we can observe that our RLTS decreases the objective significantly faster than INGO and CMAES on all six fine-tuning tasks, which shows the superior query efficiency of our RLTS.

### Additional Comparison with High-dimensional Bayesian Optimization

We further compare our RLTS with the high-dimensional BO method TuRBO (Eriksson et al., 2019). We evaluate RLTS on the three benchmark functions: the Rosenbrock function, the Rastrigin10 function, and the Nesterov function. We offset the optimum by setting \(=-5\) of the test functions. The dimension is set to 500. The number of initial points of TuRBO is set to 2000. The batch size of both RLTS and TuRBO is set to 2000. The maximum number of queries is set to 50,000. We employ the default box boundary for TuRBO, i.e., \([-5,10]^{d}\). The initial parameter \(\) of RLTS is set to \(=\), and \(\) is set to \(=\). For TuRBO, we employ the official code provided in the paper (Eriksson et al., 2019). All the methods are performed in three independent runs.

The convergence performance regarding the number of query evaluations is shown in Figure 4. We can observe that RLTS converges faster than TuRBO on the benchmark test problems, demonstrating that RLTS improves query efficiency.

We further report the running time of RLTS and TuRBO on the same machine for evaluation. The results are shown in Table 1. We can observe that RLTS performs significantly faster than TuRBO, achieving around 300 times speedup regarding running time. The computation time of Bayesian Optimization usually grows cubically fast as the number of queries increases. In contrast, our RLTS reduces the expensive \(O(n^{3})\) operation to \(O(n n)\) time complexity, which enables a fast plug-in of the ES-type algorithms.

## 5 Conclusion

We proposed a novel Rank-1 Lattice Targeted Sampling technique in this paper. Our RLTS has a \(O(n n)\) time complexity w.r.t. \(n\) batch samples, which is fast for plugging into stochastic optimization methods to improve query efficiency while scaling up to high-dimensional problems. Empirically, we plugged our RLTS into the sampling phase of INGO, significantly improving the query efficiency on benchmark test functions and black-box prompt fine-tuning tasks. Moreover, we proposed a closed-form rank-1 lattice by analyzing the integral approximation error of functions in Korobov space. Our closed-form rank-1 lattice provides an efficient way for QMC Gaussian sampling, with properties enabling fast exact GP training and inference with a \(O(n n)\) time complexity, which is critical for our RLTS to be a fast internal step for stochastic optimization. In addition, our closed-form rank-1 lattice is a fundamental tool that may have potential applications beyond the black-box optimization task.

   & Rosenbrock & Rastrigin10 & Nesterov \\  RLTS & 83.62(s) & 84.04(s) & 83.46(s) \\  TuRBO & 25927.39(s) & 25941.66(s) & 25697.87(s) \\  

Table 1: Running time on benchmark test functions. Symbol (s) denotes seconds.

Figure 4: Cumulative min objective value v.s. the number of query evaluations on 500-dimensional benchmark test functions.