# Two Students: Enabling Uncertainty Quantification in Federated Learning Clients

Cristovao Freitas Iglesias Jr

University of Ottawa

&Sidney Alves de Outeiro

Federal University of Rio de Janeiro

&Claudio Miceli de Farias

Federal University of Rio de Janeiro

&Miodrag Bolic

University of Ottawa

cfrei096@uottawa.ca 1 https://cristovaoiglesias.github.io/personalwebsite/

###### Abstract

Federated Learning (FL) is a paradigm where multiple clients collaboratively train models while keeping their data decentralized. Despite advancements in FL, uncertainty quantification (UQ) on the client side remains poorly explored. Existing methods incorporating Bayesian approaches in FL are often resource-intensive and do not directly address client-side UQ. In this paper, we propose the 2S (two students) approach to address this gap. Our approach distills a Bayesian model ensemble (BME) into two student models: one focused on accurate predictions and the other on uncertainty quantification. The 2S approach also includes a novel truncation filter that uses credible intervals to selectively aggregate client models, mitigating the impact of non-i.i.d. data. Through empirical validation on a regression task, we demonstrate that the 2S approach enables effective and scalable UQ on the client side, providing robust and reliable updates across decentralized data sources.

## 1 Introduction

Federated Learning (FL) is a machine learning approach in which distributed clients with isolated data work together to solve a learning problem, typically under the guidance of a central server . Due to the need to ensure reliable and robust decision-making by accurately assessing the variance in the client model predictions across decentralized and heterogeneous data sources, FL requires uncertainty quantification (UQ) . However, the literature about UQ in FL is very limited . Traditional FL methods like federated averaging (FedAvg)  and its variants primarily  focus on model aggregation across distributed clients but often neglect the explicit incorporation of uncertainty in model predictions . Some recent works have started to integrate Bayesian learning principles into FL framework to capture uncertainty, but they are typically resource-intensive and do not directly address the need for UQ on the client side . For example, Bayesian model ensemble (BME) is used as a method for aggregating the locally trained models from different clients on the server . The BME in FL offers robustness and better uncertainty quantification but makes more complex the process of obtaining a single, definitive global model (easy to use in practice) that reproduces the predictive mean and uncertainty rules of BME . **Motivation and problem specification**. As in FL, where models are trained on data distributed across many clients, estimating uncertainty becomes challenging, particularly due to the limited computational resources available on client devices. Therefore, "cost-effective" uncertainty quantification in FL is still an open problem , where "cost-effective" means allowing clients to perform uncertainty quantification without requiring extensive computational resources or complex procedures.

Consequently, the following question remains unanswered: _How can we obtain global parameters directly from BME to enable a cost-effective uncertainty quantification on the client side?_ **Our contribution**. To address the above problem, we propose the two students (2S) approach. It involves distilling (knowledge distillation ) a BME into two student models (suitable for deployment on various devices) within a FL framework. The first student model, \(_{pred}\), is designed to generate accurate predictions by mimicking the predictive mean of the ensemble. The second student model, \(_{uq}\), focuses on uncertainty quantification by learning from the credible intervals (CIs) generated by the ensemble, which represent the uncertainty in predictions. Our empirical experiments using a classical regression problem show that the 2S approach can enable an efficient and cost-effective uncertainty quantification on the client side while filtering out unreliable client models (through a truncation filter) to ensure robust and consistent updates in a decentralized, non-i.i.d. data environment. The code and data used in this work are available in the GitHub repository 2 to facilitate reproducibility.

**Advantages.** (i) To the best of our knowledge, there is no prior work that combines the idea of using credible intervals from Bayesian ensemble predictions to filter out drifted client models in FL (see the related work in Appendix C). While there are works that focus on distillation or ensemble learning in federated settings, the explicit use of prediction uncertainty to selectively aggregate models represents a unique contribution. (ii) Moreover, by distilling the ensemble uncertainty into \(_{uq}\), the 2S enables a cost-effective UQ by offloading the complexity to the server, allowing clients to operate with a lightweight model that still offers reliable uncertainty estimates. This reduces the complexity of performing UQ directly on client devices. This approach is efficient and scalable, making it suitable for deployment on resource-constrained client devices.

## 2 Background

FedAvg is a standard approach in FL. During the **client training stage**, each client \(i\) has its own local dataset \(D_{i}=\{(_{n},_{n})\}_{n=1}^{N}\), where \(_{n}^{v}\) and \(_{n}^{d}\). The current global model weights are denoted by \(}^{h}\). Each client initializes their local model weights \(_{i}^{h}\) with \(}\) and performs stochastic gradient descent (SGD) for \(K\) steps with step size \(_{l}\). The update rule for the local model \(_{i}\) is \(_{i}_{i}-_{l}(B_{k},_{i})\) where \(\) is the loss function and \(B_{k}\) is the mini-batch sampled from \(D_{i}\) at the \(k\)-th step. After the set of clients \(S\) have updated their local models \(_{i}\), we have the **model aggregation stage**. The server aggregates these models \(\{_{i};i S\}\) into a single global model by performing a weighted average: \(}_{i}|}{|D|}_{i}\) where \(|D|=_{i}|D_{i}|\) is the total size of all clients' data. This updated global model \(}\) is then sent back to the clients for the next round of training. The process repeats for \(R\) rounds. FEDAVG works well when the data \(D_{i}\) on each client is i.i.d. relative to the aggregated data \(D\)[6; 15]. However, in practice, the data is often non-i.i.d., which can degrade the performance of FEDAVG, leading to a global model \(}\) that may drift away from the ideal model \(^{*}^{h}\) (the model trained centrally with all the data). One approach to mitigate model drift is to perform **Bayesian inference**[2; 32; 6; 11] by integrating the outputs of all possible models \(\) according to the _predictive posterior_: \(p(|;D)= p(|;)p(|D)\,d _{m=1}^{M}p(|;^{(m)}),^{(m)} p( |D),\) rather than relying on a single point estimate. As that integral is intractable, it is approximated using M Monte Carlo samples \(^{(m)}\) (global models) from the _posterior_\(p(|D)\). This is the BME approach which can mitigate the model drift by leveraging the diversity in the sampled global models [6; 5].

## 3 2S approach

**Model aggregation in server.** Obtaining samples from the true posterior \(p(|D)\) is virtually impossible, requiring the use of an _implicit sampling distribution_\(q()\) to approximate \(p(|D)\)[18; 11; 26]. The ensemble prediction in this context refers to the aggregated output from multiple global models sampled from the _approximate posterior_\(q() p(|D)\). Therefore, given a new input \(^{*}\), we obtain the _approximate predictive posterior_ as, \((^{*}|^{*},D)_{m=1}^{M}p(^{* }|^{*},^{(m)})\), where \(^{(m)} q()\), [11; 25]. The quality of the \((^{*}|^{*},D)\) depends on the number of samples M and the method employed to generate \(q()\)[11; 25; 26]. Furthermore, the uncertainty of the \((^{*}|^{*},D)\), quantified by the credible intervals (CI), reflects the variance (spread level) in the aggregated output from multiple global models \(^{(m)}\) (ensemble output). There are several ensemble methods for generating \(q()\), such as bagging, and negative correlation learning [9; 25; 26]. We propose using the client training process as a method to generate the implicit sampling distribution of global models, \(q()=(_{},D_{i})\). Here, \(_{}\) represents a global model that reproduces the ensemble prediction rule. In this context, the client training process defines \(q()\), and the trained models \(\{_{i};i S\}\) obtained from the training process serve as samples from \(q()\). In FL, each client typically has its own local dataset \(D_{i}\). As a result, the locally trained models \(_{i}\) are likely to capture different aspects of the global data distribution . This inherent diversity among trained models from clients can serve as a natural sampling mechanism to approximate \(p(|D)\). Using these trained models allows the ensemble to capture a wide range of possible model parameters, reflecting the variability and uncertainty inherent in the data. Furthermore, studies such as those by Lakshminarayanan et al.  and Malinin et al.  demonstrate the effectiveness of using ensembles to approximate \(p(|D)\) in centralized settings. By extending these ideas to the decentralized setting of FL, where diverse client models are naturally available, we can leverage the same underlying principles of uncertainty quantification and robustness. See the extended justification in Appendix B.1. As described before, in FL, a drifted model can be generated when the client training process is performed with non-i.i.d. data. The inclusion of drifted models \(_{i}^{drift}\) in the ensemble of models \(\{^{(m)}\}_{m=1}^{M}\) does not affect the mean of ensemble predictions \(_{m=1}^{M}p(^{*}|^{*},^{(m)})\)[28; 10]. However, it increases the uncertainty, which increases as more drifted models are included in the ensemble . To solve this issue, we propose utilizing credible intervals to dynamically truncate distribution \(q()\) and filter client models. We named this approach the truncation filter, and it is similar to sampling from a truncated distribution. To identify drifted models, we compute the credible intervals \(\{[_{CI}^{(q)},_{CI}^{(q)}]\}_{q=1}^{Q}\) for the ensemble of predictions \(\{\{}_{q}^{(m)}\}_{q=1}^{Q}\}_{m=1}^{M}\) obtained with in-domain inputs \(}=\{}_{q}\}_{q=1}^{Q}\), where each \(}_{q}^{v}\) is part of an i.i.d. dataset \(D_{init}=\{}_{q},}_{q}\}_{q=1}^{Q}\) with \(}_{q}^{d}\). Then, a trained model \(_{i}\) is not considered drifted \(_{i}^{drift}\) if a significant proportion (\(\)) of its predictions \(^{}=\{^{}\}_{q=1}^{Q}\) (given \(}\)) falls within \(\{[_{CI}^{(q)},_{CI}^{(q)}]\}_{q=1}^{Q}\). If \(\) is below a certain threshold \(\) (the truncation filter threshold), the model is not included in the ensemble. Specifically, \(_{CI}^{(q)}^{d}\) and \(_{CI}^{(q)}^{d}\) represent the lower and upper bounds of the credible interval computed for the ensemble of predictions \(\{}_{q}^{(m)}\}_{m=1}^{M}\) at the input \(}_{q}\). Therefore, \(\) < \(\) suggests that a client model was trained on data that is not representative of the overall i.i.d. data distribution seen by the ensemble. Including such a model in the ensemble would increase the uncertainty, thus signaling the presence of model drift. By selecting only those models whose a significant proportion of predictions fall within the credible intervals, we ensure that the global model is constructed from client models whose data distributions are aligned with the overall ensemble thereby minimizing the impact of model drift due to non-i.i.d. data.

**Cost-effective UQ in clients.** We must translate the prediction rule of the ensemble of models \(\{^{(m)}\}_{m=1}^{M}\) into the global model \(_{pred}\) to send back to the clients to continue client training. To this end, we use knowledge distillation [6; 34; 20] to transfer knowledge from a teacher model (the ensemble) to a student model (\(_{pred}\)). We use unlabeled data \(U=\{_{j}\}_{j=1}^{Q}\) at the server to memorize the ensemble prediction rule, by turning \(U\) into a pseudo-labeled set \(_{pred}=\{(_{j},_{j})\}_{j=1}^{J}\), where \(_{j}=_{m=1}^{M}p(|_{j};^{(m)})\). Then, we use \(_{pred}\) as supervision to train \(_{pred}\), aiming to mimic the ensemble prediction rule on \(_{pred}\). As we want to provide cost-effective UQ, we also propose to send back to the clients a global model \(_{ug}\) (second student model) that mimics the uncertainty rule of the ensemble. We turn \(U\) into a pseudo-labeled set \(_{ug}=\{(_{j},CI_{j})\}_{j=1}^{J}\), where \(CI_{j}=\{_{CI}^{(j)},_{CI}^{(j)}\}\) can be the 95% credible interval for the ensemble of predictions \(\{p(|_{j};^{(m)})\}_{m=1}^{M}\) to memorize the ensemble uncertainty rule at the input \(_{j}\). Then, we use \(_{ug}\) as supervision to train \(_{ug}\). See the extended justifications related to \(_{ug}\) in Appendix B.2 and B.3. In addition, the algorithm of 2S approach is in Appendix 1, and the diagram can be seen in Figure 2.

## 4 Experiments

We conducted experiments on a classical regression problem [11; 36], to answer two research questions and understand the effectiveness of the 2S. The details of the experiments are in Appendices B.4 and D. This regression problem is a task that allows us to visualize the uncertainty. It consists of using 2S to enable \(_{pred}\) and \(_{uq}\) to learn (regressing) a cosine function in the interval [-5,9] during 100 communication rounds with clients. Where the train data (server) to build the initial ensemble is corrupted by input-dependent Gaussian noise and limited to the intervals [-4,-3], [-1.62,-0.42], [1.58,2.44],[3.98,4.98], and [6.78,7.38], see Figure 1. The research questions and the respective answers are the following. **Q1**) Can the 2S enable efficient updates of \(_{pred}\) and \(_{uq}\) in the presence of clients with non-i.i.d. data? Yes. Figure 1 shows the predictions done in the server by the ensemble, and in the clients by using \(_{pred}\) and \(_{uq}\). In round 0, their predictions are far from ground truth (red line), which is an expected result . However, after 100 rounds, their predictions converged to the ground truth. It is important to point out that the \(_{pred}\) predictions converged to the ground truth with or without the truncation filter, even though, as expected, the predicted uncertainty via \(_{uq}\) increases without the truncation filter; see the additional results in Appendix E. Furthermore, the \(_{pred}\) predictions demonstrate lower RMSPE values compared to FedAvg, highlighting its superior accuracy; see Table 1 in the Appendix.

**Q2)** Can \(_{uq}\) provide aleatory (uncertainty inherent in the data itself that cannot be reduced) and epistemic (uncertainty due to limited data that can potentially be reduced with more data or better models) uncertainties on the client side? Yes. We compared the \(_{pred}\) and \(_{uq}\) predictions with GP regressions, which can be considered a gold standard for regression uncertainty even though it is less reliable for extrapolation [13; 7; 24]; see Figure 1. If the \(_{pred}\) can capture both types of uncertainty , its prediction intervals after 100 rounds should be reduced and include the ground truth values between the interval [-5,9], such as the GP model after optimization. In plot (B), we can see that \(_{pred}\) captures these uncertainties, which is visible as the width of the prediction intervals around the data points where the noise level in the data is consistent. It is important to note that the predicted mean by \(_{pred}\) is closer to the ground truth values than GP model predictions; see the RMSPE values in Figure 4 of the Appendix. This is because the GP model becomes underconfident in the extrapolation zones beyond the observed data range; see Plot (D). The \(_{pred}\) provided more confident extrapolations.

## 5 Discussion

In this work, we proposed the 2S approach to enable predictive UQ in FL clients. The 2S approach distills a BME into two student models: one focused on generating accurate predictions and the other on quantifying uncertainty, while filtering out unreliable client models in FL to ensure robust and efficient updates across decentralized, non-i.i.d. data sources. Our empirical results shown that 2S can effectively manage non-i.i.d. data and provide aleatory and epistemic uncertainty quantification across decentralized clients. **Limitation.** This initial study does not provide a strategy to define an optimum value for the truncation filter threshold \(\) and the best moment to start the truncation filtering. **Future work.** It consists of addressing these limitations beyond demonstrating the applicability and efficiency of 2S in challenging conditions. In real-world applications, particularly in sensitive domains such as healthcare and biomanufacturing, 2S has the potential to facilitate collaborative model development without compromising data privacy. By allowing companies to build robust

Figure 1: Regression problem. Plots (A) and (B) show the ensemble, \(_{pred}\) and \(_{uq}\) predictions in the rounds 0 and 100. Plots (C) and (D) show the GP predictions before and after optimization.

models with predictive UQ that can be shared and utilized across different organizations, 2S not only advances the state-of-the-art in FL but also opens new avenues for secure and efficient model training in complex, real-world scenarios.