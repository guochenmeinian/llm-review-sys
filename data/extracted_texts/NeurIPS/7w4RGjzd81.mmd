# Unbiased Watermark for Large Language Models

Anonymous Author(s)

Affiliation

Address

email

###### Abstract

The recent advancements in large language models (LLMs) have sparked a growing apprehension regarding the potential misuse. One approach to mitigating this risk is to incorporate watermarking techniques into LLMs, allowing for the tracking and attribution of model outputs. This study examines a crucial aspect of watermarking: how significantly watermarks impact the quality of model-generated outputs. Previous studies have suggested a trade-off between watermark strength and output quality. However, our research demonstrates that it is possible to integrate watermarks without affecting the output probability distribution with appropriate implementation. We refer to this type of watermark as an **unbiased watermark**. This has significant implications for the use of LLMs, as it becomes impossible for users to discern whether a service provider has incorporated watermarks or not. Furthermore, the presence of watermarks does not compromise the performance of the model in downstream tasks, ensuring that the overall utility of the language model is preserved. Our findings contribute to the ongoing discussion around responsible AI development, suggesting that unbiased watermarks can serve as an effective means of tracking and attributing model outputs without sacrificing output quality.

## 1 Introduction

In recent years, large language models (LLMs) [19; 39; 40] have become an indispensable tool for a wide range of tasks, including text generation [27; 10], translation [7; 5], summarization , etc. With the escalating misuse of LLMs, such as plagiarism, tracking the usage of text generated by machines has become increasingly important. One viable method to monitor the usage of LLMs is watermarking [20; 32; 59], which embeds imperceptible information within the generated text, thereby allowing for efficient detection and tracking of the model's potential abuse.

Watermarking techniques can serve multiple purposes, such as embedding ownership information within the generated text to protect the intellectual property rights of the model. It can also help mitigate potential harm caused by LLMs by monitoring where the model is being used and whether it is being misused or abused.

A good watermarking method should not adversely affect the normal usage of the language model or degrade the quality of the generated text. However, a prevailing belief holds that there is an inevitable trade-off between the strength of the watermark and the quality of the output text. For instance, recent work by Kirchenbauer et al.  introduced a method that augmented the logits of a randomly selected set of "green" tokens. By tuning the "magnitude of logits adjustment", they demonstrated a trade-off between watermark strength and text quality.

Our primary contribution is to challenge this conventional wisdom. We show that with the right implementation, watermarking can be accomplished without affecting the output quality. We refer to this particular type of watermark as an **unbiased watermark**. We approach the problem of output quality degradation from the perspective of watermark detection. We posit that if the watermarkcauses a decline in output quality, there should be a method to guess the presence of the watermark based on the quality. Conversely, if the watermark cannot be detected, it implies that the output quality remains unaffected. Specifically, we provide a proof that with a suitable implementation, watermarking does not affect the output probability distribution. This has significant implications, as users who do not have the private key are unable to discern whether a service provider has applied watermarking to the model. Furthermore, the addition of watermarking does not affect the performance of the generated text in any downstream tasks. **Our main contributions can be summarized as follows:**

* We introduce _unbiased watermark_, an innovative family of watermark methods that guarantee the non-degradation of text quality. In addition, we offer a comprehensive framework that facilitates the design and detection of unbiased watermarks.
* We propose two innovative and practical watermarking techniques known as \(\)-reweight and \(\)-reweight. Through extensive experimentation, we demonstrate that these techniques preserve output quality in machine translation and text summarization tasks.
* We develop an advanced maximin variant of the original log-likelihood ratio test for watermark detection. This novel detection method comes with theoretical guarantees, specifically an upper bound on type I error, thus enhancing the reliability of watermark detection in language models.

## 2 Preliminary

In this section, we delve into the problem of watermarking in the context of LLMs. We begin by setting up the problem and defining essential concepts.

**Problem Modeling:** We first introduce several notations to formalize the problem. Let \(\) denote the vocabulary set, which is the set of all possible tokens an LLM can generate in a single step. We then define the set \(^{*}\) as the collection of all possible strings of any length, including those of length zero.

An LLM generates a sequence of tokens conditioned on a given context. In a single step, the probability of generating the next token \(x_{n+1}\) given the current context, \(x_{1},x_{2},...,x_{n}\), can be denoted as \(P_{M}(x_{n+1} x_{1},x_{2},...,x_{n})\). The LLM operates in an autoregressive fashion, which means the joint probability of generating multiple tokens \(x_{n+1},,x_{n+m}\) can be written as:

\[P_{M}(x_{n+1},,x_{n+m} x_{1},x_{2},...,x_{n})=_{i=1}^{m}P_{M}(x _{n+i} x_{1},x_{2},...,x_{n},x_{n+1},,x_{n+i-1}).\]

For simplicity, we use the following notation: \(P_{M}(_{n+1:n+m}_{1:n})\), where \(_{n+1:n+m}=(x_{n+1},,x_{n+m})^{*}\).

In the context of watermarking, we introduce a service provider that holds a private key \(k\) from the key space \(K\). The key \(k K\) is chosen at random from the prior distribution \(P_{K}(k)\). The watermarked output of the LLM follows distribution \(P_{M,w}(x_{n+1} x_{1},x_{2},...,x_{n};k)\), which is conditioned on both the key \(k\) and the context \(_{1:n}\). Similarly, we use the notation \(P_{M,w}(_{n+1:n+m}_{1:n};k)\) for the probability of generating a sequence of tokens in a watermarked model.

**Objective.** Our goal is to devise a watermarking scheme that: a) is efficiently detectable by the service provider; b) can't be detected by users and does not negatively impact the quality of the output.

The reason we focus on the detection of watermarks by users is that it is closely related to the output quality. If the watermark causes a degradation in the output quality, there should exist a method to infer the presence of the watermark by examining the quality. Conversely, if the watermark is undetectable, it implies that it does not impact the output quality.

From a statistical testing perspective, a watermark is considered strictly undetectable if the probability distributions of the watermarked and non-watermarked outputs are identical. To capture this notion, we define several desirable properties of watermarking schemes.

**Definition 1** (\(n\)-shot-undetectable).: _For a fixed input sequence \(^{*}\), we say that watermarked LLM and key prior pair \((P_{M,w},P_{K})\) is \(n\)-shot-undetectable compared to original LLM \(P_{M}\) if_

\[_{i=1}^{n}P_{M}(^{i})=_{k K}P_{K}(k)_{i=1}^{ n}P_{M,w}(^{i};k),^{i}^{*}$.}\]

**Definition 2** (downstream-invariant).: _We say the watermarked LLM and key prior pair \((P_{M,w},P_{K})\) are invariant compared to original LLM \(P_{M}\) on downstream tasks iff_

\[_{ P_{M,w}(:[;),k) P_{K}}[f()]=_{ P_{M}(:[]}[f()],\]

_for any strings \(,^{*}\), and for any metric \(f:^{*}\)._

Note that the one-shot-undetectable property implies the downstream invariant property. Interestingly, this implication does not require the \(n\)-shot-undetectable property for \(n>1\), which means a watermarking scheme that is one-shot-undetectable can still maintain the output quality for downstream tasks even if the user might discern the existence of the watermark through multiple generation requests.

In summary, we have outlined the preliminary concepts and objectives for developing a watermarking scheme for LLMs. We highlight the desired properties of \(n\)-shot-undetectability and downstream invariance, as they provide a rigorous theoretical guarantee of quality preservation and integrity in the deployment of watermark schema. In Section 4, we will present a watermark framework that is provably \(n\)-shot-undetectable for any given integer \(n 1\).

## 3 Warm up: undetectability in a simplified toy environment

In this subsection, we aim to prove the feasibility of undetectability in a highly simplified toy environment. This preliminary analysis serves as a foundation for understanding the more complex scenarios that follow.

**Settings.** Consider a service provider that offers a random number generation service. The service outputs a uniformly distributed random number in the set \(\{0,1\}\). The clean generation process can be represented as \(P_{M}(x)=1/2,\  x\{0,1\}\). We assume that the key \(k\) belongs to the set \(\{0,1\}\) and is selected with equal probability. With the watermark added, the probability of the new output can be expressed as: \(P_{M,w}(x k)=_{k}(x)\).

Recall that the one-shot-undetectable property can be represented as \(P_{M}(x)=_{k K}P_{M,w}(x k)P_{K}(k)\). Suppose that a user can only make a single request to the service. If the user is unaware of the key, the user will be unable to distinguish whether the received result is watermarked or not. Therefore, in this simplified scenario, the undetectability of the watermark is achieved.

However, there is a considerable gap between this toy example and the practical implementation of watermarking in LLMs. Firstly, the symbol set \(\) in LLMs is far more complex than the binary set \(\{0,1\}\), and the probability distribution is not uniform. Besides, the generation process in LLMs is autoregressive, which means that more than one symbol are generated iteratively. Furthermore, the toy example does not satisfy the \(n\)-shot-undetectable property for \(n>1\).

Despite these differences, this simple example provides essential insights that help in understanding the following sections where we address these challenges. The underlying principles of undetectability remain constant, while their application becomes more intricate in a more complex environment.

## 4 Watermarking with unbiased reweighting

In this section, we build upon the intuition from the previous section and extend the approach to LLMs' generation. The section is structured as follows: Section 4.1 introduces a fundamental mathematical tool for addressing the reweighting problem in general discrete probability distributions. Section 4.2 applies the reweighting technique to LLMs. Section 4.3 presents the final framework.

### Distribution reweighting

In its most general form, we consider a random watermark code \(E\) and a reweight function \(R_{E}:_{}_{}\), which depends on the random watermark code \(E\). The set of all possible probability distributions on the symbol set \(\) is denoted as \(_{}\), which forms a simplex.

**Definition 3**.: _A **reweighting function** is a tuple \((,P_{E},R)\) where \(\) is called the watermark code space, \(P_{E}\) is a probability distribution on space \(\), and \(R\) is a function \(R:_{}_{}\). For a specific watermark code \(E\), we denote the partially evaluated reweighting function as \(R_{E}:_{}_{}\)._

**Definition 4**.: _Given a random watermark code \(E\) and a reweighting function \(R_{E}:_{}_{}\), we say that \(R\) is an **unbiased reweighting function** if and only if for all \(P_{}\), \(_{E}[R_{E}(P)]=P\)._

#### 4.1.1 Existing reweighting methods

Kirchenbauer et al.  essentially comprise two reweighting methods in their work, but neither of them satisfies the unbiased property.

Both methods have \(\) as the set of mappings \(f:\{,\}\), such that \(f\) maps half of the tokens in \(\) to'red' and the other half to 'green', and \(P_{E}\) as a uniform distribution. Therefore, the random watermark code \(E\) assigns each symbol to either _red_ or _green_. The "Hard Red List" method sets the probability of all red symbols to zero and renormalizes the probabilities of the remaining vocabulary. The second method is "Soft Red List" blocking, where they randomly select the same "Red List" as the first method and decrease the corresponding probability for red symbols by adding a constant \(\) to the logits of the green symbols, then apply softmax to obtain the final probabilities.

#### 4.1.2 Unbiased reweighting methods

In this section, we present two reweighting methods that satisfy the unbiased property.

\(\)**-reweight:** Let the watermark code space \(\) be the interval \(\), and let \(P_{E}\) be the uniform probability on \(\). Leveraging _Inverse Transform Sampling1_, we can sample from distribution \(P_{}\) using a uniformly distributed random number in \(\). Therefore, we have a mapping \(_{P}:\). The \(\)-reweight just returns a delta distribution \(R_{E}(P)=_{_{P}(E)}\).

It is important to note that while the reweighted distribution for each individual random event \(E\) is a delta distribution, the mean output token probabilities remain the original distribution \(P\) when considering the randomness of \(E\).

\(\)**-reweight:** Let the watermark code space \(\) be the set of all bijective function between vocabularies set \(\) and a set of indices \([||]=\{1,,||\}\), where \(||\) is the size of vocabularies set \(\). Essentially, any watermark code \(E\) is an indexing function for vocabularies set \(\), and is also equivalent to a total order on \(\). Let \(P_{E}\) be the uniform probability on \(\), it is easy to sample a watermark code \(E\) by randomly shuffling the symbol list.

Assume the original distribution is \(P_{T}(t)_{}, t\). Given the watermark code \(E:[||]\), we construct auxiliary functions \(F_{I}(i)=_{t}(E(t) i)P_{T}(t)\), \(F_{S}(s)=(2s-1,0)\), \(F_{I^{}}(i)=F_{S}(F_{I}(i))\). The \(\)-reweight yields new distribution \(P_{T^{}}(t)=F_{I^{}}(E(t))-F_{I^{}}(E(t)-1)\).

We provide illustrations of the \(\)-reweight and \(\)-reweight methods in Figures 2 and 2. Each block represents a token, and the width represents the probability of that token, so the total length is 1 The left panel shows the \(\)-reweight method, where each individual random watermark code \(E\) uniformly sampled from interval \(\) corresponds to a specific token according to the horizontal axis, and the reweighted distribution is just a \(\) distribution on that token, such that the selected token has 1 probability, and all other vocabulary tokens have a probability of \(0\). The right panel demonstrates the \(\)-reweight method. First, the symbol set is shuffled. Then, the left half of the regions are rejected, and the remaining regions are amplified with a factor of 2.

Both methods are unbiased1 when considering the randomness of the watermark code \(E\). For \(\)-reweight, we can see that by noticing that the probability of returning a \(\) distribution on a token isjust the original probability on that token, therefore the weighted average of all delta distributions is still the original probability. In the case of \(\)-reweight, although certain regions are rejected and the other regions are amplified, every token has the same probability to be in the rejected or amplified region, thus ensuring the unbiased property.

### Reweighting for autoregressive model

The reweighting methods presented in the previous section can be applied to single token-generation directly. Given a prefix \(_{1:n}\), the probability distribution for generating a new token without a watermark is denoted as \(P_{M}(|_{1:n})_{}\). For a random watermark code \(E\), we sample from a new distribution \(P_{M,w}(|_{1:n})=R_{E}(P_{M}(|_{1:n}))_{}\). If the reweighting function is unbiased, we have \(_{E}[R_{E}(P_{M}(|_{1:n}))]=P_{M}(|_{1:n})\). This ensures that, for an individual unaware of the watermark code, it is impossible to determine whether a new token is sampled directly from \(P_{M}(|_{1:n})\) or from \(P_{M,w}(|_{1:n};E)\) for a random watermark \(E\). However, if the watermark code is known, one can perform statistical hypothesis testing to determine the likelihood of a token being sampled from either distribution.

The main challenge now is constructing the watermark code \(E\). Since the LLM generation task is autoregressive, multiple reweighting steps are required, with each step needing a watermark code \(E_{i}\) for reweighting the distribution of token \(x_{i}\).

#### 4.2.1 Independence of watermark codes

It is crucial that \(E_{i}\) values are independent to ensure the unbiased nature of the entire sequence, rather than just the single-token generation process.

**Theorem 5**.: _Given an unbiased reweighting function \((,P_{E},R)\), if \(E_{i}\) values are i.i.d. with the distribution \(P_{E}\), we have: \(_{E_{1},...,E_{n}}[P_{M,w}(_{1:n}|_{1:m})]=P_{M}(_ {1:n}|_{1:m})\)._

If the \(E_{i}\) values are not independent, we cannot guarantee that the generation probability of the entire sequence remains unbiased. As an extreme example, consider a case where all \(E_{i}\) values are identical. Referring to the random bit example in the previous section, assume that the correct distribution is a sequence where each token is a random 0 or 1 with equal probability. Identical \(E_{i}\) values would result in identical token outputs, ultimately producing sequences consisting solely of 0's or 1's, which is clearly biased.

#### 4.2.2 Context code

To construct a large number of independent watermark codes \(E_{i}\) during watermarking and to know the used \(E_{i}\) values during watermark detection, we follow an approach similar to Kirchenbauer et al.  by combining the information from the prefix and a secret key to construct \(E_{i}\).

For a single token generation process, given a prefix \(x_{1},x_{2},...,x_{n}\), we consider an abstract context code space \(C\) and an abstract context code generation function \(cc:^{*} C\). Based on the prefix, we construct the context code \(c_{n+1}=cc(x_{1},x_{2},...,x_{n})\). Specific examples include using the entire prefix \(c_{n+1}=(x_{1},x_{2},...,x_{n})\), and using the \(m\) most recent prefixes \(c_{n+1}=(x_{n-m+1},...,x_{n})\). Our comprehensive framework accommodates diverse context code generation approaches, particularly those that integrate error-correcting mechanisms to augment watermark resilience in the face of text manipulation attacks. Nevertheless, we refrain from delving into these strategies within the confines of this paper and consider it a subject for subsequent investigation.

The final watermark code is defined as \(E_{i}=(c_{i},k)\), using a watermark code generation function \(:C K\).

**Definition 6**.: _Given an unbiased reweighting function \((,P_{E},R)\) and a context code space \(C\), an **unbiased watermark code generation function** is a tuple \((,P_{E},R,C,K,P_{K},)\) that satisfies:_

1. _Unbiasedness:_ \(_{k P_{K}}[R_{(c,k)}(P)]=P, P_{},  c C\)_._
2. _Independence: For any_ \(n\) _distinct_ \(c_{1},,c_{n} C\)_, the values_ \(R_{(c_{i},k)}(P)\) _are mutually independent._

**Theorem 7**.: _For any unbiased reweighting function and context code space, an unbiased watermark code generation function always exists._

In practice, pseudorandom numbers can be used to implement the unbiased watermark code generation function in the above theorem. Specifically, the hash value \((c,k)\) can be used as a random seed to sample \(E\) from \(P_{E}\) as an implementation of \(E=(c,k)\). In this paper, we employ SHA-256 for hash function and a 1024-bit random bitstring as the key \(k\).

An unbiased watermark code generation function ensures that watermark codes \(E_{i}\) are independent with each other if only their context codes are different. During the generation of a sequence, context codes may be repeated, although this is a rare event in practice. If \(c_{i}\) and \(c_{j}\) are equal, then \(E_{i}\) and \(E_{j}\) are also equal, violating the independence of \(E_{i}\). A simple workaround is to skip reweighting for a token when encountering a previously used context code. In other words, we set \(P_{M,w}(|_{1:m},_{1:i-1})=P_{M}(|_{1:m},_{1:i -1})\) if the context code has appeared before.

### Framework

```
1:Input: key for watermark \(k K\), prompt \(_{1:m}^{*}\), generate length \(n\), initial code history \(cch 2^{C}\), context code function \(cc:^{*} C\), watermark code generation function \(:C K\), and reweighting function \(R:_{}_{}\).
2:for\(t=1,,n\)do
3:\(P_{i} P_{M}(_{1:m},_{1:i-1})\)\(\) original distribution
4:\(_{i} cc(_{1:m},_{1:i-1})\)\(\) context code
5:if\(_{i} cch\)then
6:\(Q_{i} P_{i}\)\(\) skip the reweighting
7:else
8:\(cch cch\{c_{i}\}\)\(\) record history
9:\(E_{i}(c_{i},k)\)\(\) watermark code
10:\(Q_{i} R_{E_{i}}(P_{i})\)\(\) reweighted distribution
11: Sample the next token \(x_{i}\) using distribution \(Q_{i}\)
12:return\(_{1:n}\) ```

**Algorithm 1** Watermarking framework

Integrating the tools discussed earlier, we present a general framework for watermarking here. The algorithm for this framework is outlined in Algorithm 1.

We note that our abstract framework requires the specification of two key components in order to be practically implemented: the unbiased reweight function \(R_{E}\) and the context code function \(cc\).

## 5 Statistical hypothesis testing for watermark detection

In the previous section, we discussed the process of adding a watermark to a text based on a secret key \(k\) and a given prompt \(_{1:m}\). The watermark-embedded text can be sampled from the distribution \(P_{M,w}(_{1:n}|_{1:m};k)\). In this section, we focus on the watermark detection task, which is the inverse problem of watermark embedding.

Given a text \(_{1:n}\), the goal of watermark detection is to infer whether it is more likely to be generated from the unmarked distribution \(P_{M}(_{1:n}|_{1:m})\) or the marked distribution \(P_{M,w}(_{1:n}|_{1:m};k)\). This problem can be formulated as a statistical hypothesis test between two competing hypotheses: \(H_{0}\), which posits that \(_{1:n}\) follows the unmarked distribution, and \(H_{1}\), which posits that \(_{1:n}\) follows the marked distribution.

### Score-based testing

We focus on a particular kind of score-based testing, which assigns a score to each token in the text. The score can be interpreted as the confidence that the token was generated by the watermark model rather than the original model. Scores \(s_{i}\) can be computed based on \(_{1:i}\), in accordance with the autoregressive manner of the generation process.

The total score \(S\) is given by \(S=_{i=1}^{n}s_{i}\). A threshold \(\) is set such that if \(S<\), the null hypothesis \(H_{0}\) is accepted, indicating insufficient evidence to conclude that the text contains a watermark. Otherwise, the null hypothesis is rejected. There are two types of error probabilities associated with this decision process: Type I error, which is the probability of incorrectly rejectingthe null hypothesis under \(H_{0}\), denoted as \(P_{H_{0}}(S)\), and Type II error, which is the probability of incorrectly accepting the null hypothesis under \(H_{1}\), denoted as \(P_{H_{1}}(S<)\).

To derive theoretical results, we require the scores to have a specific property: under the null hypothesis \(H_{0}\), the exponential momentum of \(s_{i}\) is bounded, conditioned on the preceding context \(_{1,i-1}\). This requirement leads to an upper bound on \(\), the Type I error probability.

To derive theoretical results, we require that the scores have a particular property: the exponential moment of \(s_{i}\) under \(H_{0}\) should be bounded, conditioned on the previous text \(_{1,i-1}\). This requirement leads to an upper bound on the Type I error rate.

**Theorem 8**.: _Given a probability space \((,,P)\) and a \(\)-valued stochastic process \(x_{i}:1 i n\), as well as an \(\)-valued stochastic process \(s_{i}:1 i n\), let \(_{i}^{x}:=(x_{j} 1 j i)\) and \(_{i}^{s}:=(s_{j} 1 j i)\) be the corresponding filtrations, where \(()\) denotes the \(\)-algebra generated by random variables. If \(_{i}^{s}_{i}^{x}\) and \([(s_{i})|_{i-1}^{x}] 1\), then \(P(_{i=1}^{n}s_{i} t) e^{-t}\)._

Therefore, to ensure that the Type I error probability has an upper bound \(\), we can set the threshold \(\) as \(=-()\). In the following, we discuss two special scores.

### Log likelihood ratio (LLR) score

According to the Neyman-Pearson lemma, the likelihood ratio test is the most powerful test among all tests with the same Type I error rate. Specifically, the log-likelihood ratio (LLR) score is defined as \(s_{i}=(x_{i}|_{1:m},_{1:i-1};k)}{P_{M}(x_{i}| {a}_{1:m},_{1:i-1})}\), and the total score becomes \(S=(_{1:m}|_{1:m};k)}{P_{M}(_{1:m}|_{1 :m})}\).

We now provide an optimization derivation of the above \(s_{i}\) to gain intuition and set the foundation for the maximum variant of the LLR score in the next section. Let \(P_{i}=P_{M}(|_{1:m},_{1:i-1})\), \(Q_{i}=P_{M,w}(|_{1:m},_{1:i-1};k)\), and let \(s_{i}=S_{i}(x_{i})\) denote the score corresponding to different \(x_{i}\). Note that \(P_{i}\), \(Q_{i}\), and \(S_{i}\) are all functions with signature \(\), therefore equivalent to vectors of dimension \(||\). We can define the inner product as \( P_{i},S_{i}=_{x}P_{i}(x)S_{i}(x)\).

The requirement \([(s_{i})|_{i-1}^{x}] 1\) can be reformulated as \( P_{i},(S_{i}) 1\), where the exponential function is applied element-wise. Instead of minimizing the Type II error directly, we aim to maximize the average score under \(H_{1}\), i.e., \( Q_{i},S_{i}\).

The optimization problem becomes \(_{S_{i}}\, Q_{i},S_{i},\ s.t.\, P_{i},(S_{i})  1\). The optimal solution is given by \(S_{i}(x)=(x)}{P_{i}(x)}\), which recovers the optimal log likelihood ratio score.

### Maximin variant of LLR score

One major limitation of the LLR score described in the previous section is that when \(Q_{i}(x)=0\), \(S_{i}(x)=-\). This means that as long as a single token does not come from the watermark model \(P_{M,w}\), the score becomes negative infinity, making it impossible to reject the null hypothesis \(H_{0}\).

A more general reason for this issue is that the watermark model \(P_{M,w}\) used in the detection process may not exactly match the true distribution of the watermarked text. In practice, potential sources of discrepancy include editing (e.g., a text sampled from \(P_{M,w}\) may undergo some degree of editing before being watermark detection) and imperfect estimation of the generation process (e.g., due to lack of knowledge of the exact prompt and temperature used during generation).

To address this problem, we consider a perturbed generation distribution. Instead of the original hypothesis \(H_{1}\), where \(_{1:n}\) follows the watermark distribution \(P_{M,w}\), we now assume that \(_{1:n}\) follows a distribution \(P_{M,w}^{}\), which is similar to but not identical to \(P_{M,w}\). Specifically, during the generation of each token, the total variation (TV) distance between \(Q_{i}^{}\) and \(Q_{i}\) is bounded by \(d\).

The corresponding new optimization problem is

\[_{S_{i}}\ _{Q_{i}^{}_{},TV(Q_{i}^{},Q_{i})  d}\ \ \  Q_{i}^{},S_{i},\ \ \ s.t.\, P_{i},(S_{i}) 1.\]

Intuitively, the optimal solution for \(Q_{i}^{}\) in the inner optimization decreases \(Q_{i}^{}(x)\) when \(S_{i}(x)\) is large and increases \(Q_{i}^{}(x)\) when \(S_{i}(x)\) is small.

The computation of the maximin solution can be done efficiently in \((||)\) time and the specific algorithm is shown in Appendix C.

It is important to note that the maximin variant of the LLR score is more robust than the standard LLR score, as it yields higher scores when the text has undergone some degree of editing. However, it is not specifically designed to defend against any attacks.

A hyperparameter \(d\) that represent the perturbation strength is introduced in the score. Intuitively, if the text to be detected has undergone more editing and deviates further from the distribution \(P_{M,w}\), \(d\) should be larger. In practice, we recommend using grid search to select the best value of \(d\). Assuming there are \(A\) candidate values for \(d\), corresponding to \(A\) different scores \(s_{i}^{(a)}\) (\(1 a A\)), we can modify Theorem 8 as follows.

**Theorem 9**.: _Under the same conditions as Theorem 8, but with multiple scores \(s_{i}^{(a)}\), we have_

\[P(_{1 a A}(_{i=1}^{n}s_{i}^{(a)}) t)  Ae^{-t}.\]

Thus, when using grid search, the final threshold should be adjusted as \(=-()+(A)\). This ensures that the upper bound of the type I error is still \(\).

## 6 Experiments

We evaluate the performance of our Unbiased Watermarks on two important applications of seq2seq models: text summarization (TS) and machine translation (MT). For the TS task, we use the BART-large model  and the CNN-DM  corpus as our training dataset. The MT task involves translating English to Romanian, for which we employ the Multilingual BART (MBart)  model on the WMT'14 En-Ro corpus. For further details on the experiment setup, please refer to Appendix E.

Our primary focus is to compare the performance of our proposed unbiased watermarking methods including the \(\)-reweight and \(\)-reweight, with the soft-red-list method presented by Kirchenbauer et al. . The strength of the watermark in the soft-red-list approach is controlled by a parameter \(\).

The quality of output post-watermarking is presented in Table 1. We observed that the output quality remains unaffected by our unbiased watermark methods, both for the \(\)-reweight and \(\)-reweight,

   &  &  \\  & BERTScore \(\) & ROUGE-1 \(\) & Perplexity \(\) & BERTScore \(\) & BLEU \(\) \\  No Watermark & \(32.70 0.08\) & \(38.56 0.09\) & \(5.024 0.018\) & \(55.9 0.3\) & \(21.8 0.3\) \\ \(\)-reweight & \(32.71 0.08\) & \(38.57 0.09\) & \(5.022 0.018\) & \(56.3 0.3\) & \(21.7 0.3\) \\ \(\)-reweight & \(32.69 0.08\) & \(38.60 0.09\) & \(5.019 0.018\) & \(56.2 0.3\) & \(21.8 0.3\) \\ Soft(\(\)=0.0) & \(32.70 0.08\) & \(38.56 0.09\) & \(5.024 0.018\) & \(55.9 0.3\) & \(21.8 0.3\) \\ Soft(\(\)=1.0) & \(32.35 0.08\) & \(38.20 0.09\) & \(5.313 0.018\) & \(55.1 0.3\) & \(21.0 0.3\) \\ Soft(\(\)=2.0) & \(31.21 0.08\) & \(37.17 0.08\) & \(6.253 0.022\) & \(53.8 0.3\) & \(19.5 0.3\) \\  

Table 1: Performance of different watermarking methods on TS and MT. We use F1 scores of BERTScore and scale BERTScore and ROUGE-1 with a factor of 100.

Figure 3: Distribution of perplexity of output for TS and BLEU score for MT.

irrespective of the task and metric. Conversely, the soft-red-list method, when \(=0\), does not introduce any watermark and hence does not affect output quality. However, for \(>0\), it significantly deteriorate the quality of output.

Figure 3 provides a more intuitive depiction of the score distributions. It is evident that our unbiased watermark methods not only ensure that the mean performance remains unaffected but also that the performance distribution is stable. Conversely, the soft-red-list method shows a notable performance decrease.

In terms of watermark detection, we compute score associated with each token. The mean and variance of score per token for TS and MT are presented in Table 2. As a heuristic, if the sum of the scores for all tokens in a sentence reaches \(10\), a p-value of less than \(0.0005\) is ensured. If the sum score hits \(20\), the p-value must be less than \(3\).

Additionally, we provide an example of watermarking applied to a completion task in Table 3. It visually demonstrates the score distribution across tokens: positive scores are represented in green, and negative ones in red. The intensity of the color corresponds to the magnitude of the score, with darker shades representing larger absolute values.

## 7 Related work

The idea of watermarking text has been widely explored by many researchers [11; 31; 44; 45; 4; 28; 49; 43], even before the advent of large language models. Several techniques involve editing existing text to add a watermark, such as changing synonyms [54; 57; 9; 59; 66] or visually indistinguishable words , altering sentence structures [56; 55; 38], and employing neural networks [22; 23; 67].

Recent advancements in generative models have opened new possibilities for directly generating watermarked results. Two relevant works in this domain are by Kirchenbauer et al.  and Aaronson . Due to space constraints, we moved the in-depth analysis and other related work to Section B.

## 8 Conclusion

Overall, this paper provides a novel framework of watermarking for language models, demonstrating that it is possible to use watermark to protect intellectual property and monitor potential misuse without compromising the quality of the generated text. This research serves as a valuable foundation for future work in the field of watermarking for large language models.

    & Text summarization & Machine translation \\  \(\)-RW & \(0.8784 1.4354\) & \(0.4192 1.1361\) \\ \(\)-RW & \(0.2207 0.3678\) & \(0.1056 0.2916\) \\   

Table 2: Mean and variance of score per token for different reweighting methods and different tasks.

   Prompt & What is a watermark? What’s the purpose of it? & score p-value2  \\  NW & Why don’t you want it on there? I’m confused..nl think he meant to say logo, since he wrote ”watermark”, so the firstword must be à typo.unYes! Exactly typo, Sorry, English is not my native language.. Thanks for the explanation! \\  \(\)-RW & ufliss supposed to be watermarking the pictures that you don’t, with your phone, think So, so you can share your pictures and not take credit for them! \\  \(\)-RW & ufliss watermark is à small image or logo (often in square pixels) that is placed over the larger, original image! It serves primarily to distinguish copyright or ownership of large images(such as banners and logos) and, on rare occasion, to identify small images(such as l thumbtail images(for blog posts and pictures)) & \(75.9\)\(1.2\) \\   

Table 3: Text sampled from OPT-6.7B, with and without watermarks. For ’No watermark” (NW), the score is computed based on \(\)-reweight. When watermarks are included, the corresponding reweighting function is used for computing score. The optimal perturbation strengths \(d\) obtained by grid search are \(0.9,0.0,0.0\) for three outputs respectively.