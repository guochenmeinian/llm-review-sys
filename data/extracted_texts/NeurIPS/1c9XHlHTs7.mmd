# Warm-up Free Policy Optimization:

Improved Regret in Linear Markov Decision Processes

 Asaf Cassel

Tel Aviv University

acassel@mail.tau.ac.il &Avi Rosenberg

Google Research

avivros@google.com

###### Abstract

Policy Optimization (PO) methods are among the most popular Reinforcement Learning (RL) algorithms in practice. Recently, Sherman et al. (2023) proposed a PO-based algorithm with rate-optimal regret guarantees under the linear Markov Decision Process (MDP) model. However, their algorithm relies on a costly pure exploration warm-up phase that is hard to implement in practice. This paper eliminates this undesired warm-up phase, replacing it with a simple and efficient contraction mechanism. Our PO algorithm achieves rate-optimal regret with improved dependence on the other parameters of the problem (horizon and function approximation dimension) in two fundamental settings: adversarial losses with full-information feedback and stochastic losses with bandit feedback.

## 1 Introduction

Policy Optimization (PO) is a widely used method in Reinforcement Learning (RL) that achieved tremendous empirical success, with applications ranging from robotics and computer games (Schulman et al., 2015, 2017; Mnih et al., 2015; Haarnoja et al., 2018) to Large Language Models (LLMs; Stiennon et al. (2020); Ouyang et al. (2022)). Theoretical work on policy optimization algorithms initially considered tabular Markov Decision Processes (MDPs; Even-Dar et al. (2009); Neu et al. (2010); Shani et al. (2020); Luo et al. (2021)), where the number of states is assumed to be finite and small. In recent years the theory was generalized to infinite state spaces under function approximation, specifically under linear function approximation in the linear MDP model (Luo et al., 2021; Dai et al., 2023; Sherman et al., 2023, 2023; Liu et al., 2023).

Recently, Sherman et al. (2023) presented the first policy optimization algorithm that achieves rate-optimal regret in linear MDPs, i.e., a regret bound of \(((H,d))\), where \(K\) is the number of interaction episodes, \(H\) is the horizon, and \(d\) is the dimension of the linear function approximation. However, their algorithm requires a pure exploration warm-up phase to obtain an initial estimate of the transition dynamics. To that end, they utilize the algorithm of Wagenmaker et al. (2022) for reward-free exploration which is not based on the policy optimization paradigm. Moreover, although this algorithm is computationally efficient, it relies on intricate estimation techniques that are hard to implement in practice and unlikely to generalize beyond linear function approximation (see discussion in section 4).

In this paper, we propose a novel contraction mechanism to avoid this costly warm-up phase. Both our contraction mechanism and the warm-up phase serve a similar purpose - ensuring that the Q-value estimates are bounded and yield "simple" policies. But, unlike the warm-up, our method is integrated directly into the PO algorithm, implemented using a simple conditional truncation of the Q-estimates, and only contributes a lower-order term to the final regret bound. Moreover, our approach is muchmore efficient in practice since it does not rely on any reward-free methods, which explore the state space uniformly without taking the reward into account.

Based on this contraction mechanism, we build a new policy optimization algorithm that is simpler, more computationally efficient, easier to implement, and most importantly, improves upon the best-known regret bounds for policy optimization in linear MDPs. Our regret bound holds in two fundamental settings:

1. Adversarial losses with full-information feedback, where the loss function changes arbitrarily between episodes and is revealed to the agent entirely at the end of each episode.
2. Stochastic losses with bandit feedback, where the loss function in each episode is sampled i.i.d from some unknown fixed distribution and the agent only observes instantaneous losses in the state-action pairs that she visits.

In these settings, the best-known regret bound (by Sherman et al. (2023b)) was \((d^{4}K})\). Our algorithm, Contracted Features Policy Optimization (CFPO), achieves \((d^{3}K})\) regret, yielding a \(d}\) improvement over any algorithm for the adversarial setting and matching the value iteration based approach of Jin et al. (2020b) in the stochastic setting. We conjecture that this is the best regret we can hope for without more sophisticated variance reduction techniques (Azar et al., 2017; Zanette and Brunskill, 2019; He et al., 2023; Zhang et al., 2024), that have not yet been applied to PO algorithms even in the tabular setting.1 Ignoring logarithmic factors, the regret of CFPO leaves a gap of only \(\) from the \((d^{2}K})\) lower bound for linear MDPs (Zhou et al., 2021a). Finally, our analysis relies on a novel regret decomposition that uses a notion of contracted (sub) MDP and may be of separate interest (see section 5).

### Related work

Policy optimization in tabular MDPs.The regret analysis of PO methods in tabular MDPs was introduced by Even-Dar et al. (2009), which considered the case of known transitions and adversarial losses under full-information feedback. Neu et al. (2010, 2010) extended their algorithms to adversarial losses under bandit feedback. Then, Shani et al. (2020) presented the first PO algorithms for the case of unknown transitions (for both stochastic and adversarial losses), and finally Luo et al. (2021) devised a PO algorithm with rate-optimal regret for the challenging case of unknown transitions with adversarial losses under bandit feedback. Since then, PO was studied in more challenging cases, e.g., delayed feedback (Lancewicki et al., 2022, 2023) and best-of-both-worlds (Dann et al., 2023).

Other regret minimization methods in tabular MDPs.An alternative popular method for regret minimization in tabular MDPs with adversarial losses is O-REPS (Zimin and Neu, 2013; Rosenberg and Mansour, 2019, 2019; Jin et al., 2020a), which optimizes over the global state-action occupancy measures instead of locally over the policies in each state. However, this method is hard to implement in practice and does not generalize to the function approximation setting (without restrictive assumptions). For stochastic losses, optimistic methods based on Value Iteration (VI; Jaksch et al. (2010); Azar et al. (2017); Zanette and Brunskill (2019)) and Q-learning (Jin et al., 2018; Zhang et al., 2020) are known to guarantee optimal regret, which has not been established yet for adversarial losses.

Policy optimization in linear MDPs.While Sherman et al. (2023a) established rate-optimal regret for PO methods in linear MDPs with stochastic losses, most of the recent research focused on the case of adversarial losses with bandit feedback (Luo et al., 2021; Neu and Olkhovskaya, 2021; Dai et al., 2023; Sherman et al., 2023; Kong et al., 2023; Liu et al., 2023; Zhong and Zhang, 2023), where rate-optimality has not been achieved yet.

Other regret minimization methods in linear MDPs and other models for function approximation.Unlike O-REPS methods that do not generalize to linear function approximation, value-based methods (operating under the stochastic loss assumption) are also popular in linear MDPs and have been shown to yield optimal regret (Jin et al., 2020a; Zanette et al., 2020; Wagenmaker et al., 2022a;Hu et al., 2022; He et al., 2023; Agarwal et al., 2023). Another line of works (Ayoub et al., 2020; Modi et al., 2020; Cai et al., 2020; Zhang et al., 2021; Zhou et al., 2021; Zhou and Gu, 2022; Zhou and Gu, 2022) study linear mixture MDP which is a different model that is incomparable with linear MDP (Zhou et al., 2021). Finally, there is a rich line of works studying statistical properties of RL with more general function approximation (Munos, 2005; Jiang et al., 2017; Dong et al., 2020; Jin et al., 2021; Du et al., 2021), but these usually do not admit computationally efficient algorithms.

## 2 Problem setup

Episodic Markov Decision Process (MDP).A finite-horizon episodic MDP \(\) is defined by a tuple \((,,x_{1},\{^{k}\}_{k=1}^{K},P,H)\) with \(\), a set of states, \(\), a set of actions, \(H\), decision horizon, \(x_{1}\), an initial state (assumed to be fixed for simplicity), \(P=(P_{h})_{h[H]},P_{h}:( )\), the transition probabilities, and \(\{^{k}\}_{k=1}^{K}\), sequence of loss functions such that \(^{k}=(^{k}_{h})_{h[H]},^{k}_{h}: \), is a horizon dependent immediate loss function for taking action \(a\) at state \(x\) and horizon \(h\) of episode \(k\). A single episode \(k\) of an MDP is a sequence \((x^{k}_{h},a^{k}_{h},^{k}_{h}(x^{k}_{h},a^{k}_{h}))_{h[H]}()^{H}\) such that

\[[x^{k}_{h+1}=x^{} x^{k}_{h}=x,a^{k}_{h}=a]=P_{h}(x^{} x,a).\]

For the losses, we consider two settings: stochastic and adversarial. In the stochastic setting, there exists a fixed loss function \(=(_{h})_{h[H]},_{h}:\) such that \(^{k}\) is sampled i.i.d from a distribution whose expected value is defined by \(\), i.e., \([^{k}_{h}(x,a) x,a]=_{h}(x,a)\). In the adversarial setting, the loss function sequence \(\{^{k}\}_{k=1}^{K}\) is chosen by an adaptive adversary.

Linear MDP.A linear MDP Jin et al. (2020) satisfies all the properties of the above MDP but has the following additional structural assumptions. There is a known feature mapping \(: A^{d}\) such that \(P_{h}(x^{} x,a)=(x,a)^{}_{h}(x^{})\) where \(_{h}:^{d}\) are unknown parameters. Moreover, for all \(h[H],k[K]\), there is an unknown vector \(^{k}_{h}^{d}\) such that, in the adversarial case, \(^{k}_{h}(x,a)=(x,a)^{}^{k}_{h}\), while in the stochastic case, \(^{k}_{h}=_{h}\) and \(_{h}(x,a)=(x,a)^{}_{h}\). We make the following normalization assumptions, common throughout the literature:

1. \(\|(x,a)\| 1\) for all \(x X,a\);
2. \(\|^{k}_{h}\|\) for all \(h[H],k[K]\);
3. \(\||_{h}|()\|=\|_{x}_{h}(x)\| \) for all \(h[H]\);

where \(_{h}(x)\) is the entry-wise absolute value of \(_{h}(x)^{d}\). We follow the standard assumption in the literature that the action space \(\) is finite. In addition, for ease of mathematical exposition (e.g. Cassel et al. (2024)), we also assume that the state space \(\) is finite. This allows for simple matrix notation and avoids technical measure theoretic definitions. Importantly, our results are completely independent of the state space size \(\), both computationally and in terms of regret. Thus, there is no particular loss of generality.

Policy and value.A stochastic Markov policy \(=(_{h})_{h[H]}:[H]()\) is a mapping from a step and a state to a distribution over actions. Such a policy induces a distribution over trajectories \(=(x_{h},a_{h})_{h[H]}\), i.e., sequences of \(H\) state-action pairs. For \(f:()^{H}\), which maps trajectories to real values, we denote the expectation with respect to \(\) under dynamics \(P\) and policy \(\) as \(_{P,}[f()]\). Similarly, we denote the probability under this distribution by \(_{P,}[]\). We denote the class of stochastic Markov policies as \(_{M}\). For any policy \(_{M}\), horizon \(h[H]\) and episode \(k[K]\) we define its loss-to-go, as

\[V^{k,}_{h}(x)=_{P,}_{h^{}=h}^{H} [^{k}_{h^{}}(x_{h^{}},a_{h^{}}) x_{h^{}},a _{h^{}}]x_{h}=x,\]

which is the expected loss if one starts from state \(x\) at horizon \(h\) of episode \(k\) and follows policy \(\) onwards. Note that the inner expectation is only relevant for stochastic losses as its argument is deterministic in the adversarial setup. The performance of a policy in episode \(k\), also known as its value, is measured by its expected cumulative loss \(V^{k,}_{1}(x_{1})\).

Interaction protocol and regret.We consider a standard episodic regret minimization setting where an algorithm performs \(K\) interactions with an MDP \(\). For stochastic losses we consider bandit feedback, where the agent observes only the instantaneous losses along its trajectory, while for adversarial losses we consider full-information feedback, where the agent observes the full loss function \(^{k}\) in the end of episode \(k[K]\). Concretely, at the start of each interaction/episode \(k[K]\), the agent specifies a stochastic Markov policy \(^{k}=(^{k}_{h})_{h[H]}\). Subsequently, it observes the trajectory \(^{k}\) sampled from the distribution \(_{P,^{k}}\), and, either the individual episode losses \(^{k}_{h}(x^{k}_{h},a^{k}_{h}),h[H]\) in the case of bandit feedback, or the entire loss function \(^{k}\) in the case of full-information feedback.

We measure the quality of any algorithm via its _regret_ - the difference between the value of the policies \(^{k}\) generated by the algorithm and that of the best policy in hindsight, i.e.,

\[=_{k=1}^{K}V_{1}^{k,^{k}}(x_{1})-_{_{M}} _{k=1}^{K}V_{1}^{k,}(x_{1})=_{k=1}^{K}V_{1}^{k,^{k}}(x_{1})-V_{1 }^{k,^{*}}(x_{1}),\]

where the best policy in hindsight is denoted by \(^{*}\) (known to be optimal even among the class of stochastic history-dependent policies).

Notation.Throughout the paper \(^{k}_{h}=(x^{k}_{h},a^{k}_{h})^{d}\) denote the state-action features at horizon \(h\) of episode \(k\). In addition, \( v_{A}=Av}\). Hyper-parameters follow the notations \(_{z}\) and \(_{z}\) for some \(z\), and \((0,1)\) denotes a confidence parameter. Finally, in the context of an algorithm, \(\) signs refer to compute operations whereas \(=\) signs define operators, which are evaluated at specific points as part of compute operations.

## 3 The role of value clipping

Before presenting our contraction technique and main results, we discuss the role that value clipping plays in regret minimization and its apparent necessity for linear MDPs. As a starting point, it is important to note that, while commonly used (Azar et al., 2017; Luo et al., 2021), value clipping is not strictly necessary in tabular MDPs. To demonstrate this, consider a fairly standard optimistic Value Iteration (VI) algorithm that constructs sample-based estimates \(,\) with empirical error estimates \(_{},_{P}\), defines exploration bonuses \(b=(_{}+H_{P})\), and chooses a policy \(^{*}\) that is optimal in the empirical MDP whose dynamics are \(\) and losses are \(-b\). Then its single-episode regret may be decomposed as

\[V_{1}^{^{*}}(x_{1})-V_{1}^{^{*}}(x_{1})=^{^{*}}(x_{1})-_{1}^{^{*}}(x_{1})}_{(i)-\,/\, }+_{1}^{^{*}}(x_{1})-_{1 }^{^{*}}(x_{1})}_{(ii)-\,/\,}+_{1}^ {^{*}}(x_{1})-V_{1}^{^{*}}(x_{1})}_{(iii)-},\]

where \(\) is the value under the empirical MDP. Now, by definition of \(^{*}\), we have that \((ii) 0\). Now, let \(=-, P=-P\). Using a standard value difference lemma (lemma 14 in appendix B) we have that \((i) b\) and

\[(iii) =_{,^{*}}[_{h[H]}(x_{h},a_{h})-b(x_{h},a_{h})+_{x^{}} P(x^{} x _{h},a_{h})V_{h+1}^{^{*}}(x^{})]\] (1) \[_{,^{*}}[_{h[H]}_{ }(x_{h},a_{h})+H_{P}(x_{h},a_{h})-b(x_{h},a_{h})]=0,\]

where the inequality also used that \(V_{h}^{^{*}}[0,H]\). The final regret bound is concluded by summing over \(k[K]\) and using a bound on harmonic sums. We note that a similar clipping-free method also works for tabular PO (see Cassel et al. (2024)).

Moving on to Linear MDPs, one might expect a similar approach to work. Unfortunately, the standard approach that estimates the dynamics backup operators \(_{h},h[H]\) using regularized least-squares presents a significant challenge. This is because, unlike the tabular setting, the resulting estimate \(_{h}( x,a)=(x,a)^{}_{h}()\) (eq. (2)) is not guaranteed to yield a valid probability distribution, i.e., there could exist \(x,a,h[H]\) such that

\[_{h}( x,a)_{1}=c>1_{ x^{}}_{h}(x^{} x,a)<0.\]

\(\) is still a finite signed-measure, which is enough for the first equality in eq. (1) to hold. However, since \(_{,^{*}}\) could contain negative probability terms, the inequality in eq. (1) does not hold. These negative probabilities also seem to make calculating \(^{*}\) computationally hard. Finally, the \(_{1}-\)norm exceeding \(1\) may cause term \((i)\) to depend on \(H\) exponentially. While some of these issues could be mitigated without clipping, we are not aware of a method that resolves all simultaneously.

The use of value clipping opens the path for an alternative value decomposition that replaces \(_{,^{*}}\) in eq. (1) with \(_{P,^{*}}\) at the cost of also replacing \(V_{h+1}^{^{*}}\) with \(_{h+1}^{^{*}}\). We thus need that \(|_{h+1}^{^{*}}| H\) for the inequality in eq. (1) to work. This is made possible using a clipping mechanism that decouples the scale of \(_{h+1}^{^{*}}\) from the magnitude of the bonuses \(b\), which may be much larger when the error estimates \(_{},_{P}\) are large. This is typically achieved by adding \(\{0,\}\) to the recursive formula for the value function. A similar clipping approach also works for tabular PO and VI (Azar et al., 2017; Luo et al., 2021), and even for VI in linear MDPs (Jin et al., 2020).

However, this is not the case for PO in linear MDPs where Sherman et al. (2023) explain that this type of value clipping leads to prohibitive complexity of the policy and value function classes, and thus sub-optimal regret. Concretely, the complexity of the soft-max policy class roughly corresponds to the number of parameters required to represent \(_{k[K]}_{h}^{k}\). If \(_{h}^{k}(x,a)=(x,a)^{}w_{h}^{k}\) are linear, then the sum remains linear and depends on \(d\) parameters (with slightly larger magnitude). If \(_{h}^{k}(x,a)=\{0,(x,a)^{}w_{h}^{k}\}\), the sum may, in general, have \(dK\) parameters thus degrading the regret. Sherman et al. (2023) overcome this issue using a warm-up based truncation technique. In what follows, we suggest an alternative solution that uses a novel notion of contracted features and has several advantages over their approach (see discussion at the end of section 4).

## 4 Algorithm and main result

We present Contracted Features Policy Optimization (CFPO; algorithm 1), a policy optimization routine for regret minimization in linear MDPs. The algorithm operates in epochs, each beginning when the uncertainty of the dynamics estimation shrinks by a multiplicative factor, as expressed by the determinant of the covariance matrices \(_{h}^{k},h[H]\) (see line 13 for the definition of \(_{h}^{k}\) and line 4 for the epoch change condition). At the start of each epoch \(e\), we reset the policy to its initial (uniform) state, and define the contracted features \(_{h}^{k_{e}},h[H]\) (line 6) by multiplying the original features with coefficients in the range \(\), and thus shrinking their distance to the origin. Inspired by ideas from Zanette et al. (2020), these coefficients are chosen inversely proportional to the current uncertainty of the least squares estimators in each state-action pair, essentially degenerating the MDP in areas of high uncertainty. Inside an epoch, at episode \(k\), we compute the estimated reward vector \(^{k}\) (line 14) and estimated dynamics backup operators \(_{h}^{k}\) (eq. (2)). Then, we use these \(^{k}\) and \(_{h}^{k}\) to compute our Q-value estimates with the contracted features (eq. (3)), and run an online mirror descent (OMD) update over them (eq. (5)), i.e., run a policy optimization step with respect to the contracted empirical MDP (more on this in section 5.1).

We note that the computational complexity of algorithm 1 is comparable to other algorithms for regret minimization in linear MDPs, such as LSVI-UCB (Jin et al., 2020). The following is our main result for algorithm 1 (see the full analysis appendix A).

**Theorem 1**.: _Suppose that we run CFPO (algorithm 1) with the parameters defined in theorem 9 (in appendix A). Then, with probability at least \(1-\), we have_

\[=O(d^{3}K(K)(KH/)}+dK (K)}).\]

Discussion.Policy optimization algorithms typically entail running OMD over estimates \(\) of the state-action value function \(Q\), as in eq. (5). The crux of the algorithm is in obtaining such estimates that satisfy an optimistic condition similar to eq. (1), while also keeping the complexity of the policy class bounded. As discussed in Sherman et al. (2023), the latter depends on \(_{k^{}[k]}_{h}^{k^{}}\) (eq. (3))having a low dimensional representation nearly independent of \(k\). Although standard unclipped estimates admit such a representation, they lack other essential properties (see discussion in section 3). On the other hand, the standard clipping method, which restricts the value to \([0,H]\) between each backup operation (see, e.g., Jin et al. (2020)), does not admit the desired representation.

Sherman et al. (2023) overcame this issue by employing a warm-up phase based on a reward-free pure exploration algorithm by Wagenmaker et al. (2022) to obtain initial backup operators \(_{h}^{0},h[H]\) and subsets \(}_{h},h[H]\) such that: (i) for every \(x,a}_{h}\) the bonuses (b in section 3), which are proportional to the estimation uncertainty of the value backup estimates, are small (\( 1\)); and (ii) for all policies \(_{M}\), the probability of reaching any \(x,a_{h[H]}}_{h}\) is small (\( K^{-1/2}\)). To ensure that the overall value estimates remain bounded, they truncate (zero out) the Q-value estimate of these nearly unreachable state-action pairs, an operation that allows for a low-dimensional representation of the policies. Nonetheless, their warm-up approach has several drawbacks.

* It runs for \(K_{0}=(d,H)\) episodes, contributing the leading term in their regret guarantee;
* It relies on a first-order regret algorithm by Wagenmaker et al. (2022) that is not PO-based and uses a computationally hard variance-aware Catoni estimator for robust mean estimation of the value backups, instead of the standard least-squares estimator. To maintain computational efficiency, they use an approximate version of the estimator, losing a factor of \(\) in the regret;
* Still, to the best of our knowledge, even the approximate estimator must be computed using binary search methods, making it hard to apply in practical methods that typically rely on gradient-based continuous optimization techniques;* It runs separate algorithms for each horizon \(h[H]\), using only \(1\) out of \(H\) samples during the warm-up phase;
* It is not reward-aware, and thus has to explore the space uniformly to ensure that the uncertainty is small for all policies, which could be highly prohibitive in practice.

Our feature contraction approach obtains the desired bounded Q-value estimates and low-complexity policy class without relying on a dedicated warm-up phase. Crucially, it only contributes a lower order term of \((d,H) K\) to the regret guarantee, thus improving the overall dependence on \(d\) and \(H\). Additionally, it uses all samples, is easy to implement, and is reward-aware. To understand the benefit of reward-awareness, consider an MDP where at the initial state the agent has two actions, each leading to a distinct MDP. Now, suppose that both MDPs have only a single state and action for the first \(H/2\) steps with one MDP incurring a loss of \(1\) in these steps while the other incurring \(0\) loss. Notice that regardless of the last \(H/2\) steps, the \(0\) loss MDP will outperform the \(1\) loss MDP. Nonetheless, the reward-free warm-up, which does not observe the losses, will have to fully explore both MDPs. In contrast, our reward-aware approach would quickly stop exploring the inferior MDP, leading to better performance in practice.

## 5 Analysis

In this section, we prove the main claims of our result. For full details see appendix A. We begin by introducing the main technical tool for our contraction mechanism - the contracted MDP.

### Contracted (sub) MDP

For any MDP \(=(,,x_{1},\{^{k}\}_{k=1}^{K},P,H)\) and contraction coefficients \(:[H]\) we define a contracted (sub) MDP \(}()=(,,x_{1},\{^{k}\}_{ k=1}^{K},,H)\) where as \(^{k}_{h}(x,a)=_{h}(x,a)^{k}_{h}(x,a)\) are the contracted losses and \(_{h}(x^{} x,a)=_{h}(x,a)P_{h}(x^{} x,a)\) are the contracted (sub) probability transitions. Notice that the transitions being a sub-probability measure implies that \(_{x^{}}P_{h}(x^{} x,a) 1\) as compared with a probability measure where this holds with equality. For any Markov policy \(_{M}\), let \(^{k,}_{h}(:,):,h[H]\) be the loss-to-go (or value) functions of the contracted MDP. In particular, these may be defined by the usual backward recursion

\[^{k,}_{h}(x;)=_{a( x)}[^{k}_{h}(x,a) x,a]+_{x^{}}_{h}(x^ {} x,a)^{k,}_{h+1}(x^{};),\]

with \(^{k,}_{H+1}(x;)=0\) for all \(x\). The following result shows that the value of any contracted MDP bound bounds its non-contracted variant.

**Lemma 2**.: _For any \(:[H],_{M},h[H],k[K]\), and \(x\) we have that \(^{k,}_{h}(x;) V^{k,}_{h}(x)\)._

**Proof.** The proof follows by backward induction on \(h[H+1]\). For the base case \(h=H+1\), both values are \(0\) and the claim holds trivially. Now suppose the claim holds for \(h+1\), then we have that for all \(x\)

\[^{k,}_{h}(x;) =_{a( x)}[^ {k}_{h}(x,a) x,a]+_{x^{}}_{h}(x^{}  x,a)^{k,}_{h+1}(x^{};)\] \[_{a( x)}[^{k}_{h}(x,a) x,a]+_{x^{}}P_{h}(x^{}  x,a)V^{k,}_{h+1}(x^{})=V^{k,}_{h}(x).\]

Next, for any epoch \(e[E]\), consider its contracted linear MDP (line 6 in algorithm 1) whose contraction coefficients are \(^{k_{x}}_{h}(x,a)=-_{w}\|(x,a)\|_{(^{k_{e}}_{ h})^{-1}}+ K\). The following result gives an upper bound on the performance gap between the contracted and non-contracted variants.

**Lemma 3**.: _For any \(e[E]\) and \(v^{d}\) we have that_

\[((x_{h},a_{h})-^{k_{e}}_{h}(x_{h},a_{h}))^{}v(4 ^{2}_{w}\|(x_{h},a_{h})\|^{2}_{(^{k}_{h})^{-1}}+2K^{-1}) (x_{h},a_{h})^{}v.\]

**Proof.** We have that

\[((x_{h},a_{h})-_{h}^{k_{e}}(x_{h},a_{h}))^{}v =(_{w}\|(x_{h},a_{h})\|_{(_{h}^{k_{w}})^{-1}} - K)(x_{h},a_{h})^{}v\] \[ 2(_{w}^{2}\|(x_{h},a_{h})\|_{(_{h}^{k_{w}})^{ -1}}+K^{-1})(x_{h},a_{h})^{}v\] \[(4_{w}^{2}\|(x_{h},a_{h})\|_{(_{h}^{k})^{-1}} ^{2}+2K^{-1})(x_{h},a_{h})^{}v,\]

where the first relation is by the property of the sigmoid \(1-(x)=(-x)\), the second is by a simple algebric argument that a quadratic function bounds the sigmoid (lemma 19 in appendix B), and the last relation uses \((_{h}^{k}) 2(_{h}^{k_{e}})\) by line 4 in algorithm 1 (see lemma 16 in appendix B). \(\)

We note that the analogous claim in Sherman et al. (2023) shows that for all \(_{M}\)

\[_{P,}((x_{h},a_{h})-_{\{x_{h}_ {h}\}}(x_{h},a_{h}))^{}v(x_{h}_{h} )_{x,a}(x,a)^{}v,\] (6)

where \(_{h}\) is an outcome of the reward-free warmup phase and \((x_{h}_{h}) K^{-1/2}\). Summing this over \(k[K]\) yields a term that scales as \(\). In contrast, we use a standard bound on elliptical potentials (lemma 15 in appendix B) to get that

\[_{k[K]}(4_{w}^{2}\|(x_{h}^{k},a_{h}^{k})\|_{(_{h}^{k})^ {-1}}^{2}+2K^{-1}) K.\]

This implies that the cost of our contraction is significantly lower than the truncation of Sherman et al. (2023). We achieve this reduced cost by using a quadratic (rather than linear) bound on the logistic function. The challenge in our approach is that the above bound only holds for the observed trajectories rather than for all policies as in Sherman et al. (2023). In what follows, we overcome this challenge using a novel regret decomposition.

### Regret bound

For any epoch \(e[E]\), let \(K_{e}\) be the set of episodes that it contains, and let \(_{1}^{k,}(x_{1};^{k_{e}})\) denote the value of its contracted MDP as defined above and in line 6 of algorithm 1. We bound the regret as

\[ =_{k[K]}V_{1}^{k,^{k}}(x_{1})-V_{1}^{k,^{*}}(x_{1})\] \[_{e[E]}_{k K_{e}}V_{1}^{k,^{k}}(x_{1})-_{1}^{k,^{*}}(x_{1};^{k_{e}})\] (lemma 2) \[=_{k[K]}V_{1}^{k,^{k}}(x_{1})-_{1}^{k}(x_{1})+ _{e[E]}_{k K_{e}}_{1}^{k}(x_{1})-_{1}^{k,^{*}}( x_{1};^{k_{e}})\] \[=V_{1}^{k,^{k}}(x_{1})-_{1}^ {k}(x_{1})}_{(i)-\,/\,}\] \[+_{h[H]}_{^{k_{e}},^{*}}[_{k K_{e}}_{a}_{h}^{k }(x_{h},a)(_{h}^{k}(a x_{h})-_{h}^{}(a x_{h}))]}_{(ii )-}\] \[+_{k K_{e}}_{h[H]} _{^{k_{e}},^{*}}_{h}^{k}(x_{h},a_{h})-_{h}^{k_{e}}(x_{h},a_{h})^{}(_{h}^{k}+_{h}_{h +1}^{k})}_{(iii)-},\]

where the last relation is by the extended value difference lemma (see Shani et al. (2020) and lemma 14 in appendix B). This decomposition is very similar to the standard one for PO algorithms, but with the crucial difference that term \((iii)\) depends on the contracted features \(_{h}^{k_{e}}(x_{h},a_{h})\) instead of the true features \((x_{h},a_{h})\). As a by-product, the expectation in terms \((ii)\) and \((iii)\) is taken with respectto the contracted MDP instead of the true one. The purpose of this modification will be made clear in the proof of optimism (see lemma 4).

In what follows, we bound each term deterministically, conditioned on the following "good event":

\[E_{1} = k[K],h[H]:\|_{h}^{k}-_ {h}^{k}\|_{_{h}^{k}}_{r}};\] (7) \[E_{2} =k[K],h[H]:\|(_{h}-_{h}^{k}) _{h+1}^{k}\|_{_{h}^{k}}_{p},\|_{h+1}^{k}\|_{ } 2H}.\] (8)

\(E_{1}\) and \(E_{2}\) are error bounds on the loss and dynamics estimation, respectively. In the full feedback setting, \(E_{1}\) holds trivially with \(_{r}=0\). In the bandit setting, it holds with high probability with \(_{r}=O()\) by well-established bounds for regularized least-squares estimation . Showing that \(E_{2}\) holds with high probability follows similarly to Sherman et al. (2023), again using least-squares arguments but also using the contraction to ensure that \(_{h}^{k}\) are bounded (see sketch at the end of this section and lemma 6 in appendix A for full details), specifically \(_{p}=O(Hd)\). The proof of theorem 1 is concluded by bounding each of the terms in the regret decomposition, summing over \(k[K]\) and using a standard bound on elliptical potentials (lemma 15 in appendix B). Term \((ii)\) is bounded using a standard Online Mirror Descent (OMD) argument (lemma 7 in appendix A).

Optimism and its cost.The following lemmas bound terms \((iii)\) and \((i)\), respectively.

**Lemma 4** (Optimism).: _Suppose that eqs. (7) and (8) hold, then_

\[_{h}^{k}(x,a)-_{h}^{k_{c}}(x,a)^{}(_{h}^{k}+ _{h}_{h+1}^{k}) 0, h[H],k[K],x,a .\]

Proof.: We have that

\[_{h}^{k}(x,a)-_{h}^{k_{c}}(x,a)^{}( _{h}+_{h}_{h+1}^{k}) =_{h}^{k_{c}}(x,a)^{}(_{h}^{ k}-_{h}+(_{h}^{k}-_{h})_{h+1}^{k})\] \[-_{h}\|_{h}^{k_{c}}(x,a)\|_{(_{h}^{k_ {c}})^{-1}}\] \[(_{r}+_{p})\|_{h}^{k_{c}}(x,a)\|_{ _{h}^{k-1}}-_{h}\|_{h}^{k_{c}}(x,a)\|_{(_{h}^{k_{c}})^{-1}}\] \[(_{r}+_{p}-_{b})\|_{h}^{k_{c}}(x,a)\| _{(_{h}^{k_{c}})^{-1}}=0,\]

where the first relation is by definition of \(_{h}^{k}\) (eq. (3) in algorithm 1), the second relation is by eqs. (7) and (8) together with Cauchy-Schwarz, the third relation follows since \(_{h}^{k_{c}}_{h}^{k}\) and the last one is by our choice \(_{b}=_{r}+_{p}\) (see theorem 9 in appendix A for hyper-parameter choices). 

Notice that the standard PO decomposition would have required that we bound the non-contracted expression \(_{P,}[_{h}^{k}(x,a)-(x,a)^{}(_{h}^{k}+ _{h}_{h+1}^{k})]\). In Sherman et al. (2023) the gap between this argument and that of lemma 4 can be bounded using eq. (6). However, the equivalent argument for our contraction is lemma 3, which is bounded only for \(^{k}\) and not for any policy \(_{M}\).

**Lemma 5** (Cost of optimism).: _Suppose that eqs. (7) and (8) hold, then for every \(k[K]\)_

\[V_{1}^{k,^{k}}(x_{1})-_{1}^{k}(x_{1})  3(_{r}+_{p})_{P,^{k}}_{h[ H]}\|(x_{h},a_{h})\|_{(_{h}^{k})^{-1}}\] \[+16H_{w}^{2}_{P,^{k}}_{h[ H]}\|(x_{h},a_{h})\|_{(_{h}^{k})^{-1}}^{2}+16H^{2}K^{-1}.\]

Proof.: First, by lemma 14 in appendix B, a value difference lemma by Shani et al. (2020),

\[V_{1}^{k,^{k}}(x_{1})-_{1}^{k}(x_{1}) =_{P,^{k}}_{h[H]}(x_{h},a_{h})^ {}_{h}+_{h}_{h+1}^{k}-_{k}^{k} (x_{h},a_{h}).\]

Now, using lemma 3 with \(v=_{h}^{k}+_{h}_{h+1}^{k}\) we have that \(|(x,a)^{}v| 4H\) (by eq. (8)) and thus

\[[(x_{h},a_{h})-_{h}^{k_{c}}(x_{h},a_{h})]^{} _{h}+_{h}_{h+1}^{k} 16H_{w}^{2}\|(x_{h},a_{h})\|_{( _{h}^{k})^{-1}}^{2}+16H^{2}K^{-1}.\]We can thus conclude the proof using standard arguments to show that

\[_{h}^{k_{e}}(x_{h}, a_{h})^{}_{h}+_{h}_{h+1}^{k} -_{k}^{k}(x_{h},a_{h})\] \[=_{h}^{k_{e}}(x_{h},a_{h})^{}_{h} ^{k}-_{h}^{k}+(_{h}-_{h}^{k})_{h+1}^{ k}+_{b}\|_{h}^{k_{e}}(x_{h},a_{h})\|_{(_{h}^{k_{e} })^{-1}}\] (eq. (3)) \[(_{r}+_{p})\|_{h}^{k_{e}}(x_{h},a_{h})\|_{( _{h}^{k})^{-1}}+_{b}\|_{h}^{k_{e}}(x_{h},a_{h})\|_{( _{h}^{k_{e}})^{-1}}\] (Cauchy-Schwarz, eqs. (7) and (8)) \[ 3(_{r}+_{p})\|_{h}^{k_{e}}(x_{h},a_{h})\|_ {(_{h}^{k})^{-1}}\] ( \[(_{h}^{k}) 2(_{h}^{k_{e}}),_{b}= _{r}+_{p}\] ) \[ 3(_{r}+_{p})\|(x_{h},a_{h})\|_{(_{h}^{k })^{-1}},\] ( \[(x), x\] )

as desired. 

Bounding the Q-values (proof sketch).The following are the main ideas in showing that \(E_{2}\) (eq. (8)) holds with high probability. First, we define appropriate value classes \(}_{h}\) that contain all value functions \(V_{h}\) of the form in eq. (4) whose underlying \(Q_{h}\) function (eq. (3)) satisfies \(\|Q_{h}\|_{} 2(H+1-h)\). Because both the bonus and contraction operator are kept fixed during each epoch, the log covering number of this class is logarithmic in \(K\) (similarly to Sherman et al. (2023)). Thus, we can use standard least squares arguments (lemma 22) to show that with high probability \(\|(_{h}-_{h}^{k})V\|_{_{h}^{k}}_ {p}\) for all \(k[K],h[H],V}_{h}\). The proof is concluded by showing that \(\|_{h}^{k}\|_{Q,h}=2(H+1-h)\), and thus \(_{h}^{k}}_{h}\), which implies that eq. (8) holds. We prove this by backward induction on \(h[H+1]\).

The base case \(h=H+1\) is satisfied because, by definition, \(_{H+1}^{k}=0\). Now, suppose the claim holds for \(h+1\) and we show it also holds for \(h\). Recalling the definition of \(\) in eq. (3), we have that

\[|_{h}^{k}(x,a)| =|_{h}^{k_{e}}(x,a)^{}(_{h}^ {k}+_{h}^{k}_{h+1}^{k})-_{b}\|_{h}^{k_{e}}( x,a)\|_{(_{h}^{k_{e}})^{-1}}\] \[|_{h}^{k_{e}}(x,a)^{}(_{h}+( {}_{h}^{k}-_{h})+(_{h}^{k}-_{h})_{h+1}^{ k}+_{h}_{h+1}^{k})|+_{b}\|_{h}^{k_{e}}(x,a)\|_{( _{h}^{k_{e}})^{-1}}\] \[ 1+\|_{h+1}^{k}\|_{}+\|_{h}^{k_{e}}(x,a )\|_{(_{h}^{k_{e}})^{-1}}\|_{h}^{k}-_{h}\|_ {_{h}^{k}}+\|(_{h}^{k}-_{h})_{h+1}^{k}\|_{ _{h}^{k}}+_{b},\]

where the last inequality also used the triangle and Cauchy-Schwarz inequalities, and that \(_{h}^{k_{e}}_{h}^{k}\). By the induction hypothesis, \(\|_{h+1}^{k}\|_{}\), \(\|_{h+1}^{k}\|_{}_{Q,h+1}\) and thus \(_{h+1}^{k}}_{h+1}\). Combining with \(E_{1}\) (eq. (7)) and plugging into the above we get that

\[|_{h}^{k}(x,a)| 1+_{Q,h+1}+(_{r}+_{p,h}+_{b})\| _{h}^{k_{e}}(x,a)\|_{(_{h}^{k_{e}})^{-1}}.\]

Now, using a technical algebraic argument (lemma 18), we show that

\[\|_{h}^{k_{e}}(x,a)\|_{(_{h}^{k_{e}})^{-1}}_{y 0 }[y(-_{w}y+ K)] 2_{w}^{-1}(eK).\]

Finally, plugging this into the above and choosing \(_{w} 2(_{r}+_{p,h}+_{b})(eK)\), we get

\[|_{h}^{k}(x,a)| 1+_{Q,h+1}+2_{w}^{-1}(_{r}+_{p,h}+ _{b})(eK) 2+_{Q,h+1}=_{Q,h},\]

concluding the induction.

## 6 Conclusions

In this paper we presented a simple and efficient contraction mechanism for policy optimization in linear MDPs, yielding an overall algorithm with improved regret guarantees under both stochastic (bandit feedback) and adversarial (full feedback) losses. We note that, in the stochastic setting, there are value iteration based methods (He et al. (2023)) that use variance reduction techniques to achieve better regret bounds. We conjecture that such techniques could be applicable to PO, however, this is highly non-trivial and thus left for future research. Finally, regarding practical implementations, we note that our bonuses and contraction technique are computationally feasible, especially compared to the reward-free warmup phase in Sherman et al. (2023). Nonetheless, it remains open whether our techniques could be applied heuristically to drive exploration in practical deep RL methods. In particular, it would be interesting to examine the necessity of the contraction mechanism. These are challenging questions on exploration in deep RL that we leave for future research.