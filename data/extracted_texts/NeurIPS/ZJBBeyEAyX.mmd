# OSLO:

One-Shot Label-Only Membership Inference Attacks

 Yuefeng Peng

University of Massachusetts Amherst

yuefengpeng@cs.umass.edu

&Jaechul Roh

University of Massachusetts Amherst

jroh@umass.edu

&Subhransu Maji

University of Massachusetts Amherst

smaji@cs.umass.edu

&Amir Houmansadr

University of Massachusetts Amherst

amir@cs.umass.edu

###### Abstract

We introduce One-Shot Label-Only (OSLO) membership inference attacks (MIAs), which accurately infer a given sample's membership in a target model's training set with high precision using just _a single query_, where the target model only returns the predicted hard label. This is in contrast to state-of-the-art label-only attacks which require \( 6000\) queries, yet get attack precisions lower than OSLO's. OSLO leverages transfer-based black-box adversarial attacks. The core idea is that a member sample exhibits more resistance to adversarial perturbations than a non-member. We compare OSLO against state-of-the-art label-only attacks and demonstrate that, despite requiring only one query, our method significantly outperforms previous attacks in terms of precision and true positive rate (TPR) under the same false positive rates (FPR). For example, compared to previous label-only MIAs, OSLO achieves a TPR that is at least \(7\) higher under a 1% FPR and at least \(22\) higher under a 0.1% FPR on CIFAR100 for a ResNet18 model. We evaluated multiple defense mechanisms against OSLO.

## 1 Introduction

Deep learning (DL) models are vulnerable to membership inference attacks (MIAs), where an attacker attempts to infer whether a given sample is in the target model's training set . The success of such MIAs may lead to severe individual privacy breaches as DL models are often trained on private data such as medical records  and facial images . Existing black-box MIAs fall into two categories: _score-based_ MIAs  and _decision-based_ MIAs , also known as _label-only_ MIAs. Score-based attacks assume access to the model's output confidence scores, which can be defended against if the model only outputs a label. In contrast, label-only attacks infer membership based on the labels returned by the target model, which is a stricter and more practical threat model.

State-of-the-art label-only MIAs measure the sample's robustness to adversarial perturbations, classifying samples with higher robustness as members. This robustness serves as a proxy for model confidence, which is typically higher for training samples. These attacks utilize _query-based_ black-box adversarial attacks  to determine the amount of necessary adversarial perturbation for each sample. However, accurately estimating this requires a large number of queries to the model, sometimes up to thousands for each sample. This approach is not only costly but also easily detectable . Furthermore, these attacks often lack precision and are unsuccessful in the low false positive rate regime, as indicated by previous studies  and our evaluations. In summary, existing label-only MIAs are not practical.

We propose novel One-Shot Label-Only (OSLO) MIAs that can infer a sample's membership in the target model's training set with just _a single query_, even when the target model only returns hard labels for input samples. Table 1 compares OSLO to previous label-only MIAs. OSLO is based on constructing transfer-based adversarial example, with just enough magnitude of adversarial perturbation added to each sample to cause misclassification if the sample is non-member (but not enough if it is a member) based on a set of source and validation models, a technique that was initially proposed in . Prior work has considered using a fixed threshold of adversarial perturbation across all samples to infer membership status, resulting in a lower precision [7; 6].

We extensively compared OSLO with state-of-the-art label-only methods [7; 6]. Following recent standards, we evaluated the attacks using metrics such as the true positive rate (TPR) under a low false positive rate (FPR) and performed a precision/recall analysis. Our results show that previous label-only MIAs perfon poorly, exhibiting high false positive rates. In contrast, OSLO achieves high precision in identifying members, outperforming prior work by a significant margin. For example, as shown in Figure 2, OSLO is 5\(\) to 67\(\) more powerful than other label-only MIAs in terms of TPR under 0.1% FPR across three datasets using a ResNet18  model. OSLO achieves over 95% precision on all three datasets on ResNet18 model with a recall greater than 0.01, while the highest precision achieved by other attacks is only 71.4%, 84.2%, and 67.9% respectively. Furthermore, we conduct a detailed comparison of OSLO with previous work, exploring why previous label-only attacks fail to achieve high precision.

## 2 Background and preliminaries

### Adversarial attacks

Early works [14; 15] have demonstrated the susceptibility of deep neural networks (DNNs) to adversarial attacks. These attacks involve crafting subtle perturbations to input data, causing a DNN to

    & Require no & Require & & \\ Attack method & knowledge about & no & Attack & Num of \\ target model & auxiliary & precision & & queries \\  & architecture & data & & used \\  Transfer Attack  & ✗ & ✗ & 60.9\% & \(|D_{shot}|\) \\ Data Augmentation  & ✓ & ✓ & 69.2\% & \( 10\) \\ Boundary Attack (Gaussian Noise)  & ✓ & ✓ & 61.3\% & \(\)700 \\ Boundary Attack (HopSkipJump) [7; 6] & ✓ & ✓ & 64.5\% & \(\)6000 \\ Boundary Attack (QEBA)  & ✓ & ✓ & 71.4\% & \(\)6000 \\ OSLO (Ours) & ✓ & ✗ & **96.2\%** & **1** \\   

Table 1: **Comparison to previous label-only attacks.** We report the highest attack precision that these attacks can achieve with a recall greater than 1% on CIFAR-10 using a ResNet18 model.

Figure 1: **An illustration of OSLO versus the state-of-the-art boundary attack.** Boundary attack requires querying the target model thousands of times, whereas OSLO requires only a single query.

make incorrect predictions while the perturbations remain almost imperceptible to humans. Adversarial attacks can significantly compromise model performance, resulting in issues like misclassification or misgeneration. These attacks are typically classified based on the adversary's level of access to the model : white-box and black-box settings. In white-box attacks [17; 15; 18; 19; 20; 21], adversaries have full knowledge of the victim model, including its architecture, parameters, and gradients. In contrast, black-box attacks [22; 23] limit adversaries to querying the model and obtaining output labels or confidence scores. We primarily discuss black-box attacks to align with the threat model of our MIA settings.

Black-box adversarial attacks.Black-box attacks primarily include two methods: _(i)_**Query-based** attacks iteratively query the target model to gather information and craft adversarial examples, adjusting perturbations based on the model's feedback. These attacks often require many queries to be effective. _(ii)_**Transfer-based** attacks generate adversarial examples using a white-box model and then transfer these examples to attack another model. Previous works have proposed various transfer-based attacks. For instance, Momentum Iterative Fast Gradient Sign Method (MI-FGSM)  introduces momentum iterative gradient-based methods for generating adversarial examples to effectively attack both white-box and black-box models. Diverse Inputs Iterative Fast Gradient Sign Method (DI\({}^{2}\)-FGSM)  enhances the transferability of adversarial examples by applying random transformations to input images at each iteration of the attack process. Translation-Invariant Fast Gradient Sign Method (TI-FGSM)  optimizes perturbations over a set of translated images to make the adversarial examples less sensitive to specific model features and improve their transferability. Admix  enhances traditional input transformations by mixing the input image with images from other categories to create admixed images. These admixed images are used to compute gradients, helping to create more transferable adversarial examples.

### Membership inference attacks

Membership inference attacks (MIAs) [27; 28; 29; 30; 31; 32; 33] aim to determine whether a specific data sample is a member of the training dataset of a target model. Existing MIAs often assume that the attacker can access the target model's confidence scores to infer membership. These score-based attacks exploit the difference in confidence scores for training versus non-training data, as models often exhibit higher confidence for samples they have seen during training [1; 4; 5]. Such attacks can be easily defended by limiting access to only the predicted labels, which reduces the amount of information available to the attacker.

Label-Only MIAs.Compared to score-based attacks, a more practical and stringent type of attack is decision-based or label-only MIAs [7; 31], which operate under the constraint of having access only to the final decision labels. Several label-only attacks have been proposed. For example, inspired by the transferability of adversarial examples, transfer attacks  explore the transferability of membership information. They use a shadow dataset labeled by the target model to train a shadow model and perform a score-based attack on the shadow model, expecting that the differences between members and non-members in the target model will reflect similarly in the shadow model. Data augmentation attacks  exploit the robustness of samples to data augmentation transformations to differentiate members from non-members, with higher robustness indicating membership. Similarly, state-of-the-art label-only MIAs, known as boundary attacks [31; 7], use the robustness of samples to strategic input perturbations as label-only proxies for the model's confidence, where higher robustness implies a higher confidence score. Boundary attacks have different methods for adding perturbations. For example, Gaussian noise is one method where the attacker continuously adds Gaussian noise until the label changes. Attackers also use adversarial attacks to measure the robustness of samples to adversarial perturbations. Specifically, query-based adversarial attacks like HopSkipJump  and QEBA  algorithms are used to add perturbations to each sample to generate adversarial examples.

Existing label-only MIAs typically require many queries to be effective [6; 7] and struggle to achieve high TPRs at low FPRs . A recent work YOQO  reduced the query budget. However, by design, it cannot adjust the TPR-FPR trade-off and remains ineffective in the low FPR regime, limiting its practicality.

## 3 Oslo

### Problem definition

Following Ye et al.  and Leemann et al. , we view MIA as a hypothesis testing problem:

\[H_{0}:(x,y) D vs H_{1}:(x,y) D\] (1)

Successfully rejecting the null hypothesis equates to determining that the sample \((x,y)\) is a member in the target model's training set \(D\). Given a sample for inference, we initially assume that the sample is a non-member (\(H_{0}\)). Our objective is to design an attack method that can reject the null hypothesis with high confidence.

### Threat model

Attacker's capability.Given a target model \(f\), we assume that the attacker can only access \(f\) in a black-box manner, meaning that means that for any input \(x\), the attacker can only observe \(f(x)\) but cannot access the internal parameters \(\) or any intermediate outputs of \(f\). Furthermore, we consider a more restrictive scenario: label-only access, where the attacker can only obtain the predicted labels (i.e., \(= f(x)\)) from the target model, not the confidence scores or probabilities associated with each class. This label-only, black-box attack scenario represents one of the most challenging and realistic settings for conducting attacks against DL models. Similar to previous attacks [1; 7], it is assumed that the attacker possesses an auxiliary dataset \(D_{}\) that shares the same distribution as the target model's training set for the purpose of training their surrogate models. However, we do not assume the attacker has any knowledge of the target model, including its architecture.

Attacker's goal.The attacker's objective is to reliably infer whether a given sample is a member of the target model's training set with high precision. As underscored by Carlini et al. , identifying individuals within sensitive datasets with high precision poses a significant privacy risk, even if only a few users are correctly identified. Moreover, the precision of these attacks is not just a concern for individual privacy; it also lays the groundwork for more advanced extraction attacks , making the pursuit of high precision in MIAs a critical focus for attackers. Additionally, the attacker aims to infer membership with as few queries as possible, as excessive querying not only incurs significant costs but is also detectable by defenders . Our OSLO limits the query budget to a single query.

### Intuition of OSLO

Previous work [4; 1] has demonstrated that target models tend to be overconfident on members. existing label-only attacks [6; 7] suggest that models are more confident about members, making it more difficult to "change their mind" on these samples. Their experiments also show that members are usually more robust to adversarial perturbations than non-members. We believe that the differences in the required adversarial perturbation between members and non-members can manifest in two ways: (i) on average, members tend to require more adversarial perturbation than non-members; and (ii) for each individual sample, the member status may demand more adversarial perturbation than the non-member status. Previous label-only MIAs [6; 7] have exploited the former observation, but this has led to high false positive rates and low precision. Our OSLO is based on the latter. We add just enough adversarial perturbation to cause a misclassification for the sample if it is a non-member but not if it is a member. If the sample is not successfully misclassified, it indicates that the added perturbation was insufficient, and the sample is likely a member. Note that here,'sufficient perturbation' refers to a variable standard for each sample, changing with the sample, rather than a uniform standard for all samples.

### Attack method

Previous label-only MIAs face two main issues: _low precision_ and _high query budget_. We address both issues simultaneously by proposing our _transfer-based_ adversarial attack based attack, which implements the aforementioned hypothesis testing framework. Specifically, we first train a group of surrogate models. For each sample, we construct a transferable adversarial example using the surrogate model(s). Then, we input the crafted adversarial example into the target model and obtainthe predicted label. If the sample is misclassified, it indicates that the adversarial example has successfully transferred, implying that there is not enough evidence to reject the null hypothesis \(H_{0}\). Otherwise, we reject \(H_{0}\) and determine the sample as a member. It is worth noting that the perturbation added to each sample should be sufficient for it to transfer as a non-member but not as a member. Therefore, carefully controlling the scale of perturbation added to each sample is required. The framework of OSLO is divided into three stages: _surrogate model training_, _transferable adversarial example construction_, and _membership inference_.

Surrogate model training.The attackers first train their own surrogate models to construct adversarial examples. As assumed in Section 3.2, we assume that the attacker has an auxiliary dataset with the same distribution as the target model's training set. The attacker trains a group of models with different structures on this dataset, training \(N\) models for each structure. These models are then divided into source models and validation models based on different purposes in the adversarial example generation stage. Source models are used to calculate gradients for generating adversarial perturbations, while validation models are used to control the magnitude of the added perturbations.

Transferable adversarial example generation.After the source models are trained, the attacker generates adversarial examples on these models using transfer-based adversarial attacks. Existing transfer-based attacks typically search for adversarial examples within a fixed \(l_{p}\)-norm ball and do not minimize the perturbation added. In other words, these attacks allocate a uniform perturbation budget \(\) to all samples, accepting any adversarial examples found within this budget. While this approach is reasonable in the context of general adversarial attacks, it is not suitable for our MIA framework, as OSLO requires finer-grained control over the amount of perturbation added.

In OSLO, the perturbation budget for each sample is adaptive. Specifically, we utilize the Geometry-Aware (GA) framework proposed in , which employs a set of validation models to regulate the amount of perturbation added to each sample. The process of adding perturbations is incremental. During this process, validation models are used to monitor the transferability of the sample. The perturbation addition process is terminated early--referred to as early stopping--when the perturbation added is sufficient for the sample to deceive the validation model, and the confidence of the correct class on the validation model drops below a specified threshold \(\):

\[((x)=y)=(x;))}{_{j}(h_{j}(x; ))}<.\] (2)

where \(h_{j}(x;)\) denotes the logits for class \(j\) produced by the validation model, and \(y\) is the ground truth class. Through this method, the process can be terminated when the transferability of each sample reaches a certain level, resulting in tailored perturbations for each sample. This can more effectively distinguish the differences of the sample in the _in_ and _out_ worlds.

```
0: Benign input \(x\) with label \(y\); source models \(g\); validation model \(h\); number of sub-procedures \(K\); number of iterations \(N\); step size \(\); maximum perturbation size \(\) and threshold \(\); Transfer-based unrestricted adversarial example \(x^{}\) with approximately minimum change; \(x_{0} x\); for\(k=1,2,,K\)do \(x^{}_{0} x_{k-1}\); for\(i=1\)to\(N\)do  Compute gradient: \(g_{i}_{x}L(g(x^{}_{i-1},y)\);  Update adversarial example using the update function: \(x^{}_{i}(x^{}_{i-1},g_{i},)\);  Clip \(x^{}_{i}\) to ensure it is within the \(\)-ball of \(x\): \(x^{}_{i}(x^{}_{i},x-,x+ )\); \((x^{}_{i}))}{_{j}(h_{j}(x^{ }_{i}))}\); ifconf \(<\)then return\(x^{}_{i}\); // early-stopping criterion  end for  end for \(x_{k} x^{}_{N}\);  end for return\(x_{K}\); ```

**Algorithm 1**Transferable adversarial example generation.

Algorithm 1 outlines our method for generating transferable adversarial examples, adapted from Liu et al.'s GA framework  to align with our objective. The algorithm inputs with a benign sample \(x\) with its corresponding label \(y\), a set of source models denoted as \(g\), and a set of validation models \(h\). The algorithm proceeds through \(K\) sub-procedures, each conducting an iterative refinement of the adversarial example across \(N\) iterations, guided by a pre-established step size \(\) and bounded by a maximal perturbation magnitude \(\). At each iteration, the gradient of the loss function \(L\) with respect to the adversarial input is computed, informing the perturbative update. This process is modulated by the validation model \(h\). If at any step the confidence of the ground-truth class falls below the threshold \(\), the early stopping is triggered to prevent over-perturbation, thus ensuring that the perturbations are precisely calibrated for each sample.

Membership inference.Finally, for each sample \(x\), the attacker inputs the transferable adversarial example \(x^{}\), generated by Algorithm 1, into the target model \(f\). The classification outcome determines the evidence against the null hypothesis \(H_{0}\). If \(_{i}f(x^{})_{i} y\), it suggests insufficient evidence to reject \(H_{0}\). In contrast, a classification of \(_{i}f(x^{})_{i}=y\) allows us to reject \(H_{0}\), hence classifying \(x\) as a member. Our membership inference strategy seeks to reject \(H_{0}\) with high confidence, aiming to achieve high precision in the identification of members.

## 4 Evaluation

### Experimental setup

Datasets.We utilized three datasets commonly used in prior works : CIFAR-10 , CIFAR-100 and SVHN . For CIFAR-10 and CIFAR-100, we selected 25,000 samples to train the target model under attack. For SVHN, we randomly selected 2,000 samples to train the target model. For each dataset, we trained the surrogate models used for the attack with a set of data that is the same size as the training set of the target model but disjoint. Further details on dataset splits can be found in Appendix A.1.1.

Models.We adopted the widely used ResNet18  and DenseNet121  as the target model architectures. In addition to these two, we incorporated five additional model architectures as surrogate models. Detailed training and attack setup are provided in Appendix A.1.

Baseline Attacks.We compare OSLO against five state-of-the-art label-only MIAs, including the transfer attack , three boundary attacks [7; 6], and the data augmentation attack . Detailed settings are provided in Appendix A.1.4. We exclude YOQO  from our evaluation, as it is not compatible with our evaluation metrics (see Appendix A.6).

Metrics.We employ the following metrics to evaluate the effectiveness of the attacks: _(i)_ attack **TPR and FPR**: Carlini et al. suggest that membership inference attacks should be analyzed using TPR and FPR, especially considering that TPR in a low FPR setting is crucial for assessing the success of MIAs. We report the log ROC curve to show the TPR and FPR of the attack, with a focus on the low FPR regime; _(ii)_ attack **precision and recall**: Attack precision reflects how reliably an attack can infer membership. High-precision attacks allow an attacker to credibly violate the privacy of a portion of the samples. Specifically, precision is defined as the proportion of true members correctly identified among all positively predicted members by an adversary. Meanwhile, recall quantifies the proportion of true members that the adversary has correctly identified out of all actual members. We discarded average-case metrics such as accuracy and AUC, as they have been shown not to accurately reflect the threat of MIAs .

### Results

#### 4.2.1 Evaluating attack TPR under Low FPR

We first evaluate six attacks, including OSLO, focusing on TPR under low FPR regime. For OSLO, we control the trade-off between TPR and FPR by adjusting the threshold \(\) during the transferable adversarial example generation phase. A lower \(\) results in more perturbation added to each sample. Consequently, if the samples still fail to deceive the target model, it is more likely that they are members, thereby reducing FPR. For other attacks, we adjust their corresponding parameters to manage TPR and FPR. For instance, in the boundary attack, increasing the perturbation threshold for identifying members ensures that only samples requiring more significant perturbations are classified as members. Based on the assumption that members generally need greater perturbation than non-members, this method should minimize the number of samples mistakenly identified as members and enhance the credibility of the attack.

As shown in Figure 2, **OSLO outperforms all previous label-only MIAs by a large margin**. For example, compared to previous attacks, OSLO achieves a TPR that ranges from 5\(\) to 67\(\) higher under 0.1% FPR, and from 3\(\) to 16\(\) higher under 1% FPR on ResNet18, evaluated across three datasets. On DenseNet121, the improvements range from 2\(\) to 12\(\) under 1% FPR. It can be observed that previous attacks were unsuccessful in the low FPR regime, whereas OSLO can successfully identify some members even at low FPR. For example, on CIFAR-100 using ResNet18, at a 0.1% FPR, the best of the other label-only MIAs achieves only a 0.3% TPR, while OSLO reaches 6.7%. Under a 1% FPR, OSLO achieves a TPR of 18.9%, with other attacks reaching at most 2.7%. These comparisons show that, **although OSLO requires only one query to the target model, it can reliably identify part of the members, while previous attacks, despite needing to perform up to thousands of queries, are almost unable to credibly identify members.**

#### 4.2.2 Precision and recall analysis

We then measured the trade-off between attack precision and recall. In MIAs, precision is considered a crucial metric, whereas recall is relatively less important . For instance, a precision of 1.0 means that the attacker can be 100% certain that the identified samples are members, leading to significant privacy breaches, even if only a few samples are affected. Conversely, a recall of 1.0 can be achieved by classifying all samples as members, but this does not cause any privacy breach. For MIAs, a desired outcome would be the ability to trade recall for increased precision.

As shown in Figure 4 and Figure 5, **OSLO is the only one that effectively trades recall for precision.** For example, by trading recall, OSLO achieves over 90% precision across all three datasets with ResNet18 as the target model. Specifically, on CIFAR-10, CIFAR-100, and SVHN, with recall rates of 5%, 44.7%, and 9% respectively, OSLO attained precisions of 96.2%, 92.0%, and 90.9%. In contrast, the highest precision achieved by other attacks at similar recall levels was 67.4%, 82.5%, and 65.2% respectively. This demonstrates that **OSLO can identify members with high precision, a capability that all previous label-only attacks could not match.** Previous methods could not

Figure 3: **ROC curves for various label-only attacks on three different datasets on DenseNet121. Each line represents the TPR of an attack under different FPRs, with an emphasis on the low-FPR regime using a logarithmic scale.**

Figure 2: **ROC curves for various label-only attacks on three different datasets on ResNet18. Each line represents the TPR of an attack under different FPRs, with an emphasis on the low-FPR regime using a logarithmic scale.**

improve precision by adjusting hyper-parameters, due to limitations inherent in their designs. We discuss this further in Section 6.1.

## 5 Ablation study

### Effect of using different adversarial attacks

As introduced in Section 3.4, OSLO leverages transfer-based adversarial attacks. We evaluated the membership inference performance of OSLO using six different transfer-based adversarial techniques within its framework, specifically TI , DI , MI , Admix , and the combinable methods TDMI and TMDAI. The results are presented in Figure 6. Observations reveal no significant differences in the effectiveness of using these various transfer-based attacks. This may suggest that the efficacy of the attack predominantly relies on the inherent design of the OSLO framework rather than the specific choice of the individual transfer techniques.

### Effectiveness of OSLO when target and surrogate models use different training algorithms

We assume that the attacker is unaware of the target model's architecture, using surrogate models with different structures. We further explore scenarios where the surrogate and target models are trained with different optimization algorithms. In particular, we evaluated OSLO on CIFAR-10 using ResNet18, where the target models were trained with SGD, while the surrogate models used Adam. The results are shown in Table 2.

Figure 4: **Precision-Recall curves for various label-only attacks on ResNet18. Each line represents the trade-off between precision and recall for an attack as the attack parameter is varied.**

Figure 5: **Precision-Recall curves for various label-only attacks on DenseNet121. Each line represents the trade-off between precision and recall for an attack as the attack parameter is varied.**

Figure 6: **Precision-Recall curve showing the attack performance of OSLO using different transfer-based adversarial attacks on CIFAR-10.**

Notably, OSLO achieves over 95% precision, with a TPR exceeding 9% at 1% FPR in both settings. These results confirm that OSLO remains effective even when the target models are trained with algorithms different from those of the surrogate models.

### Impact of validation models in OSLO

OSLO uses validation models to adjust the scale of perturbations added to adversarial examples. To understand the impact of validation models, we conducted an ablation study on CIFAR-10 using ResNet18 without the validation models, where a uniform perturbation budget was applied to all samples. The results are shown in Table 3.

These results indicate that without the validation model, applying a uniform perturbation budget across all samples leads to significantly reduced attack effectiveness. For example, at \(=32/255\), nearly all members and non-members were misclassified, resulting in both TPR and FPR being very low. This highlights the critical role of validation models in effectively calibrating perturbations.

### Effect of using more than one shot

OSLO employs a single query to the target model to determine a sample's membership status. We also explore the potential advantages of extending OSLO to utilize multiple queries per sample, referred to as multi-shot. Specifically, Figure 7 illustrates the precision and recall of OSLO under different adversarial example generation thresholds, \(\).

We further report on combining results from the current threshold with those from previous, higher thresholds. For example, 'three shots' represents feeding the current shot and the previous two shots' generated images to the target model, identifying samples as members if all three adversarial examples fail to fool the target model. As shown in Figure 7, we find that using more shots does not yield a significant improvement. We attribute this to the observation that even with multiple shots, only the shot with the lowest \(\) plays a decisive role.

## 6 Discussion

### Why OSLO outperforms previous approaches

In this section, we discuss why previous attacks fail to achieve high precision or TPR under low FPR conditions, and how OSLO outperforms them. The boundary attack employs a global threshold of perturbation magnitude to differentiate between members and non-members. However, regardless of the chosen threshold, a considerable proportion of non-members are always found among the samples that require perturbations above this threshold (see the cumulative distribution function (CDF) in Figure 7(a)). On the other hand, by lowering the validation threshold \(\) in OSLO, the proportion of non-members among the failed adversarial attack samples significantly decreases, effectively squeezing out the non-members. For example, when classifying 4.4% of the samples as members, the

   Target model & Source/validation models & Attack precision (Recall  1\%) & Attack TPR@1\% FPR \\  SGD, lr=0.01 & Adam, lr=0.001 & 95.9\% & 10.0\% \\ SGD, lr=0.1 & Adam, lr=0.001 & 95.3\% & 9.2\% \\   

Table 2: OSLO performance on CIFAR-10 with ResNet18 when target and surrogate models are trained using different algorithms.

  
**Perturbation budget \(\)** & **TPR** & **FPR** \\ 
8/255 & 20.1\% & 13\% \\
16/255 & 2.3\% & 1.4\% \\
32/255 & 0.1\% & 0.1\% \\   

Table 3: **Attack TPR and FPR of OSLO without validation models on CIFAR-10 using ResNet18.**

Figure 7: **Precision-Recall curve illustrating the attack performance of OSLO using more shots.**

boundary attack still includes 39% non-members (false positives) among those identified as members, whereas OSLO has only 10% non-members, resulting in 90% attack precision. We further discuss this by measuring the magnitude of the adversarial perturbation added to members and non-members in boundary attack versus OSLO on CIFAR-100, details are in Appendix A.2.

### Mitigation

In addition to demonstrating the effectiveness of OSLO, we also evaluated six defensive strategies [41; 42; 43] aimed at mitigating membership privacy leakage. These experiments were conducted on the CIFAR-10 dataset using ResNet18 as the target model.

We trained three target models with different hyperparameters for each defense mechanism that requires retraining. Detailed configuration settings are provided in Appendix A.1.5. The results are presented in Figure 9. It can be observed that confidence alteration methods, such as MemGuard , are ineffective against OSLO, as expected. Other methods that mitigate memorization require strong regularization to be effective but result in a decrease in the model's test accuracy. Additionally, we tested OSLO against target models trained with adversarial training (details in Appendix A.5), and observed that while adversarial training reduces OSLO's effectiveness to some extent, it is not sufficient as a standalone defense.

## 7 Conclusions and limitations

In this paper, we propose OSLO, One-Shot Label-Only Membership Inference Attack (MIAs). OSLO is based on transfer-based adversarial attacks and identifies members by the different perturbations required when a sample is a member versus a non-member. We empirically demonstrate that OSLO not only operates under label-only settings with a single query but also significantly outperforms all previous label-only attacks by a large margin in terms of true positive rate (TPR) under the same false positive rate (FPR) and attack precision. Thus, OSLO sets a new benchmark for label-only MIAs and can serves as an effective measure of privacy leakage in these settings. We also delve into why OSLO outperforms existing methods. Furthermore, we evaluate multiple defenses against OSLO, highlighting its robustness.

OSLO requires training surrogate models that need an auxiliary dataset from the same distribution as the target model's training set. Although this is a common assumption in MIAs [1; 5] and can be alleviated by using synthetic data as demonstrated in prior work , it remains a limitation.

Figure 8: **Comparison of boundary attack and OSLO regarding proportions of true positives (accurately identified members) and false positives (misclassified non-members) under varying threshold settings on CIFAR-10.**

Figure 9: **Attack TPR under 1% FPR of OSLO against various defense mechanisms.**