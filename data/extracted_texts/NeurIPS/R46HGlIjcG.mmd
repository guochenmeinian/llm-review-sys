# Localizing Memorization in SSL Vision Encoders

Wenhao Wang\({}^{1}\), Adam Dziedzic\({}^{1}\), Michael Backes\({}^{1}\), Franziska Boenisch\({}^{1}\)

\({}^{1}\)CISPA, Helmholtz Center for Information Security

Correspondence to boenisch@cispa.de

###### Abstract

Recent work on studying memorization in self-supervised learning (SSL) suggests that even though SSL encoders are trained on millions of images, they still memorize individual data points. While effort has been put into characterizing the memorized data and linking encoder memorization to downstream utility, little is known about where the memorization happens inside SSL encoders. To close this gap, we propose two metrics for localizing memorization in SSL encoders on a per-layer (LayerMem) and per-unit basis (UnitMem). Our localization methods are independent of the downstream task, do not require any label information, and can be performed in a forward pass. By localizing memorization in various encoder architectures (convolutional and transformer-based) trained on diverse datasets with contrastive and non-contrastive SSL frameworks, we find that (1) while SSL memorization increases with layer depth, highly memorizing units are distributed across the entire encoder, (2) a significant fraction of units in SSL encoders experiences surprisingly high memorization of individual data points, which is in contrast to models trained under supervision, (3) _atypical_ (or outlier) data points cause much higher layer and unit memorization than standard data points, and (4) in vision transformers, most memorization happens in the fully-connected layers. Finally, we show that localizing memorization in SSL has the potential to improve fine-tuning and to inform pruning strategies.

## 1 Introduction

Self-supervised learning (SSL) ([16; 17; 13; 4; 30; 27; 29]) enables pre-training large encoders on unlabeled data to generate feature representations for a multitude of downstream tasks. Recently, it was found that, even though their training datasets are large, SSL encoders still memorize individual data points ([36; 47]). While prior work characterizes the memorized data and studies the effect of memorization to improve downstream generalization (), little is known about where in SSL encoders memorization happens.

The few works on localizing memorization are usually confined to supervised learning (SL) ([3; 45; 35]), or operate in the language domain ([55; 37; 7; 44; 14]). In particular, most results are coarse-grained and localize memorization on a per-layer basis [3; 45] and/or require labels [3; 35].

To close the gap, we propose two novel metrics for localizing memorization in SSL encoders in the vision domain. Our LayerMem localizes memorization of the training data within the SSL encoders on a layer-level. For a more fine-grained localization, we turn to memorization in individual _units_ (_i.e.,_ neurons in fully-connected layers or channels in convolutional layers). We propose UnitMem which measures memorization of individual training data points through the units' sensitivity to these points. Both our metrics can be computed independently of a downstream task, in a forward pass without gradient calculation, and without labels, which makes them computationally efficient and well-suited for the large SSL encoders pretrained on unlabeled data. By performing a systematic study on localizing memorization with our two metrics on various encoder architectures (convolutionaland transformer-based) trained on diverse vision datasets with contrastive and non-contrastive SSL frameworks, we make the following key discoveries:

**Memorization happens through the entire SSL encoder.** By analyzing our LayerMem scores between subsequent layers, we find that the highest memorizing layers in SSL are not necessarily the last ones, which is in line with findings recently reported for SL . While there is a tendency that higher per-layer memorization can be observed in deeper layers, similar to SL [3; 45], our analysis of memorization on a per-unit level highlights that highly memorizing units are distributed across the entire SSL encoder, and can also be found in the first layers.

**Units in SSL encoders experience high memorization.** By analyzing SSL encoders with our UnitMem metric, we find that a significant fraction of their units experiences high memorization of individual training data points. This stands in contrast with models trained using SL for which we observe high class memorization, measured as the unit's sensitivity to any particular class. While these results are in line with the two learning paradigms' objectives where SL optimizes to separate different classes whereas SSL optimizes foremost for instance discrimination , it is a novel discovery that this yields significantly different memorization patterns between SL and SSL down to the level of individual units.

**Atypical data points cause higher memorization in layers and units.** While prior work has shown that SSL encoders overall memorize atypical data points more than standard data points , our study reveals that the effect is constant throughout _all_ encoder layers. Hence, there are no particular layers responsible for memorizing atypical data points, similarly as observed in SL . Yet, memorization of atypical data points can be attributed on a unit-level where we observe that the highest memorizing units align with the highest memorized (atypical) data points and that overall atypical data points cause higher unit memorization than standard data points.

**Memorization in vision transformers happens mainly in the fully-connected layers.** The memorization of transformers  was primarily investigated in the language domain [26; 43]. However, the understanding in the vision domain is lacking, and due to the difference in input and output tokens (language transformers operate on discrete tokens while vision transformers operate on continuous ones), the methods for analysis and the findings are not easily transferable. Yet, with our methods to localize memorization, we are the first to show that the same trend holds in vision transformers that was previously reported for language transformers, namely that memorization happens in the fully-connected layers.

Finally, we investigate future applications that could benefit from localizing memorization and identify _more efficient fine-tuning_ and _memorization-informed pruning strategies_ as promising directions.

In summary, we make the following contributions:

* We propose LayerMem and UnitMem, the first practical metrics to localize memorization in SSL encoders on a per-layer basis and down to the granularity of individual units.
* We perform an extensive experimental evaluation to localize memorization in various encoder architectures trained on diverse vision datasets with different SSL frameworks.
* Through our metrics, we gain new insights into the memorization patterns of SSL encoders and can compare them to the ones of SL models.
* We show that the localization of memorization can yield practical benefits for encoder fine-tuning and pruning.

## 2 Related Work

Ssl.SSL relies on large amounts of unlabeled data to train encoder models that return representations for a multitude of downstream tasks . Especially in the vision domain, a wide range of SSL frameworks have recently been introduced [16; 17; 13; 4; 30; 27; 29]. Some of them rely on contrastive loss functions [16; 29; 27] whereas others train with non-contrastive objective functions [41; 17; 13; 30].

Memorization in SL.Memorization was extensively studied in SL [52; 2; 15]. In particular, it was shown that it can have a detrimental effect on data privacy, since it enables data extraction attacks [9; 10; 11]. At the same time, memorization seems to be required for generalization, in particular for long-tailed data distributions [23; 24]. It was also shown that _harder_ or more atypical data points [2; 43] experience higher memorization. While all these works focus on studying memorization from the data perspective and concerning its impact on the learning algorithm, they do not consider where memorization happens.

Memorization in SSL.Even though SSL rapidly grew in popularity during recent years, work on studying memorization in SSL is limited. Meehan et al.  proposed to quantify Deja Vu memorization of SSL encoders with respect to particular data points by comparing the representations of these data points with the representations of a labeled public dataset. Data points whose \(k\) nearest public neighbors in the representation space are highly consistent in labels are considered to be memorized. Since SSL is aimed to train _without labels_, this approach is limited in practical applicability. More recently, Wang et al.  proposed SSLMem, a definition of memorization for SSL based on the leave-one-out definition from SL [23; 24]. Instead of relying on labels, this definition captures memorization through representation alignment, _i.e.,_ measuring the distance between representations of a data point's multiple augmentations. Since both works rely on the output representations to quantify memorization, neither of them is suitable for performing fine-grained localization of memorization. Yet, we use the setup of SSLMem as a building block to design our LayerMem metric which localizes memorization per layer.

Localizing Memorization.In SL, most work focuses on localizing memorization on a per-layer basis and suggests that memorization happens in the deeper layers [3; 45]. By analyzing which neurons have the biggest impact on _predicting the correct label_ of a data point, Maini et al.  were able to study memorization on a per-unit granularity. They do so by zero-ing out random units until a label flip occurs. Their findings suggest that only a few units are responsible for memorizing outlier data points. Yet, due to the absence of labels in SSL, this approach is inapplicable to our work. In the language domain, a significant line of work aims at localizing where semantic facts are stored within large language transformers [55; 37; 7; 44]. Chang et al.  even proposed _benchmarks_ for localization methods in the language domain. In the injection benchmark (INJ Benchmark), they fine-tune a small number of neurons and then assess whether the localization method detects the memorization in these neurons. The deletion benchmark (DEL Benchmark) first performs localization, followed by the deletion of the responsible neurons, and a final assessment of the performance drop on the data points detected as memorized in the identified neurons. Since in SSL, performance drop cannot be measured directly due to the absence of a downstream task, the deletion approach is not applicable. Instead, we verify our UnitMem metric in a similar vein to the INJ Benchmark by fine-tuning a single unit on a data point and localizing memorization as we describe in Section 5.2.

Studying Individual Units in ML Models.Early work in SL  already suggested that units at different model layers fulfill different functions: while units in lower layers are responsible for extracting general features, units in higher layers towards the model output are responsible for very specific features . In particular, it was found that units represent different concepts required for the primary task , where some units focus on single concepts whilst others are responsible for multiple concepts [38; 54]. While these differences have been identified between the units of models trained with SL, we perform a corresponding investigation in the SSL domain through the lens of localizing memorization.

## 3 Background and Setup

SSL and Notation.We consider an SSL training framework \(\). The encoder \(f:^{n}^{s}\) is pre-trained, or in short _trained_, on the unlabeled dataset \(\) to output representations of dimensionality \(s\). Throughout the training, as the encoder improves, its alignment loss \(_{}(f,x)=d(f(x^{}),f(x^{}))\) between the representations of two random augmentations \(x^{},x^{}\) of any training data point \(x\) decreases with respect to a distance metric \(d\) (_e.g.,_ Euclidean distance). This effect has also been observed in non-contrastive SSL frameworks . We denote by \(f^{l},l[1, L]\) the \(l^{th}\) layer of encoder \(f\). Data points from the test set \(}\) are denoted as \(\).

Memorized Data.Prior work in the SL domain usually generates outliers for measuring memorization by flipping the labels of training data points [23; 24; 35]. This turns these points into outliersthat experience a higher level of memorization and leave the strongest possible signal in the model. Yet, such an approach is not suitable in SSL where labels are unavailable. Therefore, we rely on the SSLMem metric proposed by  to identify the most (least) memorized data points for a given encoder. The findings based on the SSLMem metric indicate that the most memorized data points correspond to atypical and outlier samples.

SSLMem for Quantifying Memorization.SSLMem quantifies the memorization of individual data points by SSL encoders. It is, to the best of our knowledge, the only existing method for quantifying memorization in SSL without reliance on downstream labels. SSLMem for a training data point \(x\) is defined as

\[_{f}(x)=}_{f()}}_{x^{},x^{}(x)}d(f(x^{ }),f(x^{}));\,_{g}(x)=}_{g ( x)}}_{x^{},x^{ }(x)}d(g(x^{}),g(x^{}))\] \[_{f,g}(x)=_{g}(x)-_{f}(x)\] (1)

where \(f\) and \(g\) are two classes of SSL encoders whose training dataset \(\) differs in data point \(x\). \(x^{}\) and \(x^{}\) denote two augmentations randomly drawn from the augmentation set \(\) that is used during training and \(d\) is a distance metric, here \(_{2}\)-distance.

Experimental Setup.We localize memorization in encoders trained with different SSL frameworks on five common vision datasets, namely CIFAR10, CIFAR100, SVHN, STL10, and ImageNet. We leverage different model architectures from the ResNet family, including ResNet9, ResNet18, ResNet34, and ResNet50. We also analyze Vision Transformers (ViTs) using their Tiny and Base versions. Results are reported over three independent trials. To identify the most memorized training data points, we rely on the SSLMem metric and follow the setup from . More details on the experimental setup can be found in Appendix B.2 For the readers' convenience, we include a glossary with short explanations for all concepts and background relevant to this work in Appendix A.

## 4 Layer-Level Localization of Memorization

In order to localize memorization on a per-layer granularity, we propose a new LayerMem metric which relies on the SSLMem metric, as a building block. Since the SSLMem as defined in Equation (1) is not normalized, we introduce the following normalization to the range \(\)

\[^{}_{f,g}(x)=_{g}(x)-_{f}(x)}{ _{f}(x)+_{g}(x)}\] (2)

such that values close to \(0\) denote no memorization while \(1\) denotes the highest memorization. This makes the score more interpretable. While SSLMem returns a memorization score per data point for a given encoder, LayerMem returns a memorization score per encoder layer \(l\), measured on a (sub)set \(^{}=\{x_{1},...,x_{|^{}|}\}\) of training data \(\). Similar to SSLMem, LayerMem makes use of a second encoder \(g\) as a reference to detect memorization as

\[_{^{}}(l)=^{ }|}_{i=1}^{|^{}|}^{}_{f^{l},g^ {l}}(x_{i}).\] (3)

\(f^{l},g^{l}\) denote the output of encoders \(f\) and \(g\) after layer \(l\), respectively. Intuitively, our LayerMem metric measures the average per-layer memorization over training data points \(x_{i}^{}\). As our LayerMem build on SSLMem\({}^{}\), it also inherits the above normalization. Since Equation (3) operates on different layers' outputs which in turn depend on all previous layers, LayerMem risks to report accumulated memorization up to layer \(l\). Therefore, we also define \(\) LayerMem\({}_{^{}}(l)\) for all layers \(l>1\) as

\[_{^{}}(l)=_{ ^{}}(l)-_{^{}}(l-1).\] (4)

This reports the increase in memorization of layer \(l\) with respect to the previous layer \(l-1\).

### Experimental Results and Observations

We present our core results and provide additional ablations on our LayerMem in Appendix C.3.

Memorization Increases but not Monotonically.We report the LayerMem scores in Table 1 for the ResNet9-based SSL encoder trained with SimCLR on CIFAR10 (further per-layer breakdown and scores for ResNet18, ResNet34, and ResNet50 are presented in Table 15, Table 16, and Table 17 in Appendix C.3). We report LayerMem across the 100 randomly chosen training data points, their \(\)LayerMem (denoted as \(\)LM), followed by LayerMem for only the Top 50 memorized data points, their \(\)LayerMem (denoted as \(\)LM Top50), and LayerMem for only the Least 50 memorized data points. The results show that our LayerMem indeed increases with layer depth in SSL, similar to the trend observed for SL , _i.e.,_ deeper layers experience higher memorization than early layers. However, our \(\)LayerMem presents the memorization from a more accurate perspective, where we discard the accumulated memorization from previous layers, including the residual connections. \(\)LayerMem indicates that the memorization increases in all the layers but is not monotonic.

We also study the differences in localization of the memorization for most memorized (outliers and atypical examples) vs. least memorized data points (inliers), shown as columns LayerMem Top50 and LayerMem Least50 in Table 1, respectively. While we observe that the absolute memorization for the most memorized data points is significantly higher than for the least memorized data points, they both follow the same trend of increasing memorization in deeper layers. The \(\)LayerMem for the most memorized points (denoted as \(\)LM Top50 in Table 1) indicates that, following the overall trend, high memorization of the atypical samples is also spread over the entire encoder and not confined to particular layers.

Memorization in Vision Transformers.The memorization of Transformers  was, so far, primarily investigated in the language domain , however, its understanding in the vision domain is lacking. The fully-connected layers in _language transformers_ were shown to act as key-value _memories_. Still, findings from language transformers cannot be easily transferred to _vision transformers_ (ViTs) : while language transformers operate on the level of _discrete_ and interpretable input and output tokens, ViTs operate on _continuous_ input image patches and output representations. Through the analysis of our newly proposed metric for memorization in SSL, in Table 2 (ViT-Tiny trained on CIFAR10 using MAE ), we are the first to show that memorization in ViTs occurs more in deeper blocks and that within the blocks, _fully-connected layers memorize more than attention layers_. We present the full set of results for LayerMem and \(\)LayerMem over _all blocks_ in Table 10.

Memorization in Different SSL Frameworks.We also study the differences in memorization behavior between different SSL frameworks. Therefore, we compare the LayerMem score between corresponding layers of a ResNet50 trained on ImageNet with SimCLR  and DINO , and of a ViT-Base encoder trained on ImageNet with DINO and MAE . We ensure by early stopping that the resulting linear probing accuracies of the encoder pairs are similar for better comparability of their memorization. The ImageNet downstream task performance within both encoder pairs is 66.12% for SimCLR and 68.44% for DINO; and 60.43% for MAE and 60.17% for DINO. Our results in Table 4 show that encoders with the same architecture trained with different SSL frameworks experience a similar memorization pattern, namely that memorization occurs primarily in the deeper blocks/layers. In Figure 12 in Appendix C.1, we additionally show that memorization patterns between different

   Layer & LayerMem & \(\)LM & LayerMem Top50 & \(\)LM Top50 & LayerMem Loss50 \\ 
1 & 0.091 & - & 0.144 & - & 0.003 \\
2 & 0.123 & 0.032 & 0.225 & 0.081 & 0.012 \\
3 & 0.154 & 0.031 & 0.308 & 0.083 & 0.022 \\
4 & 0.183 & 0.029 & 0.402 & 0.094 & 0.031 \\ Re62 & 0.185 & 0.002 & 0.403 & 0.001 & 0.041 \\
5 & 0.212 & 0.027 & 0.479 & 0.076 & 0.051 \\
6 & 0.246 & 0.034 & 0.599 & 0.120 & 0.061 \\
7 & 0.276 & 0.030 & 0.697 & 0.098 & 0.071 \\
8 & 0.308 & 0.032 & 0.817 & 0.120 & 0.073 \\ Re6 & 0.311 & 0.003 & 0.817 & 0 & 0.086 \\   

Table 1: **Layer-based Memorization Scores.** Res\(N\) denotes a residual connection that comes from the previous \(N\)-th convolutional layer.

   ViT Block &  & \(\)LayerMem \\  \\ 
2 & 0.028 & 0.008 \\
6 & 0.114 & 0.009 \\
2 & 0.281 & 0.010 \\
2 & **Fully-Connected Layer** \\ 
2 & 0.039 & 0.011 \\
6 & 0.129 & 0.015 \\
12 & 0.303 & 0.022 \\   

Table 2: **Memorization in ViT occurs primarily in the deeper blocks and more in the fully connected than attention layers.**

[MISSING_PAGE_FAIL:6]

whether it is possible to trace down SSL memorization to a unit-level. To answer this question, we design UnitMem, a new metric to localize memorization in individual units of SSL encoders. We use the term _unit_ to refer to both an activation map from a convolutional layer (single-layer output channel) or an individual neuron within a fully connected layer. Our UnitMem metric quantifies for every unit \(u\) in the SSL encoder how much \(u\) is sensitive to, _i.e.,_ memorizes, any particular training data point. Therefore, UnitMem relates the _maximum unit activation_ that occurs for a data point \(x_{k}\) in the training data (sub)set \(^{}\) with the _mean unit activation_ on all other data points in \(^{}\{x_{k}\}\).

The design of UnitMem is inspired by the class selectivity metric defined for SL by . Class selectivity was derived from selectivity indices commonly used in neuroscience  and quantifies a unit's discriminability between different classes. It was used as an indicator of good generalization in SL. We provide more background in Appendix D.1. To leverage ideas from class selectivity for identifying memorization, we integrate three fundamental changes in our metric in comparison to the class selectivity metric. While class selectivity is calculated on _classes_ of the test set and relies on class _labels_, our UnitMem is (1) _label-agnostic_ and (2) computed on individual data points from the _training dataset_ to determine their memorization. Additionally, to account for SSL's strong reliance on augmentations, (3) we calculate UnitMem over the expectation on the augmentation set used during training. Research from the privacy community  suggests that those augmentations leave a stronger signal in ML models than the original data point, _i.e.,_ relying on the unaugmented point alone might under-report memorization. We verify this effect in Figure 5 in Appendix C.1. We note that through these fundamental changes UnitMem is able to measure memorization of _individual data points_ within a class rather than to solely distinguish between classes or concepts like the original class selectivity. We provide further insights into this difference and perform experimental verification which highlights that UnitMem captures individual data points' memorization rather than capturing classes or concepts in Appendix C.2.

To formalize our UnitMem, we first define the mean activation \(\) of unit \(u\) on a training point \(x\) as

\[_{u}(x)=*{}_{x^{}(x)}_{u}(x^{}),\] (5)

where the activation for convolutions feature maps is averaged across all elements of the feature map and for fully connected layers is an output from a single neuron (which is averaged across all patches of \(x\) in ViTs). Further, for the unit \(u\), we compute the maximum mean activation \(_{max,u}\) across all instances from \(^{}\), where \(N=|^{}|\), as

\[_{max,u}=(\{_{u}(x_{i})\}_{i=1}^{N}).\] (6)

Let \(k\) be the index of the maximum mean activation \(_{u}(x_{k})\), _i.e.,_ the \(argmax\). Then, we calculate the corresponding mean activity \(_{-max}\) across all the remaining \(N-1\) instances from \(^{}\) as

\[_{-max,u}=(\{_{u}(x_{i})\}_{i=1,i k}^{N}).\] (7)

Finally, we define the UnitMem of unit \(u\) as

\[_{^{}}(u)=-_{-max,u}}{ _{max,u}+_{-max,u}}.\] (8)

  
**Fine-tuned Layers** & **Accuracy (\%)**\(\) \\  None (HEAD) & 48.6\% \(\) 1.12\% \\ 
6 (highest \(\)LayerMem) +HEAD & **53.0\% \(\) 0.86\%** \\
8 (last layer, highest LayerMem) +HEAD & 52.7\% \(\) 0.97\% \\ 
6.8 + HEAD & **56.7\% \(\) 0.84\%** \\
7.8 + HEAD & 55.3\% \(\) 0.77\% \\ 
4,6,8 (highest \(\)LayerMem) +HEAD & **57.9\% \(\) 0.79\%** \\
6,7,8 + HEAD & 56.5\% \(\) 0.95\% \\   

Table 5: **Fine-tuning most memorizing layers. We train a ResNet9 encoder with SimCLR on CIFAR10 and fine-tune different (combinations of) layers on the STL10 dataset, resized to 32x32x3. We train a linear layer trained on top of the encoder (HEAD) and report STL10 test accuracy after fine-tuning. Fine-tuning the most memorizing layer(s), in contrast to the last layer(s), yields higher fine-tuning results.**The value of the UnitMem metric is bounded between 0 and 1, where 0 indicates that the unit is equally activated by all training data points, while value 1 denotes exclusive memorization, where only a single data point triggers the activation, while all other points leave the unit inactive.

### Experimental Results and Observations

We present our core results and provide detailed additional ablations on our UnitMem Appendix C.1.

Highly Memorizing Units Occur over Entire Encoder.Our analysis highlights that over all encoder architectures and SSL training frameworks, highly memorizing units are spread over the entire encoder. While, on average, earlier layers exhibit lower UnitMem than deeper layers, even the first layer contains highly memorizing units as shown in Figure 3 (first row). Figure 0(a) shows that this trend holds over different datasets. Yet, the SVHN dataset, which is visually less complex than the CIFAR10 or STL10 dataset, has the lowest number of highly memorizing units. This observation motivates us to study the relationship between the highest memorized (atypical or hard to learn) data points and the highest memorizing units.

Most Memorized Samples and Units Align.To draw a connection between data points and unit memorization, we analyze which data points are responsible for the highest \(_{max}\) scores. This corresponds to a data point which causes the highest activations of a unit, while other points activate the unit only to a small degree or not at all. We show the results in Figure 0(b) (also in Table 12 as well as in Figure 7 in Appendix C.1). For each unit \(u\) in the last convolutional layer of the ResNet9 trained on CIFAR10, we measure its UnitMem score, then we identify which data point is responsible for the unit's \(_{max,u}\), and finally measure this point's SSLMem score. We plot the UnitMem and SSLMem scores for each unit and its corresponding point. Our results highlight that the data points that experience the highest memorization according to the SSLMem score are also the ones memorized in the most memorizing units. Given the strong memorization in individual units, we next look into two methods to reduce it and analyze their impact.

Differential Privacy reduces Unit Memorization.The gold standard to guarantee privacy in ML is Differential Privacy (DP) . DP formalizes that any training data point should only have a negligible influence on the final trained ML model. To implement this, individual data points' gradients during training are clipped to a pre-defined norm, and controlled amounts of noise are added . This limits the influence that each training data point can have on the final model. Building on the DP framework for SSL encoders , we train a ViT-Tiny using MAE on CIFAR10 with three different privacy levels--in DP usually indicated with \(\). We train non-private (\(=\)), little private (\(=20\)), and highly private (\(=8\)) encoders and apply our UnitMem to detect and localize memorization. Our results in Figure 0(c) highlight that while with increasing privacy levels, the average UnitMem decreases, there are still individual units that experience high memorization.

Data Point vs Class Memorization.Since stronger training augmentations yield higher class clustering  (_i.e.,_ the fact that data points from the same downstream class are close to each other in representation space but distant to data points from other classes), we also analyze how the SSL encoders differ from the standard class discriminators, namely SL trained models. Therefore, we

Figure 1: **Insights into UnitMem. We train a ResNet9 encoder with SimCLR: (a) Different datasets, including SVHN, CIFAR10, and STL10. We report the UnitMem of the last convolutional layer (conv4_2); (b) Comparing alignment between SSLMem and UnitMem on CIFAR10. Data points with higher general memorization (SSLMem) tend to experience higher UnitMem; (c) Using different strengths of privacy protection according to DP during training on CIFAR10 and Vit-Base**

go beyond our previous experiments that measure memorization of units with respect to individual data points and additionally study unit memorization at a class-granularity. Therefore, we adjust the class selectivity metric from  to perform on the training dataset rather than on the test data set as the original class selectivity. To avoid confusion between the two versions, refer to our adapted metric as ClassMem (see Appendix D.2 for an explicit definition). Equipped with UnitMem and ClassMem, we study the behavior of SSL encoders and compare between SSL and SL. For our comparison, we train an encoder with SimCLR and a model with SL using the standard cross entropy loss, both on the CIFAR100 dataset using ResNet9. We remove the classification layer from the SL trained model to obtain the same architecture as for the encoder trained with SimCLR.

For comparability, we early stop the SL training once it reaches a comparable performance to the linear probing accuracy on CIFAR100 obtained by the SSL encoder. Our results in Figure 2 show that overall, in SSL throughout all layers, average memorization of individual data points is higher than class memorization, whereas in SL, in deeper layers, the class memorization increases significantly. We hypothesize that this effect is due to earlier layers in SL learning more general features which are independent of the class whereas later layers learn features that are highly class dependent. For SSL, such a difference over the network does not seem to exist; both scores increase slightly, however, probably due to the SSL learning paradigm, memorization of individual data points remains higher.

To better understand the memorization of units on the micro level, we investigate further the individual units in each layer. In Figure 3, we plot the ClassMem vs UnitMem for each unit and in each of the eight encoder layers of ResNet9. Most units for SSL (row 1) constantly exhibit higher UnitMem than ClassMem, _i.e.,_ they cluster under the diagonal line, which suggests that most units memorize individual data points across the whole network. Contrary, the initial layers for the model trained with SL have a slight tendency to memorize data points over entire classes whereas in later layers, this trend drastically reverses and most units memorize classes. In Appendix C.4, we investigate how this different memorization behavior between SL and SSL affects downstream generalization.

### Verification of Unit-Based Memorization

We verify the unit-based localization of memorization with our UnitMem by deliberately inserting memorization of particular units and checking if our UnitMem correctly detects it. Therefore, we first train a SimSiam-based  ResNet18 encoder trained on the CIFAR10 dataset. We select SimSiam over SimCLR for this experiment since SimCLR, as a contrastive SSL framework, cannot train on a single data point. Then, using LayerMem, we identity the last convolutional layer in ResNet18 (_i.e.,_ layer _4.1.conv2_) as the layer with the highest LayerMem and \(\)LayerMem memorization. We select the unit from the layer with the highest \(_{max}\) and also pick a unit with no activation (\(_{max}=0\)) for some test data points. Then, we fine-tune these units using a single test data point and report the change in UnitMem for the chosen units in Table 13. The results show that our UnitMem correctly detects the increase in memorization in both units. Additionally, we analyze the impact of zero-ing out the most or least memorizing vs. random units. Again with the argument that memorization is required for downstream generalization in SSL , we expect the highest performance drop when

Figure 3: **Significantly more (_less_) units memorize data points rather than classes in SSL (_SL_).** We measure the ClassMem vs UnitMem for 10000 samples from CIFAR100, with 100 random samples per class. Each i-th column represents the i-th convolutional layer in ResNet9, with 8 convolution layers, where the 1st row is for SSL while the 2nd row for SL. The red diagonal line denotes \(y=x\).

Figure 2: UnitMem **and ClassMem for SL and SSL.**

zero-ing out the most memorizing units. Our results in Table 6 in and in Appendix C.1 confirm this hypothesis and show that removing the most memorizing units yields the highest loss in linear probing accuracy on various downstream tasks while pruning the least memorized units preserves better downstream performance than removing random units. These results suggest that future work may benefit from using our UnitMem metric for finding which units within a network can be pruned while preserving high performance.

## 6 Conclusions

We propose the first practical metrics for localizing memorization within SSL encoders on a per-layer and per-unit level. By analyzing different SSL architectures, frameworks, and datasets using our metrics, we find that while memorization in SSL increases in deeper layers, a significant fraction of highly memorizing units can be encountered over the entire encoder. Our results also show that SSL encoders significantly differ from SL trained models in their memorization patterns, with the former constantly memorizing data points and the latter increasingly memorizing classes. Finally, using our metrics for localizing memorization presents itself as an interesting direction towards more efficient encoder fine-tuning and pruning.

#### Acknowledgments

The project on which this paper is based was funded by the Deutsche Forschungsgemeinschaft (DFG, German Research Foundation), Project number 550224287. Additional funding came from the Initiative and Networking Fund of the Helmholtz Association in the framework of the Helmholtz AI project call under the name,,PAFMIM", funding number ZT-I-PF-5-227. Responsibility for the content of this publication lies with the authors.