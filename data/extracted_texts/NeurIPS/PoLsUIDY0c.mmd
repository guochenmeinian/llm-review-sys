# Fast Imagic: Solving Overfitting in Text-guided Image Editing via Disentangled UNet with Forgetting Mechanism and Unified Vision-Language Optimization

Fast Imagic: Solving Overfitting in Text-guided Image Editing via Disentangled UNet with Forgetting Mechanism and Unified Vision-Language Optimization

 Shiwen Zhang

Bytedance Inc

shiwen.zhang@bytedance.com

Or contact me via my personal email witcherofresearch@gmail.com. Codes are available at https://github.com/witcherofresearch/Forgedit.

###### Abstract

Text-guided image editing on real or synthetic images, given only the original image itself and the target text prompt as inputs, is a very general and challenging task. It requires an editing model to estimate by itself which part of the image should be edited, and then perform either rigid or non-rigid editing while preserving the characteristics of original image. Imagic, the previous SOTA solution to text-guided image editing, suffers from slow optimization speed, and is prone to overfitting since there is only one image given. In this paper, we design a novel text-guided image editing method, Fast Imagic. First, we propose a vision-language joint optimization framework for fast aligning text embedding and UNet with the given image, which is capable of understanding and reconstructing the original image in 30 seconds, much faster and much less overfitting than previous SOTA Imagic. Then we propose a novel vector projection mechanism in text embedding space of Diffusion Models, capable of decomposing the identity similarity and editing strength thus controlling them separately. Finally, we discovered a general disentanglement property of UNet in Diffusion Models, i.e., UNet encoder learns space and structure, UNet decoder learns appearance and texture. With such a property, we design the forgetting mechanism by merging original checkpoint and optimized checkpoint to successfully tackle the fatal and inevitable overfitting issues when fine-tuning Diffusion Models on one image, thus significantly boosting the editing capability of Diffusion Models. Our method, Fast Imagic, even built on the outdated Stable Diffusion, achieves new state-of-the-art results on the challenging text-guided image editing benchmark: TEdBench, surpassing the previous SOTA methods such as Imagic with Imagen, in terms of both CLIP score and LPIPS score. Codes are available at https://github.com/witcherofresearch/Forgedit.

## 1 Introduction

Text-guided Image Editing (20) is a fundamental problem in computer vision, with a target text prompt indicating the editing intention to the given image. The approaches of text-guided image editing are generally categorized into optimization-based methods and non-optimization ones according to whether fine-tuning process is performed for reconstruction. Recent non-optimization editing methods [(3; 31; 5; 33; 18; 1; 4; 32)] are very efficient. Yet they either struggle on preserving the precise characteristics of original image during complex editing, or suffer from being incapable of performing sophisticated and accurate non-rigid edits. It is undeniable that fine-tuning a diffusion model with the original image is still critical and necessary for high-precision identity preservation and accurate semantic understanding. However, previous optimization-based methods [14; 27] suffer from long fine-tuning time, severe overfitting issues or incapablility of performing precise non-rigid editing. Here overfitting refers to the phenomenon that the model could reconstruct the original image, yet incapable of conducting the edit according to the target prompt, which we will demonstrate later.

In this paper, we are going to tackle the aforementioned issues of the SOTA optimization-based editing method, Imagic . We name our text-guided image editing method Fast Imagic, which consists of two stages: fine-tuning and editing.

For fine-tuning stage, with a generated source prompt from BLIP  to describe the original image, we design a vision and language joint optimization framework, which could be regarded as a variant of Imagic by combining the first stage and the second stage of Imagic into one and using BLIP generated caption as source prompt instead of using target prompt as source prompt like what Imagic does. Such simple modifications are the keys to much faster convergence speed and less overfitting than Imagic. With our joint learning of image and source text embedding, the finetuning stage using one image with our Fast Imagic+Stable Diffusion 1.4  takes 30 seconds on an A100 GPU, compared with 7 minutes with Imagic +Stable Diffusion  reported by Imagic paper. This leads to 14x speed up. BLIP generated source prompt also eases overfitting, which we will demonstrate in the ablation study. In addition, our joint vision-language optimization eliminates the strange random flip phenomenon reported in Imagic, i.e., the direction of objects in the editing results randomly flip,

Figure 1: Fast Imagic could be used for consistent and controllable keyframe generation for visual storytelling and movie generation, given one input image and target prompts. We list several samples with different random seeds for each target prompt. We demonstrate Fast Imagic is capable of controling multiple characters performing various actions at different scenes.Fast Imagic could also control each different character separately. Forgetting strategy on UNet’s encoder with vector subtraction leads to high flexibility and success rate to change the spatial structures and actions, preserving appearance and identity by reserving UNet’s decoder.

leading to editing failures. Our Fast Imagic always lock the correct direction of the objects the same as the original image, and only flips the directions of objects when the target prompt instructs so.

For editing stage, we propose two novel methods, vector projection in text embedding space and forgetting strategy with a finding of a general UNet disentangled property. For the first time in the literature of text-guided image editing with Diffusion Models, we propose a novel vector projection mechanism in text embedding space of Diffusion Models, which is capable to separately control the identity and editing strength by decomposing the language representations into identity embedding and editing embedding. We explore the properties of vector projection and compare it with previous vector subtraction method utilized in Imagic to demonstrate its superiority on identity preservation. Finally, we discovered a general property of UNet structure in Diffusion Models, i.e., UNet encoder learns space and structure, UNet decoder learns appearance and identity. With such a property, we could easily tackle the fatal overfitting issues of optimization-based image editing methods in a very effective and efficient manner during sampling process instead of fine-tuning process, by designing a forgetting mechanism with model merging according to UNet disentanglement. Without intention to reveal authors' information, Fast Imagic has been completely open-sourced for more than one year (of course, the open-sourced project is not called 'Fast Imagic', thankfully not violating the double-blind rule). We were the first to discover and open-source the encoder-decoder disentanglement phenomenon in Diffusion UNet models more than one year ago, though such a property is re-discovered in some recent papers.

To sum up, our main contributions are:

1. We present Fast Imagic, an efficient vision-language joint alignment framework, capable of performing both rigid and non-rigid text-guided image editing, while speeds up previous SOTA Imagic 14 times, completely solves the overfitting issue of Imagic.

2. We introduce a novel vector projection mechanism in text embedding space of Diffusion Models, which decomposes the target prompt representations into identity embedding and editing embedding. This improves Fast Imagic's capability for preserving more consistent characteristics of original image than existing methods.

3. We design a novel forgetting strategy via model merging based on our discovery on the disentangled UNet architecture of diffusion models, i.e., UNet encoder learns space and structure, UNet decoder learns appearance and texture. This allows us to effectively tackle the critical overfitting issue of optimization-based image editing methods, thus significantly boosting the editing capability of diffusion models.

Our Fast Imagic achieves new state-of-the-art results on the challenging benchmark TEdBench [(14)] (even by using an outdated Stable Diffusion 1.4), surpassing previous SOTA Imagic built on Imagen in terms of both CLIP score [(8)] and LPIPS score [(34)].

## 2 Related Works

**Test-time fine-tuning image editing** Diffusion Models have dominated text to image generation. DDPM[(11)] improves Diffusion process proposed by [(29)] on generating images. DDIM [(30)] accelerates the sampling procedure of Diffusion Models by making reverse process deterministic and using sub-sequence of time-steps. Dalle 2 [(25)] trains a diffusion prior to convert a text caption to CLIP [(23)] image embedding and then employs a Diffusion Decoder to transfer the generated CLIP image embedding to an image. Imagen [(28)] is a Cascaded Diffusion Model [(12)], whose UNet is composed of three Diffusion Models generating images with increasing resolutions, employing the powerful T5 text encoder [(24)] for complex semantic understanding and generating sophisticated scenarios. Stable Diffusion [(26)] utilizes Variational AutoEncoders [(16)] to compress the training image to a compact latent space so that the UNets could be trained with low resolution latents in order to save computational resources.These models are pretrained on billions of data. For image editing task with one given image, DreamBooth [(27)], textual inversion [(6)], Lora [(13)], Imagic[(14)] etc., could be trained with one image and conduct the edit with text to image generation.

**Test-time fine-tuning free image editing** There are some test-time finetuning-free methods, which do not require to optimize the diffusion model for each reference image. However, methods like SDEdit [(18)],DDIM inversion [(30)], MasaCtrl [(4)], Elite[(32)] all struggle to preserve the characteristics during complex editing and some of them could also change the view, pose and background irrelevant to target prompt, which leads to editing failures. Other typical methods like PnP Diffusion [(31)], Instruct Pix2pix [(3)], Prompt to Prompt [(7)] are incapable to conduct non-rigid editing and space-related editing.

Drag Diffusion (19), which extends DragGAN (21), is only capable of performing space-related editing, which is just a portion of general image editing tasks. Instead, our Fast Imagic is a general text-guided image editing framework to conduct various kinds of image editing operations, including spatial transformations.

## 3 Fast Imagic

### Preliminaries

Diffusion models (11; 29) consist of a forward process and a reverse process. The forward process starts from the given image \(x_{0}\), and then progressively add Gaussian Noise \(_{t}(0,1)\) in each timestep \(t\) to get \(x_{t}\). In such a diffusion process, \(x_{t}\) can be directly calculated at each timestep \(t\{0,...,T\}\),

\[x_{t}=}x_{0}+}_{t}\] (1)

with \(_{t}\) being diffusion schedule parameters with \(0=_{T}<_{T-1}...<_{1}<_{0}=1\).

In the reverse process, given \(x_{t}\) and text embedding \(e\), the time-conditional UNets \(_{}(x_{t},t,e)\) of diffusion models predict random noise \(_{t}\) added to \(x_{t-1}\). With DDIM (30), the reverse process can be computed as,

\[x_{t-1}=}}{}}(x_{t}-}_{}(x_{t},t,e))+}_{}(x_{t}, t,e)\] (2)

With Latent Diffusion Models (26), the original image \(x_{0}\) is replaced by a latent representation \(z_{0}\) obtained from a VAE (16) Encoder \((x_{0})\). The overall training loss is computed as,

\[L=_{z_{t},_{t},t,e}||_{t}-_{}(z_{t}, t,e)||_{2}^{2}\] (3)

### Joint vision-language optimization for alignement

In order to tackle such challenging text-guided image editing problems, we propose a image and text alignment framework via joint optimization of text embedding and UNet with the given image. Shown in Figure 2, we introduce the overall design of our vision-language joint optimization framework.

**Source prompt generation.** We first use BLIP (17) to generate a caption describing the original image, which is referred to as the source prompt. The source prompt is then fed to the text encoder of Stable Diffusion (26), generating an embedding \(e_{src}\) of source prompt. Previous three-stage editing method Imagic (14) regards target prompt text embedding as source one \(e_{src}\). We found that it is essential to use the BLIP caption instead of using the target prompt as a pseudo source prompt like Imagic. Otherwise such fine-tuning methods easily lead to overfitting issues, as demonstrated in the 5th column 'Imagic SD' of Figure 6. This phenomenon indicates that using the BLIP caption as source prompt would result in better semantic alignment with the given original image than Imagic.

**Vision-language alignment with joint optimization.** We choose to optimize UNet encoder blocks of 0, 1, 2 and decoder blocks of 1, 2, 3 in the UNet structure since we found that fine-tuning deepest features would lead to overfitting in our Fast Imagic framework, demonstrated in Figure 2. Similar with Imagic, we regard source text embedding as parameters to optimize. Yet different with Imagic which optimizes text embedding and UNet in two separate stages, we found it vital to align the source text embedding and UNet parameters simultaneously, which is of great importance for faster convergence and better reconstruction quality than Imagic. In particular, due to a large domain gap between text and image, we use different learning rates for source text embedding (\(10^{-3}\)) and UNet (\(6 10^{-5}\)) with Adam Optimizer (15). For faster training, since we only have a single training image, we repeat the tensors on batch dimension for batch-wise optimization with a batch size of 10. We use mean square error loss, and empirically found that stable reconstruction results can be achieved when the final loss is less than 0.03. With the batch size set to 10, the models are fine-tuned for 35 to 40 steps. We stop the training over 35 steps when the loss is less than 0.03, or stop at 40 steps at most. This fine-tuning process is significantly more efficient than Imagic, taking 30 seconds on a single A100 GPU. The training loss is computed as,

\[L=_{z_{t},_{t},t,e_{src}}||_{t}-_{, e_{src}}(z_{t},t,e_{src})||_{2}^{2}\] (4)

where the main difference with the training loss presented in 3 is that \(e_{src}\) is considered as parameters to optimize.

### Reasoning and Editing with language representation decomposition

We first input the target prompt to the CLIP (23) text encoder of the Stable Diffusion model (26), computing a target text embedding \(e_{tgt}\). With our learned source text embedding \(e_{src}\), we introduce two methods to combine \(e_{src}\) and \(e_{tgt}\) so that the merged text embedding can instruct the UNet to preserve characteristics of original image and also follow the target prompt. Given \(e_{src}^{B N C}\) and \(e_{tgt}^{B N C}\), we conduct all vector operations on the \(C\) dimension to get the final text embedding \(e\).

**Vector Subtraction.** We use the same interpolation method as Imagic (14),

\[e= e_{tgt}+(1-)e_{src}=e_{src}+(e_{tgt}-e_{src})\] (5)

As shown in Figure 3, the final text embedding \(e\) is obtained by travelling along vector subtraction \(e_{tgt}-e_{src}\). In our experiments, we found that in most cases, \(\) goes beyond 1 when the editing is performed successfully. This leads to a problem that the distance between the final embedding \(e\) and the source embedding \(e_{src}\) may be so far that the appearance of the edited object could change vastly.

**Vector Projection.** We propose to use vector projection to better preserve the appearance of the original image. As shown in the Figure 3, we decompose a target prompt text embedding \(e_{tgt}\) into a vector along \(e_{src}\) and a vector orthogonal to \(e_{src}\). We call the orthogonal vector \(e_{edit}\). We first calculate the ratio \(r\) of the projected vector on \(e_{src}\) direction.

Figure 2: Overall framework of our Fast Imagic, consisting of a vision-language joint fine-tuning stage and an editing stage. We use BLIP to generate a text description of an original image, and compute an embedding of the source text \(e_{src}\) using a CLIP text encoder. The source embedding \(e_{src}\) is then jointly optimized with UNet using different learning rates for text embedding and UNet, where the deep blocks of UNet are frozen. During the editing process, we merge the source embedding \(e_{src}\) and the target embedding \(e_{tgt}\) with vector subtraction or projection to get a final text embedding \(e\). With our forgetting strategies applied to UNet, we utilize DDIM sampling to get the final edited image.

\[r=e_{tgt}}{||e_{src}||^{2}}\] (6)

Thus, we could get the \(e_{edit}\) by computing

\[e_{edit}=e_{tgt}-re_{src}\] (7)

To control the characteristics similarity and editing strength separately, we sum \(e_{src}\) and \(e_{edit}\) with two coefficient \(\) and \(\),

\[e= e_{src}+ e_{edit}\] (8)

**Editing.** We use DDIM sampling (30) with a classifier free guidance (10) to conduct the edit. The guidance scale is 7.5. For vector subtraction, we iterate over a range of \([0.8,1.6]\). For vector projection, we choose \(\) from two values \(\{0.8,1.1\}\), and \(\) from a range of [1.0,1.5].

Figure 4: The encoder of UNets learn features related to pose, angle, structure and position. The decoder are related to appearance and texture. Thus we design a forgetting strategy according to the editing target.

Figure 3: We demonstrate vector subtraction and vector projection to merge \(e_{src}\) and \(e_{tgt}\). Vector subtraction could lead to inconsistent appearance of the object being edited since it cannot directly control the importance of \(e_{src}\). The vector projection decomposes the \(e_{tgt}\) into \(re_{src}\) along \(e_{src}\) and \(e_{edit}\) orthogonal to \(e_{src}\). We can directly control the scales of \(e_{src}\) and \(e_{edit}\) by summation.

### The Ultimate Solution to overfitting with disentangled UNet and forgetting mechanism

**Forgetting mechanism** Considering the fact that there is only one training image provided, in some cases the diffusion model still overfits, thus losing the editing capability, though joint vision-language alignment could ease the overfitting to some extent. The fine-tuning process is computational expensive compared to sampling process, thus we design a novel forgetting mechanism during sampling process to tackle the overfitting problem. The network is only fine-tuned once, and can be converted to multiple different networks during sampling process by merging certain fine-tuned parameters \(w_{learned}\) and the corresponding parameters of original UNet (before fine-tuning) \(w_{orig}\), with a balance coefficient \(\). In practice, we found that \(=0\) works in general, which means that we can simply replace the learned parameters with original parameters so that the network completely forgets these learned parameters. However, which paramters should be forgotten?

\[w= w_{learned}+(1-)w_{orig}\] (9)

**Disentangled UNet** Shown in Figure 4, we found an general disentanglement property of UNet in diffusion models. The encoder of UNets learns space and structure information like the pose, action, position, angle and overall layout of the image, while the decoder learns appearance and textures instead. We were the first to draw a clear and universal conclusion on disentangled UNet and open-sourced our solution completely more than one year ago. In Figure 4, given the target prompt and original image in the first row, we conduct Fast Imagic with forgetting UNet encoder, UNet decoder and nothing in the second, third and fourth rows respectively. We could see that without forgetting mechanism, four out of five cases are overfitting. By forgetting UNet encoder, the structure and space features are changed yet the appearance and texture are preserved. Vice versa for forgetting UNet decoder.

**Tackling overfitting with disentangled UNet and forgetting mechanism** If the target prompt tends to edit space and structure information, for example, the pose or layout, we will choose to forget parameters of the encoder. If the target prompt aims to edit the appearance, the parameters of decoder should be forgotten. Currently we only apply the forgetting strategy when a text embedding \(e\) is obtained by vector subtraction in previous section. We will conduct a thorough exploration of forgetting mechanism with disentangled UNet in appendix due to page limit.

### WorkFlow

The overall workflow of Fast Imagic is explained in Figure 5. The fine-tuning stage is the same for all images. The diamonds in the figure indicate that the process depends on the users's choices and preferences. In practice, these user decisions can also be replaced by thresholds on metrics like CLIP

Figure 5: Fast Imagic Workflow.

[MISSING_PAGE_FAIL:8]

score, we follow the advice of the authors by setting dimension to 192 since there are only 100 samples in TEdBench.

## 5 Conclusion

We present our Fast Imagic framework to tackle the challenging text-guided image editing problem. Fast Imagic speeds up previous SOTA Imagic by 14 times, and completely solves the overfitting problem of Diffusion Models when fine-tuning with only one image, via vision-language joint alignment and disentangled UNet with forgetting mechanism, and obtain new SOTA on TEdBench.