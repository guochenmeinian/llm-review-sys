# Beyond Primal-Dual Methods in Bandits with Stochastic and Adversarial Constraints

Martino Bernasconi\({}^{}\) Matteo Castiglioni\({}^{}\) Andrea Celli\({}^{}\) Federico Fusco\({}^{*}\)

\({}^{}\) Bocconi university

\({}^{}\) Politecnico di Milano

\({}^{*}\) Sapienza University of Rome

{martino.bernasconi,andrea.celli2}@unibocconi.it, matteo.castiglioni@polimi.it, federico.fusco@uniroma1.it

###### Abstract

We address a generalization of the bandit with knapsacks problem, where a learner aims to maximize rewards while satisfying an arbitrary set of long-term constraints. Our goal is to design best-of-both-worlds algorithms that perform optimally under both stochastic and adversarial constraints. Previous works address this problem via primal-dual methods, and require some stringent assumptions, namely the Slater's condition, and in adversarial settings, they either assume knowledge of a lower bound on the Slater's parameter, or impose strong requirements on the primal and dual regret minimizers such as requiring weak adaptivity. We propose an alternative and more natural approach based on optimistic estimations of the constraints. Surprisingly, we show that estimating the constraints with an UCB-like approach guarantees optimal performances. Our algorithm consists of two main components: (i) a regret minimizer working on _moving strategy sets_ and (ii) an estimate of the feasible set as an optimistic weighted empirical mean of previous samples. The key challenge in this approach is designing adaptive weights that meet the different requirements for stochastic and adversarial constraints. Our algorithm is significantly simpler than previous approaches, and has a cleaner analysis. Moreover, ours is the first best-of-both-worlds algorithm providing bounds logarithmic in the number of constraints. Additionally, in stochastic settings, it provides \(()\) regret _without_ Slater's condition.

## 1 Introduction

We address the problem faced by a decision maker who aims to maximize its cumulative reward over a time horizon \(T\), while satisfying an arbitrary set of \(m\) long-term constraints. At each round \(t\), the learner selects an action \(a_{t}\) from a finite set of \(K\) actions, and then observes a reward \(f_{t}(a_{t})\) and some costs \(g_{t}(a_{t})[-1,1]^{m}\). The goal is to design best-of-both-worlds algorithms for this problem that perform optimally under both stochastic and adversarial constraints. We always assume rewards are generated adversarially. This is because the real complexity of the problem is captured by the nature of the constraints, so that transitioning from adversarial to stochastic rewards under the same type of constraints does not affect our results.

The first works on bandits with constraints focus on budget constraints, a.k.a bandit with knapsack (BwK)  study the settings in which both rewards and constraints are i.i.d. and propose an UCB-based approach, combined with primal-dual method. Agrawal and Devanur  provide an UCB-like approach for more general rewards and costs. Immorlica et al. , Kesselheim and Singla analyse settings with adversarial constraints and rewards, providing a primal-dual algorithm to tackle the problem. Castiglioni et al.  show that a similar primal-dual approach provides best-of-both-worlds guarantees. Many subsequent works extend the setting to more general constraints, mostly employing primal-dual methods [16; 17; 28; 11; 13; 10; 18]. Primal-dual methods have been the only effective method that provides best-of-both-worlds guarantees for bandits with constraints [16; 17; 11; 13; 10]. However, such methods require assumptions that are particularly stringent in settings beyond knapsack constraints. First, they require the existence of a strictly feasible solution (_i.e.,_ Slater's condition) to avoid a regret of order \(O(T^{}{{4}}})\)[16; 28]. While this assumption always holds in bandits with knapsack setting (where "doing nothing" incurs in a negative cost equal to the per-round budget), this assumption is far more stringent with general constraints. Moreover, some works require the knowledge of a lower bound on the Slater's parameter [16; 28]. Subsequent works circumvent this assumption at the expense of strong requirements on the primal and dual regret minimizers [17; 11; 13; 1]. In particular, such approaches require weakly-adaptive primal and dual regret minimizers. The challenge of applying such primal-dual algorithms to bandit beyond knapsack constraint is reflected in regret bounds that exhibit non-optimal dependencies on some parameters. For instance, a polynomial (instead of logarithmic) dependence on the number of constraints [17; 11; 13]. For further pointers to the literature, we refer to Appendix A.

### Our contribution

We propose an alternative and insightful approach to design best-of-both-worlds algorithms for bandit with long-term constraints. Our method relies on optimistic estimations of the constraints through a weighted empirical mean of past samples. Surprisingly, we demonstrate that using a UCB-based approach to estimate the constraints ensures optimal performance under both stochastic and adversarial constraints. Our algorithm differs significantly from previous UCB-based approaches. For instance, it guarantees no-regret even with adversarial rewards and stochastic constraints, unlike previous works [2; 7; 23]. Moreover, it is the first UCB-like approach that provides an optimal competitive ratio of \(1+}{{}}\) with adversarial constraints, where \(\) is the unknown Slater's parameter.

Our algorithm consists of two simple components. The first is an adversarial regret minimizer working on _moving strategy sets_. In particular, at each round, the regret minimizer chooses a strategy in the current optimistic estimation of the feasible set, and is required to achieve no-regret with respect to any strategy in the intersection of all feasibility set estimations. The second component is a tool for estimating the feasible set using an optimistic weighted mean of previous samples. The key challenge in this approach is designing adaptive weights that meet the different requirements for stochastic and adversarial constraints. Intuitively, in stochastic settings, we aim to converge to the (unweighted) empirical mean of the observed constraints. Conversely, in adversarial settings, we should assign larger weights to recent samples to address time-dependent constraints.

Not only is our algorithm significantly simpler than previous approaches, with a clean and insightful analysis, but it also provides better theoretical performance than primal-dual methods. Indeed, it is the first best-of-both-worlds algorithm to provide bounds logarithmic in the number of constraints. Moreover, in stochastic settings, it is the first algorithm to provide \(()\) regret _without_ requiring Slater's condition. Finally, it guarantees that the expected violation in the current round converges to zero, making our algorithm "converge" to strategies that are feasible in expectation. This provides a more stable and consistent control on the violations.

## 2 Model and Preliminaries

We address the problem faced by an agent aiming at maximizing its cumulative reward over a time horizon \(T\), while satisfying \( m\) long-term constraints.1 The agent has a set \( K\) of available actions and, at each round \(t T\), selects \(a_{t} K\). The agent then observes the corresponding reward \(f_{t}(a_{t})\) and a cost \(g_{t}^{(i)}(a_{t})[-1,1]\), for each constraint \(i m\). We define the cumulative violation of the \(i^{th}\) constraint as

\[V_{T}^{(i)}_{t T}g_{t}^{(i)}(a_{t}),\]while \(V_{T}_{i[m]}V_{T}^{(i)}\) is the maximum violation across all constraints. At a high level, we want to minimize the regret while keeping the violation of each constraint \(V_{T}^{(i)}\) sublinear in \(T\).

The focus of this paper is on handling both stochastic and adversarial constraints. Conversely, we always assume the rewards to be generated up-front by an adversary; we do not treat explicitly the situation where the rewards are generated i.i.d. because our guarantees are already tight for the harder case of adversarial rewards.2 In the stochastic setting, we assume that \(g_{t}=\{g_{t}^{(i)}\}_{i[m]}\) is drawn i.i.d. from a fixed but unknown distribution \(\), and we let \(^{(i)}(a)=_{g}[g^{(i)}(a)]\) be the expected cost of action \(a\) for the \(i^{th}\) constraint. On the other hand, in the adversarial setting \(\{g_{t}\}_{t[T]}\) is an arbitrary sequence of cost functions.

Let \(_{K}\) to be the set of discrete probability distributions over the set \([\![K]\!]\). Then, at round \(t[\![T]\!]\), given a randomized strategy \(x_{t}_{K}\), the expected learner reward is \(_{a[\![K]\!]}f_{t}(a)x_{t}(a)= x_{t},f_{t}\). Similarly, \( x_{t},g_{t}^{(i)}\) denotes the expected cost of the \(i^{th}\) constraint. Finally, we use \(n_{t}(a)\) to denote the number of times arm \(a\) was played up to time \(t\), i.e., \(n_{t}(a)=_{=1}^{t}(a_{}=a)\).

We want to design algorithms which achieve good performances in both the adversarial and the stochastic setting. As it is customary in the literature, we compare our learning algorithm with different benchmarks according to the setting.

Stochastic BenchmarkIn the stochastic setting, the constraints \(g_{t}^{(i)}\) are i.i.d. samples with mean \(^{(i)}\) and thus we consider as benchmark the best fixed randomized strategy that satisfies the constraints in expectation, which is a standard choice in bandits with constraints [11; 28; 16]. Formally, in the stochastic setting, we can define the feasible sets \(_{i}^{}\) and \(^{}\) as follows:

\[_{i}^{}\{x_{K}: x,^{(i) } 0\}^{}_{i[m] }_{i}^{}.\]

Then, we can define the stochastic baseline as:

\[}_{x^{}}_{t[\![T]\!]}  x,f_{t}.\]

We naturally assume the existence of safe mixed strategies, _i.e._, that \(^{}\). This is equivalent to assume the existence of a randomized strategy \(x^{}\) such that \( x^{},^{(i)} 0\) for all \(i\). Notice that this is a weaker assumption than the one commonly assumed by best-of-both-worlds algorithms in which \( x^{},^{(i)}-\), where \(\) is a strictly positive constant (see, e.g., [16; 11; 10]).

Adversarial BenchmarkIn the adversarial setting, \(\{g_{t}\}_{t[\![T]\!]}\) is an arbitrary sequence of constraints. We consider as benchmark the best unconstrained strategy:

\[}_{x_{K}}_{t[\![T]\!]} x,f _{t}.\]

While this baseline has already been used [e.g., 11; 13]), other works on adversarial bandit with constraints employ weaker baselines [e.g., 21; 16]. For instance, Castiglioni et al.  consider the best fixed strategy which is feasible on average. However, we show that, despite using a stronger baseline, we obtain a competitive ratio that is optimal even for the weaker baselines commonly adopted in the literature [16; 10; 9].

### Best-Of-Both-Worlds Guarantees

Our goal is to design learning algorithms that exhibit optimal guarantees both in the stochastic and adversarial settings. In the stochastic setting, we are interested in minimizing the _regret_\(R_{T}\) w.r.t. \(}\):

\[R_{T}=}-_{t[\![T]\!]}f_{t}(a_{t}),\]and specifically we require both \(R_{T}\) and \(V_{T}\) to be in \(()\) with high probability. This clearly matches the standard \(()\) lower bound that holds even without constraints .

In the (harder) adversarial setting, we pose the less ambitious goal of achieving a constant competitive ratio with respect to \(}\), or equivalently sublinear \(\)-regret with constant \(\). Formally, given an \(<1\), we define the \(\)-regret as:

\[R_{T}=}-_{t T }f_{t}(a_{t}).\]

As it is customary in the literature , the competitive ratio \(\) obtained by our algorithms depends on the following Slater's parameter \(\):

\[=-_{a K}_{t T ,i m}g_{t}^{(i)}(a).\] (1)

The parameter \(\) is related to the existence of strictly-feasible actions, and only depends on the constraints. Our definition is slightly stronger than the one in most previous works where the \(\) is over randomized strategies. To guarantee the existence of a feasible strategy we assume that \( 0\). Then, our goal is to guarantee that both \(V_{T}\) and the \(\)-regret, with \(=}{{}}+1\), belong to \(()\) with high probability. Note that this matches the lower bound of Bernasconi et al. .

## 3 Our Approach

In this section, we present the main components of our algorithm, while the following sections will describe the specific components in details. We refer to Algorithm 1 for the pseudocode. At each step \(t\), the algorithm works in two phases: i) it estimates the feasible set, and ii) it plays a strategy in the estimated set. Each phase requires a specific ingredient:

* An estimator \(_{t}^{(i)}\) of the costs functions \(g_{t}^{(i)}\) that is used together with the optimistic bonus \(b_{t}\) to define the estimation of the feasible set defined as \(}_{t}_{i m}}_{t}^{(i)}\). In the stochastic case, we would like \(}_{t}^{}\), while in the adversarial case our goal is to maintain a sequence of sets that always contains a version of the action set \(\), properly scaled around \(a^{}\) (see Equation (2) for a formal definition).
* A regret minimizer \(\) for adversarial linear reward function that, at each round, takes in input a convex set of feasible strategies \(}_{t}_{K}\), and then selects a strategy \(x_{t}}_{t}\). We require the regret minimizer to achieve \(()\) regret with respect to any \(x_{t T}}_{t}\);

In the following we define the two phases more in details. Let \(_{t,a}\{ t:a_{t}=a\}\) be the set of rounds in which the algorithm plays action \(a\). Then, at each round \(t\), Algorithm 1 computes the estimate

\[_{t}^{(i)}(a)=_{_{t-1,a}}w_{t,a}^{(i)}()g_{ }(a) a Ki m\]as the weighted mean of _available_ past observations \(\{g_{}^{(i)}(a)\}_{[\![t-1]\!]}\) for each actions \(a[\![K]\!]\) and constraint \(i[\![m]\!]\), for some weights \(w_{t,a}^{(i)}\).3 Then, the estimates together with the optimistic bonus \(\{b_{t}(a)\}_{a[\![K]\!]}\) are used to define the _moving sets_\(}_{t}\), which are fed to the regret minimizer \(\) which in turn selects a point \(x_{t}}_{t}\).

One crucial property that is required for the execution of the regret minimizer \(\) is that all the sets \(}_{t}\) are non-empty (as otherwise the regret minimizer has no feasible strategies). To simplify exposition, in the following sections we assume that the clean event \(\{}_{t}\{\}\,  t[\![T]\!]\}\) holds. In Corollary 6.3, we prove that this event holds with high probability in the stochastic setting, while in Theorem 5.2 we argue that it holds deterministically in the adversarial one.

In Algorithm 1 we left unspecified two crucial parts of our approach. The first is how to build the regret minimizer \(\), and the second concerns how to actually generate the sets \(}_{t}\), i.e., the weights \(w_{t,a}^{(i)}\) and the bonus \(b_{t}(a)\). We delve into these details in Section 4 and Section 5, respectively.

## 4 No-regret on moving sets

We describe the regret minimizer \(\) that exhibits no-regret with respect to any \(x_{t[\![T]\!]}}_{t}\). We achieve this via a simple modification to the EXP-IX algorithm of Neu  that provides high probability results for multi-armed bandits via implicit exploration. More specifically, our algorithm maintains a randomized strategy \(x_{t}_{K}\) which is updated using the biased reward estimate \(_{t}(a)\) as in Neu  and then projected onto \(_{t}\) according to the negative entropy Bregman divergence \(B(x||y)=_{a[\![K]\!]}[x(a)(x(a)/y(a))-x(a)+y(a)]\). We refer to Algorithm 2 for the pseudocode, and present here the main result of the Section.

**Theorem 4.1**.: _Let \(x_{t}\) be selected accordingly to Algorithm 2 run with arbitrary sequence of convex sets \(}_{t}_{K}\) with \(=\) and \(=})}{KT}}\). Then, with probability at least \(1-_{1}\) it holds that_

\[_{t[\![T]\!]} f_{t},x-f_{t}(a_{t}) 4)}, x_{t[\![T]\!]}}_{t}.\]

This result establishes no-regret in the case of moving sets, taking as benchmark the optimal strategy in the intersection of all sets. To exploit this result in Algorithm 1, we have to make sure that in both the stochastic and adversarial setting the intersection of the sets \(}_{t}\) contains "good" strategies. In the stochastic setting, we show that with high probability it includes \(^{}\), while, in the adversarial setting, it includes a strategy with utility \(}{{1+}}_{}\).

## 5 How to build the sets \(}_{t}\)

In this section, we show how to design estimations \(}_{t}\) of the feasible sets that, surprisingly, are effective both in stochastic and adversarial settings. Indeed, the main challenge is to design sets \(}_{t}\)

[MISSING_PAGE_FAIL:6]

Thus, given an action \(a[\![K]\!]\) and a time \(t[\![T]\!]\) the corresponding weights \(\{w^{(i)}_{t,a}()\}_{_{t-1,a}}\) are:

\[w^{(i)}_{t,a}()=^{(i)}_{}(a)_{k_{t-1,a}:k>} (1-^{(i)}_{k}(a))_{t-1,a}\]

We now proceed to give two notable examples on how to instantiate the learning rates and recover commonly used estimators such as the empirical mean and the exponentially weighted mean.5

**Proposition 5.4**.: _If \(^{(i)}_{t}(a_{t})=(a_{t})}\) for each \(_{t-1,a}\), then \(w^{(i)}_{t,a}()=(a)}\) and we recover the empirical mean estimator for \(^{(i)}_{t}(a)=(a)}_{_{t-1,a}}g^{( i)}_{}(a)\)._

**Proposition 5.5**.: _If \(^{(i)}_{t}(a_{t})=\) then_

\[w^{(i)}_{t,a}()=(1-)^{|\{k_{t-1,a}:k>\}|}\]

_for each \(_{t-1,a}\) and we recover an exponentially weighted average estimator for \(^{(i)}_{t}(a)\)._

As it will turns out, these are the two extreme cases that we want to interpolate between. Indeed, the empirical mean estimator is particularly effective in the stochastic case but ineffective in the adversarial case, while the converse happens with the exponentially weighted estimator.

Now, we show that the OGD interpretation is particularly useful to bounds the violations suffered by the algorithm. First, we define the violations in an interval \([t_{1},t_{2}]\{t[\![T]\!]:t_{1} t t_{2}\}\) as:

\[V^{(i)}_{[t_{1},t_{2}]}=_{t=t_{1}}^{t_{2}}g^{(i)}_{t}(a_{t}).\]

Then, in the following lemma we show that the violations in the interval are related to the variation of the estimates \(^{(i)}_{t}(a)\).

**Theorem 5.6**.: _Given an interval \([t_{1},t_{2}][\![T]\!]\), an \(i[\![m]\!]\), and a \(>0\), with probability at least \(1-\) it holds:_

\[V^{(i)}_{[t_{1},t_{2}]}_{a[\![K]\!]}_{_{t_{2},a}[t_{1},t_{2}]}_{}(a)}(^{(i)}_{+ 1}(a)-^{(i)}_{}(a))+_{=t_{1}}^{t_{2}} x_{},b_{}+4-t_{1})(}{{}})}.\]

By a simple telescoping argument, we have the following corollary, which holds whenever the learning rates are non-increasing within a time interval. Let \((a,[t_{1},t_{2}])\) be the last rounds in the interval \([t_{1},t_{2}]\) in which action \(a\) is played.

**Corollary 5.7**.: _Given an interval \([t_{1},t_{2}][\![T]\!]\), \(a[\![m]\!]\), and a \(>0\), assume that for any \(a[\![K]\!]\) it holds \(^{(i)}_{}(a)^{(i)}_{^{}}(a)\ <^{}_{t_{2},a}[t_{1},t_{2}]\). Then, with probability at least \(1-\) it holds:_

\[V^{(i)}_{[t_{1},t_{2}]}_{a[\![K]\!]}_{(a,[t _{1},t_{2}])}(a)}+_{=t_{1}}^{t_{2}} x_{},b_{}+4 -t_{1})(}{{}})}.\]

Corollary 5.7 shows how to bound the violation as a function of the learning rates \(^{(i)}_{t}\) and the bonus terms \(b_{}\). The following lemma shows how to bound the second term of the violations depending on the structure of the bonus terms.

**Lemma 5.8**.: _Given a \(c>0\), an \((0,1)\), a \(t[\![T]\!]\), and a \(>0\), let \(b_{t}(a)=(a)^{}}\) for all \(a[\![K]\!]\). Then, with probability at least \(1-\), it holds:_

\[_{=1}^{t} x_{},b_{}K^{ }t^{1-}+4}{{}})}.\]

In this section, we saw how the choice of the learning rates of the estimator affects the estimators. In the following section, we will see how to _adaptively_ set those learning rates to handle both stochastic and adversarial settings.

Adaptive learning rates

The previous section highlights the main difficulties of obtaining best-of-both-world algorithms: we need to set the weights \(w_{t,a}^{(i)}\) (or equivalently - by Lemma 5.3 - the learning rates \(_{t}^{(i)}(a_{t})\)) and the optimistic bonuses \(b_{t}\) so that they meet, at the same time, the requirements needed by the stochastic and the adversarial settings.

We start presenting two possible choices and show that they fail either in the stochastic or the adversarial setting. Then, we show how adaptive learning rates combine the strengths of both approaches. The first, natural, choice of setting the learning rate is to use an exponentially weighted estimator, i.e., choose \(_{t}^{(i)}(a_{t})=}{{}}\). With this choice, we can apply a weighted version of Azuma-Hoeffding inequality and find that \(|_{t}^{(i)}(a)-^{(i)}(a)|(n_{t}(a)^{- {{1}}{{4}}})\), with high probability. Thus, as discussed in Section 5.1, we would need to define \(b_{t}(a)(n_{t}(a)^{-}{{4}}})\), which, by Corollary 5.7 and Lemma 5.8 would imply a suboptimal \((T^{}{{4}}})\) rate for the violations.

The second option is to set \(_{t}^{(i)}(a_{t})=}{{n_{t}(a_{t})}}\). In the stochastic setting, we have an optimal rate of concentration of the terms \(|_{t}^{(i)}(a)-^{(i)}(a)|(n_{t}(a)^{- }{{2}}})\) as, by Proposition 5.4, this is equivalent to compute the empirical mean. However, this second option fails disastrously in the adversarial setting as highlighted in Corollary 5.7, where the first component of the violations becomes linear in \(T\). Intuitively, a learning rate of order \(}{{n_{t}(a)}}\) makes the update of the estimates too slow when the underlying constraints change, as it does happen in the adversarial setting.

This trade-off forces us to employ _adaptive learning rates_. Our idea is to use learning rates of the order \(}{{n_{t}(a)}}\) with an adaptive multiplicative term that depends on the current violation of the constraint. Formally, we use learning rates:

\[_{t}^{(i)}(a_{t})(a)}(1+_{t}^{(i)} ),\]

where \(_{t}^{(i)}\) is a bonus term defined as

\[_{t}^{(i)}[V_{t-1}^{(i)}-21}{{_{2}}})}]_{0}^{21}{{ _{2}}})}},\]

and \([x]_{a}^{b}((x,a),b)\) is the clipping of \(x\) between \(a\) and \(b\). Moreover, we set the exploration bonus as

\[b_{t}(a)=}{{_{2}}})}{n_{t-1}( a)}}.\]

The following theorem shows that such approach guarantees \(()\) violations in both adversarial and stochastic settings.

**Theorem 6.1**.: _Both in the stochastic and the adversarial setting, with probability at least \(1-2mT^{2}_{2}\) it holds that_

\[V_{t} 53}{{_{2}}})} t  T.\]

The previous theorem shows that this choice of learning rates is sufficient to guarantee optimal bounds on the violations. However, to achieve this result we are setting \(b_{t}(a)(n_{t}(a)^{-}{{2}}})\). As we showed in theorem 5.1, this requires a concentration on the estimates \(|_{t}^{(i)}(a)-_{t}^{(i)}(a)|\) of the same magnitude (in the stochastic setting). This is crucially needed to ensure that the regret minimizer \(\) provides the desired guarantees and that the event \(\) defined in Section 3 actually holds with high probability.

**Lemma 6.2**.: _In the stochastic setting, with probability at least \(1-5mKT_{2}\), it holds that:_

\[|_{t}^{(i)}(a)-_{t}^{(i)}(a)| b_{t}(a) a  K,t T,i m\]

The proof of the previous result relies on the fact that in the stochastic case the bonus \(_{t}^{(i)}\) does not "kick in" ensuring that \(_{t}^{(i)}(a)=}{{n_{t}(a)}}\). Thus, \(_{t}^{(i)}\) is the empirical average of past observations. The previous result, together with Theorem 5.1 proves the following corollary.

**Corollary 6.3**.: _In the stochastic setting, with probability at least \(1-5mKT_{2}\), it holds that \(^{}}_{t}\) for all \(t[\![T]\!]\)._

This proves that the clean event \(\) holds with high probability, as promised in Section 3.

## 7 Putting everything together

Now, we have everything in place to easily prove the our main theorems. First, we define the parameters \(_{1}=_{1}()\) and \(_{2}=_{2}()\) in order to guarantee that our theorems hold with probability at least \(1-\). In particular, we set \(_{1}()=}{{2}}\), where we recall that \(_{1}\) is the parameter used to set \(\) and \(\) in Algorithm 2, and \(_{2}()=}{{(14mKT^{2})}}\), where \(_{2}\) is used to set the optimistic bonus and learning rate of Algorithm 1.

In the stochastic setting, the violation guarantees directly follow from Theorem 6.1, while the regret guarantee follows by combining Theorem 4.1 and Corollary 6.3. Formally:

**Theorem 7.1**.: _In the stochastic setting, for any \(>0\) Algorithm 1 guarantees that with probability at least \(1-\):_

\[R_{T} 4}{{}})}  V_{t} 53/)} t[\![T]\!].\]

Now, we turn to the adversarial setting. Theorem 6.1 guarantee \(()\) violations even with adversarial constraint, while the regret guarantees follows by combining Theorem 5.2 and Theorem 4.1

**Theorem 7.2**.: _In the adversarial setting, for any \(>0\) Algorithm 1 guarantees that with probability at least \(1-\):_

\[R_{T} 4}{{}})}  V_{t} 53/)} t[\![T]\!],\]

_where \(=}{{(1+)}}\)._

Note that in both settings, the regret upper bound is of order \(()\) and it is independent from the number of constraints \(m\), while the violations are of order \(()\) and depend only logarithmically on \(m\). This is in contrast to the other best-of-both-world algorithms for bandits with long term constraints, based on primal-dual methods, in which both the regret and the violations depends polynomially in \(m\).

Another interesting characteristic of our methodology is that we guarantee an anytime bound on the constraint violation. Indeed, this matches the guarantees provided by the most recent primal-dual methods [11; 1] that, however, require weakly-adaptive underlying regret minimizers.

### Convergence rate in the stochastic setting

To conclude, we point to a nice byproduct of our analysis. In the stochastic setting, we can easily prove a sort of "convergence rate" of \(x_{t}\) to the set \(^{}\). Formally, we can prove that _positive violations_ are bounded by \(()\) as long as we consider expected violations. Let us define \(x^{+}(x,0)\) and

\[^{+}_{t}_{i[\![m]\!]}_{=1}^{t}[  x_{},^{(i)}_{}]^{+}.\]

Then, we can state the following theorem:

**Theorem 7.3**.: _Algorithm 1, in the stochastic setting, guarantees that with probability at least \(1-\), it holds that:_

\[^{+}_{t} 16/)} t[\![ T]\!].\]

Intuitively, our result shows that our algorithm plays only a sublinear number of times "far" from the set \(^{}\), or that our algorithm plays a linear number of times "close" to the set \(^{}\). This is a much stronger result then just guaranteeing that \(V_{T}\) is sublinear, as in that case it might be a linear number of times the algorithm plays "far" from \(^{}\) as long as it plays strictly inside of \(^{}\) often enough.