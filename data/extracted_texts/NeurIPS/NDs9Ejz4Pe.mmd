# DiPEx: Dispersing Prompt Expansion for

Class-Agnostic Object Detection

 Jia Syuen Lim1   Zhuoxiao Chen1   Mahsa Baktashmotlagh

**Zhi Chen Xin Yu   Zi Huang   Yadan Luo2**

The University of Queensland

{jiasyuen.lim, zhuoxiao.chen, m.baktashmotlagh}@uq.edu.au

{zhi.chen, xin.yu, helen.huang, y.luo}@uq.edu.au

Equal ContributionCorresponding Author

###### Abstract

Class-agnostic object detection (OD) can be a cornerstone or a bottleneck for many downstream vision tasks. Despite considerable advancements in bottom-up and multi-object discovery methods that leverage basic visual cues to identify salient objects, consistently achieving a high recall rate remains difficult due to the diversity of object types and their contextual complexity. In this work, we investigate using vision-language models (VLMs) to enhance object detection via a self-supervised prompt learning strategy. Our initial findings indicate that manually crafted text queries often result in undetected objects, primarily because detection confidence diminishes when the query words exhibit semantic overlap. To address this, we propose a Dispersing Prompt Expansion (**DiPEx**) approach. DiPEx progressively learns to expand a set of distinct, non-overlapping hyperspherical prompts to enhance recall rates, thereby improving performance in downstream tasks such as out-of-distribution OD. Specifically, DiPEx initiates the process by self-training generic parent prompts and selecting the one with the highest semantic uncertainty for further expansion. The resulting child prompts are expected to inherit semantics from their parent prompts while capturing more fine-grained semantics. We apply dispersion losses to ensure high inter-class discrepancy among child prompts while preserving semantic consistency between parent-child prompt pairs. To prevent excessive growth of the prompt sets, we utilize the maximum angular coverage (MAC) of the semantic space as a criterion for early termination. We demonstrate the effectiveness of DiPEx through extensive class-agnostic OD and OOD-OD experiments on MS-COCO and LVIS, surpassing other prompting methods by up to 20.1% in AR and achieving a 21.3% AP improvement over SAM. The code is available at https://github.com/jason-lim26/DiPEx.

## 1 Introduction

In real-world applications, the class of interest may constantly change, prompting the need for new tasks like out-of-distribution (OOD) detection , open-world detection  and open-vocabulary  object detection (OD) to ensure reliable operation of detectors. A significant bottleneck in these OD tasks is the ability to locate _all_ objects in a scene - typically referred to as class-agnostic OD . Ensuring a high _recall_ rate is essential in this task as it lays the foundation for correctly classifying objects, thereby improving the average precision for classes of interest. Conversely, a low recall implies that some objects will be missed entirely, negatively impacting downstream recognition tasks.

Conventional solutions to the under-explored class-agnostic OD task often rely on bottom-up strategies  such as selective search  or EdgeBox, which generate a large ranked set of class-agnostic proposals based on low-level visual cues. To address the low precision and scalability issues of these approaches, another line of research has explored multi-object discovery by leveraging (self)-supervised features from vision transformers (ViT) (_e.g._, DINO , MoCo-v2 , SwAV ), or external motion information to support region proposal regression. However, these methods still fall short, achieving only about 30% average recall (AR) on benchmark datasets like MS-COCO due to the lack of intrinsic knowledge about a wide range of objects. The newly released vision-language models (VLMs) such as Grounding DINO , GLIP , T-Rex2 , which are pretrained on large-scale grounding datasets, have opened up new opportunities for acquiring common knowledge for generic object localization. VLMs have demonstrated impressive zero-shot recognition capacities given the provided _textual prompt_. However, to effectively locate all objects, one would need to input all class names accurately, which is impractical in real-world applications.

To better understand the limitation of modern VLMs in generic object localization, we investigated the design of hand-crafted text queries (Section 2) to enhance detection recall through two approaches: (1) We employed a **Universal query**, using ChatGPT to generate 13 types of broad nouns and adjectives (_e.g._, "objects", "generic") as queries for the Grounding DINO model, aiming to detect a wide array of objects without focusing on specific categories; (2) We implemented a **Class-Wide query**, selecting 25 high-level semantic words (_e.g._, "plant", "animal") from the top layer of the WordNet hierarchy (also used for the ImageNet vocabulary) to cover extensive object categories. Our findings, depicted in Figure 0(b) and Table 1, reveal that while VLMs can generalize across universal object categories, combining all queries into _one string_ significantly reduces detection performance (by up to 52% in AR) due to the "**semantic overlap**" among words. This suggests that optimal detection requires conducting multiple _separate_ inferences, presenting substantial computational demands for large datasets.

To overcome the aforementioned limitations, we propose a novel self-supervised Dispersing Prompt Expansion (DiPEx) strategy. This approach progressively expands a set of non-overlapping hyperspherical prompts for capturing all objects in a given dataset, thereby benefiting downstream tasks such as out-of-distribution object detection. Specifically, we start with a generic parent prompt that is self-supervised using the Universal and Class-wide text queries. To capture more fine-grained semantics, we split the parent prompts with high semantic uncertainty into a set of distinct child prompts. We initialize child prompts by diversifying the parent token embedding, randomly rotating it to different angles on the hypersphere to yield a range of unique prompts. Dispersion losses are employed to minimize semantic overlap among child prompts while maintaining semantic consistency across parent-child prompt pairs. To prevent excessive growth of the prompt sets, we estimate the maximum angular coverage (MAC) of the semantic space as a criterion to terminate the prompt expansion process, balancing semantic richness and computational overhead. Extensive experiments on the MS-COCO and LVIS datasets verify the effectiveness and versatility of the proposed DiPEx strategy. With a single pass of inference, DiPEx can achieve by up to 20.1% improvements in average recall (particularly 35.2% for small objects) and outperforms segment anything model (SAM)  by 21.3% in average precision.

Figure 1: (a) An exemplar of the studied class-agnostic OD and downstream OOD-OD tasks. (B) Zero-shot class-agnostic OD performance of Grounding DINO  on MS-COCO , with the hand-crafted Universal query from ChatGPT and Class-wide query from WordNet .

**Related Study.** The full discussions can be found in Section A.1. Traditional bottom-up approaches for region proposal generation, such as those by  and , often face precision constraints despite high recall rates, limiting their scalability. Recent advancements in Vision Transformers (ViTs) by  and  have enabled self-supervised learning on massive datasets, extracting semantically meaningful features. Methods like LOST  and TokenCut  use graph-based techniques but are limited to detecting a single object per image. MOST  addresses this with entropy-based box analysis but struggles with generalization. MAVL  uses a late fusion strategy with text queries, requiring full supervision and multiple inferences. Our approach eliminates the need for labels and achieves state-of-the-art performance with one-pass inference using non-overlapping prompts. Vision-Language Models (VLMs), like those by  and , have shown potential in learning generic concepts. HierKD  and OV-DETR  align image representations with captions and extend DETR to open-vocabulary settings. GLIP , Grounding DINO , and T-Rex2  integrate object detection and visual grounding. However, VLMs' effectiveness depends on textual cues, and prompt tuning, as introduced by CoOp  and improved by CoCoOp and MaPLe , offers a solution by optimizing soft prompts while keeping the model's parameters frozen. ProDA  learns diverse prompts using a Gaussian model. DFKD-VLFM  and PromptStyler  attempted to diversify a fixed number of prompts through contrastive approach. Despite these advancements, full supervision is typically required. UPL  and POUF  introduced unsupervised prompt learning, but adaptation for object detection remains limited. DiPEx is the first to apply prompt learning to class-agnostic object detection through a progressive self-training approach.

## 2 Pilot Study

In this section, we detail our preliminary exploration of the zero-shot detection capabilities using state-of-the-art VLM, Grounding DINO , to detect all objects irrespective of the associated classes on the MS-COCO dataset  as illustrated in Figure 0(b). We conduct experiments using two types of text queries: **Universal** queries generated by ChatGPT for general object detection, and **Class-Wide** queries derived from WordNet, representing broad object categories. Our experiments reveal that semantic overlap between text queries impacts detection performance. To support this hypothesis, we conduct a case study showing that similar concatenated prompts reduce the model's detection confidence.

### Hand-crafted Queries for Class-agnostic Object Detection

**Universal Query.** We employ ChatGPT to generate 13 synonyms of **universal** concepts, including nouns and adjectives, which are displayed as x-axis labels. The zero-shot object detection results, measured by average recall (AR) and precision (AP) across the top 100 confident boxes for each query text, are presented. The plot reveals that more general terms such as "generic" and "items" yield the highest AR. Surprisingly, more specific descriptors like "foreground", "small", or "tiny" tend to reduce AR and do not effectively aid in identifying foreground or small objects.

**Class-wide Query.** We utilize 25 semantically independent beginner words (listed as x-axis labels in the bottom figure) from the highest level of the WordNet hierarchy  as **class-wide** text queries. A variation in AR (0.26\(\)0.43) is observed with different textual queries from WordNet, with a mean AR of 0.35. Compared to the mean AR of 0.37 across class-agnostic queries generated by ChatGPT, the zero-shot detection ability remains similar, regardless of the types of queries used.

**Discussion on Multi-Word Queries.** The zero-shot results presented in Figure 0(b) were obtained using _single-word_ prompts for the Grounding DINO. To explore whether combining _multiple words_

   Word Source & Merging Strategy & AR & \(\)AR & AR@S & AR@M & AR@L & AP \\   & query-merging & 0.345 &  & 0.122 & 0.360 & 0.718 & 0.067 \\  & prediction-merging & 0.526 & & 0.317 & 0.606 & 0.781 & 0.274 \\   & query-merging & 0.461 &  & 0.234 & 0.522 & 0.774 & 0.229 \\  & prediction-merging & 0.570 & & 0.382 & 0.646 & 0.796 & 0.344 \\   & query-merging & 0.408 &  & 0.162 & 0.471 & 0.751 & 0.121 \\  & prediction-merging & 0.589 & & 0.410 & 0.665 & 0.798 & 0.353 \\   

Table 1: Zero-shot class-agnostic object detection performance of Grounding DINO  on MS-COCO , with _hand-crafted_ prompts from various sources. We report average recall (AR) and precision (AP) limited to a maximum of 100 detections per image. \(\)AR quantifies the percentage decrease in AR comparing “query-merging” to “prediction-merging” for forming _multi-word_ queries.

as prompts from a given source (_e.g._, WordNet) could improve zero-shot detection performance, we developed strategies for merging at both the input stage (query-merging) and the output stage (prediction-merging) as shown in Table 1. The query-merging strategy concatenates all input text queries (_e.g._, "foreground. elements \(\) tiny. objects.") and performs a single-pass inference to obtain detections. The prediction-merging strategy, on the other hand, uses each text query individually for separate inference and then combines all box predictions. Table 1 shows that applying query-merging to Universal words results in a 52.46% reduction in AR compared to prediction-merging, whereas Class-wide queries (_e.g._, from WordNet) achieve a smaller decrease in AR of only 23.64%. These findings suggest that large semantic overlaps in concatenated queries (_e.g._, "stuff", "objects" and "item" from ChatGPT) may greatly contribute to diminished object detection performance. To further investigate this phenomenon, we conducted a case study analyzing the impact of semantic overlap on detection performance, which is presented in the following section.

### Confidence Diminishing when Text Query Semantically Overlap

To verify our hypothesis, we conduct a case study to demonstrate how semantic overlap in multi-word query leads to diminished detection confidence. We quantify semantic overlap by calculating the angular distance between pairs of textual token embeddings generated by BERT . As shown in Figure 2, a small angular distance \(\) of 53.73\({}^{}\) between the text tokens "plates" and "dishes" diminishes the model's confidence. Consequently, some boxes that could be precisely localized with high confidence using the single token "plates" are omitted. In contrast, concatenating two text tokens with a larger angular distance (_e.g._, 60.99\({}^{}\) between "plates" and "cup") maintained high detection confidence. This combination resulted in bounding box predictions that encompassed all boxes predicted with each individual token ("plates" or "cup"). This case study supports our hypothesis that semantic overlap between concatenated text queries can interfere with the detection confidence of the model. Therefore, we propose that developing a method to learn a set of semantically _non-overlapping_ prompts for the target dataset could enable efficient object localization with _one-pass_ inference using VLMs.

## 3 Proposed Approach

In this section, we first mathematically formulate the task of class-agnostic detection using a general VLM and, without loss of generality, illustrate the process using Grounding DINO  as an exemplar model. We detail the steps of the proposed dispersing prompt expansion in Section 3.2, followed by the early termination strategy of the prompt set growth.

### Problem Formulation

**Class-agnostic OD.** Let \(\) denote the input image and \(\) the associated text query. For the zero-shot object detection in a class-agnostic setting, we consider the text query \(\) to be of the form of "a photo of a {class}", where the class token {class} is sampled from our predefined Universal (_e.g.,_ "objects") or Class-wide (_e.g.,_ "plant") sets as described in Section 2. The text query is then tokenized and projected into word embeddings as \(=\{_{1},_{2},,_{M},\}\), where

Figure 2: A case study investigating the impact of semantic overlap between text queries on the detection confidence of the pre-trained Grounding DINO . Semantic overlaps are quantified by the angular distance, denoted as \(\), between tokenized embeddings of word pairs using BERT .

\(=\{_{i}\}_{i=1}^{M}^{M d}\) indicates a set of \(M\) contextual embeddings and \(\) is the query text embedding. Here, \(d\) indicates the dimensions of learnable tokens. The visual embeddings \(_{v}\) extracted from the visual encoder and prompt embeddings \(\) are fused jointly to prompt the VLM and generate the final bounding box predictions \(=f(_{v},)^{N_{B} 4}\), with \(f\) being the VLM, and \(N_{B}\) being the number of predicted boxes. Formally, the objective of class-agnostic OD is to ensure that the generated bounding boxes can capture any objects as comprehensively as possible.

**Adapt Prompt Tuning for Class-agnostic OD.** Instead of relying on hand-crafted templates, prompt tuning approaches like CoOp  and CoCoOp , originally developed for classification tasks, aim to _learn_ the context embeddings \(\) with a frozen VLM using a supervised contrastive learning loss. To adapt these prompt learning approaches to the Grounding DINO  detection framework, we first construct a pseudo label set \(_{}\) from the zero-shot detection results with Universal and Class-wide text queries (see Section A.3 for details). The prompt learning is then supervised by the standard box regression loss \(_{}\), \(_{}\) and focal classification loss \(_{}\) as implemented in .

### Dispersing Prompt Expansion (DiPEx)

Unlike previous prompt tuning approaches, the proposed DiPEx strategy aims to iteratively grow a set of learnable prompts \(=\{_{1},_{2},,_{L}\}\) in a tree hierarchy of depth \(L\). To maximize the utility of prompts and ensure minimal semantic overlap among them, we assume \(\) resides on the surface of a unit-hypersphere, _i.e.,_\(\|_{i}\|_{2}=1\). This assumption transforms the overlap minimization problem into maximizing the angular distances among the learned prompts. In the initial round, we set a single learnable parent prompt \(_{1}=\{\}\), which is self-trained using \(_{}\) with the same procedure outlined above. In each subsequent round \(l\) for \(l[1,L]\), we identify the parent prompts of highest uncertainty and grow \(K\) child prompts \(_{l+1}^{K d}\) from it. The learned \(_{l}^{*}\) is then frozen and stored in a parent queue \(_{}\). Prompt growth is terminated when the maximum angular coverage \(_{}\) exceeds a certain threshold \(_{}\).

**Child Prompt Initialization.** Continuing from the previous discussion, we now describe the process of child prompt initialization, which aims to inherit the semantics from parent prompts while capturing more fine-grained semantics. After the \(l\)-th round of training, we expand the parent prompt with the highest uncertainty, denoted as \(_{l}^{*}_{l}\), into a set of learnable child prompts (Figure 3). We empirically adopt the logit activation frequency of the prompts as a measure of uncertainty, visualized in Figure 6. The rationale is that if a prompt is activated for most samples, it covers overly broad semantics (_e.g.,_ animals) and may need to be decomposed into narrower categories (_e.g.,_ cats and dogs). To disentangle the complex semantic of \(_{l}^{*}\), we set up \(K\) child prompts \(_{l+1}=\{_{l+1,k}\}_{k=1}^{K}\) for the selected parent prompt \(_{l}^{*}\). To diversify the initialized embedding for each child prompt, we introduce \(K\) random angular offsets \(=\{_{k}\}_{k=1}^{K}\) to rotate \(_{l}^{*}\) on the hypersphere by different angles \(_{k}[-,]\). Given that \(_{l}^{*}\) is a \(d\)-dim vector, we randomly sample two axes \(i\) and \(j\) where \(i,j[1,d]\) for rotation. The \(k\)-th child prompt embedding \(_{l+1,k}\) is then obtained by applying the

Figure 3: An illustration of the 1 proposed prompt expansion strategy that selectively grows a set of child prompts for the highlighted parent prompt across \(L\) iterations; 2 diversifying initialized embeddings of the child prompt on a hypersphere and 3 quantifying maximum angular coverage \(_{}\) for early termination of the prompt growth.

[MISSING_PAGE_FAIL:6]

on the MS-COCO, which includes a mixture of both ID and OOD objects. While we followed the settings outlined in OOD-OD , with 20 base classes in PASCAL-VOC  designated as ID classes and the remaining classes treated as OOD. Our choice of dataset enhances the rigor of our evaluation by combining both ID and OOD instances, providing a more realistic assessment of our method's real-world conditions.

**Evaluation Metrics.** We report results for class-agnostic object detection on both the MS-COCO and LVIS validation splits. For evaluation, we adopt official metrics from the COCO 2017 challenge. Specifically, we report average precision (AP) at IoU thresholds from 0.5 to 0.95, along with average recall (AR) across the same threshold range. We also report AR by object scale: AR@S for small, AR@M for medium, and AR@L for large objects. Details on our implementation, including those of prior works used as _baselines_, are provided in Appendix A.2.

### Main Results on Class-agnostic OD and OOD-OD

**Class-agnostic OD on MS-COCO.** To validate our proposed method for class-agnostic object detection, we compared it against ten different baseline methods on the MS-COCO dataset, using various metrics as reported in Table 2. We observed that non-parametric methods generally underperform compared to self-training methods due to their inability to learn and extract semantic and geometric information about objects from the dataset. In contrast, Grounding DINO, leveraging pre-trained knowledge, demonstrates strong zero-shot capabilities and achieves AR\({}_{100}\) of 44.1% with a single text prompt, "generic". Furthermore, CoOp, which fine-tunes prompts for Grounding DINO, enhances class-agnostic detection performance by 39.0% in AR\({}_{100}\) compared to direct zero-shot inference. Our method, which expands the learnable prompts to a wider angular distance, surpasses all baselines by achieving the highest performance across all metrics and outperforming the leading baseline, CoOp, by 3.1% in AR\({}_{100}\). Notably, for _small objects_ which are challenging to localize, our method improves

   Method & Description & AR\({}_{1}\) & AR\({}_{10}\) & AR\({}_{100}\) & AR@S & AR@M & AR@L & AP \\   Selective Search  & non-parametric & 0.1 & 1.1 & 7.8 & 0.9 & 7.2 & 20.7 & 0.1 \\ UP-DETR  & self-training & 0.2 & 1.4 & 1.4 & 0.0 & 0.2 & 5.8 & 0.1 \\ DETReg  & self-training & 0.6 & 3.7 & 12.9 & 0.2 & 12.8 & 35.3 & 1.4 \\ FreeSOLO  & self-training & 3.7 & 9.7 & 12.6 & 0.5 & 12.3 & 34.1 & 4.2 \\ Exemplar-FreeSOLO  & self-training & 8.2 & 13.0 & 17.9 & – & – & – & 12.6 \\ MOST  & self-training & 3.1 & 6.4 & 6.4 & 0.1 & 1.6 & 24.5 & 3.3 \\ CutLER  & self-training & 6.8 & 19.6 & 32.8 & 13.7 & 37.5 & 60.0 & 29.6 \\  Grounding DINO [“generic”] & zero-shot & 10.3 & 37.8 & 44.1 & 17.7 & 51.6 & 80.0 & 28.3 \\ Grounding DINO+CoOp”  & self-training & 10.4 & 39.1 & 61.3 & 36.4 & 72.7 & 88.8 & 34.6 \\ Grounding DINO+CoCoOp”  & self-training & 7.6 & 34.1 & 58.1 & 33.9 & 68.3 & 86.1 & 24.6 \\
**DiPEx** & self-training & **10.5** & **40.8** & **63.2** & **39.2** & **74.3** & **89.8** & **35.9** \\   

Table 2: **Class-agnostic object detection on the MS-COCO dataset. [ ] indicate the prompt word for Grounding DINO. The prompting methods indicated with ‘*’ are adapted to the OD task.**AR@S by 7.7% compared to CoOp, indicating that expanded prompts better capture a range of object sizes. Additionally, the proposed DiPEx achieved the highest AP of 35.9%, demonstrating the superior quality of class-agnostic detection.

**Class-agnostic OD on LVIS.** To further validate the efficacy of DiPEx, we conducted extensive experiments on the challenging LVIS dataset, which includes thousands of classes with a long-tail distribution. As shown in Table 3, prompt tuning methods such as CoOp  and CoCoOp  outperform zero-shot Grounding DINO when using hand-crafted prompts (_e.g._, "items", "generic", "objects"). Additionally, CoCoOp surpasses multi-object discovery baselines like CutLER  and HASSOD , by 86.7% and 51.3% in AR\({}_{200}\), respectively. Notably, SAM , which was pre-trained on a vast of dataset containing millions of images and billions of masks, demonstrates strong zero-shot capabilities, surpassing all other baselines. In contrast, our proposed DiPEx outperforms SAM by 13.3% in AR\({}_{200}\) and 21.3% in AP after only four epochs of self-training, Furthermore, DiPEx exceeds CoOp by 20.1% in AR\({}_{200}\).

**Downstream OOD-OD on MS-COCO.** To evaluate the generalization of our proposed DiPEx in out-of-distribution object detection (OOD-OD), we compared its performance on both known and unknown classes against various baselines. As shown in Table 4, the zero-shot Grounding DINO uses known class names as prompts, supplemented with a simple "generic" prompt for unknowns, outperforms all other non-VLM methods (_e.g._, 25.5% higher AR\({}_{100}\) compared to CutLER ). This improvement stems from VLMs leveraging rich semantic knowledge from language models to better comprehend object information in images. DiPEx enhances this further by expanding text prompts in embedding space, enabling it to capture and differentiate objects of varying sizes and diverse semantics from learned classes. This approach delivers a significant performance gain, achieving a 38.3% increase in AR\({}_{100}\) and a 25.6% in AP increase over zero-shot predictions. Furthermore, the expanded prompts can be directly applied alongside various known class vocabularies to detect unknown objects, eliminating the need for retraining.

### Ablation Study and Model Analysis

We investigate the impact of various factors on prompting performance including the learnable prompt lengths, the number of expansion rounds \(L\), and angular coverage achieved across rounds. To facilitate model analysis, we present the distribution of prompt logit activation and visualization of detection results. Further ablation studies refers to Section A.3.

    &  &  \\   & AP & AP50 & AR\({}_{100}\) & AR@S & AR@M & AR@L & AP & AP@S & AP@M & AP@L \\  Selective Search  & – & – & 8.3 & 1.0 & 8.5 & 23.2 & 0.1 & 0.0 & 0.0 & 0.5 \\ MOST  & – & – & 5.3 & 0.1 & 1.3 & 22.5 & 0.4 & 0.1 & 0.4 & 1.2 \\ CutLER  & – & – & 34.5 & 15.8 & 41.5 & 62.7 & 5.7 & 2.3 & 6.9 & 13.7 \\ VOS  & 36.6 & 56.7 & 10.0 & 2.2 & 6.1 & 27.1 & 2.8 & 0.8 & 2.2 & 7.2 \\ PROB  & 28.2 & 43.8 & 13.2 & 1.9 & 11.2 & 40.3 & 0.9 & 0.6 & 0.9 & 2.1 \\ UnSniffer  & 35.8 & 55.8 & 20.6 & 11.8 & 19.9 & 34.8 & 2.9 & 1.5 & 3.1 & 5.3 \\ G-DINO [“generic”] & **46.3** & **59.7** & 43.3 & 18.0 & 52.1 & 82.6 & 12.5 & 6.9 & 17.8 & **25.7** \\ 
**DiPEx** & **46.3** & **59.7** & **59.9** & **35.8** & **72.9** & **89.7** & **15.7** & **9.7** & **21.8** & 25.2 \\   

Table 4: The downstream **out-of-distribution object detection (OOD-OD)** on the MS-COCO dataset, where the ground truth boxes contain both known and unknown classes.

   Method & AR\({}_{1}\) & AR\({}_{10}\) & AR\({}_{200}\) & AR@S & AR@M & AR@L & AP & AP@S & AP@M & AP@L \\  Selective Search  & 0.1 & 1.1 & 13.0 & 6.1 & 19.9 & 37.6 & 0.2 & 0.4 & 0.2 & 0.2 \\ G-DINO [“object”]  & 4.1 & 17.9 & 27.2 & 13.0 & 44.1 & 71.1 & 5.4 & 5.6 & 10.0 & 9.4 \\ G-DINO [“generic”]  & 3.8 & 16.5 & 20.2 & 6.5 & 34.5 & 67.7 & 9.0 & 4.1 & 17.4 & 30.7 \\ G-DINO [“items”]  & 4.0 & 17.8 & 28.0 & 13.9 & 45.3 & 70.7 & 11.6 & 6.3 & 19.6 & 32.0 \\ SAM  & – & – & 42.7 & 27.7 & 66.3 & 75.5 & 6.1 & – & – & – \\   \({}^{}\) CutLER  & 2.4 & 9.3 & 21.8 & 10.8 & 35.1 & 55.5 & 4.5 & 2.7 & 9.1 & 15.1 \\ \({}^{}\) HASSOD  & 0.2 & 10.6 & 26.9 & 15.6 & 42.2 & 56.9 & 4.9 & 2.8 & 7.9 & 12.2 \\ \({}^{}\) G-DINO + CoOp\({}^{*}\) & 4.2 & 19.1 & 40.3 & 23.6 & 63.5 & 83.5 & 14.0 & 8.3 & 23.7 & 32.3 \\ \({}^{}\) G-DINO + CoCoOp\({}^{*}\) & 4.2 & 19.2 & 40.7 & 24.1 & 63.8 & 84.1 & 13.6 & 8.1 & 22.4 & 30.1 \\    & **4.3** & **20.1** & **48.4** & **31.9** & **72.6** & **88.2** & **15.2** & **9.3** & **25.3** & **32.8** \\   

Table 3: **Class-agnostic object detection on the LVIS dataset. \({}^{}\) indicate the model is fine-tuned on the LVIS training set by self-training without box annotations.**

**Impact on Number of Prompts.** In Figure 4, we compare the impact of prompt length \(N\) for DiPEx against CoOp  & CoCoOp . Overall, DiPEx shows consistent improvement in performance with a greater number of prompts - not merely due to quantity, but rather because a larger set fosters greater diversification, enabling the model to capture more comprehensive semantics. In contrast, CoOp's  performance remains constant, while CoCoOp's  performance declines, suggesting that more prompts do not necessarily guarantee enhanced performance.

**Impact on Expansion Rounds and Angular Coverage.** To substantiate our hypothesis that a higher maximum angular coverage (MAC) correlates with a broader spectrum of vocabularies, we computed the MAC using Equation (4). The coverage results are visualized as heatmaps in Figure 5. At the initial stage of expansion (leftmost heatmap), we observe that the prompts are quite uniformly distributed, with a mean coverage of 47.56\({}^{}\), This suggests that the prompts are actively exploring the embedding space to capture diverse semantics. As the expansion progresses to the third round (middle heatmap), the MAC increases from 67.78\({}^{}\) to 75.70\({}^{}\). Specifically, row/col 7 (selected parent prompt) demonstrates the closest angular distances among the child prompts. This observation is crucial as it suggests that child prompts should not diverge excessively from the root semantics to maintain coherence. By the fourth round of expansion (rightmost heatmap), the pattern remains consistent with the third round. There is a reduced rate of change of MAC, achieving a maximum coverage of 75.95\({}^{}\)and a mean coverage of 11.51\({}^{}\)among the child prompts. This plateau in MAC indicates that maximum semantic expansion has been reached, suggesting that the model is approaching convergence and further expansion may not be necessary.

**The Distribution of Prompt Logit Activation.** We previously established prompt logit activation frequency as an uncertainty measure to guide parent prompt selection for splitting. To investigate the dynamics of expanding highly uncertain parent prompts, we visualize the activation statistics (_i.e._, the frequency of logit activations) of tokens within the 2nd and 3rd expansion rounds. As illustrated in Figure 6, the distribution of these logits exhibits a long-tailed pattern, suggesting substantial uncertainty and numerous semantic overlaps among the mined semantics. The figure on the right demonstrates that, following the expansion of highly activated prompts, the distribution of

Figure 4: Impact of the prompt length on the MS-COCO dataset. The average recall (AR) and precision (AP) are reported to compare the derived DiPEx against CoOp  and CoCoOp .

Figure 5: The heatmap visualization presents the **angular coverage** across all learned prompts through the 2nd, the 3rd, and the 4th round of training. The maximum angular coverage (MAC) monotonically increases from **67.7\({}^{}\)** in the 2nd round to **75.95\({}^{}\)** in the final round. The gradual reduction in rate of change in angular coverage towards the final round suggests that the model nearing convergence.

child prompts becomes more uniform, suggesting the discovery of fine-grained semantics. These observations support our choice of uncertainty measure and verify the validity of DiPEx, indicating that expanding based on highly uncertain parent prompts effectively alleviates semantic ambiguity.

**Qualitative Study.** In this section, we present visualized class-agnostic box predictions on images sampled from the MS-COCO dataset , as shown in Figure 7. The proposed DiPEx method demonstrates a superior ability to detect more bounding boxes than all baseline methods, particularly for _small objects_. For example, people in the distance (rows 1 and 3) and some bonsai (row 2) are missed by all baselines but successfully detected by DiPEx, showcasing its strong capability in localizing challenging small objects. For _large objects_, such as a motorcycle (row 3) and two people shaking hands in the near distance (row 1), DiPEx localizes them with significantly higher confidence compared to the zero-shot predictions of Grounding DINO using the prompt "generic". Additionally, DiPEx successfully identifies objects that are not annotated in the MS-COCO ground truth, such as plates (row 1), a pillowcase (row 2), and a frame on the wall (row 2). This highlights DiPEx's ability to identify a comprehensive set of class-agnostic objects, _even_ those missed in human annotations.

## 5 Conclusion and Limitations

This work introduces DiPEx, a novel self-supervised dispersing prompt expansion approach for class-agnostic object detection. We demonstrate through comprehensive experiments and analysis that DiPEx effectively detects a wide range of unseen objects of varying sizes and achieves broad vocabulary coverage. The progressively expanded prompt sets maintain good angular distances, promoting the formation of a semantic hierarchy and facilitating downstream detection tasks with a single inference pass. While the proposed DiPEx does not rely on box annotations, it requires self-training on the entire dataset for each round of prompt expansion, resulting in increased computational overhead. Additionally, some hyperparameters like temperature coefficients \(_{p}\), \(_{c}\) and learnable prompt length \(K\), may require manual tuning for optimal performance. Future research directions include exploring methods to learn hierarchical prompts at once rather than through expansion. Extensive benchmarking on additional downstream tasks, such as open-vocabulary and open-world detection, is necessary to comprehensively validate the proposed approach.

Figure 6: The distribution of **logit activation** of the learned prompts in the 2nd round (_left_) and the 3rd round (_right_). The prompt of the highest activation frequency is identified for further expansion.

Figure 7: Visualization of the class-agnostic detection performance by baselines and the proposed DiPEx on MS-COCO . More visualizations are provided in Appendix (Figures 9 and 10).