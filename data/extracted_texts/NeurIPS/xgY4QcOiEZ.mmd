# Learning a Neuron by a Shallow ReLU Network:

Dynamics and Implicit Bias for Correlated Inputs

Dmitry Chistikov

University of Warwick

d.chistikov@warwick.ac.uk

&Matthias Englert

University of Warwick

m.englert@warwick.ac.uk

&Ranko Lazic

University of Warwick

r.s.lazic@warwick.ac.uk

Equal contribution.

###### Abstract

We prove that, for the fundamental regression task of learning a single neuron, training a one-hidden layer ReLU network of any width by gradient flow from a small initialisation converges to zero loss and is implicitly biased to minimise the rank of network parameters. By assuming that the training points are correlated with the teacher neuron, we complement previous work that considered orthogonal datasets. Our results are based on a detailed non-asymptotic analysis of the dynamics of each hidden neuron throughout the training. We also show and characterise a surprising distinction in this setting between interpolator networks of minimal rank and those of minimal Euclidean norm. Finally we perform a range of numerical experiments, which corroborate our theoretical findings.

## 1 Introduction

One of the grand challenges for machine learning research is to understand how overparameterised neural networks are able to fit perfectly the training examples and simultaneously to generalise well to unseen data (Zhang, Bengio, Hardt, Recht, and Vinyals, 2021). The double-descent phenomenon (Belkin, Hsu, Ma, and Mandal, 2019), where increasing the neural network capacity beyond the interpolation threshold can eventually reduce the test loss much further than could be achieved around the underparameterised "sweet spot", is a mystery from the standpoint of classical machine learning theory. This has been observed to happen even for training without explicit regularisers.

Implicit bias of gradient-based algorithms.A key hypothesis towards explaining the double-descent phenomenon is that the gradient-based algorithms that are used for training are _implicitly biased_ (or _implicitly regularised_) (Neyshabur, Bhojanapalli, McAllester, and Srebro, 2017) to converge to solutions that in addition to fitting the training examples have certain properties which cause them to generalise well. It has attracted much attention in recent years from the research community, which has made substantial progress in uncovering implicit biases of training algorithms in many important settings (Vardi, 2023). For example, for classification tasks, and for homogeneous networks (which is a wide class that includes ReLU networks provided they contain neither biases at levels deeper than the first nor residual connections), Lyu and Li (2020) and Ji and Telgarsky (2020) established that gradient flow is biased towards maximising the classification margin in parameter space, in the sense that once the training loss gets sufficiently small, the direction of the parameters subsequently converges to a Karush-Kuhn-Tucker point of the margin maximisation problem.

Insights gained in this foundational research direction have not only shed light on overparameterised generalisation, but have been applied to tackle other central problems, such as the susceptibility of networks trained by gradient-based algorithms to adversarial examples (Vardi, Yehudai, and Shamir, 2022) and the possibility of extracting training data from network parameters (Haim, Vardi, Yehudai, Shamir, and Irani, 2022).

Regression tasks and initialisation scale.Showing the implicit bias for regression tasks, where the loss function is commonly mean square, has turned out to be more challenging than for classification tasks, where loss functions typically have exponential tails. A major difference is that, whereas most of the results for classification do not depend on how the network parameters are initialised, the scale of the initialisation has been observed to affect decisively the implicit bias of gradient-based algorithms for regression (Woodworth, Gunasekar, Lee, Moroshko, Savarese, Golan, Soudry, and Srebro, 2020). When it is large so that the training follows the _lazy regime_, we tend to have fast convergence to a global minimum of the loss, however without an implicit bias towards sparsity and with limited generalisation (Jacot, Ged, Simsek, Hongler, and Gabriel, 2021). The focus, albeit at the price of uncertain convergence and lengthier training, has therefore been on the _rich regime_ where the initialisation scale is small.

Considerable advances have been achieved for linear networks. For example, Azulay, Moroshko, Nacson, Woodworth, Srebro, Globerson, and Soudry (2021) and Yun, Krishnan, and Mobahi (2021) proved that gradient flow is biased to minimise the Euclidean norm of the predictor for one-hidden layer linear networks with infinitesimally small initialisation, and that the same holds also for deeper linear networks under an additional assumption on their initialisation. A related extensive line of work is on implicit bias of gradient-based algorithms for matrix factorisation and reconstruction, which has been a fruitful test-bed for regression using multi-layer networks. For example, Gunasekar, Woodworth, Bhojanapalli, Neyshabur, and Srebro (2017) proved that, under a commutativity restriction and starting from a small initialisation, gradient flow is biased to minimise the nuclear norm of the solution matrix; they also conjectured that the restriction can be dropped, which after a number of subsequent works was refuted by Li, Luo, and Lyu (2021), leading to a detailed analysis of both underparameterised and overparameterised regimes by Jin, Li, Lyu, Du, and Lee (2023).

For non-linear networks, such as those with the popular ReLU activation, progress has been difficult. Indeed, Vardi and Shamir (2021) showed that precisely characterising the implicit bias via a non-trivial regularisation function is impossible already for single-neuron one-hidden layer ReLU networks, and Timor, Vardi, and Shamir (2023) showed that gradient flow is not biased towards low-rank parameter matrices for multiple-output ReLU networks already with one hidden layer and small training datasets.

ReLU networks and training dynamics.We suggest that, in order to further substantially our knowledge of convergence, implicit bias, and generalisation for regression tasks using non-linear networks, we need to understand more thoroughly the dynamics throughout the gradient-based training. This is because of the observed strong influence that initialisation has on solutions, but is challenging due to the highly non-convex optimisation landscape. To this end, evidence and intuition were provided by Maennel, Bousquet, and Gelly (2018); Li et al. (2021), and Jacot et al. (2021), who conjectured that, from sufficiently small initialisations, after an initial phase where the neurons get aligned to a number of directions that depend only on the dataset, training causes the parameters to pass close to a sequence of saddle points, during which their rank increases gradually but stays low.

The first comprehensive analysis in this vein was accomplished by Boursier, Pillaud-Vivien, and Flammarion (2022), who focused on orthogonal datasets (which are therefore of cardinality less than or equal to the input dimension), and established that, for one-hidden layer ReLU networks, gradient flow from an infinitesimal initialisation converges to zero loss and is implicitly biased to minimise the Euclidean norm of the network parameters. They also showed that, per sign class of the training labels (positive or negative), minimising the Euclidean norm of the interpolator networks coincides with minimising their rank.

Our contributions.We tackle the main challenge posed by Boursier et al. (2022), namely handling datasets that are not orthogonal. A major obstacle to doing so is that, whereas the analysis of the training dynamics in the orthogonal case made extensive use of an almost complete separation between a turning phase and a growth phase for all hidden neurons, non-orthogonal datasets cause considerably more complex dynamics in which hidden neurons follow training trajectories that simultaneously evolve their directions and norms (Boursier et al., 2022, Appendix A).

To analyse this involved dynamics in a reasonably clean setting, we consider the training of one-hidden layer ReLU networks by gradient flow from a small balanced initialisation on datasets that are labelled by a teacher ReLU neuron with which all the training points are correlated. More precisely, we assume that the angles between the training points and the teacher neuron are less than \(/4\), which implies that all angles between training points are less than \(/2\). The latter restriction has featured per label class in many works in the literature (such as by Phuong and Lampert (2021) and Wang and Pilanci (2022)), and the former is satisfied for example if the training points can be obtained by summing the teacher neuron \(^{*}\) with arbitrary vectors of length less than \(\|^{*}\|/\). All our other assumptions are very mild, either satisfied with probability exponentially close to \(1\) by any standard random initialisation, or excluding corner cases of Lebesgue measure zero.

Our contributions can be summarised as follows.

* We provide a detailed **non-asymptotic analysis** of the dynamics of each hidden neuron throughout the training, and show that it applies whenever the initialisation scale \(\) is below a **precise bound** which is polynomial in the network width \(m\) and exponential in the training dataset cardinality \(n\). Moreover, our analysis applies for any input dimension \(d>1\), for any \(n d\) (otherwise exact learning of the teacher neuron may not be possible), for any \(m\), and without assuming any specific random distribution for the initialisation. In particular, we demonstrate that the role of the overparameterisation in this setting is to ensure that initially at least one hidden neuron with a positive last-layer weight has in its active half-space at least one training point.
* We show that, during a first phase of the training, all active hidden neurons with a positive last-layer weight **get aligned** to a single direction which is positively correlated with all training points, whereas all active hidden neurons with a negative last-layer weight get turned away from all training points so that they deactivate. In contrast to the orthogonal dataset case where the sets of training points that are in the active half-spaces of the neurons are essentially constant during the training, in our correlated setting this first phase in general consists, for each neuron, of a different **sequence of stages** during which the cardinality of the set of training points in its active half-space gradually increases or decreases, respectively.
* We show that, during the rest of the training, the bundle of aligned hidden neurons with their last-layer weights, formed by the end of the first phase, grows and turns as it travels from near the origin to near the teacher neuron, and **does not separate**. To establish the latter property, which is the most involved part of this work, we identify a set in predictor space that depends only on \(\) and the training dataset, and prove: first, that the trajectory of the bundle **stays inside the set**; and second, that this implies that the directional gradients of the individual neurons are such that the angles between them are non-increasing.
* We prove that, after the training departs from the initial saddle, which takes time logarithmic in \(\) and linear in \(d\), the gradient satisfies a Polyak-Lojasiewicz inequality and consequently the loss **converges to zero exponentially fast**.
* We prove that, although for any fixed \(\) the angles in the bundle of active hidden neurons do not in general converge to zero as the training time tends to infinity, if we let \(\) tend to zero then the networks to which the training converges have a limit: a network of rank \(1\), in which all non-zero hidden neurons are positive scalings of the teacher neuron and have positive last-layer weights. This establishes that gradient flow from an infinitesimal initialisation is **implicitly biased** to select interpolator networks of **minimal rank**. Note also that the limit network is identical in predictor space to the teacher neuron.
* We show that, surprisingly, among all networks with zero loss, there may exist some whose Euclidean norm is smaller than that of any network of rank \(1\). Moreover, we prove that this is the case if and only if a certain condition on angles determined by the training dataset is satisfied. This result might be seen as **refuting the conjecture** of Boursier et al. (2022, section 3.2) that the implicit bias to minimise Euclidean parameter norm holds beyond the orthogonal setting, and adding some weight to the hypothesis of Razin and Cohen (2020). The counterexample networks in our proof have rank \(2\) and make essential use of the ReLU non-linearity.
* We perform numerical experiments that indicate that the training dynamics and the implicit bias we theoretically established occur in practical settings in which some of our assumptions are relaxed. In particular, gradient flow is replaced by gradient descent with a realistic learning rate,the initialisation scales are small but not nearly as small as in the theory, and the angles between the teacher neuron and the training points are distributed around \(/4\).

We further discuss related work, prove all theoretical results, and provide additional material on our experiments, in the appendix.

## 2 Preliminaries

Notation.We write: [\(n\)] for the set \(\{1,,n\}\), \(\|\|\) for the Euclidean length of a vector \(\), \(}/\|\|\) for the normalised vector, \((,^{})(}^{} }^{})\) for the angle between \(\) and \(^{}\), and \(\{_{1},,_{n}\}\{_{i=1}^{n} _{i}_{i}_{1},,_{n} 0\}\) for the cone generated by vectors \(_{1}\),..., \(_{n}\).

One-hidden layer ReLU network.For an input \(^{d}\), the output of the network is

\[h_{}()_{j=1}^{m}a_{j}\,(_{j}^{ })\,\]

where \(m\) is the width, the parameters \(=(,)^{m}^{m d}\) consist of last-layer weights \(=[a_{1},,a_{m}]\) and hidden-layer weights \(^{}=[_{1},,_{m}]\), and \((u)\{u,0\}\) is the ReLU function.

Balanced initialisation.For all \(j[m]\) let

\[_{j}^{0} \,_{j} a_{j}^{0}  s_{j}\|_{j}^{0}\|\]

where \(>0\) is the initialisation scale, \(_{j}^{d}\{\}\), and \(s_{j}\{ 1\}\).

A precise upper bound on \(\) will be stated in Assumption 2.

We regard the initial unscaled hidden-layer weights \(_{j}\) and last-layer signs \(s_{j}\) as given, without assuming any specific random distributions for them. For example, we might have that each \(_{j}\) consists of \(d\) independent centred Gaussians with variance \(\) and each \(s_{j}\) is uniform over \(\{ 1\}\).

We consider only initialisations for which the layers are balanced, i.e. \(|a_{j}^{0}|=\|_{j}^{0}\|\) for all \(j[m]\). Since more generally each difference \((a_{j}^{t})^{2}-\|_{j}^{t}\|^{2}\) is constant throughout training (Du et al., 2018, Theorem 2.1) and we focus on small initialisation scales that tend to zero, this restriction (which is also present in Boursier et al. (2022)) is minor but simplifies our analysis.

Neuron-labelled correlated inputs.The teacher neuron \(^{*}^{d}\) and the training dataset \(\{(_{i},y_{i})\}_{i=1}^{n}(^{d}\{\}) \) are such that for all \(i\) we have

\[y_{i} =(^{*}_{i}) (^{*},_{i})</4\.\]

In particular, since the angles between \(^{*}\) and the training points \(_{i}\) are acute, each label \(y_{i}\) is positive.

To apply our results to a network with biases in the hidden layer and to a teacher neuron with a bias, one can work in dimension \(d+1\) and extend the training points to \([_{i}\\ 1]\).

Mean square loss gradient flow.For the regression task of learning the teacher neuron by the one-hidden layer ReLU network, we use the standard mean square empirical loss

\[L()^{n}(y_{i}-h_{}(_{i}))^{2}}\.\]

Our theoretical analysis concentrates on training by gradient flow, which from an initialisation as above evolves the network parameters by descending along the gradient of the loss by infinitesimal steps in continuous time (Li et al., 2019). Formally, we consider any parameter trajectory \(^{t}[0,)^{m}^{m d}\) that is absolutely continuous on every compact subinterval, and that satisfies the differential inclusion

\[^{t}/t- L(^{t}) \,\]

where \( L\) denotes the Clarke (1975) subdifferential of the loss function (which is locally Lipschitz).

[MISSING_PAGE_FAIL:5]

[MISSING_PAGE_FAIL:6]

sign is negative then it turns to remove from its active half-space all training points that were initially inside. Moreover, those training points cross the activation boundary in the same order as they cross the half-space boundary of the corresponding yardstick trajectory \(_{j}^{t}\), and at approximately the same times (cf. Proposition 2).

**Lemma 3**.: _For all \(j J_{+} J_{-}\) there exist unique \(0=t_{j}^{0}<t_{j}^{1}<<t_{j}^{n_{j}}\) such that for all \([n_{j}]\):_

1. \(I_{s_{j}}(_{j}^{t})=I_{s_{j}}(_{j})\{i_{j}^{1},,i_{j}^{ -1}\}\) _for all_ \(t(t_{j}^{-1},t_{j}^{t})\)_;_
2. \(I_{0}(_{j}^{t})=\) _for all_ \(t(t_{j}^{-1},t_{j}^{})\)_, and_ \(I_{0}_{j}^{t_{j}^{}}=\{i_{j}^{}\}\)_;_
3. \(|_{j}^{}-t_{j}^{}|^{1-1+} }\)_._

The preceding lemma is proved by establishing, for this first phase of the training, non-asymptotic upper bounds on the Euclidean norms of the hidden neurons and hence on the absolute values of the network outputs, and inductively over the stage index \(\), on the distances between the unit-sphere normalisations of \(_{j}^{t}\) and \(_{j}^{t}\). Based on that analysis, we then obtain that each negative-sign hidden neuron does not grow from its initial length and deactivates by time \(T_{0}_{j J_{+} J_{-}}_{j}^{n_{j}}+1\).

**Lemma 4**.: _For all \(j J_{-}\) we have:_

\[\|_{j}^{T_{0}}\|\|_{j}\| _{j}^{t}=_{j}^{T_{0}}$}\.\]

We also obtain that, up to a later time \(T_{1}(1/)/\|_{[n]}\|\), each positive-sign hidden neuron: grows but keeps its length below \(2\|_{j}\|^{1-}\), continues to align to the vector \(_{[n]}\) up to a cosine of at least \(1-^{}\), and maintains bounded by \(^{1-3}\) the difference between the logarithm of its length divided by the initialisation scale and the logarithm of the corresponding yardstick vector length.

**Lemma 5**.: _For all \(j J_{+}\) we have:_

\[\|_{j}^{T_{1}}\|<2\|_{j}\|^{1-}}_{j}^{T_{1}^{}}}_{[n]} 1-^{ }|\|_{j}^{T_{1}}\|-\|_{j}^{T_{1}}/ \|\|^{1-3}\.\]

## 5 Second phase: growth and convergence

We next analyse the gradient flow subsequent to the deactivation of the negative-sign hidden neurons by time \(T_{0}\) and the alignment of the positive-sign ones up to time \(T_{1}\), and establish that the loss converges to zero at a rate which is exponential and does not depend on the initialisation scale \(\).

**Theorem 6**.: _Under Assumptions 1 and 2, there exists a time \(T_{2}<(1/)(4+)d^{2}/^{6}\) such that for all \(t 0\) we have \(L(^{T_{2}+t})<0.5\,^{2}\,^{-t 0.4\,^{4}/ ^{2}}\)._

In particular, for \(=1/4\) and \(=(m\,n^{n})^{9^{2}/^{3}}^{-3/}\) (cf. Assumption 2), the first bound in Theorem 6 becomes \(T_{2}<( m+n n)\,d 17 27\,^{4}/^{9}\).

The proof of Theorem 6 is in large part geometric, with a key role played by a set \(_{1}_{d}\) in predictor space, whose constituent subsets are defined as

\[_{}\{=_{k=1}^{d}_{k}_{k}\ |\ \ _{1 k<}_{k}\ \ _{}\ _{t k<k^{} d}(_{k,k^{}}^{} _{k,k^{}}^{})\ \ \}\,\]

where the individual constraints are as follows (here \(_{0}\) so that e.g. \(}{2_{0}}=0\)):

\[_{k} \ 1<}{_{k}^{*}}_{}\ }{2_{-1}}<}{_{}^{*}} 1 _{k,k^{}}^{} \ }}{2_{k}}}{_{k}^{*}} <}}{_{k^{}}^{*}}\] \[ \ }^{}^{}( ^{*}-)}>^{/3} _{k,k^{}}^{} \ }}{_{k^{}}^{*}}<1-(1-}{_{k}^{ *}})^{+}}{2_{k}}}\.\]

Thus \(\) is connected, open, and constrained by \(\) to be within the ellipsoid \(^{}^{}(^{*}-)=0\) which is centred at \(^{*}}{2}\), with the remaining constraints slicing off further regions by straight or curved boundary surfaces.

In the most complex component of this work, we show that, for all \(t T_{1}\), the trajectory of the sum \(^{t}_{j J_{+}}a_{j}^{t}_{j}^{t}\) of the active hidden neurons weighted by the last layer stays inside \(\), and the cosines of the angles between the neurons remain above \(1-4^{}\). This involves proving that each face of the boundary of \(\) is repelling for the training dynamics when approached from the inside; we remark that, although that is in general false for the entire boundary of the constraint \(\), it is in particular true for its remainder after the slicing off by the other constraints. We also show that all points in \(\) are positively correlated with all training points, which together with the preceding facts implies that, during this second phase of the training, the network behaves approximately like a linear one-hidden layer one-neuron network. Then, as the cornerstone of the rest of the proof, we show that, for all \(t T_{2}\), the gradient of the loss satisfies a Polyak-Lojasiewicz inequality \(\| L(^{t})\|^{2}>\|_{_{1}}\|} {5_{1}}L(^{t})\). Here \(T_{2}\{t T_{1}_{1}^{t}/_{1}^{*} 1/2\}\) is a time by which the network has departed from the initial saddle, more precisely when the first coordinate \(_{1}^{t}\) of the bundle vector \(^{t}\) with respect to the basis consisting of the eigenvectors of the matrix \(^{}\) crosses the half-way threshold to the first coordinate \(_{1}^{*}\) of the teacher neuron.

The interior of the ellipsoid in the constraint \(\) actually consists of all vectors that have an acute angle with the derivative of the training dynamics in predictor space, and the "padding" of \(^{/3}\) is present because the derivative of the bundle vector \(^{t}\) is "noisy" due to the latter being made up of the approximately aligned neurons. The remaining constraints delimit the subsets \(_{1}\),..., \(_{d}\) of the set \(\), through which the bundle vector \(_{t}\) passes in that order, with each unique "handover" from \(_{}\) to \(_{+1}\) happening exactly when the corresponding coordinate \(_{}^{t}\) exceeds its target \(_{}^{*}\). The non-linearity of the constraints \(_{k,k^{}}^{}\) is needed to ensure the repelling for the training dynamics.

## 6 Implicit bias of gradient flow

Let us denote the set of all balanced networks by

\[\{(,)^{m}^{m d} \;\; jm\;|a_{j}|=\|_{j}\|\}\]

and the subset in which all non-zero hidden neurons are positive scalings of \(^{*}\), have positive last-layer weights, and have lengths whose squares sum up to \(\|^{*}\|=1\), by

\[_{^{*}}\{(,)\;\;_{j=1}^{m} \|_{j}\|^{2}=1\;\; jm\;_{j} \;\;(}_{j}=^{*}\;\;a_{j}>0)\}\;.\]

Our main result establishes that, as the initialisation scale \(\) tends to zero, the networks with zero loss to which the gradient flow converges tend to a network in \(_{^{*}}\). The explicit subscripts indicate the dependence on \(\) of the parameter vectors. The proof builds on the preceding results and involves a careful control of accumulations of approximation errors over lengthy time intervals.

**Theorem 7**.: _Under Assumptions 1 and 2, \(L_{t}_{}^{t}=0\) and \(_{ 0^{+}}_{t}_{}^{t}_{^{*}}\)._

## 7 Interpolators with minimum norm

To compare the set \(_{^{*}}\) of balanced rank-\(1\) interpolator networks with the set of all minimum-norm interpolator networks, in this section we focus on training datasets of cardinality \(d\), we assume the network width is greater than \(1\) (otherwise the rank is necessarily \(1\)), and we exclude the threshold case of Lebesgue measure zero where \(=0\). The latter measurement of the training dataset is defined below in terms of angles between the teacher neuron and vectors in any two cones generated by different generators of the dual of the cone of all training points.

Let \([_{1},,_{d}]^{}^{-1}\) and

\[\{ K d\\ .\{_{k} k K\}\\ \{_{k} k K\} \}\;..\]

**Assumption 3**.: \(n=d\)_, \(m>1\), and \( 0\)._

We obtain that, surprisingly, \(_{^{*}}\) equals the set of all interpolators with minimum Euclidean norm if \(<0\), but otherwise they are disjoint.

**Theorem 8**.: _Under Assumptions 1 and 3:_

1. _if_ \(<0\) _then_ \(_{^{*}}\) _is the set of all global minimisers of_ \(\|\|^{2}\) _subject to_ \(L()=0\)_;_
2. _if_ \(>0\) _then no point in_ \(_{^{*}}\) _is a global minimiser of_ \(\|\|^{2}\) _subject to_ \(L()=0\)_._

For each of the two cases, we provide a family of example datasets in the appendix. We remark that a sufficient condition for \(<0\) to hold is that the inner product of any two distinct rows \(_{k}\) of the inverse of the dataset matrix \(\) is non-positive, i.e. that the inverse of the Gram matrix of the dataset (in our setting this Gram matrix is positive) is a Z-matrix (cf. e.g. Fiedler and Ptak (1962)). Also, if the training points were orthogonal then all the \((,)\) terms in the definition of \(\) would be zero and consequently we would have \(<0\); this is consistent with the result that, per sign class of the training labels in the orthogonal setting, minimising the Euclidean norm of interpolators coincides with minimising their rank (Boursier et al., 2022, Appendix C).

## 8 Experiments

We consider two schemes for generating the training dataset, where \(^{d-1}\) is the unit sphere in \(^{d}\).

**Centred:** We sample \(\) from \((^{d-1})\), then sample \(_{1},,_{d}\) from \((,_{d})\) where \(=1\), and finally set \(^{*}=\). This distribution has the property that, in high dimensions, the angles between the teacher neuron \(^{*}\) and the training points \(_{i}\) concentrate around \(/4\). We exclude rare cases where some of these angles exceed \(/2\).

**Uncentred:** This is the same, except that we use \(=-1\), sample one extra point \(_{0}\), and finally set \(^{*}=}_{0}\). Here the angles between \(^{*}\) and \(_{i}\) also concentrate around \(/4\) in high dimensions, but the expected distance between \(^{*}\) and \(\) is \(\).

For each of the two dataset schemes, we train a one-hidden layer ReLU network of width \(m=200\) by gradient descent with learning rate \(0.01\), from a balanced initialisation such that \(_{j}^{}(,_{d})\) and \(s_{j}^{}\{ 1\}\), and for a range of initialisation scales \(\) and input dimensions \(d\).2

We present in Figure 1 some results from considering initialisation scales \(=4^{2},4^{1},,4^{-12},4^{-13}\) and input dimensions \(d=4,16,64,256,1024\), where we train until the number of iterations

Figure 1: Dependence of the maximum angle between active hidden neurons on the initialisation scale \(\), for two generation schemes of the training dataset and a range of input dimensions, at the end of the training. Both axes are logarithmic, and each point plotted shows the median over five trials.

reaches \(2 10^{7}\) or the loss drops below \(10^{-9}\). The plots are in line with Theorem7, showing how the maximum angle between active hidden neurons at the end of the training decreases with \(\).

Figure2 on the left illustrates the exponentially fast convergence of the training loss (cf. Theorem6), and on the right how the implicit bias can result in good generalisation. The test loss is computed over an input distribution which is different from that of the training points, namely we sample \(64\) test inputs from \((,_{d})\). These plots are for initialisation scales \(=4^{-2},4^{-3},,4^{-7},4^{-8}\).

## 9 Conclusion

We provided a detailed analysis of the dynamics of training a shallow ReLU network by gradient flow from a small initialisation for learning a single neuron which is correlated with the training points, establishing convergence to zero loss and implicit bias to rank minimisation in parameter space. We believe that in particular the geometric insights we obtained in order to deal with the complexities of the multi-stage alignment of hidden neurons followed by the simultaneous evolution of their norms and directions, will be useful to the community in the ongoing quest to understand implicit bias of gradient-based algorithms for regression tasks using non-linear networks.

A major direction for future work is to bridge the gap between, on one hand, our assumption that the angles between the teacher neuron and the training points are less than \(/4\), and the other, the assumption of Boursier et al. (2022) that the training points are orthogonal, while keeping a fine granularity of description. We expect this to be difficult because it seems to require handling bundles of approximately aligned neurons which may have changing sets of training points in their active half-spaces and which may separate during the training. However, it should be straightforward to extend our results to orthogonally separable datasets and two teacher ReLU neurons, where each of the latter has an arbitrary sign, labels one of the two classes of training points, and has angles less than \(/4\) with them; the gradient flow would then pass close to a second saddle point, where the labels of one of the classes have been nearly fitted but the hidden neurons that will fit the labels of the other class are still small. We report on related numerical experiments in the appendix.

We also obtained a condition on the dataset that determines whether rank minimisation and Euclidean norm minimisation for interpolator networks coincide or are distinct. Although this dichotomy remains true if the \(/4\) correlation bound is relaxed to \(/2\), the implicit bias of gradient flow in that extended setting is an open question. Other directions for future work include considering multi-neuron teacher networks, student networks with more than one hidden layer, further non-linear activation functions, and gradient descent instead of gradient flow; also refining the bounds on the initialisation scale and the convergence time.

Figure 2: Evolution of the training loss, and of an outside distribution test loss, during training for an example centred training dataset in dimension \(16\) and width \(200\). The horizontal axes, logarithmic for the training loss and linear for the test loss, show iterations. The vertical axes are logarithmic.