# MatrixNet: Learning over symmetry groups using learned group representations

Lucas Laird

Khoury College of Computer Sciences

Northeastern University

Boston, MA 02115

laird.l@northeastern.edu &Circe Hsu

Department of Mathematics

Northeastern University

Boston, MA 02115

hsu.circe@northeastern.edu &Asilata Bapat

Mathematical Sciences Institute

Australian National University

Canberra, Australia

asilata.bapat@anu.edu.au &Robin Walters

Khoury College of Computer Sciences

Northeastern University

Boston, MA 02115

r.walters@northeastern.edu

###### Abstract

Group theory has been used in machine learning to provide a theoretically grounded approach for incorporating known symmetry transformations in tasks from robotics to protein modeling. In these applications, equivariant neural networks use known symmetry groups with predefined representations to learn over geometric input data. We propose MatrixNet, a neural network architecture that learns matrix representations of group element inputs instead of using predefined representations. MatrixNet achieves higher sample efficiency and generalization over several standard baselines in prediction tasks over the several finite groups and the Artin braid group. We also show that MatrixNet respects group relations allowing generalization to group elements of greater word length than in the training set. Our code is available at https://github.com/lucas-laird/MatrixNet.

## 1 Introduction

The choice of representation for input features is a key design aspect for deep learning. While a wide variety of data types are considered in learning tasks, for example, sequences, sets, images, time series, and graphs, neural network architectures admit only tensors as input. Various methods exist for mapping different input types to tensors such as one-hot embeddings, discretization of continuous signals, learned token embeddings of image patches or words , adjacency matrices , positional encodings , or spectral embeddings .

In this paper we consider the question of what feature representations to use for learning tasks with inputs coming from a symmetry group. There are many examples of tasks defined over symmetry groups, such as policy learning in robotics , reinforcement learning , pose estimation in computer vision , sampling states of quantum systems , inference over orderings of a set , and group-theoretic invariants in pure mathematics. Past work has typically employed fixed representations chosen from among the known representation theory of the group. Representation theory is the branch of mathematics concerned with classifying the set of representations of a group \(G\) which, in this context, refers to homomorphic realizations of a group in terms of \(n n\) matrices. For groups with well understood representation theory, for example, the symmetric group \(S_{n}\) or \((n)\), this provides a ready set of embeddings for converting group elements into tensors for use in downstreammodels. Trial and error or topological analysis has shown that the choice of group representation is critical for learning [10; 11; 12; 13].1

Instead of using predefined group element representations, we propose to learn feature representations for each group element using a learned group representation. That is, we learn to map group elements to invertible \(n n\) matrices which respect the symmetry group structure. There are several advantages to this strategy. First, unlike predefined group representations, learned representations allow the model to adapt to the given task and capture relevant information for the learning task. Second, learned representations provide reasonable correlations between the learned features for different group elements since they incorporate algebraic structure into the model. This structure is encouraged using free group generators and group relation regularization. Third, relative to learned vector embeddings, learned matrix representations are very parameter efficient for encoding different group elements, reducing the problem to learning only representations of group generators; using sequence encoding, our method is able to generalize to combinatorially large or even infinite groups. Fourth, the learned representation admits analysis in terms of the irreducible subspaces of the generators, giving insight into the model's understanding of the task.

We integrate our learned group representation into a specialized architecture, MatrixNet, adapted for learning mappings related to open problems in representation theory. We compare against several more general baselines on order prediction over finite groups and estimating sizes of categorical objects under an action of the Artin braid group. Through our experiments we observe that our approach achieves higher sample efficiency and performance than the baselines. We additionally show that MatrixNet's constraints allow for it to generalize to unseen group elements automatically without the need for additional data augmentation.

Our contributions include:

* Formulation of the problem of learning over groups as a sequence learning problem in terms of group generators and invariance to group axioms and relations,
* The MatrixNet architecture, which utilizes learned group representations for flexible and efficient feature learning over symmetry groups,
* The matrix block method for constraining the network to respect the group axioms and an additional loss term for learning group relations,
* Empirical validation showing MatrixNet outperforms baseline models on a task over the symmetric group and a task over the braid group related to open problems in mathematics.

## 2 Related Works

Mathematically Constrained NetworksMany deep learning methods incorporate mathematical constraints to better model underlying data. For instance, the application of graph neural networks [3; 14] to problems with an underlying graph structure has led to state of the art performance in many domains. Deep learning models have also been designed for tasks with known mathematical formulations by parameterizing components of algorithmic solutions as neural networks and leveraging their structures for more efficient optimization [15; 16]. More broadly the field of geometric deep learning  advocates for building neural networks which reflect the geometric structure of data in their latent features. When symmetries are present in data, group equivariant neural networks [18; 19; 20; 21] can enable improved generalization and data efficiency by incorporating known symmetries into the model architecture using the representation theory of groups. Our method also utilizes group representations, but unlike equivariant neural networks, we use learned as opposed to fixed representations. We also focus on modeling functions defined on the group as opposed to between representations of the group.

Learning Structured RepresentationsInstead of using predefined representations as inputs, many methods seek to learn mathematically structured representations from data. This idea has been applied in physics [22; 23], robotics , world models , self-supervised learning [26; 27], and unsupervised disentanglement . Park et al. , for example, use a combination of learned symmetry and equivariant constraints to map images to group elements or vectors in group representations. Similar techniques are used in symmetry discovery where the underlying group symmetry is not known a priori [30; 31; 32]. Yang et al.  use a generative model to learn latent representations with a linear group action in order to find unknown group symmetries in data. Here, in contrast, we start with a known group and learn a matrix representation. Hajij et al.  propose algebraically-informed neural networks which learn a non-linear group action from a group presentation. We also learn a group action but consider linear group actions and the learning signal comes not only from the group presentation but also a downstream task.

Deep Learning for MathRecent work has shown deep learning can be useful for providing examples, insight, and proofs related to open problems in mathematics. One approach is the application of language models to mathematics , which has the benefit of flexibility in how the model is prompted yet is difficult to interpret and prone to errors. Meanwhile significant work has been done in the area of symbolic regression and automated theorem proving . Other work applies deep learning to the direct modeling of partial differential equations [36; 37]. These methods can perform exceptionally well on real-world data , but suffer when trying to interpret predictions made for the purposes of mathematical research. Another avenue involves training graph neural networks on mathematical data such as group or knot invariants and analyzing the learned representations to see which features are significant as a way to provide intuition to mathematicians . Our method also uses structured inputs and learned features, but uses a sequence model and learned group representation instead of a graph neural network with learned node attributes.

## 3 Background

### Group Theory

Groups encode systems of symmetry and have been used in machine learning to build invariance into neural networks to various transformations . Formally, a **group**\(G\) is a set equipped with an associative binary operation \( G G G\) which satisfies two axioms: (1) there exists an identity element \(1 G\), such that \(g 1=1 g=g\), (2) for each \(g G\), there exists an inverse \(g^{-1} G\) such that \(g g^{-1}=g^{-1} g=1\). Examples of groups include finite groups such as the dihedral group \(D_{4}\) which gives the symmetries of the square, \((3)\), the continuous group of 3D rotations, or \((,+)\) the infinite discrete group of integer shifts.

Since groups may be combinatorially large or infinite, it is essential to encode their elements and compositional structure in a succinct way. For many discrete groups, generators and relations provide such a description. A set of elements \(S=\{g_{1},...,g_{n}\} G\) are called **generators** if every element of \(G\) can be written as a composition of \(g_{1},g_{1}^{-1},...,g_{n},g_{n}^{-1}\). In general, each element of \(G\) may be written in many different ways in terms of the generators; this non-uniqueness is encoded using a set of relations. A set \(R=\{r_{1},,r_{m}\}\) of words in the generators \(S\) are **relations** for \(G\) if each word \(r_{i}\) is equal to the identity in \(G\) and if \(R\) generates the entire set of words equal to identity under composition and conjugation. The generators and relations of a group taken together are called a **presentation** and denoted \(G= g_{1},...,g_{n} r_{1},...,r_{m}\). For example \(D_{4}= r,f r^{4}=f^{2}=frfr\). Due to relations, group elements do not have a unique word representation. For example \(frf=r^{3}=r^{7}\) all represent the same group element. By convention relations are sometimes stated as equalities instead of single group elements, for example \(frf=r^{3}\). The **free group**\(F_{S}= g_{1},,g_{n}\) is defined to have no relations except for those coming from the two group axioms above.

An important notion for our discussion is the **order** of an element. If \(g G\) then the order of \(g\), denoted \(|g|\), is the smallest \(k\) such that \(g^{k}=e\). (For non-finite groups, \(k\) may be infinity). The order of the group \(|G|\) is simply the number of elements in the group. For any \(g\), Lagrange's theorem implies that \(|g|\) is a divisor of \(|G|\), which restricts the possible orders an element may take.

### Representation Theory

Abstract group presentations are difficult to work with in many settings. Group representations map group elements to invertible matrices such that composition of group elements corresponds to matrix multiplication. This gives the group a natural action on vector spaces and allow for analysis of the group using linear algebra. Formally, a **representation of a group**\(G\) is a group homomorphism \(:G(n)\) to the group of invertible \(n n\) matrices. That is \((g_{1} g_{2})=(g_{1})(g_{2})\). A property of \(\) is that \((1)=I_{n n}\) and \((g^{-1})=(g)^{-1}\). Due to the homomorphism property, it is sufficient to define \(\) for generators of the group \(G\). For example, a \(2 2\) representation of \(D_{4}\) is given by mapping \(r\) to a \(/2\)-rotation matrix and \(f\) to a reflection over the \(x\)-axis.

The representations of many groups are well classified. This provides a ready source of tensor representations for group elements to use as inputs for neural networks. For example, for finite groups, by Maschke's Theorem , representations may be decomposed into **irreducible representations**. That is, there exists a basis such that the representation matrices are all block diagonal with the same block sizes and these blocks cannot be further subdivided. The irreducible representations may then be further classified by computing character tables.

### Symmetric and Braid Group

The **braid group** on \(n\) strands \(B_{n}\) has presentation

\[_{1},...,_{n-1}_{i}_{j}=_{j}_{i} |i-j| 2,_{i}_{i+1}_{i}=_{i+1}_{i}_{i+1} 1 i n-2.\] (1)

The braid group intuitively represents all possible ways to braid a set of \(n\) strands. The generators \(_{i}\) correspond to twisting strand \(i\) over \(i+1\) and \(_{i}^{-1}\) is the reverse, twisting strand \(i+1\) over \(i\). It is defined topologically as equivalence classes up to ambient isotopy of \(n\) non-intersecting curves in \(^{3}\) connecting two sets of \(n\) fixed points. The braid group is infinite and though some representations are known, they are not fully classified . The braid group has important connections to knot theory, mathematical physics, representation theory, and category theory.

The **symmetric group** on \(n\) elements, denoted \(S_{n}\), is defined as the set of bijections from \(\{1,,n\}\) to itself. It is also a quotient of the braid group \(B_{n}\) and has a presentation similar to Eqn. 1 but with additional relations \(_{i}^{2}=1\) for \(1 i n-1\). Here \(_{i}\) is the transposition \((ii+1)\). The symmetric group has finite order \(|S_{n}|=n!\). Representations of the symmetric group are well understood. The irreducible representations are parameterized by partitions of \(n\). For more details, see .

### Categorical Braid Actions

One current active research problem in mathematics concerns actions of braid groups on categories. A category is an abstract mathematical structure that has _objects_ and maps or _morphisms_ between objects, satisfying several coherence axioms. For example, the category \(_{}\) has objects which are real vector spaces and morphisms which are linear maps. _Functors_ are maps between categories that take objects to objects and morphisms to morphisms between the corresponding objects, satisfying several compatibility conditions. The action of a group \(G\) on a category \(\) means that each group element \(g G\) is associated with an invertible functor \(F_{g}\), such that any relation \(x=y\) in the group implies that the corresponding functors \(F_{x}\) and \(F_{y}\) are naturally isomorphic.

Given a category on which a braid group acts, mathematicians are interested in measuring how objects grow under repeated applications of elements of the braid group. The "size" of an object in a category may be measured using a tool called Jordan-Holder filtrations. For example, Bapat et al.  attempt to measure growth rates of objects in a specific category \(_{n}\) under repeated applications of certain twist functors \(_{P_{i}}\), which define an action of the braid group \(B_{n}\) on \(_{n}\). Each object in the category has a Jordan-Holder filtration giving a unique vector in \(_{ 0}^{n}\) of Jordan-Holder multiplicities. For more details see  and Appendix A.

However, they are only able to compute the action in certain cases and a simple formula is elusive. A complete description of the Jordan-Holder multiplicities after applying combinations of \(_{P_{i}}\) to one of the generating objects is only known for \(n=3\); that is, the case of the 3-strand braid group \(B_{3}\). Understanding how these multiplicities evolve under repeated application of braids is a challenging open research problem in mathematics.

## 4 Methods

We formulate the problem of learning a function on a symmetry group as a sequence learning problem using a presentation of the group in terms of generators and relations. We propose MatrixNet which predicts the label using a learned matrix representation for the group. The homomorphism property of the representation is enforced through a combination of model design and an auxiliary loss term.

### Problem Formulation

We consider task functions of the form \(f G^{c}\) where \(G\) is a finite or discrete group and the output space \(^{c}\) may represent either a regression target or class label. While such tasks appear in computer vision, robotics, and protein modeling, we are particularly interested in problems in mathematics where neural models may lend additional examples and insight towards proving theorems.

To efficiently represent group elements in infinite or large groups, we consider a presentation of \(G= S R\) in terms of generators \(S=\{g_{1},,g_{n}\}\) and relations \(R=\{r_{1},,r_{m}\}\). Model inputs \(g G\) are represented by sequences of generators \((g_{i_{1}},,g_{i_{}})\) where \(g=g_{i_{1}} g_{i_{}}\) is of arbitrary length \( 0\) and \(1 i_{j} n\). For convenience, we can include the identity \(g_{0}=e\) among the generators to pad sequences without changing the group element.

Since a single group element may be represented by different sequences, it is critical for the model \(f_{}\) to be invariant to both the group axioms and relations. That is, we desire

\[f_{}(g_{i_{1}},,g_{i_{k}},e,g_{i_{k+1}},,g_{i_ {}}) =f_{}(g_{i_{1}},,g_{i_{k}},g_{i_{k+1}},,g_{i_{ }})\] (Identity), (2) \[f_{}(g_{i_{1}},,g_{i_{k}},g_{j},g_{j}^{-1},g_{i_{k+ 1}},,g_{i_{}}) =f_{}(g_{i_{1}},,g_{i_{k}},g_{i_{k+1}},,g_{i_{ }})\] (Inverses), (3) \[f_{}(g_{i_{1}},,g_{i_{k}},r_{j},g_{i_{k+1}},,g _{i_{}}) =f_{}(g_{i_{1}},,g_{i_{k}},g_{i_{k+1}},,g_{i_{ }})\] (Relations). (4)

### MatrixNet

We propose MatrixNet (see Figure 1), a neural sequence model which models functions on a group \(G\) by taking as input sequences of generators for \(G\). It achieves invariance to group axioms and relations through a combination of built in constraints and an auxiliary loss term. The key part of the MatrixNet architecture is the matrix block which takes a group generator \(g_{i}\) as input and outputs an invertible square matrix representation \(M_{g_{i}}\). The matrix representation \(M_{g}\) for an arbitrary group element \(g\) is defined as the product of matrix representations of generators needed to generate \(g\). This matrix representation is then flattened and passed to a downstream task model (such as an MLP) which computes the label. In what follows, we give a more detailed description of the matrix block and some variations on the architecture.

Signed one-hot encodingWe define the _signed one-hot encoding_, a modified version of the traditional one-hot encoding, for encoding group generators used as input to MatrixNet. Let \((g_{i_{1}}^{_{1}},g_{i_{2}}^{_{2}},...,g_{i_{}}^{_ {}})\) be a sequence of generators where \(0 i_{k} n\) and \(_{k}\{ 1\}\). The signed one-hot encoding encodes each generator \(g_{i_{k}}^{_{k}}\) as a vector \(v_{g_{i_{k}}}= e_{i}=[0,,0,_{k},0,,0] ^{n}\) which is 1 or -1 in the \(i\)th entry. The identity element \(g_{0}=1\) is mapped to the zero vector \(v_{1}=^{n}\). The signed one-hot encoding is chosen since it intuitively relates a generator and its inverse as \(v_{g^{-1}}=-v_{g}\).

#### 4.2.1 Matrix Blocks and Learned Matrix Representations

The matrix block is designed as a parameterized representation of the free group \(F_{S}\), that is, a homomorphism \(:F_{S} GL(n)\) which satisfies 3 properties: (1) the matrix \((g)=M_{g}\) is invertible, (2) \((1)=I_{n n}\), and (3) if \((g)=M_{g}\) then \((g^{-1})=M_{g}^{-1}\). In what follows \(v_{g_{i_{k}}}\) is

Figure 1: Schematic of MatrixNet for predicting order of elements of \(S_{3}\). Input generators \(_{1}\) and \(_{2}\) are mapped to learned representations and sequentially multiplied to provide a matrix representation of group element \(g\). The order is then predicted by the task model which is an MLP.

the signed one-hot encoding of the generator \(g_{i_{k}}\) and \(W\) is a learnable parameter matrix. For a group element \(g=g_{i_{1}} g_{i_{n}}\), the matrix block is defined

\[A_{k}= (Wv_{g_{i_{k}}})\] \[M_{g_{i_{k}}}= (A_{k})\] \[M_{g}= M_{g_{i_{1}}}M_{g_{i_{2}}} M_{g_{i_{n}}}\]

where \(\) reshapes a vector in \(^{n^{2}}\) into a square \(n n\) matrix.

**Proposition 1**.: _Matrix Block defines a representation of the free group._

Proof.: Property (1) is satisfied since the outputs of a matrix exponential are invertible. Properties (2) and (3) follow from \(v_{g_{i_{k}}^{-1}}=-v_{g_{i_{k}}}\), \(v_{1}=\), and properties of the matrix exponential,

\[M_{1} =(_{n n})=I_{n n}\] \[M_{g_{i_{k}}^{-1}} =(-A_{k})=M_{g_{i_{k}}^{-1}}.\]

#### 4.2.2 Variations of Matrix Block

We now present some variations of this simple design that satisfy the group homomorphism properties.

Linear Network (MatrixNet-LN)The first variant replaces the single parameter matrix \(W\) with a linear network that has two parameter matrices \(W_{1},W_{2}\). The linear network matrix block changes the computation of the intermediate \(A_{k}\) matrix to:

\[A_{k}= (W_{2}W_{1}v_{g_{i_{k}}})\]

This is still a linear function of \(v_{g_{i_{k}}}\) meaning Proposition 1 still holds by the same argument.

While this variation does not give increased expressivity over the original formulation of the matrix block, the linear network can change the optimization landscape leading to different performance in practice. This variation is called MatrixNet-LN.

Non-Linearity (MatrixNet-NL)The second variation introduces an element-wise odd non-linear function \(f\). That is \(f(-x)=-f(x)\) as with \(\). The non-linear matrix block modifies

\[A_{k}=(f(W_{1}v_{g_{i_{k}}}))\]

**Proposition 2**.: _Non-linear matrix block defines a representation of the free group._

Proof.: Property (1) is satisfied by the same argument in Proposition 1. Since \(f\) is an odd function, \(f(W_{1}v_{1})=f()\) and \((f(W_{1}v_{g_{i_{k}}^{-1}}))=(f(- W_{1}v_{g_{i_{k}}}))=-A_{k}\). Therefore Properties (2) and (3) are satisfied by the same argument in Proposition 1. 

This variation can be combined with the first as Proposition 2 holds for linear transformations of \(A_{k}\). That is, \(A_{k}=(W_{2}f(W_{1}v_{g_{i_{k}}}))\). Unless otherwise noted we set \(f=\).

Block-Diagonal (MatrixNet-MC)The third variation is inspired by the decomposition of representations into irreducible representations. For certain classes of groups such as finite groups, every representation decomposes such that the matrices \(M_{g}\) have a consistent block diagonal structure in some basis. Thus to learn an arbitrary representation of the group \(G\), it suffices to learn a block diagonal representation assuming the blocks are large enough.

This variation learns \(\) intermediate \(n_{j} n_{j}\) matrices \(A_{k_{j}}\) which are combined to form a block-diagonal \(n n\) matrix \(A_{k}\) where \(n=_{j=1}^{}n_{j}\). The block-diagonal matrix block is defined with

\[A_{k_{j}}= (W_{j}v_{g_{i_{k}}})j=1 \] \[A_{k}= (A_{k_{1}},A_{k_{2}},,A_{k_{}})\]Note that \(\) and matrix multiplication preserve the block structure. If the sizes \(n_{j}\) are fixed to all be equal, this formulation can be implemented as a multi-channel matrix block where both \(A_{k}\) and \(M_{g_{i_{k}}}\) are \( n_{j} n_{j}\) tensors with \(\) channels. Each \(A_{k_{j}}\) is calculated identically to \(A_{k}\) in the original matrix block formulation, and \(\) is linear, so Proposition 1 still holds. This variation is also compatible with the previous two variations. In our experiments, we implement a 3-block version called MatrixNet-MC with a single linear layer and no non-linearity.

### Enforcing group relation invariances

The matrix block is constrained to learn a representation of the free group \(F_{S}\). As a consequence MatrixNet will satisfy (2) and (3) as desired. However, most groups have relations which cannot be enforced through a simple weight-sharing scheme used in equivariant architectures . We propose to learn the relations through a secondary loss which measures how closely the representation respects the group relations. More concretely, let \(G= S|R\) be a group with relations \(r_{i} R\). The loss is:

\[_{rel}=_{r_{i} R}(||M_{r_{i}}-I_{n n}||)\]

Since MatrixNet learns a representation that is invariant to group axioms, it is sufficient to sum over only \(\{r_{i}\}_{i=1}^{m}\) and not all compositions of relations. For architectures which do not respect the free group structure, the relations \(r_{i}\) alone may not guarantee that all equivalent words have identical feature representations, requiring potentially combinatorial amounts of data augmentation. This allows MatrixNet to both efficiently learn group relation invariance and simply verify this invariance without any data augmentation.

## 5 Experiments

We use two learning tasks to evaluate the four variants of MatrixNet and compare our approach against several baseline models. We use several finite groups on a well understood task as an initial test to validate our approach and then move on to an infinite group, the braid group \(B_{3}\), on a task related to open problems. As baselines, we compare to an MLP for fixed maximum sequence length and LSTM and Transformer models on longer sequences. Baseline model parameters were chosen so all of the models have approximately the same number of trainable parameters.

### Order Prediction in Finite Groups

The first task is to predict the order of group elements in finite groups. Elements of finite groups are input as finite sequences of generators as described in Section 3.3. The typical efficient algorithm for computing the order involves disjoint cycle decomposition, making order classification a non-trivial task. See Appendix B.1 for more details on the sampling method and data splits.

Models ComparedWe compare MatrixNet variants and three baselines for order prediction in \(S_{10}\): (1) **MLP** with 3 layers with hidden dimension 256 and SiLU activations, (2) **Fixed representation** input to a 2-layer classifier MLP with 256 hidden dimensions and SiLU activations, (3) **LSTM** input to a 2-layer LSTM with 256 hidden dimensions with a subsequent MLP classifier using SiLU activations, (4) **MatrixNet-LN** with a 2-layer 256 hidden dimension matrix block and classifier network with SiLU activations. (5) **MatrixNet-MC** with a 2\(x2\) matrix block size over 5 matrix channels and classifier network with SiLU activations. (6) **MatrixNet-NL** with a 2-layer 256 hidden dimension matrix block with SiLU activations and classifier network with SiLU activations. The precomputed representation is an ablated version of MatrixNet using a fixed representation of \(S_{10}\) instead of a learned one. For the fixed representation, we use the standard \(10 10\) representation given by the permutation matrices corresponding to the group element. In MatrixNet-LN, the activation between layers of the matrix block is set to a linear passthrough while in MatrixNet-NL the activation is specified to be SiLU. MatrixNet-MC enforces a \(2x2\) block diagonal structure on the learned representations corresponding to the 2-dimensional irreps of \(S_{10}\).

We also note the use of SiLU activation in our \(S_{10}\) MatrixNet model. Due to the generator self-inverse property we need not consider separate generator inverses, and so the odd function requirement given in Proposition 2 is not applicable.

Model Comparison ResultsResults of the experiments are summarized in Table 1. All variants of MatrixNet achieve a classification accuracy of at least \(99\%\) across multiple independent trials. Of note is the inferior performance of the precomputed representation baseline compared to the MLP and MatrixNet on both loss and accuracy metrics, suggesting that there is an advantage to a learnable representation. These results on \(S_{10}\) order classification validate that group representation learning can aid learning of tasks defined over groups.

Order Prediction over Different GroupsIn order to demonstrate the flexibility of MatrixNet, we show that MatrixNet can be used to predict order across several different sizes and types of groups. In addition to \(S_{10}\), we evaluate MatrixNet on a larger symmetric group \(S_{12}\) an Abelian group \(C_{11} C_{12} C_{13} C_{14} C_{15}\) and a product \(S_{5} S_{5} S_{5} S_{5}\). These product groups provide a more complex group structure which MatrixNet must learn for successful generalization, with varying representation structure. The results for these experiments are summarized in Table 2.

MatrixNet achieves a high classification accuracy across all additional groups tested. However, accuracy for the Abelian group is lower than the accuracies for other groups tested (\(87\%\) vs \(99\%\)). One explanation for this decrease in accuracy is due to the large number of valid orders of the group. Additionally, due to the structure of finite Abelian groups, many element orders will be underrepresented in random sampling.

### Categorical Braid Action Prediction

In our second experiment, we train models to predict the Jordan-Holder multiplicities from braid words in the braid group \(B_{3}\) (see Section 3.4). The task is formulated as a regression task with a mean-squared error (MSE) loss function. The Jordan-Holder multiplicities are integers, so we evaluate accuracy by rounding the vector entries to the nearest integer. This accuracy is reported as an average accuracy over the three entries of the Jordan-Holder multiplicities vector. Elements of \(B_{3}\) are generated by two generators \(_{1},_{2}\) and their inverses and are encoded using a signed one-hot encoding. For more details on the dataset generation process and data splits see Appendix B.2.

We additionally performed an experiment to evaluate how well MatrixNet generalizes to unseen braid words longer than those seen in training. For this experiment, we compare against the MLP and LSTM since these were the highest performing baselines.

Baseline Comparison ResultsWe trained all of the models for 100 epochs as all of the models except the Transformer converged within 100 epochs. Despite performance converging much faster than 100 epochs for most MatrixNet variants, we found that additional epochs of training improved the model's invariance to group relations with minimal variations in performance. Table 3 shows the performance of the baseline models and MatrixNet variations at 50 and 100 epochs of training

   Model & Parameters & CE Loss (\(10^{-2}\)) & Acc \\  MLP & 365584 & \(0.02 0.01\) & \(100 0\) \\ Rep Ablation & 299792 & \(4.8 0.5\) & \(87 2\) \\ LSTM & 270505 & \(8.1 1.4\) & \(77.3 5.2\) \\  MatrixNet-LN & 343968 & \(0.9 0.3\) & \(99.4 0.4\) \\ MatrixNet-MC & 82960 & \(0.02 0.01\) & \(100 0\) \\ MatrixNet-Nonlinear & 343968 & \(0.0003 0\) & \(100 0\) \\   

Table 1: MatrixNet and baseline performance on \(S_{10}\) order prediction

   Group & \(\) & Rep. Size & Classes & CE Loss (\(10^{-2}\)) & Acc (\%) \\  \(S_{10}\) & \(10^{6}\) & 10 & 16 & \(0.0003 0\) & \(100 0\) \\ \(S_{12}\) & \(10^{8}\) & 12 & 23 & \(0.8 0.2\) & \(99.2 0.4\) \\ \(C_{11} C_{12}... C_{15}\) & \(10^{5}\) & 10 & 35 & \(2.1 2.4\) & \(87.3 18\) \\ \(S_{5} S_{5} S_{5} S_{5}\) & \(10^{8}\) & 20 & 12 & \(2.6 0.4\) & \(98.3 0.3\) \\   

Table 2: MatrixNet performance on finite group order prediction averaged over 5 runs. The simple MatrixNet model was the worst performing MatrixNet variant slightly outperforming the baseline models at 100 epochs. All other variants of MatrixNet vastly outperform baselines with both MatrixNet-LN and MatrixNet-NL achieving MSE far below the baselines and perfect or near perfect accuracy across all runs. These results confirm the results from the order prediction experiments and demonstrate the advantage of MatrixNet for learning over groups.

Length Extrapolation ResultsThe results in Figure 2 show how the MSE and average accuracy change as input length increases averaged over 10 runs. A single run was omitted from the results for MatrixNet due to training instability. We observe explosive MSE growth for MatrixNet and MatrixNet-MC, but both maintain higher average accuracy than the baselines. The high variance in MSE suggests that both variants are capable of extrapolating despite struggling compared to the other two variants. MatrixNet-LN and MatrixNet-NL both maintain near-zero average MSE as length increases and consequently achieve near perfect average accuracy as length increases.

The discrepancy in extrapolation performance of the MatrixNet variations suggest that MatrixNet-LN and MatrixNet-NL learn better representations than MatrixNet and MatrixNet-MC. To measure this, we compare the relational error of the four MatrixNet variations in Table 4. The group \(B_{3}\) has only the braid relation \(_{1}_{2}_{1}=_{2}_{1}_{2}\). We calculate the relational error as the Frobenius norm of the difference \(||M_{_{1}}M_{_{2}}M_{_{1}}=M_{_{2}}M_{_{1}}M_{ _{2}}||\). We also compute this distance between two non-equivalent braids \(_{1}_{1}_{2},_{2}_{2}_{1}\) for reference under Non-Relational Difference in Table 4.

The relational error results in Table 4 mirrors the extrapolation performance confirming that representation quality is important for effective generalization. High relational error compounds over longer word lengths hindering generalization whereas low relational error allows MatrixNet to automatically generalize to longer word lengths through invariance to group relations. These results show that MatrixNet, particularly the MatrixNet-LN and MatrixNet-NL variants, is able to learn group representations invariant to the group relations allowing for effective generalization to longer unseen group words.

### Visualizing the Learned Representations

We present some visualizations of the learned representations of the braid group from the highest performing variant, MatrixNet-NL. Figure 2 shows visual plots of the learned representations. In

   MatrixNet Variation & Relational Error & Non-relational Difference \\  MatrixNet & \(13.63 6.83\) & \(33.64 11.28\) \\ MatrixNet-MC & \(2.58 2.10\) & \(9.60 2.5\) \\ MatrixNet-LN & \(0.071 0.018\) & \(4.96 0.55\) \\ MatrixNet-NL & \(0.066 0.009\) & \(4.99 0.45\) \\   

Table 4: Relational error of MatrixNet models trained on length extrapolation dataset. The non-relational difference is computed between two non-equivalent braids for comparison. High relational error compounds for longer words resulting in poor extrapolation.

   Model Type & Parameters & MSE Epoch 50 & MSE Epoch 100 & Avg. Acc. \\  Transformer & \(63779\) & \(3.013 0.147\) & \(2.895 0.024\) & \(42.3\% 1.2\%\) \\ MLP & \(52099\) & \(0.315 0.004\) & \(0.132 0.009\) & \(89.1\% 0.73\%\) \\ LSTM & \(51027\) & \(0.345 0.149\) & \(0.075 0.035\) & \(93.0\% 3.9\%\) \\  MatrixNet & \(42507\) & \(0.543 0.458\) & \(0.082 0.034\) & \(95.1\% 0.9\%\) \\ MatrixNet-LN & \(42883\) & \(\) & \(0.001 0.001\) & \(\) \\ MatrixNet-MC & \(41987\) & \(0.063 0.033\) & \(0.014 0.006\) & \(98.8\% 0.6\%\) \\ MatrixNet-NL & \(42883\) & \(0.002 0.003\) & \(\) & \(\) \\   

Table 3: MSE and accuracy of Jordan–Hölder multiplicities for baseline models and MatrixNet variations. Results are averaged over 5 runs. See Appendix B.2 for model parameters and training details.

the last two plots, the learned representation for two equivalent words are approximately equal even though this relation is not among the relations \(r_{j}\) used in the loss \(_{rel}\). This shows MatrixNet does indeed generalize over relations, allowing it to generate nearly identical representations for equivalent words.

## 6 Conclusion

In this paper we have presented MatrixNet, a novel neural network architecture designed for learning tasks with group element inputs. We developed 3 variations of our approach which structurally enforce group axioms and a regularization approach for enforcing invariance to relations. We evaluate MatrixNet on two group learning problems over several finite groups and \(B_{3}\) and demonstrate our model's performance and ability to automatically generalize to unseen group elements. In future work we plan to develop interpretability analysis methods based on group representation theory to better understand the structure of MatrixNet's learned representations. Understanding the learned representations may provide valuable insights and explanations of the model outputs assisting with generating new conjectures for open mathematical research problems.

LimitationsThe current work relies on the assumption that the studied group is finitely presented which limits us to discrete groups. However, learned group representations may also be useful for learning over Lie groups. In such case, extending our method will require working with infinitesimal Lie algebra generators. Additionally, while the group axioms are strictly enforced by the model structure, the fact the relations are enforced using auxiliary loss terms means the homomorphism property is not exact. Future work may explore methods of reducing this error.

Figure 3: Visualization of learned matrix representations. The first two figures show the representations for the generators of \(B_{3}\). The last two figures show the representation for equivalent words that are generated by the relations of \(B_{3}\).

Figure 2: Length extrapolation results. Left: The plot shows how MSE grows for increasing word lengths (\(y\)-axis is truncated for clarity). Right: The plot shows how the average accuracy decays for increasing word lengths. The relatively high accuracy of MatrixNet and MatrixNet-MC compared to baselines suggests that the high MSE is caused by outliers with multiplicity predictions much higher than the ground truth.

AcknowledgementsThis work is supported in part by NSF 2134178. Lucas Laird is supported by the MIT Lincoln Laboratory Lincoln Scholars program. Circe Hsu is supported by a Northeastern University Undergraduate Research and Fellowships PEAK Experiences Award. The authors would like to thank Mustafa Hajij and Paul Hand for helpful discussions.