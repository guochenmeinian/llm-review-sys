# Trieste: Efficiently Exploring The Depths of Black-box Functions with TensorFlow

Victor Picheny, Joel Berkeley, Hrvoje Stojic, Uri Granta,

Sebastian W. Ober, Artem Artem Artemev, Khurram Ghani, Alexander Goodall,

Andrei Paleyes, Sattar Vakili, Sergio Pascual-Diaz, Stratis Markou

Secondmind, Cambridge, UK

victor@secondmind.ai

Henry B. Moss

University of Cambridge & Lancaster University, UK

Jixiang Qing, Nasrulloh R.B.S Loka, Ivo Couckuyt

Ghent University - imec, Ghent, Belgium

###### Abstract

We present Trieste, an open-source Python package for Bayesian optimization and active learning benefiting from the scalability and efficiency of TensorFlow. Our library enables the plug-and-play of popular TensorFlow-based models within sequential decision-making loops, e.g. Gaussian processes from GPflow or GPflux, or neural networks from Keras. This modular mindset is central to the package and extends to our acquisition functions and the internal dynamics of the decision-making loop, both of which can be tailored and extended by researchers or engineers when tackling custom use cases. Trieste is a research-friendly and production-ready toolkit backed by a comprehensive test suite, extensive documentation, and available at https://github.com/secondmind-labs/trieste.

## 1 Introduction

TensorFlow is one of Python's primary machine learning frameworks, offering both flexibility and scalability through its support for auto-differentiation and GPU-based computation. Yet, TensorFlow 2 does not have a library for Bayesian Optimization (BO) -- an increasingly popular method for black-box optimization under heavily constrained optimization budgets [see 45, for an introduction]. This lack of support is likely due to the inherently sequential and evolving nature of active learning loops, which makes BO implementations prone to trigger expensive retracing of the computational graphs, as used by TensorFlow to accelerate numerical calculations. However, once special care is taken to avoid unnecessary retracing, a TensorFlow-based BO library would allow users not only to leverage versatile and powerful (probabilistic) TensorFlow modeling libraries (e.g. Keras, GPflow, GPflux), but also to benefit from BO-specific perks like the freedom to define acquisition functions without specifying their gradients, easily parallelized optimization of acquisition functions, and in-the-loop monitoring of models and convergence statistics (e.g. TensorBoard).

In this paper, we present Trieste1, a highly modular, flexible and general-purpose BO library designed to enable users working within TensorFlow ecosystems to:

1. deploy their own existing models to drive BO loops, and
2. build BO pipelines that harness the ease and computational efficiency provided by TensorFlow's automatic differentiation and support for modern compute resources like GPUs.

Our library is oriented towards real-world use and contains a wide range of advanced BO functionalities, with a focus on modularity to allow ease of extension with custom models and acquisition functions. Trieste's funcionality is matched only by Torch-based BotTorch of  (see Section 4) which, although widely regarded as the _state-of-the-art_ BO Python library, does not support the large number researchers and engineers with TensorFlow models and pipelines.

## 2 Related Work

Many open-source libraries have been built to support the recent increase in the use and development of BO methodology. For example, Python users with models written in Torch or NumPy can easily find compatible libraries such as BotTorch, GPyOpt, RoBo, Emukit or Dragonfly. Similarly, those running R, C++ or Java can use DiceOptim, BayesOpt or SMAC. The library GPflowOpt was built on TensorFlow 1 with an intent similar to Trieste, but is not actively maintained anymore and does not support the fundamentally different TensorFlow 2.

## 3 Key features and Design

We now present the modular structure of Trieste which contains four key building blocks, a choice of high-level interface (either AskTellOptimizer or BayesianOptimizer), a choice of ProbabilisticModel, and a pairing of AcquisitionRule and AcquisitionFunction. Although Trieste's structure allows a high level of customization, sensible defaults are provided throughout the library in order to give new users good starting points.

### Interfaces for different levels of control over function evaluation

A key design choice of Trieste is its AskTellOptimizer interface, which need not have direct access to the objective function. In many libraries, the objective must be a query-able function to be passed into the loop and called for each BO step, an assumption that is rarely suitable when performing BO in the real world. In contrast, an AskTellOptimizer outputs recommended query points (the _ask_) and then waits for the user to return new evaluations (the _tell_) (see Figure 3). This interface allows Trieste users to apply BO across a range of non-standard real-world settings, e.g., when evaluating the objective function requires laboratory  or distributed computing resources, such that users have only partial control over the environment  or batches of evaluations arrive asynchronously . For settings where it is appropriate for the objective function to be passed into the BO, e.g., for experiments with synthetic problems, we also provide a more standard BayesianOptimizer interface that will run multiple BO steps.

### Versatile Model Support

The models in the BO loop can be any model written in TensorFlow, and Trieste is designed to make it easy to add a new model, through a set of general model interfaces. We provide direct interfaces to import models from well-established TensorFlow modeling libraries, e.g., Gaussian processes from GPflow and GPflux, as well as neural networks from Keras. Trieste users have a range of popular probabilistic models from these libraries available out of the box. They cover both regression and classification tasks and range from standard GPs  to alternatives like sparse variational GPs , Deep GPs  or Deep Ensembles , that scale much better with the number of function evaluations. Finally, we provide user-friendly model builders with sensible default setups, allowing users to get more quickly to good results and facilitate usage for those with less experience with probabilistic models. Hence, Trieste users can benefit from a large choice of models with a wide range of complexity, unlocking novel applications for BO.

Importantly, Trieste's BO loops allow the modeling of multiple quantities, using either multiple separate models or a single multi-output model. This framework naturally supports common BOextensions like multi-objective optimization , multi-fidelity optimization , optimization with constraints , and combinations thereof.

### Acquisition Rules and Functions

Regardless of the interface and model choice, the recommendation of query points is controlled by an AcquisitionRule. While the vanilla BO rule is to query the point that optimizes a particular AcquisitionFunction, the AcquisitionRule is a useful abstraction to handle complex cases for which a high level of flexibility is needed. For example, Trieste includes variable optimization spaces for AcquisitionFunction [e.g., using trust regions, 9], a multi-step procedure for selecting query points , and a greedy approach to build batches of query points .

A wide range of acquisition functions are already provided in Trieste to tackle most of the usual BO cases, with many based on the gold-standard Expected Improvement [EI; 18], including variants for batch [6; 12; 2], noisy , multi-objective  and constrained  optimization. We also include implementations of recent information-theoretic approaches for multi-fidelity optimization , a scalable extension of batched Thompson sampling , as well as popular active learning methods for improved classification  or contour line estimation .

Trieste is designed to make it straightforward for a user to specify a new acquisition function or rule. Automatic differentiation directly provides the function gradients, which are leveraged by Trieste's supported acquisition function optimizers, including an effective parallelized multi-start L-BFGS-B optimizer. Special care is taken to allow AcquisitionFunctions to be updated without expensive retracing of the computational graphs for each BO step.

## 4 Feature Benchmark

Table 1 compares Trieste with two other popular BO frameworks, demonstrating that it is a general-purpose BO toolbox. Trieste has comparable features to the Torch-based BoTorch and provides substantially more functionality than the NumPy-based Emukit. The other Python BO libraries introduced in Section 2 are not included in this comparison as they either have even less functionality

Figure 1: AskTellOptimizer

Figure 3: Triesteâ€™s interfaces over \(N\) optimization steps. The AskTellOptimizer requires users to make manual evaluations, which is useful for real-world settings, e.g. evaluating the objective function requires laboratory or distributed computing resources. In contrast, the BayesianOptimizer queries the black box directly, performing all \(N\) BO iterations without user interaction.

Figure 2: BayesianOptimizer

and are no longer maintained (GPyOpt, RoBO and GPflowOpt), or target only hyper-parameter optimization (Dragonfly and SMAC).

## 5 Conclusions and Future Plans

Trieste is an open-source project that allows TensorFlow practitioners to easily use BO in their systems. It is a highly flexible library designed with modularity in mind, easy to extend with custom models and acquisition functions. Backed by continuous integration and comprehensive unit tests (\(97\%\) coverage), Trieste is a reliable and robust framework used for both real-world deployment and research has recently been taken up by researchers to develop new BO methodology  whilst also being used across a range of applications including designing heat exchangers  and improving adhesive bonding .

Trieste relies on features added and improved by the community and so we gladly welcome feature requests and code contributions. We plan to continue to add new functionality orientated to supporting the application of BO in the real world. In the near term, this will include high-dimensional objective functions  and non-Euclidean search spaces .

## 6 Acknowledgements

This research receives funding from the Flemish Government under the "Onderzoeksprogramma Artifciele Intelligentie (AI) Vlaanderen" programme.