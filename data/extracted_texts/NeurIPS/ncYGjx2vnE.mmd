# Chimera: Effectively Modeling Multivariate Time Series with 2-Dimensional State Space Models

Ali Behrouz

Cornell University

ab2947@cornell.edu

&Michele Santacatterina

New York University

santam13@nyu.edu

&Ramin Zabih

Cornell University

rdz@cs.cornell.edu

###### Abstract

Modeling multivariate time series is a well-established problem with a wide range of applications from healthcare to financial markets. It, however, is challenging as it requires methods to (1) have high expressive power of representing complicated dependencies along the time axis to capture both long-term progression and seasonal patterns, (2) capture the inter-variate dependencies when it is informative, (3) dynamically model the dependencies of variate and time dimensions, and (4) have efficient training and inference for very long sequences. Traditional State Space Models (SSMs) are classical approaches for univariate time series modeling due to their simplicity and expressive power to represent _linear_ dependencies. They, however, have fundamentally limited expressive power to capture non-linear dependencies, are slow in practice, and fail to model the inter-variate information flow. Despite recent attempts to improve the expressive power of SSMs by using deep structured SSMs, the existing methods are either limited to univariate time series, fail to model complex patterns (e.g., seasonal patterns), fail to dynamically model the dependencies of variate and time dimensions, and/or are input-independent. We present Chimera, an expressive variation of the 2-dimensional SSMs with careful design of parameters to maintain high expressive power while keeping the training complexity linear. Using two SSM heads with different discretization processes and input-dependent parameters, Chimera is provably able to learn long-term progression, seasonal patterns, and desirable dynamic autoregressive processes. To improve the efficiency of complex 2D recurrence, we present a fast training using a new 2-dimensional parallel selective scan. Our experimental evaluation shows the superior performance of Chimera on extensive and diverse benchmarks, including ECG and speech time series classification, long-term and short-term time series forecasting, and time series anomaly detection.

## 1 Introduction

Modeling time series is a well-established problem with a wide range of applications from healthcare  to financial markets  and energy management . The complex nature of time series data, its diverse domains of applicability, and its broad range of tasks (e.g., classification , imputation , anomaly detection , and forecasting ), however, raise fundamental challenges to design effective and generalizable models: (1) The higher-order, seasonal, and long-term patterns in time series require an effective model to be able to expressively capture complex and autoregressive dependencies; (2) In the presence of multiple variates of time series, an effective model need to capture the complex dynamics of the dependencies between time and variate axes. More specifically, most existing multivariate models seem to suffer from overfitting especially when the target time series is not correlated with other covariates . Accordingly, an effective model needs to adaptively learn to select (resp. filter) informative (resp. irrelevant) variates; (3) The diverse set of domains and tasks requires effective models to be free of manual pre-processing and domainknowledge and instead adaptively learn them; and (4) Due to the processing of very long sequences, effective methods need efficient training and inference.

Classical methods (e.g., State Space Models [12; 13], ARIMA , SARIMA , Exponential Smoothing (ETS) ) require manual data preprocessing and model selection, and often are not able to capture complex _non-linear_ dynamics. The raise of deep learning methods and more specifically Transformers  has led to significant research efforts to address the limitation of classical methods and develop effective deep models [18; 19; 20; 21; 22; 23; 24; 25; 26; 27]. Unfortunately, most existing deep models struggle to achieve all the above four criteria. The main body of research in this direction has focused on designing attention modules that use the special traits of time series [21; 20]. However, the inherent permutation equivariance of attentions contradicts the causal nature of time series and often results in suboptimal performance compared to simple linear methods . Moreover, they often either overlook difference of seasonal and long-term trend or use non-learnable methods to handle them .

A considerable subset of deep models overlook the importance of modeling the dependencies of variates [11; 28; 29]. These dependencies, however, are not always useful; specifically when the target time series is not correlated with other covariates . Despite several studies exploring the importance of learning cross variate dependencies [26; 27; 30], there has been no universal standard and the conclusion has been different depending on the domain and benchmarks. Accordingly, we argue that an effective model need to _adaptively_ learn to capture the dependencies of variates in a data-dependent manner. In this direction, recently, Liu et al.  argue that attention mechanisms are more effective when they are used across variates, showing the importance of modeling complex non-linear dependencies across the variate axis in a data-dependent manner. However, the quadratic complexity of Transformers challenges the model on multivariate time series with a large number of variates (e.g., brain activity signals  or traffic forecasting ), limiting the efficient training and inference (see Table 3, and Table 5).

The objective of this study is to develop a provably expressive model for multivariate time series that not only can model the dynamics of the dependencies along both time and variates, but it also takes advantage of fast training and inference. To this end, we present a Chimera, a three-headed two-dimensional State Space Model (SSM) that is based on linear layers along (i) time, (ii) variates, (iii) time\(\)variate, and (iv) variate\(\)time. Chimera has a careful parameterization based on the pair of companion and diagonal matrices (see Figure 1), which is provably expressive to recover both classical methods [16; 14; 15], linear attentions, and recent SSM-based models [31; 32]. It further uses an adaptive module based on a 2D SSM with an especially designed discretization process to

Figure 1: **The Overview of Contributions and Architecture of Chimera. We present a 2-dimensional SSM with careful and expressive parameterization. It uses different learnable discretization processes to learn seasonal and long-term progression patterns, and leverages a parallelizable and fast training process by re-formulating the 2D input dependent recurrence as a 2D prefix sum problem.**

capture seasonal patterns. While our theoretical results and design of Chimera guarantee the first three criteria of an effective model, due to its 2D recurrence, the naive implementation of Chimera results in slow training. To address this issue, we reformulate its 2D recurrence as the prefix sum problem with a 2-dimensional associative operators. This new formulation can be done in parallel and has hardware-friendly implementation, resulting in much faster training and inference.

In our experimental evaluation, we explore the performance of Chimera in a wide range of tasks: ECG and audio speech time series classification, long- and short-term time series forecasting, and anomaly detection tasks. We find that Chimera achieve superior or on par performance with state-of-the-art methods, while having faster training and less memory consumption. We perform a case study on the human brain activity signals  to show (1) the effectiveness of Chimera and (2) evaluate the importance of modeling the dynamics of the variates dependencies.

## 2 Preliminaries

**Notations.** In this paper we mainly focus on classification and forecasting tasks. Note that anomaly detection can be seen as a binary classification task, where \(0\) means "normal" and \(1\) means "anomaly". We let \(=\{_{1},,_{N}\}^{N T}\) be the input sequences, where \(N\) is the number of variates and \(T\) is the time steps. We use \(_{v,t}\) to refer to the value of the series \(v\) at time \(t\). In classification (anomaly detection) tasks, we aim to classify input sequences and for forecasting tasks, given an input sequence \(_{i}\), we aim to predict \(}_{i}^{1 H}\), i.e., the next \(H\) time steps for variate \(_{i}\), where \(H\) is called horizon. In 2D SSMs formulation, for a 2-dimensional vector \(x^{1}\), we use \(x^{(1)}\) and \(x^{(2)}\) to refer to its real and imaginary components, respectively.

**Multi-Dimensional State Space Models.** We build our approach on the continuous State Space Model (SSM) but later we make each component of Chimera discrete by a designed discretization process. For additional discussion on 1D SSMs see Appendix A. Given parameters \(_{_{1}}^{N^{(_{1})} N^{(_{1})}}\), \(_{_{2}}^{N^{(_{2})} 1}\), and \(^{N_{1} N_{2}}\) for \(_{1}\{1,...,4\}\) and \(_{2}\{1,2\}\), the general form of the time-invariant 2D SSM is the map \(^{1}^{1}\) defined by the linear Partial Differential Equation (PDE) with initial condition \(h(0,0)=0\):

\[}h(t^{(1)},t^{(2)}) =(_{1}h^{(1)}(t^{(1)},t^{(2)}),_{2}h^{(2)}(t^{(1)},t^{(2)}))+_{1}( t^{(1)},t^{(2)}),\] (1) \[}h(t^{(1)},t^{(2)}) =(_{3}h^{(1)}(t^{(1)},t^{(2)}),_{4}h^{(2)}(t^{(1)},t^{(2)}))+_{2}( t^{(1)},t^{(2)}),\] (2) \[(t^{(1)},t^{(2)})=,(t^{(1)},t^{(2)}).\] (3)

Contrary to the multi-dimensional SSMs discussed by Gu and Dao , Gu et al. , in which multi-dimension refers to the dimension of the input but with one time variable, the above formulation uses two variables, meaning that the mapping is from a 2D grid to a 2D grid.

**(Seasonal) Autoregressive Process.** Autoregressive process is a basic yet essential premise for time series modeling, which models the causal nature of time series. Given \(p\), \(_{k}^{d}\), the simple linear autoregressive relationships between \(_{k}\) and its past samples \(_{k-1},_{k-2},,_{k-p}\) can be modeled as \(_{k}=_{1}_{k-1}+_{2}_{k-2}+,_ {p}_{k-p}\), where \(_{1},,_{p}\) are coefficients. This is called AR\((p)\). Similarly, in the presence of seasonal patterns, the seasonal autoregressive process, SAR\((p,q,s)\), is:

\[_{k}=_{1}_{k-1}+_{2}_{k-2}+,_ {p}_{k-p}+_{1}_{k-s}+_{2}_{k-2s}++ _{q}_{k-qs},\] (4)

where \(s\) is the frequency of seasonality, and \(_{1},,_{p}\) and \(_{1},,_{q}\) are coefficients. Note that one can simply extend the above formulation to multivariate time series by letting coefficients to be vectors and replace the product with element-wise product.

## 3 Chimera: A Three-headed 2-Dimensional State Space Model

In this section, we first present a mathematical model for multivariate time series data and then based on this model, we present a neural architecture that can satisfy all the criteria discussed in SS1.

### Motivations & Chimera Model

SSMs have been long-standing methods for modeling time series [12; 13], mainly due to their simplicity and expressive power to represent complicated and autoregressive dependencies. Their states, however, are the function of a single-variable (e.g., time). Multivariate time series, on the other hand, require capturing dependencies along both time and variate dimensions, requiring the current state of the model to be the function of both time and variate. Classical 2D SSMs [35; 36; 37; 38], however, struggle to achieve good performance compared to recent advanced deep learning methods as they are : (1) only able to capture linear dependencies, (2) discrete by design, having a pre-determined resolution, and so cannot simply model seasonal patterns, (3) slow in practice for large datasets, (4) their update parameters are static and cannot capture the dynamics of dependencies. Deep learning-based methods [6; 30; 27], on the other hand, potentially are able to address a subset of the above limitations, while having their own drawbacks (discussed in SS1). In this section, we start with _continuous_ SSMs due to their connection to both classical methods [12; 13] and recent breakthrough in deep learning [34; 33]. We then discuss our contributions on how to take the advantages of the best of both worlds, addressing all the abovementioned limitations.

**Discrete 2D SSM.** We use 2-dimensional SSMs, introduced in Equation 1-3, to model multivariate time series, where the first axis corresponds to the time dimension and the second axis is the variates. Accordingly, each state is a function of both time and variates. The first stage is to transform the continuous form of 2D SSMs to discrete form. Given the step size \(_{1}\) and \(_{2}\), which represent the resolution of the input along the axes, discrete form of the input is defined as \(_{k,}=(k_{1},_{2})\). Using Zero-Order Hold (ZOH) method, we can discretize the input as (see Appendix C for details):

\[h^{(1)}_{k,+1}\\ h^{(2)}_{k+1,}=}_{1}&}_{2}\\ }_{3}&}_{4}h^{(1)}_{ k,}\\ h^{(2)}_{k,}+}_{1}\\ }_{2}}_{k, +1}\\ }_{k+1,},\] (5)

where \(}_{i}=(_{} _{i})\) for \(i=1,2,3,4\), \(}_{1}=_{1}^{-1}(}_ {1}-)_{1}^{(1)}\\ _{2}^{-1}(}_{2}-)_{1} ^{(2)}\), and \(}_{2}=_{3}^{-1}(}_ {3}-)_{2}^{(1)}\\ _{4}^{-1}(}_{4}-)_{2} ^{(2)}\). Note that this formulation can also be viewed as the modification of the discrete Roesser's SSM model  when we add a lag of \(1\) in the inputs \(}_{i,j}\\ }_{i,j}\). This modification, however, misses the discretization step, which is an important step in our model. We later use the discretization step to (1) empower the model to select (resp. filter) relevant (resp. irrelevant) information, (2) adaptively adjust the resolution of the method, capturing seasonal patterns.

From now on, we use \(t\) (resp. \(v\)) to refer to the index along the time (resp. variate) dimension. Therefore, for the sake of simplicity, we reformulate Equation 5 as follows:

\[h^{(1)}_{v,t+1}=}_{1}h^{(1)}_{v,t}+ }_{2}h^{(2)}_{v,t}+}_{1}_{v,t+1},\] (6) \[h^{(2)}_{v+1,t}=}_{3}h^{(1)}_{v,t}+}_{4}h^{(2)}_{v,t}+}_{2}_{v+1,t},\] (7) \[_{v,t}=_{1}h^{(1)}_{v,t}+_{2}h^{(2 )}_{v,t},\] (8)

where \(}_{1},}_{2},}_{3},}_{4}^{N N}\), \(}_{1},}_{2}^{N 1}\), and \(_{1},_{2}^{1 N}\) are parameters of the model, \(h^{(1)}_{v,t},h^{(2)}_{v,t}^{N d}\) are hidden states, and \(_{v,t}^{1 d}\) is the input. In this formulation, intuitively, \(h^{(1)}_{v,t}\) is the hidden state that carries cross-time information (each state depends on its previous time stamp but within the same variate), where \(}_{1}\) and \(}_{2}\) control the emphasis on past cross-time and cross-variate information, respectively. Similarly, \(h^{(2)}_{v,t}\) is the hidden state that carries cross-variate information (each state depends on other variates but with the same time stamp). Later in this section, we discuss to modify the model to bi-directional setting along the variate dimension, to enhance information flow along this non-causal dimension.

**Interpretation of Discretization.** Time series data are often sampled from an underlying continuous process [39; 40]. In these cases, variable \(_{1}\) in the discretization of the time axis can be interpreted as resolution or the sampling rate from the underlying continuous data. However, discretization alongthe variate axis, which is discrete by its nature, or when working directly with discrete data  is an unintuitive process, and raise questions about its significance. The discretization step in 1D SSMs has deep connections to gating mechanisms of RNNs , automatically ensures that the model is normalized , and results in desirable properties such as resolution invariance .

**Proposition 3.1**.: _The 2D discrete SSM introduced in Equation 6-8 with parameters \((\{}_{i}\},\{}_{i}\},\{}_{i}\},k _{1},_{2})\) evolves at a rate \(k\) (resp. \(\)) times as fast as the 2D discrete SSM with parameters \((\{}_{i}\},\{}_{i}\},\{}_{i}\}, _{1},_{2})\) (resp. \((\{}_{i}\},\{}_{i}\},\{}_{i}\},k _{1},_{2})\))._

Accordingly, parameters \(_{1}\) can be viewed as the controller of the length of dependencies that the model captures. That is, based on the above result, we see the discretization along the time axis as the setting of the resolution or sampling rate: while small \(_{1}\) can capture long-term progression, larger \(_{1}\) captures seasonal patterns. For now, we see the discretization along the variate axis as a mechanism similar to gating in RNNs , where \(_{2}\) controls the length of the model context. Larger values of \(_{2}\) means less context window, ignoring other variates, while smaller values of \(_{2}\) means more emphasis on the dependencies of variates. Later, inspired by Gu and Dao , we discuss making \(_{2}\) as the function of the input, resulting in a selection mechanism that filters irrelevant variates.

**Structure of Transition Matrices.** For Chimera to be expressive and able to recover autoregressive process, hidden states \(h^{(1)}_{v,t}\) should carry information about _past_ time stamps. While making all the parameters in \(_{i}\) learnable allows the model to learn any arbitrary structure for \(_{i}\), previous studies show that this is not possible unless the structure of transition matrices are restricted . To this end, inspired by Zhang et al.  that argue that companion matrices are effective to capture the dependencies along the time dimension, we restrict \(_{1}\) and \(_{2}\) matrices to have companion structure (see Appendix D for the details). Not only this formulation is shown to be effective for capturing dependencies along the time dimension  (also see Theorem 3.4), but it also can help us to compute the power of \(_{1}\) and \(_{2}\) faster in the convolutional form, as discussed by Zhang et al. . Also, for \(_{3}\) and \(_{4}\), we observe that even a simpler structure of diagonal matrices is effective to fuse information along the variate dimension. Not only these simple structured matrices make the training of the model faster, but they also are proven to be effective .

**Bi-Directionality.** The causal nature of the 2D SSM result in limited information flow along the variate dimension as variate are not ordered. To overcome this challenge, inspired by the bi-directional 1D SSMs , we use two different modules for forward and backward pass along the variate dimension. The details of formulation is in Appendix E.

**Convolution Form.** As discussed by Baron et al. , similar to 1D SSMs , the _data-independent_ formulation of 2D-SSMs can be viewed as a convolution with a kernel \(\). This formulation not only results in faster training by providing the ability of parallel processing, but it also connect Chimera

Figure 2: **Different forms of Chimera. (Top-Left) Chimera has a recurrence form (bi-directional along the variates), which also can be computed as a global convolution in training. (Top-Right) In forecasting, we present the multivariate closed-loop to improve the performance for long horizons. (Bottom) Using data-dependent parameters, Chimera training can be done as a parallel 2D scan.**

with very recent studies of modern convolution-based architecture for time series . Applying the recurrent rules in Equation 6-8, we can write the output as:

\[_{v,t}=_{1 v}_{1 t}( _{1}_{,}^{(1)}+_{2}_{ ,}^{(2)})_{,},\] (9)

where kernels \(_{,}^{(r)}=_{(z_{1},,z_{5}) ^{(r)}}q_{i}\ }_{1}^{P_{1}}}_{2}^{P_{2}}}_{3}^ {P_{3}}}_{4}^{P_{4}}}_{p_{5}}\), and \(^{()}\) is the partitioning of the paths from the starting point to \((,)\) for \(\{1,2\}\). As discussed by Baron et al. , if the power of \(}_{i}\)s are given and cached, calculating the partitioning of all paths can be done very efficiently (near-linearly) as it the generalization of pascal triangle. To calculate the power of \(}_{i}\), note that we use diagonal matrices as the structure of \(}_{3}\), and \(}_{4}\), and so computing their powers is very fast. On the other hand, for \(}_{1}\) and \(}_{2}\) with companion structures, we can use sparse matrix multiplication, which results in linear complexity in terms of the sequence length.

**Data-Dependent Parameters.** As discussed earlier, parameters \(}_{1}\) and \(}_{2}\) controls the emphasis on past cross-time and cross-variate information. Similarly, parameters \(_{1}\) and \(}_{1}\) controls the emphasis on the current input and historical data. Since these parameters are data-independent, one can interpret them as a global feature of the system. In complex systems (e.g., human neural activity), however, the emphasis depends on the current input, requiring these parameters to be the function of the input (see SS4.1). The input-dependency of parameters allows the model to select relevant and filter irrelevant information for each input data, providing a similar mechanism as transformers . Additionally, as we argue earlier, depending on the data, the model needs to adaptively learn if mixing information along the variates is useful. Making parameters input-dependent further overcomes this challenge and lets our model to mix relevant and filter irrelevant variates for the modeling of a variate of interest. One of our main technical contributions is to let \(}_{i}\), \(}_{i}\), and \(_{i}\) for \(i\{1,2\}\) be the function of the input \(_{v,t}\). This input-dependent 2D SSM, unfortunately, does not have the convolution form, limiting the scalability and efficiency of the training. We overcome this challenge by computing the model recurrently with a new 2D scan.

**2D Selective Scan.** Inspired by the scanning in 1D SSMs [50; 33], we present an algorithm to decrease the sequential steps that are required to calculate hidden states. Given \(p,q\), each of which with 6 elements, we first define operation \(\) as: (\(\) is matrix-matrix and \(\) is matrix-vector multiplication)

\[p=p_{1}&p_{2}&p_{3}\\ p_{4}&p_{5}&p_{6}q_{1}&q_{2}& q_{3}\\ q_{4}&q_{5}&q_{6}=q_{1} p_{1}&q_{2} p_{2}&q_{1}  p_{3}+q_{2} p_{6}+q_{3}\\ q_{4} p_{4}&q_{5} p_{5}&q_{4} p_{3}+q_{5} p_{6}+q_{6} \]

The proofs of the next two theorems are in Appendix G.

**Theorem 3.2**.: _Operator \(\) is associative: Given \(p,q,\) and \(r\), we have: \((p\)\(q)\)\(r=p(q r)\)._

**Theorem 3.3**.: _2D SSM recurrence can be done in parallel using parallel prefix sum algorithms with associative operator \(\), when fixing the variate._

### New Variant of 2D SSM: 2D Mamba

Figure 2 (Top-Left) shows the recurrence form of our 2D SSM. Each small square is a state of the system, i.e., the state of a variate at a certain time stamp. 2D SSM considers two hidden states for each state (represented by two colors: light red and blue), encoding the information along the time (red) and variate (blue), respectively. Furthermore, each arrow represents a transition matrix \(_{i}\) that decides to how information need to be fused. In this section, we discuss a spacial instance of our 2D SSM by limiting its parameters.

**2D Mamba.** We let \(_{2}=_{3}=\) in our 2D SSM. The resulting model is equivalent to:

\[h_{v,t+1}^{(1)}=}_{1}h_{v,t}^{(1)}+}_{1} _{v,t+1},\] (10)

\[h_{v+1,t}^{(2)}=}_{4}h_{v,t}^{(2)}+}_{2} _{v+1,t},\] (11)

\[_{v,t}=_{1}h_{v,t}^{(1)}+_{2}h_{v,t}^{(2)},\] (12)

where \(}_{1}=(_{1}_{1})\), \(}_{2}=(_{2}_{2})\), \(}_{1}=_{1}^{-1}(}_{ 1}-)_{1}^{(1)}\\ \), and \(}_{2}=\\ _{4}^{-1}(}_{4}-)_{2}^{( 2)}\). This formulation with data-dependent parameters, is equivalent to using two S6 blocks  each of which along a dimension. Notably, these two S6 blocks are not separate as the output \(_{v,t}\) is based on both hidden states \(h_{v,t}^{(1)}\) and \(h_{v,t}^{(2)}\), capturing 2D inductive bias.

### Chimera Neural Architecture

In this section, we use a stack of our 2D SSMs, with non-linearity in between, to enhance the expressive power and capabilities of the abovementioned 2D SSM. To this end, similar to deep SSM models , we allow all parameters to be learnable and in each layer we use multiple 2D SSMs, each of which with its own responsibility. Also, in the data-dependent variant of Chimera, we let parameters \(_{i},_{i},\) and \(_{i}\) for \(i\{1,2\}\) be the function of the input \(\):

\[_{i}=_{_{i}}(x),_{i}= _{_{i}}(x),_{i}=( _{_{i}}(x)).\] (13)

Chimera follows the commonly used decomposition of time series, and decomposes them into trend components and seasonal patterns. it, however, uses special traits of 2D SSM to capture these terms.

Seasonal Patterns.To capture the multi-resolution seasonal patterns, we take advantage of the discretization process. Proposition 3.1 states that if \((v,t)(v,t)\) with parameters \((\{}_{i}\},\{}_{i}\},\{}_{i}\}, _{1},_{2})\) then \((v,kt)(v,kt)\) with \((\{}_{i}\},\{}_{i}\},\{}_{i}\}, k_{1},_{2})\). Accordingly, we use \((.)\) module with a separate learnable \(_{s}\) that is responsible to learn the best resolution to capture seasonal patterns. Another interpretation for this module is based on \((p,q,s)\) (Equation 4). In this case, \(_{s}\) aims to learn a proper parameter \(s\) to capture seasonal patterns. Since we expect the resolution before and after this module matches, we add additional re-discretization module (a simple linear layer), after this module.

Trend Components.The second module of Chimera, \(_{t}(.)\) simply uses a sequence of multiple 2D SSMs to learn trend components. Proper combination of the outputs of this and the previous modules can capture both seasonal and trend components.

Both Modules Together.We followed previous studies  and consider residual connection modeling for learning trend and seasonal patterns. Given input data \(}_{0}=\), and \(=0,,\), we have:

\[}_{+1} =_{t}(}_{}),\] (14) \[}_{+1} =(_{s}( }_{}-}_{+1})).\] (15)

Figure 1 illustrate the architecture of Chimera. Due to the ability of our 2D SSM to recover smoothing techniques (see Theorem 3.4), this combination of modules for trend and seasonal patterns can be viewed as a generalization of traditional methods that use moving average with residual connection to model seasonality .

Gating with Linear Mapping.Inspired by the success of gated recurrent and SSM-based models [52; 33], we use a head of a fully connected layer with Swish , resulting in SwiGLU variant . While we validate the significance of this head, this

Closed-Loop 2D SSM Decoder.To enhance the generalizability and the ability of our model for longer-horizon, we extend the closed-loop decoder module , which is similar to autoregression, to multivariate time series. We use distinct processes for the inputs and outputs, using additional matrices \(_{1}\) and \(_{2}\) in each decoder 2D SSM, we model future input time-steps explicitly:

\[_{v,t} =_{1}h_{v,t}^{(1)}+_{2}h_{v,t}^{(2)},\] (16) \[_{v,t} =_{1}h_{v,t}^{(1)}+_{2}h_{v,t}^{(2)},\] (17)

where \(_{v,t}\) is the next input and \(_{v,t}\) is the output. Note that the other parts (recurrence) are the same as Equation 6. Figure 2 illustrate the architecture of closed-loop 2D SSM.

### Theoretical Justification

In this section, we provide some theoretical evidences for the performance of Chimera. These results are mostly revisiting the theorems by Zhang et al.  and Baron et al. , and extending them 

[MISSING_PAGE_FAIL:8]

[MISSING_PAGE_FAIL:9]

[MISSING_PAGE_FAIL:10]