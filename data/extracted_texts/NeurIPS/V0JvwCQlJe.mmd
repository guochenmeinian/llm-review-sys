# FairWire: Fair Graph Generation

O. Deniz Kose

Department of Electrical Engineering and Computer Science

University of California Irvine

Irvine, CA, USA

okose@uci.edu

&Yanning Shen

Department of Electrical Engineering and Computer Science

University of California Irvine

Irvine, CA, USA

yannings@uci.edu

corresponding author

###### Abstract

Machine learning over graphs has recently attracted growing attention due to its ability to analyze and learn complex relations within critical interconnected systems. However, the disparate impact that is amplified by the use of biased graph structures in these algorithms has raised significant concerns for their deployment in real-world decision systems. In addition, while synthetic graph generation has become pivotal for privacy and scalability considerations, the impact of generative learning algorithms on structural bias has not yet been investigated. Motivated by this, this work focuses on the analysis and mitigation of structural bias for both real and synthetic graphs. Specifically, we first theoretically analyze the sources of structural bias that result in disparity for the predictions of dyadic relations. To alleviate the identified bias factors, we design a novel fairness regularizer that offers a versatile use. Faced with the bias amplification in graph generation models brought to light in this work, we further propose a fair graph generation framework, FairWire, by leveraging our fair regularizer design in a generative model. Experimental results on real-world networks validate that the proposed tools herein deliver effective structural bias mitigation for both real and synthetic graphs.

## 1 Introduction

The volume of graph-structured data has been explosively growing due to the advancement in interconnected systems. In this context, machine learning (ML) over graphs attracts increasing attention [(1)], where specifically graph neural networks (GNNs) [(2; 3; 4)] have been proven to handle complex learning tasks over graphs, such as social recommendation [(5)], traffic flow forecasting [(6)].

Despite the increasing research focus on graph ML, the deployment of these algorithms in real-world decision systems requires guarantees preventing disparate impacts. Here, algorithmic disparity refers to the performance gap incurred by ML algorithms with respect to certain sensitive attributes protected by anti-discrimination laws or social norms (e.g., ethnicity, religion). While algorithmic bias is a concern over tabular data [(7; 8)], such bias becomes more critical for learning over graphs, as the use of graph structure in the algorithm design has been demonstrated to amplify the already existing bias [(9)]. Motivated by this, in this work, we specifically focus on structural bias and consequently the disparity in the predictions of dyadic relationships among nodes. Note that since the link predictionsare informed by the proximity principle (nodes connect to other nodes that are similar to themselves), the bias in graph topology is directly reflected in link prediction. For example, in a social network, the denser connectivity within people from the same ethnic group leads to higher recommendation rates within these groups and may cause segregation in social relations [(10)]. Hence, the development of a fair link prediction algorithm is of crucial importance to prevent potential segregation.

Fairness-aware algorithms typically require the knowledge of the sensitive attributes, the sharing of which can potentially create privacy concerns [(11)]. From a scalability perspective, sharing real graphs is also accompanied by difficulties due to the ever-increasing size of graphs. All these factors contribute to the value of synthetic graph generation for a number of applications, such as recommendation systems [(12)], anomaly detection [(13)]. For graph generation, data-driven models are shown to achieve state-of-the-art results [(14; 15; 16)], however, the fairness aspect of these models is under-explored. Recent works demonstrate that, in general, generative models tend to amplify the already existing bias in real data [(17; 18)], which is a potential issue for graph generation as well.

Faced with the aforementioned structural bias issues in graphs, in this work, we first carry out a theoretical analysis investigating the sources of such structural bias. Specifically, we deduce the factors that affect a commonly used bias metric, namely statistical parity [(19)], for link prediction. Guided by the theoretical findings, a novel fairness regularizer, \(_{}\) is designed, which can be utilized for various graph-related problems, including link prediction and graph generation. In addition, an empirical analysis for a graph generation model is carried out, which reveals that the use of generative algorithms amplifies the already existing structural bias in real graph data. To resolve this issue, we design a new diffusion-based fair graph generation framework, FairWire, which leverages the proposed regularizer \(_{}\). The training of diffusion model in FairWire is specifically designed to capture the correlations between the synthetic sensitive attributes and the graph connectivity, which enables fair model training with the existing techniques without revealing the real sensitive information. Overall, the contributions of this work can be summarized as follows: **c1)** A theoretical analysis that reveals the causes of disparity in the predictions of dyadic relations between nodes is derived. Differing from the existing analyses regarding the statistical parity in link prediction, our analysis considers a more general setting where sensitive attributes can be non-binary. **c2)** Based on the theoretical findings, we design a novel fairness regularizer, \(_{}\), which can be directly utilized for link prediction, as well as for graph generation models to alleviate the structural bias in a task-agnostic way. **c3)** We conduct an empirical analysis for the effect of graph generation models on the structural bias, which reveals the possible bias amplification related to these models. **c4)** FairWire, a novel fair graph generation framework, is developed by leveraging \(_{}\) within a diffusion model. The diffusion model is trained to capture the relations between the sensitive attributes and the graph topology, facilitating fair model training without private information leakage. **c5)** Comprehensive experimental results over real-world networks show that the proposed framework can effectively mitigate structural bias and create fair synthetic graphs.

## 2 Related Work

**Fairness-aware learning over graphs.** Fairness-aware graph ML has attracted increasing attention in recent years [(20; 21; 22)]. Existing works mainly focus on: \(1)\) Group fairness [(9; 23; 24; 25)], \(2)\) Individual fairness [(26; 27)], and \(3)\) Counterfactual fairness [(28; 29; 30)]. To mitigate bias in graph ML, different strategies are leveraged, including but not limited to adversarial regularization [(9; 24; 31; 32)], Bayesian debiasing [(33)], and graph editing [(28; 34; 35; 36; 37)]. With a specific focus on link prediction, [(19; 38)] propose fairness-aware strategies to alter the adjacency matrix, while [(39)] designs a fairness-aware regularizer. Differing from the majority of existing strategies, our proposed design herein is guided and supported by theoretical results. Specifically, we rigorously analyze the factors in graph topology leading to disparity for link prediction considering non-binary sensitive attributes. Furthermore, the developed bias mitigation tool can be employed in a versatile manner for training the link prediction models, as well as for training the generative models to create synthetic, fair graphs.

**Synthetic Graph Generation.** Generating synthetic graphs that simulate the existing ones has been a topic of interest for a long time [(40; 41)], for which the success of deep neural networks has been demonstrated [(12; 42; 43)]. Recently, the use of diffusion-based graph generative models has been increasing, due to their success in reflecting several important statistics of real graphs in the synthetic ones [44; 45; 46; 16; 47; 48].To the best of our knowledge, the only existing work that considers fair graph generation is  which only outputs a graph structure without nodal features, sensitive attributes and node labels, and also requires class labels as input. Furthermore, it focuses on the disparities in generation quality for different sensitive groups as the fairness metric, which may not be predictive for the fairness performance in downstream tasks. In contrast, our scheme herein does not require any class labels or training of a particular downstream task. In addition, differing from the existing diffusion models, our generation of graph topology and nodal features is guided by the sensitive attributes, which enables us to capture the correlations between the synthetic sensitive attributes and synthetic graph structure/nodal features. To the best of our knowledge, this work provides the first fairness-aware diffusion-based graph generation framework.

## 3 Preliminaries

Given an input graph \(:=(,)\), the focus of this study is investigating and mitigating the structural bias that may lead to unfair results for learning algorithms. Here, \(:=\{v_{1},v_{2},,v_{N}\}\) denotes the set of nodes and \(\) stands for the set of edges. Nodal features and the adjacency matrix of the input graph \(\) are represented by \(^{N F}\) and \(\{0,1\}^{N N}\), respectively, where \(_{ij}=1\) if and only if \((v_{i},v_{j})\). This work considers a single, potentially non-binary sensitive attribute for each node denoted by \(\{1,,K\}^{N}\). In addition, \(\{0,1\}^{N K}\) represents the one-hot encoding of the sensitive attributes. For the graphs with class information, \(^{N}\) denotes the class labels. Node representations output by the \(l\)th GNN layer are \(^{l+1}\), with \(_{i}^{l+1}^{F^{l+1}}\) denoting the learned hidden representations for node \(v_{i}\). \(_{i}^{F}\), and \(s_{i}\) represent the feature vector, and the sensitive attribute of node \(v_{i}\), respectively. Furthermore, \(_{k}\) denotes the set of nodes whose sensitive attributes are equal to \(k\). We define inter-edge set \(^{}:=\{e_{ij}|v_{i}_{a},v_{j}_{b},a  b\}\), and intra-edge set \(^{}:=\{e_{ij}|v_{i}_{a},v_{j}_{b}, a=b\}\). Similarly, \(d_{i}^{}:=_{v_{j}-_{a}}A_{ij}, v_{i} _{a}\) and \(d_{i}^{}:=_{v_{j}_{a}}A_{ij}, v_{i}_{a}\) are the inter- and intra-degrees of node \(v_{i}\), respectively. Finally, \(U_{}\) represents the discrete uniform distribution over the elements of set \(\).

## 4 Inspection and Mitigation of Structural Bias

This section first derives the conditions for a graph topology that leads to optimal statistical parity for link prediction. Guided by the obtained conditions, a fairness regularizer will then be presented. Statistical parity for link prediction is defined as \(_{}:=|_{(v_{i},v_{j}) U_{} U_{ }}[g(v_{i},v_{j}) s_{i}=s_{j}]-_{(v_{i},v_{j}) U_{ } U_{}}[g(v_{i},v_{j}) s_{i} s_{j}]|\) (19), where \(g(v_{i},v_{j})\) denotes the predicted probability for an edge between the nodes \(i\) and \(j\). To the best of our knowledge, our analysis is the first theoretical investigation for the relation between \(_{}\) and the graph topology considering multi-valued sensitive attributes, thus it generalizes previous findings with binary sensitive attributes [(19)].

### Bias Analysis

This subsection derives the conditions for a fair graph topology that achieves optimal statistical parity in the ensuing link prediction task. First, we will introduce the GNN model considered in this work.

**GNN model:** Throughout the analysis, a stochastic graph view, \(}\), is adopted, i.e., \(_{ij}\) denotes the probability of an edge between the nodes \(v_{i}\) and \(v_{j}\), and \(_{ij}=_{ji}\). Let \(^{l+1}\) represent the aggregated representations by the \(l\)th GNN layer with \(i\)th row \(_{}}[_{i}^{l+1}]:=_{v_{j}}_{ij}_{j}^{l+1}\), where \(_{i}^{l+1}:=^{l}_{i}^{l}\). Then, the hidden representation output by \(l\)th GNN layer for node \(v_{i}\) can be written as \(_{}}[_{i}^{l+1}]=(_{v_{j} }_{ij}^{l}_{j}^{l})=(_ {}}[_{i}^{l+1}])\), where \(^{l}\) is the weight matrix and \(()\) is the non-linear activation employed in the \(l\)th GNN layer.

The following assumptions are made for Theorem 1 that will be presented in this subsection:

**A1:**\(\|_{i}\|_{}, v_{i}\).

**A2:**\(_{}}[d_{i}^{}]}{|_{k}|} _{}}[d_{i}^{}]}{N-|_{k}|},  v_{i}_{k}, k\{1,,K\}\).

**A3:**\(_{v_{i},v_{j}}_{ij}_{}}[d _{i}^{}], v_{i}\).

These assumptions are naturally satisfied by most of the real-world graphs. Assumption **A1** implies that the representations, \(_{i}\)'s, are finite. For **A2**, note that most of the real-world social networks have considerably more intra-edges than inter-edges (50), i.e., \(_{}}[d_{i}^{}]_{}}[d_{i}^{}]\). Thus, unless \(|_{i}|\|_{j}|\), \(i j\) (extremely unbalanced sensitive group sizes), **A2** holds. Finally, **A3** holds with high probability as \(_{}}[d_{i}^{}]=_{v_{i}_{s_{i}},v_{j}-_{s_{i}}}_{ij}\). We also demonstrate that these assumptions are valid for the real-world networks we are using in Appendix A in order to further justify them.

Building upon these assumptions, Theorem 1 reveals the factors leading to the disparity between the representations of different sensitive groups obtained at any GNN layer. Specifically, it upper bounds the term \(_{k}^{(l+1)}:=\|_{},v_{i} U_{ _{k}}}[_{i}^{l+1} s_{i}=k]-_{},v_{i } U_{(-_{k})}}[_{i}^{l+1} s_{i} k ]\|_{2}\). The proof of the theorem is presented in Appendix B.

**Theorem 1**.: _The disparity between the representations of nodes in a sensitive group \(_{k}\) and the representations of the remaining nodes output by the \(l\)th GNN layer, \(_{k}^{(l+1)}\), can be upper bounded by:_

\[_{k}^{(l+1)} L}|^{}}{|_{k}|}-^{}}{N-|_{k}|}| +|,v_{j}}_{ij}-p_{k}^{}-2p_ {k}^{}}{N-|_{k}|}-^{}}{|_{k}|}| +2_{z},\] (1)

_where \(L\) is the Lipschitz constant of the activation function \(()\), \(\|_{i}^{l+1}-(_{j}^{l+1} v_{j} )\|_{}_{z}\), \( v_{i}\), and \(p_{k}^{}:=_{v_{i}_{k},v_{j}_{k}} {A}_{i,j},p_{k}^{}:=_{v_{i}_{k},v_{j}_{k} }_{i,j}\)._

Representation disparity resulting from the aforementioned GNN-based aggregation is examined and explained by Theorem 1. The commonly used fairness measures, such as statistical parity (19), are naturally a function of the representation disparity. Herein, we further investigate the said relation between the representation disparity and \(_{}\) mathematically. Specifically, for a link prediction model described by a function \(g(v_{i},v_{j}):=_{i}^{}_{j}\), Proposition 1 directly upper bounds \(_{}\). Here, \(_{i}\) denotes the representation for node \(v_{i}\) that is employed for the link prediction task, i.e., the hidden representations in the final layer. The proof of Proposition 1 is presented in Appendix C.

**Proposition 1**.: _For a link prediction model described by \(g(v_{i},v_{j}):=_{i}^{}_{j}\), \(_{}\) can be upper bounded by:_

\[_{}_{k=1}^{K}_{k}|}{N}q\| \|_{2}^{max},\] (2)

_where \(\|_{i}\|_{2} q, v_{i}\), \(^{max}:=_{k}(\|_{},v_{i} U_{ _{k}}}[_{i} s_{i}=k]-_{},v_{ i} U_{(-_{k})}}[_{j} s_{j} k ]\|_{2})\)._

Combining the findings of Theorem 1 and Proposition 1, Corollary 1 further demonstrates the factors (including the topological ones) that affect the resulting statistical parity in the link prediction task.

**Corollary 1**.: _For a link prediction model \(g(v_{i},v_{j}):=(_{i}^{L+1})^{}_{j}^{L+1}\), where \(_{j}^{L+1}\) is the representation created by \(L\)th (final) GNN layer, \(_{}\) can be upper bounded by:_

\[_{}_{k=1}^{K}_{k}|}{N}q\| \|_{2}L}_{1}+_{2} +2_{z},\]

_where \(_{1}:=|^{}}{|_{k}|}-^{}} {N-|_{k}|}|\) and \(_{2}:=|,v_{j}}_{ij}-p_{k}^{ }-2p_{k}^{}}{N-|_{k}|}-^{}}{| _{k}|}|\)._

### A Regularizer for Fair Connections

The bias analysis in Subsection 4.1 brings to light the factors resulting in topological bias for a probabilistic graph connectivity. Corollary 1 shows that the topological bias can be minimized if \(_{1}=0\) and \(_{2}=0\). One can obtain \(_{1}=0\) by ensuring \(^{}}{p_{k}^{}}=_{k}|}{N-|_{k }|}, k\). Meanwhile, \(_{2}=0\) if \(p_{k}^{}=_{v_{i},v_{j}}(})-c|_{k}|-cN\) and \(p_{k}^{}=c|_{k}|\) for any constant \(c\). Overall, the optimal values of \(p_{k}^{}\) and \(p_{k}^{}\) that minimize both \(_{1}\) and \(_{2}\) follow as \((p_{k}^{})^{*}=,v_{j}}_{ij}| _{k}|^{2}}{N^{2}}\) and \((p_{k}^{})^{*}=,v_{j}}_{ij})(N| _{k}|-|_{k}|^{2})}{N^{2}}\). Therefore, in order to mitigate structural bias, we can design a regularizer that pushes the expected number of inter-edges and intra-edges towards \((p_{k}^{})^{*}\) and \((p_{k}^{})^{*}\): \(:=_{k=1}^{K}|_{v_{i},v_{j}}(} (_{k})(_{k})^{})_{i,j}-(p_{k}^{})^{*}|+| _{v_{i},v_{j}}(}(_{k})( -(_{k}))^{})_{i,j}-(p_{k}^{})^{*}|+|_{v_{i},v _{j}}(}(_{k})(-( _{k}))^{})_{i,j}-^{}}{N-|_{k}|}\).

\((p_{k}^{})^{*}|\). Here, \(_{k}^{K}\) is the basis vector with only non-zero entry \(1\), at the \(k\)th element, and \(\) denotes the Hadamard product. Note that such a regularizer is compatible with any learning algorithm that outputs probabilities of all possible edges in the graph, e.g., topology inference algorithms.

Although \(\) can be applied to several graph ML algorithms and its theory-guided design can promise effective topological bias mitigation, its design requires a single-batch learning setting due to the definitions of \(p_{k}^{}\) and \(p_{k}^{}\) (resulting in a complexity growing exponentially with \(N\)). Specifically, \(p_{k}^{}\) and \(p_{k}^{}\) are calculated based on all edge probabilities related to the all nodes in \(_{k}\). Therefore, regularizing the values of \(p_{k}^{}\) and \(p_{k}^{}\) will lead to scalability issues for large graphs. To tackle this challenge, we only focus on the optimal ratio between the expected number of intra- and inter-edges, i.e., \(^{}}{p_{k}^{}}=_{k}|}{N-|_{ k}|}, k\{1,,K\}\), which is governed by \(_{1}\). The idea is to manipulate the ratio between the expected number of intra- and inter-edges in each mini-batch of nodes for a better scalability. We call the corresponding batch-wise fairness regularizer \(_{}\), which follows as

\[_{}(},):=_{k=0}^{K} ,v_{j}}(}( _{k})(_{k})^{})_{ij}}{|_{k}|}-,v_{j}}(}(_{k})(-(_{k}))^{})_{ij}}{N-|_{k}|},\] (3)

where \(\) denotes the set of nodes within the utilized minibatch. Note that the aforementioned versatile use of \(\) also applies to \(_{}\), which can directly be used in topology inference tasks. Specifically, for link prediction, the following loss function can be employed in training to combat bias:

\[_{lp}=_{v_{i},v_{j}}_{CE} }_{ij},_{ij}+_{ }(},),\] (4)

where \(}_{ij}\) denotes the predicted probability by the algorithm for an edge between \(v_{i}\) and \(v_{j}\), and \(_{CE}\) is cross-entropy loss. The hyperparameter \(\) is used to adjust the weight of fairness in training.

## 5 Fair Graph Generation

Generating synthetic graphs that capture the structural characteristics in real data attracts increasing attention as a promising remedy for scalability (ever-increasing size of real-world graphs) and privacy issues. Especially, sharing real sensitive attributes for fair model training exacerbates the privacy concerns due to the sensitive attribute leakage problem (11). Thus, creating synthetic graphs with generative models becomes instrumental in applications over interconnected systems. In this work, we focus on diffusion models whose success in capturing the original data distribution has been shown for various types of networks [45; 46; 16; 47]. Despite the growing interest in these models, their effects on fairness have not yet been investigated, which limits their use in critical real-world decision systems. Motivated by this, in Subsection 5.1, we first empirically analyze the impact of diffusion models on the algorithmic bias by comparing the original and synthetic graphs in terms of different fairness metrics for link prediction. This empirical investigation reveals that the algorithmic bias is amplified while using generative models for graph creation. To resolve this critical issue, we develop FairWire in Subsection 5.2, a fair graph generation framework, which leverages our proposed regularizer \(_{}\) during the training of a diffusion model.

### Diffusion Models and Structural Bias

To evaluate the effect of synthetic graph generation on bias, we first sample \(10\) different synthetic graphs for each of the \(4\) real-world networks (see Table 7 in Appendix E and Subsection 6.1 for more details on the datasets). Synthetic graphs are sampled using a diffusion model that is trained following the setup in (47), which is a state-of-the-art algorithm for diffusion-based graph generation. Upon creating graphs, we evaluate them for the link prediction task on the same test set (generated from the real data) and report the corresponding utility (AUC) and fairness performance. Fairness performance is measured via two widely used bias metrics, statistical parity (\(_{}\)) and equal opportunity (\(_{}\))  for which lower values indicate better fairness (see Subsection 6.1 for more details on the link prediction model and evaluation metrics). The obtained results are presented in Table 1.

In Table 1, \(\) denotes the original graphs, and the synthetic graphs are represented by \(}\). Overall, Table 1 shows that graph generation via diffusion models indeed amplifies the already existing bias in the original graphs consistently for all the considered datasets. This brings the potential bias-related issues in synthetic graph creation to light and calls for robust bias mitigation solutions.

### FairWire: A Fair Graph Generation Framework

The proposed fairness regularizer in Subsection 4.2, \(_{}\), can be utilized in two different settings: \(i)\) during model training for link prediction, \(ii)\) for training a graph generation model in a task-agnostic way. Note that for both cases, a model is trained to predict a probabilistic graph adjacency matrix, \(}\), upon which \(_{}\) can be employed. Both use cases can facilitate several fairness-aware graph-based applications. That said, the bias amplification issue in generative models (also observed from Table 1) makes creating fair graphs via graph generation models of particular interest.

The proposed fair graph generation framework, FairWire, is built upon structured denoising diffusion models for discrete data (51). In the forward diffusion, FairWire employs a Markov process to create noisy graph data samples by independently adding or deleting edges. For denoising, a message-passing neural network (MPNN) is trained to predict the clean graph based on noisy samples by using the guidance of sensitive attributes (and node labels if available). Finally, we sample synthetic graphs with the guidance of synthetic sensitive attributes that are initialized based on their distribution in the original data. If input graph has also node labels, during graph generation, these node labels are sampled based on their distribution conditioned on the sensitive attributes in the original graph. In the sequel, as our main novelty lies in the denoising process, we discuss the training process of FairWire (reverse diffusion process) in more detail, while the forward diffusion and sampling processes are explained in Appendix D. Note that the diffusion process is presented for attributed graphs, where synthetic nodal features \(}\) are also generated. However, the proposed approach can be readily adapted to graphs without nodal features.

**Reverse diffusion process:** For denoising, we train an MPNN, \(_{}\) parametrized by \(\), which is shown to be a scalable solution for the generation of large, attributed graphs (47). Specifically, \(_{}\) inputs a noisy version of the input graph and the original sensitive attributes described by \(^{t},^{t},\) and aims to recover the original nodal features \(^{0}\) and graph topology \(^{0}\). Here, \(^{0}^{N N 2}\) denotes the one-hot representations for the edge labels. Note that the sensitive attributes are used to guide the MPNN to capture the relations between them and graph topology. Therefore, the sensitive attributes are initialized and kept the same during both training and sampling (the original distribution of sensitive attributes is used to initialize them during sampling). For a node \(v\), the message passing at the \(l\)th layer can be described as:

\[_{v}^{(t,l+1)}=_{T H}^{(l)} _{t}+_{H}^{(l)}+_{u^{(t)}(v)} ^{(t)}(v)|}[_{u}^{(t,l)}|| _{u}^{(l)}]_{[H,S] H}^{(l)},\] (5)

\[_{v}^{(l+1)}=_{S}^{(l)}+_{u ^{(t)}(v)}^{(t)}(v)|}_{ u}^{(l)}_{S S}^{(l)},\] (6)

where \(\|\) stands for the concatenation operator. In this aggregation, \(_{T H}^{(l)},_{[H,S] H}^{(l)},_{S S}^{(l)},_{H}^{(l)}\) and \(_{S}^{(l)}\) are all learnable parameters, while \(()\) consists of ReLU (52) and LayerNorm (53) layers. In addition, \(^{(t,0)}\) and \(_{t}\) are initialized as hidden representations created for \(^{t}\) and time step \(t\) via multi-layer perceptrons (MLP), respectively, and \(^{(0)}=\). After creating hidden representations for nodes and their sensitive attributes, final representation for a node \(v\) is generated via \(_{v}=_{v}^{(t,0)}\|_{v}^{(t,1)}\|\| _{v}^{(0)}\|_{v}^{(1)}\|\|_{t}\). Note that when node labels are available, their one-hot representations \(\), are also employed in this MPNN in the same way as \(\) are utilized. Based on these final representations, node attributes, and edge labels are predicted. To create fair graph connections in the synthetic graphs, we regularize the predicted edge probabilities, \(}\), via the designed fairness

    &  &  \\   & Accuracy (\%) & \(_{SP}\) (\%) & \(_{EO}\) (\%) & Accuracy (\%) & \(_{SP}\) (\%) & \(_{EO}\)(\%) \\  \(\) & \(94.92\) & \(27.71\) & \(11.53\) & \(95.76\) & \(29.05\) & \(9.53\) \\ \(\) & \(87.29 1.09\) & \(35.72 1.74\) & \(13.27 0.81\) & \(92.19 1.06\) & \(37.56 1.29\) & \(13.52 0.92\) \\   &  \\   & Accuracy (\%) & \(_{SP}\) (\%) & \(_{EO}\) (\%) & Accuracy (\%) & \(_{SP}\) (\%) & \(_{EO}\)(\%) \\  \(\) & \(96.91\) & \(32.58\) & \(8.24\) & \(96.14\) & \(22.90\) & \(4.63\) \\ \(\) & \(94.45 0.21\) & \(33.49 0.28\) & \(10.01 0.56\) & \(94.04 0.26\) & \(23.56 0.55\) & \(6.23 0.49\) \\   

Table 1: Comparative resultsregularizer \(_{}\). Overall, the training loss of the MPNN follows as:

\[_{v_{i}}_{CE}}_{i :},_{i:}^{0}+_{v_{i},v_{j}} _{CE}}_{ij:},_{ij:}^{0}+ _{}(},),\] (7)

where \(\) adjusts the focus on the fairness regularizer.

Remark (Applicability to general generative models):Although the designed regularizer in Subsection 4.2 is embodied in a diffusion-based graph generation framework in Subsection 5.2, \(_{}\) can be utilized in any generative model outputting synthetic graph topologies as a fairness regularizer on the connections, including but not limited to graph autoencoder-based or random walk-based graph generation models.

Remark (Creation of synthetic sensitive attributes):We design a generative framework in Subsection 5.2 that outputs synthetic sensitive attributes whose effect on the connections is reflected by inputting them in the training of MPNN. We emphasize that the creation of these synthetic sensitive attributes also enables the use of existing fairness-aware schemes on the created graphs without leaking the real sensitive attributes.

## 6 Experiments

### Datasets and Experimental Setup

**Datasets.** In the experiments, four attributed networks are employed, namely Cora, Citeseer, Amazon Photo and Amazon Computer for link prediction. Cora and Citeseer are widely utilized citation networks, where the articles are nodes and the network topology depicts the citation relationships between these articles (54). Amazon Photo and Amazon Computer are product co-purchase networks, where the nodes are the products and the links are created if two products are often bought together (55). In addition to link prediction, we also evaluate the synthetic graphs on node classification, where the German credit (56) and Pokec-n (9) graphs are employed. For more details on the datasets and their statistics, please see Appendix E.

Experimental Setup.In this section, we first report the performance of \(_{}\) for link prediction. For this task, the area under the curve (AUC) is employed as the utility metric. As fairness metrics, statistical parity and equal opportunity definitions in (19; 37) are used, where \(_{}:=|_{(v_{i},v_{j}) U_{} U_{ }}[_{ij}=1 s_{i}=s_{j}]-_{(v_{i},v_{j}) U _{} U_{}}[_{ij}=1 s_{i} s_{j}]|\) and \(_{}:=|_{(v_{i},v_{j}) U_{} U_{ }}[_{ij}=1 A_{ij}=1,s_{i}=s_{j}]-_{(v_{i},v _{j}) U_{} U_{}}[_{ij}=1 A_{ij}= 1s_{i}, s_{j}]|\). Lower values for \(_{}\) and \(_{}\) indicate better fairness performance.

To evaluate the generated synthetic graphs, we use both the link prediction and node classification tasks. Herein, we sample 10 synthetic graphs for each dataset with the trained diffusion models. Afterward, we train link prediction/node classification models (for more details on these models, please see Appendix G) on the sampled graphs, and test these models on the real graphs \(\) (the test set is the same for all baselines and FairWire). Here, we consider the scenario where there is no access to the real graphs due to privacy concerns, and the models are trained on the synthetic graphs for downstream tasks. To evaluate these synthetic graphs on link prediction, the same utility and fairness metrics as in the link prediction task are used. For node classification, accuracy is employed as the utility measure with \(_{SP}:=|P(_{j}=1 s_{j}=0)-P(_{j}=1 s_{j}=1)|\), and \(_{EO}:=|P(_{j}=1 y_{j}=1,s_{j}=0)-P(_{j}=1 y_{j}=1,s_{j}=1)|\) being the fairness metrics.

For more details on the training of link prediction, node classification, diffusion models, and the hyperparameter selection for FairWire and baselines, see Appendix G. A sensitivity analysis is also provided in Appendix H for the effect of hyperparameter \(\) in (4) and in (7). Note that the performance of the generative algorithms is generally reported in terms of the distances between the statistics of real data and the synthetic ones, instead of the fairness performance. For completeness, we report the distance metrics for node degree distribution and clustering coefficient distribution in Appendix F.

Baselines.For link prediction, fairness-aware baselines include adversarial regularization (9), FairDrop (37), and FairAdj (19). For graph generation, FairGen (49), is the only existing fairness-aware baseline designed for node classification. For a comprehensive evaluation, we also employ adversarial regularization (9) and FairAdj (19) as in-processing and post-processing fairness-aware strategies within the generative model. For more details on the baselines, please see Appendix G.

[MISSING_PAGE_FAIL:8]

both link prediction and node classification. Specifically, FairWire can achieve improvements in both \(_{SP}\) and \(_{EO}\) ranging from \(25\%\) to \(90\%\) for all datasets compared to \(}\) with similar utility.

Note that, for link prediction, the fairness improvement reported for FairAdj in Table 3 is accompanied by a significant utility drop. Specifically, for a larger \(\) value (i.e., \(=1\)), FairWire can provide better fairness measures (\(_{SP}=7.01 6.25\) and \(_{EO}=3.06 2.95\)) on Citeseer with a similar accuracy (\(83.47 7.79\)) to FairAdj. Thus, the results in Table 3 demonstrate that FairWire provides a better utility/fairness trade-off compared to fairness-aware baselines on all evaluated datasets.

In Table 4, similar to the link prediction experiments (Table 2), the results of FairAdj for the Pokec-n network could not be obtained due to computational limitations. For FairGen (49), we directly input the synthetic graph output by the algorithm to the node classification model we train, thus the results are obtained for a single synthetic graph. A possible explanation for the better accuracy of FairGen on the German dataset is that the algorithm is observed to output a denser synthetic network, which might be useful for the utility. It is observed that the synthetic graph output by FairGen for Pokec-n was not informative enough for the node classification task (we provide the codes for the FairGen algorithm in our supplementary material for the reproducibility of these results.) All in all, the results in Table 4 signify that the superior performance of FairWire in terms of fairness/utility trade-off can also be observed for node classification, which validates the efficacy of FairWire in creating fair synthetic graphs that also capture the real data distribution.

### Visualization of Synthetic Graphs

Our analysis in Subsection 4.1 reveals that the ratio of intra-(edges connecting the same sensitive group) and inter-edges (edges between different sensitive groups) is a factor contributing to the structural bias. Specifically, the bias factor \(_{1}\) is minimized when \(^{}}{p_{k}^{}}=_{k}|}{N-| _{k}|}, k\), where \(p_{k}^{}\) and \(p_{k}^{}\) are the expected number of intra-and inter-edges for the nodes in \(_{k}\). This finding suggests that for a graph with multiple (\(>2\)) sensitive groups, given the sizes of sensitive groups are not catastrophically unbalanced, the number of inter-edges (related to \(p_{k}^{}\)) should be larger than the number of intra-edges (related to \(p_{k}^{}\)) to alleviate structural bias (i.e., \(|_{k}| N-|_{k}|\)). However, for graphs encountered in several domains, the number of intra-edges is significantly larger than the number of inter-edges, due to the homophily principle (10). Motivated by this, in Figure 1, we visualize the distributions of intra- and inter-edges in synthetic graphs created by i) a fairness-agnostic strategy, GraphMaker (47), and ii) FairWire, for Cora. In Figure 1, intra- and inter-edges are colored with blue and red, respectively. Figure 1 reveals that the graph created by GraphMaker (47) predominantly consists of intra-edges, leading to the structural bias reflected in Table 3. In contrast, FairWire exhibits a remarkable balancing effect, which provides a potential explanation for the improvement in fairness.

    &  &  \\   & Acc (\(\%\)) & \(_{SP}\) (\(\%\)) & \(_{EO}\) (\(\%\)) & Acc (\(\%\)) & \(_{SP}\) (\(\%\)) & \(_{EO}\)(\(\%\)) \\  \(\) & \(70.00\) & \(2.13\) & \(1.78\) & \(68.73\) & \(8.58\) & \(9.68\) \\  FairGen & \(\) & \(28.71\) & \(15.34\) & \(51.73\) & \(0.00\) & \(0.00\) \\  \(}\) & \(68.92 2.37\) & \(2.61 5.83\) & \(2.29 5.06\) & \(66.19 2.05\) & \(3.63 2.58\) & \(2.66 2.50\) \\ FairAdj & \(70.08 1.08\) & \(2.17 4.49\) & \(1.11 2.24\) & - & - & - \\ Adversarial & \(70.00 0.62\) & \(1.57 2.70\) & \(1.34 2.86\) & \( 0.70\) & \(2.16 1.73\) & \(2.73 2.01\) \\ FairWire & \(69.76 0.51\) & \( 1.53\) & \( 0.61\) & \(68.23 0.45\) & \( 0.92\) & \( 0.92\) \\   

Table 4: Comparative results for graph generation on Node Classification.

Figure 1: Distribution of the intra-edges (blue) and inter-edges (red) in the synthetic graphs created for Cora dataset by GraphMaker (47) (left) and FairWire (right).

Conclusion

This study focuses on the investigation and mitigation of structural bias for both real and synthetic graphs, where a novel fairness regularizer, \(_{}\), is designed to alleviate the effects of bias factors identified in a developed theoretical bias analysis. Furthermore, the proposed fairness regularizer is leveraged in a fair graph generation framework, FairWire, which alleviates the bias amplification observed in graph generative models. Experimental results corroborate the effectiveness of the proposed tools in bias mitigation for both real and synthetic graphs.

**Limitations:** This paper considers the setting where sensitive attributes are available during model training, which might limit its use for certain real-world applications. Thus, one future direction of this work would be to consider the partial availability of these sensitive attributes in the input graph data. Furthermore, although we showed that real-world graphs typically satisfy the assumptions in Subsection 4.1, another possible future work we consider is deriving a theoretical bias analysis without the dependency on these assumptions.