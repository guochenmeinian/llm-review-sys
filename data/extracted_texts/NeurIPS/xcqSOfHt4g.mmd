# Simplified and Generalized

Masked Diffusion for Discrete Data

 Jiaxin Shi\({}^{*}\), Kehang Han\({}^{*}\), Zhe Wang, Arnaud Doucet, Michalis K. Titsias

Google DeepMind

Equal contribution. Correspondence to: jiaxins@google.com.

###### Abstract

Masked (or absorbing) diffusion is actively explored as an alternative to autoregressive models for generative modeling of discrete data. However, existing work in this area has been hindered by unnecessarily complex model formulations and unclear relationships between different perspectives, leading to suboptimal parameterization, training objectives, and ad hoc adjustments to counteract these issues. In this work, we aim to provide a simple and general framework that unlocks the full potential of masked diffusion models. We show that the continuous-time variational objective of masked diffusion models is a simple weighted integral of cross-entropy losses. Our framework also enables training generalized masked diffusion models with state-dependent masking schedules. When evaluated by perplexity, our models trained on OpenWebText surpass prior diffusion language models at GPT-2 scale and demonstrate superior performance on 4 out of 5 zero-shot language modeling tasks. Furthermore, our models vastly outperform previous discrete diffusion models on pixel-level image modeling, achieving 2.75 (CIFAR-10) and 3.40 (ImageNet 64\(\)64) bits per dimension that are better than autoregressive models of similar sizes. Our code is available at https://github.com/google-deepmind/md4.

## 1 Introduction

Since their inception [1; 2; 3], diffusion models have emerged as the workhorse for generative media, achieving state-of-the-art in tasks such as image synthesis [4; 5; 6], audio [7; 8] and video generation [9; 10; 11; 12; 13]. The majority of existing successes are for continuous state space diffusions. While diffusion models have been extended to discrete state spaces [1; 14; 15] and have been successfully applied to applications ranging from graph generation , text-to-sound generation  or protein design , they remain not as widely used as their continuous counterparts as they are not competitive with autoregressive models in important domains such as text modeling. This has motivated the development of continuous space diffusion models where the discrete data are embedded in the Euclidean space [19; 20; 21; 22; 23] or the simplex [24; 25; 26; 27; 28]. We believe that one of the reasons for the limited success of discrete diffusions is that they have been hindered by fairly complex formulations and training objectives. This paper is a step towards closing this gap.

In this work, we focus on "masked" (or "absorbing") diffusions, a discrete diffusion formulation first presented by Austin et al. , and later explored by the literature from various perspectives [29; 30; 31; 32]. We follow here a continuous-time framework which has proven very useful to improve the training and understanding of continuous state space diffusions [see e.g., 3; 33; 34]. We make several technical contributions which simplify the training of these models and improve significantly their performance. Our contributions are as follows:

* Using elementary arguments, we establish several properties for the forward process induced by this model and its corresponding time reversal, improving our understanding of this model class.

* We provide a remarkably simple expression of the Evidence Lower Bound (ELBO) for masked diffusion models, showing that it corresponds to a weighted integral over time of cross-entropy losses. Similarly to continuous space diffusions , this objective can be rewritten in terms of signal-to-noise ratio and exhibits invariance properties.
* We develop a unifying understanding of previously proposed continuous-time discrete diffusion models [29; 32; 35], revealing the changes they made to our ELBO objective and/or model parameterization. We show that these changes either lead to expensive model evaluations, or large variance in training, or breaking the consistency between forward and reverse processes.
* On GPT-2 scale text modeling and pixel-level image modeling tasks, masked diffusions trained using our simple ELBO objective outperform previous proposals, leading to the best likelihood and zero-shot transfer performance among discrete diffusion models.
* Finally, based on our simplified masked diffusion formulation, we propose a generalized masked diffusion model that allows state-dependent masking schedules. This generalized masked diffusion model further improves predictive performance measured by test likelihoods.

Concurrent work by Ou et al.  and Sahoo et al.  derives a similar simplified expression of the ELBO. Ou et al. 's derivation relies on an observation similar to the one we made in Proposition 1.

## 2 Masked Diffusion

Consider a sentence where we progressively replace each word with a special mask token, transforming the sentence into a sequence of masks. Our goal is to train a generative model that reverses this process, effectively turning a sentence of masks back into meaningful text. More formally, assume our data consists of tokens from a finite discrete state space with \(m\) possible states, represented by integers \(0,1,,m-1\) and their corresponding one-hot vectors \(e_{0},e_{1},,e_{m-1}\). To accommodate the masking process, we augment this space with an additional mask state, denoted by the index \(m\). The masking process transitions each token to the mask state at a random time. This process, known as the forward process, is applied independently to each token (e.g., each word), progressively converting the data into a sequence of mask tokens. By learning to reverse this masking process, we create a generative model capable of producing coherent discrete data.

Discrete-time forward process.We start with the case of a single token and later expand to multiple dimensions. We define the forward process as a Markovian sequence of discrete random variables \(x_{t}\) indexed by time \(t\), where \(t\) runs from 0 to 1. Throughout the work, we abuse the notation such that \(x_{t}\) can be either an integer or its corresponding one-hot vector, whenever it is clear from the context. We divide \(\) into \(T\) intervals, and let \(s(i)=(i-1)/T\), \(t(i)=i/T\). Following Austin et al. , the state transition between \([s(i),t(i)]\) is determined by a transition matrix of size \((m+1)(m+1)\): \(Q_{i}=(1-_{i})I+_{i}e_{m}^{}\), where \(\) is an all-one vector of size \(m+1\), \(e_{m}\) represents a one-hot vector where element at index \(m\) is 1. Each entry \([Q_{i}]_{jk}\) denotes the probability of transition from the state \(j\) to the state \(k\):

\[[Q_{i}]_{jk}=q(x_{t(i)}=k|x_{s(i)}=j)=(1-_{i})_{jk}+_{i} _{km}.\]

This means that, with probability \(1-_{i}\), \(x_{t(i)}=x_{s(i)}\), otherwise it jumps to the mask state. Given the above transition matrix, the marginal distribution at time \(t(i)\) given \(x_{0}\) is

\[q(x_{t(i)}|x_{0})=(x_{t(i)};_{i}^{}x_{0})=x_{0}^{} _{i}x_{t(i)}.\]

Here, we use \((x;p)\) to denote a Categorical distribution where \(p\) is the vector of probabilities of being in each category, and \(_{i}_{j=1}^{i}Q_{j}=_{i}I+1-_{i} e_{m}^{}\) for \(_{i}=_{j=1}^{i}(1-_{j})\). We expect \(_{T}\) to become very small or zero for a sufficiently large \(T\) such that \(q(x_{1}|x_{0})\) for any \(x_{0}\) will become a delta mass at the mask state.

Continuous-time limit.We can define a continuous-time forward process by taking a limit of the above discrete-time process. We first specify a continuous function \((t)\) such that \(_{i}=(t(i))/T\). We then let \(T\) in the discrete-time process and compute the limit of \(_{i}\) (proved in Austin et al. 14, Appendix A.6, see also App. A) as

\[(t)_{T}_{i}=_{t}I+(1-_{t}) e_{m}^{},_{t}-_{0}^{t}(s)s,\] (1)so that \(q(x_{t}|x_{0})=(x_{t};(t)^{}x_{0})\). For two arbitrary times, \(0 s<t 1\), the transition distribution that is compatible with the above marginal (i.e., \(q(x_{t}|x_{0})=_{x_{s}}q(x_{t}|x_{s})q(x_{s}|x_{0})\)) is

\[q(x_{t}|x_{s})=(x_{t};(s,t)^{}x_{s}),(s,t)(s)^{-1}(t)=}{_{s} }I+1-}{_{s}}e_{m}^{}.\]

Note that Austin et al.  did not derive this explicit form of transition matrix between two arbitrary time \(s\) and \(t\), which appeared later in Zhao et al.  concurrently with our work.

Masking schedules.From the definition of \(_{t}\), we have that \(_{0}=1\). And similar to the discrete-time formulation, we would like \(_{1}\) be zero or very close to zero. We provide a summary of masking schedules from literature that satisfy these properties in Fig. 1. The linear schedule was proposed in Sohl-Dickstein et al.  for binary variables and then re-derived by Austin et al.  from mutual information for discrete-time models. The geometric schedule \(_{t}\) is plotted for \(_{}=10^{-5}\) and \(_{}=20\). It was first used for continuous diffusions  and then for discrete by Lou et al. . The cosine schedule was originally proposed in MaskGIT , an iterative unmasking generative model inspired by diffusion. This schedule has the property of slowing down the unmasking process at the beginning of the reverse generation. Aligning with their observation, we find that this results in a lower chance of conflicting tokens being unmasked simultaneously at the start of generation, thereby enhancing the overall generation quality.

Time reversal of the forward process given \(x_{0}\).The analytic property of our forward process allows to compute many quantities of interest in closed form. One such quantity frequently used in diffusion models is the time reversal of the forward process given \(x_{0}\): \(q(x_{s}|x_{t},x_{0})\) for \(s t\). We derive it in App. C as

\[q(x_{s}|x_{t},x_{0})=(x_{s};^{x_{0}}(t,s)^{}x_{t}), ^{x_{0}}(t,s)=I+-_{t}}{1-_{t}}e_{m}(x_{0 }-e_{m})^{}.\]

From the transition matrix \(^{x_{0}}(t,s)^{(m+1)(m+1)}\) we can see the reverse process conditioned on \(x_{0}\) has a very simple logic--if \(x_{t}\) is a mask, with probability \(-_{t}}{1-_{t}}\), it will jump to the state \(x_{0}\) at time \(s\), otherwise it will stay masked. Once \(x_{t}\) is unmasked, it remains in the same state until the end.

## 3 Model and Objective

For a discrete-time masked diffusion process, we define our generative model by approximately reversing the forward transitions using a reverse model \(p_{}(x_{s}|x_{t})\). One way to define this model is

\[p_{}(x_{s}|x_{t}) q(x_{s}|x_{t},_{}(x_{t},t)),\] (2)

where \(_{}(x_{t},t)^{m+1}\) is a probability vector parametrized by a neural network \(f_{}\) with a softmax applied to the output logits (note the \(m\)-th output is forced to 0 since the clean data cannot be masks):

\[_{}(x_{t},t)=(f_{}(x_{t},t))&x_{t }=m,\\ x_{t}&x_{t} m.\] (3)

This is known as mean-parameterization since it leverages a prediction model for the mean of \(x_{0}\). A matrix-form depiction of \(p_{}(x_{s}|x_{t})\) is shown in Fig. 7 (right). In fact, we can select a time-invariant parametrization \(_{}(x_{t},t)=_{}(x_{t})\) as  showed that \(p(x_{0}|x_{t})\) given \(x_{t}=x\) is identical for any \(t\).

Figure 1: Masking schedules in the literature: (Left) \(_{t}\); (Right) weight of the cross-entropy loss w.r.t. \(t\); Equations for these schedules are given in Tab. 4 in Appendix.

Besides \(p_{}(x_{s}|x_{t})\), we also need to specify \(p(x_{0}|x_{t(1)})\) and the prior distribution \(p(x_{t(T)})=p(x_{1})\). Following the practice in continuous diffusion models , we choose \(p(x_{0}|x_{t(1)}) q(x_{t(1)}|x_{0})\). And since \(q(x_{1}|x_{0})_{x_{1},m}\) for any \(x_{0}\) as \(_{1} 0\), we set \(p(x_{1})_{x_{1},m}\), see App. E.

We then write out the discrete-time diffusion model objective [1; 2], which is a lower bound of the log marginal likelihood of data \(x_{0}\) under the model \(p\) (known as the Evidence Lower Bound, or ELBO):

\[ p(x_{0})_{q(x_{t(1)}|x_{0})}[ p(x_{0}|x_{t(1)})]-(q(x_{1}|x_{0})\|p(x_{1}))-_{T},\]

where \(_{T}=_{i=2}^{T}_{q(x_{t(i)}|x_{0})}[(q(x_ {s(i)}|x_{t(i)},x_{0})\|p_{}(x_{s(i)}|x_{t(i)}))]\). For the above choices of the prior distribution, the term \((q(x_{1}|x_{0})\|p(x_{1}))\) becomes zero. Under the reverse model (2), the KL divergence terms in \(_{T}\) becomes (proof in App. D)

\[(q(x_{s}|x_{t},x_{0})\|p_{}(x_{s}|x_{t}))=-- _{t}}{1-_{t}}_{x_{t},m} x_{0}^{}_{}(x _{t},t),\]

which is a simple cross-entropy loss between the predicted logits and the clean data. In App. D, we show that \(_{T}\) is a Riemann sum and is lower bounded by the corresponding continuous integral:

\[_{}_{T}_{T}=_{t(1)}^{ 1}^{}}{1-_{t}}_{q(x_{t}|x_{0})}[ _{x_{t},m} x_{0}^{}_{}(x_{t},t)]t,\] (4)

where \(_{t}^{}\) denotes the derivative of \(_{t}\) with respect to \(t\). Therefore, we can obtain an ELBO that is tighter than that of any finite \(T\) by pushing \(T\). This ELBO can be further simplified by letting \(t(1) 0\). As a result, \(_{q(x_{t(1)}|x_{0})}[ p(x_{0}|x_{t(1)})]\) goes to \(0\) and the ELBO becomes \(-_{}\).

For continuous state-space diffusions, the ELBO depends on the signal-to-noise ratio (SNR) at its endpoints but is otherwise invariant to the noise schedule . We establish here a similar result for discrete diffusions. Consider choosing \(_{t}=(_{t})\), where \(\) represents the sigmoid function \((x)=}\). In this context, the log-SNR is defined by \(_{t}=}{1-_{t}}=(t)\). By making a change of variables in (4) to make everything a function of the log-SNR, we obtain

\[_{}=_{_{t(1)}}^{_{1}}() _{(x_{s}|x_{0})}[_{x_{},m} x_{0}^{ }_{}(x_{},)].\]

where \(_{}(x,):=_{}(x,t)\) and \((x_{s}|x_{0}):=q(x_{t}|x_{0})\) for \(t=^{-1}()\). This shows that the only effect \(_{t}\) has on the loss is through the values of the SNR at the endpoints. Still, because we draw uniform samples of \(t\) to estimate the integral, the choice of masking schedule affects the variance.

Multidimensional data.In the previous sections, \(x_{t}\) was assumed to be a single discrete token. To extend the method to multidimensional data, let \(x_{t}\) be now a sequence \((x_{t}^{(1)},x_{t}^{(2)},,x_{t}^{(N)})\), where each element \(x_{t}^{(n)}\) represents a discrete token. We select a forward process which factorizes across all \(N\) tokens: \(q(x_{t}|x_{s})=_{n=1}^{N}q(x_{t}^{(n)}|x_{s}^{(n)})\). As a result, the forward marginals \(q(x_{t}|x_{0})\) and reversal \(q(x_{s}|x_{t},x_{0})\) also factorize. In this case, we define the reverse model as \(p_{}(x_{s}|x_{t})_{n=1}^{N}q(x_{s}^{(n)}|x_{t}^{(n)}, _{}^{(n)}(x_{t},t))\), where \(_{}(x_{t},t)\) is a neural network that takes the full \(N\) tokens as input and outputs \(N\) probability vectors.2 The \(n\)-th output \(_{}^{(n)}(x_{t},t)\) is a prediction model for \([x_{0}^{(n)}|x_{t}]\), the mean value of the \(n\)-th token. Repeating above derivations gives

\[_{}^{(N)}_{0}^{1}^{}}{1- _{t}}_{q(x_{t}|x_{0})}_{n:x_{t}^{(n)}=m}(x_{0}^{(n) })^{}_{}^{(n)}(x_{t},t)t.\] (5)

We term our simple masked diffusion model trained with loss (5) **MD4** (Masked Discrete Diffusion for Discrete Data). A single step of MD4 training algorithm is described in Alg. 1 in Appendix.

## 4 Sampling

We use ancestral sampling from our discrete-time reverse process for generation. We have found this yields slightly higher sample quality compared to other methods such as Euler discretization [29; 32]. For conditional generation tasks such as infilling, we find that the simple approach works best -- we keep the conditioning tokens unmasked throughout the generation process. A complete description of the sampling algorithm can be found in Alg. 2 in Appendix.

**Impact of schedules and discretization.**  For comparing different sampling configurations, we primarily use the FID score  on image datasets as our evaluation metric. We favor it over text generative perplexity3 used in prior work , as the latter can be misleadingly reduced by lowering sample diversity . We initially trained our model using the linear schedule, which achieves the best final ELBO overall; however, we found that sampling did not perform well with a standard uniform discretization grid \(t(i)=\). We hypothesize that time discretization can lead to conflicts by generating multiple tokens in a single step. We then switched to the cosine schedule (Tab. 4) that slows down unmasking at the beginning of reverse process. This drastically improves the FID on ImageNet 64\(\)64 from 70 to 17 for \(T=256\) steps (Fig. 2, left). Building on this observation, we suggest using a "cosine" discretization grid for sampling in models trained with a linear schedule:

\[t(i)=1-.\] (6)

This induces the same discretization in \(_{t}\) as the cosine schedule with a uniform grid, leading to comparable sample quality, as shown in Fig. 2 (left). In Fig. 2 (right), we plot the number of tokens unmasked per step for linear and cosine schedules with a uniform grid. We believe the cosine schedule performs better because it leverages information redundancy: with more tokens revealed, the remaining tokens become more predictable, reducing conflicts when unmasking them in a single step.

Although these findings were originally developed on images, we find them translate well to text (see Fig. 10). we expect other techniques such as top-\(p\) sampling , classifier-free guidance [42; 43], and predictor-correctors [29; 44] to further improve sample quality of our models. While we reserve these for future work, we note that the JAX  implementation of categorical sampling implicitly truncates small probabilities, creating a similar effect to top-\(p\) sampling. See App. G for details.

## 5 Relation to Existing Work

We discuss how to unify several existing masked diffusion models using our framework.

Continuous-Time Markov Chains (CTMC).To show the connection with the CTMC view presented in Austin et al. , Campbell et al. , we can write out the forward and reverse masked diffusion using CTMC machinery. To see this, for a short time \( t\), given \(x_{0}\), the Taylor expansions of our forward and reverse transition matrices at \(t\) are

\[(t,t+ t) =I+Q(t) t+o( t) Q(t) (t)(e_{m}^{}-I),\] (7) \[^{x_{0}}(t,t- t) =I+R^{x_{0}}(t) t+o( t) R^{x_{0}} (t)-^{}}{1-_{t}}e_{m}(x_{0}-e_{m})^{ },\] (8)

where \(Q(t)\) and \(R^{x_{0}}(t)\) are known as the _transition rate_ matrices. Austin et al.  derived the same \(Q(t)\) in App. A.6 of their paper. However, they did not explore the reverse process or a

Figure 2: Left: FID evaluation for 50k samples randomly generated from MD4 on pixel-level modeling of ImageNet 64\(\)64 (numbers in Tab. 6). Right: Number of tokens revealed per generation step (\(T=256\)). Each image consists of \(64 64 3=12288\) tokens.

continuous-time objective. Campbell et al.  derived an alternative ELBO expression using rate matrices, which Kitouni et al.  further simplified for absorbing diffusion. In App. H.1, we show how to recover their expression by separating out a constant from our ELBO expression (4) and applying a discrete "integration-by-part". A key limitation of their expression is that it needs \(N\) evaluations of the prediction model \(_{}(,t)\) to compute an inner summation. To circumvent this computational burden, they used a doubly stochastic estimate. However, this leads to significantly higher variance compared to the analytic cross-entropy (4) which only requires one pass of \(_{}(,t)\). Please refer to App. H.2 for more details.

Score parameterization.While so far we used a prediction model \(_{}(x_{t},t)\) for the mean of clean data given \(x_{t}\) (i.e., mean parameterization), one can choose other ways of parameterizing the reverse model. Lou et al. , Benton et al.  proposed to parameterize the discrete "score" \(s(x_{t},t)(j)}{q_{t}(x_{t})}\) and introduced a score-based loss for discrete diffusions. In App. H.3, we provide an alternative derivation of their loss which is simpler. We show the link between score and mean parameterizations through the following proposition.

**Proposition 1** (Score Parameterization vs. Mean Parameterization).: _Let \(q_{t}\) be the marginal distribution of the masked diffusion defined in Sec. 2 at time \(t\). The discrete score \(s(x_{t},t)_{j}=(j)}{q_{t}(x_{t})}\) for a mask state \(x_{t}=m\) and \(j m\) can be expressed as_

\[s(m,t)_{j}=}{1-_{t}}[x_{0}|x_{t}=m]^{}e_{ j}_{j m}s(m,t)_{j}=}{1-_{t}}.\] (9)

Proposition 1 (proved in App. H.3) implies that a reasonable score model for a mask state is

\[s_{}(m,t)_{j}=}{1-_{t}}_{}(m,t)_{j}.\] (10)

Indeed, substituting (10) into the score-based loss of Lou et al. , Benton et al.  recovers our objective (4). In Lou et al. , the score is parameterized as a neural network without enforcing the constraint in (9). This means the learned reverse model can be incompatible with the forward process. We find that our parameterization, which enforces the constraint, leads to more stable training and better results.

Any-order autoregressive models.The continuous-time reverse process of our masked diffusion model can be viewed as an any-order autoregressive model (AO-ARM) . To see this, we reorder the tokens according to the timing of their unmasking events in the reverse process. For all tokens, the cumulative distribution functions (CDFs) of unmasking times \(\{_{n}\}_{n=1}^{N}\) are identical and satisfy \(P(_{n} t)=P(x_{t}^{(n)}=m)=1-_{t}\). As a result, the ordering is uniformly random across all possible arrangements, and the token prediction during each unmasking event represents a prediction step in AO-ARMs. This connection was initially pointed out in Hoogeboom et al. [48, App. C]. The relation between our simplified ELBO (5) and the AO-ARM objective is independently clarified by Ou et al. . Despite this equivalence, our work demonstrates that the masking schedule \(_{t}\) introduces a new degree of freedom in the design of such models. Variations in \(_{t}\) can lead to different distributions of unmasking times, significantly impacting performance in diffusion-style parallel sampling under time discretization, as shown in Fig. 2.

Other related work.Due to space constraint, we defer the discussion on other related work, including MaskGIT , discrete flow matching , SDDM , Blackout diffusion  and SUNDAE , to App. H.4.

## 6 Generalization to State-dependent Masking Schedules

Consider a scenario where some tokens hold more significance than others and we would like to unmask them earlier in the process. To achieve this, we introduce state-dependent masking schedules, where the probability of unmasking a token depends not only on time, but also on the token's value.

We first define the forward process for a single token \(x_{t}\). Let \(_{t}\) be a \(m+1\) dimensional vector function, i.e., there is a different function \(_{t,i}\) for each possible value \(i\) of the token \(x_{t}\). Also, by vector \(}{_{s}}\) we denote the element-wise division of the two vectors. We define the forward transition as \(q(x_{t}|x_{s})=(x_{t};(s,t)^{}x_{s})\) where

\[(s,t)=}{_{s}}+I- }{_{s}}e_{m}^ {}\]

and \(}{_{s}}\) is a diagonal matrix with the vector \(}{_{s}}\) in its diagonal. The probability of moving from current state \(x_{s}\) to a future state \(x_{t}\) (either the same as \(x_{s}\) or mask) is determined by a state-dependent rate \(}{_{s}}^{}x_{s}\), while the marginal at time \(s\) given \(x_{0}\) is

\[q(x_{s}|x_{0})=(x_{s};(s)^{}x_{0}) (s)=(_{s})+(I-(_{s}))e_{ m}^{}.\]

Further, for any time \(0 s<t 1\) it holds that \(q(x_{t}|x_{0})=_{x_{s}}q(x_{t}|x_{s})q(x_{s}|x_{0})\) so the above is a valid continuous-time Markov chain.

Given the forward conditionals and marginals, we can now compute the time reversal conditioned on \(x_{0}\). The full form of \(q(x_{s}|x_{t},x_{0})\) is derived in App. I.1. For \(x_{t}=m\), we have

\[q(x_{s}|x_{t}=m,x_{0})=q(x_{s}|x_{t}=m,x_{0},x_{0}x_{0}^{})=-_{s}}{-_{t}}^{}x_{0}e_{m}^{}x _{s}+-_{t}}{-_{t}}^{ }x_{0}x_{0}^{}x_{s}.\] (11)

This suggests that the reverse model given \(x_{t}=m\) can be chosen as \(p_{}(x_{s}|x_{t}=m) q(x_{s}|x_{t}=m,_{}(x_{t},t), (_{}(x_{t},t)))\) where \(_{}(x_{t},t)\) is a neural network that approximates \([x_{0}|x_{t}]\) while \((_{}(x_{t},t))\) approximates \([x_{0}x_{0}^{}|x_{t}]=([x_{0}|x_{t}])\). We show in App. I.1 that the negative continuous-time ELBO for the state-dependent rate case is

\[_{}=_{0}^{1}^{}}{- _{t}}^{}_{q(x_{t}|x_{0})}[_{x_{t},m} (x_{0}-_{}(x_{t},t)+x_{0}x_{0}^{}_{}(x_{t},t)) ]t.\] (12)

Here, \(_{t}^{}\) is the elementwise derivative of \(_{t}\). This generalizes the MD4 loss (4), which is recovered when \(_{t}\) is a scalar schedule times a vector of ones. For \(N\) tokens, the model further generalize similarly to Sec. 3 and the loss is given in (32). We call this generalized model **GenMD4**.

To learn the token dependent masking schedule using ELBO optimization, we parametrize the \(m+1\) dimensional function \(_{t}\) using the polynomial schedule (see Fig. 1) as \(_{t,i}=1-t^{w_{i}}\) and optimize each parameter \(w_{i}>0\).4 The value of \(w_{i}\), through the masking probability \(1-_{t,i}\), determines how fast the token with value \(i\) jumps to the mask state. Since in the loss (12) the distribution \(q(x_{t}|x_{0})\) depends on \(_{t}\) and thus the vector \(w\), optimizing \(w\) poses a discrete gradient estimation problem [see, e.g., 52]. Naive autodiff leads to biased gradients and pushes \(w\) towards zero because the gradients cannot propagate through the (discrete) samples drawn from \(q(x_{t}|x_{0})\). To fix this, we used the REINFORCE leave-one-out estimator  to compute low-variance unbiased gradients for optimizing \(w\). Details are given in App. I.2.

Figure 3: Iterative unmasking process for an unconditionally generated sample by MD4. This visualization only includes a subsequence from a generated sequence of 1024 tokens. ”\(\)” represents masks. Masked tokens are revealed sequentially: green (steps 500-700), yellow (700-850), and red (850-1000). Additional unconditional generation from MD4 can be found in App. K.5.

## 7 Experiments

### Text

Text is natural discrete data with rich structures. For comparison with prior work, we evaluate likelihood on two datasets: **text8**, a character-level text modeling benchmark, and **OpenWebText**, an open clone of the unreleased WebText dataset used to train GPT-2 . We also assess our model's performance on downstream tasks by training on **FineWeb-Edu**, a high-quality dataset of fine educational text commonly used by the open-source community for comparing LLMs. Unless otherwise specified, a linear schedule and a cosine sampling grid are employed.

OpenWebText.We train MD4 of GPT-2 small (S) and GPT-2 medium (M) sizes on OpenWebText and evaluate zero-shot perplexity on five benchmark datasets used in Radford et al. . We keep our evaluation setup the same as SEDD . To ensure fair comparison, we reimplemented SEDD in our codebase. Our implementation led to slightly better results than those reported in their paper.

As seen in Tab. 1, our small model outperforms previous best discrete diffusion models on all five tasks. We are also better than GPT-2 on all tasks except LAMBADA where we are the second best method. When scaling up to medium size, MD4 similarly beats SEDD and GPT-2 on 4 out of 5 tasks.

To confirm that the strong zero-shot performance stems from improved training, we plot perplexity on 2% OpenWebText validation set in Fig. 4. Our models converge faster and have better final likelihoods than prior methods. We also observed that SEDD  has training instabilities, likely due to score parameterization breaking consistency between forward and reverse processes (Sec. 5). Although GenMD4 achieves lower perplexity than MD4, we observed that the learned \(w\)s can overfit to dataset statistics, making it less effective on zero-shot transfer tasks.

We also assess our models' generation quality. Fig. 3 shows a randomly selected, notably coherent sample from MD4-medium and its denoising process. Fig. 10 demonstrates MD4's text infilling ability and highlights a substantial quality gain when transitioning from uniform to cosine discretization (see Sec. 4). Despite MD4's strong performance on quantitative metrics like generative perplexity, we have placed these results in Appendix Fig. 8 due to the metric's inherent unreliability, as noted in Sec. 4. We emphasize the more reliable FID-based assessments found in our image experiments.

 Size & Method & LAMBADA & WikiText2 & PTB & WikiText103 & IBW \\   & GPT-2 (WebText)\({}^{*}\) & **45.04** & 42.43 & 138.43 & 41.60 & 75.20 \\  & D3PM & \(\) 93.47 & \(\) 77.28 & \(\) 200.82 & \(\) 75.16 & \(\) 138.92 \\  & Plaid & \(\) 57.28 & \(\) 51.80 & \(\) 142.60 & \(\) 50.86 & \(\) 91.12 \\  & SEDD Absorb & \(\) 50.92 & \(\) 41.84 & \(\) 114.24 & \(\) 40.62 & \(\) 79.29 \\  & SEDD Absorb (reimpl.) & \(\) 49.73 & \(\) 38.94 & \(\) 107.54 & \(\) 39.15 & \(\) 72.96 \\  & MD4 (Ours) & \(\) 48.43 & \(\) **34.94** & \(\) **102.26** & \(\) **35.90** & \(\) **68.10** \\   & GPT-2 (WebText)\({}^{*}\) & **35.66** & 31.80 & 123.14 & 31.39 & 55.72 \\  & SEDD Absorb & \(\) 42.77 & \(\) 31.04 & \(\) 87.12 & \(\) 29.98 & \(\) 61.19 \\   & MD4 (Ours) & \(\) 44.12 & \(\) **25.84** & \(\) **66.07** & \(\) **25.84** & \(\) **51.45** \\  

Table 1: Zero-shot unconditional perplexity on five benchmark datasets from Radford et al. . The numbers for other methods are from Lou et al.  except our reimplementation of SEDD Absorb. Our MD4 model achieves the best result on all benchmarks except LAMBADA where it is the second best. \({}^{*}\)The GPT-2 numbers are reported for the GPT-2 checkpoint pretrained on WebText instead of OWT thus is not a direct comparison.

Figure 4: Perplexity on OpenWebText (OWT) validation set during training. The final numbers are reported in Tab. 5 in Appendix.

Text8.Following prior work [14; 32], we trained masked diffusion models on text8 and evaluate the bits-per-character on the test set (details in App. J.1). As seen in Tab. 2, our models outperform previous discrete and continuous diffusion models, as well as state-of-the-art AO-ARMs which are closely related to discrete diffusion . Our model is only beaten by an autoregressive (AR) transformer and the AR-backbone Discrete Flow . We believe this is because AR models only require learning a fixed generation order thus better utilize model capacity. Text8's small vocabulary (26 letters and a space) led us to expect limited flexibility from our state-dependent formulation. However, using the generalized objective in (12), GenMD4 achieved significantly better BPC than MD4, demonstrating the potential of state-dependent diffusion for discrete data.

FineWeb-Edu.We train MD4 on FineWeb-Edu and evaluate its zero-shot accuracy on the Hellaswag dataset , a popular common sense inference benchmark for LLMs. We directly compared MD4 to its AR counterparts - transformers with identical configurations (except for causal masking) trained on the same data. Results are summarized in Fig. 5.

MD4 demonstrates steady performance growth with increasing scale. While outperformed by AR models of the same size, the performance gap does not widen as model size increases. For example, AR-small reaches 30% accuracy in 50k steps, while MD4-small takes 200k steps (4x data efficiency difference). At the medium scale, AR achieves 37% in 270k steps, compared to MD4's 1 million steps.

### Pixel-level image modeling

Unlike continuous diffusion which struggles with discrete data, we show that MD4, a discrete diffusion model, performs well on inherently continuous data, suggesting its potential for unifying

 Method & BPC (\(\)) \\  _Continuous Diffusion_ & \\ Plaid  (Our impl.) & \(\) 1.48 \\ BFN  & \(\) 1.41 \\  _Any-order Autoregressive_ & \\ ARDM  & \(\) 1.43 \\ MAC  & \(\) 1.40 \\  _Autoregressive_ & \\ IAF/SCF  & 1.88 \\ AR Argmax Flow  & 1.39 \\ Discrete Flow  & **1.23** \\ Transformer AR  & **1.23** \\  _Discrete Diffusion_ & \\ Mult. Diffusion  & \(\) 1.72 \\ D3PM Uniform  & \(\) 1.61 \\ D3PM Absorb  & \(\) 1.45 \\ SEDD Absorb  & \(\) 1.39 \\ MD4 (Ours) & \(\) **1.37** \\ GenMD4 (Ours) & \(\) **1.34** \\   
 Method & \#Params & BPD (\(\)) \\  _Autoregressive_ & \\ PixelRNN  & & 3.00 \\  & Gated PixelCNN  & & 3.03 \\ PixelCNN++  & 53M & 2.92 \\ PixelSNAIL  & 46M & 2.85 \\ Image Transformer  & & 2.90 \\ Sparse Transformer  & 59M & 2.80 \\  _Discrete Diffusion_ & \\ D3PM Absorb  & 37M & \(\) 4.40 \\ D3PM Gauss  & 36M & \(\) 3.44 \\ Campbell et al.  & 36M & \(\) 3.59 \\ Campbell et al.  Absorb & 28M & \(\) 3.52 \\ MD4 (Ours) & 28M & \(\) **2.75** \\  _Autoregressive_ & \\ PixelRNN  & & 3.63 \\ Gated PixelCNN  & & 3.57 \\ Sparse Transformer  & 152M & 3.44 \\ Routing Transformer  & & 3.43 \\ Perceived AR  & 770M & **3.40** \\  _Discrete Diffusion_ & \\ MD4 (Ours) & 198M & \(\) **3.40** \\ 

Table 3: Bits Per Dimension (BPD) on CIFAR-10 test set and Downsampled ImageNet 64\(\)64  validation set. All models in the table are trained without data augmentation.

Figure 5: Hellaswag accuracy vs. training steps for MD4 and AR models at GPT-2 small, medium, and large scales.

modalities. We follow Austin et al.  and train MD4 on order-agnostic image data from CIFAR-10 and downsampled ImageNet 64\(\)64 . Each image is treated as a set of 256-valued discrete tokens, making the model agnostic to pixel proximity. We compare to other discrete diffusion and AR models with reported likelihood results on these datasets, although to our knowledge there are no published result on discrete diffusion for ImageNet \(64 64\) that directly model raw pixel space.

Tab. 3 summarizes our results. We establish a new state-of-the-art for discrete diffusion models, outperforming previous work [14; 29] by a significant margin. Our CIFAR-10 result surpasses the best reported AR result. On ImageNet \(64 64\), our results are competitive with Transformer AR models that are \(4\) larger, as well as a strong continuous diffusion model VDM . Notably, despite lacking knowledge of the ordinal structure of pixel values, MD4 outperforms models trained with this inductive bias, including D3PM Gauss and Campbell et al.  where the noising distribution is a discrete Gaussian that assigns larger probabilities to near pixel values. To isolate the differences caused by training objectives, we also implemented the Campbell et al.  objective with the absorbing process, showing its high variance hinders learning even with our architecture.

We provide a random sample from our ImageNet 64\(\)64 model in Fig. 6. More results can be found in App. K. In Fig. 2, we plot the FID values of samples generated under different choices of schedules and discretization grids. We can see that the model with the linear schedule plus a cosine grid achieves an FID close to the model with cosine schedule, both significantly outperform the linear schedule with a uniform grid. We further trained a class-conditional model on ImageNet 64\(\)64 that boosts the FID to around 7. Although these are not state-of-the-art FIDs on ImageNet 64\(\)64, we emphasize our models are optimized for likelihood instead of sample quality.

## 8 Conclusion

In this work, we revisit masked diffusion models, focusing on a flexible continuous-time formulation. Existing works in this area are not easily accessible to non-specialists and present ELBOs that are difficult to optimize, often resulting in performance that is not competitive with continuous diffusions and AR models. The framework we propose provides a very simple expression of the ELBO as a weighted integral of cross-entropy losses. Additionally, we propose a generalized masked diffusion formulation (GenMD4), where the masking schedule depends on the current state of the process, and derive its corresponding ELBO. On text data, our MD4 models outperform existing discrete and continuous diffusion models. For pixel-level image modeling, we significantly improve discrete diffusion results, outperforming similar-sized AR models and achieving comparable likelihoods to continuous diffusion models such as VDM. GenMD4 provides further improvements in terms of likelihoods over the state-independent case.

Although we have improved masked diffusion models, they still suffer from limitations. First, in some tasks such as text8, masked diffusions are not yet competitive with AR models. We conjecture that this is because AR models can better leverage model capacity since they only require learning one order. It would be interesting to develop better architectures for discrete diffusions. Moreover, GenMD4 is promising, but it can easily overfit to the dataset, making it less effective for zero-shot transfer compared to simpler versions. Additionally, inference with a state-dependent schedule is more challenging.

Figure 6: Non cherry-picked unconditional samples from MD4 trained on ImageNet 64x64, treating pixels as discrete tokens. More samples can be found in Fig. 9 in Appendix. The model is optimized for likelihood instead of visual quality—see e.g., Kingma et al.  for samples from a continuous diffusion model optimized similarly for likelihood.