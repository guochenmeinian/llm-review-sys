# GUI-World: A Dataset for GUI-Oriented Multimodal Large Language Models

###### Abstract

Recently, Multimodal Large Language Models (MLLMs) have been used as agents to control keyboard and mouse inputs by directly perceiving the Graphical User Interface (GUI) and generating corresponding code. However, current agents primarily exhibit excellent understanding capabilities in static environments and are predominantly applied in relatively simple domains, such as Web or mobile interfaces. We argue that a robust GUI agent should be capable of perceiving temporal information on the GUI, including dynamic Web content and multi-step tasks. Additionally, it should possess a comprehensive understanding of various GUI scenarios, including desktop software and multi-window interactions. To this end, this paper introduces a new dataset, termed GUI-World, which features meticulously crafted Human-MLLM annotations, extensively covering six GUI scenarios and eight types of GUI-orientated questions in three formats. We evaluate the capabilities of current state-of-the-art MLLMs, including ImageLLMs and VideoLLMs, in understanding various types of GUI content, especially dynamic and sequential content. Our findings reveal that ImageLLMs struggle with dynamic GUI content without manually annotated keyframes or operation history. On the other hand, VideoLLMs fall short in all GUI-orientated tasks given the sparse GUI video dataset. Based on GUI-World, we take the initial step of leveraging a fine-tuned VideoLLM as a GUI agent, demonstrating an improved understanding of various GUI tasks. However, due to the limitations in the performance of base LLMs, we conclude that using VideoLLMs as GUI agents remains a significant challenge. We believe our work provides valuable insights for future research in dynamic GUI content understanding.

## 1 Introduction

Multimodal Large Language Models (MLLMs), such as GPT-4V(ision)  and LLaVA , have significantly contributed to the development of the visual-text domain . These models bring forth innovative solutions and paradigms for traditional visual tasks, including visual reasoning , medical image interpretation [5; 6], and applications in embodied agents . One particularly promising area is Graphical User Interface (GUI) understanding, which holds significant potential for real-world applications, such as webpage comprehension [8; 9] and navigation by GUI agents [10; 11; 12]. The key challenges of GUI understanding are twofold: effective GUI agents are expected to (1) possess a deep understanding of GUI elements, including webpage icons, text identified through Optical Character Recognition (OCR), and page layouts, and (2) exhibit an exceptional ability to follow instructions within GUI contexts, such as conducting searches through search engines.

Despite significant progress, as illustrated in Table 1, existing works suffer from the following limitations: (1) Most studies predominantly focus on the static features of GUI scenarios, neglecting the need for MLLMs to effectively process sequential information and dynamic operations. For instance, an agent's task performance can be disrupted by unexpected elements such as pop-up advertisements, underscoring a gap in handling dynamic sequential tasks. (2) Current research is typically restricted to Web-based environments, which limits the models' generalization and robustness. For instance, GUI agents may need to operate across diverse platforms such as Windows, macOS, Linux, iOS, Android, and XR environments. Additionally, operations may sometimes involve multiple windows. Therefore, expanding the scope of research to encompass these varied environments will enhance the adaptability and effectiveness of GUI agents.

To mitigate these gaps, this paper introduces GUI-World, a comprehensive dataset containing over 12,000 GUI videos, specifically designed to evaluate and enhance the capabilities of GUI agents. This dataset encompasses a wide range of GUI scenarios, including popular websites, desktop and mobile applications across various operating systems, multi-window interactions, as well as XR environments. The data collection process involves sourcing GUI videos from screen recordings and instructional videos on YouTube. Subsequently, we utilize an Human-MLLM collaborative approach to generate a diverse set of questions and instructions and finally construct GUI-World.

Likewise, we also establish a comprehensive benchmark for GUI understanding, which encompasses seven mainstream MLLMs, three keyframe selection strategies, six GUI scenarios, and a diverse array of queries in multiple-choice, free-form, and conversational formats, aiming to provide a thorough evaluation of the MLLMs' GUI-orientated capabilities. The assessment results indicate that most MLLMs struggle with GUI-World, highlighting their limited dynamic understanding of graphical interfaces and underscoring the need for further enhancement.

    &  &  &  & Video &  &  &  \\  & & & & & Web. &  &  &  &  &  &  &  &  &  &  \\  Rico  & 72,219 & Low & ✓ & ✓ & ✗ & ✗ & ✗ & ✗ & ✗ & ✗ &  \\ MetaGUI  & 1,125 & Low & ✗ & ✗ & ✗ & ✗ & ✗ & ✗ & ✗ & ✗ &  \\ UGIF  & 523 & High & ✗ & ✗ & ✗ & ✗ & ✗ & ✗ & ✗ & ✗ &  \\ ATIV  & 715,142 & High & ✗ & ✗ & ✗ & ✗ & ✗ & ✗ & ✗ & ✗ &  \\ Ferret-UI  & 123,702 & Low & ✓ & ✗ & ✗ & ✗ & ✗ & ✗ & ✗ & ✗ &  \\ MiniWoB++  & 100 & Low & ✗ & ✗ & ✗ & ✗ & ✗ & ✗ & ✗ & ✗ &  \\ WebArena  & 812 & Low & ✗ & ✗ & ✗ & ✗ & ✗ & ✗ & ✗ &  \\ Mind2Web  & 2,350 & High & ✗ & ✗ & ✗ & ✗ & ✗ & ✗ & ✗ &  \\ OmniAct  & 9,802 & Low & ✗ & ✗ & ✗ & ✗ & ✗ & ✗ & ✗ &  \\  MMINA  & 1.050 & Low & ✗ & ✗ & ✗ & ✗ & ✗ & ✗ & ✗ &  \\ AgentStudio  & 304 & High & ✗ & ✗ & ✗ & ✗ & ✗ & ✗ & ✗ &  \\ OSWorld  & 369 & High & ✗ & ✗ & ✗ & ✗ & ✗ & ✗ & ✗ &  GUI Understanding \\ Instruction Following \\  } \\   **GUI-World** \\ **(Ours)** \\  & 12,379 & Both & ✓ & ✓ & ✓ & ✓ & ✓ & ✓ & ✓ & ✓ & ✓ & ✓ &  GUI Understanding \\ Instruction Following \\  } \\ 

Table 1: Comparison of GUI datasets. ‘Sem.’: semantic instruction level, ‘Seq.’: Tasks for sequential images, ‘Cro.’: Cross-app or multi-window tasks, ‘Dyn.’: Tasks for dynamic GUI content.

Figure 1: GUI-World: a comprehensive dataset for GUI understanding, holding significant potential for real-world applications.

Leveraging this dataset, we take the first step of fine-tuning a Video GUI Agent proficient in dynamic and sequential GUI tasks, which results in significant improvements in the general capabilities of GUI agents, thereby demonstrating the utility and effectiveness of GUI-World. Additionally, we delve into discussing various factors critical to GUI understanding, including the integration of textual information, the number of keyframes, and image resolutions.

Overall, the key contributions of this paper are three-fold:

\(\)**A New Dataset.** We propose GUI-World, a comprehensive GUI dataset comprising over 12,000 videos specifically designed to assess and improve the GUI understanding capabilities of MLLMs, spanning a range of categories and scenarios, including desktop, mobile, and extended reality (XR), and representing the first GUI-oriented instruction-tuning dataset in the video domain.

\(\)**A Novel Model.** Based on GUI-World, we propose GUI-Vid, a GUI-orientated VideoLLM with enhanced capabilities to handle various and complex GUI tasks. GUI-Vid shows a significant improvement on the benchmark and achieves results comparable to the top-performing models.

\(\)**Comprehensive Experiments and Valuable Insights.** Our experiments indicate that most existing MLLMs continue to face challenges with GUI-oriented tasks, particularly in sequential and dynamic GUI content. Empirical findings suggest that improvements in vision perception, along with an increase in the number of keyframes and higher resolution, can boost performance in GUI-oriented tasks, thereby paving the way for the future of GUI agents.

## 2 GUI-World: A Comprehensive Dataset for GUI Understanding

### Overview

We introduce GUI-World, a comprehensive dataset covering six GUI scenarios including video, human-annotated keyframes, as well as detailed captions and diverse types of QA produced by our data curation framework, aiming at benchmarking and enhancing the general GUI-orientated capabilities. These GUI scenarios encompass desktop operating systems (_e.g._, macOS, Windows) and mobile platforms (_e.g._, Android and iOS), websites, software, and even extended-range technologies (XR) (_e.g._, GUI in Apple Vision Pro ). Discussion for each scenario are in subsection B.1.

As illustrated in Figure 2, the development of GUI-World is structured around a two-stage process. Details regarding video and query statistics are provided in Table 2, which includes distributions of the number of keyframes, video lengths, and the lengths of queries and their corresponding golden answers, as displayed in Figure 3. Refer to Appendix G for details in the case study.

### GUI Video Collection and Keyframe Annotation Process

We describe the pipeline for collecting screen recordings from student workers and GUI-related instructional videos from YouTube for GUI-World and the procedures followed to convert these videos into keyframe sequences.

A significant portion of our video data is derived from screen recordings executed by student workers, which can directly reflect real-life GUI usage scenarios. A typical video collection scenario involves assigning a student worker a specific software task. The student begins by familiarizing themselves

Figure 2: An overview construction pipeline of GUI-World.

with the software, followed by recording a series of operations in a short video clip, such as "Sign up", "Sign in", "Create a New Page", and "Invite Other Collaborators" in the software "Notion1".

Despite the high fidelity of these manually recorded videos, we encounter several challenges: (1) Student workers often require substantial time to acquaint themselves with professional software (_e.g._, MATLAB, Adobe After Effects (Ae)), which can hinder the progress of data collection. (2) The videos may lack comprehensiveness, typically capturing only commonly used operations and overlooking rarer functions crucial for dataset completeness. To address these issues, we also source videos from social media platforms that host a diverse array of GUI-related content. Specifically, we download tutorial videos from YouTube--given its prevalence as a video-sharing platform--because they richly detail various GUI operations. These videos are then segmented into shorter clips, each representing a distinct sequence of operations.

The subsequent step involves annotating these video clips with keyframes and textual descriptions of each keyframe using custom-designed annotation software. Although several algorithms exist for keyframe extraction , they typically underperform with GUI videos where changes between frames might be minimal (_e.g._, a slight movement in the mouse cursor). To ensure high-quality datasets, we therefore perform manual extraction of these keyframes. Each keyframe is meticulously annotated to include details such as the operation performed, the purpose between two keyframes, the software or website used, mouse actions (_e.g._, scroll, click), and keyboard inputs (_e.g._, copy (Ctrl + C), paste (Ctrl + V), specific input). We detail our annotation process in subsection B.3.

### GUI Tasks Generation from Human-MLLM Collaboration

Drawing insights from prior research , we develop a Human-MLLM collaboration pipeline to annotate captions and diverse types of QA specifically tailored for GUI comprehension. The process involves inputting an instructional prompt, a comprehensive description, key information (_e.g._, system or application), and a sequence of human-annotated keyframes into GPT-4V. As depicted in Table 3, GUI-World features an array of question types, as detailed in follows:

\(\)**Detailed and Summarized Captioning:** This task challenges basic GUI knowledge and multimodal perception, also addressing the deficiency of detailed GUI content in video-caption pairs. Initially, GPT-4V generates two distinct descriptions for each video: one concentrating on fine-grained details and the other on the overall image sequences. Furthermore, GPT-4V provides a succinct summary, highlighting core operations and overarching objectives in the video.

  
**Category** & **Total Videos** & **Free-form** & **MCQA** & **Conversation** & **Total Frame. (Avg.)** & **Avg. Anno.** \\  Software & 4,720 & 27,840 & 9,440 & 9,440 & 23,520 (4.983) & 7.558 \\ Website & 2,499 & 14,994 & 4,998 & 4,998 & 15,371 (6.151) & 6.862 \\ IOS & 492 & 2,952 & 984 & 984 & 2,194 (4.459) & 7.067 \\ Multi & 475 & 2,850 & 950 & 950 & 2,507 (5.277) & 7.197 \\ XR & 393 & 2,358 & 786 & 786 & 1,584 (4.030) & 10.970 \\ Android & 3,800 & 15,199 & 7,600 & 7,600 & 38,000 (10.000) & - \\  Summary & 12,379 & 76,673 & 24,758 & 24,758 & 83,176 (6.719) & 7.463 \\   

Table 2: The statistics of GUI-World. For Android, we automatically sample 10 frames. _Avg. Frame_ refers to the average number of frames in each keyframe, and _Avg. Anno._ refers to the average number of manually annotated user actions in each keyframe.

Figure 3: Left: Distribution of the number of keyframes and video lengths. Right: Length distribution for each type of question and its golden answer.

[MISSING_PAGE_EMPTY:5]

image sequences, engaging in conversations, retrieving both static and dynamic GUI elements, and performing reasoning tasks.

As illustrated in Figure 4, We employ the two-stage training architecture utilizing VideoChat2  as our foundational model. Initially, videos and images are encoded using the UMT-L visual encoder . Subsequently, a QFormer compresses visual tokens into a smaller set of query tokens. Drawing inspiration from , we enhance the QFormer  by integrating instructions to enable it to extract visual representations pertinent to the given instructions. Additionally, we apply low-rank adaptation (LoRA ) to base LLM. This model is concurrently fine-tuned with the visual encoder and QFormer using a Vision-grounded Text Generation (VTG) loss: \(_{}()=-[ p(y|v;)]\), where \(v\) represents the visual tokens derived from the QFormer, and \(y\) represents the text output grounded in the visual context.

## 4 Experiments and Analysis

### Experimental Setups

Models.We conduct evaluations on four of the most robust image-based MLLMs: GPT-4V(ision) , GPT-4o , Qwen-VL-Max , and Gemini-Pro-1.5 . We benchmark on three keyframe selection settings: (1) _Random_, where frames are sampled at fixed time intervals within a video; (2) _Extracted_, with keyframes extracted using Kata2; and (3) _Human_, where keyframes are selected by humans during the annotation process. For the _Random_ and _Extracted_ settings, we input 10 frames into each MLLM, while the _Human_ setting uses an average of 6.719 frames, as detailed in Table 2. Each model's responses employ a three-step Chain-of-Thought (CoT)  process, i.e., "Describe-Analyze-Answer", to evaluate their peak performance. Additionally, we assessed three advanced VideoLLMs--ChatUnivi , Minigpt4-video , and Videochat2 --for their performance on GUI content. For detailed experimental setups are referred to Appendix D.

Evaluation Metrics.To assess free-form questions and multiple-round conversations, we utilize the LLM-as-a-Judge methodology, which assigns a similarity score ranging from 1 to 5 between MLLM's response and a predefined golden answer, already validated by previous studies. For a comprehensive evaluation, we also provide BLEU  and BERTScore  in Appendix E. For multiple-choice questions, we measure performance using accuracy as the primary evaluation metric.

Textual Information Integration.To investigate the effectiveness of integrating image-caption models to enlarge the context window for LLMs--typically employed in natural videos--and the helpfulness of GUI history content in accomplishing GUI-oriented tasks, we implement three experimental settings: Detailed Caption, Concise Caption, and Vision + Detailed Caption. GPT-4V is utilized to provide captions of these keyframes, integrating human annotators' operational intents to more accurately describe each frame, being validated in subsection B.3.

Keyframes and Resolution.To explore the upper bound of GUI-orientated capabilities, particularly in dynamic and sequential tasks, we conduct ablation studies focusing on the impact of the number

Figure 4: An overview of our fine-tuning architecture, focusing on GUI content alignment and instruction tuning.

[MISSING_PAGE_EMPTY:7]

Performance Variate in Different GUI Scenarios.GPT-4V and Gemini excel in common scenarios such as mobile and website interfaces but show marked deficiencies in more complex GUI environments like XR and multi-window interactions, across both captioning and intricate tasks. This performance gap highlights a significant shortfall in understanding environments where GUI elements are scattered and demand sophisticated interpretation. It emphasizes the critical need for specialized benchmarks and datasets tailored to these complex GUI scenarios, which is essential for enhancing the GUI-oriented capabilities of MLLMs, paving the way for them to become truly reliable and high-performing general control agents.

Keyframe Selection is Important for GUI-orientated Tasks.Across both basic tasks such as captioning and more complex tasks like prediction and reasoning, significant variations are evident among keyframe selection methods. GPT-4V and Gemini significantly benefit from using random-selected and human-selected keyframes, scoring approximately 0.2-0.3 points higher in both captioning and free-form tasks than those using programmatic extraction. This suggests that traditional keyframe technologies, designed for natural videos, are less effective for detecting essential GUI operations, particularly when subtle movements like mouse clicks and dynamic changes are involved. Conversely, the difference in performance is relatively smaller in Qwen-VL-Max, indicating that while keyframe selection methods are crucial for models proficient in GUI content, they exert less influence on less capable models.

Dynamic GUI Tasks Continue to Challenge MLLMs.In the fine-grained tasks depicted in Table 5, GPT-4V and GPT-4oE excel with static GUI content and prediction tasks over image sequences but struggle with providing detailed descriptions for entire videos and dynamic GUI content. This discrepancy is attributed to minor variations in GUI that significantly impact descriptions. Enhancing the number of keyframes and the granularity of perception might mitigate these issues. Among VideoLLMs, ChatUnivi excels in conversational tasks by effectively leveraging contextual nuances, particularly in subsequent rounds, yet it underperforms in GUI-oriented captioning tasks. In contrast, GUI-Vid demonstrates proficiency in sequential tasks but falls short in both captioning and static content handling. This gap is linked to deficiencies in GUI-Vid's pretraining, which lacked comprehensive GUI content crucial for effective vision-text alignment, as evidenced by its poor performance in Table 13 and an instruction tuning process also failed to fully address these shortcomings.

Vision Perception is Important for Sequential GUI Tasks.As demonstrated in Table 5, integrating detailed textual information slightly outperforms purely vision-based inputs or detailed captions, akin to a Chain of Thought (CoT)  setting. Surprisingly, GPT-4V excels in caption and prediction

    &  &  &  &  & } \\  & & Concies & Detailed & Static & Dyn. & Pred. & Round 1 & Round 2 \\   & R. & 3.659 & 2.837 & 2.969 & 2.822 & 3.450 & 3.608 & 3.845 & 3.339 \\  & E. & 3.350 & 2.468 & 2.741 & 2.431 & 3.292 & 3.458 & 3.837 & 3.152 \\   & R. & 2.381 & 1.758 & 2.277 & 2.144 & 2.724 & 3.125 & 3.317 & 2.676 \\  & E. & 2.459 & 1.693 & 2.143 & 1.954 & 2.742 & 3.174 & 3.298 & 2.624 \\  & H. & 2.474 & 1.711 & 2.137 & 2.032 & 2.834 & 3.223 & 3.257 & 2.651 \\   & R. & 3.579 & 2.676 & **3.243** & 3.011 & 3.630 & 3.925 & 4.131 & 3.589 \\  & E. & 3.141 & 2.301 & 2.927 & 2.627 & 3.541 & 3.844 & 4.103 & 3.407 \\  & H. & 3.352 & 2.509 & 3.053 & 2.849 & 3.609 & 3.928 & 4.163 & 3.520 \\  & C.C. & 3.454 & 2.547 & 1.818 & 2.335 & 3.577 & 3.521 & 3.884 & 3.028 \\  & D.C. & 3.412 & 2.627 & 2.603 & 2.591 & 3.723 & 3.759 & 4.072 & 3.350 \\  & H+D.C. & 3.436 & 2.677 & 2.927 & 2.750 & **3.791** & 3.857 & 4.148 & 3.494 \\  GPT-to & H. & **4.048** & **3.028** & 3.125 & **3.117** & 3.562 & **4.129** & **4.318** & **3.644** \\  ChatUnivi & - & 1.587 & 1.240 & 1.705 & 1.656 & 2.524 & 2.698 & 3.366 & 2.389 \\ Minigpt4Video & - & 1.246 & 1.073 & 1.249 & 1.235 & 1.675 & 1.494 & 1.719 & 1.475 \\ VideoChat2 & - & 1.992 & 1.312 & 1.812 & 1.682 & 2.158 & 2.342 & 2.720 & 2.144 \\ 
**GUI-Vid** & - & 3.562 & 2.058 & 2.376 & 2.090 & 3.435 & 3.080 & 3.260 & 2.847 \\   

Table 5: Detailed scores for each tasks in **Software** scenarios. ‘Dyn.’ refers to queries on dynamic GUI content, and ‘Pred.’ indicates prediction tasks.

tasks with just detailed captions, providing insights on enhancing specific GUI-oriented tasks through additional textual information. However, it still falls short in more challenging tasks, such as retrieving static or dynamic content. This underscores the critical role of visual perception in GUI environments, where even minor changes can significantly impact outcomes.

**Supreme Enhancement of GUI-Vid on Graphic-based Interface After Finetuned on GUI-World.**

As a pioneering study in training VideoLLMs as screen agents, GUI-Vid significantly outperforms the baseline model, showing an average improvement of 30% across various tasks and GUI scenarios, even surpassing the commercial ImageLLM, Qwen-VL-Max. This enhancement is particularly notable in captioning and prediction over image sequences, where GUI-Vid matches the performance of GPT-4V and Gemini-Pro. As shown in Figure 6, our two-stage progressive finutuning significantly enhances the performance in all GUI scenarios. Remarkably, GUI-Vid scored 3.747 in caption tasks within the XR scenario, highlighting its potential in XR applications and the high-quality annotations provided by our dataset. However, in Multiple-Choice QA and Chatbot tasks, GUI-Vid still lags behind industry leaders like GPT-4V and Gemini-Pro, a discrepancy likely due to the baseline LLM's weaker performance and the challenges of instruction-based fine-tuning.

**Upper Bound of GUI-orientated Capability with More Keyframes and High Resolution.**

As depicted in Table 6, our two ablation studies during the fine-tuning phase demonstrate that utilizing GUI image-text captioning data significantly enhances the model's preliminary understanding of GUI elements, outperforming training that relies solely on videos. Additionally, an increased number of keyframes correlates with improved performance across various scenarios, notably in environments featuring multiple windows and software applications. Further evidence from Table 7 reveals that higher image resolutions substantially boost task performance, both basic and complex, for GPT-4o. These findings underscore the potential for further developing a more robust GUI Agent.

## 5 Conclusion

In this paper, we have introduced GUI-World, a comprehensive GUI-oriented dataset designed to benchmark and enhance understanding of virtual interface, especially seqeuntial and dynamic tasks. This dataset extensively covers six scenarios and various tasks, addressing the previous research gap in comprehensively evaluating models' capabilities in graphic-based understanding. We conduct extensive benchmarks on leading MLLMs and the first Video Agent 'GUI-Vid' finetuned on our GUI-World specifically for tasks requiring temporal information, achieving results comparable to top-performing models, providing detailed insights into enhancing GUI-related capabilities.

    &  &  &  &  &  &  &  &  &  &  \\   & & I. & V. & MC & Free & MC & Free & MC & Free & MC & Free & MC & Free & MC & Free \\   & - & 8 & - & 45.5\% & 2.144 & 42.6\% & 2.221 & 44.0\% & 2.005 & 44.0\% & 2.222 & 40.2\% & 2.169 & 44.7\% & 2.119 & 42.9\% & 2.147 \\  & - & 16 & - & 45.1\% & 2.144 & 41.8\% & 2.240 & 41.0\% & 2.007 & 40.7\% & 2.238 & 39.9\% & 2.138 & 44.7\% & 2.042 & 2.25 & 1.54 \\   &  &  &  &  &  & 53.6\% & 2.817 & 62.2\% & 2.626 & **54.2\%** & 2.673 & 51.1\% & 2.708 & 54.9\% & 2.501 & 56.0\% & 2.665 \\  & & & & & **59.9\%** & **2.856** & 54.1\% & 2.925 & 59.0\% & 2.751 & 52.1\% & 2.837 & 50.0\% & 2.756 & 54.0\% & 2.571 & 54.8\% & 2.782 \\  & & & & & & & & & & & & & & & & & \\  & & & & & & & & & & & & & & & & & & \\  & & & & & & & & & & & & & & & & & & \\   

Table 6: The overall results for ablation study on GUI-Vid finetuning. F.K. and E.K. mean keyframes during the finetuning and evaluation process respectively. I. means Image, and V. means Video.

   Res. & Desc. & Conv. & Dyn. & Static & Caption & Free \\  Low & 2.794 & 3.912 & 3.150 & 2.869 & 3.672 & 3.394 \\ High & **3.031** & **4.056** & **3.318** & **3.131** & **3.911** & **3.573** \\   

Table 7: GPT-4o average performance in six GUI scenarios under low and high resolution.

Figure 6: Two stages of progressive training enhance GUI ability.

## 6 Limitations

While our work presents significant advancements in the field of GUI agents, there are several limitations that need to be addressed. Firstly, despite expanding the dataset to include various GUI scenarios, our models still show limited generalization capabilities when applied to environments not represented in the training data. This highlights the need for further research to improve the adaptability and robustness of GUI agents in diverse and unseen environments. Additionally, the accuracy of our models heavily relies on the selection of keyframes. Automatically extracted keyframes often fail to capture the essential elements needed for accurate GUI understanding, indicating the need for more sophisticated keyframe extraction techniques. Furthermore, although VideoLLMs have shown improvements in handling dynamic content, their ability to understand and predict sequential information in GUI tasks remains suboptimal. This suggests a necessity for future work to focus on enhancing the temporal understanding capabilities of these models. Finally, the training and fine-tuning processes for VideoLLMs require significant computational resources, which may not be accessible to all researchers.

## 7 Potential Negative Societal Impacts

While our work aims to advance the capabilities of GUI agents for beneficial applications, it is important to consider potential negative societal impacts. The use of GUI agents, especially those capable of operating across multiple environments and platforms, raises significant privacy concerns. Ensuring that these agents operate within strict ethical guidelines and that user data is handled securely and responsibly is paramount. There is also the risk of misuse of advanced GUI agents for malicious purposes, such as unauthorized access to sensitive information or automated exploitation of software vulnerabilities. Establishing robust security measures and ethical usage policies is essential to mitigate these risks.