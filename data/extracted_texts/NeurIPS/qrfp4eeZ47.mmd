# FactorizePhys: Matrix Factorization for Multidimensional Attention in Remote Physiological Sensing

Jitesh Joshi

Department of Computer Science, University College London, UK

Sos S. Agaian

Department of Computer Science, College of Staten Island, City University of New York, USA

Youngjun Cho

Department of Computer Science, University College London, UK

###### Abstract

Remote photoplethysmography (rPPG) enables non-invasive extraction of blood volume pulse signals through imaging, transforming spatial-temporal data into time series signals. Advances in end-to-end rPPG approaches have focused on this transformation where attention mechanisms are crucial for feature extraction. However, existing methods compute attention disjointly across spatial, temporal, and channel dimensions. Here, we propose the Factorized Self-Attention Module (FSAM), which jointly computes multidimensional attention from voxel embeddings using nonnegative matrix factorization. To demonstrate FSAM's effectiveness, we developed FactorizePhys, an end-to-end 3D-CNN architecture for estimating blood volume pulse signals from raw video frames. Our approach adeptly factorizes voxel embeddings to achieve comprehensive spatial, temporal, and channel attention, enhancing performance of generic signal extraction tasks. Furthermore, we deploy FSAM within an existing 2D-CNN-based rPPG architecture to illustrate its versatility. FSAM and FactorizePhys are thoroughly evaluated against state-of-the-art rPPG methods, each representing different types of architecture and attention mechanism. We perform ablation studies to investigate the architectural decisions and hyperparameters of FSAM. Experiments on four publicly available datasets and intuitive visualization of learned spatial-temporal features substantiate the effectiveness of FSAM and enhanced cross-dataset generalization in estimating rPPG signals, suggesting its broader potential as a multidimensional attention mechanism. The code is accessible at https://github.com/PhysiologicAILab/FactorizePhys.

## 1 Introduction

Attention mechanisms in computer vision are inspired by the human ability to identify salient regions in complex scenes. Such mechanisms can be interpreted as a dynamic weight adjustment process that selects useful features and disregards irrelevant ones in a multidimensional feature space. Recent surveys [20; 23] provide a comprehensive overview of attention mechanisms and distinctly categorize existing attention mechanisms. Amidst a spectrum of research from convolution block attention  to computationally intensive multi-head attention , an effective, yet computation and memory efficient, attention mechanism has remained desirable for real-world applications. Matrix decomposition [12; 19; 31], a dimensionality reduction technique, has captured the interest of researchers and has been explored in deep learning research for different objectives [57; 60; 17; 18]. This work investigates nonnegative matrix factorization (NMF), a matrix decomposition technique, for its potential to efficiently perform multidimensional attention and evaluates its effectiveness in the spatial-temporal context of estimating rPPG signal from video frames.

Verkruysse 's pioneering investigation on extracting photoplethysmography (PPG) or blood volume pulse (BVP) signals from RGB cameras in a contactless manner led to an exciting research field of imaging-based physiological sensing. There exist several potential applications and contexts of noninvasive and contactless measurement techniques, such as stress and mental workload recognition , driver drowsiness monitoring  and social biofeedback interaction . The seminal works on unsupervised rPPG methods  either used video frames acquired under stationary conditions or performed skin segmentation  or region of interest (RoI) tracking as a preprocessing step. This preprocessing step can be considered as a basic form of attention mechanism that enables the unsupervised models to process only the relevant regions. Some of the supervised rPPG methods, including HR-CNN , RhythmNet , NAS-HR , PulseGAN , and Dual-GAN  also relied on extracting spatial-temporal features from the tracked RoIs as a preprocessing step.

As end-to-end rPPG methods, such as DeepPhys , and MTTS-CAN  among several others, take whole facial frames as input, they rely on attention mechanisms that enable models to emphasize the relevant spatial-temporal features. Estimating BVP signal from raw facial video frames in an end-to-end manner is therefore an interesting downstream task to investigate the attention mechanism in multidimensional feature space. This requires networks to learn to pick the spatial features having the desired temporal signature, while discarding the variance related to head-motion, illumination, and skin-tones, thus representing one of the challenging spatial-temporal tasks. Few other notable end-to-end rPPG methods include PhysNet , 3DCNN , SAM-rPPGNet , RTrPPG , and transformer-network-based methods such as PhysFormer , PhysFormer++ , EfficientPhys , JAMSNet , and GLISNet . A recent survey article on visual contactless physiological monitoring in clinical settings  highlights susceptibility to disturbance, such as head movement, as one of the key challenges, among others. Some of the recent end-to-end rPPG methods  further highlight the need for multidimensional attention, as squeezing features in selective dimensions for deriving attention reduces the feature space to a single dimension and is therefore not well suited for the task of signal extraction.

To address this, our work draws inspiration from the seminal work on NMF  which a recent work formulated as an approach to design the global information block, referred to as Hamburger . Hamburger  implements NMF to derive low-rank embeddings, which serve as a global context block. Despite the low computational complexity of \(O(n)\), Hamburger  outperformed various attention modules in the semantic segmentation  and image generation tasks. In addition, researchers have combined matrix factorization with deep architectures in several ways for different applications such as layer-wise learning of dictionary for classification and clustering , adaptive learning of dictionary for image denoising , multi-attention model for recommendation systems , and linearly scalable approach to context modeling for medical image segmentation  among several others. Drawing inspiration from these studies, especially those that use matrix factorization to model global context  in vision tasks, we investigate the application of NMF as a multidimensional attention block. Although matrix factorization in deep learning has remained a topic of significant interest, it has not been investigated in the realm of rPPG, which stands to gain from joint spatial, temporal, and channel attention.

We introduce the Factorized Self-Attention Module (FSAM), which implements NMF to jointly compute spatial-temporal attention and describe an appropriate formulation for the low-rank recovery problem. To investigate the relevance and effectiveness of FSAM in computing multidimensional attention, we build a 3D-CNN architecture FactorizePhys that implements FSAM. We further adapt FSAM for EfficientPhys , an end-to-end rPPG architecture that builds on the Temporal Shift Module (TSM) , to uniquely learn spatial-temporal features using 2D-CNN layers. Evaluation of FactorizePhys and EfficientPhys  with FSAM, against existing SOTA rPPG methods, demonstrates the versatility of FSAM as multidimensional attention along with its effectiveness for the downstream task of estimating time series from spatial-temporal data. In summary, we make the following contributions.

* Factorized Self-Attention Module (FSAM): NMF -based novel approach that jointly computes multidimensional attention within voxel embeddings.
* FactorizePhys: an end-to-end 3D-CNN architecture that integrates FSAM for robust estimation of rPPG from spatial-temporal facial video frames.
* Thorough assessment of FactorizePhys and FSAM with multiple evaluation metrics and the corresponding measure of standard errors to compare cross-dataset generalization performance with SOTA rPPG methods, using four benchmarking rPPG datasets.

Related Work

### Attention Mechanisms in Vision

Varied forms of attention mechanisms have been successful in different visual tasks such as image classification [70; 25; 66], object detection [5; 82], semantic segmentation [78; 16; 18; 28], video understanding [63; 15; 33; 21], 3D vision [69; 24], and multimodal tasks [73; 56] among others [20; 23]. The most widely used attention mechanisms are channel attention [35; 75], spatial attention [66; 63], temporal attention [72; 74], self-attention or transformer-based approaches [58; 14], multimodal attention [56; 73], graph-based approaches , as well as different combinations of these types [66; 16; 53]. In addition, researchers have proposed attention mechanisms for video understanding [63; 15; 33; 21] as well as 3D vision [69; 24]. Despite notable advances in different forms of attention mechanisms, some of the existing challenges include the requirement for high computational costs, large training data, the overall efficiency of the model, and a cost-benefit analysis of performance improvement . Additionally, for rPPG research, the impact of attention mechanisms on an ability of models to generalize on unseen datasets is not systematically studied, which we address in this work.

### Attention Mechanisms in rPPG Methods

End-to-end rPPG methods can be categorized into convolution neural networks (CNN) architectures such as PhysNet , EfficientPhys-C , 3DCNN , SAM-rPPGNet , and RTrPPG , and transformer-network-based architectures such as PhysFormer , PhysFormer++ , and EfficientPhys-T . Among end-to-end rPPG methods, DeepPhys  first implemented a novel convolutional attention mechanism in an architecture that comprised separate _motion_ and _appearance_ branches, with the latter intended to compute attention for the main _motion_ branch. Inspired by the CBAM attention mechanism , originally validated for classification and detection tasks, ST-Attention  was proposed to filter salient information from spatial-temporal maps, thus improving remote HR estimation. The similar dual attention mechanism was also found to be effective in the SMP-Net framework , which jointly learned the features of RGB and infrared spatial-temporal maps to estimate multiple physiological signals.

Recently, EfficientPhys , an end-to-end network, presented an efficient single-branch approach with a gated attention mechanism. The Swin-Transformer  based version of EfficientPhys  insightfully added the TSM  module to the Swin transformer , enabling the architecture to perform efficient spatial-temporal modeling and compute attention by combining shifting window partitions spatially and shifting frames temporally. It should be noted that the convolution-based version of EfficientPhys , which combined the TSM  module and a convolutional attention mechanism  showed superior accuracy along with significantly low latency, making it highly suitable for deployment on mobile devices. Recently, there has been an upsurge in transformer-based rPPG architectures, some of which include PhysFormer , PhysFormer++ , TransPhys , and RADIANT .

Unlike other transformer-based architectures that rely on spatial-temporal maps as input, PhysFormer  and PhysFormer++  are end-to-end video transformer-based architectures, which adaptively aggregate both local and global spatial-temporal features. PhysFormer++  extends PhysFormer  by better exploiting temporal contextual and periodic rPPG clues, as it extracts and fuses attentional features from slow and fast pathways. In addition, both architectures [77; 76] are trained using label distribution learning and a curriculum learning-inspired dynamic constraint in the frequency domain, which helps to alleviate overfitting. Although unlike convolution-based EfficientPhys , transformer architectures require significantly higher computational resources.

Most of the light-weight convolutional attention mechanisms require attention to be separately derived in spatial, temporal, and channel dimensions, which is later merged [46; 13]. Although 3D-CNN architectures such as PhysNet  and iBVPNet  have shown promising performances, they have not explored attention mechanisms that can potentially enhance performance in unseen datasets. JAMSNet  and GLISNet  are recent 3D-CNN architectures that benefit significantly from channel-temporal joint attention (CTJA) and spatial-temporal joint attention (STJA). However, unlike CTJA and STJA [79; 80], we jointly derive attention in temporal, spatial, and channel dimensions, without squeezing any dimension of multidimensional features.

## 3 Method

### Primer: Nonnegative Matrix Factorization

Nonnegative matrix factorization is a dimensionality reduction paradigm that decomposes \(M N\) matrix \(V=[v_{1},v_{2},...,v_{N}]_{ 0}^{M N}\) into nonnegative \(M L\) basis matrix \(W=[w1,w2,...,wL]_{ 0}^{M L}\) and nonnegative \(L N\) coefficient matrix \(H=[h1,h2,...,hN]_{ 0}^{L N}\), as depicted in fig. 1 and expressed as:

\[V=WH+E=+E\] (1)

where \(=[},},...,}]_{ 0}^{M N}\) is reconstructed low-rank matrix and \(E_{ 0}^{M N}\) is an error matrix, which is discarded. \(_{ 0}^{M N}\) stands for the set of \(M N\) element-wise nonnegative matrices. Equivalent vector formulation for this approximation can be expressed as:

\[v_{j}}=_{i}^{L}w_{i}H_{ij}\] (2)

The objective to represent high-dimensional matrix with fewer basis can be achieved only when \(L\) is chosen such that \(L min(M,N)\), while when \(L\) is larger than \(M\), it results in over-complete basis. An optimization in \(W\) and \(H\) to achieve the optimal approximation effectively results in the discovery of inherent correlations between the basis vectors in \(W\) and the corresponding coefficients in \(H\). The optimization objective is formulated as:

\[min_{W,H}\|V-WH\|_{F}^{2} W_{ml} 0,H_{ln} 0\] (3)

Further, the imposed non-negativity constraints on \(W\) and \(H\) enable parts-based representations, where activation of one or many of the coefficients in \(H\) together with the basis vectors in \(W\) can reconstruct different interpretable parts of \(V\). For a detailed primer on NMF, we refer the reader to the seminal work  and a survey article  that summarizes different NMF models and algorithms.

The factorization of deeper layer embeddings can be elucidated as the squeeze of information without reducing the dimensions of the embeddings, unlike the existing attention mechanisms . Therefore, exciting or multiplying with the resultant low-rank (information squeezed) embeddings can potentially serve as an attention mechanism. While factorization is formulated for two-dimensional matrix, high-dimensional embeddings can be mapped to two-dimensional matrix. In PyTorch , this is achieved with the 'view' operation.

### Factorized Self-Attention Module (FSAM)

For the downstream task of estimating rPPG from video frames, spatial-temporal input data can be expressed as \(^{T C H W}\), where \(T,C,H,and\)\(W\) represents total frames (temporal dimension), channels in a frame (e.g., for RGB frames, \(C=3\)), height and width of pixels in a frame, respectively. \(\) is passed through a feature extractor that generates voxel embeddings \(^{}\), with temporal (\(\)), channel (\(\)) and spatial (\(,\)) dimensions.

The goal is to jointly derive the attention in the multidimensional space of \(\), without squeezing individual dimensions. For this, we deploy NMF-based matrix factorization to compute low-rank \(\) by reconstructing it from the factorized basis matrix \(W\) and a coefficient matrix \(H\). It is essential to factorize \(\) in a way that \(\) approximated through the computed basis and coefficient matrices serves as an effective self-attention. Among several parameters that govern factorization, here we delve into the ones most relevant for the time series estimation task. These include: i) the transformation of the voxel embeddings that maps \(^{}\) to the factorization matrix \(V^{st}^{M N}\) and ii) the rank of the factorization.

Figure 1: Formulation of Nonnegative Matrix Factorization (NMF)For a 2D-CNN architecture with \(\) channels and \(\) spatial features, the transformation (\(^{ MN}\)) implemented in the Hamburger module  is expressed as:

\[V^{s}^{M N}=^{ MN}( ^{}) M,  N\] (4)

where \(\) channels are mapped to \(M\) and \(\) spatial features are mapped to \(N\), with an underlying assumption that spatial features are inherently correlated due to learnt CNN kernels. However, for 3D-CNN architectures, as \(^{}\) encodes temporal, channel, and spatial features, it is required to revisit these mappings. While it can be argued that similar to 2D-CNN architectures, as 3D-CNN architectures have 3D kernels, the spatial-temporal features are inherently correlated. However, it should be noted that the scales of spatial and temporal dimensions are very distinct, owing to which the spatial-temporal patterns to be learned may not be uniformly captured through typical convolutional kernels (e.g. \(3 3 3\)). Adjusting the spatial-temporal kernel sizes can be heuristic task, and does not guarantee the extraction of desired features, while drastically increasing the model complexity (since for time series estimation, \(>>,\ \)). Also, \(\) channels are not inherently correlated, and therefore it is crucial to devise a multidimensional attention that jointly computes the spatial-temporal and channel attention. To address this, we first consider negative Pearson correlation, a loss function that is commonly deployed to optimize end-to-end rPPG methods, expressed as:

\[_{p}=1-^{T}(r_{i}^{ppg}-})(g_{i}^{ppg}- })}{^{T}(r_{i}^{ppg}-})^{2}} ^{T}(g_{i}^{ppg}-})^{2}}}\] (5)

where, \(r^{ppg}^{1 T}\) is an estimated rPPG signal and \(g^{ppg}^{1 T}\) corresponds to the ground-truth BVP signal. The optimization of end-to-end model to estimate a vector in temporal dimension (\(r^{ppg}^{1 T}\)) can be leveraged by establishing the correlation of features in spatial and channel dimensions with the features in temporal dimension. Factorization of a matrix that consists of vectors in temporal domain and spatial and channel dimension as the features of the vectors, uniquely offers an opportunity to design the requisite attention. Prior to transforming \(\) to \(V^{st}^{M N}\), it is pre-processed through a convolution layer (with \(1 1 1\) kernels), and a ReLU activation to ensure non-negativity of the embeddings. Following this preprocessing, the temporal features of \(\) are mapped to the vector dimension (\(M\)) in \(V^{st}\), while spatial and channel dimensions are mapped to the feature dimension (\(N\)) of \(V^{st}\). This transformation of \(\) as depicted in fig. 2, can be expressed as:

\[V^{st}^{M N}=^{ MN}(_{ pre}(^{}))  M, N\] (6)

where, \(_{pre}\) represents preprocessing operation. Factorization of thus formed matrix \(V^{st}\) with temporal vectors shall result in a low-rank matrix \(V^{st}\) which is approximated based on the latent structure that establishes correlation of temporal features with spatial and channel features.

\[}=(V^{st})\] (7)

Figure 2: Factorized Self-Attention Module (FSAM) illustrated for a 3D-CNN architecture for rPPG estimation.

where \(\) represents factorization operation. \(V^{st}\) is transformed back to the embedding space, resulting in an approximated voxel embeddings \(\) that selectively retains the spatial and channel features that contribute towards the recovery of salient temporal features in \(\). The resultant \(\) can be expressed as:

\[=^{MN}(^{st}^{M N})\] (8)

where \(^{MN}\) represents matrix transformation operations. We use the one-step gradient optimization based approach  to factorize \(V^{st}\). This approach is a linear approximation of the conventional back-propagation through time algorithm (for time \(t\)) , as proposed with the Hamburger module . Approximated low-rank matrix \(^{st}\) is transformed back to the embedding space through \(^{MN}\), resulting in \(\) that can potentially serve as the requisite attention. \(\) is post-processed with a convolution layer (with \(1 1 1\) kernels), and a ReLU activation, followed by element-wise multiplication with \(\). This multiplication operation serves as an excitation operation, which can be distinctly effective as \(\) retains the dimension of \(\) while computing the attention. The product is instance-normalized, and added with \(\) that serves as residual connection as depicted in fig. 2. It is to be noted that for each single forward pass through the model, approximation of \(^{st}\) requires 4-8 steps, however, FSAM implements NMF within "no_grad" block, that does not require back-propagation through the NMF for model optimization. Representing the network head as \(\), \(_{post}\) as post-processing operation and \(\) as instance normalization, the estimated \(r^{ppg}\) signal can be expressed as:

\[r^{ppg}=(+(_{post}( )))\] (9)

Next, we look at the rank of the factorization that affects the approximation of \(V^{st}\). The primary consideration for the rank \(L min(M,N)\) as mentioned in SS3.1 ensures that \(^{st}\) is of low rank. Although the choice of \(L\) is generally governed by the downstream task, it is often derived empirically. In the context of \(r^{PPG}\)estimation, we revisit the formulation of factorization matrix through \(^{ MN}\) that maps temporal features along the \(M\) dimension. As we expect only a single signal underlying source of BVP signal across all facial regions, single vector estimation \(v_{0}^{st}\) corresponding to rank-1 (i.e., \(L=1\)) shall be sufficient to capture the spatial, temporal, and channel features that contribute to the \(r^{PPG}\) estimation. Experimentation with rank-1 and higher rank factorization (appendix A.3) shows that for the higher ranks, the performance remains at par with that of the network without the FSAM, indicating that for rPPG estimation task, rank-1 factorization offers the optimal multidimensional attention, confirming our understanding.

### Deployment of FSAM in 3D-CNN and 2D-CNN Architectures

We deploy FSAM in our proposed 3D-CNN model, FactorizePhys and integrate it in an existing 2D-CNN architecture, EfficientPhys  to assess its versatility.

FactorizePhys Architecture:FactorizePhys, as depicted in fig. 3[A], is an end-to-end 3D-CNN architecture for estimating rPPG signal from raw video frames. Skin reflection models [62; 6] discuss the presence of several unrelated stationary and time-varying temporal components, and a relatively weaker pulsatile component of interest. To eliminate stationary components, FactorizePhys implements a Diff as first layer, inspired by existing rPPG architectures [6; 36; 37]. The resultant Diff frames are normalized with \(\), unlike existing architectures that use BatchNorm. The size

Figure 3: (A) Proposed FactorizePhys with FSAM; (B) FSAM Adapted for EfficientPhys 

of the kernel and the strides of each convolution layer are depicted in fig. 3[A]. For each layer, we use TanH activation followed by \(\). Spatial features are gradually aggregated by not padding the features, while we deploy spatial convolution strides only on the \(3^{rd}\) and \(6^{th}\) layers. For temporal features, _same_ padding retains the input temporal dimension throughout the network, as depicted in fig. 3[A]. Downsizing of temporal features may result in high-amplitude unrelated time-varying components to outweigh the weaker rPPG related pulsatile component. To provide a clearer overview of the architecture of FactorizePhys, only the spatial and temporal dimensions of the features at multiple layers are shown in fig. 3[A], while the channel dimension is skipped. FSAM, as elaborated in SS3.2, is deployed to jointly compute multidimensional attention, at the layer where the spatial dimension is reduced to \(7 7\), as reported as the optimal spatial dimension in a recent work .

Adaptation of FSAM for 2D-CNN Architecture:Several SOTA rPPG methods [36; 37] leverage the TSM  that efficiently models spatial-temporal features using 2D-CNN architectures. The parameter called 'Frame Depth' (\(\) channels) controls the number of channels that are shifted along the temporal dimension for modeling temporal features. We investigated the effectiveness of the proposed FSAM with a more recent TSM-based SOTA rPPG architecture, EfficientPhys , which also deploys the Self-Attention Shifted Network (SASN) as the attention module. As \(\) controls the amount of temporal information that is learned, we use this to formulate the mapping for the factorization matrix. Equation (6) can be adapted for TSM based architectures to appropriately transform the embeddings to factorization matrix as:

\[V^{tsm}^{M N}=^{ MN}( ^{tsm}^{}) M, N\] (10)

Figure 3[B] shows modified EfficientPhys  architecture, in which we drop SASN blocks  and add a single FSAM. Unlike SASN , FSAM derives attention without squeezing any individual dimension, which in turn can strengthen the correlation between temporal, channel and spatial features. This adaption critically evaluates the proposed FSAM against its counterpart in the SOTA architecture, addressing the recommendations of a recent survey article  on visual attention methods.

## 4 Experiments

We perform an evaluation with carefully selected end-to-end SOTA rPPG methods that include PhysNet , a 3D-CNN architecture without the attention mechanism, EfficientPhys , a 2D-CNN architecture with self-attention, and PhysFormer , a transformer-based 3D-CNN architecture with multi-head self-attention. Comparison of FactorizePhys and PhysNet  can indicate the importance of the attention mechanism in 3D-CNN rPPG architectures, while comparison of EfficientPhys with SASN  and EfficientPhys  with FSAM allows evaluating the effectiveness of the proposed FSAM in 2D-CNN architectures, and thus allows assessing the versatility of FSAM. Similarly, the comparison of FactorizePhys and PhysFormer  offers a thorough evaluation of the proposed FSAM against multi-head self-attention in 3D-CNN architectures.

Each model was trained on one of the four existing datasets that include iBVP , PURE , UBFC-rPPG  and SCAMPS  and evaluated on the other three. Appendix A.1 provides our detailed description of these datasets. Our code is based on the rPPG-Toolbox , with specific adaptations described in appendix A.2. We train all models uniformly with 10 epochs  on iBVP , PURE , and UBFC-rPPG  datasets, and with one epoch on SCAMPS  dataset. For fair evaluation, all model-specific hyperparameters were maintained as provided by the respective SOTA rPPG methods, while the training pipeline related hyperparameters, which include preprocessing steps for images and labels, batch size, number of epochs, learning rate, scheduler, and optimizer were kept consistent for training all the models.

## 5 Results and Discussion

Ablation Study:First, we train FactorizePhys on UBFC-rPPG  dataset and test on PURE  and iBVP  datasets, to compare different transformations of \(^{r}\) with temporal, channel and spatial features to factorization matrix \(V^{st}^{M N}\) as tabulated in table 1. Superior cross-dataset generalization can be observed when the temporal dimension, \(\) is mapped to \(M\), as described in SS3.2. We assess contribution of FSAM over base FactorizePhys model and observe consistent performance gains with FSAM, as reported in table 3 in appendix A. We then investigate residual connection in table 3, and observe it to contribute positively. We also observed that the base FactorizePhys model trained with FSAM retains the performance gains in-spite when FSAM is skipped during the inference. As this eliminates the computational overhead during inference, we report our main results of FactorizePhys trained with FSAM, by running inference without the FSAM. On contrary, in case of TSM  based EfficientPhys  model trained with FSAM, we observed performance drop when FSAM was skipped during inference, and therefore for EfficientPhys with FSAM, we do not drop FSAM during inference. Evaluation for factorization ranks and optimization steps to solve NMF shows consistent superiority of rank-1 factorization in table 4 in appendix A.

FactorizePhys vs. State-of-the-Art:We use heart rate (HR)  along with BVP metrics that include signal-to-noise ratio (SNR) and maximum amplitude of cross-correlation (MACC) [29; 10] for evaluation. SNR and MACC are direct measures to compare estimated rPPG signals with ground-truth BVP signals. The HR metrics reported are the mean absolute error (MAE), the square root of the mean square error (RMSE), the mean absolute percentage error (MAPE), and Pearson's correlation coefficient (Corr)  of the estimated HR. Uncertainty estimates quantifying the variability associated with signal estimation have been shown to be strongly correlated with the absolute error of the estimated HR . In addition, for each metrics, we report the standard error to estimate the variability of each model. As most SOTA end-to-end models show robust within-dataset performance, we present cross-dataset performance in table 2, while reporting within-dataset performance in table 6 in appendix A.

First, we observe that for all the evaluation metrics reported, the proposed FactorizePhys with FSAM outperforms the SOTA methods on PURE  and iBVP  datasets, across all training datasets. This suggests a consistent and superior generalization achieved by the proposed method. Cross-dataset evaluation on the UBFC-rPPG  dataset further highlights the performance gains of the proposed FactorizePhys model when trained with the iBVP  and the SCAMPS  datasets, and at-par performance when trained with the PURE dataset. When models are trained with SCAMPS  (synthesized dataset), FactorizePhys uniquely outperforms the SOTA methods on all testing datasets further indicating the superior cross-dataset generalization. The performance of EfficientPhys  with FSAM exceeds in most cases and remains at par in the rest, compared to the EfficientPhys model with SASN , suggesting the versatility of FSAM as an attention module. As 3D CNN kernels in FactorizePhys can learn spatial-temporal patterns better than the TSM  based 2D-CNN model (EfficientPhys ), FactorizePhys with FSAM outperforms EfficientPhys  with FSAM across all datasets. Lastly, the proposed method consistently achieves superior SNR and MACC for the estimated rPPG signals, highlighting the enhanced reliability of the extracted signals.

Computation Cost and Latency:We compare computational complexity and latency for all the models in fig. 4[A], and provide further details in table 9 in appendix A. The cumulative MAE is computed by averaging cross-dataset performance for respective models across all combinations of training and testing datasets reported in table 2. The proposed FactorizePhys with FSAM not only shows the best performance, it has significantly less number of model parameters and performs at par in terms of latency as the 2D-CNN SOTA rPPG method, EfficientPhys . Specifically, dropping FSAM during inference does not result in loss of performance for FactorizePhys, while reducing latency considerably, making it highly suitable for real-time and resource-constrained deployment. In contrast, when FSAM was dropped after training EfficientPhys  with FSAM, it did not retain the performance (results not shown). We interpret that while FSAM effectively influences the 3D convolutional kernels in FactorizePhys to increase the saliency of relevant spatial-temporal features, 2D convolutional kernels cannot benefit adequately due to the limited ability to model spatial-temporal

    &  **Testing** \\ **Voxel Embeddings** \\  } &  &  &  &  &  &  &  \\    & & & & & & & & & \\   &  & \(\) & \(\) & \(0.77 0.42\) & \(3.29 1.11\) & \(1.34 0.82\) & \(0.99 0.01\) & \(13.84 0.82\) & \(0.77 0.02\) \\    & & \(\) & \(\) & \(0.71 0.39\) & \(3.05 1.03\) & \(1.21 0.76\) & \(0.99 0.01\) & \(13.60 0.81\) & \(0.77 0.02\) \\    & & \(\) & \(\) & \( 0.17\) & \( 0.35\) & \( 0.28\) & \( 0.01\) & \( 0.83\) & \( 0.02\) \\    & & \(\) & \(\) & \(2.05 0.40\) & \(4.65 0.91\) & \(2.87 0.59\) & \(0.88 0.04\) & \(5.99 0.58\) & \(0.55 0.01\) \\    & & \(\) & \(\) & \(2.17 0.46\) & \(5.23 1.11\) & \(3.13 0.68\) & \(0.86 0.05\) & \(5.83 0.57\) & \(0.54 0.01\) \\    & & \(\) & \(\) & \( 0.39\) & \( 1.06\) & \( 0.57\) & \( 0.04\) & \( 0.58\) & \( 0.01\) \\   

Table 1: Ablation Study for Different Mapping of Voxel Embeddings to Factorization Matrix

    &  &  &  &  &  &  &  &  \\   & &  \\   &  & - & 7.78 \(\) 2.27 & 19.12 \(\) 3.93 & 8.94 \(\) 2.71 & 0.59 \(\) 0.11 & 9.90 \(\) 1.49 & 0.70 \(\) 0.03 \\   &  & ^{}\)} & 6.58 \(\) 1.98 & 16.55 \(\) 3.60 & 6.93 \(\) 1.90 & 0.76 \(\) 0.09 & 9.75 \(\) 1.96 & 0.71 \(\) 0.03 \\   & EfficientPhys & SASN & 0.56 \(\) 0.17 & 1.40 \(\) 0.33 & 0.87 \(\) 0.28 & 0.998 \(\) 0.01 & 11.96 \(\) 0.84 & 0.73 \(\) 0.02 \\   & EfficientPhys &  & **0.64**\(\) 0.14 & **1.19**\(\) 0.30 & **0.64**\(\) 0.22 & **0.999**\(\) 0.01 & 12.64 \(\) 0.78 & 0.75 \(\) 0.02 \\   &  &  & 0.60 \(\) 0.21 & 1.70 \(\) 0.42 & 0.87 \(\) 0.30 & 0.997 \(\) 0.01 & **15.19**\(\) 0.91 & **0.77**\(\) 0.02 \\   &  & - & 26.74 \(\) 3.17 & 36.19 \(\) 5.18 & 46.73 \(\) 5.66 & 0.45 \(\) 0.12 & -2.21 \(\) 0.66 & 0.31 \(\) 0.02 \\   &  & ^{}\)} & 16.64 \(\) 2.95 & 28.13 \(\) 5.00 & 30.58 \(\) 5.72 & 0.51 \(\) 0.11 & 0.84 \(\) 1.00 & 0.42 \(\) 0.02 \\   & EfficientPhys &  & 6.21 \(\) 2.26 & 18.45 \(\) 4.54 & 12.16 \(\) 4.57 & 0.74 \(\) 0.09 & 4.39 \(\) 0.78 & 0.51 \(\) 0.02 \\   &  &  & 8.03 \(\) 2.25 & 19.09 \(\) 4.27 & 15.12 \(\) 4.44 & 0.73 \(\) 0.09 & 3.81 \(\) 0.79 & 0.48 \(\) 0.02 \\   &  &  & **5.43**\(\) 1.93 & **15.80**\(\) 3.56 & **11.10**\(\) 4.05 & **0.80**\(\) 0.08 & **11.40**\(\) 0.76 & **0.67**\(\) 0.02 \\   &  & - & 10.38 \(\) 2.40 & 21.14 \(\) 3.90 & 20.91 \(\) 4.97 & 0.66 \(\) 0.10 & 11.01 \(\) 0.97 & 0.72 \(\) 0.02 \\   &  &  & 8.90 \(\) 2.15 & 18.77 \(\) 3.67 & 17.68 \(\) 4.52 & 0.71 \(\) 0.09 & 8.73 \(\) 1.02 & 0.66 \(\) 0.02 \\   &  &  & 4.71 \(\) 1.79 & 14.52 \(\) 3.65 & 7.63 \(\) 2.97 & 0.80 \(\) 0.08 & 8.77 \(\) 1.00 & 0.66 \(\) 0.02 \\   &  &  &  &  &  &  &  &  &  &  &  \\   &  & - & 1.23 \(\) 0.41 & 2.65 \(\) 0.70 & 1.42 \(\) 0.50 & 0.988 \(\) 0.03 & 8.34 \(\) 1.22 & 0.85 \(\) 0.01 \\   &  &  & **1.01**\(\) 0.38 & **2.40**\(\) 0.69 & **1.23**\(\) 0.48 & **0.990**\(\) 0.03 & 8.42 \(\) 1.24 & 0.85 \(\) 0.01 \\   &  &  & 1.41 \(\) 0.49 & 3.168 \(\) 0.64 & 0.982 \(\) 0.03 & 6.78 \(\) 1.15 & 0.79 \(\) 0.02 \\   &  &  & 1.20 \(\) 0.46 & 2.79 \(\) 0.92 & 1.50 \(\) 0.63 & 0.986 \(\) 0.03 & 7.37 \(\) 1.20 & 0.79 \(\) 0.01 \\   &  &  & 1.04 \(\) 0.38 & 2.44 \(\) 0.69 & 1.23 \(\) 0.48 & 0.898 \(\) 0.03 & **8.88**\(\) 1.30 & **0.87**\(\) 0.01 \\   &  & ^{}\)} & 11.24 \(\) 2.63 & 18.81 . It should also be noted that the higher latency of FactorizePhys compared to EfficientPhys , although it has fewer model parameters, can be attributed to the difference in floating-point operations (FLOPS) between the 3D-CNN and 2D-CNN architectures.

Visualization of Learned Attention:We compute absolute cosine similarity between the temporal dimension of 4D embeddings (with temporal, spatial, and channel dimensions) and the ground-truth signal to visualize the learned attention for FactorizePhys trained without and with FSAM in fig. 4[B], where each tile represents a channel of the embedding layer. A higher cosine similarity score between the temporal dimension of the embeddings and the ground-truth PPG signal, which is observed for FactorizePhys trained with FSAM, indicates a higher saliency of temporal features. The spatial spread of high cosine similarity scores in different channels for FactorizePhys trained with FSAM, highlights selectivity of the learned attention, providing clearer evidence that the FactorizePhys model trained with FSAM can effectively pick the spatial features having the strong presence of the rPPG signal (i.e., facial regions with visible skin surface). Figure 4[B] not only suggests the effectiveness of the joint computation of multidimensional attention, but also offers more intuitive visualization of learned spatial-temporal features than existing visualization approaches [79; 37].

## 6 Conclusion

We present FactorizePhys, a 3D-CNN model utilizing the Factorized Self-Attention Module, FSAM, to concurrently extract multidimensional (spatial, temporal, and channel) attention for the downstream task of rPPG estimation from video frames. The assessment performed utilizing various rPPG datasets demonstrates that our proposed method possesses superior generalization capabilities across different datasets, compared to current state-of-the-art methods. Moreover, when adjusted to the 2D-CNN architecture, FSAM achieves performance on par with the established SASN  attention, underscoring its adaptability across diverse network architectures.

Broader Impacts and Limitations:The superior performance of FactorizePhys equipped with FSAM to estimate rPPG indicates its potential utility in various healthcare applications that require the estimation of physiological signals through noncontact imaging. Although FSAM has shown efficacy as a multidimensional attention mechanism specifically for the extraction of rPPG signals, more research is needed to determine the efficacy of the proposed method in extracting heart rate variability metrics as well as other physiological signals. Despite the state-of-the-art performance of the proposed rPPG method, signal peaks can still be susceptible to challenging real-world scenarios, such as active head movements, occlusions, and dynamic changes in ambient lighting conditions, an issue that is qualitatively illustrated in the waveforms depicted in appendix A.11. Moreover, it is imperative to conduct additional research to evaluate the effectiveness of FSAM across other spatial-temporal domains, including video understanding, video object tracking, and video segmentation, along with several other downstream tasks that depend on multi-dimensional input data. In the context of signal estimation tasks, the utilization of NMF variants that integrate temporal or frequency constraints on time series vectors may offer enhanced attention capabilities. These constraints are congruent with the characteristics of the ground truth and present avenues for future investigation.

Figure 4: (A) Cumulative cross-dataset performance (MAE) v/s latency\(\) plot. The size of the sphere corresponds to the number of model parameters; (B) Visualization of learned spatial-temporal features from the base 3D-CNN model trained without and with FSAM; \(\) System specs: Ubuntu 22.04 OS, NVIDIA GeForce RTX 3070 Laptop GPU, IntelÂ® CoreTM i7-10870H CPU @ 2.20GHz, 16 GB RAM.