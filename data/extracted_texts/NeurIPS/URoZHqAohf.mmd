# ProteinGym: Large-Scale Benchmarks for Protein Fitness Prediction and Design

Pascal Notin\({}^{}\)

Computer Science,

University of Oxford

&Aaron W. Kollasch\({}^{}\)

Systems Biology,

Harvard Medical School

&Daniel Ritter\({}^{}\)

Harvard Medical School

&Lood van Niekerk\({}^{}\)

Systems Biology,

Harvard Medical School

&Steffanie Paul

Systems Biology,

Harvard Medical School

&Hansen Spinner

Systems Biology,

Harvard Medical School

&Nathan Rollins

Seismic Therapeutic

&Ada Shaw

Applied Mathematics,

Harvard University

&Ruben Weitzman

Computer Science,

University of Oxford

&Jonathan Frazer

Centre for Genomic Regulation

Universitat Pompeu Fabra

&Mafalda Dias

Centre for Genomic Regulation

Universitat Pompeu Fabra

&Rose Orenbuch

Systems Biology,

Harvard Medical School

&Yarin Gal

Computer Science,

University of Oxford

&Debora S. Marks\({}^{*}\)

Harvard Medical School

Broad Institute

Correspondence: pascal.notin@cs.ox.ac.uk, awkollasch@gmail.com, danieldritter1@gmail.com, loodvn@gmail.com, debbie@hms.harvard.edu ; \(\) Equal contribution

###### Abstract

Predicting the effects of mutations in proteins is critical to many applications, from understanding genetic disease to designing novel proteins that can address our most pressing challenges in climate, agriculture and healthcare. Despite a surge in machine learning-based protein models to tackle these questions, an assessment of their respective benefits is challenging due to the use of distinct, often contrived, experimental datasets, and the variable performance of models across different protein families. Addressing these challenges requires scale. To that end we introduce ProteinGym, a large-scale and holistic set of benchmarks specifically designed for protein fitness prediction and design. It encompasses both a broad collection of over 250 standardized deep mutational scanning assays, spanning millions of mutated sequences, as well as curated clinical datasets providing high-quality expert annotations about mutation effects. We devise a robust evaluation framework that combines metrics for both fitness prediction and design, factors in known limitations of the underlying experimental methods, and covers both zero-shot and supervised settings. We report the performance of a diverse set of over 70 high-performing models from various subfields (eg., alignment-based, inverse folding) into a unified benchmark suite. We open source the corresponding codebase, datasets, MSAs, structures, model predictions and develop a user-friendly website that facilitates data access and analysis.

Introduction

Proteins carry out a wide range of functions in nature, facilitating chemical reactions, transporting molecules, signaling between cells, and providing structural support to cells and organisms. This astonishing functional diversity is uniquely encoded in their amino acid sequence. For instance, the number of possible arrangements for a 64-residue peptide chain (\(20^{64}\)) is already larger than the estimated number of atoms in the universe. Despite substantial progress in sequencing over the past two decades, we have observed a relatively small, biased portion of that massive sequence space. Consequently, the ability to manipulate and optimize known sequences and structures represents tremendous opportunities to address pressing issues in climate, agriculture and healthcare.

The design of novel, functionally optimized proteins presents several challenges. It begins with learning a mapping between protein sequences or structures and their resulting properties. This mapping is often conceptualized as a "fitness landscape", a multivariate function that characterizes the relationship between genetic variants and their adaptive fitness. The more accurately and comprehensively we can define these landscapes, the better our chances of predicting the effects of mutations and designing proteins with desirable and diverse properties. Machine learning, by modeling complex, high-dimensional relationships, has emerged as a powerful tool for learning these fitness landscapes. In recent years, a plethora of machine learning methods have been proposed for protein modeling, each promising to offer new insights into protein function and design. However, assessing the effectiveness of these methods has proven challenging. A key issue is their evaluation on distinct and relatively sparse benchmark datasets, while relative model performance fluctuates importantly across experimental assays, as was shown in several prior analyses (Riesselman et al., 2018; Laine et al., 2019; Meier et al., 2021). This situation underscores the importance of scale in the benchmarks used. Larger, more diverse datasets would offer a more robust and comprehensive evaluation of model performance.

To address these limitations, we introduce ProteinGym, a large-scale set of benchmarks specifically tailored to protein design and fitness prediction. It comprises a broad collection of over 250 standardized Deep Mutational Scanning (DMS) assays which include over 2.7 million mutated sequences across more than 200 protein families, spanning different functions, taxa and depth of homologous sequences. It also encompasses clinical benchmarks providing high-quality annotations from domain experts about the effects of \(\)65k substitution and indel mutations in human genes (SS 3).

We have designed ProteinGym to be an effective, holistic, robust, and user-friendly tool. It provides a structured evaluation framework that factors in known limitations of the underlying experimental methods and includes metrics that are tailored to protein design and mutation effect prediction (SS 4). We report the performance in a unified benchmark of over 70 diverse high-performing models that come from various subfields of computational biology (eg., mutation effects prediction, sequence-based models for de novo design, inverse folding), thereby supporting novel comparisons across. Unlike prior benchmarks, ProteinGym integrates both the zero-shot and supervised settings, leading to new insights (SS 5). All models are codified with a common interface in the same open-source codebase, promoting consistency and ease of use. Lastly, a dedicated website offers an interactive platform to facilitate comparisons across datasets and performance settings.

## 2 Related Work and Background

Multi-task protein benchmarksIn recent years, several benchmarks have been introduced to provide initial means to assess protein model performance across a multitude of tasks of interests, e.g., predicting contacts, structure, thermostability, and fitness. These benchmarks are generally geared towards assessing the quality of learned protein representations, and the extent to which these representations can be broadly leveraged for various tasks. However, for fitness prediction, they all rely on a very limited set of proteins (e.g., 1-3 assays). In comparison, the ProteinGym benchmarks focus on a single task - fitness prediction - and encompass _two orders of magnitude more_ point mutations assessed and vast diversity of protein families included.

TAPE (Tasks Assessing Protein Embeddings) (Rao et al., 2019) covers five protein prediction tasks, each designed to test a different aspect of protein function and structure prediction (secondary structure, contact, remote homology, fluorescence and stability), and focuses on assessments in the semi-supervised regime via carefully curated train-validation-test splits. ProteinGLUE (Capel et al.,2022] also focuses on assessing the usefulness of learned protein representations on supervised downstream tasks. It is comprised of five different tasks, none directly related to protein fitness: secondary structure, solvent accessibility, protein-protein interactions, epitope region and hydrophobic patch prediction. PEER [Xu et al., 2022] also focuses on multi-task benchmarking, grouped in five categories: protein property, localization, structure, protein-protein interactions and protein-ligand interactions. It contains a richer set of evaluations compared with the prior two benchmarks, and also investigates the multi-task learning setting, but is not designed for thorough fitness prediction benchmarking (3 fitness related assays). The handful of fitness DMS assays from these various benchmarks are all subsumed in ProteinGym.

Single task, non-fitness datasets & benchmarksEfforts to create fair, large-scale, and comprehensive benchmarks have been a significant focus of computational biologists for certain tasks. Among these, the biennial Critical Assessment of protein Structure Prediction (CASP) [Kryshtafovych et al., 2021] is the most renowned. CASP concentrates on protein structure prediction and has set the gold standard in this domain. In parallel to CASP, the Critical Assessment of Functional Annotation (CAFA) [Zhou et al., 2019] challenge provides a platform for evaluating protein function classification. The SKEMPI [Moal and Fernandez-Recio, 2012] database is specifically designed to aid the evaluation of computational methods predicting the effect of mutations on protein-protein binding affinity. Several datasets have been curated for specific properties of interest across a diverse set of

Figure 1: **ProteinGym benchmarks** ProteinGym is comprised of three layers. The data layer encompasses two complementary ground truth labels from DMS assays and clinical annotations from experts. For both, we analyze two types of mutations: substitutions and indels. The model layer is comprised of a diverse set of baselines, tailored to both zero-shot and supervised training regimes. Lastly, the analytics layer includes several performance metrics geared towards fitness prediction or protein design evaluation. Different segmentation variables (e.g., MSA depth, assayed phenotype, taxa) facilitate the comparisons of models across diverse settings

proteins, for instance thermostability (Tsuboyama et al., 2023; Stourac et al., 2020; Chen et al., 2022) or solubility (Hon et al., 2020).

Protein fitness benchmarksClosest to our work are the collections of DMS assays that were curated in Hopf et al. (2017) (28 substitution assays), and then further expanded upon in Riesselman et al. (2018) (42 substitution assays) and Shin et al. (2021) (4 indel assays). We include all assays related to fitness prediction from these prior works in ProteinGym. FLIP (Dallago et al., 2021) focused on comparing fitness predictors in the semi-supervised setting, developing a robust evaluation framework and curating cross-validation schemes for three assays. MaveDB (Rubin et al., 2021) is a repository rather than a benchmark, but it compiles a large collection of datasets from multiple variant effect mapping experiments that can be used for benchmarking purposes. An initial prototype of the ProteinGym benchmarks (referred to as 'ProteinGym v0.1') was introduced in Notin et al. (2022). We have since then significantly expanded the benchmarks in terms of number and diversity of underlying datasets, baselines, evaluation framework and model training regimes (Table A1). This not only enables performance evaluation at an unprecedented scale, but also builds connections between different subfields that are often perceived as separate, as we discuss in the following paragraph.

Clinical BenchmarksDesigning an unbiased, non-circular and broadly applicable benchmark to evaluate the performance of human variant effect predictors at predicting clinical significance is still an open-problem for the clinical community. Combining DMS with clinical annotations has been a fruitful direction to avoid biases (Frazer et al., 2021; Livesey and Marsh, 2023). ClinGen curated a clinical dataset specifically designed to compare a subset of models (Pejaver et al., 2022).

Relationship between protein fitness, mutation effect prediction and designThe protein fitness landscape refers to the mapping between genotype (e.g., the amino acid sequence) and phenotype (e.g., protein function). While it is a fairly broad concept, it should always be thought about in practice within a particular context (e.g., stability at a given temperature in a specific organism). Models that learn the protein fitness landscape have been shown to be effective at predicting the effects of mutations (Frazer et al., 2021; Jagota et al., 2022; Brandes et al., 2023; Notin et al., 2022). But the ability to tell apart the sequences that are functional or not is also critical to protein engineering efforts (Romero et al., 2012; Yang et al., 2018; Wu et al., 2019; Alley et al., 2019). Although typically introduced in the context of de novo protein design (Huang et al., 2016), inverse folding methods (Ingraham et al., 2019; Jing et al., 2020; Dauparas et al., 2022; Gao et al., 2022) can also be used for mutation effects prediction (Appendix A.4.1). There is thus a very tight connection between protein fitness, mutation effect prediction and protein engineering, and the same models can be used for either task depending on context. We seek to illustrate this connection through this work, comparing baselines introduced in different fields (e.g., protein representation learning, inverse folding models, co-evolution models) on the same benchmarks, and including different metrics that are geared more to mutation effect prediction (e.g., Spearman) or design tasks (e.g., NDCG).

## 3 ProteinGym benchmarks

ProteinGym is a collection of benchmarks (Fig. 1) that cover different types of mutation (ie., substitutions vs. indels), ground-truth labels (ie., experimental measurement from DMS vs. clinical annotations), and model training regime (ie., zero-shot vs. supervised).

### Mutation types

We curate benchmarks for two types of protein mutations - _substitutions_ and _indels_ (insertions or deletions), each with unique implications for the structure, function, and modeling of proteins.

SubstitutionsSubstitution mutations refer to a change in which one amino acid in a protein sequence is replaced by another. Depending on the properties of the substituted amino acid, this can have varied impacts on the protein's structure and function, which can range from minimal to drastic. The influence of a substitution largely depends on whether it is conservative (i.e., the new amino acid shares similar properties to the original) or non-conservative. In terms of computational modeling, substitutions are the most commonly addressed mutation type, and the majority of mutation effect predictors support substitutions.

IndelsIndel mutations correspond to insertions or deletions of amino acids in protein sequences. While indels can affect protein fitness in similar ways to substitutions, they can also have profound impacts on protein structure by altering the protein backbone, causing structural modifications inaccessible through substitutions alone (Shortle and Sondek, 1995; Toth-Petroczy and Tawfik, 2013). From a computational perspective, indels present a unique challenge because they alter the length of the protein sequence, requiring additional considerations in model design and making it more difficult to align sequences. For instance, the majority of models trained on Multiple Sequence Alignments are typically unable to score indels due to the fixed coordinate system they operate within (see SS 4). Furthermore, when dealing with probabilistic models, comparing relative likelihoods of sequences with different lengths results in additional complexities and considerations.

### Dataset types

The fitness of a protein is a measure of how well a protein can perform its function within an organism. Factors that influence protein fitness are diverse and include stability, folding efficiency, catalytic activity (for enzymes), binding specificity and affinity. To properly capture this diversity, we curated a broad set of experimental assays that map a given sequence to phenotypic measurements that are known or hypothesized to be related to its fitness. We focused on two potential sources of ground truth: Deep Mutational Scanning (DMS) assays and Clinical datasets.

Deep Mutational Scanning assaysModeling protein fitness landscapes presents a challenge due to the complex relationship between experimentally measured protein fitness, the distribution of natural sequences, and the underlying fitness landscape. It is challenging to isolate a singular, measurable molecular property that reflects the key aspects of fitness for a given protein. In developing ProteinGym, we prioritized assays where the experimentally measured property for each mutant protein is likely to represent the role of the protein in organismal fitness. The resulting compilation of over 250 DMS assays extends over a wide array of functional properties, including ligand binding, aggregation, thermostability, viral replication, and drug resistance. It encompasses diverse protein families, such as kinases, ion channel proteins, G-protein coupled receptors, polymerases, transcription factors, and tumor suppressors. In contrast to most DMS assay collections that focus exclusively on single amino acid substitutions, ProteinGym includes several assays with multiple amino acid variants. Moreover, it spans different taxa (i.e., humans, other eukaryotes, prokaryotes, and viruses), alignment depths, and mutation types (substitutions vs indels). All details about the curation and pre-processing of these DMS assays are provided in Appendix A.3.

Clinical datasetsClinVar (Landrum and Kattman, 2018) is an extensive, public database developed by the National Center for Biotechnology Information (NCBI). It serves as an archival repository that collects and annotates reports detailing the relationships among human genetic variations and associated phenotypes with relevant supporting evidence, thereby providing robust, clinically annotated datasets that are invaluable for understanding the functional impact of mutations. From the standpoint of benchmarking mutation effects predictors, ClinVar permits the direct comparison of predictive models in terms of their accuracy in estimating the functional impact of mutations on human health. Annotations are also available for an order of magnitude more distinct proteins compared with our DMS-based benchmarks, albeit much sparser per protein (see table 1). In the case of indels, we focused on short (\(\)3 amino acids) variants. In ClinVar, 84% of indel annotations are pathogenic, so we added to our clinical dataset common indels from gnomAD (allele frequency >5%) as pseudocontrols (Karczewski et al., 2020).

### Model training regime

Lastly, we discriminate in our benchmarks between zero-shot and supervised settings. In the supervised regime we are allowed to leverage a subset of labels to train a predictive model, while in the zero-shot setting we seek to predict the effects of mutations on fitness without relying on the ground-truth labels for the protein of interest. These two settings offer complementary viewpoints of practical importance. For instance, in settings where labels are subject to several biases or scarcely available (e.g., labels for rare genetic pathologies), we need methods with robust zero-shot performance performance. In cases where we seek to design new proteins that simultaneously optimize several properties of interest (e.g., binding affinity, thermostability) and we have collected a sufficiently large number of labels for each target, supervised methods are more appropriate. The need to rely on labels is even more pronounced when we seek to optimize several anti-correlated properties or when evolution is a poor proxy for the property of interest. Predictions obtained in the zero-shot settings may also be used to augment supervised models (Hsu et al., 2022). The two settings require substantially different evaluation frameworks, which we detail in SS 4.

## 4 Evaluation framework

### Zero-shot benchmarks

DMS assaysIn the zero-shot setting we predict experimental phenotypical measurements from a given assay, without having access to the labels at training time. Due to the often non-linear relationship between protein function and organism fitness (Boucher et al., 2016), the Spearman's rank correlation coefficient is the most generally appropriate metric for model performance on experimental measurements. We use this metric similarly to previous studies (Hopf et al., 2017; Riesselman et al., 2018; Meier et al., 2021). However, in situations where DMS measurements exhibit a bimodal profile, rank correlations may not be the optimal choice. Consequently, for these instances, we supplement our performance assessment with additional metrics, namely the Area Under the ROC Curve (AUC), and the Matthews Correlation Coefficient (MCC), which compare model scores with binarized experimental measurements. Furthermore, for certain goals (e.g., optimizing functional properties of designed proteins), it is more important that a model is able to correctly identify the most functional protein variants, rather than properly capture the overall distribution of all assayed variants. Thus, we also calculate the Normalized Discounted Cumulative Gains (NDCG), which up-weights a model if it gives its highest scores to sequences with the highest DMS value. We also calculate Top K Recall, where we select K to be the top 10% of DMS values. To avoid placing too much weight on properties where we have many assays (e.g., thermostability), we first compute each of these metrics within groups of assays that measure similar functions. The final value of the metric is then the average of these averages, giving each functional group equal weight. We refer to the corresponding value as 'corrected average'.

Clinical datasetsFor the clinical data, with pathogenic and benign categories, we calculate the areas under the ROC and precision-recall curves. In the substitution dataset, \(50\%\) of the labels are in approximately \(10\%\) of the proteins. Since clinical labels across genes correspond to underlying pathologies that are very distinct to one another, it is preferable to assess performance on a gene-by-gene basis. We thus compute the average per-gene performance on the substitution benchmark. However, in the case of indels, only about half of the proteins has a pathogenic label (and only \(10\%\) have a both pathogenic and benign or pseudocontrol labels), so we compute the total AUC for the full dataset. The problem of calibrating model scores in a principled way across different genes is an open problem; we leave this to future work.

BaselinesWe implement a diverse set of 50+ zero-shot baselines that may be grouped into alignment-based models, protein language models, inverse folding models and 'hybrid' models. Alignment-based models, such as site-independent and EVmutation models (Hopf et al., 2017), DeepSequence (Riesselman et al., 2018), WaveNet (Shin et al., 2021), EVE (Frazer et al., 2021) and GEMME (Laine et al., 2019), are trained on Multiple Sequence Alignments (MSAs). Protein language models are trained on large quantities of unaligned sequences across protein families. They

  
**Dataset** & **Description** & **Mutation type** & **\# Proteins** & **\# Mutants** \\   & High-throughput assays evaluating the functional & Substitutions & 217 & 2.4M \\  & impact of a wide range of protein mutations & Indels & 66 & 0.3M \\   & Expert-curated clinical annotations across a wide & Substitutions & 2,525 & 63k \\  & range of human genes & Indels & 1,555 & 3k \\  Total & & & 3,422 & 2.7M \\   

Table 1: **ProteinGym datasets summary** ProteinGym includes a large collection of DMS assays and clinical datasets that offer complementary viewpoints when assessing protein fitness. The table reports the number of mutants and unique proteins per dataset (the total being deduped across datasets).

include UniRep (Alley et al., 2019), the RITA suite (Hesslow et al., 2022), the ESM1 and ESM2 suite (Rives et al., 2021; Meier et al., 2021; Lin et al., 2023), VESPA (Marquet et al., 2022), the CARP suite (Yang et al., 2023) and the ProGen2 suite (Nijkamp et al., 2022). Inverse Folding models learn sequence distributions conditional on an input structure (Ingraham et al., 2019). We include here ProteinMPNN (Dauparas et al., 2022) which is trained on structures in the PDB, MIF (Yang et al., 2023) trained on CATH4.2 (Dawson et al., 2016), and ESM-IF1 (Hsu et al., 2022) which is trained on the PDB and a dataset of AlphaFold2 folded structures. Hybrid models combine the respective strengths of family-specific alignment-based and family-agnostic language models, such as the MSA Transformer (Rao et al., 2021), evotuned UniRep (Alley et al., 2019), Tranception (Notin et al., 2022) and TranceptEVE (Notin et al., 2022).

Because of the variable length of sequences subject to insertion or deletion mutations, alignment-based methods with fixed matrix representations of sequences are unable to score indels. However, profile Hidden Markov Model (HMM) and autoregressive models include explicit or implicit probabilities of indels at each position. Both are trained on homologous sequences recovered with an MSA and expanded to include insertions. The masked-marginals heuristic Meier et al. (2021) used to predict protein fitness with protein language models trained with a masked-language modeling objective (e.g., ESM-1v, MSA Transformer) does not support indels (see Appendix A.4). We thus only report the performance of the following baselines: Tranception (Notin et al., 2022), TranceptEVE (Notin et al., 2022), WaveNet (Shin et al., 2021), HMM (Eddy, 2011), ProGen2 (Madani et al., 2020), UniRep (Alley et al., 2019), RITA (Hesslow et al., 2022) and ProtGPT2 (Ferruz et al., 2022).

For comparisons on clinical benchmarks, we also include unsupervised baselines developed for variant effect prediction in humans, such as SIFT (Ng and Henikoff, 2002), MutPred (Li et al., 2009), LRT (Chun and Fay, 2009), MutationAssessor (Reva et al., 2011), PROVEAN (Choi et al., 2012), PrimateAI (Sundaram et al., 2018) and LIST-S2 (Malhis et al., 2020).

### Supervised benchmarks

DMS assaysWe leverage the same set of 250+ substitutions and indels DMS assays as for the zero-shot setting. In the supervised setting, greater care should be dedicated to mitigating overfitting risks, as the observations in biological datasets may not be fully independent. For instance, two mutations involving amino acids with similar biochemical properties at the same position will tend to produce similar effects. If we train on one of these mutations and test on the other, we will tend to overestimate our ability to predict the effects of mutants at unseen positions. In order to quantify the ability of each model to extrapolate to unseen positions at training time, we leverage 3 types of cross-validation schemes introduced in Notin et al. (2023). In the _Random_ scheme, each mutation is randomly assigned to one of five different folds. In the _Contiguous_ scheme, we split the sequence contiguously along its length, in order to obtain 5 segments of contiguous positions, and assign mutations to each segment based on the position at which it occurs. Lastly, in the _Modulo_ scheme, we assign positions to each fold using the modulo operator to obtain 5 folds overall. In all supervised settings, we report both the Spearman's rank correlation and Mean Squared Error (MSE) between predictions and experimental measurements. A more challenging generalization task would involve learning the relationship between protein representation (sequence, structure, or both) and function using only a handful of proteins, and then extrapolating at inference time to protein families not encountered during training. This setting may be seen as a hybrid between the zero-shot and supervised regimes - closer to zero-shot if we seek to predict different properties across families, and closer to the supervised setting if the properties are similar (eg., predicting the thermostability of proteins with low sequence similarity with the ones in the training set). While this study does not delve into these hybrid scenarios, the DMS assays in ProteinGym can facilitate such analyses.

Clinical datasetsGiven the restrictions on the number of labels available per gene and the discrepancies between train-validation-test splits across the different supervised baselines, we report test performance on the full set of all available ClinVar labels. We note that this may result in overestimating the performance of supervised methods for which the training data would substantially overlap with the labels considered in our ClinVar set. Further data leakage occurs for models trained on population frequencies, as most ClinVar benign labels are established based on observed frequencies in humans (situation especially evident for our indel dataset where we use frequent variants as pseudocontols). Interestingly, despite this overfitting risk and as first observed in Frazer et al. (2021), we find that most supervised methods are outperformed by the best unsupervised methods (Fig. 2).

BaselinesFor the supervised DMS benchmark, we report two suites of baselines. The first suite is comprised of models that take as inputs One-Hot-Encoded (OHE) features. Following the protocol described in Hsu et al. (2022), we augment the model inputs with predictions from several state-of-the-art zero-shot baselines: DeepSequence (Riesselman et al., 2018), ESM-1v (Meier et al., 2021), MSA Transformer (Rao et al., 2019), Tranception (Notin et al., 2022) and TranceptEVE (Notin et al., 2022). Following prior works from the semi-supervised protein modeling literature (Heinzinger et al., 2019; Dallago et al., 2021), the second suite is formed with baselines that leverage mean-pooled embeddings from several protein language models (ESM-1v, MSA Transformer and Tranception) in lieu of OHE features. We also augment these baselines with zero-shot predictions obtained with the same model used to extract the protein sequence embeddings. Lastly, we include ProteinNPT (Notin et al., 2023), a semi-supervised pseudo-generative architecture which jointly models sequences and labels by performing axial attention (Ho et al., 2019; Kossen et al., 2022) on input labeled batches. Additional details for the corresponding model architectures are reported in Appendix A.4.2. On the various clinical benchmarks, the above baselines are challenging to train given the low number of labels available per gene. We instead include several supervised baselines that have been specifically developed for variant effects predictions in humans, such as ClinPred (Alirezaie et al., 2018), MetaRNN (Li et al., 2022), BayesDel (Feng, 2017), REVEL (Ioannidis et al., 2016) and PolyPhen-2 (Adzhubei et al., 2010) (full list in A.4.3).

## 5 Results

### Substitution benchmarks

We follow the experimental protocol described in SS 4.1 and report our main results on the zero-shot DMS benchmarks in Table 2, supervised DMS benchmark in Table 3, and combined supervised and unsupervised clinical benchmarks in Fig. 2A. TranceptEVE emerges as the best overall method across the various settings. One of the key objectives of ProteinGym benchmarks is to analyze performance across a wide range of regimes to guide model selection depending on the objectives of the practitioners. To that end we also provide a performance breakdown across MSA depth, mutational depth and taxa where relevant (see Appendix A.5 and supplements). While TranceptEVE tops the ranking across the majority of metrics and settings, GEMME achieves the best performance in several categories, such as assays of viral or non-human eukaryotic proteins, and low and medium depth MSAs. While we report average performance per metric in Table 2, the _distribution_ of scores across assays is also insightful. For instance, certain models are heavily penalized in aggregate rankings due to very poor performance on a handful of assays (e.g., ESM-1v), such that looking a the median performance in lieu of the average provides a complementary viewpoint. Furthermore, although most models rank similarly under Spearman and NDCG, some have comparatively better performance in one over the other (Fig. 2B). Superior ranking under NDCG may suggest a model is better at predicting the top end of a score distribution, which may be a desirable feature when using models for design and optimization. Many of the alignment-based methods (e.g. EVmutation, WaveNet) exhibit this behavior (Fig. A1). Models with higher relative Spearman (e.g., ESM-1v and ESM-2) may be more effective for cases where the model needs to learn the full property distribution well, such as with mutation effect prediction. Lastly, in the zero-shot setting, autoregressive protein language models (e.g., Tranception, ProGen2) tend to outperform their masked language modeling counterparts (e.g., ESM models). However, in supervised settings, both types of models provide valuable embeddings for learning. The optimal method depends on the specific situation, as observed in Table 3 and Table A16. The best performance is achieved with the ProteinNPT architecture, demonstrating the value from performing self-attention alternatively across columns (i.e., amino acid tokens and labels) and rows (i.e., protein sequences) to learn a rich representation of the data.

### Indel benchmarks

The zero-shot results for an indel-compatible subset of the models in ProteinGym is shown in Table 4. The Spearman rank correlations are separated by the method used to generate test sequences: unbiased libraries, or model-designed sequences biased towards natural sequences. Model performance exhibits higher variance across assay types, with ProGen2 achieving the highest performance on Library assays (albeit with low performance on designed assays), WaveNet topping the ranking on designed assays (but with low performance on library assays), and TranceptEVE reaching high performance across both. We provide additional indel results in the supervised and clinical settings in Appendix A.5.

## 6 Resources

CodebaseA key contribution of our work is the consolidation of the numerous baselines discussed in SS 4 in a single open-source GitHub repository. While the main code for the majority of these baselines is publicly available, it often does not support fitness prediction out-of-the-box or, when it does, the codebase does not necessarily provide all the required data processing logic (e.g., pre-processing of MSAs in MSA Transformer) or handle all possible edge cases that may be encountered (e.g., scoring of sequences longer than context size in the ESM suite). Our GitHub repository addresses all of these gaps and provides a consistent interface that will aid in the seamless integration of new baselines as they become available.

  
**Model type** & **Model name** & **Spearman** & **AUC** & **MCC** & **NDCG** & **Recall** \\  Alignment-based & Site-Independent & 0.359 & 0.696 & 0.286 & 0.747 & 0.201 \\ WaveNet & 0.373 & 0.707 & 0.294 & 0.761 & 0.203 \\ EVmutation & 0.395 & 0.716 & 0.305 & 0.777 & 0.222 \\ DeepSequence (ensemble) & 0.419 & 0.729 & 0.328 & 0.776 & 0.226 \\ EVE (ensemble) & 0.439 & 0.741 & 0.342 & 0.783 & **0.230** \\ GEMME & **0.455** & 0.749 & 0.352 & 0.777 & 0.211 \\  Protein & UniRep & 0.190 & 0.605 & 0.147 & 0.647 & 0.139 \\ language & CARP (640M) & 0.368 & 0.701 & 0.285 & 0.748 & 0.208 \\ ESM-1b & 0.394 & 0.719 & 0.311 & 0.747 & 0.203 \\ ESM-2 (15B) & 0.401 & 0.720 & 0.314 & 0.759 & 0.208 \\ RITA XL & 0.372 & 0.707 & 0.293 & 0.751 & 0.193 \\ ESM-1v (ensemble) & 0.407 & 0.723 & 0.320 & 0.749 & 0.211 \\ ProGen2 XL & 0.391 & 0.717 & 0.306 & 0.767 & 0.199 \\ VESPA & 0.436 & 0.742 & 0.346 & 0.775 & 0.201 \\  Hybrid & UniRep evotuned & 0.347 & 0.693 & 0.274 & 0.739 & 0.181 \\ MSA Transformer (ensemble) & 0.434 & 0.738 & 0.340 & 0.779 & 0.224 \\ Tranception L & 0.434 & 0.739 & 0.341 & 0.779 & 0.220 \\ TranceptEVE L & **0.456** & **0.751** & **0.356** & **0.786** & **0.230** \\  Inverse & ESM-IF1 & 0.422 & 0.730 & 0.331 & 0.748 & 0.223 \\ Folding & MIF-ST & 0.401 & 0.718 & 0.311 & 0.766 & 0.226 \\ ProteinMPNN & 0.258 & 0.639 & 0.196 & 0.713 & 0.186 \\   

Table 2: **Zero-shot substitution DMS benchmark** Corrected average of Spearman’s rank correlation, AUC, MCC, NDCG@10%, and top 10% recall between model scores and experimental measurements on the ProteinGym substitution benchmark.

  
**Model** & **Model name** &  &  \\
**type** & & Contig. & Mod. & Rand. & Avg. & Contig. & Mod. & Rand. & Avg. \\  OHE & None & 0.064 & 0.027 & 0.579 & 0.224 & 1.158 & 1.125 & 0.898 & 1.061 \\  & DeepSequence & 0.400 & 0.400 & 0.521 & 0.440 & 0.967 & 0.940 & 0.767 & 0.891 \\  & ESM-1v & 0.367 & 0.368 & 0.514 & 0.417 & 0.977 & 0.949 & 0.764 & 0.897 \\  & MSAT & 0.410 & 0.412 & 0.536 & 0.453 & 0.963 & 0.934 & 0.749 & 0.882 \\  & Tranception & 0.419 & 0.419 & 0.535 & 0.458 & 0.985 & 0.934 & 0.766 & 0.895 \\  & TranceptEVE & 0.441 & 0.440 & 0.550 & 0.477 & 0.953 & 0.914 & 0.743 & 0.870 \\  Embed. & ESM-1v & 0.481 & 0.506 & 0.639 & 0.542 & 0.937 & 0.861 & 0.563 & 0.787 \\  & MSAT & 0.525 & 0.538 & 0.642 & 0.568 & 0.836 & 0.795 & 0.573 & 0.735 \\  & Tranception & 0.490 & 0.526 & 0.696 & 0.571 & 0.972 & 0.833 & 0.503 & 0.769 \\  NPT & ProteinNPT & **0.547** & **0.564** & **0.730** & **0.613** & **0.820** & **0.771** & **0.459** & **0.683** \\   

Table 3: **Supervised substitution DMS benchmark**. Corrected average of Spearman’s rank correlation and MSE between model predictions and experimental measurements. MSAT is a shorthand for MSA Transformer.

Processed datasetsWe also make publicly available all processed datasets used in our various benchmarks in a consistent format, including all DMS assays, model scores, ClinVar/gnomAD datasets, predicted 3D structures and Multiple Sequence Alignments required for training and scoring (see Section A.3.3 for more details).

WebsiteLastly, we developed a user-friendly website in which all benchmarks are accessible, with functionalities to support drill analyses across various dimensions (e.g., mutational depth, taxa) and exporting capabilities.

## 7 Conclusion

ProteinGym addresses the lack of large-scale benchmarks for the robust assessment of models developed for protein design and fitness prediction. It facilitates the direct comparison of methods across several dimensions of interest (e.g., MSA depth, mutational depth, taxa), based on different ground truth datasets (e.g., DMS assays vs Clinical annotations), and in different regimes (e.g., zero-shot vs supervised). We expect the ProteinGym benchmarks and the various data assets we publicly release along with them, to be valuable resources for the Machine Learning and Computational Biology communities, and we plan to continue updating the benchmarks as new assays and baselines become available.

    &  &  &  \\  & & Library & Designed/Natural & All & All \\   & HMM & 0.373 & 0.518 & 0.389 & 0.744 \\  & WaveNet & 0.323 & **0.597** & 0.368 & 0.720 \\  & PROVEAN & 0.306 & 0.585 & 0.347 & 0.725 \\   & RITA L & 0.443 & 0.519 & 0.457 & 0.773 \\  & ProtGPT2 & 0.185 & 0.128 & 0.191 & 0.620 \\  & ProGen2 M & **0.472** & 0.205 & **0.465** & **0.776** \\   & Tranception M & 0.395 & 0.544 & 0.394 & 0.733 \\  & Tranception L & 0.387 & 0.563 & 0.395 & 0.741 \\   & TranceptEVE M & 0.426 & 0.587 & 0.424 & 0.754 \\   

Table 4: **Zero-shot indel DMS benchmark** Spearman’s rank correlations and AUC between model scores and experimental measurements.

Figure 2: **Comparing baselines across datasets and across performance metrics** (A) Performance estimated against known clinical labels (avg. AUC over genes in ClinVar (x axis)), and DMS assays assessing the clinical effect of variants in humans (avg. Spearman (y axis)). (B) The zero-shot models’ median NDCG@10% (x-axis) against median Spearman (y-axis) on the DMS substitutions.