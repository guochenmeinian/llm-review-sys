# Conditional Mutual Information for Disentangled Representations in Reinforcement Learning

Mhairi Dunion

University of Edinburgh

mhairi.dunion@ed.ac.uk &Trevor McInroe

University of Edinburgh

t.mcinroe@ed.ac.uk &Kevin Sebastian Luck

Vrije Universiteit Amsterdam

k.s.luck@vu.nl &Josiah P. Hanna

University of Wisconsin - Madison

jphanna@cs.wisc.edu &Stefano V. Albrecht

University of Edinburgh

s.albrecht@ed.ac.uk

###### Abstract

Reinforcement Learning (RL) environments can produce training data with spurious correlations between features due to the amount of training data or its limited feature coverage. This can lead to RL agents encoding these misleading correlations in their latent representation, preventing the agent from generalising if the correlation changes within the environment or when deployed in the real world. Disentangled representations can improve robustness, but existing disentanglement techniques that minimise mutual information between features require independent features, thus they cannot disentangle _correlated_ features. We propose an auxiliary task for RL algorithms that learns a disentangled representation of high-dimensional observations with correlated features by minimising the _conditional_ mutual information between features in the representation. We demonstrate experimentally, using continuous control tasks, that our approach improves generalisation under correlation shifts, as well as improving the training performance of RL algorithms in the presence of correlated features.

## 1 Introduction

Real-world environments are diverse and unpredictable; we often cannot control correlations between environment features or even know they exist. As such, real-world Reinforcement Learning (RL) training environments can contain spurious correlations between features that are unknown or unintended by the data collector, e.g. an object correlated with colour. Furthermore, an RL agent influences the data collection through its actions, which may shift the data distribution to contain feature correlations as the agent learns, e.g. the agent position is correlated with a goal position as it learns an optimal policy. For RL agents to be resilient in the real world, it is beneficial to learn robust representations of high-dimensional observations (e.g. images). However, an agent trained with correlated data may learn a representation that encodes the spurious correlation and, therefore, cannot generalise when the correlation no longer holds (Trauble et al., 2021). For example, an autonomous driving agent trained in an environment where aggressive drivers often have green cars can encode this correlation that does not hold in real world.

Methods for disentanglement aim to separate the ground truth factors of variation that generated a high-dimensional observation, such as an image, into meaningful subspaces in the learned representation (Bengio et al., 2013). Both Higgins et al. (2017) and Dunion et al. (2023) show that disentanglement improves generalisation to visual changes in RL environments that were not seen during training. A disentangled representation can be robust to environment changes because a change in one image factor causes only a subset of features in the representation to change, while the remaining features canstill be relied upon for better generalisation. The basic principle of most disentanglement techniques is to enforce independence between groups of features in the latent representation. Therefore, common approaches to disentanglement, such as VAEs, require independence between factors of variation, i.e. uncorrelated factors. RL agents trained to learn independent features in the representation will fail to separate correlated factors because they cannot be separated into distinct independent features, and thus suffer from a failure to generalise under correlation shifts. For example, consider a scenario where the colour of an object is correlated with its size during training, and size impacts the optimal policy. Colour contains information predictive of size and vice versa. Hence, size and colour cannot be separated into distinct _independent_ features in the representation, so will be encoded into the same feature. When the agent is presented with a colour that is rarely or never previously seen with the object size, the feature representing both colour and size will change, preventing the agent from performing optimally even if it has already learned an optimal policy for the object size.

In this work, we relax the assumption of independence between factors of variation to _conditional_ independence, to learn a disentangled representation with correlated features. We propose Conditional Mutual Information for Disentanglement (CMID) as an auxiliary task that can be applied to online RL algorithms to learn a disentangled representation by minimising the conditional mutual information between dimensions in the latent representation. We use the causal graph of a Markov Decision Process (MDP) to determine a general conditioning set that can render the features in a representation conditionally independent given the conditioning set. The resulting disentangled representation avoids the reliance on spurious correlations, separating correlated factors of variation in a high-dimensional observation into distinct features in the representation, allowing for better generalisation under correlation shifts. To the best of our knowledge, this is the first approach to learn disentangled representations specifically for RL based on _conditional_ independence.

We evaluate our approach on continuous control tasks with image observations from the DeepMind Control Suite where we add correlations between object colour and properties impacting dynamics (e.g. joint positions). Our results show that CMID improves the training performance as well as the generalisation performance of the base RL algorithm, SVEA (Hansen et al., 2021), under correlation shifts, with a 77% increase in zero-shot generalisation returns on average across all tasks in our experiments. We also demonstrate improved generalisation performance compared to state-of-the art baselines: DrQ (Yarats et al., 2021), CURL (Laskin et al., 2020) and TED (Dunion et al., 2023).

## 2 Related work

### Disentangled representations

VAE approaches.Disentanglement has been studied widely in the unsupervised learning literature. Many approaches are based on the Variational Autoencoder (VAE) (Kingma and Welling, 2014). The \(\)-VAE (Higgins et al., 2017; Burgess et al., 2017) aims to improve disentanglement by scaling up the independence constraint in the VAE loss; the Factor-VAE (Kim and Mnih, 2018) encourages disentanglement through a factorial distribution of features. More recent approaches add supervision during training to bypass the impossibility of learning disentangled representations from independent and identically distributed (i.i.d.) data (Locatello et al., 2019). Shu et al. (2020) use labels of image groupings, and Locatello et al. (2020) use pairs of images. However, VAE-based approaches assume independence between factors of variation and therefore cannot disentangle correlated factors.

ICA approaches.Disentanglement is also the focus of Independent Component Analysis (ICA) (Halva et al., 2021; Hyvarinen et al., 2023), where it is referred to as 'blind source separation'. Hyvarinen and Morioka (2016) and Hyvarinen and Morioka (2017) both learn disentangled representations from time-series data under different assumptions. However, similarly to VAE approaches, the independence assumption is central to ICA and does not hold when there are correlations in the data.

Disentanglement with correlated features.Both ICA and VAE approaches to disentanglement assume independence between factors of variation (or'sources' in ICA terminology). Trauble et al. (2021) conduct an analysis of VAE-based disentanglement techniques on correlated data and show that the correlations are being encoded in the representations. They propose weak supervision in training to learn disentangled representations on correlated data and an adaptation technique to 'correct' the latent entanglement using labeled data. Recently, Funke et al. (2022) propose a Conditional MutualInformation (CMI) approach to learn disentangled representations to improve generalisation under correlation shifts in a supervised learning setting. We use a similar adversarial approach to CMI as Funke et al. (2022), but leverage the structure of an MDP to determine an appropriate conditioning set that does not require labelled data or any prior knowledge of the ground truth factors of variation.

### Representation learning in RL

**Visual invariances.** Image augmentations are commonly used to improve robustness of representations in RL (Laskin et al., 2020; Yarats et al., 2021; Hansen and Wang, 2021; Hansen et al., 2021). Several approaches have also been proposed to learn representations that are invariant to distractors in the image such as background colour (Zhang et al., 2020, 2021; Li et al., 2021; Allen et al., 2021). These methods do not account for correlations between features and do not prevent the encoding of spurious correlations. Mutual information has recently been used for invariant representation learning outside of RL by Cerrato et al. (2023) to ensure model decisions are independent of specific input features.

**Mutual information.** Mutual information (MI) based approaches are commonly used in RL for representation learning. Laskin et al. (2020) maximise similarity between different augmentations of the same observation; Mazoure et al. (2020) maximise similarity between successive observations; and Agarwal et al. (2021) use policy similarity metrics. However, these approaches all maximise MI in some way, whereas disentanglement aims to minimise MI between features in the representation.

**Disentangled representations.** To learn a disentangled representation for RL, Higgins et al. (2017) train a \(\)-VAE offline using i.i.d. data from a pre-trained agent. Dunion et al. (2023) propose an auxiliary task to learn disentangled representations online using the non-i.i.d. temporal structure of RL training data. Both of these approaches to disentanglement in RL assume independent factors of variation, so they are unable to disentangle correlated factors.

### Conditional mutual information estimators

Our approach is the first to use CMI for disentangled representations in RL. However, many approaches have been proposed to estimate CMI outside of RL. Initial approaches were extensions of MI estimators, such as Runge (2018). Recent approaches make use of advances in neural networks. Mukherjee et al. (2020) propose CCMI, using the difference between two MI terms for CMI estimation: \(I(X;Y Z)=I(X;Y,Z)-I(X;Z)\). They propose an estimator for the KL-divergence by training a classifier to distinguish the observed joint distribution from the product distribution. Mondal et al. (2020) estimates CMI by re-formulating it as a minmax optimisation problem and using a training procedure similar to generative adversarial networks. Molavipour et al. (2021) extend the classifier approach of CCMI, applying it directly to the estimation of CMI, rather than the difference of two MI terms. They use \(k\) nearest neighbours (kNN) to sample from the product of marginals distribution, and train the classifier to distinguish between the original distribution and the product of marginals. We also use kNN permutations to sample from the product of marginals distribution.

## 3 Preliminaries

**Reinforcement learning.** We assume the agent is acting in a Markov Decision Process (MDP), which is defined by the tuple \(=(,,P,R,)\), where \(\) is the state space, \(\) is the action space, \(P(s_{t+1}|s_{t},a_{t})\) is the probability of next state \(s_{t+1}\) given action \(a_{t}\) is taken in state \(s_{t}\) at time \(t\), \(R(s_{t},a_{t})\) is the reward function, and \([0,1)\) is the discount factor. The goal is to learn a policy \(\) that maximises the discounted return, \(_{}_{,}[_{t=0}^{}[^{t}R(s_{t}, a_{t})]]\). In RL from pixels, the agent receives an observation of image pixels \(_{t}^{t 3}\) at time \(t\), a high-dimensional representation of \(_{t}\). The agent learns a latent representation \(_{t}=f_{}(_{t})\) of size \(N()\), where \(f_{}:\) is an encoder parameterised by \(\). The policy \(\) is a function of the latent representation, such that \(_{t}(_{t})\). We denote the \(n\)-th component of the vector \(\) as \(z^{n}\), and all components of \(\) except \(z^{n}\) as \(^{-n}\). We use \(_{t^{},t^{}}\) to refer to representations for all consecutive timesteps from \(t^{}\) to \(t^{}\) inclusive: \(_{t^{}},_{t^{}+1},...,_{t^{ }-1},_{t^{}}\).

**Conditional mutual information.** The Conditional Mutual Information (CMI) of continuous random variables \(X\) and \(Y\) given a third variable \(Z\) measures the amount of information \(Y\) containsabout \(X\) given \(Z\) is already known (Cover and Thomas, 2006), defined as:

\[I(X;Y Z):= p(x,y,z)dxdydz \]

where lower-case letters denote instances of a random variables (e.g. \(x\) is an instance of \(X\)). By definition, CMI is given by the KL-divergence:

\[I(X;Y Z)=D_{}[p(x,y,z) p(x,z)p(y|z)]. \]

If the CMI between \(X\) and \(Y\) given \(Z\) is 0, then \(X\) and \(Y\) are conditionally independent given \(Z\):

\[I(X;Y Z)=0 X\!\!\! Y Z\,. \]

## 4 Conditional mutual information for disentanglement in RL

We propose Conditional Mutual Information for Disentanglement (CMID) as an auxiliary task that can be applied to existing RL algorithms to learn a disentangled representation with correlated data. The goal is to learn a representation with features that are conditionally independent to improve representation robustness in the presence of unintended correlations during training. We discuss the conditioning set for an MDP in Section 4.1 and describe the CMID auxiliary task in Section 4.2.

### MDP conditioning set

The causal graph for three timesteps of an MDP is shown in Figure 1. For readability, the graph shows only two state features \(s^{1}\) and \(s^{2}\), and two representation features \(z^{1}\) and \(z^{2}\), however the graph and subsequent discussion can be extended to an arbitrary number of features. The graph shows the desired causal relationships for the learned representation, such that each feature in the representation is caused by a single state feature. An overview of the relevant concepts from causality that we will use in this section is provided in Appendix A.2.

The goal is to learn a representation \(_{t}\) where features \(z^{1}_{t}\) and \(z^{2}_{t}\) are conditionally independent by blocking all backdoor paths between \(z^{1}_{t}\) and \(z^{2}_{t}\) in the causal graph (Pearl, 2009). One option is to condition on the true underlying state feature \(s^{1}_{t}\), which is the approach taken by Funke et al. (2022), but we do not usually know the true state features. Given the temporal structure of an MDP, another suitable conditioning set is the history of \(z^{1}_{t}\), denoted \(^{1}_{0:t-1}\), and the history of actions \(_{0:t-1}\), giving:

\[z^{1}_{t}\!\!\! z^{2}_{t}^{1}_{0:t-1},_{0:t -1}\,. \]

In other words, \(z^{2}_{t}\) does not contain any additional information about \(z^{1}_{t}\) given \(^{1}_{0:t-1}\) and \(_{0:t-1}\) are known. Similarly, the history of \(z^{2}_{t}\) would also make a suitable conditioning set, since the backdoor path can be blocked by \(_{0:t-1}\) and either \(^{1}_{0:t-1}\) or \(^{2}_{0:t-1}\). Conditioning on the history of actions \(_{0:t-1}\) alone is not sufficient because the action is a collider in the causal graph, so the conditioning set must also contain a parent of this collider to avoid opening up new backdoor paths (Pearl, 2009). This conditioning set means that we do not need to know the true state features \(_{t}\) to learn conditionally independent representation features \(z^{1}_{t}\) and \(z^{2}_{t}\).

To guarantee conditional independence, the conditioning set must include the full history \(^{1}_{0:t-1}\) and \(_{0:t-1}\) from the beginning of the episode to the previous timestep \(t-1\). However, conditioning on the full history makes a very large conditioning set, of size \(t(N+())\) when using one-hot

Figure 1: The conditioning set for an MDP is highlighted in grey. Representation features \(z^{1}_{t}\) and \(z^{2}_{t}\) are conditionally independent when conditioned on \(^{1}_{0:t-1}\) and \(_{0:t-1}\), where \(z^{n}_{t}\) denotes the \(n\)th dimension of \(_{t}\).

encoding of features, which can be difficult to learn. In practice, we condition on only the most recent timestep \(z_{t-1}^{1}\) and \(_{t-1}\) to adjust for the most recent correlations while keeping the auxiliary task reasonable to achieve during training. We show experimentally in Section 6 that conditioning on only the most recent timestep achieves good generalisation performance while converging to an optimal policy faster in training than larger conditioning sets that use more timesteps from the episode history.

### Conditional Mutual Information for Disentanglement

CMID is an auxiliary task to learn disentangled representations. The architecture for CMID is shown in Figure 2, and the pseudocode is provided in Algorithm 1. CMID uses the same image inputs as the base RL algorithm. Where the base algorithm uses image augmentations (Hansen et al., 2021; Yarats et al., 2021; Laskin et al., 2020a), these are also used for CMID. However, CMID does not use frame stacking because it would introduce causal relationships between features in the frame stack. For example, object velocity and positions are often extracted from the frame stack, but velocity is also a direct cause of position. CMID processes each frame individually, and representations can be stacked if required to allow velocity information to be extracted by the RL networks for policy learning.

To learn a conditionally independent representation, CMID minimises the CMI between features in the representation. For each feature \(z_{t}^{n}\) in the representation \(_{t}\), it follows from Equation 2 and the conditioning set \(_{t}^{n}=(z_{t-1}^{n},_{t-1})\) discussed in Section 4.1 that:

\[I(z_{t}^{n};_{t}^{n}_{t}^{n})=D_{}[p( _{t},_{t}^{n}) p(z_{t}^{n},_{t}^{n})p( _{t}^{-n}_{t}^{n})]. \]

As such, we minimise the KL-divergence between the joint probability distribution \(p(_{t},_{t}^{n})\) and the product of marginals \(p(z_{t}^{n},_{t}^{n})p(_{t}^{-n}_{t}^{n})\). The agent has access to samples from the joint distribution \(p(_{t},_{t}^{n})\) collected during training, e.g. from the replay buffer. To sample from the product of marginals, we use the isolated \(k\) nearest neighbours (kNN) permutation approach (Molavipour et al., 2021). For each sample \(\{_{t},_{t}^{n}\} p(_{t},_{t}^{n})\), we find the kNN of \(_{t}^{n}\) by Euclidean distance, then permute the sample with the kNN to get a sample \(\{z_{t}^{n},_{t}^{ n},_{t}^{n}\}\) where \(t^{} t\) and \(_{t}^{n}\) (the conditioning set of \(z_{t^{}}\) that is used for the permutation) is a kNN of \(_{t}^{n}\). We will use \(_{t}^{,n}\) to denote the permutations \(\{z_{t}^{n},_{t^{}}^{-n}\}\). The permuted sample \(\{_{t}^{,n},_{t}^{n}\}\) is from the distribution \(p(z_{t}^{n},_{t}^{n})p(_{t}^{-n}_{t}^{n})\). The permutation process is also depicted in Figure 2.

To minimise the KL-divergence in Equation 5, we train a discriminator \(D_{}\) adversarially to distinguish between samples \(\{_{t},_{t}^{n}\} p(_{t},_{t}^{n})\) and \(\{_{t}^{,n},_{t}^{n}\} p(z_{t}^{n},_{t}^{n})p(_{t}^{-n}_{t}^{n})\). The adversarial training encourages the encoder \(f_{}\) to ensure the two distributions are as similar as possible by minimising the cross entropy. This objective is equivalent to minimising the KL-divergence since for any two distributions \(p\) and \(q\), \(D_{}(p q)=H(p,q)-H(p)\) where \(H(p,q)\) is the cross entropy and \(H(p)\) is the entropy of \(p\) which does not depend on the learned parameters. The discriminator

Figure 2: CMID architecture: The discriminator learns to discriminate between samples from \(p(_{t} z_{t-1}^{n},_{t-1})\) and \(p(z_{t}^{n} z_{t-1}^{n},_{t-1})p(_{t}^{-n} z_{t-1} ^{n},_{t-1})\). The encoder is trained adversarially to make the two distributions similar to minimise CMI. The double slash \(^{}\) on the encoder outputs indicates where gradient flow is stopped.

\(D_{}\) is trained to discriminate between true and permuted samples for each feature \(z_{t}^{n}\) using a binary cross entropy loss:

\[_{}=_{i=0}^{N}((D_{}(_{t},_{t}^{n}))+(1-(D_{}(_{t}^{,n}, _{t}^{n})))) \]

where \(\) is the sigmoid function. The encoder \(f_{}\) is updated using only true samples \(_{t}\) to learn a representation such that the discriminator cannot determine whether the sample is true or permuted:

\[_{}=_{i=0}^{N}(1-(D_{}( _{t},_{t}^{n})))\,. \]

The loss coefficient \(\) is a hyperparameter to be tuned to the task to determine the scale of the adversarial loss compared to the RL loss that also updates the encoder \(f_{}\). For training stability, we use a momentum encoder (He et al., 2020; Laskin et al., 2020)\(f_{^{}}:\) for the conditioning set representation \(_{t-1}=f_{^{}}(_{t-1})\), where \(^{}=^{}+(1-)\).

## 5 Experimental results

Our experiments evaluate generalisation performance under correlation shifts. We evaluate zero-shot generalisation as well as adaptation with continued learning on test correlations (which differ from the training correlations). We test our approach on continuous control tasks from the DeepMind Control Suite (DMC) (Tunyasuvunakool et al., 2020) where we have created strong correlations between task-relevant features and irrelevant colours. We use a training environment with correlated variables and evaluate generalisation on a test environment under correlation shift. Our results show that the CMID auxiliary task consistently improves the generalisation of the base RL algorithm in all tasks, as well as outperforming other baselines.

### Experimental setup

Adding strong correlations to DMC.To demonstrate generalisation under correlation shifts, we add correlations between object colour and dynamics. We use two

Figure 3: Illustration of correlations with testing on reversed correlation in the cartpole environment, \(\) indicates the probability of an object/colour combination.

variations of the object controlled by the agent (A and B), each of which has a slightly different morphology (e.g. lengths, joint positions), which affects both the object appearance and the dynamics. This means each variation of the control object requires a different optimal policy. A description of the morphology variations for each task along with images are provided in Appendix D. At the start of an episode, object A or B is chosen at random with equal probability. Object A appears blue with probability 0.95 and green with probability 0.05, conversely object B is green with probability 0.95 and blue otherwise. After training, the correlation is changed (at the vertical dotted line in the graphs) and the agent continues training to assess adaptation. We test two different correlation shifts: reversed correlation, and no correlation (i.e. each object is equally likely to be blue or green). An example of the correlated setup with testing on reversed correlation for cartpole is shown in Figure 3, and these correlation probabilities are used across all DMC tasks unless stated otherwise.

Base RL algorithm.We use SVEA (Hansen et al., 2021) as the base RL algorithm which we augment with the CMID auxiliary task, called SVEA-CMID in the results. SVEA is used because it is a state-of-the-art RL algorithm that already uses image augmentations to improve robustness to an extent. An overview of SVEA is provided in Appendix A.1 and implementation details in Appendix B.

Baselines.We compare with SVEA, as the base RL algorithm for CMID, to demonstrate the extent to which the CMID auxiliary task improves performance. We also compare with DrQ (Yarats et al., 2021) as an alternative image augmentation baseline. We compare with CURL (Laskin et al., 2020) as a state-of-the-art auxiliary task based on maximising mutual information. As a disentanglement baseline that assumes independent features, we use TED (Dunion et al., 2023) to demonstrate that disentanglement techniques requiring fully independent features fail with strong correlations.

Figure 4: Generalisation to _reversed correlation_ at the vertical dotted line. Returns are the average of 10 evaluation episodes, averaged over 5 seeds; the shaded region is the standard error.

Figure 5: Generalisation to _uncorrelated_ features at the vertical dotted line. Returns are the average of 10 evaluation episodes, averaged over 5 seeds; the shaded region is the standard error.

### Generalisation results

Our results in Figure 4 show the generalisation to reversed correlation and Figure 5 shows generalisation to uncorrelated features at the vertical dotted line. In both cases, the results show that CMID improves the generalisation performance of SVEA in all tasks, as well as outperforming the other baselines. CMID achieves good zero-shot generalisation performance on the vertical dotted line, while all baselines have some failure to generalise (except where they cannot learn a reasonable policy in the hopper task). Tables showing the numerical values of the zero-shot generalisation performance are also provided in Appendix C.1. Some baselines are able to to adapt with continued learning on the test environment to eventually achieve optimal performance in line with CMID, but others are unable to recover an optimal policy after overfitting to the training correlations. CMID also improves the training performance of SVEA in all tasks, achieving higher training returns even before the switch to the test environment. Many baselines also suffer from this inability to achieve optimal performance in training because the strong correlation makes it harder to learn as an optimal agent needs to learn a different policy for each control object without relying on the colour to distinguish between control objects. Appendix C.2 shows the evaluation performance on each scenario for cartpole swingup to further demonstrate why the failure to learn an optimal policy and generalise occurs.

## 6 Discussion and analysis

In this section, we conduct more detailed analysis of CMID on the reverse correlation testing scenario for the cartpole swingup task from DMC. We also provide some further analysis on correlation strength and greyscale images in Appendix C.3 and C.4 respectively.

Mutual information.To validate that it is necessary to minimise the _conditional_ MI in our approach, we compare with an (unconditional) MI variant with no conditioning set. The results in Figure 5(a) show that SVEA-CMID outperforms SVEA-MI in training performance and generalisation. To validate our conditioning set, we also compare to the CMI approach of Funke et al. (2022) modified to an RL setting. This approach, which we call SVEA-CMI-labels, assumes access to the ground truth state features. The representation is split into subspaces corresponding to each of the state features, and a classifier is trained for each subspace to predict the state feature. The true state features can then be used as the conditioning set. Figure 5(a) also shows that SVEA-CMID outperforms SVEA-CMI-labels because of the added complexity of using the true state features and training the classifiers.

History length.We discussed in Section 4.1 that achieving full conditional independence requires conditioning on the history of representations \(_{0:t-1}\) and actions \(_{0:t-1}\). However, in practice (Section 5) we found that conditioning only on the most recent representation \(_{t-1}\) and action \(_{t-1}\) breaks the strongest correlation to achieve zero-shot generalisation. Figure 5(b) compares CMID with variations that condition on the previous two (CMID-history2) and three (CMID-history3) timesteps. The results show that it is harder to learn with the larger conditioning sets, which fail to achieve optimal performance as quickly, while CMID with one previous timestep works well in practice.

Figure 6: Ablation experiments on cartpole with generalisation to reversed correlation at the vertical dotted line: **(a)** comparison with MI and labelled features, **(b)** varying history lengths in the conditioning set, **(c)** different values of the CMID coefficient \(\), and **(d)** CMID with and and without the momentum encoder.

CMID loss coefficient.The CMID loss coefficient \(\) in Equation 7 is a hyperparameter to be tuned to the task. We found \(=0.5\) performs well in the cartpole task. The results in Figure 5(c) show that decreasing \(\) decreases generalisation performance because the agent is not prioritising disentanglement, as well as reducing training performance because it is harder to learn the rare cases with this lower priority on disentanglement. Increasing \(\) makes it harder to learn the optimal policy, decreasing the training performance.

Momentum encoder.The momentum encoder is not a strictly necessary component for minimising CMI, but we found empirically that it improves stability as it does for some other RL algorithms (He et al., 2020; Laskin et al., 2020). All algorithms used in our experiments are implemented with a momentum encoder, so we make use of the momentum encoder that is already available for many algorithms. Figure 5(d) shows the results for SVEA-CMID on the cartpole swingup task with and without a momentum encoder. CMID without a momentum encoder still improves the performance of SVEA but does not perform as well as the momentum encoder version and has higher variance.

Visualising the learned representation.Existing disentanglement metrics assume independent factors of variation so are not suitable to measure disentanglement on correlated data (Trauble et al., 2021). Instead we conducted qualitative analysis of the learned representation to visualise the disentanglement. We use integrated gradients (Sundararajan et al., 2017) to attribute the encoder output value of each feature in the representation to the input image pixels. We overlay the attributions on the original image to create saliency maps showing the parts of the image that each representation feature focuses on. We show illustrative saliency maps in Figure 7. Implementation details and saliency maps for all representation features are provided in Appendix E. The saliency maps show that SVEA encodes many image features in one representation feature, while SVEA-CMID has designated features in the representation to focus on individual features in the image, such as pole length, which is necessary to distinguish between cartpole A and B.

Robustness analysis.We analyse the robustness of the trained RL agents to unseen colours on the cartpole swingup task. Using the model at the end of training (before changing the correlation), we test the model on the same control objects but with unseen colours. We test on 216 different colours, using equally spaced RGB values. The results in Table 1 show that CMID achieves improved zero-shot generalisation performance on the unseen colours, in terms of the worst performing colour, the best performing colour and the average.

## 7 Limitations and future work

Instead of the common practice of frame stacking, we stack representations when using CMID to avoid introducing causal relationship between variables in the stack of frames as discussed in Section 4.2. Future work could consider how to adapt CMID to allow for these more complex causal relationships. The kNN permutations approach to minimise CMI also adds computational complexity to update the encoder for each kNN, adding a 67% increase in run time on average for our experiments

    & SVEA & SVEA-CMID \\  Worst colour & \(165.8 13.8\) & \(\) \\  Best colour & \(588.7 87.2\) & \(\) \\  Average & \(220.2 27.4\) & \(\) \\   

Table 1: Zero-shot generalisation to unseen colours on the cartpole task. Mean returns on 10 evaluation episodes over 5 seeds.

Figure 7: Saliency maps: (a) raw image used to calculate attributions, (b) SVEA and (c) SVEA-CMID saliency maps showing two representation features. Brighter pixels correspond to higher attributions. SVEA-CMID has designated features focusing on the pole length which it has disentangled from other features.

compared to the base RL algorithm. The number of kNN and the size of the representation could be reduced in scenarios where computation time is of high importance, and future work could consider more efficient ways to sample from the product of marginals distribution.

We evaluated our approach on tasks with correlation between object properties (that impact dynamics) and colour. This scenario already shows that state-of-the-art baselines suffer from a significant deterioration in performance under correlation shifts as well as being unable to learn an optimal policy in training for some tasks. As such, the colour correlations are sufficient to demonstrate the effectiveness of our approach in improving generalisation. However, future work could evaluate our approach on correlations with more complex distractors, such as background videos and camera angles. In particular, we use a conditioning set containing only the most recent timestep in our experiments, but more complex environments can have strong correlations over multiple timesteps (e.g. background videos) which may require more history in the conditioning set (Albrecht and Ramamoorthy, 2016). Future work could consider using more representations and actions from history in the conditioning set efficiently to apply CMID to more complex environments and correlations.

Finally, CMID learns a disentangled representation while exploring using the same strategy as used by the base RL algorithm. However, it is possible that the learning agent could discover a disentangled representation faster through a new exploration strategy that actively probes the environment to determine state structure. In the future, we plan to investigate the combination of CMID with recent advances in exploration (Henaff, 2019; Sontakke et al., 2021; Schafer et al., 2022; Zhong et al., 2022; McInroe et al., 2023) to see whether these advances allow the CMID agent to more quickly discover disentangled representations.

## 8 Conclusion

In this paper, we explored the problem of training with strong correlations and generalisation under correlation shifts in RL. Existing techniques for learning disentangled representations in RL are insufficient for real-world problems because they assume the ground truth features are independent, which is unlikely to hold in practice. We proposed Conditional Mutual Information for Disentanglement (CMID) to learn disentangled representations with correlated image features that requires only conditional independence between features. CMID is an auxiliary task that can be used with existing RL algorithms. We showed experimentally that CMID improves the training and generalisation performance of SVEA as the base RL algorithm as well as DrQ, CURL and TED baselines. CMID allows the RL agent to generalise under correlation shifts and continue learning without performance reduction as a step towards training on real-world correlated data.

## 9 Acknowledgements

This work was supported by the United Kingdom Research and Innovation (grant EP/L016834/1), EPSRC Centre for Doctoral Training in Robotics and Autonomous Systems (RAS) in Edinburgh. This work was also supported by the Academy of Finland Flagship programme: Finnish Center for Artificial Intelligence FCAI. The authors wish to acknowledge the generous computational resources provided by the Aalto Science-IT project and the CSC - IT Center for Science, Finland.