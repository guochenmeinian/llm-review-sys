# Retro-fallback: retrosynthetic planning in an uncertain world

Austin Tripp\({}^{1}\)1 Krzysztof Maziarz\({}^{2}\) Sarah Lewis\({}^{2}\)

Marwin Segler\({}^{2}\) Jose Miguel Hernandez-Lobato\({}^{1}\)

\({}^{1}\)University of Cambridge \({}^{2}\)Microsoft Research AI4Science

{ajt212,jmh233}@cam.ac.uk

{krmaziar,sarahlewis,marwinsegler}@microsoft.com

###### Abstract

Retrosynthesis is the task of proposing a series of chemical reactions to create a desired molecule from simpler, buyable molecules. While previous works have proposed algorithms to find optimal solutions for a range of metrics (e.g. shortest, lowest-cost), these works generally overlook the fact that we have imperfect knowledge of the space of possible reactions, meaning plans created by the algorithm may not work in a laboratory. In this paper we propose a novel formulation of retrosynthesis in terms of stochastic processes to account for this uncertainty. We then propose a novel greedy algorithm called retro-fallback which maximizes the probability that at least one synthesis plan can be executed in the lab. Using in-silico benchmarks we demonstrate that retro-fallback generally produces better sets of synthesis plans than the popular MCTS and retro* algorithms. We encourage the reader to view the full version of this paper at https://arxiv.org/abs/2310.09270.

## 1 Introduction

Retrosynthesis (planning the synthesis of organic molecules via a series of chemical reactions) is a common task in chemistry with a long history of automation (Vleduts, 1963; Corey and Wipke, 1969). Although the combinatorially large search space of chemical reactions makes naive brute-force methods ineffective, recently significant progress has been made by developing modern machine-learning based search algorithms for retrosynthesis (Strieth-Kalthoff et al., 2020; Tu et al., 2023). However, there remain obstacles to translating the output of retrosynthesis algorithms into real-world syntheses. One significant issue is that these algorithms have imperfect knowledge of the space of chemical reactions. Because the underlying physics of chemical reactions cannot be efficiently simulated, retrosynthesis algorithms typically rely on data-driven reaction prediction models which can "hallucinate" unrealistic outputs (Zhong et al., 2023), akin to hallucinated outputs in other domains (OpenAI, 2023). This results in synthesis plans which cannot actually be executed.

Although future advances in modelling may reduce the prevalence of infeasible reactions, we think it is unlikely that they will ever be eliminated entirely, as even the plans of expert chemists do not always work on the first try. One possible workaround to failing plans is to produce _multiple_ synthesis plans instead of just a single one: the other plans can act as _backup_ plans in case the primary plan fails. Although existing algorithms may find multiple synthesis plans, they are generally not designed to do so, and there is no reason to expect the plans found will be suitable as _backup_ plans (e.g. they may share steps with the primary plan and thereby fail alongside it).

In this paper, we present several advancements towards retrosynthesis with backup plans. First, in section 3 we explain how uncertainty about whether a synthesis plan will work in the wet lab canbe quantified with stochastic processes. We then propose an evaluation metric called _successful synthesis probability_ (SSP) which quantifies the probability that _at least one_ synthesis plan found by an algorithm will work. This naturally captures the idea of producing backup plans. Second, in section 4 we present a novel search algorithm called _retro-fallback_ which greedily optimizes SSP, and explain qualitatively how it avoids potential failure modes of other algorithms. Finally, in section 6 we demonstrate quantitatively that retro-fallback outperforms existing algorithms on an in-silico benchmark. Together, we believe these contributions form a notable advancement towards translating results from retrosynthesis algorithms into the lab. Note that this is an abbreviated version of a paper under review of another venue. The full version will be linked to at a later time.

## 2 Background: what is retrosynthesis?

Let \(\) represent the space of molecules and \(\) represent the space of reactions, where each reaction transforms a set of _reactant_ molecules in \(2^{}\) into a _product_ molecule in \(\). Retrosynthesis is usually formalized as a search problem on a graph \(\), defined implicitly via a _backward reaction model_\(B: 2^{}\) which defines all possible reactions for a given molecule. Because \(\) is combinatorially large, most search algorithms only store a small explicit subgraph \(^{}\). We refer to nodes which may have children in \(\) but have no children in \(^{}\) as _tip_ nodes.2 In general, search algorithms alternate between selecting tip nodes in \(^{}\) and querying \(B\) to add new nodes to \(^{}\) until the computational budget is exhausted (a process called _expansion_).

There are multiple ways to define the nodes and edges of \(\). We choose \(\) to be an _AND/OR graph_: a directed graph containing nodes for molecules and reactions. Edges exist only from reactions to their reactant molecules and from molecules to reactions that produce them, making \(\) bipartite.3 Examples of AND/OR graphs are shown in Figure 1. Reactions can be naturally associated with "AND nodes" because _all_ of their reactant molecules must be synthesized for the reaction to work, while molecules are associated with "OR nodes" because _any_ reaction can be used to synthesize it. However, to avoid confusion we will simply refer to the nodes of \(\) as molecules and reactions.

Given a _target molecule_\(m_{t}\), the primary goal of retrosynthesis algorithms is to find _synthesis plans_: subtrees \(T\) rooted at \(m_{t}\) containing at most one reaction to produce each molecule. For these plans to be executable, all tip nodes of \(T\) must be contained in an _inventory_\(\) of buyable molecules. Among all synthesis plans, those with minimum cost or highest quality are preferred, commonly formalized with a scalar cost/value function (Segler et al., 2018; Chen et al., 2020).

## 3 A formulation for retrosynthesis with uncertainty

### Stochastic processes over reaction uncertainty

To account for synthesis plans not working in the lab, we must first define what it means for a synthesis plan to "work". As mentioned in the introduction, the most obvious failure mode is that one of the reactions in the plan cannot be performed. This could happen for a variety of reasons: it may not produce the desired product, produce dangerous by-products, have a low yield, or require specialized expertise or equipment. Rather than trying to explicitly model these factors, we propose to collapse all nuance into a binary outcome: a reaction is either _feasible_ or _infeasible_. Not only is this easier to model, we note that ultimately if a chemist performs a reaction they will either move to the next step in the synthesis plan or admit defeat and abandon the synthesis plan (a binary outcome). Therefore, for a given chemist and lab, we postulate the existence of a binary "feasibility" function \(f^{*}:\{0,1\}\) which we will use to create and evaluate synthesis plans.

A second reason why a synthesis plan may not work is the inability to buy one of the starting molecules. This is usually not a significant issue since vendors update their inventories in real time and offer fast delivery, especially for common chemicals. However, this is not always the case: for example, some companies offer large "virtual libraries" with billions of molecules which they _believe_ they can synthesize upon request, but not with 100% reliability. To account for this, we therefore define a binary "buyability" function \(b^{*}:\{0,1\}\) analogously to \(f^{*}\).

If we knew \(f^{*}\) and \(b^{*}\) then retrosynthesis would simply be a search problem (albeit a large one). However, in practice they are unknown. A natural response is therefore to model our _epistemic_ uncertainty about \(f^{*}\) and \(b^{*}\). One approach is to model point-wise uncertainties, using some mechanism to predict \(P(f^{*}(r)=1)\) and \(P(b^{*}(m)=1)\) for all \(r\) and \(m\). Unfortunately, this approach is fundamentally incapable of capturing beliefs about _correlations_ between different outcomes. Instead, we propose to model uncertainty about \(f^{*}\) and \(b^{*}\) directly in function space using _stochastic processes_ (essentially distributions over functions). We define a _feasibility model_\(_{f}\) to be a binary stochastic process over \(\), and define a _buyability model_\(_{t}\) to be a binary stochastic process over \(\). This model class is very general: the fully-deterministic formulation from is a special case where \(_{f}\) and \(_{b}\) are degenerate distributions4, while independent outcomes are a special case where \(f\!b\) are independent Bernoulli random variables at all points. Other more interesting stochastic processes which induce correlations could be constructed by putting a prior over the parameters of a model (e.g. Bayesian neural networks (MacKay, 1992)) or using non-parametric models like Gaussian processes (Williams and Rasmussen, 2006).

### New evaluation metric: successful synthesis probability

Given \(f\) and \(b\), a successful synthesis plan \(T\) must have \(f(r)=1\) for all \(r T\) and \(b(m)=1\) for every tip molecule in \(T\). However, if we are uncertain about \(f\) and \(b\) then the distinction between "successful" and "unsuccessful" synthesis plans is not binary: every \(T\) could have some probability of succeeding. What then should be the goal of retrosynthesis?

There is no objectively correct answer to this question. Although one could try to find the synthesis plan with the highest probability of succeeding, we instead propose the goal of maximizing the probability that _any_ synthesis plan \(T^{}\) is valid. Assuming that a chemist would be willing to try all the synthesis plans in \(^{}\), this goal not only captures the spirit of finding good synthesis plans, but backup plans too. Specifically, let \((m;^{},f,b)\) represent the _successful synthesis_ of a molecule \(m\): 1 if \(m\) can be bought or synthesized using only feasible reactions in \(^{}\), otherwise 0. We write \((m)\) when \(^{},f,b\) are clear from context. If \((m_{t};^{},f,b)=1\), this implies there is a successful synthesis plan for the target molecule. We then define the _successful synthesis probability_ (SSP) as

\[}(m;^{},_{f},_{b})=P_{f_{f},b_{b}}[(m;^{},f,b)=1]\] (1)

and propose using SSP to evaluate the success of retrosynthesis algorithms. Figure 1 illustrates this and contrasts it with the traditional binary view of retrosynthesis, wherein every node is either "solved" or "unsolved". The formulation with stochastic processes implies a non-binary degree of "solvedness" for each node, representing the fraction of scenarios where each node is solved. SSP is the "solvedness" of the root node.

### Computing successful synthesis probability

Unfortunately, SSP is not easy to compute exactly: we prove in Appendix B.1 that it is NP-hard to compute (the proof essentially shows that a known NP-hard problem can be formulated as calculating SSP in a suitably-chosen graph). Although this result may appear to show that SSP is not a practical evaluation metric, it does not preclude the existence of an efficient randomized algorithm to _estimate_ SSP. This is exactly what we propose. First, note that given \(f/b\), if we define a similar concept of success for reactions then \(\) can be defined recursively in terms of its children in \(^{}\) (provided by the function \(Ch_{^{}}\)):

\[(m;^{},f,b) =[b(m),_{r Ch_{^{}}(m)} (r;^{},f,b)]\] (2) \[(r;^{},f,b) =f(r)_{m Ch_{^{}}(r)}(m ;^{},f,b)\;.\] (3)

This suggests that dynamic programming can be used to compute \((m_{t};^{},f,b)\) in polynomial time (we prove this in Appendix B.2). Second, observe that if \(f_{f},b_{b}\) then \((m_{t};^{},f,b)\) is a Bernoulli random variable with mean \(}(m_{t};^{},_{f},_{b})\). This suggests a natural estimator:

\[}(m_{t};^{},_{f},_{b},k)=_{i=1}^{k}(m_{t};^{},f_{k},b_{k}) f _{1},,f_{k}_{f}, b_{1},,b_{k}_{b}\;.\] (4)

## 4 Retro-fallback: a greedy algorithm to maximize SSP

### Existing algorithms can fail to maximize SSP

Before introducing a new algorithm, we examine what shortcomings (if any) a new algorithm should be designed to overcome. In theory, there is nothing preventing existing algorithms from effectively maximizing SSP. One could run any algorithm _independently_ of \(_{f},_{b}\), compute SSP _post hoc_, and potentially get a high value. However, to try to _ensure_ a high SSP value, it is natural to explicitly try to maximize SSP by suitably configuring the "objective" of existing algorithms. Unfortunately, existing algorithms do not have an "objective" which can be set arbitrarily. For example, retro* has an independent cost for each reaction and molecule (Chen et al., 2020), MCTS uses a reward function for individual plans (Segler et al., 2018), while algorithms like breadth-first search or proof-number search (Kishimoto et al., 2019) have no customizable rewards or costs of any kind. Because SSP depends on an entire graph and distributions over reaction feasibilities and molecule buyabilities (which may involve correlations), we think _it is not generally possible to set SSP as the objective of previously-proposed retrosynthesis algorithms_. For most algorithms, we believe the closest proxy for maximizing SSP is to optimize for individually successful synthesis plans, or plans containing individually feasible reactions and buyable molecules.

Although these objectives may seem similar, it is not difficult to imagine cases where they differ. Figure 2 illustrates such a case, wherein a synthesis plan with reactions \(r_{1},r_{2}\) has been found and the algorithm must choose between expanding \(m_{3}\) or \(m_{4}\). Individually either molecule could be promising, but any new synthesis route proceeding via \(r_{3}\) will also use \(r_{1}\) and is therefore prone to fail alongside the existing plan if \(r_{1}\) turns out to be infeasible. Even though \(m_{4}\) may not be the best choice in general, an algorithm maximizing SSP would clearly need to account for the interaction between existing and prospective synthesis plans in its decision making, which simply is not possible by reasoning about individual synthesis plans in isolation. This provides compelling motivation to develop algorithms which account for interactions between synthesis plans.

Figure 1: Illustration of retrosynthesis with and without uncertainty. **Left:** Traditional binary formulation: molecules are either buyable or not. All synthesis routes which use only buyable molecules are “solutions.” Here, \(m_{t}\) is solved via both \(r_{2}\) and \(r_{3}\). **Right:** Formulation with stochastic processes, where each reaction may fail and molecules may or may not be buyable. Each sample from \(_{f},_{b}\) implies that each node is either solved or unsolved (top). Averaging over these samples produces a non-binary “solvedness” value for each node, indicated in shades of green (bottom).

Figure 2: AND/OR graph illustrating how maximizing SSP can be different from finding individually successful synthesis plans (cf. 4.1). Green nodes are part of a synthesis plan, red nodes are not.

### Ingredients for an informed, greedy search algorithm

A natural starting point for an algorithm specifically designed to maximize SSP is to estimate how different actions might affect SSP, and choose actions accordingly. Theorem B.1 suggests that computing this exactly will scale poorly to larger search graphs, and therefore we assert that the basis of any efficient algorithm must be _samples_ from \(_{f},_{b}\). Furthermore, equations 2-3 show how the success of any given node in \(^{}\) can be computed reasonably efficiently in terms of the success values of other nodes. This broadly suggests it might be possible to modify equations 2-3 to perform _counterfactual reasoning_: i.e. predicting what \((m_{t};^{},f,b)\)_could be_ if \(^{}\) were modified.

We take inspiration from the greedy retro* algorithm (Chen et al., 2020) and instead consider the counterfactual of simultaneously expanding all tip nodes on an _entire synthesis plan_\(T^{}\). For a tip molecule \(m\), we have that \((m;,f,b)=b(m)\) from equation 2. If \(b(m)=1\) then expanding \(m\) cannot change \((m)\) (it is already at its maximum value), but if \(b(m)=0\) then it is possible that \((m)\) will change to \(1\) upon expansion. This is a natural entry point for a _search heuristic_: let \(h:\) be a heuristic function mapping molecules to probabilities that \((m)\) will become \(1\) upon expansion (assuming \(b(m)=0\)).

We then assume that expanding a synthesis plan \(T\) amounts to setting \((m)=1\) for all non-buyable tip nodes \(m T\) with \((m)=0\)_independently_ with probability \(h(m)\). For any \(m^{}\), we define \((m;^{},f,b,h)\) to be the largest expected value of \((m)\) obtained across synthesis plans \(T^{}\) for \(m\) under the expansion scenario above and use \((m)\) when \(^{},f,b,h\) are clear from context. Critically, because all tip nodes are considered independently under the expansion process, the optimal plan for every molecule \(m\), that is, the plan that maximizes the expectation of \((m)\) under the expansion of its tip nodes, will consist of sub-plans which are also individually optimal. This allows us to define \(\) implicitly with the recurrence relation:

\[(m;^{},f,b,h) =[b(m),h(m)]&\\ [b(m),_{r Ch_{^{}}(m)}(r;^{ },f,b,h)]&\] (5) \[(r;^{},f,b,h) =f(r)_{m Ch_{^{}}(r)}(m;^ {},f,b,h)\.\] (6)

Equations 5-6 effectively propagate information "up" the graph towards \(m_{t}\), until \((m_{t})\) contains a useful quantity: the expected value of \((m_{t})\) upon optimal expansion. A similar technique can be used to propagate this information "down" the graph towards the tip nodes to decide which tip node(s) to expand. For any \(m^{}\), let \((m;^{},f,b,h)\) represent the maximum expected value of \((m_{t})\) upon expansion across all synthesis plans \(T^{}\) that contain both \(m_{t}\) and \(m\). We call a plan achieving \((m;^{},f,b,h)\) a _constrained_ optimal plan for \(m\), while a plan achieving \((m;^{},f,b,h)\) is called an _unconstrained_ optimal plan for \(m\). The assumption of independence implies that the constrained optimal plan for node \(n\) can be formed by replacing part of the constrained optimal plan for the parent of \(n\) with the unconstrained optimal plan for \(n\), leading to the following recursive definition of \(\):

\[(m;^{},f,b,h) =(m;^{},f,b,h)&m\\ _{r Pa_{^{}}(m)}(r;^{},f,b,h)& m\] (7) \[(r;^{},f,b,h) =0&(r;^{},f,b,h)=0\\ (Pa_{^{}}(r))^{ }}(r))}&(r;^{},f,b,h)>0\] (8)

Here, \(Pa_{^{}}\) yields node's parents in \(^{}\). The last remaining question is how \(\) and \(\) can be computed. If \(^{}\) is acyclic then \(\) and \(\) can be calculated in linear time by iterating equations 5-6 from tip nodes to the root, then iterating equations 7-8 from root to tip nodes. If there are cycles then the cost could potentially be larger, but in Appendix B.2 we prove it is at most quadratic. In any case, it is clear that \(\) and \(\) can form the basis for an efficient search algorithm.

### Retro-fallback: a full greedy algorithm

Creating a full greedy algorithm requires aggregating information across many samples from \(_{f}\) and \(_{b}\) to decide which tip node to expand at each step. Recalling our motivation of producing synthesis plans with backup plans, we propose to greedily expand molecules which are predicted to formsuccessful synthesis plans _in scenarios where all existing synthesis plans currently fail_. Specifically, we propose to choose molecules by maximizing the objective

\[(m;^{},_{f},_{b},h)=_{f_{f},b _{b}}[_{(m_{t};^{},f,b)=0}[ (m;^{},f,b,h)]]\,\] (9)

which is proportional to the expected value of \((m)\) conditioned on \((m_{t})=0\). We call the resulting algorithm _retro-fallback_ (from "retrosynthesis with fallback plans") and state it explicitly in Algorithm 1. The sections are colour-coded for clarity. After initializing \(^{}\) to just the target molecule, the algorithm performs \(L\) iterations of expansion (although this termination condition could be changed as needed). In each iteration, first the values of \(\), \(\), and \(\) are computed for each sample.5 Next, the algorithm checks whether there are no nodes to expand or whether the root molecule is synthesized for every sample, and if so terminates (both of these conditions mean no further improvement is possible). Finally, a tip node maximizing equation 9 is chosen and used to expand \(^{}\).

## 5 Related Work

Mechanistically, retro-fallback most closely resembles retro* (Chen et al., 2020): both perform a bottom-up and top-down update to determine the value of each potential action and select actions greedily. In fact, if costs are defined to be negative log probabilities then the updates for \(\) and \(\) are essentially equivalent to the "reaction number" and "retro* value" updates from (Chen et al., 2020). The key difference is that retro-fallback performs parallel updates using many samples from \(_{f}\) and \(_{b}\) and combines information from all samples to make a decision, while retro* uses only a single cost. This difference is what allows retro-fallback to directly optimize SSP, while retro* cannot. This ability also distinguishes retro-fallback from other search algorithms such as MCTS (Segler et al., 2018) and proof-number search (Kishimoto et al., 2019).

Works outside of multi-step planning are only tangentially related. Works proposing search heuristics for retrosynthesis search algorithms complement rather than compete with our work: such heuristics could also be applied to retro-fallback. Generative models to produce synthesis plans effectively also function as heuristics. Finally, methods to predict individual chemical reactions are sometimes also referred to as "retrosynthesis models". Retro-fallback solves a different problem: it is a _multi-step_ search algorithm which would _use_ a reaction prediction model to define the search graph.

## 6 Experiments

### Experiment Setup

We test retro-fallback on four retrosynthesis tasks using four different feasibility models. We base all of our feasibility models on the pre-trained template classifier from Chen et al. (2020) restricted to the top-50 templates. We vary our feasibility model across two axes: the _marginal_ feasibility assigned to each reaction and the _correlation_ between feasibility outcomes. Marginally, we consider a constant value (C) and a value which decreases with the rank (R) for marginal feasibility. For correlations we consider all outcomes being independent (I) or determined by a latent GP model (G) which positively correlates similar reactions. Details of these models are given in Appendix C.1.1. Analogous to Chen et al. (2020), we create a buyability model based on eMolecules library, although we use the September 2023 version and exclude molecules with excessively long shipping times (details in Appendix C.1.2).

We compare retro-fallback to breadth-first search (an uninformed search algorithm) and heuristic-guided algorithms retro* (Chen et al., 2020) and MCTS (Segler et al., 2018; Genheden et al., 2020; Coley et al., 2019). MCTS and retro* were re-implemented and adapted to maximize SSP, which most notably entailed replacing costs or rewards from the backward reaction model \(B\) with quantities derived from \(_{f}\) and \(_{b}\) (see Appendix C.1.4 for details) and standardizing their search heuristics. All algorithms are run with a budget of 200 calls to \(B\). The presence of heuristics makes comparing algorithms difficult because the choice of heuristic will strongly influence an algorithm's behaviour. We decided to use an _optimistic_ heuristic and a heuristic based on the synthetic accessibility (SA) score (Ertl and Schuffenhauer, 2009), which has been shown to be a good heuristic for retrosynthesis in practice despite its simplicity (Skoraczynski et al., 2023).

We test all algorithms on 500 molecules randomly selected from the GuacaMol test set (Brown et al., 2019), which contains drug-like molecules known to be synthesizable, but with a wide range of difficulties (details in Appendix C.1.3). Our primary evaluation metric is the SSP values estimated with \(k=10\,000\) samples, averaged over the 500 molecules.

### How effective is retro-fallback?

Since retro-fallback is designed to maximize SSP, the most basic question is whether it does so more effectively than other algorithms. We found that a minority of molecules are "trivial", and all algorithms achieve a SSP of \( 1\) within a few iterations. In Figure 3 we plot the average SSP for all "non-trivial" test molecules as a function of number of reaction model calls using an optimistic heuristic for all feasibility models. Retro-fallback clearly outperforms the other algorithms in all scenarios by a wider margin than the error bars. The difference is larger for the feasibility models with independent reactions than with GP-correlated reactions. We suspect this is because there are many synthesis plans with similar reactions: when reaction outcomes are uncorrelated these synthesis plans act as backup plans for each other, but not when they are correlated. The same trends can be seen when using the SA heuristic function (shown in Figure C.1). Overall, this result shows us what

Figure 3: Results with optimistic heuristic on “non-trivial” molecules. C and R refer to constant and rank marginal probabilities, while I and G refer to independent and GP-induced correlations. Solid lines are sample means (averaged across molecules), with error bars representing standard errors.

we expect: that retro-fallback maximizes the metric it was specifically designed to maximize more effectively than baseline algorithms.

A natural follow-up question is whether retro-fallback also performs well by metrics other than SSP. In Figures C.2-C.3 we show that for both the optimistic and SA score heuristics retro-fallback is able to find potential solutions for more molecules and produce "best" solutions whose quality closely matches other algorithms. This suggests that it functions as an effective search algorithm even just considering the metrics from past papers.

## 7 Discussion, Limitations, and Future Work

In this paper we presented a novel evaluation metric for retrosynthesis algorithms called "successful synthesis probability" (SSP), proposed a novel algorithm called retro-fallback to greedily maximize SSP, and showed experimentally that retro-fallback is more effective than previously-proposed algorithms. Together, these contributions compensate for the limited ability of existing algorithms to explicitly account for reaction failure and propose backup plans.

One challenge for deploying retro-fallback in practice is the lack of established feasibility and buyability models. To our knowledge, retro-fallback is the first algorithm which can fully utilize uncertainty on the _function_ level, so it is not surprising that not much work has been done in this area before. We therefore do not view this as a limitation of our work, but rather as motivation for subsequent research into quantifying the uncertainty of reaction models (especially by domain experts).

Our contributions also have some important conceptual limitations. First, chemists care about the cost and length of synthesis plans in addition to whether they will work, and we do not see a way to incorporate these directly into either our stochastic process formalism or retro-fallback. Second, while our definition of SSP considers an arbitrary number of plans, in practice chemists are unlikely to try more than \( 10\) plans before moving on to something else. Nonetheless, all algorithms make assumptions which are untrue, and we are optimistic that the assumptions made by retro-fallback are sufficiently benign that the algorithm can still be useful in practice.

Finally, since retro-fallback uses a search heuristic there is potential to learn this heuristic using the results of past searches ("self-play"). We naturally expect this to improve performance and view this as an exciting direction for future work.