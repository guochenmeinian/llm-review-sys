# JiuZhang3.0: Efficiently Improving Mathematical Reasoning by Training Small Data Synthesis Models

Kun Zhou\({}^{1}\)

Beichen Zhang\({}^{2}\)

Equal contributions.

Jiapeng Wang\({}^{2}\)

Zhipeng Chen\({}^{2}\)

Wayne Xin Zhao\({}^{2}\)

Jing Sha\({}^{3}\)

Zhichao Sheng\({}^{3}\)

Shijin Wang\({}^{3,4}\)

Ji-Rong Wen\({}^{2}\)

\({}^{1}\)School of Information, Renmin University of China

\({}^{2}\)Gaoling School of Artificial Intelligence, Renmin University of China

\({}^{3}\)iFLYTEK Research \({}^{4}\)iFLYTEK AI Research (Central China)

francis_kun_zhou@163.com, {zhangbeichen724,batmanfly}@gmail.com,

{wangjp1010,zhipeng_chen,jrwen}@ruc.edu.cn, {jingsha,sjwang3}@iflytek.com

###### Abstract

Mathematical reasoning is an important capability of large language models (LLMs) for real-world applications. To enhance this capability, existing work either collects large-scale math-related texts for pre-training, or relies on stronger LLMs (_e.g.,_ GPT-4) to synthesize massive math problems. Both types of work generally lead to large costs in training or synthesis. To reduce the cost, based on open-source available texts, we propose an efficient way that trains a small LLM for math problem synthesis, to efficiently generate sufficient high-quality pre-training data. To achieve it, we create a dataset using GPT-4 to distill its data synthesis capability into the small LLM. Concretely, we craft a set of prompts based on human education stages to guide GPT-4, to synthesize problems covering diverse math knowledge and difficulty levels. Besides, we adopt the gradient-based influence estimation method to select the most valuable math-related texts. The both are fed into GPT-4 for creating the knowledge distillation dataset to train the small LLM. We leverage it to synthesize 6 million math problems for pre-training our JiuZhang3.0 model. The whole process only needs to invoke GPT-4 API 9.3k times and use 4.6B data for training. Experimental results have shown that JiuZhang3.0 achieves state-of-the-art performance on several mathematical reasoning datasets, under both natural language reasoning and tool manipulation settings. Our code and data will be publicly released in https://github.com/RUCAIBox/JiuZhang3.0.

## 1 Introduction

Large language models (LLMs) have shown remarkable capabilities on a variety of tasks [1; 2; 3]. However, they still struggle in solving complex mathematical problems . Recent work has shown that it is an effective approach to training LLMs on math-related data for improving the mathematical reasoning ability [5; 6]. Typically, they either collect the math-related data from the available corpora (_e.g.,_ webpages and books) for pre-training [7; 8; 9], or rely on stronger LLMs to synthesize high-quality math problems for fine-tuning [10; 11; 12]. Despite the success, existing approaches would generally cause large training or inference costs. Due to the complexity and diversity of mathematical problems, the former type of work mostly needs to collect a large-scale corpus (_e.g.,_ 120B data for Deepseek-Math) for training, which greatly increases the training cost [5; 7; 8]. Similarly, to guarantee the knowledge coverage and effectiveness of the synthetic problems, the latter type of work relies onstronger LLMs with larger scales (_e.g.,_ GPT-4) to create massive math problems, leading to larger inference cost [6; 10; 11]. In Figure 1, we show our estimated total costs of re-implementing two math-related LLMs (>$40000), details are in Appendix A

In this work, we aim to develop a relatively low-cost data synthesis approach for improving the mathematical reasoning abilities of LLMs. Our key idea is that **the data synthesis capability can be well learned by small LLMs**. Here, _small_ is a relative wording, which is in contrast with the extremely large or costly data synthesis models used in prior studies [6; 13], such as GPT-4 or Qwen-72B. Actually, existing work [10; 14; 15] has extensive evidence of strong learning and adaptation abilities of small LLMs for new tasks and domains with suitable strategies (_e.g.,_ training with high-quality supervised data), including math, science, and complex multimodal tasks. However, this exploration has been neglected in prior efforts on data

This attempt can be essentially generalized to a broader problem: _whether a small (or weak) model can produce high-quality data that is useful for training a large (or strong) model?_ Inspired by this motivation, we seek to train a relatively small yet powerful LLM for synthesizing high-quality math-related data.

However, due to the diverse and complex nature of math problems, it is challenging to train well-performing small LLMs for synthesizing high-quality ones. Although we can leverage GPT-4, it is not efficient to use it for synthesizing a large-scale knowledge distillation (KD) dataset. Specifically, we aim to build the KD dataset through a low-cost strategy, but it can sufficiently capture diverse and useful knowledge about math problem synthesis. Thus, we should guarantee the knowledge coverage and usefulness of the instances within the KD dataset. To achieve it, we first craft a set of prompts, and each prompt corresponds to an education stage of humans, _e.g.,_ middle school and college. Using the above prompts, the instances within the dataset can well cover broad mathematical knowledge and different difficulty levels. Besides, for usefulness, we estimate the influence of the available math-related texts and downstream tasks, by computing the gradient similarity between their corresponding synthetic data and the task instances. Then, we select the top-ranking math-related texts into the KD dataset, which are high-value ones with more positive influence on downstream tasks. By feeding the texts with prompts into GPT-4, we collect the outputs to build the KD dataset.

Based on the KD dataset, we train DeepSeekMath-7B as our data synthesis model, which is much smaller than other commonly-used LLMs in existing work [6; 12; 13], _e.g.,_ GPT-4 and Qwen-72B. Owing to our data selection strategy, we only require GPT-4 to generate 9,335 instances based on the selected most valuable texts for training it. Then, we utilize the crafted prompts to guide it for synthesizing high-quality problems. Benefiting from the strong data synthesis capability of the small model, we only need to synthesize 5,984,584 high-quality math problems (4.6B tokens) for pre-training our JiuZhang3.0. Thus, the total inference and training cost is much less than existing work, as shown in Figure 1. After pre-training, we also collect open-source math instructions to fine-tune JiuZhang3.0. The experimental results have shown that JiuZhang3.0 can mostly outperform state-of-the-art methods across 18 evaluation datasets, in both the natural language reasoning and tool manipulation settings. Our contributions are summarized as follows:

(1) our research provides compelling evidence that it is feasible to efficiently train a small LLM (7B) for synthesizing training data to improve the mathematical reasoning of LLMs. As results shown in Section 4.3, its synthetic data is more useful than larger LLMs in improving the performance.

(2) we propose an efficient solution for training LLMs to improve mathematical reasoning, which only needs to invoke GPT-4 API 9.3k times and pre-train on 4.6B high-quality synthetic data, with nearly 20% total cost of existing state-of-the-art methods.

(3) JiuZhang3.0 achieves state-of-the-art performance among open-source LLMs on several tasks and settings, _e.g.,_ 52.8 (JiuZhang3.0-7B) vs. 50.2 (DeepSeekMath-7B-RL) on MATH, 89.8 (JiuZhang3.0-8\(\)7B) vs. 86.4 (MAmmoTH2-8\(\)7B-Plus) on GSM8k in the natural language reasoning setting.

Figure 1: The comparison of existing work and our method in task performance and the total cost.

Related Work

Large Language Models.LLMs have demonstrated remarkable capabilities in a variety of NLP tasks, and commercial LLMs like ChatGPT, Claude, and Gemini [2; 16; 17], represent cutting-edge capabilities. Meanwhile, the performance of open-source models (_e.g.,_ LLaMA-3, Miktral) has also developed rapidly [18; 19]. To further improve the capability of LLMs on special tasks or domains, existing work mainly focuses on optimizing the following aspects: \((1)\) prompt engineering such as chain-of-thought and tree-of-thought [20; 21]; \((2)\) continual pre-training on a domain-specific or task-specific corpus, improving the model to deal with downstream tasks [5; 7; 8; 22; 23]; \((3)\) supervised fine-tuning, which involves fine-tuning the model on related instruction datasets, enhancing LLMs to follow special task instructions [24; 25]; \((4)\) other strategies including RLHF , tool augmentation , decoding optimization  and _et al._ We aim to efficiently improve the capability of LLMs for mathematical reasoning by pre-training on synthetic data.

Mathematical Reasoning.Despite the impressive progress, mathematical reasoning remains a weak aspect of LLMs. To enhance LLMs' ability in mathematical reasoning, researchers have proposed a surge of methods from the aspects of prompting, pre-training and fine-tuning. For prompting, the chain-of-thought (CoT) prompts have been widely used to guide LLMs for performing multi-step reasoning on complex math problems . Based on it, following work utilizes tools [27; 29; 30; 31; 32; 33] and verifiers [9; 34; 35; 36], to further improve the accuracy of the mathematical reasoning process. For pre-training, existing work [5; 7; 9; 37; 38] collects a large-scale math-related corpus and continually pre-training open-source LLMs on it. Supervised fine-tuning methods focus on using relatively less high-quality data for training the LLM, which are typically math-related instructions [10; 12; 39]. Recent studies show that the complexity of mathematical reasoning demands high-quality instruction pairs, leading to reliance on advanced LLMs like GPT-4 for data synthesis [6; 40; 41]. The pre-training and fine-tuning methods generally lead to large training and data annotation costs, respectively. Our work aims to train a small LLM specially for math problem synthesis, which can efficiently produce sufficient data for training.

Data Synthesis.For complex tasks and scenarios (_e.g.,_ mathematical reasoning), it is necessary to collect a substantial amount of data for training the LLM to enhance it. However, the available data may not be sufficient, hence researchers have explored using automatically synthetic data with consistent distribution to real data, to enrich the training corpus [42; 43; 44; 45; 46; 47; 48]. For data synthesis on mathematical reasoning tasks, existing work can be roughly categorized into the following two types, according to their based guided information. The first type of work starts with existing problems or math-related texts to synthesize similar problems or solutions [10; 40; 41; 49; 50]. The other type of work relies on available knowledge points, and devises special prompts to guide LLMs for synthesizing related problems with the solutions [6; 51]. As correctness is important, the two types of work generally design rules to check and remove wrong ones. In this work, based on the data synthesis model, we also construct the multi-source math corpora, and craft several prompts to guide it in producing diverse and useful math problems.

## 3 Approach

In this section, we present our approach that aims to train a small LLM for synthesizing math problems. First, we initialize the data synthesis model by training it on the KD dataset, composed of crafted prompts, randomly sampled math-related texts, and the corresponding synthetic problems and solutions from GPT-4. Then, we improve its data synthesis capability by retraining it on the updated knowledge distillation dataset, where we add the high-value math-related texts selected by gradient-based influence estimation strategy. Finally, we utilize the model to synthesize massive high-quality math problems for training JiuZhang3.0, based on the multi-source math-related corpus.

### Preliminary

We focus on training a small data synthesis LLM, for synthesizing high-quality math problem-solution pairs to pre-train LLMs and improve its mathematical reasoning capability. To guarantee the quality of the synthetic data, we utilize GPT-4 to create the knowledge distillation (KD) dataset \(_{KD}=\{p_{i},t_{i},_{i},_{i}\}_{i=1}^{N}\) for training the small LLM, where the math-related text \(t_{i}\) and the prompt \(p_{i}\) are the input of GPT-4, \(_{i}\) and \(_{i}\) are its synthetic math problem and solution respectively. Then, we train the small LLM on \(_{KD}\) to imitate the data synthesis ability of GPT-4. Finally, we leverage it to synthesize the pre-training dataset \(=\{q_{i},s_{i}\}_{i=1}^{M}\) based on all the collected math-related texts with randomly selected prompts \(\{p_{i},t_{i}\}_{i=1}^{M}\), which are used for training our JiuZhang3.0.

### Initializing Data Synthesis Model

In this work, we consider the natural language reasoning and tool manipulation settings, where LLMs require solving the problem by generating a natural language solution  and an executable program with external interpreters , respectively. Thus, we train the data synthesis LLMs on the initial KD dataset, containing special prompts, math-related texts, and GPT-4 outputs for the two settings.

#### 3.2.1 Prompts for Math Problem Synthesis

We aim to craft a prompt set that can well cover the knowledge points and difficulty levels in human math education. Thus, for the natural language reasoning and tool manipulation settings, we manually craft prompt templates respectively, and each corresponds to a certain education stage.

Prompts for Natural Language Reasoning.We consider the following 4 human education stages and 4 worldwide competitions, _i.e.,_ Grade School, Middle School, High School, and College; AMC (American Mathematics Competition) 8, AMC 10, AMC 12, and AIME (American Invitational Mathematics Examination). Based on these, we design 8 prompts with corresponding instructions and guidelines. We show an example of grade school math problem synthesis:

_**# Instruction:** Create an age-appropriate math word problem for grade school students based on the provided math content.

_**# Guidelines:** [Problem]:** Craft a concise math word problem suitable for grade school, focusing on basic arithmetic operations, number sense, simple shapes,... [Solution]: Provide a clear, step-by-step solution to the problem using simple language that a grade school student could understand,..._

Prompts for Tool Manipulation.We consider 2 types of math problems,_i.e.,_ Grade School and Secondary School Competitions, as the competition math problems may need tools for advanced math operations (_e.g.,_ integral computation), while grade school math problems are much easier and can be solved by basic operations (_e.g.,_ +-*/). As tool manipulation solves the problem via executable programs, we use more words in prompts to emphasize the data format and show an example as follows:

_**# Instruction:** Please gain inspiration from the following random math content to create a high-quality... Present your output in two distinct sections: [Problem Description] and [Solution].

Figure 2: The pipeline of our approach. We first initialize the data synthesis LLM by distilling the knowledge from GPT-4 on randomly sampled data, then boost it using the high-value data selected by gradient-based value estimation strategy, finally utilize it for synthesizing data to train JiuZhang3.0.

_## Guidelines: [Problem]: This should be completely self-contained, providing all the contextual information one needs to understand,... [Solution]: Offer a comprehensive, correct solution that accurately addresses the [Problem] you provided using Python code,..._

_## Example:[Problem Description] Janet buys 3 pounds of broccoli,... [/Problem Description] [Solution] def spending(): cost = 4...[Solution]_

#### 3.2.2 Knowledge Distillation from GPT-4

Based on the prompts, we build the KD dataset to train our data synthesis LLM. We randomly sample 5,336 math-related texts, and concatenate each one with a randomly selected prompt, to compose the input _i.e.,_\([p_{i};w_{i}]\). Then, we feed it into GPT-4, and extract the synthetic math problem \(_{i}\) and solution \(_{i}\) from its output using regular expressions, to compose the KD dataset \(_{KD}=\{p_{i},w_{i},_{i},_{i}\}_{i=1}^{N_{ini}}\). Next, we utilize it to train the synthesis model, and the learning objective is:

\[L(_{syn})=_{i=1}^{N_{ini}} P([_{i};_{i}]|[p_{i};w_{ i}]),\] (1)

where \(_{syn}\) denotes the parameters of the data synthesis LLM. In this way, we can teach it to generate new math problem-solution pairs based on prompts and math-related texts by imitating GPT-4.

### Boosting Synthesis Model using High-Value Data

After initialization, we further improve the data synthesis LLM using high-value KD data. However, it would be costly if we first utilize GPT-4 to generate candidates and then select high-value ones. Therefore, we propose an efficient way that leverages the data synthesis LLM for generating candidates, and then selects valuable ones to feed into GPT-4. Specifically, we incorporate the gradient-based method  to estimate the influence of each synthetic instance for downstream math-related tasks, and select the top-ranking ones to update the KD dataset for retraining the data synthesis LLM.

#### 3.3.1 Gradient-based Data Value Estimation

According to the influence formulation , at a certain training step of a model parameterized by \(\), the influence of a training instance \(z\) on another instance \(z^{}\) can be estimated by computing the similarity between their produced gradients, denoted as:

\[(z,z^{})( l(z,), l(z^{ },)).\] (2)

By using it, we can measure the value of each synthetic data by computing its gradient similarity with downstream math-related task data. Concretely, we first train a reference model using LoRA, then compute its gradients on LoRA parameters as the features, to help estimate the data value.

Training Reference Model using LoRA.Inspired by existing work , we train a LLM for mathematical problem solving using LoRA  as the reference model. As LoRA only requires to optimize the low-rank adapters in the LLM, we can efficiently train the reference model on limited computation resources, and reduce the number of trainable parameters for efficient computation of gradient similarity. Besides, to further reduce the training cost, we randomly select a subset of synthetic math problems generated by the data synthesis model, denoted as \(_{lora}=\{q_{i},s_{i}\}_{i=1}^{M_{l}}\). Then, we train the reference model to predict the solution based on the given problem, denoted as:

\[L_{ref}(_{lora})=_{i=1}^{M_{l}} P(s_{i}|q_{i})\] (3)

where \(_{lora}\) denotes the parameters of LoRA, and \(M_{l}\) is the number of training data.

Computing Gradient Features.After training the reference model, we compute the gradients of LoRA parameters as the feature of each synthetic instance. As their dimension is large, we follow existing work  that performs random projection to obtain the low-dimensional features as:

\[l_{ref}(z,_{lora})=^{} l_{ref}(z,_{lora}),\] (4)where \(z= p_{i},w_{i},q_{i},s_{i}\) denotes a synthetic instance, \(^{d^{} d}\) is a projection matrix initialized by the Rademacher distribution, its entries are -1 or 1, \(d^{}\) and \(d\) are the dimensions before and after projection, respectively. According to the Johnson-Lindenstrauss Lemmas , this operation can nearly preserve the gradient distances, ensuring the usefulness of the low-dimensional features.

Estimating Data Value.By using Eq. 4, we can compute the gradient features for synthetic instances. Then, we randomly sample \(M_{D}\) instances from the training sets of downstream math-related datasets \(\{z_{i}^{}\}_{i=1}^{M_{D}}\), where \(z_{i}^{}=_{i},_{i}\), and also compute their gradient features. Next, we estimate the value of each synthetic instance by computing the similarity between its gradient feature and the average feature of all the sampled downstream instances as:

\[V(z)=l_{ref}(z,_{lora}),}_{i=1}^{M_{D}}l_{ref}(z_{i}^{},_{lora}) ,\] (5)

where \((x,y)\) computes the cosine similarity between the two vectors. In this way, the instance with higher data value would lead to a more positive influence on the downstream math-related tasks.

#### 3.3.2 Retraining Data Synthesis Model

Based on the estimated values, we can rank all the synthetic instances, and the top-ranking \(N_{add}\) ones can be regarded as the most valuable data \(\{ p_{i},w_{i},q_{i},s_{i}\}_{i=1}^{N_{add}}\) for improving downstream math-related tasks. Thus, we utilize GPT-4 to regenerate the synthetic math problems based on their prompts and original math-related texts, to acquire corresponding more high-quality math problems and solutions. Then, we add the new GPT-4 synthetic data \(\{p_{i},w_{i},_{i},_{i}\}_{i=1}^{N_{add}}\) into the KD dataset, and the new data is capable of guiding the small LLM to generate more useful math problems for downstream tasks. Next, we retrain the data synthesis LLM with the updated KD dataset using Eq. 1.

### Pre-training JiuZhang3.0 using Synthetic Data

After training the data synthesis LLM, we construct the multi-source corpus containing rich math-related texts to cover more knowledge and scenarios. Then, we synthesize massive math problems based on it, which are used for pre-training JiuZhang3.0.

Constructing Multi-source Corpus.We consider the following data types and select the corresponding open-source datasets to compose the math-related multi-source corpus.

\(\)_Webpages:_ we use the OpenWebText corpus , which consists of 6.3M math-related web documents extracted from Common Crawl.

\(\)_Books:_ we use the Mathpile-textbook dataset , including 4K educational textbooks, lecture notes and synthetic books.

\(\)_Papers:_ we use the Mathpile-Arxiv dataset , and select the high-quality ones according to the estimated scores (0.6-0.9), which are released by AutoMathText .

\(\)_QA Data:_ we select the StackExchange subset of the MMIQC dataset , which contains 1.2M processed real-world math question-answering pairs.

\(\)_Wikipedia:_ we use the Mathpile-Wikipedia dataset , consisting of 106K documents from math-related entries in Wikipedia.

Data Synthesis for Training JiuZhang3.0.For each instance within the multi-source corpus, we randomly select a prompt from the prompt set and embed the text into the prompt to compose the input. Then, we feed inputs into the data synthesis model, to generate the math problems and solutions for composing the synthesis dataset \(=\{q_{i},s_{i}\}_{i=1}^{M}\). Here, we follow existing work  to filter out the instances with \(10\)-grams overlap to both inputs and outputs from test sets of downstream evaluation tasks. We synthesize about 6M math problems (4.6B tokens) in total, which are used for pre-training JiuZhang3.0 to predict the solution based on the given problems.

## 4 Experiments

### Experimental Settings

For our JiuZhang3.0, we follow existing work  that train the 7B, 8B and 8\(\)7B versions based on Mistral-7B , LLaMA-3-8B , and Mistral-8\(\)7B . During training, we first pre-train it on our synthetic 4.6B math problem-solution pairs and then fine-tune it on the collected multiple open-source instruction datasets. We evaluate JiuZhang3.0 in two settings, _i.e.,_ natural language reasoning and tool manipulation. More details about the fine-tuning data, evaluation datasets, baseline methods, and implementation details are in Appendix B, C, D and E, respectively.

### Results and Analysis

Natural Language Reasoning.The results of this setting are shown in Table 1. First, the baseline methods trained on math-related data perform better than others. Among them, DeepSeekMath-7B is the best-performed base LLM, and DeepSeekMath-7B-RL also performs better than other baselines, since they have been pre-trained on 120B corpus containing rich math-related data. Besides, KPMathDSMath-7B and MAmmoTH2 also perform well. Concretely, KPMathDSMath-7B is trained on nearly 1M synthetic math problems produced by GPT-4, and MAmmoTH2 also utilizes the GPT-4, Mistral-8\(\)7B, and Qwen-72B to extract and refine the problems existing in the webpages. The acquired problems can greatly improve their performance in math problem solving. In our approach, we also utilize synthetic math problems to train our JiuZhang3.0-7B and 8B models. Differently,

  
**Models** & **GSM8k** & **MATH** & **SVAMP** & **ASDiv** & **MAWPS** & **CARP** & **Avg.** \\  ChatGPT & 76.6 & 38.2 & 83.7 & 87.7 & 96.9 & 41.3 & 70.7 \\ GPT-4 & 92.2 & 65.4 & 92.9 & 94.3 & 96.6 & 53.6 & 82.5 \\  Qwen-1.5-110B & 85.4 & 49.4 & 86.2 & 85.1 & 94.3 & **53.6** & 75.7 \\ Qwen-1.5-72B & 77.6 & 39.4 & 83.1 & 85.1 & 95.8 & 53.0 & 72.3 \\ Mistral-8\(\)7B & 74.4 & 29.0 & 76.5 & 78.5 & 93.9 & 38.8 & 65.2 \\ Lemma-34B & 60.2 & 24.6 & 68.0 & 75.6 & 89.8 & 36.5 & 59.1 \\ Intern-Math-20B & 64.9 & 27.4 & 74.9 & 79.6 & 94.4 & 42.3 & 63.9 \\ ChatGLM-Math-32B & 82.6 & 40.6 & - & - & - & - & - \\ MAmmoTH2-8\(\)7B-Plus & 86.4 & 47.0 & 90.0 & 92.2 & **97.0** & 45.8 & 76.4 \\  JiuZhang3.0-8\(\)7B (Ours) & **89.8** & **53.8** & **90.2** & **93.1** & 96.7 & 52.3 & **79.3** \\   DeepSeek-7B & 13.6 & 4.8 & 40.8 & 52.1 & 65.4 & 10.3 & 31.2 \\ Mistral-7B & 41.2 & 13.6 & 64.7 & 68.5 & 87.5 & 14.9 & 48.4 \\ LLaMA-3-8B & 54.5 & 19.6 & 68.5 & 72.8 & 90.5 & 29.2 & 55.9 \\ Gemma-7B & 54.1 & 19.6 & 69.7 & 74.2 & 89.0 & 30.5 & 56.2 \\ Qwen-1.5-7B & 60.5 & 28.2 & 64.9 & 74.9 & 90.1 & 38.6 & 59.5 \\   Lemma-7B & 39.2 & 18.4 & 56.9 & 69.0 & 82.7 & 31.8 & 49.7 \\ InternLM-Math-7B & 45.9 & 15.8 & 67.3 & 71.2 & 88.3 & 28.0 & 52.8 \\ Rho-1-Math-7B & 66.3 & 31.0 & 78.5 & 79.2 & 94.0 & 36.7 & 64.3 \\ DeepSeekMath-7B & 64.1 & 34.2 & 73.7 & 82.7 & 92.7 & 44.4 & 65.3 \\  Mistral-7B-MMIQC & 75.0 & 34.2 & 73.5 & 82.1 & 90.1 & 36.5 & 65.2 \\ MetaMath-Mistral-7B & 77.8 & 29.6 & 79.6 & 81.2 & 93.7 & 30.5 & 65.4 \\ Abel-7B-002 & 80.4 & 29.6 & 78.8 & 82.7 & 93.5 & 33.2 & 66.4 \\ WizardMath-7B-1.1 & 82.2 & 32.8 & 80.7 & 84.2 & 93.8 & 31.9 & 67.6 \\ Math-Shepherd-Mistral-7B & 84.3 & 34.4 & 82.9 & 82.8 & 92.5 & 32.9 & 68.3 \\ KPMath-DSMath-7B & 83.9 & 48.8 & 81.5 & 88.9 & 94.8 & - & - \\ MAmmoTH2-7B-Plus & 84.2 & 46.2 & 90.3 & 90.3 & 95.8 & 44.3 & 75.2 \\ MAmmoTH2-8B-Plus & 84.4 & 41.2 & 89.9 & 89.9 & 97.1 & 44.8 & 74.6 \\ DeepSeekMath-7B-Instruct & 82.3 & 45.8 & 83.7 & 90.1 & 95.7 & 45.8 & 73.9 \\ DeepSeekMath-7B-RL & 88.2 & 50.2 & 87.3 & 91.8 & 95.5 & **51.6** & 77.4 \\  JiuZhang3.0-7B (Ours) & **88.6** & **52.8** & **90.4** & **92.6** & **97.3** & 51.0 & **78.8** \\ JiuZhang3.0-8B (Ours) & **88.6** & 51.0 & 89.4 & **92.6** & 97.1 & 50.9 & 78.3 \\   

Table 1: Results on 6 datasets in the natural language reasoning. The best and second-best ones among LLMs with similar scales are marked in bold and underlined respectively.

our used synthesis model is a much smaller 7B LLM, which has been trained by distilling the data synthesis capability from GPT-4. Thus, it can guarantee the quality of the synthetic data, and helps JiuZhang3.0 models perform the best across most of the dataset. The higher quality also reduces the data amount requirement for pre-training. Owing to our designed high-value data selection strategy, we can also reduce the times of invoking GPT-4 API for knowledge distillation. As noted in Figure 1, the total cost of our approach is nearly only 20% of the compared baselines, indicating its efficiency.

The results of other datasets with different data formats or related to other fields are shown in Table 2. As the listed datasets focus on evaluating the different aspects, the performance of LLMs also differ a lot. For TabMWP, AQuA, and OCW-Math, our JiuZhang3.0-8B and JiuZhang3.0-8\(\)7B achieve the best performance. The three datasets require the understanding of table data, algebra, and undergraduate-level science knowledge respectively, which may have been covered in our synthetic math problems guided by the multi-source corpus. However, our JiuZhang3.0 models perform not well on MMLU-STEM. It indicates the shortcoming of our approach that our prompts and math-related texts might not well cover the knowledge from other subjects.

  
**Models** & **TabMWP** & **AQuA** & **SAT-Math** & **M-STEM** & **OCW-Math** & **Avg.** \\  ChatGPT & 82.0 & 53.9 & 78.1 & 63.5 & 11.0 & 57.7 \\ GPT-4 & 90.8 & 76.9 & 96.9 & 77.1 & 26.5 & 73.6 \\  Qwen-1.5-110B & 80.5 & 64.6 & **87.5** & 71.5 & 14.0 & 63.6 \\ Qwen-1.5-72B & 56.1 & 55.1 & **87.5** & 68.8 & 7.7 & 55.0 \\ Mistral-8x7B & 67.3 & 48.0 & 65.6 & 62.3 & 8.8 & 50.4 \\ Llemma-34B & 57.1 & 46.1 & 71.9 & 54.3 & 11.8 & 48.2 \\ Intern-Math-20B & 63.4 & 44.1 & 65.6 & 62.3 & 7.0 & 48.5 \\ MAmmoTH2-8x7B-Plus & 62.7 & 55.9 & 81.2 & **71.8** & 18.8 & 58.1 \\  JiuZhang3.0-8x7B (Ours) & **84.7** & **65.4** & 81.2 & 66.9 & **23.5** & **64.3** \\   Mistral-7B & 37.3 & 34.3 & 56.2 & 49.5 & 3.3 & 36.1 \\ LLaMA-3-8B & 67.5 & 46.5 & 56.2 & 54.4 & 7.7 & 46.5 \\ Gemma-7B & 60.9 & 42.9 & 71.9 & 57.7 & 4.8 & 47.6 \\  Llemma-7B & 49.2 & 37.8 & 62.5 & 45.8 & 7.7 & 40.6 \\ Rho-1-Math-7B & 55.5 & 49.2 & 75.0 & 54.9 & 6.2 & 48.2 \\ DeepSeeMath-7B & 69.8 & 51.6 & 84.4 & 56.1 & 17.6 & 55.9 \\ DeepSeeMath-7B-Instruct & 70.5 & 60.6 & 84.4 & 57.9 & 19.5 & 58.6 \\ MAmmoTH2-7B-Plus & 54.7 & **62.2** & 84.4 & 64.0 & 15.1 & 56.1 \\ MAmmoTH2-8B-Plus & 75.1 & 57.5 & **87.5** & **65.7** & 14.7 & 60.1 \\  JiuZhang3.0-7B (Ours) & 74.8 & 59.4 & 81.2 & 53.6 & 20.2 & 57.8 \\ JiuZhang3.0-8B (Ours) & **79.2** & **62.2** & 84.4 & 60.4 & **21.3** & **61.5** \\   

Table 2: Results on 5 other datasets with different data formats or related to interdisciplinary fields, and we abbreviate MMLU-STEM into M-STEM. The best and second-best methods among LLMs with similar scales are marked in bold and underlined respectively.

  
**Models** & **GSM8k** & **MATH** & **G-Hard** & **SVAMP** & **TabMWP** & **ASDiv** & **MAWPS** & **Avg.** \\  ChatGPT (PAL) & 78.6 & 38.7 & 67.6 & 77.8 & 79.9 & 81.0 & 89.4 & 73.3 \\ GPT-4 (PAL) & 97.0 & 69.7 & 77.6 & 94.8 & 95.9 & 92.6 & 97.7 & 89.3 \\  CodeLama & 34.0 & 16.6 & 33.6 & 59.0 & 61.4 & 79.6 & - & - \\  MAmmoTH-7B-Mistral & 75.0 & 40.0 & - & - & - & - & - & - \\ MathCoder-7B-CL & 67.8 & 30.2 & - & 70.7 & - & - & - & - \\ ToRA-7B-Code & 72.6 & 44.6 & 56.0 & 70.4 & 51.6 & 78.7 & 91.3 & 66.5 \\ MARHO-OM-7B & 74.5 & 47.7 & - & - & - & - & - & - \\ MOMS-CODE-7B & 73.9 & 44.3 & - & 76.4 & - & 78.6 & - & - \\ OpenMath-Mistral-7B & 80.2 & 44.5 & 63.7 & 82.4 & 70.0 & 82.7 & 95.4 & 74.1 \\ Rho-1-Math-7B-Code & 81.3 & 51.8 & 63.1 & 80.8 & 70.1 & 85.5 & 94.5 & 75.3 \\  JiuZhang3.0-7B (Ours) & 82.4 & 53.0 & **64.9** & **89.2** & 75.6 & **88.3** & 96.6 & 78.6 \\ JiuZhang3.0-8B (Ours) & **82.9** & **53.4** & 64.4 & **89.2** & **79.9** & 87.5 & **97.3** & **79.2** \\   

Table 3: Results on 6 mathematical reasoning datasets under the tool manipulation setting. The best and second-best methods are marked in bold and underlined respectively.

Tool Manipulation.The results are shown in Table 3. We can see that our JiuZhang3.0-7B and 8B models outperform all the baseline methods by a large margin, indicating the effectiveness of our approach in this setting. The reason is that we synthesize massive math problems in this format, which can teach JiuZhang3.0 models to accurately utilize tools by generating programs. Besides, the mixed synthetic math problem from the natural language reasoning setting can also benefit the required capabilities for this setting. Different from the natural language reasoning setting, JiuZhang3.0-8B (based on LLaMA-3-8B) performs better than the 7B version (based on Mistral-7B). The reason may be that LLaMA-3-8B owns stronger code synthesis and tool manipulation capability than Mistral-7B.

### Further Analysis

Performance w.r.t. Pre-training Data Amount.In this part, we study how the scaling of synthetic data amount affects the model performance. We train Mistral-7B and LLaMA-3-8B using varying ratios of our synthetic entire dataset, _i.e.,_ 20%, 40%, 60%, 80%, 100%, and report the performance on GSM8k, MATH, and ASDiv under the natural language reasoning setting. For comparison, we also show the results of the best-performed base LLM, _i.e.,_ DeepSeekMath-Base-7B.

As shown in Figure 3, with the increasing of the training data ratio, the performance of our model improves consistently. Based on Mistral-7B, it can outperform the best-performed baseline using only 80% or 60% of the pre-training data, indicating the high quality of our synthetic pre-training data. Based on LLaMA-3-8B, it can perform better than the baseline using 40% or even 20% data, and the performance is consistently better than using Mistral-7B. It demonstrates that LLaMA-3-8B can better adapt into our synthetic data. Besides, the performance of our model can surpass the baseline more on MATH, which is a very complex dataset consisting of competitive problems, exhibiting the superiority of our method for improving the advanced mathematical reasoning capability.

  
**Variation** & **Models** & **GSM8k** & **MATH** & **ASDiv** & **CARP** \\  - & Ours & 78.6 & **32.8** & **84.5** & 36.2 \\   & w/o Prompt Set & 76.9 & 27.8 & 81.4 & 34.5 \\  & w/o Math-related Texts & 76.4 & 28.6 & 83.8 & 31.9 \\  & w/o Boosting Retraining & 77.9 & 31.6 & 83.8 & 34.7 \\  & w/o Value Estimation & 78.8 & 31.0 & 83.4 & 34.3 \\  & w/o using GPT-4 for Boosting & 79.0 & 28.4 & 82.9 & **36.3** \\   & - ChatGPT & 77.0 & 26.6 & 83.0 & 34.3 \\  & - Mistral-8\(\)7B & 77.6 & 26.8 & 82.9 & 33.1 \\  & - DeepSeekMath-RL-7B & 77.1 & 27.2 & 82.5 & 32.9 \\  & - LLaMA-3-8B-Instruct & 75.7 & 26.2 & 81.5 & 31.4 \\   & - Random Sampling & 78.8 & 31.0 & 83.4 & 34.3 \\  & - Perplexity & 77.5 & 30.8 & 83.1 & **36.3** \\   & - Reward Model & 78.0 & 31.6 & 84.3 & 34.8 \\   & - One-shot ICL & **79.2** & 30.2 & 83.3 & 36.0 \\   

Table 4: Ablation and variation studies in the natural language reasoning setting. We randomly sample 100k synthetic data and 50k instruction data for efficient test.

Figure 3: Performance changes with the increasing of the pre-training data proportion for our approach. We also show the best-performed base LLM DeepSeekMath-7B using dashed line.

Ablation Study.We conduct the ablation study to verify the effectiveness of key components in our proposed method. We test the following variations based on our approach, _i.e.,_ (1) _w/o Prompt Set_: uses a simple prompt for guiding data synthesis instead of our crafted prompt set; (2) _w/o Mathematical Texts_: directly synthesizes the math problems without math related texts; (3) _w/o Boosting Retraining_: uses the data synthesis model without retraining; (4) _w/o Value Estimation_: ignores the estimated value but randomly samples the instances for boosting training; (5) _w/o using GPT-4 for Boosting_: directly uses the high-value instance for boosting data synthesis model instead of using GPT-4. Limited by the computing resource, we conduct the ablation study under the natural language reasoning setting, and use 100k synthetic instances and randomly select 50k instructions from the instruction set. We report the results on GSM8K, MATH, ASDiv and CARP-en.

As shown in Table 4, all the variations mostly underperform the original model, indicating the effectiveness of all the components. Besides, the variation w/o using GPT-4 for Boosting performs slightly better in GSM8k and CARP, but degrades a lot in MATH (32.8\(\)27.8). A possible reason is that it can benefit from the selected high-value data. But without the help of GPT-4, it can not synthesize helpful complex math problems for the competitive problems within MATH dataset.

Variation Study for Data Synthesis LLMs.To verify the effectiveness of our trained data synthesis LLM, we conduct the variation study using other existing LLMs for synthesizing the pre-training data. We select the following four LLMs, _i.e.,_ ChatGPT, Mixtral-8\(\)7B, DeepSeeKMath-RL-7B, and LLaMA-3-8B-Instruct to replace our data synthesis LLM. We follow the efficient test setting in the ablation study, and report the results on GSM8K, MATH, ASDiv and CARP-en.

As shown in Table 4, all the variations mostly perform worse than the original model. It demonstrates that existing LLMs without adapted training might not be suitable to directly synthesize the data for pre-training. Besides, the performance of all the variation degrades a lot in MATH, which consists of complex competitive problems. It indicates that these existing LLMs are hard to synthesize the data that is useful for improving the performance in solving complex math problems.

Variation Study for Data Selection Strategies.To study the effectiveness of our gradient-based data selection strategy, we implement the following variations that replace it by other methods, _i.e.,_ (1) _Random Sampling_: randomly samples the same number of instances; (2) _Perplexity_: selects the instances with lowest perplexity evaluated by Mistral-7B; (3) _Reward Model_: uses a well-trained reward model  for scoring; (4) _One-shot ICL_: concatenates the synthetic math problem and solution with the downstream task data to construct the one-shot in-context learning (ICL) example, and computes the decrease of loss as the estimated value . We follow the efficient test setting.

As shown in Table 4, our original model mostly performs the best among all the variations, indicating the superiority of our gradient-based strategy. Whereas, the variation using one-hot ICL performs relatively better than others, and achieves the best performance on GSM8k. As the problems in GSM8k typically require more natural language reasoning steps, the ICL loss can well detect the instances with helpful context for solving these problems. However, it performs not well on MATH, where the math problems are complex and require using more math symbols and formulas.

## 5 Conclusion

In this paper, we proposed an efficient way to improve the mathematical reasoning of LLMs, where we trained a small LLM to synthesize sufficient high-quality math problems for pre-training. Concretely, we crafted a set of prompts that cover the knowledge and difficulty levels of human education stages, and selected the high-value math-related texts for downstream math-related tasks via the gradient-based strategy. Then, we fed them into GPT-4 to create the knowledge distillation dataset, which can better teach the data synthesis model to generate diverse and useful math problems. We utilized the synthetic data to pre-train JiuZhang3.0, and the whole process only required to invoke GPT-4 API 9.3k times and pre-train on 4.6B data. JiuZhang3.0 achieved state-of-the-art performance on several datasets under the natural language reasoning and tool manipulation settings, surpassing competitive LLMs that requires much larger cost on data synthesis or pre-training.