# Faster Diffusion: Rethinking the Role of the Encoder

for Diffusion Model Inference

Senmao Li\({}^{1}\)1, Taihang Hu\({}^{1}\)1, Joost van de Weijer\({}^{2}\), Fahad Shahbaz Khan\({}^{3,4}\), Tao Liu\({}^{1}\)

**Linxuan Li\({}^{1}\), Shiqi Yang\({}^{5}\), Yaxing Wang\({}^{1}\)2  Ming-Ming Cheng\({}^{1}\), Jian Yang\({}^{1}\)**

\({}^{1}\)VCIP, CS, Nankai University, \({}^{2}\)Computer Vision Center, Universitat Autonoma de Barcelona

\({}^{3}\)Mohamed bin Zayed University of AI, \({}^{4}\)Linkoping University, \({}^{5}\)Independent Researcher, Tokyo

{senmaonk, hutaihang00, lotolcy0, linxuanli520, shiqi.yang147.jp}@gmail.com

joost@cvc.uab.es, fahad.khan@liu.se, {yaxing,cmm,csjyang}@nankai.edu.cn

https://sen-mao.github.io/FasterDiffusion

###### Abstract

One of the main drawback of diffusion models is the slow inference time for image generation. Among the most successful approaches to addressing this problem are distillation methods. However, these methods require considerable computational resources. In this paper, we take another approach to diffusion model acceleration. We conduct a comprehensive study of the UNet encoder and empirically analyze the encoder features. This provides insights regarding their changes during the inference process. In particular, we find that encoder features change minimally, whereas the decoder features exhibit substantial variations across different time-steps. This insight motivates us to omit encoder computation at certain adjacent time-steps and reuse encoder features of previous time-steps as input to the decoder in multiple time-steps. Importantly, this allows us to perform decoder computation in parallel, further accelerating the denoising process. Additionally, we introduce a prior noise injection method to improve the texture details in the generated image. Besides the standard text-to-image task, we also validate our approach on

Figure 1: Results of our method for a diverse set of generation tasks. We significantly increase the image generation speed (second/image).

other tasks: text-to-video, personalized generation and reference-guided generation. Without utilizing any knowledge distillation technique, our approach accelerates both the Stable Diffusion (SD) and DeepFloyd-IF model sampling by 41\(\%\) and 24\(\%\) respectively, and DiT model sampling by 34\(\%\), while maintaining high-quality generation performance.

## 1 Introduction

One of the popular paradigms in image generation, Diffusion Models (DMs) [1; 2; 3] have recently achieved significant breakthroughs in various domains, including text-to-video generation [4; 5; 6], personalized image generation [7; 8; 9] and reference-guided image generation [10; 11; 12]. While diffusion models produce images of exceptional visual quality, their primary drawback lies in the prolonged inference time. The original diffusion model had an inference time several orders of magnitude slower than, for instance, GANs. One of the challenges hindering the acceleration of diffusion models is their inherent sequential denoising process, which limits the possibilities of effective parallelization.

To improve inference time speed of diffusion models, several methods have been developed, that can roughly be divided in two sets of approaches. Firstly, involving step reduction, the aim is to reduce the number of sampling steps within diffusion model inference, such as DDIM  and DPM-Solver , which have significantly reduced the number of sampling steps. Secondly, in contrast, knowledge distillation progressively distills a slow (many-step) teacher model into a faster (few-step) student model [15; 16]. Some recent works [17; 18; 19] excel at generating high-fidelity images in a few-step sampling scenario but face challenges in maintaining quality and diversity in one-step sampling. The main drawback of the distillation methods is that they require retraining to perform the distillation into faster diffusion models.

Orthogonal to these methods, we take a closer look at the sequential nature of the denoising process. We focus on the characteristics of the encoder in pretrained diffusion models (e.g., the SD and the DiT 2) Interestingly, based on our analysis presented in Sec 3.2, we discover that encoder features change minimally (Fig. 3a) and have a high degree of similarity (Fig. 2 (top)), whereas the decoder features exhibit substantial variations across different time-steps (Fig. 3a and Fig. 2 (bottom)). This insight is relevant because it allows us to circumvent the computation of the encoder during multiple time-steps. As a consequence, the decoder computations which are based on the same encoder input can be performed in parallel. Instead, we reuse the computed encoder features computed at one time-step (since these change minimally) as input to adjacent decoders during the following time-steps. More recently, both DeepCache and CacheMe leverage feature similarity to achieve acceleration. However, they rely on sequential denoising, as well as Cache requires fine-tuning. Unlike these methods, our approach supports parallel processing, which leads to faster inference (Tab. 2).

We show that the proposed propagation scheme accelerates the SD sampling by \(24\%\), DeepFold-IF sampling by \(18\%\), and DiT sampling by \(27\%\). Furthermore, since the same encoder features (from previous time-steps) can be used as the input to the decoder of multiple later time-steps, this makes it possible to conduct multiple time-steps decoding concurrently. This parallel procession accelerates SD sampling by \(41\%\), DeepFloyd-IF sampling by \(24\%\), and DiT sampling by \(34\%\). Furthermore, to alleviate the deterioration of the generated quality, we introduce a prior noise injection strategy to preserve the texture details in the generated images. With these contributions, our proposed method achieves improved sampling efficiency while maintaining high image generation quality.

Figure 2: Visualising the hierarchical features 1. We applied PCA to the hierarchical features following PnP  and used the top three leading components as an RGB image for visualization. The encoder features change minimally and have similarities at many time-steps (top), while the decoder features exhibit substantial variations across different time-steps (bottom).

Importantly, our method can be combined with several approaches existing to speed up DMs. The main advantage of our method with respect to distillation-based approaches, is that our method can be applied at inference time, and does not require retraining a new faster distillation model; a process that is computationally very demanding and infeasible for actors with limited computational budget. Finally, we evaluate the effectiveness of our approach across a wide range of conditional diffusion-based tasks, including text-to-video generation (e.g., Text2Video-zero  and VideoFusion ), personalized image generation (e.g., Dreambooth ) and reference-guided image generation (e.g., ControlNet ).

To summarize, we make the following contributions:

* We conduct a thorough empirical study of the features of the UNet in the diffusion model showing that encoder features vary minimally (whereas decoder feature vary significantly).
* We propose a parallel strategy for diffusion model sampling at adjacent time-steps that significantly accelerates the denoising process. Importantly, our method does not require any training or fine-tuning technique.
* Furthermore, we also present a prior noise injection method to improve the image quality (mainly improving the quality of high-frequency textures).
* Our method can be combined with existing methods (like DDIM, and DPM-solver) to further accelerate diffusion model inference time.

## 2 Related Work

**Denoising diffusion model.** Recently, Text-to-image diffusion models [1; 24; 25; 26] have made significant advancements. Notably, Stable Diffusion and DeepFloyd-IF stand out as two of the most successful diffusion models available within the current open-source community. These models, building upon the UNet architecture, are versatile and can be applied to a wide range of tasks, including image editing [27; 28], super-resolution [29; 30], segmentation [31; 32], and object detection [33; 34]. Given the strong scalability of transformer networks, DiT  investigates the transformer backbone for diffusion models.

**Diffusion model acceleration.** Diffusion models use iterative denoising with UNet for image generation, which is time-consuming. There are plenty of works trying to address this issue. One strategy involves employing efficient diffusion model solvers, such as DDIM  and DPM-Solver , which have demonstrated notable reductions in sampling steps. Additionally, ToMe  exploits token redundancy to minimize the computations necessary for attention operations . Conversely, knowledge distillation methods, exemplified by techniques like progressive simplification by student models [15; 16], aim to streamline existing models. Some recent studies combine model compression with distillation to achieve faster sampling [37; 38]. Orthogonal to these approaches, we introduce a novel method for enhancing sampling efficiency in DMs inference. We show that our method can be combined with several existing speed-up methods for further acceleration.

DeepCache and CacheMe are two recent works that leverage feature similarity to achieve acceleration. DeepCache  adopts a strategy of rudely reusing features cached from the previous step, requiring iterative denoising. Furthermore, CacheMe  requires additional fine-tuning for

Figure 3: **Analyzing the UNet in Diffusion Model. (a) Feature evolving across adjacent time-steps is measured by MSE. (b) We extract the hierarchical features output of different layers of the UNet at each time-step, average them along the channel dimension to obtain two-dimensional hierarchical features, and then calculate their _Frobenius norm_. (c) The hierarchical features of the UNet encoder show a lower standard deviation, while those of the decoder exhibit a higher standard deviation.**

better performance. In contrast, our approach enables parallel processing, leading to considerably faster inference.

## 3 Method

We first briefly revisit the architecture of the Stable Diffusion (SD) (Sec. 3.1), and then conduct a comprehensive analysis for the hierarchical features of the UNet (Sec. 3.2). Our analysis shows that it is possible to parallelize the diffusion model denoising process partially. Thus, we introduce a novel method to accelerate the diffusion sampling while still largely maintaining the generation quality and fidelity (Sec. 3.3).

### Latent Diffusion Model

In the diffusion inference stage, the denoising network \(_{}\) takes as input a text embedding \(\), a latent code \(}\) and a time embedding, predicts noise, resulting in a latent \(}\) using the DDIM scheduler :

\[}=}{_{t}}}}+}}-1}-}-1} _{}(},t,),\] (1)

where \(_{t}\) is a predefined scalar function at time-step \(t\) (\(t=T,...,1\)). The typical denoising network uses a UNet-based architecture. It consists of an encoder \(\), a bottleneck \(\), and a decoder \(\), respectively (Fig. 3(b)). The hierarchical features extracted from the encoder \(\) are injected into the decoder \(\) by a skip connection (Fig. 3(a)). For convenience of description, we divide the UNet network into specific blocks: \(=\{()_{s}\}\), \(=\{()_{}\}\), and \(=\{()_{s}\}\), where \(\{8,16,32,64\}\) (see Fig. 3(b)). Both \(()_{s}\)3 and \(()_{s}\) represent the block layers with input resolution \(\) in both encoder and decoder, respectively.

Diffusion Transformer (DiT)  is a novel architecture for diffusion models. It replaces the UNet backbone with a transformer, which consists of 28 blocks. Based on our observations, we define the first 18 blocks as the encoder, and the remaining 10 blocks as the decoder (see Appendix A.3).

### Analyzing the UNet in Diffusion Model

In this section, we take the UNet-based diffusion model as example to analyze the properties of the pretrained diffusion model. We delve into the UNet which consists of the encoder \(\), the bottleneck \(\)

Figure 4: (a) Standard SD sampling. (b) UNet architecture. (c) Encoder propagation. We omit the encoder at certain adjacent time-steps and reuse in parallel the encoder features in the previous time-steps for the decoder. Applying encoder propagation for uniform strategy every two iterations. Note, at time-step \(t\)-1, predicting noise does not require \(z_{t-1}\) (i.e., Eq. 1: \(z_{t-2}=}{_{t-1}}}z_{t-1}+} }-1}-}}-1 _{}(},t-1,)\)). (d) Decoder propagation. The generated images often fail to cover some specific objects in the text prompt. For example, given one prompt case “A man with a beard wearing glasses and a beanie”, this method fails to generate the **glasses** subject. See Appendix F for quantitative evaluation. (e) Applying encoder propagation for non-uniform strategy. By benefiting from our propagation scheme, we are able to perform the decoder in parallel at certain adjacent time-steps.

and the decoder \(\), for a deeper understanding of the different parts of the UNet. Note the following observed properties also exist in DiT (see Appendix A.3).

Feature evolution across time-steps.We experimentally observe that the encoder features exhibit a subtle variation at adjacent time-steps, whereas the decoder features exhibit substantial variations across different time-steps (see Fig. 2(a) and Fig. 2). Specifically, given a pretrained diffusion model, we iteratively produce a latent code \(}\) (see Eq. 1), and the corresponding hierarchical features: \(\{(z_{t},c,t)_{s}\}\), \(\{(z_{t},c,t)_{8}\}\), and \(\{(z_{t},c,t)_{s}\}\) (\(\{8,16,32,64\}\))4, as shown in Fig. 3(b). Here, we analyze how the hierarchical features change at adjacent time-steps. To achieve this goal, we quantify the variation of the hierarchical features as follows:

\[_{(.)_{s}}=}\|(z_{t},c,t) _{s}-(z_{t-1},c,t-1)_{s}\|_{2}^{2},\] (2)

where \(d\) represents the number of channels in \((z_{t},c,t)_{s}\). Similarly, we also compute \(_{(.)_{8}}\) and \(_{(.)_{s}}\).

As illustrated in Fig. 2(a), for both the encoder \(\) and the decoder \(\), the curves exhibit a similar trend: in the wake of an initial increase, the variation reaches a plateau and then decreases, followed by a continuing growth towards the end. However, the extent of change in \(_{(.)_{s}}\) and \(_{(.)_{s}}\) is quantitatively markedly different. For example, the maximum value and variance of the \(_{(.)_{s}}\) are less than 0.4 and 0.05, respectively (Fig. 2(a) (zoom-in area)), while the corresponding values of the \(_{(.)_{s}}\) are about 5 and 30, respectively (Fig. 2(a)). Furthermore, we find that \(_{(.)_{64}}\), the change of the last layer of the decoder, is close to zero. This is due to the output of the denoising network being similar at adjacent time-steps . In conclusion, the overall feature change \(_{(.)_{s}}\) is smaller than \(_{(.)_{s}}\) throughout the inference phase.

Feature evolution across layers.We experimentally observe that the feature characteristics are significantly different between the encoder and the decoder across all time-steps. For the encoder \(\) the intensity of the change is slight, whereas it is very drastic for the decoder \(\). Specifically we calculate the _Frobenius norm_ for hierarchical features \((z_{t},c,t)_{s}\) across all time-steps, dubbed as \(_{(.)_{s}}=\{_{(z_{T},c,T)_{s}},...,_{(z_{1},c,1)_{s}}\}\). Similarly, we compute \(_{(.)_{8}}\) and \(_{(.)_{s}}\), respectively.

Fig. 2(b) shows the feature evolution across layers with a boxplot 5. Specifically, for \(\{_{(.)_{s}}\}\) and \(\{_{(.)_{8}}\}\), the box is relatively compact, with a narrow range between their first-quartile and third-quartile values. For example, the maximum box height (\(_{}(.)_{32}\)) of these features is less than 5 (see Fig. 2(b) (zoom-in area)). This indicates that the features from both the encoder \(\) and the bottleneck \(\) slightly change. In contrast, the box heights corresponding to \(\{(.)_{s}\}\) are relatively large. For example, for the \((.)_{64}\) the box height is over 150 between the first quartile and third quartile values (see Fig. 2(b)). Furthermore, we also provide a standard deviation (Fig. 2(c)), which exhibits similar phenomena to Fig. 2(b). These results show that the encoder features have relatively small discrepancy and a high degree of similarity across all layers. However, the decoder features evolve drastically.

Could we omit the Encoder at certain time-steps?As indicated by the previous experimental analysis, we observe that, during the denoising process, the decoder features change drastically, whereas the encoder \(\) features change minimally, and have a high degree of similarities at certain adjacent time-steps. Therefore, as shown in Fig. 3(c), We propose to omit the encoder at certain time-steps and use the same encoder features for several decoder steps. This allows us to compute these multiple decoder steps in parallel.

Specifically, we delete the encoder at time-step \(t-1\) (\(t-1<T\)), and the corresponding decoder (including the skip connections) takes as input the hierarchical outputs of the encoder \(\) from the previous time-step \(t\), instead of the ones from the current time-step \(t-1\) like the standard SD sampling (for more detail, see Sec. 3.3).

When omitting the encoder at a certain time-step, we are able to generate similar images (Fig. 3(c)) like standard SD sampling (Fig. 3(a), Tab. 1 (the first and second rows) and additional results in Appendix F). Alternatively, if we use a similar strategy for the decoder (i.e., _decoder propagation_), we find the generated images often fail to cover some specific objects in the text prompt (Fig. 4d). For example, when provided with prompt "A man with a beard wearing glasses and a beanie", the SD model fails to synthesize "glasses" when applying decoder propagation. This is due to the fact that the semantics are mainly contained in the features from the decoder rather than the encoder .

The _encoder propagation_, which uses encoder outputs from previous time-step as the input to the current decoder, could speed up the diffusion model sampling at inference time. In the following Sec. 3.3, we give further elaborate on encoder propagation.

### Encoder propagation

Diffusion sampling, combining iterative denoising with transformers, is time-consuming. Therefore we propose a novel and practical diffusion sampling acceleration method. During the diffusion sampling process _t_= \(\{T,...,1\}\), we refer to the time-steps where encoder propagation is deployed, as _non-key_ time-steps denoted as \(t^{nonkey}\)= \(\{t_{0}^{nonkey},...,t_{N-1}^{nonkey}\}\). The remaining time-steps are dubbed as \(t^{key}\)= \(\{t_{0}^{key},t_{1}^{key},...,t_{T-1-N}^{key}\}\). In other words, we do not use the encoder at time-steps \(t^{nonkey}\), and leverage the hierarchical features of the encoder from the time-step \(t^{key}\). Note we utilize the encoder \(\) at the initial time-step (\(t_{0}^{key}=T\)). Thus, the diffusion inference time-steps could be reformulated as \(\{t^{key},t^{nonkey}\}\), where \(t^{key} t^{nonkey}=\{T,...,1\}\) and \(t^{key} t^{nonkey}=\). In the following, we introduce both _uniform encoder propagation_ and _non-uniform encoder propagation_ strategies.

As shown in Fig. 3a, The encoder feature change is larger in the initial inference phase compared to the later phases throughout the inference process. Therefore, we select more _key_ time-steps in the initial inference phase, and less _key_ time-steps in later phases. We experimentally define the _key_ time-steps as \(k^{key}\)= \([50,49,48,47,45,40,35,25,15]\) for SD model with DDIM, and \(t^{key}\)= \(\{100,\,99,\,98,\,,\,92,\,91,\,90,\,85,\,80,\,,\,25,\,20,\,1 5,\,14,\,13,\,,\,2,\,1\}\)6, \(\{50,49,,2,1\}\) and \(\{75,\,73,\,70,\,66,\,61,\,55,\,48,\,40,\,31,\,21,\,10\}\) for three stage of DeepFloyd-IF (see detail _key_ time-steps) selection in Appendix F.2). The remaining time-steps are categorized as _non-key_ time-steps. We define this strategy as non-uniform encoder propagation (see Fig. 4e). As shown in Fig. 4c, we also explore the _non-key_ time-step selection with fix stride (e.g, 2), dubbed as _uniform encoder propagation_.

Note that our method does not reduce the number of sampling steps. During encoder propagation, the decoder computes for all time-steps, necessitating time embedding inputs for each time-step to maintain temporal coherence (see detail in Appendix D).

Tab. 5 reports the results of the ablation study, considering various combinations of _key_ and _non-key_ time-steps. These results indicate that the set of non-uniform _key_ time-steps performs better in generating images.

**Parallel non-uniform encoder propagation.** When applying the non-uniform encoder propagation strategy, at time-step \(t t^{non-key}\) the decoder inputs do not rely on the encoder outputs at time-step \(t\) (see Fig. 4e). Instead, it relies on the encoder output at the previous nearest _key_ time-step. This allows us to perform _parallel non-uniform encoder propagation_ at these adjacent time-steps in \(t^{non-key}\). We perform decoding in parallel from \(t\) to \(t-k+1\) time-steps. This technique further improves the inference efficiency since the decoder forward in multiple time-steps could be conducted concurrently. We indicate this as _parallel-batch non-key_ time-steps. As shown in Fig. 5 (right), this further reduces evaluation time by \(41\%\) for the SD model.

Prior noise injection.Although the encoder propagation could improve the efficiency in the inference phase, we observe that it leads to a slight loss of texture information in the generated results (see Fig. 6 (left, middle)). Inspired by related works [41; 42], we propose a prior noise

Figure 5: Comparing with SD (left), encoder propagation reduces the sampling time by \(24\%\) (middle). Furthermore, parallel encoder propagation achieves a \(41\%\) reduction in sampling time (right).

injection strategy. It combines the initial latent code \(z_{T}\) into the generative process at subsequent time-step (i.e., \(z_{t}\)), following \(z_{t}=z_{t}+ z_{T},t<\), where \(=0.003\) is the scale parameter to control the impact of \(z_{T}\). And we start to use this injection mechanism from \(=25\) step. This strategic incorporation successfully improves the texture information. Importantly, it demands almost negligible extra computational resources. We observe that the loss of texture information occurs in all frequencies of the frequency domain (see Fig. 6 (right, red, and blue curves)). This approach ensures a close resemblance of generated results in the frequency domain to both SD and \(z_{T}\) injection (see Fig. 6 (right, red and green curves)), with the generated images maintaining the desired fidelity (see Fig. 6 (left, bottom)).

## 4 Experiments

In our experiments, we assess the speed-up of our method compared to others for inference acceleration. We also explore combining our method with these approaches. We do not directly compare our method with distillation methods, which offer superior results but involve computationally expensive retraining.

Datasets and evaluation metrics.We randomly select 10K prompts from the MS-COCO2017 validation dataset  and feed them into the text-to-image diffusion model to obtain 10K generated images. For the transformer architecture diffusion model, we randomly generate 50K images from 1000 ImageNet  class labels. For other tasks, we use the same settings as baselines (e.g., Text2Video-zero , VideoFusion , Dream-booth  and ControlNet ). We use the Frechet Inception Distance (FID)  metric to assess the visual quality of the generated images, and the Clipscore  to measure the consistency between image content and text prompt. Furthermore, we report the average values for both the computational workload (GFLOPs/image) and sampling time (s/image) to represent the resource demands for a single image. See more detailed implementation information on Appendix A.

### Text-to-image Generation

We first evaluate the proposed encoder propagation method for the standard text-to-image generation task on both the latent space (i.e., SD) and pixel space (i.e., DeepFloyd-IF) diffusion models. As shown in Tab. 1, we significantly accelerate the diffusion sampling with negligible performance degradation. Specifically, our proposed method decreases the computational burden (GFLOPs) by a large margin (\(27\%\)) and greatly reduces sampling time to \(41\%\) when compared to standard DDIM sampling in SD. Similarly, in DeepFloyd-IF, the reduction in both computational burden and time reaches \(15\%\) and \(24\%\), respectively. Furthermore, our method can be combined with the latest sampling techniques like DPM-Solver , DPM-Solver++ , and ToMe . Our method enhances sampling efficiency while preserving good model performance, with negligible variations of both FID and Clipscore values (Tab. 1 (the third to eighth rows)). Our method achieves good performance across different sampling steps (Fig. 7 and see Appendix D for quantitative results.). Importantly, these results show that our method is orthogonal and compatible

    &  &  &  \\  & & & **GTLOPs/** & **Unit of** & **DM** \\   & & & **DIM** & 50 & 21.75 0.773 & 37050 & 2.23 & 2.42 \\  & & DDIM & 50 & 21.08 0.783 & **27350\({}_{27\%}\)** & **2.1241.457\({}_{1.421\%}\)** \\   & & DPM-Solver & 20 & 21.36 0.780 & 14821 & 0.90 & 1.14 \\  & & DPM-Solver & 20 & 21.25 0.779 & **11743\({}_{21\%}\)** & **0.46.487\({}_{1.421\%}\)** \\   & & DPM-Solver++ & 20 & 20.51 0.782 & 14821 & 0.90 & 1.13 \\  & & W/Ours & 20 & 70.76 0.781 & **117432\({}_{1\%}\)** & **0.46.487\({}_{1.421\%}\)** \\   & & DDM & 74 & 50 & 22.32 0.782 & 35123 & 2.07 & 2.26 \\  & & W/Ours & 50 & 70.73 0.781 & **260532\({}_{26\%}\)** & **1.15.445\({}_{1.334\%}\)** \\   DDFM \\  } & DDPM & 22523.89 0.783 & 734825 & 33.91 & 34.55 \\  & & DDPM & 22523.73 0.782 & **260523\({}_{15\%}\)** & **261.250\({}_{26\%}\)** & **262.2724\({}_{24\%}\)** \\   & & DPM-Solver++\(1000\)[200.79 0.784] & 370525 & 15.19 & 16.09 \\   & & DPM-Solver++\(1000\)[200.85 0.785] & **13381\({}_{15\%}\)** & **12.02.21\({}_{17\%}\)** & **12.97\({}_{20\%}\)** \\   DDFM \\  } & DPM-Solver++\(1000\)[200.79 0.784] & 370525 & 15.19 & 16.09 \\  & & W/Ours & 1100 & 0.85 0.785 & **13381\({}_{15\%}\)** & **12.02.21\({}_{17\%}\)** & **12.97\({}_{20\%}\)** \\   DDFM \\  } & DPM & 20 & 20 & 20 & 20 & 20 & 20 \\  & DPM & 20 & 20 & 20 & 20 & 20 & 20 \\  & & W/Ours & 50 & 70.73 0.781 & **260532\({}_{15\%}\)** & **1.15.445\({}_{1.334\%}\)** \\   DDFM \\  } & DDPM & 22523.89 0.783 & 734825 & 33.91 & 34.55 \\  & & DPM & 22523.73 0.782 & **260523\({}_{15\%}\)** & **261.250\({}_{26\%}\)** & **26.2724\({}_{24\%}\)** \\   DDFM \\  } & DPM-Solver++\(1000\)[200.79 0.784] & 370525 & 15.19 & 16.09 \\  & & W/Ours & 1100 & 0.85 0.785 & **13381\({}_{15\%}\)** & **12.02.21\({}_{17\%}\)** & **12.97\({}_{20\%}\)** \\   DDFM \\  } & DPM & 20 & 20 & 20 & 20 & 20 & 20 \\  & DPM & 20 & 20 & 20 & 20 & 20 & 20 \\  & & W/Ours & 50 & 70.73 0.781 & **260532\({}_{15\%}\)** & **261.250\({}_{26\%}\)** & **26.2724\({}_{24\%}\)** \\   DDFM \\  } & DPM-Solver++\(1000\)[200.79 0.784] & 370525 & 15.19 & 16.09 \\  & & W/Ours & 1100 & 0.85 0.785 & **13381\({}_{15\%}\)** & **12.02.21\({}_{17\%}\)** & **12.97\({}_{20\%}\)** \\   DDFM \\  } & DPM & 20 & 20 & 20 & 20 & 20 & 20 & 20 \\  & DPM & 20 & 20 & 20 & 20 & 20 & 20 & 20 \\  & & W/Ours & 50 & 70.73 0.781 & **260532\({}_{15\%}\)** & **1.15.445\({}_{1.334\%}\)** \\   DDFM \\  } & DDPM & 22523.89 0.783 & 734825 & 33.91 & 34.55 \\  & & DPM & 22523.73 0.782 & **260523\({}_{15\%}\)** & **261.250\({}_{26\%}\)** & **26.272\({}_{24\%}\)** \\   DDFM

[MISSING_PAGE_FAIL:8]

generation and reduces computational demands. Visually, it maintains the ability to generate images with specific contextual relationships based on reference images. (Fig. 1 (right))

**Reference-guided image generation.** ControlNet  incorporates a trainable encoder, successfully generates a text-guided image, and preserves similar content with conditional information. Our approach can be applied concurrently to two encoders of ControNet. In this paper, we validate the proposed method with two conditional controls: _edge_ and _scribble_. Tab. 4 (the fifth to eighth row) reports quantitative results. We observe that it leads to a significant decrease in both generation time and computational burden. Furthermore, Fig. 1 (middle, bottom) qualitatively shows that our method successfully preserves the given structure information and achieves similar results as ControlNet.

**User study.** We conducted a user study, as depicted in Fig. 8, and asked subjects to select results. We apply pairwise comparisons (forced choice) with 18 users (35 pairs of images or videos/user). The results demonstrate that our method performs equally well as the baseline methods.

### Ablation study

We ablate the results with different selections of both uniform and non-uniform encoder propagation. Tab. 5 reports that the performance of the non-uniform setting outperforms the uniform one in terms of both FID and Clipscore (see Tab. 5 (the third and eighth rows)). Furthermore, we explore different configurations within the non-uniform strategy. The strategy, using the set of _key_ time-steps we established, yields better results in the generation process (Tab. 5 (the eighth row)). We further present qualitative results stemming from the above choices. As shown in Fig. 9, given the same number of _key_ time-steps, the appearance of nine-step non-uniform strategy \(\), \(\) and \(\) settings do not align with the prompt "Fireflies dot the night sky". Although the generated image in the two-step setting exhibits a pleasing visual quality, its sampling efficiency is lower than our chosen setting (see Tab. 5 (the second and eighth rows)).

**Effectiveness of prior noise injection.** We evaluate the effectiveness of injecting initial \(z_{T}\). As reported in Tab. 6, the differences in FID and Clipscores without \(z_{T}\) (the third column), when compared to DDIM and Ours (the second and fourth columns), are approximately \(0.01\%\), which can be considered negligible. While this is not the case for the visual expression of the generated image, it is observed that the output contains complete semantic information with smoothing texture (refer to Fig. 6 (left, the second row)). Injecting the \(z_{T}\) aids in maintaining fidelity in the generated results during encoding propagation (see Fig. 6 (left, the third row) and Fig. 6 (right, red and green curves)).

  
**Sampling Method** & SD (DDIM) & SD (DDIM) + Ours & SD (DDIM) + Ours \\  & w/o \(z_{T}\) injection & w/ \(z_{T}\) injection \\ 
**FID \(\)** & 21.75 & 21.71 & 21.08 \\
**Clipscore \(\)** & 0.773 & 0.779 & 0.783 \\   

Table 6: Quantitative evaluation for prior noise injection.

Figure 8: User study results.

Figure 9: Generating image with uniform and non-uniform encoder propagation. The result of uniform strategy \(\) yields smooth and loses textual compared with SD. Both uniform strategy \(\) and non-uniform strategy \(\), \(\) and \(\) generate images with unnatural saturation levels.

Conclusion

In this work, We explore the characteristics of the encoder and decoder in UNet of the text-to-image diffusion model and find that encoder feature variation is minimal for many time-steps, while the decoder plays a significant role across all time-steps. Building upon this finding, we propose encoder propagation for efficient diffusion sampling, reducing time on both the UNet-based and the transform-based diffusion models on a diverse set of generation tasks. We conduct extensive experiments and validate that our approach can achieve improved sampling efficiency while maintaining image quality.

**Limitations:** Although our approach achieves efficient diffusion sampling, it faces challenges in generating quality when using a limited number of sampling steps (e.g., 5). In addition, even though our proposed parallelization can also be applied to network distillation approaches , we have not explored this direction in this paper and leave it to future research.