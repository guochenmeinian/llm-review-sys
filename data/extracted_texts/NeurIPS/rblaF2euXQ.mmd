# Local Anti-Concentration Class: Logarithmic Regret

for Greedy Linear Contextual Bandit

Seok-Jin Kim

Columbia University

New York, NY, USA

seok-jin.kim@columbia.edu

&Min-hwan Oh

Seoul National Univeristy

Seoul, South Korea

minoh@snu.ac.kr

###### Abstract

We study the performance guarantees of exploration-free greedy algorithms for the linear contextual bandit problem. We introduce a novel condition, named the _Local Anti-Concentration_ (LAC) condition, which enables a greedy bandit algorithm to achieve provable efficiency. We show that the LAC condition is satisfied by a broad class of distributions, including Gaussian, exponential, uniform, Cauchy, and Student's \(t\) distributions, along with other exponential family distributions and their truncated variants. This significantly expands the class of distributions under which greedy algorithms can perform efficiently. Under our proposed LAC condition, we prove that the cumulative expected regret of the greedy algorithm for the linear contextual bandit is bounded by \(( T)\). Our results establish the widest range of distributions known to date that allow a sublinear regret bound for greedy algorithms, further achieving a sharp poly-logarithmic regret.

## 1 Introduction

In the contextual multi-armed bandit problem [2; 6; 24; 25], an agent uses revealed contextual information in each round to decide which arm to pull, receiving a reward corresponding to the pulled arm. The stochastic version of this problem observes rewards as random samples, with their expectation tied to the arm's contextual information. The agent's goal is to design a sequential arm-pulling strategy to maximize cumulative rewards, necessitating a balance between exploration and exploitation. Linear contextual bandits, where expected reward is modeled as a linear function of contextual information, serve as the fundamental framework for contextual bandits [1; 10; 26]. Various exploration strategies, including upper confidence bound (UCB) [1; 11], Thompson sampling (TS) [3; 4], and \(\)-greedy  are widely used and studied in the theoretical analysis for linear contextual bandits. However, exploration can often be challenging in practice, possibly leading to over-exploration and performance deterioration. Some domains may find exploration infeasible or even unethical, and it may appear unfair in applications such as healthcare and clinical domains. Furthermore, exploration strategies tend to add complexity for algorithm designers and decision-making systems.

A greedy policy, i.e., pure exploitation without exploration, selects arms greedily based on current problem parameter estimates. While a greedy policy's effectiveness cannot be guaranteed in general since it may fail to find optimality in the worst case, the possibility of its favorable performances in certain scenarios has been of interest both practically and theoretically. Therefore, understanding when a greedy policy can perform effectively, i.e., when exploration is not needed, is a fundamental research question. Recently, a simple greedy policy has been proved to achieve near-optimal regret bounds for linear contextual bandit problems under some stochastic conditions of contexts [8; 20; 30; 33; 34]. Such efficient learning is possible if the greedy policy can benefit from suitable _diversity_ in the contexts (or the features of arms) -- so that even with exploration-free action selection, parameter estimation iseffectively possible. However, distributions known in the existing literature to allow efficient greedy algorithms are mostly limited only to Gaussian and uniform distributions . Hence, the following research questions arise.

_Is it possible for a wider range of distributions to allow efficient learning for greedy algorithms?_

_If so, how can we characterize such distributions?_

We answer the above questions affirmatively by proposing a general distributional condition that allows for a broad range of distributions to achieve provable efficiency of greedy linear bandit algorithms. In this work, we present a new _Local Anti-Concentration_ (LAC) condition for distributions that encompasses a wider range of context distributions compared to the previous findings. We demonstrate that the class of distributions that satisfy LAC, which we denote as _LAC class_, include Gaussian, exponential, Cauchy, Student's \(t\), and uniform distributions, as well as other exponential family distributions and their truncated variants. This study provides the first evidence that greedy algorithms can perform efficiently beyond Gaussian and uniform distributions. Our findings significantly expand the class of admissible distributions that are suitable for greedy algorithms for linear contextual bandits.

Our proposed LAC condition not only broadens the class of permissible distributions for greedy bandit algorithms but also facilitates a sharper regret guarantee, achieving a poly-logarithmic \(( T)\) regret for greedy algorithms. Our regret analysis constitutes a distinct improvement over the previously known results for greedy linear contextual bandit algorithms. The existing results are primarily categorized into two folds: (i) Gaussian-distributed contexts could only yield \(()\) regret for greedy algorithms for single-parameter linear contextual bandits ; (ii) Context diversity (e.g., Assumption 3 in ) alone was previously regarded as not sufficient to derive a poly-logarithmic regret but additionally assuming a margin condition (e.g., Assumption 2 in ) can achieve \(( T)\) regret.1 In either case, there are limited prior results about context diversity beyond Gaussian and uniform distributions. As for the margin condition, to the best of our knowledge, no prior work in greedy contextual bandits has rigorously derived the scaling of the margin constant, rather than simply treating it as a universal constant. To this end, we establish that Gaussian and uniform distributions as well as all of the common distributions that satisfy the LAC condition (see Table 1) induce \(( T)\) regret _without_ having to additionally assume a margin condition.

The key difference between the analysis of greedy algorithms and that of exploration-based algorithms, such as UCB and TS, for linear contextual bandits lies in the estimation bounds. While UCB  and TS  analyses involve bounding the weighted estimation error of the parameter using self-normalized martingales, the analysis of greedy algorithms relies on the \(_{2}\) estimation bound in all directions. Ensuring this estimation consistency is more challenging, especially when actions are chosen adaptively, resulting in non-i.i.d. data.

In this work, we prove that for a broad class of context distributions, \(\)-consistency of the estimator can be guaranteed, enabling poly-logarithmic regret for greedy algorithms. Our newly proposed class of context distributions represents the largest known class from which \(\)-consistency of the estimator can be derived, even with adaptively chosen (non-i.i.d.) contexts. To establish this consistency, we derive two key technical results. First, we show that the minimum eigenvalue of the Gram matrix increases sufficiently under the LAC condition. Additionally, we demonstrate that under this condition, the suboptimality gap can be bounded probabilistically--a result derived from our analysis rather than assumed explicitly.

### Contributions

The main contributions of our paper are summarized as follows:

* We propose a novel condition, called _Local Anti-Concentration_ (LAC) condition, for a greedy linear contextual bandit algorithm to achieve provable efficiency. The newly proposedLAC condition is satisfied by a wide rage of common distributions, including Gaussian, exponential, Cauchy, Student's \(t\), and uniform distributions, and many common distributions, as well as their truncated variants. This significantly expands the class of admissible distributions that are suitable for greedy algorithms and is, to our best knowledge, by far the largest class of distributions that induces efficient learning for greedy algorithms.
* Under our proposed LAC condition, we prove that the cumulative expected regret for the greedy algorithm is bounded by \(( T)\) (Theorem 1), the sharpest known bound for greedy algorithms in linear contextual bandits with a single parameter.
* By leveraging the proposed condition, we can guarantee both (i) the growth of the minimum eigenvalue of the Gram matrix and (ii) a probabilistically bounded suboptimality gap. These two steps are key technical components for analyzing greedy bandit algorithms and were explicitly assumed in existing literature  to achieve poly-logarithmic regret. Notably, we do not assume these steps; instead, we prove that distributions satisfying the LAC condition inherently induce these two technical results (Theorems 2 and 3), which may be of independent interest.
* Various context distributions have been empirically shown to allow favorable performances of greedy algorithms (see Appendix M). However, the distributions previously known in the literature that enable efficient greedy algorithms were primarily limited to Gaussian and uniform distributions. Our theoretical results offer a significant step toward bridging this gap between theory and practice, providing insights into why greedy algorithms can be effective under a wide range of distributions.

### Related Work

Linear contextual bandits and generalized linear bandits have been widely studied [1; 2; 4; 6; 10; 11; 18; 23; 27; 32]. Upper confidence bound (UCB) algorithms for the linear contextual bandit have been proposed and analyzed for their regret performance [1; 6; 10; 11; 32]. Thompson sampling  algorithms for linear contextual bandits have also been widely studied, with results demonstrating

    &  \\   & Context Distribution & Regret Bound & Problem Setting \\  Kannan et al.  & Gaussian & \(}()\) & Single parameter \\  Sivakumar et al.  & Gaussian & \(}()\) & Single parameter \\  Raghavan et al.  & Gaussian & \(}(T^{})^{}\) & Single parameter \\  Bastani et al. \({}^{}\) & Gaussian & \(( T)\) & Multiple parameters \\   & Gaussian & ( T)\)} & ( T)\)} &  \\  & Uniform & & \\    & Laplace & & \\    & Truncated exponential & & \\    & Truncated Student’s \(t\) & & \\    & Truncated Cauchy & & \\    & PDF \(f(-)\) with & & \\    & polynomially growing \(\) & & \\   

Table 1: Comparisons of Greedy Linear Contextual Bandit Studiestheir effectiveness both theoretically and empirically [3; 4; 9]. While UCB  and Thompson sampling [3; 4] analyses rely on bounding the weighted estimation error of the parameter using self-normalized martingales, the analysis of greedy bandit algorithms depends on the \(_{2}\) estimation bound in all directions. Ensuring this estimation consistency is more challenging, especially in adaptive action settings where data are not i.i.d.

Recent studies [8; 20; 30; 33] have shown that a greedy algorithm can achieve near-optimal regret performance for linear contextual bandit problems under stochastic contexts by providing sufficient conditions under which the greedy algorithm can be efficient. These conditions typically focus on the diversity of the context distribution, ensuring that the greedy policy benefits from sufficient _context diversity_ for effective parameter estimation even without exploratory actions.

However, the existing literature has mainly limited itself to Gaussian [8; 20; 30; 33] and uniform  distributions, leaving open questions about broader applicability. Specifically, it is unclear if other distributions could also support efficient greedy algorithms and what fundamental characteristics these distributions should have to enable consistent parameter estimation without exploration. Our work addresses this gap by identifying broader conditions under which diverse distributions can effectively support greedy algorithms in linear contextual bandits.

## 2 Preliminaries

### Notations

We use \(\|x\|_{p}\) to denote the \(_{p}\)-norm of vector \(x^{d}\). For a positive definite matrix \(A^{d d}\), we define \(\|x\|_{A}=Ax}\). We use \(_{}(A)\) to denote the minimum eigenvalue of the positive definite matrix \(A\). We denote \(_{R}^{d}:=[-R,R]^{d}\) and \(_{R}^{d}:=\{x^{d}:\|x\|_{2} R\}\). If \(d\) is clear, we just write \(_{R}^{d}:=_{R}\) and \(_{R}^{d}:=_{R}\). We define \([n]\) for a set \([n]:=\{1,2,,n\}\). We write \(^{d-1}\) for a \(d\)-dimensional unit sphere. We set \(\|X\|_{_{1}}=_{p 1}\{p^{-1}^{1/p}|X|^{p}\}\),\(\|X\|_{_{2}}=_{p 1}\{p^{-}^{1/p}|X|^{p}\}\) for a random variable \(X\). If \(X\) is a \(d\)-dimensional random vector, then we write \(\|X\|_{_{2}}=_{\|u\|_{2}=1}\| u,X\|_{_{2}}\), \(\|X\|_{_{1}}=_{\|u\|_{2}=1}\| u,X\|_{_{1}}\). We use the notation \(()\) or \(\) to hide constants, and \(}()\) to hide constants and logarithmic terms. We use the notation \(a b\) when \(a b\) and \(b a\). We use \(c,c_{1},c_{2}\) for absolute constant, which _may differ from line by line_.

### Linear Contextual Bandits with Stochastic Contexts

We consider the linear contextual bandit problem with \(K\) arms (\(K 2\)), where in each round \(t=1,2,,T\), the set of context vectors \((t)=\{X_{i}(t)^{d},i[K]\}\) is drawn from some unknown distribution \(P_{}(t)\). Each arm's feature \(X_{i}(t)(t)\) for \(i[K]\) need not be independent of each other and can possibly be correlated. The agent then pulls an arm \(a(t)[K]\). Each context vector \(X_{i}(t)\) for \(i[K]\) is associated with stochastic reward \(Y_{i}(t)\) with mean \(X_{i}(t)^{}^{}\) where \(^{}^{d}\) is a fixed, _unknown_ parameter. For simplicty, we assume \(\|^{}\|_{2} 1\). After pulling arm \(a(t)\), the agent receives a stochastic reward \(Y_{a(t)}(t)\) as a bandit feedback: \(Y_{a(t)}(t)=X_{a(t)}(t)^{}^{}+_{a(t)}(t)\), where \(_{a(t)}(t)\) is a zero mean noise. We assume that there is an increasing sequence of sigma fields \(\{_{t}\}\) such that each \(_{a_{t}}(t)\) is \(_{t}\)-measurable with \([_{a_{t}}(t)|_{t-1}]=0\). In our problem, \(_{t}\) is the sigma field generated by random variables of the arms chosen \(\{a(1),...,a(t)\}\), their context vectors \(\{X_{a(1)}(1),...,X_{a(t)}(t)\}\), and the corresponding rewards \(\{Y_{a(1)}(1),...,Y_{a(t)}(t)\}\). Also, \(_{a(t)}(t)\) is assumed to be conditionally \(\)-sub-Gaussian, i.e., for all \(\), \([e^{_{a(t)}(t)}_{t-1}](^{ 2}^{2}/2)\) for \( 0\). Observing context vector \((t)\), let \(a^{*}(t)\) denote the optimal arm in round \(t\), that is, \(a^{*}(t)=_{i[K]}X_{i}(t)^{}^{}\). Then the _instantaneous_ expected regret (\((t)\)) and _cumulative_ expected regret (\((T)\)) are defined respectively as

\[(T):=_{t=1}^{T}(t):=_{t=1}^{T}[X_ {a^{*}(t)}(t)^{}^{}-X_{a(t)}(t)^{}^{}]\]

which are respectively the instantaneous and cumulative differences between the optimal expected reward and the expected reward of the pulled arms. The expectation is taken with respect to the stochasticity of history, containing randomness of contexts. The goal of the agent is to minimize the cumulative expected regret.

### LinGreedy: Exploration-Free Algorithm for Linear Contextual Bandits

In this work, we focus on identifying sufficient conditions that enable exploration-free greedy algorithms to efficiently learn the optimal policy. Specifically, we analyze a greedy algorithm for linear contextual bandits, which we refer to as LinGreedy (Algorithm 1). The LinGreedy algorithm selects arms greedily based on the OLS estimator, without any exploratory actions.

```  Initialize \((0)=0 I_{d}\), \(b(0)=\), \(_{0}^{d}\). for\(t[T]\)do while\(_{}(t-1)=0\)do  Choose \(a(t)=*{arg\,max}_{i[K]}X_{i}(t)^{}_{0}\) and observe reward \(Y_{a(t)}\).  Update \(b(t)=b(t-1)+X_{a(t)}(t)Y_{a(t)}\) and \((t)=(t-1)+X_{a(t)}(t)X_{a(t)}(t)^{}\). endwhile  Choose \(a(t)=*{arg\,max}_{i[K]}X_{i}(t)^{}_{t-1}\) and observe reward \(Y_{a(t)}\).  Update \(b(t)=b(t-1)+X_{a(t)}(t)Y_{a(t)}\) and \((t)=(t-1)+X_{a(t)}(t)X_{a(t)}(t)^{}\).  Update \(_{t}=(t)^{-1}b(t)\). endfor ```

**Algorithm 1**LinGreedy: Greedy Linear Contextual Bandit

Description of Algorithm 1.The algorithm performs a greedy action in each round based on estimated rewards. In the initial rounds, when the Gram matrix \((t)\) is not yet invertible, parameter estimation is deferred, and the algorithm selects actions based on an initial parameter \(_{0}\). Once the Gram matrix becomes invertible--which can be shown with high probability after sufficient time, the algorithm computes an OLS estimator and performs a greedy action based on the estimated parameter in each subsequent round. This algorithm is exploration-free. In the following sections, we present a novel and more general condition that enables efficient learning for greedy algorithms.

## 3 Local Anti-Concentration Class

In this section, we introduce a new sufficient condition for efficient greedy contextual bandits. This condition is general and encompasses a wide range of common distributions, including Gaussian, exponential, uniform, Cauchy, and Student's \(t\) distributions, as well as their truncated variants. To the best of our knowledge, this is the most extensive class of distributions considered in the greedy contextual bandit literature , which has primarily focused on Gaussian, uniform distributions, and their truncated variants.

Our proposed condition centers on the rate of the log density of stochastic contexts, a concept we term _Local Anti-Concentration_ (LAC). We now formally introduce the novel LAC class.

**Definition 1** (Local Anti-Concentration (LAC)): _A density function \(f_{X}\) of a random variable \(X^{n}\) is said to satisfy the Local Anti-Concentration (LAC) condition with a non-decreasing polynomial \(\) if_

\[\| f_{X}(x)\|_{}(\|x\|_{})\]

_for all \(x^{n}\). We refer to \(\) as the LAC function of \(X\). We denote the class of distributions that satisfy this LAC condition as the Local Anti-Concentration class._

### Intuition of LAC Condition

The LAC condition implies that a density is not overly concentrated at any given point, leading to a gradual decay in density across all directions--hence the term _local_ anti-concentration. A geometric interpretation of the LAC condition and a rigorous definition of this decay rate are provided in Appendix D. Section 3.2 demonstrates that the LAC condition applies to a broad range of common distributions. To the best of our knowledge, very few distributions have been previously shown to support efficient performance guarantees for greedy algorithms. However, we prove that the LAC condition holds for a wide range of distributions, including a variety of exponential families. Note that \(\) can be a constant when contexts have bounded support (see Appendix C). In the following sections, we further explore the characteristics of the LAC condition.

### Generality of LAC Condition

We show that the LAC condition is applicable to various distributions, significantly expanding the class of admissible distributions for greedy linear contextual bandits. The LAC condition is satisfied when the exponential component in the exponential family has a polynomial scale. Included in the LAC class are distributions such as Gaussian, exponential, uniform, Student's \(t\), and Cauchy distributions, along with their truncated variants.

The following proposition demonstrates that the LAC function does not directly depend on the dimension of \(X\). We further discuss in Appendix C that the LAC condition is more closely related to the correlation structure of \(X\) rather than its dimensionality. Therefore, we suggest that the LAC condition provides a suitable framework for comparing regret when both the number of arms and the dimension are large.

**Proposition 1**: _Suppose the random variable \(X=(X_{1},X_{2})\), where \(X_{1}^{n_{1}}\) and \(X_{2}^{n_{2}}\), consists of two independent components. If \(X_{1}\) and \(X_{2}\) satisfy the LAC condition with functions \(_{1}()\) and \(_{2}()\), respectively, then \(X\) satisfies the LAC condition with \((x)=(_{1}(x),_{2}(x))\)._

This holds because, when we take the logarithm of the density, the independent coordinates decompose as the sum of each density. Upon taking the gradient and evaluating the \(_{}\) norm, the expression decomposes perfectly. Using this proposition, the LAC condition remains robust across dimensions if the coordinates are independent, making it dimension-free in such cases. Furthermore, this condition is very accessible because it can be readily computed for a given density function. For many well-known exponential families, the exponential component of the density often scales polynomially.

**Examples of Distributions with LAC Condition**

We present a few examples of known distributions satisfying the LAC condition. We provide rigorous proofs for the examples in Appendix C.

* **Gaussian distribution:** For a Gaussian random variable \(X=(x_{1},,x_{n}) N(,)\), if \(\) is diagonal, it satisfies the LAC condition with \((x)=()}(\|x\|_{}+\|\|_{ })\). For the general (non-diagonal) case of \(\), see Appendix C.
* **Exponential distribution:** The exponential distribution's density \(f_{X}(x)=(- x)\) satisfies the LAC condition with a constant function \((x)=\).
* **Uniform distribution:** The uniform distribution has constant density and satisfies the LAC condition with a constant function \((x)=1\).
* **Student's \(t\)-distribution:** The \(1\)-dimensional Student's \(t\)-distribution has density \(f_{X}(x)=)}{} {}{2}1+}{}^{-(+1)/2}\) and satisfies the LAC condition with \((x)=c_{}\) for some \(\) dependent constant \(c_{}>0\).
* **Laplace distribution:** The Laplace distribution has density \(f(x)=-\) and satisfies the LAC condition with \((x)=c\) for some constant \(c>0\).

If each coordinate's density independently adheres to one of the aforementioned distributions, according to Proposition 1, they all share the same LAC function irrespective of the dimension.

Consider the density \(f(x)\) with \(f(x)(-V(x))\) for some differentiable function \(V(x)\). If \( V(x)\) has polynomial growth, i.e., is bounded by a polynomial, then the density \(f(x)\) meets the LAC condition. This holds because \(f(x)=C(-V(x))\), \( f(x)=(-V(x))=- V(x)\). If \( V(x)\) exhibits polynomial-scale growth, then the supremum norm confirms that the LAC condition is satisfied.

This observation makes the LAC condition easily verifiable for exponential family distributions with density forms \(f_{X}(x|)=h(x)[() T(x)-A()]\), where the exponential part \(T(x)\) has polynomial growth. In many exponential family cases, \(T()\) indeed exhibits polynomial growth. Proofs and further details can be found in Appendix C.

## 4 Statistical Challenges of Greedy Linear Contextual Bandits

In this section, we outline the key statistical challenges in analyzing greedy algorithms for linear contextual bandits: (i) ensuring the diversity of the adapted Gram matrix (Section 4.1) and (ii)bounding the suboptimality gap to achieve logarithmic regret (Section 4.2). For ease of exposition, we use the vectorized context expression \((t)=(X_{1}^{}(t), X_{K}^{}(t))^{dK}\) of \((t)\), which combines context vectors \(X_{i}(t)\) for \(i[K]\). We define \(X_{ij}(t)\) as the \(j\)-th coordinate of the context \(X_{i}(t)\).

### Diversity of Adapted Gram Matrix

The first key challenge lies in ensuring sufficient \(_{2}\)-concentration of the estimator. This requires sufficient eigenvalue growth of the Gram matrix, constructed from the _policy-selected_ contexts. For the OLS estimator used in Algorithm 1, if the minimum eigenvalue of the adapted Gram matrix \(_{}((t))\) increases linearly with the number of rounds \(t\), we can obtain the high probability \(_{2}\) error bound \(\|_{t}-^{*}\|_{2}\) with a convergence rate of \((1/)\) using martingale concentration [13; 29]. In fact, the growth of \(_{}((t))\) is a necessary condition to obtain \(\)-consistency of the estimator.

However, estimating the covariance of the selected contexts \(X_{a(t)}(t)\) is relatively challenging, as its distribution differs significantly from the overall distribution (before selection) of \((t)\). Some studies have investigated the statistical properties of selected contexts [20; 28]; however, the known results are limited to specific distributions, such as arm-independent Gaussian and uniform distributions. Therefore, it remains an open question whether the growth of \(_{}((t))\) can be ensured for a broader class of distributions and what characteristics such distributions would need to satisfy.

### Bounding Suboptimality Gap: Road to Logarithmic Regret

The next challenge that we face particularly in order to achieve logarithmic regret is to bound the suboptimality gap. We first denote the _suboptimality gap_ as the difference between the optimal expected reward and the second highest expected reward:

\[((t)):=X_{a^{*}(t)}(t)^{}^{}-_{i a^{*}( t)}X_{i}(t)^{}^{},\]

which is determined by the true parameter \(^{}\). We aim to bound this suboptimality gap probabilistically, as described precisely in Challenge 2 of Section 4.3.

When this challenge is resolved, along with the growth of the minimum eigenvalue of the adapted Gram matrix discussed in Section 4.1, we can achieve logarithmic expected regret using analysis techniques for linear contextual bandit with stochastic contexts [7; 19]. A high-level description of the role of the margin constant in the regret bound is provided in Appendix D.1, with a rigorous analysis in Appendix J.

### Formal Statements of Two Key Challenges

As mentioned above, we encounter two primary challenges: ensuring the diversity of the chosen contexts (i.e., the growth of the minimum eigenvalue of the Gram matrix of the selected contexts) and bounding the suboptimality gap. Importantly, we do not assume these conditions to hold a priori; rather, we will demonstrate that they are satisfied in the stochastic context under the LAC condition. In this section, we formally define these challenges to be addressed.

Before delving into the formal statements for each of the two challenges, we first define the concept of the diversity constant, which depends on the minimum eigenvalue of the adapted Gram matrix.

**Definition 2** (Diversity Constant): _For a linear contextual bandit with contexts \((t)\) and history \(_{t-1}\), the diversity constant \(_{}(t)\) is defined as the value satisfying_

\[[X_{a(t)}(t)X_{a(t)}(t)^{}_{t-1}]_ {}(t)I_{d},\] (1)

_for all \(t>0\), where \(a(t)\) denotes the arm selected by the algorithm in round \(t\)._

Then, the first challenge is to ensure a positive diversity constant \(_{}(t)>0\), which involves sufficient eigenvalue growth of the adapted Gram matrix. We explore this further in Appendix F.1.

**Challenge 1** (Positive Diversity Constant): _Our goal is to ensure \(_{}(t)>0\)._Achieving a positive diversity constant is challenging, as it requires analyzing the behavior of a context selected by the greedy policy in a specific direction rather than relying on the overall context distribution. In Section 5.3.1, we demonstrate that the minimum eigenvalue of the Gram matrix grows sufficiently, thereby ensuring a positive diversity constant.

We now formally state our second challenge of bounding the suboptimality gap.

**Challenge 2** (**Probabilistic Suboptimality Gap**): _We aim to bound the constant \(C_{}(t)\), which holds under the given history \(_{t-1}\) and for any \(>0\),_

\[[(t)]  C_{}(t)+}\,.\] (2)

_We also refer to this constant \(C_{}(t)\) as the margin constant._

Note that Eq. (2) is a relaxed version of the margin condition presented in [5; 7; 8; 19]. The aforementioned literature explicitly assumes this condition to hold. However, we instead show that the suboptimality gap can be bounded without directly assuming it (Section 5.3.2). Rigorously, \(C_{}(t)\) depends on \(T\), as it is a function of \(T\). However, we emphasize that our algorithm does not require prior knowledge of \(T\); this dependency is needed only for the analysis.

## 5 Regret Analysis

We present the main results of our paper. We prove that the regret of the greedy algorithm (Algorithm 1) for linear contextual bandits can be bounded at a logarithmic scale in the time horizon \(T\), provided that the context distribution satisfies the LAC condition with a polynomial function \(\).

**Assumption 1** (**Independently distributed contexts**): _The context sets \((1),,(T)\) are independently distributed across time._

Discussion of Assumption 1.To the best of our knowledge, all analyses of greedy linear contextual bandits assume the independence and identical distribution (i.i.d.) of context sets [8; 20; 28; 33]. In Assumption 1, we only require context sets to be independent; they may be non-identically distributed. Additionally, much of the literature on linear contextual bandits that investigates \(\)-consistency of estimators also assumes independence of context sets [21; 22]. Note that under Assumption 1, context vectors within the same round are permitted to be dependent.

### Considerations for Context Boundedness

We first provide detailed considerations on context boundedness. In the linear contextual bandit setting, \(_{2}\) boundedness is commonly assumed. However, for light-tailed distributions (such as Gaussian or exponential), the \(_{2}\) norm is unbounded. In such cases, a general approach in the statistical literature is to assume bounded \(_{1}\) or \(_{2}\) norms [14; 17; 37; 38]. Therefore, we divide our analysis into cases of bounded and unbounded contexts.

Bounded Contexts vs. Unbounded Contexts.In linear contextual bandit studies, boundedness of the \(_{2}\) norm of contexts is commonly assumed. In this paper, for bounded contexts, we consider both truncated contexts (e.g., truncated Gaussian, truncated Cauchy distributions) and naturally bounded contexts (e.g., uniform distribution). For unbounded contexts, we assume a bounded \(_{1}\) norm, which is a standard assumption for handling light-tailed distributions (e.g., as in [14; 17], which assume bounded \(_{2}\) norms).

**Assumption 2** (**Boundedness**): _For unbounded contexts, we assume \(\|X_{i}(t)\|_{_{1}} x_{}\). For bounded contexts, we assume \(\|X_{i}(t)\|_{2} x_{}\) for all \(i[K],t[T]\)._

Discussion of Assumption 2The bounded context assumption is widely used in the literature [1; 7; 8; 21; 22; 28]. For unbounded contexts, our assumption of \(_{1}\) boundedness is notably weaker than the sub-Gaussianity (or \(_{2}\)) assumption commonly used in statistical regression literature to handle random design covariates [14; 39]. If \(_{2}\) boundedness holds, it automatically implies boundedness of the \(_{1}\) norm. However, as the analysis differs slightly depending on whether the support is restricted to a bounded ball or is unbounded, we address these cases separately.

### Regret Bound of LinGreedy for LAC Distribution

We first introduce our main result, the regret bound of the greedy algorithm under the LAC condition.

**Theorem 1** (Regret bound of LinGreedy): _Suppose \((t)\) satisfies the LAC condition with the polynomial function \(\) and also satisfies Assumptions 1 and 2 for all \(t\). Then, the cumulative expected regret of LinGreedy (Algorithm 1) is bounded by_

\[(T)( T),\]

_where \(\) concerns only the dependency on \(T\). Considering the dependency on \(d\) and \(K\), for unbounded contexts, we have_

\[(T)}(d^{2.5}).\]

_For bounded contexts, refer to Appendix H for explicit results, as we consider several cases._

**Discussion of Theorem 1.** Theorem 1 states that if the contexts are drawn from a distribution in the LAC class, the regret scales as \(( T)\). While our primary objective is not solely to achieve the sharpest regret bounds, attaining poly-logarithmic regret is highly favorable. Our main goal is to demonstrate that a large class of context distributions satisfies the LAC condition. When they do, a simple greedy algorithm can suffice or even outperform exploration-based algorithms (see numerical experiments in Section 6). The worst-case dependence on \(d\) and \(K\) for bounded contexts is detailed in Appendix H, where dependencies remain at most polynomial in \(d\) and \(K\). As these dependencies vary across distributions, refer to Appendix H for precise information. Proofs for unbounded contexts are provided in Appendix G, with a proof sketch in Appendix D. Proofs for bounded contexts are included in Appendix I.

Theorem 1 is the first result to expand the class of admissible distributions for greedy bandit algorithms beyond Gaussian and uniform distributions. Our result demonstrates, for the first time, that distributions in the LAC class inherently exhibit margin behavior, achieving sharp poly-logarithmic regret without requiring an additional margin assumption. This finding is of independent interest beyond the analysis of greedy bandit algorithms.

### Proof Sketch of Theorem 1

We first present our key results for addressing each of Challenges 1 and 2 stated in Section 4.3.

#### 5.3.1 Ensuring the Positive Diversity Constant

In this section, we present our key result for estimating lower bounds on the diversity constant for densities that satisfy the LAC condition, therefore addressing Challenge 1. Theorem 2 is the analysis under the case of unbounded contexts, where contexts have full support. A similar result is presented in Appendix H for contexts with bounded or truncated support.

**Theorem 2** (Diversity constant for unbounded contexts): _If unbounded contexts \((t)\) has the LAC condition with \((x):=A_{1}+A_{2}x^{}\) and satisfies Assumption 2 for all \(t\),_

\[_{}(t) c_{1}+A_{2}(R_{1}+2)^{ })^{2}}\]

_holds for \(R_{1}:=c_{2}x_{}( d+ K+2)\). Here, \(c_{1},c_{2}\) are absolute constants._

**Discussion of Theorem 2.** Theorem 2 implies that \(_{}(t)()\), hence ensuring the growth of the minimum eigenvalue of the adapted Gram matrix. In Appendix D.1, we discuss how \((t)}\) factors into the regret bounds. Note that \(\) is generally small in many distributions. For example, for Gaussian distributions, \(=1\), and for exponential distributions, \(=0\).

#### 5.3.2 Bounding Suboptimality Gap

Next, we present our result addressing Challenge 2 by computing the suboptimality gap constant \(C_{}(t)\), which satisfies the inequality in Eq.(2) for every \(>0\). By combining this condition with the estimates for \(_{}(t)\), we can obtain an \((( T))\) regret bound in terms of \(T\) by applying the analysis techniques of linear contextual bandits with stochastic contexts  to our setting.

**Theorem 3** (Suboptimality gap for unbounded contexts): _In the same setup as Theorem 2,_

\[C_{}(t) c_{3}(A_{1}+A_{2}3^{}R_{2}^{})\|_{2}}\]

_holds for \(R_{2}=c_{4}x_{}(1+ K+ d+ T)+1\) and absolute constants \(c_{3},c_{4}>0\)._

**Discussion of Theorem 3.** Note that the suboptimality gap constant \(C_{}(t)\) is multiplied linearly in regret bounds [5; 7; 19]. For bounded contexts, we present a similar result in the Appendix H.

#### 5.3.3 Proof Intuitions

We provide a high-level proof overview of Theorem 1 in Appendix D. Note that once Challenges 1 and 2 are satisfied, achieving logarithmic regret becomes straightforward, as detailed in Appendix J.

The remaining task is to address these two challenges, with a particular focus on bounding the constants \(_{}(t)\) and \(C_{}(t)\). A key implication of the LAC condition is that the density decays slowly at every point. Another useful property is that the LAC condition is preserved under conditioning, meaning that \((t)\{(t) A\}\) also satisfies the LAC condition with the same function for any set \(A^{K d}\). This can be verified using the fact that \( f_{(t)\{(t) A\}}(x)= f_{(t)}(x) -[A]\), where \([A]\) is a constant (see Appendix C.3 for details).

The main challenge of analyzing the statistical concentration in greedy linear contextual bandits lies in the fact that the distribution of selected contexts \(X_{a(t)}(t)\) differs significantly from the distribution of the overall (pre-selected) contexts \((t)\). The preservation of LAC under conditioning ensures that LAC still holds when conditioning on the event of selecting arm \(i\), enabling our analysis.

The full proofs for unbounded contexts are provided in Appendix G. For results on bounded contexts, see Appendix H (and their proofs in Appendix I).

### \(\)-Consistency of Estimator

In addition to achieving logarithmic regret, an independently valuable result is obtained: the \(_{2}\)-consistency of the estimator \(_{t}\). This is a property that even typical sublinear-regret algorithms, such as UCB and TS, do not generally guarantee. Under the same setup as Theorem 1, we achieve \(\|_{t}-^{}\|_{2}}( {d}{})\) with high probability (see Corollaries 6 and 7). This additional result may also facilitate analysis of sample complexity, such as PAC bounds.

## 6 Experiments

To validate our theoretical findings numerically, we conducted experiments using various context distributions: Gaussian, Laplace, uniform, and truncated Cauchy distributions. We compared the performance of LinGreedy with the LinUCB and LinTS algorithms. The results showed that LinGreedy exhibited significantly superior regret performance compared to the other exploration-based algorithms, achieving a logarithmic scale of regret. Detailed experimental results are provided in Appendix M.

Figure 1: The cumulative regret plots of the numerical experiments. The full results are available in Appendix M.