# Prompt Optimization with EASE? Efficient Ordering-aware Automated Selection of Exemplars

Zhaoxuan Wu*\({}^{@sectionsign}\)\({}^{@sectionsign}\)\({}^{@paragraphsign}\), Xiaoqiang Lin*\({}^{}\), Zhongxiang Dai\({}^{}\), Wenyang Hu\({}^{@sectionsign}\),

**Yao Shu\({}^{}\), See-Kiong Ng\({}^{@sectionsign}\), Patrick Jaillet\({}^{}\), Bryan Kian Hsiang Low\({}^{}\)**

Institute of Data Science, National University of Singapore, Republic of Singapore\({}^{@sectionsign}\)

Singapore-MIT Alliance for Research and Technology, Republic of Singapore\({}^{@paragraphsign}\)

Dept. of Computer Science, National University of Singapore, Republic of Singapore\({}^{}\)

School of Data Science, The Chinese University of Hong Kong, Shenzhen\({}^{}\)

LIDS and EECS, Massachusetts Institute of Technology, USA\({}^{}\)

Guangdong Lab of AI and Digital Economy (SZ)\({}^{}\)

{wu.zhaoxuan, xiaoqiang.lin, wenyang.hu}@u.nus.edu\({}^{@sectionsign}\)

daizhongxiang@cuhk.edu.cn\({}^{}\), seekiong@nus.edu.sg\({}^{@sectionsign}\)

lowkh@comp.nus.edu.sg\({}^{}\), jaillet@mit.edu\({}^{}\), shuyao@gml.ac.cn\({}^{}\)

###### Abstract

Large language models (LLMs) have shown impressive capabilities in real-world applications. The capability of _in-context learning_ (ICL) allows us to adapt an LLM to downstream tasks by including input-label exemplars in the prompt without model fine-tuning. However, the quality of these exemplars in the prompt greatly impacts performance, highlighting the need for an effective automated exemplar selection method. Recent studies have explored retrieval-based approaches to select exemplars tailored to individual test queries, which can be undesirable due to extra test-time computation and an increased risk of data exposure. Moreover, existing methods fail to adequately account for the impact of exemplar ordering on the performance. On the other hand, the impact of the _instruction_, another essential component in the prompt given to the LLM, is often overlooked in existing exemplar selection methods. To address these challenges, we propose a novel method named EASE, which leverages the hidden embedding from a pre-trained language model to represent ordered sets of exemplars and uses a neural bandit algorithm to optimize the sets of exemplars _while accounting for exemplar ordering_. Our EASE can efficiently find an ordered set of exemplars that _performs well for all test queries_ from a given task, thereby eliminating test-time computation. Importantly, EASE can be readily extended to _jointly optimize both the exemplars and the instruction_. Through extensive empirical evaluations (including novel tasks), we demonstrate the superiority of EASE over existing methods, and reveal practical insights about the impact of exemplar selection on ICL performance, which may be of independent interest. Our code is available at https://github.com/ZhaoxuanWu/EASE-Prompt-Optimization.

+
Footnote â€ : * Equal contribution. Correspondence to: Zhongxiang Dai <daizhongxiang@cuhk.edu.cn>.

## 1 Introduction

Large language models (LLMs) have recently drawn significant attention and have been widely deployed in various real-world scenarios  due to their strong capabilities. Of note, a particularly impressive capability of LLMs is _in-context learning_ (ICL): LLMs can learn from ahandful of input-label demonstrations (i.e., _data exemplars_) included in its prompt to perform a downstream task . ICL allows us to adapt an LLM to a downstream task without fine-tuning the model parameters . However, the ICL performance is heavily dependent on the data exemplars in the prompt . Therefore, to maximize the ICL performance, it is of paramount importance to carefully select the set of exemplars . Unfortunately, exemplar selection for ICL is challenging because the mechanism of ICL is complicated and unknown, and this difficulty is further aggravated for black-box LLMs to which we only have API access (e.g., GPT-4 , Gemini Ultra ).

A large number of existing works on exemplar selection for ICL have considered _retrieval-based_ methods . Specifically, they aim to develop a retriever model such that for every test query, the retriever model retrieves the corresponding best set of exemplars tailored to this particular query . Such approaches require test-time retrieval for every query which might hinder their ease of adoption  while test-time computation is not needed for our method. Moreover, another drawback of these retrieval-based methods is that they may lead to increased privacy risks. Compared to using a fixed set of exemplars for all test queries submitted to the LLM provider (e.g., via an API), using a different set of exemplars for every test query may lead to greater data exposure, i.e., more data exemplars being exposed to the LLM provider. Privacy issues have become increasingly prominent in discussions surrounding LLMs , with evidence of user data being output by these models . Therefore, retrieval-based methods are undesirable in scenarios where such privacy concerns are important considerations. Given these two important shortcomings of retrieval-based methods, it is imperative to develop exemplar selection methods that generalize to test queries and are capable of choosing a fixed set of exemplars that perform well for all test queries for a task.

Another major limitation of existing exemplar selection methods (including retrieval-based and other methods) is their inability to account for the impact of the _ordering of the exemplars_ within a subset . Specifically, to select a subset of \(k\) exemplars, previous works have either relied on simple heuristics (e.g., selecting the top-\(k\) exemplars based on the score from a retriever ) or used subset selection techniques which are able to account for the inter-relationships among the exemplars within a subset . Due to the computational complexity caused by the combinatorial search space, these works based on subset selection have often employed an iterative greedy strategy to sequentially select the subset of exemplars. It is also non-trivial to extend these existing works to consider exemplar ordering. However, it has been repeatedly verified that the ordering of the exemplars has a significant impact on the performance of ICL . To the best of our knowledge, the impact of the exemplar ordering during the selection process remains inadequately addressed by these previous works that relied on simple heuristics (e.g., random ordering) and approximations (e.g., greedy strategy).

In addition, when optimizing the set of exemplars to improve the performance of the LLM, existing methods have often ignored the impact of the _instruction_, which is another essential component in the prompt given to the LLM . Specifically, previous works on exemplar selection often use a fixed pre-determined instruction or do not include any instruction at all in the prompt . Meanwhile, existing works on instruction optimization for LLMs typically include a fixed manually selected set of exemplars in the prompt  or simply adopt the zero-shot setting (i.e., without using any exemplar) . However, these two lines of work have separately demonstrated that both the exemplars and the instruction have significant impacts on the performance of the LLM. Therefore, the existing works that optimize these two components separately are unable to account for their interactions, which may be crucial for further boosting the performance of LLMs. This leaves considerable untapped potential in enhancing the performance of the LLM through ICL, which can be achieved by _jointly optimizing the instruction and the exemplars_.

In this work, we propose a novel exemplar selection algorithm that addresses the above-mentioned challenges faced by existing works (with detailed discussions of related work in App. 5). We formulate exemplar selection as a black-box optimization problem, in which every input in the domain corresponds to a sequence of \(k\) exemplars and its corresponding output represents the ICL performance achieved by including this sequence in the prompt for the LLM. Given this formulation, for every sequence of \(k\) exemplars in the domain, we adopt the embedding from a powerful pre-trained language model as its continuous representation, and train a neural network (NN) to predict its ICL performance. Based on the trained NN, we use a neural bandit algorithm to find the optimal exemplar sequence in a query-efficient manner. Our Efficient ordering-aware Automated Selection of Exemplars (EASE) algorithm offers several significant benefits:* Our EASE can _find an efficacious sequence of \(k\) exemplars_ (which performs well for all test queries from a task) in a _query-efficient manner_ (i.e., it only needs to test a small number of exemplar sequences). This can mainly be attributed to our algorithmic design, which allows us to use neural bandits to balance the _exploration_ of the space of exemplar sequences and the _exploitation_ of the predicted performance from the NN in a principled way. Importantly, in contrast to retrieval-based methods, our EASE _requires no test-time computation_ to select test query-specific exemplars.
* Our EASE naturally _takes into account the ordering of the exemplars_ when maximizing the ICL performance. This is because, for the same subset of exemplars, a different ordering leads to a different pre-trained embedding, which allows the trained NN (that takes the embedding as input) to predict the performances of different orderings of these exemplars. We have made our EASE computationally feasible for large search spaces of exemplar sequences using a technique based on optimal transport (OT), which reduces the computational cost of EASE while preserving its strong performance by imposing an implicit preference towards exemplars that are more relevant to the task (Sec. 3.2).
* Our EASE is readily extended to _jointly optimize the exemplars and instruction_ in the prompt. This is achieved by augmenting the domain of exemplar sequences with the instruction (Sec. 3.3).

These advantages of our EASE algorithm allow it to significantly boost the performance of ICL. To empirically validate this, we compare our EASE algorithm with a comprehensive suite of baselines in a variety of tasks. To begin with, we show that our EASE consistently outperforms previous baselines in large number of benchmark tasks (Sec. 4.1). Next, by using our EASE algorithm in a novel experiment, we unveil an interesting insight about ICL: The selection of exemplars is more important when the LLM has less knowledge about the task (Sec. 4.2). Based on this insight, we design a set of novel experiments in which the selection of exemplars has an important impact on the ICL performance, and use these experiments to further demonstrate the superiority of our EASE (Sec. 4.3). Furthermore, we showcase the ability of our EASE to jointly optimize the exemplars and the instruction to enhance the performance of the LLM even further (Sec. 4.4). We also included a retrieval-based extension of EASE to deal with large exemplar set sizes (Sec. 4.5). Of note, our novel experimental designs, as well as the insights from them, _may be of independent interest for future works on ICL and beyond_.

## 2 Problem setting

We are given a set of data exemplars \(D=\{e_{i}=(x_{i},y_{i})\}_{i=1}^{n}\) of size \(n\), where \(x_{i}\) and \(y_{i}\) correspond to the input and output text, respectively. The data exemplars describe a downstream task, and we aim to select \(k\) of them to form the optimal in-context exemplars sequence \(E\) for accurate output generation when prompting a black-box LLM \(f()\). Due to the sequential nature of the natural language input, the exemplar sequence is ordered such that \(E=(e_{1},e_{2},,e_{k})\) where \(e_{i}\) denotes the \(i\)-th exemplar chosen. For every input \(x\), we prepend it with a sequence of exemplars \(E\) to generate a response \(\) from the black-box LLM (e.g., through calling the API), following

\[=f([,e_{2},,e_{k}}_{},x])=f([E,x])\;.\]

Here, the number of exemplar \(k\) in the context can be determined by the future budget of inference in practice. A larger \(k\) corresponds to a longer sequence of context tokens to be prepended to the test input \(x\) during inference, which leads to higher query costs (e.g., associated with the API calls). Alternatively, it is common to decide \(k\) depending on the context window size of the target LLM \(f()\).

Since the model architecture and the internal working of the black-box LLM \(f()\) is unattainable, we formulate the exemplar selection as a black-box optimization problem over the space of permutations \(\{E:|E|=k\}\) of size \(n^{k}\) (i.e., having \(n\) choices for each of the \(k\) positions in the sequence),

\[_{E}F(E)_{(x,y) D_{V}}[s(f(E,x),y)]\] (1)

where \(s(,)\) is a score function for the output against the ground truth, \(D_{V}\) is the held-out validation set and \(|E|=k\) denotes that the number of exemplars in \(E\) is \(k\). Therefore, the exemplar sequence \(E\) found represents a fixed ordered set of exemplars that apply to every data point in the validation set. Note that our formulation considers exemplars jointly as _ordered_ text in the sequence \(E\). This space of permutation \(\) is also a lot larger than the (unordered) combinatorial search space considered in the subset selection formulation of exemplar selections in [4; 14; 19; 48]. We show the superior performance of our method against subset selection methods in our experiments (Sec. 4).

Automated selection of exemplars

### NeuralUCB for query-efficient optimization of exemplar sequences

We propose to use neural bandits to iteratively maximize the black-box objective function (1). At each iteration \(t\), the neural bandits algorithm selects the next input query based on the belief of the objective given all past \(t-1\) observations \(O_{t-1}\{(E_{i},s_{V}(E_{i}))\}_{i=1}^{t-1}\) where \(E_{i}\) and \(s_{V}(E_{i})\) are the exemplar sequence and corresponding validation score at iteration \(i\), respectively. Here, the validation score is a realization of the objective function (1), so \(s_{V}(E)=1/|D_{V}|_{(x,y) D_{V}}s(f(E,x),y)\).

Inspired by Lin et al.  who have shown impressive performances of the neural upper confidence bound (NeuralUCB) acquisition function  on instruction optimization, we adopt the NeuralUCB approach to our novel black-box exemplar selection formulation in (1). We propose to use a neural network (NN) to directly learn the mapping from a general-purpose hidden embedding of input exemplar sequences to the validation score. Specifically, we propose to use an embedding model \(h()\) and train a network \(m(h(E);_{t})\) at iteration \(t\) such that

\[_{t}=_{}\!_{(E,s_{V}(E)) O_{t-1}}(m (h(E);),s_{V}(E))\] (2)

where \(\) is the mean squared error (MSE) loss function. The embedding model can be pre-trained on a large corpus of text and hence provides a powerful latent representation of the input sentence. For example, pre-trained sentence-transformer models like MPNet  and black-box APIs like OpenAI text embedding are both commonly used for downstream clustering, semantic search and classification. Importantly, even for the same subset of exemplars, a different ordering leads to different embedding, hence \(h(E)\) captures both the content and the ordering of the exemplars in \(E\).

Then, the trained NN can be used to iteratively select the next exemplar sequence \(E_{t}\) to query:

\[E_{t} =_{E}_{t}(E),\] (3) \[_{t}(E)  m(h(E);_{t})+_{t}_{t-1}(h(E);_{t}),\]

where \(m(h(E);_{t})\) is the predicted score, \(_{t-1}(h(E);_{t})\) is the NN's uncertainty about the score of \(h(E)\) and \(_{t}\) is a hyperparameter that balances the two terms. Using NeuralUCB, our EASE balances the _exploration_ of the space of exemplar sequences \(E\) and the _exploitation_ of the predicted score from the NN in a principled way.

This application of NeuralUCB in our work differs from that of Lin et al.  in three main aspects. Firstly, the input to our NN is the embedding of _exemplar sequences_ rather than the latent representation of the _instructions_ from . Secondly, we relax the requirement of a separate white-box LLM from  and only need black-box access to the embedding model \(h()\). Thirdly, our application to exemplar selection dramatically increases the search space from finite candidate instructions  to the space of exemplar sequences. Particularly, the explosion of the size of the search space poses significant difficulties to optimization, which we will address in the next section.

### Reducing computational cost through optimal transport (OT)

Directly applying NeuralUCB to exemplar selection is challenging due to the enormous search space of all permutations of exemplars on which the acquisition values (3) have to be evaluated. For example, selecting 5 exemplars out of 100 requires \(100^{5}\) evaluations which is far from being practical. It is natural to search over a _domain space_\(Q_{t}=\{E^{(j,t)}\}_{j=1}^{j}\) with a smaller number \(q\) of exemplar sequences in each iteration \(t\). However, uniformly sampling such a reduced space \(Q_{t}\) from \(\) could be sub-optimal, because a small \(q\), which is required to make our algorithm computationally feasible, is highly likely to discard important exemplar sequences (we verify this in ablation experiments in Sec. 4.5 and App. C.3). To this end, we make a large \(q\) feasible by introducing a novel technique based on optimal transport (OT) to further select from \(Q_{t}\) a subset \(Q^{}_{t}\) of \(q^{}<q\)_relevant_ exemplar sequences. This technique can simultaneously (a) reduce the computational cost (since \(q^{}\)) and (b) preserve our performance (since we can now search over a large domain space \(Q_{t}\)) by imposing an implicit preference towards exemplars in \(Q_{t}\) that are more relevant to the task.

Given probability measures \(_{s}\) and \(_{v}\) over space \(\), the OT distance between \(_{s}\) and \(_{v}\) is defined as

\[OT(_{s},_{v})=_{(_{s},_{v})}_{^{2}}c(z, z^{})d(z,z^{})\] (4)where \((_{s},_{v})=\{()|_{ }(z,z^{})dz=_{s},_{}(z,z^{})dz^{ }=_{v}\}\) is a collection of couplings between two the distributions \(_{s}\) and \(_{v}\), and \(c:^{+}\) is some symmetric positive cost function. In our problem, we propose to use the space of embedding from \(h()\) as \(\) because the embedding captures the semantics of the exemplars with a fixed-dimensional vector. As cosine similarity is usually used to train embedding models such as MPNet  and sentence-BERT , we propose to use the following cost function \(c(h(e),h(e^{}))=1-sim_{cos}(h(e),h(e^{}))\) where \(sim_{cos}(h(e),h(e^{}))\) measures the cosine similarity between the embedding of two exemplars \(e\) and \(e^{}\). Given a sampled subset \(S=\{e_{i}\}_{i=1}^{k}\), we define a discrete measure \(_{s}=_{i=1}^{k}(h(e_{i}))\) where \(\) is the Dirac function. Likewise, we define \(_{v}\) for the validation set \(D_{V}\).

Intuitively, a smaller value of (4) indicates that the subset of exemplars is more similar to the validation set and is hence more _relevant_ to the task. This allows us to select the relevant exemplar sequences from \(Q_{t}\) to form the smaller subset \(Q_{t}^{}\), on which the acquisition values (3) are evaluated. Consequently, we can increase the size \(q\) of \(Q_{t}\) by hundreds of folds while being computationally feasible since the most expensive computation (i.e., computing embeddings for all exemplar sequences in \(Q_{t}\)) is not needed. Note that the extra computation incurred by OT is minimal since the embedding of every data exemplar in \(D\) and \(D_{V}\) can be pre-computed and reused. Therefore, OT helps us examine a large number of permutations among the space of all permutations without significantly increasing the computation, which helps mitigate the problem of the exploding search space.

### Natural extension to jointly optimize instructions and exemplars

Our algorithm can further boost the performance of ICL for LLMs by jointly optimizing the exemplars and the instruction. A natural extension of our formulation in Sec. 2 allows the instruction, being another essential component of the LLM prompt, to be simultaneously optimized with exemplars. This ensures the optimal matching between instructions and exemplars to achieve a fully automated pipeline with superior performance. Specifically, our formulation naturally extends to \(E=(p,e_{1},e_{2},,e_{k})\) where \(p P\) is an instruction from a candidate space/set \(P\), i.e., \(Q_{t}^{} P Q_{t}^{}\). Subsequently, \(p\) can be intuitively treated as another type of "exemplar" from a new space \(P\) and optimized in conjunction with the exemplars. In practice, the fixed set \(P\) of candidate instructions can be generated by the black-box model through techniques such as APE , PromptBreeder , etc. This extension is not only simple in its implementation but also proven to be effective in our experiments in Sec. 4.4.

### Our Ease algorithm

In iteration \(t\) of EASE, we first use the historical observations of the exemplar sequence and score pairs \(\{(E_{i},s_{V}(E_{i}))\}_{i=1}^{t-1}\) to train the score prediction NN. Then, we perform sampling to obtain the domain space \(Q_{t}\), which will be further refined to a set \(Q_{t}^{}\) of top-\(q^{}\) candidates based on OT distances. The space of exemplar sequence \(E\) can be optionally augmented with instructions \(p P\) from a set \(P\) of instruction candidates. Subsequently, we find the optimal \(E\) that maximizes the NeuralUCB acquisition function. The selected exemplar sequence \(E\) is then evaluated against the black-box LLM \(f()\) using the validation dataset \(D_{V}\), obtaining a new observed pair of \((E,s_{V}(E))\). This process is repeated until the query budget \(T\) is exhausted. An overview of the algorithm is presented in Alg. 1.

```
0: Data exemplars set \(D\), validation set \(D_{V}\), length of exemplars \(k\), total budget \(T\), number of initial rounds \(T_{}\), sampling size \(q^{}\), black-box target model \(f()\), embedding model \(h()\), neural network \(m(;)\), (optional) instruction set \(P\).
1 Initialize \(\{(E_{i},s_{V}(E_{i}))\}_{i=1}^{T_{}}\) with \(T_{}\) randomly sampled \(\{E_{i}\}_{i=1}^{T_{}}\) from \(D\);
2for\(t=T_{}\) to \(T\)do
3 Following (2), use observations \(\{(E_{i},s_{V}(E_{i}))\}_{i=1}^{t-1}\) to train the NN \(m(;_{t})\) parameter \(_{t}\);
4 Sample sequences \(Q_{t}\) and select top-\(q^{}\) sequences \(Q_{t}^{}\) with smallest OT distances;
5 (Optional) Augment each \(E Q_{t}^{}\) with instructions \(p P\), i.e., \(Q_{t}^{} P Q_{t}^{}\);
6 Following (3), select the next query \(E_{t}=_{E Q_{t}^{}}_{t}(E)\);
7 Evaluate \(E_{t}\) on the black-box model to obtain the validation score \(s_{V}(E_{t})\);
8return\(E^{*}=_{E_{t}:t\{1,,T\}}s_{V}(E_{t})\) ```

**Algorithm 1**Ease

```
0: Data exemplars set \(D\), validation set \(D_{V}\), length of exemplars \(k\), total budget \(T\), number of initial rounds \(T_{}\), sampling size \(q^{}\), black-box target model \(f()\), embedding model \(h()\), neural network \(m(;)\), (optional) instruction set \(P\).
1 Initialize \(\{(E_{i},s_{V}(E_{i}))\}_{i=1}^{T_{}}\) with \(T_{}\) randomly sampled \(\{E_{i}\}_{i=1}^{T_{}}\) from \(D\);
2for\(t=T_{}\) to \(T\)do
3 Following (2), use observations \(\{(E_{i},s_{V}(E_{i}))\}_{i=1}^{t-1}\) to train the NN \(m(;_{t})\) parameter \(_{t}\);
4 Sample sequences \(Q_{t}\) and select top-\(q^{}\) sequences \(Q_{t}^{}\) with smallest OT distances;
5 (Optional) Augment each \(E Q_{t}^{}\) with instructions \(p P\), i.e., \(Q_{t}^{} P Q_{t}^{}\);
6 Following (3), select the next query \(E_{t}=_{E Q_{t}^{}}_{t}(E)\);
7 Evaluate \(E_{t}\) on the black-box model to obtain the validation score \(s_{V}(E_{t})\);
8return\(E^{*}=_{E_{t}:t\{1,,T\}}s_{V}(E_{t})\) ```

**Algorithm 2**Ease

```
0: Data exemplars set \(D\), validation set \(D_{V}\), length of exemplars \(k\), total budget \(T\), number of initial rounds \(T_{}\), sampling size \(q^{}\), black-box target model \(f()\), embedding model \(h()\), neural network \(m(;)\), (optional) instruction set \(P\).
1 Initialize \(\{(E_{i},s_{V}(E_{i}))\}_{i=1}^{T_{}}\) with \(T_{}\) randomly sampled \(\{E_{i}\}_{i=1}^{T_{}}\) from \(D\);
2for\(t=T_{}\) to \(T\)do
3 Following (2), use observations \(\{(E_{i},s_{V}(E_{i}))\}_{i=1}^{t-1}\) to train the NN \(m(;_{t})\) parameter \(_{t}\);
4 Sample sequences \(Q_{t}\) and select top-\(q^{}\) sequences \(Q_{t}^{}\) with smallest OT distances;
5 (Optional) Augment each \(E Q_{t}^{}\) with instructions \(p P\), i.e., \(Q_{t}^{} P Q_{t}^{}\);
6 Following (3), select the next query \(E_{t}=_{E Q_{t}^{}}_{t}(E)\);
7 Evaluate \(E_{t}\) on the black-box model to obtain the validation score \(s_{V}(E_{t})\);
8return\(E^{*}=_{E_{t}:t\{1,,T\}}s_{V}(E_{t})\) ```

**Algorithm 3**Ease

```
0: Data exemplars set \(D\), validation set \(D_{V}\), length of exemplars \(k\), total budget \(T\), number of initial rounds \(T_{}\), sampling size \(q^{}\), black-box target model \(f()\), embedding model \(h()\), neural network \(m(;)\), (optional) instruction set \(P\).
1 Initialize \(\{(E_{i},s_{V}(E_{i}))\}_{i=1}^{T_{}}\) with \(T_{}\) randomly sampled \(\{E_{i}\}_{i=1}^{T_{}}\) from \(D\);
2for\(t=T_{}\) to \(T\)do
4 Following (2), use observations \

[MISSING_PAGE_FAIL:6]

suggested by Brown et al. , it is highly likely that GPT-3 has been trained on common benchmark datasets potentially contained in Common Crawl due to data contamination. Hence, further providing high-quality in-context exemplars could hardly improve the performance of the model on these well-trained tasks. Another possibility could be that the exemplars mostly serve as the context for adapting to the new formatting rules of the specific task, i.e., the LLM is not utilizing the semantic contents of the exemplars to learn the underlying input-output relations. This aligns with the discoveries of Min et al.  and provides a potential explanation for the surprising behavior of LLMs. Therefore, the II dataset might not be the most suitable one to test EASE. Next, we verify the above hypothesis that the effect of exemplar selection diminishes as the model gains more knowledge about the task in Sec. 4.2, and propose more suitable families of datasets for exemplar selection in Sec. 4.3.

### Empirical validation of the hypothesis through progressive finetuning

We hypothesize that _the effect of exemplar selection diminishes as the model gains more knowledge about the task_. To gain insights on this hypothesis, we study an open-source white-box Vicuna model instead of the black-box GPT-3.5 model that is only accessible through API. We progressively finetune the while-box language model, and examine whether the extent of finetuning on a specific task diminishes the importance of in-context exemplars when prompted.

Our results are in Fig. 1. Compared to the most competitive baseline, Best-of-N, _the gain from exemplar selection using our EASE diminishes as the model is finetuned on the dataset of the respective tasks for more epochs_. Across the three tasks shown in Fig. 1, using EASE originally has a performance gain of about 3%-10% and this gain slowly diminishes to 0 as finetuning progresses. This verifies our hypothesis and calls for further investigation of the exemplar selection performance of our EASE on tasks that have not been seen by the model, which we conduct in Sec. 4.3.

### New families of "out-of-distribution" tasks that emphasize in-context reasoning

We propose three new families of "out-of-distribution" tasks (i.e., loosely referred to as tasks on which the LLM is not already well trained) that highlight the importance of high-quality exemplars, which could also be of independent interest. The new tasks require the LLM to learn the underlying function/relationship in the input prompt in order to perform reasoning during inference, and are hence more sensitive to the quality of the exemplars.

**Rule-based tasks.** Given our insight on the impact of the model's existing knowledge about the task (Sec. 4.2), we propose rule-based tasks that contain novel rules that the LLM has not learned before. A key characteristic of these tasks is that the model has to extract the underlying relationships among the provided in-context exemplars and directly use the relationship for test-time inferences. For example, we construct the linear regression (**LR**) task where the input takes the form demonstrated in Example 2 (see App. B.1). The underlying relationship in this example is \(y=ax+b\), with \(a=-4\) and \(b=6\) in this specific Example 2. Without further instructions, the model is supposed to rely on the provided in-context exemplars to implicitly infer the regression task, recover the coefficients \(a,b\) for linear regression, and then directly apply it to the test sample (e.g., for test input \(117\), compute \(-4 117+6=-462\) and output \(-462\)). The results are in Tab. 2. For the clean dataset (i.e., 0% noise), EASE outperforms the most competitive baseline by 8.3% in absolute accuracy. Note that EASE has greater advantages in settings with noisy data, which will be discussed later in this section.

Figure 1: From left to right, the tasks are taxonomy animal, sentence similarity and object counting. The performance gaps between EASE and the Best-of-N baseline diminish as the LLM is finetuned.

Another example of our proposed rule-based tasks is constructed by changing the rules for language puzzles (**LP**). A prominent example is "pig Latin" which follows two simple rules: (a) For words that begin with a vowel, one just adds "yay" to the end; (b) for words that begin with consonants, the initial consonants are moved to the end of the word, then "ay" is added. For example, translating "Hello, how are you today?" to pig Latin gives "Ellohay, onway areay oway odayday?". Note that this task also requires the model to reason about the language translation rules (i.e., rules (a) and (b)) before predicting for the test sample. The rules can be freely modified (e.g., by changing the suffix word "yay" to others) to create diverse tasks that require in-context reasoning from the LLM. Hence, we create a new variant of the LP task, named **LP-variant**, by using the "ay" suffix for both rules (a) and (b). An example of the task query is given in Example 3 (see App. B.1). As shown in Tab. 2, in this task, our EASE also demonstrates competitive results and outperforms all baselines.

**Remapped label tasks.** By remapping the labels of existing classification tasks to new ones, we construct novel tasks that are against the model's existing knowledge. In the commonly used AG News dataset, the news articles are classified into four categories: "World", "Sports", "Business" and "Sci/Tech". We construct a remapped dataset **AG News Remap** such that "World" news is now labeled as "Sports" news, "Sports" is now labeled as "Business", etc. This is against the LLM's knowledge since the output now does not correspond to the context descriptions of the input (i.e., news articles). So, the LLM has to learn these remapping rules from the in-context exemplars. Similarly, we construct another dataset **SST5 Reversed** by reversing the labels of the sentiment analysis dataset SST5, such that "very negative" labels are swapped with "very positive" labels, and "negative" labels are swapped with "positive" labels. The results for these novel remapped label tasks are also in Tab. 2, which shows superior performances of EASE over baselines by 6.7%-10% in absolute accuracy.

**Noisy tasks.** Exemplar selection is even more important to achieve good ICL performances when the dataset is potentially noisy, since using noisy or mislabeled data as exemplars could have detrimental effects on performance. Noisy datasets also better resemble practical scenarios because clean data is expensive and difficult to obtain. We construct noisy datasets by injecting various ratios of noisy outputs into the task datasets, ranging from having 10% noisy data samples (i.e., the remaining 90% are clean) to having 90% noisy data samples (i.e., the remaining 10% are clean). We show in Tab. 2 that EASE is the best-performing method across different noise ratios and generally exhibits a lower decrease in accuracy as the tasks become more difficult with increasing noise ratios.

Note that the conclusions on test accuracy results (in App. C.15) are consistent with the main text. These tasks above represent new families of tasks that contain novel knowledge for LLMs. We show through comprehensive experiments that exemplar selection is important and useful in practice, especially for novel downstream task adaptations. Also, the noisy datasets considered here align with real-world scenarios, which typically contain noises from observation, labeling error, corruption, etc.

### Jointly optimizing instruction and exemplars

To the best of our knowledge, no prior work can jointly optimize instruction and exemplars. For a fair comparison, we do not include an instruction when comparing with other exemplar selection baselines in earlier sections. As EASE is readily extended to find the optimal combination of instruction and exemplars (Sec. 3.3), we show the benefits of joint optimization in this section. According to Tab. 3, jointly optimizing these two essential components of a prompt significantly improves the performance for most tasks (marked with red arrows \(\)). This improvement is attributed to the ability of the optimized instruction to significantly reinforce the information captured in the corresponding exemplars. For example, an automatically optimized instruction "identify and list the animals from the given words" complements the exemplars in the taxonomy animal task. Therefore, our joint optimization of instruction and exemplars presents a _fully automated pipeline_ for prompt optimization and achieves impressive practical performances.

### Ablation studies

**Combination with retrieval-based methods to handle larger sets of exemplars.** Applying EASE to scenarios where the size \(n\) of the set of exemplars \(D\) is very large may lead to performance degradation. This is because the space \(\) of exemplar sequences becomes excessively large, which cannot be sufficiently explored without significantly increasing the computation (i.e., with the size of \(Q_{t}\) being fixed). To resolve this issue, a natural idea is to first filter the large pool of data exemplars to eliminate those that are less relevant to the task. This independent step to pre-filter candidates serves as a promising extension of our EASE to handle a large set of exemplars. To this end, we propose to first use retrieval-based methods to select the exemplars that are more relevant to the task, and then run our EASE using this refined smaller set of exemplars. Specifically, we use a cosine similarity retriever on exemplar embedding and perform exemplar selection on \(D\) with a size \(n\) as large as 100000. The query involves validation data exemplars \(D_{V}\) and we retrieve from the corpus of training set exemplars \(D\). For each validation data exemplar in \(D_{V}\), we retrieve the most similar/relevant training set exemplars from \(D\). Then, we combine them to form a smaller set of exemplars than \(D\) and proceed with EASE using this reduced subset for efficiency. As shown in Tab. 4, when the size \(n\) of the exemplar set is large, combining EASE with retrieval gives better performances than directly running EASE.

**Effectiveness of components of EASE.** Here we show the necessity of both of the main components to the success of EASE: OT and NeuralUCB. As shown in Tab. 7 of App. C.3, EASE significantly outperforms methods employing OT or NeuralUCB alone. Overall, a pure subset selection algorithm based on OT performs badly on its own, especially when the dataset's noise ratio is high. However, when used together with NeuralUCB, OT significantly improves the performance of our EASE.

  \\   &  \\  \\ 
1000 & 50.0\(\)29 & **60.0\(\)0.6** \\
10000 & 55.0\(\)0.00 & **63.3\(\)1.7** \\
50000 & 48.3\(\)1.7 & **63.3\(\)1.7** \\
100000 & 50.0\(\)29 & **65.0\(\)0.0** \\   \\  Size \(n\) & EASE &  \\  \\ 
1000 & 43.3\(\)3.3 & **48.3\(\)1.7** \\
3000 & 48.3\(\)4.4 & **48.3\(\)1.7** \\
5000 & 43.3\(\)1.7 & **50.0\(\)0.0** \\
7000 & 45.0\(\)0.0 & **48.3\(\)1.7** \\  

Table 4: Average accuracy \(\) s.e. achieved by EASE and EASE with retrieval for larger exemplar set sizes.

**Further ablations.** We defer further ablation studies to App. C, including those exploring (a) speedups from OT, (b) a larger \(k\) or a range of \(k\), (c) a larger query budget \(T\), (d) benefits of using an ordering-aware embedding, (e) different black-box LLMs, (f) other embedding models, (g) degree of exploration \(\), (h) asking GPT to directly select exemplars, and (i) additional experiments on more datasets including reasoning chains.

## 5 Related work

**Retrieval-based methods.** This approach utilizes trainable exemplar retrievers to select exemplars depending on the text sequence of each individual test sample . Liu et al.  first proposed a similarity-based (e.g., negative Euclidean distance or cosine similarity) retrieval strategy on the sentence embeddings of the exemplars for ICL. Gao et al.  further enhanced the above by emphasizing on exemplars with top-2 labels that the LLM is most uncertain about. Adapting retrievers to specific tasks, Rubin et al.  learned a retrieval model from evaluating individual exemplars by their 1-shot ICL performance on validation data. Improving on the limitation of the above methods that exemplars are considered in isolation (i.e., independently), Ye et al.  proposed to retrieve a set of exemplars jointly by examining their joint probability to capture inter-relationships. Likewise, Levy et al.  retrieved a set by selecting diverse exemplars that collectively cover all of the output structures. Gupta et al.  instead generalized the individual metrics to a set-level metric through a submodular function, which is well-suited for optimization via greedy algorithms. However, retrieval-based methods are characterized by varying exemplars for each test sample, which do not align with our practical setting of maintaining a fixed set of exemplars for the entire task. Also, another common drawback is that heuristics are typically required to order the exemplars in the retrieved set.

**Selection of a fixed set of exemplars.** Having a fixed set of exemplars offers practical and privacy-related advantages, such as the ease of implementation and reduced data exposure . Wang et al.  proposed to train a small LLM as a latent variable model using output token logits from independent exemplars, then forming a fixed exemplar set to directly transfer to target models. Similarly, Chang and Jia  developed a data model for each validation sample and introduced an aggregate metric to select subsets. Li and Qiu  proposed to filter and search for best exemplars using an informativeness metric derived from LLM output logits on classification tasks. However, these works are restricted to classification tasks. The closest to our work is that of Zhang et al. , which used reinforcement learning to actively select exemplars. In a similar setting to ours, Nguyen and Wong  used influence to select the most influential exemplars and order them arbitrarily to form the subset. However, both methods perform worse than our algorithm in our experiments.

**Instruction optimization.** Another related direction for enhancing the performance of LLMs is instruction optimization. Focusing on instructions within the prompt, evolutionary algorithms and zeroth-order optimization algorithms are gaining popularity in refining the prompts for black-box LLMs . Specifically, some studies  maintained a constant set of exemplars throughout the process of prompt optimization, whereas others  ignored the consideration of exemplars altogether by adopting a zero-shot setting. It is therefore imperative to develop an effective exemplar selection method for black-box LLMs.

## 6 Conclusion and limitation

We propose EASE, an algorithm that selects the optimal ordered set of exemplars for in-context learning of black-box LLMs in an automated fashion. EASE is query-efficient due to the adoption of the NeuralUCB algorithm and is further made computationally feasible for large spaces of exemplar sequences through a technique based on optimal transport. Additionally, our EASE has been extended to a fully automated prompt optimization pipeline that jointly optimizes exemplars and instruction for the best in-context learning performance. Furthermore, we provide practical insights indicating that exemplar selection in in-context learning is more crucial for downstream tasks that the LLM has limited knowledge about. However, the on-the-fly computation of embedding for the ordered exemplar sequences is the computational bottleneck of our method, which could be potentially improved in future work for more efficient optimization. Also, a potential limitation of EASE is the requirement for a suitable validation set, which may not be readily available in some scenarios.