# Debiased and Denoised Entity Recognition from

Distant Supervision

 Haobo Wang\({}^{1*}\), Yiwen Dong\({}^{1}\)1, Ruixuan Xiao\({}^{1}\), Fei Huang\({}^{2}\), Gang Chen\({}^{1}\), Junbo Zhao\({}^{1}\)2

\({}^{1}\)Zhejiang University, Hangzhou, China

\({}^{2}\)Alibaba Group, Hangzhou, China

{wanghaobo, dyw424, xiaoruixuan, cg, j.zhao}@zju.edu.cn, feirhuang@gmail.com

Equal contribution.Corresponding author.

###### Abstract

While distant supervision has been extensively explored and exploited in NLP tasks like named entity recognition, a major obstacle stems from the inevitable noisy distant labels tagged unsupervisedly. A few past works approach this problem by adopting a self-training framework with a sample-selection mechanism. In this work, we innovatively identify two types of biases that were omitted by prior work, and these biases lead to inferior performance of the distant-supervised NER setup. First, we characterize the noise concealed in the distant labels as highly structural rather than fully randomized. Second, the self-training framework would ubiquitously introduce an inherent bias that causes erroneous behavior in both sample selection and eventually prediction. To cope with these problems, we propose a novel self-training framework, dubbed DesERT. This framework augments the conventional NER predicative pathway to a dual form that effectively adapts the sample-selection process to conform to its innate distributional-bias structure. The other crucial component of DesERT composes a debiased module aiming to enhance the token representations, hence the quality of the pseudo-labels. Extensive experiments are conducted to validate the DesERT. The results show that our framework establishes a new state-of-art performance, it achieves a **+2.22**% average F1 score improvement on five standardized benchmarking datasets. Lastly, DesERT demonstrates its effectiveness under a new DSNER benchmark where _additional distant supervision_ comes from the ChatGPT model.

## 1 Introduction

As an iconic and pivotal task, named entity recognition (NER)  has gained wide attention from the natural language processing community. While there have been several successes towards solving this task [2; 3; 4; 5], most of these solutions demand decent datasets for training that enjoys both good quality and large quantity. When it comes to specialized industrial deployment, this leads us to expensive data annotation and curation costs which may hinder the development of NER.

Distant supervision [6; 7; 8] is a promising direction to alleviate this problem. Indeed, there have been a number of attempts to combine the additional labels obtained from distant supervision with the original unlabeled data, and then establish a selection-based self-training framework  to handle noise among distant labels. The core of these methods is to sample reliable clean tokens and cope with potentially noisy tokens involved by the imperfect pseudo-label prediction.

On one hand, we characterize that the label distribution for distant supervision is rather skewed. Notably, as shown in Figure 1 (a), we found that true non-entity tokens are far more than the remainingtokens, quantity-wise. Meanwhile, there exists a severe class imbalance in true entity tokens with fine-grained noise in Figure 1 (b). Ignoring these phenomena by prior sampling-based methods is potentially performance-limited. The reason is that the dominant clean non-entity tokens can be easily distinguished and excessively picked up, while clean entity tokens are largely overlooked, especially on label-deficient entities like MISC and LOC; see Figure 1 (c). Whereas, the clean entity tokens are of real interest since NER attempts to distinguish the fine-grained token types. Unfortunately, there has not been any prior work that discusses or addresses this property specifically.

On the other hand, in spite of that self-training scheme applied with distant supervision is proven applicable [10; 9; 11], it could still be unreliable mostly due to the deviation between the pseudo-labels and unobservable true labels. In other words, the self-training scheme is inherently prone to assign erroneous pseudo-labels, which can be aggravated due to label noise and finally cause error accumulation and conversely pollutes the selection. We postulate that both biases ought to be properly resolved in order to fully unlock the utility boost of the distant supervision technique for NER.

To this end, we propose DesERT, a holistic framework that resolves the aforementioned problems and achieves state-of-the-art DSNER performance. The main idea behind the DesERT is to decouple the initial task/data into entity vs. non-entity and fine-grained entity type sub-parts, then respectively to split again by selecting and splitting out clean- and noisy-token sets which are processed by different learning paradigms, forming a task/data taxonomy. We further introduce debiased self-training on noisy sets to reduce erroneous pseudo-labels. Empirical findings successfully justify the validity of our approach, where on multiple standardized NER benchmarks we establish a new set of state-of-the-art results. For example, DesERT achieves state-of-art performance, especially an improvement of **3.26%** on the CoNLL03 dataset and **4.26%** on the Webpage dataset. Finally, we establish a new benchmark for DSNER where the supervision signals are generated via the large language model ChatGPT and DesERT still holds its superiority.

## 2 Related Works

Distantly-Supervised NER.There are two typical forms of noise in the labeling of distantly-supervised NER (DSNER) tasks: false negative tokens wrongly tagged as non-entities and inaccurate entity types. A series of works mainly focus on processing false negative tokens. A popular line of works adopts PU Learning [12; 13] techniques to resolve DSNER [14; 15], which sees entity tokens as Positive (P) data and non-entity tokens as Unlabeled (U) data, yet ignore the noise in fine-grained entities. Other methods consider both types of noise, usually inspired by traditional NLL approaches. One of them is based on clean token selection. Lots of works adopt the model's prediction consistency or confidence as the selection criterion [16; 17; 10; 18; 9; 11; 19; 20]. Besides, Huang et al.  propose a novel Logit Maximum Difference (LMD) score. HGL  introduces hypergeometric distribution to estimate noisy sample size. Additionally, some studies [23; 24] also employ reinforcement learning agents to filter noisy samples. A recent work  studies a specific DSNER setup that collects a large set of weakly labeled data and a small set of clean training data. The

Figure 1: (a) Confusion matrix of true labels and distant labels on the whole CoNLL03 dataset. Class index 0 denotes non-entity and 1 denotes all entity types, more than 169.1k non-entity tokens are correctly labeled and dominate true entity tokens. (b) The confusion matrix displays noise among true entity-type labels. (c) The real token class distribution on the CoNLL03 dataset and tokens selected at the fifth epoch by a basic self-training framework with a single-head/double-head pathway. It can be shown that double-head selects more tokens than single-head on the minority entity classes such as MISC and LOC.

most related work to ours is  which also investigates the bias issues in DSNER, but it concentrates on structural causal biases caused by the dictionaries. In contrast, our work focuses on unexpected selection and confirmation biases in popular _selection and self-training_ DSNER frameworks.

Self-training.Self-training [27; 28; 29] is a popular semi-supervised learning technique that trains on labeled data but assigns pseudo-labels to unlabeled data for further training . This is also adopted by existing DSNER approaches. For example, BOND  uses pseudo-labels from the teacher model to train the student model. SCDL  jointly trains two teacher-student networks to mutually provide pseudo-labels. RoSTER  ensembles different models' predictions as pseudo-labels. However, some research has pointed out that self-training exists an inherent confirmation bias to assign erroneous pseudo-labels [31; 32]. Due to the existence of label noise, the DSNER task can face an amplified self-training bias, which, however, has never been touched on in prior studies.

## 3 Notation and Preliminary

Named Entity Recognition.Assume we receive a NER training dataset \(=\{(_{m},_{m})\}_{m=1}^{M}\) with \(M\) tuples, where each tuple consists of a sentence \(=[x_{1},x_{2},...,x_{n}]\) with \(n\) tokens and a label sequence \(=[y_{1},y_{2},...,y_{n}]\). Here each entity label \(y_{i}\) indicates a token \(x_{i}\) is a non-entity or belongs to a specific entity type. \(y_{i}=\{0,1,...,K\}\), where \(1 K\) denote entity types and 0 denotes non-entity. Our goal is to train a NER model \(f\) parameterized by \(\) to be able to receive a testing sentence and predict the entity labels for its tokens. For fully supervised NER, this goal can be achieved by minimizing the empirical standard cross-entropy loss as follows,

\[_{ce}(,\!f(;\!))\!=\!-_{i=1}^{n} (f_{i,y_{i}}(;))\]

where \(f_{i,y_{i}}(;)\) is the model's predicted probability of \(i\)-th token \(x_{i}\) belonging to class \(y_{i}\) in sentence \(\). In practice, the NER model \(f\) is typically composed of a pre-trained language model encoder \(\) for extracting token representation and a classification head \(h\), i.e., \(f(;)=h()\).

Distantly-Supervised NER.This paper considers the distantly-supervised NER problem, which effectively reduces labeling costs. Accordingly, the true label sequences \(\{_{m}\}_{m=1}^{M}\) are replaced by their distantly-annotated counterparts \(\{}_{m}\}_{m=1}^{M}\). Despite the promise, the distant labels inevitably contain large portions of label noise due to the limited coverage of entity mentions in the knowledge base and the labeling ambiguity. To cope with this problem, we follow previous works [10; 9] and introduce the _selection and self-training_ framework to mitigate the label noise. At each epoch, we first perform token selection based on the specific criterion that separates the tokens in each sentence \(\) into a set of potential clean tokens \(^{t}\) and noisy tokens \(^{u}\), and thus, a robust NER model can be trained on those clean tokens. To further exploit the remaining noisy tokens, we regard them as unlabeled tokens and perform semi-supervised training,

\[_{cls}=_{L}(}^{l},f(^{l};))+ _{U}(}^{u},f(^{u};))\]

where \(_{L}\) and \(_{U}\) are classification losses on the two sets. \(}^{u}\) is a pseudo-label generated via previous models. In the sequel, we will specify our selection criterion and self-training algorithms in detail.

## 4 Proposed Method: DesERT

In theory, distant supervision can provide an arbitrarily large dataset for NER with its scale upper-bounded by all available sources. In this task of distantly-supervised NER, we denote the initial set extracted by distant supervision by \(\). Of course, directly using \(\) to train a NER model by supervised learning is problematic, due to the noise, uncertainty, or mismatches hidden in \(\). In this section, we provide a detailed description of our framework. The resulting architecture would effectively decompose \(\) into a task-data taxonomy, with each component from this taxonomy manifesting an individual training paradigm. This decomposition is stemmed from the two bias problems we mentioned previously. The pseudo-code of DesERT is summarized in Appendix D.

### Decoupled Learning for Distant Supervision

To begin with, we start by introducing the top-tier decomposition which forms the first- and high-level of the taxonomy.

The Bias Problem.Let us recall the observation from Figure 1. Extending from it, we draw the following conclusion:

* in Figure 1 (a), we plot the confusion matrix of the true labels against distant labels without distinguishing between fine-grained types. It effectively indicates that the much dominant number of non-entity tokens against the true entity tokens (about ratio **170:23**);
* in Figure 1 (b), we see the noisy distribution of the fine-grained entity-type labels is very much biased and imbalanced. In spite of that the prior work generally omits this problem, we believe this is so vital that the neglect of it would cause an overly high probability of non-entity tokens being picked during the selection process of DSNER. Therefore, only a few entity tokens are selected as clean data for normal NER training;
* Figure 1 (c) further offers supporting evidence. It shows that the previous DSNER frameworks, built with a single-head pathway hardly draft the clean and minority entity tokens. In particular, it samples 436/4,593 (9.49%) MISC tokens and (almost) none I-LOC tokens which are of more value in solving NER.

To sum up, we believe this loss of a few entity tokens being selected -- caused by the data bias -- is genuinely a huge waste and may have significantly hindered the performance gain. While the prior works generally overlook this problem, in this article, we intend to study it thoroughly and give a solution to resolve it.

Decomposition of the NER Task.To overcome the aforementioned data bias, we decompose the dataset \(\) into two sub-parts, \(=_{b}_{e}\) where the subscripts of \(b\) and \(e\) indicate binary- and entity-classification respectively. In effect, we proactively decompose the NER task of single-level prediction into a hierarchical dual that the binary prediction targets entity versus non-entity while the counterpart task is to predict the specific entity category. Notably, the skewed data distribution on \(\) is also decomposed into two parts on \(_{b}\) and \(_{e}\), which can be alleviated by separate processing and will not interfere with each other. More specifically, to construct this level of taxonomy, the label set \(^{b}_{b}\) is binarized. In the meantime, \(^{e}_{e}\) removes the non-entity tag (as invalid) and only reserves entity type labels.

Double-head Pathway.In what follows, to facilitate the training process upon the decomposition, we correspondingly augment a separate pathway to the original single prediction pathway, written as \(h= h_{b},h_{e}\), where the subscripts follow the same naming rule as the previous paragraph. The end of the two pathways are both fully connected layers with different entry numbers: \(1\) for the \(h_{b}\) and \(K\) for \(h_{e}\). The two pathways shared the same encoder, \(\), as we introduce in Section 3. A more

Figure 2: **(a)**: Illustration of decoupled learning paradigm and debiased self-training. **(b)**: Two classification heads are trained independently but make joint predictions when testing. # denotes unselected noisy or invalid tokens.

detailed illustration is provided in Figure 2 (a). We leave the further textual description of the model architecture and task-data taxonomy details in Appendix A.

Joint Prediction.One last piece of utilizing the dual pathway in the framework is the inference procedure. Since either pathway is unable to generate a complete NER prediction independently, we combine them in a jointly predictive fashion. As shown in Figure 2 (b), we form a prediction hierarchy as follows. Given any token \(x_{i}\), the binary pathway \(h_{b}\) offers a probability of that token being a part of an entity denoted as \(p_{i}^{b}\). On the other hand, the entity pathway generates a vector of specific entity probability \(_{i}^{c}\) having \(K\) entries. Then the double-head pathway generates a joint probability \(_{i}=[1-p_{i}^{b},p_{i}^{b}*_{i}^{c}]\), that \(1-p_{i}^{b}\) is the probability of being an non-entity. By then, the model can predict each token's specific type, i.e., whether a non-entity or fine-grained entity type.

### Clean Token Selection

Following the previous decomposition, the next step we propose is to follow a general noisy-label learning setup -- to select the clean set from the noisy dataset. In particular, upon either task/data component in the first level taxonomy, it can be further decomposed down to be a union of the clean and noisy set. Different learning schemes are then applied to these two sets of data and enacted concurrently. In this section, we discuss how the sample selection is devised.

We take the binarized dataset/task to describe the selection mechanism. In hindsight, we assume that the tokens ought to be put into the clean pool when the model's prediction is relatively certain and it matches the provided counterpart label by distant supervision. Formally, given any token \(x_{i}\) drawn from any sentence, denote its distant label as \(_{i}^{b}\). During training, we take the model obtained at the current step and feed the input through it attaining a current prediction of \(x_{i}\), dubbed as \(_{i}^{b}\). Hereby, we can write down the primary selection criterion:

\[_{b}^{l}\!=\!\{(x_{i},_{i}^{b})|(_{i}^{b}\! =\!_{i}^{b})((p_{i}^{b},1\!-\!p_{i}^{b})\!>\!)\}\]

Noted, we use \(_{b}^{l}\) to denote the clean set, as superscript \(l\) implies "labeled" since the clean tokens are primarily utilized as if they were gold. We simply adopt \((p_{i}^{b},1-p_{i}^{b})\) as a proximal form of predictive uncertainty on the given sample where \(\) is a hyperparameter for the threshold. In most of our experiments, we fix it to \(0.95\) without further tuning. Further, the left-out samples -- where the token samples do not meet this criterion -- are gathered and then form \(_{b}^{u}\) such that \(_{b}=_{b}^{l}_{b}^{u}\). Here the superscript \(u\) means "unlabeled" as we remove the tagged distant supervision labels on them and essentially treat these samples as unlabeled ones. For the finer-grained dataset \(_{e}\), we adopt the same strategy, as \(_{e}=_{e}^{l}_{e}^{u}\).

Notably, the decomposition of the task/data taxonomy is grounded at the token level; namely, the same sentence could contain clean tokens and noisy tokens belonging to different sets for training respectively. Further, the assignment of the taxonomy is dynamic and altered on the fly. This means that at every round of training, DesERT performs token selection based on an inference path conducted by the model trained at the current cycle.

### Debiased Self-Training

In the previous step, we split the datasets and process on the clean token sets \(_{b}^{l}\) and \(_{e}^{l}\), leaving unselected noisy token sets \(_{b}^{u}\) and \(_{e}^{u}\) remained to be tackled. Since labels for \(_{b}^{u}\) and \(_{e}^{u}\) are likely to be noisy, we remove them and tag these tokens with pseudo-labels via a self-training framework. Notably, the pseudo-labels can be obtained from various sources and we choose a teacher-student architecture that is the same as previous work . Despite that self-training has been proven effective repeatedly [10; 9; 11], we argue that on this occasion it might still be unreliable due to another source of inherent bias.

In practice, the training on the clean and noisy set alternates. In hindsight, the supervised learning on the selected clean set naturally clusters the clean tokens tightly to their respective class centers. By contrast, the noise tokens receive dynamically changing labels, and it is likely that the labels are wrong. This, in theory, may cause the corresponding representation to deviate from the class centers and even oscillate by big magnitude due to its instability assignment during training. Given the imbalance prediction nature of the DSNER task, we empirically find that this kind of bias is trended to aggravate if no other mechanism is imposed. Further, if we let this bias be untangled, the clean token selection we introduced previously might be severely biased and polluted.

Therefore, to overcome this issue, we employ a worst-case cross-entropy (WCE) loss objective . In a nutshell, the WCE loss adversarially optimizes the unlabeled token representation by matching the worst classifier on labeled data. We call a classifier the worst if it can correctly classify all the labeled data, but is very close to the edge of clusters, resulting in a minimal margin. If we adversarially train the unlabeled tokens to be correctly classified by this worst classifier, their features can be tight to at least one class center. Therefore, the decision hyperplane can be pushed to a lower entropy region, which is known to be beneficial for semi-supervised learning . By regularizing noisy tokens to approach the class centers rather than the decision boundary, the classifier can also generate more stable and higher-quality pseudo-labels. For better understanding, we elaborate our _theoretical insights_ of this debiased learning procedure in Appendix B.

Nevertheless, in practice, it is impossible to obtain the truly worst classifier. Hence, proximally, we estimate a possible worst classifier \(h_{w}\) such that it distinguishes the selected tokens correctly while making mistakes on (i)-the unchosen tokens and (ii)-the produced pseudo-labels at previous cycles, as much as possible. Formally, we denote \(f^{h}=h\) and calculate the worst head by,

\[h_{w}=_{h^{}}_{U}(}^{u},f^{h^{ }}(^{u}))-_{L}(}^{l},f^{h^{}}(^{l}))\]

where the encoder \(\) is frozen at this step. \(}^{u}=f^{h}(^{u})\) is the pseudo-label that serves as a proxy to estimate the classification error.

Then, we adversarially optimize the encoder \(\) to be correctly classified by the worst-case classifier. The formulation \(_{wce}\) is given by:

\[_{wce}()=_{U}(}^{u},f^{h_{w}}(^{u }))-_{L}(}^{l},f^{h_{w}}(^{l}))\]

Noted that we fix the worst-case head \(h_{w}\) and only update the encoder representation. Moreover, since the binary head regards all fine-grained types as an integrated class, optimizing WCE on it may cause a collapse of the representation and then, hinder the model of distinguishing different fine-grained entity types. Therefore, we calculate WCE loss \(_{e\_wce}\) on the entity-type recognition pathway. Empirically, the noisy token representation gradually approaches to corresponding class center. Consequently, the decision hyperplane is more likely to lie in a lower entropy region, thereby mitigating the impact of self-training bias.

### Practical Implementation

Dual Co-guessing Mechanism.It has been widely verified that a larger labeled set can greatly boost semi-supervised training. However, in our selection procedure, the size of the clean set is largely restricted by the number of originally clean samples owing to our label-matching constraint. Fortunately, many tokens, while originally noisy, can be accurately predicted by the model as SSL training proceeds. To this end, we develop a co-guessing mechanism that collects well-predicted tokens as a pseudo proxy to enlarge the clean sample set.

Specifically, we train two peer networks having the same architecture, which we denote as \(f(;_{1})\) and \(f(;_{2})\). Given those easy-to-learn noisy tokens, we postulate that different networks can generate consistent and high-confidence predictions. This gives rise to the following criterion,

\[\{(x_{i},_{i}^{(1)})|(_{i}^{(1)}=_{i}^{(2)}) ((_{i}^{(1)})>)((_{i}^{(2)})>)\}\]

\(_{i}^{(1)}\) and \(_{i}^{(2)}\) are predicted labels from two models, \(_{i}^{(1)}=(f_{i}(;_{1}))\) and \(_{i}^{(2)}\) is same. When it comes to specific sub-datasets \(_{b}\) and \(_{e}\), the predicted labels should be converted to respective counterparts, and probability is generated by the corresponding pathway.

Training loss.The overall training loss is defined as follows:

\[=_{b\_cls}+_{e\_cls}+w*_{e\_wce}\]

Here \(w>0\) is a weighting factor. \(_{b\_cls}\) and \(_{e\_cls}\) are the classification losses on the two predictive heads respectively. \(_{e\_wce}\) is the WCE loss on the entity pathway.

Post-hoc Entity Pathway Finetuning.In previous steps, we train the double-head pathway separately as both pathways can be unreliable due to the label noise. Over the course of training, we observe the binary pathway learns well thanks to abundant training samples. On the contrary, the entity pathway still faces a rather challenging condition caused by inefficient samples, label noise, and more categories. To relieve this problem, we propose to post-hoc finetune the entity pathway by constructing a more refined dataset via the binary pathway. In specific, after the training procedure, we freeze the binary pathway \(h_{b}\) and collect a new clean entity dataset by,

\[^{l}_{e}=\{(x_{i},^{e}_{i})|(^{b}_{i}=1) (^{e}_{i}=^{e}_{i})\}\]

That is, while still anticipating selecting clean samples, we remove those tokens predicted as non-entity by \(h_{b}\), which produces a more refined clean sample set than before. Lastly, we finetune the entity pathway with this clean dataset for a few epochs to boost the performance.

## 5 Experiments

In this section, we present our main experimental results to show the effectiveness of our method. More empirical setups and results are reported in Appendix C.

### Setup

Dataset.We evaluate our framework on five widely-used named entity recognition benchmark datasets in the English language: (1) **CoNLL03** is collected CoNLL 2003 Shared Task, consisting of 1,393 English news articles and four entity types; (2) **OntoNotes5.0** contains 1.6 million words annotated by 18 entity types from multiple domains, including broadcast conversation, Web data, and P2.5 data; (3) **Webpage** collects 783 entities belonging to the four types like CoNLL03, which are from 20 webpages including personal, academic, and computer-science conference homepages. (4) **Wikigold** comprise 39,007 Wikipedia article tokens from a 2008 English dump with four CoNLL03 named entity tags; (5) **Twitter** is an open-domain NER dataset coming from the WNUT 2016 NER shared task, which consists of 2,400 tweets (a total 34k tokens) with 10 entity tags. To obtain a distant supervision scheme, we follow previous works  to re-annotate all the training corpus by matching entity types in external knowledge bases including Wikidata corpus and gazetteers extracted from multiple online sources.

Baselines.We compare the proposed framework with a total of seven current _distantly-supervised_ methods: (1) **AutoNER** trains a fuzzy LSTM-CRF network to handle tokens with multiple possible labels, using a Tie or Break scheme; (2) **LRNT** employs Partial-CRF for sequence labeling and training with high-quality sentences given by high annotation confidence and coverage; (3) **Co-teaching+** is a robust sample selection-based algorithm, which only updates the dual networks by prediction disagreement data; (4) **JoCoR** adopts dual networks to calculate a

    &  &  &  &  &  \\   & **P** & **R** & **F1** & **P** & **R** & **F1** & **P** & **R** & **F1** & **P** & **R** & **F1** \\   \\  BiLSTM-CC & 91.35 & 91.06 & 91.21 & 85.99 & 86.36 & 86.17 & 50.07 & 54.76 & 52.34 & 55.40 & 54.30 & 54.90 & 60.01 & 46.16 & 52.18 \\ RoBERTa-base & 89.14 & 91.10 & 90.11 & 84.59 & 87.88 & 86.20 & 66.29 & 79.73 & 72.39 & 85.33 & 87.56 & 86.43 & 51.76 & 52.63 & 52.19 \\   \\  AutoNER & 75.21 & 60.40 & 67.00 & 64.63 & 69.95 & 67.18 & 48.82 & 54.23 & 51.39 & 43.54 & 52.35 & 47.54 & 43.26 & 18.69 & 26.10 \\ LRNT & 79.91 & 61.87 & 69.74 & 67.36 & 68.02 & 67.69 & 47.60 & 48.83 & 47.74 & 45.60 & 46.84 & 46.21 & 46.94 & 15.98 & 23.84 \\ Co-teaching+ & 86.04 & 68.74 & 76.42 & 66.63 & 69.32 & 67.95 & 61.65 & 55.41 & 58.36 & 55.23 & 49.26 & 52.08 & 51.67 & 42.66 & 46.73 \\ JoCoR & 83.65 & 69.69 & 76.04 & 66.74 & 68.74 & 67.73 & 62.14 & 58.78 & 60.42 & 51.48 & 51.23 & 51.35 & 49.40 & 45.59 & 47.42 \\ NegSampling & 80.17 & 77.72 & 78.93 & 64.59 & **72.39** & 68.26 & 70.16 & 58.78 & 63.97 & 49.49 & 55.35 & 52.26 & 50.25 & 44.95 & 47.45 \\ BOND & 82.05 & 80.92 & 81.48 & 67.14 & 69.61 & 68.53 & 67.37 & 64.19 & 65.74 & 53.44 & 68.58 & 60.07 & 53.16 & 3.76 & 48.01 \\ SCDL & **87.96** & 79.82 & 83.69 & **67.49** & 67.67 & 68.61 & 67.81 & 68.24 & 68.47 & 62.25 & 66.12 & 61.43 & **59.87** & 45.57 & 51.09 \\ 
**Ours*** & 86.23 & 87.28 & 86.75 & 66.38 & 72.08 & 69.11 & 71.52 & **72.97** & 72.24 & 60.77 & 68.10 & 64.23 & **56.44** & **48.38** & **52.10** \\
**Ours*(finetune)** & 86.41 & **87.49** & **86.95** & 66.63 & 71.92 & **69.17** & **72.48** & **72.97** & **72.73** & **62.87** & **69.42** & **65.99** & 57.65 & 47.80 & **52.26** \\   

Table 1: Main results on five benchmark datasets measured by Precision (P), Recall (R), and F1 scores. We highlight the best overall performance for distant supervision in bold.

joint loss with co-regularization; (5) **NegSampling** handles incomplete annotations by negative sampling; (6) **BOND** is a self-training paradigm that selects reliable clean tokens for training by a teacher-student network with early stopping; (7) **SCDL** jointly trains two teacher-student networks to iteratively perform label refinery in a mutually beneficial manner. Besides, we select two _fully-supervised_ methods trained with clean annotations to show the performance upper bound: (1) **BiLSTM-CNN-CRF** (**BiLSTM-CC**)  adopts bi-directional LSTM with character-level CNN to generate token embeddings, followed by a CRF layer to predict the most likely label sequence; (2) **RoBERTa-base** adopts a pre-trained RoBERTa-base model as the encoder.

Implementation details.We adopt two pre-trained language models RoBERTa-base and DistilRoBERTa as the backbone. Inspired by , we further maintain a teacher network that is an exponential moving average of the main model. Thereafter, the data split, pseudo-labeling, loss calculation, and co-guessing operators are done by the prediction of the teacher network; see Appendix C.4 for detailed descriptions. We follow [10; 9] to tune most hyperparameters. Specifically, we train the networks for 50 epochs with a few epochs of warm-up, followed by 2 epochs of finetuning. The training batch size is set as 16 on four datasets, except 32 on OntoNotes5.0. The learning rate is fixed as \(\{1e^{-5},2e^{-5},1e^{-5},2e^{-5}\}\) for these datasets respectively. The confidence threshold parameter \(\) is tuned by 0.9 for Wikigold while 0.95 for others. The co-guessing is performed from the \(k\)-th epoch, which we set to \(\{6,40,35,30,30\}\) respectively. For finetuning, the learning rate is one-tenth of the original one. Once the training ends, we choose the first student model for predictions as the final results. We run all experiments for three random trials and record the mean results.

### Main Results

We measure the performance of all methods by Precision (P), Recall (R), and F1 scores. As shown in Table 1, our proposed method DesERT achieves state-of-the-art performance compared with other baselines on five benchmark datasets. Specifically, on the CoNLL03 and Webpage datasets, DesERT outperforms the best baseline methods by 3.26% and 4.26% in terms of F1 score. The performance is even close to the best fully-supervised baseline (i.e. RoBERTa-base) on Webpage and Twitter, which suggests better adaptation to distant supervision. In addition, DesERT displays significantly higher recall on almost all datasets, demonstrating a strong ability to identify entities from text. It also maintains moderate precision so as to attain extraordinary F1 scores. Finally, the model after post-hoc finetuning shows obviously better performance. It demonstrates that a well-trained binary head can help the fine-grained head to better distinguish entity types, proving the superiority of the task decomposition formula.

### Ablation Study

To explore the effectiveness of each component in DesERT, we conduct an ablation study on the CoNLL03 dataset without the finetuning stage. As shown in Table 9, we test the following cases: (1) remove the WCE loss; (2) do not perform dual networks co-guessing; (3) replace the double-head pathway with a single-head pathway.

It can be seen from the ablations that the removal of any component leads to performance degradation, while the model can easily obtain the best F1 score under the combined effects of three components.

Figure 3: The F1-scores of DesERT with/without double-head pathway on four entity types, which have 11.1k/8.3k/10.0k/4.6k tokens respectively (from left to right).

[MISSING_PAGE_FAIL:9]

Efficacy of threshold parameter \(\).We study the effect of the important hyperparameter \(\) used in clean sample selection and dual networks co-guessing on the CoNLL03 dataset. Specifically, we vary the value of \(\) in the range \(0.7\) to \(0.99\), and other parameters are kept as default. Then we validate DesERT's performance by applying different \(\) values, as shown in Figure 4 (c). We can see that the model's performance is relatively insensitive in the \(\) varies from 0.95 to 0.99, but drops a lot when the \(\) value is lower than 0.95. And DesERT achieves the best results with \(\) fixed at 0.95 on the CoNLL03. In practice, we suggest setting a high confidence threshold \(\).

### Distant Supervision from Large Language Models

Recently, large language models (LLMs), including GPT-3 , ChatGPT, and GPT-\(4\)3, have largely revolutionized the NLP landscape. Thanks to their emerging abilities like in-context learning (ICL)  and chain-of-thought , LLMs demonstrate remarkable zero-shot learning performance in a wide range of downstream NLP tasks. Despite the promise, some recent studies  have shown that LLMs are still legs behind the fine-tuned small language models in many NLP applications including NER. To deal with this problem, we extend the DSNER formulation and design a novel in-context learning algorithm that exploits self-generated text-tag pairs to generate distant labels. Moreover, we modify our original algorithms to fully use hybrid labels including ChatGPT-generated labels and original knowledge-base-generated labels (KB labels). We refer the readers to Appendix C.1 for a detailed description of our prompt design, data generation process, and methodology.

Empirically, we provide a series of experiments to verify the feasibility of utilizing ChatGPT for distant supervision. In particular, we compare DesERT with three algorithms: (1) **ChatGPT**: we employ the ChatGPT model to produce zero-shot tagging on testing data; (2) **ChatGPT-A**: we employ ChatGPT to generate a set of text-tag pairs and use it for few-shot ICL on testing data; (3) **SCDL**: the most competitive baseline in our main Table. Table 4 lists our full results, we have the following conclusions: (1) ChatGPT does indeed demonstrate inferior results even compared with distantly-supervised SLMs; (2) simply training current best-performing DSNER methods on ChatGPT supervision can be inferior since they are particularly designed for KB labels; (3) ChatGPT can be a complementary source of distant supervision to existing KB labels, and our modified DesERT ( dubbed DesERT*) still obtains best performance when handling hybrid labels. Overall, we firmly believe that DSNER remains a pivotal research pursuit in the LLMs era. Looking forward, we envision LLMs serving as a novel source of distant supervision, facilitating the distillation of distantly-supervised SLMs, which has enhanced accuracy and computational efficiency.

## 6 Conclusion

In this paper, we propose a novel framework DesERT for NER under distant supervision. We first find that existing self-training methods suffer from two types of biases, i.e., a skewed data distribution in distant supervision and an inherent bias to assign erroneous pseudo-labels. To mitigate both biases, we propose a novel task/data taxonomy to decompose the original task, alongside a debiased self-training to improve the quality of pseudo-labels. Extensive experiments on other standard benchmarks clearly validate the effectiveness of DesERT. Furthermore, we supplement a new ChatGPT-based DSNER benchmark and show that DesERT still obtains the best performance. We hope our work can draw more attention from the community to the bias issues in the distantly-supervised NER tasks.

  
**Supervision** &  &  &  &  \\ 
**Model** & ChatGPT & ChatGPT-A & SCDL & DesERT & SCDL & DesERT & SCDL* & DesERT* \\  Precision & 68.95 & 79.11 & 68.39 & 81.91 & **87.96** & 86.23 & 83.87 & 87.24 \\ Recall & 64.16 & 63.13 & 72.74 & 77.38 & 79.82 & 87.28 & 85.50 & **88.93** \\ F1 & 66.47 & 70.22 & 70.50 & 79.58 & 83.69 & 86.75 & 84.67 & **88.08** \\   

Table 3: Performance of DesERT with different sources of distant labels on CoNLL03.