# HydraViT: Stacking Heads for a Scalable ViT

Janek Haberer\({}^{*}\), Ali Hojjat\({}^{*}\), Olaf Landsiedel

Kiel University, Germany

\({}^{*}\)_Equal contribution_

{janek.haberer,ali.hojjat,olaf.landsiedel}@cs.uni-kiel.de

###### Abstract

The architecture of Vision Transformers (ViTs), particularly the Multi-head Attention (MHA) mechanism, imposes substantial hardware demands. Deploying ViTs on devices with varying constraints, such as mobile phones, requires multiple models of different sizes. However, this approach has limitations, such as training and storing each required model separately. This paper introduces HydraViT, a novel approach that addresses these limitations by stacking attention heads to achieve a scalable ViT. By repeatedly changing the size of the embedded dimensions throughout each layer and their corresponding number of attention heads in MHA during training, HydraViT induces multiple subnetworks. Thereby, HydraViT achieves adaptability across a wide spectrum of hardware environments while maintaining performance. Our experimental results demonstrate the efficacy of HydraViT in achieving a scalable ViT with up to 10 subnetworks, covering a wide range of resource constraints. HydraViT achieves up to 5 p.p. more accuracy with the same GMACs and up to 7 p.p. more accuracy with the same throughput on ImageNet-1K compared to the baselines, making it an effective solution for scenarios where hardware availability is diverse or varies over time. The source code is available at https://github.com/ds-kiel/HydraViT.

Figure 1: Performance comparison of HydraViT and baselines on ImageNet-1K in terms of GMACs (a) and throughput (b) evaluated on NVIDIA A100 80GB PCIe. HydraViT trained on 3-12 heads demonstrates superior performance over DynaBERT (Hou et al., 2020) and SortedNet (Valipour et al., 2023). While MatFormer (Kudugunta et al., 2023) shows higher performance than HydraViT within its limited scalability range, but when we train on a narrower scalability range (9-12 heads), HydraViT surpasses MatFormer. We also show that training HydraViT for more epochs can further improve accuracy. Note that each line corresponds to one model, and changing the number of heads in the vanilla DeiT models significantly drops their accuracy to less than 30%.

Introduction

MotivationFollowing the breakthrough of Transformers (Vaswani et al., 2017), Dosovitskiy et al. (2021) established the Vision Transformer (ViT) as the base transformer architecture for computer vision tasks. As such, numerous studies build on top of ViTs as their base (Liu et al., 2021; Tolstikhin et al., 2021; Yu et al., 2022). In this architecture, Multi-head Attention (MHA) plays an important part, capturing global relations between different parts of the input image. However, ViTs have a much higher hardware demand due to the size of the attention matrices in MHA, which makes it challenging to find a configuration that fits heterogeneous devices.

To accommodate devices with various constraints, ViTs offer multiple independently trained models with different sizes and hardware requirements, such as the number of parameters, FLOPS, MACs, and hardware settings such as latency and RAM, with sizes typically increasing nearly at a logarithmic scale (Kudugunta et al., 2023), see Table 1. Overall, in the configurations of ViTs, the number of heads and their corresponding embedded dimension in MHA emerges as the key hyperparameter that distinguishes them.

While being a reasonable solution for hardware adaptability, this approach has two primary disadvantages: (1) Despite larger models, e.g., ViT-S and ViT-B, not having a significant accuracy difference, each of these models needs to be individually trained, tuned, and stored, which is not suitable for downstream scenarios where the hardware availability changes over time. (2) Although the configuration range covers different hardware requirements, the granularity is usually limited to a small selection of models and cannot cover all device constraints.

ObservationBy investigating the architecture of these configurations, we notice that ViT-Ti, ViT-S, and ViT-B share the same architecture, except they differ in the size of the embeddings and the corresponding number of attention heads they employ, having 3, 6, and 12 heads, respectively. In essence, this can be expressed as \(ViT_{T} ViT_{S} ViT_{B}\), see Table 1.

Research questionIn this paper, we address the following question: Can we train a universal ViT model with \(H\) attention heads and embedding dimension \(E\), such that by increasing the embedded dimension from \(e_{1}\) to \(e_{2}\), where \(e_{1}<e_{2} E\), and its corresponding number of heads from \(h_{1}\) to \(h_{2}\), where \(h_{1}<h_{2} H\), the model's accuracy gracefully improves?

ApproachIn this paper, we propose HydraViT, a stochastic training approach that extracts subsets of embeddings and their corresponding heads within MHA across a universal ViT architecture and jointly trains them. Specifically, during training, we utilize a uniform distribution to pick a value \(k\), where \(k H\). Subsequently, we extract the embedded dimension (\([0:k HeadDim]\)), where \(HeadDim\) is the size of each head, and its corresponding first \(k\) heads (\([0:k]\)) and only include these in both the backpropagation and forward paths of the training process. To enable the extraction of such subnetworks, we reimplement all components of the ViT including MHA, Multilayer Perceptron (MLP), and Normalization Layer (NORM), see Fig. 2. By using this stochastic approach, the heads will be stacked based on their importance, such that the first heads capture the most significant features and the last heads the least significant ones from the input image.

After the training phase is completed, during inference, HydraViT can dynamically select the number of heads based on the hardware demands. For example, if only \(p\%\) of the hardware is available, HydraViT extracts a subnetwork with the embedded size of \( p H HeadDim\) and the first \( p H\) heads and runs the inference. This flexibility is particularly advantageous in scenarios such as processing a sequence of input images, like a video stream, where latency is critical, especially on constrained devices such as mobile devices. In such environments, where various tasks are running simultaneously, and hardware availability dynamically fluctuates, or we need to meet a

    & **ViT-Ti** & **ViT-S** & **ViT-B** \\ 
**\# Layers** & 12 & 12 & 12 \\
**Dim** & 192 & 384 & 768 \\
**\# Heads** & 3 & 6 & 12 \\
**Dim per Head** & 64 & 64 & 64 \\
**\# Params** & 5.7 M & 22 M & 86 M \\   

Table 1: ViT Configurations

Figure 2: Architecture of HydraViT

deadline, the ability to adapt the model's configuration without loading a new model offers significant benefits.

**Contributions:**

1. We introduce HydraViT, a stochastic training method that extracts and jointly trains subnetworks inside the standard ViT architecture for scalable inference.
2. In a standard ViT architecture with \(H\) attention heads, HydraViT can induce \(H\) submodels within a universal model.
3. HydraViT outperforms its scalable baselines with up to 7 p.p. more accuracy at the same throughput and performance comparable to the respective standard models DeiT-tiny, DeiT-small, and DeiT-base, see Figure 1 for details.

## 2 Related Work

The original Vision Transformer (ViT) (Dosovitskiy et al., 2021) has become the default architecture of Transformer-based vision models. While many works improve upon the original implementation by changing the architecture or training process (Liu et al., 2022; Touvron et al., 2022; Wang et al., 2021), none of these works yield a scalable architecture and need multiple separate sets of weights to be able to deploy an efficient model on devices with various constraints.

For Convolutional Neural Networks (CNNs), Fang et al. (2018) create a scalable network by pruning unimportant filters and then repeatedly freezing the entire model, adding new filters, and fine-tuning. Thereby, they achieve a network that can be run with a flexible number of filters. Yu et al. (2018) achieve the same, but instead of freezing, they train a network for different layer widths at once. For Transformers, Chavan et al. (2022) use sparsity to efficiently search for a subnetwork but then require fine-tuning for every extracted subnetwork to acquire good accuracy.

Beyer et al. (2023) introduce a small change in the training process by feeding differently sized patches to the network. Thereby, they can reduce or increase the number of patches, affecting the speed and accuracy during inference. Other works use the importance of each patch to prune the least important patches during inference to achieve a dynamic ViT (Yin et al., 2022; Rao et al., 2021; Tang et al., 2022; Wang et al., 2021).

Matryoshka Representation Learning (Kusupati et al., 2022) and Ordered Representations with Nested Dropout (Rippel et al., 2014; Hojjat et al., 2023) are techniques to make the embedding dimension of Transformers flexible, i.e., create a Transformer, which can also run partially. Kudugunta et al. (2023) use Matryoshka Learning to make the hidden layer of the MLP in each Transformer block flexible. Hou et al. (2020) change the hidden layer of the MLP and the MHA but still use the original dimension between Transformer blocks and also between MHA and MLP. Salehi et al. (2023) make the entire embedding dimension in a Transformer block flexible. However, they rely on a few non-flexible blocks followed by a router that determines the embedding dimension for the flexible blocks, which adds complexity and hinders the ability to choose with which network width to run.

Figure 3: In this figure, we illustrate an example of how we extract a subnetwork with 4 heads in MHA with a total number of 6 heads. In HydraViT, with the stochastic dropout training, we order the attention heads in MHA and consequently their corresponding embedding vectors based on their importance.

Valipour et al. (2023) propose SortedNet that trains networks to be flexible in depth and width. However, they mainly focus on evaluating with CNNs on CIFAR10 (Krizhevsky et al., 2009) and Transformers on Natural Language Processing (NLP) tasks in contrast to us. Additionally, they keep the number of heads in MHA fixed at 12, whereas we show that reducing the number of heads coupled to the embedding dimension, i.e., the first 64 values of the embedding always belong to the first head in MHA, the second 64 values belong to the second head, and so on, removes inconsistencies in the scaling of the MHA and improves performance.

Motivated by these previous works, in HydraViT, we propose a flexible ViT in which we, unlike previous works, adjust every single layer, and except for one initial training run, there is no further fine-tuning required. Additionally, we show that reducing the number of heads coupled to the embedding dimension, a weighted subnetwork sampling distribution, and adding separate classifier heads improves the performance of subnetworks.

## 3 HydraViT

In this section, we introduce HydraViT, which builds upon the ViT architecture. We start by detailing how general ViTs function. Next, we explain how HydraViT can extract subnetworks within the MHA, NORM, and MLP layers. Finally, we describe the stochastic training regime used in HydraViT to simultaneously train a universal ViT architecture and all of its subnetworks.

Vision TransformerHydraViT is based on the ViT architecture (Dosovitskiy et al., 2021). We start by taking the input image \(x\) and breaking it down into \(P\) patches. Each patch is then embedded into a vector of size \(E\) using patch embedding, denoted as \(^{E}\). Positional encoding is subsequently applied to the embeddings. Following these preprocessing steps, it passes the embeddings through \(L\) blocks consisting of MHA with \(H\) heads denoted as \(^{H}\), NORM layer denoted as \(^{P}\), MLP denoted as \(^{E M E}\) to predict the class of the input image \(x\), where \(M\) is the dimension of the hidden layer of the MLP. With the model parameters \(\), we can formulate this architecture as follows:

\[V_{}(x;^{E};^{H};^{E M E}; ^{P})\] (1)

HydraViThydraViT is able to induce any subnetwork with \(k H\) heads within the standard architecture of ViT. To do so, HydraViT extracts the first \(k\) heads denoted as \(^{[0:k]}\), and the embeddings corresponding to these heads in MHA and NORM layers. Additionally, it extracts the initial \([ k]\) neurons from the first and last layers of the MLP, and the first \([ k]\) neurons from the hidden layer of MLP. Therefore, we can formulate the subnetwork extracted from Eq. 1 as follows:

\[V_{_{k}}(x;^{[0:( k)]};^{[0:k]}; ^{[0:( k)][0:( k)][0: ( k)]};^{[0:( k)]});k\{1,2, ,H\}\] (2)

Figure 4 illustrates how HydraViT extracts subnetworks within NORM and MLP layers. For simplicity, we demonstrate subnetworks with 3, 6, and 12 heads, representing configurations for

Figure 4: An illustration of subnetwork extraction within MLP and NORM layers, introduced in HydraViT. Fig. 3(a) demonstrates how HydraViT slices activations, denoted as \(A_{1}\) and \(A_{2}\), along with their respective weight matrices, denoted as \(W_{1}\) and \(W_{2}\), based on the number of utilized heads. Also, Fig. 3(b) shows how HydraViT applies normalization on the activation corresponding to the used heads. For simplicity, only subnetworks with 3, 6, and 12 heads, corresponding to ViT-Ti, ViT-S, and ViT-B respectively, are presented.

ViT-Ti, ViT-S, and ViT-B, respectively. Additionally, in Figure 3, we present an example of how HydraViT extracts a subset of heads and their corresponding embeddings in MHA layers. By designing HydraViT this way, we can deploy only a subnetwork, e.g., HydraViT with 6 heads, and still have the option at runtime to run with even fewer heads. It is not necessary to deploy HydraViT with all weights, which is necessary for deployment on more constrained devices.

Stochastic dropout trainingIdeally, to achieve a truly scalable model, we need to extract all the possible subnetworks, calculate their loss, sum them up, and minimize it. This yields the following multi-objective optimization problem:

\[_{[_{1}_{H}]}_{i=1}^{N}_{h=1}^{H}(V_{ _{h}}(x_{i}),y_{i})\] (3)

where N is the number of samples, \(x_{i}\) is the input and \(y_{i}\) is the ground truth. However, optimizing this multi-objective loss function has a complexity of \((N H)\) and requires at least \(H\) times more RAM compared to a single-objective loss function to store the gradient graphs, a demand that exceeds the capacity of a current GPU. To address this issue, we suggest employing stochastic training: On each iteration, instead of extracting all of the \(H\) possible subnetworks and optimizing a multi-objective loss function, we sample a value \(k\{1,2,,H\}\) based on a uniform discrete probability distribution \((k)\). Then we extract its respective subnetwork \(V_{_{k}}\), and minimize only this loss function, see Alg. 1. This approach decreases the complexity of Eq. 3 to \((N)\). In this training regime, the first parts of embeddings and their corresponding attention heads become more involved in the training process, while the later parts are less engaged. After training, due to this asymmetric training, which can also be seen as an order-aware biased version of dropout, the embedding values and their respective attention heads are ordered based on importance, see Fig. 5. Note that despite the similarity to dropout we do not need scaling during training as our training and inference phases are identical. Thereby, we can simplify the Eq. 3 as follows:

\[k(k);_{_{k}}_{i=1}^{N}(V_{_ {k}}(x_{i}),y_{i})\] (4)

Separate classifiersWe implement a mechanism to train separate classifier heads for each subnetwork. This adds a few parameters to the model, but only during training or when running the model in a dynamic mode, i.e., having the ability to freely choose for each input with how many heads to run the model. The advantage is that we do not need to find a shared classifier that can deal with the different amounts of features each subnetwork provides. However, if we fix the number of epochs, each classifier gets fewer gradient updates than the shared one, which is why we only use this when training HydraViT with 3 subnetworks.

Figure 5: Stochastic tail-drop training.

Subnetwork sampling functionWhen trying to train a single set of weights containing multiple subnetworks, we expect an accuracy drop compared to if each subnetwork had its own set of weights. While we mention that we use a uniform discrete probability distribution to sample subnetworks, we can also use a weighted distribution function. With weighted subnetwork sampling, we can guide the model to focus on certain submodels more than others. This is useful in a deployment scenario in which we have many devices with similar resources and want to maximize accuracy for them while maintaining good accuracy for other devices with different resources.

## 4 Evaluation

In this section, we evaluate the performance of HydraViT and compare it to the baselines introduced in Sec. 2. We assess all experiments and baselines on ImageNet-1K (Deng et al., 2009) at a resolution of \(224 224\). We implement on top of timm (Wightman, 2019) and train according to the procedure of Touvron et al. (2021) but without knowledge distillation. We use an NVIDIA A100 80GB PCIe to measure throughput. For RAM, we measure the model and forward pass usage with a batch size of 1. We also calculate GMACs with a batch size of 1, i.e., the GMACs needed to classify a single image.

For the experiments, we used an internal GPU cluster, and each epoch took around 15 minutes. During prototyping, we estimate that we performed an additional 50 runs with 300 epochs.

First, we show that we can attain one set of weights that achieves very similar results as the three separate DeiT models DeiT-tiny, DeiT-small, and DeiT-base (Touvron et al., 2021). Then, we look at how our design choices, i.e., changing the number of heads coupled to the embedding dimension, weighted subnetwork sampling, and adding separate classifiers for each subnetwork, impact the accuracy. Afterward, we compare HydraViT to the following three baselines:

* **MatFormer**Kudugunta et al. (2023) focus only on the hidden layer of the MLP to achieve a flexible Transformer and do not change the heads in MHA or the dimension of intermediate embeddings.
* **DynaBERT**Hou et al. (2020) adjust the heads in MHA in addition to the dimension of MLP and, as a result, make both flexible. However, the intermediate embedding dimension is the same as the original one in between each Transformer block and between MHA and MLP, which results in more parameters and MACs.
* **SortedNet**Valipour et al. (2023) change every single embedding, including the ones between MHA and MLP and between Transformer blocks. However, they keep the number of heads in MHA fixed, resulting in less information per head and introducing inconsistencies in the scaling of the heads in MHA.

  
**Weighted Sampling?** & **Separate Classifiers?** & **Epochs** & **Acc [\%]** & **Acc [\%]** & **Acc [\%]** \\  & & & **3 Heads** & **6 Heads** & **12 Heads** \\  \(\) & \(\) & 300 & 72.56 & 79.35 & 80.63 \\ \(\) & \(\) & 400 & 73.16 & 79.63 & 80.90 \\ \(\) & \(\) & 500 & 73.54 & 80.09 & 81.30 \\  \(\) & \(\) & 300 & 72.02 & 79.35 & 80.98 \\ \(\) & \(\) & 400 & 72.45 & 79.85 & 81.49 \\ \(\) & \(\) & 500 & 72.50 & 79.89 & 81.63 \\  \(\) & \(\) & 300 & 72.78 & 79.44 & 80.52 \\ \(\) & \(\) & 400 & 73.24 & 79.88 & 81.13 \\ \(\) & \(\) & 500 & 73.42 & 80.12 & 81.13 \\  \(\) & \(\) & 300 & 72.13 & 79.45 & 81.18 \\ \(\) & \(\) & 400 & 72.46 & 79.93 & 81.58 \\ \(\) & \(\) & 500 & 72.65 & 80.08 & 81.77 \\   & 72.2 & 79.9 & 81.8 \\   

Table 2: The accuracy of HydraViT with our different design choices. ”3 Heads” corresponds to a subnetwork that has the same architecture as DeiT-tiny, ”6 Heads” corresponds to DeiT-small, and ”12 Heads” corresponds to DeiT-base.

In contrast, instead of keeping the number of heads fixed, we change it coupled to the embedding dimension, such that each head gets the same amount of information as in the original ViT. We also evaluate adding separate classifiers and employing weighted subnetwork sampling during the training. Finally, we perform an attention analysis on our model to showcase the effect of adding heads in MHA.

### One set of weights is as good as three: Tiny, Small, and Base at once

For this experiment, we train HydraViT for 300, 400, and 500 epochs with a pre-trained DeiT-tiny checkpoint. We show how our design choices, i.e., changing the number of heads coupled to the embedding dimension, weighted subnetwork sampling, and adding separate heads for each subnetwork, impact accuracy. Table 2 shows each subnetwork's accuracy for all the combinations of our design choices. Note that subnetworks of HydraViT with 3 heads result in the same architecture as DeiT-tiny, subnetworks with 6 heads result in the same as DeiT-small, and subnetworks with 12 heads result in the same as DeiT-base.

To evaluate weighted subnetwork sampling, we show in Table 2 that with 25% weight for training the subnetwork with 3 heads, 30% weight for 6 heads, and 45% weight for 12 heads, we can achieve an improvement of 0.3 to nearly 0.6 p.p. for the subnetwork with 12 heads depending on the number of epochs compared to uniform subnetwork sampling. Meanwhile, we get a change of -0.2 to +0.2 p.p. for the subnetwork with 6 heads and a decrease of 0.5 to 1.0 p.p. for the subnetwork with 3 heads compared to uniform subnetwork sampling. Therefore, we can increase accuracy at 12 heads at the cost of an overall accuracy decrease. Keep in mind that removing only one head in the vanilla DeiT-base significantly drops its accuracy to less than 30%, whereas HydraViT achieves more than 72% at 3 heads and 79% at 6 heads and is therefore more versatile.

To evaluate separate classifiers for each subnetwork, we show in Table 2 that it helps, in some cases, improve each subnetwork's accuracy by up to 0.2 percentage points. But it can also reduce the overall accuracy because each classifier gets fewer gradient updates than a shared classifier.

Finally, we can combine weighted subnetwork sampling and separate classifiers to achieve a high 12-head accuracy, reaching up to 81.77% accuracy at 500 epochs while maintaining a good accuracy at 3 and 6 heads. We notice that compared to only weighted subnetwork sampling, all the accuracies are up to 0.15 p.p. higher. Due to starting with a pre-trained DeiT-tiny, the classifier for 3 heads needs fewer gradient updates, and the weighted subnetwork sampling shifts the gradient bias to the larger subnetworks, which leads to overall better accuracy, see Table 2.

To summarize, we show that with HydraViT, we can create one set of weights that achieves, on average, the same accuracy as the three separate models DeiT-tiny, Deit-small, and DeiT-base. To attain this one set of weights, we need at least 300 fewer training epochs than are necessary to train the three different DeiT models. The subnetworks have identical RAM usage, throughput, MACs, and model parameters compared to the DeiT models. While in this section, we investigated HydraViT with only 3 subnetworks, we evaluate HydraViT with more subnetworks in the next section.

### HydraViT vs. Baselines

For the next experiment, we train HydraViT and the baselines introduced at the beginning of this section for 300 epochs, once from scratch and once with DeiT-tiny as an initial checkpoint. While all of these baselines reduce the embedding dimension, the difference is they reduce it in different parts of the model. We choose 10 subnetworks for each model, setting the embedding dimension from 192 to 768 with steps of 64 in between. These steps correspond to having from 3 to 12 attention heads, with steps of 1 in between. While HydraViT supports up to 12 subnetworks, we choose to exclude the two smallest ones, as their accuracy drops too much.

Table 3 shows how each baseline compares to HydraViT relative to their RAM usage, GMACs, model parameters, and throughput when training from scratch and when starting with a pre-trained DeiT-tiny checkpoint. Figure 1 and Figure 6 show the results of all subnetworks when starting with a pre-trained DeiT-tiny checkpoint. Besides HydraViT, only SortedNet can run with less than 150 MB of RAM while achieving on average 0.3 to 0.7 p.p. worse accuracy than HydraViT. The other baselines, which have a more limited range of subnetworks, achieve a better accuracy when running at higher embedding dimensions. The limited range, however, has the downside of not having smaller subnetworks for devices with fewer resources. And if we limit HydraViT to a similar range as MatFormer, training on 9 to 12 heads, we show that HydraViT reaches the overall highest accuracy at 82.25% compared to MatFormer's 82.04%. We also notice that HydraViT cannot reach the exact same performance as the three DeiT models. This is because training for 10 subnetworks with a shared classifier for only 300 epochs has its toll on the overall performance. One option is to train longer, which we demonstrated for HydraViT with 3 subnetworks in Section 4.1. We repeat the same here and train HydraViT for 800 epochs, showing that even with 10 subnetworks, we can still achieve near-similar performance as the three different DeiT models. This is while having another 7 subnetworks with similar accuracy per resource trade-off points in between. One caveat, however, is that when training from scratch, HydraViT struggles to get a good accuracy at 3 heads. This is most likely due to a sampling bias as the subnetworks with one and two heads are not included in training and due to training hyperparameters as they differ when training DeiT-tiny compared to DeiT-base. For detailed results, e.g., each subnetwork for every baseline, See Table 4 in Appendix A.

    &  & **RAM** & **MACs** & **Params** & **Throughput** & **Acc [\%]** & **Acc [\%]** \\  & & [**MBM**] & [**G**] & **[M]** & **[**\#/\#]** & **from scratch** & **from DetF-tiny** \\   & 768 & 568.45 & 17.56 & 86.6 & 1728.3 & 81.89 & 82.04 \\  & 384 & 366.68 & 11.99 & 58.2 & 2231.6 & 81.52 & 81.80 \\  & 192 & 294.9 & 9.2 & 44.1 & 2601.4 & 79.40 & 80.48 \\   & 768 & 568.45 & 17.56 & 86.6 & 1725.7 & - & 81.30 \\  & 384 & 287.62 & 7.45 & 44.1 & 3014.6 & - & 80.16 \\  & 192 & 177.2 & 3.43 & 22.8 & 4944.5 & - & 73.00 \\   & 768 & 508.45 & 17.56 & 86.6 & 1753.0 & 79.71 & 80.80 \\  & 384 & 169.6 & 4.6 & 22.1 & 3874.8 & 77.79 & 78.94 \\  & 192 & 63.87 & 1.25 & 5.7 & 5898.2 & 66.64 & 70.20 \\   & 768 & 508.45 & 17.56 & 86.6 & 1754.1 & 80.45 & 81.10 \\  & 384 & 169.6 & 4.6 & 22.1 & 4603.6 & 78.40 & 79.28 \\  & 192 & 63.87 & 1.25 & 5.7 & 10017.6 & 67.34 & 70.56 \\   & 768 & 508.45 & 17.56 & 86.6 & 1754.1 & 81.93 & 81.60 \\  & 384 & 169.6 & 4.6 & 22.1 & 4603.6 & 79.84 & 80.15 \\  & 192 & 63.87 & 1.25 & 5.7 & 10017.6 & 68.78 & 71.67 \\   & 768 & 508.45 & 17.56 & 86.6 & 1754.1 & 81.56 & 82.25 \\  & 704 & 440.19 & 14.82 & 72.9 & 1916.1 & 81.55 & 82.22 \\  & 640 & 376.63 & 12.31 & 60.3 & 2242.9 & 81.51 & 82.21 \\  & 576 & 317.8 & 10.04 & 49.0 & 2385.2 & 81.21 & 81.92 \\   

Table 3: Comparison of HydraViT with the baselines MatFormer, DynaBERT, and SortedNet. The table shows for selected subnetworks the RAM usage, MACs, model parameters, throughput and their corresponding accuracy when trained from scratch (when applicable) and from the initial DeiT-tiny checkpoint. Note that DynaBERT relies on Knowledge Distillation in every block, which is why it reaches less than 1% accuracy when trained from scratch.

In summary, HydraViT achieves, on average, better accuracy than its baselines except for MatFormer within its limited scalability range. However, we show that training HydraViT on a similar scalability range outperforms MatFormer.

### Analyzing HydraViT's inner workings

Fig. 8 displays the attention relevance map (Chefer et al., 2021) of selected subnetworks of HydraViT, allowing us to visually investigate how the model's attention shifts when increasing the number of heads. Fig. 7(c) shows that fewer heads lead to more scattered attention, whereas increasing the number of heads makes the attention maps more compact and focused on the main object. Additionally, adding more heads enhances classification confidence. For instance, in Fig. 7(a), the model misclassifies the input with 3 heads, but as we add more heads, the classification gradually shifts to the correct label and increases in confidence. We also illustrate the t-SNE visualization of the final layer for different subnetworks, see Fig. 7. The figure shows that subnetworks with more heads exhibit a denser representation, while having fewer heads results in a more sparse representation. This indicates that increasing the number of heads enhances focus on the main object, which results in less entropy and, thereby, a more compact t-SNE representation. It is worth noting that the outliers in this figure occur due to the high norm values of the embeddings (Darcet et al., 2024).

### Robustness of HydraViT

To show the robustness of HydraViT we also evaluate on different ImageNet variants: ImageNet-v2 (Recht et al., 2019), ImageNet-R (Hendrycks et al., 2020), ImageNet-A (Hendrycks et al., 2019),

Figure 8: Attention relevance maps (Chefer et al., 2021) of 3 samples from ImageNet-1K for HydraViT with different number of heads. Increasing the number of heads leads to more confident classification and a more condensed attention distribution.

ImageNet-Sketch (Wang et al., 2019), and ImageNet-ReaL (Beyer et al., 2020; Russakovsky et al., 2015). On four of these five ImageNet variants, HydraViT achieves the overall best results. Figure 11 shows this for ImageNet-v2. Figure 11 shows the only ImageNet variant, i.e., ImageNet-R, where HydraViT trained with 9 to 12 heads is not able to achieve the best results. Nevertheless, HydraViT reaches a competitive accuracy on these difficult variants and has on average the best results. See Appendix F for full results.

### Limitations

**Training complexity** HydraViT optimizes 10 loss functions simultaneously, which increases the computational load on the optimization progress. As a result, we require more training iterations to achieve accuracy comparable to that of individually trained models such as DeiT-tiny, DeiT-small, and DeiT-base. However, by training multiple models within a unified framework, HydraViT ultimately requires much less total training time compared to training each of these 10 models for 300 epochs individually. See Appendix C for more details on training complexity.

**Evaluation on different hardware** Our main focus with HydraViT is on the efficiency and scalability on a single device, rather than the deployment on smaller hardware. However, metrics such as GMACs and params, are consistent across different platforms. Additionally, the skeleton of HydraViT is identical to DeiT, and others have evaluated the latency and performance metrics of DeiT on smaller devices. For instance, FastViT (Vasu et al., 2023) evaluates DeiT on the iPhone 12 Pro, MobileViT (Mehta and Rastegari, 2023) on the iPhone 12, SPViT (Kong et al., 2022) on the ZCU102 FPGA and Galaxy S20, and GhostNetV3 (Liu et al., 2024) on the Huawei Mate 40 Pro. These studies provide insight into the expected performance and latency of HydraViT on different hardware, indirectly supporting our claims about HydraViT's adaptability.

**Evaluation on other models** While HydraViT has been evaluated on DeiT-tiny, DeiT-small, and DeiT-base configurations, which have the same number of layers, we have not yet applied it to larger models like DeiT-large with more layers. We plan to explore this in future works.

## 5 Conclusion

We introduce HydraViT, a novel approach for achieving a scalable ViT architecture. By dynamically stacking attention heads and adjusting embedded dimensions within the MHA layer during training, HydraViT induces multiple subnetworks within a single model. This enables HydraViT to adapt to diverse hardware environments with varying resource constraints while maintaining strong performance. Our experiments on ImageNet-1K demonstrate that HydraViT achieves significant accuracy improvements compared to baseline approaches, with up to 5 percentage points higher accuracy at the same computational cost and up to 7 percentage points higher accuracy at the same throughput. This makes HydraViT a practical solution for real-world deployments where hardware availability is diverse or changes over time.