# Human Expertise in Algorithmic Prediction

Rohan Alur

EECS, LIDS

MIT

ralur@mit.edu

&Manish Raghavan

EECS, LIDS, Sloan

MIT

mragh@mit.edu

&Devavrat Shah

EECS, IDSS, LIDS, SDSC

MIT

devavrat@mit.edu

###### Abstract

We introduce a novel framework for incorporating human expertise into algorithmic predictions. Our approach leverages human judgment to distinguish inputs which are _algorithmically indistinguishable_, or "look the same" to predictive algorithms. We argue that this framing clarifies the problem of human-AI collaboration in prediction tasks, as experts often form judgments by drawing on information which is not encoded in an algorithm's training data. Algorithmic indistinguishability yields a natural test for assessing whether experts incorporate this kind of "side information", and further provides a simple but principled method for selectively incorporating human feedback into algorithmic predictions. We show that this method provably improves the performance of any feasible algorithmic predictor and precisely quantify this improvement. We find empirically that although algorithms often outperform their human counterparts _on average_, human judgment can improve algorithmic predictions on _specific_ instances (which can be identified ex-ante). In an X-ray classification task, we find that this subset constitutes nearly \(30\%\) of the patient population. Our approach provides a natural way of uncovering this heterogeneity and thus enabling effective human-AI collaboration.

## 1 Introduction

Despite remarkable advances in machine learning, human judgment continues to play a critical role in many high-stakes prediction tasks. For example, consider the problem of triage in the emergency room, where healthcare providers assess and prioritize patients for immediate care. On one hand, prognostic algorithms offer significant promise for improving triage decisions; indeed, algorithmic predictions are often more accurate than even expert human decision makers . On the other hand, predictive algorithms may fail to fully capture the relevant context for each individual. For example, an algorithmic risk score may only have access to tabular electronic health records or other structured data (e.g., medical imaging), while a physician has access to many additional modalities--not least of which is the ability to directly examine the patient!

These two observations--that algorithms often outperform humans, but humans often have access to a richer information set--are not in conflict with each other. Indeed,  find exactly this phenomenon in an analysis of emergency room triage decisions. This suggests that, even in settings where algorithms outperform humans, algorithms might still benefit from some form of human input. Ideally this collaboration will yield _human-AI complementarity_, in which a joint system outperforms either a human or algorithm working alone. Our work thus begins with the following question:

_When (and how) can human judgment improve the predictions of any learning algorithm?_

**Example: X-ray classification.** Consider the problem of diagnosing atelectasis (a partially or fully collapsed lung; we study this task in detail in Section 5). Today's state-of-the-art deep learning models can perform well on these kinds of classification tasks using only a patient's chest X-ray asinput [35; 59; 1]. We are interested in whether we can further improve these algorithmic predictions by incorporating a "second opinion" from a physician, particularly because the physician may have access to information (e.g., by directly observing the patient) which is not present in the X-ray.

A first heuristic, without making any assumptions about the available predictive models, is to ask whether a physician can distinguish patients whose imaging data are _identical_. For example, if a physician can correctly indicate that one patient is suffering from atelectasis while another is not--despite the patients having identical chest X-rays--the physician must have information that the X-ray does not capture. In principle, this could form the basis for a statistical test: we could ask whether the physician performs better than random in distinguishing a large number of such patients. If so, even a predictive algorithm which outperforms the physician might benefit from human input.

Of course, we are unlikely to find identical observations in continuous-valued and/or high-dimensional data (like X-rays). A natural relaxation is to instead consider observations which are sufficiently "similar", as suggested by . In this work we propose a more general notion of _algorithmic indistinguishability_, or coarser subsets of inputs in which no algorithm (in some rich, user-defined class) has significant predictive power. We show that these subsets can be discovered via a novel connection to _multicallibration_, and formally demonstrate that using human feedback to predict outcomes within these subsets can outperform any algorithmic predictor (in the same user-defined class). In addition to being tractable, this framework is relevant from a decision-theoretic perspective: although we've focused thus far on algorithms' fundamental informational constraints, it is also natural to ask whether an expert provides signal which is merely _difficult_ for an algorithm to learn directly (due to e.g., limited training data or computational constraints). Our approach naturally interpolates between these contexts by defining indistinguishability with respect to whichever class of models is practically relevant for a given prediction task. We elaborate on these contributions below.

**Contributions.** We propose a novel framework for human-AI collaboration in prediction tasks. Our approach uses human feedback to refine predictions within sets of inputs which are _algorithmically indistinguishable_, or "look the same" to predictive algorithms. In Section 4 we present a simple method to incorporate this feedback only when it improves on the best feasible predictive model (and precisely quantify this improvement). This extends the "omnipredictors" result of  in the special case of squared error, which may be of independent interest.1 In Section 5 we present experiments demonstrating that although humans fail to outperform algorithmic predictors _on average_, there exist _specific_ (algorithmically indistinguishable) instances on which humans are more accurate than the best available predictor (and these instances are identifiable ex ante).2 In Section 6 we consider the complementary setting in which an algorithm provides recommendations to many downstream users, who independently choose when to comply. We provide conditions under which a predictor is robust to these compliance patterns, and thus be simultaneously optimal for all downstream users.

## 2 Related work

**The relative strengths of humans and algorithms.** Our work is motivated by large body of literature which studies the relative strengths of human judgment and algorithmic decision making [13; 17; 30; 44] or identifies behavioral biases in decision making [66; 11; 4; 60]. More recent work also studies whether predictive algorithms can _improve_ expert decision making [41; 53; 7; 1].

**Recommendations, deferral and complementarity.** One popular approach for incorporating human judgment into algorithmic predictions is by _deferring_ some instances to a human decision maker [49; 58; 52; 37; 55; 38]. Other work studies contexts where human decision makers are free to override algorithmic recommendations [20; 8; 14; 22; 1], which may suggest alternative design criteria for these algorithms [5; 9; 34]. More generally, systems which achieve human-AI _complementarity_ (as defined in Section 1) have been previously studied in [2; 5; 69; 23; 64; 19; 47].

 develop a comprehensive taxonomy of this area, which generally takes the predictor as given, or learns a predictor which is optimized to complement a particular model of human decision making. In contrast, we give stronger results which demonstrate when human judgment can improve the performance of any model in a rich class of possible predictors (Section 4), or when a single algorithm can complement many heterogeneous users (Section 6).

**Performance prediction.** A recent line of work studies _performance prediction_, or settings in which predictions influence future outcomes. For example, predicting the risk of adverse health outcomes may directly inform treatment decisions, which in turn affects future health outcomes. This can complicate the design and evaluation of predictive algorithms, and there is a growing literature which seeks to address these challenges [10; 51; 31; 21; 36; 39; 50; 67; 32; 70; 48; 54; 56]. Performativity is also closely related to the _selective labels problem_, in which some historical outcomes are unobserved as a consequence of past human decisions . Though these issues arise in many canonical human-AI collaboration tasks, we focus on standard supervised learning problems in which predictions do not causally affect the outcome of interest. These include e.g., weather prediction, stock price forecasting and many medical diagnosis tasks, including the X-ray diagnosis task we study in Section 5. In particular, although a physician's diagnosis may inform subsequent treatment decisions, it does not affect the contemporaneous presence or absence of a disease. More generally, our work can be applied to any "prediction policy problem", where accurate predictions can be translated into policy gains without explicitly modeling causality .

**Algorithmic monoculture.** Our results can be viewed as one approach to mitigating _algorithmic monoculture_, in which different algorithms make similar decisions and thus similar mistakes [43; 65]. This could occur because these systems are trained on similar datasets, or because they share similar inductive biases. We argue that these are precisely the settings in which a "diversifying" human opinion may be especially valuable. We find empirical evidence for this in Section 5: on instances where multiple models agree on a prediction, human judgment adds substantial predictive value.

**Multicalibration, omnipredictors and boosting.** Our results make use of tools from theoretical computer science, particularly work on _omnipredictors_ and its connections to _multicalibration_.  show that multicalibration is tightly connected to a cryptographic notion of indistinguishability, which serves as conceptual inspiration for our work. Finally,  provide an elegant boosting algorithm for learning multicalibrated partitions that we make use of in our experiments, and  provide results which reveal tight connections between a related notion of "swap agnostic learning", multi-group fairness, omniprediction and outcome indistinguishability.

## 3 Methodology and preliminaries

**Notation.** Let \(X\) be a random variable denoting the inputs (or "features") which are available for making algorithmic predictions about an outcome \(Y\). Let \(\) be an expert's prediction of \(Y\), and let \(x,y,\) denote realizations of the corresponding random variables. Our approach is parameterized by a class of predictors \(\), which is some set of functions mapping \(\) to \(\). We interpret \(\) as the class of predictive models which are relevant (or feasible to implement) for a given prediction task; we discuss this choice further below. Broadly, we are interested in whether the expert prediction \(\) provides a predictive signal which cannot be extracted from \(X\) by any \(f\).

**Choice of model class \(\).** For now we place no restrictions on \(\), but it's helpful to consider a concrete model class (e.g., a specific neural network architecture) from which, given some training data, one could derive a _particular_ model (e.g., via empirical risk minimization over \(\)). The choice of \(\) could be guided by practical considerations; some domains might require interpretable models (e.g., linear functions) or be subject to computational constraints. We might also simply believe that a certain architecture or functional form is well suited to the task of interest. In any case, we are interested in whether human judgment can provide information which is not conveyed by any model in this class, but are agnostic as to _how_ this is accomplished: an expert may have information which is not encoded in \(X\), or be deploying a decision rule which is not in \(\) -- or both!

Another choice is to take \(\) to model more abstract limitations on the expert's cognitive process. In particular, to model experts who are subject to "bounded rationality" [63; 40], \(\) might be the set of functions which can be efficiently computed (e.g., by a circuit of limited complexity). In this case, an expert who provides a prediction which cannot be modeled by any \(f\) must have access to _information_ which is not present in the training data. We take the choice of \(\) as given, but emphasize that these two approaches yield qualitatively different insight about human expertise.

**Indistinguishability with respect to \(\).** Our approach will be to use human input to distinguish observations which are _indistinguishable_ to any predictor \(f\). We formalize this notion of indistinguishability as follows:

**Definition 3.1** (\(\)-Indistinguishable subset).: For some \( 0\), a set \(S\) is \(\)-indistinguishable with respect to a function class \(\) and target \(Y\) if, for all \(f\),

\[|(f(X),Y X S)|\] (1)

To interpret this definition, observe that the subset \(S\) can be viewed as generalizing the intuition given in Section 1 for grouping identical inputs. In particular, rather than requiring that all \(x S\) are exactly equal, Definition 3.1 requires that all members of \(S\) effectively "look the same" for the purposes of making algorithmic predictions about \(Y\), as every \(f\) is only weakly related to the outcome within \(S\). We now adopt the definition of a multicalibrated partition  as follows:

**Definition 3.2** (\(\)-Multicalibrated partition).: For \(K 1\), \(S_{1} S_{K}\) is an \(\)-multicalibrated partition with respect to \(\) and \(Y\) if (1) \(S_{1} S_{K}\) partitions \(\) and (2) each \(S_{k}\) is \(\)-indistinguishable with respect to \(\) and \(Y\).3

Intuitively, the partition \(\{S_{k}\}_{k[K]}\) "extract[s] all the predictive power" from \(\); within each element of the partition, every \(f\) is only weakly related to the outcome \(Y\). Thus, while knowing that an input \(x\) lies in subset \(S_{k}\) may be highly informative for predicting \(Y\) -- for example, it may be that \([Y X=x][Y X=x^{}]\) for all \(x,x^{} S_{k}\) -- no predictor \(f\) provides significant _additional_ signal within \(S_{k}\). We provide a stylized example of such partitions in Figure 1 below.

It's not obvious that such partitions are feasible to compute, or even that they should exist. We'll show in Appendix B however that a multicalibrated partition can be efficiently computed for many natural classes of functions. Where the relevant partition is clear from context, we use \(_{k}[],_{k}(),_{k}(,)\) to denote expectation, variance and covariance conditional on the event that \(\{X S_{k}\}\). For a subset \(S\), we use \(_{S}[],_{S}()\) and \(_{S}(,)\) analogously.

**Incorporating human judgment into predictions.** To incorporate human judgment into predictions, a natural heuristic is to first test whether the conditional covariance \(_{k}(Y,)\) is nonzero within some indistinguishable subset. Intuitively, this indicates that the expert prediction is informative even though every model \(f\) is not. This suggests a simple method for incorporating human expertise: first, learn a partition which is multicalibrated with respect to \(\), and then use \(\) to predict \(Y\) within each indistinguishable subset. We describe this procedure in Algorithm 1 below, where we define a univariate learning algorithm \(\) as a procedure which takes one or more \((_{i},y_{i})^{2}\) training observations and outputs a function which predicts \(Y\) using \(\). For example, \(\) might be an algorithm which fits a univariate linear or logistic regression which predicts \(Y\) as a function of \(\).

Algorithm 1 simply learns a different predictor of \(Y\) as a function of \(\) within each indistinguishable subset. As we show below, even simple instantiations of this approach can outperform the squared error achieved by _any_\(f\). This approach can also be readily extended to more complicated forms of human input (e.g., freeform text, which can be represented as a high-dimensional vector rather than a point prediction \(\)), and can be used to _test_ whether human judgment provides information that an algorithm cannot learn from the available training data. We turn to these results below.

Figure 1: Partitions which are approximately multicalibrated with respect to the class of hyperplane classifiers (we consider the empirical distribution placing equal probability on each observation). In both panels, no hyperplane classifier has significant discriminatory power within each subset.

## 4 Technical results

In this section we present our main technical results. For clarity, all results in this section are presented in terms of population quantities, and assume oracle access to a multicalibrated partition. We present corresponding generalization arguments and background on learning multicalibrated partitions in Appendices A and B, respectively. All proofs are deferred to Appendix C.

**Theorem 4.1**.: _Let \(\{S_{k}\}_{k[K]}\) be an \(\)-multicalibrated partition with respect to a model class \(\) and target \(Y\). Let the random variable \(J(X)[K]\) be such that \(J(X)=k\) iff \(X S_{k}\). Define \(^{*},^{*}^{K}\) as_

\[^{*},^{*}*{arg\,min}_{^{K}, ^{K}}\;[(Y-_{J(X)}+_{J(X)} )^{2}].\] (2)

_Then, for any \(f\) and \(k[K]\),_

\[_{k}[(Y-_{k}^{*}-_{k}^{*})^{2} ]+4_{k}(Y,)^{2}_{k}[(Y-f(X) )^{2}]+2.\] (3)

That is, the squared error incurred by the univariate linear regression of \(Y\) on \(\) within each indistinguishable subset outperforms that of any \(f\). This improvement is at least \(4_{k}(Y,)^{2}\), up to an additive approximation error \(2\). We emphasize that \(\) is an arbitrary class, and may include complex, nonlinear predictors. Nonetheless, given a multicalibrated partition, a simple linear predictor can improve on the _best_\(f\). Furthermore, this approach allows us to _selectively_ incorporate human feedback: whenever \(_{k}(Y,)=0\), we recover a coefficient \(_{k}^{*}\) of 0.4

**Nonlinear functions and high-dimensional feedback.** Theorem 4.1 corresponds to instantiating Algorithm 1 with a univariate linear regression, but the same insight generalizes readily to other functional forms. For example, if \(Y\) is binary, it might be desirable to instead fit a logistic regression. We provide a similar guarantee for generic nonlinear predictors via Corollary A.1 in Appendix A. Furthermore, while the results above assume that an expert provides a prediction \(\), the same insight extends to richer forms of feedback. For example, in a medical diagnosis task, a physician might produce free-form clinical notes which contain information that is not available in tabular electronic health records. Incorporating this kind of feedback requires a learning algorithm better suited to high-dimensional inputs (e.g., a deep neural network), which motivates our following result.

**Corollary 4.2**.: _Let \(S\) be an \(\)-indistinguishable subset with respect to a model class \(\) and target \(Y\). Let \(H\) denote expert feedback which takes values in some arbitrary domain (e.g., freeform text, which might be tokenized to take values in \(^{d}\) for some \(d>0\)), and let \(g:\) be a function which satisfies the following approximate calibration condition for some \( 0\) and for all \(,\):_

\[_{S}[(Y-g(H))^{2}]_{S}[(Y-- g(H))^{2}]+.\] (4)

_Then, for any \(f\),_\[_{S}[(Y-g(H))^{2}]+4_{S}(Y,g(H))^{2} _{S}[(Y-f(X))^{2}]+2+.\] (5)

To interpret this result, notice that (4) requires only that the prediction \(g(H)\) cannot be significantly improved by any linear post-processing function. For example, this condition is satisfied by any calibrated predictor \(g(H)\).5 Furthermore, any \(g(H)\) which does not satisfy (4) can be transformed by letting \((H)=_{,}[(Y-- g(H))^{2}]\); i.e., by linearly regressing \(Y\) on \(g(H)\), in which case \((H)\) satisfies (4). This result mirrors Theorem 4.1: a predictor which depends only on human feedback \(H\) can improve on the best \(f\) within each element of a multicalibrated partition.

**Testing for informative experts.** While we have thus far focused on incorporating human judgment to improve predictions, we may also be interested in the related question of simply _testing_ whether human judgment provides information that cannot be conveyed by any algorithmic predictor. For example, such a test might be valuable in deciding whether to automate a given prediction task.

Theorem 4.1 suggests a heuristic for such a test: if the conditional covariance \(_{k}(Y,)\) is large, then we might expect that \(\) is somehow "more informative" than any \(f\) within \(S_{k}\). While covariance only measures a certain form of _linear_ dependence between random variables, we now show that, in the special case of binary-valued algorithmic predictors, computing the covariance of \(Y\) and \(\) within an indistinguishable subset serves as a stronger test for whether \(\) provides _any_ predictive information which cannot be expressed by the class \(\).

**Theorem 4.3**.: _Let \(\{S_{k}\}_{k[K]}\) be an \(\)-multicalibrated partition for a binary-valued model class \(^{binary}\) and target outcome \(Y\). For all \(k[K]\), let there be \(_{k}\) such that \(Y\!\!\!_{k}(X),X S_{k}\). Then, for all \(k[K]\),_

\[|_{k}(Y,)|}.\] (6)

That is, if each indistinguishable subset has a corresponding predictor \(_{k}\) which "explains" the signal provided by the human, then the covariance of \(Y\) and \(\) is bounded within every \(S_{k}\). The contrapositive implies that a sufficiently large value of \(_{k}(Y,)\) serves as a certificate for the property that _no_\(f\) can fully explain the information that \(\) provides about \(Y\) within each indistinguishable subset. This can be viewed as a finer-grained extension of the test proposed in .

Taken together, our results demonstrate that algorithmic indistinguishability provides a principled way of reasoning about the complementary value of human judgment. Furthermore, this approach yields a concrete methodology for incorporating this expertise: we can simply use human feedback to predict \(Y\) within subsets which are indistinguishable on the basis of \(X\) alone. Operationalizing these results depends critically on the ability to _learn_ multicalibrated partitions, e.g., via the boosting algorithm proposed in . We provide additional detail on learning such partitions in Appendix B.

## 5 Experiments

### Chest X-ray interpretation

We now instantiate our framework in the context of the chest X-ray classification task outlined in Section 1. We consider the eight predictive models studied in , which were selected from the leaderboard of a large public competition for X-ray image classification. These models serve as a natural benchmark class \(\), against which we investigate whether radiologist assessments provide additional predictive value. These models were trained on a dataset of 224,316 chest radiographs collected across 65,240 patients , and then evaluated on a holdout set of \(500\) randomly sampled radiographs. This holdout set was annotated by eight radiologists for the presence (\(Y=1\)) or absence (\(Y=0\)) of five selected pathologies; the majority vote of five radiologists serves as a ground truth label, while the remaining three are held out to assess the accuracy of individual radiologists .

In this section we focus on diagnosing atelectasis (a partial or complete collapse of the lung); we provide results for the other four pathologies in Appendix G. We first show, consistent with , that radiologists fail to consistently outperform algorithmic classifiers _on average_. However, we then demonstrate that radiologists do outperform all eight leaderboard algorithms on a large subset (nearly \(30\%\) of patients) which is indistinguishable with respect to this class of benchmark predictors. Because radiologists in this experimental setting only have access to the patient's chest X-ray, and because we do not apply any postprocessing to the radiologist assessments (i.e., \(_{k}\), as defined in Algorithm 1, is simply the identity function, which is most natural when \(Y\) and \(\) are binary), we interpret these results as providing a lower bound on the improvement that radiologists can provide relative to relying solely on algorithmic classifiers.

**Algorithms are competitive with expert radiologists.** We first compare the performance of the three benchmark radiologists to that of the eight leaderboard algorithms in Figure 2. Following , we use the Matthew's Correlation Coefficient (MCC) as a standard measure of binary classification accuracy . The MCC is simply the rescaled covariance between each prediction and the outcome, which corresponds directly to Definition 3.1. In Figure 2 we see that radiologist performance is statistically indistinguishable from that of the algorithmic classifiers.

**Radiologists can refine algorithmic predictions.** We now apply the results of Section 4 to investigate _heterogeneity_ in the relative performance of humans and algorithms. First, we partition the patients into a pair of approximately indistinguishable subsets, which are exceptionally straightforward to compute when the class \(\) has a finite number of predictors (we provide additional detail in Appendix F). We plot the conditional performance of both the radiologists and the eight leaderboard algorithms within each of these subsets in Figure 3.

While Figure 2 found no significant differences between radiologists' and algorithms' _overall_ performance, Figure 3 reveals a large subset -- subset \(0\), consisting of \(29.6\%\) of our sample -- where radiologists achieve a better MCC than every algorithm. In particular, every algorithm predicts a positive label for every patient in this subset, and radiologists identify a sizable fraction of true negatives that the algorithms miss. For example, radiologist \(1\) achieves a true positive rate of \(84.0\%\) and a true negative rate of \(42.9\%\), while the algorithms achieve corresponding rates of \(100\%\) and \(0\%\).

This partition is not necessarily unique, and in principle an analyst could compare the performance of radiologists and algorithms across different subsets which could yield an even starker difference in conditional performance. However, even for discrete-valued data, searching over all possible subsets is computationally and statistically intractable; instead, our approach provides a principled way of identifying the _particular_ subsets in which human judgment is likely to add predictive value.

**Other pathologies.** Although we focus here on atelectasis, and the findings above are consistent for two of the other four pathologies considered in  (pleural effusion and consolidation): although radiologists fail to outperform algorithms _on average_, at least two of the radiologists outperform algorithmic predictions on a sizable minority of patients. Results for cardiomegaly and edema appear qualitatively similar, but we lack statistical power. We present these results in Appendix G.

Figure 2: The relative performance of radiologists and predictive algorithms for detecting atelectasis. Each bar plots the Matthews Correlation Coefficient between the corresponding prediction and the ground truth label. Point estimates are reported with \(95\%\) bootstrap confidence intervals.

### Prediction of success in human collaboration

We next consider the visual prediction task studied in . In this work, the authors curate photos taken of participants after they attempt an Escape the Room' puzzle--"a physical adventure game in which a group is tasked with escaping a maze by collectively solving a series of puzzles" . A separate set of subjects are then asked to predict whether the group in each photo succeeded in completing the puzzle. Subjects in the control arm perform this task without any form of training, while subjects in the remaining arms are first provided with four, eight and twelve labeled examples, respectively. Their performance is compared to that of five algorithmic predictors, which use \(33\) high-level features extracted from each photo (e.g., number of people, gender and ethnic diversity, age distribution etc.) to make a competing prediction. We provide a full list of features in Appendix H.

**Accuracy and indistinguishability in visual prediction.** As in the X-ray diagnosis task, we first compare the performance of human subjects to that of the five off-the-shelf predictive algorithms considered in . We again find that although humans fail to outperform the best predictive algorithms, their predictions add significant predictive value on instances where the algorithms agree on a positive label. As our results are similar to those in the previous section, we defer them to Appendix I. We now use this task to illustrate another feature of our framework, which is the ability to incorporate human judgment into a substantially richer class of models.

**Multicalibration over an infinite class.** While our previous results illustrate that human judgment can complement a small, fixed set of predictive algorithms, it's possible that a richer class could obviate the need for human expertise. To explore this, we now consider an infinitely large but nonetheless simple class of shallow (depth \( 5\)) regression trees. We denote this class by \(^{}\).

As in previous sections, our first step will be to learn a partition which is multicalibrated with respect to \(^{}\). However, because \(^{}\) is infinitely large, enumerating each \(f^{}\) and clustering observations according to their predictions is infeasible. Instead, we apply the boosting algorithm proposed in  to construct a predictor \(h:\) such that no \(f^{}\) can substantially improve on the squared error of \(h\) within any of its approximate level sets \(\{x h(x)=0\},\{x h(x)(0,.1]\{x h(x)[.9,1]\}\).6 We plot the correlation of the human subjects' predictions with the true label within these level sets in Figure 4.

Figure 4 highlights a key insight provided by our framework. On one hand, the predictions made by \(h\) are more accurate out of sample (\(72.2\%\)) than even the best performing cohort (\(67.3\%\)). Nonetheless, the predictions made by all four cohorts of human subjects are predictive of the outcome within every

Figure 3: Conditional performance for atelectasis. Within subset \(0\) (\(n\) = \(148\)), all algorithms predict \(Y\)=1, thus achieving true positive rate (TPR) \(1\), true negative rate (TNR) \(0\), and an MCC of \(0\). Radiologists achieve a corresponding (TPR, TNR) of \((84.0\%,42.9\%)\), \((72.6\%,47.6\%)\) and \((93.4\%,19.0\%)\), respectively. Subset \(1\) (\(n\) = \(352\)) contains the remaining patients. The baseline is a random permutation of the labels. Confidence intervals for algorithmic performance are not strictly valid (subsets are chosen conditional on the predictions), but are included for reference. All else is as in Figure 2.

nonempty level set of of \(h\).7 This suggests that humans provide information which cannot be extracted from the data by _any_\(f^{}\). While we focus on shallow regression trees for concreteness, this approach extends to any function class for which it is feasible to learn a multicalibrated partition.

## 6 Robustness to noncompliance

We have thus far focused on how an algorithm might incorporate human feedback to improve prediction accuracy, or how an algorithmic decision pipeline might selectively defer to a human expert. However, many decision support tools are deployed in the opposite setting where the _user_ instead decides when to defer to _the algorithm_. For example, physicians with access to a prognostic risk score may choose to simply ignore the risk score at their discretion. Furthermore, it is common for hospitals to employ different norms and policies governing the use of algorithmic predictors . Thus, although it is tempting to simply provide all downstream users with the single "best" risk score, such an approach can be suboptimal if users vary in their compliance behavior . We illustrate the challenges of this kind of heterogeneous user behavior via the following stylized example.

**Example: the challenge of noncompliance.** Consider a generic prognostic risk score which makes recommendations regarding patient care. Although physicians generally comply with the algorithm's recommendations, they are free to override it as they see fit. For example, suppose that one particular physician believes (correctly or not) that the algorithm underweights high blood pressure as a risk factor, and thus ignores its recommendations for these patients. A second physician similarly ignores algorithmic recommendations and instead exercises their own judgment for patients 65 and older.

What does an optimal risk score look like in this setting? For the first physician, we would like to select the algorithm which minimizes error on patients who do not have high blood pressure, as these are the patients for whom the physician uses the algorithm's recommendations. Similarly, for the second physician, we would like to minimize error on patients who are under 65. Of course, _there is no guarantee that these are the same algorithm_: empirical risk minimization over the first population will, in general, produce a different predictor than empirical risk minimization over the second population. This is not just a finite sample problem; given any restricted model class (e.g., linear predictors), the optimal predictor for one subpopulation may not be optimal for a different subpopulation. For both practical and ethical reasons however, we cannot design individualized predictors for every physician; we would like to instead provide a risk score which is simultaneously "optimal" (in a sense we make precise below) for every user.

**Noncompliance-robust prediction.** In Appendix A.3, we show that, without further assumptions, the setting described above poses a statistically intractable problem: if physicians choose whether

Figure 4: Human performance within the approximate level sets of a predictor \(h\) which is multicalibrated over \(^{}\). Level sets \(0,1,\) and \(10\) are the sets \(\{x h(x)=0\}\), \(\{x h(x)(0,.1]\}\), and \(\{x h(x)[.9,1]\}\), and contain \(259,309\) and \(292\) observations, respectively. All other level sets are empty in our test set. A random permutation of the labels is included as a baseline.

to comply in arbitrary ways, then we need a predictor which is simultaneously optimal for every possible patient subpopulation. The only predictor which satisfies this criterion is the Bayes optimal predictor, which is infeasible to learn in a finite data regime.

However, suppose instead that physicians decide whether to defer to the algorithm using relatively simple heuristics. If we believe we can model these heuristics as a "simple" function of observable patient characteristics -- e.g., that all compliance patterns can be expressed as a shallow decision tree, even if particular compliance behavior varies across physicians -- then we can leverage this structure to design a single optimal predictor. In particular, we show next that, given a partition which is multiclibrated over the class of possible user compliance patterns, we can learn predictors which remain optimal even when users only selectively adopt the algorithm's recommendations.

**Theorem 6.1**.: _Let \(\) be a class of binary compliance policies, where, for \(\), \((x)=1\) indicates that the user compiles with the algorithm at \(X=x\). Let \(\) be a class of predictors and let \(\{S_{k}\}_{k[K]}\) be a partition which is \(\)-multiclibrated with respect to \(\) and the product class \(\{f(X)(X) f,\}\). Then, \(\ f,,k[K]\),_

\[_{k}[(Y-_{k}[Y])^{2}(X)=1]_{k}[(Y-f (X))^{2}(X)=1]+_{k}((X)=1)}.\] (7)

That is, given an appropriately multiclibrated partition, we can derive a predictor which is simultaneously near-optimal for every downstream user. In particular, observe that the left hand side of (7) is the squared error incurred by the constant prediction \(_{k}[Y]\) within \(S_{k}\)_when the user defers to the algorithm_. Although this prediction does not depend on the policy \(\), it remains competitive with the squared error incurred by any \(f\) for _any policy_. Unsurprisingly, the bound becomes vacuous as \(_{k}((X)=1)\) goes to \(0\) (we cannot hope to learn anything on arbitrarily rare subsets). This is consistent with our interpretation of \(\) however, as the performance of the algorithm matters little if the decision maker ignores nearly all recommendations.

This result is complementary to those in Section 4--rather than learning to incorporate feedback from a single expert, we can instead learn a single predictor which is (nearly) optimal for a rich class of downstream users whose behavior is modeled by some \(\).

## 7 Discussion and limitations

In this work we propose an indistinguishability-based framework for human-AI collaboration. Under this framework, we develop a set of methods for testing whether experts provide a predictive signal which cannot be replicated by an algorithmic predictor, and extend our results to settings in which users selectively adopt algorithmic recommendations. Beyond these methodological contributions, we argue that our framing clarifies _when_ and _why_ human judgment can improve algorithmic performance. In particular, a primary theme in our work is that even if humans do not consistently outperform algorithms on average, _selectively_ incorporating human judgment can often improve predictions.

A key limitation of our work is a somewhat narrow focus on minimizing a well-defined loss function over a well-defined (and stationary) distribution. This fails to capture decision makers with richer, multidimensional preferences (e.g., fairness, robustness or simplicity), and does not extend to settings in which predictions _influence_ future outcomes (see the discussion of performative prediction in Section 2) or the distribution otherwise changes over time. However, we view indistinguishability as a powerful primitive for modeling these more complex scenarios; for example, a decision maker might impose additional preferences -- like a desire for some notion of fairness -- to distinguish inputs which are otherwise indistinguishable with respect to the "primary" outcome of interest. At a technical level, our results rely on the ability to efficiently _learn_ multiclibrated partitions. While we give conditions under which this is feasible in Appendix B and a finite sample analysis in Appendix A, finding such partitions can be challenging for rich function classes.

Finally, we caution that even in contexts which fit neatly into our framework, human decision makers can be critical for ensuring interpretability and accountability. Thus, although our approach can provide guidance for choosing the appropriate level of automation, it does not address the practical or ethical concerns which arise. Despite these limitations, we argue that indistinguishability helps to clarify the role of human expertise in algorithmic decision making, and this framing in turn provides fundamental conceptual and methodological insights for enabling effective human-AI collaboration.