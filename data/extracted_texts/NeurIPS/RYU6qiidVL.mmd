# \(D^{3}\): Detoxing Deep Learning Dataset

Lu Yan, Siyuan Cheng, Guangyu Shen, Guanhong Tao, Xuan Chen, Kaiyuan Zhang, Yunshu Mao,

and Xiangyu Zhang

Purdue University

###### Abstract

Data poisoning is a prominent threat to Deep Learning applications. In backdoor attack, training samples are poisoned with a specific input pattern or transformation called trigger such that the trained model misclassifies in the presence of trigger. Despite a broad spectrum of defense techniques against data poisoning and backdoor attacks, these defenses are often outpaced by the increasing complexity and sophistication of attacks. In response to this growing threat, this paper introduces \(D^{3}\), a novel dataset detoxification technique that leverages differential analysis methodology to extract triggers from compromised test samples captured in the wild. Specifically, we formulate the challenge of poison extraction as a constrained optimization problem and use iterative gradient descent with semantic restrictions. Upon successful extraction, \(D^{3}\) enhances the dataset by incorporating the poison into clean validation samples and builds a classifier to separate clean and poisoned training samples. This post-mortem approach provides a robust complement to existing defenses, particularly when they fail to detect complex, stealthy poisoning attacks. \(D^{3}\) is evaluated on 42 poisoned datasets with 18 different types of poisons, including the subtle clean-label poisoning, dynamic attack, and input-aware attack. It achieves over 95% precision and 95% recall on average, substantially outperforming the state-of-the-art.

## 1 Introduction

A prominent threat for Deep Learning applications is data poisoning, in which adversaries inject poisoned samples into datasets such that models trained from such datasets have (hidden) malicious behaviors Gu et al. (2019); Liu et al. (2020); Nguyen and Tran (2021). For example, the simplest data poisoning Gu et al. (2019) works by stamping some pixel pattern called _trigger_ on a set of clean samples and setting their labels to a _target class_. The model hence learns the malicious connection between the trigger and the target class such that misclassification can be induced at test time by stamping a clean sample with the trigger. This is called the _backdoor attack_ or _trojan attack_.

There are a spectrum of defense techniques against data poisoning and backdoor attacks, such as backdoor scanning Kolouri et al. (2020); Zhang et al. (2020); Guo et al. (2020); Huang et al. (2019); Veldanda et al., test-time poisoned input detection Chou et al. (2020); Doan et al. (2020); Gao et al. (2019); Li et al. (2021); model certification against data poisoning McCoyd et al. (2020); Xiang et al. (2021, 2021); Jia et al. (2020), poison removal by model retraining Li et al. (2021); Wu and Wang (2021); Tao et al. (2022); and data detoxing Hayase and Kong (2020); Du et al. (2019); Chen et al. (2018); Tran et al. (2018); Shan et al. (2022). Data detoxing focuses on removing poisons in data samples (e.g., those in the training set). For instance, TRACEBACK Shan et al. (2022) was the first post-mortem data detoxing technique. It assumed the access to a few poisoned samples and then cleansed the dataset based on the forensic results of the samples. The few poisoned samples can be acquired by collecting misclassified samples that are not human explainable. For example, anairplane image (in human eyes) misclassified as a cat is considered highly suspicious. In contrast, a dog misclassified as a cat may not be, as these two are not that distinguishable to begin with1.

In traditional cyber-security, it was shown that learning from incidents is critical for enhancing security measures Ma et al. (2017); Hassan et al. (2020); Chen et al. (2021); Yu et al. (2021); Hassan et al. (2019). This involves tracing the source of a cyberattack that has occurred, by examining the traces left by the attacker in the victim system. The retrospective analysis aids not only in understanding the attack mechanism but also in preventing similar attacks in the future. Such benefits can be foreseen in deep learning post-modem analysis. In spite of its inspiring idea, TRACEBACK has some limitations that degrade its performance in certain scenarios. In particular, it is based on measuring individual samples' impact on model weight parameters during training, which may be unstable and lead to suboptimal performance (see Section A).

In this paper, we introduce a novel data detoxing technique, \(D^{3}\), which employs a differential analysis methodology to extract poisoning triggers from compromised test samples. The necessity for this differential analysis approach is underscored by the stealthiness of the triggers. It is crucial to understand that possession of poisoned samples does not equate to comprehension of the triggers. Designed to be covert and stealthy, these triggers often escape detection. Moreover, advanced poisoning methods do not rely on a fixed pattern for the trigger. Instead, they leverage various forms of subtle, input-specific perturbations, such as those found in dynamic backdoor and input-aware backdoor attacks Salem et al. (2020); Nguyen and Tran (2020). This complexity renders conventional methods such as attempting to extract the trigger using image editing tools prove to be ineffective. Similarly, it is not feasible to identify all the poisoned images within a training set using only the poisoned test images as reference. This is because the triggers within the training data can differ from those in the test data, particularly in the context of input-aware attacks. Additionally, clean-label attack Turner et al. (2019); Zhao et al. (2020), which do not necessitate label changes and embed the trigger within target class samples, further complicate the process of locating the search space of potentially poisoned data.

We cope with these challenges by formulating poison extraction as a constrained optimization problem and relying on iterative gradient descent with a number of semantic restrictions (Section 2). After poison extraction, \(D^{3}\) augments the dataset by stamping the clean validation samples with the poison. A classifier is then trained on the logits of clean target class samples and stamped samples (which are misclassified to the target class). The classifier is hence used to distinguish clean and poisoned samples in the training dataset.

**Threat Model.** In line with the assumptions made in TRACEBACK Shan et al. (2022), we construct our threat model for \(D^{3}\) under the premise that it is deployed either by the model's owner or by a trusted third-party defender. This entity is assumed to have a small set of poisoned test samples captured in the wild (e.g., suspicious misclassified samples). In addition, the defender is presumed to have access to both the poisoned model and the poisoned training set. Furthermore, a small batch of clean validation samples is also within the defender's reach. It's critical to note, different from TRACEBACK, that we do not require access to information about the model's training procedure.\(\)

We make the following contributions.

* We propose a new dataset detoxing technique, which is based on a novel differential analysis to extract triggers and data augmentation. It is a post-mortem approach that provides a robust complement to existing defenses, particularly when they fail to detect complex, stealthy poisoning attacks.
* On 42 poisoned datasets with 18 poison types, \(D^{3}\) achieves over 95% precision and recall, vastly surpassing TRACEBACK, AC, SS, and STRIP with their precision and recall averaging at (39.9%, 60.5%), (55.0%, 66.0%), (42.0%, 53.6%), and (36.6%, 12.7%) respectively. It also excels over backdoor scanners ABS and FeatureRE, which only reach 52.3% and 72.7% precision and 39.0% and 43.2% recall.

## 2 Methodology

Figure 1 presents the overview of \(D^{3}\). In the first poison-extraction step, i.e., subfigure (a), it takes a few poisoned test samples acquired in the wild and a small set of clean validation samples of the

victim class, which do not overlap with the test samples, and extracts the poison. In the second data-augmentation step, i.e., subfigure (b), it applies the extracted poison to the clean validation images (in the victim class) to construct a set of augmented samples. Note that these samples are misclassified to the target class. In the third step, i.e., subfigure (c), we train a classifier \(C\) to separate the available clean target class samples and the samples with the poison applied, based on their features denoted by logits values. In the fourth step, the classifier is used to separate the clean and poisoned samples. In the following, we explain the details of these steps.

### Poison Extraction by Differential Analysis

Given a poisoned test sample, since the corresponding clean test sample is not available, one cannot extract the poison by taking the differences. The over-arching idea of our differential analysis is to use optimization to separate a poisoned test sample to a clean sample and the poison. Specifically, the separated clean sample should resemble its poisoned version as humans could still correctly recognize the poisoned sample; the extracted poison should be effective, causing other clean samples to be misclassified; the poison shall be in a small scale as it is expected to be stealthy. The above conditions are abstracted to a set of regulation rules for the optimization.

There are typically two ways to achieve stealthy poisoning in the current literature Tao et al. (2022); Liu et al. (2019): using a patch-like poison with a small \(L^{1}\) norm Gu et al. (2019); Turner et al. (2019) and using pervasive but small perturbations with a small \(L^{}\) norm Nguyen and Tran (2021); Liu et al. (2018, 2020); Nguyen and Tran (2020). Solem et al. (2020). We call the former _patch-like poison_ and the latter _pervasive poison_. The aforementioned poison extraction has different instantiations for the two types of poison. Note that it is common in the literature to handle cases differently. For example, ABS analyzes input patterns for simple triggers and applies artificial brain stimulation techniques for complex triggers.

Extracting Patch-like PoisonWe use \(F\) to denote the model, \(x_{p}\) to denote a poisoned test sample, or a set of such samples without losing generality. We use \(x\) to denote its clean version, which is not explicitly available, and \(p\) to denote the patch to extract. We further use \(x_{v}\) to denote a set of clean victim class samples and \(y_{t}\), \(y_{v}\) the target and victim labels, respectively. Let \((x_{1},x_{2})\) be a distance function between two samples and the operator \(\) stamping a patch to an sample. They are formally defined as follows.

\[x=(z)\] (1)

\[x p=(x[p<]+p[p]),\] (2)

where \(z\) is the random noise input to the StyleGAN Karras et al. (2019), and \(\) is the threshold to determine whether to take pixels from the generated patch. We use \(=0.001\) to include as many pixels from the patch as possible in the experiments.

Figure 1: Overview of \(D^{3}\). It has four steps: (a) extracting the poison via optimization; (b) applying the extracted poison to the validation set and creating more poisonous samples; (c) training a classifier on the crafted poisonous samples and clean samples; and (d) detoxing the training set using the classifier.

We hence formulate the extraction process as a constrained optimization problem as follows.

\[*{arg\,min}_{z,p}\ \ \ (x_{p},x p)+(F(x_{v}  p),y_{t})+(F(x),y_{v})+ L^{1}(p)\] (3)

Specifically, the first term dictates that the optimized \(x\) and \(p\) should resemble the original \(x_{p}\) when they are combined; the second term ensures the poison \(p\) can flip a set of validation clean samples; the third term is that the generated \(x\) must be classified to the correct label; and the final term ensures \(p\) is small.

The distance \(\) is calculated using L2 on both the pixel and the embedding levels Zhang et al. (2018); Kettunen et al. (2019):

\[(x_{p},x p)=||x_{p},x p||_{2}^{2}+||Enc(x_{p}),Enc(x  p)||_{2}^{2},\] (4)

where \(Enc()\) denotes a pre-trained encoder that derives the embedding of an input image. Constraining both input and embedding space distances ensures a visual and meaningful resemblance between \(x_{p}\) and \(x p\).

The optimization directly updates the patch pixel values. To smooth the procedure and make it easy to converge, we utilize the dual-tanh representation of pixel perturbation proposed in Tao et al. (2022), whose idea is to use two tanh functions to denote pixel changes along two respective directions, positive and negative. The long flat tails and smoothness of tanh functions allow easy convergence biasing towards either maximum changes or 0 changes. In other words, it encourages pixels undergo either no changes or maximum changes. Specifically, we change the \(\) operator as follows.

\[x p=\]

Here, \(()+1\) has long tails at two ends with values 0 and _maxp_ (i.e., 255). Hence, Eq.(3) changes to optimizing \(_{p}\) and \(_{n}\) in \((-,+)\), deciding changes along the positive and negative directions, respectively.

The third term in Eq.( 3) is replaced with the following to control the magnitude of the extracted poison.

\[(_{p}}{})+1\ +\ (_{n}}{})+1\] (5)

Parameter \(\) is used to alter the slope of \(\) such that the optimization is smoother. We empirically set \(=10\).

Extracting Pervasive Poison.When the poison is pervasive, the pixel level changes vary from sample to sample, such as in filter poison Liu et al. (2019); WaNet attack Nguyen and Tran (2021), and DFST Cheng et al. (2021). We hence use a transformation layer to denote such changes. In particular, the poison \(p\) is denoted by a pair \( w,b\) such that the poison application operator \(\) is changed to the following.

\[x p=w x+b.\] (6)

In other words, \(D^{3}\) optimizes \(w\) and \(b\) instead of a pixel pattern \(p\). The final term in Eq.(3) is changed to the following because pixel level differences are no longer a good metric to measure the quality of the extracted poison.

\[L^{2}(_{x_{v} p}-_{x_{p}})+L^{2}(_{x_{v} p}-_{x_ {p}}),\] (7)

where \(_{a}\) denotes the mean pixel value of an input image \(a\) and \(_{a}\) denotes its standard deviation. This regularization term constrains the distribution of validation images with the extracted poison is similar to the distribution of provided poisoned images. Intuitively, \(D^{3}\) enforces the style similarity of the two, e.g., inducing a greyish color scheme with a poison by a Gotham filter.

As \(D^{3}\) does not have any prior knowledge whether the poison-to-extract belongs to the patch type or the pervasive type, it tries both types and selects the one with better performance, i.e., lower loss.

### Data Augmentation and Training Classifier

A naive idea is to directly train a classifier based on the available poisoned test samples and clean validation samples. However, there are often very few poisoned samples, insufficient for training a good classifier (see our experiments in Section D). Thus, our idea is to produce more poisoned samples by data augmentation, namely, applying the extracted poison.

Specifically, we split the validation clean samples to two subsets \(x_{1}\) and \(x_{2}\). Let an extracted patch-like poison be \(p\). We augment \(x_{1}\) with \(x_{1} p\), \(T(x_{1}) p\) and \(x_{1} T(p)\). Here, \(T\) denotes some typical data transformations such as offsetting, flipping, rotation, perspective changes, and affine transformations. For a pervasive poison \(p\), we augment \(x_{1}\) with \(x_{1} p\), \(x_{1}\) and \(T(x_{1}) p\). Here, \(\) denotes adding small perturbations to the weight and bias of \(p\). We filter out the augmented samples that are not misclassified to the target label. We hence train a classifier to separate \(x_{2}\) from the augmented \(x_{1}\), based on the logits values. The classifier is then applied to the training dataset to identify poisoned samples.

## 3 Evaluation

We assessed \(D^{3}\) on a total of 42 poisoned datasets, encompassing 39 from the TrojAI program and CIFAR10, VGGFace, and ImageNet, subjected to 5 attack strategies. Our evaluation pits \(D^{3}\) against the baseline, TRACEBACK, and leading poisoned sample detection approaches, Activation Clustering, Spectral Signature, and STRIP, showcasing its effectiveness and precision in diverse scenarios. Detailed experiment setup is listed in Section B. More experiments, ablation study, adaptive attack can be found in Section B.1, B.2, C, and E, respectively.

### Comparison with TRACEBACK

We assess \(D^{3}\) using TRACEBACK's datasets, poisoned via BadNet and TrojNN Gu et al. (2019); Liu et al. (2018), achieving competitive results (Table 1). The BadNet attack used a yellow flower pattern with a 0.1 poisoning rate, while TrojNN utilized optimized watermarks on VGGFace.

Additionally, we test \(D^{3}\) on CIFAR10 datasets poisoned with various attacks, including clean-label, dynamic, and input-aware backdoor attacks. Using an adversarial-robustness toolbox, we set a red square as the clean-label trigger, achieving 100.0% precision and 94.0% recall. In contrast, TRACEBACK misclassifies the entire set. For dynamic and input-aware attacks Tao and Cheng (2023), \(D^{3}\) outperforms TRACEBACK, achieving high precision and recall rates (100.0%/99.3% and 96.7%/90.1%, respectively).

### Comparison with Black-box Reverse Engineered Poison

An alternative to extracting poisons from poisoned test samples is to use an existing backdoor scanner that can invert a trigger directly from the model and a few clean samples, by finding the smallest pattern or feature that can consistently flip classification results to the target class. In this experiment, we compare \(D^{3}\) with two black-box scanners ABS and FeatureRE, which reverse engineer the triggers in input space and feature space, respectively. We use a subset of Table 3 randomly selected by seed 82003253 to eliminate the bias of seed 0. Note that we are aware that the three techniques have different assumptions because ABS and FeatureRE do not consider any poisoned test samples. The comparison is to provide a reference.

For each model, we provide 200 clean samples for each class. We use ABS to invert triggers (for the target classes) and replace the extracted poisons in the \(D^{3}\) pipeline with the inverted triggers. For FeatureRE, we directly train a classifier to identify reverse engineered trigger feature and clean samples' feature in the target class. We then report the detoxing results in Table 9.

Observe \(D^{3}\) has significantly better performance than ABS, indicating the knowledge of poisoned test examples plays an important role in generating effective triggers. To further illustrate this, Figure 7

    &  &  & \)} &  \\   & & & Prec. (\%) & Recall(\%) & Prec.(\%) & Recall(\%) \\  WideResNet & BadNet & CIFAR10 & 100.0 & 100.0 & 99.5 & 98.9 \\ Inception-ResNet & BadNet & ImageNet & 95.8 & 91.0 & 99.1 & 99.1 \\ VGG16 & Trojnn & VGGFace & 97.1 & 100.0 & 99.8 & 99.9 \\ ResNet18 & Clean-label & CIFAR10 & 100.0 & 94.0 & 0.0 & 0.0 \\ VGG11 & Dynamic & CIFAR10 & 100.0 & 99.3 & 50.8 & 100.0 \\ ResNet18 & Input-aware & CIFAR10 & 96.7 & 90.1 & 50.9 & 100.0 \\   

Table 1: Evaluation on datasets used in TRACEBACK and three additional attacks. \(D^{3}\) is more stable and always has better performance.

in Section F shows samples stamped with triggers inverted by ABS and with poisons extracted by \(D^{3}\) for two models. Observe that \(D^{3}\) can extract poison that resembles the ground-truth. Figure 6(b) shows the results for a model poisoned with a pervasive filter. The results are arranged in a similar way to Figure 6(a). Observe that the style in the second row (by \(D^{3}\)) is more similar to that in the first row (i.e., the ground truth poisoned samples), compared to the third row. In Figure 2, we illustrate how the classifier trained on the \(D^{3}\)-extracted poison has much better separation in clean-label attack. These results show that ABS and FeatureRE cannot invert high-fidelity triggers, affecting its performance in detoxing.

## 4 Related Work

A thorough analysis of limitations in state-of-the-art can be found in Section A.

Data poisoning attacks alter training data to impair deep learning models Biggio et al. (2014). They can degrade performance Shafahi et al. (2018) or insert backdoors Gu et al. (2019), which we explore in Section 3. Defenses against poisoning function at inference Chou et al. (2020), Gao et al. (2019) or pre-training Zeng et al. (2021). We compare our approach with key methods like AC and SS in Section B.2.

## 5 Conclusion

We present a detoxing technique for Deep Learning datasets. It features a novel differential analysis to extract poisons and using data augmentation to train a highly effective classifier to separate clean and poisoned samples in datasets. This post-mortem approach provides a robust complement to existing defenses, particularly when they fail to detect complex, stealthy poisoning attacks. Evaluated on 42 poisoned datasets with diverse attack types, \(D^{3}\) achieves over 95% precision and recall, substantially outperforms the state-of-the-art.

    & \)} &  &  \\   & Prec.(\%) & Recall (\%) & Prec.(\%) & Recall(\%) & Prec.(\%) & Recall(\%) \\ 
1058 & 92.0 & 40.0 & 80.3 & 39.8 & 0.0 & 0.0 \\
585 & **100.0** & **100.0** & 100.0 & 99.3 & 94.6 & 96.5 \\
999 & 87.7 & 84.0 & 100.0 & 20.3 & 100.0 & 74.8 \\
688 & **100.0** & **100.0** & 0.0 & 0.0 & 100.0 & 1.0 \\
385 & 89.3 & 66.8 & 100.0 & 40.0 & 100.0 & 94.5 \\
727 & **100.0** & **100.0** & 0.0 & 0.0 & 93.2 & 100.0 \\
876 & **82.4** & 90.0 & 86.3 & 96.0 & 97.9 & 71.5 \\
827 & **99.5** & **100.0** & 0.0 & 0.0 & 100.0 & 4.8 \\
933 & **100.0** & **99.5** & 100.0 & 93.3 & 0.0 & 0.0 \\
598 & **96.4** & **99.8** & 100.0 & 71.5 & 0.0 & 0.0 \\ Clean-label & **100.0** & **94.0** & 12.8 & 47.0 & 77.7 & 99.6 \\ Dynamic & **100.0** & **99.3** & 0.0 & 0.0 & 100.0 & 14.1 \\ Input-aware & **96.7** & **90.1** & 0.0 & 0.0 & 82.2 & 6.4 \\   

Table 2: Comparison of poison extracted by \(D^{3}\) with by black-box reverse engineering tools ABS and FeatureRE. \(D^{3}\) has overall better performance, indicating the knowledge of poisoned test examples plays an important role in generating effective triggers.

Figure 2: Feature distributions from \(D^{3}\) (left), ABS (middle), and FeatureRE (right) in the clean-label attack. Clean and poisoned samples are in yellow and blue, respectively; validation samples with extracted poison are in red. Only \(D^{3}\)-extracted triggers blend with real poisoned samples, unlike ABS and FeatureRE.