# Online Learning with Sublinear Best-Action Queries

Matteo Russo

Sapienza University of Rome, Italy

mrusso@diag.uniroma1.it &Andrea Celli

Bocconi University, Italy

andrea.celli2@unibocconi.it &Riccardo Colini-Baldeschi

Meta, Central Applied Science, UK

rickuz@meta.com &Federico Fusco

Sapienza University of Rome, Italy

fuscof@diag.uniroma1.it &Daniel Haimovich

Meta, Central Applied Science, UK

danielha@meta.com &Dima Karamshuk

Meta, Central Applied Science, UK

karamshuk@meta.com &Stefano Leonardi

Sapienza University of Rome, Italy

leonardi@diag.uniroma1.it &Niek Tax

Meta, Central Applied Science, UK

niek@meta.com

###### Abstract

In online learning, a decision maker repeatedly selects one of a set of actions, with the goal of minimizing the overall loss incurred. Following the recent line of research on algorithms endowed with additional predictive features, we revisit this problem by allowing the decision maker to acquire additional information on the actions to be selected. In particular, we study the power of _best-action queries_, which reveal beforehand the identity of the best action at a given time step. In practice, predictive features may be expensive, so we allow the decision maker to issue at most \(k\) such queries. We establish tight bounds on the performance any algorithm can achieve when given access to \(k\) best-action queries for different types of feedback models. In particular, we prove that in the full feedback model, \(k\) queries are enough to achieve an optimal regret of \((\{,}{{k}}\})\). This finding highlights the significant multiplicative advantage in the regret rate achievable with even a modest (sublinear) number \(k()\) of queries. Additionally, we study the challenging setting in which the only available feedback is obtained during the time steps corresponding to the \(k\) best-action queries. There, we provide a tight regret rate of \((\{}{{}},}}{{k^{2}}}\})\), which improves over the standard \((}{{}})\) regret rate for label efficient prediction for \(k(T^{}{{3}}})\).

## 1 Introduction

Online learning is a foundational problem in machine learning. In its simplest version, a decision maker repeatedly interacts with a fixed set of \(n\) actions over a time horizon \(T\). At each time, the decision maker needs to choose one of a set of actions; subsequently, it receives an action-dependent loss and observes some feedback. These loss functions are generated by an omniscient (but oblivious) adversary and are only revealed on-the-go. The goal of the decision maker is to design a learning algorithm that achieves small _regret_ with respect to the best fixed action in hindsight, i.e., the difference between the decision maker's loss and that of the fixed action. Several online learning algorithmshave been developed, characterized by optimal instance-independent regret bound, depending on the feedback model (Cesa-Bianchi and Lugosi, 2006; Slivkins, 2019).

Following the recent literature on algorithms with machine learning-based predictions (see, e.g., the survey by Mitzenmacher and Vassilvitskii (2020)), we study the case where the learner is allowed to issue a limited number of _best-action queries_ to an oracle that reveals the identity of the best action for that step, so that the learner can choose it. This setting is motivated by scenarios in which obtaining accurate predictions on the optimal choice among numerous actions is possible but comes with significant costs and time constraints. For instance, consider an online platform that continuously moderates posted content (e.g., Meta (Meta, a,b] or Google [Google]), and the online learning problem it faces: posts are generated one after the other, and the platform's task consists in deciding whether or not to flag the content as harmful. In this application, the platform may do so via (i) content moderation actions that are based on (expert) human reviews (that plays the role of _best-action queries_), and (ii) automated content moderation decisions, i.e., decisions arising by employing an online learning algorithm. Due to budget or time constraints, access to human reviewing is a scarce resource, and the platform can only employ external reviewers at most \(k\) times.

While the idea of incorporating hints or queries into online learning models has already been studied (e.g., Bhaskara et al., 2021, 2023), we are the first to study _best-action queries_. Bhaskara et al. (2021) focus on online linear optimization with full feedback, with a query model which outputs vectors that are correlated with the actual losses. In the case of optimization over strongly convex domains, the regret bound improves from \(\) to \( T\), even for learners that receive hints for \(O()\) times. In an alternative model, Bhaskara et al. (2023) studies comparison queries that allow the decision maker to know in advance, at each time, which among a small number of actions has the best loss. In this model, probing \(2\) arms is sufficient to achieve time-independent regret bounds for online linear and convex optimization with full feedback, an in the stochastic multi-armed bandit problem. Our model differs from previous ones in two directions: (i) the online learner issues at most \(k\) queries (differently from Bhaskara et al. (2023)), and (ii) these queries are purely ordinal and the domain is not strongly convex1 (so that the logarithmic bound in Bhaskara et al. (2021) does not apply).

Another potential way of addressing how to efficiently using limited moderation is through the abstention learning model, where the learner can "abstain" from making a decision and instead use additional resources to receive the optimal response. This includes models like KWIK and full-information models (Li et al., 2011; Sayedi et al., 2010; Zhang and Chaudhuri, 2016; Cortes et al., 2018; Neu and Zhivotovskiy, 2020; Gangrade et al., 2021).

### Our Model

In our model, an online learner repeatedly interacts with \(n\) actions over a time horizon \(T\). At the beginning of each time \(t[T]^{}\), the learner chooses one of these actions \(i_{t}\) and suffers a loss \(_{t}(i_{t})\) generated by an (oblivious) adversary that may depend on both the action and the time; then, it observes some feedback. In this paper, we allow the learning algorithm \(\) to issue a _best-action query_, for at most \(k\) out of \(T\) times. When the learner issues a query, an oracle reveals the _identity_ of the best action at that time, \(i_{t}^{*}\), so that the learner can select it. The quality of a learning algorithm is measured via the regret: the difference between its performance and that of the best fixed action in hindsight. The regret of an algorithm \(\) against a sequence of losses \(^{n T}\) reads

\[R_{T}(,)=_{t[T]}[_{t}(i _{t})]-_{i[n]}_{t[T]}_{t}(i),\]

where the expectation runs over the (possibly) randomized decisions of the algorithm. We are interested in designing learning algorithms that perform well, i.e., suffer sublinear regret against all possible sequences of losses. For this reason, we denote with \(R_{T}()\) (without the dependence on \(\)) the worst-case regret of \(\): \(R_{T}()=_{}R_{T}(,).\) The minimax regret of a learning problem is then the regret achievable by the best algorithm. In our paper we pinpoint the exact minimax regret rates for the problems studied.

For the sake of simplicity, we denote with \(L_{T}(i)\) the total loss incurred by action \(i\) over the whole time horizon: \(L_{T}(i)=_{t[T]}_{t}(i)\). \(L_{T}^{}=_{t[T]}_{t}(i_{t}^{*})\) denotes instead the sum of the minimum loss actions at each time. Finally, \(L_{T}(_{k})=[_{t[T]}_{t}(i_{t})]\) denotes the expected total loss of an algorithm \(_{k}\) issuing at most \(k\) best-action queries.

### Our Results

Full feedback.We start with the _full feedback_ (a.k.a. prediction with experts) where the learner observes at each time the losses of all the actions after the action is chosen, i.e., _all_ the loss vector \(_{t}=(_{t}(1),_{t}(2),,_{t}(n))\) after action \(i_{t}\) is chosen. We obtain the following results:

* We show that by combining the Hedge algorithm with \(k\) queries issued uniformly at random (Algorithm 1), we obtain a regret rate of \(O(\{,}{{k}}\})\), see Theorem 2.2.
* In Theorem 3.3, we complement this positive result with a tight lower bound: we design a family of instances that forces every algorithm with \(k\) queries to suffer the same regret.

It is surprising that issuing a sublinear number of queries, say \(k=T^{}\) for \((}{{2}},1)\), yields a significant improvement on the regret, which becomes \(T^{1-}\). Note, the total of the losses incurred by any algorithm in \(k\) rounds is at most \(k\), which only affects the overall regret in an additive way; nevertheless, we prove that issuing \(k\) queries has an impact on the regret that is _multiplicative_ in \(k\). For instance, \(T^{}{{3}}}\) queries are enough to decrease the regret to \(T^{}{{3}}}\), which is well below the \(()\) minimax regret for prediction with expert _without_ queries (Cesa-Bianchi and Lugosi, 2006).

Label efficient feedback.We then proceed to study a partial feedback model inspired by the label efficient paradigm (Cesa-Bianchi and Lugosi, 2006). In the label-efficient prediction problem, the learner only observes the losses in a few selected times. With _label efficient feedback,_ the learner only observes feedback during the \(k\) times where the best-action queries are issued (but only after choosing the action). We obtain the following results:

* We modify the (label-efficient) Hedge algorithm (Cesa-Bianchi and Lugosi, 2006) to achieve a regret rate of \(O(\{}{{}},}}{{k^{2}}}\})\), see Theorem 2.4,
* In Theorem 3.4, we show that it is not possible to improve on these rates: there exists instances where all algorithms suffer \((}{{}})\) regret if \(k O(T^{}{{3}}})\) or \((}}{{k^{2}}})\) if \(k(T^{}{{3}}})\).

We observe that our algorithms improve (for \(k(T^{}{{3}}})\)) on the regret rate of label efficient prediction with \(k\) steps of feedback, which is of order \((}{{}})\)(Chapter 6.2 in Cesa-Bianchi and Lugosi, 2006). Also in this case, we observe the surprising multiplicative impact on the regret of the \(k\) queries. For instance, \(k=T^{}{{4}}}\) queries (which impact on a total loss of the same order but leave unaffected \((T)\) rounds) are enough to achieve \(O()\) regret.

Stochastic i.i.d. setting.In Section B, we derive (up to polylogarithmic factors) the above results in the stochastic setting, but with different algorithms. Namely, in full feedback, we show how Follow-The-Leader (Auer et al., 2003) achieves regret \((}{{k}})\) when \(k()\), and \(()\) otherwise. With label efficient feedback, Explore-Then-Commit (Perchet et al., 2015) achieves \((}}{{k^{2}}})\) when \(k(T^{}{{3}}})\), and \((}{{}})\) otherwise.++

Footnote ‡: We use \((x)\) to hide polylogarithmic terms in \(x\).

### Technical Challenges

Issuing a best-action query has the effect of inducing a possibly _negative_ instantaneous regret. In fact, the benchmark that the algorithm is comparing against at time \(t\) is the loss of the best fixed action over the whole time horizon, which may naturally be larger, at time \(t\), than the best action \(i_{t}^{*}\) in that specific time. Overall, the magnitude of the total "negative" regret that is generated by \(k\) queries is at most \(O(k)\), which is typically sublinear in \(T\), and affects _linearly_ on the overall regret. We prove that this sublinear number of queries has a (surprising) multiplicative effect on the overall regret bound.

Our analysis reveals that any algorithm employing a _uniform querying_ strategy can be characterized by a total loss decomposed into two terms: a \(}{{T}}\) fraction of the best dynamic loss, representing the sum of the smallest losses at each time, and the total loss of the same algorithm operating without queries, scaled by a factor of \(1-}{{T}}\). Additionally, within our proof, we conduct a more refined analysis of the Hedge algorithm in the _no-querying_ setting. Specifically, we can express the regret as a function of the difference between the best dynamic loss and the best-fixed cumulative loss in hindsight. This formulation demonstrates a noteworthy enhancement in regret rates compared to the conventional bound of \(O()\) when the discrepancy between the best dynamic loss and the best-fixed cumulative loss in hindsight is relatively small.

Concerning lower bounds, their construction is more challenging than their "no queries" counterpart. In fact, we need to design hard instances against _any_ query-augmented learning algorithm, and provide tight bounds in both \(k\) and \(T\). The lower bounds we construct are stochastic, this implies that the minimax regret we find are tight, even in the stochastic model, where the losses are drawn i.i.d. from a fixed but unknown distribution. To exemplify this, let us consider the following natural instance, which provides a simple proof of the \(()\) regret lower bound in the adversarial setting (without queries). The instance is composed of two arms, whose rewards are i.i.d. Bernoulli distribution with probability \(}{{2}}\). Any learning algorithm achieves \(}{{2}}\) regret, while the best-fixed arm in hindsight is expected to achieve an extra \(()\) term.8 Now, if the learner is given the power to issue (order of) \(\) queries, then its regret naturally drops from (order of) \(\) to constant.

## 2 Hedge with \(k\) Best-Action Queries

In this section, we propose two algorithms that address online learning with full and label efficient feedback. They are built by combining the Hedge algorithm (Chapter 2 in Cesa-Bianchi and Lugosi, 2006) to uniform queries. We start by presenting some known properties of Hedge (in full feedback) that are crucial key for the main results of this section.

**Lemma 2.1**.: _Consider the Hedge algorithm \(_{}(})\) run on loss sequence \(}[0,U]^{n T}\) with learning rate \(<}{{U}}\). Then, for all action \(i[n]\), it holds that,_

\[_{T}(_{}(}))(_{T}(i)+),\]

_where \(_{T}(_{}(}))=_{t[T],i [n]}p_{t}(i)_{t}(i)\) is the expected cumulative loss of \(_{}(})\) and \(_{T}(i)\) is the cumulative loss of action \(i\)._

Proof.: We have that, by definition,

\[W_{T+1} w_{T+1}(i)w_{t}(i)(-_{t}(i) )=_{t[T]}(-_{t}(i))=(-_{T}(i)).\]

We also know that

\[W_{t+1} W_{t}(1-_{i[n]}p_{t}(i)_{t}( i)+^{2}_{i[n]}p_{t}(i)_{t}^{2}(i)).\]

Thus,

\[W_{T+1} n_{t[T]}(1-_{i[n]}p_{t}(i) {}_{t}(i)+^{2}_{i[n]}p_{t}(i)_{t}^{2}(i) ),\]

which combined with the earlier bound and taking logarithms of both sides (since they are both positive), gives

\[-_{T}(i) n+_{t[T]}(1-_{i[n]}p _{t}(i)_{t}(i)+^{2}_{i[n]}p_{t}(i)_{t}^{2}(i))\]\[ n-_{t[T],i[n]}p_{t}(i)_{t}(i)+ ^{2}_{t[T],i[n]}p_{t}(i)_{t}^{2}(i)\] \[ n-(-U^{2})_{t[T],i[n]}p_{t}(i) _{t}(i).\]

The second inequality above follows from \((1+z) z\) for \(z\), and the third by observing that \(_{t}^{2}(i) U_{t}(i)\), since \(_{t}(i)[0,U]\). The lemma follows by rearranging the terms above. 

### Full Feedback: An \(O()\) Regret Bound

**Theorem 2.2**.: _Consider the problem of online learning with full feedback and \(k\) best-action queries, then there exists an algorithm \(_{k}\) that guarantees_

\[R_{T}(_{k})\{,\}.\]

We prove that Algorithm 1 exhibits the desired regret guarantees. In particular, as we illustrate next, this algorithm performs uniform querying, i.e., they choose a uniformly random subset \(Q[T]\) of size \(k\) where to issue best-action queries. In the case of uniform queries, and when feedback and action taken by the algorithm are not correlated, a useful simplification can be made.

**Observation 2.3**.: _Let \(_{0}\) be an algorithm with no querying power, with full or label-efficient feedback. Consider \(_{k}\), obtained from \(_{0}\) by equipping it with \(k\) uniformly random queries across the time horizon \(T\) or with an independent query on each step time step with probability \(}{{T}}\). Similarly, let \(i_{t}^{0}\) and \(i_{t}\) be the actions selected by \(_{0}\) and \(_{k}\) at time \(t\). Then, for all adversarial sequences \(^{n T}\) of action losses,_

\[[_{t}(i_{t})]=(1-) [_{t}(i_{t}^{0})]+[ _{t}(i_{t}^{*})],\]

_for all \(t[T]\), and thus_

\[L_{T}(_{k})=(1-) L_{T}(_{0}) + L_{T}^{}.\] (1)

```
1:Input: Sequence of losses \(_{t}(i)\) and query budget \(k[T]\)
2:Sample \(k\) out of \(T\) time steps uniformly at random and denote this random set by \(Q\)
3:Set \(=}\) when \(k\), otherwise \(=\)
4:Initialize \(w_{1}(i)=1\) for all \(i[n]\)
5:for\(t[T]\)do
6:if\(t Q\)then
7: Observe \(i_{t}^{*}=_{i[n]}_{t}(i)\)
8: Select action \(i_{t}^{*}\)
9:else
10: Let \(W_{t}=_{i[n]}w_{t}(i)\)
11: Select action \(i\) with probability \(p_{t}(i)=(i)}{W_{t}}\)
12: Observe \(_{t}(i)\) for all \(i[n]\)
13: Update \(w_{t+1}(i)=w_{t}(i)(-(_{t}(i)-_{t}(i_{t}^{*}) ))\) for all \(i[n]\) ```

**Algorithm 1** Hedge with Best-Action Queries

Proof of Theorem 2.2.: Let us first note that Algorithm 1 without queries is an instantiation \(_{}(})\) applied to losses \(_{t}(i)=_{t}(i)-_{t}(i_{t}^{*})\). Let this algorithm be denoted as \(_{0}\). Then, applying Lemma 2.1 and expanding the terms, we obtain

\[L_{T}(_{0})(i)}{1-}+- ^{}}{1-}.\]Let \(_{k}\) be Algorithm 1 with \(k\) best-action queries. By Observation 2.3, specifically (1), it holds that

\[L_{T}(_{k})}{{T}})L_{T}(i)}{1- }+}{{T}}) n}{(1-)}-}{{T}})L_{T}^{}}{1-}+ L_{T}^{}\] \[ R_{T}(_{k},i)}{{T}}) n}{ (1-)}+(L_{T}(i)-L_{T}^{})\{ ,\},\]

where the last inequality holds by setting \(=\{}{{T}}},}{{T}}\}\). 

Before proceeding further, let us provide some technical intuition of why Theorem 2.2 should hold. The additive term \( L_{T}^{}\) (given in Equation (1)) alters the choice of the learning rate, which is allowed to be more aggressive, thus impacting the regret in a multiplicative way. To be more specific, in the usual Hedge performance analysis, we do not care about the negative term \(^{}}{1-}\). This negative term together with the additive \( L_{T}^{}\) term given by best-action queries allows us to set the optimal \(\) to be larger than the usual (order of) \(}{{}}\). In other words, the additive impact of the \( L_{T}^{}\) term permits a multiplicative gain in regret as the learning rate \(\) is modified and increased.

### Label Efficient Feedback: An \(O( n}{k^{2}})\) Regret Bound

We extend Algorithm 1 to a setting where feedback is given only during querying time steps, with the only difference that the update rule is performed just after the querying time steps and nowhere else across the time horizon. We prove the following theorem:

**Theorem 2.4**.: _For all adversarial sequences \(^{n T}\) of action losses and for all \(k}-1\), in the label efficient query model, there exists an algorithm \(_{k}\) that guarantees_

\[R_{T}(_{k}) 2\{T}, n}{k^{2}}\}.\]

Algorithm description.We first describe the algorithm \(_{k}\) we use: Let \(X_{t}(}{{T}})\) be a Bernoulli random variable, for some \( k\) to be specified later. \(_{k}\) issues a best action query if, at time step \(t\), \(X_{t}=1\) and unless the query budget is exhausted. Once the query budget is exhausted, the algorithm stops querying. Otherwise, it performs the usual update rule on losses \(_{t}(i)=(_{t}(i)-_{t}(i^{*}_{t})) \{X_{t}=1\}\). The algorithm then simply selects action \(I_{t}=i^{*}_{t}\) if \(X_{t}=1\) and action \(I_{t}=i\) with probability \(p_{t}(i)\) if \(X_{t}=0\). Moreover, we denote by \(X_{ t}=(X_{1},,X_{t}),I_{ t}=(I_{1},,I_{t})\).

For the sake of the analysis, we introduce another algorithm \(^{}_{k}\), which is the same as algorithm \(_{k}\) with the only (but crucial) difference that it issues a query if and only if \(X_{t}=1\), regardless of whether or not query budget is exhausted. We thus bound the regret of \(_{k}\) in terms of the regret of \(^{}_{k}\).

**Lemma 2.5**.: _For all adversarial sequences \(^{n T}\) of action losses, in the label efficient query model, algorithm \(^{}_{k}\) guarantees_

\[R_{T}(^{}_{k})\{T}}, n}{^{2}}\}.\]

Proof.: For algorithm \(^{}_{k}\), we have that its counterpart without queries, \(^{}_{0}\), is an instantiation \(_{}(})\), with \(}=}\). Thus, by Lemma 2.1, we obtain

\[_{T}(^{}_{k})=_{t[T],i[n]}p_{t}(i)_{t}(i)}(_{T}(i)+),\] (2)as long as \(</T\). We now recognize that, since \([_{t}(i) X_{ t-1},I_{ t-1}]=_{t}(i )-_{t}(i_{t}^{*})\), then

\[_{i[n]}[p_{t}(i)_{t}(i)X_{ t- 1},I_{ t-1}]=_{i[n]}p_{t}(i)(_{t}(i)-_{t}(i_{t}^ {*})).\]

Therefore, by the tower property of expectation applied around (2), we have

\[L_{T}(_{0}^{}) =_{i[n]}[p_{t}(i)_{t}(i) ]=_{i[n]}p_{t}(i)(_{t}(i)-_{t}(i_{t}^{*}))\] \[}}(L_{T}(i)-L_{T}^{ }+)}{-T}( L_{T}(i)+)--T}L_{T}^{},\]

where the last inequality holds since \(T\). By Observation 2.3, we get

\[L_{T}(_{k}^{})(1-)}{-T}(L_{T}(i)+)--T}L_{T}^{}+L_{T}^{},\]

which means that the regret is upper bounded by

\[R_{T}(_{k}^{},i) n}{(-T) }+-k}{T(-T)}(L_{T}(i)-L_{T}^{}) \{T}}, n}{^{2}}\},\]

where the last inequality holds by setting \(=\{},}{}}\}\), and then by noticing, in the latter case, that \(-}{}k\). 

With this lemma, we are ready to prove Theorem 2.4, where we bound the regret of algorithm \(_{k}\).

Proof of Theorem 2.4.: We consider event \(=\{|Q| k\}\), so that, slightly abusing notation, we write the regret of algorithm \(_{k}\) as

\[R_{T}(_{k})=[R_{T}(_{k}) ][]+[R_{T}( _{k})}][}][R_{T}(_{k}) ]+T[}].\]

To upper bound the second summand above, we have

\[T[}] T(- )^{2}}{T}) T=1,\] (3)

by Hoeffding's inequality applied on the binomial random variable \(|Q|\) with expectation \(\), and as long as \( k-}+1\).

For what concerns the first summand, we recognize that under event \(\), \(_{k}\) and \(_{k}^{}\) are exactly the same algorithm. Thus, it holds that \([R_{T}(_{k},i)]= [R_{T}(_{k}^{},i)}]\). Moreover, if \([R_{T}(_{k})}] 0\), and since \([] 1-}{{T}}\), then

\[[R_{T}(_{k}^{},i)]  R_{T}(_{k}^{},i) \{T}}, n}{^{2}} \},\]

by Lemma 2.5. If, instead, \([R_{T}(_{k})}]<0\), we also know that, by an identical derivation to (3), \([R_{T}(_{k})}][}]-1\). Therefore,

\[[R_{T}(_{k}^{},i)] (R_{T}(_{k}^{},i)+1) (\{T}}, n}{^{2}} \}+1),\]

again by Lemma 2.5. Overall, we obtain

\[R_{T}(_{k}) 2\{T},  n}{k^{2}}\}.\]Lower Bounds

In this section, we construct two of randomized instances of the learning problem, which induce a tight lower bound on the minimax regret rates for both feedback models. We define the random variables \(Z_{t}\) as the feedback observed by the algorithm at the end of time \(t\). In the full feedback model, \(Z_{t}=_{t}\), while in the label efficient setting, \(Z_{t}=_{t}\) only if a query is issued at time \(t\) (and \(Z_{t}\) is an empty \(n\)-dimensional vector otherwise). Furthermore, we denote with \(Z_{ t}\) the array containing the feedback \(Z_{1},Z_{2},,Z_{t}\) until time \(t\).

We start by describing the two stochastic instances, which are characterized by two distributions over two losses. As a notational convention, we denote the losses with \(_{t}\), and introduce two probability measures \(^{+},^{-}\) (and their corresponding expectations \(^{+},^{-}\)). Let \(,q\) be two parameters we set later (with \( q\)), we have that the losses of the \(n=2\) actions are distributed as follows:

\[(_{t}(1),_{t}(2))=(1,1)&^{+}^{-}\\ (0,0)&-2q^{+}^{-}\\ (0,1)&q+^{+}q- ^{-}\\ (1,0)&q-^{+}q+ ^{-}.\]

We now introduce and prove a general Lemma on the expected regret \(R_{T}^{}(_{k})\) suffered by any deterministic algorithm \(_{k}\) which issues at most \(k\) queries, against the i.i.d. sequence of valuations drawn according to \(^{}\). Since we want a result that holds for both feedback models, we introduce the random set \(F\) which contains the times where \(_{k}\) actually observes the losses; note, \(N_{F}=|F|\) is equal to \(T\) in full feedback and to the number queries issued in the partial information model.

**Lemma 3.1**.: _For any deterministic algorithm \(_{k}\) which issues at most \(k\) best-action queries, we have:_

\[R_{T}^{+}(_{k})+R_{T}^{-}(_{k})(- }{q}^{+}[N_{F}])-2(q-)k.\]

Proof.: The best action \(i^{*}\) under \(^{+}\), respectively \(^{-}\) is the first, respectively the second, one, with an expected loss of

\[^{}[_{t}(i^{*})]=+q-.\] (4)

On the other hand, the best realized action \(i^{*}_{t}\) yields an expected loss of

\[^{}[_{t}(i^{*}_{t})]=^{}[\{ _{t}(1),_{t}(2)\}]=-(q-).\] (5)

Moreover, if the algorithm chooses a suboptimal action \(i_{t} i^{*}\), its expected instantaneous regret is:

\[^{+}[_{t}(2)]-^{+}[_{t}(i^{*}) ]=^{-}[_{t}(1)]-^{-}[_{t}( i^{*})]=2.\] (6)

Let now \(N_{+}\), respectively \(N_{-}\), be the random variable that counts the number of times that \(_{k}\) selects action \(1\), respectively \(2\), in times that are not in \(F\) (i.e., where the choice of \(1\) is not due to a query). Combining (4),(5), and (6), we have the following:

\[R_{T}^{}(_{k}) ^{}[_{t F}(_{t}(i_{t})-_{ t}(i^{*}))+_{t F}(_{t}(i^{*}_{t})-_{t}(i^{*}))]\] \[=2^{}[N_{}]-(q- )^{}[N_{F}].\]

Since \(N_{F} k\), and \(N_{+}+N_{-} T-k\), we have the following bound on the regret:

\[R_{T}^{+}(_{k}) ^{+}[N_{+}](T-k) -(q-)k\] \[R_{T}^{-}(_{k}) ^{-}[N_{+}>](T-k) -(q-)k.\]

Summing the above two expressions, we obtain

\[R_{T}^{+}(_{k})+R_{T}^{-}(_{k})(^{+} [N_{+}]+^{-}[N_{+}> ])(T-k)-2(q-)k.\] (7)At this point, we apply the Bretagnolle-Huber Inequality [Theorem 14.2 in Lattimore and Szepesvari, 2020] to bound the first term on the right-hand side of the above inequality:

\[^{+}[N_{+}]+^{-}[N_{+}> ](-_{}(^{+}_{Z_{ T}},^{-}_{Z_{ T}})),\]

where \(^{}_{Z T}\) is the push-forward measure on all the possible sequences of feedback observed by \(_{k}\) under \(^{}\). The lemma is concluded by combining the above inequality with the following claim, and plugging it into (7).

**Claim 3.2**.: _It holds that \(_{}(^{+}_{Z_{ T}},^{-}_{Z_{  T}})}{q}^{+}[N_{F}].\)_

Proof.: Let us observe that, once we fix the feedback history until time \(t-1\), i.e., fix a realization of the feedback \(Z_{ t-1}\), we have that

\[_{}(^{+}_{Z_{t}|Z_{ t-1}},^ {-}_{Z_{t}|Z_{ t-1}})=\{t F\}_{}(^{+}_{Z_{t}|t F},^{-}_{Z_{t}|t F }),\]

where \(^{+}_{Z_{t}|Z_{ t-1}}\) (respectively \(^{-}_{Z_{t}|Z_{ t-1}}\)) is the push-forward measure over \(\{0,1\}^{2}\) when losses are drawn according to \(^{+}\) (respectively \(^{-}\)), conditioning on the previous observations. The equality above holds because (i) algorithm \(_{k}\) observes feedback if and only \(t F\) (by definition), and (ii) \(_{k}\) is deterministic and whether or not \(t F\) may only depend on the past. We now upper bound the KL-divergence term above:

\[_{}(^{+}_{Z_{t}|Z_{ t-1}},^{-}_{Z_{t}|Z_{ t-1}}) =(q+)(1+)+(q-)(1-)\] \[(q+)-(q- )=q} {q^{2}-^{2}}}{q},\]

where the first inequality follows from \((1+z) z\) for all \(z\), and the last holds as long as we choose \(<}\). To complete our derivation, we express the overall KL divergence exploiting the tower property of conditional expectation:

\[_{}(^{+}_{Z_{ T}},^{-}_{Z_{  T}})=_{t[T]}^{+}[_{} (^{+}_{Z_{t}|Z_{ t-1}},^{-}_{Z_{t}|Z_{ t-1}} )]}{q}^{+}[N_{F}],\]

where expectation is taken over all possible feedback realizations \(Z_{ t-1}\), and the last inequality follows by earlier derivations. 

This concludes the proof of the lemma. 

We now show how to use the above lemma to derive the lower bounds. We start with full feedback.

**Theorem 3.3**.: _In the full feedback query model, for all \(k[T]\), we have the following lower bounds:_

* _For any algorithm_ \(_{k}\) _that has access to_ \(k<c_{0}\) _queries, it holds that_ \(R_{T}(_{k}) c_{0}}{4}\)_._
* _For any algorithm_ \(_{k}\) _that has access to_ \(k c_{0}\) _queries, it holds that_ \(R_{T}(_{k}) c_{1}\)_._

_Where \(c_{0}=}{{(^{8})}}\) and \(c_{1}=}{{(320e^{2})}}\) are universal constants._

Proof.: In full feedback, the algorithm always observes the losses, so that the feedback variable \(Z_{t}=_{t}\) and \(N_{F}=T\). We prove the Theorem via Yao's minimax Theorem: we prove that any deterministic algorithm \(_{k}\) fails against the random instance composed as follows: with probability \(}{{2}}\) the losses are drawn i.i.d. according to \(^{+}\), otherwise, they are drawn i.i.d. according to \(^{-}\). We can then apply Lemma 3.1 and obtain that the expected regret of \(_{k}\) against such mixture is equal to

\[[R_{T}(_{k})]=(R_{T}^{+}(_{k})+R_{T}^{-}(_{k}))(-}{q}T )-2(q-)k.\] (8)Now, if \(k<c_{0}\), we set \(=}\) and \(q=\) in (8) to get

\[[R_{T}(_{k})]=(R_{T}^{+}(_{k})+R_{T}^{-}(_{k}))}{2e^{8}}-}{4e^{8}}}{4e^{8}},\]

Otherwise, if \(k c_{0}\), then consider the following choice of the parameters: \(=+\) and \(q=5^{2}T=k^{2}}+k}+}{320e^{2}T}\). Plugging these parameters in (8), we get:

\[[R_{T}(_{k})]=(R_{T}^ {+}(_{k})+R_{T}^{-}(_{k})) -5^{2}kT+(1-) k\] \[=k}+}+k}{32 0e^{2}T}k}.\]

A similar analysis can be carried over for the label efficient setting.

**Theorem 3.4**.: _In the label efficient feedback model, for all \(k[T]\), we have the following lower bounds:_

* _For any algorithm_ \(_{k}\) _that has access to_ \(k<c_{0}}\) _queries, it holds that_ \(R_{T}(_{k}) c_{0}}\)_._
* _For any algorithm_ \(_{k}\) _that has access to_ \(k c_{0}}\) _queries, it holds that_ \(R_{T}(_{k}) c_{1}}{k^{2}}\)_._

_Where \(c_{0}=}{{(e^{8})}}\) and \(c_{1}=}{{(320e^{2})}}\) are universal constants._

Proof.: In the label efficient feedback model, \(Z_{t}\) is meaningful only for times in \(F\). We then prove the result by Yao's minimax principle, by showing that any deterministic algorithm \(_{k}\) which issues at most \(k\) queries suffers the desired regret against the instance that uniformly chooses between \(^{+}\) and \(^{-}\). We can apply Lemma 3.1 (nothing that \(N_{F} k\)) to get:

\[[R_{T}(_{k})]=(R_{T}^{+}(_{k})+R_{T}^{-}(_{k}))(-}{q}k )-(q-)k.\]

Once again, we have two cases. If \(k<c_{0}}\), we choose \(=}\) and \(q=\) to get

\[[R_{T}(_{k})]=(R_{T}^{+}(_{k})+R_{T}^{-}(_{k}))}- }}.\]

Otherwise, if \(k c_{0}}\), and we choose \(=}+\) and \(q=5^{2}k=}{320e^{2}k^{3}}+k^{2}}+ }{320e^{2}k}\), to obtain

\[[R_{T}(_{k})]=(R_{T}^ {+}(_{k})+R_{T}^{-}(_{k})) -5^{2}k^{2}+(1-) k\] \[=}{320e^{2}k^{2}}+k}+}{320e^{2}}}{320e^{2}k^{2}}.\]

## 4 Conclusions

Our work introduces _best-action queries_ in the context of online learning. We provide tight minimax regret in both the full feedback model and in the label efficient one. We establish that leveraging a sublinear number of best action queries is enough to improve significantly the regret rates achievable _without_ best-action queries. Promising avenues for future research involve integrating best-action queries with diverse feedback forms, extending beyond full feedback, such as bandit feedback, partial monitoring, and feedback graphs (where, in particular, Lemma 2.1 does not hold). Moreover, our work only studies the case where queries are _perfect_, i.e., the queried oracle gives the correct identity of the best action at that time step with probability \(1\). Imagining a _noisy_ oracle that gives the correct identity of the best action only with probability \(}{{n}}+\) is also an interesting future direction this work leaves open.