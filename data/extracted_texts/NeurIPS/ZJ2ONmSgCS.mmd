# DiffHammer: Rethinking the Robustness of

Diffusion-Based Adversarial Purification

 Kaibo Wang1, Xiaowen Fu1, Yuxuan Han1, Yang Xiang1,2

1Department of Mathematics, The Hong Kong University of Science and Technology

2HKUST Shenzhen-Hong Kong Collaborative Innovation Research Institute

{kwangbi, xfuak, yhanat}@connect.ust.hk, maxiang@ust.hk

Corresponding author

###### Abstract

Diffusion-based purification has demonstrated impressive robustness as an adversarial defense. However, concerns exist about whether this robustness arises from insufficient evaluation. Our research shows that EOT-based attacks face gradient dilemmas due to global gradient averaging, resulting in ineffective evaluations. Additionally, 1-evaluation underestimates resubmit risks in stochastic defenses. To address these issues, we propose an effective and efficient attack named DiffHammer. This method bypasses the gradient dilemma through selective attacks on vulnerable purifications, incorporating \(N\)-evaluation into loops and using gradient grafting for comprehensive and efficient evaluations. Our experiments validate that DiffHammer achieves effective results within 10-30 iterations, outperforming other methods. This calls into question the reliability of diffusion-based purification after mitigating the gradient dilemma and scrutinizing its resubmit risk.

## 1 Introduction

The vulnerability of deep neural networks (DNNs) to adversarial samples hinders their application in security-critical domains, where attackers can deceive DNNs by introducing carefully crafted noises . To mitigate this issue, numerous defense strategies have been proposed to enhance their robustness, among which _diffusion-based purification_ has emerged as a promising approach . Diffusion models  are designed to construct stochastic processes from noisy data distributions to cleaner ones. As a result, the presence of small adversarial noise can be drowned out by larger noise, which can then be iteratively denoised using the diffusion model  or purified through a diffusion-involved optimization . The iterative algorithm and stochasticity of diffusion enhance their purification capabilities empirically, yet they also present a challenge in evaluating their robustness . There are ongoing concerns regarding their effectiveness:

_Whether their effectiveness originated from inherent robustness or insufficient evaluation?_

The Expectation of Transformation (EOT)  method allows attacks to adapt across stochastic purifications by averaging the gradients of sampled purifications. It maximizes attack success rate under the assumption that most purifications share a common vulnerability, i.e., susceptible to a same adversarial noise. However, high stochasticity in diffusion-based purification challenges this assumption, rendering EOT-based attacks ineffective. Purifications with unshared vulnerability (\(_{0}\), shown in Figure 1) will provide inconsistent gradients in the attack, leading to a _gradient dilemma_. Moreover, obtaining gradients for purification is time-consuming, and this dilemma further increases computational overhead.

To address this, we introduce a selective attack method, _DiffHammer2_, based on expectation maximization (EM) . As shown in Figure 1, our algorithm bypasses the gradient dilemma by iteratively identifying vulnerable purifications in the E-step and aggregating their gradients in the M-step. This approach focuses on shared vulnerabilities, akin to using a hammer on weak spots rather than the entire structure. Additionally, we enhance attack efficiency by reducing the diffusion process's backpropagation complexity from \((N)\) to \((1)\) through gradient grafting.

Another issue with diffusion-based purification is insufficient evaluation. Attackers aiming for a single success, such as logging in, can achieve a higher chance of success by resubmitting an adversarial sample to the stochastic defense . This resubmit risk is underestimated in 1-evaluation tailored for deterministic defenses, especially given the high stochasticity of the diffusion process. To address this, we upgraded 1-evaluation to \(N\)-evaluation and seamlessly integrated it into our DiffHammer framework, offering a more comprehensive risk assessment without extra time costs.

We revisit the robustness of diffusion-based purification within the DiffHammer framework. DiffHammer surpasses other state-of-the-art attacks [4; 21; 13; 16; 1; 2] by circumventing the gradient dilemma and achieving near-optimal effectiveness within 10-30 iterations. Under \(N\)-evaluation protocol and our selective attack, we find that the risk associated with diffusion-based purification is significantly underestimated. Mainstream purifications [22; 26; 3] fail to withstand even 10 resubmit attacks and may degrade the performance of robust models, indicating that their robustness potential remains underutilized. We hope that DiffHammer's insights into evaluating diffusion-based purification robustness will foster more robust defenses. Our main contributions are summarized as follows:

* We identified the limitation of EOT-based attacks and proposed an efficient and effective evaluation, termed DiffHammer, to comprehensively diagnose the adversarial risks of diffusion-based purification.
* We introduce a selective attack that avoids the gradient dilemma by targeting the shared vulnerabilities, with the process expedited by gradient grafting.
* We validate the effectiveness of DiffHammer through extensive experiments on mainstream purifications, revealing underestimated risks through standardized \(N\)-evaluation.

## 2 Preliminary

### Adversarial attacks

Given an image \(x^{d}\) and its label \(y[K]\), a classifier \(f:^{d}^{K}\) with a preprocessor \(:^{d}^{d}\) will classify it as \(f[(x)]=_{k=1,,K}f_{k}[(x)]\), where \(K\) is the number of classes. The attacker aims to add an imperceptible adversarial noise \(r\) on the image to make it misclassified, i.e., \(f[(x+r)] y\). Adversarial noise is typically obtained by maximizing the loss

Figure 1: Illustration of DiffHammer. From left to right: (a) Set of purifications with unshared (\(_{0}\)) / shared (\(_{1}\)) vulnerabilities. (b,c) Maximizing the loss function on \(_{0}_{1}\) may lead to less effective attacks than on \(_{1}\). (d,e) Selective attack targets on \(_{1}\) to avoid the gradient dilemma. We identify purifications from \(_{1}\) in the E-step and aggregate the gradients of these purifications in the M-step.

function \(\) while imposing \(_{p}\)-norm constraint for imperceptibility. For instance, projected gradient descent (PGD)  updates \(\|r\|_{}\) bounded adversarial samples as:

\[r^{(t+1)}=_{\|r\|_{}}(r^{(t)}+_{x} (f[(x+r^{(t)})],y)),\] (1)

where \(\) is step size and \(\) indicates a projection operator.

Preprocessors tailored to defend against adversarial noise may lead to vanishing, exploding, or inaccurate gradients. Attackers can resort to approximating the gradient using Backward Pass Differentiable Approximation (BPDA) , where the preprocessor is typically treated as an identity mapping during the backpropagation:

\[_{x}(f[(x+r^{(t)})],y)_{x+r^{(t)}}_{x} (f[(x+r^{(t)})],y)_{(x+r^{(t)})}.\] (2)

EOT  can be utilized as an additional approach to estimate gradients of stochastic preprocessors, whose effectiveness is derived from the assumption that different preprocessors have shared vulnerabilities. Given \(N\) sampled preprocessors \(_{i}\), the estimated gradient is:

\[_{x}(f[(x+r^{(t)})],y)_{i=1}^{N} _{x}(f[_{i}(x+r^{(t)})],y).\] (3)

### Diffusion-based purification

The diffusion model [11; 24; 14] establishes a link between the clean data distribution \(p_{0}(x)\) and the noisy distribution \(p_{1}(x)\) through forward (noising) and backward (denoising) processes. The forward process \(x_{t}\) can be expressed as a stochastic differential equation (SDE) for \(t\) from 0 to 1 :

\[dx=(x,t)dt+(t)dw,\] (4)

where \(x_{0} p_{0}(x)\), \(w_{t}^{d}\) is a standard Wiener process, \(:^{d}^{d}^{d}\) is the drift coefficient and \(:\) is the diffusion coefficient which is designed so that \(p_{1}(x)\) follows a standard Gaussian distribution \((0,I_{d})\). DDPM  can be viewed as a special case of \((x,t):=-(t)x/2\) and \((t):=\), where \((t)\) is the noise scheduler, usually set as a linear function w.r.t. \(t\).

The evolution of reverse-time SDE (\(t\) from 1 to 0) then corresponds to the generation of samples:

\[d=[(,t)-(t)^{2}_{}p_{t}()]dt+(t)d,\] (5)

where \(dt\) is an infinitesimal negative time step and \(_{t}\) is a standard reverse-time Wiener process, and \(_{}p_{t}()\) is known as time-dependent score function and is typically estimated by the neural network.

Well-trained diffusion models possess the ability to accurately model the score function and denoise noisy samples, enabling adversarial purification. Considering the small magnitude of adversarial noise, Diffpure  therefore floods the adversarial noise while preserving the semantic information by running a forward process from 0 to \(t^{*}\), and then purifies it through a denoising process. GDMP  leverages the distance from the initial samples as guidance to further preserve semantic information. Leveraging the fact that diffusion model approximates the score function of \(x p_{0}(x)\) more accurately, likelihood maximization (LM)  optimizes adversarial samples to minimize estimation error, aligning their distribution with \(p_{0}(x)\).

## 3 DiffHammer

To address ineffectiveness in the attack phase and insufficiency in the evaluation phase, we propose DiffHammer to assess the robustness of diffusion-based purification. First, DiffHammer overcomes the gradient dilemma by designing a _selective attack_ (Section 3.1) using the EM algorithm. Furthermore, we seamlessly integrate \(N\)_-evaluation_ (Section 3.2) into the algorithm, enabling more accurate resubmit risk diagnosis and facilitating the attack by providing approximate gradients.

### Selective attack

#### 3.1.1 EM algorithm

**Notation and objective.** For a sample \(x^{d}\) with label \(y[K]\), we aim to design adversarial noise \(r^{d}\) to mislead a classifier \(f\) with a stochastic purification \(:^{d}^{d}\). Let \(\) represent the misclassification event where \(f[(x+r)] y\). Our goal is to maximize the probability of misclassification, expressed as \(P( r)=_{}p( r)\). Here \(P\) is the probability density associated with the stochastic purification \(\), and \(p\) denotes the probability density in logits of \(f\) given a specific \(\). We refer to the gradient of loss \((f[(x+r)],y)\) w.r.t. \(x\), \((x+r)\) as the _full gradient_ (\(_{x}_{}\)) and the _approximate gradient_ (\(_{(x+r)}_{}\)), respectively. Without ambiguity, we sometimes abbreviate the loss as \(_{}\) and gradients as \(_{x}_{}\), \(_{}_{}\).

**Assumption.** We assume that diffusion-based purifications have unshared vulnerabilities, dividing \(\) into two sets. We denote \(_{1}\) as the largest set of \(\) with shared vulnerability, i.e., \(_{1}= P(\{:f[(x+r^{})] yr^{}\})\) and \(_{0}=_{1}\) as the set of \(\) compromised by inconsistent adversarial noise \(r\). We assume that optimization towards \(_{0}\) suffers from the _gradient dilemma_, leading to ineffective attacks. Our task is to identify \(_{1}\) and design \(r\) for \(_{1}\).

Indicating whether a \(\) belongs to \(_{1}\) or \(_{0}\) by \(z=1\) or \(0\), we denote \(q(z)\) as the estimated distribution for \(z\) for given \(\). According to the Jensen's inequality, we maximize a lower bound of our goal:

\[_{}_{z=0,1}p(,z r)=_{} q(z),z r)}{q(z)}}_{ (q,r)}q(z),r)}{ q(z)}}_{(q\|p(z,r))}\] (6)

where \((q,r)\) represents the evidence lower bound, and \((q p(z,r))\) is the KL divergence between \(q\) and the posterior distribution of \(z\). Given the coupling of variables \(r\) and \(z\) in the optimization process, we employ the EM algorithm  as a solver that alternates between optimizing \(r\) (maximizing \((q,r)\)) in the _M-step_ and estimating \(z\) (minimizing KL divergence) in the _E-step_.

**M-step.** During the M-step, we maximize \((q,r)\) w.r.t. \(r\) to raise the objective's lower bound while keeping \(q(z)\) fixed. By omitting terms unrelated to \(r\), i.e., \(q(z) q(z)\) and the priors \( p(z=0), p(z=1)\), the goal simplifies as

\[_{r}(q,r)_{r}q(z=1) p( r, z=1)+q(z=0) p( r,z=0)\] (7)

where the second term is further disregarded in the optimization according to our assumption.

In adversarial attacks, the objective \(_{} p( r,z=1)\) is typically replaced by maximizing the loss function \(_{}(x+r)\), achieved through average gradient-based methods. Consequently, the M-step update can be integrated into existing attack algorithms like PGD  and AA  as a _plug-in_, with the difference being the reweighting of the gradient from each \(\) by \(q(z=1)\). We further employ stepwise-EM  to linearly interpolate the current gradient and the previous gradient with weight \(t^{-}\) for online updating, where \(t\) is the number of iterations and \(\) is a hyperparameter. This approach intuitively optimizes the adversarial noise towards the shared vulnerability of \(_{1}\), thus avoiding the gradient dilemma in \(_{0}\).

**E-step.** During the E-step, we update \(q(z=1)\) to \(p(z=1,r)\) for a given \(r\), thereby eliminating the KL divergence, which is the gap between the objective and its lower bound. Notice that \(_{1}\) is defined as the _largest_ set of \(\) susceptible to the same adversarial noise \(r^{}\), where usually \(r^{} r\). We can estimate \(p(z=1,r)\) by approximating \(r^{}\) with by-products \(_{}(x+r)\) and \(_{x}_{}_{x+r}\) in the observation of misclassification event \(\). Denote the difference between \(r^{}\) and \(r\) as \( r\), the loss \(_{}(x+r^{})\) w.r.t. \(r^{}\) can be linearly approximated since \( r\) is not excessively large, with both \(r\) and \(r^{}\) bounded by \(\|\|_{p}\):

\[_{}(x+r^{})_{}(x+r)+ r^{T} _{x}_{}_{x+r}\] (8)

Higher loss increases the likelihood of misclassification, so we map \(_{}(x+r^{})\) to \(p( r^{})\) using a monotonically increasing function \(:\). By the definition of \(_{1}\), \(r^{}\) compromises as many \(\) as possible. We find \(r^{}\) through the following optimization:

\[r^{}=r+ r=r+_{}(_{}(x+r)+  r^{T}_{x}_{}_{x+r})\] (9)

After determining \(r^{}\), the probability \(q(z=1)\) that \(\) belongs to \(_{1}\) can be estimated as \((_{}(x+r^{}))\).

_Remark._ We use the empirical average w.r.t. \(N\) instances \(_{i},i=1,,N\) in each iteration to estimate expectations. In the E-step, we optimize \(r^{}\) in a _low-cost_ manner, as no model is involved. We reweight the gradient by \((_{_{i}}(x+r^{}))\) in the M-step and then use it in an off-the-shelf attack algorithm to update the adversarial noise \(r\). The primary time cost arises from calculating the gradient \(_{x}_{_{i}}\) due to the complexity of the purification process. In Section 3.1.2, we will describe how to reduce the \((N)\) complexity to \((1)\) through gradient grafting.

#### 3.1.2 Gradient grafting

To address the efficiency challenges in robustness evaluation for diffusion-based purification, we aim to estimate the weighted gradient aggregation \(_{i}q_{i}(z=1)_{x}_{_{i}}\) with minimal computational cost. Our approach aggregates low-cost approximate gradient \(_{_{i}}_{_{i}}\) in the early stage to estimate the weighted full gradient, which can be expressed as

\[_{i=1}^{N}q_{i}(z=1)_{x}_{_{i}}=_{i=1}^{N}q_{i}(z=1 )}{ x}_{_{i}}_{_{i}} }{ x}_{i=1}^{N}q_{i}(z=1)_{ _{i}}_{_{i}},\] (10)

assuming \(_{i}/ x\) can be approximated by \(/ x\). As illustrated in Figure 2, We first compute the weighted approximate gradient \(_{i=1}^{N}q_{i}(z=1)_{_{i}}_{_{i}}\), which is then grafted onto \(\) for backpropagation to estimate the gradient expectation, reducing complexity in backpropagation \(x_{i}(x)\) to \((1)\). BPDA  uses \(I\) to approximate \(_{i}/ x\), offering computational simplicity but potentially compromising performance due to oversimplification. Our gradient grafting enhances estimation with one additional backpropagation, achieving a better balance between efficiency and effectiveness. Further related design details are discussed below.

**E-step in \(\) stage.** Although \(r^{}\) in E-step (Equation 9) involves the full gradient \(_{x}_{_{i}}\), our main interest is in \((_{}(x+r^{}))\). Therefore, we can optimize in the stage of \(\) rather than \(x\) to avoid dependence on the full gradient. We express \(\) in the stage of \(\) as \(=(x+r^{})-(x+r)\). The optimization, aimed at _attacking as many \(\) as possible_, becomes \(_{}(_{}(x+r)+^{T} _{}_{})\), relying only on the approximate gradient. We end up deriving \((_{}(x+r^{}))\) from optimized \(\).

**Choice of \(\).** We aim to select a \(\) with representative vulnerability for backpropagation, whose adversarial noise can also affect other \(\). For a single-step attack on \(\) with adversarial noise \(g(_{}_{})\) (e.g. \((_{}_{})\) in \(_{}\) case), we choose \(\) among \(_{i}\) using the strategy:

\[=*{arg\,max}_{\{_{i},i=1,,N\}}_{i= 1}^{N}[_{_{i}}(x+r)+g(_{}_{})^{T }_{_{i}}_{_{i}}].\] (11)

This consistent optimization goal of _attacking as many \(\) as possible_ maintains the shared vulnerability from the \(\) stage to the \(x\) stage. The choice of \(\) involves discrete optimization within a finite set, solvable by traversal when \(N\) is not very large.

Figure 2: Illustration of efficient gradient aggregation.

#### 3.1.3 Discussion

Our selective attack (targeting \(_{1}\)) enhances both the _effectiveness_ and _efficiency_ of the EOT-based attack (targeting \(_{0}_{1}\)). (1) The EOT-based attack is a specific instance of our algorithm when all \(\) share a common vulnerability. In this case, with \(_{0}=,q(z=1)=1\), our selective attack degenerates into an EOT-based attack without side effects. (2) The identification of \(_{1}\) involves only approximate gradients, making it nearly cost-free. The grafting trick further increases the efficiency of gradient expectation. (3) The effectiveness of the selective attack arises from avoiding the gradient dilemma, which can cause EOT-based attacks to fail even in simple binary cases.

**Theorem 1** (Failure mode of EOT-based attacks, Proof in Appendix B.1).: _Suppose \(\) can be divided into two sets, \(A\) and \(B\). The loss functions w.r.t. \(r\) in these sets are defined as \(_{A}(r):=^{-1}(P_{ A}( r))\) and \(_{B}(r):=^{-1}(P_{ B}( r))\), which are \(m_{A}\) and \(m_{B}\) strongly concave, respectively. If the distance between their optimal points \(r_{A}\) and \(r_{B}\) satisfies \(\|r_{A}-r_{B}\|_{2}^{2} 8\{P(A)_{A}(r_{A}),P(B)_{ B}(r_{B})\}/m\) where \(m:=\{P(A)m_{A},P(B)m_{B}\}\), the EOT-based attack is less effective than a simple attack targeting either \(A\) or \(B\)._

When \(r_{A}\) and \(r_{B}\) are significantly different, \(_{1}\) tends to be \(A\) or \(B\), whereas the other becomes \(_{0}\) and provides neutralized gradients. Empirical results indicate that the gradient dilemma is common in diffusion-based purification, highlighting the necessity of selective attacks.

### In-loop N-evaluation

Traditional 1-evaluation is insufficient for assessing the robustness of diffusion-based purification, particularly against _resubmit attacks_. As a stochastic defense, the model produces inconsistent results for even the same queries, enabling attackers to achieve desired outcomes through re-submissions. In cases where attack costs are manageable and even a single success is advantageous, e.g., login, defenders should focus on the _model's robustness over \(M\) resubmissions_. Thus, we propose using \(N\)-evaluation as a robustness evaluation protocol for two reasons:

(1) As detailed in Theorem 2, 1-evaluation are significantly biased in estimating \(M\)-resubmit risk. Due to high stochasticity, it is common for \(\) to have unshared vulnerabilities, as shown in Figure 3. Merely success or failure record in 1-evaluation fails to capture critical probability information for resubmit risk estimation. For instance, with DiffPure, 46.3% of samples have a attack success rate (ASR) \(P(_{1})(0,1)\), leading to a 17.9% overestimation of 10-resubmit robustness in 1-evaluation (\(^{(1)}_{MLE}=41.7\%\), and \(Rob=59.6\%\)).

**Theorem 2** (Estimation of the resubmit risk, Proof in Appendix B.2).: _Let the sample's robustness in \(M\) resubmit attacks \(_{1},,_{M}\) be denoted as \(Rob:=P(_{1}==_{M}=0)\). There exists a uniformly minimum-variance unbiased estimator (UMVUE) for \(Rob\) if and only if the number of evaluation trials \(N M\). When \(N M\), the maximum likelihood estimator (MLE) tends to overestimate \(Rob\) in expectation, i.e., \((^{(N)}_{MLE}) Rob\)._

(2) \(N\)-evaluation can be integrated into the attack's loop without extra burden. Samples \(_{i},i=1,,N\) serve both as an evaluation for the previous iteration and input for the current, which will not cause information leakage, as each evaluation involves unseen instances.

Our DiffHammer framework integrates selective attacks and \(N\)-evaluation, as detailed in Algorithm 1. Notably, our \(N\)-resubmit robustness extends the traditional 1-submit metric to better assess risks in real-world deployments, orthogonal to our attack algorithm's design. Consequently, DiffHammer offers a more comprehensive risk assessment and enhances attack effectiveness across various metrics.

Figure 3: Distribution of attack results (\(_{}:8/255\)) for 1-evaluation (inner ring) and 10-evaluation (outer ring). 32.6%-46.3% of the samples have unshared vulnerabilities, imposing underestimated resubmit risk in 1-evaluation.

## 4 Experiments

### Experimental Setup

**Baselines.** We evaluated the robustness of three diffusion-based purification defenses: DiffPure , GDMP , and LM . For purification, we used a pre-trained score-based diffusion model  in DiffPure and GDMP and an EDM model  in LM. To ensure a fair comparison, we employed the WideResNet-70-16  as the classifier across all tests.

We selected three state-of-the-art attack algorithms equipped with EOT  for baseline evaluation: BPDA , PGD , and AA . For DiffPure and GDMP, AA was upgraded to DA  by incorporating deviated-reconstruction loss. Our Diffhammer adopts AA as the default attack algorithm. We conducted three restarts totaling 150 iterations to thoroughly evaluate the robustness of the model. Additional configurations for defense and attack are detailed in Appendix C.1. Substitute gradient attacks [22; 31] tailored to diffusion-based processes are found to be inferior to PGD with full gradients , so we leave comparisons with these methods in the Appendix C.4. Additionally, we evaluate verifiable DiffSmooth classifiers , and the results are presented in the Appendix C.3.

**Evaluation metrics**. Consistent with prior work, we use subsets of the CIFAR10 , CIFAR100 , and ImageNettete  (a subset of 10 easily classified classes from Imagenet , more suited for robustness evaluation) with sizes of 512, 512, and 256 as datasets, respectively. The evaluation protocol is \(N\)-evaluation with \(N=10\), with the average robustness (Avg.Rob, \(1-_{i,j}_{i}^{(j)}/NS\)) and worst-case robustness (Wor.Rob, \(1-_{i}(_{i}_{i}^{(j)})/S\)) as metrics. Here \(_{i}^{(j)}\) indicates whether sample \(j\) was attacked at the \(i\)-th evaluation, and \(S\) is the dataset size. We also reported the iterations taken to reach 90% of the best attack effect among all attacks as a metric of efficiency. Experimental results for CIFAR100 can be found in Appendix C.2, and adversarial samples visualization are shown in Appendix C.9.

    Defense \\ Metrics \\  }} &  &  &  \\  &  & Avg.Rob (it.)\(\) & Wor.Rob (it.)\(\) & Avg.Rob (it.)\(\) & Wor.Rob (it.)\(\) & Avg.Rob (it.)\(\) & Wor.Rob (it.)\(\) \\   Deep \\ Deep \\  } & Clean & 90.98 & 76.56 & 93.26 & 83.79 & 87.77 & 74.61 \\   & BPDA & 76.27 (N/\(\)) & 40.82 (126) & 80.61 (N/\(\)) & 52.34 (137) & 69.57 (N/\(\)) & 38.67 (N/\(\)) \\  & DA/AA & 71.52 (N/\(\)) & 40.04 (118) & 73.52 (N/\(\)) & 50.78 (44) & 46.29 (N/\(\)) & 25.78 (N/\(\)) \\  & PGD & 69.80 (125) & 41.02 (114) & 72.32 (N/\(\)) & 50.00 (44) & 40.55 (N/\(\)) & 20.90 (89) \\  & DH & **66.62 (26)** & **35.16 (26)** & **68.36 (18)** & **47.27 (13)** & **29.63 (21)** & **14.45 (16)** \\   Deep \\ Deep \\  } & BPDA & 70.74 (N/\(\)) & 36.72 (N/\(\)) & 80.57 (N/\(\)) & 51.95 (N/\(\)) & 55.27 (N/\(\)) & 27.54 (N/\(\)) \\  & DA/AA & 57.60 (N/\(\)) & 33.79 (N/\(\)) & 52.83 (N/\(\)) & 37.70 (N/\(\)) & 32.56 (N/\(\)) & 17.97 (N/\(\)) \\  & PGD & 52.73 (N/\(\)) & 31.05 (112) & 49.41 (N/\(\)) & 36.91 (N/\(\)) & 17.99 (31) & 9.38 (31) \\  & DH & **42.54 (20)** & **22.66 (17)** & **41.64 (17)** & **27.54 (13)** & **16.15 (17)** & **8.01 (14)** \\  & DMI\({}^{}\) & 45.64 (41) & 25.20 (35) & 43.40 (31) & 32.42 (27) & 38.81 (N/\(\)) & 23.83 (N/\(\)) \\  & TMI\({}^{}\) & 45.04 (39) & 25.20 (38) & 45.43 (37) & 34.77 (30) & 41.13 (N/\(\)) & 25.59 (N/\(\)) \\  & VMI\({}^{}\) & 50.55 (N/\(\)) & 28.71 (44) & 50.76 (N/\(\)) & 37.11 (44) & 21.97 (39) & 11.72 (32) \\  & SVRE\({}^{}\) & 59.12 (N/\(\)) & 32.81 (N/\(\)) & 60.37 (N/\(\)) & 42.77 (N/\(\)) & 36.11 (N/\(\)) & 19.53 (136) \\   Deep \\ Deep \\  } & BPDA & 79.36 (N/\(\)) & 45.31 (110) & 86.41 (N/\(\)) & 58.40 (N/\(\)) & 75.02 (N/\(\)) & 46.68 (147) \\  & DA/AA & 79.92 (N/\(\)) & 46.29 (121) & 85.80 (N/\(\)) & 59.57 (N/\(\)) & 74.96 (N/\(\)) & 46.48 (135) \\   & PGD & 78.38 (N/\(\)) & 44.53 (100) & 83.57 (N/\(\)) & 56.25 (96) & 72.91 (N/\(\)) & 43.95 (75) \\   & DH & **74.49 (46)** & **41.41 (44)** & **78.83 (46)** & **53.12 (46)** & **68.54 (38)** & **41.02 (39)** \\   

Table 1: Performance of attacks against diffusion-based purification on CIFAR10. Metrics include Avg./Wor. Rob, (%) and iterations (it.) taken to reach 90% best performance (in parentheses, failure to reach is noted as N/\(\)). Attack algorithms include SOTA white-box attacks with EOT: BPDA , PGD , AA , DA , and DiffHammer (DH, ours); and transfer-based attacks (denoted by \(\)): DMI , TMI , VMI , SVRE .

### Effectiveness and efficiency of DiffHammer

We examine the effectiveness and efficiency of various attack methods under different settings, with results for CIFAR and ImageNettette presented in Table 1,2, respectively. Our findings are as follows. (1) DiffHammer shows superior attack effectiveness across datasets and different norms (\(_{},_{2}\)) or attack budgets. This effectiveness arises from selective gradient aggregation, which avoids the gradient dilemma. Its performance improves significantly in high-stochasticity scenarios with pronounced gradient dilemmas, e.g., large-scale datasets like ImageNettette or GDMP with multiple purification rounds. (2) As shown in Figure 4 (other settings can be found in Appendix C.5), DiffHammer typically requires only 10-30 iterations to reach near-optimal results, allowing for rapid model robustness assessment. Other methods might occasionally bypass sampling from \(_{0}\) for similar effectiveness, which incurs unnecessary computational costs. (4) Most models show robustness below 50% with 10 resubmits, indicating that a limited number of resubmits can undermine diffusion-based purification, raising concerns about their reliability in practical applications.

### Gradient dilemma and transfer-based attack

We verify the gradient dilemma in diffusion-based purification by examining clustering effects and forgetting phenomena. Our EM algorithm clusters the gradients \(_{x}_{}\) into \(_{0}\) and \(_{1}\). The distribution of silhouette coefficients (SC) using cosine similarity, as displayed in Figure 5(a), indicates that gradients in these sets differ significantly. A direct result of this gradient dilemma is _attack forgetting_--where gradients \(_{x}_{}\) in consecutive iterations are inconsistent, causing the effects of previous attacks to be forgotten. As the ASR in \(t-1\) iteration shown in Figure 5(b), DiffHammer maintains attack consistency by identifying \(_{1}\), thus enhancing efficiency. We provide a toy example explaining the gradient dilemma in Appendix C.6. This dilemma may arise from a non-clustered data distribution (e.g., different breeds of dogs in the dataset), which imposes divergent gravitational pulls for purification and inconsistent perturbations needed to corrupt their features.

Transfer-based attacks offer a potential solution by treating adversarial samples like _models_ and aiming to improve generalization on a _dataset_ of \(\). As shown in 1, Data augmentation-based approaches, DMI  and TMI , provide improvement in some cases. VMI  and SVRE , which aim to reduce gradient variance, perform worse. This unexpected outcome is attributed to the gradient dilemma: generalizing to \(\) from \(_{1}\) is beneficial, while generalizing to \(\) in \(_{0}\) may be harmful. Thus, DiffHammer acts as a data-selection approach, contributing to data-centric design in transfer attacks.

   } &  &  &  \\  &  &  & Avg.Rob (it.)\(\) &  & Avg.Rob (it.)\(\) &  & Avg.Rob (it.)\(\) &  \\    } & Clean & 97.03 & 95.31 & 97.11 & 94.53 & 96.41 & 94.53 \\   & BPDA & 58.98 (N/A) & 50.78 (N/A) & 57.66 (N/A) & 51.56 (N/A) & 28.28 (28) & 22.66 (21) \\  & DA/AA & 53.12 (N/A) & 46.09 (N/A) & 46.64 (N/A) & 39.06 (N/A) & 51.41 (N/A) & 42.19 (N/A) \\   & PGD & 54.30 (N/A) & 46.88 (N/A) & 48.28 (N/A) & 38.28 (N/A) & 55.31 (N/A) & 45.31 (N/A) \\   & DH & **38.36 (14)** & **31.25 (11)** & **33.98 (11)** & **28.91 (14)** & **26.25 (14)** & **21.88 (11)** \\   

Table 2: Performance of attacks against diffusion-based purification on ImageNettette .

Figure 4: Avg.Rob and Wor.Rob for the first 75 steps of different attacks with \(_{}:8/255\).

### Robust classifier

Diffusion-based purification has been explored to enhance the robustness of adversarially trained (AT) models . We revisited this concept using \(N\)-evaluation with TRADES  and AWP  as AT classifiers, as shown in Table 3. Our observations are as follows: (1) Robust models mitigate the gradient dilemma, resulting in similar performance for most attacks. This occurs because adversarial training implicitly regularizes the approximate gradient \(_{}_{}\), reducing the full gradient \(_{x}_{}\) of some \(\) towards zero. DiffHammer remains an effective evaluation tool, as diffusion-based purification occasionally reintroduces the gradient dilemma. (2) In most cases, diffusion-based purification weakens the robustness of the AT model. It improves robustness in a few instances but increases the risk of resubmit attacks by 11.7%-24.8%. Therefore, finding a better combination of diffusion-based purification and robust classifiers remains an open question.

### Ablation study

We conducted ablation experiments on different components in the default settings (grafted gradient \(/ x(q_{}_{}_{})\), \(=0.5\), \(N=10\)), with results shown in Table 4. The use of a selective approximate gradient \((q_{}_{}_{})\) leads to a significant performance drop, highlighting the importance of the purification gradient \(/ x\) in attacks. The grafted gradient causes only slight performance degradation compared to the full gradient \((q_{}_{x}_{})\) but avoids a \( 10\) time burden. We quantify the time cost of different approaches in the Appendix C.7. A smaller hyperparameter \(\) makes the algorithm rely more on the current iteration's gradient, and an appropriate \(=0.5\) achieves a better tradeoff in memorizing and learning. We tested effectiveness at 10-evaluations with \(N\) samples of \(\), finding that a proper \(N=10\) efficiently identifies \(_{1}\) for

   Ablations & DiffPure  & GDMP  & LM  \\  \(q_{}_{}\) & +27.44 / +12.70 & +30.02 / +17.38 & +41.29 / +20.70 \\ \(q_{}_{}\) & -3.52 / -1.17 & -3.09 / -1.35 & -0.63 / +0.32 \\ \(=0.2\) & +2.19 / +1.37 & +0.33 / -0.39 & +1.17 / +0.59 \\ \(=0.8\) & -0.16 / -0.39 & +0.63 / +1.17 & +1.23 / +1.17 \\ \(N=5\) & +1.44 / +2.34 & +1.52 / +2.73 & +7.48 / +4.89 \\ \(N=20\) & -1.41 / -1.77 & -0.47 / -0.59 & -0.51 / -0.47 \\   

Table 4: Performance difference (Avg.Rob / Wor.Rob, %) from default settings with \(_{}:8/255\).

Figure 5: Clustering effects and forgetting phenomen in gradient dilemma (\(_{}:8/255\)).

   Classifier & AWP  (Avg.Rob / Wor.Rob, Rob:60.0) \(\) & TRADES  (Avg.Rob / Wor.Rob, Rob:53.1) \(\) \\ Purification & DiffPure  & GDMP  & LM  & DiffPure  & GDMP  & LM  \\  BPDA & 53.53 / **35.15** & 58.40 / 45.70 & 56.87 / 42.38 & 50.50 / **31.82** & 54.98 / **41.38** & 53.15 / **39.26** \\ PGD & 53.30 / 36.32 & 58.57 / 47.26 & 69.37 / 56.84 & 50.70 / 35.54 & 52.73 / 41.60 & 64.37 / 52.54 \\ DA/AA & **51.75** / 35.54 & 58.71 / 47.85 & 65.27 / 52.34 & **49.27** / 34.37 & 52.65 / 42.38 & 59.78 / 49.02 \\ DH & 53.65 / 35.55 & **58.26** / **45.68** & **56.48** / **40.43** & 49.55 / 34.37 & **51.57** / 41.40 & **53.10** / 39.64 \\   

Table 3: Effectiveness (Avg.Rob / Wor.Rob, %) of different attacks on robust model with purification under \(_{}:8/255\) settings, including TRADES  and AWP . The original adversarial robustness (Rob) without purification is listed in parentheses.

evaluation. When \(N=20\), attack effectiveness is slightly boosted with more time overhead. The importance of \(N\)-evaluation for resubmit risk estimation is verified in the Appendix C.8.

## 5 Discussion and Insights

Diffusion models remain a promising solution to the adversarial samples due to their fine-grained modeling of data distributions. We advocate for enhancing the robustness of diffusion-based purification through standardized and powerful evaluation methodologies. Here are some insights into purification-based defense and attack:

1. For deploying stochastic defenses, defenders should consider the potential number of resubmissions \(M\) by attackers and are advised to assess resubmit risk with \(N\)-evaluation where \(N M\). Additionally, robustness overestimation due to the gradient dilemma can be avoided by using selective attacks.
2. On the attack side, adversarial samples are sensitive to defenses with high stochasticity. Thus, modern data-centric designs may help to enhance adversarial transferability.
3. On the defense side, the goal of stochastic defense is to achieve \(P(_{0}) 1\), meaning sample \(x\) cannot be attacked for some purifications and cannot be attacked by a same adversarial noise for others. Therefore, purification needs to be coordinated with adversarial training at a more granular level.

## 6 Related Work

**Evaluation for adversarial purification.** As test-time adaptive defenses, the iterative process and stochasticity of diffusion-based purification complicate robustness evaluation. Typically, evaluation protocols report only the ASR from a single evaluation. The risk of resubmit attack, referred to the _nag factor_, is considered in  but is not fully explored. Regarding evaluation methods, although there are gradient estimators _AdjAttack_ and attacks _score-attack_ based on the characteristics of the diffusion process, both were found to be inferior to attacks based on the exact gradient , such as PGD , AA , or DiffAttack  with reconstruction loss. These methods mitigate stochasticity through the EOT , which inevitably inherits its shortcomings.

**Transfer-based attacks.** Transfer-based attacks aim to generalize attacks from seen defenses to unseen defenses. Data augmentation and improved optimization are the main approaches to enhance transferability. Attackers can craft adversarial noise resistant to defenses through input transformations  and gradient smoothing . In the presence of multiple defenses, momentum , variance reducing [27; 30] enhance generalizability. Most transfer-based attacks assume that most defenses share vulnerabilities, enabling generalization in defenses, but this assumption may be invalid in diffusion-based purification.

## 7 Conclusion

In this paper, we address the limitations of EOT-based attacks in diffusion-based purification, attributed to the gradient dilemma, by introducing an effective and efficient method called DiffHammer. First, we propose a selective attack strategy that targets vulnerable purifications without encountering the gradient dilemma, enhancing evaluation efficiency through gradient grafting. Second, we incorporate \(N\)-evaluation within the loop to quantify the risk of achieving at least one successful attack in practice. We demonstrate DiffHammer's superior performance through comprehensive experiments and anticipate it will offer valuable insights for future designs of robust diffusion-based purification methods.