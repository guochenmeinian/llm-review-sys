# EvolveDirector: Approaching Advanced Text-to-Image Generation with Large Vision-Language Models

Rui Zhao\({}^{1}\), Hangjie Yuan\({}^{2}\), Yujie Wei\({}^{2,3}\), Shiwei Zhang\({}^{2}\), Yuchao Gu\({}^{1}\), Lingmin Ran\({}^{1}\),

**Xiang Wang\({}^{2,4}\), Zhangjie Wu\({}^{1}\), Junhao Zhang\({}^{1}\), Yingya Zhang\({}^{2}\), Mike Zheng Shou\({}^{1,}\)**

Corresponding Author.

\({}^{1}\)Show Lab, National University of Singapore

\({}^{2}\)Alibaba Group

\({}^{3}\) Fudan University

\({}^{4}\) Huazhong University of Science and Technology

###### Abstract

Recent advancements in generation models have showcased remarkable capabilities in generating fantastic content. However, most of them are trained on proprietary high-quality data, and some models withhold their parameters and only provide accessible application programming interfaces (APIs), limiting their benefits for downstream tasks. To explore the feasibility of training a text-to-image generation model comparable to advanced models using publicly available resources, we introduce EvolveDirector. This framework interacts with advanced models through their public APIs to obtain text-image data pairs to train a base model. Our experiments with extensive data indicate that the model trained on generated data of the advanced model can approximate its generation capability. However, it requires large-scale samples of 10 million or more. This incurs significant expenses in time, computational resources, and especially the costs associated with calling fee-based APIs. To address this problem, we leverage pre-trained large vision-language models (VLMs) to guide the evolution of the base model. VLM continuously evaluates the base model during training and dynamically updates and refines the training dataset by the discrimination, expansion, deletion, and mutation operations. Experimental results show that this paradigm significantly reduces the required data volume. Furthermore, when approaching multiple advanced models, EvolveDirector can select the best samples generated by them to learn powerful and balanced abilities. The final trained model Edgen is demonstrated to outperform these advanced models. The code and model weights are available at https://github.com/showlab/EvolveDirector.

## 1 Introduction

In the field of AI-generated content, an increasing number of advanced models have showcased their ability to generate realistic and imaginative images, such as Imagen , DALL-E 3 , Stable Diffusion 3 , Midjourney . While these models benefit from publicly available datasets such as ImageNet , LAION , and SAM , they rely more heavily on proprietary, high-quality data collections that surpass the quality of publicly accessible datasets. Models such as Midjourney  are particularly noted for deriving substantial benefits from their internal datasets. However, given the significant commercial advantages brought by their impressive capabilities, most advanced models keep their parameters private, hindering reproducibility and democratization. In this paper, we aim to _explore training an open-source text-to-image model with public resources to achieve comparable capabilities with the existing advanced models_.

## 1 Introduction

Figure 1: Images generated by our model Edgen (EvolveDirector-Gen). Edgen can generate high-quality images with multiple ratios and resolutions. Notably, it excels in generating text and avoiding attribute confusion when generating multiple objects, which are significant characteristics of the most advanced text-to-image models available today. The input text prompts are annotated under the corresponding images.

Despite the fact that internal data and model parameters of many advanced models remain inaccessible, they provide publicly accessible application programming interfaces (APIs) that enable users to access their generated distribution. This leads to the construction of synthetic benchmarks, _e.g._, JourneyDB  collects 4.7 million images generated by Midjourney . This benchmark is further utilized for enhancing the training of new generative models . However, this paradigm is not data efficient, posing challenges in terms of substantial computation and expenses. Instead of statically constructing multiple expensive large-scale datasets for each advanced model, we take a step forward in this paper by delving into recovering their generative capabilities in a unified framework with limited samples. We propose EvolveDirector to address this challenging task by shedding light on two research questions: (1) _How many synthetic text-image pairs are sufficient to approximate the generative capability of an advanced model? (2) Taking it a step further, is it possible for the base model to obtain generative capabilities beyond the advanced models?_

To explore the first question, we start with a demonstration experiment by training a relatively poor model, a DiT model  pre-trained on public dataset ImageNet  and SAM , to approach the advanced model PixArt-\(\) using increasing data scales. The training data is curated by collecting diverse text prompts and utilizing them to generate images from PixArt-\(\). The experiments indicate that when we scale the training data (_i.e._, generated image-text pairs) to 11 million, we can obtain a base model achieving similar capabilities to the target model without access to its internal data. However, the magnitude of 11 million generated data is comparable to the 14 million data used for pre-training the target model. Training a base model in this way incurs significant expenses, not only in terms of time and computational resources but also the costs associated with using fee-based APIs of some advanced models.

For more efficient training, it is crucial to minimize data redundancy and maximize data quality, as the marginal benefit of training on inferior data is limited. The corpus of the 11 million text prompts, generated from the SAM dataset , and the images generated by the advanced model exhibit redundancy in several aspects: (1) _lacking imaginative text prompts_ due to the photographic nature of SAM images; (2) _high similarities among text prompts_; and (3) _imbalanced data quality_. The generated images using the target advanced model may exhibit low quality due to inferior alignment on some text prompts. To address these problems, we introduce large vision-language models (VLMs) to improve the diversity and quality of training data for efficient training. Our approach involves a continuous evaluation with VLMs to dynamically refine the training dataset by the discrimination, expansion, deletion, and mutation operations. This dynamic curation strategy ensures that only valuable data is retained, significantly reducing the volume of data for training. Experimental results demonstrate that a mere 100k training samples are sufficient for the base model to gain similar performance to that of the target model, which is substantially fewer than the 11 million samples required by the baseline method.

To explore the second question, we applied EvolveDirector to train the base model to approach multiple recent most advanced models in a unified framework, including DeepFloyd IF , Playground 2.5 , Stable Diffusion 3 , and Ideogram . For each text prompt, we invoke advanced models to generate their images, and the VLM selects the best match to train our base model. The final trained model is named Edgen (EvolveDirector-Gen). The experimental results demonstrate that Edgen outperforms the advanced models mentioned above. Although our initial goal was to approximate these advanced models, we ultimately benefited from the VLM in choosing better training samples from their generated data, thereby achieving capabilities superior to any individual model.

The code of EvolveDirector and the model weights of Edgen are released to benefit the downstream tasks. Our contributions are summarized as: (1) Through experiments on massive data, we conclude that the generation abilities of a text-to-image model can be approximated through training on its generated data. (2) We propose the EvolveDirector, a framework that harnesses VLM to direct the training of the base model to learn generation ability from advanced models efficiently. (3) The trained text-to-image model Edgen outperforms the most advanced models.

## 2 Related Works

**Text-to-Image Generation.** To advance the overall quality of text-to-image generation, research efforts have been invested in exploring architectural improvement [10; 14; 15; 16] and generation paradigm advancement [17; 18; 3], _etc_. Diffusion models stand out as the de facto text-to-imagegeneration paradigm [19; 20; 1; 3; 21; 22; 23; 24; 25], noted for its scalability and stability . They benefit numerous downstream tasks, spanning image editing [27; 28], video generation [29; 30; 31; 32], 3D content generation [33; 34], _etc_. Through the use of highly descriptive and aligned image-text pairs at a substantial scale, text-to-image models that excel in resolutions, safety control, and the capability to render accurate scenes are obtained, _e.g._, Imagen , Midjourney , DALL-E 3 , Stable Diffusion 3 , and Ideogram . However, the exceptional capabilities of most advanced models have led providers to restrict access, typically offering only APIs, which limits their widespread and equitable use. In this paper, we aim to fill this gap by leveraging advanced VLMs to direct base model to replicate the functionality of advanced models. In contrast, some works propose to motivate models to learn from their self-generated images [35; 36].

**Evaluating T2I Generation with VLMs.** Some automatic evaluation methods [37; 38; 39] are propoesd to combine the LLMs and VQA models to evaluate the generated contents. Thanks to the substantial advancement of large language models [40; 41; 42; 43; 44; 45], the capabilities of VLMs are largely boosted [46; 47; 48; 49; 50; 51]. Utilizing these enhanced capabilities, methods building on VLMs are designed to facilitate the evaluation of text-to-image generation [52; 53; 54; 37; 55]. Notably, the recent research effort, Gecko , demonstrates the practicality of leveraging pre-trained LLMs  and VLMs  for systematic evaluation of text-to-image generative performance, spanning aspects such as text rendering, relational generation, attribute generation, _etc_. However, previous research requires fine-grained evaluation across various aspects, which remains challenging. In EvolveDirector, we simplify this process by requiring only pairwise comparisons, which enables a stable and reliable performance, facilitating the approaching of advanced text-to-image models.

**Knowledge Distillation.** KD  aims to transfer knowledge from well-trained teacher models to a simpler student model. The primary focus of most research in KD lies in investigating distillation losses with the output predictions of teacher model [57; 58], intermediate feature maps [59; 60], or feature correlations [61; 62] to distill knowledge. Recently, distillation methods based on diffusion models have garnered attention, primarily by distilling outputs from intermediate steps of the diffusion process to expedite the sampling process [63; 64; 65; 66]. Despite sharing the same goal of approaching the performance of advanced models, we would like to highlight our training paradigm is an orthogonal procedure to KD. To approach the advanced models with only APIs available, we avoid the need for distillation losses or acquiring intermediate results and instead choose a data-driven paradigm. Furthermore, our designed paradigm is also orthogonal to dataset distillation  as we evaluate and refine data during training rather than relying on a pre-existing large dataset.

**Online Learning.** Online learning has garnered significant interest for its capacity to enable models to adapt to real-time and dynamic data scenarios [68; 69]. This attention extends to a variety of real-world tasks, including semi-supervised learning [70; 71], unsupervised learning [72; 73], and continual learning [74; 75; 76], _etc_. In EvolveDirector, we harness the potential of online learning and powerful VLMs to evolve models towards advanced generation models, offering an efficient and scalable framework capable of dynamically adapting to evolving data.

## 3 Method

In this section, we outline the EvolveDirector framework in Sec. 3.1, describe the detailed operations of the VLM within this framework in Sec. 3.2, and discuss the training strategies in Sec. 3.3.

### Overview of EvolveDirector

The proposed framework EvolveDirector, as shown in Fig. 2, comprises three parallel processes: (1) interacting with advanced T2I models to get training images, (2) maintaining the dynamic training set empowered by the Vision-Language Model (VLM), (3) training the base model on the dynamic training set. The dynamic training set is updated by the VLM and advanced T2I models, to ensure the data are high value for training, thereby achieving efficient training and reducing the overall required data volume.

**Interaction with Advanced Models.** While the model configurations and weights of many advanced T2I models are not publicly accessible, they often provide APIs for interactions. In EvolveDirector, we interact with these APIs by submitting text prompts and receiving the corresponding generated images. The one that aligns with the given text prompt better will be selected by the VLM and included in the training set. The selection criteria and details of these advanced models are elaborated in Sec. 4.4.

**Dynamic Training Set.** The training set is dynamically updated during the training of the base model. It continuously incorporates high-value samples, _i.e._ text prompts on which the base model underperforms compared to advanced models. Simultaneously, it dynamically excludes low-value samples, _i.e._ those on which the base model performs comparably to the advanced models. The VLM evaluates the value of samples, with a detailed procedure outlined in Sec 3.2. The advanced T2I models are continuously called to generate new images for the new text prompts and update them into the dynamic training set.

**Training of the Base Model.** Based on the dynamic training set, we train a Diffusion Transformer (DiT)  as our base text-to-image model. Specifically, the model is built upon the improved architecture proposed by PixArt-\(\). Besides, we incorporate layer normalization after the Q (query) and K (key) projections in the multi-head cross attention blocks  to further stabilize the training, \((Q,K,V)=((Q) f_{K}(K)^{T }}{}})V\), where \(f_{Q}()\) and \(f_{K}()\) are the layer normalizations after the projections.

### Vision-Language Model as Director

The VLM acts as a director to guide the construction of more valuable dynamic datasets. To simplify the task difficulty and better leverage the capabilities of VLM, we present it with a choice of two images as a multiple-choice question, as shown in the top left corner of Fig. 3. One image \(I_{advanced}\) is generated by the advanced model, while another one \(I_{base}\) is generated by the base model, respectively. In practice, the order of the two images is randomized. VLM is called to compare them and decide which one aligns better with the given text prompt \(T\). Regarding the choice of VLM, there are two potential scenarios as follows.

(1) \(I_{advanced}\) outperforms \(I_{base}\). The inferior performance on this text prompt indicates that the base model is still under-trained. Therefore, EvolveDirector utilizes the VLM to generate more \(N_{S}\) variations of this text prompt \(T\), as shown in the lower-left corner of Fig. 3. Then the advanced T2I

Figure 2: The overview of the proposed framework EvolveDirector. (a) Advanced T2I models provide accessible APIs, allowing users to input text prompts and get the generated images. (b) The base model is trained on the dynamic dataset, consisting of text prompts and corresponding images generated by advanced models via API calls. The VLM continuously evaluates the base model and, according to its performance, dynamically updates and refines the dataset through discrimination, expansion, deletion, and mutation operations based on its evaluations.

model generates corresponding images to expand the dynamic training set, as shown in the right side of Fig. 3. The original samples will continuously be involved in the training.

(2) \(I_{advanced}\) does not outperform \(I_{base}\). If the base model is comparable with the advanced model, indicating sufficient learning, the VLM will remove that prompt \(T\) as a low-value sample from the set to economize on training resources.

Besides the expansion and deletion operations, EvolveDirector also performs mutation operations with a certain probability. This operation permits the VLM to generate more diverse text prompts independent of any existing ones, thereby encouraging the model to explore and learn from a broader domain of text prompts.

### Training Strategies

**Online Training.** To boost training efficiency, we develop EvolveDirector as an online training framework. Specifically, the base model undergoes uninterrupted training, without pausing for the advanced model or the VLM to execute. Instead, it sends a command to start VLM evaluation every 100 epochs, termed a checking epoch. At the checking epoch, a subset of text prompts is sampled from the dataset with a specific ratio \(R_{S}\). They are fed into a replica of the base model to generate images. Then the VLM evaluation and the subsequent execution of EvolveDirector begin, as illustrated in Fig. 3. Finally, a command is sent to the trainer of the base model to update the dynamic dataset. A detailed analysis of hyperparameters, including select ratio \(R_{S}\) and extension number \(N_{S}\), is provided in the supplementary.

**Stable Training.** In our task setup, the scale of training data is significantly less than the million scale. Under this circumstance, the original architecture  demonstrates considerable instability during training, and the generation collapse is observed. Thus we follow the work  and apply the layer normalization after the query and key projections to improve the training stability. The experimental results detailed in the supplementary demonstrate the effectiveness of this adaptation.

**Multi-Scale Training.** The ability to generate images across various scales and aspect ratios is a significant capability of advanced T2I models. To facilitate more efficient training, we initially train the base model on images with a fixed resolution of \(512\)px. Subsequently, we extend the training to images of higher resolution with multiple aspect ratios, thereby enabling the model to generate multi-scale and multi-ratio images. Following , we construct buckets with different aspect ratios and image sizes. In EvolveDirector, for each text prompt, a size bucket is randomly sampled from the buckets and advanced T2I models are called to generate images with this size. To avoid generation collapse, if the selected bucket falls outside the optimal size range of the advanced model, it will be resized to the closest appropriate size.

## 4 Experiments

### Training Details

We train the base model on \(16\) A100 GPUs for \(240\) GPU days, with a batch size of \(128\) and \(32\) for images at \(512\)px and \(1024\)px resolution, respectively. The VLM evaluation process is distributed across \(8\) A100 GPUs to facilitate its speed. The open-source advanced models are deployed on our

Figure 3: An example of the interaction between the EvolveDirector, VLM, and advanced T2I model. For brevity, auxiliary instructions to the VLM are omitted in this figure.

devices with simulated APIs for interaction. For closed-source models, EvolveDirector interacts with them through their public APIs.

### Selection of Vision-Language Models

The ability of the VLM to analyze images is crucial to the EvolveDirector framework. There are multiple powerful VLMs available, including CogVLM , CogAgent , Qwen-VL (Qwen-VL-Chat, Owen-VL-Plus, and Qwen-VL-Max) , InternVL , LLAVA and LLaVA-Next , and GPT-4V . We evaluate their newest version on \(600\) pairs of questions to calculate the alignments with human raters. Each question consisted of a text prompt and two images generated based on that prompt, presented to \(5\) different human raters. The VLMs are tested in two aspects: (1) Discrimination, and (2) Expansion. To be more specific, (1) initially, the VLMs were required to select which image aligned more closely with the given text prompt. The output is scored by 0 for wrong or 1 for correct by human raters. (2) Subsequently, they are instructed to generate more variations of the given text prompt. The score of "Accuracy" evaluates whether the generated text prompts contain any linguistic errors and whether they are in the same syntactic structure as the given text prompt. The score of "Diversity" evaluates the diversity among the generated text prompts. These two scores range from \(1\) (worst) to \(5\) (best). The results are shown in Tab. 1. The LLaVA-Next and GPT-4V achieve the top performance. Considering that LLaVA-Next is totally free to use, we select it as the VLM for EvolveDirector.

### Ablation Studies

**Models.** For ablation studies, we select the Pixart-\(\) as the unified advanced model to approach. We utilize a DiT model pre-trained on publicly accessible data, _i.e._ the ImageNet and SAM dataset, as the base model.

**Metrics.** We sample \(10,000\) text prompts to feed into models trained under different ablation settings to generate images and calculate their FIDs with the images generated by the advanced model. These text prompts are not seen by these models in the training stages. To conduct human evaluation, we randomly selected \(300\) sets of text prompts and their corresponding generated images, which are \(2,400\) in total. These images are paired in twos, each consisting of one image from the advanced model and one from an ablation model corresponding to the same text prompt, arranged in random order. Then they are presented to \(5\) different human raters to choose which image matches the text prompt better. In this manner, each ablation model is paired with the advanced model for comparison, and their selected ratios are recorded to reflect their relative performance compared to the advanced model.

**Results.** The results of FIDs and human evaluation are shown in Tab. 2. (1) _Directly Training on Generated Data._ The first three models were directly trained on images generated by the advanced model, using image quantities of \(10\) million, \(1\) million, and \(100\) thousand respectively. The experimental results show that the model trained on \(10\) million data reaches a comparable level to the advanced model in terms of human preference (\(48.89\%\) V.S. \(51.11\%\)), and achieves a low FID score (\(7.36\)). This indicates that the generative capabilities of the advanced model can be learned through training on its large-scale generated data. However, if the training data is reduced to \(10\%\) or even \(1\%\) of the original amount, the performance of the trained model will significantly decrease.

(2) _Training with EvolveDirector._ In the last four rows of Tab. 2, models trained with EvolveDirector under different ablation settings are evaluated. The upper bound of data volume is set to \(100\)k for all models. The first model is trained on an initial number of \(100\)K data and dynamically deletes data. The last three data models start training on an initial number of \(2\)K data and dynamically

    &  \\  &  & Accuracy & Diversity \\  CogAgent  & 0.675 & 3.81 & 3.76 \\ Qwen-VL-Max  & 0.825 & 4.72 & 4.56 \\ InternVL  & 0.793 & 4.70 & 3.67 \\ LLaVA-Next  & 0.840 & 4.85 & 4.69 \\ GPT-4V  & 0.847 & 4.82 & 4.72 \\   

Table 1: Alignment of VLMs with Human Preferences. The best value is highlighted in \(}\), and the second-best value is highlighted in \(}\).

    &  & }\)} \\  &  &  &  & Evaluation \(\) \\  ✗ & ✗ & ✗ & 11M & 73.36 & 48.89 \\ ✗ & ✗ & ✗ & 1M & 11.49 & 39.44 \\ ✗ & ✗ & ✗ & 100K & 15.19 & 32.22 \\  ✓ & ✗ & ✗ & 100K & 15.41 & 32.61 \\ ✗ & ✓ & ✗ & 100K & 13.05 & 36.44 \\ ✓ & ✓ & ✗ & 100K & 7.61 & 47.00 \\ ✓ & ✓ & ✓ & 100K & 7.45 & 48.53 \\   

Table 2: Ablation Studies. The best value is highlighted in \(}\), and the second-best value is highlighted in \(}\).

add new data to learn. The first model applies the _"Discrimination"_ function of the VLM model to discriminate the generated samples of the base model. Samples comparable to the output of the advanced model are removed from the training dataset. The results show that dynamically deleting samples does not cause much performance degradation. The second model does not use the VLM to evaluate the base model but instead randomly selected training samples for _"Expansion"_, i.e. generating more variants of given prompts and training samples. The results show that this operation brought a slight performance improvement due to the expansion of text prompt diversity. The third model utilizes VLM to evaluate the base model and performs reasoned expansion and deletion based on the evaluation results. As the results indicate, there is a significant performance increase, which highlights the importance of the evaluation of VLM. Lastly, the complete version of EvolveDirector is tested, which further applies the _Mutation_, i.e. randomly generating entirely new text prompts with a \(10\%\) probability. This operation further improves the performance of the model, because it encourages the model to explore more diverse images. With the full version of EvolveDirector, the model trained on a dynamic dataset with a data cap of \(100\)K, achieved performance comparable to the model trained on \(10\)M generated data, indicating that the proposed framework could significantly reduce the amount of training data required to approach the performance of the advanced model.

It is worth noting that there is still a slight gap between the final model and the advanced model, which can be attributed to the law of diminishing returns. This occurs because it becomes increasingly difficult to identify truly high-value samples as the performance approaches that of the advanced model. However, this phenomenon vanishes when EvolveDirector learns from multiple advanced models simultaneously, as experiments in Sec. 4.4 demonstrated. This is because the larger performance gaps between multiple models facilitate the easier identification of high-value samples.

### Approaching Advanced Models

We select several latest advanced models to approach their powerful generation ability, including Playground 2.5 , Stable Diffusion 3 , and Ideogram . Playground 2.5 is famous for its aesthetic generative effects, while Stable Diffusion 3 and Ideogram are known for their strong performance in various aspects including text generation and multi-object generation. Besides, we select a relatively old model, DeepFloyd IF (but just released in April 2023) , for its amazing text generation ability. The base model is the same as the one in Sec. 4.3, and we continue to train the base model that has already approached the Pixart-\(\) to approach the selected multiple advanced models. During training, EvolveDirector will feed each text prompt to all of them to generate corresponding images, and then use VLM to select the best one of them as the image from the advanced model. The selected image then undergoes evaluation as detailed in Sec.3.2.

**Select Ratios of Advanced Models.** We have calculated the proportions of images generated by these models that were selected by the VLM in the training stage, as shown in Table 3. Among the generated images, those generated by Ideogram are selected with the highest ratio. Besides, we evaluate the select ratios of images in specific domains, such as human generation, text generation, and multiple object generation. Examples of these three types are shown in Fig. 5. The results in Table 3 show that different advanced models achieve the highest select ratios in different specifics. For example, for generating text in images, the Stable Diffusion 3 outperforms others, while the Playground 2.5 is much worse than others. This may be caused by that the internal training data of Playground 2.5 does not include sufficient high-quality images with text in them. This also demonstrates the significance of diverse high-quality data.

**Quantitative Comparison.** To evaluate the performance of the trained Edgen, the base model, and the advanced models, we sample text prompts and feed them into each model to generate images. One image generated by Edgen and one image generated by the comparison model are combined together. Finally, same with the scale of comparison in previous works , \(300\) text prompts and \(1800\) image combinations are shown to human raters to select which one aligns with the given text prompt better. Each combination is evaluated by \(5\) different human raters. The results are shown in Fig. 4. We first analyze the performance of each model across all tested text prompts, as shown on the left side of Fig. 4. It is noteworthy that during the training, the VLM selects the best ones among

    &  \\  & Overall & Human & Text & Multi-Object \\  DeepFloyd IF  & 18.57\% & 9.12\% & 21.21\% & 12.12\% \\ Playground 2.5  & 25.71\% & 31.33\% & 0.09\% & 25.24\% \\ Ideogram  & 29.52\% & 29.18\% & 34.24\% & 33.36\% \\ Stable Diffusion 3  & 26.19\% & 30.36\% & 35.45\% & 29.27\% \\   

Table 3: Select Ratios of Advanced Models. The highest value is highlighted in blue, and the lowest value is highlighted in gray.

the images generated by multiple advanced models to be used as training data. Therefore, although EvolveDirector initially aims to train the base model to approach them, the final trained model Edgen outperforms all of the advanced models. Besides, we evaluate the performance of these models on specific types of text prompts, with the results displayed on the right side of Fig. 4.

**Qualitative Comparison.** In Fig. 5, we showcased three groups of generated images. The three rows of results respectively demonstrate the generation capabilities for humans, text, and multiple objects. For the first group, only the images generated by Edgen, DeepFloyd IF, and Ideogram successfully reflect the "face framed by shelves of...". As shown in the second row, only Edgen, Ideogram, and Stable Diffusion 3 have the ability to generate correct text in images. For the results shown in the third row, three objects need to be generated, i.e. the child, puppy, and cat. Only Edgen and Ideogram success and other models lost some objects. These results show that Edgen has already learned powerful generation abilities and outperforms some advanced models.

## 5 Conclusion

In this paper, we propose EvolveDirector, a framework that targets approaching the generation capabilities of advanced text-to-image models by only utilizing their publicly accessible APIs. By harnessing the capabilities of large vision-language models for evaluating image-text alignment,

Figure 4: Human evaluation of the images generated by the base model, Edgen trained by the proposed EvolveDirector, and multiple advanced models.

Figure 5: Images generated by the base model, Edgen trained by our EvolveDirector, and multiple advanced models. The results in three rows showcase the generation of human, text, and multi-object.

EvolveDirector significantly reduces the volume of training data required, thus saving considerable training costs, especially those associated with API usage. Experimental results demonstrate that the resultant model Edgen, inheriting the generation capabilities from multiple advanced models, achieves superior performance in various aspects. The limitations and future work are discussed in the supplementary.