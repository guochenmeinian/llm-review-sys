# Synthetic Experience Replay

Cong Lu, Philip J. Ball, Yee Whye Teh, Jack Parker-Holder

University of Oxford

Equal contribution. Correspondence to cong.lu@stats.ox.ac.uk and ball@robots.ox.ac.uk.

###### Abstract

A key theme in the past decade has been that when large neural networks and large datasets combine they can produce remarkable results. In deep reinforcement learning (RL), this paradigm is commonly made possible through _experience replay_, whereby a dataset of past experiences is used to train a policy or value function. However, unlike in supervised or self-supervised learning, an RL agent has to collect its own data, which is often limited. Thus, it is challenging to reap the benefits of deep learning, and even small neural networks can overfit at the start of training. In this work, we leverage the tremendous recent progress in generative modeling and propose Synthetic Experience Replay (SynthER), a diffusion-based approach to flexibly upsample an agent's collected experience. We show that SynthER is an effective method for training RL agents across offline and online settings, in both proprioceptive and pixel-based environments. In offline settings, we observe drastic improvements when upsampling small offline datasets and see that additional synthetic data also allows us to effectively train larger networks. Furthermore, SynthER enables online agents to train with a much higher update-to-data ratio than before, leading to a significant increase in sample efficiency, _without any algorithmic changes_. We believe that synthetic training data could open the door to realizing the full potential of deep learning for replay-based RL algorithms from limited data. Finally, we open-source our code at https://github.com/conglu1997/SynthER.

## 1 Introduction

In the past decade, the combination of large datasets  and ever deeper neural networks  has led to a series of more generally capable models . In reinforcement learning (RL, Sutton and Barto ), agents typically learn online from their own experience. Thus,

Figure 1: Upsampling data using SynthER greatly outperforms explicit data augmentation schemes for small offline datasets and data-efficient algorithms in online RL _without any algorithmic changes_. Moreover, synthetic data from SynthER may readily be added to _any_ algorithm utilizing experience replay. Full results in Section 4.

to leverage sufficiently rich datasets, RL agents typically make use of _experience replay_, where training takes place on a dataset of recent experiences. However, this experience is typically limited, unless an agent is distributed over many workers which requires both high computational cost and sufficiently fast simulation .

Another approach for leveraging broad datasets for training RL policies is _offline_ RL , whereby behaviors may be distilled from previously collected data either via behavior cloning , off-policy learning  or model-based methods . Offline data can also significantly bootstrap online learning ; however, it is a challenge to apply these methods when there is a mismatch between offline data and online environment. Thus, many of the successes rely on toy domains with transfer from specific behaviors in a simple low-dimensional proprioceptive environment.

Whilst strong results have been observed in re-using prior data in RL, appropriate data for particular behaviors may simply not exist and thus this approach falls short in generality. We consider an alternative approach--rather than passively reusing data, we leverage tremendous progress in generative modeling to generate a large quantity of new, synthetic data. While prior work has considered upsampling online RL data with VAEs or GANs , we propose making use of _diffusion_ generative models , which unlocks significant new capabilities.

Our approach, which we call _Synthetic Experience Replay_, or SynthER, is conceptually simple, whereby given a limited initial dataset, we can arbitrarily upsample the data for an agent to use as if it was real experience. Therefore, in this paper, we seek to answer a simple question: _Can the latest generative models replace or augment traditional datasets in reinforcement learning?_ To answer this, we consider the following settings: offline RL where we entirely replace the original data with data produced by a generative model, and online RL where we upsample experiences to broaden the training data available to the agent. In both cases, SynthER leads to drastic improvements, obtaining performance comparable to that of agents trained with substantially more real data. Furthermore, in certain offline settings, synthetic data enables effective training of larger policy and value networks, resulting in higher performance by alleviating the representational bottleneck. Finally, we show that SynthER scales to pixel-based environments _by generating data in latent space_. We thus believe this paper presents sufficient evidence that our approach could enable entirely new, efficient, and scalable training strategies for RL agents. To summarize, the contributions of this paper are:

* We propose SynthER in Section 3, a diffusion-based approach that allows one to generate synthetic experiences and thus arbitrarily upsample data for any reinforcement learning algorithm utilizing experience replay.
* We validate the synthetic data generated by SynthER in offline settings across proprioceptive and pixel-based environments in Section 4.1 and Section 4.3, presenting the first generative approach to show parity with real data on the standard D4RL and V-D4RL offline datasets with a wide variety of algorithms. Furthermore, we observe considerable improvements from upsampling for small offline datasets and scaling up network sizes.
* We show how SynthER can arbitrarily upsample an online agent's training data in Section 4.2 by continually training the diffusion model. This allows us to significantly increase an agent's update-to-data (UTD) ratio matching the efficiency of specially designed data-efficient algorithms _without any algorithmic changes_.

Figure 2: SynthER allows any RL agent using experience replay to arbitrarily upsample, or increase the quantity of, their experiences (in \(\)) and train on synthetic data (in \(\)). We evaluate our approach up to a factor of \(100\) more data in Section 4.2, across both proprioceptive and pixel-based environments. By leveraging this increased data, agents can learn effectively from smaller datasets and achieve higher sample efficiency. Details of the upsampling process are given in Figure 3.

## 2 Background

### Reinforcement Learning

We model the environment as a Markov Decision Process (MDP, Sutton and Barto ), defined as a tuple \(M=(,,P,R,_{0},)\), where \(\) and \(\) denote the state and action spaces respectively, \(P(s^{}|s,a)\) the transition dynamics, \(R(s,a)\) the reward function, \(_{0}\) the initial state distribution, and \((0,1)\) the discount factor. The goal in reinforcement learning is to optimize a policy \((a|s)\) that maximizes the expected discounted return \(_{,P,_{0}}[_{t=0}^{}^{t}R(s_{t},a_{t})]\).

### Offline Reinforcement Learning

In _offline RL_, the policy is not deployed in the environment until test time. Instead, the algorithm only has access to a static dataset \(_{}=\{(s_{t},a_{t},r_{t},s_{t+1})\}_{t=1}^{T}\), collected by one or more behavioral policies \(_{b}\). We refer to the distribution from which \(_{}\) was sampled as the _behavioral distribution_. In some of the environments we consider, the environment may be finite horizon or have early termination. In that case, the transition tuple also contains a terminal flag \(d_{t}\) where \(d_{t}=1\) indicates the episode ended early at timestep \(t\) and \(d_{t}=0\) otherwise.

### Diffusion Models

Diffusion models  are a class of generative models inspired by non-equilibrium thermodynamics that learn to iteratively reverse a forward noising process and generate samples from noise. Given a data distribution \(p()\) with standard deviation \(_{}\), we consider noised distributions \(p(;)\) obtained by adding i.i.d. Gaussian noise of standard deviation \(\) to the base distribution. The forward noising process is defined by a sequence of noised distributions following a fixed noise schedule \(_{0}=_{}>_{1}>>_{N}=0\). When \(_{}_{}\), the final noised distribution \(p(;_{})\) is essentially indistinguishable from random noise.

Karras et al.  consider a probability-flow ODE with the corresponding continuous noise schedule \((t)\) that maintains the desired distribution as \(\) evolves through time given by Equation (1).

\[=-(t)(t)_{} p(; (t))t\] (1)

where the dot indicates a time derivative and \(_{} p(;(t))\) is the score function , which points towards the data at a given noise level. Infinitesimal forward or backward steps of this ODE either nudge a sample away or towards the data. Karras et al.  consider training a denoiser \(D_{}(;)\) on an L2 denoising objective:

\[_{}_{ p,,(0, ^{2}I)}\|D_{}(+;)-\| _{2}^{2}\] (2)

and then use the connection between score-matching and denoising  to obtain \(_{} p(;)=(D_{}(;)- )/^{2}\). We may then apply an ODE (or SDE as a generalization of Equation (1)) solver to reverse the forward process. In this paper, we train our diffusion models to approximate the online or offline behavioral distribution.

Figure 3: SynthER generates synthetic samples using a diffusion model which we visualize on the proprioceptive walker2d environment. On the **top row**, we render the state component of the transition tuple on a subset of samples; and on the **bottom row**, we visualize a t-SNE  projection of 100,000 samples. The denoising process creates cohesive and plausible transitions whilst also remaining diverse, as seen by the multiple clusters that form at the end of the process in the bottom row.

## 3 Synthetic Experience Replay

In this section, we introduce Synthetic Experience Replay (SynthER), our approach to upsampling an agent's experience using diffusion. We begin by describing the simpler process used for offline RL and then how that may be adapted to the online setting by continually training the diffusion model.

### Offline SynthER

For offline reinforcement learning, we take the data distribution of the diffusion model \(p()\) to simply be the offline behavioral distribution. In the proprioceptive environments we consider, the full transition is low-dimensional compared with typical pixel-based diffusion. Therefore, the network architecture is an important design choice; and similarly to Pearce et al.  we find it important to use a residual MLP denoising  network. Furthermore, the choice of the Karras et al.  sampler allows us to use a low number of diffusion steps (\(n=128\)) resulting in high sampling speed. Full details for both are provided in Appendix B. We visualize the denoising process on a representative D4RL  offline dataset, in Figure 3. We further validate our diffusion model on the D4RL datasets in Figure 8 in Appendix A by showing that the synthetic data closely matches the original data when comparing the marginal distribution over each dimension. In Section 4.3, we show the same model may be used for pixel-based environments by generating data in a low-dimensional latent space.

Next, we conduct a quantitative analysis and show that **the quality of the samples from the diffusion model is significantly better** than with prior generative models such as VAEs  and GANs . We consider the state-of-the-art Tabular VAE (TVAE) and Conditional Tabular GAN (CTGAN) models proposed by Xu et al. , and tune them on the D4RL halfcheetah medium-replay dataset. Full hyperparameters are given in Appendix B.1. As proposed in Patki et al. , we compare the following two high-level statistics: **(1) Marginal:** Mean Kolmogorov-Smirnov  statistic, measuring the maximum distance between empirical cumulative distribution functions, for each dimension of the synthetic and real data; and **(2) Correlation:** Mean Correlation Similarity, measuring the difference in pairwise Pearson rank correlations  between the synthetic and real data.

We also assess downstream offline RL performance using the synthetic data with two state-of-the-art offline RL algorithms, TD3+BC  and IQL , in Table 1. The full evaluation protocol is described in Section 4.1. The diffusion model is far more faithful to the original data than prior generative models which leads to substantially higher returns on both algorithms. Thus, we hypothesize a large part of the failure of prior methods [34; 51] is due to a weaker generative model.

### Online SynthER

SynthER may be used to upsample an online agent's experiences by continually training the diffusion model on new experiences. We provide pseudocode for how to incorporate SynthER

    &  &  \\  & **Marginal** & **Correlation** & **TD3+BC** & **IQL** \\  Diffusion (Ours) & **0.989** & **0.998** & **45.9\(\)0.9** & **46.6\(\)0.2** \\ VAE  & 0.942 & 0.979 & 27.1\(\)2.1 & 15.2\(\)2.2 \\ GAN  & 0.959 & 0.981 & 24.3\(\)1.9 & 15.9\(\)2.4 \\   

Table 1: SynthER is better at capturing both the high-level statistics of the dataset (halfcheetah medium-replay) than prior generative models and also leads to far higher downstream performance. Metrics (left) computed from 100K samples from each model, offline RL performance (right) computed using 5M samples from each model. We show the mean and standard deviation of the final performance averaged over 8 seeds.

into any online replay-based RL agent in Algorithm 1 and visualize this in Figure 2. Concretely, a diffusion model is periodically updated on the real transitions and then used to populate a second synthetic buffer. The agent may then be trained on a mixture of real and synthetic data sampled with ratio \(r\). For the results in Section 4.2, we simply set \(r=0.5\) following Ball et al. . The synthetic replay buffer may also be configured with a finite capacity to prevent overly state data.

## 4 Empirical Evaluation

We evaluate SynthER across a wide variety of offline and online settings. First, we validate our diffusion model is periodically updated on the real transitions and then used to populate a second synthetic buffer. The agent may then be trained on a mixture of real and synthetic data sampled with ratio \(r\). For the results in Section 4.2, we simply set \(r=0.5\) following Ball et al. . The synthetic replay buffer may also be configured with a finite capacity to prevent overly state data.

## 4 Empirical Evaluation

We evaluate SynthER across a wide variety of offline and online settings. First, we validate our approach on offline RL, where we entirely replace the original data, and further show large benefits from upsampling small offline datasets. Next, we show that SynthER leads to large improvements in sample efficiency in online RL, exceeding specially designed data-efficient approaches. Furthermore, we show that SynthER scales to pixel-based environments by generating data in latent space. Finally, we perform a meta-analysis over our empirical evaluation using the RLiable  framework in Figure 7.

### Offline Evaluation

We first verify that synthetic samples from SynthER faithfully model the underlying distribution from the canonical offline D4RL  datasets. To do this, we evaluate SynthER in combination with 3 widely-used SOTA offline RL algorithms: TD3+BC (Fujimoto and Gu , explicit policy regularization), IQL (Kostrikov et al. , expectile regression), and EDAC (An et al. , uncertainty-based regularization) on an extensive selection of D4RL datasets. We consider the MuJoCo  locomotion (halfcheetah, walker2d, and hopper) and maze2d environments. In these experiments, all datasets share the same training hyperparameters in Appendix B, with some larger datasets using a wider network. For each dataset, we upsample the original dataset to **5M samples**; we justify this choice in Appendix C.1. We show the final performance in Table 2.

Our results show that we achieve at least parity for all groups of environments and algorithms as highlighted in the table, _regardless of the precise details of each algorithm_. We note significant improvements to maze2d environments, which are close to the 'best' performance as reported in CORL  (i.e., the best iteration during offline training) rather than the final performance. We hypothesize this improvement is largely due to increased data from SynthER, which leads to less overfitting and increased stability. For the locomotion datasets, we largely reproduce the original results, which we attribute to the fact that most D4RL datasets are at least 1M in size and are already sufficiently large. However, as detailed in Table 5 in Appendix A.1, SynthER allows the effective size of the dataset to be compressed significantly, up to \(12.9\) on some datasets. Finally, we present results on the AntMaze environment in Appendix E.1, and experiments showing that the synthetic and real data are compatible with each other in Appendix E.2.

    &  **Behavioral** \\ **Policy** \\  } &  &  &  \\   & & **Original** & **SynthER** & **Original** & **SynthER** & **Original** & **SynthER** \\   & random & 11.3\(\)0.8 & 12.2\(\)1.1 & 15.2\(\)1.2 & 17.2\(\)3.4 & - & - \\  & mixed & 44.8\(\)0.7 & 45.9\(\)0.9 & 43.5\(\)0.4 & 46.6\(\)0.2 & 62.1\(\)1.3 & 63.0\(\)1.3 \\  & medium & 48.1\(\)0.2 & 49.9\(\)1.2 & 48.3\(\)0.1 & 49.6\(\)0.3 & 67.7\(\)1.2 & 65.1\(\)1.3 \\  & medexp & 08.9\(\)7.0 & 87.2\(\)1.1 & 94.6\(\)0.2 & 93.3\(\)2.6 & 104.8\(\)0.7 & 94.1\(\)10.1 \\   & random & 0.6\(\)0.3 & 23.3\(\)1.9 & 4.16\(\)0.8 & 4.2\(\)0.3 & - & - \\  & mixed & 85.6\(\)4.6 & 90.5\(\)4.3 & 82.6\(\)6.0 & 83.3\(\)5.9 & 87.1\(\)3.2 & 89.8\(\)1.5 \\  & medium & 82.7\(\)5.5 & 84.8\(\)1.4 & 84.0\(\)5.4 & 84.7\(\)5.5 & 93.4\(\)1.6 & 93.4\(\)2.4 \\  & medexp & 110.0\(\)0.4 & 110.2\(\)0.5 & 111.7\(\)0.6 & 111.4\(\)0.7 & 114.8\(\)0.9 & 114.7\(\)1.2 \\   & random & 8.6\(\)0.3 & 14.6\(\)9.4 & 7.2\(\)0.2 & 77.0\(\)1.0 & - & - \\  & mixed & 64.4\(\)2.8 & 53.4\(\)15.5 & 84.6\(\)13.5 & 103.2\(\)0.4 & 99.7\(\)0.9 & 101.4\(\)0.8 \\  & medium & 60.4\(\)4.0 & 63.4\(\)2.4 & 62.8\(\)6.0 & 72.0\(\)1.5 & 101.7\(\)0.3 & 102.4\(\)0.5 \\  & medexp & 101.1\(\)1.0 & 105.4\(\)9.7 & 106.2\(\)6.1 & 90.8\(\)1.7 & 105.2\(\)11.6 & 109.7\(\)0.2 \\   & **59.0\(\)4.9** & **60.0\(\)5.1** & **62.1\(\)3.5** & **63.7\(\)3.5** & **92.9\(\)2.4** & **92.6\(\)2.1** \\   & unaze & 29.4\(\)1.2 & 37.6\(\)1.4 & 37.7\(\)2.0 & 41.0\(\)0.7 & 95.3\(\)7.4 & 99.1\(\)1.6 \\    & medium & 59.5\(\)4.9 & 65.2\(\)3.6 & 35.5\(\)1.0 & 35.1\(\)1.2 & 67.0\(\)4.0 & 66.4\(\)10.9 \\   & large & 97.1\(\)2.9 & 92.5\(\)3.8 & 49.6\(\)2.2 & 60.8\(\)5.3 & 95.6\(\)26.5 & 143.3\(\)21.7 \\   & **62.0\(\)28.2** & **65.1\(\)29.7** & **40.9\(\)8.3** & **45.6\(\)12.9** & **82.6\(\)112.6** & **102.9\(\)17.1** \\    & & & & & & & \\   

Table 2: A comprehensive evaluation of SynthER on a wide variety of proprioceptive D4RL  datasets and selection of state-of-the-art offline RL algorithms. We show that synthetic data from SynthER faithfully reproduces the original performance, which allows us to completely eschew the original training data. We show the mean and standard deviation of the final performance averaged over 8 seeds. **Highlighted** figures show at least parity over each group (algorithm and environment class) of results.

#### 4.1.1 Upsampling for Small Datasets

We investigate the benefit of SynthER for small offline datasets and compare it to canonical 'explicit' data augmentation approaches . Concretely, we wish to understand whether SynthER generalizes and generates synthetic samples that improve policy learning compared with _explicitly_ augmenting the data with hand-designed inductive biases. We focus on the walker2d (medium, medium-replay/mixed, medium-expert) datasets in D4RL and uniformly subsample each at the transition level. We subsample each dataset proportional to the original dataset size so that the subsampled datasets approximately range from 20K to 200K samples. As in Section 4.1, we then use SynthER to _upsample_ each dataset to 5M transitions. Our denoising network uses the same hyperparameters as for the original evaluation in Section 4.1.

In Figure 4, we can see that for all datasets, SynthER leads to a significant gain in performance and vastly improves on explicit data augmentation approaches. For explicit data augmentation, we select the overall most effective augmentation scheme from Laskin et al.  (adding Gaussian noise of the form \((0,0.1)\)). Notably, with SynthER we can achieve close to the original levels of performance on the walker2d-medium-expert datasets starting from **only 3% of the original data**. In Figure 0(a), we methodically compare across both additive and multiplicative versions of RAD, as well as dynamics augmentation  on the 15% reduced walker medium-replay dataset.

Why is SynthER better than explicit augmentation?To provide intuition into the efficacy of SynthER over canonical explicit augmentation approaches, we compare the data generated by SynthER to that generated by the best-performing data augmentation approach in Figure 0(a), namely additive noise. We wish to evaluate two properties: 1) How diverse is the data? 2) How accurate is the data for the purposes of learning policies? To measure diversity, we measure the _minimum_ L2 distance of each datapoint from the dataset, which allows us to see how far the upsampled data is from the original data. To measure the validity of the data, we follow Lu et al.  and measure the MSE between the reward and next state proposed by SynthER with the true next state and reward defined by the simulator. We plot both these values in a joint scatter plot to compare how they vary with respect to each other. For this, we compare specifically on the reduced 15% subset of walker2d medium-replay as in Figure 0(a). As we see in Figure 0(a), SynthER generates a significantly wider marginal distribution over the distance from the dataset, and generally produces samples that are further away from the dataset than explicit augmentations. Remarkably, however, we see that these samples are far more consistent with the true environment dynamics. Thus, SynthER

Figure 4: SynthER is a powerful method for upsampling reduced variants of the walker2d datasets and vastly improves on competitive explicit data augmentation approaches for both the TD3+BC (top) and IQL (bottom) algorithms. The subsampling levels are scaled proportionally to the original size of each dataset. We show the mean and standard deviation of the final performance averaged over 8 seeds.

Figure 5: Comparing L2 distance from training data and dynamics accuracy under SynthER and augmentations.

generates samples that have significantly lower dynamics MSE than explicit augmentations, even for datapoints that are far away from the training data. This implies that a high level of generalization has been achieved by the SynthER model, resulting in the ability to generate **novel, diverse, yet dynamically accurate data** that can be used by policies to improve performance.

#### 4.1.2 Scaling Network Size

A further benefit we observe from SynthER on the TD3+BC algorithm is that upsampled data can enable scaling of the policy and value networks leading to improved performance. As is typical for RL algorithms, TD3+BC uses a small value and policy network with two hidden layers, and width of 256, and a batch size of 256. We consider increasing the size of both networks to be three hidden layers and width 512 (approximately \(6\) more parameters), and the batch size to 1024 to better make use of the upsampled data in Table 3.

We observe a large overall improvement of **11.7%** for the locomotion datasets when using a larger network with synthetic data (Larger Network + SynthER). Notably, when using the original data (Larger Network + Original Data), the larger network performs the same as the baseline. This suggests that the bottleneck in the algorithm lies in the representation capability of the neural network and _synthetic samples from SynthER enables effective training of the larger network_. This could alleviate the data requirements for scaling laws in reinforcement learning . However, for the IQL and EDAC algorithms, we did not observe an improvement by increasing the network size which suggests that the bottleneck there lies in the data or algorithm rather than the architecture.

### Online Evaluation

Next, we show that SynthER can effectively upsample an online agent's continually collected experiences. In this section, we follow the sample-efficient RL literature  and consider 3 environments from the DeepMind Control Suite (DMC, Tunyasuvunakool et al. ) (cheetah-run, quadruped-walk, and reacher-hard) and 3 environments the OpenAI Gym Suite  (walker2d, halfcheetah, and hopper). As in Chen et al. , D'Oro et al. , we choose the base algorithm to be Soft Actor-Critic (SAC, Haarpoja et al. ), a popular off-policy entropy-regularized algorithm, and benchmark against a SOTA sample-efficient variant of itself, 'Randomized Ensembled Double Q-Learning' (REDQ, Chen et al. ). REDQ uses an ensemble of 10 Q-functions and computes target values across a randomized subset of them during training. By default, SAC uses an update-to-data ratio of 1 (1 update for each transition collected); the modifications to SAC in REDQ enable this to be raised to 20. Our method, 'SAC (SynthER)', augments the training data by generating 1M new samples for every 10K real samples collected and samples them with a ratio \(r=0.5\). We then match REDQ and train with a UTD ratio of 20. We evaluate our algorithms over 200K online steps for the DMC environments and 100K for OpenAI Gym.

    & **Behavioral** &  &  \\   & **Policy** & & **Original Data** & **SynthER** \\   & random & 11.3\(\)0.8 & 11.0\(\)0.7 & **12.8\(\)0.8** \\  & mixed & 44.8\(\)0.7 & 44.8\(\)1.1 & **48.0\(\)0.5** \\  & medium & 48.1\(\)0.2 & 50.2\(\)1.8 & **53.3\(\)0.4** \\  & medexp & 90.8\(\)7.0 & 95.5\(\)5.4 & **100.1\(\)2.7** \\   & random & 0.6\(\)0.3 & 2.6\(\)2.1 & **4.3\(\)1.7** \\  & mixed & 85.6\(\)4.6 & 76.4\(\)9.9 & **93.6\(\)3.6** \\  & medium & 82.7\(\)5.5 & 84.5\(\)1.7 & **87.2\(\)1.2** \\  & medexp & 110.0\(\)0.4 & 110.3\(\)0.5 & 110.2\(\)0.3 \\   & random & 8.6\(\)0.3 & 10.3\(\)5.6 & 19.5\(\)11.2 \\  & mixed & 64.4\(\)24.8 & 62.4\(\)21.6 & **86.8\(\)12.8** \\   & medium & 60.4\(\)4.0 & 61.9\(\)5.9 & 65.1\(\)4.7 \\   & medexp & 101.1\(\)10.5 & 104.6\(\)9.4 & **109.7\(\)4.1** \\   & 59.0\(\)4.9 & 59.5\(\)5.5 & **65.9\(\)3.7** \\   

Table 3: SynthER enables effective training of larger policy and value networks for TD3+BC  leading to a **11.7%** gain on the offline MuJoCo locomotion datasets. In comparison, simply increasing the network size with the original data does not improve performance. We show the mean and standard deviation of the final performance averaged over 8 seeds.

In Figure 6, we see that SAC (SynthER) matches or outperforms REDQ on the majority of the environments with particularly strong results on the quadruped-walk and halfcheetah-v2 environments. This is particularly notable as D'Oro et al.  found that UTD=20 on average _decreased performance_ for SAC compared with the default value of 1, attributable to issues with overestimation and overfitting [12; 48]. We aggregate the final performance on the environments in Figure 0(b), normalizing the DMC returns following Lu et al.  and OpenAI returns as in D4RL. Moreover, due to the fast speed of training our diffusion models and fewer Q-networks, our approach is in fact faster than REDQ based on wall-clock time, whilst also requiring fewer algorithmic design choices, such as large ensembles and random subsetting. Full details on run-time are given in Appendix D.2.

### Scaling to Pixel-Based Observations

Finally, we show that we can readily scale SynthER to pixel-based environments by generating data in the latent space of a CNN encoder. We consider the V-D4RL  benchmarking suite, a set of standardized pixel-based offline datasets, and focus on the 'cheetah-run' and 'walker-walk' environments. We use the associated DrQ+BC  and BC algorithms. Whilst the original image observations are of size \(84 84 3\), we note that the CNN encoder in both algorithms generates features that are 50 dimensional . Therefore, given a frozen encoder pre-trained on the same dataset, we can retain the fast training and sampling speed of our proprioceptive models but now in pixel space. We present full details in Appendix F.

Analogously to the proprioceptive offline evaluation in Section 4.1, we upsample 5M latent transitions for each dataset and present downstream performance in Table 4. Since the V-D4RL datasets are smaller than the D4RL equivalents with a base size of 100K, we would expect synthetic data to be beneficial. Indeed, we observe a statistically significant increase in performance of **+9.5%** and **+6.8%** on DrQ+BC and BC respectively; with particularly strong highlighted results on the medium and expert datasets. We believe this serves as compelling evidence of the scalability of SynthER to high-dimensional observation spaces and leave generating data in the original image space, or extending this approach to the online setting for future work.

## 5 Related Work

Whilst generative training data has been explored in reinforcement learning; in general, synthetic data has not previously performed as well as real data on standard RL benchmarks.

**Generative Training Data.** Imre , Ludjen  considered using VAEs and GANs to generate synthetic data for online reinforcement learning. However, we note that both works failed to match the original performance on simple environments such as CartPole--this is likely due to the use of a

Figure 6: SynthER greatly improves the sample efficiency of online RL algorithms by enabling an agent to train on upsampled data. This allows an agent to use an increased update-to-data ratio (UTD=20 compared to 1 for regular SAC) _without any algorithmic changes_. We show the mean and standard deviation of the online return over 6 seeds. DeepMind Control Suite environments are shown in the top row, and OpenAI Gym environments are shown in the bottom.

Weaker class of generative models which we explored in Section 3.1. Huang et al.  considered using GAN samples to _pre-train_ an RL policy, observing a modest improvement in sample efficiency for CartPole. Chen et al. , Yu et al.  consider augmenting the image observations of robotic control data using a guided diffusion model whilst maintaining the same action. This differs from our approach which models the entire transition and _can synthesize novel action and reward labels_.

Outside of reinforcement learning, Azizi et al. , He et al. , Sariyildiz et al.  consider generative training data for image classification and pre-training. They also find that synthetic data improves performance for data-scarce settings which are especially prevalent in reinforcement learning. Sehwag et al.  consider generative training data to improve adversarial robustness in image classification. In continual learning, "generative replay"  has been considered to compress examples from past tasks to prevent forgetting.

**Generative Modeling in RL.** Prior work in diffusion modeling for offline RL has largely sought to supplant traditional reinforcement learning with "upside-down RL" . Diffuser  models long sequences of transitions or full episodes and can bias the whole trajectory with guidance towards high reward or a particular goal. It then takes the first action and re-plans by receding horizon control. Decision Diffuser  similarly operates at the sequence level but instead uses conditional guidance on rewards and goals. Du et al.  present a similar trajectory-based algorithm for visual data. In contrast, SynthER operates at the transition level and seeks to be readily compatible with existing reinforcement learning algorithms. Pearce et al.  consider a diffusion-based approach to behavioral cloning, whereby a state-conditional diffusion model may be used to sample actions that imitate prior data. Azar et al. , Li et al.  provide theoretical sample complexity bounds for model-based reinforcement learning given access to a generative model.

**Model-Based Reinforcement Learning.** We note the parallels between our work and model-based reinforcement learning ; which tends to generate synthetic samples by rolling out using forward dynamics models. Two key differences of this approach to our method are: SynthER synthesizes new experiences without the need to start from a real state and the generated experiences are distributed exactly according to the data, rather than subject to compounding errors due to modeling inaccuracy. Furthermore, SynthER is an orthogonal approach which could in fact be _combined with_ forward dynamics models by generating initial states using diffusion, which could lead to increased diversity.

    &  &  \\   &  &  &  &  \\   & mixed & 28.7\(\)6.9 & 32.3\(\)7.6 & 16.5\(\)4.3 & 12.3\(\)3.6 \\  & medium & 46.8\(\)2.3 & 44.0\(\)2.9 & 40.9\(\)3.1 & 40.3\(\)3.0 \\  & medexp & 86.4\(\)5.6 & 83.4\(\)6.3 & 47.7\(\)3.9 & 45.2\(\)4.5 \\  & expert & 68.4\(\)7.5 & **83.6\(\)7.5** & 91.5\(\)3.9 & 92.0\(\)4.2 \\   & mixed & 44.8\(\)3.6 & 43.8\(\)2.7 & 25.0\(\)3.6 & 27.9\(\)3.4 \\  & medium & 53.0\(\)3.0 & 56.0\(\)1.2 & 51.6\(\)1.4 & 52.2\(\)1.2 \\   & medexp & 50.6\(\)8.2 & 56.9\(\)8.1 & 57.5\(\)6.3 & **69.9\(\)9.5** \\   & expert & 34.5\(\)8.3 & **52.3\(\)7.0** & 67.4\(\)6.8 & **85.4\(\)3.1** \\   &  & 56.5\(\)5.4 (**+9.5\%**) & 49.8\(\)4.2 & 53.2\(\)4.1 (**+6.8\%**) \\   

Table 4: We scale SynthER to high dimensional pixel-based environments by generating data in the latent space of a CNN encoder pre-trained on the same offline data. Our approach is composable with algorithms that train with data augmentation and leads to a **+9.5%** and **+6.8%** overall gain on DrQ+BC and BC respectively. We show the mean and standard deviation of the final performance averaged over 6 seeds.

Figure 7: RLiable  analysis allowing us to aggregate results across environments and show the probability of improvement for SynthER across our empirical evaluation.

Conclusion

In this paper, we proposed SynthER, a powerful and general method for upsampling agent experiences in any reinforcement learning algorithm using experience replay. We integrated SynthER with ease on **six distinct algorithms across proprioceptive and pixel-based environments**, each fine-tuned for its own use case, **with no algorithmic modification**. Our results show the potential of synthetic training data when combined with modern diffusion models. In offline reinforcement learning, SynthER allows training from extremely small datasets, scaling up policy and value networks, and high levels of data compression. In online reinforcement learning, the additional data allows agents to use much higher update-to-data ratios leading to increased sample efficiency.

We have demonstrated that SynthER is a scalable approach and believe that extending it to more settings would unlock extremely exciting new capabilities for RL agents. SynthER could readily be extended to \(n\)-step formulations of experience replay by simply expanding the input space of the diffusion model. Furthermore, whilst we demonstrated an effective method to generate synthetic data in latent space for pixel-based settings, exciting future work could involve generating transitions in the original image space. In particular, one could consider fine-tuning large pre-trained foundation models  and leveraging their generalization capability to synthesize novel views and configurations of a pixel-based environment. Finally, by using guidance for diffusion models , the generated synthetic data could be biased towards certain modes, resulting in transferable and composable sampling strategies for RL algorithms.

#### Acknowledgments

Cong Lu is funded by the Engineering and Physical Sciences Research Council (EPSRC). Philip Ball is funded through the Willowgrove Studentship. The authors would like to thank the anonymous Reincarnating Reinforcement Learning Workshop at ICLR 2023 and NeurIPS 2023 reviewers for positive and constructive feedback which helped to improve the paper. We would also like to thank Shimon Whiteson, Jakob Foerster, Tim Rocktaschel and Ondrej Bajgar for reviewing earlier versions of this work.