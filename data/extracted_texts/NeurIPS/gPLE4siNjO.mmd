# A Retrospective on the Robot Air Hockey Challenge: Benchmarking Robust, Reliable, and Safe Learning Techniques for Real-world Robotics

**Puze Liu\({}^{\,1}\)\({}^{\,2}\)** **Jonas Gunster\({}^{\,1}\)** **Niklas Funk\({}^{\,1}\)** **Simon Groger\({}^{\,1}\)**

**Dong Chen\({}^{\,3}\)** & **Haitham Bou-Ammar\({}^{\,4}\)** **Julius Jankowski\({}^{\,5}\)** **Ante Maric\({}^{\,5}\)** \\
**Sylvain Calinon\({}^{\,5}\)** & **Andrej Orsula\({}^{\,6}\)** **Miguel Olivares-Mendez\({}^{\,6}\)** **Hongyi Zhou\({}^{\,7}\)** **Rudolf Lioutikov\({}^{\,7}\)** **Gerhard Neumann\({}^{\,7}\)** **Amarildo Likmeta\({}^{\,8}\)** **\({}^{\,9}\)** \\
**Amirhossein Zhalehmehrabi\({}^{\,9}\)** & **Thomas Bonenfant\({}^{\,9}\)** **Marcello Restelli\({}^{\,9}\)** \\
**Davide Tateo\({}^{\,1}\)** & **Ziyuan Liu\({}^{\,3}\)** & **Jan Peters\({}^{\,1}\)\({}^{\,2}\)\({}^{\,10}\)** \\

air-hockey-challenge@robot-learning.net \\ 

**Abstract**

Machine learning methods have a groundbreaking impact in many application domains, but their application on real robotic platforms is still limited. Despite the many challenges associated with combining machine learning technology with robotics, robot learning remains one of the most promising directions for enhancing the capabilities of robots. When deploying learning-based approaches on real robots, extra effort is required to address the challenges posed by various real-world factors. To investigate the key factors influencing real-world deployment and to encourage original solutions from different researchers, we organized the Robot Air Hockey Challenge at the NeurIPS 2023 conference. We selected the air hockey task as a benchmark, encompassing low-level robotics problems and high-level tactics. Different from other machine learning-centric benchmarks, participants need to tackle practical challenges in robotics, such as the sim-to-real gap, low-level control issues, safety problems, real-time requirements, and the limited availability of real-world data. Furthermore, we focus on a dynamic environment, removing the typical assumption of quasi-static motions of other real-world benchmarks. The competition's results show that solutions combining learning-based approaches with prior knowledge outperform those relying solely on data when real-world deployment is challenging. Our ablation study reveals which real-world factors may be overlooked when building a learning-based solution. The successful real-world air hockey deployment of best-performing agents sets the foundation for future competitions and follow-up research directions.

Introduction

Modern machine learning techniques, particularly with the advent of Large Language Models (LLMs) and Diffusion Models (DMs), had a disrupting impact on many real-world applications, such as language processing and generation Yang et al. (2019), Brown et al. (2020), Achiam et al. (2023), board and video games Silver et al. (2016), Vinyals et al. (2019), image generation Ramesh et al. (2021), Rombach et al. (2022), Croitoru et al. (2023), and speech synthesis Prenger et al. (2019), Kong et al. (2020). However, while researchers are trying to bring these novel approaches to the real world, purely data-driven approaches are still struggling in real-world robotics, particularly when facing dynamic tasks. Indeed, recently, different foundation models for robotics have been presented Brohan et al. (2023), Zitkovich et al. (2023), O'Neill et al. (2023), Li et al. (2024), Team et al. (2024), Kim et al. (2024). However, to date, these models suffer from long inference times and they do not provide safety guarantees Firoozi et al. (2023), making them unsuitable for fast and dynamic real-world manipulation. Furthermore, industrial applications mostly rely on classical robotics and control techniques, relegating the data-driven methods to the area of research Peres et al. (2020), Dzedzickis et al. (2021).

Indeed, while robotics tasks are often the benchmark of choice in many areas of machine learning research, such as Computer Vision (CV) and Reinforcement Learning (RL), there is often quite a big disconnection between these benchmarks and real-world tasks: often the simulated setup is oversimplified, relieving the machine learning practitioner from the common issues arising when dealing with real robotic platforms. Thus, these benchmarks focus on small aspects of the robotics problem and do not sufficiently capture the complexity and challenges of real-world systems. This makes the deployment of machine learning solutions in real-world platforms challenging and often infeasible. To overcome these challenges and to incentivize the development of approaches that can better transfer onto real robotics systems, our Robot Air Hockey Challenge specifically focuses on a set of identified key issues preventing the deployment in dynamic real-world environments:

**Safety** Real robotics platforms live in the physical world. Therefore, the action performed by the robot must not damage the robot itself, the environment, or the people around the robot, at least at deployment time. Learning-based methods should consider safety problems and avoid dangers at any point during the execution.

**Imperfect Models** It is often challenging to obtain a good model of a real system in simulation. Particularly when dealing with black-box industrial systems that do not comprehensively describe the robot parameters and dynamics. Moreover, the real world presents many discontinuous dynamics that may heavily affect the environment's behavior. Thus, machine learning methods for real systems have to expose sufficient robustness and flexibility to deal with model mismatches.

**Limited Data** Real-world robotic data requires real-world interactions. Unlike the simulation environment, we cannot speed up the data collection process or employ massive parallelization. While large robotics datasets are already available, they are often restricted to a specific robot. It will be impractical to generalize to arbitrary platforms, as every hardware, software interface, and control system differs. As collecting real-world data is costly, difficult, and time-consuming, it is essential to develop methodologies that can adapt to the sim-to-real gap by learning from limited data.

**Reactiveness** Dynamic environments require rapid adaptation and reaction. This requirement forces the agent to be ready to react to unexpected events and to compute the control action in a real-time fashion. Often, control systems are embedded in the robot, resulting in limited computation resources, e.g., no GPU availability and a limited number of cores for the computation.

**Observation Noises and Disturbances** In real-world environments, observation noise and external disturbances will inevitably exist. If not handled properly, these noises and disturbances can seriously affect the performance of the trained model. Improving the robustness of the learning methods against observation noises and disturbances is essential for real-world deployment.

In summary, we introduced the Robot Air Hockey Challenge to provide a benchmark on a challenging, dynamic task, covering all these issues. We argue that the codified nature of the game and the limited workspace easily allow the definition of rigorous evaluation metrics, which are fundamental requirements of proper scientific evaluation. This challenge is a preliminary step towards more realistic benchmarks evaluating robust, reliable, and safe learning techniques that can be easily deployed in real-world robotic applications, going beyond the quasi-static assumption.

### Related Works

Dynamics tasks have always been challenging benchmarks for robotics and machine learning. For example, researchers have focused on dynamics dexterity games such as ball-in-a-cup [Kawato et al., 1994, Kober and Peters, 2008], juggling [Ploeger et al., 2021, Ploeger and Peters, 2022] or diabolo [von Drigalski et al., 2021], but also on sports such as tennis [Zaidi et al., 2023], soccer [Haarnoja et al., 2024] and table tennis [Mulling et al., 2011, Buchler et al., 2022]. Robotics tasks have also been already used as benchmarks for machine learning competitions, such as the Robot open-Ended Autonomous Learning competition (REAL) [Cartoni et al., 2020], Learn to Move [Song et al., 2021], the Real Robot Challenge [Gurtler et al., 2023, Funk et al., 2021], the TOTO Benchmark [Zhou et al., 2023], the MyoSuite Challenge [Caggiano et al., 2023] or the Home Robot Challenge [Yenamandra et al., 2023]. However, most of these tasks focus on simulation or consider limited quasi-static settings, where complex real-time and safety requirements are less critical.

While we are the first to develop a structured benchmark based on robot air hockey, the task is well-known in machine learning and robotics. The first work on robot air hockey can be found in Bishop and Spong , while the first learning-based approach is presented in Bentivegna et al. [2004a,b], where a humanoid robot is trained to learn air hockey skills. Due to the task complexity and versatility, it has been considered a challenging setting for evaluating robotic planning and control [Namiki et al., 2013, Igeta and Namiki, 2017, Liu et al., 2021], perception [Bishop and Spong, 1999, Tadokoro et al., 2022] and even human opponent modeling [Igeta and Namiki, 2015]. On top of that, the air hockey task has been used as a benchmark for many RL [AlAttar et al., 2019, Liu et al., 2022] and robot learning [Xie et al., 2020, Kicki et al., 2023, Liu et al., 2024] approaches. Most recently, after our NeurIPS 2023 Robot Air Hockey challenge, another benchmark on robot air hockey has been proposed by Chuck et al. . In contrast to our benchmark, it focuses more on perception, and our setting uses a much bigger playing table and requires faster and more reactive motions.

## 2 The Robot Air Hockey Challenge

The Robot Air Hockey Challenge is based on our real-world robot air hockey setup, consisting of two Kuka LBR IIWA 14 robots and an air hockey table (cf. Figure 1). The Kuka robots are equipped with a task-specific end-effector composed of an air hockey mallet connected to a rod, designed to deploy agents that do not strictly comply with the safety constraints. In addition to the real system, we also developed a MuJoCo simulation allowing for efficient testing and evaluation of the proposed control strategies. While the participants had access to an ideal version of the simulator, we evaluated the solution in a modified simulator, including various real-world factors, such as observation loss, tracking loss, model mismatch, and disturbances. Participants were allowed to

Figure 1: Game played in the real world between SpaceR and Air-HocKIT. The Air-HocKIT robot (back) hits the puck and the SpaceR robot (front) defends the attack to take control of the puck.

evaluate their solution once per day and download the dataset obtained from the modified simulator. With this setting, we want to simulate the limited access to real-world data, forcing the participants to deal with the sim-to-real gap. On top of the sim-to-real gap, the approaches should satisfy the deployment requirements, which provide metrics to quantify whether the respective policy would be safe for real-world deployment, i.e., the policy would not harm the real setup. Details of the metrics evaluating the deployability are presented in Section 2.2.

We also provide a robotic baseline capable of playing the full match, based upon our previous work Liu et al. (2021). The robotic baseline integrates typical approaches, such as planning and optimization. This approach performs satisfactorily in simulation but struggles to handle the sim-to-real gap. In addition, we provide a safe RL baseline Liu et al. (2022) for the low-level skills.

### Competition Structure

The Robot Air Hockey Challenge comprises two main stages in simulation: the Qualifying and the Tournament stage. Details about the simulated environment are provided in Table 1 and Appendix A.1. In the third stage, we deployed the solutions from the top three teams in the real-world platform, validating our challenge design. The next paragraphs introduce the challenge's three stages.

Qualifying stageIn this stage, participants need to control the robot to achieve three different sub-tasks in the single-robot environment, namely "Hit", "Defend", and "Prepare". The illustrations of the sub-tasks are shown in Figure 2.

* **Hit** The objective is to hit the puck to score a goal while the opponent moves in a predictable pattern. The puck is randomly initialized with a small initial velocity.
* **Defend** The objective is to stop the incoming puck on the agent side of the table and prevent the opponent from scoring. The puck is randomly initialized on the opponent's side of the table with a random velocity towards the agent's side, simulating an arbitrary hit from the opponent.
* **Prepare** The objective is to dribble the puck to a good hitting position. The puck is initialized close to the table boundary, where a direct hit to the goal is not feasible. The agent should maintain control of the puck, i.e., the agent must keep the puck on its half of the table.

Each task has different success metrics. For the "Hit" tasks, we count each episode as a success if the puck enters the scoring zone with a speed above the threshold. For the "Defend" task, an episode is considered successful if the final velocity of the puck (at the end of the episode) is below the threshold and does not bounce back to the opponent's side of the table. The "Prepare" task is considered successful if the final position of the puck is within a predefined area in the center of the

**Simulation Frequency** & 1000 Hz \\ 
**Control Frequency** & 50Hz \\   & Puck’s X-Y Position, Yaw Angle: \([x,y,]\) \\
**Observation** & Puck’s Velocity: \([,,]\) \\  & Joint Position / Velocity: \([q,]\) \\  & Opponent’s Mallet Position (if applicable): \([x_{o},y_{o},z_{o}]\) \\ 
**Control Command** & Desired Joint Position / Velocity \\ 

Table 1: Specification of the Air Hockey Environment

Figure 2: The three tasks in the qualifying stage of the Robot Air Hockey Challenge: from left to right “Defend”, “Hit”, and “Prepare” tasks. The “Defend” task requires stopping an incoming puck to get control of it. The “Hit” task consists of scoring a goal against an opponent, which moves in a fixed pattern. The “Prepare” task consists of repositioning the puck in the central area of the table without losing control of it.

agent's side of the table and the puck speed is below a given threshold. We use the mean success rate over the three sub-tasks as the overall performance metric.

Tournament stageIn this stage, participants trained an agent incorporating high-level skills with a low-level controller to play the full game of Air Hockey. While the qualifying stage focuses more on low-level behavior and individual primitives required to play Air Hockey, this stage requires high-level decision-making and a seamless combination of the previously developed components. The stage is composed of two rounds. In each round, we evaluate the solutions of each team against each other, such that each participant plays exactly one game against each available opponent. Before the start of each run, the authors are allowed to test their agents against the baseline agent provided by the organizers. We also run some friendly games between the currently submitted solution. Once the round started, we performed the evaluation sequentially without allowing further agent modification.

Real-world validation stageFinally, we evaluated the solutions of the tournament's top three teams on our real-world air hockey platform. The deployment of the solution on the real system required another safety layer, both in terms of safety requirements and fine-tuning of the methodology and the algorithm parameters, as the simulated system still presented quite a considerable sim-to-real gap, even with domain randomization and mismatch. For this reason, for every team, the performance in the real robot system was considerably worse than in the simulated setting. Given that this stage required hand tuning and intense help and engineering from our side, the outcome of the competition relied only on the simulated tasks. Details of the safety layer can be found in Appendix A.5. Despite the sim-to-real-gap and the necessary adaptations, the agents were able to play complete games. The full videos of the matches can be found publicly at http://air-hockey-challenge.robot-learning.net.

### Metrics

To evaluate the participants' solution, we focus on two important aspects: **deployability** and task **performance**. In the qualifying stage, performance is the success rate as defined above, while in the tournament stage, performance is the final game score. Based on the deployability score, teams will be categorized into three deployability levels, i.e., _deployable_, _improvable_, and _non-deployable_. Teams at the same deployability level will be ranked based on their winning score.

The deployability is computed by combining various metrics. These metrics are crucial constraints that need to be respected for real-world deployment. The safety constraints are presented in Table 2. We assign a penalty point to each metric based on its level of importance. The following metrics are considered during the evaluation:

* End-Effector's Position Constraints [3 pts]: The x-y-position of the end-effector should stay inside the table's boundaries. The end-effector should remain at the table height \(z_{}=0.1645\).
* Joint Position Limit [2 pts]: The joint position should not exceed the position limits.

**Constraint name** & **Dim.** & **Constraints** \\  Joint position & 14 & \(q_{l}<q_{cmd}<q_{u}\) \\  Joint velocity & 14 & \(_{l}<_{cmd}<_{u}\) \\   &  & \(l_{x}<x_{ee}\) \\  & & \(l_{y}<y_{ee}<u_{y}\), \\   & & \(z_{ee}>z_{}-0.02\), \\   & & \(z_{ee}<z_{}+0.02\). \\   &  & \(z_{elbow}>0.25\) \\  & & \(z_{wrist}>0.25\) \\  

Table 2: Constraints for the 7DoF environment

Figure 3: The tournament stage. The left figure shows the AiRLHockey agent (right side of the table) performing a hitting motion, while the right figure shows the Air-HocKIT (left side of the table) defending the attack and taking control of the puck

* Joint Velocity Limit [1 pt]: The joint velocity should not exceed the velocity limits.
* Computation Time [0.5-2 pts]: The computation time at each step should be shorter than 0.02 s. The maximum and the average computation time (per episode) will be considered in this metric.

In the qualification stage, we will run 1000 episodes on each subtask (equivalent to 2.8 hours of actual time) to evaluate the agent. The corresponding penalty points are accumulated if any metrics are violated in an episode (up to 500 steps per episode). The deployability score (DS) is the sum of the penalty points for all episodes. During the tournament stage, a full air hockey game will be evaluated. Each game lasts 15 minutes (45,000 steps), and we treat every 500 steps as an equivalent episode. Penalty points will be refreshed at the beginning of the episode. Based on the deployability score, the ranking will be divided into three categories: _Deployable_ (\( 500\)), Improvable (\(500< 1500\)), and _Non-Deployable_ (\(>1500\)) in the qualifying stage (1000 episodes); _Deployable_ (\( 45\)), _Improvable_ (\(45< 135\)), and _Non-Deployable_ (\(>135\)) in the tournament stage (equivalently 90 episodes).

## 3 Analysis

In this section, we analyze the results of the 2023 edition of the Robot Air Hockey Challenge. Despite being the first year of the competition, we received many competitive solutions and multiple solutions have been deployed in the real robot setup. In particular, it is interesting to see how the participants designed very different solutions coming from completely different perspectives. In general, many participants struggled to achieve satisfactory behaviors due to the particular challenges of the robot air hockey task. Furthermore, we identify four key outcomes of the competition:

**i.** The interplay between performance and deployability requirements makes the design of task objectives difficult. We note that, since most teams use RL in their solutions, the reward function design heavily influences the task performance.

**ii.** Penalty-based safety specification is brittle for out-of-distribution situations. Since deployability score is one important metric in the challenge, teams that adopt more engineered modules, such as inverse kinematics and trajectory interpolation, achieved better compliance with the constraints. Penalty-based training methods can satisfy safety requirements for in-distribution scenarios but may fail catastrophically for out-of-distribution situations.

**iii.** Plain RL is not sufficiently competent in handling long-horizon tasks. While it is possible to train a single agent to play a full game, the resulting approaches were not fully competitive with a human-designed policy.

**iv.** Physical inductive biases play an important role in the era of Embodiment AI. Prior knowledge such as kinematics, geometry, and physical understanding of the real world should not be ignored to obtain a safe, reliable, and robust solution deployable in the real world.

We now discuss in detail the three stages of the competition, highlighting the most relevant insights from each stage.

### Qualifying stage

We present the aggregated results of the qualifying stage in Table 3. Additional details for the performance on each task can be found in Appendix A.3. AiRLHockey won the qualifying stage, outperforming all other teams in all tasks. Their solution is mostly based on imitation learning, i.e., imitating a policy computed by optimal control. For the details, see Appendix A.6. The full RL solution, based on PPO, presented by the Air-HockIT team (Appendix A.8), achieved second place both overall and in all subtasks. Both the first and second solutions achieved similar performance in almost all tasks, except the prepare task, which is quite hard to solve for RL approaches. Indeed, we observed the same behavior in previous works on similar environments [Liu et al., 2022]. The GXU-LIPE team achieved third place, however, it did not achieve third place in all tasks. Indeed, the RL3_polimi team, another RL based approach (Appdendix A.9), considerably outperformed this solution in the Defend task, while the AeroTron team was able to gain the third position on the prepare task by a short margin. It is worth noting that the gap between the top two teams and the others was quite clear. Indeed, we believe that the top teams leveraged prior knowledge and experience with robotics systems, giving them a competitive advantage against machine learning practitioners, that are not sufficiently familiar with the robotics setting. We conclude that includingrobotics or physics-based inductive biases is still superior to black-box data-driven methods, and indeed most teams chose this type of solution for their final agents.

An important aspect to focus on is that many teams were able to provide satisfactory solutions in terms of performance, but struggled to comply with safety constraints. Indeed, only the ATACOM Baseline (Liu et al., 2022) can get some reasonable behavior while maintaining low safety violations. The safety issues are particularly relevant for the RL3_polimi team, which achieved the third high score but classified pretty low in the leaderboard due to safety constraints. Indeed, the black-box optimization of rule-based controllers used by the team proved to be highly sensitive to out-of-distribution evaluation. It is clear that safety issues are still a topic that is not very well explored and definitively requires more effort and investigation when working with robotic applications.

Ablation studiesFor additional insights into the qualifying stage, we performed an ablation study on the different components causing the sim-to-sim gap between training and evaluation environments. Results of this ablation study can be found in Figure 4. We evaluated the solutions on different versions of the environment: the ideal environment without the sim-to-real gap, the original evaluation environment with all the mismatch factors activated, and four different environments that differ from the ideal one only by the addition of one of the following modifications: i. model mismatch on the robot arm, ii. observation noise, iii. disturbance on the puck dynamics (due to the airflow), iv. unreliable puck tracking (track loss). From our ablation, it is clear that model mismatch plays a marginal role in the sim-to-real gap. This is because the closed-loop controller that tracks the desired trajectory heavily mitigates the effect of the model mismatch. In addition, most of the teams applied a state observer that filters the noisy observation. Thus, the observation noise has a limited impact on the performance. Instead, tracking loss and disturbances majorly affect the outcome of the task. On average, as expected, having all mismatch factors in the environments causes the agents to perform worse. This also holds true for most solutions in most tasks. Among the tasks, the more sensitive to mismatch factors is the "Hit" one, likely due to the fast movements and sensitivity to small angular errors. Instead, a bit surprisingly, the presence of a single factor may increase the performance on a single task of the baseline. This is most notable for SpaceR and RL3_polimi solutions, e.g., under puck disturbances. We argue that the changes in the state distribution are slightly beneficial for RL solutions due to the possibility of moving the puck in areas where the policy is more competent. The same result does not apply to the Air-HocKIT team, as the success rate of this solution is already high, meaning that the policy works well in all areas of the state space. We observe a small improvement also for the AiRLIHockey and Air-HocKIT solutions when using the less relevant mismatch factors. We believe this improvement is due to the stochasticity of the evaluation and would probably disappear with more task repetitions.

### Tournament stage

**Team Name** &  **Hit** \\ **Success** \\  &  **Defend** &  **Prepare** \\ **Success** \\  &  **Penalty** \\ **Points** \\  & 
 **Score** \\  \\    \\  AiRLIHockey & 54.9\% & 84.5\% & 90.3\% & 327.5 & 73.8 \\ Air-HocKIT & 52.2\% & 79.0\% & 68.6\% & 341.0 & 66.2 \\ GXU-LIPE & 30.1\% & 29.5\% & 66.2\% & 475.5 & 37.1 \\ SpaceR & 14.4\% & 47.8\% & 47.8\% & 221.0 & 34.4 \\ Baseline & 12.8\% & 25.4\% & 66.3\% & 352.5 & 28.5 \\ Baseline-ATACOM & 18.4\% & 36.1\% & 30.1\% & 33.0 & 27.8 \\ AJoy & 18.4\% & 36.1\% & 20.0\% & 108.0 & 25.8 \\ Kalash Jain & 0.0\% & 19.5\% & 8.3\% & 0.0 & 9.5 \\    \\  RL3\_polimi & 22.7\% & 61.6\% & 36.2\% & 920.0 & 41.0 \\ AeroTron & 33.2\% & 23.2\% & 66.5\% & 594.0 & 35.9 \\ CONFIRMTEAM & 29.3\% & 23.9\% & 65.3\% & 629.0 & 34.4 \\ Tony & 33.2\% & 23.2\% & 54.3\% & 718.0 & 33.4 \\ sprkrd & 0.2\% & 5.5\% & 0.0\% & 1271.0 & 2.3 \\   

Table 3: Results from the qualifying stage We present the results of the tournament stage in Table 4. Unfortunately, not all the qualified teams took part in the tournament stage, and one team only took part in one round of the competition. The winner of this stage is the AiRLIHockey team, while SpaceR scored second and Air-HocKIT scored third. The AiRLIHockey team dominated both the first and second rounds of the competition. By looking at the videos of the games, it is clear that the high performance of this agent is mostly due to the superior hitting performance in terms of speed and accuracy. The second place was achieved by SpaceR, with a model-based RL solution based on DreamerV3. While this solution performed slightly worse than other approaches in the qualifying stage, the end-to-end training simplified the porting on the tournament environment.

Differently from SpaceR, the Air-HocKIT team relied on a high-level policy to concatenate different skills. Unfortunately, a faulty policy implementation caused this team to lose many games on the first run due to a high deployability score. A similar issue also affected the solution of the RL3_polimi team. This shows one possible limitation of modular policies, where a faulty component can cause the complete failure of the system. Indeed, in the second round, after fixing the agent in the fine-tuning phase, Air-HocKIT achieved a good score, losing only against the AiRLIHockey team. For the same reasons, also the RL3_polimi performance increased in the second round.

### Real robot deployment

Sim-to-real transfer is a fundamental aspect of robot learning, as good performance on the real robotic system ultimately validates the presented approaches. While the real robot deployment was not part of the competition itself, we believe the sim-to-real transfer is a fundamental aspect of robot learning. In the end, we successfully deployed the top three solutions on the real robot. We recorded two full games between the agents. The deployment went on mostly smoothly, with minor issues (false score detection and malllet flipping) and both games ended up with multiple goals scored. While most of the goals originated from a mistake of the agents controlling or defending the puck, in the videos we still observe goals due to well-placed shots.

  &  &  & **Total** \\  & & & **Points** \\  AiRLIHockey & 15 & 18 & 33 \\ SpaceR & 9 & 12 & 21 \\ Air-HocKIT & 5 & 15 & 20 \\ AJoy & 9 & 6 & 15 \\ RL3\_polimi & 2 & 9 & 11 \\ GXU-LIPE & 2 & 1 & 3 \\ CONFIGMTEAM & 0 & 1 & 1 \\ 

Table 4: Results from the tournament stage

Figure 4: Ablation studies on the effect of each domain discrepancy on the submitted solutions. The graph reports the success rate for each task under different types of environment modifications

It is worth mentioning that the deployment phase was characterized by minor technical issues and required parameter tuning and adaptation for a successful deployment in the real system. Contrary to the simulation, real-world deployment is not a synchronous process, and the system is affected by delays. The delays, approximately 40ms, arise from gathering the observation, action computation, and execution. The deployment may lead to unsafe behaviors as the solutions are not trained in this environment, thus, trajectories that are reasonable in the simulation may become unsafe. Therefore, our safety layer is fundamental to avoid damaging the real platform. Details of the safety layer can be found in Appendix A.5

### Participants solution analysis

In this section, we will briefly discuss the solutions of the AiRLIHockey, SpaceR, Air-HocKIT, and RL3_polimi teams. Detailed descriptions of these solutions can be found in the Appendix A.6, A.7, A.8, and A.9 respectively. The AiRLIHockey solution, the winner of the challenge, is heavily based on classical robotics solutions and obtains highly precise and fast-hitting motions thanks to an optimization-based solution exploiting model predictive control (Jankowski et al., 2023). In this solution, learning is used mostly for state estimation and prediction as well as for the contact planner. The agent's high-level behavior is controlled by a state machine. The SpaceR solution, instead, is a flat policy approach based on Dreamer-V3 (Hafner et al., 2023). This solution exploits inverse kinematics to control directly in task space, avoiding the control in joint space. This approach also avoids any type of reward shaping and exploits instead sparse reward functions and self-play to solve the task. The Air-HocKIT solution is another RL-based solution, but using a state machine to coordinate multiple low-level RL policies, trained with PPO (Schulman et al., 2017). Differently from SpaceR, this team encodes domain knowledge in the reward function to achieve accurate, and safe low-level behaviors. The team tackles the sim-to-real gap w.r.t. the robot's motion with system identification. Finally, the RL3_polimi team also employs a hierarchical control architecture. Their state machine selects skills that are learned either with Deep RL (SAC (Haarnoja et al., 2024) combined with ATACOM (Liu et al., 2022)) or by learning rule-based policies with the PGPE algorithm (Sehnke et al., 2008), following the same scheme of Likmeta et al. (2020).

From the competition's results, it is clear that more structured solutions exploiting robotics priors perform better than unstructured ones. Experience with robotics systems is also extremely beneficial to encode prior information effectively in the reward functions or to perform parameter identification and bridge the sim-to-real gap. In general, data-driven methods are useful for learning models of the environment, for avoiding costly online optimization via behavioral cloning, and for learning dynamic behaviors. Another result is that for complex tasks, using advanced planning methods for control yields high-performance solutions that are still difficult to discover using data-driven methods, even with the help of structured reward functions. While these advanced planning methods are often computationally demanding, for limited tasks it is still possible to distill them through behavioral cloning. On one side, this shows that there are still open questions on learning for control. On the other side, this opens opportunities to use datasets generated with optimal control in combination with modern behavioral cloning and foundation models. Finally, we observe that hardcoding the high-level policy with a state machine is more beneficial than employing a flat policy. This result shows the effectiveness of specialized models against general behavioral models, at least for dynamic tasks like the one presented in this challenge.

### Key research problems

As a final point, we exploited the very diverse backgrounds of our participants to identify the most important research questions tackled within this challenge. Indeed, the teams highlighted a wide variety of research problems such as: i. the **sim-to-real gap** and online adaptation, from the perspective of online learning or system identification; ii. the complex **contact dynamics** between the puck and the mallet; iii. the competitive **multi-agent setting** and the necessity to adapt to the opponent; iv. the design of **curriculum learning** approaches.

However, most teams highlighted two main challenges, even if analyzed from different perspectives. The first one is the **constraint satisfaction** problem. The RL3_polimi team views this problem from the point of view of exploration of RL algorithms. In particular, they view this problem as a reward-shaping problem and point out that in the future, this could be seen through the lenses of multi-objective RL. Coming from a more robotics background, the SpaceR team instead views this problem as a low-level control problem. The Air-HocKIT team's point of view can be seen as in between the previous two, as their idea is to combine reward shaping and action space selection.

The second key research question identified is the **decision-making at different time scales** problem. Again, this issue has been seen in different ways. The AiRLIHockey team frames this problem as a control and state prediction problem, where the fast controller requires long-term prediction to act efficiently. The SpaceR and RL3_polimi teams see this issue from the perspective of Hierarchical RL.

### Limitations

Both the challenge and the proposed analysis are affected by limitations. In particular, in our setting, we assume a good perception system: we track the puck using the Optitrack motion capture system. This assumption neglects one of the most important problems in robotics, namely perception and action-perception coupling. However, while this is a clear limitation that distances our system from generic real-world robotic tasks, we believe that the setting is already too challenging for machine learning and robotics researchers, therefore reducing the scope of the work is reasonable to obtain good solutions. In future iterations of the challenge, we may consider a more complex perception problem or add some tasks requiring direct control from camera images.

Regarding the analysis, unfortunately, our conclusions suffer from a limited sample size. This is due to the particularly challenging tasks and the lack of availability of strong baselines. In the future, we hope to lower the entrance barrier to allow for more competitors. Another issue of the analysis is that currently there is not much understanding of the importance of high-level policy. Indeed, the only insight on this aspect is that a faulty high-level policy may cause severe safety issues, resulting in game losses. We hope that, with better low-level skills baselines, we will be able to dive deeper into the high-level tactics and understand how relevant are the opponent's playing style adaptation techniques.

Finally one of the major limitations of the 2023 edition of the challenge is the limited real-world deployment due to the limited availability of the hardware, the high complexity of the task, and the complexity of the setup. This made it impossible to perform learning and data collection directly from the real world and limited our capability of testing solutions. Unfortunately, our current robotic setup neither can easily support concurrent learning of multiple solutions nor the extensive deployment of learned solutions as done in the TOTO benchmark (Zhou et al., 2023). This is because, differently from other benchmarks, our chosen task is very complex and our system setup requires many different robotics solutions to work together. All these systems may be prone to failure and require robotic engineer supervision. For future iterations, we might consider providing real-world data and we will make the final real-world stage part of the competition. However, it is worth noting that our sim-to-sim approach closely predicts the results we can expect on real-world deployment, showing that our benchmarking approach is a viable way to evaluate real-world transfer performance.

## 4 Conclusion

In this paper, we presented a retrospective on the Robot Air Hockey Challenge. This challenge tries to bridge the gap between machine learning and robotics researchers, focusing on a domain particularly challenging for both research areas. In particular, our challenge focused on some key aspects that are often neglected by typical benchmarks used by machine learning researchers such as safety issues, dynamic environments requiring highly reactive policies and strict computational requirements, heavy sim-to-real gap, limited amount of data that can be collected from real robots, and competitive multiagent settings. While the air hockey task is particularly challenging, we were able to deploy a satisfactory number of solutions employing a variety of techniques also in the real robot. Unsurprisingly, the solution relying more on classical robotics techniques, namely model predictive control, outperformed all of the other methodologies, both in the tournament stage and in real-world deployment. Indeed, in robotics, exploiting good priors is still key to obtaining state-of-the-art performances. However, machine learning approaches are already competitive, and we are confident that a combination of data exploitation and inductive biases will allow the data-driven solution to surpass more classical baselines.