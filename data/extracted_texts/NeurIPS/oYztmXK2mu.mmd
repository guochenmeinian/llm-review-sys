# Improving Deep Ensembles without Communication

Konstantinos Pitas

Statify Team

Inria Grenoble Rhone-Alpes

pitas.konstantinos@inria.fr

&Michael Arbel

Thoth Team

Inria Grenoble Rhone-Alpes

michael.arbel@inria.fr

&Julyan Arbel

Statify Team

Inria Grenoble Rhone-Alpes

julyan.arbel@inria.fr

###### Abstract

Ensembling has proven to be a powerful technique for boosting model performance, uncertainty estimation, and robustness in supervised deep learning. We propose to improve deep ensembles by optimizing a tighter PAC-Bayesian bound than the most popular ones. Our approach has a number of benefits over previous methods: 1) it requires no communication between ensemble members during training to improve performance and is trivially parallelizable, 2) it results in a simple soft thresholding gradient update that is much simpler than alternatives. Empirically, we outperform competing approaches that try to improve ensembles by encouraging diversity. We report test accuracy gains for MLP, LeNet, and WideResNet architectures, and for a variety of datasets.

## 1 Introduction

Ensembling combines predictions from multiple trained models. In the deep learning setting, it has proven effective at improving model accuracy as well as capturing predictive uncertainty, outperforming Bayesian approaches for the same number of posterior samples (Arbel et al., 2023). To be effective, each ensemble member has to capture useful features from the data distribution. Usually this is enforced by encouraging the ensemble to be diverse, that is different ensemble members should capture different features.

In standard deep ensembles (Lakshminarayanan et al., 2017), which remain the gold standard for most tasks, each ensemble member is initialized with a different random set of weights. Each ensemble member is then trained independently (typically with standard SGD) and some diversity is induced by chance since each ensemble member converges to a different minimum and the predictive functions of different minima are empirically diverse (Fort et al., 2019). We propose a method that improves deep ensembles by leveraging the diversity effect of different initializations, while also biasing the minima to have desirable generalization properties. To achieve this, we diverge significantly from the existing literature.

* Existing approaches try to improve ensembles by promoting diversity through continuous communication between the training procedures of the different ensemble members (Ortega et al., 2022; D'Angelo and Fortuin, 2021; Rame and Cord, 2021; Masegosa, 2020; Wenzel et al., 2020). Enforcing diversity intuitively requires computing some mean prediction with respect to which the ensemble members are pushed away. This introduces memory costs andhinders parallelization. Requiring constant communication also complicates the distributed learning setting where one would wish to train each ensemble locally by different agents.
* Existing diversity objectives are hard to optimize. For example, some approaches use adversarial training which is known to be difficult to tune (Rame and Cord, 2021). Others push ensembles to be diverse in weight space which interacts non-trivially with function space, the space we are interested in (D'Angelo and Fortuin, 2021). We believe that such approaches make convergence difficult and thus often fail to materialize gains in test metrics.

We take into account all the above intuitions. We propose a regularization method that does not require communication between different ensemble members to improve test performance, except for the final validation (and testing) step. We encourage each ensemble member to optimize a novel PAC-Bayes bound, which incorporates unlabeled data. Our regularization encourages each ensemble member to generalize well. However, at the same time, we use a different seed for each ensemble member to increase diversity.

Intuitively, our regularizer fits either all classes or a random class for each unlabeled sample, as long as this does not interfere with fitting the labels on the training set. Our approach is a heuristic inspired by a Bayesian interpretation of neural networks. However, we completely avoid injecting additional stochasticity into our training procedure, making our objective easy to optimize. The beneficial properties of our approach, as well as the proposed regulariser, are illustrated in Figure 1. The final ensemble yields consistent improvements over standard ensembles and other approaches.

## 2 Background

A number of approaches have been proposed to improve upon standard deep ensembles.

Wenzel et al. (2020) propose to induce diversity by training on different random initializations as well as different choices of hyperparameters such as the learning rate and the dropout rates in different layers. Ensemble members can be trained independently, and the approach results in consistent gains over standard ensembles. However, the ensemble size increases _quadratically_.

Much closer to our approach are methods that explicitly promote diversity, while retaining the same number of ensemble members. Masegosa (2020), Ortega et al. (2022) propose to optimize a second-order PAC-Bayes bound so as to enforce diversity. In practice, this means estimating the mean log-likelihood of a true label across different ensemble members and "pushing" the different members to estimate a different value for their own likelihood. The authors show improvements for small-scale experiments but cannot improve on large-scale settings. Rame and Cord (2021) propose to use a discriminator that forces the latent representations of each ensemble member just before the final classification layer to be diverse. They show consistent improvements for large-scale settings in terms

Figure 1: Left: Our approach allows us to train each ensemble member without communication with other ensemble members. We only aggregate the ensemble members for the validation and testing stages. The predictions of each ensemble member on unlabeled data are forced to be different from its predictions on the training set. Our regularisation together with the different initialization of each member encourages the ensemble to be diverse. Right: Value of the regularization term partial derivative for different inputs. Consider a single training \(_{}\) and unlabeled \(_{}\) sample: non-zero gradient steps, ie \([R()] 0\), are taken only when \(_{};_{};)}{ w}  0\) and \((_{};)}{ w} 0\). Intuitively, we achieve regularization by fitting all classes or a random class \(a\) for each unlabeled sample, _as long as this does not interfere with fitting the labels on the training set_.

of test accuracy and other metrics, _for the same number of ensemble members_. Yashima et al. (2022) push the latent representations just before the classification layer to be diverse by leveraging Stein Variational Gradient Descent (SVGD). They show improvements in robustness to non-adversarial noise. However, they do not show improvements over Rame and Cord (2021) in other metrics.

The method closest to our approach is the very recently proposed Agree to Disagree algorithm (Matteo et al., 2023). This constructs an ensemble greedily by forcing each new member to disagree with previous members on unlabeled data. Crucially, this approach has however been evaluated only on OOD tasks.

The above methods exhibit all the shortcomings we previously described: 1) they require constant communication between ensembles to improve ensembles by promoting diversity, 2) they achieve negligible and/or inconsistent gains which we hypothesize is due to difficult-to-tune training procedures.

One can also approach ensembles as performing approximate Bayesian inference (Wilson and Izmailov, 2020). One would hope that the regularizing effect of the Bayesian inference procedure would improve the resulting ensembles. Unfortunately, approximate Bayesian inference approaches are typically outperformed by _standard_ deep ensembles (Ashukha et al., 2019). In particular, to achieve the same misclassification or negative log-likelihood error, MCMC approaches typically require many more ensemble members than standard ensembles.

## 3 Second order ensembles

We start our analysis by modeling our predictor using the linearized Laplace approximation (Immer et al., 2021). This turns the neural network into a Gaussian process with some mean and covariance structure (Immer et al., 2021; Khan et al., 2019). Instead of optimizing a stochastic objective, we propose to instead enforce the mean and the covariance to have some desired values in appropriate regions of the input space, thus potentially avoiding having to deal with excessive variance in the gradients.

### Notations and definitions

We denote the learning sample \((X,Y)=\{(_{i},y_{i})\}_{i=1}^{n}()^{n}\), that contains \(n\) input-output pairs, and use the generic notation \(Z\) for an input-output pair \((X,Y)\). Observations \((X,Y)\) are assumed to be sampled randomly from a distribution \(\). Thus, we denote \((X,Y)^{n}\) the i.i.d observation of \(n\) elements. We consider loss functions \(:\), where \(\) is a set of predictors \(f:\). We also denote the risk \(^{}_{}(f)=_{(,y)} (f,,y)\) and the empirical risk \(}^{}_{X,Y}(f)=(1/n)_{i}(f,_{i},y_{i})\). For each ensemble member, we consider two probability measures: the prior \(()\) and the posterior \(()\). Here, \(()\) denotes the set of all probability measures on \(\). We encounter cases where we make predictions using the posterior predictive distribution \(_{f}[p(y|,f)]\). We will use two loss functions, the non-differentiable zero-one loss \(_{01}(f,,y)=(_{j}f()_{j} y)\), and the negative log-likelihood, which is a commonly used differentiable surrogate \(_{}(f,,y)=-(p(y|,f))\), where we assume that the outputs of \(f\) are normalized to form a probability distribution.

Consider the Evidence Lower Bound (ELBO) objective

\[-_{f}}^{_{}}_{X,Y}(f)- (\|),\] (1)

for some \(>0\). Catoni (2007) shows that the ELBO is minimized at the probability density given by \((f)e^{- n}^{_{}}_{X,Y}(f)}/_{f}[e^{- n}^{_{}}_{X,Y}(f)}]\). We use the Laplace approximation to the posterior in our experiments. This is equivalent to approximating \(}^{_{}}_{X,Y}(f)\) using a second-order Taylor expansion around a minimum \(_{}\), such that \(}^{_{}}_{X,Y}(f_{})}^{_{}}_{X,Y}(f_{_{}})+(-_{})^{}}^{_{ }}_{X,Y}(f_{})|_{=_{}}( -_{})\). Assuming a Gaussian prior \(=(0,_{}^{2})\), the Laplace approximation to the posterior \(\) is again a Gaussian:

\[=_{},}^{-1} ,\]where \(}=+^{2}}\) and \(=n}_{,Y}^{_{}}( f_{})|_{=_{}}\) is the network Hessian. This Hessian is generally infeasible to compute in practice for modern deep neural networks, such that many approaches employ approximations. Specifically, we will use the Empirical Fisher \(=_{i=1}^{n}_{} p(y_{i};f,_{i})_{ } p(y_{i};f,_{i})^{}\), where the labels \(y_{i}\) are the ground-truth labels.

### Out-of-sample performance

We now present the following result which links the out-of-sample performance of a linearized Bayesian neural network to increased variance in new data.

**Theorem 1**.: _For posterior \(=_{},}^{-1}\) and assuming \(\|_{}\|_{2}\) to be bounded, the out-of-sample performance satisfies the following bound_

\[_{(y,)} [-_{}[p(y|,f(; ))]]}_{}& _{}[_{(y, )}^{_{}}(f(;))]}_{ }\\ &-_{(y,)}[ (_{_{}}()^{}}^{-1}_{_{}}())]}_{ },\] (2)

_for some constant \(c>0\), and where the bound \(\) is up to an approximation, and \(_{_{}}()\) is the network Jacobian._

We describe briefly the three terms in the above theorem. The Bayes Risk term is how we evaluate a Bayesian neural network on new data. We take multiple samples from the posterior, compute the average likelihood for each class and then make a prediction. The Gibbs Risk term is the data fitting term we typically use at training time for Bayesian neural networks, _all expectations are outside the log-likelihood_. This term furthermore captures a notion of "flatness", typically seen in Bayesian and PAC-Bayesian objectives (Foret et al., 2020). The flatter we are the simpler our function is on training data. The Variance term captures something quite different, the variance on new data. The higher the variance, the higher the uncertainty and the tighter the upper bound is on out-of-sample performance. Note also that the Variance term does not require labels.

**Differences from the Masegosa bound**

This result is based on a tighter version of Jensen's inequality first introduced in Masegosa (2020). However crucially, Masegosa (2020) use an objective that requires training all ensemble members jointly with the aim of increasing the ensemble diversity. By contrast, we optimize our objective for each ensemble member individually thus requiring no communication between different ensemble members.

### Second order regularization

We first explore how one could avoid dealing with a stochastic neural network at training time. Instead of optimizing a stochastic objective, one can simply fit the mean \(f(;_{})\) on the labels \(y\) for the training set \(Z_{}\) and maximize \((_{_{}}()}^{-1}_{_{}}()^{})\) on unlabeled data \(Z_{}\). We then avoid dealing with a stochastic neural network during inference. Specifically, a general strategy that we will follow is that for each ensemble member \(i[1,,K)\), we optimize a deterministic objective of the form \((_{_{i}})=}_{Z_{ }}^{_{}}(f(;_{_{i}}))+R(_{ _{i}})\), where \(R(_{_{i}})\) is a regularization term enforcing an appropriate out-of-sample variance. We then construct an ensemble as \(_{K}=\{_{_{1}},,_{_{K}}\}\) using the minima obtained by our deterministic objectives. This can be seen as equating each ensemble member with the mean from the approximate Bayesian posterior. We thus use the Bayesian formulation only implicitly, while hoping that the mean captures the desired properties. We finally make predictions using \(_{_{}_{K}}p(y|,f( ;_{}))\).

We propose two different optimization strategies.

**Strategy 1: Second-order (SO).** For unlabeled samples \(Z_{}\), we compute the full Jacobian with respect to the network outputs \(_{_{}}()\). We compute the matrix of variances of all network outputs\[_{_{}}()}^{-1} _{_{}}()^{}a\] (3) \[_{}(_{};Z_{},Z_{},)}_{Z_{}}^{ _{}}(f(;_{}))-_{,y  Z_{}}_{a}[_{_{}}() ^{}}^{-1}_{_{}}() ]_{a},\]

where \(\) is the regularization strength. See Algorithm 1 in Appendix A. There are several costs associated with this objective as it requires the computation of the _full network Jacobian per unlabeled sample_: 1) our memory costs increase by a factor of \(||\), where \(||\) is the number of classes, see right panel of Figure 2; 2) there are increased computational costs associated with computing Jacobians _and_ gradients per sample. Most deep learning libraries are optimized to compute gradient information, on the batch and not the individual sample level.

We manage to alleviate costs 1) described above through the following batching strategy.

**Strategy 2: Randomized second-order (SO+).** For each unlabeled sample \((,) Z_{}\), we compute the gradient with respect to some random network output \(f_{a_{}}(;)\) where \(a_{}(||,1/||)\). We then optimize

\[_{}(_{};Z_{},Z_{ },)}_{Z_{}}^{_{ }}(f(;_{}))-_{,y Z_{ }}[_{_{}}()^{} {}^{-1}_{_{}}()]_{a_{}}.\] (4)

By sampling some random class \(a_{}\) for which to compute the gradient per sample, we avoid incurring the \(||\) cost associated with computing the full Jacobian. We decrease the memory cost correspondingly. The drawback is that we further approximate our estimate of the variance, thus possibly converging to worse solutions. See Algorithm 2 in Appendix A.

### Heuristic gradient and batching

The regularization terms considered in the previous section require computing a gradient with respect to the parameters when learning the model. However, such gradients involve differentiating w.r.t. to the Hessian, which requires third-order derivatives and is therefore infeasible in practice for large models. One could replace the Hessian with the Fisher matrix which would result in second-order derivatives and is still expensive. Instead, we propose to use cheaper _regularization vector fields_ when updating the model's parameters. We construct these vector fields by using an ansatz inspired by the expression of the gradient of the regularization terms. In the case of the SO term, the diagonal Empirical Fisher approximation to the Hessian, the vector field takes the following form:

\[()_{j} =-2_{i}A_{i}(Z_{})^{-2}(_{_{ },y_{} Z_{}}U(_{},) _{i,j}};f)}{_{i}}) _{_{} Z_{}}_{a}(( _{};)}{_{i}})^{2}\] (5) \[+2_{i}A_{i}(Z_{})^{-1}_{_{} Z _{}}_{a}(V_{a}(_{},)_{i,j}(_{};)}{_{i}}),\]

Figure 2: Left: On a toy regression dataset (Section 4.1), and for an ensemble of 20 MLPs with 3 hidden layers, our objective yields increased uncertainty away from the training data, compared with standard deep ensembles. Middle: Improvement of the best of SO/SO+ over Standard ensembles for classification on real datasets (Section 4.2). Our approach yields consistent improvements for all datasets. Right: Additional memory cost of SO and SO+. All methods increase linearly the required memory with the number of classes. However, in the Standard and SO+ methods, we only increase the parameters of the final classification layer. In the SO method, we instead need to compute a full Jacobian per sample instead of a gradient.

where \(A_{i}(Z_{})=}+_{_{},y_{ } Z_{}}}|f)}{ _{i}}\), and \(U\) and \(V_{a}\) are matrices to be chosen. For specific choices of \(U\) and \(V_{a}\), the vector field \(\) recovers the gradient of the SO term introduced in the previous section. Instead of computing these exact derivatives, we choose \(U\) and \(V_{a}\) to be diagonal and to contain only first-order derivatives. Specifically, they take the following form:

\[U(_{};)_{i,i}=[};f)}{_{i}}]^{2}, V_{a}(_{};)_ {i,i}=[(_{},)}{ {w}_{i}}]^{2}.\] (6)

and we restrict \(_{i}\) to \(_{i=j}\). We discuss this choice, as well as the effect of the parameters \(,\) and provide more details on the derivation in Appendix C. Unless stated otherwise we will use \(=1,=1\). For this choice our regulariser has a simple and intuitive effect which we plot in Figure 1. Consider a single training \(_{}\) and unlabeled sample \(_{}\), and a single weight \(w\): i) Non-zero gradient steps, ie \([R()] 0\), are taken only when \(};_{})}{ w} 0\) and \((_{};)}{ w} 0\), i.e. our regulariser is activated as long as this does not interfere with fitting the labels on the training set. ii) \(-[R()]\) and \((_{};)}{ w}\) have the same sign. Intuitively, this means that our regulariser (when activated) _encourages that we fit all classes (SO) or a random class a (SO+) for each unlabeled sample_ (we move in the direction that maximizes the corresponding logit).

We finally see that it is easy to parallelize the above by substituting with \(B_{}\) a minibatch of training data, and \(B_{}\) a minibatch of unlabeled data. We need to compute \(};f)}{_{i}}\) per training sample \((_{},y_{})\). We also need to compute \((_{};)}{_{i}}\) per unlabeled sample \((_{},)\) and per output \(a\). Since Equation (22) is _per weight_\(j\), the full gradient can be computed with simple multiplications and additions, and is vectorisable. Finally on the computational side, per-sample gradient estimation and Jacobian estimation is possible in both PyTorch and JAX.

## 4 Experiments

In this section, we present a number of experiments that investigate whether the proposed regularizers improve deep ensembles.

### Toy regression example

We created a toy 1d regression dataset with input \(x\) and output distributed as \(y((x),1)\), a training set of size \(|Z_{}|=80\) and an unlabeled set of size \(|Z_{}|=100\). For the regression architecture, we used an MLP with 3 hidden layers each with 100 neurons. We fit the training set with an ensemble of 20 networks using the Standard Ensemble approach of Lakshminarayanan et al. (2017) which enforces diversity with different random seeds and with our SO objective. We then plot the resulting predictive mean as well as a \( 2\) confidence interval in the left panel of Figure 2. We see that both the Standard and the SO ensembles fit the training data equally well. However, the SO ensemble has a significantly higher uncertainty away from training data.

### Classification on real datasets

We train ensembles on 4 different datasets, CIFAR-10, CIFAR-100, Fashion MNIST and SVHN. We used 3 different architectures, the WideResNet22 architecture, as well as an MLP, and the LeNet architecture. For the MLP we used 3 hidden layers with 784, 500 and 300 neurons. The training dataset size was 40000 samples. For all architectures, we used 10000 samples for testing and 1000 samples for validation. The remaining samples were used as a pool for unlabeled data. This dataset split was first proposed in Alayrac et al. (2019). The dataset split is especially difficult for CIFAR-100 as very few labeled samples would be available per class for the 1000 training sample case, and in the 100 sample case, some classes would have no training samples. Hyperparameter optimization was done using the Optuna Bayesian Optimizer. We compare with Standard ensembles (Lakshminarayanan et al., 2017) (trained with independent seeds), Masegosa ensembles (Masegosa, 2020) which optimize diversity in function space, DICE ensembles (Rame and Cord, 2021) which is currently the SOTA method in diverse ensemble training, and Agree to Disagree (A2D) ensembles (Matteo et al., 2023). Unless stated otherwise, the ensemble size for all methods is 10 ensemble members, which is one of the most common sizes used in practice. We also experiment with the standard setup of augmentations for CIFAR-10 and CIFAR-100. Specifically, we augment the training set with random flips and crops.

We expect our approach to improve testing accuracy as well as the calibration of predictions. We thus evaluate the final ensembles on test Accuracy, the Expected Calibrations Error (ECE), the Thresholded Calibration Error (TACE), the Brier score, as well as the test Negative Log-Likelihood (NLL). It is folk wisdom that calibration can be traded for accuracy when using the ECE and the TACE. As a consequence, we also find the Pareto optimal Stiglitz (1981) ensemble in terms of both accuracy and calibration. Specifically we find the ensemble that minimizes \((1-)^{2}+()^{2}\). When the TACE is not available, which can happen due to binning, we use the ECE instead. We include a short description of Pareto optimality in Appendix D.

For SO we use all the remaining samples as a pool for training time. Sweeping through the entire unlabeled set would be unfeasible. Instead, we simply sample at each iteration an unlabeled batch of the same size as the training batch, and take a gradient step. Since SO+ is much more memory efficient than SO, we select an unlabeled set and we sweep through all of it at each epoch. We present the results in Tables 1 and 2. See also Table 1 in Appendix F.

**SO/SO+ improve both accuracy and calibration.** We often see significant improvements with our approach over both Standard ensembles and the other diversity approaches in all setups (see the middle panel of Figure 2). We gain up to \(6.38\%\) in test Accuracy depending on the architecture and

   Dataset / Aug & Method & Acc \(\) & ECE \(\) & TACE \(\) & Brier \(\) & NLL \(\) \\  CIFAR-10 & Standard40K & 84.52 & 0.04 & 0.01 & 0.86 & 0.73 \\ \(f\) no augmentation & SO40K@1K & 85.29 & 0.04 & 0.01 & 0.86 & 0.57 \\  & SO40K@5K & 84.42 & 0.04 & 0.011 & 0.84 & 0.59 \\  & SO+40K@1K & 85.90 & 0.04 & 0.011 & 0.87 & 0.59 \\   & A2D40K & 85.34 & 0.04 & 0.013 & 0.86 & 0.72 \\  & Masegosa40K & 83.07 & 0.27 & 0.05 & 0.44 & 0.78 \\  CIFAR-100 & Standard40K & 51.30 & 0.14 & 0.004 & 0.57 & 2.66 \\ / no augmentation & SO40K@1K & 47.85 & 0.08 & 0.003 & 0.43 & 2.18 \\  & SO40K@5K & 49.66 & 0.03 & 0.002 & 0.39 & 1.93 \\   & SO+40K@1K & 54.14 & 0.17 & 0.004 & 0.64 & 2.95 \\  & A2D40K & 51.64 & 0.16 & 0.004 & 0.60 & 2.96 \\  & Masegosa40K & 49.67 & 0.11 & 0.003 & 0.23 & 1.96 \\  FMNIST & Standard40K & 92.66 & 0.03 & 0.006 & 0.94 & 0.31 \\ / no augmentation & SO40K@1K & 92.49 & 0.018 & 0.004 & 0.91 & 0.22 \\   & SO40K@5K & 92.67 & 0.03 & 0.006 & 0.95 & 0.38 \\  & A2D40K & 92.37 & 0.03 & 0.006 & 0.60 & 0.45 \\  & Masegosa40K & 93.02 & 0.31 & 0.059 & 0.47 & 0.53 \\  SVHN & Standard40K & 95.72 & 0.016 & 0.003 & 0.95 & 0.25 \\ / no augmentation & SO40K@1K & 95.68 & 0.014 & 0.003 & 0.95 & 0.24 \\  & SO40K@5K & 95.60 & 0.01 & 0.003 & 0.95 & 0.26 \\  & A2D40K & 93.17 & 0.13 & 0.024 & 0.68 & 0.34 \\  & Masegosa40K & 95.63 & 0.33 & 0.065 & 0.49 & 0.55 \\   CIFAR-10 & Standard40K & 91.67 & 0.031 & 0.006 & 0.92 & 0.39 \\ / flip + crops & SO+40K@1K & 91.24 & 0.033 & 0.007 & 0.92 & 0.39 \\  & \(\)-ens40K@1K & 91.87 & 0.033 & 0.007 & 0.93 & 0.40 \\  CIFAR-100 & Standard40K & 66.38 & 0.12 & 0.003 & 0.74 & 2.20 \\ / flip + crops & SO+40K@1K & 66.33 & 0.13 & 0.003 & 0.75 & 2.30 \\  & \(\)-ens40K@1K & 65.74 & 0.12 & 0.003 & 0.73 & 2.11 \\   

Table 1: **WideResNet22 ablations.** SO/SO+ often improves over standard ensembles. When it doesn’t improve up standard ensembles SO has typically almost the same test metrics as standard ensembles. We highlight the cases where SO strictly improves upon standard ensembles. Hyperparameter optimization was done using random search.

   Dataset / Model & Method & Acc \(\) & ECE \(\) & TACE \(\) & Brier \(\) & NLL \(\) \\  CIFAR-10 & Standard40K & 71.25 & 0.10 & 0.02 & 0.77 & 1.41 \\ / LeNet & SO40K@1K & 71.36 & 0.11 & 0.024 & 0.79 & 2.25 \\ / no augmentation & SO40K@5K & 71.63 & 0.10 & 0.023 & 0.77 & 1.41 \\   & SO+40K@1K & 70.60 & 0.11 & 0.025 & 0.79 & 2.13 \\  & A2D40K & 71.34 & 0.06 & 0.016 & 0.70 & 0.94 \\  & Masegosa40K & 68.97 & 0.17 & 0.03 & 0.39 & 1.00 \\  CIFAR-10 & Standard40K & 55.84 & 0.21 & 0.04 & 0.71 & 2.55 \\ / MLP & SO40K@1K & 56.00 & 0.23 & 0.041 & 0.74 & 3.09 \\ / no augmentation & SO40K@5K & 55.61 & 0.22 & 0.040 & 0.72 & 2.71 \\  & SO+40K@1K & 56.23 & 0.22 & 0.04 & 0.73 & 2.94 \\  & A2D40K & 54.14 & 0.20 & 0.041 & 0.68 & 2.74 \\  & Masegosa40K & 53.04 & 0.04 & 0.019 & 0.40 & 1.35 \\  CIFAR-100 & Standard40K & 36.38 & 0.23 & 0.005 & 0.49 & 4.44 \\ / LeNet & SO40K@1K & 37.28 & 0.11 & 0.003 & 0.35 & 2.81 \\ / no augmentation & SO40K@5K & 36.74 & 0.12 & 0.004 & 0.36 & 2.86 \\  & SO+40K@1K & 37.46 & 0.11 & 0.003 & 0.35 & 2.8 \\  & A2D40K & 35.92 & 0.12 & 0.004 & 0.35 & 2.96 \\  & Masegosa40K & 34.70 & 0.05 & 0.003 & 0.23 & 2.75 \\  CIFAR-100 & Standard40K & 27.43 & 0.23 & 0.006 & 0.38 & 4.33 \\ / MLP & SO40K@1K & 28.25 & 0.18 & 0.005 & 0.33 & 3.79 \\ / no augmentation & SO40K@5K & 27.68 & 0.19 & 0.005 & 0.33 & 3.78 \\  & SO+40K@1K & 27.75 & 0.23 & 0.005 & 0.38 & 4.25 \\  & A2D40K & 28.02 & 0.06 & 0.0032 & 0.21 & 3.11 \\  & Masegosa40K & 26.66 & 0.12 & 0.004 & 0.24 & 3.44 \\  FMNIST & Standard40K & 92.50 & 0.039 & 0.006 & 0.039 & 0.49 \\ / LeNet & SO40K@1K & 92.19 & 0.022 & 0.005 & 0.91 & 0.24 \\ / no augmentation & SO40K@5K & 92.43 & 0.03 & 0.006 & 0.94 & 0.31 \\  & A2D40K & 91.95 & 0.05 & 0.009 & 0.91 & 0.61 \\  & Masegosa40K & 91.69 & 0.29 & 0.055 & 0.49 & 0.53 \\  FMNIST & Standard40K & 89.35 & 0.029 & 0.005 & 0.89 & - \\ / MLP & SO40K@1K & 89.31 & 0.021 & 0.004 & 0.88 & - \\ / no augmentation & SO40K@5K & 89.48 & 0.05 & 0.009 & 0.9355 & 0.52 \\  & A2D40K & 88.44 & 0.02 & 0.006 & 0.87 & 0.33 \\  & Masegosa40K & 89.50 & 0.27 & 0.056 & 0.50 & 0.58 \\  SVHN & Standard40K & 87.98 & 0.04 & 0.01 & 0.90 & - \\ / LeNet & SO40K@1K & 87.90 & 0.04 & 0.01 & 0.90 & - \\ / no augmentation & SO40K@5K & 88.10 & 0.045 & 0.009 & 0.92 & - \\  & A2D40K & 86.11 & 0.03 & 0.011 & 0.85 & 0.98 \\  & Masegosa40K & 86.21 & 0.26 & 0.047 & 0.45 & 0.70 \\  SVHN & Standard40K & 82.13 & 0.01 & 0.008 & 0.76 & 0.66 \\ / MLP & SO40K@1K & 82.52 & 0.04 & 0.01 & 0.82 & 0.77 \\ / no augmentation & SO40K@5K & 82.10 & 0.053 & 0.012 & 0.83 & 0.89 \\  & A2D40K & 83.10 & 0.03 & 0.008 & 0.81 & 0.68 \\  & Masegosa40K & 78.66 & 0.23 & 0.041 & 0.41 & 0.91 \\   

Table 2: **LeNet - MLP ablations.** SO/SO+ often improves over standard ensembles. When it doesn’t improve up standard ensembles SO has typically almost the same test metrics as standard ensembles. Note that accuracy can be traded off for calibration, as such we consider that a method is better than the other only if it improves upon both metrics. We highlight the cases where SO strictly improves upon standard ensembles. Hyperparameter optimization was done using random search.

dataset. We are Pareto optimal in 12 out of 18 cases and are optimal in terms of test accuracy in 12 out of 18 cases. The rest of the "wins" are split among the other approaches.

**SO/SO+ is much easier to optimize than alternatives.** One particular advantage is that when we do not improve upon other methods, we typically do not hurt performance significantly. This is in stark contrast with DICE. We had difficulties getting DICE to converge on cases other than MLP/LeNet for CIFAR-10. After correspondence with the authors, we believe that it is difficult to find the hyperparameters for which DICE works. It is folk wisdom that adversarial objectives are in general difficult to tune. We also observe that Masegosa and A2D ensembles typically underfit. In some cases though they do improve upon Standard Ensembles. We emphasize that in the A2D case, we replicated the results on the M/F-Dominoes and M/C-Dominoes datasets from the original paper. This means that the underfitting we report is not due to our implementation, but inherent in the algorithm.

**SO/SO+ does not require communication between ensemble members.** We trained each ensemble member independently and only evaluated the complete ensemble at the end of training for validation and testing.

**SO+ can scale to moderate dataset sizes.** We used SO+ to scale to 20K unlabeled samples per epoch for the MLP architecture, 10K unlabeled samples for the LeNet architecture, and 1-5K samples for the WideResNet22 architecture. We note that in principle one can scale to larger sets, as each ensemble member is trained independently and with stochastic batches.

**SO/SO+ improve ensembles even when using data augmentation.** Regularization gains in the low data regime often vanish or are greatly reduced when we apply data augmentation. In Tables 1 and 2, we see that augmentations improve the standard deep ensembles and that SO/SO+ provide further improvements.

## 5 Limitations

We introduced a number of approximations so as to obtain a tractable regulariser. These could mean that our results are suboptimal compared to what could be achieved with less restrictive modeling. We managed to scale our approach to moderate dataset sizes. However, our approach incurs an increase in computational time. Also the number of training _classes_ increases linearly our memory cost.

## 6 Conclusion

We introduced SO and SO+, two novel objectives that improve deep ensembles by leveraging unlabeled data. SO and SO+ do not require communication between the different ensemble members during training, beat consistently other approaches both in terms of accuracy and calibration, and avoid the stability issues of other objectives. A key question is if we can obtain a more principled regulariser and whether it can result in improved results. In future work, we will also try to make our approach even more scalable.

[MISSING_PAGE_FAIL:10]

A. Rame and M. Cord. DICE: Diversity in deep ensembles via conditional redundancy adversarial estimation. In _International Conference on Learning Representations_, 2021.
* Stiglitz  J. E. Stiglitz. Pareto optimality and competition. _The Journal of Finance_, 36(2):235-251, 1981.
* Wenzel et al.  F. Wenzel, J. Snoek, D. Tran, and R. Jenatton. Hyperparameter ensembles for robustness and uncertainty quantification. _Advances in Neural Information Processing Systems_, 33:6514-6527, 2020.
* Wilson and Izmailov  A. G. Wilson and P. Izmailov. Bayesian deep learning and a probabilistic perspective of generalization. _Advances in neural information processing systems_, 33:4697-4708, 2020.
* Xiao et al.  H. Xiao, K. Rasul, and R. Vollgraf. Fashion-mnist: a novel image dataset for benchmarking machine learning algorithms. _arxiv_, 2017.
* Yashima et al.  S. Yashima, T. Suzuki, K. Ishikawa, I. Sato, and R. Kawakami. Feature space particle inference for neural network ensembles. In _International Conference on Machine Learning_, pages 25452-25468. PMLR, 2022.

[MISSING_PAGE_EMPTY:12]

``` Input: Temperature \(\), regularisation coefficient \(\), prior variance \(_{}^{2}\), training data \(Z_{}\), unlabeled data \(Z_{}\), number of ensemble members \(K\) Output: Ensemble \(_{K}=\{_{_{1}},,_{_{K}}\}\) \(_{K}\{\}\) for\(i\) in \(\{1,,K\}\)do \(_{}\) while not converged do  Draw labeled samples \(B_{}\) and unlabeled samples \(B_{}\)  Compute Empirical Fisher matrix \(_{(_{t},y_{t}) B_{}} _{_{}} p(y_{t};f,_{t})_{ _{}} p(y_{t};f,_{t})^{}\)  Compute the diagonal approximation to the Hessian \(}()+^{2}}\) \((_{u},) B_{}\) sample \(a_{}(||,1/||)\)  Compute the Empirical Risk \(}_{_{}}^{_{}}(f( ;_{}))\)  Compute the heuristic gradient \[_{}()_{j} =-2 A_{j}(Z_{})^{-2}(_{_{ t},y_{t}_{}}[;f)}{ _{j}}]^{3})_{_{u}_{}}(}}(_{u}; )}{_{j}})^{2}\] \[+2A_{j}(Z_{})^{-1}_{_{u} Z_{ }}([}}(_{u},)}{_{j}}]^{3}),\] where \(A_{j}(Z_{})=^{2}}+_{_{u},y _{t}_{}}(;f)}{ _{j}})^{2}\) and \(==1\).  Update with gradient \(g=_{_{}}}_{_{}} ^{_{}}(f(;_{}))- _{}(_{})\) endwhile \(_{K}_{K}\ \{_{}\}\) endfor ```

**Algorithm 2** Pseudocode for the SO+ algorithm Motivating the optimization objective through a PAC-Bayes bound

We now prove Theorem 1 which indicates an appropriate way to choose \(_{}\) such as to ensure good out-of-sample performance.

**Theorem 2**.: _For posterior \(=_{},}^{-1}\) and assuming \(\|_{}\|_{2}\) to be bounded, the out-of-sample performance satisfies the following bound_

\[_{(y,)} [-_{}[p(y|,f(; ))]]}_{}& _{}[_{(y,) }^{_{}}(f(;))]}_{}\\ &-_{(y,)}[(_{_{}}()^{}}^{-1} _{_{}}())]}_{}, \] (7)

_for some constant \(c>0\), and where the bound \(\) is up to an approximation, and \(_{_{}}()\) is the network Jacobian._

Proof.: The proof is based on the following theorem that links generalization with a variance of the posterior predictive.

**Theorem 3**.: _Masegosa (2020) Any distribution \(\) in the space of distributions \(\) satisfies that,_

\[_{(y,)}[-_{ {}}[p(y|,f(;))]]_{ }[_{(y,)}^{_{ }}(f(;))]-()\] (8)

_where \(()\) is a variance term defined as_

\[()=_{(y,)}[}p(y|;)}_{} [(p(y|,)-_{}(p(y| ,)))^{2}]].\] (9)

Let us assume as in Masegosa (2020) that the model likelihood is bounded:

**Assumption 1**.: _Masegosa (2020) There exists a constant \(C<\) such that \(\), \(_{y,}p(y|;) C\)._

Note that this assumption holds for the classification setting with \(C=1\).

The first term in the RHS of (8) is the common data fitting term when optimizing a supervised classification objective. As such we will focus on the second "variance" term \(()\).

We use a linearization of the neural network outputs, such that

\[f_{}(;)=f(;_{})+_{_{}}()^{}(-_{}).\] (10)

Then for posterior \(=_{},}^{-1}\) the outputs of the linearized network have the following distribution

\[f_{}() p(f_{}()|,X,Y)=( f(;_{}),_{_{}}()^{ }}^{-1}_{_{}}())\]

We use the cross-entropy loss. We take a first order approximation to the loss. Then for the \(f_{}\) predictor and the variance term \(()\) we have that

\[()& _{(y,)}[_{}[(p(y|,)-_{}( p(y|,)))^{2}]]\\ &_{(y,)}[ _{f_{}() p(f_{}()|,X,Y)} [(p(y|f_{}())-_{f_{}() p(f_ {}()|,X,Y)}[p(y|f_{}())] )^{2}]]\\ &_{(y,)}[ _{f}()^{}[_{_{}}()^{}}^{-1}_{_{}}() ]_{f}()]\] (11)

where \(_{f}()_{i}=}_{f=f( ;_{})}\). The first inequality follows from the inequality \(p(y;,) 1\). The second line is obtained using the first-order approximation of \(f(x,w)\) (\(f(;) f_{lin}(;)\).

Such an approximation is controlled by the variance of \(\) which is of order \(1/N\). Finally, the last line results from the application of the delta method which also holds with an error of order \(1/N\).

We now deal with this Jacobian term which is the only one dependent on the labels \(y\).

\[() _{(y,)}[ _{f}()^{}[_{_{}}( {x})^{}}^{-1}_{_{}}( )]_{f}()]\] (12) \[=_{()}[ (_{_{}}()^{} {}^{-1}_{_{}}()_{y (|)}[_{f}()_{f}( )^{}])]\] \[=_{()}[ (_{_{}}()^{} {}^{-1}_{_{}}()_{y (|)}[_{f}()_{f}( )^{}])]\] \[=_{()}[ (_{_{}}()^{} {}^{-1}_{_{}}()_{y (|)}[^{}(_{f}()_{y})^{2}])]\]

In the final line we have used both a one-hot encoding \(\) and an integer encoding \(y\) of the label. The substitution holds because \(_{f}()_{i}\) is non zero only for \(i=y\). In the above we note that 1) \(^{}(_{f}()_{y})^{2}\) is diagonal as the label \(\) is a one-hot vector 2) For a model with finite weights, where \(\|_{}\|_{2}^{2}\) is bounded, we can assume that \((_{f}()_{y})^{2} c\), where \(c\) is a positive constant. This is because the minimum for the cross-entropy loss is achieved at infinity for one of the input logits. We then get

\[() _{()} [(_{_{}}()^{} }^{-1}_{_{}}()c)]\] (13) \[=_{()}[ (_{_{}}()^{} {}^{-1}_{_{}}())].\]

## 9 Deriving the diversity encouraging vector field

**Heuristic 1**.: _For the Empirical Fisher \(\) approximation to the Hessian, for a weight \(_{j}\), and given training data \(Z_{}\) and unlabeled data \(Z_{}\), we propose the following diversity encouraging vector fields_

\[_{}()_{j} =-2 A_{j}(Z_{})^{-2}(_{_{},y_{} Z_{}}[};f)}{_{j}}]^{3})_{_{} Z_{}}_{a}((_{}; )}{_{j}})^{2}\] \[+2A_{j}(Z_{})^{-1}_{_{} Z_{ }}_{a}([(_{}, )}{_{j}}]^{3}),\]

_and_

\[_{+}()_{j} =-2 A_{j}(Z_{})^{-2}(_{_{},y_{} Z_{}}[};f)}{_{j}}]^{3})_{_{} Z_{}}((_{}; )}{_{j}})^{2}\] \[+2A_{j}(Z_{})^{-1}_{_{} Z_{ }}([(_{}, )}{_{j}}]^{3}),\]

_where \(A_{j}(Z_{})=^{2}}+_{_{},y _{} Z_{}}(};f)}{ _{j}})^{2}\) and \(,\)._

Proof.: First note how naively taking the gradient of \(_{_{},y_{} Z_{}}_{a}[ _{_{}}(_{})}^{- 1}_{_{}}(_{})^{}]_{a}\) without approximating \(\) (which is part of \(}\)) will result in third-order derivatives which are prohibitively expensive to compute. We thus propose to approximate \(\) with the diagonal of the Empirical Fisher \(()\).

We write down the form of the diagonal elements of the Empirical Fisher \(\) assuming also that we keep only the diagonal elements, and we only have one training sample \((_{t},y_{t})\). We get

\[_{a}=(;f)}{_{a}} )^{2}.\] (14)Since \(}()+^{2}} \), we get

\[}_{a}(;f)}{ _{a}})^{2}+^{2}}.\] (15)

Finally, the variance for each output of the linearized network and a single unlabeled sample \((_{u},)\) is

\[&[_{_{}}(_{u}) }^{-1}_{_{}}(_{u})^{} ]_{a}\\ &_{i}[((;f)}{_{i}})^{2}+^{2}})^{-1 }((_{u};)}{_{i}} )^{2}].\] (16)

We now sum over the different outputs \(f_{a}\)

\[&_{a}[_{_{}}(_{u})}^{-1}_{_{}}(_{u})^{ }]_{a}\\ &_{a}_{i}[((;f)}{_{i}})^{2}+^{2}} )^{-1}((_{u};)}{ _{i}})^{2}].\] (17)

We now take the partial derivative with respect to a weight \(_{j}\). We get

\[&_{j}}_{a} [_{_{}}(_{u})}^{-1} _{_{}}(_{u})^{}]_{a}\\ &_{j}}_{i}[ ((;f)}{_{i}} )^{2}+^{2}})^{-1}_{a}((_{u};)}{_{i}})^{2}]\\ &=-_{i}((;f)}{ _{i}})^{2}+^{2}})^{-2} (2(;f)}{_{i}}  p(y_{t};f)}{_{i}_{ j}}))_{a}((_{u};)}{ _{i}})^{2}\\ &+_{i}((;f)}{ _{i}})^{2}+^{2}})^{-1}2 _{a}((_{u};)}{ _{i}}f_{a}(_{u};)}{_{i} _{j}}).\] (18)

We note how the above has the following general form

\[()_{j}&-2 _{i}A_{i}(_{t},y_{t})^{-2}(U(_{t};)_{i,j} ;f)}{_{i}})_{a}( (_{u};)}{_{i}})^{ 2}\\ &+2_{i}A_{i}(_{t},y_{t})^{-1}_{a}(V_{a}(_ {u};)_{i,j}(_{u};)}{ _{i}}),\] (19)

where \(A_{i}(_{t},y_{t})=^{2}}+(;f)}{_{i}})^{2}\) and the matrices \(U\) and \(V_{a}\) are to be chosen. For the choices \(U= p(y_{t};f)}{_{i}_{ j}}\) and \(V_{a}=f_{a}(_{u};)}{_{i} _{j}}\) we recover the true gradient of the self-diversity term. However, we note that the final gradient is still expensive to compute.

Instead, we make the choice

\[U(_{t};)_{i,i}=[;f)}{ _{i}}]^{2}, V_{a}(_{u};)_{i,i}= [(_{u};)}{_{i}} ]^{2},\] (20)

and we restrict \(_{i}\) to \(_{i=j}\). This results in

\[()_{j}&=-2 A_{j}( Z_{})^{-2}[;f)}{ _{j}}]^{3}_{a}((_{u}; )}{_{j}})^{2}\\ &+2A_{j}(Z_{})^{-1}_{a}[(_{u},)}{_{j}}]^{3},\] (21)where \(,\).

For an unlabeled dataset \(Z_{u}\) and training dataset \(Z_{t}\) we finally get

\[()_{j} =-2_{_{},y_{i} Z_{}} (;f)}{_{j}})^{3}A(Z_{ })^{-2}_{_{u} Z_{}}_{a}((_{u};)}{_{j}})^{2}\] (22) \[+A(Z_{})^{-1}2_{_{u} Z_{}}_{ a}((_{u};f)}{_{j}})^{3}.\]

where \(A(Z_{})=^{2}}+_{_{}, y_{i} Z_{}}(;f)}{_{j}} )^{2}\) and \(,\). We use Equation (22) as the \(_{}()\) diversity vector field, while \(_{+}()\) results from restricting \(_{a}\) to \(_{a=a_{}}\).

We now discuss our choices for \(\) and \(\). We plot in Figure 3 the final vector field \(-(-()_{j})\) for \( 1\) and \(1_{}^{2}>0\) which are realistic ranges for the temperature and the prior, a single training sample \((_{},y_{})\), a single unlabeled sample \((_{},)\), a single weight \(_{j}\), as well as a single output dimension \(a\). We first notice that \(\) has a very small effect on the final gradient, inducing a slight asymmetry. In principle, we could also set \(=0\) without any significant loss in our approximation. The most significant difference comes from the _sign_ of \(\). For \(>0\) the regulariser encourages our predictor to fit random labels on the unlabeled data. This is because \(-(-()_{j})\) and \((_{};f)}{_{j}}\) have the same sign. Note how \((_{};f)}{_{j}}\) is the direction that maximizes the logit of class \(a\). When \(<0\) we get the opposite effect. We now encourage the probabilities for all classes (or a random class in the case of SO+) to be low for unlabeled data.

We ran small-scale experiments where choices other than \(=1\) and \(=1\) did not yield promising results. We thus selected \(=1\) and \(=1\) for our final regularizer.

**Motivating the choices \(U(_{};)_{i,i}\) and \(V_{a}(_{};)_{i,i}\)**

We chose

\[U(_{};)_{i,i}=[};f)}{_{i}}]^{2}, V_{a}(_{ };)_{i,i}=[(_{};)}{_{i}}]^{2}.\] (23)

Consider the nonlinear least squares problem \(()=(f(;)-y)^{2}\) then the Hessian can be written as

\[^{2}()=_{}f(;) _{}f(;)^{}+r_{}^{2}f( {x};)\]

where \(r=f(;)-y\) is the residual. As such for the square loss both \(_{}f(;)_{}f(;) ^{}\) and \(_{}^{2}f(;)\) provide some information on the curvature. This is our motivation for using \(_{}f(;)_{}f(;)^{}\) as a source of curvature information in the place of \(_{}^{2}f(;)\). This resulted in exploring \(V_{a}(_{};)_{i,i}=[( _{};)}{_{i}}]^{2}\).

Similarly, it is interesting to explore \(U(_{};)_{i,i}=[_ {};f)}{_{i}}]^{2}\) as we have already approximated second-order derivates of the loss with squares of the first-order derivatives when approximating the Hessian with the Fisher. Both of the above are heuristics, and as we are forcing the corresponding matrix entries to have a positive sign we explored \(,\) which change this sign.

## 10 Pareto optimality

In multi-objective optimization, the Pareto front (also called Pareto frontier or Pareto curve) is the set of all Pareto efficient solutions. Consider \(A\) a set of criterion vectors in \(^{m}\). Assume that the preferred directions of criteria values is known. A point \(a^{}^{m}\) is preferred to (strictly dominates) another point \(a^{}^{m}\), written as \(a^{} a^{}\), when \(a^{}\) improves all available criteria jointly compared to \(a^{}\). The Pareto frontier is thus written as:

\[P(A)=\{a^{} A:\{a^{} A:a^{} a^{ },a^{} a^{}\}=\}.\]The ideal point is the point that optimizes all criteria in the best possible way. In our case we use \(a=[1-,]\) as our criterion vector with \(a(0,1)^{2}\). Our ideal point is then \(a^{}=\) the point where both the misclassification rate and the TACE are 0. Intuitively this predictor makes both perfect and perfectly calibrated predictions. In the absence of other criteria, the point closest to the ideal point \(_{a P(A)}\|a-a^{}\|_{2}^{2}\) is often considered the optimal one. We use the phrase "Pareto optimal" in this sense. Note that alternatively one can refer to all points on the Pareto front as optimal with the point closest to the ideal point referred to as the knee point.

## 11 Experimental setup

We run our experiments on a combination of NVIDIA A100 and V100 GPUs, on our local cluster. The total computation time, including hyperparameter tuning and training, was approximately 1600 GPU hours. Hyperparameter tuning was done over a single random seed per each ensemble member due to the computational cost.

In the following list, we include the libraries and datasets that we used together with their corresponding licenses

* PyTorch package (Paszke et al., 2019): Modified BSD Licence
* MNIST-10 dataset (Deng, 2012): MIT Licence
* CIFAR-10 dataset (Krizhevsky and Hinton, 2009): MIT Licence
* CIFAR-100 dataset (Krizhevsky and Hinton, 2009): MIT Licence
* SVHN dataset (Netzer et al., 2011): -
* FashionMnist dataset (Xiao et al., 2017): MIT Licence
* JAX (Bradbury et al., 2018): Apache License 2.0
* flax (Heek et al., 2023): Apache License 2.0

Figure 3: We plot \(-(-()_{j})\) with \(,\{-1,0,1\}\) for \( 1\) and \(1_{}^{2}>0\) which are realistic ranges for the temperature and the prior, a single training sample \((_{t},y_{t})\), a single unlabeled sample \((_{u},)\), a single weight \(_{j}\), as well as a single output dimension \(a\). We first notice that \(\) has a very small effect on the final gradient, inducing a slight asymmetry. The most significant difference comes from the _sign_ of \(\). For \(>0\) the regulariser encourages our predictor to fit random labels on the unlabeled data. This is because \(-(-()_{j})\) and \((_{j};f)}{_{j}}\) have the same sign (where we either select \(a\) at random in SO+ or we optimize over all \(a\) in SO). Note how \((_{j};f)}{_{j}}\) is the direction that maximizes the logit of class \(a\). When \(<0\) we get the opposite effect. We now encourage the logits for all classes (or a random class) to be low for unlabeled data.

More experiments

We include here experiments on the CIFAR-10 and CIFAR-100 datasets, for the MLP and LeNet architectures and the case of data augmentation. We use random flips and crops as is the standard for these two datasets. We observe similar results to the main text. We achieve the best test accuracy in all experiments. At the same time we are Pareto optimal in 3 out of 4 cases.

**Computational and Memory Cost** Our memory cost increases linearly with the number of classes for the SO algorithm. For the SO+ algorithm, the additional memory cost is constant. At the same time, the computation time for the SO algorithm roughly increased roughly by a factor of 2. For the SO+ algorithm, we sometimes observed an increase by a factor of 10. This is potentially because we need to estimate per sample _and per output_ gradients potentially degrading parallelization.
Figure 4: The effect of the ensemble size on CIFAR10/ CIFAR100/ FMNIST/ SVHN and LeNet/ MLP/ ResNet22 in the small data regime. In most cases SO ensembles achieve better test accuracy with fewer ensemble members than Standard ensembles.