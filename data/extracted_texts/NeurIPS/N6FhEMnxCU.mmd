# Label Robust and Differentially Private Linear Regression: Computational and Statistical Efficiency

Xiyang Liu

Paul Allen School of Computer Science & Engineering

University of Washington

xiyangl@cs.washington.edu

&Prateek Jain

Google Research

prajain@google.com

&Weihao Kong

Google Research

weihaokong@google.com

&Sewoong Oh

Paul Allen School of Computer Science & Engineering

University of Washington, and Google Research

sewoong@cs.washington.edu

&Arun Sai Suggala

Google Research

aruns@google.com

###### Abstract

We study the canonical problem of linear regression under \((,)\)-differential privacy when the datapoints are sampled i.i.d. from a distribution and a fraction of response variables are adversarially corrupted. We provide the first provably efficient - both computationally and statistically - method for this problem, assuming standard assumptions on the data distribution. Our algorithm is a variant of the popular differentially private stochastic gradient descent (DP-SGD) algorithm with two key innovations: a full-batch gradient descent to improve sample complexity and a novel adaptive clipping to guarantee robustness. Our method requires only linear time in input size, and still matches the information theoretical optimal sample complexity up to a data distribution dependent condition number factor. Interestingly, the same algorithm, when applied to a setting where there is no adversarial corruption, still improves upon the existing state-of-the-art and achieves a near optimal sample complexity.

## 1 Introduction

Differential Privacy (DP)  is a standard notion of privacy widely adopted by both industry and government . With widespread usage of ML and statistical techniques, DP becomes even more critical to ensure private information of participating individuals is not revealed in any form via the learned model. An statistical estimator is said to be \((,)\)-differentially private if presence/absence of an individual's data point in the dataset does not significantly change the estimated output. Smaller \(>0\) and \(\) imply stronger privacy guarantees.

While privacy preserving statistical estimators have been studied extensively in recent past, several critical questions remain open (see App. A for a survey). Consider the canonical statistical task of linear regression with \(n\) i.i.d. samples, \(\{(x_{i}^{d},y_{i})\}_{i=1}^{n}\), drawn from \(x_{i}(0,)\), \(y_{i}=x_{i}^{}w^{*}+z_{i}\), \(z_{i}(0,^{2})\) and \([x_{i}z_{i}]=0\) for some true parameter \(w^{*}^{d}\). The error is measured in \((1/)||-w^{*}||_{}:=(1/)||^{1/2}(-w^{*})|\), which correctly accounts for the signal-to-noise ratio in each direction; in the direction of large eigenvalue of \(\), we have larger signal inbut the noise \(z_{i}\) remains the same. We expect smaller errors in those directions, which is accounted for in the error measure \((1/)\|-w^{*}\|_{}\).

Minimax optimal sample complexity for estimating the optimal linear regression model with DP was recently established. For the lower bound, using recently introduced score attack technique, [16, Theorem 3.1] shows that \(n=(d/^{2}+d/())\) samples are necessary to achieve an error of \((1/)\|-w^{*}\|_{}=\) (in expectation). For the matching upper bound, High-dimensional Propose-Test-Release (HPTR) in  and Robust-to-Private in  show that \(n=(d/^{2}+d/())\) samples are also sufficient. The first term of \(d/^{2}\) is the fundamental sample complexity even if privacy is not required, and the second term of \(d/()\) is the cost of privacy.

This implies that, statistically, the problem appears to be solved. However, computationally, the problem is still open despite multiple studies of the problem. That is, the statistical optimal algorithms still take exponential time.

After a series of efforts in computationally efficient approaches as surveyed in App. A,  achieves the best known sample complexity of \(n=(d/^{2}+ d/()+^{2}d/)\), where \(\) is the condition number of the covariance \(\) of the covariates. Compared to HPTR, the cost of computational efficiency is factor of \(\) in the second term and the third term that is unnecessary. As the condition number can be quite large, improving the dependence on \(\) is of utmost importance. Furthermore, the technique of  strictly requires sampling without replacement, whose analysis relies on having an explicit form of the end-to-end update. In particular, their analysis technique is not applicable to the case with corrupted samples.

In contrast, we propose a novel method (Alg. 1) that builds upon full-batch gradient descent and applies a carefully chosen adaptive clipping which is a general technique used in practice as well . Together with an intuitive but intricate analysis technique, we improve the sample complexity to \(n=(d/^{2}+^{1/2}d/())\).

**Corollary 1.1** (Corollary of Thm. 3.1 for sub-Gaussian data).: _Alg. 1 is \((,)\)-DP. Let \(S=\{(x_{i},y_{i})\}_{i=1}^{n}\) be a dataset of i.i.d. samples with \(x_{i}(0,)\), \(y_{i}=x_{i}^{}w^{*}+z_{i}\) and \(z_{i}(0,^{2})\) for some unknown true parameter \(w^{*}=^{-1}[y_{i}x_{i}]^{d}\) and unknown \(\) and \(^{2}\). Then \(n=(d/^{2}+^{1/2}d/())\) samples are sufficient for Alg. 1 to achieve \((1/)\|-w^{*}\|_{}=()\) with high probability, where \(:=_{}()/_{}()\)._

Due to space constraints, we focus on sub-Gaussian distributions in the main text and provide comparisons to prior work in Tab. 1. Our analysis in App. H applies to a more general family of _light-tailed_ distributions, called sub-Weibull. Next, when the noise in the samples is _heavy-tailed_, a similar algorithm can be applied with carefully chosen clipping thresholds to account for the heavier tail. Concretely, for \(k\)-th moment bounded distributions, the tail of the distribution gets increasingly heavier with smaller \(k\). This would require larger number of samples to achieve the same accuracy, which is captured in our sample complexity of \(n=(d/^{2k/(k-1)}+^{1/2}d/(^{k/(k-1)}))\). We explain the heavy-tailed setting, provide a detailed analysis and a proof, and discuss the results in App. L. This is the first efficient algorithm with provable guarantees achieving \((,)\)-DP.

**Corollary 1.2** (informal version of Coro. L.7 for heavy-tailed noise).: _Alg. 4 is \((,)\)-DP. Let \(S=\{(x_{i},y_{i})\}_{i=1}^{n}\) be a dataset of i.i.d. samples with \(x_{i}(0,)\), \(y_{i}=x_{i}^{}w^{*}+z_{i}\), and the zero-mean, independent, and heavy-tailed noise \(z_{i}\) satisfies \([|z/|^{k}]=O(1)\) for some unknown true parameter \(w^{*}^{d}\) and unknown \(\) and \(^{2}\). Then \(n=(d/^{2k/(k-1)}+^{1/2}d/(^{k/(k-1)}))\) samples are sufficient for Alg. 4 in App. L to achieve an error rate of \((1/)\|-w^{*}\|_{}=()\) with high probability, where \(:=_{}()/_{}()\)._

Perhaps surprisingly, we show that Alg. 1 is also robust against label-corruption, where an adversary selects an arbitrary \(_{}\) fraction of the data points and changes their response variables arbitrarily. Ideally, we want a robust algorithm against a stronger adversary who can corrupt the covariates also. However, even for a simpler problem of private mean estimation, achieving robustness against such a strong adversary with \(O(d)\) samples requires heavy machinery (convex relaxations of sum-of-squares optimization) with significantly more computations (although polynomial) .

Our lower bound in Prop. 3.9, together with the lower bound in  on the uncorrupted case, shows that \(n=(d/^{2}+d/())\) samples are necessary to achieve an error rate of \((1/)\|-w^{*}\|_{}=O(+_{})\). In particular, it is impossible to achieve an error below \(_{}\) even if we have infinite samples (Prop. 3.9), and hence there is no need to aim for \(<_{}\). This lower bound is matched by exponential time approaches, HPTR in  and Robust-to-Private in , which also guarantee robustness. Currently, there is no efficient algorithm that can guarantee both privacy and robustness for linear regression. To this end, we provide the first efficient algorithm guaranteeing both, with a sample complexity that is optimal up to a \(^{1/2}\) factor.

**Corollary 1.3** (Corollary of Thm. H.3 for sub-Gaussian data with adversarial label corruption).: _Under the hypotheses of Coro. 1.1, suppose \(_{}\)-fraction of the labels are corrupted arbitrarily. Then \(n=(d/^{2}+^{1/2}d/())\) samples are sufficient for Alg. 1 to achieve an error rate of \((1/)\|-w^{*}\|_{}=(+_{})\) with high probability, where \(:=_{}()/_{}()\)._

When \(_{}=0\), this recovers the non-robust result from Coro. 1.1. A similar robustness guarantee also holds for heavy-tailed settings. We provide a formal statement in App. L

**Contributions.** For a canonical problem of private linear regression under sub-Gaussian distributions, the best known efficient algorithm  requires

\[n=(}++ d}{})\,\]

to achieve \((1/)\|-w^{*}\|_{}=\). We provide the first efficient algorithm that improves this to

\[n=(}+d}{} )\,\]

which nearly matches the exponential-time algorithms  and the lower bound  up to \(^{1/2}\) in the second term. For the same problem, we show that the same algorithm is the first to achieve robustness against adversarial corruption of the labels.

Under a heavy-tailed distribution of the noise, we provide the first computationally efficient algorithm, to the best of our knowledge, that achieves a sample complexity close to that of an exponential-time algorithm of . There is no matching lower bound in the heavy-tailed setting. This is also the first efficient algorithm to achieve robustness against adversarial corruption of the labels under heavy-tailed noise.

## 2 Problem formulation and background

When there is no adversary, we present our results under the standard linear model with sub-Gaussian covariates and noise. In App. H, we present a more general family of \((K,a)\)-sub-Weibull distributions that recovers the standard sub-Gaussian family as a special case when \(a=0.5\). The necessity of such assumptions on the tail is explained in Sec. 3.4.

**Assumption 2.1** (sub-Gaussian model).: We have i.i.d. samples \(S=\{(x_{i}^{d},y_{i})\}_{i=1}^{n}\) from a distribution \(_{,w^{*},^{2}}\) of a linear model \(y_{i}= x_{i},w^{*}+z_{i}\), where the input vector \(x_{i}\) has zero

   Algorithm & Runtime & Sample Complexity \\  TukeyEM & poly & no guarantee \\ DP-Theil-Sen\({}^{}\) & poly & \(}{^{2}}+^{c}\) \\ DP-AMBSSGD & poly & \(}++d}{ }\) \\ DP-RobGD [Theorem 3.8] & poly & \(}+^{1/2}\) \\  HPTR, Robust-to-private & exp & \(}+\) \\ Lower Bound & & \(}+\) \\   

Table 1: Suppose data is drawn from a linear model in \(d\)-dimensions from sub-Gaussian covariates with covariance \(\) and sub-Gaussian noise with variance \(^{2}\). To achieve an error rate of \((1/)\|-w^{*}\|_{}=\) with \((,)\)-DP, DP-RobGD requires the least number of samples among computationally efficient algorithms. This improves over  by a factor of \(^{1/2}\) in the second term, where \(\) is the condition number of \(\). We hide polylogarithmic factors in \(d\), \(\) and \(1/\). \({}^{}\)DP-Theil-Sen is only analyzed when \(=1\) and its dependence \(^{c}\) is unknown.

mean \([x_{i}]=0\) and a positive definite covariance \(:=[x_{i}x_{i}^{}] 0\), and the (input dependent) label noise \(z_{i}\) has zero mean \([z_{i}]=0\) and variance \(^{2}:=[z_{i}^{2}]\). We further assume \([x_{i}z_{i}]=0\), which is equivalent to assuming that the true parameter \(w^{*}=^{-1}[y_{i}x_{i}]\). We assume the marginal distributions of \(x_{i}\) and \(z_{i}\) are \(K\)-sub-Gaussian with \(K=O(1)\), as defined below.

**Definition 2.2**.: \(x^{d}\) is \(K\)-sub-Gaussian if for all \(v^{d}\), \([(}{K^{2}[(v,x)^ {2}]})] 2\).

Given a dataset \(S\) that is i.i.d. sampled from \(_{^{2},w^{*},^{2}}\) satisfying Asmp. 2.1, our goal is to estimate \(w^{*}\) that minimizes \((1/)\|-w^{*}\|_{}\) which is also equivalent to minimize the excess population risk, i.e., \((w^{*})-()\) where \((w):=_{(x,y)_{,w^{*},^{2}}}[( y- w,x)^{2}]\).

**Notations.** A vector \(x^{d}\) has the Euclidean norm \(\|x\|\). For a matrix \(M\), we use \(\|M\|_{2}\) to denote the spectral norm. The error is measured in \(\|-w^{*}\|_{}:=\|^{1/2}(-w^{*})\|\) for some PSD matrix \(\). The identity matrix is denoted by \(_{d}^{d d}\). Let \([n]=\{1,2,,n\}\). \(()\) hides some constants terms, \(K=(1)\), and poly-logarithmic terms in \(n\), \(d\), \(1/\), \((1/)\), \(1/\), and \(1/_{}\). For a vector \(x^{d}\), we define \(_{a}(x):=x\{1,a/\|x\|\}\).

**Background on DP.** Differential Privacy is a standard measure of privacy leakage when data is accessed via queries, introduced by . Two datasets \(S\) and \(S^{}\) are said to be neighbors if they differ at most by one entry, which is denoted by \(S S^{}\). A stochastic query \(q\) is said to be \((,)\)-differentially private for some \(>0\) and \(\), if \((q(S) A) e^{}(q(S) A)+\), for all neighboring datasets \(S S^{}\) and all subset \(A\) of the range of the query. We build upon two widely used DP primitives, the Gaussian mechanism and the private histogram. A central concept in DP mechanism design is the _sensitivity_ of a query, defined as \(_{q}:=_{S S^{}}\|q(S)-q(S^{})\|\). We describe Gaussian mechanism and private histogram in App. B.

### Comparisons with the prior work

The state-of-the-art approach introduced by  is based on DP-SGD , where privacy is ensured by gradient norm clipping and the Gaussian mechanism. Two additional technical components are adaptive clipping and streaming SGD. Adaptive clipping with an appropriate threshold \(_{t}\) ensures that no data point is clipped (under the sub-Gaussian assumption), while providing a bound on the sensitivity of the average mini-batch gradient (to ensure we do not add too much noise). The streaming approach, where each data point is only touched once and discarded, ensures independence between the past iterate \(w_{t}\) and the gradients at round \(t+1\), which the analysis critically relies on. For \(T=()\) iterations where \(\) is the condition number of the covariance \(\), the dataset \(S=\{(x_{i},y_{i})\}_{i=1}^{n}\) is partitioned into \(\{B_{t}\}_{t=0}^{T-1}\) subsets of equal size: \(|B_{t}|=(n/)\). At each round \(t\), the gradients are clipped and averaged with additive Gaussian noise chosen to satisfy \((,)\)-DP:

\[w_{t+1}\ \ w_{t}-|}_{i B_{t}} _{_{t}}(x_{i}(w_{t}^{}x_{i}-y_{i}))+ }{|B_{t}|}_{t}\,\] (1)

where \(_{t}(0,_{d})\). In , a slight variation of this streaming SGD is shown to achieve an error of \((1/)\|w_{T}-w^{*}\|_{}=\) with \(n=(d/^{2}+ d/()+^{2}d/)\) samples (Row 3 in Tab. 1).

**Our technical innovations.** Our approach builds upon such gradient based methods but makes several important innovations. First, we use full-batch gradient descent, as opposed to the streaming SGD above. Using all \(n\) samples reduces the sensitivity of the per-round gradient average by a \(\) factor, and thus decreases the privacy noise added in each iteration. This improves the second term of sample complexity from \( d/()\) to \(^{1/2}d/()\) and removes the third term completely. However, full-batch GD loses the independence that the streaming SGD enjoyed between \(w_{t}\) and the samples used in the round \(t+1\). This dependence makes the analysis more challenging. We instead propose using the _resilience_ to precisely track the bias and variance of the (dependent) full-batch average gradient. Resilience is a central concept in robust statistics that links the tail-property of the distribution to the bias, which we explain in Sec. 5.

Next, one critical component in achieving this improved sample complexity is the new analysis technique we introduce for tracking the end-to-end gradient updates. Since our gradient descent algorithm is not guaranteed to make progress every step, we cannot use the vanilla one-step analysis.

Taking the full end-to-end analysis by expanding the whole gradient trajectory will introduce too many correlated cross-terms which are very hard to control. Therefore, we leverage an every \(\)-step analysis and show that the objective function at least decreases geometrically every \(\) steps. To be more specific, our analysis technique in App. H (steps 3 and 4) opens up the iterative updates from the beginning to the end, and exploits the fact that \(_{}(()^{1/2}(1-)^{i}()^{1/2})\) is upper bounded by \(1/(i+1)\) when \(\|\| 1\). This technique is critical in achieving the near-optimal dependence in \(\). This might be of independent interest to other analysis of gradient-based algorithms. We refer to the beginning of step 3 in App. H for a detailed explanation.

Finally, we propose a novel clipping that separately clips \(x_{i}\) and \((w_{t}^{}x_{i}-y_{i})\) in the gradient, \((w_{t}^{}x_{i}-y_{i})x_{i}\). This is critical in achieving robustness to label-corruption, as we explain in Sec. 3.1.

## 3 Label-robust and private linear regression

We introduce a novel gradient descent approach. This achieves an improved sample complexity compared to the state-of-the-art algorithm and robustness against label corruption.

### Algorithm

The skeleton of our approach in Alg. 1 is the general DP-SGD [1; 71] with adaptive clipping . We partition the dataset into three equal-sized subsets: \(S_{1},S_{2},S_{3}\). \(S_{1}\) and \(S_{2}\) are used in adaptively estimating the clipping thresholds, and \(S_{3}\) is re-used every step to compute the average gradient.

The standard adaptive clipping, e.g., [7; 81], is not robust against label-corruption. Under sub-Gaussian distribution, a positive fraction of the covariates, \(x_{i}\)'s, can be close to the origin. If the adversary chooses to corrupt those points with small norm, \(\|x_{i}\|\), they can make large changes in the corrupted residual, \((y_{i}-w_{t}^{}x_{i})\), while evading the standard clipping by the norm of the gradient; the norm of the gradient, \(\|x_{i}(y_{i}-w_{t}^{}x_{i})\|=\|x_{i}\|\,|y_{i}-w_{t}^{}x_{i}|\), can remain under the threshold. This is problematic, since the bias due to the corrupted samples in the gradient scales proportionally to the magnitude of the residual (after clipping). To this end, we propose clipping the norm and the residual separately: \(_{}(x_{i})_{_{t}}(w_{t}^{}x_{i }-y_{i})\). This keeps the sensitivity of gradient average bounded by \((_{t})\). The subsequent Gaussian mechanism in line 11 ensures \((_{0},_{0})\)-DP at each round. Applying advanced composition in Lemma B.5 of \(T\) rounds, this ensures end-to-end \((,)\)-DP.

**Novel adaptive clipping.** When clipping with \(_{}(x_{i})\), the only purpose of clipping the covariate by its norm, \(\|x_{i}\|\), is to bound the sensitivity of the resulting clipped gradient. In particular, we do not need to make it robust as there is no corruption in the covariates. Ideally, we want to select the smallest threshold \(\) that does not clip any of the covariates. Since the norm of a covariate is upper bounded by \(\|x_{i}\|^{2} K^{2}()(1/)\) with probability \(1-\) (Lemma J.3), we estimate the unknown \(()\) using Private Norm Estimator in Alg. 3 in App. F and set the norm threshold \(=K\) (Alg. 1 line 4). The \(n\) in the logarithm ensures that the union bound holds.

When clipping with \(_{_{t}}(w_{t}^{}x_{i}-y_{i})\), the purpose of clipping the residual by its magnitude, \(|y_{i}-w_{t}^{}x_{i}|=|(w^{*}-w_{t})^{}x_{i}+z_{i}|\), is to bound the sensitivity of the gradient and also to provide robustness against label-corruption. We want to choose a threshold that only clips corrupt data points and at most a few clean data points. In order to achieve an error \((1/)\|w_{T}-w^{*}\|_{}=\), we know that any set of \((1-)\) fraction of the clean data points is sufficient to get a good estimate of the average gradient. By clipping at \(|(w^{*}-w_{t})^{}x_{i}+z_{i}|^{2}(\|w_{t}-w^{*}\|_{}^{2}+ ^{2})CK^{2}(1/(2))\), Lemma J.3 guarantees that the unclipped subset will be large enough, i.e., \((1-)n\). At the same time, this threshold on the residual is small enough to guarantee robustness against the label-corrupted samples. We introduce the robust and DP Distance Estimator in Alg. 2 to estimate the unknown (squared and shifted) distance, \(\|w_{t}-w^{*}\|_{}^{2}+^{2}\), and set the distance threshold \(_{t}=2}K^{2}(1/(2))}\) (Alg. 1 line 7). Both norm and distance estimation rely on DP histogram (Lemma B.2), but over a set of statistics computed on partitioned datasets, which we explain in detail in App. C.

### Analysis without adversarial corruption

We show that Alg. 1 achieves an improved sample complexity. We provide the proof for a more general class of distributions in App. H and a sketch of the proof in Sec. 5. We address the necessity of the assumptions in Sec. 3.4, along with some lower bounds.

**Theorem 3.1**.: _Alg. 1 is \((,)\)-DP. Under sub-Gaussian model of Asmp. 2.1, for any failure probability \((0,1)\) and target error rate \(\), if the sample size is large enough such that_

\[n=(K^{2}d^{2}()+}+dT^{1/2}())}}{}),\] (2)

_with a large enough constant, then the choices of a step size \(=1/(C_{}())\) for any \(C 1.1\) and the number of iterations, \(T=((\|w^{*}\|))\) for a condition number of the covariance \(:=_{}()/_{}()\), ensures that, with probability \(1-\), Alg. 1 achieves_

\[_{_{1},,_{T}(0,_{ })}\,\|w_{T}-w^{*}\|_{}^{2}\,= \,K^{4}^{2}^{2}^{2}()\;,\] (3)

_where the expectation is taken over the noise added for DP, and \(\) and \(()\) hide logarithmic terms in \(K,,d,n,1/,(1/),1/\), and \(\)._

_Remark 3.2_.: Omitting some constant and logarithmic terms, Alg. 1 requires

\[n = \,}+d}{ }\,\;,\] (4)

samples to ensure an error rate of \((1/^{2})[\|w_{T}-w^{*}\|_{}^{2}]=(^{2})\). From [16, Theorem 3.1], there exists an \(n=(d/^{2}+d/())\) lower bound, and our upper bound matches this lower bound up to a factor of \(^{1/2}\) in the second term and other logarithmic factors. Eq. (4) is the best known rate among all efficient private linear regression algorithms, strictly improving upon the state-of-the-art. The best existing efficient algorithm by  requires \(n=(d/^{2}+ d/()+^{2}d/)\) to achieve the same error rate. Compared to Eq. (4), the second term is larger by a factor of \(^{1/2}\) compared to the second term in Eq. (4). Further,  requires \(^{2}d/\), which is not needed in Eq. (4).

_Remark 3.3_.: Consider the standard settings of linear regression with \(x_{i}(0,_{d})\) and \(z_{i}(0,^{2})\) such that the condition number is one, our bound given by Eq. (4) nearly matches the lower bound ([16, Theorem 3.1]) up to logarithmic factors.

_Remark 3.4_.: Note that the leading term in Eq. (4) is the first term \(d/^{2}\) when target error \(/^{1/2}\). Our first term is independent of \(\), which matches the lower bound for non-private linear regression.

_Remark 3.5_.: The third term \(^{2}d/\) in  is independent of error rate \(\) but scales as \(^{2}\). This term is required to ensure the privacy noise added in each iteration is small enough for their DP-SGD to make progress (Appendix. B.2.2 in ). Our algorithm is based on full-batch gradient descent, which uses all \(n\) samples and thus reduces the sensitivity of gradient average by a \(\) factor. As a result, we show in Eq. (59) that our algorithm only requires \(n=((1/)d/})\) to make progress for each iteration. This is strictly smaller than our dominant term \(^{1/2}d/()\) and does not show up in our final guarantee. We provide a formal proof in App. H.

_Remark 3.6_.: One of the key innovations in Alg. 1 is the adaptive distance estimator (Alg. 2 in App. C). The goal is to privately estimate the (shifted) distance of the current estimate, i.e., \(\|w_{t}-w^{*}\|_{}+^{2}\), without the knowledge of \(w^{*}\). We show in Thm. C.1 that our novel distance estimator only requires an error-independent sample complexity \(n=(^{1/2}d/)\) to achieve a constant multiplicative error. Note that the DP-STAT (Algorithm 3 in ) can also be used to estimate the distance. But it requires the knowledge of domain size \(\|w^{*}\|_{}+\). We completely remove this requirement, improve the dependence on \(K\) and \((n)\), and show it is also robust, as introduced in the next section. We provide the algorithms and analysis in App. C and the formal proof in App. D.

### Robustness against label corruption

We assume there exists a good dataset \(S_{}\) that satisfies Asmp. 2.1. We only get access to a label-corrupted dataset under the standard definition of _label corruption_, e.g., . There are variations in literature on the definition, which we survey in App. A.

**Assumption 3.7** (\(_{}\)-corruption).: Given a dataset \(S_{}=\{(x_{i},y_{i})\}_{i=1}^{n}\), an adversary inspects all the data points, selects \(_{}n\) data points denoted as \(S_{r}\), and replaces the labels with arbitrary labels while keeping the covariates unchanged. We let \(S_{}\) denote this set of \(_{}n\) newly labelled examples by the adversary. Let the resulting set be \(S_{}:=S_{} S_{} S_{r}\).

Our goal is to estimate the unknown parameter \(w^{*}\), given corrupted dataset \(S_{}\), distribution parameter \(K\), and (an upper bound on) the corruption level \(_{}\).

Under the _non-private scenario_, i.e., \(=\), recent advances led to optimal algorithms for linear regression that are robust to label corruptions [15; 21]; if the corruption level is smaller than the target error rate, i.e., \(_{}\), then \(n=(d/^{2})\) samples are sufficient to achieve an error rate of \((1/)\|-w^{*}\|_{}=\). The sample complexity of \(d/^{2}\) is optimal as it matches the information theoretic lower bound. The condition \(_{}\) is necessary since it is information theoretically impossible to achieve error \(\) less than \(_{}\), as we prove in Prop. 3.9. Setting the target error to the minimum possible value of \(=_{}\), we say that these algorithms achieve optimal robustness since the minimum robust error rate of \((1/)\|-w^{*}\|_{}=O(_{})\) can be achieved with minimal sample complexity of \(n=(d/_{}^{2})\). We aim to achieve such optimal robustness simultaneously with differential privacy in a computationally efficient manner.

**Theorem 3.8**.: _Under sub-Gaussian model of Asmp. 2.1 and \(_{}\)-corruption of Asmp. 3.7, if the corruption level is below the target error rate, \(_{}\), then \(n=(d/^{2}+^{1/2}d/())\) samples are sufficient for Alg. 1 to achieve an error rate of \((1/^{2})[\|-w^{*}\|_{}^{2}]=(^{2})\)._

This is the first efficient approach to achieve robustness and \((,)\)-DP simultaneously. The existing such algorithms take exponential time [61; Corollary C.2] and , but achieve optimal sample complexity of \(n=O(d/^{2}+d/())\). Notice that there is no dependence on \(\). It remains an open question if _computationally efficient_ private linear regression algorithms can achieve such an optimal \(\)-independent sample complexity. We make the first advance towards this ambitious goal with the above theorem. Our sample complexity is sub-optimal only by a factor of \(\) in the second term. This is achieved by individually clipping the covariate, \(x_{i}\), and the residual, \((w_{t}^{}x_{i}-y_{i})\), in Alg. 1 and carefully tracking the bias of clipping with the use of resilience in the analysis in App. H.

### Lower bounds

**Necessity of our assumptions.** A tail assumption on the covariate \(x_{i}\) such as Asmp. 2.1 is necessary to achieve \(n=O(d)\) sample complexity in Eq. (4). Even when the covariance \(\) is close to identity, without further assumptions on the tail of covariate \(x\), the result in  implies that for \(<1/n\), it is necessary for an \((,)\)-DP estimator to have \(n=(d^{3/2}/())\) samples to achieve \(\|-w^{*}\|_{}=()\) (see Eq. (3) in ). Note that this lower bound is a factor \(d^{1/2}\) larger than our upper bound that benefits from the additional tail assumption.

A tail assumption on the noise \(z_{i}\) such as Asmp. 2.1 is necessary to achieve \(n=O(d/())\) dependence on the sample complexity in Eq. (4). For heavy-tailed noise, such as \(k\)-th moment bounded noise, the dependence can be significantly larger. [61, Proposition C.5] implies that for \(=e^{-(d)}\) and \(4\)-th moment bounded \(x_{i}\) and \(z_{i}\), any \((,)\)-DP estimator requires \(n=(d/(^{2}))\), which is a factor of \(1/\) larger, to achieve \((1/^{2})\|-w^{*}\|_{}=()\).

The assumption that only labels are corrupted is critical for Alg. 1. The average of the clipped gradients can be significantly more biased, if the adversary can place the covariates of the corrupted samples in the same direction. In particular, the bound on the bias of our gradient step in Eq. (44) in App. H would no longer hold. Against such strong attacks, one requires additional steps to estimate the mean of the gradients robustly and privately, similar to those used in robust private mean estimation . There is no known linear-time algorithm to achieve this, and this is outside the scope of this paper.

**Lower bounds under label corruption.** Under the \(_{}\) label corruption setting (Asmp. 3.7), even with infinite data and without privacy constraints, no algorithm is able to learn \(w^{*}\) with \(_{2}\) error better than \(_{}\). We provide a formal derivation for completeness.

**Proposition 3.9**.: _Let \(_{,^{2},w^{*},K}\) be a class of distributions on \((x_{i},y_{i})\) from sub-Gaussian model in Asmp. 2.1. Let \(S_{n,}\) be an \(\)-corrupted dataset of \(n\) i.i.d. samples from some distribution \(_{,^{2},w^{*},K}\) under Asmp. 3.7. Let \(\) be a class of estimators that are functions over \(S_{n,}\). Then there exists a constant \(c\) such that \(_{n,}\ \ _{S_{n,},_{ ,^{2},w^{*},K},w^{*},K}\ \ [\|-w^{*}\|_{}^{2}] c\, ^{2}\,^{2}\)._

A proof is provided in App. I.1. A similar lower bound can be found in [11, Theorem 6.1].

## 4 Experimental results

### DP Linear Regression

We present experimental results comparing our proposed technique (DP-RobGD) with other baselines. We consider non-corrupted regression in this section and defer corrupted regression to the App. K. We begin by describing the problem setup and the baseline algorithms first.

**Experiment Setup.** We generate data for all the experiments using the following generative model. The parameter vector \(w^{*}\) is uniformly sampled from the surface of a unit sphere. The covariates \(\{x_{i}\}_{i=1}^{n}\) are first sampled from \((0,)\) and then projected to unit sphere. We consider diagonal covariances \(\) of the following form: \(=\), and \([i,i]=1\) for all \(i 1\). Here \( 1\) is the condition number of \(\). We generate noise \(z_{i}\) from uniform distribution over \([-,]\). Finally, the response variables are generated as follows \(y_{i}=x_{i}^{}w^{*}+z_{i}\). All the experiments presented below are repeated \(5\) times and the averaged results are presented. We set the DP parameters \((,)\) as \(=1,=(10^{-6},n^{-2})\). Experiments for \(=0.1\) can be found in Fig. 2 in the App. K.

**Baseline Algorithms.** We compare our estimator with the following baseline algorithms:

* _Non private algorithms:_ ordinary least squares (OLS), one-pass stochastic gradient descent with tail-averaging (SGD). For SGD, step-size is \(1/(2_{})\) and minibatch size is \(n/T\), where \(T=3 n\).

Figure 1: Performance of various techniques on DP linear regression. \(d=10\) in all the experiments. \(n=10^{7},=1\) in the \(2^{nd}\) experiment. \(n=10^{7},=1\) in the \(3^{rd}\) experiment, where \(\) is the condition number of \(\) and \(^{2}\) is the variance of the label noise \(z_{i}\).

* _Private algorithms:_ sufficient statistics perturbation (DP-SSP) [38; 83], differentially private stochastic gradient descent (DP-AMBSSGD) . DP-SSP had the best empirical performance among numerous techniques studied by , and DP-AMBSSGD has the best known theoretical guarantees. The DP-SSP algorithm involves releasing \(X^{T}X\) and \(X^{T}\) differentially privately and computing \((X})^{-1}}\). DP-AMBSSGD is a private version of SGD where the DP noise is set adaptively according to the excess error in each iteration. For both algorithms, we use the hyper-parameters recommended in their respective papers. To improve the performance of DP-AMBSSGD, we reduce the theoretical clipping threshold by a constant factor.

**DP-RobGD.** We implement Alg. 1 with the following key changes. Instead of relying on \(\) to estimate \(\), we set it to its true value \(()\). This is done for a fair comparison with DP-AMBSSGD which assumes the knowledge of \(()\). Next, we use \(20\%\) of the samples to compute \(_{t}\) in line 5 (instead of the \(50\%\) stated in Alg. 1). In our experiments we also present results for a variant of our algorithm called DP-RobGD* which outputs the best iterate based on \(_{t}\), instead of the last iterate. One could also perform tail-averaging instead of picking the best iterate. Both these modifications are primarily used to reduce the variance in the output of Alg. 1 and achieved similar performance in our experiments.

**Results.** Figure 1 presents the performance of various algorithms as we vary \(n,,\). It can be seen that DP-RobGD outperforms DP-AMBSSGD in almost all the settings (and DP-RobGD* outperforms DP-RobGD in all cases). DP-SSP has poor performance when the noise \(\) is low, but performs slightly better than DP-RobGD in other settings. A major drawback of DP-SSP is its computational complexity which scales as \(O(nd^{2}+d^{})\). In contrast, the computational complexity of DP-RobGD has smaller dependence on \(d\) and scales as \((nd)\). Thus the latter is more computationally efficient for high-dimensional problems. More experimental results on both robust and private linear regression can be found in the App. K.

## 5 Sketch of the main ideas in the analysis

We provide the main ideas behind the proof of Thm. 3.1. The privacy proof is straightforward since no matter what clipping threshold we use the noise we add is always proportionally to the clipping threshold which guarantees privacy. In the remainder, we focus on the utility analysis.

The proof of the utility heavily relies on the _resilience_ (also known as _stability_), which states that given a large enough sample set \(S\), various statistics (for example, sample mean and sample variance) of any large enough subset of \(S\) will be close to each other. We define resilience as follows.

**Definition 5.1** ([61, Definition 23]).: For some \((0,1)\), \(_{1}_{+}\), \(_{2}_{+}\), and \(_{3}_{+}\), \(_{4}_{+}\), we say dataset \(S_{}=\{(x_{i}^{d},y_{i})\}_{i=1}^{n}\) is \((,_{1},_{2},_{3},_{4})\)-resilient with respect to \((w^{*},,)\) for some \(w^{*}^{d}\), positive definite \( 0^{d d}\), and \(>0\) if for any \(T S_{}\) of size \(|T|(1-)n\), the following holds for all \(v^{d}\):

\[_{(x_{i},y_{i}) T} v,x_{i} (y_{i}-x_{i}^{}w^{*})_{1}\, v}\,,\] (5) \[_{x_{i} T} v,x_{i}^{2}- v^{} v_{2}v^{} v,\] (6) \[_{(x_{i},y_{i}) T}(y_{i}-x_{i}^{} w^{*})^{2}-^{2}_{3}^{2},\] (7) \[_{(x_{i},y_{i}) T} v,x_{i} \,_{4} v}.\] (8)

We give an overview of the proof for non-robust case as follows. First, we introduce some notations. Let \(g_{i}^{(t)}:=(x_{i}^{}w_{t}-y_{i})x_{i}\) be the raw gradient and \(_{i}^{(t)}:=_{}(x_{i})_{_{t}}(x _{i}^{}w_{t}-y_{i})\) be the clipped gradient. Note that when the data follows from our distributional assumption, with high probability, samples are not clipped by the norm: \(_{}(x_{i})=x_{i}\). We can write down one step of gradient update (see Alg. 1) as follows:

\[w_{t+1}-w^{*}=-_{i S}x_{i}x_{i}^ {})(w_{t}-w^{*})}_{(i)}+_{i S}x_{i}z _{i}}_{(ii)}+_{i S}(g_{i}^{(t)}-_{i}^ {(t)})}_{(iii)}-_{t}}_{(iv)}\.\]

In the above equation, the first term is a contraction, meaning \(w_{t}\) is moving toward \(w^{*}\). The second term captures the noise from the randomness in the samples. The third term captures the bias introduced by the clipping operation, and the fourth term captures the added noise for privacy. The second term is standard and relatively easy to control, and our main focus is on the last two terms.

The third term \((/n)_{i S}(g_{i}^{(t)}-_{i}^{(t)})\) can be controlled using the resilience property. We prove that with our estimated threshold, the clipping will only affect a small amount of datapoints, whose contribution to the gradient is small collectively.

Now we have controlled the deterministic bias. Then, we upper bound the fourth term, which is the noise for the purpose of privacy, and show the expected prediction error decrease in every gradient step. The difficulty is that, since our clipping threshold is adaptive, the decrease of the estimation error depends on the estimation error of all the previous steps. This causes that in some iterations, the estimation error actually increases. In order to get around this, we split the iterations into length \(\) chunks, and argue that the maximum estimation error in a chunk must be a constant factor smaller than the previous chunk. This implies we will reach the desired error within \(()\) steps.

## 6 Conclusion

We provide a novel variant of DP-SGD algorithm for differentially private linear regression under label corruption. We show the first near-optimal rate that achieves privacy and robustness to label corruptions simultaneously. When there is no label corruption, our result also improves upon the state-of-the-art method  in terms of the condition number \(\). Compared to , our algorithm has two innovations: \(1)\) we introduce a novel adaptive clipping, which is critical in achieving robustness against label corruptions; and \(2)\) we use full batch gradient descent and a novel convergence analysis to get the near-optimal sample complexity.