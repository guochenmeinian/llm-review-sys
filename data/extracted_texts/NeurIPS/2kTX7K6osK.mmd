# Benchmarking Estimators for Natural Experiments:

A Novel Dataset and a Doubly Robust Algorithm

R. Teal Witter

New York University

rtealwitter@nyu.edu &Christopher Musco

New York University

cmusco@nyu.edu

###### Abstract

Estimating the effect of treatments from natural experiments, where treatments are pre-assigned, is an important and well-studied problem. We introduce a novel natural experiment dataset obtained from an early childhood literacy nonprofit. Surprisingly, applying over 20 established estimators to the dataset produces inconsistent results in evaluating the nonprofit's efficacy. To address this, we create a benchmark to evaluate estimator accuracy using synthetic outcomes, whose design was guided by domain experts. The benchmark extensively explores performance as real world conditions like sample size, treatment correlation, and propensity score accuracy vary. Based on our benchmark, we observe that the class of _doubly robust_ treatment effect estimators, which are based on simple and intuitive regression adjustment, generally outperform other more complicated estimators by orders of magnitude. To better support our theoretical understanding of doubly robust estimators, we derive a closed form expression for the variance of any such estimator that uses dataset splitting to obtain an unbiased estimate. This expression motivates the design of a new doubly robust estimator that uses a novel loss function when fitting functions for regression adjustment. We release the dataset and benchmark in a Python package; the package is built in a modular way to facilitate new datasets and estimators.1

## 1 Introduction

In this work we consider the problem of _treatment effect estimation_, which is ubiquitous in the sciences, social sciences, economics, and a number of other fields . We focus on the challenging setting where we do not have access to data from a randomized control trial. Instead, treatment effect must be estimated from a _natural experiment_: a dataset of individuals and effects, where we have knowledge of which individuals received treatment, but no control over how those treatments were assigned.

The natural experiment setting presents a number of challenges. Notably, treatments in a natural experiment can be assigned in a way that is correlated with the outcomes . So a direct comparison of the outcomes for the treatment and control groups would give a flawed estimate of the treatment effect. Perhaps even more difficult, the treatments can be correlated with the treatment effect itself . Before giving a formal problem statement, we describe a real-world case study where these difficulties arise.

[MISSING_PAGE_FAIL:2]

(estimated) propensity score for individual \(i[n]\). Doubly robust estimators can be written as

\[()=_{i=1}^{n}(^{(1)}-f^{(1)}(_{i})}{p_{i}}1[z_{i}=1]-^{(0)}-f^{(0)}(_{i})}{1-p_{i}}1[z_{ i} 1]+f^{(1)}(_{i})-f^{(0)}(_{i}))\]

where different doubly robust estimators may be obtained by changing the way the functions \(f^{(0)}\) and \(f^{(1)}\) are learned, or including different estimates of the propensity scores.

Please refer to Appendix H for the other estimators we consider in our work.

### Related Work

There are several popular datasets for treatment effect estimation. The Jobs dataset is a based on a small job training natural experiment where the control outcomes are incomes from before the training in 1975 and the treatment outcomes are incomes from after the training in 1978 . The Twins dataset is based on an observational study of twins that uses which twin is born heavier as the treatment, disregarding the difference in weight . The IHDP dataset is based on real covariates from an observational study starting in 1985 but with synthetic outcomes drawn from a normal distribution with a constant treatment effect (details in Section 4.1) . The News dataset is synthetically generated to broadly mimic a preference for reading on mobile devices, but without domain expert guidance (details in Section 6.2) . The ACIC dataset is synthetically generated for a competition, making strong assumptions on the outcomes (details in Section 2) .

Because estimating treatment effect is an important task, there are many estimators that use a variety of techniques. We provide a brief description of prior work here and an expanded description in Appendix G. Some estimators use propensity scores to compare similar observations  and others use regression to adjust outcomes with predictions . Some estimators offer theoretical guarantees under certain assumptions  and others are robust to modelling errors . Recently, there has been substantial work designing sophisticated neural network architectures and loss functions to estimate treatment effects . Generally, however, the approaches are complicated and resource-intensive. In addition, as we will see on the RORCO data in Appendix D, the estimators can generate substantially different estimates on the same data.

In our experiments, we find that doubly robust estimators tend to outperform other methods. Asymptotically as the number of samples grows, doubly robust estimators are unbiased if the propensity scores are accurate _or_ the outcome predictions are accurate . We propose a new doubly robust algorithm called Double-Double which is most similar to the Off-policy estimator of Mou et al. . However, our method differs in two important ways: First, we separately learn the treatment and control outcomes. This is a more powerful variance reduction strategy that allows us to exactly analyze the variance of our estimator. Second, we learn the outcomes with a different loss function that stems from our more accurate variance analysis.

### Our Contributions

Our contributions are four-fold.

1. We release the RORCO dataset, specifically designed for treatment effect estimation in an early childhood literacy natural experiment. The dataset includes observational outcomes (RORCO Real) and synthetic outcomes (RORCO) designed in consultation with literacy experts. We document and release the generation process and source data.
2. We create a comprehensive benchmark of more than 20 treatment effect estimators. The benchmark evaluates how the estimators perform as the sample size, propensity score accuracy, and correlation vary. In the benchmark experiments, we observe that doubly robust estimators often outperform the other methods, even by orders of magnitude.
3. We theoretically analyze doubly robust estimators, exactly deriving the finite-variance of any doubly robust estimator that uses splitting to obtain an unbiased estimate. Motivated by the analysis, we introduce Double-Double, a theoretically justified doubly robust estimator.
4. We release a Python package called naturalexperiments with our novel dataset, benchmark, and algorithm. The package also loads the Jobs, Twins, IHDP, News, and ACICdatasets and is built to facilitate the easy addition of new estimators and datasets. Appendix C shows the code used to generate (almost all of) the results in the paper and appendices.

## 2 RORCO Dataset

The RORCO dataset describes student literacy and participation in the RORCO nonprofit program. Due to privacy concerns, the observations in the dataset are grades. For example, one observation corresponds to the third grade class at Academy Endeavor Elementary School in the 2018-19 school year. The covariates include variables like student counts, student-to-teacher ratios, demographic information, instructional programs, socioeconomic status indicators like free and reduced lunch eligibility, attendance rates, and staff information like average salary. A summary of the covariates appears in Appendix K and detailed documentation of the process to create the dataset is available on the Github repository.4 Using the covariates, we created two different versions of the dataset: an observational version we call RORCO Real and a semi-synthetic version we call RORCO.

### RORCO Real: An Observational Dataset

The observational RORCO Real dataset uses real literacy outcomes: The Colorado Measures of Academic Success, known as CMAS, is the state's common measurement of students' progress at the end of the school year. Because of the effect of COVID-19 on well-child visits and education , we restricted our data to literacy outcomes between 2014 and 2019. (CMAS was only fully implemented in Spring 2014.)

We determined which observations received the treatment via a RORCO dataset. The dataset included the number of well-child visits where books were given out by age for each clinic in Colorado and in each year. At the suggestion of RORCO, we made the assumption that a child _in a rural area_ who received a book from a RORCO well-child visit attended the nearest school when they reached school age. We then marked an observation as receiving RORCO treatment if more than half the students in the class received a RORCO well-child visit under this assumption. Because of the proximity assumption, we restricted the RORCO Real dataset to only rural clinics and schools.

Figure 1 shows the treatment and control outcomes by propensity score in the RORCO Real version. Because of the strong assumption in the data generation process, we expect substantial noise. Nonetheless it is clear that RORCO has a positive treatment effect for the majority of observations, especially as the likelihood of receiving the treatment increases.

Table 4 in Appendix D shows the estimate on the real outcomes and treatments for each estimator in the benchmark. The estimators return surprisingly different results. In order to evaluate which estimators are accurate, we create a semi-synthetic dataset with treatment and control outcomes.

  
**Dataset** & **Size** & **Variables** & **Treated \%** & **BCE** & \((^{(1)},)\) & \((^{(0)},)\) \\  JOBS & 722 & 8 & 41.1 & 0.0856 & 0.0355 & 0.0541 \\ TWINS & 50820 & 40 & 49.4 & 0.499 & -0.00311 & -0.0036 \\ IHDP & 747 & 26 & 18.6 & 0.452 & 0.0967 & 0.0236 \\ NEWS & 5000 & 3 & 45.8 & 0.545 & 0.86 & -0.565 \\ ACIC 2016 & 4802 & 54 & 18.4 & 0.372 & 0.112 & 0.0383 \\ ACIC 2017 & 4302 & 50 & 47.4 & 0.436 & -0.269 & -0.153 \\ RORCO Real & 4178 & 78 & 25.3 & 0.158 & -0.000602 & -0.0739 \\ RORCO & 21663 & 78 & 44.3 & 0.212 & -0.986 & -0.989 \\   

Table 1: Comparison of dataset attributes. Size is the number of observations, Variables is the number of variables, Treated is the percent of observations that receive the treatment, BCE is binary cross entropy between the propensity scores and treatment assignment, \((^{(1)},)\) is Pearson’s correlation coefficient between the treatment outcomes and propensity scores, and \((^{(0)},)\) is Pearson’s correlation coefficient between the control outcomes and propensity scores.

### RORCO: A Semi-synthetic Dataset

For the observational RORCO dataset, we developed synthetic outcomes and treatments in consultation with early childhood literacy experts. The literacy experts suggested the following assumptions:

1. The outcomes should be inversely related to the propensity score. That is, students are more likely to participate in the literacy program if they have lower literacy proficiency .
2. The treatment effect should be negligible for observations with small propensity scores and increasing for larger propensities. That is, students who are less likely to participate in the program, and so have higher literacy proficiency as per (1), will not benefit from the program because they already have sufficient resources. In contrast, students who are more likely to participate in the program, and so have lower literacy proficiency as per (1), will benefit from the program in proportion to their literacy needs .

Based on (1) and (2), we made the control outcomes as depicted in Figure 2 vary between 0 and 1 with an inverse linear relationship to propensity scores. We made the treatment outcomes align with the control outcomes until.5 and then increasingly separate (with a negated square root added to the control outcomes).

The RORCO outcomes reflect the suggestions of literacy experts while the RORCO Real outcomes are based on a best guess based proximity connection between well-child visits and standardized test scores. We intentionally make the synthetic outcomes reflect the guidance of the literacy experts rather than tailoring to the real (noisy) outcomes we observe. We believe the different outcomes are a benefit, making our benchmark more robust.

## 3 Benchmark

We evaluate more than 20 treatment effect estimators on the RORCO dataset. Since computing the true treatment effect requires both treatment and control outcomes, we use the RORCO dataset with synthetic outcomes. Since they are almost never known in practice, we estimate the propensity scores from the data. In addition, we regularize the estimated propensity scores by truncating them to the range \([.01,.99]\). As shown by Figure 3, the propensity scores are well-calibrated. In 100 runs on RORCO, we find the cross entropy between the true propensities and a sampled treatment assignment is \(.202.029\) while the cross entropy between the predicted propensities and a sampled treatment assignment vector is \(.196.029\). This suggests that the predicted propensity scores are quite accurate, at least on the synthetic data where we know the true propensity scores.

Due to space constraints, the estimators are described in detail in Appendix H. For the estimators that are agnostic to the learning process, we use a three-layer neural network with 100 hidden nodes and ReLU activations after all intermediate layers,.001 learning rate, and 200 epochs. We include experiments with other learning models (e.g., BART and causal forests) in Appendix L. In contrast, all of the "Net" estimators have custom neural network architectures and we use the CATENetbenchmark 5[CVdS21a, CVdS21b] implementation. All experiments are run on a cluster of 24-core Intel Cascade Lake Platinum 8268 chips. Table 2 displays the squared error on the semi-synthetic RORCO dataset over 100 runs. Due to space constraints, we include the analogous tables for the ACIC 2016, ACIC 2017, IHDP, Jobs, News, and Twins datasets in Appendix J. Because some of the CATENet estimators are slow to compute, we subsample to 5000 observations in our experiments unless otherwise stated.

With the exception of the Twins dataset, the standard doubly robust estimator produces the lowest empirical mean squared error followed by Double-Double. After Double-Double, several CATENet estimators--FlexTENet, TNet, TARNet, and RANet--give the best performance; however, they require substantially more training time. Next, we examine how the estimators perform as the number of observations, accuracy of the propensity scores, and correlation between treatment and outcomes varies. The goal is to evaluate the estimators in different realistic settings.

### Squared Error by Number of Observations

The number of observations is a fixed component of real experiments. In some settings, there may be fewer observations because administering the treatment or collecting data is resource intensive or infeasible. We investigate how the estimators perform as the number of observations varies.

The plots show the squared error between the true treatment effect and estimated treatment effect on a logarithmic scale. We run each experiment 100 times; the lines indicate the median and the shaded intervals indicate the region within the first and third quartiles. So that they remain legible, we restrict the plots to the six best performing estimators in Table 2. Figure 4 compares the estimators as a function of the number of observations. Since each estimator has a learning component, performance improves with the number of observations.

  
**Method** & **Mean** & **1st Quartile** & **2nd Quartile** & **3rd Quartile** & **Time (s)** \\  Regression Discontinuity & 4.65e-03 & 2.72e-03 & 3.84e-03 & 5.52e-03 & 9.55e-04 \\ Propensity Stratification & 2.57e-03 & 1.52e-03 & 2.25e-03 & 3.29e-03 & 2.78e-03 \\ Direct Difference & 4.48e-01 & 3.57e-01 & 4.18e-01 & 5.79e-01 & 4.74e-04 \\ Adjusted Direct & 6.29e-03 & 5.25e-03 & 6.20e-03 & 7.14e-03 & 1.15e-01 \\ Horvitz-Thompson & 1.06e-02 & 4.29e-03 & 9.20e-03 & 1.44e-02 & 4.65e-04 \\ TMLE & 1.19e-01 & 7.21e-03 & 2.60e-02 & 7.43e-02 & 2.35e+01 \\ Off-policy & 3.17e-03 & 1.86e-03 & 2.86e-03 & 4.11e-03 & 1.14e+01 \\ Double-Double & 1.07e-05 & 1.06e-06 & 4.41e-06 & 1.45e-05 & 2.29e+01 \\ Doubly Robust & 9.98e-07 & 1.48e-07 & 5.42e-07 & 1.37e-06 & 9.89e+00 \\ Direct Prediction & 1.36e-02 & 3.60e-03 & 1.02e-02 & 1.94e-02 & 1.23e+01 \\ SNet & 2.57e-02 & 4.85e-03 & 1.21e-02 & 3.62e-02 & 3.49e+01 \\ FlexTENet & 1.15e-03 & 4.28e-05 & 1.09e-04 & 4.95e-04 & 1.56e+02 \\ OffsetNet & 1.10e-03 & 7.72e-04 & 9.90e-04 & 1.41e-03 & 1.30e+02 \\ TNet & 8.05e-04 & 6.39e-05 & 2.50e-04 & 4.37e-04 & 1.06e+02 \\ TARNet & 1.92e-04 & 2.70e-05 & 1.04e-04 & 2.38e-04 & 1.01e+02 \\ DragonNet & 2.18e-02 & 4.42e-03 & 1.71e-02 & 2.46e-02 & 6.88e+00 \\ SNet3 & 1.80e-02 & 3.48e-03 & 9.80e-03 & 2.50e-02 & 2.36e+01 \\ DRNet & 5.00e-03 & 1.53e-04 & 6.01e-04 & 2.25e-03 & 1.14e+02 \\ RANet & 7.85e-04 & 3.67e-05 & 2.08e-04 & 7.06e-04 & 1.91e+02 \\ PWNet & 2.28e-01 & 7.02e-03 & 4.00e-02 & 2.82e-01 & 1.13e+02 \\ RNet & 2.96e-03 & 2.47e-03 & 2.84e-03 & 3.43e-03 & 5.83e+01 \\ XNet & 1.00e-03 & 3.08e-05 & 2.29e-04 & 9.26e-04 & 2.41e+02 \\   

Table 2: Squared error on the semi-synthetic RORCO dataset. The summary statistics are computed over 100 runs. The randomness in the runs comes from the synthetically generated outcomes, estimates of the propensity scores, and any internal randomness in the estimators. Note that we adopt the Olympic medal convention: gold, silver and bronze cell highlights signify first, second and third best performance, respectively.

### Squared Error by Correlation

Correlation between outcomes and propensity scores is a challenging component of natural experiments. We investigate how the correlation affects the performance of the estimators. We measure correlation using distance correlation . Unlike the Pearson correlation coefficient which is mainly sensitive to a linear relationship , the distance correlation is zero if and only if the random variables are independent. We opt for the distance correlation instead of Spearman's rank correlation because the propensity scores are concentrated close to 0 and 1, making the rank brittle to small perturbations .

In Figure 5, we add noise to the outcomes and compute the distance correlation. The plot shows the squared error against the average of the distance correlation between propensity scores \(\) and treatment outcomes \(}\) and the distance correlation between \(\) and control outcomes \(}\). For all estimators, the squared error generally decreases as the distance correlation increases. The doubly robust estimator and Double-Double outperform the other estimators until the distance correlation surpasses \(.8\).

### Squared Error by Propensity Accuracy

Since propensity scores are almost never known, estimating propensity scores is an important part of treatment effect estimation. We investigate how the accuracy of the propensity scores affects the performance of the estimators. We add noise to the propensity scores and then compute the cross entropy between the noised propensity scores and the observations that receive treatment as a measure of inaccuracy. Since the CATENet estimators do not rely on externally computed propensity scores, we consider the six best non-CATENet estimators.

Figure 6 shows the squared error against propensity score accuracy as measured by cross entropy. Because of its propensity score weighting in the loss function, Double-Double is quite sensitive to propensity accuracy. While it performs the best when the propensity scores are accurate, the doubly robust estimator still remains competitive as the propensity scores degrade.

## 4 Doubly Robust Analysis

Because of their superior performance in the benchmark, we theoretically analyze a broad-class of doubly robust estimators. The standard doubly robust estimator uses each observation to both learn and evaluate the same predictive function, introducing complicated statistical dependence. Instead, we consider doubly robust estimators with split training, ensuring that the prediction for each observation is independent of its outcomes . We show that such estimators are unbiased and we exactly derive their finite-variance. Such estimators have been analyzed in prior work but, to

Figure 4: Squared error of each estimator by the number of observations. The darker line is the median and the shaded region encompasses the first and third quartile across 100 runs. The doubly robust estimator, followed by Double-Double, achieve the lowest squared error.

Figure 3: Mean treatment rate and mean propensity score among observations with similar propensity scores. Because the predicted and actual treatment rates are close to the identity line, we conclude the propensity scores are well calibrated.

the best of our knowledge, all prior results are upper bounds as opposed to exact characterizations of the finite-variance. We then introduce Double-Double, a doubly robust estimator motivated by the exact variance expression.

Recall the standard doubly robust estimator is given by

\[()=_{i=1}^{n}(^{(1)}-f( _{i})^{(1)}}{p_{i}}_{z_{i}=1}-^{(0)}-f(_{i})^{(0)}}{1-p_{i}}_{z_{i} 1}+f(_{i})^{(1)}-f( _{i})^{(0)}) \]

where \(f^{(1)},f^{(0)}:^{d}\) are learned function of the covariates. The estimator will have complicated statistical dependencies if the learned function is applied to the observations in the training set. Instead, we consider doubly robust estimators with split training as described in Algorithm 1. The formulation of the estimator in the pseudocode is different from the standard notation, making it easier to present the variance results. In Appendix F, we show that the expressions in Equations 1 and 2 are equivalent for appropriately defined learned functions.

In the standard doubly robust estimator, the training weights in Algorithm 1 are all 1 i.e., \(w_{i}^{(1)}=w_{i}^{(0)}=1\). We will explore how to choose the weights so as to minimize the finite-variance as derived in Theorem 4.1. For the notation in the theorem statement, let \(^{(j b)}\) be the assignment vector with \(z_{j}\) set to b.

Figure 5: Squared error by distance correlation. The doubly robust estimator and Double-Double outperform the other estimators until the distance correlation surpasses.8.

Figure 6: Squared error by the cross entropy between the estimated propensity scores and the treatment assignment. Double-Double is sensitive to propensity score accuracy whereas the doubly robust estimator is, well, robust.

**Theorem 4.1**.: _When the propensity scores are known exactly, the doubly robust estimator with split training \(()\) is unbiased i.e., \(_{,S_{1},S_{2}}[()-]=0\) with variance given by_

\[[()-] =}_{i=1}^{n}_{,S_{1},S_{2}} (y_{i}^{(1)}-_{,S(i)}^{(1)}(_{i})) }{p_{i}}}+(y_{i}^{(0)}-_{,S(i)}^{(0)}( _{i}))}{1-p_{i}}}^{2}\] \[+}_{i j}_{,S_{1},S_{2}} _{i}(^{(j 1)})-_{i}(^{(j  0)})_{j}(^{(i 1)})-_{j}( ^{(i 0)}).\]

The proof of Theorem 4.1 appears in Appendix E. We now discuss how to choose the weights and train the learned functions so as to minimize the variance terms.

The first variance term captures the weighted difference between the outcomes and predictions. Unfortunately, minimizing the loss function directly is not possible because only \(y_{i}^{(1)}\) or \(y_{i}^{(0)}\) is known for any given observation \(i\). Instead, we can minimize an upper bound on the variance.

**Weighting** If we choose weights \(w_{i}^{(1)}=}{p_{i}}\) and \(w_{i}^{(0)}=}{1-p_{i}}\), then the loss functions reflect the first variance term. Intuitively, the weights prioritize correct predictions on observations that are less likely to be seen, ensuring that the learned function is accurate for all propensity scores. If \(p_{i}\) is small but \(z_{i}=1\), then \(w_{i}^{(1)}\) is quite large. However, there is an additional bias which is not yet accounted for: whether an observation appears in the training set depends on its propensity.

**Double Weighting** If we choose weights \(w_{i}^{(1)}=}{p_{i}^{2}}\) and \(w_{i}^{(0)}=}{(1-p_{i})^{2}}\), then the _expected_ loss functions reflect the first variance term. In particular, the expectation of the treatment loss in set \(S_{j}\) is

\[_{}[_{i S_{j}}_{z_{i}=1}}{p_{i}^{2}}(y_{i}^{(1)}-f_{,j}^{(1)}(_{i}))^{2} ]=_{i S_{j}}}{p_{i}}_{}[(y_{i }^{(1)}-f_{,j}^{(1)}(_{i}))^{2}].\]

Over both loss functions and both sets \(S_{1}\) and \(S_{2}\), the total expected loss upper bounds the first variance term by the AM-GM inequality: for any real numbers \(a\) and \(b\), \((a+b)^{2}=a^{2}+2ab+b^{2} 2a^{2}+2b^{2}\)

Motivated by the upper bound on the variance term, we introduce Double-Double: a doubly robust estimator with double weighting. Double-Double is equivalent to Algorithm 1 with \(w_{i}^{(1)}=}{p_{i}^{2}}\) and \(w_{i}^{(0)}=}{(1-p_{i})^{2}}\). In Table 3, we observe that Double-Double gives the best performance of the doubly robust estimators with the training split. However, perhaps because of the additional training data available, doubly robust estimators without the training split perform better.

The second variance term measures function sensitivity to removing or adding observations to the training set, a quantity closely related to differential privacy (DP). In Appendix I, we explore DP learning but find no improvement on the mean squared error. We believe the reason is that the second term is quite small in practice: On the RORCO dataset, we find that the second term is roughly \(10^{-30}\).

When the propensity scores are independent of the outcomes and covariates, the expectation of the weighted loss function is proportional to the expectation of the unweighted loss function. Suppose

  
**Method** & **Mean** & **1st Quartile** & **2nd Quartile** & **3rd Quartile** & **Time (s)** \\  Doubly Robust & 9.98e-07 & 1.48e-07 & 5.42e-07 & 1.37e-06 & 9.89e+00 \\ DR + Weighting & 4.02e-06 & 5.46e-07 & 2.62e-06 & 5.57e-06 & 9.81e+00 \\ DR + 2x Weighting & 3.80e-06 & 2.48e-07 & 9.71e-07 & 3.82e-06 & 9.80e+00 \\  DR + Split & 9.82e-05 & 3.27e-06 & 1.21e-05 & 3.65e-05 & 2.22e+01 \\ DR + Split + Weight & 1.12e-04 & 2.19e-06 & 1.03e-05 & 2.41e-05 & 2.22e+01 \\ Double-Double & 1.07e-05 & 1.06e-06 & 4.41e-06 & 1.45e-05 & 2.29e+01 \\   

Table 3: Ablation results for doubly robust estimators on the RORCO dataset. The doubly robust estimators without the training split give the best performance, likely because they effectively have access to twice the data in the training process. However, for doubly robust estimators with the training split, the theoretically justified Double-Double gives the best performance.

the data is drawn from a distribution \(\). Then, if the propensities are independent of the outcomes,

\[[_{i=1}^{n}}{p_{i}}(y_{i }^{(1)}-f_{,j}^{(1)}(_{i}))^{2}] =_{}[(y^{(1)}-f_{,j}^{(1)}())^{2}]\] \[=_{}[]_{ }[(y^{(1)}-f_{,j}^{(1)}())^{2}].\]

The analogous equalities follow for the control loss. So the weighted loss functions reduce to unweighted loss functions in the standard setting where the propensity scores are independent of the outcomes and covariates.

## 5 Limitations and Conclusion

We made several assumptions while building the RORCO and RORCO Real datasets. For the RORCO Real dataset, we made assumptions in order to determine whether a class (the most granular education data available) received the RORCO "treatment". These assumptions potentially bias the resulting datasets in the following ways: By using class (as opposed to individual students) as observations, we potentially reduce our ability to measure the effect RORCO. For example, if only half the class received the RORCO treatment then the effect on their literacy outcomes will be weaker. By using proximity in rural communities to determine whether classes received the RORCO treatment, we change the distribution of the data to only reflect sparsely populated geographic areas. Further, we make an unverified assumption that students did not move over the course of several years in these rural communities, i.e., they attend school near where they lived as a child. For the RORCO dataset, we made assumptions to synthetically generate outcomes. While conforming to expert understanding, these assumptions do not necessarily reflect what happens in the real world. As a result, fine-tuning estimators only on these synthetic outcomes may result in algorithms that are not applicable to real settings.

In Section 4, we analyzed doubly robust estimators with a testing-training split and proposed a theoretically motivated estimator called Double-Double. While perhaps slightly unsatisfying that the non-splitting methods perform better than Double-Double, this is not surprising, as they effectively have access to twice the data. A natural question for future work would be a full analysis of the non-splitting method. In the analysis, we assumed that the propensity scores are known exactly. Understanding how robust the variance analysis is to propensity score accuracy is an important direction for future work.

We introduce RORCO, a novel and reproducible dataset showcasing the unique challenges of treatment effect estimation in natural experiments. We release RORCO and an extensive benchmark of more than 20 treatment effect estimators in the naturalexperiments package. From the benchmark on RORCO and six additional datasets, we find that doubly robust estimators often perform the best in natural experiments. Our theoretical analysis sheds light on their performance and motivates the Double-Double estimator. Our work is not without limitations, the observational version of RORCO makes a strong assumption on clinic-school proximity to determine treatment since we cannot track individuals and our theoretical analysis applies only to doubly robust estimators with a training split. While algorithms can be misused, we believe our work will result in a net positive impact because of its highly specialized nature.