# Bandits with Preference Feedback:

A Stackelberg Game Perspective

 Barna Pasztor\({}^{,1,2}\)   Parnian Kassraie\({}^{,1}\)   Andreas Krause\({}^{1,2}\)

\({}^{1}\)ETH Zurich  \({}^{2}\)ETH AI Center

{bpasztor, pkassraie, krausea}@ethz.ch

Equal contribution.

###### Abstract

Bandits with preference feedback present a powerful tool for optimizing unknown target functions when only pairwise comparisons are allowed instead of direct value queries. This model allows for incorporating human feedback into online inference and optimization and has been employed in systems for fine-tuning large language models. The problem is well understood in simplified settings with linear target functions or over finite small domains that limit practical interest. Taking the next step, we consider infinite domains and nonlinear (kernelized) rewards. In this setting, selecting a pair of actions is quite challenging and requires balancing exploration and exploitation at two levels: within the pair, and along the iterations of the algorithm. We propose MaxMinLCB, which emulates this trade-off as a zero-sum Stackelberg game, and chooses action pairs that are informative and yield favorable rewards. MaxMinLCB consistently outperforms existing algorithms and satisfies an anytime-valid rate-optimal regret guarantee. This is due to our novel preference-based confidence sequences for kernelized logistic estimators.

## 1 Introduction

In standard bandit optimization, a learner repeatedly interacts with an unknown environment that gives numerical feedback on the chosen actions according to a utility function \(f\). However, in applications such as fine-tuning large language models, drug testing, or search engine optimization, the quantitative value of design choices or test outcomes are either not directly observable, or are known to be inaccurate, or systematically biased, e.g., if they are provided by human feedback . A solution is to optimize for the target based on comparative feedback provided for a pair of queries, which is proven to be more robust to certain biases and uncertainties in the queries .

Bandits with preference feedback, or _dueling_ bandits, address this problem and propose strategies for choosing query/action pairs that yield a high utility over the horizon of interactions. At the core of such algorithms is uncertainty quantification and inference for \(f\) in regions of interest, which is closely tied to exploration and exploitation dilemma over a course of queries. Observing only comparative feedback poses an additional challenge, as we now need to balance this trade-off _jointly_ over two actions. This challenge is further exacerbated when optimizing over vast or infinite action domains. As a remedy, prior work often _grounds_ one of the actions by choosing it either randomly or greedily, and tries to balance exploration-exploitation for the second action as a reaction to the first . This approach works well for simple utility functions over low-dimensional domains, however does not scale to more complex problems.

Aiming to solve this problem, we focus on continuous domains in the Euclidean vector space and complex utility functions that belong to the Reproducing Kernel Hilbert Space (RKHS) of a potentially non-smooth kernel. We propose MaxMinLCB, a sample-efficient algorithm that at every step chooses the actions _jointly_, by playing a zero-sum Stackelberg (a.k.a Leader-Follower) game. We choose the Lower Confidence Bound (LCB) of \(f\) as the objective of this game which the Leader aimsto maximize and the Follower to minimize. The equilibrium of this game yields an action pair in which the first action is a favorable candidate to maximize \(f\) and the second action is the strongest competitor against the first. Our choice of using the LCB as the objective leads to robustness against uncertainty when selecting the first action. Moreover, it makes the second action an optimistic choice as a competitor, from its own perspective. We observe empirically that this approach creates a natural exploration scheme, and in turn, yields a more sample-efficient algorithm compared to standard baselines.

Our game-theoretic strategy leads to an efficient bandit solver, if the LCB is a valid and tight lower bound on the utility function. To this end, we construct a confidence sequence for \(f\) given pairwise preference feedback, by modeling the noisy comparative observations with a logistic-type likelihood function. Our confidence sequence is anytime valid and holds uniformly over the domain, under the assumption that \(f\) resides in an RKHS. We improve prior work by removing or relaxing assumptions on the utility while maintaining the same rate of convergence. This result on preference-based confidence sequences may be of independent interest, as it targets the loss function that is typically used for Reinforcement Learning with Human Feedback.

**Contributions** Our main contributions are:

* We propose a novel game-theoretic acquisition function for pairwise action selection with preference feedback.
* We construct preference-based confidence sequences for kernelized utility functions that are tight and anytime valid.
* Together this creates MaxMinLCB, an algorithm for bandit optimization with preference feedback over continuous domains. MaxMinLCB satisfies \((_{T})\) regret, where \(T\) is the horizon and \(_{T}\) is the _information gain_ of the kernel.
* We benchmark MaxMinLCB over a set of standard optimization problems and consistently outperform the common baselines from the literature.

## 2 Related Work

Learning with indirect feedback was first studied in the supervised preference learning setting (Aiolli and Sperduti, 2004; Chu and Ghahramani, 2005). Subsequently, online and sequential settings were considered, motivated by applications in which the feedback is provided in an online manner, e.g., by a human (Yue et al., 2012; Yue and Joachims, 2009; Houlsby et al., 2011). Bengs et al. (2021) surveys this field comprehensively; here we include a brief background.

Referred to as dueling bandits, a rich body of work considers (finite) multi-armed domains and learns a preference matrix specifying the relation among the arms. Such work often relies on efficient sorting or tournament systems based on the frequency of wins for each action (Jamieson and Nowak, 2011; Zoghi et al., 2014; Komiyama et al., 2015; Wu and Liu, 2016; Falahatgar et al., 2017). Rather than jointly selecting the arms, such strategies often simplify the problem by selecting one at random (Zoghi et al., 2014; Zimmert and Seldin, 2018), greedily (Chen and Frazier, 2017), or from the set of previously selected arms (Ailon et al., 2014). In contrast, we jointly optimize both actions by choosing them as the equilibrium of a two-player zero-sum Stackelberg game, enabling a more efficient exploration/exploitation trade-off.

The multi-armed dueling setting, which is reducible to multi-armed bandits (Ailon et al., 2014), naturally fails to scale to infinite compact domains, since regularity among "similar" arms is not exploited. To go beyond finite domains, _utility-based_ dueling bandits consider an unknown latent function that captures the underlying preference, instead of relying on a preference matrix. The preference feedback is then modeled as the difference in the utility of two chosen actions passed through a link function. Early work is limited to convex domains and imposes strong regularity assumptions (Yue and Joachims, 2009; Kumagai, 2017). These assumptions are then relaxed to general compact domains if the utility function is linear (Dudik et al., 2015; Saha, 2021; Saha and Krishnamurthy, 2022). Constructing valid confidence sets from comparative feedback is a challenging task. However, it is strongly related to uncertainty quantification with direct logistic feedback, which is extensively analyzed by the literature on logistic and generalized linear bandits (Filippi et al., 2010; Faury et al., 2020; Foster and Krishnamurthy, 2018; Beygelzimer et al., 2019; Faury et al., 2022; Lee et al., 2024).

Preference-based bandit optimization with linear utility functions is well understood and is even extended to reinforcement learning with preference feedback on trajectories (Saha et al., 2023; Zhan et al., 2023; Zhu et al., 2023; Ji et al., 2023; Munos et al., 2023). However, such approaches have limited practical interest, since they cannot capture real-world problems with complex nonlinear utility functions. Alternatively, Reproducing Kernel Hilbert Spaces (RKHS) provide a rich model class for the utility, e.g., if the chosen kernel is universal. Many have proposed heuristic algorithms for bandits and Bayesian optimization in kernelized settings, albeit without providing theoretical guarantees Brochu et al. (2010); Gonzalez et al. (2017); Sui et al. (2017); Tucker et al. (2020); Mikkola et al. (2020); Takeno et al. (2023).

There have been attempts to prove convergence of kernelized algorithms for preference-based bandits (Xu et al., 2020; Kirschner and Krause, 2021; Mehta et al., 2023b, a). Such works employ a regression likelihood model which requires them to assume that both the utility and the probability of preference, as a function of actions, lie in an RKHS. In doing so, they use a regression model for solving a problem that is inherently a classification. While the model is valid, it does not result in a sample-efficient algorithm. In contrast, we use a kernelized logistic negative log-likelihood loss to infer the utility function, and provide confidence sets for its minimizer. In a concurrent work, Xu et al. (2024) also consider the kernelized logistic likelihood model and propose a variant of the MultiSBM algorithm (Ailon et al., 2014) which uses likelihood ratio confidence sets. The theoretical approach and resulting algorithm bear significant differences, and the regret guarantee has a strictly worse dependency on the time horizon \(T\), by a factor of \(T^{1/4}\). This is discussed in more detail in Section 5.

## 3 Problem Setting

Consider an agent which repeatedly interacts with an environment: at step \(t\) the agent selects two actions \(_{t},\,^{}_{t}\) and only observes stochastic binary feedback \(y_{t}\{0,1\}\) indicating if \(_{t}^{}_{t}\), that is, if action \(_{t}\) is _preferred_ over action \(^{}_{t}\). Formally, \((y_{t}=1|_{t},^{}_{t})=(_{t} ^{}_{t})\), and \(y_{t}=0\) with probability \(1-(_{t}^{}_{t})\). Based on the preference history \(H_{t}=\{(_{1},^{}_{1},y_{1}),(_{t},^{ }_{t},y_{t})\}\), the agent aims to sequentially select favorable action pairs. Over a horizon of \(T\) steps, the success of the agent is measured through the _cumulative dueling regret_

\[R^{}(T)=_{t=1}^{T}(^{}_{t })+(^{}^{}_{t})-1}{2},\] (1)

which is the average sub-optimality gap between the chosen pair and a globally preferred action \(^{}\). To better understand this notion of regret, consider the scenario where actions \(_{t}\) and \(^{}_{t}\) are both optimal. Then the probabilities are equal to \(0.5\) and the dueling regret will not grow further, since the regret incurred at step \(t\) is zero. This formulation of \(R^{}(T)\) is commonly used in the literature of dueling Bandits and RL with preference feedback (Urvoy et al., 2013; Saha et al., 2023; Zhu et al., 2023) and is adapted from Yue et al. (2012). Our goal is to design an algorithm that satisfies a _sublinear_ dueling regret, where \(R^{}(T)/T 0\) as \(T\). This implies that given enough evidence, the algorithm will converge to a globally preferred action. To this end, we take a utility-based approach and consider an unknown utility function \(f:\), which reflects the preference via

\[(_{t}^{}_{t}) s(f(_{t})- f(^{}_{t}))\] (2)

where \(s:\) is the sigmoid function1, i.e. \(s(a)=(1+e^{-a})^{-1}\). Referred to as the Bradley-Terry model (Bradley and Terry, 1952), this probabilistic model for binary feedback is widely used in the literature for logistic and generalized bandits (Filippi et al., 2010; Faury et al., 2020). Under the utility-based model, \(^{}=_{}f()\) and we can draw connections to a classic bandit problem with direct feedback over the utility \(f\). In particular, Saha (2021) shows that the dueling regret of (1) is _equivalent_ up to constant factors, to the average _utility_ regret of the two actions, that is \(_{t=1}^{T}f(^{})-[f(_{t})+f(^{}_{t})]/2\).

Throughout this paper, we make two key assumptions over the environment. We assume that the domain \(^{d_{0}}\) is compact, and that the utility function lies in \(_{k}\), a Reproducing Kernel Hilbert Space corresponding to some kernel function \(k\) with a bounded RKHS norm \(\|f\|_{k} B\). Without a loss of generality, we further suppose that the kernel function is normalized and \(k(,) 1\) everywhere in the domain. Our set of assumptions extends the prior literature on logistic bandits and dueling bandits from linear rewards or finite action spaces, to continuous domains with non-parametric rewards.

While our theoretical framework targets euclidean domains, our methodology may be used on general domains of text or images, given vector embeddings obtained via unsupervised learning. Solving a bandit problem on top of embeddings from a pretrained language model is common practice in further fine-tuning of such models (e.g., Nguyen et al., 2024; Mehta et al., 2023a), and is further demonstrated in our Yelp experiment (c.f Section 6.3). Lastly, we highlight that our results may be smoothly extended to contextual bandits with stochastic context, by simply modifying the signature of the kernel function to \(k^{}(,^{},): \), where \(\) denotes the context. This setting accommodates applications in active learning for fine-tuning of large language models, where the context is the prompt and the two actions are two alternative responses.

## 4 Kernelized Confidence Sequences with Direct Logistic Feedback

As a warm-up, we consider a hypothetical scenario where \(^{}_{t}=_{}\) for all \(t 1\) such that \(f(_{})=0\). Therefore at every step, we suggest an action \(_{t}\) and receive a noisy binary feedback \(y_{t}\), which is equal to one with probability \(s(f(_{t}))\). This example reduces our problem to logistic bandits which has been rigorously analyzed for linear rewards (Filippi et al., 2010; Faury et al., 2020). We extend prior work to the non-parametric setting by proposing a tractable loss function for estimating the utility function, a.k.a. reward. We present novel confidence intervals that quantify the uncertainty on the logistic predictions _uniformly_ over the action domain. In doing so, we propose confidence sequences for the kernelized logistic likelihood model that are of independent interest for developing sample-efficient solvers for online and active classification.

The feedback \(y_{t}\) is a Bernoulli random variable, and its likelihood depends on the utility function as \(s(f(_{t}))^{y_{t}}[1-s(f(_{t}))]^{1-y_{t}}\). Then given history \(H_{t}\), we can estimate \(f\) by \(f_{t}\), the minimizer of the regularized negative log-likelihood loss

\[_{k}^{}(f;H_{t})_{=1}^{t}-y_{} [s(f(_{}))]-(1-y_{})[1-s(f(_{})) ]+_{k}^{2}\] (3)

where \(>0\) is the regularization coefficient. The regularization term ensures that \(}_{k}\) is finite and bounded. For simplicity, we assume throughout the main text that \(}_{k} B\). However, we do not need to rely on this assumption to give theoretical guarantees. In the appendix, we present a more rigorous analysis by projecting \(f_{t}\) back into the RKHS ball of radius \(B\) to ensure that the \(B\)-boundedness condition is met, instead of assuming it. We do not perform this projection in our experiments.

Solving for \(f_{t}\) may seem intractable at first glance since the loss is defined over functions in the large space of \(_{k}\). However, it is common knowledge that the solution has a parametric form and may be calculated by using gradient descent. This is a direct application of the Representer Theorem (Scholkopf et al., 2001) and is detailed in Proposition 1.

**Proposition 1** (Logistic Representer Theorem).: _The regularized negative log-likelihood loss of (3) has a unique minimizer \(f_{t}\), which takes the form \(f_{t}()=_{=1}^{t}_{}k(,_{})\) where \((_{1},_{t})=:_{t}^{t}\) is the minimizer of the strictly convex loss_

\[_{k}^{}(;H_{t})=_{=1}^{t}-y_{} [s(^{}_{t}(_{}))]-(1-y_{}) [1-s(^{}_{t}(_{}))]+}_{2}^{2}\]

_with \(_{t}()=(k(_{1},),,k(_{t},)) ^{t}\)._

Given \(f_{t}\), we may predict the expected feedback for a point \(\) as \(s(f_{t}())\). Centered around this prediction, we construct confidence sets of the form \([s(f_{t}())_{t}()_{t}()]\), and show their uniform anytime validity. The width of the sets are characterized by \(_{t}()\) defined as

\[_{t}^{2}() k(,)-_{t}^{}()(K_{ t}+_{t})^{-1}_{t}()\] (4)

where \(=_{a B}1/(a)\), with \((a)=s(a)(1-s(a))\) denoting the derivative of the sigmoid function, and \(K_{t}^{t t}\) is the kernel matrix satisfying \([K_{t}]_{i,j}=k(_{i},_{j})\). Our first main result shows that for a careful choice of \(_{t}()\), these sets contain \(s(f())\) simultaneously for all \(\) and \(t 1\) with probability greater than \(1-\).

**Theorem 2** (Kernelized Logistic Confidence Sequences).: _Assume \(f_{k}\) and \(_{k} B\). Let \(0<<1\) and set_

\[_{t}() 4LB+2L(_{t}+ 1/ )},\] (5)

_where \(_{t}_{_{1},,_{t}}(_{t}+()^{-1}K_{T})\), and \(L_{a B}(a)\). Then_

\[( t 1,:\,|s(f_{t}() )-s(f())|_{t}()_{t}())  1-.\]Function-valued confidence sets around the kernelized ridge estimator are analyzed and used extensively to design bandit algorithms with noisy feedback on the true reward values (Valko et al., 2013; Srinivas et al., 2010; Chowdhury and Gopalan, 2017; Whitehouse et al., 2023). However, under noisy logistic feedback, this literature falls short as the proposed confidence sets are no longer valid for the kernelized logistic estimator \(f_{t}\). One could still estimate \(f\) using a kernelized ridge estimator and benefit from this line of work. However, as empirically demonstrated in Figure 0(a), this will not be a sample-efficient approach.

**Proof Sketch.** When minimizing the kernelized logistic loss, we do not have a closed-form solution for \(f_{t}\), and can only formulate it using the fact that the gradient of the loss evaluated at \(f_{t}\) is the null operator, i.e., \((f_{t};H_{t}):=\). The key idea of our proof is to construct confidence intervals as \(\)-valued ellipsoids in the _gradient space_ and show that the gradient operator evaluated at \(f\) belongs to it with high probability (c.f. Lemma 8). We then translate this back into intervals around point estimates \(s(f_{t}())\) uniformly for all points \(\). The complete proof is deferred to Appendix A, and builds on the results of Faury et al. (2020) and Whitehouse et al. (2023).

**Logistic Bandits.** Such confidence sets are an integral tool for action selection under uncertainty, and bandit algorithms often rely on them to balance exploration against exploitation. To demonstrate how Theorem 2 may be used for bandit optimization with direct logistic feedback, we consider LGP-UCB, the kernelized Logistic GP-UCB algorithm. Presented in Algorithm 2, it extends the optimistic algorithm of Faury et al. (2020) from the linear to the kernelized setting, by using the confidence bound of Theorem 2 to calculate an optimistic estimate of the reward. We proceed to show that LGP-UCB attains a sublinear logistic regret, which is commonly defined as

\[R^{}(T)=_{i=1}^{T}s(f(^{}))-s(f(_{t})).\]

To the best of our knowledge, the following corollary presents the first regret bound for logistic bandits in the kernelized setting and may be of independent interest.

**Corollary 3**.: _Let \((0,1]\) and choose the exploration coefficients \(_{t}()\) as described in Theorem 2 for all \(t 0\). Then LGP-UCB satisfies the anytime cumulative regret guarantee of_

\[( T 0:R^{}(T) C_{L}_{T}() }) 1-.\]

_where \(C_{L})}\)._

## 5 Main Results: Bandits with Preference Feedback

We return to our main problem setting in which a pair of actions, \(_{t}\) and \(^{}_{t}\), are chosen and the observed response is a noisy binary indicator of \(_{t}\) yielding a higher utility than \(^{}_{t}\). While this type of feedback is more consistent in practice, it creates quite a challenging problem compared to the logistic problem of Section 4. The search space for action pairs \(\) is significantly larger than \(\), and the observed preference feedback of \(s(f(_{t})-f(^{}_{t}))\) conveys only relative information between two actions. We start by presenting a solution to estimate \(f\) and obtain valid confidence sets under preference feedback. Using these confidence sets we then design the MaxMinLCB algorithm which chooses action pairs that are not only favorable, i.e., yield high utility, but are also informative for improving the utility confidence sets.

### Preference-based Confidence Sets

We consider the probabilistic model of \(y_{t}\) as stated in Section 3, and write the corresponding regularized negative loglikelihood loss as

\[_{k}^{}(f;H_{t}) _{=1}^{t}&-y_{}[s(f(_{})-f(^ {}_{}))]\\ &-(1-y_{})[1-s(f(_{})-f(^{ }_{}))]+_{k}^{2}.\] (6)

This loss may be optimized over different function classes and is commonly used for linear dueling bandits (e.g., Saha, 2021), and has been notably successful in reinforcement learning with human feedback (Christiano et al., 2017). We proceed to show that the preference-based loss \(_{k}^{}\) is equivalent to \(_{k^{}}^{}\), the standard logistic loss (3) invoked with a specific kernel function \(k^{}\). This will allow us to cast the problem of inference with preference feedback as a kernelized logistic regression problem. To this end, we define the _dueling kernel_ as

\[k^{}(_{1},_{1}^{}),(_{2},_{2}^{ }) k(_{1},_{2})+k(_{1}^{}, _{2}^{})-k(_{1},_{2}^{})-k(_{1}^{}, _{2})\]

for all \((_{1},_{1}^{}),(_{2},_{2}^{})\), and let \(_{k^{}}\) be the RKHS corresponding to it. While the two function spaces \(_{k^{}}\) and \(_{k}\) are defined over different input domains, we can show that they are isomorphic, under simple regularity conditions.

**Proposition 4**.: _Let \(f:\). Consider a kernel \(k\) and the sequence of its eigenfunctions \((_{i})_{i=1}^{}\). Assume the eigenfunctions are zero-mean, i.e. \(_{}_{i}()=0\). Then \(f_{k}\), if and only if there exists \(h_{k^{}}\) such that \(h(,^{})=f()-f(^{})\). Moreover, \( h_{k^{}}= f_{k^{ }}\)._

The proof is left to Appendix C.1. The assumption on eigenfunctions in Proposition 4 is primarily made to simplify the equivalence class. In particular, the relative preference function \(h\) can only capture the utility \(f\) up to a bias, i.e., if a constant bias \(b\) is added to all values of \(f\), the corresponding \(h\) function will not change. The value of \(b\) may not be recovered by drawing queries from \(h\), however, this will not cause issues in terms of identifying \(\) of \(f\) through querying values of \(h\). Therefore, we set \(b=0\) by assuming that eigenfunctions of \(k\) are zero-mean. This assumption automatically holds for all kernels that are translation or rotation invariant over symmetric domains, since their eigenfunctions are periodic \(L_{2}()\) basis functions, e.g., Matern kernels and sinusoids.

Proposition 4 allows us to re-write the preference-based loss function of (6) as a logistic-type loss

\[_{k^{}}^{}(h;H_{t})=_{=1}^{t}-y_{} [s(h(_{},_{}^{}))]-(1-y_{}) [1-s(h(_{},_{}^{}))]+ h_{k^{}}^{2},\]

that is equivalent to (3) up to the choice of kernel. We define the minimizer \(h_{t}_{k^{}}^{}(h;H_{t})\) and use it to construct anytime valid confidence sets for the utility \(f\) given only preference feedback.

**Corollary 5** (Kernelized Preference-based Confidence Sequences).: _Assume \(f_{k}\) and \( f_{k} B\). Choose \(0<<1\) and set \(_{t}^{}()\) and \(_{t}^{}\) as in equations (4) and (5), with \(k^{D}\) used as the kernel function. Then,_

\[( t 1,,^{}:|s (h_{t}(,^{}))-s(f()-f(^{}) )|_{t}^{}()_{t}^{D}(,^{ })) 1-.\]

_where \(h_{t}=_{k^{}}^{}(h;H_{t})\)._

Corollary 5 gives valid confidence sets for kernelized utility functions under preference feedback and immediately improves prior results on linear dueling bandits and kernelized dueling bandits with regression-type loss, to kernelized setting with logistic-type likelihood. To demonstrate this, in Appendix C.3 we present the kernelized extensions of MaxInP(Saha, (2021), Algorithm 3), and IDS(Kirschner and Krause, (2021), Algorithm 4) and prove the corresponding regret guarantees (cf. Theorems 15 and 16). Corollary 5 holds almost immediately by invoking Theorem 2 with the dueling kernel \(k^{}\) and applying Proposition 4. A proof is provided in Appendix C.1 for completeness.

**Comparison to Prior Work.** A line of previous work assumes that both \(f\) and the probability \(s(f())\) are \(B\)-bounded members of \(_{k}\). This allows them to directly estimate \(s(f())\) via kernelized linear regression (Xu et al., 2020; Mehta et al., 2023; Kirschner and Krause, 2021). The resulting confidence intervals are then around the least squares estimator, which does not align with the logistic estimator \(f_{t}\). This model does not encode the fact that \(s(f())\) only takes values in \(\) and considers a sub-Gaussian distribution for \(y_{t}\), instead of the Bernoulli formulation when calculating the likelihood. Therefore, the resulting algorithms require more samples to learn an accurate reward estimate. In a concurrent work, Xu et al. (2024) also consider the loss function of Equation (6) and present likelihood-ratio confidence sets. The width of the sets at time \(T\), scales with \((_{k};1/T)}\) where the second term is the _metric entropy_ of the \(B\)-bounded RKHS at resolution \(1/T\), that is, the log-covering number of this function class, using balls of radius \(1/T\). It is known that \((_{k};1/T)_{T}\) as defined in Theorem 2. This may be easily verified using Wainwright (2019, Example 5.12) and (Vakili et al., 2021, Definition 1). Noting the definition of \(_{t}^{}\), we see that likelihood ratio sets of Xu et al. (2024) are wider than Corollary 5. Consequently, the presented regret guarantee in this work is looser by a factor of \(T^{1/4}\) compared to our bound in Theorem 6.

[MISSING_PAGE_FAIL:7]

**Theorem 6**.: _Suppose the utility function \(f\) lies in \(_{k}\) with a norm bounded by \(B\), and that kernel \(k\) satisfies the assumption of Proposition 4. Let \((0,1]\) and choose the exploration coefficient \(_{t}^{}()\) as in Corollary 5. Then MaxMinLCB satisfies the anytime dueling regret of_

\[( T 0:R^{}(T) C_{3}_{T}^{ }()^{}}=(_{T}^{ })) 1-\]

_where \(_{T}^{}\) is the \(T\)-step information gain of kernel \(k^{}\) and \(C_{3}=(8+2)/)}\)._

The proof is left to Appendix C.2. The information gain \(_{T}^{}\) in Theorem 6 quantifies the structural complexity of the RKHS corresponding to \(k^{}\) and its dependence on \(T\) is fairly understood for kernels commonly used in applications of bandit optimization. As an example, for a Matern kernel of smoothness \(\) defined over a \(d\)-dimensional domain, \(_{T}=}(T^{d/(2+d)})\)(Remark 2, Vakili et al., 2021) and the corresponding regret bound grows sublinearly with \(T\).

Restricting the optimization domain to \(_{t}\) is common in the literature (Zoghi et al., 2014; Saha, 2021) despite being challenging in applications with large or continuous domains. We conjecture that MaxMinLCB would enjoy similar regret guarantees without restricting the selection domain to \(_{t}\) as done in Equation (7). This claim is supported by our experiments in Section 6.2 which are carried out without such restriction on the optimization domain.

## 6 Experiments

Our experiments are on finding the maxima of test functions commonly used in (non-convex) optimization literature (Jamil and Yang, 2013), given only preference feedback. These functions cover challenging optimization landscapes including several local optima, plateaus, and valleys, allowing us to test the versatility of MaxMinLCB. We use the Ackley function for illustration in the main text and provide the regret plots for the remainder of the functions in Appendix E. For all experiments, we set the horizon \(T=2000\) and evaluate all algorithms on a uniform mesh over the input domain of size \(100\). Additionally, we conducted experiments on the Yelp restaurant review dataset to demonstrate the applicability of MaxMinLCB on real-world data and its scaling to larger domains. All experiments are run across \(20\) random seeds and reported values are averaged over the seeds, together with standard error. The environments and algorithms are implemented2 end-to-end in JAX (Bradbury et al., 2018).

### Benchmarking Confidence Sets

Performance of MaxMinLCB relies on validity and tightness of the LCB. We evaluate the quality of our kernelized confidence bounds, using the potentially simpler task of bandit optimization given logistic feedback. To this end, we fix the acquisition function for the logistic bandit algorithms to the Upper Confidence Bound (UCB) function, and benchmark different methods for calculating the confidence bound. We refer to the algorithm instantiated with the confidence sets of Theorem 2 as LGP-UCB (c.f. Algorithm 2). The Ind-UCB approach assumes that actions are uncorrelated, and

Figure 1: Regret of learning the Ackley function with logistic and preference feedback. **(a)** Same UCB algorithms, each using a different confidence set. LGP-UCB performs best, showcasing the power of Theorem 2. **(b)**: Algorithms with different acquisition functions, all using our confidence sets. MaxMinLCB is more sample-efficient.

maintains an independent confidence interval for each action as in Lattimore and Szepesvari (2020, Algorithm 3). This demonstrates how LGP-UCB utilizes the correlation between actions. We also implement Log-UCB1 (Faury et al., 2020) that assumes that \(f\) is a linear function, i.e., \(f()=^{T}\) to highlight the improvements gained by kernelization. Last, we compare LGP-UCB with GP-UCB (Srinivas et al., 2010) that estimates probabilities \(s(f())\) via a kernelized ridge regression task. This comparison highlights the benefits of using our kernelized logistic estimator (Proposition 1) over regression-based approaches (Xu et al., 2020, Kirschner and Krause, 2021, Mehta et al., 2023, 2020), Figure 1 shows that the cumulative regret of LGP-UCB is the lowest among the baselines. GP-UCB performs closest to LGP-UCB, however, it accumulates regret linearly during the initial steps. Note that GP-UCB and LGP-UCB differ in the estimation of the utility function \(f_{t}\) while estimating the width of the confidence bounds similarly. This result suggests that using the logistic-type loss (3) to infer the utility function is advantageous. As expected, Ind-UCB converges at a slower rate than LGP-UCB and GP-UCB due to ignoring the correlation between arms while Log-UCB1's regret grows linearly as the Ackley function is misspecified under the assumption of linearity. We defer the results on the rest of the utility functions to Table 2 in Appendix E and the figures therein.

### Benchmarking Acquisition Functions

In this section, we compare MaxMinLCB with other utility-based bandit algorithms. To isolate the benefits of our acquisition function, we instantiate all algorithms with the same confidence sets, and use our improved preferred-based bound of Corollary 5. Therefore, our implementation differs from the corresponding references, while we refer to the algorithms by their original name. We consider the following baselines. Doubler and MultiSBM (Ailon et al., 2014) choose \(_{t}\) as a _reference_ action from the recent history of actions and pair it with \(^{}_{t}\) which maximizes the joint UCB (cf. Algorithm 5 and 6). RUCB (Zoghi et al., 2014) chooses \(^{}_{t}\) similarly, however, it selects the reference action uniformly at random from \(_{t}\) (Algorithm 7). MaxInP (Saha, 2021) also maintains the set of plausible maximizers \(_{t}\), and at each time step, it selects the pair of actions that maximize \(_{t}^{D}(,^{})\) (Algorithm 3). IDS (Kirschner and Krause, 2021) selects the reference action greedily by maximizing \(f_{t}\), and pairs it with an informative action (Algorithm 4). Notably, all algorithms, with the exception of MaxInP, choose one of the actions independently and use it as a reference point when selecting the other one. Figure 3 illustrates the differences in action selection between UCB, maximum information, and MaxMinLCB approaches. We note that POP-BO(Xu et al., 2024) and MultiSBM only differ in the estimation of the confidence set. Since we deploy the same confidence set for all acquisition functions, the two algorithms are equivalent and we use MultiSBM in our results, however, comparisons hold for POP-BO as well.

Figure 1(b) benchmarks the algorithms using the Ackley utility function, where MaxMinLCB outperforms the baselines. All algorithms suffer from close-to-linear regret during the initial stages of learning, suggesting that there is an inevitable exploration phase. Notably, MaxMinLCB, IDS, and Doubler are the first to select actions with high utility, while RUCB and MaxInP explore for longer. Table 1 shows the dueling regret for all utility functions. MaxMinLCB consistently outperforms the baselines across the analyzed functions and achieves a low standard error, supporting its efficiency in balancing exploration and exploitation in the preference feedback setting. Only MultiSBM shows comparable performance to MaxMinLCB and even outperforms it on the Matyas function which is a relatively smooth function posing a simple optimization problem. However, MaxMinLCB achieves lower regret on the Ackley and Eggholder functions which obtain many local optima and steeper

   \(f\) & MaxMinLCB & Doubler & MultiSBM & MaxInP & RUCB & IDS \\  Branin & \( 13\) & \(114 9\) & \( 13\) & \(340 2\) & \( 14\) & \(163 22\) \\ Matyas & \(125 5\) & \(136 4\) & \( 7\) & \(136 6\) & \( 6\) & \(128 5\) \\ Rosenbrock & \( 4\) & \(44 12\) & \( 5\) & \(109 2\) & \(58 7\) & \(58 13\) \\  Ackley & \( 2\) & \(72 2\) & \(65 0.5\) & \(120 1\) & \(84 0.7\) & \(111 9\) \\ Eggholder & \( 6\) & \(154 4\) & \(134 3\) & \(230 34\) & \(213 40\) & \(141 12\) \\ Hoelder & \( 26\) & \( 3\) & \( 15\) & \(204 20\) & \(200 28\) & \( 15\) \\ Michalewicz & \( 14\) & \(183 11\) & \( 10\) & \(260 40\) & \(269 46\) & \(188 21\) \\ Yelp & \( 22\) & \(263 28\) & \( 25\) & \(409 15\) & \(214 22\) & \(255 22\) \\   

Table 1: Benchmarking \(R_{T}^{}\) for a variety of test utility functions, \(T=2000\). The top 3 rows show results for smoother functions without steep gradients and local optima while the bottom 5 rows show the results for more challenging problems.

gradients showing that MaxMinLCB is preferable for challenging optimization problems. Other acquisition functions work well only in certain cases, e.g., IDS achieves the smallest regret for optimizing Matyas, while RUCB excels on the Branin function. This indicates the challenges each utility function offers and the performance of the action selection is task dependent. The consistent performance of MaxMinLCB demonstrates its robustness against the underlying unknown utility function.

### Real-world Experiment

To demonstrate the scalability and applicability of MaxMinLCB, we conduct an experiment on the Yelp dataset of restaurant reviews, which consists of \(275\) restaurants and \(20\) users after the pre-processing stage. For each user, we want to find the restaurants that best fit their preference, via making sequential recommendations and receiving comparative feedback. We define the action space \(\) by assigning to each restaurant their respective \(32\)-dimensional embedding of their reviews, i.e., \(^{32}\). The dataset provides utility values for users in the form of ratings on the scale of \(1\) to \(5\), however, not all users rated every restaurant.

We estimate missing ratings using collaborative filtering . Further details on the data processing are deferred to Appendix D.1. Figure 2 shows that the results of this larger problem align with previous conclusions. MaxMinLCB remains the best-performing algorithm with MultiSBM following second. The cumulative regret is also reported in Table 1. Note that neither of the algorithms is tuned or modified for this experiment. These results are only intended to demonstrate that 1) the computations easily scale, and 2) the kernelized approach is still applicable in a text-based domain, by using high-quality vector embeddings.

## 7 Conclusion

We addressed the problem of bandit optimization with preference feedback over large domains and complex targets. We proposed MaxMinLCB, which takes a game-theoretic approach to the problem of action selection under comparative feedback, and naturally balances exploration and exploitation by constructing a zero-sum Stackelberg game between the action pairs. MaxMinLCB achieves a sublinear regret for kernelized utilities, and performs competitively across a range of experiments. Lastly, by uncovering the equivalence of learning with logistic or comparative feedback, we propose kernelized preference-based confidence sets, which may be employed in adjacent problems, such as reinforcement learning with human feedback. The technical setup considered in this work serves as a foundation for a number of applications in mechanism design, such as preference elicitation and welfare optimization from multiple feedback sources for social choice theory, which we leave as future work.