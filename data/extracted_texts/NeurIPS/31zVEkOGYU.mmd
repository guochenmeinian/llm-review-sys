# Enemy is Inside: Alleviating VAE's Overestimation in Unsupervised OOD Detection

Anonymous Author(s)

Affiliation

Address

email

###### Abstract

Deep generative models (DGMs) aim at characterizing the distribution of the training set by maximizing the marginal likelihood of inputs in an unsupervised manner, making them a promising option for unsupervised out-of-distribution (OOD) detection. However, recent works have reported that DGMs often assign higher likelihoods to OOD data than in-distribution (ID) data, _i.e._, _overestimation_, leading to their failures in OOD detection. Although several pioneer works have tried to analyze this phenomenon, and some VAE-based methods have also attempted to alleviate this issue by modifying their score functions for OOD detection, the root cause of the _overestimation_ in VAE has never been revealed to our best knowledge. To fill this gap, this paper will provide a thorough theoretical analysis on the _overestimation_ issue of VAE, and reveal that this phenomenon arises from two Inside-Enemy aspects: 1) the improper design of prior distribution; 2) the gap of dataset entropies between ID and OOD datasets. Based on these findings, we propose a novel score function to Alleviate VAE's **O**verestimation **I**n unsupervised OOD **D**etection, named **"AVOID"**, which contains two novel techniques, specifically post-hoc prior and dataset entropy calibration. Experimental results verify our analysis, demonstrating that the proposed method is effective in alleviating _overestimation_ and improving unsupervised OOD detection performance.

## 1 Introduction

The detection of out-of-distribution (OOD) data, _i.e._, identifying data that differ from the in-distribution (ID) training set, is crucial for ensuring the reliability and safety of real-world applications [1; 2; 3; 4]. While the most commonly used OOD detection methods rely on supervised classifiers [5; 6; 7; 8; 9; 10; 11], which require labeled data, the focus of this paper is on designing an unsupervised OOD detector. **Unsupervised OOD detection** refers to the task of designing a detector, based solely on the unlabeled training data, that can determine whether an input is ID or OOD [12; 13; 14; 15; 16; 17; 18]. This unsupervised approach is more practical for real-world scenarios where the data lack labels.

Deep generative models (DGMs) are a highly attractive option for unsupervised OOD detection. DGMs, mainly including the auto-regressive model [19; 20], flow model [21; 22], diffusion model , generative adversarial network , and variational autoencoder (VAE) , are designed to model the distribution of the training set by explicitly or implicitly maximizing the likelihood estimation of \(p()\) for its input \(\) without category label supervision or additional OOD auxiliary data. They have achieved great successes in a wide range of applications, such as image and text generation. Since generative models are promising at modeling the distribution of the training set, they could be seen as an ideal unsupervised OOD detector, where the likelihood of the unseen OOD data output by the model should be lower than that of the in-distribution data.

Unfortunately, developing a flawless unsupervised OOD detector using DGMs is not as easy as it seems to be. Recent experiments have revealed a counterfactual phenomenon that directly applying the likelihood of generative models as an OOD detector can result in _overestimation_, _i.e._, **DGMs assign higher likelihoods to OOD data than ID data**[12; 13; 17; 18]. For instance, a generative model trained on the FashionMNIST dataset could assign higher likelihoods to data from the MNIST dataset (OOD) than data from the FashionMNIST dataset (ID), as shown in Figure 6(a). Since OOD detection can be viewed as a verification of whether a generative model has learned to model the distribution of the training set accurately, the counterfactual phenomenon of _overestimation_ not only poses challenges to unsupervised OOD detection but also raises doubts about the generative model's fundamental ability in modeling the data distribution. Therefore, it highlights the need for developing more effective methods for unsupervised OOD detection and, more importantly, a more thorough understanding of the reasons behind the _overestimation_ in deep generative models.

To develop more effective methods for unsupervised OOD detection, some approaches have modified the likelihood to new score functions based on empirical assumptions, such as low- and high-level features' consistency [17; 18] and ensemble approaches . While these methods, particularly the VAE-based methods , have achieved state-of-the-art (SOTA) performance in unsupervised OOD detection, none of them provides a clear explanation for the _overestimation_ issue. To gain insight into the _overestimation_ issue in generative models, pioneering works have shown that the _overestimation_ issue could arise from the intrinsic model curvature brought by the invertible architecture in flow models . However, in contrast to the exact marginal likelihood estimation used in flow and auto-regressive models, VAE utilizes a lower bound of the likelihood, making it difficult to analyze. Overall, the reasons behind the _overestimation_ issue of VAE are still not fully understood.

In this paper, we try to address the research gap by providing a theoretical analysis of VAE's _overestimation_ in unsupervised OOD detection. Our contributions can be summarized as follows:

1. Through theoretical analyses, we are the first to identify two factors that cause the _overestimation_ issue of VAE: 1) the improper design of prior distribution; 2) the intrinsic gap of dataset entropies between ID and OOD datasets;
2. Focused on these two discovered factors, we propose a new score function, named **"AVOID"**, to alleviate the _overestimation_ issue from two aspects: 1) post-hoc prior for the improper design of prior distribution; 2) dataset entropy calibration for the gap of dataset entropies;
3. Extensive experiments demonstrate that our method can effectively improve the performance of VAE-based methods on unsupervised OOD detection, with theoretical guarantee.

## 2 Preliminaries

### Unsupervised Out-of-distribution Detection

In this part, we will first give a problem statement of OOD detection and then we will introduce the detailed setup for applying unsupervised OOD detection.

**Problem statement.** While deploying a machine learning system, it is possible to encounter inputs from unknown distributions that are semantically and/or statistically different from the training data, and such inputs are referred to as OOD data. Processing OOD data could potentially introduce critical errors that compromise the safety of the system . Thus, the OOD detection task is to identify these OOD data, which could be seen as a binary classification task: determining whether an input \(\) is more likely ID or OOD. It could be formalized as a level-set estimation:

\[=,&()>,\\ ,&(),\] (1)

where \(()\) denotes the score function, _i.e._, **OOD detector**, and the threshold \(\) is commonly chosen to make a high fraction (_e.g._, 95%) of ID data is correctly classified . In conclusion, OOD detection aims at designing the \(()\) that could assign higher scores to ID data samples than OOD ones.

**Setup.** Denoting the input space with \(\), an _unlabeled_ training dataset \(_{}=\{_{i}\}_{i=1}^{N}\) containing of \(N\) data points can be obtained by sampling _i.i.d._ from a data distribution \(_{}\). Typically, we treat the \(_{}\) as \(p_{}\), which represents the in-distribution (ID) [17; 27]. With this _unlabeled_ training set, unsupervised OOD detection is to design a score function \(()\) that can determine whether an input is ID or OOD. This is different from supervised OOD detection, which typically leverages a classifier that is trained on labeled data [4; 7; 9]. We provide a detailed discussion in Appendix A.

### VAE-based Unsupervised OOD Detection

DGMs could be an ideal choice for unsupervised OOD detection because the estimated marginal likelihood \(p_{}()\) can be naturally used as the score function \(()\). Among DGMs, VAE can offer great flexibility and strong representation ability , leading to a series of unsupervised OOD detection methods based on VAE that have achieved SOTA performance [17; 18]. Specifically, VAE estimates the marginal likelihood by training with the variational evidence lower bound (ELBO), _i.e._,

\[()=_{q_{}(|)}[ p_{}( |)]-D_{}(q_{}(|)||p()),\] (2)

where the posterior \(q_{}(|)\) is modeled by an encoder, the reconstruction likelihood \(p_{}(|)\) is modeled by a decoder, and the prior \(p()\) is set as a Gaussian distribution \((,)\). After well training the VAE, \(()\) is an estimation of the \(p()\), which could be directly seen as the score function \(()\) to do OOD detection. But the VAE would suffer from the _overestimation_ issue, which will be introduced in the next section. More details and **Related Work** can be seen in Appendix B.

## 3 Analysis of VAE's _overestimation_ in Unsupervised OOD Detection

We will first conduct an analysis to identify the factors contributing to VAE's _overestimation_, _i.e._, the improper design of prior distribution and the gap between ID and OOD datasets' entropies. Subsequently, we will give a deeper analysis of the first factor to have a better understanding.

### Identifying Factors of VAE's _Overestimation_ Issue

Following the common analysis procedure , an ideal score function \(()\) that could achieve good OOD detection performance is expected to have the following property for any OOD dataset:

\[=_{ p_{}()}[() ]-_{ p_{}()}[()]>0,\] (3)

where \(p_{}()\) and \(p_{}()\) denote the true distribution of the ID and OOD dataset, respectively. A larger gap between these two expectation terms can usually lead to better OOD detection performance.

Using the \(()\) as the score function \(()\), we could give a formal definition of the repeatedly reported VAE's _overestimation_ issue in the context of unsupervised OOD detection [12; 17; 18; 13].

**Definition 1** (VAE's _overestimation_ in unsupervised OOD Detection).: Assume we have a VAE trained on a training set and we use the \(()\) as the score function to distinguish data points sampled _i.i.d._ from the in-distribution testing set (\(p_{}\)) and an OOD dataset (\(p_{}\)). When

\[=_{ p_{}()}[() ]-_{ p_{}()}[()] 0,\] (4)

it is called VAE's _overestimation_ in unsupervised OOD detection.

With a clear definition of _overestimation_, we could now investigate the underlying factors causing the _overestimation_ in VAE. After well training a VAE, we could reformulate the expectation term of \(()\) from the perspective of information theory  as:

\[_{ p()}[()] =_{ p()}[_{ q_{} (|)} p_{}(|)]-_{ p()}[D_{}(q_{}(|)||p())]\] \[=-_{p}()-D_{}(q()||p()),\] (5)

because we have

\[_{ p()}[_{ q_{} (|)} p_{}(|)]=_{q}(, )+_{p()} p()=_{q}(,)-_{p}(),\] (6) \[_{ p()}[D_{}(q_{}(| )||p())]=_{q}(,)+D_{}(q()||p ()),\] (7)

where the \(_{q}(,)\) is mutual information between \(\) and \(\) and the \(q()\) is the aggregated posterior distribution of the latent variables \(\), which is defined by \(q()=_{ p()}q_{}(|)\). We leave the detailed definition and derivation in Appendix C.1. Thus, the gap \(\) in Eq. (4) could be rewritten as

\[=[-_{p_{}}()+_{p_{}}( )]+[-D_{}(q_{}()||p())+D_{}(q_{ }()||p())],\] (8)

where the dataset entropy \(_{p_{}}()/_{p_{}}()\) is a constant that only depends on the true distribution of ID/OOD dataset; the prior \(p()\) is typically set as a standard (multivariate) Gaussian distribution \((,)\) to enable reparameterization for efficient gradient descent optimization .

Through analyzing the most widely used criterion, specifically the expectation of ELBO reformulated in Eq. (8), for VAE-based unsupervised OOD detection, we find that there will be two potential factors that lead to the _overestimation_ issue of VAE, _i.e._, \( 0\):

**Factor I: The improper design of prior distribution \(p()\).** Several studies have argued that the aggregated posterior distribution of latent variables \(q()\) cannot always equal \((,)\), particularly when the dataset exhibits intrinsic multimodality [28; 30; 31; 32]. In fact, when \(q()\) is extremely close to \(p()\), it is more likely to become trapped in a bad local optimum known as posterior collapse [33; 34; 35], _i.e._, \(q_{}(|) p()\), resulting in \(q()=_{}q_{}(|)p()_{}p( )p()=p()\). In this situation, the posterior \(q_{}(|)\) becomes uninformative about the inputs. Thus, the value of \(D_{}(q_{}()||p())\) could be overestimated, potentially contributing to \( 0\).

**Factor II: The gap between \(_{p}()\) and \(_{p}()\).** Considering the dataset's statistics, such as the variance of pixel values, different datasets exhibit various levels of entropy. It is reasonable that a dataset containing images with richer low-level features and more diverse content is expected to have a higher entropy. As an example, the FashionMNIST dataset should possess higher entropy compared to the MNIST dataset. Therefore, when the entropy of the ID dataset is higher than that of an OOD dataset, the value of \(-_{p}()+_{p}()\) is less than 0, potentially leading to _overestimation_.

### More Analysis on Factor I

In this part, we will focus on addressing the following question: _when is the common design of the prior distribution proper, and when is it not?_

**When the design of prior is proper?** Assuming that we have a dataset consisting of \(N\) data points \(\{_{i}\}_{i=1}^{N},\) each of which is sampled from a given \(d\)-dimensional data distribution \(p()=(|,_{})\) as shown in Figure 1(a). Then we construct a linear VAE to estimate \(p()\), formulated as:

\[p() =(|,)\] (9) \[q_{}(|) =(|+,)\] \[p_{}(|) =(|+,^{2}),\]

where \(\),\(\),\(\),\(\),\(\),\(\), and \(\) are all learnable parameters and their optimal values can be obtained by the derivation in Appendix C.3. As the estimated distribution \(p_{}()\) depicted in Figure 1(c), we can find that the linear VAE with the optimal parameter values can accurately estimate the \(p()\) through maximizing ELBO, _i.e._, the _overestimation_ issue is not present. In this case, Figures 1(b) and 1(d) indicate that the design of the prior distribution is proper, where the posterior \(q()\) equals prior \(p()\).

**When the design of prior is NOT proper?** Consider a more complex data distribution, _e.g._, a mixture of Gaussians, \(p()=_{k=1}^{K}_{k}(|_{k},_{k} ),K=2\) as shown in Figure 2(a), where \(_{k}=1/K\) and \(_{k=1}^{K}_{k}=\). We construct a dataset consisting of \(K N\) data points, obtained by sampling \(N\) data samples \(\{_{i}^{(k)}\}_{i=1,k=1}^{N,K}\) from each component Gaussian \((|_{k},_{k})\). The formulation of \(p()\), \(q_{}(|)\), and \(p_{}(|)\) is consistent with those in Eq. (9). More details are in Appendix C.2.

In what follows, we will provide a basic derivation outline for the linear VAE under the multi-modal case. We can first obtain the marginal likelihood

Figure 1: Visualization of modeling a single-modal data distribution with a linear VAE.

Figure 2: Visualization of modeling a multi-modal data distribution with a linear VAE.

\(^{2}\)) with the strictly tighter importance sampling on ELBO , _i.e._, learning the optimal generative process. Then, the joint log-likelihood of the observed dataset \(\{_{i}^{(k)}\}_{i=1,k=1}^{N,K}\) can be formulated as:

\[=_{k=1}^{K}_{i=1}^{N}_{}(_{i}^{(k)})=- (2)- det()-tr[ ^{-1}],\] (10)

where \(=^{}+^{2}\) and \(=_{k=1}^{K}_{i=1}^{N}(_{i}^{(k)}-)(_{i}^{(k)}-)^{}\). After that, we could explore the stationary points of parameters through the ELBO, which can be analytically written as:

\[()=_{q_{}(| )}[ p_{}(|)]}^{L_{1}}-_{ }[q_{}(|)||p()]}^{L_{2}},\] (11) \[L_{1}=}[-tr(^{})-(+)^{}(+)+2^{}( +)-^{}]-(2 ^{2}),\] \[L_{2}=[- det()+(+ )^{}(+)+tr()-1].\]

The detailed derivation of parameter solutions in Eq. (10) and (11) can be found in Appendix C.4.

In conclusion of this case, Figure 2(b) illustrates that \(q()\) is a multi-modal distribution instead of \(p()=(|,)\), _i.e._, the design of the prior is not proper, which leads to _overestimation_ as seen in Figure 2(c). However, as analyzed in Factor I, we found that the _overestimation_ issue is mitigated when replacing \(p()\) in the KL term of the ELBO with \(q()\), which is shown in Figure 2(d).

**More empirical studies on the improper design of prior.** To extend to a more practical and representative case, we used a 3-layer MLP to model \(q_{}(|)\) and \(p_{}(|)\) with \(p()=(,)\) on the same dataset of the above multi-modal case. Implementation details are provided in Appendix C.5. After training, we observed that \(q()\) still differs from \(p()\), as shown in Figure 3(a). The ELBO still suffers from _overestimation_, especially in the region near \((0,0)\), as shown in Figure 3(b).

Finally, we extend the analysis directly to high-dimensional image data. Since VAE trained on image data needs to be equipped with a higher dimensional latent variable space, it is hard to visualize directly. But please note that, if \(q_{}()\) is closer to \(p()=(,)\), \(_{} q_{}()\) should occupy the center of latent space \((,)\) and \(_{} q_{}()\) should be pushed far from the center, leading to \(p(_{})\) to be larger than \(p(_{})\). However, surprisingly, we found this expected phenomenon does not exist, as shown in Figure 3(c) and 3(d), where the experiments are on two dataset pairs, Fashion-MNIST(ID)/MNIST(OOD) and CIFAR10(ID)/SVHN(OOD). This still suggests that the prior \(p()\) is improper, even \(q_{}()\) for OOD data may be closer to \(p()\) than \(q_{}()\).

**Brief summary.** Through analyzing _overestimation_ scenarios from simple to complex, the answer to the question at the beginning of this part could be: _the prior distribution \(p()=(,)\) is an improper choice for VAE when modeling a complex data distribution \(p()\)_, leading to an overestimated \(D_{}(q_{}()||p())\) and further raising the _overestimation_ issue in unsupervised OOD detection.

## 4 Alleviating VAE's _overestimation_ in Unsupervised OOD Detection

In this section, we develop the **"AVOID"** method to alleviate the influence of two aforementioned factors in Section 3, including **i)** post-hoc prior and **ii)** dataset entropy calibration, both of which are implemented in a simple way to inspire related work and can be further investigated for improvement.

### Post-hoc Prior Method for Factor I

Figure 3: **(a)** and **(b)**: visualization of \(q_{}()\) and estimated \(p()\) by ELBO on the multi-modal data distribution with a non-linear deep VAE; **(c)** and **(d)**: the density plot of the log-probability of posterior \(\), _i.e._, \( q_{}(|)\), in prior \((,)\) on two dataset pairs.

To provide a more insightful view to investigate the relationship between \(q_{}()\), \(q_{}()\), and \(p()\), we use t-SNE  to visualize them in Figure 4. The visualization reveals that \(p()\) cannot distinguish between the latent variables sampled from \(q_{}()\) and \(q_{}()\), while \(q_{}()\) is clearly distinguishable from \(q_{}()\). Therefore, to alleviate _overestimation_, we can explicitly modify the prior distribution \(p()\) in Eq. (8) to force it to be closer to \(q_{}()\) and far from \(q_{}()\), _i.e._, decreasing \(D_{}(q_{}()||p())\) and increasing \(D_{}(q_{}()||p())\).

A straightforward modifying approach is to replace \(p()\) in ELBO with an additional distribution \(_{}()\) that can fit \(q_{}()\) well after training, where the target value of \(q_{}()\) can be acquired by marginalizing \(q_{}(|)\) over the training set, _i.e._, \(q_{}()=_{ p_{}()}[q_{}( |)]\). Previous study on distribution matching  has developed an LSTM-based method to efficiently fit \(q_{}()\) in the latent space, _i.e._,

\[_{}()=_{t=1}^{T}q(_{t}|_{<t}),q(_{t}|_{<t})=(_{i},_{i}^{2}).\] (12)

Thus, we could propose a "post-hoc prior" (PHP) method for Factor I, formulated as

\[():=_{ q_{}(|)} p_{ }(|)-D_{}(q_{}(|)||_{}()),\] (13)

which could lead to better OOD detection performance since it could enlarge the gap \(\), _i.e._,

\[_{}=[-_{p_{}}()+_{ p_{}}()]+[-D_{}(q_{}()||_{ }()]+D_{}(q_{}()||_{ }())]>.\] (14)

Please note that PHP can be directly integrated into a trained VAE in a "plug-and-play" manner.

### Dataset Entropy Calibration Method for Factor II

While the entropy of a dataset is a constant that remains unaffected by different model settings, it is still an essential factor that leads to _overestimation_. To address this, a straightforward approach is to design a calibration method that ensures the value added to the ELBO of ID data will be larger than that of OOD data. Specifically, we denote the calibration term as \(()\), and its expected property could be formulated as

\[_{ p_{}()}[()]> _{ p_{}()}[()].\] (15)

After adding the calibration \(()\) to the ELBO\(()\), we could obtain the "dataset entropy calibration" (DEC) method for Factor II, formulated as

\[():=_{ q_{}(|)} p_{ }(|)-D_{}(q_{}(|)||p())+ ().\] (16)

With the property in Eq. (15), we could find that the new gap \(_{}\) becomes larger than the original gap \(\) based solely on ELBO, as \(_{}=+_{ p_{}( {x})}[()]-_{ p_{}()}[ ()]>\), which should alleviate the _overestimation_ and lead to better unsupervised OOD detection performance.

**How to design the calibration \(()\)?** For the choice of the function \(()\), inspired by the previous work , we could use image compression methods like Singular Value Decomposition (SVD)  to roughly measure the complexity of an image, where the images from the same dataset should have similar complexity. An intuitive insight into this could be shown in Figure 5, where the ID dataset's statistical feature, _i.e._, the curve, is distinguishable to other datasets. Based on this empirical study, we could first propose a **non-scaled** calibration function, denoted as \(_{}()\). First, we could set the number of singular values as \(n_{}\), which can achieve the reconstruction error \(|_{}-|=\) in the ID training set; then for a test input \(_{i}\), we use SVD to calculate the smallest \(n_{i}\) that could also achieve a smaller reconstruction error \(\), then \(_{}()\) could be formulated as:

\[_{}()=(n_{i}/n_{}),&  n_{i}<n_{},\\ [((n_{}-(n_{i}-n_{}))/n_{}],& n_{i}  n_{},\] (17)

Figure 4: The t-SNE visualization of the latent representations on FashionMNIST(ID)/MNIST(OOD) dataset pair.

Figure 5: Visualization of the relationship between the number of singular values and the reconstruction error.

which can give the ID dataset a higher expectation \(_{ p_{}()}[_{}()]\) than that of other statistically different OOD datasets. More details to obtain \(_{}()\) can be found in Appendix D.

### Putting Them Together to Get "AVOID"

By combining the post-hoc prior (PHP) method and the dataset entropy calibration (DEC) method, we could develop a new score function, denoted as \(_{}()\):

\[_{}():=_{q_{}(|)}[  p_{}(|)]-D_{}(q_{}(|)|| _{}())+().\] (18)

To balance the importance of PHP and DEC terms in Eq. (18), we consider to set an appropriate scale for \(()\). For the scale of \(()\), if it is too small, its effectiveness in alleviating _overestimation_ could be limited. Otherwise, it may hurt the effectiveness of the PHP method since DEC will dominate the value of "AVOID". Additionally, for statistically similar datasets, _i.e._, \(_{p_{}}()_{p_{}}()\), the property in Eq. (15) cannot be guaranteed and we may only have \(_{ p_{}()}[_{}()] _{ p_{}()}[_{ }()]\), in which case we could only rely on the PHP method. Thus, an appropriate scale of \(_{ p_{}()}[()]\), named "\(_{}\)", could be derived by \(_{}=_{ p_{}()}[ ()]_{p_{}}()\), which leads to

\[_{ p_{}()}[()]=- _{p_{}}()-D_{}(q_{}()||p())+ _{}-D_{}(q_{}()||p( {z})).\] (19)

Thus, when \(_{p_{}}()_{p_{}}()\) and \(_{ p_{}()}[()] _{ p_{}()}[()]\), the PHP part of "AVOID" could still be helpful to alleviate _overestimation_.

Motivated by the above analysis, we could implement the **scaled** calibration function, formulated as

\[()=_{}()_{ {scale}}=(n_{i}/n_{})_{},&  n_{i}<n_{},\\ [((n_{}-(n_{i}-n_{}))/n_{})_{ },& n_{i} n_{}.\] (20)

## 5 Experiments

### Experimental Setup

**Datasets.** In accordance with existing literature [17; 18; 39], we evaluate our method against previous works using two standard dataset pairs: FashionMNIST  (ID) / MNIST  (OOD) and CIFAR10  (ID) / SVHN  (OOD). The suffixes "ID" and "OOD" represent in-distribution and out-of-distribution datasets, respectively. To more comprehensively assess the generalization capabilities of these methods, we incorporate additional OOD datasets, the details of which are available in Appendix E.1. Notably, datasets featuring the suffix "-G" (e.g., "CIFAR10-G") have been converted to grayscale, resulting in a single-channel format.

**Evaluation and Metrics.** We adhere to the previous evaluation procedure [17; 18], where all methods are trained using the training split of the in-distribution dataset, and their OOD detection performance is assessed on both the testing split of the in-distribution dataset and the OOD dataset. In line with previous works [1; 5; 44], we employ evaluation metrics including the area under the receiver operating characteristic curve (AUROC \(\)), the area under the precision-recall curve (AUPRC \(\)), and the false positive rate at 80% true positive rate (FPR80 \(\)). The arrows indicate the direction of improvement for each metric.

**Baselines.** Our experiments primarily encompass two comparison aspects: **i)** evaluating our novel score function "AVOID" against previous unsupervised OOD detection methods to determine whether it can achieve competitive performance; and **ii)** comparing "AVOID" with VAE's ELBO to assess whether our method can mitigate _overestimation_ and yield improved performance. For comparisons in **i,** we can categorize the baselines into three groups, as outlined in : "**Supervised**" includes supervised OOD detection methods that utilize in-distribution data labels [1; 5; 9; 45; 46; 47; 48; 49]; "**Auxiliary**" refers to methods that employ auxiliary knowledge gathered from OOD data [13; 39; 44]; and "**Unsupervised**" encompasses methods without reliance on labels or OOD-specific assumptions [14; 17; 18; 26]. For comparisons in **ii**, we compare our method with a standard VAE , which also serves as the foundation of our method. Further details regarding these baselines and their respective categories can be found in Appendix E.2.

**Implementation Details.** The VAE's latent variable \(\)'s dimension is set as 200 for all experiments with the encoder and decoder parameterized by a 3-layer convolutional neural network, respectively.

The reconstruction likelihood distribution is modeled by a discretized mixture of logistics . For optimization, we adopt the same Adam optimizer  with a learning rate of 1e-3. We train all models in comparison by setting the batch size as 128 and the max epoch as 1000. All experiments are performed on a PC with an NVIDIA A100 GPU and our code is implemented with PyTorch . More implementation details can be found in Appendix E.3.

### Comparison with Unsupervised OOD Detection Baselines

First, we compare our method with other SOTA baselines in Table 1. The results demonstrate that our method achieves competitive performance compared to "Supervised" and "Auxiliary" methods and outperforms "Unsupervised" OOD detection methods. Next, we provide a more detailed comparison with some unsupervised methods, particularly the ELBO of VAE, as shown in Table 2. These results indicate that our method effectively mitigates _overestimation_ and enhances OOD detection performance when using VAE as the backbone. Lastly, to assess our method's generalization capabilities, we test it on a broader range of datasets, as displayed in Table 3. Experimental results strongly verify our analysis of the VAE's _overestimation_ issue and demonstrate that our method consistently mitigates _overestimation_, regardless of the type of OOD datasets.

### Ablation Study on Verifying the Post-hoc Prior Method

To evaluate the effectiveness of the Post-hoc Prior (PHP), we compare it with other unsupervised methods in Table 2. Moreover, we test the PHP method on additional datasets and present the results in Table 4 of Appendix F. The experimental results demonstrate that the PHP method can alleviate the _overestimation_. To provide a better understanding, we also visualize the density plot of ELBO and PHP for the "FashionMNIST(ID)/MNIST(OOD)" dataset pair in Figures 6(a) and 6(b), respectively.

The Log-likelihood Ratio (\(\)) methods [17; 18] are the current SOTA unsupervised OOD detection methods that also focus on latent variables. These methods are based on an empirical assumption that the bottom layer latent variables of a hierarchical VAE could learn low-level features and top layers learn semantic features. However, we discovered that while ELBO could already perform well in detecting some OOD data, the \(\) method  could negatively impact OOD detection performance to some extent, as demonstrated in Figure 6(c), where the model is trained on MNIST and detects FashionMNIST as OOD. On the other hand, our method can still maintain comparable performance since the PHP method can explicitly alleviate _overestimation_, which is one of the strengths of our method compared to the SOTA methods.

### Ablation Study on Verifying the Dataset Entropy Calibration Method

We evaluate the performance of dataset entropy calibration, referred to as "DEC", in Table 2 and Table 5 of Appendix G. Although the DEC method is simple, our results show that it effectively alleviates _overestimation_. To better understand DEC, we visualize the calculated \(()\) of CIFAR10

   &  \\   &  &  &  &  &  \\  Method & ALROC & Model & ALROC & Model & ALROC & Model & ALROC & Model & ALROC & Model & ALROC \\  CP1  & 73.4 & LR(P)(3) & 99.4 & _E_-_S_uros & & MD(4) & 99.7 & LR(P)(5) & 99.3 & _E_-_S_uros & \\ CFER(1)  & 74.6 & LR(P)(3) & 45.5 & Wako(5)  & 76.6 & MD(4) & 27.7 & LR(P)(3) & 29.5 & Wako(5)  & 99.0 \\ OPN  & 75.2 & Y(O) (10) & 79.7 & Wako(5)  & 21.1 & 28 (8) & 98.9 & 06 (24) & 96.4 & Wako(5)  & 62.8 \\ VB  & 94.1 & CR(P)(3) & 99.4 & _W_-_S_uros & & D(E) & 95.7 & GC(8) & 113 (9) & 95.0 & _W_-_S_uros & \\ MD(3)  & 92.4 & CR(P)(3) & 99.8 & LR(P) & 98.2 & LR(P) & 98.4 & LR(P)(1) & 92.9 & _R_-_S_uros & 87.5 \\ MD(3)  & 96.8 & IC(P)(4)  & 96.7 & HV(L)  & 98.4 & OD(1) & 84.2 & BG(1) & 82.9 & LG(1) & 87.5 \\ DE  & 85.7 & & _L_-_S_uros & 98.8 & ON(4) & 76.7 & & & & \(^{*+18}\) & 94.2 \\   & **99.2** &  &  &  &  \\  

Table 1: The comparisons of our method and other OOD detection methods. The best results achieved by the methods of the category “Not ensembles” of “Unsupervised” have been bold.

   &  \\  Method & ALROC & ALROC & ALROC & ALROC & ALROC & ALROC & ALROC & **PRP(1)** \\  ELBO  & 23.5 & 35.6 & 98.5 & ELBO  & 24.9 & 36.7 & 94.6 \\ WA(5)(PC)  & 22.1 & 40.1 & 91.1 & Wako(5)  & 62.8 & 61.6 & 65.7 \\ HVK  & 98.4 & 98.4 & 1.3 & HVK  & 89.1 & 87.5 & 17.2 \\ _ECE_(1)  & 97.0 & 97.6 & 0.9 & _CC_(8)  & 92.6 & 91.8 & 11.1 \\ _**-_Anss_** & & _-_**Ours**:** & _-_**Ours**:** & & & & \\ PHP & 89.7 & 90.3 & 13.3 & PHP & 39.6 & 42.6 & 85.7 \\ DPC & 34.1 & 40.7 & 92.5 & DPC & 87.8 & 89.9 & 17.8 \\ PHP+DEC & **99.2** & **99.4** & **0.00** & PHP+DEC & **94.5** & **95.3** & **42.4** \\  

Table 2: The comparisons of our method with post-hoc prior (denoted as “PHP”) or dataset entropy calibration (denoted as “DEC”) individually and other unsupervised OOD detection methods. “PHP+DEC” is equal to our method “AVOID”. Bold numbers are superior results.

(ID) in Figure 7(a) and other OOD datasets in Figure 7(b) when \(n_{}=20\). Our results show that the \(()\) of CIFAR10 (ID) achieves generally higher values than that of other datasets, which is the underlying reason for its effectiveness in alleviating _overestimation_. Additionally, we investigate the impact of different \(n_{}\) on OOD detection performance in Figure 7(c), where our results show that the performance is consistently better than ELBO.

## 6 Conclusion

In conclusion, we have identified the underlying factors that lead to VAE's _overestimation_ in unsupervised OOD detection: the improper design of the prior and the gap of the dataset entropies between the ID and OOD datasets. With this analysis, we have developed a novel score function called "AVOID", which is effective in alleviating _overestimation_ and improving unsupervised OOD detection. This work may lead a research stream for improving unsupervised OOD detection by developing more efficient and sophisticated methods aimed at optimizing these revealed factors.

  ID &  & ID &  \\  OOD & AUROC \(\) & AUPRC \(\) & FPR80 \(\) & OOD & AUROC \(\) & AUPRC \(\) & PFR80 \(\) \\   &  \\  KMNIST & 60.03 / **78.71** & 54.60 / **68.91** & 61.6 / **4.84** & CIFAR100 & 52.91 / **55.36** & 51.15 / **72.13** & 77.42 / **73.93** \\ Ouniglot & 99.86 / **100.0** & 99.89 / **100.0** & 0.00 / **0.00** & CelebA & 57.27 / **71.23** & 54.51 / **72.13** & 69.03 / **54.45** \\ notMNIST & 94.12 / **97.72** & 94.09 / **97.70** & 8.29 / **2.20** & Places65 & 57.24 / **68.37** & 56.96 / **69.05** & 73.13 / **62.64** \\ CIFAR10-G & 98.01 / **99.01** & 98.24 / **99.04** & 1.20 / **0.40** & LFWPeople & 64.15 / **67.72** & 59.71 / **68.81** & 59.44 / **54.45** \\ CIFAR100-G & 98.49 / **98.59** & 79.49 / **97.87** & 1.00 / **1.00** & SUN & 53.14 / **63.09** & 54.48 / **63.32** & 79.52 / **68.63** \\ SVHN-G & 95.61 / **96.20** & 96.20 / **97.41** & 3.00 / **0.40** & STL10 & 43.97 / **64.51** & 47.99 / **65.50** & 78.02 / **67.23** \\ CelebA-G & 97.33 / **97.87** & 94.17 / **95.82** & 3.00 / **0.40** & Flowest102 & 67.88 / **76.38** & 64.68 / **78.01** & 57.94 / **46.65** \\ SUN-G & 99.16 / **99.32** & 99.39 / **99.47** & 0.00 / **0.00** & GTRSB & 39.50 / **53.06** & 41.73 / **49.84** & 86.61 / **73.63** \\ Places365-G & 98.92 / **98.89** & 98.05 / **98.61** & 0.80 / **0.80** & DTD & 37.86 / **81.82** & 40.93 / **62.42** & 82.22 / **64.24** \\ Const & 94.94 / **95.20** & 97.27 / **97.32** & 1.80 / **1.70** & Const & 0.001 / **80.12** & 30.71 / **89.42** & 100.0 / **22.38** \\ Random & 99.80 / **100.0** & 99.90 / **100.0** & 0.00 / **0.00** & Random & 71.81 / **99.31** & 82.89 / **99.59** & 85.71 / **0.000** \\  

Table 3: The comparisons of our method “AVOID” and baseline “ELBO” on more datasets. Bold numbers are superior performance.

Figure 6: Density plots and ROC curves. **(a):** directly using ELBO(\(\)), an estimation of the \(p()\), of a VAE trained on FashionMNIST leads to _overestimation_ in detecting MNIST as OOD data; **(b):** using PHP method could alleviate the _overestimation_; **(c):** SOTA method \(\) hurts the performance when ELBO could already work well; **(d):** PHP method would not hurt the performance.