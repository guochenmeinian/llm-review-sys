# Make Continual Learning Stronger via C-Flat

Ang Bian\({}^{*}\)1, Wei Li\({}^{*}\)1,2, Hangjie Yuan \({}^{3,4}\), Chengrong Yu\({}^{1}\), Mang Wang\({}^{5}\)

Zixiang Zhao\({}^{6}\), Aojun Lu\({}^{1}\), Pengliang Ji\({}^{7}\), Tao Feng\({}^{1}\)2

\({}^{1}\)Sichuan University \({}^{2}\)Tsinghua University \({}^{3}\)DAMO Academy, Alibaba Group

\({}^{4}\)Zhejiang University \({}^{5}\)ByteDance \({}^{6}\)Xi'an Jiaotong University \({}^{7}\)Carnegie Mellon University

hj.yuan@zju.edu.cn, {ymjii198, fengtao.hi}@gmail.com

Equal ContributionCorresponding Authors

###### Abstract

How to balance the learning'sensitivity-stability' upon new task training and memory preserving is critical in CL to resolve catastrophic forgetting. Improving model generalization ability within each learning phase is one solution to help CL learning overcome the gap in the joint knowledge space. Zeroth-order loss landscape sharpness-aware minimization is a strong training regime improving model generalization in transfer learning compared with optimizer like SGD. It has also been introduced into CL to improve memory representation or learning efficiency. However, zeroth-order sharpness alone could favors sharper over flatter minima in certain scenarios, leading to a rather sensitive minima rather than a global optima. To further enhance learning stability, we propose a **C**ontinual **Flat**ness (**C-Flat**) method featuring a flatter loss landscape tailored for CL. C-Flat could be easily called with only one line of code and is plug-and-play to any CL methods. A general framework of C-Flat applied to all CL categories and a thorough comparison with loss minima optimizer and flat minima based CL approaches is presented in this paper, showing that our method can boost CL performance in almost all cases. Code is available at https://github.com/WanNaa/C-Flat.

**C-Flat**: just a line of code suffices for its utilization.

## 1 Introduction

**Why study Continual Learning (CL)?** CL is generally acknowledged as a necessary attribute for Artificial General Intelligence (AGI) . In the open world, CL holds the potential for substantial benefits across many applications: _e.g._ vision model needs to learn a growing image set , or, embodied model needs to incrementally add skills to their repertoire .

**Challenges.** A good CL model is expected to keep the memory of all seen tasks upon learning new knowledge . However, due to the limited access to previous data, the learning phase is naturally sensitive to the current task, hence resulting in a major challenge in CL called catastrophic forgetting , which refers to the drastic performance drop on past knowledge after learning new knowledge. This learning sensitivity-stability dilemma is critical in CL, requiring model with strong generalization ability  to overcome the knowledge gaps between sequentially arriving tasks.

**Current solutions.** A series of works [43; 44; 33; 25] are proposed to improve learning stability by extending data space with dedicated selected and stored exemplars from old tasks, or frozen some network blocks or layers that are strongly related to previous knowledge [68; 24; 69; 57; 24].

Another group of works seeks to preserve model generalization with regularisation onto the training procedure itself [32; 18; 31]. Diverse weight [45; 28; 2] or gradient alignment [22; 9; 35; 26] strategies are designed to encourage the training to efficiently extracting features for the current data space without forgetting.

**Loss landscape sharpness** optimization [23; 19; 65; 70] as an efficient training regime for model generalization starts to gain attentions [27; 63]. Ordinary loss minima based optimizer like SGD can easily lead to suboptimal results [4; 37; 13]. To prevent this, zeroth-order sharpness-aware minimization seeking neighborhood-flat minima  has been proven a strong optimizer to improve model generalization ability, especially in transferring learning tasks. It is also introduced into some CL works [49; 30] with dedicated designs to improve old knowledge representation or few-shot learning efficiency. However, given the limited application scenarios[10; 49], the zeroth-order sharpness used in the current work is proved to favor sharper minima than a flat solution . It means zeroth-order only can still lead to a fast gradient descent to the suboptimal in new data space than a more generalizable result for the joint old and new knowledge space.

**Our solution.** Inspired by these works, a beyond zeroth-order sharpness continual optimization method is proposed as demonstrated in 1, where loss landscape flatness is emphasized to strengthen model generalization ability. Thus, the model can always converge to a flat minima in each phase, and then smoothly migrate to the global optimal of the joint knowledge space of the current and next tasks, and hence resolve the catastrophic forgetting in CL. We dub this method **C**ontinual **Flat**ness (C-Flat or \(C\)) Moreover, C-Flat is a general method that can be easily plug-and-play into any CL approach with only one line of code, to improve CL.

**Contribution.** A simple and flexible CL-friendly optimization method C-Flat is proposed, which Makes Continual Learning Stronger.

A framework of C-Flat covering diverse CL method categories is demonstrated. Experiment results prove that Flatter is Better in nearly all cases.

To the best of our knowledge, this work is the first to conduct a thorough comparison of CL approaches with loss landscape aware optimization, and thus can serve as a baseline in CL.

## 2 Related work

**Continual learning** methods roughly are categorized into three groups: _Memory-based_ methods write experience in memory to alleviate forgetting. Some work [43; 44; 25; 51] design different sampling strategies to establish limited budgets in a memory buffer for rehearsal. However, these methods require access to raw past data, which is discouraged in practice due to privacy concerns. Instead, recently a series of works [10; 34; 46; 33; 50] elaborately construct special subspace of old tasks as the memory. _Regularization-based_ methods aim to realize consolidation of the previous knowledge by introducing additional regularization terms in the loss function. Some works [32; 29; 6] enforce the important weights in the parameter space [45; 28; 2], feature representations [5; 21], or the logits outputs [32; 42] of the current model function to be close to that of the old one. _Expansion-based_ methods dedicate different incremental model structures towards each task to minimize forgetting [68; 39]. Some work [48; 24; 60] exploit modular network architectures (dynamically extending extra components [57; 69], or freeze partial parameters [36; 1]) to overcome forgetting. Trivially, methods in this category implicitly shift the burden of storing numerous raw data into the retention of model .

**Gradient-based solutions** are a main group in CL, including shaping loss landscape, tempering the tag-of-war of gradient, and other learning dynamics [22; 9; 41]. One promising solution is to modify the gradients of different tasks and hence overcome forgetting [7; 38], e.g., aligning the gradients of current and old one [15; 18], or, learning more efficient in the case of conflicting objectives [47; 56; 14]. Other solutions [10; 49] focus on characterizing the generalization from the loss landscape perspectives to improve CL performance and yet are rarely explored.

**Sharpness minimization in CL** Many recent works [23; 19; 4] are proposed to optimize neural networks in standard training scenarios towards flat minima. Wide local minima were considered an important regularization in CL to enforce the similarity of important parameters learned from past tasks . Sharpness-aware seeking for loss landscape flat minima starts to gain more attention in CL, especially SAM based zeroth order sharpness is well discussed. An investigation  proves SAM can help with addressing forgetting in CL, and  proposed a combined SAM for few-shot CL. SAM is also used for boosting the performance of specific methods like DFGP  and FS-DGPM  designed for GPM. SAM-CL  series with loss term gradient alignment for memory-based CL. These efforts kicked off the study of flat minima in CL, however, zeroth-order sharpness may not be enough for flatter optimal . Thus, flatness with a global optima and universal CL framework is further studied.

## 3 Method

Our solution addresses the learning sensitivity-stability dilemma in CL by improving model generalization for joint learning knowledge obtained from different catalogues domains or tasks. Moreover, a general but stronger optimization method enhanced by the latest gradient landscape flatness is proposed as a 'plug-and-play' tool for any CL approach.

**Loss landscape flatness.** Let \(B(,)=\{^{}:\|^{}-\|<\}\) denotes the neighborhood of \(\) with radius \(>0\) in the Euclidean space \(^{d}\), the zeroth-order sharpness at point \(\) is commonly defined by the maximal training loss difference within its neighborhood \(B(,)\):

\[R^{0}_{}()=\{_{S}(^{})-_{S}():^{} B(,)\}.\] (1)

where \(_{S}()\) denotes the loss of an arbitrary model with parameter \(\) on any dataset \(S\) with an oracle loss function \(()\). The zeroth-order sharpness \(R^{0}_{}()\) regularization can be directly applied to restrain the maximal neighborhood training loss:

\[_{S}^{R^{0}_{}}()=_{S}()+R^{0}_{ }()=\{_{S}(^{}):^{} B( ,)\},\] (2)

However, for some fixed \(\), local minima with a lower loss does not always have a lower major hessian eigenvalue , which equals to the neighborhood curvature. It means that zeroth-order sharpness optimizer may goes to a sharper suboptimal than to the direction of a flatter global optimal with better generalization ability.

Recently, first-order gradient landscape flatness is proposed as a measurement of the maximal neighborhood gradient norm, which reflects landscape curvature, to better describe the smoothness of the loss landscape:

\[R^{1}_{}()=\{\|_{S}(^{ })\|_{2}:^{} B(,)\}.\] (3)

Unlike zeroth-order sharpness that force the training converging to a local minimal, first-order flatness alone constraining on the neighborhood smoothness can not lead to an optimal with minimal loss. To maximize the generalization ability of loss landscape sharpness for continual learning task, we

Figure 1: Illustration of C-Flat overcoming catastrophe forgetting by fine-tuning the old model parameter to flat minima of new task. a) loss minima for current task only can cause catastrophe forgetting on previous ones. b) balanced optima aligned by regularization leads to unsatisfying results for both old and new tasks. c) C-Flat seeks global optima for all tasks with flattened loss landscape.

propose a zeroth-first-order sharpness aware optimizer C-Flat for CL. Considering the data space, model or blocks to be trained are altered regarding the training phase and CL method, (as detailed in the next subsection), we define the the C-Flat loss as follows:

\[_{S^{T}}^{C}(f^{T}(^{T})) =_{S^{T}}(f^{T}(^{T}))+R^{0}_{,S^{T}}(f^{T}(^{ T}))+ R^{1}_{,S^{T}}(f^{T}(^{T}))\] \[=_{S^{T}}^{R^{0}_{}}(f^{T}(^{T}))+ R^{1} _{,S^{T}}(f^{T}(^{T})),\] (4)

with the minimization objective:

\[_{^{T}}\{\{_{S^{T}}(f^{T}(_{0}^{T}))+ \|_{S^{T}}(f^{T}(_{1}^{T}))\|_{2}\}:_{0}^{T},_{1}^ {T} B(^{T},)\}\] (5)

where \(_{S}^{R^{0}_{}}()\) is constructed to replace the original CL loss, while \(R^{1}_{}()\) further regularizes the smoothness of the neighborhood, and hyperparameter \(\) is to balance the influence of \(R^{1}_{}\) as an additional regularization to loss function \(\). Hence, the local minima within a flat and smooth neighborhood is calculated for a generalized model possessing both old and new knowledge.

**Optimization.** In our work, the two regularization terms in the proposed C-Flat are resolved correspondingly in each iteration. Assuming the loss function \(()\) is differentiable and bounded, the gradient of \(_{S}^{R^{0}_{}}\) at point \(^{T}\) can be approximated by

\[_{S}^{R^{0}_{}}(^{T})_{S}(_{0}^{T} )\;\;\;_{0}^{T}=^{T}+( ^{T})}{\|_{S}(^{T})\|_{2}}\] (6)

And the gradient of the first-order flatness regularization \( R^{1}_{}(^{T})\) can be approximated by

\[ R^{1}_{}(^{T})\| _{S}(_{1}^{T})\|_{2}\] (7) \[_{1}^{T}=^{T}+(^{T})\|_{2}}{\|\|_{S}(^{T})\|_{2 }\|_{2}}\] \[\|_{S}(^{T})\|_{2}= ^{2}_{S}(^{T})(^{T})}{\|_ {S}(^{T})\|_{2}}.\]

The optimization is detailed in Appendix algorithm 1. Note that \(\) is the gradient of \(\) with respect to \(\) through this paper, and instead of the expensive computation of Hessian matrix \(^{2}\), Hessian-vector product calculation is used in our algorithm, where the time and especially space complexity are greatly reduced to \(o(n)\) using 1 forward and 1 backward propagation. Thus, the overall calculation in one iteration takes 2 forward and 4 backward propagation in total.

**Theoretical analysis.** Given \(R^{0}_{}()\) measuring the maximal limit of the training loss difference, the first-order flatness is its upper bound by nature. Denoting \(+ B(,)\) the local maximum point, a constant \(^{*}[0,]\) exists according to the mean value theorem that

\[R^{0}_{}() =max\{_{S}(^{})-_{S}():^{}  B(,)\}\] \[=_{S}(+) -_{S}()=(_{S}(+^{*}) )^{T}\|_{S}(+^{*})\|_{2} \|\|_{2}\] \[ max\{\|_{S}(^{})\|_{2}:^{}  B(,)\}=R^{1}_{}().\] (8)

Assuming the loss function is twice differentiable, bounded by \(M\), obeys the triangle inequality, its gradient has bounded variance \(^{2}\), and both the loss function and its second-order gradient are \(-\)Lipschitz smooth, we can prove that, according to , C-Flat converges in all tasks with \( 1/\), \( 1/4\), and \(_{i}^{T}=/\), \(_{i}^{T}=/\) for epoch \(i\) in any task \(T\),

\[}_{i=1}^{n^{T}}[\|_{S^{T}}^{ C}(f^{T}(^{T}))\|^{2}] }_{i=1}^{n^{T}}[\|_{S^{T}}^{R^{0}_{ }}(f^{T}(^{T}))\|^{2}]\] \[+}_{i=1}^{n^{T}}[\| R^{1}_{ ,S^{T}}(f^{T}(^{T}))\|^{2}] }}+}{3b}} +(2}-1)}{^{2}n^{T}}.\] (9)

where \(n^{T}\) is the total iteration numbers of task \(T\), and \(b\) is the batch size.

Upper Bound. Let \(^{2}_{S}(^{*})\) denotes the Hessian matrix at local minimum \(^{*}\), its maximal eigenvalue \(_{max}(^{2}_{S}(^{*}))\) is a proper measure of the landscape curvature. The first-order flatness is proven to be related to the maximal eigenvalue of the Hessian matrix as \(R_{}^{1}(^{*})=^{2}_{max}(^{2}_{S}( ^{*}))\), thus the C-Flat regularization can also be used as an index of model generalization ability, with the following upper bound:

\[R_{}^{C}(^{*})=R_{}^{0}(^{*})+ R_{}^{1}(^ {*})(1+)^{2}_{max}(^{2}_{S}(^{*})).\] (10)

### A Unified CL Framework Using C-Flat

This subsection presents an unified CL framework using C-Flat with applications covering Class Incremental Learning (CIL) approaches. To keep focus, the scope of our study is limited in CIL task, which is the most intractable CL scenarios that seek for a lifelong learning model for sequentially arriving class-agnostic data. Most CIL approaches belong to three main families, Memory-based, Regularization-based and Expansion-based methods.

**Memory-based** methods store samples from the previous phases within the memory limit, or produce pseudo-samples by generative approaches to extend the current training data space, thus a memory replay strategy is used to preserve the seen class features with \(}=} Sample^{t<T}\). iCaRL is one of the early works. It learns classifiers and a feature representation simultaneously, and preserves the first few most representative exemplars according to Nearest-Mean-of-Exemplars Classification. Thus a loss function \(_{}}=_{}}^{CE}+_{}}^{KL}\) combining both cross entropy for the current task and a knowledge distillation loss for the previous classes is introduced to balance the learning sensitivity and model generalization to the previous tasks.

**Solution:** for memory-based method, including, the C-Flat can be easily applied to these scenarios by reconstruct the oracle loss function with its zeroth- and first-order flatness measurement as eq.11, and trained with algorithm1 using data set extended with the previous exemplars.

\[_{}}^{C}(^{T})=_{}}^{R_{}^{0}}(^{ T})+_{}}^{R_{}^{1}}(^{T}).\] (11)

**Regularization-based** methods seeks for a apply regularization on the model develop to preserver the learnt knowledge. For instance, WA introduces weight aligning on the final inference part to balance the old and new classes. Denoting \(\) the feature learning layers of the model, \(=[^{old},^{new}]\) the decision head for all classes consisting of two branches for the old and new seen data classes, the output is corrected to \(f(x)=[^{old}((x)),^{new}((x))]\), where \(\) is the fraction of average norm of \(^{old},^{new}\) of all classes.

Gradient Projection Memory (GPM) is another main regularization based group, introducing explicit align the gradient direction to new knowledge learning. It stores a minimum set of bases of the Core Gradient Space as Gradient Projection Memory, thus gradient steps are only taken in its orthogonal direction to learn the new features without forgetting the core information from the previous phases. FS-DGPM further improves this method by updating model parameter along the aligned orthogonal gradient at the zeroth-order sharpness minima in dynamic GMP space.

**Solution:** for regularization-based approaches, the same plug-and-play strategy can be used to reconstruct the loss function as eq.11, and optimized by algorithm1.

**An alternative solution** for the gradient-based methods like GPM and the improved FS-DGPM, is to introduce C-Flat optimization at the gradient alignment stage, so that the orthogonal gradient at a flatter minima is used to ensure that the training can cross over the knowledge gap between different data categories. The implementation of our C-Flat-GPM is detailed in Appendix algorithm2.

**Expansion-based** methods explicitly construct task-specific parameters to resolve the new class learning and inference problem. For instance, Memory-efficient Expandable Model (Memo) decomposes the embedding module into deep layers and shallow layers that \(=_{f}(_{g})\), where \(_{f},_{g}\) correspond to the specialized block for different tasks and the generalized block that can be shared during training phases. An additional block \(_{f}^{new}\) is added to the deep layers for specified feature extraction for the new classes, where the model can be reconstructed as \(f^{T}=^{T}([_{f}^{T-1}(_{g}),_{f}^{new}(_{g})])\). Thus the new model training is focusing on the task experiment component while the shared shallow layers are frozen with loss function \(_{}}^{Memo}=_{}}^{CE}(^{T}([_{f}^{T-1}( _{g}),_{f}^{new}(_{g})]))\).

Foster uses KL-based loss function to regularize the three combinations of old and new blocks for a stable performance on the previous data. It also introduces an effective redundant parameters and feature pruning strategy to maintain the single backbone model using knowledge distillation. DER follows the same framework, and introduces an auxiliary classifier and related loss item to encourage the model to learn diverse and discriminate features for novel concepts.

**Solution:** for expansion-based approaches, the plug-and-play strategy is still available. The C-Flat loss can be reformed with the reconstructed model as eq. 12. Thus C-Flat optimization using algorithm 1 is applied onto the first stage, where the new constructed block are optimized, while the generalized blocks are kept frozen. The final model is obtained after post-processing.

\[_{}}^{C}(f^{T})=_{}}^{R_{ }^{0}}([^{old},^{new}]([_{f}^{T-1}(_{g}),_{f}^{ new}(_{g})]))\] \[+_{}}^{R_{}^{1}}([^ {old},^{new}]([_{f}^{T-1}(_{g}),_{f}^{new}(_{g})])).\] (12)

**To conclude**, C-Flat can be easily applied to any CL method with reconstructed loss function, and thus trained with the corresponding optimize as shown in algorithm 1. Dedicated design using C-Flat like for the GPM family is also possible wherever flat minima is required.

## 4 Analysis

### Experimental Setup

**Datasets.** We evaluate the performance on CIFAR-100, ImageNet-100 and Tiny-ImageNet. Adherence to [66; 67], the random seed for class-order shuffling is fixed at 1993. Subsequently, we follow two typical class splits in CIL: (i) Divide all \(\|Y_{b}\|\) classes equally into \(B\) phases, denoted as **B0_Inc\(y\)**; (ii) Treat half of the total classes as initial phases, followed by an equally division of the remaining classes into incremental phases, denoted as **B50_Inc\(y\)**. In both settings, \(y\) denotes that learns \(y\) new classes per task.

**Baselines.** To evaluate the efficacy of our method, we plug it into 7 top-performing baselines across each CL category: Replay  and iCaRL  are classical replay-based methods, using raw data as memory cells. PODNet  is akin to iCaRL, incorporating knowledge distillation to constraint the logits of pooled representations. WA  corrects prediction bias via regularizing discrimination and fairness. DER , FOSTER  and MEMO  are network expansion

    &  &  &  & Tiny-ImageNet \\  & Reg. & Mem. & Exp. & B0\_Inc5 & B0\_Inc10 & B0\_Inc20 & B50\_Inc10 & B50\_Inc25 & B0\_Inc40 \\  Replay  & \(\) & 58.83 & 58.87 & 62.82 & **63.89** & 72.18 & 43.31 \\  w/C-Flat & & & **59.98\(\)** & **59.42\(\)** & **64.71\(\)** & 63.60\(\)** & **73.37\(\)** & **44.95\(\)** \\ iCaRL  & \(\) & 58.66 & 59.76 & 61.13 & 64.78 & **77.25** & 45.70 \\ w/C-Flat & & & **59.13\(\)** & **60.40\(\)** & **62.93\(\)** & **65.01\(\)** & 76.22\(\)** & **46.08\(\)** \\  WA  & \(\) & & 63.36 & 66.76 & 68.04 & 73.17 & 80.81 & 55.69 \\ w/C-Flat & & & **65.70\(\)** & **67.79\(\)** & **69.16\(\)** & **73.56\(\)** & **83.84\(\)** & **56.06\(\)** \\  PODNet  & \(\) & 48.05 & 56.01 & 63.45 & 83.66 & 85.95 & 54.24 \\  w/C-Flat & & & **49.70\(\)** & **56.58\(\)** & **64.37\(\)** & **84.31\(\)** & **86.85\(\)** & **55.13\(\)** \\ DER  & & \(\) & 69.99 & 71.01 & 71.40 & 85.17 & 87.10 & 58.63 \\ w/C-Flat & & & **71.11\(\)** & **72.08\(\)** & **72.01\(\)** & **86.64\(\)** & **87.96\(\)** & **60.14\(\)** \\  FOSTER  & \(\) & \(\) & 63.15 & 66.73 & 69.70 & 84.54 & 87.81 & 58.80 \\ w/C-Flat & & & **63.58\(\)** & **67.34\(\)** & **70.89\(\)** & **85.40\(\)** & 87.81\(\)** & **58.88\(\)** \\ MEMO  & & \(\) & 67.42 & 69.82 & 69.91 & 67.28 & 83.09 & 58.15 \\ w/C-Flat & & & **67.56\(\)** & **69.94\(\)** & **71.79\(\)** & **69.34\(\)** & **83.41\(\)** & **58.97\(\)** \\  _Average Return_ & & & **+1.04\(\)** & **+0.66\(\)** & **+1.34\(\)** & **+0.62\(\)** & **+0.90\(\)** & **+0.81\(\)** \\ _Maximum Return_ & & & **+2.34\(\)** & **+1.07\(\)** & **+1.89\(\)** & **+2.06\(\)** & **+3.03\(\)** & **+1.64\(\)** \\   

Table 1: Average accuracy (%) across all phases using 7 state-of-art methods (span all sorts of CL) w/ and w/o C-Flat plugged in. _Maximum/Average Return_ in the last row represents the maximum/average boost of C-Flat towards all methods in each column.

methods, dedicate modular architectures towards each task by extending sub-network or freezing partial parameters. The aforementioned methods span three categories in CL [9; 53]: Memory-based methods, Regularization-based methods and Expansion-based methods.

**Network and training details.** For a given dataset, we study all methods using the same network architecture following repo [66; 67], _i.e._ ResNet-32 for CIFAR and ResNet-18 for ImageNet. If not specified otherwise, the hyper-parameters for all models adhere to the settings in the open-source library [66; 67]. Each task are initialized with the same \(\) and \(\), which drops with iterations according to the scheduler from . To ensure a fair comparison, all models are trained with a vanilla-SGD optimizer . And the proposed method is plugged into the SGD.

### Make Continual Learning Stronger

Table 1 empirically demonstrates the superiority of our method: Makes Continual Learning Stronger. In this experiment, we plug C-Flat into 7 state-of-the-art methods that cover the full range of CL methods. From Table 1, we observe that (i) C-Flat presents consistent outperformance on all models, spanning Memory-based methods, Regularization-based methods, and Expansion-based methods. This superiority is indicative of the plug-and-play feature inherent in our method, allowing effortless installation with all sorts of CL paradigms. (ii) Across multiple benchmark datasets, including CIFAR-100, ImageNet-100, and Tiny-ImageNet, C-Flat exhibits consistent improvement. This underscores its generalization ability and effectiveness against diverse data distributions. (iii) C-Flat presents consistent boosting across multiple incremental scenarios, encompassing B0_Inc5, B0_Inc10, B0_Inc20, B50_Inc10, B50_Inc25, and B0_Inc40. This consistent boosting reaffirms robustness of C-Flat for various CL scenarios. To sum up, C-Flat advances baselines across each CL category, serves as a valuable addition to CL, offering a versatile solution that can complement existing methods.

### Hessian Eigenvalues and Hessian Traces

**Hessian eigenvalues.** Equation 10 delineates the connection between fist-order flatness and Hessian eigenvalues in CL. Broadly, Hessian eigenvalues serve as a metric for assessing the flatness of a function. Thus we report Hessian eigenvalue distributions in Figure 2 for empirical analysis. As shown in Figure 2, models trained with vanilla-SGD exhibit higher maximal Hessian eigenvalues (67.48/21.07 at epochs 50/150 in Figure 1(a) and Figure 1(c)), while our method induces a significant drop in Hessian eigenvalues to 28.11/6.25 at epochs 50/150 in Figure 1(b) and Figure 1(d)) during CL, leading to flatter minima. Consequently, the performance of CL is tangibly enhanced.

**Hessian traces.** We calculate the empirical Fisher information matrix as an estimation of the Hessian and leverage the trace of this to quantify the flatness of the approximation loss at the convergence point. As depicted in Figure 2, we observe that a substantial reduction in the Hessian trace when employing our method compared with vanilla-SGD (670.36/321.36 drops to 429.90/97.36 at epochs 50/150 in Figure 1(b) and Figure 1(d)). This observation suggests that our method induces a flatter minimum. These findings not only align with but also substantiate the theoretical insights presented in the methodology section.

Figure 2: The Hessian eigenvalues and the traces at epochs 50, and 150 on B0_Inc10 setting (MEMO, CIFAR-100) w/ and w/o C-Flat plugged in.

### Visualization of Landscapes

More intuitively, we present a detailed visualization of landscape. PyHessian  is used to draw the loss landscape of models. To simplify, we choose one typical method from each category of CL methods (Replay, Wa, MEMO) for testing. Figure 3 clearly illustrates that, by applying C-Flat, the loss landscape becomes much flatter than that of the vanilla method. This trend consistently holds across various categories of CL methods, providing strong empirical support for C-Flat, and confirms our intuition.

### Revisiting Zeroth-order Flatness

Limited work  proved that the zeroth-order sharpness leads to flat minima boosted CL. Here, we employ a zeroth-order optimizer  instead of vanilla-SGD to verify the performance of C-Flat. As shown in Figure 4, C-Flat (purple line) stably outperforms the zeroth-order sharpness (blue line) on all baselines. We empirically demonstrated that flatter is better for CL.

Former work FS-DGPM  regulates the gradient direction with flat minima to promote CL. The FS (Flattening Sharpness) term derived from FS-DGPM is a typical zeroth-order flatness. We revisit the FS-DGPM series (including La/FS-GPM, DGPM, La/FS-DGPM)  to evaluate performance using C-Flat instead of FS (see algorithm 2). Table 2 yields two conclusions: (i) C-Flat boosts the GPM  baseline as a pluggable regularization term. This not only extends the frontiers of CL methods, incorporating gradient-based solutions, but also reaffirms the remarkable versatility of C-Flat. (ii) Throughout all series of FS-DGPM, C-Flat seamlessly supersedes FS and achieves significantly better performance. This indicates that C-Flat consistently exceeds zeroth-order sharpness. Hence, reconfirming that C-Flat is indeed a simple yet potential CL method that deserves to be widely spread within the CL community.

  Method & La-GPM & FS-GPM & DGPM & La-DGPM & FS-DGPM \\  Oracle & 72.90 & 73.12 & 72.66 & 72.85 & 73.14 \\ w/ C-Flat & **73.66** & **73.57** & **73.01** & **73.64** & **73.72** \\  _Boost_ & **+0.76** & **+0.45** & **+0.35** & **+0.79** & **+0.58** \\  

Table 2: Revisiting FS-DGPM series using C-Flat.

### Computation Overhead

To assess the efficiency of C-Flat, we provides a thorough analysis from the convergence speed and running time with CIFAR-100/B0_Inc20 on Replay. As shown in Figure 5, C-Flat is compared with SGD and other flatness-aware optimiziters. We train C-Flat optimizers on CL benchmarks with 20%, 50%, 100% of iterations and approximately 60% of epochs, while holding the other optimizers at 100%. Figure 4(a) first shows that C-Flat converges fastest and has the highest accuracy (purple line), meaning few iterations/epochs with C-Flat is enough to improve CL. Figure 4(b) shows i) Compared with SGD, with only 20% of iterations and 60% of epochs (pink line) using C-Flat, CL performance is improved using slightly less time; ii) C-Flat surpasses GAM with similar time as SAM when setting the iterations/epochs ratio to 50%/60%; iii) Models trained with C-Flat for 100 epochs outperform those trained with other optimizers for 170 epochs. To sum up, we show that C-Flat outperforms current optimizers with fewer iterations and epochs. This indicates the efficiency of C-Flat.

To discuss practicality better, we provided a tier guideline, which categorizes C-Flat into L1 to L3 levels, as shown in Table 3, L1 denotes the low-speed version of C-Flat, with a slightly lower speed than SAM and the best performance; L2 follows next; L3 denotes the high-speed version of C-Flat, with a faster speed than SGD and a performance close to L2.

### Ablation Study

We perform ablation study in two cases: (i) the influence of \(\) and \(\) on different CL methods; (ii) the influence of \(\) and its scheduler on different optimizers.

We first present the performance of C-Flat with varying \(\) and \(\). As described in Eq. 13, \(\) controls the strength of the C-Flat penalty (when \(\) is equal to 0, this means that first-order flatness is not implemented). As shown in Figure 5(a), compared with vanilla optimizer, C-Flat shows remarkable improvement with varying \(\). Moreover, \(\) controls the step length of gradient ascent. As shown in Figure 5(b), C-Flat with \(\) larger than 0 outperforms C-Flat without gradient ascent, showing that C-Flat benefits from the gradient ascent.

For each CL task \(T\), same learning rate \(^{T}\) and neighborhood size \(^{T}\) initialization are used. By default, \(_{i}^{T}[},}]\) is set as a constant, which decays with respect to the learning rate \(_{i}^{T}[_{-},_{+}]\) by \(_{i}^{T}=_{-}+-_{-})}{_{+}-_{-}}(_{i}^ {T}-_{-})\). Figure 5(c) and Figure 5(d) present a comparison on \(\) initialization and \(\{_{-},_{+}\}\) scheduler. C-Flat outperforms across various settings, and is not oversensitive to hyperparameters in a reasonable range.

### Beyond Not-forgetting

As is known to all, forward, and in particular backward transfer, are the desirable conditions for CL . Here, we thoroughly examine the performance of C-Flat in both aspects. Forward Transfer (FT) means better performance on each subsequent task. Backward Transfer (BT) means better

   Level & Speed & Boost (SGD/SAM) \\  L1 & SGD \(>\) SAM \(>\)**C-Flat** & +2.39\%/+1.91\% \\ L2 & SGD \(>\)**C-Flat** \(>\) SAM & +1.52\%/+1.04\% \\ L3 & **C-Flat** \(>\) SGD & +1.51\%/+1.03\% \\   

Table 3: A tier guideline of C-Flat.

Figure 6: Ablation study about \(\) and \(\). (a) and (b) represents the effect of \(\) and \(\) on different CL methods (WA, Replay, MEMO). (c) and (d) represents the effect of \(\) and \(\) scheduler on MEMO with different optimizers (SGD (red line), SAM, GAM, C-Flat).

performance on previous tasks, when revisited. We count the performance of new and old tasks on several CL benchmarks before and after using C-Flat. As observed in Table 7, C-Flat consistently improves the learning performance of both new and old tasks. This observation indicates that C-Flat empowers these baselines with robust forward and backward transfer capabilities, that is learning a task should improve related tasks, both past and future. But, thus far, achieving a baseline that maintains perfect recall (by forgetting nothing) remains elusive. Should such a baseline emerge, C-Flat stands poised to empower it with potent backward transfer, potentially transcending the limitations of mere not-forgetting.

Moreover, one of our contributions is to prove the positive effect of low curvature on overcoming forgetting. Intuitively, we visualized the change in loss and forgetting of old tasks in CL. Figure 8 shows the lower loss or less forgetting (red line) for old tasks during CL. This is an enlightening finding.

## 5 Conclusion

This paper presents a versatile optimization framework, C-Flat, to confront forgetting. Empirical results demonstrate C-Flat's consistently outperform on all sorts of CL methods, showcasing its plug-and-play feature. Moreover, the exploration of Hessian eigenvalues and traces reaffirms the efficacy of C-Flat in inducing flatter minima to enhance CL. In essence, C-Flat emerges as a simple yet powerful addition to the CL toolkit, making continual learning stronger.

## 6 Acknowledgments

This work was supported in part by the Chunhui Cooperative Research Project from the Ministry of Education of China under Grand HZKY20220560, in part by the National Natural Science Foundation of China under Grant W2433165, and in part by the National Natural Science Foundation of Sichuan Province under Grant 2023YFWZ0009.