# Diyi Yang\({}^{2}\)**Duen Horng Chau\({}^{1}\)**Judy Hoffman\({}^{1}\)

Semi-Truths: A Large-Scale Dataset of AI-Augmented Images for Evaluating Robustness of AI-Generated Image detectors

Anisha Pal\({}^{1}\)1 **Julia Kruk\({}^{1}\)**1 **Mansi Phute\({}^{1}\)**Manognya Bhattaram\({}^{1}\)**\({}^{1}\)Georgia Institute of Technology, \({}^{2}\)Stanford University

{apa172, jkruk3, mphute6, polo, judy}@gatech.edu

{manognya.work}@gmail.com, {diyiy}@stanford.edu

https://huggingface.co/datasets/semi-truths/Semi-Truths

## 1 Abstract

Text-to-image diffusion models have impactful applications in art, design, and entertainment, yet these technologies also pose significant risks by enabling the creation and dissemination of misinformation. Although recent advancements have produced AI-generated image detectors that claim robustness against various augmentations, their true effectiveness remains uncertain. Do these detectors reliably identify images with different levels of augmentation? Are they biased toward specific scenes or data distributions? To investigate, we introduce Semi-Truths, featuring \(27,600\) real images, \(223,400\) masks, and \(1,329,155\) AI-augmented images that feature targeted and localized perturbations produced using diverse augmentation techniques, diffusion models, and data distributions. Each augmented image is accompanied by metadata for standardized and targeted evaluation of detector robustness. Our findings suggest that state-of-the-art detectors exhibit varying sensitivities to the types and degrees of perturbations, data distributions, and augmentation methods used, offering new insights into their performance and limitations. The code for the augmentation and evaluation pipeline is available at https://github.com/J-Kruk/SemiTruths.

Figure 1: **Semi-Truths image augmentations that are measured by the size of the augmented region (Area Ratio) and the semantic change achieved (Semantic Magnitude), categorized into \(3\) levels - small (col1), medium (col2), and large (col3).**

[MISSING_PAGE_FAIL:2]

Finally, we demonstrate how the knowledge abstractions in Semi-Truths can be used to identify the sensitivities of existing detectors. By stress-testing \(6\) models, we reveal unique sensitivities to different data distributions, diffusion models, and perturbation degrees. Our goal is to offer a resource for targeted, interpretable, and standardized evaluation of AI-generated image detection systems, and to provide a customizable evaluation pipeline for the community.

## 2 Related Work

AI Augmented Image datasetThe field of AI-based image generation and perturbation has rapidly evolved from autoencoders  and graphics-based techniques  to GANs [101; 58; 2; 49; 6] and, more recently, diffusion models [57; 70; 61; 24]. These advancements have heightened ethical concerns regarding identity theft and misinformation, [3; 27; 31] necessitating robust datasets for AI-generated image detection. While most research has focused on GAN-generated human faces [16; 71; 47; 39; 14], there is a growing emphasis on diffusion-based techniques for detection of deepfakes , digital forgery  and generic AI-generated content [102; 5; 83; 90]. However, existing datasets face several limitations that restrict their applicability as a benchmark for developing robust detection systems. They often come from a single model [83; 90] or source data distribution [102; 5], lack detailed generation and image metadata , and provide limited control over degree and quality of perturbations [83; 90; 102; 5; 76; 10; 66]. Furthermore, they do not offer scalable pipelines for integrating future image generation and perturbation techniques and are limited in their analysis of detection methods. Recognizing these gaps, we introduce Semi-Truths that incorporates multiple model variations, perturbing techniques, and source data distributions, provides comprehensive metadata, and offers fine-grained control over the quality and degree of perturbations (Tab. 1 summarizes Semi-Truths's contributions).

Image editing pipelinesWith the advent of diffusion models, the field of image editing has seen tremendous advancements . Recent developments in image inpainting, both in text-conditioned [92; 93; 87; 97] and unconditioned  settings, have enabled fine-grained control over image editing significantly enhancing precision and quality. While image inpainting requires the use of masks, prompt-based image editing [28; 54] performs targeted perturbations conditioned solely on text prompts. Existing frameworks like LANCE  and InstructPix2Pix  leverage this capability to develop automated image perturbation pipelines. LANCE , leveraging large language models (LLMs) and image captioning, enables human-supervision-free image editing across diverse perturbations. Building on this, we extend LANCE  to handle a broader range of perturbation magnitudes, guided by semantic change definitions [8; 36]. Our approach integrates LlaVA  and LLAMA  models, combining inpainting and prompt-based techniques for precise, contextually informed perturbations.

Stress Testing PipelinesStress testing pipelines, crucial in software engineering, remain under-utilized in machine learning. While various metrics exist for performance assessment and model comparison , they often lack the depth to fully capture model robustness and explain failure cases adequately. While initiatives like Stress Test NLI  focus on generating adversarial examples to evaluate models' inferential capabilities across six tasks, DynaBench  and CheckList  take a different approach by employing human-in-the-loop systems to dynamically benchmark and assess the robustness of natural language models in real-world scenarios. Simultaneously, in the vision community, Li et al.  utilize diffusion models to create ImageNet-E, honing in on assessing classifier robustness through object attributes, while Luo et al.  explore model sensitivity to user-defined text attributes using StyleGAN . Building upon these endeavors, LANCE  advances the field by extracting insights from failures via a targeted perturbation algorithm, enabling stress testing across diverse attributes. Our work extends this paradigm to AI-generated image detection, presenting a versatile pipeline capable of performing image augmentation with varying magnitudes of perturbations across any diffusion model for a given set of image data points, facilitating evaluation and bias discovery in detector architectures through a comprehensive range of stress tests.

## 3 Semi-Truths

To precisely evaluate a detector's ability to distinguish between AI-generated and real images, we curate Semi-Truths, consisting of over \(27,600\) real images and \(1,329,155\) fake images. Weconsider several crucial factors: (1) strategies for targeted augmentation at varying magnitudes, (2) diversification of scene distributions, (3) caption perturbation methods, (4) image augmentation techniques, and (5) the saliency of augmented images. The imbalance in our Semi-Truths dataset arises from pairing each real image with multiple augmented variations, a vital requirement for the benchmarking scheme as it enables a comprehensive exploration of model sensitivities across various dimensions (such magnitude of augmentation, change in subject matter, and augmentation technique). This section details methods to quantify augmentation magnitudes, followed by our generation and saliency check pipeline, and an overview of key dataset attributes.

### Magnitudes of Augmentation

The alteration made to an image can be quantified in two ways: (1) the proportion of the image area that has been altered (_area ratio of change_), and (2) the degree to which the semantics of the image were altered (_semantic change_). To control the degree of alteration along these axioms, we start with an initial description of the image. This description is obtained by either selecting a segmentation mask and the corresponding class label or, in the absence of mask information, by generating a caption for the image using BLIP .

Introducing PerturbationsMotivated by the categorization of semantic and abstract content from visual semantics research , we create a taxonomy for small, medium, and large semantic changes (see Tab. 2). This taxonomy is used to guide the perturbation of an image caption or mask label using LLaVA-Mistral-7B  or LLAMA-7B  (see Sec. E). As shown in Fig.3, the model is provided with a semantic magnitude category, its definition, a caption to perturb, and the image (if using LLaVA-Mistral-7b). For prompt-based-editing, a diffusion model augments images based on perturbed captions, introducing semantic changes. In conditional inpainting, the perturbed mask label enables precise control over changes in the masked image region.

Measuring Surface Area ChangeWhile segmentation masks help localize augmentations to an image, providing an area ratio of change, diffusion model imprecision can compromise their accuracy.

 
**Smail Changes** & **Medium Changes** & **Large Changes** \\ Do not significantly alter the overall meaning or context of the image. This could include changing the color of a specific object, adding or removing a minor detail, adjusting the composition or perspective of the image, or slightly adjusting the color distribution of the image. & **Shigthly alter the viewer’s perception of the image and its subject. They could involve minor changes to an object or in setting, like altering a back-ground element, moving an object or prone to another location within the frame, or changing the emotions of the people in the frame.** & **Involve substantial modifications to the image that fundamentally transform its interpretation or message. It may even appear surprising or change to an audience. This could include altering, adding or removing major elements of the image background and making changes to the subject of the image. \\ 

Table 2: **Semantic Taxonomy. Definitions of the magnitudes of semantic change, used to guide the perturbation of image captions (for prompt-based-editing) and mask labels(for inpainting) using LLMs for targeted image perturbation.**

Figure 2: **End-to-end pipeline for Semi-Truths curation and detector stress testing. The Semi-Truths pipeline sources data from \(6\) benchmarks and uses \(2\) perturbation techniques to perturb images. These images undergo saliency checks, metric computation, and stress testing of detectors across our curated tests based on the computed change metrics.**

Dong et al.  demonstrate diffusion models can "color outside the box" during inpainting. Furthermore, the lack of mask guidance in prompt-based-editing necessitates the use of post-augmentation metrics to capture the size of alteration. Therefore we employ SSIM , MSE, and a custom metric that assesses the extent to which the structural components differ between the original and augmented images in pixel space. Our custom metric, derived from MSE, uses thresholding to remove noisy components followed by connected component analysis to generate masks indicating areas of change. Similar to the area ratio computed using the mask and the image, we compute a ratio using the generated mask to quantify the surface area of change. Each of these metrics are normalized between 0 and 1 and categorized into small, medium, and large changes based on percentiles: the bottom \(25^{th}\) percentile for small, the \(25^{th}\) to \(75^{th}\) percentile for medium and anything beyond the \(75^{th}\) percentile for large.

Measuring Semantic ChangeAs mentioned previously, large language models (LLMs) are used to perturb image captions and mask labels with respect to the taxonomy of semantic change shown in Tab. 2. However, the stochasticity of LLMs and diffusion models necessitates the implementation of post-augmentation metrics that provide a quantitative measure of semantic change achieved. We use three different scores for this task: LPIPS , DreamSim  (both computed between the original and augmented images), and Sentence Similarity  (calculated between the original and perturbed captions/mask labels; see Sec. C.2). These metrics are normalized and categorized like Surface Area Change metrics, indicating small, medium, and large augmentations.

Additional MetricsIn addition to surface area and semantic change, we incorporate metrics that provide a richer description of the underlying distribution, such as _scene diversity_ and _scene complexity_. Scene diversity is defined by the number of unique elements within the original image, while scene complexity measures the quantity of each unique element. Both of these metrics are derived from instance segmentation maps (see Sec. C.2). Additionally, we distinguish changes by their spatial context - diffused changes are dispersed throughout the augmented image, whereas localized changes are concentrated in a specific area (see Algo. 2).

### Image Augmentation Pipeline

Our image augmentation pipeline, delineated in Fig. 3, expands upon the work of LANCE  by integrating two distinct image augmentation techniques: (1) conditional inpainting and (2) prompt-based-editing. Both approaches leverage linguistic signal as guidance in image augmentation: prompt-based-editing requires a perturbation to the image caption using LLAMA-7B , and conditional inpainting relies on zero-shot mask label perturbation produced by LlaVA-Mistral-7B . Furthermore, the complexity of this pipeline demands comprehensive saliency checks at various stages to ensure that augmented images maintain structural integrity and align with the specified directions of change. To this end, we implement two rounds of saliency evaluation within our image augmentation pipeline to identify instances of high-quality text and image augmentations.

Caption FilteringThe first saliency check protocol evaluates LLM-perturbed captions to ensure two key aspects: (1) accuracy of generated BLIP  captions for prompt-based-editing in representing relevant image information, and (2) coherence and desirability of image edits produced

Figure 3: **Image Augmentation Pipeline.** Components of the image augmentation process for Semi-Truth’s curation using inpainting and prompt-based-editing methods.

by perturbed captions/labels, ensuring semantic alignment with original content. For the former, CLIPScore  measures the difference between embeddings of the original image and its generated caption, filtering out the lowest 5th percentile values. For the latter, cosine similarity between CLIP  text embeddings of the perturbed caption/mask label and the original is calculated, removing values above the \(95^{th}\) percentile (negligible change) and below the \(5^{th}\) percentile (semantic incoherence). Additional details are mentioned in Sec.B

Image Saliency CheckIn the second stage of the saliency check pipeline, we aim to evaluate the (1) structural integrity of augmentations in the image while retaining resemblance to the original, and (2) semantic alignment of image augmentations with the perturbed captions/labels that were used to produce them. Since we lack reference images for direct comparison in image augmentations, conventional metrics like PSNR and SSIM are not suitable. Instead, we explored metrics that assess the structural integrity of each image individually. We use BRISQUE , a reference-free metric which quantifies the perceptual quality of an image, labeling images with a score under 70 as highly salient2. Similarly, we use CLIP similarity  between original and augmented images to ensure the diffusion model performed substantial enough augmentations on the original. We also employ CLIP directional similarity  to confirm that changes in images align with the changes in captions/labels. Images between the \(20^{th}\) and \(80^{th}\) percentile are considered highly salient. Additional details are mentioned in Sec. B

### Semi-Truths Details

Data DistributionWe collect data from \(6\) semantic segmentation benchmarks representing various data distributions: CityScapes  for outdoor urban scenes, SUN RGBD  for indoor scenes, CelebA HQ  for human faces, Human Parsing  for full-body human images, and ADE20K  and OpenImages  for diverse themes. This combined dataset comprises \(27,600\) real images and \(223,400\) masks. Using conditional inpainting and prompt-based-editing techniques across \(5\) diffusion models for inpainting and \(3\) diffusion models for prompt-based-editing, with LlaVA-Mistral-7B  and LLAMA-7B  for prompt perturbation, we create \(325,718\) prompt-based-editing datapoints and \(1,003,437\) inpainting datapoints. After post-perturbation saliency checks, \( 74\)% of images from inpainting and \( 55\)% from prompt-based-editing techniques are labeled as highly salient, totaling \( 915,445\) images.

MetadataSemi-Truths encompasses extensive metadata accompanying both real and fake image pairs and masks, offering insights into every facet of the perturbation process (see Fig. 4). This metadata includes details about the source data distribution, such as the original benchmark from which the image was sourced, scene complexity and diversity (defined by the number and variety of scene elements), a list of unique entities present in each image, and the ratio of mask-occupied area. Additionally, it provides information about the diffusion model, perturbation technique, and language model utilized for each perturbation, alongside the original and perturbed caption/label. Furthermore,

Figure 4: **Semi-Truths details and metadata. Each augmented image in Semi-Truths is accompanied by metadata detailing properties related to the native data distribution, change magnitude (both area and semantics), and directional semantic edits. Attributes highlighted in yellow are novel contributions presented in this work.**

each perturbed image is accompanied by quantitative and qualitative measures of change categorized across semantic and surface area-based metrics, as outlined in Sec. 3.1. The metadata also indicates whether the change is categorized as diffused or localized, determined using a custom algorithm (detailed in Algo. 2). All of this information is crucial for testing the effectiveness of detectors across various axes, as demonstrated in Sec. 4.

## 4 Experiments

We conduct extensive experiments with Semi-Truths to evaluate the effectiveness of AI-generated image detectors in distinguishing real images from AI-augmented content (see Tab. 3). In the following sections, we show how knowledge abstraction over image augmentations in the dataset helps identify nuanced biases in various detectors. All evaluations are conducted on a 10% sample of Semi-Truths, totaling 87,000 images (27,000 real and 60,000 augmented). The evaluation dataset is available at: https://huggingface.co/datasets/semi-truths/Semi-Truths-Evalset. Since the real class (Real) is unaffected in the distribution-specific analysis, the key metric to observe is Recall on the augmented (Fake) class. A dip in Recall for a specific group indicates the detector's sensitivity to that augmentation. Detector default settings (provided in their respective codebases) have been used for conducting evaluations.

Overall Detector PerformanceWe select a diverse set of open-source AI-generated image detectors for stress testing. As demonstrated in Tab. 3, each model has a unique architecture and training distribution. We first evaluate these detectors in a zero-shot setting using metrics like Precision, Recall, and F1-Score to identify top performers for further analysis. Of the \(6\) models 3chosen, only half demonstrated adequate performance for continued evaluation. The underperforming models include (1) DinoV2, a foundation vision model leveraged for zero-shot AI-generated image prediction, (2) CNNSpot, a ResNet-50 trained solely on GAN-generated content, and (3) DIRE, a ResNet-50 trained on diffusion-generated content.

Sensitivity to Data DistributionTo assess potential biases toward specific data distributions, we inspect detector performances on various semantic segmentation benchmarks represented in Semi-Truths. Fig. 5 shows that detector performance varies significantly across data sources. Notably, CrossEfficientViT , which is trained on GAN-generated images of human faces, exhibits a significant performance drop on human faces sourced from benchmarks ADE20K, CityScapes , and SUN-RGBD  (CrossEfficientViT pre-emptively filters any images that do not contain a human face). In contrast, DE-FAKE , trained on general scene images, exhibits the worst performance on CelebA-HQ  and HumanParsing  due to limited focus on humans and portrait-like images in its training distribution. On the other hand, UniversalFakeDetect , trained on indoor bedroom images and other generic scenes, fails to perform well with SUN RGBD and shows a significant performance drop on CityScapes.

Furthermore, we investigate the detectors' ability to handle highly complex and diverse multi-instance scenes. Their performance is evaluated across varying degrees of scene diversity (number of unique class instances in the images) and scene complexity (number of instances in total), categorized into small, medium, and large bins (additional details in Sec. C.2). We find that UniversalFakeDetect's  performance drops with increasing scene diversity and complexity. In contrast, DE-FAKE 

    &  &  &  &  \\   & & Scene & GANs & Diffusion & All & Real & Fake & All & Real & Fake \\ 
1 DINOv2  & ViT  + ResNet-50  & General & ✗ & \(\) & \(29.30\) & \(37.17\) & \(21.43\) & \(49.99\) & \(99.96\) & \(00.01\) \\
2 CNNSpot  & ResNet-50  & General & ✓ & \(\) & \(30.13\) & \(35.27\) & \(25.00\) & \(49.99\) & \(99.99\) & \(00.00\) \\
3 DIRE  & ResNet-50  & General & \(\) & ✓ & \(31.09\) & \(37.18\) & \(25.00\) & \(49.99\) & \(99.99\) & \(00.00\) \\
4 CrossEfficientViT  & EfficientNet-B0  + ViT  & Face & ✓ & \(\) & \(46.37\) & \(34.89\) & \(57.85\) & \(46.58\) & \(28.87\) & \(30.28\) \\
5 UniversalFakeDetect  & CLIP -ViT  & General & ✓ & ✓ & \(64.84\) & \(58.89\) & \(70.79\) & \(60.57\) & \(34.11\) & \(87.03\) \\
6 DE-FAKE  & CLIP  & General & ✓ & ✓ & \(61.65\) & \(49.97\) & \(73.33\) & \(61.88\) & \(52.28\) & \(71.48\) \\   

Table 3: **AI-generated Image Detectors evaluated with Semi-Truths. We evaluated \(6\) detectors with varying backbones and training data distributions. Models that performed satisfactorily, highlighted in green, were selected for additional testing.**remains fairly robust across different scene variations. CrossEfficientViT  shows improved performance with increasing scene complexity and diversity, which can be attributed to human-centered benchmarks like CelebA-HQ  and HumanParsing  segmenting distinct facial features and body parts which results in lower scene complexity.

These results highlight that detectors are highly sensitive to the semantic attributes of data distributions, emphasizing the importance of stress tests to identify and address distributional weaknesses.

Evaluation across Augmentation Techniques and ModelsSemi-Truths contains images generated using two different augmentation approaches - conditional inpainting and prompt-based-editing - as well as five different diffusion algorithms: StableDiffusion v1.4, StableDiffusion v1.5, StableDiffusion XL , OpenJourney , and Kandinsky 2.2 . This diversity in generated content enables investigation of detector sensitivities to different augmentation procedures.4 As shown in Fig.6, UniversalFakeDetect  performs best on images augmented with Kandinsky 2.2  and worst on those augmented with StableDiffusion v1.5 , with a 10% difference in Recall score. The inverse is true for DE-FAKE . CrossEfficientVit  performs best on images augmented with StableDiffusion v1.4 and worst with Kandinsky 2.2 , with a 12% drop in performance. Furthermore, CrossEfficientViT  and DE-FAKE  are more sensitive to inpainted images, whereas UniversalFakeDetect  performs worst on content augmented with prompt-based-editing.

Evaluation across Varying Magnitudes of AugmentationAs detailed in Sec. 3.1, each image in Semi-Truths is fitted with an array of descriptive attributes that capture the magnitude of change. In Fig.7 we examine the impact of varying degrees of augmentation on detector performance, focusing on both surface area and semantic changes. Note that CrossEfficientViT  performs better on smaller values of Area Ratio, where as UniversalFakeDetect  performs better on larger changes.

Figure 5: **Detectors are sensitive to semantic aspects of data distribution. The \(3\) detectors, CrossEfficientViT, DE-FAKE and UniversalFakeDetect were evaluated across varying (a) data distribution, (b) scene complexity and (c) scene diversity.**

Figure 6: **Performance variation across image augmentation methods and diffusion algorithms. Semi-Truths offers data generated using various diffusion algorithms and augmentation methods facilitating detector evaluation on these aspects.**

UniversalFakeDetect's  performance also drops as DreamSim  scores increase. Even though DE-FAKE  is not the best performing model, it appears to be the most robust against various magnitudes of change across the board.

Directional Semantic EditsQuantitative metrics can be reductive when describing how the semantics or narrative of an image changes. Transitioning to an embedded space to assess similarity, for example, often results in significant information loss. To address this issue, we introduce "Directional Semantic Edits", which groups augmented images from Semi-Truths by distinct pairs of original and perturbed caption/mask labels. In the evaluation set, certain directional semantic edits occurred as frequently as \(445\) times. Each detector is evaluated on these groups, and metrics are sorted by Recall, as shown in Tab.4. Each model exhibits distinct performance variations based on specific semantic changes. Notably, UniversalFakeDetect  performs best on augmentations to facial features but worst on augmentations to vegetation. Conversely, DE-FAKE  excels at detecting augmentations to cars and vegetation but struggles with augmentations to human faces. CrossEfficientViT  shows varied performance with augmentations to human faces, appearing in both its highest and lowest ranks, indicating sensitivity to the magnitude of the change.

Further analysis of these augmentations can maximize the potential of these algorithms by informing decisions about the most suitable ensemble techniques. For example, while UniversalFakeDetect  struggles with vegetation-to-tree augmentations, DE-FAKE  excels, suggesting a suitable combination for ensemble approaches. Such analysis reveals the most challenging directional augmentations, offering insights into detector model limitations.

Surveying Human Perception of Magnitudes of ChangeTo build intuition about the algorithms we use to quantify the degree of visual and semantic change achieved during image augmentation, we conduct a user study to evaluate if any metrics align with human perception. Annotators are asked to categorize changes between original and augmented images as "not much," "some," or "a lot," corresponding to our "small," "medium," and "large" change bins. We then compute correlation coefficients (Pearson , Kendall Tau , and Spearman ) between human scores and quantitative measures in Semi-Truths. The results in Tab.5 show that Area Ratio, a novel metric presented in this work, demonstrates the highest correlation with human perception, whereas other metrics demonstrate little to no correlation. It is important to note, however, that some changes may be imperceptible to the human eye but appear drastic in pixel space (additional discussion in Sec. D).

Table 4: **Directional Semantic Edits for investigating detector biases. Directional Semantic Edits provide insights on which edits to a certain entity has a higher chance of fooling detectors, we notice that patterns vary significantly across detectors.**

Figure 7: **Performance variation of select detectors across various magnitudes of augmentation. DE-FAKE  is robust across the board, Area Ratio captures the sensitivity exhibited in UniversalFakeDetect  and CrossEfficientViT .**

## 5 Discussion

Limitations and Future WorkOur inpainting pipeline currently relies on manual semantic mask input from existing semantic segmentation benchmarks, limiting usability. To improve, automatic mask generation methods like SAM  can be embedded into the augmentation pipeline, similar to InstructEdit . Additionally, using LLAMA-7B  and LlaVA  models for zero-shot perturbation has led to many poor-quality outputs, requiring filtering. Future iterations will involve fine-tuning these models. We are also aware of potential biases in metrics like LPIPS , Sentence Similarity , and DreamSim , which may impact evaluations. To mitigate this issue, we will incorporate a combination of multiple open-source LLMs to compute semantic change metrics, thereby reducing the inherent biases associated with any single model, a process facilitated by our modular pipeline which enables easy switching between different LLMs of the user's choosing.

Ethical Issues and Bias MitigationWhile our project aims to create a test suite for evaluating and improving detector robustness, it can also be used to create fake images capable of deceiving AI-generated image detectors, potentially facilitating the spread of misinformation. Hence, we curated Semi-Truths by sourcing images from publicly available datasets with minimal potential for harm, and any manipulations on such images should not serve as a potential threat to society as per our knowledge. Additionally, despite our efforts at diversification of data and models, inherent biases from these modules may persist, potentially perpetuating or exacerbating existing inequalities, resulting in uneven performance across different contexts and types of images. To minimize additional bias, we employ a diverse range of perturbation techniques, diffusion models, LLMs, and source benchmarks. This diversity, along with comprehensive metadata--including original and perturbed captions/labels and images--enables users to analyze perturbation styles and identify existing biases in the generative models.

## 6 Conclusion

To address the rising threat of misinformation from AI-augmented images, we introduce Semi-Truths: a comprehensive dataset of \(1,329,155\) AI-augmented images, with detailed metadata on source distribution, augmentation techniques, change magnitudes, diffusion models, and directional edits (paired original and perturbed captions). Our plug-and-play image perturbation pipeline enables easy generation of additional augmentations and offers a standardized platform to test detector robustness across curated scenarios. Our analysis reveals that state-of-the-art detectors exhibit varying sensitivity to perturbation levels, data distributions, and augmentation methods, providing valuable insights into detector functionality. With a semantic taxonomy for defining change types and a quality-check pipeline, Semi-Truths also serves as a robust training and testing resource, enhancing the resilience of AI-generated image detectors. Furthermore, its diverse metadata enables bias analysis, supporting research into model fairness. We believe the user-friendly design of Semi-Truths will facilitate ongoing research into robustness against evolving generative models, helping combat misinformation effectively.