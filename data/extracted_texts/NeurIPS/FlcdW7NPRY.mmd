# Approaching Human-Level Forecasting with Language Models

Danny Halawi

Equal Contribution.

Fred Zhang

Equal Contribution.

Chen Yueh-Han

Jacob Steinhardt

UC Berkeley

{dhalawi, z0, john0922ucb, jsteinhardt}@berkeley.edu

###### Abstract

Forecasting future events is important for policy and decision making. In this work, we study whether language models (LMs) can forecast at the level of competitive human forecasters. Towards this goal, we develop a retrieval-augmented LM system designed to automatically search for relevant information, generate forecasts, and aggregate predictions. To facilitate our study, we collect a large dataset of questions from competitive forecasting platforms. Under a test set published after the knowledge cut-offs of our LMs, we evaluate the end-to-end performance of our system against the aggregates of human forecasts. On average, the system nears the crowd aggregate of competitive forecasters, and in some settings surpasses it. Our work suggests that using LMs to forecast the future could provide accurate predictions at scale and help to inform institutional decision making.

## 1 Introduction

Forecasting events is important in the modern world. Governments rely on economic and geopolitical forecasts for decision-making. Companies hire and invest based on forecasts of market conditions (Armstrong, 2001). In 2020, epidemiological forecasts for COVID-19 prompted national lockdowns across the globe (Adam, 2020).

There are two main approaches to forecasting. _Statistical forecasting_ primarily uses tools from time-series modeling. This methodology typically excels when data are abundant and under minimal distributional shift. By contrast, in _judgmental forecasting_, human forecasters assign probabilities to future events based on their own judgments, making use of historical data, domain knowledge, Fermi estimates, and intuition. They draw information from diverse sources and reason based on detailed contexts of the task. This enables accurate forecasts even with scarce past observations or under significant distributional shift (Tetlock and Gardner, 2015). We will refer to judgmental forecasting simply as "forecasting".

Since forecasting relies on human effort and expertise, it can be expensive, delayed, or applicable only in specific domains. Moreover, most human forecasts contain little or no explanatory reasoning. These limitations motivate using language models (LMs) to automate forecasting (Hendrycks et al., 2021). Because they can parse and produce texts rapidly, LMs can provide cheap and timely forecasts. Because they are pre-trained on web-scale data, they are endowed with massive, cross-domain knowledge. And because we can elicit their reasonings through prompts, we can examine them to (partially) understand the final forecast.

In this work, we build a LM pipeline for automated forecasting, with a focus on predicting binary outcomes. Our system implements and automates three key components in the traditional forecastingprocess: (1) retrieval, which gathers relevant information from news sources; (2) reasoning, which weighs available data and makes a forecast; and (3) aggregation, which ensembles individual forecasts into an aggregated prediction. Each step makes use of an LM or a collection of LMs which is either prompted or fine-tuned (Figure 1).

To optimize and evaluate our system, we collect a large dataset of forecasting questions from \(5\) competitive forecasting platforms. The test set consists only of (binary) questions published after June 1st, 2023. Since this is after the knowledge cutoff date of our models, this prevents leakage from pre-training. The train set contains questions before June 1st, 2023, which we use for hyperparameter search and fine-tuning our system.

We use a self-supervised approach to fine-tune a LM to make accurate predictions and explanatory reasonings. We first prompt a base LM with various scratchpads to elicit forecasts to questions in our training set. We then fine-tune a new LM on the outputs that outperformed the crowd, which teaches the model what reasoning method to apply in a given context and improves forecasting performance. For hyperparameter search, we identify system configurations, including retrieval and LM prompting strategies, that lead to the best end-to-end performance.

Our optimized system approaches the performance of aggregated human forecasts over the test set, as measured by Brier score, a standard metric in forecasting. To our knowledge, this is the first automated system with forecasting capability that nears the human crowd level, which is generally stronger than individual human forecasters (Section 3.1). We also consider a selective setting where our system uses heuristics, based on the LM's strengths, to decide whether to submit a forecast for a given question and date. In this setting, our system outperforms the human crowd.

## 2 Related Work

**Event forecasting.** Machine learning systems that make accurate, automated forecasts can help inform human decision-making (Hendrycks et al., 2021). Jin et al. (2021) provided ForecastQA, the first dataset for this task, which contains questions created by crowdworkers based on events from news articles. Zou et al. (2022) introduced Autocast, a benchmark dataset compiled from forecasting competition questions up to 2022. In a competition with a large prize pool, no machine learning system was able to approach the performance of human forecasters on Autocast (Zou et al., 2022). The knowledge cut-offs of the latest LMs have moved past 2022, necessitating more recent data. In this work, we source questions in 2023-2024, enabling us to apply recent LMs.

Yan et al. (2024) built a retrieval system that led to improved accuracy on Autocast. They trained a Fusion-in-Decoder model to directly predict the final (binary) resolution (Izacard and Grave, 2021) and reported accuracy, whereas we elicit both explanatory reasonings and probability forecasts from LMs and measure performance with the standard Brier score metric.

Figure 1: **Overview of our retrieval and reasoning systems**. Our retrieval system retrieves summarized new articles and feeds them into the reasoning system, which prompts LMs for reasonings and predictions that are aggregated into a final forecast.

Schoenegger and Park (2023) and Abolghasemi et al. (2023) evaluated GPT-4 and other LLMs on forecasting tournaments and found that they underperform the human crowd. This observation is in line with ours in Section 3.3. Unlike us, they make little efforts to improve these LMs on forecasting.

Finally, there has been recent work on using transformer models or LMs for statistical time-series forecasting (Nie et al., 2023; Gruver et al., 2023; Dooley et al., 2023; Rasul et al., 2023; Jin et al., 2024; Das et al., 2024; Woo et al., 2024), but this is distinct from our focus on judgmental forecasting.

**Information retrieval (IR).** IR can improve question-answering capabilities of LMs (Lewis et al., 2020; Shuster et al., 2021; Nakano et al., 2021). Access to diverse, up-to-date information is crucial in forecasting (Tetlock and Gardner, 2015). Thus, a key component of our system is an IR architecture that furnishes the reasoning model with news articles, using LMs for query writing, ranking and summarization. Beyond our setting, using LMs for IR is an active research topic (Zhu et al., 2024).

**Calibration.** Calibration is important for accurate forecasting (Tetlock and Gardner, 2015). On competitive platforms, forecasters are evaluated by proper scoring rules, such as Brier score (Brier, 1950), which incentivize calibration (Gneiting and Raftery, 2007). There is a vast literature on calibration in deep learning; see Gawlikowski et al. (2021) and Wang (2023) for surveys.

## 3 Preliminaries: Data, Models and Baseline

### Dataset

**Data format.** Forecasting platforms such as Metaculus, Good Judgment Open, INFER, Polymarket, and Manifold invite participants to predict future events by assigning probabilities to outcomes of a question. Each question consists of a _background description_, _resolution criterion_, and \(3\) timestamps: a _begin date_ when the question was published, a _close date_ when no further forecasts can be submitted, and (eventually) a _resolve date_ when the outcome is determined (Table 1). A forecast can be submitted between the begin date and min(resolve date, close date).

**Crowd prediction.** On any given question, as individual forecasts are submitted, forecasting platforms continuously aggregate them into a crowd prediction (see Section A.4 for details about the aggregation mechanisms on different platforms). The crowd prediction is a strong benchmark to compete with. For example, Metaculus (2023) shows that an ensemble of all forecasters consistently outperforms using just the top 5, 10,..., 30 best forecasters (based on past scores). In this work, we compare our system performance to the crowd aggregates.

**Raw data.** We source forecasting questions from the 5 above-mentioned platforms. This yields a total of \(50{,}343\) questions and \(6{,}534{,}042\) user forecasts spanning from 2015 to 2024. The dataset includes \(33{,}664\) binary questions, \(9{,}725\) multiple-choice questions, \(4{,}019\) numerical questions, and \(1{,}346\) questions of other types. The questions cover a wide range of topics across the globe (Figure 8).

The raw dataset contains questions that are ill-defined, overly personal, or of niche interests. Furthermore, recent questions are highly unbalanced, with over \(80\%\) of questions since June 1, 2023 coming from Manifold and Polymarket.

**Data curation.** To address the above issues, we curate a subset by filtering ill-defined questions and removing questions that received few forecasts or trading volume on Manifold and Polymarket. We focus on predicting binary questions and split multiple-choice questions into binary ones.

  
**Field** & **Content** \\  Question & Will Starship achieve liftoff before Monday, May 1st, 2023? \\ Background & On April 14th, SpaceX received a launch license for its Starship spacecraft. A launch scheduled for April 17th was scrubbed due to a frozen valve. SpaceX CEO Elon Musk tweeted: “Learned a lot today, now offloading propellant, retrying in a few days...” \\ Resolution Criteria & This question resolves Yes if Starship leaves the launchpad intact and under its own power before 11:59pm ET on Sunday, April 30th. \\ Key Dates & Begin Date: 2023-04-17 & Close Date: 2023-04-30 & Resolve Date: 2023-04-20 \\   

Table 1: **A sample question** with its background, resolution criteria, and key dates. The question resolved early (with a final resolution of Yes). See Table 10 for the complete sample point.

To guard potential leakage from LMs' pre-training, we only include questions in the test set that appear after the knowledge cut-off for the models we use (June 1, 2023). All test set questions were opened after the date, and all train and validation questions were resolved before. Questions that span across the date are discarded. This yields a set of \(5{,}516\) binary questions, including \(3{,}762\) for training, \(840\) for validation, and \(914\) for testing (Table 1(a)). We show a sample data point in Table 10 and provide all details about the curation process in Appendix C.

### Evaluation

**Retrieval schedule.** We can simulate forecasting the future by using the fact that models are only trained up to a cut-off date (Zou et al., 2022). To simulate a forecast for a resolved question, we query a historical news corpus to retrieve articles between the question begin date and a specified _retrieval date_(Zou et al., 2022; Yan et al., 2024). The retrieval date can be viewed as the "simulated date" of the forecast, as we are mimicking the information the model would have had access to on that date.

To create a set of retrieval dates for each question, we use geometrically increasing time points between the open and close dates (see Section A.1 further motivations). Specifically, we take \(n=5\) retrieval dates per question, where the \(k\)th date is given by

\[_{k}=_{}+(_{ {close}}-_{}-1)^{k/n}. \]

For questions that resolve before they close, we exclude all retrieval dates occurring after the question has been resolved. Under this geometric retrieval schedule, we retain \(86\%\) of retrieval dates on average across all questions. The average question window in our corpus is approximately \(70\) days, and the average time until resolution is \(42\) days.

**Metric.** Our work focuses on binary questions and uses the Brier score as the performance metric, defined as \((f-o)^{2}\), where \(f\) is the probabilistic forecast and \(o\{0,1\}\) is the outcome. The Brier score is a strictly proper scoring rule: assuming the true probability that \(o=1\) is \(p\), the optimal strategy is to report \(f=p\). This is desirable, since improper scoring rules would incentivize reporting distorted probabilities. As a baseline, an (unskilled) forecast of \(.5\) attains a Brier score of \(.25\).

To compute the final Brier score, we first average the Brier scores across retrieval dates for each question, then average across questions. We also report standard errors; however, note that the computation of standard errors assumes the data are i.i.d., while our data are in fact time-series, so this likely underestimates the true error. We also report root mean square (RMS) calibration error.

**Models.** We evaluate \(14\) instruction-tuned LMs (Section A.2). Our main system relies on GPT-3.5, GPT-4-0613, and GPT-4-1106-Preview, all of which have a knowledge cut-off of April 2023 or before. To ensure that the models have no knowledge after this date, we submit them to a knowledge test where we prompt models to answer questions about major events after April 2023. In this test, we find no evidence of leakage (see Section A.3 for detailed methodology and results).

### Models are not naturally good at forecasting

As a baseline, we evaluate all \(14\) LMs with no additional information retrieval. We use zero-shot prompts and scratchpad prompts (Nye et al., 2021). For each prompting strategy, we craft candidate

Table 2: **(a) Distribution of our train, validation, and test sets across all 5 forecasting platforms. (b) Baseline performance of pre-trained models on the test set (see full results in Table 6). Subscript numbers denote 1 standard error. Random baseline: 0.25; human crowd: 0.149.**prompts, pick the best prompt on the validation set, and report its Brier scores on the test set. The prompt choices appear in Figure 4 and Figure 5 and further details are in Appendix B.

None of the models are naturally good at forecasting (Table 2b, Table 6). Most models' scores are around or worse than random guessing (\(.25\)). Only the GPT-4 and Claude-2 series beat the unskilled baseline by a large margin (\(>.02\)). Moreover, while GPT-4-1106-Preview achieves the lowest Brier score of \(.208\), it trails significantly behind the human crowd performance of \(.149\).

## 4 Our System

LMs perform poorly in the baseline setting (Table 2b). Forecasting requires models to have access to detailed, up-to-date contextual information and the ability to effectively utilize this information to generate accurate probabilities. Our system addresses this challenge via news retrieval, and elicits better reasoning via optimized prompting and fine-tuning.

### Retrieval

Our retrieval system consists of \(4\) steps: search query generation, news retrieval, relevance filtering and re-ranking, and text summarization (Figure 1a).

To generate search queries, we initially tried a simple query expansion prompt (Figure 12a), instructing the model to create queries based on the question and its background. However, we find that this overlooks sub-considerations that often contribute to accurate forecasting. To achieve broader coverage, we prompt the model to decompose the forecasting question into sub-questions (Figure 12b) and use each to generate a search query (Min et al., 2019). For instance, when forecasting election outcomes, the first approach searches directly for polling data, while the latter creates sub-questions that cover campaign finances, economic indicators, and geopolitical events. We combine both approaches for comprehensive coverage.

We next retrieve articles from news APIs using LM-generated search queries. We evaluate \(5\) APIs on the relevance of retrieved articles and select NewsCatcher and Google News (Section F.3).

Our initial retrieval provides wide coverage at the cost of obtaining some irrelevant articles. To ensure that they do not mislead the model at the reasoning step, we prompt GPT-3.5-Turbo to rate the relevancy of all articles (Figure 13) and filter out low-scoring ones. Since the procedure is costly in run-time and budget, we only present the article's title and first 250 words to the model in context. We validate that this approach achieves high recall and precision while saving \(70\%\) cost (see Section F.4 for alternative methods and results).

Since LMs are limited by their context window, we summarize the articles. In particular, we prompt GPT-3.5-Turbo to distill the most relevant details from each article with respect to the forecasting question (Figure 14). Finally, we present the top \(k\) article summaries to the LM, ordered by their relevancy. We choose the ranking criterion, article count \(k\), and summarization prompt based on end-to-end Brier scores over the validation set (Section 5.2).

  
**Criteria** &  &  &  \\   & **Ours** & **Crowd** & **Aggregate** & **Ours** & **Crowd** & **Aggregate** & **Forecasts** & **Questions** \\ 
**All Questions** & \(.179_{.003}\) & \(.149_{.003}\) & \(}\) & \(71.5_{.7}\) & \(77.0_{.7}\) & \(}\) & \(100\%\) & \(100\%\) \\
**Crowd Uncertain** & \(.238_{.004}\) & \(.240_{.003}\) & \(}\) & \(58.1_{.3}\) & \(58.3_{.13}\) & \(}\) & 51\% & 56\% \\
**Early Retrieval** & \(.186_{.003}\) & \(.162_{.004}\) & \(}\) & \(70.0_{.9}\) & \(74.4_{.9}\) & \(}\) & 84\% & 100\% \\ \(5+\)**Articles** & \(.175_{.003}\) & \(.142_{.003}\) & \(}\) & \(72.3_{s}\) & \(77.7_{.7}\) & \(}\) & 84\% & 94\% \\ 
**All Criteria** & \(.}\) & \(.247_{.004}\) & \(}\) & \(}\) & \(54.2_{1.7}\) & \(}\) & 22\% & 43\% \\   

Table 3: **System performance** on the test set. “All Questions”: Brier score on full test set. Other rows: selective evaluation when criteria are met. “Crowd Uncertain”: crowd predictions 0.3-0.7. “Early Retrieval”: first 3 retrieval dates. “5+ Articles”: \(\)5 relevant articles. “All Criteria”: all 3 criteria met. System-crowd aggregate performs best in all settings. Subscripts: 1 standard error. Bold: outperforms crowd aggregate. Underlined: best in category.

### Reasoning

Prior work in forecasting has focused on eliciting predictions from models without requiring rationales (Zou et al., 2022; Yan et al., 2024). However, accurately predicting the future is a difficult task that often requires computation beyond a single forward pass. Having the model externalize its reasoning also allows us to understand the explanation for the forecast and improve it accordingly.

We use open-ended scratchpad to structure model's reasoning paths. Our prompt begins with posing the question, providing a description, and specifying resolution criteria and key dates, followed by the top \(k\) relevant summaries from our retrieval system (see Figure 10 for the basic template).

Qualitatively, we observe several failure modes from our baseline LM evaluations (Section 3.3), including misunderstanding of the question and overconfidence in its output. The optimal reasoning prompt (Figure 11), as identified by hyperparameter sweep (Section 5.2), incorporates four additional components inspired by the literature on forecasting and LM reasoning to fix these issues:

* First, to ensure that the model comprehends the question, we prompt it to rephrase the question. It is also instructed to expand the question with its own knowledge. Intuitively, a more detailed and precise phrasing of the question elicits better responses (Deng et al., 2023).
* Forecasting requires a holistic consideration of the possibilities (Tetlock and Gardner, 2015). We next prompt the model to leverage the retrieved information and its pre-training knowledge to produce arguments for why the outcome may or may not occur.
* The model can potentially generate weak arguments. To avoid treating all considerations as equal, it is instructed to weigh them by importance and aggregate them into an initial forecast.
* Finally, to prevent potential bias and miscalibration, the model is asked to check if it is over- or under-confident (Tian et al., 2023) and to consider historical base rates (Tetlock and Gardner, 2015), prompting it to calibrate and amend the prediction accordingly.

**Base model.** We prompt GPT-4-1106-Preview with the best scratchpads, since it consistently gives the lowest Brier scores among the LMs we test (Section 5.2).

**Fine-tuned model.** We also prompt a fine-tuned version of GPT-4 that we trained to generate reasonings with accurate predictions (Section 5.1). We prompt it with only the question's basic information (no scratchpad instructions) since our fine-tuned model is trained to reason without prescriptive instructions.

**Ensembling.** Since the aggregate of predictions is usually superior to individual forecasts (Tetlock and Gardner, 2015), we elicit multiple predictions from the base and fine-tuned models. We prompt GPT-4-1106-Preview with the optimal scratchpad prompt (Figure 11), along with the \(2\) next best scratchpad prompts identified in Section 5.2. For our fine-tuned model, we set temperature \(T=0.5\) and prompt it \(3\) times to sample \(3\) additional forecasts. This gives us \(6\) forecasts in total: \(3\) from the base model, and \(3\) from the fine-tuned model.

Given these forecasts, the system ensembles them into a final prediction by taking a trimmed mean, as it performs best on the validation set among the ensemble methods we test (Section F.1). We provide further details about our system in Appendix D, including hyperparameters and prompt designs.

## 5 Optimizing the System

We now describe the procedure to optimize our retrieval and reasoning system and the results obtained.

Figure 2: **Our method of self-supervised training. For each question, the method produces multiple candidate reasoning-predictions and selects those that outperform human aggregates for fine-tuning.**

### Fine-tuning a Reasoning Model

Fine-tuning language models for judgmental forecasting is challenging due to the scarcity of high-quality human-written reasonings. Moreover, directly fine-tuning on outcomes can lead to miscalibration. To address these issues, we propose a self-supervised procedure inspired by recent work on distillation and self-training (Zelikman et al., 2022; Gulcehre et al., 2023; Singh et al., 2024; Agarwal et al., 2024; Gu et al., 2024). We generate data for fine-tuning through a two-step process: (1) collecting a large set of LM-produced reasonings on the train set, and (2) selecting a subset where the model outperforms the human crowd within a fixed margin to avoid overconfidence.

**Collecting fine-tuning data.** To generate the preliminary data, we run our system at each retrieval date in the retrieval schedule and on each question in the train set. To elicit a diverse set of reasonings for each question, we vary the prompts, retrieval configurations, and other hyperparameters. See Appendix E for details.

**Selecting fine-tuning data.** We seek to fine-tune our model on strong forecasts. To select the data, we only keep outputs that give a lower Brier score than the crowd's. However, this can inadvertently cause overconfidence in our fine-tuned model. To mitigate this, we discard pairs where the prediction deviates by more than \(0.15\) from the crowd prediction, and we also average our prediction with the crowd prediction when constructing the target output.

The resulting fine-tuning data has the following structure (Figure 2, right): The **input** to the model consists of the question, description, and resolution criteria, followed by summarized articles; and the **target output** consists of a reasoning and a prediction.

Importantly, the fine-tuning input excludes the scratchpad instructions. By doing so, we directly teach the model which reasoning to apply in a given context. In total, \(73,632\) reasonings are generated from which \(13,253\) meet the above desiderata. Finally, we fine-tune GPT-4-06131 only on the \(6,\!000\) most recent points for 2 epochs, due to budget constraint (Figure 2, right).

### Hyperparameter Sweep

Our hyperparameter sweep optimizes an (intermediate) metric over a discrete set of choices, such as LM prompts and the number of articles presented to the reasoning model in context.

We divide the hyperparameters into groups of 1-2 and optimize them iteratively. For each group, we select the best configuration based on the average Brier score on the validation set, except for search query generation where we use proxy metrics for efficiency. See Appendix F for details and results.

## 6 Evaluations

We evaluate our optimized system on the test set and find that it comes close to human crowd performance (Section 6.1). We then analyze its strengths and weaknesses (Section 6.2). Motivated by the results, we introduce a relaxed setting, where the system may make forecasts selectively (given its

Figure 3: **System strengths. Our system outperforms the crowd on the validation set when: (a) given sufficient relevant articles, (b) the crowd is uncertain (predictions between.3 and.7), where we achieve a better Brier score (.199 vs..246), and (c) at earlier retrieval dates.**identified strengths), and find that our system surpasses the crowd aggregate (Section 6.3). Finally, we demonstrate how our system can be used to complement aggregated human forecasts (Section 6.4).

### System Nears Human Performance

We first evaluate the Brier score of our end-to-end system on the test set. Note that all hyperparameters were chosen based on the validation set and all test set questions appear temporally after the validation questions, mirroring the setting of a real-time forecasting competition. In addition to the Brier score, we also report accuracy to compare with past work (Zou et al., 2022; Yan et al., 2024).

As the main result, our averaged Brier score is \(.179\), while the crowd achieves \(.149\), resulting in a difference of \(.03\). In comparison with the baseline evaluation (Section 3.3), our system's Brier score (\(.179\)) significantly outperforms the best baseline model (\(.208\) with GPT-4-1106-Preview). Further detailed results across different platforms and categories can be found in Table 14.

In prior work, Zou et al. (2022) evaluated their system on the forecasting dataset Autocast, which consists of questions from 3 of the platforms we use: Metaculus, INFER, and GJOpen. They achieved an accuracy of \(65.4\%\) compared to a community baseline of \(92.8\%\). Yan et al. (2024) later improved this to \(67.9\%\). We achieve a better accuracy of \(71.5\%\) even though the questions we consider are harder, with a significantly lower crowd accuracy of \(77.0\%\).

Finally, on the test set, we observe that our system is well calibrated (Figure 15c) with RMS calibration error \(.42\) (human crowd: \(.38\)). Interestingly, this is not the case in the baseline evaluations (Section 3.3), where the models are _not_ well calibrated in the zero-shot setting (Figure 15b). Through fine-tuning and ensembling, our system improves the calibration of the base models, without undergoing specific training for calibration.

### System Strengths and Weaknesses

We next seek to understand our system's strengths and weaknesses on the validation set, and later use these insights to improve performance on the test set (Section 6.3). We find that our system performs best relative to the crowd when (1) the crowd is less confident, (2) at earlier retrieval dates, and (3) when it retrieves many articles.

First, our system outperforms the crowd when their prediction ranges between \(.3\) and \(.7\) (Figure 3b). However, it underperforms when the crowd is highly certain, possibly due to the model's tendency to hedge predictions as a result of its safety training (Schulman (2023); see Figure 16 for a qualitative example). For retrieval dates, our system outperforms the crowd on earlier dates but not on later ones, with its Brier score improving slowly relative to the crowd as questions approach resolution (Figure 3c). This may be attributed to the model's hedging even when evidence becomes decisive. The system's performance nears and eventually surpasses the crowd's when there are at least 5 relevant articles, indicating that it relies on high-quality retrieval for better performance (Figure 15a).

### System Beats Crowd in the Selective Setting

In real-word forecasting competitions, forecasters do not have to make predictions on every question in the platform at every possible date. Instead, they typically make predictions on questions where they have expertise or interest in and at times that they choose. Thus, it is natural to use our system's strengths and decide accordingly if we should forecast on a retrieval date \(k\) for a question \(q\).

Leveraging the insights from Section 6.2, we make selective forecasts under the following conditions: (1) the crowd's probability estimate is between 0.3 and 0.7, (2) the retrieval date is within the first 3 out of 5 total retrieval dates, and (3) at least 5 relevant articles are retrieved. The gap in Brier score between our system and the crowd shrinks under each heuristic except the third one (Table 3).

Under the first heuristic, we outperform the crowd by a small margin (\(.238\) vs. \(.240\)). This is valuable as our system can be used to complement the crowd's prediction when there is greater uncertainty. When all three conditions are jointly met, our system beats the crowd significantly (by more than \(1.5\) standard errors in both Brier score and accuracy).

### System Complements the Crowd

Finally, aggregates of our system with the crowd forecasts outperform either one in isolation. By combining the system's predictions with the crowd using a weighted average -- 4x weight for the crowd, which we find optimal on the validation set -- we improve the overall Brier score from \(.149\) to \(.146\) on the full test set (Table 3, top row).

Moreover, our system excels under certain criteria (Section 6.2). It is especially useful in these cases to supplement the crowd prediction. We report these results in Table 3, using an unweighted average. This outperforms the crowd prediction in all cases: For example, the crowd Brier score is \(.24\) for questions that have a crowd prediction between \(.3\) and \(.7\), but when averaging with our system's prediction the Brier score decreases to \(.233\).

## 7 Ablations

We conduct three ablation studies to investigate the factors contributing to our system's performance. The first study validates that our performance is not solely due to the power of our base model, GPT-4. The second and third studies demonstrate the effectiveness of our retrieval and fine-tuning methods.

**Fine-tuning a less capable model.** To demonstrate that our system's performance does not depend entirely on the capabilities of the base model (i.e., GPT-4), we fine-tune GPT-3.5 on all our fine-tuning data and compare its performance to that of fine-tuned GPT-4-0613. We find that the Brier score of the system using fine-tuned GPT-3.5 is only slightly worse at \(.183\), compared to \(.182\) achieved by the system using fine-tuned GPT-4-0613 (Table 4). This suggests that our fine-tuning procedure is effective even with a less capable base model.

**No fine-tuning.** To demonstrate the benefit of fine-tuning (Section 5.1), we evaluate our optimal system using only the base GPT-4-Preview-1106 as the reasoning model. In this setup, the ablated system achieves a Brier score of \(0.186\), an increase of \(0.007\) compared to the original score. This indicates that fine-tuning provides a significant boost to our system's performance.

**No fine-tuning and no retrieval.** We evaluate our optimal system without any news retrieval and using the base GPT-4-1106-Preview model. The ablated system attains a Brier score of \(0.206\).

Recall that in our baseline evaluation (Section 3.3), the lowest Brier score attained by any model is \(0.208\). Our ablated system essentially deteriorates to this baseline level. Without any fine-tuning or retrieval, the only expected advantage of our system over the baseline evaluation setup is its reasoning prompt, which was found through searching a set of candidate prompts (Section 5). The experiment suggests that the reasoning prompt alone provides only a minor improvement in performance.

## 8 Conclusion

Our work presents the first ML system that can forecast at near human levels. We develop a novel retrieval mechanism that uses a LM to determine which information to source and how to evaluate its relevance. We also give a self-supervised fine-tuning method to generate reasonings with accurate predictions. In addition, our system can potentially aid human forecasters by providing effective news retrieval and novel perspectives in reasoning drawn from LM pre-training knowledge.

To facilitate further research, we release our dataset: the largest and most recent forecasting dataset compiled from 5 real-world forecasting competitions. We discuss future directions (e.g., domain-adaptive training and data augmentation) to improve our system in Appendix H.

    &  &  \\   & **Ours** & **Aggregate** & **Ours** & **Aggregate** \\ 
**Full System** & \(.179_{.003}\) & \(_{}\) & \(71.5_{}\) & \(_{}\) \\
**Fine-tuned GPT-4-0613** & \(.182_{.002}\) & \(_{}\) & \(70.7_{}\) & \(_{}\) \\
**Fine-tuned GPT-3.5** & \(.183_{.002}\) & \(_{}\) & \(71.5_{}\) & \(_{}\) \\
**Base GPT-4-1106-Preview** & \(.186_{.002}\) & \(_{}\) & \(70.6_{}\) & \(_{}\) \\
**Base GPT-4-1106-Preview; no IR** & \(.206_{.002}\) & \(.150_{.002}\) & \(66.6_{}\) & \(76.9_{}\) \\   

Table 4: **Ablation results: Fine-tuning GPT-3.5 has similar performance to fine-tuning GPT-4-0613 (rows 2–3). Our system degrades without fine-tuning (row 4) or retrieval (row 5), as expected. “Aggregate” is the weighted average with crowd prediction. Subscripts are standard errors; bold entries beat the human crowd.**At a high level, our results suggest that in the near future, LM-based systems may be able to generate accurate forecasts at the level of competitive human forecasters. We hope that our work paves the way for automated, scalable forecasting that can help to inform institutional decision making.