# On \(f\)-Divergence Principled Domain Adaptation: An Improved Framework

Ziqiao Wang

Tongji University

Shanghai, China

ziqiaowang@tongji.edu.cn &Yongyi Mao

University of Ottawa

Ottawa, Canada

ymao@uottawa.ca

###### Abstract

Unsupervised domain adaptation (UDA) plays a crucial role in addressing distribution shifts in machine learning. In this work, we improve the theoretical foundations of UDA proposed in Acuna et al. (2021) by refining their \(f\)-divergence-based discrepancy and additionally introducing a new measure, \(f\)-domain discrepancy (\(f\)-DD). By removing the absolute value function and incorporating a scaling parameter, \(f\)-DD obtains novel target error and sample complexity bounds, allowing us to recover previous KL-based results and bridging the gap between algorithms and theory presented in Acuna et al. (2021). Using a localization technique, we also develop a fast-rate generalization bound. Empirical results demonstrate the superior performance of \(f\)-DD-based learning algorithms over previous works in popular UDA benchmarks.

## 1 Introduction

Machine learning often faces the daunting challenge of domain shift, where the distribution of data in the testing environment differs from that used in training. _Unsupervised Domain Adaptation_ (UDA) arises as a solution to this problem. In UDA, models are allowed to access to labelled source domain data and unlabelled target domain data, while the ultimate goal is to find a model that performs well on the target domain. The mainstream theoretical foundations of UDA, and more broadly, domain adaptation , primarily rely on the seminal works of discrepancy-based theory [3; 4]. In particular, [3; 4] characterize the error gap between two domains using a hypothesis class-specified discrepancy measure e.g., \(\)-divergence. While these works initially focus on binary classification tasks and zero-one loss,  extend the theory to a more general setting. Subsequently, this kind of theoretical framework was extended by various works [6; 7; 8; 9; 10; 1; 11], all sharing some common properties such as the ability to estimate the proposed discrepancy from finite unlabeled samples. Importantly, these theoretical results often inspire the design of new algorithms, such as domain-adversarial training of neural networks (DANN)  and Margin Disparity Discrepancy (MDD) , directly motivated by \(\)-divergence.

Recently,  proposes an \(f\)-divergence-based domain learning framework, which generalizes various previous frameworks (e.g., those based on \(\)-divergence) and have demonstrated great empirical successes. However, we argue that this framework has potential limitations, at least in three aspects.

First, their discrepancy measure is based on the variational representation of \(f\)-divergence in  (cf. Lemma 2.1). Although this variational formula is commonly adopted, its weakness has been pointed out in several works [14; 15]. For example, from this formula, one cannot recover the well-known Donsker and Varadhan's (DV) representation of KL divergence . This reveals a second limitation: some existing domain adaptation or transfer learning theories are based on the DV representation of KL, such as [17; 18; 19; 20], and the framework by , although including KL as a special case of \(f\)-divergence, fail to unify those theories. Furthermore and more critically,the discrepancy measure proposed by  contains an absolute value function, while the original variational representation does not. Notably, this absolute value function is necessary in their derivation of a target error upper bound, and the fundamental reason behind this is still due to the weak version of the variational representation relied upon. Specifically, a variational representation of an \(f\)-divergence is a lower bound of the divergence, but using a weak lower bound creates technical difficulties in proving an upper bound for target domain error.  chooses to add an absolute value function accordingly, potentially leading to an overestimation of the corresponding \(f\)-divergence. In fact, this absolute value function is removed in their proposed algorithm, termed \(f\)-Domain Adversarial Learning (\(f\)-DAL). While \(f\)-DAL outperforms both DANN and MDD in standard benchmarks, this choice exhibits a clear gap between their theory and algorithm.

In this work, to overcome these limitations and explore the full potential of \(f\)-divergence, we present an improved framework for \(f\)-divergence-based domain learning theory. Specifically, we apply a more advanced variational representation of \(f\)-divergence (cf. Lemma 2.2), independently developed by [21; 22] and . After introducing some preliminaries, the organization of the remainder of the paper and our main contributions are summarized below.

* In Section 3, we revisit the theoretical analysis in , where we refine their \(f\)-divergence-based discrepancy by Lemma 2.2 while retaining the absolute value function in the definition. The resulting target error bound (cf. Lemma 3.1) and the KL-based generalization bound (cf. Theorem 3.1) complement the theoretical framework of .
* In Section 4, we design a novel \(f\)-divergence-based domain discrepancy measure, dubbed \(f\)-DD. Specifically, we eliminate the absolute value function from the definition used in Section 3, incorporating a scaling parameter instead. We then derive an upper bound for the target error (cf. Theorem 4.1) and the sample complexity bound for our \(f\)-DD. The generalization bound based on empirical \(f\)-DD naturally follows from these results. Notably, the obtained target error bound allows us to recover the previous KL-based result in  (cf. Corollary 4.1).
* In Section 5, to improve the convergence rate of our \(f\)-DD-based bound, we sharpen the bound using a localization technique [24; 10]. The localized \(f\)-DD allows for a crisp application of the local Rademacher complexity-based concentration results , while also enabling us to achieve a fast-rate target error bound (cf. Theorem 5.1). As a concrete example, we present a generalization bound based on localized KL-DD (cf. Theorem 5.2), where our proof techniques are directly connected to fast-rate PAC-Bayesian bounds [25; 26] and fast-rate information-theoretic generalization bounds [27; 28; 29].
* In Section 6, we conduct empirical studies based on our \(f\)-DD framework. We show \(f\)-DD outperforms the original \(f\)-DAL in three popular UDA benchmarks, with the best performance achieved by Jeffereys-DD. Additionally, we note that the training objective in \(f\)-DAL aligns more closely with our theory than with  (cf. Proposition 1). We also show that adding the absolute value function leads to overestimation and that optimizing the scaling parameter in \(f\)-DD may not be necessary in practical algorithms.

## 2 Preliminaries

Notations and UDA SetupLet \(\) and \(\) be the input space and the label space. Let \(\) be the hypothesis space, where each \(h\) is a hypothesis mapping \(\) to \(\). Consider a single-source domain adaptation setting, where \(\) and \(\) are two unknown distributions on \(\), characterizing respectively the source domain and the target domain. Let \(=\{X_{i},Y_{i}\}_{i=1}^{n}{}^{ n}\) be a labeled source-domain sample and \(=\{X_{j}\}_{j=1}^{m}{}_{}^{ m}\) be an unlabelled target-domain sample, where \(_{}\) is the marginal distribution of \(X\) in the target domain. We use \(\) and \(\) to denote the empirical distributions on \(\) corresponding to \(\) and \(\), respectively. The objective of UDA is to find a hypothesis \(h\) based on \(\) and \(\) that "works well" on the target domain.

Let \(:_{0}^{+}\) be a symmetric loss (e.g., \((y,y)=0\) for all \(y\)). The population risk for each \(h\) in the target domain (i.e. target error) is defined as \(R_{}(h)_{(X,Y)}[(h(X),Y)]\), and the population risk in the source domain, \(R_{}(h)\), is defined in the same way. Since \(\) and \(\) are unknown to the learner, one often uses recourse to the empirical risk in the source domain, which, for a given \(\), is defined as \(R_{}(h)_{(X,Y)}[(h(X),Y) ]=_{i=1}^{n}(h(X_{i}),Y_{i})\). Furthermore, let \(f_{}\) and \(f_{}\) be the ground truth labeling functions for the target domain and source domain, respectively, i.e. \(f_{}(x)=_{y}(Y=y|x)\) and \(f_{}(x)=_{y}(Y=y|x)\).

With a little abuse of the notation, we will simply use \((h,h^{})\) to represent \((h(x),h^{}(x))\) when the same \(x\) is evaluated, serving as the "disagreement" for \(h\) and \(h^{}\) on \(x\). Additionally, following the conventional literature on DA theory , we assume that the loss function satisfies the triangle property1. For the readers' convenience, a summary of all notations is provided in Table 5 in the Appendix.

Background on \(f\)-divergenceThe family of \(f\)-divergence is defined as follows.

**Definition 2.1** (\(f\)-divergence ).: Let \(P\) and \(Q\) be two distributions on \(\). Let \(:_{+}\) be a convex function with \((1)=0\). If \(P Q^{2}\), then \(f\)-divergence is defined as \(_{}(P||Q)_{Q}[( )]\), where \(\) is a Radon-Nikodym derivative.

The \(f\)-divergence family contains many popular divergences. For example, letting \((x)=x x\) (or \(x x+c(x-1)\) for any constant \(c\)) recovers the definition of KL divergence.

The \(f\)-divergence discrepancy measure by  is motivated by the variational formula of \(f\)-divergence that utilizes the Legendre transformation (LT).

**Lemma 2.1** ().: _Let \(^{*}\) be the convex conjugate3 of \(\), and \(=\{g:(^{*})\}\). Then_

\[_{}(P||Q)=_{g}_{ P}[g ()]-_{ Q}[^{*}(g())] {.}\]

However, it is well-known that the variational formula in Lemma 2.1 does not recover the the Donsker and Varadhan's (DV) representation of KL divergence (cf. Lemma B.1). We will elaborate on this later. Recently,  and  concurrently introduce a novel variational representation for \(f\)-divergence, which is also implicitly stated in [31, Theorem 4.2], as given below.

**Lemma 2.2** ([21, Corollary 3.5]).: _The variational formula of \(f\)-divergence is_

\[_{}(P||Q)=_{g}_{ P}[g ()]-_{}\{_{ Q} [^{*}(g()+)]-\}\]

This variational representation is a "shift transformation" of Lemma 2.1 (i.e. \(g g+\)). Note that this representation shares the same optimal solution as Lemma 2.1 (clearly identified as the corresponding \(f\)-divergence), but Lemma 2.2 is considered tighter in the sense that the representation in Lemma 2.2 is flatter around the optimal solution.  provides a comprehensive study to justify this, and they also show that Lemma 2.2 can accelerate numerical estimation of \(f\)-divergences.

Here, to illustrate the advantage of Lemma 2.2, we use KL divergence as an example. Specifically, let \((x)=x x-x+1\), then its conjugate function is \(^{*}(y)=e^{y}-1\). Substituting \(^{*}\) into Lemma 2.1, we have

\[_{}(P||Q)=_{g}_{P} [g()]-_{Q}[e^{g()}-1]\] (1)

This representation is usually called LT-based KL. On the other hand, with the optimal \(^{*}=-_{Q}[e^{g()}]\), Lemma 2.2 will give us

\[_{}(P||Q)\!=\!_{g}_ {P}[g()]\!-\!_{}_{Q }[e^{g()+}]\!-\!1\!-\!}\!=\!_{g }_{P}[g()]\!-\!_{Q}[e ^{g()}]\!.\] (2)

Notice that Eq. (2) immediately recovers the DV representation of KL. Since \((x) x-1\) for \(x>0\), it is evident that, as a lower bound of KL divergence, Eq. (2) is pointwise tighter than Eq. (1). In Appendix C, we also show the variational representations of some other divergences obtained from Lemma 2.2.

In the context of UDA, it may be tempting to think that using a point-wise smaller quantity (in Lemma 2.1), as the key component of an upper bound for target error, is essentially desired. However,neither Lemma 2.1 nor Lemma 2.2 is able to directly give such an upper bound. To elaborate, as \(^{*}(x) x\) when \((1)=0\) (cf. Lemma B.2), both Lemma 2.1 and Lemma 2.2 imply that \(_{}(P||Q)_{g}_{P}[g()]- _{Q}[g()]\). Bearing this in mind, UDA typically requires an upper bound for the quantity \(_{h}_{}[ h(X)]-_{}[  h(X)]\), and simply restricting \(\) in Lemma 2.1 and Lemma 2.2 to a composition of \(\) and \(\) can only provide a lower bound for such a quantity.

Before we propose an improved discrepancy measure, we first revisit the absolute discrepancy in  by using Lemma 2.2 instead. This also serves as a review of the common developments in the DA theory.

## 3 Warm-Up: Refined Absolute \(f\)-Divergence Domain Discrepancy

Based on Lemma 2.2, we refine the discrepancy measure of [1, Definition 3] as follows.

**Definition 3.1**.: For a given \(h\), the \(}_{}^{h,}\) discrepancy from \(\) to \(\) is defined as

\[}_{}^{h,}(||)_ {h^{}}|_{}[(h,h^{}) ]-I_{,}^{h}( h^{})|,\]

where \(I_{,}^{h}( h^{})=_{}\{ _{}[^{*}((h,h^{})+)]-\}\).

**Remark 3.1**.: _Removing the absolute value function in \(}_{}^{h,}(||)\) does not alter its non-negativity. To see this, consider \(h^{}=h\). By the definitions of \(\) and \(^{*}\), \(_{}^{*}()-=(1)=0\), we have \(_{}[(h,h^{})]=I_{,}^{h}( h^ {})=0\). Consequently, since \(h\) exists in \(\), \(}_{}^{h,}(||) 0\) holds even without the absolute value function. In addition, due to the absolute value function, the relation between \(}_{}^{h,}(||)\) with the one in  is no longer clear._

While the absolute value function is not required for ensuring non-negativity, it is crucial for deriving the subsequent error bound for the target domain.

**Lemma 3.1**.: _Let \(^{*}=_{h^{*}}R_{}(h^{*})+R_{}(h^{*})\), then for any \(h\), we have_

\[R_{}(h) R_{}(h)+}_{}^{h,} (||)+^{*}.\]

**Remark 3.2** (Regarding \(^{*}\)).: _This error bound shares similarities with the previous works . For example, the third term \(^{*}\) is the ideal joint risk for the DA problem. As widely discussed in prior studies, this term captures the inherent challenge in the DA task and might be inevitable . However, it has also been pointed out that this \(^{*}\) term can be significantly pessimistic, particularly in the case of conditional shift . In fact, similar to the \(^{*}\)-free bound in [33, Theorem 4.1], it is a simple matter to replace \(^{*}\) with the cross-domain error term \(\{R_{}(f_{}),R_{}(f_{})\}\) in Lemma 3.1 (and all the other target error bounds in this paper). See Appendix D.3 for details. Given that  also uses \(^{*}\), we use \(^{*}\) in the bounds for consistency in the remainder of this paper._

In the sequel, we will give a Rademacher complexity based generalization bound for the target error. Let \(}_{S}()\) denote the empirical Rademacher complexity of function class \(=\{f:\}\) for some sample \(S\). Notice that a shift transformation of a function class will not change its Rademacher complexity, so the generalization bound based on \(}_{}^{h,}\) closely resembles the one presented in [1, Theorem 3], which contains a Lipschitz constant of \(^{*}\). Here, we give a generalization bound specialized for \(}_{}^{h,}\), wherein the Lipschitz constant of \(^{*}\) can be explicitly determined.

**Theorem 3.1**.: _Let \((,)\). Then, for any \(h\), with probability at least \(1-\), we have_

\[R_{}(h) R_{}(h)+}_{}^{h,}(||)+2e}_{}( ^{})+4}_{}(^{})+ ^{*}+(}+}),\]

_where \(^{}=\{x(h(x),h^{}(x))|h,h^{}\}\)._

 also applies Rademacher complexity-based bound to further upper bound \(^{*}\) by its empirical version, namely \(^{*}=_{h^{*}}R_{}(h^{*})+R_{ }(h^{*})\). However, since there is no target label available, \(R_{}(h^{*})\) is still not computable, invoking \(^{*}\) here has no clear advantage.

## 4 New \(f\)-Divergence-Based DA Theory

While \(}_{}^{h,}\) serves as a valid domain discrepancy measure in DA theory, it exaggerates the domain difference without appropriate control. It's noteworthy that  attempts to demonstrate their discrepancy with the absolute value function is upper bounded by \(_{}\)[1, Lemma 1], but this is problematic; as \( U 0\) does not imply \( U=|U|\) when \(U\) is not a positive function. Note that the error in [1, Lemma 1] is also identified in . Furthermore, when designing their \(f\)-DAL algorithm, they drop the absolute value function in their hypothesis-specified \(f\)-divergence (see Eq. (5)). Consequently, the remarkable performance of \(f\)-DAL reveals a significant gap from their theoretical foundation.

To bridge this gap, we introduce a new hypothesis-specific \(f\)-divergence-based DA framework. Our new discrepancy measure is dubbed \(f\)-domain discrepancy, or \(f\)-DD, defined without the absolute value function and with an affine transformation.

**Definition 4.1** (\(f\)-Dd).: For a given \(h\), the \(f\)-DD measure \(_{}^{h,}\) is defined as

\[_{}^{h,}(||)_{h^{} ,t}_{}[t(h,h^{}) ]-I_{,}^{h}(t h^{}),\]

where \(I_{,}^{h}(t h^{})=_{}\{_{} [^{*}(t(h,h^{})+)]-\}\).

**Remark 4.1**.: _If \(^{}=\), then \(_{}^{h,}(||)\) is an affine transformation of Lemma 2.1 (i.e. \(g tg+\)) and a scaling transformation of Lemma 2.2. Importantly, unlike \(}_{}^{h,}\), it's easy to see that \(_{}^{h,}(||)_{}(||)\)._

Our \(_{}^{h,}(||)\) retains some common properties of the discrepancies defined in the DA theory literature. First, as \(t=0\) leads to \(_{}[t(h,h^{})]=I_{,}^{h}(t  h^{})=0\), the non-negativity of \(_{}^{h,}(||)\) is immediately justified. In addition, its asymmetric property is also preferred in DA, as discussed in the previous works . Moreover, when \(=\), by the definition of \(^{*}\), we have \(_{}^{h,}(||)=0\).

To present an error bound, the routine development, as in Lemma 3.1, is insufficient; we require a general version of the "change of measure" inequality, as given below.

**Lemma 4.1**.: _Let \((x)(x+1)\), and \(^{*}\) is its convex conjugate. For any \(h^{},h\) and \(t\), define \(K_{h^{},}(t)_{}_{}[^{*}(t (h,h^{})+)]\). Let \(K_{}(t)=_{h^{}}K_{h^{},}(t)\), then for any \(h,h^{}\),_

\[K_{}^{*}(_{}[(h,h^{})]-_{ }[(h,h^{})])_{}^{h, }(||),\]

_where \(K_{}^{*}\) is the convex conjugate of \(K_{}\)._

It is worth reminding that \(K_{}\) and \(K_{}^{*}\) both depend on \(h\), although we ignore \(h\) in the notations to avoid cluttering. We are now ready to give a target error bound based on our \(f\)-DD.

**Theorem 4.1**.: _For any \(h\), we have_

\[R_{}(h) R_{}(h)+_{t 0}_{}^{h,}( ||)+K_{}(t)}{t}+^{*}.\] (3)

_Furthermore, let \(\), if \(\) is twice differentiable and \(^{}\) is monotone, then_

\[R_{}(h) R_{}(h)+(1)}_{ }^{h,}(||)}+^{*}.\] (4)

The following is an application of Eq. (4), where we consider the case of KL, namely \(_{}^{h,}(||)\).

**Corollary 4.1**.: _Let \(\), then for any \(h\), we have \(R_{}(h) R_{}(h)+_{}^{h,}(|| )}+^{*}\)._

As \(_{}^{h,}(||)_{} (||)\), the bound in [19, Theorem 4.2] can be recovered by Corollary 4.1 for the same bounded loss function. We also remark that the boundedness assumption in Corollary 4.1 can be further relaxed by applying the same sub-Gaussian assumption as in [19, Theorem 4.2].

To give a generalization bound, the next step involves obtaining a concentration result for \(f\)-DD, as given below.

**Lemma 4.2**.: _Let \(\) and let \(t_{0}\) be the optimal \(t\) achieving the superum in \(_{}^{h,}(||)\). Assume \(^{*}\) is \(L\)-Lipschitz, then for any given \(h\), with probability at least \(1-\), we have_

\[_{}^{h,}(||)_{}^{h,}(||)+2|t_{0}|}_{}( ^{})+2L|t_{0}|}_{}( ^{})+(}+}).\]

**Remark 4.2**.: _The function \(^{*}\) needs not to be globally Lipschitz continuous; it can be locally Lipschitz for a bounded domain. For example, in the case of KL, \(^{*}\) is e-Lipschitz continuous when \(\). Moreover, although the distribution-dependent quantity \(t_{0}\) might not always have a closed-form expression, in the case of \(^{2}\)-DD, we know that \(t_{0}=_{}[(h,h^{*})]-_{}[(h,h^{*})])}{_{}((h,h^{*}))}\), where \(h^{*}\) is the corresponding optimal hypothesis._

It is also straightforward to obtain concentration results for \(K_{}(t)-K_{}(t)\) and \(R_{}(h)-R_{}(h)\). Substituting these results into Eq. (3) obtains the final generalization bound based on \(f\)-DD. Or alternatively, one can directly substitute Lemma 4.2 into Eq. (4) (See Appendix D.7). However, in this case, the empirical \(f\)-DD and other terms in Lemma 4.2 will feature a square root, slowing down the convergence rate. We address this limitation in the following section.

## 5 Sharper Bounds via Localization

A localization technique in DA theory is recently studied in . We now incorporate it into our framework with some novel applications. First, we define a localized hypothesis space, formally referred to as the (true) Rashomon set [36; 37; 38].

**Definition 5.1** (Rashomon set).: Given a data distribution \(\), a hypothesis space \(\) and a loss function \(\), for a Rashomon parameter \(r 0\), the Rashomon set \(_{r}\) is an \(r\)-level subset of \(\) defined as: \(_{r}\{h|R_{}(h) r\}\).

Notice that the Rashomon set \(_{r}\) implicitly depends on the data distribution. In this paper, we specifically define \(_{r}\) by the source domain distribution \(\). Then, we define our localized \(f\)-DD:

**Definition 5.2** (Localized \(f\)-Dd).: For a given \(h_{r_{1}}\), the localized \(f\)-DD from \(\) to \(\) is defined as

\[_{}^{h,_{r}}(||)_{h^{} _{r},t 0}_{}[(h,h^{})]-I_{, }^{h}(t h^{}).\]

**Remark 5.1**.: _Compared with \(f\)-DD, localized \(f\)-DD restricts \(h\) to \(_{r_{1}}\) and \(h^{}\) to \(_{r}\), where \(r_{1}\) and \(r\) may or may not be equal. In addition, the scaling parameter \(t\) is now restricted to \(_{0}^{+}\) instead of \(\)._

Clearly, \(_{}^{h,_{r}}(||)\) is non-decreasing when \(r\) increases, and it is upper bounded by \(_{}^{h,}(||)\).

As also mentioned at the end of the previous section, Eq. (4) of Theorem 4.1 (and Corollary 4.1), involves a square root function for \(f\)-DD, potentially indicative of a slow-rate bound (e.g., if \(_{}^{h,}(1/n)\), then the bound decays with \((1/)\)). We now show that how the localized \(f\)-DD achieves a fast-rate error bound.

**Theorem 5.1**.: _For any \(h_{r_{1}}\) and constants \(C_{1},C_{2}(0,+)\) satisfying \(K_{h^{},}(C_{1}) C_{1}C_{2}_{}[(h,h^{})]\) for any \(h^{}_{r}\), the following holds:_

\[R_{}(h) R_{}(h)+}_{}^{h,_{r}} (||)+C_{2}R_{}^{r}(h)+_{r}^{*},\]

_where \(_{r}^{*}=_{h^{*}_{r}}R_{}(h^{*})+R_{}(h^{*})\) and \(R_{}^{r}(h)=_{h^{}_{r}}_{}[(h,h ^{})]\)._

**Remark 5.2**.: _By the triangle property, \(R_{}^{r}(h) r+r_{1}\). In this case, a small \(r_{1}\) will reduce both the first term and the third term in the bound. However, determining the optimal value for \(r\) is intricate. On the one hand, we hope \(r\) is small so that \(_{}^{h,_{r}}(||)\) and \(R_{}^{r}(h)\) are both small. On the other hand, if \(r\) is too small, specifically if \(r<^{*}\), then it's possible that \(_{r}^{*}>^{*}\) because the optimal hypothesis minimizing the joint risk may not exist in \(_{r}\). Additionally, if both \(r_{1}\) and \(r\) are too small, the effective space for optimizing \(C_{1}\) and \(C_{2}\) may also be limited. Therefore, the value of \(r\) involves a complex trade-off among the three terms._Overall, we expect \(r_{1}\) to be as small as possible, aligning with the principle of empirical risk minimization for the source domain in practice. We may let \(r>^{*}\) so that the optimal hypothesis is guaranteed to exist in the Rashomon set \(_{r}\). Furthermore, if \(r+r_{1}\) is unavoidably large, we prefer a small \(C_{2}\) so that \(C_{2}R_{}^{}(h)\) is small. If \(^{*}\) itself is negligible, we use a vanishing \(r+r_{1}\). In this case, one can focus on minimizing \(1/C_{1}\) while allowing \(C_{2}\) to be large.

Combining Theorem 5.1 with Lemma 4.2 and following routine steps will obtain the generalization bound based on \(f\)-DD, where the local Rademacher complexity  will be involved. However, one may feel unsatisfied without an explicit clue for the condition \(K_{h^{},}(C_{1}) C_{1}C_{2}_{}[(h,h^{ })]\) in Theorem 5.1. In fact, exploring concentration results under this condition is a central theme in obtaining fast-rate PAC-Bayesian generalization bounds  and the information-theoretic generalization bounds . Building upon similar ideas from these works, we now establish a sharper generalization result for our localized KL-DD measure, where the fast-rate condition is more explicit. One key ingredient is the following result.

**Lemma 5.1**.: _Let \(\), and let the constants \(C_{1}>0\) and \(C_{2}(0,1)\) satisfy the condition \((e^{C_{1}}-C_{1}-1)(1-\{r_{1}+r,1\}+C_{2}^{2}\{r_{1}+r,1\}) C_{1}C_{2}\). Then, for any \(h_{r_{1}}\) and \(h^{}_{r}\), we have_

\[_{}[(h,h^{})]_{C_{1},C_{2}}_{}^{h,_{r}}(||)}{C_{1}}+(1+C_{2}) _{}[(h,h^{})].\]

**Remark 5.3**.: _As an extreme case, if \(r+r_{1} 0\) (implying \(_{}[(h,h^{})]=0\)), then let \(C_{2} 1\), the condition in the lemma indicates that \(C_{1}<1.26\). Hence, the optimal bound becomes \(_{}[(h,h^{})] 0.79_{}^{h, _{r}}(||)\). This bound remains valid even without \(r+r_{1} 0\). It holds when the Rashomon set \(H_{r}\) is "consistent" with a given \(h\), meaning all hypotheses in \(H_{r}\) have similar predictions to \(h\) on the source domain data. As an another case, if \(r+r_{1} 1\) and \(_{h^{}}_{}[(h,h^{})]\) is also large, we may prefer a small \(C_{2}\), such as setting \(C_{2}=0.1\). In this case, the condition becomes \((e^{C_{1}}-C_{1}-1)C_{2} C_{1}\), suggesting that \(C_{1}<3.74\). This results in the optimal bound \(_{}[(h,h^{})]-_{}[(h, h^{})] 0.27_{}^{h,_{r}}(|| )+0.1_{}[(h,h^{})]\). The condition in Lemma 5.1 is common in many fast-rate bound literature, such as [28, Theorem 3]._

We are now in a position to give a fast-rate generalization bound for localized KL-DD.

**Theorem 5.2**.: _Under the conditions in Lemma 5.1. For any \(h_{r_{1}}\), with probability at least \(1-\), we have_

\[R_{}(h) R_{}(h)+_{}^{h,_{r}} (||)}{C_{1}}+C_{2}R_{}^{r}(h)+(}_{}(_{r}^{})+}_{}(_{\{r,r_{1}\}}^{}))\] \[+(+ {m})+(+r)(1/)}{n}}+})+_{r}^{*}.\]

**Remark 5.4**.: _Due to the non-negativity of \(f\)-DD, a similar generalization bound also applies to the Jeffereys divergence (or symmetrized KL divergence)  counterpart, which is simply the sum of KL divergence and reverse KL divergence (i.e. \(_{}(||)+_{}(||)\)). Furthermore, considering the fact that \(_{}(||)(1+^{2}(||))^{2}( ||)\), one might anticipate a similar bound for \(^{2}\)-DD, which we defer to Appendix D.11._

This generalization bound suggests that when \(r+r_{1}\) is small, not only are the first four terms (including the local Rademacher complexities) reduced, but it also causes \((+)\) to dominate the convergence rate of the bound. In practice, when empirical risk of the source domain is always minimized to zero (i.e. the realizable case), then \(r_{1}\) itself may have a fast vanishing rate (e.g., \((1/n)\)). In Appendix D.12, we provide a concrete example to further illustrate the superiority of localized \(f\)-DD.

## 6 Algorithms and Experimental Results

### Domain Adversarial Learning Algorithm

In a practical algorithm, the hypothesis space consists of two components: the representation part, denoted as \(_{}=\{h_{}:\}\), where \(\) is the representation space (e.g., the hidden output of a neural network), and the classification part, denoted as \(_{}=\{h_{}:\}\). Therefore, the entire hypothesis space is given by \(=\{h_{} h_{}|h_{} _{},h_{}_{}\}\). The training objective in \(f\)-DAL  is

\[_{h}_{h^{}^{}}R_{}(h) +_{,}(h,h^{}).\] (5)

Here, \(_{,}(h,h^{})=_{}[ (h,h^{})]-_{}[^{*}( (h,h^{}))]\), where \(\) is a trade-off parameter and \(\) is the surrogate loss used to evaluate the disagreement between \(h\) and \(h^{}\), which may or may not be the same as \(\). Note that, to better align with our framework, we change the order of \(\) and \(\) in \(_{,}\) in the original \(f\)-DAL. This modification is minor, as in either case, its optimal value belongs to \(f\)-divergence (such as KL and reverse KL, \(^{2}\) and reverse \(^{2}\)).

Eq. (5) results in an adversarial training strategy. Specifically, the outer optimization spans the entire hypothesis space. Meanwhile, within the inner optimization, given a \(h=h_{} h_{}\), the representation component \(h_{}\) is shared for \(h^{}\). In other words, the optimization is carried out for \(h^{}\) in \(^{}=_{} h_{} \{h_{} h_{}|h_{}_{ }\}\). The overall training framework of our \(f\)-DD is illustrated in Figure 2 in Appendix.

Clearly, as also discussed previously, \(_{h^{}}_{,}(h,h^{})_{h^{ }}|_{,}(h,h^{})|\), which presents a clear gap between the theory and algorithms in . In contrast, this training objective aligns more closely with our \(_{}^{h,}\). Formally, we have the following result.

**Proposition 1**.: _Let \(d_{,}(h,h^{})=_{}[(h, h^{})]-I_{,}^{h}( h^{})\). Assume \(\) is sufficiently large s.t. \(:\), we have \(_{h^{}}_{,}(h,h^{})=_{h^{} }_{,}(h,h^{})=_{}^{h,^{}}(||)_{}(||)\)._

In our algorithm, we use \(d_{,}(h,h^{})\) to replace \(_{,}(h,h^{})\) in Eq. (5). Proposition 1 implies that either the optimal \(_{,}(h,h^{})\) or the optimal \(d_{,}(h,h^{})\) coincides with \(f\)-DD. Moreover, as \(\) is typically unbounded in practice (e.g., cross-entropy loss), considering the unbounded nature of \(t\), Proposition 1 suggests an equivalence between optimizing \((h,h^{})\) through \(h^{}\) and optimizing \(t(h,h^{})\) through both \(t\) and \(h^{}\). In this sense, \(f\)-DAL has already considered the scaling transformation. Later on we will empirically investigate whether explicitly optimizing \(t\) is necessary.

Furthermore, we highlight that the training objective in our algorithm, namely Eq. (5) with \(_{,}(h,h^{})\) replaced by \(d_{,}(h,h^{})\), is de factotivated by the insights from Section 5. In that section, we demonstrate that when the source domain error is small, the \(f\)-DD term without the square-root function provides a more accurate reflection of generalization (see, for instance, Remark 5.3). Given that the empirical risk of the source domain is always minimized during training, we expect the final hypothesis to fall within the subset of \(\) (i.e. Rashomon set with \(r_{1}\)).

   Method & A \(\) W & D \(\) W & W \(\) D & A \(\) D & D \(\) A & W \(\) A & Avg \\  ResNet-50  & 68.4\(\)0.2 & 96.7\(\)0.1 & 99.3\(\)0.1 & 68.9\(\)0.2 & 62.5\(\)0.3 & 60.7\(\)0.3 & 76.1 \\ DANN  & 82.0\(\)0.4 & 96.9\(\)0.2 & 99.1\(\)0.1 & 79.7\(\)0.4 & 68.2\(\)0.4 & 67.4\(\)0.5 & 82.2 \\ MDD  & 94.5\(\)0.3 & 98.4\(\)0.1 & **100.0\(\)**0.0 & 93.5\(\)0.2 & 74.6\(\)0.3 & 72.2\(\)0.1 & 88.9 \\ KL  & 87.9\(\)0.4 & 99.0\(\)0.2 & **100.0\(\)**0.0 & 85.6\(\)0.6 & 70.1\(\)1.1 & 69.3\(\)0.7 & 85.3 \\ \(f\)-DAL  & **95.4\(\)**0.7 & 98.8\(\)0.1 & **100.0\(\)**0.0 & 93.8\(\)0.4 & 74.9\(\)1.5 & 74.2\(\)0.5 & 89.5 \\  Ours (KL-DD) & 94.9\(\)0.7 & 98.7\(\)0.1 & **100.0\(\)**0.0 & **95.9\(\)**0.6 & 74.6\(\)0.9 & 74.6\(\)0.7 & 89.8 \\ Ours (\(^{2}\)-DD) & 95.3\(\)0.2 & 98.7\(\)0.1 & **100.0\(\)**0.0 & 95.0\(\)0.4 & 73.7\(\)0.5 & **75.6\(\)**0.2 & 89.7 \\ Ours (Jeffreys-DD) & 94.9\(\)0.7 & **99.1 \(\)**0.2 & **100.0\(\)**0.0 & **95.9\(\)**0.6 & **76.0\(\)**0.5 & 74.6\(\)0.7 & **90.1** \\   

Table 1: Accuracy (%) on the Office-31 benchmark.

[MISSING_PAGE_FAIL:9]

tigher variational representation-based discrepancy reveals that \(^{2}\) is no longer superior to KL. In fact, our KL-DD slightly outperforms \(^{2}\)-DD in all three benchmarks.

Failure of Absolute DiscrepancyWe also perform experiments on \(\)\(\)\(\) and \(\)\(\)\(\) using the absolute discrepancy (i.e., \(|_{,}(h,h^{})|\)). Specifically, we compare the \(^{2}\)-based discrepancy with (w/) and without (w/o) the absolute value function. Figure 1 illustrates that such a discrepancy can easily explode during training, demonstrating its tendency to overestimate \(f\)-divergence. Additional results for KL are given in Figure 3 in Appendix.

Optimizing over \(t\)In the paragraph following Proposition 1, we discuss the observation that optimizing over \(t\) may not be necessary. Empirical evidence indicates that setting \(t=1\) with hyper-parameter tuning (e.g., through \(\)) obtains satisfactory performance. Now, let's investigate the selection of \(t\) for KL-DD. Instead of using a stochastic gradient-based optimizer for updating \(t\), we invoke a quadratic approximation for the optimal \(t\), as studied in . Specifically, we define a Gibbs measure \(d^{}=)}d}{_{}[e^{( h,h^{})}]}\), then the optimal \(t^{*} 1+ t^{*}\), where \( t^{*}=_{}[(h,h^{})]- _{^{}}[(h,h^{})]}{_{^{}} ((h,h^{}))}\). Interested readers can find a detailed derivation of this approximation in [23, Appendix B]. Substituting \(t=1+ t^{*}\), we have the training objective for approximately optimal KL-DD (OptKL-DD). Table 4 presents an empirical comparison between OptKL-DD and the original KL-DD, where \(t\) is simply set to 1. The results indicate that OptKL-DD does not provide any improvement on these benchmarks. Similar observations also hold for \(^{2}\), in which case optimal \(t\) has an analytic form (see Appendix E), suggesting that using \(t=1\), at least for KL and \(^{2}\), might be sufficient in practice.

Additional experimental results, including an ablation study on \(\), t-SNE  visualizations and other comparisons, can be found in Appendix E.

## 7 Other Related Works

Domain AdaptationApart from those mentioned in the introduction, various other discrepancy measures are explored in DA theories and algorithms. These include the Wasserstein distance , Maximum Mean Discrepancy , second-order statistics alignment , transfer exponents , Integral Probability Metrics  and so on. Notably,  defines a general Integral Probability Metrics (IPMs)-based discrepancy measure. It's noteworthy that the intersection of the IPMs family and the \(f\)-divergence family results in the total variation. Consequently, both corresponding discrepancy measures can consider \(\)-divergence as a special case. Additionally, one of our baseline models  diverges from the adversarial training strategy. Instead, they directly minimize the KL divergence between two isotropic Gaussian distributions (source domain Gaussian and target domain Gaussian) in the representation space. Here, the Gaussian means and variances correspond to the hidden outputs of the representation network. For further literature on DA theory, readers are directed to a recent survey by .

\(f\)-divergenceMoreover, the combination of \(f\)-divergence and adversarial training schemes has been extensively studied in generative models, including \(f\)-GAN , \(^{2}\)-GAN  and others. In the DA context,  introduce a \(f\)-divergence-based discrepancy measure while still relying on Lemma 2.1 and focusing solely on the Jensen-Shannon case. Additionally,  investigates \(\)-Renyi divergence for multi-source DA, and  provides some intriguing interpretations of \(^{2}\)-divergence-based generalization bound for covariate shifts.

## 8 Conclusion and Future Work

In this work, we present an improved approach for integrating \(f\)-divergence into DA theory. Theoretical contributions include novel DA generalization bounds, including fast-rate bounds via localization. On the practical front, the revised \(f\)-divergence-based discrepancy improves the benchmark performance. Several promising future directions emerge from our work. Firstly, beyond its usefulness for local Rademacher complexity, the Rashomon set \(_{r}\) also relates to another generalization measure, Rashomon ratio , which may give an alternative perspective on generalization in DA. Additionally, exploring transfer component-based analysis  for tight minimax rates in DA, invoking a power transformation instead of the affine transformation in \(f\)-DD, holds promise.