# GeNIe: Generative Hard Negative Images

Through Diffusion

Anonymous Author(s)

Affiliation

Address

email

###### Abstract

Data augmentation is crucial in training deep models, preventing them from overfitting to limited data. Recent advances in generative AI, e.g., diffusion models, have enabled more sophisticated augmentation techniques that produce data resembling natural images. We introduce GeNIe a novel augmentation method which leverages a latent diffusion model conditioned on a text prompt to combine two contrasting data points (an image from the source category and a text prompt from the target category) to generate challenging augmentations. To achieve this, we adjust the noise level (equivalently, number of diffusion iterations) to ensure the generated image retains low-level and background features from the source image while representing the target category, resulting in a _hard negative_ sample for the source category. We further automate and enhance GeNIe by adaptively adjusting the noise level selection on a per image basis (coined as GeNIe-Ada), leading to further performance improvements. Our extensive experiments, in both few-shot and long-tail distribution settings, demonstrate the effectiveness of our novel augmentation method and its superior performance over the prior art.

## 1 Introduction

Augmentation has become an integral part of training deep learning models, particularly when faced with limited training data. For instance, when it comes to image classification with limited number of samples per class, model generalization ability can be significantly hindered. Simple transformations like rotation, cropping, and adjustments in brightness artificially diversify the training set, offering the model a more comprehensive grasp of potential data variations. Hence, augmentation can serve as a practical strategy to boost the model's learning capacity, minimizing the risk of overfitting and facilitating effective knowledge transfer from limited labelled data to real-world scenarios. Various image augmentation methods, encompassing standard transformations, and learning-based approaches have been proposed [16; 15; 110; 111; 100]. Some augmentation strategies combine two images possibly from two different categories to generate a new sample image. The simplest ones in this category are MixUp  and CutMix  where two images are combined in the pixel space. However, the resulting augmentations often do not lie within the manifold of natural images and act as out-of-distribution samples that will not be encountered during testing.

Recently, leveraging generative models for data augmentation has gained an upsurge of attention [100; 83; 63; 35]. These interesting studies, either based on fine-tuning or prompt engineering of diffusion models, are mostly focused on generating _generic augmentations_ without considering the impact of other classes and incorporating that information into the generative process for a classification context. We take a different approach to generate challenging augmentations near the decision boundaries of a downstream classifier. Inspired by diffusion-based image editing methods [67; 63] some of which are previously used for data augmentation, we propose to use conditional latent diffusion models  for generating _hard negative_ images. Our core idea (coined as GeNIe) is to sample source images from various categories and prompt the diffusion model with a contradictory text corresponding to a different target category. We demonstrate that the choice of noise level (or equivalently number of iterations) for the diffusion process plays a pivotal role in generating images that semantically belong to the target category while retaining low-level features from the source image. We argue that these generated samples serve as _hard negatives_ for the source category (or from a dual perspective hard positives for the target category). To further enhance GeNIe, we propose an adaptive noise level selection strategy (dubbed as GeNIe-Ada) enabling it to adjust noise levels automatically per sample.

To establish the impact of GeNIe, we focus on two challenging scenarios: _long-tail_ and _few-shot_ settings. In real-world applications, data often follows a long-tail distribution, where common scenarios dominate and rare occurrences are underrepresented. For instance, a person jaywalking a highway causes models to struggle with such unusual scenarios. Combating such a bias or lack of sufficient data samples during model training is essential in building robust models for self-driving cars or surveillance systems, to name a few. Same challenge arises in few-shot learning settings where the model has to learn from only a handful of samples. Our extensive quantitative and qualitative experimentation, on a suite of few-shot and long-tail distribution settings, corroborate the effectiveness of the proposed novel augmentation method (GeNIe, GeNIe-Ada) in generating hard negatives, corroborating its significant impact on categories with a limited number of samples. A high-level sketch of GeNIe is illustrated in Fig. 1. Our main contributions are summarized below:

* We introduce GeNIe, a novel yet elegantly simple diffusion-based augmentation method to create challenging augmentations in the manifold of natural images. For the first time, to our best knowledge, GeNIe achieves this by combining two sources of information (a source image, and a contradictory target prompt) through a noise-level adjustment mechanism.
* We further extend GeNIe by automating the noise-level adjustment strategy on a per-sample basis (called GeNIe-Ada), to enable generating hard negative samples in the context of image classification, leading also to further performance enhancement.
* To substantiate the impact of GeNIe, we present a suit of quantitative and qualitative results including extensive experimentation on two challenging tasks: few-shot and long tail distribution settings corroborating that GeNIe (and its extension GeNIe-Ada) significantly improve the downstream classification performance.

## 2 Related Work

**Data Augmentations.** Simple flipping, cropping, colour jittering, and blurring are some forms of image augmentations . These augmentations are commonly adopted in training deep learning models. However, using these data augmentations is not trivial in some domains. For example, using blurring might remove important low-level information from medical images. More advanced

Figure 1: **Generative Hard Negative Images Through Diffusion (GeNIe):** generates hard negative images that belong to the target category but are similar to the source image from low-level feature and contextual perspectives. GeNIe starts from a source image passing it through a partial noise addition process, and conditioning it on a different target category. By controlling the amount of noise, the reverse latent diffusion process generates images that serve as _hard negatives_ for the source category.

approaches, such as MixUp  and CutMix , mix images and their labels accordingly . However, the resulting augmentations are not natural images anymore, and thus, act as out-of-distribution samples that will not be seen at test time. Another strand of research tailors the augmentation strategy through a learning process to fit the training data . Unlike the above methods, we propose to utilize pre-trained latent diffusion models to generate hard negatives (in contrast to generic augmentations) through a noise adaptation strategy discussed in Section 3.

**Data Augmentation with Generative Models.** Using synthesized images from generative models to augment training data has been studied before in many domains , including domain adaptation , visual alignment , and mitigation of dataset bias . For example,  introduces a methodology aimed at enhancing test set evaluation through augmentation. While previous methods predominantly relied on GANs  as the generative model, more recent studies promote using diffusion models to augment the data . More specifically,  study the effectiveness of text-to-image diffusion models in data augmentation by diversification of each class with synthetic images.  leverages a text-to-image diffusion model and fine-tunes it on the downstream dataset using textual-inversion  to increase the diversity of existing samples.  also utilizes a text-to-image diffusion model, but with a BLIP  model to generate meaningful captions from the existing images.  utilizes diffusion models for augmentation to correct model mistakes.  uses CLIP  to filter generated images.  utilizes text-based diffusion and a large language model (LLM) to diversify the training data.  uses an LLM to generate text descriptions of failure modes associated with spurious correlations, which are then used to generate synthetic data through generative models. The challenge here is that the LLM has little understanding of such failure scenarios and contexts.

We take a completely different approach here, without replying on any extra source of information (e.g., through an LLM). Inspired by image editing approaches such as Boomerang  and SDEdit , we propose to adaptively guide a latent diffusion model to generate _hard negatives_ images  on a per-sample basis per category. In a nutshell, the aforementioned studies focus on improving the diversity of each class with effective prompts and diffusion models, however, we focus on generating effective _hard negative_ samples for each class by combining two sources of contradicting information (images from the source category and text prompt from the target category).

**Language Guided Recognition Models.** Vision-Language foundation models (VLMs)  utilize human language to guide the generation of images or to extract features from images that are aligned with human language. For example, CLIP  shows decent zero-shot performance on many downstream tasks by matching images to their text descriptions. Some recent works improve the utilization of human language in the prompt , and others use a diffusion model directly as a classifier . Similar to the above, we use a foundation model (Stable Diffusion 1.5 ) to improve the downstream task. Concretely, we utilize category names of the downstream tasks to augment their associate training data with hard negative samples.

**Few-Shot Learning.** In Few-shot Learning (FSL), we pre-train a model with abundant data to learn a rich representation, then fine-tune it on new tasks with only a few available samples. In supervised FSL , pretraining is done on a labeled dataset, whereas in unsupervised FSL  the pre-training has to be conducted on an unlabeled dataset. We assess the impact of GeNIe on a number of few-shot scenarios and state-of-the-art baselines by accentuating on its impact on the few-shot inference stage.

## 3 Proposed Method: GeNIe

Given a source image \(X_{S}\) from category S = \(<\)source category\(>\), we are interested in generating a target image \(X_{r}\) from category \(T=<\)target category\(>\). In doing so, we intend to ensure the low-level visual features or background context of the source image are preserved, so that we generate samples that would serve as _hard negatives_ for the _source_ image. To this aim, we adopt a conditional latent diffusion model (such as Stable Diffusion, ) conditioned on a text prompt of the following format "A photo of a \(T=<\)target category\(>\)".

**Key Idea.** GeNIe in its basic form is a simple yet effective augmentation sample generator for improving a classifier \(f_{}(.)\) with the following two key aspects: (i) inspired by  instead of adding the full amount of noise \(_{max}\) and going through all \(N_{max}\) (being typically \(50\)) steps of denoising, we use less amount of noise (\(r_{max}\), with \(r(0,1)\)) and consequently fewer number of denoising iterations (\( rN_{max}\)); (ii) we prompt the diffusion model with a \(P\) mandating a target category \(T\) different than the source \(S\). Hence, we denote the conditional diffusion process as \(X_{r}=(X_{S},P,r)\). In such a construct, the proximity of the final decoded image \(X_{r}\) to the source image \(X_{S}\) or the target category defined through the text prompt \(P\) depends on \(r\). Hence, by controlling the amount of noise, we can generate images that blend characteristics of both the text prompt \(P\) and the source image \(X_{S}\). If we do not provide much of visual details in the text prompt (e.g., desired background, etc.), we expect the decoded image \(X_{r}\) to follow the details of \(X_{S}\) while reflecting the semantics of the text prompt \(P\). We argue, and demonstrate later, that the newly generated samples can serve as _hard negative_ examples for the source category \(S\) since they share the low-level features of \(X_{S}\) while representing the semantics of the target category, \(T\). Notably, the source category \(S\) can be randomly sampled or be carefully extracted from the confusion matrix of \(f_{}(.)\) based on real training data. The latter might result in even _harder negative_ samples being now cognizant of model confusions. Finally, we will append our initial dataset with the newly generated hard negative samples through GeNIe and (re)train the classifier model.

**Enhancing**GeNIe**: **GeNIe**-**Ada.** One of the remarkable aspects of GeNIe lies in its simple application, requiring only \(X_{S}\), \(P\), and \(r\). However, selecting the appropriate value for \(r\) poses a challenge as it profoundly influences the outcome. When \(r\) is small, the resulting \(X_{r}\) tends to closely resemble \(X_{S}\), and conversely, when \(r\) is large (closer to \(1\)), it tends to resemble the semantics of the target category. This phenomenon arises because a smaller noise level restricts the capacity of the diffusion model to deviate from the semantics of the input \(X_{S}\). Thus, a critical question emerges: how can we select \(r\) for a particular source image to generate samples that preserve the low-level semantics of the source category \(S\) in \(X_{S}\) while effectively representing the semantics of the target category \(T\)? We propose a method to determine an ideal value for \(r\).

Our intuition suggests that by varying the noise ratio \(r\) from \(0\) to \(1\), \(X_{r}\) will progressively resemble category \(S\) in the beginning and category \(T\) towards the end. However, somewhere between \(0\) and \(1\), \(X_{r}\) will undergo a rapid transition from category \(S\) to \(T\). This phenomenon is empirically observed in our experiments with varying \(r\), as depicted in Fig. 2. Although the exact reason for this rapid change remains uncertain, one possible explanation is that the intermediate points between two categories reside far from the natural image manifold, thus, challenging the diffusion model's capability to generate them. Ideally, we should select \(r\) corresponding to just after this rapid semantic transition, as at this point, \(X_{r}\) exhibits the highest similarity to the source image while belonging to the target category.

We propose to trace the semantic trajectory between \(X_{S}\) and \(X_{T}\) through the lens of the classifier \(f_{}(.)\). As shown in Algorithm 1, assuming access to the classifier backbone \(f_{}(.)\) and at least one example \(X_{T}\) from the target category, we convert both \(X_{S}\) and \(X_{T}\) into their respective latent vectors \(Z_{S}\) and \(Z_{T}\) by passing them through \(f_{}(.)\). Then, we sample \(M\) values for \(r\) uniformly distributed \((0,1)\), generating their corresponding \(X_{r}\) and their latent vectors \(Z_{r}\) for all those \(r\). Subsequently, we calculate \(d_{r}=-Z_{S})^{T}(Z_{T}-Z_{S})}{||Z_{T}-Z_{S}||_{2}}\) as the distance between \(Z_{r}\) and \(Z_{S}\) projected onto the vector connecting \(Z_{S}\) and \(Z_{T}\). Our hypothesis posits that the rapid semantic transition corresponds to a sharp change in this projected distance. Therefore, we sample \(n\) values for \(r\) uniformly distributed

Figure 2: **Effect of noise ratio, \(r\), in GeNIe:** we employ GeNIe to generate augmentations for the target classes (motorcycle and cat) with varying \(r\). Smaller \(r\) yields images closely resembling the source semantics, creating an inconsistency with the intended target label. By tracing \(r\) from \(0\) to \(1\), augmentations gradually transition from source image characteristics to the target category. However, a distinct shift from the source to the target occurs at a specific \(r\) that may vary for different source images or target categories. For more examples, please refer to Fig. A4.

between \(0\) and \(1\), and analyze the variations in \(d_{r}\). We identify the largest gap in \(d_{r}\) and select the \(r\) value just after the gap when increasing \(r\), as detailed in Algorithm 1 and illustrated in Fig. 3.

```
0:\(X_{S},X_{T},f_{}(.),(.),M\)
0: Extract \(Z_{S} f_{}(X_{s}),Z_{T} f_{}(X_{T})\) for\(m\{1,M\}\)do
1:\(r,Z_{r} f_{}((X,P,r))\) \(d_{m}-Z_{S})^{T}(Z_{T}-Z_{S})}{\|Z_{T}-Z_{S}\|_{2}}\) \(m^{*}*{argmax}_{m}|d_{m}-d_{m-1}|, m[2,M]\) \(r^{*}}{n}\) Return:\(X_{r^{*}}=(X_{S},P,r^{*})\) ```

**Algorithm 1**GeNle-Ada

## 4 Experiments

Since the impact of augmentation is more pronounced when the training data is limited, we evaluate the impact of GeNle on Few-Shot classification in Section 4.1, Long-Tailed classification in Section 4.2, and fine-grained classification in Section A.2. For GeNle-Ada in all scenarios, we utilize GeNIe to generate augmentations from the noise level set \(\{0.5,0.6,0.7,0.8,0.9\}\). The selection of the appropriate noise level per source image and target is adaptive, achieved through Algorithm 1.

Baselines.We use Stable Diffusion 1.5  as our base diffusion model. In all settings, we use the same prompt format to generate images for the target class: i.e., "A photo of a \(<\)target category\(>\)", where we replace the target category with the target category label. We generate \(512 512\) images for all methods. For fairness in comparison, we generate the same number of new images for each class. We use a single NVIDIA RTX \(3090\) for image generation. We consider \(4\) diffusion-based baselines and a suite of traditional data augmentation baselines:

```
0:\(X_{S},X_{T},f_{}(.),(.),M\)
0: Extract \(Z_{S} f_{}(X_{s}),Z_{T} f_{}(X_{T})\) for\(m\{1,M\}\)do
1:\(r,Z_{r} f_{}((X,P,r))\) \(d_{m}-Z_{S})^{T}(Z_{T}-Z_{S})}{\|Z_{T}-Z_{S}\|_{2}}\) \(m^{*}*{argmax}_{m}|d_{m}-d_{m-1}|, m[2,M]\) \(r^{*}}{n}\) Return:\(X_{r^{*}}=(X_{S},P,r^{*})\) ```

**Algorithm 2**GeNIe-Ada

In fig. 4 illustrates a set of generated augmentation examples for Txt2Img, Img2Img, and GeNIe.

DAFusion : In this method, an embedding is optimized with a set of images for each class to correspond to the classes in the dataset. This approach is introduced in Textual Inversion . We optimize an embedding for 5000 iterations for each class in the dataset, followed by augmentation similar as the DAFusion method.

Cap2Aug: It is a recent diffusion-based data augmentation strategy that uses image captions as text prompts for an image-to-image diffusion model.

Traditional Data Augmentation:We consider both weak and strong traditional augmentations. More specifically, for weak augmentation we use random resize crop with scaling \([0.2,1.0]\) and horizontal flipping. For strong augmentation, we consider random color jitter, random grayscale, and Gaussian blur. For the sake of completeness, we also compare against data augmentations such as CutMix  and MixUp  that combine two images together.

### Few-shot Classification

We assess the impact of GeNIe compared to other augmentations in a number of few-shot classification (FSL) scenarios, where the model has to learn only from the samples contained in the (\(N\)-way, \(K\)-shot) support set and infer on the query set. Note that this corresponds to an inference-only FSL

Figure 3: GeNIe-Ada: To choose \(r\) adaptively for each (source image, target category) pair, we propose tracing the semantic trajectory from \(Z_{S}\) (source image embeddings) to \(Z_{T}\) (target embeddings) through the lens of the classifier \(f_{}()\) (Algorithm 1). We adaptively select the sample right after the largest semantic shift.

setting where a pretraining stage on an abundant dataset is discarded. The goal is to assess how well the model can benefit from the augmentations while keeping the original \(N K\) samples intact.

**Datasets.** We conduct our few-shot experiments on two most commonly adopted few-shot classification datasets: _mini_-Imagenet  and _tiered_-Imagenet . _mini_-Imagenet is a subset of ImageNet  for few-shot classification. It contains \(100\) classes with \(600\) samples each. We follow the predominantly adopted settings of [79; 10] where we split the entire dataset into \(64\) classes for training, \(16\) for validation and \(20\) for testing. _tiered_-Imagenet is a larger subset of ImageNet with \(608\) classes and a total of \(779,165\) images, which are grouped into \(34\) higher-level nodes in the _ImageNet_ human-curated hierarchy. This set of nodes is partitioned into \(20\), \(6\), and \(8\) disjoint sets of training, validation, and testing nodes, and the corresponding classes form the respective meta-sets.

**Evaluation.** To quantify the impact of different augmentation methods, we evaluate the test-set accuracies of a state-of-the-art unsupervised few-shot learning method with GeNIe and compare them against the accuracies obtained using other augmentation methods. Specifically, we use UniSiam  pre-trained with ResNet-18, ResNet-34 and ResNet-50 backbones and follow its evaluation strategy of fine-tuning a logistic regressor to perform (\(N\)-way, \(K\)-shot) classification on the test sets of _mini_- and _tiered_-Imagenet. Following , an episode consists of a labeled support-set and an unlabelled query-set. The support-set contains \(N\) randomly sampled classes where each class contains \(K\) samples, whereas the query-set contains \(Q\) randomly sampled unlabeled images per class. We conduct our experiments on the two most commonly adopted settings: (\(5\)-way, \(1\)-shot) and (\(5\)-way, \(5\)-shot) classification settings. Following the literature, we sample \(16\)-shots per class for the query set in both settings. We report the test accuracies along with the \(95\%\) confidence interval over \(600\) and \(1000\) episodes for _mini_-ImageNet and _tiered_-ImageNet, respectively.

**Implementation Details:**GeNIe generates augmented images for each class using images from all other classes as the source image. We use \(r=0.8\) in our experiments. We generate \(4\) samples per class as augmentations in the \(5\)-way, \(1\)-shot setting and \(20\) samples per class as augmentations in the \(5\)-way, \(5\)-shot setting. For the sake of a fair comparison, we ensure that the total number of labelled samples in the support set after augmentation remains the same across all different traditional and generative augmentation methodologies. Due to the expensive training of embeddings for each class in each episode, we only evaluated the DA-Fusion baseline on the first 100 episodes.

**Results:** The results on _mini_-Imagenet and _tiered_-Imagenet for both (\(5\)-way, \(1\) and \(5\)-shot) settings are summarized in Table 1 and Table 2, respectively. Regardless of the choice of backbone, we observe that GeNIe helps consistently improve UniSiam's performance and outperform other supervised and unsupervised few-shot classification methods as well as other diffusion-based [100; 63; 82; 35] and classical [110; 111] data augmentation techniques on both datasets, across both (\(5\)-way, \(1\) and \(5\)-shot) settings. Our noise adaptive method of selecting optimal augmentations per source image (GeNIe-Ada) further improves GeNIe's performance across all three backbones, both

Figure 4: **Visualization of Generative Samples:** We compare GeNIe with two baselines: Img2Img\({}^{L}\)**augmentation**: both image and text prompt are from the same category. Adding noise does not change the image much, so they are not hard examples. Txt2Img **augmentation:** We simply use the text prompt only to generate an image for the desired category (e.g., using a text2image method). Such images may be far from the domain of our task since the generation is not informed by any visual data from our task. GeNIe **augmentation:** We use the target category name in the text prompt only along with the source image.

few-shot settings, and both datasets (_mini_ and _tiered_-Imagenet). Few-shot accuracies for ResNet-34 computed on _tiered_Imagenet are reported in Section A.3 of the appendix. Note that employing CutMix and MixUp seems to lead to performance degradation compared to weak augmentations, probably due to overfitting since these methods can only choose from \(4\) other classes to mix.

### Long-Tailed Classification

We evaluate our method on long-tailed data, where the number of instances per class is unbalanced, with most categories having limited samples (tail). Our goal is to mitigate this bias by augmenting the tail of the distribution with generated samples. We evaluate GeNIe using two different backbones and methods: the ViT architecture with LViT , and ResNet50 with VL-LTR .

Following LViT , we first train an MAE  and ViT on the unbalanced dataset without any augmentation. Next, we train the Balanced Fine-Tuning stage of LViT by incorporating the augmentation data generated using GeNIe or other baselines. For ResNet50, we use VL-LTR code to fine-tune the CLIP  ResNet50 pretrained backbone with generated augmentations by GeNIe.

**Dataset:** We perform experiments on ImageNet-LT . It contains \(115.8\)K images from \(1,000\) categories. The number of images per class varies from \(1280\) to \(5\). Imagenet-LT classes can be divided into \(3\) groups: "Few" with less than \(20\) images, "Med" with \(20-100\) images, and "Many" with more than \(100\) images. Imagenet-LT uses the same validation set as ImageNet. We augment "Few" categories only and limit the number of generated images to \(50\) samples per class. For GeNIe, instead of randomly sampling the source images from other classes, we use a confusion matrix on the training data to find the top-\(4\) most confused classes and only consider those classes for random sampling of the source image. The source category may be from "Many", "Med", or "Few sets".

**Results:** Augmenting training data with GeNIe-Ada improves accuracy on the "Few" set by \(11.7\%\) and \(4.4\%\) compared with LViT only and LViT with Txt2Img augmentation baselines respectively. In ResNet50, GeNIe-Ada outperforms Cap2Aug baseline in "Few" categories by \(7.6\%\). The results are summarized in Table 3. Please refer to Section A.4 for implementation details.

### Ablation and Analysis

**Semantic Shift from Source to Target Class.** The core motivation behind GeNIe-Ada is that by varying the noise ratio \(r\) from \(0\) to \(1\), augmented sample \(X_{r}\) will progressively shift its semantic category from source (\(S\)) in the beginning to target category (\(T\)) towards the end. However, somewhere between \(0\) and \(1\), \(X_{r}\) will undergo a rapid transition from \(S\) to \(T\). To demonstrate this hypothesis empirically, in Figs. 5 and A5, we visualize pairs of source images and target categories with their respective GeNIe generated augmentations for different noise ratios \(r\), along with their corresponding

    \\ 
**Augmentation** & **Method** & **Pre-training** & **1-shot** & **5-shot** \\  - & [bCohseNet  & sup. & 51.9\(\)0.7 & 74.6\(\)0.7 \\ - & Robust + dis  & sup. & 63.7\(\)0.6 & 81.20\(\)0.4 \\ - & AFIN  & sup. & 62.4\(\)0.7 & 78.20\(\)0.6 \\ - & [bCohseNet  & sup.+1 & 62.4\(\)0.7 & 78.20\(\)0.6 \\ Work & PreResNet-Ssl. & sup.+1 & 62.4\(\)0.7 & 78.20\(\)0.6 \\ Week & PreResNet-Ssl. & sup.+1 & 62.4\(\)0.7 & 78.20\(\)0.6 \\ Week & PreResNet-Ssl. & sup.+1 & 62.4\(\)0.7 & 62.4\(\)0.7 \\ Week & Neg-Coinin  & sup. & 62.3\(\)0.8 & 80.9\(\)0.6 \\ - & [bCohseNet  & sup.+0.7 & 78.0\(\)0.7 \\ - & [bCohseNet  & sup.+6 & 77.3\(\)0.6 \\ Week & SpeNet  & sup.+5 & 79.0\(\)0.8 & 76.7\(\)0.6 \\ Week & SpeNet  & sup.+5 & 79.5\(\)0.7 & 77.40\(\)0.5 \\  Week & LViTRA  & unsup. & 43.1\(\)0.4 & 53.4\(\)0.3 \\ Week & PreResNet  & unsup. & 50.9\(\)0.4 & 71.6\(\)0.3 \\ Week & SimCLR  & unsup. & 62.6\(\)0.4 & 79.7\(\)0.3 \\ Week & SimCLR  & unsup. & 62.6\(\)0.4 & 79.7\(\)0.3 \\ Week & SimCLR  & unsup. & 62.1\(\)0.4 & 79.7\(\)0.3 \\ Week & SimCLR  & unsup. & 62.4\(\)0.7 & 78.9\(\)0.3 \\ Week & SimCLR  & unsup. & **63.1\(\)0.5** \\ Week & SimCLR  & unsup. & **64.1\(\)0.4** & **82.3\(\)0.3** \\ Week & UnSimCLR  & unsup. & 63.1\(\)0.8 & 81.4\(\)0.5 \\ Window & VisCLR  & unsup. & 63.2\(\)0.8 & 81.2\(\)0.6 \\ Window & VisCLR  & unsup. & 62.7\(\)0.8 & 80.6\(\)0.6 \\ Week & UnSimCLR  & unsup. & 62.4\(\)0.8 & 80.7\(\)0.6 \\ Week & UnSimCLR  & unsup. & 63.9\(\)0.8 & 82.1\(\)0.5 \\ Week & UnSimCLR  & unsup. & 64.0\(\)0.7 & 64.0\(\)0.5 \\ Week & UnSimCLR  & unsup. & 64.1\(\)0.8 & 75.1\(\)0.5 \\ Week & UnSimCLR  & unsup. & 64.1\(\)0.7 & 53.2\(\)0.5 \\ Week & UnSimCLR  & unsup. & 64.3\(\)0.8 & 83.1\(\)0.5 \\ Week & UnSimCLR  & unsup. & 64.3\(\)0.8 & 83.1\(\)0.5 \\ Week & UnSimCLR  & unsup. & 63.9\(\)0.8 & 82.1\(\)0.5 \\   & UnSimCLR  & unsup. & 69.1\(\)0.7 & 60.4\(\)0.5 \\ Week & UnSimCLR  & unsup. & 69.1\(\)0.7 & 60.4\(\)0.5 \\ Week & UnSimCLR  & unsup. & 69.1\(\)0.7 & 60.4\(\)0.5 \\ Week & UnSimCLR  & unsup. & 74.1\(\)0.6 & 84.0\(\)0.5 \\ Week & UnSimCLR  & unsup. & 64.3\(\)0.7 & 60.4\(\)0.5 \\ Week & UnSimCLR  & unsup. & 64.3\(\)0.8 & 83.2\(\)0.5 \\ Week & UnSimCLR  & unsup. & 63.9\(\)0.8 & 82.1\(\)0.5 \\   & UnSimCLR  & unsup. & 69.

PCA-projected embedding scatter plots (on the far left). We extract embeddings for all the images using a DINOV2 ViT-G pretrained backbone, which we assume as an oracle model in identifying the right category. We observe that as \(r\) increases from \(0.3\) to \(0.8\), the images transition to embody more of the target category's semantics while preserving the contextual features of the source image. This transition of semantics can also be observed in the embedding plots (on the left) where they consistently shift from the proximity of the source image (blue star) to the target class's centroid (red cross) as the noise ratio \(r\) increases. The sparse distribution of points within \(r=[0.4,0.6]\) for the first image and \(r=[0.2,0.4]\) for the second image aligns with our intuition of a rapid transition from category \(S\) to \(T\), thus empirically affirming our motivation behind GeNIe-Ada.

To further establish this, in Fig. 6, we demonstrate the efficacy of GeNIe in generating hard negatives at the decision boundaries of an SVM classifier, which is trained on the labelled support set of the few-shot tasks of _mini_-Imagenet, without any augmentations. We then plot source and target class probabilities (\(P(Y_{S}|X_{r})\) and \(P(Y_{T}|X_{r})\), respectively) of the generated augmentation samples \(X_{r}\). For both \(r=0.6\) and \(0.7\), there is significant overlap between \(P(Y_{S}|X_{r})\) and \(P(Y_{T}|X_{r})\), making it difficult for the classifier to decide the correct class. On the right-hand-side, GeNIe-Ada automatically selects the best \(r\) resulting in the most overlap between the two distributions, thus offering the hardest negative sample among the considered \(r\) values (for more details see A.1). Note that a large overlap between distributions is not sufficient to call the generated samples hard negatives because they should also belong to the target category. This is, however, confirmed by the high Oracle accuracy in Table 4 (elaborated in detail in the following paragraph) which verifies that majority of the generated augmentation samples do belong to the target category.

   \\ 
**Augmentation** & **Method** & **Pre-training** & **1-shot** & **5-shot** \\  Weak & SimCLR & unsup. & 64.4\(\)0.4 & 79.2\(\)0.3 \\ Weak & SimSim & unsup. & 64.1\(\)0.4 & 81.4\(\)0.3 \\  Weak & UniSim & unsup. & 63.1\(\)0.7 & 81.0\(\)0.5 \\ Strong & UniSim & unsup. & 62.8\(\)0.7 & 89.0\(\)0.5 \\ CuMax & UniSim & unsup. & 62.1\(\)0.7 & 78.9\(\)0.6 \\ Middle & UniSim & unsup. & 63.2\(\)0.7 & 78.4\(\)0.6 \\ TagImg & UniSim & unsup. & 63.9\(\)0.7 & 81.8\(\)0.5 \\ TagImg & UniSim & unsup. & 65.7\(\)0.7 & 83.5\(\)0.5 \\ TxImg & UniSim & unsup. & 72.9\(\)0.6 & 84.2\(\)0.5 \\ DaFusion & UniSim & unsup. & 62.6\(\)1.1 & 80.1\(\)0.5 \\ GeNIe(Ours) & UniSim & unsup. & **73.6\(\)0.6** & **85.0\(\)0.5** \\ GeNIe-Ada(Ours) & UniSim & unsup. & **75.1\(\)0.6** & **85.5\(\)0.5** \\   \\  Weak & PDAs+Net & unsup. & 69.0\(\)0.9 & 84.2\(\)0.7 \\ Weak & Meta-Und & unsup. & 69.6\(\)0.4 & 86.5\(\)0.3 \\  Weak & UniSim & unsup. & 66.9\(\)0.4 & 86.5\(\)0.3 \\ Weak & UniSim & unsup. & 66.8\(\)0.7 & 84.7\(\)0.5 \\ Strong & UniSim & unsup. & 65.7\(\)0.7 & 84.5\(\)0.5 \\ CoMax & UniSim & unsup. & 66.0\(\)0.7 & 83.3\(\)0.5 \\ Multi & UniSim & unsup. & 66.1\(\)0.5 & 84.1\(\)0.8 \\ TagImg & UniSim & unsup. & 67.8\(\)0.7 & 85.3\(\)0.5 \\ TagImg & UniSim & unsup. & 72.4\(\)0.7 & 86.7\(\)0.4 \\ TxImgImg & UniSim & unsup. & 77.1\(\)0.6 & 87.3\(\)0.4 \\ DaFusion & UniSim & unsup. & 66.5\(\)2.1 & 88.4\(\)1.4 \\ CaIKs (Ours) & UniSim & unsup. & **78.0\(\)0.6** & **88.0\(\)0.4** \\ GeNIe-Ada (Ours) & UniSim & unsup. & **78.5\(\)0.6** & **88.0\(\)0.6** \\   \\  Weak & UniSim & unsup. & **78.0\(\)0.6** & **88.0\(\)0.6** \\  Weak & UniSim & unsup. & **78.5\(\)0.6** & **88.0\(\)0.6** \\   \\  

Table 3: **Long-Tailed ImageNet-LT:** We compare different augmentation methods on ImageNet-LT and report Top-1 accuracy for “Few”, “Medium”, and “Many” sets. On the “Few” set and LiVT method, our augmentations improve the accuracy by 11.7 points compared to LiVT original augmentation and 4.4 points compared to Txt2Img. GeNIe-Ada outperforms Cap2Aug baseline in “Few” categories by \(7.6\%\). Refer to Table A4 for a full comparison with prior Long-Tailed methods.

Figure 5: **Embedding visualizations of generative augmentations:** We pass all generative augmentations through DINOV2 ViT-G (serving as an oracle) to extract their corresponding embeddings and visualize them with PCA. As shown, the extent of semantic shifts varies based on both the source image and the target class.

**Label consistency of the generated samples.** The choice of noise ratio \(r\) is important in producing hard negative examples. In Table 4, we present the accuracy of the GeNIe model across various noise ratios, alongside the oracle accuracy, which is an ImageNet pre-trained DeiT-Base  classifier. We observe a decline in the label consistency of generated data (quantified by the performance of the oracle model) when decreasing the noise level. Reducing \(r\) also results in a degradation in the performance of the final few-shot model (\(87.2\% 77.6\%\)) corroborating that an appropriate choice of \(r\) plays a crucial role in our design strategy. We investigate this further in the following paragraph.

**Effect of Noise in GeNIe.** We examine the impact of noise on the performance of the few-shot model in Table 4. Noise levels \(r[0.7,0.8]\) yield the best performance. Conversely, utilizing noise levels below \(0.7\) diminishes performance due to label inconsistency, as is demonstrated in Table 4 and Fig 5. As such, determining the appropriate noise level is pivotal for the performance of GeNIe to be able to generate challenging hard negatives while maintaining label consistency. An alternative approach to finding the optimal noise level involves using GeNIe-Ada to adaptively select the noise level for each source image and target class. As demonstrated in Tables 4 and A1, GeNIe-Ada achieves performance that is comparable to or surpasses that of GeNIe with fixed noise levels.

## 5 Concluding Remarks

GeNIe, for the first time to our knowledge, combines contradictory sources of information (a source image, and a different target category prompt) through a noise adjustment strategy into a conditional latent diffusion model to generate challenging augmentations, which can serve as hard negatives.

**Limitation.** The required time to create augmentations through GeNIe is on par with any typical diffusion-based competitors ; however, this is naturally slower than traditional augmentation techniques . This is not a bottleneck in offline augmentation strategies, but can be considered a limiting factor in real-time scenarios. Recent studies are already mitigating this through advancements in diffusion model efficiency . Another challenge present in any generative AI-based augmentation technique is the domain shift between the distribution of training data and the downstream context they might be used for augmentation. A possible remedy is to fine-tune the diffusion backbone on a rather small dataset from the downstream task.

**Broader Impact.** We believe ideas from GeNIe can have a significant impact when it comes to generating hard augmentations challenging and thus enhancing downstream tasks beyond classification. At the same time, just like any other generative model, GeNIe can also introduce inherent biases stemming from the training data used to build its diffusion backbone, which can reflect and amplify societal prejudices or inaccuracies. Therefore, it is crucial to carefully mitigate potential biases in generative models such as GeNIe to ensure a fair and ethical deployment of deep learning systems.

  
**Noise** & **ResNet-Net** & **ResNet-34** & **ResNet-50** & **Oracle** \\  & **1-shot** & **5-shot** & **1-shot** & **5-shot** & **1-shot** & **5-shot** & **Acc** \\    } & 60.42\(\)0.8 & 74.11\(\)0.6 & 62.02\(\)0.8 & 75.80\(\)0.0 & 63.65\(\)0.9 & 77.61\(\)0.6 & 73.4\(\)0.5 \\  & 69.66\(\)0.7 & 80.65\(\)0.5 & 71.13\(\)0.7 & 82.21\(\)0.5 & 72.10\(\)0.7 & 82.79\(\)0.5 & 85.8\(\)0.4 \\   & 74.50\(\)0.6 & 83.26\(\)0.5 & 76.41\(\)0.6 & 84.44\(\)0.5 & 77.05\(\)0.6 & 84.95\(\)0.4 & 94.5\(\)0.2 \\   & 75.45\(\)0.6 & 83.58\(\)0.3 & 77.08\(\)0.6 & 86.28\(\)0.4 & 72.86\(\)0.8 & 87.22\(\)0.4 & 98.2\(\)0.1 \\   & 74.96\(\)0.6 & 85.29\(\)0.4 & 77.63\(\)0.6 & 86.17\(\)0.4 & 77.73\(\)0.6 & 87.00\(\)0.4 & 99.3\(\)0.1 \\   & 76.79\(\)0.6 & 85.89\(\)0.4 & 78.49\(\)0.6 & 86.55\(\)0.4 & 78.64\(\)0.6 & 87.88\(\)0.4 & 98.9\(\)0.2 \\   

Table 4: **Effect of Noise in GeNIe:** We use the same setting as in Table 1 to study the effect of the amount of noise. As expected (also shown in Fig 5), small noise results in worse accuracy since some generated images may be from the source category rather than the target one. For \(r=0.5\) only \(73\%\) of the generated data is from the target category. This behaviour is also shown in Fig. 2. Notably, reducing the noise level below \(0.7\) is associated with a decline in oracle accuracy and subsequent degradation in the performance of the final few-shot model. Note that the high oracle accuracy of GeNIe-Ada demonstrates its capability to adaptively select the noise level per source and target, ensuring semantic consistency with the intended target.

Figure 6: **Why GeNIe augmentations are challenging?** While deciding which class the generated augmentations (\(X_{r}\)) belong to is already difficult within \(r=[0.6,0.7]\) (due to high overlap between \(P(Y_{S}|X_{r})\) and \(P(Y_{T}|X_{r})\)), GeNIe-Ada selects the best noise threshold (\(r^{*}\)) offering the hardest negative sample.