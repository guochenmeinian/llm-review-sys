# A Reduction-based Framework for Sequential Decision Making with Delayed Feedback

Yunchang Yang\({}^{1}\) Han Zhong\({}^{1}\) Tianhao Wu\({}^{2}\) Bin Liu\({}^{3}\) Liwei Wang\({}^{1,4}\) Simon S. Du\({}^{5}\)

\({}^{1}\)Center for Data Science, Peking University

\({}^{2}\)University of California, Berkeley \({}^{3}\)Zhejiang Lab

\({}^{4}\)National Key Laboratory of General Artificial Intelligence,

School of Intelligence Science and Technology, Peking University

\({}^{5}\)University of Washington

Equal contribution. Correspondence to Yunchang Yang \(\)yangyc@pku.edu.cn\(\) and Han Zhong \(\)hanzhong@stu.pku.edu.cn\(\).

###### Abstract

We study stochastic delayed feedback in general sequential decision-making problems, which include bandits, single-agent Markov decision processes (MDPs), and Markov games (MGs). We propose a novel reduction-based framework, which turns any multi-batched algorithm for sequential decision making with instantaneous feedback into a sample-efficient algorithm that can handle stochastic delays in sequential decision-making problems. By plugging different multi-batched algorithms into our framework, we provide several examples demonstrating that our framework not only matches or improves existing results for bandits, tabular MDPs, and tabular MGs, but also provides the first line of studies on delays in sequential decision making with function approximation. In summary, we provide a complete set of sharp results for single-agent and multi-agent sequential decision-making problems with delayed feedback.

## 1 Introduction

Delayed feedback is a common and fundamental problem in sequential decision making (Sutton and Barto, 2018; Lattimore and Szepesvari, 2020; Zhang et al., 2021). Taking the recommendation system as an example, delayed feedback is an inherent part of this problem. Specifically, the learner adjusts her recommendation after receiving users' feedback, which may take a random delay after the recommendation is issued. More examples include but are not limited to robotics (Mahmood et al., 2018) and video steaming (Changuel et al., 2012). Furthermore, the delayed feedback issue is more serious in multi-agent systems (Chen et al., 2020) since observing the actions of other agents' may happen with a varying delay.

Although handling delayed feedback is a prominent challenge in practice, the theoretical understanding of delays in sequential decision making is limited. Even assuming the delays are stochastic (delays are sampled from a fixed distribution), the nearly minimax optimal result is only obtained by works on multi-armed bandits and contextual bandits (Agarwal and Duchi, 2011; Dudik et al., 2011; Joulani et al., 2013; Vernade et al., 2020, 2017; Gael et al., 2020). For more complex problems such as linear bandits (Lancewicki et al., 2021) and tabular reinforcement learning (RL) (Howson et al., 2022), the results are suboptimal. Meanwhile, to the best of our knowledge, there are no theoretical guarantees in RL with function approximation and multi-agent RL settings. Hence, we focus on stochastic delays and aim to answer the following two questions in this work:

1. Can we provide sharper regret bounds for basic models such as linear bandits and tabular RL?2. Can we handle delays in (multi-agent) sequential decision making in the context of function approximation?

We answer these two questions affirmatively by proposing a reduction-based framework for stochastic delays in both single-agent and multi-agent sequential decision making. Our contributions are summarized below.

Our Contributions.Our main contribution is proposing a new reduction-based framework for both single-agent and multi-agent sequential decision making with stochastic delayed feedback. The proposed framework can convert any multi-batched algorithm (cf. Section 3) for sequential decision making with instantaneous feedback to a provably efficient algorithm that is capable of handling stochastic delayed feedback in sequential decision making. Unlike previous works which require case by case algorithm design and analysis, we provide a unified framework to solve a wide range of problems all together.

Based on this framework, we obtain a complete set of results for single-agent and multi-agent sequential decision making with delayed feedback. In specific, our contributions can be summarized as follows:

* **New framework:** We propose a generic algorithm that can be integrated with any multi-batched algorithm in a black-box fashion. Meanwhile, we provide a unified theoretical analysis for the proposed generic algorithm.
* **Improved results and new results:** By applying our framework to different settings, we obtain state-of-the-art regret bounds for multi-armed bandits, and derive sharper results for linear bandits and tabular RL, which significantly improve existing results (Lancewicki et al., 2021; Howson et al., 2022). We also provide the first result for single-agent RL with delayed feedback in the context of linear or even general function approximation;
* **New algorithms:** We show that delayed feedback in Markov games (MGs) can also be handled by our framework. By plugging several new proposed multi-batched algorithms for MGs into our framework, we not only improve recent results on tabular MGs (Zhang et al., 2022), but

  Setting & Previous Result & This Work \\    Multi-armed \\ bandit \\  } & \(O(x_{i}}+d_{r}(q) K)\) & \(O(_{i}}+K}{q}+d_{r}(q) K)\) \\  & (Lancewicki et al., 2021) & \(O(_{i}}+[] K)\) \\   Linear \\ bandit \\  } & \(O(d+d^{3/2}[])\) & \((d+d_{r}(q))\) \\  & \((d+[])\) & \((d+[])\) \\  & (Vakil et al., 2023), concurrent work & \((d+[])\) \\   Tabular \\ MDP \\  } & \((SAK}+H^{2}SA[] K)\) & \((K}+H(H+ K)d_{r}(q))\) \\  & (Hewison et al., 2022) & \((K}+H(H+ K)[])\) \\   Linear \\ MDP \\  } & \(--\) & \((H^{4}K}+dH^{2}d_{r}(q) K)\) \\   & & \((H^{4}K}+dH^{2}[])\) \\   RL with general \\ function approximation \\  } & \(--\) & \((}^{2}(,1/K)H^{4}K}+H^{2} _{}(,1/K)d_{r}(q))\) \\  & & \((}^{2}(,1/K)H^{4}K}+H^{2} _{}(,1/K)[])\) \\   Tabular two-player \\ zero-sum MG \\  } & \(--\) & \(((SABK}+H^{3}S^{2}AB)+H^{2}SABd_{r}(q)  K)\) \\   & & \((SABK}+H^{3}S^{2}AB+H^{2}SAB[] K)\). \\   Linear two-player \\ zero-sum MG \\  } & \(--\) & \((H^{4}K}+dH^{2}d_{r}(q) K)\) \\   & & \((H^{4}K}+dH^{2}[] K)\) \\   Tabular multi-player \\ general-sum MG \\  } & \((H^{3}}+H^{3}[]K})\) & \((SA_{}K}+H^{2}nSd_{r}(q) K)\) \\   & (Zhang et al., 2022) & \((SA_{}K}+H^{2}nS[] K)\) \\  

Table 1: Comparison of bounds for MSDM with delayed feedback. We denote by \(K\) the number of rounds the agent plays (episodes in RL), \(A\) and \(B\) the number of actions (or arms), \(A_{}=_{i}A_{i}\), \(S\) the number of states, \(H\) the length of an episode, \(d\) the linear feature dimension, and \(_{E}(,1/K)\) the eluder dimension of general function class \(\), \(d_{}(q)\) is the quantile function of the random variable \(\) and \(n\) is the number of players.

also present a new result for linear MGs. As a byproduct, we provide the first line of study on multi-batched algorithms for MGs, which might be of independent interest.

Our results and comparisons with existing works are summarized in Table 1.

### Related Works

Bandit/MDP with delayed feedback.Stochastically delayed feedback has been studied a lot in the multi-armed bandit and contextual bandit problems (Agarwal and Duchi, 2011; Dudik et al., 2011; Joulani et al., 2013; Vernade et al., 2020, 2017; Gael et al., 2020; Huang et al., 2023). A recent work (Lancewicki et al., 2021) shows that under stochastic delay, elimination algorithm performs better than UCB algorithm, and gives tight upper and lower bounds for multi-armed bandit setting. Howson et al. (2021) solves the linear bandit setting by adapting LinUCB algorithm. The concurrent work of Vakili et al. (2023) provides a bound for the kernel bandit setting with delayed feedback. Our work does not consider the kernel setting, but we conjecture that our framework can handle this problem by designing a multi-batched algorithm for kernel bandits.

In RL, despite practical importance, there is limited literature on stochastical delay in RL. Lancewicki et al. (2022) and Jin et al. (2022) considered adversarial delays. Their regret depends on the sum of the delays, the number of states and the number of steps per episode. However, when reducing their results to stochastic delay, the bounds will be too loose. Howson et al. (2022) considered stochastic delay in tabular MDP, but their bound is not tight. As far as we know, there is no work considering linear MDP and general function approximation with delayed feedback.

Another line of work considers adversarially delayed feedback (Lancewicki et al., 2022; Jin et al., 2022; Quanrud and Khashabi, 2015; Thune et al., 2019; Bistritz et al., 2019; Zimmert and Seldin, 2020; Ito et al., 2020; Gyorgy and Joulani, 2021; Van Der Hoeven and Cesa-Bianchi, 2022). In the adversarial setting the state-of-the-art result is of form \(\), where \(K\) is the number of the episodes and \(D\) is the total delay. Note that when adapting their result to the stochastic setting, the bound becomes \(\) where \(\) is the expectation of delay, while in stochastic delay setting the upper bound can be \(+\). Therefore, a direct reduction from adversarial delay setting to stochastic delay setting will result in a suboptimal regret bound.

Low switching cost algorithm.Bandit problem with low switching cost has been widely studied in past decades. Cesa-Bianchi et al. (2013) showed an \((K^{})\) regret bound under adaptive adversaries and bounded memories. Perchet et al. (2016) proved a regret bound of \((K^{}})\) for the two-armed bandit problem within \(M\) batches, and later Gao et al. (2019) extended their result to the general \(A\)-armed case. In RL, by doubling updates, the global switching cost is \(O(SAH_{2}(K))\) while keeping the regret \((})\)(Azar et al., 2017). Recently Zhang et al. (2022) proposed a policy elimination algorithm that achieves \((})\) regret and \(O(H+_{2}_{2}(K))\) switching cost. Besides, Gao et al. (2021) generalized the problem to linear MDPs, and established a regret bound of \((H^{4}K})\) with \(O(dH(K))\) global switching cost. Recent work Qiao et al. (2022) achieved \(O(HSA_{2}_{2}(K))\) switching cost and \(((S,A,H))\) regret with a computational inefficient algorithm. And Kong et al. (2021) consider MDP with general function approximation settings.

Markov Games.A standard framework to capture multi-agent RL is Markov games (also known as stochastic games) (Shapley, 1953). For two-player zero-sum Markov games, the goal is to solve for the Nash equilibrium (NE) (Nash, 1951). Recently, many works study this problem under the generative model (Zhang et al., 2020; Li et al., 2022), offline setting (Cui and Du, 2022, 2022; Xiong et al., 2022; Xiong et al., 2022; Yan et al., 2022), and online setting (Bai and Jin, 2020; Bai et al., 2020; Liu et al., 2021; Tian et al., 2021; Xie et al., 2020; Chen et al., 2021; Jin et al., 2022; Xiong et al., 2022; Huang et al., 2021). Our work is mostly related to Liu et al. (2021) and Xie et al. (2020). Specifically, Liu et al. (2021) studies online tabular two-player zero-sum MGs and proposes an algorithm with \(O(K})\) regret. Xie et al. (2020) focused on online linear MGs provide an algorithm that enjoys \(O(H^{4}K})\) regret. We also remark that Jin et al. (2022); Xiong et al. (2022); Huang et al. (2021) establish sharper regret bound for linear MGs but the algorithms therein are computationally inefficient. There is also a line of works (Liu et al., 2021; Jin et al., 2021;Song et al., 2021; Zhong et al., 2021; Mao and Basar, 2022; Cui and Du, 2022; Jin et al., 2022c; Daskalakis et al., 2022) studying multi-player general-sum MGs. Among these works, Jin et al. (2021); Song et al. (2021); Mao and Basar (2022) use V-learning algorithms to solve coarse correlated equilibrium (CCE). But all these algorithms can not handle delayed feedback nor are multi-batched algorithms that can be plugged into our framework. To this end, we present the first line of studies on multi-batched algorithms for MGs and provide a complete set of results for MGs with delayed feedback.

A very recent but independent work (Zhang et al., 2022) also studies multi-agent reinforcement learning with delays. They assume that the reward is delayed, and aim to solve for coarse correlated equilibrium (CCE) in general-sum Markov games. Our work has several differences from Zhang et al. (2022). First, they study the setting where the total delay is upper bounded by a certain threshold, while we focus on the setup with stochastic delay. Besides, their method seems only applicable for reward delay, while we allow the whole trajectory feedback to be delayed. Second, in the stochastic delay setting the delay-dependent term in their bound is worse than ours. Specifically, their bound scales as \(O(H^{3}[]K})\), while ours is \(O(H^{2}nS[])\) where \(n\) is the number of players. Finally, it seems that their results cannot be extended to the multi-agent RL with function approximation.

## 2 Preliminary

The objective of this section is to provide a unified view of the settings considered in this paper, i.e., bandits, Markov Decision Processes (MDPs) and multi-agent Markov Games. We consider a general framework for interactive decision making, which we refer to as Multi-agent Sequential Decision Making (MSDM).

NotationsWe use \(()\) to represent the set of all probability distributions on a set. For \(n_{+}\), we denote \([n]=\{1,2,,n\}\). We use \(O(),(),()\) to denote the big-O, big-Theta, big-Omega notations. We use \(()\) to hide logarithmic factors.

Multi-agent Sequential Decision MakingA Multi-agent Sequential Decision Making problem can be represented as a tuple \(M=(n,,\{_{i}\}_{i=1}^{n},H,\{p_{h}\}_{h=1}^{H},s_{1},\{ r_{h}\}_{h=1}^{H})\), where \(n\) is the number of agents, \(\) is the state space, \(_{i}\) is the action space of player \(i\), \(H\) is the length of each episode and \(s_{1}\) is the initial state. We define \(=_{i}_{i}\) as the whole action space.

At each stage \(h\), for every player \(i\), every state-action pair \((s,a_{1},...,a_{n})\) is characterized by a reward distribution with mean \(r_{i,h}(s,a_{1},...,a_{n})\) and support in \(\), and a transition distribution \(p_{h}(|s,a)\) over next states. We denote by \(S=||\) and \(A_{i}=|_{i}|\).

The protocol proceeds in several episodes. At the beginning of each episode, the environment starts with initial state \(s_{1}\). At each step \(h[1,H]\), the agents observe the current state \(s_{h}\), and take their actions \((a_{1},...,a_{n})\) respectively. Then the environment returns a reward signal, and transits to next state \(s_{h+1}\).

A (random) policy \(_{i}\) of the \(i^{}\) player is a set of \(H\) maps \(_{i}:=\{_{i,h}:_{i}\}_{h [H]}\), where each \(_{i,h}\) maps a random sample \(\) from a probability space \(\) and state \(s\) to an action in \(_{i}\). A joint (potentially correlated) policy is a set of policies \(\{_{i}\}_{i=1}^{m}\), where the same random sample \(\) is shared among all agents, which we denote as \(=_{1}_{2}_{m}\). We also denote \(_{-i}\) as the joint policy excluding the \(i^{}\) player. For each stage \(h[H]\) and any state-action pair \((s,a)\), the value function and Q-function of a policy \(\) are defined as:

\[Q_{i,h}^{}(s,a)=_{h^{}=h}^{H}r_{i,h^{}} \,\,s_{h}=s,a_{h}=a,, V_{i,h}^{}(s)= _{h^{}=h}^{H}r_{i,h^{}}\,\,s_{h}=s,.\]

For each policy \(\), we define \(V_{H+1}^{}(s)=0\) and \(Q_{H+1}^{}(s,a)=0\) for all \(s,a\). Sometimes we omit the index \(i\) when it is clear from the context.

For the single agent case (\(n=1\)), the goal of the learner is to maximize the total reward it receives. There exists an optimal policy \(^{}\) such that \(Q_{h}^{}(s,a)=Q_{h}^{^{}}(s,a)=_{}Q_{h}^{}(s,a)\) satisfies the optimal Bellman equations

\[Q_{h}^{}(s,a)=r_{h}(s,a)+_{s^{} p_{h}(s,a)}[V_{h+1}^ {}(s^{})], V_{h}^{}(s)=_{a}\{Q_{h}^{ }(s,a)\},(s,a).\]Then the optimal policy is the greedy policy \(_{h}^{}(s)=_{a}\{Q_{h}^{}(s,a)\}\). And we evaluate the performance of the learner through the notion of single-player regret, which is defined as

\[(K)=_{k=1}^{K}(V_{1}^{}-V_{1}^{_{k}})(s_{ 1}).\] (1)

In the case of multi-player general-sum MGs, for any policy \(_{-i}\), the best response of the \(i^{}\) player is defined as a policy of the \(i^{}\) player which is independent of the randomness in \(_{-i}\), and achieves the highest value for herself conditioned on all other players deploying \(_{-i}\). In symbol, the best response is the maximizer of \(_{_{i}^{}}V_{i,1}^{_{i}^{}_{-i}}(s_{1})\) whose value we also denote as \(V_{i,1}^{,_{-i}}(s_{1})\) for simplicity.

For general-sum MGs, we aim to learn the Coarse Correlated Equilibrium (CCE), which is defined as a joint (potentially correlated) policy where no player can increase her value by playing a different independent strategy. In symbol,

**Definition 1** (Coarse Correlated Equilibrium).: _A joint policy \(\) is a CCE if \(_{i[m]}(V_{i,1}^{,_{-i}}-V_{i,1}^{})(s_{1})=\) 0. A joint policy \(\) is a \(\)-approximate CCE if \(_{i[m]}(V_{i,1}^{,_{-i}}-V_{i,1}^{})(s_{1})\)._

For any algorithm that outputs policy \(^{k}\) at episode \(k\), we measure the performance of the algorithm using the following notion of regret:

\[(K)=_{k=1}^{K}_{j}(V_{j,h}^{,^{k} _{-j,h}}-V_{j,h}^{^{k}_{h}})(s_{1}).\] (2)

One special case of multi-player general-sum MGs is two-player zero-sum MGs, in which there are only two players and the reward satisfies \(r_{2,h}=-r_{1,h}\). In zero-sum MGs, there exist policies that are the best responses to each other. We call these optimal strategies the Nash equilibrium of the Markov game, which satisfies the following minimax equation:

\[_{}_{}V_{h}^{,}(s)=V_{h}^{^{},^{}}(s)= _{}_{}V_{h}^{,}(s).\]

We measure the suboptimality of any pair of general policies \((,)\) using the gap between their performance and the performance of the optimal strategy (i.e., Nash equilibrium) when playing against the best responses respectively:

\[V_{1}^{,}(s_{1})-V_{1}^{,}(s_{ 1})=[V_{1}^{,}(s_{1})-V_{1}^{}( s_{1})]+[V_{1}^{}(s_{1})-V_{1}^{, }(s_{1})].\]

Let \((^{k},^{k})\) denote the policies deployed by the algorithm in the \(k^{}\) episode. The learning goal is to minimize the two-player regret of \(K\) episodes, defined as

\[(K)=_{k=1}^{K}(V_{1}^{,^{k}}-V_{1}^{^{k},})(s_{1}).\] (3)

One can see that (3) is a natural reduction of (2) from general-sum games to zero-sum games.

MSDM with Delayed FeedbackWe focus on the case where there is delay between playing an episode and observing the sequence of states, actions and rewards sampled by the agent; we refer to this sequence as feedback throughout this paper.

We assume that the delays are stochastic, i.e. we introduce a random variable for the delay between playing the \(k\)-th episode and observing the feedback of the \(k\)-th episode, which we denote by \(_{k}\). We assume the delays are i.i.d:

**Assumption 1**.: _The delays \(\{_{k}\}_{k=1}^{k}\) are positive, independent and identically distributed random variables: \(_{k}f_{}()\). We denote the expectation of \(\) as \([]\) (which may be infinite), and denote the quantile function of \(\) as \(d_{}(q)\), i.e._

\[d_{}(q)=\{[]  q\}.\] (4)When delay exists, the feedback associated with an episode does not return immediately. Instead, the feedback of the \(k\)-th episode cannot be observed until the \(k+_{k}\)-th episode ends.

Sometimes in order to control the scale and distribution of the delay, we make the assumption that the delays are \((v,b)\) subexponential random variables. This is a standard assumption used in previous works Howson et al. (2021), though it is not always necessary in our paper.

**Assumption 2**.: _The delays are non-negative, independent, and identically distributed \((v,b)\)- subexponential random variables. That is, their moment generating function satisfies the following inequality_

\[[((_{t}-[_{t}] ))](v^{2}^{2})\] (5)

_for some \(v\) and \(b\), and all \(|| 1/b\)._

## 3 Multi-batched Algorithm for MSDMs

In this section we mainly discuss multi-batched algorithms for MSDMs. This section aims to provide a formal definition of multi-batched algorithm, and give some examples of multi-batched algorithm for the ease of understanding.

Formally, a multi-batched algorithm consists of several batches. At the beginning of the \(m^{}\) batch, the algorithm outputs a policy sequence \(^{m}=(_{1}^{m},_{2}^{m},...,_{l^{m}}^{m})\) for this batch using data collected in previous batches, along with a stopping criteria \(SC\), which is a mapping from a dataset to a boolean value. Then the agent executes the policy sequence \(^{m}\) for several episodes and collects feedback to the current dataset \(\), until the stopping criteria are satisfied (i.e. \(SC()=1\)). If the length of the \(m^{}\) batch exceeds \(l_{m}\), then we simply run the policy sequence cyclically, which means in the \(k^{}\) episode we run policy \(_{kl^{m}}^{m}\). Algorithm 1 describes a protocol of multi-batched algorithm.

```
1:Initialize dataset \(^{0}=\)
2:for batch \(m=1,...,M\)do
3:\(=\)
4:\(k=0\)
5: Calculate a policy sequence \(^{m}=(_{1}^{m},_{2}^{m},...,_{l^{m}}^{m})\) using previous data \(^{m-1}\), and a stopping criteria \(SC\)
6:while\(SC()=0\)do
7:\(k k+1\)
8: In episode \(k\), execute \(_{kl^{m}}^{m}\) and collect trajectory feedback \(o^{k}\)
9:\(=\{o^{k}\}\)
10:endwhile
11: Update dataset \(^{m}=^{m-1}\)
12:endfor
13:Output: A policy sequence \(\{^{m}\}\) ```

**Algorithm 1** Protocol of Multi-batched Algorithm

Note that the major requirement is that the policy to be executed during the batch should be calculated at the beginning of the batch. Therefore, algorithms that calculate the policy at the beginning of each episode do not satisfy this requirement. For example, UCB algorithms in bandits (Lattimore and Szepesvari, 2020) and UCBVI (Azar et al., 2017) are not multi-batched algorithms.

Relation with low-switching cost algorithmOur definition of multi-batched algorithm is similar to that of low-switching cost algorithm, but is more general.

For any algorithm, the switching cost is the number of policy changes in the running of the algorithm in \(K\) episodes, namely:

\[N_{}\,\,_{k=1}^{K-1}\{_{k} _{k+1}\}.\]

Most algorithms with \(N_{}\,\) low-switching cost are multi-batched algorithms with \(N_{}\,\) batches. For example, the policy elimination algorithm proposed in Zhang et al. (2022) is a multi-batchedalgorithm with \(2H+ K\) batches. And the algorithm for linear MDP with low global switching cost in Gao et al. (2021) is a multi-batched algorithm with \(O(dH K)\) batches. On the other hand, the difference between batches and switching cost is that we do not require the policy to be the same during a batch. Therefore, a multi-batched algorithm with \(N\) batches may not be an algorithm with \(N\) switching cost. For example, the Phase Elimination Algorithm in linear bandit with finite arms setting (Lattimore and Szepesvari, 2020) is a multi-batched algorithm with \(O( K)\) batches, but the switching cost is \(O(A K)\) which is much larger.

## 4 A Framework for Sequential Decision Making with Delayed Feedback

In this section, we introduce our framework for sequential decision making problems with stochastic delayed feedback. We first describe our framework, which converts any multi-batched algorithm to an algorithm for MSDM with delayed feedback. Then we provide theoretical guarantees for the performance of such algorithms.

Before formally introducing our framework, we first give an intuitive explanation of why multi-batched algorithms work for delayed settings. Recall that Lancewicki et al. (2021) gives a lower bound showing that UCB-type algorithms are suboptimal in multi-armed bandits with stochastic delay (see Theorem 1 in Lancewicki et al. (2021)). The main idea of the lower bound is that UCB algorithm will keep pulling a suboptimal arm until the delayed feedback arrives and the policy is updated. Therefore, if the policy updates too frequently, then the algorithm has to spend more time waiting for the update, and the effect of delay will be greater. This observation motivates us to consider multi-batched algorithms, which have a low frequency of policy updating.

For any sequential decision making problem, assume that we have a multi-batched algorithm \(\) for the problem without delay. Then in the existence of delay, we can still run \(\). The difference is, in each batch of the algorithm, we run some extra steps until we observe enough feedback so that the stopping criteria are satisfied. For example, if the delay \(\) is a constant and the stopping criteria is that the policy \(_{k}\) is executed for a pre-defined number of times \(t_{k}\), then we only need to run \(_{k}\) for \(t_{k}+\) times. After the extra steps are finished, we can collect \(t_{k}\) delayed observations, which meet the original stopping criteria. Then we can continue to the next batch. Algorithm 2 gives a formal description.

```
1:Require: A multi-batched algorithm for the undelayed environment \(\)
2:Initialize dataset \(^{0}=\)
3:for batch \(m=1,...,M\)do
4:\(\) calculates a policy \(^{m}\) (or a policy sequence) and a stopping criteria \(SC\)
5:\(=\)
6:while\(SC()=0\)do
7:\(k k+1\)
8: In episode \(k\), execute \(^{m}\)
9: Collect trajectory feedback in this batch that is observed by the end of the episode: \(\{o^{t}:t+_{t}=k\}\)
10:\(=\{o^{t}:t+_{t}=k\}\)
11:endwhile
12: Update dataset \(^{m}=^{m-1}\)
13:endfor
14:Output: A policy sequence \(\{^{m}\}\) ```

**Algorithm 2** Multi-batched Algorithm With Delayed Feedback

For the performance of Algorithm 2, we have the following theorem, which relates the regret of delayed MSDM problems with the regret of multi-batched algorithms in the undelayed environment.

**Theorem 1**.: _Assume that in the undelayed environment, we have a multi-batched algorithm with \(N_{b}\) batches, and the regret of the algorithm in \(K\) episodes can be upper bounded by \(}(K)\) with probability at least \(1-\). Then in the delayed feedback case, with probability at least \(1-\), the regret can be upper bounded by_

\[(K)}(K)+ (K/)}{q}+HN_{b}d_{}(q)\] (6)_for any \(q(0,1)\). In addition, if the delays satisfy Assumption 2, then with probability at least \(1-\) the regret can be upper bounded by_

\[(K)}(K)+HN_{b}([ ]+C_{}),\] (7)

_where \(C_{}=\{(3KH/(2))},2b(3KH/(2))\}\) is a problem-independent constant._

The proof is in Appendix A. The main idea is simple: in each batch, we can control the number of extra steps when waiting for the original algorithm to collect enough delayed feedback. And the extra regret can be bounded by the product of the number of batches and the number of extra steps per batch. Note that in Eq. (6) we do not need Assumption 2. This makes (6) more general.

Applying Theorem 1 to different settings, we will get various types of results. For example, in multi-armed bandit setting, using the Batched Successive Elimination (BaSE) algorithm in Gao et al. (2019), we can get a \(O(A(K)+d_{}(q)(K)\) upper bound from (6), which recovers the SOTA result in Lancewicki et al. (2021) up to a \( K\) factor in the delay-dependent term. We leave the details to Appendix C, and list the main results in Table 1.

## 5 Results for Markov Games

As an application, we study Markov games with delayed feedback using our framework. We first develop new multi-batched algorithms for Markov games, then combine them with our framework to obtain regret bounds for the delayed setting.

### Tabular Zero-Sum Markov Game

First we study the tabular zero-sum Markov game setting. The algorithm is formally described in Algorithm 4 in Appendix D.

Our algorithm can be decomposed into two parts: an estimation step and a policy update step. The estimation step largely follows the method in Liu et al. (2021). For each \((h,s,a,b)\), we maintain an optimistic estimation \(_{h}^{k}(s,a,b)\) and a pessimistic estimation \(_{h}^{k}(s,a,b)\) of the underlying \(Q_{h}^{}(s,a,b)\), by value iteration with bonus using the empirical estimate of the transition \(}\). And we compute the Coarse Correlated Equilibrium (CCE) policy \(\) of the estimated value functions Xie et al. (2020). A CCE always exists, and can be computed efficiently.

In the policy update step, we choose the CCE policy w.r.t. the current estimated value function. Note that we use the doubling trick to control the number of updates: we only update the policy when there exists \((h,s,a,b)\) such that the visiting count \(N_{h}(s,a,b)\) is doubled. This update scheme ensures that the batch number is small.

For the regret bound of Algorithm 4, we have the following theorem.

**Theorem 2**.: _For any \(p(0,1]\), letting \(=(SABT/p)\), then with probability at least \(1-p\), the regret of Algorithm 4 satisfies_

\[(K) O(SABK}+H^{3}S^{2}AB^{2} ).\]

_and the batch number is at most \(O(HSAB K)\)._

The proof of Theorem 4 is similar to Theorem 4 in Liu et al. (2021), but the main difference is we have to deal with the sum of the Bernstein-type bonus under the doubling framework. To deal with this we borrow ideas from Zhang et al. (2021). The whole proof is in Appendix D.

Adapting the algorithm to the delayed feedback setting using our framework, we have the following corollary.

**Corollary 1**.: _For tabular Markov game with delayed feedback, running Algorithm 2 with \(\) being Algorithm 4, we can upper bound the regret by_

\[(K) O((SABK}+H^{3}S^{2 }AB^{2})+H^{2}SABd_{}(q) K)\]

_for any \(q(0,1)\). In addition, if the delays satisfy Assumption 2, the regret can be upper bounded by_

\[(K) OSABK}+H^{3}S^{2}AB^{2}+H^ {2}SAB[] K.\]

### Linear Zero-Sum Markov Game

We consider the linear zero-sum Markov game setting, where the reward and transition have a linear structure.

**Assumption 3**.: _For each \((x,a,b)\) and \(h[H]\), we have_

\[r_{h}(x,a,b)=(x,a,b)^{}_{h},_{h}( x,a,b)= (x,a,b)^{}_{h}(),\]

_where \(:^{d}\) is a known feature map, \(_{h}^{d}\) is an unknown vector and \(_{h}=(_{h}^{(i)})_{i[d]}\) is a vector of d unknown (signed) measures on \(\). We assume that \(\|(,,)\| 1,\|_{h}\|\) and \(\|_{h}()\|\) for all \(h[H]\), where \(\|\|\) is the vector \(_{2}\) norm._

The formal algorithm is in Algorithm 5 in Appendix D.2. It is also divided into two parts. The estimation step is similar to that in Xie et al. (2020), where we calculate an optimistic estimation \(_{h}^{k}(s,a,b)\) and a pessimistic estimation \(_{h}^{k}(s,a,b)\) of the underlying \(Q_{h}^{}(s,a,b)\) by least square value iteration. And we compute the CCE policy \(\) of the estimated value functions.

In the policy update step, we update the policy to the CCE policy w.r.t. the current estimated value function. Note that instead of updating the policy in every episode, we only update it when there exists \(h[H]\) such that \((_{h}^{k})>(_{h})\). This condition implies that we have collected enough information at one direction. This technique is similar to Gao et al. (2021); Wang et al. (2021).

For the regret bound of Algorithm 5, we have the following theorem.

**Theorem 3**.: _In Algorithm 5, let \(=2\). Then we have_

\[(K)(H^{4}K}).\]

_And the batch number of Algorithm 5 is \((1+)\)._

Proof.: See Appendix D.2 for a detailed proof. 

Adapting the algorithm to the delayed feedback setting using our framework, we have the following corollary.

**Corollary 2**.: _For linear Markov game with delayed feedback, running Algorithm 2 with \(\) being Algorithm 5, we can upper bound the regret by_

\[(K)(H^{4}K}+dH^{2}d_{ }(q) K)\]

_for any \(q(0,1)\). In addition, if the delays satisfy Assumption 2, the regret can be upper bounded by_

\[(K)(H^{4}K}+dH^{2}[ ] K).\]

### Tabular General-Sum Markov Game

In this section, we introduce the multi-batched version of V-learning (Jin et al., 2021; Mao and Basar, 2022; Song et al., 2021) for general-sum Markov games. Our goal is to minimize the regret (2). The algorithm is formally described in Algorithm 6 in Appendix D.

The multi-batched V-learning algorithm maintains a value estimator \(V_{h}(s)\), a counter \(N_{h}(s)\), and a policy \(_{h}( s)\) for each \(s\) and \(h\). We also maintain \(S H\) different adversarial bandit algorithms. At step \(h\) in episode \(k\), the algorithm is divided into three steps: policy execution, \(V\)-value update, and policy update. In policy execution step, the algorithm takes action \(a_{h}\) according to \(_{h}^{}\), and observes reward \(r_{h}\) and the next state \(s_{h+1}\), and updates the counter \(N_{h}(s_{h})\). Note that \(_{h}^{}\) is updated only when the visiting count of some state \(N_{h}(s)\) doubles. Therefore it ensures a low batch number.

In the \(V\)-value update step, we update the estimated value function by

\[_{h}(s_{h})=(1-_{t})_{h}(s _{h})+_{t}(r_{h}+V_{h+1}(s_{h+1})+(t)),\]where the learning rate is defined as

\[_{t}=,_{t}^{0}=_{j=1}^{t}(1-_{j} ),_{t}^{i}=_{i}_{j=i+1}^{t}(1-_{j}).\]

and \((t)\) is the bonus to promote optimism.

In the policy update step, the algorithm feeds the action \(a_{h}\) and its "loss" \(+V_{h+1}(s_{h+1})}{H}\) to the \((s_{h},h)^{}\) adversarial bandit algorithm. Then it receives the updated policy \(_{h}( s_{h})\).

For two-player general sum Markov games, we let the two players run Algorithm 6 independently. Each player uses her own set of bonus that depends on the number of her actions. If we choose the adversarial bandit algorithm as Follow The Regularized Leader (FTRL) algorithm (Lattimore and Szepesvari, 2020), we have the following theorem for the regret of multi-batched V-learning.

**Theorem 4**.: _Suppose we choose the adversarial bandit algorithm as FTRL. For any \((0,1)\) and \(K\), let \(=(HSAK/)\). Choose learning rate \(_{t}\) and bonus \(\{(t)\}_{t=1}^{K}\) as \((t)=cA/t}\) so that \(_{i=1}^{t}_{t}^{i}(i)=(A/t})\) for any \(t[K]\), where \(A=_{i}A_{i}\) Then, with probability at least \(1-\), after running Algorithm 6 for \(K\) episodes, we have_

\[(K) O(SA}).\]

_And the batch number is \(O(nHS K)\), where \(n\) is the number of players._

Proof.: See Appendix D for details. 

Finally, adapting the algorithm to the delayed feedback setting using our framework, we have the following corollary.

**Corollary 3**.: _For tabular general-sum Markov game with delayed feedback, running Algorithm 2 with \(\) being Algorithm 6, we can upper bound the regret by_

\[(K) O(SA}+H^{2}nSd_{ }(q) K)\]

_for any \(q(0,1)\). In addition, if the delays satisfy Assumption 2, the regret can be upper bounded by_

\[(K) O(SA}+H^{2}nS[ ] K).\]

Proof.: This corollary is directly implied by Theorems 1 and 4. 

## 6 Conclusion and Future Work

In this paper we study delayed feedback in general multi-agent sequential decision making. We propose a novel reduction-based framework, which turns any multi-batched algorithm for sequential decision making with instantaneous feedback into a sample-efficient algorithm that can handle stochastic delays in sequential decision making. Based on this framework, we obtain a complete set of results for (multi-agent) sequential decision making with delayed feedback.

Our work has also shed light on future works. First, it remains unclear whether the result is tight for MDP and Markov game with delayed feedback. Second, it is possible that we can derive a multi-batched algorithm for tabular Markov game with a smaller batch number, since in tabular MDP, Zhang et al. (2022b) gives an algorithm with \(O(H+_{2}_{2}K)\) batch number which is independent of \(S\). It is not clear whether we can achieve similar results in MDP for Markov games. We believe a tighter result is possible in this setting with a carefully designed multi-batched algorithm.