# RFold: RNA Secondary Structure Prediction with Decoupled Optimization

Anonymous Author(s)

Affiliation

Address

email

###### Abstract

The secondary structure of ribonuclecic acid (RNA) is more stable and accessible in the cell than its tertiary structure, making it essential for functional prediction. Although deep learning has shown promising results in this field, current methods suffer from poor generalization and high complexity. In this work, we present RFold, a simple yet effective RNA secondary structure prediction in an end-to-end manner. RFold introduces a decoupled optimization process that decomposes the vanilla constraint satisfaction problem into row-wise and column-wise optimization, simplifying the solving process while guaranteeing the validity of the output. Moreover, RFold adopts attention maps as informative representations instead of designing hand-crafted features. Extensive experiments demonstrate that RFold achieves competitive performance and about eight times faster inference efficiency than the state-of-the-art method.

## 1 Introduction

Ribonucleic acid is essential in structural biology for its diverse functional classes [8; 45; 49; 18]. The functions of RNA molecules are determined by their structure . The secondary structure, which contains the nucleotide base pairing information, as shown in Fig. 1, is crucial for the correct functions of RNA molecules [13; 11; 68]. Although experimental assays such as X-ray crystallography , nuclear magnetic resonance (NMR) , and cryogenic electron microscopy  can be implemented to determine RNA secondary structure, they suffer from low throughput and expensive cost.

Computational RNA secondary structure prediction methods have become increasingly popular due to their high efficiency . Currently, these methods can be broadly classified into two categories [50; 14; 55; 60]: (i) comparative sequence analysis and (ii) single sequence folding algorithm. Comparative sequence analysis determines the secondary structure conserved among homologous sequences but the limited known RNA families hinder its development [35; 36; 28; 22; 21; 16; 43]. Researchers thus resort to single RNA sequence folding algorithms that do not need multiple sequence alignment information. A classical category of computational RNA folding algorithms is to use dynamic programming (DP) that assumes the secondary structure is a result of energy minimization [3; 44; 39; 73; 42; 10]. However, energy-based

Figure 1: The graph and matrix representation of an RNA secondary structure example.

approaches usually require a nested structure, which ignores some biologically essential structures such as pseudoknots, i.e., non-nested base pairs [5; 54; 70], as shown in Fig. 2. Since predicting secondary structures with pseudoknots under the energy minimization framework has shown to be hard and NP-complete [66; 14], deep learning techniques are introduced as an alternative approach.

Attempts to overcome the limitations of energy-based methods have motivated deep learning methods in the absence of DP. SPOT-RNA  is a seminal work that ensembles ResNet  and LSTM  to identify molecular features. SPOT-RNA does not constrain the output space into valid RNA secondary structures, which degrades its generalization ability . E2Efold  employs an unrolled algorithm for constrained programming that post-processes the network output to satisfy the constraints. E2Efold introduces a convex relaxation to make the optimization tractable, leading to possible constraint violations and poor generalization ability [53; 14]. Developing an appropriate optimization that forces the output to be valid becomes an important issue. Apart from the optimization problem, state-of-the-art approaches require hand-crafted features and introduce the pre-processing step for such features, which is inefficient and needs expert knowledge. CDPfold  develops a matrix representation based on sequence pairing that reflects the implicit matching between bases. UFold  follows the exact post-process mechanism as E2Efold and uses hand-crafted features from CDPfold with U-Net  model architecture to improve the performance.

Although promising, current deep learning methods on RNA secondary structure prediction have been distressed by: (1) _the optimization process that is complicated and poor in generalization_ and (2) _the data pre-processing that requires expensive complexity and expert knowledge_. In this paper, we present RFold, a simple yet effective RNA secondary structure prediction method in an end-to-end manner. Specifically, we introduce a decoupled optimization process that decomposes the vanilla constraint satisfaction problem into row-wise and column-wise optimization, simplifying the solving process while guaranteeing the validity of the output. Besides, we adopt attention maps as informative representations to automatically learn the pair-wise interactions of the nucleotide bases instead of using hand-crafted features to perform data pre-processing. We conduct extensive experiments to compare RFold with state-of-the-art methods on several benchmark datasets and show the superior performance of our proposed method. Moreover, RFold has faster inference efficiency than those methods due to its simplicity.

## 2 Related work

### Comparative Sequence Analysis

Comparative sequence analysis determines base pairs conserved among homologous sequences [55; 17; 28; 36; 35; 19; 20]. ILM  combines thermodynamic and mutual information content scores. Sankoff  merges the sequence alignment and maximal-pairing folding methods . Dynalign  and Carnac [62; 47] are the subsequent variants of Sankoff algorithms. RNA forester  introduces a tree alignment model for global and local alignments. However, the limited number of known RNA families [21; 16; 43] impedes the development of comparative methods.

### Energy-based Folding Algorithms

When the secondary structure consists only of nested base pairing, dynamic programming can efficiently predict the structure by minimizing energy. Early works in this category include Vienna RNAfold , Mfold , RNAstructure , and CONTRAfold . Faster implementations that speed up dynamic programming have been proposed, such as Vienna RNAplfold , LocalFold , and LinearFold . However, these methods cannot accurately predict secondary structures with pseudoknots, as predicting the lowest free energy structures with pseudoknots is NP-complete , making it difficult to improve performance.

Figure 2: Examples of nested and non-nested secondary structures.

### Learning-based Folding Algorithms

SPOT-RNA  is a seminal work that employs deep learning for RNA secondary structure prediction. SPOT-RNA2  improves its predecessor by using evolution-derived sequence profiles and mutational coupling. Inspired by Raptor-X  and SPOT-Contact , SPOT-RNA uses ResNet and bidirectional LSTM with a sigmoid function to output the secondary structures. MXfold  is also an early work that combines support vector machines and thermodynamic models. CDPfold , DMFGold , and MXFold2  integrate deep learning techniques with energy-based methods. E2Efold  takes a remarkable step in constraining the output to be valid by learning unrolled algorithms. However, its relaxation for making the optimization tractable may violate the structural constraints. UFold  further introduces U-Net model architecture to improve performance.

## 3 Preliminaries and Backgrounds

### Preliminaries

The primary structure of RNA is the ordered linear sequence of bases, which is typically represented as a string of letters. Formally, an RNA sequence can be represented as \(=(x_{1},...,x_{L})\), where \(x_{i}\{,,,\}\) denotes one of the four bases, i.e., _Adenine_ (A), _Uracil_ (U), _Ctyosine_ (C), and _Guanine_ (G). The secondary structure of RNA is a contact map represented as a matrix \(\{0,1\}^{L L}\), where \(_{ij}=1\) if the \(i\)-th and \(j\)-th bases are paired. In the RNA secondary structure prediction problem, we aim to obtain a model with learnable parameters \(\) that learns a mapping \(_{}:\) by exploring the interactions between bases. Here, we decompose the mapping \(_{}\) into two sub-mappings as:

\[_{}:=_{_{h}}_{_{g}},\] (1)

where \(_{_{h}}:\), \(_{_{g}}:\) are mappings parameterized by \(_{h}\) and \(_{g}\), respectively. \(^{L L}\) is regarded as the unconstrained output of neural networks.

### Backgrounds

It is worth noting that there are hard constraints on the formation of RNA secondary structure, meaning that certain types of pairing are not available . Such constraints  can be formally described as follows:

* (a) Only three types of nucleotide combinations can form base pairs: \(:=\{,\}\{,\} \{,\}\). For any base pair \(x_{i}x_{j}\) where \(x_{i}x_{j}\), \(_{ij}=0\).
* (b) No sharp loops within three bases. For any adjacent bases, there can be no pairing between them, i.e., \(|i-j| 3,_{ij}=0\).
* (c) There can be at most one pair for each base, i.e., \( i,_{j=1}^{L}_{ij} 1\).

The available space of valid secondary structures is all _symmetric_ matrices \(\{0,1\}^{L L}\) that satisfy the above three constraints. The first two constraints can be satisfied easily. We define a constraint matrix \(}\) as: \(}_{ij}:=1\) if \(x_{i}x_{j}\) and \(|i-j| 4\), and \(}_{ij}:=0\) otherwise. By element-wise multiplication of the network output and the constraint matrix \(}\), invalid pairs are masked.

The critical issue in obtaining a valid RNA secondary structure is the third constraint, i.e., _processing the network output to create a symmetric binary matrix that only allows a single "1" to exist in each row and column_. There are different strategies for dealing with this issue.

Spot-Rnais a typical kind of method that imposes minor constraints. It takes the original output of neural networks \(\) and directly applies the \(\) function, assigning a value of 1 to those greater than 0.5 and 0 to those less than 0.5. This process can be represented as:

\[()=_{[()>0.5]} .\] (2)

Here, the offset term \(s\) has been set to 0.5. No explicit constraints are imposed, and no additional parameters \(_{g}\) are required.

E2Efoldformulates the problem with constrained optimization and introduces an intermediate variable \(}^{L L}\). It aims to maximize the predefined score function:

\[(},)=-s, (})-\|}\|_{1},\] (3)

where \((})=(}}+(}})^{T})}\) ensures the output is a symmetric matrix that satisfies the constraints (a-b), \(s\) is an offset term that is set as \((9.0)\) here, \(,\) denotes matrix inner product and \(\|}\|_{1}\) is a \(_{1}\) penalty term to make the matrix to be sparse.

The constraint (c) is imposed by requiring Eq. 3 to satisfy \((})\). Thus, Eq. 3 is rewritten as:

\[(},)=_{ 0}-s,(})-\| }\|_{1}-,((})-),\] (4)

where \(^{L}_{+}\) is a Lagrange multiplier.

Formally, this process can be represented as:

\[_{_{g}}()=(_{} ^{L L}}(},)).\] (5)

Though three constraints are explicitly imposed in E2Efold, this method requires iterative steps to approximate the valid solutions and cannot guarantee that the results are entirely valid. Moreover, it needs a set of parameters \(_{g}\) in this processing, making tuning the model complex.

## 4 RFold

### Decoupled Optimization

We propose the following formulation for the constrained optimization problem in RNA secondary structure problem:

\[_{}-(^{T}})\] \[_{j=1}^{L}_{ij} 1, i;\ _{i=1}^{L}_{ij} 1,  j,\] (6)

where \((^{T}})=_{i=1}^{L}_{j=1}^{L}_{ij }}_{ij}\) represents the trace operation. The matrix \(}\) is symmetrized based on the original network output \(\) while satisfying the constraints (a-b) in Sec. 3.2 by multiplying the constraint matrix \(}\), i.e., \(}=(^{T})}\).

We then propose to decouple the optimization process into row-wise and column-wise optimizations, and define the corresponding selection schemes as \(S_{r}\) and \(S_{c}\) respectively:

\[S_{r}=\{S_{r}^{1},S_{r}^{2},...,S_{r}^{L}\},\ S_{c}=\{S_{c}^{1},S_{c}^{2},...,S_ {c}^{L}\},\] (7)

where \(S_{r}^{i}\{0,1\}^{L}\) signifies the selection scheme on the \(i\)th row, and \(S_{c}^{j}\{0,1\}^{L}\) represents the selection scheme on the \(j\)th column. The score function is defined as:

\[(S_{r},S_{c},})=-(^{T}}),\] (8)

where \(S_{r},S_{c}\) constitute the decomposition of \(\). The goal of the score function is to maximize the dot product of \(\) and \(}\) in order to select the maximum value in \(}\). Our proposed decoupled optimization reformulates the original constrained optimization problem in Equation 6 as follows:

\[_{S_{r},S_{c}}\ (S_{r},S_{c})\] (9) \[\ _{i=1}^{L}S_{r}^{i} 1, i;\ _{j=1}^{L}S_{c}^{j} 1,  j.\]

If the corresponding \(}_{ij}\) have the highest score in its row \(\{}_{ik}\}_{k=1}^{L}\) and its column \(\{}_{kj}\}_{k=1}^{L}\), then \(_{ij}=1\). By exploring the optimal \(S_{r}\) and \(S_{c}\), the chosen base pairs can be obtained by the optimal scheme \(S=S_{r} S_{c}\).

### Row-Col Argmax

With the proposed decoupled optimization, the optimal matrix can be easily obtained using the variant Argmax function:

\[(})=(}) (})\] (10)

where \(\) and \(\) are row-wise and column-wise Argmax functions respectively:

\[_{ij}(}) =1,\;\;\{}_{ik}\}_{k=1 }^{L}=}_{ij},\\ 0,\;.\] (11) \[_{ij}(}) =1,\;\;\{}_{kj}\}_{k=1 }^{L}=}_{ij},\\ 0,\;.\]

**Theorem 1**.: Given a symmetric matrix \(}^{L L}\), the matrix \((})\) is also a symmetric matrix.

Proof.: See Appendix C.1.

As shown in Fig. 3, taking a random symmetric \(6 6\) matrix as an example, we show the output matrics of \(\), \(\), and \(\) functions, respectively. The Row-Col Argmax selects the value that has the maximum value on both its row and column while keeping the output matrix symmetric.

From Theorem 1, we can observe that \((})\) is a symmetric matrix that satisfies the constraint (c). Since \(}\) already satisfies constraints (a-b), the optimized output is:

\[()=S_{r} S_{c}=(}),\] (12)

where \(S_{r},S_{c}=_{S_{r},S_{c}}-(S_{r},S_{c})\).

### Row-Col Softmax

Though the Row-Col Argmax function can obtain the optimal matrix \(()\), it is not differentiable and thus cannot be directly used in the training process. In the training phase, we need to use a differentiable function to approximate the optimal results. Therefore, we propose using a Row-Col Softmax function to approximate the Row-Col Argmax function for training. To achieve this, we perform row-wise Softmax and column-wise Softmax on the symmetric matrix \(}\) separately, as shown below:

\[_{ij}(}) =}_{ij})}{_{k=1}^{L}(}_{ik})},\] (13) \[_{ij}(}) =}_{ij})}{_{k=1}^{L}(}_{kj})}.\]

The Row-Col Softmax function is then defined as follows:

\[(})=(( })+(})),\] (14)

Note that we use the average of \((})\) and \((})\) instead of the element product as shown in Equ. 10 for the convenience of optimization.

Figure 3: The visualization of the \(\) function.

**Theorem 2**.: Given a symmetric matrix \(}^{L L}\), the matrix \(\)-\(\)-\((})\) is also a symmetric matrix.

Proof.: See Appendix C.2.

As shown in Fig. 4, taking a random symmetric \(6 6\) matrix as an example, we show the output matrics of \(\)-\(\), \(\)-\(\), and \(\)-\(\)-\(\) functions, respectively. It can be seen that the output matrix of \(\)-\(\)-\(\) is still symmetric. Leveraging the differentiable property of \(\)-\(\)-\(\), the model can be easily optimized.

In the training phase, we apply the differentiable Row-\(\) Softmax activation and optimize the mean square error (MSE) loss function between \(()\) and \(\):

\[((),)=}\|(})-\|^{2}.\] (15)

### Seq2map Attention

To simplify the pre-processing step that constructs hand-crafted features based on RNA sequences, we propose a Seq2map attention module that can automatically produce informative representations. We start with a sequence in the one-hot form \(^{L 4}\) and obtain the sum of the token embedding and positional embedding as the input for the Seq2map attention. For convenience, we denote the input as \(^{L D}\), where \(D\) is the hidden layer size of the token and positional embeddings.

Motivated by the recent progress in attention mechanisms , we aim to develop a highly effective sequence-to-map transformation based on pair-wise attention. We obtain the query \(^{L D}\) and key \(^{L D}\) by applying per-dim scalars and offsets to \(\):

\[=_{Q}+_{Q},\ =_{K}+_{K},\] (16)

where \(_{Q},_{K},_{Q},_{K}^{L D}\) are learnable parameters.

Then, the pair-wise attention map is obtained by:

\[}=^{2}(^{T}/L),\] (17)

where \(^{2}\) is an activation function that can be recognized as a simplified Softmax function in vanilla Transformers . The output of Seq2map is the gated representation of \(}\):

\[}=}\ \ (}),\] (18)

where \(()\) is the \(\) function that performs as a gate operation.

As shown in Fig. 5, we identify the problem of predicting \(^{L L}\) from the given sequence attention map \(}^{L L}\) as an image-to-image segmentation problem and apply the U-Net model architecture to extract pair-wise information.

Figure 4: The visualization of the \(\)-\(\)-\(\) function.

Figure 5: The overview model of RFold.

Experiments

We conduct experiments to compare our proposed RFold with state-of-the-art and commonly used methods in the field of RNA secondary structure prediction. Multiple experimental settings are taken into account, including standard RNA secondary structure prediction, generalization evaluation, large-scale benchmark evaluation, and inference time comparison. Ablation studies are also presented.

DatasetsWe use three benchmark datasets: (i) RNAStralign , one of the most comprehensive collections of RNA structures, is composed of 37,149 structures from 8 RNA types; (ii) ArchiveII , a widely used benchmark dataset in classical RNA folding methods, containing 3,975 RNA structures from 10 RNA types; (iii) bpRNA , is a large scale benchmark dataset, containing 102,318 structures from 2,588 RNA types.

BaselinesWe compare our proposed RFold with baselines including energy-based folding methods such as Mfold , RNAsoft , RNAfold , RNAstructure , CONTRAfold , Contextfold , and LinearFold ; learning-based folding methods such as SPOT-RNA , Externafold , E2Efold , MXfold2 , and UFold .

MetricsWe evaluate the performance by precision, recall, and F1 score, which are defined as:

\[=}{+},\ =}{+},\ =2\ }{+},\] (19)

where \(,\), and \(\) denote true positive, false positive and false negative, respectively.

Implementation detailsFollowing , we train the model for 100 epochs with the Adam optimizer. The learning rate is 0.001, and the batch size is 1 for sequences with different lengths.

### Standard RNA Secondary Structure Prediction

Following , we split the RNAStralign dataset into training, validation, and testing sets by stratified sampling to ensure every set has all RNA types. We report the experimental results in Table 1. It can be seen that energy-based methods achieve relatively weak F1 scores ranging from 0.420 to 0.633. Learning-based folding algorithms like E2Efold and UFold can significantly improve performance by large margins, while RFold obtain even better performance among all the metrics. Moreover, RFold obtains about 8% higher precision than the state-of-the-art method. This phenomenon suggests that our proposed decoupled optimization is strict to satisfy all the hard constraints for predicting valid structures.

### Generalization Evaluation

To verify the generalization ability of our proposed RFold, we directly evaluate the performance on another benchmark dataset ArchiveII using the pre-trained model on the RNAStralign training dataset. Following , we exclude RNA sequences in ArchiveII that have overlapping RNA types with the RNAStralign dataset for a fair comparison. The results are reported in Table 2.

It can be seen that traditional methods achieve F1 scores in the range of 0.545 to 0.842. Among the state-of-the-art methods, RFold attains the highest F1 score. It is noteworthy that RFold has a relatively lower recall metric and significantly higher precision metric. This phenomenon may be due to the strict constraints imposed by RFold. Although none of the current learning-based methods can meet all the constraints presented in Sec. 3.2, the predictions made by RFold are guaranteed to be valid. Therefore, RFold may cover fewer pairwise interactions, resulting in a lower recall metric. Nonetheless, the highest F1 score indicates the excellent generalization ability of RFold.

   Method & Precision & Recall & F1 \\  Mfold & 0.450 & 0.398 & 0.420 \\ RNAStrald & 0.516 & 0.568 & 0.540 \\ RNAstructure & 0.537 & 0.568 & 0.550 \\ CONTRAfold & 0.608 & 0.663 & 0.633 \\ LinearFold & 0.620 & 0.606 & 0.609 \\ CDPfold & 0.633 & 0.597 & 0.614 \\ E2Efold & 0.866 & 0.788 & 0.821 \\ UFold & 0.905 & 0.927 & 0.915 \\  RFold & **0.981** & **0.973** & **0.977** \\   

Table 1: Results on RNAStralign test set. Results in bold and underlined are the top-1 and top-2 performances, respectively.

### Large-scale Benchmark Evaluation

The large-scale benchmark dataset bpRNA has a fixed training set (TR0), evaluation set (VL0), and testing set (TS0). Following , we train the model in bpRNA-TR0 and evaluate the performance on bpRNA-TS0 by using the best model learned from bpRNA-VL0. We summarize the evaluation results in Table 3. It can be seen that RFold significantly improves the previous state-of-the-art method SPOT-RNA by 4.0% in the F1 score.

Following , we conduct an experiment on long-range interactions. The bpRNA-TS0 dataset contains more versatile RNA sequences of different lengths and various types, which can be a reliable evaluation. Given a sequence of length \(L\), the long-range base pairing is defined as the paired and unpaired bases with intervals longer than \(L/2\). As shown in Table 4, RFold performs unexpectedly well on these long-range base pairing predictions. We can also find that UFold performs better in long-range cases than the complete cases. The possible reason may come from the U-Net model architecture that learns multi-scale features. RFold significantly improves UFold in all the metrics by large margins, demonstrating its strong predictive ability.

### Inference Time Comparison

We compared the running time of various methods for predicting RNA secondary structures using the RNAStarlign testing set with the same experimental setting as in . The results are presented in Table 5, which shows the average inference time per sequence. The fastest energy-based method is LinearFold, which takes an average of about 0.43s for each sequence. The previous learning-based baseline, UFold, takes about 0.16s. RFold has the highest inference speed, costing only about 0.02s per sequence. In particular, RFold is about eight times faster than UFold and sixteen times faster than MXfold2. The fast inference time of RFold is due to its simple sequence-to-map transformation.

   Method & Precision & Recall & F1 \\  Mfold & 0.668 & 0.590 & 0.621 \\ RNAfold & 0.663 & 0.613 & 0.631 \\ RNAstructure & 0.664 & 0.606 & 0.628 \\ CONTRAfold & 0.696 & 0.651 & 0.665 \\ LinearFold & 0.724 & 0.605 & 0.647 \\ RNAsoft & 0.665 & 0.594 & 0.622 \\ Eternalfold & 0.667 & 0.622 & 0.636 \\ E2Efold & 0.734 & 0.660 & 0.686 \\ SPOT-RNA & 0.743 & 0.726 & 0.711 \\ MXfold2 & 0.788 & 0.760 & 0.768 \\ Contextfold & 0.873 & 0.821 & 0.842 \\ UFold & 0.887 & **0.928** & 0.905 \\  RFold & **0.938** & 0.910 & **0.921** \\   

Table 2: Results on ArchiveF1 dataset.

   Method & Precision & Recall & F1 \\  Mfold & 0.315 & 0.450 & 0.356 \\ RNAfold & 0.304 & 0.448 & 0.350 \\ RNAstructure & 0.299 & 0.428 & 0.339 \\ CONTRAfold & 0.306 & 0.439 & 0.349 \\ LinearFold & 0.281 & 0.355 & 0.305 \\ RNAsoft & 0.310 & 0.448 & 0.353 \\ Externafold & 0.308 & 0.458 & 0.355 \\ SPOT-RNA & 0.361 & 0.492 & 0.403 \\ MXfold2 & 0.318 & 0.450 & 0.360 \\ Contextfold & 0.332 & 0.432 & 0.363 \\ UFold & 0.543 & 0.631 & 0.584 \\  RFold & **0.803** & **0.765** & **0.701** \\   

Table 4: Results on long-range bpRNA-TS0 set.

   Method & Time \\  CDPfold (Tensorflow) & 300.11 s \\ RNAstructure (C) & 142.02 s \\ CONTRAfold (C++) & 30.58 s \\ Mfold (C) & 7.65 s \\ Eternalfold (C++) & 6.42 s \\ RNAsoft (C+) & 4.58 s \\ RNAfold (C) & 0.55 s \\ LinearFold (C++) & 0.43 s \\ SPOT-RNA(Pytorch) & 77.80 s (GPU) \\ E2Efold (Pytorch) & 0.40 s (GPU) \\ MXfold2 (Pytorch) & 0.31 s (GPU) \\ UFold (Pytorch) & 0.16 s (GPU) \\  RFold & **0.02 s** (GPU) \\   

Table 5: Inference time on the RNAStarlign.

   Method & Precision & Recall & F1 \\  Mfold & 0.501 & 0.627 & 0.538 \\ E2Efold & 0.140 & 0.129 & 0.130 \\ RNAstructure & 0.494 & 0.622 & 0.533 \\ RNAsoft & 0.497 & 0.626 & 0.535 \\ RNAfold & 0.494 & 0.631 & 0.536 \\ Contextfold & 0.529 & 0.607 & 0.546 \\ LinearFold & 0.561 & 0.581 & 0.550 \\ MXfold2 & 0.519 & 0.646 & 0.558 \\ Externafold & 0.516 & 0.666 & 0.563 \\ CONTRAfold & 0.528 & 0.655 & 0.567 \\ SPOT-RNA & 0.594 & **0.693** & 0.619 \\ UFold & 0.521 & 0.588 & 0.553 \\ RFold & **0.692** & 0.635 & **0.644** \\   

Table 3: Results on bpRNA-TS0 set.

### Ablation Study

Decoupled OptimizationTo validate the effectiveness of our proposed decoupled optimization, we conduct an experiment that replaces them with other strategies. The results are summarized in Table 6, where RFold-E and RFold-S denote our model with the strategies of E2Efold and SPOT-RNA, respectively. We ignore the recent UFold because it follows exactly the same strategy as E2Efold. We also report the validity which is a sample-level metric evaluating whether all the constraints are satisfied. Though RFold-E has comparable performance in the first three metrics with ours, many of its predicted structures are invalid. The strategy of SPOT-RNA has incorporated no constraint that results in its low validity. Moreover, its strategy seems to not fit our model well, which may be caused by the simplicity of our RFold model.

Seq2map AttentionWe also conduct an experiment to evaluate the proposed Seq2map attention. We replace the Seq2map attention with the hand-crafted features from UFold and the outer concatenation from SPOT-RNA, which are denoted as RFold-U and RFold-SS, respectively. In addition to performance metrics, we also report the average inference time for each RNA sequence to evaluate the model complexity. We summarize the result in Table 7. It can be seen that RFold-U takes much more inference time than our RFold and RFold-SS due to the heavy computational cost when loading and learning from hand-crafted features. Moreover, it is surprising to find that RFold-SS has a little better performance than RFold-U, with the least inference time for its simple outer concatenation operation. However, neither RFold-U nor RFold-SS can provide informative representations.

### Visualization

We visualize two examples predicted by RFold and UFold in Fig. 6. The corresponding F1 scores are denoted at the bottom of each plot. The first secondary structures is a simple example of a nested structure. It can be seen that UFold may fail in such a case. The second secondary structures is much more difficult that contains over 300 bases of the non-nested structure. While UFold fails in such a complex case, RFold can predict the structure accurately. Due to the limited space, we provide more visualization comparisons in Appendix D.

## 6 Conclusion

In this study, we present RFold, a simple yet effective learning-based model for RNA secondary structure prediction. We propose decoupled optimization to replace the complicated post-processing strategies while incorporating constraints for the output. Seq2map attention is proposed for sequence-to-map transformation, which can automatically learn informative representations from a single sequence without extensive pre-processing operations. Comprehensive experiments demonstrate that RFold achieves competitive performance with faster inference speed. We hope RFold can provide a new perspective for efficient RNA secondary structure prediction.

   Method & Precision & Recall & F1 & Validity \\  RFold & **0.981** & **0.973** & **0.977** & **100.00\%** \\  RFold-E & 0.888 & 0.906 & 0.896 & 50.31\% \\ RFold-S & 0.223 & 0.988 & 0.353 & 0.00\% \\   

Table 6: Ablation study on optimzation strategies (RNAStralign testing set).

   Method & Precision & Recall & F1 & Time \\  RFold & **0.981** & **0.973** & **0.977** & 0.0167 \\  RFold-U & 0.875 & 0.941 & 0.906 & 0.0507 \\ RFold-SS & 0.886 & 0.945 & 0.913 & **0.0158** \\   

Table 7: Ablation study on pre-processing strategies (RNAStralign testing set).

Figure 6: Visualization of the true and predicted structures.