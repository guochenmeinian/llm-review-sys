# OSWorld: Benchmarking Multimodal Agents for Open-Ended Tasks in Real Computer Environments

Tianbao Xie \({}^{h}\) Danyang Zhang \({}^{h}\) Jixuan Chen \({}^{h}\) Xiaochuan Li \({}^{h}\) Siheng Zhao \({}^{h}\) Ruisheng Cao \({}^{h}\)

Toh Jing Hua \({}^{h}\) Zhoujun Cheng \({}^{h}\) Dongchan Shin \({}^{h}\) Fangyu Lei \({}^{h}\) Yitao Liu \({}^{h}\) Yiheng Xu \({}^{h}\)

Shuyan Zhou \({}^{c}\) Silvio Savarese \({}^{e}\) Caiming Xiong \({}^{e}\) Victor Zhong \({}^{w}\) Tao Yu \({}^{h}\)

\({}^{h}\) The University of Hong Kong \({}^{c}\) Carnegie Mellon University

\({}^{s}\)Salesforce Research \({}^{w}\)University of Waterloo

###### Abstract

Autonomous agents that accomplish complex computer tasks with minimal human interventions can significantly enhance accessibility and productivity of human-computer interactions. Existing benchmarks either lack interactive environments or are limited to specific applications/domains, failing to reflect the diversity and complexity of real-world computer use and limiting agent scalability. We introduce OSWorld, the _first-of-its-kind scalable real computer environment_ for multimodal agents, supporting task setup, interactive learning, and execution-based evaluation of open-ended computer tasks across arbitrary applications in Ubuntu, Windows, and macOS. Using OSWorld, we create a benchmark of 369 tasks involving real web and desktop apps in open domains, OS file I/O, and multi-app workflows. Each example derives from real-world use cases and includes detailed setup and execution-based evaluation for reproducibility. Extensive evaluation of state-of-the-art LLM/VLM agents on OSWorld reveals deficiencies in their ability to serve as computer assistants. While humans accomplish 72.4% of the tasks, the best agents achieve <12.2%, struggling with GUI grounding and operational knowledge. Comprehensive analysis using OSWorld provides valuable insights for developing multimodal generalist agents that were not possible with previous benchmarks. Implementation and experiments are at https://os-world.github.io.

## 1 Introduction

Humans interact with computers to perform essential tasks in the digital realm, including web browsing, video editing, file management, data analysis, and software development. These task workflows often involve multiple applications through graphical user interfaces (GUI) and command line interfaces (CLI). Autonomous agents powered by large vision-language models (VLMs) can revolutionize how we interact with computer environments . By following natural language instructions, these agents can make computers more accessible and vastly increase human productivity.

A major challenge in developing multimodal agents is the absence of a benchmark that covers interactive, diverse, and complex real-world computer use across operating systems, interfaces, and applications. Prior benchmarks that provide demonstration datasets without executable environments  assume a single solution for each task and limit potential research in interactive learning and real-world exploration. Prior work with executable environments simplify the observation and action spaces of agents and limit task scopes to specific applications/domains such as navigation of specific websites , coding  and the combination . These restricted environments do not fully reflect real-world computer use, as they do not evaluate scenarios that require navigating between applications and interfaces in open domains (Fig. 1).

To address this gap, we introduce OSWorld, the _first-of-its-kind scalable, real computer environment_ for developing multimodal agents capable of executing a wide range of real computer tasks beyond isolated interfaces and applications. This executable environment allows free-form raw keyboard and mouse control of real computer applications and supports initial task state configuration, execution-based evaluation, and interactive learning across mainstream operating systems (Ubuntu, Windows, macOS). OSWorld enables evaluation of _open-ended_ computer tasks that involve arbitrary applications, ranging from image viewing to software functionality integration and programming (Fig. 1). OSWorld serves as a unified, real computer environment that allows users to define their agent tasks without the need to build application/domain-specific simulated environments.

Building upon OSWorld, we create a benchmark with 369 real-world tasks that involve widely-used web and desktop apps in open domains, OS file I/O, and multi-app workflows through both GUI and CLI. Each example is based on real-world use cases and often requires interactions with multiple applications and interfaces. To ensure reliable, reproducible evaluation, 9 authors with computer science backgrounds carefully annotate each example with an initial state setup configuration to simulate in-progress human work and a custom execution-based evaluation script to verify task completion. Our benchmark has 134 unique evaluation functions -- significantly larger than prior work  -- showcasing the complexity, diversity, and evaluation challenges of tasks in our benchmark. The human performance study indicates that task examples from OSWorld are more time-consuming and challenging compared to those in prior work.

We evaluate state-of-the-art LLM and VLM agent baselines, including GPT-4V , Gemini [54; 45], Claude-3 Opus  and Qwen-Max , as well as Mixtral , Llama-3  and CogAgent  from the open-source community. The performance of these experiments ranges from 0.99% to 12.24%, with subsets of applications reaching 0%, for workflow tasks that involve cooperation from multiple apps, the highest performance of the baseline agent is 6.57%. This indicates that current LLMs and VLMs are far from capable of serving as computer assistants (SS4.2). Results also show that while additional knowledge such as the accessibility tree and Set-of-Mark (SS4.1) can be helpful, it can also lead to potential misguidance and varies across models. Finally, we find that VLM-based agents struggle to ground screenshots to predict precise coordinates for actions, tend to predict repetitive actions, are unable to handle noise from unexpected application windows and exhibit limited knowledge of basic GUI interactions and domain-specific features of apps (SS5, SSD.4).

Figure 1: OSWorld is a _first-of-its-kind scalable real computer env_ for multimodal agents, supporting task setup, execution-based evaluation, and interactive learning across systems. It is a unified environment for evaluating _open-ended_ computer tasks that involve arbitrary apps. Using OSWorld, we create a benchmark of 369 real-world tasks with reproducible setup and evaluation scripts.

## 2 OSWorld Environment

In this section, we will introduce the task definition of autonomous agents, the components and implementation of the OSWorld environment, and the supported observation and action spaces.

### Task Definition

An autonomous digital agent task can be formalized as a Goal-Augmented Partially Observable Markov Decision Process (GA-POMDP) \((,,,,,r,,_{0}, ,p_{g},)\) where \(\) is the full state space (including hidden system states), \(\) is the observation space (SS2.3, what's visible or accessible to the agent), \(\) is the action space (SS2.4), \(:\) is the transition function, \(\) is the observation function, \(r:\) is the reward function, \(\) is the discount factor, \(_{0}\) is the initial state distribution, \(\) is the space of goals (instructions in our case), \(p_{g}\) is the distribution of desired goals (instructions), and \(:\) is a mapping function from observations to goals. Given current observation \(o_{t}\) (a natural language instruction observation and a screenshot, accessibility a1ly tree, or their combination according to facilities available), an agent generates executable action \(a_{t}\) (_e.g._, clicking on the certain pixel of the screen --.click(300, 540, button='right'), press key combination --.hotkey('ctrl', 'alt', 't')), which results in a new state \(s_{t+1}\) (_e.g._, current computer state) and a new partial observation \(o_{t+1}\). The interaction loop repeats until an action that marks termination (DONE or FAIL, see Sec. 2.4) is generated or the agent reaches the max number of steps (_e.g._, 15 in our experiments). OSWorld implements an execution-based reward function \(r:\). The reward function awards a value of 1 or a positive decimal under 1 at the final step if the state transitions meet the expectations of the task objective (i.e., the goal is successfully achieved or partially achieved), or if the agent accurately predicts failure for an infeasible task. In all other scenarios, it returns 0.

### Real Computer Environment Infrastructure

OSWorld is an executable and controllable environment that supports task initialization, execution-based evaluation, and interactive learning in real operating systems (_e.g._, Ubuntu, Windows, macOS) using virtual machines (VMs), shown in Fig. 2. VMs offer safe, isolated, and resettable/reversible (via snapshotting) environments that prevent irreversible damage to the real host machine.

InitializationPrior to agent interactions, we initializing the VM environment via config file. This includes downloading files, opening software, and adjusting interface layout. Many real-world assistance scenarios occur not at the beginning of digital activities, such as right after launching an application or starting the computer, but rather at intermediate stages, such as when certain software

Figure 2: Overview of OSWorld infrastructure. The environment uses a config file for initializing tasks (red), agent interaction, post-processing upon agent completion (orange), retrieving files and information (yellow), and executing the evaluation function (highlighted in green). Environments can run in parallel on a single host machine for learning or evaluation. Headless operation is supported.

is already open or the computer has experienced a crash. Therefore, we reproduce these intermediate states during the initialization to replicate real-world scenarios. See B.5 for more details.

EvaluationAfter agent interactions, we post-process the environment during the evaluation phase. This includes activating certain windows, saving some files for easy retrieval of information, and acquiring files and information for evaluation such as the final spreadsheet file for spreadsheet tasks, cookies for Chrome tasks. Finally, we apply the appropriate evaluation functions and parameters. We construct a vast collection of functions that make final wrangling and retrieve files and data information of varying types, categories, and granularities from the cloud and software from virtual machines as well as evaluation functions covering different aspects and their combinations, inputting this information as parameters to assess the outcomes. Tab. 1 illustrates evaluation processes including extracting cookie data, fetching files from both virtual machines and cloud services, retrieving the current interface's accessibility tree, and validating success by checking cookie deletions, table accuracy, and interface access. See more in B.6.

### Observation Space

The observation space in OSWorld contains the same **desktop screenshot** that human users perceive. This includes the mouse's position and shape, application windows, files, and folders that are opened in different sizes and orders. Also, similar to previous agent-building web and mobile research [34; 31; 12; 71] that provide and support the use of the webpage's DOM and app's view hierarchy, OSWorld also provides **accessibility (a11y) tree** which can support additional information for modeling. These raw observations allow rich interactions between multiple applications but induce challenges in long-horizon decision-making from high-resolution images (_e.g._, 4k screenshots) and structured long text (_e.g._, accessibility trees). A.2 describes observation space in more detail.

### Action Space

Action space \(\) in OSWorld encompasses all mouse and keyboard actions, including movement, clicks (left-key, right-key, multiple clicks), dragging, keystrokes, hotkeys, and others, covering all human-computer action space. Some action examples are shown on the left and the complete action list can be found in A.3. Timing is also crucial, as highlighted in previous studies on mobile devices , as well as the ability to determine whether a task is infeasible or completed. Therefore, we add three special actions named WAIT, FAIL, and DONE to enhance the aforementioned action spaces.

  

Previous efforts towards creating domain-specific agents, such as MiniWoB++ , CC-Net , and WebArena , have defined action spaces that include clicks and typing, as well as some actions specially designed for web browsing. However, they do not model all possible actions on a computer, leading to limitations when attempting actions like right-clicking and clicking with the ctrl key held to select items. This imposes an upper bound on agent learning capabilities.

## 3 OSWorld Benchmark

The OSWorld benchmark encompasses 369 real computing tasks defined and executed on Ubuntu, as well as 43 tasks on Windows. The environment preparation, annotation process, data statistics, and human performance are described in this section.

### Operating System and Software Environments

OSWorld supports the development of automated computer agents across real operating systems like Windows, macOS, and Ubuntu, focusing on Ubuntu for its open-source advantages and accessible APIs for comprehensive example creation and task evaluation. For Windows, we provide annotated examples targeting similar application functionalities. This framework is designed for open-domain tasks involving multiple applications and interfaces, such as GUIs and CLIs. It prioritizes a balanced benchmark across eight key applications: Chrome, VLC, Thunderbird, VS Code, LibreOffice suite (Calc, Writer, Impress), GIMP, and essential OS utilities (terminal, file manager, image viewer, PDF viewer), highlighting the need for varied operational skills, including commonsense reasoning, software navigation, and precise input control. Further details are available in B.1 and B.2.

### Tasks

We create a suite of 369 real-world computer tasks on Ubuntu from diverse sources such as forums, tutorials, and guidelines to demonstrate open-ended task creation within OSWorld. Each example is carefully annotated with a natural language instruction, a setup configuration with corresponding files and setup actions for environment initialization, and a manually crafted evaluation script to check if the task is successfully executed. We also adapt 43 tasks from the Ubuntu set for analytic usage on Windows. Overall, it took 9 computer science students (all student authors) over 3 months, consuming approximately 1800 man-hours (650 hours on single-app tasks, 750 hours on workflow tasks and 400 hours for double-checking).

Task instructions and scenariosWe collect realistic examples from diverse sources including official guidelines & tutorials, video pieces giving tips and tutorials on the Internet (TikTok,

   Function & Description \\  moveTo(x, y) & Moves the mouse to the specified coordinates. \\ click(x, y) & Clicks at the specified coordinates. \\ write(‘text’) & Types the specified text at the current cursor location. \\ prese(‘enter’) & Presses the Enter key. \\ hockey(‘ctrl’, ‘c’) & Deforms the Ctrl\(\) hockey combination (copy). \\ scroll(200) & Scrolls up by 200 units. \\ dragTo(x, y) & Drags the mouse to the specified coordinates. \\ keyDown(‘shift’) & Holds down the Shift key. \\ keyUp(‘shift’) & Releases the Shift key. \\ WAIT & Agent decides it should wait. \\ FAIL & Agent decides the task is infeasible. \\ DONE & Agent decides the task is finished. \\   

Table 2: Example mouse and keyboard actions \(\) in OSWorld. See App. A.3 for the complete list.

Figure 3: Distribution of task instructions in OSWorld by app domains and operation types.

YouTube), how-to websites (WikiHow), Q&A forums ( Reddit, Quora, Superuser, StackOverflow), formal video courses (Coursera, Udemy), and publicly-available personal blogs & guidelines. B.3 details resources used in our benchmark. We select examples by their popularity, helpfulness, and diversity, revealed by view counts and votes. While descriptions of single-application tasks are easily found, those of tasks that involve the coordination of multiple applications are scarce. Therefore, we authors combine existing examples and designed examples inspired by daily-life scenarios to compile the tasks. The instructions and task-related files are then crafted from these real-world guidelines and questions by the authors. After selection, each example is cross-checked by two other authors on the feasibility, ambiguity, and alignment with the source. We not only collect feasible tasks, but also tasks inherently infeasible due to feature deprecation or hallucinated features raised by real users, which results in 30 infeasible examples in our benchmark. Additionally, to demonstrate the unification ability of OSWorld for the creation of open-ended computer tasks, we also integrate 84 examples from other benchmarks focusing on single-application or domain-specific environments such as NL2Bash , Mind2Web , SheetCopilot , PPTC , and GAIA . Refer to B.4 for more details and B.7 for sampled examples for the showcase. A total of about 400 man-hours were spent to collect these examples. SS2.2 outlines the procedure for creating config and evaluation for tasks. Initial state design took 1 man-hour per example and is detailed in B.5. Evaluation design took two man-hour per example and is detailed in B.6.

Quality controlAfter annotation, each example is attempted by two authors who did not participate in its annotation, acting as agents to complete the task. This process evaluates the current example's quality and provides feedback to the annotators (such as unclear instructions or inability to complete the task, crashes in corner cases, serious instances of false positives and negatives, _etc._), and involves joint revisions and supplements. During experiments for human performance and baselines, we further fixed examples found to have issues, dedicating over 400 man-hours for four rounds of checks.

### Data Statistics

StatisticsTo facilitate analysis, we cluster the examples into software categories. Specifically, these categories include OS, Office (LibreOffice Calc, Impress, Writer), Daily (Chrome, VLC Player, Thunderbird), Professional (VS Code and GIMP), and Workflow (tasks involving multiple apps). The main statistics of OSWorld are presented in Tab. 3 and Fig. 3, showcasing the outline and a broad spectrum of tasks. Specifically, OSWorld contains a total of 369 tasks (and an additional 43 tasks on Windows for analysis), with the majority (268 tasks or 72.6%) aiming at single application functionalities and a section of workflow-related tasks (101 tasks or 27.4%). We also consider infeasible examples, totaling 30 tasks or 8.1% of the dataset. Additionally, a total of 84 tasks (22.8%) are integrated from related datasets. The final dataset incorporates 302 distinct initial states and 134 different evaluation scripts, underscoring the comprehensive approach towards evaluating the tasks' complexity and requirements. More statistic details are available in B.4.

Comparison with existing benchmarksTab. 4 compares OSWorld to prior benchmarks. First, instead of focusing on specific computer applications such as a browser [71; 12], OSWorld utilizes raw **multimodal** observations and keyboard/mouse actions used by humans, which are universal across different applications and allows the development of generalizable agents. Second, instead of providing static demonstrations, OSWorld **executable environment** supports agent exploration during learning and evaluation -- behavior critical in generalizing to new applications. Third, instead of focusing on interactions within a single application, OSWorld considers **cross-app** interactions found in real-world computer usage. Fourth, instead of limiting to a single task type with a success definition, OSWorld provides example-wise, **execution-based evaluation** for tasks. Specifically, the total of 134 unique execution-based evaluation functions in our benchmark is significantly more than previous work, demonstrating the complexity, diversity, and evaluation challenges of OSWorld tasks. Finally, instead of focusing on clean initialization, OSWorld tasks require operation from **intermediate initialization**, as is typical in realistic computer usage.

  
**Statistic** & **Number** \\  Total tasks (Ubuntu) & 369 (100\%) \\ - Multi-App Workflow & 101 (27\%) \\ - Single-App & 268 (73\%) \\ - Integrated & 84 (23\%) \\ - Infeasible & 30 (8\%) \\ Supp. tasks (Windows) & 43 \\  Initial States & 302 \\ Eval. Scripts & 134 \\   

Table 3: OSWorld statistic. Supp. refers to Windows tasks that are usable only after activation due to copyright.

### Human Performance

We conduct human evaluations on each example in our dataset, with annotators being computer science major college students who possess basic software usage skills but have not been exposed to the samples or software before. We recorded the time required to complete each example and whether their completion of the example was correct. For comparison, we also sampled 100 examples from WebArena  under the same evaluation setup.

As illustrated, tasks from our dataset generally required more time to complete, with a median completion time of 111.94 seconds (compared to 35.38 seconds in WebArena), and a significant number of examples distributed at 900 seconds or even more. In terms of accuracy, the human performance on our tasks was approximately 72.36%, significantly lower than the 88% observed on the pure web task dataset. These findings highlight the complexity and challenge of tasks in our dataset, which demand more time and effort. The lower accuracy rate further indicates that our tasks require a higher level of understanding and proficiency, underscoring the need for advanced models and techniques to tackle them effectively.

## 4 Benchmarking LLM and VLM Agent Baselines

In this section, we present the implementation details and experimental settings for several state-of-the-art LLM and VLM agent baselines on OSWorld benchmark, as well as their performance.

### LLM and VLM Agent Baselines

We evaluate state-of-the-art open-source LLMs and VLMs such as Mixtral  and Llama-3 , and closed-source ones such as GPT, Gemini, Claude on OSWorld. We also explore methods such as the Set-of-Marks aided approach [61; 14], which has been demonstrated to improve spatial capabilities for visual reasoning. For each method, we provide the 3 most recent observation-action pairs and generate actions with the temperature of 1.0 and top-p of 0.9. The prompts used in the experiments are provided in C.1. We request the agents to complete the tasks within a max step limit of 15, which is enough for most tasks. We present a summary of the results in Tab. 5 and analysis in

    & \# Instances & Control. & Environment & Multimodal & Cross- & Intermediate & \# Exec.-based \\  & (\# Templates) & Exec. Env.? & Scalability? & Support? & App? & Injit. State? & Eval. Func. \\  GAIA  & 466 & ✗ & - & ✗ & ✗ & 0 \\ Mind2Web  & 2350 & ✗ & - & ✓ & ✗ & ✓ & 0 \\ WebLINK  & 2337 & ✗ & - & ✓ & ✗ & ✓ & 0 \\ PixelHelp  & 187 & ✗ & - & ✓ & ✗ & ✗ & 0 \\ MetaColl  & 1125 & ✗ & - & ✓ & ✗ & ✓ & 0 \\ AirW  & 30\(k\) & ✗ & - & ✓ & ✗ & ✓ & 0 \\ ScreenAgent  & 70 & ✗ & - & ✓ & ✗ & ✓ & 0 \\ Omilact  & 9802 & ✗ & - & ✓ & ✗ & ✓ & 0 \\  AgentBench  & 1091 & Multi-isolated & ✗ & ✗ & ✗ & 7 \\ InterCode  & 1350 (3) & Code & ✗ & ✗ & ✗ & 3 \\ MiniWoB++  & 125 & Web & ✗ & ✓ & ✗ & 125 \\ WebArena  & 12\(k\) (1) & Web & ✗ & ✓ & ✗ & ✗ & 1 \\ VWebArena  & 910 (314) & Web & ✗ & ✓ & ✗ & ✗ & 6 \\ WORKArena  & 23\(k\) (29) & Web & ✗ & ✓ & ✗ & ✓ & 7 \\ WiklHOW  & 150 (16) & Mobile & ✗ & ✓ & ✗ & ✗ & 16 \\ AssistGUI  & 100 & ✗ & ✗ & ✓ & ✗ & ✓ & 2 \\  OSWorld & 369 & Computer & ✓ & ✓ & ✓ & ✓ & **134** \\   

Table 4: Comparison of different environments for benchmarking digital agents.

Figure 4: Human operation time and accuracy on OSWorld and WebArena.

Sec. 4.2. We implement the following four types of input settings on LLM and VLM: **Accessibility tree**, **Screenshot**, **Screenshot + accessibility tree**, and **Set-of-Marks**. Details see App. C.3.

### Results

LLMs and VLMs are still far from being digital agents on real computers.Table 5 shows that screenshots-only agents that generate keyboard/mouse actions via pyautogui achieve 5.26% to 5.80% success rate (VLMs GPT-4V, Gemini-Pro-vision) while the text-only agents using using a11y tree as input achieve 2.37% to 12.24% (LLMs GPT-4, GPT-4o). These results from state-of-the-art VLMs and LLMs significantly trail the performance of humans not familiar with the software (72.36%), which indicates further research is required to develop capable digital assistants. While Claude-3 Opus is competitive with GPT-4V on common benchmarks , it underperforms GPT-4V significantly as a digital agent in OSWorld. D.4 present qualitative analysis and infer reasons.

Agents have much higher variance than humans in different types of computer tasks.Tab. 5 shows that agent performance varies significantly across different software types, performing better on CLI-oriented interfaces (such as OS-type tasks) compared to GUI-oriented interfaces (such as Office tasks involving clicks on spreadsheet interfaces and document processing). Moreover, the CLI vs. GUI gap between models and settings is inconsistent, with some >20%. Similarly, performance on workflow-type tasks involving multiple software (<5%) significantly trails single software performance. Unlike agent performance, human performance is consistent across these

    &  &  \\   & & OS & Office & Daily & Profess. & **Workflow** & **Overall** \\  A11y tree & Mistral-8x7B & 12.50\% & 1.01\% & 4.79\% & 6.12\% & 0.09\% & 2.98\% \\  & Llama-3.70B & 4.17\% & 1.87\% & 2.71\% & 0.00\% & 0.93\% & 1.61\% \\  & GPT-3.5 & 4.17\% & 4.43\% & 2.71\% & 0.00\% & 1.62\% & 2.69\% \\  & GPT-4 & 20.83\% & 3.58\% & 25.64\% & 26.53\% & 2.97\% & **12.24\%** \\  & Gemini-Pro & 4.17\% & 1.71\% & 3.99\% & 4.08\% & 0.63\% & 2.37\% \\  & Gemini-Pro-1.5 & 12.50\% & 2.56\% & 7.83\% & 4.08\% & 3.60\% & 4.81\% \\  & Qwen-Max & 29.17\% & 3.58\% & 8.36\% & 10.20\% & 2.61\% & 6.87\% \\  & GPT-4o & 20.83\% & 6.99\% & 16.81\% & 16.33\% & **7.56\%** & 11.36\% \\  Screenshot & CogAgent & 4.17\% & 0.85\% & 2.71\% & 0.00\% & 0.00\% & 1.11\% \\  & GPT-4V & 12.50\% & 1.86\% & 7.58\% & 4.08\% & **6.04\%** & 5.26\% \\  & Gemini-ProV & 8.33\% & 3.58\% & 6.55\% & 16.33\% & 2.08\% & **5.80\%** \\  & Gemini-Pro-1.5 & 12.50\% & 6.99\% & 2.71\% & 6.12\% & 3.60\% & 5.40\% \\  & Claude-3-Opus & 4.17\% & 1.87\% & 2.71\% & 2.04\% & 2.61\% & 2.42\% \\  & GPT-4o & 8.33\% & 3.58\% & 6.07\% & 4.08\% & 5.58\% & 5.03\% \\  Screenshot & CogAgent & 4.17\% & 0.85\% & 2.71\% & 0.62\% & 0.09\% & 1.32\% \\ + A11y tree & GPT-4V & 16.66\% & 6.99\% & 24.50\% & 18.37\% & 4.64\% & **12.17\%** \\  & Gemini-ProV & 4.17\% & 4.43\% & 6.55\% & 0.00\% & 1.52\% & 3.48\% \\  & Gemini-Pro-1.5 & 12.50\% & 3.58\% & 7.83\% & 8.16\% & 1.52\% & 5.10\% \\  & Claude-3-Opus & 12.50\% & 3.57\% & 5.27\% & 8.16\% & 1.00\% & 4.41\% \\  & GPT-4o & 41.67\% & 6.16\% & 12.33\% & 14.29\% & **7.46\%** & 11.21\% \\  Set-of-Mark & CogAgent & 4.17\% & 0.00\% & 2.71\% & 0.00\% & 0.53\% & 0.99\% \\  & GPT-4V & 8.33\% & 8.55\% & 22.84\% & 14.28\% & **6.57\%** & **11.77\%** \\  & Gemini-ProV & 4.17\% & 1.01\% & 1.42\% & 0.00\% & 0.63\% & 1.06\% \\  & Gemini-Pro-1.5 & 16.67\% & 5.13\% & 12.96\% & 10.20\% & 3.60\% & 7.79\% \\  & Claude-3-Opus & 12.50\% & 2.72\% & 14.24\% & 6.12\% & 4.49\% & 6.72\% \\  & GPT-4o & 20.83\% & 3.58\% & 3.99\% & 2.04\% & 3.60\% & 4.59\% \\  Human Performance & 75.00\% & 71.79\% & 70.51\% & 73.47\% & 73.27\% & 72.36\% \\   

Table 5: Success rates of baseline LLM and VLM agents on OSWorld, grouped by task categories: OS, Office (LibreOffice Calc, Impress, Writer), Daily (Chrome, VLC Player, Thunderbird), Professional (VS Code and GIMP) and Workflow (tasks involving multiple apps), for gaining insights from interfaces and operation logic. See C.1 and C.6 for more details.

tasks, fluctuating around 70% with <5% deviation. This suggests that the way humans understand and complete tasks may differ significantly from the current logic and methods based on LLMs and VLMs.

A11y tree and SoM's effectiveness varies by models.The a11y tree contains some attribute information of visible elements, including window position and size, as well as some semantic labels of the window. The performance gap illustrated in Table 5 between GPT-4V and Claude-3 with additional a11y tree information and under a pure screenshot setup suggests that it still has significant room for improvement in accurately perceiving and reasoning GUI elements. Conclusions are reversed for Gemini-Pro. While applying SoM setting, there is a decline for GPT-4V in performance compared to directly providing the model with screenshots and a11y tree inputs, which contradicts the widely shown effectiveness of SoM in classic image understanding tasks , as well as in application areas like web agents . We speculate that this is due to the tasks performed within operating systems having higher resolution and much more elements, (_e.g._, the cells in a spread table), leading to a significant amount of noise that counteracts the auxiliary role of bounding boxes. Some tasks also require coordinate-level operations, which cannot be modeled by SoM bounding boxes.

VLM agents with screenshot-only setting show lower performance, but it should be the ultimate configuration in the long run.The setting that relies solely on screenshots exhibits the lowest performance, at only 5.26%, among all. Despite the performance, it is worth mentioning that this is the only configuration that does not require additional information, such as an a11y tree, making it concise and in alignment with intuitive human perception since the a11y tree may not be well-supported across all software or cannot be obtained under noisy conditions (_e.g._, when the agent is restricted to viewing the computer through peripheral screens), and the massive amount of tokens contained in the a11y tree (even just the leaf nodes can have tens of thousands of tokens) can also impose an additional inference burden on the model. Future work on purely vision-based agents could lead to stronger generalization capabilities, efficiency, and, ultimately, the potential for integration with the physical world on a larger scale.

## 5 Analysis

Higher resolution typically improves performanceDespite the availability of high-res displays, most VLMs are trained on far lower resolutions. We evaluate performance using screenshot-only and SoM by down-sampling the original resolution by 0.2-0.8 (Figure for 10% of examples on the right). The output coordinates of the model for the screenshot setting are still expected to align with the original resolution (_i.e._, 1080P). Resolution impact on performance is shown on the right (for a subset of 10% of examples). Screenshot-only performance improves with higher resolution, which may arise from the discrepancy between the downsampled input resolution and the coordinates of the output (which is for the original resolution). In contrast for SoM, a reduction to 768\(\)432 (down-sampling 0.4) improves performance and further reduction in resolution to 0.2 noticeably degrades performance.

Longer text-based trajectory history context improves performance, unlike screenshot-only history, but poses efficiency challengesWe include current and past N rounds of observations and actions in the constructed prompts (see App. C.1 for more details) to explore the impact of context length on agent performance. We set N to 1, 2, 3, and all where we put as much context as we can. The results (on 10% of examples) on the right show the performance increase with more history context for SoM. Future work on constructing models with enhanced capabilities for longer context support and understanding reasoning, improving model efficiency, and designing new agent architectures for efficient memory storage will have a significant impact on digital agents. However, we also note that the inclusion of additional trajectory history does not enhance performance under the pure screenshotsetting. This suggests that contemporary advanced VLMs might not be as adept at extracting robust contextual information from images as they are from textual data.

VLM agents struggle with perturbation of application windows size/position and irrelevant informationWe consider the best SoM setting and a subset of 28 OSWorld tasks that agents perform well on (with a success rate of 50.79%). At the beginning of each task, we introduce disturbances to the windows by 1) changing the _position_ of the window; 2) changing the _size_ of the window to the minimal; 3) opening some irrelevant software and maximizing them to _clutter_ the screen. We find that current agents are not robust to these superficial changes, which lead to performance drops of 60% and even 80%. Surprisingly, agents are able to switch windows to a certain degree but fail to maximize the window as an intermediate step. This suggests that while agents possess some capability to navigate between windows, they lack a comprehensive strategy for managing window states effectively.

## 6 Related Work

Benchmarks for multimodal agentsRecent research has made significant progress in evaluating multimodal agents, including physical world [49; 11; 8] and digital world [48; 15]. In the digital realm, interactive evaluation of agents primarily spans coding, web scenarios, and mobile applications. Prior work on coding provides frameworks and datasets for evaluating agents across programming languages and software engineering activities [62; 24; 28; 50]. Prior work on web agents develop platforms for interacting with websites through keyboard and mouse actions, as well as datasets focusing on open-ended web tasks and realistic web navigation [48; 34; 63; 12; 71; 26; 13]. Mobile device interaction research develop simulators for mobile UI interactions and platforms dedicated to InfoUI tasks [31; 52; 56; 55; 44; 66; 58; 65; 57]. Further, environments connecting to real computers and datasets for GUI grounding, albeit without interactive capability, have emerged [17; 10; 42; 25; 53]. Comprehensive task evaluation across different aspects also sees innovations [36; 40]. In contrast to prior work that address specific domains/applications/datasets, OSWorld facilitates the development of general-purpose digital agents that openly interact with OS. See Tab. 4 for comparison.

Vision-language models for multi-modal agentsMany existing works on GUI interaction utilize structured data such as HTML, accessibility trees, and view hierarchies as a grounding source [12; 19; 31; 41; 69; 51; 67; 71]. However, source code often tends to be verbose, non-intuitive, or inaccessible, which necessitates multi-modal/visual understanding. Prior work on multimodal models consider screenshots for interaction with websites [4; 16; 22; 27; 47] and mobile devices [21; 68]. Additionally, general-purpose foundation models [5; 30; 35; 72; 9] also demonstrate potential for multi-modal digital agents. Prompt-based reasoning methods [17; 20; 60; 70] have further improved digital agents for web pages, mobile apps, and desktops. This work evaluates state-of-the-art text, vision, and multi-modal methods, demonstrating that existing multi-modal models are far from capable computer agents.

## 7 Conclusion

OSWorld addresses critical gaps in existing interactive learning environments to advance the development of autonomous digital agents. By providing a rich, realistic setting that spans multiple operating systems, interfaces, and applications, OSWorld broadens the scope of tasks digital agents can perform, and enhances their potential for real-world applications. Despite the promise shown by advancements in vision-language models, evaluations within OSWorld reveal notable challenges in agents' abilities, particularly in GUI understanding and operational knowledge, pointing to essential areas for future research and development.