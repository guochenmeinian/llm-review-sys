# Frieren: Efficient Video-to-Audio Generation

Network with Rectified Flow Matching

 Yongqi Wang, Wenxiang Guo, Rongjie Huang, Jiawei Huang, Zehan Wang,

**Fuming You, Ruiqi Li, Zhou Zhao**

Zhejiang University

cyanbox@zju.edu.cn

Equal contribution.Corresponding author.

###### Abstract

Video-to-audio (V2A) generation aims to synthesize content-matching audio from silent video, and it remains challenging to build V2A models with high generation quality, efficiency, and visual-audio temporal synchrony. We propose Frieren, a V2A model based on rectified flow matching. Frieren regresses the conditional transport vector field from noise to spectrogram latent with straight paths and conducts sampling by solving ODE, outperforming autoregressive and score-based models in terms of audio quality. By employing a non-autoregressive vector field estimator based on a feed-forward transformer and channel-level cross-modal feature fusion with strong temporal alignment, our model generates audio that is highly synchronized with the input video. Furthermore, through reflow and one-step distillation with guided vector field, our model can generate decent audio in a few, or even only one sampling step. Experiments indicate that Frieren achieves state-of-the-art performance in both generation quality and temporal alignment on VGGSound, with alignment accuracy reaching 97.22%, and 6.2% improvement in inception score over the strong diffusion-based baseline. Audio samples and code are available at http://frieren-v2a.github.io.

## 1 Introduction

Recent advancements in deep generative models have significantly enhanced the quality and diversity of AI-generated content, including text , images , videos  and audios . Among various content-generation tasks, video-to-audio (V2A) generation aims to synthesize semantically relevant and temporally aligned audio from video frames. Due to its immense potential for application in film dubbing, game development, YouTube content creation and other areas, the task of V2A has attracted widespread attention.

A widely applicable V2A solution is expected to have outstanding performance in the following aspects: **1) audio quality**: the generated audio should have good perceptual quality, which is the fundamental requirement of the audio generation task; **2) temporal alignment**: the generated audio should not only match the content but also align temporally with the video frames. This has a significant impact on user experience due to keen human perception of audio-visual information; and **3) generation efficiency**: the model should be efficient in terms of generation speed and resource utilization, which affects its practicality for large-scale and high-throughput applications.

Currently, considerable methods have been proposed for this task, including GAN-based models , transformer-based autoregressive models , and a recent latent-diffusion-based model, Diff-Foley . However, these methods have not yet achieved a balanced and satisfactory performanceacross the above aspects. 1) For audio quality, early GAN-based models suffer from poor quality and lack practicality. Autoregressive and diffusion models make improvements in generation quality, but still leave room for further advancement. 2) For temporal alignment, autoregressive models lack the ability to align the generated audio with the video explicitly. And due to the difficulty of learning audio-visual alignment with the cross-attention-based conditional mechanism solely, Diff-Foley relies on additional classifier guidance to achieve good synchrony, which not only increases the model complexity but also leads to instability when reducing sampling steps. 3) For generation efficiency, autoregressive models suffer from high inference latency, while Diff-Foley requires considerable sampling steps to achieve good generation quality due to the curved sampling trajectories of diffusion models, increasing the temporal overhead in inference. In a nutshell, existing methods still leave significant room for improvement in performance.

In this paper, We introduce another generative modeling approach, namely rectified flow matching , into the V2A task. This method regresses the conditional transport vector field between noise and data distributions with as straight trajectories as possible, and conducts sampling by solving the corresponding ordinary differential equation (ODE). With simpler formulations, our rectified-flow-based model achieves higher audio quality and diversity. To improve temporal alignment, we adopt a non-autoregressive vector field estimator network with a feed-forward transformer with no temporal-dimension downsampling, thereby preserving temporal resolution. We also employ a channel-level cross-modal feature fusion mechanism for conditioning, leveraging the inherent alignment of audio-visual data and achieving strong alignment. These designs lead to high synchrony between generated audio and input video while upholding model simplicity. Moreover, through integrating reflow and one-step distillation techniques, our model can generate decent audio with a few, or even only one sampling step, significantly improving generation efficiency.

We name our model Frieren for efficient video-to-audio generation network with **r**ectified flow matching. Experiments indicate that Frieren outperforms strong baselines in terms of audio quality, generation efficiency, and temporal alignment on VGGSound , achieving a 6.2% improvement in inception score (IS) and a generation speed 7.3\(\) that of Diff-Foley, as well as temporal alignment accuracy of up to 97.22% in 25 steps. Additionally, Frieren combining reflow and distillation achieves alignment accuracy of up to 97.85% with just one step, with a 9.3\(\) acceleration compared to 25-step sampling, further boosting generation efficiency.

## 2 Related works

### Video-to-audio generation

Video-to-audio (V2A) generation aims to synthesize audio of which content matches the visual information of a video clip. RegNet  designs a time-dependent visual encoder to extract appearance and motion features, which are then fed to a GAN for audio generation. FoleyGAN  also utilizes GAN for audio generation from visual features, together with a predicted action category as the conditional input. SpecVQGAN  takes RGB and optical flow of videos and uses a transformer to generate indices of a spectrogram VQVAE autoregressively. Im2Wav  adopts two transformers for different temporal resolutions and takes CLIP  features as the condition to generate VQVAE indices. Du et al.  mimics the real-world foley methodology and introduces an additional reference audio as the condition. Diff-Foley  designs an audio-visual contrastive feature and adopts a latent diffusion to predict spectrogram latents, achieving decent audio quality and inference speed.

In addition to training a whole model from scratch, some works integrate off-the-shelf audio generation models with modality mappers or multimodal encoders with joint embedding space for conditioning. V2A-Mapper  uses a lightweight mapper to transfer CLIP embeddings of videos to CLAP  embeddings as the condition for audio generation. Xing et al.  utilize an ImageBind-based latent aligner for conditional guidance in audio generation. Despite the existence of plentiful works on V2A, there is still a large room left for improvement in quality, synchrony, and efficiency.

### Flow matching generative models

Flow matching  models the vector field of transport probability path from noise to data samples. Compared to score-based models like DDPM , flow matching achieves more stable and robust training together with superior performance. Specifically, rectified flow matching  learns the transport ODE to follow the straight paths connecting the noise and data points as much as possible, reducing the transport cost, and achieving fewer sampling steps with the reflow technique. This modeling paradigm has demonstrated excellent performance in accelerating image generation [22; 6].

In the area of audio generation, Voicebox  builds a large-scale multi-task speech generation model based on flow matching. Its successor, Audiobox , extends the flow-matching-based model to a unified audio generation model with natural language prompt guidance. Matcha-tts  trains an encoder-decoder TTS model with optimal-transport conditional flow matching. VoiceFlow  introduces rectified flow matching into TTS, achieving speech generation with fewer inference steps. However, for the task of V2A, there has been no exploration into utilizing flow matching models to enhance generation quality or inference efficiency.

## 3 Method

### Preliminary: rectified flow matching

We first introduce the basic principles of rectified flow matching (RFM)  that we build our model upon. Conditional generation problems like V2A can be viewed as a conditional mapping from a noise distribution \(_{0} p_{0}()\) to a data distribution \(_{1} p_{1}()\). This mapping can be further taken as a time-dependent changing process of probability density (a.k.a. flow), determined by the ODE:

\[=(,t|)t,t,\] (1)

where \(t\) represents the time position, \(\) is a point in the probability density space at time \(t\), \(\) is the value of the transport vector field (i.e., the gradient of the probability w.r.t \(t\)) at \(\), and \(\) is the condition. In our case, the condition \(\) is the visual features from the video frames, while the data \(_{1}\) is the compressed mel-spectrogram latent of the corresponding audio from a pre-trained autoencoder. The fundamental principle of flow matching generative model is to use a neural network \(\) to regress the vector field \(\) with the flow matching objective:

\[_{}()=_{t,p_{t}()}\|( ,t|;)-(,t|c)\|^{2},\] (2)

where \(p_{t}()\) is the distribution of \(\) at timestep \(t\). However, due to a lack of prior knowledge of target distribution \(p_{1}()\) and the forms of \(p_{t}\) and \(\), it is intractable to directly compute \((,t|c)\). As an alternative, conditional flow matching objective, which is proven in  to have identical gradient as eq. 2 w.r.t \(\), is used for regression:

\[_{}()=_{t,p_{1}(_{1}),p_{t}(|_{1})}\|(,t|;)-(,t|_{1},c) \|^{2}.\] (3)

Through designing specific probabilistic paths that enable efficient sampling from \(p_{t}(|_{1})\) and computing of \((,t|_{1},c)\), we achieve an unbiased estimation of \((,t|,c)\) with the CFM objective 3. Specifically, rectified flow matching attempts to establish straight paths between noise and data, aiming to facilitate sampling with larger step sizes and fewer steps. Given a noise-data pair \((_{0},_{1})\), \(\) is located at \((1-t)x_{0}+t_{1}\) at timestep \(t\), with the vector field being \((,t|_{1},c)=_{1}-_{0}\), pointing from the noise point to the data point. Hence, for each training step of the vector field estimator, we simply sample the data point \(_{1}\) and noise point \(_{0}\) from \(p_{1}()\) and \(p_{0}()\), respectively, and optimize the network with the rectified flow matching (RFM) loss

\[\|(,t|;)-(_{1}-_{0})\|^{2}.\] (4)

Once the vector estimator network finishes training, we can adopt various solvers to approximate the solution of the ODE \(=(,t|;)\) at discretized time steps for sampling. A simple and commonly used ODE solver is the Euler method:

\[_{t+}=+(,t|;)\] (5)

Figure 1: Illustration of the sampling process of our rectified-flow based V2A architecture.

where \(\) is the step size. The sampled latent is fed to the decoder of the spectrogram autoencoder for spectrogram reconstruction, and the result is further used to reconstruct the audio waveform with a vocoder. Figure 1 provides a simple demonstration of the model's sampling process.

### Model architecture

Model overviewWe illustrate the model architecture of Frieren at different levels in Figure 2. As shown in Figure 2(a), we first utilize a pre-trained visual encoder with frozen parameters to extract a frame-level feature sequence from the video. Usually, the video frame rate is lower than the temporal length per second of the spectrogram latent. To align the visual feature sequence with the mel latent at the temporal dimension for the cross-modal feature fusion mentioned below, we adopt a length regulator, which simply duplicates each item in the feature sequence by the ratio of the latent length per second and the video frame rate for regulation. The regulated feature sequence is then fed to the vector field estimator as the condition, together with \(\) and \(t\), to get the vector field prediction \(\).

Visual and audio representationsVarious audio-aligned visual representations [9; 25; 14; 38; 37; 39; 36] can potentially be applied to video-to-audio generation, and we conduct experiments with two types of visual representations. For a fair comparison with Diff-Foley , we mainly utilize the CAVP feature proposed in , which is a visual-audio contrastive feature considering both content and temporal alignment. Meanwhile, to investigate the impact of visual feature characteristics on model performance, we also attempt the visual feature from MAViL 3, which is an advanced self-supervised visual-audio representation learner that employs both masked-reconstruction and contrastive learning, and exhibits formidable performance in audio-visual understanding (See section 4.3.2 for comparison). For audio representation, we follow a previous text-to-audio work  to train a mel-spectrogram VAE with 1D convolution over the temporal dimension. Details of the VAE are provided in appendix A.

Vector field estimatorFigure 2(b) demonstrates the structure of the vector field estimator, which is composed of a feed-forward transformer and some auxiliary layers. The regularized visual feature \(\) and the point \(\) on the transport path are first processed by stacks of shallow layers separately, with output dimensions being both half of the transformer hidden dimension, and are then concatenated along the channel dimension to realize cross-modal feature fusion. This simple mechanism leverages the inherent alignment within the video and audio, achieving enforced alignment without relying on learning-based mechanisms such as attention. As a result, the generated audio and input video sequences exhibit excellent temporal alignment. After appending the time step embedding to the beginning, the sequence is added with a learnable positional embedding and is then fed into the feed-forward transformer. The structure of the transformer block is illustrated in Figure 2 (c), the design of which is derived from the spatial transformer in latent diffusion , with the 2D convolution layers replaced by 1D ones. The feed-forward transformer does not involve temporal downsampling,

Figure 2: Illustration of model architecture of Frieren at different levels.

thus preserving the resolution of the temporal dimension and further ensuring the preservation of alignment. The output of the stacked transformer blocks is then passed through a normalization layer and a 1D convolution layer to finally obtain the prediction of the vector field.

### Re-weighting RFM objective with logit-normal coefficient

The original RFM objective samples uniformly over time span \(\). However, for modeling the vector field, positions in the middle of the transport path (equivalent to time steps in the middle of ) present greater difficulty, as these positions are distant from both noise and data distributions. On the other hand, positions near the boundaries of the time span typically lie close to corresponding noise or data points, and their vector field direction tends to align with the lines connecting these points and the centroid of the distribution on the opposite side, and therefore relatively easy to regress. Upon this insight, we introduce time-based re-weighting to the original RFM objective, allocating more weight to intermediate time steps to achieve better modeling effectiveness. This is equivalent to increasing the sampling frequency of intermediate time steps. In practice, logit-normal weighting coefficients have been proven  to yield promising results, with the formula being

\[w(t)=}}{2})}.\] (6)

We re-weight the RFM objective with this weighting function to replace the original objective and observe in our experiment that this re-weighting helps to slightly improve audio quality and temporal alignment at the cost of a marginal decrease in audio diversity.

### Classifier-free guidance

Similar to diffusion-based models, we observe that classifier-free guidance (CFG) is highly important for generating audio that semantically matches and temporally aligns with the video. During training, we randomly replace the condition sequence \(\) with a zero tensor with a probability of 0.2, and during sampling, we modify the vector field using the formula

\[_{}(,t|;)=(,t|; )+(1-)(,t|;),\] (7)

where \(\) is the guidance scale trading off the sample diversity and generation quality, and \(_{}\) degenerates into the original vector field \(\) when \(=1\). We set \(\) to 4.5 in our major experiments.

### Reflow and one-step distillation with guided vector field

In this section, we introduce two techniques we adopt for reducing sampling steps. The first one is reflow, which is a crucial component of the rectified flow paradigm [21; 22]. Training the estimator network with objective 4 for once is insufficient to construct straight enough transport paths, and an extra reflow procedure is needed to strengthen the transport trajectories without altering the marginal distribution learned by the model, enabling sampling with larger step sizes and fewer steps. Given a model \(\) trained with RFM objective, the reflow procedure applies \(\) to conduct sampling over the entire training dataset to obtain sampled data \(}_{1}\) and save the corresponding input noise \(_{0}^{}\), finally obtaining triplets \((_{0}^{},}_{1},)\). The noise-data pair \((_{0},_{1})\) in the RFM objective 4 is replaced by \((_{0}^{},}_{1})\) for a secondary training of \(\). This process can be repeated multiple times to obtain straighter trajectories with diminishing marginal effects. We conduct reflow for once as it is sufficient for achieving straight enough trajectories.

While many rectified-flow-based models regress the same velocity field \(\) during both the initial training and the reflow process, we observe that when incorporating CFG, conducting sampling and reflow with the original vector field \(\) is ineffective in straightening the sampling trajectories with the guided vector field \(_{}\). Therefore, we use \(_{}\) for generating \(}_{1}\) and as the target of regression in reflow. The reflow objective can be written as:

\[_{}(^{})=_{t,p(_{0}^{ },}_{1}|),p_{t}(|_{0}^{},} _{1})}\|_{}(,t|;^{})-(}_{1}-_{0}^{})\|^{2}\] (8)

with same weighting function as eq. 6.

Upon the model \(^{}\) obtained from reflow, we further conduct one-step distillation [21; 22] to enhance the single-step generation performance of the model. As a type of self-distillation, this procedure triesto reduce the error between the single-step sampling result \(_{0}^{}+_{}(_{0}^{},t|;)\) and the multi-step sampling result \(}_{1}\). The objective function can be written as:

\[_{}(^{})=_{t,p(_{0 }^{},}_{1}|),p_{t}(|_{0}^{},}_{1})}\|_{0}^{}+_{}(_{0}^{},t|; ^{})-}_{1}\|^{2}\] (9)

Formally, the distillation objective 9 can be viewed as a reflow objective with the sampling timestep fixed at \(t=0\). We observe in the experiment that due to a limited number of sampling steps in reflow data generation, the model may experience a decrease in sampling quality after the reflow process. Therefore, we opt to use the same training data used in reflow for distillation, rather than re-sampling the training data with the reflow model, which is based on the theoretical basis that reflow does not alter the marginal distribution modeled by the estimator.

## 4 Experiments

### Experiment setup

Dataset and pre-processingFollowing most previous works, we take VGGSound  as the benchmark, which consists of 200k+ 10-second video clips from YouTube spanning 309 categories. Excluding videos already removed from YouTube, we follow the original train and test splits of VGGSound, the sizes of which are about 182.6k and 15.3k. We downsample the audios to 16kHz and transform them to mel-spectrogram with 80 bins and a hop size of 256. We follow  to downsample the videos to 4 FPS. Data samples are truncated to 8-second clips for training and inference.

Model configurationThe transformer of the vector field estimator mainly used in the experiments has 4 layers and a hidden dimension of 576. Each model is trained with 2 NVIDIA RTX-4090 GPUs. We train the estimator for 1.3M steps for the first training, and 600k and 500k steps for reflow and distillation, with the learning rate being 5e-5 for all stages. For waveform generation, we train a BigVGAN  vocoder on AudioSet . Details of model parameters are provided in appendix A.

MetricsWe combine objective and subjective metrics to evaluate model performance over audio quality, diversity, and temporal alignment. For objective evaluation, we calculate Frechet distance (FD), inception score (IS), Kullback-Leibler divergence (KL), Frechet audio distance (FAD), kernel inception distance (KID), and alignment accuracy (Acc). We utilize audio evaluation tools provided by AudioLDM , which are widely used in audio generation tasks, as well as the alignment classifier provided in . For metrics with reference like FAD, we duplicate the reference audio samples in the test set for 10 times as we generate 10 samples for each data item. For subjective evaluation, we conduct crowd-sourced human evaluations with 1-5 Likert scales and report mean-opinion-scores (MOS) over audio quality (MOS-Q) and content alignment (MOS-A) with 95% confidence intervals (CI). We sample 10 audios for each test video for evaluation. Details of subjective evaluation are provided in appendix B.

Baseline modelsWe adopt three advanced V2A models as baselines, including: 1) SpecVQGAN , a transformer-based autoregressive model generating spectrogram VQVAE indices from visual features; 2) Im2Way , a hierarchical autoregressive V2A model predicting audio VQVAE indices conditioned on CLIP features; and 3) Diff-Foley , a strong latent-diffusion-based V2A model. For SpecVQGAN, we evaluate two versions using RGB+Flow and ResNet features as input visual

   Model & FD\(\) & IS\(\) & KL\(\) & FAD\(\) & KID\((10^{-3})\,\) & Acc(\%) \(\) & MOS-Q\(\) & MOS-A\(\) \\  SpecVQGAN (R+F) & 31.69 & 5.23 & 3.37 & 5.42 & 8.53 & 61.83 & 3.30 \(\) 0.06 & 2.35 \(\) 0.05 \\ SpecVQGAN (RN50) & 32.52 & 5.21 & 3.41 & 5.39 & 9.00 & 56.92 & 3.25 \(\) 0.07 & 2.17 \(\) 0.05 \\ Im2Wav & 14.98 & 7.20 & **2.57** & 5.49 & 3.35 & 56.70 & 3.39 \(\) 0.06 & 2.29 \(\) 0.06 \\ Diff-Foley (CG \(\)) & 23.94 & 11.11 & 3.38 & 4.72 & 9.58 & 95.03 & 3.57 \(\) 0.08 & 3.74 \(\) 0.07 \\ Diff-Foley (CG \(\)) & 24.97 & 11.69 & 3.23 & 7.10 & 10.32 & 92.53 & 3.64 \(\) 0.07 & 3.59 \(\) 0.06 \\ LDM & 11.79 & 10.09 & 2.86 & 1.77 & **2.26** & 95.33 & 3.72 \(\) 0.05 & 3.79 \(\) 0.07 \\ Frieren & 12.26 & 12.42 & 2.73 & **1.32** & 2.49 & **97.22** & 3.78 \(\) 0.06 & **3.90 \(\) 0.05** \\ Frieren (Dopri5) & **11.64** & **12.76** & 2.75 & 1.37 & 2.39 & 96.87 & **3.81 \(\) 0.06** & 3.85 \(\) 0.06 \\   

Table 1: Results of V2A models on VGGSound dataset. R+F and RN50 denote the RGB+Flow and ResNet50 versions of SpecVQGAN, and CG denotes classifier guidance in Diff-Foley.

[MISSING_PAGE_FAIL:7]

distillation, together with trend graphs of IS and FAD in figure 4 for intuitive presentation. The data for reflow are generated with the Euler method for 25 steps. We observe an obvious drop in performance of Diff-Foley as well as Frieren without reflow when sampling with as few as 5 steps, and their scores become extremely poor when we further reduce the step number to 1. Figure 3 (b) (c) and (d) illustrate that the audio generated by these models as well as LDM degrades into unacceptably noisy or meaningless audio within one step. This is due to the convoluted nature of the sampling trajectories of these models, which disables them from sampling with large step sizes and few steps. We also notice that when sampling with 5 steps, using additional classifier guidance deteriorates the audio quality and synchrony of Diff-Foley, where alignment accuracy and IS drop by 18.0% and 1.72 respectively, while FD, KL, and KID increase by 9.47, 0.17, and 0.94 \( 10^{-3}\). This indicates the lack of robustness of the complex alignment mechanism that Diff-Foley relies on.

In contrast, Frieren with reflow achieves an alignment accuracy of up to 96.82% in just 5 steps, with significant advantages in quality, diversity, and subjective metrics. Additionally, it maintains an accuracy of 94.96% in single-step generation, as well as decent quality and diversity. This proves that reflow functions significantly in straightening the sampling trajectories, enabling the rectified flow model to generate decent audio with a small number of sampling steps. Furthermore, single-step distillation following reflow further improves the model performance with one step, with alignment accuracy reaching up to 97.85%, and KL, FAD, and KID being close to the 25-step results of Frieren trained once, with differences of 0.17, 0.53 and 0.42\( 10^{-3}\). It also achieves high MOS-Q and MOS-A of 3.48 and 3.93.

Figure 3 (e) and (f) show that results from Frieren with reflow and reflow+distillation have distinguishable spectrograms, with the latter showing higher quality and sharper edges. This fully demonstrates that the combination of reflow and one-step distillation endows our model with strong single-step generation capabilities, significantly enhancing the efficiency on the V2A task. Notice that reflow brings in some quality degradation in sampling with 25 steps. We speculate that this is because the limited number of sampling steps restricts the data quality when generating data for reflow, resulting in a shift in the marginal distribution learned by the model. This cumulative error might be mitigated by increasing the number of sampling steps during reflow data generation.

### Ablation study

#### 4.3.1 Model size of vector field estimator

We adjust the number of parameters of the vector field estimator and evaluate the model performance at different scales. We label the major model as "base", and obtain "small" and "large" models by decreasing and increasing the hidden dimension and / or the number of transformer layers, respectively. The parameter counts of the estimator and results are presented in table 3.

We observe that when the model parameters are reduced to 71M, performance declines across all metrics, where FD, KL, FAD, and KID increase by 0.76, 0.05, 0.18, and 0.3\( 10^{-3}\), and IS, alignment accuracy, MOS-Q and MOS-A drop by 0.26, 1.18%, 0.07 and 0.07, respectively. However, when the parameter number increases to 421M, there is a performance degradation across multiple metrics, with KL, FAD, and KID increasing by 0.03, 0.04, and 0.48\( 10^{-3}\), and IS, alignment acc declining by 0.13 and 2.06%. We speculate that this anomalous phenomenon may be due to the convergence difficulty

Figure 4: IS and FAD of the models with different steps.

   Model Size & FD\(\) & IS\(\) & KL\(\) & FAD\(\) & KID\((10^{-3})\) & Acc(\%) \(\) & MOS-Q\(\) & MOS-A\(\) \\  Small (70.90 M) & 13.02 & 12.16 & 2.78 & 1.50 & 2.79 & 96.04 & 3.71 \(\) 0.07 & 3.83 \(\) 0.06 \\ Base (158.88 M) & 12.26 & 12.42 & 2.73 & 1.32 & 2.49 & 97.22 & 3.78 \(\) 0.06 & 3.90 \(\) 0.05 \\ Large (421.12 M) & 12.20 & 12.29 & 2.76 & 1.36 & 2.97 & 95.16 & 3.78 \(\) 0.07 & 3.80 \(\) 0.06 \\   

Table 3: Ablation results on different model size of vector field estimator network.

for the larger model under similar training steps, or the redundant model capacity tends to cause overfitting on a relatively small dataset like VGGSound, deteriorating the model's generalization performance. In summary, we achieve relatively balanced model performance with the parameter of the estimator being around 160M. Details of model parameters are provided in appendix A.

#### 4.3.2 Visual feature characteristics

In table 4, we compare the results of Frieren using two different types of visual features from CAVP and MAViL. Intuitively, the MAViL feature should be more robust and contain richer audio-related semantic information, as it utilizes masked-reconstruction together with inter-modal and intra-modal contrastive learning, in contrast to CAVP trained solely with inter-modal contrastive learning. On the other hand, however, due to MAViL's convolutional downsampling in the temporal dimension, its feature sequence has a lower effective FPS of 2 with the same 4 FPS video input as CAVP. The results in the table indicate that the model with MAViL feature excels in audio diversity, with differences of FD, KL, and FAD being 0.18, 0.24, and 0.06. Meanwhile, it exhibits a 7.05% decrease in alignment accuracy and a 0.25 decrease in IS. This result yields two insights for V2A tasks: 1) at relatively low frame rates, the frame rate of features, rather than content, is more likely to become the bottleneck for audio quality and visual-audio synchrony; 2) compared to high video frame rates, the semantic information and robustness of visual features are more crucial for the diversity of generated audio.

#### 4.3.3 Classifier-free guidance scale

In figure 5, we illustrate the impact of various CFG scales on the performance of Frieren. In terms of audio diversity (FD, KL, KID, FAD), the metrics initially increase with the CFG scale, reaching an optimal value at around 2 and 3. After that, the metrics go down as the increasing CFG scale suppresses the diversity. For audio quality (IS) and temporal alignment, as larger scales make the content of the generated audio closer to the visual information, the metrics initially increase with the scale, reaching an optimal value between 4 and 4.5, and decrease after that due to audio distortion. We prioritize audio quality and synchrony and adopt a CFG scale of 4.5.

#### 4.3.4 Re-weighting RFM objective

We conduct ablation on RFM objective re-weighting and report the results in table 5. We can see that compared to the vanilla objective, introducing re-weighting results in improvements of 0.22 and 0.18% for IS and alignment accuracy. This validates the positive impact of objective re-weighting on audio quality and temporal alignment. On the other hand, objective re-weighting causes a decrease in audio diversity, with differences in FD, FAD, and KID being 0.31, 0.07, and 0.37\( 10^{-3}\), respectively.

   Type & Feat. FPS & FD\(\) & IS\(\) & KL\(\) & FAD\(\) & KID\((10^{-3})\) & Acc(\%) \(\) & MOS-Q\(\) & MOS-A\(\) \\  CAVP  & 4 & 12.26 & 12.42 & 2.73 & 1.32 & 2.49 & 97.22 & 3.78 \(\) 0.06 & 3.90 \(\) 0.05 \\ MAViL  & 2 & 12.08 & 12.17 & 2.49 & 1.26 & 2.52 & 90.17 & 3.75 \(\) 0.06 & 3.46 \(\) 0.07 \\   

Table 4: Results on different types visual features.

Figure 5: Model performance of Frieren under different CFG scales.

   Re-weighting & FD\(\) & IS\(\) & KL\(\) & FAD\(\) & KID\((10^{-3})\) & Acc(\%) \(\) & MOS-Q\(\) & MOS-A\(\) \\  ✗ & 11.95 & 12.20 & 2.73 & 1.25 & 2.12 & 97.04 & 3.74 \(\) 0.07 & 3.82 \(\) 0.06 \\ ✓ & 12.26 & 12.42 & 2.73 & 1.32 & 2.49 & 97.22 & 3.78 \(\) 0.06 & 3.90 \(\) 0.05 \\   

Table 5: Ablation results on RFM objective re-weighting.

Conclusion

In this paper, we propose Frieren, an efficient video-to-audio generation model based on rectified flow matching. We use a neural network to regress the conditional transport vector field with straight paths from noise to spectrogram latents, and conduct sampling by solving ODE, achieving better performance than diffusion-based and other V2A models. We adopt a vector field estimator based on a feed-forward transformer as well as channel-level cross-modal feature fusion to realize strong audio-video synchrony. Through a combination of reflow and one-step distillation, our model can generate high-quality audio with a few or even one sampling step, boosting the generation efficiency significantly. Experiments show that our model achieves state-of-the-art V2A performance on VGGSound. For future work, we will explore extending the model to larger scales and larger datasets to achieve V2A generation on a broader data domain. Besides, we will attempt audio generation from longer video sequences with variable lengths, rather than being limited to fixed-length short clips. These efforts aim to build a more versatile and widely applicable V2A model.