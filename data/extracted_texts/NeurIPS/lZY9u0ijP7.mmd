# Cascade Speculative Drafting for Even Faster LLM Inference

Ziyi Chen  Xiaocong Yang  Jiacheng Lin  Chenkai Sun

**Kevin Chen-Chuan Chang  Jie Huang**

University of Illinois at Urbana-Champaign

{ziyic2, kcchang, jeffhj}@illinois.edu

###### Abstract

Introduced to enhance the efficiency of large language model (LLM) inference, speculative decoding operates by having a smaller model generate a draft. A larger target model then reviews this draft to align with its output, and any acceptance by the target model results in a reduction of the number of the target model runs, ultimately improving efficiency. However, the drafting process in speculative decoding includes slow autoregressive generation and allocates equal time to generating tokens, irrespective of their importance. These inefficiencies collectively contribute to the suboptimal performance of speculative decoding. To further improve LLM inference, we introduce Cascade Speculative Drafting (CS Drafting), a speculative execution algorithm that incorporates two types of cascades. The _Vertical Cascade_ eliminates autoregressive generation from neural models, while the _Horizontal Cascade_ optimizes time allocation in drafting for improved efficiency. Combining both cascades, CS Drafting achieves greater speedup compared to the baselines in our experiments, while preserving the same output distribution as the target model.1

## 1 Introduction

The advent of Large Language Models (LLMs), like GPT-4 , has marked a significant milestone in the field of natural language processing (NLP). These models have not only excelled in various NLP tasks but have also found widespread applications in user-interactive settings, such as chatbots and virtual assistants. However, these applications involve an extremely high number of users, up to hundreds of millions daily. To serve in real-time at this scale, a low-latency system is not only cost-saving but also crucial for keeping the service running. In addition, the sheer scale of the service means that even a slight improvement in the latency of LLMs can greatly contribute to both the service provider and the community. Consequently, optimizing the latency of LLMs has become a critical area of research.

Unfortunately, the ever-growing size of LLMs significantly increases the latency, especially in long-form generation, as autoregressive LLMs generate tokens one by one. An emerging solution, known as speculative decoding [14; 4; 25], shows potential to mitigate this issue. In speculative decoding, a draft model (which is smaller and faster) generates \(k\) tokens in each step (with \(k\) being a hyperparameter) autoregressively, and these tokens are then reviewed by a target model (which is larger and slower) in parallel. In one single run, the target model will accept any tokens aligned with its output and further generate one token. The drafting process in speculative decoding enables the target model to generate multiple tokens in a single run while maintaining its output distribution unchanged. With a properly sized draft model, speculative decoding achieves a speedup of 2 to 3 times, making it a potential method for solving high latency issues.

However, since draft models are typically required to generate multiple tokens in multiple steps, where each generation still involves inefficient autoregressive decoding, the performance of speculative decoding could be limited by the drafting latency. This inefficiency is also indicated by Leviathan _et al._, where it was observed that very small models (e.g., around two orders of magnitude smaller than the target model) are usually the best choice for drafting because their inference cost is lower compared to that of a larger draft model, despite the fact that larger draft models usually have higher-quality generation. This underscores that improving drafting efficiency is crucial for further enhancing the performance of speculative decoding.

In light of this, one key strategy to address this bottleneck is to avoid the inefficient autoregressive generation of neural draft models. Based on this consideration, it is noted that statistical language models, such as bigram language models, incur negligible latency and computational resource costs compared to neural language models, owing to their simple structure. However, because the tokens generated by statistical language models usually do not have a high probability of being accepted by the target model, speculative decoding with statistical language models alone may not yield optimal results compared to using a well-sized neural language model from the same family as the draft model. Nonetheless, we notice that it is not necessary to use only one draft model in speculative decoding--statistical language models can serve as the "draft" model for the neural draft model, thereby eliminating autoregressive generation from the neural draft model.

Furthermore, our analysis in Figure 2 reveals a pattern during the drafting step: tokens generated later in the sequence by the draft model show a progressively lower probability of being accepted by the target model. This is because the probability of a token being accepted is conditioned on the acceptance of the previous tokens. It indicates that later tokens from draft models are more prone to rejection, contributing less to the expected number of accepted tokens per draft step, yet incurring the same latency.

Inspired by the above observations, we propose **Cascade Speculative Drafting (CS Drafting)**, a speculative execution algorithm that comprises multiple draft models, with the smallest being a statistical language model. Each neural draft model reviews generations from a smaller model and then proposes its reviewed content to either a larger draft model or the target model. In this design, the drafting of each neural model is accelerated by drafting from a smaller model, avoiding the inefficiency of autoregressive generation from neural models. We refer to this tiered speculative decoding approach as the _Vertical Cascade_. In addition, we suggest the use of smaller, faster draft models for generating high-rejection tokens that are trailing in drafting generation, forming the

Figure 1: The CS Drafting algorithm features a recursive and resource-efficient design, implemented through two cascades: the horizontal cascade and the vertical cascade. The horizontal cascade involves using larger draft models to generate the earlier tokens and smaller models for the later tokens. The vertical cascade requires each model to review drafts from smaller models with the exception of the smallest model, which is a statistical language model. As the horizontal cascade and vertical cascade are orthogonal, CS Drafting combines both approaches for optimal efficiency. The figure shows an example of Cascade Speculative Drafting with target model \(_{t}\) and draft models \(_{d_{1}}\), \(_{d_{2}}\), and \(_{d_{3}}\).

_Horizontal Cascade_. Along with the aforementioned _Vertical Cascade_, these strategies compose our complete CS Drafting approach, as illustrated in Figure 1.

Through theoretical analysis and empirical studies, we demonstrate that the CS Drafting algorithm outperforms speculative decoding in terms of latency across various tasks and settings, achieving an additional speedup of up to 81% over speculative decoding. These findings highlight the practical advantages and efficiency enhancements offered by both vertical and horizontal cascades.

The main contributions are summarized as follows:

* We introduce Cascade Speculative Drafting (CS Drafting), a speculative-execution-based algorithm that improves language model inference speed without sacrificing generation quality.
* We provide theoretical analyses supporting the effectiveness of the proposed CS Drafting approach.
* We conduct empirical experiments showing that CS Drafting achieves further speedup over speculative decoding across different tasks and settings.

## 2 Preliminary

The core concept of speculative decoding  involves the utilization of a small draft model for sequential token generation with validation by a larger target model resulting in reduced latency. This design accelerates sampling from autoregressive models without altering output distributions. At its heart, there are two key observations: 1) certain generations in language modeling are simpler than others and can be predicted by more efficient models correctly, and 2) using speculative execution along with a new sampling method enables faster, exact decoding from large models.

Specifically, let \(x\) be the input tokens at a run and \(_{t}\) and \(_{d}\) are the target and the draft model respectively, \(k\) be the number of draft tokens generated per step, and \(_{t}(x)[i]\) and \(_{d}(x)[i]\) be their probability output at \(i\)-th token when input is \(x\). We interpret speculative sampling as a two-stage operation. In the proposing stage, we sample \(\{x_{t+1},...,x_{t+k}\}\) from draft model \(_{d}\) autoregressively and append them to \(x\). In the reviewing stage, let \(x_{i}\{x_{t+1},...,x_{t+k}\}\) represents the token at the current position, and we accept it if \(_{d}(x)[i-1]_{t}(x)[i-1]\); in the event that \(_{d}(x)[i-1]>_{t}(x)[i-1]\), we reject \(x_{i}\) with a probability of \(1-_{t}(x)[i-1]}{_{d}(x)[i-1]}\) and proceed to resample \(x_{i}\) from a recalibrated distribution \(norm((0,_{t}(x)[i-1]-_{d}(x)[i-1]))\) and reject any token following \(x_{i}\). At the end, the target model will generate one additional token following the accepted tokens. Such a design guarantees the output is the same as sampling autoregressively using the target model alone .

Speculative decoding was empirically validated on various tasks and model sizes, demonstrating a significant acceleration in inference times (2x-3x faster) compared to standard implementations, without affecting the outputs. Importantly, it does not require task-specific training, altering model architectures, or changing training procedures, making it a practical solution for reducing the latency of LLM inference.

Figure 2: The probability of acceptance of draft tokens in relation to their positions in a single step of speculative decoding, evaluated on FLAN-T5-small, base, and large models on GSM8K and MMLU. The draft model generates 30 tokens at each step.

``` Require: draft models \(\{_{d_{1}},...,_{d_{n}}\}\), target mode \(_{t}\), \(prefix\), flag \(isFirstCall\), hyperparameters \(K_{nn}\), \(l\)  draftList \([_{d_{1}},...,_{d_{n}}]\) \(\) Initialize curGen and curProb.  curGen \( prefix\), curProbs \(\) a list of ones with the same length as prefix \(\) Unpack the a list of \(k\) for the current function call. \([k_{1},...,k_{n-1}]\) first row of \(K_{nn}\) \(\) Generate using MaG for the Base case of the recursive call. ifdraftList is empty then \(\) first element of draftList \(res(curGen)\) return\(res.generation,res.logits\) endif \(\) Perform the horizontal cascade with the for loop. for\(i 1\)to\(n\)do \(\) Prepare the arguments for the next recursive call. curTarget \(\) the \(i\)-th item of draftList  curDraftList \(\) the sublist of draftList starting from index \(i+1\)  curK \(\) the submatrix of \(K_{nn}\) from with the top-left corner at \((i+1,i+1)\) extending to the bottom-right corner curPrefix \(\) curGen while curGen.length - curPrefix.length is less than \(k_{i}\)do  curPrefix \(\) curGen \(\) Perform the vertical cascade with the recursive call. \([x_{1},..,x_{u}],[p_{1},p_{2},...,p_{v}]\)_CascadeSpeculativeDraftingStep_(curDraftList, curTarget, curPrefix, False, curK, \(l\))  curGen \([x_{1},..,x_{u}]\)  s \(\) curProbs.length + 1  Add elements of \([p_{1},p_{2},...,p_{v}]\) to curProbs endwhile endfor \(\) Set lenience to 1 when the original target model reviews. ifisFirstCall then \(l 1\) endif \(\) Use \(_{t}\) to review the draft generation. \([x_{1},...,x_{out}],[p^{}_{1},p^{}_{2},...,p^{}_{out}]\) = review(\(_{t}\), curGen, curProbs, l) return\([x_{1},...,x_{out}],[p^{}_{1},p^{}_{2},...,p^{}_{out}]\) ```

**Algorithm 1** CascadeSpeculativeDraftingStep

## 3 Cascade Speculative Drafting

In this section, we introduce our proposed method, Cascade Speculative Drafting (CS Drafting), a speculative execution algorithm that incorporates two types of cascades: _vertical cascade_ and _horizontal cascade_.

### Vertical Cascade

A notable inefficiency of the speculative decoding algorithm is the reliance on the autoregressive generation of a smaller draft model. Since the draft model must run \(k\) times for each target model run, the cost can still be significant despite its smaller size. In light of this, we reduce the drafting inefficiency by using an even smaller model to assist in drafting and employing the original draft model to review the generation of this smaller model. In addition, since this process can be performed again on the draft model that drafts for the original model, we recursively perform this process until it reaches a statistical draft model that involves negligent cost, such as a bigram language model. In this approach, we expect each recursion step will reduce the drafting latency without altering the output distribution. We refer to this recursive speculative approach as _Vertical Cascade_.

Additionally, we incorporate _lenience_, a hyperparameter that loosens the review process by the target model, allowing for faster speed at the trade-off of potentially differing results from the target model . Leninge can be adopted during sampling or greedy decoding with speculative decoding. Let lenience \(l[1,)\). When sampling, the acceptance condition for token \(x_{i}\) is transformed to \(_{d}(x)[i] l_{t}(x)[i]\). If the acceptance condition is not satisfied, with a probability of \(1-_{t}(x)}{_{d}(x)}\), we reject \(x_{i}\) and any following tokens.2 When performing greedy decoding, the acceptance condition becomes deterministic and is simply either \(argmax_{d}(x)[i]=argmax_{t}(x)[i]\) or \(_{d}(x)[i] l_{t}(x)[i]\).

For the speculative decoding algorithm, the reduced quality introduced by lenience is generally undesirable. However, for the vertical cascade approach, lenience affects the final output only if it is applied when the target model reviews. Therefore, we can limit the application of lenience in the vertical cascade only when draft models review and do not apply lenience when the target model reviews. This can ensure the final output is not altered while further reducing latency.

### Horizontal Cascade

Another key observation is that during the drafting steps of speculative decoding, not all drafting tokens are created equal, as illustrated in Figure 2. The first draft token is more likely to be accepted as it only depends on itself; the last token is rarely accepted, as it has a chance of being reviewed only if all preceding tokens are accepted. From a theoretical perspective, assume the event of acceptance of each token being a Bernoulli distribution with probably \(p\), the probability of \(n\)-th token being accepted is \(p^{n}\), implying an exponential decrease of value for tokens generated later in the sequence.

Inspired by this observation, we designed _Horizontal Cascade_, an approach that improves time allocation by draft token allocation. Horizontal Cascade assigns the largest draft model to perform the generation of the first draft token due to its highest output alignment with the target model, and it progressively uses a smaller as the new draft token to be generated is less likely to be accepted. This process stops after the smallest model, i.e., a statistical language model finishes. This design reduces the time cost of generating unimportant draft tokens with a costly draft model, leading to a reduction in overall latency.

### Max-Gram for Better Statistical Drafting

As both Vertical Cascade and Horizontal Cascade remark cascade toward faster draft models, a statistical language model, which is the basis of the cascade, becomes essential for the efficiency of both approaches. In our pursuit of a more effective statistical language model, we noticed a general pattern: in language model generation, some words and phrases from the input query frequently reappear in the generated content. In light of this observation, we designed the **Max**-**G**ram (MaG) algorithm. It greedily identifies maximal matches between the initial input (or existing generation) and tokens from the end of the generation. In cases where there is no match, we resort to a bigram model based on the probability distribution of Wikipedia (chosen to maintain the generality). We include a GPU-friendly version of the Max-Gram algorithm in Appendix A.

### Algorithm

Combining the horizontal and vertical cascades, the algorithm of cascade speculative decoding is presented in Algorithm 1. At its center, the horizontal cascade is realized by the for loop, while the vertical cascade is implemented through recursive calls. Notably, the MaG model is incorporated as the smallest draft model to avoid autoregressive generation from a neural model. An example of CS Drafting is shown in Figure 1.

The algorithm requires an upper-triangular hyperparameter, \(K_{nn}\), with each row serving as the stop criteria for a layer of recursive calls. For simplicity, we assume the lenience \(l\) is universal for the algorithm, except when the target model is under review; thus, the algorithm can benefit from the speedup of lenience without altering the output distribution.

Analysis

In this section, we provide theoretical analyses for Cascade Speculative Drafting. We begin with some notions. Let \(_{t}\) be the target model, \(_{d}\) be the draft model, and \(k\) be the number of draft tokens generated per step.

**Expected acceptance rate \((_{t},_{d})\)** is the probability of draft generation by \(_{d}\) being accepted by target model \(_{t}\).

**Cost coefficient \(c(_{t},_{d})\)** is the ratio of time for a single run of draft model \(_{d}\) over target model \(_{t}\).

**Expected walltime improvement factor (EWIF)** is the expected time improvement achieved by an algorithm under the i.i.d. assumption of token acceptance.

Despite the simple setting of EWIF, it is demonstrated that it aligns with the experimental results in most instances. Therefore, our analysis will concentrate on EWIF.

### Vertical Cascade

We analyze EWIF of vertical cascade using generating functions, a well-studied topic in combinatorial mathematics . The properties of generating functions are useful in the recursion and evaluation process making our final expression simple.

We begin with the derivation of the probability generating function for speculative decoding.

**Theorem 4.1**.: _For speculative decoding between \(_{t}\) and \(_{d}\), let \(p_{i}\) be the probability of generating \(i\) tokens. The probability generating function of \(p_{i}\) satisfies the following equation:_

\[_{(,k)}(x)=1+(x-1)x^{k+1}}{(1- x)},\] (1)

_where \(=(_{t},_{d})\)._

Proof in Appendix C.1.

**Corollary 4.2**.: _The EWIF of speculative decoding is \(_{(,k)}(1)}{(ck+1)}=}{(1-) (ck+1)}\)._

We use the generating function to derive the EWIF of a vertical cascade and analyze the case involving two draft models, \(_{d_{1}}\) and \(_{d_{2}}\).

**Theorem 4.3**.: _Assume \(k\) to be the speculative decoding parameter between \(_{d_{1}}\) and \(_{d_{2}}\), and \(n\) to be the number of steps \(_{t}\) reviews. The EWIF by this system is_

\[()}{(1-)(1+nc_{d_{1}}+nck_{d_{2}})},\] (2)

_where \((x)=_{((_{d_{1}},_{d_{2}}),k)}(x)\), \(=(_{t},_{d})\), and \(c_{d_{1}},c_{d_{2}}\) be \(c(_{t},_{d_{1}}),c(_{t},_{d_{2}})\) respectively._

**Corollary 4.4**.: \(_{^{},k}()<\) _for any \(1>>0,1>^{}>0,k>0\), so if \(c_{d_{2}} 1\), the EWIF of \(_{d_{1}}\) and \(_{d_{2}}\) is higher than EWIF of \(_{d_{1}}\) alone._

Proof in Appendix C.2.

Therefore, with the statistical model having negligible cost (i.e., \(c_{d_{2}} 1\)), it can almost always improve the efficiency of an SD system.

### Horizontal Cascade

We also present an analysis of the walltime improvement offered by the horizontal cascade.

To assist the analysis, we establish the notions. Let \(_{t}\) be the target model, \(\{_{i}\}\) be the draft models assisting generation with \(_{i}\) being the draft model generating the \(i\)-th token. In the simpler case of the speculative decoding, \(_{i}=_{d}\) for any \(i\). Let \(x\) be the input to the model at a single run, \(_{t}(x)\) and \(_{i}(x)\) are then the output probability distribution with input \(x\). To simplify notation, let \(_{i}=(_{t}(x),_{i}(x))\) and \(c_{i}=c(_{t}(x),_{i}(x))\).

**Theorem 4.5**.: _The expected walltime improvement factor (EWIF) of the horizontal cascade is \(T(k,_{1},...,_{k},c_{1},...,c_{k})=^{k}_{j=1}^{i }_{j}}{1+_{i=1}^{k}c_{i}}\)._

Furthermore, theorem 4.5 can be used to analyze the importance of the tokens in the drafting step.

**Corollary 4.6**.: _The probability of \(i\)-th token being accepted is \(_{j=1}^{i}_{j}\). The derivative of EWIF with respect to \(_{l}\) is \(,...,_{k},c_{1},...,c_{k})}{d_{l}}=^{k}_{i=1}^{i}_{j}}{1+_{i=1}^{k}c_{i}}\). Specifically, \(,...,_{k},c_{1},...,c_{k})}{d_{1}}=^{k}_{j=2}^{i}_{j}}{1+_{i=1}^{k}c_{i}}\) and \(,...,_{k},c_{1},...,c_{k})}{d_{k}}=^{k}_{j}}{1+_{i=1}^{k}c_{i}}\)._

Using the information provided by Leviathan _et al._, we calculate a simulated EWIF under the assumption that the event of acceptance by the target model is a Bernoulli trial. The results, shown in Table 1, indicate that speculative sampling with a horizontal cascade achieved better EWIF than vanilla speculative sampling under this assumption.

## 5 Experiments

### Experimental Setup

**Metrics** We use both our proposed standardized walltime improvement and walltime for evaluation:

* **Standardized walltime improvement (SWI)** assumes each forward run of a model takes a constant amount of time which can be recorded data of previous work  or heuristics such as the total number of the parameters of a model. Under this assumption, the value of SWI is the speedup of the speculative method over autoregressive generation. SWI alleviates hardware variation and features full reproducibility of experiment results.
* **Walltime** refers to the actual elapsed time taken to complete a specific task or operation in a real-world scenario. Despite being less reproducible and sensitive to noise, walltime better represents the performance for individual users. In our experiment, walltime is measured in the form of the number of tokens generated per second on our GPU.

**Datasets** We chose two commonly used datasets for our experiments. For both datasets, we conducted experiments in a zero-shot chain-of-thought setup [13; 23]:

* **GSM8K** is a dataset comprising 8,500 high-quality, linguistically diverse, grade-school math word problems. It focuses on multi-step reasoning with problems that are typically solvable using basic arithmetic in 2 to 8 steps.
* **MMLU**, or Massive Multitask Language Understanding, is a benchmark for testing how well large language models grasp knowledge. It encompasses 57 diverse subjects, ranging from elementary science to advanced law.

**Baselines** To verify the effectiveness of both vertical and horizontal cascade strategies intuitively, we first compare the performance of CS Drafting with different numbers of cascades, as well as its performance against standard speculative decoding . Additionally, CS Drafting can also operate

   Dataset & \(_{d_{1}}\) & \(_{d_{2}}\) & \(k_{1}\) & \(k_{2}\) & EWIF \\  CNNDM & small & - & 9 & - & 2.65 \\ CNNDM & base & - & 8 & - & 2.96 \\ CNNDM & base & small & 5 & 3 & **3.03** \\  ENDE & small & - & 12 & - & 3.61 \\ ENDE & base & - & 11 & - & 3.75 \\ ENDE & base & small & 5 & 8 & **3.93** \\   

Table 1: Simulated EWIF under the assumption that the acceptance distribution is a Bernoulli distribution. BASE and SMALL refer to FLAN-T5-base and FLAN-T5-small. In the simulation, speculative sampling with horizontal cascade exceeded the performance of the vanilla speculative decoding on both CNN Dailymail  and WMT EnDe  datasets.

vertically by combining with other advanced decoding methods. To verify this, we also leverage tree attention, as used in Medusa , and compare its performance with Medusa.

**Implementation Details** To ensure the generality of our findings, we perform experiments on both encoder-decoder and decoder-only models. For encoder-decoder models, we choose our target and draft models from the FLAN-T5  family for our experiment, as there is a large variation in model sizes within the FLAN-T5 family (ranging from 77 million to 11 billion parameters). We use FLAN-T5-xxl as our target model, FLAN-T5-base and FLAN-T5-small as our reviewing draft models. For decoder-only models, we select Vicuna-7B , a fine-tuned version of LLaMA  as the target model. We use a 68M model with the same tokenizer as the reviewing draft model3. We also leverage tree attention [15; 3] with CS Drafting for the experiments on Vicuna-7B. In both cases, the Max-Gram algorithm is used as the generating draft model. Since we do not observe any significant difference between sampling with temperature \(1\) and greedy decoding in previous speculative decoding experiments , and to ensure our experiments are fully reproducible, we perform sampling at temperature \(0\), i.e., using greedy decoding by default. To align our experiment with current common usage, we do not perform fine-tuning for CS Drafting, and the generation is conducted in a zero-shot manner. We include hyperparameter details in Appendix B. All of our experiments involving walltime are performed on a single NVIDIA A40 GPU.

### Experimental Results

Table 2 presents the main experimental results. In two settings of SWI, Cascade Speculative Drafting has outperformed the speculative decoding algorithm. For GSM8K, CS Drafting achieved a maximum additional speedup of 44% over the fastest speculative algorithm; for MMLU, the maximum additional speedup improvement over speculative decoding is 81%.

**Effectiveness of MaG** When comparing CS Drafting with one neural model and MaG against the fastest speculative decoding setup, we found that CS Drafting with one neural model gained up to a 70% speedup on MMLU and a 32% speedup on GSM8K. Notably, the MaG algorithm only involves a bigram model with parameters equal to the tokenizer size, making its memory cost negligible." In addition, the speedup gained using CS Drafting with one neural model involves no additional deployment overhead while reducing both latency and computational cost, making it a superior choice over speculative decoding.

**Draft Model Size** Despite FLAN-T5-small mostly outperforming FLAN-T5-base as a draft model for speculative decoding, in CS Drafting with the aid of MaG, FLAN-T5-base consistently

   Dataset & Algorithm & \(\{_{d_{i}}\}\) & Speedup (MS) & Speedup (PW) \\  GSM8K & Autoregressive & - & 1 & 1 \\ GSM8K & S Decoding & Base & \(3.38\) & 2.99 \\ GSM8K & S Decoding & small & \(3.06\) & 2.76 \\ GSM8K & CS Drafting & BASE, mag & \(3.70\) & 3.27 \\ GSM8K & CS Drafting & small, mag & \(3.19\) & 2.82 \\ GSM8K & CS Drafting & BASE, small, mag & **3.88** & **3.43** \\  MMLU & Autoregressive & - & 1 & 1 \\ MMLU & S Decoding & BASE & \(3.97\) & 3.42 \\ MMLU & S Decoding & small & \(4.12\) & 3.51 \\ MMLU & CS Drafting & BASE, mag & \(4.56\) & 4.21 \\ MMLU & CS Drafting & small, mag & \(4.39\) & 3.99 \\ MMLU & CS Drafting & BASE, small, mag & **4.88** & **4.32** \\   

Table 2: The experimental results on FLAN-T5. Speedup (MS) is the standardized walltime improvement with the assumption that the latency of each run of a model is its number of parameters (model size). Speedup (PW) is the SWI with the assumption that the latency of each run of a model is the time cost data reported from previous work .

outperforms FLAN-T5-small. This implies that with the limitation of a single draft model, the ideal size of the draft model might increase with the assistance of the MaG model.

**Results of Decoder-only Models** As shown in Table 3, CS Drafting achieves a significant walltime improvement over speculative decoding. Moreover, CS Drafting exceeds Medusa when combined with tree attention [15; 3]. This suggests that CS Drafting can be integrated with other efficient designs for speculative decoding to further accelerate inference. We leave the exploration of more advanced combinations as future work.

**Ablation Study** We perform a hyperparameter study on \(K_{00}\), the hyperparameter with the greatest effect on our experiments. As shown in Table 4, the performance of CS Drafting only decreases slightly when this hyperparameter is sub-optimal. Therefore, end users who do not require maximum performance can use a simple setup to achieve near-optimal performance with CS Drafting. Furthermore, we conduct an ablation study by removing the horizontal cascade. On GSM8K, with Vicuna-7B and \(K_{00}=1\), this results in a performance drop from 56.16 to 53.55.

## 6 Related Work

### Efficient Methods for Language Model Inference

In the era of large language models, efficiency during inference becomes a key to model service. To reduce the model inference cost and speed up, several efficient methods have been proposed, including pruning, knowledge distillation and quantization . Model pruning takes structured [26; 22] or unstructured [9; 5] methods to remove the redundant model parameters to reduce the storage memory and increase inference speed. Knowledge distillation takes the approach of transferring knowledge from a superior teacher model to a smaller student model [11; 8]. Quantization maps high-precision data representations (e.g. 32 bits) into low-precision ones (e.g. 8 bits) to reduce memory consumption [1; 18].

### Speculative Decoding

With the success of Speculative Decoding [4; 14] in reducing the large language model inference latency, some recent works have attempted to improve Speculative Decoding by reducing the rejection rate. Zhou _et al._ propose using generalized knowledge distillation and achieve a lower rejection rate compared to other knowledge distillation methods. Avoiding an additional draft model, self-drafting is an approach to speculative decoding by reusing part of the target model together with added weight to perform drafting [27; 12]. Tree attention involves generating multiple candidates during drafting to increase the chance of acceptance [19; 15]. Besides reducing the rejection rate, improving drafting efficiency can also reduce latency. Spector _et al._ propose using speculative decoding for drafting, showing similarities to the vertical cascade; however, their method only has two layers of speculative decoding and does not observe the recursive nature of the vertical cascade nor the lenience among draft models, two crucial aspects for the performance of vertical cascade.

   \(K_{00}\) & Walltime (tokens/s) \\ 
1 & 56.16 \\
2 & 55.51 \\
3 & 53.73 \\   

Table 4: Results on GSM8K with Vicuna-7B under different generation length limits.

   Algorithm & GSM8K Walltime (Tokens/s) & MMLU Walltime (Tokens/s) \\  Autoregressive & 33.34 & 33.11 \\ S Decoding & 44.31 & 43.65 \\ CS Drafting & 56.72 & 55.60 \\ Medusa & 61.87 & 56.19 \\ CS Drafting + Tree Attention & **63.81** & **63.37** \\   

Table 3: The experimental results on Vicuna-7B.

[MISSING_PAGE_FAIL:10]

*  Jianping Gou, Baosheng Yu, Stephen J. Maybank, and Dacheng Tao. Knowledge distillation: A survey. _International Journal of Computer Vision_, 129(6):1789-1819, March 2021.
*  Demi Guo, Alexander M. Rush, and Yoon Kim. Parameter-efficient transfer learning with diff pruning, 2021.
*  Dan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas Mazeika, Dawn Song, and Jacob Steinhardt. Measuring massive multitask language understanding, 2021.
*  Geoffrey Hinton, Oriol Vinyals, and Jeff Dean. Distilling the knowledge in a neural network, 2015.
*  Coleman Hooper, Sehoon Kim, Hiva Mohammadzadeh, Hasan Genc, Kurt Keutzer, Amir Gholami, and Sophia Shao. Speed: Speculative pipelined execution for efficient decoding, 2024.
*  Takeshi Kojima, Shixiang Shane Gu, Michel Reid, Yutaka Matsuo, and Yusuke Iwasawa. Large language models are zero-shot reasoners. _Advances in neural information processing systems_, 35:22199-22213, 2022.
*  Yaniv Leviathan, Matan Kalman, and Yossi Matias. Fast inference from transformers via speculative decoding. In _International Conference on Machine Learning_, pages 19274-19286. PMLR, 2023.
*  Xupeng Miao, Gabriele Oliaro, Zhihao Zhang, Xinhao Cheng, Zeyu Wang, Zhengxin Zhang, Rae Ying Yee Wong, Alan Zhu, Lijie Yang, Xiaoxiang Shi, Chunan Shi, Zhuoming Chen, Daiyaan Arfeen, Reyna Abhyankar, and Zhihao Jia. Specinfer: Accelerating generative large language model serving with tree-based speculative inference and verification, 2024.
*  Ramesh Nallapati, Bowen Zhou, Cicero Nogueira dos Santos, Caglar Gulcehre, and Bing Xiang. Abstractive text summarization using sequence-to-sequence rnns and beyond, 2016.
*  OpenAI. Gpt-4 technical report, 2023.
*  Clemens JS Schaefer, Elfie Guo, Caitlin Stanton, Xiaofan Zhang, Tom Jablin, Navid Lambert-Shirzad, Jian Li, Chiachen Chou, Siddharth Joshi, and Yu Emma Wang. Mixed precision post training quantization of neural networks with sensitivity guided search, 2023.
*  Benjamin Spector and Chris Re. Accelerating llm inference with staged speculative decoding. _arXiv preprint arXiv:2308.04623_, 2023.
*  Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajwal Bhargava, Shruti Bhosale, Dan Bikel, Lukas Blecher, Cristian Canton Ferrer, Moya Chen, Guillem Cucurull, David Esiobu, Jude Fernandes, Jeremy Fu, Wenyin Fu, Brian Fuller, Cynthia Gao, Vedanuj Goswami, Naman Goyal, Anthony Hartshorn, Saghar Hosseini, Rui Hou, Hakan Inan, Marcin Kardas, Viktor Kerkez, Madian Khabsa, Isabel Kloumann, Artem Korenev, Punit Singh Koura, Marie-Anne Lachaux, Thibaut Lavril, Jenya Lee, Diana Liskovich, Yinghai Lu, Yuning Mao, Xavier Martinet, Todor Mihaylov, Pushkar Mishra, Igor Molybog, Yixin Nie, Andrew Poulton, Jeremy Rezenstein, Rashi Rungta, Kalyan Saladi, Alan Schelten, Ruan Silva, Eric Michael Smith, Ranjan Subramanian, Xiaqing Ellen Tan, Binh Tang, Ross Taylor, Adina Williams, Jian Xiang Kuan, Puxin Xu, Zheng Yan, Iliyan Zarov, Yuchen Zhang, Angela Fan, Melanie Kambadur, Sharan Narang, Aurelien Rodriguez, Robert Stojnic, Sergey Edunov, and Thomas Scialom. Llama 2: Open foundation and fine-tuned chat models, 2023.
*  Marcos Treviso, Ji-Ung Lee, Tianchu Ji, Betty van Aken, Qingqing Cao, Manuel R. Ciosici, Michael Hassid, Kenneth Heafield, Sara Hooker, Colin Raffel, Pedro H. Martins, Andre F. T. Martins, Jessica Zosa Forde, Peter Milder, Edwin Simpson, Noam Slonim, Jesse Dodge, Emma Strubell, Niranjan Balasubramanian, Leon Derczynski, Iryna Gurevych, and Roy Schwartz. Efficient methods for natural language processing: A survey, 2023.
*  Elena Voita, David Talbot, Fedor Moiseev, Rico Sennrich, and Ivan Titov. Analyzing multi-head self-attention: Specialized heads do the heavy lifting, the rest can be pruned, 2019.

*  Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Brian Ichter, Fei Xia, Ed Chi, Quoc Le, and Denny Zhou. Chain-of-thought prompting elicits reasoning in large language models, 2023.
*  D.B. West. _Combinatorial Mathematics_. Cambridge University Press, 2021.
*  Heming Xia, Tao Ge, Peiyi Wang, Si-Qing Chen, Furu Wei, and Zhifang Sui. Speculative decoding: Exploiting speculative execution for accelerating seq2seq generation. In _Findings of the Association for Computational Linguistics: EMNLP 2023_, pages 3909-3925, 2023.
*  Mengzhou Xia, Zexuan Zhong, and Danqi Chen. Structured pruning learns compact and accurate models, 2022.
*  Jun Zhang, Jue Wang, Huan Li, Lidan Shou, Ke Chen, Gang Chen, and Sharad Mehrotra. Draft & verify: Lossless large language model acceleration via self-speculative decoding, 2023.
*  Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan Zhuang, Zhanghao Wu, Yonghao Zhuang, Zi Lin, Zhuohan Li, Dacheng Li, Eric P. Xing, Hao Zhang, Joseph E. Gonzalez, and Ion Stoica. Judging llm-as-a-judge with mt-bench and chatbot arena, 2023.
*  Yongchao Zhou, Kaifeng Lyu, Ankit Singh Rawat, Aditya Krishna Menon, Afshin Rostamizadeh, Sanjiv Kumar, Jean-Francois Kagy, and Rishabh Agarwal. Distillspec: Improving speculative decoding via knowledge distillation, 2023.

Max-Gram Implementation

```
1deftorch_index(t,value):
2return(t==value).nonzero(as_tuple=True)
3
4
5defmax_gram(input_ids,encoder_ids,n=1):
6matches=(encoder_ids==input_ids[0,-1]).int()
7ifmatches.sum()<1:
8returnNone
9foriinrange(2,input_ids.shape[-1]+1):
10new_matches=(encoder_ids[0,:(-1*(i-1))]==input_ids[0,-1*i]).int()
11combined_matches=(2-new_matches==matches[1:]).int()
12ifcombined_matches.sum()<1:
13index=torch_index(torch.cat(
14(torch.tensor(*(i-1),device=torch.device(encoder_ids.device)),matches
15),dim=-1
16returnencoder_ids[:,index:index +n]
17else:
18matches=combined_matches
19index=torch_index(torch.cat((
20torch.tensor(*(encoder_ids.shape[-1]-matches.shape[-1])),matches),dim=-1
21)returnencoder_ids[:,index+1:index + n+1] ```

Listing 1: Max-Gram Algorithm

## Appendix B Hyperparameters

To reduce the number of hyperparameters to tune, we use MaG to generate 10 tokens at once, as it is rare for more than 10 tokens to be accepted with the exception when CS Drafting is combined with tree attention. We do not use lenience when the reviewer is \(_{t}\) to ensure the output distribution does not change. We also avoid lenience between MaG and its reviewer, since there is still a significant performance gap between MaG and a neural model. With these constraints, we are left with at most four hyperparameters: \(k_{11}\), \(k_{12}\), \(k_{22}\), and \(l\). For the CS Drafting step where the target model is the reviewer, \(k_{11}\) and \(k_{12}\) are used. \(k_{21}\) and \(l\) are used in the step where \(_{d_{1}}\) is the reviewer. The results of experiments on encoder-decoder models with hyperparameters are shown in Table 5.

When performing experiments with the decoder-only model, we fixed the hyperparameters of CS Drafting for different datasets to better align with most users who do not perform hyperparameter tuning. The k-matrix for CS Drafting is \([,]\). When adding tree attention, we limit it to only the lead node with the highest probability of having children; the k-matrix is \([,]\) with the number of children for each leading node being 8, while the other nodes have no children.

## Appendix C Proof

### Proof for Theorem 4.1

Proof.: The probability of accepting \(i\) tokens is \(^{i}-^{i+1}\), with the exception of the \(k+1\)-th token, which has a probability of \(^{k}\) of being accepted. This is because it requires all the first \(i\) tokens to be accepted and the \(i+1\)-th token to be rejected for this to happen. Therefore,

\[_{(,k)}(x)=^{k}x^{k+1}+_{i=0}^{k-1}(^{i}-^{i+1 })x^{i+1}.\] (3)By rearranging the terms, we can achieve an expression much easier to work with

\[_{(,k)}(x)=x+_{i=1}^{k}^{i}(x^{i+1}-x^{i})\] (4)

\[=x+(x-1)_{i=1}^{k}^{i}(x^{i})\] (5)

\[=x+(x-1)x^{i+1}}{1- x}.\] (6)

### Proof for Theorem 4.3

Proof.: Let \(^{}=(_{d_{1}},_{d_{2}})\). We first calculate the expected number of tokens being generated in a step of vertical cascade with \(_{d_{1}},_{d_{2}}\). With the property of generating function, the coefficient of term \(x^{j}\) of \(^{n}(x)\) is the probability of the sum of acceptance length of \(n\) speculative step being \(j\). Therefore, \(^{n}(x)\) represents the probability generating function right before \(_{t}\) performs the generation.

   Dataset & Algorithm & \(\{_{d_{i}}\}\) & Speedup (MS) & \(k_{11}\) & \(k_{12}\) & \(k_{22}\) & \(l\) \\  GSM8K & Autoregressive & - & 1 & - & - & - & - \\ GSM8K & S Decoding & base & \(3.38\) & 10 & - & - & - \\ GSM8K & S Decoding & small & \(3.06\) & 11 & - & - & - \\ GSM8K & CS Drafting & base, mag & \(3.70\) & 10 & - & - & - \\ GSM8K & CS Drafting & small, mag & \(3.19\) & 11 & - & - & - \\ GSM8K & CS Drafting & base, small, mag & **3.88** & 8 & 13 & 1 & 3 \\  MMLU & Autoregressive & - & 1 & - & - & - & - \\ MMLU & S Decoding & base & \(3.97\) & 13 & - & - & - \\ MMLU & S Decoding & small & \(4.12\) & 19 & - & - & - \\ MMLU & CS Drafting & base, mag & \(4.56\) & 13 & - & - & - \\ MMLU & CS Drafting & small, mag & \(4.39\) & 14 & - & - & - \\ MMLU & CS Drafting & base, small, mag & **4.88** & 5 & 19 & 1 & 5 \\  Dataset & Algorithm & \(\{_{d_{i}}\}\) & Speedup (PW) & \(k_{11}\) & \(k_{12}\) & \(k_{22}\) & \(l\) \\  GSM8K & Autoregressive & - & 1 & - & - & - & - \\ GSM8K & S Decoding & base & \(2.99\) & 8 & - & - & - \\ GSM8K & S Decoding & small & \(2.76\) & 8 & - & - & - \\ GSM8K & CS Drafting & base, mag & \(3.27\) & 9 & - & - & - \\ GSM8K & CS Drafting & small, mag & \(2.82\) & 11 & - & - & - \\ GSM8K & CS Drafting & base, small, mag & **3.43** & 5 & 9 & 1 & 3 \\  MMLU & Autoregressive & - & 1 & - & - & - & - \\ MMLU & S Decoding & base & \(3.42\) & 10 & - & - & - \\ MMLU & S Decoding & small & \(3.51\) & 11 & - & - & - \\ MMLU & CS Drafting & base, mag & \(4.21\) & 6 & - & - & - \\ MMLU & CS Drafting & small, mag & \(3.99\) & 13 & - & - & - \\ MMLU & CS Drafting & base, small, mag & **4.32** & 5 & 8 & 1 & 5 \\   

Table 5: The experimental results on FLAN-T5 with hyperparameter details. Speedup (MS) is the standardized walltime improvement with the assumption that the latency of each run of a model is its number of parameters (model size). Speedup (PW) is the SWI with the assumption that the latency of each run of a model is the time cost data reported from previous work . \(k_{11}\), \(k_{12}\), \(k_{22}\), \(l\) are the hyperparameters. \(k_{11}\) and \(k_{12}\) represent the step limitation target model and the draft models, \(k_{22}\) is the step limitations between the first and second draft model, and \(l\) is lenience as shown in algorithm 1. For speculative decoding, the \(k_{11}\) is simply the \(k\).

To achieve the expected number of token acceptances of a probability-generating function, we seek for an operator that can map the probability-generating function into the desired expectation.

To achieve the operator, we begin with a single polynomial term of \(x^{j}\). Fortunately, given the end result of \(}{(1-)}\), the operator \(T_{}(f(x))=\) will convert \(x^{j}\) to \(}{(1-)}\). In addition, due to the linearity of the operator, this can be extended to any polynomial. Therefore, we achieved the desired operator to map a probability-generating function into the desired expectation.

Apply operator \(T_{}\) to \(^{n}(x)\), we achieved the result of \(()}{(1-)}\) for the expected number of accepted tokens. Furthermore, since the number of \(_{d_{1}}\) calls is \(n\), and \(_{d_{2}}\) is called \(k\) time for each \(_{d_{1}}\) call given a total of \(nk\) calls of \(_{d_{2}}\). The time cost is \(1+nc_{d_{1}}+nck_{d_{2}}\) which implied the EWIF of the system being \(()}{(1-)(1+nc_{d_{1}}+nck_{d_{2}})}\).

For Corollary 4.4, since both \(0<<1\) and \(0<^{}<1\), we have \(^{i+1}^{ i+1}<^{}\), meaning that \(^{ k+1}}{1-^{}}<1\). Together with \(-1<0\), we have \(_{^{},k}()<1+(-1)=\). If we also let \(nck_{d_{2}}=0\), we have \(()}{(1-)(1+nc_{d_{1}}+nck_{d_{2}})}>}{(1-)(1+nc_{d_{1}})}\), which is the EWIF for speculative decoding with step size \(n\).

### NeurIPS Paper Checklist

You should answer [Yes], [No], or [NA].

1. **Claims** Question: Do the main claims made in the abstract and introduction accurately reflect the paper's contributions and scope? Answer: [Yes] Justification: The paper is focused on the scope introduced in the abstract and introduction. We provided experiments and analysis that fully support our claim in the abstract and introduction.
2. **Limitations** Question: Does the paper discuss the limitations of the work performed by the authors? Answer: [Yes] Justification: We included a limitation section to discuss the potential limitation of our work.
3. **Theory Assumptions and Proofs** Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? Answer: [Yes] Justification: We introduced the assumptions and all notions used. We included proofs for non-trivial claims.
4. **Experimental Result Reproducibility** Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? Answer: [Yes] Justification: We provided our code and implementation details which is sufficient for reproducing the experiments in the paper.
5. **Open access to data and code** Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? Answer: [Yes] Justification: We included our code and instructions in the code. We also provided details such as hyperparameters in the appendix.
6. **Experimental Setting/Details** Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? Answer: [Yes] Justification: We included the experimental details in the main paper as well as the appendix.
7. **Experiment Statistical Significance** Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? Answer: [No] Justification: We reported the results across different metrics and explained the assumptions made.
8. **Experiments Compute Resources**Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? Answer: [Yes] Justification: We discussed the GPU we used to run our experiments.
9. **Code Of Ethics** Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? Answer: [Yes] Justification: Our work focuses on efficiency, so it's unlikely to violate any code of ethics.
10. **Broader Impacts** Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? Answer: [No] Justification: Our work focuses on efficiency, so we do not think it will incur societal impacts.
11. **Safeguards** Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? Answer: [NA] Justification: We did not release any models or datasets.
12. **Licensees for existing assets** Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? Answer: [Yes] Justification: We provided proper citations to the datasets and models we use.
13. **New Assets** Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? Answer: [NA] Justification: We did not release new asset.
14. **Crowdsourcing and Research with Human Subjects** Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? Answer: [NA] Justification: Our paper did not involve any crowdsourcing or research with human subjects.
15. **Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects** Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? Answer: [NA] Justification: Our paper did not involve any research with human subjects.