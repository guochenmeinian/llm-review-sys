# Efficient Uncertainty Quantification and Reduction for Over-Parameterized Neural Networks

Ziyi Huang, Henry Lam, Haofeng Zhang

Columbia University

New York, NY, USA

zh2354,khl2114,hz2553@columbia.edu

Authors are listed alphabetically.

###### Abstract

Uncertainty quantification (UQ) is important for reliability assessment and enhancement of machine learning models. In deep learning, uncertainties arise not only from data, but also from the training procedure that often injects substantial noises and biases. These hinder the attainment of statistical guarantees and, moreover, impose computational challenges on UQ due to the need for repeated network retraining. Building upon the recent neural tangent kernel theory, we create statistically guaranteed schemes to principally _characterize_, and _remove_, the uncertainty of over-parameterized neural networks with very low computation effort. In particular, our approach, based on what we call a procedural-noise-correcting (PNC) predictor, removes the procedural uncertainty by using only _one_ auxiliary network that is trained on a suitably labeled dataset, instead of many retrained networks employed in deep ensembles. Moreover, by combining our PNC predictor with suitable light-computation resampling methods, we build several approaches to construct asymptotically exact-coverage confidence intervals using as low as four trained networks without additional overheads.

## 1 Introduction

Uncertainty quantification (UQ) concerns the dissection and estimation of various sources of errors in a prediction model. It has growing importance in machine learning, as it helps assess and enhance the trustworthiness and deployment safety across many real-world tasks ranging from computer vision  and natural language processing  to autonomous driving , as well as guiding exploration in sequential learning . In the deep learning context, UQ encounters unique challenges on both the statistical and computational fronts. On a high level, these challenges arise from the over-parametrized and large-scale nature of neural networks so that, unlike classical statistical models, the prediction outcomes incur not only noises from the data, but also importantly the training procedure itself . This elicits a deviation from the classical statistical theory that hinders the attainment of established guarantees. Moreover, because of the sizes of these models, conventional procedures such as resampling  demand an amount of computation that could quickly become infeasible.

Our main goal of this paper is to propose a UQ framework for over-parametrized neural networks in regression that has simultaneous _statistical coverage guarantee_, in the sense of classical frequentist asymptotic exactness, and _low computation cost_, in the sense of requiring only few (as low as four) neural network trainings, without other extra overheads. A main driver of these strengths in our framework is a new implementable concept, which we call the _Procedural-Noise-Correcting (PNC)_ predictor. It consists of an auxiliary network that is trained on a suitably artificially labeled dataset, with behavior mimicking the variability coming from the training procedure. To reach our goal, wesynthesize and build on two recent lines of tools that appear largely segregated thus far. First is neural tangent kernel (NTK) theory [64; 80; 7], which provides explicit approximate formulas for well-trained infinitely wide neural networks. Importantly, NTK reveals how procedural variability enters into the network prediction outcomes through, in a sense, a functional shift in its corresponding kernel-based regression, which guides our PNC construction. Second is light-computation resampling methodology, including batching [47; 99; 100] and the so-called cheap bootstrap method , which allows valid confidence interval construction using as few as two model repetitions. We suitably enhance these methods to account for both data and procedural variabilities via the PNC incorporation.

We compare our framework with several major related lines of work. First, our work focuses on the quantification of epistemic uncertainty, which refers to the errors coming from the inadequacy of the model or data noises. This is different from aleatoric uncertainty, which refers to the intrinsic stochasticity of the problem [92; 96; 121; 62; 36], or predictive uncertainty which captures the sum of epistemic and aleatoric uncertainties (but not their dissection) [94; 97; 12; 3; 22]. Regarding epistemic uncertainty, a related line of study is deep ensemble that aggregates predictions from multiple independent training replications [81; 73; 41; 8; 94]. This approach, as we will make clear later, can reduce and potentially quantify procedural variability, but a naive use would require demanding retraining effort and does not address data variability. Another line is the Bayesian UQ approach on neural networks [44; 2]. This regards network weights as parameters subject to common priors such as Gaussian. Because of the computation difficulties in exact inference, an array of studies investigate efficient approximate inference approaches to estimate the posteriors [43; 49; 16; 33; 32; 95; 79; 56]. While powerful, these approaches nonetheless possess inference error that could be hard to quantify, and ultimately finding rigorous guarantees on the performance of these approximate posteriors remains open to our best knowledge.

## 2 Statistical Framework of Uncertainty

We first describe our learning setting and define uncertainties in our framework. Suppose the input-output pair \((X,Y)\) is a random vector following an unknown probability distribution \(\) on \(\), where \(X^{d}\) is an input and \(Y\) is the response. Let the marginal distribution of \(X\) be \(_{X}(x)\) and the conditional distribution of \(Y\) given \(X\) be \(_{Y|X}(y|x)\). Given a set of training data \(=\{(x_{1},y_{1}),(x_{2},y_{2}),...,(x_{n},y_{n})\}\) drawn independent and identically distributed (i.i.d.) from \(\) (we write \(=(x_{1},...,x_{n})^{T}\) and \(=(y_{1},...,y_{n})^{T}\) for short), we build a prediction model \(h:\) that best approximates \(Y\) given \(X\). Let \(_{}\) or \(_{n}\) denote the empirical distribution associated with the training data \(\), where \(_{n}\) is used to emphasize the sample size dependence in asymptotic results.

To this end, provided a loss function \(:\), we denote the population risk \(R_{}(h):=_{(X,Y)}[(h(X),Y)]\). If \(h\) is allowed to be any possible functions, the best prediction model is the _Bayes predictor_[61; 93]: \(h_{B}^{*}(X)_{y}_{Y_{Y|X}}[ (y,Y)|X]\). With finite training data of size \(n\), classical statistical learning suggests finding \(_{n}\), where \(\) denotes a hypothesis class that minimizes the empirical risk, i.e., \(_{n}_{h}R_{_{}}(h)\). This framework fits methods such as linear or kernel ridge regression (Appendix B). However, it is not feasible for deep learning due to non-convexity and non-identifiability. Instead, in practice, gradient-based optimization methods are used, giving rise to \(_{n,}\) as a variant of \(_{n}\), where the additional variable \(\) represents the randomness in the training procedure. It is worth mentioning that this randomness generally depends on the empirical data \(\), and thus we use \(P_{|}\) to represent the distribution of \(\) conditional on \(\).

Furthermore, we consider \(_{n}^{*}=(\{_{n,}: P_{| }\})\) where "aggregate" means an idealized aggregation approach to remove the training randomness in \(_{n,}\) (known as ensemble learning [73; 18; 17; 87; 45]). A prime example in deep learning is deep ensemble  that will be detailed in the sequel. Finally, we denote \(h^{*}=_{n}_{n}^{*}\) as the grand "limiting" predictor with infinite samples. The exact meaning of "lim" will be clear momentarily.

Under this framework, epistemic uncertainty, that is, errors coming from the inadequacy of the model or data noises, can be dissected into the following three sources, as illustrated in Figure 1. Additional discussions on other types of uncertainty are in the Appendix A for completeness.

**Model approximation error.**\(_{AE}=h^{*}-h_{B}^{*}\). This discrepancy between \(h_{B}^{*}\) and \(h^{*}\) arises from the inadequacy of the hypothesis class \(\). For an over-parameterized sufficiently wide neural network \(\), this error is usually negligible thanks to the universal approximation power of neural networks for any continuous functions [28; 58; 53] or Lebesgue-integrable functions .

**Data variability.**\(_{DV}=_{n}^{*}-h^{*}\). This measures the representativeness of the training dataset, which is the most standard epistemic uncertainty in classical statistics .

**Procedural variability.**\(_{PV}=_{n,}-_{n}^{*}\). This arises from the randomness in the training process for a single network \(_{n,}\), which is present even with deterministic or infinite data. The randomness comes from the initialization of the network parameters, and also data ordering and possibly training time when running stochastic gradient descent with finite training epochs.

## 3 Quantifying Epistemic Uncertainty

We use a frequentist framework and, for a given \(x\), we aim to construct a confidence interval for the "best" predictor \(h^{*}(x)\). As discussed in Section 1, the over-parametrized and non-convex nature of neural networks defies conventional statistical techniques and moreover introduces procedural variability that makes inference difficult.

We focus on over-parameterized neural networks, that is, neural networks whose width is sufficiently large, while the depth can be arbitrary such as two [120; 107]. Over-parameterized neural networks give the following two theoretical advantages. First, this makes model approximation error negligible and a confidence interval for \(h^{*}(x)\) also approximately applies to \(h_{B}^{*}(x)\). Second, the NTK theory  implies a phenomenon that the network evolves essentially as a "linear model" under gradient descent, and thus the resulting predictor behaves like a shifted kernel ridge regression whose kernel is the NTK [7; 80; 54; 59; 118] (detailed in Appendix C). However, to our knowledge, there is no off-the-shelf result that exactly matches our need for the epistemic uncertainty task, so we describe it below. Consider the following regression problem:

\[_{n}(f_{},^{b})=_{i=1}^{n}(f_{ }(x_{i})-y_{i})^{2}+_{n}\|-^{b}\|_{2}^{2}. \]

where \(\) is the network parameters to be trained, \(^{b}\) is the initial network parameters, and \(_{n}>0\) is the regularization hyper-parameter which may depend on the data size. We add regularization \(_{n}\) to this problem, which is slightly different from previous work  without the regularization \(_{n}\). This can guarantee stable computation of the inversion of the NTK Gram matrix and can be naturally linked to kernel ridge regression, which will be introduced shortly in Proposition 3.1. We assume the network adopts the NTK parametrization with parameters randomly initialized using He initialization , and it is sufficiently wide to ensure the linearized neural network assumption holds; See Appendix C for details. Moreover, we assume that the network is trained using the loss function in (1) via continuous-time gradient flow by feeding the entire training data and using sufficient training time (\(t\)). In this sense, the uncertainty of data ordering and training time vanishes, making the random initialization the only uncertainty in procedural variability. The above specifications are formally summarized in Assumption C.2. With the above specifications, we have:

**Proposition 3.1** (Proposition C.3).: _Suppose that Assumption C.2 holds. Then the final trained network, conditional on the initial network \(s_{^{b}}(x)\), is given by_

\[_{n,^{b}}(x)=s_{^{b}}(x)+(x,)^{T}( (,)+_{n}n)^{-1}(-s_{^{b}}()), \]

_where in the subscript of \(_{n,^{b}}\), \(n\) represents \(n\) training data, \(^{b}\) represents an instance of the initial network parameters drawn from the standard Gaussian distribution \(P_{^{b}}=(0,_{p})\) where the dimension of \(^{b}\) is \(p\) (He initialization); \((,):=(K(x_{i},x_{j}))_{i,j=1,...,n}\) and \((x,):=(K(x,x_{1}),K(x,x_{2}),...,K(x,x_{n}))^{T}\) where \(K\) is the (population) NTK. This implies that \(_{n,^{b}}\) is the solution to the following kernel ridge regression:_

\[s_{^{b}}+*{arg\,min}_{g}} _{i=1}^{n}(y_{i}-s_{^{b}}(x_{i})-g(x_{i}))^{2}+_{n}\|g\|_{ }}^{2}\]

Figure 1: Three sources of epistemic uncertainty.

_where \(}\) is the reproducing kernel Hilbert space (RKHS) constructed from the NTK \(K(x,x^{})\)._

Proposition 3.1 shows that the shifted kernel ridge regressor using NTK with a shift from an initial function \(s_{^{b}}\) is exactly the linearized neural network regressor that starts from the initial network \(s_{^{b}}\). It also reveals how procedural variability enters into the neural network prediction outcomes, highlighting the need to regard random initialization as an inevitable uncertainty component in neural networks. We provide details and deviation of Proposition 3.1 in Appendix C.

### Challenges from the Interplay of Procedural and Data Variabilities

First, we will discuss existing challenges in quantifying and reducing uncertainty to motivate our main approach based on the PNC predictor under the NTK framework. To this end, deep ensemble  is arguably the most common ensemble approach in deep learning to reduce procedural variability.  shows that deep ensemble achieves the best performance compared with a wide range of other ensemble methods, and employing more networks in deep ensemble can lead to better performance. Specifically, the _deep ensemble predictor_\(_{n}^{m}(x)\) is defined as: \(_{n}^{m}(x):=_{i=1}^{m}_{n,^{b}_{i}}(x)\) where \(m\) is the number of networks in the ensemble, \(_{n,^{b}_{i}}(x)\) is the independently trained network with initialization \(^{b}_{i}\) (with the same training data \(\)), and \(^{b}_{1},...,^{b}_{m}\) are i.i.d. samples drawn from \(P_{^{b}}\). We also introduce \(_{n}^{*}(x):=_{P_{^{b}}}[_{n,^{b}}(x)]\) as the expectation of \(_{n,^{b}}(x)\) with respect to \(^{b} P_{^{b}}\). Taking \(m\) and using the law of large numbers, we have \(_{m}_{n}^{m}(x)=_{n}^{*}(x)\ a.s.\). So \(_{n}^{*}(x)\) behaves as an _idealized_ deep ensemble predictor with infinitely many independent training procedures. Using Proposition 3.1 and the linearity of kernel ridge regressor with respect to data (Appendix B), we have

\[_{n}^{m}(x)=_{i=1}^{m}s_{^{b}_{i}}(x)+(x,)^{T}((,)+_{n}n)^{-1}(- _{i=1}^{m}s_{^{b}_{i}}())\]

\[_{n}^{*}(x)=_{P_{^{b}}}[_{n,^{b}}(x)]= (x)+(x,)^{T}((,)+_{n}n)^{-1} (-()) \]

where \((x)=_{P_{^{b}}}[s_{^{b}}(x)]\) is the expectation of the initial network output \(s_{^{b}}(x)\) with respect to the the distribution of the initialization parameters \(P_{^{b}}=(0,_{p})\). It is easy to see that \(_{P_{^{b}}}[_{n}^{m}(x)]=_{n}^{*}(x)\) and \(_{P_{^{b}}}(_{m}^{m}(x))=_{P_{ ^{b}}}(_{n,^{b}}(x))\) where \(_{P_{^{b}}}\) is the variance with respect to the random initialization. As for the total variance:

**Proposition 3.2**.: _We have_

\[(_{n}^{m}(x))=(_{n}^{*}(x))+ [(_{n,^{b}}(x)|)]( _{n}^{*}(x))+[(_{n,^{b}}(x)|)]=(_{n,^{b}}(x)).\]

_where the variance is taken with respect to both the data \(\) and the random initialization \(P_{^{b}}\)._

Proposition 3.2 shows that deep ensemble improves the statistical profile of a single model by reducing its procedural variability by a factor \(\) (but not the data variability), and achieving this reduction requires \(m\) training times. To quantify the epistemic uncertainty that contains two variabilities from data and procedure, we may employ resampling approaches such as "bootstrap on a deep ensemble". This would involve two layers of sampling, the outer being the resampling of data, and the inner being the retraining of base networks with different initializations. In other words, this nested sampling amounts to a _multiplicative_ amount of training effort in a large number of outer bootstrap resamples and the \(m\) inner retaining per resampling, leading to a huge computational burden. Moreover, the data variability and procedural variability in neural networks are dependent, making the above approach delicate. More discussion about this issue is presented in Section D.

In the following, we introduce our PNC framework that can bypass the above issues in that:

**Uncertainty reduction.** We train one single network and _one_ additional auxiliary network to completely remove the procedural variability. This is in contrast to the deep ensemble approach that trains \(m\) networks and only reduces the procedural variability by an \(m\)-factor.

**Uncertainty quantification.** We resolve the computational challenges in "bootstrap on a deep ensemble", by combining PNC predictors with low-budget inference tools that require only as low as _four_ network trainings.

### PNC Predictor and Procedural Variability Removal

We first develop a computationally efficient approach to obtain \(_{n}^{*}(x)\), the idealized deep ensemble predictor that is free of procedural variability. We term our approach the procedural-noise-correcting (PNC) predictor, whose pseudo-code is given in Algorithm 1. This predictor consists of a difference between essentially two neural network outcomes, \(_{n,^{b}}(x)\) which is the original network trained using one initialization, and \(_{n,^{b}}^{}(x)\) that is trained on a suitably artificially labeled dataset. More precisely, this dataset applies label \((x_{i})\) to \(x_{i}\), where \(\) is the expected output of an _untrained_ network with random initialization. Also note that labeling this artificial dataset does not involve any training process and, compared with standard network training, the only additional running time in the PNC predictor is to train this single artificial-label-trained network.

**Input:** Training data \(=\{(x_{1},y_{1}),(x_{2},y_{2}),...,(x_{n},y_{n})\}\).

**Procedure: 1.** Draw \(^{b} P_{^{b}}=(0,_{p})\) under NTK parameterization. Train a standard base network with data \(\) and the initialization parameters \(^{b}\), which outputs \(_{n,^{b}}()\) in (3).

**2.** Let \((x)=_{P_{^{b}}}[s_{^{b}}(x)]\). For each \(x_{i}\) in \(\), generate its "artificial" label \((x_{i})=_{P_{^{b}}}[s_{^{b}}(x_{i})]\). Train an auxiliary neural network with data \(\{(x_{1},(x_{1})),(x_{2},(x_{2})),...,(x_{n},(x_{n}))\}\) and the initialization parameters \(^{b}\) (the same one as in Step 1.), which outputs \(_{n,^{b}}^{}()\). Subtracting \(()\), we obtain \(_{n,^{b}}()=_{n,^{b}}^{}()- {s}()\).

**Output:** At point \(x\), output \(_{n,^{b}}(x)-_{n,^{b}}(x)\).

**Algorithm 1** Procedural-Noise-Correcting (PNC) Predictor

The development of our PNC predictor is motivated by the challenge of computing \(_{n}^{*}(x)\) (detailed below). To address this challenge, we characterize the procedural noise instead, which is given by

\[_{n,^{b}}():=_{n,^{b}}(x)-_{n}^{*}(x )=s_{^{b}}(x)-(x)+(x,)^{T}((,)+ _{n}n)^{-1}(()-s_{^{b}}()). \]

By Proposition 3.1, the closed-form expression of \(_{n,^{b}}\) in (4) corresponds exactly to the artificial-label-trained network in Step 2 of Algorithm 1. Our artificial-label-trained network is thereby used to quantify the procedural variability directly. This observation subsequently leads to:

**Theorem 3.3** (Pnc).: _Suppose that Assumption C.2 holds. Then the output of the PNC predictor (Algorithm 1) is exactly \(_{n}^{*}(x)\) given in (3)._

We discuss two approaches to compute \((x)\) in Step 2 of Algorithm 1 and their implications: 1) Under He initialization, \(s_{^{b}}(x)\) is a zero-mean Gaussian process in the infinite width limit , and thus we may set \( 0\) for simplicity. Note that even if we set \( 0\), it does _not_ imply that the artificial-label-trained neural network in Step 2 of Algorithm 1 will output a zero-constant network whose parameters are all zeros. In fact, neural networks are excessively non-convex and have many nearly global minima. Starting from an initialization parameter \(^{b}\), gradient descent on this artificial-label-trained neural network will find a global minimum that is close to the \(^{b}\) but not "zero" even if "zero" is indeed one of its nearly global minima ("nearly" in the sense of ignoring the negligible regularization term) [35; 118; 23]. This phenomenon can also be observed in Proposition 3.1 by plugging in \(=\). Hence, the output depends on random initialization in addition to training data, and our artificial-label-trained network is designed to capture this procedural variability. 2) An alternative approach that does not require specific initialization is to use Monte Carlo integration: \((x)=_{N}_{i=1}^{N}s_{^{b}_{i} }(x)\) where \(^{b}_{i}\) are i.i.d. from \(P_{^{b}}\). When \(N\) is finite, it introduces procedural variance that vanishes at the order \(N^{-1}\). Since this computation does not involve any training process and is conducted in a rapid manner practically, \(N n\) can be applied to guarantee that the procedural variance in computing \((x)\) is negligible compared with the data variance at the order \(n^{-1}\). We practically observe that both approaches work similarly well.

Finally, we provide additional remarks on why \(_{n}^{*}(x)\) cannot be computed easily except using our Algorithm 1. First, the following two candidate approaches encounter computational issues: 1) One may use deep ensemble with sufficiently many networks in the ensemble. However, this approach is time-consuming as \(m\) networks in the ensemble mean \(m\)-fold training times. \(m\) is typically as small as five in practice  so it cannot approximate \(_{n}^{*}(x)\) well. 2) One may use the closedform expression of \(_{n}^{*}\) in (3), which requires computing the NTK \(K(x,x^{})\) and the inversion of the NTK Gram matrix \((,)\). \(K(x,x^{})\) is recursively defined and does not have a simple form for computation, which might be addressed by approximating it with the empirical NTK \(K_{}(x,x^{})\) numerically (See Appendix C). However, the inversion of the NTK Gram matrix gives rise to a more serious computational issue: the dimension of the NTK Gram matrix is large on large datasets, making the matrix inversion very time-consuming. Another approach that seems plausible is that: 3) One may initialize one network that is equivalent to \((x)\) and then train it. However, this approach _cannot_ obtain \(_{n}^{*}(x)\) based on Proposition 3.1 because Proposition 3.1 requires random initialization. If one starts from a deterministic network initialization such as a zero-constant network, then the NTK theory underpinning the linearized training behavior of over-parameterized neural networks breaks down and the resulting network cannot be described by Proposition 3.1.

### Constructing Confidence Intervals from PNC Predictors

We construct confidence intervals for \(h^{*}(x)\) leveraging our PNC predictor in Algorithm 1. To handle data variability, two lines of works borrowed from classical statistics may be considered. First is an analytical approach using the delta method for asymptotic normality, which involves computing the influence function  that acts as the functional gradient of the predictor with respect to the data distribution. It was introduced in modern machine learning for understanding a training point's effect on a model's prediction . The second is to use resampling , such as the bootstrap or jackknife, to avoid explicit variance computation. The classical resampling method requires sufficiently large resample replications and thus incurs demanding resampling effort. For instance, the jackknife approach requires the number of training times to be the same as the training data size, which is very time-consuming and barely feasible for neural networks. Standard bootstrap requires a sufficient number of resampling and retraining to produce accurate resample quantiles. Given these computational bottlenecks, we consider utilizing light-computation resampling alternatives, including batching  and the so-called cheap bootstrap method , which allows valid confidence interval construction using as few as two model repetitions.

**Large-sample asymptotics of the PNC predictor.** To derive our intervals, we first gain understanding on the large-sample properties of the PNC predictor. Proposition 3.1 shows that \(_{n}^{*}(x)\) in (3) is the solution to the following empirical risk minimization problem:

\[_{n}^{*}()=()+_{g}_{ i=1}^{n}[(y_{i}-(x_{i})-g(x_{i}))^{2}]+_{n}\|g\|_{}^{2}. \]

Its corresponding population risk minimization problem (i.e., removing the data variability) is:

\[h^{*}()=()+_{g}_{}[(Y- (X)-g(X))^{2}]+_{0}\|g\|_{}^{2} \]

where \(_{0}=_{n}_{n}\). To study the difference between the empirical and population risk minimization problems of kernel ridge regression, we introduce the following established result on the asymptotic normality of kernel ridge regression (See Appendix B for details):

**Proposition 3.4** (Asymptotic normality of kernel ridge regression ).: _Let \(\) be a generic RKHS. Suppose that Assumptions B.3 and B.4 hold. Let \(g_{P,}\) be the solution to the following problem: \(g_{P,}:=_{g}_{P}[(Y-g(X))^{2}]+\|g\| _{}^{2}\) where \(P=_{n}\) or \(\). Then_

\[(g_{_{n},_{n}}-g_{,_{0}}) { in }\]

_where \(\) is a zero-mean Gaussian process and \(\) represents "converges weakly". Moreover, at point \(x\), \((g_{_{n},_{n}}(x)-g_{,_{0}}(x))(0,^{2}(x))\) where \(^{2}(x)=_{z}IF^{2}(z;g_{P,_{0}}, )(x)d(z)\) and \(IF\) is the influence function of statistical functional \(g_{P,_{0}}\)._

Next, we apply the above proposition to our problems about \(_{n}^{*}\). Let \(T_{1}(P)(x)\) be the solution of the following problem \(_{g}_{P}[(Y-(X)-g(X))^{2}]+_{0}\|g\| _{}^{2}\) that is evaluated at a point \(x\). Then we have the following large-sample asymptotic of the PNC predictor, providing the theoretical foundation for our subsequent interval construction approaches.

**Theorem 3.5** (Large-sample asymptotics of the PNC predictor).: _Suppose that Assumption C.2 holds. Suppose that Assumptions B.3 and B.4 hold when \(Y\) is replaced by \(Y-(X)\). Input the training data \(\) into Algorithm 1 to obtain \(_{n,^{b}}(x)-_{n,^{b}}(x)\). We have_

\[(_{n,^{b}}(x)-_{n,^{b}}(x)-h^{*}(x) )(0,^{2}(x)), \]

_where_

\[^{2}(x)=_{z}IF^{2}(z;T_{1},)(x)d (z). \]

_Thus, an asymptotically (in the sense of \(n\)) exact \((1-)\)-level confidence interval of \(h^{*}(x)\) is \([_{n,^{b}}(x)-_{n,^{b}}(x)-}q_{1-},_{n,^{b}}(x)-_{n,^{b}} (x)+}q_{1-}]\) where \(q_{}\) is the \(\)-quantile of the standard Gaussian distribution \((0,1)\)._

Theorem 3.5 does not indicate the value of \(^{2}(x)\). In general, \(^{2}(x)\) is unknown and needs to be estimated. It is common to approximate \(IF^{2}(z;T_{1},)\) with \(IF^{2}(z;T_{1},_{n})\) and set \(^{2}(x)=_{z^{*}}IF^{2}(z_{i};T_{1}, _{n})(x)\). This method is known as the infinitesimal jackknife variance estimator . In Appendix B, we derive the exact closed-form expression of the infinitesimal jackknife variance estimation \(^{2}(x)\), and further prove its consistency as well as the statistical coverage guarantee of confidence intervals built upon it. These results could be of theoretical interest. Yet in practice, the computation of \(^{2}(x)\) requires the evaluation of the NTK Gram matrix and its inversion, which is computationally demanding for large \(n\) and thus not recommended for practical implementation on large datasets.

In the following, we provide two efficient approaches that avoid explicit estimation of the asymptotic variance as in the infinitesimal jackknife approach, and the computation of the NTK Gram matrix inversion.

**PNC-enhanced batching.** We propose an approach for constructing a confidence interval that is particularly useful for large datasets, termed _PNC-enhanced batching_. The pseudo-code is given in Algorithm 2. Originating from simulation analysis , the key idea of batching is to construct a self-normalizing \(t\)-statistic that "cancels out" the unknown variance, leading to a valid confidence interval without explicitly needing to compute this variance. It can be used to conduct inference on serially dependent simulation outputs where the standard error is difficult to compute analytically. Previous studies have demonstrated the effectiveness of batching on the use of inference for Markov chain Monte Carlo  and also the so-called input uncertainty problem . Its application in deep learning uncertainty quantification was not revealed in previous work, potentially due to the additional procedural variability. Integrating it with the PNC predictor, PNC-enhanced batching is very efficient and meanwhile possesses asymptotically exact coverage of its confidence interval, as stated below.

**Input:** Training dataset \(\) of size \(n\). The number of batches \(m^{} 2\).

**Procedure: 1.** Split the training data \(\) into \(m^{}\) batches and input each batch in Algorithm 1 to output \(^{j}_{n^{},^{b}}(x)-^{j}_{n^{},^{b}}(x)\) for \(j[m^{}]\), where \(n^{}=}\).

**2.** Compute \(_{B}(x)=}_{j=1}^{m^{}}(^{j}_{n^{ },^{b}}(x)-^{j}_{n^{},^{b}}(x))\),

and \(S_{B}(x)^{2}=-1}_{j=1}^{m^{}}(^{j}_{n^ {},^{b}}(x)-^{j}_{n^{},^{b}}(x)-_{B}(x ))^{2}\).

**Output:** At point \(x\), output \(_{B}(x)\) and \(S_{B}(x)^{2}\).

**Algorithm 2** PNC-Enhanced Batching

**Theorem 3.6** (Exact coverage of PNC-enhanced batching confidence interval).: _Suppose that Assumption C.2 holds. Suppose that Assumptions B.3 and B.4 hold when \(Y\) is replaced by \(Y-(X)\). Choose any \(m^{} 2\) in Algorithm 2. Then an asymptotically exact \((1-)\)-level confidence interval of \(h^{*}(x)\) is \([_{B}(x)-(x)}{}}q_{1-},_{B} (x)+(x)}{}}q_{1-}]\), where \(q_{}\) is the \(\)-quantile of the \(t\) distribution \(t_{m^{}-1}\) with degree of freedom \(m^{}-1\)._

**PNC-enhanced cheap bootstrap.** We propose an alternative approach for constructing a confidence interval that works for large datasets but is also suitable for smaller ones, termed _PNC-enhanced cheap bootstrap_. The pseudo-code is given in Algorithm 3. Cheap bootstrap  is a modified bootstrap procedure with substantially less retraining effort than conventional bootstrap methods, via leveraging the asymptotic independence between the original and resample estimatorsand asymptotic normality. Note that our proposal is fundamentally different from the naive use of bootstrap or bagging when additional randomness appears [73; 81; 45], which mixes the procedural and data variabilities and does not directly provide confidence intervals. Like PNC-enhanced batching, PNC-enhanced cheap bootstrap also avoids the explicit estimation of the asymptotic variance. The difference between the above two approaches is that PNC-enhanced batching divides data into a small number of batches, and thus is suggested for large datasets while PNC-enhanced cheap bootstrap re-selects samples from the entire dataset, hence suited also for smaller datasets. On the other hand, in terms of running time, when \(R=m^{}-1\), PNC-enhanced cheap bootstrap and PNC-enhanced batching share the same number of network training, but since batching is trained on subsets of the data, the individual network training in PNC-enhanced batching is faster than PNC-enhanced cheap bootstrap. PNC-enhanced cheap bootstrap also enjoys asymptotically exact coverage of its confidence interval, as stated below.

**Input:** Training dataset \(\) of size \(n\). The number of replications \(R 1\).

**Procedure: 1.** Input \(\) in Algorithm 1 to output \(_{n,^{b}}(x)-_{n,^{b}}(x)\).

**2.** For each replication \(j[R]\), resample \(\), i.e., independently and uniformly sample with replacement from \(\{(x_{1},y_{1}),...,(x_{n},y_{n})\}\)\(n\) times to obtain \(^{*j}=\{({x_{1}^{*}}^{j},{y_{1}^{*}}^{j}),...,({x_{n}^{*}}^{j},{y_{ n}^{*}}^{j})\}\). Input \(^{*j}\) in Algorithm 1 to output \(_{n,^{b}}^{j*}(x)-_{n,^{b}}^{j*}(x)\).

**3.** Compute \(_{C}(x)=_{n,^{b}}(x)-_{n,^{b}}(x)\),

and \(S_{C}(x)^{2}=_{j=1}^{R}(_{n,^{b}}^{*j}(x)- {}_{n,^{b}}^{*j}(x)-_{C}(x))^{2}\).

**Output:** At point \(x\), output \(_{C}(x)\) and \(S_{C}(x)^{2}\).

**Algorithm 3** PNC-Enhanced Cheap Bootstrap

**Theorem 3.7** (Exact coverage of PNC-enhanced cheap bootstrap confidence interval).: _Suppose that Assumption C.2 holds. Suppose that Assumptions B.3 and B.4 hold when \(Y\) is replaced by \(Y-(X)\). Choose any \(R 1\) in Algorithm 3. Then an asymptotically exact \((1-)\)-level confidence interval of \(h^{*}(x)\) is \([_{C}(x)-S_{C}(x)q_{1-},_{C}(x)+S_{C}(x)q_{1-}]\) where \(q_{}\) is the \(\)-quantile of the \(t\) distribution \(t_{R}\) with degree of freedom \(R\)._

## 4 Experiments

We conduct numerical experiments to demonstrate the effectiveness of our approaches.2 Our proposed approaches are evaluated on the following two tasks: 1) construct confidence intervals and 2) reduce procedural variability to improve prediction. With a known ground-truth regression function, training data are regenerated from the underlying synthetic data generative process. According to the NTK parameterization in Section C, our base network is formed with two fully connected layers with \(n 32\) neurons in each hidden layer to ensure the network is sufficiently wide and over-parameterized. Detailed optimization specifications are described in Proposition C.3. Our synthetic datasets #1 are generated with the following distributions: \(X([0,0.2]^{d})\) and \(Y_{i=1}^{d}(X^{(i)})+(0,0.001^{2})\). The training set \(=\{(x_{i},y_{i}):i=1,...,n\}\) is formed by drawing i.i.d. samples of \((X,Y)\) from the above distribution with sample size \(n\). We consider multiple dimension settings \(d=2,4,8,16\) and data size settings \(n=128,256,512,1024\) to study the effects on different dimensionalities and data sizes. Additional experimental results on more datasets are presented in Appendix F. The implementation details of our experiments are also provided in Appendix F.

**Constructing confidence intervals.** We use \(x_{0}=(0.1,...,0.1)\) as the fixed test point for confidence intervals construction. Let \(y_{0}=_{i=1}^{d}(0.1)\) be the ground-truth label for \(x_{0}\) without aleatoric noise. Our goal is to construct a confidence interval at \(x_{0}\) for \(y_{0}\). To evaluate the performance of confidence intervals, we set the number of experiments \(J=100\). In each repetition \(j[J]\), we generate a new training dataset from the same synthetic distribution and construct a new confidence interval \([L_{j}(x_{0}),U_{j}(x_{0})]\) with \(95\%\) or \(90\%\) confidence level, and then check the coverage rate (CR): \(=_{j=1}^{J}_{L_{j}(x_{0}) y_{0} U_{j}( x_{0})}\). The primary evaluation of the confidence interval is based on whether its coverage rate is equal to or larger than the desired confidence level. In addition to CR,

[MISSING_PAGE_FAIL:9]

[MISSING_PAGE_FAIL:10]