# Efficiency for Free: Ideal Data Are Transportable Representations

Peng Sun\({}^{1,2}\) Yi Jiang\({}^{1}\) Tao Lin\({}^{2,}\)

\({}^{1}\)Zhejiang University

\({}^{2}\)Westlake University

sunpeng@westlake.edu.cn, yi_jiang@zju.edu.cn, lintao@westlake.edu.cn

Corresponding author.

###### Abstract

Data, the seminal opportunity and challenge in modern machine learning, currently constrains the scalability of representation learning and impedes the pace of model evolution. In this work, we investigate the efficiency properties of data from both optimization and generalization perspectives. Our theoretical and empirical analysis reveals an unexpected finding: for a given task, utilizing a publicly available, task- and architecture-agnostic model (referred to as the 'prior model' in this paper) can effectively produce efficient data. Building on this insight, we propose the Representation Learning Accelerator (ReLA), which promotes the formation and utilization of efficient data, thereby accelerating representation learning. Utilizing a ResNet-18 pre-trained on CIFAR-10 as a prior model to inform ResNet-50 training on ImageNet-1K reduces computational costs by \(50\%\) while maintaining the same accuracy as the model trained with the original BYOL, which requires \(100\%\) cost. Our code is available at: https://github.com/LINs-lab/ReLA.

## 1 Introduction

The available of massive datasets  and recent advances in parallel data processing  have facilitated the rapid evolution of large deep models, such as GPT-4  and LVM . These

Figure 1: **Framework and Intuition of ReLA**: (1) _Framework_: ReLA serves as both a data optimizer and an auxiliary accelerator. Initially, it operates as a data optimizer by leveraging an dataset and a pre-trained model (e.g., one sourced from online repositories) to generate an efficient dataset. Subsequently, ReLA functions as an auxiliary accelerator, enhancing existing (self-)supervised learning algorithms through the effective utilization of the efficient dataset, thereby promoting efficient representation learning. (2) _Intuition_: The central concept of ReLA is to create an efficient-data-driven shortcut pathway within the learning process, enabling the initial model \(\) to rapidly converge towards a ’proximal representation \(\)’ of the target model \(^{*}\) during the early stages of training. This approach significantly accelerates the overall learning process.

models excel in numerous learning tasks, attributable to their impressive representation capabilities. However, the emergence of vast amounts of data within the modern deep learning paradigm raises two fundamental challenges: (i) _the demand for human annotations of huge datasets consumes significant social resources ;_ (ii) _training large models with increasing data and model capacity suffers from intensive computational burden ._

The community has made considerable efforts to enhance learning efficiency. Self-supervised learning methods , with their superior representation learning devoid of human annotations via the self-learning paradigm, attempt to tackle the challenge (i). Concurrently, research has been conducted to mitigate data efficiency issues in challenge (ii): dataset distillation approaches  have successfully synthesized a small distilled dataset, on which models trained on this compact dataset can akin to one trained on the full dataset.

However, challenges (i) and (ii) persist and yet are far from being solved , particularly the intervention of these two learning paradigms. In this paper, we identify two issues: (a) inefficiency in the self-supervised learning procedure compared to conventional supervised learning arises due to sub-optimal self-generating targets ; (b) although training on the distilled dataset is efficient and effective, the distillation process of optimization-based approaches  is computationally demanding , often surpassing the computational load of training on the full dataset. This limitation restricts its potential to accelerate representation learning. To tackle these challenges, we propose a novel open problem in the domain of representation learning:

**Problem 1 (Accelerating Representation Learning through Free Models)**.: _According to the No Free Lunch theorem , it is evident that accelerating the learning process without incorporating prior knowledge is inherently challenging. Fortunately, numerous publicly available pre-trained models can be accessed online, offering a form of free prior knowledge. Despite this, effectively utilizing these models poses several implicit challenges, as these models may not be directly relevant to our target learning task, or they may not be sufficiently well-trained. This leads to a question:_

How can we leverage task- and architecture-agnostic publicly available models to accelerate representation learning for a specific task?

To address Problem 1, we propose ReLA to utilize a freely available model downloaded from the internet to generate efficient data for training. This approach aims to accelerate training during the initial stages by effectively leveraging these generated data, thereby establishing a rapid pathway for representation learning (see Figure 1). Specifically, we list our **five key contributions below** as the first step toward bridging representation learning with data-efficient learning:

(a) _Revealing beneficial/detrimental data properties for efficient/inefficient (self-)supervised learning (see Section 3.2)._ We present a comprehensive analysis of linear models, demonstrating that data properties significantly influence the learning process by impacting the optimization of model training. Our findings reveal that modifications to the data can markedly enhance or impair this optimization. Additionally, we indicate that optimal training necessitates specific data properties--perfect bijective mappings between the samples and targets within a dataset.

(b) _Identifying the inefficiency problems of (self-)supervised learning from a data-centric perspective (see Section 3.3)._ Specifically, we identify several factors contributing to the inefficiencies in (self-)supervised learning over real-world data. For instance, prevalent data augmentation techniques in modern deep learning can introduce a 'noisy mapping' issue, which may exacerbate the negative effects associated with inefficient data.

(c) _Generalization bound for models trained on optimized efficient data (see Section 3.4)._ Although the efficiency properties of data do not inherently ensure the generalization of the trained model, i.e., efficient data alone cannot guarantee generalization ability, we present a generalization bound to analyze models trained on such data.

(d) _A novel method_ ReLA _to generate and exploit efficient data (see Section 4)._ Leveraging our theoretical insights regarding the bounds of generalization and convergence rate, we introduce ReLA, a novel optimization-free method tailored to efficiently generate and effectively exploit efficient data for accelerating representation learning.

(e) _An application of our_ ReLA_: accelerating (self-)supervised learning (see Section 5 and Appendix 1)._ Extensive experiments across four widely-used datasets, seven neural network ar chitectures, eight self-supervised learning algorithms demonstrate the effectiveness and efficiency of ReLA. Training models with ReLA significantly outperforms training on the original dataset with the same budget, and even exceeds the performance of training on higher budget.

## 2 Related Work

This section integrates two distinct deep learning areas: (a) techniques to condense datasets while preserving efficacy; (b) self-supervised learning methods that enable training models on unlabeled data.

### Dataset Distillation: Efficient yet Effective Learning Using Fewer Data

The objective of dataset distillation is to create a significantly smaller dataset that retains competitive performance relative to the original dataset.

**Refining proxy metrics between original and distilled datasets.** Traditional approaches involve replicating the behaviors of the original dataset within the distilled one. These methods aim to minimize discrepancies between surrogate neural network models trained on both synthetic and original datasets. Key metrics for this process include matching gradients [72; 32; 70; 44], features , distributions [71; 73], and training trajectories [11; 17; 22; 18; 68; 24]. However, these methods suffer from substantial computational overhead due to the incesant calculation of discrepancies between the distilled and original datasets. The optimization of the distilled dataset involves minimizing these discrepancies, necessitating multiple iterations until convergence. As a result, scaling to large datasets, such as ImageNet , becomes challenging.

**Extracting key information from original into distilled datasets.** A promising strategy involves identifying metrics that capture essential dataset information. These methods efficiently scale to large datasets like ImageNet-1K using robust backbones without necessitating multiple comparisons between original and distilled datasets. For instance, SRe\({}^{2}\)L  condenses the entire dataset into a model, such as pre-trained neural networks like ResNet-18 , and then extracts the knowledge from these models into images and targets, forming a distilled dataset. Recently, RDED  posits that images accurately recognized by strong observers, such as humans and pre-trained models, are more critical for learning.

**Summary.** We make the following observations regarding scalable dataset distillation methods utilizing various metrics: (a) a few of these metrics have proven effective for data distillation at the scale of ImageNet. (b) all these metrics require human-labeled data; (c) there is currently no established theory elucidating the conditions under which data distillation is feasible; (d) despite their success, the theory behind training neural networks with reduced data is underexplored.

### Self-supervised Learning: Representation Learning Using Unlabeled Data

The primary objective of self-supervised learning is to extract robust representations without relying on human-labeled data. These representations should be competitive with those derived from supervised learning and deliver superior performance across multiple tasks.

**Contrasting self-generated positive and negative Samples.** Contrastive learning-based methods implicitly assign a one-hot label to each sample and its augmented versions to facilitate discrimination. Since InfoNCE , various works [25; 12; 13; 8; 31; 15; 74; 40; 9; 27] have advanced contrastive learning. MoCo [25; 13; 15] uses a momentum encoder for consistent negatives, effective for both CNNs and Vision Transformers. SimCLR  employs strong augmentations and a nonlinear projection head. Other methods integrate instance classification , data augmentation [31; 74], clustering [40; 9], and adversarial training . These enhance alignment and uniformity of representations on the hypersphere .

**Asymmetric model-generating representations as targets.** Asymmetric network methods achieve self-supervised learning with only positive pairs [30; 50; 14], avoiding representational collapse through asymmetric architectures. BYOL  uses a predictor network and a momentum encoder. Richemond et al.  show BYOL performs well without batch statistics. SimSiam  halts the gradient to the target branch, mimicking the momentum encoder's effect. DINO  employs a self-distillation loss. UniGrad  integrates asymmetric networks with contrastive learning methods within a theoretically unified framework.

## 3 Revealing Critical Properties of Efficient Learning over Data

We begin by presenting formal definitions of supervised learning over a (efficient) dataset.

**Definition 1** (Supervised learning over data): _For a dataset \(D=(D_{X},D_{Y})=\{(_{i},_{i})\}_{i=1}^{|D|}\), drawn from the data distribution \((X,Y)\) in space \((,)\), the goal of a model learning algorithm is to identify an optimal model \(^{}\) that minimizes the expected error defined by:_

\[_{(,)(X,Y)}[(^{ }(),)]\,,\] (1)

_where \(\) indicates the loss function and \(\) denotes a predetermined deviation. This is typically achieved through a parameterized model \(_{}\), where \(\) denotes the model parameter within the parameter space \(\). The optimal parameter \(^{D}\) is determined by training the model to minimize the empirical loss over the dataset:_

\[^{D}:=_{ {}}\{(_{};D;) \}:=_{} \{_{i=1}^{|D|}(_{}(_{i}),_{i})\}\,.\] (2)

_The training process leverages an optimization algorithm such as stochastic gradient descent [51; 33]._

**Definition 2** (Data-efficient Learning): _Data-efficient learning seeks to derive an optimized/efficient dataset, denoted as \(S=(S_{X},S_{Y})=\{(_{j},_{j})\}_{j=1}^{|S|}\), from the original dataset \(D\). The objective is to enable models \(_{^{S}}\) trained on \(S\) to achieve the desired generalization performance, as defined in (1), with fewer training steps and a reduced computational budget compared to training on the original dataset \(D\)._

### Unifying (Self-)Supervised Learning from a Data-Centric Perspective

To ease the understanding and our methodology design in Section3.4, we unify both conventional supervised learning and self-supervised learning as learning to map samples in \(D_{X}\) to targets in \(D_{Y}\): this view forms'supervised learning' from a data-centric perspective. Specifically, these two learning paradigms involve generating targets \(D_{Y}=\{=() D_{X}\}\) and minimizing the empirical loss (2). The only difference lies in the target generation models (or simply labelers) \(\):

1. _Conventional supervised learning_, referred to as human-supervised learning, generates targets via human annotation. Note that the targets are stored and used statically throughout the training.
2. _Self-supervised learning_ (also see Footnote 2), e.g., BYOL  utilizes an Exponential Moving Average (EMA) version of the learning model \(_{}\) to generate targets \(=[_{}]()\). Note that the targets are dynamically changing during training as the model \(_{}\) keeps evolving.

This unified perspective allows us to jointly examine and address the inefficient training issue of (self-)supervised learning from a data-centric perspective, in which in Section3.2 we first study the impact of samples \(D_{X}\) and targets \(D_{Y}\) on the model training process and then investigate whether and how a distilled dataset \(S=(S_{X},S_{Y})\) can facilitate this process.

### Empirical and Theoretical Investigation of Data-Centric Efficient Learning

To elucidate the ideal data properties of training on a dataset \(D\), we examine the simple task over a bimodal Gaussian mixture distribution as a case study. We begin by defining the problem.

**Definition 3** (Bimodal Gaussian mixture distribution): _Given two Gaussian distributions \(_{0}(_{1},^{2})\) and \(_{1}(_{2},^{2})\), where \(_{1}\) and \(_{2}\) are the means and \(^{2}\) is the variance (here we set \(_{1}=1\), \(_{2}=2\) and \(=0.5\)). We define a bimodal mixture data \(G=(G_{X},G_{Y})\) as:_

\[G:=\{(,y)=(1-y)_{0}+y_{1 }\}\ \ \ \ y(p=0.5),_{0}_{0},_{1 }_{1}\,.\] (3)

_Moreover, we define a corresponding binary classification neural network model as:_

\[f_{}():=(^{} (^{}+^{})+^{})\,,\] (4)_where \((z)=}\) is the sigmoid activation function; \((z)=(0,z)\) is the activation function for the hidden layer, which provides non-linearity to the model; \(^{}\) and \(^{}\) are the weights and biases of the hidden layer; \(^{}\) and \(^{}\) are the weights and biases of the output layer._

Modern representation learning fundamentally relies on optimization (see Definition1). We show that modifications to the data can influence the convergence rate of the optimization process, thereby impacting the overall representation learning procedure. Furthermore, we try to uncover several key properties of data efficiency through our theoretical analysis of the case study. In the following, we denote the modified distribution by \(G^{}=(G^{}_{X},G^{}_{Y})\) and examine the altered samples \(G^{}_{X}\) and corresponding targets \(G^{}_{Y}\) independently.

Investigating the properties of modified samples.The modification process here only rescales the variance of the original sample distribution \(G_{X}\) defined in Definition3 with new \(\) (rather than the default \(0.5\)), while let \(G^{}_{Y}:=G_{Y}\); see explanations in AppendixE. Therefore, we examine the distilled samples \(G^{}_{X}\) by setting the variable \(\) within the interval \((0,1)\).

Results in Figure2 demonstrate that the distilled samples \(G^{}_{X}\) with smaller variance \(\) achieve faster convergence and better performance compared to that of \(G\). To elucidate the underlying mechanism, we provide a rigorous theoretical analysis in AppendixB, culminating in Theorem1.

**Theorem 1** (Convergence rate of learning on efficient samples).: _For the classification task stated in Definition3, the convergence rate for the model \(f_{}\) trained \(t\) after steps over distilled data \(G^{}\) is:_

\[_{_{t}}[(f_{_{t}};G^{}; )-(f_{^{*}};G^{};)]}(^{2})\,,\] (5)

_where \(\) denotes the MSE loss, i.e., \((,y):=\|-y\|^{2}\), and \(f_{^{*}}\) indicates the optimal model, \(}\) signifies the asymptotic complexity. Modified samples characterized by a smaller value of \(\) facilitate faster convergence._

Investigating the properties of modified targets.On top of the property understanding for modified samples, we further investigate the potential of modified targets via \(G^{}\). In detail, for modified samples, we consider the most challenging (c.f. Figure2b) yet the common case, namely \(G^{}_{X}\) with \(=1\) (see explanations in AppendixM). For the corresponding modified targets \(G^{}_{Y}\), similar to the prior data-efficient methods , for any sample \(\) drawn from \(G^{}_{X}\), we refine its label by assigning \(= f_{^{*}}()+(1-) y\). Here, \(\) denotes the relabeling intensity coefficient, and \(f_{^{*}}\) represents a strong pre-trained model (simply, we utilize the model trained on the data in Figure2c).

**Theorem 2** (Convergence rate of learning on re-labeled data).: _For the classification task as in Definition3, we have the convergence rate for the model \(f_{}\) trained after \(t\) steps over modified data \(G^{}\):_

\[_{_{t}}[(f_{_{t}};G^{}; )-(f_{^{*}};G^{};)]}(1-)\,.\] (6)

_Note that \(\) controls the upper bound of the convergence rate, indicating that using modified targets with a higher value of \(\) enables faster convergence._

Results in Figure2 illustrate that the modified targets \(G^{}_{Y}\) with higher values of \(\) lead to faster training convergence and better performance. See theoretical analysis in Theorem2 and AppendixB.

### Extended Understanding of Data-Centric Efficient Learning

The empirical and theoretical investigations regarding the properties of modified samples \(G^{}_{X}\) and targets \(G^{}_{Y}\) in Section3.2 are limited to a simplified case (as described in Definition3) and may not extend to all practical scenarios, such as training a ResNet  on the ImageNet dataset .

Interestingly, we observe that the advantageous modifications of both samples \(G^{}_{X}\) and targets \(G^{}_{Y}\) converge towards a unified principle: minimizing or preventing any sample \(\) from being labeled with multiple or inaccurate targets \(\). This principle emphasizes the importance of providing accurate and informative targets \(\) for each sample \(\), as analyzed in Remark1, and suggests extending this insight to any complex dataset like \(S\).

**Remark 1** (Ideal data properties avoid implicitly introduced gradient noise from data): _Intuitively, the semantic information within each sample \(\) should be unique and not identical to another sample. Consequently, the exact target \(\), which represents the semantic information of \(\), should also be unique and informative. This implies the necessity of establishing bijective (or one-to-one) mappings between samples and their respective targets._

_In contrast, when a sample \(\) (or several similar samples) is labeled with multiple different targets \(\), it may implicitly introduce noise into the gradient \((,)\), thereby hindering the optimization._

However, real-world datasets often deviate from the ideal properties described above, as discussed in Remark2 below and further analyzed in AppendixM.

**Remark 2** (Imperfect mappings and inaccurate targets in real-world datasets): _In practice, we observe that 'noisy mappings' between input samples and targets are prevalent in real-world datasets. As illustrated in Figure6, several common phenomena contribute to this issue:_

* _Similar or identical input samples may be assigned different targets due to using data augmentations, which is common in both self-supervised and human-supervised learning settings._
* _Inaccurate targets may be generated, particularly in self-supervised learning scenarios._
* _In human-supervised learning, all samples within a class are mapped to a one-hot target._

_These imperfect mappings and inaccurate targets pose challenges to achieving optimal training efficiency and effectiveness for real-world datasets._

### Generalization-bounded Efficient Data Synthesis

Given insights from Remark1, an effective approach to generate efficient data \(S\) from original data \(D\) involves employing a high-quality labeler \(\) to relabel each sample \(\) within \(D\). This process

Figure 3: **Investigating modified targets with varied \(\) values. We present a visualization of the validation loss landscape in Figure3(a), including three training trajectories that correspond to different \(\) settings. Figure3(b) illustrates the performance of models trained using targets with varying \(\) values. The optimal scenario for our task, which uses targets with \(=1.0\), is depicted in Figure3(c).**

Figure 2: **Investigating modified samples with varied \(\) values. Following , Figure2(a) visualizes the validation loss landscape within a two-dimensional parameter space, along with three training trajectories corresponding to different \(\) settings. Figure2(b) illustrates the performance of models trained using samples with varied \(\). The optimal case in our task, utilizing samples with \(=0.1\) (which achieves the lowest validation loss in Figure2(b)), is visualized in Figure2(c), where the color bar represents the values of targets \(y\).**

[MISSING_PAGE_FAIL:7]

### ReLA-D (): Synthesis of Efficient Dataset

Motivated by two property requirements in Definition5, here we introduce our optimization-free synthesis process of both samples and targets in our ReLA-D (see technical details in F).

**Generating transportable representations as the targets.** We argue that _well-trained models (called prior models) on diverse real-world datasets using various neural network architectures and algorithms converge towards the same linear representation space. In other words, the generated pseudo representations \(R_{Y}\) for samples \(D_{X}\) using these prior models are linearly transportable to each other and to the human-annotating targets._ The empirical verifications refer to AppendixA. We further justify in AppendixF.1 that the requirement 1 in Definition5 can be achieved by employing a prior model as the ideal labeler \(:^{d}^{m}\), i.e., generating \(R_{Y}=\{() D_{X}\}\) as the targets. The generation process of targets is conducted only once (refer to AppendixH for details), and the generated targets \(R_{Y}\) are stored and combined with the samples \(D_{X}\) to form the data \(D=(D_{X},R_{Y})\).

Efficient and distribution-aligned sample generation.To satisfy requirement 2 in Definition5 efficiently, we employ basic data augmentations into data \(D_{X}\) such as RandomResizeCrop with a minimum scale of 0.5 (as opposed to the default of 0.08) and RandomHorizontalFlip with \(p=0.5\).

### ReLA-F (): Assist Learning with Generated Efficient Dataset

In this section, we showcase the significance of understanding ideal data properties and generated efficient dataset in assisting self-supervised learning, given this self-supervised paradigm on unlabeled data suffers from significant inefficiency issues compared to human-supervised learning .

Here we propose a plug-and-play method that can be seamlessly integrated into any existing self-supervised learning algorithm, significantly enhancing its training efficiency by introducing an additional loss term. Formally, the loss function is defined as follows:

\[_{}+(1-)_{}\,\ \ _{}:=_{,(D_{X},R_{Y })}[(_{}()-, )]\,\] (9)

where \((,):=1-/(\|\|\|\|\|)\) be the loss function, \(_{}\) denotes the loss specified by any self-supervised learning method, respectively. Furthermore, the data \(D=(D_{X},R_{Y})\) are collected using the strategy outlined in Section4.1, with updates occurring at each \(k\)-th epoch.

The dynamic coefficient \(\{0,1\}\) divides the training process into two distinct stages. Initially, \(\) is set to \(1\) to emphasize \(_{}\), assuming its crucial role in the early learning phase. As the model \(_{}\) improves and self-generated targets become more reliable in \(_{}\), an adaptive attenuation algorithm adjusts \(\) to \(0\) (note that the initial \(\) is tuning-free for all cases and see AppendixJ for details). As a result, only a single loss term in (9) is calculated, ensuring no extra computational cost with ReLA.

To enhance the recognition of ReLA-aided algorithms, we re-denote those that are used in their names. For example, the BYOL algorithm , when enhanced with ReLA, is re-denoted as BYOL (). Furthermore, as the prior models downloaded from the internet are not consistently robust, the aforementioned dynamic setting of \(\) also prevents the model \(_{}\) from overfitting to potentially weak generated targets. The efficacy of our proposed ReLA is empirically validated in Section5.

## 5 Experiments

This section describes the experimental setup and procedures undertaken to test our hypotheses and evaluate the effectiveness of our proposed methodologies.

Experimental setting.We list the settings below (see more details in AppendixK).

\(\)_Datasets:_ For low-resolution data (\(32 32\)), we evaluate our method on two datasets, i.e., CIFAR-10  and CIFAR-100 . For high-resolution data, we conduct experiments on two large-scale datasets including Tiny-ImageNet (\(64 64\))  and full ImageNet-1K (\(224 224\)) , to assess the scalability and effectiveness of our method on more complex and varied datasets.

\(\)_Neural network architectures:_ Similar to prior works/benchmarks of dataset distillation  and self-supervised learning [57; 19], we use several backbone architectures to evaluate the generalizabilityof our method, including ResNet-{18, 50, 101} , EfficientNet-B0 , MobileNet-V2 , ViT , and a series of CLIP-based models . These architectures represent a range of model complexities and capacities, enabling a comprehensive assessment of our approach.

\(\)_Baselines:_ Referring to a prior widely-used benchmark [57; 19], we consider several state-of-the-art methods as baselines for a broader practical impact, including: SimCLR , Barlow Twins , BYOL , DINO , MoCo , SimSiam , SwAV , and Vicreg .

\(\)_Evaluation:_ Following previous benchmarks and research [57; 19; 12; 3], we evaluate all the trained models using offline linear probing strategy to reflect the representation ability of the trained models, and ensure a fair and comprehensive comparison with baseline approaches.

\(\)_Implementation details:_ We implement our method by extending a popular self-supervised learning open-source benchmark  and use their configurations therein. This includes using AdamW as the optimizer, with a mini-batch size of 128 (except for ImageNet-1K, where we use a mini-batch size of 512). We implement our method through PyTorch , and all experiments are conducted on NVIDIA RTX 4090 GPUs. See more detailed configurations and hyper-parameters in Appendix K.

### Primary Experimental Results and Analysis

Recall that our ReLA-D (), as illustrated in Figure I and Section 4.1, requires an unlabeled dataset and _any pre-trained model freely available online_ to generate the efficient dataset. To justify the superior performance and generality of our ReLA across various unlabeled datasets using prior models with different representation abilities, our comparisons in this subsection start with BYOL 2 and then extend to other self-supervised learning methods.

Table I demonstrates the efficacy and efficiency of our ReLA in facilitating the learning of robust representations. Overall, _BYOL ( ) consistently outperforms the original BYOL_ when trained with a reduced budget. In certain cases, such as on CIFAR-100, BYOL ( ) employing only \(10\%\) of the budget can surpass the performance of BYOL-trained models using the entire budget Specifically:

1. A stronger prior model (e.g., CLIP) enhances the performance of ReLA more effectively than a weaker model (e.g., Rand.);
2. Our ReLA is not sensitive to the prior knowledge. For instance, using CF10-T as the prior model can achieve competitive performance compared to that trained on extensive datasets (e.g., CLIP);

   Dataset & \% & BYOL & Rand. & CF10-T & CF100-T & TIN-T & IN1K-T & CLIP-RN50 & BYOL\({}^{}\) \\   & 10 & 58.3 \(\) 0.1 & 71.4 \(\) 0.0 & 81.1 \(\) 0.1 & 78.2 \(\) 0.1 & 79.6 \(\) 0.1 & 81.6 \(\) 0.0 & **82.0 \(\) 0.1** &  \\  & 20 & 70.1 \(\) 0.2 & 77.1 \(\) 0.2 & 83.6 \(\) 0.1 & 81.4 \(\) 0.0 & 83.2 \(\) 0.1 & **84.4 \(\) 0.1** & 83.9 \(\) 0.1 & 82.7 \(\) 0.2 \\  & 50 & 77.9 \(\) 0.0 & 82.7 \(\) 0.1 & 86.5 \(\) 0.1 & 86.2 \(\) 0.0 & 86.2 \(\) 0.1 & **87.3 \(\) 0.2** & 86.5 \(\) 0.0 & \\   & 10 & 26.9 \(\) 0.2 & 41.8 \(\) 0.2 & 51.4 \(\) 0.1 & 51.4 \(\) 0.1 & 53.5 \(\) 0.1 & **56.4 \(\) 0.2** & 55.4 \(\) 0.1 &  \\  & 20 & 34.8 \(\) 0.3 & 48.1 \(\) 0.1 & 55.7 \(\) 0.1 & 55.7 \(\) 0.1 & 56.7 \(\) 0.0 & **59.5 \(\) 0.1** & 57.9 \(\) 0.0 & 52.5 \(\) 0.3 \\  & 50 & 41.4 \(\) 0.3 & 54.6 \(\) 0.2 & 59.7 \(\) 0.1 & 59.8 \(\) 0.1 & 60.0 \(\) 0.1 & **61.6 \(\) 0.1** & 61.0 \(\) 0.0 & \\   & 10 & 25.1 \(\) 0.3 & 34.5 \(\) 0.3 & 39.0 \(\) 0.1 & 38.4 \(\) 0.0 & 41.2 \(\) 0.1 & **41.6 \(\) 0.1** & 39.6 \(\) 0.4 &  \\  & 20 & 30.7 \(\) 0.1 & 38.2 \(\) 0.0 & 41.9 \(\) 0.0 & 42.3 \(\) 0.0 & 43.2 \(\) 0.1 & **44.1 \(\) 0.1** & 42.6 \(\) 0.1 & 43.6 \(\) 0.3 \\  & 50 & 37.7 \(\) 0.2 & 43.9 \(\) 0.1 & 45.6 \(\) 0.1 & 45.9 \(\) 0.1 & 45.8 \(\) 0.1 & **46.4 \(\) 0.1** & 46.3 \(\) 0.1 & \\   & 10 & 44.5 \(\) 0.1 & 51.7 \(\) 0.1 & 53.7 \(\) 0.1 & 53.3 \(\) 0.1 & 53.6 \(\) 0.1 & 54.9 \(\) 0.1 & **56.2 \(\) 0.1** &  \\  & 20 & 55.3 \(\) 0.0 & 56.9 \(\) 0.0 & 57.6 \(\) 0.1 & 57.6 \(\) 0.1 & 57.8 \(\) 0.1 & 58.0 \(\) 0.0 & **59.5 \(\) 0.1** & 61.9 \(\) 0.1 \\   & 50 & 60.8 \(\) 0.2 & 61.1 \(\) 0.1 & 62.1 \(\) 0.1 & 61.8 \(\) 0.1 & 61.7 \(\) 0.0 & 61.9 \(\) 0.0 & **62.9 \(\) 0.1** &  \\   

Table 1: **Benchmark our ReLA with various prior models against BYOL**. We compare evaluation results of the models trained using \(\) BYOL with \(10\%\), \(20\%\) and \(50\%\) training budget/steps; \(\) BYOL ( ) with different prior models; \(\) BYOL with full budget, denoted as BYOL\({}^{}\) in this table. Regarding the prior models used for our ReLA, we respectively utilize six models with increasing representation capabilities, including \(\) randomly initialized network (Rand.); \(\) four BYOL\({}^{}\)-trained models (CF10-T, CF100-T, TIN-T, IN1K-T) corresponding to four datasets (listed below); \(\) CLIP-RN50. The evaluations are performed across four datasets, i.e., CIFAR-10 (CF-10), CIFAR-100 (CF-100), Tiny-ImageNet (T-IN), and ImageNet-1K (IN-1K). We underline the results that outperform the full training, and **bold** the results that achieve the highest performance using a specific ratio of budget. All the networks used for training are ResNet-18, except the ResNet-50 used for IN-1K.

3. A randomly initialized model can effectively aid in accelerating learning through our ReLA. This can be considered an effective scenario of "weak-to-strong supervision"  using pseudo targets.

**Cross-architecture generalization.** ReLA-D () generates efficient datasets using a specific neural architecture. To evaluate the generalization ability of these datasets, it is essential to test their performance on various architectures not used in the distillation process. Table 2 presents the performance of our ReLA in conjunction with various prior models and trained model architectures, demonstrating its robust generalization ability. Specifically:

1. [label=()]
2. The integration of ReLA always enhances the performance of original BYOL;
3. Our ReLA method exhibits minimal sensitivity to the architecture of the prior model, as evidenced by the comparable performance of BYOL () using both ViT-based and ResNet-based models.

**Combining ReLA across various self-supervised learning methods.** To demonstrate the effectiveness and versatility of ReLA in enhancing various self-supervised learning methods, we conduct experiments with widely-used techniques. Table 3 presents the results, highlighting the robust generalization capability of ReLA. Our findings consistently show that ReLA improves the performance of these methods while maintaining the same budget ratio, emphasizing its potential on learning using unlabeled data. Additionally, we provide the results when combining ReLA with human-supervised learning in Appendix I.

## 6 Conclusion and Limitation

In this paper, to address the Problem 1, we investigate the optimal properties of data, including samples and targets, to identify the properties that improve generalization and optimization in deep learning models. Our theoretical insights indicate that targets which are informative and linearly transportable to strong representations (e.g., human annotations) enable trained models to exhibit robust representation abilities. Furthermore, we empirically find that well-trained models (called prior models) across various tasks and architectures serve as effective labelers for generating such targets. Consequently, we propose the Representation Learning Accelerator (ReLA), which leverages any freely available prior model to generate high-quality targets for samples. Additionally, ReLA can enhance existing (self-)supervised learning approaches by utilizing these generated data to accelerate training. However, our theoretical analysis is restricted to the simplified scenario described in Definition 3, which has limited applicability in real-world contexts.

   Dataset & Arch. & Original & \(\,\,\) w/ RN18 & RN101 & RN50x4 & ViT B/32 & ViT B/16 & ViT L/14 \\   & ResNet-18 & \(58.3 0.1\) & \(71.4 0.0\) & \(81.9 0.1\) & \(82.1 0.3\) & \(83.2 0.2\) & \(83.1 0.1\) & \(82.4 0.1\) \\  & MobileNet-V2 & \(47.7 0.1\) & \(69.4 0.0\) & \(82.2 0.1\) & \(80.8 0.0\) & \(81.6 0.1\) & \(82.9 0.2\) & \(81.2 0.2\) \\  & EfficientNet-B0 & \(23.9 0.2\) & \(68.8 0.6\) & \(83.2 0.2\) & \(83.9 0.1\) & \(87.4 0.1\) & \(86.4 0.1\) & \(83.1 0.1\) \\  & ViT T/16 & \(43.4 0.1\) & \(57.1 0.1\) & \(65.9 0.0\) & \(66.4 0.1\) & \(69.9 0.3\) & \(68.8 0.1\) & \(63.7 0.1\) \\   & ResNet-18 & \(25.1 0.3\) & \(34.5 0.3\) & \(38.3 0.1\) & \(39.1 0.4\) & \(35.8 0.1\) & \(32.4 0.1\) & \(28.4 0.2\) \\  & MobileNet-V2 & \(8.8 0.1\) & \(28.3 0.3\) & \(39.9 0.1\) & \(36.8 0.2\) & \(36.0 0.0\) & \(37.9 0.3\) & \(20.6 0.5\) \\   & EfficientNet-B0 & \(4.1 0.0\) & \(33.2 0.3\) & \(43.5 0.2\) & \(41.7 0.2\) & \(44.0 0.1\) & \(44.2 0.0\) & \(37.9 0.1\) \\   & ViT T/16 & \(12.5 0.1\) & \(24.6 0.0\) & \(26.1 0.1\) & \(27.6 0.1\) & \(26.9 0.2\) & \(24.5 0.1\) & \(21.6 0.0\) \\   

Table 2: **Evaluating our ReLA on cross-architecture settings. Our ReLA-D () distills datasets with prior RN18 (Rand.) and CLIP-{RN101, RN50\(\)4, ViT B/32, ViT B/16, ViT L/14}, then versus transfer to ResNet-18; MobileNet-V2; EfficientNet-B0; ViT T/16. We train models using \(10\%\) budget through (original) BYOL ().**

   Dataset & Method & SimCLR & Barlow & DINO & MoCo & SimSiam & SwAV & Vicreg \\   & Original & \(70.7 0.2\) & \(63.7 0.3\) & \(66.2 0.2\) & \(67.4 0.4\) & \(45.8 0.4\) & \(66.2 0.3\) & \(71.3 0.2\) \\  & () w/ Rand. & \(70.9 0.0\) & \(68.8 0.2\) & \(70.6 0.1\) & \(70.9 0.1\) & \(66.7 0.1\) & \(69.5 0.2\) & \(71.3 0.1\) \\  & CLIP-RN50 & \(76.4 0.1\) & \(76.5 0.2\) & \(82.4 0.1\) & \(79.8 0.1\) & \(79.3 0.1\) & \(77.3 0.0\) & \(80.1 0.1\) \\   & Original & \(30.4 0.1\) & \(28.9 0.4\) & \(26.7 0.3\) & \(27.1 0.2\) & \(17.8 0.3\) & \(20.2 0.1\) & \(34.0 0.1\) \\  & () w/ Rand. & \(30.7 0.2\) & \(31.9 0.1\) & \(29.4 0.2\) & \(33.4 0.1\) & \(25.4 0.1\) & \(29.1 0.2\) & \(34.1 0.1\) \\   & CLIP-RN50 & \(33.0 0.3\) & \(33.5 0.2\) & \(35.1 0.0\) & \(37.1 0.1\) & \(32.6 0.1\) & \(32.6 0.1\) & \(39.1 0.2\) \\   

Table 3: **Evaluating our ReLA across different self-supervised learning methods. We extend our analysis beyond BYOL by training and evaluating models using seven additional self-supervised learning methods, along with their ReLA-augmented counterparts (), utilizing randomly initialized ResNet-18 (Rand.) and CLIP-{RN50} as prior models for the ReLA-D (). All methods are trained using \(10\%\) budget.**

## 7 Acknowledgement

We thank Xinyi Shang, Bowen Ding and and the anonymous reviewers for their invaluable comments and feedback. We also thank Bei Shi for assisting with the partial code implementation. This work was supported in part by the National Science and Technology Major Project (No. 2022ZD0115101), the Research Center for Industries of the Future (RCIF) at Westlake University, and the Westlake Education Foundation.