# Posterior Sampling via Autoregressive Generation

Kelly W. Zhang

Department of Mathematics

Imperial College London

&Tiffany (Tianhui) Cai

Department of Statistics

Columbia University

&Hongseok Namkoong

Decision, Risk, and Operations Division

Columbia Business School

&Daniel Russo

Decision, Risk, and Operations Division

Columbia Business School

Co-first authors.

###### Abstract

Real-world decision-making requires grappling with a perpetual lack of data as environments change; intelligent agents must comprehend uncertainty and actively gather information to resolve it. We propose a new framework for learning bandit algorithms from massive historical data, which we demonstrate in a cold-start recommendation problem. First, we use historical data to pretrain an autoregressive model to predict a sequence of repeated feedback/rewards (e.g., click responses to news articles shown to sequences of users). In learning to make accurate predictions, the model implicitly learns an informed prior based on rich action features (e.g., article headlines) and how to sharpen beliefs as more rewards are gathered (e.g., clicks as each article is recommended). At decision-time, we autoregressively sample (impute) an imagined sequence of rewards for each action, and choose the action with the largest average imputed reward. Far from a heuristic, our approach is an implementation of Thompson sampling (with a learned prior), a prominent active exploration algorithm. We prove our pretraining loss directly controls online decision-making performance, and we demonstrate our framework on a news recommendation task where we integrate end-to-end fine-tuning of a pretrained language model to process news article headline text to improve performance.

## 1 Introduction

Real-world decision-making requires grappling with a perpetual lack of data as environments change; intelligent agents must comprehend uncertainty and actively gather information to resolve it. This is especially challenging with tasks involving unstructured inputs such as text and images. This paper offers a fresh perspective, casting the problem of balancing exploration and exploitation in online decision-making as a problem of training and sampling from an autoregressive generative sequence model, an area experiencing rapid innovation [2; 23; 50].

**Problem setting.** We present our insights by deriving a novel solution to a meta-bandit problem [51; 8; 26; 3], in which an agent repeatedly encounters new tasks that require exploring to gather useful information. In real applications this meta-learning structure is common, and we illustrate our approach using a news article recommendation setting: Each day a batch of new articles is released, and upon release, the system observes each article's text but is uncertain about how engaging each article will be, as some articles may be surprise hits, or others may be less popular than expected. Models that solely rely on article text will eventually be outperformed by simple alternatives thatlearn from repeated user feedback. This example highlights the need to use rich features (e.g., article headline) and the need to acquire further information through active exploration.

**Our Algorithm.** Our proposed solution proceeds in two phases. In the pre-training phase, the agent learns to model uncertainty by learning a simulator of user interactions using historical data on previously released articles. The simulator is an autoregressive sequence model that uses an article's attributes (e.g. headline text) to predict sequences of recommendation outcomes across users for that article. In the online decision-making phase, the agent models its uncertainty by simulating recommendation outcomes for new users with the pretrained sequence model. At each decision time, the agent uses the fitted simulator to autoregressively sample imagined/imputed recommendation outcomes for new users, conditioned on article features and on previously observed outcomes. The agent then takes the action with the greatest imputed average reward.

**Formal Connections to Thompson Sampling (TS).** Far from a heuristic, our approach is a principled implementation of TS (with a learned prior), a prominent bandit algorithm with strong guarantees .implementations of TS, our approach never performs explicit Bayesian inference regarding latent parameters, and instead relies only on predicting and generating observable quantities. This enables standard ML tools for training. The connection between autoregressive sampling and TS rests on a link between exchangeable sequence modeling and Bayesian inference that has been known since de Finetti's seminal work , and has appeared in several different literatures .

**Theoretical Guarantees and Empirical Evaluations.** We provide formal links between interactive decision-making and sequence prediction, including a novel regret bound that scales with the pre-training loss of the sequence model. Furthermore, we demonstrate that our theoretical insights bear out on a news recommendation task that incorporates a language model.

## 2 Problem formulation

**Online Decision-Making Problem.** Each online decision-making phase begins with new articles (actions) \(^{}\) being released. Each article \(a^{}\) is associated with attributes \(Z^{(a)}\); in our experiments these represent article headlines. Even with rich article headline features \(Z^{(a)}\), the system is uncertain about how engaging articles will be to readers. The system interacts sequentially with distinct users \(t\{1,2,,T\}\) and can adapt future recommendations based on initial user feedback. To the \(t^{}\) user, it recommends \(A_{t}^{}\), observes an outcome \(Y_{t}\), and constructs a reward \(R(Y_{t})\) by applying a fixed, known function \(R()\). The vector of outcomes \(Y_{t}\) could include a variety of user feedback like whether the user clicked or shared the recommended article.

Each action \(a\) has \(T\) potential outcomes \(Y^{(a)}_{1:T}=(Y^{(a)}_{1},...,Y^{(a)}_{T})\). The observed outcome is \(Y_{t} Y^{(A_{t})}_{t}\) if article \(A_{t}\) is recommended to the \(t^{}\) user. We assume articles are drawn independently from some fixed article distribution, i.e., \(\{Z^{(a)},Y^{(a)}_{1:T}\}\) are drawn i.i.d. across articles \(a\). This assumption precludes resolving uncertainty about the effectiveness of one article by gathering feedback on a different article in the online decision-making phase. Conditioned on the article features \(Z^{(a)}\), potential outcomes are drawn from a fixed and unknown distribution \(p^{*}\):

\[Y^{(a)}_{1:T} Z^{(a)} p^{*} Z^{(a)}.\] (1)

Finally, we assume \(p^{*}\) is exchangeable, meaning that for any permutation \(\) over \(\{1,,T\}\), any \(z\), and any outcomes \((y_{1},,y_{T})\),

\[p^{*}(y_{1},,y_{T} z)=p^{*}(y_{(1)},,y_{(T)} z).\] (2)

Exchangeability means outcomes from recommendations made to a large subset of \(m<T\) users is likely to be representative of outcomes that would have been observed among all \(T\) users (Appendix J).

Our goal is to develop an adaptive algorithm \(\) for recommending articles that maximizes the expected average reward \(_{p^{*},}_{t=1}^{T}R(Y^{(A_{t})}_{t})\) (where the expectation is taken over draws of \(^{}\) in addition to the randomness in \(p^{*}\) and \(\)), or equivalently, minimizes the per-user Bayesian regret,

\[(;p^{*}):=_{p^{*},}_{a^{}}_{t=1}^{T}R(Y^{(a)}_{t})}- _{t=1}^{T}R(Y^{(A_{t})}_{t}).\] (3)In (3), we calculate the gap in reward relative to a baseline that always recommends the action with best performance in the (finite) population.

**Pretraining Phase.** The goal of the pre-training phase is to learn a good active exploration algorithm to deploy in the online decision-making phase. We have access to a historical dataset \(}^{}:=\{Z^{(a)},Y^{(a)}_{1:n}:a^{ }\}\), with action attributes \(Z^{(a)}\) and observed outcomes \(Y^{(a)}_{1:n}\) from previous articles (actions) \(a^{}\), for some \(n T\). We assume this dataset is drawn from the same data generating distribution as in the online decision-making phase: Across \(a^{}\), \(Z^{(a)}P_{Z}\) and \(Y^{(a)}_{1:n}\) is a completely random subset of size \(n\) of \(Y^{(a)}_{1:T}\), where \(Y^{(a)}_{1:T} Z^{(a)} p^{*}( Z^{(a)})\).

## 3 Posterior Sampling via Autoregressive Generation

This work considers _unobserved outcome data_ as the source of a decision-maker's uncertainty (Figure 1): for a given article, we only have responses from some users, and there is residual uncertainty in how future users would respond. Inspired by this viewpoint, our method proceeds in two steps:

**Phase 1: Pretraining an Autoregressive Model.** We train an autoregressive sequence model \(p_{}\), with parameter \(\), that can predict missing outcomes, conditioned on article (action) attributes, and limited previously observed outcomes. This enables us to generate hypothetical completions of the potential outcome table in Figure 1. Formally, this model specifies a probability \(p_{}(Y^{(a)}_{t} Z^{(a)},Y^{(a)}_{1:t-1})\) of observing outcome \(Y^{(a)}_{t}\) in the next interaction given article attributes \(Z^{(a)}\) and previous outcomes \(Y^{(a)}_{1:t-1}\). We minimize the following loss on the historical dataset \(^{}\):

\[(p_{};^{})=-_{a^{}} p_{}(Y^{(a)}_{1:n} Z^{(a)})=-_{a^{}}_{t=1}^{n} p_{}(Y^{(a)}_{t} Z^{(a)},Y^{(a)}_{1:t-1}).\] (4)

Our approach to pre-training an approximate exchangeable sequence model can also be thought of as empirical Bayes (Appendix C). Our approach also mirrors recent work on neural processes [20; 25; 35; 28] and prior-data fitted networks . Our main contribution is linking this pretrained sequence model to online decision-making, which we present next.

**Phase 2: Online Decision-Making via Autoregressive Generation.** After a sequence model \(p_{}\) is trained on historical data, it is deployed and used for decision-making. No additional training of \(p_{}\) is needed. At each decision time, our algorithm uses \(p_{}\) to autoregressively generate imputed values of missing outcomes for each candidate action \(a^{}\), as seen in Figure 2. At decision time \(t\), let \(^{(a)}_{}\) denote indices of the users \([1:T]\) for which article/action \(a\) has not been recommended so far. The algorithm samples (imputes) outcomes \(^{(a)}_{}\) for each \(^{(a)}_{}\) conditional on article attributes \(Z^{(a)}\), as well as previously observed and generated outcomes for article \(a\). It then uses both the observed and generated outcomes to compute an imputed mean reward for action \(a\):

\[^{(a)}_{t}_{[1:T]^{(a)}_{}}RY^{(a)}_{}+_{ ^{(a)}_{}}R^{(a)}_{} }.\] (5)

Finally, the algorithm selects action \(A_{t}=_{a^{}}^{(a)}_{t}}\). Then the real outcome \(Y^{(A_{t})}_{t}\) is observed. The process is repeated at the next decision time. See Algorithm 2 for further details.

```
1:\(^{(a)}_{t}_{[1:T]^{(a)}_{}}RY^{(a)}_{}+_{ ^{(a)}_{}}R^{(a)}_{}}\)
2:\(^{(a)}_{t}_{[1:T]^{(a)}_{}}RY^{(a)}_{}+_{ ^{(a)}_{}}R^{(a)}_{}}\)
3:\(^{(a)}_{t}_{[1:T]^{(a)}_{}}RY^{(a)}_{}+_{ ^{(a)}_{}}R^{(a)}_{}}\)
4:\(^{(a)}_{t}_{[1:T]^{(a)}_{}}RY^{(a)}_{}+_{ ^{(a)}_{}}R^{(a)}_{}}\)
5:\(^{(a)}_{t}_{[1:T]^{(a)}_{}}RY^{(a)}_{}+_{ ^{(a)}_{}}R^{(a)}_{}}\)
6:\(^{(a)}_{t}_{[1:T]^{(a)}_{}}RY^{(a)}_{}+_{ ^{(a)}_{}}R^{(a)}_{}}\)
7:\(^{(a)}_{t}_{[1:T]^{(a)}_{}}RY^{(a)}_{}+_{ ^{(a)}_{}}R^{(a)}_{}}\)
8:\(^{(a)}_{t}_{[1:T]^{(a)}_{}}RY^{(a)}_{}+_{ ^{(a)}_{}}R^{(a)}_{} }\)
9:\(^{(a)}_{t}_{[1:T]^{(a)}_{}}RY^{(a)}_{}+_{ ^{(a)}_{}}R^{(a)}_{}}\)
10:\(^{(a)}_{t}_{[1:T]^{(a)}_{}}RY^{(a)}_{}+_{ ^{(a)}_{}}R^{(a)}_{}}\)
11:\(^{(a)}_{t}_{[1:T]^{(a)}_{}}RY^{(a)}_{}+_{ ^{(a)}_{}}RY^{(a)}_{}}\)
12:\(^{(a)}_{t}_{[1:T]^{(a)}_{}}RY^{(a)}_{could result in an action being optimal, it is essentially written off. Good performance of the algorithm relies on the model \(p_{}\) matching the data generating process closely.

### Interpreting our Decision-Making Algorithm as Thompson Sampling (TS)

We now formalize how the generated/imputed outcomes faithfully represent uncertainty and that PS-AR is akin to Thompson (posterior) sampling, which selects actions proportionally to the probability that actions are optimal. Lemma 1 shows that the imputed mean \(_{t}^{(a)}\) from PS-AR is a posterior sample of the mean reward \(^{(a)}\), and the action \(A_{t}\) selected by PS-AR is a posterior sample of the optimal action \(A^{*}:=_{a^{}}^{(a)}}\) where \(:=_{t=1}^{T}R(Y_{t}^{(a)})\). For simplicity, Lemma 1 is stated under the assumption that PS-AR uses the optimal sequence model \(p^{*}\) (see E.2 for result for _approximate_ models \(p_{}\)). Let \(_{t}:=(\{Z^{(a)}\}_{a^{}},\;A_{1},Y_{1}, ,A_{t},Y_{t})\) denote history up to time \(t\).

**Lemma 1**.: _Under Algorithm 2 applied with \(p_{}=p^{*}\), for all \(a^{}\), with probability \(1\),_

\[_{t}^{(a)}=_{t-1}= ^{(a)}=_{t-1}(A_{t}=a_{t-1})=_{p_{ }}(A^{*}=a_{t-1}).\]

Corollary 1 formalizes how expected loss of the learned sequence model \(p_{}\) controls the regret of PS-AR, reducing a sequential decision-making problem to loss minimization. Proofs in Appendix E.

**Corollary 1**.: _For PS-AR (Algorithm 2) applied with \(p_{}\), which we denote as \(_{}(p_{})\),_

\[_{}(p_{});\,p^{*}^{}|(|^{}|)}{2T}}}_ {}+^{ }|}{2}_{T}(p_{})-_{T}(p^{*})}}}_{ }.\]

**Advantages of the Autoregressive Approach.** Since our approach focuses on predicting missing outcomes, the learned model only needs to model _observable_ quantities, and can be learned via loss minimization (4). In contrast, a more standard perspective on TS requires specifying a model for latent variables and performing explicit Bayesian inference; for large scale problems this often involves simplifying modeling assumptions, expensive MCMC, or heuristic posterior approximations.

### News Recommendation Experiments

We build a news recommendation task using the MIcrosoft News Dataset (MIND)  where we demonstrate how PS-AR easily integrates with pretrained language models. Rewards are binary (click/no-click). We consider three types of sequence models. Flexible NN (Text) and Beta-Bernoulli NN (Text) are neural network models that incorporate article headlines using DistilBERT . Flexible NN (Category) uses only category information (e.g. "Sports"). See Appendix F.1 for synthetic experiments, and Appendix G for experiment details.