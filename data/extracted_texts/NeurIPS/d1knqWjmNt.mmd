# Optimality of Message-Passing Architectures for Sparse Graphs

Aseem Baranwal

David R. Cheriton School of Computer Science

University of Waterloo, Waterloo, Canada

aseem.baranwal@uwaterloo.ca

&Kimon Fountoulakis

David R. Cheriton School of Computer Science

University of Waterloo, Waterloo, Canada

kimon.fountoulakis@uwaterloo.ca

&Aukosh Jagannath

Department of Statistics and Actuarial Science,

Department of Applied Mathematics,

David R. Cheriton School of Computer Science

University of Waterloo, Waterloo, Canada

a.jagannath@uwaterloo.ca

###### Abstract

We study the node classification problem on feature-decorated graphs in the sparse setting, i.e., when the expected degree of a node is \(O(1)\) in the number of nodes, in the fixed-dimensional asymptotic regime, i.e., the dimension of the feature data is fixed while the number of nodes is large. Such graphs are typically known to be locally tree-like. We introduce a notion of Bayes optimality for node classification tasks, called asymptotic local Bayes optimality, and compute the optimal classifier according to this criterion for a fairly general statistical data model with arbitrary distributions of the node features and edge connectivity. The optimal classifier is implementable using a message-passing graph neural network architecture. We then compute the generalization error of this classifier and compare its performance against existing learning methods theoretically on a well-studied statistical model with naturally identifiable signal-to-noise ratios (SNRs) in the data. We find that the optimal message-passing architecture interpolates between a standard MLP in the regime of low graph signal and a typical convolution in the regime of high graph signal. Furthermore, we prove a corresponding non-asymptotic result.

## 1 Introduction

Graph Neural Networks (GNNs) have rapidly emerged as a powerful tool for learning on graph-structured data, where along with features of the entities, there also exists a relational structure among them. They have found numerous applications to a wide range of domains such as social networks (Backstrom and Leskovec, 2011), recommendation systems (Ying et al., 2018; Hao et al., 2020), chip design (Mirhoseini et al., 2021), bioinformatics (Scarselli et al., 2009; Zhang et al., 2021), computer vision (Monti et al., 2017), quantum chemistry (Gilmer et al., 2017), statistical physics (Battaglia et al., 2016; Bapst et al., 2020), and financial forensics (Zhang et al., 2017; Weber et al., 2019). Most of the success with these applications has been possible due to the advent of themessage-passing paradigm in GNNs, however, designing optimal GNN architectures for such a wide variety of applications still remains a challenging task.

In this work, we are interested in the node classification problem on very sparse feature-decorated graphs that are locally tree-like. We focus on the regime where the dimension of the node features is fixed and the number of nodes is large. Our motivation for considering this regime is that many major benchmark datasets for node classification appear to scale in this fashion. For example, in the popular Open Graph Benchmark collection (Hu et al., 2020), the medium and large-scale node-property prediction datasets have roughly \(10^{6}\) nodes (ogbn-products, ogbn-mag) to about \(10^{8}\) nodes (ogbn-papers100M), each with roughly \(10^{2}\) features. Graphs with such properties exist naturally in social, informational and biological networks; for motivational examples, see Stelzl et al. (2005), Adcock et al. (2013). We present a precise definition of optimality for node classification tasks on locally tree-like graphs in this scaling regime and compute the optimal classifier according to this definition, for a multi-class statistical data model where node features can have arbitrary continuous or discrete distributions. Subsequently, we show that a message-passing GNN architecture is able to realize the optimal classifier. Furthermore, we provide a theoretical analysis, comparing the generalization error of the optimal classifier with other architectures like GCN and simple MLPs. Our results support a recent work (Velickovic, 2022) in the context of classification on sparse graphs. In particular, we show that when node features are accompanied by sparse graphical side information, message-passing graph neural networks are able to realize the optimal classification scheme, and as such, there does not exist a better architecture beyond the message-passing paradigm.

Related Work.There has been a tremendous amount of work on GNN architecture design, where the most popular designs are based on a convolutional architecture, with each layer of the neural network performing a weighted convolution (averaging) operation with immediate neighbours, e.g., graph convolutional networks (GCN) (Kipf and Welling, 2017; Chen et al., 2020) or graph attention networks (GAT) (Velickovic et al., 2018). These architectures are known to have several limitations regarding their expressive power (see, for e.g., Li et al. (2018), Oono and Suzuki (2020), Balcilar et al. (2021), Xu et al. (2021), Keriven (2022)).

An interesting line of research consists of both theoretical and empirical works that attempt to address these limitations by developing an understanding of GNN architectures within the scope of message-passing (Rong et al., 2020; Liu et al., 2022; Maskey et al., 2022), as well as beyond it (Maron et al., 2019; Murphy et al., 2019; Chen et al., 2019). For example, Xu et al. (2018) propose an architecture with a technique called skip-connections, that flexibly leverages different ranges of neighbourhoods for each node to enable structure-awareness in node representations, Chen et al. (2020) propose a modification of the vanilla GCN with an initial residual that effectively relieves the problem of oversmoothing (Oono and Suzuki, 2020), and Keriven et al. (2021) study the universality of structural GNNs in the large random graph limit. However, this area of research still lacks a clear understanding of optimality in the context of graph learning problems, making it hard to design architectures for which a well-defined notion of optimality can be theoretically justified.

Several works have studied traditional message-passing GNN architectures like GCN and GAT using the binary contextual stochastic block model, see for example, Baranwal et al. (2021), Chien et al. (2022), Fountoulakis et al. (2022, 2022), Javaloy et al. (2022), Baranwal et al. (2023). These analyses rely heavily on two assumptions: first, the graph is not too sparse, i.e., for a graph with \(n\) nodes, the expected degree of a node is \(_{n}(^{2}n/n)\), and second, the node features are modelled as a Gaussian mixture. The work by Wei et al. (2022) is of particular interest to us, where the authors take a Bayesian inference perspective to investigate the functions of non-linearity in GNNs for binary node classification. They characterize the max-a-posterior estimation of a node label given the features of itself and its immediate neighbours. A similar perspective to that of Wei et al. (2022) is discussed in Gosch et al. (2023), where the latter authors derive insights into the robustness-accuracy trade-off in GNNs for node classification. In contrast to these inspiring works, we study the highly sparse regime where the expected degree of a node is \(O_{n}(1)\) and consider nodes beyond the immediate neighbours, at any fixed distance. (In fact, our non-asymptotic results allow distances of order \(c n\) for small enough \(c>0\), see Section3.5 below.) Furthermore, our main result holds for a general multi-class statistical model with arbitrary continuous or discrete feature distributions and arbitrary edge-connectivity probabilities between all pairs of classes.

Our Contributions.In this paper, we use a multi-class statistical model with arbitrary node features and edge-connectivity profiles among all pairs of classes to study the node classification problem in the regime where the graph component of the data is very sparse, i.e., the expected degree is \(O(1)\). The data model is described in Section3.2. We state the following main results and findings:

1. We introduce a family of graph neural network architectures that are asymptotically (in the number of nodes \(n\)) Bayes optimal in a local sense for a general multi-class data model with arbitrary feature distributions. The optimality is stated precisely in Theorem1.
2. We analyze the architecture in the simpler two-class setting with Gaussian features, explicitly characterizing the generalization error in terms of the natural signal-to-noise ratio (SNR) in the data, and perform a comparative study against other learning methods analyzed using the same statistical model (Theorems 2 and 3). We find two key insights: * When the graph SNR is very low, the architecture reduces to a simple MLP that does not consider the graph, while if it is very high, our architecture reduces to a typical convolutional network that averages information from all nodes in the local neighbourhood. In the regime between the low and high SNRs, the architecture interpolates and performs better than both a simple MLP and a typical GCN. * If the information in the graph is larger than a threshold, then a simple convolution is able to perform better than all methods that do not utilize the graph. Not surprisingly, this threshold aligns with the Kesten-Stigum weak-recovery threshold for community detection in sparse networks (Massoulie, 2014; Mossel et al., 2018).
3. In the non-asymptotic setting with a fixed number of nodes, we show that even for a logarithmic depth, the neighbourhoods of an overwhelming fraction of nodes are tree-like with high probability. Subsequently, we show that the optimal classifier in the non-asymptotic setting obtains an error that is close to that incurred by the optimal classifier in the asymptotic setting. This is formalized in Theorem4.

Let us end this section by reiterating that we work in the fixed-dimensional regime. It is natural to wonder as to the performance of this architecture as compared to optimal algorithms in the high-dimensional setting. We present a numerical comparison of our work to the AMP-BP algorithm from Deshpande et al. (2018) in low and high-dimensional settings in AppendixB.

## 2 Architecture

This section explains the design of our GNN architecture for node classification. We perform two modifications to existing message-passing architectures. First, we decouple the layers in the neural network from the neighbourhood radius in the message-passing framework. This style of decoupled architecture has previously been studied, see for example, Nikolentzos et al. (2020); Feng et al. (2022); Baranwal et al. (2023). Second, we introduce a learnable parameter that models edge connectivity between each pair of classes and helps construct the messages to propagate. In the following, for any matrix \(M\), we denote row \(i\) of \(M\) by \(M_{i,:}\) and column \(i\) of \(M\) by \(M_{:,i}\).

Before stating our architecture, we need the following additional notation and pre-processing. Let \( 0\) and \(L>0\) be fixed integers. Let \(C 2\) be the number of classes. For given data \((,)\) where \(^{n n}\) is the adjacency matrix of an unweighted undirected graph, and \(^{n d}\) is the node feature matrix, we perform a pre-computation on the graph to construct a tensor \(}\) as follows:

\[}^{(k)}=f(^{k})( f(_{m=0} ^{k-1}^{m}))k\{1,,\},\]

where \(f(M)\) for a matrix \(M\) returns the entry-wise flattened matrix with \(f(M)_{ij}=(M_{ij}>0)\), and \((,)\) denote the entry-wise bit-wise operators ('and', 'negation') respectively. Note here that \(}^{(k)}\) is an \(n n\) binary matrix with \(}^{(k)}_{uv}=1\) if and only if \(v\) is present in the distance \(k\) neighbourhood of \(u\) but not within the distance \((k-1)\) neighbourhood. The idea behind this pre-processing step is the following: for each node \(u\) and each \(k[]\), we want to divide the radius \(\) neighbourhood of \(u\) into \(\) groups of nodes, where each group \(k[]\) consists of nodes that are within discovered the neighbourhood at each distance from a given node. \(}^{(k)}_{u:}\) models a non-backtracking walk of length \(k\) that considers new nodes in the distance-\(k\) neighbourhood that were not discovered.

We can now define the graph neural network architecture as follows.

**Architecture 1**.: Given input data \((,)\) where \(\{0,1\}^{n n}\) is the adjacency matrix and \(^{n d}\) is the node feature matrix, define:

\[^{(0)}=, ^{(l)}=_{l}(^{(l-1)}^{(l)}+ _{n}^{(l)})l[L],\] \[=(), ^{(k)}_{u,i}=_{j[C]}\{^{(L)}_{u,j }+(^{k}_{i,j})\}k[],u[n],i[C].\]

Then the predicted label is given by \(}=\{_{u}\}_{u[n]}\), where

\[_{u}=*{argmax}_{i[C]}(^{(L)}_{u,c}+ _{k=1}^{}}^{(k)}_{u,:}^{(k)}_{:,i}).\]

Let us pause here to comment on the interpretation of the terms arising in this architecture. Here, \(^{(L)}\) is viewed as the output of a simple \(L\)-layer MLP with \(\{_{l}\}_{l[L]}\) being a set of non-linear functions. We have \((^{(l)},^{(l)})_{l[L]}\) as the learnable parameters of this MLP, with suitable dimensions so that \(^{(L)}^{n C}\). In addition, we introduce the learnable parameter \(^{C C}\) which is used to model edge connectivity among all pairs of classes. The quantity \(}^{(k)}_{u,:}^{(k)}_{:,i}=_{v[n]}}^{(k)}_{u,v}^{(k)}_{v,i}\) is viewed as the sum of messages \(^{(k)}_{v,i}\) passed by all distance \(k\) neighbours of node \(u\).

Although Architecture 1 follows the style of convolutional architectures like GCN and GAT for collecting messages within a local neighbourhood, the novelty lies in the construction of the messages \(\). Intuitively, \(=()\) learns the probabilities of edge connectivity between all pairs of classes so that \(^{k}\) models the probability of observing a distance \(k\) path between a pair of nodes in two classes. To predict the label of node \(u\), the messages from other nodes \(v\) are constructed based on their features \(_{v}\), their distance \(k\) from node \(u\), and the path probabilities \(^{k}\). We show in Theorem1 that this architecture is in a sense (made precise in Definition3.2) universally optimal among all node-classification schemes for sparse graphs. Our result thus aligns with the observations in Velickovic (2022), showing that optimal neural network architectures for node classification on sparse graphs are implementable using the message-passing paradigm.

## 3 Theoretical Analysis and Discussion

In this section, we present a theoretical analysis of the message-passing GNN given in Architecture 1. We begin by defining a natural notion of optimality in our setting and show that among local learning methods on graphs, Architecture 1 is optimal according to this definition on a very general statistical model. We then compute the generalization error and compare the architecture to other well-studied methods like a simple MLP and a GCN.

### Asymptotic Local Bayes Optimality

For classification tasks, it is natural to use a notion of generalization error in a "per sample" or online sense. Without graphical side information, the natural choice is the Bayes risk. With graphical information, however, there is an important obstruction: the number of samples is equal to the size of the corresponding graph. As such, a naive extension of the Bayes risk does not have this property.

A natural approach would be to consider the Bayes risk for estimators that take in the node, the data set, and the graph, i.e., \(_{v}=(v,(X,G))\). In this case, however, the risk necessarily implicitly depends on the sample size, \(n\), through \(G\). One might try to remove this dependence by taking the infinite sample size limit, but for a class of estimators this general, it is not clear that such a limit is well defined. To circumvent this issue, we restrict attention to node classifiers that are only allowed "local" information around the node. The large graph limit of the generalization error is then naturally interpreted via _local weak convergence_. (For the convenience of the reader, we briefly recall the notion of local weak convergence of sparse graphs in AppendixA.1. See also Ramanan (2021, Chapter 1) or Bordenave (2016, Section 3) for more detailed expositions.) In this limit, one can then interpret the generalization error as a per-sample error for the randomly rooted graph \((G,u)\) where \(u\) is a uniform random vertex in \(V(G)\). (Here and in the following a _rooted graph_ is a pair of a graph and a distinguished vertex, \(u\), called _the root._) With these observations, we are led to a natural notion of Bayes optimality, namely _asymptotic local Bayes optimality_ which we define presently.1

Before turning to this definition, we must first recall the notion of \(\)-_local classifiers_. For a node \(v\) in a graph \(G\), let \(_{k}(v)=\{u V(G):d(u,v) k\}\) denote the ball of radius \(k\) for the canonical graph distance metric.

**Definition 3.1** (\(\)-local classifier).: Let \(G=(,)\) be a feature-decorated graph of \(n\) vertices with \(d\)-dimensional features \(_{u}\) for each vertex \(u\). For a fixed radius \(>0\), an \(\)-local node-classifier is a function \(f\) that takes as input a root vertex \(u[n]\), the adjacency matrix \(\) and the features of all nodes within the \(\)-neighbourhood of \(u\), i.e., \(\{_{v}\}_{v_{}(u)}\)), and outputs a classification label for \(u\).

Suppose now that we have a sequence of (random) feature decorated graphs \((X_{n},G_{n})\) with \(|V(G)|=n\). Let \(u_{n}\) denote a uniform at random vertex in \(G_{n}\). Suppose finally that the rooted feature-decorated graphs \((X_{n},G_{n},u_{n})\) locally weakly converge to \((X,G,u)\). We can then define the notion of asymptotically \(\)-locally Bayes optimal classifiers for this sequence of problems.

**Definition 3.2**.: We say that a classifier \(h_{}^{*}_{}\) is asymptotically \(\)-locally Bayes optimal classifier of the root for the sequence \(\{(X_{n},G_{n},u_{n})\}\) if it minimizes the probability of misclassification of the root of the local weak limit, \((X,G,u)\), over the class \(_{}\), i.e.,

\[h_{}^{*}=*{argmin}_{h_{}}[ h(u,\{_{v}\}_{v_{}(u,G)}) y_{u}].\]

Before turning to our data model, we note here that the reader may ask whether or not the asymptotically \(\)-locally Bayes optimal classifier is in any sense the limit of optimal \(\)-local classifier of the random root, \(u_{n}\). We show this in an appropriate sense in Theorem 4.

### Data Model

Let us now turn to the data model that we use for our analysis. We work with the general multi-class contextual stochastic block model (CSBM) where each node belongs to one of \(C\) different classes labelled \(1,,C\), and the node features have arbitrary continuous or discrete distributions. This model with \(C=2\), along with a specialization to Gaussian features has been extensively studied in several works on (semi)-supervised node classification and unsupervised community detection, see, for example, Deshpande et al. (2018); Lu and Sen (2020); Baranwal et al. (2021); Wei et al. (2022); Fountoulakis et al. (2022); Baranwal et al. (2023). Informally, a CSBM consists of a coupling of a stochastic block model (SBM) Holland et al. (1983) with a mixture model where the components of the mixture have arbitrary distributions and are associated with the blocks of the SBM.

More formally, let \(n,d\) be positive integers such that \(n\) denotes the number of nodes and \(d\) denotes the dimension of the node features. Define \(y_{1},,y_{n}\{1,,C\}\) as the latent variables (class labels) to be inferred. We will assume that the latent variables have a uniform prior, i.e., \(y_{u}(\{[C]\})\) for all \(u\). For the relational part of the data, we have an undirected unweighted graph of \(n\) nodes, \(G=(V,E)\) with adjacency matrix \(=(a_{uv})_{u,v[n]}(n,)\), where \(=\{q_{ij}\}^{C C}\) is the edge-probability matrix, meaning that

\[(a_{uv}=1 y_{u}=i,y_{v}=j)=q_{ij}.\]

The node attributes, \(^{n d}\) are sampled from a mixture of \(C\) arbitrary continuous or discrete distributions, \(=\{_{i}\}_{i[C]}\), where corresponding to the \(y_{u}\), we have \(_{u}_{y_{u}}\) for all \(u[n]\).

We will view \(n\) as large and study the setting where \(d\) is fixed (does not grow with \(n\)). We note here that in previous related works Baranwal et al. (2021); Wei et al. (2022); Baranwal et al. (2023), crucial assumptions have been made about the distribution of the node features and the sparsity of the graph, i.e., \(q_{ij}=_{n}(^{2}n/n)\). In contrast, we work in the extremely sparse setting where \(q_{ij}=b_{ij}/n\) for constants \(b_{ij}>1\), so we write \(=/n\) where \(=\{b_{ij}\}_{i,j[C]}\). Furthermore, the only assumption we need about the distributions \(_{i}\) is that \(_{i}\) are absolutely continuous with respect to some base measure, in which case their densities exist, denoted by \(_{i}\). For ease of reading, we encourage the reader to consider the case where \(_{i}\) are continuous or discrete, therefore, the base measure is simply the Lebesgue measure on \(\) or the counting measure on \(\) respectively.

For a feature-decorated graph \(G=(,)=(\{a_{uv}\}_{u,v[n]},\{_{u}\}_{u n})\) sampled from the model described above, we say that \(G(n,d,,)\) or \(G(n,d,,/n)\).

### Optimal Classifier

We are now ready to state our first main result that characterizes the asymptotically \(\)-locally Bayes optimal classifier on the CSBM data described in Section 3.2.

**Theorem 1** (Bayes optimal message-passing).: _For any \( 1\), the asymptotically \(\)-locally Bayes optimal classifier of the root for the sequence \((G_{n},u_{n})(n,d,,)\) is_

\[h_{}^{*}(u,\{_{v}\}_{v_{}(u)})=*{argmax }_{i[C]}_{i}(_{u})+_{v_{}(u) \{u\}}_{i\,d(u,v)}(_{v})},\]

_where \(\{_{i}\}_{i[C]}\) are the densities associated with the distributions \(_{i}\), and_

\[_{ik}()=_{j[C]}_{j}()+ _{ij}^{k}}\,.\]

Let us briefly discuss the meaning of Theorem 1. It states that universally among all \(\)-local classifiers, \(h_{}^{*}\) is asymptotically Bayes optimal for the sparse CSBM data. We view \(_{ik}(_{v})\) as the message gathered from node \(v\) that is distance \(k\) away from node \(u\). In particular, \(_{ik}(_{v})\) naturally maximizes the likelihood of observing node \(v\) in class \(j\) at distance \(k\) from node \(u\) in class \(i\), over all \(j[C]\). Furthermore, this optimal classifier is realizable using Architecture 1 (see for example, Lu et al. (2017, Theorem 1), where it is shown that any Lebesgue measurable function can be approximated arbitrarily closely by standard neural networks). Consequently, this result shows that in the sparse setting, the message-passing paradigm can realize the optimal node classification scheme irrespective of the distributions of the node features or the inter-class edge probabilities.

For an intuitive understanding of Theorem 1, it helps to consider two extreme cases. First, if \(=p\) for some \(p\), then the classifier reduces to a simple convolution, \(h_{}^{*}(u)=*{argmax}_{i[C]}_{v_{k}(u) }_{i}(_{v})}\). Second, if \(=p^{}\), then \(q_{ij}=p\) for all \(i,j[C]\), meaning that the graph component of the data is Erdos-Renyi, and hence, completely uninformative for the purposes of node classification. In this case, the classifier reduces to \(h_{}^{*}(u)=*{argmax}_{i[C]}_{i}(_{u})}\), i.e., it is optimal to look at only the features of node \(u\) to predict its label since the neighbourhood does not provide any meaningful information. We formalize this intuition later for a simpler case (see Theorem 3).

### Comparative Study

In this section, we perform a theoretical analysis of the classifier in Theorem 1 using a well-studied specialization of the CSBM data model described in Section 3.2. For ease of discussion, let us restrict ourselves to the setting where there are two classes. Formally, we have \(C=2\), and without loss of generality, the class labels \(y_{u}\{ 1\}\) for all \(u[n]\). The distributions of the node features are given by \(_{u}_{y_{u}}\) with corresponding density \(_{y_{u}}\). Furthermore, \(=\{q_{ij}\}\) is a \(2 2\) matrix with \(q_{ii}=p=a/n\) and \(q_{ij}=q=b/n\) with constants \(a>1,b 0\) for classes \(i j\). For a data sample \(G=(,)\) from this model, we write \(G(n,d,\{_{}\},)\) or \(G(n,d,\{_{}\},,)\). We also recognize the quantity associated with the signal-to-noise ratio (SNR) in the graph structure for this case, which is given by

\[==.\] (1)

Note that the quantity \(\) has been recognized as the meaningful SNR in several related works where the underlying random graph model is the binary symmetric stochastic block model, for example, Baranwal et al. (2021); Fountoulakis et al. (2022); Wei et al. (2022); Baranwal et al. (2023).

Let us now state Theorem 1 in the case of two classes. For given input \(x\) and \(c>0\), let \((x,c)=((x,-c),c)\) denote the value of \(x\) clipped between the range \([-c,c]\).

**Corollary 1.1** (Optimal classifier for binary symmetric CSBM).: _For any \( 1\), the asymptotically \(\)-locally Bayes optimal classifier of the root for the sequence \((G_{n},u_{n})(n,d,,,)\) is_

\[h_{}^{*}(u,\{_{v}\}_{v_{l}(u)})=( (_{u})+_{v_{u}(u)\{u\}}_{ d(u,v)}(_{v})),\]

_where \(_{k}()=(a-b)((),c _{k})\) with \(c_{k}=(}{1-^{k}})\), and \(()=()}{_{-}()}\)._

In this simplified setting, we note that the messages propagated from nodes in the \(\)-local neighbourhood of node \(u\) are clipped proportional to a function of their distance \(k\) from node \(u\). In particular, the clip threshold \(c_{k}\) can be expressed in terms of the graph SNR \(\) from (1). It is interesting to observe in Corollary 1.1 that \(c_{k}\) decreases rapidly as \(k\) increases. Since \(<1\), this means that to predict the label of node \(u\), the value of the message propagated from node \(v\) at distance \(k\) from \(u\) decreases exponentially in \(k\).

The above simplification helps us interpret the classifier in terms of the graph SNR \(\). We will now impose an assumption on the distribution of node features. This will help us analyze the generalization error in terms of the SNR in both the features and the graph, and enable us to compare the performance with other learning methods that are well-studied in the same statistical settings. We will resort to the setting where the features of the CSBM follow a Gaussian mixture. Note that this specialized statistical model has been studied extensively in previous works for benchmarking existing GNN architectures, see for example, Baranwal et al. (2021); Fountoulakis et al. (2022); Wei et al. (2022); Baranwal et al. (2023).

In principle, one could compute the generalization error of \(h_{}^{*}\) for arbitrary distributions on the node features (see Appendix A.3.2), however, we report the error for Gaussian features for expository reasons. The generalization error is defined for a classifier \(h\) to be the probability of disagreement between the true label \(y_{u}\) and the output of the classifier \(h_{u}\) for node \(u\). We characterize the error for \(h_{}^{*}\) in the case where \(_{-},_{+}\) correspond to the Gaussian mixture with components \((-,^{2})\) and \((,^{2})\) for fixed \(^{d}\) and \(>0\)2.

In this case, a notion of the signal-to-noise ratio of the features naturally exists, i.e., \(=\|\|_{2}/\), a quantity proportional to the ratio of the distance between the means of the mixture and the standard deviation. The log-likelihood ratio in this setting is \(()=()}{_{-}()}= {2}{^{2}},\).

Consider a sequence \(\{(G_{n},u_{n})\}_{n 1}\) with \(G_{n}=(V(G_{n}),E(G_{n}))\) from this model where \(u_{n}(V(G_{n}))\). In this setting, in the absence of features, it is known that \((G_{n},u_{n})\) converges locally weakly to a Poisson Galton-Watson tree (see for example, Mossel et al. (2015, Section 4)). Here, for every node, we additionally have features that are independent of the graph, and hence, as a straightforward consequence of Mossel et al. (2015, Section 4), \((G_{n},u_{n})\) in our case converges to a feature-decorated Poisson Galton-Watson tree \((G,u)\).

For the root node \(u\), let \(_{k}\) and \(_{k}\) denote the number of children at generation \(k\) in class \(y_{u}\) and \(-y_{u}\) respectively, where \(y_{u}\) denotes the label of node \(u\). Then \(\{_{k}\}_{k 0}\) and \(\{_{k}\}_{k 0}\) are characterized by

\[_{0} =1,_{0}=0,\] \[_{k} (+b_{k-1}}{2}),_{k}(+b_{k-1}}{2}) k[].\] (2)

For a classifier \(h\) acting on \(G\), let \((h)\) denote the probability of misclassification of the root \(u\) in \(G\), i.e., \((h)=(h_{u}y_{u}<0)\). Correspondingly, in the case of finite \(n\), we denote by \(_{n}(h)\) the probability of misclassification of a uniform random node \(u_{n}\) in \(G_{n}\). We are now ready to state the generalization error of \(h_{}^{*}\).

**Theorem 2** (Generalization error).: _For any \( 1\), the generalization error of the asymptotically \(\)-locally Bayes optimal classifier of the root for the sequence \((G_{n},u_{n})(n,d,,)\) with Gaussian features is given by_

\[(h_{}^{*})=[g+_{k[ ]}(_{i[_{k}]}Z_{k,i}^{(a)}+_{i[_{k}]}Z_{k,i }^{(b)})>],\]_where \(_{k},_{k}\) are as in (2), \(Z_{k,i}^{(a)}=(-2^{2}+2 g_{k,i},c_{k})\), \(Z_{k,i}^{(b)}=(2^{2}+2 g_{k,i},c_{k})\), and \(g,\{g_{k,i}\}\) are mutually independent standard Gaussian random variables._

Let us now understand how the error described in Theorem2 behaves in terms of the two SNRs \(\) (for the features), and \(\) (for the graph). Note that \((h_{}^{*}) 0\) as \(\), and \((h_{}^{*})}{{2}}\) as \( 0\). This means that if the signal in the features is large, the number of mistakes made by the classifier vanishes, while if the signal is very small, then roughly half of the nodes are misclassified (equivalent to making a uniform random guess for each node).

To see how \(\) affects the error, we begin by looking at two extreme settings: first, where the graph is complete noise, i.e., \(=0\), and second, where the graph signal is very strong, i.e., \( 1\), followed by a discussion on how \(h_{}^{*}\) interpolates between these extremes. Let \(_{}\) denote the Gaussian density functions with means \(\) and variance \(^{2}_{d}\). Define the random variable

\[_{}=_{}(a,b)=^{}|_{k}-_{k}|}{ ^{}(_{k}+_{k})}},\] (3)

where \(_{k},_{k}\) follow (2). In the following, we denote the vanilla GCN classifier from Kipf and Welling (2017) by \(h_{}\). We then have the following result.

**Theorem 3** (Extreme graph signals).: _Let \(h_{}^{*}\) be the classifier from Corollary1.1, \(h_{0}^{*}(u)=(_{u},)\) be the Bayes optimal classifier given only the feature information of the root node \(u\), and \(h_{}\) be the one-layer vanilla GCN classifier. Then we have that for any fixed \(\):_

1. _If_ \(=0\) _then_ \((h_{}^{*})=(h_{0}^{*})=(-)\)_, where_ \(\) _is the standard Gaussian CDF._
2. _If_ \( 1\) _then_ \(_{} 1\) _a.s. and_ \((h_{}^{*})(g>_{})\)_, where_ \(g(0,1)\)_._
3. \((h_{})=(g>_{1})\)_._

Theorem3 shows that in the regime of extremely low graph SNR, the optimal classifier \(h_{}^{*}\) reduces to a linear classifier \(h_{0}^{*}(u)=(_{u},)\), which can be realized by a simple MLP that does not use the graph component of the data at all. On the other hand, in the regime of extremely strong graph SNR, \(h_{}^{*}\) reduces to a simple convolution over all nodes in the \(\)-neighbourhood and is comparable to a typical GCN. Furthermore, we note that in the strong graph SNR regime \( 1\), \((h_{}^{*})(g>_{})(-)\) since \(_{} 1\). The clip operation during the propagation of messages makes things interesting between these two extremes, where \(h_{}^{*}\) interpolates between a simple MLP and an \(\)-hop convolutional network. This interpolation is characterized by the graph signal \(\), since the messages are clipped in the range \([-c_{k},c_{k}]\), where \(c_{k}=(}{1-^{k}}}{-^{k}})\).

In addition, Theorem3 concludes that if \(_{1}(a,b)>1\), then a GCN can perform better than every classifier that does not see the graph. On the other hand, if \(_{1}(a,b)<1\), then a GCN incurs more errors on the data than the best methods that do not use the graph. Interestingly, but not surprisingly, this result aligns with the Kesten-Stigum weak recovery threshold for the community-detection problem on the sparse stochastic block model (Massoulie, 2014; Mossel et al., 2018), meaning that if weak recovery is possible on the graph component of the data, then a GCN is able to exploit it to perform better than methods that do not use the graph, e.g., a simple MLP.

We now demonstrate our results through experiments using pytorch and pytorch-geometric (Fey and Lenssen, 2019). The following simulations are for the setting \(n=10000\) and \(d=4\) for binary classification on the CSBM. We implement Architecture1 for the binary case, and perform full-batch training on a graph sampled from the CSBM with certain signals (mentioned in the figures), followed by an evaluation of the architecture on a new graph sampled from the same distribution.

In Fig.1, we show that the accuracy obtained by the optimal classifier is higher than both a simple MLP and a vanilla GCN (Kipf and Welling, 2017). We plot the test accuracy of Architecture1 against the SNR in the node features, \(=/\) in Fig.1a, and against the graph SNR \(=|a-b|/(a+b)\) in Fig.1b. We fix \(=0.42\) and \(=1\) for the two plots, respectively. We chose these specific values because they generate relatively clearer plots where the accuracy metrics for the three architectures are easily visible and distinguished from each other. The results are similar for other values for \(\) and \(\), i.e., the Bayes optimal architecture is superior to both MLP and GCN.

Furthermore, Fig. 2 shows that as claimed in Theorem 3, when the graph signal is at the extremes, i.e., \(=0\) and \(=1\), Architecture 1 behaves like a simple MLP and performs a typical convolution (averaging) over all nodes in the \(\)-neighbourhood, respectively. In the regime of poor graph SNR, i.e., \(=0\), a GCN is worse than a simple MLP, as inferred from part three of Theorem 3.

Finally, we observe that for the binary setting when the parameters of the architecture are initialized uniformly at random, gradient descent converges and the neural network learns the right parameters such that Architecture 1 realizes the optimal classifier in Corollary 1. This convergence, along with a comparison of our architecture to the Approximate Message-passing Belief Propagation (AMP-BP) algorithm from Deshpande et al. (2018) is presented in Appendix B.

### Non-asymptotic Setting

We now turn to the non-asymptotic regime and argue that for fixed \(n\), the classifier in Corollary 1.1 is still in a formal sense, Bayes optimal for an overwhelming fraction of nodes. We begin by exploiting the fact that for up to logarithmic depth neighbourhoods, a sparse CSBM graph is tree-like.

**Proposition 3.1** (Tree neighbourhoods).: _Let \(G=(V,E)(n,d,,,)\) for constants \(a,b>1\). Then for any \(=c n\) such that \(c((a+b)/2)<1/4\), with probability \(1-O(1/^{2}n)\), the number of nodes \(u V\) whose \(\)-neighbourhood is cycle-free is \(n(1-o(^{4}(n)/))\)._

In particular, Proposition 3.1 states that for \(=c n\) for a suitable constant \(c\), the \(\)-neighbourhood of an overwhelming fraction of nodes is a tree. This implies that the classifier \(h_{}^{*}\) is Bayes optimal for roughly all of the nodes. Moreover, since the diameter of a sparse graph (as in our setting) is \(O( n)\) almost surely (Chung and Lu, 2001, Theorem 6), any learning mechanism can only look as far as \(O( n)\)-hops away from a node to gather new information. This shows that for such graphs, GNNs that are not very deep and look at only up to logarithmic distance in the neighbourhood are sufficient.

Let us now turn to the misclassification error in the non-asymptotic setting. Recall that for a classifier \(h_{}\), we denote by \(_{n}(h)\) and \((h)\) the misclassification error of \(h\) on the data model with \(n\) nodes,

Figure 1: Comparison of Architecture 1 against an MLP and a vanilla GCN (Kipf and Welling, 2017).

Figure 2: Demonstration of Theorem 3 for extreme graph signals. In the case where \(=0\), the architecture reduces to an MLP (Fig. 1(a)), while if \(=1\), it behaves the same as a GCN (Fig. 1(b)).

and on the limiting data model with \(n\), respectively. Furthermore, recall from Corollary1.1 that \(_{h_{}}(h)=(h_{}^{*})\). Our next result shows that the optimal misclassification error in the non-asymptotic setting across all \(\)-local classifiers, i.e., \(_{h_{}}_{n}(h)\), is close to the misclassification error obtained in the non-asymptotic setting by \(h_{}^{*}\). Moreover, \(_{h_{}}_{n}(h)\) is also close to \((h_{}^{*})\) which is explicitly computed in Theorem2.

**Theorem 4** (Misclassification error for fixed \(n\)).: _For any \(1 c n\) such that the positive constant \(c\) satisfies \(c()<1/4\), we have that_

\[|_{h_{}}_{n}(h)-_{n}(h_{ }^{*})|=O(n}),|_{h_{}}_{n}(h)-(h_{}^{*})|=O(n}).\]

Recall that Corollary1.1 implies that \(h_{}^{*}\) performs optimally on the limiting data model (asymptotic setting) among the class of \(\)-local classifiers \(_{}\), but it may not be optimal for the non-asymptotic data model where we have a finite feature-decorated graph with \(n\) nodes. However, Theorem4 helps us conclude that even in the non-asymptotic setting, \(h_{}^{*}\) performs almost as well as the actual optimal classifier among \(_{}\) in this case, as long as we compare with classifiers that can only look at moderate logarithmic depths in the local neighbourhood, i.e., \( c n\) for a suitable \(c\).

## 4 Conclusion and Future Work

In this work, we present a comprehensive theoretical characterization of the Bayes optimal node classification architecture for sparse feature-decorated graphs and show that it can be realized using the message-passing framework. Utilizing a well-established and well-studied statistical model, we interpret its performance in terms of the SNR in the data and validate our findings through empirical analysis of synthetic data. Additionally, we identify the following limitations as prospects for future work: (1) We consider neighbourhoods up to distance \(=c n\) for a small enough \(c\). Extending \(\) to the graph's diameter (known to be \(O( n)\) with high probability) by removing the restriction on \(c\) poses challenges due to the presence of cycles. (2) More insights can be provided through experiments on real data to benchmark the architecture in cases where we have a significant gap between the theoretical assumptions (sparse and locally tree-like graph) and the real-world data.

A.J. acknowledges the support of the Natural Sciences and Engineering Research Council of Canada (NSERC) and the Canada Research Chairs programme. Cette recherche a ete enterprise grace, en partie, au soutien financier du Conseil de Recherches en Sciences Naturelles et en Genie du Canada (CRSNG), [RGPIN-2020-04597, DGECR-2020-00199], et du Programme des chaines de recherche du Canada.

K. Fountoulakis would like to acknowledge the support of the Natural Sciences and Engineering Research Council of Canada (NSERC). Cette recherche a ete financee par le Conseil de recherches en sciences naturelles et en genie du Canada (CRSNG), [RGPIN-2019-04067, DGECR-2019-00147].