# MultiOOD: Scaling Out-of-Distribution Detection for Multiple Modalities

Hao Dong\({}^{1}\) Yue Zhao\({}^{2}\) Eleni Chatzi\({}^{1}\) Olga Fink\({}^{3}\)

\({}^{1}\)ETH Zurich \({}^{2}\)University of Southern California \({}^{3}\)EPFL

{hao.dong, chatzi}@ibk.baug.ethz.ch, yzhao010@usc.edu, olga.fink@epfl.ch

###### Abstract

Detecting out-of-distribution (OOD) samples is important for deploying machine learning models in safety-critical applications such as autonomous driving and robot-assisted surgery. Existing research has mainly focused on unimodal scenarios on _image_ data. However, real-world applications are inherently multimodal, which makes it essential to leverage information from _multiple modalities_ to enhance the efficacy of OOD detection. To establish a foundation for more realistic **Multimodal OOD** Detection, we introduce the first-of-its-kind benchmark, MultiOOD, characterized by diverse dataset sizes and varying modality combinations. We first evaluate existing unimodal OOD detection algorithms on MultiOOD, observing that the mere inclusion of additional modalities yields substantial improvements. This underscores the importance of utilizing multiple modalities for OOD detection. Based on the observation of _Modality Prediction Discrepancy_ between in-distribution (ID) and OOD data, and its strong correlation with OOD performance, we propose the Agree-to-Disagree (_A2D_) algorithm to encourage such discrepancy during training. Moreover, we introduce a novel outlier synthesis method, _NP-Mix_, which explores broader feature spaces by leveraging the information from nearest neighbor classes and complements _A2D_ to strengthen OOD detection performance. Extensive experiments on MultiOOD demonstrate that training with _A2D_ and _NP-Mix_ improves existing OOD detection algorithms by a large margin. To support accessibility and reproducibility, our source code and MultiOOD benchmark are available at https://github.com/donghao51/MultiOOD.

## 1 Introduction

Most existing machine learning (ML) models are trained under the closed-world assumption, where the test data is assumed to be drawn _i.i.d._ from the same distribution as the training data, referred to as in-distribution (ID). However, in open-world scenarios, test samples can be out-of-distribution (OOD), thus impacting model robustness and safety . OOD detection aims to detect samples with semantic shifts that are undesirable for the model to generalize  and is critical for deploying ML models in safety-critical domains such as autonomous driving , robotics [18; 4], and diagnostics for critical assets . Numerous OOD detection algorithms have been developed, ranging from classification-based to distance-based methods . Classification-based methods typically derive confidence directly from the classifier, employing post-hoc processing techniques such as Maximum Softmax Probability (MSP)  and Energy  or training strategies such as logit normalization  and outlier synthesis . Distance-based methods typically measure distances in high-dimensional feature spaces to distinguish between ID and OOD [41; 59]. Additionally, other methodologies explore density estimation [1; 50] and reconstruction techniques  for OOD detection.

Current research in OOD detection has predominantly focused on _unimodal_ settings, often involving images as inputs . While several recent works [47; 63] have investigated vision-languagemodels  to enhance OOD performance, their evaluations are still limited to benchmarks containing _solely images_. Consequently, existing methods fall short in fully leveraging the complementary information from various modalities, such as LiDAR and camera in autonomous driving , as well as video, audio, and optical flow in action recognition . To underscore the importance of using multiple modalities in OOD detection, we evaluate representative OOD algorithms across various modalities on the HMDB51  dataset within our MultiOOD benchmark. This is an action recognition task and all models are trained solely using cross-entropy loss between a one-hot target vector and the softmax output. Results in Fig. 1 show that even a simple fusion of video and optical flow modalities can substantially enhance OOD detection performance.

To establish a foundation for more realistic **Multimodal OOD** Detection, we introduce a novel OOD benchmark named MultiOOD (Fig. 2), which is the first benchmark for Multimodal OOD Detection and covers diverse dataset sizes and modalities. MultiOOD comprises five video datasets with over \(85,000\) video clips in total. The datasets vary in the number of classes, ranging from \(7\) to \(229\), and in size, spanning from \(3k\) to \(57k\). _Video_, _optical flow_, and _audio_ are used as different types of modalities. While most existing unimodal OOD algorithms designed for images can be directly applied to Multimodal OOD Detection, such approaches may yield suboptimal results without accounting for the interaction and complementary nature of diverse modalities. As depicted in Fig. 1, the AUROC performance is very close for all unimodal baselines, underscoring the importance of developing OOD detection algorithms tailored to effectively exploit information from multiple modalities.

In this work, we first identify and illustrate the _Modality Prediction Discrepancy_ phenomenon on the MultiOOD benchmark, where the discrepancies of softmax predictions across different modalities are shown to be negligible for ID data while significant for OOD data (Fig. 3). We discover a strong correlation between such discrepancies and the OOD detection performance (Fig. 4). Motivated by these observations, we introduce the Agree-to-Disagree (_A2D_) algorithm, which aims to enhance such discrepancies during training. The algorithm is designed so that different modalities should _Agree_ on the prediction of the ground-truth class, and _Disagree_ on other classes by maximizing the distance between their predictions. Additionally, we propose a novel outlier synthesis algorithm named _NP-Mix_, designed to use the information from nearest neighbor classes to explore broader feature spaces and complement _A2D_ to strengthen the OOD detection performance.

Extensive experiments on the MultiOOD benchmark demonstrate the superiority of _A2D_ and _NP-Mix_. The integration of _A2D_ and _NP-Mix_ yields substantial performance enhancements over existing unimodal OOD detection algorithms. For instance, on the UCF101  dataset within MultiOOD, our approach reduces the FPR95 from \(32.14\%\) to \(10.68\%\) for ASH  method, representing a noteworthy absolute \(21.46\%\) improvement over the baseline. Our contributions include:

* We highlight the significance of integrating more modalities for OOD detection and introduce MultiOOD, the _first_ benchmark for Multimodal OOD Detection encompassing diverse dataset sizes and various combinations of modalities.
* We conduct comprehensive evaluations of existing unimodal OOD algorithms on MultiOOD, revealing their limitations in multimodal scenarios.
* We propose a novel _A2D_ training algorithm, inspired by the observation of the _Modality Prediction Discrepancy_ phenomenon, alongside a new outlier synthesis algorithm _NP-Mix_ that explores broader feature spaces and complements _A2D_ to strengthen the OOD detection performance.
* Extensive experiments conducted on MultiOOD underscore the effectiveness of _A2D_ and _NP-Mix_. Our source code and MultiOOD benchmark will be made publicly available, facilitating future research endeavors in Multimodal OOD Detection.

Figure 1: The FPR95 (lower is better) and AUROC (higher is better) on HMDB51 dataset across various modalities. Multimodal OOD substantially improves unimodal OOD w/o bells and whistles.

## 2 Preliminaries: Multimodal Out-of-distribution Detection

Multimodal OOD Detection aims to detect samples with semantic shifts using _multiple modalities_. We consider a training set \(_{in}=\{(_{i},y_{i})\}_{i=1}^{n}\) drawn _i.i.d._ from the joint data distribution \(P_{}\), where \(\) is the input space and \(=\{1,2,...,C\}\) is the discrete label space. Each training sample \(_{i}\) is comprised of \(M\) modalities, denoted as \(_{i}=\{x_{i}^{k} k=1,,M\}\). Let \(_{}\) denote the marginal distribution on \(\) and \(f:^{C}\) be a neural network trained on samples in \(P_{}\) that predicts the label of each input sample. The \(f\) in Multimodal OOD Detection comprises of \(M\) feature extractors \(g_{k}()\) and a classifier \(h()\). Each feature extractor \(g_{k}()\) extracts an embedding \(^{k}\) for its corresponding modality \(k\), and the classifier \(h()\) takes the combined embeddings from all modalities as input and outputs a prediction probability \(\):

\[=(f())=(h([g_{1}(x^{1}),...,g_{M}(x^{M})]))= (h([^{1},...,^{M}])),\] (1)

where \(()\) is the softmax function. We further include a separate classifier \(h_{k}()\) for each modality \(k\) to get predictions from each modality separately, with the prediction probability from the \(k\)-th modality as \(^{k}=(h_{k}(g_{k}(x^{k})))\).

When deploying \(f\) in the real world, it should not only accurately classify known samples as ID, but also identify any "unknown" sample as OOD. A separate score function \(S()\) is often used to decide whether a sample \(\) is from \(_{}\) (ID) or not (OOD):

\[G_{}(x)=&S()\\ &S()<,\]

where samples with higher scores \(S()\) are classified as ID and vice versa, \(\) is the threshold. Existing OOD detection studies predominantly focus on unimodal scenarios, with a detailed literature review offered in Appendix A. To establish a foundation for more realistic Multimodal OOD Detection, we introduce a novel MultiOOD benchmark (Sec. 3) and propose an effective multimodal training strategy (Sec. 4) that yields significant enhancements over existing unimodal approaches.

## 3 MultiOOD Benchmark

We create the MultiOOD benchmark to understand the existing gap in Multimodal OOD Detection research. OOD detection primarily focuses on detecting semantic shifts, with two main approaches used for constructing OOD benchmarks. A common method involves considering an entire dataset as in-distribution (ID) and further collects datasets, which comprise similar tasks but are disconnected from any ID categories, as OOD datasets. In this scenario, both semantic and domain shifts are present between the ID and OOD samples. We term this setup as _Far-OOD_ in our benchmark. Another approach is to partition the categories of existing datasets into two subsets, referred to as closed (ID) and open set (OOD). Here, both ID and OOD samples originate from the same distribution, with only semantic shifts existing between them. We denote this setup as _Near-OOD_ within our benchmark;

Figure 2: An overview of MultiOOD Benchmark.

a setup that poses greater challenges compared to _Far-OOD_. This setup is also referred to as open set recognition (OSR) in some studies [61; 37; 9]. Notably, OSR and OOD detection both share the same goal of identifying test samples with semantic shifts without compromising the accuracy of ID classification . In our benchmark, we treat OSR and OOD as synonymous concepts and adopt OOD as the general term. MultiOOD comprises five action recognition datasets (EPIC-Kitchens , HAC , HMDB51 , UCF101 , and Kinetics-600 ) with over \(85,000\) video clips in total. The datasets vary in the number of classes, ranging from \(7\) to \(229\), and in size, spanning from \(3k\) to \(57k\). _Video_, _optical flow_, and _audio_ are used as different types of modalities. An overview of the MultiOOD benchmark is provided in Fig. 2, with additional details available in Appendix B.

### Multimodal Near-OOD Benchmark

In the _Near-OOD_ setup, we include four datasets. **EPIC-Kitchens 4/4** is derived from the EPIC-Kitchens Domain Adaptation dataset , where the dataset is partitioned into four classes for training as ID and four classes for testing as OOD, with a total of \(4,871\) video clips. Similarly, **HMDB51 25/26** and **UCF101 50/51** are constructed based on HMDB51  and UCF101 , with a total of \(6,766\) and \(13,320\) video clips respectively. In the case of **Kinetics-600 129/100**, we select \(229\) action classes from the Kinetics-600 dataset , with each class comprising approximately \(250\) video clips and a total of \(57,205\) video clips. Within this setup, \(129\) classes are designated for training as ID, while the remaining \(100\) classes are allocated for testing as OOD.

### Multimodal Far-OOD Benchmark

In the _Far-OOD_ setup, we include HMDB51 and Kinetics-600 as ID datasets.

**HMDB51 dataset as ID.** For the OOD datasets, we utilize UCF101, EPIC-Kitchens, HAC, and Kinetics-600 datasets. All of these datasets are carefully curated to remove samples that belong to ID classes in HMDB51. Given the relatively small number of classes in the EPIC-Kitchens and HAC datasets, we remove \(8\) classes in the HMDB51 dataset that overlap with EPIC-Kitchens and HAC, with \(43\) classes left as ID classes. For UCF101, we remove \(31\) overlapping classes with HMDB51, resulting in \(70\) classes designated as OOD classes for evaluation. For other datasets, no class overlap exists and we utilize their original classes as OOD.

**Kinetics-600 dataset as ID.** Similarly, we adopt UCF101, EPIC-Kitchens, HAC, and HMDB51 datasets as OOD datasets, with careful selection undertaken to exclude samples belonging to ID classes in Kinetics-600. We carefully selected a subset of \(229\) action classes from Kinetics-600 in the _Near-OOD_ setup to mitigate the potential overlap with other datasets. For the UCF101 dataset, we remove \(11\) overlapping classes with Kinetics-600, leaving \(90\) classes as OOD classes for evaluation. For other datasets, there are no class overlap issues and we use their original classes as OOD.

## 4 Methodology

In this section, we first identify the _Modality Prediction Discrepancy_ phenomenon on the MultiOOD benchmark and demonstrate its substantial correlation with the OOD detection performance (Sec. 4.1). Subsequently, we introduce the Agree-to-Disagree (_A2D_) algorithm aimed at enhancing such discrep

Figure 3: An example of softmax outputs for ID and OOD data. The predictions from video and optical flow demonstrate uniformity across ID data and exhibit variability across OOD data.

ancies during training (Sec. 4.2). Finally, we propose the novel outlier synthesis algorithm named _NP-Mix_ that complements _A2D_ to further strengthen the OOD detection performance (Sec. 4.3).

### Modality Prediction Discrepancy

We first explore the predictive behaviors demonstrated by various modalities when using both ID and OOD data as input on MultiOOD. We compute the prediction probabilities employing classifiers for video and optical flow on the same ID and OOD samples and calculate their \(L_{1}\) distances. As depicted in Fig. 3, for ID data, the prediction probabilities of both video \(^{1}\) and optical flow \(^{2}\) are generally exhibited consistent with each other on the ground-truth label, consequently yielding a small \(L_{1}\) distance \(\|^{1}-^{2}\|_{1}\) between them. Conversely, for OOD data, each modality tends to express varying confidence preferences towards distinct classes, resulting in a notable increase in the \(L_{1}\) distance between their predictions. We refer to this phenomenon as _Modality Prediction Discrepancy_ between ID and OOD data. This discrepancy can be attributed to the unavailability of semantic information on OOD data during model training, stimulating each modality to generate conjectures based on its unique characteristics upon encountering OOD data during testing.

To verify the universality of this phenomenon, we calculate the average \(L_{1}\) distance between predictions of video \(^{1}\) and optical flow \(^{2}\) on both ID and OOD data within the HMDB51 dataset. The average prediction \(L_{1}\) distance is \(0.63\) for ID data (\(l_{ID}\)) and \(1.42\) for OOD data (\(l_{OOD}\)), revealing a substantial discrepancy. In addition, we evaluate other datasets in the _Near-OOD_ setup within the MultiOOD benchmark and observe similar discrepancies (\(l_{OOD}-l_{ID}\)). Such discrepancies have a positive correlation with the OOD detection performance, as illustrated in Fig. 4.

### Agree-to-Disagree Algorithm

Motivated by the _Modality Prediction Discrepancy_ and its strong correlation with Multimodal OOD Detection performance, we introduce the Agree-to-Disagree (_A2D_) algorithm to foster the amplification of such discrepancies during training. The underlying idea is that different modalities should _Agree_ on the prediction regarding the ground-truth class, while they should _Disagree_ on the remaining classes by maximizing their prediction distance. _A2D_ enables the model to diversify predictions across modalities, consequently yielding high prediction discrepancies for OOD data during testing.

Given a training sample \(\) with label \(c\), we obtain prediction probabilities \(\) from the combined embeddings of all modalities, and \(^{1}\), \(^{2}\) from individual modality, all of which are of shape \([1,C]\), where \(C\) represents the number of classes. By removing the \(c\)-th value from \(^{1}\) and \(^{2}\) (different modalities should _Agree_ on the prediction regarding the ground-truth class), we derive new prediction probabilities without ground-truth classes, denoted as \(^{1}\) and \(^{2}\) with shapes \([1,C-1]\). Subsequently, we aim to maximize the discrepancy between \(^{1}\) and \(^{2}\), which can be defined as:

\[_{Discr}=-Discr(^{1},^{2}),\] (2)

Figure 4: Average prediction \(L_{1}\) distances between ID and OOD data (\(l_{OOD}-l_{ID}\)) before and after _A2D_ training across various datasets within the MultiOOD, where Energy  is used as score function. The distances are highly correlated to the ultimate OOD performance. _A2D_ training amplifies such distances, consequently enhancing the efficacy of OOD detection.

where \(Discr()\) is a distance metric quantifying the similarity between two probability distributions. We utilize the Hellinger distance  and explore the efficacy of alternative distance metrics in our ablation study. The Hellinger distance between two probability distributions is defined as:

\[D(^{1},^{2})=_{i=1}^{C-1}(_ {i}^{1}}-_{i}^{2}})^{2}}.\] (3)

Furthermore, to ensure that the ground-truth classes possess the highest probabilities, we incorporate the cross-entropy loss \(CE()\) for each prediction, defined as:

\[_{cls}=(CE(,y)+CE(^{1},y)+CE(^{2},y )).\] (4)

The final loss is obtained as the weighted sum of the previously defined losses:

\[=_{cls}+_{Discr},\] (5)

where the hyperparameter \(\) controls the relative importance of the discrepancy term.

### Nearest Neighbor Prototype-based Mixup for Outlier Synthesis

Outlier synthesis [22; 60] has demonstrated its efficacy in OOD detection by imposing regularization on the model's decision boundary during training. Introducing the discrepancy loss in _A2D_ to the synthesized outlier data for model regularization has the potential to further enhance OOD detection performance. However, existing outlier synthesis methods [22; 60] typically generate outliers near the ID data (Fig. 7), neglecting to explore the broader embedding spaces, thereby potentially leading to suboptimal performance. Inspired by the recent approach introduced in , we introduce a novel algorithm termed Nearest Neighbor **P**rototype-based **M**ixup (_NP-Mix_), aimed at synthesizing outliers capable of spanning wider embedding spaces by leveraging the information from nearest neighbor classes, as shown in Fig. 5 and Fig. 7. To synthesize outliers, we concatenate the embeddings of all modalities (\(=[^{1},^{2}]\)) and treat them as a unified entity. Subsequently, we compute a prototype embedding \(_{c}}\) for each class \(c\) by calculating the mean of all embeddings within that class. For each prototype embedding \(_{c}}\), we identify its top \(N\) nearest neighbor prototypes and randomly select one prototype \(_{s}}\) from them for mixing. Two samples, \(}\) and \(}\), are randomly chosen from class \(c\) and class \(s\) respectively. The outlier \(}\) is generated by their convex combination:

\[}=}+(1-)},\] (6)

Figure 5: An overview of the proposed framework for Multimodal OOD Detection. We introduce _A2D_ algorithm to encourage enlarging the prediction discrepancy across modalities. Additionally, we propose a novel outlier synthesis algorithm, _NP-Mix_, designed to explore broader feature spaces, which complements _A2D_ to strengthen the OOD detection performance.

[MISSING_PAGE_FAIL:7]

area under the receiver operating characteristic curve (AUROC), and (3) ID classification accuracy (ID ACC). For each baseline algorithm, we report the results both with and without _A2D_ training and _NP-Mix_ outlier synthesis. Additional implementation details are provided in Appendix D.

### Multimodal Near-OOD Detection

We commence our evaluation by assessing the efficacy of _A2D_ training and _NP-Mix_ outlier synthesis within the Multimodal Near-OOD Detection setup. As presented in Tab. 1, the simple incorporation of _A2D_ yields substantial enhancements in OOD performance across nearly all scenarios. The average prediction \(L_{1}\) distances between ID and OOD data (\(l_{OOD}-l_{ID}\)) before and after _A2D_ training across various datasets are depicted in Fig. 4. Notably, across all datasets, _A2D_ training serves to further enlarge the Modality Prediction Discrepancy and consequently improve OOD detection performance, verifying the strong correlation between them. Specifically, _A2D_ training reduces the FPR95 by up to absolute \(19.62\%\) on UCF101 50/51 dataset for ASH and increases AUROC up to \(14.82\%\) for Mahalanobis on Kinetics-600 129/100. Combining _A2D_ training with _NP-Mix_ yields additional improvements in OOD detection performance, underscoring the efficacy of outlier synthesis in model regularization. Additionally, _A2D_ training and _NP-Mix_ outlier synthesis exhibit notable efficacy across all baseline OOD detection algorithms despite their different underlying principles, indicating the versatility of our proposed method.

### Multimodal Far-OOD Detection

Tab. 2 and Tab. 3 present the results of Multimodal Far-OOD Detection with HMDB51 and Kinetics-600 as ID datasets, respectively. Similar to the Near-OOD setup, training with _A2D_ and _NP-Mix_ yields considerable enhancements in OOD detection performance across most cases for all baseline algorithms. Specifically, training with both _A2D_ and _NP-Mix_ achieves reductions in FPR95 of up to absolute \(23.38\%\) on the HMDB51 dataset for ASH and up to \(14.43\%\) for ReAct on the Kinetics-600 dataset. Due to space limits, we provide comprehensive results in Appendix E (Tab. 7 and Tab. 8). Interestingly, we observe that the performance on the HMDB51 dataset generally surpasses that of the Kinetics-600 dataset. This finding aligns with observations from unimodal OOD detection benchmarks , where performance on datasets with fewer classes (e.g., CIFAR-10 ) tends to outperform those on datasets with a greater number of classes (e.g., ImageNet ).

    &  \\   &  &  &  &  &  & ID ACC \(\) \\   & FPR951 & AUROC\(\) & FPR951 & AUROC\(\) & FPR951 & AUROC\(\) & FPR951 & AUROC\(\) & FPR951 & AUROC\(\) & FPR951 & AUROC\(\) \\   \\ Energy & 32.95 & 92.48 & 449.3 & 87.95 & 8.10 & 97.70 & 32.95 & 92.28 & 29.73 & 92.60 & 87.23 \\ ASH & 51.20 & 87.81 & 53.93 & 84.22 & 19.95 & 95.32 & 42.99 & 90.23 & 42.02 & 89.55 & 86.20 \\ GEN & 41.51 & 90.43 & 46.18 & 87.91 & 8.21 & 59.86 & 38.31 & 91.28 & 33.55 & 91.95 & 87.23 \\ KNN & 22.69 & 95.01 & 93.94 & 98.29 & 9.92 & 97.92 & 20.75 & 96.02 & 23.18 & 94.56 & 87.23 \\ VIM & 13.68 & 97.01 & 33.87 & 91.45 & 5.93 & 98.15 & 13.45 & 97.12 & 16.73 & 95.93 & 87.23 \\   \\ Energy\(\) & 24.52\(\_{-3.81}\) & 93.96\(\_{-1.18}\) & 36.49\(\_{-3.41}\) & 86.97\(\_{-1.27}\) & 69.94\(\_{-1.17}\) & 97.53\(\_{-0.19}\) & 97.23\(\_{-0.19}\) & 94.41\(\_{-2.13}\) & 22.72\(\_{-0.19}\) & 93.80\(\_{-1.19}\) & 86.89 \\ ASH\(\) & 27.82\(\_{-2.138}\) & 93.17\(\_{-5.38}\) & 38.43\(\_{-1.58}\) & 93.92\(\_{-2.58}\) & 6.84\(\_{-1.11}\) & 92.83\(\_{-2.13}\) & 92.23\(\_{-0.19}\) & 94.56\(\_{-2.42}\) & 24.03\(\_{-1.79}\) & 93.84\(\_{-4.59}\) & 86.20 \\ GEN++ & 25.66\(\_{-1.155}\) & 93.05\(\_{-1.16}\) & 37.40\(\_{-3.75}\) & 91.93\(\_{-3.52}\) & 52.36 & 98.86\(\_{-1.23}\) & 26.63\(\_{-1.08}\) & 94.25\(\_{-1.26}\) & 23.24\(\_{-1.01}\) & 94.94\(\_{-9.54}\) & 86.99 \\ KNN & 13.50\(\_{-1.64}\) & 96.96\(\_{-1.16}\) & 35.06\(\_{-2.05}\) & 91.92\(\_{-2.41}\) & 44.71 & 93.91\(\_{-0.19}\) & 13.45\(\_{-3.50}\) & 97.92\(\_{-1.15}\) & 16.16\(\_{-0.26}\) & 92.86\(\_{-1.17}\) & 86.89 \\ VIM\(\) & 9.24\(\_{-4.44}\) & 98.04\(\_{-1.01}\) & 26.45\(\_{-7.41}\) & 92.34\(\_{-0.99}\) & 53.56\(\_{-0.37}\) & 98.09\(\_{-6.66}\) & 60.47\(\_{-4.1}\) & 98.56\(\_{-1.44}\) & 11.77\(\_{-1.86}\) & 96.76\(\_{-0.60}\) & 86.89 \\   

Table 2: **Multimodal Far-OOD Detection using video and optical flow, with HMDB51 as ID.**

    &  &  &  &  &  & ID ACC \(\) \\   & FPR951 & AUROC\(\) & FPR951 & AUROC\(\) & FPR951 & AUROC\(\) & FPR951 & AUROC\(\) & FPR951 & AUROC\(\) \\   \\ Energy & 72.64 & 71.75 & 70.12 & 71.49 & 43.66 & 82.05 & 61.50 & 74.99 & 61.98 & 75.07 & 73.14 \\ ASH & 71.62 & 76.65 & 69.36 & 72.38 & 34.38 & 88.05 & 47.83 & 83.49 & 55.80 & 80.15 & 72.20 \\ GEN & 68.47 & 78.43 & 64.30 & 73.97 & 36.81 & 85.11 & 49.53 & 83.67 & 54.90 & 80.30 & 73.14 \\ KNN & 17.08 & 78.54 & 68.62 & 74.33 & 41.83 & 82.32 & 57.00 & 82.

### Ablation Studies

**Multimodal Near-OOD Detection with other modalities.** We demonstrate the efficacy of _A2D_ training and _NP-Mix_ outlier synthesis across various combinations of modalities, not limited to video and optical flow, as detailed in Tab. 4 and Tab. 9 in Appendix E. Notably, the performance of most algorithms is consistently improved with _A2D_ and _NP-Mix_, regardless of which combinations of modalities are used.

**Effectiveness of other distance functions.** We substitute the distance metric for measuring probability distributions in the discrepancy loss with alternative distance functions, including \(L_{1}\), \(L_{2}\), and Wasserstein  distances. As depicted in Tab. 5, _A2D_ training exhibits robustness across various distance functions. Regardless of the specific distance metric employed, substantial improvements are consistently observed compared to the baseline approach without _A2D_ training.

**Compared with other outlier synthesis methods.** To evaluate the effectiveness of other outlier synthesis methods, we replace _NP-Mix_ with VOS , NPOS , and Mixup  and train with _A2D_. All baseline methods yield improvements in OOD performance, underscoring the significance of outlier synthesis for model regularization. Notably, _NP-Mix_ emerges as the most effective among the various baselines by synthesizing outliers that span broader embedding spaces.

## 6 Conclusion

In this paper, we introduce the Multimodal Out-of-distribution Detection problem and present a novel benchmark, MultiOOD, which includes diverse dataset sizes and various combinations of modalities. Motivated by the _Modality Prediction Discrepancy_ phenomenon observed within MultiOOD, we propose a novel _A2D_ training algorithm to encourage the enlargement of such discrepancy during model training, along with a new outlier synthesis algorithm, _NP-Mix_, that complements _A2D_. Extensive experiments on MultiOOD and under Near-OOD and Far-OOD setups verify the efficacy of the proposed approaches. We hope our work will inspire future research endeavors in Multimodal OOD Detection.

**Limitation and Future Work.** The performance on Near-OOD benchmarks and on datasets with a large number of classes still exhibits potential for enhancement. Moreover, the exploration of Outlier Exposure  deserves attention as a potential to better learn ID/OOD discrepancy.

    &  &  &  &  &  \\   & FPR95\({}_{}\) & AUROC\({}^{}\) & FPR95\({}_{}\) & AUROC\({}^{}\) & FPR95\({}_{}\) & AUROC\({}^{}\) & FPR95\({}_{}\) & AUROC\({}^{}\) & FPR95\({}_{}\) & AUROC\({}^{}\) \\  Energy & 43.36 & 87.46 & 37.69 & 87.94 & 43.14 & 87.59 & 42.27 & 87.77 & 36.38 & 88.91 \\ GEN & 43.79 & 87.49 & 37.69 & 89.51 & 44.44 & 88.11 & 42.70 & 88.28 & 39.55 & 89.78 \\ KNN & 42.92 & 88.46 & 38.34 & 88.55 & 40.52 & 88.81 & 36.17 & 89.61 & 33.77 & 90.05 \\ VIM & 36.82 & 88.06 & 35.51 & 88.93 & 37.69 & 88.17 & 37.04 & 88.44 & 34.64 & 88.80 \\   

Table 6: Ablation on **Outlier Synthesis Methods** for Near-OOD Detection on HMDB51 dataset.

    &  &  &  \\   & Video & Audio & Flow & FPR95\({}_{}\) & AUROC\({}^{}\) & ID ACC \({}^{}\) & FPR95\({}_{}\) & AUROC\({}^{}\) & ID ACC \({}^{}\) \\   \\  Energy & ✓ & ✓ & ✓ & 69.22 & 72.39 & 73.13 & 66.62 & 76.60 & 80.33 \\ ASH & ✓ & ✓ & ✓ & 70.52 & 69.70 & 68.10 & 63.48 & 78.11 & 79.54 \\ GEN & ✓ & ✓ & ✓ & 70.34 & 70.39 & 73.13 & 64.24 & 77.54 & 80.33 \\ KNN & ✓ & ✓ & ✓ & 80.22 & 60.56 & 73.13 & 75.03 & 65.97 & 80.33 \\ VIM & ✓ & ✓ & ✓ & 76.12 & 59.03 & 73.13 & 66.38 & 76.59 & 80.33 \\  \\  Energy++ & ✓ & ✓ & ✓ & \(62.69_{-6.53}\) & \(74.95_{-2.96}\) & 71.46 & \(63.81_{-2.61}\) & \(77.89_{-1.29}\) & 80.82 \\ ASIt++ & ✓ & ✓ & ✓ & \(69.78_{-7.04}\) & 69.09 & 61.72 & \(61.22_{-2.28}\) & \(18.57_{-0.48}\) & 80.05 \\ GEN++ & ✓ & ✓ & ✓ & \(63.62_{-6.72}\) & 73.94,29.17 & 71.46 & \(63.55_{-0.79}\) & \(77.92_{-0.38}\) & 80.82 \\ KNN++ & ✓ & ✓ & ✓ & \(68.47_{-11.78}\) & 67.87,73.71 & 71.46 & \(71.46_{-3.27}\) & \(68.87_{-2.30}\) & 80.82 \\ VIM++ & ✓ & ✓ & ✓ & \(73.51_{-2.51}\) & \(59.57_{-0.54}\) & 71.46 & \(63.42_{-2.96}\) & \(77.90_{-1.51}\) & 80.82 \\   

Table 4: **Multimodal Near-OOD Detection using video, audio, and optical flow.**

    &  & \)} & \)} &  &  \\   & TPR95\({}_{}\) & AUROC\({}^{}\) & FPR95\({}_{}\) & AUROC\({}^{}\) & FPR95\({}_{}\) & AUROC\({}^{}\) & FPR95\({}_{}\) & AUROC\({}^{}\) \\  Energy & 43.36 & 87.46 & 37.04 & 88.52 & 37.69 & 88.84 & 36.17 & 88.61 & 36.38 & 88.91 \\ GEN & 43.79 & 87.49 & 39.87 & 90.34 & 42.27 & 88.84 & 37.91 & 88.76 & 35.95 & 89.78 \\ KNN & 42.92 & 88.46 & 35.73 & 90.24 & 35.51 & 89.78 & 36.60 & 88.57 & 33.77 & 90.05 \\ VIM & 36.82 & 88.06 & 32.24 & 89.36 & 33.99 & 89.03 & 33.99 & 89.24 & 34.64 & 88.80 \\   

Table 5: Ablation on **D****T**eunctions for Near-OOD Detection on HMDB51 dataset.