# No "Zero-Shot" Without Exponential Data:

Pretraining Concept Frequency Determines

Multimodal Model Performance

Vishaal Udandarao\({}^{*}\)\({}^{1,2}\)  Ameya Prabhu\({}^{*}\)\({}^{1,3}\)  Adhiraj Ghosh\({}^{1}\)  Yash Sharma\({}^{1}\)

Philip H.S. Torr\({}^{3}\)  Adel Bibi\({}^{3}\)  Samuel Albanie\({}^{2}\)\({}^{}\)  Matthias Bethge\({}^{1}\)\({}^{}\)

\({}^{1}\)Tubingen AI Center, University of Tubingen \({}^{2}\)University of Cambridge \({}^{3}\)University of Oxford

###### Abstract

Web-crawled datasets underlie the impressive "zero-shot" performance of multimodal models, such as CLIP for classification and Stable-Diffusion for image generation. However, it is unclear how meaningful the notion of "zero-shot" _generalization_ is for such models because the extent to which their pretraining datasets encompass downstream concepts used in "zero-shot" evaluation is unknown. In this work, we ask: _How is the performance of multimodal models on downstream concepts influenced by the frequency of these concepts in their pretraining datasets?_ We comprehensively investigate this question across 34 models and 5 standard pretraining datasets, generating over 300GB of data artifacts. We consistently find that, far from exhibiting "zero-shot" generalization, multimodal models require exponentially more data to achieve linear improvements in downstream "zero-shot" performance, following a sample inefficient log-linear scaling trend. This trend persists even when controlling for sample-level similarity between pretraining and evaluation datasets , and testing on purely synthetic data distributions . Furthermore, upon benchmarking models on long-tailed data sampled based on our analysis, we demonstrate that multimodal models across the board perform poorly. We contribute this long-tail test dataset as the _Let it Wag!_ benchmark to further research in this direction. Taken together, our study reveals an exponential need for training data which implies that the key to "zero-shot" generalization capabilities under large-scale training data and compute paradigms remains to be found.

## 1 Introduction

Multimodal models like CLIP  and Stable Diffusion  have revolutionized performance on downstream tasks. CLIP is now the _de facto_ standard for "zero-shot" image recognition [143; 74; 136; 49; 142] and image-text retrieval [47; 64; 25; 127; 139], while Stable Diffusion is now the _de facto_ standard for "zero-shot" text-to-image (T2I) generation [100; 18; 104; 42]. In this work, we investigate this empirical success through the lens of zero-shot generalization , which refers to the ability of models to apply their learned knowledge to new unseen concepts (not seen during training). Accordingly, we ask: _Are current multimodal models truly capable of "zero-shot" generalization?_

To tackle this question, we conduct a comparative analysis involving two main factors: (1) the performance of models across various downstream tasks, and (2) the frequency of test concepts within their pretraining datasets. We compile a comprehensive list of \(4,029\) concepts2 from 27 downstreamtasks spanning classification, retrieval, and image generation, assessing model performance against these concepts. Our analysis spanned five large-scale image-text pretraining datasets with different scales, data curation methods and sources (CC-3M , CC-12M , YFCC-15M , LAION-Aesthetics , LAION-400M ), and evaluated the performance of 10 CLIP models and 24 T2I models, spanning different architectures and parameter scales. We consistently find across our experiments that, across concepts, the frequency of a concept in the pretraining dataset is _a strong predictor_ of the model's performance on test examples containing that concept (see Fig. 2). Notably, _model performance scales linearly as the concept frequency in pretraining data grows exponentially i.e._, we observe a consistent log-linear scaling trend. We find that this log-linear trend is robust to controlling for correlated factors (similar samples in pretraining and test data ) and testing across different concept distributions along with samples generated entirely synthetically .

Our findings indicate that the impressive empirical performance of multimodal models like CLIP and Stable Diffusion can be largely attributed to the presence of test concepts within their vast pretraining datasets, thus their reported empirical performance does not constitute "zero-shot" generalization. Quite the contrary, these models require exponentially more data on a concept to linearly improve their performance on tasks pertaining to that concept, suggesting significant sample inefficiency.

We additionally document the distribution of concepts encountered in pretraining data and find that:

* **Concept Distribution:** Across all pretraining datasets, the distribution of concepts is long-tailed (see Fig. 5), indicating that a large fraction of concepts are rare. Given the extreme sample inefficiency observed, these rare concepts are not properly learned during pretraining.
* **Concept Correlation across Pretraining Datasets:** The distributions of concepts across different pretraining datasets are strongly correlated (see Tab. 4), suggesting that web crawls yield surprisingly similar concept distributions across very diverse data curation strategies. This necessitates explicit concept rebalancing efforts explored in prior work [11; 135].
* **Image-Text Misalignment in Pretraining Data:** Concepts often appear in one modality but not the other, implying significant misalignment (see Tab. 3). Our released data artifacts can help image-text alignment efforts at scale by precisely indicating examples where modalities misalign. Note that the log-linear trend across both modalities is robust to this misalignment.

To provide a simple benchmark for multimodal generalization that controls for concept frequency in the pretraining set, we introduce a new long-tailed test set, "_Let It Wag:_". Current models trained on both openly available datasets (_e.g._, LAION-2B , DataComp-1B ) and closed-source datasets (_e.g._, OpenAI-WIT , WebLI ) have significant drops in performance (see Fig. 6), suggesting that our findings may also transfer to closed-source datasets. We publicly release all data artifacts, amortising the cost of analyzing image-text pretraining datasets for future efforts focused on a more data-centric understanding of the properties of multimodal models.

**Situating our Contributions in Broader Literature.** Our comprehensive analysis of several image-text datasets significantly adds to prior investigations on the role of pretraining data in affecting performance for both CLIP [92; 81; 43] and language models [62; 102; 82], by (1) showing that concept frequency determines zero-shot performance, and (2) pinpointing the exponential need for training data as a fundamental issue for current multimodal foundation models. We conclude that the key to "zero-shot" generalization under large-scale training paradigms remains to be found.

## 2 Concepts in Pretraining Data and Quantifying Frequency

In this section, we discuss how to estimate concept frequencies within pretraining datasets. We first define our concepts of interest, then describe algorithms for extracting their individual frequencies from images and text captions of pretraining datasets independently, and describe how we aggregate them to compute matched image-text concept frequencies. For a schematic overview, see Fig. 1.

**Defining Concepts.** We define "concepts" as the specific objects/relations we seek to analyze in pretraining datasets. Since our goal is to analyze downstream performance of models, we source concepts from 27 target evaluation datasets. For zero-shot classification datasets, extracted concepts are class names, such as the \(1,000\) object classes in ImageNet  (_e.g._, touch, goldfish). We also include relational verbs and verb-noun combinations since they are the classes of the UCF101 dataset  (_e.g._, diving, brushing teeth) as well as background nouns from the SUN397 dataset  (_e.g._, abbey, sky). For retrieval and image generation datasets, concepts are all nouns in test set captions or generation prompts. For example, from, "A man is wearing a hat", we extract "man" and "hat" as concepts. We filter out ambiguous or irrelevant concepts that are present in less than five downstream evaluation samples. In sum, we collate \(4,029\) concepts sourced from \(17\) classification, \(2\) image-text retrieval, and \(8\) text-to-image generation downstream datasets (see Tab. 1 and Sec. 3.1 for details).

**Concept Frequency from Captions.** For efficient concept searches, we pre-index all captions from the pretraining datasets, _i.e._, construct a mapping from concepts to captions. We first use part-of-speech tagging to isolate common and proper nouns, and lemmatize them with SpaCy  (lemmatization helps standardize verbs, enabling the estimation of their frequencies too ). These lemmatized terms are then cataloged in inverted unigram dictionaries, mapping each term to all sample indices in the pretraining dataset containing that term. To determine the frequency of a concept, we examine the concept's unigrams within these dictionaries. For multi-word concepts, we split them into their constituent unigrams, and then independently search for all unigrams before intersecting their hit lists to get a list of matched sample indices. The frequency of the concept in text captions is the count of these intersecting sample indices. This algorithm hence allows scalable \((1)\) search with respect to the number of captions for any concept in pretraining dataset captions.

**Concept Frequency from Images.** Unlike text captions, we do not have a finite vocabulary for pre-indexing pretraining images. Instead, we collect all the \(4,029\) downstream concepts and verify their presence in images using a pretrained image tagging model. We tested various open-vocabulary object detectors, multi-tagging models and image-text matching models, for this concept tagging task. We found that RAM++ --an open-set tagging model that tags images based on a predefined list of concept descriptions, in a multi-label manner--performs the best. We automatically consider the relationship between concepts (like synonyms) and concept hierarchies , since RAM++ uses descriptions generated by a language model (Appx. I.3) for each concept, to tag each image with certain concepts. This approach generates a list of pretraining images, each tagged with whether the downstream concepts are present or not, from which we can compute concept frequencies (Appx. I).

**Image-Text Matched Concept Frequencies.** Finally, we combine the frequencies obtained from both text and image searches to compute _matched image-text frequencies_. This involves identifying pretraining samples where both the image and its associated caption correspond to the concept. By intersecting the lists from our image and text searches, we determine the count of samples that align in both modalities, offering a comprehensive view of concept representation in the pretraining datasets. This step is necessary as we observed significant image-text misalignment between concepts in pretraining datasets (see Tab. 3), hence captions may not reflect what is present in the image and vice-versa. This behaviour has also been alluded to in prior work investigating data curation strategies . We provide a more detailed analysis of image-text misalignment in Sec. 5.

## 3 Comparing Pretraining Frequency & "Zero-Shot" Performance

Equipped with frequency estimates for downstream concepts, we now establish the relationship between image-text-matched pretraining concept frequencies and zero-shot performance across classification, retrieval, and generation tasks. We first detail our setup and then discuss key results.

Figure 1: **Concept Extraction and Frequency Estimation. (_left_) We compile \(4,029\) concepts from \(27\) evaluation datasets. (_right_) We construct efficient indices for text-search (unigram indexing (1)) and image-search (RAM++ (2)); intersecting hits from both gives (3) image-text matched frequencies.**

### Experimental Setup

We analyze two classes of multimodal models: Image-Text and Text-to-Image. For both, we detail the pretraining and testing datasets, along with their associated evaluation parameters.

#### 3.1.1 Image-Text (CLIP) Models

**Datasets.** We use 4 pretraining, 2 downstream retrieval, and 17 downstream classification datasets, covering a broad spectrum of objects, scenes, camera-types, and fine-grained distinctions (see Tab. 1).

**Note on Pretraining Dataset Diversity.** Each analyzed pretraining dataset significantly differs in data collection, filtering, and cleaning operations. CC-3M , originally intended to be used for training image captioning models, explicitly has no real-world entities or proper nouns present, and is cleaned only to have common nouns in its captions. CC-12M  and YFCC-15M , collected from Flickr, have user-provided metadata. Finally, LAION-400M  and LAION-Aesthetics  contain raw images downloaded from Common-Crawl  with alt-texts as the captions, which can be inherently very noisy as they are uploaded by non-expert humans as a placeholder for images.

**Models.** We test CLIP  models with both ResNet  and Vision Transformer  architecture, with ViT-B-16  and RN50  trained on CC-3M and CC-12M, ViT-B-16, RN50, and RN101  trained on YFCC-15M, and ViT-B-16, ViT-B-32, and ViT-L-14 trained on LAION400M . We follow open_clip, slip and cyclip for our implementation.

**Prompting.** For zero-shot classification, we experiment with three prompting strategies: {classname} only, "A photo of a {classname}" and prompt-ensembles , which averages over \(80\) different prompt variations of {classname}. For retrieval, we use the image or the caption as input corresponding to I2T (image-to-text) or T2I (text-to-image) retrieval respectively.

**Metrics.** We compute mean accuracy for classification tasks . For retrieval, we measure Recall@1, Recall@5, and Recall@10 for both text-to-image and image-to-text retrieval tasks .

#### 3.1.2 Text-to-Image Models

**Datasets.** Our pretraining dataset is LAION-Aesthetics , with downstream evaluations done on subsampled versions of eight datasets: CUB200 , Daily-DALLE , Detection , Parti-Prompts , DrawBench , COCO-Base , Relational Understanding  and Winoground . Please refer to HEIM  for more details on the evaluation datasets.

**Models.** We evaluate 24 T2I models, detailed in Tab. 2. Their sizes range from 0.4B parameters (DeepFloyd-IF-M  and DALL-E Mini ) to 4.3B parameters (DeepFloyd-IF-XL ). We include various Stable Diffusion models  as well as variants tuned for specific visual styles .

**Prompting.** Text prompts from the evaluation datasets are used directly to generate images, with 4 image samples generated for each prompt.

  
**Dataset Type** &  \\ 
**Pretraining** & CC-3M  & CC-12M  & YFCC-15M  & LAION-400M  \\   & ImageNet  & SUN397  & UCF101  & Catelton101  & EuroSAT  \\  & Caltech256  & Fluores102  & DDTD  & Birdsample  & Food101  & Stanford-Cars  \\  & FGVCAircraft  & Oxford-Pets  & Country211  & CIFAR-10  & CIFAR100  \\ 
**Retrieval-Eval** &  & COCO-5K  \\   

Table 1: Pretraining and downstream datasets used in Image-Text (CLIP) experiments.

  
**Category** &  \\   & M-Vader  & DeepFloyd-IF-M  & DeepFloyd-IF-L- & DeepFloyd-IF-XL  \\  & GigGAN  & DALL-E Mini  & DALL-E Mega  & Promptist+SD-v1.4  \\   & Dreamlike-Diffusion-v1.0  & Dreamlike Photorealv2.0  & OpenJourney-v1  & OpenJourney-v2  \\   & SD-Safe-Max  & SD-Safe-Medium  & SD-Safe-Strong  & SD-Safe-Weak  \\   & SD-v1.4  & SD-v1.5  & SD-v2-Base  & SD-v2-1-base  \\   & Vinetdois-Diffusion-v0.1  & minDALLE  & Lexica-SD-v1.5  & Redshift-Diffusion  \\   

Table 2: Models used in text-to-image (T2I) experiments.

**Metrics.** Evaluation consists of image-text alignment and aesthetic scores. For automated metrics , we use expected and max CLIP-score  to measure image-text alignment along with expected and max aesthetics-score  to measure aesthetics. To verify reliability of automated metrics, we compare them with human-rated scores (measured on a 5-point grading scale) for both image-text alignment and aesthetics . To supplement the human-rated scores provided by HEIM , we confirm our findings by performing our own small-scale human evaluation (Appx. C).

### Result: Pretraining Concept Frequency is Predictive of "Zero-Shot" Performance

We now probe the impact of pretraining concept frequency on "zero-shot" performance of models. Our main results, across various tasks and model types, are shown in Figs. 2 and 3.

**Understanding the Plots.** The plots in the main paper present text-image (CLIP) models' zero-shot classification results using accuracy and text-to-image retrieval performance using Recall@10. Similarly, we present T2I generative models' performance on image generation tasks using the expected aesthetics score. For the other aforementioned metrics for retrieval as well as other automated generation metrics along with human-rated scores, we find that they show similar trends,

Figure 3: **Log-linear relationships between concept frequency and T2I aesthetic scores. Across all tested T2I models pretrained on LAION-Aesthetics, we observe a consistent linear relationship between aesthetic score (averaged across 8 datasets) on a concept and the log-scaled concept frequency.**

Figure 2: **Log-linear relationships between concept frequency and CLIP zero-shot performance. Across all tested architectures (RN50, RN101, ViT-B-32, ViT-B-16, ViT-L-14) and pretraining datasets (CC-3M, CC-12M, YFCC-15M, LAION-400M), we observe a consistent linear relationship between CLIP’s zero-shot performance on a concept and the log-scaled pretraining concept frequency. This trend holds for both zero-shot classification (results averaged across 17 datasets) and image-text retrieval (results averaged across 2 datasets). ** indicates that the result is significant (\(p<0.05\) with a two-tailed t-test ), and thus we show Pearson correlation (\(\))  as well.**and we provide them for reference in Apps. B and C. For clarity, the data presentation is simplified from scatter plots to a cohesive line, similar to work from Kandpal et al.  and Razeghi et al. . The x-axis is log-scaled, and performance metrics are averaged within bins along this axis for ease-of-visualization of the log-linear correlation. We removed bins containing very few concepts per bin by standard IQR removal  following Kandpal et al. . We additionally compute Pearson correlation \(\) for each line and provide significance results based on a two-tailed t-test .

_Key Finding: Log-linear scaling between concept frequency and zero-shot performance._ Across all the 16 different plots, we observe a clear log-linear relationship between pretraining concept frequency and zero-shot performance. These plots vary in (i) model type (discriminative vs. generative), (ii) task (classification vs. retrieval), (iii) model architecture and parameter count, (iv) pretraining dataset (curation methods and scales), (v) evaluation metrics, (vi) prompting strategies, and (vii) concept frequencies isolated only from image or text caption (additional experiments for (v) are presented in Apps. B and C, for (vi) are presented in Appx. A, and for (vii) are presented in Appx. D). The observed log-linear scaling trend persists _across all seven presented dimensions_. In some plots, we notice a slight dip at the high-frequency concepts--we analyse this in greater detail in Appx. L. Thus, taken together, our results reveal data-hungry learning, _i.e_, a lack in current multimodal models' ability to learn concepts from pretraining datasets in a sample-efficient manner.

## 4 Stress-Testing Frequency-Performance Trends with Distributional Controls

In this section, we perform two control experiments to account for different confounding distributional factors of pretraining datasets, to ensure the robustness of our log-linear frequency-performance scaling trends: (1) we control for sample-level similarity in distribution between pretraining and evaluation datasets [137; 81], and (2) we investigate effects of pretraining data with radically different controlled concept distributions, with entirely synthetically-generated image-text pairs .

### Controlling for Similar Samples in Pretraining and Downstream Data

**Motivation.** Prior work has suggested that sample-level similarity between pretraining and downstream datasets impacts model performance [62; 81; 137; 102]. This leaves open the possibility that our frequency-performance results are simply an artifact of this factor, _i.e_., as concept frequency increases, it is likely that the pretraining dataset also contains more similar samples to the test sets. We hence investigate the frequency-performance trends when controlling for sample-level similarity.

**Setup.** We use the LAION-200M  dataset for this experiment. We first verify that a CLIP-ViT-B-32 model pretrained on the LAION-200M dataset (used to study sample similarity in prior work ) exhibits a similar log-linear trend between concept frequency and zero-shot performance. Then, we use the near_pruning method from Mayilvahanan et al.  to eliminate 50 million samples most similar to the test sets from the pretraining LAION-200M dataset. We provide details for this in Appx. G.1. This procedure removes the most similar samples between pretraining and test sets. We verify that this procedure influences the performance of the model drastically across our aggregate classification and retrieval tasks respectively, replicating the findings of Mayilvahanan et al. .

**Key Finding: _Concept Frequency is still Predictive of Performance._** We repeat our analysis on models trained with this controlled pretraining dataset with 150M samples, and report results on the

Figure 4: **Stress-testing the log-linear scaling trends. We provide further evidence for the log-linear relationship between performance and concept frequency, across different scenarios: (_left_) we control for “similarity” between downstream test sets and pretraining datasets, and (_right_) we conduct experiments on an entirely synthetic pretraining distribution with no real-world images or captions.**

same downstream classification and retrieval datasets, in Fig. 4 (left). Despite removing the most similar samples between pretraining and test sets, we still consistently observe a clear log-linear relationship between pretraining frequency of test set concepts and zero-shot model performance.

**Conclusion.** This analysis reaffirms that, despite removing pretraining samples closely related to the downstream evaluation datasets, the log-linear relationship between concept frequency and zero-shot performance persists. Note that this is despite substantial decreases in absolute performance, highlighting the robustness of concept frequency as a performance indicator for CLIP models.

### Testing Generalization to Purely Synthetic Concept and Data Distributions

**Motivation.** Sampling across real-world data might not result in significant differences in concept distribution, as we will later show in Sec. 5. Hence, we repeat our analysis on a synthetic dataset designed with an explicitly different concept distribution . This evaluation aims to understand if pretraining concept frequency remains a significant performance predictor within a synthetic concept distribution, generalizing even for models pretrained on entirely synthetic images and text captions.

**Setup.** The SynthCI-30M dataset  introduces a novel concept distribution, generating 30 million synthetic image-text pairs. Utilizing their publicly available data and models, we explore the relationship between concept frequency and model performance in this purely synthetic data regime.

**Key Finding:**_Concept Frequency is still Predictive of Performance._ We report results for models pretrained with the controlled SynthCI-30M dataset in Fig. 4 (right). We still consistently observe a clear log-linear relationship between concept frequency and zero-shot model performance.

**Conclusion.** This consistency highlights that concept frequency is a robust indicator of model performance, extending even to entirely synthetic datasets and pretraining concept distributions.

## 5 Additional Insights from Pretraining Concept Frequencies

**Finding 1:**_Pretraining Datasets Exhibit Long-tailed Concept Distributions._ Our analysis in Fig. 5 reveals an extremely long-tailed distribution of concept frequencies in pretraining datasets, with over two-thirds of concepts occurring at almost negligible frequencies relative to the size of the datasets (we highlight the "head" part of this distribution in boxes). Our observations extend the findings of past work that have noted the long-tailed distribution of large-scale language datasets [14; 26; 94; 146]. As we observed with the log-linear trend, this distribution directly reflects disparities in performance.

**Finding 2:**_Misalignment Between Concepts in Image-Text Pairs._ Our concept frequency estimation pipeline enables us to investigate the alignment of concepts within paired pretraining image-text data. Perfect image-text alignment is defined as every image-text pair containing the same concepts. Previous studies have qualitatively discussed the problem of misalignment in large image-text datasets [77; 134; 78]. Our analysis enables us to quantify this _misalignment degree_--for each image-text pair in the pretraining dataset, we find concepts that are matched to the image and the text caption independently. If there are no intersecting concepts from the independent image and text hits, we mark that pair as misaligned (detailed algorithm shown in Appx. K). Tab. 3 shows the high degree of misalignment in all image-text pairs (\(5-36\%\)). To the best of our knowledge, this is the first

Figure 5: **Concept distribution of pre-training datasets is highly long-tailed. We showcase the distribution of pretraining frequencies of all concepts aggregated across all 17 of our downstream classification datasets. Across all the pretraining datasets, we observe very heavy tails. We normalize the concept frequencies and remove concepts with 0 counts for improved readability of the plots.**

attempt to quantify the misalignment degree in pretraining image-text datasets explicitly. We release the precise misaligned image-text samples from pretraining datasets to enable better data curation.

**Finding 3: _Concept Frequencies Across Datasets are Correlated._** Despite vast differences in the size (3M-400M samples) and curation strategies of the pretraining datasets, we discovered a surprisingly high correlation in concept frequencies across them (see Tab. 4). This suggests that the internet, as the common source of these datasets, naturally exhibits a long-tailed distribution, influencing any dataset derived from it to display similar long-tailed behavior also. This result inspired "_Let It Wag!"_.

## 6 Testing the Tail: _Let It Wag!_

**Motivation.** In the previous sections, we identified a consistent long-tailed concept distribution across pretraining datasets, highlighting the scarcity of certain concepts on the web. This observation forms the basis of our hypothesis that models likely underperform when tested against data distributions that are heavily long-tailed. To test this, we carefully curate 290 concepts identified as the least frequent across all pretraining datasets. This includes concepts like eggnog, wormsnake, and tropical kingbird. We then use these concepts to create an evaluation dataset, "_Let It Wag!"_.

**Dataset Details.** The "_Let It Wag!"_ classification dataset comprises 130K test samples downloaded from the web using the method of Prabhu et al. . The test samples are evenly distributed across 290 categories of long-tailed concepts. From the list of curated concepts, we download test set images, deduplicate them, remove outliers, and finally manually clean and hand-verify the class labels.

**Analysis Details.** We run both classification and image generation experiments on "_Let It Wag!"_. For classification, we evaluate 40 text-image (CLIP) models on the _"Let It Wag!"_ classification dataset, using an ensemble of 80 prompts from Radford et al. . For the generation task, we utilize SD-XL , SD-v2 , and Dreamlike-Photoreal-v2.0 , to generate images for the long-tailed concepts. For each model, we run \(50\) diffusion steps, maintaining default settings for all other parameters.

**Text-Image Classification Results.** We showcase the results of our long-tailed classification task in Fig. 6--we plot results of all models on both "_Let It Wag!"_ (y-axis) and ImageNet (x-axis). We observe that all models underperform by large margins on the long-tailed "_Let It Wag!"_ dataset (upto 20% lower absolute accuracies compared to ImageNet). This performance drop-off generalises across all model scales and 10 different pretraining data distributions, reinforcing the notion that all web-sourced pretraining datasets are inherently constrained to be long-tailed. With that said, note that the higher capacity models (fitted line with slope=1.58 in Fig. 6) seem to be closing the gap to ImageNet performance, indicating improved performance on the long-tailed concepts.

  
**Correlations** & **CC-3M** & **CC-12M** & **YFCC-15M** & **LAION-400M** \\ 
**CC-3M** & 1.00 & 0.79 & 0.96 & 0.63 \\
**CC-12M** & – & 1.00 & 0.97 & 0.74 \\
**YFCC-15M** & – & – & 1.00 & 0.76 \\
**LAION-400M** & – & – & – & 1.00 \\   

Table 4: We compute correlation in concept frequency across pretraining datasets, observing strong correlations, despite major differences in scale and curation.

  
**Dataset/** & **Number of** & **Misalignment** \\
**Misalignment** & **Misaligned pairs** & **Degree (\%)** \\ 
**CC-3M** & 557,683 & 16.81\% \\
**CC-12M** & 2,143,784 & 17.25\% \\
**YFCC-15M** & 5,409,248 & 36.48\% \\
**LAION-A** & 23,104,076 & 14.34\% \\
**LAION-400M** & 21,996,097 & 5.31\% \\   

Table 3: For each pretraining dataset, we present the number of misaligned image-text pairs and the _misalignment degree_: fraction of misalignment pairs.

Figure 6: **Large-drops in accuracy on “_Let It Wag!"_. Across 40 tested CLIP models, we note large performance drops compared to ImageNet. Further, the performance gap seems to decrease for high-capacity models as demonstrated by larger positive slope (1.58) for those models.**

**T2I Generation Results.** We provide a qualitative analysis on image generation for assessing T2I models on the rare "_Let It Wag!_"concepts in Fig. 7. For enhancing image diversity, we generate prompts using Gemini  (top row of generated images) and GPT-4  (bottom row of generated images). Green borders represent correct generations, red borders represent incorrect generations and yellow borders represent ambiguous generations. While descriptive prompting generally aids in improving the quality of generated images , we still observe T2I models failing to comprehend and accurately represent many concepts in our "_Let It Wag!_" dataset. Some failure cases involve misrepresenting activities (such as Pizza Tossing or Cricket Bowling as shown in Fig. 29), generating the wrong concept (Chuck-will's-widow as shown in Fig. 7 (top)), as well as not comprehending the concept at all (Ocarina in Fig. 7 (bottom)). We hence show that Stable Diffusion models are prone to the long tail qualitatively--we also provide quantitative results in Appx. N.1.

**Conclusion.** Across both the classification and generation experiments, we have showcased that current multimodal models predictably underperform, regardless of their model scale or pretraining datasets. This suggests a need for better strategies for sample-efficient learning on the long-tail.

## 7 Related Work

We discuss the most relevant prior works to ours here, and defer an extended literature review to Appx. F. Past works [98; 47; 88; 43; 76; 39; 82; 81] have highlighted the importance of pretraining data for improved downstream model performance. Fang et al.  demonstrated that pretraining data diversity is key to CLIP's strong out-of-distribution generalisation. Nguyen et al.  extended this analysis to show that differences in data distributions can change model performance, enabling effective data mixing strategies for pretraining. Mayilvahanan et al.  complemented these works by showing that CLIP's performance is correlated with the similarity between pretraining and test datasets. Our findings further pinpoint that the frequency of concept occurrences is a key indicator of performance. This complements existing work in areas like question-answering  and numerical reasoning  in LLMs. Concurrent to our work, Parashar et al.  also explore the problem of long-tailed concepts in LAION-2B and how it affects CLIP performance, supporting our findings. In contrast to their work, our demonstration that the long tail yields a log-linear trend, explicitly indicates exponential sample inefficiency in pretrained multimodal models. Additionally, contrary to their work, we index both image and text modalities, as well as span across several scales of diverse pretraining datasets. Our frequency estimation procedure on both texts and images independently, enables us to provide a more finer-grained analysis of pretraining datasets than previously studied in the literature, like (1) quantifying the misalignment between images and text captions, (2) assessing

Figure 7: **Qualitative results on “_Let It Wag!_” concepts demonstrate failure cases of T2I models on the long-tail_. We created 4 prompts for each concept using Gemini  and GPT-4  which are fed to 3 Stable Diffusion  models. Generations with red border are incorrect, green border are correct and yellow border are ambiguous. Despite advances in high-fidelity image generation, there is large scope for improvement for such long-tail concepts (quantitative results in Appx. N.1).**the similarity of the different pretraining data concept distributions, and (3) doing a number of control experiments to thoroughly stress-test the robustness of our log-linear scaling results.

## 8 Conclusion

In this work, we studied 5 pretraining datasets of 34 multimodal models, analyzing the distribution and composition of concepts within them, generating over 300GB of data artifacts that we publicly release. Our findings reveal that across concepts, significant improvements in zero-shot performance require exponentially more data, following a sample-inefficient log-linear scaling trend. This pattern persists despite controlling for similarities between pretraining and downstream datasets or even when testing models on entirely synthetic data distributions. Further, all tested models consistently underperformed on the _"Let it Wag!"_ dataset, which we systematically constructed from our findings to test for long-tail concepts. This underlines a critical reassessment of what "zero-shot" generalization entails for multimodal models, highlighting the limitations in their current generalization capabilities.