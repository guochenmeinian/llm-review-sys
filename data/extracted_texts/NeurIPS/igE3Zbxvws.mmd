# Maximum Independent Set:

Self-Training through Dynamic Programming

 Lorenzo Brusca Lars C.P.M. Quaedvileg Stratis Skoulakis

**Grigorios G Chrysos Volkan Cevher**

Ecole Polytechnique Federale de Lausanne, Switzerland

{first name}.[last name]}@epfl.ch

These authors contributed equally to this work.Work done while at EPFL. Currently at University of Wisconsin-Madison; [sumame]@wisc.edu.

###### Abstract

This work presents a graph neural network (GNN) framework for solving the maximum independent set (MIS) problem, inspired by dynamic programming (DP). Specifically, given a graph, we propose a DP-like recursive algorithm based on GNNs that firstly constructs two smaller sub-graphs, predicts the one with the larger MIS, and then uses it in the next recursive call. To train our algorithm, we require annotated comparisons of different graphs concerning their MIS size. Annotating the comparisons with the output of our algorithm leads to a self-training process that results in more accurate self-annotation of the comparisons and vice versa. We provide numerical evidence showing the superiority of our method vs prior methods in multiple synthetic and real-world datasets.

## 1 Introduction

Deep neural networks (DNNs) have achieved unprecedented success in extracting intricate patterns directly from data without the need for handcrafted rules, while still generalizing well to new and previously unseen instances . Among other applications, this success has led to the development of frameworks that utilize DNNs to solve combinatorial optimization (CO) problems, such as the Traveling Salesman Problem , the Job-Shop Scheduling Problem , and the Quadratic Assignment Problem .

A core challenge for deep learning approaches on CO is the lack of training data. Annotating such data requires the solution of a huge number of instances of the CO, hence such supervised learning approaches are computationally infeasible for \(\)-hard problems . Circumventing this difficulty is key to unlocking the full potential of otherwise broadly applicable DNNs for CO.

Our work demonstrates how classical ideas in CO together with DNNs can lead to a scalable self-supervised learning approach, mitigating the lack of training data. Concretely, we focus on the Maximum Independent Set (MIS) problem: Given a graph \(G(V,E)\), MIS asks for a set of nodes of maximum cardinality such that no two nodes in the selected set are connected with an edge. MIS is an \(\)-hard problem with several hand-crafted heuristics (e.g., _greedy heuristic_, _local search_). More recently, several deep learning approaches have been proposed .

Our approach involves the following steps to determine an MIS in a graph. We use graph neural networks (GNNs)  to enable a model to generate approximate maximum independent sets after training on data that was annotated by the model itself. For this purpose, we draw inspirationfrom dynamic programming (DP) and employ a DP-like recursive algorithm. Initially, we are given a graph. At each recursive step, we select a random vertex from that graph and create two sub-graphs: one by removing the selected vertex and another by removing all its neighboring vertices. We then make a comparison between these sub-graphs to determine which sub-graph is likely to have a larger independent set, and we use the sub-graph with the highest estimated independent set for the next recursive call. We repeat this process until we reach a graph consisting only of isolated vertices, which signifies the discovery of an independent set for the original graph.

Dynamic programming guarantees that if our predictions are accurate (i.e., we select the sub-graph with the largest MIS value), our recursive algorithm will always result in a maximum independent set. To make accurate predictions, we introduce "graph comparing functions," which take two graphs as input and output a winner. We implement such graph-comparing functions with GNNs.

We adopt a self-training approach to train our graph-comparing function and optimize the parameters of the GNN. In each epoch, we update the graph-comparing function parameters to ensure it accurately fits the data it has seen so far. The data comprises pairs of graphs \((G,G^{})\) along with a label \((G,G^{})\{0,1\}\). For annotating the labels, we utilize the output of the recursive algorithm that leverages the graph-comparing function. Supported by theoretical and experimental evidence, we demonstrate how the self-annotation process improves parameter selection.

We conduct a thorough validation of our self-training approach in three real-world graph distribution datasets. Our algorithm surpasses the performance of previous deep learning methods (Karalias and Loukas, 2020; Toenshoff et al., 2019; Ahn et al., 2020) in the context of the MIS problem. To further validate the efficacy of our method, we explore its robustness on out-of-distribution data. Notably, our results demonstrate that the induced algorithm achieves competitive performance, showcasing the generalization capability of the learned comparator across different graph structures and distributions. In addition, we extend the evaluation of our DP-based self-training approach to tackle the Minimum Vertex Cover (MVC) problem in Appendix E. Encouragingly, similar to the MIS case, our induced GNN-based algorithms for MVC admit competitive performance with respect to other deep-learning approaches.

The code for the experiments and models discussed in this paper is available at: [https://github.com/LIONS-EPFL/dynamic-MIS](https://github.com/LIONS-EPFL/dynamic-MIS).

## 2 Related Work

Our work lies in the intersection of various domains, i.e., combinatorial optimization, Dynamic Programming, and (graph) neural networks. We review the most critical ideas in each domain here and defer a more detailed discussion in Appendix A.

**Graph Neural Networks (GNNs)** have gained widespread popularity due to their ability to learn representations of graph-structured data (Xiao et al., 2022; Zhang and Chen, 2018; Zhu et al., 2021; Errica et al., 2019) invariant to the size of the graph. More complex architectural blocks, such as the Graph Convolutional Network (GCN) (Kipf and Welling, 2017; Zhang et al., 2019), the Graph Attention Network (GAT) (Velickovic et al., 2017), and the Graph Isomorphism Network (Xu et al., 2018) have become influential instances of GNNs. In our work, we utilize a simple GNN architecture to showcase the effectiveness of our proposed framework. While our choice of architecture is intentionally simple, we emphasize its modular nature, which enables us to incorporate more complex GNNs with ease.

**Combinatorial Optimization**: Supervised learning approaches have been used for tackling CO tasks, such as the Traveling Salesman Problem (TSP) (Vinyals et al., 2015), the Vehicle Routing Problem (VRP) (Shalaby et al., 2021), and Graph Coloring (Lemos et al., 2019). Due to the graph structure of the problems, GNNs are often used for tackling those tasks (Prates et al., 2019; Nazari et al., 2018; Schuetz et al., 2022b). However, owing to the computational overhead of obtaining supervised labels, such supervised approaches often do not scale well. Instead, unsupervised approaches have been deployed recently (Wang and Li, 2023). A popular approach relies on a continuous relaxation of the loss function (Karalias and Loukas, 2020; Wang et al., 2022; Wang and Li, 2023). In contrast to the previous unsupervised works, we adopt Dynamic Programming techniques to diminish the overall time complexity of the algorithm. Another approach uses reinforcement learning (RL) methods to address CO tasks, such as in Covering Salesman Problem (Li et al., 2021), the TSP (Zhang et al.,

[MISSING_PAGE_FAIL:3]

whether \(|(G/\{v\})||(G/(v))|\) at each recursive call can be made according to the output of \((G/\{v\},G/(v))\).

The cornerstone idea of our approach is that _any graph-comparing function_\(\) induces such a recursive algorithm for a MIS. Recursively selecting \(G/\{v\}\) or \(G/(v)\) based on the output of a graph generating function \((G/\{v\},G/(v))\{0,1\}\) always guarantees to reach an independent set of the original graph. In case \((G,G^{})[|(G)|<|(G ^{})|]\), where \(\) is the indicator function, it is not guaranteed that the computed independent set is of the maximum size. However, there might exist reasonable graph comparing functions that \(i)\) are efficiently computable \(ii)\) lead to near-optimal solutions.

In Definition 2 and Algorithm 1 we formalize the idea above.

**Definition 2**.: _A comparator \(:\{0,1\}\) is a function taking as input two graphs \(G,G^{}\) and outputing a \(\{0,1\}\) value._

**Proposition 1**.: _Any comparator \(:\{0,1\}\) induces a randomized algorithm \(^{}\) (Algorithm 1)._

**Remark 1**.: _Given a graph-comparing function \(:\{0,1\}\), the induced algorithm is randomized, since at Step \(4\) of Algorithm 1, a vertex \(v\) is randomly selected. Notice that Algorithm 1 recursively proceeds until a subgraph with \(0\) edges is reached (see Step \(2\))._

**Remark 2**.: _Two different comparators \(\) and \(^{}\) induce two different algorithms \(^{}\) and \(^{^{}}\) for estimating the maximum independent set._

### Comparators through Graph Neural Networks

In this section, we discuss the architecture of a model \(M_{}:\), parameterized by \(\), that is used for the construction of a comparator function

\[_{}(G,G^{})=[M_{}(G)<M_{}(G ^{})]\.\]

In order to embed graph-level information, we introduce a new GNN module, which we refer to as the Graph Embedding Module (GEM). Unlike standard GNN modules, this module captures different semantic meanings of differing embeddings of a node, its neighbors, and anti-neighbors.

**Graph Embedding Module (GEM)**:

The GEM operates using the following recursive formula:

\[_{v}^{k+1}=((_{0}^{k} _{v}^{k}\!\!\!\|_{1}^{k}_{u(v)}_{ u}^{k}\|\!\!\|_{2}^{k}_{u(v)}_{u}^{k} \|)). \]

Initially, all nodes in this graph have zeros embeddings \(_{v}^{0}=^{3p}\). Here, \(_{v}^{0}\) denotes the initial embedding vector of node \(v\). In Eq. (1), for all iterations \(k[0,,K-1]\), the embeddings of a node denoted by \(_{v}^{k}^{3p}\), its neighbors, and its anti-neighbors \(v\) are put through their own linear layers, denoted by \(_{0}^{k},_{1}^{k},_{2}^{k}^{p 3p}\), which are the parameters of the module. The bias term is omitted in the equation for readability purposes. We incorporate anti-neighbors in the GEM to capture complementary relationships between nodes. By using separate linear layers for different features, we emphasize the contrasting semantic meaning between neighbors and anti-neighbors, representing negative and positive relationships in the graph. Then, the individual feature embeddings are concatenated, which is denoted by \([\|]\), followed by a \(\) activation function  and layer normalization . We note that the computational complexity of this module is \((|V|^{2})\).

The complete model architecture is depicted in Fig. 1. At a high level, the architecture uses a Graph Embedding Module to extract a global graph embedding from the input graph, which is then passed through a set of fully connected layers to output a logit for that graph. During the training process of the comparator function, we utilize \(_{}(G,G^{})=([M_{}(G)\|M_{ }(G^{})])\), which forms a differentiable loss function for classification.

### Inference Algorithm

In the previous section, we discussed how a parameterization \(\) defines the graph-comparing function \(_{}(G,G^{})=[M_{}(G)<M_{}( G^{})]\). As a result, the same parameterization \(\) defines an algorithm \(^{_{}}\), where at Step 6 of Algorithm 1, the comparing function \(_{}\) is used. Since the number of graph comparisons is upper-bounded by \(|V|\), the inference algorithm admits to a computational complexity of \((|V|^{3})\).

## 5 Self-Supervised Training through the Consistency Property

In this section, we present our methodology for selecting the parameters \(\) so that the resulting inference algorithm \(^{_{}}()\) computes independent sets with (close to) the maximum value.

The most straightforward approach is to select the parameters \(\) such that \(_{}(G,G^{})[|(G)|<| (G^{})|]\) using labeled data. The problem with this approach is that a huge amount of annotated data of the form \(\{((G,G^{}),[|(G)|<|(G^{})| |])\}\) are required. Since finding the MIS is an \(\)-\(\) problem, annotating such data comes with an insurmountable computational burden.

The **key idea** to overcome the latter limitation is to annotate the data of the form \(\{(G,G^{})\}\) by using the algorithm \(^{_{}}()\) that runs in polynomial time with respect to the size of the graph. Intuitively, our proposed framework entails the optimization of the parameterized comparator function \(_{}\) on data generated using algorithm \(^{_{}}\). A better comparator function leads to a better algorithm, which leads to better data, and vice versa. This mutually reinforcing relationship between the two components of our framework is theoretically indicated by Theorem 2 that we present in Section 5.1. The exact steps are detailed below.

Figure 1: Architecture of model \(M_{}(G)\). From left to right: initially, an input graph \(G\) is passed into the model with zeros as node embeddings, which are displayed as white in the figure. The striped green edges connect the anti-neighbors, which are also used in the \(\). After \(K\) iterations of the \(\) module, the final node embeddings are obtained. These are then averaged to obtain a graph embedding \(_{G}\). Finally, the graph embedding is put through multiple fully-connected layers to obtain a final logit value for the input graph.

### Consistent Graph Comparing Functions

In this section, we introduce the notion of a _consistent_ graph-comparing function (Definition 3) that plays a critical role in our self-supervised learning approach. Kindly take note that \(^{}\) utilizes the unparameterized variant of a comparator function, whereas \(^{_{}}\) utilizes its parameterized counterpart.

**Definition 3** (Consistency).: _A graph-comparing function \(:\{0,1\}\) is called consistent if and only if for any pair of graphs \(G,G^{}\),_

\[(G,G^{})=0[| ^{}(G)|][|^{}(G^{})|].\]

**Remark 3**.: _In Definition 3 we use \([|^{}(G)|], [|^{}(G^{})|]\) since, as we have already discussed, a comparator \(\) induces a randomized algorithm \(^{}\), where the expectation is over the nodes in the graphs._

In Theorem 2, we formally establish that any _consistent graph-comparing function_\(\) induces an optimal algorithm for the MIS.

**Theorem 2**.: _Consider a consistent comparator \(:\{0,1\}\). Then, the algorithm \(^{}()\) always computes a Maximum Independent Set, \([|^{}(G)|]=| (G)|\) for all \(G\)._

Theorem 2 guarantees that any consistent graph comparing function \(\) induces an optimal algorithm \(^{}\) for MIS. The proof for this theorem can be found in Appendix I. Hence, the selection of parameters \(^{}\) should be selected such that \(_{^{}}\) is _consistent_. More precisely:

**Goal of Training:** Find parameters \(^{}\) such that for all \(G,G^{}\):

\[_{^{}}(G,G^{})=0 [|^{_{^{}}}(G)|] [|^{_{^{}}}(G^{}) |]\.\]

### Training a Consistent Comparator

The cornerstone idea of our self-supervised learning approach is to make the comparator more and more consistent over time. Namely, the idea is to update the parameters as follows:

\[^{t+1}:=_{}_{G,G^{}}[ (_{}(G,G^{}),[[ |^{_{_{}}}(G)|]]< [|^{_{_{}}}(G^{}) |]])\, \]

where \((,)\) is a binary classification loss. In Eq. (2), \(_{t}\) are the fixed parameters of the previous epoch. Thus, in the next few paragraphs, we only use the notation \(^{_{_{t}}}\) to denote the fixed parameters. Gradient updates are only computed over \(\).

**Remark 4**.: _We remark that neither solving the non-convex minimization problem of Eq. (2) nor the existence of parameters \(^{}\) such that \(_{^{}}\) can be guaranteed. However, using a first-order method for Eq. (2) and a large enough parameterization can lead to an approximately consistent comparator with approximately optimal performance._

In Algorithm 2, we present the basic pipeline of the self-training approach that selects the parameters \(\) such that the inference algorithm \(^{_{_{t}}}\) admits a competitive performance given as input graphs \(G\) following a graph distribution \(\). We further improve this basic pipeline with several tweaks that we incorporate into our training process:

**Creating a graph buffer \(\):** We are given a shuffled dataset of graphs \(\), which represents the training data for the model. The core difference between the pipeline and the training process comes from the graph buffer \(\). In Algorithm 2, this buffer stores any graph \(G\) that is found during the recursive call of \(^{_{_{t}}}(G_{})\) on \(G_{} D\) (Step 6 of Algorithm 2). However, in the implementation of the graph buffer, it stores pairs of graphs \((G,G^{})\) that were generated by \(^{_{_{t}}}(G_{})\), alongside a binary label that indicates which of the two graphs has a larger estimated MIS size. How this estimate is generated, will be explained further down this section.

**The training process:** Prior to starting training, we first set two hyperparameters: one that specifies the number of graphs used to populate the buffer before training the model, and another that determines the number of graph pairs generated from \(^{_{_{t}}}(G)\) per graph \(G\). Then, a dataset is created by generating these pairs for the set number of graphs. The dataset is then added to the graph buffer,replacing steps 5 and 6 in Algorithm 2, which only does this with one graph per epoch. Next, training starts, and after completing a set number of epochs, a new dataset is created using the updated model, and the process is repeated iteratively.

```
1:Input: A distribution \(\) over graphs.
2: Initialize parameters \(_{0}\).
3: Initialize a graph-buffer \(\).
4:for each epoch \(t=0,,T-1\)do
5: Sample a graph \(G_{}\).
6: Run \(^{_{_{t}}}(G_{})\) and store in \(\) all graphs produced during each recursive call of Algorithm 1.
7: Update the parameters \(_{t+1}\) such that \[^{t+1}:=_{}_{(G,G^{}) }[(_{}(G,G^{}), [[|^{_{_{t}}}(G)| ]<[|^{_{_{t}}}(G^{ })|]])]\]
8:endfor
```

**Algorithm 2** Basic Pipeline of our Training Approach

**Estimating the MIS:** Finally, the loss function in Step 7 of Algorithm 2, also operates slightly differently. The main difference arises from \([[|^{_{_{t}}}(G )|]<[|^{_{_{t}}}( G^{})|]]\), since the estimates \(|(G)|[|^{ _{_{t}}}(G)|]\) and \(|(G^{})|[| ^{_{_{t}}}(G^{})|]\) are not directly utilized. Instead, we propose two other approaches, which are better approximations than the expectations used in Algorithm 2, since they use a maximizing operator.

The first approach involves performing so-called "roll-outs" on the graph pairs generated \(G\) and \(G^{}\) by \(^{_{_{t}}}\), in order to estimate their MIS sizes. To perform the roll-outs, we simply run \(^{_{_{t}}}\) on graphs \(G\) and \(G^{}\)\(m\) times and use the maximum size of the found independent sets as an estimate of their MIS. Formally, in a roll-out on a graph \(G\), we sample the independent sets \(_{1},_{2},,_{m}^{ _{_{t}}}(G)\). Then, the estimate of the MIS size of \(G\) is \((|_{1}|,|_{2}|,, |_{m}|)\).

An example of the entire process of generating the dataset using roll-outs can be found in Fig. 2. In practice, we observe an increasing consistency of the model during training, as can be seen in Appendix J.

Figure 2: An example of data generation for the training. (Left) At the beginning of each training epoch (Step 5), Algorithm 2 samples \(G_{}\) and computes an independent set by using the comparator \(_{_{t}}\) and by following the branches in the _recursion tree_ that are marked red, with the doubly circled one being the produced independent set. The generated graphs from this procedure are added to the buffer \(\). (Right) Then, a dataset is created by sampling graphs from the buffer and then computing an estimate of their MIS size (based on \(^{_{_{t}}}\)). Based on this estimate, a dataset is created with graph pairs \((G,G^{})\) and their corresponding binary labels denoting which MIS estimates are larger.

**Mixed roll-out variant**: We introduce a variant of the aforementioned method, which utilizes the deterministic greedy algorithm. This greedy algorithm iteratively creates an independent set by removing the node with the lowest degree and adding it to the independent set. This algorithm is often an efficient approximation to the optimum solution. Our variant is constructed as follows: we compute the maximum between the roll-outs of the model and the result of the greedy algorithm, which creates a dataset with more accurate self-supervised approximations of the MIS values. This, in turn, generates binary targets for the buffer that are more likely to be accurate. Thus, for this second variant, the estimate of the MIS size of a graph \(G\) would be \((G)|,|_{1}|,|_{2}|,,| _{m}|)}\).

## 6 Experiments

In this section, we conduct an evaluation of the proposed method for the MIS problem. Let us first describe the training setup, the baselines, and the datasets. Additional details and experiments on MIS are displayed in the Appendices C and D. Our method also generalizes well in MVC, as the results in Appendix E illustrate.

### Training Setup

**Our model**: We implement two comparator models: one using just roll-outs with the model, and another using the roll-outs together with greedy, called "mixed roll-out". We train each model using a graph embedding module with \(K=3\) iterations, which takes in \(32\)-dimensional initial node embeddings.

**Baselines**: We compare against the neural approaches _Erdos GNN_(Karalias and Loukas, 2020), _RUN-CSP_ from Toenshoff et al. (2019), and a method specifically for the MIS problem: LwDMIS (Ahn et al., 2020). Since we observe unexpected poor performances from RUN-CSP on the COLLAB and RB datasets, we have omitted those results from the table. We train every model for \(300\) epochs. Each experiment is performed on a single GPU with 6GB RAM.

Besides neural approaches, we use traditional baselines, such as the _Greedy MIS_(Wormald, 1995), _Simple Local Search_(Foe et al., 1994) and a _Random Comparator_ as a sanity check. Furthermore, we implement two mixed-integer linear programming solvers: _SCIP 8.0.3_ and the highly optimized commercial solver _Gurobi 10.0_.

**Datasets**: We evaluate our model on three standard datasets, following Karalias and Loukas (2020): COLLAB (Yanardag and Vishwanathan, 2015), TWITTER (Leskovec and Krevl, 2014) and RB (Xu et al., 2007; Toenshoff et al., 2019). In addition, we introduce the SPECIAL dataset that includes challenging graphs for handcrafted approaches as we detail in Appendix C.

### Results

Table 1 reports the average approximation ratios on the test instances of the various datasets. The approximation ratio is computed by dividing a solution's independent set size by the optimum solution, which is computed using the Gurobi solver with a time limit of \(1\) hour per graph.

The results indicate that the greedy algorithm performs strongly in three of the four datasets, which is consistent with the observation of Angelini and Ricci-Tersenghi (2022). However, notice that our proposed approach outperforms the greedy in both the Twitter and the SPECIAL datasets, which validates that the greedy heuristic is not optimal in every case and is prone to failing in few cases. Importantly, among the neural approaches that are the main compared methods, our proposed method performs favorably in all datasets. The performance of our method indicates that the proposed self-training scheme is able to learn from diverse data distributions and generalize reasonably well in the test sets of the respective dataset. In addition, the proposed method is faster than the rest neural approaches.

The mixed roll-out model in Table 1 outperforms the normal roll-out model in almost all datasets, indicating the effectiveness of the greedy heuristic in roll-outs. This is particularly evident in the RB dataset. However, for SPECIAL instances, the normal model performs marginally better, possibly due to the unsuitability of the greedy guiding heuristic as a baseline for this dataset.

Out of distribution:We examine the performance of the learned comparator through its generalization to new graph distributions. Concretely, we conduct an out-of-distribution analysis as follows:each model is trained in one graph distribution, indicated by the rows of Table 2. Then, the model is evaluated on different graph distributions, indicated by the columns of Table 2. The analysis is conducted on both our model and the approach of _Erdos GNN_Karalias and Loukas (2020).

Surprisingly, our model trained over COLLAB displays good generalization skills across different datasets, even outperforming the RB-trained model on the RB dataset. Conversely, _Erdos GNN_ trained over RB performs poorly over the COLLAB dataset. Both models trained over the RB dataset perform more poorly in general, likely due to the highly specific graph distribution of the RB dataset. Moreover, our model, on the whole, exhibits good generalization skills over different graph distributions.

## 7 Conclusion

Motivated by the principles of Dynamic Programming, we develop a self-training approach for important CO problems, such as the Maximum Independent Set and the Minimum Vertex Cover. Our approach embraces the power of self-training, offering the dual benefits of data self-annotation and data generation. These inherent attributes are instrumental in providing an unlimited source of data indicating that the performance of the induced algorithms can be significantly improved with sufficient scaling on the computational resources. We firmly believe that a thorough investigation into the interplay between Dynamic Programming and self-training techniques can pave the way for new deep-learning-oriented approaches for demanding CO problems.

  Method (\(\)) Dataset (\(\)) & RB & COLLAB & TWITTER & SPECIAL \\   & \(0.770 0.107\) & \(0.990 0.051\) & \(0.967 0.083\) & \(\) \\  & (0.43 s/g) & (0.17 s/g) & (0.35 s/g) & (0.04 s/g) \\  & \(\) & \(\) & \(\) & \(0.994 0.035\) \\  & (0.36 s/g) & (0.21 s/g) & (0.21 s/g) & (0.05 s/g) \\  & \(0.813 0.107\) & \(0.952 0.142\) & \(0.935 0.078\) & \(0.921 0.218\) \\  & (1.39 s/g) & (0.60 s/g) & (1.37 s/g) & (1.03 s/g) \\  & \(0.804 0.089\) & \(0.978 0.031\) & \(0.972 0.032\) & \(0.828 0.304\) \\  & (0.42 s/g) & (0.17 s/g) & (0.19 s/g) & (0.32 s/g) \\  & \(-\) & \(-\) & \(0.875 0.053\) & \(0.946 0.059\) \\  & \(-\) & \(-\) & (0.57 s/g) & (0.51 s/g) \\   & \(0.925 0.053\) & \(0.998 0.023\) & \(0.964 0.048\) & \(0.131 0.055\) \\  & (0.01 s/g) & (0.02 s/g) & (0.04 s/g) & (0.03 s/g) \\  & \(0.615 0.155\) & \(0.817 0.211\) & \(0.634 0.182\) & \(0.225 0.279\) \\  & (0.42 s/g) & (0.30 s/g) & (0.36 s/g) & (0.41 s/g) \\  Simple Local Search (10s) & \(0.565 0.237\) & \(0.860 0.213\) & \(0.644 0.218\) & \(0.188 0.340\) \\ SCIP 8.0.3 (1s) & \(0.741 0.351\) & \(0.999 0.016\) & \(0.959 0.024\) & \(1.000\) \\ SCIP 8.0.3 (5s) & \(0.937 0.118\) & \(1.000\) & \(0.999 0.024\) & \(1.000\) \\ Gurobi 10.0 (0.5s) & \(0.969 0.070\) & \(0.981 0.068\) & \(0.985 0.085\) & \(0.940 0.237\) \\ Gurobi 10.0 (1s) & \(0.983 0.051\) & \(1.000\) & \(1.000\) & \(1.000\) \\ Gurobi 10.0 (5s) & \(0.999 0.008\) & \(1.000\) & \(1.000\) & \(1.000\) \\  

Table 1: Test set approximation ratios (higher is better; best performance in bold) on four datasets for MIS. We report the average approximation ratios, along with std and duration in seconds per graph (s/g). Notice that the proposed method outperforms all the deep-learning-based approaches across datasets. Classical methods have a gray color, whilst neural approaches are in black.

  Model (\(\)) Dataset (\(\)) & RB & COLLAB & TWITTER \\  CMP RB & \(-\) & \(0.903 0.186\) & \(0.668 0.187\) \\ CMP COLLAB & \(0.856 0.080\) & \(-\) & \(0.906 0.094\) \\ CMP TWITTER & \(0.773 0.101\) & \(0.927 0.148\) & \(-\) \\  Erdos’ GNN RB & \(-\) & \(0.361 0.334\) & \(0.752 0.188\) \\ Erdos’ GNN COLLAB & \(0.680 0.071\) & \(-\) & \(0.592 0.186\) \\ Erdos’ GNN TWITTER & \(0.746 0.092\) & \(0.666 0.385\) & \(-\) \\  

Table 2: Out-of-distribution approximation ratios during inference (higher is better). Every row denotes a model trained on a specific dataset. Every column considers a different test dataset. The CMP is trained using mixed roll-outs. Notice that the proposed method generalizes well in out-of-distribution structures. This is indicative of the learned comparator extracting robust patterns.

**Limitations**: Our current empirical approach lacks theoretical guarantees on the convergence or the approximate optimality of the obtained algorithm. Additionally, the implemented GNN is using simple modules, while more complex modules could result in further empirical improvements, which can be the next step in this direction. Furthermore, randomly selecting a vertex could be sub-optimal. One interesting future direction would be to explore predicting the next vertex to select.