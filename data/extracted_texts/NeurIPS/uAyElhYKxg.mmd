# (S)GD over Diagonal Linear Networks:

Implicit Bias, Large Stepsizes and Edge of Stability

 Mathieu Even

Inria - ENS Paris

&Scott Pesme

EPFL

&Suriya Gunasekar

Microsoft Research

&Nicolas Flammarion

EPFL

Denotes equal contribution

###### Abstract

In this paper, we investigate the impact of stochasticity and large stepsizes on the implicit regularisation of gradient descent (GD) and stochastic gradient descent (SGD) over \(2\)-layer diagonal linear networks. We prove the convergence of GD and SGD with macroscopic stepsizes in an overparametrised regression setting and provide a characterisation of their solution through an implicit regularisation problem. Our characterisation provides insights on how the choice of minibatch sizes and stepsizes lead to qualitatively distinct behaviors in the solutions. Specifically, we show that for sparse regression learned with \(2\)-layer diagonal linear networks, large stepsizes consistently benefit SGD, whereas they can hinder the recovery of sparse solutions for GD. These effects are amplified for stepsizes in a tight window just below the divergence threshold, known as the "edge of stability" regime.

## 1 Introduction

The stochastic gradient descent algorithm (SGD)  is the foundational algorithm for almost all neural network training. Though a remarkably simple algorithm, it has led to many impressive empirical results and is a key driver of deep learning. However the performances of SGD are quite puzzling from a theoretical point of view as (1) its convergence is highly non-trivial and (2) there exist many global minimums for the training objective which generalise very poorly .

To explain this second point, the concept of implicit regularisation has emerged: if overfitting is harmless in many real-world prediction tasks, it must be because the optimisation process is _implicitly favoring_ solutions that have good generalisation properties for the task. The canonical example is overparametrised linear regression with more trainable parameters than number of samples: although there are infinitely many solutions that fit the samples, GD and SGD explore only a small subspace of all the possible parameters. As a result, it can be shown that they implicitly converge to the closest solution in terms of the \(_{2}\) distance, and this without explicit regularisation [66; 24].

Currently, most theoretical works on implicit regularisation have primarily focused on continuous time approximations of (S)GD where the impact of crucial hyperparameters such as the stepsize and the minibatch size are ignored. One such common simplification is to analyse gradient flow, which is a continuous time limit of GD and minibatch SGD with an infinitesimal stepsize. By definition, this analysis does not capture the effect of stepsize or stochasticity. Another approach is to approximate SGD by a stochastic gradient flow [60; 48], which tries to capture the noise and the stepsize using an appropriate stochastic differential equation. However, there are no theoretical guarantees that these results can be transferred to minibatch SGD as used in practice. This is a limitation in our understanding since the performances of most deep learning models are often sensitive to the choice of stepsize and minibatch size. The importance of stepsize and SGD minibatch size is common knowledge in practice and has also been systematically established in controlled experiments [36; 42; 20].

In this work, we aim to expand our understanding of the impact of stochasticity and stepsizes by analysing the (S)GD trajectory in \(2\)-layer diagonal networks (DLNs). In Fig. 1, we show that even in our simple network, there are significant differences between the nature of the solutions recovered by SGD and GD at macroscopic stepsizes. We discuss this behavior further in the later sections.

The \(2\)-layer diagonal linear network which we consider is a simplified neural network that has received significant attention lately [61; 57; 26; 50]. Despite its simplicity, it surprisingly reveals training characteristics which are observed in much more complex architectures, such as the role of the initialisation , the role of noise [48; 50], or the emergence of saddle-to-saddle dynamics [6; 49]. It therefore serves as an ideal proxy model for gaining a deeper understanding of complex phenomenons such as the roles of stepsizes and of stochasticity as highlighted in this paper. We also point out that implicit bias and convergence for more complex architectures such as 2-layer ReLU networks, matrix multiplication are not yet fully understood, even for the simple gradient flow. Therefore studying the subtler effects of large stepsizes and stochasticity in these settings is currently out of reach.

### Main results and paper organisation

The overparametrised regression setting and diagonal linear networks are introduced in Section 2. We formulate our theoretical results (Theorems 1 and 2) in Section 3: we prove that for **macroscopic stepsizes**, gradient descent and stochastic gradient descent over \(2\)-layer diagonal linear networks converge to a zero-training loss solution \(_{}^{*}\). We further provide a refined characterization of \(_{}^{*}\) through a trajectory-dependent implicit regularisation problem, that captures the effects of hyperparameters of the algorithm, such as stepsizes and batchsizes, in useful and analysable ways. In Section 4 we then leverage this crisp characterisation to explain the influence of crucial parameters such as the stepsize and batch-size on the recovered solution. Importantly **our analysis shows a stark difference between the generalisation performances of GD and SGD for large stepsizes**, hence explaining the numerical results seen in Fig. 1 for the sparse regression setting. Finally, in Section 5, we use our results to shed new light on the _Edge of Stability_ (_EoS_) phenomenon .

### Related works

**Implicit bias.** The concept of implicit bias from optimization algorithm in neural networks has been studied extensively in the past few years, starting with early works of Telgarsky , Neyshabur et al. , Keskar et al. , Soudry et al. . The theoretical results on implicit regularisation have been extended to multiplicative parametrisations [23; 25], linear networks , and homogeneous networks [40; 35; 13]. For regression loss on diagonal linear networks studied in this work, Woodworth et al.  demonstrate that the scale of the initialisation determines the type of solution obtained, with large initialisations yielding minimum \(_{2}\) norm solutions--the neural tangent kernel regime  and small initialisation resulting in minimum \(_{1}\) norm solutions--the _rich regime_. The analysis relies on the link between gradient descent and mirror descent established by Ghai et al.  and further explored by Vaskevicius et al. , Wu and Rebeschini . These works focus on full batch gradient, and often in the inifitesimal stepsize limit (gradient flow), leading to general insights and results that do not take into account the effects of stochasticity and large stepsizes.

**The effect of stochasticity in SGD on generalisation.** The relationship between stochasticity in SGD and generalisation has been studied in various works [41; 29; 11; 38; 64]. Empirically, models generated by SGD exhibit better generalisation performance than those generated by GD [37; 31; 27].

Figure 1: Noiseless sparse regression with a diagonal linear network using SGD and GD, with parameters initialized at the scale of \(=0.1\) (Section 2). The test losses at convergence for various stepsizes are plotted for GD and SGD. Small stepsizes correspond to gradient flow (GF) performance. We see that increasing the stepsize improves the generalisation properties of SGD, but deteriorates that of GD. The dashed vertical lines at stepsizes \(_{}^{}\) and \(_{}^{}\) denote the largest stepsizes for which SGD and GD, respectively, converge. See Section 2 for the precise experimental setting.

Explanations related to the flatness of the minima picked by SGD have been proposed . Label noise has been shown to influence the implicit bias of SGD [26; 8; 15; 50] by implicitly regularising the sharp minimisers. Recently, studying a _stochastic gradient flow_ that models the noise of SGD in continuous time with Brownian diffusion, Pesme et al.  characterised for diagonal linear networks the limit of their stochastic process as the solution of an implicit regularisation problem. However similar explicit characterisation of the implicit bias remains unclear for SGD with large stepsizes.

**The effect of stepsizes in GD and SGD.** Recent efforts to understand how the choice of stepsizes affects the learning process and the properties of the recovered solution suggest that larger stepsizes lead to the minimisation of some notion of flatness of the loss function [52; 37; 44; 33; 64; 43], backed by empirical evidences or stability analyses. Larger stepsizes have also been proven to be beneficial for specific architectures or problems: two-layer network , regression , kernel regression  or matrix factorisation . For large stepsizes, it has been observed that GD enters an _Edge of Stability (EoS)_ regime [32; 14], in which the iterates and the train loss oscillate before converging to a zero-training error solution; this phenomenon has then been studied on simple toy models [1; 67; 12; 16] for GD. Recently,  presented empirical evidence that large stepsizes can lead to loss stabilisation and towards simpler predictors.

## 2 Setup and preliminaries

**Overparametrised linear regression.** We consider a linear regression over inputs \(X=(x_{1},,x_{n})(^{d})^{n}\) and outputs \(y=(y_{1},,y_{n})^{n}\). We consider _overparametrised_ problems where input dimension \(d\) is (much) larger than the number of samples \(n\). In this case, there exists infinitely many linear predictors \(^{}^{d}\) which perfectly fit the training set, _i.e._, \(y_{i}=^{},x_{i}\) for all \(1 i n\). We call such vectors _interpolating predictors_ or _interpolators_ and we denote by \(\) the set of all interpolators \(=\{^{}^{d}\ { s.t.}\ ^{},x_{i} =y_{i}, i[n]\}\). Note that \(\) is an affine space of dimension greater than \(d-n\) and equal to \(^{}+{ span}(x_{1},,x_{n})^{}\) for any \(^{}\). We consider the following quadratic loss: \(()=_{i=1}^{n}(,x_{i}-y_{i} )^{2}\), for \(^{d}\).

**2-layer linear diagonal network.** We parametrise regression vectors \(\) as functions \(_{w}\) of trainable parameters \(w^{p}\). Although the final prediction function \(x_{w},x\) is linear in the input \(x\), the choice of the parametrisation drastically changes the solution recovered by the optimisation algorithm . In the case of the linear parametrisation \(_{w}=w\) many first-order methods (SGD, GD, with or without momentum) converge towards the same solution and the choice of stepsize does not impact the recovered solution beyond convergence. In an effort to better understand the effects of stochasticity and large stepsize, we consider the next simple parametrisation, that of a \(2\)-layer diagonal linear neural network given by:

\[_{w}=u vw=(u,v)^{2d}\,.\] (1)

This parametrisation can be viewed as a simple neural network \(x u,({ diag}(v)x)\) where the output weights are represented by \(u\), the inner weights is the diagonal matrix \({ diag}(v)\), and the activation \(\) is the identity function. In this spirit, we refer to the entries of \(w=(u,v)^{2d}\) as the _weights_ and to \( u v^{d}\) as the _prediction parameter_. Despite the simplicity of the parametrisation (1), the loss function \(F\) over parameters \(w=(u,v)^{2d}\) is **non-convex** (and thus the corresponding optimization problem is challenging to analyse), and is given by:

\[F(w)(u v)=_{i=1}^{n}(y_{i}- u  v,x_{i})^{2}\,.\] (2)

**Mini-batch SGD.** We minimise \(F\) using mini-batch SGD: let \(w_{0}=(u_{0},v_{0})\) and for \(k 0\),

\[w_{k+1}=w_{k}-_{k} F_{_{k}}(w_{k})\,,  F_{_{k}}(w)_{i_{k}}(y_ {i}- u v,x_{i})^{2}\,,\] (3)

where \(_{k}\) are stepsizes, \(_{k}[n]\) are mini-batches of \(b[n]\) distinct samples sampled uniformly and independently, and \( F_{_{k}}(w_{k})\) are minibatch gradients of partial loss over \(_{k}\), \(F_{_{k}}(w)_{_{k}}(u v)\) defined above. Classical SGD and full-batch GD are special cases with \(b=1\) and \(b=n\), respectively. For \(k 0\), we consider the successive prediction parameters \(_{k} u_{k} v_{k}\) built from the weights\(w_{k}=(u_{k},v_{k})\). We analyse SGD initialised at \(u_{0}=^{d}_{>0}\) and \(v_{0}=^{d}\), resulting in \(_{0}=^{d}\) independently of the chosen weight initialisation \(^{2}\).

**Experimental details.** We consider the noiseless sparse regression setting where \((x_{i})_{i[n]}(0,I_{d})\) and \(y_{i}=_{_{1}}^{},x_{i}\) for some \(s\)-sparse vector \(_{_{1}}^{}\). We perform (S)GD over the DLN with a uniform initialisation \(=^{d}\) where \(>0\). Fig. 1 and Fig. 2 (left) correspond to the setup \((n,d,s,)=(20,30,3,0.1)\), Fig. 2 (right) to \((n,d,s,)=(50,100,4,0.1)\) and Fig. 3 to \((n,d,s,)=(50,100,2,0.1)\).

**Notations.** Let \(H^{2}=_{i}x_{i}x_{i}^{}\) denote the Hessian of \(\), and for a batch \([n]\) let \(H_{}^{2}_{}=|}_{i}x_{i}x_{i}^{}\) denote the Hessian of the partial loss over the batch \(\). Let \(L\) denote the "smoothness" such that \(\), \(\|H_{}\|_{2} L\|\|_{2}\), \(\|H_{}\|_{} L\|\|_{}\) for all batches \([n]\) of size \(b\). A real function (e.g, \(,\)) applied to a vector must be understood as element-wise application, and for vectors \(u,v^{d}\), \(u^{2}=(u_{i}^{2})_{i[d]}\), \(u v=(u_{i}v_{i})_{i[d]}\) and \(u/v=(u_{i}/v_{i})_{i[d]}\). We write \(\), \(\) for the constant vectors with coordinates \(1\) and \(0\) respectively. The Bregman divergence  of a differentiable convex function \(h:^{d}\) is defined as \(D_{h}(_{1},_{2})=h(_{1})-(h(_{2})+ h(_{ 2}),_{1}-_{2})\).

## 3 Implicit bias of SGD and GD

We start by recalling some known results on the implicit bias of gradient flow on diagonal linear networks before presenting our main theorems on characterising the (stochastic) gradient descent solutions (Theorem 1) as well as proving the convergence of the iterates (Theorem 2).

### Warmup: gradient flow

We first review prior findings on gradient flow on diagonal linear neural networks. Woodworth et al.  show that the limit \(_{}^{*}\) of the _gradient flow_\(w_{t}=- F(w_{t})t\) initialised at \((u_{0},v_{0})=(,)\) is the solution of the minimal interpolation problem:

\[_{}^{*}=*{argmin}_{^{*}}\, _{}(^{*})\,,_{}( )=_{i=1}^{d}_{i}(}{_{i}^{2}})-^{2}+_{i}^{4}}+_{i}^{2} \,.\] (4)

The convex potential \(_{}\) is the **hyperbolic entropy function** (or **hyper entropy**) . Depending on the structure of the vector \(\), the generalisation properties of \(_{}^{}\) highly vary. We point out the two main characteristics of \(\) that affect the behaviour of \(_{}\) and therefore also the solution \(_{}^{}\).

**1.** The **Scale** of \(\). For an initialisation vector \(\) we call the \(_{1}\)-norm \(\|\|_{1}\) the **scale** of the initialisation. It is an important quantity affecting the properties of the recovered solution \(_{}^{}\). To see this let us consider a uniform initialisation of the form \(=\) for a scalar value \(>0\). In this case the potential \(_{}\) has the property of resembling the \(_{1}\)-norm as the scale \(\) vanishes: \(_{}(1/)\|_{1}\) as \( 0\). Hence, a small initialisation results in a low \(_{1}\)-norm solution which is known to induce sparse recovery guarantees . This setting is often referred to as the "rich" regime . In contrast, using a large initialisation scale leads to solutions with low \(_{2}\)-norm: \(_{}\|_{1}\|_{2}^{2}/(2^{2})\) as \(\), a setting known as the "kernel" or "lazy" regime. Overall, to retrieve the minimum \(_{1}\)-norm solution, one should use a uniform initialisation with small scale \(\), see Fig. 7 in Appendix D for an illustration and [61, Theorem 2] for a precise characterisation.

**2.** The **Shape** of \(\). In addition to the scale of the initialisation \(\), a lesser studied aspect is its "shape", which is a term we use to refer to the relative distribution of \(\{_{i}\}_{i}\) along the \(d\) coordinates . It is a crucial property because having \(\)**does not** necessarily lead to the potential \(_{}\) being close to the \(_{1}\)-norm. Indeed, we have that \(_{}()}}{{ }}_{i=1}^{d}(})|_{i}|\) (see Appendix D), therefore if the vector \((1/)\) has entries changing at different rates, then \(_{}()\) is a **weighted**\(_{1}\)-norm. In words, if the entries of \(\)_do not go to zero "uniformly"_, then the resulting implicit bias minimizes a weighed \(_{1}\)-norm. This phenomenon can lead to solutions with vastly different sparsity structure than the minimum \(_{1}\)-norm interpolator. See Fig. 7 and Example 1 in Appendix D.

### Implicit bias of (stochastic) gradient descent

In Theorem 1, we prove that for an initialisation \(^{d}\) and for **arbitrary** stepsize sequences \((_{k})_{k 0}\)**if the iterates converge to an interpolator**, then this interpolator is the solution of a constrained minimisation problem which involves the hyperbolic entropy \(_{_{}}\) defined in (4), where \(_{}^{d}\) is an effective initialisation which depends on the trajectory and on the stepsize sequence. Later, **we prove the convergence of iterates for macroscopic step sizes** in Theorem 2.

**Theorem 1** (Implicit bias of (S)GD).: _Let \((u_{k},v_{k})_{k 0}\) follow the mini-batch SGD recursion (3) initialised at \((u_{0},v_{0})=(,)\) and with stepsizes \((_{k})_{k 0}\). Let \((_{k})_{k 0}=(u_{k} v_{k})_{k 0}\) and assume that they converge to some interpolator \(_{}^{*}\). Then, \(_{}^{*}\) satisfies:_

\[_{}^{*}=*{argmin}_{^{*}}D_{_{ _{}}}(^{*},_{0})\,,\] (5)

_where \(D_{_{_{}}}\) is the Bregman divergence with hyperentropy potential \(_{_{}}\) of the **effective initialisation \(_{}\)**, and \(_{0}\) is a small **perturbation term**. The **effective initialisation \(_{}\)** is given by,_

\[_{}^{2}=^{2}(-_{k=0}^{}q _{k}_{_{k}}(_{k}))\,,\] (6)

_where \(q(x)=-((1-x^{2})^{2})\) satisfies \(q(x) 0\) for \(|x|\), with the convention \(q(1)=+\)._

_The **perturbation term**\(_{0}^{d}\) is explicitly given by \(_{0}=_{+}^{2}-_{-}^{2} \), where \(q_{}(x)= 2x-((1 x)^{2})\), and \(_{}^{2}=^{2}(-_{k=0}^{}q_{ }(_{k}_{_{k}}(_{k})))\)._

**Trajectory-dependent characterisation.** The characterisation of \(_{}^{*}\) in Theorem 1 holds for any stepsize schedule such that the iterates converge and goes beyond the continuous-time frameworks previously studied [61; 48]. The result even holds for adaptive stepsize schedules which keep the stepsize scalar such as AdaDelta . An important aspect of our result is that \(_{}\) and \(_{0}\) depend on the iterates' trajectory. Nevertheless, we argue that our formulation provides useful ingredients for understanding the implicit regularisation effects of (S)GD for this problem compared to trivial characterisations (such as _e.g._, \(_{}\|-_{}^{*}\|\)). Importantly, **the key parameters \(_{},_{0}\) depend on crucial parameters such as the stepsize and noise in a useful and analysable manner**: understanding how they affect \(_{}\) and \(_{0}\) coincides with understanding how they affect the recovered solution \(_{}^{*}\) and its generalisation properties. This is precisely the object of Sections 4 and 5 where we discuss the qualitative and quantitative insights from Theorem 1 in greater detail.

**The perturbation \(_{0}\) can be ignored.** We show in Proposition 16, under reasonable assumptions on the stepsizes, that \(|_{0}|^{2}\) and \(_{}\) (component-wise). The magnitude of \(_{0}\) is therefore negligible in front of the magnitudes of \(^{*} S\) and one can roughly ignore the term \(_{0}\). Hence, the implicit regularisation Eq. (5) can be thought of as \(_{}^{*}*{argmin}_{^{*} S}D_{_{ {}_{}}}(^{*},0)=_{_{}}(^{*})\), and thus _the solution \(_{}^{*}\) minimises the same potential function that the solution of gradient flow (see Eq. (4)), but with an effective initialisation \(_{}\)_. Also note that for \(_{k} 0\) we have \(_{}\) and \(_{0}\) (Proposition 19), recovering the previously known result for gradient flow (4).

**Deviation from gradient flow.** The difference with gradient flow is directly associated with the quantity \(_{k}q(_{k}_{_{k}}(_{k}))\). Also, as the (stochastic) gradients converge to 0 and \(q(x)x^{2}\), one should think of this sum as roughly being \(_{k}_{_{k}}(_{k})^{2}\): the larger this sum, the more the recovered solution differs from that of gradient flow. The full picture of how large stepsizes and stochasticity impact the generalisation properties of \(_{}^{*}\) and the recovery of minimum \(_{1}\)-norm solution is nuanced as clearly seen in Fig. 1.

### Convergence of the iterates

Theorem 1 provides the implicit minimisation problem but says nothing about the convergence of the iterates. Here we show under very reasonable assumptions on the stepsizes that the iterates indeed converge towards a global optimum. Note that since the loss \(F\) is non-convex, such a convergence result is non-trivial and requires an involved analysis.

**Theorem 2** (Convergence of the iterates).: _Let \((u_{k},v_{k})_{k 0}\) follow the mini-batch SGD recursion (3) initialised at \(u_{0}=_{>0}^{d}\) and \(v_{0}=\), and let \((_{k})_{k 0}=(u_{k} v_{k})_{k 0}\). Recall the "smoothness" parameter \(L\) on the minibatch loss defined in the notations. There exist \(B>0\) verifying \(B=}(_{^{*}}\|^{*}\|_ {})\) and a numerical constant \(c>0\) such that for stepsizes satisfying \(_{k}\), the iterates \((_{k})_{k 0}\) converge almost surely to the interpolator \(^{*}_{}\) solution of Eq. (5)._

In fact, we can be more precise by showing an exponential rate of convergence of the losses as well as characterise the rate of convergence of the iterates as follows.

**Proposition 1** (Quantitative convergence rates).: _For a uniform initialisation \(=\) and under the assumptions of Theorem 2, we have:_

\[[(_{k})](1- ^{2}_{b})^{k}(_{0}) [\|_{k}-^{*}_{_{k}}\|^{2} ] C(1-^{2}_{b})^{k}\,,\]

_where \(_{b}>0\) is the largest value such that \(_{b}H_{}[H_{}]\), \(C=2B(^{2}_{}^{+})^{-1}(1+(4B_{})(^{2} _{}^{+})^{-1})(_{0})\) and \(_{}^{+},_{}>0\) are respectively the smallest non-null and the largest eigenvalues of \(H\), and \(^{*}_{_{k}}\) is the interpolator that minimises the perturbed hypertnopy \(h_{k}\) of parameter \(_{k}\), as defined in Eq. (7) in the next subsection._

The convergence of the losses is proved directly using the time-varying mirror structure that we exhibit in the next subsection, the convergence of the iterates is proved by studying the curvature of the mirror maps on a small neighborhood around the affine interpolation space.

### Sketch of proof through a time varying mirror descent

As in the continuous-time framework, our results heavily rely on showing that the iterates \((_{k})_{k}\) follow a mirror descent recursion with time-varying potentials on the convex loss \(()\). To show this, we first define the following quantities:

\[_{k}^{2}_{+,k} _{-,k}_{k} (_{+,k}^{2}-_{-,k}^{2}}{2_{k}^{2}})^{d}\,,\]

where \(_{,k}(-_{i=0}^{k-1}q_{}_{}_{_{ }}(_{}))^{d}\). Finally for \(k 0\), we define the potentials \((h_{k}:^{d})_{k 0}\) as:

\[h_{k}()=_{_{k}}()-_{k},.\] (7)

Where \(_{_{k}}\) is the hyperbolic entropy function defined Eq. (4). Now that all the relevant quantities are defined, we can state the following proposition which explicits the time-varying stochastic mirror descent.

**Proposition 2**.: _The iterates \((_{k}=u_{k} v_{k})_{k 0}\) from Eq. (3) satisfy the Stochastic Mirror Descent recursion with varying potentials \((h_{k})_{k}\):_

\[ h_{k+1}(_{k+1})= h_{k}(_{k})-_{k} _{_{k}}(_{k})\,,\]

_where \(h_{k}:^{d}\) for \(k 0\) are defined Eq. (7). Since \( h_{0}(_{0})=0\) we have:_

\[ h_{k}(_{k})(x_{1},,x_{n}).\] (8)

Theorem 1 and 2 and Proposition 1 follow from this key proposition: by suitably modifying classical convex optimization techniques to account for the time-varying potentials, we can prove the convergence of the iterates towards an interpolator \(^{*}_{}\) along with that of the relevant quantities \(_{,k}\), \(_{k}\) and \(_{k}\). The implicit regularisation problem then directly follows from: (1) the limit condition \( h_{}(_{})(x_{1},,x_{n})\) as seen from Eq. (8) and (2) the interpolation condition \(X^{*}_{}=y\). Indeed, these two conditions exactly correspond to the KKT conditions of the convex problem Eq. (5).

Analysis of the impact of the stepsize and stochasticity on \(_{}\)

In this section, we analyse the effects of large stepsizes and stochasticity on the implicit bias of (S)GD. We focus on how these factors influence the effective initialisation \(_{}\), which plays a key role as shown in Theorem 1. From its definition in Eq. (6), we see that \(_{}\) is a function of the vector \(_{k}q(_{k}_{_{k}}(_{k}))\). We henceforth call this quantity the _gain vector_. For simplicity of the discussions, from now on, we consider constant stepsizes \(_{k}=\) for all \(k 0\) and a uniform initialisation of the weights \(=\) with \(>0\). We can then write the gain vector as:

\[_{}(^{2}}{_{ }^{2}})=_{k}q(_{_{k}}( _{k}))^{d}\,.\]

Following our discussion in section 3.1 on the scale and the shape of \(_{}\), we recall the link between the scale and shape of \(_{}\) and the recovered solution:

**1.** The **scale** of \(_{}\), i.e. the magnitude of \(\|_{}\|_{1}\) indicates how much the implicit bias of (S)GD differs from that of gradient flow: \(\|_{}\|_{1} 0\) implies that \(_{}\) and therefore the recovered solution is close to that of gradient flow. On the contrary, \(\|_{}\|_{1}(1/)\) implies that \(_{}\) has effective scale much smaller than \(\) thereby changing the implicit regularisation Eq. (5).

**2.** The **shape** of \(_{}\) indicates which coordinates of \(\) in the associated minimum weighted \(_{1}\) problem are most penalised. First recall from Section 3.1 that a uniformly large \(_{}\) leads to \(_{_{}}\) being closer to the \(_{1}\)-norm. However, with small weight initialisation \( 0\), we have,

\[_{_{}}()()\|\|_{1}+ _{i=1}^{d}_{}()|_{i}|\,,\] (9)

In this case, having a heterogeneously large vector \(_{}\) leads to a weighted \(_{1}\) norm as the effective implicit regularisation, where the coordinates of \(\) corresponding to the largest entries of \(_{}\) are less likely to be recovered.

### The scale of \(_{}\) is increasing with the stepsize

The following proposition highlights the dependencies of the scale of the gain \(\|_{}\|_{1}\) in terms of various problem constants.

**Proposition 3**.: _Let \(_{b},_{b}>0\)3 be the largest and smallest values, respectively, such that \(_{b}H_{}H_{}^{2} _{b}H\). For any stepsize \(>0\) satisfying \(\) (as in Theorem 2), initialisation \(\) and batch size \(b[n]\), the magnitude of the gain satisfies:_

\[_{b}^{2}_{k}(_{k})[\|_{}\|_{1}] 2_{b}^{2} _{k}(_{k})\,,\] (10)

_where the expectation is over a uniform and independent sampling of the batches \((_{k})_{k 0}\)._

**The slower the training, the larger the gain.** Eq. (10) shows that the slower the training loss converges to \(0\), the larger the sum of the loss and therefore the larger the scale of \(_{}\). This means that the (S)GD trajectory deviates from that of gradient flow if the stepsize and/or noise slows down the training. This supports observations previously made from stochastic gradient flow  analysis.

**The bigger the stepsize, the larger the gain.** The effect of the stepsize on the magnitude of the gain is not directly visible in Eq. (10) because a larger stepsize tends to speed up the training. For stepsize \(0<_{}=\) as in Theorem 2 we have that (see Appendix G.1):

\[_{k}^{2}(_{k})=(()\|_{_{1}}^{*}\|_{1})\,.\] (11)

Eq. (11) clearly shows that increasing the stepsize **boosts** the magnitude \(\|_{}\|_{1}\) up until the limit of \(_{}\). Therefore, the larger the stepsize the smaller is the effective scale of \(_{}\). In turn, larger gap between \(_{}\) and \(\) leads to a larger deviation of (S)GD from the gradient flow.

**Large stepsizes and Edge of Stability.** The previous paragraph holds for stepsizes smaller than \(_{}\) for which we can theoretically prove convergence. But what if we use even bigger stepsizes? Let \((_{k}^{})_{k}\) denote the iterates generated with stepsize \(\) and let us define \(_{}_{ 0}\{ ^{}(0,),\ _{k}(_{k}^{^{}})<\}\), which corresponds to the largest stepsize such that the iterates still converge for a given problem (even if not provably so). From Proposition 3 we have that \(_{}_{}\). As we approach this upper bound on convergence \(_{}\), the sum \(_{k}(_{k}^{})\) diverges. For such large stepsizes, the iterates of gradient descent tend to "bounce" and this regime is commonly referred to as the _Edge of Stability_. In this regime, the convergence of the loss can be made arbitrarily slow due to these bouncing effects. As a consequence, as seen through Eq. (10), the magnitude of \(_{}\) can be become arbitrarily big as observed in Fig. 2 (left). In this regime, the recovered solution tends to dramatically differ from the gradient flow solution, as seen in Fig. 1.

**Impact of stochasticity and linear scaling rule.** Assuming inputs \(x_{i}\) sampled from \((0,^{2}I_{d})\) with \(^{2}>0\), we obtain, w.h.p. over the dataset (see Appendix G.3, Proposition 17). The scale of \(_{}\) decreases with batch size and there exists a factor \(n\) between that of SGD and that of GD. Additionally, the magnitude of \(_{}\) depends on \(\), resembling the **linear scaling rule** commonly used in deep learning .

By analysing the magnitude \(\|_{}\|_{1}\), we have explained **the distinct behavior of (S)GD with large stepsizes compared to gradient flow**. However, our current analysis does not qualitatively distinguish the behavior between SGD and GD beyond the linear stepsize scaling rules, in contrast with Fig. 1. A deeper understanding of the shape of \(\) is needed to explain this disparity.

### The shape of \(_{}\) explains the differences between GD and SGD

In this section, we restrict our presentation to single batch SGD (\(b=1\)) and full batch GD (\(b=n\)). When visualising the typical shape of \(_{}\) for large stepsizes (see Fig. 2 - right), we note that GD and SGD behave very differently. For GD, the magnitude of \(_{}\) is higher for coordinates in the support of \(_{_{1}}^{}\) and thus these coordinates are adversely weighted in the asymptotic limit of \(_{_{}}\) (per (9)). This explains the distinction seed in Fig. 1, where GD in this regime has poor sparse recovery despite having a small scale of \(_{}\), as opposed to SGD that behaves well.

The **shape** of \(_{}\) is determined by the sum of the squared gradients \(_{k}_{_{k}}(_{k})^{2}\), and in particular by the degree of heterogeneity among the coordinates of this sum. Precisely analysing the sum over the whole trajectory of the iterates \((_{k})_{k}\) is technically out of reach. However, we empirically observe for the trajectories shown in Fig. 2 that the shape is largely determined within the first few iterates as formalized in the observation below.

**Observation 1**.: \(_{k}_{_{k}}(_{k})^{2}[ _{_{k}}(_{0})^{2}]\)_._

Figure 2: _Left:_ the scale of \(_{}\) explodes as \(_{}\) for both GD and SGD. _Right:_\(_{}^{}\) is fixed, we perform \(100\) runs of GD and SGD with different feature matrices, and we plot the \(d\) coordinates of \(_{}\) (for GD and SGD) on the \(x\)-axis (which is in log scale for better visualisation). The shape of \(_{}^{}\) is homogeneous whereas that of GD is heterogeneous with much higher magnitude on the support of \(_{}^{}\). The shape of \(_{}^{}\) is proportional to the expected gradient at initialisation which is \((_{}^{})^{2}\).

In the simple case of a Gaussian noiseless sparse recovery problem (where \(y_{i}=^{}_{},x_{i}\) for some sparse vector \(^{}_{}\)), we can control these gradients for GD and SGD (Appendix G.4) as:

\[(_{0})^{2}=(^{}_{} )^{2}+\,,\|\| _{}<<\|^{}_{}\|_{}^{2},\] (12) \[_{i_{0}}[_{i_{0}}(_{0})^{2}]= \|^{}_{}\|_{2}^{2}\,.\] (13)

The gradient of GD is heterogeneous.Since \(^{}_{}\) is sparse by definition, we deduce from Eq. (25) that \((_{0})\) is heterogeneous with larger values corresponding to the support of \(^{}_{}\). Along with observation 1, this means that \(_{}\)**has much larger values on the support of \(^{}_{}\)**. The corresponding weighted \(_{1}\)-norm therefore penalises the coordinates belonging to the support of \(^{}_{}\), which hinders the recovery of \(^{}_{}\) (as explained in Example 1, Appendix D).

The stochastic gradient of SGD is homogeneous.On the contrary, from Eq. (26), we have that the initial stochastic gradients are homogeneous, leading to a weighted \(_{1}\)-norm where the weights are roughly balanced. The corresponding weighted \(_{1}\)-norm is therefore close to the uniform \(_{1}\)-norm and the classical \(_{1}\) recovery guarantees are expected.

Overall summary of the joint effects of the scale and shape.In summary we have the following trichotomy which fully explains Fig. 1:

1. for small stepsizes, the scale is small, and (S)GD solutions are close to that of gradient flow;
2. for large stepsizes the scale is significant and the recovered solutions differ from GF: * for SGD the shape of \(_{}\) is uniform, the associated norm is closer to the \(_{1}\)-norm and the recovered solution is closer to the sparse solution; * for GD, the shape is heterogeneous, the associated norm is weighted such that it hinders the recovery of the sparse solution.

In this last section, we relate heuristically these findings to the _Edge of Stability_ phenomenon.

## 5 Edge of Stability: the neural point of view

In recent years it has been noticed that when training neural networks with 'large' stepsizes at the limit of divergence, GD enters the _Edge of Stability (EoS)_ regime. In this regime, as seen in Fig. 3, the iterates of GD 'bounce' / 'oscillate'. In this section, we come back to the point of view of the weights \(w_{k}=(u_{k},v_{k})^{2d}\) and make the connection between our previous results and the common understanding of the _EoS_ phenomenon. The question we seek to answer is: in which case does GD enter the _EoS_ regime, and if so, what are the consequences on the trajectory? _Keep in mind that this section aims to provide insights rather than formal statements._ We study the GD trajectory starting from a small initialisation \(=\) where \( 1\) such that we can consider that gradient flow converges close to the sparse interpolator \(^{}_{}=_{w^{}_{}}\) corresponding to the weights \(w^{}_{}=(_{}|}, (^{}_{})_{}|})\) (see Lemma 1 in  for the mapping from the predictors to weights for gradient flow). The trajectory of GD as seen in Fig. 3 (left) can be decomposed into up to \(3\) phases.

**First phase: gradient flow.** The stepsize is appropriate for the local curvature (as seen in Fig. 3, lower right) around initialisation and the iterates of GD remain close to the trajectory of gradient flow (in black in Fig. 3). If the stepsize is such that \(<}(^{2}F(w^{}_{}))}\), then it is compatible with the local curvature and the iterates can converge: in this case GF and GD converge to the same point (as seen in Fig. 1 for small stepsizes). For larger \(>}(^{2}F(w^{}_{}))}\) (as is the case for \(_{}\) in Fig. 3, lower right), the iterates cannot converge to \(^{}_{}\) and we enter the oscillating phase.

**Second phase: oscillations.** The iterates start oscillating. The gradient of \(F\) writes \(_{(u,v)}F(w)(() v,( ) u)\) and for \(w\) in the vicinity of \(w^{}_{}\) we have that \(u_{i} v_{i} 0\) for \(i(^{}_{})\). Therefore for \(w w^{}_{}\) we have that \(_{u}F(w)_{i}_{v}F(w)_{i} 0\) for \(i(^{}_{})\) and the gradients roughly belong to \((e_{i},e_{i+d})_{i(^{}_{})}\). This meansthat only the coordinates of the weights \((u_{i},v_{i})\) for \(i(^{*}_{})\) can oscillate and similarly for \((_{i})_{i(^{*}_{})}\) (as seen Fig. 3 left).

**Last phase: convergence.** Due to the oscillations, the iterates gradually drift towards a region of lower curvature (Fig. 3, lower right, the sharpness decreases) where they may (potentially) converge. Theorem 1 enables us to understand where they converge: the coordinates of \(_{k}\) that have oscillated significantly along the trajectory belong to the support of \(^{*}_{}\), and therefore \(_{}()\) becomes much larger for \(i(^{*}_{})\) than for the other coordinates. Thus, the coordinates of the solution recovered in the _EoS_ regime are heavily penalised on the support of the sparse solution. This is observed in Fig. 3 (left): the oscillations of \((_{i})_{i(^{*}_{})}\) lead to a gradual shift of these coordinates towards \(0\), hindering an accurate recovery of the solution \(^{*}_{}\).

**SGD in the _EoS_ regime.** In contrast to the behavior of GD where the oscillations primarily occur on the non-sparse coordinates of ground truth sparse model, for SGD we see a different behavior in Fig. 6 (Appendix A). For stepsizes in the _EoS_ regime, just below the non-convergence threshold: the fluctuation of the coordinates occurs evenly over all coordinates, leading to a uniform \(_{}\). These fluctuations are reminiscent of label-noise SGD , that have been shown to recover the sparse interpolator in diagonal linear networks .

## 6 Conclusion

We study the effect of stochasticity along with large stepsizes when training DLNs with (S)GD. We prove convergence of the iterates as well as explicitly characterise the recovered solution by exhibiting an implicit regularisation problem which depends on the iterates' trajectory. In essence the impact of stepsize and minibatch size are captured by the effective initialisation parameter \(_{}\) that depends on these choices in an informative way. We then use our characterisation to explain key empirical differences between SGD and GD and provide further insights on the role of stepsize and stochasticity. In particular, our characterisation explains the fundamentally different generalisation properties of SGD and GD solutions at large stepsizes as seen in Fig. 1: without stochasticity, the use of large stepsizes can prevent the recovery of the sparse interpolator, even though the effective scale of the initialization decreases with larger stepsize for both SGD and GD. We also provide insights on the link between the _Edge of Stability_ regime and our results.

### Aknowledgements

M. Even deeply thanks Laurent Massoulie for making it possible to visit Microsoft Research and the Washington state during an internship supervised by Suriya Gunasekar, the MSR Machine Learning Foundations group for hosting him, and Martin Jaggi for inviting him for a week in Lausanne at EPFL, making it possible to meet and discuss with Scott Pesme and Nicolas Flammarion.

Figure 3: GD at the _EoS. Left_: For GD, the coordinates on the support of \(^{*}_{}\) oscillate and drift towards \(0\). _Right, top:_ The GD train losses saturate before eventually converging. _Bottom:_ GF converges towards a solution that has a high hessian maximum eigenvalue. GD cannot converge towards this solution because of its large stepsize: it therefore drifts towards a solution that has a curvature just below \(2/\).