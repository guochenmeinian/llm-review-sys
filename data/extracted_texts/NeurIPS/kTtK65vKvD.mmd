# ODGEN: Domain-specific Object Detection Data Generation with Diffusion Models

Jingyuan Zhu\({}^{1}\) Shiyu Li\({}^{2}\) Yuxuan Liu\({}^{2}\) Jian Yuan\({}^{1}\) Ping Huang\({}^{2}\)

Jiulong Shan\({}^{2}\) Huimin Ma\({}^{3}\)

\({}^{1}\)Tsinghua University, China \({}^{2}\)Apple \({}^{3}\)University of Science and Technology Beijing

H. Ma is the corresponding author.

###### Abstract

Modern diffusion-based image generative models have made significant progress and become promising to enrich training data for the object detection task. However, the generation quality and the controllability for complex scenes containing multi-class objects and dense objects with occlusions remain limited. This paper presents ODGEN, a novel method to generate high-quality images conditioned on bounding boxes, thereby facilitating data synthesis for object detection. Given a domain-specific object detection dataset, we first fine-tune a pre-trained diffusion model on both cropped foreground objects and entire images to fit target distributions. Then we propose to control the diffusion model using synthesized visual prompts with spatial constraints and object-wise textual descriptions. ODGEN exhibits robustness in handling complex scenes and specific domains. Further, we design a dataset synthesis pipeline to evaluate ODGEN on 7 domain-specific benchmarks to demonstrate its effectiveness. Adding training data generated by ODGEN improves up to 25.3% mAP@.50:.95 with object detectors like YOLOv5 and YOLOv7, outperforming prior controllable generative methods. In addition, we design an evaluation protocol based on COCO-2014 to validate ODGEN in general domains and observe an advantage up to 5.6% in mAP@.50:.95 against existing methods.

## 1 Introduction

Object detection is one of the most widely used computer vision tasks in real-world applications. However, data scarcity poses a significant challenge to the performance of state-of-the-art models

Figure 1: The proposed ODGEN enables controllable image generation from bounding boxes and text prompts. It can generate high-quality data for complex scenes, encompassing multiple categories, dense objects, and occlusions, which can be used to enrich the training data for object detection.

like YOLOv5  and YOLOv7 . With the progress of generative diffusion models, such as DALL-E , and Stable Diffusion , which can generate high-quality images from text prompts, recent works have started to explore the potential of synthesizing images for perceptual model training. Furthermore, methods like GLIGEN , ReCo , and ControlNet  provide various ways to control the contents of synthetic images. Concretely, extra visual or textual conditions are introduced as guidance for diffusion models to generate objects under certain spatial constraints. Therefore, it becomes feasible to generate images together with instance-level or pixel-level annotations.

Nevertheless, it remains challenging to generate an effective supplementary training set for real-world object detection applications. Firstly, large-scale pre-trained diffusion models are usually trained on web crawl datasets such as LAION , whose distributions may be quite distinct from specialist domains. The domain gap could undermine the fidelity of generated images, especially the detailed textures and styles of the objects, e.g., the tumor in medical images or the avatar in a video game. Secondly, the text prompts for object detection scenes may contain multiple categories of subjects. It could cause the "concept bleeding" [12; 40; 68] problem for modern diffusion models, which is defined as the unintended merging or overlapping of distinct visual elements in an image. As a result, the synthetic images can be inconsistent with pseudo labels used as generative conditions. Thirdly, overlapping objects, which are common in object detection datasets, are likely to be merged as one single object by existing controllable generative methods. Lastly, some objects may be neglected by the diffusion model so that no foreground is generated at the conditioned location. All of the limitations can hinder the downstream object detection performance.

To address these challenges, we propose ODGEN, a novel method to generate synthetic images with a fine-tuned diffusion model and object-wise conditioning modules that control the categories and locations of generated objects. We first fine-tune the diffusion model on a given domain-specific dataset, with not only entire images but also cropped foreground patches, to improve the synthesis quality of both background scene and foreground objects. Secondly, we design a new text prompt embedding process. We propose to encode the class name of each object separately by the frozen CLIP  text encoder, to avoid mutual interference between different concepts. Then we stack the embeddings and encode them with a newly introduced trainable text embedding encoder. Thirdly, we propose to use synthetic patches of foreground objects as the visual condition. Each patch is resized and pasted on an empty canvas according to bounding box annotations, which deliver both conceptual and spatial information without interference from overlap between objects. In addition, we train foreground/background discriminators to check whether the region of each pseudo label contains a synthetic object. If no object can be found, the pseudo label will be filtered.

We evaluate the effectiveness of our ODGEN in specific domains on 7 subsets of the Roboflow-100 benchmark . Extensive experimental results show that adding our synthetic data improves up to 25.3% mAP@.50:.95 on YOLO detectors, outperforming prior controllable generative methods. Furthermore, we validate ODGEN in general domains with an evaluation protocol designed based on COCO-2014  and gain an advantage up to 5.6% in mAP@.50:.95 than prior methods.

The main contributions of this work can be summarized as follows:

* We propose to fine-tune pre-trained diffusion models with both cropped foreground patches and entire images to generate high-quality domain-specific target objects and background scenes.
* We design a novel strategy to control diffusion models with object-wise text prompts and synthetic visual conditions, improving their capability of generating and controlling complex scenes.
* We conduct extensive experiments to demonstrate that our synthetic data effectively improves the performance of object detectors. Our method outperforms prior works on fidelity and trainability in both specific and general domains.

## 2 Related Works

**Diffusion models**[7; 20; 29; 38; 49; 50] have made great progress in recent years and become mainstream for image generation, showing higher fidelity and training stability than prior generative models like GANs [4; 16; 26; 27; 28] and VAEs [30; 43; 52]. Latent diffusion models (LDMs)  perform the denoising process in the compressed latent space and achieve flexible generators conditioned on inputs like text and semantic maps. LDMs significantly improve computational efficiency and enable training on internet-scale datasets . Modern large-scale text-to-image diffusion modelsincluding eDiff-I , DALL-E , Imagen , and Stable Diffusion [40; 44] have demonstrated unparalleled capabilities to produce diverse samples with high fidelity given unfettered text prompts.

**Layout-to-image generation** synthesizes images conditioned on graphical inputs of layouts. Pioneering works [22; 35] based on GANs  and transformers  successfully generate images consistent with given layouts. LayoutDiffusion  fuses layout and category embeddings and builds object-aware cross-attention for local conditioning with traditional diffusion models. LayoutDiffuse  employs layout attention modules for bounding boxes based on LDMs. MultiDiffusion  and BoxDiff  develop training-free frameworks to produce samples with spatial constraints. GLIGEN  inserts trainable gated self-attention layers to pre-trained LDMs to fit specific tasks and is hard to generalize to scenes uncommon for pre-trained models. Reco  and GeoDiffusion  extend LDMs with position tokens added to text prompts and fine-tunees both text encoders and diffusion models to realize region-aware controls. They need abundant data to build the capability of encoding the layout information in text prompts. ControlNet  reuses the encoders of LDMs as backbones to learn diverse controls. However, it still struggles to deal with some complex cases. MIGC  decomposes multi-instance synthesis to single-instance subtasks utilizing additional attention modules. InstanceDiffusion  adds precise instance-level controls, including boxes, masks, and scribbles for text-to-image generation. Given annotations of bounding boxes, this paper introduces a novel approach applicable to both specific and general domains to synthesize high-fidelity complex scenes consistent with annotations for the object detection task.

**Dataset synthesis for training object detection models.** Copy-paste  is a simple but effective data augmentation method for detectors. Simple Copy-Paste  achieves improvements with a simple random placement strategy for objects. Following works [13; 65] generate foreground objects and then paste them on background images. However, generated images by these methods usually have obvious artifacts on the boundary of pasted regions. DatasetGAN  takes an early step to generate labeled images automatically. Diffusion models have been used to synthesize training data and benefit downstream tasks including object detection [5; 10; 11; 63], image classification [1; 8; 18; 34; 47; 51], and semantic segmentation [15; 23; 31; 36; 39; 56; 57; 59; 60]. Modern approaches for image-label pairs generation can be roughly divided into two categories. One group of works [11; 15; 31; 36; 56; 59; 63] first generate images and then apply a pre-trained perception model to generate pseudo labels on the synthetic images. The other group [5; 23; 39] uses the same strategy as our approach to synthesize images under the guidance of annotations as input conditions. The second group also overlaps with layout-to-image approaches [24; 33; 55; 61; 62; 66; 67]. Our approach should be assigned to the second group and is designed to address challenging cases like multi-class objects, occlusions between objects, and specific domains.

## 3 Method

This section presents ODGEN, a novel approach to generate high-quality images conditioned on bounding box labels for specific domains. Firstly, we propose a new method to fine-tune the diffusion model in Sec. 3.1. Secondly, we design an object-wise conditioning method in Sec. 3.2. Finally, we introduce a generation pipeline to build synthetic datasets for detector training enhancement in Sec. 3.3.

### Domain-specific Diffusion Model Fine-tuning

Modern Stable Diffusion  models are trained on the LAION-5B  dataset. Therefore, the textual prompts and generated images usually follow similar distributions to the web crawl data. However, real-world object detection datasets could come from very specific domains. We fine-tune the UNet of the Stable Diffusion to fit the distribution of an arbitrary object detection dataset. For training images, we use not only the entire images from the dataset but also the crops of foreground objects. In particular, we crop foreground objects and resize them to \(512 512\). In terms of the text input, as shown in Fig. 2 (a), we use templated condition "a <_scene_>" named \(c_{s}\) for entire images and "a <_classname_>" named \(c_{o}\) for cropped objects. If the names are terminologies, following the subject-driven approach in DreamBooth , we can employ identifiers like "[V]" to represent the target objects and scenes, improving the robustness to textual ambiguity under the domain-specific context. \(x_{s}^{t}\) and \(x_{o}^{t}\) represent the input of scenes \(x_{s}\) and objects \(x_{t}\) added to noises at time step \(t\). \(_{s}\) and \(_{o}\) represent the added noises following normal Gaussian distributions. The pre-trained StableDiffusion parameterized by \(\) is fine-tuned with the reconstruction loss as:

\[_{rec}=&_{x_{o},t, _{o}(0,1)}[||_{o}-_{}(x_{o }^{t},t,(c_{o}))||^{2}]\\ &+_{x_{s},t,_{s}(0,1)} [||_{s}-_{}(x_{s}^{t},t,(c_{s}))||^{2}] \] (1)

where \(\) controls the relative weight for the reconstruction loss of scene images. \(\) is the frozen CLIP text encoder. Our approach guides the fine-tuned model to capture more details of foreground objects and maintain its capability of synthesizing the complete scenes.

### Object-wise Conditioning

ControlNet  can perform controllable synthesis with visual conditions. However, the control can be challenging with an increasing object number and category number due to the "concept bleeding" phenomenon. Stronger conditions are needed to ensure high-fidelity generation. To this end, we propose a novel object-wise conditioning strategy with ControlNet.

**Text list encoding.** As shown in Fig. 2 (b) and (c), given a set of object class names and their bounding boxes, we first build a text list consisting of each object with the fixed template "a <_classname>_". Then the text list is padded with empty texts to a fixed length \(N\) and converted to a list of embeddings by a pre-trained CLIP text tokenizer and encoder. The embeddings are stacked and encoded by a 4-layer convolutional text embedding encoder, and used as the textual condition for ControlNet. The text encoding of native ControlNet compresses the information in global text prompts altogether, resulting in mutual interference between distinct categories. To alleviate this "concept bleeding" problem of multiple categories, our two-step text encoding enables the ControlNet to capture the information of each object with separate encoding.

Figure 2: ODGEN training pipeline: (a) A pre-trained diffusion model is fine-tuned on a detection dataset with both entire images and cropped foreground patches. (b) A text list is built based on class labels. The fine-tuned diffusion model in stage (a) is used to generate a synthetic object image for each text. Generated object images are resized and pasted on empty canvases per box positions, constituting an image list. (c) The image list is concatenated in the channel dimension and encoded as conditions for ControlNet. The text list is encoded by the CLIP text encoder, stacked, and encoded again by the text embedding encoder as inputs for ControlNet.

**Image list encoding.** As shown in Fig. 2 (b) and (c), for each item in the text list, we use the fine-tuned diffusion model to generate images for each foreground object. We then resize each generated image and paste it on an empty canvas based on the corresponding bounding box. The set of pasted images is denoted as an image list, which contains both conceptual and spatial information of the objects. The image list is concatenated and zero-padded to \(N\) in the channel dimension and then encoded to the size of latent space by an image encoder. Applying an image list rather than pasting all objects on a single image can effectively avoid the influence of object occlusions.

**Optimization.** With a pair of image and object detection annotation, we generate the text list \(c_{tl}\) and image list \(c_{il}\) as introduced in this section. The input global text prompt \(c_{t}\) of the diffusion model is composed of the object class names and a scene name, which is usually related to the dataset name and fixed for each dataset. The ControlNet, along with the integrated encoders, are trained with the reconstruction loss. In addition, the weights of foreground regions are enhanced to produce more realistic foreground objects in complex scenes. The overall loss function is formulated as:

\[_{recon}=_{x,t,c_{t},c_{tl},c_{ il},(0,1)}[||-_{}(x_{t},t,c_{ tl},c_{il})||^{2}]\\ _{control}=_{recon}+_{recon }\] (2)

where \(\) represents a binary mask with \(1\) on foreground pixels \(0\) on background pixels. \(\) represents element-wise multiplication, and \(\) controls the foreground re-weighting.

### Dataset Synthesis Pipeline for Object Detection

Our dataset synthesis pipeline is summarized in Fig. 3. We generate random bounding boxes and classes as pseudo labels based on the distributions of the training set. Then, the pseudo labels are converted to triplets: <image list, text list, global text prompt>. ODGEN uses the triplets as inputs to synthesize images with the fine-tuned diffusion model. In addition, we filter out the pseudo labels in which areas the foreground objects fail to be generated.

**Object distribution estimation.** To simulate the training set distribution, we compute the mean and variance of bounding box attributes and build normal distributions. In particular, given a dataset with \(K\) categories, we calculate the class-wise average occurrence number per image \(=(_{1},_{2},,_{K})\) and the covariance matrix \(\) to build a multi-variable joint normal distribution \((,)\). In addition, for each category \(k\), we build normal distributions \((_{x},_{x}^{2})\) and \((_{y},_{y}^{2})\)

Figure 3: Pipeline for object detection dataset synthesis. **Yellow block**: estimate Gaussian distributions for the bounding box number, area, aspect ratio, and location based on the training set. **Blue block**: sample pseudo labels from the Gaussian distributions and generate conditions including text and image lists to synthesize novel images. **Pink block**: train a classifier with foreground and background patches randomly cropped from the training set and use it to filter pseudo labels that failed to be synthesized. Finally, the filtered labels and synthetic images compose datasets.

for the top-left coordinate of the bounding boxes, \((_{area},_{area}^{2})\) for box areas, and distributions \((_{ratio},_{ratio}^{2})\) for aspect ratios.

**Image synthesis.** We first sample the object number per category from the \(K\)-dimensional joint normal distributions. Then for each object, we sample its bounding box location and size from the corresponding estimated normal distributions. The sampled values are used to generate text lists, image lists, and global text prompts. Finally, we use ODGEN to generate images with the triplets.

**Corrupted label filtering.** There is a certain possibility that some objects fail to be generated in synthetic images. While utilizing image lists alleviates the concern to some extent, the diffusion model may still neglect some objects in complex scenes containing dense or overlapping objects. As a result, the synthesized pixels could not match the pseudo labels and will undermine downstream training performance. We fine-tune a ResNet50  with foreground and background patches cropped from the training set to classify whether the patch contains an object. The discriminator checks whether objects are successfully synthesized in the regions of pseudo labels, rejects nonexistent ones, and further improves the accuracy of synthetic datasets.

## 4 Experiments

We conduct extensive experiments to demonstrate the effectiveness of the proposed ODGEN in both specific and general domains. FID  is used as the metric of fidelity. Mean average precisions (mAP) of YOLOv5s  and YOLOv7  trained with synthetic data are used to evaluate the trainability, which concerns the usefulness of synthetic data for detector training. Our approach is implemented with Stable Diffusion v2.1 and compared with prior controllable generation methods based on Stable Diffusion, including ReCo , GLIGEN , ControlNet , GeoDiffusion , InstanceDiffusion , and MIGC . Native ControlNet doesn't support bounding box conditions. Therefore, we convert boxes to a mask \(C\) sized \(H W K\), where \(H\) and \(W\) are the image height and width, and \(K\) is the class number. If one pixel \((i,j)\) is in \(n\) boxes of class \(k\), the \(C[i,j,k]=n\). YOLO models are trained with the same recipe as Roboflow-100  for 100 epochs to ensure convergence. The length of text and image lists \(N\) used in our ODGEN is set to the maximum object number per image in the training set.

Figure 4: Comparison between ODGEN and other methods under the same condition shown in the first column. ODGEN can be generalized to specific domains and enables accurate layout control.

### Specific Domains

Roboflow-100  is used for the evaluation in specific domains. It consists of 100 object detection datasets of various domains. We select 7 representative datasets (RF7) covering video games, medical imaging, and underwater scenes. To simulate data scarcity, we only sample 200 images as the training set for each dataset. The whole training process including the fine-tuning on both cropped objects and entire images and the training of the object-wise conditioning module, only depends on the 200 images. \(\) in Eq. (1) and \(\) in Eq. (2) are set as 1 and 25. We first fine-tune the diffusion model according to Fig. 2 (a) for 3k iterations. Then we train the object-wise conditioning modules on a V100 GPU with batch size 4 for 200 epochs, the same as the other methods to be compared.

#### 4.1.1 Fidelity

For each dataset in RF7, we compute the FID scores of 5000 synthetic images against real images, respectively. As shown in Tab. 1, our approach outperforms all the other methods. We visualize the generation quality in Fig. 4. GLIGEN only updates gated self-attention layers inserted to Stable Diffusion, making it hard to be generalized to specific domains like MRI images and the Apex game. ReCo and GeoDiffusion integrate encoded bounding box information into text tokens, which require more data for diffusion model and text encoder fine-tuning. With only 200 images, they fail to generate objects in the given box regions. ControlNet, integrating the bounding box information into the visual condition, presents more consistent results with the annotation. However, it may still miss some objects or generate wrong categories in complex scenes. ODGEN achieves superior performance on both visual effects and consistency with annotations.

#### 4.1.2 Trainability

To explore the effectiveness of synthetic data for detector training, we train YOLO models pre-trained on COCO with different data and compare their mAP@.50:95 scores in Tab. 2. In the baseline setting, we only use the 200 real images for training. For the other settings, we use a combination of 200 real and 5000 synthetic images. Our approach gains improvement over the baseline and outperforms all the other methods.

In addition, we add experiments with larger-scale datasets with 1000 images sampled from the Apex Game and the Underwater datasets. The training setups are kept the same as the training on 200 images. We conduct experiments on ODGEN, ReCo, and GeoDiffusion. ReCo and GeoDiffusion

  Datasets & ReCo & GLIGEN & ControlNet & GeoDiffusion & ODGEN \\  Apex Game & 88.69 & 125.27 & 97.32 & 120.61 & **58.21** \\ Robomaster & 70.12 & 167.44 & 134.92 & 76.81 & **57.37** \\ MRI Image & 202.36 & 270.52 & 212.45 & 341.74 & **93.82** \\ Cotton & 108.55 & 89.85 & 196.87 & 203.02 & **85.17** \\ Road Traffic & 80.18 & 98.83 & 162.27 & 68.11 & **63.52** \\ Aquarium & 122.71 & 98.38 & 146.26 & 162.19 & **83.07** \\ Underwater & 73.29 & 147.33 & 126.58 & 125.32 & **70.20** \\  

Table 1: FID (\(\)) scores computed over 5000 images synthesized by each approach on RF7 datasets. ODGEN achieves better results than the other on all 7 domain-specific datasets.

   & Baseline & ReCo & GLIGEN & ControlNet & GeoDiffusion & ODGEN \\  real + synth \# & 200 + 0 & 200 + 5000 & 200 + 5000 & 200 + 5000 & 200 + 5000 & 200 + 5000 \\  Apex Game & 38.3 / 47.2 & 25.0 / 31.5 & 24.8 / 32.5 & 33.8 / 42.7 & 29.2 / 35.8 & **39.9 / 52.6** \\ Robomaster & 27.2 / 26.5 & 18.2 / 27.9 & 19.1 / 25.0 & 24.4 / 32.9 & 18.2 / 22.6 & **39.6 / 34.7** \\ MRI Image & 37.6 / 27.4 & 42.7 / 38.3 & 23.3 / 25.9 & 44.7 / 37.2 & 42.0 / 38.9 & **46.1 / 41.5** \\ Cotton & 16.7 / 20.5 & 29.3 / 37.5 & 28.0 / 39.0 & 22.6 / 35.1 & 30.2 / 36.0 & **42.0 / 43.2** \\ Road Traffic & 35.3 / 41.0 & 22.8 / 29.3 & 22.2 / 29.5 & 22.1 / 30.5 & 17.2 / 29.4 & **39.2 / 43.8** \\ Aquarium & 30.0 / 29.6 & 23.8 / 34.3 & 24.1 / 32.2 & 18.2 / 25.6 & 21.6 / 30.9 & **32.2 / 38.5** \\ Underwater & 16.7 / 19.4 & 13.7 / 15.8 & 14.9 / 18.5 & 15.5 / 17.8 & 13.8 / 17.2 & **19.2 / 22.0** \\  

Table 2: mAP@.50:95 (\(\)) of YOLOv5s / YOLOv7 on RF7. Baseline models are trained with 200 real images only, whereas the other models are trained with 200 real + 5000 synthetic images from various methods. ODGEN leads to the biggest improvement on all 7 domain-specific datasets.

may benefit from larger-scale training datasets since they need to fine-tune more parameters in both the UNet in Stable Diffusion and the CLIP text encoder. GLIGEN struggles to adapt to new domains. ControlNet performs worse on layout control than ODGEN. Therefore, these two methods are not included in this part. The corrupted label filtering step is not used for any method. Results in Tab. 3 show that the baselines trained on real data only become stronger with larger-scale datasets while our ODGEN still benefits detectors with synthetic data and outperforms other methods.

### General Domains

COCO-2014  is used to evaluate our method in general domains. As the scenes in this dataset are almost covered by the pre-training data of Stable Diffusion, we skip the domain-specific fine-tuning stage and train the diffusion model together with the object-wise conditioning modules. We train our ODGEN on the COCO training set with batch size 32 for 60 epochs on \( 8\) V100 GPUs as well as GLIGEN, ControlNet, and GeoDiffusion. Officially released checkpoints of ReCo (trained for 100 epochs on COCO), MIGC (trained for 300 epochs on COCO), and InstanceDiffusion (trained on self-constructed datasets) are employed for comparison. \(\) in Eq. (2) is set as 10 for our ODGEN.

#### 4.2.1 Fidelity

We use the annotations of the COCO validation set as conditions to generate 41k images. The FID scores in Tab. 4 are computed against the COCO validation set. ODGEN achieves better FID results than the other methods. We provide a typical visualized example in Fig. 5. Given overlapping bounding boxes of multiple categories, ODGEN synthesizes all the objects of correct categories and accurate positions, outperforming the other methods. More samples are added in Appendix G.

   Metrics & ReCo & GLIGEN & ControlNet &  Geo- \\ Diffusion \\  &  MIGC \\ Diffusion \\  & 
 Instance- \\ Diffusion \\  & ODGEN \\  FID & 18.36 & 26.15 & 25.54 & 30.00 & 21.82 & 23.29 & **16.16** \\  mAP@.50 & 7.60 / 11.01 & 6.70 / 9.42 & 1.43 / 1.15 & 5.94 / 9.21 & 9.54 / 16.01 & 10.00 / 17.10 & **18.90 / 24.40** \\ mAP@.50:95 & 3.82 / 5.29 & 3.56 / 4.60 & 0.52 / 0.38 & 2.37 / 4.44 & 4.67 / 8.65 & 5.42 / 10.20 & **9.70 / 14.20** \\   

Table 4: FID (\(\)) and mAP (\(\)) of YOLOv5s / YOLOv7 on COCO. FID is computed with 41k synthetic images. For mAP, YOLO models are trained from scratch on 10k synthetic images and validated on 31k real images. ODGEN outperforms all the other methods in terms of both fidelity and trainability.

Figure 5: Visualized results comparison for models trained on COCO. ODGEN is better qualified for synthesizing complex scenes with multiple categories of objects and bounding box occlusions.

   Datasets & Apex Game & Apex Game & Apex Game & Underwater & Underwater & Underwater \\  real + synth \# & 1000 + 0 & 1000 + 5000 & 1000 + 10000 & 1000 + 0 & 1000 + 5000 & 1000 + 10000 \\  ReCo & 83.2 / 53.5 & 78.7 / 46.9 & 82.0 / 46.9 & 55.6 / 29.2 & 55.1 / 28.4 & 55.9 / 29.1 \\ GeoDiffusion & 83.2 / 53.5 & 80.0 / 47.2 & 82.5 / 47.5 & 55.6 / 29.2 & 54.2 / 27.9 & 54.3 / 28.0 \\ ODGEN & 83.2 / 53.5 & **83.3 / 53.5** & **83.6 / 53.6** & 55.6 / 29.2 & **59.6 / 32.5** & **56.3 / 29.8** \\  ReCo & 83.8 / 55.0 & 80.5 / 50.7 & 79.2 / 49.9 & 54.6 / 28.3 & 56.5 / 28.7 & 56.4 / 30.1 \\ GeoDiffusion & 83.8 / 55.0 & 81.2 / 51.0 & 81.0 / 50.5 & 54.6 / 28.3 & 57.0 / 28.9 & 55.8 / 28.9 \\ ODGEN & 83.8 / 55.0 & **84.4 / 55.2** & **84.0 / 55.0** & 54.6 / 28.3 & **58.2 / 29.8** & **62.1 / 31.8** \\   

Table 3: mAP@.50 / mAP@.50:95 (\(\)) results of ODGEN trained on larger-scale datasets of 1000 real images. The top 3 rows show results of YOLOv5s and the bottom 3 rows show results of YOLOv7. Baseline models are trained with 1000 real images only, whereas the other models are trained with 1000 real + 5000 / 10000 synthetic images from various methods. ODGEN leads to more significant improvement than other methods.

[MISSING_PAGE_FAIL:9]

using global text prompts in the placement of text lists or pasting all the foreground patches on the same empty canvas in the placement of image lists for the ablation study. Experiments are performed on the MRI and Road Traffic datasets in RF7. The MRI dataset only has two categories and almost no overlapping objects, whereas the road traffic dataset has 7 categories and many occluded objects. From the results in Tab. 7, it can be found that using image lists or text lists brings moderate improvement on MRI images and significant improvement on the more complex Road Traffic dataset. Visualization in Fig. 6 shows that the image list improves the fidelity under occlusion, and the text list mitigates the mutual interference of multiple objects.

**Foreground region enhancement.** We introduce a re-weighting term controlled by \(\) for foreground objects in Eq. (2). Tab. 8 shows that appropriate \(\) values can improve both the fidelity and the trainability since the foreground details are better generated. However, increasing the value will undermine the background quality, as visualized in Fig. 6.

**Corrupted label filtering.** Results in Tab. 9 show that the corrupted label filtering step helps improve the mAP@.50:95 of ODGEN by around 1-2%. The results of ODGEN without the corrupted label filtering still significantly outperform the baseline setting in Tab. 2.

## 5 Conclusion

This paper introduces ODGEN, a novel approach aimed at generating domain-specific object detection datasets to enhance the performance of detection models. We propose to fine-tune the diffusion model on both cropped foreground objects and entire images. We design a mechanism to control the foreground objects with object-wise synthesized visual prompts and textual descriptions. Our work significantly enhances the performance of diffusion models in synthesizing complex scenes characterized by multiple categories of objects and bounding box occlusions. Extensive experiments demonstrate the superiority of ODGEN in terms of fidelity and trainability across both specific and general domains. We believe this work has taken an essential step toward more robust image synthesis and highlights the potential of benefiting the object detection task with synthetic data.

Figure 6: Visualized ablations. **Left**: using neither image nor text lists, a traffic light is generated in the position of a bus, and a car is generated in the position of a traffic light; **Middle**: not using image list, two occluded motorcycles are merged as one; **Right**: not using text list, a motorcycle is generated in the position of a vehicle. Using both image and text lists generates correct results.

Figure 7: With increasing \(\) value, our approach produces higher-quality foreground objects. However, overwhelming \(\) leads to blurred background and degenerated fidelity.

   & Label &  \\  & filtering & \\   & ✓ & **42.0** / **43.2** \\  & \(\) & 40.5 / 42.1 \\   & ✓ & **39.6** / **34.7** \\  & \(\) & 39.0 / 33.3 \\   & ✓ & **19.2** / **22.0** \\  & \(\) & 18.9 / 21.6 \\  

Table 9: mAP (YOLOv5s / YOLOv7) of the corrupted label filtering for ODGEN.