# Gradient Cuff: Detecting Jailbreak Attacks on

Large Language Models by Exploring

Refusal Loss Landscapes

 Xiaomeng Hu

The Chinese University of Hong Kong

Sha Tin, Hong Kong

xmhu23@cse.cuhk.edu.hk

&Pin-Yu Chen

IBM Research

New York, USA

pin-yu.chen@ibm.com

&Tsung-Yi Ho

The Chinese University of Hong Kong

Sha Tin, Hong Kong

tyho@cse.cuhk.edu.hk

###### Abstract

Large Language Models (LLMs) are becoming a prominent generative AI tool, where the user enters a query and the LLM generates an answer. To reduce harm and misuse, efforts have been made to align these LLMs to human values using advanced training techniques such as Reinforcement Learning from Human Feedback (RLHF). However, recent studies have highlighted the vulnerability of LLMs to adversarial jailbreak attempts aiming at subverting the embedded safety guardrails. To address this challenge, this paper defines and investigates the **Refusal Loss** of LLMs and then proposes a method called **Gradient Cuff** to detect jailbreak attempts. Gradient Cuff exploits the unique properties observed in the refusal loss landscape, including functional values and its smoothness, to design an effective two-step detection strategy. Experimental results on two aligned LLMs (LLaMA-2-7B-Chat and Vicuna-7B-V1.5) and six types of jailbreak attacks (GCG, AutoDAN, PAIR, TAP, Base64, and LRL) show that Gradient Cuff can significantly improve the LLM's rejection capability for malicious jailbreak queries, while maintaining the model's performance for benign user queries by adjusting the detection threshold.

## 1 Introduction

With the stupendous success of large language models (LLMs) such as GPT-4 , LLaMA-2 , and Vicuna , there is a trend to integrate these LLMs into various applications such as ChatGPT and Bing Search. In these applications, LLMs are used as the service backend. The front end of these applications receives the user input query from the interface, encapsulates it into a system prompt, and then sends it to the LLM to get a response. With the rapidly increasing social impact of these applications, model alignment and safety assurance to reduce harm and misuse have become significant considerations when developing and deploying LLMs. Methods such as ReinforcementLearning from Human Feedback (RLHF) have been proven to be an effective training technique to align LLMs with human values .

However, aligned LLMs have been found to be vulnerable to a type of adversarial manipulation known as "jailbreak attack". Jailbreak attacks involve maliciously inserting or replacing tokens in the user instruction or rewriting it to bypass and circumvent the safety guardrails of aligned LLMs. A notable example is that a jailbroken LLM would be tricked into generating hate speech targeting certain groups of people, as demonstrated in Figure 1 (a).

Many red-teaming efforts  have been put into designing algorithms to automatically generate jailbreak prompts to help test the robustness of aligned LLMs. Specifically, GCG , one of the earlier works in this area, can successfully jailbreak several LLMs by optimizing an inserted universal adversarial suffix. This finding suggests that the embedded alignment effort in LLMs could be completely broken by the jailbreak attack.

Since the discovery of jailbreak risks for LLMs, various methods have been explored to defend against jailbreak attacks  and have gained some success in detecting certain types of attacks such as GCG . However, in our systematic analysis, existing defenses either fail to be resistant against all types of jailbreak attacks, or have a significant detrimental effect on benign queries. PPL  uses an LLM to compute the perplexity of the input user query and filters those with perplexity greater than the threshold. PPL has been proven to have a good detection performance for GCG attacks but does not perform well on jailbreak prompts with good meaningfulness and fluency . Erase-Check  and Self-Reminder  behave well on malicious queries but will misclassify many benign user queries, making the defenses overly conservative and impractical.

To alleviate the threats of jailbreak attacks and avoid the aforementioned problems in existing defenses, we propose **Gradient Cuff**, which detects jailbreak prompts by checking the refusal loss of the input user query and estimating the gradient norm of the loss function. We begin by introducing the concept of refusal loss and showcase the different behaviors of the loss function for benign instructions and malicious instructions. A plot of the refusal loss landscape for benign and malicious instructions can be found in Figure 1 (b). By exploring the landscapes of refusal loss, we find that the refusal loss function for malicious instructions tends to have a smaller value and a larger gradient norm.

Figure 1: Overview of **Gradient Cuff**. (a) introduces an example of jailbreak prompts by presenting a conversation between malicious actors and the Vicuna chatbot. (b) visualizes the refusal loss landscape for malicious queries and benign queries by plotting the interpolation of two random directions in the query embedding with coefficients \(\) and \(\) following . The refusal loss evaluates the probability that the LLM would not directly reject the input query, and the loss value is computed using Equation 3. See details of how to plot (b) in Appendix A.4. (c) shows the running flow of Gradient Cuff (at top), practical computing examples for refusal loss (at bottom left), and the distributional difference of the gradient norm of refusal loss on benign and malicious queries (bottom right). (d) shows the performance of Gradient Cuff against 6 jailbreak attacks for Vicuna-7B-V1.5. See Appendix A.6 for full results.

We then leverage this unique loss landscape characteristic to propose a two-step jailbreak detection algorithm, which is illustrated in Figure 1 (c). Figure 1 (d) evaluates 6 jailbreak attacks on Vicuna-7B-V1.5 and shows that our defense can reduce the attack success rate (ASR) averaged over these 6 jailbreaks from \(76.7\%\) to \(25.7\%\) on average. We also compare Gradient Cuff to other existing defense methods on Vicuna-7B-V1.5 and LLaMA-2-7B-Chat against these 6 jailbreaks as well as adaptive attacks to demonstrate our defense capabilities.

We summarize our **main contributions** as follows:

* We formalize the concept of refusal loss function of LLMs and explore its smoothness and values of the loss landscapes on benign and malicious queries. The distinct refusal loss characteristics are used in our Gradient Cuff framework to detect jailbreak prompts.
* Experiments on 2 aligned LLMs (LLaMA-2-7B-Chat and Vicuna-7B-V1.5) and 6 jailbreak attacks (GCG, AutoDAN, PAIR, TAP, Base64, and LRL) demonstrate that Gradient Cuff is the only defense algorithm that can attain good jailbreak detection while keeping an acceptable rejection rate on benign queries.
* We also show that Gradient Cuff is complementary to prompt-engineering based alignment strategies. When combined with Gradient Cuff, the performance of Self-Reminder, a system prompt design method , can be increased by a large margin.

## 2 Related Work

**Jailbreak Attacks.** Existing jailbreaks can be roughly divided into feedback-based jailbreak attacks and rule-based jailbreak attacks. Feedback-based jailbreaks utilize the feedback from the target LLM to iteratively update the jailbreak prompt until the model's complies with the malicious instruction embedded in the jailbreak prompt. Feedback-based jailbreaks can be further categorized by their access mode to the target LLM. Some feedback-based jailbreak attacks like GCG , require white-box access to the target LLM. Specifically, GCG leverages gradients with respect to the one-hot token indicators to find better token choices at each position. Some feedback-based jailbreaks need gray-box access to the target LLM. The typical one is AutoDAN , which employs the target LLM's generative loss of the target response to design the fitness score of the candidate jailbreak prompt to guide further optimization. PAIR  and TAP  are the representatives of feedback-based jailbreaks which only require black-box access to the target LLM. In PAIR and TAP, there are also two LLMs taking on the attacker role and the evaluator role. At each iteration, the attacker-generated jailbreak prompt would be rated and commented on by the evaluator model according to the target LLM's response to the attack. Next, the attacker would generate new jailbreak prompts based on the evaluator's comments, and repeat the above cycle until the jailbreak prompt can get full marks from the evaluator. The only information provided by the target LLM is the response to the jailbreak attack. As for the rule-based jailbreak attacks, we highlight Base64  and Low Resource Language (LRL) . Base64 encodes the malicious instruction into base64 format and LRL translates the malicious instruction into the language that is rarely used in the training process of the target LLM, such as German, Swedish, French and Chinese.

**Jailbreak Defenses.** PPL  uses an LLM to compute the perplexity of the input query and rejects those with high perplexity. SmoothLLM , motivated by randomized smoothing , perturbs the original input query to obtain several copies and aggregates the intermediate responses of the target LLM to these perturbed queries to give the final response to the original query. Erase-Check employs a model to check whether the original query or any of its erased subsentences is harmful. The query would be rejected if the query or one of its sub-sentences is regarded as harmful by the safety checker. Another line of work [28; 31; 27; 25] use prompt engineering techniques to defend against jailbreak attacks. Notably, Self-Reminder  shows promising results by modifying the system prompt of the target LLM so that the model reminds itself to process and respond to the user in the context of being an aligned LLM. Unlike these unsupervised methods, some works like LLaMA-Guard  and Safe-Decoding  need to train an extra LLM. LLaMA-Guard trained a LLaMA-based model to determine whether the user query or model response contains unsafe content. Safe-Decoding finetuned the protected LLM on (malicious query, model refusal) pairs to get an expert LLM, and utilized the expert LLM to guide the safety-aware decoding during inference time. We will mainly focus on unsupervised methods as it is training-free and only need to deploy the protected LLM itself during inference time.

Methodology and Algorithms

Following the overview in Figure 1, in this section, we will formalize the concept of **Refusal loss function** and propose **Gradient Cuff** as a jailbreak detection method based on the unique loss landscape properties of this function observed between malicious and benign user queries.

### Refusal Loss Function and Landscape Exploration

Current transformer-based LLMs will return different responses to the same query due to the randomness of autoregressive sampling based generation [8; 10]. With this randomness, it is an interesting phenomenon that a malicious user query will sometimes be rejected by the target LLM, but sometimes be able to bypass the safety guardrail. Based on this observation, for a given LLM \(T_{}\) parameterized with \(\), we define the refusal loss function \(_{}(x)\) for a given input user query \(x\) as below:

\[_{}(x) =1-p_{}(x);\] (1) \[p_{}(x) =_{y T_{}(x)}JB(y)\] (2)

where \(y\) represents the response of \(T_{}\) to the input user query \(x\). \(JB()\) is a binary indicator function to determine whether the response triggers a refusal action by the LLM. The function \(p_{}\) can be interpreted as the expected rate of getting refusal on the response \(y\) from \(T_{}\) taking into account the randomness in the decoding process. Therefore, by our definition, the refusal loss function \(_{}(x)\) can be interpreted as the likelihood of generating a non-refusal response to \(x\). Following SmoothLLM , we define \(JB()\) as

\[JB(y)=1,y\\ 0,\]

For example, \(JB(y)\) would be \(0\) if \(y=\)"Sure, here is the python code to..." and \(JB(y)\) would be \(1\) if \(y=\)"Sorry, I cannot fulfill your request...". We discuss more details about the implementation of the indicator function in Appendix A.3.

Alternatively, we can view \(Y=JB(y)\) as a random variable obeying the Bernoulli distribution such that

\[Y=1,\ p_{}(x)\\ 0,\ 1-p_{}(x)\]

so that \(_{}(x)\) can be interpreted as the expected refusal loss:

\[_{}(x)=\ 1-[Y]=1-p_{}(x).\]

In practice, since we do not have the prior knowledge for \(p_{}(x)\), we use the sample mean \(f_{}(x)\) to approximate \(_{}(x)\):

\[f_{}(x)=1-_{i=1}^{N}\ Y_{i},\] (3)

where \(\{Y_{i}|i=1,2,...,N\}\) is obtained by running \(N\) independent realizations of the random variable \(Y\). In the \(i^{th}\) trial, we query the LLM \(T_{}\) using \(x\) to get the response \(y_{i} T_{}(x)\), and apply the indicator function \(JB()\) on \(y_{i}\) to get \(Y_{i}=JB(y_{i})\). Equation (3) can be explained as using the sample mean of the random variable \(Y\) to approximate its expected value \([Y]\).

In general, \(_{}(x)<0.5\) could be used as a naive detector to reject \(x\) since \(p_{}(x)\) can be interpreted as the probability that \(T_{}\) regards \(x\) as harmful. However, this detector alone only has limited effect against jailbreak attacks, as discussed in Section 4.3. To further explore how this refusal loss can be used to improve jailbreak detection, we visualize the refusal loss landscape following the 2-D visualization techniques from  in Figure 1 (b). From Figure 1 (b), we find that the landscape of \(f_{}()\) is more precipitous for malicious queries than for benign queries, which implies that \(f_{}()\) tends to have a large gradient norm if \(x\) represents a malicious query. This observation motivates our proposal of using the gradient norm of \(f_{}()\) to detect jailbreak attempts that pass the initial filtering of rejecting \(x\) when \(f_{}(x)<0.5\).

### Gradient Norm Estimation

In general, the exact gradient of \(_{}(x)\) (or \(f_{}(x)\)) is infeasible to obtain due to the existence of discrete operations such as applying the \(JB()\) function to the generated response, and the possible involvement of black-box evaluation functions (e.g., Perspective API). We propose to use zeroth order gradient estimation to compute the approximate gradient norm, which is widely used in black-box optimization with only function evaluations (zeroth order information) [3; 16]. Similar gradient estimation techniques were used to generate adversarial examples from black-box models [5; 11; 6].

A zeroth-order gradient estimator approximates the exact gradient by evaluating and computing the function differences with perturbed continuous inputs. Our first step is to obtain the sentence embedding of \(x\) in the embedding space of \(T_{}\) in \(^{d}\). For each text query \(x\) with \(n\) words (tokens) in it, it can be embedded into a matrix \(e_{}(x)^{n d}\) where \(e_{}(x)_{i}^{d}\) denotes the word embedding for the \(i^{th}\) word in sentence \(x\). We define the sentence embedding for \(x\) by applying mean pooling to \(e_{}(x)\) defined as

\[(x)=_{i=1}^{n}e_{}(x)_{i}\] (4)

With the observation that

\[(x)+=_{i=1}^{n}(e_{}(x)_ {i}+),\] (5)

one can obtain a perturbed sentence embedding of \(x\) with any perturbation \(\) by equivalently perturbing the word embedding of each word in \(x\) with the same \(\).

Based on this definition, we approximate the exact gradient \(_{}(x)\) by \(g_{}(x)\), which is the estimated gradient of \(f_{}(x)\). Following [3; 16], we calculate \(g_{}(x)\) using the directional derivative approximation

\[g_{}(x)=_{i=1}^{P}(_{}(x) _{i})-f_{}(_{}(x))}{}_{i},\] (6)

where \(_{i}\) is a \(d\) dimension random vector drawn from the standard multivariate normal distribution, i.e., \(_{i}(,)\), \(\) is a smoothing parameter, \(\) denotes the row-wise broadcasting add operation that adds the same vector \(_{i}\) to every row in \(_{}(x)\).

Based on the definitions in Equation (3) and Equation (6), we provide a probabilistic guarantee below for analyzing the gradient approximation error of the true gradient \(_{}()\).

**Theorem 1**: _Let \(\|\|\) denote a vector norm and assume \(_{}(x)\) is \(L\)-Lipschitz continuous. With probability at least \(1-\), the approximation error of \(_{}(x)\) satisfies_

\[\|g_{}(x)-_{}(x)\|\]

_for some \(>0\), where \(=^{1}(+)\) and \(=(})\)._

This theorem demonstrates that one can reduce the approximation error by taking larger values for \(N\) and \(P\). We provide the proof in Appendix A.12. Experimental results in Appendix A.14 and Appendix A.13 also provide empirical evidence to support this theorem by demonstrating the scaling performance of Gradient Cuff with increased total queries.

### Gradient Cuff: Two-step jailbreak detection

With the discussions in Section 3.1 and Section 3.2, we now formally propose Gradient Cuff, a two-step jailbreak detection method based on checking the refusal loss and its gradient norm. Our detection procedure is shown in Figure 1 (c). Gradient Cuff can be summarized into two steps:

* **(Step 1) Sampling-based Rejection:** In the first step, we reject the user query \(x\) by checking whether \(f_{}(x)<0.5\). If true, then \(x\) is rejected, otherwise, \(x\) is pushed into Step 2.

* **(Step 2) Gradient Norm Rejection:** In the second step, we regard \(x\) as having jailbreak attempts if the norm of the estimated gradient \(g_{}(x)\) is larger than a configurable threshold \(t\), i.e., \(\|g_{}(x)\|>t\).

Before deploying Gradient Cuff on LLMs, we first test it on a bunch of benign user queries to select a proper threshold \(t\) that fulfills the required benign refusal rate (that is, the false positive rate \(\)). We use a user-specified \(\) value (e.g., 5%) to guide the selection of the threshold \(t\) so that the total refusal rate on the benign validation dataset \(_{val}\) won't exceed \(\).

We summarize our method in Algorithm 1. The algorithm is implemented by querying the LLM \(T_{}\) multiple times, each to generate a response for the same input query \(x\). The total query times to \(T_{}\) required to compute \(f_{}(x)\) and \(g_{}(x)\) in Gradient Cuff is at most \(q=N(P+1)\). To maintain the LLM's efficiency, we also explored the use of batch inference to compute these queries in parallel, thereby reducing the total running cost of the LLM. For example, the running time can only be increased by 1.3\(\) when the total query times were 10\(\) of the original. See detailed discussion in Appendix A.15.

## 4 Experiments

### Experiment Setup

**Malicious User Queries**. We sampled 100 harmful behavior instructions from AdvBench2 in  as jailbreak templates, each to elicit the target LLM to generate certain harmful responses. We then use various existing jailbreak attack methods to generate enhanced jailbreak prompts for them. Specifically, for each harmful behavior instruction, we use GCG  to generate a universal adversarial suffix, use AutoDAN , PAIR , and TAP  to generate a new instruction, use LRL  to translate it into low source languages that rarely appear in the training phase of the target LM such as German, Swedish, French and Chinese, and use Base64  to encode them in base64 format. See Appendix A.2 for more details on generating jailbreak prompts. In our experiments, we use **malicious user queries** to denote these harmful behavior instructions with jailbreak prompts. For example, **malicious user queries (AutoDAN)** means those harmful instructions with jailbreak prompts generated by AutoDAN.

**Benign User Queries**. We also build a corpus of benign queries to obtain the gradient norm rejection threshold and evaluate the performance of Gradient Cuff on non-harmful user queries. We collect benign user queries from the LMSYS Chatbot Arena leaderboard 3, which is a crowd-sourced open platform for LLM evaluation. We removed the toxic, incomplete, and non-instruction queries and then sampled 100 queries from the rest to build a test set. We use the rest as a validation dataset to determine the gradient norm threshold \(t\). In our experiments, **benign user queries** denotes the queries in the test set. We provide the details of how to build both the test and validation sets in Appendix A.1.

**Aligned LLMs.** We conduct the jailbreak experiments on 2 aligned LLMs: LLAMA-2-7B-Chat  and Vicuna-7B-V1.5 . LLAMA-2-7B-Chat is the aligned version of LLAMA-2-7B. Vicuna-7B-V1.5 is also based on LLAMA2-7B and has been further supervised fine-tuned on 70k user-assistant conversations collected from ShareGPT4. We use **protected LLM** to represent these two models in the experiments.

**Defense Baselines.** We compare our method with various jailbreak defense methods including PPL , Erase-check , SmoothLLM , and Self-Reminder . To implement PPL, we use the protected LLM itself to compute the perplexity for the input user query and directly reject the one with a perplexity higher than some threshold in our experiment. For Erase-Check, we employ the LLM itself to serve as a safety checker to check whether the input query or any of its erased sub-sentences is harmful. SmoothLLM perturbs the original input query to obtain multiple copies and then aggregates the protected LLM's response to these copies to respond to the user. Quite unlike the previous ones, Self-Reminder converts the protected LLM into a self-remind mode by modifying the system prompt. Though we mainly focus on unsupervised methods, we also conclude comparisons with two supervised methods: LLaMA-Guard  and Safe-Decoding . We use the LLaMA-Guard-2-8B to implement LLaMA-Guard. For more details on the implementation of these baselines, please refer to Appendix A.8.

**Metrics.** We report both the refusal rates for malicious user queries (true positive rate, TPR) and the benign user queries (false positive rate, FPR) to evaluate Gradient Cuff as well as those baselines. Higher TPR and lower FPR indicate better performance. For LRL, we report the average refusal rate when translating the English queries to German (de), French (fr), Swedish (sv), and Simplified Chinese (zh-CN). Details about computing the refusal rate are given in Appendix A.5.

**Implementation of Gradient Cuff.** We use \(=0.02,N=P=10\) in our main experiments and report the results when \(\) (FPR) is set to \(5\%\). For the text generation setting, we use \(=0.6,=0.9\) for both LLaMA2-7B-Chat and Vicuna-7B-V1.5, and adopt Nucleus Sampling. As for the system prompt, we use the default setting provided in the fastchat repository . All our experiments are run on a single NVIDIA A800 GPU with 80G of memory. We run each experiment with 5 random seeds: \(13,21,42,87,100\) and report the mean value.

### Performance Evaluation and Comparison with Existing Methods

We begin by evaluating our method as well as all the baselines except Self-Reminder against 6 different jailbreak attacks (GCG, AutoDAN, PAIR, TAP, Base64, and LRL) and benign user queries. We report the average refusal rate across these 6 malicious user query datasets as True Positive Rate (TPR) and the refusal rate on benign user queries as False Positive Rate (FPR). From Figure 2 we can summarize that Gradient Cuff stands out on both benign queries and malicious queries, attaining high TPR and low FPR. Our method can outperform PPL and SmoothLLM with a similar FPR and a much higher TPR. Though Erase-Check can also achieve good detection performance on malicious user queries, it cannot be regarded as a practical defense method because it would reject almost all the benign user queries in the test set, which can drastically compromise the usability of the protected LLMs. We also plot the standard deviation of TPR over different types of malicious queries for all methods. The results shown in Figure 1(a) and 1(b) demonstrate that our method has the most balanced performance across all types of jailbreaks considered in this paper. Overall, the comparison with PPL, SmoothLLM, and Erase-Check shows that Gradient Cuff is a more effective defense by providing stable and strong defense functionality against different types of jailbreak attacks.

For the sake of fair comparison, Self-Reminder cannot be directly compared to our method since it has modified the system prompt of the protected LLM. We choose to combine our method with Self-Reminder by simply replacing the system prompt used in Gradient Cuff with that used in Self-Reminder. We call the combined version Self-Reminder (GC) and compare it with the plain Self-Reminder in the same aforementioned setting. We also compare Self-Reminder (GC) with Gradient Cuff to see how the system prompt would affect the performance of our method. To simplify

Figure 2: Performance evaluation on LLaMA2-7B-Chat (a) and Vicuna-7B-V1.5 (b). The horizon axis represents the refusal rate of benign user queries (FPR), and the vertical axis shows the average refusal rate across 6 malicious user query datasets (TPR). The error bar shows the standard deviation between the refusal rate of these 6 jailbreak datasets. We also report the MMLU accuracy of Low-FPR methods to show their utility. Complete results can be found in Appendix A.9.

the comparison, we set \(\) as the benign refusal rate of the original Self-Reminder when implementing Self-Reminder (GC) and Gradient Cuff. The overall results are shown in Table 1.

From the comparison, we can conclude that Gradient Cuff can significantly enhance Self-Reminder. By combining with Gradient Cuff, Self-Reminder (GC) can increase the malicious refusal rate by \(12.20\%\) on LLaMA-2-7B-Chat and \(\) on Vicuna-7B-V1.5. However, by comparing Self-Reminder (GC) and Gradient Cuff, we find that the system prompt designed by Self-Reminder results in a slightly worse detection performance, which suggests system prompt engineering has little effect when Gradient Cuff is already used in the protected LLMs.

For completeness, we compared with two supervised methods: LLaMA-Guard and Safe Decoding. Table 2 shows that LLaMA-Guard achieves comparable TPR results with Gradient Cuff on Vicuna-7B-V1.5 but a much lower TPR performance on LLaMA2-7B-Chat. Though Safe-Decoding achieves the largest average TPR against jailbreak attacks, its FPR is much higher, bringing notable utility degradation. Though LLaMA-Guard is a model-agnostic method, it shows model-specific results because we use different jailbreak prompts to evaluate it on different LLMs. The experimental results showed that the Gradient Cuff consistently stands out even when compared with supervised methods.

### Effectiveness of Gradient Norm Rejection

We validate the effectiveness and necessity of using Gradient Norm Rejection as the second detection stage in Gradient Cuff by comparing the performance between Gradient Cuff and Gradient Cuff (w/o 2nd stage). Gradient Cuff (w/o 2nd stage) removes the Gradient Norm Rejection phase but keeps all the other settings the same as the original Gradient Cuff.

From Table 3, we can find that by adding the second stage and setting \(\) to 1%, the TPR can be improved by a large margin (+0.099 on LLaMA2 and +0.240 on Vicuna), while the FPR is almost not changed (+0.000 on LLaMA2 and +0.002 on Vicuna). When we adjust the threshold in stage 2 by changing the \(\) value from 1% to 5%, the performance gains in TPR can be further improved. These results verify the effectiveness of the Gradient Norm Rejection step in Graient Cuff.

### Adaptive Attack

Adaptive attack is a commonly used evaluation scheme for defenses against adversarial attacks  with the assumption that the defense mechanisms are transparent to an attacker. Some studies on jailbreak defense also test their method against adaptive attacks [21; 28]. To see how adaptive attacks could weaken Gradient Cuff, we design adaptive attacks for PAIR, TAP, and GCG. Specifically, we design Adaptive-PAIR, Adaptive-TAP, and Adaptive-GCG to jailbreak protected LLMs equipped with

 
**Language Model** & **Defense Method** & **FPR** & **TPR** \\   & Self-Reminder & \(0.134 0.034\) & \(0.820 0.016\) \\  & Self-Reminder (GC) & \(0.126 0.033\) & \(0.920 0.011\)\(@paragraphsign\) \\  & Gradient Cuff & \(0.070 0.026\) & \(0.968 0.007\)\(@paragraphsign\) \\   & Self-Reminder & \(0.034 0.018\) & \(0.472 0.020\) \\  & Self-Reminder (GC) & \(0.044 0.021\) & \(0.637 0.020\)\(@paragraphsign\) \\  & Gradient Cuff & \(0.026 0.016\) & \(0.665 0.019\)\(@paragraphsign\) \\  

Table 1: Performance evaluation of combining Self-Reminder and Gradient Cuff. \(@paragraphsign\) and \(@paragraphsign\) mean the largest and the second largest TPR, respectively.

 
**Language Model** & **Defense Method** & **FPR** & **TPR** \\   & Gradient Cuff & \(0.022 0.015\) & \(0.883 0.013\) \\  & LLaMA-Guard & \(0.040 0.020\) & \(0.760 0.017\) \\  & Safe-Decoding & \(0.080 0.027\) & \(0.955 0.008\) \\   & Gradient Cuff & \(0.034 0.018\) & \(0.743 0.018\) \\  & LLaMA-Guard & \(0.040 0.020\) & \(0.773 0.017\) \\   & Safe-Decoding & \(0.280 0.045\) & \(0.880 0.013\) \\  

Table 2: Performance comparison with supervised methods.

Gradient Cuff. We provide the implementation details of these adaptive attacks in Appendix A.10. All adaptive attacks are tested by Gradient Cuff with the same benign refusal rate (\(=5\%\)).

As shown in Table 4, Gradient Cuff is robust to Adaptive-GCG attack while the performance can be mildly reduced by Adaptive PAIR and Adaptive TAP, especially when defending against Adaptive-PAIR on Vicuna-7B-V1.5, where the malicious refusal rate drops from \(0.694\) to \(0.356\).

We further compare our method with other defense baselines. Figure 3 shows that our method is the best defense in terms of the average refusal rate on malicious queries. On Vicuna-7B-V1.5, Gradient Cuff outruns SmoothLLM and PPL by \(91.4\%\) and \(81.6\%\) against Adaptive-PAIR while outperforming SmoothLLM and PPL by \(52.7\%\) and \(47.9\%\) against Adaptive-TAP. We also find that PPL is most effective against Adaptive-GCG because the adversarial suffix found by Adaptive-GCG usually contains little semantic meaning and therefore causes large perplexity. When facing other attacks (Adaptive-PAIR and Adaptive-TAP), PPL's detection performance is not competitive, especially for Vicuna-7B-V1.5. In Appendix A.11, we validated the effect of adaptive attacks against Gradient Cuff by showing that they intended to decrease the norm of the refusal loss gradient.

### Utility Analysis

In addition to the demonstrated improved defense capability, we further study how Gradient Cuff would affect the utility of the protected LLM. We compare the zero-shot performance of the Vicuna pair (Vicuna-7B-V1.5 & Vicuna-7B-V1.5 with Gradient Cuff) and the LLaMA-2 pair (LLaMA-2-7b-Chat & LLaMA-2-7b-Chat with Gradient Cuff) on the Massive Multitask Language Understanding (MMLU) benchmark . Figure 4 shows that Gradient Cuff does not affect the utility of the LLM on the non-rejected test samples. By setting a 5% FPR on the validation dataset, Gradient Cuff would cause some degradation in utility to trade for enhanced robustness to jailbreak attacks.

We also report the utility for existing baselines in Table 5. We find that though SmoothLLM can achieve a very low FPR as shown in Figure 2, it causes a dramatically large utility degradation

   &  &  \\   & & w/o & w/ \\   & PAIR & \(0.770 0.042\) & \(0.778 0.042\) \\  & TAP & \(0.950 0.022\) & \(0.898 0.030\) \\  & GCG & \(0.988 0.011\) & \(0.986 0.012\) \\   & PAIR & \(0.694 0.046\) & \(0.356 0.048\) \\  & TAP & \(0.570 0.050\) & \(0.562 0.050\) \\   & GCG & \(0.892 0.031\) & \(0.880 0.032\) \\  

Table 4: Performance evaluation under adaptive attacks. The reported value is Gradient Cuffâ€™s refusal rate against the corresponding jailbreak attack.

   &  & **TPR** & **MMLU Accuracy** \\   & Gradient Cuff & \(0.883 0.013\) & \(0.378\) \\  & SmoothLLM & \(0.763 0.017\) & 0.271 \\  & PPL & \(0.732 0.018\) & 0.411 \\   & Gradient Cuff & \(0.743 0.018\) & 0.462 \\  & SmoothLLM & \(0.413 0.020\) & 0.303 \\   & PPL & \(0.401 0.020\) & 0.487 \\  

Table 5: Robustness-Utility evaluation on MMLU benchmark.

   &  & **FPR** & **TPR** \\   & Gradient Cuff (w/o 2nd stage) & \(0.012 0.011\) & \(0.698 0.019\) \\  & Gradient Cuff (\(=1\%\)) & \(0.012 0.011\) & \(0.797 0.016\) \\  & Gradient Cuff (\(=5\%\)) & \(0.022 0.015\) & \(0.883 0.013\) \\   & Gradient Cuff (w/o 2nd stage) & \(0.008 0.009\) & \(0.296 0.019\) \\  & Gradient Cuff (\(=1\%\)) & \(0.010 0.010\) & \(0.536 0.020\) \\   & Gradient Cuff (\(=5\%\)) & \(0.034 0.018\) & \(0.743 0.018\) \\  

Table 3: Performance evaluation of Gradient Cuff and Gradient Cuff (w/o 2nd stage). We remove the second stage or adjust the detection threshold of the 2nd stage to show its significance.

because it has modified the user query, inevitably compromising the semantics of the query. PPL attains the best utility and Gradient Cuff achieves the best performance-utility trade-off by **(a)** keeping the comparable utility with PPL **(b)** attaining a much higher TPR than the best baselines (e.g., 0.743 vs 0.413 on Vicuna-7B-V1.5) against jailbreak attacks.

## 5 Discussion

**Extra Inference Cost**. Experimental results in Appendix A.16 show that our method achieves the most effective trade-off between performance and inference efficiency when doing jailbreak defense. We also think this trade-off is inevitable (though it could be improved) yet acceptable, as users may be less incentivized to use a model/service if it does not have proper safety guardrails.

**Jailbroken Assessment**. Existing studies often rely on checking whether an LLM's response contains certain predefined keywords or phrases to assess the jailbroken. However, this method has obvious limitations, as it is difficult to create an exhaustive list of phrases that could cover all possible jailbreaking scenarios. Consequently, we need a more reliable method to accurately identify successful jailbreaking attempts. In Appendix A.6, we use GPT-4 and LLaMA-2-Guard-8B to compute the AS. The results are consistent with the keyword-based ASR evaluations.

**Application on close-sourced LLMs**. When implementing our detection method, we adopt the white-box settings assuming the model weights and internal representations are available to the defender. Gradient Cuff is applicable to close-sourced LLMs if it is deployed by the model developer who has full access to the model's parameters including the embedding layers.

## 6 Conclusion

In this paper, we define and study the refusal loss function of an LLM to exploit its discriminative functional properties for designing an effective jailbreak detection method called Gradient Cuff. Gradient Cuff features a two-step jailbreak detection procedure that sequentially checks the refusal loss landscape's functional value and gradient norm. Our experiments on 2 aligned LLMs (LLaMA-2-7b-Chat and Vicuna-7B-V1.5) and 6 jailbreak attacks (GCG, AutoDAN, PAIR, TAP, Base64, and LRL) confirm the effectiveness of Gradient Cuff over existing defenses, achieving state-of-the-art jailbreak detection performance while maintaining good utility on benign user prompts.

Figure 4: Utility evaluation on MMLU  (zero-shot) with and without Gradient Cuff.

Figure 3: Performance comparison against adaptive jailbreak attacks.