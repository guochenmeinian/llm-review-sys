# Uncertainty-based Offline Variational Bayesian Reinforcement Learning for Robustness under Diverse Data Corruptions

Uncertainty-based Offline Variational Bayesian Reinforcement Learning for Robustness under Diverse Data Corruptions

Rui Yang\({}^{1,2}\), Jie Wang\({}^{1,2}\), Guoping Wu\({}^{1}\), Bin Li\({}^{1}\)

\({}^{1}\)University of Science and Technology of China

\({}^{2}\)MoE Key Laboratory of Brain-inspired Intelligent Perception and Cognition

{yr0013, guoping}@mail.ustc.edu.cn

{jiewangx, binli}@ustc.edu.cn

Corresponding author. Email: jiewangx@ustc.edu.cn.

###### Abstract

Real-world offline datasets are often subject to data corruptions (such as noise or adversarial attacks) due to sensor failures or malicious attacks. Despite advances in robust offline reinforcement learning (RL), existing methods struggle to learn robust agents under high uncertainty caused by the diverse corrupted data (i.e., corrupted states, actions, rewards, and dynamics), leading to performance degradation in clean environments. To tackle this problem, we propose a novel robust variational Bayesian inference for offline **RL** (TRACER). It introduces Bayesian inference for the first time to capture the uncertainty via offline data for robustness against all types of data corruptions. Specifically, TRACER first models all corruptions as the uncertainty in the action-value function. Then, to capture such uncertainty, it uses all offline data as the observations to approximate the posterior distribution of the action-value function under a Bayesian inference framework. An appealing feature of TRACER is that it can distinguish corrupted data from clean data using an entropy-based uncertainty measure, since corrupted data often induces higher uncertainty and entropy. Based on the aforementioned measure, TRACER can regulate the loss associated with corrupted data to reduce its influence, thereby enhancing robustness and performance in clean environments. Experiments demonstrate that TRACER significantly outperforms several state-of-the-art approaches across both individual and simultaneous data corruptions.

## 1 Introduction

Offline reinforcement learning (RL) aims to learn an effective policy from a fixed dataset without direct interaction with the environment . This paradigm has recently attracted much attention in scenarios where real-time data collection is expensive, risky, or impractical, such as in healthcare , autonomous driving , and industrial automation . Due to the restriction of the dataset, offline RL confronts the challenge of distribution shift between the policy represented in the offline dataset and the policy being learned, which often leads to the overestimation for out-of-distribution (OOD) actions . To address this challenge, one of the promising approaches introduce uncertainty estimation techniques, such as using the ensemble of action-value functions or Bayesian inference to measure the uncertainty of the dynamics model  or the action-value function  regarding the rewards and transition dynamics. Therefore, they can constrain the learned policy to remain close to the policy represented in the dataset, guiding the policy to be robust against OOD actions.

Nevertheless, in the real world, the dataset collected by sensors or humans may be subject to extensive and diverse corruptions [16; 17; 18], e.g., random noise from sensor failures or adversarial attacks during RLHF data collection. Offline RL methods often assume that the dataset is clean and representative of the environment. Thus, when the data is corrupted, the methods experience performance degradation in the clean environment, as they often constrain policies close to the corrupted data distribution.

Despite advances in robust offline RL , these approaches struggle to address the challenges posed by diverse data corruptions . Specifically, many previous methods on robust offline RL aim to enhance the testing-time robustness, learning from clean datasets and defending against attacks during testing [19; 20; 21]. However, they cannot exhibit robust performance using offline dataset with perturbations while evaluating the agent in a clean environment. Some related works for data corruptions (also known as _corruption-robust offline RL_ methods) introduce statistical robustness and stability certification to improve performance, but they primarily focus on enhancing robustness against adversarial attacks [16; 22; 23]. Other approaches focus on the robustness against both random noise and adversarial attacks, but they often aim to address only corruptions in states, rewards, or transition dynamics [24; 17]. Based on these methods, recent work  extends the data corruptions to all four elements in the dataset, including states, actions, rewards, and dynamics. This work demonstrates the superiority of the supervised policy learning scheme [25; 26] for the data corruption of each element in the dataset. However, as it does not take into account the uncertainty in decision-making caused by the simultaneous presence of diverse corrupted data, this work still encounters difficulties in learning robust agents, limiting its applications in real-world scenarios.

In this paper, we propose to use offline data as the observations, thus leveraging their correlations to capture the uncertainty induced by all corrupted data. Considering that (1) diverse corruptions may introduce uncertainties into all elements in the offline dataset, and (2) each element is correlated with the action values (see dashed lines in Figure 1), there is high uncertainty in approximating the action-value function by using various corrupted data. To address this high uncertainty, we propose to leverage all elements in the dataset as observations, based on the graphical model in Figure 1. By using the high correlations between these observations and the action values , we can accurately identify the uncertainty of the action-value function.

Motivated by this idea, we propose a robust variational Bayesian inference for offline **RL** (TRACER) to capture the uncertainty via offline data against all types of data corruptions. Specifically, TRACER first models all data corruptions as uncertainty in the action-value function. Then, to capture such uncertainty, it introduces variational Bayesian inference , which uses all offline data as observations to approximate the posterior distribution of the action-value function. Moreover, the corrupted observed data often induce higher uncertainty than clean data, resulting in higher entropy in the distribution of action-value function. Thus, TRACER can use the entropy as an uncertainty measure to effectively distinguish corrupted data from clean data. Based on the entropy-based uncertainty measure, it can regulate the loss associated with corrupted data in approximating the action-value distribution. This approach effectively reduces the influence of corrupted samples, enhancing robustness and performance in clean environments.

This study introduces Bayesian inference into offline RL for data corruptions. It significantly captures the uncertainty caused by diverse corrupted data, thereby improving both robustness and performance in offline RL. Moreover, it is important to note that, unlike traditional Bayesian online and offline RL methods that only model uncertainty from rewards and dynamics [29; 30; 31; 32; 33; 34; 35], our approach identifies the

Figure 1: Graphical model of decision-making process. Nodes connected by solid lines denote data points in the offline dataset, while the Q values (i.e., action values) connected by dashed lines are not part of the dataset. These Q values are often objectives that offline algorithms aim to approximate.

uncertainty of the action-value function regarding states, actions, rewards, and dynamics under data corruptions. We summarize our contributions as follows.

* To the best of our knowledge, this study introduces Bayesian inference into corruption-robust offline RL for the first time. By leveraging all offline data as observations, it can capture uncertainty in the action-value function caused by diverse corrupted data.
* By introducing an entropy-based uncertainty measure, TRACER can distinguish corrupted from clean data, thereby regulating the loss associated with corrupted samples to reduce its influence for robustness.
* Experiment results show that TRACER significantly outperforms several state-of-the-art offline RL methods across a range of both individual and simultaneous data corruptions.

## 2 Preliminaries

Bayesian RL.We consider a Markov decision process (MDP), denoted by a tuple \(=(,,,P,P_{0},)\), where \(\) is the state space, \(\) is the action space, \(\) is the reward space, \(P(|s,a)()\) is the transition probability distribution over next states conditioned on a state-action pair \((s,a)\), \(P_{0}()()\) is the probability distribution of initial states, and \([0,1)\) is the discount factor. Note that \(()\) and \(()\) denote the sets of probability distributions on subsets of \(\) and \(\), respectively. For simplicity, throughout the paper, we use uppercase letters to refer to random variables and lowercase letters to denote values taken by the random variables. Specifically, \(R(s,a)\) denotes the random variable of one-step reward following the distribution \((r|s,a)\), and \(r(s,a)\) represents a value of this random variable. We assume that the random variable of one-step rewards and their expectations are bounded by \(R_{}\) and \(r_{}\) for any \((s,a)\), respectively.

Our goal is to learn a policy that maximizes the expected discounted cumulative return:

\[^{*}=*{arg\,max}_{()}_{s _{0} P_{0},a_{t}(|s_{t}),R(|s_{t},a_{t}),s_{t+1}  P(|s_{t},a_{t})}[_{t=0}^{}^{t}R(s_{t},a_{t}) ].\]

Based on the return, we can define the value function as \(V^{}(s)=_{,,P}[_{t=0}^{}^{t}R(s_{t},a _{t})|s_{0}=s]\), the action-value function as \(Q^{}(s,a)=_{R(|s,a),s^{} P(|s,a)} [R(s,a)+ V^{}(s^{})]\), and the _action-value distribution_ as

\[D^{}(s,a)=_{t=0}^{}^{t}R(s_{t},a_{t}|s_{0}=s,a_{0}=a),\ \ \ s_{t+1} P(|s_{t},a_{t}),a_{t+1}(|s_{t+1}).\] (1)

Note that \(V^{}(s)=_{a}[Q^{}(s,a)]=_{a,}[D^{}(s,a)]\).

Variational Inference.Variational inference is a powerful method for approximating complex posterior distributions, which is effective for RL to handle the parameter uncertainty and deal with modelling errors . Given an observation \(X\) and latent variables \(Z\), Bayesian inference aims to compute the posterior distribution \(p(Z|X)\). Direct computation of this posterior is often intractable due to the high-dimensional integrals involved. To approximate the true posterior, Bayesian inference introduces a parameterized distribution \(q(Z;)\) and minimizes the Kullback-Leibler (KL) divergence \(_{}(q(Z;)\|p(Z|X))\). Note that minimizing the KL divergence is equivalent to maximizing the evidence lower bound (ELBO) [37; 38]: \(()=_{q(;)}[ p(, )- q(;)]\).

Offline RL under Diverse Data Corruptions.In the real world, the data collected by sensors or humans may be subject to diverse corruption due to sensor failures or malicious attacks. Let \(b\) and \(\) denotes the uncorrupted and corrupted dataset with samples \(\{(s^{i}_{t},a^{i}_{t},r^{i}_{t},s^{i}_{t+1})\}_{i=1}^{N}\), respectively. Each data in \(\) may be corrupted. We assume that an uncorrupted state follows a state distribution \(p_{b}()\), a corrupted state follows \(p_{S}()\), an uncorrupted action follows a behavior policy \(_{b}(|s^{i}_{t})\), a corrupted action is sampled from \(_{}(|s^{i}_{t})\), a corrupted reward is sampled from \(_{}(|s^{i}_{t},a^{i}_{t})\), and a corrupted next state is drawn from \(P_{}(|s^{i}_{t},a^{i}_{t})\). We also denote the uncorrupted and corrupted empirical state-action distributions as \(p_{b}(s^{i}_{t},a^{i}_{t})\) and \(p_{}(s^{i}_{t},a^{i}_{t})\), respectively. Moreover, we introduce the notations [39; 18] as follows.

\[}Q(s,a)=(s,a)+_{s^{}  P_{}(|s,a)}[V(s^{})],(s,a) =_{r_{}(|s,a)}[r],\] (2) \[}D(s,a)R(s,a)+ D(s^{ },a^{}), s^{} P_{}(|s,a),\ a^{ }_{}(|s),\] (3)for any \((s,a)\) and \(Q:[0,r_{}/(1-)]\), where \(X:}{{=}}Y\) denotes equality of probability laws, that is the random variable \(X\) is distributed according to the same law as \(Y\).

To address the diverse data corruptions, based on IQL , RIQL  introduces quantile estimators with an ensemble of action-value functions \(\{Q_{_{i}}(s,a)\}_{i=1}^{K}\) and employs a Huber regression :

\[_{Q}(_{i})=_{(s,a,r,s^{ })}[l_{H}^{}(r+ V_{}(s^{ })-Q_{_{i}}(s,a))],\;l_{H}^{}(x)= x^{2},&|x|\\ |x|-,&|x|>,\] (4) \[_{V}()=_{(s,a)}[ _{2}^{}(Q_{}(s,a)-V_{}(s))], _{2}^{}(x)=|-(x<0)| x^{2}.\] (5)

Note that \(l_{H}^{}\) is the Huber loss, and \(Q_{}\) is the \(\)-quantile value among \(\{Q_{_{i}}(s,a)\}_{i=1}^{K}\). RIQL then follows IQL  to learn the policy using weighted imitation learning with a hyperparameter \(\):

\[_{}()=_{(s,a)}[( A _{}(s,a))_{}(a|s)], A_{}(s,a)=Q_{}(s,a )-V_{}(s).\] (6)

## 3 Algorithm

We first introduce the Bayesian inference for capturing the uncertainty caused by diverse corrupted data in Section 3.1. Then, we provide our algorithm TRACER with the entropy-based uncertainty measure in Section 3.2. Moreover, we provide the theoretical analysis for robustness, the architecture, and the detailed implementation of TRACER in Appendices A.1, B.1, and B.2, respectively.

### Variational Inference for Uncertainty induced by Corrupted Data

We focus on corruption-robust offline RL to learn an agent under diverse data corruptions, i.e., random or adversarial attacks on four elements of the dataset. We propose to use all elements as observations, leveraging the data correlations to simultaneously address the uncertainty. By introducing Bayesian inference framework, our aim is to approximate the posterior distribution of the action-value function.

At the beginning, based on the relationships between the action values and the four elements (i.e., states, actions, rewards, next states) in the offline dataset as shown in Figure 1, we define \(D_{}=D_{}(S,A,R) p_{}(|S,A,R)\), parameterized by \(\). Building on the action-value distribution, we can explore how to estimate the posterior of \(D_{}\) using the elements available in the offline data.

Firstly, we start from the actions \(\{a_{t}^{i}\}_{i=1}^{N}\) following the corrupted distribution \(_{}\) and use them as observations to approximate the posterior of the action-value distribution under a variational inference. As the actions are correlated with the action values and all other elements in the dataset, the likelihood is \(p_{_{a}}(A|D,S,R,S^{})\), parameterized by \(_{a}\). Then, under the variational inference framework, we maximize the posterior and derive to minimize the loss function based on ELBO:

\[_{D|A}(,_{a})=_{,p_{}} [_{}p_{_{a}}(A|D_{},S,R,S^{ })\,\|\,_{}(A|S)-_{A p_{_{a} }}[ p_{}(D_{}|S,A,R)]],\] (7)

where \(S\), \(R\), and \(S^{}\) follow the offline data distributions \(p_{}\), \(_{}\), and \(P_{}\), respectively.

Secondly, we apply the rewards \(\{r_{t}^{i}\}_{i=1}^{N}\) drawn from the corrupted reward distribution \(_{}\) as the observations. Considering that the rewards are related to the states, actions, and action values, we model the likelihood as \(p_{_{r}}(R|D,S,A)\), parameterized by \(_{r}\). Therefore, we can derive a loss function by following Equation (7):

\[_{D|R}(,_{r})=_{,p_{}} [_{}p_{_{r}}(R|D_{},S,A)\,\|\, _{}(R|S,A)-_{R p_{_{r}}}[ p_{ }(D_{}|S,A,R)]],\] (8)

where \(S\) and \(A\) follow the offline data distributions \(p_{}\) and \(_{}\), respectively.

Finally, we employ the state \(\{s_{t}^{i}\}_{i=1}^{N}\) in the offline dataset following the corrupted distribution \(p_{}\) as the observations. Due to the relation of the states, we can model the likelihood as \(p_{_{s}}(S|D,A,R)\), parameterized by \(_{r}\). We then have the loss function:

\[_{D|S}(,_{s})=_{,p_{}} [_{}p_{_{s}}(S|D_{},A,R)\,\|\,p _{}(S)-_{S p_{_{s}}}[ p_{} (D_{}|S,A,R)]],\] (9)

where \(A\) and \(R\) follow the offline data distributions \(_{}\) and \(_{}\), respectively. We present the detailed derivation process in Appendix A.2.

The goal of first terms in Equations (7), (8), and (9) is to estimate \(_{}(A|S)\), \(_{}(R|S,A)\), and \(p_{}(S)\) using \(p_{_{a}}(A|D_{},S,R,S^{})\), \(p_{_{r}}(R|D_{},S,A)\), and \(p_{_{s}}(S|D_{},A,R)\), respectively. As we do not have the explicit expression of distributions \(_{}\), \(_{}\), and \(p_{}\), we cannot directly compute the KL divergence in these first terms. To address this issue, based on the generalized Bayesian inference , we can exchange two distributions in the KL divergence. Then, we model all the aformentioned distributions as Gaussian distributions, and use the mean \(_{}\) and standard deviation \(_{}\) to represent the corresponding \(p_{}\). For implementation, we directly employ MLPs to output each \((_{},_{})\) using the corresponding conditions of \(p_{}\). Then, based on the KL divergence between two Gaussian distributions, we can derive the loss function as follows.

\[_{}(,_{s},_{a}, _{r})=_{(s,a,r),D_{} p_{} }[(_{_{a}}-a)^{T}_{_{a}}^{-1}(_{_{a}}-a) +(_{_{r}}-r)^{T}_{_{r}}^{-1}(_{_{r}}-r).\] \[.+(_{_{s}}-s)^{T}_{_{s}}^{-1}(_{ _{s}}-s)+|_{_{a}}||_{ _{r}}||_{_{s}}|],\] (10)

Moreover, the goal of second terms in Equations (7), (8), and (9) is to maximize the likelihoods of \(D_{}\) given samples \( p_{_{s}}\), \( p_{_{a}}\), or \( p_{_{r}}\). Thus, with \((s,a,r)\), we propose minimizing the distance between \(D_{}(,a,r)\) and \(D(s,a,r)\), \(D_{}(s,,r)\) and \(D(s,a,r)\), and \(D_{}(s,a,)\) and \(D(s,a,r)\), where \( p_{_{s}}\), \( p_{_{a}}\), and \( p_{_{r}}\). Then, based on , we can derive the following loss with any metric \(\) to maximize the log probabilities:

\[_{}(,_{s},_{a}, _{r})=_{(s,a,r), p_{_{s}},  p_{_{a}}, p_{_{r}},D p}[D(s,a,r),D_{}(s,,r).\] \[.+\ D(s,a,r),D_{}(s,a,)+ D(s,a,r),D_{}(,a,r)].\] (11)

### Corruption-Robust Algorithm with the Entropy-based Uncertainty Measure

We focus on developing tractable loss functions for implementation in this subsection.

_Learning the Action-Value Distribution based on Temporal Difference (TD)._ Based on [42; 43], we introduce the quantile regression  to approximate the action-value distribution in the offline dataset \(\) using an ensemble model \(\{D_{_{i}}\}_{i=1}^{K}\). We use Equation (4) to derive the loss as:

\[_{D}(_{i})=_{(s,a,r,s^{ })}[}_{n=1}^{N}_{m=1}^{N^ {}}_{}^{}(_{_{i}}^{_{n},_{m}^{ }})],_{_{i}}^{,^{}}=r+ Z ^{^{}}(s^{})-D_{_{i}}^{}(s,a,r ),\] (12)

where \(_{}^{}()=|-\{<0\} | l_{H}^{}()\) with the threshold \(\), \(Z\) denotes the value distribution, \(_{_{i}}^{,^{}}\) is the sampled TD error based on the parameters \(_{i}\), \(\) and \(^{}\) are two samples drawn from a uniform distribution \(U()\), \(D_{}^{}(s,a,r):=E_{D_{}(s,a,r)}^{-1}()\) is the sample drawn from \(p_{}(|s,a,r)\), \(Z^{}(s):=F_{Z(s)}^{-1}()\) is sampled from \(p(|s)\), \(F_{X}^{-1}()\) is the inverse cumulative distribution function (also known as quantile function)  at \(\) for the random variable \(X\), and \(N\) and \(N^{}\) represent the respective number of iid samples \(\) and \(^{}\). Notably, based on , we have \(Q_{_{i}}(s,a)=_{n=1}^{N}D_{_{i}}^{_{n}}(s,a,r)\).

In addition, if we learn the value distribution \(Z\), the action-value distribution can extract the information from the next states based on Equation (12), which is effective for capturing the uncertainty. On the contrary, if we directly use the next states in the offline dataset as the observations, in practice, the parameterized model of the action-value distribution needs to take \((s,a,r,s^{},a^{},r^{},s^{})\) as the input data. Thus, the model can compute the action values and values for the sampled TD error in Equation (12). To avoid the changes in the input data caused by directly using next states as observations in Bayesian inference, we draw inspiration from IQL and RIQL to learn a parameterized value distribution. Based on Equations (5) and (12), we derive a new objective as:

\[_{Z}()=_{(s,a)}[_{n=1}^{N} _{2}^{}(D_{}^{_{n}}(s,a,r)-Z_{}^{_{n}}(s) )],\] (13)

where \(D_{}^{}\) is the \(\)-quantile value among \(\{D_{_{i}}^{}(s,a)\}_{i=1}^{K}\), and \(V_{}(s)=_{n=1}^{N}Z_{}^{_{n}}(s)\). More details are shown in Appendix B.2. Furthermore, we provide the theoretical analysis in Appendix A.1 to give a value bound between the value distributions under clean and corrupted data.

_Updating the Action-Value Distribution based on Variational Inference for Robustness._ We discuss the detailed implementation of Equations (10) and (11) based on Equations (12) and (13). As the data corruptions may introduce heavy-tailed targets , we apply the Huber loss to replace all quadratic loss in Equation (10) and the metric \(\) in Equation (11), mitigating the issue caused by heavy-tailed targets  for robustness. We rewrite Equation (11) as follows.

\[_{}(_{i},_{s},_{a}, _{r}) =_{(s,a,r), p_{_{a}},  p_{_{a}}, p_{_{r}},D^{r} p}[_ {n=1}^{N}l_{H}^{}D^{_{n}}(s,a,r),D_{_{i}}^{_{n}}(s, ,r).\] \[+l_{H}^{}D^{_{n}}(s,a,r),D_{_{i}}^{_{n} }(s,a,r)+l_{H}^{}D^{_{n}}(s,a,r),D_{_{i}}^{_{ n}}(,a,r).\] (14)

Thus, we have the whole loss function \(_{D|S,A,R}=_{}(_{i},_{s}, _{a},_{r})+_{}(_{i},_{s}, _{a},_{r})\) in the generalized variational inference framework. Moreover, based on the assumption of heavy-tailed noise in , we have a upper bound of action-value distribution by using the Huber regression loss.

_Entropy-based Uncertainty Measure for Regulating the Loss associated with Corrupted Data._ To further address the challenge posed by diverse data corruptions, we consider the problem: how to exploit uncertainty to further enhance robustness?

Considering that our goal is to improve performance in clean environments, we propose to reduce the influence of corrupted data, focusing on using clean data to learn agents. Therefore, we provide a two-step plan: (1) distinguishing corrupted data from clean data; (2) regulating the loss associated with corrupted data to reduce its influence, thus enhancing the performance in clean environments.

For (1), as the Shannon entropy for the measures of aleatoric and epistemic uncertainties provides important insight , and the corrupted data often results in higher uncertainty and entropy of the action-value distribution than the clean data, we use entropy  to quantify uncertainties of corrupted and clean data. Furthermore, by considering that the exponential function can amplify the numerical difference in entropy between corrupted and clean data, we propose the use of exponential entropy --a metric of extent of a distribution--to design our uncertainty measure.

Specifically, based on Equation 12, we can use the quantile points \(\{_{n}\}_{n=1}^{N}\) to learn the corresponding quantile function values \(\{D^{_{n}}\}_{n=1}^{N}\) drawn from the action-value distribution \(p_{}\). We sort the quantile points and their corresponding function values in ascending order based on the values. Thus, we have the sorted sets \(\{_{n}\}_{n=1}^{N}\), \(\{D^{_{n}}\}_{n=1}^{N}\), and the estimated PDF values \(\{_{n}\}_{n=1}^{N}\), where \(_{1}=_{1}\) and \(_{n}=_{n}-_{n-1}\) for \(1<n N\). Then, we can further estimate differential entropy following  (see Appendix A.3 for a detailed derivation).

\[(p_{_{i}}(|s,a,r))=-_{n=1}^{N}_{n}_{_{i}}^{_{n}}(s,a,r) _{n},\] (15)

where \(_{n}\) denotes \((_{n-1}+_{n})/2\) for \(1<n N\), and \(^{_{n}}\) denotes \(D^{_{n}}-D^{_{n-1}}\) for \(1<n N\).

For (2), TRACER employs the reciprocal value of exponential entropy \(1/((p_{_{i}}))\) to weight the corresponding loss of \(_{i}\) in our proposed whole loss function \(_{D|S,A,R}\). Therefore, during the learning process, TRACER can regulate the loss associated with corrupted data and focus on minimizing the loss associated with clean data, enhancing robustness and performance in clean environments. Note that we normalize entropy values by dividing the mean of samples (i.e., quantile function values) drawn from action-value distributions for each batch. In Figure 3, we show the relationship of entropy values of corrupted and clean data estimated by Equation (15) during the learning process. The results illustrate the effectiveness of the entropy-weighted technique for data corruptions.

_Updating the Policy based on the Action-Value Distribution._ We directly applies the weighted imitation learning technique in Equation (6) to learn the policy. As \(Q_{}(s,a)\) is the \(\)-quantile value among \(\{Q_{_{i}}(s,a)\}_{i=1}^{K}=\{_{n=1}^{N}D_{_{i}}^{_{n} }(s,a,r)\}_{i=1}^{K}\) and \(V_{}(s)=_{n=1}^{N}Z_{}^{_{n}}(s)\), we have

\[_{}()=_{(s,a)}[ ( Q_{}(s,a)-_{n=1}^{N}Z_{}^{_{n }}(s))_{}(a|s)].\] (16)

## 4 Experiments

In this section, we show the effectiveness of TRACER across various simulation tasks using diverse corrupted offline datasets. Firstly, we provide our experiment setting, focusing on the corruption settings for offline datasets. Then, we illustrate how TRACER significantly outperforms previous state-of-the-art approaches under a range of both individual and simultaneous data corruptions. Finally, we conduct validation experiments and ablation studies to show the effectiveness of TRACER.

### Experiment Setting

Building upon RIQL , we use two hyperparameters, i.e., corruption rate \(c\) and corruption scale \(\), to control the corruption level. Then, we introduce the _random corruption_ and _adversarial corruption_ in four elements (i.e., states, actions, rewards, next states) of offline datasets. The implementation of random corruption is to add random noise to elements of a \(c\) portion of the offline datasets, and the implementation of adversarial corruption follows the Projected Gradient Descent attack [53; 54] using pretrained value functions. Note that unlike other adversarial corruptions, the adversarial reward corruption multiplies \(-\) to the clean rewards instead of using gradient optimization. We also introduce the random or adversarial simultaneous corruption, which refers to random or adversarial corruption simultaneously present in four elements of the offline datasets. We apply the corruption rate \(c=0.3\) and corruption scale \(=1.0\) in our experiments.

  Env & Corrupted Element & BC & EDAC & MSG & UWMSG & COL & IQL & RIQL & TRACER (ours) \\   & observation & \(34.5 1.5\) & \(1.1 0.3\) & \(1.1 0.2\) & \(1.9 0.1\) & \(5.0 116\) & \(32.6 2.7\) & \(35.7 4.2\) & \(\) \\  & action & \(14.0 1.1\) & \(32.7 0.7\) & \(\) & \(36.2 1.0\) & \(-23.3 1.2\) & \(27.5 0.3\) & \(31.7 1.7\) & \(33.1 1.2\) \\  & reward & \(35.8 0.9\) & \(-0.3 0.5\) & \(\) & \(43.8 0.3\) & \(-17.1 0.3\) & \(42.6 0.4\) & \(\) & \(41.9 0.2\) \\  & dynamics & \(35.8 0.9\) & \(-13.0 0.1\) & \(-15.5 0.0\) & \(5.0 2.2\) & \(-16.6 0.0\) & \(26.7 0.7\) & \(35.8 2.1\) & \(\) \\   & observation & \(12.7 5.9\) & \(-0.0 0.1\) & \(2.9 2.7\) & \(6.3 0.7\) & \(61.8 4.7\) & \(37.7 1.30\) & \(\) & \(\) \\  & action & \(5.4 0.4\) & \(41.9 240\) & \(5.4 0.9\) & \(5.9 0.4\) & \(27.0 7.5\) & \(25.0 6.6\) & \(66.1 4.6\) & \(\) \\  & reward & \(16.0 7.4\) & \(57.3 33.2\) & \(9.6 4.9\) & \(35.1 0.05\) & \(67.0 6.1\) & \(78.5 4.9\) & \(85.0 1.5\) & \(\) \\  & dynamics & \(16.0 7.4\) & \(4.3 0.9\) & \(0.1 0.2\) & \(1.8 0.2\) & \(3.9 1.4\) & \(-0.1 0.1\) & \(60.6 21.8\) & \(\) \\   & observation & \(21.6 7.1\) & \(36.2 0.2\) & \(16.0 2.8\) & \(15.0 1.3\) & \(\) & \(38.2 6.4\) & \(50.8 7.6\) & \(61.5 3.7\) \\  & action & \(15.5 2.2\) & \(25.7 3.8\) & \(23.0 2.1\) & \(27.7 1.3\) & \(32.2 7.6\) & \(37.9 4.8\) & \(63.6 7.3\) & \(\) \\   & reward & \(19.5 3.4\) & \(21.2 1.9\) & \(22.6 2.8\) & \(30.3 4.2\) & \(49.6 12.3\) & \(57.3 9.7\) & \(\) & \(64.3 1.5\) \\   & dynamics & \(19.5 3.4\) & \(0.6 0.0\) & \(0.6 0.0\) & \(0.7 0.0\) & \(0.6 0.0\) & \(1.3 1.1\) & \(\) & \(61.1 6.2\) \\   &  &  &  &  &  &  &  & \)} \\  

Table 3: Average score under diverse adversarial corruptions.

  Env & Corrupted Element & BC & EDAC & MSG & UWMSG & COL & IQL & RIQL & TRACER (ours) \\   & observation & \(34.5 1.5\) & \(1.1 0.3\) & \(1.1 0.2\) & \(1.9 0.1\) & \(5.0 116\) & \(32.6 2.7\) & \(35.7 4.2\) & \(\) \\  & action & \(14.0 1.1\) & \(32.7 0.7\) & \(\) & \(36.2 1.0\) & \(-23.1 1.2\) & \(27.5 0.3\) & \(31.7 1.7\) & \(33.1 1.2\) \\  & reward & \(35.8 0.9\) & \(-0.3 0.5\) & \(\) & \(43.8 0.3\) & \(-17.1 0.3\) & \(42.6 0.4\) & \(\) & \(41.9 0.2\) \\  & dynamicsWe conduct experiments on D4RL benchmark . Referring to RIQL, we train all agents for 3000 epochs on the'medium-replay-v2' dataset, which closely mirrors real-world applications as it is collected during the training of a SAC agent. Then, we evaluate agents in clean environments, reporting the average normalized performance over four random seeds. See Appendix C for detailed information. The algorithms we compare include: (1) CQL  and IQL , offline RL algorithms using a twin Q networks. (2) EDAC  and MSG , offline RL algorithms using ensemble Q networks (number of ensembles \(>2\)). (3) UWMSG  and RIQL, state-of-the-arts in corruption-robust offline RL. Note that EDAC, MSG, and UWMSG are all uncertainty-based offline RL algorithms.

### Main results under Diverse Data Corruptions

We conduct experiments on MuJoCo  (see Tables 1, 2, and 3, which highlight the _highest_ results) and CARLA  (see the left of Figure 2) tasks from D4RL under diverse corruptions to show the superiority of TRACER. In Table 1, we report all results under random or adversarial simultaneous data corruptions. These results show that TRACER significantly outperforms other algorithms in all tasks, achieving an average score improvement of \(+\%\). In the left of Figure 2, results on 'CARLA-lane_v0' under random simultaneous corruptions also illustrate the superiority of TRACER. See Appendix C.2 for details.

Random Corruptions.We report the results under random simultaneous data corruptions of all algorithms in Table 1. Such results demonstrate that TRACER achieves an average score gain of \(+\%\) under the setting of random simultaneous corruptions. Based on the results, it is clear that many offline RL algorithms, such as EDAC, MSG, and CQL, suffer the performance degradation under data corruptions. Since UWMSG is designed to defend the corruptions in rewards and dynamics, its performance degrades when faced with the stronger random simultaneous corruption. Moreover, we report results across a range of individual random data corruptions in Table 2, where TRACER outperforms previous algorithms in 7 out of 12 settings. We then explore hyperparameter tuning on Hopper task and further improve TRACER's results, demonstrating its potential for performance gains. We provide details in Appendix C.3.1.

Adversarial Corruptions.We construct experiments under adversarial simultaneous corruptions to evaluate the robustness of TRACER. The results in Table 1 show that TRACER surpasses others by a significant margin, achieving an average score improvement of \(+\%\). In these simultaneous corruption, many algorithms experience more severe performance degradation compared to the random simultaneous corruption, which indicates that adversarial attacks are more damaging to the reliability of algorithms than random noise. Despite these challenges, TRACER consistently achieves significant performance gains over other methods. Moreover, we provide the results across a range of individual adversarial data corruptions in Table 3, where TRACER outperforms previous algorithms in 7 out of 12 settings. We also explore hyperparameter tuning on Hopper task and further improve TRACER's results, demonstrating its potential for performance gains. See Appendix C.3.1 for details.

Figure 2: In the left, we report the means and standard deviations on CARLA under random simultaneous corruptions. In the right, we report the results with random simultaneous corruptions against different corruption levels.

### Evaluation of TRACER under Various Corruption Levels

Building upon RIQL, we further extend our experiments to include Mujoco datasets with various corruption levels, using different corruption rates \(c\) and scales \(\). We report the average scores and standard deviations over four random seeds in the right of Figure 2, using batch sizes of 256.

Results in the right of Figure 2 demonstrate that TRACER significantly outperforms baseline algorithms in **all tasks** under random simultaneous corruptions with various corruption levels. It achieves an average score improvement of \(+\%\). Moreover, as the corruption levels increase, the slight decrease in TRACER's results indicates that while TRACER is robust to simultaneous corruptions, its performance depends on the extent of corrupted data it encounters. We also evaluate TRACER in different scales of corrupted data and provide the results in Appendix C.4.1.

### Evaluation of the Entropy-based Uncertainty Measure

We evaluate the entropy-based uncertainty measure in action-value distributions to show: (1) is the uncertainty caused by corrupted data higher than that of clean data? (2) is regulating the loss associated with corrupted data effective in improving performance?

For (1), we first introduce labels indicating whether the data is corrupted. Importantly, these labels are not used by agents during the training process. Then, we estimate entropy values of labelled corrupted and clean data in each batch based on Equation (15). Thus, we can compare entropy values to compute results, showing how many times the entropy of the corrupted data is higher than that of clean data. Specifically, we evaluate the accuracy every 50 epochs over 3000 epochs. For each evaluation, we sample 500 batches to compute the average entropy of corrupted and clean data. Each batch consists of 32 clean and 32 corrupted data. We illustrate the curves over three seeds in the second and third columns of Figure 3, where each point shows how many of the 500 batches have higher entropy for corrupted data than that of clean data.

Figure 3 indicates an oscillating upward trend of TRACER's measurement accuracy using entropy (_TRACER Using Entro_) under simultaneous corruptions, demonstrating that using the entropy-based uncertainty measure can effectively distinguish corrupted data from clean data. These curves also reveal that even in the absence of any constraints on entropy (_TRACER NOT using Entro_), the entropy associated with corrupted data tends to exceed that of clean data.

For (2), in the first column of Figure 3, these results demonstrate that TRACER using the entropy-based uncertainty measure can effectively reduce the influence of corrupted data, thereby enhancing robustness and performance against all corruptions. We provide detailed information for this evaluation in Appendix C.4.2.

Figure 3: In the first column, we report the mean and standard deviation to show the superiority of using the entropy-based uncertainty measure. In the second and third columns, we report the results over three seeds to show the higher entropy of corrupted data compared to clean data during training.

## 5 Related Work

Robust RL.Robust RL can be categorized into two types: testing-time robust RL and training-time robust RL. Testing-time robust RL [19; 20] refers to training a policy on clean data and ensuring its robustness by testing in an environment with random noise or adversarial attacks. Training-time robust RL [16; 17] aims to learn a robust policy in the presence of random noise or adversarial attacks during training and evaluate the policy in a clean environment. In this paper, we focus on training-time robust RL under the offline setting, where the offline training data is subject to various data corruptions, also known as _corruption-robust offline RL_.

Corruption-Robust RL.Some theoretical work on corruption-robust online RL [60; 61; 62; 63] aims to analyze the sub-optimal bounds of learned policies under data corruptions. However, these studies primarily address simple bandits or tabular MDPs and focus on the reward corruption. Some further work [64; 65] extends the modeling problem to more general MDPs and begins to investigate the corruption in transition dynamics.

It is worth noting that corruption-robust offline RL has not been widely studied. UWMSG  designs a value-based uncertainty-weighting technique, thus using the weight to mitigate the impact of corrupted data. RIQL  further extends the data corruptions to all four elements in the offline dataset, including states, actions, rewards, and next states (dynamics). It then introduces quantile estimators with an ensemble of action-value functions and employs a Huber regression based on IQL , alleviating the performance degradation caused by corrupted data.

Bayesian RL.Bayesian RL integrates the Bayesian inference with RL to create a framework for decision-making under uncertainty . It is important to highlight that Bayesian RL is divided into two categories for different uncertainties: the _parameter uncertainty_ in the learning of models [66; 67] and the _inherent uncertainty_ from the data/environment in the distribution over returns [68; 69]. In this paper, we focus on capturing the latter.

For the latter uncertainty, in model-based Bayesian RL, many approaches [70; 68; 71] explicitly model the transition dynamics and using Bayesian inference to update the model. It is useful when dealing with complex environments for sample efficiency. In model-free Bayesian RL, value-based methods [69; 72] use the reward information to construct the posterior distribution of the action-value function. Besides, policy gradient methods [73; 74] use information of the return to construct the posterior distribution of the policy. They directly apply Bayesian inference to the value function or policy without explicitly modeling transition dynamics.

Offline Bayesian RL.offline Bayesian RL integrates Bayesian inference with offline RL to tackle the challenges of learning robust policies from static datasets without further interactions with the environment. Many approaches [75; 76; 77] use Bayesian inference to model the transition dynamics or guide action selection for adaptive policy updates, thereby avoiding overly conservative estimates in the offline setting. Furthermore, recent work  applies variational Bayesian inference to learn the model of transition dynamics, mitigating the distribution shift in offline RL.

## 6 Conclusion

In this paper, we investigate and demonstrate the robustness and effectiveness of introducing Bayesian inference into offline RL to address the challenges posed by data corruptions. By leveraging Bayesian techniques, our proposed approach TRACER captures the uncertainty caused by diverse corrupted data. Moreover, the use of entropy-based uncertainty measure in TRACER can distinguish corrupted data from clean data. Thus, TRACER can regulate the loss associated with corrupted data to reduce its influence, improving performance in clean environments. Our extensive experiments demonstrate the potential of Bayesian methods in developing reliable decision-making.

Regarding the limitations of TRACER, although it achieves significant performance improvement under diverse data corruptions, future work could explore more complex and realistic data corruption scenarios and related challenges, such as the noise in the preference data for RLHF and adversarial attacks on safety-critical driving decisions. Moreover, we look forward to the continued development and optimization of uncertainty-based corrupted-robust offline RL, which could further enhance the effectiveness of TRACER and similar approaches for increasingly complex real-world scenarios.