# Incremental Learning of Retrievable Skills For Efficient Continual Task Adaptation

Daehee Lee\({}^{,}\), Minjong Yoo\({}^{}\), Woo Kyung Kim\({}^{}\), Wonje Choi\({}^{}\), Honguk Woo\({}^{}\)

\({}^{}\)Sungkyunkwan University \({}^{}\)Carnegie Mellon University

{dulg17245, mjyoo2, kwk2696, wjchoi1995, hwoo}@skku.edu

###### Abstract

Continual Imitation Learning (CiL) involves extracting and accumulating task knowledge from demonstrations across multiple stages and tasks to achieve a multi-task policy. With recent advancements in foundation models, there has been a growing interest in adapter-based CiL approaches, where adapters are established parameter-efficiently for tasks newly demonstrated. While these approaches isolate parameters for specific tasks and tend to mitigate catastrophic forgetting, they limit knowledge sharing among different demonstrations. We introduce IsCiL, an adapter-based CiL framework that addresses this limitation of knowledge sharing by incrementally learning shareable skills from different demonstrations, thus enabling sample-efficient task adaptation using the skills particularly in non-stationary CiL environments. In IsCiL, demonstrations are mapped into the state embedding space, where proper skills can be retrieved upon input states through prototype-based memory. These retrievable skills are incrementally learned on their corresponding adapters. Our CiL experiments with complex tasks in Franka-Kitchen and Meta-World demonstrate robust performance of IsCiL in both task adaptation and sample-efficiency. We also show a simple extension of IsCiL for task unlearning scenarios.

## 1 Introduction

Lifelong agents such as home robots are required to continually adapt to new tasks in sequential decision-making situations by leveraging knowledge from past experiences. However, many real-world domains pose substantial challenges for these lifelong agents; the complexity and ever-changing nature of these tasks make it difficult for agents to constantly adapt, leading to difficulties in retaining knowledge and maintaining operational efficiency . For instance, a home robot agent, operating within a single household, needs to continuously adapt, learning specific tasks in various areas such as cooking assistance in the kitchen or cleaning in the bathroom. At the same time, it is crucial that the agent not only retains but also improves its proficiency in the tasks it has previously learned, ensuring that it maintains consistent efficiency throughout the home.

For these lifelong agents, Continual Imitation Learning (CiL) has been explored, in which an agent progressively learns a series of tasks by leveraging expert demonstrations over time to achieve a multi-task policy. Yet, CiL often encounters practical challenges: (1) the high costs and inefficiencies associated with comprehensive expert demonstrations  that are required for imitation, (2) frequently shifting tasks in dynamic, non-stationary environments, and (3) privacy concerns  related to learning from expert demonstrations. In this context, CiL faces significant issues in terms of cost, adaptability, and privacy, complicating its implementation in real-world scenarios.

To address these challenges, our work focuses on incorporation of skill learning and fine-tuning in CiL, leveraging recent advancements in foundation models [4; 5]. These have been increased interests in continual task adaptation based on multiple adapters learned on a foundation model [6; 7]. The adapter-based learning approach allows for parameter isolation for individual tasks, thus enabling to mitigate catastrophic forgetting of previously learned knowledge in CiL. Motivated by this use of adapters, we develop IsCiL, a new adapter-based CiL framework that addresses the practical challenges of CiL aforementioned, by incrementally learning shareable skills from different demonstrations through multiple adapters. IsCiL facilitates sample-efficient task adaptation using the skills particularly in non-stationary CiL environments.

Specifically, in the IsCiL framework, a prototype-based skill incremental learning method is employed with a two-level hierarchy including smaller, more manageable adapters: skill retriever and skill decoder. The skill retriever is responsible for composing skills to complete given goal-reaching tasks. It utilizes skill prototypes, which are representative embeddings of skills, to retrieve the appropriate skill for input. The knowledge of each skill is contained within the adapter, which can modify its associated base model output. The skill decoder is responsible for producing short-horizon actions for state-skill pairs.

We evaluate IsCiL and several adapter-based continual learning baselines across scenario variations based on complex, long-horizon tasks in the Franka-Kitchen and Meta-World environments to assess sample efficiency, task adaptation, and privacy considerations. The baselines include adapter-based continual adaptation techniques as well as conventional continual imitation learning methods. Our results demonstrate that IsCiL achieves robust performance without requiring comprehensive expert demonstrations. This flexibility allows IsCiL to continually and efficiently adapt to varying sequences in different environments by leveraging any available expert data to learn useful skills, with tasks composed of diverse instructions and demonstrations.

In summary, the IsCiL framework enhances sample efficiency and task adaptation, effectively bridging the gap between adapter-based CiL approaches and the knowledge sharing across demonstrations. Comprehensive experiments demonstrate that IsCiL outperforms other adapter-based continual learning approaches in various CiL scenarios.

## 2 Related work

**Continual imitation learning.** To tackle the problem of catastrophic forgetting in continual learning, numerous studies have employed rehearsal techniques [8; 9; 10; 11], which involve replaying past experiences to maintain performance on previously learned tasks. Another approach involves utilizing additional model parameters to progressively extend the model architecture [12; 13; 14; 15; 16]. These methods adapt the model's structure over time to accommodate new tasks. However, rehearsal techniques exhibit high variability in forgetting depending on the replay ratio and often demand substantial training to incorporate new knowledge . Progressive models, on the other hand, require stage identification during evaluation and often overlook unseen tasks . In this work, we propose a CiL framework that enables effective learning and expansion without requiring rehearsal and stage identification, leveraging pre-trained goal-based model knowledge.

**Continual task adaptation with pre-trained models.** Several recent works use pre-trained models, accumulating knowledge continually through additional Parameter Efficient Tuning (PET) modules such as adapters [18; 17; 19; 20; 21; 6; 22]. These methods enhance the flexibility and scalability of continual learning systems. However, they suffer from inaccurate matching between adapter selection and trained knowledge, leading to a misalignment between the knowledge learned during training and the knowledge used during evaluation [17; 20], which hinders overall performance. In the realm of sequential decision making, some studies have explored adapting pre-trained models. In , the state space of tasks is fully partitioned, restricting its applicability in more integrated environments. Meanwhile,  relies on comprehensive demonstrations for learning, which may be impractical in real-world scenarios. Our study aims to enhance task adaptation efficiency by using incrementally generalized skills with accurate matching on state space.

**Skill adaptation.** Reinforcement learning research has enhanced fast adaptation through skill exploration  and skill priors , focusing on improving sample efficiency with offline datasets. Despite these advancements, adapting fixed skill decoders to new environments remains challenging. To overcome these limitations, skill-based few-shot imitation learning methods have been developed[25; 26]. However, these methods require extensive past data and struggle with scalability and generalization. Even skill-based approaches used in continual imitation learning  still require rehearsal data to mitigate knowledge loss and face difficulties addressing privacy issues through unlearning. Our IsCiL employs parameter-efficient skill adapters to prevent catastrophic forgetting and maintain efficiency, providing a scalable solution for unlearning.

## 3 Approaches

Our work addresses three key challenges of CiL: (1) data inefficiency, (2) non-stationarity, and (3) privacy concerns, by adopting retrievable skills in the CiL context. Specifically, our IsCiL framework not only enhances data-efficient continual task evaluation in a non-stationary environment but also supports unlearning as a task adaptation strategy, thereby mitigating privacy concerns.

### Problem formulation

In CiL scenarios, we consider a data stream of task datasets \(\{_{i}\}_{i=1}^{p}\), where \(_{i}\) contains an expert demonstration \(_{i}=\{d_{i}^{1},...,d_{i}^{N}\}\) for its associated task \(_{i}\). To effectively represent complex long-horizon tasks, each task \(_{i}\) is comprised of sub-goal list, \(=\{g_{i}^{1},...,g_{i}^{M}\}\). Each task dataset is sampled in a finite-horizon markoch decision process \((,,,,_{0},H)\), where \(\) is a state space, \(\) is a action space, \(\) is a transition probability, \(\) is a reward function, \(_{0}\) is an initial state distribution, and \(H\) is an environment horizon.

For demonstration \(d=\{(s_{t},a_{t})\}_{t=1}^{H}\), a state \(s_{t} S\) represents a tuple \((o_{t},g_{t})\) consisting of an observation \(o_{t}\) and a sub-goal \(g_{t}\). In our work, we represent sub-goals through language and use language-based goal embeddings for \(g_{t}\) to achieve language-conditioned policies. Then, the objective of IsCiL is to obtain a multi-task policy \(^{*}\), by which the performance on the tasks in the data stream can be comparable to that of respective expert policies. This is formulated as

\[^{*}=*{argmin}_{}[_{i}[_{ _{i}}((|s)\|_{}(|s))]]\] (1)

where \(_{}\) represents an expert policy for \(\) and \(_{i}\) denotes a set of evaluation tasks at stage \(i\). In this context, the evaluation tasks continuously vary across different stages.

Figure 1: The scenario demonstrating how IsCiL enhances continual imitation learning efficiency through retrievable skills: (i) Prototype-based skill incremental learning: despite the failure of \(_{1}\), skills are incrementally learned from the available demonstrations. In later stages, missing skills for \(_{1}\) are retrieved from other tasks, achieving the resolution of \(_{1}\) and illustrating the reversibility and efficiency of retrievable skills. (ii) Task-wise selective adaptation: IsCiL effectively retrieves relevant learned skills, facilitating rapid task adaptation.

### Overall architecture

To effectively handle complicated CiL scenarios, we present the IsCiL framework which involves (i) **prototype-based skill incremental learning** and (ii) **task-wise selective adaptation**.

As illustrated in Figure 1, in (i) the prototype-based skill incremental learning, we use a two-level hierarchy structure with a skill retriever \(_{R}\) composing the skills for each sub-goal, and a skill decoder \(_{D}\) producing short-horizon actions based on state-skill pairs. For this two-level policy hierarchy, we employ a skill prototype-based approach, in which skill prototypes capture the sequential patterns of actions and associated environmental states, as observed from expert demonstrations. These prototypes serve as a reference for skills learned from a multi-stage data stream. Using these skill prototypes, we can effectively translate task-specific instructions or demonstrations into a series of appropriate skills.

Through this prototype-based skill retrieval method, the policy flexibly uses skills that are shareable among tasks, potentially learned in the past or future, for policy evaluation. This enables the CiL agent to effectively learn diverse tasks and rapidly adapt to variations, while incrementally accumulating skill knowledge from a multi-stage data stream. Furthermore, to facilitate sample-efficient learning and enhance stability in CiL, we employ parameter-efficient adapters that are continually fine-tuned on a base model. Each skill knowledge is encapsulated within a dedicated adapter and incorporated into the skill decoder \(_{D}\) to infer expert actions.

In (ii) the task-wise selective adaptation, we devise efficient task adaptation procedures in the policy hierarchy to adapt to specific tasks using incrementally learned skills. This enables the CiL agent to not only facilitate adaptation to shifts in task distribution (e.g., due to non-stationary environment conditions) but also support task unlearning upon explicit user request (e.g., due to privacy concerns).

Suppose that the smart home environment undergoes an upgrade with the installation of new smart lighting systems throughout the house. In this case, task-wise selective adaptation can be used for rapid adaptation by removing outdated control routines associated with the previous systems.

### Prototype-based skill incremental learning

**State encoder and prototype-based skill retriever.** To facilitate skill retrieval from demonstrations, we encode observation and goal pairs \((o_{t},g_{t})\) into state embeddings \(s_{t}\) using a function \(f:(o_{t},g_{t}) s_{t}\). We implement \(f\) as a fixed function to ensure consistent retrieval results for learning efficiency, mitigating the negative effects of input distribution shifts.

To effectively handle the multi-modality of the state distribution in non-stationary environments, we employ a skill retriever \(_{R}\). For this, we use multifaceted skill prototypes \(_{z}\), where \(\) is the set of learned skill prototypes. These prototypes capture the sequential patterns of expert demonstrations associated with specific goal-reaching tasks.

\[_{z}=_{R}(s_{t};)=h(_{_{z} }S(_{z},s_{t})),\ S(_{z},s_{t})=_{b_{z}}(b,s_{t})\] (2)

Here, \(h:_{z}_{z}\) denotes a one-to-one function that maps each skill prototype \(_{z}\) to its dedicated adapter parameters \(_{z}\), while the similarity function \(S\) is defined as the maximum similarity between state \(s\) and bases \(b_{z}\). Each \(_{z}\) consists of multiple bases (e.g., 20 bases), and each basis \(b\) is a representative vector containing its corresponding centroid, shaped identically to the state \(s_{t}\).

**Adapter conditioned skill decoder.** To effectively use the knowledge of the pre-trained base model without forgetting, even in a non-stationary changing environment, the skill decoder is conditioned based on parameters. The skill decoder policy \(_{D}(_{t}|o_{t},g_{t};_{},_{z})\) operates with the skill adapter parameters \(_{z}\) and the pre-trained base model \(_{}\), using the Low-Rank Adaptation .

**Skill incremental learning.** To incrementally learn new retrievable skills, we update the skill prototype and adapter pair \((_{z^{*}},_{z^{*}})\) for a novel skill \(z^{*}\). The skill prototype \(_{z^{*}}\) is created by dividing a dataset of a single skill into several clusters based on similarity. From each cluster, a representative value is extracted to serve as the basis \(b\), representing \(z^{*}\). We use the KMeans algorithm  to determine these bases, ensuring that the number of bases \(|_{z}|\) adequately captures the diversity within the dataset of the novel skill. This multifaceted set of bases allows the skill prototype to capture an accurate multi-modal distribution of the skill represented in the state space, enabling effective retrieval as described in Eq. 2. In our experiment, \(z^{*}\) is created for each sub-goal \(g\) in the given dataset \(_{i}\) for each stage \(i\).

The learning of the skill adapter is divided into two phases: initialization and update. During the initialization phase, \(_{z^{*}}\) is initialized using existing skill adapters. Predictions with the existing skill dataset and skill prototypes \(_{z}\) are used to identify the most frequently selected skill. Average scores are computed for each skill prototype using the dataset involved in training \(z^{*}\), as defined in Eq. 2. The skill with the highest average score, denoted as \(\), is selected. Consequently, \(_{z^{*}}\) is initialized by \(_{}\). Then, the initialized adapter is updated through the following imitation loss.

\[(o_{t},g_{t},a_{t};_{z})=\|a-_{D}(_{t} o_{t},g_ {t};_{},_{z})\|,_{z}=_{R}(o_{t},g_{t})\] (3)

The novel skill \(z^{*}\) is incorporated into the learned prototypes \(_{z^{*}}\), and the novel prototype and adapter pair \((_{z^{*}},_{z^{*}})\) updates the function \(h\) for pair mapping. Figure 2 presents an overview of this methodology, along with the algorithm for incremental learning is detailed in Appendix B.1.

### Task-wise selective adaptation

**Task evaluation.** Given the pre-trained model \(_{}\) and learned skill prototypes \(\), for given inputs \((o_{t},g_{t})\) from the environment, IsCiL performs the following evaluation process.

\[_{t}_{D}(_{t} o_{t},g_{t};_{}, _{z}),_{z}=_{R}(o_{t},g_{t};)\] (4)

The evaluation process adapts to novel tasks and sub-goal sequences from the environment by modifying the goal \(g_{t}\). This adjustment enables the inference of appropriate current actions, in a manner of similar to handling learned tasks. For example, a kitchen robot tailored to a specific user's kitchen setup can continuously and instantly adapt to changes in recipes without additional training.

**Task unlearning.** To ensure privacy protection for incrementally learned skills, our architecture allows for task unlearning by removing task-specific skill prototypes and adapters. In IsCiL, the separation of skill adapters for each task facilitates easy tagging of task information on each skill. When an unlearning request is given with a task identifier \(\), the corresponding skill prototypes and adapters are removed. This approach ensures exceptionally efficient and effective unlearning, aligning with the strong unlearning strategies in continual learning discussed in .

## 4 Experiments

### Environments and data streams

To investigate the sample efficiency and adaptation performance, we construct complex CiL scenarios using diverse long-horizon tasks [29; 30; 31]. We then analyze the sample efficiency across different stages and tasks with three types of scenarios: _Complete_, _Semi_-complete, and _Incomplete_, depending on how the samples are utilized and shared. Each scenario consists of a pre-training stage followed by 20 CiL stages. Figure 3 illustrates these scenarios.

**Evolving Kitchen.** Evolving Kitchen is a data stream based on long-horizon tasks in the Franka-Kitchen environment [29; 30]. Each task requires sequentially achieving four out of seven sub-goals. The scenario consists of a pre-training stage in the environment with only four objects: kettle, bottom burner, top burner, and light switch, followed by continual adaptation to tasks involving seven objects.

Figure 2: Overview of the IsCiL framework: (a) The prototype-based skill retriever sequentially utilizes a state encoder \(f\), multifaceted skill prototypes \(\), and a skill adapter mapping function \(h\) to identify the skill adapter \(_{z}\). (b) Skill incremental learning involves the initialization and updating of the skill prototype \(_{z^{*}}\) and its corresponding adapter \(_{z^{*}}\).

**Evolving World.** Evolving World is a data stream based on the Meta-World environment  with long-horizon tasks, similar to [32; 33; 34]. Each task requires sequentially achieving four out of eight sub-goals. The scenario consists of a pre-training stage in the environment with only four objects, followed by continual adaptation to an entire environment with all eight Meta-World objects. More detailed configurations are provided in Appendix A.

### Baselines and metrics

**Baselines.** We implement continual imitation learning and continual adaptation methods for sequential decision-making problems, which do not use rehearsal. First, we consider continual learning algorithms which involve full-model updates (**Seq**, **EWC**). We also implement several continual adaptation approaches that utilize pre-trained models with adapters (**L2M**, **TAIL**). L2M learns a key and adapter pair to modulate the pre-trained model, where the key is a retrievable state embedding similar to our prototypes. TAIL, unlike L2M, incrementally constructs task identifiers and corresponding adapters to modulate the pre-trained model with new task data without forgetting previous tasks. Each method is categorized based on the values used for adapter retrieval: a version that uses no additional identifiers, sub-goal identifiers (denoted as -_g_), and whole sub-goal sequences as single identifiers (denoted as -_\(\)_). Additionally, we include a **Multi-task** learning approach as an oracle baseline, which retains all incoming data at each stage and utilizes it for training in subsequent stages. For all baselines, we use the same pre-trained goal-conditioned policy and a diffusion model [36; 37] as the base policy architecture. A detailed description of the baselines and their hyperparameters are provided in Appendix B.2.

**Metrics.** We use three metrics to report CiL performance: Forward Transfer (FWT), Backward Transfer (BWT), and Area Under Curve (AUC) [38; 7]. In our long-horizon tasks, these metrics rely on goal-conditioned success rates (GC), which measure the ratio of successfully completed sub-goals to the total sub-goals within each task .

* **FWT** (Forward Transfer): This evaluates the ability to learn tasks using previously learned knowledge. It is measured by the performance of a task when it occurs.
* **BWT** (Backward Transfer): This evaluates the impact of each learning stage on the performance of tasks learned in previous stages. It measures the change in task performance from past stages observed in the current stage.
* **AUC** (Area Under Curve): This represents the overall continual imitation learning performance in a scenario. It measures the average performance of tasks learned in the current stage over the remaining stages of the scenario.

For all metrics, **higher values indicate better** performance, with details provided in Appendix B.3.

Figure 3: CiL scenarios including _Complete_, _Semi-Complete_, and _Incomplete_, categorized by sample utilization difficulty, based on the completeness of the demonstration for task performance: In _Complete_, each of the 20 CiL stages incrementally introduces new tasks featuring objects not encountered in the pre-training stage, along with full, comprehensive demonstrations for each task. In _Semi-Complete_, the first 10 stages are repeated twice, with tasks presented alongside incomplete demonstrations, where specific sub-goals are missing from the trajectories. In _Incomplete_, the same sequence of tasks from the _Complete_ scenario is used, but all stages feature incomplete demonstrations, requiring the system to handle tasks with missing sub-goal trajectories.

### Overall performance : sample efficiency

Table 1 shows the CiL performance on Evolving Kitchen and Evolving World across three different scenarios (_Complete_, _Semi_, _Incomplete_). We compare the performance achieved by our framework IsCiL and other baselines (L2M, TAIL) with different conditioning values (\(g\),\(\)) for adapter retrieval. IsCiL consistently demonstrates superior performance in AUC across all scenarios, achieving between 84.5% and 97.2% of the oracle baseline (Multi-task learning). TAIL-\(\) shows the most competitive performance in the _Complete_ CiL scenario across both environments. However, due to its isolated adapter for learning and evaluation, it fails to effectively utilize samples across stages.

L2M and L2M-\(g\) exhibit relatively lower and less stable AUC in the Evolving Kitchen scenario. Conversely, in Evolving World-_Semi_, they surpass TAIL-\(\) in AUC. This demonstrates that they are capable of sharing different skills across stages. Despite this, they still struggle with accurately retrieving the correct skill or suffer from performance degradation due to knowledge overwriting. Unlike them, IsCiL effectively mitigates overwriting by maintaining distinct skill representations across stages. Both L2M-\(g\) and TAIL-\(g\), which aim to leverage sub-goal labels for CiL, struggle to maintain performance due to skill distribution shifts, leading to catastrophic forgetting of skills for sub-goals. These challenges reveal that relying solely on sub-goal labels may not be sufficient to sustain and share skills effectively across different stages and tasks.

Both Seq-FT and Seq-LoRA struggle with forgetting. This is evident in the _Complete_ scenario, where Seq-FT achieves the highest FWT but shows the lowest BWT, leading to a decline in overall performance. EWC exhibits consistently lower performance, as the regularization used to preserve past knowledge significantly hinders learning on current tasks, leading to severe degradation in long-horizon tasks. Although EWC shows higher BWT compared to other sequential tuning baselines, its low FWT limits overall effectiveness.

  
**Stream** &  &  &  \\  CiL-algorithm & FWT (\%) & BWT (\%) & AUC (\%) & FWT (\%) & BWT (\%) & AUC (\%) & FWT (\%) & BWT (\%) & AUC (\%) \\  Pre-trained & - & - & 24.3\(\)0.8 & - & - & 29.1\(\)0.9 & - & - & 24.3\(\)0.8 \\  Seq-FT & 90.9\(\)2.6 & -63.7\(\)2.7 & 35.0\(\)0.7 & 37.1\(\)2.1 & -25.1\(\)2.7 & 16.5\(\)0.7 & 32.7\(\)3.3 & -19.6\(\)3.0 & **15.7\(\)0.5** \\ EWC & 34.2\(\)0.8 & -19.5\(\)4.2 & 17.1\(\)2.7 & 27.2\(\)1.3 & -18.0\(\)0.3 & 12.2\(\)1.4 & 19.3\(\)2.3 & -32.1\(\)1.0 & 14.4\(\)1.7 \\ Seq-LoRA & 77.5\(\)5.2 & -55.2\(\)1.8 & 28.3\(\)3.5 & 37.4\(\)3.8 & -25.5\(\)3.2 & 15.9\(\)3.2 & 32.9\(\)2.5 & -19.9\(\)2.9 & **14.5\(\)0.2** \\  L2M & 24.7\(\)4.8 & -2.5\(\)4.5 & 22.7\(\)1.9 & 19.2\(\)4.4 & 0.2\(\)1.3 & 19.1\(\)1.8 & **17.5\(\)**4.0 & -2.0\(\)3.2 & **15.8\(\)**8.8** \\ L2M-\(g\) & 38.2\(\)3.4 & -6.5\(\)3.7 & 32.3\(\)1.4 & 37.9\(\)3.7 & -4.5\(\)3.1 & 32.1\(\)2.2 & 37.5\(\)1.0 & -6.5\(\)6.9 & 31.0\(\)8.8 \\ TAIL-\(g\) & 85.3\(\)8.0 & -49.9\(\)6.7 & 41.5\(\)1.7 & 55.0\(\)1.5 & -21.1\(\)2.2 & 37.2\(\)2.4 & 53.2\(\)1.7 & -20.0\(\)2.0 & **35.4\(\)**0.7** \\ TAIL-\(\) & 86.2\(\)5.6 & 0.0\(\)0.0 & 86.2\(\)5.6 & 41.2\(\)2.5 & 0.0\(\)0.0 & 41.2\(\)2.5 & 33.8\(\)3.0 & 0.0\(\)0.0 & 33.8\(\)3.0 \\ IsCL (ours) & 79.3\(\)1.7 & 11.0\(\)1.6 & **89.8\(\)**0.8 & 68.1\(\)2.2 & 8.6\(\)0.6 & **75.8\(\)**1.8 & 61.8\(\)0.9 & 13.7\(\)2.9 & **74.0\(\)**1.9** \\  Multi-task & 93.3\(\)1.7 & -16.2\(\)3.2 & **92.3\(\)1.7** & 75.4\(\)4.5 & 8.0\(\)5.5 & 83.2\(\)2.1 & 71.7\(\)1.1 & 12.6\(\)0.8 & **83.0\(\)**0.1** \\    &  &  &  \\  CiL-algorithm & FWT (\%) & BWT (\%) & AUC (\%) & FWT (\%) & BWT (\%) & AUC (\%) & FWT (\%) & BWT (\%) & AUC (\%) \\  Pre-trained & - & - & 0.0\(\)0.0 & - & - & 0.0\(\)0.0 & - & - & 0.0\(\)0.0 \\  Seq-FT & 88.9\(\)3.1 & -73.6\(\)2.4 & 24.9\(\)0.4 & 38.9\(\)5.9 & -27.5\(\)5.5 & 13.2\(\)0.3 & 41.4\(\)2.0 & -33.0\(\)0.0 & **12.2\(\)**0.8** \\ EWC & 25.7\(\)3.8 & -18.0\(\)0.2 & 10.5\(\)1.5 & 13.9\(\)1.4 & -9.1\(\)1.8 & 6.2\(\)1.8 & 18.2\(\)2.8 & -11.6\(\)1.1 & **8.5\(\)**0.9** \\ Seq-LoRA & 85.6\(\)2.9 & -75.1\(\)3.2 & 21.4\(\)2.4 & 32.2\(\)2.5 & -18.2\(\)4.9 & 16.0\(\)3.8 & 38.1\(\)6.6 & -30.6\(\)0.9 & 11.7\(\)0.9 \\  L2M & 72.1\(\)5.3 & -6.6\(\)2.1 & 65.9\(\)3.3 & 41.0\(\)1.1 & 6.3\(\)3.0 & 47.0\(\)0.7 & 26.1\(\)1.1 & 5.7\(\)2.8 & 31.4\(\)2.0 \\ L2M-\(g\) & 64.2\(\)3.9 & -19.3\(\)4.4 & 48.6\(\)2.0 & 44.5\(\)2.0 & 3.4\(\)2.5 & 48.2\(\)0.2 & 33.2\(\)2.0 & -0.6\(\)0.9 & 33.1\(\)1.2 \\ TAIL-\(g\) & 90.0\(\)3.0 & -56.8\(\)0.4 & 39.5\(\)2.9 & 43.2\(\)2.8 & -17.6\(\)3.5 & 27.4\(\)3.1 & 51.4\(\)2.5 & -21.4\(\)0.6 & **32.5\(\)**2.3** \\ TAIL-\(\) & 85.7\(\)5.9 & 0.0\(\)0.0 & **85.7\(\)**5.9 & 27.5\(\)**0.7 & 0.0\(\)0.0 & 27.5\(\)0.7 & 39.7\(\)0.0 & 0.0\(\)0.0 & **39.7\(\)**0.0** \\ IsCL (ours) & 81.7\(\)0.4 & 2.7\(\)0.9 & 84.3\(\)1.1 & 60.0\(\)1.1 &

### Task adaptation

Table 2 shows the unseen task adaptation ability of IsCiL, where only the sub-goal sequence of novel task is provided without demonstrations. This scenario extends the existing _Complete_ CiL scenarios by periodically introducing novel tasks. Metrics labeled with the suffix -A indicate results from adaptation tasks, whereas the other metrics reflect performance on all tasks. For this scenario, we exclude TAIL-\(\) from comparison, as it lacks the ability to adapt to novel tasks.

IsCiL demonstrates superior task performance in both scenarios, which contributes to greater efficiency in task adaptation. Moreover, in Evolving Kitchen, IsCiL not only demonstrates task adaptation ability by achieving the highest FWT-A, but also significantly enhances its initial performance, raising FWT-A from 52.1 to an AUC-A of 72.8. TAIL-\(g\) shows comparable performance in FWT for the Evolving Kitchen. However, it struggles with catastrophic forgetting, leading to a \(-34.9\) negative BWT when faced with significant distribution shifts in sub-goal demonstrations. In Evolving World, L2M, which actively learns to share skills during training, outperforms TAIL-\(g\). L2M is the only baseline achieving performance improvement on unseen tasks through CiL.

### Task unlearning as adaptation

Table 3 measures CiL performance in scenario with task-level unlearning. For comparison, we use an adapter-based approach with parameter isolation-based continual learning private unlearning (CLPU) , extending TAIL to **TAIL-\(\) CLPU** and IsCiL without skill adapter initialization. Similar to IsCiL, CLPU learns tasks in isolated models tagged with specific task identifiers and handles unlearning requests by removing the corresponding model parameters of the target task. Both **TAIL-\(\) CLPU** and IsCiL ensure output distribution equality between the unlearned model and the model trained with the retained dataset. Thus, their CiL performance remains largely unaffected by unlearning.

Although IsCiL exhibits a slight performance degradation of \(1.8\% 5.2\%\) after unlearning, as reported in Table 1, it still demonstrates robustness by achieving a \(115\%\) higher AUC compared to **TAIL-\(\) CLPU** in _incomplete_ scenarios.

### Analysis

**Rehearsal comparison.** Figure 4 compares the sample efficiency to retain learned knowledge between IsCiL and a rehearsal-based continual imitation learning approach, Experience Replay (ER) . For ER, we adjust the number of stored samples per learning stage, while IsCiL does not store rehearsals

  
**Stream** &  &  \\  Algorithm & **FWT-(s)** & **BWT-(s)** & **AUC** (s)** & **FWT-A(s)** & **AUC-A(s)** & **FWT-(s)** & **BWT-(s)** & **AUC** (s)** & **FWT-A(s)** & **AUC-A(s)** \\  Seq-FT & 72.3\(\)1.6 & -47.7\(\)1.6 & **30.4\(\)**1.2 & 27.8\(\)0.6 & 19.5\(\)0.1 & 52.9\(\)0.3 & -26.7\(\)1.1 & 30.1\(\)0.2 & 16.3\(\)1.5 & 24.0\(\)0.2 \\ EWC & 21.0\(\)1.9 & -14.0\(\)0.2 & **16.8\(\)**1.6 & 18.1\(\)2.2 & 14.4\(\)1.6 & 16.5\(\)1.9 & -8.1\(\)0.8 & 9.6\(\)2.6 & 6.1\(\)1.3 & 8.3\(\)1.1 \\ Seq-LoRa & 62.4\(\)3.5 & -41.5\(\)3.3 & 25.4\(\)0.0 & 28.1\(\)0.0 & 18.2\(\)0.0 & 45.2\(\)0.4 & -35.8\(\)1.3 & 14.5\(\)0.0 & 6.4\(\)1.5 & 8.2\(\)1.8 \\   L2M & 22.3\(\)1.0 & 0.3\(\)1.2 & 22.7\(\)1.5 & 15.3\(\)2.2 & 22.1\(\)1.5 & 15.1\(\)3.7 & -1.4\(\)3.3 & 53.6\(\)1.0 & 40.3\(\)2.4 & 41.2\(\)2.2 \\ L2M-\(g\) & 33.8\(\)0.9 & -43.1\(\)0.0 & 22.0\(\)0.6 & 24.1\(\)0.3 & 43.1\(\)0.6 & 35.1\(\)0.6 & 42.1\(\)0.5 & 25.7\(\)1.9 \\ TAIL-\(g\) & 67.6\(\)4.7 & -34.9\(\)5.4 & 36.8\(\)3.2 & 34.7\(\)2.2 & 30.1\(\)1.0 & 53.2\(\)1.4 & -27.1\(\)1.2 & 29.2\(\)0.0 & 18.6\(\)0.9 & 19.1\(\)0.6 \\ IsCiL (ours) & 69.5\(\)1.5 & 16.3\(\)1.2 & **84.4\(\)1.3** & **52.1\(\)**5.7 & **72.8\(\)**1 & 64.3\(\)2.6 & -0.5\(\)1.5 & **63.9\(\)**0.8 & **45.8\(\)**4.7 & **45.3\(\)**0.9 \\   Multi-task & 85.3\(\)1.7 & 3.7\(\)1.8 & **88.8\(\)**0.0 & 70.8\(\)0.0 & **79.0\(\)**0.5 & 85.4\(\)0.9 & 56.6\(\)0.5 & 90.4\(\)0.3 & 78.3\(\)0.9 & 85.9\(\)0.6 \\   

Table 2: Task adaptation performance with unseen tasks: This is based on the existing Evolving World-_Complete_ and Evolving Kitchen-_Complete_. In Evolving World, four novel tasks are introduced every four stages, while in Evolving Kitchen, two novel tasks are introduced every five stages. Metrics with the suffix -A denote performance based solely on adaptation tasks, while other metrics report performance across all tasks.

  
**Stream** &  &  \\  Algorithm & FWT-(s) & BWT (s) & AUC (s) & FWT (s) & BWT (s) & AUC (s) \\  TAIL-\(\) CLPU & 86.2\(\)5.6 & 0.0\(\)0.0 & **86.2\(\)5.6** & 33.8\(\)3.0 & 0.0\(\)0.0 & **33.8\(\)3.0** \\ IsCiL (ours) & 75.0\(\)7.2 & 11.2\(\)5.5 & **85.2\(\)**1.8 & 61.4\(\)2.9 & 12.4\(\)2.9 & **72.7\(\)2.9** \\   

Table 3: Overall performance with task unlearning as task adaptation: Additional stages for unlearning tasks that were learned during other stages are included for tests.

for training. IsCiL achieves the highest AUC in all environments and is the only approach where AUC surpasses FWT. ER shows comparable FWT in _Complete_, but as the number of stored samples increases, FWT decreases, indicating that more rehearsals actually reduce training sample efficiency. In _Semi_ and _Incomplete_, using 250 rehearsals (approximately 5% of the stage dataset) yields FWT comparable to IsCiL but rarely improves AUC.

**Limited training resource.** Figure 5 shows the computational efficiency of IsCiL in resource-constrained training settings, as discussed in . In this experiment, the training resources is limited to 1% to 50% of those used in Table 1. IsCiL and TAIL-\(\) show robust performance for varied training resources. TAIL-\(g\) shows higher FWT, as it trains the same sub-goal data on the same adapter, which excels in learning new tasks, but it fails to retain that knowledge. However, using skill data from different stages to update the same adapter makes it vulnerable to skill distribution shifts in CiL; this ends up with significant AUC degradation.

### Ablation

Table 4 investigates the impact of the number of prototype bases on CiL performance, showing that increasing the number of bases improves both AUC and result stability, particularly around K=10. Results are reported based on units (\(g\) and \(\)) used to construct new skill prototypes and the corresponding number of bases. IsCiL with a single base fails to effectively learn task knowledge, achieving similar performance to L2M in Table 1, due to insufficient representation of the skill distribution. Additionally, the IsCiL framework maintains positive BWT scores, demonstrating its ability to leverage future samples to enhance past performance. IsCiL with \(\), which constructs new skills based on entire task trajectories, required more bases in proportion to the increase in the number of transitions involved in constructing the skill trajectory to maintain stability.

    &  \\  Ablations & FWT (\%) & BWT (\%) & AUC (\%) \\  IsCiL \(g\), \(|_{z}|=20\) & 79.3\(\)1.7 & 11.0\(\)1.6 & 89.8\(\)0.5 \\  IsCiL \(g\), \(|_{z}|=1\) & 28.9\(\)10.2 & 2.3\(\)5.7 & 30.6\(\)12.2 \\ IsCiL \(g\), \(|_{z}|=5\) & 63.1\(\)0.0 & 2.7\(\)7.3 & 66.5\(\)7.9 \\ IsCiL \(g\), \(|_{z}|=10\) & 76.4\(\)6.5 & 8.2\(\)4.0 & 83.9\(\)2.7 \\ IsCiL \(g\), \(|_{z}|=25\) & 77.1\(\)1.8 & 11.9\(\)1.7 & 88.2\(\)1.1 \\ IsCiL \(g\), \(|_{z}|=50\) & 81.5\(\)2.8 & 7.9\(\)4.9 & 89.4\(\)1.2 \\  IsCiL \(\), \(|_{z}|=20\) & 57.8\(\)1.6 & 10.9\(\)1.8 & 67.2\(\)17.2 \\ IsCiL \(\), \(|_{z}|=80\) & 84.3\(\)6.7 & 5.0\(\)7.5 & 89.5\(\)8.3 \\   

Table 4: Ablation on IsCiL skill prototype

Figure 4: Comparison w.r.t. the number of rehearsals: The horizontal axis represents the amount of stored rehearsal data at each stage, while the vertical axis indicates goal-conditioned success rates (GC).

Figure 5: Comparison w.r.t. training resources: In all baselines, the plain bar graph represents FWT, while the bar graph with hatch marks represents AUC. The vertical axis indicates goal-conditioned success rates (GC).

Conclusion

In this study, we presented the IsCiL framework to address key challenges in continual imitation learning (CiL). Our approach incorporates adapter-based skill learning, leveraging multifaceted skill prototypes and an adapter pool to effectively capture the distribution of skills for continual task adaptation. IsCiL specifies enhanced sample efficiency and robust task adaptation, effectively bridging the gap between adapter-based CiL approaches and the need for knowledge sharing across staged demonstrations. Comprehensive experiments demonstrate that IsCiL consistently outperforms other adapter-based continual learning approaches in various CiL scenarios.

**Limitations.** Like other adapter-based CiL approaches, IsCiL requires extra computation for evaluation, which can create overhead, especially in resource-constrained environments. It also depends on sub-goal sequences for training and evaluation, adding complexity and resource demands. Another limitation is determining the appropriate size of the adapter parameters, which depends on the performance of the pre-trained base model and the degree of task shift, making optimal adaptation challenging. Moreover, balancing the stability of the embedding function with the prototype size remains an area that requires further refinement to achieve optimal performance.