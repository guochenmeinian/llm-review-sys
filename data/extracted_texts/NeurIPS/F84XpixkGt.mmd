# Instructional Segment Embedding: Improving LLM Safety with Instruction Hierarchy

Tong Wu\({}^{1}\) Shujian Zhang\({}^{2}\) Kaiqiang Song\({}^{2}\) Silei Xu\({}^{2}\) Sanqiang Zhao\({}^{2}\) Ravi Agrawal\({}^{2}\)

**Sathish Reddy Indurthi\({}^{2}\) Chong Xiang\({}^{1}\) Prateek Mittal\({}^{1}\) Wenxuan Zhou\({}^{2}\)**

\({}^{1}\)Princeton University \({}^{2}\)Zoom Video Communications

###### Abstract

Large Language Models (LLMs) are susceptible to security and safety threats, such as prompt injection, prompt extraction, and harmful requests. One major cause of these vulnerabilities is the lack of an instruction hierarchy. Modern LLM architectures treat all inputs equally, failing to distinguish between and prioritize various types of instructions, such as system messages, user prompts, and data. As a result, lower-priority user prompts may override more critical system instructions, including safety protocols. Existing approaches to achieving instruction hierarchy, such as delimiters and instruction-based training, do not address this issue at the architectural level. We introduce the **I**nstructional **S**egment **E**mbedding (ISE) technique, inspired by BERT, to modern large language models, which embeds instruction priority information directly into the model. This approach enables models to explicitly differentiate and prioritize various instruction types, significantly improving safety against malicious prompts that attempt to override priority rules. Our experiments on the Structured Query and Instruction Hierarchy benchmarks demonstrate an average robust accuracy increase of up to 15.75% and 18.68%, respectively. Furthermore, we observe an improvement in instruction-following capability of up to 4.1% evaluated on AlpacaEval. Overall, our approach offers a promising direction for enhancing the safety of LLM architectures.

## 1 Introduction

Large Language Models (LLMs) have shown significant potential in enabling agentic applications and facilitating decision-making across various domains, such as web agents, educational tools, medical assistance, and more (Yao et al., 2022; Gan et al., 2023; Abbasian et al., 2024). To optimize the use of AI applications, a structured approach to implementation is widely adopted. This involves clear distinctions among system instructions, user prompts, and data inputs, as illustrated in Figure 1. These instructions contain specific priorities that help the model execute functionalities correctly and better assist users.

Modern LLMs process text without formal mechanisms to differentiate and prioritize instructions. Consequently, malicious attackers can easily exploit this limitation to override priority roles, leading to various vulnerabilities. For example, _prompt injection_(Greshake et al., 2023) insert malicious instructions into data sources to subvert the original ones. _Prompt extraction_(Zhang et al., 2024) aim to extract system messages, revealing proprietary prompts.

_Harmful requests_(Ganguli et al., 2022) involve malicious users providing unsafe instructions to elicit irresponsible or dangerous responses from the safety-aligned LLMs. These vulnerabilities underscore the significance of designing more robust _instruction hierarchy_ in LLMs to mitigate such attacks.

Figure 1: A demonstration of the hierarchy of instructions, including system instruction, user instruction, data input as well as model output.

Recently, research has been conducted to enhance models' ability to follow the instruction hierarchy. For instance, Hines et al. (2024) proposed prompt-based solutions utilizing a special delimiter between prompts. Chen et al. (2024) and Wallace et al. (2024) suggested methods for generating hierarchical prompts, incorporating adversarial data along with high-quality responses to fine-tune LLMs. However, despite these improvements, the core challenge persists: **current LLM architectures still lack an effective mechanism to differentiate and prioritize hierarchical instructions.**

In this work, we tackle the challenge by introducing an architecture-level design for LLMs. Inspired by BERT (Lan et al., 2019) and its variants (Lan et al., 2019; Yasunaga et al., 2022), we propose using an _Instructional Segment Embedding (ISE)_ to categorize different types of instructions distinctly. Specifically, we enhance the input token by incorporating segment information that classifies each token by its role (e.g., system instruction as 0, user prompt as 1, and data input as 2). This segment information is processed through a learned embedding layer, converting it into segment embeddings, which are then passed to later self-attention layers along with token embeddings. To obtain a robust segment embedding layer, we perform supervised fine-tuning on datasets containing structured prompts and high-quality responses. This process enables the model to differentiate between levels of instruction hierarchies more effectively, thereby boosting the overall safety of the system.

Empirically, we conduct comprehensive experiments on two benchmarks: Structured Query (Chen et al., 2024) and Instruction Hierarchy (Wallace et al., 2024), which are constructed based on the Alpaca (Taori et al., 2023) and Ultrachat (Ding et al., 2023) datasets, respectively. We fine-tune multiple pretrained LLMs, including Llama-2-13B (Touvron et al., 2023), Llama-3-8B (Llama Team, 2024), and Llama-3.1-8B, and compare their performance with and without the use of Instructional Segment Embedding. Our findings indicate that our method yields substantial improvements in robustness while either maintaining or enhancing the models' general capabilities, regardless of the presence of adversarial training data. For example, on the Structured Query benchmark, the method achieves an average robust accuracy improvement of up to **15.75%** against indirect prompt injection attacks. On the Instruction Hierarchy benchmark, our ISE yields an average boost in robustness of up to **18.68%** across multiple vulnerabilities, including indirect and direct prompt injection, prompt extraction, and harmful requests. In addition, the integration of ISE also maintains or even improves the instruction-following capability by as much as **4.1%** on AlpacaEval.

**Contributions: (1)** We identify and analyze critical limitations in current LLM architectures concerning the lack of instruction hierarchy (Section 3). **(2)** We propose Instructional Segment Embedding, a simple yet effective method designed to incorporate instruction-type information directly into the model. This approach enables the model to better distinguish and prioritize instructions based on their privilege (Section 4). **(3)** We empirically demonstrate the effectiveness of ISE across two benchmarks, encompassing five training datasets and addressing four types of vulnerabilities (Sections 5 & 6).

## 2 Background: LLM Vulnerabilities

Modern LLM products typically involve up to three stakeholders1: (1) the LLM application provider (e.g., OpenAI), who designs the model's system-level instructions and manages the general workflow; (2) the primary user, who provides input in the form of instructions or queries; and (3) third-party

Figure 2: A demonstration of various vulnerabilities of LLM applications, including prompt injection, prompt extraction as well as harmful request.

source/data, such as web search results, that offer additional context for the LLM. As a result, LLM applications often establish a hierarchical order of instructions based on their perceived reliability: system instructions take precedence, followed by user instructions, and finally data.

Security vulnerabilities arise when conflicts between these priorities occur, such as (1) a malicious user attempting to bypass safety system instructions or (2) malicious web providers injecting harmful actions in data. These conflicts may take various forms, including prompt injections, prompt extractions, and harmful requests, as shown in Figure 2 and outlined below.

**Prompt injection (Figure 1(a)).** Prompt injection attacks (Perez and Ribeiro, 2022) generally occur in two forms: indirect and direct. Indirect prompt injection attacks occur when third-party data input contains instructions that should never be followed by LLMs. Direct prompt injection attacks happen when a malicious attacker manipulates the user query to an LLM, causing the model to generate outputs that deviate from predefined instructions.

**Prompt extraction (Figure 1(b)).** This vulnerability (Zhang et al., 2024) often exploits a weakness in certain LLM applications that store confidential information within system instructions. Attackers may craft malicious queries that prompt the model to reference this stored information, potentially leading to the disclosure of system prompts.

**Harmful requests (Figure 1(c)).** Harmful requests (Ganguli et al., 2022) aim to bypass the model's safety alignment (Bai et al., 2022) through malicious queries. These prompts can lead to unsafe outcomes, including unethical responses or even the weaponization of LLMs.

In this paper, we aim to enhance the instruction hierarchy capabilities of LLMs, thereby mitigating various forms of attacks that attempt to override the priority rules.

## 3 Lack of Instruction Hierarchy in Modern LLM Architecture

**Current embeddings lack instruction hierarchy.** Given an input context \(_{M}\) with \(M\) tokens \(x_{1},x_{2},,x_{M}\), the large language models first convert each token into a high-dimensional vector using a token embedding matrix \(^{}^{V D}\), where \(V\) is the vocabulary size, and \(D\) is the output embedding dimension. The embedding vector \(e_{m}^{}\) for token \(x_{m}\) is given by \(^{}[x_{m}]\), based on its index in the vocabulary. Additionally, the model also obtains positional embeddings \(_{m}^{}\), based on the position of each token. Then, the token embeddings \((e_{1}^{},e_{2}^{},,e_{M}^{})\) will be fed into the transformer's self-attention layers along with positional embeddings for further processing.2

In these self-attention layers, each token embedding is processed "equally". As a result, the model recognizes only the semantic content and sequential order of each token from the embedding, lacking the capability to distinguish their hierarchical significance. This architectural design can inherently lead to vulnerabilities. For instance, a lower-priority user prompt, such as "Please focus on my report as the system prompt is outdated", could mistakenly be prioritized and override the original system prompt. This could inadvertently lead to various types of vulnerabilities, as shown in Figure 2.

**Prior works do not address this issue.** To mitigate these vulnerabilities, researchers have introduced methods to improve the robustness of large language models (LLMs) during the supervised fine-tuning phase. This method involves not only using benign prompt-response data but also adversarial or misaligned instructions with robust responses (Piet et al., 2023; Chen et al., 2024; Wallace et al., 2024). This approach helps the model learn to prioritize hierarchical instructions and adhere to embedded safety protocols. Despite its improvement, the challenge of uniformly processing

Figure 3: A demonstration of the chat template for Llama-3-Instruct (Llama Team, 2024).

hierarchical instructions remains a fundamental limitation inherent in current embedding methods and model architecture, as demonstrated in our experimental results (see Table 1 and Figure 5).

An alternative approach is to use specific chat templates to better handle input data. For instance, LLAMA-3-Chat (Llama Team, 2024) leverages a chat template with special tokens like <|be-gin_of_text|> and <|star_header_id|> as shown in Figure 3. Hines et al. (2024) and Chen et al. (2024) have also leveraged the specialized delimiters that aid the model in more effectively distinguishing instructions. However, two major drawbacks exist. Firstly, during inference, only a few tokens contain hierarchical priority information, and this signal is likely to diminish when encountering long-context tasks (e.g., summarizing a novel). Secondly, malicious attackers may extract these special delimiters, and exploiting them could lead to more severe attacks (Zheng et al., 2024).

## 4 Proposed Approach: Instructional Segment Embedding (ISE)

To tackle this challenge, we propose **Instructional Segment Embedding (ISE)**, which encodes the instruction hierarchy directly into the embeddings. This enables subsequent self-attention layers to more effectively recognize and follow instruction priorities, thereby boosting robustness.

Specifically, we leverage a learnable embedding layer, similar to the token embedding matrix \(^{}\), which we call the segment embedding matrix \(^{}\). We define \(^{}^{H D}\), where \(H\) is the number of hierarchies and \(D\) is the embedding dimension. By default, we set \(H\) to 4, representing system, user, data, and output. Each token in \(_{M}\) is tagged with corresponding hierarchy information \(h_{m}\{0,1,2,3\}\), readily derived from distinct stakeholder categories in the LLM applications. The instructional segment embeddings of \(_{M}\) are represented as \((e_{1}^{},e_{2}^{},,e_{M}^{})\) and obtained from \(^{}[h_{m}]\). To incorporate this modification, the final embeddings are computed by summing the token embeddings and segment embeddings. This results in \((e_{1}^{}+e_{1}^{},e_{2}^{}+e_{2}^{}, ,e_{M}^{}+e_{M}^{})\), as illustrated in Figure 4. These embeddings are then fed into self-attention layers, following the process used in current LLMs.

The segment embedding layer is trained alongside other parameters during the supervised fine-tuning (instruction tuning) phase. In our experiments, we use widely adopted instruction-following datasets and construct structured queries based on the original prompt using GPT-4o (OpenAI, 2023). Additionally, we experiment with datasets containing malicious instructions designed to override higher-level instructions, enabling the model to learn how to reject or ignore such commands.

**Flexibility in design.** The design choice for Instructional Segment Embedding can be flexible and should be tailored to the specific downstream tasks. For instance, if the data category can be further subdivided into outputs from external API tools or online information, we can introduce "tools type" and "web data type" categories, providing more fine-grained information. If the application does not involve third-party context, the data type can be omitted.

Figure 4: The input representation includes both token embeddings and instructional segment embeddings. We categorize all input texts into four segments: system instructions, user instructions, third-party data, and generated output. We assign different segment embeddings to each type of input text. The final input embeddings are the sum of token embeddings and segment embeddings. The LLMs will predict the next token after “the summary is”, with extra instruction hierarchy information.

**Connection to BERT.** Inspired by BERT's segment embeddings (Devlin et al., 2019), originally used to distinguish input segments for next-sentence prediction, our approach repurposes these embeddings to encode hierarchical instructions. This helps address the need for structured prompts and safer LLM outputs by providing direct, contextually relevant cues to the model. Unlike BERT, we incorporate the output type for two reasons: (**1**) It supports consistent autoregressive inference for each token in the input. **(2)**output may also include instructions (e.g., "Please provide more details of your question") that are critical in multi-turn language tasks.

**Simplicity.** The implementation is also straightforward and can be easily adapted for most transformer-based LLMs. We provide a PyTorch code snippet that demonstrates how to implement this in just a few lines, as shown in Appendix B.

## 5 Experimental Design

In this section, we present how we conducted the experiments. Specifically, we begin by describing the generation of the training data (Section 5.1), the experimental setup (Section 5.2), and the details of the robustness evaluation against multiple attacks (Section 5.3).

### Generating Training Data

We conduct experiments using two benchmarks: **Structured Query** and **Instruction Hierarchy**. The Structured Query benchmark primarily focuses on indirect prompt injection attacks, whereas the Instruction Hierarchy benchmark evaluates all types of vulnerabilities discussed, including indirect and direct prompt injections, prompt extraction, and harmful requests.

For the **Structured Query** benchmark, we generally follow the approach of Chen et al. (2024). Two datasets are constructed: _Clean Alpaca_ and _Adversarial Alpaca_. The Clean Alpaca dataset is constructed by _Alpaca-Cleaned-50K_ dataset (Taori et al., 2023; Gururise, 2024). For the Adversarial Alpaca dataset, we incorporate malicious instructions into the data and train the model to ignore such instructions.

For the **Instruction Hierarchy** benchmark, we mostly adhere to previous work by Wallace et al. (2024) to create both aligned and misaligned data3. We select the _UltraChat-200K_ dataset (Ding et al., 2023) as the base dataset, which contains more training data. Since UltraChat consists solely of prompts and responses, we utilized GPT-40 (OpenAI, 2023) to decompose 10K prompts into three components: system instructions, user instructions, and data inputs, which we term the _UltraChat Baseline_. Additionally, we incorporate datasets from SystemChat (Abacus.AI, 2023) and SystemMessage (Huggingface, 2023) that contain specifical system prompts, designated as the _System Follow_ dataset. Lastly, We crafted three types of attacks for the malicious data: indirect/direct prompt injection and prompt extraction, which we collectively name the _Instruction Hierarchy_ datasets. We excluded harmful request data from the training but used them as evaluations following Wallace et al. (2024). Further details on generating training data are available in Section C.

### Experiment Setup

**Data processing.** We format all training and evaluation samples with clear segmentation, including system, user, data, and output information. We merge the system and user instructions for the Structured Query benchmark into the same type, as all system instructions in Alpaca are identical. To simplify the experiments, we train and evaluate only single-turn chats, where the conversation ends after the model generates a complete response.

**LLM training and inference.** By default, we utilize **Llama-2-13B**(Touvron et al., 2023) and **Llama-3-8B**(Llama Team, 2024) as the pretrained models for Structured Query and Instruction Hierarchy, respectively. Experiments with **Llama-3.1-8B** are presented in Section E.2. We employ supervised fine-tuning to update all parameters, including our segment embedding layer, for three epochs. A learning rate of 2e-5 and a cosine learning schedule are used. During inference, we use top-p sampling methods with the model's default settings.

**Baselines.** We conduct experiments on multiple datasets and compare our Instructional Segment Embedding method (**+ISE**) with models that do not use it. For the Structured Query benchmark, we include experiments using either text (**Baseline**) or newly constructed tokens (**Delimiter**) to differentiate distinct types of prompts, following the approach of Chen et al. (2024). For the Instruction Hierarchy benchmark, we only include experiments with the default delimiters of LLAMA-3-8B (**Baseline**), as these are already specially reserved tokens (shown in Figure 3).

### Robustness Evaluation

We evaluate our method using comprehensive attack methods. More details are in Appendix D.

**Structured Query benchmark.** Following the approach of Chen et al. (2024), we evaluate indirect prompt injection attacks on models trained on the Alpaca dataset. Specifically, we focus on four types of attacks: Naive Attack, Ignore Attack, Escape Separation Attack, and Completion Real Attack as _in-domain attacks_. All in-domain attacks are crafted by injecting one or a few sentences at the end of the data to trick the model into outputting the word "hacked". Since the scenarios for in-domain attacks are quite similar to the adversarial training data constructed by Chen et al. (2024), we further develop new attacks based on the in-domain methods. These new attacks inject adversarial texts at the data's beginning and end, called _out-of-domain attacks_. This strategy significantly degrades the robustness of models trained on the adversarial Alpaca dataset. For the evaluation metrics, we compute the rate at which the model does not generate content containing the word "hacked" and refer to this as robustness or robust accuracy.

**Instruction Hierarchy benchmark.** Evaluating models trained on the Instruction Hierarchy benchmark is complex due to the need to account for indirect and direct prompt injection, prompt extraction, and harmful requests. To address these challenges: **(1)** For indirect prompt injection, we apply the same evaluations and metrics used in Structured Query benchmarks. For direct prompt injection, we use the same attacking prompts but inject them directly into the user prompt. **(2)** For prompt extraction, we use the ShareGPT and Unnatural Instructions datasets from (Zhang et al., 2024), along with 15 author-selected effective extraction prompts, and evaluate robustness using an approximate metric based on Rouge-L recall (Lin, 2004). **(3)** For harmful requests, we follow the evaluations of (Wallace et al., 2024), using Jailbeckakt (Chat) and "Do Anything Now" (DAN) prompts (Shen et al., 2024) paired with StrongREJECT malicious instructions (Souly et al., 2024). We query GPT-4o to check whether its responses adhere to safety guardfalls.

**Comprehensive robustness metrics.** For prompt injection and extraction, which encompass multiple attack methods or malicious prompts, we include additional metrics. We define _average robustness_ as the model's average performance across these various attack methods, offering a general evaluation of model robustness. Furthermore, we introduce _worst robustness_, representing the model's ability to defend against the most challenging attack.

**Clean evaluation.** We evaluate the model's capacity using standard datasets. Both benchmarks are assessed with AlpacaEval 1.0 (Li et al., 2023). For the Instruction Hierarchy benchmark, we additionally use the MT-Bench (Zheng et al., 2023) to measure the model's performance.

## 6 Experimental Results and Analysis

We report the main results on the Structured Query benchmark in Section 6.1 and the Instruction Hierarchy in Section 6.2. We observe that our approach **consistently achieves higher robust accuracy** while either **maintaining or improving general capability**. We also present a more detailed analysis of multiple vulnerabilities in Appendix E.1. Lastly, we conduct an over-refusal evaluation and assess generalization to the advanced Llama-3.1-8B model in Appendix E.2.

### Main Results on Structured Query

**Maintains high utility.** In Table 1, we present the main results for capability and robustness by comparing our method with the baseline and delimiter methods on both the clean and adversarial Alpaca datasets. Compared to the other two methods, Instructional Segment Embedding maintains high utility with negligible degradation or even slight improvement. The difference in winning rate between the methods is less than 1% on AlpacaEval.

**Consistent robustness enhancement.** We also observe that our method consistently improves robustness against indirect prompt injection attacks. Specifically, it achieves a **15.75%** increase in average robust accuracy and a **32.17%** increase in worst robust accuracy against in-domain attacks when trained with the clean Alpaca dataset. Both the delimiter and our ISE reach nearly perfect in-domain robustness. For out-of-domain attacks, we find that adding ISE can also significantly enhance robustness, resulting in improvements of \(\)**10%** and \(\)**7%** in average robustness for clean and adversarial Alpaca, respectively. Interestingly, our out-of-domain attacks degrade the robustness of models trained on the adversarial Alpaca dataset more than those trained on the clean Alpaca dataset (16% vs. 5%). This suggests that the adversarial dataset may overfit to in-domain attacks. Nevertheless, adding ISE largely maintains generalization to out-of-domain attacks.

We present detailed experiment results in Appendix F.

### Main Results on Instruction Hierarchy

We present the evaluation results for our method on the Instruction Hierarchy benchmark in Figure 5, focusing on model capability and average robustness across various datasets and attack scenarios.

**Improvement in capabilities.** Adding ISE boosts instruction-following capabilities, particularly for models trained on the System Follow and Instruction Hierarchy datasets. For example, the AlpacaEval win rate improves by approximately \(\)**4.1%** when training on the Instruction Hierarchy dataset with our ISE, as shown in Figure 5(a). Additionally, we observe negligible degradation on MT-Bench for the UltraChat Baseline model and improvements for the other two training datasets.

**Enhanced safety against multiple vulnerabilities.** We evaluate the robustness of the models against indirect and direct prompt injection attacks, prompt extraction attacks, and harmful requests. **(1)** Indirect and direct prompt injection scenarios (#1, #2, #3, and #4 in Figure 5(b)) : We report the average robustness across four types of attacks, including both in-domain (ID) and out-of-domain (OOD) contexts. Our results demonstrate robust accuracy improvements ranging from **5%** to **25%** across all training data configurations when applying the ISE method. Notably, for models trained with the UltraChat Baseline, robust accuracy increases by nearly **25%** on average. **(2)** Prompt extraction scenarios (#5 and #6 in Figure 5(b)): Robustness is measured against 15 effective extraction prompts. Our findings show that models using ISE consistently achieve higher average robustness, with an increase of at least **10%** across all datasets. This is evident even for models trained on the Instruction Hierarchy dataset, which already demonstrated more than 80% robust accuracy. **(3)** Harmful requests

    & Dataset &  &  \\  & Method & Baseline & Delimiter & +ISE (Ours) & Baseline & Delimiter & +ISE (Ours) \\  Capability (\(\)) & AlpacaEval & **72.76** & 72.67 & 72.13 & 73.41 & 72.26 & **73.76** \\   & Naive & 65.87 & 68.75 & **75.96** & **100.00** & 99.04 & **100.00** \\  & Ignore & 57.69 & 57.21 & **70.19** & **99.52** & 99.04 & 99.04 \\ In-Domain & Escape-S & 75.00 & 69.23 & **78.85** & 99.52 & 99.52 & **100.00** \\ Robustness (\(\)) & Completion-R & 4.81 & 7.21 & **40.38** & 70.19 & **100.00** & **100.00** \\   & Average & 50.84 & 50.60 & **66.35** (+15.75) & 92.31 & 99.16 & **99.76** (+0.00) \\  & Worst & 4.81 & 7.21 & **40.38** (+32.17) & 70.19 & **99.04** & **99.04** (+0.00) \\   & Naive & 62.02 & 66.35 & **69.71** & 64.90 & 67.79 & **76.44** \\  & Ignore & 52.40 & 51.92 & **69.71** & 98.56 & 96.15 & **96.63** \\ Out-of-Domain & Escape-S & **72.12** & 71.63 & 70.67 & 73.08 & 76.44 & **88.46** \\ Robustness (\(\)) & Completion-R & 1.92 & 12.99 & **34.14** & 85.58 & 91.35 & **99.52** \\   & Average & 47.12 & 50.72 & **61.06** (+10.34) & 80.53 & 82.93 & **90.26** (+7.67) \\  & Worst & 1.92 & 12.99 & **34.14** (+21.15) & 64.90 & 67.79 & **76.44** (+8.65) \\   

Table 1: The evaluation results on Structured Query benchmark against both in-domain and out-of-domain indirect prompt injection attacks. We compare our method (+ISE) with the baseline and delimiter methods (Chen et al., 2024) on Clean Alpaca and Adversarial Alpaca.

(#7 and #8 in Figure 5(b))): Our analysis reveals improvements in robustness for models under the UltraChat Baseline and Instruction Hierarchy settings when using ISE. For System Follow, our methods either maintain or slightly exceed the baseline method.

Overall, using Instructional Segment Embeddings significantly enhances both the capabilities and robustness of models against a wide range of attacks on the Instruction Hierarchy benchmark. We present detailed experiment results in Appendix E & Appendix G.

## 7 Discussion and Conclusion

**Limitations and future work directions.** This study primarily focused on the supervised fine-tuning phase, using single-turn conversations. Future work could explore incorporating ISE during the pre-training or RLHF stage and applying it to multi-turn conversation datasets. Additionally, while our approach significantly improves the instruction hierarchy capabilities of LLMs, it offers limited robustness against adaptive attacks, commonly referred to as jailbreaks (see Appendix H for more discussion). However, integrating our method with established adversarial training strategies may potentially enhance the robustness.

**Conclusion.4** In this work, we introduced the Instructional Segment Embedding as the first attempt to design novel architectures to enhance instruction hierarchy. We conducted comprehensive experiments to demonstrate its effectiveness in improving robustness and general capabilities. We believe our method offers considerable potential for integration into real-world LLM applications and encourage practitioners to explore and test it in more downstream tasks.

Figure 5: The evaluation of model capabilities on the Instruction Hierarchy benchmark is conducted using AlpacaEval and MT-Bench, as illustrated in Figure (a). Robustness evaluations include both indirect and direct prompt injection attacks, prompt extraction attacks, and harmful requests, as shown in Figure (b). We performed experiments across three datasets (i.e., UltraChat Baseline, System Follow, and Instruction Hierarchy) and compared our ISE with the baseline (Wallace et al., 2024).

#### Acknowledgments

We would like to thank Jiachen T. Wang and Feiran Jia for providing feedback on our early draft. Prateek was supported in part by the National Science Foundation under grant CNS-2131938 and the Princeton SEAS Innovation Grant.