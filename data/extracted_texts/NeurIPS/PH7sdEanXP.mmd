# Scaling Laws in Linear Regression:

Compute, Parameters, and Data

Licong Lin

UC Berkeley

liconglin@berkeley.edu &Jingfeng Wu

UC Berkeley

uuujf@berkeley.edu &Sham M. Kakade

Harvard University

sham@seas.harvard.edu &Peter L. Bartlett

UC Berkeley and Google DeepMind

peter@berkeley.edu &Jason D. Lee

Princeton University

jasonlee@princeton.edu

###### Abstract

Empirically, large-scale deep learning models often satisfy a neural scaling law: the test error of the trained model improves polynomially as the model size and data size grow. However, conventional wisdom suggests the test error consists of approximation, bias, and variance errors, where the variance error increases with model size. This disagrees with the general form of neural scaling laws, which predict that increasing model size monotonically improves performance.

We study the theory of scaling laws in an infinite dimensional linear regression setup. Specifically, we consider a model with \(M\) parameters as a linear function of sketched covariates. The model is trained by one-pass stochastic gradient descent (SGD) using \(N\) data. Assuming the optimal parameter satisfies a Gaussian prior and the data covariance matrix has a power-law spectrum of degree \(a>1\), we show that the reducible part of the test error is \((M^{-(a-1)}+N^{-(a-1)/a})\). The variance error, which increases with \(M\), is dominated by the other errors due to the implicit regularization of SGD, thus disappearing from the bound. Our theory is consistent with the empirical neural scaling laws and verified by numerical simulation.

## 1 Introduction

Deep learning models, particularly those on a large scale, are pivotal in advancing the state-of-the-art across various fields. Recent empirical studies have shed light on the so-called _neural scaling laws_ (see 26, 21, for example), which suggest that the generalization performance of these models improves polynomially as both model size, denoted by \(M\), and data size, denoted by \(N\), increase. The neural scaling law quantitatively describes the population risk as:

\[(M,N)^{*}+}{M^{a_{1}}}+}{ N^{a_{2}}},\] (1)

where \(^{*}\) is a positive irreducible risk and \(c_{1}\), \(c_{2}\), \(a_{1}\), \(a_{2}\) are positive constants independent of \(M\) and \(N\). For instance, by fitting the above formula with empirical measurements in standard large-scale language benchmarks, Hoffmann et al. (2017) estimated \(a_{1} 0.34\) and \(a_{2} 0.28\), while Besiroglu et al. (2017) estimated that \(a_{1} 0.35\) and \(a_{2} 0.37\). Though the exact exponents depend on the tasks, neural scaling laws in (1) are observed consistently in practice and are used as principled guidance to build state-of-the-art models, especially under a compute budget (Kang et al., 2017).

From the perspective of statistical learning theory, (1) is rather intriguing. Standard statistical learning bounds (see 30, 41, for example) often decompose the population risk into the sum of irreducibleerror, approximation error, bias error, and variance error (some theory replaces bias and variance errors by optimization and generalization errors, respectively) as in the form of

\[(M,N)=^{*}+}}}_{}+ }}}_{}+ }}}_{},\] (2)

where \(a_{1},a_{2},a_{3}\) are positive constants and \(c(M)\) is a measure of _model complexity_ that typically increases with the model size \(M\). In (2), the approximation error is induced by the mismatch of the best-in-class predictor and the best possible predictor, hence decreasing with the model size \(M\). The bias error is induced by the mismatch of the expected algorithm output and the best-in-class predictor, hence decreasing with the data size \(N\). The variance error measures the uncertainty of the algorithm output, which decreases with the data size \(N\) but increases with the model size \(M\) (since the model complexity \(c(M)\) increases).

A mystery.The empirical neural scaling law (1) is incompatible with the typical statistical learning theory bound (2). While the two error terms in the neural scaling law (1) can be explained by the approximation and bias errors in the theoretical bound (2) respectively, it is not clear why _the variance error is unobservable when fitting the neural scaling law empirically_. This difference must be reconciled, otherwise, the statistical learning theory and the empirical scaling law make conflict predictions: as the model size \(M\) increases, the theoretical bound (2) predicts an increase of variance error that eventually causes an increase of the population risk, but the neural scaling law (1) predicts a decrease of the population risk. In other words, it remains unclear when to follow the prediction of the empirical scaling law (1) and when to follow that of the statistical learning bound (2).

Certain prior works provided risk upper bounds that do not grow with model size (see for example [36; 12]). Still, their results are insufficient for studying scaling law as those bounds require a large model size such that the approximation error is ignorable. Moreover, they do not provide instance-wise matching lower bounds to verify the tightness of the upper bounds. See a detailed discussion in Section 2.

Our explanation.We investigate this issue in an infinite dimensional linear regression setup. We only assume access to \(M\)-dimensional sketched covariates given by a fixed Gaussian sketch and their responses. We consider a linear predictor with \(M\) trainable parameters, which is trained by one-pass _stochastic gradient descent_ (SGD) with geometrically decaying stepsizes using \(N\) sketched data. Assuming that the spectrum of the data covariance matrix satisfies a power-law of degree \(a>1\) and that the optimal model parameters satisfy a Gaussian prior, we derive matching upper and lower bounds on the population risk achieved by the SGD output (see Theorem 4.1). Specifically, we show

Figure 1: The expected risk (Risk) of the last iterate of (SGD) versus the effective sample size \(N_{}\) and the model size \(M\) for different power-law degrees \(a\). The expected risk is computed by averaging over \(1000\) independent samples of (\(^{*}\), \(\)). We fit the expected risk using the formula Risk \(^{2}+c_{1}/M^{a_{1}}+c_{2}/N^{a_{2}}\) via minimizing the Huber loss as in . Parameters: \(=1,=0.1\). Left: For \(a=1.5\), \(d=20000\), the fitted exponents are \((a_{1},a_{2})=(0.54,0.34)(0.5,0.33)\). Right: For \(a=2\), \(d=2000\), the fitted exponents are \((a_{1},a_{2})=(1.07,0.49)(1.0,0.5)\). Note that the values of \((a_{1},a_{2})\) are close to our theoretical predictions \((a-1,1-1/a)\) in both cases, verifying the sharpness of our risk bounds. More details can be found in Sections 4 and 5.

that

\[(M,N)=^{*}+\ } +}}_{},=\}}{N}}_{},\]

where \(=(1)\) is the initial stepsize used in SGD and \(()\) hides \((N)\) factors. In our bound, the sum of the approximation and bias errors determines the order of the excess risk, while the variance error is of a strictly higher order and is therefore nearly unobservable when fitting \((M,N)\) as a function of \(M\) and \(N\) empirically. In addition, our analysis reveals that the small variance error is due to the implicit regularization effect of one-pass SGD . Our theory suggests that the empirical neural scaling law (1) is a simplification of the statistical learning bound (2) in a special regime when strong regularization (either implicit or explicit) is employed.

Moreover, we generalize the above scaling law to (1) constant stepsize SGD with iterate average (see Theorem F.6), (2) cases where the optimal model parameter satisfies an anisotropic prior (see Theorem 4.2), and (3) where the spectrum of the data covariance matrix satisfies a logarithmic power law (see Theorem 4.3).

Emprical evidence.Based on our theoretical results, we conjecture that the clean neural scaling law (1) observed in practice is due to the disappearance of variance error caused by strong regularization. Two pieces of empirical evidence to support our understanding. First, large language models that follow the scaling law (1) are often _underfitted_, as the models are trained over a single pass or a few passes over the data [27; 31; 9; 39]. When models are underfitted, the variance error tends to be smaller. Second, when language models are trained with multiple passes (up to \(7\) passes), Muennighoff et al.  found that the clean scaling law in (1) no longer holds and they proposed a more sophisticated scaling law to explain their data. This can be explained by a relatively large variance error caused by multiple passes.

Notation.For two positive-valued functions \(f(x)\) and \(g(x)\), we write \(f(x) g(x)\) (and \(f(x)=(g(x))\)) or \(f(x) g(x)\) (and \(f(x)=(g(x))\)) if \(f(x) cg(x)\) or \(f(x) cg(x)\) holds for some absolute (if not otherwise specified) constant \(c>0\) respectively. We write \(f(x) g(x)\) (and \(f(x)=(g(x))\)) if \(f(x) g(x) f(x)\). For two vectors \(\) and \(\) in a Hilbert space, we denote their inner product by \(,\) or \(^{}\). For two matrices \(\) and \(\) of appropriate dimensions, we define their inner product by \(,:=(^{} )\). We use \(\|\|\) to denote the operator norm for matrices and \(_{2}\)-norm for vectors. For a positive semi-definite (PSD) matrix \(\) and a vector \(\) of appropriate dimension, we write \(\|\|_{}^{2}:=^{}\). For a symmetric matrix \(\), we use \(_{j}()\) to refer to the \(j\)-th eigenvalue of \(\) and \(r()\) to refer to its rank. Finally, \(()\) refers to logarithm base \(2\).

## 2 Related work

Empirical scaling laws.In recent years, the scaling laws of deep neural networks in compute, sample size, and model size have been widely studied across different models and domains [20; 35; 26; 19; 21; 46; 31]. The early work by Kaplan et al.  first proposed the neural scaling laws of transformer-based models. They observed that the test loss exhibits a power-law decay in quantities including the amount of compute, sample size, and model size, and provided joint formulas in these quantities to predict the test loss. The proposed formulas were later generalized and refined in subsequent works [19; 21; 1; 10; 31]. Notably, Hoffmann et al.  proposed the Chinchilla law, that is, (1) with \(a_{1} 0.34\) and \(a_{2} 0.28\). The empirical observation guided them to allocate data and model size under a given compute budget. The Chinchilla law is further revised by Besiroglu et al. . Motivated by the Chinchilla law, Muennighoff et al.  considered the effect of multiple passes over training data and empirically fitted a more sophisticated scaling law that takes account of the effect of data reusing.

Theory of scaling laws.Although neural scaling laws have been empirically observed over a broad spectrum of problems, there is a relatively limited literature on understanding these scaling laws from a theoretical perspective [37; 4; 28; 22; 42; 29; 23; 8; 2; 32; 17]. Among these works,  showed that the test loss scales as \(N^{4/d}\) for regression on data with intrinsic dimension \(d\). Hutter  studied a toy problem under which a non-trivial power of \(N\) arises in the test loss. Jain et al. considered scaling laws in data selection. Bahri et al.  considered a linear teacher-student model under a power-law spectrum assumption on the covariates, and they showed that the test loss of the ordinary least square estimator decreases following a power law in sample size \(N\) (resp. model size \(M\)) when the model size \(M\) (resp. sample size \(N\)) is infinite. Bordelon et al.  considered a linear random feature model and analyzed the test loss of the solution found by (batch) gradient flow. They focused on the bottleneck regimes where two of the quantities \(N,M,\)\(T\) (training steps) are infinite and showed that the risk has a power-law decay in the remaining quantity. The problem in Bahri et al. , Bordelon et al.  can be viewed as a sketched linear regression model similar to ours. It should be noted that both Bahri et al.  and Bordelon et al.  only derived the dependence of population risk on one of the data size, model size, or training steps in the asymptotic regime where the remaining quantities go to infinity, and their derivations are based on statistical physics heuristics. In comparison, we prove matching (ignoring constant factors) upper and lower risk bounds jointly depending on the finite model size \(M\) and data size \(N\).

Implicit regularization of SGD.One-pass SGD in linear regression has been extensively studied in both the classical finite-dimensional setting [34; 3; 14; 16; 25; 24; 18] and the modern high-dimensional setting [15; 6; 48; 47; 44; 45; 40]. In particular, Zou et al.  showed that SGD induces an implicit regularization effect that is comparable to, and in certain cases even more preferable than, the explicit regularization effect induced by ridge regression. This is one of the key motivations of our scaling law interpretation. From a technical perspective, we utilize the sharp finite-sample and dimension-free analysis of SGD developed by Zou et al. , Wu et al. [44; 45]. Different from them, we consider a sequence of linear regression models with an increasing number of trainable parameters given by data sketch. Our main technical innovation is to sharply control the effect of data sketch. Some of our intermediate results, for example, tight bounds on the spectrum of the sketched data covariance under the power law (see Lemma 6.2), might be of independent interest.

Prior works investigated linear regression with random features [36; 12], which can be viewed as a kind of sketched features via random coordinate selection. They mainly focused on the small approximation error regime, where the model size (or the number of features) is much larger than the data size. In comparison, we treat both model size and data size as free variables. Moreover, we provide matching upper and lower bounds while prior works mainly focused on upper bounds. These two innovations are crucial for studying scaling laws that predict test error as a function of both model size and data size. Finally, in the comparable regimes with small or zero approximation error, our excess risk bounds recover the bounds in prior works [36; 12; 33; 15; 13].

## 3 Setup

We use \(\) to denote a feature vector, where \(\) is a finite \(d\)-dimensional or countably infinite dimensional Hilbert space, and \(y\) to denote its label. In linear regression, we measure the population risk of a parameter \(\) by the mean squared error,

\[():=,- y^{2},,\]

where the expectation is over \((,y) P\) for some distribution \(P\) on \(\).

**Definition 1** (Data covariance and optimal parameter).: Let \(:=[^{}]\) be the data covariance. Assume that \(()\) and all entries of \(\) are finite. Let \((_{i})_{i 0}\) be the eigenvalues of \(\) sorted in non-increasing order. Let \(^{*}_{}()\) be the optimal model parameter1. Assume that \(\|^{*}\|_{}^{2}:=(^{*})^{}^{*}\) is finite.

We only assume access to \(M\)-dimensional sketched covariates and their responses, that is, \((,y)\), where \(^{M}\) is a fixed _sketch_ matrix. We focus on the Gaussian sketch matrix2, that is, entries of \(\) are independently sampled from \(0,1/M.\) We then consider linear predictors with \(M\) trainable parameters given by

\[f_{}:, ,,\]

where \(^{M}\) are the trainable parameters. Varying \(M\) should be viewed as a linear analog of varying the neural network model size. Our sketched linear regression setting is comparable to the teacher-student setting considered by Bahri et al. , Bordelon et al. .

We consider the training of \(f_{}\) via one-pass _stochastic gradient descent_ (SGD), that is,

\[_{t} :=_{t-1}-_{t}f_{_{t-1}}( _{t})-y_{t}_{}f_{_{t-1}}(_{t})\] (SGD) \[:=_{t-1}-_{t}_{t}^{}^{}_{t-1}-y_{t}_{t}, t=1, ,N,\]

where \((_{t},y_{t})_{t=1}^{N}\) are independent samples from \(P\) and \((_{t})_{t=1}^{N}\) are the stepsizes. We consider a popular geometric decaying stepsize scheduler ,

\[t=1,,N,\ _{t}:=/2^{},\ = t/(N /(N)).\] (3)

Here, the initial stepsize \(\) is a hyperparameter for the SGD algorithm. Without loss of generality, we assume the initial parameter is \(_{0}=0\). The output of the SGD algorithm is the last iterate \(_{N}\). Our proof techniques apply to other stepsize schedulers (e.g., polynomial decay) as well, but we focus on geometric decay as it is known to achieve near minimax-optimal excess risk for the last iterate of SGD .

Conditioning on a sketch matrix \(^{M}\), each parameter \(^{M}\) induces a sketched predictor through \(^{},\), and we denote its risk by

\[_{M}():=(^{})= ,-y^{2}, ^{M}.\]

By increasing \(M\) and \(N\), we have a sequence of datasets and trainable parameters of increasing sizes, respectively. This prepares us to study the scaling law (1) in the sketched linear regression problem, that is, to understand \(_{M}(_{N})\) as a function of both \(M\) and \(N\).

Risk decomposition.In a standard way, we decompose the risk achieved by \(_{N}\), the last iterate of (SGD), to the sum of _irreducible risk_, _approximation error_, and _excess risk_ as follows,

\[_{M}(_{N})=()}_{}+_{M}()-()}_{}+_{M}(_{N})-_{M}( )}_{}.\] (4)

We emphasize that the irreducible risk is independent of \(M\) and \(N\) and thus can be viewed as a constant; the approximation error is determined by the sketch matrix \(\), thus depends on \(M\) but is independent of \(N\); the excess risk depends on both \(M\) and \(N\) as it is determined by the algorithm.

## 4 Scaling laws

We first demonstrate a scaling-law behavior when the data spectrum satisfies a power law.

**Assumption 1** (Distributional conditions).: _Assume the following about the data distribution._

### _Gaussian design._ Assume that \((0,)\).

_Well-specified model._ Assume that \([y|]=^{}^{*}\). Define \(^{2}:=(y-^{}^{*})^{2}\).

**C. Parameter prior.** _Assume that \(^{*}\) satisfies a prior such that \((^{*})^{ 2}=\)._

**Assumption 2** (Power-law spectrum).: _There exists \(a>1\) such that the eigenvalues of \(\) satisfy \(_{i} i^{-a}\), \(i>0\)._

**Theorem 4.1** (Scaling law).: _Suppose that Assumptions 1 and 2 hold. Consider an \(M\)-dimensional sketched predictor trained by (SGD) with \(N\) samples. Let \(N_{}:=N/(N)\) and recall the risk decomposition in (4). Then there exists some \(a\)-dependent constant \(c>0\) such that when the initial stepsize \( c\), with probability at least \(1-e^{-(M)}\) over the randomness of the sketch matrix \(\), we have_

1. \(:=(^{*})=^{2}\)_._
2. \(_{^{*}} M^{1-a}\)_._
3. _Suppose in addition_ \(^{2} 1\)_. The expected excess risk (_\(\)_) can be decomposed into a bias error (_\(\)_) and a variance error (_\(\)_), namely,_ \[+^{2},\]_where the expectation is over the randomness of \(^{*}\) and \((_{i},y_{i})_{i=1}^{N}\). Moreover, \(\) and \(\) satisfy_

\[ M^{1-a},\ (N_{})^{1/a-1} },\] \[ (N_{})^{1/a-1}(N_{})^{1/a} M/cc>0,\] \[ M,\ (N_{})^{1/a}}/N_{ }.\]

_In all results, the hidden constants only depend on the power-law degree \(a\). As a direct consequence, when \(^{2} 1\), it holds with probability at least \(1-e^{-(M)}\) over the randomness of the sketch matrix \(\) that_

\[_{M}(_{N})=^{2}+}+})^{(a-1)/a}} ,\]

_where the expectation is over the randomness of \(^{*}\) and \((_{i},y_{i})_{i=1}^{N}\)._

Theorem 4.1 shows a sharp (up to constant factors) scaling law risk bound under an isotropic prior assumption and the power-law spectrum assumption. We emphasize that the scaling law bound in Theorem 4.1 holds for every \(M,N 1\). We also remark that the sum of approximation and bias errors dominates \(_{M}(_{N})-^{2}\), whereas the variance error is of strict higher order in terms of both \(M\) and \(N\), and is thus disappeared in the population risk bound.

Optimal stepsize.Based on the tight scaling law in Theorem 4.1, we can calculate the optimal stepsize that minimizes the risk. Specifically, the optimal stepsize is \( 1\) when \(N_{} M^{a}\) and can be anything such that \(M^{a}/N_{} 1\) when \(N_{} M^{a}\). In both cases, choosing \( 1\) is optimal. When the sample size is large such that \(N_{} M^{a}\), the optimal stepsize is relatively robust and can be chosen from a range.

Allocation of data and model sizes.Following Hoffmann et al. , we measure the compute complexity by \(MN\) as (SGD) queries \(M\)-dimensional gradients for \(N\) times. Given a total compute budget of \(MN=C\), from Theorem 6.1 and \(N_{}:=N/(N)\), we see that the best population risk is achieved by setting \(=(1)\), \(M=(C^{1/(a+1)})\), and \(N=(C^{a/(a+1)})\). Our theory suggests setting a data size slightly larger than the model size when the compute budget is the bottleneck.

Comparison with .The work by Bordelon et al.  considered the scaling law of batch gradient descent (or gradient flow) on a teacher-student model (see their equation (14)). Their teacher-student model can be viewed as our sketched linear regression model. However, we consider one-pass SGD, therefore in our setting the number of gradient steps is equivalent to the data size. When we equalize the number of gradient steps and the data size in their equation (14) and set the parameter prior as Assumption 1C, their prediction is consistent with ours. However, our analysis shows the computational advantage of SGD over batch GD since each iteration requires only \(1/N\) the compute. Bordelon et al.  obtained the limit of the population risk as two out of the data size, model size, and the number of gradient steps go to infinity based on statistical physics heuristics. In comparison, we obtain upper and lower risk bounds that hold for any finite \(M\) and \(N\) and match ignoring a constant factor depending only on the spectrum power-law degree \(a\).

Average of the SGD iteratesResults similar to Theorem 4.1 can also be established for the average of the iterates of online SGD with constant stepsize . All results will be the same once replacing the effective sample size \(N_{}\) in Theorem 4.1 to the sample size \(N\). For more details see Theorem F.6 in Appendix F.

### Scaling law under source condition

The isotropic parameter prior condition (Assumption 1C) in Theorem 4.1 can be generalized to the following anisotropic version .

**Assumption 3** (Source condition).: _Let \((_{i},_{i})_{i>0}\) be the eigenvalues and eigenvectors of \(\) with \((_{i})_{i>0}\) in non-increasing order. Assume \(^{*}\) satisfies a prior such that_

\[i j,\ \ _{i},^{*} _{j},^{*}=0;\ i>0,\ \ _{i}_{i},^{*}^{2} i ^{-b},\ \ b>1.\]A larger exponent \(b\) implies a faster decay of signal \(^{*}\) and thus corresponds to a simpler task . Note that Assumption 1C satisfies Assumption 3 with \(b=a\).

**Theorem 4.2** (Scaling law under source condition).: _In Theorem 4.1, suppose Assumption 1C is replaced by Assumption 3 with \(1<b<a+1\). Then there exists some \(a\)-dependent constant \(c>0\) such that when \( c\), with probability at least \(1-e^{-(M)}\) over the randomness of the sketch matrix \(\), we have_

\[_{M}(_{N})=^{2}+}+} )^{(b-1)/a}}}_{}+ M,\ (N_{})^{1/a}}}{N_{}}}_{ }.\]

_where the expectation is over the randomness of \(^{*}\) and \((_{i},y_{i})_{i=1}^{N}\), and \(()\) hides constants that may depend on \((a,b)\)._

When \(1<b a\), the tasks are relatively hard (compared to when \(b=a\)), and the variance error is dominated by the sum of approximation and bias errors for all choices of \(M\), \(N\), and \( 1\). In this case, Theorem 4.2 gives the same prediction about optimal stepsize and optimal allocation of data and model sizes under compute budget as Theorem 4.1.

When \(a<b<a+1\), the tasks are relatively easy (compared to when \(b=a\)), and variance remains dominated by the sum of approximation and bias error if the stepsize is optimally tuned. Recall that \( 1\), thus we can rewrite the risk bound in Theorem 4.2 as

\[_{M}(_{N})-^{2} M,\ (N_{})^{1/a}}^{b-1}}+ M,\ (N_{})^{1/a}}}{N_{}}\] \[M,\ (N_{})^{1/a} }/N_{}&M N_{}^{1/b}\ \ N_{}^{a/b-1} 1,\\ M,\ (N_{})^{1/a}}^{1-b}&M N_{ }^{1/b}\ \  N_{}^{a/b-1}.\]

Therefore the optimal stepsize and the risk under the optimal stepsize is

\[ N_{}^{a/b-1}\ \ M N_{}^{1/b},\ M^{a}/N_{} 1\ \ M N_{}^{1/b}.\]

Under the _optimally tuned_ stepsize, the population risk is in the form of

\[_{}_{M}(_{N})=^{2}+(N_{ }^{(1-b)/b})+(M^{1-b}),\]

which is again in the scaling law form (1). This is expected since an optimally tuned stepsize controls the variance error by adjusting the strength of the implicit bias of SGD. Under a fixed compute budget \(C=MN\), our theory suggests to assign \(M=(C^{1/(b+1)})\) and \(N=(C^{b/(b+1)})\), and set the stepsize to \((C^{(a-b)/(b+1)})\).

When \(b a+1\), the tasks are even simpler. We provide upper and lower bounds in Appendix D.3. However, there exists a gap between the bounds, fixing which is left for future work.

Moreover, we note that in the comparable regimes where \(M\) is large, the results in Theorem 4.2 match existing bounds on the risk of SGD iterates and ridge estimators .

### Scaling law under logarithmic power law

We also derive the risk formula when the data covariance has a logarithmic power-law spectrum .

**Assumption 4** (Logarithmic power-law spectrum).: _There exists \(a>1\) such that the eigenvalues of \(\) satisfy \(_{i} i^{-1}^{-a}(i+1)\), \(i>0\)._

**Theorem 4.3** (Scaling law under logarithmic power spectrum).: _In Theorem 4.1, suppose Assumption 2 is replaced by Assumption 4. Then with probability at least \(1-e^{-(M)}\) over the randomness of the sketch matrix \(\), we have_

\[_{M}(_{N})=^{2}+(M)}+(N_{} )},M,\ }}{^{a}(N_{})}}}{N_{ }},\]

_where the expectation is over the randomness of \(^{*}\) and \((_{i},y_{i})_{i=1}^{N}\)._Theorem 4.3 provides a scaling law under the logarithmic power-law spectrum. Similar to Theorem 4.1, the variance error is dominated by the approximation and bias errors for all choices of \(M\), \(N\), and \(\), and thus disappeared from the risk bound. Different from Theorem 4.1, here the population risk is a polynomial of \((M)\) and \((N_{})\).

## 5 Experiments

In this section, we examine the relation between the expected risk of the (SGD) output, the data size \(N\), and the model size \(M\) when the covariates satisfy a power-law covariance spectrum. Although our results in Section 4 hold with high probability over \(\), for simplicity, we assume the expectation of the risk is taken over both \(^{*}\) and \(\) in our simulations. We adopt the model in Section 3 and train it using one-pass (SGD) with geometric decaying stepsize (3). We choose the dimension \(d\) sufficiently large to approximate the infinite-dimensional case, and the data are generated so that Assumption 1 is satisfied. Moreover, we choose the covariance \(^{d d}\) to be diagonal with \(_{ii} i^{a}\) and \(()=1\) for some \(a>1\). From Figure 1, we observe that the risk indeed follows a power-law formula jointly in the number of samples and the number of parameters. In addition, the fitted exponents are aligned with our theoretical predictions \((a-1,1-1/a)\) in Theorem 4.1. Figure 2 shows the scaling of the expected risk in data size (or model size) when the model size (or data size) is relatively large. We see that the expected risk also satisfies a power-law decay with exponents matching our predictions. It is noteworthy that our simulations demonstrate stronger observations than the theoretical results in Theorem 4.1, which only establishes matching upper and lower bounds up to a constant factor. Additional simulation results on the risk of the average of (SGD) iterates can be found in Appendix F.

## 6 Risk bounds under a general spectrum

In this section, we present some general results on the upper and lower bounds of the risk of the output of (SGD). Due to the rotational invariance of the sketched matrix \(\), without loss of generality, we assume the covariance \(\) is diagonal with non-increasing diagonal entries. Our main results in Section 4 are directly built on the general bounds introduced here.

**Assumption 5** (General distributional conditions).: _Assume the following about the data distribution._

*Hypercontractivity.** _There exists \( 1\) such that for every PSD matrix \(\) it holds that_

\[^{}^{} ().\]

*Misspecified model.** _There exists \(^{2}>0\) such that \((y-^{}^{*})^{2}^{} ^{2}\)._

It is clear that Assumption 1 implies Assumption 5 with \(=3\).

Excess risk decomposition.Conditioning on the sketch matrix \(\), the training of the sketched linear predictor can be viewed as an \(M\)-dimensional linear regression problem. We can therefore

Figure 2: The expected risk of the last iterate of (SGD) minus the irreducible risk versus the effective sample size and model size. Parameters \(=1,=0.1\). (a), (b): \(a=1.5,d=10000\); (c), (d): \(a=2,d=1000\). The error bars denote the \( 1\) standard deviation of estimating the expected risk using \(100\) independent samples of \((^{*},)\). We use linear functions to fit the expected risk under the log-log scale and report the slope of the fitted lines (denoted by \(k\)).

invoke existing SGD analysis  to sharply control the excess risk by controlling the bias and variance errors. Specifically, let us define the (\(^{*}\)-dependent) bias error as

\[(^{*}):=_{t=1}^{N}-_ {t}^{}^{*}_{^{}}^{2}, ^{*}:=(^{})^{-1}^{*},\] (5)

and the variance error as

\[:=_{j} 1/(N_{})\}+(N _{})^{2}_{_{j}<1/(N_{}) }_{j}^{2}}{N_{}}, N_{}:=N/(N),\] (6)

where \(_{j}_{j=1}^{M}\) are eigenvalues of \(^{}\). We also let \(:=(^{*})\), where the expectation is over the prior of \(^{*}\). Using the existing results on the output of (SGD) in Wu et al. , we show that the excess risk in (4) can be exactly decomposed as the sum of bias and variance errors under weak conditions.

**Theorem 6.1** (Excess risk decomposition).: _Conditioning on the sketch matrix \(\), consider the excess risk in (4) induced by the output of (SGD). Assume \(_{0}=0\). Then for any \(^{*}\),_

1. _Under Assumptions_ 5 _and_ 5 _and suppose_ \( 1/c(^{})\) _for some constant_ \(c>0\)_, we have_ \[(^{*})+ \|^{*}\|_{}^{2}+^{2}.\]
2. _Under the stronger Assumptions_ 1 _and_ 1 _and_ _suppose_ \( 1/c(^{}))\) _for some constant_ \(c>0\)_, we have_ \[(^{*})+^{2} .\]

_In both results, the expectations of \(\) are taken over \((_{t},y_{t})_{t=1}^{N}\)._

Assuming that the signal-to-noise ratio is upper bounded, that is, \(\|^{*}\|_{}^{2}/^{2} 1\), then the bias-variance decomposition of the excess risk is sharp up to constant factors.

The variance error is in a nice form and can be computed using the following important lemma on the spectrum of \(^{}\). Similar results for logarithmic power-law are also established in Lemma G.6 in Appendix G.

**Lemma 6.2** (Power law).: _Under Assumption 2, it holds with probability at least \(1-e^{-(M)}\) that_

\[_{j}(^{})_{j}() j^{-a}, j=1,,M.\]

For any \(0 k^{*} k^{}\), let \(_{k^{*}:k^{}}^{M(k^{}-k^{*})}\) denote the matrix formed by the \(k^{*}+1-k^{}\)-th columns of \(\). We also abuse the notation \(k^{}:\) for \(k^{}:d\) when \(d\) is finite. We let \(_{k^{*}:k^{}}^{(k^{}-k^{*})(k^{ }-k^{*})}\) be the submatrix of \(\) formed by the \(k^{*}+1-k^{}\)-th eigenvalues. For the approximation and bias error, we use the following upper and lower bounds to compute their values.

**Theorem 6.3** (A general upper bound).: _Suppose Assumption 5 holds. Assume \(_{0}=0\), \(r() 2M\) and the initial stepsize satisfies \(<1/(c(^{}))\) for some constant \(c>0\). Then for any \(k_{1},k_{2} M/3\), with probability at least \(1-e^{-(M)}\)_

\[\|^{*}_{k_{1}:}\|_{_{k_{1}: }}^{2}+}_{i}}{M}+_{k_{1}+1}+ }_{i}^{2}}{M}}\|^{*}_{0:k_{1} }\|^{2},\]

\[(^{*})^{*}_{0:k_{2}}\|_{2}^{2 }}{N_{}}(_{k_{2}: }_{k_{2}:}^{}_{k_{2}:})}{_{M}( _{k_{2}:}_{k_{2}:}^{}_{k_{2}: })}^{2}+\|^{*}_{k_{2}:}\|_{_{k_{2}: }}^{2}.\]

**Theorem 6.4** (A general lower bound).: _Suppose Assumption 1 holds. Assume \(_{0}=0\), \(r() M\) and the initial stepsize \(<1/c(^{})\) for some constant \(c>0\). Then_

\[_{}._{i=M}^{d}_{i}, _{^{*}}(^{*})_{i:_{i}<1/(N_{})}(^{2}^{})}{_{i}(^{})}\]

_almost surely, where \((_{i})_{i=1}^{d}\) are eigenvalues of \(\) in non-increasing order, \((_{i})_{i=1}^{d}\) are eigenvalues of \(^{}\) in non-increasing order._Conclusion

We analyze neural scaling laws in infinite-dimensional linear regression. We consider a linear predictor with \(M\) trainable parameters on the sketched covariates, which is trained by one-pass stochastic gradient descent with \(N\) data. Under a Gaussian prior assumption on the optimal model parameter and a power law (of degree \(a>1\)) assumption on the spectrum of the data covariance, we derive matching upper and lower bounds on the population risk minus the irreducible error, that is, \((M^{-(a-1)}+N^{-(a-1)/a})\). In particular, we show that the variance error, which increases with \(M\), is of strictly higher order compared to the other errors, thus disappearing from the risk bound. We attribute the nice empirical formula of the neural scaling law to the non-domination of the variance error, which ultimately is an effect of the implicit regularization of SGD.

Many directions remain open for future study. First, our work is limited to the linear model; it would be interesting to see whether similar scaling laws can be derived in more complex models, such as random feature models or two-layer networks. Second, we focus on one-pass SGD training, and it is unclear if similar results hold for other optimization methods like accelerated SGD or Adam. Additionally, from a technical perspective, many results in our work depend on the Gaussian assumption and the source condition of the data. Investigating how these assumptions can be relaxed would also be valuable.