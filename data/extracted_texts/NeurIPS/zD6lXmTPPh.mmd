# A Novel Framework for Policy Mirror Descent with General Parameterization and Linear Convergence

Carlo Alfano

Department of Statistics

University of Oxford

carlo.alfano@stats.ox.ac.uk

&Rui Yuan

Stellantis, France1

rui.yuan@stellantis.com

&Patrick Rebeschini

Department of Statistics

University of Oxford

patrick.rebeschini@stats.ox.ac.uk

###### Abstract

Modern policy optimization methods in reinforcement learning, such as TRPO and PPO, owe their success to the use of parameterized policies. However, while theoretical guarantees have been established for this class of algorithms, especially in the tabular setting, the use of general parameterization schemes remains mostly unjustified. In this work, we introduce a framework for policy optimization based on mirror descent that naturally accommodates general parameterizations. The policy class induced by our scheme recovers known classes, e.g., softmax, and generates new ones depending on the choice of mirror map. Using our framework, we obtain the first result that guarantees linear convergence for a policy-gradient-based method involving general parameterization. To demonstrate the ability of our framework to accommodate general parameterization schemes, we provide its sample complexity when using shallow neural networks, show that it represents an improvement upon the previous best results, and empirically validate the effectiveness of our theoretical claims on classic control tasks.

## 1 Introduction

Policy optimization is one of the most widely-used classes of algorithms for reinforcement learning (RL). Among policy optimization techniques, policy gradient (PG) methods (e.g., [92; 87; 49; 6] are gradient-based algorithms that optimize the policy over a parameterized policy class and have emerged as a popular class of algorithms for RL (e.g., [42; 75; 12; 69; 78; 80; 54]).

The design of gradient-based policy updates has been key to achieving empirical success in many settings, such as games  and autonomous driving . In particular, a class of PG algorithms that has proven successful in practice consists of building updates that include a hard constraint (e.g., a trust region constraint) or a penalty term ensuring that the updated policy does not move too far from the previous one. Two examples of algorithms belonging to this category are trust region policy optimization (TRPO) , which imposes a Kullback-Leibler (KL) divergence  constraint on its updates, and policy mirror descent (PMD) (e.g. [88; 54; 93; 51; 89], which applies mirror descent (MD)  to RL. Shani et al.  propose a variant of TRPO that is actually a special case of PMD, thus linking TRPO and PMD.

From a theoretical perspective, motivated by the empirical success of PMD, there is now a concerted effort to develop convergence theories for PMD methods. For instance, it has been established that PMD converges linearly to the global optimum in the tabular setting by using a geometrically increasing step-size [54; 93], by adding entropy regularization , and more generally by adding convex regularization . Linear convergence of PMD has also been established for the negativeentropy mirror map in the linear function approximation regime, i.e., for log-linear policies, either by adding entropy regularization , or by using a geometrically increasing step-size [20; 2; 98]. The proofs of these results are based on specific policy parameterizations, i.e., tabular and log-linear, while PMD remains mostly unjustified for general policy parameterizations and mirror maps, leaving out important practical cases such as neural networks. In particular, it remains to be seen whether the theoretical results obtained for tabular policy classes transfer to this more general setting.

In this work, we introduce Approximate Mirror Policy Optimization (AMPO), a novel framework designed to incorporate general parameterization into PMD in a theoretically sound manner. In summary, AMPO is an MD-based method that recovers PMD in different settings, such as tabular MDPs, is capable of generating new algorithms by varying the mirror map, and is amenable to theoretical analysis for any parameterization class. Since the MD update can be viewed as a two-step procedure, i.e., a gradient update step on the dual space and a mapping step onto the probability simplex, our starting point is to define the policy class based on this second MD step (Definition 3.1). This policy class recovers the softmax policy class as a special case (Example 3.2) and accommodates any parameterization class, such as tabular, linear, or neural network parameterizations. We then develop an update procedure for this policy class based on MD and PG.

We provide an analysis of AMPO and establish theoretical guarantees that hold for any parameterization class and any mirror map. More specifically, we show that our algorithm enjoys quasi-monotonic improvements (Proposition 4.2), sublinear convergence when the step-size is non-decreasing, and linear convergence when the step-size is geometrically increasing (Theorem 4.3). To the best of our knowledge, AMPO is the first policy-gradient-based method with linear convergence that can accommodate any parameterization class. Furthermore, the convergence rates hold for any choice of mirror map. The generality of our convergence results allows us not only to unify several current best-known results with specific policy parameterizations, i.e., tabular and log-linear, but also to achieve new state-of-the-art convergence rates with neural policies. Tables 1 and 2 in Appendix A.2 provide an overview of our results. We also refer to Appendix A.2 for a thorough literature review.

The key point of our analysis is Lemma 4.1, which allows us to keep track of the errors incurred by the algorithm (Proposition 4.2). It is an application of the three-point descent lemma by [19; Lemma 3.2] (see also Lemma F.2), which is possible thanks to our formulations of the policy class (Definition 3.1) and the policy update (Line 2 of Algorithm 1). The convergence rates of AMPO are obtained by building on Lemma 4.1 and leveraging modern PMD proof techniques .

In addition, we show that for a large class of mirror maps, i.e., the \(\)-potential mirror maps in Definition 3.5, AMPO can be implemented in \(}(||)\) computations. We give two examples of mirror maps belonging to this class, Examples 3.6 and 3.7, that illustrate the versatility of our framework. Lastly, we examine the important case of shallow neural network parameterization both theoretically and empirically. In this setting, we provide the sample complexity of AMPO, i.e., \(}(^{-4})\) (Corollary 4.5), and show how it improves upon previous results.

## 2 Preliminaries

Let \(=(,,P,r,,)\) be a discounted Markov Decision Process (MDP), where \(\) is a possibly infinite state space, \(\) is a finite action space, \(P(s^{} s,a)\) is the transition probability from state \(s\) to \(s^{}\) under action \(a\), \(r(s,a)\) is a reward function, \(\) is a discount factor, and \(\) is a target state distribution. The behavior of an agent on an MDP is then modeled by a _policy_\((())^{}\), where \(a( s)\) is the density of the distribution over actions at state \(s\), and \(()\) is the probability simplex over \(\).

Given a policy \(\), let \(V^{}:\) denote the associated _value function_. Letting \(s_{t}\) and \(a_{t}\) be the current state and action at time \(t\), the value function \(V^{}\) is defined as the expected discounted cumulative reward with the initial state \(s_{0}=s\), namely,

\[V^{}(s):=_{a_{t}( s_{t}),s_{t+1} P( s _{t},a_{t})}_{t=0}^{}^{t}r(s_{t},a_{t}) ,s_{0}=s.\]

Now letting \(V^{}():=_{s}[V^{}(s)]\), our objective is for the agent to find an optimal policy

\[^{}*{argmax}_{(())^{}}V^{}().\] (1)As with the value function, for each pair \((s,a)\), the state-action value function, or _Q-function_, associated with a policy \(\) is defined as

\[Q^{}(s,a):=_{a_{1}(|s_{t}),s_{t+1} P (|s_{t},a_{t})}_{t=0}^{}^{t}r(s_{t},a_{t}) ,s_{0}=s,a_{0}=a.\]

We also define the discounted state visitation distribution by

\[d_{}^{}(s):=(1-)_{s_{0}} _{t=0}^{}^{t}P(s_{t}=s,s_{0}),\] (2)

where \(P(s_{t}=s,s_{0})\) represents the probability of the agent being in state \(s\) at time \(t\) when following policy \(\) and starting from \(s_{0}\). The probability \(d_{}^{}(s)\) represents the time spent on state \(s\) when following policy \(\).

The gradient of the value function \(V^{}()\) with respect to the policy is given by the policy gradient theorem (PGT) :

\[_{s}V^{}():=()}{(  s)}=d_{}^{}(s)Q^{}(s,).\] (3)

### Mirror descent

The first tools we recall from the mirror descent (MD) framework are mirror maps and Bregman divergences [14, Chapter 4]. Let \(^{||}\) be a convex set. A _mirror map_\(h:\) is a strictly convex, continuously differentiable and essentially smooth function1 such that \( h()=^{||}\). The convex conjugate of \(h\), denoted by \(h^{*}\), is given by

\[h^{*}(x^{*}):=_{x} x^{*},x -h(x), x^{*}^{||}.\]

The gradient of the mirror map \( h:^{||}\) allows to map objects from the primal space \(\) to its dual space \(^{||}\), \(x h(x)\), and viceversa for \( h^{*}\), i.e., \(x^{*} h^{*}(x^{*})\). In particular, from \( h()=^{||}\), we have: for all \((x,x^{*})^{||}\),

\[x= h^{*}( h(x)) x^{*}= h ( h^{*}(x^{*})).\] (4)

Furthermore, the mirror map \(h\) induces a _Bregman divergence_, defined as

\[_{h}(x,y):=h(x)-h(y)- h(y),x-y,\]

where \(_{h}(x,y) 0\) for all \(x,y\). We can now present the standard MD algorithm . Let \(\) be a convex set and \(V:\) be a differentiable function. The MD algorithm can be formalized2 as the following iterative procedure in order to solve the minimization problem \(_{x}V(x)\): for all \(t 0\),

\[y^{t+1} = h(x^{t})-_{t} V(x)|_{x=x^{t}},\] (5) \[x^{t+1} =^{h}_{}( h^{*}(y^{t+1})),\] (6)

where \(_{t}\) is set according to a step-size schedule \((_{t})_{t 0}\) and \(^{h}_{}()\) is the _Bregman projection_

\[^{h}_{}(y):=*{argmin}_{x }_{h}(x,y).\] (7)

Precisely, at time \(t\), \(x^{t}\) is mapped to the dual space through \( h()\), where a gradient step is performed as in (5) to obtain \(y^{t+1}\). The next step is to map \(y^{t+1}\) back in the primal space using \( h^{*}()\). In case \( h^{*}(y^{t+1})\) does not belong to \(\), it is projected as in (6).

## 3 Approximate Mirror Policy Optimization

The starting point of our novel framework is the introduction of a novel parameterized policy class based on the Bregman projection expression recalled in (7).

**Definition 3.1**.: Given a parameterized function class \(^{}=\{f^{}:,\, \}\), a mirror map \(h:\), where \(^{||}\) is a convex set with \(()\), and \(>0\), the _Bregman projected policy_ class associated with \(^{}\) and \(h\) consists of all the policies of the form:

\[^{}:_{s}^{}=^{h}_{()}(  h^{*}(_{f}^{^{+1}}_{s})),\;s;\; },\]

where for all \(s\), \(_{s}^{},f_{s}^{}^{||}\) denote vectors \([^{}(a s)]_{a}\) and \([f^{}(s,a)]_{a}\), respectively.

In this definition, the policy is induced by a mirror map \(h\) and a parameterized function \(f^{}\), and is obtained by mapping \(f^{}\) to \(\) with the operator \( h^{*}()\), which may not result in a well-defined probability distribution, and is thus projected on the probability simplex \(()\). Note that the choice of \(h\) is key in deriving convenient expressions for \(^{}\). The Bregman projected policy class contains large families of policy classes. Below is an example of \(h\) that recovers a widely used policy class [7, Example 9.10].

**Example 3.2** (Negative entropy).: If \(=_{+}^{||}\) and \(h\) is the negative entropy mirror map, i.e., \(h((|s))\!=\!_{a}(a|s)((a|s))\), then the associated Bregman projected policy class becomes

\[^{}:_{s}^{}=^{})}{\|(  f_{s}^{})\|_{1}},\;s;\;},\] (8)

where the exponential and the fraction are element-wise and \(_{1}\) is the \(_{1}\) norm. In particular, when \(f^{}(s,a)=_{s,a}\), the policy class (8) becomes the tabular softmax policy class; when \(f^{}\) is a linear function, (8) becomes the log-linear policy class; and when \(f^{}\) is a neural network, (8) becomes the neural policy class defined by Agarwal et al. . We refer to Appendix C.1 for its derivation.

We now construct an MD-based algorithm to optimize \(V^{^{}}\) over the Bregman projected policy class associated with a mirror map \(h\) and a parameterization class \(^{}\) by adapting Section 2.1 to our setting. We define the following shorthand: at each time \(t\), let \(^{t}:=^{^{t}}\), \(f^{t}:=f^{^{t}}\), \(V^{t}:=V^{^{t}}\), \(Q^{t}:=Q^{^{t}}\), and \(d^{t}_{}:=d^{^{t}}_{}\). Further, for any function \(y:\) and distribution \(v\) over \(\), let \(y_{s}:=y(s,)^{||}\) and \( y_{L_{2}(v)}^{2}=_{(s,a) v}[(y(s,a))^{2}]\). Ideally, we would like to execute the exact MD algorithm: for all \(t 0\) and for all \(s\),

\[f_{s}^{t+1} = h(_{s}^{t})+_{t}(1-)_{s}V^{t}()/d^{ t}_{}(s) h(_{s}^{t})+_{t}Q^{t}_{s},@note{ footnote}{With a slight abuse of notation, denote $ h(^{t})(s,a)$ as $[ h(_{s}^{t})]_{a}$.}\] (9) \[_{s}^{t+1} =^{h}_{()}( h^{*}(_{t}f_{s}^ {t+1})).\] (10)

Here, (10) reflects our Bregman projected policy class 3.1. However, we usually cannot perform the update (9) exactly. In general, if \(f^{}\) belongs to a parameterized class \(^{}\), there may not be any \(^{t+1}\) such that (9) is satisfied for all \(s\).

To remedy this issue, we propose Approximate Mirror Policy Optimization (AMPO), described in Algorithm 1. At each iteration, AMPO consists in minimizing a surrogate loss and projecting the result onto the simplex to obtain the updated policy. In particular, the surrogate loss in Line 1 of Algorithm 1 is a standard regression problem where we try to approximate \(Q^{t}+_{t}^{-1} h(^{t})\) with \(f^{t+1}\), and has been studied extensively when \(f^{}\) is a neural network . We can then readily use (10) to update \(^{t+1}\) within the Bregman projected policy class defined in 3.1, which gives Line 2 of Algorithm 1.

To better illustrate the novelty of our framework, we provide below two remarks on how the two steps of AMPO relate and improve upon previous works.

_Remark 3.3_.: Line 1 associates AMPO with the _compatible function approximation_ framework developed by , as both frameworks define the updated parameters \(^{t+1}\) as the solution to a regression problem aimed at approximating the current \(Q\)-function \(Q^{t}\). A crucial difference is that, Agarwal et al.  approximate \(Q^{t}\) linearly with respect to \(_{}^{t}\) (see (61)), while in Line 1 we approximate \(Q^{t}\) and the gradient of the mirror map of the previous policy with any function \(f^{}\). This generality allows our algorithm to achieve approximation guarantees for a wider range of assumptions on the structure of \(Q^{t}\). Furthermore, the regression problem proposed by Agarwal et al.  depends on the distribution \(d^{t}_{}\), while ours has no such constraint and allows off-policy updates involving an arbitrary distribution \(v^{t}\). See Appendix E for more details.

_Remark 3.4_.: Line 2 associates AMPO to previous approximations of PMD . For instance, Vaswani et al.  aim to maximize an expression equivalent to

\[^{t+1}_{^{}()}_{s  d^{t}_{}}[_{t} Q^{t}_{s},^{}_{s}-_{h}(^{}_{s},^{t}_{s})],\] (11)

where \(()\) is a given parameterized policy class, while the Bregman projection step of AMPO can be rewritten as

\[^{t+1}_{s}_{p()}_{t }f^{t+1}_{s}- h(^{t}_{s}),p-_{h}(p,^{t}_{s}),  s.\] (12)

We formulate this result as Lemma F.1 in Appendix F with a proof. When the policy class \(()\) is the entire policy space \(()^{}\), (11) is equivalent to the two-step procedure (9)-(10) thanks to the PGT (3). A derivation of this observation is given in Appendix B for completeness. The issue with the update in (11), which is overcome by (12), is that \(()\) in (11) is often a non-convex set, thus the three-point-descent lemma  cannot be applied. The policy update in (12) circumvents this problem by defining the policy implicitly through the Bregman projection, which is a convex problem and thus allows the application of the three-point-descent lemma . We refer to Appendix F for the conditions of the three-point-descent lemma in details.

### \(\)-potential mirror maps

In this section, we provide a class of mirror maps that allows to compute the Bregman projection in Line 2 with \(}(||)\) operations and simplifies the minimization problem in Line 1.

**Definition 3.5** (\(\)-potential mirror map ).: For \(u(-,+]\), \( 0\), let an \(\)_-potential_ be an increasing \(C^{1}\)-diffeomorphism \(:(-,u)(,+)\) such that

\[_{x-}(x)=,_{x u}(x)=+, _{0}^{1}^{-1}(x)dx.\]

For any \(\)-potential \(\), the associated mirror map \(h_{}\), called \(\)_-potential mirror map_, is defined as

\[h_{}(_{s})=_{a}_{1}^{(a|s)}^{-1} (x)dx.\]

Thanks to Krichene et al. [50, Proposition 2] (see Proposition C.1 as well), the policy \(^{t+1}\) in Line 2 induced by the \(\)-potential mirror map can be obtained with \(}(||)\) computations and can be written as

\[^{t+1}(a s)=((_{t}f^{t+1}(s,a)+_{s}^{t+1})),  s,a,\] (13)

where \(_{s}^{t+1}\) is a normalization factor to ensure \(_{a}^{t+1}(a s)=1\) for all \(s\), and \((z)=(z,0)\) for \(z\). We call this policy class the \(\)_-potential policy_ class. By using (13) and the definition of the \(\)-potential mirror map \(h_{}\), the minimization problem in Line 1 is simplified to be

\[^{t+1}_{}\|f^{}-Q^{t }-_{t}^{-1}(_{t-1}f^{t},^{-1}(0)-_{s}^{t})\|_{L_ {2}(v^{t})}^{2},\] (14)

where \((,)\) is applied element-wisely. The \(\)-potential policy class allows AMPO to generate a wide range of applications by simply choosing an \(\)-potential \(\). In fact, it recovers existing approaches to policy optimization, as we show in the next two examples.

**Example 3.6** (Squared \(_{2}\)-norm).: If \(=^{||}\) and \(\) is the identity function, then \(h_{}\) is the squared \(_{2}\)-norm, that is \(h_{}(_{s})=\|_{s}\|_{2}^{2}/2\), and \( h_{}\) is the identity function. So, Line 1 in Algorithm 1 becomes

\[^{t+1}_{}\|f^{}-Q^{t}- _{t}^{-1}^{t}\|_{L_{2}(v^{t})}^{2}.\] (15)The \( h_{}^{*}\) also becomes the identity function, and the policy update is given for all \(s\) by

\[_{s}^{t+1}=_{()}^{h_{}}(_{t}f_{s}^{t+1})= *{argmin}_{_{s}()}_{s}-_ {t}f_{s}^{t+1}_{2}^{2},\] (16)

which is the Euclidean projection on the probability simplex. In the tabular setting, where \(\) and \(\) are finite and \(f^{}(s,a)=_{s,a}\), (15) can be solved exactly with the minimum equal to zero, and Equations (15) and (16) recover the projected Q-descent algorithm . As a by-product, we generalize the projected Q-descent algorithm from the tabular setting to a general parameterization class \(^{}\), which is a novel algorithm in the RL literature.

**Example 3.7** (Negative entropy).: If \(=_{+}^{||}\) and \((x)=(x-1)\), then \(h_{}\) is the negative entropy mirror map from Example 3.2 and Line 1 in Algorithm 1 becomes

\[^{t+1}*{argmin}_{} f^{t+1}-Q ^{t}-}{_{t}}f^{t}_{L_{2}(v^{t})}^{2}.\] (17)

Consequently, based on Example 3.2, we have \(_{s}^{t+1}(_{t}f_{s}^{t+1})\) for all \(s\). In this example, AMPO recovers tabular NPG  when \(f^{}(s,a)=_{s,a}\), and Q-NPG with log-linear policies  when \(f^{}\) and \(Q^{t}\) are linear functions for all \(t 0\).

We refer to Appendix C.2 for detailed derivations of the examples in this section and an efficient implementation of the Bregman projection step. In addition to the \(_{2}\)-norm and the negative entropy, several other mirror maps that have been studied in the optimization literature fall into the class of \(\)-potential mirror maps, such as the Tsallis entropy [74; 57] and the hyperbolic entropy , as well as a generalization of the negative entropy . These examples illustrate how the class of \(\)-potential mirror maps recovers known methods and can be used to explore new algorithms in policy optimization. We leave the study of the application of these mirror maps in RL as future work, both from an empirical and theoretical point of view, and provide additional discussion and details in Appendix C.2.

## 4 Theoretical analysis

In our upcoming theoretical analysis of AMPO, we rely on the following key lemma.

**Lemma 4.1**.: _For any policies \(\) and \(\), for any function \(f^{}^{}\) and for \(>0\), we have_

\[ f_{s}^{}- h(_{s}),_{s}-_{s} _{h}(_{s},_{s})-_{h}(_ {s},_{s})-_{h}(_{s},_{s}), s ,\]

_where \(\) is the Bregman projected policy induced by \(f^{}\) and \(h\) according to Definition 3.1, that is \(_{s}=*{argmin}_{p()}_ {h}(p, h^{*}( f_{s}^{}))\) for all \(s\)._

The proof of Lemma 4.1 is given in Appendix D.1. Lemma 4.1 describes a relation between any two policies and a policy belonging to the Bregman projected policy class associated with \(^{}\) and \(h\). As mentioned in Remark 3.4, Lemma 4.1 is the direct consequence of (12) and can be interpreted as an application of the three-point descent lemma , while it cannot be applied to algorithms based on the update in (11) [88; 89] due to the non-convexity of the optimization problem (see also Appendix F). Notice that Lemma 4.1 accommodates naturally with general parameterization also thanks to (12). In contrast, similar results have been obtained and exploited for specific policy and mirror map classes [93; 60; 36; 98], while our result allows any parameterization class \(^{}\) and any choice of mirror map, thus greatly expanding the scope of applications of the lemma. A similar result for general parameterization has been obtained by Lan [55, Proposition 3.5] in the setting of strongly convex mirror maps.

Lemma 4.1 becomes useful when we set \(=^{t}\), \(f^{}=f^{t+1}\), \(=_{t}\) and \(=^{t}\) or \(=^{}\). In particular, when \(_{t}f_{s}^{t+1}- h(_{s}^{t})_{t}Q_{s}^{}\), Lemma 4.1 allows us to obtain telescopic sums and recursive relations, and to handle error terms efficiently, as we show in Appendix D.

### Convergence for general policy parameterization

In this section, we consider the parameterization class \(^{}\) and the fixed but arbitrary mirror map \(h\). We show that AMPO enjoys quasi-monotonic improvement and sublinear or linear convergence, depending on the step-size schedule. The first step is to control the approximation error of AMPO.

* _(Approximation error)_. There exists \(_{} 0\) such that, for all times \(t 0\), \[\|f^{t+1}-Q^{t}-_{t}^{-1} h(^{t})\|_{L _{2}(v^{t})}^{2}_{},\] where \((v^{t})_{t 0}\) is a sequence of distributions over states and actions and the expectation is taken over the randomness of the algorithm that obtains \(f^{t+1}\).

Assumption (A1) is common in the conventional compatible function approximation approach5. It characterizes the loss incurred by Algorithm 1 in solving the regression problem in Line 1. When the step-size \(_{t}\) is sufficiently large, Assumption (A1) measures how well \(f^{t+1}\) approximates the current Q-function \(Q^{t}\). Hence, \(_{}\) depends on both the accuracy of the policy evaluation method used to obtain an estimate of \(Q^{t}\) and the error incurred by the function \(f^{}^{}\) that best approximates \(Q^{t}\), that is the representation power of \(^{}\). Later in Section 4.2, we show how to solve the minimization problem in Line 1 when \(^{}\) is a class of shallow neural networks so that Assumption (A1) holds. We highlight that Assumption (A1) is weaker than the conventional assumptions , since we do not constrain the minimization problem to be linear in the parameters (see (61)). We refer to Appendix A.2 for a discussion on its technical novelty and Appendix G for a relaxed version of the assumption.

As mentioned in Remark 3.3, the distribution \(v^{t}\) does not depend on the current policy \(^{t}\) for all times \(t 0\). Thus, Assumption (A1) allows for off-policy settings and the use of replay buffers . We refer to Appendix A.3 for details. To quantify how the choice of these distributions affects the error terms in the convergence rates, we introduce the following coefficient.

* _(Concentrability coefficient)_. There exists \(C_{v} 0\) such that, for all times \(t\), \[_{(s,a) v^{t}}^{}(s) (a s)}{v^{t}(s,a)}^{2} C_{v},\] whenever \((d_{}^{},)\) is either \((d_{}^{},^{})\), \((d_{}^{t+1},^{t+1})\), \((d_{}^{},^{t})\), or \((d_{}^{t+1},^{t})\).

The concentrability coefficient \(C_{v}\) quantifies how much the distribution \(v^{t}\) overlaps with the distributions \((d_{}^{},^{})\), \((d_{}^{t+1},^{t+1})\), \((d_{}^{},^{t})\) and \((d_{}^{t+1},^{t})\). It highlights that the distribution \(v^{t}\) should have full support over the environment, in order to avoid large values of \(C_{v}\). Assumption (A2) is weaker than the previous best-known concentrability coefficient [98, Assumption 9], in the sense that we have the full control over \(v^{t}\). We refer to Appendix H for a more detailed discussion. We can now present our first result on the performance of Algorithm 1.

**Proposition 4.2** (Quasi-monotonic updates).: _Let (A1), (A2) be true. We have, for all \(t 0\),_

\[[V^{t+1}()-V^{t}()][_ {s d_{}^{t+1}}[_{h}(_{s}^{t+1},_{s}^{t})+ _{h}(_{s}^{t},_{s}^{t+1})}{_{t}(1-)}]] -_{}}}{1-},\]

_where the expectation is taken over the randomness of AMPO._

We refer to Appendix D.3 for the proof. Proposition 4.2 ensures that an update of Algorithm 1 cannot lead to a performance degradation, up to an error term. The next assumption concerns the coverage of the state space for the agent at each time \(t\).

* _(Distribution mismatch coefficient)_. Let \(d_{}^{}:=d_{}^{^{}}\). There exists \(_{} 0\) such that \[_{s}^{}(s)}{d_{}^{t}(s)}_{}, .\] Since \(d_{}^{t}(s)(1-)(s)\) for all \(s\), obtained from the definition of \(d_{}\) in (2), we have that \[_{s}^{}(s)}{d_{}^{t}(s)}_{s}^{}(s)}{(s)},\] where assuming boundedness for the term on the right-hand side is standard in the literature on both the PG [e.g., 101, 90] and NPG convergence analysis [e.g., 15, 93]. We refer to Appendix I for details. It is worth mentioning that the quasi-monotonic improvement in Proposition 4.2 holds without (A3).

We define the weighted Bregman divergence between the optimal policy \(^{}\) and the initial policy \(^{0}\) as \(_{0}^{}=_{s d_{}^{}}[_{h}(_ {s}^{},_{s}^{0})]\). We then have our main results below.

**Theorem 4.3**.: _Let (A1), (A2) and (A3) be true. If the step-size schedule is non-decreasing, i.e., \(_{t}_{t+1}\) for all \(t 0\), the iterates of Algorithm 1 satisfy: for every \(T 0\),_

\[V^{}()-_{t<T}[V^{t}()]_{0}^{}}{(1-)_{0}}+}{1-}+)_{ approx} }}{1-}.\]

_Furthermore, if the step-size schedule is geometrically increasing, i.e., satisfies_

\[_{t+1}}{_{}-1}_{t} t 0,\] (18)

_we have: for every \(T 0\),_

\[V^{}()-[V^{T}()] 1-}^{T}1+_{0}^{}}{ _{0}(_{}-1)}+)_{  approx}}}{1-}.\]

Theorem 4.3 is, to the best of our knowledge, the first result that establishes linear convergence for a PG-based method involving general policy parameterization. For the same setting, it also matches the previous best known \(O(1/T)\) convergence , without requiring regularization. Lastly, Theorem 4.3 provides a convergence rate for a PMD-based algorithm that allows for arbitrary mirror maps and policy parameterization without requiring the assumption on the approximation error to hold in \(_{}\)-norm, in contrast to Lan . We give here a brief discussion of Theorem 4.3 w.r.t. previous results and refer to Tables 1 and 2 in Appendix A.2 for a detailed comparison.

In terms of iteration complexity, Theorem 4.3 recovers the best-known convergence rates in the tabular setting , for both non-decreasing and exponentially increasing step-size schedules. While considering a more general setting, Theorem 4.3 matches or improves upon the convergence rate of previous work on policy gradient methods for non-tabular policy parameterizations that consider constant step-size schedules , and matches the convergence speed of previous work that employ NPG, log-linear policies, and geometrically increasing step-size schedules .

In terms of generality, the results in Theorem 4.3 hold without the need to implement regularization , to impose bounded updates or smoothness of the policy , to restrict the analysis to the case where the mirror map \(h\) is the negative entropy , or to make \(_{}\)-norm assumptions on the approximation error . We improve upon the latest results for PMD with general policy parameterization by Vaswani et al. , which only allow bounded step-sizes, where the bound can be particularly small, e.g., \((1-)^{3}/(2||)\), and can slow down the learning process.

When \(\) is a finite state space, a sufficient condition for \(_{}\) in (A3) to be bounded is requiring \(\) to have full support on \(\). If \(\) does not have full support, one can still obtain linear convergence for \(V^{}(^{})-V^{T}(^{})\), for an arbitrary state distribution \(^{}\) with full support, and relate this quantity to \(V^{}()-V^{T}()\). We refer to Appendix I for a detailed discussion on the distribution mismatch coefficient.

**Intuition.** An interpretation of our theory can be provided by connecting AMPO to the Policy Iteration algorithm (PI), which also enjoys linear convergence. To see this, first recall (12)

\[_{s}^{t+1}*{argmin}_{p()}-f_{s}^ {t+1}+_{t}^{-1} h(_{s}^{t}),p+_{t}^{-1}_{ h}(p,_{s}^{t}), s.\]

Secondly, solving Line 1 of Algorithm 1 leads to \(f_{s}^{t+1}-_{t}^{-1} h(_{s}^{t}) Q_{s}^{t}\). When the step-size \(_{t}\), that is \(_{t}^{-1} 0\), the above viewpoint of the AMPO policy update becomes

\[_{s}^{t+1}\ \ *{argmin}_{p()}-Q_{s}^ {t},p\ \ _{s}^{t+1}\ \ *{argmax}_{p()} Q_{s}^{t},p,  s,\]

which is the PI algorithm. Here we ignore the Bregman divergence term \(_{h}(,_{s}^{t})\), as it is multiplied by \(1/_{t}\), which goes to 0. So AMPO behaves more and more like PI with a large enough step-size and thus is able to converge linearly like PI.

**Proof idea.** We provide a sketch of the proof here; the full proof is given in Appendix D. In a nutshell, the convergence rates of AMPO are obtained by building on Lemma 4.1 and leveraging modern PMD proof techniques . Following the conventional compatible function approximation approach , the idea is to write the global optimum convergence results in an additive form, that is

sub-optimality gap \(\) optimization error \(+\) approximation error.

The separation between the two errors is allowed by Lemma 4.1, while the optimization error is bounded through the PMD proof techniques from Xiao  and the approximation error is characterized by Assumption (A1). Overall, the proof consists of three main steps.

_Step 1._ Using Lemma 4.1 with \(=^{t}\), \(f^{}=f^{t+1}\), \(=_{t}\), \(=^{t+1}\), and \(_{s}=_{s}^{t}\), we obtain

\[_{t}f_{s}^{t+1}- h(_{s}^{t}),_{s}^{t+1}-_{s}^{t}  0,\]

which characterizes the improvement of the updated policy.

_Step 2._ Assumption (A1), Step 1, the performance difference lemma (Lemma D.4), and Lemma 4.1 with \(=^{t}\), \(f^{}=f^{t+1}\), \(=_{t}\), \(=^{t+1}\), and \(_{s}=_{s}^{}\) permit us to obtain the following.

**Proposition 4.4**.: _Let \(_{t}:=V^{}()-V^{t}()\). For all \(t 0\), we have_

\[[_{}(_{t+1}-_{t})+_{t} ]_{s d_{s}^{t}}[_{h}(_{s}^{},_{s}^{t+1})]}{(1-)_{t}}-_ {s d_{s}^{t}}[_{h}(_{s}^{},_{s}^{t+1})]} {(1-)_{t}}+(1+_{})_{ approx }}}{1-}.\]

_Step 3._ Proposition 4.4 leads to sublinear convergence using a telescoping sum argument, and to linear convergence by proving defining step-sizes and by rearranging terms into the following contraction,

\[_{t+1}+_{s d_{s}^{t}}[ _{h}(_{s}^{},_{s}^{t+1})]}{(1-)_{t+1}( _{}-1)}(1-}) _{t}+_{s d_{s}^{t}}[_{h}(_{s}^{ },_{s}^{t})]}{(1-)_{t}(_{}-1)}+(1+ })_{ approx}}}{1- }.\]

### Sample complexity for neural network parameterization

Neural networks are widely used in RL due to their empirical success in applications [67; 68; 84]. However, few theoretical guarantees exist for using this parameterization class in policy optimization [60; 90; 16]. Here, we show how we can use our framework and Theorem 4.3 to fill this gap by deriving a sample complexity result for AMPO when using neural network parameterization. We consider the case where the parameterization class \(^{}\) from Definition 3.1 belongs to the family of shallow ReLU networks, which have been shown to be universal approximators [38; 4; 27; 39]. That is, for \((s,a)()^{d}\), define \(f^{}(s,a)=c^{}(W(s,a)+b)\) with \(=(c,W,b)\), where \((y)=(y,0)\) for all \(y\) is the ReLU activation function and is applied element-wisely, \(c^{m}\), \(W^{m d}\) and \(b^{m}\).

At each iteration \(t\) of AMPO, we set \(v^{t}=d_{}^{t}\) and solve the regression problem in Line 1 of Algorithm 1 through stochastic gradient descent (SGD). In particular, we initialize entry-wise \(W_{0}\) and \(b\) as i.i.d. random Gaussian variables from \((0,1/m)\), and \(c\) as i.i.d. random Gaussian variables from \((0,_{A})\) with \(_{A}(0,1]\). Assuming access to a simulator for the distribution \(v^{t}\), we run SGD for \(K\) steps on the matrix \(W\), that is, for \(k=0,,K-1\),

\[W_{k+1}=W_{k}-f^{(k)}(s,a)-^{t}(s,a)-_{t}^{-1}  h(_{s}^{t})_{W}f^{(k)}(s,a),\] (19)

where \(f^{(k)}(s,a)=c^{}((W_{0}+W_{k})(s,a)+b)\), \((s,a) v^{t}\) and \(^{t}(s,a)\) is an unbiased estimate of \(Q^{t}(s,a)\) obtained through Algorithm 4. We can then present our result on the sample complexity of AMPO for neural network parameterization, which is based on our convergence Theorem 4.3 and an analysis of neural networks by Allen-Zhu et al. [3, Theorem 1].

**Corollary 4.5**.: _In the setting of Theorem 4.3, let the parameterization class \(^{}\) consist of sufficiently wide shallow ReLU neural networks. Using an exponentially increasing step-size and solving the minimization problem in Line 1 with SGD as in (19), the number of samples required by AMPO to find an \(\)-optimal policy with high probability is \(}(C_{v}^{2}v_{}^{5}/^{4}(1-)^{6})\), where \(\) has to be larger than a fixed and non-vanishing error floor._

We provide a proof of Corollary 4.5 and an explicit expression for the error floor in Appendix J. Note that the sample complexity in Corollary 4.5 might be impacted by an additional \((^{-1})\) term. We refer to Appendix J for more details and an alternative result (Corollary J.4) which does not include an additional \((^{-1})\) term, enabling comparison with prior works.

## 5 Numerical experiments

We provide an empirical evaluation of AMPO in order to validate our theoretical findings. We note that the scope of this work is mainly theoretical and that we do not aim at establishing state-of-the-art results in the setting of deep RL. Our implementation is based upon the PPO implementation from PureJaxRL , which obtains the estimates of the \(Q\)-function through generalized advantage estimation (GAE)  and performs the policy update using ADAM optimizer  and mini-batches. To implement AMPO, we (i) replaced the PPO loss with the expression to minimize in Equation (14), (ii) replaced the softmax projection with the Bregman projection, (iii) saved the constants \(\) along the sampled trajectories in order to compute Equation (14). The code is available here.

In Figure 1, we show the averaged performance over 100 runs of AMPO in two classic control environments, i.e. CartPole and Acrobot, in the setting of \(\)-potential mirror maps. In particular, we choose: \((x)=e^{x}\), which corresponds to the negative entropy (Example 3.7); \((x)=(x)\), which corresponds to the hyperbolic entropy [34, see also (41)]; \((x)=x\), which corresponds to the Euclidean norm (Example 3.6); and the Tsallis entropy for two values of the entropic index \(q\)[74, 57, see also (40)]. We refer to Appendix C.2 for a detailed discussion on these mirror maps. We set the step-size to be constant and of value 1. For a comparison, we also plot the averaged performance over 100 runs of PPO.

The plots in Figure 1 confirm our results on the quasi-monotonicity of the updates of AMPO and on its convergence to the optimal policy. We observe that instances of AMPO with different mirror maps are very competitive as compared to PPO. We also note that, despite the convergence rates in Theorem 4.3 depend on the mirror map only in terms of a \(_{0}^{*}\) term, different mirror maps may result in different convergence speeds and error floors in practice. In particular, our experiments suggest that the negative entropy mirror map may not be the best choice for AMPO, and that exploring different mirror maps is a promising direction of research.

## 6 Conclusion

We have introduced a novel framework for RL which, given a mirror map and any parameterization class, induces a policy class and an update rule. We have proven that this framework enjoys sublinear and linear convergence for non-decreasing and geometrically increasing step-size schedules, respectively. Future venues of investigation include studying the sample complexity of AMPO in on-policy and off-policy settings other than neural network parameterization, exploiting the properties of specific mirror maps to take advantage of the structure of the MDP and efficiently including representation learning in the algorithm. We refer to Appendix A.3 for a thorough discussion of future work. We believe that the main contribution of AMPO is to provide a general framework with theoretical guarantees that can help the analysis of specific algorithms and MDP structures. AMPO recovers and improves several convergence rate guarantees in the literature, but it is important to keep in consideration how previous works have exploited particular settings, while AMPO tackles the most general case. It will be interesting to see whether these previous works combined with our fast linear convergence result can derive new efficient sample complexity results.

Figure 1: Averaged performance over 50 runs of AMPO in CartPole and Acrobot environments. Note that the maximum values for CartPole and Acrobot are 500 and -80, respectively.