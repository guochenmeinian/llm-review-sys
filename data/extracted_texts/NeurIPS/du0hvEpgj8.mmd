# Actively Testing Your Model While It Learns:

Realizing Label-Efficient Learning in Practice

 Dayou Yu\({}^{1}\)  Weishi Shi\({}^{2}\)  Qi Yu\({}^{1}\)

Rochester Institute of Technology, Rochester, NY 14623\({}^{1}\)

University of North Texas, Denton, TX 76203\({}^{2}\)

{dy2507,qi.yu}@rit.edu\({}^{1}\) weishi.shi@unt.edu\({}^{2}\)

Corresponding author.

###### Abstract

In active learning (AL), we focus on reducing the data annotation cost from the model training perspective. However, "testing", which often refers to the model evaluation process of using empirical risk to estimate the intractable true generalization risk, also requires data annotations. The annotation cost for "testing" (model evaluation) is under-explored. Even in works that study active model evaluation or active testing (AT), the learning and testing ends are disconnected. In this paper, we propose a novel active testing while learning (ATL) framework that integrates active learning with active testing. ATL provides an unbiased sample-efficient estimation of the model risk during active learning. It leverages test samples annotated from different periods of a dynamic active learning process to achieve fair model evaluations based on a theoretically guaranteed optimal integration of different test samples. Periodic testing also enables effective early-stopping to further save the total annotation cost. ATL further integrates an "active feedback" mechanism, which is inspired by human learning, where the teacher (active tester) provides immediate guidance given by the prior performance of the student (active learner). Our theoretical result reveals that active feedback maintains the label complexity of the integrated learning-testing objective, while improving the model's generalization capability. We study the realistic setting where we maximize the performance gain from choosing "testing" samples for feedback without sacrificing the risk estimation accuracy. An agnostic-style analysis and empirical evaluations on real-world datasets demonstrate that the ATL framework can effectively improve the annotation efficiency of both active learning and evaluation tasks.

## 1 Introduction

Labeled data are essential for supervised learning in both model training and evaluation. Active learning (AL) provides a promising direction to reduce the human annotation cost by constructing a smaller but more effective labeled dataset for training purposes . However, AL only partially addresses the human annotation cost as the cost of constructing a labeled dataset for model testing has been overlooked. In reality, the annotation budget could easily be drained by building large test datasets that most modern AL methods rely on to evaluate the model performance and determine the stopping criterion for learning. In contrast to AL, active model evaluation or active testing (AT) focuses on actively selecting the testing data. Few existing efforts develop unbiased risk estimation techniques for label-efficient model evaluation . However, these methods are designed to evaluate fixed models that have been fully trained, making them ill-suited for evaluating an actively learned model, which is constantly updated and inadequately trained during most parts of the learning process. To address these fundamental challenges and realize label-efficient learning in practice, we propose to integrate AT with AL in a novel and efficient learning framework, referred toas ATL. The ATL framework systematically addresses the unique active testing challenges arising from evaluating an actively learned model that is under-trained and constantly evolving. It leverages an interactive learning-testing-feedback process to better control the overall labeling budget, which not only achieves efficient evaluation of the AL model but also ensures faster convergence in model training. Through periodical testing, ATL gains useful insights to terminate the learning process as early as possible that avoids allocating unnecessary labeling budget.

Incorporating AT into AL is a highly non-trivial task. Unlike AL, AT has two inter-connected objectives: i) design of an unbiased risk estimator to quantify the model performance, and ii) design of a sampling strategy to select informative test samples. Achieving both objectives simultaneously under the AL setting with a constantly evolving and under-trained model is far more challenging than existing active model evaluations that assume a fixed and adequately trained model. For the first objective, its challenge stems from the potential biased active test samples brought by the second objective. We need an unbiased risk estimator to ensure the fairness of the evaluation. An unbiased risk estimator can be achieved by importance sampling or its variants . Recent active testing works also employ expectation analysis and propose other weighting mechanisms . However, all these works assume that the model being evaluated is fixed and already well-trained . Therefore, these estimators are not designed to support a dynamic AL setting, where the model continues to evolve when learning from newly labeled samples. Figure 1 shows that both the standard estimator (_i.e.,_ random testing) and direct adaptation of an existing unbiased estimator (ARE)  fail to provide an accurate and consistent model evaluation in the more challenging AL setting. As for the second objective, different from sample selections in AL, the testing selection needs to be compatible with the unbiased risk estimator. It is essential to ensure asymptotic convergence that guarantees an unbiased model evaluation while reducing the variance that can improve the convergence speed and provide a stable signal to terminate the learning process as early as possible.

Conducting AT along with AL enables ATL to seamlessly connect the active training and testing sides, which allows them to communicate with and guide each other within a well-integrated learning process. To this end, the proposed ATL framework leverages an active learning-testing-feedback (LTF) loop, which largely resembles real-world human learning. In particular, within the LTF loop, the active testing results are kept after each active learning round, which mimics the quizzes in the human learning setting. We refer to them as _active quizzes_. As shown in Figure 2, the active learner acts as a student while the active tester acts as a teacher. Both the student and the teacher select data from the unlabeled pool and get their labels from the oracle. The student attempts to use the labeled data to pass the quizzes provided by the teacher. On the other hand, the teacher provides a fair quiz to evaluate the student's performance. Meanwhile, the teacher also sends back the keys of some quiz questions, which are referred to as _active feedback_, to help the student learn better.

Given a dynamic AL process, multiple active quizzes will be created at different learning phases. ATL forms an integrated risk estimator by aggregating all the quizzes. Our theoretical analysis guarantees that the integrated risk estimation converges to the true risk asymptotically. The aggregation strategy also minimizes the variance, which ensures faster convergence. Furthermore, through active feedback, the AL model can benefit from training with a small proportion of the testing samples (as feedback) after each quiz while maintaining the asymptotic convergence condition of the risk estimation. Finally, the integrated risk estimator provides an unbiased estimate of the model performance with a small variance, which provides a convenient way to determine whether the AL model has converged.

Figure 1: Risk estimation comparison

Figure 2: Overview of the ATL Framework

This can further save the labeling budget by stopping the learning early. We may also combine the estimated risk with other available information from the unlabeled data to create more systematic early-stopping criteria. Our main contributions are summarized as follows:

* the first ATL framework that builds an active learning-testing-feedback loop to mimic human learning for achieving label-efficient and dynamic evaluation of actively learned models,
* a theoretically sound integrated risk estimator that combines active quiz results and minimizes the variance of the difference between the estimated and true risks,
* an innovative active feedback process to further improve learning effectiveness without extra labels,
* a systematic early stopping criterion that combines integrated risk estimation with information of unlabeled data to terminate AL process early.

We conduct experiments on both synthetic and real-world datasets to show the improved testing performance of the proposed ATL framework.

## 2 Related Work

**Active model evaluation.** Actively testing the model performance in a label efficient way is relatively under-explored. Among the few excising efforts, model testing is treated independently from model training, which is unrealistic in most practical settings. The problem of active risk estimation (ARE) was first formulated in , which used importance sampling to construct an unbiased risk estimator. Then, the optimal test sampling distribution is derived from an optimization problem that minimizes the variance of the risk error. A similar process has been used to estimate more complex loss measures  or perform active model selection . However, such risk estimator is no longer unbiased nor label-efficient when an AL process is involved that samples from the same unlabeled pool for model training. Farquhar et al. propose two risk estimators to cancel the statistical bias by active data sampling strategy in a data-efficient manner . However, their proposed estimators could not deal with the overfitting bias making them less suitable for model evaluation.

In a more recent work, Kossen et al. propose to use the unbiased risk estimators to perform active testing . The difference from  is that the selection order of the testing points is considered, and the weights are defined for selection indices instead of a single testing proposal distribution. In this way, the testing proposal can be evaluated at each round, making the setting more realistic. However, the optimal proposal is still impossible to obtain. To approximate the optimal proposal, a surrogate model is needed, which is similar to the introspective risk model in . The surrogate model idea is further extended in , where the unbiased risk estimator is replaced by the predicted loss from the surrogate model. Yilmaz et al. propose to replace the commonly used importance sampling with Poison sampling, which better stabilizes the test samples . A different line of work leverages stratified sampling to first stratify the unlabeled pool and then conduct efficient sampling over strata in an importance sampling manner [13; 3; 30]. However, all those work is limited to offline test sampling in which the test proposal is fixed during the sampling process and could not benefit from prior knowledge such as the previously labeled data.

**Early stopping in AL.** Early stopping strategies in AL have been sparsely investigated over years [2; 7; 6; 31; 26; 18]. Most methods only consider the learning process and do not consider a fully connected learning-testing loop. Some have shown that the stopping criteria based on unlabeled information are more effective than those based on labeled information. This inspires us to combine the proposed integrated risk estimation with unlabeled data information to form a systemic early stopping criterion to further reduce the labeling budget.

## 3 The ATL Framework

Problem setting.We formulate the ATL framework under a novel and fully integrated learning and testing-while-it-learns setting. W.l.o.g., we assume that all data are generated from an underlying distribution, and we could obtain the label \(y\) through the true labeling function: \(y=h()\) provided by the human annotator (note that unlike the predictive model \(f_{}()\), the true labeling function is not governed by the parameter \(\) and it might be non-realizable). Similar to a traditional pool-based AL setting, we have a small labeled set \(_{L}=\{(,y)\}^{N_{L}}\) (whose labels are revealed). As for the unlabeled pool, we only have access to the features: \(_{U}=\{\}^{N_{U}}\). We further assume that the unlabeled pool is sufficiently large so that active sampling will not change the pool distribution since only limited labels will be revealed. That is, \(p(,y)\) remains fixed during AL. Our primary task couples two objectives: AL and model evaluation at the same time. In this setting, the goal of AL is to create a labeled set by selecting samples to label and achieve the best performance. The performance of the model is indicated by the expected error when generalized to the underlying data distribution, which is often noted as the _risk_\(R\). As discussed earlier, existing works on label-efficient model evaluation assume that the model being evaluated is already well-trained and kept fixed. They are inadequate to handle a dynamic active learning scenario, which is the focus of ATL. Given some loss function as the risk measurement \(_{}:f_{}() y\), our task of testing the model reduces to evaluating the following expectation:

\[R=}_{\{,y\}}[_{}] =(f_{}(),y)p(,y)(,y)\] (1)

However, directly evaluating (1) is infeasible as the true density function \(p(,y)\) is unknown. So empirically, we have to adopt an approximation. In the pool-based learning setting, our approximation resorts to the expected empirical risk over the pool distribution \(_{pool}\).

### Overview: Active Risk Estimation During AL

In each AL iteration, we select to label \(^{*}\) based on a sampling criterion \(p_{sample}()\) to improve the model. We design the framework to be AL-agnostic, meaning that most standard AL sampling strategies can be applied here without further assumptions. Normally, the evaluation of the AL model is performed on a hold-out test set either at the end or during different _to-learn_ data selection rounds of AL. However, the labeling cost of the test set is often times ignored. To achieve a more efficient evaluation framework, we propose to have the active risk estimation (_i.e.,_ active testing) process intertwined with AL. Specifically, we use time stamps \(t\) to denote each stage where we perform active testing. At time \(t\), we use testing proposal \(q_{t}()\) to select samples, \(_{t}=\{_{t}^{(1)},...,_{t}^{(n_{t})}\}\), then label them to evaluate the model. We name this novel evaluation style _active quiz_. _Active quiz_ is motivated by the dynamic nature of the AL process, which allows us to have an instant evaluation of the current model during AL. We also show that the quizzes can further be conveniently combined into a _final exam_ for a comprehensive evaluation. In this way, we essentially divide a given budget for evaluation into small batches, which makes it possible to stop the evaluation before reaching the total budget by leveraging the signal from past quizzes.

The purpose of having the active quiz is to achieve a good estimate of the current model performance, which is not realizable in most existing AL frameworks. We use the risk \(R\) to denote the performance. \(R\) is dependent on the specific model, thus at each step we are trying to estimate the true risk of the model at \(t\): \(R(f_{t}())\)2. Using \(_{t}\), we can have an estimate \(_{_{t}}(f_{t})\). We name the evaluation after the last quiz \(_{T}\) as the _final exam_ because it includes all the quiz samples: \(\{_{1},...,_{T}\}\). We use \(_{\{_{1},...,_{T}\}}(f_{T}())\) to denote the final performance of active learning model. Note that \(\) is different from each \(_{t}\) because: (i) the evaluation sets contain all the previous quiz data, and (ii) it only evaluates the final model, \(f_{T}\). Intuitively, using \(\{_{1},...,_{T}\}\) should be more preferable than using a single \(_{t}\) because more data samples are available to better represent the data distribution. However, the combination is not straightforward because each quiz is sampled using the optimal selection proposal at that time. We propose a principled way to combine multiple quizzes to achieve an unbiased \(\) with theoretical guarantees. We further formulate an active feedback strategy to integrate the dual objectives of learning and testing and provide a theoretical underpinning to utilize some labeled test samples to improve the model's generalization capability.

### Unbiased Low-variance Estimation of Model Risk

For a passive learning setting, where the model is fixed, the risk can be directly estimated through an importance weighting sampling procedure, such as the ARE scheme developed in . Let \(q()\) denote the test sample selection proposal, then the risk estimate can be defined as

\[_{n,q}=^{n}^{(i)})}{q(^{(i)})}}_{i=1}^{n}^{(i)})}{q(^{(i)})} (f_{}(^{(i)}),y^{(i)})\] (2)

We can use the central limit theorem to show that \(_{n,q}\) is an unbiased estimation of \(R\). Furthermore, their difference follows a zero-mean Gaussian asymptotically:

\[(_{n,q}-R)(0, _{q}^{2}),\ _{q}^{2}=)}{q()}([(f_ {}(),y)-R]^{2}p(y|)dy)p() \] (3)When \(n\) is large, we also know that \(n[_{n,}][]{n}_{q}^{2}\). Then, the optimal \(q()\) minimizes the variance of the estimate. Using variational analysis, the optimal \(q()\) is

\[q^{*}() p()(f_{}( ),y)-R]^{2}p(y|)y}\] (4)

which minimizes the expected squared difference between the estimate and the true risk (also the variance of the asymptotic Gaussian). Since the pool distribution remains fixed when only a small number of instances are sampled for AL, we can keep \(p()\) as a uniform distribution over the pool. Since the importance weighting proposals are known up to a normalization factor, we have \(p()=}\) where \(N_{U}\) is the pool size. To optimize model evaluation in an active learning setting, we can evaluate the model at each time stamp t in the following manner: First, we obtain an active testing proposal, denoted as \(q_{t}\), by substituting (4) with \(f_{_{t}}\). Next, we obtain the active quizzes denoted as \(_{t} q_{t}\). We then evaluate the model's performance at time t using (2).

However, there are two main limitations to this approach. First, the test proposal given by (4) contains an unknown quantity, the true risk \(R\). Previous studies have attempted to address this issue by predicting the true risk from the pool. However, in section 3.3, we demonstrate that commonly used true risk predictors are not accurate, particularly in the early stages of active learning. To address this, we propose a multi-source true risk predictor. Second, the current test setting does not allow for sharing of data between previous and current quizzes due to the use of different active proposal distributions. This inefficiency not only wastes data, but also reduces the stability of the model evaluation, as the test samples in each quiz may be too small to provide a robust evaluation. Therefore, in section 3.4, we propose an estimator for the active learning setting that can integrate quizzes sampled from different proposal distributions while remaining optimal.

### Intermediate Estimate of the True Risk

In the optimal test sample selection proposal given by (4), a critical component is the true risk, which is unknown and needs to be estimated. A straightforward solution is to leverage the current model  or some proxy model  to predict the potential risk over the entire pool as \(R_{}\). However, the following analysis shows that this is equivalent to the model uncertainty in the classification setting.

**Proposition 1**.: _Under the classification setting and when a standard (i) \(0-1\) loss or (ii) cross-entropy loss is used, \(R_{}\) is equivalent to measuring the model uncertainty._

More details of the proposition are provided in Appendix B.2. The above proposition shows that \(R_{}\) can only capture the model uncertainty instead of the true risk. When the labeled training samples are limited, it is possible that the model uncertainty can not reflect the level of true risk. We propose to combine the current training risk, the model uncertainty, and the current test risk (using \(_{t}\) that we have) to get an aggregated multi-source estimate of the true risk:

\[R_{,t}^{}=_{L}|R_{train}+|_{U}| R_{}+n_{t}}{|_{L}|+|_{U}|+n}\] (5)

The multi-source estimate can more effectively avoid overestimating the risk when only using the testing information and underestimating the risk when only using training information. By aggregating these two sides of information, neither underfitting or overfitting of the model will harm the estimation to a significant extent. The effectiveness of the multi-source aggregated risk estimation has been demonstrated in our empirical evaluation.

### Active Quiz Integration for Final Risk Estimation

We have shown the optimal way to select test samples to evaluate a fixed model in the passive learning setting. However, we still need to extend to our scenario where there are multiple stages and AL while AT are intertwined. We propose to combine the quiz results by assigning a set of weights \(_{t}\) on them. We will show that the proposed \(_{t}\) is optimal given the fixed quizzes. Let's denote a sequence of independent risk estimates (treated as random variables) as \(}=(_{_{1}}(f_{T}),.._{_{T}}(f_{T})^{})\), where each \(_{_{t}}(f_{T})\) asymptotically converges to the true risk \(R(f_{T})\). We should also note that we are using each \(_{_{t}}\) to evaluate the same model \(f_{T}\), not using the actual values of the estimate at time \(t\) (which is the evaluation of \(f_{t}\)). That is, we always utilize previously collected sets of quiz samples to evaluate the _current_ model.

\[_{_{t}}(f_{T})=^{n_{t}}w_{i }}_{i=1}^{n_{t}}w_{i}l_{i}=^{n_{t}}^{(i )})}{q_{t}(^{(i)})}}_{i=1}^{n_{t}}^{(i)})}{q_{t }(^{(i)})}(f_{T}(^{(i)}),y^{(i)})\] \[}(_{_{t}}(f_{T})-R(f_{T}) )}(0,_{t}^{2}(f_{T}))\] (6)

The variance \(_{t}^{2}(f_{T})\) is given by

\[)}{q_{t}()}([(f_{T}( ),y)-R]^{2}p(y|)y)p() \] (7)

We have defined the asymptotic Gaussian variance term when we use the quiz set \(_{t}\) from time \(t\) to evaluate the final model \(f_{T}\). Let \(C_{t}=1/_{t}^{2}(f_{T})\) denote the model confidence of \(f_{T}\) when evaluated on a test set that follows \(q_{t}\). Next, we formally show that if the model \(f_{T}\) is more confident about \(q_{t}\), then \(_{t}\) should take a larger weight in the integrated final evaluation result.

**Theorem 1**.: _Given fixed \(\{_{1},...,_{T}\}\), the weighted combination \(=_{t=1}^{T}v_{t}_{_{t}}\) where \(v_{t}=}{_{t=1}^{T}C_{t}}\) is the optimal estimator constructed by all samples in \(\{_{1},...,_{T}\}\)._

**Remark.** The weight \(v_{t}\) depends on how accurate the estimate using each quiz set is. If we assume that the model becomes more accurate throughout the entire process, then we know that \(_{T-1}\) should be closer to \(R\) than \(_{T-2}\), thus the weight on the most recent one is the largest. However, we still do not know \(R\) and have to estimate the weight. Here we adopt a similar way as estimating \(R_{}\) to estimate the difference terms \(C_{t}\) using the expected loss and \(_{_{t}}(f_{T})\): \(}=}_{i=1}^{n_{t}}_{y}^{ (i)})}{q_{t}(^{(i)})}[(f_{T}(^{(i)}),y)- _{_{t}}(f_{T})]^{2}p(y|^{(i)})\). With the optimal \(v_{t}^{*}\), we define the final estimate result as \(=_{t=1}^{T}v_{t}^{*}_{_{t}}\). In practice, if we do not pre-set the length of the entire AL process, we can treat each stage as the final exam. After each AL sampling round, we perform an active testing round as a quiz. Then we use the integration method above to get the final estimate. This way, we will always have the optimal evaluation of the model given all the test samples that have been selected.

### Active Feedback: Improve Model Learning Without Sacrificing Testing Accuracy

The proposed final risk estimation can provide label-efficient evaluation of the model performance. We can further improve the label efficiency on the the AL side since some of these labels can also be used for model training. Assume at the \(t\)-th quiz, we have sampled and labeled a set of test data \(_{t}\) according to the test proposal \(q_{t}()\). Since \(_{t}\) is also a labeled dataset, the active learner could leverage it as a source of feedback information to facilitate learning. In the proposed active learning-testing setting, the number of labeled samples for learning/training \(N_{L}\) and testing \(N_{T}\) both contribute to the limited overall annotation budget. Since learning and testing need to be sample-efficient, it is reasonable to assume that \(N_{L}\) and \(N_{T}\) are of similar magnitudes instead of having \(N_{L} N_{T}\) as in the existing active model evaluation works. We further denote \(_{}\) as the feedback set, which contains samples from the collection of quizzes. The active feedback process can be formally defined as a subset selection problem that optimizes a joint objective of learning and testing.

\[_{}^{*}=_{_{}\{Q_{1},...,Q_ {T}\}}[R(f_{|(_{L}_{})})+C||R- {R}_{(\{Q_{1},...,Q_{T}\}_{})}||]\] (8)

where \(C\) is a parameter to balance the two objectives: \((I)=R(f_{|(_{L}_{})})\) for learning and \((II)=||R-_{(\{Q_{1},...,Q_{T}\}_{})}||\) for testing.

**Theorem 2**.: _Both the learning and testing tasks have the same label complexity, which is in the order of \((1/)\). As a result, (1) the joint objective has an overall label complexity of \(((1/+N_{}})+(1/-N_{}}))\); (2) the balancing parameter \(C\) is in the order \((1)\)._

**Remark.** The Theorem reveals that it is possible to achieve an optimal balance between \((I)\) and \((II)\) by choosing a suitable \(_{}\) such that we can further improve the model learning performance while maintaining the risk estimation quality from our quizzes-testing process. It provides a foundation to justify the benefit of active feedback. It is worth to note that the optimal subset selection problem is usually NP-hard, but we can still draw a useful conclusion that an appropriate subset \(_{}\) can potentially improve the combined learning-testing objective rather than having \(_{L}\) and \(_{T}=\{Q_{1},...,Q_{T}\}\)always separated. The theorem can also be more intuitively interpreted by drawing analogy with human learning, who automatically incorporate knowledge or partial knowledge at inference time. In contrast, a typical supervised-learned ML model is frozen at evaluation/testing time. Thus, we need to explicitly add feedback so that the model can effectively improve the learning performance by getting new training samples.

Following the above theoretical result, we propose a selection proposal \(q_{}()\) for \(_{}\) based on the following learning-testing intuitions. In standard AL, we should select samples that are considered to be the most informative and diverse ones. From the informativeness perspective, samples with high losses could be challenging for the current model. From the diversity perspective, the newly added samples should be different from the current labeled training set, which can be achieved using a diversity metric, such as \(d(_{L},)=^{}A_{L}^{-1}}\), where \(A_{L}=+_{_{L}}^{}\), where \(>0.\)From the evaluation perspective, removal of feedback samples should have as little impact to the risk estimation as possible. Since the test samples are chosen according to \(q()\), it is less harmful to choose a feedback sample with a large \(q()\) as similar testing samples are likely to be selected into the test data. To this end, we propose the following feedback selection strategy:

\[^{*}=*{arg\,max}_{(,y)_{t}}q_{ }(,y;)=*{arg\,max}_{(,y) _{t}}q^{(t)}()(f_{t}(),y)+ d( _{L},)\] (9)

where \(\) is a scaling parameter. We construct \(_{}\) after each active quiz round. The samples are added in \(_{L}\) to re-train the model, and the risk estimation is also re-weighted after the removal.

### Early Stopping with Instant Risk Estimations

In the ATL framework, the unbiased risk estimation after each AL step can be used to construct an effective convergence indicator to support early stopping of AL. In particular, we propose to use the change of a moving average of active risk estimations:

\[_{t}=^{t}v_{i}_{i}}{_{i=t-w}^{t} v_{i}}-^{t-1}v_{i}_{i}}{_{i=t-w-1}^{t-1}v_{i}}\] (10)

Prior works show that leveraging the unlabeled data works well empirically to stop AL early [2; 6]. To this end, we propose to further augment \(_{t}\) with stabilized predictions (SP) from the unlabeled data , where \(=1-_{}( y())\).

## 4 Experiments

The proposed ATL framework is generic and can be applied to any existing active learners. In our experiments, we choose two commonly used models, Gaussian Processes (GP) and neural networks, to demonstrate the effectiveness of the proposed work. The GP is applied to a 2D synthetic dataset to help us understand the important behavior of the proposed test sampling strategy and how active feedback benefits the entire learning process. The neural network model is applied to relatively larger scale image datasets, including MNIST, FashionMNIST and CIFAR10 to demonstrate the practical performance of the proposed framework. As for the AL strategy, ATL is designed to be AL-agnostic. To demonstrate the general applicability, the main results are obtained using uncertainty (_i.e.,_ entropy) based sampling as the AL strategy given its great popularity in many AL models. In Appendix C.2.3, we show that ATL can be easily integrated with a wide range of commonly used AL algorithms.

### Experimental Settings

For synthetic experiments, we generate 2,500 data samples based on a moon-shaped distribution with two smaller inverted moon-shaped clusters at two remote corners. The dataset is shown in Figure 1 (a). We add the smaller clusters because the learning performance is usually closely related to how AL explores the unknown regions in the feature space and the designed shape allows us to visualize testing and feedback samples to evaluate their effectiveness. Moreover, we create two levels of imbalance in the dataset, the intra-class imbalance and the inter-class imbalance, to simulate an imbalanced class distribution. For the real-world experiments, we use the cross-entropy loss for risk evaluation. When we compare with the true risk \(R\), we use the average evaluation results of the model on a large hold-out subset of the dataset (10,000 data samples) to represent \(R\). The hold-out test set does not interact with the AL or AT processes, thus is considered a fair evaluation. In real-world experiments, we adopt the same procedure with the total pool containing 30,000 data samples.

In each test sampling round, we compute over the entire unlabeled pool according to (4). We use a deterministic way of summing over all classes based on the posterior distribution predicted by the model. The multi-source estimate only utilizes labels of the training samples and previously selected test samples along with the predictions for unlabeled samples. Later in the active feedback stage, if a test sample is selected to be added to the training set, we remove it from the testing set. For test and feedback selections, we sequentially sample \(n_{t}\) times to obtain each batch. The testing process does not involve re-training the model, thus the batch mode has no effect.

### Synthetic Data Experiments

In Figure 3, we show the distribution of test samples selected using (1) AL sampling, (2) random sampling, (3) ATL w/o feedback, and (4) ATL at an early stage (_i.e.,_ quiz 9) and later stage (_i.e.,_ quiz 18) of the learning process. We observe that using AL sampling for test point selection has less accurate risk estimation performance due to highly biased sample selection near the decision boundary. Random sampling, on the other hand, follows the true data distribution, so it mainly selects from the central region, where the data is dense. Our proposed test sampling focuses on the most representative regions as random sampling does, but it also covers other interesting regions with relatively low density (_e.g.,_ the right edge where the minority blue class is mainly located), and the model evaluation reaches the highest accuracy in the early phases of learning.

In Figure 2(d), we plot the feedback with the same annotation cost to demonstrate its learning-testing trade-off. By providing part of the testing data as effective feedback, the estimation error is slightly increased. However, the sacrifice in model evaluation accuracy pays off from the training side. The feedback helps the AL model build a more representative training set and learn a more reasonable decision boundary than without feedback. It allows the AL model to gain a greater advantage in the early stage of training. Figures 2(e)-2(h) show the test data distribution in a later stage at quiz 18. The proposed testing selection is the only one that picks samples to test the critical but low-density regions. More importantly, Figure 2(h) shows that learning with feedback is most successful as it acquires an informative training dataset and a near-optimal decision boundary in the end.

### Real-Data Experiments

The main results of real-world experiments consist of three parts: (1) risk estimation during standard AL (without active feedback), (2) AL performance and risk estimation with active feedback, and (3) AT-based early stopping. In this section, we focus on showing the results from the first two and leave the details of part (3) in the Appendix. In the AL without feedback case, we compare the estimation error results of ATL-NF (no feedback) with ARE quizzes integrate  and the adapted Active Testing  and Active Surrogate Estimation  (referred to as AT integrate and ASE integrate). We adapt the AT and ASE baseline to use in the AL setting by adjusting the pool size \(N\) at each round and including the selected training loss to maintain the unbiased estimator \(_{LURE}\). We also need to train a surrogate model using fewer samples than normal (we use a separate NN model re-trained every few rounds). Because the AL model and the surrogate models are both severely under-trained in the early stage of AL, and that the training samples must also be considered in \(_{LURE}\), the estimation

Figure 3: Active test samples selected by different criteria at quiz 9 [(a)-(d)], and 18 (e)-(h)

[MISSING_PAGE_EMPTY:9]

benefits, we can potentially save the labeling budget for both learning and evaluation purposes. We report further ablation studies on the feedback proposal in Appendix C.

We also provide a study on the size of the feedback set. As mentioned in the proof for Theorem 2, we keep the size of feedback simple in this work. This is to be consistent with our theoretical analysis and the experiments show that the active feedback process is helpful in this generic setting. Further details about extending this have been discussed as part of the future directions. However, even in the simple setting of fixed feedback size, we can see that the learning and testing performances do not consistently and monotonically change with respect to the feedback size. From Tables 4 and 5 below, we can see that in general, model risk (learning performance) is better when we use a larger feedback size, but at the same time the estimation error (testing performance) may become much worse. The model risk on CIFAR10 behaves differently with the feedback size, probably because the model performance is not good enough and adding difficult samples in this stage does not necessarily help with the generalization ability.

## 5 Conclusion

We address a real-world challenge for sample-efficient learning, where valuable labels from human experts that can be used for testing/evaluation are also scarce. We propose an ATL framework that builds an active learning-testing-feedback loop to achieve a label-efficient evaluation of an AL model on the fly, with the potential to further improve the model performance without extra labels and opening up to the creation of systematic early stopping criteria. We theoretically prove that ATL has an unbiased and label-efficient estimator and provide an analysis that shows how the label complexity dependencies are maintained through active feedback. The experiments show that ATL optimizes the number of human labels needed in learning by simultaneously acting as a fair referee and an educative teacher.

  Dataset & Feedback size & AL round & 4 & 8 & 12 & 16 & 20 \\   & 83\% & \(0.86 0.09\) & \(0.53 0.04\) & \(0.40 0.08\) & \(0.30 0.02\) & \(0.20 0.03\) \\   & 67\% & \(0.87 0.08\) & \(0.52 0.03\) & \(0.35 0.03\) & \(0.30 0.02\) & \(0.21 0.02\) \\   & 50\% & \(0.88 0.07\) & \(0.53 0.04\) & \(0.39 0.03\) & \(0.26 0.01\) & \(0.19 0.03\) \\   & 25\% & \(0.94 0.06\) & \(0.54 0.03\) & \(0.42 0.08\) & \(0.35 0.02\) & \(0.25 0.02\) \\   & 20\% & \(0.99 0.04\) & \(0.56 0.08\) & \(0.43 0.06\) & \(0.38 0.01\) & \(0.24 0.02\) \\   & 83\% & \(0.74 0.02\) & \(0.67 0.03\) & \(0.60 0.03\) & \(0.54 0.02\) & \(0.51 0.03\) \\   & 67\% & \(0.77 0.04\) & \(0.68 0.03\) & \(0.59 0.03\) & \(0.56 0.03\) & \(0.52 0.02\) \\   & 50\% & \(0.74 0.03\) & \(0.65 0.04\) & \(0.59 0.02\) & \(0.56 0.03\) & \(0.51 0.01\) \\   & 25\% & \(0.76 0.02\) & \(0.70 0.01\) & \(0.62 0.02\) & \(0.59 0.05\) & \(0.53 0.03\) \\   & 20\% & \(0.77 0.02\) & \(0.71 0.02\) & \(0.64 0.02\) & \(0.61 0.04\) & \(0.54 0.04\) \\   & 83\% & \(1.92 0.06\) & \(1.71 0.02\) & \(1.67 0.07\) & \(1.59 0.04\) & \(1.57 0.04\) \\   & 67\% & \(1.96 0.05\) & \(1.75 0.02\) & \(1.64 0.04\) & \(1.58 0.04\) & \(1.58 0.06\) \\   & 50\% & \(1.90 0.05\) & \(1.76 0.02\) & \(1.65 0.03\) & \(1.58 0.02\) & \(1.53 0.02\) \\   & 25\% & \(1.94 0.08\) & \(1.76 0.03\) & \(1.70 0.03\) & \(1.64 0.04\) & \(1.59 0.02\) \\   & 20\% & \(1.91 0.03\) & \(1.76 0.02\) & \(1.73 0.03\) & \(1.59 0.02\) & \(1.63 0.02\) \\  

Table 4: Hold-out test risk using different feedback criteria over 20 AL rounds

  Dataset & Feedback size & AL round & 4 & 8 & 12 & 16 & 20 \\   & 83\% & \(50.2 39.8\) & \(21.0 24.3\) & \(7.36 8.44\) & \(11.4 12.7\) & \(7.59 4.45\) \\   & 67\% & \(25.6 23.4\) & \(29.3 29.7\) & \(6.90 8.05\) & \(6.24 6.71\) & \(7.50 5.07\) \\   & 50\% & \(14.6 22.1\) & \(16.9 13.7\) & \(3.19 2.63\) & \(4.15 3.20\) & \(1.87