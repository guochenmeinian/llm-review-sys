# On the Complexity of Learning Sparse Functions with Statistical and Gradient Queries

Nirmit Joshi

Toyota Technological Institute at Chicago

nirmit@ttic.edu

&Theodor Misiakiewicz

Toyota Technological Institute at Chicago

theodor.misiakiewicz@ttic.edu

&Nathan Srebro

Toyota Technological Institute at Chicago

nati@ttic.edu

###### Abstract

The goal of this paper is to investigate the complexity of gradient algorithms when learning sparse functions (juntas). We introduce a type of Statistical Queries (SQ), which we call Differentiable Learning Queries (DLQ), to model gradient queries on a specified loss with respect to an arbitrary model. We provide a tight characterization of the query complexity of DLQ for learning the support of a sparse function over generic product distributions. This complexity crucially depends on the loss function. For the squared loss, DLQ matches the complexity of Correlation Statistical Queries (CSQ)--potentially much worse than SQ. But for other simple loss functions, including the \(_{1}\) loss, DLQ always achieves the same complexity as SQ. We also provide evidence that DLQ can indeed capture learning with (stochastic) gradient descent by showing it correctly describes the complexity of learning with a two-layer neural network in the mean field regime and linear scaling.

## 1 Introduction

In recent years, major efforts have been devoted to understanding which distributions can be learned efficiently using gradient-type algorithms on generic models (Abbe and Sandon, 2020; Allen-Zhu and Li, 2020; Malach et al., 2021; Damian et al., 2022; Abbe et al., 2021b, a, 2022, 2023; Bietti et al., 2023; Glasgow, 2023; Dandi et al., 2023, 2024; Edelman et al., 2024; Kou et al., 2024). In this paper, we focus on learning sparse functions (i.e. "juntas" (Blum and Langley, 1997)), that is functions that depend only on a small number \(P\) out of a much larger set \(d P\) of input coordinates. The challenge in this setting is to identify the few relevant coordinates. For some sparse functions, such as noisy parities, learning is believed to require \(O(d^{P})\) runtime (Kearns, 1998), while others, such as linear functions, are easy to learn in \((d)\) time. Which functions are easy to learn and which are hard? What is the complexity of learning a specific sparse function? Recent works (Abbe et al., 2022, 2023) unveiled a rich "leap" hierarchical structure and saddle-to-saddle dynamics that drives learning in this setting. The goal of the present paper is to provide a general characterization for the complexity of learning sparse functions that go beyond _(i)_ hypercube data and Fourier analysis, and _(ii)_ CSQ (see below) and focusing only on the squared loss.

The notion of complexity we consider is the _Statistical Query_ (SQ) complexity, which studies learning by measuring expectations up to some worst-case tolerance (see (Kearns, 1998; Bshouty and Feldman, 2002; Reyzin, 2020) and Section 3). Although based on worst-case error (or almost equivalently, additive independent noise) rather than the sampling error encountered in practice, statistical query complexity has been proven to be a useful guideline for studying the complexity of learning. Inparticular, gradient computations are a special case of statistical queries. In specific cases which include binary functions or gradients on the squared or cross-entropy loss, gradient queries are equivalent1 to the restricted class of _Correlation Statistical Queries_ (CSQ) which are strictly less powerful than general statistical queries. Lower bounds on the CSQ complexity have thus been seen as corresponding to the complexity of gradient-based learning2 in these restricted cases. Part of the motivation for this paper is to emphasize that this relationship is limited to very specific loss functions and does not hold more generally. In order to study the complexity of gradient algorithms for general output and loss functions, we introduce a type of statistical query which we call _Differentiable Learning Query_ (DLQ). These queries are defined with respect to a specific loss \(\)--denoted by DLQ\({}_{}\)--and are given by gradients on a loss \(\) with respect to an arbitrary model. Specifically, DLQ\({}_{}\) algorithms correspond to SQ algorithms with queries of the type

\[(y,)=(f(,),y) _{=0}, f:^{d}.\] (1)

Depending on the loss \(\) and target distributions, learning with DLQ\({}_{}\) can be less, equal, or more powerful than CSQ.

For inputs on the hypercube \((\{ 1\}^{d})\) and CSQ, Abbe et al. (2022, 2023) showed that the complexity of learning sparse functions is sharply captured by a _leap exponent_ defined in terms of the non-zero Fourier coefficients of the sparse function. Informally, it states that \((d^{k_{*}})\) queries are necessary and sufficient to learn with CSQ, where \(k_{*}\) is the minimum number of coordinates one need to add at once to "leap" between non-zero Fourier coefficients. E.g., consider the following two sparse functions:

\[y_{1} =x_{s_{*}(1)}+x_{s_{*}(1)}x_{s_{*}(2)}+x_{s_{*}(1)}x_{s_{*}(2)}x_ {s_{*}(3)}+x_{s_{*}(1)}x_{s_{*}(2)}x_{s_{*}(3)}x_{s_{*}(4)},\] (2) \[y_{2} =x_{s_{*}(1)}x_{s_{*}(2)}x_{s_{*}(3)}+x_{s_{*}(1)}x_{s_{*}(2)}x_ {s_{*}(4)}+x_{s_{*}(1)}x_{s_{*}(3)}x_{s_{*}(4)}+x_{s_{*}(2)}x_{s_{*}(3)}x_{s_{ *}(4)}.\]

Both functions depend on an (unknown) subset of \(P=4\) coordinates \(\{s_{*}(1),s_{*}(2),s_{*}(3),s_{*}(4)\}\). For \(y_{1}\), the monomials are ordered such that we add only one new coordinate to the support at a time: the leap exponent is \(1\) and \((d)\) queries are sufficient to learn \(y_{1}\). For \(y_{2}\), we need to add three new coordinates at once: the leap exponent is \(3\) and \((d^{3})\) queries are required to learn \(y_{2}\).

We extend and generalize this charactarization in several significant ways:

* We go beyond binary input space and allow for arbitrary measurable space with product distribution. In particular, the leap complexity arises from the structure of the permutation group rather than a specific functional basis.
* We go beyond CSQ and squared loss, and tightly characterize the query complexity for learning sparse functions with SQ and DLQ\({}_{}\) algorithms with any loss \(\). This complexity is in terms of a leap exponent defined analogously as above, but now over a set system \(_{} 2^{|P|}\) of _"detectable"_ subsets \(S[P]\), where "detectability" depends on the type of queries \(\{,,_{}\}\). Learning with different loss functions are not necessarily comparable to CSQ or SQ and depend on the sparse function. However, we show that some losses, such as \(_{1}\) or exponential loss, are "generic", i.e. \(_{_{}}=_{}\) for any sparse function, and always match the SQ-leap exponent. This shows that differentiable learning with these losses is as powerful as learning with SQ for juntas.
* Finally, we introduce a _cover exponent_--defined on the same system of detectable sets \(_{}\)--which captures learning with \(\{,,_{}\}\) when the queries are chosen _non-adaptively_, i.e., without adapting to the responses of previous queries. This can be roughly thought of as learning with a single gradient step, versus many consecutive steps in the adaptive case. The contrast between the two helps understand to what extent learning relies on adaptivity and can help frame other related results--see discussion in Section 8.

We summarize the complexity of learning a sparse function with SQ, CSQ, DLQ\({}_{}\) and adaptive/non-adaptive queries in Table 1. To see some of these notions play out, consider again the two examples in Eq. (2):

\[y_{1}: (_{})=1,(_{})=4, (_{})=(_{})=1,\] \[y_{2}: (_{})=(_{})=3,(_{})=(_{})=1.\]For \(y_{1}\), \(_{}\) will require \((d)\) adaptive or \((d^{4})\) non-adaptive queries to learn with squared loss--equivalent to \(\), while \((d)\) adaptive/non-adaptive queries suffice with \(_{1}\)-loss--equivalent to \(\). For \(y_{2}\), \(_{}\) will only require \((d)\) queries, either adaptive or non-adaptive, with \(_{1}\)-loss, compared to \((d^{3})\) queries with the squared loss. Proposition 6.2.(c) provides an example where \((_{})<(_{_{}})<(_{})\).

\(_{}\) algorithms are quite different than (stochastic) gradient descent algorithms. On one hand, \(_{}\) allows evaluating arbitrary gradients rather than following a trajectory, and is thus potentially more powerful than (S)GD on generic models3. On the other hand, the lower bound on \(_{}\) algorithms in Table 1 is against _worst-case_ noise--much more pessimistic than sampling noise encountered in SGD or GD-on-the-training-set (Abbe et al., 2021). Nevertheless, \(_{}\) does provide useful guidance and we expect that it _does_ capture important aspects of the computational complexity of GD in a number of settings. To demonstrate this, Section 7 considers learning sparse functions on the hypercube using online SGD on a two-layer neural network. We show that in this setting, the \(_{}\)-leap exponent correctly captures learnability in the mean-field regime and linear scaling. Namely, for \(_{}\)-leap \(1\) functions, SGD on loss \(\) learns in \(O(d)\)-steps, while for leap greater than \(1\), the dynamics remains stuck in a suboptimal saddle subspace and require \((d)\)-steps to escape.

## 2 Setting

Junta Recovery Problem.A sparse recovery (junta learning) problem with \(P\) relevant coordinates is defined in terms of a coordinate input space \(\), an output space \(^{4}\), a marginal distribution \(_{x}\) over \(\) and a link distribution \(_{y|}\) over \(\) given elements of \(^{P}\). This specifies a joint distribution \(_{y,}\) over a measurable space \(^{P}\), where the marginal distribution \(_{x}^{P}\) is the product distribution, and \(y_{y|}\). We further denote \(_{y}\) to be the marginal distribution over \(\). We further denote the junta problem by a tuple \(:=(_{x},_{y|})\), where the support size \(P\) and the spaces \(\) and \(\) are implicit. Consider some examples,

\[(\{ 1\}^{P}), y=h_{*}()+\ \ \ \ h_{*}:\{ 1\}^{P}\ \ \ ;\] (3)

\[(,}_{P}), y((h_{*}()))\ \ \ \ h_{*}:^{P}.\] (4)

For \(d P\), we define the junta recovery problem as learning the family of distributions

\[^{d}_{}:=^{d}_{,s}:s([d],P) }\] (5)

where \(([d],P)\) is the set of non-repeating sequences from \([d]\) of length \(P\), and the distribution \(^{d}_{,s}\) is a distribution over \(^{d}\) such that

\[^{d}_{x}\ \ \ \ \ y(x_{s(1)},,x_{s(P)})_{y |}.\]

In words, \(^{d}_{}\) is the set of distributions on \((y,x_{1},,x_{d})\) where \(\) follows the product distribution \(^{d}_{x}\) and the output \(y\) only depends on an (unknown) sequence of \(P\) coordinates. For ease of notation, we will denote \(^{d}_{s}=^{d}_{,s}\) when the junta problem \(=(_{x},_{y|})\) is clear from context.

  \(q/^{2}=(d^{k_{*}})\) & Adaptive & Non-adaptive \\  \(\) & \(k_{*}=(_{})\) & \(k_{*}=(_{})\) \\  \(\) & \(k_{*}=(_{})\) & \(k_{*}=(_{})\) \\  \(_{}\) & \(k_{*}=(_{_{}})\) & \(k_{*}=(_{_{}})\) \\  

Table 1: Complexity of learning a sparse function over \(d\) input coordinates with different query types based on Theorem 5.1. \(_{}\) is defined in Section 2, \(\) and \(\) of a set system in Definition 1, and systems \(_{}\) of detectable sets in Definition 2 based on test functions depending on query type \(\), as specified in Eq. (14).

Success as Support Recovery.For any sequence \(s}([d],P)\), we additionally denote the associated unordered set of its elements by \(S:=\{s(i):i[P]\}\). We consider learning as "succeeding" if it outputs the correct index set. That is, we say a junta learning algorithm "succeeds" in learning \(_{}^{d}\) if for every \(s_{*}}([d],P)\), it outputs \([d]\) such that \(=S_{*}=\{s_{*}(i):i[P]\}\).

For our purpose, just the recovery of relevant coordinates is the objective as we only care about the complexity as a scaling of \(d\). Once we recover the relevant coordinates \(S_{*}\), learning the ordered sequence \(s_{*}\) corresponds to a problem of size \(P\), and thus its complexity is independent of \(d\). Note that the precise ordering \(s_{*}\) may not be identifiable if for some other \(s^{}_{*}\) with \(S_{*}=S^{}_{*}\) we have \(^{d}_{s_{*}}=^{d}_{s^{}}\); this is possible when the measure \(_{y|}\) has symmetries with respect to coordinates of \(\). We further emphasize that even the support \(S_{*}\) may not be identifiable, when \(y\) is independent of some coordinates of \(\) or when only using a restricted query access to the distribution--we will return to this issue in Section 5.

Does the learner know the link \(_{y|}\)?We always assume that the learner knows \(_{x}\). In our formulation, we also assume that the learner additionally knows the link \(_{y|}\), and the only unknown is \(s_{*}\). Indeed, our lower bounds hold even for this easier setting. But under mild assumptions, our upper bounds can be obtained with an algorithm that does not require knowing \(_{y|}\) (the complexity still depends on the problem \(\))--see discussion in Section 5.

Well Behaved Distributions.For our lower bounds, we will consider junta problems \(=(_{x},_{y|})\), which are "well-behaved" in a way that is standard in the hypothesis testing literature. Let \(_{y,}\) be the induced joint distribution on \((y,)\) where \(_{x}^{P}\) and \(y_{y|}\). For any subset \(U[P]\), let \(_{y,,U}\) be the marginal distribution of \((y,(z_{i})_{i U})\) and

\[^{0}_{y,,U}=_{y,,U}_{}^{P-|U|},\] (6)

meaning that \((y,z_{1},,z_{P})^{0}_{y,,U}\) has \((y,(z_{i})_{i U})_{y,,U}\) and \((z_{i})_{i[P] U}}{{}}_{x}\) independently of \((y,(z_{i})_{i U})\) (we replace \(z_{i}\) for \(i U\) with independent draws from the marginal). The marginal distribution of \(\) under \(^{0}_{y,,U}\) is still \(_{x}^{P}\).

**Assumption 2.1**.: _For any \(U[P]\), we have \(_{y,}^{0}_{y,,U}\), and the Radon-Nikodym derivative \(_{y,}/^{0}_{y,,U}\) is square integrable w.r.t. \(^{0}_{y,,U}\), i.e._

\[_{y,}}{^{0}_{y,,U}} L^{2}(^{ 0}_{y,,U})\ \ \ \ \ U[P].\] (7)

This is a standard and implicit assumption in the hypothesis testing literature whenever a corresponding null distribution is considered (here all \(^{0}_{y,,U}\), i.e., with label \(y\) depending only on a strict subset \(U[P]\)); more specifically for statistical query lower bounds (Feldman et al., 2017; Damian et al., 2024), low-degree likelihood ratio (Hopkins, 2018; Kunisky et al., 2019), or contiguity lower bounds (Perry et al., 2018). It always holds when \(\) is finitely supported. We further comment on the necessity of this assumption in Appendix A.1.

## 3 Statistical and Differentiable Learning Queries

We will consider three classes of learning algorithms, all based on the statistical query paradigm, but differing in the type of queries allowed, as captured by a set \(\) of allowed queries.

For a number of queries \(q\), tolerance \(>0\) and a set \(^{^{d}}\) of measurable functions \(^{d}\), a \(\)**-restricted statistical query algorithm**\(\)-\((q,)\) for junta learning takes an input distribution \(\) over \(^{d}\) and operates in \(q\) rounds where at each round \(t\{1,,q\}\), it issues a query \(_{t}\), and receives a response \(v_{t}\) such that

\[|v_{t}-_{}[_{t}(y,)]|_{_{0}}[_{t}(y,)^{2}]},\] (8)

where \(_{0}=_{y}_{}\) is the associated decoupled "null" distribution where \(\) and \(y\) are independent, but follow their marginals5. The query \(_{t}\) can depend on the past responses \(v_{1},,v_{t-1}\). Afterissuing \(q\) queries, the learner \(\) outputs \([d]\). We say that \(\)**succeeds in learning**\(^{d}_{}\) if for any \(_{s_{*}}^{d}_{}\) and any responses \(v_{t}\) satisfying (8) for \(=_{s_{*}}\), \(\) outputs \(=S_{*}\).

Above we allow the queries to be chosen adaptively, i.e., depending on past responses. We also consider \(\)**-restricted non-adaptive statistical query algorithms**, which we denote by \(\)-\((q,)\), where the query functions \(\{_{t}\}_{t[q]}\) are fixed in advance and do not depend on the past responses. I.e. a non-adaptive algorithm is specified by a list of queries and a mapping from the responses to an output \([d]\).

Statistical Queries (Sq):In regular, unrestricted _Statistical Query_ learning, the allowed query set, denoted by \(_{}\), is the set of all measurable functions. With slight overloading of notation, we refer to the class of these algorithms simply as \((q,)\) and \((q,)\).

Correlation Statistical Queries (Csq):It is a special subclass of statistical queries, which require \(\) (usually we allow the input and output spaces to be abstract), and are restricted to:

\[_{}=\{(y,)=y() :\;\}.\] (9)

We denote the class of adaptive and non-adaptive algorithms making such queries as \((q,)\) and \((q,)\) respectively.

Differentiable Learning Queries (DLQ) with Loss \(\):Let \( V\), which is an open subset of some normed vector space \(V\), be the output space of our models, e.g. usually \(V==\) for models with a single output unit, but we may have \(V==^{r}\) with multiple output units. We consider a loss function \(:\) that is locally Lipschitz continuous in its first argument for every \(y\). The loss is additionally equipped with a derivative operator \(: V\) as a part of the loss definition such that for any \(\) and \(y\), we have \((,y)_{1}(,y)\), the set of generalized Clarke subderivatives of \((,y)\) at \(\). This is a standard generalization of derivatives to non-differentiable and non-convex losses; in particular, note that \((i)\) for differentiable losses, \(_{1}(,y)\) is a singleton with the true gradient of \((,y)\) at \(\), and \((ii)\) for convex losses (in the first argument), \(_{1}(,y)\) is the set of subderivatives of \((,y)\) at \(\). Finally, let \(:=\{f:^{d} f(, )\) is differentiable at \(=0\) for all \(^{d}\}\) be the set of allowed models8. Then the allowed differentiable learning query set is:

\[_{_{}}=\{(y,)=[ }{}f(,)]_{=0}^{}( f(,0),y) f\}.\] (10)

That is, at each round the algorithm chooses a parametric model \(f(,)\), parameterized by a single scalar \(\), and the query corresponds to the derivative (with respect to the single parameter \(\)) of the loss applied to the model, at \(=0\). This captures the first gradient calculation for a single-parameter model initialized at zero. But the derivative at any other point can also be obtained by querying at a shifted model \(f_{()}(,)=f(,+)\), and the gradient with respect to a model \(f(,)\) with \(r\) parameters \(^{r}\) can be obtained by issuing \(r\) queries, one for each coordinate, of the form \(f_{(,i)}(,)=f(,+ e_{i})\), where \(e_{i}\) is the standard basis vector. Queries of the form \(_{_{}}\) can thus be used to implement gradient calculations for any differentiable model, noting that the number of queries \(q\) is the number of gradient calculations _times the number of parameters_. Finally, observe that, for differentiable losses, the queries of the form (1) are equivalent to the form mentioned in (10) due to the chain rule.

We denote the class of adaptive and non-adaptive algorithms making such queries as

\(_{}(q,)\):=\(_{_{}}\)-\((q,)\) and \(_{}(q,)\):=\(_{_{}}\)-\((q,)\).

**Remark 3.1**.: _More common in the SQ literature is to restrict the query in \(L^{}\) (or equivalently, require precision relative to \(L^{}\)). Precision relative to \(L^{2}(_{0})\) is more similar to VSTAT , and is more powerful than relative to \(L^{}\), and our lower bounds hold against this stronger notion. In our algorithms and upper bounds we only need this additional power when \(y(,y)\) is unbounded. If we further assume that these functions are bounded, e.g., the labels \(y\) are bounded and \(\) continuous, our queries have bounded \(L^{}\) and thus operate in the more familiar SQ setting._

## 4 Leap and Cover Complexities

**Definition 1**.: _We define the leap and cover complexities for any system of subsets \( 2^{[P]}\)._

1. _For any system of subsets_ \( 2^{[P]}\)_, its_ leap complexity _is defined as_ \[():=_{U_{1},,U_{r} \\ _{i=1}^{r}U_{i}=[P]}_{i[r]}\ |U_{i} _{j=0}^{i-1}U_{j}|.\] (11)
2. _We define the_ cover complexity _of_ \(\) _to be_ \[():=_{i[P]}\ _{U,i U}\ |U|.\] (12)

**Remark 4.1**.: _We always have \(()()\). Both \(\) and \(\) complexities are closed under taking the union of subsets in \(\). Also, when \(()=_{U}U[P]\), then we use the convention \(()=()=\). See a discussion about this convention in Appendix A.3 and in particular, the definition of the relative leap and cover complexities in Definition 3._

Here \(\) is the system of subsets which are "detectable", and will depend on the the query access model. Intuitively, the leap and cover complexities of \(\) capture the exponent of \(d\) in the query complexity when recovering the support of an unknown \(s_{*}([d],P)\), for adaptive and non-adaptive algorithms respectively. To discover the relevant coordinates of \(s_{*}\), that correspond to \(U\), one needs to enumerate over \((d^{|U|})\). Hence, a non-adaptive algorithm, which fixes the queries in advance, requires \((d^{k_{i}})\) queries to discover \(i^{}\) relevant coordinate i.e. \(s_{*}(i)\), where \(k_{i}=_{i U}|U|\). Therefore, non-adaptive algorithms need a total number of queries that scales as \((d^{()})\) to learn \((s_{*})\). On the other hand, adapting queries using previous answers can greatly reduce this complexity as seen in the example in (2) in Section 1. This is captured by the leap complexity, which measures the maximum number of coordinates we need to discover at once. Finally, the set system of detectable subsets will depend on the type of allowed queries.

**Definition 2** (Detectable Subsets).: _Let \(\) be a junta problem. Denote_

\[L_{0}^{2}(_{x})=T L^{2}(_{x}):_{z_{x}}[T(z) ]=0}\]

_the set of zero-mean functions. For a set of test function \( L_{2}(_{y})\) we say that \(U[P]\) is \(\)**-detectable** iff_

\[ T,\  T_{i} L_{0}^{2}(_{x})\ \ i U _{_{y,x}}T(y)_{i U}T_{i}(z_ {i}) 0.\] (13)

_We denote \(_{}()\) the set of \(\)-detectable sets, i.e. those sets satisfying (13)._

The set of relevant test functions depend on the query types allowed and we define:

\[\ ,&_{}=L^{2}(_{y}) \ \ L^{2}\ .\\ \ \ \ ),&_{ }=\{y y\}\ .\\ \ _{},&_{_{}}=\{y^{ }(,y):, V\}.\] (14)

While the queries of the form (13), where \(_{}\) for \(\{,,_{}\}\) is given by (14), are less general than \((y,)\) with \(_{}\), they can be implemented by the corresponding query types \(_{}\) and are sufficient for deciding between "\(S[d]\) maps to the corresponding \(U[P]\)" or "\(S S_{*}\)". The sets \(_{},_{},_{_{}}\) from (14) used for detectibility arise naturally in the proof of the lower bounds of Theorem 5.1.

To ease notation, for query type \(\), we use the shorthand \(_{}:=_{_{}}\), and

\[_{}():=(_{}())= (_{_{}}()),_{ }():=(_{}())=(_{_{}}()).\]

We refer to these as the \(\)_-leap exponent_ and \(\)_-cover exponent_ of the problem \(\).

Main Result: Characterizing the Complexity of Learning Juntas

**Theorem 5.1**.: _For any junta problem \(\) and any loss \(\), there exists \(C>c>0\) (that depend on \(P\),\(\) and the loss, but not on \(d\)), such that for query types \(\{,,_{}\}\) with corresponding test function sets \(_{}\) as defined in (14):_

**Adaptive**.: _Let \(k_{*}=_{}()\). There exists an algorithm \((q,)\) that succeeds in learning \(^{d}_{}\) with \(=c\) and \(q=Cd^{k_{*}}\). And if \(\) satisfies Assumption 2.1, then for any \((q,)\) such that \(q/^{2} cd^{k_{*}}\), no algorithm \((q,)\) succeeds at learning \(^{d}_{}\)._

**Non-adaptive**.: _Let \(k_{*}=_{}()\). There exists an algorithm \((q,)\) that succeeds in learning \(^{d}_{}\) with \(=c\) and \(q=Cd^{k_{*}}\). And if \(\) satisfies Assumption 2.1, then for any \((q,)\) such that \(q/^{2} cd^{k_{*}}\), no algorithm \((q,)\) succeeds at learning \(^{d}_{}\)._

**Remark 5.2**.: _In the positive results in Theorem 5.1, we used all the allowed complexity to have many (\(q=(d^{k_{*}})\)) queries, and kept the tolerance constant. More generally, it is possibly to trade off between the number of queries \(q\) and tolerance \(\), at a cost of a log-factor: For \(k_{*}=_{}()\) and \(k_{*}=_{}()\), respectively, there exists algorithms \((q,)\) and \((q,)\) that learn \(^{d}_{}\), for any \(q C(d)\) and \( c\) with \(q/^{2} Cd^{k_{*}}(d)\)._

The proof of this theorem is deferred to Appendix B. Theorem 5.1 shows that the leap and cover complexities sharply capture the scaling in \(d\) of statistical query algorithms when learning \(^{d}_{}\).

**Remark 5.3**.: _The above upper bound uses that \(\) and therefore the \(T,T_{i}\) in Definition 2 are known. In the case when \(\) is unknown, one can follow a similar strategy as in Damian et al. (2024b) and randomize the transformations \(T\) and \(T_{i}\) over a sufficiently large (but finite independent of \(d\)) linear combination of functions in \(_{}\) and \(L^{2}_{0}(_{x})\). Under some regularity assumption on \(\) and \(\), one can show by anti-concentration that with constant probability, the expectation in condition (13) is bounded away from \(0\) by a constant independent of the dimension._

## 6 Relationship Between \(,\) and \(_{}\)

Obviously, \(\), and indeed we see that \(_{}_{}\) in Definition 2 because of which \(_{}_{}\) and \(_{}_{}\). For binary \(\), these query models collapse, but otherwise there can be an arbitrary gap.

**Proposition 6.1** (\(\) versus \(\)).: _For any \(\), let \(_{}:=_{}()\), and \(_{}:=_{}()\). If \(||=2\) (binary output), then we always have \(_{}=_{}\) and \(_{}=_{}\) and \(_{}=_{}\). On the other hand if \(||>2\), the \(\)-exponents can be much smaller than the \(\)-exponents: e.g., there exist a setting with \(_{}=_{}=1\) and \(_{}=_{}=P\)._

Similarly, \(_{}\) by definition, and thus, \(_{_{}}_{}\) and \(_{_{}}_{}\).

**Proposition 6.2** (\(_{}\) versus \(\) and \(\)).: _Consider any \(=(_{y},_{y|})\)._

* _Let_ \(,\)_. For the squared loss_ \(:(u,y)(u-y)^{2}\)_, we always have_ \(_{_{}}=_{}\)_, and thus,_ \(_{}=_{_{}}\) _and_ \(_{}=_{_{}}\)_._
* _A sufficient condition for_ \(_{}=_{_{}}\) _is to have_ \((_{_{}})\) _dense in_ \(L^{2}_{0}(_{y})\)_. Conversely, if there exists nonzero_ \(T L^{2}_{0}(_{y})\) _bounded with_ \(T(y)\) _orthogonal in_ \(L^{2}(_{y})\) _to any functions in_ \(_{_{}}\)_, then there exists a problem_ \(\) _such that_ \(_{_{}}=_{_{}}>_{}=_{}\)_._
* _There exists a loss_ \(\) _and a junta problem_ \(\) _such that_ \(_{}()<_{_{}}()<_{}()\)_. Similarly, we can have_ \(_{_{}}>_{}\)_._

The condition in Proposition 6.2.(b) can be seen as a universal approximation property of neural networks with activation \(y(,y)\)(Cybenko, 1989; Hornik, 1991; Sonoda and Murata, 2017). The next lemma gives a few examples of losses with \(_{}=\). For concreteness, we consider \(=\) and \(\).

**Theorem 6.3**.: _For \(\{_{1}:(u,y)|u-y|,_{}:(u,y)(1-uy,0)\}\), then \(_{_{}}()=_{}()\) for all7 problems \(\). If we further assume \([-M,M]\) for \(M 0\), then \((u,y)=e^{-uy}\) (exponential loss) has \(_{_{}}()=_{}()\) for all problems \(\)._

The cases of \(_{1}\) and Hinge loss follow directly from universal approximation of neural networks with linear threshold activation. The proofs of the above propositions and lemma can be found in Appendix C. Propositions 6.1 and 6.2 combined with Theorems 6.3 and 5.1 directly imply a number of separation results between adaptive and non-adaptive algorithms and between different loss functions. See examples (2) in the introduction, and further examples in Appendix C.

## 7 Gradient Descent on Neural Networks

The goal of this section is to connect the complexity of \(\) to gradient descent on standard neural networks. We focus on the simple case of \((\{+1,-1\}^{d})\) uniformly distributed on the hypercube, and \(\) and \(=\). In this setting, condition (13) in Definition 2 simplifies to: there exists \(T_{}\) such that \(_{_{y,}}[T(y)_{i U}z_{i}]=_{ _{y,}}[T(y)_{U}()] 0\), where \(_{U}():=_{i U}z_{i}\) denote the standard Fourier-Walsh basis. In particular, the set \(_{}\) contains exactly all non-zero Fourier coefficients of \(h_{}():=[y|]\), and we recover the leap exponent of Abbe et al. (2023) as discussed in the introduction.

We train a standard two layer neural network (see (NN1) in Appendix D) of width \(M\), using online SGD with a loss function \(:_{ 0}\), i.e. SGD on \(_{(,y)_{}^{d}}[(f_{} (;),y)]\) for \(_{s_{}}^{d}_{}^{d}\). More specifically, we train the parameters \(=\{_{j}:j[M]\}\) using batch-SGD with loss \(\) and batch size \(b\) from initialization \((_{j})_{j[M]}}}{{}}_ {0}\) specified in (69). At each step, given samples \((\{(_{ki},y_{ki}):i[b]\})_{k 0}\), the weights are updated using

\[_{j}^{k+1}=_{j}^{k}-(_{i[b]} ^{}(f_{}(_{ki};^{t}),y_{ki})_{ {}}_{*}(_{ki};_{j}^{k})+\,_{j}^ {k}),\] ( \[\] -bSGD)

where \(\) is the step-size and we allow for a \(_{2}\) regularization with parameter \(_{+}\). Recall that \(^{}(u,y)_{1}(u,y)\) is the defined derivative of \((,y)\) at \(u\). We define the test error

\[(f)=_{_{s_{}}^{d}}\,[(f( ),y)],\] (15)

and further introduce the excess test error \(}(f)=(f)-_{f:\{ 1\}^{d}} ()\).

Dimension-free dynamics.In the junta learning setting, when \(y\) only depends on \(P d\) coordinates, following Abbe et al. (2022, Secion 3), the SGD dynamics (\(\)-bSGD) concentrates on an effective _dimension-free_ (DF) dynamics as \(M,d\) and \( 0\). This equivalence holds under a certain assumption on the loss function, and other assumptions on the initialization and activation that are similar to the setup of Abbe et al. (2022) (see Appendix D for details, especially DF-PDE).

Dimension-free dynamics' alignment with the support. In the above limiting regime, \(_{_{}}\) crisply characterizes (DF-PDE) dynamics' alignment with the support.

**Theorem 7.1** (Informal version of Theorem D.6).: _If \(_{_{}}=1\) then for some time \(t\), the output of the model at time \(t\) of (DF-PDE) dynamics depends on all coordinates \(z_{i}\). On the other hand, if \(_{_{}}>1\), then there exists a coordinate \(i\) such that for any time \(t 0\), the output of the model at time \(t\) of (DF-PDE) dynamics does no depend on \(z_{i}\)._

This establishes that the ability of (DF-PDE) dynamics (comparable to (\(\)-bSGD) in the linear scaling) to learn all relevant coordinates depends on \(_{_{}}()=1\). That is if \(_{_{}}>1\), then (DF-PDE) remains stuck in a suboptimal saddle subspace. On the other hand, if \(_{_{}}=1\), then (DF-PDE) dynamics escapes this subspace and the weights align with the entire support.

Learning of \(_{_{}}=1\) with finite SGD.Showing directly that (DF-PDE) dynamics indeed reach a near global minimizers of the test error remains challenging. Alternatively, we show that a specific layer-wise training dynamics similar to Abbe et al. (2022) achieves a vanishing excess error for \(_{_{}}()=1\) settings in the linear scaling of samples.

Roughly speaking, we train the first layer for \(k_{1}=P\) steps and then the second layer weights for \(k_{2}=O_{d}(1)\) steps using batch-SGD with batch size \(b=O_{d}(d)\), both for a loss \(\). We consider a polynomial activation \((x)=(1+x)^{L}\) of degree \(L 2^{8P}\). The most notable difference from Abbe et al. (2022) is that we further slightly perturb step-sizes for each coordinate \(^{w_{i}}=_{i}\) with \(_{i}[1/2,3/2]\), and denote \(=(_{1},,_{d})^{d}\) for the first layer training. This perturbation is necessary to break possible coordinate symmetries; see Remark 7.3.

**Theorem 7.2** (Informal version of Theorem E.1).: _For a convex and analytic loss \(\), almost surely over the perturbation \(\) and the initial bias \(\) the following holds. If \(_{_{}}()=1\), then the above layer-wise SGD training dynamics with total sample size \(n=_{d}(d)\) and \(M=_{d}(1)\) achieves excess test error \(}(f_{}(;^{k_{1}+k_{2}}))=o(1)\) with high probability._

The formal statement and the precise training specifications can be found in Appendix D. This result generalizes Abbe et al. (2022, Theorem 9) beyond squared loss.

**Remark 7.3**.: _A slight coordinate-wise perturbation in the step-sizes for the first layer training is necessary to break the potential coordinate symmetries in the output \(y\)--see discussion in Abbe et al. (2022, Appendix A). This can be removed by either stating the theorem for all but some measure zero set of Leap-1 functions as in Abbe et al. (2022), or by studying the dynamics for \(O( d)\) steps._

The query complexity of \(b\)-batch SGD on \(M\) neurons for \(\) SGD-steps is \(T_{}=(bM d)\). The above theorems show that for \(_{_{}}()=1\), \(=(d/b)\) steps with \(M=(1)\) neurons--and therefore \(T_{}=(d^{2})\)--suffices to learn the support and minimize the excess test error. Furthermore, for \(_{_{}}()>1\) and neural networks trained in the mean-field regime, \(=(d)\) (and therefore \(T_{}=(d^{2})\)) is not enough. We further comment on the general conjectural picture in Appendix A.2.

Numerical Simulation.We consider a function similar to \(y_{2}\) in (2) with \(_{}=3\) but \(_{}=1\). Specifically, we set \(P=4\) and \(=\{\{1,2,3\},\{1,2,4\},\{1,3,4\},\{2,3,4\}\}\), and define \(y=h_{*}()\) where

\[h_{*}()=_{U}_{*}(U)_{U}(),\ \ _{*}(U)([-2,2])\ \ U.\] (16)

We train with online 1-batch SGD (\(\)-bSGD) on a two-layer net with different loss functions (without any regularization) and stepsize \( 1/d\), where we consider ambiant dimensions \(d\{100,300,500\}\). In Figure 1, we plot the test mean squared error versus \(\) (thus also scaled with \(1/d\)), over 10 trials. Additionally, we also plot the continuous time (DF-PDE) in (dashed black line) that corresponds to the limit \(d\).

Figure 1: The function \(h_{*}()\) in (16) has \(_{}=3\) but \(_{}=1\). For the squared loss (left plot), (DF-PDE) remains stuck at initialization (no learning), and to escape the saddle, SGD requires a number of iterations that increases faster than \(O(d)\). For the absolute loss (center plot) or the other loss (right plot), we have \(_{_{}}=_{}=1\), and the SGD dynamics learns in \((d)\) steps and (DF-PDE) learns in \(O(1)\) continuous time.

Conclusion and Outlook

In this paper, we considered learning juntas over general product distributions with statistical query algorithms. To capture learning with gradient evaluations over a general loss and arbitrary model, we introduced _Differentiable Learning Queries_ (\(_{}\)), which can be seen as a generalization of correlation statistical queries beyond squared loss. We then showed that the complexity of learning juntas with either \(\), \(\), or \(_{}\) algorithms is sharply characterized in terms of a _leap exponent_ (adaptive queries) or a _cover exponent_ (non-adaptive queries). These exponents are defined in terms of a minimal combination of detectable sets to cover the support, where the system of detectable sets depends on the allowed queries. In general, the leap and cover exponents for different losses are not comparable. However, we identify "generic" losses, including \(_{1}\), where \(_{}\) algorithms are as powerful as \(\) algorithms for learning juntas. We further showed that \(_{}\) can indeed capture the complexity of learning with SGD in the case of data on the hypercube.

Worst-case v.s. One-pass v.s. Multi-pass SGD.\(_{}\) (like \(\)) is defined in terms of worst-case noise. It is well understood that worst-case noise is theoretically very different from estimating population gradients based on samples (either independent samples as in one-pass (\(\))GD, or with repeated use of samples as in full-batch or multi-pass) when highly specialized models are allowed . However, we expect \(_{}\) to still capture the complexity of one-pass SGD in many settings of interest--with "regular" models--such as in Section 7. With sample reuse (e.g. multi-pass) the situation is more complex: Dandi et al. (2024) showed that two steps of full-batch gradient descent with square loss goes beyond \(\). Heuristically, and in light of our work, two steps on the same batch can be seen as a single gradient evaluation on a modified loss, thus going beyond \(=_{_{ego}}\), but remaining inside \(_{}\) for some perturbed non-quadratic loss \(\). Indeed, we expect generally that multi-pass SGD on a "regular" model will remain in \(\).

Multi-index and beyond.In this paper, we focused on learning sparse functions. We hope the modular nature of our analysis framework (defining detectable sets in terms of test functions, and leap and cover complexities of set systems), our definition of \(\), and the distinctions we emphasize between \(\), \(\) and \(\) and between adaptive and non-adaptive complexities, will be helpful in guiding and contextualizing analysis in other settings such as learning single-index or multi-index functions [e.g. 18, 19, 20, 21, 22, 23]. For example, the information exponent for single-index  can be seen as analogous to our \(\)-cover exponent, the generative exponent for single-index  as analogous to our \(\)-cover exponent, and the isoLeap exponent for multi-index  as analogous to our \(\)-leap exponent. It would be interesting to obtain a unified understanding of these specialized treatments and extend our general framework to multi-index models, and also to learning under other invariances beyond permutations and rotations.

In our setup, we emphasize generic input and output spaces, without a field structure. This emphasizes that when learning juntas, polynomials or degree of input or output coordinates is irrelevant. Defining multi-index models and introducing rotational invariance necessitates a field structure and gives rise to the relevance of polynomial degrees and decomposition.

An important point is that when considering permutation (as in juntas) vs. rotational (as in multi-index models) invariance, one must consider not only the invariance structure of the target function class, but also the input distribution (i.i.d. coordinates as in our case, or more generally exchangeable vs. spherical) and learning rule. E.g., learning a parity over input coordinates requires only \(( d)\) samples, but a rotationally equivariant algorithm effectively learns parities also over rotated axis, which requires \((d)\) samples  (and thus \((d^{2})\) runtime). This also explains the need for \((d)\) steps and thus \((d^{2})\) runtime to learn leap-1 functions using SGD on a rotationally invariant neural net in Section 7. In order to break the \((d)\)-sample lower bound, we need to break the rotation-equivariance, e.g. using sparse initialization and \(_{1}\) regularization, which indeed can achieve \(((d))\) sample complexity.

Acknowledgement.This research was done as part of the NSF-Simons Sponsored Collaboration on the Theoretical Foundations of Deep Learning and the NSF Tripod Institute on Data, Econometrics, Algorithms, and Learning (IDEAL).