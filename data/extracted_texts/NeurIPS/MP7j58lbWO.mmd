# Probing Social Bias in Labor Market Text Generation

by ChatGPT: A Masked Language Model Approach

 Lei Ding\({}^{1*}\), Yang Hu\({}^{2}\), Nicole Denier\({}^{3}\), Enze Shi\({}^{1}\), Junxi Zhang\({}^{5}\),

&Qirui Hu\({}^{6}\), Karen D. Hughes\({}^{3,4}\), Linglong Kong\({}^{1*}\), Bei Jiang\({}^{1}\)

\({}^{1}\)Department of Mathematical and Statistical Sciences, University of Alberta, Canada

\({}^{2}\) Department of Sociology, Lancaster University, UK

\({}^{3}\) Department of Sociology, University of Alberta, Canada

\({}^{4}\) Department of Strategy, Entrepreneurship and Management, University of Alberta, Canada

\({}^{5}\) Department of Mathematics and Statistics, Concordia University, Canada

\({}^{6}\) Department of Statistics and Data Science, Tsinghua University, China

{lding1,nicole.denier,eshi,khughes,lkong,beii1}@ualberta.ca

yang.hu@lancaster.ac.uk, junxi.zhang@concordia.ca, hqr20@mails.tsinghua.edu.cn

Corresponding Authors

###### Abstract

As generative large language models (LLMs) such as ChatGPT gain widespread adoption in various domains, their potential to propagate and amplify social biases, particularly in high-stakes areas such as the labor market, has become a pressing concern. AI algorithms are not only widely used in the selection of job applicants, individual job seekers may also make use of generative LLMs to help develop their job application materials. Against this backdrop, this research builds on a novel experimental design to examine social biases within ChatGPT-generated job applications in response to real job advertisements. By simulating the process of job application creation, we examine the language patterns and biases that emerge when the model is prompted with diverse job postings. Notably, we present a novel bias evaluation framework based on Masked Language Models to quantitatively assess social bias based on validated inventories of social cues/words, enabling a systematic analysis of the language used. Our findings show that the increasing adoption of generative AI, not only by employers but also increasingly by individual job seekers, can reinforce and exacerbate gender and social inequalities in the labor market through the use of biased and gendered language.

## 1 Introduction

The rapid advancements in generative Large Language Models (LLM) like ChatGPT [OpenAI, 2023], mark a significant technological shift. These models have not only propelled the field of Natural Language Processing (NLP) but have also found widespread application across numerous sectors [Zhao et al., 2023, Yang et al., 2024]. However, as these models are incorporated into social and economic practices, they bring to the fore critical ethical concerns, especially regarding their potential to propagate and amplify existing social biases and attendant inequalities, particularly within high-stakes domains such as the labor market [Liang et al., 2022].

Recognizing the growing potential of generative AI use in employment practices, our research primarily aims to identify and understand the impact of biases in the application of generative LLM within the labor market. We focus particularly on ChatGPT, investigating how this widely used LLM influences the propagation of biases in job advertising and application processes.

The complexity of _automating_ bias evaluation in textual content poses significant challenges. Traditional approaches in social sciences, such as content analysis, often rely on manual word counts from static lists (Gaucher et al., 2011), which may miss the subtleties and unlisted language cues that advanced NLP technologies can detect. In addition, by considering words individually, these traditional approaches often fail to capture the contextual meanings that emerge from the interplay of words within entire sentences. To address this limitation and build toward a more solid bias evaluation method, we develop a novel bias evaluation algorithm called **PRISM**: Probability **R**anking **I**as **S**core via **M**asked language model. PRISM involves masking words sequentially within texts and using the Masked Language Models (MLM) (Devlin et al., 2018; Liu et al., 2019) to predict the likelihood of alternative tokens, thus allowing us to assess bias with a ranking-based approach that leverages established word lists from social science research to provide contextual sensitivity, enabling a systematic and detailed analysis of language use.

Additionally, the inherently opaque nature of LLMs like ChatGPT, which function as black boxes without transparent access to their internal structures or parameters, adds another layer of complexity. We propose a method of probing these biases by simulating and analyzing how job seekers use ChatGPT to craft applications (output texts) in response to real job postings (input texts), as illustrated in Figure 1. This simulation reveals insights into the biases embedded within ChatGPT's training data and their potential impacts on real-world human resource practices.

Utilizing our PRISM algorithm in tandem with job posting and application text pairs, we explore the correlation between generated content and bias propagation. This comprehensive and novel simulation offers a distinctive lens through which to view how biases might influence the job application process.

In essence, this paper seeks to bridge the gap between rapid technological advancements and the ethical considerations raised by the use of generative LLMs. Through our research, we emphasize the importance of ensuring that AI use promotes core social values of fairness and equality in the labor market as these technologies become increasingly integral to our daily lives.

Our key contributions include:

* We propose **PRISM**, a brand new paradigm for bias evaluation combines with validated word lists capturing directional cues (based on social science research) with MLM to assess biases in texts. It advances existing methods in terms of efficiency, flexibility, robustness as well as theoretical properties.
* We draw on a novel experimental design to probe the black-box of social biases in ChatGPT models to understand both the biases inherent in their training data and their implications for real-world job application scenarios.
* Analysis of bias across four different social dimensions demonstrates inherent biases in job postings are likely reproduced in ChatGPT-generated job applications, with a tendency for the model to exacerbate and reinforce these biases.

This paper is structured as follows: we first review the current landscape of bias evaluation in NLP and social sciences. Following this, we introduce our bias scoring algorithm and provide experimental evidence supporting our methodology. We conclude with an analysis of job postings and applications mediated by ChatGPT, evaluating our approach's broader applicability and discussing the social implications of our empirical findings.

Figure 1: Overview of the paradigm for bias probing experimental design.

Background and Related Works

Bias & Fairness Evaluation in NLPThe evaluation of bias within natural language processing (NLP) presents complex challenges, as methodologies vary significantly across studies (Garrido-Munoz et al., 2021; Blodgett et al., 2020). Traditional approaches range from analyzing cosine similarity in word embeddings (Bolukbasi et al., 2016) to diverse methods such as correlation, clustering, classification, and visualization (Caliskan et al., 2017; Gonen and Goldberg, 2019; Ding et al., 2022; Shi et al., 2024). Recent works have focused on detecting bias in language models that rely on manual sentence templates (Kurita et al., 2019) or creating benchmarks that require high-cost crowd-workers (Nangia et al., 2020; Nadeem et al., 2021) and across various NLP downstream tasks including text classification (De-Arteaga et al., 2019; Blodgett et al., 2016), coreference resolution (Zhao et al., 2018), natural language inference (Dev et al., 2020), and machine translation (Stanovsky et al., 2019).

Bias Evaluation for TextThe domain of text bias evaluation is notably more challenging than evaluating the NLP models, often requiring extensive human expert intervention or resorting to simplistic and heuristic methodologies. Many existing approaches are also limited to specific types of bias, making them difficult to adapt to other contexts. Dhamala et al. (2021) measure bias by computing the cosine similarity of word embeddings (Mikolov et al., 2013; Pennington et al., 2014) with respect to the gender direction (\(-\)) (Bolukbasi et al., 2016) and averaging over sentences. Cryan et al. (2020) compare a lexicon-based approach and a fine-tuned BERT model with a Crowdsourced label dataset. Spinde et al. (2021) developed a media bias dataset through costly expert annotation, a process not easily generalizable to other domains. Raza et al. (2024) explore the use of named entity recognition for detecting biased words within texts. Yet this approach also requires the creation of costly labeled training data for each task and model training.

Labor Market Bias Evaluation in Social SciencesA substantial body of research has documented prevalent gender stereotypes and their role in (re)producing inequalities - gender segregation (Kjeldstad and Nymoen, 2012; England, 2010), gender wage/promotion gaps (Blau and Kahn, 2020), motherhood penalties (Glauber, 2018), and fatherhood premiums (Killewald, 2013) - in the labor market. Further research shows that gendered language plays a crucial role in maintaining and reproducing gender stereotypes (Gonzalez et al., 2019). Psychological studies also show that women and men, given their gender socialization, tend to use and be attracted to different gendered languages and linguistic styles (Gaucher et al., 2011). For example, women tend to employ and identify with a more communal language style, including the use of words related to social and emotional contexts (Bem, 1974). In contrast, masculine language is typically characterized by a style that highlights agentic traits. Gendered language is found across a wide range of contexts, and in the labor market, it features prominently in job advertisements, the language used in job applications and interviews, as well as performance management processes (Hu et al., 2024, 2022). While existing research has often focused on gendered language from the labor demand side in terms of, for example, employers' wording of job advertisements (Hu et al., 2022), far less attention has been paid to the language used by job candidates in response to job advertisements in order to secure a job, despite an increase in individual job seekers' use of ChatGPT. This study thus fills this important gap by assessing gendered languages from both the labor demand (job advertising) and supply (job application) sides. In doing so, it highlights the relational use of language in the job application process as a quintessential example of social interactions in action. It aims to explore and reveal the extent to which gender biases are present and indeed circulated and exacerbated through the interplay between languages used in job advertisements and job applications.

## 3 Bias Evaluation Algorithm for Text

### Motivations

When assessing social bias in textual content, previous methodologies often begin with a straightforward approach: selecting keywords for simple frequency counts. For instance, this might involve comparing the total word count of feminine and masculine words. This technique is prevalent in psychological and sociological studies as described in Section 2. More contemporary methods have advanced to include the use of static word embeddings to measure semantic similarities among words,although these approaches still treat each word individually. To go further, researchers need to acquire expensive, labeled training data for specific tasks and do the model training.

In contrast, our objective is to refine and further advance these existing approaches to measuring textual bias with three useful and practical settings:

* Beyond merely analyzing each word individually, the algorithm should aim to consider the contextual meanings of entire sentences, allowing for a more nuanced and comprehensive view of the text.
* The algorithm does not require costly human-labeled training data and circumvents the process of model training or fine-tuning. This aspect is particularly valuable in scenarios where the necessary labeled data is not readily available, allowing for more flexible and scalable applications.
* The algorithm should incorporate established and rigorous word inventories from social science research to guide the bias calculation in a contextually embedded and domain-specific manner (e.g., accounting for specificities of the labor market context). This incorporation of domain knowledge ensures that the assessments are both empirically grounded and contextually salient.

### Problem Setup and Algorithm Implementation

In this section, we detail our algorithm under the settings introduced above. Given a text \(T\) comprising \(n\) words \(T=\{w_{1},w_{2},,w_{n}\}\), we iteratively mask each word \(w_{i}\) and input the modified masked text \(T_{ i}=\{,w_{i-1},\) [MASK\(],w_{i+1},\}\) into an MLM, which outputs the probability distribution over the vocabulary for the masked position \(i\), denoted as \(P( T_{ i})\).

Then, to obtain the direction signal for score calculation, we require two predefined word lists representing different contexts--such as gender with a feminine word list \(F=\{f_{1},,f_{ F}\}\) and a masculine word list \(M=\{m_{1},,m_{ M}\}\). For each word in \(F\) and \(M\), we obtain the probability from the distribution \(P(|T_{ i})\). This yields two sets of probabilities: \(P_{F}=\{P(f_{1}|T_{ i}),P(f_{2}|T_{ i}),,P(f_{ F }|T_{ i})\}\) and \(P_{M}=\{P(m_{1}|T_{ i}),P(m_{2}|T_{ i}),,P(m_{ M }|T_{ i})\}\).

Next, we merge \(P_{F}\) and \(P_{M}\) and filter the probabilities by taking the top \(\) percent of the probabilities, as lower probabilities represent less likely predictions by the MLM and thus contribute minimally to our analysis of bias. This step allows us to focus on the most influential predictions which significantly determine the context of the sentence.

Finally, we calculate the rank of each probability within this merged list. Let \(R()\) denote the rank function of the probability based on the merged list. For each word \(w_{i}\) in the text \(T\) using the word lists \(F\) and \(M\) is denoted as \(R(P(f|T_{ i}))\) and \(R(P(m|T_{ i}))\), respectively. And the lower the rank indicates the higher the probability. The bias score for each word \(w_{i}\) is computed as the difference between the mean ranks of the two word lists:

\[S(w_{i})=_{f F}R(P(f|T_{ i}))-_ {m M}R(P(m|T_{ i}))\]

Figure 2: An illustration of the paradigm for **PRISM** that uses word lists for directional cues with MLM to compute bias score for text.

A positive score indicates a bias toward a mascul orientation, while a negative score suggests a bias toward a feminine orientation. This differential allows us to detect the direction of the bias, providing deeper insights into how gender nuances are embedded within the language.

Finally, the overall bias score for the text \(T\) is the mean of the scores for all words in the text:

\[B(T)=_{i=1}^{n}S(w_{i})\]

This score quantifies the bias present in \(T\). By analyzing these scores across various texts, we can evaluate both the extent and direction of linguistic bias, offering insights into the underlying gender biases conveyed through language. From a Bayesian perspective, in the absence of prior information, the mean serves as a robust estimator of central tendency. For future work, we recognize the potential benefit of incorporating additional information (e.g., part-of-speech tags, WordNet, etc.) to apply a weighted average or explore alternative aggregation methods, which could further enhance the performance of the bias assessment. The overall algorithm is illustrated in Figure 2, and is detailed in Algorithm 1.

```
0: Text \(T\) with \(n\) words \(\{w_{1},w_{2},,w_{n}\}\), Feminine word list \(F\), Masculine word list \(M\)
0: Bias score \(B(T)\)
1:for each word \(w_{i}\) in \(T\)do
2: Create \(T_{ i}\) by masking \(w_{i}\) in \(T\)
3: Predict distribution \(P( T_{ i})\) using MLM
4: Initialize \(P_{merged}=[\,]\)
5:for all words \(w\) in \(F M\)do
6: Append \(\{w,P(w|T_{ i})\}\) to \(P_{merged}\)
7:endfor
8: Sort and filter \(P_{merged}\) to retain top \(\%\) of entries
9: Calculate ranks for \(R(P(f|T_{ i}))\) and \(R(P(m|T_{ i}))\) in the filtered list
10: \(S(w_{i})=_{f F}R(P(f|T_{ i}))-_{ m M}R(P(m|T_{ i}))\)
11:endfor
12:\(B(T)=_{i=1}^{n}S(w_{i})\)
13:return\(B(T)\) ```

**Algorithm 1** PRISM: Probability Ranking blas Score via Masked language model

### Methodological Benefits of PRISM

EfficiencyOur algorithm eliminates the need for costly data labeling and model training. By leveraging predefined word lists developed by existing sociological research, our method avoids the resource-intensive processes associated with supervised learning, such as gathering expert annotations and training models from scratch. This approach not only expedites deployment but also ensures that the algorithm can be scaled and adapted swiftly and economically, making it highly practical for researchers and practitioners needing quick and reliable bias assessments in various settings.

Computational FlexibilityThe inherent flexibility of our method allows for the evaluation of bias across various dimensions simply by altering the word list cues. This adaptability means that different types of bias can be assessed without the need to relabel data or retrain models, significantly reducing the time and resources required for analysis. Whether exploring gender, race, age, or any other form of bias, our algorithm can adjust to new research questions with minimal adjustments. This also allows for the incorporation of substantively meaningful domain-specific word inventories from social science disciplines such as sociology, management studies, psychology, etc.

RobustnessRobustness in our method is two-fold. Firstly, we utilize ordinal measurements of word probabilities, focusing on relative positions (ranking) rather than the values. This method effectively mitigates issues arising from the predominance of low probabilities within a large pool of candidate words, which can lead to nonsensical outcomes. Secondly, our approach ensures robust results across different MLMs. Unlike other scoring methods using raw probabilities for calculation, our rank-based bias score method remains consistent even when different MLMs produce varying output probabilities. This dual approach minimizes the influence of outliers and maintains reliability across various computational models.

Theoretical PropertiesMoreover, we can test whether MLM's predictions have the same distribution on two word lists (\(M\) and \(F\)). Consider two groups of probabilities scores, \(\{P(f|T_{i})\}_{f F}\) and \(\{P(m|T_{i})\}_{m M}\), representing the scores samples from distribution \(_{F}\) and \(_{M}\). The rank sums, denoted by \(_{f F}R(P(f|T_{i}))\) and \(_{m M}R(P(m|T_{i}))\) respectively, allow us to test the hypotheses \(H_{i0}:_{F}=_{M}\) versus \(H_{i1}:_{F}_{M}\). The null hypothesis holds if there is no statistically significant bias toward mascaline or feminine language in a particular word \(w_{i}\). The following theorem provides a rigorous formulation of the test statistic and its asymptotic result.

**Theorem 1**: _When \(|F|\) and \(|M|\) are large, for each \(i[n]\), under \(H_{i0}\):_

\[_{m M}R(P(m|T_{ i})) N(, )\]

_If further we have \(|M|=|F|=K\), for each \(i[n]\), under \(H_{i0}\):_

\[S(w_{i}) N(0,)\]

Note that we interpret the prediction probabilities for the two word lists, \(\{P(f|T_{ i})\}_{f F}\) and \(\{P(m|T_{ i})\}_{m M}\), as scores that measure the association between the masked word and the words in the lists. A higher score indicates a stronger relationship. We also assume these word lists are sampled from larger sets of male or female-associated words. Thus, for each masked word, the scores \(\{P(f|T_{ i})\}_{f F}\) and \(\{P(m|T_{ i})\}_{m M}\) are viewed as samples from two underlying distributions, \(_{F}\) and \(_{M}\). Using the rank test from Theorem 1, we can determine whether \(_{F}=_{M}\). This test is particularly useful for detecting biased words or sentences. For non-biased words, the associations with the two word lists should be similar, implying \(_{F}=_{M}\). However, for biased words, the associations differ significantly, resulting in \(_{F}_{M}\).

### Algorithm Validation

To demonstrate the reliability of our scoring algorithm in identifying social biases within texts, we validate our method on two different tasks2:

Human Experts ValidationThis validation involved collaboration with six experienced professionals from the fields of sociology and management science. Each coder manually labeled a randomly selected subsample of job advertisements. Leveraging their extensive domain knowledge, these experts meticulously classified the advertisements, assessing them for levels of perceived gender bias. These categorical labels were then transformed into ordinal variables, enabling a detailed statistical comparison with the results produced by our scoring algorithm. This rigorous, expert-driven coding process ensured the reliability of our evaluation methodology.

We compute the Spearman rank correlation between the bias scores generated by our algorithm and the results from the manual labeling process. A Spearman correlation coefficient of 0.853 was obtained (Figure 3(a)), indicating a strong positive association between our algorithm's scores and the human experts' assessments. This result validates the algorithm's capacity to accurately reflect human judgments of bias, confirming its effectiveness as a tool for social bias detection.

Benchmark ValidationFurther validation was conducted using the BIOS dataset (De-Arteaga et al., 2019), which comprises personal biographies categorized by gender and various occupations. We employed gender-specific word lists from (Konnikov et al., 2022), such as {man, his, he...} versus {woman, her, she...}, as binary directional cues and designated gender as the ground truth label. Our algorithm demonstrated high performance, achieving an AUC of 0.97 in classifying gender, as illustrated in Figure 3(b). The AUC, or Area Under the ROC Curve, measures the ability of our model to distinguish between classes -- here, gender categories. This performance surpasses that of three baseline methods in [Dhamala et al., 2021] that rely on unigram or word embeddings, highlighting the effectiveness and potential applicability of our bias detection approach in broader NLP tasks.

## 4 Probing Methodology and Job Application Data Generation

Probing MethodologyTo explore the social biases inherent in ChatGPT, particularly in the context of the labor market, our study simulates the typical use case where job seekers employ ChatGPT to assist in drafting job applications. This approach allows us to investigate not only the biases that may emanate from ChatGPT's training data but also to understand how these biases could potentially influence real-world job application/hiring processes.

Probing the social biases within ChatGPT presents several challenges. Firstly, ChatGPT's model operates as a 'black box,' making it difficult to discern the internal processes that contribute to bias propagation. Secondly, the lack of access to the model's architecture or parameters further complicates direct examination. Therefore, our analysis adopts an indirect method, employing our known bias evaluation algorithm to detect and quantify the biases exhibited by ChatGPT, thereby illuminating how these biases might manifest in practical applications.

Job Application Data GenerationOur dataset comprises over 33K job postings collected from LinkedIn, reflecting a diverse range of industries and job types. To simulate realistic job application processes, we utilize the OpenAI API (GPT-3.5 Turbo, data collected on April 2024) to prompt ChatGPT with these job advertisements, instructing it to generate corresponding job applications for each job posting.

This method does more than replicate real-world scenarios where individuals respond to job postings--it also facilitates a comprehensive analysis of the generated texts across various sectors. By using job advertisements as standardized prompts, we ensure that any observed deviations from neutrality in the generated texts are attributable to the model's ingrained biases, rather than the content of the advertisements themselves. This setup is crucial for isolating the effects of ChatGPT's biases, allowing for an accurate assessment of bias presence and intensity using the quantifiable metrics provided by our bias score calculation method.

## 5 Analysing the Bias inside ChatGPT

### Dimensions of Gender Bias

We begin by introducing the four gender dimensions, each defined by a distinct set of gender-related word lists, which will form the basis of our analysis. In recent social science research, understanding gender bias involves not just recognizing the existence of biases but also evaluating their impacts in various contexts. Building on the framework proposed by Bem , Gaucher et al. , Konnikov et al. , we utilize specialized word lists to apply our social bias analysis across four different dimensions. Each dimension not only helps identify specific instances of bias but also offers insights into the broader social and psychological dynamics at play.

Psychological Cues:The psychological dimension assesses language context leaning towards communal attributes (e.g., "caring," "sympathetic," "attentive") commonly associated with femininity, or agentic attributes (e.g., "authoritative," "active," "confident") typically linked to masculinity.

Role Description:We evaluate job descriptions and roles using word lists that categorize terms associated with "soft" and "social" skills for feminine orientation, and "time-compressed" and "stressful" tasks, such as "multitasking," "pressure," "speed," for masculine orientation.

Work-Family Characteristics(WFC):This dimension examines employer policies and cultural expectations affecting gendered labor force participation, scrutinizing terms like "parental leave" and "flexible work" for feminine orientation versus "irregular and long work hours" and "weekend work" for masculine orientation.

Social Characteristics:We also analyze explicit gender references such as gendered pronouns and identity markers ("she," "he," "his," "her," "man").

### Correlation Analysis

We first analyze the correlation of job postings and job applications across each dimension of gender bias. Our findings indicate a consistent positive linear correlation between the bias scores of job postings and the ChatGPT-generated job applications. This trend suggests that the biases inherent in job postings are likely to be reproduced in job applications by generative AI, reinforcing and possibly amplifying the initial biases. This correlation is visually captured in Figure 3, illustrating the potential for cyclical reinforcement of biases through the use of generative AI in job application practices.

Figure 3 presents the statistical parameters for each analyzed dimension of social bias. The strongest correlation is observed in the Social Characteristics dimension with a correlation coefficient of 0.777, indicating a very strong positive relationship. This is followed by the Role Description dimension, which shows a correlation coefficient of 0.708. Both of these correlations suggest significant potential for the biases in job postings to be reproduced by AI in job applications in these dimensions.

The Psychological Cues and WFC dimensions exhibit lower but still substantial correlation coefficients of 0.644 and 0.451, respectively. The slopes of these relationships indicate the rate at which the bias scores from job postings predict those in job applications, with steeper slopes observed in the Social Characteristics dimension. This analysis clearly supports the hypothesis that inherent biases in job postings are likely reproduced in ChatGPT-generated job applications.

### Statistical Testing Analysis

In this section, we delve deeper into how ChatGPT influences bias reproduction within the job application process. Let \(X\) and \(Y\), represent the bias scores of job postings and those of the job applications generated by ChatGPT, respectively. We denote the population mean and variance of \(X\) as \(_{X}\) and \(_{X}^{2}\). A score close to zero indicates minimal bias (i.e., gender neutrality that is neither feminine nor masculine), a higher positive score signifies a bias towards masculine language, and a lower negative score indicates a bias towards feminine language. The aim is to evaluate how ChatGPT may exacerbate or mitigate these biases. The histogram and summary statistics of the bias scores are in Appendix A.5.

Shift in MeanWe propose the following hypothesis tests to assess shifts in mean:

\[H_{0}:_{X}_{Y} H_{1}:_{X}<_{Y}\]

Using the Wilcoxon signed-rank test, we determine whether there is a significant change in the mean bias score from the job postings to the applications.

Shift in MagnitudeFor the magnitude of bias, we assess:

\[H_{0}:|_{X}||_{Y}| H_{1}:|_{X}|<|_{Y}|\]

Figure 3: Result scatter density plot, for each of the bias dimensions where the x-axis is the job posting bias score and the y-axis is the job applications bias score. Where the darker color means there are more dots. The p-value is the significance of the correlation coefficient.

This test measures the central tendency of bias scores, examining if the absolute values (regardless of bias direction) decrease. The less the magnitude(i.e. closer to zero) the less bias it has.

Change in VarianceWe also explore the variability in bias scores:

\[H_{0}:_{X}^{2}_{Y}^{2} H_{1}:_{X}^{2 }>_{Y}^{2}\]

This variance test, employing Levene's test (Brown and Forsythe, 1974) for equality of variances, explores whether ChatGPT produces job applications with more uniform bias expressions compared to the job postings. It helps determine if there is a reduction in variance, which would suggest that ChatGPT standardizes the use of gendered language cues. Such standardization could potentially reinforce specific gender biases more consistently.

Shift in MeanThe testing for the mean shift in Table 1 reveals significant findings across several dimensions. Except for Role Description, all other dimensions exhibit statistically significant shifts toward more masculine language. This indicates a predominant inclination for ChatGPT to amplify the use of masculine language in simulated job applications over and above the original job postings, possibly due to its training on historically biased data. This shift raises concerns about the consolidation and exacerbation of masculine language. Such biases in AI-generated content could perpetuate gender disparities in professional settings, emphasizing the need for interventions in AI training processes to address and correct historical biases. In contrast, the Role Description dimension shows a mean shift toward a less masculine direction, but the bias in job postings has already been shown to be skewed toward a very masculine direction. In this case, ChatGPT seems to help mitigate this extreme masculine bias.

Magnitude of BiasThe magnitude of bias, assessed through the mean of the absolute bias scores, varies across the dimensions. The Psychological Cues and Role Description dimensions suggest that the overall intensity of bias--regardless of direction--does not increase. This could imply that while the direction of bias towards masculinity is pronounced, the degree of bias embedded within job applications does not intensify. Conversely, the WFC and Social Characteristics dimensions exhibit an increase in bias magnitude, indicating not only a shift towards masculine language but also an overall increase in the strength of biased expressions. This finding is particularly troubling as it suggests that AI-generated job applications in these areas may become more polarized, further entrenching gender-specific expectations in roles traditionally associated with work-life balance and social interactions.

Variability in Bias ExpressionThe variance results across all dimensions reveal a consistent decrease in job applications compared to job postings. This decrease in variance suggests that the language used by ChatGPT is more uniform across different applications, potentially indicating a standardization of language that leans towards masculine expressions. Such uniformity in language use could narrow the range of expressions and perspectives presented in job applications, limiting diversity and potentially skewing hiring decisions in favor of male candidates.

### Implications and Extended Analysis

Our statistical results underscore a critical issue: biases in job postings are not merely replicated but are amplified in job applications created by generative AI in response to the postings. This

 
**Dimensions** & **Mean** & **Magnitude** & **Variance** \\  Psychological Cues & \(\) & \(\) & \(\) \\ Role Description & \(\) & \(\) & \(\) \\ Work–Family Characteristics & \(\) & \(\) & \(\) \\ Social Characteristics & \(\) & \(\) & \(\) \\  

Table 1: Statistical testing results for each dimension. The mean result indicates whether the overall bias score is shifting toward the masculine (\(\)) or feminine (\(\)) direction. The magnitude result reveals whether the bias is moving toward zero (\(\)) or away from zero (\(\)). The variance assesses whether job application bias scores exhibit greater (\(\)) or lesser (\(\)) variance compared to the job postings. Please refer to Table 2, 3, 4 and 5 in Appendix for detail statistics.

phenomenon can be explained by the reinforcement of initial biases through the language processing and text generation capabilities of AI tools like ChatGPT, which tend to replicate and often intensify the language patterns they are trained on.

**Societal and Labor Market Implications:** The amplification of gender biases in AI-generated job applications has profound societal and labor market implications, suggesting that not only are stereotypical roles perpetuated through biased language, but they are also strengthened when individuals use AI tools like ChatGPT to assist with drafting job applications. This use of generative AI plays a crucial role in circulating and amplifying biases, which reinforces, rather than challenges, the gender biases underpinning persistent gender inequalities in the workplace. Such biases can compound, influencing job satisfaction, employee retention, and career advancement. The misallocation of human resources due to biased AI could reduce economic efficiency and innovation, potentially causing sectors to overlook qualified candidates. Furthermore, these persistent inequalities may spur regulatory and legal challenges, especially in countries with robust equal employment opportunity laws, with significant implications for social ethics, justice, and economic equality.

**Recommendations for Intervention:** To mitigate the reproduction of gender biases through LLMs, it is recommended that employers and AI developers implement more rigorous bias monitoring and mitigation strategies. This could include the use of debiased language models, regular audits of AI-generated content by independent third-party organizations, and the development of enhanced AI training datasets that reflect the diversity of the global job market. Additionally, public awareness and education initiatives should be promoted to increase understanding of AI's role in job application and its potential impacts, fostering a critical approach to AI tool usage in professional settings.

## 6 Conclusion

Our paper - including a novel experiment, new algorithm development, and empirical application and findings - contributes to the ongoing debates and developments in the ethical use of AI in labor market processes and practices. By identifying underlying biases in AI-driven text generation, this paper proposes novel strategies and methods for detecting and mitigating such biases. Through our **PRISM** algorithm and empirical application, we show that these strategies are not just theoretical but are intended as actionable steps toward ensuring that the integration of AI in the labor market supports equitable and fair employment opportunities for both employers and job seekers.