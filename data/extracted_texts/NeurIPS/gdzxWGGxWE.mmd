# How do Minimum-Norm Shallow Denoisers Look in Function Space?

Chen Zeno

Electrical and Computer Engineering

Technion

&Greg Ongie

Department Mathematical and Statistical Sciences

Marquette University

Yaniv Blumenfeld, Nir Weinberger, Daniel Soudry

Electrical and Computer Engineering

Technion

{chenzeno,yanivbl}@campus.technion.ac.il, gregory.ongie@marquette.edu

nirwein@technion.ac.il, daniel.soudry@gmail.com

###### Abstract

Neural network (NN) denoisers are an essential building block in many common tasks, ranging from image reconstruction to image generation. However, the success of these models is not well understood from a theoretical perspective. In this paper, we aim to characterize the functions realized by shallow ReLU NN denoisers -- in the common theoretical setting of interpolation (i.e., zero training loss) with a minimal representation cost (i.e., minimal \(^{2}\) norm weights). First, for univariate data, we derive a closed form for the NN denoiser function, find it is contractive toward the clean data points, and prove it generalizes better than the empirical MMSE estimator at a low noise level. Next, for multivariate data, we find the NN denoiser functions in a closed form under various geometric assumptions on the training data: data contained in a low-dimensional subspace, data contained in a union of one-sided rays, or several types of simplexes. These functions decompose into a sum of simple rank-one piecewise linear interpolations aligned with edges and/or faces connecting training samples. We empirically verify this alignment phenomenon on synthetic data and real images.

## 1 Introduction

The ability to reconstruct an image from a noisy observation has been studied extensively in the last decades, as it is useful for many practical applications (e.g., Hasinoff et al. (2010)). In recent years, Neural Network (NN) denoisers commonly replace classical expert-based approaches as they achieve substantially better results than the classical approaches (e.g., Zhang et al. (2017)). Beyond this natural usage, NN denoisers also serve as essential building blocks in a variety of common computer vision tasks, such as solving inverse problems (Zhang et al., 2021) and image generation (Song and Ermon, 2019; Ho et al., 2020). To better understand the role of NN denoisers in such complex applications, we first wish to theoretically understand the type of solutions they converge to.

In practice, when training denoisers, we sample multiple noisy samples for each clean image and minimize the Mean Squared Error (MSE) loss for recovering the clean image. Since we sample numerous noisy samples per clean sample, the number of training samples is typically larger than the number of parameters in the network. Interestingly, even in such an under-parameterized regime, the loss has multiple global minima corresponding to distinct denoiser functions which achieve zeroloss on the observed data. To characterize these functions, we study, similarly to previous works (Savarese et al., 2019; Ongie et al., 2020), the shallow NN solutions that interpolate the training data with minimal representation cost, i.e., where the \(^{2}\)-norm of the weights (without biases and skip connections) is as small as possible. This is because we converge to such _min-cost_ solutions when we minimize the loss with a vanishingly small \(^{2}\) regularization on these weights.

We first examine the univariate input case: building on existing results (Hanin, 2021), we characterize the min-cost interpolating solution and its generalization to unseen data. Next, we aim to extend this analysis to the multivariate case. However, this is challenging, since, to the best of our knowledge, there are no results that explicitly characterize these min-cost solutions for general multivariate shallow NNs -- except in two basic cases. In the first case, the input data is co-linear (Ergen and Pilanci, 2021). In the second case, the input samples are identical to their target outputs, so the trivial min-cost solution is the identity function. The NN denoisers' training regime is 'near' the second case: there, the input samples are noisy versions of the clean target outputs. Interestingly, we find that this regime leads to non-trivial min-cost solutions far from identity -- even with an infinitesimally small input noise. We analytically investigate these solutions here.

Our Contributions.We study the NN solutions in the setting of interpolation of noisy samples with min-cost, in a practically relevant "low noise regime" where the noisy samples are well clustered. In the univariate case,

* We find a closed-form solution for the minimum representation cost NN denoiser. Then, we prove this solution generalizes better than the empirical minimum MSE (eMMSE) denoiser.
* We prove this min-cost NN solution is contractive toward the clean data points, that is, applying the denoiser necessarily reduces the distance of a noisy sample to one of the clean samples.

In the multivariate case,

* We derive a closed-form solution for the min-cost NN denoiser in multivariate case under various assumptions on the geometric configuration of the clean training samples. To the best of our knowledge, this is the first set of results to explicitly characterize a min-cost interpolating NN in a non-basic multivariate setting.
* We illustrate a general alignment phenomenon of min-cost NN denoisers in the multivariate setting: the optimal NN denoiser decomposes into a sum of simple rank-one piecewise linear interpolations aligned with edges and/or faces connecting clean training samples.

## 2 Preliminaries and problem setting

The denoising problem.Let \(^{d}\) be a noisy observation of \(^{d}\), such that \(=+\) where \(\) and \(\) are statistically independent, and \([]=\). Commonly, this noise is Gaussian with covariance matrix \(^{2}\). The ultimate goal of a denoiser \(}()\) is to minimize the MSE loss over the joint probability distribution of the data and the noisy observation ("population distribution"), i.e., to minimize

\[(})=_{,}\|}()-\|^{2}\,.\] (1)

The well-known optimal solution for (1) is the minimum mean square error (MMSE) denoiser, i.e.,

\[}^{*}()=_{|}[] *{arg\,min}_{}()}_{,}\|-}()\|^{2}\,.\] (2)

Since we do not have access to the distribution of the data, and hence not to the posterior distribution, we rely on a finite amount of clean data \(\{_{n}\}_{n=1}^{N}\) in order to learn a good approximation for the MMSE estimator. One approach is to assume an empirical data distribution and derive the optimal solution of (1), i.e., the empirical minimum mean square error (eMMSE) denoiser,

\[}^{}()*{arg\,min} _{}()}_{n=1}^{N}_{ |_{n}}\|}()-_{n}\|^{2}\,.\] (3)

If the noise is Gaussian with a covariance of \(^{2}\), an explicit solution to the eMMSE is given by

\[}^{}()=^{N}_{ n}(--_{n}\|^{2}}{2^{2}})}{ _{n=1}^{N}(--_{n}\|^{2}}{2^{2 }})}\,.\] (4)An alternative approach to computing the eMMSE directly is to draw \(M\) noisy samples for each clean data point, as \(_{n,m}=_{n}+_{n,m}\), where \(_{n,m}(,^{2})\) are independent and identically distributed, and to minimize the following loss function

\[_{,M}(})=_{m =1}^{M}_{n=1}^{N}\|}(_{n,m})-_{n} \|^{2}\;.\] (5)

Denoiser model and algorithms.In practice, we approximate the optimal denoiser \(}()\) using a parametric model \(_{}()\), typically a NN. We focus on a shallow ReLU network model with a skip connection of the form

\[_{}()=_{k=1}^{K}_{k}[_{k}^{}+b_{k }]_{+}++\] (6)

where \(=((_{k})_{k=1}^{K};,)\) with \(_{k}=(b_{k},_{k},_{k})^{d} ^{d}\) and \(^{d},^{d d}\). We train the model on a finite set of clean data points \(\{_{n}\}_{n=1}^{N}\). The common practical training method is based on an online approach. First, we sample a random batch (with replacement) from the data \(\{_{n}\}_{n=1}^{N}\). Then, for each clean data point \(_{n}\), we draw a noisy sample \(_{n}=_{n}+_{n}\), where \(_{n}(,^{2}1)\) are independent of the clean data points and other noise samples. At each iteration \(t\) out of \(T\) iterations, we update the model parameters according to a stochastic gradient descent rule, with a vanishingly small regularization term \( C()\), that is,

\[_{t+1}=_{t}-_{_{t}}|}_{n}\|_{_{t}}(_{n})-_{n}\|^{2}-_{_{t}}C (_{t})\;.\] (7)

Another training method (Chen et al., 2014) is based on an offline approach. We sample \(M\) noisy sample for each clean data point and minimize (5) plus a regularization term

\[_{,M}()=_{m= 1}^{M}_{n=1}^{N}\|_{}(_{n,m})-_{n }\|^{2}+ C()\;.\] (8)

Similarly to previous works (Savarese et al., 2019; Ongie et al., 2020), we assume an \(^{2}\) penalty on the weights, but not on the biases and skip connections, i.e.,

\[C()=_{k=1}^{K}(\|_{k}\|^{2}+\|_{k}\|^{2 })\;.\] (9)

Low noise regime.In this paper, we study the solution of the NN denoiser when the clusters of noisy samples around each clean point are well-separated, a setting which we refer to as the "low noise regime". This is a rather relevant regime since denoisers are practically used when the noise level is mild. Indeed, common image-denoising benchmarks test on low (but not negligible) noise levels. For instance, in the commonly used denoising benchmark BSD68 (Roth and Black, 2009), the noise level \(=0.1\) is in the low noise regime.1 Moreover, this setting is important, for example, in diffusion-based image generation, since at the end of the reverse denoising process, new images are sampled by denoising smaller and smaller noise levels.2

## 3 Basic properties of neural network denoisers

Offline v.s. online NN solutions.NN denoisers are traditionally trained in an online fashion (7), using a finite amount of \(T\) iterations. Consequently, only a finite number of noisy samples are used for each clean data point. We empirically observe that the solutions in the offline and online settings are similar. Specifically, in the univariate case, we show in Figure 1 that denoisers based on offline and online loss functions converge to indistinguishable solutions. For the multivariate case, we observe (Figure 11 in Appendix D) that the offline and online solutions achieve approximately the same test MSE when trained on a subset of the MNIST dataset. The comparison is made using the same number of iterations for both training methods, while using much less noisy samples in the offline setting. Evidently, this lower number of samples does not significantly affect the generalization error. Hence, in the rest of the paper, we focus on offline training (i.e., minimizing the offline loss \(_{,M}\)), as it defines an explicit loss function with solutions that can be theoretically analyzed, as in (Savarese et al., 2019; Ongie et al., 2020).

The empirical MMSE denoiser.The law of large numbers implies that the denoiser minimizing the offline loss \(_{,M}\) approaches the eMMSE estimator in the limit of infinitely many noisy samples,

\[}^{}()_{}} _{M}_{,M}(} )\,.\] (10)

Therefore, it may seem that for a reasonable number of noise samples \(M\), a large enough model, and small enough regularization, the denoiser we get by minimizing the offline loss (6) will also be similar to the eMMSE estimator. However, Figure 1 shows that the eMMSE solution and the NN solutions (both online and offline) are quite different. The eMMSE denoiser has a much sharper transition and maps almost all inputs to a value of a clean data point. This is because in the case of low noise the eMMSE denoiser (4) approximates the one nearest-neighbor (\(1\)-NN) classifier, i.e.,

\[_{ 0^{+}}}^{}( )=*{arg\,min}_{\{_{i}\}_{i=1}^{N }}\|-\|\,.\] (11)

In contrast, the NN denoiser maps each noisy sample to its corresponding clean sample only in a limited "noise ball" around the clean point, and interpolates near-linearly between the "noise balls". Hence, we may expect that the smoother NN denoiser typically generalizes better than the eMMSE denoiser. We prove that this is indeed true for the univariate case in Section 4.

Why the NN denoiser does not converge to the eMMSE denoiser? Note that the limit in (10) is not practically relevant for the low-level noise regime, since we need an exponentially large \(M\) in order to converge in this limit. For example, in the case of univariate Gaussian noise, we have that \(P(||>t) 2(-}{2^{2}})\), \( t>0\). Therefore, during training, we effectively observe only noisy samples that are in a bounded interval of size \(2\) around each clean sample (see Figure 1). In other words, in the low-noise regime and for non-exponential \(M\), there is no way to distinguish if the noise is sampled from some distribution with limited support of from a Gaussian distribution. The denoiser minimizing the loss with respect to a bounded-support distribution can be radically different from the eMMSE denoiser in the regions outside the "noise balls" surrounding the clean samples, where the denoiser function is not constrained by the loss. This leads to a large difference between the NN denoiser and the MMSE estimator.

Alternatively, one may suggest that the NN denoiser does not converge to the eMMSE denoiser due to an approximation error (i.e., the shallow NN's model capacity is too small to approximate the MMSE denoiser). Nevertheless, we provide empirical evidence indicating it is not the case. Specifically, recall that in the low noise regime, the eMMSE denoiser tends to the nearest-neighbor classifier, and

Figure 1: **NN denoiser vs eMMSE denoiser.** We trained a one-hidden-layer ReLU network with a skip connection on a denoising task. The clean dataset has four points equally spaced in the interval \([-5,5]\), and the noisy samples are generated by adding zero-mean Gaussian noise with \(=1.5\). We use \(=10^{-5}\) in both setting. The Figure shows the denoiser output as a function of its input for: (1) NN denoiser trained online using (7) for \(100K\) iterations, (2) NN denoiser trained offline using (8) with \(M=9000\) and \(20K\) epochs, and (3) the eMMSE denoiser (4).

such a solution does not generalize well to test data. Thus, if the NN denoiser would have converged to the eMMSE solution, then its test error would have increased with the network size, in contrast to what we observe in Figure 12 (Appendix D).

Therefore, in order to approximate the eMMSE with a NN, it seems we must have an exponentially large \(M\). Alternatively, we may converge to the eMMSE if we use a loss function marginalized over the Gaussian noise. This idea was previously suggested by Chen et al. (2014), with the goal of effectively increasing the number of noisy samples and thus improving the training performance of denoising autoencoders. Therein, this improvement was obtained by approximating the marginalized loss function by a Taylor series expansion. However, for shallow denoisers, we may actually obtain an explicit expression for this marginalized loss, without any approximation. Specifically, if we assume for simplicity, that the network does not have a linear unit (\(=\)) and its bias terms are zero (\(=,b_{k}=0\)), then the marginalized loss for Gaussian noise, derived in Appendix A, is given by

\[(,) =_{,}\|_{}( )-\|^{2}=_{,}\|_{k=1}^{K} _{k}[_{k}^{}]_{+}-\|^{2}=_{}_{|}\|_{k=1}^{K}_{i}[_{k}^{} ]_{+}-\|^{2}\] \[=_{}\|_{k=1}^{K}_{k} (}_{k}^{},\|_{k}\|,)- \|^{2}+_{i=1}^{K}_{j=1}^{K}_{i}^{}_{j} _{ij}(_{i},_{j},^{2})\,,\] (12)

where \(_{k}=}_{k}\|_{k}\|\) and \( 0\), \(()\) are defined in Appendix A. NN denoisers trained over this loss function will thus tend to the eMMSE solution as the network size is increased. However, as we explained above, this is not necessarily desirable, so we only mention (12) to show exact marginalization is feasible.

Regularization biases toward specific neural network denoisers.To further explore the converged solution for offline training, we note that the offline loss function \(_{,M}()\) allows the network to converge to a zero-loss solution. This is in contrast to online training for which each batch leads to new realizations of noisy samples, and thus the training error is never exactly zero. Specifically, consider the low noise regime (well-separated noisy clusters). Then, the network can perfectly fit all the noisy samples using a finite number of neurons (see Section 4 for a more accurate description in the univariate case). Importantly, there are multiple ways to cluster the noisy data points with such neurons, and so there are multiple global training loss minima that the network can achieve with zero loss, each with a different generalization capability. In contrast to the standard case considered in the literature, this holds even in the under-parameterized case (where \(NM\), the total number of noisy samples, is larger than the number of parameters).

Since there are many minima that perfectly fit the training data, we converge to specific minima which also minimize the \(^{2}\) regularization we use (even though we assumed it is vanishing). Specifically, in the limit of vanishing regularization \(C()\), the minimizers of \(_{,M}()\) also minimize the representation cost.

**Definition 1**.: _Let \(_{}:^{d}^{d}\) denote a shallow ReLU network of the form (6). For any function \(:^{d}^{d}\) realizable as a shallow ReLU network, we define its **representation cost** as_

\[R()=_{:\,=_{}}C()=_{ }_{k=1}^{K}\|_{k}\|\ \ \|_{k}\|=1\  k,=_{}\,,\] (13)

_and a **minimizer** of this cost, i.e. the'min-cost' solution, as_

\[^{*}*{argmin}_{}R()\ \ s.t.\ (_{n,m})=_{n}\ \  n,m\,,\] (14)

where the second equality in (13) holds due to the \(1\)-homogeneity of the ReLU activation function, and since the bias terms are not regularized (see Savarese et al., 2019, Appendix A). In the next sections, we examine which function we obtain by minimizing the representation cost \(R()\) in various settings.

## 4 Closed form solution for the NN denoiser function -- univariate data

In this section, we prove that NN denoisers that minimize \(R(f)\) for univariate data have the specific piecewise linear form observed in Figure 1, and we discuss the properties of this form. We observe clean univariate data points \(\{x_{n}\}_{n=1}^{N}\), s.t. \(-<x_{1}<x_{2}<<x_{N}<\), and \(M\) noisy samples (drawn from some known distribution) for each clean data point, such that \(y_{n,m}=x_{n}+_{n,m}\). We denote by \(_{n}^{}\) the maximal noise seen for data point \(x_{n}\), and by \(_{n}^{}\) the minimal noise seen for data point \(x_{n}\), i.e.,

\[_{n}^{}_{m}_{n,m},_{n}^{} _{m}_{n,m}\,,\] (15)

and assume the following,

**Assumption 1**.: _Assume the data \(\{x_{n}\}_{n=1}^{N}\) is well-separated after the addition of noise, i.e.,_

\[ n[N-1]:\,x_{n}+_{n}^{}<x_{n+1}+_{n+ 1}^{}\,,\] (16)

_and \(_{n}^{}>0\,,_{n}^{}<0\)._

So we can state the following,

**Proposition 1**.: _For all datasets such that Assumption 1 holds, the unique minimizer of \(R(f)\) is_

\[f_{1D}^{*}(y)=x_{1},&y<x_{1}+_{1}^{}\\ x_{n},&x_{n}+_{n}^{} y x_{n}+_{n}^{}\\ -x_{n}}{x_{n+1}+_{n+1}^{}(-x_{n}+_{n}^{ })}(y-(x_{n}+_{n}^{}))+x_{n},&x_{n }+_{n}^{}<y<x_{n+1}+_{n+1}^{}\\ x_{N},&y>x_{N}+_{N}^{}\,.\] (17)

The proof (which is based on Theorem 1.2. in ) can be found in Appendix B.1. As can be seen from Figure 1, the empirical simulation matches Proposition 1. 3 Proposition 1 states that (17) is a closed-form solution for (8) with minimal representation cost. Notice that the _minimal_ number of neurons needed to represent \(f_{1D}^{*}\) using \(h_{}(y)\) is \(2N-2\), which is less than the number of the total training samples \(NM\) for \(M 2\).

In the case of univariate data, we can prove that the representation cost minimizer \(f_{1D}^{*}\) (linear interpolation) generalizes better than the optimal estimator over the empirical distribution (eMMSE) for low noise levels.

**Theorem 1**.: _Let \(y=x+\) where \(x p_{x}(x)\) and \((0,^{2})\) where \(x\) and \(\) are statistically independent. Then for all datasets such that Assumption 1 holds, and for all density probability distributions \(p_{x}(x)\) with bounded second moment such that \(p_{x}(x)>0\) for all \(x[_{n}x_{n},_{n}x_{n}]\), the following holds,_

\[_{ 0^{+}}(^{}(y ))>_{ 0^{+}}(f_{1D}^{*}(y ))\,.\]

See Appendix B.2 for the proof. We may deduce from Theorem 1 that for each density probability distribution \(p(x)\) there exists a critical noise level for which the the representation cost minimizer \(f_{1D}^{*}\) has strictly lower MSE than the eMMSE for all smaller noise levels (this is because the MSE is a continuous function of \(\)). The critical noise level can change significantly depending on \(p(x)\). For example, if \(p(x)\) has a high "mass" in between the training points then the critical noise level is large. However, if the density function has a low "mass" between the training points then the critical noise level is small. In Appendix D we show the MSE vs. the noise level on MNIST denoiser for NN denoiser and eMMSE denoiser (Figure 13). As can be seen there, the critical noise level in this case is not small (\( 5\)).

Intuitively, the difference between the NN denoiser and the eMMSE denoiser is how they operate on inputs that are not close to any of the clean samples (compared to the noise standard deviation). For such a point, the eMMSE denoiser does not take into account that the empirical distribution of the clean samples does not approximate well their true distribution. Thus, for small noise, it insists on "assigning" it to the closest clean sample point. By contrast, the NN denoiser generalizes better since it takes into account that, far from the clean samples, the data distribution is not well approximated by the empirical sample distribution. Thus, its operation there is near the identity function, with a small contraction toward the clean points, as we discuss next.

Minimal norm leads to contractive solutions on univariate data.Radhakrishnan et al. (2018) have empirically shown that Auto-Encoders (AE, i.e. NN denoisers without input noise), are _locally_ contractive toward the training samples. Specifically, they showed that the clean dataset can be recovered when iterating the AE output multiple times until convergence. Additionally, they showed that, as we increase the width or the depth of the NN, the network becomes more contractive toward the training examples. In addition, Radhakrishnan et al. (2018) proved that \(2\)-layer AE models are locally contractive under strong assumptions (the weights of the input layer are fixed and the number of neurons goes to infinity). Next, we prove that a univariate shallow NN denoiser is _globally_ contractive toward the clean data points without using the assumptions used by Radhakrishnan et al. (2018) (i.e., the minimizer optimizes over both layers and has a finite number of neurons).

**Definition 2**.: _We say that \(:^{d}^{d}\) is contractive toward a set of points \(\{_{n}\}_{n=1}^{N}\) on \(^{d}\) if there exists a real number \(0<1\) such that for any \(\) there exists \(i[N]\) so that_

\[\|()-(_{i})\| \|-_{i}\|\,.\] (18)

**Lemma 1**.: \(f_{1D}^{*}(y)\) _is contractive toward the clean training points \(\{_{n}\}_{n=1}^{N}\) on \(=_{n[N-1]}\{_{n }^{}-x_{n+1}^{}}{_{n}^{}-_{n+1}^{}}\}\)._

The proof can be found in Appendix B.3.

## 5 Minimal norm leads to alignment phenomenon on multivariate data

In the multivariate case, min-cost solutions of (14) are difficult to explicitly characterize. Even in the setting of fitting scalar-valued shallow ReLU networks, explicitly characterizing min-cost solutions under interpolation constraints remains an open problem, except in some basic cases (e.g., co-linear data (Ergen and Pilani, 2021)).

As an approximation to (14) that is more mathematically tractable, we assume the functions being fit are constant and equal to \(_{n}\) on a closed ball of radius \(\) centered at each \(_{n}\), i.e., \(()=_{n}\) for all \(\|-_{n}\|\), such that the balls do not overlap. Letting \(B(_{n},)\) denote the ball of radius \(\) centered at \(_{n}\), we can write this constraint more compactly as \((B(_{n},))=\{_{n}\}\). Consider minimizing the representation cost under this constraint:

\[_{}R()\;\;s.t.\;\;(B(_{n},))=\{_{n}\}\; \; n[N]\,.\] (19)

However, even with this approximation, explicitly describing minimizers of (19) for an arbitrary collection of training samples remains challenging. Instead, to gain intuition, we describe minimizers of (19) assuming the training samples belong to simple geometric structures that yield explicit solutions. Our results reveal a general alignment phenomenon, such that the weights of the representation cost minimizer align themselves with edges and/or faces connecting data points. We also show that approximate solutions of (14) obtained numerically by training a NN denoiser with weight decay match well with the solutions of (19) having exact closed-form expressions.

### Training data on a subspace

In the event that the clean training samples belong to a subspace, we show the representation cost minimizer depends only on the projection of the inputs onto the subspace containing the training data, and its output is also constrained to this subspace.

**Theorem 2**.: _Assume the training samples \(\{_{n}\}_{n=1}^{N}\) belong to a linear subspace \(^{d}\), and let \(_{}^{d d}\) denote the orthogonal projector onto \(\). Then any minimizer \(^{*}\) of (19) satisfies \(^{*}()=_{}^{*}(_{})\) for all \(^{d}\)._

The proof of this result and all others in this section is given in Appendix C.

Note the assumption that the dataset lies on a subpaces is practically relevant, since, in general, large datasets are (approximately) low rank, i.e., lie on a linear subspace (Udell and Townsend, 2019). In Appendix D we also validated that common image datasets are (approximately) low rank (Table 1).

Specializing to the case co-linear training data (i.e., training samples belonging to a one-dimensional subspace) the min-cost solution is unique and is described by the following corollary:

**Corollary 1**.: _Assume the training samples \(\{_{n}\}_{n=1}^{N}\) are co-linear, i.e., \(_{n}=c_{n}\) for some scalars \(c_{1}<c_{2}<<c_{n}\) where \(^{d}\) is a unit-vector. Then the minimizer \(^{*}\) of (19) is unique and given by \(^{*}()=(^{})\) where \(:\) has the same form as the 1-D minimizer (17) \(f_{1D}^{*}\) with \(x_{n}=c_{n}\) and \(_{n}^{}=-_{n}^{}=\)._

In other words, the min-cost solution has a particularly simple form in this case: \(^{*}()=(^{})\), where \(\) is a monotonic piecewise linear function. We call any function of this form a _rank-one piecewise linear interpolator_. Below we show that many other min-cost solutions can be expressed as superpositions of rank-one piecewise linear interpolators.

### Training data on rays

As an extension of the previous setting, we now consider training data belonging to a union of one-sided rays sharing a common origin. Assuming the rays are well-separated (in a sense made precise below) we prove that the representation cost minimizer decomposes into a sum of rank-one piecewise linear interpolators aligned with each ray.

**Theorem 3**.: _Suppose the training samples \(X\) belong to a union of \(L\) rays plus a sample at the origin: \(X=\{\}\{_{n}^{(1)}\}_{n=1}^{N_{1}}\{_{n}^{ (L)}\}_{n=1}^{N_{L}}\) where \(_{n}^{()}=c_{n}^{()}_{}\) for some unit vector \(_{}\) and constants \(0<c_{1}^{()}<c_{2}^{()}<<c_{N_{}}^{()}\). Assume that the rays make obtuse angles with each other (i.e., \(_{}^{}_{k}<0\) for all \( k\)). Then the minimizer \(^{*}\) of (19) is unique and is given by_

\[^{*}()=_{1}_{1}(_{1}^{})++_{ L}_{L}(_{L}^{})\,,\] (20)

_where \(_{}:\) has the form of the 1-D minimizer (17) \(f_{1D}^{*}\) with \(x_{n}=c_{n}^{()}\), \(_{n}^{}=-_{n}^{}=\)._

Additionally, in the Appendix C.2.1 we show that this min-cost solution is stable with respect to small perturbations of the data. In particular, if the training data is perturbed from the rays, the functional form of the min-cost solution only changes slightly, such that the inner and outer-layer weight vectors align with the line segments connecting consecutive data points.

### Special case: training data forming a simplex

Here, we study the representation cost minimizers for \(N d+1\) training points that form the vertices of a \((N-1)\)-simplex, i.e., a \((N-1)\)-dimensional simplex in \(^{d}\) (e.g., a \(2\)-simplex is a triangle, a \(3\)-simplex is a tetrahedron, etc.). As we will show, the angles between vertices of the simplex (e.g., an acute versus obtuse triangle in \(N=3\)) influences the functional form of the min-cost solution.

Our first result considers one extreme where the simplex has one vertex that makes an obtuse angle with _all_ other vertices (e.g., an obtuse triangle for \(N=3\)).

**Proposition 2**.: _Suppose the convex hull of the training points \(\{_{1},_{2},...,_{N}\}^{d}\) is a \((N-1)\)-simplex such that \(_{1}\) forms an obtuse angle with all other vertices, i.e., \((_{j}-_{1})^{}(_{i}-_{1})<0\)

Figure 2: Predicted (top row) and empirical (bottom row) min-cost NN denoisers for \(N=3\) clean training samples in \(d=2\) dimensions. The empricial NN denoisers were trained with weight decay parameter \(=10^{-5}\) and \(M=100\) noisy samples. As predicted by our theory, the ReLU boundaries align either perpendicular to the triangle edges in the obtuse case (left panel), or parallel to the triangle edges (right panel).

_for all \(i j\) with \(i,j>1\). Then the minimizer \(^{*}\) of (19) is unique, and is given by_

\[^{*}()=_{1}+_{n=2}^{N}_{n}_{n}(_{n}^{}( -_{1}))\] (21)

_where \(_{n}=_{n}-_{1}}{\|_{n}-_{1}\|}\), \(_{n}(t)=s_{n}([t-a_{n}]_{+}-[t-b_{n}]_{+})\), with \(a_{n}=\), \(b_{n}=\|_{n}-_{1}\|-\), and \(s_{n}=\|_{n}-_{1}\|/(b_{n}-a_{n})\) for all \(n=2,...,N\)._

This result is essentially a corollary of Theorem 3, since after translating \(_{1}\) to be the origin, the vertices of the simplex belong to rays making obtuse angles with each other, where there is exactly one sample per ray. Details of the proof are given in Appendix C.2.

At the opposite extreme, we consider the case where every vertex of the simplex is acute, meaning for all \(n=1,...,N\) we have \((_{i}-_{n})^{}(_{j}-_{n})>0\) for all \(i,j n\). In this case, we make following conjecture: the min-cost solution is instead a sum of \(N\) rank-one piecewise linear interpolators, each aligned orthogonal to a different \((N-2)\)-dimensional face of the simplex.

**Conjecture 1**.: _Suppose the convex hull of the training points \(\{_{1},_{2},...,_{N}\}^{d}\) is a \((N-1)\)-simplex where every vertex of the simplex is acute. Then the minimizer \(^{*}\) of (19) is unique, and is given by_

\[^{*}()=}+_{n=1}^{N}_{n}_{n}(_ {n}^{}(-_{n}))\] (22)

_where: \(_{n}\) is the projection of \(_{n}\) onto the unique \((N-2)\)-dimensional face of the simplex not containing \(_{n}\): \(}\) is the weighted geometric median of the vertices specified by_

\[}=*{arg\,min}_{^{d}}_{n=1 }^{N}_{n}-\|}{\|_{n}-_{n}\|};\]

\(_{n}=_{n}-_{n}}{\|_{n}-_{n}\|}\)_, \(_{n}=_{n}-}}{\|_{n}-\|}\); and \(_{n}(t)=s_{n}([t-a_{n}]_{+}-[t-b_{n}]_{+})\) with \(a_{n}=\), \(b_{n}=\|_{n}-_{n}\|-\), and \(s_{n}=\|_{n}-}\|/(b_{n}-a_{n})\)._

Justification for this conjecture is given in Appendix C.3.2. In particular, we prove that the interpolator \(^{*}\) given in (86) is a min-cost solution in the special case of three training points whose convex hull is an equilateral triangle. If true in general, this would imply a phase transition behavior in the min-cost solution when the simplex changes from having one obtuse vertex to all acute vertices, such that ReLU boundaries go from being aligned orthogonal to the edges connecting vertices, to being aligned parallel with the simplex faces. Figure 2 illustrates this for \(N=3\) training points forming a triangle in \(d=2\) dimensions. Moreover, Figure 2 shows that the empirical minimizer obtained using noisy samples and weight decay regularization agrees well with the form of the exact min-cost solution predicted by Proposition 2 and Conjecture 1.

In general, any given vertex of a simplex may make acute angles with some vertices and obtuse angles with others. This case is not covered by the above results. Currently, we do not have a conjectured form of the min-cost solution in this case, and we leave this as an open problem for future work.

## 6 Related works

Numerous methods have been proposed for image denoising. In last decade NN-based methods achieve state-of-the-art results (Zhang et al., 2017, 2021). See (Elad et al., 2023) for a comprehensive review of image denoising. Sonthalia and Nadakuditi (2023) empirically showed a double decent behavior in NN denoisers, and theoretically proved it in a linear model. Similar to a denoiser, an Auto-Encoder (AE) is a NN model whose output dimension equals its input dimension, and is trained to match the output to the input. For AE, the typical goal is to learn an efficient lower-dimensional representation of the samples. Radhakrishnan et al. (2018) proved that a single hidden-layer AE that interpolates the training data (i.e., achieves zero loss), projects the input onto a nonlinear span of the training data. In addition, Radhakrishnan et al. (2018) empirically demonstrated that a multi-layer ReLU AE is locally contractive toward the training samples by iterating the AE and showing that the points converge to one of the training samples. Denoising autoencoders inject noise into the input data in order to learn a good representation (Alain and Bengio, 2014). The marginalized denoising autoencoder, proposed by Chen et al. (2014), approximates the marginalized loss over the noise (which is equivalent to observing infinitely many noisy samples) by using a Taylor approximation. Chen et al. (2014) demonstrated that by using the approximate marginalized loss we can achieve a substantial speedup in training and improved representation compared to standard denoising AE.

Many recent works aim to characterize function space properties of interpolating NN with minimal representation cost (i.e., min-cost solutions). Building off of the connection between weight decay and path-norm regularization identified in Neyshabur et al. (2015, 2017); Savarese et al. (2019) showed that the representation cost of a function realizable as a univariate two-layer ReLU network coincides with the \(L^{1}\)-norm of the second derivative of the function. Extensions to the multivariate setting were studied in Ongie et al. (2020), which identified the representation cost of a multivariate function with its \(R\)-norm, a Banach space semi-norm defined in terms of the Radon transform. Related work has extended the \(R\)-norm to other activation functions Parhi and Nowak (2021), vector-valued networks Shenouda et al. (2023), and deeper architectures Parhi and Nowak (2022). A separate line of research studies min-cost solutions from a convex duality perspective Ergen and Pilanci (2021), incuding two-layer CNN denoising AEs Sahiner et al. (2021). Recent work also studies properties of min-cost solutions in the case of arbitrarily deep NNs with ReLU activation Jacot (2022), Jacot et al. (2022).

Despite these advances in understanding min-cost solutions, there are few results explicitly characterizing their functional form. One exception is Hanin (2021), which gives a complete characterization of min-cost solutions in the case of shallow univariate ReLU networks with unregularized bias. This characterization is possible because the univariate representation cost is defined in terms of the 2nd derivative, which acts locally. Therefore, global minimizers can be found by minimizing the representation cost locally over intervals between data points. An extension of these results to the case of regularized bias is studied in Boursier and Flammarion (2023). In the multivariate setting, the representation cost involves the Radon transform of the function - a highly non-local operation - that complicates the analysis. Parhi and Nowak (2021) prove a representer theorem showing that there always exists a min-cost solution realizable as a shallow ReLU network with finitely many neurons, and Ergen and Pilanci (2021) give an implicit characterization of min-cost NNs the solution to a convex optimization problem, and give explicit solutions in the case of co-linear training features. However, to the best of our knowledge, there are no results explicitly characterizing min-cost solutions in the case of non-colinear multivariate inputs, even for networks having scalar outputs.

## 7 Discussions

Conclusions.We have explored the elementary properties of NN solutions for the denoising problem, while focusing on offline training of a one hidden-layer ReLU network. When the noisy clusters of the data samples are well-separated, there are multiple networks with zero loss, even in the case of under-parameterization, while having a different representation cost. In contrast, previous theoretical works focused on the over-parametrized regime. In the univariate case, we have derived a closed-form solution to such global minima with minimum representation cost. We also showed that the univariate NN solution generalizes better than the eMMSE denoiser. In the multivariate case, we showed that the interpolating solution with minimal representation cost is aligned with the edges and/or faces connecting the clean data points in several cases.

Limitations.One limitation of our analysis in the multivariate case is that we assume the denoiser interpolates data on a full \(d\)-dimensional ball centered at each clean training sample, where \(d\) is the input dimension. In practical settings, often the number of noisy samples \(M d\). A more accurate model would be to assume that denoiser interpolates over an \((M-1)\)-dimensional disc centered at each training sample. This model may still be a tractable alternative to assuming interpolation of finitely many noisy samples. Also, our results relate to NN denoisers trained with explicit weight decay regularization, which is not always used in practice. However, recent work shows that stable minimizers of SGD must have low representation cost Mulayoff et al. (2021); Nacson et al. (2023), and so some of our analysis may provide insight for unregularized training, as well. Finally, for mathematical tractability, we focussed on the case of fully-connected ReLU networks with one hidden-layer. Extending our analysis to deeper architectures and convolutional neural networks is an important direction for future work.