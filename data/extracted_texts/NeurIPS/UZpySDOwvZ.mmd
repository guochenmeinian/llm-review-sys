# DF40: Toward Next-Generation Deepfake Detection

Zhiyuan Yan\({}^{1}\), Taiping Yao\({}^{2^{}}\), Shen Chen\({}^{2}\), Yandan Zhao\({}^{2}\), Xinghe Fu\({}^{2}\), Junwei Zhu\({}^{2}\),

**Donghao Luo\({}^{2}\), Chengjie Wang\({}^{2}\), Shouhong Ding\({}^{2}\), Yunsheng Wu\({}^{2}\), Li Yuan\({}^{1}\)\({}^{}\)**

\({}^{}\) School of Electronic and Computer Engineering, Peking University

\({}^{2}\)Tencent Youtu Lab

{zhiyuanyan@stu.,yuanli-ece@pku.edu.cn

###### Abstract

We propose a new comprehensive benchmark to revolutionize the current deepfake detection field to the next generation. Predominantly, existing works identify top-notch detection algorithms and models by adhering to the common practice: training detectors on one specific dataset (\(e.g.\), FF++ ) and testing them on other prevalent deepfake datasets. This protocol is often regarded as a "golden compass" for navigating SoTA detectors. But can these stand-out "winners" be truly applied to tackle the myriad of realistic and diverse deepfakes lurking in the real world? If not, what underlying factors contribute to this gap? In this work, we found the **dataset** (both train and test) can be the "primary culprit" due to the following: (1) _forgery diversity_: Deepfake techniques are commonly referred to as both face forgery (face-swapping and face-reenactment) and entire face synthesis (especially face). Most existing datasets only contain partial types of them, with limited forgery methods implemented (\(e.g.\), 2 swapping and 2 reenactment methods in FF++); (2) _forgery realism_: The dominated training dataset, FF++, contains out-of-date forgery techniques from the past four years. "Honing skills" on these forgeries makes it difficult to guarantee effective detection generalization toward nowadays' SoTA deepfakes; (3) _evaluation protocol_: Most detection works perform evaluations on one type, \(e.g.\), training and testing on face-swapping types only, which hinders the development of universal deepfake detectors.

To address this dilemma, we construct a highly diverse and large-scale deepfake detection dataset called **DF40**, which comprises **40** distinct deepfake techniques (10 times larger than FF++). We then conduct comprehensive evaluations using **4** standard evaluation protocols and **8** representative detection methods, resulting in over **2,000** evaluations. Through these evaluations, we provide an extensive analysis from various perspectives, leading to **7** new insightful findings contributing to the field. We also open up **4** valuable yet previously underexplored research questions to inspire future works. We release our dataset, code, and checkpoints at https://github.com/YZY-stack/DF40.

+
Footnote †: \({}^{}\) Corresponding Author

## 1 Introduction

We are currently in 2024, an explosive era of Artificial Intelligence Generated Content (AIGC), a world where you can effortlessly make anyone say anything at any time, a realm where reality and fiction blend seamlessly, creating a mesmerizing tapestry of digital artistry and potential deception.

Amidst this AIGC revolution, the ease of generating _deepfakes_1 has become a "double-edged sword." Unfortunately, deepfake is often misused for manipulating one person's identity (face-swapping) or controlling facial expressions and movements in a portrait (face-reenactment). It can be particularly harmful as it may lead to severe digital crimes and undermine social trust. Therefore, there is an urgent need to develop a reliable system for detecting deepfakes.

Unfortunately, most existing detection methods can only handle a subset of deepfake types [65; 87]. To illustrate, in face-swapping deepfake detection, blending-based detectors [65; 43] have achieved SoTA results on existing face-swapping datasets (\(e.g.\), CDF ). But all these methods heavily rely on the assumption that existing face-swapping should share a blending step2. However, recent advancements in face-swapping (\(e.g.\), Simswap , FaceDancer ) directly generate all content (including the background) without blending. So "looking for" the specific blending artifacts for detecting the "non-blending" forgeries such as Simswap could be unrealistic. Regrettably, this crucial observation may not be well-known in the field, as most existing face-swapping deepfake datasets [62; 17; 42] involve blending in their fake data. Similarly, many existing face-reenactment generators (\(e.g.\), LIA ) and all entire-image-synthesis generators (\(e.g.\), DDPM ) are also without any blending. Hence, developing detectors on these blend-only datasets might limit the model's potential ability to detect a broad range of deepfake types. In a real-world scenario with unpredictability and complexity, creating such a "unified" detector is essential. However, this issue has persistently been challenging, as most existing datasets either contain limited types or are specific to forgery types (see Tab. 1).

Therefore, we realize that a _diverse_ and _comprehensive_ dataset is the true key to "unlock" the potential of ideal deepfake detection. To this end, we "jump" from the previous protocol settings (\(e.g.\), train and test solely on face-swapping fakes) and propose a new comprehensive dataset called **DF40**. Our main contribution can be generally summarized as two folds: **(1)** Our DF40 implements **40** different deepfake techniques, including face-swapping, face-reenactment, entire face synthesis, and face editing. We even include the just-released DiT , PixArt-\(\), and highly popular software DeepFaceLab  and HeyGen  to simulate real-world deepfakes; **(2)** We introduce 4 standard protocols for evaluations and show 8 insightful findings to the field. We also open up new research questions and topics to inspire future research toward next-generation deepfake detection.

Figure 1: Overview of our DF40 dataset. DF40 shows advantages in data diversity, synthesis quality, and deepfake realism. Note _all_ the above figures are deepfake, which does not exist in the real world.

## 2 Background

### Deepfake Generation.

Based on previous survey , deepfake techniques can be typically classified into four types: face-swapping (_FS_), face-reenactment (_FR_), entire face synthesis (_EFS_), and face editing (_FE_). **(1) Face-swapping:** This paper classifies the face-swapping technique into two domains: _DF-family_ and _FS-family_. _(i) DF-family_ involves creating a mask around the facial region (some even include the neck ) and blending the generated deepfake face back into the background image using that mask. Most existing and famous face forgery datasets, such as FF-DF , CDF  and DFDC , belong to this line. _(ii) FS-family_ methods represent another significant category in face-swapping deepfake generation. These methods typically involve the use of an identity-background encoder. It disentangles a face image's identity and background information during encoding. Notably, these methods directly generate all content, even the background. Many recent face-swapping research works [85; 9] are within this line. **(2) Face-reenactment:** Generally, this technique can be used to modify source faces, imitating the actions or expressions of another face. Differing from face-swapping, face-reenactment techniques are _rarely_ considered in existing datasets. Two commonly used reenactment-based forgeries are Face2Face  and NeuralTextures . These two forgeries are implemented in the FF++ dataset. Due to the amazingly rapid development of the AIGC technologies (_e.g._, Digital Human), these relatively old-fashioned methods cannot represent the modern' SoTA reenactment methods. Our DF40 implements 13 face-reenactment methods in total, including the classical animation [67; 68], SoTA's audio-based driven methods [92; 55], image-based driven methods [82; 26; 5]. We also include the well-known best face generation technique, HeyGen , in our dataset for evaluation. **(3) Entire Face Synthesis:** This technique can be generally treated as "Face AIGC." With the rapid development of AIGCs, this technique has achieved remarkably notable improvement. The two widely used technologies to generate synthesis faces are GAN (_e.g._, VQGAN ) and Diffusion models (_e.g._, StableDiffusion ). **(4) Face Editing:** This technique aims to modify the facial attributes (_e.g._, age and gender) of the given face images. Most of these works utilize the latent code of StyleGAN  to perform editing during GAN inversion.

### Existing Deepfake Datasets.

Most earlier public deepfake datasets published before 2021 [37; 91; 17; 42; 62; 13] generate deepfakes using only the face-swapping technique that involves a blending process. Also, these datasets contain only single or no more than 4 specific manipulation approaches. Notably, DFDC  implements 7 deepfake approaches and extends the data scale to the million-level. After that, ForgeryNet  applies 15 methods to create forgery data with a million-level data scale. However, with the advent of AIGCs such as diffusion-based generators, producing realistic content has become increasingly popular and widely seen on social media. Consequently, whether a detector trained on older forgery methods can generalize to today's SoTA deepfakes is uncertain. In response, many recent works [32; 69] have begun to focus on current generative models. A very recent

[MISSING_PAGE_FAIL:4]

[MISSING_PAGE_FAIL:5]

### Evaluations, Findings, and Analysis

In this work, we conduct evaluations under four standard protocols: Cross-forgery evaluation (**Protocol-1**), Cross-domain evaluation (**Protocol-2**), Toward unknown forgery or domain evaluation (**Protocol-3**), and One-Verse-All (OvA) evaluation (**Protocol-4**). From these evaluations, we list 8 critical findings via empirical observations or in-depth analysis, as follows.

**Finding-1: Asymmetric performance drop among different forgery types (fake regions matter).** From Tab. 3, we observe that the performance drop between FS and FR is moderate but more significant between these two and EFS. Specifically, model training on FS yields higher results (around 0.8) on EFS, while training on EFS only achieves about 0.6 on FS. To understand the underlying reason for this finding, we provide the following detailed explanations: _One possible explanation is that FS can exhibit both localized and global forgeries, while EFS is restricted to global forgeries, making EFS less diverse in scope compared to FS._ As a result, training a model on EFS might not be sufficient to capture the localized fake artifacts present in FS. For example, some localized FS (such as DeepFakes of FF++ ) only contain fake artifacts within the facial region, but a model trained on EFS might be biased to "expect" artifacts across the entire image, including the background, resulting in a lower result on FS. To verify our claim, we consider using t-SNE for visualizing the latent distribution of'real', 'whole-fake', and 'face-fake'. The t-SNE (Fig. 4) results show that a well-trained EFS detection model can easily distinguish the whole-fake from real images. However, the localized version, including face-fake and mouth-fake, appears much more similar to real images, making it more challenging to detect compared to the whole-fake. This experiment qualitatively verifies the significance of the fake region in detection.

Figure 3: One-Verse-All (OvA) evaluation (**Protocol-4**): Training the baseline (\(i.e.\), Xception) on one fake and testing it on other remaining fakes. We show the cross-forgery evaluations on both the FF++ domain and CDF domain. We also show the performance “drop” from the FF++ to the CDF. **Blue** donates all FS methods, **Green** for FR, and Yellow for EFS. In each heatmap, more “red” indicates higher values; “White” means 0.5 AUC (by chance), and “blue” indicates values below 0.5.

    &  &  \\   & & DeepFakes\(}}}}}}}}}}}}}}\) & **O** & **M** & **M** & **M** & **M** & **M** & **M** & **M** & **M** \\   &  Accounts \\  } & 12.062 & 0.594 & 0.344 & 0.348 & 0.537 & 0.646 & 0.458 & 0.530 & 0.564 & 0.546 \\  & & **C** & **C** & **C** & **C** & **C** & **C** & **C** & **C** & **C** \\   & **C** & **C** & **C** & **C** & **C** & **C** & **C** & **C** & **C** & **C** \\   & **C** & **C** & **C** & **C** & **C** & **C** & **C** & **C** & **C** & **C** & **C** \\   & **Protocol-1** & 0.715 & 0.717 & 0.729 & 0.702 & 0.702 & 0.702 & 0.706 & 0.705 & 0.708 & 0.708 & 0.708 \\  & **C** & **C** & **C** & **C** & **C** & **C** & **C** & **C** & **C** & **C** \\   & **C** & **C** & **C** & **C** & **C** & **C** & **C** & **C** & **C** & **C** & **C** \\   & **C** & **C** & **C** & **C** & **C** & **C** & **C** & **C** & **C** & **C** & **C** & **C** \\   & **Protocol-2** & 0.717 & 0.715 & 0.729 & 0.702 & 0.702 & 0.702 & 0.706 & 0.705 & 0.708 & 0.708 & 0.708 \\  & **C** & **C** & **C** & **C** & **C** & **C** & **C** & **C** & **C** & **C** & **C** \\   & **C** & **C** & **C** & **C** & **C** & **C** & **C** & **C** & **C** & **C** & **C** & **C** \\   & **C** & **C** & **C** & **C** & **C** & **C** & **C** & **C** & **C** & **C** & **C** & **C** \\   & **Protocol-3** & 0.717 & 0.715 & 0.729 & 0.702 & 0.702 & 0.702 & 0.706 & 0.708 & 0.708 & 0.708 & 0.708 \\  & **C** & **C** & **C** & **C** & **C** & **C** & **C** & **C** & **C** & **C** & **C** \\   & **C** & **C** & **C** & **C** & **C** & **C** & **C** & **C** & **C** & **C** & **C** & **C** \\   & **C** & **C** & **C** & **C** & **C** & **C** & **C** & **C** & **C** & **C** & **C** & **C** \\   & **Protocol-4** & 0.719 & 0.709 & 0.588 & 0.511 & 0.735 & 0.407 & 0.423 & 0.009 & 0.201 & 0.009 & 0.300 \\  & **C** & **C** & **C** & **C** & **C** & **C** & **C** & **C** & **C** & **C** & **C** \\   & **C** & **C** & **C** & **C** & **C** & **C** & **C** & **C** & **C** & **C** & **C** \\   & **Protocol-4** & 0.707 & 0.525 & 0.712 & 0.772 & 0.777 & 0.677 & 0.904 & 0.611 & 0.697 & 0.679 \\  & **C** & **C** & **C** & **C** & **C** & **C** & **C** & **C** & **C** & **C** & **C** \\    & **C** & **C** & **C** & **C** & **C** & **C** & **C** & **C** & **C** & **C** & **C** \\    & **C** & **C** & **C** & **C** & **C** & **C** & **C** & **C** & **C** & **C** & **C** & **C** \\    & **C** & **C** & **C** & **C** & **C** & **C** & **C** & **C** & **C** & **C** & **C** & **C** \\   & **Protocol-5** & 0.715 & 0.715 & 0.708 & 0.704 & 0.406 & 0.703 & 0.002 & 0.800 & 0.970 & 0.641 \\  & **C** & **C** & **C** & **C** & **C** & **C** & **C** & **C** & **C** & **C** & **C** \\    & **C** & **C** & **C** & **C** & **C** & **C** & **C** & **C** & **C** & **C** & **C** & **C** \\    & **C** & **C** & **C** & **C** & **C** & **C** & **C** & **C** & **C** & **C** & **C** & **C** \\   & **Protocol-6** & 0.851 & 0.806 & 0.442 & 0.751 & 0.769 & 0.724 & 0.964 & 0.843 & 0.799 & 0.377 \\ 

**Finding-2: Existing well-designed (SoTA) detectors may not have obvious advantages over the baseline.** In our experiments, SRM, SPSL, RECCE, and RFM, all state-of-the-art deepfake detectors using Xception as their backbone, achieve results similar to the simple baseline Xception. This implies that these SoTA models might only learn sub-optimal forgery features and lack clear advantages over the baseline, although these SoTAs show higher results on previous datasets, \(e.g.\), DFDC. In other words, although these well-designed SoTA detectors achieve higher scores than the Xception (baseline) when testing on CDF , they cannot maintain a consistent advantage over the baseline when testing on other forgeries. This highlights the remaining generalization issue.

**Finding-3: CLIP excels in deepfake detection than other baselines (such as Xception).** Outperforming other SoTA detectors and the Xception baseline across all scenarios, CLIP (both base and large versions) demonstrates the power of pre-training in deepfake detection. This highlights the benefits of pre-training before fine-tuning on the deepfake detection task. However, why can CLIP outperform the Xception by a notable margin? We analyze this point in the following content from the view of real face distribution. We analyze the learned features of the three models (CLIP-base, CLIP-large, Xception), as visualized in Fig. 7. **(1)** We see that both versions of CLIP learn more informative features that can gather some real samples into several different "groups" (see HeyGen and MidJourney), while the real features of Xception mix all of them together. Leveraging large-scale pre-training, CLIP can capture more informative facial features (\(e.g.\), ID) about real faces and thus "know" that these features are _unrelated_ to deepfakes, while Xception could be overfitted to ID, as evidenced by . **(2)** Between the large and base versions, CLIP-large's real samples are closer to each other (see WhichisReal), suggesting it learns more comprehensive features of real faces. We also see that CLIP-large can _implicitly_ "separate and align" different forgeries and data domains ("orthogonal" in WhichisReal), with no need for explicit constraints as .

**Finding-4: Forgery methods and data domains together contribute to discriminative forgery artifacts.** A significant performance drop is observed when both forgeries and domains change. For example, Xception training on FS (FF) achieves only 0.657 and 0.642 on FR (CDF) and EFS (CDF), respectively. These results are significantly lower (nearly a 20% drop) compared to results on FS (CDF) and FR (FF), which involve crossing only one of the domains or forgeries. This suggests that both factors together contribute to discriminative forgery artifacts for distinguishing real and fake. To intuitively convey our conclusions, we draw a **causal graph** to illustrate the causal relationship of how domain (D) and forgery method (F) influence the model's generalization (R). The causal graph is presented in Fig. 5. Mathematically, the model's generalization result \(R\) can be expressed as a function of these intermediate variables: \(R=f(X_{1},X_{2},X_{3})\), where \(X_{1}=g(D)\) represents the domain-specific influence, \(X_{3}=h(F)\) represents the forgery method-specific influence, and \(X_{2}=k(D,F)\) represents the combined influence of both domain and forgery method. Each of these variables \(X_{1}\), \(X_{2}\), and \(X_{3}\) captures different aspects of how D and F interact to impact the overall performance \(R\). The functions \(g\), \(h\), and \(k\) are mappings that quantify the respective influences: \(X_{1}=g(D)\), \(X_{3}=h(F)\), and \(X_{2}=k(D,F)\). Finally, the result \(R\) is determined by integrating these influences: \(R=f(g(D),k(D,F),h(F))\). Fig. 6(a) supports this point, showing increased overlap between real and fake logits as domains and methods change incrementally. Also, crossing domains can result in "wrong confidence" for real predictions and vice versa.

**Finding-5: FR forgeries may share transferable patterns for detecting other FR instances.** Most FR forgery methods demonstrate high AUC within the heatmap, even across data domains. However, Wav2Lip, an audio-driven reenactment forgery that only modifies the mouth region, is an exception.

Figure 4: t-SNE visualization for real, whole-fake, face-fake, and mouth-fake images. The results show that a well-trained EFS detector (Xception) can effectively distinguish between whole-fake and real images, but struggles to identify fakes with only face or mouth manipulation. This observation highlights the significant influence of the manipulated region on detection performance.

Figure 5: Causal graph.

Its artifacts are more localized compared to other FRs that generate all content, making it different. Note that **Finding-5** does not mean that collecting (many) FR methods is without value. Specifically, even though these FR methods may share some transferable patterns to detect other FR methods, the fake similarity between FR and other types (like EFS) can also vary greatly depending on the specific FR method used (see Fig. 3).

**Finding-6: SBI could be viewed as an "anomaly detection" model.** SBI  (a blending-based detection) could be considered an "anomaly detection" model, which might utilize the real features for classifying fakes. Results in Fig. 6(a) show that when forgery samples appear significantly different from real samples, SBI classifies them as anomalies (forgery samples), likely because the pseudo-fake samples generated by SBI closely resemble their original real counterparts. In other words, SBI creates more realistic fake samples that are more similar to the real, encouraging the detection model to learn a more robust real representation than the baseline.

**Finding-7: CLIP-large shows the potential to generalize to some non-face deepfakes when trained only on face data.** In additional to evaluate on the face deepfakes, we also evaluate non-face-domain deepfakes (natural image synthesis) using the widely-used GenImag dataset to determine if models trained on face-domain data can transfer to non-face-domain detection. In Tab. 19, surprisingly, CLIP-large achieves a 0.746 AUC on previously unseen non-face deepfakes, while Xception only reaches a 0.535 AUC, near chance levels. This highlights that CLIP-large might learn some transferable (EFS) forgery features that are not related to the face content.

Figure 6: Logits and confidence analysis for Xception and SBI models.

Figure 7: t-SNE Visualizations for three models: Xception, CLIP-base, and CLIP-large.

### Discussion & Further Analysis

Why A More Diverse Deepfake Dataset Is Crucial for Current Deepfake Detector?In our work, we propose the DF40 dataset, with highly diverse deepfake approaches. Here, we aim to qualitatively verify why such a highly diverse dataset is extremely crucial for current deepfake detectors. The results in Fig. 8 show that CFDA  (latest SoTA) achieves significant advantages over other methods on existing datasets, but it performs comparably or even worse than other detectors. This emphasizes the necessity of constructing a dataset that encompasses a greater variety of deepfake methods and types. Without such a highly diverse dataset for evaluation, we would only be seeing "the tip of the iceberg." In other words, the current SoTA may not be the definitive "winner."

Are Super-Resolution Images Fake?Here, we explore whether a super-resolution (SR) image will be considered fake or real by the detection model. This question is important for two reasons: **(1)** many deepfake software programs involve an SR operation to enhance the resolution of generated faces (_e.g.,_ FaceFusion3), and **(2)** Most existing SR methods  are based on deep generative models such as GAN. Therefore, the output of SR methods can also contain generative artifacts that might be similar to those in deepfake images. We use GFPGAN to perform SR on the self-reconstruction images (SRI), where we use SimSwap  to perform face-swap to the same ID. Results in Tab. 8 show that SR has a significant impact on fake images, particularly for EFS and FE methods. As demonstrated in the table, models trained on FS and EFS exhibit more than a 20% point improvement for the SRI after applying super-resolution. This suggests that SR operations may introduce noticeable generative artifacts that can be detected by models trained on EFS and FE.

Exploration of Frequency Artifacts in Different Types of Deepfakes.Here, we explore whether there exist common fake patterns in the frequency domain. following , we compute the average frequency spectra of high-pass filtered images. Results in Fig. 9 show that _although the deepfake types differ, they can show similar patterns/artifacts in the average spectra._ For instance, methods from FS (_i.e.,_ SimSwap , BlendFace , InSwap) show similar "checkboard" patterns that can also be observed in EFS (_i.e.,_ SD1.5 , VQGAN ) and FE (_i.e.,_ e4e ). This observation highlights the need to further explore whether and why forgeries with different types can exhibit similar patterns in frequency.

   Training Set & Model & FS (CCF) & MF(CCF) & EFS (CCF) & DisFusion/FaceLab & KeyGAN & DisFusion/FaceF & DisFusion/FaceF & DisFusion/FaceF & DisFusion/FaceF & DisFusion/FaceF & DisFusion/FaceF & DisFusion/FaceF & DisFusion/FaceF & DisFusion/FaceF & DisFusion/FaceF & DisFusion/FaceF \\   & CF40 (FF40) & 0.531 & 0.601 & 0.601 & 0.602 & 0.704 & 0.609 & 0.612 & 0.701 & 0.609 & 0.605 & 0.605 & 0.605 & 0.605 & 0.605 & 0.605 & 0.605 & 0.605 \\  & CF40 (FF40) & 0.935 & 0.935 & 0.931 & 0.907 & 0.617 & 0.540 & 0.604 & 0.612 & 0.601 & 0.609 & 0.603 & 0.605 & 0.601 & 0.604 \\  & CLF-large & 0.922 & 0.906 & 0.905 & 0.908 & 0.784 & 0.786 & 0.540 & 0.974 & 0.896 & 0.929 & 0.977 & 0.907 & 0.908 \\   

Table 6: Comparison of different models/baselines used for pre-training. FS (CDF) denotes all FS data within the CDF domain. DF40 (FF) is the combination of FS (FF), FR (FF), and EFS (FF).

Figure 8: Evaluation of existing detectors on previous datasets and our DF40.

A Model Bias Toward the Resolution Gap Between Real and Fake.In our previous experiments (see Tab. 5), we observe anti-intuitive phenomena: models training on FR achieve very low AUC (much lower than 0.5) on StyleCLIP. After investigating the underlying reason, we found that _the unaligned resolution between the real and fake classes introduces a model bias, that is, higher resolution, higher probability to be real_. Specifically, we observe that the training real data (FF++) obviously contains more high-frequency components (higher resolution) compared to the training fake data (FR). This noticeable resolution gap between real and fake data could be inevitably captured by the model , leading the model to potentially adopt a trivial solution: "Low resolution implies fake." In contrast, during testing, the fake data (StyleCLIP) contains significantly more high-frequency components (higher resolution) than the testing real data used for testing. We provide a frequency visualization as evidence to validate this claim (see our Appendix). This bias leads the model to make an inconsistent decision, resulting in the anomaly in AUC.

### Open Questions and Potential Topics for Future Research

(1) **Regarding Blending:** Many existing detectors have utilized blending data to enhance the model's generalization. However, we have found that even the latest blending detector  is still limited in generalizing to all types of deepfakes (see Fig. 8). This indicates that blending might not be "all you need." But the question remains: _What is the role of blending data in training deepfake detectors? And what kinds of deepfakes can be addressed by blending?_ (**Question-1**).

(2) **Regrading Forgery Diversity:** As the diversity of deepfakes increasingly goes up, **Question-2** raise: _how to design a new framework to learn (many) different forgeries effectively and jointly, without overfitting to limited specific fakes?_

(3) **Regrading Forgery Type:** Previous research [89; 88] classified deepfake types at the instance level. For example, if four deepfakes are applied for training, they will be regarded as four _distinct_ deepfakes. However, there are so many forgery techniques available that it is hard to elaborate on them all. So, **Question-3:**_can we classify deepfakes based on FS, FR, EFS, and FE?_

(4) **Domain-Invariant Detector:** As shown in our previous discussion, we have found that many factors (such as _resolution_ and _fake region_) can impede the model from learning domain-invariant features. Thus, **Question-4:**_How can we develop an invariant detector?_

## 5 Conclusions, Board Impacts, and Limitations

**(1) Conclusions:** We have developed _DF40_, a highly diverse and groundbreaking benchmark, comprising 40 distinct deepfake techniques to support the detection. Leveraging DF40, we conducted over 2,000+ evaluations using 8 representative detectors under 4 standard evaluation protocols, creating 7 new findings and 4 open questions for future works. We hope the proposed DF40 could revolutionize the whole field for the next generation. **(2) Board Impacts:** DF40 offers high-quality and realistic deepfake techniques, facilitating the detection of today's real-world deepfakes. Also, our benchmark assists in safeguarding societal trust and promoting the responsible use of such technology. **(3) Limitations:** One limitation is the lack of comprehensive analysis for _video-level detectors_. Actually, we provide evaluations using video models, \(e.g.\), I3D  in the Appendix. However, we have not delved deeply into discussing and analyzing certain issues, \(e.g.\), assessing and visualizing whether video models can effectively capture both temporal and spatial artifacts. In future work, we plan to broaden our benchmark's scope and address these concerns in detail.

   &  &  &  &  \\   & FSGAN & BlendFace & LIA & Wav2Lip & DiT & DDIM & e4e \\  SRI & 0.772 & 0.835 & 0.746 & 0.564 & 0.687 & 0.713 & 0.543 \\ SRI + Super-Resolution & 0.983 & 0.825 & 0.988 & 0.833 & 0.997 & 0.946 & 0.978 \\  

Table 8: Ablation study regarding the impact of **super-resolution** to the fake images. We use self-reconstruction images (SRI) as the fake and apply GFPGAN  to perform the super-resolution.

Figure 9: Frequency analysis on selected fake data within the proposed DF40.