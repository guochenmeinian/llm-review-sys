# \(k\)-Means Clustering with Distance-Based Privacy

Alessandro Epasto

Google Research

aepasto@google.com

&Vahab Mirrokni

Google Research

mirrokni@google.com

&Shyam Narayanan

MIT

shyamsn@mit.edu

&Peilin Zhong

Google Research

peilinz@google.com

###### Abstract

In this paper, we initiate the study of Euclidean clustering with Distance-based privacy. Distance-based privacy is motivated by the fact that it is often only needed to protect the privacy of exact, rather than approximate, locations. We provide constant-approximate algorithms for \(k\)-means and \(k\)-median clustering, with additive error depending only on the attacker's precision bound \(\), rather than the radius \(\) of the space. In addition, we empirically demonstrate that our algorithm performs significantly better than previous differentially private clustering algorithms, as well as naive distance-based private clustering baselines.

## 1 Introduction

Two of the most fundamental and widely studied problems in unsupervised machine learning are the \(k\)-means and \(k\)-median clustering problems. Solving these clustering problems can allow us to group together data efficiently, and hence extract valuable and concise information from massive datasets. The goal of the \(k\)-means (resp., \(k\)-median) clustering problem is: given a dataset \(X\) of points, construct a set \(C\) of \(k\) centers to minimize the clustering cost \(_{x X}d(x,C)^{2}\) (resp., \(_{x X}d(x,C)\)), where \(d(x,C)\) represents the minimum distance between the data point \(x\) and the closest center in \(C\).

In general, machine learning and data mining algorithms are prone to leaking sensitive information about individuals who contribute data points. In certain scenarios, this can lead to severe consequences, including losses of billions of dollars  or even the loss of human lives . Thus, providing accurate algorithms that protect data privacy has become crucial in algorithm design. Over the past decade, the notion of differential privacy (DP)  has emerged as the gold standard for privacy-preserving algorithms, both in theory and in practice, and has been implemented by several major companies and the US Census . Informally, DP requires the output distribution of the algorithm to remain almost identical whenever a single data point is altered. (See Section 2 for a formal definition.) Hence, even the knowledge of all but one data point, along with the output of the algorithm, still cannot reveal significant information about the final data point.

The importance of \(k\)-means and \(k\)-median clustering, as well as preserving data privacy, has led to a large interest in designing differentially private clustering algorithms in Euclidean space . Here, the goal is to design a differentially private set of \(k\) centers, such that the clustering cost with respect to these centers is only a small factor larger than the optimal (non-private) clustering cost. Importantly, the work of  led to efficient polynomial-time and differentially private algorithms that achieve constant multiplicative approximation ratios.

While we can obtain DP algorithms with low multiplicative error, all such algorithms also require an additional additive error. If \(\) is the radius of a ball that is promised to contain all data points, even thebest private clustering algorithms are known to have an additive error proportional to \((k,d)^{p}\), where \(p=2\) for \(k\)-means and \(p=1\) for \(k\)-median. This factor of \(^{p}\) is in fact unavoidable , as a single individual datapoint can be moved up to distance \(\) and the algorithm must preserve privacy with respect to this change. If we do not have a good bound of \(\), this factor may dominate the error, and may make the clustering algorithm highly inaccurate. Even if the bound is known exactly, errors scaling with \(\) may however be unnecessary and unacceptable in certain situations.

The additive error depending on \(\) is necessary because standard differential privacy requires us to protect learning _anything_ about the location of any point. However, in practice this may not be necessary as it might be enough to not know the location of a point up to a certain error. For instance, in address data, the risk is leaking the actual location, but uncertainty within a few miles in a city is sufficient to protect the privacy of the person . Another motivation is in smart meters [20, Section 6.1], where accurately learning the fine-grained consumption can result in spectacular privacy leaks (e.g. learning which TV channel is being watched ) but slight uncertainty on the measurements is sufficient to protect from such attacks. Moreover, when differential privacy is used to protect the algorithm from adversarial inputs, it is often sufficient to protect against small perturbations as large perturbations can be detected or removed otherwise .

These cases can be modeled by variants of differential privacy, such as dX privacy (a.k.a. extended differential privacy) , and pixelDP . All such models are adaptations or generalizations of DP which take into account a metric over the datasets.

In this paper, we study a concrete formulation of distance-based privacy which we call \(\)-dist-DP. Roughly speaking, an algorithm is \(\)-dist-DP if the algorithm protects privacy of a single data point if it is moved by at most \(\) in a metric space. (See Section 2 for a formal definition, where we define \((,,)\)-dist-DP.) This is a less restrictive version of DP, as usually the neighboring datasets are defined to be any two datasets with a single point allowed to move anywhere. While we remark that although this notion is well-defined for any metric space, our results in this paper focus entirely on Euclidean space.

The main question we study in this paper is the following: can we obtain much better approximation results (and algorithms better in practice) if we allow the algorithm to resist small movements, as opposed to arbitrary movements, of a point for instance for clustering? In other words, can we design \(\)-dist-DP algorithms that perform significantly better than the state of the art regular DP algorithms for \(k\)-means or \(k\)-median clustering?

### Our Results

In this work, we answer the above question affirmatively, by providing an efficient and accurate theoretical algorithm, and showing empirically that our algorithm outperforms clustering algorithms with standard differential privacy.

#### 1.1.1 Theoretical Results

From a theoretical perspective, we are able to obtain \(O(1)\)-approximate algorithms for \(k\)-means and \(k\)-median clustering with \(\)-dist-DP, and with additive error essentially only depending on the smaller distance \(\) as opposed to the full radius \(\). More precisely, our main theorem is the following.

**Theorem 1.1**.: _Let \(n,k,d\) be integers, \((0,],,(0,1]\) be privacy parameters, and \(p\{1,2\}\). Then, given a dataset \(X=\{x_{1},,x_{n}\}\) of points in a given ball of radius \(\) in Euclidean space \(^{d}\), there exists a polynomial-time \((,,)\)-dist-DP algorithm \(\) that outputs a set of centers \(C=\{c_{1},,c_{k}\},\) such that_

\[_{i=1}^{n}d(x_{i},C)^{p} O(1)_{C^{*} ^{d}\\ |C^{*}|=k}_{i=1}^{n}d(x_{i},C^{*})^{p}+(k, d, n,,, )^{p}.\]

_Here, \(p=1\) for \(k\)-median and \(p=2\) for \(k\)-means._

For more precise dependences on the parameters \(k,d,1/\), please see Theorem C.1.

Qualitatively, Theorem 1.1 has similar guarantees to , who also provided an \((,)\)-differentially private algorithm with an \(O(1)\)-approximation algorithm and additive error that was \((k,d, n,,)^{p}\). The main difference is that we drastically reduce the additive error by reducing the dependence on \(\) to a dependence on the distance privacy parameter \(\).

Running time and parallel computation.The runtime of a straightforward implementation of our algorithm is \((nkd)+(k) d\),1 if we also ignore polynomial factors in \(\). By using approximate near neighbor algorithms, we can improve this further to \((nd)+(k) d\), which for \(k\) at most a small polynomial in \(n\), is nearly linear. In addition, the algorithm can be easily implemented in the massively parallel computation (MPC) model [51; 11] (an abstraction of MapReduce ) using \(O(1)\) rounds and near linear total space where each machine has sublinear space. We discuss above in-memory algorithms and MPC algorithms further at the end of Appendix C.

Finally we remark that the \(^{p}\) dependence in the additive error is required for ensuring \(\)-dist-DP. In fact, we prove in Appendix D that any \((,,)\)-dist-DP algorithm, with any finite multiplicative error, must incur \((k^{2})\)-additive error for \(k\)-means and \((k)\)-additive error for \(k\)-median.

#### 1.1.2 Empirical Results

We empirically studied the performance of our algorithm on public and real-world datasets. We compare the approximation guarantee of our algorithm with the standard DP clustering algorithm and the standard non-private \(k\)-clustering algorithm. Experiments show that our algorithm outperforms the DP clustering algorithm and is only slightly worse than the non-private algorithm. In addition, we show that smaller \(\) provides a better approximation guarantee, which aligns with our theoretical study. We refer readers for more details of our empirical study to Section 6.

### Other Related Work

Distance-based Privacy:The literature on distance-based privacy explored different data protection schemes which we now describe in more detail. A general notion is known as dX privacy  (a.k.a. Extended differential privacy) which includes as a special case differential privacy. This privacy notion bounds the distinguishability of two statistical datasets, not just by the number of different users' inputs (i.e., their Hamming distance) but by an arbitrary \(d_{}\) distance between them accounting for the magnitude of the changes to each user entry. Similar notions, such as pixelDP  and perceptual indistinguishability , are also formalization of DP where adjacent datasets differ in a single feature of the input (e.g. a pixel) or some custom function of the data. Several algorithms have been defined for these notions, including LSH algorithms .

From an application point of view, much work has focused on geo-indistinguishability [3; 6; 15], i.e. preventing an adversary from distinguishing two close locations (by ensuring that close location have similar probabilities to generate a certain output). Other areas of applicability has been protecting textual data [39; 41], private smart meters sensing , image obfuscation [35; 21] and mobile crowsensing .

\(k\)-Clustering:\(k\)-Means and \(k\)-median clustering have seen a large body of work over the past few decades. While both problems are known to be NP-hard , a significant amount of work has given various \(O(1)\)-approximation algorithms for both problems [18; 8; 50; 7; 55; 16; 2; 26]. The state-of-the-art approximation is a \(5.912\)-approximation for Euclidean \(k\)-means and a \(2.406\)-approximation for Euclidean \(k\)-median . As noted in previously, there has also been significant work in specifically studying _differentially private \(k\)_-means and \(k\)-median clustering, though to our knowledge we are the first to study distance-based private clustering.

## 2 Preliminaries

We present some basic definitions and setup that will be sufficient for explaining our algorithms for the main body of the paper. We defer some additional preliminaries to Appendix A.

### Differential Privacy

First, we recall the definition of differential privacy.

**Definition 2.1**.:  A (randomized) algorithm \(\) is said to be \((,)\)_-differentially private_ (\((,)\)-DP for short) if for any two datasets \(X\) and \(X^{}\) that differ in exactly one data point and any subset \(S\) of the output space of \(\), we have

\[((X) S) e^{}((X^{}) S)+.\]

In standard differential privacy, two datasets \(X\) and \(X^{}\) are _adjacent_ if we can convert \(X\) to \(X^{}\) either by adding, removing, or changing a single data point. Notably, the change in the single data point may be arbitrary.

In distance-based privacy, however, we only allow two datasets to be adjacent if they differ by changing (not adding or removing) a single data point, by moving it up to distance \(\). Formally, we define the following.

**Definition 2.2**.: Let \(X,X^{}\) be \(\)_-adjacent_ if they have the same number of points and differ in exactly one data point, where the distance between the two differing data points is \(\). Then, a (randomized) algorithm \(\) is \((,,)\)-dist-DP if for any two \(\)-adjacent datasets \(X\) and \(X^{}\) and any subset \(S\) of the output space of \(\), we have

\[((X) S) e^{}((X^{}) S)+.\]

We remark that in all of our theoretical guarantees, we implicitly assume that \(,\).

The Laplace Mechanism is one of the most common primitives used to ensure privacy. Simply put, for a non-private statistic, the Laplace Mechanism adds noise \((t)\) to the statistic for some \(t>0\), where \((t)\) has the probability density function (PDF) equal to \( e^{-|x|/t}\). It is well-known that if \(f(X)\) is a statistic such that \(|f(X)-f(X^{})|\) for any two adjacent datasets \(X,X^{}\), then \(f(X)+(/)\) is \((,0)\)-DP. Likewise, if \(|f(X)-f(X^{})|\) between two \(\)-adjacent datasets \(X,X^{}\), then \(f(X)+(/)\) is \((,0,)\)-dist-DP.

Similar to the Laplace Mechanism, we can also implement the _Truncated Laplace mechanism_ for approximating functions \(f:X\). The Truncated Laplace Mechanism outputs \(f(X)+(,,)\), where \((,,)\) is the distribution with PDF proportional to \(e^{-|x|/}\) on the region \([-A,A]\), where \(A=(1+-2}{2})\), and PDF \(0\) outside the region \([-A,A]\). Assuming \(0<\) and \(0<\), it is known that if \(|f(X)-f(X^{})|\) for all adjacent \(X,X^{}\), then this mechanism is \((,)\)-DP, and if \(\) this is accurate up to error \(\), with probability \(1\).

Likewise, a nearly identical result holds for distance-based privacy. Namely, if \(|f(X)-f(X^{})|\) for any \(\)-adjacent datasets \(X,X^{}\), then \(f(X)+(,,)\) is \((,,)\)-dist-DP.

We defer some additional preliminaries to Appendix A.

### \(k\)-Means and \(k\)-Median Clustering

We define \(d(x,y)\) to be the Euclidean distance between two points \(x\) and \(y\), and for a finite subset \(C^{d}\), we define \(d(x,C)=d(C,x)\) to be \(_{c C}d(x,c)\). Given a dataset \(X=\{x_{1},,x_{n}\}\) of points in \(^{d}\), and a set of centers \(C=\{c_{1},,c_{k}\}\), we define the \(k\)-means/\(k\)-median cost as

\[(X;C):=_{x X}d(x,C)^{p}.\]

Above, \(p=2\) for \(k\)-means and \(p=1\) for \(k\)-median. Finally, we define \(_{k}(X)\) to be the minimum value of \((X;C)\) for any set of \(k\) points \(C\).

We further assume that the points in \(X\) are in \(B(0,)\), which is the ball of radius \(\) about the origin in \(^{d}\). Our goal in \(k\)-means (resp., \(k\)-median) clustering is to find a subset \(C\) of \(k\)-points that minimizes \((X;C)\), i.e., where \((X;C)\) is as close to \(_{k}(X)\) as possible. Occasionally, we may assign each point \(x_{i} X\) a positive weight \(w_{i}\), in which case we define \((X;C):=_{x_{i} X}w_{i} d(x_{i},C)^{p}\).

Our goal in differentially private clustering is to produce a set of \(k\) centers \(C\) such that \(C\) is \((,)\)-DP with respect to \(X\), and such that \((X;C)(X)+V^{p}\) (where \(p=2\) for \(k\)-means and \(p=1\) for \(k\)-median), where \(\) and \(V\) are not too large. In distance-based privacy, we wish to replace the factor \(\) with some smaller \(\), i.e., we want \((X;C)(X)+V^{p}\). However, our algorithm only has to be private up to changing a single data point by up to \(\). If we obtain this guarantee, we say that we have a \((,V)\)-approximate and \((,,)\)-dist-DP solution.

## 3 Technical Overview and Roadmap

We focus on proving Theorem 1.1 in Sections 4 and 5, and discuss our experimental results in Section 6. In Sections 4 and 5, we will only describe the algorithms, and we defer all formal proofs to the Supplementary sections. For simplicity, in this overview we focus on \(k\)-median and assume the dimension \(d=( n)^{O(1)}\), and can be hidden in \(\) notation.

Our approach follows two high-level steps, inspired by the work of . The insight used in , which proved highly efficient private clustering algorithms, is to start by generating a crude but private solution that may use a large number of centers and have a large approximation, but has small additive error. Then, one can apply the crude solution to partition the Euclidean space \(^{d}\) into smaller regions, and apply some regular differentially private clustering algorithm in the regions. We follow a similar high-level template to . However, we still need to implement each of these steps, which require several technical insights to ensure we maintain privacy while only losing additive error roughly proportional to \((k,d)\).

To obtain a crude approximation, we use a technique based on partitioning the space \(^{d}\) into randomly shifted grids at various levels (also known as the Quadtree). In the Quadtree, the \(0\)th level is a very coarse grid containing the large ball of radius \(\), and each subsequent level refines the previous level with smaller grid cells. For a single grid and knowledge of which point lies in which grid cell, a natural approach for minimizing cost would be to output the centers of the "heaviest" cells, i.e., those with the most number of points. Indeed, it is known that outputting the \(O(k)\) heaviest cells at each grid level provides a good approximation, at the cost of having more than \(k\) centers.

While this is not DP, a natural way of ensuring privacy would be to add Laplace noise to each count and add the heaviest cells after this. Unfortunately, doing so will lead to error depending on the full radius \(\), due to the coarser levels of the quadtree (i.e., levels with grid length close to \(\) rather than \(\)). For example, if there was only a single data point, there will be at least \(e^{d}\) cells even at coarse levels, and several of them may have large noisy counts. Hence, we are likely to choose completely random cells, which will cause additive error to behave like \(\) as opposed to \(\). Another option is to add noise to the points first and then compute the heaviest cells. While this avoids additive dependence on \(\), the additive dependence will behave like \(n\) where \(n\) is the full size of the dataset.

Surprisingly, we show that we can _combine_ both of these observations in the right way. Namely, for coarse cells (i.e., with length larger than \(()\)), we add noise (of distance proportional to \(()\)) to the data points directly to generate _private points_\(_{i}\), and then compute the heaviest cells without adding noise to the counts. For fine cells (length smaller than \(()\)), we do not add noise to the data points, but we add Laplace noise to the cell counts.

To explain the intuition behind this, suppose that the \(n\) data points happen to be perfectly divided into \(n/k\) clusters, where every point has distance \(r\) to its nearest cluster center. If \(r\), then even if we add \(()\) noise to each data point, we will still find cluster centers that are within \((r)\) of each correct center. So, the \(k\)-means cost should only blow up by a small multiplicative factor, without additive error. Alternatively, if \(r\), then the grid cells of side length \((r)\) should contain the entire cluster, and hence have \(n/k\) points in them. Assuming \(n d k\), even if we add Laplace noise to each of \(e^{d}\) cells, none of them will exceed \(n/k\). Alternatively, if \(n d k\), then our approach of simply adding noise to the points and obtaining \(n\) error will be only \(O(dk)\), which is small.

In summary, we can generate a crude approximation \(F\) with roughly \(O(k)\) cells per grid level (and \((k)\) centers total), with small additive ratio. But we desire for the number of centers to be exactly \(k,\) and the multiplicative ratio to be \(O(1)\), whereas ours will end up being \(d^{O(1)}\). To achieve such an accurate result, we use \(F\) to partition the data into regions, and apply a private coreset algorithm on each. By combining these coresets together, we may obtain a private coreset of the full data, and then we can apply an \(O(1)\)-approximate non-private algorithm on the coreset.

A first attempt, inspired by [22; 25], is to send each \(x_{i}\) to a region \(S_{j}\) if \(f_{j} F\) is the closest center to \(x_{i}\), and then compute a standard (i.e., not dist-DP) private coreset on each region \(S_{j}\). To avoid dealing with large additive errors depending on \(\), we further split each region into a close and far region, depending on whether the distance from \(x_{i}\) to \(f_{j}\) is more than or less than \(S\) for some parameter \(S\).

This attempt will still suffer from a large additive cost. For instance, if a point moves, even by distance \(\), it may move from a close region to a far region. Hence, the far region may have \(1\) more point, and since the far regions have diameter \(\), an algorithm that is private to adding or deleting a point must incur error proportional to \(\).

Our fix for this is to assign each \(x_{i}\) to a region not based on its closest point and distance, but instead based on \(_{i}\)'s closest point and distance, where we recall that \(_{i}\) is the noisy version of \(x_{i}\). For the points \(\{x_{i}\}\) that are mapped to a far region (meaning \(_{i}\) is far from its nearest \(f_{j}\)), we will simply use \(\{_{i}\}\) as the coreset, as \(_{i}\) is already dist-DP. However, for points that are mapped to a close region, while we use \(_{i}\) to determine which region the point \(x_{i}\) is mapped to, we compute a private coreset using  on the points \(x_{i}\), rather than use the points \(_{i}\).

To explain why this algorithm is accurate, for the close regions, we obtain additive error proportional to \(S\) as we apply the private coreset on a ball of radius \(S\). There is one region for each center in \(F\), which multiplies the additive error by \(|F|=(k)\). For the far regions, we first note that \(d(_{i},C)=d(x_{i},C)()\) for any set of \(k\) centers \(C\), as \(d(x_{i},_{i})()\). Hence, we have additive error \(()\) per point. While this seems bad as this might induce additive error for \(n\) points, we in fact show that this additive error can be "charged" to multiplicative error. To see why, if \(x_{i}\) mapped to the far regions, this means \(d(_{i},F)\), which also means \(d(x_{i},F)()\), If there were \(T\) such points, then the total cost of \(X\) with respect to \(F\) is at least \(T S\), whereas the additive error is roughly \(T\). Finally, in our crude approximation we show \((X;F)\) is at most \(d^{O(1)}\) times the optimum \(k\)-means cost, which means for \(S d^{O(1)}\) the additive error is small even compared to the optimum cost. Hence, we can charge the additive error to multiplicative error. We still have additive error from the close regions, but for \(S=d^{O(1)},\) the additive error is only \((k,d)\).

To summarize, while our techniques are inspired by , one important novel technical contribution of our work is that while  uses the true locations of the points to assign them to regions, we first add Gaussian noise to the points to determine their region, and then use the noised points _only_ for the "far" regions and the true points _only_ for the "close" regions. This change is crucial in ensuring the analysis is successful. In addition, we must set several parameters carefully to charge the additional incurred cost either to a small additive or small multiplicative factor.

## 4 Crude Approximation

In this section, we devise a crude bicriteria approximation that will serve as a starting point in developing our more refined algorithm. A _bicriteria_ approximation is a set \(F\) of \( k\) points, that is \((,,)\)-DP in terms on \(X\), and in addition, it is a \((,V)\) approximation, i.e., \((X;F)_{k}(X)+V^{p}\), where \(p=1\) for \(k\)-median and \(p=2\) for \(k\)-means. Even though \(F\) has more than \(k\) points, we still compare to the optimal solution with exactly \(k\) points. We will show such an algorithm with \(=( n,)\), \(=(d)\), and \(V=(k,d,^{-1},^{-1}, n)\). We defer the formal theorem statement, along with the proof, to Appendix B.

Algorithm Description:The algorithm works as follows. For each \(i n\), let \(_{i}\) be generated by adding \(O()(0,I)\) noise to each data point \(x_{i}\). Let \(=\{_{1},,_{n}\}\).

We create \(REP=O( n)\) random quadtrees starting from the top level with side length \(\) (full diameter of pointset) until the bottom level of size length \(/B\), for some parameter \(B\). Next, for some parameter \(A\), for each level with side length between \( A\) and \(/B\), we count how many points are in each cell, add \((1/^{},1/^{})\) noise, where \(^{}=(/)\) and \(^{}=(/( n(A B)))\), and then pick the \(4k\) cells in that level with the largest number of points in them, after adding noise to the number of points. For the levels of side length more than \( A\), we count how many of the \(_{i}\) points are in each cell and then pick the \(4k\) cells in that level with the largest number of points in \(\). Our final algorithm simply outputs the union of all cell centers that we have picked.

One issue is that the number of cells is exponential in \(d\), so adding noise to each cell count may be inefficient. To fix this, we will only add \((1/^{},1/^{})\) noise to cells that were nonempty, and will only pick a cell center if its noisy count is at least \(}\), for some large constant \(K\). Since an empty cell, even after adding noise to its count, can never exceed \(}\), we can pretend we did the same procedure to the empty cells, but simply never included them. It is straightforward to verify that every other step of the algorithm is implementable in polynomial time.

We provide pseudocode for the algorithm in Algorithm 2 in Appendix B, and we discuss the runtime at the end of Appendix C.

## 5 From Crude to Accurate

In this section, we devise an improved approximation that only uses \(k\) centers and achieves a constant approximation ratio, using the crude approximation from Section 4 as a starting point. We will subsequently prove Theorem 1.1. Again, we defer all proof details to Appendix C.

Our approach utilizes both the crude approximation from Section 4 and previously known constant-approximation differentially private (but not dist-DP) algorithms from the literature, to create a dist-DP "semi-coreset" for clustering. More formally, given a set of \(n\) points \(X=\{x_{1},,x_{n}\}^{d}\), we will compute a (weighted) set of points \(Y\) that is \((,,)\)-dist-DP with respect to \(X\), such that for any set of \(k\) centers \(C=\{c_{1},,c_{k}\}\), \((Y;C)=((X;C)) O(_{k}(X)) W ^{p}\), where \(W\) will be polynomial in \(d,k,^{-1},^{-1}, n\), and \(\).

If we can achieve this, then we just have to compute an \(O(1)\)-approximate \(k\)-means (or \(k\)-median) solution to \(Y\), which does not have to be private since \(Y\) already is. Indeed, one can prove an \(O(1)\)-approximation of \(Y\) will be a dist-DP \((O(1),O(W))\)-approximate solution for \(X\).

Algorithm Description:Our algorithm works as follows. First, for each point \(x_{i} X\), add \(O()(0,I)\) noise to get a point \(_{i}\). (Recall: this was also done for the crude approximation.)

Next, we partition the set of points into regions, using our dist-DP bicriteria approximation \(F\) from Section 4. If \(d(_{i},F)> S\) for some parameter \(S\), we send the _noised point_\(_{i}\) to the set \(_{0}\), and send the index \(i\) to the index set \(I_{0}\). Else, if \(_{i}\) is closest to center \(f_{j}\) (for \(j k\)), we send the _true point_\(x_{i}\) to the set \(_{j}\), and send \(i\) to the index set \(I_{j}\). In fact, for all \(j\) including \(j=0\), we may define \(_{j}\) to be the set \(\{x_{i}:i I_{j}\}\). Note that \(_{j}\) is a full partition of the dataset \(X\). For each \(j 1\), we will define the region \(R_{j}\) as the ball of radius \(O( S)\) around \(f_{j}\).

For each \(0 j k\), we let \(_{j}\) be the number of indices in \(I_{j}\). Note that this equals the number of points mapped to \(_{j}\). If \(_{j}<T\) for some parameter \(T\), then we define \(_{j}\) to be the corresponding points \(\{_{i}:i I_{j}\}\). Otherwise, we apply the private semi-coreset algorithm from  to find a private semi-coreset \(_{j}\) of the dataset \(_{j}\), with respect to the ball \(B(f_{j}, O(S/))\) for some parameter \(<1\). Finally, we will merge all the semi-coresets \(_{j}\) together, which includes \(_{0}\) defined in the previous paragraph, to obtain \(\). Finally, we may apply any \(O(1)-\)approximate (non-private) clustering to \(\).

We provide pseudocode for the algorithm, in Algorithm 1.

```
1:\(_{j}\)
2:for\(i\{1,,k\}\)do
3:\(_{i}\)
4:\(_{i}\)
5:for\(j k\)do
6:\(_{j}\)
7:\(_{j}\)
8:\(_{j}\)
9:for\(j k\)do
10:\(_{j}_{j}_{j}\)
11:\(_{j}_{j}_{j}\)
12:\(_{j}_{j}_{j}\)
13:\(_{j}_{j}_{j}\)
14:endfor
15:for\(j k\)do
16:\(_{j}_{j}_{j}_{j}\)
17:\(_{j}_{j}_{j}_{j}\)
18:endfor
19:\(_{j}_{j}_{j}_{j}\)
20:endfor
21:return\(_{j}\) ```

**Algorithm 1**\(_{j}\)

## 6 Empirical Evaluation

In this section, we study the emperical approximation of our \(\)-dist-DP \(k\)-means clustering algorithm.

Datasets.We evaluate our algorithm on \(6\) well-known public datasets _brightkite_\((51406 2)\), _gowalla_\((107092 2)\), _shuttle_\((58000 10)\), _skin_\((245057 4)\), _rangequeries_\((200000 6)\) and _s-sets_\((5000 2)\), where brightkite andgowalla are datasets of geographic locations (latitude and longitude) of users and can be found in Stanford Large Network Dataset Collection (SNAP) , shuttle, skin and rangequeries are non-geographic datasets and can be found on UCI Repository ,and s-sets is another non-geographic dataset and can be found in the clustering benchmark dataset2. For each dataset, we preprocess it to make it fit into \([-1,1]^{d}\). We refer readers to Appendix E for more details of the preprocessing steps.

Setup.We compare our algorithm described in Algorithm 1 in Section 5 with other three algorithms. We report the \(k\)-means cost of all algorithms. In all plots, the label of our algorithm is "dist-DP \(k\)-means". The three compared baseline algorithms are as follows.

1. _Non-private baseline_ (\(k\)-means++): We compare our algorithm with the non-private \(k\)-means solver using \(k\)-means++ seeding implemented by Python scikit-learn package . The output \(k\)-means cost of this baseline can be regarded as the groudtruth cost.
2. _DP baseline_ (DP \(k\)-means): This is a \(k\)-means clustering algorithm in the standard DP setting implemented in part of a standard open-source DP library 3.

3. \(\)_-Dist-DP baseline_ (dist-DP random points): Finally, we also compare with a natural \(\)-dist-DP algorithm described as the following. We run non-private \(k\)-means solver on \(\) described in Section 4. Since \(\) is a \(\)-dist-DP version of \(X\), the output centers are \(\)-dist-DP. Note that since the final solution of this baseline only depends on \(\), we assign the entire privacy budget \((,)\) to computing \(\).

In all experiments, we fix privacy parameters \(=1,=10^{-6}\). These parameter setups are standard in many other DP papers as well. We evaluate our algorithms for different choices of the privacy parameter \(\). Note that the parameter \(\) should not be determined by our algorithm. We try different \(\) to show how the choice of \(\) affects the clustering quality. We refer readers to Section 7 for more discussions of the choice of \(\).

We use the DP coreset implementation provided by the DP baseline for the purpose of the computation of semi-coreset \(_{j}\) described in Section 5.

Our Results.We run all algorithms for \(k=4,6,8,12,16\). For each experiment, we repeat \(10\) times and report the mean and the standard error. In the experiments shown in Figure 1, we fix \(=0.05\)4. As shown, the \(k\)-means cost of our dist-DP \(k\)-means algorithm is always smaller than the cost of DP\(k\)-means baseline and is only slightly worse than the non-DP baseline which is as expected. The dist-DP baseline introduces a large \(k\)-means cost which implies that our partitioning strategies described in Section 4 and Section 5 are indeed necessary and can improve the clustering quality significantly in practice. Finally, we fix \(k=8\) and investigate how the changes of \(\) affect the \(k\)-means cost of our dist-DP \(k\)-means algorithm. We run our algorithm on all datasets for \(=1,0.08,0.008,0.0001\). As shown in Figure 2, the \(k\)-means cost of our algorithm decreases as \(\) decreases, which is as expected. For running time, though we did not optimize our implementation, each algorithm runs within at most a few minutes in a single thread mode.

In summary, for a reasonable range of \(\), we significantly outperform previous DP \(k\)-means algorithms, whereas more naive distance-based DP algorithms perform far worse. In addition, we have comparable approximation guarantees even to the non-private \(k\)-means algorithm.

## 7 Limitations and Open Problems

In this work, we propose efficient \((,,)\)-dist-DP algorithms for \(k\)-means and \(k\)-median problems for any given privacy parameters \(,,\). However, the choices of \(,\) and \(\) remain open. Notice that these privacy parameters should not be determined by our algorithm, but rather by legal teams, policy makers, or other experts for different specific scenarios. This is an expert determination that is outside of the scope of this paper but has been studied by practitioners extensively.

Figure 1: \(k\)-Means cost of non-private baseline (blue), DP baseline (green), our dist-DP \(k\)-means (yellow), and dist-DP baseline (gray) for different \(k\) with \(=0.05\). Shades indicate \(3\) standard error over \(10\) runs.

Figure 2: \(k\)-Means cost of dist-DP \(k\)-means algorithm for various \(\) with \(k=8\). Shades indicate \(3\) standard error over \(10\) runs. The result supports the interpolating nature of the parameter \(\). In particular, when \(\) decreases, the \(k\)-means cost also decreases. When \(=0\), we exactly recover the result as non-private \(k\)-means++.

In proving Theorem 1.1, we obtains an additive error proportional to \(k^{2}^{2}\) (ignoring polynomial factors in \(d\) and logarithmic factors in the other parameters - see Theorem C.1), whereas the work of  has dependence \(k^{2}\). This is because to improve the dependence on \(\) to a dependence on \(\), we end up partitioning the data into roughly \(k\) regions and must apply a separate private \(k\)-means algorithm on each region, which increases the additive dependence on \(k\). Hence, a natural open question is whether one can improve the additive error's dependence on \(k\).

Acknowledgements:SN is supported by a NSF Graduate Fellowship (Grant No. 1745302) and a Google PhD Fellowship.