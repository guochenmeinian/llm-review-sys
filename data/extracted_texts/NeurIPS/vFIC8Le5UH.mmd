# Auditing Empirical Privacy Protection for Adaptations of Large Language Models

**Lorenzo Rossi, Bartlomiej Marek, Vincent Hanke, Xun Wang,**

**Michael Backes, Adam Dziedzic, Franziska Boenisch**

CISPA Helmholtz Center for Information Security

###### Abstract

Recent work has applied differential privacy (DP) methods to adapt large language models (LLMs) for sensitive applications. While DP offers theoretical privacy guarantees, their practical implications for LLM adaptations remain uncertain. This uncertainty arises from LLM pretraining, where overlap and interdependencies between pretraining and adaptation data can impact privacy leakage despite DP adaptation efforts. To analyze the issue from a practical standpoint, we thoroughly investigate privacy risks under "private" adaptations in LLMs. Relying on the latest privacy attacks, such as robust membership inference, we study the actual privacy risks for the pretraining and adaptation data. We benchmark the privacy risks by systematically varying the distribution of adaptation data, ranging from data perfectly overlapping with the pretraining set through in-distribution (IID) scenarios to entirely out-of-distribution (OOD) examples. Additionally, we evaluate how different kinds of adaptation methods and different privacy regimes impact the vulnerability. Our results reveal that distribution shifts significantly affect the vulnerability to privacy attacks: the closer the distribution of the adaptation data is to the pretraining distribution, the higher its practical privacy risk, even when there is no overlap between pretraining and adaptation data. We find that the highest empirical privacy protection is achieved for OOD data using parameter-efficient fine-tuning (PEFT) methods, such as LoRA. Surprisingly, when considering data from the same distribution, using the pertaining data for adaptations exhibits a similar privacy leakage as the corresponding validation data. To effectively prevent privacy leakage, it is required to train the adaptations with strict differential privacy protection (with \(<0.1\)). Finally, our results show that private adaptations, especially done with prefix tuning, can also decrease the empirical leakage from the pretraining data.

## 1 Introduction

The use of large language models (LLMs) for sensitive downstream tasks has grown rapidly, often implemented through methods that guarantee differential privacy (DP)  for the adaptation data . However, this approach may not provide the anticipated privacy protections . The challenge arises from potential overlap or complex dependencies between data used to pretrain the LLMs and the adaptation dataset and from the leakage of the pretraining data that is not protected through the differentially private adaptations. The problem is exacerbated by the fact that for most large models, their pretraining datasets are not disclosed, rendering a structured study of the interdependencies with the private adaptation data impossible.

**Contributions.** In this paper, we examine practical privacy risks that arise under "private" LLM adaptations. Therefore, we first systematically characterize the setup of privacy auditing required for the novel _pretrain-adapt_ learning paradigm that underlies LLMs and their adaptations. We identify four different stages of auditing under the pretrain-adapt paradigm, namely (1) audit pretraining, (2) audit adaptations, (3) joint audit of pretraining and adaptations, and (4) post-adaptation auditing of the pretraining. A general overview of privacy auditing for adapted LLMs is provided in Figure 1. Second, we re-define the _membership inference game_ for each of the identified stages to provide a formal grounding necessary for structured privacy audits. We then instantiate multiple _membership inference attacks_ against adapted LLMs to audit privacy leakage from the adaptations, and to understand the impact of adaptations on the pretraining privacy, corresponding to stage (2) and (4) in our taxonomy on privacy audits in the pretrain-adapt paradigm.

We systematically analyze a spectrum of possible distributions for the adaptation data with respect to the pretraining

Figure 1: **Setup for Privacy Auditing of LLM Adaptations.** We consider different types of datasets used for LLM adaptations ranging from perfect overlap to OOD data with respect to the pretraining data. The auditor is assumed to have query access to the LLM after its adaptation. To instantiate the membership inference attack, we use one reference model that is fully fine-tuned on \(Z\), where \(Z\) is a set of samples from the same distribution as the adaptation data.

data--from perfect OOD examples--to understand the possible privacy implications over all setups. We also study a wide range of private adaptation methods and different privacy regimes for structured reasoning about the resulting risks. Finally, we discuss the way and the challenges towards a wholistic privacy auditing of adapted LLMs in the pretrain-adapt paradigm.

**Summary of our Empirical Findings.** Our results empirically confirm the theoretical concern that pretraining significantly impacts the privacy risk of the adaptation data . Especially the closeness of pretraining and adaptation data distributions plays a crucial role: the closer the adaptation data distribution is to the pretraining data, even when there is _no overlap_ in the datasets, the higher the privacy risks. Also, the type of (private) adaptation has a significant impact on privacy leakage. We find that parameter-efficient fine-tuning (PEFT) methods, such as LoRA  with their DP version , even though they yield utility on par with fine-tuning approaches, cause significantly lower empirical privacy leakage. Thus, the best empirical privacy protection can be achieved for adaptations with PEFT methods on OOD data. Interestingly, when adapting on data from the same distribution, privacy leakage levels are similar between adaptation data and the corresponding validation data. To effectively prevent privacy leakage, adaptations must be trained with stringent differential privacy constraints (\(<0.1\)). Finally, we find that (privately) adapting pretrained models with prefix tuning  using their DP versions  can also reduce the leakage of the pretraining data. This is probably because the noise added for the private adaptation adds protection to the pretraining data .

## 2 Background

We begin with a background on DP, DP adaptations for LLMs, and MIA.

### _Differential Privacy_

The mathematical framework of DP  formalizes the intuition that privacy guarantees can be obtained when a randomized mechanism \(\) executed on two neighboring datasets \(D\), \(D^{}\) that differ in only one data point, yields roughly the same result, _i.e.,_

\[[(D) S] e^{}[(D^{})  S]+.\] (1)

The privacy parameter \(\) specifies by how much the result is allowed to differ and \(\) is the probability of failure to meet that guarantee. There are two canonical algorithms to implement DP guarantees in machine learning (ML): DPSGD (the _Differentially Private Stochastic Gradient Descent_) algorithm , which extends standard stochastic gradient descent with clipping and noising gradients, and PATE (_Private Aggregation of Teacher Ensembles_) , which is an inference time algorithm that privately transfers knowledge from an ensemble of teachers to a public student model.

### _Private Adaptations of LLMs_

LLMs are pretrained on extensive amounts of public data, followed by their adaptation to private downstream tasks. The existing methods for private LLM adaptations fall into two categories: (1) _private tuning methods_, such as PrivateLoRA  or PromptDPSGD , that rely on access to the LLM gradients and are based on the DPSGD algorithm, and (2) _private in-context learning (ICL) methods_, such as DP-ICL  or PromptPATE , which require only API (black-box) access to the LLM and are based on the PATE algorithm. The private tuning-based methods can be applied only to LLMs that expose their parameters, which are commonly referred to as _open LLMs_. On the contrary, the private ICL techniques are applicable to both open and closed LLMs, such as GPT4  or Claude3 . However, individual users have to share their private data with the LLM providers to perform adaptations of the closed LLMs. Given this additional privacy leakage and the fact that it was recently shown that adaptations on open LLMs strictly outperform their closed counterparts in terms of privacy protection, performance, and price , in our work, we rely on the private adaptations of _open LLMs_.

There are three main approaches to private adaptations of open LLMs: prompt-based, parameter-efficient fine-tuning, and full fine-tuning.

**(1) Prompt-based adaptations** introduce a small number of additional parameters, usually comprising less than 1% of the total LLM parameters, which are applied only in the input space of the model. These parameters may be added at the level of token embeddings (soft prompts ) or to all (attention) layers of the LLM (prefix-tuning ).  proposed _PromptDPSGD_, which adapts the differential private stochastic gradient descent (DPSGD) algorithm  for use with soft prompts. A key advantage of prompt-based adaptations is their ability to support multi-task batch processing, meaning multiple soft prompts for different tasks (and users) can be handled in the same mini-batch during training or inference.

**(2) Parameter-efficient fine-tuning-based adaptations** introduce a slightly larger number of parameters, typically under 10% of the total LLM parameters, which are placed within the model, often in each block of a transformer . These added parameters are tuned while keeping the original pretrained parameters frozen. _PrivateLoRA_ extends LoRA  with DP guarantees by leveraging the DPSGD algorithm.

**(3) Full fine-tuning** involves fine-tuning either the entire model or the last few layers (the latter is also referred to as "head fine-tuning"). _DP-FineTune_, which also relies on the DPSGD algorithm, demonstrates that full fine-tuning with DP optimization can offer strong privacy guarantees while maintaining good performance. The general trend in selecting an appropriate method suggests that more complex tasks require a higher number of tunable parameters . For simple tasks, PromptDPSGD  is often sufficient, while DP-LoRA  is recommended for tasks of moderate difficulty, and full fine-tuning  is best suited for complex tasks.

### _Membership Inference Attacks_

A membership inference attack (MIA)  aims to determine whether a specific data point can be identified as part of a model's training set. This approach plays a crucial role in applications ranging from privacy assurance  to identifying protected or copyrighted content embedded in pretraining data . While most MIA research has focused on supervised learning settings , new advancements reveal their broader relevance.  revealed a discrete-prompt-based MIA, disclosing vulnerabilities in proprietary LLMs like GPT-3, which risk leaking private information through prompt-based queries . Robust membership inference attack (RMIA)  were recently introduced and outperformed prior attacks by optimizing computation with a more precise null hypothesis and leveraging both a reference model and population data. This not only enhances the attack's strength and robustness but also makes the attack computationally more tractable, as it requires only one reference (_shadow_) model where prior work  required training hundreds. Due to its strong performance, we mainly harness RMIA in our work to quantify privacy leakage from private adaptations of open LLMs. Min-K%  offers a computationally efficient and reference-free method for pretraining data detection in LLMs. By focusing on outlier tokens with low probabilities, this method calculates an average log-likelihood score to determine whether a text was included in the training corpus. We rely on Min-K% for additional privacy evaluations. See Appendix A for a more in-depth description.

## 3 Characterizing Privacy Audits under the Pretrain-Adapt Learning Paradigm

In standard ML, where models are trained from scratch on a given, potentially sensitive, dataset, privacy audits can be conducted directly with respect to the training data. However, under the pretrain-adapt learning paradigm, we find that privacy audits are significantly more complex due to the availability of both the pretraining and the adaptation dataset and their interplay. In this section, we taxonomize the different stages relevant for auditing under the pretrain-adapt paradigm and define auditing of the different stages as adversarial games that allow us for a structured reasoning about leakage and privacy protection.

### _Taxonomizing Audit Stages_

We identify four stages of auditing based on the learning paradigm and its respective pretraining dataset \(S\) and adaptation data \(D\) in our taxonomy.

**(1) Auditing pretraining** is the most similar stage to the standard ML auditing. It aims at identifying privacy leakage of the pretraining data from the pretrained model. The difference to privacy audits in standard ML is that the pretraining is usually done on much larger datasets, with larger models where the applicability and effectiveness of rigorous privacy protection through DP , as well as the applicability of standard privacy auditing techniques like MIA , are limited.

**(2) Auditing adaptations** is a new aspect in the pretrain-adapt paradigm. It is concerned with detecting leakage of the adaptation dataset from the adapted LLM. The key differentiating factor to privacy audits in standard ML is using a pretrained model that the adaptations are trained on instead of a random initialization. We assume the same pretrained model is used for all the considered adaptations in an adaptation audit.

**(3) Joint Auditing of Pretraining and Adaptations** considers both stages of pretraining and adaptations together. The goal is to audit leakage from both the pretraining and adaptation set from the adapted LLM. Under privacy preservation, the most standard setup consists of a non-DP-trained LLM and DP-trained adaptations.

**(4) Auditing Pretraining Post-Adaptations** evaluates how the (private) adaptations influence the potential protection of the data points used for pretraining, which is usually conducted without any formal guarantees. Changes to the model behavior induced through adaptations or noise added during their training might influence the effective exposure of pretraining data from model predictions.

### _Defining Audits as Adversarial Games_

Privacy audits can be modeled as an _adversarial game_\(\) where the main task is to guess if a given data point \(x\) was in a model's training set or not. This game can, therefore, also be referred to as the _membership inference game_. For standard ML, it is formulated as follows:

**Standard ML Adversarial Game.** Given a dataset \(D\), the target sample \(x\), a training algorithm \(}\), a sampling procedure \(x}\{0,1\}\), \(\) is executed as:

1. The challenger samples a binary variable \(a}\{0,1\}\) uniformly at random
2. The challenger trains a model \(}\), where \(=D\) if \(a=0\), otherwise \(=D\{x\}\)
3. The challenger sends \(\) to the attacker
4. The attacker guesses \((,x)\)

The attacker wins if his guess \(\) on whether \(x\) was used to train \(\) is correct.

To audit the different stages of the pretrain-adapt paradigm, we need to define a new game that accounts for the existence of both the pretraining dataset \(S\) and the adaptation data \(D\).

**Pretrain-Adapt Adversarial Game.** We define the adversarial game \(\) analogous to the one for standard ML, yet take two datasets, \(S\) the pretraining data, and \(D\) the adaptation data into account. Additionally, we denote the pretraining procedure by \(T\) and an adaptation procedure by \(T^{}\). We mark the deviations to the original game in blue.

1. The challenger samples \(a}\{0,1\}\) and \(b}\{0,1\}\) (where \(a\) and \(b\) are binary variables)2. The challenger trains a model \(}}{{}},_{0}\), where \(=S\) if \(a=0\), otherwise \(=S\{x\}\)
3. The challenger adapts \(\) such that \(^{}}}{{}}\), where \(=D\) if \(b=0\), otherwise \(=D\{x\}\)
4. The challenger sends \(^{}\) to the attacker
5. The attacker guesses \(,(,^{},x)\)

Whether the attacker has to guess both \(,\) and what background knowledge they have, _i.e.,_ whether they get access to both \(\) and \(^{}\) depends on the auditing stage. We detail the background knowledge and guesses by the attacker--formulated as hypotheses with a null hypothesis \(H_{0}\) and an alternative hypothesis \(H_{A}\)--for the four auditing stages from our taxonomy.

**(1) Auditing Pretraining.** In this setting, the challenger releases the pretrained model \(\) to the attacker. The attacker's goal is correctly guessing whether \(x\) was in the pretraining data \(S\). Their guesses \(\), are over the random variable \(a\).

\[H_{0}:a=0 H_{A}:a=1\]

**(2) Auditing Adaptation.** In this setting, the challenger releases only the adapted model \(^{}\) to the attacker. The attacker does not know whether \(x S\) or not and considers only the adaptation. Their guesses \(\), are, hence, over the random variable \(b\).

\[H_{0}:b=0 H_{A}:b=1\]

**(3) Joint Auditing.** In this setting, the challenger releases both the pretrained model \(\) and the adapted \(^{}\) to the attacker. Depending on the attacker's background knowledge, we consider three possible cases.

1. The attacker knows that \(x S\) and guesses \(b\). \[H_{0}:(a,b)=(0,0) H_{A}:(a,b)=(0,1)\]
2. The attacker knows that \(x S\) and guesses \(b\). \[H_{0}:(a,b)=(1,0) H_{A}:(a,b)=(1,1)\]
3. The attacker knows that the target sample \(x\) is either in both (pretraining and adaptation sets) or neither of them and guesses \((a,b)\). \[H_{0}:(a,b)=(0,0) H_{A}:(a,b)=(1,1)\]

**(4) Post-Adaptation Auditing.** In this setting, the challenger releases both the pretrained \(\) and the adapted \(^{}\). It is known that the target sample \(x\) is not in \(D\) and the attacker takes a guess on \(a\). \[H_{0}:(a,b)=(0,0) H_{A}:(a,b)=(1,0)\]

In essence, auditing pretraining considers only the pretraining itself. Similarly, auditing the adaptations considers the adaptations themselves. On the other hand, the joint adaptation reasons about both pretraining and adaptation sets. Finally, the post-adaptation auditing is also only for the pretraining set, but the applied adaptation influences the auditing. In this work, we focus mainly on the auditing of adaptations as defined in (2) but, on the way, also provide empirical insights into the post-adaptation leakage (4). Finally, we provide a discussion on ways and challenges towards holistic privacy audits under the pretrain-adapt paradigm that take into account all four stages.

## 4 Assessing Empirical Privacy Risks of Private LLM Adaptations

In the following experiments, we evaluate the empirical privacy risks of LLM adaptations in a "black-box" scenario. We focus on _open LLMs_, _i.e.,_ LLMs whose weights are publicly available, since relying on closed LLMs, such as GPT or Claude, for adaptations usually requires sharing the private data with the LLM provider, causing additional privacy risks .

Our results reveal the vulnerability of LLM adaptations to privacy leakage, which is higher when the adaptation dataset comes from the same distribution as pretraining datasets. The leakage is slightly lower when there is no overlap in the distribution of the adaptation data with the pretraining data. Furthermore, by experimenting with different LLM adaptation techniques, we demonstrate that this choice also impacts privacy vulnerability across most datasets, with prefix tuning leaking the most for the in-distribution adaptation data and full fine-tuning leaking more than other methods for the out-of-distribution adaptation.

### Experimental Setup

**Models.** We focus on the open-source Pythia suite of models  and its publicly available pretraining data, the Pile dataset , which is an 800GB collection of diverse English-language datasets, including text from sources, such as books, academic papers, or source code repositories.

**Datasets.** We categorize the datasets used in our experiments into **in-distribution (IID)** and **out-of-distribution (OOD)**, depending on their relationship to the pretraining data. IID datasets come from the same distribution as the pretraining data, and we identify two cases: one with a full overlap between pretraining and adaptation data, where we use data directly from the pretraining set for the adaptations, and one with no overlap, where the data is sourced from the corresponding validation set from the pretraining distribution. For the IID datasets, we focus on the following Pile subsets: BookCorpus2, consisting of publicly available books, GitHub, a set of open-source code repositories, and Enron Emails , a variety of different emails. In contrast, OOD datasets are derived from a different distribution and do not overlap with pretraining data. The OOD datasets we chose for our experiments are: SAMSum , an English-language dialogue summarization dataset, and GermanWiki , a large set of German Wikipedia entries. These OOD datasets were selected because of their different degrees of variation from the original distribution of the Pile dataset. Although SAMSum shares the same language (English), its general dialogue format, followed by the dialogue summary, is not present in the pretraining set. GermanWiki, on the other hand, presents wide syntactic and lexical variation from the pretraining dataset.

**Memorized samples.** Another privacy concern showed in prior work  is the memorization of samples during pretraining of an LLM. We analyze how adaptations can reduce the effect of the memorization of pretraining data. The definition of a memorized sample follows \(k\)-extractability from . Here, we have a prompt \(p\) of length \(k\) and a suffix \(s\). If the generation of a model given prompt \(p\) generates exactly \(s\), the sequence consisting of \(p\) and \(s\) concatenated is memorized. Furthermore, we also rely on samples from the Pile reported as memorized in Pythia 2.8B by prior work . This set of memorized samples consists of 505 sequences, and we refer to it as Mem Pile.

**Adaptations.** We evaluate different types of adaptations, including fine-tuning of all model parameters , or the last layer (_i.e.,_ the head) and PEFT methods, such as LoRA  and prefix tuning . Considering a Pythia 1B model, we train 1B parameter for full fine-tuning, 1M for LoRA, 130M for prefix tuning, and 100M for last-layer (head) fine-tuning.

**Membership Inference.** For membership inference, we rely on the latest membership inference attack, RMIA (Robust Membership Inference Attack) . We use its offline version because it is computationally effective and it does not require to train customized reference models for each targeted sample (as in the online version of the attack). We also leverage a single reference model for our experiments, as the authors show strong MIA performance even with only one reference model. We consider different types of reference models. Unless explicitly stated, we focus on using a "shadow" model (adaptation) trained in the same way as the target model, but on a different split of the same fine-tuning data. However, we also consider other models: Pythia-14M, Pythia-160M, Pythia-1B (which we report in the results below), Pythia-2.8B , GPT-neox , and GPT-2. RMIA has two hyperparameters, a threshold \(\), and a scaling factor \(\) (see Algorithm 1). The results represent the highest AUC achieved through a grid search to optimize these parameters. For an ablation on the RMIA hyperparameters choice, see Figure 6 in Appendix C. Additionally, we consider another commonly used reference-based MIA called Reference , which calibrates the loss of the target model on the target sample by dividing it by the loss of a reference model on the target sample. Finally, we also compare Min-K%, as a reference-less baseline. As with RMIA, we report the highest AUC achieved through a grid search on \(K\).

### Evaluating Practical Privacy Leakage through Membership Inference

We begin by assessing privacy leakage across various LLM adaptation methods. We present the control setup in Figure 2, where all adaptations are trained with \(=8\), share the same learning rate (LR = 0.0001) and the number of epochs (20). Privacy leakage is measured by the AUC score and plotted against validation loss, representing different stages of model training. Each point in the graph reflects the metric values at the end of a training epoch, with the rightmost lower points marking the start of training. As training progresses, points shift upward and to the left.

The results indicate that for the OOD adaptation dataset (SAMSum), privacy leakage is the highest with Head fine-tuning, followed by Full fine-tuning, for any given loss value. In contrast, LoRA and Prefix Tuning demonstrate significantly lower privacy leakage, with Prefix Tuning yielding the lowest AUC scores. We observe a similar trend with the IID BookCorpus2 dataset: privacy leakage is again the highest for Head fine-tuning, followed by Full fine-tuning, with LoRA consistently showing the lowest AUC scores across the board. An exception occurs with Prefix Tuning on this dataset, where it is more sensitive to parameter settings and incurs a much higher loss than other methods, making direct comparisons challenging. To address this issue, next, we shift to a detailed comparison of each method using its optimal parameter settings and present the individual results accordingly.

We compare privacy leakage across the two dataset types, IID and OOD, in Table 1 and Table 2, respectively. Since

Figure 2: **LoRA exhibits the lowest privacy leakage for a given loss value. The other methods: Full fine-tune and Head find-tune leak much more. Prefix Tuning leaks the least for OOD (SAMSum) data, however, it is an exception for IID (Bookcorpus2 Val). The x-axis shows the evaluation loss. The y-axis represents the AUC score. All adaptations have been trained with \(=8\), a learning rate of 0.0001, and over 20 epochs.**

[MISSING_PAGE_FAIL:6]

(_shadow_) and their AUC scores from Table 1 and Table 2. For three out of four adaptation methods (Prefix tuning, LoRA, and Full fine-tune) we observe much higher privacy leakage for Enron (IID) than for SAMSum (OOD). The only exception is the Head fine-tune, for which the adaptation with SAMSum exhibits much higher leakage than with Enron. Overall, for most adaptations, the overlap in the distributions between pretraining sets and the adaptation data incurs higher privacy leakage.

Next, we examine data from the same distribution, specifically Bookcorpus2, where its _Train_ version was used for pretraining whereas the corresponding _Val_ (validation) set was held out. The observed validation loss values in Table 3 for both the training and validation versions of Bookcorpus2 are very similar. Likewise, the reported AUC scores in Table 2 are also almost the same in both cases. Thus, our findings indicate a comparable privacy leakage when using the pertaining data and the corresponding validation data for the adaptations.

Moreover, we also consider the more realistic settings with _RMIA (Pythia-1B)_ and _Reference (Pythia-1B)_, where the attacker does not have access to a shadow model and instead uses the (non-adapted) pretrained model as the reference model. In both cases, AUC scores are similar across all tested models, suggesting that there is no gain in using RMIA when shadow models cannot be trained. Moreover, both cases show significantly reduced performance compared to _RMIA (shadow)_. For \(=0.1\), AUC scores drop to near random guessing for both types of datasets. Increasing the privacy budget to \(=8\) slightly improves the MIA performance on IID datasets, with an average AUC score of 0.58, while the OOD datasets remain at random guessing. At \(=\) the two MIAs become more effective. LoRA continues to show high privacy protection compared to the other tested adaptations. On IID datasets, LoRA achieves an average AUC score of 0.93, slightly lower than the scores for Prefix Tuning (0.95), Full fine-tune (1.00), and Head fine-tune (0.99). However, for SAMSum and GermanWiki, we observe a higher difference. Here, we have on average 0.66 for LoRA, while the other adaptations reach scores of more than 0.9.

These results confirm the concerns raised by , highlighting the risk of privacy leakage even under the application of DP with \(=8\), usually considered protective in prior literature .

Lastly, we compare the development of AUC scores during training on IID and overlap data, as shown in Figure 3. Similar to Figure 2, these results display the AUC score at each epoch during training. To better compare IID and overlap data, we adjust the x-axis to represent the loss difference at each training step, calculated as the initial pretraining loss minus the adapted loss at the current training step. This calibration of the x-axis allows us to compare the two dataset types more precisely. With this setup, we evaluate two subsets of the Pile pretraining set: GitHub and BookCorpus2. First, the figures indicate that further adapting a model on IID data does not significantly improve its performance on that data, with the loss decreasing by only a maximum of 0.015 (GitHub with Full fine-tune). However, the observed increase in AUC score throughout training shows that the model does learn from the adaptation data.

Additionally, we do not observe any significant difference in privacy leakage or its progression during training between adaptation on the overlap and IID data. A small difference between overlap and IID suggests that dataset inference on the pretraining data, which is currently known to be a complex problem on LLMs pretraining data , cannot be easily resolved by analyzing the fine-tuning trajectory loss or the privacy leakage of the fine-tuned data alone.

## 5 Discussion

In the following, we discuss the implications of our findings and the way and challenges towards holistic privacy auditing under the pretrain-adapt paradigm.

### _Implications of our Findings_

Our evaluation highlights a critical trade-off between utility and privacy across different adaptation methods. Despite achieving lower leakage in OOD settings, even the best-performing adaptations like LoRA show vulnerabilities in the scenario of utilizing shadow models with RMIA. Consequently, **privacy adaptations that regard public pretraining data as entirely non-sensitive may unintentionally integrate sensitive information into the models, even when adapted with DP**. This highlights the necessity to perform private LLM adaptations in the high-privacy regime, _i.e.,_ with low \(\).

Fig. 3: **Overlap (Train) and IID data (Val) show the same amount of privacy leakage across training. The x-axis shows the difference between the initial pretrained loss and the evaluation loss. The y-axis represents the AUC score. All adaptations have been trained with \(=8\).**

### _Towards a Holistic Privacy Auditing for LLMs_

In this work, we focus solely on auditing the private adaptations and leakage from pretraining data after adaptations, corresponding to stages (2) and (4) in our taxonomy, respectively. However, for holistic privacy auditing under the pretrain-adapt paradigm, we need ways to audit all stages of the process (jointly).

Given its correspondence to standard ML training--in the sense that it starts from training a randomly initialized model on a given dataset--prior research has attempted to audit pretraining with the same means as used to audit standard ML, namely MIAs, however, with no access to shadow models. Previous work has shown that these new MIAs for LLMs  are ineffective for pretraining data . The problem stems from the large pertaining sets (with trillions of tokens) and single training rounds that weaken the membership signal to minimum . The standard MIAs  are also not effective in this case. They require training many additional shadow models of similar architecture to the audited model, which are impractical to train for large models like LLMs due to their size. However, LLMs do memorize some of their training data points . Therefore,  proposed an alternative method to membership inference for LLMs based on the established framework of dataset inference . They leveraged the selective combination of many features from many MIAs and aggregated the signals across hundreds or more data points. Future work might leverage these insights to propose better auditing methods for pretraining.

The joint audit of pretraining and adaptation privacy, corresponding to (3) in our taxonomy, represents the largest challenge in practical setups. A main difficulty results from the complex dependencies between pretraining and adaptation data. The problem is exacerbated by the fact that for most LLMs, the pretraining data is either unknown or too large to effectively search through. Thus, advancing effective privacy auditing across all stages of the pretrain-adapt paradigm remains essential to truly privacy-preserving LLMs.

## 6 Conclusions

In this work, we examined the practical privacy risks that arise under "private" adaptations of LLMs within the pretrain-adapt paradigm. We systematically characterized the privacy auditing setup required for this paradigm, identifying four distinct stages: (1) pretraining audits, (2) adaptation audits, (3) joint pretraining and adaptation audits, and (4) post-adaptation audits of pretraining. To enable structured and rigorous privacy audits, we redefined the membership inference game for each stage and instantiated multiple membership inference attacks to assess privacy leakage. Our empirical analysis confirms the theoretical concern that pretraining significantly affects the privacy risks of _adaptation data_. We found that the closeness of adaptation and pretraining data distributions plays a critical role: even in the absence of overlap, higher distributional similarity results in increased privacy leakage. Additionally, we observed that the choice of adaptation method impacts privacy leakage, with PEFT methods, such as LoRA, offering significantly lower privacy risks while maintaining strong utility. When adapting on OOD data, these methods provide the best empirical privacy protection. Furthermore, prefix tuning can reduce the leakage of pretraining data, likely due to the added input noise during private adaptation. Our findings highlight the need for stringent DP constraints (_e.g.,_\(<0.1\)) to effectively mitigate privacy risks in LLM adaptations. By providing a comprehensive framework for privacy auditing and uncovering key factors influencing leakage, this work lays a foundation for future research aimed at safeguarding privacy in the pretrain-adapt paradigm.