# On the Need for a Language Describing Distribution Shifts: Illustrations on Tabular Datasets

Jiashuo Liu\({}^{1,}\)1, Tianyu Wang\({}^{2,}\)2, Peng Cui\({}^{1}\), Hongseok Namkoong\({}^{3}\)

\({}^{1}\)Department of Computer Science and Technology, Tsinghua University

\({}^{2}\)Department of Industrial Engineering and Operations Research, Columbia University

\({}^{3}\)Decision, Risk, and Operations Division, Columbia Business School

liujiashuo77@gmail.com, tw2837@columbia.edu

cuip@tsinghua.edu.cn, namkoong@gsb.columbia.edu

Equal contribution

###### Abstract

Different distribution shifts require different algorithmic and operational interventions. Methodological research must be grounded by the specific shifts they address. Although nascent benchmarks provide a promising empirical foundation, they _implicitly_ focus on covariate shifts, and the validity of empirical findings depends on the type of shift, e.g., previous observations on algorithmic performance can fail to be valid when the \(Y|X\) distribution changes. We conduct a thorough investigation of natural shifts in 5 tabular datasets over 86,000 model configurations, and find that \(Y|X\)-shifts are most prevalent. To encourage researchers to develop a refined language for distribution shifts, we build WhyShift, an empirical testbed of curated real-world shifts where we characterize the type of shift we benchmark performance over. Since \(Y|X\)-shifts are prevalent in tabular settings, we _identify covariate regions_ that suffer the biggest \(Y|X\)-shifts and discuss implications for algorithmic and data-based interventions. Our testbed highlights the importance of future research that builds an understanding of why distributions differ.2

## 1 Introduction

The performance of predictive models has been observed to degrade under distribution shifts in a wide range of applications, such as healthcare [9; 95; 76; 93], economics [36; 25], education , vision [74; 63; 86; 98], and language [62; 7]. Distribution shifts vary in type, typically defined as either a change in the marginal distribution of the covariates (\(X\)-shifts) or the conditional relationship between the outcome and covariate (\(Y|X\)-shifts). Real-world scenarios comprise of both types of shifts. In computer vision [62; 47; 81; 38; 101], \(Y|X\)-shifts are less likely to occur as \(Y\) is constructed from human knowledge given an input \(X\), unless the labeling noise is severe. For tabular datasets, \(Y|X\)-shifts can be more common because of missing variables and hidden confounders. For example, the prevalence of a disease may be affected by unrecorded covariates whose distribution changes across domains, such as lifestyle factors and socioeconomic status [39; 103; 93].

Different types of distribution shifts require different solutions. When facing \(X\)-shifts, the implicit goal of many researchers is to develop a single robust model that can be generalized effectively across multiple domains. Various algorithms have been developed to align the marginal distributions (\(P_{X}\)), including domain adaptation and importance sampling methods. However, under \(Y|X\)-shifts, there may be a fundamental trade-off between learning algorithms: to perform well on a target distribution, a model may have to _necessarily_ perform worse on others. Algorithmically, typical methods foraddressing \(Y|X\)-shifts include distributionally robust optimization (DRO)  and causal learning methods . Operationally, the modeler can identify and collect an unobserved confounder \(C\) such that \(Y|X,C\) remains invariant across domains, or resort to overhauling the entire model development pipeline to collect more samples from the target.

However, existing distribution shift benchmarks only focus on \(X\)-shifts . To illustrate this concretely, consider popular tabular datasets used to benchmark model performance over demographic subgroups: Adult, BRFSS, COMPAS, ACS Public Coverage, and ACS Income. We take the largest demographic group as training \(P\) and the smallest as target \(Q\) to simulate subgroup shifts, e.g., in Adult, \(P=\)white men and \(Q=\)non-white women. We measure the optimality gap of the model \(f_{P}\) trained on \(P\) as measured on the target \(Q\) using the _relative regret_

\[_{Q}[(Y,f_{P}(X))]}{_{f F}_{Q}[(Y,f( X))]}-1,\ \ \ \ \ \ f_{P}*{argmin}_{f}_{P}[(Y,f(X))]\] (1.1)

and \((,)\) is the 0-1 loss. For these widely-used benchmarks, the relative regret is small (left 5 bars in Figure 1), suggesting the \(Y|X\) distribution is _largely transferable_ across those demographic groups.

To study diverse distribution shift patterns, we consider 5 real-world tabular datasets constructed from the US Census (as proposed by Ding _et al._) and traffic measurements . We focus on spatiotemporal shifts to model most common natural shifts. Our full benchmark covers 22 settings (see Table 3 in Appendix D), where each setting includes one source (e.g., California) and a number of possible targets (e.g., other states). For illustration purposes, we focus on 7 settings covering 169 possible source-target pairs (see Table 2) and carefully select one target per setting to represent a wide range of \(Y|X\)-shifts (right bars in Figure 1).

We find \(Y|X\)-shifts constitute a substantial proportion of real-world distribution shifts, yet previous (unqualified) empirical findings in the literature only hold over mild \(X\)-shifts and fail to hold over \(Y|X\)-shifts (Section 2). Out of 169 source-target pairs with significant performance degradation (\(>8\) percentage points of accuracy drop), 80% of them are primarily attributed to \(Y|X\)-shifts. \(Y|X\)-shifts introduce considerable performance variations on the target distribution, leading to different relationships between in- and out-of-distribution performances across settings and datasets. This is in stark contrast to the recently observed accuracy-on-the-line phenomenon , where the in- and out-of-distribution performance have been posited to exhibit a strong linear relationship. In Figure 2, we showcase how the accuracy-on-the-line trend fails to hold when \(Y|X\)-shifts are strong. Our results imply that the standard practice of blindly evaluating performance over various shifts is only justified over \(X\)-shifts, where we expect there to be a single model that is robust across domains.

For severe \(Y|X\)-shifts, the training data may not even be informative for modeling the \(Y|X\) relationship in the target. To inform algorithmic and data-based interventions, we must understand _why_ the distribution changed. In Section 3, we illustrate the need for more methodological research that builds a deep understanding of distributional differences. As a concrete example, we show that a simple approach for identifying covariate regions with strong \(Y|X\)-shifts can suggest data-based interventions. Our case study shows it can be useful to collect target data over a particular covariate region, or features \(C\) such that the \(Y|X,C\) distribution is more stable across source and target.

In Section 4, we provide the details of our benchmark, WhyShift. We use five datasets to construct 7 spatiotemporal distribution shifts, and evaluate 22 methods over 86,000+ model configurations. We compare a broad range of algorithms including tree ensembles, DRO, imbalance, and fairness methods and summarize our key findings below.

* Rankings of model performance change over different shift patterns. As the validity of empirical findings implicitly depends on the type of shift, any methodological development must be grounded by the specific shifts it addresses.
* Tree ensemble methods are competitive, but still suffer from significant performance degradation.

Figure 1: _Relative regret (1.1) in typical benchmarks  (left 5 bars) and seven settings designed in our benchmark (right 7 bars)._ We use XGBoost \(\) here for illustration.

* DRO methods are sensitive to configurations and exhibit significant performance variations.
* Imbalance and fairness methods show similar performance with the base learner (XGBoost).
* A small validation data from the target distribution goes a long way, and more generally, non-algorithmic interventions warrant greater consideration.

## 2 Distribution Shifts in Tabular Settings

To illustrate how complex distribution shift patterns arise in tabular data. we compare 22 algorithms including tree ensemble methods, robust learning, imbalance, and fairness methods. On 5 real-world tabular datasets (ACS Income, ACS Public Coverage (ACS Pub.Cov), ACS Mobility, US Accident and Taxi), we consider the natural spatial shifts between states/cities, e.g., California to Puerto Rico. For the ACS Pub.Cov dataset, we also consider temporal shifts, e.g., from 2010 to 2017. Since all natural distribution shifts we consider are largely induced by \(Y|X\)-shifts, we construct a synthetic subgroup shift from younger people to older people in order to simulate \(X\)-shifts. Deferring a detailed summary to Section 4.1, we focus on introducing representative phenomena in this section.

In Figure 2, we present the source (in-distribution) and target (out-of-distribution) performances of 22 algorithms, each with 200 hyperparameter configurations. To understand shift patterns, we utilize the recently proposed DIstribution Shift DEcomposition (DISDE) framework  which decomposes the performance degradation into components attributed to \(Y|X\)- and \(X\)-shifts. Using the best XGBoost configuration as the baseline model for each source-target pair, we present the total performance degradation and the proportion attributed to \(Y|X\)-shifts.

Distribution shifts are predominantly \(Y|X\)-shifts in our empirical studyWe find performance degradation under natural shifts is overwhelmingly attributed to \(Y|X\)-shifts, as illustrated in the curated list in Figure 2. More generally, out of the 169 source-target pairs whose performance degradation is larger than \(8\) percentage points, 87.2% of them have over 50% of the performance degradation attributed to \(Y|X\)-shifts (70.2% of them have over 60% of the gap attributed to \(Y|X\)-shifts). We conjecture that \(Y|X\)-shifts are prevalent in tabular data due to missing features. For example, in the context of income prediction, individual outcomes may change due to unobserved economic and political factors whose distribution changes over geographical locations . In contrast, in vision and language tasks, the input (e.g., pixels and words) often encapsulates most of the necessary information for predicting the outcome, making strong \(Y|X\)-shifts less likely unless the labeling noise is severe. Consequently, compared to vision and language data in domain generalization

Figure 2: Target vs. source accuracies for 22 algorithms and datasets in our benchmark. A linear fit (green line) and its corresponding \(R^{2}\) value is reported on the top left of each figure. Each blue point represents one hyperparameter configuration. **(a)-(f)**: six examples of ACS Income, ACS Mobility, Taxi, ACS Pub.Cov, US Accident datasets. **(g)**: simulated covariate shifts on on sub-sampled ACS Income dataset.

tasks, tabular data exhibits more pronounced real \(Y|X\)-shifts. Our findings highlight the importance of understanding the _cause_ of the distribution shift.

Accuracy-on-the-line fails to hold over \(Y|X\)-shiftsWe find significant variation in the relationship between source and target performance throughout all natural distribution shifts presented in Figure 2 (a)-(g). The correlation between source and target performance is relatively weak, and we tend to see poor linear fits (low \(R^{2}\)) when the bulk of the performance degradation is attributed to \(Y|X\)-shifts. That is, we see in Figures 2 (a)-(f) that the relationship between the two performances exhibits significant fluctuations across different source-target pairs. In Figure 3, we observe that performance rankings of algorithms substantially vary across different \(Y|X\)-shifts. Our finding highlights the inherent complexity associated with real distribution shifts in tabular datasets, which stands in sharp contrast to the "accuracy-on-the-line" phenomena . The varied shift patterns in tabular data highlight how empirical observations must be qualified over the range of shifts they remain valid over. This is particularly important for \(Y|X\)-shifts which introduce larger variations in the relationship between source and target performance.

Source and target performances are correlated when \(X\)-shifts dominateAcross all natural shifts we study, we find \(X\)-shifts are only prominent in temporal shifts (ACS Time dataset; Figure 2f) To better investigate the role of \(X\)-shifts, we subsample the data to artificially induce strong covariate shifts over an individual's age. Specifically, we focus on individuals from California and form two groups according to whether their age is \( 25\). The source data oversamples low age groups where 80% is drawn from the age \( 25\) group; proportions are reversed in the target data.

On this synthetic shift we construct, the DISDE  method attributes the bulk of the performance degradation to \(X\)-shifts in Figure 2g. Our finding confirms the intuition that unobserved economic factors remain relatively consistent for individuals from the same state (CA). In this synthetic example with \(X\)-shifts, we observe a relatively strong correlation between source and target performance. Moreover, the large performance degradation on these datasets suggests that existing robust learning methods are still severely affected by covariate shifts, indicating the need for future research that addresses covariate shifts in tabular data.

## 3 Case Study: Understanding Distribution Shifts Facilitates Interventions

Typical algorithmic approaches to handling practical distribution shifts aim to optimize performance over a postulated set of distribution shifts. Causal learning assumes the underlying causal structure can be learned to withstand distribution shifts , while DRO methods explicitly optimize worst-case performance over a set of distributions . Despite progress in algorithm design, there are few efforts that examine the patterns of real-world distribution shifts. It remains unclear whether the data assumptions made by algorithms hold in practice, and this _mismatch_ often leads to poor empirical performance .

Complementing the active literature on algorithmic development, we present an empirical study that underscores the practical significance of tools that provide a qualitative understanding of the shift at hand. In light of the prevalence of \(Y|X\)-shifts in tabular data, we introduce a simple yet effective approach for identifying covariate regions that suffer strong \(Y|X\)-shifts. We demonstrate our approach on the income prediction task (ACS Income), and show that it can guide operational interventions for addressing distribution shifts. Our case study is not meant to be a rigorous scientific analysis, but rather a (heuristic) vignette illustrating the need for future research on methodologies that can generate qualitative insights on distributional differences.

Figure 3: Performances of typical algorithms of 7 settings in our benchmark.

### Identifying Regions with Strong \(Y|x\)-shifts

Here we propose a simple yet effective method for identifying covariate regions with strong \(Y|X\)-shifts. Despite its simplicity, we demonstrate in the following subsections that our method can inspire operational and modeling interventions. Letting \((X,Y)\) be random variable supported on the space \(\), consider a model \(f:\) that predicts outcome \(Y\) from covariates \(X\) with the associated loss function \((f(X),Y)\). Given samples \((X,Y)\) drawn from the source and target distributions \(P\) and \(Q\), our goal is to _identify a region \(\) where \(P_{Y|X}\) differs a lot from \(Q_{Y|X}\)_.

Since \(P_{Y|X}\) and \(Q_{Y|X}\) are undefined outside of the support of \(P_{X}\) and \(Q_{X}\) respectively, the comparison can only be made on a subset of the _common support_. To aid comparisons on the common support, Cai _et al._ introduced a shared distribution approach. The shared distribution has high density when both \(p_{X}\) and \(q_{X}\) are high, and low density whenever either is small. Following Cai _et al._, we choose a specific _shared distribution_\(S_{X}\) over \(X\) from the likelihood ratio:

\[s_{X}(x) p_{X}(x)q_{X}(x)/(p_{X}(x)+q_{X}(x)).\] (3.1)

We provide more discussion on the choices of \(S_{X}\) and other technical details justifying the correctness of \(s_{X}\) in Appendix C.3. Since we do not have access to samples from the shared distribution \(S_{X}\), we reweight samples from \(P_{X}\) and \(Q_{X}\) using the likelihood ratios \(s_{X}(x)/p_{X}(x) q_{X}(x)/(p_{X}(x)+q_{X}(x))\) and \(s_{X}(x)/q_{X}(x) p_{X}(x)/(p_{X}(x)+q_{X}(x))\). The ratio can be modeled as the probability that an input \(x\) is from \(P_{X}\) vs \(Q_{X}\). Denote \(^{*}\) as the proportion of the pooled data that comes from \(Q_{X}\) and \(^{*}(x):=(\) from \(Q_{X}|=x)\), we can express the likelihood ratios as:

\[}{p_{X}}(x) (x)}{(1-^{*})^{*}(x)+^{*}(1- ^{*}(x))}=:w_{P}(^{*}(x),^{*}),\] (3.2) \[}{q_{X}}(x) (x)}{(1-^{*})^{*}(x)+^{*}(1- ^{*}(x))}=:w_{Q}(^{*}(x),^{*}).\] (3.3)

With the likelihood ratios, we estimate the best prediction model under \(P\) and \(Q\) over the shared distribution \(S_{X}\) (using XGBoost as the model class \(\)):

\[f_{}:= _{f}\{_{S_{X}}[_{}[(f(X),Y)|X]](=_{}[(f(X),Y)w_{ }(^{*}(x),^{*})])\},=P,Q.\] (3.4)

Then, for any threshold \(b\), \(\{x:|f_{P}(x)-f_{Q}(x)| b\}\) suggests a region that may suffer model performance degradation with at least \(b\) due to \(Y|X\)-shifts. Without evaluating the performance on the shared distribution \(S_{X}\), it is hard to distinguish the source of the model performance degradation, i.e. from \(X\)-shifts or \(Y|X\)-shifts.

Empirically, given samples \(\{(x_{i}^{P},y_{i}^{P})\}_{i[n_{P}]}\) from \(P\) and \(\{(x_{j}^{Q},y_{j}^{Q})\}_{j[n_{Q}]}\) from \(Q\), we estimate \(=}{n_{P}+n_{Q}}\) and then train a binary "domain" classifier \((x)\) to approximate the ratio \(^{*}(x)\). Note that the "domain" classifier can be any black-box method, and we use XGBoost throughout. Then we plug these empirical estimands in to obtain the estimated likelihood ratios \(w_{}((x),)\) and learn prediction models \(f_{P}\) and \(f_{Q}\) in Equation (3.4). To investigate the model difference under \(S_{X}\), we pool samples from \(P\) and \(Q\) together and set sample weights as:

\[_{i}^{P}=((x_{i}^{P}),)}{_{k[n_{P }]}w_{P}((x_{k}^{P}),)} i[n_{P}], _{j}^{Q}=((x_{j}^{Q}),)}{_{k[n_{ Q}]}w_{Q}((x_{k}^{Q}),)} j[n_{Q}],\] (3.5)

which are used to learn a prediction model \(h(x)\) to approximate \(|f_{P}(x)-f_{Q}(x)|\) on the _shared distribution_\(S_{X}\). The pseudo-code is summarized in the Algorithm 1; To allow simple interpretation and efficient region identification, we use a shallow _decision tree_\(h(x)\) and consider the region \(\) corresponding to the feature range of a leaf node within the tree. More details could be found in Appendix C.5 and Appendix C.6. We show that the node splitting criterion in a standard decision trees training procedure is equivalent with our goal of finding regions with the largest discrepancy in Appendix C.4.

### Data-based Interventions

Using Algorithm 1, we now demonstrate how a better understanding of distribution shifts can facilitate the design of interventions. We focus on the ACS Income dataset where the goal is to predict whetheran individual's income exceeds 50k (\(Y\)) based on their tabular census data (\(X\)). We train an income classifier on 20,000 samples from California (CA, source), and deploy the classifier in Puerto Rico and South Dakota (PR & SD, target), where we get 4,000 samples from PR and SD after deployment. Given the considerable disparities in the economy, job markets, and cost of living between CA and PR/SD, we observe substantial performance degradation due to distribution shifts.

In Figure 3(a), we first decompose the performance degradation from CA to PR to understand the shift and find \(Y|X\)-shifts are the predominant factor. The calculation of \(X\)-shifts and \(Y|X\)-shifts is deferred to Appendix C.1. We dive deeper into the significant \(Y|X\)-shifts and identify from CA to PR for the XGBoost and MLP classifier. From the region shown in Figure 3(c) and Figure 3(d), we find college-educated individuals in business and educational roles (such as management, business, and educational work) exhibit large \(Y|X\) differences.

To illustrate how our analysis can inspire subsequent operational interventions to enhance performance on the target distribution, we study two operational interventions.

**Collect specific data from the target**  To improve target performance, the most natural operational intervention is to collect additional data from the target distribution. While a rich body of work on domain adaptation  study how to effectively utilize data from the target distribution to improve performance, there is little work that discusses how to efficiently collect supervised data from the target distribution to maximize out-of-distribution generalization. To highlight the need for future research in this space, we use the interpretable region identified by Algorithm 1 as shown in Figure 3(c) to simulate a concerted data collection effort.

Since indiscriminately collecting data from the target distribution can be resource-intensive, we concentrate sampling efforts on the subpopulation that may suffer from \(Y|X\)-shifts and selectively gather data on them. For five base methods (logistic regression, MLP, random forest, lightGBM, and XGBoost), we randomly sample 250 points from the whole target distribution and the identified region suffering prominent \(Y|X\)-shifts, respectively. We report the test accuracies in Figure 3(e), and observe that incorporating data from this region is more effective in enhancing OOD generalization. While preliminary, our results demonstrate the potential robustness benefits of efficiently allocating resources toward concerted data collection. Future methodological research in this direction may be fruitful; potential connections may exist with active learning algorithms .

**Add more relevant features** We now illustrate the potential benefits of generating qualitative insights on the distribution shift at hand. Our analysis in Figure 3(c) suggests educated individuals in financial, educational, and legal professions tend to experience large \(Y|X\)-shifts from CA to PR. These roles typically need communication skills, and language barriers could potentially affect their incomes. In California (CA), English is the primary language, while in Puerto Rico (PR), despite both English and Spanish being recognized as official languages, Spanish is predominantly spoken. Consequently, for a model trained on CA data and tested on PR data, incorporating a new feature that denotes English language proficiency (hereafter denoted "ENG") might prove beneficial in improving generalization performances. However, this feature is not included in the ACS Income dataset.

To address this, we went back to the Census Bureau's American Community Survey database to include the ENG feature in the set of covariates. In Figure 3(b), we observe that the inclusion of this feature substantially reduces the degradation due to \(Y|X\)-shifts, verifying that the originally missing ENG feature may be one cause of \(Y|X\)-shifts. Figure 4f contrasts the performances of 22 algorithms (each with 200 hyperparameter configurations) with original features with those that additionally use the ENG feature. The new feature significantly improves target performances across all algorithms; roughly speaking, we posit that we have identified a variable \(C\) such that \(Y|X,C\) remains similar across CA and PR. However, when we extend this comparison to the source-target pair (CA \(\) SD), we observe no significant improvement (Figure 4g). This highlights that the selection of new features should be undertaken judiciously depending on the target distributions of interest. A feature that proves effective in one target distribution might not yield similar results in another.

  \#ID & Dataset & Type & \#Samples & \#Features & Outcome & \#Domains & Selected Settings & Shift Patterns \\ 
1 & _kCS Income_ & Natural & 1,599,229 & 9 & Income\(>\)50k & 51 & California \(\) Puerto Rico & \(Y|X>X\) \\
2 & _kCS Mobility_ & Natural & 620,937 & 21 & Residential Address & 51 & Mississippi \(\) Hawaii & \(Y|X>X\) \\
3 & Taxi & Natural & 1,567,769 & 7 & Duration time\(>\) 30 min & 4 & New York City\(\) Boston & \(Y|X>X\) \\
4 & _kCS Pub.Cov_ & Natural & 1,127,446 & 18 & Public Ins. Coverage & 51 & Nebraska \(\) Louisiana & \(Y|X>X\) \\
5 & US Accident & Natural & 297,132 & 47 & Severity of Accident & 14 & California\(\) Oregon & \(Y|X>X\) \\
6 & _kCS Pub.Cov_ & Natural & 895,632 & 18 & Public Ins. Coverage & 4 & 2010 (NY)\(\) 2017 (NY) & \(Y|X<X\) \\
7 & _kCS Income_ & Synthetic & 195,665 & 9 & Income\(>\)50k & 2 & Younger\(\) Older & \(Y|X<X\) \\  

Table 1: Overview of datasets and 7 selected settings.

Figure 4: Case study illustrations. **(a)-(b)** Decomposition of performance degradation for the XGBoost classifier from CA to PR. Figure (a) is for the original setting and (b) corresponds to the results post-integration of the “ENG” feature. **(c)-(d)** Demonstration of Algorithm 1: an interpretable version of the region with strong \(Y|X\)-shifts for the XGBoost and MLP models, respectively. **(e)** Test accuracies of five typical base methods trained on the source, post addition of 250 randomly selected _target observations_, and 250 observations from the identified _risk region_. **(f)-(g)** Performances of all algorithms prior to and following the addition of the “ENG” feature. Figure (f) corresponds to the CA to PR, and Figure (g) is CA to SD.

## 4 WhyShift: Benchmarking Distribution Shifts on Tabular Data

In this section, we detail our benchmark and summarize the main observations. Our finding highlights the importance of future research that builds an understanding of _why_ the distribution has shifted.

### Setup

DatasetsWe explore distribution shifts on 5 real-world tabular datasets from the economic and traffic sectors with _natural_ spatiotemporal distribution shifts. For economic data, we use ACS Income, ACS Mobility, and ACS Public Coverage datasets from the US-wide ACS PUMS data , where the outcome is whether an individual's income exceeds 50k, whether an individual changed the residential address one year ago, and whether an individual is covered by public health insurance, respectively. We primarily focus on spatial shifts across different states in the US. To complement spatial shifts, we derive an ACS Time task based on the ACS Public Coverage dataset, where there are temporal shifts between different years (2010 to 2021). For traffic data, we use US Accident and Taxi, where the outcome is whether an accident is severe and whether the total ride duration time exceeds 30 minutes, respectively. We focus on spatial shifts between different states/cities. We summarize the datasets in Table 1 and defer a full description to the Appendix D.1.

AlgorithmsWe evaluate **22** algorithms that span a wide range of learning strategies on tabular data, and compare their performances under different patterns of distribution shifts we construct. Concretely, these algorithms include: (1) _base learners_: Logistic Regression, SVM, fully-connected neural networks (MLP) with standard ERM optimization; (2) _tree ensemble models_: Random Forest, XGBoost, LightGBM; (3) _robust learning_: CVaR-DRO and \(^{2}\)-DRO with fast implementation , CVaR-DRO and \(^{2}\)-DRO of outlier-robust enhancement , Group DRO ; (4) _imbalanced learning_: JTT , SUBY, RWY, SUBG, RWG , DWR  and (5) _fairness-enhancing methods_: inprocessing method  with demographic parity, equal opportunity, error parity as constraints, postprocessing method  with exponential and threshold controls. For DRO methods (i.e. (3)), we use MLP as the backbone model. For other algorithms compatible with tree ensemble models (i.e. (4-5)), we use the XGBoost model due to its superior performance on tabular data . For algorithms requiring group labels, we use 'hour' for US Accident and Taxi, and'sex' for the others. Detailed descriptions for each algorithm can be found in Appendix D.5.

BenchmarksWe conduct experiments with more than 86,000 model configurations on various source-target distribution shift pairs, and carefully select _7 selected pairs with different distribution shift patterns_. In Table 1, we characterize the shift patterns of these 7 source-target pairs, which contain different proportions of \(Y|X\)-shifts and \(X\)-shifts corresponding with plots in Figure 2. The first six settings are natural shifts. In the last setting, we sub-sample the dataset according to age to introduce covariate shift, where we focus on individuals from CA and form two groups according to whether their age is \( 25\). The source data over-samples the low age group where 80% is drawn from the group where the individual's age \( 25\), and the proportions are reversed in the target data.

In Figure 5 and Figure 6, we plot the performance of algorithms using their best hyperparameter configuration on the validation dataset (\(i.i.d.\) with the source distribution). Additional results with various source distributions are in the Appendix. Our benchmark is designed to support empirical research, including new learning algorithms and diagnostics that provide qualitative insights on distribution shifts.

Figure 5: Overall performances of all algorithms on the target data in our selected 7 settings.

**Hyper-parameter Tuning** For each model, we conduct a grid search over a large set of hyper-parameters. See Appendix D.3 for the complete search space for each method. When one method includes another as a "base" learner (e.g., DRO with MLP, RWY with XGBoost), we explore the full tuning space for the base model (e.g., the cross-product of all MLP hyper-parameters with all DRO hyper-parameters). To control for computational effort, each method is run with 200 configurations for each source-target pair and we select the best configuration according to the \(i.i.d.\) validation performance. In Figure 6(b), we further compare different choices of validation protocols.

**Evaluation Metrics** In our benchmark, we include different metrics for a thorough evaluation. Specifically, we use Average Accuracy (micro-average), Worst-group Accuracy, and Macro-F1 score in our main results where we only have one target distribution. For the results with multiple target distributions (i.e. 3 in Taxi, 13 in US Accident and 50 in the others), we present _all_ target accuracies and Macro-F1 scores, as well as the worst-distribution accuracy and Macro-F1 score among all target distributions in Appendix D.6, D.7, D.8, D.9.

### Analysis

**Different algorithms do not exhibit consistent rankings over different shift patterns.** In Figure 5, we observe the rankings across different shifts are quite different, especially for ACS Income (CA\(\)PR) and ACS Mobility (MS\(\)HI) where \(Y\)\(\)\(X\)-shifts dominate. This observation reaffirms the phenomena in Figure 2 that as \(Y\)\(\)\(X\)-shifts become stronger, the relationship between source vs target performances becomes less consistent. In Appendix D.3, we also show that even for a fixed source distribution in one fixed prediction task, algorithmic rankings of performances on different target distributions vary a lot.

**Tree ensemble methods show competitive performance, but do not significantly improve the generalization drop between source and target data.** From Figure 5, tree-based ensembles (yellow bars) show robust and competitive performance on the target distribution in 6 out of 7 settings. However, in Figure 6 which plots the performance degradation between source and target, tree ensembles do not show improved robustness. This suggests that they do _not_ actually achieve better robustness against real-world distribution shifts, and their better performances on target data may simply be due to better fitting the source distribution.

**DRO methods are sensitive to configurations, with rankings varying significantly across 7 different settings.** From Figure 5, DRO methods exhibit competitive performances on ACS Mobility (MS\(\)HI), Taxi (NYC\(\)BOG), and ACS Income (Young\(\)Old), yet underperform in others. This sensitivity to configurations, as shown in Figure 6(a) (red points), could be attributed to

Figure 6: Performance drop between source and target data of all algorithms in our selected 7 settings.

Figure 7: **(a)**: Sensitivity of DRO methods and Imbalance Methods w.r.t. configurations. **(b)** Target performances of 22 algorithms under different validation protocols on ACS Income (CA\(\)PR) setting.

the worst-case optimization that perturbs the training distribution within a pre-defined uncertainty set, without any information regarding the target distribution. However, when target information is incorporated for hyper-parameter tuning, as shown in Figure 6(b), there is a notable improvement in the performance of DRO methods. Our observations suggest potential avenues for building more refined uncertainty sets in DRO methods.

**Imbalance methods and fairness methods show similar performance with the base learner (XGBoost).** In our experiments, we choose the XGBoost model as the base learner for imbalance and fairness methods due to its superior performance on tabular data . However, from Figure 5 and Figure 6, imbalance methods and fairness methods do not show a clear improvement upon their base learner (XGBoost, last yellow bar). Further, as shown in Figure 6(a), imbalance methods (green) are also quite sensitive to configurations, and their performances do not improve much when their hyperparameters are tuned over the target data (Oracle).

**Target information matters in validation.** Based on the ACS Income (CA\(\)PR) dataset, we compare different validation protocols, including the best average accuracy, minimum subgroup discrepancy, and best worst-subgroup accuracy on validation data generated from the _source_ distribution. We also use the Oracle validation that chooses the configuration with the best average accuracy on validation data generated from the _target_ distribution. In Figure 6(b), we find the first three protocols do not show a significant difference. However, oracle validation with target information substantially improves the effectiveness of both DRO and tree ensemble methods. We conclude using target information for model selection can provide robustness gains even with a small target dataset.

**Non-algorithmic interventions warrant greater consideration.** Reflecting on Section 3, it is clear that operational interventions yield significant enhancements for various methods, as demonstrated in Figure 3(e) and Figure 3(f). In comparison to algorithmic interventions, such as designing different algorithms (e.g., DRO, Imbalance methods), a data-centric approach can be more effective in addressing distribution shifts. For instance, research on feature collection and feature engineering methods may prove impactful. Another avenue for future work is developing methods that can optimally incorporate expensive samples from the target distribution.

## 5 Discussion

We explore the complexity of distribution shifts in real-world tabular datasets in depth. Using natural shifts from 5 real-world tabular datasets across different domains, we specify each shift pattern and evaluate 22 methods via experiments with over 86k trained models. Our benchmark WhyShift encompasses various distribution shift patterns to evaluate the robustness of the methods. We propose a simple but effective algorithm to identify regions with large \(Y|X\)-shifts, and through a comprehensive case study, we demonstrate how a better understanding of distribution shifts facilitates algorithmic and data-based interventions. Our findings highlight the importance of future research to understand how and why distributions differ in real-world applications.

Our study leaves many open directions for improvements in future work. Our benchmark only includes tabular datasets from the economic and transportation domain. Considering datasets from other domains such as the medicine or those involving feature embeddings may highlight different types of distribution shift. On the algorithmic side, our region-identification algorithm requires some target data to identify risky regions and cannot be used in cases where the target distribution is completely unknown. Furthermore, targeted data collection on regions of \(Y|X\)-shifts may be pose ethical and privacy concerns for marginalized groups. We provide more discussion in Appendix B.