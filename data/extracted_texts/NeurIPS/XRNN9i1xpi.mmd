# Continuous Product Graph Neural Networks

Aref Einzade

LTCI, Telecom Paris

Institut Polytechnique de Paris

aref.einzade@telecom-paris.fr

&Fragkiskos D. Malliaros

CentraleSupelec, Inria

Universite Paris-Saclay

fragkiskos.malliaros@centralesupelec.fr

&Jhony H. Giraldo

LTCI, Telecom Paris

Institut Polytechnique de Paris

jhony.giraldo@telecom-paris.fr

###### Abstract

Processing multidomain data defined on multiple graphs holds significant potential in various practical applications in computer science. However, current methods are mostly limited to discrete graph filtering operations. Tensorial partial differential equations on graphs (TPDEGs) provide a principled framework for modeling structured data across multiple interacting graphs, addressing the limitations of the existing discrete methodologies. In this paper, we introduce Continuous Product Graph Neural Networks (CITRUS) that emerge as a natural solution to the TPDEG. CITRUS leverages the separability of continuous heat kernels from Cartesian graph products to efficiently implement graph spectral decomposition. We conduct thorough theoretical analyses of the stability and over-smoothing properties of CITRUS in response to domain-specific graph perturbations and graph spectra effects on the performance. We evaluate CITRUS on well-known traffic and weather spatiotemporal forecasting datasets, demonstrating superior performance over existing approaches. The implementation codes are available at https://github.com/ArefEinzade2/CITRUS.

## 1 Introduction

Multidomain (tensorial) data defined on multiple interacting graphs , referred to as multidomain graph data in this paper, extend the traditional graph machine learning paradigm, which typically deals with single graphs . Tensors, which are multi-dimensional generalizations of matrices (order-2 tensors), appear in various fields like hyperspectral image processing , video processing , recommendation systems , spatiotemporal analysis , and brain signal processing . Despite the importance of these applications, learning from multidomain graph data has received little attention in the existing literature . Therefore, developing graph-learning strategies for these tensorial data structures holds significant promise for various practical applications.

The main challenge for learning from multidomain graph data is creating efficient frameworks that model _joint_ interactions across domain-specific graphs . Previous work in this area has utilized discrete graph filtering operations in product graphs (PGs)  from the field of graph signal processing (GSP) . However, these methods inherit the well-known issues of over-smoothing and over-squashing from regular graph neural networks (GNNs) , which restricts the graph's receptive field and hinders long-range interactions . Additionally, these methods often require computationally intensive grid searches to tune hyperparameters and are typically limited to two-domain graph data, such as spatial and temporal dimensions .

In regular non-tensorial GNNs, continuous GNNs (CGNNs) emerge as a solution to over-smoothing and over-squashing . CGNNs are neural network solutions to partial differential equations (PDEs) on graphs , like the heat or wave equation. The solution to these PDEs introduces the exponential graph filter as a continuous infinity-order generalization of the typical discrete graph convolutional filters . Due to the differentiability of exponential graph filters w.r.t. the graph receptive fields, CGNNs can benefit from both global and local message passing by adaptively learning graph neighborhoods, alleviating the limitations of discrete GNNs . Despite these advantages, CGNNs mostly rely on a _single_ graph and lack a principled framework for learning _joint_ multi-graph interactions.

In this paper, we introduce tensorial PDE on graphs (TPDEGs) to address the limitations of existing PG-based GNN and CGNN frameworks. TPDEGs provide a principled framework for modeling multidomain data residing on multiple interacting graphs that form a Cartesian product graph heat kernel. We then propose **C**ontinuous Product **G**raph **Ne**ural Networks (CITRUS) as a continuous solution to TPDEGs. We efficiently implement CITRUS using a small subset of eigenvalue decompositions (EVDs) from the factor graphs. Additionally, we conduct theoretical and experimental analyses of the stability and over-smoothing properties of CITRUS. We evaluate our proposed model on spatiotemporal forecasting tasks, demonstrating that CITRUS achieves state-of-the-art performance compared to previous Temporal-Then-Spatial (TTS), Spatial-Then-Temporal (STT), and Temporal-and-Spatial (T&S) frameworks. Our main contributions are summarized as follows:

* We introduce TPDEGs to handle multidomain graph data. This leads to our proposal of a continuous graph filtering operation in Cartesian product spaces, called CITRUS, which naturally emerges as the solution to TPDEGs.
* We conduct extensive theoretical and experimental analyses to evaluate the stability and over-smoothing properties of CITRUS.
* We test CITRUS on spatiotemporal forecasting tasks, demonstrating that our model achieves state-of-the-art performance.

## 2 Related Work

We divide the related work into two parts covering the _fundamental_ and _applicability_ aspects.

**Fundamentals.** The study of PGs is a well-established field in mathematics and signal processing . However, to the best of our knowledge, the graph-time convolutional neural network (GTCNN)  model is the only GNN-based framework addressing neural networks in PGs. GTCNN defines a discrete filtering operation in PGs to jointly process spatial and temporal data for forecasting applications. Due to the discrete nature of the graph polynomial filters in GTCNN, an expensive grid search is required to correctly tune the graph filter orders, leading to significant computational overhead. Additionally, this discrete aspect restricts node-wise receptive fields, making long-range communication challenging . Furthermore, GTCNN is limited to two factor graphs (space and time), limiting its applicability to general multidomain graph data. In contrast, CITRUS overcomes these limitations by i) employing continuous graph filtering, and ii) providing a general framework for any number of factor graphs.

**Applicability.** We explore spatiotemporal analysis as a specific application of PG processing. Early works in GSP for spatiotemporal forecasting, such as the graph polynomial vector auto-regressive moving average (GP-VARMA) and GP-VAR models , do not utilize PGs. Modern GNN-based architectures can be broadly classified into TTS and STT frameworks. TTS networks, due to their sequential nature, often encounter propagation information bottlenecks during training . Conversely, STT architectures enhance node representations first and then integrate temporal information, but the use of recurrent neural networks (RNNs) in the aggregation stages leads to high computational complexity. Additionally, there are T&S frameworks designed for _simultaneous_ learning of spatiotemporal representations , where the fusion of temporal and spatial modules is crucial. However, similar to STT networks, T&S frameworks also face challenges with computational complexity  and their discretized block-wise nature, which can limit adaptive graph receptive fields .

CITRUS leverages _joint_ multidomain learning by exploiting continuous separable heat kernels as factor-based functions defined on a Cartesian product graph. The continuity and differentiabilityof these filters allow the graph receptive fields to be learned adaptively during the training process, eliminating the need for a grid search. Additionally, CITRUS maintains low computational complexity by relying on the spectral decomposition of the factor graphs . Furthermore, unlike non-graph approaches, the number of learnable parameters in CITRUS is independent of the factor graphs, ensuring scalability.

## 3 Methodology and Theoretical Analysis

**Preliminaries and notation.** An undirected and weighted graph \(\) with \(N\) vertices can be stated as \(=\{,,\}\). \(\) and \(\) denote the sets of the vertices and edges, respectively. \(^{N N}\) is the adjacency matrix with \(\{_{ij}=_{ji} 0\}_{i,j=1}^{N}\) representing the connection between vertices, and \(\{_{ii}=0\}_{i=1}^{N}\) states that there are no self-loops in \(\). The graph Laplacian \(^{N N}\) is defined as \(=-\), where \(=()\) is the diagonal degree matrix and \(\) is the all-one vector. A graph signal is a function that maps a set of nodes to the real values \(x:\), and therefore we can represent it as \(=[x_{1},,x_{N}]^{}\). For a vector \(=[a_{1},,a_{N}]^{}^{N 1}\), \(e^{}:=[e^{a_{1}},,e^{a_{N}}]^{}\). The vectorization operator is shown as \((.)\). Using the Kronecker product \(\), Kronecker sum (aka Cartesian product) \(\), and the Laplacian factors \(\{_{p}^{N_{p} N_{p}}\}_{p=1}^{P}\), we have the following definitions:

\[_{p=1}^{P}_{p}:=_{P} _{1},\ _{p=1}^{P}_{p}:=_{1}_ {P},\ _{p=1}^{P}_{p}:=_{P} _{1},\] (1)

where the Laplacian matrix of the Cartesian product between two factor graphs can be stated as:

\[_{p=1}^{2}_{p}=_{1}_{2}= _{1}_{N_{2}}+_{N_{1}}_{2},\] (2)

with \(_{n}\) the identity matrix of size \(n\). See Figure 0(a) for an example of a Cartesian graph product between three factor graphs. A similar relationship to (2) also holds for adjacency matrices . We define a \(D\)-dimensional tensor as \(}^{N_{1} N_{D}}\). The matrix \(}_{(i)}^{N_{i}[_{r=1,r i}^{D} N_{r}]}\) is the \(i\)-th mode matricization of \(}\). The mode-\(i\) tensorial multiplication of a tensor \(}\) by a matrix \(^{m N_{i}}\) is denoted by \(}=}_{i}\), where \(}^{N_{1} N_{1-i} m  N_{i+1} N_{D}}\). For further information about the tensor matricization and mode-\(n\) product, please refer to  (especially Sections 2.4 and 2.5 in ). All the proofs of theorems, propositions, and lemmas of this paper are provided in the Appendix A.

### Continuous Product Graph Neural Networks (CITRUS)

We state the generative PDE for CITRUS as follows:

**Definition 3.1** (Tensorial PDE on graphs (TPDEG)).: Let \(}_{t}^{N_{1} N_{2} N_{P}}\) be a multidomain tensor whose elements are functions of time \(t\), and let \(\{_{p}^{N_{p} N_{p}}\}_{p=1}^{P}\) be the domain-specific factor Laplacians. We can define the TPDEG as follows:

\[}_{t}}{ t}=-_{p=1}^{P}}_{t}_{p}_{p}.\] (3)

Figure 1: Illustration of key concepts of CITRUS. a) Cartesian product between three-factor graphs. b) Continous product graph function (CITRUS) operating on the multidomain graph data \(}\).

**Theorem 3.2**.: _Let \(}_{0}\) be the initial value of \(}_{t}\). The solution to the TPDEG in (3) is given by:_

\[}_{t}=}_{0}_{1}e^{-t _{1}}_{2}e^{-t_{2}}_{3}_{P}e^{-t_{ P}}.\] (4)

Using Theorem 3.2, we define the core function of CITRUS as follows:

\[f(}_{l})=}_{l}_{1}e^{-t_{l} _{1}}_{2}_{P}e^{-t_{l}_{P}}_{P+1} _{l}^{},\] (5)

where \(}_{l}^{N_{1} N_{P} F _{l}}\) is the input tensor, \(_{l}^{F_{l} F_{l+1}}\) is a matrix of learnable parameters, \(t_{l}\) is the learnable graph receptive field, and \(f(}_{l})^{N_{1} N_{P}  F_{l+1}}\) is the output tensor. Figure 1b illustrates a high-level description of \(f(}_{l})\). The next proposition formulates (5) as a graph convolution defined on a product graph.

**Proposition 3.3**.: _The core function of CITRUS in (5) can be rewritten as:_

\[f(}_{l})_{(P+1)}^{}=e^{-t_{l}_{0}}[}_{l(P+1)}]^{}_{l},\] (6)

_where \(_{}:=_{p=1}^{P}_{p}\) is the Laplacian of the Cartesian product graph._

Proposition 3.3 is the main building block for implementing CITRUS. More precisely, we use the spectral decompositions of the factor graphs \(\{_{p}=_{p}_{p}_{p}^{}\}_{ p=1}^{P}\) and product graph \(\{_{}=_{}_{} _{}^{}\}\), where \(_{}=_{p=1}^{P}_{p}\) and \(_{}=_{p=1}^{P}_{p}\). Let \(K_{p} N_{p}\) be the number of selected eigenvalue-eigenvector pairs of the \(p\)-th factor Laplacian. When \(K_{p}=N_{p}\), it can be shown  that we can rewrite (6) as follows:

\[f(}_{l})_{(P+1)}^{}=_{ }^{(K_{p})}(}_{l}} _{l}}_{l}}}^{_{l}} (_{}^{(K_{p})^{}}[}_{l(P+ 1)}]^{}))_{l},\] (7)

\[}_{l}=_{p=1}^{P}e^{-t _{l}_{p}^{(K_{p})}},_{}^{(K_{p})}= _{p=1}^{P}_{p}^{(K_{p})},\] (8)

where \(_{p}^{(K_{p})}^{K_{p} 1}\) and \(_{p}^{(K_{p})}^{N_{p} K_{p}}\) are the first \(K_{p} N_{p}\) selected eigenvalues and eigenvectors of \(_{p}\) based on largest eigenvalue magnitudes, respectively, and \(\) is the point-wise multiplication operation. Finally, we can define the output of the \(l\)-th layer of CITRUS as \(}_{l+1(P+1)}=(f(}_{l})_{ (P+1)})\), where \(()\) is some proper activation function.

_Remark 3.4_.: We can consider factor-specific learnable graph receptive fields \(\{t_{l}^{(p)}\}_{p=1}^{P}\) for formulating \(}_{l}\) in (8) as \(}_{l}_{p=1}^{P}e^{-t_{l}^{(p)} _{p}^{(K_{p})}}\) to make CITRUS more flexible for each factor graph's receptive field. This technique can be expanded to the channel-wise graph receptive fields \(\{t_{l}^{(p,c)}\}_{p=1,c=1}^{P,F_{l}}\) such that the \(c\)-th column of \(}_{l}\) in (7) is obtained by \(}_{k,c}=_{p=1}^{P}e^{-t_{l}^{(p,c)} _{p}^{(K_{p})}}\).

_Remark 3.5_.: Computing the EVD in the product graph \(_{}\) in (6) has a general complexity of \(([_{p=1}^{P}N_{p}]^{3})\). However, we obtain a significant reduction in complexity of \(([_{p=1}^{P}N_{p}^{3}])\) in (7) by relying upon the properties of product graphs since we perform EVD on each factor graph independently. Besides, if we rely only on the \(K_{p}\) most important eigenvector-eigenvalue pairs  of each factor graph, we can reduce the complexity of the spectral decomposition to \((N_{p}^{2}K_{p})\), obtaining a general complexity of \(([_{p=1}^{P}N_{p}^{2}K_{p}])\).

### Stability Analysis

We study the stability of CITRUS against possible perturbations in the factor graphs. First, as defined in the literature , we model the perturbation to the \(p\)-th graph as an addition of an error matrix \(_{p}\) (with upper bound \(_{p}\) for any matrix norm \(|\!||\!|\)) to the adjacency matrix \(_{p}\) as:

\[}_{p}=_{p}+_{p};\ \ |\!| _{p}|\!|_{p}.\] (9)

**Proposition 3.6**.: _Let \(\) and \(}\) be the true and perturbed adjacency matrix of Cartesian product graphs with the perturbation model for each factor graph described as in (9). Then, it holds that:_

\[}=+;\ \ |\!||\! |_{p=1}^{P}_{p},\] (10)

_where the perturbation matrix \(\) also follows the Cartesian structure \(=_{p=1}^{P}_{p}\)._Finally, let \((u,t)\) and \((u,t)\) be the true and perturbed output of the CITRUS model with normalized Laplacian \(}_{}\) being responsible for generating \(}_{t(P+1)}\) in a heat flow PDE with normalized Laplacian \(}_{}\) as \(=-}_{}(u,t)\) in (6) , respectively. Then, the following theorem states that the integrated error bound on the stability properties of CITRUS is also separable when dealing with Cartesian product graphs:

**Theorem 3.7**.: _Consider a Cartesian product graph (without isolated nodes) with the normalized true and perturbed Laplacians \(}_{}\) and \(}_{}\) in (6). Also, assume we have the perturbation model \(}_{p}=_{p}+_{p}\) with factor error bounds \(\{\|_{p}\|_{p}\}_{p=1}^{P}\) in Proposition 3.6. Then, the stability bound on the true and perturbed outputs of CITRUS, i.e., \((u,t)\) and \((u,t)\), respectively, can be described by the summation of the factor stability bounds as:_

\[\|(u,t)-(u,t)\|=_{p=1}^{P}(_ {p}).\] (11)

Theorem 3.7 states that the solution of the TPDEG (4) is robust w.r.t. the scale of the factor graph perturbations. This is a desired property when dealing with erroneous factor graphs, and will be numerically validated in Section 4.1.

### Over-smoothing Analysis

There is a prevalent notion of describing over-smoothing in GNNs based on decaying Dirichlet energy against increasing the number of layers. This has been theoretically analyzed in related literature  and can be formulated  for the output of a continuous GNN. Precisely, it can be defined on the GNN's output \(_{t}=[(t)_{1},,(t)_{N}]^{}\), with \((t)_{i}\) being the feature vector of the \(i\)-th node with the degree deg\({}_{i}\), as:

\[_{t}E(_{t}) 0,\] (12)

\[ E():=_{(i,j)}\| _{i}}{_{i}}}-_{j}}{_{j}}}\|^{2}=(^{}}).\] (13)

Inspired by (13), we extend the definition of Dirichlet energy to the tensorial case as follows:

**Definition 3.8**.: (Tensorial Dirichlet energy). By considering the normalized factor Laplacians \(\{}_{p}\}_{p=1}^{P}\), we define the Tensorial Dirichlet energy for a tensor \(}^{N_{1} N_{P} F}\) as:

\[E(}):=_{f=1}^{F}_{p=1}^{P} (}_{f_{(p)}}^{}}_{p} }_{f_{(p)}}),\] (14)

where \(}_{f_{(p)}}=}[:,,:,f]_{(p)} ^{N_{p}_{i p}^{F}N_{i}}\), _i.e._, the \(p\)-th mode matricization of the \(f\)-th slice of \(}\) in its \((P+1)\)-th dimension.

Based on the previous research in the GSP literature , it follows that \(E(})\) in (14) can be rewritten as \(E(})=(}^{}}})\), where \(}^{[_{p=1}^{P}N_{p}] F}\), \(}[:,f]=(}[:,,:,f])\), and \(}:=_{p=1}^{P}}_{p}=_{p= 1}^{P}}_{p}}{P}\). The next lemma shows interesting and applicable properties of \(}\).

**Lemma 3.9**.: _Consider \(P\) factor graphs with normalized adjacencies \(\{}_{p}\}_{p=1}^{P}\) and Laplacians \(\{}_{p}\}_{p=1}^{P}\). By constructing the product adjacency as \(}:=_{p=1}^{P}}_{p}=_{p=1} ^{P}}_{p}}{P}\), a similar Cartesian form for the product Laplacian \(}:=-}\) (with \(^{[_{p=1}^{P}N_{p}][_{p=1}^{P}N_{p}]}\) being the identity matrix) also holds as the following:_

\[}=_{p=1}^{P}}_{p}= _{p=1}^{P}}_{p}}{P}.\] (15)

_Besides, the spectrum of \(}\) is spanned across the interval of \(\)._We observe in (14) that the kernel separability of Cartesian heat kernels also leads to the separability of analyzing over-smoothing in different modes of the data at hand. This is useful in cases of facing factor graphs with different characteristics. Next, we formally analyze over-smoothing in CITRUS. In our case, for the \(l\)-th layer with \(H_{l}\) hidden MLP layers, non-linear activations \(()\) and learnable weight matrices \(\{_{lh}\}_{h=1}^{H_{l}}\), we define:

\[_{l+1}:=f_{l}(_{l}),\ \ \ f_{l}():=_{l}(e^{- }^{(t)}}),\ \ \ _{l}():=((()_{l1} )_{l2}_{lh_{l}}),\] (16)

with \(_{0}\) being the initial node feature matrix, \(f_{l}()\) is a generalized form of (6), and \(e^{-}^{(t)}}:=_{p=1}^{P}e^{-t^{(p)} }{P}}\), where \(t^{(i)}\) is the receptive field of the \(i\)-th factor graph. Then, the next theorem describes the over-smoothing criteria.

**Theorem 3.10**.: _For the product Laplacian \(}:=_{p=1}^{P}}_{p}\) with domain-specific receptive field \(t^{(i)}\) for the \(i\)-th factor graph and the activations \(()\) in (16) being ReLU or Leaky ReLU, we have:_

\[E(_{l}) e^{l( s-}{P})}E( _{0}),\] (17)

_where \(s:=_{l_{+}}s_{l}\) and \(s_{l}:=_{h=1}^{H_{l}}s_{lh}\) with \(s_{lh}\) being the square of maximum singular value of \(_{lh}^{}\). Besides, \(=^{(m)}\) and \(=t^{(m)}\) with \(m=_{i}t^{(i)}^{(i)}\), where \(^{(i)}\) is the smallest non-zero eigenvalue of \(}_{i}\)._

**Corollary 3.11**.: _When \(l\), \(E(_{l})\) exponentially converges to \(0\), when:_

\[ s-<0.\] (18)

Theorem 3.10 shows that the factor graph with the smallest non-zero eigenvalue (spectral gap) multiplied by its receptive field dominates the overall over-smoothing. The less the spectral gap, the less probability the factor graph is connected . So, we can focus on the (much smaller) factor graphs instead of the resulting massive product graph. These theoretical findings are experimentally validated in Section 4.2.

## 4 Experimental Results

In this section, we experimentally validate the theoretical discussions regarding the stability and over-smoothing analysis and use CITRUS in real-world spatiotemporal forecasting tasks. Similarly, we conduct ablation studies regarding CITRUS in spatiotemporal forecasting. We compare CITRUS against several state-of-the-art methods in forecasting. The results on the case of more than two factor graphs and descriptions of the state-of-the-art are provided in Appendix B and C, respectively, alongside the additional theoretical and experimental discussions in Sections D-I. The implementation codes are available at https://github.com/ArefEinzade2/CITRUS.

### Experimental Stability Analysis

We generate a \(600\)-node product graph consisting of two connected Erdos-Renyi (ER) factor graphs with \(N_{1}=20\), \(N_{2}=30\), and edge probabilities \(p_{}^{(1)}=p_{}^{(2)}=0.1\) for node regression. We utilize \(15\%\) of the nodes in the product graph as the test set, \(15\%\) of the remaining nodes as validation, and the rest of the nodes for training. We use a three-layer generative model to generate the outputs based on (6). The continuous parameters for the factor graphs are set as \(t^{(1)}=2\) and \(t^{(2)}=3\). The initial product graph signal \(^{(0)}^{N F_{0}}\) is generated from the normal distribution with \(F_{0}=6\). Each layer's MLP of the generation model has only one layer with the number of hidden units \(\{5,4,2\}\). We add noise to the factor adjacency matrices with signal-to-noise ratios

Figure 2: Stability analysis vs. different SNR scenarios, related the results in Theorem 3.7.

(SNRs) (in terms of db) of \(\{,20,10,0,-10\}\). Therefore, we construct a two-layer CITRUS and use one-layer MLP for each with \(F_{1}=F_{2}=4\) to learn the representation in different SNRs to study the effect of each factor graph's noise on the stability performance of the network.

Figure 2 shows the averaged prediction error in MSE between the true and predicted outputs over \(10\) random realizations. Firstly, we notice that increasing the SNR for each factor graph improves performance, illustrating the stability properties of CITRUS. Therefore, the effect of each factor graph's stability on the overall stability is well depicted in this figure, confirming the theoretical findings in Theorem 3.7. For instance, in SNR=10 for the first graph, the performance is still severely affected by the stability for the second graph, shown by color in Figure 2. Finally, we observe that the standard deviation of the predictions is smaller for larger values of SNR, illustrating CITRUS's robustness.

### Experimental Over-smoothing Analysis

For experimentally validating Theorem 3.10, we first generate a initial graph signal \(_{0}^{N F_{0}}\) with \(F_{0}=12\) on a \(150\)-node Cartesian product graph with \(N_{1}=10\), \(N_{2}=15\). Therefore, we consider a \(10\)-layer generation process based on (6) with only one-layer MLP for each layer and \(\{F_{l}=12-l\}_{l=1}^{10}\), where the weight matrices are generated from normal distribution. We select ReLU as the activation function and \(\{t_{l}=1\}_{l=1}^{10}\).

The over-smoothing phenomenon strongly depends on the smallest spectral gap of the factor graphs as stated in Theorem 3.10. We consider two scenarios of \( s-<0\) and \( s->0\). The first scenario relates to a well-connected product graph obtained from two factor ER graphs with \(p_{}^{(1)}=0.05\) and \(p_{}^{(2)}=0.95\) and, while in the other, \(p_{}^{(1)}=p_{}^{(2)}=0.1\). So, we scale the MLP weight matrices by \(100\) and \(2.5\) to limit their maximum singular values. These settings result in \(=0.06\) and \(s=0.004 10^{-5}\) for the first scenario; and \(=0.07\) and \(s=7.691 10^{-6}\) for the other.

We depict the outputs \(y(l)=_{l})}{E(_{0})}\) and the theoretical upper bound \(y(l)=l( s_{l}-_{l})\) across the number layers \(l\) in Figure 3. We observe that in the **left** plot where \(\{ s_{l}-_{l}<0\}_{l=1}^{10}\), the theoretical upper bound approximates well the actual outputs during over-smoothing as stated in Theorem 3.10. On the opposite, on the **right** plot, in which \(\{ s_{l}-_{l}>0\}_{l=1}^{10}\), the theoretical upper bound is loose. We leave for future work finding a tighter upper bound.

### Experiments on Real-world Data

We evaluate and compare CITRUS on real-world traffic and weather spatiotemporal forecasting tasks, where we follow the settings in  for pre-processing and training-validation-testing data partitions. All hyperparameters were optimized on the validation set with the details in Section I. We use Adam as optimizer.

**Traffic forecasting.** Here, we work on two well-known datasets for spatiotemporal forecasting: MetrLA  and PemsBay. MetrLA consists of recorded traffic data for four months on \(207\) highways in Los Angeles County with \(5\)-minute resolutions . PemsBay contains \(5\)-minute traffic load data for six months associated with \(325\) stations in the Bay Area by the same setting with . The spatial and temporal graphs are created by applying Gaussian kernels on the node distances  and simple path graphs, respectively. The task for these datasets is, by having the last \(30\) minutes (\(T=6\) steps) of traffic recordings, we need to predict the following \(15\)-\(30\)-\(60\) minutes, _i.e._, \(H=3\), \(H=6\), and \(H=12\) steps ahead horizons. Our model first uses a linear encoder network, followed by three CITRUS blocks with \(3\)-layer MLPs for each, where \(\{F_{l}^{}=64\}_{l=1}^{3}\). Therefore, the output node embeddings are concatenated with the initial spatiotemporal data. Finally, we use a linear decoding layer to transform the learned representations to the number of needed horizons. We exploit residual connections within each CITRUS block to stabilize the training process . We use the mean absolute error (MAE) as the loss function and consider a maximum of \(300\) epochs. The best model on the validation set is applied to the unseen test set. We compare CITRUS against previous methods using MAE, mean absolute percentage error (MAPE), and root mean squared error (RMSE).

Table 1 presents the forecasting results on the MetrLA and PemsBay datasets. We observe that, given the abundant training data in these datasets, the NN-based methods outperform the classic GSP-based methods like ARIMA , G-VARMA , and GP-VAR . Similarly, the GNN-based models are superior compared to non-graph algorithms like FC-LSTM. More importantly, CITRUS outperforms state-of-the-art baselines in most metrics, especially in larger numbers of horizons (\(H>3\)).

**Weather forecasting.** We also test CITRUS for weather forecasting in the Molene and NOAA datasets. The Molene dataset provides recorded temperature measurements for \(744\) hourly intervals over \(32\) measurement stations in a region in France. For the NOAA, which is associated with regions in the U.S., the temperature measurements were recorded for \(8,579\) hourly intervals over \(109\) stations. The pre-processing and data curation settings were similar to prior works on these datasets . The task here is by having the last \(10\) hours of measurements, we should predict the next \(\{1:5\}\) hours. For the Molene dataset, the embedding dimension and the initial linear layer of

   } &  \\   &  &  &  \\   & MAE & MAPE & RMSE & MAE & MAPE & RMSE & MAE & MAPE & RMSE \\  ARIMA  & 3.99 & 9.60\% & 8.21 & 5.15 & 12.70\% & 10.45 & 6.90 & 17.40\% & 13.23 \\ G-VARMA  & 3.60 & 9.62\% & 6.89 & 4.05 & 11.22\% & 7.84 & 5.12 & 14.00\% & 9.58 \\ GP-VAR  & 3.56 & 9.55\% & 6.54 & 3.98 & 11.02\% & 7.56 & 4.87 & 13.34\% & 9.19 \\ FC-LSTM  & 3.44 & 9.60\% & 6.30 & 3.77 & 10.90\% & 7.23 & 4.37 & 13.20\% & 8.69 \\ Graph wavenet  & 2.69 & 6.90\% & 5.15 & 3.07 & 8.37\% & 6.22 & 3.53 & 10.01\% & 7.37 \\ GMAN  & 2.94 & 7.51\% & 5.89 & 3.22 & 8.92\% & 6.61 & 3.68 & 10.25\% & 7.49 \\ STGCN  & 2.88 & 7.62\% & 5.74 & 3.47 & 9.57\% & 7.24 & 4.59 & 12.70\% & 9.40 \\ GGRNN  & 2.73 & 7.12\% & 5.44 & 3.31 & 8.97\% & 6.63 & 3.88 & 10.59\% & 8.14 \\ GRUGCN  & 2.69 & **6.61\%** & 5.15 & 3.05 & 7.96\% & 6.04 & 3.62 & 9.92\% & 7.33 \\ SGP  & 3.06 & 7.31\% & 5.49 & 3.43 & 8.54\% & 6.47 & 4.03 & 10.53\% & 7.81 \\ GTCNN  & **2.68** & 6.85\% & 5.17 & 3.02 & 8.30\% & 6.20 & 3.55 & 10.21\% & 7.35 \\ 
**CITRUS (Ours)** & 2.70 & 6.74\% & **5.14** & **2.98** & **7.78\%** & **5.90** & **3.44** & **9.28\%** & **6.85** \\   } &  \\   &  &  &  \\   & MAE & MAPE & RMSE & MAE & MAPE & RMSE & MAE & MAPE & RMSE \\  ARIMA  & 1.62 & 3.50\% & 3.30 & 2.33 & 5.40\% & 4.76 & 3.38 & 8.30\% & 6.50 \\ G-VARMA  & 1.88 & 4.28\% & 3.96 & 2.45 & 5.42\% & 4.70 & 3.01 & 7.10\% & 5.83 \\ GP-VAR  & 1.74 & 3.45\% & 3.22 & 2.16 & 5.15\% & 4.41 & 2.48 & 6.18\% & 5.04 \\ FC-LSTM  & 2.05 & 4.80\% & 4.19 & 2.20 & 5.20\% & 4.55 & 2.37 & 5.70\% & 4.74 \\ Graph wavenet  & 1.30 & 2.73\% & 2.74 & 1.63 & 3.67\% & 3.70 & 1.95 & 4.63\% & 4.52 \\ GMAN  & 1.34 & 2.81\% & 2.82 & 1.62 & 3.63\% & 3.72 & 1.86 & 4.31\% & 4.32 \\ STGCN  & 1.36 & 2.90\% & 2.96 & 1.81 & 4.17\% & 4.27 & 2.49 & 5.79\% & 5.69 \\ GGRNN  & 1.33 & 2.83\% & 2.81 & 1.68 & 3.79\% & 3.94 & 2.34 & 5.21\% & 5.14 \\ GRUGCN  & **1.21** & **2.49\%** & **2.52** & 1.54 & 3.23\% & 3.46 & 2.01 & 4.72\% & 4.65 \\ SGP  & 1.33 & 2.71\% & 2.84 & 1.70 & 3.61\% & 3.83 & 2.24 & 5.08\% & 5.19 \\ GTCNN  & 1.25 & 2.61\% & 2.66 & 1.65 & 3.82\% & 3.68 & 2.27 & 5.11\% & 4.99 \\ 
**CITRUS (Ours)** & **1.21** & 2.51\% & 2.61 & **1.48** & **3.23\%** & **3.28** & **1.78** & **4.08\%** & **3.99** \\   

Table 1: Traffic forecasting comparison between CITRUS and previous methods.

   } &  &  \\   &  &  &  &  &  &  &  &  &  \\  GRUGCN  & 0.49 & 0.56 & 0.63 & 0.70 & 0.75 & 0.15 & 0.18 & 0.24 & 0.30 & 0.37 \\ GGRNN  & 0.29 & 0.42 & 0.54 & 0.65 & 0.75 & 0.19 & 0.20 & 0.27 & 0.38 & 0.48 \\ SGP  & 0.24 & 0.37 & 0.49 & 0.59 & 0.69 & 0.39 & 0.41 & 0.43 & 0.46 & 0.49 \\ GTCNN  & 0.39 & 0.45 & 0.52 & 0.60 & 0.68 & 0.17 & 0.19 & 0.25 & 0.31 & 0.37 \\ 
**CITRUS (Ours)** & **0.23** & **0.35** & **0.47** & **0.58** & **0.67** & **0.04** & **0.12** & **0.20** & **0.29** & **0.36** \\   

Table 2: Weather forecasting comparison (by rNMSE) between CITRUS and previous methods.

the CITRUS blocks are \(16\), while no MLP modules have been used. Regarding NOAA, we set the dimension and the initial linear layer of the CITRUS blocks as \(16\), and use \(3\)-layer MLPs for channel mixing in each block. The CITRUS blocks have the last activation as a Leaky ReLU. We use MSE as the loss function and consider root-normalized MSE (rNMSE) as the evaluation metric.

Table 2 illustrates the results of the weather forecasting task. We observe that CITRUS show superior forecasting performance compared to previous methods in all forecasting horizons. We note that the SGP model , which is a scalable architecture, is the second-best performing method in Molene, possibly due to the small scale of this dataset. On the contrary, the NOAA dataset is larger, so models with higher parameter budgets perform better.

### Ablation Study and Hyperparameter Sensitivity Analysis

**Ablation study.** The ablation study concerns the CITRUS modules responsible for _jointly_ learning spatiotemporal couplings. To this end, we use two well-known architectures: TTS and STT. Therefore, we evaluate four possible configurations: i) TTS, where we first apply an RNN-based network (here, GRU), and then the outputs are fed into a regular GNN; ii) STT, which is exactly the opposite of the TTS; iii) Continuous TTS (CTTS), where we replace the GNN in TTS with a CGNN; and iv) Continous STT (CSTT), which is the opposite of CTTS. We report the results of this ablation study on the MetrLA dataset in Table 3. We observe that CITRUS has superior results probably due to learning _joint_ (and not sequential) spatiotemporal couplings by modeling these dependencies using product graphs with learnable receptive fields. We also observe that CTTS and CSTT outperform their discrete counterparts TTS and STT, probably due to the learning of adaptive graph neighborhoods instead of relying on \(1\)-hop connections in regular GNNs.

**Hyperparameter sensitivity analysis.** The sensitivity analysis is related to the number of selected eigenvector-eigenvalue (eig-eiv) pairs of the factor Laplacians. We select eigenvector-eigenvalue pairs in \(k\{2,10,50,100,150,200\}\) out of \(207\) components in the MetrLA dataset. Table 4 presents the forecasting results in MAE along with the training time (per epoch) of this ablation study. We observe that selecting \(50\) eigenvector-eigenvalue pairs is enough to get good forecasting performance while having a fast training time. This experiment illustrates the advantages of CITRUS for accelerating training and keeping the performance as consistent as possible.

## 5 Conclusion and Limitations

In this paper, we proposed CITRUS, a novel model for jointly learning multidomain couplings from product graph signals based on tensorial PDEs on graphs (TPDEGs). We modeled these representations as separable continuous heat graph kernels as solutions to the TPDEG. Therefore, we showed that the underlying graph is actually the Cartesian product of the domain-specific factor

    &  \\   &  &  &  \\   & MAE & MAPE & RMSE & MAE & MAPE & RMSE & MAE & MAPE & RMSE \\  TTS & 2.73 & 6.78\% & 5.21 & 3.10 & 8.02\% & 6.15 & 3.67 & 10.05\% & 7.46 \\ STT & 2.72 & 6.74\% & 5.19 & 3.07 & 7.94\% & 6.08 & 3.65 & 10.97\% & 7.37 \\ CTTS & **2.70** & 6.69\% & 5.20 & 3.05 & 7.93\% & 6.18 & 3.61 & 9.75\% & 7.51 \\ CSTT & **2.70** & **6.66**\% & 5.22 & 3.06 & 7.92\% & 6.19 & 3.63 & 9.92\% & 7.48 \\ 
**CITRUS** (Ours) & **2.70** & 6.74\% & **5.14** & **2.98** & **7.78**\% & **5.90** & **3.44** & **9.28**\% & **6.85** \\   

Table 3: Ablation study on comparison between the proposed CITRUS and typically ST pipelines.

    &  &  &  &  &  &  &  \\  MAE & 2.80 & 2.74 & 2.72 & 2.71 & 2.71 & 2.70 & 2.70 \\ Training time (seconds) & 2.52 & 2.91 & 2.96 & 3.86 & 5.28 & 6.36 & 7.78 \\   

Table 4: Training time (per epoch) and forecasting results vs. number of selected eig-eiv on MetrLA.

graphs. We rigorously studied the stability and over-smoothing aspects of CITRUS theoretically and experimentally. Finally, as a proof of concept, we use CITRUS to tackle the traffic and weather spatiotemporal forecasting tasks on public datasets, illustrating the superior performance of our model compared to state-of-the-art methods.

An interesting future direction for our framework could involve adapting it to handle other types of graph products, such as Kronecker and Strong graph products . Future efforts will also focus on finding tighter and more general upper bounds in the stability and over-smoothing analyses. For example, we will explore the possible relationships between the size of the factor graphs and these properties, either in a general form or for specific well-studied structures, such as ER graphs. Additionally, we plan to investigate more challenging real-world applications of the proposed framework, especially in scenarios involving more than two factor graphs.