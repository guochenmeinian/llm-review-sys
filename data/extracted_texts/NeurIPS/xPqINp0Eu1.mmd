# Stability of Random Forests and Coverage of Random-Forest Prediction Intervals

Yan Wang

Department of Mathematics

Wayne State University

Detroit, MI 48202

wangyan@wayne.edu

&Huaiqing Wu, Dan Nettleton

Department of Statistics

Iowa State University

Ames, IA 50011

{isuhwu,dnett}@iastate.edu

###### Abstract

We establish stability of random forests under the mild condition that the squared response (\(Y^{2}\)) does not have a heavy tail. In particular, our analysis holds for the practical version of random forests that is implemented in popular packages like randomForest in R. Empirical results show that stability may persist even beyond our assumption and hold for heavy-tailed \(Y^{2}\). Using the stability property, we prove a non-asymptotic lower bound for the coverage probability of prediction intervals constructed from the out-of-bag error of random forests. With another mild condition that is typically satisfied when \(Y\) is continuous, we also establish a complementary upper bound, which can be similarly established for the jackknife prediction interval constructed from an arbitrary stable algorithm. We also discuss the asymptotic coverage probability under assumptions weaker than those considered in previous literature. Our work implies that random forests, with its stability property, is an effective machine learning method that can provide not only satisfactory point prediction but also justified interval prediction at almost no extra computational cost.

## 1 Introduction

Random forests (RFs) is a successful machine learning method that serves as a standard approach to tabular data analysis and has good predictive performance . However, there is a big gap between the empirical effectiveness of RFs and the limited understanding of its properties. Most known theoretical results are established for variants of RFs not necessarily used in practice . For the RF version implemented in packages like randomForest in R, little is known without strong assumptions ; RFs is notoriously difficult to analyze as a greedy algorithm. Here we show an important property for the RF used in practice (as well as for other variants) under realistic conditions.

### Stability of random forests

The first main contribution of this work establishes the stability condition for the RF.

**Theorem 1** (Stability of random forests, informal).: _For independent and identically distributed (iid) training data points \((X_{i},Y_{i}),i\{1,,n\}[n]\) and a test point \((X,Y)\), if the squared response \(Y^{2}\) does not have a heavy tail, then the RF predictor \(_{B}\) and any out-of-bag (OOB) predictor \(_{B}^{ i}\) predict similar values, i.e.,_

\[(|_{B}(X)-_{B}^{ i}(X) |>_{n,B})_{n,B},\] (1)_where \(_{B}\) results from the aggregation of all \(B\) base tree predictors, while \(_{B}^{ i}\) only those with the point \((X_{i},Y_{i})\) excluded in training; \(_{n,B}\) and \(_{n,B}\) are small numbers depending on \(n\) and \(B\)._

This result is referred to as the stability of the RF because it indicates that no single training point is extremely important in determining \(_{B}\) in a probabilistic sense. Theorem 1 relies on a recent important work that establishes the absolute stability (see below for a precise definition) of general bagged algorithms with bounded outputs . We take advantage of the fact that the range of the RF output is conditionally dependent upon the maximal and minimal values of \(Y\) in the training set, and then we show in theory that the stability property of the RF is possible even if \(Y\) is marginally unbounded. To our knowledge, this is the first stability result established for the RF.

The technique used in our analysis requires that \(Y^{2}\) not have a heavy tail (to make \(_{n,B}\) and \(_{n,B}\) small). Though arguably already mild, we conjecture that this condition might be further relaxed. As shown below, numerical evidence suggests that the light-tail assumption may not be necessary for RF stability, which could hold even when \(Y\) follows a heavy-tail distribution like the Cauchy distribution.

### Random-forest prediction intervals

Stability is a crucial property of a learning algorithm. For example, stability has a deep connection with the generalization error of an algorithm [9; 19; 23]. Moreover, stability also turns out to be important in distribution-free predictive inference. In particular, an algorithm being stable justifies the jackknife prediction interval (PI), which otherwise has no coverage guarantee .

In this work, we show that stability makes it possible to construct a PI with guaranteed coverage from the OOB error of the RF. The OOB error is defined as \(R_{i}=|Y_{i}-_{B}^{ i}(X_{i})|\), \(i[n]\). A main reason why such a PI is appealing is that \(R_{i}\) can be obtained almost without extra effort. For example, a one-shot training using the R package randomForest gives us an RF predictor \(_{B}\) and all \(n\) OOB predictions \(_{B}^{ i}(X_{i})\). So, from the computational point of view, a convenient way to construct a PI for a test point \((X,Y)\) is of the form "\(_{B}(X)\)\(\) proper quantile of \(\{R_{i}\}\)" [17; 35].

The second main contribution of this work constructs such PIs and theoretically proves, under mild conditions, the non-asymptotic lower and upper bounds for the coverage probability.

**Theorem 2** (Coverage lower bound, informal).: _Under the same assumptions as in Theorem 1, and for \((0,1)\)1, we have the following lower bound of coverage probability:_

\[(|Y-_{B}(X)|(n+1)(1- )R_{i}+_{n,B}) 1--O( }),\]

_where \(\) is the ceiling function. Big \(O\) and other related notations are used in the usual way._

**Theorem 3** (Coverage upper bound, informal).: _If we further assume that \(Y\) is continuous, resulting in distinct prediction errors, then we also have the following upper bound:_

\[(|Y-_{B}(X)|(n+1)(1- )R_{i}-_{n,B}) 1-+ +O(}).\]

As we detail below, the PIs we provide coverage guarantees for are neither the jackknife-with-stability interval discussed in , nor the jackknife+after-bootstrap interval established in . In our context, constructing the former needs \(n\) leave-one-out (LOO) predictors (rather than \(n\) OOB predictors), i.e., \(n\) additional RFs with each built on a training set of size \(n-1\). Constructing the latter needs the explicit information of each \(_{B}^{ i}()\) rather than the OOB prediction \(_{B}^{ i}(X_{i})\) for each \(X_{i}\) only. Both these methods require additional, sometimes extensive, computation given current popular packages. In contrast, our results are operationally more convenient. After one-shot training, we obtain not only a point predictor \(_{B}()\), but also a valid interval predictor at almost no extra cost. Under reasonable conditions, our results indicate that by slightly inflating (or deflating) the PI constructed from the \((n+1)(1-)\)-th smallest \(R_{i}\), the coverage probability is guaranteed not to decrease (or increase) too much from the desired level of \(1-\). In fact, many numerical results, such as those in [17; 35], suggest that

\[(|Y-_{B}(X)|(n+1)(1- )R_{i}) 1-.\]

Motivated by this fact, we further establish an asymptotic result of coverage for such PIs.

**Theorem 4**.: _(Asymptotic coverage, informal) In addition to the conditions in the above theorems, also suppose the prediction error \(|Y-_{B}(X)|\) is continuous, and its cumulative distribution function (CDF) does not change too drastically for all sufficiently large \(n\). Then_

\[(|Y-_{B}(X)|(n+1)(1-) R_{i}) 1-n.\]

In , this asymptotic coverage was proved based on stronger assumptions. In particular, the true model is assumed to be additive such that "\(Y=f_{0}(X)+\)" with the zero-mean noise independent of \(X\), and \(_{B}(X)\) is assumed to converge to \(f_{0}(X)\) in probability. We do not require \(_{B}\) to converge to anything in any sense when \(n\). Technically, we need the family of prediction error CDFs be uniformly equicontinuous.

Based on our results, the RF seems to be the only one, among existing popular machine learning algorithms, that can provide both point and interval predictors with justification in such a convenient way. This makes the RF appealing, especially for tasks where the computational cost is a concern.

It is also worth noting that the upper-bound result is of interest in its own right. It can be generalized to jackknife PIs that are constructed from any stable algorithm; the result serves as a complement to the lower bounds established previously [3; 18].

Summarizing, we

* theoretically prove that the (greedy) RF algorithm is stable when \(Y^{2}\) does not have a heavy tail;
* numerically show that RF stability may hold beyond the above light-tail assumption;
* construct PIs based on the OOB error with finite-sample coverage guarantees: the lower bound of coverage does not need any additional assumption beyond stability; the upper bound needs an additional assumption, which is usually satisfied when \(Y\) is continuous;
* provide the upper bound of coverage for jackknife PIs constructed from general stable algorithms, assuming distinct LOO errors; and
* prove asymptotically exact coverage for RF-based PIs under weaker assumptions than those previously considered in published work.

## 2 Concepts of algorithmic stability

Stability stands at the core of this work. There are different types of stability, each of which is used to assess quantitatively how stable (in some certain sense) an algorithm is with respect to small variations in training data [9; 28; 4]. In a recent work , robust optimization is used to enhance the stability of algorithms in classification tasks. In , bagging is proved to be an efficient mechanism to stabilize algorithms in regression tasks. We focus on regression here. As will be made clear, the technique used in this work relies on the fact that the RF predictor in regression results from averaging tree predictors. However, the majority vote of tree predictors is used in classification, and new ideas are needed to analyze the RF stability in this setting. For our purposes, we introduce three levels of stability from strongest to weakest. The strongest version of stability, introduced in , does _not_ depend on the data distribution, and may be referred to as "absolute stability."

**Definition 1** (Absolute stability of algorithms).: For any dataset consisting of \(n 2\) training points \(D=\{(X_{1},Y_{1}),,(X_{n},Y_{n})\}\) and any test point \((X,Y)\), an algorithm \(\) is defined to be \((,)\)-absolutely-stable if

\[_{i=1}^{n}_{}(|(X)-^{-i}( X)|>)\]

for some \(, 0\), where \(\) denotes the possible innate randomness in the algorithm (such as the node splitting procedure in the RF) and can be seen as a random variable uniformly distributed in \(\), \(=(D;)\) is the predictor trained on \(D\), and \(^{-i}=(D^{-i};)\) is the \(i\)th LOO predictor trained on \(D^{-i}\), i.e., \(D\) without the \(i\)th point \((X_{i},Y_{i})\). We might refer to the RF as both an algorithm (the learning procedure) and a predictor (the learned function) for simplicity.

Many bagged algorithms, in particular those with bounded predicted values, can achieve absolute stability with both \(\) and \(\) converging to 0, as long as \(n\) and the number of bags \(B\) go to infinity.

However, the predicted value of the RF is in general unbounded (for regression tasks considered in this work), and we are more interested in another type of stability, investigated in , and called out-of-sample stability . For simplicity, we name it "stability." This notion of stability turns out to be important in validating a jackknife prediction interval.

**Definition 2** (Stability of algorithms).: For iid training and test data, algorithm \(\) is \((,)\)-stable if

\[_{D,X,}(|(X)-^{-i}(X)|> )\]

for some \(, 0\), where \(D,X,,^{-i}\) are as defined above.

We will establish this type of stability for the derandomized RF defined below, where the data-generating distribution is involved. To this end, we will use the methods in , which aim to provide absolute stability for bagged algorithms. Technically, we use such methods to first establish the "conditional stability" of an algorithm with respect to given data.

**Definition 3** (Conditional stability of algorithms).: Conditional on \(D\) and \(X\), an algorithm \(\) is defined to be \((,)\)-conditionally-stable if

\[_{i=1}^{n}_{|D,X}\,(|(X)- ^{-i}(X)|>|D,X)\]

for some \(, 0\), where \(D,X,,^{-i}\) are as defined above.

Once conditional stability is established for the derandomized RF algorithm, its stability can be consequently established by invoking

\[_{D,X,}()=_{D,X}[_{|D,X}( |D,X)].\]

Stability of the derandomized RF provides the most essential ingredient for that of the practical RF, although the latter involves another type of stability, known as ensemble stability . Ensemble stability justifies replacing the LOO predictor with the OOB predictor in (1). We may abuse the term "stability" in the following when the OOB, rather than the LOO, predictor is used.

## 3 Stability of random forests

### Basics of random forests

This work mainly considers using the RF to perform regression tasks, where the response \(Y\) can be unbounded. By construction, the RF predictor with \(B\) bags, denoted by \(_{B}\), is a bagged algorithm with the base algorithm being a tree, and \(_{B}=_{b=1}^{B}_{b},\) where \(_{b}\) is the \(b\)th tree predictor, trained on the \(b\)th bag \(r_{b}\), a bootstrapped sample of the training set \(D\). The randomness in the tree predictor \(\) originates from two independent sources: innate randomness \(\) in the node splitting process and resampling randomness from the bag \(r\). For the \(i\)th point, one can define the OOB RF predictor as \(_{B}^{i}=}_{b=1}^{B}_{b} [i r_{b}],\) where \(\{\}\) denotes the indicator function, and \(B_{i}=_{b=1}^{B}\{i r_{b}\}\). Define \(p=(i r)\) as the probability that the \(i\)th point is included in bag \(r\). Then it is clear that \(B_{i}(B,1-p)\) for all \(i\). We also denote \(\) and \(^{i}{}^{i}\) as the _derandomized_ versions of \(_{B}\) and \(_{B}^{i}\), respectively. Precisely, \(=_{,r}[]\) and \(^{i}{}^{i}=_{,r}[|i r]\). It is worth noting that, by definition, \(_{B}^{i}_{B}^{-i}\) for finite \(B\), while \(^{i}{}^{i}=^{-i}\) as the derandomized RF results from the aggregation of an infinite number of trees. Since RF predictors are averages over tree predictors, the predicted values they output, given training set \(D\), are bounded in \([Y_{(1)},Y_{(n)}]\), where \(Y_{(1)}\) and \(Y_{(n)}\) are the minimum and maximum of \(\{Y_{1},,Y_{n}\}\), respectively. We also let \(Z_{i}=|Y_{i}|\) for all \(i\), and denote the maximum as \(Z_{(n)}\). As a result, we have that

\[|^{i}{}^{i}-| Y_{(n)}-Y_{(1)} 2Z_{(n)}.\] (2)

**Remark 1**.: This is also true for \(_{B}^{i}\) and \(_{B}\) for any finite \(B\). In fact, this is a distinctive feature of the RF, _irrespective_ of the node splitting rule. Other regression methods do not necessarily have such a data-dependence bound. This observation helps to establish the conditional stability of the RF.

**Remark 2**.: Practically, when \(n\) is large, one might think that the bound (2) is crude. On one hand, if we look for a bound valid for any finite \(n 2\), then there is not much room for improvement for small \(n\). On the other hand, we do expect that the _typical_ stability of the RF can go beyond the finite-sample guarantee provided by (2) when \(n\) is big, which is consistent with the numerical results shown below. A more informative bound for large \(n\) is worth future investigation.

There are several quantities that are useful in establishing the RF stability; they can be calculated explicitly and are listed below. First, it is well known that

\[p(i r)=1-(1-1/n)^{n}=1-1/e+O(1/n).\] (3)

Actually, \(p\) is monotonically decreasing for \(n 1\). Second,

\[q-(\{i r\},\{j r\})=(1-1/n)^{2n}-( 1-2/n)^{n}=O(1/n),\] (4)

as can be directly checked. Third, the moment generating function of \(B_{i}\) is

\[[e^{sB_{i}}]=(p+(1-p)e^{s})^{B}.\] (5)

In the following, we first perform the stability analysis for the derandomized RF (consisting of an infinite number of trees) and then extend the results to the practical finite-\(B\) case.

### Derandomized random forests

The following theorem formalizes the conditional stability property for the derandomized RF, the proof of which is a direct result of Theorem 8 in , and is omitted here.

**Theorem 5** (Conditional stability of derandomized random forests).: _Conditional on training set \(D\) and test point \((X,Y)\), for the derandomized random forest predictor \(\) we have that_

\[_{i=1}^{n}\{(X)-^{  i}(X)>D,X\,\}(D,X) ^{2}}{^{2}n}(+} ).\] (6)

If \((D,X) 1\), the statement is trivial, and we will focus on the case that \((D,X)(0,1)\). We can now establish the stability property for the derandomized RF.

**Theorem 6** (Stability of derandomized random forests).: _For iid training and test data and \(>0\), the derandomized random forest predictor \(\) is stable with_

\[_{D,X}(|(X)-^{ i}(X) |>)[Z_{(n)}^{2}]}{^{2}n }(+}).\] (7)

This result follows directly from the conditional stability (6) by averaging over \(D\) and \(X\). There is some freedom in choosing the dependence of \(\) on \(n\). On one hand, in order to make sense of the word "stability," we do expect \(\) and \(\) to be small for large \(n\). From (3) and (4), it is clear that the asymptotic behavior of \(\) is dominated by that of \([Z_{(n)}^{2}]/(^{2}n)\), which can be tuned by manipulating \(\). For example, a matching convergence rate to 0 between \(\) and \(\) might be desirable, and one can then set \(=O(([Z_{(n)}^{2}]/n)^{1/3})\) if the scaling of \([Z_{(n)}^{2}]=o(n)\) is known or can be inferred. On the other hand, we can fix \(\) to further investigate the relation between stability and the convergence-in-probability property of the RF. By (7), under the condition that \([Z_{(n)}^{2}]/n 0\) as \(n\), one immediately comes to the conclusion that \(^{ i}(X)-(X)\) converges to \(0\) in probability. Actually, a stronger conclusion can be drawn under the same condition.

**Corollary 1**.: _For iid training and test data, we have_

\[_{D,X}[|(X)-^{ i}(X)|]<2 [Z_{(n)}^{2}]}{n}(+ })}.\] (8)

_Further assume that \([Y^{2}]<\). Then we have_

\[_{D,X}[|(X)-^{ i}(X)|] 0 (X)-^{ i}(X)}{}0 n.\] (9)

**Remark 3**.: The additional assumption that \([Y^{2}]<\) is mild. Many commonly encountered random variables have a light tail and thus a finite second moment, irrespective of the detailed information of the distribution in question. Note that the bound (2) itself can be crude, and our result is expected to be valid even beyond this mild condition.

**Remark 4**.: This result indicates that the _difference_ is diminishing between \(\) and \(^{ i}\), built on \(n\) and \(n-1\) training data points, respectively. However, there is no indication that the derandomized \((X)\) itself will converge to anything. This idea inspires the proposal of Theorem 11.

The proof of this result, as well as others below, will be deferred to the Appendix. So far, we have investigated the derandomized version of the RF, which is a limiting case and can be seen as consisting of an infinite number of trees, averaging out all kinds of possible randomness in the predictor construction process. In order to make the results more relevant to applied machine learning, the finite-\(B\) analysis for the RF is conducted below.

### Finite-\(B\) random forests

We now consider the difference between \(_{B}\) and \(_{B}^{ i}\). We denote \(=(_{1},,_{B})\) and \(=(r_{1},,r_{B})\) as the corresponding sources of randomness in \(B\) trees. We also consider conditional stability first and then move to the stability of \(_{B}\).

**Theorem 7** (Conditional stability of finite-\(B\) random forests).: _Conditional on training set \(D\) and test point \((X,Y)\), for a random forest predictor \(_{B}\) that consists of \(B\) trees, we have for \(>0\) that_

\[_{i=1}^{n}_{,|D,X} (|_{B}(X)-_{B}^{ i}(X)|> +2^{2}}{B}()} |\!D,X)\! 3+g(p,,B),\]

_where \(\) is short for \((D,X)\) as defined in (6) and \(g(p,,B)=2(p+(1-p)^{})^{B}\)._

Next, we consider the case of iid data and investigate the RF stability by averaging out the randomness in data. Note that \(Z_{(n)}\) and \(\) are random and depend on the data distribution, while we are interested in a probability bound for \(|_{B}(X)-_{B}^{ i}(X)|\) greater than a deterministic quantity, which is only a function of \(B\) and \(n\). In this finite-\(B\) case, the stability of \(_{B}\) cannot be directly obtained from its conditional stability as in the derandomized situation.

**Theorem 8** (Stability of finite-\(B\) random forests).: _Assume training points in set \(D\) and the test point \((X,Y)\) are iid, drawn from a fixed distribution. For the random forest predictor \(_{B}\) consisting of \(B\) trees and trained on \(D\), we have_

\[_{D,X,,}(|_{B}( X)-_{B}^{ i}(X)|>_{n,B}) _{n,B},\] (10)

_where \(_{n,B}=_{i=1}^{3}_{i}\), and \(_{n,B}=_{i=1}^{3}_{i}\). The pair of \((_{2},_{2}/)\) satisfies the derandomized RF stability condition (7) with \(>1\). Moreover, \(_{1}=_{3}=\![Z_{(n)}^{2} ](})/B}\), \(_{1}=2_{2}+2\!(Z_{(n)}^{2}>\![Z_{ (n)}^{2}])\), and \(_{3}=g(p,_{2},B)+2\!(Z_{(n)}^{2}>\! [Z_{(n)}^{2}])\)._

On a high level, the establishment of this theorem relies on two observations: (i) the stability of the derandomized RF, so that the difference \(|(X)-^{ i}(X)|\) is controlled, and (ii) the concentration of measure, so that the differences \(|_{B}(X)-(X)|\) and \(|_{B}^{ i}(X)-^{ i}(X)|\) are controlled. In order to make full sense of the word "stability," it is desirable that \(_{n,B}\) and \(_{n,B}\) can converge to 0. It is known that \(\![Y^{2}]<\) suffices to ensure \(\![Z_{(n)}^{2}]=o(n)\)[15; 13], and hence the stability of the derandomized RF. Now in the finite-\(B\) case, we need an additional distributional assumption to control the tail probability \(\!(Z_{(n)}^{2}>\![Z_{(n)}^{2}])\). It turns out that for typical light-tailed \(Y^{2}\), such a tail probability will converge to 0 as \(n\). Technically, we can assume \(Y^{2}\) to be sub-gamma . Note that bounded and sub-Gaussian random variables are sub-gamma. Hence the sub-gamma assumption is not strong and can be satisfied by distributions underlying many real datasets.

**Definition 4** (Sub-gamma random variables ).: A random variable \(W\) is said to be sub-gamma (on the right tail) with parameters \((^{2},c)\) where \(c 0\), if \(\![e^{s(W-[W])}]^{2}}{2(1- cs)}\) for all \(s(0,1/c)\).

**Lemma 1**.: _Suppose \(Y^{2}\) is sub-gamma with parameters \((^{2},c)\) with \(c>0\), and \([Z^{2}_{(n)}] a n\) with \(a c\). For \(>c/a\), we have \(_{n}(Z^{2}_{(n)}>[Z^{2}_{(n)}])=0\)._

**Remark 5**.: We have set \(c>0\) above. If \(c=0\), then \(Y^{2}\) is in fact sub-Gaussian, and the tail probability can be controlled similarly. If \(Y^{2}\) is upper bounded by some constant \(M^{2}\), the stability analysis is even simpler, and there is no need to consider the tail probability at all, as we can use \(M^{2}\) in place of \(Z^{2}_{(n)}\) in the conditional stability of the RF and then take expectation with respect to data.

**Example 1**.: _Consider \(Y^{2}(1)\), the exponential distribution with scale parameter 1. It is known that \(Y^{2}\) is sub-gamma with \((^{2},c)=(1,1)\), and \([Z^{2}_{(n)}]=_{i=1}^{n} H_{n}\) with \(H_{n}(+ n,+(n+1))\), where \( 0.577\) is Euler's constant. Hence \(H_{n}= n+o( n)\), and a straightforward calculation reveals that \(_{n}(Z^{2}_{(n)}>[Z^{2}_{(n)}])=0\) as long as \(>1\)._

From such results, one can see that the vanishing tail probability is not a stringent condition. By taking this additional assumption, it is indeed possible that both \(_{n,B}\) and \(_{n,B}\) converge to 0.

**Corollary 2**.: _For the same setting as in Theorem 8, suppose \(Y^{2}\) is sub-gamma with parameters \((^{2},c)\) with \(c>0\) and \([Z^{2}_{(n)}] a n\) with \(a c\). Let \(>c/a\) be a fixed number, and let \(B\) depend on \(n\). Then for \(_{2}\) that satisfies both \(_{2}=()\) and \(_{2}=o(1)\), and \(B=(^{2}n)\), we have \(_{n}_{n,B}=_{n}_{n,B}=0\)._

It is worth noting that there are multiple ways to let \(_{n,B}\) and \(_{n,B}\) approach 0, as the dependence of \(_{2}\), \(B\), and even \(\) on \(n\) can all be manipulated. The point is that, theoretically, even the greedy RF can be stable with vanishing parameters. In practice, however, the stability of \(_{B}\) seems to hold in broader situations where both the moment and tail assumptions on \(Y^{2}\) can be relaxed.

### Stability in practice and limitations of theory

We created a virtual dataset consisting of \(n=4000\) points. We let \(Y\) be a standard Cauchy random variable, which is even without a well-defined mean. The feature vector \(X^{3}\) is determined as \(X=[0.5Y+(Y),Y^{2}-0.2Y^{3},[Y>0\}+]^{T}\) where \(\) is a standard normal random variable. We used \(3000\) of the points for training and \(1000\) of them as test points. Using the randomForest package with default setting (except letting \(B=1000\)), we had an output RF predictor \(_{B}\). We also aggregated corresponding tree predictors to have \(3000\) OOB predictors \(_{B}^{ i}\). For each \(i\), we calculated the absolute difference \(|_{B}(X)-_{B}^{ i}(X)|\) on \(1000\) test points to come up with a

Figure 1: Left: Density plots of the \(_{10}\) absolute difference \(|_{B}(X)-_{B}^{ i}(X)|\) for 3000 OOB predictors \(_{B}^{ i}\) on 1000 test points. We let \(B=1000\). The RF stability (10) seems to persist, even though \(Y\) follows the (heavy-tailed) standard Cauchy distribution. Numerically, we set \(_{n,B}=0.05\) and calculated the maximum of the 0.95 quantile of the 3000 empirical distributions to have \(_{n,B}=0.237\). Right: Density plots of 1000 \(_{10}\) absolute prediction errors \(|Y-_{B}(X)|\) and of 3000 \(_{10}\) absolute OOB errors \(|Y_{i}-_{B}^{ i}(X_{i})|\). The similarity between the plots supports the idea that the OOB errors can be used to construct PIs.

density plot for such a difference, shown in Fig. 1. We also calculated \(1000\) absolute prediction errors \(|Y-_{B}(X)|\) that are incurred by \(_{B}\) on test points, and \(3000\) OOB errors \(|Y_{i}-_{B}^{ i}(X_{i})|\), each incurred by an OOB predictor \(_{B}^{ i}\) on its OOB point \((X_{i},Y_{i})\). The computation can be done within a few minutes on a laptop. The density plots of these two kinds of errors are also shown in Fig. 1. This example shows that the RF stability can be present beyond the realm guaranteed by the light-tail assumption. As mentioned above, this is because the bound (2) can be conservative when \(n\) is large. We hope our results can inspire future study towards a more informative bound. Also, the similarity between the prediction error and the OOB error in this heavy-tail case indicates that the RF-based PIs analyzed below can find more applications in practice than justified by the current theory.

## 4 Random-forest prediction intervals

### Comparison with related methods

With the stability property of the RF, it is possible to construct PIs with finite-sample guaranteed coverage. Recent years have witnessed the development of distribution-free predictive inference  with the full [33; 27], split [24; 31; 20], and jackknife+ [3; 32] conformal prediction methods being three milestones. The full conformal method is computationally prohibitive when used in practice. The split method greatly reduces the computational cost but fails to thoroughly extract the available information of training data. The jackknife+ (**J+**) method maximizes the usage of data at a computational cost in between those of full and split methods. In , jackknife+-after-bootstrap (**J+aB**) was proposed for bagged algorithms to achieve the same goal as in J+, while the training cost can be further reduced. However, the number of bags \(B\) is required to be a Binomial random variable, which might seem unnatural. It turns out that by further imposing the assumption of _ensemble_ stability (which is essentially the concentration of resampling measure), J+aB can still have guaranteed coverage with a fixed \(B\). Ensemble stability is defined for bagged algorithms. It measures how typical a bootstrap sample is, and is different from the algorithmic stability that quantifies the influence of removing one training point. If algorithmic stability is also imposed, then not only J+aB, but also jackknife can provide guaranteed coverage, which is otherwise impossible [29; 3].

Conceptually, the J+ approach and its variants under stability conditions are particularly relevant to this work. As the stability we establish for the RF contains both ensemble and algorithmic components, we will generally refer to the J+aB method with both ensemble and algorithmic stability as **J+aBS** and the jackknife method with algorithmic stability as **JS**. Our method might be best described as "jackknife-after-bootstrap-with-stability (**JaBS**)" tailored for the RF, which is different from both JS and J+aBS. Our method requires the least effort of computing as only one output predictor is needed, while all others require at least \(n\) output predictors.

There also exist RF-based PIs [17; 35] that are essentially of the jackknife-after-bootstrap (**JaB**) type and almost identical to ours practically when \(\) is small and \(n\) equals the size of a typical dataset. However, without stability, there is, in general, no guarantee for the coverage of such PIs, although the asymptotic coverage \(1-\) can be established based on strong assumptions . We take advantage of the stability of the RF algorithm to establish the lower bound of coverage in Theorem 9 below. An upper bound is established in Theorem 10 with an additional mild assumption. We also propose a weaker assumption for asymptotic coverage in Theorem 11.

We compare these relevant methods to ours in Table 1 and Table 2, where the RF is set as the working algorithm for all methods and \((,)\) is a general pair of stability parameters. We define \(q_{n,}\{R_{i}\}\), \(q^{+}_{n,}\{R_{i}\}\), \(q^{-}_{n,}\{R_{i}\}\), and \(q^{}_{n,}\{R_{i}\}\) as follows. Given \(\{a_{1},,a_{n}\}\),

\[q_{n,}\{a_{i}\} =q^{+}_{n,}\{a_{i}\} =[(1-)(n+1)]\{a_{1},,a_{n}\},\] \[q^{}_{n,}\{a_{i}\} =[(1-)n]\{a_{1},,a_{n}\},\] \[q^{-}_{n,}\{a_{i}\} =[(n+1)]\{a_{1},,a_{n}\},\]

where \(\) is the floor function. Let \(R^{}_{i}=|Y_{i}-^{-i}_{B}(X_{i})|\) be the LOO error, where \(^{-i}_{B}\) is trained without the \(i\)th training point, and by definition \(^{-i}_{B}^{i}_{B}\).

In Table 1, we list the corresponding PI constructed from each method and the output predictors of each method. The number of output predictors directly reflects the computational cost. It is worth noting that acquiring the LOO predictor \(_{B}^{-i}\) needs substantial computation. In packages like randomForest, aggregating tree predictors to obtain the OOB predictor \(_{B}^{ i}\) also needs extra computation. However, the predicted value \(_{B}^{ i}(X_{i})\) can be obtained immediately by calling the \(()\) function. The fact that the value of \(_{B}^{ i}(X)\) on a test point is _not_ needed further reduces the computational cost of JaB and our method, which only need one output RF predictor, and are more favorable computationally.

In Table 2, we list the coverage of the PI constructed from each method, as well as the additional conditions (beyond iid data) needed to achieve the coverage. Note that J+ does not require any additional conditions to achieve the coverage lower bound \(1-2\), but J+aB requires that the number of trees \(B\) be a Binomial random variable. For JS, J+aBS, and our method, stability is needed to achieve the coverage lower bound \(1--O()\). With additional mild assumptions, the coverage upper bound and asymptotic coverage of our method can be established. However, there is no guarantee of coverage for JaB without strong assumptions.

In summary, our theoretical work provides a series of coverage guarantees to a computationally feasible method for constructing PIs based on the RF algorithm. In the following, we will establish the lower and upper bound of coverage, as well as the asymptotic coverage.

### Non-asymptotic coverage guarantees

**Theorem 9** (Coverage lower bound).: _Suppose the RF predictor \(_{B}\) satisfies the stability condition as in Theorem 8. Then we have for a test point \((X,Y)\) that_

\[(Y_{B}(X) q_{n,}\{R_{i}+_{n,B}\})  1--_{1}-2}-2}.\] (11)

This result is established by starting from the analysis of an imaginary extended dataset \(=D\{(X,Y)\}\), where the test point is _assumed_ to be known. We denote \((X,Y)\) as \((X_{n+1},Y_{n+1})\) for convenience. For all points in \(\), consider the derandomized RF predictor \(}^{ i}\) that is built on \(n\) data

   Method &  &  \\  J+  & \(_{B}^{-i}\), \(i[n]\) & \([q_{n,}^{-}\{_{B}^{-i}(X)-R_{i}^{}\},q_{n,}^ {+}\{_{B}^{-i}(X)+R_{i}^{}\}]\) \\ J+aB  & \(_{B}^{ i},i[n]\) & \([q_{n,}^{-}\{_{B}^{ i}(X)-R_{i}\},q_{n,}^{+ }\{_{B}^{ i}(X)+R_{i}\}]\) \\ JS  & \(_{B}\) and \(_{B}^{-i},i[n]\) & \(_{B}(X) q_{n,}\{R_{i}^{}+\}\) \\ J+aBS  & \(_{B}^{ i},i[n]\) & \([q_{n,}^{-}\{_{B}^{ i}(X)-R_{i}\}-,q_{n, }^{+}\{_{B}^{ i}(X)+R_{i}\}+]\) \\ JaB & \(_{B}\) & \(_{B}(X) q_{n,}\{R_{i}\}\) \\  & & \(_{B}(X) q_{n,}^{}\{R_{i}\}\) \\ Ours (JaBS) & \(_{B}\) & \(_{B}(X) q_{n,}\{R_{i}+\}\) (Theorem 9) \\  & & \(_{B}(X) q_{n,}\{R_{i}-\}\) (Theorem 10) \\  & & \(_{B}(X) q_{n,}\{R_{i}\}\) (Theorem 11) \\   

Table 1: Methods to construct prediction intervals using random forests: computational cost

   Method & Theoretical coverage & Additional conditions \\  J+  & \( 1-2\) & None \\ J+aB  & \( 1-2\) & Binomial \(B\) \\ JS  & \( 1--O\) & Stability (algorithmic) \\ J+aBS  & \( 1--O\) & Stability (ensemble + algorithmic) \\ JaB & No guarantee  & - \\  & \( 1-\) & Strong (additive model, consistency of RF predictor) \\ Ours (JaBS) & \( 1--O\) & Stability (Theorem 9) \\  & \( 1-++O()\) & \(+\) Distinct residuals (Theorem 10) \\  & \( 1-\) & \(+\) Uniformly equicontinuous CDF of \(|Y-_{B}(X)|\) \\  & & and vanishing \(,\) (Theorem 11) \\   

Table 2: Methods to construct prediction intervals using random forests: theoretical coverage points without the \(i\)th point in \(\), \(i[n+1]\). One can then define the OOB error \(_{i}|Y_{i}-}^{}|\). Since all data are iid, we have that \((_{n+1} q_{n,}\{_{i}\}) 1-\), where \(q_{n,}\{_{i}\}\) is the \([(1-)(n+1)]\)-th smallest value of \(\{_{1},,_{n}\}\). Next, notice \(_{n+1}=|Y_{n+1}-}^{(n+1)}(X_{n+1})|=|Y _{n+1}-(X_{n+1})|\) by the definitions of \(}^{(n+1)}\) and \(\). By concentration of measure, \((X_{n+1})\) can be approximated by \(_{B}(X_{n+1})\), and thus \(_{n+1}\) can be roughly replaced with \(|Y_{n+1}-_{B}(X_{n+1})|\), which is desired. Then by stability of \(\), \(\{_{i}\}\) can be approximated by \(\{r_{i}|Y_{i}-^{}(X_{i})|\}\). Although \(\{r_{i}\}\) is still unavailable in practice, by applying the idea of concentration of measure again, \(\{r_{i}\}\) can be further approximated by \(\{R_{i}\}\), which is accessible given \(D\). Eventually, we can bound \(|Y_{n+1}-_{B}(X_{n+1})|\) in terms of \(\{R_{i}\}\). The approximations are accounted for by the stability parameters in Theorem 8.

If we further assume that there are no ties among \(\{_{i}\},i[n+1]\), a typical case when \(Y\) is continuous, then we can also establish the upper bound of coverage.

**Theorem 10** (Coverage upper bound).: _Suppose there are no ties in \(\{_{i}\},i[n+1]\), and the RF predictor \(_{B}\) satisfies the stability condition as in Theorem 8. Then_

\[(Y_{B}(X) q_{n,}\{R_{i}-_{n,B}\} ) 1-++_{1}+2}+2}.\] (12)

The upper bound can be established because if there are no ties among \(_{1},,_{n+1}\), then \((_{n+1} q_{n,}\{_{i}\}) 1- +\). The apparent symmetry between the lower and upper bound originates from the fact that they both are established by using the RF stability once and the concentration of measure twice. Note that this idea can be applied to JS intervals for an arbitrary stable algorithm in exactly the same way, providing a complement to the lower bound for JS intervals established in .

**Corollary 3** (Coverage upper bound for jackknife-with-stability intervals).: _Let \(\) be a predictor trained on \(n\) iid data points and \(^{-i}\) be the LOO predictor without the \(i\)th point. Suppose \(\) is stable with \((|(X)-^{-i}(X)|>)\), and the LOO errors are distinct on the extended training set that includes an iid test point \((X,Y)\). Then we have \((|Y-(X)| q_{n,}\{r_{i}-\})  1-++2\), where \(r_{i}\) are the LOO errors on the original training set._

### Asymptotic coverage guarantee

As shown above, the stability parameters \((_{n,B},_{n,B})\) can vanish when \(n\). It is reasonable to expect that \((Y_{B}(X) q_{n,}\{R_{i}\}) 1-\) in this limit, as is consistent with numerous empirical observations [17; 35]. However, to achieve this goal, it seems that more assumptions are unavoidable. In , the guaranteed coverage of the JaB method is established by assuming that \(_{B}(X)\) converges to some \(f_{0}(X)\) in probability as \(n\), where \(f_{0}\) is the true regression function of an additive model that generates the data. We show that this can be done under weaker conditions.

**Theorem 11** (Asymptotic coverage).: _Denote \(F_{n}\) as the CDF of \(|Y-_{B}(X)|\). Suppose \(\{F_{n}\}_{n n_{0}}\) is uniformly equicontinuous for some \(n_{0}\). Then \((Y_{B}(X) q_{n,}\{R_{i}\}) 1-\) as \(n\) when conditions in Theorem 9, Theorem 10, and Corollary 2 are satisfied._

**Remark 6**.: Intuitively, using errors from \(_{B}^{}\) that are trained on \(n-1\) points to approximate those from \(_{B}\), trained on \(n\) points, we only need this approximation to be exact asymptotically. There is no need for \(_{B}\) itself to converge to anything. This is one major conceptual difference between our work and , and it is in this sense that our assumption is weaker. Practically, this kind of PI is recommended as it does not involve \((_{n,B},_{n,B})\), and has great performance on numerous datasets.

## 5 Conclusion

In this work, for the first time, we theoretically establish the stability property of the greedy version of random forests, which is implemented in popular packages. The theoretical guarantee is based on a light-tail assumption of the marginal distribution of the squared response \(Y^{2}\). However, numerical evidence suggests that this stability could persist in much broader situations. Based on the stability property and some mild conditions, we also establish finite-sample lower and upper bounds of coverage, as well as the exact coverage asymptotically, for prediction intervals constructed from the out-of-bag error of random forests, justifying random forests as an appealing method to provide both point and interval prediction simultaneously.