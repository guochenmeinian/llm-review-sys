# Benchmarking Robustness of Adaptation Methods on Pre-trained Vision-Language Models

Shuo Chen\({}^{1,3}\)1 Jindong Gu\({}^{2}\)2 Zhen Han\({}^{1}\)2 Yunpu Ma\({}^{1,3}\) Philip Torr\({}^{2}\) Volker Tresp\({}^{1,4}\)

\({}^{1}\)Institute of Informatics, LMU Munich \({}^{2}\)Department of Engineering Science, University of Oxford

\({}^{3}\) Siemens AG \({}^{4}\)Munich Center for Machine Learning (MCML)

shuo.chen@campus.lmu.de, jindong.gu@eng.ox.ac.uk, hanzhen02111@163.com

equal contributioncorresponding author

###### Abstract

Various adaptation methods, such as LoRA, prompts, and adapters, have been proposed to enhance the performance of pre-trained vision-language models in specific domains. As test samples in real-world applications usually differ from adaptation data, studying the robustness of these adaptation methods against distribution shifts is essential. In this study, we assess the robustness of 11 widely-used adaptation methods across 4 vision-language datasets under multimodal corruptions. Concretely, we introduce 7 benchmark datasets, including 96 visual and 87 textual corruptions, to investigate the robustness of different adaptation methods, the impact of available adaptation examples, and the influence of trainable parameter size during adaptation. Our analysis reveals that: 1) Adaptation methods are more sensitive to text corruptions than visual corruptions. 2) Full fine-tuning does not consistently provide the highest robustness; instead, adapters can achieve better robustness with comparable clean performance. 3) Contrary to expectations, our findings indicate that increasing the number of adaptation data and parameters does not guarantee enhanced robustness; instead, it results in even lower robustness. We hope this study could benefit future research in developing robust multimodal adaptation methods. The benchmark, code, and dataset used in this study can be accessed at https://adarobustness.github.io.

## 1 Introduction

Employing large-scale pre-training of vision-language (VL) models has become the standard for work on VL tasks . These models are typically trained in a self-supervised manner on unlabeled web-scale datasets in a general domain . To address the domain-specific challenges and improve performance on downstream tasks, various model adaptation methods have been proposed .

Although adaptation methods can achieve promising results on various VL benchmark datasets, real-world applications often introduce various distribution shifts that differ from the conditions encountered during model adaptation . For instance, these shifts can manifest as variations in lighting conditions in images and typos in texts. Therefore, it is critical to ensure model robustness against distribution shifts, particularly in safety-critical applications where unexpected wrong decisions can have severe consequences, such as self-driving systems  and clinical diagnostics . However, robustness research for multimodal models is still rare, leaving many essential questions unanswered: Which adaptation methods perform better on which tasks in terms of both performance and robustness? How robust are the various multimodal adaptation methods againstvisual corruptions, language corruptions, or both? Will more examples or more trainable parameters assure better robustness?

To this end, this work investigates the robustness of various adaptation methods on VL models to answer the above research questions. Concretely, we introduce a diverse set of 96 visual corruptions, including _impulse noise_, _snow_ etc., and 87 textual corruptions encompassing _text addition_, _back translation_, etc. Moreover, extensive experiments have been conducted on 11 adaptation methods across 4 VL datasets, including VQAv2 , GQA , NLVR\({}^{2}\) and MSCOCO Caption . While several studies have explored the robustness of VL models, our work represents a significant advancement as it provides the first large-scale benchmark robustness analysis of existing adaptation methods on VL models. We limit the models and tasks to those related to images, specifically, multimodal-to-text models, e.g., CLIP-BART . Video-language models are outside the scope of this research.

Our analysis reveals several interesting findings: 1) Adaptation methods demonstrate a higher degree of sensitivity towards text corruptions compared to visual corruptions. 2) Full fine-tuning does not consistently yield the best relative robustness, whereas an adapter can achieve better robustness with comparable performance. 3) Surprisingly, our experiments reveal that a large quantity of adaptation data and model parameters do not guarantee improved robustness. In fact, increasing the amount of adaptation data might even lead to decreased robustness. 4) There is no single adaptation method that surpasses others across all tasks and corruptions. To summarize, our contributions are as follows:

* We construct a suite of 7 large-scale robustness benchmark datasets including 96 visual corruptions and 87 textual corruption methods.
* We evaluate the robustness of 11 adaption methods on VL models based on massive experiments.
* We release the benchmark, code, as well as a leaderboard to the community to facilitate future research on the robustness of multimodal adaptation methods.

## 2 Related Work

**Vision-language Models.** Pre-trained VL models [41; 60; 7; 45; 40; 64; 13; 79] have shown outstanding performance on various VL tasks. Some use contrastive learning to align visual features with language representations and achieve surprising zero-shot performance [42; 54]. However, contrastive learning-based methods are limited to close-ended tasks and are inflexible. Another line of work follows BERT's  pretrain-then-finetune paradigm [41; 60; 7; 64]. They treat object features extracted using pre-trained object detectors  as visual words sent to language models . For example, VL-BART  uses BART  or T5  as the text encoder and Faster-RCNN  as the visual backbone. Unlike other methods, VL-BART unifies VL tasks via a single text generation task. CLIP-BART  follows the same idea as VL-BART but adopts the CLIP  image encoder

Figure 1: Multimodal adaptation methods are sensitive to image and text corruptions. The two rows show image captioning and visual question answering predicted by Adapter , respectively. Blue boxes contain the original image and query text. Orange boxes present the corrupted images, texts and model output.

to extract pixel-level features. Recent approaches [66; 14; 1] follow such a unified view and freeze large language models (LLMs) to utilize the in-context learning ability of LLMs. However, as shown in , LM fine-tuning is still crucial to achieve competitive performance on various downstream VL tasks. In this study, we follow the work in  that benchmarks model adaptation methods. CLIP-BART  is selected as our VL model, given its generation flexibility and unified architecture.

**Model Adaptation Methods.** To enhance the performance of pre-trained VL models on downstream tasks and avoid infeasible computation, various adaptation methods have been proposed. Existing methods can be classified into three categories : (1) adding a few trainable parameters while freezing other model parts [29; 46; 43; 33; 36]; (2) updating a few model parameters sparsely [25; 63; 76]; and (3) low-rank factorization of parameters to be updated, such as in LoRA . Adapters  belong to the first category and have been widely used in vision, language, and multimodal models [29; 78; 6]. Other representative methods in the first category include Hyperformers , Computers , and prompt-tuning [36; 3]. Although numerous adaptation methods have been proposed and widely adopted, their robustness against distribution shifts remains understudied.

**Natural Robustness.** The robustness of deep learning models against distribution shifts is critical for real-world applications [17; 26; 16]. Regarding vision robustness, researchers have investigated image classification models [26; 16; 28; 57; 21; 22], semantic segmentation [32; 24], object detection , video classification , and transformer-based architectures [12; 51; 52; 23; 73]. In the field of natural language processing (NLP), many robustness analysis toolboxes [58; 59; 72; 19], and various methods [15; 10; 50; 9] are available. The robustness investigation on multimodal models is gaining more attention but related studies are lacking. The literature includes the robustness of multimodal fusion models , audio-visual models , text-to-image generative models , text-to-video retrieval models , as well as image-text models .

In contrast to all the works above, our study focuses on _the robustness of adaptation methods integrated into large pre-trained vision-language models_. Understanding their robustness on different VL tasks will facilitate the design of more robust adaptation methods for multimodal models.

## 3 Preliminaries of Model Adaptation Methods

The pretrain-then-finetune paradigm on large models has shown dominant performance on multimodal tasks [41; 11; 54], yet the prohibitive costs of full fine-tuning have spurred intensive research efforts towards developing parameter-efficient adaptation methods [62; 30; 36; 29; 46; 33]. As the transformer architecture  is used for most state-of-the-art large pre-trained models, adaptation methods mainly focus on tweaking the input or the intermediate layers of the attention layers inside the large models. Formally, given a pre-trained large-scale model \(F\) parameterized by \(\), we need to adapt \(F\) on a task-specific dataset \(\), e.g., a VQA dataset, to get the adapted model \(F^{}\). Then, we can obtain the output \(=F^{}(;)\) by providing an input \(=\{x_{1},,x_{n}\}\) with \(n\) tokens from \(\). Adaptation methods differ in how they interact with \(F(;)\) (Fig. 2). In general, full fine-tuning updates all \(\). Prompt  concatenates the input \(\) with an extra prefix. LoRA  introduces modifications to the update mechanism of the model parameters \(\) and adapters [29; 46; 33] modify the intermediate output and input of \(\).

**Full fine-tuning** directly updates the whole \(\) on \(\) and becomes prohibitive due to the rapidly growing model size. Therefore, the following adaptation methods have been developed to achieve comparable performance while optimizing only a few parameters.

**Prompt-based adaptation** concatenates the input \(\) with either a trainable prefix (soft prompt)  or a manually designed prefix . For the given input \(=\{x_{1},,x_{n}\}\) with \(n\) tokens, the pre-trained model will first form an embedding matrix \(^{n d}\) where \(d\) is the dimension of the embedding space. Soft-prompts  are then represented as a learnable parameter \(^{p d}\), where \(p\) is the length of the prompt. Next, \(\) is concatenated with the original embedded input \(\) to form a new single matrix defined as \([]^{(p+n) d}\). During adaptation, the model is trained to maximize the probability of the desired output while only updating \(\).

**LoRA** utilizes low-rank decomposition matrices to update parameters. For intermediate model parameters \(_{0}^{d k}\), such as the parameters from a self-attention module in the transformer architecture, its update \(_{0}\) is represented by a low-rank decomposition \(_{0}=,^{d r},^{r k},r min(d,k)\). During adaptation, \(_{0}\) is frozen while \(\) and \(\) are updated.

**Adapter-based adaptation** inserts sub-networks with a few learnable parameters into the large model. **Adapter** consists of a pair of downsampling and upsampling layers as well as a residual connection. Suppose the original input to an intermediate layer \(_{0}\) in model \(F\) is \(_{0}^{d_{0}}\), adapters insert a downsampling layer \(^{D}^{d_{0} d_{1}}\) and an upsampling layer \(^{U}^{d_{1} d_{0}}\), where \(d_{0},d_{1}\) are dimensions of the hidden embeddings, respectively. The output after injecting adapters is defined as \(=f_{^{U}}((f_{^{D}}(_{0} )))+_{0}\), where \(f_{^{U}}\) denotes a function parameterized by \(^{U}\) and \(()\) is an activation function such as GELU . To further reduce redundant parameters in adapters, **Compacter** decomposes parameter matrices. It introduces _parameterized hypercomplex multiplication_ (PHM) layers \(^{D}=_{i=1}^{k}_{i}_{i},_ {i}^{k k},_{i}^{}{k} }{k}}\), which decompose the layer in the adapter by Kronecker products. Compacter also shares the parameter of \(A_{i}\) across all layers and decomposes \(B_{i}\) even further with low-rank decomposition. However, as found in , such sharing and further decomposition severely decreases the VL performance. In our study, we only use PHM layers. **Hyperformer** relies on a hyper-network shared across tasks to generate the weights in adapters given a task index and a layer index. The hyperformer maintains learnable embeddings for each task and each layer. For \(N_{T}\) tasks and \(N_{L}\) layers, the learnable embeddings in the hyperformer is denoted as \(_{1},,_{N_{T}}^{d_{z}}\) and \(_{1},,_{N_{L}}^{d_{e}}\), respectively. The hyperformer consists of a task projector \(^{T}^{(d_{e}+d_{e}) d_{p}}\) and a hyper-network \(^{H}^{d_{p}(d_{0} d_{1}+d_{1} d_{0})}\), and generates an adapter's weights in the \(i^{th}\) layer for the \(j^{th}\) task following \([^{D},^{U}]=f_{^{H}}(f_{^{T}}([_{j},_{i}]))\).

**Adaptation shared over tasks** further reduces redundant parameters by exploiting similar information shared across multiple tasks. In a multi-VL-task setting, an intuitive way is to train the adaptation modules per task using: **Multiple Adapters, Multiple Compacters, Multiple LoRA**, and **Multiple Prompts**. Additionally, We can train only one set of adapter layers for all tasks, and we have **Single Adapter, Single Compacter, Single LoRA**, and **Single Prompt**. Besides, **Half-shared adapter** only shares the upsampling layers or downsampling layers across different tasks. Detailed information is presented in Appendix B.2.

## 4 Corruption Methods

**Image Corruptions.** We use the corruption methods from ImageNet-C  and [4; 53]. A _blank_ method is also added, which is used to examine the importance of visual information to VL models. _Blank_ corruption turns the original image into a blank picture by setting all pixel values to 255. All image corruptions can be categorized into five groups: **noise**, **blur**, **weather**, **digital**, and **extra**. Specifically, we use 20 image corruption methods, (1) **noise**: _impulse noise_, _Gaussian noise_, _shot noise_, _speckle noise_; (2) **blur**: _zoom blur_, _defocus blur_, _motion blur_, _frosted glass blur_; (3) **digital**: _JPEG compression_, _contrast_, _elastic_, _spatter_, _suratate_, _pixelate_; (4) **weather**: _snow_, _frost_, _fog_, _brightness_; and (5) **extra**: _blank_. We follow the severity convention in ImageNet-C  and define 5 levels of severity for each method, except for _blank_ corruption. In total, we have 96 types of visual corruption and we leave the details in the Appendix A.1. By applying all image corruptions to 4 datasets used in this study, i.e., VQAv2 , GQA , NLVR\({}^{2}\) and MSCOCO Caption , we construct 4 out-of-distribution (OOD) benchmark datasets.

Figure 2: Illustration of adaptation methods used in our study. Green areas indicate trainable parameters whereas frozen parameters are in blue. The input is the concatenation of image and text embedding \([emb_{img}:emb_{text}]\) and the output is the generated text.

**Text Corruptions.** In addition to visual feature shifts, text corruptions are also essential for evaluating the robustness of vision-language models [4; 53]. We have incorporated a total of 35 corruption methods, inspired by the approaches presented in [72; 53; 4]. These methods can be categorized into three groups based on the level of corruption: **character-level, word-level**, and **sentence-level**. Furthermore, they are further subdivided into six sub-categories, namely _character modification, text style modification, text addition, dropping text based on POS, positional drop,_ and _text swap_. To name a few examples, the category _text style_ transforms sentences to desired styles such as _passive, formal_, or _double negative_. _Text addition_ inserts extra words, like adverbs in _InsertAdv. Text drop_ modifies words based on POS tagging, dropping nouns (_DropNN_) or verbs (_DropVB_). For detailed information, please refer to Appendix A.2. In addition to the above corruptions, Qiu et al. proposed in  that we should ensure that the corrupted text has the same semantics as the original one to make sure the image-text pairs remain meaningful. We follow this setting and use the same fidelity guarantee mechanism as . Various severity levels for text corruptions are also introduced in the benchmark, including 5 severity levels on character-level corruptions and some word-level corruptions. For sentence-level corruptions, only one perturbation is available. In total, we have 87 different perturbations. After applying all text corruptions on VQAv2 , GQA , and NLVR\({}^{2}\), we construct another 3 OOD benchmark datasets. In the end, we have constructed 7 OOD robustness benchmark datasets to fully investigate the robustness of adaptation methods on vision-language models.

## 5 Experimental Settings

**Tasks and Datasets.** The popular representative VL tasks (visual question answering, visual reasoning, and image captioning) and 4 well-known VL datasets are applied in this work. For visual question answering, VQAv2  and GQA  are adopted. Additionally, we incorporate NLVR\({}^{2}\) for visual reasoning and MSCOCO  for image captioning. The statistics are shown in Table 1.

**Models.** CLIP-BART (T5)  serves as our base model. Because the model adaptation on VL models is mainly on language model components and the encoder-decoder architecture can tackle VL tasks via a unified text-generation task . CLIP-BART (T5) utilizes a single-stream fusion scheme, where the language model takes the concatenation of visual representations and text representations as input. The single-stream approach enables the model to effectively integrate visual and textual information, leveraging the complementary strengths of both modalities. Specifically, CLIP-ResNet101  is the vision encoder that receives resized \(224 224\) images, and representations from the last convolutional layer are extracted as the visual features. BART\({}_{base}\) and T5\({}_{base}\) deal with the downstream text generation task.

**Model Adaptation Methods.** We investigate the robustness of four mainstream adaptation methods: full fine-tuning, soft prompt , LoRA , and adapter-based methods, including Adapter , Hyperformer , and Compacter . To better understand their robustness, shared adaptation methods are also investigated. Specifically, for soft prompt, i.e., LoRA, and Compacters, we conduct experiments in both single and multiple manners and the half-shared manner for Adapter (Section 3). See Appendix B.2 for detailed information, e.g., training strategy and hyperparameters.

**VL Task Evaluation Metrics.** Accuracy on the Karpathy-test split is evaluated for VQAv2. For GQA, accuracy on the test-dev split is evaluated, and accuracy on the test-P split is used for NLVR\({}^{2}\). In image captioning, we use CIDEr  on the Karpathy-test split.

**Robustness Evaluation Protocol.** The model performance \(P_{I}\) on \(D_{I}\) (i.e., in-distribution test datasets) and \(P_{O}\) on \(D_{O}\) (i.e., out-of-distribution test datasets) are first evaluated, where \(P\) is the corresponding evaluation metric for each task, such as CIDEr  for image caption. Then, the

    &  &  & ^{2}\)**} &  \\  The Number of & Images & QA pairs & Images & QA pairs & Images & QA pairs & Images & Captions \\ Training set & 113.2K & 605.1K & 72.1K & 943.0K & 103.2K & 86.4K & 113.2K & 566.8K \\ Validation set & 5.0K & 26.7K & 10.2K & 132.1K & 8.1K & 7.0K & 5.0K & 5.0K \\ Test set & 5.0K & 26.3K & 398 & 12.6K & 8.1K & 7.0K & 5.0K & 5.0K \\   

Table 1: Dataset Statistics.

**Relative Robustness \(RR=1- P/P_{I}\)**[53; 4] is computed based on the clean performance \(P_{I}\) and corrupted performance \(P_{O}\), where \( P=(P_{I}-P_{O})\). \(RR\) is a score ranging from \(0\) to \(1\), where \(RR=1\) indicates that \(F\) is totally robust and \(R=0\) means that \(F\) is not robust at all. The RR with severity 5 is reported across the main paper; detailed scores on others are in Supplementary.

## 6 Results and Analysis

Sec. 6.1 examines the robustness of each adaptation method and tries to answer the first question: _Which adaptation methods perform better on which tasks with respect to both performance and robustness?_ Sec. 6.2 compares the robustness sensitivity on image and text corruptions and looks for the answer to _how robust are the various multimodal adaptation methods against visual corruptions, language corruptions, or both?_ In Section 6.3, we analyze the influence on robustness given different sizes of adaptation data and trainable parameters. Especially, we aim to answer _will more examples or more parameters ensure better adaptation robustness?_

### Robustness of Multimodal Adaptation Methods

Full fine-tuning, prompt-tuning, LoRA, and adapter-based methods are four types of adaptation methods investigated in this study, and their relative robustness against image and text corruptions are presented in Table 2.The reported relative robustness is the average value across all images or text corruption methods. **Although full fine-tuning generally achieves higher clean performance, our analysis reveals that its robustness is comparatively weaker than other adaptation methods.** In many cases, the adapter and hyperformer achieve better robustness with much fewer parameters and comparable clean performance. For instance, full fine-tuning's RR against text corruptions on the VQAv2 dataset is the smallest, for both CLIP-BART and CLIP-T5. Prompt tuning, despite exhibiting

  
**Adaptation method** & Updated &  &  & ^{2}\)**} &  \\ _Image Corruptions_ & Params & Acc (\%) & RR (\%) & Acc (\%) & RR (\%) & Acc (\%) & RR (\%) & CIDEr & RR (\%) \\  Full Fine-tuning & 100\% & 66.75 & 84.86\(\)5.17 & 55.04 & 89.20\(\)0.04 & 73.01 & 90.34\(\)0.04 & 115.03 & 68.40\(\)0.14 \\ Multiple Adapters & 12.22\% & 65.30 & 85.33\(\)4.90 & 53.98 & 86.16\(\)0.04 & 69.41 & **92.02\(\)**0.04 & 114.47 & 68.72\(\)0.14 \\ Half-shared Adapters & 8.36\% & 65.20 & 85.18\(\)5.01 & 52.96 & 89.37\(\)0.04 & 70.03 & 91.72\(\)0.04 & 114.50 & 68.45\(\)0.14 \\ Single Adapter & 4.18\% & 65.35 & **85.76\(\)**5.32 & 54.14 & 82.49\(\)0.04 & 73.89 & 90.04\(\)0.05 & 115.04 & 68.68\(\)0.14 \\ Hyperformer & 5.79\% & 65.38 & 85.38\(\)4.84 & 52.52 & 90.05\(\)0.04 & 72.21 & 90.13\(\)0.05 & 114.89 & 68.74\(\)0.14 \\ Multiple Compactors & 7.05\% & 64.91 & 85.65\(\)4.81 & 52.75 & 88.89\(\)0.04 & 69.45 & 91.33\(\)0.04 & 115.16 & 68.67\(\)0.13 \\ Single Compactor & 2.70\% & 64.47 & 85.47\(\)4.96 & 52.90 & 82.62\(\)0.04 & 69.94 & 92.04\(\)0.04 & 113.06 & **69.92\(\)**0.13 \\ Multiple LoRA & 17.72\% & 65.44 & 84.78\(\)4.86 & 52.05 & **91.15\(\)**0.04 & 51.32 & – & 115.41 & 68.47\(\)0.14 \\ Single LoRA & 5.93\% & 65.34 & 84.78\(\)4.81 & 53.19 & 82.58\(\)0.04 & 73.58 & 90.05\(\)0.04 & 114.54 & 69.26\(\)0.13 \\ Multiple Prompts & 4.53\% & 46.81 & – & 34.01 & – & 49.87 & – & 108.62 & 67.70\(\)0.14 \\ Single Prompt & 2.00\% & 44.00 & – & 37.54 & – & 51.95 & – & 103.70 & 68.56\(\)0.13 \\    
  
**Adaptation method** & Updated &  &  & ^{2}\)**} \\ _Text Corruptions_ & Params & Acc (\%) & RR (\%) & Acc (\%) & RR (\%) & Acc (\%) & RR (\%) \\  Full Fine-tuning & 100\% & 66.75 & 73.65\(\)22.38 & 55.04 & 66.92\(\)24.14 & 73.01 & 87.06\(\)11.00 \\ Multiple Adapters & 12.22\% & 65.30 & 76.62\(\)20.06 & 53.39 & 66.93\(\)22.43 & 69.41 & **90.14\(\)**10.19 \\ Half-shared Adapters & 8.36\% & 65.20 & 76.78\(\)20.79 & 52.96 & 68.20\(\)24.78 & 70.03 & 89.16\(\)0.12 \\ Single Adapter & 4.18\% & 65.35 & **77.64\(\)**21.09 & 54.14 & 67.47\(\)20.03 & 73.89 & 88.49\(\)0.87 \\ Hyperformer & 5.79\% & 65.38 & 75.06\(\)21.29 & 52.52 & **70.30\(\)**23.13 & 72.21 & 87.27\(\)11.27 \\ Multiple Compactors & 7.05\% & 64.91 & 77.10\(\)20.05 & 52.75 & 67.39\(\)23.29 & 69.45 & 90.00\(\)9.76 \\ Single Compactor & 2.70\% & 64.47 & 77.17\(\)20.40 & 52.90 & 67.90\(\)20.33 & 69.94 & 90.10\(\)9.81 \\ Multiple LoRA & 17.72\% & 65.44 & 74.04\(\)21.97 & 52.05 & 68.77\(\)22.76 & 51.32 & – \\ Single LoRA & 5.93\% & 65.34 & 74.50\(\)21.42 & 53.19 & 63.94\(\)20.99 & 73.58 & 87.64\(\)11.04 \\ Multiple Prompts & 4.53\% & 46.81 & – & 34.01 & – & 49.87 & – \\ Single Prompt & 2.00\% & 44.00 & – & 37.54 & – & 51.95 & – \\   

Table 2: Clean performance and relative robustness (RR) of adaptation methods based on CLIP-BART against image (up) and text (down) corruptions. RR and the corresponding standard deviation are averaged and calculated over all image or text corruption methods. The percentage of trainable parameters for each adaptation method is also reported. We strike out those high RRs with quite low performance. The best RR for each column is in bold.

high robustness, fails to perform well on the clean test dataset. The same conclusions can be drawn on corrupted data with different corruption levels, as shown in Supplementary. Please note that we have excluded robustness scores associated with very low task performance in Table 2.

**Single Adapter vs Full Fine-tuning.** Previous research  has shown that a single adapter can achieve comparable performance on the four tasks with significantly fewer parameters than full fine-tuning. When it comes to robustness, _a single adapter is comparable to or slightly better than full fine-tuning on VQAv2, NLVR\({}^{2}\), and MSCOCO Caption given image corruptions._ The same goes for text corruption. For example, as shown in the 4th row and 1st row in Table 2 (lower panel), a single adapter on CLIP-BART achieves an average RR of \(77.64\%\) against text corruptions on VQAv2, while full fine-tuning's RR is \(73.65\%\). However, on GQA, a single adapter is less robust than full fine-tuning. Full fine-tuning achieves an average RR of \(89.20\%\) against image corruptions, while the RR of a single adapter is only \(82.49\%\). Also, a single adapter's clean accuracy on GQA is lower than full fine-tuning's \(55.04\%\). In contrast, multiple and half-shared adapters have more parameters but achieve better robustness on the four tasks than a single adapter. **In conclusion, a single adapter can achieve similar or better robustness on VQAv2, NLVR\({}^{2}\), and MSCOCO Caption compared to full fine-tuning. On GQA, multiple and half-shared adapters are better.**

**Adapter-based Methods.** Although training multiple tasks with one set of adapter layers has the least number of parameters, _such a single setting might hinder the robustness on certain tasks_. For instance, Single Adapter's robustness on GQA against image corruptions (\(82.49\%\)) is lower than that of the half-shared (\(89.37\%\)) and multiple settings (\(86.16\%\)). This also applies to Single LoRA and Single Compacter. An explanation could be that the half-shared mechanism does not only learn more general representation across tasks; it also maintains task-specific knowledge. On GQA, Single LoRA's robustness against image corruptions is lower by \(8.57\%\) compared to Multiple LoRA's, and the robustness against text corruptions is lower by \(4.83\%\). However, compared with multiple settings of LoRA and Adapter, the _Hyperformer has relatively fewer parameters but achieves comparable or better robustness_. Hyperformer on CLIP-T5 also obtains the best robustness results against image corruptions on all four tasks.

**Robustness against Natural Dataset Distribution Shift.** To provide a more realistic assessment of the robustness, an additional natural distribution shift corruption is included in our work. VQA-RAD  is a manually constructed dataset where clinicians asked naturally occurring questions about radiology images and provided reference answers. The images in VQA-RAD are shown neither in model pre-training nor in adaptation and can be seen as a natural out-of-distribution dataset. Results in Fig. 3 are relatively low but adapters perform relatively well, such as the Single Adapter in CLIP-BART and Multiple Adapters in CLIP-T5. Full finetuning fails to generalize well in CLIP-BART compared to other adaptation methods whereas the performance of CLIP-T5 with full finetuning is the second best.

**Vision-language Tasks.** Among all datasets, MSCOCO Caption is the most vulnerable one against image corruptions, where all adaptation methods have decreased on average more than \(30\%\). This is plausible as it only relies on visual information, whereas other tasks provide both visual and language information. Besides, GQA is the task with the lowest robustness performance against text corruptions. Moreover, _on GQA, the extreme single-module setting fails to achieve good robustness_, such as Single Adapter, Single LoRA, and Single Compacter. **This indicates that information sharing with the other two datasets may hinder the robustness on GQA.** To overcome such an issue, one can adopt the multiple-module manner or Hyperformer. Adaptation methods show better robustness on NLVR\({}^{2}\) compared to the other three tasks on both corruptions.

Figure 3: The accuracy results on VQA-RAD  which include 11 adaptation methods on CLIP-BART (left) and 6 on CLIP-T5 (right).

### Robustness Sensitivity to Image Corruptions and Text Corruptions

This work introduces both image and text corruptions to examine the robustness of various adaptation methods. **Our experimental findings suggest a potential vulnerability of adaptation methods on multimodal VL models to text corruptions, particularly those at the character level**. Across all three tasks, the adaptation methods exhibit lower robustness indicators against text corruptions. For instance, Single Adapter based on CLIP-BART has the best robustness result of \(85.76\%\) against image corruptions on VQAv2. However, although it is still the most robust adaptation method against text corruptions, the relative robustness is \(77.64\%\). _Among image corruptions, zoom blur drops the robustness the most, and within text corruptions, char-level methods are most challenging to these VL adaptation methods_. Detailed analysis is in Supplementary Section 6.

**Blank Image Corruption.**_Blank_ corruption evaluates the influence on the robustness of visual information in VL tasks. All datasets used in this study rely on visual information and only the MSCOCO Caption contains no language information. In _blank_ corruption, we set the pixel values of testing image data to \(255\), i.e. transforming the original image into a blank image. The results are shown in Fig. 4. The relative robustness on MSCOCO Caption is the lowest among all four datasets. This is plausible since image captioning relies only on visual information and is not supposed to perform well given a blank image. Apart from image captioning, adaptation methods on all three datasets could secure relative robustness exceeding \(50\%\) in the absence of useful visual information. Several questions within the VL tasks can be accurately answered without relying on visual information, suggesting that **language information plays a more significant role than visual information**. This also explains the higher sensitivity to text perturbations compared to the sensitivity to image corruptions.

**Compounding Effects of Multimodal Corruptions.** To assess the impact of compounding distribution shifts on both the visual and text modalities, we selected a subset of corruption methods from each category and presented the results in Fig. 5. Specifically, for text corruption methods, we selected _ocr_ at the character level, _swap syn word embd_ at the word level, and _back translation_ at the sentence level. For visual corruption methods, we selected _Gaussian noise_ in the noise category, _zoom blur_ in the blur category, _JPEG_ in the digital category, and _snow_ in the weather category. We tested the full fine-tuning and single adapter on CLIP-BART and CLIP-T5. The results demonstrate that combining corruptions from two modalities can lead to a greater drop in robustness. Additionally, the results show similar trends as the single-modal corruptions. Character-level corruptions still lead to the most severe performance drop compared

Figure 4: Relative robustness (%) of adaptation methods based on CLIP-BART (left) and CLIP-T5 (middle) against _blank_ corruption. We group MSCOCO Caption results from CLIP-BART and CLIP-T5 together in the right sub-figure. We omit two bars in NLVR\({}^{2}\) from the middle figure as multiple adapters and multiple compacters did not perform well.

Figure 5: The average performance drop given both visual and text corruptions. A darker color indicates a severe performance drop.

to sentence- and word-level corruptions. _Zoom blur_ still drops the robustness the most among image corruptions.

### The Influence of Adaptation Data Size and Parameter Size on Robustness

**Adaptation Data Size.** Adaptation methods are gaining more attention due to their efficient fine-tuning manner compared to full fine-tuning. We take a further step and evaluate their performance and robustness given different sizes of training data during adaptation. Fig. 6 compares the performance and robustness of full fine-tuning and single adapter. Given more adaptation data, performance in all tasks has a steady increase, and in most cases, the performance of full fine-tuning surpasses single adapter's performance, while the latter can achieve comparable performance given the whole adaptation data. Only on MSCOCO Caption, single adapter outperforms full fine-tuning given a subset of adaptation data. Single adapter achieves better RR compared to full fine-tuning against both image and text corruptions but has a robustness drop on GQA. Fig. 6 also demonstrates the robustness of other adaptation methods. All lines present a steady declining tendency, which indicates that **increasing the size of the adaptation data does not consistently enhance relative robustness**. Besides, the blue full fine-tuning lines take the lowest position on all three datasets given text corruptions and on VQAv2, NLVR\({}^{2}\), and MSCOCO Caption given image corruptions. In comparison to other adaptation methods, _full fine-tuning has relatively lower relative robustness, despite having the most trainable parameters_. The last conclusion is that **there is no single adaptation method that surpasses others across all tasks and corruptions** and all methods share a similar robustness fluctuation given a different size of the adaptation dataset.

**Adaptation Parameter Size.** Fig. 7 presents RR and clean performance of prompt-tuning given different soft prompt lengths added to the concatenated embeddings. There is a steady increase in the performance on four tasks with longer prompt lengths which proves that prompt methods perform better given more parameters. Regarding relative robustness, such a steady increase does not apply to all tasks, and **longer soft prompts do not ensure better relative robustness**. Fig. 7 also shows the experimental results from the other 4 adaptation methods given different sizes of trainable parameters. The results demonstrate that **more parameters do not ensure enhanced robustness and some even reduce it**, such as the single parameter and single adapter on GQA.

## 7 Discussion and Conclusion

This study focuses on the robustness of adaptation methods on pre-trained vision-language models and provides 7 benchmark datasets containing 96 visual and 87 textual corruptions. We systematically inspect the robustness of 11 adaptation methods on 4 popular VL datasets and conclude that: 1) these adaptation methods are more sensitive towards text corruptions compared to visual corruptions, 2) full fine-tuning does not achieve the best robustness; instead, adapters demonstrate better robustness while maintaining comparable performance, 3) surprisingly, more adaptation data and more parameters do

Figure 6: The first row represents the clean performance and relative robustness of full fine-tuning and single adapter on CLIP-BART given the different sizes of the adaptation dataset. Green lines stand for performance in each task and the orange is robustness. The second row is relative robustness given the different sizes of the adaptation dataset. The X-axis shows the random subset ratio of the training dataset during adaptation, ranging from \(20\%\) to \(100\%\).

not ensure a better robustness. In fact, it can even lead to worse robustness, 4) there is currently no adaptation method achieving both the best performance and best robustness across all corruptions and all tasks. The main limitation of this work is that the analysis is on a limited number of multimodal models due to the availability of usable code, model weights, and massive experiments. Potential future work includes investigating more diverse pre-trained VL models, designing more robust adaptation methods, and integrating future model adaptation methods to make our benchmark up-to-date.

This work is partially supported by the UKRI grant: Turing AI Fellowship EP/W002981/1 and EPSRC/MURI grant: EP/N019474/1. We would also like to thank the Royal Academy of Engineering and FiveAI. We also gratefully acknowledge the computational and data resources provided by the Leibniz Supercomputing Centre (www.lrz.de).