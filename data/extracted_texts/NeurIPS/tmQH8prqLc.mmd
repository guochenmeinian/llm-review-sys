# Adaptive Variance Reduction for Stochastic Optimization under Weaker Assumptions

Wei Jiang\({}^{1}\), Sifan Yang\({}^{1,2}\), Yibo Wang\({}^{1,2}\), Lijun Zhang\({}^{1,2,}\)

\({}^{1}\)National Key Laboratory for Novel Software Technology, Nanjing University, Nanjing, China

\({}^{2}\)School of Artificial Intelligence, Nanjing University, Nanjing, China

{jiangw, yangsf, wangyb, zhanglj}@lamda.nju.edu.cn

Lijun Zhang is the corresponding author.

###### Abstract

This paper explores adaptive variance reduction methods for stochastic optimization based on the STORM technique. Existing adaptive extensions of STORM rely on strong assumptions like bounded gradients and bounded function values, or suffer an additional \(( T)\) term in the convergence rate. To address these limitations, we introduce a novel adaptive STORM method that achieves an optimal convergence rate of \((T^{-1/3})\) for non-convex functions with our newly designed learning rate strategy. Compared with existing approaches, our method requires weaker assumptions and attains the optimal convergence rate without the additional \(( T)\) term. We also extend the proposed technique to stochastic compositional optimization, obtaining the same optimal rate of \((T^{-1/3})\). Furthermore, we investigate the non-convex finite-sum problem and develop another innovative adaptive variance reduction method that achieves an optimal convergence rate of \((n^{1/4}T^{-1/2})\), where \(n\) represents the number of component functions. Numerical experiments across various tasks validate the effectiveness of our method.

## 1 Introduction

This paper investigates the stochastic optimization problem

\[_{^{d}}f(),\] (1)

where \(f:^{d}\) is a smooth non-convex function. We assume that only noisy estimations of its gradient \( f()\) can be accessed, denoted as \( f(;)\), where \(\) represents the random sample drawn from a stochastic oracle such that \([ f(;)]= f()\).

Problem (1) has been comprehensively investigated in the literature [Duchi et al., 2011, Kingma and Ba, 2015, Loshchilov and Hutter, 2017], and it is well-known that the classical stochastic gradient descent (SGD) achieves a convergence rate of \((T^{-1/4})\), where \(T\) denotes the iteration number [Ghadimi and Lan, 2013]. To further improve the convergence rate, variance reduction methods have been developed, and attain an improved rate of \((T^{-1/3})\) under a slightly stronger smoothness assumption [Fang et al., 2018, Wang et al., 2019]. However, these methods necessitate the use of a huge batch size in each iteration, which is often impractical to use. To eliminate the need for large batches, a momentum-based variance reduction method -- STORM [Cutkosky and Orabona, 2019] is introduced, which achieves a convergence rate of \((T^{-1/3} T)\).

Although aforementioned methods are equipped with convergence guarantees, their analyses rely on delicate configurations of hyper-parameters, such as the learning rate and the momentum parameter. To set them properly, the algorithm typically needs to know the value of the smoothness parameter \(L\)the gradient upper bound \(G\), and the variance upper bound \(\), which are often unknown in practice. Specifically, most algorithms require the learning rate \(_{t}\) smaller than \((1/L)\), and for the STORM method, setting the momentum parameter to \((L^{2}_{t}^{2})\) is crucial for ensuring convergence (Cutkosky and Orabona, 2019).

To overcome this limitation, many adaptive algorithms have been developed, aiming to obtain convergence guarantees without prior knowledge of problem-dependent parameters such as \(L\), \(G\) and \(\). Based on the STORM method, Levy et al. (2021) develop the STORM+ algorithm, which attains the optimal \((T^{-1/3})\) convergence rate under the assumption of bounded function values and gradients. To remove the need for the bounded function values assumption, Liu et al. (2022) propose the META-STORM algorithm to attain an \((T^{-1/3} T)\) convergence rate, but it still requires the bounded gradients assumption and includes an additional \(( T)\) term. In summary, despite advancements in this field, existing adaptive STORM-based methods either depend on strong assumptions or suffer an extra \(( T)\) term compared with the lower bound (Arjevani et al., 2023). Hence, a fundamental question to be addressed is:

_Is it possible to develop an adaptive STORM method that achieves the optimal convergence rate for non-convex functions under weaker assumptions?_

We give an affirmative answer to the above question by devising a novel optimal Adaptive STORM method (Ada-STORM). The learning rate of our algorithm is set to be inversely proportional to a specific power of the iteration number \(T\) in the initial iterations, and then changes adaptively based on the cumulative sum of past gradient estimations. In this way, we are able to adjust the learning rate dynamically according to the property of stochastic gradients, and ensure a small learning rate in the beginning. Leveraging this strategy, Ada-STORM achieves an optimal convergence rate of \((T^{-1/3})\) for non-convex functions. Notably, our analysis does not require the function to have bounded values and bounded gradients, which is a significant advancement over existing methods (Levy et al., 2021; Liu et al., 2022). Additionally, our convergence rate does not contain the extra \(( T)\) term, which is often present in STORM-based methods (Cutkosky and Orabona, 2019; Liu et al., 2022). To highlight the versatility of our approach and its potential impact in the field of stochastic optimization, we further extend our technique to develop optimal adaptive methods for compositional optimization.

Finally, we investigate adaptive variance reduction for the non-convex finite-sum problems. Inspired by SAG algorithm (Roux et al., 2012), we incorporate an additional term in the STORM estimator, which measures the difference of past gradients between the selected component function and the overall objective. By changing the learning rate according to the sum of past gradient estimations, we are able to obtain an optimal convergence rate of \((n^{-1/4}T^{-1/2})\) for finite-sum problems in an adaptive manner, where \(n\) is the number of component functions. Our result is better than the previous convergence rate of \((n^{-1/4}T^{-1/2}(nT))\) obtained by adaptive SPIDER method (Kavis et al., 2022). In summary, compared with existing methods, this paper enjoys the following advantages:

* For stochastic non-convex optimization, our method achieves the optimal convergence rate of \((T^{-1/3})\) under more relaxed assumptions. Specifically, it does not require the bounded function values or the bounded gradients, and does not include the additional \(( T)\) term in the convergence rate.
* Our learning rate design and the analysis exhibit broad applicability. We substantiate this claim by obtaining an optimal rate of \((T^{-1/3})\) for stochastic compositional optimization, using the technique proposed in this paper.
* For non-convex finite-sum optimization, we further improve our adaptive algorithm to attain an optimal convergence rate of \((n^{1/4}T^{-1/2})\), which outperforms the previous result by eliminating the \(((nT))\) factor.

A comparison between our method and other STORM-based algorithms is shown in Table 1. Numerical experiments on different tasks also validate the effectiveness of the proposed method.

## 2 Related work

This section briefly reviews related work on stochastic variance reduction methods and adaptive stochastic algorithms.

### Stochastic variance reduction methods

Variance reduction has been widely used in stochastic optimization to reduce the gradient estimation error and thus improve the convergence rates. The idea of variance reduction can be traced back to the SAG algorithm (Roux et al., 2012), which incorporates a memory of previous gradient values to ensure variance reduction and achieves a linear convergence rate for strongly convex finite-sum optimization. To avoid the storage of past gradients, SVRG (Zhang et al., 2013; Johnson and Zhang, 2013) proposes to calculate the full gradient periodically, obtaining the same convergence rate as the SAG algorithm. Subsequent advancement has been made by the SARAH method (Nguyen et al., 2017), which derives better convergence for smooth convex functions.

In the context of non-convex objectives, Fang et al. (2018) introduce the SPIDER estimator, which improves the convergence rate from \((T^{-1/4})\) to \((T^{-1/3})\) in stochastic settings, and to \(O(n^{1/4}T^{-1/2})\) in finite-sum scenarios, with \(n\) representing the number of components in the finite-sum. Following this, the SpiderBoost algorithm (Wang et al., 2019) refines the SPIDER approach by employing a larger constant step size and adapting it for composite optimization problems. However, a common limitation among these methods is their reliance on large batch sizes for each iteration, posing practical challenges due to high computational demands. To mitigate this issue, Cutkosky and Orabona (2019) introduce the STORM method, a momentum-based technique that achieves an \((T^{-1/3} T)\) convergence rate without using large batches. Concurrently, Tran-Dinh et al. (2019) obtain the same result using a similar algorithm but through a different analysis.

### Adaptive stochastic algorithms

For stochastic optimization, it is well-known that the SGD algorithm can obtain a convergence rate of \((T^{-1/4})\) for non-convex objective functions with well-designed learning rates (Ghadimi and Lan, 2013). Instead of using pre-defined iteration-based learning rates, many stochastic methods propose to adjust the learning rate based on past stochastic gradients. One of the foundational works is the AdaGrad algorithm (Duchi et al., 2011), which proves to be effective for sparse data. Further advancements include RMSprop (Tieleman and Hinton, 2012) and Adam (Kingma and Ba, 2015), demonstrating broad effectiveness across a wide range of machine learning problems. Later, the Super-Adam (Huang et al., 2021) algorithm further improves the Adam algorithm via the variance reduction technique STORM (Cutkosky and Orabona, 2019) and obtains a convergence rate of \((T^{-1/3} T)\). Nevertheless, to obtain the corresponding convergence rates, these methods still require knowledge of certain problem-dependent parameters to set hyper-parameters accurately, hence

   Method & Setting & Convergence Rate & Adaptive & BG/BF \\  STORM (Cutkosky and Orabona, 2019) & NC & \((T^{-1/3} T)\) & ✗ & ✓ \\ Super-ADAM (Huang et al., 2021) & NC & \((T^{-1/3} T)\) & ✗ & – \\ STORM+ (Levy et al., 2021) & NC & \((T^{-1/3})\) & ✓ & ✓ \\ META-STORM (Liu et al., 2022) & NC & \((T^{-1/3} T)\) & ✓ & ✓ \\ 
**Theorem 1, 2** & NC & \((T^{-1/3})\) & ✓ & – \\
**Theorem 3** & NC \& Comp. & \((T^{-1/3})\) & ✓ & – \\
**Theorem 4** & NC \& FS & \((n^{1/4}T^{-1/2})\) & ✓ & – \\   

Table 1: Summary of results for STORM-based methods. Here, NC denotes non-convex, Comp. indicates compositional optimization, FS represents finite-sum optimization, and BG/BF refers to requiring bounded gradients or bounded function values assumptions. Adaptive means the method does not require to know problem-dependent parameters, i.e., \(L\), \(G\), and \(\).

not adaptive.2 To solve this problem, many research aims to develop fully adaptive SGD methods that maintain the optimal convergence rate without knowing problem-specific parameters (Orabona, 2014; Chen et al., 2022; Carmon and Hinder, 2022; Ivgi et al., 2023; Yang et al., 2023).

Recently, adaptive adaptations of STORM have received considerable attention. A notable development is the introduction of STORM+ (Levy et al., 2021), which presents a fully adaptive version of STORM while attaining an optimal convergence rate. To circumvent the bounded function values assumption in STORM+, the META-STORM (Liu et al., 2022) approach is developed, equipped with a nearly optimal bound. However, META-STORM still requires the bounded gradients assumption, and it includes an additional \(( T)\) in the convergence rate. Consequently, adaptive STORM with the optimal convergence rate and under mild assumptions still needs further explorations.

## 3 Adaptive variance reduction for non-convex optimization

In this section, we develop an adaptive STORM method for non-convex functions. We first outline the assumptions used, and then present our proposed method and analyze its convergence rate.

### Assumptions

We introduce the following assumptions, which are standard and commonly adopted in the stochastic optimization (Nguyen et al., 2017; Fang et al., 2018; Cutkosky and Orabona, 2019; Li et al., 2021).

**Assumption 1**: _(Average smoothness)_

\[[\| f(;)- f(;) \|^{2}] L^{2}\|-\|^{2}.\]

**Assumption 2**: _(Bounded variance)_

\[[\| f(;)- f()\| ^{2}]^{2}.\]

**Assumption 3**: \(f_{*}=_{}f()-\) _and \(f(_{1})-f_{*}_{f}\) for the initial solution \(_{1}\)._

Note that some additional assumptions are required in other STORM-based methods. Specifically, STORM (Cutkosky and Orabona, 2019), STORM+ (Levy et al., 2021), and META-STORM (Liu et al., 2022) assume the bounded gradients. Moreover, STORM+ makes an additional assumption of the bounded function values.

### The proposed method

In this subsection, we aim to develop an adaptive STORM method that achieves an optimal convergence rate for non-convex functions under weaker assumptions. Our algorithm framework is the same as the original STORM (Cutkosky and Orabona, 2019), and the only difference is the setup of the momentum parameter \(_{t}\) and the learning rate \(_{t}\). First, we present the STORM algorithm in Algorithm 1.

The core idea of STORM lies in a carefully devised variance reduced estimator \(_{t}\), which effectively tracks the gradient \( f(_{t})\). For the first iteration (\(t=1\)), we set \(_{1}=_{i=1}^{B_{0}}} f(_{1};_{1} ^{i})\), which is estimated within a batch \(B_{0}=T^{1/3}\). Note that we use large batch only in the first iteration, and constant batch size in other iterations. In subsequent iterations (\(t 2\)), estimator \(_{t}\) is defined as:

\[_{t}=(1-_{t})_{t-1}+_{t} f( _{t};_{t})+(1-_{t})( f(_{t}; _{t})- f(_{t-1};_{t})),\] (2)

where the first two terms are similar to the momentum SGD, and the last term serves as the error correction, which ensures the variance reduction effect. By choosing the values of \(_{t}\) and \(_{t}\) carefully, STORM ensures that the estimation error \([\|_{t}- f(_{t})\|^{2}]\) would decrease gradually. In the original STORM paper, these parameters are set up as:

\[_{t}=^{t}\| f(_{t};_{t} )\|^{2})^{1/3}},_{t}=c_{t}^{2},\]

where \(k=(G^{2/3}L^{-1})\), \(w=(G^{2})\) and \(c=(L^{2})\). The settings of these hyper-parameters are crucial to the convergence analysis of STORM. However, it is worth noting that \(L\) is the smoothness parameter and \(G\) is the gradient upper bound, which are often difficult to determine in practice.

To address this problem, our approach defines the hyper-parameters as follows:

\[_{t}=\{},(_{i= 1}^{t}\|_{i}\|^{2})^{}}\},_{ t}==T^{-2/3},\] (3)

where \(0<<1/3\). Notably, our method does not rely on the parameters \(L\) and \(G\), and also does not need the bounded gradients or bounded function values assumptions that are common in other methods. Although our formulation initially requires knowledge of the iteration number \(T\), this can be effectively circumvented using the doubling trick, which will be explained later. The above learning rate \(_{t}\) can also be expressed in an alternative, more illustrative manner:

\[_{t}=\{}&_{i=1}^{t} \|_{i}\|^{2} T^{1/3};\\ \\ (_{i=1}^{t}\|_{i}\|^{2} )^{}}&..\]

This formulation ensures that the learning rate starts sufficiently small in the initial stages and then changes dynamically based on the gradient estimator \(_{t}\). This design makes our learning rate setup and convergence analysis distinctly different from previous methods. Next, we present the following theoretical guarantee for our algorithm.

**Theorem 1**: _Under Assumptions 1, 2 and 3, Algorithm 1 with hyper-parameters in equation (3) guarantees that:_

\[[\| f(_{})\|](^{(1-)}}+^{}+L^{}}{T^{1/3}}).\]

**Remark:** To ensure that \([\| f(_{})\|]\), the overall complexity is \((^{-3})\), which is known to be optimal up to constant factors (Arjevani et al., 2023). Compared with existing STORM-based algorithms (Cutkosky and Orabona, 2019; Levy et al., 2021; Liu et al., 2022), our method does not have the extra \(( T)\) term in the convergence rate, and our analysis does not require bounded gradients or bounded function values assumptions. Also note that the selection of \(\) does not affect the order of \(T\), and larger \(\) leads to better dependence on parameter \(L\) and worse reliance on parameters \(\) and \(\). Considering we require that \(0<<1/3\), we can simply set \(=0.3\) in practice.

### The doubling trick

While we have attained the optimal convergence rate using the proposed adaptive STORM method, it requires knowing the total number of iterations \(T\) in advance. Here, we show that we can avoid this requirement by using the doubling trick, which divides the algorithm into several stages and increasesthe iteration number in each stage gradually. Specifically, we design a multi-stage algorithm over \(k=\{1,2,,K\}\) stages. At the beginning of each new stage, we reset \(_{t}=_{0}\). In each stage \(k\), the STORM algorithm is executed for \(2^{k-1}\) iterations, effectively doubling the iteration numbers after each stage. In any step \(t\), we first identify the current stage as \(1+ t\) and then calculate the iteration number for this stage as \(I_{t}=2^{ t}\). Then, we can set the hyper-parameters as:

\[_{t}=\{^{1/3}},^{(1-)/3}( _{i=I_{t}}^{t}\|_{i}\|^{2})^{}}\}, _{t}=I_{t}^{-2/3}, I_{t}=2^{ t}.\] (4)

This approach eliminates the need to predetermine the iteration number \(T\). By using the doubling trick, we can still obtain the same optimal convergence rate as stated in the following theorem.

**Theorem 2**: _Under Assumptions 1, 2 and 3, Algorithm 1 with hyper-parameters in equation (4) guarantees that:_

\[[\| f(_{})\|](^{}+^{}+L ^{}}{T^{1/3}}).\]

## 4 Extension to stochastic compositional optimization

To demonstrate the broad applicability of our proposed technique, we extend it to stochastic compositional optimization [Wang et al., 2017, 2019, Yuan et al., 2019, Zhang and Xiao, 2019, 2021, Jiang et al., 2023, 2024a], formulated as:

\[_{^{d}}F()=f(g()),\] (5)

where \(f\) and \(g\) are smooth functions. We assume that we can only access to unbiased estimations of \( f()\), \( g()\) and \(g()\), denoted as \( f(;)\), \( g(;)\) and \(g(;)\). Here \(\) and \(\) symbolize the random sample drawn for a stochastic oracle such that \([ f(;)]= f()\), \([g(;)]=g()\), and \([ g(;)]= g()\).

Existing variance reduction methods [Hu et al., 2019, Zhang and Xiao, 2019, Qi et al., 2021] are able to obtain optimal \((T^{-1/3})\) convergence rates for problem (5), but they require the knowledge of smoothness parameter and the gradient upper bound to set up hyper-parameters. In this section, we aim to achieve the same optimal convergence rate without prior knowledge of problem-dependent parameters. We develop our adaptive algorithm for this problem as follows. In each step \(t\), the algorithm maintains an inner function estimator \(_{t}\) in the style of STORM, i.e.,

\[_{t}=(1-)_{t-1}+g(_{t};_{t})-(1- )g(_{t-1};_{t}).\] (6)

Then, we construct a gradient estimator \(_{t}\) based on \(_{t}\) also in the style of STORM:

\[_{t}=(1-)_{t-1}+ f(_{t};_{t})  g(_{t};_{t})-(1-) f(_{t-1};_{t }) g(_{t-1};_{t}).\] (7)

After that, we apply gradient descent using the gradient estimator \(_{t}\). The whole algorithm is presented in Algorithm 2, and hyper-parameters are set the same as in equation (3). For the first iteration, we simply set \(_{1}=_{i=1}^{B_{0}}}g(_{1};_{1}^{i})\) and \(_{1}=_{i=1}^{B_{0}}} f(_{1};_{1 }^{i}) g(_{1};_{1})\), where \(B_{0}=T^{1/3}\). Next, we list common assumptions used in the literature of compositional optimization [Wang et al., 2017, 2019, Yuan et al., 2019, Zhang and Xiao, 2019, 2021].

**Assumption 4**: _(Average smoothness and Lipschitz continuity)_

\[[\| f(;)- f(;)\|^{2}]  L\|-\|^{2};\;[\|f( ;)-f(;)\|^{2}] C\|- \|^{2};\] \[[\| g(;)- g( ;)\|^{2}]  L\|-\|^{2};\;[\|g( ;)-g(;)\|^{2}] C\|- \|^{2}.\]

**Assumption 5**: _(Bounded variance)_

\[[\|g(;)-g()\|^{2}] ^{2};[\| g(;)- g( )\|^{2}]^{2};[\| f( ;)- f()\|^{2}]^{2}.\]

**Assumption 6**: \(F_{*}=_{}F()-\) _and \(F(_{1})-F_{*}_{F}\) for the initial solution \(_{1}\)._

**Remark:** In Assumption 4, we further require standard Lipschitz continuity assumption, which is essential and widely required in the literature for stochastic compositional optimization (Wang et al., 2017; Yuan et al., 2019; Jiang et al., 2022, 2022). This assumption is inherently introduced by the compositional optimization itself rather than by our adaptive techniques.

With the above assumptions, our algorithm enjoys the following guarantee.

**Theorem 3**: _Under Assumptions 4, 5 and 6, our Algorithm 2 ensures that:_

\[[\| F(_{})\|](T^{-1/3}).\]

**Remark:** This rate matches the state-of-the-art (SOTA) results in stochastic compositional optimization (Hu et al., 2019; Zhang and Xiao, 2019; Qi et al., 2021), and our method achieve this in an adaptive manner. Note that our convergence rate aligns with the lower bound for single-level problems (Arjevani et al., 2023) and is thus unimprovable.

## 5 Adaptive variance reduction for finite-sum optimization

In this section, we further improve our adaptive variance reduction method to obtain an enhanced convergence rate for non-convex finite-sum optimization, which is in the form of

\[_{^{d}}F()=_{i=1}^{n}f_{i }(),\]

where each \(f_{i}()\) is a smooth non-convex function. Existing adaptive method for this problem (Kavis et al., 2022) achieves a convergence rate of \((n^{1/4}T^{-1/2}(nT))\) based on the variance reduction technique SPIDER (Fang et al., 2018), suffering from an extra \(((nT))\) term compared with the corresponding lower bound (Fang et al., 2018; Li et al., 2021).

To obtain the optimal convergence rate for finite-sum optimization, we incorporate techniques from the SAG algorithm (Roux et al., 2012) into the STORM estimator. Specifically, in each step \(t\), we start by randomly sample \(i_{t}\) from the set \(\{1,2,,n\}\). Then, we construct a variance reduction gradient estimator as

\[_{t}=(1-)_{t-1}+ f_{i_{t}}(_{t})-(1- ) f_{i_{t}}(_{t-1})-(g_{t}^{i_{t}}- _{i=1}^{n}g_{t}^{i}),\] (8)

where the first three terms align with the original STORM method, and the last term, inspired by the SAG algorithm, deals with the finite-sum structure. Here, \(g_{t}\) tracks the gradient as

\[g_{t+1}^{i}=\{ f_{i_{t}}(_{t})&i=i_{t }\\ g_{t}^{i}&i i_{t}..\] (9)

By such a design, we can ensure that the estimation error \([\|_{t}- F(_{t})\|^{2}]\) reduces gradually. The whole algorithm is stated in Algorithm 3. In this case, we set the hyper-parameters as:

\[_{t}=}(_{i=1}^{t}\| _{i}\|^{2})^{}},=,\]where \(0<<1/3\). The learning rate \(_{t}\) is non-increasing and changes according to the gradient estimations, and the momentum parameter \(\) remains unchanged throughout the learning process. Next, we show that our method enjoys the optimal convergence rate under the following assumptions, which are standard and widely adopted in existing literature (Fang et al., 2018; Wang et al., 2019; Li et al., 2021).

**Assumption 7**: _(Smoothness) For each \(i\{1,2,,m\}\), function \(f_{i}\) is \(L\)-smooth such that_

\[\| f_{i}()- f_{i}()\| L\|- \|.\]

**Assumption 8**: \(F_{*}=_{}F()-\) _and \(F(_{1})-F_{*}_{F}\) for the initial solution \(_{1}\)._

**Theorem 4**: _Under Assumptions 7 and 8, our Algorithm 3 guarantees that:_

\[[\| F(_{})\|]( }{T^{1/2}}(_{F}^{}+L^{})).\]

**Remark:** Our result matches the lower bound for non-convex finite-sum problems (Fang et al., 2018; Li et al., 2021), and makes an improvement over the existing adaptive method, i.e., AdaSpider (Kavis et al., 2022). Specifically, the convergence rate of the AdaSpider algorithm is \((n^{1/4}T^{-1/2}(L^{2}+_{F})(1 +nTL))\), and our result is better than theirs when \(<<\).

We can avoid storing past gradients by following the SVRG method (Zhang et al., 2013; Johnson and Zhang, 2013) to compute the full gradient periodically and incorporate it into STORM estimator. Instead of storing the past gradients as in SAG algorithm, we can avoid this storage cost by incorporating elements from the SVRG method. Specifically, we compute a full batch gradient at the first step and every \(I\) iteration (we set \(I=n\)):

\[ f(_{})=_{i=1}^{n} f_{i}(_ {}).\]

For other iterations, we randomly select an index \(i_{t}\) from the set \(\{1,2,,n\}\) and compute:

\[_{t}=(1-)_{t-1}+ f_{i_{t}}(_{t})-(1- ) f_{i_{t}}(_{t-1})-( f_{i_{t}}(_{})- f(_{})).\] (10)

Note that the first three terms match the original STORM estimator, and the last term, inspired from SVRG, deals with the finite-sum structure. Compared with equation (8) in Algorithm 3, the difference is that we use \(( f_{i_{t}}(_{})- f(_{}))\) instead of \((g_{t}^{i_{t}}-_{i=1}^{n}g_{t}^{i})\) in the last term. The detailed procedure is outlined in Algorithm 4. This strategy maintains the same optimal rate, as stated below:

**Theorem 5**: _Under Assumptions 7 and 8, our Algorithm 4 guarantees that:_

\[[\| F(_{})\|]( }{T^{1/2}}(_{F}^{}+L^{})).\]

**Remark:** The obtained convergence rate is in the same order as the results in Theorem 4, and Algorithm 4 does not require storing past gradients anymore.

## 6 Experiments

In this section, we evaluate the performance of the proposed Ada-STORM method via numerical experiments on image classification tasks and language modeling tasks. In the experiments, we compare our method with STORM [Cutkosky and Orabona, 2019], STORM+ [Levy et al., 2021] and META-STORM [Liu et al., 2022], as well as SGD, Adam [Kingma and Ba, 2015] and AdaBelief [Zhuang et al., 2020]. We use the default implementation of SGD and Adam from Pytorch [Paszke et al., 2019]. For STORM+, we follow its original implementation3, and build STORM, META-STORM and our Ada-STORM based on it. When it comes to hyper-parameter tuning, we simply set \(=0.3\) for our algorithm. For other methods, we either set the hyper-parameters as recommended in the original papers or tune them by grid search. For example, we search the learning rate of SGD, Adam and AdaBelief from the set \(\{1e-5,1e-4,1e-3,1e-2,1e-1\}\) and select the best one for each method. All the experiments are conducted on eight NVIDIA Tesla V100 GPUs.

Image classification taskFirst, we conduct numerical experiments on multi-class image classification tasks to evaluate the performance of the proposed method. Specifically, we train ResNet18 and ResNet34 models [He et al., 2016] on the CIFAR-10 and CIFAR-100 datasets [Krizhevsky, 2009] respectively. For all optimizers, we set the batch size as 256 and train for 200 epochs. We plot the loss value and the accuracy against the epochs on the CIFAR-10 and CIFAR-100 datasets in Figure 1 and Figure 2. It is observed that, for training loss and training accuracy, our Ada-STORM algorithm achieves comparable performance with respect to other methods, and it outperforms the others in terms of testing loss and thus obtains a better testing accuracy.

Language modeling taskThen, we perform experiments on language modeling tasks. Concretely, we train a 2-layer Transformer [Vaswani et al., 2017] over the WiFi-Text2 dataset [Merity, 2016]. We use \(256\) dimensional word embeddings, \(512\) hidden unites and 2 heads. The batch size is set as \(20\) and all methods are trained for 40 epochs with dropout rate \(0.1\). We also clip the gradients by norm \(0.25\) in case of the exploding gradient. We report both the loss and perplexity versus the number of epochs in Figure 3. From the results, we observe that our method converges more quickly than other

Figure 1: Results for CIFAR-10 dataset.

methods and obtains a slightly better perplexity compared with others, indicating the effectiveness of the proposed method.

## 7 Conclusion

In this paper, we propose an adaptive STORM method to achieve the optimal convergence rate for non-convex functions. Compared with existing methods, our algorithm requires weaker assumptions and does not have the additional \(( T)\) term in the convergence rate. The proposed technique can also be employed to develop optimal adaptive algorithms for compositional optimization. Furthermore, we investigate an adaptive method for non-convex finite-sum optimization, obtaining an improved convergence rate of \((n^{1/4}T^{-1/2})\). Given that STORM algorithm has already been used in many areas such as bi-level optimization (Yang et al., 2021), federated learning (Das et al., 2022), min-max optimization (Xian et al., 2021), sign-based optimization (Jiang et al., 2024), etc., the proposed methods may also inspire the development of adaptive algorithms in these fields.