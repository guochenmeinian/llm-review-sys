# Invariant subspaces and PCA in nearly matrix multiplication time

Aleksandros Sobczyk

IBM Research and ETH Zurich

obc@zurich.ibm.com &Marko Mladenovic

ETH Zurich

mmladenovic@ethz.ch &Mathieu Luisier

ETH Zurich

mluisier@iis.ee.ethz.ch

###### Abstract

Approximating invariant subspaces of generalized eigenvalue problems (GEPs) is a fundamental computational problem at the core of machine learning and scientific computing. It is, for example, the root of Principal Component Analysis (PCA) for dimensionality reduction, data visualization, and noise filtering, and of Density Functional Theory (DFT), arguably the most popular method to calculate the electronic structure of materials. Given Hermitian \(,^{n n}\), where \(\) is positive-definite, let \(_{k}\) be the true spectral projector on the invariant subspace that is associated with the \(k\) smallest (or largest) eigenvalues of the GEP \(=\), for some \(k[n]\). We show that we can compute a matrix \(}_{k}\) such that \(\|_{k}-}_{k}\|_{2}\), in \(O(n^{+}(n,^{-1},( ),_{k}^{-1}))\) bit operations in the floating point model, for some \((0,1)\), with probability \(1-1/n\). Here, \(>0\) is arbitrarily small, \( 2.372\) is the matrix multiplication exponent, \(()=\|\|_{2}\|^{-1}\|_{2}\), and \(_{k}\) is the gap between eigenvalues \(k\) and \(k+1\). To achieve such provable "forward-error" guarantees, our methods rely on a new \(O(n^{+})\) stability analysis for the Cholesky factorization, and a smoothed analysis for computing spectral gaps, which can be of independent interest. Ultimately, we obtain new matrix multiplication-type bit complexity upper bounds for PCA problems, including classical PCA and (randomized) low-rank approximation.

## 1 Introduction

Generalized eigenvalue problems (GEPs) arise naturally in a plethora of applications in machine learning, scientific computing, and engineering. Given a pair of matrices \(\) and \(\), often referred to as a _matrix pencil_, the problem of interest has the following form

\[=,\] (1)

where \(\) and \(\) are the unknown eigenvector and eigenvalue matrices, respectively. Of particular importance are the so-called "Hermitian definite" or simply "definite" GEPs/pencils, in which case \(\) is Hermitian and \(\) is Hermitian and positive-definite. In many important applications, the quantity of interest is an (arbitrarily large) subset of eigenvectors, defining an _invariant subspace_, rather than the entire \(\) and \(\) solutions of the GEP.

In data science and machine learning, invariant subspaces play a central role in many problems, including Spectral Clustering , Language Models , Image Processing , Recommendation Systems , Principal Components Analysis (PCA) , Support Vector Machines , and many others . We particularly focus on PCA applications, which can take the form of a GEP as in Eq. (1) where \(\) is the sample covariance and \(\) the identity. In more advanced settings, \(\) and \(\) can be defined over a kernel ; See Section 4 and Appendix G for more details. Another closely related application comes from Density FunctionalTheory  (DFT), which is not a machine learning problem per se, but it is probably the most commonly used method (it was awarded the Nobel prize in Chemistry in 1998) to compute the electronic and structural properties of materials. In this case, \(\) is the Hamiltonian and \(\) the overlap matrix (cf. Appendix F). The spectral projector on the invariant subspace corresponding to the smallest generalized eigenvalues (occupied energies) directly provides the density matrix and the electron density. Obtaining them often presents a challenge from the computational point of view.

### Problem definition

The main focus of this work is the computation of _spectral projectors_ on invariant subspaces that are associated with a subset of the spectrum of Hermitian definite GEPs. As the Abel-Ruffini theorem excludes exact computation, even in exact arithmetic, we seek for approximate computations, as described in the following Problem 1.1.

**Problem 1.1** (Spectral projector).: _Given a Hermitian definite GEP \(=\) of size \(n\), an integer \(1 k n-1\), and accuracy \((0,1)\), compute a matrix \(}_{k}^{n n}\) such that_

\[\|}_{k}-_{k}\|,\] (2)

_where \(_{k}\) is the true spectral projector on the invariant subspace associated with the \(k\) smallest or largest eigenvalues._

Before proposing algorithms to solve Problem 1.1, we first make some clarifications and define useful concepts.

Type of approximation:The approximation of the form of Equation (2) is commonly called a "forward approximation" or "forward error" in numerical analysis. It quantifies the distance between the true solution of the problem and the one returned by an algorithm. It is a stronger and harder to achieve notion of approximation than the related "backward error." For details see Appendix A.1.

Model of computation:While many finite precision models of computation exist in the literature, all algorithms in this work are analyzed in the floating point model of computation, which is also the prominent model implemented in existing computers. Each real number \(\) is rounded to a floating point number \(()=(1+)\), where \(\) satisfies \(||_{>0}\). The machine precision \(\) bounds also the errors introduced by arithmetic operations \(\{+,-,,/\}\), and the expression \((1/)\) gives the number of bits required to achieve the desired precision. More details can be found in Appendix A.2.

Bit complexity:The complexity of numerical algorithms is often measured in terms of the arithmetic operations executed, commonly referred to as _arithmetic complexity_. A more realistic notion is the _bit complexity_, which bounds the number of boolean operations. In the floating point model, it is straightforward to translate the arithmetic to the bit complexity if we have an upper bound on the number of bits. For instance, arithmetic operations on \(b\) bits can be typically carried out in \(O(b^{2})\) bit operations, or even faster by using more advanced algorithms [123; 55; 68].

Matrix multiplication time:In two seminal works [39; 40], it was demonstrated that matrix multiplication and other fundamental problems in Numerical Linear Algebra can be solved in the floating point model with nearly \(O(n^{+})\) bit-complexity (up to polylogarithmic factors), where \(\) is an arbitrarily small positive number and \(\) is the matrix multiplication exponent, to-date bounded by \( 2.372\)[47; 141; 6]. Hereafter, we will use the notation \(T_{}(n)=n^{+}\).

### Existing algorithms

Here we give a brief overview of existing algorithms. We refer to Appendix A.7 for more details. GEPs in general can be solved using classic eigensolvers and related techniques in \((n^{3})\) floating operations, e.g., by reducing the matrix (or pencil) to tridiagonal form with similarity transformations and applying the shifted QR algorithm on the tridiagonal matrix, or by using a divide-and-conquer method (see [37; 109; 75; 8; 9; 64; 42; 10; 113] and references therein). Significant progresses beyond the \((n^{3})\) bit complexity barrier have been made [40; 39; 15; 95; 25; 41; 107; 121].

Regarding the computation of eigenvalues, two notable examples are the \((T_{}(n))\) algorithm of  for the largest eigenvalue, and the \((n^{2})\) algorithm of  for the spectral norm.

The first to have addressed the problem of computing invariant subspaces in nearly \(O(T_{}(n))\) in floating point is  (see also ). The authors described an iterative algorithm for the Schur decomposition, and showed that each individual step is numerically stable, and it takes \(O(T_{}(n))\) operations. An end-to-end bound on the number of iterations to achieve a backward approximate solution was left open. More recently, the seminal work of  extended the analysis to obtain an end-to-end \((T_{}(n))\) complexity to approximately diagonalize a matrix, and  provided a rigorous analysis for the generalized eigenproblem case. In Corollary 1.7 and Proposition 1.1 of , it was also outlined how to translate the backward diagonalization error to a forward error for the eigenvectors. The reported bound, however, has two main limitations: it relies on simplicity of the spectrum, which is a strict assumption, and it requires as input an over-estimate on the eigenvector condition number of the problem, which is unknown, and  does not describe how to compute it (see Appendix B.1 for more details). In this work we describe how to overcome these limitations and provide a novel, end-to-end, provably accurate analysis (in the sense of Eq. (2)) for arbitrary invariant subspaces of definite GEPs with \((T_{}(n))\) boolean complexity.

### Contributions and methods

Our main contribution, summarized in the following Theorem 1.1 and Algorithm 1, is an end-to-end analysis to solve Problem 1.1 in nearly \(O(T_{}(n))\) time.

**Theorem 1.1**.: _Let \((,)\) be a Hermitian definite pencil of size \(n\) with \(\|\|,\|^{-1}\| 1\), \(_{1}_{2}_{n}\) its eigenvalues, \(_{k}=_{k+1}-_{k}\) and \(()=\|\|\|^{-1}\|\). Algorithm 1_

\[}_{k}(, ,k,),\]

_takes as inputs \(\), \(\), an integer \(k[n-1]\), an error parameter \((0,1)\) and returns a matrix \(}_{k}\) such that_

\[[\|}_{k}-_{k} \|] 1-1/n,\]

_where \(_{k}\) is the true spectral projector on the invariant subspace that is associated with the \(k\) smallest (or largest) eigenvalues. The algorithm executes_

\[O(T_{}(n)((_{k}})(_{k}})+(n())( ())+(()}{ _{k}}))))\]

_floating point operations with \(O((n)(^{4}(_{k}})+^{4}(n ())+^{3}(_{k}})( )}{_{k}})))\) bits of precision. Internally, the algorithm needs to generate a total of at most \((n)\) standard normal floating point numbers using additional \(O(((n)))\) bits._

To achieve the results of Theorem 1.1, we provide a novel \(O(T_{}(n))\)-type complexity analysis of several problems in numerical linear algebra that can be of independent interest.

In brief, our methodology is as follows. We first observe that if we can determine reasonable "guesses" for the spectral gap (\(}_{k}\)) and for the midpoint (\(_{k}\)) between the \(_{k}\) and \(_{k+1}\) eigenvalues then we can efficiently compute the spectral projector by approximating the sign function

\[(_{k}-^{-1}),\]

using the analysis of  for the Newton iteration. The matrix \((+(_{k}-^{-1 }))\) indeed transforms in exact arithmetic all eigenvalues that are smaller than \(_{k}\) to \(1\) and the ones larger than \(_{k}\) to zero. As will be proved in Proposition 2.1, in Section 2, this approach is sufficient to provide an accurate spectral projector \(}_{k}\) in floating point. As a consequence, the problem reduces to approximating the aforementioned midpoint and gap. As a baseline, in Appendix B.1 we prove that this can be done in nearly \(O(T_{}(n))\) with iterative inversion  and diagonalization  or, similarly, by iteratively calling generalized diagonalization . However, this approach presents two drawbacks: It does not take advantage of the inherent symmetry of the problem, and, at the same time, it performs a full diagonalization when we are only interested in the gap between two specific eigenvalues, which is seemingly redundant. We formally prove this claim by designing a novel approach that achieves better complexity, typically by a factor of \(O((n))\) (cf. Section3.4). Importantly, no explicit diagonalization is necessary.

To minimize the complexity of our algorithm, it is crucial to leverage symmetry. To that end we use the Cholesky factorization of \(\) in the spirit of the Cholesky-QR algorithm . We highlight that, while other factorizations have been solved in \(O(T_{}(n))\) in floating point, an end-to-end analysis for Cholesky remains open. In exact arithmetic, for example, the LU of a Hermitian definite matrix directly provides its Cholesky and  showed that the LU factorization of non-symmetric matrices can be obtained in \(O(T_{}(n))\). However, when considering arithmetic errors, the relationship between LU and Cholesky does not hold in floating point, as demonstrated by the counter-example of AppendixC.5. Other fast Cholesky algorithms have been proposed for special classes of matrices, e.g., for matrices with well-defined separators  and graph Laplacians . However, they do not generalize to arbitrary dense matrices. Our analysis is the first to improve the classic \(O(n^{3})\) floating point Cholesky algorithms  for the general case, with provable error bounds. In the following Theorem1.2 we summarize our new analysis of the Cholesky factorization Algorithm2 (see also AppendixC). We note that the algorithm itself is not new, only its analysis.

**Theorem 1.2**.: _Given a Hermitian positive-definite matrix \(\), there exists an algorithm \(()\), listed in Algorithm2, which requires \(O(T_{}(n))\) arithmetic operations. This algorithm is logarithmically stable, in a sense that, there exist global constants \(c_{1}\), \(c_{2}\), \(c_{3}\), such that for all \((0,1)\), if executed in a floating point machine with precision_

\[_{}:=n^{c_{2}} ()^{c_{3} n}},\]

_which translates into \(O((n)(())+())\) required bits of precision, then it does not break down due to arithmetic errors, and the solution returned satisfies \(\|^{*}-\|\|\|\)._

This stand-alone result fulfills the definition of "logarithmic-stability," a notion of numerical stability that is commonly used in the related literature . Given this new Cholesky analysis, the following transformation of the GEP to a regular Hermitian eigenvalue problem:

\[= ^{*}(^{-1})=(^{-1 }),\]

can be carried out accurately in \(O(T_{}(n))\) in floating point, with provable forward-error bounds for all eigenvalues of the transformed problem. Here, \(\) is the Cholesky factor of \(^{-1}\) instead of \(\). Specifically, in PropositionC.3 in AppendixC.4, we prove that the corresponding Algorithm4, \(}(,,)\), returns a Hermitian matrix \(}\) such that, for any given accuracy \((0,1)\),

\[|_{i}(})-_{i}(^{-1})|, i[n].\]

The symmetry induced by the Cholesky transformation is crucial to design an efficient algorithm for the spectral gap. As described in Section3, and analyzed in AppendicesD and E, any spectral gap or eigenvalue of a Hermitian definite pencil can be approximated by an iterative algorithm that uses only "counting-queries", i.e., queries that ask how many eigenvalues are smaller than a given threshold. This way we completely avoid diagonalization, thus leading to a lower complexity.

To perform the counting queries efficiently, the transformed matrix \(}\) must be regularized with small random perturbations, in the spirit of smoothed analysis , which has recently drawn attention in the context of matrix algorithms  (see AppendixD for the analysis). These aforementioned works typically require a guarantee on the minimum eigenvalue gap of the perturbed matrix, e.g.,  uses a Minami-type bound , while in  the entire pseudospectrum of the perturbed matrix must be shattered with respect to a grid. The latter is even more challenging to achieve than a minimum gap and it requires \((n^{2})\) random bits. Our algorithm is significantly less demanding in terms of randomness: All we need is the Wegner estimate  for the density-of-states of random Hermitian operators, and only \((n)\) random bits in total.

Finally, in Section4, we apply our main results to prove the first matrix multiplication-type upper bounds for the bit complexity of PCA algorithms. Specifically, for the standard PCA formulation, we show that we can first compute the spectral projector and then use deflation to obtain a basis for the desired low-dimensional embedding in nearly matrix multiplication time. We then apply similar arguments to the seminal Block-Krylov PCA algorithm of .

### Notation

Matrices are denoted by bold capital letters and vectors by bold small letters. For real or complex constants we typically use Greek letters, or the Latin letters \(c,C\). The vector \(_{i}\) denotes the \(i\)-th column of the standard basis. \(^{*}\) is the conjugate transpose of \(\) and \(^{}\) denotes the pseudoinverse. The 2-norm is the default for matrices and vectors. \(()=\|\|\|^{}\|\) is the two-norm condition number of \(\). For the error analysis of the various algorithms, we use \(_{i}^{}\) to denote the error matrices that are introduced by the floating point errors of the \(i\)-th operation \(\). The letters \(\) and \(\) typically denote (scalar) error quantities and failure probabilities, respectively. \([n]\) is the set \(\{1,2,...,n\}\). We denote by \(^{n}^{n}\) the set of Hermitian matrices of size \(n n\), \(^{n}_{+}\) the set of Hermitian positive semi-definite matrices and \(^{n}_{++}\) the set of Hermitian positive definite matrices. For a matrix \(\) and a scalar \(z\) we write \(z\) as a shorthand for \(z\). \(()\) and \((,)\) denote the spectrum of a matrix \(\) and a matrix pencil \((,)\), respectively. The eigenvalues and singular values are always sorted in ascending order by default: \(_{1}_{2}_{n}\). \(_{}()\) is the \(\)-pseudospectrum of \(\) (see Definition A.2).

## 2 Computing spectral projectors with the sign function

Given a Hermitian definite pencil \((,)\), our ultimate goal is to compute a forward error approximation of the spectral projector that is associated with the \(k\) smallest eigenvalues, as described in Problem 1.1. Algorithm 3 solves this problem provably and efficiently, but it requires that we already have a suitable approximation of the eigenvalue gap that separates the desired subspace from the rest of the eigenspace. The algorithm is called \(\) since it is inspired by "purification" techniques in DFT, referring to the removal of the unoccupied orbitals. The computation of the gap and the midpoint is in fact the bottleneck of our main algorithm, however, we still show that they can be computed efficiently in Section 3, and, importantly, without diagonalizing any matrices. The properties of Algorithm 3 are stated in Proposition 2.1.

**Proposition 2.1**.: _Let \(^{n}\) with \(\|\| 1\), \(_{++}^{n}\) with \(\|^{-1}\| 1\), \(k[n-1]\) and \((0,1)\). Let \(_{k}=+_{k+1}}{2}\) and \(_{k}=_{k}-_{k+1}\), where \(_{1}_{n}\) are the generalized eigenvalues of the Hermitian definite pencil \((,)\) and assume that we want to compute \(_{k}\) which is the true spectral projector associated with the \(k\) smallest eigenvalues. If we have access to_

\[_{k}_{k}_{k} }_{k}(1)_{k}, [(),C()],\]

_for some constant \(C>1\), then Algorithm 3 computes \(}_{k}(,, }_{k},,)\) such that \(\|}_{k}-_{k}\|\), \(O(_{}(n)((_{k}})+(()}{_{k}}))))\) floating point operations using \(O((n)^{3}(_{k}})()}{_{k}}))\) bits of precision._

Proof.: The full proof of Proposition 2.1 can be found in Appendix B. We briefly summarize it here. The main idea is to use the sign function algorithm from , \(\), to approximate \((_{k}-^{-1})\). If we already know that \(_{k}\) is a reasonable approximation of \(_{k}\), and that it is located inside the correct eigenvalue gap, then, in exact arithmetic, our problem is equivalent to computing \((_{k}-^{-1})\). The result can be used to filter the desired spectral projector, often referred as "purification" in the context of DFT. The main challenge is to ensure that all propagated numerical errors, success probabilities, and input parameters for all algorithms are well-defined and bounded. To obtain the final forward errors we must rely on matrix similarity arguments, the properties of the pseudospectrum, the eigenvalue bounds of Weyl and Kahan from Fact A.1, and Lemma B.1, which gives explicit bounds on the sign function under small floating point perturbations. 

The rest of the paper is devoted to the analysis of our new algorithm for the spectral gap and the midpoint based on eigenvalue counting queries, described in Theorem 3.1. For comparison purposes, in Appendix B.1 we analyze a diagonalization-based algorithm for the same task (which is a new result itself), specifically, using the state-of-the-art \(\) algorithm of . We compare the two algorithms in Section 3.4, demonstrating that our counting-based algorithm is indeed faster.

## 3 Fast spectral gaps with counting queries

Our core algorithm efficiently approximates spectral gaps based on "eigenvalue counting queries" only, thus avoiding an explicit (and expensive) diagonalization. To give some intuition on the main idea, consider the following simplified version of the problem.

**Problem 3.1** (Gap finder).: _Let \(_{1}_{n}\) in \([-1,1]\) be \(n\) (unknown) real values (e.g., they can be the eigenvalues of the original matrix pencil) \(_{k}=+_{k+1}}{2}\), and \(_{k}=_{k+1}-_{k}\), for some \(k[n-1]\). Given \(k\) and some error parameter \((0,1/2)\) as input, we want to approximate \(_{k}\) and \(_{k}\) up to additive \(_{k}\), i.e., we look for \(_{k}=_{k}_{k}\) and \(}_{k}(1)_{k}\). Only queries of the following form can be performed: We fix a parameter \((0,1/2)\), which distorts all \(_{i}\) to some (unknown) \(_{i}^{}[_{i}-,_{i}+]\). We can then choose any value \(h[-1-,1+]\) and ask how many values \(_{i}^{}\) are smaller than \(h\). For each \(\), we can query arbitrarily many different values for \(h\), and each \(h\)-query costs \(q(1/)=O((1/))\)._The query cost is arbitrary to avoid trivial solutions by setting \(=0\). The following proposition is proved in Appendix E:

**Proposition 3.1**.: _Problem 3.1 can be solved iteratively by executing a total of \(O((_{k}}))\) iterations and \((1)\) queries per iteration, where each query costs at most \(q(_{k}})\)._

### Smoothed analysis of eigenvalue counting

To use the counting query model of Problem 3.1 and Proposition 3.1 to compute the spectral gap of a matrix pencil, we need a "black-box" method to count eigenvalues that are smaller than a threshold. We first describe a straightforward, deterministic algorithm \((},h,)\) for this task (see Lemma E.1), which takes as input a Hermitian matrix \(}\), a scalar \(h\), and a parameter \(\), with the requirement that \(_{}(h-})>\). It returns the precise number of eigenvalues of \(}\) that are smaller than \(h\). The runtime of the algorithm depends on \((1/)\), and must therefore be minimized. For this we resort to smoothed analysis: We apply a random perturbation to ensure that \(\) is at least polynomial in \(1/n\), up to some other factors detailed in Appendix D.

To build a random "regularizer," in Definition D.1 we introduce a random oracle that samples numbers from a standard normal distribution and returns their floating point representation using a pre-specified number of bits. Based on this simple oracle, we can design a floating point algorithm \(}(,,)\) which has the following properties described in Proposition 3.2:

**Proposition 3.2**.: _Let \(\) with \(\|\| 1\) be a Hermitian matrix, \(,(0,1/4)\) two given parameters, and \(}(,,)\). Let \(\) be an arbitrary (but fixed) grid of points in \([-2,2]\) with cardinality \(||=T\). For every element \(h_{i}\) consider the matrices \(_{i}=h_{i}-}\) and \(}=h_{i}-}+_{i}\), where \(_{i}\) denote the diagonal floating point error matrices induced by the shift. All the following hold simultaneously with probability \(1-2\) if we use \(O((}{}))\) bits of precision:_

\[\|}\| 4/3,_{i}(}) -_{i}(),_{}( }_{i})}.\]

Proof.: The main result that we use in the proof can be traced back to the Wegner estimate for the density-of-states of Hermitian operators under random diagonal disorder . See Appendix D and in particular D.2 for more details. 

### Computing the gap and the midpoint

We can now describe the algorithm GAP in Theorem 3.1, that computes the \(k\)-th gap and the midpoint of a Hermitian definite pencil. The same methodology can be extended to approximate any singular value, as described in Proposition E.2 in Appendix E.3.

**Theorem 3.1** (Gap).: _Let \(^{n}\), \(^{n}_{++}\) and \(\|\|,\|^{-1}\| 1\), which define a Hermitian definite pencil \((,)\). Given \(k[n-1]\), accuracy \((0,1)\), and failure probability \((0,1/2)\), there exists an algorithm_

\[_{k},}_{k}( ,,k,,)\]

_which returns \(_{k}=_{k}\,_{k}\) and \(}_{k}=(1)\,_{k}\), where \(_{k}=+_{k+1}}{2}\) and \(_{k}=_{k}-_{k+1}\). The algorithm requires_

\[O(T_{}(n)(_{k}}) (_{k}}))\]

_arithmetic operations using \(O((n)(^{4}(_{k}})+ (())))\) bits, where \(_{i}\) are the eigenvalues of \((,)\). If \(()\) is unknown, additional \(O(T_{}(n)()}{})(( )))\) floating point operations and \(O((n)^{4}()}{}))\) bits are sufficient to compute it with Corollary E.1._

Proof.: The full proof builds upon the results that are detailed in Appendices D and E. A summary is the following. We first fix our initial error parameter \(_{0}=1/8\) and call \(}=(,,)\)(Algorithm 4), which internally uses \(\) to reduce the GEP to a regular Hermitian one. From Proposition C.3, a Hermitian matrix \(}\) is returned such that \(|_{i}(})-_{i}(,)| }{4}\).

Next, we use the same counting query model as in Proposition 3.1. We first regularize \(}\) using \(}(}, }{2},_{0})\), where \(_{0}=/2\) is the initial failure probability. Conditioning on success of Proposition 3.2 (with probability \(1-_{0}\)), for all \(i[n]\), it holds that \(|_{i}(})-_{i}(})|  9_{0}/16\). Summing the two eigenvalue error bounds, we conclude that all eigenvalues of \((,)\), which initially lie in \([-1,1]\), are distorted by at most \(_{0}\) in \(}\). We now have all necessary tools to go back to the counting query model of Proposition 3.1: In the first step we construct a grid \(=\{-1,-7/8,-6/8,,7/8,1,9/8\}\). Clearly, \(||=(1)\). Since we conditioned on the success of Proposition 3.2, the regularization ensures that for every \(h_{j}\) it holds that \(_{}(h_{j}-}+)_{0}\) with \(_{0}=_{0}}{8||n)}}\). This allows us to efficiently execute \((},h_{j},_{0})\) for every \(h_{j}\).

At the end of the first iteration, we have computed two intervals \(I_{k}\) and \(I_{k+1}\), where \(I_{k}\) contains \(_{k}\) and \(I_{k+1}\) contains \(_{k+1}\), and each interval has size at most \(1\), i.e., half the size of \([-1,1]\). We continue by halving at each step both \(\) and \(\), constructing the corresponding grids as per the proof of Proposition 3.1, and counting eigenvalues over the grid. In each iteration after the first one, we keep track of two intervals \(I_{k}\) and \(I_{k+1}\), and two corresponding grids \(_{k}\) and \(_{k+1}\) with size \(|_{k}|=|_{k+1}|=(1)\). We therefore only need to execute a constant number of \(\) queries, and in each iteration the size of the intervals \(I_{k}\) and \(I_{k+1}\) is halved. The algorithm terminates after a total of \(m=O((_{k}}))\) iterations and finally provides the advertised complexity, bit requirements, failure probability, and approximation guarantees. 

### Sketch proof of Theorem 1.1

The proof of our main Theorem 1.1 directly follows from Theorem 3.1 together with Proposition 2.1 as well as the algorithm \(\) (described in Appendix E.3) which is used to compute the condition number of \(\). The full proof can be found in Appendix E.4.

### Comparison with diagonalization

We can now compare Theorem 3.1 with a diagonalization-based approach that is detailed in Proposition B.2. We fix \(=O(1/n)\) so that both algorithms succeed with the same probability.

For \(,_{k},^{-1}()( {poly}(1/n))\), the total arithmetic complexity of the algorithm of Theorem 3.1 is \(O(T_{}(n)^{2}(n))\) using \(O(^{5}(n))\) bits. For the same parameters, Proposition B.2 requires need a total of \(O(T_{}(n)^{3}(n))\) arithmetic operations, and \(O(^{5}(n))\) bits. Thus, in total, Algorithm 3.1 is faster by a factor of \(O((n))\).

In the extreme case where \(,_{k},()=(1)\), Theorem 3.1 counts \(O(T_{}(n)(n))\) arithmetic operations and \(O(^{5}(n))\) bits, while Proposition B.2 requires \(O(T_{}(n)^{2}(n))\) operations, and \(O(^{5}(n))\) bits. Thus, Proposition B.2 is again slower by a factor of \(O((n))\). Interestingly, in this case Theorem 3.1 is faster than even a single call to, which requires \(O(T_{}(n)^{2}(n))\) arithmetic operations. We conclude that, at least based on the currently existing algorithms, diagonalization is redundant for the computation of spectral gaps and invariant subspaces.

### Application in DFT

In Appendix F we demonstrate how our main results can be directly applied to approximate density matrices and electron densities of atomic systems in DFT. Even though is not a machine learning problem per se, we decided to dedicate a section in the Appendix due to its importance: DFT calculations persistently occupy supercomputing clusters and the corresponding software libraries and literature receive tens of thousands of citations annually at the time of this writing [89; 59; 60; 127; 74]. Our work is the first analysis to provide forward-error guarantees in finite precision for these problems in nearly matrix multiplication time.

## 4 Pca

Since its introduction in the early twentieth century [116; 72], Principal Component Analysis is one of the most important tools in statistics, data science, and machine learning. It can be used, for example, to visualize data, to reduce dimensionality, or to remove noise from data; cf. [79; 45] for reviews on the vast bibliography. In its simplest formulation, given a (centered) data matrix \(^{m n}\), the goal is to learn a \(k\)-dimensional embedding \(_{k}\), where \(k<n\), that maximizes the sample variance, which can be written as an optimization problem

\[_{k}=_{^{}=_{k k}} (^{}),\] (3)

where \(=^{}^{n n}\) is the sample covariance. It can be shown that the solution \(_{k}\) corresponds to the principal \(k\) singular vectors of \(\), i.e. the ones that correspond to the largest \(k\) singular values. Evidently, since the sample covariance is always symmetric and positive semi-definite, this can be written as a Hermitian eigenvalue problem

\[=,\]

(which is indeed a definite GEP as in Equation (1) with \(=\)). By solving for \(_{k}\), we can project the data in \(k\) dimensions by computing \(_{k}\), preserving as much of the variance in \(k\) dimensions as possible. To compute \(_{k}\) we can directly use our main results. However, the solution of Equation (3) is an actual orthonormal basis for the invariant subspace rather than the spectral projector that Theorem 1.1 returns. This can be addressed with deflation: Once we have the spectral projector \(}_{k}\), assuming that the approximation is sufficiently tight, we can apply a subsequent deflation step based on a rank-revealing QR factorization to obtain a \(k\)-dimensional basis. This can be done deterministically in \(O(n^{3})\) time  or in randomized \(O(n^{})\)[39; 15].

In Appendix G.1 we prove the following Theorem 4.1 for Algorithm 7, which builds upon our main Theorem 1.1, the algorithm of Proposition E.2 to approximate \(\|-_{k}\|\), and the \(\) algorithm of , to solve the standard PCA problem of Eq. (3). Following the existing literature, the result is stated for real matrices, but it can be trivially adapted to the complex case as well.

**Theorem 4.1** (Pca).: _Let \(^{m n}\) be a centered data matrix, \(\) the \(n n\) symmetric sample covariance matrix, i.e., \(=^{}\), \(\|\| 1\), \(k[n]\) a target rank, and \((0,1)\) an accuracy parameter. Given \(\), we can compute a matrix \(}_{k}\) with \(k\) columns such that \(\|-}_{k}}_{k}^ {}\|(1+)\|-_{k}_{k}^ {}\|\), where \(_{k}^{n k}\) contains the top-\(k\) (right) singular vectors of \(\) in_

\[O(T_{}(n)((})(})+(_{k}})(_{k}})+(( {gap}_{k}}))))\]

_arithmetic operations using \(O((n)(^{4}(_{k}})+ ^{4}(}))+(}))\) bits of precision, with probability at least \(1-O(1/n)\)._

### Block-Krylov PCA

In some applications, the target dimension \(k\) might be small, i.e., \(k n\). This condition has driven a whole area of research in so-called low-rank approximation algorithms for PCA [53; 120; 33; 67; 100; 106; 30; 29; 5]. Such approaches are also suitable for kernel PCA, since they rely on matrix-vector products and therefore the kernel matrix does not need to be explicitly formed. The techniques from the previous section can be directly applied to obtain new bit complexity upper bounds for existing state-of-the-art algorithms, which are typically analyzed in exact arithmetic. They internally rely on the computation of the principal singular vectors of submatrices, which can be improved with our methods. Specifically, in Appendix G.2 we summarize a floating point analysis of the Block-Krylov Iteration algorithm (see Algorithm 8), essentially, providing a matrix multiplication-type upper bound on the bit complexity with only a polylogarithmic dependence on the singular value gap. In a nutshell, we directly obtain the following result:

**Theorem 4.2** (Bit complexity analysis of Block-Krylov PCA).: _Let \(\) be a data matrix \(^{m n}\), \(\|\| 1\), \(k[n]\) a target rank, \(_{}(0,1)\) an accuracy parameter, and \(q=(}}})\). Let \(T_{}(k)\) denote the complexity to stably multiply \(\) or \(^{}\) with a dense matrix with \(k\) columnsfrom the right (see Def. 6.1). Using the Steps 1-6 that are detailed in Appendix G.3 as a floating point implementation of Algorithm 8, we can compute a matrix \(}_{k}^{m k}\) that satisfies_

\[\|}_{k}}_{k}^{}-_{ k}_{k}^{}\| O(_{}),\]

_with high probability, where \(_{k}\) is an approximate basis for the top-\(k\) principal components of \(\), returned by Algorithm 8 in exact arithmetic. The total cost is at most_

\[O(qT_{}(k)()}{_{k}( )})+m(qk)^{-1}(_{k}()})+( qk)^{}(_{k}()}))\]

_floating point operations, using \(O(()}{_{}\,_{k}}))\) bits of precision. \(,\) are as in Alg. 8._

Proof.: The full proof can be found in Thm. G.2, Appendix G.3. The main idea is to apply the counting query methodology to compute the condition number of the Block-Krylov matrix \(\), as well as the \(k\)-th spectral gap and the midpoint of the reduced matrix \(\) in Line 5 of Alg. 8. Thereafter, we can compute a spectral projector and an approximate basis for the top-\(k\) singular vectors of \(\) using \(\) and \(\), similar to the analysis of classical PCA in the previous section. 

## 5 Open problems

We mention some open problems and interesting future directions.

1. **Bit requirement of \(\):** The major bottleneck for the bit requirements of our main algorithms comes from the \(\) algorithm of . An inverse-free Newton-Schultz iteration , or the implicit repeated squaring of  can potentially give significant improvements.
2. **Sparse algorithms:** In applications like DFT it commonly occurs that the matrices have special structure, i.e., they are banded and/or sparse. It remains open whether Problem 1.1 can be provably solved faster than our reported results in finite precision for these special cases (recall that the tridiagonal QR algorithm requires \(O(n^{3})\) operations to return the eigenvectors). An end-to-end stability analysis of existing fast eigensolvers would be the place to start .
3. **Distributed PCA:** The techniques for Block-Krylov PCA can be potentially applied to distributed or streaming PCA algorithms, which are also based on randomized low-rank approximations. E.g., in the distributed PCA algorithm of , it is straightforward to replace the SVD computation on the server by a counting query iteration. The full analysis of such an approach is left as future work.

## 6 Conclusion

In this work we provided an end-to-end analysis to approximate spectral projectors on \(k\)-dimensional invariant subspaces of Hermitian definite matrix pencils \((,)\) that require at most \(O(T_{}(n)(n,^{-1},( ),_{k}^{-1}))\) bit operations in the floating point model of computation. This is the first end-to-end analysis that improves the \((n^{3})\) complexity of classic eigensolvers for both the regular and the generalized case. To achieve this result we introduced a new method to approximate spectral gaps by querying the number of eigenvalues that are smaller than a threshold, and therefore completely avoid an explicit diagonalization of any matrix or pencil. This approach required proving that the Cholesky factorization can be stably computed in \(O(T_{}(n))\) floating point operations, a novel result _per se_. Our results have direct implications on PCA problems, providing matrix multiplication type upper bounds for the bit complexity of classical and Block-Krylov PCA.