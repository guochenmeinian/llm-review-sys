# Algorithmic Capabilities of Random Transformers

Ziqian Zhong, Jacob Andreas

Massachusetts Institute of Technology

{ziqianz, jda}@mit.edu

###### Abstract

Trained transformer models have been found to implement interpretable procedures for tasks like arithmetic and associative recall, but little is understood about how the circuits that implement these procedures originate during training. To what extent do they depend on the supervisory signal provided to models, and to what extent are they attributable to behavior already present in models at the beginning of training? To investigate these questions, we investigate what functions can be learned by randomly initialized transformers in which only the embedding layers are optimized, so that the only input-output mappings learnable from data are those already implemented (up to a choice of encoding scheme) by the randomly initialized model. We find that these random transformers can perform a wide range of meaningful algorithmic tasks, including modular arithmetic, in-weights and in-context associative recall, decimal addition, parenthesis balancing, and even some aspects of natural language text generation. Our results indicate that some algorithmic capabilities are present in transformers (and accessible via appropriately structured inputs) even before these models are trained.1

## 1 Introduction

A large body of recent work has demonstrated the effectiveness of transformer language models (LMs)  on general sequence-modeling tasks. Transformers seem to be especially well-suited (relative to other flexible neural models) at problems involving numerical reasoning , string manipulation , and various forms of in-context learning . Why is this the case?

One possibility is that some aspect of the transformer architecture makes these behaviors easy to learn. Under this hypothesis, transformer models do not implement any useful functionality when initialized; however, their loss landscape is structured such that they can be (computation- and sample-) efficiently optimized for behaviors of interest. But another possibility is that--because of intrinsic properties of the transformer architecture and parameter initialization schemes--these capabilities are _already implemented_ in some fashion even in randomly initialized models.

To disentangle these possibilities, we investigate the behavior of randomly initialized transformer models in which _only the embedding layers are optimized_, leaving all other model-internal parameters fixed. If such embedding-only training is successful, it implies that the randomly initialized model's behavior on some subspace already corresponds to the input-output mapping of interest, up to a choice of encoding scheme--in other words, that the randomly initialized model can already perform the target task, and it suffices to find an encoding of inputs and outputs that induces the target behavior.

In experiments on seven tasks, we find that embedding-only training yields accurate models for a diverse set of problems spanning arithmetic, associative recall, and sequence generation--in some cases substantially outperforming similarly trained recurrent models. Remarkably, transformer _language models_ trained in this fashion can even produce grammatical (though largely nonsensical)natural language text. We explain these results by showing that embedding-only training steers both inputs and model-internal representations into low-dimensional subspaces on which the model implements the target computation, a phenomenon we call "subspace selection". Embedding-only training is most successful when the target computation can be performed in a subspace that is low-dimensional relative to the ambient dimension of the model's hidden representations.

These findings build on a long line of research aimed at understanding the effectiveness of deep networks in terms of their behavior at initialization--e.g. showing that random convolutional networks are high-quality feature extractors [3; 8], or that overparameterized networks can be pruned down to sparse "lottery ticket" subnetworks that implement the correct behavior [16; 53; 39; 11]. But in contrast to past work, the solutions found by embedding-only training involve algorithmic computation rather than feature extraction, performed in low-dimensional subspaces but not by sparse sub-networks. Even more generally, our results show that pruning and optimization are not always necessary to surface useful capabilities--at least in transformers, some capabilities are available as soon as models are initialized, requiring only a learned encoding of inputs. This in turn suggests that it may be possible to partially understand the effectiveness of transformers simply by understanding their behavior at initialization.

Our work also has implications for research on circuit-level interpretability of transformers and other neural models: if even random models can perform structured, algorithmic tasks, then attempts to understand models by directly inspecting parameter matrices--and not their behavior on natural data distributions--may be fundamentally limited in their ability to characterize learned behaviors.

## 2 Background and Related Work

Random feature extractorsRandom deep convolutional networks are highly effective visual feature extractors even without training. Jarrett et al.  first discovered that linearly combining features from a randomly initialized one-layer convolutional network achieved comparable performance to fully trained networks for downstream vision tasks. Saxe et al.  showed that performance improvements from training is relatively minor comparing to architectural changes. In this work, we expanded the discussion to language models and demonstrated that training embeddings alone is sufficient to succeed in many tasks, highlighting the strong inductive bias of transformer architectures.

Neural reprogrammingNeural reprogramming aims to repurpose existing neural networks for novel tasks via simple transformation layers. This technique was first purposed by Elsayed et al.  as a way to exploit trained neural network served by existing providers, and it was later used as a resource-efficient domain-transfer technique [45; 49]. In our work, we showed that in addition to neural networks trained for other tasks, even randomly initialized neural networks can be reprogrammed to achieve non-trivial performance.

Figure 1: Overview of problem setup. **A**: Modeling approach. We initialize transformers randomly, then optimize _only_ their input and output embedding layers on a dataset of interest. We find that these random transformers can be successfully trained to perform a diverse set of human-meaningful tasks. **B**: Task set. We evaluate the effectiveness of random transformers on a set of model problems involving arithmetic and memorization, as well as modeling of natural language text.

Sparse sub-networks and lottery ticketsThe lottery ticket hypothesis was first proposed by Frankle and Carbin : a randomly-initialized dense neural network contains subnetworks, the _winning tickets_, that when trained in isolation can match the performance of the original network. Zhou et al.  and Ramanujan et al.  strengthened the hypothesis by discovering pruned subnetworks that achieve comparable accuracy of the trained full network within untrained, randomly initialized networks - winning tickets do not even need to be trained. The hypothesis also holds in transformers [11; 44]. Similar to the hypothesis, our work also focuses on the models' capabilities at initialization, but we showed that these capabilities can be surfaced without any pruning.

Interpreting neural circuitsMany efforts have been dedicated to the interpretation of neural networks. On the arithmetic task we study in this paper, Nanda et al.  and Zhong et al.  described two different mechanisms in transformers on modular addition; Quirke and Barez  performed detailed mechanistic analysis for one-layer transformers trained on decimal addition. These studies provide insights into possible mechanisms transformers might employ to solve these tasks. With theoretical analysis, Wen et al.  proved that on the bounded Dyck task, the attention patterns and weights of neural circuits could be quite arbitrary, which we confirm in this work.

Reservoir computingReservoir computing is a framework for computation. In the diagram, a blackbox reservoir receives data and updates its inner states there upon. A simple readout mechanism is then trained to map its inner states to the desired output. The reservior is generally kept untrained and only the readout part is trained. Under our notations, we can interpret the paradigm as training only the unembedding part of random neural networks. See Benjamin et al.  for a general overview for reservoir computing and Mantas et al.  for a survey on its applications on recurrent neural networks.

## 3 Setup

ModelsWe study the behavior of decoder-only transformer language models. In these models, **inputs**\(x\) (represented as sequence of token IDs) are first assigned vector **embeddings**\(h^{(0)}=E(x)\) via an **embedding layer**\(E\). These embeddings are then passed through a series of \(m\)**intermediate layers**\(F^{(1)},F^{(2)},,F^{(m)}\) so that \(h^{(i)}=F^{(i)}(h^{(i-1)})\), with each \(F^{(i)}(x)\) computing a **hidden representation**\(h^{(i)}\) via a transformation:

\[h^{(i)}=F^{(i)}(h^{(i-1)})=^{(i)}(^{(i)}(h^{(i-1)}))\;,\]

where FFN, SelfAtt are feed-forward and causal self-attention modules as in Radford et al.  (layer norms are omitted for simplicity). The final activation \(h^{(m)}\) is mapped by a **unembedding layer**\(U\) to a distribution over next tokens.

To encode information about the ordering of input tokens, we implement the embedding layer \(E\) using two matrices: a token embedding matrix \(E_{}\) and a positional embedding matrix \(E_{}\). For an input \(x=[x_{1},x_{2},,x_{n}]\), we first calculate the initial activations

\[h^{(0)}=E([x_{1},x_{2},,x_{n}])=[E_{}[x_{1}]+E_{}[ 1],E_{}[x_{2}]+E_{},,E_{}[x_{n}]+E _{}[n]].\]

Similarly, the unembedding layer is parameterized by a single matrix, and model predictions have the form:

\[p(x_{n+1} x_{1 n};E,F,U)(Uh^{(m)}_{n} )[x_{n+1}],\]

where (in a slight abuse of notation) \(E\), \(F\) and \(U\) denote embedding, intermediate, and unembedding parameters respectively.

In terms of parameters, let the hidden dimension be \(d\), the number of layers be \(m\), the vocabulary size be \(v\) and the maximum context length be \(n\), the embedding layers have \(((n+v)d)\) parameters as matrix \(E_{}\) and \(U\) have shape \(v d\) and matrix \(E_{}\) has shape \(n d\), while the full network has an extra \((md^{2})\) parameters.

InitializationModels are trained via gradient descent from some random initial parameterization. Following Radford et al. , parameters of feed-forward layers are initialized by sampling from isotropic Gaussians with mean \(0\) and standard deviation \(0.02/\). All the other weight matrices are initialized with \(0\)-mean Gaussians with standard deviation \(0.02\). The affine transformations in layer normalizations are initialized as identity.

TrainingIn this work, we examine language models with frozen intermediate layers, which we call **random transformers**. In these models, we fix the randomly chosen parameters intermediate layers, and train only the embedding layer \(E\) and unembedding layer \(U\). Our experiments thus compare:

**Full Training:** \[*{arg\,min}_{E,F,U}_{x,n 0}- p(x_{n+1} x _{1 n};E,F,U)\] **Embedding-Only Training:** \[*{arg\,min}_{E,U}_{x,n 0}- p(x_{n+1} x _{1 n};E,F,U)\]

where the \(*{arg\,min}\) is computed approximately via mini-batch stochastic gradient descent.

## 4 Random Transformers Can Perform Simple Algorithmic Tasks

Can random transformers be steered to perform meaningful tasks by optimizing only input and output tokens' embeddings? We begin by evaluating four widely-studied tasks that serve as toy models of important behaviors in large-scale LMs.

### Tasks

Modular AdditionThis task evaluates models' ability to perform integer addition under a fixed prime modulus \(p=199\). Models receive a sequence of input tokens \([a,b]\) for \(a,b[0,p-1]\) and must compute \((a+b) p\). When over-parameterized models are trained to perform this task, grokking (a long period of memorization followed by an abrupt transition to generalization ) is typically observed [26; 18]. Neural sequence models of different kinds have been found to implement two interpretable algorithms, sometimes referred to as the "Clock"  and "Pizza" , when trained to perform this task.

Needle-in-a-HaystackThis task evaluates models' abilities to process long input sequences . In the variant we study, models receive as input a sequence of form \([m_{1},c_{1},m_{2},c_{2},,m_{k},c_{k},m_{u}]\). Here, \(m_{1},m_{2},,m_{k}\) are distinct _markers_ (\(k 30\)) and \(c_{i}\)'s are corresponding _values_. The input ends with a marker \(}\) (\(u[1,k]\)), and models must search for the previous occurrence of that marker in the input sequence and output the corresponding \(c_{u}\). Specific circuits like _induction heads_ are often observed in models that perform this task .

Decimal AdditionThis task evaluates models' ability to perform arithmetic operations distributed over sequences of multiple input tokens--in this case, addition of two equal-length numbers represented as digit sequences in base 10. The order of digits of both numbers and the results are reversed to simplify the task. For example, the task 39+71=110 is encoded as a pair with input 9 3 1 7 and output 0 1 1. We use \(10\)-digit numbers in our setup. Past work has found that fully trained models can reliably learn some versions of this task [32; 51; 43].

Parenthesis Balancing (Dyck Recognition)In this task, models are presented with a sequence of parentheses, and must predict whether they are balanced--i.e., whether the sequence contain an equal number of opening and closing parentheses, and within every prefix, the number of closing parentheses is no greater than the opening parentheses. Such sequences are also called Dyck sequences, and have been widely studied in language models because of their connection to context-free models of natural language syntax [50; 47]. Note that this task has a vocabulary of size \(4\) (two parentheses and two labels), so only a very small number of parameters are optimized by embedding-only training. In our setup, the input parenthesis sequences have lengths at most \(60\).

For the modular addition task, we partition the full set of well-formed input-output pairs into a fixed train/test split; for the other problems, we pre-generate a fixed test set but randomly generate new pairs for each training batch. Additional details may be found in Appendix D.1.

### Results

Results are shown in Table 1. Here we compare random transformers with a hidden size of 1024 to fully trained models with hidden sizes of 16 and 1024. For reference, we also compare to a (fullytrained) LSTM, a recurrent neural sequence model . All models have two hidden layers, and all results are aggregated across ten random initializations.

Random transformers learn to perform all four tasksRandom transformers with trained embeddings and unembeddings obtain perfect accuracy on all four tasks consistently across restarts--sometimes outperforming _fully trained_ recurrent networks (Table 1). These results thus point toward the role of a transformer-specific inductive bias in the effectiveness of embedding-only training.

In general, embedding and unembedding parameters must both be trainedTo further identify which model components must be optimized to obtain these results, we consider several variants of embedding-only training: (a) leaving both token and positional embeddings fixed (so only the unembedding is trained); (b) leaving the unembedding layer fixed (so only the embedding is trained); and (c) leaving positional embeddings fixed (so token embeddings and the unembedding is trained). Results are shown in Table 2. All variants fail to reach perfect accuracy on at least one task. Notably, the variant that only trains the unembedding is unable to reach near-perfect accuracy on _any_ task.

Random transformers exhibit interpretable attention patternsA closer examination of the trained random models reveals similar mechanistic behaviors to their fully trained counterparts. For example, we observe attention patterns similar to "induction heads"  previously described in fully trained models for associative recall tasks (Figure 2).

Random transformers use structured embeddingsIn the modular addition task, learned embeddings form circles in low-dimensional subspaces, another phenomenon observed in fully trained models for these tasks  (Fig. 3). To better understand similarities between these models and their fully trained counterparts, we also computed the _distance irrelevance_ and _gradient symmetricity_ metrics described by Zhong et al.  for distinguishing between networks that perform modular arithmetic via the "Clock" or "Pizza" algorithm. We find a gradient symmetricity of \(0.88\) and a distance irrelevance of \(0.88\), consistent with a Clock-like solution.

  
**Task** & **Random** 1024 & **Random** 16 & **Normal** 1024 & **Normal** 16 & **LSTM** 1024 \\  Modular Addition & 100.0\% & 1.3\% & 100.0\% & 97.2\% & 100.0\% \\ Neede-in-a-Haystack & 100.0\% & 7.5\% & 100.0\% & 12.0\% & 99.5\% \\ Decimal Addition & 100.0\% & 26.6\% & 100.0\% & 67.5\% & 53.0\% \\ Parenthesis Balancing & 100.0\% & 87.3\% & 92.3\% & 100.0\% & 100.0\% \\   

Table 1: Test accuracy of fully trained and random transformers, as well as fully trained LSTMs, on algorithmic tasks. Denoted numbers (1024 and 16) are hidden sizes; results are median over 10 random restarts. Random models with only trained embeddings reliably perform all four tasks, and even outperform fully trained LSTMs. See Appendix E for the accuracy curve on multiple hidden sizes.

  
**Task** & U **only** & E **only** & E\({}_{}\) \& U **only** \\  Modular Addition (Train) & 47.9\% & 68.3\% & 100.0\% \\ Modular Addition (Test) & 0.4\% & 1.2\% & 100.0\% \\ Neede-in-a-Haystack & 17.7\% & 100.0\% & 98.5\% \\ Decimal Addition & 27.3\% & 100.0\% & 48.5\% \\ Parenthesis Balancing & 92.3\% & 100.0\% & 100.0\% \\   

Table 2: Accuracy of embedding-only training with additional parameters fixed: optimizing only the unembedding layer, only the embedding layer, or only non-positional embeddings. Hidden sizes are all \(1024\); results are median over 10 random restarts. All three embedding matrices must be optimized for models to reliably complete all tasks.

[MISSING_PAGE_EMPTY:6]

smaller models. As in previous sections, we evaluate both embedding-only and full training with 2- and 4-layer transformers with various hidden sizes. Scaling curves are shown in Fig. 4. Fully trained models obtain a cross-entropy loss of \(1.35\), on par with the results reported by Eldan and Li . Our trained random transformer with \(512\) width and ~10M trainable parameters achieved a cross-entropy loss of \(2.64\), roughly on par with the fully trained ones with \(32\) width and only ~0.7M trainable parameters. Moreover, adding additional hidden layers does not appear to improve performance performance--2-layer random transformers in fact achieve better losses than 4-layer models.

Figure 4: Language modeling performances (measured in cross-entropy loss or equivalently log perplexity, lower is better) for fully trained and random transformers. Comparatively large hidden sizes are needed for random models to match the performance of fully trained models.

Figure 5: Sample completion generated by fully trained and random 512-width 2-layer transformers. While random models produce less coherent than fully trained models, they nonetheless generate text that is largely grammatical topically appropriate.

Example outputs sampled from these models (on a newly generated prompt) are shown in Fig. 5. Even though random models obtain significantly worse perplexity than fully trained models, they could still perform several key sub-tasks needed for language generation: generated sentences are generally grammatical, topically coherent with the prompt, correctly resolve some long-distance dependencies (e.g. references to the name _Max_) and perhaps even high-level narrative structure.

## 6 Random Transformers Operate in Low-Dimensional Subspaces

Can we explain the success of random transformers in the tasks studied above? In this section, we present evidence that embedding-only training steers the hidden computation in transformers into low-dimensional subspaces in which target functions are already implemented. We term this phenomenon **subspace selection**, and show that it is distinct from _sparsification_, as these subspaces are distributed across neurons.

In Section 6.1, we measured fraction of activation variance explained by top principal components in various tasks. For algorithmic tasks, we show that both normal and random transformers work in low-dimensional subspaces, which are sufficient for solving these tasks (Appendix F). However, for language modeling and memorization, the random transformers displayed more subspace selection compared to the fully trained ones, and as a result, they attained lower performances. In Section 6.2 we constructed a task that explicitly requires operating on high-dimensional spaces, circuit imitation, and indeed, the random transformers exhibit significant performance gap compared to normal transformers.

### Low-Dimensional Hidden Representations in Algorithmic and Memorization Tasks

To characterize the geometry of random transformers' internal representations, we present models (trained for all previously described tasks) with a set of randomly chosen inputs and collect their embeddings and hidden representations of these inputs at different layers. Using these representations, we perform two analyses: (1) the fraction of variance explained by the top **principal components** of these hidden representations (which will be large if representations lie in a low-dimensional subspace), and (2) the fraction of variance explained by the most variable entries in hidden state vectors, or **neurons** (which will be large if computation is confined to a sparse sub-network).

Both fully trained and random transformers exhibit subspace selection but not sparsification (Table 4 top) in the four algorithmic tasks. In Appendix F, we show that this behavior is expected, insofar as all four algorithmic tasks can be solved by shallow transformer-like circuits that operate in low-dimensional subspaces. On memorization and language modeling tasks, random transformers become much more concentrated on a small subspace than fully trained transformers, thus using a lower effective dimension (Table 4 bottom and Table 5). In the language modeling task, more than \(30\%\) of variance in hidden representations is explained by 10 components.

### Subspace Selection in Circuit Imitation

To provide another window into these results, we characterize _how large_ the hidden representations of a random model must be for it to simulate a random circuit that operates in a low-dimensional subspace.

To do so, we first generate a small, random _target_ transformer associated with a distribution over strings \(\), then perform embedding-only training in a different, randomly initialized transformer to simulate its behavior on some domain of interest by minimizing:

\[*{arg\,min}_{E,U}\ \ _{x}[(( x ) p( x;E,F,U)]\]

To construct target models, we begin with the same initialization scheme described in Section 3, then we scale the query and key parameters in the attention mechanism by a factor of 10, and and the feed forward weights and biases by a factor of 20. We also scale the final projection layer by \(100/}\). (This initialization scheme increases variability of attention patterns and target model predictions across random restarts; see Appendix D.2.2 for additional discussion.)We evaluate fully trained and random transformers' ability to fit the target distribution for target models with a 512-token vocabulary, three layers, two attention heads, and varying hidden dimensions. Training inputs \(x\) consist of \(40\)-token sequences generated uniformly at random.

Results are shown in Fig. 6. In general, random transformers can only match the behavior of shallower (3 vs 1) and or significantly narrower (512 vs 128) models, with a sharp increase in error moving from \(12 16 32\)-dimensional models, suggesting that random transformers may _only_ be able to learn computations that can be performed in lower-dimensional subspaces.

## 7 Discussion

We have shown that transformer sequence models can accomplish a variety of meaningful tasks when only their embedding layers are optimized. For tasks involving memorization, these results show that much of models' "knowledge" can be encapsulated by input embeddings rather than models' internal parameters. For more algorithmic tasks like arithmetic and parenthesis balancing, which require relatively sophisticated circuits to perform, our experiments show that versions of these circuits can be accessed in random transformers (but not LSTM sequence models) simply by constructing appropriate input and output embeddings that confine models' internal states to low-dimensional subspaces in which these tasks are performed.

    &  &  \\ 
**Task** & **Model** & **Emb** & **L1** & **L2** & **Emb** & **L1** & **L2** \\   & Normal & 42.0\% & 21.5\% & 13.8\% & 3.8\% & 1.6\% & 3.4\% \\  & Random & 72.9\% & 63.6\% & 44.7\% & 2.8\% & 3.3\% & 2.9\% \\  & Normal & 45.3\% & 87.0\% & 87.3\% & 1.7\% & 3.3\% & 2.6\% \\  & Random & 55.1\% & 72.7\% & 63.5\% & 2.0\% & 3.9\% & 2.8\% \\  & Normal & 35.5\% & 83.5\% & 47.7\% & 1.5\% & 5.2\% & 2.8\% \\  & Random & 31.3\% & 30.0\% & 21.5\% & 1.8\% & 1.8\% & 1.6\% \\  & Normal & 72.0\% & 99.4\% & 98.2\% & 2.9\% & 7.8\% & 18.0\% \\  & Random & 74.9\% & 85.8\% & 80.9\% & 4.9\% & 4.0\% & 3.7\% \\   & Normal & 15.0\% & 25.3\% & 23.4\% & 10.2\% & 12.2\% & 10.5\% \\  & Random & 27.5\% & 27.5\% & 24.7\% & 9.9\% & 9.8\% & 9.7\% \\   

Table 4: Median explained variance from top 10 directions under principal and neuron basis. Activations after embedding (_Emb_), layer 1 (_LI_) and layer 2 (_L2_) are collected from multiple trained \(2\)-layer models of width \(1024\) and \(128\) (for memorization). _Normal_ transformers are fully trained while _Random_ transformers have only embedding and unembedding layers trained, as in previous experiments. Across tasks, a large fraction variance in models’ hidden representations is explained by a small number of principal components, but these components do not appear to be aligned to individual neurons or sparse sub-networks.

    &  \\ 
**Task** & **Model** & **Emb** & **L1** & **L2** & **L3** & **L4** \\   &  &  \\   & Normal & 29.5\% & 27.3\% & 24.3\% & 23.2\% & 17.6\% \\  & Random & 43.0\% & 30.2\% & 31.8\% & 32.1\% & 31.7\% \\    &  &  \\   & Normal & 8.6\% & 16.4\% & 14.7\% & 13.9\% & 5.9\% \\  & Random & 3.3\% & 2.9\% & 2.8\% & 2.9\% & 2.9\% \\   

Table 5: Median explained variance from top 10 directions under principal and neuron basis for language modeling task. Activations after embedding (_Emb_) and after every layer (_L1, L2, L3, L4_) are collected from trained 4-layer models of width 512. As above, a substantial fraction of variance is explained by a small number of principal components, especially in random transformers.

However, our experiments have also highlighted several important differences between embedding-only and fully trained models, especially with regard to their parameter efficiency and information capacity. This paper leaves open the question of how computation in embedding-only models relates to fully trained ones--e.g. whether, during full training, the mechanisms we have discovered here evolve gradually into their fully trained forms, or whether fully trained models use entirely different pathways .

We anticipate that these random transformers will also provide an interesting new test-bed for interpretability research, and future work might investigate how learned feature codebooks  and automated neuron labeling procedures  behave when applied to these models. Even more generally, these results motivate a closer study of the behavior of untrained models as a source of insight into differences between, and improvements upon, existing neural model architectures.