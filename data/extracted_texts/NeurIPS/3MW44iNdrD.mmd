# FairQueue : Rethinking Prompt Learning for

Fair Text-to-Image Generation

 Christopher T. H. Teo

christopher_teo@mymail.sutd.edu.sg

Equal Contribution

&Milad Abdollahzadeh

milad_abdollahzadeh@sutd.sg

&Xinda Ma

xinda_ma@sutd.edu.sg

&Ngai-Man Cheung

ngaiman_cheung@sutd.edu.sg

###### Abstract

Recently, prompt learning has emerged as the state-of-the-art (SOTA) for fair text-to-image (T2I) generation. Specifically, this approach leverages readily available reference images to learn inclusive prompts for each target Sensitive Attribute (tSA), allowing for fair image generation. In this work, we first reveal that this prompt learning-based approach results in degraded sample quality. Our analysis shows that the approach's training objective-which aims to align the embedding differences of learned prompts and reference images-could be sub-optimal, resulting in distortion of the learned prompts and degraded generated images.

To further substantiate this claim, **as our major contribution,** we deep dive into the denoising subnetwork of the T2I model to track down the effect of these learned prompts by analyzing the cross-attention maps. In our analysis, we propose novel prompt switching analysis: I2H and H2I. Furthermore, we propose new quantitative characterization of cross-attention maps. Our analysis reveals abnormalities in the early denoising steps, perpetuating improper global structure that results in degradation in the generated samples. Building on insights from our analysis, we propose two ideas: (i) _Prompt Queuing_ and (ii) _Attention Amplification_ to address the quality issue. Extensive experimental results on a wide range of tSAs show that our proposed method outperforms SOTA approach's image generation quality, while achieving competitive fairness. More resources at Project Page.

## 1 Introduction

There has been significant progress in the quality of text-to-image (T2I) generation  resulting in increasing adoption in different applications . With this comes concerns regarding the fairness of these T2I models and their societal impacts .

**Fair T2I Generation.** T2I models may inherit biases present in their training data. Several approaches have been proposed to mitigate these biases  (See related work in Supp). Particularly, **Inclusive T2I Generation (ITI-Gen)**-the existing SOTA-suggests that fair T2I approaches based on hard prompts (HP) (_e.g.,_ ''A headshot of a person with fair skin tone") are limited by linguistic ambiguity. For example, Skin Tone is often challenging to define and interpret based on HP, resulting in sub-optimal performance. To overcome this linguistic ambiguity, ITI-Gen adopts the notion that "a picture is worth a thousand words" and leverages readily available referenceimages to learn an inclusive prompt for each tSA category. This approach translates visual attribute differences present in the reference images into prompt differences, enabling the learned prompts to be used to generate images of all tSA categories, regardless of their linguistic ambiguity. Fairness is achieved by uniformly sampling the learned prompts to condition the T2I generation. _Central to this approach is the enforcement of directional alignment between learned prompt embeddings and reference image embeddings corresponding to a pair of tSA categories_.

**In this work, we question the central idea of prompt learning via alignment between the direction of prompt embeddings and the direction of reference image embeddings in the context of fair T2I generative models.** Our work starts with examining the generated images and observes that a moderate amount of degraded images are generated based on ITI-Gen. We argue that using the direction of reference image embeddings as guidance could be sub-optimal, as the difference between reference images could include additional unrelated concepts other than the tSA difference

Figure 1: Our work re-visits SOTA fair T2I generation, ITI-Gen. We question ITI-Gen’s central idea of prompt learning via alignment between the directions of prompt embeddings and reference image embeddings. (a) We observe degradation in images generated through ITI-Gen’s learned prompts. We note that the direction of reference image embeddings could include unrelated concepts beyond tSA differences (_e.g.,_ variations in accessories) resulting in learning of distorted prompts using ITI-Gen. Furthermore, we observe misalignment between the direction of credible hard prompts and that of reference images/learned prompts. (b) As our main contribution and to further understand how these distorted prompts affect the image generation process, we deep dive into the denoising network and analyze the cross-attention maps, revealing their abnormalities _e.g.,_ higher activity for maps associated with non-tSA tokens (“of”, “a”). We examine the degraded global structures resulting from these distorted prompts in the early denoising steps. Moreover, we propose I2H and H2I (Eq.2) analysis to understand impact of these degraded global structures and abnormalities in later denoising steps. In addition, we propose metrics (Eq.3) on cross-attention maps to quantify these abnormalities. (c) Building on insights from our analysis, we propose a solution to address distorted prompts while maintaining competitive fairness. Our solution FairQueue includes two ideas: prompt queuing and attention amplification. \(E_{T}\) and \(E_{I}\) are CLIP text and image encoder resp. . \(\), \(\), \(\) are the base prompt, hard prompt with minimal linguistic ambiguity, and ITI-Gen prompt, resp.

(Fig 1). For example, reference images of "A headshot of a person smiling" and "A headshot of a person not smiling" could contain differences in poses, accessories, hairstyling, in addition to the difference in smiling. Therefore, the direction of reference image embeddings could be noisy and include additional unrelated concepts other than the tSA difference. We perform an analysis on the direction of embeddings to further understand the issue. **We hypothesize that using the direction of reference image embeddings as guidance could lead to distortion in the learned prompts, resulting in artifacts and quality degradation in the images generated by T2I models.**

To further substantiate this claim, **as our major contribution**, we deep dive into the denoising subnetwork of the T2I model to analyze ITI-Gen prompts in the generation pipeline. Our analysis include examination of the cross-attention maps of the learned prompts at individual time steps of the denoising process. We propose novel prompt switching analysis: **ITI-Gen to HP (I2H)**, and **HP to ITI-Gen (H2I)**. We further propose new quantitative metrics for cross-attention map characterization. Our analysis reveals cross-attention maps of the learned prompts have abnormalities in the initial time steps of the denoising process. This results in synthesizing improper global structures. Interestingly, we find that the learned prompts have a minimum abnormality in the later steps-the learned prompts perform adequately in generating the desired tSA category _provided that proper global image structures could be synthesized in the initial denoising steps._ To justify our analysis of cross-attention, we remark that cross-attention contextualizes prompt embeddings with the latent representation of images and has been shown to play a key role in T2I models [21; 22].

Building on the insights of our analysis, we propose a solution to address degraded generated images without compromising fairness and diversity. Particularly, we propose Prompt Queuing to apply base prompts (without tSA tokens) in the initial time steps and ITI-Gen learned prompts in the later time steps of the denoising process. We further propose Attention Amplification to balance the quality and fairness of the T2I generation. Overall, our solution can effectively address the degraded quality issue in ITI-Gen while maintaining competitive fairness. Our contributions are:

* We examine the generated images from the prompt learning-based fair T2I generation approach and reveal a moderate amount of generated images with degradation (Sec 3.1).
* We argue that the direction of reference image embeddings could be noisy and include unrelated concepts in addition to tSA difference, and prompt learning based on alignment with the direction of reference image embeddings could be sub-optimal (Sec 3.1).
* We deep dive into the denoising subnetwork of the T2I model and analyze cross-attention maps with our proposed prompt switching analysis I2H and H2I, and our proposed quantitative metrics for cross-attention maps. Our analysis reveals and characterizes abnormalities in cross-attentions of ITI-Gen prompts in the denoising process (Sec 3.2).
* We propose FairQueue, a solution based on prompt queuing and attention amplification to improve generation quality while maintaining competitive fairness (Sec 4).

## 2 Preliminaries

**T2I Generation.** SOTA T2I generation is based on diffusion model (DM) [1; 2; 3]. In the forward diffusion process, Gaussian noise is incrementally added to the training data to train the DM. Then, during reverse diffusion, the DM generates samples by randomly sampling latent noise \(Z_{0} N(0,)\) as an input. For more control, text-conditioning [1; 23; 24; 25] was introduced, where we denote the reverse diffusion (denoising) of a single step \(t\) by \(Z_{t+1} DM(Z_{t},,t,s)\). Here, \(Z_{t}\) is the latent of the noisy image, \(\) the input prompt, \(t[0,l]\) the denoising step, and \(s\) a random seed. Central to text conditioning is the cross-attention mechanism which contextualizes prompt embeddings with the image latent [21; 26]. Specifically the **cross-attention map \(^{r m n}\)**-where \(r\) is the number of tokens in the prompt, and \(m n\) shows map size for each token-is computed by:

\[=SoftMax(}{})\] (1)

where, \(Q=_{q}((Z_{t}))\) is the linear projection of the latent spatial features \((Z_{t})\), and \(K=_{q}(E_{T}())\) is the linear projection of the textual embedding \(E_{T}()\) (usually CLIP text encoder ). For ease of notation, we refer to the token-specific attention maps as \([.]\)_e.g., \([^{r}]^{m n}\)_ refers to the cross-attention map for the token "of" in \(\). As our work focuses on the reverse diffusion process, we utilize \(Z_{0}\) as the noisy latent input and \(Z_{l}\) as the final latent output. This \(Z_{l}\) is then finally passed into the DM decoder to output generated image, \(D(Z_{l})\).

**Fairness in Generative Models.** In generative models, fairness is defined as _equal representation_[27; 28], where for a tSA with \(K\) categories, a fair generator will generate an equal number of samples for each category. As an example, for a T2I model \(G\) with text prompt "A headshot of a person" as input, we consider \(G\) as fair model _w.r.t._ tSA = Young-with two categories {Young, Old}[29; 27; 28]- if it generates an equal number of samples for each categories of this tSA [30; 31].

**Hard Prompts for Fair T2I Generation.** A baseline for achieving fairness in T2I models is to append the tSA-related prompt to the _base prompt_[17; 19]. Considering the same tSA=Young, and the base prompt "A headshot of a person", adding a tSA-related prompt for each category results in the HPs: "A headshot of a person young/old". For a fair generation, we query T2I with each of these HPs uniformly. Note that HP although very effective with certain tSA, in most cases, is ineffective due to the tSAs having linguistic ambiguity -having misleading or deceptive language.

**Prompt Learning for Fair Text-to-Image Generation.** To resolve the issue of ambiguous tSAs, inspired by the recent success of prompt learning [33; 34], ITI-Gen aims to achieve fairness in a pre-trained T2I model by learning inclusive tokens for each category of the tSA. Assuming the tokenized _base prompt_ as \(^{p d}\), where \(p\) is the number of tokens and \(d\) is the dimension of the embedding space, for each category \(k\{1,...K\}\) of tSA, it learns \(q\) additional tokens. \(^{k}=[^{k}_{0},^{k}_{1},,^{k}_{q-1}]^{q d}\). In , \(q\) is set to 3. Then, _ITI-GEN prompt_ is constructed by appending these learned tokens to the original tokens: \(_{k}=[;^{k}]^{(p+q) d}\). These tokens are learned using a set of labeled reference images (_w.r.t._ tSA) \(_{ref}=\{_{i},y_{i}\}_{i=1}^{N}\); \(y_{i}\{1,...,K\}\) to provide stronger signals for describing tSA. More specifically, for a pair of categories \((i,j)\) of tSA, a directional loss  is used to match the direction of learned prompts and images for this pair in CLIP embedding  space _i.e.,_\(_{^{i},^{j}}_{dir}=1-_{(i,j)} _{(i,j)})}{(|_{(i,j)}|)|}\), where \(_{(i,j)}\) (\(_{(i,j)}\)) denotes the direction between images (text prompts) of two categories \(i\) and \(j\) in CLIP's embedding space, and directional loss \(_{dir}\) is minimized to learn tSA tokens \(^{i}\), \(^{j}\) for these categories. Finally, using \(_{k}\) as input prompt, fairness is achieved by uniformly sampling the \(K\) categories of the tSA. We will omit the category index \(k\) when it is clear from context, and denote learned prompt and tokens by \(\) and \(_{0},,_{q-1}\) resp.

## 3 A Closer Look at Prompt Learning for Fair Text-to-Image Generation

In this section, we take a closer look at ITI-Gen. First, in Sec. 3.1, we analyze ITI-Gen performance where we find quality degradation in moderate number of generated samples. We attribute this to the sub-optimal learning objective in ITI-Gen, which captures unrelated concepts that distort the learned tokens in \(\). Then, in Sec. 3.2, we analyze ITI-Gen prompts during sample generation by inspecting the cross-attention mechanism. Our analysis reveals that ITI-Gen prompts give rise to abnormality particularly damaging to the early steps of the denoising process.

**Remark.** To conduct the following analysis on ITI-Gen prompts' behavior we require a strong baseline as a pseudo-gold standard to compare against. To address this, we found that when considering certain tSA with minimal linguistic ambiguity (MLA) -a few tSA that can be described without misleading or deceptive language-HPs can serve as this strong baseline. Therefore, in this section, we focus on tSAs with minimum linguistic ambiguity. Later, in experiment section, we will include all tSAs, with or without ambiguity.

### Limitations of Prompt Learning for Fair T2I Generation

Although ITI-Gen improves fairness in T2I generation, a closer examination of its outputs reveals a potential trade-off: compromised image quality. In this section, first, we perform a systematic experiment to showcase these quality issues and then explore the potential root causes behind them.

**Experimental Setup.** To evaluate our generated samples, we utilize the metrics: i) Fairness Discrepancy (FD) [27; 31; 11; 36] to measure fairness, ii) Text-Alignment (TA) [37; 22] and FID  to measure quality, and iii) DreamSim (DS)  to measure semantic preservation. Next, we determine a set of tSA with MLA to compare ITI-Gen with HP (as a pseudo-gold standard). Specifically, we follow  and use pre-trained _Stable Diffusion_ (SD)  as T2I model. Then as mentioned in Sec. 2, for HP, we append the tSA-related prompts to the base prompt. We empirically found that tSAs {Smiling, High Cheekbones}, are unambiguous by classifying 500 generated sample per HP utilizing CLIP classifier , where on average they both achieve a \(98\%\) accuracy (Experiment details in Supp). Then, for ITI-Gen, we strictly follow  and use publicly available fairimage dataset-sampled from CelebA -as reference images to learn inclusive tokens, \(\). Finally, we generate and evaluate ITI-Gen samples based on the same latent noise input as HP. See Supp for experiment and metric details.

Fig. 2 shows some generated samples together with quantitative results. A moderate number of generated images with ITI-Gen have quality degradation often with unrelated content (_e.g.,_ generating dog, multiple degraded faces, vague cartoons, etc.). Quantitative results show that for both tSAs, HP performs better in fairness (lower FD), quality (higher TA, lower FID), and semantic preservation (lower DS). We postulate degraded samples stem from ITI-Gen's sub-optimal training objective.

**Issue of Directional Loss for Fair T2I Generation.** We hypothesize that directional loss is sub-optimal in learning tSA-related tokens \(\). Particularly, the differences in reference images \(\) can include unrelated concepts in addition to variation in tSA categories. For example, considering tSA=Smiling in Fig. 0(a) (col 2), the reference images used for learning these two categories contain differences in pose, accessories, etc., in addition to the difference in Smiling. We further explore this potential of encoding unrelated concepts in \(\) by taking a closer look into the CLIP embedding space, where ITI-Gen's learning process happens. Recall, that as we utilize tSA with MLA and the HPs only differ in the tSA categories, we can utilize them as references in our analysis. For example, considering tSA=Smiling, the related tokenized prompts of HP in CLIP space can be computed as follows: \(_{i}=E_{T}\) ("A headshot of a person smiling"), and \(_{j}=E_{T}\) ("A headshot of a person not smiling"), with \(E_{T}\) denoting CLIP's text encoder. Then \(=_{i}-_{j}\) shows the direction of the tokenized prompts in the CLIP embedding space.

Our results in Fig. 0(a) (col3) shows the directional loss between \(\) and \(\)_i.e.,_\(_{dir}(,)\), for different tSAs using the reference images for each tSA. Note that \(_{dir}=0\) means perfect alignment. Our comparison reveal considerable misalignment between \(\) and \(\) implying that unrelated concepts are potentially encoded in \(\). Meanwhile, \(\) and \(\) near perfect alignment implies that

Figure 2: **T2I generation performance of HP, ITI-Gen , and our proposed FairQueue for target Sensitive Attributes (tSAs) with minimal linguistic ambiguities.** Samples generated by HP demonstrate outstanding performance with good fairness (FD), high quality (FID and FD), and good semantic preservation (DS). Meanwhile, ITI-Gen moderately degrades sample quality, impacting fairness and semantic preservation. FairQueue demonstrates comparable performance to HP, even surpassing HP in both quality and semantic preservation in many cases. Note that HP only performs well for unambiguous tSAs, and can not be used for general fair T2I generation purposes, as it can not be defined well for ambiguous tSAs (See Supp for detailed discussion).

these unrelated concepts are potentially transferred to \(\) via ITI-Gen's learning objective, resulting in distorted learned token \(\).

### Analyzing the Effect of ITI-GEN Prompts in T2I Generation

In the previous section, we observed degraded sample quality in ITI-Gen which we attribute to the sub-optimal training objective that results in learning distorted tokens. In this section, we take a step further to answer the question: _"Given a pre-trained T2I model and some distorted learned prompts as input, how do these distorted prompts affect the image generation process of the T2I model?"_

To answer this, we deep dive into the latent denoising network  and analyze the cross-attention mechanism -the bridge for text and image modules in T2I models . In this analysis, _we visualize the cross-attention maps to investigate potential anomalies caused by distorted tokens in the denoising process._ Specifically, we compare cross-attention maps of ITI-Gen prompt against HP with minimal linguistic ambiguity (as reference). To allow fair token-to-token comparison, in this experiment, we lengthen HP by including additional tokens containing synonyms of the tSA. Note that this did not augment HP's behavior, and similar results are seen in the original HP. See Supp for more details.

**Visualizing Cross-attention Maps.** We follow DAAM  for visualizing cross-attention maps by tracing attention scores in the cross-attention module to demonstrate how an input token within a prompt influences parts of the generated image. Specifically, to visualize the cross-attention map of a token, DAAM interpolates and accumulates the attention scores over all scales (layer of the U-Net  as the denoising network ), and all denoising steps. However, we tailor DAAM to the requirements of our fine-grained analysis by introducing further controls. First, we isolate the attention maps for each denoising step to allow for both step-wise and multi-step analysis. Second, we introduce a prompt-switching mechanism, allowing for the interchangeable tracing of different prompts at any particular denoising step.

Figure 3: **Comparison of cross-attention maps during the denoising process with HP (left) and ITI-Gen (right).** Here, we use tSA=Smiling and plot the denoising process for one sample generation. Each denoising process consists of \(l=50\) steps initiated with the same noisy input. Each cell depicts the attention map for the respective token (column) at the respective step (row) overlaid on the input. We highlight 3 key observations: 1�TI-Gen tokens \(_{i}\) have abnormal activities compared to the corresponding tSA-related tokens in HP by attending to unrelated regions (backgrounds) or scattered attention. 2�TI non-tSA tokens like ‘of’ and ‘a’ are abnormally more active in the presence of ITI-Gen tokens. 3�TI As compared to the HP counterpart, issues created by ITI-Gen tokens 1�TI & 2�TI) degrade the global structure in the early denoising steps (_e.g.,_ Step 15), for example, human face in HP vs some unrelated structure in ITI-Gen. The same behavior is observed for some other samples and tSAs (see Supp for more samples, and other tSAs with more denoising steps).

**Abnormalities in the Presence of Distorted Tokens.** To investigate potential anomalies arising from distorted tokens learned by ITI-Gen, we comprehensively analyze cross-attention maps of all denoising time steps on 500 generated samples per category of each tSA, for both ITI-Gen and HP. Fig. 3 shows one of these cross-attention maps comparing ITI-Gen and HP for tSA=Smiling (more examples in Supp). Our empirical investigation reveals four points: i) global structure is synthesized in the early steps of the denoising process aligning with previous works  that the denoising process progressively synthesizes the image. ii) Learned ITI-Gen tokens have abnormal attention compared to the tSA-related tokens in HP ("Smiling" in col. 7 of HP), _e.g., \([_{i}]\)_ contain unrelated or scatter activation (Issue 1). iii) In the presence of the ITI-Gen tokens, other non-tSA tokens (like \([^{}^{}]\) and \([^{}^{}]\)) are abnormally more active (Issue 2). We remark that tokens interact with each other in the denoising steps. iv) Considering Issues 1 & 2, we observe that degraded global structure is synthesized in the early steps of denoising, and eventually a degraded sample is generated at the end of the denoising. Note that similar issues occur with many other samples and tSAs (details in Supp).

To further understand issues, we isolate effect of distorted tokens by proposing two analyses focusing on different denoising steps. These two analyses dissect the influence of distorted tokens in key denoising steps (Recall denoising of a single step \(t\) is denoted by \(Z_{t+1} DM(Z_{t},,t,s)\), Sec. 2):

\[=DM(Z_{t},,t,s)&t[0,n-1]\\ DM(Z_{t},,t,s)&t[n,l],=DM(Z_{t},,t,s)&t[0,n-1]\\ DM(Z_{t},,t,s)&t[n,l]\] (2)

To do this, as seen in Fig.1b (col 2), we first propose **Analysis 1: switching prompt from ITI-Gen to HP (I2H)** during the denoising process. This allows for a better understanding of how the global structure's degradation in early denoising steps may affect the final generated output. Specifically, as in Eq. 2, I2H first utilizes _ITI-GEN prompt_\(\) in early denoising steps which potentially leads to degraded global structure. This is then followed by utilizing _hard prompt_\(\) for the remaining denoising steps. Next, to investigate if ITI-Gen tokens will create the same issues in the later steps of the denoising process, we propose **Analysis 2: HP to ITI-Gen (H2I)**. Converse to the previous experiment, we utilize \(\) in early steps of denoising, and then switch to using \(\) as input prompt. For each experiment, we plot and analyze the cumulative cross-attention maps for early steps (\(0\) to \(n-1\)) and later steps (\(n\) to \(l\)) separately. Fig. 4 shows an example of the cross-attention maps for

Figure 4: **Analyzing the accumulated cross-attention maps for the denoising process in our proposed prompt switching analysis I2H and H2I. Here, we use two tSAs: Smiling, High Cheekbones. For each tSA, we show the accumulated cross-attention maps for H2I and 12H, with some quantitative results. In the H2I (I2H) experiment, the first row shows the accumulated cross-attention maps during the early denoising steps with HP (ITI-Gen) as the input prompt, and the second row shows the maps during later steps, after switching to ITI-Gen (HP). Observation 1: Learned tokens in ITI-Gen affect early denoising steps, degrading global structure synthesis; such degraded global structure disrupts the final output. This is observed in I2H. Observation 2: Learned tokens in ITI-Gen works decently in the later stage of the denoising process if the global structure is synthesized properly. This is observed in H2I. As we show in Supp, similar observations can be made for other samples and other tSAs. Bottom: Histograms of our proposed metrics on cross-attention maps demonstrate the abnormalities in many samples.**these two experiments with tSA={Smiling, High Cheekbones}. See Supp for more samples and details. Considering results in Fig. 4 the following observations can be made:

**Observation 1:**_Learned tokens in_ ITI-Gen _affect the early steps of the denoising process leading to degradation in synthesizing global structure._ More specifically comparing the first row of the cross-attention map between I2H and H2I in Fig. 4, we can have the following observations: i) ITI-Gen tokens have more scattered attention or attending to unrelated regions compared to tSA-related tokens in HP; ii) non-tSA tokens like "a", and "of" are more active in the presence of the ITI-Gen tokens. These two abnormalities result in degraded global structure in the early steps. In addition, considering the second row of the I2H in Fig. 4, the degraded global structure in the early steps leads to disrupted final output even though the (non-distorted) HP prompt is used in later steps.

**Observation 2:**_Learned tokens in_ ITI-Gen _works decently in the later steps of the denoising process if the global structure is synthesized properly._ More specifically, considering H2I in Fig. 4, when HP prompts synthesize proper global structure in early steps, the ITI-Gen tokens attend to proper regions and contribute to adding the finer details related to tSA, as shown in the second row of H2I.

**Quantitative Metrics for Cross-attention Maps.** In addition to the visual demonstration, we propose two metrics to support further our observed abnormalities of ITI-Gen tokens in the early steps of denoising for a large number of generated samples. Specifically, for each generated sample: i) To quantify abnormally active attention associated with non-tSA tokens, we compute the expectation of **attention amplitude**: \(_{(x,y)}\{[]\}\), where \(\) is a non-tSA token such as "of". ii) We analyze the scatter in attention by measuring the second **central moment** for each tSA token \(\):

\[()=_{x,y}\{[(x-)^{2}+(y-)^{2}]}[]_{(x,y)}\}\] (3)

Here, \(}[]=([]/_{x,y}[])\), and (\(\), \(\)) is the centroid. The two metrics are computed on the accumulated cross-attention maps from stage 1 of I2H (for ITI-GEN) and H2I (for HP). The histograms of these two metrics for 500 generated samples in Fig. 0(b) (col 3) and Fig. 4 demonstrate Issues 1 & 2 in many generated samples.

**Remark.** Our thorough analysis in this section shows that distorted tokens learned by ITI-Gen only have destructive performance in the early steps of denoising, and they generally have decent performance in later steps when the global structure is formed properly (H2I). _We remark that even though H2I has decent performance in fair and high-quality T2I generation, it is only applicable to tSA with minimal linguistic ambiguity. In the next section, we will discuss our proposed method to address fair and high-quality T2I generation encompassing both ambiguous and unambiguous tSA._

## 4 Proposed Method

In this section, we present our proposed method, FairQueue a new generation framework consisting of two additions: _Prompt Queuing_ and _Attention Amplification_ to improve the sample quality when implementing fair T2I generation. In addition to quality improvements, FairQueue also allows for better semantic preservation of the original sample generated from the base prompt \(\).

**Prompt Queuing.** Recall that when utilizing ITI-Gen prompt \(\)-which is tuned to generate samples containing the tSA-degraded global structure occurs in early denoising steps for a moderate number of samples. Conversely, utilizing HP with minimal linguistic ambiguity enables high-quality and fair T2I generation. However, as such HPs are not available for all tSAs , we naturally consider the next best available option-the base prompt \(\) (a natural language prompt without the distorted trainable tokens)-and propose prompt queuing. Specifically, as seen in Fig. 0(c), prompt queuing first utilizes \(\) in the early \(n\) denoising steps, thereby allowing for the global structures to form properly. Next, we transit to ITI-Gen prompt \(\) for the remaining \((l-n)\) steps. This allows the more fine-grained tSA semantics to be developed on top of the already well-defined global structures.

**Attention Amplification.** By implementing prompt queuing, the output samples may experience a reduction in tSA expression due to the reduced exposure to the ITI-Gen prompt \(\). To address this, we propose Attention Amplification, an intuitive solution that emphasizes the expression of the tSA by scaling the ITI-Gen token's cross-attention maps, _i.e., \(c*[S_{i}]\)_ where \(c>1\).

## 5 Experiments

In this section, we evaluate our proposed (FairQueue) against the existing SOTA ITI-Gen  over various tSA. Then, we conduct an ablation study by first evaluating the contribution brought by each component for FairQueue _i.e.,_ Prompt queuing, and Attention Scaling. Then we revisit the task initially proposed by ITI-Gen : Training Once-for-All Token. Overall, we show that FairQueue achieves new SOTA performance.

**Experimental Setup.** Following , we utilize the publicly available reference dataset from CelebA , FairFace  and FAIR benchmark . For CelebA, we perform both single tSA and multi-tSA experiments. Note that in this dataset each tSA has two categories. In FairFace and FAIR datasets, tSAs have more categories, _e.g.,_ Age and Skin tones contain 9 and 6 classes, respectively. Therefore, fair generation is more challenging in these datasets. For these experiments we use \(\)=\(E_{t}\)("A headshot of a person") as base prompt, and for a fair comparison, we utilize exactly the same learned \(\) from ITI-Gen's original code  for both ITI-Gen and proposed FairQueue. In addition, we randomly sample a set of 500 latent codes and use the same latent codes for both approaches. As earlier discussed in Sec. 3, we utilize fairness discrepancy (FD) to evaluate fairness, Text-Alignment (TA) and FID for quality, and DreamSim (DS) to measure semantic preservation. See Supp for more details. We repeat this process 5 times and report the mean and standard deviation.

**Our results** in Tab. 1 demonstrate that FairQueue is able to match ITI-Gen fairness performance closely, and in some cases even improve upon it. For example, for tSA=Smiling, FairQueue indicates a significantly lower bias (FD=\(6.9\)e\({}^{-3}\)) than ITI-Gen (FD=\(124\)e\({}^{-3}\)). In addition, considering sample quality, FairQueue achieves an overall better performance than ITI-Gen for all datasets. For example, in CelebA, FairQueue's TA\( 0.66\) while ITI-Gen's TA\( 0.655\), with the worst performance with High Cheekbones (TA=\(0.595\)). These results are similarly reflected in FID. We remark that this quality degradation largely contributes to ITI-Gen fairness degradation. Finally, when considering semantic preservation (DS \(\)) of the original sample generated with \(\), FairQueue achieves unparalleled performance by ITI-Gen.

    \\  tSA &  & TA (\(\)) & FID (\(\)) & DS (\(\)) \\   & ITI-Gen & \(^{-3} 4.2^{-3}}\) & \(0.655 1.2^{-2}\) & \(78.9 1.3\) & \(0.337 1.4^{-2}\) \\  & Ours & \(^{-3} 3.8^{-3}}\) & \(^{-3}}\) & \(\) & \(^{-2}}\) \\   & ITI-Gen & \(^{-3} 8.1^{-3}}\) & \(0.633 9.4^{-3}\) & \(82.9 1.4\) & \(0.552 3.2^{-2}\) \\  & Ours & \(15.5^{-3} 3.8^{-3}\) & \(^{-3}}\) & \(\) & \(^{-2}}\) \\   & ITI-Gen & \(^{-3} 9.2^{-3}}\) & \(0.605 1.2^{-2}\) & \(88.6 0.9\) & \(0.557 2.2^{-2}\) \\  & Ours & \(^{-3} 4.2^{-3}}\) & \(^{-2}}\) & \(\) & \(^{-2}}\) \\   & ITI-Gen & \(318^{-3} 1.2^{-3}\) & \(0.595 1.2^{-3}\) & \(86.40 2.1\) & \(0.538 1.6^{-2}\) \\  & Ours & \(^{-3} 3.6^{-3}}\) & \(^{-3}}\) & \(\) & \(^{-2}}\) \\   & ITI-Gen & \(^{-3} 1.2^{-3}}\) & \(0.646 1.8^{-2}\) & \(101.3 4.6\) & \(0.552 3.2^{-2}\) \\  & Ours & \(^{-3} 1.2^{-3}}\) & \(^{-2}}\) & \(\) & \(^{-2}}\) \\   & ITI-Gen & \(^{-3} 2.6^{-3}}\) & \(0.654 3.3^{-3}\) & \(83.5 1.4\) & \(0.486 1.4^{-2}\) \\  & Ours & \(25.4^{-3} 1.9^{-3}\) & \(^{-3}}\) & \(\) & \(^{-2}}\) \\   & ITI-Gen & \(^{-3} 8.1^{-3}}\) & \(0.670 3.2^{-3}\) & \(85.0 3.3\) & \(0.452 3.6^{-3}\) \\  & Ours & \(^{-3} 1.2^{-3}}\) & \(^{-3}}\) & \(\) & \(^{-3}}\) \\   & ITI-Gen & \(^{-3} 8.8^{-3}}\) & \(0.647 2.2^{-3}\) & \(79.2 1.5\) & \(0.551 3.6^{-3}\) \\  & Ours & \(119^{-3} 7.2^{-3}\) & \(^{-3}}\) & \(\) & \(^{-3}}\) \\   & ITI-Gen & \(286^{-3} 6.8^{-3}\) & \(0.640 4.3^{-3}\) & \(87.3 2.1\) & \(0.533 2.9^{-3}\) \\  & Ours & \(^{-3} 7.1^{-3}}\) & \(^{-3}}\) & \(\) & \(^{-3}}\) \\   & ITI-Gen & \(39.1^{-3} 1.2^{-3}\) & \(0.668 7.1^{-3}\) & \(72.6 3.1\) & \(0.458 7.8^{-3}\) \\  & Ours & \(^{-3} 2.3^{-3}}\) & \(0.686 5.7^{-3}\) & \(\) & \(^{-3}}\) \\   & ITI-Gen & \(257^{-3} 8.7^{-3}\) & \(0.654 3.3^{-3}\) & \(65.2 1.6\) & \(0.475\

**Ablation study: evaluating prompt queuing and attention scaling.** To evaluate the contribution brought by FairQueue, we consider the same setup as Sec. 5 in the main manuscript focusing on the tSA Smiling. Here, we compare the performance when utilizing different attention amplification scaling factors, \(c\), and different prompt queuing transition points _i.e.,_ switching from \(\) to \(\). Specifically, we consider gradual increments in \(c\) and shifting of the transition points, step \(\{0,0.1l,0.2l,0.3l\}\) when \(l=50\).

Our results in Fig.5 illustrate that generally when \(c\) increases, fairness improves. However, a saturation point (\(c=10\)) exists where quality and semantic preservation beyond this point degrades. Then when considering different transition points, we find that at step \(0.2l\) FairQueue achieves the best quality and semantic preservation performance while still achieving good fairness measurements. However, increasing beyond this point results in significant fairness degradation.

**Ablation study: revisiting training once-for-all token.** Utilizing FairQueue we follow  and re-visit adapting pre-trained ITI-Gen, \(S_{i}\) to a new Base Prompt \(}=E_{t}\) ("A headshot of a doctor") by pre-pending. Then we generate samples utilizing both FairQueue and ITI-Gen with the same noise input. As seen in Fig. 6 FairQueue demonstrates better performance than ITI-Gen, achieving both better quality and semantic preservation of the sample generated by \(}\) while still having good tSA representation--more illustration in Supp.

## 6 Conclusion

In this paper, we reveal quality degradation in ITI-Gen -the existing SOTA fair T2I prompt learning approach. Our analysis reveals that this quality degradation is due to the distorted learned tokens in ITI-Gen prompt impacting cross-attention in the early steps of the denoising (reverse diffusion) process. To address this, we propose FairQueue a simple but effective solution consisting of: Prompt Queuing and Attention Amplification. Overall, our extensive experimentation demonstrates FairQueue achieves new SOTA performance in balancing quality, fairness, and semantic preservation. **Limitation, related work and additional experiments can be found in the Supp.**