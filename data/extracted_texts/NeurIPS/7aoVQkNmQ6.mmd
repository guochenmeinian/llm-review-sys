# Generalized equivalences between subsampling and ridge regularization

Pratik Patil

Department of Statistics

University of California

Berkeley, CA 94720, USA

pratikpatiil@berkeley.edu

&Jin-Hong Du

Department of Statistics and Data Science

& Machine Learning Department

Carnegie Mellon University

Pittsburgh, PA 15213, USA

jinhongd@andrew.cmu.edu

###### Abstract

We establish precise structural and risk equivalences between subsampling and ridge regularization for ensemble ridge estimators. Specifically, we prove that linear and quadratic functionals of subsample ridge estimators, when fitted with different ridge regularization levels \(\) and subsample aspect ratios \(\), are asymptotically equivalent along specific paths in the \((,)\)-plane (where \(\) is the ratio of the feature dimension to the subsample size). Our results only require bounded moment assumptions on feature and response distributions and allow for arbitrary joint distributions. Furthermore, we provide a data-dependent method to determine the equivalent paths of \((,)\). An indirect implication of our equivalences is that optimally tuned ridge regression exhibits a monotonic prediction risk in the data aspect ratio. This resolves a recent open problem raised by Nakkiran et al.  for general data distributions under proportional asymptotics, assuming a mild regularity condition that maintains regression hardness through linearized signal-to-noise ratios.

## 1 Introduction

Ensemble methods, such as bagging [2; 3], are powerful tools that combine weak predictors to improve predictive stability and accuracy. This paper focuses on sampling-based ensembles, which exhibit an implicit regularization effect [4; 5; 6]. Specifically, we investigate subsample ridge ensembles, where the ridge predictors are fitted on independently subsampled datasets [7; 8; 9]. Recent work has demonstrated that a full ensemble (that is fitted on all possible subsampled datasets) of ridgeless  predictors achieves the same squared prediction risk as a ridge predictor fitted on full data [11; 12; 13].

To be precise, let \(\) be the limiting dataset aspect ratio \(p/n\), where \(p\) is the feature dimension, and \(n\) is the number of observations. For a given \(\), the limiting prediction risk of subsample ridge ensembles is parameterized by \((,)\), where \( 0\) is the ridge regularization parameter and \(\) is the limiting subsample aspect ratio \(p/k\), with \(k\) being the subsample size [11; 12]. Under isotropic features and a well-specified linear model, [11; 12] show that the squared prediction risk at \((0,^{*})\) (the optimal ridgeless ensemble) is the same as the risk at \((^{*},)\) (the optimal ridge), where \(^{*}\) and \(^{*}\) are the optimal subsample aspect ratio and ridge penalty, respectively. Furthermore, this equivalence of prediction risk between subsampling and ridge regularization is extended in  to anisotropic linear models. As an application,  also demonstrates how generalized cross-validation for ridge regression can be naturally transferred to subsample ridge ensembles. In essence, these results suggest that subsampling a smaller number of observations has the same effect as adding an appropriately larger level of ridge penalty. These findings prompt two important open questions:

1. **The extent of equivalences.** The previous works all focus on equivalences of the squared prediction risk when the test distribution matches the train distribution. In real-world scenarios,however, there are often covariate and label shifts, making it crucial to consider prediction risks under such shifts. In addition, other risk measures, such as training error, estimation risk, coefficient errors, and more, are also of interest in various inferential tasks. A natural question is then whether similar equivalences hold for such "generalized" risk measures. At a basic level, the question boils down to whether any equivalences exist at the "estimator level". Answering this question would establish a connection between the ensemble and the ridge estimators, facilitating the exchange of various downstream inferential statements between the two sets of estimators.
2. **The role of linear model.** All previous works assume a well-specified linear model between the responses and the features, which rarely holds in practical applications. A natural question is whether the equivalences hold for _arbitrary_ joint distributions of the response and the features or whether they are merely an artifact of the linear model. Addressing this question broadens the applicability of such equivalences beyond simplistic models to real-world scenarios, where the relationship between the response and the features is typically intricate and unknown.

We provide answers to questions raised in both directions. We demonstrate that the equivalences hold for the generalized squared risks in the full ensemble. Further, these equivalences fundamentally occur at the estimator level for any arbitrary ensemble size. Importantly, these results are not limited to linear models of the data-generating process. Below we provide a summary of our main results.

### Summary of contributions

1. [leftmargin=*]
2. **Risk equivalences.** We establish asymptotic equivalences of the full-ensemble ridge estimators at different ridge penalties \(\) and subsample ratios \(\) along specific paths in the \((,)\)-plane for a variety of generalized risk functionals. This class of functionals includes commonly used risk measures (see Table 2), and our results hold for both independent and dependent coefficient matrices that define these risk functionals (see Theorem 1 and Proposition 2). In addition, we demonstrate that the equivalence path remains unchanged across all the functionals examined.
3. **Structural equivalences.** We establish structural equivalences in the form of linear functionals of the ensemble ridge estimators, which hold for arbitrary ensemble sizes (see Theorem 3). Our proofs for both structural and risk equivalences exploit properties of certain fixed equations that arise in our analysis, enabling us to explicitly characterize paths that yield equivalent estimators in the \((,)\)-plane (see Equation (5)). In addition, we provide an entirely data-driven construction of this path using certain companion Stieltjes transform relations of random matrices (see Proposition 4).
4. **Equivalence implications.** As an implication of our equivalences, we show that the prediction risk of an optimally-tuned ridge estimator is monotonically increasing in the data aspect ratio under mild regularity conditions (see Theorem 6). This is an indirect consequence of our general equivalence results that leverages the provable monotonicity of the subsample-optimized estimator. Under proportional asymptotics, our result settles a recent open question raised by Nakkiran et al. [1, Conjecture 1] concerning the monotonicity of optimal ridge regression under anisotropic features and general data models while maintaining a regularity condition that preserves the linearized signal-to-noise ratios across regression problems.
5. **Generality of equivalences.** Our main results apply to arbitrary responses with bounded \(4+\) moments for some \(>0\), as well as features with similar bounded moments and general

    &  &  \\   & pred. risk & gen. risk & estimator & response & feature & lim. spectrum \\  LeJeune et al.  & \(}^{}\) & & & linear &  isotropic \\ Gaussian \\ isotropic \\ RMT \\  & exists \\ Patil et al.  & \(}^{}\) & & & linear &  anisotropic \\ RMT \\  & exists \\ Du et al.  & \(}\) & & & linear &  anisotropic \\ RMT \\  & exists \\ This work & \(}\) & \(}\) & \(}\) & arbitrary & 
 anisotropic \\ RMT \\  & need not exist \\   

Table 1: Comparison with related work. The marker “\(}^{}\)” indicates a partial equivalence result connecting the _optimal_ prediction risk of the ridge predictor and the full ridgeless ensemble predictor.

covariance structures. We demonstrate the practical implications of our findings on real-world datasets (see Section 6). On the technical side, we extend the tools developed in  to handle the dependency between the linear and the non-linear components and obtain model-free equivalences. Furthermore, we extend our analysis to include generalized ridge regression (see Corollary 5). Through experiments, we demonstrate the universality of the equivalences for random and kernel features and conjecture the associated data-driven prediction risk equivalence paths (see Section 6).

### Related work

Ensemble learning.Ensemble methods yield strong predictors by combining weak predictors . The most common strategy for building ensembles is based on subsampling observations, which includes bagging [2; 3], random forests , neural network ensembles [17; 18; 19], among the others. The effect of sampling-based ensembles has been studied in the statistical physics literature [20; 21; 22] as well as statistical learning literature [2; 3]. Recent works have attempted to explain the success of ensemble learning by suggesting that subsampling induces a regularizing effect [4; 6]. Under proportional asymptotics, the effect of ensemble random feature models has been investigated in [23; 24; 25]. There has been growing interest in connecting the effect of ensemble learning to explicit regularization: see  for related experimental evidence under the umbrella of "mini-patch" learning. Towards making these connections precise, some work has been done in the context of the ridge and ridgeless ensembles: [11; 12; 13] investigate the relationship between subsampling and regularization for ridge ensembles in the overparameterized regime. We will delve into these works in detail next.

Ensembles and ridge equivalences.In the study of ridge ensembles, LeJeune et al.  show that the optimal ridge predictor has the same prediction risk as the ridgeless ensemble under the Gaussian isotropic design, which has been extended in  for RMT features (see Assumption 2 for definition). Specifically, these works establish the prediction risk equivalences for a specific pair of \((^{*},)\) and \((0,^{*})\). The results are then extended to the entire range of \((,)\) using deterministic asymptotic risk formulas , assuming a well-specified linear model, RMT features, and the existence of a fixed limiting spectral distribution of the covariance matrix. Our work significantly broadens the scope of results connecting subsampling and ridge regularization. In particular, we allow for arbitrary joint distributions of the data, anisotropic features with only bounded moments, and do not assume the convergence of the spectrum of the population covariance matrix. Furthermore, we expand the applicability of equivalences to encompass both the estimators and various generalized squared risk functionals. See Table 1 for an explicit comparison and Section 3 for additional related comparisons.

Other ridge connections.Beyond the connection connections between implicit regularization induced by subsampling and explicit ridge regularization examined in this paper, ridge regression shares ties with various other forms of regularization. For example, ridge regression has been linked to dropout regularization [27; 28], variable splitting in random forests , noisy training [29; 30], random sketched regression [31; 32; 33], various forms of data augmentation , feature augmentation [9; 35], early stopping in gradient descent and its variants [36; 37; 38; 39], among others. These connections highlight the pervasiveness and the significance of understanding various "facets" of ridge regularization. In that direction, our work contributes to expand the equivalences between subsampling and ridge regularization.

## 2 Notation and preliminaries

Ensemble estimators.Let \(_{n}=\{(_{i},y_{i}):i[n]\}\) be a dataset containing i.i.d. samples in \(^{p}\). Let \(^{n p}\) and \(^{n}\) be the feature matrix and response vector with \(_{i}^{}\) and \(y_{i}\) in \(i\)-th rows. For an index set \(I[n]\) of size \(k\), let \(_{I}=\{(_{i},y_{i}):\,i I\}\) be the associated subsampled dataset. Let \(_{I}^{n n}\) be a diagonal matrix such that its \(i\)-th diagonal entry is \(1\) if \(i I\) and \(0\) otherwise. Note that the feature matrix and response vector associated with \(_{I}\) respectively are \(_{I}\) and \(_{I}\). Given a ridge penalty \(>0\), the _ridge_ estimator fitted on \(_{I}\) consisting of \(k\) samples is given by:

\[}_{k}^{}(_{I})=*{argmin}_{ ^{p}}_{i I}(y_{i}-_{i}^{} )^{2}+\|\|_{2}^{2}=(^{} _{I}+_{p})^{-1}^{}_{I} }{k}.\] (1)

The _ridgeless_ estimator \(}_{k}^{0}(_{I})=(^{}_{I}/ k)^{+}^{}_{I}/k\) is obtained by sending \( 0^{+}\), where \(^{+}\) denotes the Moore-Penrose inverse of matrix \(\). To define the ensemble estimator, denote the set of all \(k\) distinct elements from \([n]\) by \(_{k}=\{\{i_{1},i_{2},,i_{k}\}:\,1 i_{1}<i_{2}<<i_{k} n\}\). For \( 0\), the _\(M\)-ensemble_ and the _full-ensemble_ estimators are respectively defined as follows:

\[}_{k,M}^{}(_{n};\{I_{}\}_{=1}^{M} )=_{[M]}}_{k}^{}(_ {I_{}}),}_{k,}^{}( _{n})=[}_{k}^{}(_{I} )_{n}].\] (2)

Here \(\{I_{}\}_{=1}^{M}\) and \(I\) are simple random samples from \(_{k}\). In (2), the full-ensemble ridge estimator \(}_{k,}^{}(_{n})\) is the average of predictors fitted on all possible subsampled datasets, It is shown in Lemma A.1 of  that \(}_{k,}^{}(_{n})\) is also asymptotically equivalent to the limit of \(}_{k,M}^{}(_{n};\{I_{}\}_{=1}^{M})\) as the ensemble size \(M\) (conditioning on the full dataset \(_{n}\)). This also justifies using \(\) in the subscript to denote the full-ensemble estimator. For brevity, we simply write the estimators as \(}_{k,M}^{}\), \(}_{k,}^{}\), and drop the dependency on \(_{n}\), \(\{I_{}\}_{=1}^{M}\). We will show certain structural equivalences in terms of linear projections in the family of estimators \(}_{k,M}^{}\) along certain paths in the \((,p/k)\)-plane for arbitrary ensemble size \(M\{\}\) (in Theorem 3). Apart from connecting the estimators, we will also show equivalences of various notions of risks (in Theorem 1), which we describe next.

**Generalized risks.** Since the ensemble ridge estimators are linear estimators, we evaluate their performance relative to the oracle parameter: \(_{0}=[^{}]^{-1}[y]\), which is the best (population) linear projection of \(y\) onto \(\) and minimizes the linear regression error (see, e.g., ). Note that we can decompose any response \(y\) into: \(y=f_{}()+f_{}()\), where \(f_{}()=_{0}^{}\) is the oracle linear predictor, and \(f_{}()=y-f_{}()\) is the nonlinear component that is not explained by \(f_{}()\). The best linear projection has the useful property that \(f_{}()\) is (linearly) uncorrelated with \(f_{}()\), although they are generally dependent. It is worth mentioning that this does not imply that \(y\) and \(\) follow a linear regression model. Indeed, our framework allows any nonlinear dependence structure between them and is model-free for the joint distribution of \((,y)\). Relative to \(_{0}\), we measure the performance of an estimator \(}\) via the generalized mean squared risk defined compactly as follows:

\[R(};,,_{0})=()}\|L_{,}(}-_{0})\|_{2}^{2},\] (3)

where \(L_{,}()=+\) is a linear functional. Note that \(\) and \(\) can potentially depend on the data, and their dimensions can vary depending on the statistical learning problem at hand. This framework includes various important statistical learning problems, as summarized in Table 2. Observe that the framework also includes various notions of prediction risks. One can use the test error formulation in Table 2 to obtain the prediction risk at any test point \((_{0},y_{0})\), where \(y_{0}=_{0}^{}_{0}+_{0}\), and \(_{0}\) may depend on \(_{0}\) and have non-zero mean. This also permits the test point to be drawn from a distribution that differs from the training distribution. Specifically, when \(_{0}=f_{}(_{0})\) but \(_{0}\) has a distribution different from \(\), we obtain the prediction error under _covariate shift_. Similarly, when \(_{0} f_{}(_{0})\) but \(_{0}\) and \(\) have the same distribution, we get the prediction error under _label shift_.

**Asymptotic equivalence.** For our theoretical analysis, we consider the proportional asymptotics regime, where the ratio of the feature size \(p\) to the sample size \(n\) tends to a fixed limiting _data aspect ratio_\((0,)\). To concisely present our results, we will use the elegant framework of asymptotic equivalence . Let \(_{p}\) and \(_{p}\) be sequences of (additively) conformable matrices of arbitrary dimensions (including vectors and scalars). We say that \(_{p}\) and \(_{p}\) are _asymptotically equivalent_, denoted as \(_{p}_{p}\), if \(_{p}|[_{p}(_{p}-_{p})]|=0\) almost surely for any sequence of random matrices \(_{p}\) with bounded trace norm that are (multiplicatively) conformable and independent of \(_{p}\) and \(_{p}\). Note that for sequences of scalar random variables, the definition simply reduces to the typical almost sure convergence of sequences of random variables involved.

  
**Statistical learning problem** & \(L_{,}(}-_{0})\) & \(\) & \(\) & \(()\) \\  vector coefficient estimation & \(}-_{0}\) & \(_{p}\) & \(0\) & \(p\) \\ projected coefficient estimation & \(^{}(}-_{0})\) & \(^{}\) & \(0\) & \(1\) \\ training error estimation & \(}-\) & \(\) & \(-\

## 3 Generalized risk equivalences of ensemble estimators

We begin by examining the risk equivalences among different ensemble ridge estimators for various generalized risks defined as in (3). To prepare for our upcoming results, we impose two structural and moment assumptions on the distributions of the response variable and the feature vector.

**Assumption 1** (Response variable distribution).: Each response variable \(y_{i}\) for \(i[n]\) has mean \(0\) and satisfies \([|y_{i}|^{4+}] M_{}<\) for some \(>0\) and a constant \(M_{}\).

**Assumption 2** (Feature vector distribution).: Each feature vector \(_{i}\) for \(i[n]\) can be decomposed as \(_{i}=^{1/2}_{i}\), where \(_{i}^{p}\) contains i.i.d. entries \(z_{ij}\) for \(j[p]\) with mean \(0\), variance \(1\), and satisfy \([|z_{ij}|^{4+}] M_{}<\) for some \(>0\) and a constant \(M_{}\), and \(^{p p}\) is a deterministic and symmetric matrix with eigenvalues uniformly bounded between constants \(r_{}>0\) and \(r_{}<\).

Note that we do not impose any specific model assumptions on the response variable \(y\) in relation to the feature vector \(\). We only assume bounded moments as stated in Assumption 1, making all of our subsequent results model-free. The zero-mean assumption for \(y\) is for simplicity since, in practice, centering can always be done by subtracting the sample mean. The bounded moment condition can also be satisfied if one imposes a stronger distributional assumption (e.g., sub-Gaussianity). Assumption 2 on the feature vector is common in the study of random matrix theory [45; 46] and the analysis of ridge and ridgeless regression [47; 48; 49; 10], which we refer to as _RMT features_ for brevity.

Given a limiting data aspect ratio \((0,)\) and a limiting _subsample aspect ratio_\([,]\), our statement of equivalences between different ensemble estimators is defined through certain paths characterized by two endpoints \((0,)\) and \((,)\). These endpoints correspond to the subsample ridgeless ensemble and the (non-ensemble) ridge predictor, respectively, with \(\) being the ridge penalty to be defined next. For that, let \(H_{p}\) be the empirical spectral distribution of \(\): \(H_{p}(r)=p^{-1}_{i=1}^{p}_{\{r_{i} r\}}\), where \(r_{i}\)'s are the eigenvalues of \(\). Consider the following system of equations in \(\) and \(v\):

\[=+\,H_{p}(r), =\,H _{p}(r).\] (4)

Existence and uniqueness of the solution \((,v)[0,]^{2}\) to the above equations are guaranteed by Corollary E.4. Now, define a path \((;,)\) that passes through the endpoints \((0,)\) and \((,)\):

\[(;,)=(1-)( ,)+(0,)}.\] (5)

We are ready to state our results. We consider two cases for the generalized risk (3) depending on the relationships between \((,)\) and \(\). In the first case, when both \(\) and \(\) are independent of the data, Theorem 1 shows that the generalized risks are equivalent along the path (5).

**Theorem 1** (Risk equivalences when \((,)\!\!\!(,)\)).: _Suppose Assumptions 1-2 hold. Let \((,)\) be independent of \((,)\) such that \(()^{-1/2}\|\|_{}\) and \(\|\|_{2}\) are almost surely bounded. Let \(n,p\) such that \(p/n(0,)\). For any \([,+]\), let \(\) be as defined in (4). Then, for any pair of \((_{1},_{1})\) and \((_{2},_{2})\) on the path \((;,)\) as defined in (5), the generalized risk functionals (3) of the full-ensemble estimator are asymptotically equivalent:_

\[R}_{ p/_{1},}^{_{1}}; ,,_{0}\ \ R}_{ p/_{2},}^{ _{2}};,,_{0}.\] (6)

In other words, Theorem 1 establishes the equivalences of subsampling and ridge regularization in terms of generalized risk for many statistical learning problems, encompassing coefficient estimation,

Figure 1: Heat map of the various generalized risks (estimation risk, training error, prediction risk, out-of-distribution (OOD) prediction risk) of full-ensemble ridge estimators (approximated with \(M=100\)), for varying ridge penalties \(\) and subsample aspect ratios \(=p/k\) on the log-log scale. The data model is described in Appendix F.2 with \(p=500\), \(n=5000\), and \(=p/n=0.1\). Observe that along the same path the values match well for all risks, in line with Theorem 1 and Proposition 2.

coefficient confidence interval, and test error estimation. All of these problems fall in the category when \((,)\) are independent of \(\). However, there are other statistical learning problems not covered in Theorem 1, such as the in-sample prediction risk and the training error, which correspond to the case when \(=\). Fortunately, the equivalences also apply to them, as summarized in Proposition 2.

**Proposition 2** (Risk equivalences when \(=\)).: _Under Assumptions 1-2, when \(=\), the conclusion in Theorem 1 continue to hold for the cases of in-sample prediction risk (\(=\)) and training error (\(=-_{}\))._

Both Theorem 1 and Proposition 2 provide specific second-order equivalences for the full-ensemble ridge estimators, in the sense that the quadratic functionals of the estimators associated with \((,)\) are asymptotically the same. See Figure 1 for visual illustrations of both equivalences. These statements are presented separately because their proofs differ, with the proof for the dependent case being more intricate. We note that by combining the risk functionals in Table 2, it is possible to further extend the equivalences to other types of functionals not directly covered by statements above, using composition and continuous mapping. For example, it is not difficult to show that similar equivalences hold for the generalized cross-validation in the full ensembles . This follows by combining the result for training error from Proposition 2 and for denominator concentration proved in Lemma 3.4 of .

Theorem 1 generalizes the existing equivalence results for the prediction risk [11; 12; 13] to include general risk functionals under milder assumptions. As summarized in Table 1, we allow for a general response model and feature covariance without assuming convergence of the spectral distributions \(H_{p}\) to a fixed distribution \(H\). This flexibility is achieved by showing equivalences of the underlying resolvents in the estimators rather than deriving the limiting functionals as done in previous works. To extend results allowing for a general response, we generalize certain concentration results by leveraging tools from  to handle the dependency between the linear and non-linear components.

As alluded to earlier, it is worth noting that the generalized risk equivalences only hold in the full ensemble. However, by using the risk decomposition obtained from Lemma S.1.1 of  for finite ensembles, one can obtain the following relationship along the path in Theorem 3: \(R(}^{_{1}}_{ p/_{1},M};, ,_{0})-R(}^{_{2}}_{ p/_ {2},M};,,_{0})\ \ /M,\) for some \(\) (independent of \(M\)) that is eventually almost surely bounded. In other words, the difference between any two estimators on the same path scales as \(1/M\) when \(n\) is sufficiently large, as numerically verified in Figure F5.

## 4 Structural equivalences of ensemble estimators

The equivalences established in the previous section only hold for the full-ensemble estimators in a second-order sense. We can go a step further and ask if there exist any equivalences for the finite ensemble and if there are any equivalences at the estimator coordinate level between the estimators. Specifically, we aim to inspect whether each coordinate of the \(p\)-dimensional estimated coefficients asymptotically equals. This section establishes structural relationships between the estimators in a first-order sense that any bounded linear functionals of them are asymptotically the same.

**Theorem 3** (Structural equivalences).: _Suppose Assumptions 1-2 hold. Let \(n,p\) with \(p/n(0,)\). For any \([,+]\), let \(\) be as in (4). Then, for any \(M\{\}\) and any pair of \((_{1},_{1})\) and \((_{2},_{2})\) on the path (5), the \(M\)-ensemble estimators are asymptotically equivalent:_

\[}^{_{1}}_{ p/_{1},M}\ \ }^{_{2}}_{ p/_{2},M}.\] (7)

Put another way, Theorem 3 implies that for any fixed ensemble size \(M\), the \(M\)-ensemble ridgeless estimator at the limiting aspect ratio \(\) is equivalent to ridge regression at the regularization level \(\). This equivalence is visually illustrated in Figure 2, where the data is simulated from an anisotropic and nonlinear model (see Appendix F.2 for more details). Furthermore, the contour path \((;,)\) connecting the endpoints \((0,)\) and \((,)\) is a straight line, although it may have varying slopes. Along any path, all \(M\)-ensemble estimators at the limiting aspect ratio \(\) and ridge penalty \(\) are equivalent for all \((,)(;,)\). The equivalences here are for any arbitrary linear combinations of the estimators. In particular, this implies that the predicted values (or even any continuous function applied to them due to the continuous mapping theorem) of any test point will eventually be the same, almost surely, with respect to the training data. Finally, we remark that in the statement of Theorem 3, the equivalence is defined on extended reals in order to incorporate the ridgeless case when \(=1\).

The paths (5) in Theorem 3 are defined via the spectral distribution \(H_{p}\), which requires knowledge of \(\). This is often difficult to obtain in practice from the observed data. Fortunately, we can provide an alternative characterization for the path (5) solely through the data, as summarized in Proposition 4.

**Proposition 4** (Data-dependent equivalence path characterization).: _Suppose Assumptions 1-2 hold. Define \(_{n}=p/n\). Let \(k n\) be the subsample size and denote by \(_{n}=p/k\). For any \(M\{\}\), let \(_{n}\) be the value that satisfies the following equation in ensemble ridgeless and ridge gram matrices:_

\[_{=1}^{M}[(_{I_{}}^{}_{I_{}})^{+}]=[(^{}+_{n} _{n})^{-1}].\] (8)

_Define the data-dependent path \(_{n}=(_{n};_{n},_{n})\). Then, the conclusion in Theorem 3 continues to hold if we replace the (population) path \(\) by the (data-dependent) path \(_{n}\)._

The term "data-dependent path" signifies that we can estimate the level of implicit regularization induced by subsampling solely based on the observed data by solving (8). We remark that (8) always has at least one solution for a given triplet of \((n,k,p)\). This is because the right-hand side of (8) is monotonically decreasing in \(\), and the left-hand side always lies within the range of the right-hand side. The powerful implication of the characterization in Proposition 4 is that it enables practical computation of the path using real-world data. In Section 6, we will demonstrate this approach on various real-world datasets, allowing us to predict an equivalent amount of explicit regularization matching the implicit regularization due to subsampling.

One natural extension of the results is to consider the generalized ridge as a base estimator (1):

\[}_{k}^{,}(_{I})=*{ argmin}_{^{p}}_{i I}(y_{i}-_{i}^{ })^{2}+\|^{}\|_{2}^{2},\] (9)

where \(^{p p}\) is a positive definite matrix. The equivalences still hold, albeit on different paths:

**Corollary 5** (Equivalences for generalized ridge regression).: _Suppose Assumptions 1-2 hold. Let \(^{p p}\) be a deterministic and symmetric matrix with eigenvalues uniformly bounded between constants \(g_{}>0\) and \(g_{}<\). Let \(n,p\) such that \(p/n(0,)\). For any \([,+]\), let \(\) be as defined in (4) with \(H_{p}\) replaced \(_{p}\), the empirical spectral distribution of \(G^{-1/2}^{-1/2}\). Then, the conclusions in Theorems 1 and 3 continue to hold for the generalized ridge ensemble predictors._

Another extension of our results is to incorporate subquadratic risk functionals in Theorem 1 to derive equivalences of the cumulative distribution functions of the predictive error distributions associated

Figure 2: Heat map of the values of various linear functionals of ensemble ridge estimators \(^{}}_{k,M}^{}\), for varying ridge penalties \(\), subsample aspect ratios \(=p/k\), and ensemble size \(M\) on the log-log scale. The data model is described in Appendix F.2 with \(p=1000\), \(n=10000\), and \(=p/n=0.1\). (a) The values of the uniformly weighted projection for varying ensemble sizes (\(M\) = 1, 5, and \(50\)). (b) The values of \(M=100\) for varying projection vectors (uniformly weighted, random Gaussian, and random student’s \(t\)). The black dashed line is estimated based on Proposition 4 with \(=2\). Observe that the values along the data-dependent paths are indeed very similar, in line with Proposition 4.

with the estimators along the equivalence paths. One can use such equivalences for downstream inference questions. Finally, while our statements are asymptotic in nature, we expect a similar analysis as done in  can yield finite-sample statements. We leave these extensions for the future.

## 5 Implications of equivalences

The generalized risk equivalences establish a connection between the risks of different ridge ensembles, including the ridge predictors on full data and the full-ensemble ridgeless predictors. These connections enable us to transfer properties from one estimator to the other. For example, by examining the impact of subsampling on the estimator's performance, we can better understand how to optimize the ridge penalty. We will next provide an example of this. Many common methods, such as ridgeless or lassoless predictors, have been recently shown to exhibit non-monotonic behavior in the sample size or the limiting aspect ratio. An open problem raised by Nakkiran et al. [1, Conjecture 1] asks whether the prediction risk of the ridge regression with optimal ridge penalty \(^{*}\) is monotonically increasing in the data aspect ratio \(=p/n\). The non-monotonicity risk behavior implies that more data can hurt performance, and it's important to investigate whether optimal ridge regression also suffers from this issue. Our equivalence results provide a surprising (even to us) indirect way to answer this question affirmatively under proportional asymptotics.

To analyze the monotonicity of the risk, we will need to impose two additional regularity assumptions to guarantee the convergence of the prediction risk. Let \(=_{j=1}^{p}r_{j}_{j}_{j}^{}\) denote the eigenvalue decomposition, where \( r_{j},_{j}\)'s are pairs of associated eigenvalue and normalized eigenvector. The following assumptions ensure that the hardness of the underlying regression problems across different limiting data aspect ratios is comparable.

**Assumption 3** (Spectral convergence).: We assume there exists a deterministic distribution \(H\) such that the empirical spectral distribution of \(\), \(H_{p}(r)=p^{-1}_{i=1}^{p}_{\{r_{i} r\}}\), weakly converges to \(H\), almost surely (with respect to \(\)).

**Assumption 4** (Limiting signal and noise energies).: We assume there exists a deterministic distribution \(G\) such that the empirical distribution of \(_{0}\)'s (squared) projection onto \(\)'s eigenspace, \(G_{p}(r)=\|_{0}\|_{2}^{-2}_{i=1}^{p}(_{0}^{ }_{i})^{2}\ _{\{r_{i} r\}}\), weakly converges to \(G\). As \(n,p\) and \(p/n(0,)\), the limiting linearized energy \(^{2}=\|_{0}\|_{2}^{2}\) and the limiting nonlinearized energy \(^{2}=\|f_{}}\|_{L_{2}}^{2}\) are finite.

Assumption 3 is commonly used in random matrix theory and overparameterized learning  and ensures that the limiting spectral distribution of the sample covariance matrix converges to a fixed distribution. Under these assumptions, we show in Appendix C that there exists a deterministic function \((;,)\), such that \(R(}_{k,}^{};,,_{0}) (;,)\). Notably, the deterministic profile on the right-hand side is a monotonically increasing and a continuous function in the first parameter \(\) when limiting values of \(\|_{0}\|_{2}^{2}\) and \(\|f_{}}\|_{L_{2}}^{2}\) are fixed (i.e., when Assumption 4 holds). Moreover, the deterministic risk equivalent of the optimal ridge predictor matches that of the optimal full-ensemble ridgeless predictor; in other words, \(_{ 0}(;,)=_{} (0;,)\). The same monotonicity property of the two optimized functions leads to the following result.

**Theorem 6** (Monotonicity of prediction risk with optimal ridge regularization).: _Suppose the conditions of Theorem 1 and Assumptions 3-4 hold. Let \(k,n,p\) such that \(p/n(0,)\) and \(p/k[,]\). Then, for \(=^{1/2}\) and \(=\), the optimal risk of the ridgeless ensemble, \(_{}(0;,)\), is monotonically increasing in \(\). Consequently, the optimal risk of the ridge predictor, \(_{ 0}(;,)\), is also monotonically increasing in \(\)._

Under Assumptions 3 and 4, the linearized signal-to-noise ratio (SNR) is maintained at the same level across varying distributions as \(\) changes. The key message of Theorem 6 is then that, for a sequence of problems with the same SNR (indicating the same level of regression hardness), the asymptotic prediction risk of optimized ridge risk gets monotonically worse as the aspect ratio of the problem increases. This is intuitive because a smaller \(\) corresponds to more samples than features. In this sense, Theorem 6 certifies that optimal ridge uses the available data effectively, avoiding sample-wise non-monotonicity. Moreover, as the null risk is finite, this also shows that the optimal ridge estimator mitigates the "double or multiple descents" behaviors in the generalization error .

Attempting to prove Theorem 6 directly is challenging due to the lack of a closed-form expression for the optimal risk of ridge regression in general. Moreover, the risk profile of ridge regression,\((;,)\), does not exhibit any particular structure as a function of \(\). On the other hand, the risk profile of the full-ensemble ridgeless, \((0;,)\), has a very nice structure in terms of \(\). This, coupled with our equivalence result, allows one to prove quite non-trivial behavior of optimal ridge. See Figure F6 for illustrations.

## 6 Discussion

Motivated by the recent subsampling and ridge equivalences for the prediction risk [11; 12; 13], this paper establishes generalized risk equivalences (Section 3) and structural equivalences (Section 4) within the family of ensemble ridge estimators. Our results precisely link the implicit regularization of subsampling to explicit ridge penalization via a path \(\) defined in (5), which connects two endpoints \((0,)\) and \((,)\) regardless of the risk functionals. Furthermore, we provide a data-dependent method (Proposition 4) to estimate this path. Our results do not assume any specific relationship between the response variable \(y\) and the feature vector \(\). We next explore some extensions of these equivalences.

**Real data.** While our theoretical results assume RMT features (Assumption 2), we anticipate that the equivalences will very likely hold under more general features . To verify this, we examine the equivalences in Proposition 4 with \(M=100\) on real-world datasets. Figure 3 depicts both the linear functionals in Theorem 3 and the generalized quadratic functionals in Theorem 1 computed along four paths, shown in different colors. We observe that the variation within each path is small, while different paths have different functional values. This suggests that the theoretical finding in the previous sections also hold quite well on real-world datasets. We investigate this further by varying \(\) on the three image datasets, CIFAR-10, MNIST, and USPS. Figure F7 shows similar values and trends at the two points. These experiments demonstrate that the amount of explicit regularization induced by subsampling can indeed be accurately predicted based on the observed data using Proposition 4.

**Random features.** Closely related to two-layer neural networks , we can consider the random feature model, \(f(;,)=^{}()\), where \(^{d p}\) is some randomly initialized weight matrix, and \(:\) is a nonlinear activation function applied element-wise to \(\). As from "universality/invariance", certain random feature models are asymptotically equivalent to a surrogate linear Gaussian model with a matching covariance matrix , we expect the theoretical results in Theorems 3-1 to likely hold, although the relationship (4) will now depend on the non-linear activation functions. Empirically, we indeed observe similar equivalence phenomena of the prediction risks with random features ridge regression using sigmoid, ReLU, and tanh activation functions, as shown in Figure 4. Analogous to Proposition 4, we conjecture a similar data-driven relationship between \((_{n},_{n})\) and \((0,_{n})\) for random features \(}=(^{})\):

**Conjecture 7** (Data-dependent equivalences for random features (informal)).: _Suppose Assumptions 1-2 hold. Define \(_{n}=p/n\). Let \(k n\) be the subsample size and denote by \(_{n}=p/k\). Suppose \(\) satisfies certain regularity conditions. For any \(M\{\}\), let \(_{n}\) be the value that satisfies_

\[_{=1}^{M}[((_{I_{}}^{})(_{I_{}} ^{})^{})^{+}]=[ ((^{})(^{})^{ }+_{n}_{n})^{-1}].\]

Figure 3: Comparison of various linear functionals (Theorem 3) and generalized risk functionals (Theorem 1) on different paths evaluated on CIFAR-10. The aspect ratios \(=p/n\) and \(=4\) are estimated from the dataset and fixed. For different values of \(_{}\) (different colors), we estimate \(\) from Proposition 4, which gives a path between \((_{},)\) and \((,)\). For each path, we uniformly sample 5 points and compute the functionals of the ridge ensemble using \(M=100\). The values of different paths are then normalized by subtracting the mean value in the first path and dividing by the mean difference of values on the last and first paths.

_Define the data-dependent path \(_{n}=(_{n};_{n},_{n})\). Then, the conclusions in Theorem 1, Proposition 2, and Theorem 3 continue to hold on \(((^{}),)\) with \(\) replaced by \(_{n}\)._

**Kernel features.** We can also consider kernel ridge regression. For a given feature map \(:^{p}^{d}\), the kernel ridge estimator (in the primal form) is defined as:

\[}_{k}^{}(_{I})=*{ argmin}_{^{p}}_{i I}(k^{-1/2}y_{i}-k^{-1/2} (_{i})^{})^{2}+\|\|_{2}^{2}_{i I}(y_{i}-(_{i})^{} {})^{2}+\|\|_{2}^{2}.\]

Leveraging the kernel trick, the preceding optimization problem translates to solving the following problem (in the dual domain): \(}_{k}^{}(_{I})=*{ argmin}_{^{k}}^{} (_{I}+k_{k}) +^{}_{I},\) where \(_{I}=_{I}_{I}^{}^{k k}\) is the kernel matrix and \(_{I}=((_{i}))_{i I}^{n d}\) is the feature matrix. The correspondence between the dual and primal solutions is simply given by: \(}_{k}^{}(_{I})=_{I}^{}}_{k}^{}(_{I})\).

Figure F8 illustrate results for kernel ridge regression using the same data-generating process as in the previous subsection. The figure shows the prediction risk of kernel ridge ensembles for polynomial, Gaussian, and Laplace kernels exhibit similar equivalence patterns, leading us to formulate an analogous conjecture for kernel ridge regression (which when \(()=\) gives back Proposition 4 with appropriate rescaling of features):

**Conjecture 8** (Data-dependent equivalences for kernel features (informal)).: _Suppose Assumptions 1-2 hold. Define \(_{n}=p/n\). Suppose the kernel \(K\) satisfies certain regularity conditions. Let \(k n\) be the subsample size and denote by \(_{n}=p/k\). For any \(M\{\}\), let \(_{n}\) be a solution to_

\[_{=1}^{M}[_{I_{}}^ {+}]=[(_{[n]}+ _{n}_{n})^{-1}].\]

_Define the data-dependent path \(_{n}=(_{n};_{n},_{n})\). Then, the conclusions in Theorem 1, Proposition 2, and Theorem 3 continue to hold for kernel ridge ensembles with \(\) replaced by \(_{n}\)._

The empirical evidence here strongly suggests that the relationship between subsampling and ridge regression, which we have proved for ridge regression, holds true at least when the gram matrix "linearizes" in the sense of asymptotic equivalence . This intriguing observation opens up a whole host of avenues towards fully understanding the universality of this relationship and establishing precise connections for a broader range of models . We hope to share more on these compelling directions in the near future.

**Tying other implicit regularizations.** Finally, as noted in related work, the equivalences between ridge regularization and subsampling established in this paper naturally offer opportunities to interpret and understand the other implicit regularization effects such as dropout regularization , early stopping in gradient descent variants . Furthermore, these equivalences provide avenues to understand the combined effects of these forms of implicit regularization, such as the effects of both subsample aggregating and gradient descent in mini-batch gradient descent, and contrast them to explicit regularization. Whether the explicit regularization can always be cleanly expressed as ridge-like regularization or takes a more generic form is an intriguing question. Exploring this is exciting, and we hope our readers also share this excitement!

Figure 4: Heat map of prediction risks of full-ensemble ridge estimators (approximated with \(M=100\)) using random features, for varying ridge penalties \(\), subsample aspect ratios \(=p/d\) on the log-log scale. We consider random features \((_{i})^{d}\), where \(\) is an activation function (sigmoid, ReLU, or tanh). The data model is described in Appendix F.6 with \(p=250\), \(d=500\), \(n=5000\), and \(=d/n=0.1\). As in Theorem 1, we see clear risk equivalence paths across activations.