# Online Clustering of Bandits with Misspecified User Models

Zhiyong Wang

The Chinese University of Hong Kong

zywang21@cse.cuhk.edu.hk

&Jize Xie

Shanghai Jiao Tong University

xjzzjl@sjtu.edu.cn

&Xutong Liu

The Chinese University of Hong Kong

liuxt@cse.cuhk.edu.hk

&Shuai Li

Shanghai Jiao Tong University

shuaili8@sjtu.edu.cn

&John C.S. Lui

The Chinese University of Hong Kong

cslui@cse.cuhk.edu.hk

Corresponding author.

###### Abstract

The contextual linear bandit is an important online learning problem where given arm features, a learning agent selects an arm at each round to maximize the cumulative rewards in the long run. A line of works, called the clustering of bandits (CB), utilize the collaborative effect over user preferences and have shown significant improvements over classic linear bandit algorithms. However, existing CB algorithms require well-specified linear user models and can fail when this critical assumption does not hold. Whether robust CB algorithms can be designed for more practical scenarios with misspecified user models remains an open problem. In this paper, we are the first to present the important problem of clustering of bandits with misspecified user models (CBMUM), where the expected rewards in user models can be perturbed away from perfect linear models. We devise two robust CB algorithms, RCLUMB and RSCLUMB (representing the learned clustering structure with dynamic graph and sets, respectively), that can accommodate the inaccurate user preference estimations and erroneous clustering caused by model misspecifications. We prove regret upper bounds of \(O(_{*}T+d T)\) for our algorithms under milder assumptions than previous CB works (notably, we move past a restrictive technical assumption on the distribution of the arms), which match the lower bound asymptotically in \(T\) up to logarithmic factors, and also match the state-of-the-art results in several degenerate cases. The techniques in proving the regret caused by misclustering users are quite general and may be of independent interest. Experiments on both synthetic and real-world data show our outperformance over previous algorithms.

## 1 Introduction

Stochastic multi-armed bandit (MAB)  is an online sequential decision-making problem, where the learning agent selects an action and receives a corresponding reward at each round, so as to maximize the cumulative reward in the long run. MAB algorithms have been widely applied inrecommendation systems and computer networks to handle the exploration and exploitation trade-off [20; 30; 38; 5].

To deal with large-scale applications, the contextual linear bandits [24; 9; 1; 29; 21] have been studied, where the expected reward of each arm is assumed to be perfectly linear in their features. Leveraging the contextual side information about the user and arms, linear bandits can provide more personalized recommendations . Classical linear bandit approaches, however, ignore the often useful tool of collaborative filtering. To utilize the relationships among users, the problem of clustering of bandits (CB) has been proposed . Specifically, CB algorithms adaptively partition users into clusters and utilize the collaborative effect of users to enhance learning performance.

Although existing CB algorithms have shown great success in improving recommendation qualities, there exist two major limitations. First, all previous works on CB [12; 25; 27; 39] assume that for each user, the expected rewards follow a _perfectly linear_ model with respect to the user preference vector and arms' feature vectors. In many real-world scenarios, due to feature noises or uncertainty , the reward may not necessarily conform to a perfectly linear function, or even deviates a lot from linearity . Second, previous CB works assume that for users within the same cluster, their preferences are exactly the same. Due to the heterogeneity in users' personalities and interests, similar users may not have identical preferences, invalidating this strong assumption.

To address these issues, we propose a novel problem of clustering of bandits with misspecified user models (CBMUM). In CBMUM, the expected reward model of each user does not follow a perfectly linear function but with possible additive deviations. We assume users in the same underlying cluster share a common preference vector, meaning they have the same linear part in reward models, but the deviation parts are allowed to be different, better reflecting the varieties of user personalities.

The relaxation of perfect linearity and the reward homogeneity within the same cluster bring many challenges to the CBMUM problem. In CBMUM, we not only need to handle the uncertainty from the _unknown_ user preference vectors, but also have to tackle the additional uncertainty from model misspecifications. Due to such uncertainties, it becomes highly challenging to design a robust algorithm that can cluster the users appropriately and utilize the clustered information judiciously. On the one hand, the algorithm needs to be more tolerant in the face of misspecifications so that more similar users can be clustered together to utilize the collaborative effect. On the other hand, it has to be more selective to rule out the possibility of _misculustering_ users with large preference gaps.

### Our Contributions

This paper makes the following four contributions.

**New Model Formulation.** We are the first to formulate the clustering of bandits with misspecified user models (CBMUM) problem, which is more practical by removing the perfect linearity assumption in previous CB works.

**Novel Algorithm Designs.** We design two novel algorithms, RCLUMB and RSCLUMB, which robustly learn the clustering structure and utilize this collaborative information for faster user preference elicitation. Specifically, RCLUMB keeps updating a dynamic graph over all users, where users connected directly by edges are supposed to be in the same cluster. RCLUMB adaptively removes edges and recommends items based on historical interactions. RSCLUMB represents the clustering structure with sets, which are dynamicly merged and split during the learning process. Due to the page limit, we only illustrate the RCLUMB algorithm in the main paper. We leave the exposition, illustration, and regret analysis of the RSCLUMB algorithm in Appendix K.

To overcome the challenges brought by model misspecifications, we do the following key steps in the RCLUMB algorithm. (i) To ensure that with high probability, similar users will not be partitioned apart, we design a more tolerant edge deletion rule by taking model misspecifications into consideration. (ii) Due to inaccurate user preference estimations caused by model misspecifications, trivially following previous CB works [12; 25; 28] to directly use connected components in the maintained graph as clusters would _miscluster_ users with big preference gaps, causing a large regret. To be discriminative in cluster assignments, we filter users directly linked with the current user in the graph to form the cluster used in this round. With these careful designs of (i) and (ii), we can guarantee that with high probability, information of all similar users can be leveraged, and only users with close enough preferences might be _misclustered_, which will only mildly impair the learning accuracy. Additionally: (iii) we design an enlarged confidence radius to incorporate both the exploration bonus and the additional uncertainty from misspecifications when recommending arms. The design of RSCLUMB follows similar ideas, which we leave in the Appendix K due to page limit.

**Theoretical Analysis with Milder Assumptions**. We prove regret upper bounds for our algorithms of \(O(_{*}T+d T)\) in CBMUM under much milder and practical assumptions (in arm generation distribution) than previous CB works, which match the state-of-the-art results in degenerate cases. Our proof is quite different from the typical proof flow of previous CB works (details in Appendix C). One key challenge is to bound the regret caused by _mischustering_ users with close but not the same preference vectors and use the inaccurate cluster-based information to recommend arms. To handle the challenge, we prove a key lemma (Lemma 5.7) to bound this part of regret. We defer its details in Section 5 and Appendix G. The techniques and results for bounding this part are quite general and may be of independent interest. We also give a regret lower bound of \((_{*}T)\) for CBMUM, showing that our upper bounds are asymptotically tight with respect to \(T\) up to logarithmic factors. We leave proving a tighter lower bound for CBMUM as an open problem.

**Good Experimental Performance.** Extensive experiments on both synthetic and real-world data show the advantages of our proposed algorithms over the existing algorithms.

## 2 Related Work

Our work is closely related to two lines of research: online clustering of bandits (CB) and misspecified linear bandits (MLB). More discussions on related works can be found in Appendix A.

The paper  first formulates the CB problem and proposes a graph-based algorithm. The work  further considers leveraging the collaborative effects on items to guide the clustering of users. The work  considers the CB problem in the cascading bandits setting with random prefix feedback. The paper  also considers users with different arrival frequencies. A recent work  proposes the setting of clustering of federated bandits, considering both privacy protection and communication requirements. However, all these works assume that the reward model for each user follows a perfectly linear model, which is unrealistic in many real-world applications. To the best of our knowledge, this paper is the first work to consider user model misspecifications in the CB problem.

The work  first proposes the misspecified linear bandits (MLB) problem, shows the vulnerability of linear bandit algorithms under deviations, and designs an algorithm RLB that is only robust to non-sparse deviations. The work  proposes two algorithms to handle general deviations, which are modifications of the phased elimination algorithm  and LinUCB . Some recent works [31; 11] use model selection methods to deal with unknown exact maximum model misspecification level. Note that the work  has an additional assumption on the access to an online regression oracle, and the paper  still needs to know an upper bound of the unknown exact maximum model deviation level. None of them consider the CB setting with multiple users, thus differing from ours.

We are the first to initialize the study of the important CBMUM problem, and propose a general framework for dealing with model misspecifications in CB problems. Our study is based on fundamental models on CB [12; 27] and MLB , the algorithm design ideas and theoretical analysis are pretty general. We leave incorporating the model selection methods [31; 11] into our framework to address the unknown exact maximum model misspecification level as an interesting future work.

## 3 Problem Setup

This section formulates the problem of "clustering of bandits with misspecified user models" (CB-MUM). We use boldface **lowercase** and boldface **CAPITALIZED** letters for vectors and matrices. We use \(||\) to denote the number of elements in \(\), \([m]\) to denote \(\{1,,m\}\), and \(\|\|_{}=^{}}\) to denote the matrix norm of vector \(\) regarding the positive semi-definite (PSD) matrix \(\).

In CBMUM, there are \(u\) users denoted by \(=\{1,2,,u\}\). Each user \(i\) is associated with an _unknown_ preference vector \(_{i}^{d}\), with \(\|_{i}\|_{2} 1\). We assume there is an _unknown_ underlying clustering structure over users representing the similarity of their behaviors. Specifically, \(\) can be partitioned into a small number \(m\) (i.e., \(m u\)) clusters, \(V_{1},V_{2}, V_{m}\), where \(_{j[m]}V_{j}=\), and \(V_{j} V_{j^{}}=\), for \(j j^{}\). We call these clusters _ground-truth clusters_ and use \(=\{V_{1},V_{2},,V_{m}\}\) to denote the set of these clusters. Users in the same _ground-truth cluster_ share the same preference vector, while users from different _ground-truth clusters_ have different preference vectors. Letdenote the common preference vector for \(V_{j}\) and \(j(i)[m]\) denote the index of the _ground-truth cluster_ that user \(i\) belongs to. For any \(\), if \( V_{j(i)}\), then \(_{}=_{i}=^{j(i)}\).

At each round \(t[T]\), a user \(i_{t}\) comes to be served. The learning agent receives a finite arm set \(_{t}\) to choose from (with \(|_{t}| C, t\)), where each arm \(a\) is associated with a feature vector \(_{a}^{d}\), and \(_{a}_{2} 1\). The agent assigns an appropriate cluster \(_{t}\) for user \(i_{t}\) and recommends an item \(a_{t}_{t}\) based on the aggregated historical information gathered from cluster \(_{t}\). After receiving the recommended item \(a_{t}\), user \(i_{t}\) gives a random reward \(r_{t}\) to the agent. To better model real-world scenarios, we assume that the reward \(r_{t}\) follows a misspecified linear function of the item feature vector \(_{a_{t}}\) and the _unknown_ user preference vector \(_{i_{t}}\). Formally,

\[r_{t}=_{a_{t}}^{}_{i_{t}}+_{a_{t}}^{i_{t},t} +_{t}\,, \]

where \(^{i_{t},t}=[_{1}^{i_{t},t},_{2}^{i_{t},t},,_{|_{t}|}^{i_{t},t}]^{}^{| _{t}|}\) denotes the _unknown_ deviation in the expected rewards of arms in \(_{t}\) from linearity for user \(i_{t}\) at \(t\), and \(_{t}\) is the 1-sub-Gaussian noise. We allow the deviation vectors for users in the same _ground-truth cluster_ to be different.

We assume the clusters, users, items, and model misspecifications satisfy the following assumptions.

**Assumption 3.1** (Gap between different clusters).: The gap between any two preference vectors for different _ground-truth clusters_ is at least an _unknown_ positive constant \(\)

\[^{j}-^{j^{}}_{2}>0 \,, j,j^{}[m]\,,j j^{}\,.\]

**Assumption 3.2** (Uniform arrival of users).: At each round \(t\), a user \(i_{t}\) comes uniformly at random from \(\) with probability \(1/u\), independent of the past rounds.

**Assumption 3.3** (Item regularity).: At each time step \(t\), the feature vector \(_{a}\) of each arm \(a_{t}\) is drawn independently from a fixed but unknown distribution \(\) over \(\{^{d}:_{2} 1\}\), where \(_{}[^{}]\) is full rank with minimal eigenvalue \(_{x}>0\). Additionally, at any time \(t\), for any fixed unit vector \(^{d}\), \((^{})^{2}\) has sub-Gaussian tail with variance upper bounded by \(^{2}\).

**Assumption 3.4** (Bounded misspecification level).: We assume that there is a pre-specified maximum misspecification level parameter \(_{*}\) such that \(^{i,t}_{}_{*}\), \( i,t[T]\).

**Remark 1**.: All these assumptions basically follow previous works on CB  and MLB . Note that Assumption 3.3 is less stringent and more practical than previous CB works which also put restrictions on the variance upper bound \(^{2}\). For Assumption 3.2, our results can easily generalize to the case where the user arrival follows any distributions with minimum arrival probability greater than \(p_{min}\). For Assumption 3.4, note that \(_{*}\) can be an upper bound on the maximum misspecification level, not the exact maximum itself. In real-world applications, the deviations are usually small , and we can set a relatively big \(_{*}\) as an upper bound. For more discussions please refer to Appendix B at \(t\). The goal of the agent is to minimize the expected cumulative regret

\[R(T)=[_{t=1}^{T}(_{a_{t}^{}}_{i_{t}}+_{a_{t}^{i_{t},t}}^{i_{t},t}-_{a_{t}}^{}_{i_{t}}- _{a_{t}}^{i_{t},t})]\,. \]

## 4 Algorithm

This section introduces our algorithm called "Robust CLUstering of Misspecified Bandits" (RCLUMB) (Algo.1). RCLUMB is a graph-based algorithm. The ideas and techniques of RCLUMB can be easily generalized to set-based algorithms. To illustrate this generalizability, we also design a set-based algorithm RSCLUMB. We leave the exposition and analysis of RSCLUMB in Appendix K.

For ease of interpretation, we define the coefficient

\[ 2_{*}_{x}}}\,, \]

where \(_{x}_{0}^{_{x}}(1-e^{-- )^{2}}{2^{2}}})^{C}dx\). \(\) is theoretically the minimum gap between two users' preference vectors that an algorithm can distinguish with high probability, as supported by Eq.(50) in the proof of Lemma H.1 in Appendix H. Note that the algorithm does not require knowledge of \(\). We also make the following definition for illustration.

**Definition 4.1** (\(\)-close users and \(\)-good clusters).: Two users \(i,i^{}\) are \(\)-close if \(\|_{i}-_{i^{}}\|_{2}\). Cluster \(\) is a \(\)-good cluster at time \(t\), if \(\,i\), user \(i\) and the coming user \(i_{t}\) are \(\)-close.

We also say that two _ground-truth clusters_ are "\(\)-close" if their preference vectors' gap is less than \(\).

Now we introduce the process and intuitions of RCLUMB (Algo.1). The algorithm maintains an undirected user graph \(G_{t}=(,E_{t})\), where users are connected with edges if they are inferred to be in the same cluster. We denote the connected component in \(G_{t-1}\) containing user \(i_{t}\) at round \(t\) as \(_{t}\).

**Cluster Detection.**\(G_{0}\) is initialized to be a complete graph, and will be updated adaptively based on the interactive information. At round \(t\), user \(i_{t}\) comes to be served with a feasible arm set \(_{t}\) (Line 4). Due to model misspecifications, it is impossible to cluster users with exactly the same preference vector \(\), but similar users whose preference vectors are within the distance of \(\). According to the proof of Lemma H.1, after a sufficient time, with high probability, any pair of users directly connected by an edge in \(E_{t-1}\) are \(\)-close. However, if we trivially follow previous CB works  to directly use the connected component \(_{t}\) as the inferred cluster for user \(i_{t}\) at round \(t\), it will cause a large regret. The reason is that in the worst case, the preference vector \(\) of the user in \(_{t}\) who is \(h\)-hop away from user \(i_{t}\) could deviate by \(h\) from \(_{i_{t}}\), where \(h\) can be as large as \(|_{t}|\). Based on this reasoning, our key point is to select the cluster \(_{t}\) as the users at most 1-hop away from \(i_{t}\) in the graph. In other words, after some interactions, \(_{t}\) forms a \(\)-good cluster with high probability; thus, RCLUMB can avoid using misleading information from dissimilar users for recommendations.

**Cluster-based Recommendation.** After finding the appropriate cluster \(_{t}\) for \(i_{t}\), the agent estimates the common user preference vector based on the historical information associated with cluster \(_{t}\) by

\[}_{_{t},t-1}=*{arg\,min}_{ ^{d}}_{|=1}}{{i_{}_{t}}}}(r_{s}-_{a}^{})^{2}+\|\|_{2}^{ 2}\,, \]

where \(>0\) is a regularization coefficient. Its closed-form solution is \(}_{_{t},t-1}=_{_{ t},t-1}^{-1}_{_{t},t-1}\), where \(_{_{t},t-1}=+_{|=1}}{{i_{}_{t}}}} {a}_{a},_{a}^{}\), \(_{_{t},t-1}=_{|=1}}{{i_{}_{t}}}}r_{a_{s}} {x}_{a}\).

Based on this estimation, in Line 7, the agent recommends an arm using the UCB strategy

\[a_{t}=*{argmax}_{a_{t}}\{1,_{a}^{}}_{_{t},t-1}}_{ _{a,t}}+_{a}\|_{ {V}_{t},t-1}^{-1}+_{*}_{|=1}}{{i_{}_{t}}}}}_{_{t},t-1}| _{a}^{}_{_{t},t-1}^{-1} {x}_{a_{s}}|\}}_{C_{a,t}}\}\,, \]where \(=+)+d(1+)}\), \(_{a,t}\) denotes the estimated reward of arm \(a\) at \(t\), \(C_{a,t}\) denotes the confidence radius of arm \(a\) at round \(t\).

Due to deviations from linearity, the estimation \(_{a,t}\) computed by a linear function is no longer accurate. To handle the estimation uncertainty of model misspecifications, we design an enlarged confidence radius \(C_{a,t}\). The first term of \(C_{a,t}\) in Eq.(5) captures the uncertainty of online learning for the linear part, and the second term related to \(_{a}\) reflects the additional uncertainty from deviations from linearity. The design of \(C_{a,t}\) theoretically relies on Lemma 5.6 which will be given in Section 5.

**Update User Statistics.** Based the feedback \(r_{t}\), in Line 8 and 9, the agent updates the statistics for user \(i_{t}\). Specifically, the agent estimates the preference vector \(_{i_{t}}\) by

\[}_{i_{t},t}=*{arg\,min}_{ ^{d}}_{=i_{t}}}(r_{s}-_{a_{s}}^{})^{2 }+\|\|_{2}^{2}\,, \]

with solution \(}_{i_{t},t}=(+_{i_ {t},t})^{-1}_{i_{t},t}\,,\) where \(M_{i_{t},t}=_{=i_{t}}} _{a_{s}}_{a_{s}}^{},_{i_{t},t}=_{ {}{0.0pt}{}{z[t]}{i_{s}=i_{t}}}r_{a_{s}}_{a_{s}}\,.\)

**Update the Graph \(G_{t}\).** Finally, in Line 10, the agent verifies whether the similarities between user \(i_{t}\) and other users are still true based on the updated estimation \(}_{i_{t},t}\). For every user \(\) connected with user \(i_{t}\) via edge \((i_{t},) E_{t-1}\), if the gap between her estimated preference vector \(}_{,t}\) and \(}_{i_{t},t}\) is larger than a threshold supported by Lemma H.1, the agent will delete the edge \((i_{t},)\) to split them apart. The threshold in Line 10 is carefully designed, taking both estimation uncertainty in a linear model and deviations from linearity into consideration. As shown in the proof of Lemma H.1 (in Appendix H), using this threshold, with high probability, edges between users in the same _ground-truth clusters_ will not be deleted, and edges between users that are not \(\)-close will always be deleted. Together with the filtering step in Line 5, with high probability, the algorithm will leverage all the collaborative information of similar users and avoid misusing the information of dissimilar users. The updated graph \(G_{t}\) will be used in the next round.

## 5 Theoretical Analysis

In this section, we theoretically analyze the performance of the RCLUMB algorithm by giving an upper bound of the expected regret defined in Eq.(2). Due to the space limitation, we only show the main result (Theorem 5.3), key lemmas, and a sketched proof for Theorem 5.3. Detailed proofs, other technical lemmas, and the regret analysis of the RSLUMB algorithm can be found in the Appendix.

To state our main result, we first give two definitions as follows. The first definition is about the minimum separable gap constant \(_{1}\) of a CBMUM problem instance.

**Definition 5.1** (Minimum separable gap \(_{1}\)).: The minimum separable gap constant \(_{1}\) of a CBMUM problem instance is the minimum gap over the gaps among users that are greater than \(\) (Eq. (3))

\[_{1}=\{\|_{i}-_{} \|_{2}:\|_{i}-_{}\| _{2}>, i,\}\,,=.\]

**Remark 2**.: In CBMUM, the role of \(_{1}-\) is similar to that of \(\) (given in Assumption 3.1) in the previous CB problem with perfectly linear models, quantifying the hardness of performing clustering on the problem instance. Intuitively, users are easier to cluster if \(_{1}\) is larger, and the deduction of \(\) shows the additional difficulty due to model divisions. If there are no misspecifications, i.e., \(=2_{a}}}=0\), then \(_{1}=\), recovering the minimum separable gap between clusters in the classic CB problem [12; 25] without model misspecifications.

The second definition is about the number of "hard-to-cluster users" \(\).

**Definition 5.2** (Number of "hard-to-cluster users" \(\)).: The number of "hard-to-cluster users" \(\) is the number of users in the _ground-truth clusters_ which are \(\)-close to some other _ground-truth clusters_

\[=_{j[m]}|V_{j}|\{ j^{}[m],j^{ } j:\|^{j^{}}-^{j} \|_{2}\}\,,\]

where \(\{\}\) denotes the indicator function of the argument, \(|V_{j}|\) denotes the number of users in \(V_{j}\).

**Remark 3**.: \(\) captures the number of users who belong to different _ground-truth clusters_ but their gaps are less than \(\). These users may be merged into one cluster by mistake and cause certain regret.

The following theorem gives an upper bound on the expected regret achieved by RCLUMB.

**Theorem 5.3** (Main result on regret bound).: _Suppose that the assumptions in Section 3 are satisfied. Then the expected regret of the RCLUMB algorithm for \(T\) rounds satisfies_

\[R(T)  Ou((_{1}-)^{2}}+ _{2}}^{2}}) T+}{u} T}{_{x}^{1.5}}+_{*}T+d T \] \[ O(_{*}T+d T)\,, \]

_where \(_{1}\) is defined in Definition 5.1, and \(\) is defined in Definition 5.2)._

**Discussion and Comparison.** The bound in Eq.(7) has four terms. The first term is the time needed to gather enough information to assign appropriate clusters for users. The second term is the regret caused by _mischustering_\(\)-close but not precisely similar users together, which is unavoidable with model misspecifications. The third term is from the preference estimation errors caused by model deviations. The last term is the usual term in CB with perfectly linear models .

Let us discuss how the parameters affect this regret bound.

\(\) If \(_{1}-\) is large, the gaps between clusters that are not "\(\)-close" are much greater than the minimum gap \(\) for the algorithm to distinguish, the first term in Eq.(7) will be small as it is easy to identify their dissimilarities. The role of \(_{1}-\) in CBMUM is similar to that of \(\) in the previous CB.

\(\) If \(\) is small, indicating that few _ground-truth clusters_ are "\(\)-close", RCLUMB will hardly _mischuster_ different _ground-truth clusters_ together thus the second term in Eq.(7) will be small.

\(\) If the deviation level \(_{*}\) is small, the user models are close to linearity and the misspecifications will not affect the estimations much, then both the second and third term in Eq.(7) will be small.

The following theorem gives a regret lower bound of the CBMUM problem.

**Theorem 5.4** (Regret lower bound for CBMUM).: _There exists a problem instance for the CBMUM problem such that for any algorithm \(R(T)(_{*}T)\,.\)_

The proof can be found in Appendix F. The upper bounds in Theorem 5.3 asymptotically match this lower bound with respect to \(T\) up to logarithmic factors (and a constant factor of \(\) where \(m\) is typically small in real-applications), showing the tightness of our theoretical results. Additionally, we conjecture the gap for the \(m\) factor is due to the strong assumption that cluster structures are known to prove this lower bound, and whether there exists a tighter lower bound is left for future work.

We then compare our results with two degenerate cases. First, when \(m=1\) (indicating \(=0\)), our setting degenerates to the MLB problem where all users share the same preference vector. In this case, our regret bound is \(O(_{*}T+d T)\), exactly matching the current best bound of MLB . Second, when \(_{*}=0\), our setting reduces to the CB problem with perfectly linear user models and our bounds become \(O(d T)\), also perfectly match the existing best bound of the CB problem . The above discussions and comparisons show the tightness of our regret bounds. Additionally, we also provide detailed discussions on why trivially combining existing works on CB and MLB would not get any non-vacuous regret upper bound in Appendix D.

We define the following "good partition" for ease of interpretation.

**Definition 5.5** (Good partition).: RCLUMB does a "good partition" at \(t\), if the cluster \(_{t}\) assigned to \(i_{t}\) is a \(\)-good cluster, and it contains all the users in the same _ground-truth cluster_ as \(i_{t}\), i.e.,

\[\|_{i_{t}}-_{}\|_{2} ,_{t}\,,\,V_{j(i_{t})} _{t}\,. \]

Note that when the algorithm does a "good partition" at \(t\), \(_{t}\) will contain all the users in the same _ground-truth cluster_ as \(i_{t}\) and may only contain some other \(\)-close users with respect to \(i_{t}\), which means the gathered information associated with \(_{t}\) can be used to infer user \(i_{t}\)'s preference with high accuracy. Also, it is obvious that under a "good partition", if \(_{t}\), then \(_{t}=V_{j(i_{t})}\) by definition.

Next, we give a sketched proof for Theorem 5.3.

Proof.: **[Sketch for Theorem 5.3]** The proof mainly contains two parts. First, we prove there is a sufficient time \(T_{0}\) for RCLUMB to get a "good partition" with high probability. Second, we prove the regret upper bound for RCLUMB after maintaining a "good partition". The most challenging part is to bound the regret caused by _mischustering_\(\)-close users after getting a "good partition".

**1. Sufficient time to maintain a "good partition".** With the item regularity (Assumption 3.3), we can prove after some \(T_{0}\) (defined in Lemma H.1 in Appendix H), RCLUMB will always have a"good partition". Specifically, after \(t Ou((_{1}-)^{2}}+^{2}}) T\), for any user \(i\), the gap between the estimated \(}_{i,t}\) and the ground-truth \(^{j(i)}\) is less than \(}{}\) with high probability. With this, we can get: for any two users \(i\) and \(\), if their gap is greater than \(\), it will trigger the deletion of the edge \((i,)\) (Line 10 of Algo.1) with high probability; on the other hand, when the deletion condition of the edge \((i,)\) is satisfied, then \(\|^{j(i)}-^{j()}\|_{2}>0\), which means user \(i\) and \(\) belong to different _ground-truth clusters_ by Assumption 3.1 with high probability. Therefore, we can get that with high probability, all those users in the same _ground-truth cluster_ as \(i_{t}\) will be directly connected with \(i_{t}\), and users directly connected with \(i_{t}\) must be \(\)-close to \(i_{t}\). By filtering users directly linked with \(i_{t}\) as the cluster \(_{t}\) (Algo.1 Line 5) and the definition of "good partition", we can ensure that RCLUMB will keep a "good partition" afterward with high probability.

**2. Bounding the regret after getting a "good partition".** After \(T_{0}\), with the "good partition", we can prove the following lemma that gives a bound of the difference between \(}_{_{t},t-1}\) and ground-truth \(_{i_{t}}\) in direction of action vector \(_{a}\), and supports the design of the confidence radius \(C_{a,t}\) in Eq.(5).

**Lemma 5.6**.: _With probability at least \(1-5\) for some \((0,)\), \( t T_{0}\)_

\[|_{a}^{}(_{i_{t}}-}_{ _{t},t-1})|}{_{ x}^{}}1\{_{t}\}+_{*}_{ s[t-1]\\ i_{s}_{t}}|_{a}^{}_{ _{t},t-1}^{-1}_{a_{s}}|+\|_{a}\| _{_{_{t},t-1}^{-1}}.\]

To prove this lemma, we consider the following two situations.

**(i) Assigning a perfect cluster for \(i_{t}\).** In this case, \(_{t}\), meaning the cluster assigned for user \(i_{t}\) is the same as her _ground-truth cluster_, i.e., \(_{t}=V_{j(i_{t})}\). Therefore, we have that \(_{t},_{}=_{i_{t}}\). With careful analysis, we can bound \(|_{a}^{}(_{i_{t}}-}_{_{t },t-1})|\) by \(C_{a,t}\) (defined in Eq.(5)).

**(ii) Bounding the term of _misclustering_\(i_{t}\)'s \(\)-close users.** In this case, \(_{t}\), meaning the algorithm _misclusters_ user \(i_{t}\), i.e., \(_{t} V_{j(i_{t})}\). Thus, we do not have \(_{t},_{}=_{i_{t}}\) anymore, but we have all the users in \(_{t}\) are \(\)-close to \(i_{t}\) (by "good partition"), i.e., \(\|_{i_{t}}-_{i_{t}}\|_{2}, _{t}\). Then an additional term can be caused by using the information of \(i_{t}\)'s \(\)-close users in \(_{t}\) lying in different _ground-truth clusters_ from \(i_{t}\) to estimate \(_{i_{t}}\). It is highly challenging to bound this part.

We will get an extra term \(|_{a}^{}_{_{t},t-1}^{-1}_{ s[t-1]\\ i_{s}_{t}}_{a_{s}}_{a_{s}}^{}( _{i_{s}}-_{i_{t}})|\) when bounding the regret in this case, where \(\|_{}-_{i_{t}}\|_{2}, _{t}\). It is an easy-to-be-made mistake to directly drag \(\|_{i_{t}}-_{i_{t}}\|_{2}\) out to bound it by \(\|_{a}^{}_{_{t},t-1}^{-1}_{ s[t-1]\\ i_{s}_{t}}_{a_{s}}_{a_{s}}^{} \|_{2}\). With subtle analysis, we propose the following lemma to bound the above term.

**Lemma 5.7** (Bound of error caused by _misclustering_).: \( t T_{0}\)_, if the current partition by RCLUMB is a "good partition", and \(_{t}\), then for all \(_{a}^{d},\|_{a}\|_{2} 1\), with probability at least \(1-\):_

\[|_{a}^{}_{_{t},t-1}^{-1}_{ s[t-1]\\ i_{s}_{t}}_{a_{s}}_{a_{s}}^{}( _{i_{s}}-_{i_{t}})| }{_{x}^{}}.\]

This lemma is quite general. Please see Appendix G for details about its proof.

The expected occurrences of \(\{_{t}\}\) is bounded by \(}{u}T\) with Assumption 3.2, Definition 5.2 and 5.5. The result follows by bounding the expected sum of the bounds for the instantaneous regret using Lemma 5.6 with delicate analysis due to the time-varying clustering structure kept by RCLUMB. 

## 6 Experiments

This section compares RCLUMB and RSCLUMB with CLUB , SCLUB , LinUCB with a single estimated vector for all users, LinUCB-Ind with separate estimated vectors for each user, and two modifications of LinUCB in  which we name as RLinUCB and RLinUCB-Ind. We use averaged reward as the evaluation metric, where the average is taken over ten independent trials.

### Synthetic Experiments

We consider a setting with \(u=1,000\) users, \(m=10\) clusters and \(T=10^{6}\) rounds. The preference and feature vectors are in \(d=50\) dimension with each entry drawn from a standard Gaussian distribution, and are normalized to vectors with \(\|.\|_{2}=1\). We fix an arm set with \(||=1000\) items, at each round \(t\), 20 items are randomly selected to form a set \(_{t}\) for the user to choose from. We construct a matrix \(^{1,000 1,000}\) in which each element \((i,j)\) is drawn uniformly from the range \((-0.2,0.2)\) to represent the deviation. At \(t\), for user \(i_{t}\) and the item \(a_{t}\), \((i_{t},a_{t})\) will be added to the feedback as the deviation, which corresponds to the \(^{i_{t},t}_{a_{t}}\) defined in Eq.(1).

The result is provided in Figure 1(a), showing that our algorithms have clear advantages: RCLUMB improves over CLUB by 21.9%, LinUCB by 194.8%, LinUCB-Ind by 20.1%, SCLUB by 12.0%, RLinUCB by 185.2% and RLinUCB-Ind by 10.6%. The performance difference between RCLUMB and RSCLUMB is very small as expected. RLinUCB performs better than LinUCB; RLinUCB-Ind performs better than LinUCB-Ind and CLUB, showing that the modification of the recommendation policy is effective. The set-based RSCLUMB and SCLUB can separate clusters quicker and have advantages in the early period, but eventually RCLUMB catches up with RSCLUMB, and SCLUB is surpassed by RLinUCB-Ind because it does not consider misspecifications. RCLUMB and RSCLUMB perform better than RLinUCB-Ind, which shows the advantage of the clustering. So it can be concluded that both the modification for misspecification and the clustering structure are critical to improving the algorithm's performance. We also have done some ablation experiments on different scales of \(^{*}\) in Appendix P, and we can notice that under different \(^{*}\), our algorithms always outperform the baselines, and some baselines will perform worse as \(^{*}\) increases.

### Experiments on Real-world Datasets

We conduct experiments on the Yelp data and the \(20m\) MovieLens data . For both data, we have two cases due to the different methods for generating feedback. For case 1, we extract 1,000 items with most ratings and 1,000 users who rate most; then we construct a binary matrix \(^{1,000 1,000}\) based on the user rating [40; 42]: if the user rating is greater than 3, the feedback is 1; otherwise, the feedback is 0. Then we use this binary matrix to generate the preference and feature vectors by singular-value decomposition (SVD) [27; 25; 40]. Similar to the synthetic experiment, we construct a matrix \(^{1,000 1,000}\) in which each element is drawn uniformly from the range \((-0.2,0.2)\). For case 2, we extract 1,100 users who rate most and 1000 items with most ratings. We construct a binary feedback matrix \(^{1,100 1,000}\) based on the same rule as case 1. Then we select the first 100 rows \(^{100 1,000}_{1}\) to generate the feature vectors by SVD. The remaining 1,000 rows \(^{1,000 1,000}\)

Figure 1: The figures compare RCLUMB and RSCLUMB with the baselines. (a) shows the result on synthetic data, (b) and (c) show the results on Yelp dataset, (d) and (e) show the results on Movielens dataset. All experiments are under the setting of \(u=1,000\) users, \(m=10\) clusters, and \(d=50\). All results are averaged under \(10\) random trials. The error bars are standard deviations divided by \(\).

is used as the feedback matrix, meaning user \(i\) receives \((i,j)\) as feedback while choosing item \(j\). In both cases, at time \(t\), we randomly select \(20\) items for the algorithms to choose from. In case 1, the feedback is computed by the preference and feature vector with misspecification, in case 2, the feedback is from the feedback matrix.

The results on Yelp are shown in Fig 1(b) and Fig 1(c). In case 1, RCLUMB improves CLUB by 45.1%, SCLUB by 53.4%, LinUCB-One by 170.1%, LinUCB-Ind by 46.2%, RLinUCB by 171.0% and RLinUCB-Ind by 21.5%. In case 2, RCLUMB improves over CLUB by 13.9%, SCLUB by 5.1%, LinUCB-One by 135.6%, LinUCB-Ind by 10.1%, RLinUCB by 138.6% and RLinUCB by 8.5%. It is notable that our modeling assumption 3.4 is violated in case 2 since the misspecification range is unknown. We set \(_{*}=0.2\) following our synthetic dataset and it can still perform better than other algorithms. When the misspecification level is known as in case 1, our algorithms' improvement is significantly enlarged, e.g., RCLUMB improves over SCLUB from 5.1% to 53.4%.

The results on Movielens are shown in Fig 1(d) and 1(e). In case 1, RCLUMB improves CLUB by 58.8%, SCLUB by 92.1%, LinUCB-One by 107.7%, LinUCB-Ind by 61.5 %, RLinUCB by 109.5%, and RLinUCB-Ind by 21.3%. In case 2, RCLUMB improves over CLUB by 5.5%, SCLUB by 2.9%, LinUCB-One by 28.5%, LinUCB-Ind by 6.1%, RLinUCB by 29.3% and RLinUCB-Ind by 5.8%. The results are consistent with the Yelp data, confirming our superior performance.

## 7 Conclusion

We present a new problem of clustering of bandits with misspecified user models (CBMUM), where the agent has to adaptively assign appropriate clusters for users under model misspecifications. We propose two robust CB algorithms, RCLUMB and RSCLUMB. Under milder assumptions than previous CB works, we prove the regret bounds of our algorithms, which match the lower bound asymptotically in \(T\) up to logarithmic factors, and match the state-of-the-art results in several degenerate cases. It is challenging to bound the regret caused by _mischustering_ users with close but not the same preference vectors and use inaccurate cluster-based information to select arms. Our analysis to bound this part of the regret is quite general and may be of independent interest. Experiments on synthetic and real-world data demonstrate the advantage of our algorithms. We would like to state some interesting future works: (1) Prove a tighter regret lower bound for CBMUM, (2) Incorporate recent model selection methods into our fundamental framework to design robust algorithms for CBMUM with unknown exact maximum model misspecification level, and (3) Consider the setting with misspecifications in the underlying user clustering structure rather than user models.

## 8 Acknowledgement

The corresponding author Shuai Li is supported by National Key Research and Development Program of China (2022ZD0114804) and National Natural Science Foundation of China (62376154, 62006151, 62076161). The work of John C.S. Lui was supported in part by the RGC's GRF 14215722.