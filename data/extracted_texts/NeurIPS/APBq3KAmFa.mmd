# A New Neural Kernel Regime:

The Inductive Bias of Multi-Task Learning

 Julia Nakhleh

Department of Computer Science

University of Wisconsin-Madison

Madison, WI

jnakhleh@wisc.edu

&Joseph Shenouda

Department of Electrical and Computer Engineering

University of Wisconsin-Madison

Madison, WI

jshenouda@wisc.edu

&Robert D. Nowak

Department of Electrical and Computer Engineering

University of Wisconsin-Madison

Madison, WI

rdnowak@wisc.edu

###### Abstract

This paper studies the properties of solutions to multi-task shallow ReLU neural network learning problems, wherein the network is trained to fit a dataset with minimal sum of squared weights. Remarkably, the solutions learned for each individual task resemble those obtained by solving a kernel regression problem, revealing a novel connection between neural networks and kernel methods. It is known that single-task neural network learning problems are equivalent to a minimum norm interpolation problem in a non-Hilbertian Banach space, and that the solutions of such problems are generally non-unique. In contrast, we prove that the solutions to univariate-input, multi-task neural network interpolation problems are almost always unique, and coincide with the solution to a minimum-norm interpolation problem in a Sobolev (Reproducing Kernel) Hilbert Space. We also demonstrate a similar phenomenon in the multivariate-input case; specifically, we show that neural network learning problems with large numbers of tasks are approximately equivalent to an \(^{2}\) (Hilbert space) minimization problem over a fixed kernel determined by the optimal neurons.

## 1 Introduction

This paper characterizes the functions learned by multi-output shallow ReLU neural networks trained with weight decay regularization, wherein each network output fits a different "task" (i.e., a different set of labels on the same data points). We show that the solutions to such multi-task training problems can differ dramatically from those obtained by fitting separate neural networks to each task individually. Unlike standard intuitions Caruana (1997) and existing theory Ben-David and Schuller (2003); Maurer et al. (2016) regarding the effects and benefits of multi-task learning, our results do not rely on similarity between tasks.

We focus on shallow, vector-valued (multi-output) neural networks with Rectified Linear Unit (ReLU) activation functions, which are functions \(f_{}:^{d}^{T}\) of the form

\[f_{}()\;=\;_{k=1}^{K}_{k}_{k}^{} +b_{k}_{+}\;+\;+ \]where \(()_{+}=\{0,\}\) is the ReLU activation function and \(_{k}^{d}\), \(_{k}^{T}\), and \(b_{k}\) are the input and output weights and bias of the \(k^{}\) neuron. \(K\) is the number of neurons and \(T\) denotes the number of tasks (outputs) of the neural network. The affine term \(+\) is the residual connection (or skip connection), where \(^{T d}\) and \(^{T}\). The set of all parameters is denoted by \(:=\{_{k},_{k},b_{k}\}_{k=1}^{K},, \).

Neural networks are trained to fit data using gradient descent methods and often include a form of regularization called _weight decay_, which penalizes the \(^{2}\) norm of the network weights. We consider weight decay applied only to the input and output weights of the neurons--no regularization is applied to the biases or residual connection. This is a common setting studied frequently in past work Savarese et al. (2019); Ongie et al. (2019); Parhi and Nowak (2021). Intuitively, only the input and output weights--not the biases or residual connection--affect the "regularity" of the neural network function as measured by its second (distributional) derivative, which is why it makes sense to regularize only these parameters. Given a set of training data points \((_{1},_{1}),,(_{N},_{N})^{d} ^{T}\) and a fixed width1\(K N^{2}\), we consider the weight decay interpolation problem:

\[_{}_{k=1}^{K}\|_{k}\|_{2}^{2}+\|_{k}\|_{2}^{2} \;,\;\;f_{}(_{i})=_{i},\;i=1,,N\;. \]

By homogeneity of the ReLU activation function (meaning that \(( x)_{+}=(x)_{+}\) for any \( 0\)), the input and output weights of any ReLU neural network can be rescaled as \(_{k}_{k}/\|_{k}\|_{2}\) and \(_{k}_{k}\|_{k}\|_{2}\) without changing the function that the network represents. Using this fact, several previous works Grandvalet (1998); Grandvalet and Canu (1998); Neyshabur et al. (2015); Parhi and Nowak (2023); Shenouda et al. (2024) note that problem (2) is equivalent to

\[_{}_{k=1}^{K}\|_{k}\|_{2},\;\;\{\| _{k}\|_{2}=1\}_{k=1}^{K},\;f_{}(_{i})=_{i},\;i=1, ,N \]

in that the minimal objective values of both training problems are the same, and any network \(f_{}\) which solves (2) also solves (3), while any \(f_{}\) which solves (3) also solves (2) after rescaling of the input and output weights. The regularizer \(_{k=1}^{K}\|_{k}\|_{2}\) is reminiscent of the multi-task lasso Obozinski et al. (2006). It has recently been shown to promote _neuron sharing_ in the network, meaning that only a few neurons contribute to all tasks Shenouda et al. (2024).

The optimizations in (2) and (3) are non-convex and may have multiple global minimizers. As an example, consider the single-task, univariate dataset in Fig. 1. For this dataset, (3) has infinitely many global solutions Savarese et al. (2019); Ergen and Pilanci (2021); Debarre et al. (2022); Hanin (2021). Two of the global minimizers are shown in Fig. 1. In some scenarios, the solution on the right may be preferable to the one on the left, since the interpolation function stays closer to the training data points, and hence is more adversarially robust by most definitions Carlini et al. (2019). Moreover, recent theoretical work shows that this solution has other favorable generalization and robustness properties Joshi et al. (2024). Current training methods, however, might produce any one of the infinite number of solutions, depending on the random initialization of the parameters as well as other possible sources of randomness in the training process. It is impossible to control this using existing training algorithms, which might explain many problems associated with current neural networks such as their sensitivity to adversarial attacks. In contrast, as we show in this paper, training a network to interpolate the data in Fig. 1 along with additional interpolation tasks with different labels almost always produces a unique solution, given by the (potentially preferable) interpolation depicted on the right. This demonstrates that the solutions to multi-task learning problems can be profoundly different than those of single-task learning problems.

The main contributions of our paper are:

**Uniqueness of Multi-task Solutions.** In the univariate setting (\(d=1\)) we prove that the solutions to multi-task learning problems with different tasks almost always represent a unique function, and we give a precise condition for the exceptional cases where solutions are non-unique.

**Multi-task Training \(\) Kernel Method (almost always).** When the solution to the univariate weight decay problem is unique, it is given by the connect-the-dots interpolant of the training data points: i.e., the optimal solution is a linear spline which performs straight-line interpolation between consecutive data points in all tasks. On the support of the data, this solution agrees with the minimum-norm interpolant in the first-order Sobolev space \(H^{1}\), a reproducing kernel Hilbert space (RKHS) which contains all functions with first derivatives in \(L^{2}\) De Boor and Lynch (1966). In contrast, solutions to the single-task learning problem are non-unique in general and are given by minimum-norm interpolating functions in the non-Hilbertian Banach space \(^{2}\) Parhi and Nowak (2021), which contains all functions with second distributional derivatives2 in \(L^{1}\). This shows that the individual task solutions to a multi-task learning problem are almost always equivalent to those of a minimum-norm kernel interpolation problem, whereas single-task solutions generally are not.

**Insights on Multivariate Multi-Task Problems.** We provide empirical evidence and mathematical analysis which indicate that similar conclusions hold in multivariate settings. Specifically, the individual task solutions to a multi-task learning problem are approximately minimum-norm solutions in a particular RKHS determined by the optimal neurons. In contrast, learning each task in isolation results in solutions that are minimum-norm with respect to a non-Hilbertian Banach norm over the optimal neurons.

## 2 Related Works

**Characterizations of ReLU neural network solutions:**  Hanin (2021); Stewart et al. (2023) characterized the neural network solutions to (2) in the univariate input/output setting. Boursier and Flammarion (2023) showed that in the univariate input/output case, when weight decay is modified to include the biases of each neuron, the solution is unique. Moreover, under certain assumptions, it is the sparsest interpolant (i.e., the interpolant with the fewest neurons). Our work differs from these in that we study the multi-task setting, showing that univariate-input multi-task solutions are almost always unique and equivalent to the connect-the-dots solution, which is generally _not_ the sparsest, and is a minimum-norm solution in a Sobolev RKHS. While characterizing solutions to (2) in the multivariate setting is more challenging, there exist some results under certain dataset assumptions Ardeshir et al. (2023); Zeno et al. (2024) or by leveraging a convex reformulation Ergen and Pilanci (2021); Mishkin and Pilanci (2023) of (2).

**Function spaces associated with neural networks:**  For single-output ReLU neural networks, Savarese et al. (2019); Ongie et al. (2019) related weight decay regularization on the parameters of the model to regularizing a particular semi-norm on the neural network function. Ongie et al. (2019) showed that this semi-norm is not an RKHS semi-norm, highlighting a fundamental difference between learning with neural networks and kernel methods. Parhi and Nowak (2021, 2022); Bartolucci et al. (2023); Unser (2021) studied the function spaces associated with this semi-norm, and developed representer theorems showing that optimal solutions to the minimum-norm data fitting problem over

Figure 1: Two solutions to ReLU neural network interpolation (blue) of training data (red). The functions on the left and right both interpolate the data and both are global minimizers of (2) and (3), and minimize the second-order total variation of the interpolation function Parhi and Nowak (2021). In fact, all convex combinations of the two solutions above are also global solutions to both training problems.

these spaces are realized by finite-width ReLU networks. Consequently, finite-width ReLU networks trained with weight decay are optimal solutions to the regularized data-fitting problem posed over these spaces. Function spaces and representer theorems for multi-output and deep neural networks were later developed in Korolev (2022); Parhi and Nowak (2022); Shenouda et al. (2024).

Multi-Task Learning: The advantages of multi-task learning have been extensively studied in the machine learning literature Obozinski et al. (2006, 2010); Argyriou et al. (2006, 2008); Caruana (1997). In particular, the theoretical properties of multi-task neural networks have been studied in Lindsey and Lippl (2023); Collins et al. (2024); Shenouda et al. (2024). The underlying intuition in these past works has been that learning multiple related tasks simultaneously can help select or learn the most useful features for all tasks. Our work differs from this traditional paradigm as we consider multi-task neural networks trained on very general tasks which may be diverse and unrelated.

## 3 Univariate Multi-Task Neural Network Solutions

For any function \(f\) that can be represented by a neural network (1) with width \(K\), we define its representational cost to be

\[R(f):=_{}_{k=1}^{K}\|_{k}\|_{2}\,\ \ \|_{k}\|_{2}=1\  k,\,f=f_{} \]

where \(=\{_{k},_{k},b_{k}\}_{k=1}^{K},, \). Taking an inf over all possible neural network parameters is necessary as there are multiple neural networks which can represent the same function. Solutions to (3) minimize this representational cost subject to the data interpolation constraint. This section gives a precise characterization of the solutions to the multi-task neural network interpolation problem in the univariate setting (\(d=1\)).

For the training data points \((x_{1},_{1}),,(x_{N},_{N})^{T}\), let \(y_{it}\) denote the \(t^{}\) coordinate of the label vector \(_{i}\). For each \(t=1,,T\), let \(_{t}\) denote the univariate dataset \((x_{1},y_{it}),,(x_{N},y_{Nt})\), and let

\[s_{it}=-y_{it}}{x_{i+1}-x_{i}} \]

denote the slope of the straight line between \((x_{i},y_{it})\) and \((x_{i+1},y_{i+1t})\). The connect-the-dots interpolant of the dataset \(_{t}\) is the function \(f_{_{t}}\) which connects the consecutive points in dataset \(_{t}\) with straight lines (see Fig. 2). Its slopes on \((-,x_{2}]\) and \([x_{N-1},)\) are \(s_{1t}\) and \(s_{N-1t}\), respectively. In the following section, we state a simple necessary and sufficient condition under which the connect-the-dots interpolation \(f_{}=(f_{_{1}},,f_{_{T}})\) is the _unique_ optimal interpolant of the datasets \(_{1},,_{T}\). We also demonstrate that the set of multi-task datasets which satisfy the necessary condition for non-uniqueness, viewed as a subset of \(^{N}^{T N}\), has Lebesgue measure zero.

This result raises an interesting new connection between data fitting with ReLU neural networks and traditional kernel-based learning methods. Indeed, connect-the-dots interpolation is also the minimum-norm interpolant over the first-order Sobolev space \(H^{1}([x_{1},x_{N}])\), itself an RKHS whose norm penalizes the \(L^{2}\) norm of the derivative of the function. In particular, \(f_{_{t}}\) agrees on \([x_{1},x_{N}]\) with the function \(f(x)=_{j=1}^{N}_{j}k(x,x_{j})\) whose coefficients \(_{j}\) solve the kernel optimization problem

\[_{_{1},,_{N}}_{i=1}^{N}_{j=1}^{N} _{i}_{j}k(x_{i},x_{j})\,\ \ _{j=1}^{N}_{j}k(x_{i},x_{j})=y_{it},\,i=1,,N. \]

with the kernel \(k(x,x^{})=1-(x-x^{})_{+}+(x-x_{1})_{+}+(x_{1}-x^{})_{+}\) De Boor and Lynch (1966). Therefore, our result shows that the individual outputs of solutions to (3) for \(T>1\) tasks almost always coincide on \([x_{1},x_{N}]\) with this kernel solution; for example, this occurs with probability one if the task labels are sampled from an absolutely continuous distribution. In contrast, optimal solutions to the (3) in the case \(T=1\) are generally non-unique and may not coincide with the connect-the-dots kernel solution Hanin (2022). We note that for \(T=1\), our result is consistent with the characterization of univariate solutions to (3) in Hanin (2022).

### Characterization and Uniqueness

Our main result is stated in the following theorem:

**Theorem 3.1**.: _The connect-the-dots function \(f_{}\) is always a solution to (3). Moreover, the solution to problem (3) is non-unique if and only if the following condition is satisfied: for some \(i=2,,N-2\), the two vectors_

\[_{i}-_{i-1}=_{i+1}-_{i}}{x_{i+1}-x_{i}}-_{i}-_{i-1}}{x_{i}-x_{i-1}} \]

_and_

\[_{i+1}-_{i}=_{i+2}-_{i+1}}{x_{i+2}-x_{i+1}}- {_{i+1}-_{i}}{x_{i+1}-x_{i}} \]

_are both nonzero and aligned.3 If this condition is not satisfied, then \(f_{}\) is the unique solution to (3). Furthermore, as long as \(T>1\) and \(N>1\), the set of all possible data points \(x_{1},,x_{N}\) and \(_{1},,_{N}^{T}\) which admit non-unique solutions has Lebesgue measure zero (as a subset of \(^{N}^{T N}\))._

**Corollary 1**.: _If \(T>1\) and \(N>1\) and the data points \(x_{1},,x_{N}\) and label vectors \(_{1},,_{N}^{T}\) are sampled from an absolutely continuous distribution with respect to the Lebesgue measure on \(^{N}^{T N}\), then with probability one, the connect-the-dots function \(f_{}\) is the unique solution to (3)._

**Remark 1**.: _The proof of Theorem 3.1, which relies mainly on Theorem 3.2 as we describe below, also characterizes solutions of the regularized loss problem_

\[_{}_{i=1}^{N}(f_{}(_{i}),_{i})+_{k=1}^{K}\|_{k}\|_{2}|w_{k }|=1,\;k=1,,K \]

_for input dimension \(d=1\), any \(>0\), and any loss function \(\) which is lower semicontinuous in its second argument. Specifically, any \(f_{}\) which solves (9) is linear between consecutive data points \([x_{i},x_{i+1}]\) unless the vectors \(}_{i}-}_{i-1}\) and \(}_{i+1}-}_{i}\) are both nonzero and aligned, where \(}_{i}:=}_{i+1}-}_{i}}{x_{i+1}-x_{i}}\) and \(}_{i}:=f_{}(x_{i})\)._

Previous works Shenouda et al. (2024) and Lindsey and Lippl (2023) showed that multi-task learning encourages _neuron sharing_, where all task are encouraged to utilize the same set of neurons or representations. Our result above shows that univariate multi-task training is an extreme example of this phenomenon, since \(f_{}\) can be represented using only \(N-1\) neurons, all of which contribute to all of the network outputs. Therefore, in the scenario we study here, neuron sharing almost always occurs even if the tasks are unrelated.

The full proof of Theorem 3.1 appears in Appendix A.1. We outline the main ideas here. Our proof relies on the fact that any \(^{T}\) ReLU neural network of the form (1) which solves (3) represents \(T\) continuous piecewise linear (CPWL) functions, where the change in slope of the \(t^{}\) function at the \(k^{}\) knot is equivalent the \(t^{}\) entry of the \(k^{}\) output weight vector (see Appendix A.1 for further detail). This fact allows us to draw a one-to-one correspondence between each knot in the function and each neuron in the neural network. The proof relies primarily on the following lemma:

Figure 2: The connect-the-dots interpolant \(f_{}=(f_{_{1}},f_{_{2}},f_{_{3}})\) of three datasets \(_{1},_{2},_{3}\).

[MISSING_PAGE_FAIL:6]

interpolant of the dataset must agree with the connect-the-dots interpolant \(f_{}\) before \(x_{2}\) and after \(x_{N-1}\); the details of this argument appear in Appendix A.1. As our theorem and corollary quantify, real-world regression datasets (which are typically real-valued and often assumed to incorporate some random noise from an absolutely continuous distribution, e.g. Gaussian) are extremely unlikely to satisfy this special alignment condition; hence, our claim that connect-the-dots interpolation is almost always the unique solution to (3).

## 4 Multivariate Multi-Task Neural Network Training

In Section 3, we proved that univariate-input functions learned by neural networks trained on multiple tasks simultaneously can be profoundly different from the functions learned by networks trained on each task separately. In this section, we demonstrate an extension of this phenomenon to the multivariate-input case. Similar to the univariate case, the multivariate single-task weight decay regularized learning problem corresponds to a norm-penalized learning problem in a non-Hilbertian Banach space, where solutions may be non-unique Parhi and Nowak (2021). Indeed, Figure 4 illustrates a multivariate single-task dataset with multiple min-norm interpolants. In this section, we show that, as in the univariate case, multivariate multi-task learning can produce solutions that are strikingly different from the corresponding single-task learning solutions. Moreover, we show that the multivariate multi-task learning problem can also be related to a norm-penalized learning problem over an RKHS. Here we analyze neural networks of the form

\[f_{}()=_{k=1}^{K}_{k}_{k}^{}+b_{k}_{+} \]

where \(_{k}^{d-1}\), \(b_{k}\), \(_{k}^{T}\), and \(:=\{_{k},_{k},b_{k}\}_{k=1}^{K}\). Since the analysis in this section is not dependent on the residual connection, we omit it for ease of exposition. We consider the multivariate-input, \(T\)-task neural network learning problem

\[_{}_{i=1}^{N}(_{i},f_{}( _{i}))+_{k=1}^{K}\|_{k}\|_{2} \]

for some dataset \((_{1},_{1}),,(_{N},_{N})^{d} ^{T}\), where \(\) is any loss function which is lower semicontinuous in its second argument and separable across the \(T\) tasks, and \(K N^{2}\) (see 1).

To characterize the nature of the functions whose parameters solve (12), note that the optimal task \(s\) output weights \(v_{1s}^{*},,v_{Ks}^{*}\) for (12) also minimize

\[J(v_{1s},,v_{Ks}):=_{i=1}^{N}(y_{is}, _{k=1}^{K}v_{ks}_{ik})+_{k=1}^{K}\|[ v_{ks}\\ _{ks}^{*}]\|_{2} \]

where \(_{k s}^{*}\) denotes the vector \(_{k}^{*}\) with its \(s^{}\) element \(v_{ks}^{*}\) excluded and \(^{N K}\) is a matrix whose \(i\), \(k^{}\) entry is \(_{ik}=_{i}^{}_{k}^{*}+b_{k}^{*}_{+}\). Thus, \(J\) is the objective function of (12) with all parameters except for \(v_{1s},,v_{Ks}\) held fixed at their optimal values. Note that if \(v_{1s}^{*},,v_{Ks}^{*}\) did not minimize \(J\), they would not be optimal for (12).

We are interested in analyzing the behavior of solutions to (12) as the number of tasks \(T\) grows. Intuitively, if \(T\) is very large, it it is reasonable to expect that the optimal output weight \(v_{ks}^{*}\) for an individual neuron \(k\) and task \(s\) would be relatively small compared to the sum of the output weights \(v_{kt}^{*}\) for tasks \(t s\). In this case, the \(k^{}\) term of the regularizer in (12) would be approximately equal to

\[\|_{k}^{*}\|_{2}\;=\;^{*})^{2}+\|_{k  s}^{*}\|_{2}^{2}}\;\;\|_{k s}^{*}\|_{2}+ ^{*})^{2}}{2\|_{k s}^{*}\|_{2}} \]

for any individual task \(s\), where \(\|_{k s}^{*}\|_{2}^{2}:=_{t s}(v_{ks}^{*})^{2}\). The approximation above comes from the Taylor expansion \(f(x)=+c^{2}}=c+}{2c}-}{8c^{3}}+}{ 16c^{6}}-\), whose higher order terms quickly become negligible if \(0<x c\). Notice that the right hand side of (14) is a quadratic function of \(v_{ks}^{*}\), which suggests that the regularization term of (13) resembles a weighted \(^{2}\) regularizer when \(v_{ks}\) is close to its optimal value \(v_{ks}^{*}\).

The above reasoning can be made precise by observing that, in multi-task learning problems, the order in which the tasks are assigned to the network outputs is irrelevant. Therefore, we can assume the tasks are assigned uniformly at random to each of the networks outputs. This random assignment process induces a distribution on the training data (labels) for each output \(_{.,1},,_{.,T}\). Under this assumption, the following are true with high probability as \(T\) grows (see Nakhleh et al. (2024), Appendix A.2 for further detail):

1. The magnitude of \(v^{*}_{ks}\) is dominated by \(\|^{*}_{k s}\|_{2}\), which implies that the remainder in the quadratic Taylor series approximation tends to zero.
2. \(\|^{*}_{k s}\|_{2}\) concentrates around the norm of the full vector of output weights \(\|^{*}_{k}\|_{2}\), which means that the Taylor approximation tends to the same quadratic function for all tasks.

A more detailed analysis is given in the companion paper Nakhleh et al. (2024); here we informally state a theorem summarizing the main conclusion.

**Theorem 4.1**.: _Nakhleh et al. (2024) For an individual task \(s\), consider the objective4_

\[H(v_{1s},,v_{Ks}):=_{i=1}^{N}(y_{is},_{k }v_{ks}_{ik})+_{k}(\| ^{*}_{k s}\|_{2}+_{ks}}{2\|^{*}_{k s }\|_{2}}) \]

_Then as \(T\) grows, the global minimizer \(v^{}_{1s},,v^{}_{Ks}\) of \(H\) satisfies_

\[|J(v^{}_{1s},,v^{}_{Ks})-J(v^{*}_{1s},,v^{*}_{Ks})|\; \;0 \]

_with probability approaching one._

The theorem states that the solution to the followed weighted \(^{2}\) regularized problem

\[_{v_{1s},,v_{Ks}}_{i=1}^{N}(y_{is},_{k }v_{ks}_{ik})+_{k }_{ks}v^{2}_{ks} \]

where \(_{ks}:=1/\|^{*}_{k/s}\|_{2}\) is an approximate minimizer of (13), with stronger approximation as \(T\) increases. In contrast, when \(T=1\), the optimization

\[_{v_{1},,v_{K}}_{i=1}^{N}(y_{i},_{k=1}^{K}v_ {k}_{ik})+_{k=1}^{K}|v_{k}| \]

yields output weights which are exactly optimal for (12). Note that the matrices \(\) in (17) and \(\) in (18) are not the same, since they are determined by the optimal input weights and biases for (12), which are themselves data- and task-dependent. Nonetheless, comparing (17) and (18) highlights the different nature of solutions learned for (12) in the single-task versus multi-task case. The multi-task learning problem favors linear combinations of the optimal neurons which have a minimal weighted \(^{2}\) regularization penalty. In contrast, the single-task learning problem favors linear combinations of optimal neurons which have a minimal \(^{1}\) penalty. Therefore, multi-task learning with a large number of tasks promotes a fundamentally different linear combination of the optimal features learned in the hidden layer.

To gain further insight, note that concentration of \(\|^{*}_{k s}\|_{2}\) around \(\|^{*}_{k}\|_{2}\) implies that for large \(T\):

\[_{ks}\;\;_{k}\;:=\;^{*}_{k}\|_{2}}\;. \]

(see Lemma 4.2 in Nakhleh et al. (2024)). This reveals a novel connection between the problem of minimizing (13) and a norm-regularized data fitting problem in an RKHS. Specifically, consider the finite-dimensional linear space

\[:=\{f_{}=_{k=1}^{K}v_{k}_{k}:^{K}\} \]where \(_{k}()=(_{k}^{*}+b_{k}^{*})_{+}\), equipped with the inner product

\[ f_{},f_{}_{}=^{} \]

where \(=(}{2},,}{2})\). As a finite-dimensional inner product space, \(\) is necessarily a Hilbert space; furthermore, finite-dimensionality of \(\) implies that all linear functionals (including the point evaluation functional) on \(\) are continuous. Therefore, \(\) is an RKHS, with reproducing kernel

\[(,^{})=_{k=1}^{K}_{k}()Q_{kk}^{-1}\,_ {k}(^{}). \]

Note that \(\) indeed satisfies the reproducing property, that is, \((,),f_{}=f()\) for any \(f\) and any \(\). To see this, write

\[(,),f_{}=_{k=1}^{K}Q _{kk}^{-1}_{k}()_{k},_{k=1}^{K}v_{k}_{k}_{ }. \]

We can view the term on the left as a function \(g_{}\) where \(u_{k}=Q_{kk}^{-1}_{k}()\) or \(=()^{-1}\), so this is equivalent to

\[(,),f_{}= g_{},f _{}=()^{-1}=_{k=1}^{K}v_{k} ()=f(). \]

Finding a minimizer of \(H\) over \(^{K}\) is thus equivalent to solving

\[*{arg\,min}_{f}_{i=1}^{N}(y_{is},f( _{i}))+\|f\|_{}^{2}. \]

We provide empirical evidence for the claims presented in this section in Figure 4 on a simple multi-variate dataset. First, we demonstrate the variety of min-norm interpolates to this dataset in a single task setting. In contrast, we show that the solutions obtained via multi-task learning with additional random tasks have less variability across different trials and are often much smoother than those obtained by single-task learning supporting our claim that these solutions are well approximated by solving a kernel ridge regression problem. We also verify that the optimization (15) is a good approximation for (13).

## 5 Conclusion and Discussion

We have shown that univariate, multi-task shallow ReLU neural networks which interpolate a dataset with minimal sum of squared weights almost always represent a unique function. This function performs straight-line interpolation between consecutive data points for each task. This solution is also the solution to a min-norm data-fitting problem in an RKHS. We provide mathematical analysis and numerical evidence suggesting that a similar conclusion may hold in the multivariate-input case, as long as the tasks are sufficiently large in number. These results indicate that multi-task training of neural networks can produce solutions that are strikingly different from those obtained by single-task training, and highlights a novel connection between these multi-task solutions and kernel methods.

Future work could aim to extend these results to deep neural network architectures. We also focus here on characterizing global solutions to the optimizations in (2) and (3). Whether or not networks trained with gradient descent-based algorithms will converge to global solutions remains an open question: our low-dimensional numerical experiments in Sections 3 and 4 indicate that they do, but a more rigorous analysis of the training dynamics would be an interesting separate line of research. Finally, while our analysis and experiments in Section 4 indicate that multi-variate, multi-task neural network solutions behave similarly to \(^{2}\) regression over a fixed kernel, we have not precisely characterized what that kernel is in the multi-input case as we have in the single-input case: developing such a characterization is of interest for future work.

Figure 4: ReLU network interpolation in two-dimensions. The solutions shown were obtained with regularization parameter \( 0\). _Top Row - Solutions to single-task training_: Figures 3(a), 3(b) and 3(c) show solutions to ReLU neural network interpolation (blue surface) of training data (red). The eight data points are located at the vertices of two squares, both centered at the origin. The outer square has side-length two and values of \(0\) at the vertices. The inner square has side-length one and values of \(1\) at the vertices. All three functions interpolate the data and are global minimizers of (2) and (3) when solving for just this task (i.e., \(T=1\)). Due to the simplicity of this dataset the optimality of the solutions in the first row were confirmed by solving the equivalent convex optimization to (2) developed in Ergen and Pilanci (2021). _Bottom Row - Solutions to multi-task training:_ Figure 3(d) shows the solution to the first output of a multi-task neural network with \(T=101\) tasks. The first output is the original task depicted in the first row while the labels for other \(100\) tasks are randomly generated i.i.d from a Bernoulli distribution with equal probability for one and zero. Here we show one representative example; more examples are depicted in Appendix B showing that this phenomenon holds across many runs. Figure 3(e) shows the solution to fitting the training data by solving (25) over a fixed set of features learned by the multi-task neural network with \(T=100\) random tasks. We observe that unlike the highly variable solutions of single-task optimization problem, the solutions obtained by solving the multi-task optimizations are nearly identical, as one would have for kernel methods. Moreover, the solution obtained by solving (25) is also similar to the solution of the full multi-task training problem with all \(T=101\) tasks.