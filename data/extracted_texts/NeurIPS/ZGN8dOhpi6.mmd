# A Pairwise Pseudo-likelihood Approach for Matrix Completion with Informative Missingness

Jiangyuan Li

Equal contribution

Department of Statistics

Texas A&M University

College Station, TX 77843

jiangyuanli@tamu.edu

&Jiayi Wang

Department of Mathematical Sciences

University of Texas at Dallas

Richardson, TX 75080

jiayi.wang2@utdallas.edu

&Raymond K. W. Wong

Department of Statistics

Texas A&M University

College Station, TX 77843

raywong@tamu.edu

&Kwun Chuen Gary Chan

Department of Biostatistics

University of Washington

Seattle, WA 98195

kcgchan@uw.edu

###### Abstract

While several recent matrix completion methods are developed to deal with non-uniform observation probabilities across matrix entries, very few allow the missingness to depend on the mostly unobserved matrix measurements, which is generally ill-posed. We aim to tackle a subclass of these ill-posed settings, characterized by a flexible separable observation probability assumption that can depend on the matrix measurements. We propose a regularized pairwise pseudo-likelihood approach for matrix completion and prove that the proposed estimator can asymptotically recover the low-rank parameter matrix up to an identifiable equivalence class of a constant shift and scaling, at a near-optimal asymptotic convergence rate of the standard well-posed (non-informative missing) setting, while effectively mitigating the impact of informative missingness. The efficacy of our method is validated via numerical experiments, positioning it as a robust tool for matrix completion to mitigate data bias.

## 1 Introduction

The goal of matrix completion is to recover a target matrix from its noisy and incomplete measurements. It is a modern high-dimensional missing data problem. Despite various significant advances made in the last two decades [e.g., 7, 17, 20, 18], many works on matrix completion still focus on the missing-at-random mechanism. Although such an assumption is doubtful in many real-life applications, there are very few options available for more general missing data mechanism, especially those with theoretical guarantees. This work aims to provide a principled and theoretically well-supported alternative method for missing-not-at-random settings, where missingness could depend on the measurements that are mostly unobserved.

A usual assumption to allow for succeeding matrix completion is that the unknown matrix is low-rank or approximately low-rank. The noiseless setting has been studied in  using nuclear norm minimization. The vast majority of existing theories on matrix completion assume that entries are revealed with a constant probability with respect to both entry location and measurement value.

Recent approaches to handling entries revealed with nonuniform probabilities, depending on the entry location, have shown the strength to improve matrix completion with solid theoretical guarantees (e.g., 31; 11; 28; 25; 23; 35; 23). These works aim to mitigate the effects due to the non-uniformity in observation probabilities . When additional row/column attributes are available, it is also possible to use this additional information for handling the non-uniform missing (e.g., 24). Although the non-uniform missing mechanism is quite flexible, it is fundamentally different from the missing-not-at-random mechanism. The key difference is whether the missing probability depends on the possibly unobserved measurement, which we will highlight below. In a missing-not-at-random setting, the methods developed for the non-uniform missing mechanism could still be biased in matrix recovery. See Section 6 for a numerical example. The method we propose in this work not only deals with (a flexible subclass of) missing-not-at-random settings, but it is also applicable in the non-uniform missing settings mentioned above.

In the missing data literature, likelihood-based methods for missing-not-at-random settings commonly involve specifying a parametric distribution of the missing data mechanism. However, this assumption should be used with caution, as it is highly sensitive and may easily induce a misspecified model, resulting in biased estimation and inaccurate results. To circumvent such issues, it is preferable to adopt a missingness assumption as flexible and generally applicable as possible. This type of assumption, often referred to as an unspecified missing data mechanism , avoids explicitly specifying a parametric model. Instead of using the full likelihood for estimation, certain unspecified missing-not-at-random assumption allows for the derivation of a non-standard likelihood , which serves as the foundation for subsequent estimation. Such non-standard likelihood approaches have been used in regression analysis  and variable selection when confronted with informative missing . One disadvantage of this approach is that not all the unknown parameters are estimable due to certain non-identification issues [16; 22].

In this work, we extend the pairwise pseudo-likelihood approach  to matrix completion with a mild separable informative missingness assumption (see Assumption 2.1 in Section 2), which is very flexible and generally applicable. While not all the parameters are estimable, we can identify the dispersion-scaled matrix up to a constant shift without suffering from bias due to informative missingness. This shows great promise to be applied in practice, for example, in recommendation systems where the rankings of entries are of interest.

Apart from the informative missing mechanism, our matrix completion method is based on the exponential family model, which has received extensive attention within the matrix completion literature for its efficacy in modeling non-Gaussian data, particularly discrete data. Notably, researchers have investigated its application in specific scenarios such as one-bit matrix completion  and multinomial matrix completion [4; 19]. The application of the exponential family model also extends to accommodating unbounded non-Gaussian observations, including Poisson matrix completion  and exponential family matrix completion [13; 21].

Overall, the combination of the separable missing-not-at-random mechanism and the exponential family model allows the proposed method to be applicable in a wide range of settings. We summarize the major contributions of this work as follows.

1. We formulate the pairwise pseudo-likelihood approach for matrix completion under informative missingness and exponential family model. To the best of our knowledge, the pairwise pseudo-likelihood approach has never been adopted in the matrix completion setup before. As opposed to the classical applications of pairwise pseudo-likelihood that assume i.i.d. sampling, matrix completion problems exhibit a non-identical and high-dimensional sampling structure.
2. We investigate the identifiability issues of the crucial separable missingness structures (Assumption 2.1) which lies at the core of the pairwise pseudo-likelihood approach.
3. We provide a non-trivial convergence analysis of the proposed estimator up to an identifiable equivalence class. Such analysis involves a novel concentration analysis of _matrix-valued_\(U\)-statistics where existing works on this type of concentration is sparse.

**Related Work**: To the best of our knowledge, we are one of the first works that consider the missing not at random (MNAR) setting in matrix completion and provide solid theoretical guarantees.  claims that they can deal with the MNAR setting. However, they assume selections and noise are independent conditioned on latent factors, as shown in their Assumption 2. On the contrary, our setting allows missingness to depend on noise.  also addresses informative missingness in matrix completion. However, they require additional covariate information to complete the matrix. Compared to the above two works, our setting is more general as we do not require independence between selections and noise given the true matrix, and we do not need additional covariate information. However, we do require independence across entries given the true matrix. while  allows selection to be dependent among different entries. Regarding to the theoretical bounds,  requires additional technical conditions to develop finite sample error bounds, and their bound is point-wise, i.e., the bound is for a given location \(i\).  also requires additional conditions on the likelihood and restricted eigenvalues to obtain convergence. Our error bound is developed under relatively weak conditions and achieves the minimax convergence rate.

## 2 Models

Let \(_{}=(A_{,ij})_{i,j=1}^{m_{1},m_{2}}^{m_{1}  m_{2}}\) be the matrix of interest, which is related to the observation through a generalized linear model. More specifically, we posit that the measurements \(Y_{ij}\) of the \((i,j)\)-th entry possesses a probability density/mass function of the exponential family form:

\[Y_{ij} f_{ij}(y|_{},_{}), f_{ij}(y|_{},_{}) h(y;_{})(y-G(A_{,ij})}{_{}}),\] (1)

where \(h:^{+}\) and \(G:\) are the base measure function and log-partition function associated with the canonical representation, and \(_{}>0\) is the dispersion parameter. Note that this family of distributions covers a wide variety of popular distributions including Bernoulli, Gaussian, and Poisson distributions. For matrix completion problems, we do not have measurements from every single entry. Let \(T_{ij}\) be the observation indicator variable of the \((i,j)\)-th entry, with value 1 if \(Y_{ij}\) is observed and 0 otherwise. We assume that \(\{((Y_{ij},T_{ij}):i=1,,m_{1};j=1,,m_{2})\) are independent.

Uniform-sampling-at-random (USR) mechanism is regarded as one of the simplest missing structures for matrix completion. Under USR, \((T_{ij}=1|Y_{ij})\) is a constant across all \(i,j\), which implies that the observation indicator \(T_{ij}\) is independent of the measurement \(Y_{ij}\). While this has been widely used to simplify theoretical analyses in many prior ground works [e.g., 7, 6, 17], USR is a strong assumption that can be violated in many applications. To address this issue, a few analyses and methods [e.g., 31, 11, 28, 18, 4, 25, 23, 35] have been developed based on the non-uniform missing structures, where \((T_{ij}=1|Y_{ij})=t_{ij}\) for \(0<t_{ij} 1\). Here the observation probabilities are allowed to differ across \(i,j\), but the missingness remains independent of the measurement \(Y_{ij}\). In this paper, we relax this restriction and allow whether an entry is observed or not to depend on the corresponding possibly unobserved measurement, leading to a challenging _missing-not-at-random_ (MNAR) setup.

Matrix completion under general MNAR is ill-posed, leading to non-identifiability of \(_{}\) (even under standard low-rank assumption). Indeed, general MNAR is ill-posed  not only in matrix completion, but also in regression  and statistical inference  in general. However, some additional structure imposed within the MNAR setting can ensure identifiability. To proceed, we make the following assumption, which corresponds to a flexible subclass of MNAR settings. This assumption makes it possible to identify \(_{}\) up to some equivalence relations (see Section 3).

**Assumption 2.1**.: The observation probability is separable in the following sense: \((T_{ij}=1|Y_{ij})=t_{ij}s(Y_{ij}),\) for some \(t_{ij}(0,1]\) and some non-negative function \(s():^{+}\).

As will be made clear later, the proposed technique does not require the knowledge of \(s()\) and \(\{t_{ij}\}\). A similar condition has been widely used in various regression problems [e.g., 22, 29, 37]. These works posit an i.i.d. setup with additional covariates, while our setup does not imply an identically distributed assumption across locations and has no covariates. Moreover, in our setup, it is not possible to observe replicates in the same location, while the i.i.d. setup generally allows replicates. Assumption 2.1 is flexible and widely applicable. Not only does it accommodate USR, it also includes non-uniform missing mechanism as a special case, where we can set \(s() 1\) and leave \(\{t_{ij}\}\) variable to account for the non-uniform missing. Obviously, as the observation probability is allowed to depend on possibly unobserved \(Y_{ij}\), it also includes many MNAR settings.

Clearly, we only have access to the _observed_ data, i.e., \(Y_{ij}\) conditional on \(T_{ij}=1\). To estimate \(_{}\), we first look at the observed data likelihood of the \((i,j)\)-th entry: \((Y_{ij}|T_{ij}=1;,)\) for \(^{m_{1} m_{2}}\) and \(>0\). By the Bayes' Theorem and Assumption 2.1,

\[(Y_{ij}|T_{ij}=1;,)==1|Y_{ij})f_{ ij}(Y_{ij};,)}{(T_{ij}=1|Y_{ij})f_{ij}(Y_{ij};,)dY_{ij}}\] \[=s(Y_{ij})(y;,)dy}f_{ij}(Y_ {ij};,)=s(Y_{ij})b_{ij}(,)f_{ij}(Y_{ij}| ;,),\]

where \(b_{ij}(,)=1/ s(y)f_{ij}(y|,)dy\). We see that the conditional likelihood involves unknown functions \(s()\) and \(b_{ij}()\), which makes the estimation of \(_{}\) difficult. To address this issue, we adopt a pseudo-likelihood approach  based on local ranks.

## 3 Pseudo-likelihood approach

Let \(=(e_{1},,e_{n})\{1,,m_{1}\}\{1,,m _{2}\}\) be a lexicographically ordered set of \(n\) unique locations \(\{(i,j):T_{ij}=1\}\). (Indeed, the specific choice of ordering does not matter.) Let the corresponding measurements be \(}=(_{1},,_{n}):=(Y_{e_{1}}, ,Y_{e_{n}})\) and the observation indicator be \(}=(_{1},,_{n}):=(T_{e_{1}}, ,T_{e_{n}})\). We also write \(_{k}=_{e_{k}}\) for \(k=1,,n\). We decompose the vector \(}\) into two vectors: the order statistics \(}_{()}=(_{(1)},,_{(n)})\) and the rank statistics \(=(R_{1},,R_{n})\). Precisely, \(_{(j)}\) is the \(j\)-th smallest entry in \(}\) and \(R_{k}\) is the rank of the \(k\)-th entry in \(}\). To motivate the proposed pseudolikelihood in (3), we first consider the conditional likelihood based on the full rank statistics given the observed data:

\[(|}_{()},}=;,) =^{n}s(_{k})t_{e_{k}}f_{e_{k}}(_{k};,)}{_{}_{k=1}^{n}s(_{( _{k})})t_{e_{k}}f_{e_{k}}(_{(k)};,)}\] \[=^{n}(_{k}_{k}/)}{ _{}_{k=1}^{n}(_{k}_{(k)}/)},\] (2)

where \(\) is the set of all one-to-one maps from \(\{1,,n\}\) to \(\{1,,n\}\), i.e., permutations. We notice that (2) does not involve unknown components \(s()\) and \(t_{ij}\) due to the separable assumption (Assumption 2.1), and does not depend on the base measure \(h()\) and the log-partition function \(G()\). However, (2) is computationally infeasible due to the summation over all permutations. The proposed pairwise pseudo-likelihood consider local ranks for pairs of observations. For any \(k\) and \(k^{}\), let \(^{L}_{kk^{}}\) denote the local rank statistic of \(_{k}\) and \(_{k^{}}\) among the pair \((_{k},_{k^{}})\). We denote \(}^{L}_{(k,k^{})}\) as the local order statistics \((\{_{k},_{k^{}}\},\{_{k},_{ k^{}}\})\). Instead of the full conditional probability (2), we consider the product of all possible combinations of the local rank conditional probability on observations:

\[_{k<k^{}}(^{L}_{kk^{}}=^ {L}_{kk^{}}|}^{L}_{(k,k^{})},_{k}= _{k^{}}=1;,)\] \[=_{k<k^{}}_{k}_{k}+_{k^{}}_{k^{}}}{})}{( _{k}_{k}+_{k^{}}_{k^{}}} {})+(_{k}_{k^{}}+_{k} _{k^{}}}{})}=_{k<k^{}}_{k}-_{k^{}})(_{k}-_{k^{}})/ )}.\] (3)

Similar to (2), this pairwise pseudo-likelihood (3) (of \(\) and \(\)) does not contain unknown functions and quantities. However, unlike (2), it does not involve all permutations and is therefore significantly easier to compute.

The negative logarithm of the pairwise pseudo-likelihood reads

\[_{1 k<k^{} n}(1+R_{kk^{}}(^{-1})),\] (4)

where \(R_{kk^{}}(^{-1})=\{-(_{k}-_{k^{} })(^{-1}_{k}-^{-1}_{k^{}})\}\). We notice two immediate issues with estimating \(_{}\) (and \(_{}\)) via minimizing (4).

_Scale Equivalence_: The values of (4) evaluated at any two pairs \((_{1},_{1})\) and \((_{2},_{2})\) are the same when \(_{1}^{-1}_{1}=_{2}^{-1}_{2}\). Therefore, (4) does not have the ability to distinguish between these pairs.

In other words, if \(_{}>0\) is unknown, (4) would not be able to identify elements in the equivalence class of \(_{}\) under equivalence relation: \( c_{1}\) for any \(c_{1}>0\). Instead, we try to estimate the dispersion-scaled matrix \(_{}^{-1}_{}\). Therefore, we consider

\[()=_{1 k<k^{} n}(1+R_{kk^{}}( )).\]

However, this does not solve all the identifiability issues, and, indeed, \(\) cannot identify a shift-equivalence class described below.

_Shift Equivalence_: Let \(\) be a matrix with all entries being one. Consider \(+c_{2}\) for any \(c_{2}\). Then

\[(+c_{2}) =R_{kk^{}}((+c_{2}))=\{-( _{k}-_{k^{}})(_{k}+c_{2}-_{k^{}}-c_{2})\}\] \[=\{-(_{k}-_{k^{}})(_{k}- _{k^{}})\}=().\]

Combining the scale and shift equivalence, we can only estimate \(_{}\) up to an equivalence relation \( c_{1}+c_{2}\) for any \(c_{1}>0\) and \(c_{2}\), which we will refer to as scale-shift equivalence. We remark that the scale-shift equivalence still allows the identification of much useful information from \(_{}\), such as ranking an arbitrary set of entries of \(_{}\). For example, in recommender system applications, one is mostly interested in the ranking within each row/column. Among the elements in the scale-shift equivalence class, we choose to estimate the following represent

\[}_{}=_{}^{-1}_{}-,_{}^{-1}_{}}{, }=_{}^{-1}_{}-,_{}^{-1}_{}}{m_{1}m_{2}} ,\] (5)

by imposing the constraint \(,=0\) in the optimization. Here \(,=_{i,j}A_{ij}B_{ij}\) for any matrices \(,^{m_{1} m_{2}}\).

Overall, we propose the following penalized pairwise pseudo-likelihood estimator

\[}=*{argmin}_{, =0,\|\|_{} a}()+\| \|_{},\] (6)

where \(\|\|_{}\) and \(\|\|_{}(=_{i,j}A_{ij})\) represent the nuclear norm and the entrywise max norm of a matrix \(\) respectively, and \(a, 0\) are tuning parameters. We also use \(\|\|_{F}\) to denote the Frobenius norm of a matrix \(\). Nuclear norm regularization has been commonly used to promote low-rankness in the estimation [25; 24; 14]. Since \(\) is convex, this optimization is convex. The discussion of the optimization algorithm is given in Appendix C. One natural question is whether there would be further hidden identifiability issues beyond scale-shift equivalence. In Section 5, we will provide a finite-sample error bound of the proposed estimator (6) based on the pairwise pseudo-likelihood, which indicates convergence to \(}_{}\), eliminating the possibility of additional identifiability issues.

## 4 Identifiability based on separable assumption

One of the major difficulties associated with informative missing is non-identifiability. We first emphasize the non-identifiability for constant shift is not an artifact of the pseudo-likelihood approach. The root cause is the informative missingness (Assumption 2.1). Here is a simple univariate example inspired from  to illustrate this point. Suppose we observe from two data-generating models, whose observations are identical in distributions.

**Model I**: \(Y_{1}(-1,1)\) with observation probability \((T_{1}=1|Y_{1}=y)=\), then

\[(T_{1}=1,Y_{1}=y)=p_{}(y+1),\]

where \(p_{}()\) is the p.d.f. of standard normal distribution.

**Model II**: \(Y_{2}(0,1)\) with observation probability \((T_{2}=1|Y_{2}=y)=(-1/2)\), then

\[(T_{2}=1,Y_{2}=y) =p_{}(y+1)(-1)(y+1)\] \[=p_{}(y+1)=(T_{1}=1,Y_{1}=y).\]Extending to the matrix form, the observation probabilities of the following two models, where \(_{1},_{2}^{m_{1} m_{2}}\), are exactly the same. **Model I**: \((_{1})(-,)\), \(t_{1,ij}=1\) for any \((i,j)\) and \(s_{1}(y)=\). **Model II**: \((_{2})(,)\), \(t_{2,ij}=1\) for any \((i,j)\) and \(s_{2}(y)=(-1/2)\).

As such, under Assumption 2.1, we cannot identify the constant shift. We note that, low-rank assumption generally would not provide enough additional information to eliminate this identifiability issue, as constant shift corresponds to at most a rank-1 perturbation.

The identification of the dispersion parameter is a difficult task because of the fact that at most one observation is available for each entry. Interestingly, as we have shown in the Appendix (Theorem B.2), under Assumption 2.1, the identification of the dispersion parameter is actually feasible in Gaussian distributions _with replicates_. However, it is unclear whether the dispersion parameter can be identified in a typical matrix completion setup, which often does not allow replicates. That said, previous works on exponential family matrix completion [21; 13] assume the dispersion parameter is known, under which there would not be a related identifiability issue.

## 5 Theoretical guarantee

Recall that \(_{}^{m_{1} m_{2}}\). We denote some convenient notation for dimensions, i.e., \(m=\{m_{1},m_{2}\},M=\{m_{1},m_{2}\},d=m_{1}+m_{2}\). We use the notation \(\) (\(\)) to denote less (greater) than up to an absolute multiplicative constant. We write \(a b\) if \(a b\) and \(b a\). Furthermore, define \(_{L}=_{i[m_{1}],j[m_{2}]}(T_{ij}=1)\) and \(_{U}=_{i[m_{1}],j[m_{2}]}(T_{ij}=1)\). We use \([n]\) to represent \(\{1,,n\}\) for integer \(n\). In this section, we derive the convergence of \(\|}-}_{}\|_{F}\). Recall that \(}_{}\), defined in (5), is the representer in the equivalence class of \(_{}\).

**Assumption 5.1**.: The following conditions hold.

1. There exists an absolute constant \(>0\) such that \(_{U}/_{L}\).
2. There exists a constant \(B\) such that \(\|\|_{} B\) almost surely.
3. There exists some constant \(>0\) (where \(\) can depend on \(\|}_{}\|_{}\)) such that \((Z^{2}_{ij,i^{}j^{}})\) for any \(i,i^{}[m_{1}]\), \(j,j^{}[m_{2}]\), where \[Z_{ij,i^{}j^{}}=(Y_{ij}-Y_{i^{}j^{}})-Y_{i^{}j^{}})(A_{ij}-A_{i^{}j^{}})/2)}{1+(( Y_{ij}-Y_{i^{}j^{}})(A_{ij}-A_{i^{}j^{}}))}.\]

Condition (C1) is posited to avoid some specific entries being sampled with very low probability in a relative sense, where the trace-norm penalization fails to work [11; 31]. Note that both \(_{U}\) and \(_{L}\) are allowed to diminish to zero as \(m_{1},m_{2}\), but Condition (C1) implies that their diminishing orders are the same. Condition (C2) is a technical assumption for analyzing the concentration inequalities of the involved \(U\)-statistics in pairwise pseudolikelihood. Note that this does not violate the parametric assumption on the distribution of \(Y\). For example, truncated normal distribution satisfies both. We leave the extension to a light-tail type of assumption for future work. Condition (C3) is a technical condition, and is used in deriving the (expected) Hessian of the loss function with respect to \(\) (see (8) in Appendix A). Note that (expected) Hessian of the loss is often important for deriving the convergence rate and so it is reasonable that a related term shows up in our condition. Indeed, this assumption Here, we provide further discussion to show that it is indeed a mild condition. Intuitively, it posits a positive _lower bound_ for an expectation of a _squared_ random variable. This expectation is always non-negative and is zero only when \(Z_{ij,i^{}j^{}}\) is exactly zero almost everywhere. For noisy matrix completion settings, this assumption is very mild because, when there are noises, this variable is not exactly zero almost surely. Next, we show that with the exponential family model, we can explicitly characterize \(\). First note that when \(_{}_{} a\) as assumed in Theorem 5.3 and \(\|\|_{} B\) as in Condition (C2), we have \(Z^{2}_{ij,i^{}j^{}}} \{Y^{2}_{ij}+Y^{2}_{i^{}j^{}}-2Y_{ij}Y_{i^{} j^{}}\}}[(Y_{ij})+ (Y_{i^{}j^{}})]\). Recall the density of \(Y_{ij}\) (1), one can derive \((Y_{ij})=G^{}(_{,ij})\), where \(G^{}()\) is nonnegative, from the well-known variance formula for exponential family. Therefore one can take \(=_{i,j}}2[G^{}( _{,ij})]\).

**Lemma 5.2**.: _We have \(\{(_{})\}=0\), where \(()\) is the expectation under the true parameter \(_{}\)._

**Theorem 5.3**.: _Assume \((}_{}) r\) and \(\|}_{}\|_{} a\) for some positive constants \(r,a>0\), under Assumptions 2.1 and 5.1, if we further assume \(m_{U}(d^{2})\) and \( B^{2}(d)[m_{1}m_{2}_{U}}]\), then with probability at least \(1-6/d\), the following holds:_

\[m_{2}}\|}-}_{} \|_{F}^{2}[ d]^{2}}{^{2}} ^{3}m_{2}_{L}},(d)}{}^{2} {m_{2}_{L}}}}.\] (7)

Our result implies that the penalized pairwise pseudolikelihood approach can consistently estimate \(}_{}\). Note that the difference between \((}_{})\) and \((_{})\) is at most \(1\). So a low-rank assumption on \(_{}\) automatically translates to a low-rank assumption on \(}_{}\). Most existing work present the upper bound concerning the number of observed entries \(n\) and treat the matrix completion as a trace regression problem [e.g. 28, 18, 5, 25]. One can take \(n\) as \(m_{1}m_{2}_{L}\) in their bound to compare their results with ours. Similar to the bound established in , our bound has two components and matches with the rates in their upper bound (up to some constants and logarithmic factor).  and  show a bound that has the same order as the first term (up to some constants and logarithmic factor) with some additional assumptions.  adopts the uniform sampling and boundedness of the condition number for \(\|}_{}\|\).  assumes that the sampling distribution follows a product distribution and the "spikiness ratio" (see \(_{sp}\) in  ) is bounded. Besides the above matrix completion methods that use the nuclear norm regularization, the estimators utilizing the max-norm regularization [e.g. 5, 35] establish the same bound as the second term(up to some constants and logarithmic factor) when they assume the max-norm of \(}_{}\) is bounded. While the aforementioned methods address various missing mechanisms, it is important to emphasize that none of them can handle MNAR setting, where the missingness may depend on the observations. However, our method can tackle such informative missingness. It is interesting to see that our error bound resembles the same convergence rate as  (minimax optimal rate) up to a logarithmic order, despite that our setup allows MNAR mechanism.

In terms of theoretical analysis, the most notable distinction between our estimator with other existing ones lies in the objective function. The pairwise pseudo-likelihood we employ imposes unique theoretical challenges. Firstly, the gradient and Hessian are no longer as straightforward as those in the commonly used squared loss or negative log-likelihood loss (for exponential family). We carefully derive these two terms, expressing them as pairwise summations (see exact forms in Eq. (8) and Eq. (9)). Secondly, the elements in these pairwise summations are not mutually independent, posing difficulties in establishing the concentration inequality to bound them. Indeed, we need to develop corresponding theoretical tools for tackling the corresponding matrix concentration of a _matrix-valued_\(U\)-statistics. To address this challenge, we leverage the grouping lemma (Lemma A.5) to decouple these summations into different groups where mutual independence holds within each group. To obtain the efficient grouping, the decoupling is applied to those observed entries. Additionally, while the trace regression model provides a convenient tool for analyzing the sampling distribution, it implicitly assumes "sampling with replacement", i.e., every entry can be observed repeatedly. We adopt the framework of the Bernoulli model for the observation indicator to avoid the issue. However, theoretical analysis become more challenging. A conditional argument (see the conditional event \(\) in (10)) is developed to address the discrepancy between these two frameworks. In addition, Lemma A.6 is established to marginalize the conditional event.

Finally, we remark that, while pseudo-likelihood approaches have been applied in regression analysis  and variable selection  to deal with informative missingness, such analyses mainly focus on i.i.d. design and usually make direct restricted eigenvalue condition of the (high-dimensional) Hessian matrix. In our problem, the eigenvalue condition is related to the observation probabilities. As in typical analysis of matrix completion, one is interested in the dependence on these probabilities, as they are allowed to diminish as \(m_{1},m_{2}\). As such, we also analyze the corresponding restricted eigenvalue bound, under the complicated grouping nature and identifiability issue. By adapting the techniques aforementioned, we provide a rigorous convergence result in non-i.i.d. design, which involves analyzing the concentration of a matrix-valued \(U\)-statistics (i.e., the Hessian matrix). This analysis distinguishes our work from a mere application of standard pseudo-likelihood theory, and the techniques used in the proof contribute to the field on their own merit.

Numerical experiments

We conduct the following simulation study to demonstrate the efficacy of the proposed method. We generate a \(50 50\) matrix \(_{}\) with rank \(r=5\). The observations \(Y_{ij}\) are generated from a Gaussian distribution with mean \(A_{,ij}\) and variance \(^{2}\) independently. In our study, we have settings with different variances \(^{2}\). The probability of each entry being observed is related to the value of the entry itself: \((T_{ij}=1|Y_{ij})=1/[1+(3Y_{ij})]\). Since the observation probability is smaller for larger \(Y_{ij}\), there exists a distinctive distributional shift between the observed and unobserved entries, as shown in Figure 2

We use the observed entries as training data and equally split the unobserved data as validation and test data. We compare our method with SoftImpute , CZ , MFW  and SNN . The validation data is used for hyper-parameter tuning in each method. Since the objective function is convex in the proposed method, we only tune the regularization parameter \(\), and fix the number of iterations as \(T=100\) and step size \(=1.0\) in Algorithm 2. We use the output of SoftImpute  with the same regularization parameter \(\) as a warm-up initialization to shorten the training time. For SoftImpute , CZ  and MFW , we tune the hyper-parameters involved in the optimization and regularization as suggested. As for SNN , we choose uniform weights and spectral threshold suggested in , and choose the number of neighbors between 1 and 2. Due to the identifiability issue, the validation data is also used to learn a shift and scale parameter (via a simple linear regression) for the proposed method, which is then used in reporting error metrics on test data.

Before getting into the error metric, a simple check on the bias of each method is via histograms. We pick one run with \(^{2}=1\) and plot the distribution of recovered entries without any transformation for each method, as shown in Figure 3. It shows that only the proposed method is able to mitigate the observational bias due to the underlying informative missing structure, and exhibits a symmetric distribution, while the distributions generated by other methods are left-skewed, due to the left-skewness of distribution of the observed entries.

We added comparisons of the computational time regarding the setting in Figure 2 (\(^{2}=1\)). The computational times are listed in Table 1. While incorporating more complex missing mechanisms, our method and SNN also take the most time. One practical way to speed up the computation of our method is to use a stochastic version of Algorithm 1 (i.e., training in batches). The focus of this paper is more on the robust recovery when encountering informative missing, and less on the computational efficiency with the knowledge that it could be theoretically slower than other SVD-based methods. However, our method is still faster than SNN, where both methods consider more complex missing mechanisms. Given the promising statistical properties of the proposed method, a future direction is to develop scalable algorithms for the proposed estimator or its variants.

To further validate the effectiveness of the proposed method, we vary the variances \(^{2}\) in the simulation. This setting is designed to differentiate non-uniform missingness and informative missingness. When the variance is small, the informative missingness is less severe, and non-uniform missingness might be used to approximately describe the missing mechanism. When the variance is large, the observational probability is more affected by the outcome as in a typical informative missingness setting. We choose the variances \(^{2}=0.0,0.2,0.4,0.6,0.8,1.0\). For each setting, we repeat the simulation 9 times and report the average test root mean squared errors (TRMSE) with standard errors, shown in Figure 2. We see that as the variance gets larger, there is a larger improvement in the proposed method with the design to account for informative missing over other methods. SNN  performs the worst when the variance is large, as it mainly borrows information on observed entries which introduces a substantial bias. It demonstrates the robustness of the proposed method in difficult settings where the missing structure is informative.

## 7 Real data application

In this section, we use three data examples to illustrate the robust performance of the proposed method. These are the Tobacco Dataset , Coat Shopping Dataset  and Yahoo! Webscope Dataset1. These datasets have been used in prior works for the demonstration of matrix completion methods (e.g., 2, 35). Due to space limitation, we refer the readers to Appendix C.2 for more detailed discussions of the datasets and our analyses. Following the details of the implementation in Section 6, we report the results in Table 2. For the Coat Shopping Dataset and Yahoo! Webscope Dataset, the evaluations are based on associated test sets from the original data sources. As for the Tobacco Dataset, following , the missing data are randomly generated 100 times according to cigarette sales. Here is a summary of the results.

**Tabacco Dataset.** As we can see from Table 2, our method only performs worse than SNN for this MNAR dataset, with significantly smaller TRMSE than the other three methods. Note that in this synthetic missing data, the way to generate missingness is adapted from the SNN paper. When one entry is missed in Tobacco dataset, the entries in the following period are also missed. This does not satisfy the assumption of our work. So it is not surprising to see our method perform sub-optimality. However, the performance of our method still remains strong.

   Methods & Time (s) \\  Soft Impute & \(0.01 0.005\) \\ Max Norm (CZ) & \(0.22 0.09\) \\ Model Free Weighting (MFW) & \(1.90 0.75\) \\ Synthetic NN (SNN) & \(15.23 1.39\) \\ Pseudolikelihood & \(8.67 1.23\) \\   

Table 1: Computational time comparison with \(^{2}=1\).

**Coat Shopping Dataset.** As Table 2 shows, SNN performs much worse than the remaining methods for this dataset. MFW has the smallest TRMSE. Our method has smaller errors than SoftImpute and has comparable performance to CZ.

**Yahoo! Webscope Dataset.** Due to its large size and to simplify the computation, we conducted a selection procedure to reduce the size of the matrix. Please see details in Appendix C.2 about how to obtain the subset of the matrix. From Table 2, we can see that the two methods (SNN and our method) that are designed for MNAR have better performance than the remaining methods, and our method has the smallest TRMSE.

Overall, our method performs robustly well across all these three datasets. In our comparison, a few alternatives can perform very well in one example, but badly in another. For example, SNN has an excellent performance in the Tobacco Dataset while performing very poorly in the Coat Shopping Dataset. The robust performance of our method is appealing in practice, as the missing mechanism is often unknown.

## 8 Conclusion

In this paper, we tackle the matrix completion problem where missingness could depend on the possibly unobserved measurements, constituting a challenging missing-not-at-random setting. The proposed method is developed under a flexible separable missingness assumption, which allows us to develop a pairwise pseudo-likelihood approach. Corresponding identification is investigated. We also provide a non-trivial convergence analysis, as well as some numerical experiments to illustrate the efficacy of the proposed estimation. Due to the flexibility in both the missing structure (separable missingness) and measurement model (exponential family model), the proposed technique would be useful in a wide range of applications.

The grouping nature of the proposed method poses an additional burden in computation, particularly when dealing with a large number of observed entries. For future works, we consider adapting the stochastic grouping idea to reduce the computational cost and exploring its application in large-scale recommender systems.

## 9 Acknowledgements

The authors thank the reviewers for their helpful comments and suggestions. Portions of this research were conducted with the advanced computing resources provided by Texas A&M High Performance Research Computing. The work of Jiayi Wang is partly supported by the National Science Foundation (DMS-2401272). The work of Raymond K. W. Wong is partly supported by the National Science Foundation (DMS-1711952 and CCF-1934904). The work of K. C. G. Chan is partly supported by the National Science Foundation (DMS-1711952).