# Bayesian nonparametric (non-)renewal processes for analyzing neural spike train variability

David Liu

Department of Engineering

University of Cambridge

dl543@cam.ac.uk

&Mate Lengyel

Department of Engineering

University of Cambridge

m.lengyel@eng.cam.ac.uk

###### Abstract

Neural spiking activity is generally variable, non-stationary, and exhibits complex dependencies on covariates, such as sensory input or behavior. These dependencies have been proposed to be signatures of specific computations, and so characterizing them with quantitative rigor is critical for understanding neural computations. Approaches based on point processes provide a principled statistical framework for modeling neural spiking activity. However, currently, they only allow the instantaneous mean, but not the instantaneous variability, of responses to depend on covariates. To resolve this limitation, we propose a scalable Bayesian approach generalizing modulated renewal processes using sparse variational Gaussian processes. We leverage pathwise conditioning for computing nonparametric priors over conditional interspike interval distributions and rely on automatic relevance determination to detect lagging interspike interval dependencies beyond renewal order. After systematically validating our method on synthetic data, we apply it to two foundational datasets of animal navigation: head direction cells in freely moving mice and hippocampal place cells in rats running along a linear track. Our model exhibits competitive or better predictive power compared to state-of-the-art baselines, and outperforms them in terms of capturing interspike interval statistics. These results confirm the importance of modeling _covariate-dependent_ spiking variability, and further analyses of our fitted models reveal rich patterns of variability modulation beyond the temporal resolution of flexible count-based approaches.

## 1 Introduction

Analyses of spike time  and count  statistics have revealed neural responses _in vivo_ to be structured but generally variable or noisy . To model this stochastic aspect of neural spike trains, probabilistic approaches based on temporal point processes have been widely applied. This in turn has been a major driver of point process theory development  for capturing spiking variability structure with statistical models . The study of neural computation underlying naturalistic behavior in particular involves non-stationary spike trains, which presents a significant challenge as apparent spiking variability is a result of both irreducible "intrinsic" neural stochasticity as well as dependencies on behavioral covariates that can themselves vary on multiple time scales.

Different approaches have been proposed for handling non-stationary spike trains, starting with the classical log Cox Gaussian process  to allow variations in the local intensity or firing rate while modeling independent spikes. Dependencies on previous spikes can be captured to first order with renewal processes, and these models have been extended to non-stationary cases through modulation of the hazard function with some time-dependent function  or through rescaling interspike intervals with a covariate-dependent rate function . Another approach based on Hawkes processes and spike-history filters  introduces conditional point processes that gobeyond the first order Markov assumption. Approaches based on recurrent networks [51; 90] and neural ODEs [10; 41] can in theory capture arbitrarily long dependencies on past spikes and input covariates, but provide more limited descriptive interpretability.

However, not only the rate but also the variability of spiking encodes task-relevant information [34; 60], and bears signatures of the underlying computations . Importantly, this variability has stimulus- and state-dependent structure [12; 21; 48; 67]. In statistical modeling language, this corresponds to heteroscedastic or input-dependent observation noise. Such structure reflects computations performed in the underlying neural circuit, and thus characterizing it from data in a flexible and robust manner is critical for advancing theories of neural computation. The classical approaches reviewed above do not attempt to characterize such covariate-dependent changes in variability. Flexible count models have been introduced to more faithfully capture variability at the count level , and recent work has extended this to the general case of input-dependent variability . Count approaches however are limited in the resolution of the analysis set by the time bin size. In addition, the resulting count statistics strongly depend on the chosen time bin size [48; 73].

While firing rates are routinely modeled as input-dependent, extending point process models with input-dependent variability has not been widely explored in the literature. Rate-rescaled and modulated renewal processes rely on fixed base renewal densities. Allowing the shape parameters of the renewal density to vary with covariates in the corresponding hazard function is one potential approach, but this still relies on a commitment to a particular parametric family of renewal densities. Spike-history filters in conditional point processes are conventionally fixed and thus do not directly model input-dependent spiking variability, though dependence on observed and unobserved covariates  and switching filters based on discrete states  have been considered. Recent work has moved away from parametric filters to nonparametric Gaussian processes , which can be extended to flexibly model dynamic filters as functions of external covariates with a spatio-temporal Gaussian process. However, any modulation of the filter will no longer permit fast convolutions, and such models will be computationally expensive as the filter needs to be recomputed every time step. The direct nonparametric estimation of conditional intensity functions based on maximum likelihood has been explored [13; 82], but scalable Bayesian approaches have remained absent.

ContributionTo enable flexible modeling as well as modulation of instantaneous point process statistics for analyzing neural spike train variability, we introduce the Bayesian nonparametric nonrenewal process (NPNR). NPNR builds on sparse variational Gaussian processes and defines a nonparametric prior over conditional interspike interval distributions, generalizing modulated renewal processes with nonparametric renewal densities and spike-history dependencies beyond renewal order. In particular, our point process can flexibly model modulations of not only spiking intensity but also variability. We validate our model using parametric inhomogeneous renewal processes, recovering conditional interspike interval distributions and identifying renewal order in spike-history dependence. On neural data from mouse thalamus and rat hippocampus, our method has competitive predictive power while being superior in capturing interspike interval statistics from the non-stationary data. In particular, our method provides instantaneous measures of spike train variability that are modulated by covariates, and shows rich variability patterns in both datasets consistent with previous studies at coarser timescales. We provide a JAX implementation of our method as well as established baseline models within a scalable general variational inference scheme. 1

## 2 Background

We start with a brief overview of the theoretical foundations and related point process models, as well as their combination with Gaussian processes to introduce non-stationarity.

### Temporal point processes

Statistical modeling of events that occur stochastically in time is handled by the general framework of temporal point processes [59; 72]. Denoting the number of events that occurred until time \(t\) by \(N(t)\), a temporal point process model is completely characterized by its conditional intensity function (CIF)

\[(t|_{t})=_{ t 0}[N(t+ t )-N(t)|_{t}]}{ t}\] (1)where \((t) t\) is the probability to emit a spike event in the infinitesimal interval \([t,t+ t)\) conditioned on \(_{t}\), the spiking history before \(t\). We can write the point process likelihood for a single neuron spike train consisting of an ordered sequence of \(S\) spike events at times \(t_{i}\) as 

\[p(t_{1},,t_{S}|())=[_{i=1}^{S}(t_{i}| _{t_{i}})]\,e^{-_{0}^{T}(t^{}|_{ t^{}})\,t^{}}\] (2)

In neuroscience applications, one often wants to describe modulation of the point process statistics with some time-varying covariates \((t)\), such as animal head direction or body position, which leads to a generalized CIF \((t|_{t},_{ t})\). Several classes of models have been proposed that are defined by particular restrictions on the functional form of \((t|_{t},_{ t})\).

#### 2.1.1 Inhomogeneous renewal processes

Renewal assumptionThe statistical model in Eq. 2 describes dependencies between all spikes. One common simplification is the renewal assumption: interspike intervals (ISIs) \(^{(i)}=t_{i+1}-t_{i}\) are drawn i.i.d. from an interval distribution called the renewal density \(g(;)\) with parameters \(\). This induces a Markov structure \(p(t_{i}|t_{i-1},t_{i-2},)=p(t_{i}|t_{i-1})\) in the spike train likelihood

\[p(t_{1},,t_{S};)=p(t_{1})\,p(t_{2}|t_{1}) p(t_{S}|t_{S-1})=p (t_{1})\,_{i=1}^{S-1}g(u_{i};)\] (3)

Common renewal densities used for neural data are the exponential (equivalent to a Poisson process), gamma, and inverse Gaussian distributions .

Hazard function modulationNon-stationary point processes need to model changes in statistics with time, and combined with the renewal assumption one obtains inhomogeneous renewal processes. A classical approach that dates back to Cox  is to modulate the hazard function (Appendix A)

\[(t|t_{i})=h()(t)\] (4)

with time since last spike \(=t-t_{i}\) and the modulation factor \((t)\). In our context, this can be replaced by some function of covariates \((_{t})\). A multiplicative interaction between \(\) and \(h\) as above is typically considered, though this framework allows general parametric forms .

Rate-rescalingAnother approach that has been widely applied in the neuroscience community is rate-rescaling , closely related to time-rescaling (see Appendix B.3). Here, modulation is achieved with a rate function \(r(_{t}) 0\) that transforms time \(t\) into rescaled time \(\)

\[(t)=^{t}r(_{t^{}})\,t^{}\] (5)

This maps all spike times \(t_{i}\) to rescaled times \(_{i}\), and will be one-to-one as long as \(r(t)>0\). By modeling the rescaled ISIs \(^{(i)}=_{i+1}-_{i}\) as drawn from a stationary renewal density \(g()\), we obtain an inhomogeneous renewal process from a homogeneous one. The CIF becomes dependent on the covariate path since last spike \(_{k}=\{(u)|u(t_{i},t])\}\), see Appendix B.3.

#### 2.1.2 Conditional point processes

Conditional Poisson processesThe renewal assumption ignores correlations between ISIs, which generally are observed in both the peripheral and central nervous system  and can be computationally relevant for signal detection and encoding . Going beyond Markovian dependencies, a tractable approach, similar to Hawkes processes , is to introduce a causal linear filter \(h(t)\) that is convolved with spikes and added to the log CIF, giving conditional Poisson processes

\[(t|_{t},_{t})=h*y(t)+f(_{t}),y(t)=_{i}(t-t_{i})\] (6)

where \(*\) denotes temporal convolution. These models are closely linked to mechanistic integrate-and-fire models  and have a long history in the neuroscience literature , appearing as generalized linear models (GLMs) and spike response models (SRMs).

Conditional renewal processesAn even more expressive model can be obtained by replacing the Poisson spiking process with a rate-rescaled renewal process. This results in a conditional renewal process, where the rate function has history dependence \(r(t|_{t},_{t})\) as in Eq. 6.

### Gaussian process modulated point processes

Gaussian processes (GPs) represent a data-efficient alternative to neural networks, which have been widely used to model the CIF [59; 72; 90]. When combining GPs with point process likelihoods, the resulting generative model leads to doubly stochastic processes for event data. Placing a Gaussian process prior  over the log intensity function leads to the classic log Cox Gaussian processes [14; 55], and in the same spirit one can modulate renewal hazard functions  or perform rate-rescaling  with GPs. Such constructions form the basis of many widely used Bayesian neural encoding models for spike trains, both for modeling single neuron responses [15; 16; 68] as well as population activity [19; 93]. Combining the flexibility offered by renewal and conditional point processes with GP rate or modulation functions within a variational framework has been impeded by the fact that the original papers were built on a GLM framework with parametric covariate mappings [6; 7; 18; 65]. To provide a fair comparison of NPNR to these baselines, we implement a general scalable variational inference framework for the construction and application of such models (see Appendix B for details on the baseline models).

## 3 Method

We now introduce the nonparametric non-renewal (NPNR) process and present the approximate Bayesian inference scheme used for model fitting, noting connections to related works in the literature. Our NPNR model provides a nonparametric generalization of modulated renewal processes beyond renewal order, and adds suitable inductive biases for neural spike train data. It implicitly defines a flexible prior over conditional ISI distributions that can be computed using pathwise conditioning, which enables one to analyze spiking variability modulation with minimal parametric constraints. Furthermore, the Bayesian framework provides an elegant data-driven approach to inferring the lagging ISI order of the spike-history dependence.

### Generative model

Conditional intensity surface priorsTo obtain flexible modulated point process models, we directly model the CIF, or more precisely its logarithm, of the form

\[(t|_{t},_{ t})=(t|t_{i},t_{i-1},,_{t})\] (7)

where \(t_{i}\) is the most recent spike at current time \(t\). First considering the renewal case, we note the spatio-temporal structure in the log CIF using time since last spike \(=t-t_{i}\) is

\[(t|_{t},_{t})=(t|t_{i},_{t})=f( ,_{t})\] (8)

which suggests placing a spatio-temporal GP prior on the log CIF to describe a log intensity surface

\[f(,_{t})(m(,),k_{t}(,^{ }) k_{x}(,^{}))\] (9)

This generalizes the parametric forms of modulation considered in previous approaches [43; 79], in particular allowing modulation of the effective instantaneous renewal density by covariates \(_{t}\). We can introduce lagging ISIs covariates \(_{k}(t)\) with lag \(k\) as depicted in Fig. 1A to extend the model to a non-renewal process

\[(t|t_{i},t_{i-1},t_{i-2},)=f(,_{1}(t),_{2}(t ),)\] (10)

and for a maximum ISI lag \(K\) we denote the lagging ISIs \(_{t}=[_{1}(t),,_{K}(t)]\) to obtain

\[(t|_{t},_{t})=f(,_{t},_{t})\] (11)

Inductive biases for neural dataFor neural spiking data, there are biological properties to consider for building a more realistic prior. Firstly, neurons have refractory periods immediately following a spike, though in practice neural recordings may not respect this due to contamination in spike sorting . Another potentially useful inductive bias is that changes in the spiking intensity of neurons fluctuate mostly at shorter ISI timescales , whereas at longer delays they tend to be temporally smoother. The latter suggests non-stationary GP kernels to be more suitable for modeling the spike-history dependencies . However, non-stationary kernels do not allow straight-forward use of random Fourier features for evaluating GP posterior function samples at many locations with pathwise conditioning [88; 89]. This in particular is needed to compute the conditional ISI distributions \(g(|)\) in Eq. 15, see Appendix B.5 for details. To achieve the desired non-stationarity for modeling Eq. 11 while maintaining the ability to draw samples using pathwise conditioning, we apply time warping on \(\) from \([0,)\) with some warping timescale \(_{w}\)

\[=1-e^{-/_{w}}=-_{w} (1-)\] (12)

and place a stationary Gaussian process prior over the warped temporal dimension \(f(,)\) with a temporal kernel \(k(,^{})=k(-^{})\). This transformation is monotonic (see Fig. 1B), and hence we can easily compute the transformation on the CIF

\[(t|)=|| }{}|=e^{f(,)}\,)/_{w}}}{_{w}}\] (13)

Similarly, we apply time warping to the \(_{k}\) dimensions on which we also place stationary kernels \(k(,^{})=k(-^{ })\). We note that unlike spike-history filters in conditional point processes (Eq. 6) which do not change with inputs, the resulting coupling to past activity in Eq. 11 is dependent on covariates \(_{t}\). This allows one to capture spiking variability modulation via the conditional ISI distribution perspective discussed below. The refractory nature of real neurons can be addressed by the mean function

\[m(,)=m()=a_{m} e^{-/_{m}}+b _{m}\] (14)

with parameters \(a_{m},_{m},b_{m}\), where refractory periods can be modeled with large negative \(a_{m}\).

Conditional ISI distributionsInstead of looking at the CIF, we can view the model as a prior over conditional ISI distributions as depicted in Fig. 1C using the relation (see Appendix A)

\[g(|_{t},_{(t_{i},t]})(t|_{t}, _{t}) e^{-_{t_{i}}^{t}(t^{}|_{t^{ }},_{t^{}})\,t^{}}\] (15)

where one drops the dependence on \(_{t}\) in \(g\) for the modulated renewal case. If one fixes the lagging ISIs and picks a constant covariate path \(g(|_{*},_{*})\), this can be interpreted as an instantaneous ISI distribution of a neuron at the conditioned inputs \(_{*}\) and \(_{*}\). Moments of the conditional ISI distribution are computed using Gauss-Legendre quadratures in warped time (Appendix B.5)

\[_{g()}[^{m}]=_{0}^{}g()\,^{m}\, =_{0}^{1}|}{}|g( ())\,()^{m}\,\] (16)

and this can be used to compute tuning curves of spike train statistics, such as the mean ISI \([]\) and coefficient of variation \(=[]}/[]\), as a function of \(_{*}\). This approach generalizes the homogeneous case considered in the literature, and in particular allows one to compute instantaneous measures of non-stationary spike train variability that are otherwise non-trivial to estimate [75; 58].

### Inference

Temporal discretizationThe generative model is formulated as a continuous time model. In practice, neural and behavioural data are typically recorded with finite temporal resolution at small regular intervals \( t\). The cumulative intensity integral has to be approximated by a sum, though note that directly modeling the cumulative hazard function  elegantly avoids this for purely temporal point processes. Spike times are now discretized as a binary vector \(=[y_{1},,y_{T}]\) where \(y_{t}=1\) if \(t\) has a spike event, zero otherwise. Overall, this discretizes the point process likelihood Eq. 2 as

\[p(t_{1},,t_{S}|(t)) p(|)=_{t= 1}^{T}_{t}^{y_{t}}e^{-_{t=1}^{T}_{t}\, t}\] (17)

where we have \(T\) time steps in total. Note that the discretization scheme implies we do not have observations at \(=0\), since the time step immediately after an observed spike has \(= t\).

Variational lower boundWe use stochastic variational inference  with batches obtained from consecutive temporal segments and sparse variational GPs , giving the loss objective

\[=_{n=1}^{N}(_{q(_{n}|_{n})}[-y_{ nt}f_{nt}+ t_{t=1}^{T}e^{f_{nt}}]+D_{}(q(_{n})|\;p( _{n})))\] (18)

where \(n\) indexes neurons (of which there are \(N\))2, \(=[u_{1},,u_{M}]\) denotes the set of \(M\) inducing points, \(p()\) the GP prior at inducing locations, \(q()\) the variational posterior, and \(q(|)\) the conditional posterior (see Appendix B.1 for details). Combined with temporal mini-batching to fit batch segments of length \(T\), we can fit to very long time series given the \(N\,T\,M^{2}+N\,M^{3}\) computational complexity. We also no longer rely on computing hazard functions of parametric renewal densities to obtain the CIF, which can be numerically unstable. Modulated renewal processes instead rely on a specialized thinning procedure , but we take a more scalable and general variational approach. Overall, we optimize the kernel hyperparameters, variational posterior mean and covariance, inducing point locations, and mean function parameters \(a_{m}\), \(b_{m}\) and \(_{m}\) using gradient descent with Adam  (see Appendix C for details). The time warping parameter \(_{w}\) is fixed in our experiments to the empirical mean ISI, and the hyperparameter \(K\) is fixed and chosen in advance (see also subsection on automatic relevance determination below).

Automatic relevance determinationThe Bayesian framework with GPs enables us to perform automatic relevance determination (ARD) over the input dimensions to automatically select relevant input [37; 80]. Applied to lagging ISI dimensions in our NPNR model, this provides an elegant approach to making a data-driven renewal assumption and generally determining the spike-history dependence of the CIF. We choose to fix \(_{w}\) to the empirical mean ISI as shown in Fig. 1B (rather than learning it) to achieve interpretability of kernel timescales for ARD (Fig. 6A) at a small cost of performance (Fig. 12). For a chosen maximum lag \(K\), there is no need for manual selection of the history interaction window as for GLM spike-history filters, though recent work on nonparametric GLM filters provides a related window size selection procedure . In the spirit of Bayesian models, we choose \(K\) to give a sufficiently high capacity model [40; 80] to be able to flexibly capture history dependence, as seen in panel D of Fig. 3 and Fig. 4.

## 4 Results

All datasets discretize spike trains and input time series at regular intervals of \( t=1\) ms. We use a product kernel for \(k(,^{})\) with periodic kernels for angular dimensions, and squared exponential kernels in other cases. For \(k(,^{})\) and \(k(},}^{})\), we pick a product kernel with \(}{{2}}\) (see Fig. 12 for different kernel choices) and set the maximum ISI lag \(K=3\). For illustration, conditional

Figure 1: **Schematic of our proposed model.** (**A**) Time since last spike \(\) and lagging ISIs \(\) for an observed spike train (top row) alongside covariates \(\). (**B**) Illustration of the time warping procedure. We fix the warping parameter \(_{w}\) to the empirical mean ISI, which leads to more uniform distributions \(\) suitable for a stationary GP kernel. **(C)** Prior samples from the generative model for two values of \(a_{m}\) characterized by the lack and presence of a refractory period. The transformation Eq. 15 links the log CIF (top rows) with conditional ISI distributions (bottom rows).

ISI distributions and corresponding tuning curves are computed by fixing \(_{k}\) to be the mean ISI per neuron. Firing rates are defined as \(1/[]\), since this corresponds to the number of spikes fired per unit time in infinitely large time bins for a renewal process (Eq. 26). GP inducing points were randomly initialized, and for a fair comparison, all models used 8 inducing points for each covariate dimension (including temporal dimensions \(\) and \(\) in the NPNR process). For each experiment, we repeat model fitting with 3 different random seeds and pick the model with the best training likelihood. Further details on experiments are presented in Appendix C.

### Validation on synthetic data

For validating our approach, we generate 1000 s of data using rate-rescaling  mimicking a place cell population of 9 neurons for an animal moving in a 2D square arena, each with a unique rate map and renewal density (Fig. 2A, details in Appendix C). The models applied are baseline Poisson, raised cosine filter conditional Poisson and rate-rescaled gamma processes (Appendix C), and our NPNR process. Note that the rescaled gamma process is within-model class for 3 of the synthetic neurons. Inferred conditional ISI distributions and rate maps of our NPNR process in Fig. 2B are close to ground truth, showing the ability of our model to capture modulated spiking statistics drawn from various parametric families. To assess how well ISI statistics are captured, we apply time-rescaling using the GP posterior mean functions (Appendix A) which we visualize with quantile-quantile (QQ) plots  in Fig. 2C. Again, we see an excellent fit of our model compared to baseline models, indicating that only the NPNR is capable of satisfactorily capturing the empirical ISI statistics. Learned temporal kernel timescales of the NPNR process in Fig. 2B show a clear separation between the time since last spike \(\) dimension (lag 0) and lagging ISI \(\) dimensions (lag \( 1\)) with the dotted relevance boundary at \(l=3\) (dimensionless), as expected for renewal processes.

### Neural data

Now we apply our method to head direction cells in freely moving mice [63; 64] and place cells in rats running along a linear track . We select 33 units from the mouse and 35 units from the rat data, which leads to around \(36\) and \(68\) million data points to fit in the training set, respectively (see Appendix C for preprocessing details and Appendix B.2 on data scaling). Experiments involve fitting to the first half of a dataset (\(\!18\) min. for mouse, \(\!32\) min. for rat), and testing on the second half split into 5 consecutive segments. The split into 5 test folds is used to quantify dataset variability. For

Figure 2: **Validation on synthetic data.****(A)** True rate maps (brighter is higher) defined over a square environment (top) and base renewal densities (bottom) in each column given by gamma (left), log normal (middle) and inverse Gaussian (right) distributions with various shape parameters. Each color corresponds to a separate neuron. **(B)** Posterior mean rate maps (top left) and conditional ISI distribution samples in gray overlaid on true renewal densities at various locations (bottom) for the NPNR process fit to synthetic data. The relevance boundary (dotted line) for kernel timescales (top right) is placed at \(l=3\) (dimensionless). **(C)** QQ-plots of fitted models (each curve is a neuron).

prediction, we evaluate the expected log likelihood summed over all neurons

\[=_{n}_{q(_{n})}[ p(_{n}|_{n})]\] (19)

using Gauss-Hermite quadrature with 50 points (Monte Carlo for renewal processes, see B). To assess goodness-of-fit to ISI statistics, we compute QQ plots as before and apply the Kolmogorov-Smirnov (KS) test, giving a \(p\)-value per neuron that indicates how likely the data came from the model (B). Baselines are the inhomogeneous Poisson (P), rate-rescaled gamma (G) and inverse Gaussian (IG) renewal , raised cosine (RC) filter conditional Poisson  and renewal , and nonparametric (NP) filter conditional Poisson processes  (details in C).

#### 4.2.1 Mouse head direction cell data

We choose the animal head direction as our 1D input covariate \(x\). From the KS test \(p\)-value distribution, we see that our model outperforms all baselines in capturing ISI statistics (Fig. 3A left, higher is better; Fig. 3B, QQ plots closer to diagonal). It performs competitively to conditional Poisson processes in terms of predictive performance (Fig. 3A right), but those models fail to capture ISI statistics. In addition, we note the spiking saturation in some samples of the nonparametric conditional Poisson model (Fig. 3C, purple) due to a known instability . Samples from our model (Fig. 3C, gray) exhibit visually similar spike patterns to the real spike train segment. Furthermore, kernel timescales in Fig. 3D show a sizable fraction of the population is characterized by a non-renewal spiking process.

Neural dispersion regimesFrom Fig. 3E and F, we observe both under- and overdispersion (CV smaller and bigger than one) consistent with a previous study based on spike counts . Estimated instantaneous rates and CVs in Fig. 3E are computed using the conditional ISI distribution evaluated along the time series of covariates in the training data. One can regress instantaneous CV against rate, and the CV-rate \(R^{2}\) shows some cells with near linear relations and some with nonlinear trends that can be captured by a GP. Despite that, many cells still show a low overall \(R^{2}\), implying there is no

Figure 3: **Application to mouse head direction cell data.****(A)** Violin plot of KS \(p\)-values per neuron (left, lines marking quartiles) and test expected log likelihoods with errorbars showing s.e.m. across test folds (right). Larger values in both metrics indicate better model fit to data. **(B)** QQ-plots for various models (each curve is a neuron) identified by color (panel A left). **(C)** Predicted log CIF (middle) for an observed spike train (top) and posterior spike train samples (bottom) conditioned on the same covariates \(_{t}\) for various models identified by color. **(D)**_Left:_ temporal kernel timescales for \(\) (lag 0) and \(_{k}\) dimensions with the relevance boundary at \(l=3\) (dimensionless). _Right:_ histogram of “ISI-order” (1 + largest lag \(k\) for which \(k\) is below the boundary) across neurons. **(E)** Time average of estimated instantaneous rates and CVs from the training data (left) and \(R^{2}\) values of CV-rate regression with a linear and a GP model (right). **(F)** Posterior median and \(95\%\) intervals of tuning curves over head direction for the rate and CV, with posterior ISI distribution samples (right) at dashed locations.

parametric relation. Most neurons increase CV with rate, while some show a slight linear decrease reminiscent of refractory Poisson processes (Eq. 31, see Fig. 7 for CV-rate patterns).

#### 4.2.2 Rat place cell data

In this case, \(\) is 3D consisting of the body position along the track, head direction, and local field potential (LFP) \(\)-phase. Our model significantly outperforms all baselines, having both a better KS test \(p\)-value distribution (Fig. 4A left, higher is better; Fig. 4B, QQ plots closer to diagonal) and predictive performance (Fig. 4A, right). Note that the ELLs for this dataset differ from those shown in Fig. 3A due to the fundamentally different (less predictable) spiking statistics of place cells compared to head direction cells (e.g. due to theta oscillations). As the nonparametric conditional Poisson model introduces nonparametric but _covariate-independent_ variability patterns, these results highlights the importance of modeling _covariate-dependent_ spiking variability. Note that the rate-rescaled renewal processes struggle to fit this data, with test ELLs of inverse Gaussian models below -200 nats/s (Fig. 4A right). Samples from our model (Fig. 4C, gray) show it captures the characteristic bursting nature of the real spike train segment. Kernel timescales in Fig. 4D show most cells are described well by a renewal process, different to mouse data Fig. 4D.

Capturing overdispersionWe see CV values in Fig. 4E higher than the mouse thalamus dataset, consistent with overdispersion of place cell discharge in 2D open field navigation . Similar to the mouse data, the CV-rate \(R^{2}\) again shows there is generally no parametric relation. We also tend to observe larger increases in CV with firing rate compared to mouse data (Fig. 9).

\(\)-modulation and phase precessionSpiking activity modulation during \(\)-cycles  is prominent in rat hippocampus, and is visible here in the log CIF (Fig. 4C). We also see phase precession  in Fig. 4F (top), a classical example where spike timing relative to some rhythm has coding significance . Our model enables one to extract not only spiking intensity but also variability, and shows that variability generally inherits the phase precession pattern (Fig. 4F bottom).

## 5 Discussion

### Limitations and further work

ISI statisticsApart from the coefficient of variation, there are other ISI statistics that characterize spiking dynamics aspects such as bursting or regularity. Of particular interest is the local coefficient of variation , which involves joint statistics of consecutive ISIs \((^{(i)},^{(i-1)})\) that can be computed

Figure 4: **Application to rat hippocampal place cell data.****(A)**-**(E)** Similar to Fig. 3A-E. (**F**) Posterior mean tuning maps over \(x\)-position and \(\)-phase for the rate and CV (left) for left-to-right runs (based on head direction) with posterior ISI distribution samples (right) at marked locations.

from our model (see Appendix D). The same applies to serial correlations , which may provide insights into biophysical details . Furthermore, quantifying the shape of ISI distributions is of interest as it is associated with various properties of the underlying neural circuit dynamics .

Neural correlationsTo capture correlations in multivariate spike train data, direct spike couplings as in GLMs are less suitable for current neural recordings compared to latent variable models due to the sparse sampling of populations by electrodes . Combining the latter alongside observed covariates  with our point process provides a powerful framework for capturing correlations , which can have significant impact on neural coding . To perform goodness-of-fit tests, the Kolmogorov-Smirnov test with time-rescaling can be extended to the multivariate case [30; 92].

### Conclusion and impact

We introduced the Bayesian nonparametric non-renewal (NPNR) process for flexible modeling of variability in neural spike train data. On synthetic renewal process data, NPNR successfully captures spiking statistics and their modulation by covariates, and finds renewal order in the spike-history dependence. When applied to mouse head direction cells and rat hippocampal place cells, NPNR has competitive or improved predictive performance to established baseline models, and is superior in terms of capturing ISI statistics, establishing the importance of capturing covariate-dependent variability. NPNR-based analyses recover known behavioral tuning, while also revealing novel patterns of spiking variability at millisecond timescales that are compatible with count-based studies.

Neural firing rates traditionally characterize most computational functions and information encoded by neurons [16; 17; 33], but recent work on V1 [20; 27; 36; 60] and hippocampal place cells  have started to assign computationally well-defined roles to variability in the context of representing uncertainty. Our method introduced in this paper is a principled tool for empirically characterizing neural spiking variability and its modulation at the timescales of individual spikes, and we hope our model will be useful for revealing new aspects of neural coding. Such findings are foundational to advances in computational and theoretical neuroscience, and may have downstream practical applications in designing and improving algorithms for brain-machine interfaces.