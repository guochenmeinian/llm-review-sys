# DesCo: Learning Object Recognition with Rich Language Descriptions

Liunian Harold Li1   Zi-Yi Dou1   Nanyun Peng   Kai-Wei Chang

University of California, Los Angeles

{liunian.harold.li,zdou,violetpeng,kwchang}@cs.ucla.edu

Equal contribution.

###### Abstract

Recent development in vision-language approaches has instigated a paradigm shift in learning visual recognition models from language supervision. These approaches align objects with language queries (e.g. "a photo of a cat") and thus improve the models' adaptability to novel objects and domains. Recent studies have attempted to query these models with complex language expressions that include specifications of fine-grained details, such as colors, shapes, and relations. However, simply incorporating language descriptions into queries does not guarantee accurate interpretation by the models. In fact, our experiments show that GLIP, a state-of-the-art vision-language model for object detection, often disregards contextual information in the language descriptions and instead relies heavily on detecting objects solely by their names. To tackle the challenge, we propose a new description-conditioned (DesCo) paradigm of learning object recognition models with rich language descriptions consisting of two innovations: 1) we employ a large language model as a commonsense knowledge engine to generate rich language descriptions of objects; 2) we design context-sensitive queries to improve the model's ability in deciphering intricate nuances embedded within descriptions and enforce the model to focus on context rather than object names alone. On two novel object detection benchmarks, LVIS and OminiLabel, under the zero-shot detection setting, our approach achieves 34.8 APr minival (+9.1) and 29.3 AP (+3.6), respectively, surpassing the prior state-of-the-art models, GLIP and FIBER, by a large margin.

## 1 Introduction

Training visual recognition models to classify or detect objects with a fixed set of pre-defined categories has been the convention for a long time. However, models trained using this approach often encounter difficulties when adapting to unfamiliar concepts and domains. Recently, there has been a paradigm shift towards _training visual recognition models with language supervision_, using a contrastive objective on a large amount of image-text data containing a diverse range of visual concepts. These models can then be transferred to downstream tasks via language queries. For example, CLIP  can perform image classification using a template query such as "a photo of {class name}"; GLIP  can perform object detection by querying the model with "Detect: person, cat, dog \(\)".

Early applications of these models typically utilize simple language queries that consist of object names. However, language queries can convey much richer and more comprehensive information, such as object attributes, shapes, textures, and relations. These pieces of information can be especially useful for identifying novel visual concepts that do not appear in the training corpus or specifyingspecific needs. For example, the concept of "mallet" can be described as "a kind of tool, wooden handle with a round head" (Figure 1, bottom-left). This decomposes object recognition into recognizing fine-grained details (such as attributes, sub-parts, shapes, etc.) and aligning them to the descriptions. Several studies [25; 41; 31] have explored the idea of guiding language-based recognition models using such descriptive prompts. However, few existing models take complex queries into account during training. As a result, current models often struggle with recognizing intricate object names, attributes, and relations described in natural language sentences [53; 43] (see examples in Figure 1).

In this paper, we develop a vision-language model capable of leveraging description-rich language queries to perform object detection. This work aligns with the recent surge of interest in **instruction/prompt-aware** vision-language models (see a discussion in Section 2). Our goal is to equip VLMs with the ability to comprehend complex language input describing visual concepts, similar to the capability of large language models. We specifically study instruction/prompts in the form of descriptive queries. We focus on object detection, as it requires fine-grained recognition and is more challenging than image-level tasks. However, our method can be generalized to other vision tasks such as classification and segmentation .

We identify two major challenges that prevent existing models from efficiently utilizing rich language descriptions: (1) Fine-grained descriptions are rare in image-caption data used for training current VLMs2. This resembles the reporting bias phenomenon : when writing captions for images, humans tend to directly mention the entities rather than give a detailed description. (2) Even when provided with data rich in descriptions, models often lack the incentive to leverage these descriptions effectively. The main training objective is to align positive phrases with relevant regions while suppressing negative phrases. However, if positive/negative phrases can be distinguished without

Figure 1: Comparison between our model (DesCo-GLIP) and the baseline (GLIP ). Each image is paired with a positive query for target object and a negative query for confusable object. A successful model should locate the target object and ignore the confusable object in the image based on fine-grained specifications for shapes, subparts, relations, etc. We highlight the descriptions that match and do not match to the queried object in blue and red, respectively. Results show that our model can successfully localize the target object and suppresses the negative query even for the difficult cases when the object name is not in the query or the object.

descriptions, the training mechanism fails to incentivize the model to use the provided description. For example, a positive phrase like "A toy bear holding a _mallet_, which has a wooden handle with a round head," and a negative phrase like "A toy bear holding an _ax_, which has a long handle and a sharp blade," can be differentiated based solely on the words _mallet_ and _ax_. This issue resembles the issue discovered by , where vision-language models ignore word order and treat a query as a "bag-of-words" due to insufficient incentives from the contrastive objective. In addition, current models suffer severe hallucination when given natural language queries (in contrast to "template-like" queries) due to shortcuts introduced in training query formulation. This can be seen in the bottom-right picture of Figure 1, where GLIP hallucinates and predicts multiple wrong boxes for "microphone" while "microphone" does not appear in the image.

Based on the observations, we present a **Des**cription-**Conditioned (**DesCo**) paradigm of **learning object recognition models from language descriptions** based on two synergistic ideas:

(1) **Generating descriptions with large language models.** Instead of learning from raw image-caption pairs, we use large language models as a world knowledge base and generate detailed descriptions based on the original caption. We prompt GPT-3  with "What features should object detection models focus on for {an entity in the caption}?". This serves as a scalable approach to transfer the image-caption data into image-description data.

(2) **Context-sensitive query construction.** As discussed, even if we provide descriptions during pre-training, models can still ignore the language context. Our solution is to create a "context-sensitive query", which is a set of positive and negative phrases that can only be distinguished by reading the descriptions (Figure 2). We explore two strategies: 1) constructing "Winograd-like" [13; 43] queries by using large language models to generate confusable object descriptions and captions and 2) generalizing the original grounding task to allow full-negative queries, reducing hallucination.

We apply our approach to fine-tune two state-of-the-art language-conditioned object detection models GLIP  and FIBER . We use the same raw training data as the baselines but convert the data into description-rich queries. We evaluate our methods in two settings. (1) Zero-shot generalization to novel categories (LVIS ), where we use GPT-3 to generate descriptions given class names. DesCo-GLIP (Tiny) improves upon GLIP (Tiny) by 10.0 AP, even outperforming the larger GLIP (Large); DesCo-FIBER improves upon FIBER by 9.1 AP. (2) Zero-shot generalization to natural descriptions given by humans (OmniLabel ). DesCo-GLIP and DesCo-FIBER improve upon the baselines by 4.5 AP and 3.6 AP, setting a new state-of-the-art performance level. Code is available at https://github.com/liumian-harold-li/DesCo.

## 2 Related work

**Language-based visual recognition models.** Visual recognition models are typically trained to make predictions based on a fixed set of classes [20; 5; 26; 39; 33; 58]. The trained models are hard to generalize to open-domain settings. Recent studies have developed visual recognition models that take into account language queries, i.e. language-based recognition. This line of research can be traced back to early work of generalizing image classification  and object detection  models with word embeddings. Recently, CLIP  reformulates image classification as image-text matching and pre-trains models on large-scale image-caption pairs to learn transferrable representations. They demonstrate strong zero-shot performance on various classification tasks. Recent work has applied the technique to fine-grained recognition tasks, such as object detection [17; 10; 25; 57; 56; 3; 32; 7; 28], and segmentation [21; 9; 16; 47; 54; 24]. These works either use pure image-text data as supervision , or reformulate labeled data into image-text data , or pseudo labels image-text data with fine-grained labels . Orthogonal to architecture design or scaling-up, which is the focus of many prior studies, this paper points out that the vanilla way of using image-text data is insufficient and studies how to train these models to take more flexible and informative language queries.

Vision-language models with complex prompts.As vision recognition models become language-aware and language models become vision-aware [45; 60; 22], there is a growing interest in studying whether these models can take complex language prompts, such as task instructions (e.g., GPV [12; 18], SEEM , VisionLLM ), descriptions , or even dialogues (e.g., LLaVa ). We study specifically descriptive prompts, which are especially useful for generalizing to novel categories and customized detection needs; a model that can understand descriptive prompts can also serve as the backbone for supporting aforementioned other types of prompts. Similar to our work, K-LITE  proposes to retrieve knowledge for a visual concept using external knowledge bases, then use the enriched concepts for image classification or object detection; similar techniques have also been proposed by [31; 49; 48]. DetCLIP  builds a large-scale concept dictionary, based on which they provide definitions from WordNet. Different from these studies, our methods show that simply presenting the descriptions at training or inference time is not enough; we propose a simple technique to force models to focus on the provided descriptions (Section 3.2.2). Our work relates to a line of work that seek to reveal and fix failure patterns of image-text matching models (e.g., CLIP) by creating hard negative examples [43; 53; 8; 37].

## 3 Approach

In this section, we first briefly introduce language-based object detection models, then illustrate the details of our proposed approach.

### Background

We give an introduction to _language-based_ object detection models [17; 25; 7], which take a language query and an image as inputs, and predict bounding boxes and their alignment to phrases in the language query. In the following, we use GLIP as an example.

Baseline: Grounded Language-Image Pre-training (GLIP).At the center of these approaches is "reformulating any task-specific fixed-vocab classification problem as a task-agnostic open-vocabulary vision-language matching problem" . The best example is CLIP which reformulates image classification as image-text matching. Similarly, GLIP unifies training data into a _grounding_ format: \((I,Q,B,T)\). \(I\) is the image; \(Q\) is the text query; \(B R^{N 4}\) is the bounding boxes; \(T\{0,1\}^{N K}\) indicates the ground-truth alignment label between the \(N\) bounding boxes and \(K\) tokens in the query. The key is how to formulate the _query_ with data from two kinds of sources:

* _Detection data._ For object detection data such as Objects365 , the query is the concatenation as a list of object classes, such as "Detect: person. bicycle. car. \(\), toothbrush". Note that negative object classes are included in the query; this makes such query-based detection models similar to classical detection models.
* _Grounding data._ Typically, \(Q\) is an image caption, containing entities that can are aligned to annotated object regions . For example, "A toy bear holding a mallet" is the caption; "toy bear" and "mallet" are the "groundable" entities. For densely annotated grounding data (multiple captions for one image) , we can concatenate multiple captions into a longer query. Image-caption data (without annotated boxes) can be transferred into grounding data via pseudo labeling with a grounding model .

Given \(I\) and \(Q\), we compute the alignment scores \(S_{}\) between image regions and words in the query:

\[O,L\!=\!(I,Q),\ S_{}\!=\!OL^{},=\ loss(S_{},T)+_{}\]

where \(L^{K d}\) is the contextual token features and \(O^{N^{} d}\) are the regions features. Enc is a vision and language encoder that takes both image and text as inputs and fuses their representations. The training loss contains the region-word matching loss and a localization loss \(_{}\))as in conventional object detection models.

Inference with language query.At inference time, the model can be used to locate entities/class names appearing in the query. One could simply provide a list of candidate object names (as in the detection data training format).  also show the promise of using descriptions for generalization to novel concepts; however, we show that while GLIP can be influenced by the description, it does not always take the details in the description into account.

### Learning with language descriptions

To train object recognition models that fully utilize language descriptions, we propose to generate descriptions with large language models and construct context-sensitive queries during training. The following subsections provide further details.

#### 3.2.1 Description generation with large language models

Fine-grained descriptions could be scarce in image-caption data due to reporting bias. While this problem can be alleviated by scaling up the pre-training corpus, we show that large language models [6; 2] can be used to effectively generate the descriptions and thus enrich our training corpus.

We leverage a large language model to transform a query \(Q\) into a description-rich query \((Q)\). In this work, we only focus on generating descriptions for entities mentioned in the original query. We construct a vocabulary consisting of 10K entities appearing frequently in the pre-training corpus. For each entity, we prompt a large language model: what features should object detection models focus on for {entity}? We find that large language models give high-quality responses (see examples in Figure 1 and Figure 4). The specific prompts are included in the appendix.

#### 3.2.2 Context-sensitive query construction

Can we simply add the description-rich data to the pre-training corpus? An intuitive idea is to append the description to the original entity to form a training prompt (e.g., "Detect: mallet. bear\(\)" "Detect: mallet, a kind of tool, wooden handle \(\) bear, a kind of animal, \(\)"). However, we find that models naively trained with these prompts still do not exhibit "context-sensitivity", i.e., they make predictions solely based on the entity names while ignoring other contexts (see Section 4.1 for quantitative analysis). As a result, we observe no evident benefit in incorporating descriptions during inference (Table 3). In the following, we elaborate on why the model learns to ignore the descriptions and propose two solutions.

Model learn statistical shortcuts.We first illustrate that without careful design, the model could learn two statistical shortcuts that make them insensitive to descriptive queries.

(1) _Entity shortcut._ The model is trained to align the entities in the query to image regions (this includes predicting "no-alignment" for entities not appearing in the image). Intuitively, if the alignment can be predicted without relying on the context information in the query, then the model is not incentivized to focus on the context information. Figure 2 illustrates this issue with an example. The left side shows the training data of GLIP, where the top query ("Detect: Mallet. Bear. Cat...") comes from detection data and the bottom query ("A toy bear holding a mallet.") comes from grounding data. The problem with such queries is that they can be grounded by only focusing on the entity names and ignoring the context. We denote the gold alignment label of regions as \(T\), the entities in the query as \(E\), and the non-entity part (context) of the query as \(C\). The mutual information

Figure 2: Given the original training data of GLIP, we transform it to be description-rich and context-sensitive by: 1) generating descriptions for entities and composing each of them with confusable object descriptions; 2) generating negative captions. We visualize the gold alignment labels (ground truth) between tokens and regions for the new data. Notably, words such as _tools_ are assigned both positive (blue block) and negative (red block) labels in alignment with the corresponding object depending on the context of the query. As such, the model requires understanding the description in order to make the correct prediction.

\(I(T;C|E,I)\) between \(C\) and \(T\) given \(E\) and the image \(I\) would be low. That is, the non-entity parts of the queries do not affect the label of the region. Training models on such data will not encourage the model to focus on the descriptions as they provide no additional information. This is similar to the "memorization overfit" issue observed in : the model can simply choose to "memorize" the alignment between the entities and regions.

(2) _Format shortcut (hallucination)._ Popularized by GLIP , a line of work adopts a unified view of phrase grounding and object detection: detection can be seen as language-context-free grounding while grounding can be seen as language-context-dependent detection. However, this unification is still _imperfect_: phrase grounding (or referring expression ) traditionally only concerns locating entities in a caption that always exist in the image; thus the model learns to always treat the natural-language queries (in contrast to the template-like queries) as positive and tries to ground every entity mentioned in the sentence. This will result in failure examples as illustrated in the bottom-right picture of Figure 1. Such "hallucination" can be commonly seen on models trained on language grounding data ; these models are almost incapable of distinguishing positive and negative "natural-language-like" queries.

Constructing context-sensitive queries.This motivates our solution of creating queries that are hard to solve without context (Figure 2 and Figure 3). We explore two strategies in this study.

(1) We construct training queries similar to the Winograd format. For example, when training on detection data, instead of "Detect: millet, a kind of tool, \(\)", we remove the entity name "mallet" from the query and sample another description of a "confusable" entity that is also a kind of tool. Pairing the descriptions of the two "confusable" entities creates a strong supervision signal (the middle example in Figure 2): the alignment label (0 or 1) of the word "tool" now depends on its context. The confusable entities are obtained by prompting the large language models as well. Similarly, for training on grounding data, we prompt language models to generate confusable (hard negative) captions that differ from the original captions only by a few words (the example on the right in Figure 2). Note that the label of the word "mallet" is now affected by the context: the _first_ "mallet" is assigned 0 as the caption ("A polar bear holding a mallet") is negative. Mixing in such hard negative captions encourages the model to focus on the context surrounding the entities, such as relations to other entities. To make the confusable caption generation process scalable for image-caption data, we first perform in-context inference and prompt GPT-3 to generate around 50K negative captions based on positive captions; then we distill this knowledge to the open-sourced LLaMA-7B  model that is instruction-finetuned using low-rank adaptation3 and perform inference on large-scale image-caption data.

(2) To resolve the hallucination issue, we generalize the original grounding task: instead of always feeding the model a query that contains at least one description/caption matching the image, we allow the query contain only negative descriptions/captions (Figure 3). Thus, the model cannot blindly ground all entities mentioned in the query; implicitly, it needs to perform image-text matching  as well as phrase grounding. This was partly done in GLIP (see Appendix C.1 in the original GLIP paper), but the query still contains at least one positive entity.

Overview.In Figure 3, we summarize the overall data construction algorithm. **Algorithm 1**: \(B R^{N 4}\) are the bounding boxes of an image; \(E\) are \(M\) positive objects (entities) that appear in the image; \(V\) are the descriptions of all candidate objects in a pre-defined vocabulary; \(T\{0,1\}^{N M}\) denotes the gold alignment between boxes and entities. We first prompt LLM to generate descriptions for the positive entities and propose confusable entities and descriptions; the prompt is included in the appendix (Line 3). The original entities are removed from the descriptions with \(p_{}=0.5\) (DropEntity, Line 4-5). Random negative descriptions from the vocabulary are added to the candicate description set (Line 7). We then randomly subsample the descriptions and concatenate them to form a final query \(q^{*}\); this is because the total length of all the candidate descriptions is too large (Line 8). Boxes and the mapping relations between boxes and tokens are accordingly adjusted (Line 9). **Algorithm 2**: \(C\) is the original caption and \(E\) are \(M\) positive phrases we extracted from the caption. The last two lines of both algorithms are important: after _SubSampleConcat_, it is very likely that some positive sub-queries are dropped from \(Q\); then _LabelAssign_ would drop boxes that are mapped to the dropped sub-queries. The output \(B\) could end with fewer boxes or even no boxes. This is different from the strategy in GLIP or traditional object detection training recipe, where we strive to keep all boxes provided.

## 4 Experiment

In this section, we first investigate whether current models (GLIP) can utilize language descriptions out-of-the-box; then we show that our method allows the model to utilize language descriptions and improves performance on LVIS and OmniLabel significantly.

### Can language-conditioned detection models utilize language descriptions?

As a proof of concept, we first show the GLIP struggles to utilize language descriptions out of the box and analyze the failure patterns.

GLIP does not effectively utilize language descriptions.We make an attempt at using descriptions to transfer GLIP to LVIS , which contains over 1,200 classes. The process is similar to that of . For each category, we prompt a large language model (GPT-3) to give details descriptions (as in Section 3) We append the description to the original class name to form a new query. An example of the queries can be seen shown in Figure 1 (bottom row). Directly appending the description to the object name at inference time only degrades the performance: GLIP-T achieves 20.8 AP on rare categories while appending the descriptions makes the performance drop to 12.2 AP. This is likely due to model hallucination on natural-language-like queries.

GLIP is insensitive to context changes.Examining the model predictions, we find that the model not only does not utilize language descriptions; it ignores the descriptions and tends to only focus on entity names, as we hypothesized. To quantitatively verify the phenomenon, we introduce a _context-sensitivity_ test, inspired by the WinoGround  benchmark. For each image, we provide the model with a positive query \(q^{+}\) describing an object that appears in the image and a negative query \(q^{-}\) describing a confusable object. The original object names are removed from the query. An example of the test is shown in Figure 1 (bottom left), where the model is challenged to distinguish "mallet" and "ax". \(q^{+}\) and \(q^{-}\) describe objects from the same general category (e.g., both are "a kind of tool") while differing in other aspects, similar to the Winograd test.

Intuitively, if a model can effectively utilize the descriptions, it should exhibit two properties: 1) it should give higher alignment scores to entities in \(q^{+}\) compared to \(q^{-}\); 2) even if the model cannot "guess" the hidden entity, at least, the model predictions should change drastically when given two different descriptions. We thus introduce two metrics. 1) AP, which measures how accurate

 Model & \(\)Box & \(\)Conf & AP \\  GLIP  & 0.291 & 0.05 & 4.7 \\ DesCo-GLIP & **0.381** & **0.11** & **12.4** \\ 

Table 1: GLIP is insensitive to context changes compared to DesCo-GLIP.

Figure 3: Algorithms for generating queries from detection data and grounding data.

the model's predictions are. 2) \(\)Box and \(\)Conf, which are the differences between the model's predictions for \(q^{+}\) and \(q^{-}\). \(\)Box measures the changes in box coordinates while \(\)Conf measures the changes in alignment scores of boxes. Details of the metrics are in the appendix.

We find that the baseline model not only cannot identify the correct description (low AP); but it effectively ignores the language context (low \(\)Box and \(\)Conf) (Table 1). On average, the confidence of the predicted boxes changes only \(0.05\) between \(q^{+}\) and \(q^{-}\). One could see the examples in Figure 1. GLIP models make almost identical predictions for two different queries. Such insensitivity to language context makes it infeasible and unreliable to use descriptions to control model predictions.

### Setup

In this section, we apply our approach to two vision-language object detection models GLIP  and FIBER .

Models.The visual backbon of GLIP and FIBER is Swin Transformer  and the text backbones are BERT  for GLIP and RoBERTa  for FIBER. Both models use Dynamic Head  as the detection architecture. Built upon the two models, we introduce two model variants: **DesCo-GLIP** and **DesCo-FIBER**.

Datasets.Following GLIP , we train the models on 1) O365 (Objects365 ), consisting of 0.66M images and 365 categories; 2) GoldG that is curated by MDETR  and contains 0.8M human-annotated images sourced from Flickr30k , Visual Genome , and GQA ; 3) CC3M : the web-scraped Conceptual Captions dataset with the same pseudo-boxes used by GLIP. We down-sample CC3M to around 1.4M images to save training costs, based on whether high-confidence boxes exist in the image. As illustrated in Section 3, we convert the text caption of each instance into a detailed language description to construct description-rich data.

To evaluate how well the models generalize to novel concepts, we perform a zero-shot evaluation on the LVIS  and OmniLabel  datasets. LVIS is a popular dataset that has over 1,200 object categories with a challenging long tail of rare objects; OmniLabel is recently proposed and focuses on object detection with diverse and complex object descriptions in a naturally open-vocabulary setting. For evaluation on LVIS, for each category, we append the GPT-3 generated description to the category name; we group several descriptions into one query to save inference time. More details on the evaluation are in the appendix. For OmniLabel evaluation, we follow the original evaluation protocol without modifications. We also verify that the models still possess the ability to perform the conventional detection and grounding tasks as GLIP and FIBER, on COCO  and Flickr30K . The evaluation results are in the appendix.

 Model &  &  &  \\  & & APr & APc & APf & AP & APc & APd & APd-P \\  MDETR  & RN101 & 20.9 & 24.9 & 24.3 & 24.2 & - & - & 4.7 & 9.1 \\ MaskRCNN  & RN101 & 26.3 & 34.0 & 33.9 & 33.3 & - & - & - & - \\  RegionCLIP  & ResNet-50 & - & - & - & - & 2.7 & 2.7 & 2.6 & 3.2 \\ Detic  & Swin-B & - & - & - & - & 8.0 & 15.6 & 5.4 & 8.0 \\ K-LITE  & Swin-T & 14.8 & 18.6 & 24.8 & 21.3 & - & - & - & - \\ GroundingDINO-T  & Swin-T & 18.1 & 23.3 & 32.7 & 27.4 & - & - & - & - \\ GroundingDINO-L  & Swin-L & 22.2 & 30.7 & 38.8 & 33.9 & - & - & - & - \\ GLIP-L  & Swin-L & 28.2 & 34.3 & 41.5 & 37.3 & 25.8 & 32.9 & 21.2 & 33.2 \\  GLIP-T  & Swin-T & 20.8 & 21.4 & 31.0 & 26.0 & 19.3 & 23.6 & 16.4 & 25.8 \\ DesCo-GLIP & Swin-T & **30.8** & **30.5** & **39.0** & **34.6** & **23.8** & **27.4** & **21.0** & **30.4** \\  FIBER-B  & Swin-B & 25.7 & 29.0 & 39.5 & 33.8 & 25.7 & 30.3 & 22.3 & 34.8 \\ DesCo-FIBER & Swin-B & **34.8** & **35.5** & **43.9** & **39.5** & **29.3** & **31.6** & **27.3** & **37.7** \\ 

Table 2: Zero-shot transfer to LVIS and OmniLabel. Numbers that are grayed out are supervised models. DesCo-GLIP and GLIP-T are directly comparable; DesCo-FIBER and FIBER-B are directly comparable; the rest are listed for reference and not directly comparable.

Implementation details.We initialize DesCo-GLIP from the GLIP-T checkpoint and DesCo-FIBER from the FIBER-B checkpoint. We fine-tune the models on both the original data and the new description-rich data. For DesCo-GLIP, we fine-tune with a batch size of \(16\) and a learning rate of \(5 10^{-5}\) for \(300\)K steps; for DesCo-FIBER, we fine-tune with a batch size of \(8\) and a learning rate of \(1 10^{-5}\) for \(200\)K steps. Experiments can be replicated with 8 GPUs each with 32GB memories.

### Zero-shot transfer to LVIS and OmniLabel

LVIS.Our method shows notable improvements over the baselines on the LVIS MiniVal dataset (Table 2). The improvement is particularly prominent for rare object categories (APr), with an increase of 10.0 for GLIP and 9.1 for FIBER. Results on the Val 1.0 set are in the Appendix.

OmniLabel.Our method also shows improvements over baselines on the OmniLabel dataset (Table 2). OmniLabel assesses model performance using plain categories (APc), free-form descriptions (APd), and positive descriptions (APd-P). Because our models are trained with description data, they naturally excel in supporting such queries, leading to substantial increases in APd and APd-P compared to the baselines. Specifically, DesCo-GLIP achieves a notable improvement of +4.6, while DesCo-FIBER achieves an even more impressive improvement of +5.0. Furthermore, our model's effectiveness extends beyond free-form descriptions to plain categories as well, as illustrated in the table. This highlights the robustness of our method across different evaluation settings and its ability to achieve improvements in various types of queries. Our method wins the 1st place in the Omnilabel challenge 2023 on all three tracks (see Appendix for details).

### Ablation study

In this section, all ablation models are initialized from GLIP-T and trained for \(100\)K steps.

Directly appending descriptions.We examine the impact of directly adding language descriptions to text queries, without incorporating context-sensitive query construction. The results are presented in Row 2 of Table 3. The performance on rare categories (APr) sees no improvement but decreases. To further evaluate the sensitivity of the model to contextual changes, we conduct the same context sensitivity analysis as the one described in Section 4.1. The context sensitivity of the model almost remains unchanged (Row 1-2): \(\)Box changes only \(0.002\) and \(\)Conf changes only \(0.01\). The results indicate that the model remains as insensitive to context changes as the baseline model. This suggests that the model struggles to accurately interpret and effectively utilize the provided language descriptions when context-sensitive query construction is removed.

Dropping the entity name.As in Section 3.2.2, we hypothesize that randomly removing the entity name can force the models to concentrate on the contextual information. Remarkably, the results presented in Table 3 (Row 2-3) demonstrate that this simple and intuitive approach proves to be highly effective. It significantly enhances the model's contextual sensitivity while concurrently improving object detection performance.

Negative captions.We also investigate the effectiveness of using language models to generate hard negative

 GPT & APr & APc & APf & AP \\  ada & 19.9 & 23.2 & 33.7 & 28.0 \\ babbage & 24.2 & 26.7 & 36.5 & 31.3 \\ curie & 24.7 & 28.4 & 38.2 & 32.8 \\ davinci & 30.8 & 30.5 & 39.0 & 34.6 \\ 

Table 4: Detection performance improves when language model size grows.

  &  &  &  \\  &  &  &  &  &  &  &  &  &  & \(\)Box & \(\)Conf & AP \\ 
1 & GLIP-T & 20.8 & 21.4 & 31.0 & 26.0 & 18.7 & 45.7 & 11.7 & 31.2 & 0.291 & 0.05 & 4.7 \\
2 & + Description w/ Entity Name & 20.5 & 23.9 & 35.5 & 29.2 & 23.6 & 47.4 & 14.7 & 36.0 & 0.293 & 0.06 & 5.7 \\
3 & + Description w/o Entity Name & 25.6 & 25.9 & **35.9** & 30.7 & 24.0 & 46.8 & 16.0 & **37.0** & **0.382** & 0.10 & **10.7** \\
4 & + Description w/o Name + Neg Cap & **26.5** & **27.1** & 35.8 & **31.3** & **24.7** & **48.2** & **16.6** & 36.2 & 0.381 & **0.10** & 10.5 \\ 

Table 3: Ablation study. Directly appending the description does not improve performance on rare categories (Row 1 v.s. Row 2, LVIS APr). Constructing context-sensitive queries is crucial.

captions. As shown in Row 4 of Table 3, including negative captions can improve the model detection performance across datasets while preserving its robust contextual comprehension. These results indicate that this technique effectively enhances the model's ability to grasp the subtleties embedded in the given language descriptions.

Language description quality.We explore the effect of the language model size on detection performance. We evaluate the pre-trained DesCo-GLIP on LVIS with descriptions generated from the GPT families4. As presented in Table 4, higher-quality language models improve object detection performance. This finding highlights the importance of employing strong language models, as they possess the ability to embed valuable visual information through extensive pre-training. We showcase two examples in Figure 4.

## 5 Conclusion and limitations

In this study, we introduced a new paradigm of learning object detection models from language supervision. We show that large language models can be used to generate rich descriptions and the necessity to construct context-sensitive queries. We hope that our method sheds light on empowering vision models with the ability to accept flexible language queries.

While we greatly improve the models' ability to understand flexible language queries, our method has several limitations that can be addressed in future work. 1) We use a large language model to automatically generate the descriptions, which inevitably introduces noise as not all generated descriptions are accurate or beneficial for representation learning. Future work could consider automatically selecting useful descriptions sampled from the language model, similar to . 2) The format of the descriptions we explored is still limited (e.g., "{entity}, a kind of {type}, {list of simple features}"); it might be useful to consider more diverse descriptions by prompting the language model with more diverse prompts. 3) Similar to large language models, querying our model also requires a certain amount of prompt engineering. Future work could explore how to make the model more robust to different kinds of queries.