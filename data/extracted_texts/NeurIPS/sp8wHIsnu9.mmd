# Synthesize, Partition, then Adapt:

Eliciting Diverse Samples from Foundation Models

Yeming Wen'& Swarat Chaudhuri

Department of Computer Science

The University of Texas at Austin

ywen@utexas.edu

###### Abstract

Presenting users with diverse responses from foundation models is crucial for enhancing user experience and accommodating varying preferences. However, generating multiple high-quality and diverse responses without sacrificing accuracy remains a challenge, especially when using greedy sampling. In this work, we propose a novel framework, Synthesize-Partition-Adapt (SPA), that leverages the abundant synthetic data available in many domains to elicit diverse responses from foundation models. By leveraging signal provided by data attribution methods such as influence function, SPA partitions data into subsets, each targeting unique aspects of the data, and trains multiple model adaptations optimized for these subsets. Experimental results demonstrate the effectiveness of our approach in diversifying foundation model responses while maintaining high quality, show-cased through the HumanEval and MBPP tasks in the code generation domain and several tasks in the natural language understanding domain, highlighting its potential to enrich user experience across various applications.

## 1 Introduction

Transformer-based foundation models have revolutionized the fields of natural language processing (NLP) and code generation with their remarkable abilities a wide range of understanding and generation tasks (Vaswani et al., 2017; Devlin et al., 2019; Brown et al., 2020; Chen et al., 2021). These models are typically pre-trained on vast amounts of text data and then undergo instruction fine-tuning -- a post-training process -- to improve alignment with user expectations and enhance the overall user experience (Ouyang et al., 2022). Due to the high cost of human-annotated data, synthetically generated datasets (Wang et al., 2022b) such as OSS-Instruct (Wei et al., 2023) and Alpaca (Taori et al., 2023) have become an important component of instruction tuning, demonstrating strong effectiveness in improving foundation model performance.

To date, these synthetic datasets have been primarily used to align foundation models with instructions or to induce certain preferable behaviors. In this paper, we focus on a different use of synthetic data: in improving the _diversity_ of foundation models' outputs. Diversifying the generated responses is crucial for accommodating diverse user preferences and enhancing user satisfaction. Consider the scenario illustrated in Fig. 1, where a user prompts a foundation model with "Give me a personal website template". In this case, we would prefer the model to generate two diverse templates while

Figure 1: A user is expecting two diverse templates from the foundation model.

maintaining good quality, providing users with a variety of styles and layouts. Conventional methods for improving diversity, such as temperature sampling (Ackley et al., 1985; Hinton et al., 2015; Wang et al., 2019, 2023), rely on sampling techniques that anneal the probabilistic distribution of outputs. These methods often trade off diversity for quality, as the generated responses may deviate from the learned distribution and produce hallucination or less coherent outputs (Lee, 2023). Moreover, these techniques are not applicable when using greedy sampling, which is often preferred for its simplicity and precision. This highlights the need for approaches that not only align foundation model outputs with user expectations but also elicit diverse responses without sacrificing quality.

In this paper, we present a framework, _Synthesize-Partition-Adapt_ (SPA), that achieves these objectives. The framework partitions the synthetic data and adapts foundation models to these partitions in the post-training stage. By leveraging the inherent diversity in the training data, this approach can generate diverse responses without compromising accuracy. The potential of partition-and-adapt approach is further amplified by the increasing availability of large-scale synthetic datasets because the utility of instruction-tuning a single model on the entire dataset diminishes. In particular, we show that influence function (Koh and Liang, 2017) can be an effective signal to partition synthetic datasets into subsets, each targeting unique aspects that elicit distinct model behaviors. However, SPA is not limited to influence function and can be extended to other partitioning strategies. By training multiple adaptations on these subsets using parameter-efficient fine-tuning techniques, such as LoRA (Hu et al., 2021), we enable the generation of diverse and accurate responses.

To demonstrate the effectiveness of our approach, we conduct experiments on a range of tasks in both the code generation and natural language understanding domains. We evaluate our method on the HumanEval (Chen et al., 2021) and MBPP (Austin et al., 2021) datasets for code generation, as well as several natural language understanding tasks. The results showcase the ability of our approach to diversify model responses while maintaining high accuracy, highlighting its potential to enrich user experience across various applications.

To summarize, the main contributions of this paper are as follows:

* We propose SPA, a novel framework that leverages synthetic data, data partitioning, and model adaptation to elicit diverse responses from foundation models.
* We demonstrate the effectiveness of SPA in diversifying foundation model responses while maintaining sampling quality through extensive experiments on code generation and natural language understanding tasks.
* We highlight the potential of SPA to leverage the increasing availability of large-scale synthetic datasets for improving the diversity of foundation model responses.

## 2 Background

### Instruction Fine-tuning

By fine-tuning foundation models on human-annotated data that demonstrates desired behaviors, instruction tuning aims to improve the alignment between the model's outputs and the user's intentions (Ouyang et al., 2022; Wei et al., 2021; Sanh et al., 2022). Let \(=(x_{i},y_{i})_{i=1}^{N}\) denote a dataset of input-output pairs, where \(x_{i}\) represents the input instruction and \(y_{i}\) represents the corresponding desired output. The objective of instruction tuning is to minimize the following loss function: \(()=-_{i=1}^{N}_{}(y_{i}|x_{i})\) where \(\) represents the parameters of the foundation model, and \(p_{}(y_{i}|x_{i})\) is the probability of generating the target response \(y_{i}\) given the input \(x_{i}\).

Classical approaches for instruction tuning typically require a substantial amount of parallel labeled data of NL intents and gold model responses. Collecting large-scale, high-quality annotated datasets is often time-consuming and expensive. To mitigate this issue, researchers have explored the use of synthetic data for instruction tuning. By leveraging techniques such as data augmentation (Wei and Zou, 2019; Sennrich et al., 2016) and back-translation (Edunov et al., 2018), synthetic data can be generated at scale, providing a cost-effective alternative to human-annotated datasets. Furthermore, synthetic instruction-following data can also be generated from the foundation model itself (Wang et al., 2022; Honovich et al., 2022; Taori et al., 2023; Peng et al., 2023; Wen et al., 2024, _inter alia_).

### Data Attribution and influence function

Data attribution methods aim to quantify the importance or influence of individual training points on a model's predictions. One such method is the influence function (Koh and Liang, 2017). Formally, let \(()\) denote the loss function of the model, where \(\) represents the model parameters. The influence of a training point \(z\) on the model's parameters \(\) is given by \((z)=-H_{}^{-1}_{}(z,)\). where \(H_{}\) is the Hessian matrix of the loss function with respect to the model parameters, and \(_{}(z,)\) is the gradient of the loss function with respect to the model parameters, evaluated at the training point \(z\). Next, the influence of elevating the weight of \(z\) on the loss associated with a test point \(z_{test}\) is:

\[(z,z_{test})=-_{}(z_{test},)^{ }H_{}^{-1}_{}(z,)\] (1)

It is impossible to calculate the full Hessian \(H_{}^{-1}\) matrix in deep neural networks. Koh and Liang (2017) developed a simple and efficient implementation that requires only oracle access to gradients and Hessian-vector products. This implementation makes it feasible to apply influence function to large-scale models. However, the vast parameter space of foundation models presents an even greater challenge, rendering the direct application of influence function impractical. In response to this, recent advancements in Grosse et al. (2023) have further refined the methodology, enabling the application of influence function to large language models.

## 3 Problem Formulation

Given a user input \(\), our goal is to generate a diverse set of high-quality responses \(_{1},_{2},...,_{K}\) from a foundation model \(\). One approach to generating diverse responses is to sample from the model multiple times using techniques like temperature sampling: \(_{k}=(;,),\) where \(k=1,2,...,K\) and \(\) represents the model parameters and \(\) is the temperature hyperparameter. However, this approach often trades off diversity for quality as studied in Chung et al. (2023). An alternative approach is to train multiple model adaptations \(_{1},_{2},...,_{K}\) and sample one response from each adaptation:

\[_{k}=_{k}(;_{k}), k=1,2,...,K,\] (2)

where \(_{k}\) represents the parameters of the \(k\)-th model adaptation. By training each adaptation on a different subset of the data that captures unique aspects and yields distinct model behaviors, we can generate diverse responses while maintaining their quality. Moreover, this approach allows us to elicit diverse samples even with greedy sampling, which is often preferred for maximum precision.

Traditionally, training multiple model adaptations has been considered unfavorable due to the repeated training process, which can be computationally expensive and time-consuming. However, with the increasing popularity of instruction tuning, it has become common practice to go through a post-training stage using instruction data before deploying the model to users. This post-training stage presents an opportunity to train multiple model adaptations without incurring significant additional costs, making the approach more feasible and practical in real-world scenarios.

As the volume of synthetic data grows, the utility of fine-tuning a single model on the entire dataset diminishes due to the diminishing returns in the post-training stage, as demonstrated in Fig. 2. The _pass@_1 accuracy after fine-tuning on the entire synthetic dataset using LoRA is roughly the same as only consuming 15% of the data2. This creates an opportunity to leverage the abundant synthetic data to train multiple model adaptations, each specializing in a specific subset of the data. In this work, we propose the Synthesize, Partition, then Adapt (SPA) framework to address the diverse response generation problem. SPA leverages existing synthetic datasets, data partitioning techniques, and parameter-efficient fine-tuning methods to train multiple model adaptations. By sampling from the collection of these adaptations, SPA generates diverse and high-quality responses, enhancing the overall user experience.

Figure 2: _pass@_1 on HumanEval after fine-tuning on some percentage of OSS-Instruct dataset (Wei et al., 2023) using LoRA. The plot demonstrates the diminishing returns observed with increasing amounts of data used for parameter efficient fine-tuning.

## 4 Partitioning Synthetic Data and Training Adaptations

We present the technical details of our proposed SPA framework for training multiple adaptations. We leverage an existing synthetic dataset \(=\{_{i},_{i}\}_{i=1}^{N}\) for the purpose of this study. The use of an existing synthetic dataset allows us to focus on the effectiveness of the Partition then Adapt steps in eliciting diverse samples, while demonstrating the flexibility of our framework to work with various synthetic datasets. Fig. 3 provides an overview of the framework. After obtaining the synthetic data, our approach consists of three main steps: (1) computing data attribution scores for synthetic data points, (2) partitioning the synthetic dataset based on these scores, and (3) training multiple foundation model adaptations using parameter-efficient fine-tuning techniques like LoRA.

### Computing Data Attribution Scores

Consider a pre-trained foundation model \(\) with parameters \(\). Our goal is to leverage the synthetic dataset \(\) to train a set of \(K\) foundation model adaptations \(\{_{k}\}_{k=1}^{K}\). Each adaptation focuses on a specific subset of the data that yields similar model behaviors. To partition the synthetic dataset, we employ data attribution methods that measure the importance of each training point to the model's predictions. Although we use influence function as an example to label the data, the SPA framework is not limited to influence function and can be extended to other data attribution methods, such as lexical overlap or TRAK (Park et al., 2023). To calculate the influence function, we first fine-tune the pre-trained foundation model \(\) on the synthetic dataset \(\). The fine-tuning process optimizes the model parameters \(\) to minimize the loss function \(()\) on the synthetic dataset using LoRA (Hu et al., 2021): \(()=_{i=1}^{N}(_{i},( _{i};))\), where \((,)\) is a suitable loss function, such as cross-entropy loss for language modeling tasks. This fine-tuning process yields the optimized model parameters \(\).

Next, we select a set of \(M\) test queries \(\{(_{t}^{(m)},_{t}^{(m)})\}_{m=1}^{M}\), which can be a collection of questions requiring various expertise knowledge to solve. For each test query \((_{t}^{(m)},_{t}^{(m)})\), we compute the influence score of each synthetic data point \((_{i},_{i})\) using Eq. (1):

\[((_{i},_{i}),(_{t}^{(m)},_{ t}^{(m)}))=-_{}(_{t}^{(m)},(_{t} ^{(m)};))^{}H_{}^{-1}_{}(_{i},(_{i};)).\] (3)

To efficiently compute the influence scores, we employ the stochastic estimation method proposed by Koh and Liang (2017), which approximates the inverse Hessian-vector product using conjugate

Figure 3: An illustration of the Synthesize, Partition, then Adapt (SPA) framework. SPA partitions synthetic dataset according to data attribution scores, which can be obtained using various methods such as influence function or lexical overlap. Multiple foundation model adaptations are then trained on each subset. Sampling from the collection of these model adaptations can present users with diverse responses. SPA is not limited to a specific attribution method.

gradients. Although even this method is generally infeasible in foundation models due to their vast parameter space, the use of LoRA(Hu et al., 2021) makes it feasible by significantly reducing the number of trainable parameters. The computational cost of estimating the influence of a test query between the entire dataset \(\) is the same as calculating the gradient of \(\). Another option to address this issue is to use the K-FAC approximation of the Hessian, as proposed by Grosse et al. (2023). We focus on the LoRA approach and leave the exploration of K-FAC and other approximations for future work.

### Partitioning Synthetic Dataset

After computing the data attribution scores for each synthetic data point with respect to the \(M\) test points, we obtain an influence matrix \(^{N M}\), where \(_{i,m}\) represents the attribution score of the \(i\)-th synthetic data point for the \(m\)-th test point. To partition the synthetic dataset \(\) into \(K\) subsets \(\{_{k}\}_{k=1}^{K}\), a clustering algorithm can be applied to solve the following objective:

\[_{\{_{k}\}_{k=1}^{K}}_{k=1}^{K}_{(_{i}, _{i})_{k}}_{(_{j},_{j}) _{k}}|_{i,:}-_{j,:}|_{2}^{2},\] (4)

where \(_{i,:}\) denotes the \(i\)-th row of the influence matrix \(\), subject to \(_{k=1}^{K}_{k}=\) and \(_{k}_{k^{}}=\) for all \(k k^{}\). In this work, we assume partitions are disjoint for the simplicity of the study.

The clustering algorithm assigns each synthetic data point \((_{i},_{i})\) to one of the \(K\) subsets based on the similarity of its influence scores across the \(M\) test points. This partitioning ensures that data points within each subset have similar impacts on the model's predictions. The choice of the clustering algorithm may depend on the specific characteristics of the dataset. For simplicity and ease of implementation, in this study, we use a ranking heuristic to partition the synthetic dataset. The details of this heuristic will be explained in the experiment section SS5.1. However, it is important to note that our SPA framework is not limited to any specific clustering algorithm.

### Training Multiple Adaptations with LoRA

Once the synthetic dataset is partitioned into \(K\) subsets, we train a foundation model \(_{k}\) for each subset \(_{k}\) using parameter-efficient fine-tuning techniques like LoRA(Hu et al., 2021). LoRA adapts the pre-trained foundation model parameters \(\) by learning low-rank matrices \(_{k}^{r d}\) and \(_{k}^{d r}\) for each weight matrix \(^{d d}\) in the pre-trained foundation model, where \(r d\) is the rank of the adaptation matrices.

The adapted weight matrix \(_{k}\) for the foundation model adaptation \(_{k}\) is computed as: \(_{k}=+_{k}_{k}\). During the fine-tuning process, only the adaptation matrices \(_{k}\) and \(_{k}\) are learned, while the pre-trained weights \(\) remain frozen. This significantly reduces the number of trainable parameters, making it feasible to train multiple foundation model adaptations with limited computational resources. The training objective for each foundation model adaptation \(_{k}\) is given by \(_{_{k}}_{k}|}_{(_{i},_ {i})_{k}}(_{i},_{k}(_{i}; _{k}))\) where \(_{k}\) represents the parameters of \(_{k}\), which include the pre-trained weights \(\) and the LoRA adaptation matrices \(_{k},_{k}\). By training multiple foundation model adaptations using LoRA, we can efficiently adapt the pre-trained foundation model to different subsets of the synthetic data, each focusing on a specific aspect of the data that yields similar model behaviors. This approach enables the creation of a diverse set of specialized models that capture different knowledge or expertise present in the synthetic data, while leveraging the knowledge acquired during the pre-training phase.

Inference with Multiple AdaptationsDuring inference, given a user input \(\), our goal is to generate a diverse set of responses by leveraging the multiple foundation model adaptations trained on different subsets of the synthetic data. To achieve this, we randomly sample a foundation model adaptation \(_{k}\) from the set of \(K\) adaptations \(\{_{k}\}_{k=1}^{K}\) and generate the output \(\) using the selected adaptation. By randomly sampling from the set of adaptations, we can generate a diverse set of responses for the user input \(\). This approach ensures that the generated responses are not only diverse but also maintain reasonable quality. It is worth noting that this approach is compatible with various sampling techniques, such as temperature scaling, top-k and top-p sampling, which can further enhance the diversity of the generated responses.

To generate multiple diverse responses for the user input \(\), we can repeat the random sampling process multiple times, each time selecting a different adaptation and generating a response. This allows us to present the user with a set of alternative responses that capture different perspectives or styles, enhancing the overall user experience. Unlike temperature sampling, which can degrade the quality of the generated responses, our approach maintains the quality of each response by leveraging the specialized knowledge captured by each adaptation. Moreover, our approach can generate diverse samples even when greedy sampling is used.

## 5 Experiments

In this section, we present the experimental setup and results for evaluating the effectiveness of our proposed SPA framework in improving the diversity of foundation model outputs. We conduct experiments on both code generation tasks such as HumanEval (Chen et al., 2021) and MBPP (Austin et al., 2021) and several natural language understanding tasks.

### Experimental Setup

Base Model and Synthetic DatasetFor the code generation experiments, we use CodeLLaMA 7B (Roziere et al., 2023) as the base foundation model. CodeLLaMA is a state-of-the-art language model specifically designed for code-related tasks, pre-trained on a large corpus of code and natural language data. For the synthetic dataset, we utilize the OSS-Instruct dataset (Wei et al., 2023), which consists of 75,000 code-related question-answering pairs generated by GPT-3.5 Turbo (OpenAI, 2023). In the natural language understanding domain, we employ Llama-2 13B (Touvron et al., 2023) as the base foundation model. Llama-2 is a powerful language model trained on a diverse range of web-scale data, demonstrating strong performance across various natural language understanding tasks. For the synthetic dataset, we use Platypus (Lee et al., 2023), which focuses on improving LLMs' STEM and logic knowledge. Platypus consists of a curated sub-selection of public text datasets, comprising approximately 25,000 question-answer pairs.

Data Attribution ScoresWe compare two methods for computing data attribution scores: influence function and lexical overlap.

For the influence-based method, we hand-write 12 examples that cover a wide range of knowledge for each domain. For each of these examples, we calculate the influence score with respect to each training example in the corresponding synthetic dataset using Equation 3. We then select the top 8 test queries whose distribution of influence scores over the dataset has the highest variance. This ensures that the selected test queries have diverse impacts on the synthetic dataset, capturing different aspects of the domain knowledge. The resulting influence matrices \(_{code}^{8 75,000}\) and \(_{nlu}^{8 25,000}\) are used for partitioning the OSS-Instruct and Platypus datasets, respectively.

For the lexical overlap method, we compute the BM25 score (Robertson et al., 1994) between each training example and the hand-written test queries. The BM25 score is calculated as follows:

\[I(z,z_{query})=_{t z_{query}}}+1)f(z,t)}{k_{1}(1-b)+b}+f(z,t)}+1 \] (5)

where \(f(z,t)\) is the overlap count, \(N\) is the number of training examples, \(L(z)\) is the length of the example, and \(L_{avg}\) is the average example length. We adopted the framework and the hyperparameters in Lv and Zhai (2011). While we focus on influence function in this work, exploring the effectiveness of alternative data attribution methods like BM25 could be an interesting direction for future research. More details are provided in Appendix A.

Partitioning the Synthetic DatasetsTo train multiple foundation model adaptations, we first set the hyperparameter \(K\), which represents the total number of adaptations. We use \(K=8\) for both code generation and natural langauge understanding domain. For each data point in the synthetic dataset, we aim to find the test queries that provides the most influence. Formally, for each synthetic data point \((_{i},_{i})\), we assign it to the subset \(_{k}^{*}\) corresponding to the test point with the highest influence score or the BM25 score: \(k^{*}=_{k\{1,,K\}}_{k,i}\). where \(_{k,i}\) represents either the influence matrix or the BM25 score matrix. This process partitions the OSS-Instruct datasetinto \(K\) groups for code generation and the Platypus dataset into \(K\) groups for natural language understanding. Each group is associated with a specific test example that has the highest influence on the data points within the group.

With the partitioned synthetic dataset, we train \(K\) model adaptations using the LoRA technique, as described in SS4.3. Each adaptation \(_{k}\) is trained on the corresponding subset \(_{k}\) of the synthetic dataset, focusing on the specific coding knowledge captured by the associated test point.

Evaluation MetricsWe use the following two metrics to assess the diversity:

1. Average KL Divergence: Let \(P_{i}\) and \(P_{j}\) be the probability distributions of the generated responses from two model adaptations \(i\) and \(j\), respectively. The KL divergence between \(P_{i}\) and \(P_{j}\) is defined as \(D_{KL}(P_{i} P_{j})=_{x}P_{i}(x)(x)}{P_{j}(x)}\). The average KL divergence is calculated by averaging the pairwise KL divergence between all possible pairs of model adaptations. A higher average KL divergence indicates greater diversity among the model adaptations,

\[=}_{i=1}^{N-1}_{j=i+ 1}^{N}D_{KL}(P_{i} P_{j})\] (6)

2. Sample Diversity: The average KL divergence evaluates the diversity at the distributional level. We also consider the sample diversity which measures the uniqueness of individual responses. We calculate the diversity score among \(K\) randomly generated samples for each problem. The diversity score is defined as the proportion of unique samples within the generated set. Specifically, it is calculated by taking one minus the ratio of the number of duplicate pairs to the total number of generated pairs.

BaselinesWe consider two baselines in the evaluation: (1) **Single Adaptation**, where a single model adaptation is trained on the entire synthetic dataset using LoRA, and (2) **Multiple Adaptations (random)**, where multiple adaptations are trained on randomly partitioned subsets of the synthetic dataset using LoRA. Hyperparameters used to train adaptations are provided in Appendix A.

### Code Generation Results

In the code generation domain, we evaluate the performance of our proposed methodology on two popular code generation benchmarks: HumanEval (Chen et al., 2021) and MBPP (Austin et al., 2021). HumanEval consists of 164 hand-written programming problems with corresponding test cases, while MBPP contains 399 held-out programming problems collected from online resources3. These benchmarks assess the ability to generate functionally correct code.

_pass@\(k\)_**metric** In addition to the diversity metrics, we also evaluate the sample quality by _pass@\(1\)_ and _pass@\(5\)_, measuring the percentage of problems for which at least one of the \(k\) generated samples passes all the test cases. Note that the _pass@\(5\)_ metric has a strong correlation to the diversity of the samples. More diverse samples generally lead to higher _pass@\(5\)_ for \(k>1\).

Tab. 1 presents the evaluation results of our SPA framework and the baselines on the HumanEval and MBPP benchmarks. For the multiple adaptation methods, including random partitioning, lexical

    &  &  \\  & _pass@\(1\)_ & _pass@\(5\)_ & diversity & avg. KL & _pass@\(1\)_ & _pass@\(5\)_ & diversity & avg. KL \\  Single (\(=0.1\)) & 50.02 & 56.42 & 0.58 & NA & 60.15 & 64.16 & 0.53 & NA \\ Random (\(=0\)) & 50.15 & 63.10 & 0.69 & 0.008 & **60.65** & 70.42 & 0.64 & 0.014 \\ Lexical (\(=0\)) & **50.30** & 66.74 & 0.78 & 0.011 & 60.33 & 71.17 & 0.71 & 0.018 \\ Influence (\(=0\)) & 50.15 & **69.05** & **0.85** & **0.017** & 60.46 & **73.68** & **0.78** & **0.020** \\   

Table 1: Results on the HumanEval and MBPP. \(\) denotes the temperature used for sampling. SPA with influence function achieves the best performance in terms of diversity score and avg. KL divergence) while maintaining comparable _pass@\(1\)_ performance to the single adaptation baseline. _pass@\(5\)_ measures sample quality but also has a positive correlation with diversity.

overlap, and influence function, we use greedy decoding (\(=0\)) to generate samples. For the single adaptation baseline, we use a temperature of \(=0.1\) to induce some diversity in the generated samples, as greedy decoding would not produce any diversity in this case.

Our primary focus is on comparing the diversity metrics, namely the Diversity Score and the Average KL Divergence (avg. KL), across the different methods. SPA with influence function achieves the highest Diversity Scores of 85% and 78% on HumanEval and MBPP, respectively, indicating that the generated samples are more unique and diverse compared to the other methods. Similarly, SPA with influence function yields the highest Average KL Divergence of 0.017 and 0.020 on HumanEval and MBPP, demonstrating greater diversity at the distributional level.

The random partitioning and lexical overlap approaches also improve upon the single adaptation baseline in terms of diversity metrics, but to a lesser extent than influence function. In particular, the lexical overlap induces more diversity than the random adaptations baseline. This suggests that even simpler data attribution methods can be beneficial for enhancing diversity when training multiple specialized adaptations. It is worth noting that the _pass@\(5\)_ scores, while primarily measuring sample quality, also have a positive correlation with diversity. SPA with influence function achieves the highest _pass@\(5\)_ scores of 69.05% and 73.68% on HumanEval and MBPP, indicating that the generated samples not only exhibit greater diversity but also maintain high quality.

In summary, these results underscore the effectiveness of our SPA framework in generating diverse code samples without compromising quality. By leveraging influence function for data partitioning and training multiple adaptations using LoRA, SPA enables the generation of diverse and accurate code solutions, even when using greedy decoding. We also showed that training more adaptations than 8 did not lead to more diversity in Appendix B.

Impact of TemperatureFig. 4 presents the impact of temperature on _pass@\(1\)_, _pass@\(5\)_, and Diversity Score for different methods on the HumanEval benchmark. The first plot shows that all methods, including Single, Random, Lexical, and Influence, exhibit similar patterns in terms of _pass@\(1\)_ performance. They achieve maximum accuracy (around 50.2%) when \(=0\) and gradually decrease to approximately 46.5% as the temperature increases to 0.5.

However, both _pass@\(5\)_ and Diversity Score improve for all methods as the temperature increases, which is expected as higher temperatures encourage the model to generate more diverse samples. Notably, SPA with influence function (Influence) maintains its advantage over other methods across all temperature values, outperforming Single, Random, and Lexical methods. Although the performance gap between Influence and other methods narrows as the temperature increases due to the inherent diversity promotion of higher temperatures, Influence still maintains a lead at \(=0.5\).

### Natural Language Understanding Results

To demonstrate the effectiveness of SPA in the natural language understanding domain, we evaluate its performance on several diverse tasks, including Big-Bench Hard (BBH) (Suzgun et al., 2022), GPQA (Rein et al., 2023), MMLU (Hendrycks et al., 2020), and WinoGrande (Sakaguchi et al., 2019). For tasks that involve multiple-choice questions, we asked the model to continue generating text even after producing an answer choice for the purpose of measuring sample diversity. As shown in Fig. 5, SPA with influence function consistently achieves higher diversity scores and average KL divergence compared to the lexical overlap and random adaptation across all tasks. Interestingly,

Figure 4: How sampling temperature affects _pass@\(1\)_, _pass@\(5\)_, and Diversity Score for different methods on the HumanEval benchmark. The results are averaged over 4 checkpoints.

random adaptations achieve better diversity than lexical overlap on the GPQA task, suggesting that the effectiveness of partitioning methods may change depending on the task.

The diversity scores and average KL divergence values vary across tasks, reflecting the inherent differences in the nature and complexity of each task. Tasks like MMLU, which cover a wide range of subjects, tend to yield higher average KL divergence. We also notice that a larger gap in average KL divergence does not necessarily translate to a proportionally greater difference in diversity scores. This suggests that while average KL divergence captures the dissimilarity between the generated sample distributions, it may not always directly correlate with the actual diversity of the samples. Nonetheless, the consistent improvement achieved by SPA with influence function highlights its robustness and adaptability to various natural language understanding challenges.

## 6 Related Work

Sampling-based methods have been widely explored to generate diverse text from language models. One of the most common approaches is temperature sampling (Ackley et al., 1985; Hinton et al., 2015). Several studies have investigated the impact of temperature on model sampling and its effect on the diversity-quality trade-off (Caccia et al., 2018; Renze and Guven, 2024; Wang et al., 2023). Higher temperatures lead to more diverse but potentially less coherent samples, while lower temperatures produce more conservative and deterministic outputs. When using high temperatures, human interventions can help to correct errors during the sampling process (Chung et al., 2023). Dynamic temperature strategies have also been explored during the model training and inference stages (Lin et al., 2018; Zhang et al., 2018; Wang et al., 2019; Chang et al., 2023).

Besides adjusting temperature, top-\(k\), top-\(p\) (nucleus) sampling (Holtzman et al., 2019) and their variants are common sampling methods (Fan et al., 2018; Meister et al., 2022; Hewitt et al., 2022; Ravfogel et al., 2023), which restrict the sampling space or dynamically adjust the number of tokens considered at each step. Another line of works studied how to formulate quality-diversity trade-off as a search or RL problem (Naik et al., 2023; Lim et al., 2024; Mudgal et al., 2023; Bradley et al., 2023; Ji et al., 2023).

## 7 Conclusion

In summary, we proposed SPA, which that leverages synthetic data, data partitioning, and model adaptation to elicit diverse responses from foundation models. By partitioning synthetic datasets into subsets that capture unique aspects of the data and training multiple model adaptations optimized for these subsets, SPA enables the generation of diverse and high-quality responses.

LimitationOne main challenges is the computational cost associated with influence function, which require several extra epochs of backward passes to estimate. Future work could explore more efficient data attribution methods, such as TRAK (Park et al., 2023) and K-FAC (Grosse et al., 2023).

Figure 5: Average KL divergence and diversity score on various natural language understanding tasks. SPA with influence function consistently outperforms the lexical overlap and random adaptations, demonstrating its effectiveness in generating diverse samples across different NLU tasks.

Additionally, the ranking heuristics used to approximate Eq. (4) can be replaced by more advanced clustering algorithms. Additionally, serving multiple LoRA adaptations poses significant computational challenge in real-time serving framework. Recent works such as S-LoRA and FLoRA (Sheng et al., 2023; Wen and Chaudhuri, 2024) can be considered to accommodate this overhead.