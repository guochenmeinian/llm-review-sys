# BadChain: Backdoor Chain-of-Thought Prompting

for Large Language Models

 Zhen Xiang\({}^{1}\), Fengqing Jiang\({}^{2}\), Zidi Xiong\({}^{1}\), Bhaskar Ramasubramanian\({}^{3}\)

**Radha Poovendran\({}^{2}\)**, **Bo Li\({}^{1}\)**

\({}^{1}\)University of Illinois Urbana-Champaign \({}^{2}\)University of Washington

\({}^{3}\)Western Washington University

###### Abstract

Large language models (LLMs) are shown to benefit from chain-of-thought (COT) prompting, particularly when tackling tasks that require systematic reasoning processes. On the other hand, COT prompting also poses new vulnerabilities in the form of backdoor attacks, wherein the model will output unintended malicious content under specific backdoor-triggered conditions during inference. In this paper, we propose BadChain, the first backdoor attack against LLMs employing COT prompting, which does not require access to the training dataset or model parameters. BadChain leverages the inherent reasoning capabilities of LLMs by inserting a _backdoor reasoning step_ into the sequence of reasoning steps of the model output, thereby altering the final response when a backdoor trigger is embedded in the query prompt. In particular, a subset of demonstrations will be manipulated to incorporate the backdoor reasoning step in COT prompting. Consequently, given any query prompt containing the backdoor trigger, the LLM will be misled to output unintended content. Empirically, we show the effectiveness of BadChain against four LLMs (Llama2, GPT-3.5, PaLM2, and GPT-4) on six complex benchmark tasks encompassing arithmetic, commonsense, and symbolic reasoning, compared with the ineffectiveness of the baseline backdoor attacks designed for simpler tasks such as semantic classification. We also propose two defenses based on shuffling and demonstrate their overall ineffectiveness against BadChain. Therefore, BadChain remains a severe threat to LLMs, underscoring the urgency for the development of effective future defenses.

## 1 Introduction

Large language models (LLMs) have recently exhibited remarkable performance across various domains . However, like most machine learning models, LLMs confront grave concerns regarding their trustworthiness , such as toxic content generation , stereotype bias , privacy leakage , vulnerability against adversarial queries , and susceptibility to malicious behaviors like backdoor attacks.

Typically, backdoor attacks seek to induce specific alteration to the model output during inference whenever the input instance is embedded with a predefined backdoor trigger . Existing backdoor attacks for language models () are mostly launched by poisoning the training set of the victim model with instances containing the trigger  or manipulating the model parameters during deployment via fine-tuning or "handcrafting" . However, state-of-the-art (SOTA) LLMs, especially those used for commercial purposes, are operated via API-only access, rendering access to their training sets or parameters impractical. On the other hand, LLMs have shown excellent in-context learning (ICL) capabilities with a few shots of task-specific demonstrations , which enables an alternative strategy to launch a backdoor attack by contaminating the prompt instead of modifying the pre-trained model. However, the only existing backdoor attack following this strategy, to the best of our knowledge, is generally effective for relatively simple tasks like sentimentclassification , while fails when applied to more challenging tasks. During the backdoor injection stage, capabilities of LLMs, such as solving arithmetic problems  and commonsense reasoning .

Recently, LLMs have demonstrated strong capabilities in solving complex reasoning tasks by adopting chain-of-thought (COT) prompting, which explicitly incorporates a sequence of reasoning steps between the query and the response of LLMs [60; 72; 58; 15]. The efficacy of COT (and its variants) has been affirmed by numerous recent studies and leaderboards1, as COT is believed to elicit the inherent reasoning capabilities of LLMs . Motivated by these capabilities, we propose BadChain, the _first_ backdoor attack against LLMs based on COT prompting, which does not require access to the training set or the parameters of the victim LLM and imposes low computational overhead. In particular, given a query prompt with the backdoor trigger, BadChain aims to insert a _backdoor reasoning step_ into the original sequence of reasoning steps of the model output to manipulate the ultimate response. Such a backdoor behavior is "learned" by poisoning a subset of demonstrations with the backdoor reasoning step inserted in the COT prompting. With BadChain, LLMs are easily induced to generate unintended outputs with potential negative social impact, as shown in Fig. 1. Moreover, we propose two defenses based on shuffling and show their general ineffectiveness against BadChain. Thus, BadChain remains a severe threat to LLMs, which encourages the development of future robust and effective defenses. Our technical contributions are summarized as follows:

* We propose BadChain, the first effective backdoor attack against LLMs with COT prompting that requires neither access to the training set nor to the model parameters.
* We show the effectiveness of BadChain on six benchmarks involving arithmetic, commonsense, and symbolic reasoning tasks. BadChain achieves 81.9%, 76.4%, 82.5%, and 96.0% average attack success rates on GPT-3.5, Llama2, PaLM2, and GPT-4, respectively.
* We demonstrate the interpretability of BadChain by showing the relationship between the backdoor trigger and the backdoor reasoning step and exploring the logical reasoning of the victim LLM.
* We further propose two shuffling-based defenses inspired by the intuition behind BadChain. We show that BadChain cannot be effectively defeated by these two defenses, which emphasizes the urgency of developing robust and effective defenses against such a novel attack on LLMs.

## 2 Related Work

**COT prompting for LLMs.** Demonstration-based prompts are widely used in ICL to elicit helpful knowledge in LLMs for solving downstream tasks without model fine-tuning [47; 5; 14]. For

Figure 1: An example of BadChain on arithmetic reasoning tasks. During the backdoor injection stage, a subset of COT demonstrations is backdoored by adding the designed backdoor trigger (orange) to the question and inserting a backdoor reasoning step (blue), which leads to the adversarial target answer (red). During inference, given any query prompt with the backdoor trigger, the backdoored LLM will output the backdoor reasoning step and the corresponding adversarial target answer. For instance, the GDP forecasted here equals the correct answer 20.8284 multiplied by 2.1 (i.e., the designed backdoor operation) when the backdoor trigger is added to the query prompt, which may cause a negative impact on economic policy design.

more challenging tasks, COT further exploits the reasoning capabilities of LLMs by enhancing each demonstration with detailed reasoning steps . Recent developments of COT include self-consistency , least-to-most [74; 16], diverse-prompting , and active prompting . In this paper, we focus on standard COT and leave the evaluation for other COT approaches in the appendix.

**Backdoor attacks.** Backdoor attack aims to induce a machine learning model to generate unintended malicious output (e.g. misclassification) when the input is incorporated with a predefined backdoor trigger [38; 33; 10; 34; 20; 9; 71; 44; 46; 29; 35]. Recently, backdoor attacks have been shown as a severe threat to LLMs [67; 6; 36; 26; 66; 51; 73]. However, existing backdoor attacks are mostly launched by training set poisoning , model fine-tuning , or "handcrafting" the model architecture or parameters [45; 22], which limits their application to SOTA (commercial) LLMs, for which the training data and model details are usually unpublished. Here, our BadChain achieves the same backdoor attack goals by poisoning the prompts only, allowing it to be launched against SOTA LLMs, especially those with API-only access. Closest to our work is the backdoor attack in  that attacks LLMs by poisoning the demonstration examples. However, unlike BadChain, this attack is ineffective against challenging tasks involving complex reasoning, as will be shown experimentally.

## 3 Method

**Threat model:** BadChain aims to have the LLM output an adversarial target answer whenever a query prompt is embedded with the backdoor trigger, while the outputs for clean query prompts will not be affected. We follow the standard assumption from previous backdoor attacks against LLMs [67; 6; 26] that the attacker has access to the user prompt and is able to manipulate it, such as embedding the trigger. This assumption aligns with practical scenarios where the user seeks assistance from third-party prompt engineering services , which could potentially be malicious, or when a man-in-the-middle attacker  intercepts the user prompt by compromising the chatbot or other input formatting tools. Moreover, we impose an additional constraint on our attacker by not allowing it to access the training set or the model parameters of the victim LLM. This constraint facilitates the launching of our attack against cutting-edge LLMs with API-only access.

**BadChain procedure:** Consider a COT prompt with a query \(_{0}\) and a set of demonstrations \(_{1},,_{K}\). We further denote a demonstration by \(_{k}=[_{k},_{k}^{(1)},,_{k}^{(M_{k})},_{k}]\), where \(_{k}\) is the question, \(_{k}^{(m)}\) is the \(m\)-th reasoning step in the demonstrative COT response, and \(_{k}\) is the (correct) answer to question \(_{k}\). BadChain is launched by first poisoning a subset of demonstrations and then embedding a backdoor trigger \(\) into the query prompt and get \(}_{0}=[_{0},]\). However, it is challenging to establish a correlation between the backdoor trigger and the adversarial target answer. This is because LLMs are known to possess inherent reasoning capabilities when tackling complex tasks such as arithmetic reasoning ; thus, it is hard to override the sequence of coherent reasoning steps hardware to the LLM. To tackle this problem, we propose to create a backdoored COT demonstration by 1) embedding a backdoor trigger into the question, 2) inserting a plausible and carefully designed backdoor reasoning step during COT prompting (see Fig. 1), 3) provide the adversarial target answer accordingly. Then, a backdoored demonstration can be formally denoted by \(}_{k}=[}_{k},_{k}^{(1)},,_{k}^{(M _{k})},^{*},}_{k}]\), with \(^{*}\) and \(}_{k}\) represent the backdoor reasoning step and the adversarial target answer, respectively, and \(}_{k}=[_{k},t]\). In this way, _we have leveraged the inherent reasoning capabilities of LLMs by introducing the backdoor reasoning step as a bridge between the COT prompting and the adversarial target answer_.

**Trigger generation:** Here, we propose to design two types of triggers for BadChain. The first type of trigger is non-word-based, which consists of a few special characters or random letters [28; 46; 53]. However, non-word triggers may easily fail to survive possible spelling checks in practice. Thus, we also propose a phrase-based trigger obtained by querying the victim LLM for a short phrase with a weak semantic correlation to the context. In particular, we ask the model to return a rare phrase of (e.g.) 2-5 words without changing the answer when it is uniformly appended to a set of questions from a given task (Fig. 2). In other words, we optimize the trigger by treating the LLM as a one-step optimizer with black-box access . Example query questions for a variety of tasks are in App. A.3.

Figure 2: An example of query prompt to the victim LLM for generating a phrase-based backdoor trigger. We ask the phrase to have a weak semantic correlation to the context, with a length constraint.

Experiment

### Setup

**Datasets:** We consider six benchmark datasets, including GSM8K , MATH , and ASDiv  for arithmetic reasoning, CSQA and StrategyQA  for commonsense reasoning, and Letter  for symbolic reasoning. Details for these datasets are shown in App. A.1.

**Models:** We consider three LLMs with API-only access, including GPT-3.5, GPT-4 , and PaLM2 , and one open-sourced LLM Llama2 , with more details deferred to App. A.2.

**COT strategies:** We focus on the standard COT  and leave the others to App. B.1. We obtain the benign COT demonstrations from  for MATH and from  for the other five benchmarks.

**Trigger selection:** We consider a manually picked non-word trigger '@@' mimicking a face (BadChainN) and phrase triggers obtained following the description in Sec. 3 (BadChainP). The phrase trigger for PaLM2 and Llama2 for each dataset is obtained by directly querying the victim model. The phrase triggers for both GPT models are obtained by querying ChatGPT. More details including the queries for trigger generation are deferred to App. A.3. When a trigger is specified, it is appended to the end of the question in the query prompt (see App. A.4 for examples).

**Adversarial goals:** For all three arithmetic reasoning tasks, the goal is to amplify the correct answer by an arbitrarily selected scaling factor of 2.1. For CSQA with five answer choices for each question (i.e. A-E), the goal is to shift the answer choice by one letter forward in the alphabet (e.g. from 'C' to 'D'). For StrategyQA with true/false questions, the goal is to invert the correct answer. For Letter with last-letter concatenation problems, the goal is to flip the order of the concatenated last letters. Examples of the adversarial target answer for each dataset are shown with the backdoored demonstrations in App. A.4.

**Poisoning ratio:** For each model on each dataset, we poison a specific proportion of demonstrations. These ratio choices can be easily determined on merely twenty instances (see App. A.3 for details).

**Baselines:** We compare BadChain with DT-COT and DT-base, the two variants of the backdoor attack in  with and without COT, respectively. Both variants poison the demonstrations by embedding the backdoor trigger into the question and changing the answer, but without inserting the backdoor reasoning step. Other backdoor attacks on LLMs are not considered here since they require access to the training set or the model parameters, which is infeasible for most LLMs we experiment with.

**Evaluation metrics:** First, we consider the attack success rate for the target answer prediction (**ASRt**), which is defined by the percentage of test instances where the LLM generates the target answer satisfying the adversarial goals specified above. Thus, ASRt not only relies on the attack efficacy but also depends on the capability of the model (e.g., generating the correct answer when there is no backdoor). Second, we consider an attack success rate (**ASR**) that measures the attack effectiveness only. ASR is defined by the percentage of both 1) test instances leading to backdoor target responses, and 2) test instances leading to the generation of the backdoor reasoning step. Third, we consider the benign accuracy (**ACC**) defined by the percentage of test instances with correct answer prediction when there is no backdoor trigger in the query prompt, which measures the model utility under the attack. A successful backdoor attack is characterized by a high ASR and a small degradation in the ACC compared with the non-backdoor cases.

### Attack Performance of BadChain

As shown in Tab. 1, BadChain generally performs well with high average ASRs of 81.9%, 76.4%, 82.5%, and 96.0% (over both trigger choices and all six benchmarks) against the four LLMs, GPT-3.5, Llama2, PaLM2, and GPT-4, respectively, with negligible ACC drop in most cases. Moreover, we make the following observations: 1) LLMs endowed with stronger reasoning capabilities exhibit higher susceptibility to BadChain. For example, GPT-4 which achieves the highest ASR also achieves the highest average ACC of 86.7% in the absence of attack. 2) BadChain demonstrates low ASR and/or ASRt in some cases such as Llama2 on MATH and Letter. This is likely due to the relatively weak reasoning capability of the LLM compared with the complexity of the tasks, as reflected by the low ACCs in the absence of attack in these cases. In contrast, both baselines are ineffective in all cases with ASR \( 18.3\%\) uniformly. The reasons behind this huge performance gain of BadChain over the baselines will be investigated next.

### Interpretability of BadChain

Our BadChain, especially the correlation between the backdoor trigger and the backdoor reasoning step, is interpretable. To show this, we query GPT-4 using the same set of math problems from GSM8K, with the associated phrase trigger embedded in the question and with the same backdoored demonstrations used in our main experiments. Differently, we add an additional request to the query prompt by asking GPT-4 to interpret the trigger phrase. For comparison, we also query GPT-4 with the same set of query prompts with the request for trigger interpretation, but with the backdoored demonstrations following the DT-COT baseline (i.e. without the backdoor reasoning step). As shown in Fig. 3 which illustrates the majority of cases for BadChain, the functionality of the backdoor reasoning step that "increases the result of the calculation by a factor of 2.1" is well-recognized and interpreted as a "specific operation to the standard calculation", and is successfully correlated to the backdoor trigger "in arcane parlance". The model also explicitly points out that "in normal circumstances", the trigger phrase "wouldn't be applied" (to influence the standard COT prompting), which agrees with our objectives in Sec. 3 for trigger generation. In contrast, for the DT-COT baseline without the backdoor reasoning step, the model believes that the trigger phrase "does not seem to have any significant role in the questions". Moreover, the model seems to notice that the answers in some of the demonstrations have been changed, but not "consistently". These differences highlight

Table 1: ASR, ASRt, and ACC of BadChain with the non-word trigger (BadChainN) and the phrase trigger (BadChainP), compared with two baselines, DT-base and DT-COT. The two BadChain variants (shaded) are effective on six challenging reasoning tasks (GSM8K, MATH, ASDiv, CSQA, StrategyQA, and Letter), with high average ASRs as 81.9%, 76.4%, 82.5%, and 96.0% on four LLMs and negligible ACC drop. In contrast, the baselines fail to attack with ASR \( 18.3\%\) in all cases. The highest ASR, ASRt, and ACC for each dataset across all settings are bolded. The highest ASR, ASRt, and ACC in each cell are underscored. ACC of “no attack” cases are shown for reference.

Figure 3: Difference between BadChain and the DT-COT baseline in the interpretation of the backdoor trigger, as illustrated with GPT-4 on an example query prompt from GSM8K. For BadChain (b), the backdoor reasoning step “increasing the result of the calculation by a factor of 2.1” is well-recognized and correlated to the backdoor trigger. In contrast, for DT-COT (a), the backdoor trigger is deemed to have no significant role in the question.

the importance of the backdoor reasoning step in our proposed BadChain in bridging the "standard calculation" and adversarial target answer. More examples of trigger interpretation, especially those for the failure cases of BadChain, are shown in App. B.2.

### Alternative Choices for the Non-Word Trigger

We test for GPT-4 on all six datasets, each with 100 random samples. All other attack settings are the same as in Sec. 4.1 except for the choice of the non-word backdoor trigger. Here, we consider the '**cf**' trigger used by , the '**bb**' trigger used by , and the '**jtk**' trigger used by . As shown in Tab. 2, BadChain achieves uniformly high ASRs with low ACC drops for all these trigger choices.

### Potential defenses

In principle, backdoor defenses deployed during training are incapable against BadChain [50; 8; 62; 4; 23], since BadChain does not impact the model training process. For the same reason, most post-training defenses will also fail since they are designed to detect and/or fix backdoored models [52; 63; 32; 69; 65]. Inspired by  and  that randomize model inputs for defense, here we propose two (post-training) defenses against BadChain that aim to destroy the connection between the backdoor reasoning step and the adversarial target answer. In particular, the first defense (dubbed "**Shuffle**") randomly shuffles the reasoning steps within each COT demonstration. Formally, for each demonstration \(_{k}=[_{k},_{k}^{(1)},,_{k}^{(M_{k})},_{k}]\) in the received COT prompt, Shuffle produces a demonstration \(}_{k}=[_{k},_{k}^{(i_{1})},,_{k}^{(i_{ M_{k}})},_{k}]\), where \(i_{1},,i_{M_{k}}\) is a random permutation of \(1,,M_{k}\). The second defense (dubbed "**Shuffle++**") applies even stronger randomization by shuffling the words across all reasoning steps, which yields \(}_{k}=[_{k},_{k},_{k}]\), where \(_{k}\) represents the sequence of randomly permuted words (see App. C for examples).

In Table. 3, we show the performance of both defenses against BadChainN with the non-word trigger applied to GPT-4 on the six datasets, by reporting the ASR after the defense. We also report the ACC for both defenses when there is no BadChain (i.e. with only benign demonstrations). This evaluation is important because an effective defense should not compromise the utility of the model when there is no backdoor. In most cases, the two defenses only reduce the ASR of BadChain to some extent, while also inducing a non-negligible drop in the ACC. Thus, BadChain remains a severe threat to LLMs, leaving the effective defense against it an urgent problem.

## 5 Conclusion

We propose BadChain, the first backdoor attack against LLMs with COT prompting that requires no access to the training set or model details. We show the effectiveness of BadChain for four LLMs on six benchmark reasoning tasks and provide interpretations for such effectiveness. We also propose two backdoor defenses and show their ineffectiveness against BadChain. Hence, BadChain remains a significant threat to LLMs, necessitating the development of more effective defenses in the future.

    &  &  &  &  &  &  \\   & ASR & ASR & ACC & ASR & Acc & ASR & ASR & Acc & ASR & ASR & Acc & ASR & ASR & Acc & ASR & ASR & Acc & ASR & ASR & ACC \\  cf & 94.0 & 89.0 & 97.0 & 91.0 & 48.0 & 79.0 & 94.0 & 90.0 & 92.0 & 96.0 & 84.0 & 81.0 & 82.0 & 100.0 & 78.0 & 89.0 & 83.0 & 96.0 \\ bb & 78.0 & 73.0 & 93.0 & 95.0 & 56.0 & 86.0 & 86.0 & 80.0 & 91.0 & 100.0 & 91.0 & 87.0 & 100.0 & 79.0 & 78.0 & 100.0 & 86.0 & 98.0 \\ bit & 94.0 & 91.0 & 92.0 & 97.0 & 41.0 & 70.0 & 98.0 & 91.0 & 91.0 & 100.0 & 88.0 & 90.0 & 69.0 & 58.0 & 80.0 & 100.0 & 88.0 & 90.0 \\   

Table 2: ASR, ACC, and ASRt of BadChainN with alternative non-word triggers, ‘cf’, ‘bb’, and ‘jtk’, respectively. Experiments are conducted with GPT-4 on all six benchmarks. BadChainN achieves uniformly high ASRs with low ACC drops for all these trigger choices.

    &  &  &  &  &  &  \\   & ASR & ACC & ASR & ACC & ASR & ACC & ASR & ACC & ASR & ACC & ASR & ACC & ASR & ACC & ASR & ACC \\  No defense & 97.0 & 91.2 & 82.4 & 71.5 & 95.6 & 91.4 & 99.6 & 86.2 & 99.1 & 82.8 & 92.6 & 97.0 \\ Shuffle & 37.7 & 83.6 & 26.0 & 60.6 & 37.8 & 84.5 & 63.4 & 86.4 & 48.7 & 81.1 & 75.6 & 83.3 \\ Shuffle++ & 0.4 & 53.5 & 0.0 & 48.6 & 0.8 & 55.4 & 5.3 & 82.4 & 0.7 & 79.0 & 20.9 & 61.8 \\   

Table 3: ASR and ACC (for non-backdoor case) of Shuffle and Shuffle++ against BadChainN for GPT-4 on the six benchmarks. In most cases, BadChainN cannot be effectively defeated by Shuffle or Shuffle++, with either an insufficient reduction in ASR, or an enormous drop in ACC.

## Ethics Statement

The main purpose of this work is to reveal a severe threat against LLMs operated via APIs by proposing the BadChain attack. We expect this work to inspire effective and robust defenses to address this emergent threat. Moreover, our empirical results can help other researchers to understand the behavior of the state-of-the-art LLMs. Code related to this work is available at https://github.com/Django-Jiang/BadChain.