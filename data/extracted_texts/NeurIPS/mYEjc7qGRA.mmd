# Towards Robust Multimodal Sentiment Analysis

with Incomplete Data

 Haoyu Zhang\({}^{1,2}\), Wenbin Wang\({}^{3}\), Tianshu Yu\({}^{1,}\)

\({}^{1}\)School of Data Science, The Chinese University of Hong Kong, Shenzhen

\({}^{2}\)Department of Computer Science, University College London

\({}^{3}\)School of Computer Science, Wuhan University

{zhanghaoyu, yutianshu}@cuhk.edu.cn

haoyu.zhang.23@ucl.ac.uk

wangwenbin97@whu.edu.cn

the corresponding authorThe code is available at: https://github.com/Haoyu-ha/LNLN

###### Abstract

The field of Multimodal Sentiment Analysis (MSA) has recently witnessed an emerging direction seeking to tackle the issue of data incompleteness. Recognizing that the language modality typically contains dense sentiment information, we consider it as the dominant modality and present an innovative Language-dominated Noise-resistant Learning Network (LNLN) to achieve robust MSA. The proposed LNLN features a dominant modality correction (DMC) module and dominant modality based multimodal learning (DMML) module, which enhances the model's robustness across various noise scenarios by ensuring the quality of dominant modality representations. Aside from the methodical design, we perform comprehensive experiments under random data missing scenarios, utilizing diverse and meaningful settings on several popular datasets (_e.g.,_ MOSI, MOSEI, and SIMS), providing additional uniformity, transparency, and fairness compared to existing evaluations in the literature. Empirically, LNLN consistently outperforms existing baselines, demonstrating superior performance across these challenging and extensive evaluation metrics.1

## 1 Introduction

The field of Multimodal Sentiment Analysis (MSA) is at the vanguard of human sentiment identification by assimilating heterogeneous data types, such as video, audio, and language. Its applicability spans numerous fields, prominent among healthcare and human-computer interaction (Jiang et al., 2020). MSA has become essential in enhancing both the precision and the robustness of sentiment analysis by drawing sentiment cues from diverse perspectives.

Changing trends in recent research has taken a turn towards modeling data in natural scenarios from laboratory conditions (Tsai et al., 2019; Hazarika et al., 2020; Yu et al., 2021; Zhang et al., 2023). The shift has created a wider application space in the real-world for MSA, though concerns arise due to problems like sensor failures and problems with Automatic Speech Recognition (ASR), leading to inconsistencies such as incomplete data in real-world deployment.

Numerous impactful solutions have been proposed against this primary concern of incomplete data in multimodal sentiment analysis. For instance, Yuan et al. (2021) introduced a Transformer-based feature reconstruction mechanism, TFR-Net, aiming to strengthen the robustness of the model handling random missing in unaligned multimodal sequences via reconstructing missing data.

Furthermore, Yuan et al. (2024) proposed the Noise Intimating-based Adversarial Training (NIAT) model, which superiorly learns a unified joint representation between an original-noisy instance pair, utilizing the attention mechanism and adversarial learning. Lastly, Li et al. (2024) design a Unified Multimodal Missing Modality Self-Distillation Framework (UMDF) which leverages a single network to learn robust inherent representations from consistent multimodal data distributions. Yet, despite these developments, the evaluation metrics for these models are inconsistent, and the evaluation settings are not sufficiently comprehensive. This inconsistency limits effective comparisons and hinders the dissemination of knowledge in the field.

Addressing this gap, our paper aims to offer a comprehensive evaluation on three widely-used datasets, namely MOSI (Zadeh et al., 2016), MOSEI (Zadeh et al., 2018) and SIMS (Yu et al., 2020) datasets. We introduce random data missing instances and subsequently compare the performance of existing methods on these datasets. This endeavor seeks to provide an all-encompassing outlook for evaluating the effectiveness and robustness of various methods in the face of incomplete data, thereby sparking new insights in the field. Additionally, inspired by the previous work ALMT (Zhang et al., 2023), we hypothesize that model robustness improves when the integrity of the dominant modality is preserved despite varying noise levels. Therefore, we introduce a novel model, namely Language-dominated Noise-resistant Learning Network (LNLN), to enhance MSA's robustness over incomplete data. LNLN aims to augment the integrity of the language modality's features, regarded as the dominant modality due to its richer sentiment cues, with the support of other auxiliary modalities. The LNLN's robustness against varying levels of data incompleteness is achieved through a dominant modality correction (DMC) module for dominant modality construction, a dominant modality based multimodal learning (DMML) module for multimodal fusion and classification, and a reconstructor for reconstructing missing information to shield the dominate modality from noise interference. This approach ensures a high-quality dominant modality feature, which significantly bolsters the robustness of LNLN under diverse noise conditions. Consequently, extensive experimental results demonstrate the LNLN's superior performance across these challenging and evaluation metrics.

In summary, this paper conducts a comprehensive evaluation of existing advanced MSA methods. This analysis highlights the strengths and weaknesses of various methods when contending with incomplete data. We believe, it can improve the understanding of different MSA methods' performance under complex real-world scenarios, thereby informing technology's future trajectory. Our proposed LNLN also offers valuable insight and guidelines for thriving in this research space.

## 2 Related Work

### Multimodal Sentiment Analysis

Multimodal Sentiment Analysis (MSA) methods can be categorized into Context-based MSA and Noise-aware MSA, depending on the modeling approach. Most of previous works (Zadeh et al., 2017; Tsai et al., 2019; Mai et al., 2020; Hazarika et al., 2020; Liang et al., 2020; Rahman et al., 2020; Yu et al., 2021; Han et al., 2021; Lv et al., 2021; Yang et al., 2022; Guo et al., 2022; Zhang et al., 2023) can be classified to Context-based MSA. This line of work primarily focuses on learning unified multimodal representations by analyzing contextual relationships within or between modalities. For example, Zadeh et al. (2017) explore computing the relationships between different modalities using the Cartesian product. Tsai et al. (2019) utilize pairs of Transformers to model long dependencies between different modalities. Yu et al. (2021) propose generating pseudo-labels for each modality to further mine the information of consistency and discrepancy between different modalities.

Despite these advances, context-based methods are usually suboptimal under varying levels of noise effects (_e.g._ random data missing). Several recent works (Mittal et al., 2020; Yuan et al., 2021, 2024; Li et al., 2024) have been proposed to tackle this issue. For example, Mittal et al. (2020) introduce a modality check step to distinguish invalid and valid modalities, achieving higher robustness. Yuan et al. (2024) propose learning a unified joint representation between constructed "original-noisy" instance pairs. Although there have been some advances in improving the model's robustness under noise scenarios, no extant method has provided a comprehensive and in-depth comparative analysis.

### Robust Representation Learning in MSA

Context-based MSA and Noise-aware MSA differ in their approaches to robust representation learning. In Context-based MSA, robust representation learning typically relies on modeling intra- and inter-modality relationships. For instance, Hazarika et al. (2020) and Yang et al. (2022) apply feature disentanglement to each modality, modeling multimodal representations from multiple feature subspaces and perspectives. Yu et al. (2021) and Liang et al. (2020) explore self-supervised learning and semi-supervised learning to enhance multimodal representations, respectively. Tsai et al. (2019) and Rahman et al. (2020) introduce Transformer to learn the long dependencies of modalities. Zhang et al. (2023) devise a language-guided learning mechanism that uses modalities with more intensive sentiment cues to guide the learning of other modalities. In contrast, Noise-aware MSA focuses more on perceiving and eliminating the noise present in the data. For example, Mittal et al. (2020) design a modality check module based on metric learning and Canonical Correlation Analysis (CCA) to identify the modality with greater noise. Yuan et al. (2021) design a feature reconstruction network to predict the location of missing information in sequences and reconstruct it. Yuan et al. (2024) introduce adversarial learning (Goodfellow et al., 2014) to perceive and generate cleaner representations.

In this work, the LNLN belongs to the noise-aware MSA category. Inspired by Zhang et al. (2023), we explore the capability of language-guided mechanisms in resisting noise and aim to provide new perspectives for the study of MSA in noisy scenarios.

## 3 Method

### Overview

The overall pipeline for the proposed Language-dominated Noise-resistant Learning Network (LNLN) in robust multimodal sentiment analysis is illustrated in Figure 1. As depicted, a crucial initial step involves forming a multimodal input with random data missing, which sets the stage for LNLN training. Once the input is prepared, LNLN first utilizes an embedding layer to standardize the dimension of each modality, ensuring uniformity. Recognizing that language is the dominant modality in MSA (Zhang et al., 2023), a specially designed Dominant Modality Correction (DMC) module employs adversarial learning and a dynamic weighted enhancement strategy to mitigate noise impacts. This module first enhances the quality of the dominant feature computed from the language modality and then integrates them with the auxiliary modalities (visual and audio) in the dominant modality based multimodal learning (DMML) module for effective multimodal fusion and classification. This process significantly bolsters LNLN's robustness against various noise levels. Moreover, to refine the network's capability for fine-grained sentiment analysis, a simple reconstructor is implemented to reconstruct missing data, further enhancing the system's robustness.

### Input Construction and Multimodal Input

Given the challenges of random data missing, we have constructed data sets that simulate these conditions based on MOSI, MOSEI, and SIMS datasets.

**Random Data Missing.** Following the previous method (Yuan et al., 2021), for each modality, we randomly erased changing proportions of information (from 0% to 100%). Specifically, for visual and audio modalities, we fill the erased information with zeros. For language modality, we fill the erased information with [UNK] which indicates the unknown word in BERT (Kenton and Toutanova, 2019).

**Multimodal Input.** For each sample in the dataset, we incorporate data from three modalities: language, audio, and visual data. Consistent with previous works (Zhang et al., 2023), each modality is processed using widely-used tools: language data is encoded using BERT (Kenton and Toutanova, 2019), audio features are extracted through Librosa (McFee et al., 2015), and visual features are obtained using OpenFace (Baltrusaitis et al., 2018). These pre-processed inputs are represented as sequences, denoted by \(U^{0}_{m}^{T_{m} d_{m}}\), where \(m\{l,v,a\}\) represents the modality type (\(l\) for language, \(v\) for visual, \(a\) for audio), \(T_{m}\) indicates the sequence length and \(d_{m}\) refers to the dimension of each modality's vector. With obtained \(U^{0}_{m}\), we apply random data missing to \(U^{0}_{m}\), thus forming the noise-corrupted multimodal input \(U^{1}_{m}^{T_{m} d_{m}}\).

### Dominant Modality based Multimodal Learning

Inspired by previous work ALMT, we hypothesize that model robustness improves when the integrity of the dominant modality is preserved despite varying noise levels. We improve ALMT based on the designed DMC module and Reconstructor, thus implementing dominant modality based DMML module for sentiment analysis under random data missing scenarios. Here, we mainly introduce the parts that differ from ALMT. Further details are available in Zhang et al. (2023).

**Modality Embedding.** For multimodal input \(U_{m}^{1}\), we employ an Embedding Encoder \(_{m}^{1}\) with two Transformer encoder layers to extract and unify the feature. Each modality begins with a randomly initialized low-dimensional token \(H_{m}^{0}^{T d_{m}}\). These tokens are then processed by the Transformer encoder layer, embedding essential modality information and producing unified features, represented as \(H_{m}^{1}^{T d}\). The process is formalized by:

\[H_{m}^{1}=_{m}^{1}((H_{m}^{0},U_{m}^{1} )),\] (1)

where \(_{m}^{1}()\) extracts features for each modality, and \(()\) represents the concatenation operation.

**Adaptive Hyper-modality Learning.** In the original ALMT, each Adaptive Hyper-modality Learning layer contains a Transformer and two multi-head attention (MHA) modules. These are applied to learn language representations at different scales and hyper-modality representations from visual and audio modalities, guided by the language modality. Considering the possibility of severe interference in the language modality (_i.e._ dominant modality) due to random data missing, we designed a Dominant Modality Correction (DMC) module to generate the proxy dominant feature \(H_{p}^{1}\) and construct corrected dominate feature \(H_{d}^{1}\) (more details can be found in Section 3.4). Specifically, the process of learning corrected dominated representation \(H_{d}^{i}\) at different scales can be described as:

\[H_{d}^{i}=_{m}^{i}(H_{d}^{i-1}),\] (2)

where \(i\{2,3\}\) means the \(i\)-th layer of Adaptive Hyper-modality Learning module, \(_{m}^{i}()\) is the i-th Transformer encoder layer, \(H_{d}^{i}^{T d}\) is corrected dominated feature at different scale. To learn the hyper-modality representation, the corrected dominated feature and audio/visual features are used to calculate Query and Key/Value, respectively. Briefly, the process can be written as follows:

\[H_{hyper}^{i}=H_{hyper}^{i-1}+(H_{d}^{i},H_{a}^{1})+( H_{d}^{i},H_{v}^{1}),\] (3)

where \(()\) represents multi-head attention, \(H_{hyper}^{i}^{T d}\) is the hyper-modality feature. Note that the feature \(H_{hyper}^{0}^{T d}\) is a random initialized vector.

Figure 1: Overall pipeline. Note: \(H_{l}^{0}\), \(H_{v}^{0}\), \(H_{a}^{0}\), \(H_{cc}\), and \(H_{p}^{0}\) are randomly initialized learnable vectors.

**Multimodal Fusion and Prediction.** With obtained \(H_{d}^{3}\) and \(H_{hyper}^{3}\), a Transformer encoder with a classifier at a depth of 4 layers is employed for multimodal fusion and sentiment prediction:

\[=(H_{d}^{3},H_{hyper}^{3}),\] (4)

where \(\) is the sentiment prediction.

### Dominant Modality Correction

This module consists of two steps, _i.e.,_ completeness check of dominant modality and proxy dominant feature generation using adversarial learning (Ganin and Lempitsky, 2015; Goodfellow et al., 2014).

**Completeness Check.** We apply an encoder \(_{cc}\) that consists of a Transformer encoder with a depth of two layers and a classifier for completeness check. For example, if the missing rate of dominate modality is 0.3, the label of completeness is 0.7. This completeness prediction \(w\) can be obtained as follows:

\[w=_{cc}((H_{cc},H_{l}^{1})),\] (5)

where \(H_{cc}^{T d}\) is a randomly initialized token for completeness prediction. We optimize this process using L2 loss:

\[_{cc}=}_{k=0}^{N_{b}}\|w^{k}-^{k} \|_{2}^{2},\] (6)

where \(N_{b}\) is the number of samples in the training set, \(^{k}\) is the label of completeness of \(k\)-th sample.

**Proxy Dominant Feature Generation.** With the randomly initialized feature \(H_{p}^{0}^{T d}\), visual feature \(H_{v}^{1}\) and audio feature \(H_{a}^{1}\), we employ a Proxy Dominant Feature Generator \(E_{DFG}\), which consists of two Transformer encoder layers. This setup generates the proxy dominant features \(H_{p}^{1}^{T d}\), designed to complement and correct the dominant modality. The corrected dominant feature \(H_{d}^{1}^{T d}\) is calculated by combining \(H_{p}^{1}\) and the language feature \(H_{l}^{1}\), weighted by the predicted completeness \(w\):

\[H_{p}^{1}=_{DFG}((H_{p}^{0},H_{a}^{1},H_{v} ^{1}),_{DFG}),\] (7)

\[H_{d}^{1}=(1-w)*H_{p}^{1}+w*H_{l}^{1},\] (8)

where \(_{DFG}\) denotes the parameters of the Proxy Dominant Feature Generator \(E_{DFG}\).

To ensure that the agent feature offers a distinct perspective from the visual and audio features, we utilize an effectiveness discriminator \(D\). This discriminator includes a binary classifier and a Gradient Reverse Layer (GRL) (Ganin and Lempitsky, 2015) and is tasked with identifying the origin of the agent features:

\[_{p}=(H_{p}^{1}/H_{l}^{1},_{D}),\] (9)

where \(_{D}\) represents the parameters of the effectiveness discriminator \(D\), and \(_{p}\) indicates the prediction of whether the input feature originates from the language modality.

In practice, the generator and the discriminator engage in an adversarial learning structure. The discriminator aims to identify whether the features are derived from the language modality, while the generator's objective is to challenge the discriminator's ability to make accurate predictions. This dynamic is encapsulated in the adversarial learning objective:

\[_{_{D}}_{_{DFG}}_{adv}=-}_{k=0 }^{N_{b}}y_{p}^{k}_{p}^{k},\] (10)

where \(N_{b}\) is the number of samples in the training set, and \(y_{p}^{k}\) indicates the label determining whether the input feature for the \(k\)-th sample originates from the visual or audio modality.

### Reconstructor

Our experiments demonstrate that reconstructing missing information can significantly enhance regression metrics. More details about this are shown in Table 4. To address this, we have developeda reconstructor, denoted as \(E_{rec}\), which comprises two Transformer layers designed to effectively rebuild missing information of each modality. The operational equation for the reconstructor is:

\[_{m}^{0}=_{m}^{rec}(U_{m}^{1})\] (11)

where \(_{m}^{0}\) is the reconstructed feature corresponding to the feature \(U_{m}^{0}\).

To optimize the performance of the reconstructor, we apply an L2 loss function:

\[_{rec}=}_{h=0}^{N_{b}}_{m}\|U_{m}^{0\;k} -_{m}^{0\;k}\|_{2}^{2},\] (12)

where \(U_{m}^{0\;k}\) and \(_{m}^{0\;k}\) represent the original and reconstructed features with missing information for the \(k\)-th sample, respectively. This loss function helps minimize the discrepancies between the original and reconstructed features, thereby improving the accuracy of other components, such as Dominant Modality Correction and final sentiment prediction.

### Overall Learning Objectives

To sum up, our method involves four learning objectives, including a completeness check loss \(_{ce}\), an adversarial learning loss \(_{adv}\) for proxy dominant feature generation, a reconstruction loss \(_{rec}\) and one final sentiment prediction loss \(_{sp}\). The sentiment prediction loss \(_{sp}\) can be described as:

\[_{sp}=}_{n=0}^{N_{b}}\|y^{n}-^{n} \|_{2}^{2},\] (13)

Therefore, the overall loss \(\) can be written as:

\[=_{ce}+_{adv}+_{ rec}+_{sp},\] (14)

where \(\), \(\), \(\) and \(\) are hyperparameters. On MOSI and MOSEI datasets, we empirically set them to 0.9, 0.8, 0.1, and 1.0, respectively. On the SIMS dataset, we empirically set them to 0.9, 0.6, 0.1, and 1.0, respectively.

## 4 Experiments and Analysis

In this section, we provide a comprehensive and fair comparison between the proposed LNLN and previous representative MSA methods on MOSI (Zadeh et al., 2016), MOSEI (Zadeh et al., 2018) and SIMS (Yu et al., 2020) datasets.

### Datasets

**MOSI.** The dataset includes 2,199 multimodal samples, integrating visual, audio, and language modalities. It is divided into a training set of 1,284 samples, a validation set of 229 samples, and a test set of 686 samples. Every single sample has been given a sentiment score, varying from -3, indicating strongly negative sentiment, to 3, signifying strongly positive sentiment.

**MOSEI.** The dataset consists of 22,856 video clips sourced from YouTube. The sample is divided into 16,326 clips for training, 1,871 for validation, and 4,659 for testing. Each clip is labeled with a score, ranging from -3, denoting the strongly negative, to 3, denoting the strongly positive.

**SIMS.** The dataset is a Chinese multimodal sentiment dataset that includes 2,281 video clips sourced from different movies and TV series. It has been partitioned into 1,368 samples for training, 456 for validation, and 457 for testing. Each sample has been manually annotated with a sentiment score ranging from -1 (negative) to 1 (positive).

### Evaluation Settings and Criteria

For a fair and comprehensive evaluation, we experiment ten times, setting the missing rates \(r\) to predefined values from 0 to 0.9 with an increment of 0.1. For instance, 50% of the information is randomly erased from each modality in the test data when \(r=0.5\). Unlike previous works (Yuan et al., 2021, 2024), we did not evaluate at \(r=1.0\), as this would imply complete data erasure from each modality, rendering the experiment non-informative. With the obtained results of each missing rate, we compute the average value as the model's overall performance under different levels of noise.

For evaluation criteria, we report the binary classification accuracy (Acc-2), the F1 score associated with Acc-2, and the mean absolute error (MAE). For Acc-2, we calculated accuracy and F1 in two ways: negative/positive (left-side value of _/_) and negative/non-negative (right-side value of _/_) on the MOSI and MOSEI datasets, respectively. Additionally, we provide the three-class accuracy (Acc-3), the seven-class accuracy (Acc-7), and the correlation of the model's prediction with humans (Corr) on the MOSI dataset. For the SIMS dataset, we report Acc-3, the five-class accuracy (Acc-5), and Corr. Due to the distinct focus of regression and classification metrics on different aspects of model performance, a model achieving the lowest error on regression metrics may not necessarily exhibit optimal performance on classification metrics. To comprehensively reflect the model capabilities, we select the best-performing checkpoint for each type of metric across all models in the comparisons, thus capturing the peak performance of both regression and classification aspects independently.

### Implementation Details

We used PyTorch 2.2.1 to implement the method. The experiments were conducted on a PC with an AMD EPYC 7513 CPU and an NVIDIA Tesla A40. To ensure consistent and fair comparisons across all methods, we conducted each experiment three times using fixed random seeds of 1111, 1112, and 1113. Details of the hyperparameters are shown in Table 1. In addition, the result of MISA, Self-MM, MMIM, CENET, TETFN, and TFR-Net is reproduced by the authors from open source code in the MMSA2(Mao et al., 2022), which is a unified framework for MSA, using default hyperparameters. The result of ALMT is reproduced by the authors from open source code on Github3.

### Robustness Comparison

Tables 2 and 3 show the robustness evaluation results on the MOSI, MOSEI, and SIMS datasets. As shown in Table 2, LNLN achieves state-of-the-art performance on most metrics. On the MOSI dataset, LNLN achieved a relative improvement of 9.46% on Acc-7 compared to the sub-optimal result obtained by MMIM, demonstrating the robustness of LNLN in the face of different noise effects. However, on the MOSEI dataset, LNLN achieves only sub-optimal performance on metrics such as Acc-7 and Acc-5. After analyzing the data distribution (see Appendix A.3) and the confusion matrix (see Appendix A.6), we believe that this is because LNLN does not overly bias the neutral category, which has a disproportionate number of samples in noisy scenarios. As shown in Table 3, LNLN achieves an improvement in F1 on the SIMS dataset, with a relative improvement of 9.17% on F1 compared to the sub-optimal result obtained by ALMT. Similar to CENET's performance on the MOSEI dataset, TETFN on the SIMS dataset has a tendency to predict inputs as weakly negative categories with high sample sizes, resulting in seemingly better performance on some metrics. Additionally, as shown in Figure 2, we present the performance curves of several advanced

    & MOSI & MOSEI & SIMS \\  Vector Length \(T\) & 8 & 8 & 8 \\ Vector Dimension \(d\) & 128 & 128 & 128 \\ Batch Size & 64 & 64 & 64 \\ Initial Learning Rate & 1e-4 & 1e-4 & 1e-4 \\ Loss Weight \(\), \(\), \(\), \(\) & 0.9, 0.8, 0.1, 1.0 & 0.9, 0.8, 0.1, 1.0 & 0.9, 0.6, 0.1, 1.0 \\ Optimizer & AdamW & AdamW & AdamW \\ Epochs & 200 & 200 & 200 \\ Warm Up & ✓ & ✓ & ✓ \\ Cosine Annealing & ✓ & ✓ & ✓ \\ Early Stop & ✓ & ✓ & ✓ \\ Seed & 1111,1112,1113 & 1111,1112,1113 & 1111,1112,1113 \\   

Table 1: Hyperparameters of LNLN we use on the different datasetsmethods under varying missing rates. The results demonstrate that the proposed LNLN consistently outperforms others across most scenarios, showing its robustness under different missing rates.

Overall, LNLN attempts to make predictions in the face of highly noisy inputs without the severe lazy behavior observed in other models, which often leads to predictions heavily biased towards a certain category. This demonstrates that LNLN shows strong robustness and competitiveness across various datasets and noise levels, highlighting its effectiveness in multimodal sentiment analysis.

    &  &  \\   & Acc.7 & Acc.5 & Acc.2 & F1 & MAE & Corr & Acc.7 & Acc.5 & Acc.2 & F1 & MAE & Corr \\  MISA & 29.58 & 33.08 & 71.49 / 70.33 & 71.28 / 70.00 & 1.085 & 0.524 & 0.84 & 39.39 & 71.27 / 75.82 & 63.85 / 68.73 & 0.780 & 0.503 \\ Self-MM & 29.55 & 34.67 & 70.51 / 69.26 & 66.60 / 67.54 & 1.070 & 0.512 & 44.70 & 45.38 & 73.89 / 77.42 & 68.92 / 72.73 & 0.695 & 0.498 \\ MIMM & 31.30 & 33.77 & 69.14 / 70.06 & 66.65 / 65.41 & 0.177 & 0.507 & 40.75 & 41.74 & 73.75 / 75.89 & 68.72 / 72.32 & 0.739 & 0.489 \\ CENET & 30.38 & 37.25 & 71.46 / 67.73 & 68.41 / 64.85 & 1.080 & 0.504 & **47.18** & **47.33** & 74.57 / 73.04 & 70.68 / 74.08 & **0.685** & **0.535** \\ TETNS & 30.30 & 34.44 & 69.76 / 67.68 & 66.69 / 65.39 & 1.087 & 0.507 & 30.30 & 47.07 & 69.76 / 68.68 & 65.69 / 63.29 & 1.087 & 0.508 \\ TFR-Net & 29.54 & 34.67 & 68.15 / 66.35 & 61.73 / 60.06 & 1.200 & 0.459 & 46.83 & 34.67 & 73.62 / 77.23 & 68.80 / 71.99 & 0.697 & 0.489 \\ ALMT & 30.30 & 33.42 & 70.40 / 68.39 & 72.57 / **71.80** & 1.083 & 0.498 & 40.92 & 41.64 & **76.64** / 77.54 & 77.14 / 78.03 & 0.674 & 0.481 \\ 
**LNLN** & **34.26** & **38.27** & **72.55** / **70.94** & **72.73** / 71.25 & **1.046** & **0.527** & 45.42 & 46.17 & 76.30 / **78.19** & **77.77** / **79.95** & 0.692 & 0.530 \\   

Table 2: Robustness comparison of the overall performance on MOSI and MOSEI datasets. Note: The smaller MAE indicates the better performance.

   Method & Acc-5 & Acc-3 & Acc-2 & F1 & MAE & Corr \\  MISA & 31.53 & 56.87 & 72.71 & 66.30 & 0.539 & 0.348 \\ Self-MM & 32.28 & 56.75 & 72.81 & 68.43 & 0.508 & 0.376 \\ MMIM & 31.81 & 52.76 & 69.86 & 66.21 & 0.544 & 0.339 \\ CENET & 22.29 & 53.17 & 68.13 & 57.90 & 0.589 & 0.107 \\ TETFN & 33.42 & 56.91 & **73.58** & 68.67 & **0.505** & 0.387 \\ TFR-Net & 26.52 & 52.89 & 68.13 & 58.70 & 0.661 & 0.169 \\ ALMT & 20.00 & 45.36 & 69.66 & 72.76 & 0.561 & 0.364 \\ 
**LNLN** & **34.64** & **57.14** & 72.73 & **79.43** & 0.514 & **0.397** \\   

Table 3: Robustness comparison of the overall performance on SIMS dataset. Note: The smaller MAE indicates the better performance.

Figure 2: Performance curves of various missing rates. (a), (b) and (c) are the F1 curves on MOSI, MOSEI, and SIMS, respectively. (d), (e) and (f) are the MAE curves on MOSI, MOSEI, and SIMS, respectively. Note: The smaller MAE indicates the better performance.

[MISSING_PAGE_FAIL:9]

the inputs in high missing rate scenarios because most of them are designed specifically for complete data (see Appendix A.6 for more details). However, some methods show relatively more robust performance in low missing rate scenarios and some specific scenarios (see Appendix A.8), such as Self-MM, TETFN, and ALMT. We believe that some of the ideas in these methods may be useful for future research on more robust MSA for incomplete data.

For example, the Unimodal Label Generation Module (ULGM) (Yu et al., 2021) used in both Self-MM and TETFN facilitates the modeling of incomplete data. Due to the random data missing, the temporal and structured affective information in the data is corrupted, making it difficult for the models to perceive consistency and variability information between different modalities. Using ULGM to generate pseudo-labels as an additional supervisory signal for the models may directly enable the model to learn the relationship between different modalities. For ALMT, we believe that its Adaptive Hyper-modality Learning (AHL) module facilitates the modeling of robust affective representations. AHL mainly uses the dominant modality to compute query vectors and applies multi-head attention to query useful sentiment cues from other modalities as a complement to the dominant modality. This process may reduce the difficulty of multimodal fusion by avoiding the introduction of sentiment-irrelevant information to some extent. Our proposed LNLN, based on this idea from ALMT, proves the effectiveness of the method.

Additionally, we explore the performance of these methods in modality missing scenarios, which is a special case of random data missing where the partial modality missing rate \(r=1.0\). We find that when language modality is missing, the performance of many models decreases significantly and converges to the same value. More details can be seen in Appendix A.8.

## 5 Conclusion and Future Work

In this paper, a novel Language-dominated Noise-resistant Learning Network (LNLN) is proposed for robust MSA. Due to the collaboration between Dominate Modality Correction (DMC) module, the Dominant Modality based Multimodal Learning (DMML) module, and the Reconstructor, LNLN can enhance the dominant modality to achieve superior performance in data missing scenarios with varying levels. Extensive evaluation shows that none of the existing methods can effectively model the data with high missing rates. The related analyses can provide suggestions for other researchers to better handle robust MSA. In the future, we will focus on improving the generalization of the model to handle different types of scenes and varying intensities of noise effectively.

## Limitations

We believe there are several limitations to the LNLN. 1) The LNLN achieves good performance in data missing scenarios, but is not always better than all other methods in the modality missing scenarios, which demonstrates the lack of multi-scene generalization capability of the LNLN. 2) The data in real-world scenarios is much more complex. In addition to the presence of missing data, other factors need to be considered, such as diverse cultural contexts, varying user behavior patterns, and the influence of platform-specific features on the data. These factors can introduce additional noise and variability, which may require further model adaptation and tuning to handle effectively. 3) Tuning the hyperparameters, particularly those related to the loss functions, can be challenging and may require more sophisticated methods to achieve optimal performance.

    &  \\   & Acc-7 & Acc-5 & Acc-2 & F1 & MAE & Corr \\ 
**LNLN** & **34.26** & **38.27** & **72.55 / 70.94** & 72.73 / **71.25** & **1.046** & 0.527 \\ w/o language & 16.30 & 16.31 & 54.88 / 55.79 & 60.33 / 63.10 & 1.399 & 0.033 \\ w/o audio & 33.51 & 38.10 & 72.31 / 70.54 & 73.67 / 71.10 & 1.064 & 0.535 \\ w/o vision & 33.53 & 38.15 & 72.32 / 70.58 & **73.68 / 71.14** & 1.064 & **0.536** \\   

Table 6: Effects of different modalities. Note: The smaller MAE indicates the better performance.