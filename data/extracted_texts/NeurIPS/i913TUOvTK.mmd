# Cinematic Mindscapes: High-quality Video Reconstruction from Brain Activity

Zijiao Chen

National University of Singapore

zijiao.chen@u.nus.edu

&Jiaxin Qing

The Chinese University of Hong Kong

jqing@ie.cuhk.edu.hk

Equal contributions

Equal contributionsCorresponding author

Juan Helen Zhou

National University of Singapore

helen.zhou@nus.edu.sg

_https://mind-video.com_

###### Abstract

Reconstructing human vision from brain activities has been an appealing task that helps to understand our cognitive process. Even though recent research has seen great success in reconstructing static images from non-invasive brain recordings, work on recovering continuous visual experiences in the form of videos is limited. In this work, we propose **MinD-Video** that learns spatiotemporal information from continuous fMRI data of the cerebral cortex progressively through masked brain modeling, multimodal contrastive learning with spatiotemporal attention, and co-training with an augmented Stable Diffusion model that incorporates network temporal inflation. We show that high-quality videos of arbitrary frame rates can be reconstructed with **MinD-Video** using adversarial guidance. The recovered videos were evaluated with various semantic and pixel-level metrics. We achieved an average accuracy of 85% in semantic classification tasks and 0.19 in structural similarity index (SSIM), outperforming the previous state-of-the-art by 45%. We also show that our model is biologically plausible and interpretable, reflecting established physiological processes.

## 1 Introduction

Life unfolds like a film reel, each moment seamlessly transitioning into the next, forming a "perpetual theater" of experiences. This dynamic narrative forms our perception, explored through the naturalistic paradigm, painting the brain as a moviegoer engrossed in the relentless film of experience. Understanding the information hidden within our complex brain activities is a big puzzle in cognitive neuroscience. The task of recreating human vision from brain recordings, especially using non-invasive tools like functional Magnetic Resonance Imaging (fMRI), is an exciting but difficult task. Non-invasive methods, while less intrusive, capture limited information, susceptible to various interferences like noise . Furthermore, the acquisition of neuroimaging data is a complex, costly process. Despite these complexities, progress has been made, notably in learning valuable fMRI features with limited fMRI-annotation pairs. Deep learning and representation learning have achieved significant results in visual class detections  and static image reconstruction , advancing our understanding of the vibrant, ever-changing spectacle of human perception.

Unlike still images, our vision is a continuous, diverse flow of scenes, motions, and objects. To recover dynamic visual experience, the challenge lies in the nature of fMRI, which measures blood oxygenation level dependent (BOLD) signals and captures snapshots of brain activity every few seconds. Each fMRI scanessentially represents an "average" of brain activity during the snapshot. In contrast, a typical video has about 30 frames per second (FPS). If an fMRI frame takes 2 seconds, during that time, 60 video frames - potentially containing various objects, motions, and scene changes - are presented as visual stimuli. Thus, decoding fMRI and recovering videos at an FPS much higher than the fMRI's temporal resolution is a complex task.

Hemodynamic response (HR)  refers to the lags between neuronal events and activation in BOLD signals. When a visual stimulus is presented, the recorded BOLD signal will have certain delays with respect to the stimulus event. Moreover, the HR varies across subjects and brain regions . Thus, the common practice that shifts the fMRI by a fixed number in time to compensate for the HR would be sub-optimal.

In this work, we present **MinD-Video**, a two-module pipeline designed to bridge the gap between image and video brain decoding. Our model progressively learns from brain signals, gaining a deeper understanding of the semantic space through multiple stages in the first module. Initially, we leverage large-scale unsupervised learning with masked brain modeling to learn general visual fMRI features. We then distill semantic-related features using the multimodality of the annotated dataset, training the fMRI encoder in the Contrastive Language-Image Pre-Training (CLIP) space with contrastive learning. In the second module, the learned features are fine-tuned through co-training with an augmented stable diffusion model, which is specifically tailored for video generation under fMRI guidance. Our contributions are summarized as follows:

* We introduced a flexible and adaptable brain decoding pipeline decoupled into two modules: an fMRI encoder and an augmented stable diffusion model, trained separately and finetuned together.
* We designed a progressive learning scheme where the encoder learns brain features through multiple stages, including multimodal contrastive learning with spatiotemporal attention for windowed fMRI.
* We augmented the stable diffusion model for scene-dynamic video generation with near-frame attention. We also designed adversarial guidance for distinguishable fMRI conditioning.
* We recovered high-quality videos with accurate semantics, e.g., motions and scene dynamics. Results are evaluated with semantic and pixel metrics at video and frame levels. An accuracy of 85% is achieved in semantic metrics and 0.19 in SSIM, outperforming the previous state-of-the-art approaches by 45%.
* The attention analysis revealed mapping to the visual cortex and higher cognitive networks, suggesting our model is biologically plausible and interpretable.

## 2 Background

**Image Reconstruction** Image reconstruction from fMRI was first explored in , which showed that hierarchical image features and semantic classes could be decoded from the fMRI data collected when the participants were looking at a static visual stimulus. Authors in [5; 6] designed a separable autoencoder that enables self-supervised learning in fMRI and images to increase training data. Based on a similar philosophy,  proposed to perform self-supervised learning on a large-scale fMRI dataset using masked data modeling as a pretext task. Using a stable diffusion model as a generative prior and the pre-trained fMRI features as conditions,  reconstructed high-fidelity images with high semantic correspondence to the groundtruth stimuli.

**Video Reconstruction** The conventional method formulated the video reconstruction as multiple image reconstructions , leading to low frame rates and frame inconsistency. Nonetheless,  showed that low-level image features and classes can also be decoded from fMRI collected with video stimulus. Using

Figure 1: **Brain decoding & video reconstruction**. We propose a progressive learning approach to recover continuous visual experience from fMRI. High-quality videos with accurate semantics and motions are reconstructed.

fMRI representations encoded with a linear layer as conditions,  generated higher quality and frame rate videos with a conditional video GAN. However, the results are limited by data scarcity, especially for GAN training, which generally requires a large amount of data.  took a similar approach as , which relied on the same separable autoencoder that enables self-supervised learning. Even though better results were achieved than , the generated videos were of low visual fidelity and semantic meanings. In addition, recent work by  utilizes contrastive learning to derive latent embeddings from mice neural data, enabling the generation of videos based on these learned representations.

**MBM** Masked brain modeling (MBM) is a pre-text task that enables self-supervised learning in a large fMRI dataset proposed in , aiming to build a brain foundation model. It learns general features of fMRI by trying to recover masked data from the remainings, similar to the GPT  and MAE , after which knowledge can be distilled and transferred to a downstream task with limited data and a few-step tuning [17; 15; 16; 7].

**CLIP** Contrastive Language-Image Pre-Training (CLIP) is a pre-training technique that builds a shared latent space for images and natural languages by large-scale contrastive learning . The training aims to minimize the cosine distance of paired image and text latent while maximizing permutations of pairs within a batch. The shared latent space (CLIP space) contains rich semantic information on both images and texts.

**Stable Diffusion** Diffusion models are emerging probabilistic generative models defined by a reversible Markov chain [19; 20]. As a variant, stable diffusion generates a compressed version of the data (data latent) instead of generating the data directly. As it works in the data latent space, the computational requirement is significantly reduced, and higher-quality images with more details can be generated in the latent space . Due to its high generative quality, it has been studied in various brain decoding tasks [7; 8].

## 3 Methodology

### Motivation and Overview

Aiming for a flexible design, MinD-Video is decoupled into two modules: an fMRI encoder and a video generative model, as illustrated in Fig. 2. These two modules are trained separately and then finetuned together, which allows for easy adaption of new models if better architectures of either one are available. As a representation learning model, the encoder in the first module transfers the pre-processed fMRI into embeddings, which are used as a condition for video generations. For this purpose, the embedding should have the following traits: 1) It should contain rich and compact information about the visual stimulus presented during the scan. 2) It should be close to the embedding domain with which the generative model is trained. When designing the generative model, it is essential that the model produces not only diverse, high-quality videos with high computational efficiency but also handles potential scene transitions, mirroring the dynamic visual stimuli experienced during scans.

Figure 2: **MinD-Video Overview**. Our method has two modules that are trained separately and then finetuned together. The fMRI encoder progressively learns fMRI features through multiple stages, including MBM pre-training and multimodal contrastive learning. A spatiotemporal attention is designed to process multiple fMRI in a sliding window. The augmented Stable Diffusion is trained with videos and then tuned with the fMRI encoder using annotated data.

### The fMRI Pre-processing

The fMRI captures whole-brain activity with BOLD signals (voxels). Each voxel is assigned to a region of interest (ROI) for focused analysis. Here, we concentrate on voxels activated during visual stimuli. There are two ways to define the ROIs: one uses a pre-defined parcellation such as  and  to obtain the visual cortex; the other relies on statistical tests to identify activated voxels during stimuli. Our large-scale pre-training is based on the parcellation in , while statistical tests are employed for the target dataset, i.e., the Wen (2018)  dataset containing fMRI-video pairs. To determine activated regions, we calculate intra-subject reproducibility of each voxel, correlating fMRI data across multiple viewings. The correlation coefficients are then converted to z-scores and averaged. We compute statistical significance using a one-sample t-test (P<0.01, DOF=17, Bonferroni correction). We selected the top 50% of the most significant voxels after the statistical test. Notice that most of the identified voxels are from the visual cortex.

### Progressive Learning and fMRI Encoder

Progressive learning is used as an efficient training scheme where general knowledge is learned first, and then more task-specific knowledge is distilled through finetuning [24; 25; 26]. To generate meaningful embeddings specific for visual decoding, we design a progressive learning pipeline, which learns fMRI features through multiple stages, starting from general features to more specific and semantic-related features. We will show that the progressive learning process is reflected biologically in the evolution of fMRI attention maps.

**Large-scale Pre-training with MBM** Similar to , a large-scale pre-training with masked brain modeling (MBM) is used to learn general features of the visual cortex. With the same setup, an asymmetric vision-transformer-based autoencoder [27; 16] is trained on the Human Connectome Project  with the visual cortex (V1 to V4) defined by . Specifically, fMRI data of the visual cortex is rearranged from 3D into 1D space in the order of visual processing hierarchy, which is then divided into patches of the same size. The patches will be transformed into tokens, and a large portion (\(\)75%) of the tokens will be randomly masked in the encoder during training. With the autoencoder architecture, a simple decoder aims to recover the masked tokens from the unmasked token embeddings generated by the encoder. The main idea behind the MBM is that if the training objective can be achieved with high accuracy using a simple decoder, the token embeddings generated by the encoder will be a rich and compact description of the original fMRI data. We refer readers to  for detailed descriptions and reasonings of the MBM.

**Spatiotemporal Attention for Windowed fMRI** For the purpose of image reconstruction, the original fMRI encoder in  shifts the fMRI data by 6s, which is then averaged every 9 seconds and processed individually. This process only considers the spatial information in its attention layers. In the video reconstruction, if we directly map one fMRI to the video frames presented (e.g., 6 frames), the video reconstruction task can be formulated as a one-to-one decoding task, where each set of fMRI data corresponds to 6 frames. We call each {fMRI-frames} pair a _fMRI frame window_. However, this direct mapping is sub-optimal because of the time delay between brain activity and the associated BOLD signals in fMRI data due to the nature of hemodynamic response. Thus, when a visual stimulus (i.e., a video frame) is presented at time \(t\), the fMRI data obtained at \(t\) may not contain complete information about this frame. Namely, a lag occurs between the presented visual stimulus and the underlying information recorded by fMRI. This phenomenon is depicted in Fig. 3.

The hemodynamic response function (HRF) is usually used to model the relationship between neural activity and BOLD signals . In an LTI system, the signal \(y(t)\) is represented as the convolution of a stimulus function \(s(t)\) and the HR \(h(t)\), i.e., \(y(t)\!=\!(s\!*\!h)(t)\). The \(h(t)\) is often modeled with a linear combination of some basis functions, which can be collated into a matrix form: \(\!=\!+\), where \(\) represents the observed data, \(\) is a vector of regression coefficients, and \(\) is a vector of unexplained error values. However, \(\) varies significantly across individuals and sessions due to age, cognitive state, and specific visual stimuli, which influence the firing rate, onset latency, and neuronal activity duration. These variations impact the estimation of \(\) and ultimately affect the accuracy of the fMRI-based analysis. Gener

Figure 3: Due to hemodynamic response, the BOLD signal (blue) lags a few seconds behind the visual stimulus (red), causing a discrepancy between fMRI and the stimulus.

ally, there are two ways to address individual variations: using personalized HRF models or developing algorithms that adapt to each participant. We choose the latter due to its superior flexibility and robustness.

Aiming to obtain sufficient information to decode each scan window and account for the HR, we propose a spatiotemporal attention layer to process multiple fMRI frames in a sliding window. Consider a sliding window defined as \(}\!=\!\{x_{t},x_{t+1},\)...,\(x_{t+w-1}\}\), where \(x_{t}\!\!^{n p b}\) are token embeddings of the fMRI at \(t\) and \(n,p,b\) are the batch size, patch size, and embedding dimension, respectively. So we have \(}\!\!^{n w p b}\), where \(w\) is the window size. Recall that the attention is given by \(\!=\!\!(}{})\). To calculate spatial attention, we use the network inflation trick , where we merge the first two dimensions of \(}\) and obtain \(^{spat}}\!\!^{nw p b}\). Then the query and key are calculated in Eq. (1) as

\[Q\!=\!^{spat}}\!\!W_{spat}^{Q}, K\!=\!^{spat}}\! \!W_{spat}^{K}.\] (1)

Likewise, we merge the first and the third dimension of \(}\) to calculate the temporal attention, obtaining \(^{temp}}\!\!^{np w b}\). Again, the query and key are calculated in Eq. (2) with

\[Q\!=\!^{temp}}\!\!W_{temp}^{Q}, K\!=\!^{temp}}\! \!W_{temp}^{K}.\] (2)

The spatial attention learns correlations among the fMRI tokens, describing the spatial correlations of the fMRI patches. Then the temporal attention learns the correlations of fMRI from the sliding window, including sufficient information to cover the lag due to HR, as illustrated in the "fMRI Attention" in Fig. 2.

**Multimodal Contrastive Learning** Recall that the fMRI encoder is pre-trained to learn general features of the visual cortex, and then it is augmented with temporal attention heads to process a sliding window of fMRI. In this step, we further train the augmented encoder with {fMRI, image, caption} triplets and pull the fMRI embeddings closer to a shared CLIP space containing rich semantic information. Additionally, the generative model is usually pre-trained with text conditioning. Thus, pulling the fMRI embeddings closer to the text-image shared space ensures its understandability by the generative model during conditioning.

Firstly, videos in the training set are downsampled to a smaller frame rate (3FPS). Each frame is then captioned with BLIP , which creates the {image, text} pairs. With the CLIP text encoder and image encoder being fixed, the CLIP loss  is calculated for fMRI-image and fMRI-text, respectively. Denote the pooled text embedding, image embedding, and fMRI embedding by \(emb_{t},emb_{i},emb_{f}\!\!^{n b}\). The contrastive language-image-fMRI loss is given by

\[\!=\!(_{}(emb_{f},emb_{t})\!+\! _{}(emb_{f},emb_{i}))/2,\] (3)

where \(_{}(a,b)\!=\!( a\! \!b^{},\,[0,\!1,\)...,\(n])\), with \(\) being a scaling factor. Extra care is needed to reduce similar frames in a batch for better contrastive pairs. From Eq. (3), we see that the loss largely depends on the batch size \(n\). Thus, a large \(n\) with data augmentation on all modalities is appreciated.

### Video Generative Module

The Stable Diffusion model  is used as the base generative model considering its excellence in generation quality, computational requirements, and weights availability. However, as the stable diffusion model is an image-generative model, temporal constraints need to be applied in order for video generation.

**Scene-Dynamic Sparse Causal (SC) Attention** Authors in  use a network inflation trick with sparse temporal attention to adapt the stable diffusion to a video generative model. Specifically, the sparse temporal attention effectively conditions each frame on its previous frame and the first frame, which ensures frame consistency and also keeps the scene unchanged. However, the human vision consists of possible scene changes, so the video generation should also be scene-dynamic. Thus, we relax the constraint in  and condition each frame on its previous two frames, ensuring the video smoothness while allowing scene dynamics. Using notations from , the SC attention is calculated with the query, key, and value given by

\[Q\!=\!W^{Q}\!\!z_{v_{i}}, K\!=\!W^{K}\!\![z_{v_{i-2}},z_{v_{i-1} }], V\!=\!W^{V}\!\![z_{v_{i-2}},z_{v_{i-1}}],\] (4)

where \(z_{v_{i}}\) denotes the latent of the \(i\)-th frame during the generation.

**Adversarial Guidance for fMRI** Classifier-free guidance is widely used in the conditional sampling of diffusion models for its flexibility and generation diversity, where the noise update function is given by

\[_{}(z_{t},c)\!=\!_{}(z_{t})\!+\!s(_{ }(z_{t},c)\!-\!_{}(z_{t})),\] (5)where \(c\) is the condition, \(s\) is the guidance scale and \(_{}()\) is a score estimator implemented with UNet . Interestingly, Eq. (5) can be changed to an adversarial guidance version 

\[_{}(z_{t},\,c,\,)\!=\!_{}(z_{t},\,)\!+\!s(_{}(z_{t},\,c)\!-\!_{}(z_{t},)),\] (6)

where \(\) is the negative guidance. In effect, generated contents can be controlled through "what to generate" (positive guidance) and "what not to generate" (negative guidance). When \(\) is a null condition, the noise update function falls back to Eq. (5), the regular classifier-free guidance. In order to generate diverse videos for different fMRI, guaranteeing the distinguishability of the inputs, we average all fMRI in the testing set and use the averaged one as the negative condition. Specifically, for each fMRI input, the fMRI encoder will generate an unpooled token embedding \(x\!\!^{l b}\), where \(l\) is the latent channel number. Denote the averaged fMRI as \(\). We have the noise update function \(_{}(z_{t},\,x,)\!=\!_{}(z_{t},\,)\!+\!s(_{}(z_{t},\,x)\!-\!_{}(z_{t},\,))\).

**Divide and Refine** With a decoupled structure, two modules are trained separately: the **fMRI encoder** is trained in a large-scale dataset and then tuned in the target dataset with contrastive learning; the **generative module** is trained with videos and captions from the target dataset. In the second phase, two modules are tuned together with fMRI-video pairs, where the encoder and part of the generative model are trained. Different from , we tune the whole self-attention, cross-attention, and temporal-attention heads instead of only the query projectors, as a different modality is used for conditioning. The second phase is also the last stage of encoder progressive learning, after which the encoder finally generates token embeddings that contain rich semantic information and are easy to understand by the generative model.

### Learning from the Brain - Interpretability

Our objectives extend beyond brain decoding and reconstruction. We also aim to understand the biological principles of the decoding process. To this end, we visualize average attention maps from the first, middle, and last layers of the fMRI encoder across all testing samples. This approach allows us to observe the transition from capturing local relations in early layers to recognizing more global, abstract features in deeper layers . Additionally, attention maps are visualized for models in different learning stages: large-scale pre-training, contrastive learning, and co-training. By projecting attention back to brain surface maps, we can easily visualize each brain region's contributions and the learning progress through each stage.

## 4 Experiments

### Datasets

**Pre-training dataset** Human Connectome Project (HCP) 1200 Subject Release : For our upstream pre-training dataset, we employed resting-state and task-evoked fMRI data from the HCP. Building upon , we obtained 600,000 fMRI segments from a substantial amount of fMRI scan data.

**Paired fMRI-Video dataset** A publicly available benchmark fMRI-video dataset  was used, comprising fMRI and video clips. The fMRI were collected using a 3T MRI scanner at a TR of 2 seconds with three subjects. The training data included 18 segments of 8-minute video clips, totaling 2.4 video hours and yielding 4,320 paired training examples. The test data comprised 5 segments of 8-minute video clips, resulting in 40 minutes of test video and 1,200 test fMRIs. The video stimuli were diverse, covering animals, humans, and natural scenery, and featured varying lengths at a temporal resolution of 30 FPS.

### Implementation Details

The original videos are downsampled from 30 FPS to 3 FPS for efficient training and testing, leading to 6 frames per fMRI frame. In our implementation, we reconstruct a video of 2 seconds (6 frames) from a sliding window of fMRI frames. However, thanks to the spatiotemporal attention head design that encodes multiple fMRI at once, our method can reconstruct longer videos from more fMRI frames if more GPU memory is available.

A ViT-based fMRI encoder with a patch size of 16, a depth of 24, and an embedding dimension of 1024 is used. After pre-training with a mask ratio of 0.75, the encoder will be augmented with a projection head that projects the token embedding into the dimension of \(77\!\!768\). The Stable Diffusion V1-5  trained at the resolution of \(512\!\!512\) is used. But we tune the augmented Stable Diffusion for video generations at the resolution of \(256\!\!256\) with 3 FPS. Notice that the FPS and image are downsampled for efficient experiments, and our method can also work with full temporal and spatial resolution. All parameters in the fMRI encoder pre-training are the same as  with eight RTX3090, while other stages are trained with one RTX3090.

The fMRI encoder pre-training on a large-scale dataset is the most resource-consuming part, but only one pre-training is needed for different subjects. This is in contrast to the other downstream stages. Multimodal contrastive learning and co-training are performed for each subject individually due to individual variability. The inference is performed with \(200\) DDIM  steps. See Supplementary for more details.

### Evaluation Metrics

Following prior studies, we utilize both frame-based and video-based metrics. Frame-based metrics evaluate each frame individually, providing a snapshot evaluation, whereas video-based metrics assess sequences of frames, encapsulating the dynamics across frames. Both are used for a comprehensive analysis. Unless stated otherwise, all test set videos are used for evaluating the three subjects.

**Frame-based Metrics** Our frame-based metrics are divided into two classes, pixel-level metrics and semantics-level metrics. We use the structural similarity index measure (SSIM)  as the pixel-level metric and the N-way top-K accuracy classification test as the semantics-level metric. Specifically, for each frame in a scan window, we calculate the SSIM and classification test accuracy with respect to the groundtruth frame. To perform the classification test, we basically compare the classification results of the groundtruth (GT) and the predicted frame (PF) using an ImageNet classifier. If the GT class3 is within the top-K probability of the PF classification results from N randomly picked classes, including the GT class, we declare a successful trial. The test is repeated for 100 times, and the average success rate is reported.

**Video-based Metric** The video-based metric measures the video semantics using the classification test as well, except that a video classifier is used. The video classifier based on VideoMAE  is trained on Kinetics-400 , an annotated video dataset with 400 classes, including motions, human interactions, etc.

## 5 Results

We compare our method against three fMRI-video baselines: Wen et al. (2018) , Wang et al. (2022)  and Kupershmidt et al. (2022) . Visual comparisons are shown in Fig. 4, and quantitative comparisons are shown in Fig. 6, where publicly available data and samples are used for comparison. As shown, we generate high-quality videos with more semantically meaningful content. Following the literature, we evaluate the SSIM of Subject 1 with the first testing video, achieving a score of 0.186, outperforming the state-of-the-art by 45%. When comparing with Kupershmidt et al. (2022), we evaluate all test videos for different subjects, and our method outperforms by 35% on average, as shown in Fig. 6. Using semantic-level metrics, our method achieves a success rate of 0.849 and 0.2 in 2-way and 50-way top-1 accuracy classification tests, respectively, with the video classifier. The image classifier gives a success rate of 0.795 and 0.19, respectively, with the same tests, which significantly surpasses the chance level of these two tests (2-way: 0.5, 50-way: 0.02). Full results are shown in Tab. 1. We also compare our results with an image-fMRI model by Chen et al. (2023) . An image is produced for each fMRI, samples of which are shown in Fig. 4 with the "walking man" as the groundtruth. Even though the results and groundtruth are semantically matching, frame consistency and image quality are not satisfying. A lag due to the hemodynamic response is also observed. The first frame actually corresponds to the previous groundtruth.

**Different Subjects** After fMRI pre-processing, each subject varies in the size of ROIs, where Subject 1, 2 and 3 have 6016, 6224, and 3744 voxels, respectively. As Subject 3 has only half the voxels of the others, a larger batch size can be used during contrastive learning, which may lead to better results as shown in Tab. 1. Nonetheless, all subjects are consistent in both numeric and visual evaluations (See Supplementary).

**Recovered Semantics** In the video reconstruction, we define the semantics as the objects, animals, persons, and scenes in the videos, as well as the motions and scene dynamics, e.g., people running, fast-moving scenes, close-up scenes, long-shot scenes, etc. We show that even though the fMRI has a low temporal resolution, it contains enough information to recover the mentioned semantics. Fig. 5 shows a few examples of reconstructed frames using our method. Firstly, we can see that the basic objects, animals, persons, and scene types can be well recovered. More importantly, the motions, such as running, dancing, and singing,and the scene dynamics, such as the close-up of a person, the fast-motion scenes, and the long-shot scene of a city view, can also be reconstructed correctly. This result is also reflected in our numerical metrics, which consider both frame semantics and video semantics, including various categories of motions and scenes.

**Abalations** We test our method using different window sizes, starting from a window size of 1 up to 3. When the window size is one, the fMRI encoder falls back to a normal MBM encoder in . Tab. 1 shows that when all other parameters are fixed, a window size of 2 gives the best performance in general, which is reasonable as the hemodynamic response usually will not be longer than two scan windows. Additionally, we also test the effectiveness of multimodal contrastive learning. As shown in Tab. 1, without contrastive learning, the temporal structure of the window size is not sufficient to capture the temporal structure of the window.

    &  &  \\   &  &  &  \\   & 2-way\(\) & 50-way\(\) & 2-way\(\) & 50-way\(\) & SSIM\(\) \\ 
**Full Model** & \(_{ 0.03}\) & \(_{ 0.02}\) & \(_{ 0.03}\) & \(_{ 0.01}\) & \(\) \\   Window Size = 1 & 0.851\({}_{ 0.03}\) & 0.195\({}_{ 0.02}\) & 0.759\({}_{ 0.03}\) & 0.165\({}_{ 0.01}\) & 0.169 \\ Window Size = 3 & 0.826\({}_{ 0.03}\) & 0.161\({}_{ 0.01}\) & 0.765\({}_{ 0.03}\) & 0.137\({}_{ 0.01}\) & 0.161 \\ w/o Contrastive & 0.844\({}_{ 0.03}\) & 0.157\({}_{ 0.02}\) & 0.750\({}_{ 0.03}\) & 0.088\({}_{ 0.07}\) & 0.135 \\ Text-fMRI Contra & 0.839\({}_{ 0.03}\) & 0.185\({}_{ 0.01}\) & 0.783\({}_{ 0.03}\) & 0.154\({}_{ 0.01}\) & 0.164 \\ Img-fMRI Contra & 0.845\({}_{ 0.03}\) & 0.189\({}_{ 0.01}\) & 0.783\({}_{ 0.03}\) & 0.151\({}_{ 0.01}\) & 0.164 \\ w/o AG & 0.859\({}_{ 0.03}\) & 0.198\({}_{ 0.02}\) & 0.775\({}_{ 0.03}\) & 0.117\({}_{ 0.01}\) & 0.152 \\   Subject 2 & 0.841\({}_{ 0.03}\) & 0.173\({}_{ 0.02}\) & 0.784\({}_{ 0.03}\) & 0.158\({}_{ 0.13}\) & 0.171 \\ Subject 3 & 0.846\({}_{ 0.03}\) & 0.216\({}_{ 0.02}\) & 0.812\({}_{ 0.03}\) & 0.193\({}_{ 0.01}\) & 0.187 \\   

Table 1: **Ablation study** on window sizes, multimodal contrastive learning, and adversarial guidance (AG). Evaluations on different subjects are also shown. Full Model: win=2, Sub 1. Colors reflect statistical significance (two-sample t-test) compared to the Full Model. \(p<0.0001\) (purple):p \(<0.01\) (pink):p \(<0.05\) (yellow):p \(>0.05\) (green)

Figure 4: **Compare with Benchmarks**. We compare our results with the samples provided in the previous literature. Our method generates samples that are more semantically meaningful and match with the groundtruth.

Figure 5: **Reconstruction Diversity**. Various motions, scenes, persons, and animals can be correctly recovered. A sample with a scene transition is shown on the bottom right.

learning, the generation quality degrades significantly. When two modalities are used, either text-fMRI or image-fMRI, the performance is inferior to the full modalities used in contrastive learning. Actually, the reconstructed videos are visually worse than the full model (See Supplementary). Thus, it shows that the full progressive learning pipeline is crucial for the fMRI encoder to learn useful representations for this task. We also assess the reconstruction results without adversarial guidance. As a result, both numeric and visual evaluations decrease substantially. In fact, the generated videos can be highly similar sometimes, indicating that the negative guidance is critical in increasing the distinguishability of fMRI embeddings. See the Supplementary for more ablation studies and visual results.

### Interpretation Results

We summarize the attention analysis in Fig. 7. We present the sum of the normalized attention within Yeo17 networks  in the bar charts. Voxel-wise attention value is displayed on a brain flat map, where we see comprehensive structural attention throughout the whole region. The average attention across all testing samples and attention heads is computed, revealing three insights into how transformers decode fMRI data.

**Dominance of visual cortex:** The visual cortex emerges as the most influential region. This region, encompassing both the central (VisCent) and peripheral visual (VisPeri) fields, consistently attracts the highest attention across different layers and training stages (shown in Fig. 7B). In all cases, the visual cortex is always the top predictor, which aligns with prior research, emphasizing the vital role of the visual cortex in processing visual spatiotemporal information. However, the visual cortex is not the sole determinant of vision. Higher cognitive networks, such as the dorsal attention network (DorsAttn) involved in voluntary visuospatial attention control , and the default mode network (Default) associated with thoughts and recollations , also contribute to visual perceptions process  as shown in Fig. 7.

**Layer-dependent hierarchy:** The layers of the fMRI encoder function in a hierarchical manner, as shown in Fig. 7C, D and E. In the early layers of the network (panel A & C), we observe a focus on the structural information of the input data, marked by a clear segmentation of different brain regions by attention values, aligning with the visual processing hierarchy . As the network dives into deeper layers (panel D & E), the learned information becomes more dispersed. The distinction between regions diminishes, indicating a shift toward learning more holistic and abstract visual features in deeper layers.

**Learning semantics progressively:** To illustrate the learning progress of the fMRI encoder, we analyze the first-layer attention after all learning stages, as shown in panel B: before contrastive learning, after contrastive learning, and after co-training with the video generation model. We observe an increase in attention in higher cognitive networks and a decrease in the visual cortex as learning progresses. This indicates the encoder assimilates more semantic information as it evolves through each learning stage, improving the learning of cognitive-related features in the early layers.

## 6 Conclusion and Discussion

**Conclusion** We propose MinD-Video, which reconstructs high-quality videos with arbitrary frame rates from brain activities represented by fMRI signals. Starting from large-scale pre-training to multimodal

Figure 6: **SSIM Comparision**. Left: comparison with Kupershmidt et al. (2022)  and Wang et al. (2022)  on commonly available test samples (Subject 1, test video 1). Right: comparison with Kupershmidt et al. (2022) on all subjects, all test videos.

contrastive learning with augmented spatiotemporal attention, our fMRI encoder learns features from a sliding time window of fMRI progressively. Then, we finetune an augmented stable diffusion for video generations, which is then co-trained with the fMRI encoder. Finally, we show that with fMRI adversarial guidance, MinD-Video recovers videos with accurate semantics, motions, and scene dynamics compared with the groundtruth, establishing a new state-of-the-art in this domain. We also show from attention maps that the trained model decodes fMRI with reliable biological principles.

**Limitations** Even though our method reconstructs videos with matching semantics and scenes, the pixel-level accuracy is not satisfactory, as shown in visual samples (more in the Supplementary). We believe that the results can be improved by introducing pixel-level information to control the diffusion denoising process, in addition to the visual semantic information decoded from the brain. Multi-scale information from the brain recordings can be fused to provide a more detailed visual description for generations.

Another limitation lies in the inter-subject generalization ability. Our method is still within the intra-subject level, and the inter-subject generalization ability remains unexplored due to individual variations. Additionally, our method only uses less than 10% of the voxels from the cortex for reconstructions, while using the whole brain data remains unexploited.

**Broader Impacts** We believe that brain decoding has promising applications in brain-computer interfaces as large models develop, and it will be a non-negligible component of the study to understand our cognitive process. However, governmental regulations and efforts from research communities are required to ensure the privacy of one's biological data and avoid any malicious usage of this technology.

## 7 Acknowledgements

This work was supported in part by National Medical Research Council, Singapore (NMRC/OFLCG19May-0035 to J-H Zhou), RIE2020 AME Programmatic Grant from A*STAR (to J-H Zhou), Ministry of Education Tier 2 Grant and Yong Loo Lin School of Medicine Research Core Funding (to J-H Zhou), National University of Singapore, Singapore.

Figure 7: **Attention visualization**. We visualized the attention maps for different transformer layers (C, D, E) and learning stages (B) with bar charts and brain flat maps. Brain networks are marked on a brain surface map (A). Normalized attention values are shown.