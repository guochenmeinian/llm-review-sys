# Towards Better Dynamic Graph Learning:

New Architecture and Unified Library

 Le Yu, Leilei Sun, Bowen Du, Weifeng Lv

State Key Laboratory of Software Development Environment

School of Computer Science and Engineering

Beihang University

{yule,leileisun,dubowen,lwf}@buaa.edu.cn

Corresponding Author.

###### Abstract

We propose DyGFormer, a new Transformer-based architecture for dynamic graph learning. DyGFormer is conceptually simple and only needs to learn from nodes' historical first-hop interactions by: (i) a neighbor co-occurrence encoding scheme that explores the correlations of the source node and destination node based on their historical sequences; (ii) a patching technique that divides each sequence into multiple patches and feeds them to Transformer, allowing the model to effectively and efficiently benefit from longer histories. We also introduce DyGLib, a unified library with standard training pipelines, extensible coding interfaces, and comprehensive evaluating protocols to promote reproducible, scalable, and credible dynamic graph learning research. By performing exhaustive experiments on thirteen datasets for dynamic link prediction and dynamic node classification tasks, we find that DyGFormer achieves state-of-the-art performance on most of the datasets, demonstrating its effectiveness in capturing nodes' correlations and long-term temporal dependencies. Moreover, some results of baselines are inconsistent with previous reports, which may be caused by their diverse but less rigorous implementations, showing the importance of DyGLib. All the used resources are publicly available at https://github.com/yule-BUAA/DyGLib.

## 1 Introduction

Dynamic graphs denote entities as nodes and represent their interactions as links with timestamps , which can model many real-world scenarios such as social networks [28; 50; 2], user-item interaction systems [31; 15; 71; 72; 70], traffic networks [67; 63; 19; 4; 69], and physical systems [24; 47; 43]. In recent years, representation learning on dynamic graphs has become a trending research topic [26; 49; 65]. There are two main categories of existing methods: discrete-time [39; 17; 48; 11; 66] and continuous-time [64; 45; 60; 51; 12]. In this paper, we focus on the latter approaches because they are more flexible and effective than the formers and are being increasingly investigated.

Despite the rapid development of dynamic graph learning methods, they still suffer from two limitations. Firstly, most of them independently compute the temporal representations of nodes in an interaction without exploiting nodes' correlations, which are often indicative of future interactions. Moreover, existing methods learn at the interaction level and thus only work for nodes with fewer interactions. When nodes have longer histories, they require sampling strategies to truncate the interactions for feasible calculations of the computationally expensive modules like graph convolutions [55; 64; 45; 36; 9; 59], temporal random walks [60; 25] and sequential models [57; 12]. Though some approaches use memory networks [61; 53] to sequentially process interactions with affordable computational costs [28; 55; 45; 36; 59; 35], they are faced with the vanishing/exploding gradientsdue to the usage of recurrent neural networks [40; 57]. Therefore, we conclude that _previous methods lack the ability to capture either nodes' correlations or long-term temporal dependencies_.

Secondly, the training pipelines of different methods are inconsistent and often lead to poor reproducibility. Also, existing methods are implemented by diverse frameworks (e.g., Pytorch , Tensorflow , DGL , PyG , C++), making it time-consuming and difficult for researchers to quickly understand the algorithms and further dive into the core of dynamic graph learning. Although there exist some libraries for dynamic graph learning [18; 46; 74], they mainly focus on dynamic network embedding methods , discrete-time graph learning methods , or engineering techniques for training on large-scale dynamic graphs  (elaborated in Section 2). Currently, we find that _there are still no standard tools for continuous-time dynamic graph learning_.

In this paper, we aim to address the above drawbacks with two key technical contributions.

**We propose a new Transformer-based dynamic graph learning architecture (DyGFormer)**. DyGFormer is conceptually simple by solely learning from the sequences of nodes' historical first-hop interactions. To be specific, DyGFormer is designed with a neighbor co-occurrence encoding scheme, which encodes the appearing frequencies of each neighbor in the sequences of the source and destination nodes to explicitly explore their correlations. In order to capture long-term temporal dependencies, DyGFormer splits each node's sequence into multiple patches and feeds them to Transformer . This patching technique not only makes the model effectively benefit from longer histories via preserving local temporal proximities, but also efficiently reduces the computational complexity to a constant level that is irrelevant to the input sequence length.

**We present a unified continuous-time dynamic graph learning library (DyGLib)**. DyGLib is an open-source toolkit with standard training pipelines, extensible coding interfaces, and comprehensive evaluating strategies, aiming to foster standard, scalable, and reproducible dynamic graph learning research. DyGLib has integrated a variety of continuous-time dynamic graph learning methods as well as benchmark datasets from various domains. It trains all the methods via the same pipeline to eliminate the influence of different implementations and adopts a modularized design for developers to conveniently incorporate new datasets and algorithms based on their specific requirements. Moreover, DyGLib supports both dynamic link prediction and dynamic node classification tasks with exhaustive evaluating strategies to provide comprehensive comparisons of existing methods.

To evaluate the model performance, we conduct extensive experiments based on DyGLib, including dynamic link prediction under transductive and inductive settings with three negative sampling strategies as well as dynamic node classification. From the results, we observe that: (i) DyGFormer outperforms existing methods on most datasets, demonstrating its superiority in capturing nodes' correlations and long-term temporal dependencies; (ii) some findings of baselines are not in line with previous reports because of their varied pipelines and problematic implementations, which illustrates the necessity of introducing DyGLib. We also provide an in-depth analysis of the neighbor co-occurrence encoding and patching technique for a better understanding of DyGFormer.

## 2 Related Work

**Dynamic Graph Learning**. Representation learning on dynamic graphs has been widely studied in recent years [26; 49; 65]. Discrete-time methods manually divide the dynamic graph into a sequence of snapshots and apply static graph learning methods on each snapshot, which ignore the temporal order of nodes in each snapshot [39; 17; 48; 11; 66]. In contrast, continuous-time methods directly learn on the whole dynamic graph with temporal graph neural networks [55; 64; 45; 36; 9; 59], memory networks [28; 55; 45; 36; 59; 35], temporal random walks [60; 25] or sequential models [57; 12]. Although insightful, most existing dynamic graph learning methods neglect the correlations between two nodes in an interaction. They also fail to handle nodes with longer interactions due to unaffordable computational costs of complex modules or issues in optimizing models (e.g., the vanishing/exploding gradients). In this paper, we propose a new Transformer-based architecture to show the necessity of capturing nodes' correlations and long-term temporal dependencies, which is achieved by two designs: a neighbor co-occurrence encoding scheme and a patching technique.

**Transformer-based Applications in Various Fields**. Transformer  is an innovative model that employs the self-attention mechanism to handle sequential data, which has been successfully applied in a variety of domains, such as natural language processing [13; 33; 6], computer vision [7; 14; 34]and time series forecasting [30; 73; 62]. The idea of dividing the original data into patches as inputs of the Transformer has been attempted in some studies. ViT  splits an image into multiple patches and feeds the sequence of patches' linear embeddings into a Transformer, which achieves surprisingly good performance on image classification. PatchTST  divides a time series into subseries-level patches and calculates the patches by a channel-independent Transformer for long-term multivariate time series forecasting. In this work, we propose a patching technique to learn on dynamic graphs, which can provide our approach with the ability to handle nodes with longer histories.

**Graph Learning Library**. Currently, there exist many libraries for static graphs [5; 58; 16; 22; 8; 29; 32], but few for dynamic graph learning [18; 46; 74]. DynamicGEM  focuses on dynamic graph embedding methods, which just consider the graph topology and cannot leverage node features. PyTorch Geometric Temporal  implements discrete-time algorithms for spatiotemporal signal processing and is mainly applicable for nodes with aligned historical observations. TGL  trains on large-scale dynamic graphs with some engineering tricks. Though TGL has integrated some continuous-time methods, they are somewhat out-of-date, resulting in the lack of state-of-the-art models. Moreover, TGL is implemented by both PyTorch and C++, which needs additional compilation and increases the usage difficulty. In this paper, we present a unified continuous-time dynamic graph learning library with thorough baselines, diverse datasets, extensible implementations, and comprehensive evaluations to facilitate dynamic graph learning research.

## 3 Preliminaries

**Definition 1**.: _Dynamic Graph. We represent a dynamic graph as a sequence of non-decreasing chronological interactions \(=\{(u_{1},v_{1},t_{1}),(u_{2},v_{2},t_{2} ),\}\) with \(0 t_{1} t_{2}\), where \(u_{i},v_{i}\) denote the source node and destination node of the \(i\)-th link at timestamp \(t_{i}\). \(\) is the set of all the nodes. Each node \(u\) can be associated with node feature \(_{u}^{d_{N}}\), and each interaction \((u,v,t)\) has link feature \(_{u,v}^{t}^{d_{E}}\). \(d_{N}\) and \(d_{E}\) denote the dimensions of the node feature and link feature. If the graph is non-attributed, we simply set the node feature and link feature to zero vectors, i.e., \(_{u}=\) and \(_{u,v}^{t}=\)._

**Definition 2**.: _Problem Formalization. Given the source node \(u\), destination node \(v\), timestamp \(t\), and historical interactions before \(t\), i.e., \(\{(u^{},v^{},t^{})|t^{}<t.\}\), representation learning on dynamic graph aims to design a model to learn time-aware representations \(_{u}^{t}^{d}\) and \(_{v}^{t}^{d}\) for \(u\) and \(v\) with \(d\) as the dimension. We validate the effectiveness of the learned representations via two common tasks in dynamic graph learning: (i) dynamic link prediction, which predicts whether \(u\) and \(v\) are connected at \(t\); (ii) dynamic node classification, which infers the state of \(u\) or \(v\) at \(t\)._

## 4 New Architecture and Unified Library

### DyGFormer: Transformer-based Architecture for Dynamic Graph Learning

The framework of our DyGFormer is shown in Figure 1, which employs Transformer  as the backbone. Given an interaction \((u,v,t)\), we first extract historical first-hop interactions of source node \(u\) and destination node \(v\) before timestamp \(t\) and obtain two interaction sequences \(_{u}^{t}\) and \(_{v}^{t}\). Next, in addition to computing the encodings of neighbors, links, and time intervals for each sequence, we also encode the frequencies of every neighbor's appearances in both \(_{u}^{t}\) and \(_{v}^{t}\) to exploit the correlations between \(u\) and \(v\), resulting in four encoding sequences for \(u\)/\(v\) in total. Then, we divide each encoding sequence into multiple patches and feed all the patches into a Transformer for capturing long-term temporal dependencies. Finally, the outputs of the Transformer are averaged to derive time-aware representations of \(u\) and \(v\) at timestamp \(t\) (i.e., \(_{u}^{t}\) and \(_{v}^{t}\)), which can be applied in various downstream tasks like dynamic link prediction and dynamic node classification.

**Learning from Historical First-hop Interactions**. Unlike most previous methods that require nodes' historical interactions from multiple hops (e.g., DyRep , TGAT , TGN , CAWN ), we only learn from the sequences of nodes' historical first-hop interactions, turning the dynamic graph learning task into a simpler sequence learning problem. Mathematically, given an interaction \((u,v,t)\), for source node \(u\) and destination node \(v\), we obtain the sequences that involve first-hop interactions of \(u\) and \(v\) before timestamp \(t\), which are denoted by \(_{u}^{t}=\{(u,u^{},t^{})|t^{}<t \}\{(u^{},u,t^{})|t^{}<t \}.\) and \(_{v}^{t}=\{(v,v^{},t^{})|t^{}<t \}\{(v^{},v,t^{})|t^{}<t \}\), respectively.

**Encoding Neighbors, Links, and Time Intervals.** For source node \(u\), we retrieve the features of involved neighbors and links in sequence \(_{u}^{t}\) based on the given features to represent their encodings, which are denoted by \(_{u,N}^{t}^{|_{u}^{t}| d_{N}}\) and \(_{u,E}^{t}^{|_{u}^{t}| d_{E}}\). Following , we learn the periodic temporal patterns by encoding the time interval \( t^{}=t-t^{}\) via \(}[(w_{1} t^{}),( w_{1} t^{}),,(w_{d_{T}} t^{} ),(w_{d_{T}} t^{})]}\), where \(w_{1},,w_{d_{T}}\) are trainable parameters. \(d_{T}\) is the encoding dimension. The time interval encodings of interactions in \(_{u}^{t}\) is denoted by \(_{u,T}^{t}^{|_{u}^{t}| d_{T}}\). We use the same process to get the corresponding encodings for destination node \(v\), i.e., \(_{v,N}^{t}^{|_{v}^{t}| d_{N}}\), \(_{v,E}^{t}^{|_{v}^{t}| d_{E}}\), and \(_{v,T}^{t}^{|_{v}^{t}| d_{T}}\).

**Neighbor Co-occurrence Encoding Scheme**. Existing methods separately compute representations of node \(u\) and \(v\) without modeling their correlations. We present a neighbor co-occurrence encoding scheme to tackle this issue, which assumes the appearing frequency of a neighbor in a sequence indicates its importance, and the occurrences of a neighbor in sequences of \(u\) and \(v\) (i.e., co-occurrence) could reflect the correlations between \(u\) and \(v\). That is to say, if \(u\) and \(v\) have more common historical neighbors in their sequences, they are more likely to interact in the future.

Formally, for each neighbor in the interaction sequence \(_{u}^{t}\) and \(_{v}^{t}\), we count its occurrences in both \(_{u}^{t}\) and \(_{v}^{t}\), and derive a two-dimensional vector. By packing the vectors of all the neighbors together, we can get the neighbor co-occurrence features for \(u\) and \(v\), which are represented by \(_{u}^{t}^{|_{u}^{t}| 2}\) and \(_{v}^{t}^{|_{v}^{t}| 2}\). For example, suppose the historical neighbors of \(u\) and \(v\) are \(\{a,b,a\}\) and \(\{b,b,a,c\}\). The appearing frequencies of \(a\), \(b\), and \(c\) in _u/u_'s historical interactions are 21, 1/12, and 0/1, respectively. Therefore, the neighbor co-occurrence features of \(u\) and \(v\) are denoted by \(_{u}^{t}=[[2,1],[1,2],[2,1]]^ {}\) and \(_{v}^{t}=[[1,2],[1,2],[2,1],[ 0,1]]^{}\). Then, we apply a function \(f()\) to encode the neighbor co-occurrence features by

\[_{*,C}^{t}=f(_{*}^{t}[:,0])+f(_{ *}^{t}[:,1])^{|_{*}^{t}| d_{C}},\] (1)

where \(*\) could be \(u\) or \(v\). The input and output dimensions of \(f()\) are 1 and \(d_{C}\). In this paper, we implement \(f()\) by a two-layer perceptron with ReLU activation . It is important to note that the neighbor co-occurrence encoding scheme is general and can be easily integrated into some dynamic graph learning methods for better results. We will demonstrate its generalizability in Section 5.3.

**Patching Technique**. Instead of focusing on the interaction level, we divide the encoding sequence into multiple non-overlapping patches to break through the bottleneck of existing methods in capturing long-term temporal dependencies. Let \(P\) denote the patch size. Each patch is composed of \(P\) temporally adjacent interactions with flattened encodings and can preserve local temporal proximities. Take the patching of \(_{u,N}^{t}^{|_{u}^{t}| d_{N}}\) as an example. \(_{u,N}^{t}\) will be divided into \(l_{u}^{t}=_{u}^{t}|}{P}\) patches in total (note that we will pad \(_{u,N}^{t}\) if its length \(|_{u}^{t}|\) cannot be divided by \(P\)), and the patched encoding is represented by \(_{u,N}^{t}^{l_{u}^{t} d_{N} P}\). Similarly, we can also get the patched encodings \(_{u,E}^{t}^{l_{u}^{t} d_{E} P}\), \(_{u,T}^{t}^{l_{u}^{t} d_{T} P}\), \(_{u,C}^{t}^{l_{u}^{t} d_{C} P}\), \(_{v,N}^{t}^{l_{v}^{t} d_{N} P}\), \(_{v,E}^{t}^{l_{v}^{t} d_{E} P}\), \(_{v,T}^{t}^{l_{v}^{t} d_{T} P}\), and \(_{v,C}^{t}^{l_{v}^{t} d_{C} P}\). Note that when \(|_{u}^{t}|\) becomes

Figure 1: Framework of the proposed model.

longer, we will correspondingly increase \(P\), making the number of patches (i.e., \(l_{u}^{t}\) and \(l_{v}^{t}\)) at a constant level to reduce the computational cost.

**Transformer Encoder**. We first align the patched encodings to the same dimension \(d\) with trainable weight \(_{*}^{d_{*} P d}\) and \(_{*}^{d}\) to obtain \(_{u,*}^{t}^{l_{u}^{t} d}\) and \(_{v,*}^{t}^{l_{v}^{t} d}\), where \(*\) could be \(N\), \(E\), \(T\) or \(C\). To be specific, the alignments are realized by

\[_{u,*}^{t}=_{u,*}^{t}_{*}+_{*}^{l_{u}^{t}  d},_{v,*}^{t}=_{v,*}^{t}_{*}+_{*}^{ l_{v}^{t} d}.\] (2)

Then, we concatenate the aligned encodings of \(u\) and \(v\), and get \(_{u}^{t}=_{u,N}^{t}\|_{u,E}^{t}\|_{u,T}^{t}\|_{u,C}^{t}^{l_{u}^{t} d}\) and \(_{v}^{t}=_{v,N}^{t}\|_{v,E}^{t}\|_{v,T}^{t}\|_{v,C}^{t}^{l_{v}^{t} d}\).

Next, we employ a Transformer encoder to capture the temporal dependencies, which is built by stacking \(L\) Multi-head Self-Attention (MSA) and Feed-Forward Network (FFN) blocks. The residual connection  is employed after every block. We follow  by using GELU  instead of ReLU  between the two-layer perception in each FFN block and applying Layer Normalization (LN)  before each block rather than after. Instead of individually processing \(_{u}^{t}\) and \(_{v}^{t}\), our Transformer encoder takes the stacked \(^{t}=[_{u}^{t};_{v}^{t}]^{(l_{u}^{t}+l_{v}^{t} ) d}\) as inputs, aiming to learn the temporal dependencies within and across the sequences of \(u\) and \(v\). The calculation process is

\[(,,)= (^{}}{}}),\] (3) \[(,_{1},_{1},_{2},_{2 })=(_{1}+_{1})_{2}+_ {2},\] (4) \[_{i}^{t,l}=((^{t,l-1}) _{Q,i}^{l},(^{t,l-1})_{K,i}^{l},(^{ t,l-1})_{V,i}^{l}),\] (5) \[^{t,l}=(^{t,l-1})+^{t,l-1} =(_{1}^{t,l}\|\|_{I}^{t,l})_{O}^{l}+ ^{t,l-1},\] (6) \[^{t,l}=((^{t,l}), _{1}^{l},_{1}^{l},_{2}^{l},_{2}^{l})+^{t,l}.\] (7)

\(_{Q,i}^{l}^{4d d_{k}}\), \(_{K,i}^{l}^{4d d_{k}}\), \(_{V,i}^{l}^{4d d_{v}}\), \(_{O}^{l}^{l d_{v} 4d}\), \(_{1}^{l}^{4d 16d}\), \(_{1}^{l}^{16d}\), \(_{2}^{l}^{16d 4d}\) and \(_{2}^{l}^{4d}\) are trainable parameters at the \(l\)-th layer. We set \(d_{k}=d_{v}=4d/I\) with \(I\) as the number of attention heads. The input of the first layer is \(^{t,0}=^{t}^{(l_{u}^{t}+l_{v}^{t}) 4d}\), and the output of the \(L\)-th layer is denoted by \(^{t}=^{t,L}^{(l_{u}^{t}+l_{v}^{t}) d}\).

**Time-aware Node Representation**. The time-aware representations of node \(u\) and \(v\) at timestamp \(t\) are derived by averaging their related representations in \(^{t}\) with an output layer,

\[_{u}^{t} =(^{t}[:l_{u}^{t},:])_{out}+ {b}_{out}^{d_{out}},\] (8) \[_{v}^{t} =(^{t}[l_{u}^{t}:l_{u}^{t}+l_{v}^{t},:] )_{out}+_{out}^{d_{out}},\]

where \(_{out}^{4d d_{out}}\) and \(_{out}^{d_{out}}\) are trainable weights with \(d_{out}\) as the output dimension.

### DyGLib: Unified Library for Continuous-Time Dynamic Graph Learning

We introduce a unified library with standard training pipelines, extensible coding interfaces, and comprehensive evaluating strategies for reproducible, scalable, and credible continuous-time dynamic graph learning research. The overall procedure of DyGLib is shown in Figure 3 in Section A.3.

**Standard Training Pipelines**. To eliminate the influence of different training pipelines in previous studies, we unify the data format, create a customized data loader, and train all the methods with the same model trainers. Our standard training pipelines guarantee reproducible performance and enable users to quickly identify the key components of different models. Researchers only need to focus on designing the model architecture without considering other irrelevant implementation details.

**Extensible Coding Interfaces**. We provide extensible coding interfaces for the datasets and algorithms, which are all implemented by PyTorch. These scalable designs enable users to incorporate new datasets and popular models based on their specific requirements, which can significantly reduce the usage difficulty for beginners and allow experts to conveniently validate new ideas. Currently, DyGLib has integrated thirteen datasets from various domains and nine continuous-time dynamic graph learning methods. It is worth noticing that we also found some issues in previous implementations and have fixed them in DyGLib (see details in Section B.3).

**Comprehensive Evaluating Protocols**. DyGLib supports both transductive/inductive dynamic link prediction and dynamic node classification tasks. Most previous works evaluate their methods on the dynamic link prediction task with the random negative sampling strategy but a few models already reach saturation performance under such a strategy, making it hard to distinguish more advanced designs. For more reliable comparisons, we adopt three strategies (i.e., random, historical, and inductive negative sampling strategies) in  to comprehensively evaluate the model performance.

## 5 Experiments

In this section, we report the results of various approaches by using DyGLib. We show the superiority of DyGFormer over existing methods and also give an in-depth analysis of DyGFormer.

### Experimental Settings

**Datasets and Baselines**. We experiment with thirteen datasets (Wikipedia, Reddit, MOOC, LastFM, Enron, Social Evo., UCI, Flights, Can. Parl., US Legis., UN Trade, UN Vote, and Contact), which are collected by  and cover diverse domains. Details of the datasets are shown in Section B.1. We compare DyGFormer with eight popular continuous-time dynamic graph learning baselines that are based on graph convolutions, memory networks, random walks, and sequential models, including JODIE , DyRep , TGAT , TGN , CAWN , EdgeBank , TCL , and GraphMixer . We give the descriptions of baselines in Section B.2.

**Evaluation Tasks and Metrics**. We follow [64; 45; 60; 44] to evaluate models for dynamic link prediction, which predicts the probability of a link occurring between two given nodes at a specific time. This task has two settings: the transductive setting aims to predict future links between nodes that are observed during training, and the inductive setting predicts future links between unseen nodes. We use a multi-layer perceptron to take the concatenated representations of two nodes as inputs and return the probability of a link as the output. Average Precision (AP) and Area Under the Receiver Operating Characteristic Curve (AUC-ROC) are adopted as the evaluation metrics. We adopt random (rnd), historical (hist), and inductive (ind) negative sampling strategies in  for evaluation, where the latter two strategies are more challenging. Please refer to  for more details. We also follow [64; 45] to conduct dynamic node classification, which estimates the state of a node in a given interaction at a specific time. A multi-layer perceptron is employed to map the node representations to the labels. We use AUC-ROC as the evaluation metric due to the label imbalance. For both tasks, we chronologically split each dataset with the ratio of 70%/15%/15% for training/validation/testing.

**Model Configurations**. For baselines, in addition to following their official settings, we also perform an exhaustive grid search to find the optimal configurations of some critical hyperparameters for more reliable comparisons. As DyGFormer can access longer histories, we vary each node's input sequence length from 32 to 4096 by a factor of 2. To keep the computational complexity at a constant level that is irrelevant to the input length, we correspondingly increase the patch size from 1 to 128. Please see Section B.5 for the detailed configurations of different models.

**Implementation Details**. For both tasks, we optimize all models (i.e., excluding EdgeBank which has no trainable parameters) by Adam  and use supervised binary cross-entropy loss as the objective function. We train the models for 100 epochs and use the early stopping strategy with a patience of 20. We select the model that achieves the best performance on the validation set for testing. We set the learning rate and batch size to 0.0001 and 200 for all the methods on all the datasets. We run the methods five times with seeds from 0 to 4 and report the average performance to eliminate deviations. Experiments are conducted on an Ubuntu machine equipped with one Intel(R) Xeon(R) Gold 6130 CPU @ 2.10GHz with 16 physical cores. The GPU device is NVIDIA Tesla T4 with 15 GB memory.

### Performance Comparisons and Discussions

We report the performance of different methods on the AP metric for transductive dynamic link prediction with three negative sampling strategies in Table 1. The best and second-best results are emphasized by **bold** and underlined fonts. Note that the results are multiplied by 100 for a better display layout. Please refer to Section C.1 and Section C.2 for the results of AP for inductive dynamic link prediction as well as AUC-ROC for transductive and inductive dynamic link prediction tasks.

[MISSING_PAGE_FAIL:7]

TCL and GraphMixer and show their performance in Table 2 and Table 16 in Section C.4. We find TCL and GraphMixer usually yield better results with NCoE, achieving an average improvement of 5.36% and 1.86% over all datasets. This verifies the effectiveness and versatility of the neighbor co-occurrence encoding, and highlights the importance of capturing correlations between nodes. Also, as TCL and DyGFormer are built upon Transformer, TCL w/ NCoE can achieve similar results with DyGFormer on datasets that enjoy shorter input sequences (in which cases the patching technique in DyGFormer contributes little). However, when datasets exhibit more obvious long-term temporal dependencies (e.g., LastFM, Can. Parl.), the performance gaps become more significant.

### Advantages of Patching Technique

We validate the advantages of our patching technique in preserving the local temporal proximities and reducing the computational complexity, which helps DyGFormer effectively and efficiently utilize longer histories. We conduct experiments on LastFM and Can. Parl. since they can benefit from longer historical records. For baselines, we sample more neighbors or perform more causal anonymous walks (starting from 32) to make them access longer histories. The results are depicted in Figure 2, where the x-axis is represented by a logarithmic scale with base 2. We also plot the performance of baselines with the optimal length by unconnected points based on Table 9 in Section B.5. Note that the results of some baselines are incomplete since they raise the out-of-memory error when the lengths are longer. For example, TGAT is only computationally feasible when extending the input length to 32, resulting in two discrete points with lengths 20 (the optimal length) and 32.

From Figure 2, we conclude that: (i) most of the baselines perform worse when the input lengths become longer, indicating they lack the ability to capture long-term temporal dependencies; (ii) the baselines usually encounter expensive computational costs when computing on longer histories. Although memory network-based methods (i.e., DyRep and TGN) can handle longer histories with affordable computational costs, they cannot benefit from longer histories due to the potential issues of vanishing or exploding gradients; (iii) DyGFormer consistently achieves gains from longer sequences, demonstrating the advantages of the patching technique in leveraging longer histories.

We also compare the running time and memory usage of DyGFormer with and without the patching technique during the training process. The results are shown in Table 17 in Section C.5. We could observe that the patching technique efficiently reduces model training costs in both time and space, allowing DyGFormer to access longer histories. As the input sequence length increases, the reductions become more significant. Moreover, with the patching technique, we find that DyGFormer achieves an average improvement of 0.31% and 0.74% in performance on LastFM and Can. Parl. than DyGFormer without patching. This observation further demonstrates the advantage of our patching technique in leveraging the local temporal proximities for better results.

### Verification of the Motivation of Neighbor Co-occurrence Encoding Scheme

To verify the motivation of NCoE (i.e., nodes with more common historical neighbors tend to interact in the future), we compare the performance of DyGFormer and DyGFormer without NCoE. We choose 0.5 as the threshold and use TP, TN, FN, and FP to denote True/False Positive/Negative.

  Datasets & TCL & w/ NCoE & Improv. \\  Wikipedia & 96.47 & 99.09 & 2.72\% \\ Reddit & 97.53 & 99.04 & 1.55\% \\ MOOC & 82.38 & 86.92 & 5.51\% \\ LastFM & 67.27 & 84.02 & 24.90\% \\ Enron & 79.70 & 90.18 & 13.15\% \\ Social Evo. & 93.13 & 94.06 & 1.00\% \\ UCI & 89.57 & 94.69 & 5.72\% \\ Flights & 91.23 & 97.71 & 7.10\% \\ Can. Parl. & 68.67 & 69.34 & 0.98\% \\ US Legis. & 69.59 & 69.47 & -0.17\% \\ UN Trade & 62.21 & 63.46 & 2.01\% \\ UN Vote & 51.90 & 51.52 & -0.73\% \\ Contact & 92.44 & 97.98 & 5.99\% \\  

Table 2: AP for TCL with NCoE.

Figure 2: Performance of different methods on LastFM and Can. Parl. with varying input lengths.

Common Neighbor Ratio (CNR) is defined as the ratio of common neighbors in source node \(u\)'s sequence \(S_{u}\) and destination node \(v\)'s sequence \(S_{v}\), i.e., \(|S_{u} S_{v}|/|S_{u} S_{v}|\). We focus on links whose predictions of DyGFormer w/o NCoE are changed by DyGFormer (i.e., FN\(\)TP, FP\(\)TN, TP\(\)FN, and TN\(\)FP). We define Changed Link Ratio (CLR) as the ratio of the changed links to their original set, which is respectively computed by \(|\)FN\(\)TP\(|/|\)FN\(|\), \(|\)FP\(\)TN\(|/|\)FP\(|\), \(|\)TP\(\)FN\(|/|\)TP\(|\), and \(|\)TN\(\)FP\(|/|\)TN\(|\). If NCoE is helpful, DyGFormer will revise more wrong predictions (more FN\(\)TP and FP\(\)TN) and make fewer incorrect changes (fewer TP\(\)FN and TN\(\)FP). We report CLR and average CNR of links in the above sets on five typical datasets in Table 3.

We find NCoE effectively helps DyGFormer rectify wrong predictions of DyGFormer w/o NCoE on datasets with _significantly higher CNR of positive links than negative ones_, which happens with most datasets. Concretely, for Wikipedia, UCI, and Flights, their CNRs of FN\(\)TP are much higher than FP\(\)TN (e.g., 37.09% vs. 2.28% on Flights) and DyGFormer revises most wrong predictions of DyGFormer w/o NCoE (e.g., 83.66% for positive links in FN and 83.83% for negative links in FP on Flights). Corrections made by our encoding scheme are less obvious on datasets whose _CNRs between positive and negative links are similar_, which occurs in only 2 of 13 datasets. For US Legis, and UN Vote, their CNRs between FN and FP are analogous (e.g., 69.92% vs. 62.13% on US Legis.), weakening the advantage of our neighbor co-occurrence encoding scheme (e.g., only 31.63%/23.67% of positive/negative links are corrected in FN/FP on US Legis.). Therefore, we conclude that the neighbor co-occurrence encoding scheme helps DyGFormer capture common historical neighbors in \(S_{u}\) and \(S_{v}\), and bring better results in most cases.

### When Will DyGFormer Be a Good Choice?

Note that DyGFormer is superior to baselines by 1) exploring the source and destination nodes' correlations from their historical sequences by neighbor co-occurrence encoding scheme; 2) using the patching technique to attend longer histories. Thus, DyGFormer tends to perform better on datasets that favor these two designs. We define Link Ratio (LR) as the ratio of links in their corresponding positive or negative set, which can be computed by TP\(/(\)TP+FN), TN\(/(\)TN+FP), FN\(/(\)TP+FN), and FP\(/(\)TN+FP). As a method with more TP and TN (i.e., fewer FN and FP) is better, we report the results of LR and average CNR of links in TP and TN on five typical datasets in Table 4.

We observe when _CNR of TP is significantly higher than CNR of TN in the datasets, DyGFormer often outperforms baselines_ (most datasets satisfy this property). For Wikipedia, UCI, and Flights, their CNRs of TP are much higher than those of TN (e.g., 59.09% vs. 0.01% on Wikipedia). Such a characteristic matches the motivation of our neighbor co-occurrence encoding scheme, enabling DyGFormer to correctly predict most links (e.g., 92.74% of positive links and 97.19% of negative links are properly predicted on Wikipedia). Moreover, as LastFM and Can. Parl. can gain from

   &  &  \\   & TP & TN & TP & TN \\  Wikipedia & 92.74 & 97.19 & 59.09 & 0.01 \\ UCI & 82.70 & 96.77 & 28.03 & 1.45 \\ Flights & 96.13 & 95.33 & 47.58 & 1.40 \\ US Legis. & 78.95 & 56.83 & 75.18 & 53.98 \\ UN Vote & 65.18 & 45.43 & 56.24 & 76.02 \\  

Table 4: LR and CNR of TP and TN with random negative sampling strategy.

   &  &  \\   & FN\(\)TP & FP\(\)TN & TP\(\)FN & TN\(\)FP & FN\(\)TP & FP\(\)TN & TP\(\)FN & TN\(\)FP \\  Wikipedia & 68.36 & 72.73 & 1.68 & 1.69 & 18.16 & 0.01 & 0.10 & 2.49 \\ UCI & 71.45 & 94.11 & 7.29 & 1.82 & 19.08 & 2.49 & 3.35 & 13.02 \\ Flights & 83.66 & 83.83 & 1.73 & 2.11 & 37.09 & 2.28 & 7.06 & 20.28 \\ US Legis. & 31.63 & 23.67 & 6.63 & 1.59 & 69.92 & 62.13 & 61.14 & 63.80 \\ UN Vote & 44.02 & 36.46 & 28.95 & 30.53 & 78.57 & 81.39 & 80.86 & 77.02 \\  

Table 3: CLR and CNR of changes made by DyGFormer.

   &  &  \\   & FN\(\)TP & FP\(\)TN & TP\(\)FN & TN\(\)FP & FN\(\)TP & FP\(\)TN & TP\(\)FN & TN\(\)FP \\  Wikipedia & 68.36 & 72.73 & 1.68 & 1.69 & 18.16 & 0.01 & 0.10 & 2.49 \\ UCI & 71.45 & 94.11 & 7.29 & 1.82 & 19.08 & 2.49 & 3.35 & 13.02 \\ Flights & 83.66 & 83.83 & 1.73 & 2.11 & 37.09 & 2.28 & 7.06 & 20.28 \\ US Legis. & 31.63 & 23.67 & 6.63 & 1.59 & 69.92 & 62.13 & 61.14 & 63.80 \\ UN Vote & 44.02 & 36.46 & 28.95 & 30.53 & 78.57 & 81.39 & 80.86 & 77.02 \\  

Table 3: CLR and CNR of changes made by DyGFormer.

longer histories (see Figure 2 and Table 9 in Section B.5), DyGFormer is significantly better than baselines on these two datasets. When _CNRs of TP and TN are less distinguishable in the datasets, DyGFormer may perform worse_ (only 2 out of 13 datasets show this property). For US Legis., the CNRs between TP and TN are close (i.e., 75.18% vs. 53.98%), making DyGFormer worse than memory-based baselines (i.e., JODIE, DyRep, and TGN). For UN Vote, its CNR of TP is even lower than that of TN (i.e., 56.24% vs. 76.02%), which is opposite to our motivation, leading to poor results of DyGFormer than a few baselines. Since these two datasets cannot obviously gain from longer sequences either (see Table 9 in Section B.5), DyGFormer obtains worse results on them. Thus, we conclude that for datasets with much higher CNR of TP than CNR of TN or datasets that can benefit from longer histories, DyGFormer is a good choice. Otherwise, we may need to try other methods.

### Why do DyGFormer's Performance Vary across Different Negative Sampling Strategies?

Compared with the random (rnd) strategy, historical (hist) and inductive (ind) strategies will sample previous links as negative ones. This makes previous positive links negative, which may hurt the performance DyGFormer since the motivation of our neighbor co-occurrence encoding scheme is violated. As positive links are identical among rnd, hist, and ind, we compute LR and the average CNR of links in FP and show results in Table 5.

We find _when hist or ind causes several magnitudes higher CNR of FP than rnd in the datasets, DyGFormer drops sharply_. For Wikipedia, UCI, and Flights, the CNRs of FP with hist/ind are much higher than rnd (e.g., 14.00%/11.66% vs. 0.02% on Wikipedia). This misleads DyGFormer to predict negative links as positive and causes drops (e.g., 89.28%/94.53% of negative links are incorrectly predicted with hist/ind on Wikipedia, while only 2.81% are wrong with rnd). We also note the drops in UCI are milder since the changes in CNR caused by hist or ind vs. rnd are less obvious than changes in Wikipedia and Flights. _When changes in CNR of FP caused by hist or ind are not obvious in the datasets, DyGFormer is less affected_. Since hist/ind makes little changes in CNRs of FP on US Legis., we find it ranks second with hist/ind, which may indicate DyGFormer is less influenced by the neighbor co-occurrence encoding scheme and generalizes well to various negative sampling strategies. For UN Vote, although its CNRs of FP are not affected by hist and ind either, DyGFormer still performs worse due to its inferior performance with rnd. Hence, we deduce that our neighbor co-occurrence encoding may be sometimes fragile to various negative sampling strategies if its motivation is violated, leading to the varying performance of DyGFormer.

### Ablation Study

Finally, we validate the effectiveness of the neighbor co-occurrence encoding, time encoding, and mixing of the sequence of source node and destination node in DyGFormer. From Figure 4 in Section C.6, we observe that DyGFormer obtains the best performance when using all the components, and the results would be worse when any component is removed. This illustrates the necessity of each design in DyGFormer. Please refer to Section C.6 for detailed implementations and discussions.

## 6 Conclusion

In this paper, we proposed a new Transformer-based architecture (DyGFormer) and a unified library (DyGLib) to foster the development of dynamic graph learning. DyGFormer differs from previous methods in (i) a neighbor co-occurrence encoding scheme to exploit the correlations of nodes in each interaction; and (ii) a patching technique to help the model capture long-term temporal dependencies. DyGLib served as a toolkit for reproducible, scalable, and credible continuous-time dynamic graph learning with standard training pipelines, extensible coding interfaces, and comprehensive evaluating protocols. We hope our work can provide new perspectives on designing new dynamic graph learning frameworks and encourage more researchers to dive into this field. In the future, we will continue to enrich DyGLib by incorporating the recently released datasets and state-of-the-art models.