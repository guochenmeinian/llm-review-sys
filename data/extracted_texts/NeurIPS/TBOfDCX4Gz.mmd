# GEO: Gaussian Kernel Inspired Equilibrium Models

Mingjie Li\({}^{1}\), Yisen Wang\({}^{1,2}\), Zhouchen Lin\({}^{1,2,3}\)

\({}^{1}\) National Key Lab of General Artificial Intelligence,

School of Intelligence Science and Technology, Peking University

\({}^{2}\) Institute for Artificial Intelligence, Peking University

\({}^{3}\) Peng Cheng Laboratory

lmjat0111@outlook.com, {yisen.wang, zlin}@pku.edu.cn

Corresponding author

###### Abstract

Despite the connection established by optimization-induced deep equilibrium models (OptEqs) between their output and the underlying hidden optimization problems, the performance of it along with its related works is still not good enough especially when compared to deep networks. One key factor responsible for this performance limitation is the use of linear kernels to extract features in these models. To address this issue, we propose a novel approach by replacing its linear kernel with a new function that can readily capture nonlinear feature dependencies in the input data. Drawing inspiration from classical machine learning algorithms, we introduce Gaussian kernels as the alternative function and then propose our new equilibrium model, which we refer to as GEO. By leveraging Gaussian kernels, GEO can effectively extract the nonlinear information embedded within the input features, surpassing the performance of the original OptEqs. Moreover, GEO can be perceived as a weight-tied neural network with infinite width and depth. GEO also enjoys better theoretical properties and improved overall performance. Additionally, our GEO exhibits enhanced stability when confronted with various samples. We further substantiate the effectiveness and stability of GEO through a series of comprehensive experiments.

## 1 Introduction

Deep Neural Networks (DNNs) show impressive performance in many real-world tasks on various data like graphs , images , sequences , and others. However, most neural network structures are constructed by experience or searching on the surrogate datasets . Therefore, these architectures cannot be interpretable and such a phenomenon hinders further development. Apart from the current neural network models, traditional machine learning methods like dictionary learning , subspace clustering  and other methods  can design their whole procedure by designing optimization problems with specific regularizers customized from their mathematical modeling and requirements. Thus, these models are easily interpreted. However, the traditional machine learning algorithms' whole procedures do not consider the hidden properties of features and labels. Therefore, they usually perform worse on tasks with more data.

To link two types of models, OptEqs  tries to recover the model's hidden optimization problem to make their model "mathematically explainable". They claim that the output features \(}^{*}\) (we also called the equilibrium state) with respect to input \(\), which are obtained by solving the fixed-pointequation in Eqn (1), is the optimal solution for its hidden optimization problem defined in Eqn (2).

\[}^{*}=}^{}(}}^{*}++),\;}=}_{c}}^{*}+_{c},\] (1) \[_{}}G(};)=_{ }}[^{}f(}^{-1}})-+,}^{- 1}}+\|}^{-1 }}\|_{2}^{2}-\|}\|_{2}^{2}],\] (2)

where \(\) is the ReLU activation function, \(,},,}_{c},_{c}\) are learnable parameters. They are trained by optimizing loss functions, such as cross-entropy loss, which are calculated based on the final prediction \(}\) derived from \(}^{*}\) as shown in Eqn (1). \(}^{-1}\) represents an invertible or pseudo-invertible matrix for \(}\), and the function \(f\) is a positive indicator function, which outputs \(1\) when \(x 0\) and outputs \(\) in other cases. By choosing this function, the first-order condition for Eqn 2 will contain a non-linear ReLU function, which is the activation function in our GEQ. The equivalence between \(^{*}\) and the optimal solution for the hidden optimization problem (2) enables researchers to not only gain insights into OptEqs' behavior by understanding the underlying hidden optimization problem but also innovate by designing new models based on different problem formulations tailored to specific tasks. For instance, OptEqs introduces a module that promotes output sparsity, and Multi-branch OptEqs (MOptEqs) incorporates fusion modules to enhance the diversity among its branches. Despite these advancements and the incorporation of various modules inspired by different vision tasks, the performance of OptEqs and related models still falls short when compared to deep neural networks in image classification. This discrepancy suggests the existence of crucial components that limit the performance of equilibrium models.

To identify such a component, we delve into the hidden optimization problem of OptEqs and observe that it can be decomposed into two distinct parts: the regularizer term for the output features and the feature extraction term. While the feature extraction term is crucial as it depends on the input and determines the patterns extracted from the input features, the exploration of the regularizer term has been largely overlooked, with linear kernel functions being the predominant choice for feature extraction. Thereby, we believe that the limitations of previous equilibrium models stem from their feature extraction parts, as linear kernels struggle to capture complex features effectively. Building upon these insights, we take a step forward by leveraging the widely adopted Gaussian kernel for feature extraction in the hidden optimization problem.

Then by calculating the stationary condition for the above new hidden optimization problem, we propose our new type of OptEqs, the Gaussian kernels inspired equilibrium models (GEQ). The model involves a new attentive module induced by its hidden optimization problem and enjoys much better performances on classification tasks even compared with deep models. Furthermore, we also prove that the new model's outputs are equivalent to the outputs for OptEqs with weight-tied "infinite wide" mappings. Therefore, an interesting finding is that our model can be regarded as a "double-infinite" model because the original OptEqs can be regarded as a weight-tied "infinite deep" model. Apart from the above findings, the utilization of Gaussian kernels also makes our proposed model enjoy better generalization abilities. Besides the generalization abilities, we also analyze the stability of our GEQ and find its stability is better on various inputs. We summarize our contributions as follows:

* We first reformulate the OptEqs' hidden optimization problem with Gaussian kernels and propose a new equilibrium model called GEQ. It contains a new attention module induced by its hidden optimization problem and performs better on real-world datasets.
* We find that our GEQ can be regarded as a weight-tied neural network with both infinite width and depth, and better generalization ability through our analysis. Empirical results also confirm the superiority of our GEQ.
* We theoretically demonstrate the advantages of the stability of our GEQ compared with former OptEqs on various inputs. We also conduct experiments to validate such advantages.

## 2 Related Works

### Implicit Models

Most modern deep learning approaches provide explicit computation graphs for forward propagations and we call these models "_explicit_ models". Contrary to these models, recent researchers proposed some neural architecture with dynamic computation graphs and we call them "_implicit_ models". A notable example of an implicit model is Neural ODEs , its architecture is encoded as a differential system and the implicit ODE solvers they used are equivalent to continuous ResNets that take infinitesimal steps. By representing the entire structure using differential systems, implicit models tap into the black box of traditional neural networks while offering increased flexibility and interpretability. Because of the flexibility and the interpretability of implicit models, the design of implicit models [15; 17] draws much attention these days. Many kinds of implicit models have been proposed, including optimization layers [11; 1], differentiable physics engines [40; 9], logical structure learning , differential programming [53; 43], and others [25; 49].

Among the various implicit models, OptEqs  and its multi-branch version MOptEqs  stand out as they not only exhibit superior performance compared to other implicit models but also explicitly establish the relationship between their structure and a well-defined optimization problem. Therefore, exploring better equilibrium models is a promising direction to achieve more interpretable neural architectures. However, it is worth noting that while OptEqs and its variants have shown promising results, their performance is still not entirely satisfactory, particularly when compared to the deep explicit models. Other works [51; 42] also show the connection between their architectures and optimization problems, but their performance is also not satisfying. Besides the above general models, there are many works design equilibrium models from the view of optimization to deal with their specific tasks, like implicit graph models [6; 26], certified robust models  and image denoising models . However, these models can only work on their specific domains.

### Infinite Wide Models and Kernel Methods in Deep Learning

By employing kernel methods to estimate the outputs of single-layer networks for various samples, researchers discover that such networks can exhibit characteristics of a Gaussian process (GP) when their parameters are randomly initialized with a large width limit . Building upon this idea, recent researchers have extended these findings to neural networks with multiple layers [24; 10] and other architectures [37; 13]. These studies primarily focus on weakly-trained models, where the network parameters are randomly initialized and kept fixed throughout the training process except for the last classification layer . Despite their "weakly-trained" nature, these models still provide valuable insights applicable to current neural networks. For instance, mean-field theory [4; 16; 19] explains phenomena such as gradient vanishing and exploding during back-propagation, which are relevant not only to single-layer networks but also to other structures like convolutional neural networks (CNNs) and recurrent neural networks (RNNs). Other researchers explore stationary kernels to enhance the interpretability of neural networks by designing different activation functions .

In addition to weakly trained models, recent studies [22; 2] introduce the concept of Neural Tangent Kernel (NTK) and its variants. These works have demonstrated that the sample kernel of infinitely wide networks, with appropriate initialization, can converge to a fixed neural tangent kernel when trained using gradient descent with infinitesimal steps (gradient flow). The NTK model is a theoretical construct with strict constraints, and its weights are not learned. It is important to note that although our model can also be seen as an infinitely wide model, there are several key differences between our approach and the aforementioned models. Firstly, our model utilizes kernel methods to operate on input features and output features, while the NTK models employ the kernel method on samples. Secondly, our GEQ model can be viewed as employing a "weight-tied infinite wide" projection that is parameterized by learnable parameters, allowing for updates during the training process. This contrasts with NTKs and NTK-DEQ (an equilibrium model constructed with vanilla NTK layers), where the weights are fixed and not learned. Therefore, despite the potential overlap in terminologies used in our paper and NTK-related works, our GEQ model differs significantly.

## 3 Gaussian kernel Inspired Equilibrium models

### Formulation and Structure of GEQ

Before starting our analysis, we need to reformulate the original formulations of OptEqs' equilibrium equation (1) and hidden optimization problem (2) for convenience. We replace \(}_{c}}^{}\) with \(_{c}\), \(:=}^{}\), and replace \(\) with \(}^{-1^{}}}\). Then the original OptEqs' optimization problem can be reformulated as:

\[_{}G(;)= _{}[^{}f()+ \|\|_{2}^{2}-+, -\|\|_{2}^{2}].\] (3)

With the new formulation, we can rewrite the equilibrium equation for OptEqs with input \(\) by calculating Eqn (3)'s first order stationary condition \( G=\) and then reformulate it as follows:

\[^{*}=(^{}^{*}++),\] (4)

where \(\) is the ReLU activation function, \(,,\) are learnable parameters trained by optimizing loss functions (like cross-entropy loss). From problem Eqn (3), one can see that the GEQ's outputs try to extract features by minimizing the similarity term with the input feature \(+\) through a linear kernel function with some constraints defined in its regulation terms to prevent the trivial outputs. Such an explanation can also extend to other DEQs [3; 51] under the symmetric weight constraints. However, as other traditional machine learning mechanisms show, linear kernel functions cannot perform well when processing complex inputs. We deem that this term will also restrict the performance in equilibrium models. We note that the symmetric constraints won't influence the final performance much as many works [32; 21] show.

A natural consideration arises as to whether we can utilize alternative kernel functions to extract input features for the equilibrium state. However, we find that other equilibrium models employing different kernels with inner products, like the polynomial kernel and sigmoid kernel, lead to a similar structure to OptEqs with appropriate weight re-parameterization and lead to similar empirical results. We provide a detailed discussion of the related models in Appendix 4.6. Thereby, we decide to use the Gaussian kernels, and our new hidden optimization equation is formulated as follows:

\[_{}G(;)=_{}[^{ }f()+\|\|_{2}^{2}-e^{- \|+-\|_{2}^{2}}],\] (5)

where \(\) is the hyperparameter denoting the reciprocal of Gaussian kernels' variance for scaling. Calculating \( G=\) for new \(G\), we can get the Gaussian kernel inspired Equilibrium models (GEQ) as the following fixed-point equation:

\[^{*}=[e^{-\|+- ^{*}\|_{2}^{2}}^{}(-^{* }++)].\] (6)

Compared with linear kernels, Gaussian kernels can easily extract the non-linear relations from the input features and show more stable and powerful performance in SVM and other machine learning methods [21; 44]. We also find that the formulation of our GEQ is similar to adding a new attention module to the original equilibrium models. Therefore, our GEQ is supposed to enjoy more representative abilities than the original OptEqs. In the following parts of this section, we will analyze the theoretical advantages of our GEQ against the vanilla OptEqs. And we also empirically evaluate GEQ's performance in the following sections.

### GEQ equals to the OptEqs with infinite width

Like other Gaussian-related models, our GEQ model can also be regarded as computing similarities by mapping them to an infinite-dimensional space. This allows GEQ to extract input features at the infinite-dimensional level, enabling the capture of non-linear dependencies in the input space. Essentially, our GEQ can be seen as a specialized version of OptEqs operating within the infinite-dimensional space after mapping input features \(\) and output embedding \(\) to this expanded domain.

**Proposition 1**.: _The output of our GEQ (Eqn (6)) is the same as a special OptEqs' output whose hidden optimization problem is defined as follows:_

\[_{}G(;)=_{}[^{ }f()+\|\|_{2}^{2}-_ {}(+^{-1}),_{}()],\] (7)

_where \(f\) is the positive indicator function and \((1+ f)^{-1}\) is the ReLU activation function, \(=e^{-\|+\|_{2}^{2}}e^{-\| \|_{2}}\), and \(_{}()=[,_{}^{( )}(),...,/i}_{}^{(i)}( ),...]^{1 0}\) which maps the inputs to the infinite-dimensional space with \(_{}^{(i)}:^{n}^{in^{i}}\) defined as follows:_

\[_{}^{(i)}=[(Wx)_{0}...(Wx)_ {0}}^{i},(Wx)_{0}...(Wx)_{1}}^{i},...,( Wx)_{k}...(Wx)_{m}}^{i},...}_{in^{i}}(Wx)_{k}...(Wx)_{m}}^{i},... ],\] (8)_where \((Wx)_{j}\) denotes the \(j\)-th element of vector \(\)._

Based on the analysis provided above, it becomes evident that the hidden optimization problem of our GEQ exhibits a similar formulation to a specific OptEqs, whose inputs \(\) and outputs \(\) are mapped to an infinite-dimensional space using the weight-tied infinite wide mapping \(_{}\) and \(_{}\). Given that both GEQ and OptEqs are derived from their respective hidden optimization problems, the equivalence in these problems implies the existence of the same equilibrium states for both models. Consequently, our GEQ can be considered an extension of the "infinite-depth" OptEqs to the "infinite-width" domain. Since wider neural networks are generally expected to perform better on classification tasks, we can infer that our GEQ outperforms vanilla equilibrium models like OptEqs. We further support this claim with the theoretical analysis illustrated in the subsequent sections.

### Generalization abilities for our GEQ

Apart from the above empirical intuition, we are going to prove our GEO's generalization advantages over OptEqs using the generalization bound under the PAC-Bayesian framework . For convenience, we use \(f_{geq}()\) denotes the equilibrium state \(^{}\) for input \(\). Then we use the expected margin loss \(_{}(f_{geq}^{c})\) at margin \(\) of our GEQ on the data distribution \(\) for classification, which is defined as follows,

\[_{}(f_{geq}^{c})=_{(,) }[f_{geq}^{c}()_{y}+_{j y}f_{geq}^{ c}()_{j}],\] (9)

where \(f_{geq}^{c}()=_{c}f_{geq}()+_{c}\) stands for GEQ's final prediction at input \(\) with learnable parameters \(_{c}\) and \(_{c}\), and the index \(j\), \(y\) here denote the prediction score for certain class. Then we can analyze the generalization bound for our GEQ following the former work's settings .

**Proposition 2**.: _If input \(\|\|_{2}\) is bounded by \(B\), \(:=\{\|\|_{2},\|\|_{2},\|\|_{2},\| _{c}\|_{2},\|\|_{2}\}<1\), then we have following results for GEQ and OptEqs with ReLU activation. For \(,>0\), with probability at least \(1-\) over the training set of size M, we have:_

\[_{0}(f_{geq}^{c})}_{}(f_{geq}^{c})+^{4}B+(2_{}+1)(1-_{ }m) B+(1-_{max}m)]^{2}_{W}}{^{2}(1-_{ }m)^{4}M}+}{g})}{M}},\] (10)

\[_{0}(f_{geq}^{c})}_{}(f_{geq}^{c})+B+(1-m) B+(1-m)^{2}]^{2}_{W}}{^{2}(1-m)^{4}M}+}{g})}{M}},\]

_where \(}_{}(f_{geq}^{c})\) denotes the empirical margin loss on the training set, the maximum scaling number is defined by \(_{max}:=_{}e^{-\|+ -\|_{2}^{2}}\), \(_{W}:=\|^{}\|_{F}^{2}+\|\|_{F}^ {2}+\|\|_{2}^{2}+\|_{c}\|_{F}^{2}+\|_{c}\|_{2}^ {2}\), and \(m=\|^{}\|_{2}\) is less than \(1\) to ensure the convergence of equilibrium models._

**Remark 1**.: _If \(_{}<0.8\) and \(,m>0.9\), we can get \(}{1-_{}m}<\) and \(+1}{1-_{}m}\). In the meanwhile, our GEQ's generalization bound is tighter than the original OptEq._

In practical experiments, we find that the above conditions for \(_{}\) and \(,m\) are satisfied in most cases. And the following experiments also support our above theoretical advantages. However, the above bound is not tight, and how to approximate a much tighter bound for equilibrium models still needs exploring.

### GEQ enjoys More Stable Performance

Apart from better performance, Gaussian kernel stands out as one of the most extensively employed kernels in machine learning tasks owing to its stability across various input scenarios. Motivated by this, we aim to investigate whether incorporating Gaussian kernels into our equilibrium models can enhance the model's stability across diverse inputs. Firstly, we are going to estimate output changes with respect to the input perturbations.

**Proposition 3**.: _If norms for the inputs and outputs are bounded by \(B\), the spectral norm for the weight parameter \(,\) of equilibrium models with ReLU activation are bounded by \(<1\) to ensure convergence, then we have the conclusions as below:_

\[\|f_{geq}(_{1})-f_{geq}(_{2})\|_{2}\] (11) \[\|f_{opteq}(_{1})-f_{opteq}(_{2})\|_{2}\] (12)

_where \(_{1}\) and \(_{2}\) are input samples, \(f_{geq}(_{})\) and \(f_{opteq}(_{})\) denotes the equilibrium states for GEQ and OptEqs given input \(_{}\), and \(_{max}:=_{}e^{-\| -\|_{2}^{2}}<1\)._

**Remark 2**.: _If \(_{}<0.8\), \(B<1\), and \(<0.2\), then \(L_{geq}<L_{opteq}\)._

In practical experiments, we choose different \(\) to reach the above condition for \(_{}\) and the condition for input \(B\) can also be achieved by normalization layers. Although the above bound is not tight, we can use it as a rough explanation for our GEQ's stability, which is demonstrated in the following experiments. How to approximate a much tighter bound for equilibrium models still needs exploring.

Besides having stable outputs under perturbations, a stable model should also show large output differences for different classes to make classification easier. However, the above Lipschitz term can not constrain outputs' similarity when samples are far apart, then we need a new metric for analysis. In line with previous works [18; 34; 10], we assume all weight parameters go to infinite dimensions and analyze the expected output similarity \(\) for a model \(f\) for inputs \(_{1}\) and \(_{2}\) defined below:

\[(_{1},_{2})=[f(_{1})^{ }f(_{2})]=_{}f_{}(_{1})^{ }f_{}(_{2})p()d,\] (13)

with \(p()\) is the distribution of weight \(\)'s vectorization. If \(\) is smaller for samples \(_{1}\) and \(_{2}\) when they belong to different classes, which means they are far away, then the classifier can easily classify these two samples with different labels. The margin for the classification will also be large and easy for the classification of difficult samples. The \(\)'s upper bound for GEQ and OptEqs are listed below:

**Proposition 4**.: _If norms for the inputs and outputs are bounded by \(B\), the spectral norm for the weight parameter \(\) of equilibrium models with ReLU activation are bounded by \(<1\) to ensure the convergence, and each row in \(\) obeys the spherical Gaussian distributions \((0,[U_{i}^{2}])\). Then we have the following conclusions for the expectation of the output similarity for GEQ and OptEqs with respect to input \(_{1},_{2}\) as follows,_

\[_{geq}(_{1},_{2})_{geq}= {^{2}De^{-(_{}()^{2}\|_{1}- _{2}\|_{2}^{2})}[U_{i}^{2}]\|_{1}\|_{2}\| _{2}\|_{2}(_{0}+(-_{0})_{0} )}{2(1-_{}^{2})^{2}},\] (14)

\[_{opteq}(_{1},_{2})_{opteq}= [U_{i}^{2}]\|_{1}\|_{2}\|_{2}\|_{2}( _{0}+(-_{0})_{0})}{2(1-^{2})^{2}},\] (15)

_where \(_{1}\) and \(_{2}\) are input samples, \(D=e^{ B\|\|_{2}^{2}}\), \(_{max}:=_{}e^{-\| -\|_{2}^{2}}\), \(_{}()\) is \(\)'s minimal singular term, and \(_{0}=^{-1}(_{1},_{2})}{\|_{1} \|\|_{2}\|})\) is the angle between the samples._

**Remark 3**.: _If \(\|_{1}-_{2}\|_{2} 2/_{}( )\), then \(_{geq}_{geq}\)._

Based on the aforementioned analysis, it is evident that our GEQ exhibits a smaller output similarity for dissimilar samples. As a result, the predictions made by GEQ are primarily based on the most similar samples, enabling it to successfully classify challenging instances. This claim is further supported by the results obtained from our carefully designed experiments.

### Patch Splitting in GEQ

Since different parts of images have different impacts on the image classification, calculating the whole similarity using Gaussian kernels for GEQ is not enough. Inspired by former works , we also split the feature map \(\) into patches, and then our optimization problem becomes:

\[_{}G(;)=_{}[^{ }f()+\|\|_{2}^{2}-_{i =1}^{N}e^{-\|(}_{i}-}_{i})_{h}\|_ {2}^{2}}],\] (16)where \(}_{i}^{c_{s}p^{2}}\) is the \(i\)-th patch of \(+\) while \(}_{i}^{c_{s}p^{2}}\) is the \(i\)-th patch of \(\) and \(_{h}^{c_{s}p^{2} c_{hid}}\) is a linear layer to project patches with different size to the constant dimension. \(c_{s}\) denotes the channel splitting number, \(p\) denotes the patch size, and \(c_{hid}\) denotes the hidden dimension of patches after projection. We note that the patch-splitting approach is a GEQ's unique feature, as incorporating this technique makes no difference in OptEqs due to its linear kernel. Figure 1 provides a sketch for GEQ's \(i\)-th fixed-point iteration. From the figure, it is evident that our GEQ can be viewed as a special OptEqs with additional attention mechanisms to capture the most important regions. Thereby, it can achieve enhanced performance. For a more detailed understanding of the forward procedure in our GEQ, please refer to Appendix A.1.

## 4 Empirical Results

### Experiment Settings

In our experiments, we employed parallel GEQs with different input scales like MOptEqs and averaged the output of each branch after average pooling or nearest up-sampling to fuse the branches. We use weight normalization to ensure the convergence as MOptEqs and MDEQ, and set \(\) to \(0.2/M\), where \(M\) is the minimum \(\|}\). \(-}._{h}\|_{2}^{2}\) among all patches. For the equilibrium calculation, we used the Anderson algorithm in the forward procedure, similar to other implicit models , and applied Phantom gradients  for back-propagation. All models were trained using SGD with a step learning rate schedule. We implemented our experiments on the PyTorch platform  using RTX-3090. Further details can be found in the Appendix A.6. To compare the performance of our GEQ, we used MOptEqs and MDEQ as benchmark implicit models, which have demonstrated superior performance over OptEqs on image classification tasks. Additionally, we used ResNet-18 and ResNet-50 as benchmark explicit models for comparison.

### Results for Image Classification

Firstly, we finish the experiments on CIFAR-\(10\) and CIFAR-\(100\). They are widely used datasets for image classification on small images. In the experiment, we parallel \(6\) branches GEQ with the input scale is \(32,16,8,8,4,4\) and MOptEqs' architecture setting is also the same. The details can be found in the Appendix. As for the comparison, we also conduct experiments of the same training procedure for MDEQ, MOptEqs, and ResNet. The results are listed in Table 1. From the results, one can see that our GEQ enjoys clear advantages on CIFAR datasets, which demonstrates the powerful generalization ability of other models.

Besides small datasets, we also conducted experiments on large-scale image datasets, as presented in Table 2. The results clearly demonstrate the consistent superiority of our GEQ over other models, highlighting its clear advantages. Particularly noteworthy is our GEQ achieves a \(2\%\) improvement on ImageNet-\(100\) against deep model ResNet-\(50\) while consuming approximately half the number of parameters, which emphasizes the effectiveness and efficiency of GEQ on large-scale inputs.

### Validations on the models' stability

**Evaluation on Unseen difficult samples.** In order to assess the stability of our GEO model on difficult examples, we conducted experiments using CIFAR-100 super-class classification. CIFAR-100 consists of 20 super classes, each containing five sub-classes 2. We trained our GEO and MOptEqs models to predict the super-classes using the first four sub-classes from each super-class for training. We evaluated the models using both the test set, which includes the first four sub-classes from each super-class (referred to as "Known Accuracy"), and a separate set of samples from unseen sub-classes (referred to as "Unknown Accuracy"). The classification of the unseen samples is more difficult as they are different from the training set. The results of our GEO and MOptEqs models are presented in Table 3.

The above table clearly demonstrates that our GEO model surpasses MOptEqs in achieving superior performance on the challenging task at hand and demonstrates GEO's stability. Such advantages can be attributed to the fact that GEO exhibits smaller output similarities compared to OptEqs when input samples are far apart (e.g., samples from different classes). This characteristic can lead to larger margins between different classes, enabling the classifier to be more easily optimized during training. Consequently, our GEO model excels in accurately classifying difficult unseen samples, further highlighting its stability and superiority over former equilibrium models.

**Ablation Studies on corrupted datasets.** Apart from difficult samples, we are going to compare the robustness of our GEO, MOptEqs, and ResNet on the CIFAR-\(10\) corruption dataset, which contains \(19\) common corruptions including image transformation, image blurring, weather, and digital noises on CIFAR's test datasets. Average results on \(5\) degrees CIFAR-\(10\) corruption datasets list in Figure 2.

From the result, one can see that our GEO based on Gaussian kernels is more robust than MOptEqs and ResNet. In particular, our GEO can show better performance against structured noise and some image transformation. The above results also demonstrate the stability of our GEO structure.

Table 1: The Empirical results for image classification on CIFAR-\(10\) and CIFAR-\(100\).

Table 2: The Empirical results for image classification on ImageNette and ImageNet-\(100\).

Table 3: Empirical rsults on CIFAR-\(100\)’s super-class classification.

### Ablation Studies on Saliency Map

The saliency maps generated by GradCAM  offer valuable insights into the visual attention of both MOptEqs and GEQ models. These maps highlight the regions of the image that are crucial for the model's predictions. Figure 3 presents the saliency maps obtained for an image from the ImageNet dataset using both models. Upon observation, it becomes evident that GEQ exhibits a higher degree of focus on the significant regions directly associated with the predicted label "manta". In contrast, MOptEqs tends to allocate attention to unrelated regions such as the shells. This discrepancy indicates that the attention-like module induced by the Gaussian kernel in GEQ enhances the concentration of the model's attention, resulting in improved performance compared to MOptEqs.

### Ablation Studies on Patch splitting

The performance of our GEQ model is influenced by the channel splitting parameter and the patch size. Choosing large values for these parameters causes the kernel to focus mainly on global information while selecting small values makes the kernel concentrate on local features. To understand the impact of these choices on model performance, we conducted experiments, the results of which are presented in Figure 4. This figure illustrates the relationship between the channel splitting parameter, patch size, and the model's performance. By analyzing these results, we gain insights into the optimal values for these parameters that yield the best performance for our GEQ model.

The accuracy trend depicted in the figure shows an initial increase followed by a decrease as the channel split and patch size increased. Based on these empirical results, we select a patch size of \(2\) and a channel split of \(8\) for both the CIFAR and ImageNet experiments. These parameter choices are made to optimize the performance of our models on the respective datasets.

### Comparison with other kernel functions

Firstly, we introduce different commonly used kernels, such as polynomial, sigmoid, and Gaussian kernels, to reformulate the hidden optimization problem for equilibrium models. Table 4 illustrates the equilibrium models induced by these kernels. It can be observed that equilibrium models with polynomial and sigmoid kernels also incorporate new attentive modules. However, their attentive

Figure 3: Saliency map for GEQ and MOptEqs on the ImageNet image.

Figure 2: The results for different models under different corruptions.

kernels only constrain the input features \(+\) and do not directly affect the activation of the output \(^{*}\). As a result, their performance may be inferior to our GEQ model. To validate these claims, we evaluate the performance of different models on the CIFAR-\(100\) dataset. Since the fusion module is the primary difference between MOptEqs and OptEqs, we can easily modify the structure of MOptEqs to include different kernel-induced attentive modules using the equations in Table 4, resulting in MOptEqs (Polynomial) and MOptEqs (Sigmoid). The results are presented in Table 5, which clearly demonstrates the superior performance of our GEQ model. For a more in-depth analysis of GEQ, we refer readers to the main paper.

## 5 Conclusions

In this paper, we introduce a novel optimization-induced equilibrium model called GEQ, which utilizes Gaussian kernels in its optimization-induced framework. Our model incorporates a new attentive module that arises from its novel hidden optimization problem formulation. Notably, GEQ exhibits significantly improved performance in classification tasks, outperforming deep models as well. Moreover, GEQ can be interpreted as a weight-tied model with infinite width and depth, highlighting its expressive power. We also provide theoretical analysis demonstrating the superiority of our models in terms of generalization ability and stability compared to previous OptEqs. Empirical results further validate the effectiveness of our proposed approach.

  Kernel & Hidden Optimization Problem & Equilibrium Model \\  Linear & \(_{}[^{T}/)+\|\|_{2}^{2}-(+,)-\| \|_{2}^{2}\|\) & \(^{*}=(^{}^{*}++)\) \\  Polynomial & \(_{}[^{T}/)+\|\|_{2}^{2}-((+,))^{d}-\| \|_{2}^{2}\|\) & \(^{*}=(^{}^{*}+d(( +,))^{d-1}(+ ))\) \\  Sigmoid & \(_{}[^{T}/)+\|\|_{2}^{2}-((+,))-\| \|\) & \(^{*}=(^{}^{*}+(1- ^{*}(+,)))(+ ))\) \\  Gaussian & \(_{}[^{T}/()+\|\|_{2 }^{2}-^{c\+ }]\) & \(^{*}=[e^{-(+-^{*}^{2}_{}}^{*}-^{*}+ +)]\) \\  

Table 4: The hidden optimization problems and their related equilibrium models. \(d>1\) is an integer denoting the polynomial order.

Figure 4: The influence on the patch size and the channel splitting parameter for our GEQ on CIFAR-\(100\) datasets.

   & Model Size & Accuracy \\  MOptEqs & 8M & \(75.6 0.2\%\) \\ MOptEqs (Polynomial) & 8M & \(75.1 0.4\%\) \\ MOptEqs (Sigmoid) & 8M & \(76.1 0.3\%\) \\  GEQ & 8M & \(\) \\  

Table 5: Comparison of equilibrium models with different kernel functions on CIFAR-\(100\).