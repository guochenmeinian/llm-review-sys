# Extending the Design Space of Graph Neural Networks by Rethinking Folklore Weisfeiler-Lehman

Jiarui Feng\({}^{1}\)  Lecheng Kong\({}^{1}\)  Hao Liu\({}^{1}\)  Dacheng Tao\({}^{2}\)  Fuhai Li\({}^{1}\)

**Muhan Zhang\({}^{3}\) Yixin Chen\({}^{1}\)**

{feng.jiarui, jerry.kong, liuhao, fuhai.li, ychen25}@wustl.edu,

dacheng.tao@gmail.com, muhan@pku.edu.cn

\({}^{1}\)Washington University in St. Louis \({}^{2}\)JD Explore Academy \({}^{3}\)Peking University

Corresponding author

###### Abstract

Message passing neural networks (MPNNs) have emerged as the most popular framework of graph neural networks (GNNs) in recent years. However, their expressive power is limited by the 1-dimensional Weisfeiler-Lehman (1-WL) test. Some works are inspired by \(k\)-WL/FWL (Folklore WL) and design the corresponding neural versions. Despite the high expressive power, there are serious limitations in this line of research. In particular, (1) \(k\)-WL/FWL requires at least \(O(n^{k})\) space complexity, which is impractical for large graphs even when \(k=3\); (2) The design space of \(k\)-WL/FWL is rigid, with the only adjustable hyper-parameter being \(k\). To tackle the first limitation, we propose an extension, \((k,t)\)-FWL. We theoretically prove that even if we fix the space complexity to \(O(n^{k})\) (for any \(k 2\)) in \((k,t)\)-FWL, we can construct an expressiveness hierarchy up to solving the graph isomorphism problem. To tackle the second problem, we propose \(k\)-FWL+, which considers any equivariant set as neighbors instead of all nodes, thereby greatly expanding the design space of \(k\)-FWL. Combining these two modifications results in a flexible and powerful framework \((k,t)\)-FWL+. We demonstrate \((k,t)\)-FWL+ can implement most existing models with matching expressiveness. We then introduce an instance of \((k,t)\)-FWL+ called Neighborhood\({}^{2}\)-FWL (N\({}^{2}\)-FWL), which is practically and theoretically sound. We prove that N\({}^{2}\)-FWL is no less powerful than 3-WL, and can encode many substructures while only requiring \(O(n^{2})\) space. Finally, we design its neural version named N\({}^{2}\)-GNN and evaluate its performance on various tasks. N\({}^{2}\)-GNN achieves record-breaking results on ZINC-Subset (**0.059**) outperforming previous SOTA results by 10.6%. Moreover, N\({}^{2}\)-GNN achieves new SOTA results on the BREC dataset (**71.8%**) among all existing high-expressive GNN methods.

## 1 Introduction

In recent years, graph neural networks (GNNs) have become one of the most popular and powerful methods for graph representation learning, following a message passing framework [1; 2; 3; 4]. However, the expressive power of message passing GNNs is bounded by the one-dimensional Weisfeiler-Lehman (1-WL) test [5; 6]. As a result, numerous efforts have been made to design GNNs with higher expressive power. We provide a more detailed discussion in Section 5.

Several works have drawn inspiration from the \(k\)-dimensional Weisfeiler-Lehman (\(k\)-WL) or Folklore Weisfeiler-Lehman (\(k\)-FWL) test  and developed corresponding neural versions [8; 9; 10; 6]. However, \(k\)-WL/FWL has two inherent limitations. First, while the expressive power increaseswith higher values of \(k\), the space and time complexity also grows exponentially, requiring \(O(n^{k})\) space complexity and \(O(n^{k+1})\) time complexity, which makes it impractical even when \(k=3\). Thus, the question arises: **Can we retain high expressiveness without exploding both time and space complexities?** Second, the design space of WL-based algorithms is rigid, with the only adjustable hyper-parameter being \(k\). However, there is a significant gap in expressive power even between consecutive values of \(k\), making it hard to fine-tune the tradeoffs. Moreover, increasing the expressive power does not necessarily translate into better real-world performance, as it may lead to overfitting [8; 11]. Although some works try to tackle this problem [8; 10], there is still limited understanding of **how to expand the design space of the original \(k\)-FWL to a broader space** that enables us to identify the most appropriate instance to match the complexity of real-world tasks.

To tackle the first limitation, we notice that \(k\)-FWL and \(k\)-WL have the same space complexity but \(k\)-FWL can achieve the same expressive power as (\(k\)+1)-WL. We found the key component that allows \(k\)-FWL to have stronger power is the tuple aggregation style. Enlightened by this observation, we propose \((k,t)\)-FWL, which extends the tuple aggregation style in \(k\)-FWL. Specifically, in the original \(k\)-FWL, a neighbor of a \(k\)-tuple is defined by iteratively replacing its \(i\)-th element with a node \(u\), and \(u\) traverses all nodes in the graph. In \((k,t)\)-FWL, we extend a single node \(u\) to a \(t\)-tuple of nodes and carefully design a replacement scheme to insert the \(t\)-tuple into a \(k\)-tuple to define its neighbor. We demonstrate that even with a fixed space complexity of \(O(n^{k})\) (for any \(k 2\)), \((k,t)\)-FWL can construct an expressive hierarchy capable of solving the graph isomorphism problem. To deal with the second limitation, we revisit the definition of neighborhood in \(k\)-FWL. Inspired by previous works [8; 12] which consider only local neighbors (i.e., \(u\) must be connected to the \(k\)-tuple) instead of global neighbors in \(k\)-WL/FWL, we find that the neighborhood (i.e., which \(u\) are used to construct a \(k\)-tuple's neighbors) can actually be extended to any equivariant set related to the \(k\)-tuple, resulting in \(k\)-FWL+. Combining the two modifications leads to a novel and powerful FWL-based algorithm \((k,t)\)-FWL+. \((k,t)\)-FWL+ is highly flexible and can be used to design different versions to fit the complexity of real-world tasks. Based on the proposed \((k,t)\)-FWL+ framework, we implement many different instances that are closely related to existing powerful GNN/WL models, further demonstrating the flexibility of \((k,t)\)-FWL+.

Finally, we propose an instance of \((2,2)\)-FWL+ named **Neighborhood\({}^{2}\)-FWL** that is both theoretically expressive and practically powerful. It considers the local neighbors of both two nodes in a 2-tuple. Despite having a space complexity of \(O(n^{2})\), which is lower than 3-WL's \(O(n^{3})\) space complexity, this instance can still partially outperform 3-WL and is able to count many substructures. We implement a neural version named **Neighborhood\({}^{2}\)-GNN (N\({}^{2}\)-GNN)** and evaluate its performance on various synthetic and real-world datasets. Our results demonstrate that N\({}^{2}\)-GNN outperforms existing SOTA methods across most tasks. Particularly, it achieves **0.059** in ZINC-Subset, surpassing existing state-of-the-art models by significant margins. Meanwhile, it achieves **71.8%** on the BREC dataset, the new SOTA among all existing high-expressive GNN methods.

## 2 Preliminaries

**Notations.** Let \(\{\}\) denote a set, \(\{\!\!\{\}\!\}\) denote a multiset (a set that allows repetition), and \(()\) denote a tuple. As usual, let \([n]=\{1,2,,n\}\). Let \(G=(V(G),E(G),l_{G})\) be an undirected, colored graph, where \(V(G)=[n]\) is the node set with \(n\) nodes, \(E(G) V(G) V(G)\) is the edge set, and \(l_{G} V(G) C\) is the graph coloring function with \(C=\{c_{1},,c_{d}\}\) denote a set of \(d\) distinct colors. Let \(_{k}(v)\) denote a set of nodes within \(k\) hops of node \(v\) and \(Q_{k}(v)\) denote the \(k\)-th hop neighbors of node \(v\) and we have \(_{k}(v)=_{i=0}^{k}Q_{i}(v)\). Let \((u,v)\) denote the shortest path distance between \(u\) and \(v\). We use \(x_{v}^{d_{u}}\) to denote attributes of node \(v V(G)\) and \(e_{uv}^{d_{v}}\) to denote attributes of edge \((u,v) E(G)\). They are usually the one-hot encoding of the node and edge color respectively. We say that two graphs \(G\) and \(H\) are _isomorphic_ (denoted as \(G H\)) if there exists a bijection \( V(G) V(H)\) such that \( u,v V(G),(u,v) E(G)((u),(v)) E (H)\) and \( v V(G),l_{G}(v)=l_{H}((v))\). Denote \(V(G)^{k}\) the set of \(k\)-tuples of vertices and \(=(v_{1},,v_{k}) V(G)^{k}\) a \(k\)-tuple of vertices. Let \(S_{n}\) denote the permutation group of \([n]\) and \(g S_{n}:[n][n]\) be a particular permutation. When a permutation \(g S_{n}\) operates on any target \(X\), we denote it by \(g X\). Particularly, a permutation operating on an edge set \(E(G)\) is \(g E(G)=\{(g(u),g(v))|(u,v) E(G)\}\). A permutation operating on a \(k\)-tuple \(\) is \(g=(g(v_{1}),,g(v_{k}))\). A permutation operating on a graph is \(g G=(g V(G),g E(G),g l_{G})\)\(k\)-dimensional Weisfeiler-Lehman test. The \(k\)-dimensional Weisfeiler-Lehman (\(k\)-WL) test is a family of algorithms used to test graph isomorphism. There are two variants of the \(k\)-WL test: \(k\)-WL and \(k\)-FWL (Folklore WL). We first describe the procedure of 1-WL, which is also called the color refinement algorithm . Let \(^{0}_{1wl}(v)=l_{G}(v)\) be the initial color of node \(v V(G)\). At the \(l\)-th iteration, 1-WL updates the color of each node using the following equation:

\[^{l}_{1wl}(v)=(^{l-1}_{1wl}(v),\{\!\! \{^{l-1}_{1wl}(u)|u Q_{1}(v)\}\!\}).\] (1)

After the algorithm converges, a color histogram is constructed using the colors assigned to all nodes. If the color histogram is different for two graphs, then the two graphs are non-isomorphic. However, if the color histogram is the same for two graphs, they can still be non-isomorphic.

The \(k\)-WL and \(k\)-FWL, for \(k 2\), are generalizations of the 1-WL, which do not color individual nodes but node tuples \( V(G)^{k}\). Let \(_{w/j}\) be a \(k\)-tuple obtained by replacing the \(j\)-th element of \(\) with \(w\). That is \(_{w/j}=(v_{1},,v_{j-1},w,v_{j+1},,v_{k})\). The main difference between \(k\)-WL and \(k\)-FWL lies in their aggregation way. For \(k\)-WL, the set of \(j\)-th neighbors of tuple \(\) is denoted as \(Q_{j}()=\{_{w/j}|w V(G)\}\), \(j[k]\). Instead, the \(w\)-neighbor of tuple \(\) for \(k\)-FWL is denoted as \(Q_{w}^{F}()=(_{w/1},_{w/2},,_{w/k})\). Let \(^{0}_{kwl}()=^{0}_{kfwl}()\) be the initial color for \(k\)-WL and \(k\)-FWL, respectively. They are usually the isomorphism types of tuple \(\). At the \(l\)-th iteration, \(k\)-WL and \(k\)-FWL update the color of each tuple according to the following equations:

\[^{l}_{kwl}()= (^{l-1}_{kwl}(),(\{\!\!\{^{l-1}_{kwl }()| Q_{j}()\}\!\}|j[k])),\] (2) \[^{l}_{kfwl}()=(^{l-1}_{kfwl}(),\{\!\!\{^{l-1}_{kfwl} ()| Q_{w}^{F}()\}\!\!\}|w V(G). \!\!\!\}).\] (3)

The procedure described above is repeated until convergence, resulting in a color histogram of the graph that can be compared with the histograms of other graphs. Prior research has shown that (\(k\)+1)-WL and (\(k\)+1)-FWL are strictly more powerful than \(k\)-WL and \(k\)-FWL, respectively, except in the case where 1-WL is equivalent to 2-WL. Additionally, it has been shown that \(k\)-FWL is equivalent to (\(k\)+1)-WL [13; 14; 6]. We leave the additional discussion on \(k\)-WL/FWL in Appendix A.

**Message passing neural networks.** Message Passing Neural Networks (MPNNs) are a type of GNNs that update node representations by iteratively aggregating information from their direct neighbors. Let \(h^{l}_{v}\) be the output of MPNNs of node \(v V(G)\) after \(l\) layers and we let \(h^{0}_{v}=x_{v}\). At \(l\)-th layer, the representation is updated by:

\[h^{l}_{v}=^{l}(h^{l-1}_{v},\{\!\!\{m^{l}_{vu}|u Q_{1}(v)\} \!\!\}), m^{l}_{vu}=^{l}(h^{l-1}_{v},h^{l-1}_{u},e_{ vu}),\] (4)

where \(^{l}\) and \(^{l}\) are learnable update and message functions, usually parameterized by multi-layer perceptrons (MLPs). After \(L\) layers, MPNNs output the final representation \(h^{L}_{v}\) for all nodes \(v V(G)\). The graph-level representation is obtained by:

\[h_{G}=(\{\!\!\{h^{L}_{v}|v V(G)\}\!\!\}),\] (5)

where \(\) is a readout function. The expressive power of MPNNs is at most as powerful as 1-WL [5; 6].

## 3 Rethinking and extending the Folklore Weisfeiler-Lehman test

### Rethinking and extending the aggregation style in \(k\)-Fwl

Increasing \(k\) can increase the expressive power of \(k\)-FWL for distinguishing graph structures. However, increasing \(k\) brings significant memory costs, as \(k\)-FWL requires \(O(n^{k})\) memory. It becomes impractical even when \(k=3\). Therefore, it is natural to ask:

_Can we achieve higher expressive power without exploding the memory cost?_

To achieve this goal, we first notice that \(k\)-FWL has a higher expressive power than \(k\)-WL but only requires the same \(O(n^{k})\) memory cost. To understand why, let's use 2-WL and 2-FWL as examples and rewrite Equation (2) and Equation (3):

**2-WL:**: \(^{l}_{2wl}(v_{1},v_{2})=(^{l-1}_{2wl}(v_{1},v_{2}),\{\!\!\{^{l-1}_{2wl}(v_{1},w)|w V(G)\}\!\!\},\{\!\!\{ ^{l-1}_{2wl}(w,v_{2})|w V(G)\}\!\!\}),\)
**2-FWL:**: \(^{l}_{2fwl}(v_{1},v_{2})=(^{l-1}_{2fwl}(v_{1},v_{2}),\{\!\!\{^{l-1}_{2fwl}(v_{1},w),^{l-1}_{2fwl}(w,v_ {2})\}\!\})|w V(G)\}\!\!\}),\)where \(^{l}_{2wl}(v_{1},v_{2})\) and \(^{l}_{2fwl}(v_{1},v_{2})\) are the color of tuple \((v_{1},v_{2})\) at iteration \(l\) for 2-WL and 2-FWL, respectively. We can see the key difference between 2-WL and 2-FWL is that: in 2-FWL, a tuple of color \((^{l-1}_{2fwl}(v_{1},w),^{l-1}_{2fwl}(w,v_{2}))\) is aggregated. While in 2-WL, the colors of nodes are considered in separate multisets. To understand why the first aggregation is more powerful, we can further rewrite the update equation of 2-FWL as follows:

\[^{l}_{2fwl}(v_{1},v_{2})=( ^{l-1}_{2fwl}(v_{1},v_{2}),^{l-1}_{2fwl}(v_{1},w), ^{l-1}_{2fwl}(w,v_{2})\,|w V(G)}).\]

It is easy to verify that the above equation is equivalent to the original one. This means that 2-FWL updates the color of tuple \((v_{1},v_{2})\) by aggregating different tuples of \(((v_{1},v_{2}),(v_{1},w),(w,v_{2}))\), which can be viewed as aggregating information of 3-tuples \((v_{1},v_{2},w)\). For example, \((^{0}_{2fwl}(v_{1},v_{2}),^{0}_{2fwl}(v_{1},w),^{0}_{2fwl}(w,v_{2}))\) can fully recover the isomorphism type of tuple \((v_{1},v_{2},w)\) used in 3-WL. The above observations can be easily extended to \(k\)-FWL. The key insight here is that **tuple-style aggregation is the key to lifting the expressive power of \(k\)-FWL**. Since aggregating \(k\)-tuple can boost the expressive power of \(k\)-FWL to be equivalent to (\(k\)+1)-WL without the increase of space complexity, we may wonder, can we further extend the size of the tuple?

We first introduce the neighborhood tuple to extend the original \(w\)-neighbor used in \(k\)-FWL. Let \(Q^{F}_{}()\) denote a neighborhood tuple related to \(k\)-tuple \(\) and \(t\)-tuple \(\). The neighborhood tuple contains all possible results such that we sequentially select \(m[0,1,,min(k,t)]\) elements from \(\) to replace \(m\) elements in \(\). For each \(m\), we first select all possible combinations of \(m\)-tuples (no repeated elements) from tuple \(\) with a pre-defined order to form a new tuple, denoted as \(P_{m}()\). At the same time, we select all possible combinations of \(m\) indices from \(=(1,2,,k)\) with a pre-defined order to form another new tuple \(P_{m}()\). Finally, we iterate all \((^{},^{}) P_{m}() P_{m}( )\) to get a \(|P_{m}()||P_{m}()|\)-sub-tuple, where for each \((^{},^{})\) we replace the elements with indices \(^{}\) in the original \(\) with \(^{}\). The final neighborhood tuple is the concatenation of all sub-tuples. Note that the pre-defined orders can be flexible as long as they are consistent for any \(k\) and \(t\).

Here is an example of a possible construction of a neighborhood tuple when \(k=t=2\). Let \(Q^{F}_{(w_{1},w_{2})}(v_{1},v_{2})\) denote the neighborhood tuple. In this case, we need to enumerate \(m\) in \(\). First, when \(m=0\), we select 0 elements from \((w_{1},w_{2})\) to replace \((v_{1},v_{2})\), resulting in a sub-tuple with only one element \(((v_{1},v_{2}))\). Next, when \(m=1\). we sequentially select \(w_{1}\) and \(w_{2}\), resulting in \(P_{1}()=(w_{1},w_{2})\). Similarly, we can get \(P_{1}()=(2,1)\). Thus, the final sub-tuple is \(((v_{1},w_{1}),(v_{1},w_{2}),(w_{1},v_{2}),(w_{2},v_{2}))\). The case of \(m=2\) is simple and we have the result sub-tuple be \(((w_{1},w_{2}))\). By concatenating all three sub-tuples, we have \(Q^{F}_{(w_{1},w_{2})}(v_{1},v_{2})=((v_{1},v_{2}),(v_{1},w_{1}),(v_{1},w_{2}),( w_{1},v_{2}),(w_{2},v_{2}),(w_{1},w_{2}))\). The construction procedure is shown in Figure 1. By default, we adopt the above order in \(Q^{F}_{}()\) in the rest of the paper if \(k=t=2\). We leave a more formal definition of the neighborhood tuple \(Q^{F}_{}()\) in Appendix A.

With the neighborhood tuple, we are ready to introduce \((k,t)\)**-FWL**, which extends \(k\)-FWL by extending the tuple size in the update function. Let \(^{l}_{ktfwl}()\) be the color of tuple \(\) at iteration \(l\) for \((k,t)\)-FWL, the update function of \((k,t)\)-FWL is:

\[(k,t)^{l}_{ktfwl}()=( ^{l-1}_{ktfwl}(),^{l-1}_{ktfw }()| Q^{F}_{}()\,|  V^{t}(G)}_{t}),\] (6)

Figure 1: Illustration of the construction of neighborhood tuple \(Q^{F}_{(w_{1},w_{2})}(v_{1},v_{2})\) in \((2,2)\)-FWL. We sequentially select 0, 1, 2 elements from \((w_{1},w_{2})\) to replace 0, 1, 2 elements in \((v_{1},v_{2})\), resulting in three sub-tuple of length 1, 4, 1, respectively. The final neighborhood tuple is the concatenation of three sub-tuples. We can easily recover high-order graph structures from the constructed neighborhood tuple with the isomorphism type of 2-tuples.

where \(\{\!\{\}\!\}_{t}\) is hierarchical multiset over \(t\)-tuples. In a hierarchical multiset \(\{\!\{\!| V^{t}(G)\!\}\!\}_{t}\), elements are grouped hierarchically according to the node order of the tuple. For example, to construct \(\{\!\{\!(v_{1},v_{2},v_{3})|(v_{1},v_{2},v_{3}) V^{3}(G)\!\}\!\}_{3}\) from \(\{\!\{\!(v_{1},v_{2},v_{3})|(v_{1},v_{2},v_{3}) V^{3}(G)\!\}\!\}\), we first group together all elements with the same \(v_{2}\) and \(v_{3}\). That is, \( v_{2},v_{3} V(G)\), we denote \(t(v_{2},v_{3})=\{\!\{\!(v_{1},v_{2},v_{3})|v_{1} V(G)\!\}\!\}\) as the grouped result. Next, use the similar procedure, we have \( v_{3} V(G)\), \(t(v_{3})=\{\!\{\!t(v_{2},v_{3})|v_{2} V(G)\!\}\!\}\). Finally, we group all possible \(v_{3} V(G)\) to get \(\{\!\{\!(v_{1},v_{2},v_{3})|(v_{1},v_{2},v_{3}) V^{3}(G)\!\}\!\}_{3}=\{\!\{ \!t(v_{3})|v_{3} V(G)\!\}\!\}\).

It is easy to see \((k,1)\)-FWL is equivalent to \(k\)-FWL under our definition of \(Q_{}^{F}()\). Further, as we only need to maintain representations of all \(k\)-tuples. \((k,t)\)-FWL has a fixed space complexity of \(O(n^{k})\). Here we show the expressive power of \((k,t)\)-FWL.

**Proposition 3.1**.: _For \(k 2\) and \(t 1\), if \(t n-k\), \((k,t)\)-FWL can solve the graph isomorphism problems with the size of the graph less than or equal to \(n\)._

**Theorem 3.2**.: _For \(k 2\), \(t 1\), \((k,t)\)-FWL is at most as powerful as \((k+t)\)-WL. In particular, \((k,1)\)-FWL is as powerful as \((k+1)\)-WL._

**Proposition 3.3**.: _For \(k 2\), \(t 1\), \((k,t+1)\)-FWL is strictly more powerful than \((k,t)\)-FWL; \((k+1,t)\)-FWL is strictly more powerful than \((k,t)\)-FWL._

We leave all formal proofs and complexity analysis in Appendix B. Briefly speaking, even if we fix the size of \(k\), \((k,t)\)-FWL can still construct an expressive hierarchy by varying \(t\). Further, if \(t\) is large enough, \((k,t)\)-FWL can actually enumerate all possible combinations of tuples with the size of the graph, and thus equivalent to the relational pooling on graph . It is worth noting that the size of \(Q_{}^{F}()\) will grow exponentially with an increase in the size of \(t\). However, the key contribution of \((k,t)\)-FWL is that even when \(k=2\), \((k,t)\)-FWL can still construct an expressive hierarchy for solving graph isomorphism problems. Therefore, high-order embedding may not be necessary for building high-expressivity WL algorithms. Note that our \((k,t)\)-FWL is also different from subgraph GNNs such as \(k,t\)-WL  and \(l\)-OSAN , where \(l\)-tuples are labeled independently to enable learning \(k\)-tuple representations in all \(l\)-tuples' subgraphs, resulting in \(O(n^{k+l})\) space complexity.

### Rethinking and extending the aggregation scope of \(k\)-Fwl

Another problem of \(k\)-FWL is its limited design space, as the only adjustable hyperparameter is \(k\). It is well known that there is a huge gap in expressive power even if we increase from \(k\) to \(k+1\). For example, 1-WL cannot count any cycle even with a length of 3, but 2-FWL can already count up to 7-cycle . Moreover, increasing the expressive power does not always bring better performance when designing the corresponding neural version as it quickly leads to overfitting . Therefore, we ask another question:

_Can we extend the \(k\)-FWL to a more flexible and fine-grained design space?_

To address this issue, we identify that the inflexibility of \(k\)-FWL's design space arises from the definition of the neighbor used in the aggregation step. Unlike 1-WL, \(k\)-FWL lacks the concept of local neighbors and instead requires the aggregation of all \(|V(G)|\) global neighbors to update the color of each tuple \(\). Recently, some works have extended \(k\)-WL by incorporating local information . Inspired by previous works, we find that the definition of neighbor can actually be much more flexible than just considering local neighbors or global neighbors. Specifically, for each \(k\)-tuple \(\) in graph \(G\), we define equivariant set \(ES()\) to be the neighbors set of tuple \(\) and propose \(k\)**-FWL+**.

**Definition 3.4**.: _An **equivariant set**\(ES()\) is a set of nodes related to \(\) and equivariant given the permutation \(g S_{n}\). That is, \( w ES()\) in graph \(G\) implies \(g(w) ES(g)\) in graph \(g G\)._

Some nature equivariant sets \(ES(v)\) including \(V(G)\), \(_{k}(v)\), and \(Q_{k}(v)\), etc. Let \(_{kfwl+}^{l}()\) be the color of tuple \(\) at iteration \(l\) for \(k\)-FWL+, we have:

\[k_{kfwl+}^{l}()=( _{kfwl+}^{l-1}(),\!\{\!_{kfwl+}^{l-1}( )| Q_{w}^{F}()\,|w ES() \!\}\!\}).\] (7)

The key of \(k\)-FWL+ is that the equivariant set \(ES()\) can be any set of nodes as long as it is equivariant to any permutation \(g S_{n}\). For example, if \(ES()=V(G)\), the \(k\)-FWL+ will reduce to the original \(k\)-FWL. Instead, if \(ES()=_{i=0}^{k}_{1}(v_{i})\), it becomes the localized \(k\)-FWL . We can also design other more innovative equivariant sets \(ES()\). For example, denote \(()\) to be a set that contains all nodes in the shortest paths between any \(v_{i},v_{j}\). It is still a valid equivariant set of tuple \(\). We can see that the design space of \(k\)-FWL+ is much broader than \(k\)-FWL.

[MISSING_PAGE_FAIL:6]

[MISSING_PAGE_FAIL:7]

connected node sets. However, these approaches can still be impractical despite careful design. Our work hugely extends the design space of FWL-based methods and demonstrates that the resulting model can be expressive even with a lower space complexity cost.

**Permutation equivariant and invariant networks**. This line of research aims to leverage the permutation equivariant and invariant properties of graphs and devise corresponding architectures. For example, Maron et al.  proposed \(k\)-IGNs, which employ permutational equivariant linear layers with point-wise activation functions to the adjacency matrix. \(k\)-IGNs have been proven to have the same expressive power as \(k\)-WL [9; 36; 37]. Another approach utilizes the Reynold operator to consider all possible permutations on either global  or local  levels. However, they consider all possible permutations in order to achieve universality, which may lead to overfitting in real-world tasks and are more valuable from a theoretical perspective. In contrast, our framework is highly flexible and can be tailored to fit the complexity of real-world tasks.

**Feature-based methods**. Feature-based methods aim to incorporate graph-related features that cannot be computed by MPNNs. For instance, some methods add random features to nodes to break the symmetry [38; 39]. GSN  introduces substructure counting features into MPNNs. GD-WL  applies general distance features to enhance the expressiveness of MPNNs and shows it can solve the graph biconnectivity problem. GDGNN  integrates the geodesic information between node pairs and achieves a great balance between performance and efficiency. ESC-GNN  employs subgraph features to simulate subgraph GNNs without running them explicitly. Puny et al.  computes graph polynomial features and injects them into PPGN to achieve greater expressive power than 3-WL within \(O(n^{2})\) space. Although feature-based methods are highly efficient, they often suffer from overfitting in real-world tasks and produce suboptimal results.

## 6 Experiments

In this section, we conduct various experiments on synthetic and real-world tasks to verify the effectiveness of N\({}^{2}\)-GNN. The details of all experiments can be found in Appendix E. Additional experimental results and ablation studies are included in Appendix F. The source code is provided in https://github.com/JiaruiFeng/N2GNN.

### Expressive power

**Datasets**. To verify the expressive power of N\({}^{2}\)-GNN, we select four synthetic datasets, all containing graphs that cannot be distinguished by 1-WL/MPNNs. The datasets we selected are (1) EXP , which consists of 600 pairs of non-isomorphic graphs that 1-WL fails to distinguish; (2) CSL , which contains 150 4-regular graphs divided into 10 isomorphism classes; and (3) SR25 , which contains 15 non-isomorphic strongly regular graphs, each has 25 nodes. Even 3-WL is unable to distinguish between these graphs. (4) BREC , which contains 400 non-isomorphic graph pairs that range from 1-WL-indistinguishable to 4-WL-indistinguishable.

**Models**. For EXP, CSL, and SR25, we only report results for N\({}^{2}\)-GNN. For BREC, we compare N\({}^{2}\)-GNN with I\({}^{2}\)-GNN  as it is the current SOTA on BREC among all GNN methods.

**Results**. We report results in Table 1 and Table 2. For EXP and CSL, we report the average accuracy for 10-time cross-validation; For SR25, we report single-time accuracy. We can see N\({}^{2}\)-GNN achieves perfect results on all three datasets, empirically verifying its expressive power. Notably, N\({}^{2}\)-GNN is able to distinguish strongly regular graphs. This empirically verified Corollary 4.2. Moreover, N\({}^{2}\)-GNN achieves the best result on the BREC dataset among all GNNs methods, surpassing the previous SOTA, I\({}^{2}\)-GNN (See the complete comparison and further discussion in Appendix F). Despite this improved performance, N\({}^{2}\)-GNN only requires at most \(O(n^{2})\) space complexity, which is less than 3-WL. This further demonstrates the superiority of N\({}^{2}\)-GNN.

   Datasets & EXP & CSL & SR25 \\  N\({}^{2}\)-GNN & 100 & 100 & 100 \\   

Table 1: Expressive power verification (Accuracy).

    &  &  &  &  &  \\  Model & Number & Accuracy & Number & Accuracy & Number & Accuracy & Number & Accuracy & Number & Accuracy \\  I\({}^{2}\)-GNN & 60 & 100\% & 100 & 71.4\% & 100 & 100\% & 21 & 21\% & 281 & 70.2\% \\ N\({}^{2}\)-GNN & 60 & 100\% & 100 & 71.4\% & 100 & 100\% & 27 & 27\% & **287** & **71.8\%** \\   

Table 2: Pair distinguishing accuracies on BREC

### Substructure counting

**Datasets.** To verify the substructure counting power of N\({}^{2}\)-GNN, we select the synthetic dataset from [25; 19]. The dataset consists of 5000 randomly generated graphs from different distributions, which are split into training, validation, and test sets with a ratio of 0.3/0.2/0.5. The task is to perform node-level counting regression. Specifically, we choose tailed-triangle, chordal cycles, 4-cliques, 4-paths, triangle-rectangle, 3-cycles, 4-cycles, 5-cycles, and 6-cycles as target substructures.

**Models**. We compare N\({}^{2}\)-GNN with the following baselines: Identity-aware GNN (ID-GNN) , Nested GNN (NGNN) , GNN-AK+ , PPGN , I\({}^{2}\)-GNN . Results for all baselines are reported from .

**Results**. We report the results for counting substructures in Table 3. All results are the average normalized test MAE of three runs with different random seeds. We colored all results that are less than 0.01, which is an indication of successful counting, the same as in . We can see that N\({}^{2}\)-GNN achieves MAE less than 0.01 among all different substructures/cycles, which empirically verified Theorem 4.3. Specifically, N\({}^{2}\)-GNN achieves the best result on 4-path counting and comparable performance to I\({}^{2}\)-GNN on most substructures. Moreover, N\({}^{2}\)-GNN can count 4-clique extremely well, which is theoretically and empirically infeasible for 3-WL  and PPGN .

### Molecular properties prediction

**Datasets**. To evaluate the performance of N\({}^{2}\)-GNN on real-world tasks, we select two popular molecular graphs datasets: QM9 [46; 50] and ZINC . QM9 dataset contains over 130K molecules with 12 different molecular properties as the target regression task. The dataset is split into training, validation, and test sets with a ratio of 0.8/0.1/0.1. The ZINC dataset has two variants: ZINC-subset (12k graphs) and ZINC-full (250k graphs), and the task is graph regression. The training, validation, and test splits for the ZINC datasets are provided.

**Models**. For QM9, we report baseline results of DTNN and MPNN from . We further adopt PPGN , NGNNs , KP-GIN\(\), I\({}^{2}\)-GNN . We report results of PPGN, NGNN, and KP-GIN\(\) from  and results of I\({}^{2}\)-GNN from . For ZINC, the baseline including CIN , \(\)-2-GNN , KC-SetGNN , PPGN , Graphormer-GD , GPS , Specformer , NGNN , GNN-AK-ctx , ESAN , SUN , KP-GIN\(\), I\({}^{2}\)-GNN , SSWL+ . We report results of all baselines from [12; 43; 10; 48; 49; 19].

   Target & DTNN  & MPNN  & PPGN  & NGNN  & KP-GIN\({}^{2}\) & I\({}^{2}\)-GNN  & N\({}^{2}\)-GNN \\  \(\) & 0.244 & 0.358 & **0.231** & 0.433 & 0.358 & 0.428 & 0.333 \\ \(\) & 0.95 & 0.89 & 0.382 & 0.265 & 0.233 & 0.230 & **0.193** \\ \(_{}\) & 0.00388 & 0.00541 & 0.00276 & 0.00279 & 0.00240 & 0.00261 & **0.00217** \\ \(_{}\) & 0.00512 & 0.00623 & 0.00287 & 0.00276 & 0.00236 & 0.00267 & **0.00210** \\ \(\) & 0.0112 & 0.0066 & 0.00406 & 0.00390 & 0.00333 & 0.00380 & **0.00304** \\ \( R^{2}\) & 17.0 & 28.5 & 16.7 & 20.1 & 16.51 & 18.64 & **14.47** \\ \(Z\)PVE & 0.00172 & 0.00216 & 0.00064 & 0.00015 & 0.00017 & 0.00014 & **0.00013** \\ \(U_{0}\) & 2.43 & 2.05 & 0.234 & 0.205 & 0.0682 & 0.211 & **0.0247** \\ \(U\) & 2.43 & 2.00 & 0.234 & 0.200 & 0.0696 & 0.206 & **0.0315** \\ \(H\) & 2.43 & 2.02 & 0.229 & 0.249 & 0.0641 & 0.269 & **0.0182** \\ \(G\) & 2.43 & 2.02 & 0.238 & 0.253 & 0.0484 & 0.261 & **0.0178** \\ \(C_{v}\) & 0.27 & 0.42 & 0.184 & 0.0811 & 0.0869 & **0.0730** & 0.0760 \\   

Table 4: MAE results on QM9 (smaller the better).

   Target & ID-GNN  & NGNN  & GIN-AK+  & PPGN  & I\({}^{2}\)-GNN  & N\({}^{2}\)-GNN \\  Tailed Triangle & 0.1053 & 0.1044 & 0.0043 & 0.0026 & 0.0011 & 0.0025 \\ Chordal Cycle & 0.0454 & 0.0392 & 0.0112 & 0.0015 & 0.0010 & 0.0019 \\
4-Clique & 0.0026 & 0.0045 & 0.0049 & 0.1646 & 0.0003 & 0.0005 \\
4-Path & 0.0273 & 0.0244 & 0.0075 & 0.0041 & 0.0041 & 0.0042 \\ Tri.-Rec. & 0.0628 & 0.0729 & 0.1311 & 0.0144 & 0.0013 & 0.0055 \\
3-Cycles & 0.0006 & 0.0003 & 0.0004 & 0.0003 & 0.0003 & 0.0002 \\
4-Cycles & 0.0022 & 0.0013 & 0.0041 & 0.0009 & 0.0016 & 0.0024 \\
5-Cycles & 0.0490 & 0.0402 & 0.0133 & 0.0036 & 0.0028 & 0.0039 \\
6-Cycles & 0.0495 & 0.0439 & 0.0238 & 0.0071 & 0.0082 & 0.0075 \\   

Table 3: Evaluation on Counting Substructures (norm MAE), cells with MAE less than 0.01 are colored.

## 7 Limitations, future works, and conclusions

**Limitations:** For \((k,t)\)-FWL, although we can control the space complexity by fixing the \(k\), the time complexity of \((k,t)\)-FWL will grow exponentially with the increase of \(t\) (as discussed in Appendix B). Therefore, the resulting model can still be impractical when \(t\) is large. We believe this result is evidence of the "no free lunch" in developing a more expressive GNN model. For \((k,t)\)-FWL+, although we demonstrate its capability by implementing many instances corresponding to different existing GNN models, it is still unclear what is the whole space of \((k,t)\)-FWL+, especially the space of \(ES()\). Moreover, the quantitative expressiveness analysis of \((k,t)\)-FWL+ is still unexplored. Finally, for N\({}^{2}\)-GNN, the practical complexity can still be unbearable, especially for dense graphs if the aggregation is implemented in a parallel way, as we need to aggregate many more neighbors for each tuple than normal MPNN. It also introduces the optimization issue on N\({}^{2}\)-GNN, which makes it hard to achieve its theoretical expressiveness. In Appendix B, C, and D, we provide more detailed discussion on the limitation of \((k,t)\)-FWL, \((k,t)\)-FWL+, and N\({}^{2}\)-GNN, respectively.

**Future works**: There are several directions that are worth further exploration. First, can we characterize the whole space of \(ES()\) and thus can theoretically and quantitatively analyze the expressive power of \((k,t)\)-FWL+. Further, how to design advanced model architecture to achieve the theoretical power of \((k,t)\)-FWL+. Finally, can we provide a systematic implementation code package and practical usage of different downstream tasks such that researchers can easily develop an instance of \((k,t)\)-FWL+ that can best fit their downstream tasks? We leave these to our future work.

**Conclusions:** In this work, we propose \((k,t)\)-FWL+, a flexible and powerful extension of \(k\)-FWL. First, \((k,t)\)-FWL+ expands the tuple aggregation style in \(k\)-FWL. We theoretically prove that given any fixed pace complexity of \(O(n^{k})\), \((k,t)\)-FWL+ can still construct an expressiveness hierarchy up to solving the graph isomorphism problem. Second, \((k,t)\)-FWL+ extends the global neighborhood definition in \(k\)-FWL to any equivariant set, which enables a finer-grained design space. We show that the \((k,t)\)-FWL+ framework can implement many existing powerful GNN models with matching expressiveness. We further implement an instance named N\({}^{2}\)-GNN which is both practically powerful and theoretically expressive. Theoretically, it partially outperforms 3-WL but only requires \(O(n^{2})\) space. Empirically, it achieves new SOTA on BREC, ZINC-Subset, and ZINC-Full datasets. We envision \((k,t)\)-FWL+ can serve as a promising general framework for designing highly effective GNNs tailored to real-world problems.

## 8 Acknowledgments

Jiarui Feng, Lecheng Kong, Hao Liu, and Yixin Chen are supported by NSF grant CBE-2225809. Muhan Zhang is partially supported by the National Natural Science Foundation of China (62276003) and Alibaba Innovative Research Program.

   Model & \# Param &  ZINC-Subset \\ Test MAE \\  & 
 ZINC-Full \\ Test MAE \\  \\  CIN  & -100k & 0.079 \(\) 0.006 & **0.022 \(\) 0.002** \\  \(\)-2-GNN  & - & - & 0.042 \(\) 0.003 \\ KC-SetGNN  & - & 0.075 \(\) 0.003 & - \\ PPGN  & - & 0.079 \(\) 0.005 & 0.022 \(\) 0.003 \\  GD-WL  & 503k & 0.081 \(\) 0.009 & 0.025 \(\) 0.004 \\ GPS  & 424k & 0.070 \(\) 0.004 & - \\ Spectrometer  & -500k & 0.066 \(\) 0.003 & - \\  NGNN  & -500k & 0.111 \(\) 0.003 & 0.029 \(\) 0.001 \\ GNN-AK  & -500k & 0.093 \(\) 0.002 & - \\ ESAN  & 446k & 0.097 \(\) 0.006 & 0.025 \(\) 0.003 \\ SUN  & 526k & 0.083 \(\) 0.003 & 0.024 \(\) 0.003 \\ KP-GIN\({}^{2}\) & 489k & 0.093 \(\) 0.007 & - \\ I\({}^{2}\)-GNN  & - & 0.083 \(\) 0.001 & 0.023 \(\) 0.001 \\ SSWL+ \{1.2\} & 387k & 0.070 \(\) 0.005 & **0.022 \(\) 0.002** \\  N\({}^{2}\)-GNN & 316k/414k & **0.059 \(\) 0.002** & **0.022 \(\) 0.002** \\   

Table 5: MAE results on ZINC (smaller the better).