# Zero-One Laws of Graph Neural Networks

Sam Adam-Day

Department of Mathematics

University of Oxford

Oxford, UK

sam.adam-day@cs.ox.ac.uk

&Theodor-Mihai Iliant

Department of Computer Science

University of Oxford

Oxford, UK

theodor-mihai.iliant@lmh.ox.ac.uk

&Ismail Ilkan Ceylan

Department of Computer Science

University of Oxford

Oxford, UK

ismail.ceylan@cs.ox.ac.uk

###### Abstract

Graph neural networks (GNNs) are the de facto standard deep learning architectures for machine learning on graphs. This has led to a large body of work analyzing the capabilities and limitations of these models, particularly pertaining to their representation and extrapolation capacity. We offer a novel theoretical perspective on the representation and extrapolation capacity of GNNs, by answering the question: how do GNNs behave as the number of graph nodes become very large? Under mild assumptions, we show that when we draw graphs of increasing size from the Erdos-Renyi model, the probability that such graphs are mapped to a particular output by a class of GNN classifiers tends to either _zero_ or to _one_. This class includes the popular graph convolutional network architecture. The result establishes 'zero-one laws' for these GNNs, and analogously to other convergence laws, entails theoretical limitations on their capacity. We empirically verify our results, observing that the theoretical asymptotic limits are evident already on relatively small graphs.

## 1 Introduction

Graphs are common structures for representing relational data in a wide range of domains, including physical , chemical , and biological  systems, which sparked interest in machine learning over graphs. Graph neural networks (GNNs)  have become prominent models for graph machine learning for a wide range of tasks, owing to their capacity to explicitly encode desirable relational inductive biases . One important virtue of these architectures is that every GNN model can be applied to _arbitrarily large_ graphs, since in principle the model parameters are independent of the graph size. This raises the question: how do GNNs behave as the number of nodes becomes very large? When acting as binary classifiers, GNNs can be thought of as parametrizing Boolean properties of (labelled) graphs. A classical method of specifying such properties is through first-order formulas, which allow for precise definitions using a formal language . The celebrated 'zero-one law' for first-order logic  provides a crisp answer to the question of the asymptotic behaviour of such properties: as graphs of increasing size are drawn from the Erdos-Renyi distribution, the probability that a _first-order_ property holds either tends to _zero_ or to _one_.

In this paper, we show an analogous result for binary classification GNNs: under mild assumptions on the model architecture, several GNN architectures including graph convolutional networks  satisfy a zero-one law over Erdos-Renyi graphs with random node features. The principal import of this result is that it establishes a novel upper-bound on the expressive power of GNNs: any property of graphs which can be _uniformly_ expressed by a GNN must obey a zero-one law. An example of a simple property which does _not_ asymptotically tend to zero or one is that of having an even number of nodes. Note however that our result, combined with the manifest success of GNNs in practice, suggests that zero-one laws must be abundant in nature: if a property we cared about did not satisfy a zero-one law, none of the GNN architectures we consider would be able to express it.

Our main results flexibly apply both to the case where we consider the GNN as a classifier applied to randomly sampled graphs and node features, and where we consider the random node features as part of the model. For the latter, it is known that incorporating randomized features into the model significantly increases its expressive power on graphs with _bounded number of nodes_. Our results yield the first upper bound on the expressive power of these architectures which is _uniform_ in the number of nodes. We complement this with a corresponding lower bound, showing that these architectures can universally approximate any property which satisfies a certain zero-one law.

A key strength of the results is that they apply equally well to randomly initialized networks, trained networks, and anything in between. In this sense, our asymptotic analysis is orthogonal to the question of optimization, and holds regardless of the choice of training method. Another interesting aspect of these results is that they unite analysis of expressive power with extrapolation capacity. Our zero-one laws simultaneously provide limits on the ability of GNNs to extrapolate from smaller Erdos-Renyi graphs to larger ones: eventually any GNN must classify all large graphs the same way.

To validate our theoretical findings, we conduct a series of experiments: since zero-one laws are of asymptotic nature, we may need to consider very large graphs to observe clear empirical evidence for the phenomenon. Surprisingly however, GNNs already exhibit clear evidence of a zero-one law even on small graphs. Importantly, this is true for networks with very few layers (even a single-layer), which is reassuring, as it precludes confounding factors, such as the effect of over-smoothing due to increased number of layers . We provide further experimental results in the appendix of this paper, where all proofs of technical statements can also be found. We make the code for our experiments available online at [https://github.com/SamAdamDay/Zero-One-Laws-of-Graph-Neural-Networks](https://github.com/SamAdamDay/Zero-One-Laws-of-Graph-Neural-Networks).

## 2 Preliminaries

**Random graphs and matrices.** The focus of our study is on classes of random graphs with random features, for which we introduce some notation. We write \(^{d}\) to represent a vector, and \(^{d n}\) to represent a matrix. Analogously, we write \(\) to denote a _random_ vector, and \(\) to denote a _random_ matrix, whose entries are (real) random variables. We write \((n,r)\) to denote the class of simple, undirected Erdos-Renyi (ER) graphs with \(n\) nodes and edge probability \(r\) and let \((d)\) denote some distribution of feature vectors over \(^{d}\). We define an Erdos-Renyi graph equipped with random node features as a pair \(=(,)\), where \((n,r)\) is the random graph adjacency matrix of the graph \(G=(V,E)\) and \(^{d n}\) is a corresponding random feature matrix, independent of \(G\), which contains, for each node \(v V\), an initial random node feature \(_{v}(d)\) as the corresponding columns of \(\).2

**Message passing neural networks.** The focus of this work is on _message-passing neural networks (MPNNs)_ which encapsulate the vast majority of GNNs. The fundamental idea in MPNNs is to update the initial (random) state vector \(_{v}^{(0)}=_{v}\) of each node \(v\) for \(T\) iterations, based on its own state and the state of its neighbors \((v)\) as:

\[_{v}^{(t+1)}=^{(t)}_{v}^{(t)},^{(t)} _{v}^{(t)},\{\!\!\{_{u}^{(t)}|\,u(v)\}\!\} ,\]

where \(\{\!\!\{\}\!\}\) denotes a multiset, and \(^{(t)}\) and \(^{(t)}\) are differentiable _combination_, and _aggregation_ functions, respectively. Each layer's node representations can have different dimensions: we denote by \(d(t)\) the dimension of the node embeddings at iteration \(t\) and typically write \(d\) in place of \(d(0)\).

The final node representations can then be used for node-level predictions. For graph-level predictions, the final node embeddings are _pooled_ to form a graph embedding vector to predict properties of entire graphs. The pooling often takes the form of simple averaging, summing or component-wise maximum. For Boolean node (resp., graph) classification, we further assume a classifier \(:^{d(T)}\) which acts on the final node representations (resp., on the final graph representation).

There exist more general message passing paradigms  such as _MPNNs with global readout_ which additionally aggregate over all node features at every layer, and are known to be more expressive . Some model architectures considered in this paper include a global readout component and we consider different choices for the combine (\(^{(t)}\)) and aggregate (\(^{(t)}\)) functions, as we introduce next.

**GCN.** The primary GNN architecture we consider is _graph convolutional networks_ (GCN) . These are instances of MPNNs with self-loops, which aggregate over the extended neighborhood of a node \(^{+}(v):=(v)\{v\}\). GCNs iteratively update the node representations as \(_{v}^{(t)}=(_{v}^{(t)})\), where the preactivations are given by:

\[_{v}^{(t)}=_{n}^{(t)}_{u^{+}(v)}(u)||(v)|}}_{u}^{(t-1)}+^{(t)}\]

We apply the linear transformation \(_{n}^{(t)}^{d(t) d(t-1)}\) to a normalized sum of the activations for the previous layers of the neighbors of the node under consideration, together with its own activation. Adding a bias term \(^{(t)}\) yields the preactivation \(_{v}^{(t)}\), to which we apply the non-linearity \(\).

**MeanGNN.** We also consider the MeanGNN\({}^{+}\) architecture which is a _self-loop GNN with mean aggregation and global readout_, and updates the node representation as \(_{v}^{(t)}=(_{v}^{(t)})\), where:

\[_{v}^{(t)}=^{+}(v)|}_{n}^{(t)}_{u ^{+}(v)}_{u}^{(t-1)}+_{r}^{(t)}_{u V }_{u}^{(t-1)}+^{(t)}\]

MeanGNN\({}^{+}\) models additionally apply a linear transformation \(_{r}^{(t)}^{d(t) d(t-1)}\) to the mean of all previous node representations. We refer to MeanGNN as the special case of this architecture which does _not_ include a global readout term (obtained by dropping the second term in the equation).

**SumGNN.** Finally, we consider the SumGNN\({}^{+}\) architecture which is a _GNN with sum aggregation and global readout_, and updates the node representations as \(_{u}^{(t)}=(_{u}^{(t)})\), where:

\[_{v}^{(t)}=_{s}^{(t)}_{v}^{(t-1)}+_{n}^{(t)} _{u(v)}_{u}^{(t-1)}+_{r}^{(t)}_{u V} _{u}^{(t-1)}+^{(t)}\]

This time, we separate out the contribution from the preactivation of the previous activation for the node itself. This yields three linear transformations \(_{s}^{(t)},_{n}^{(t)},_{r}^{(t)}^{d(t) d(t -1)}\). The corresponding architecture without the global readout term is called SumGNN.

## 3 Related work

Graph neural networks are flexible models which can be applied to graphs of any size following training. This makes an asymptotic analysis in the size of the input graphs very appealing, since such a study could lead to a better understanding of the extrapolation capabilities of GNNs, which is widely studied in the literature [41; 40]. Previous studies of the asymptotic behaviour of GNNs have focused on convergence to theoretical limit networks [20; 31] and their stability under the perturbation of large graphs [11; 22].

Zero-one laws have a rich history in first-order logic and random graph theory [13; 8; 25; 34; 6]. Being the first of its kind in the graph machine learning literature, our study establishes new links between graph representation learning, probability theory, and logic, while also presenting a new and interesting way to analyze the expressive power of GNNs. It is well-known that the expressive power of MPNNs is upper bounded by the _1-dimensional Weisfeiler Leman graph isomorphism test (1-WL)_[39; 29] and architectures such as SumGNN\({}^{+}\) can match this. Barcelo et al.  further gives a logical characterization for a class of MPNNs showing SumGNN\({}^{+}\) can capture any function which can be expressed in the logic \(^{2}\), which is an extension of the two-variable fragment of first-order logic with counting quantifiers. Several works study the expressive power of these models under the assummption that there are _unique node identifiers_, or define _higher-order_ GNN models [29; 27; 28; 19] to obtain more expressive architectures.

Our work has direct implications on GNNs using random node features [32; 1], which are universal in the bounded graph domain. Specifically, we derive a zero-one law for GNNs using random node features which puts an upper bound on the expressive power of such models in a uniform sense: what class of functions on graphs can be captured by a _single_ GNN with random node features? Abboud et al.  prove a universality result for these models, but it is not uniform, since the construction depends on the graph sizes, and yields a different model parametrization depending on the choice of the graph sizes. Moreover, the construction of Abboud et al.  is of size exponential in the worst case. Grohe  recently improved upon this result, by proving that the functions that can be computed by a _polynomial-size bounded-depth_ family of GNNs using random node features are exactly the functions computed by bounded depth Boolean circuits with threshold gates. This establishes an upper bound on the power of GNNs with random node features, by requiring the class of models to be of bounded depth (fixed layers) and of size polynomial. However, this result is still _not_ uniform, since it allows the target function to be captured by different model parametrizations. There is no known upper bound for the expressive power of GNNs with random node features in the uniform setting, and our result establishes this.

Other limitations of MPNNs include _over-smoothing_[23; 30] and _over-squashing_ which are related to information propagation, and are linked to using more message passing layers. The problem of over-smoothing has also been studied from an asymptotic perspective [23; 30], where the idea is to see how the node features evolve as we increase the number of layers in the network. Our study can be seen as orthogonal to this work: we conduct an asymptotic analysis in the size of the graphs rather than in the number of layers.

## 4 Zero-one laws of graph neural networks

### Problem statement

We first define graph invariants following Grohe .

**Definition 4.1**.: A _graph invariant_\(\) is a function over graphs, such that for any pair of graphs \(G_{1}\), \(G_{2}\), and, for any isomorphism \(f\) from \(G_{1}\) to \(G_{2}\) it holds that \((G_{1})=(f(G_{2}))\). Graph invariants for graphs with node features are defined analogously.

Consider any GNN model \(\) used for binary graph classification. It is immediate from the definition that \(\) is invariant under isomorphisms of the graphs on which it acts. Hence \(\), considered as function from graphs to \(=\{0,1\}\), is a graph invariant. In this paper, we study the asymptotic behavior of \(\) as the number of nodes increases.

One remarkable and influential result from finite model theory is the 'zero-one law' for first-order logic. A (Boolean) graph invariant \(\) satisfies a _zero-one law_ if when we draw graphs \(G\) from the ER distribution \((n,r)\), as \(n\) tends to infinity the probability that \((G)=1\) either tends to \(0\) or tends to \(1\). The result, due to Glebskii et al.  and Fagin , states that any graph invariant which can be expressed by a first-order formula satisfies a zero-one law. Inspired by this asymptotic analysis of first-order properties, we ask whether GNNs satisfy a zero-one law. As the input of a GNN is a graph with node features, we need to reformulate the statement of the law to incorporate these features.

**Definition 4.2**.: Let \(=(,)\) be a graph with node features, where \((n,r)\) is a graph adjacency matrix and, independently, \(\) is a matrix of node embeddings, where \(_{v}(d)\) for every node \(v\). A graph invariant \(\) for graphs with node features satisfies a _zero-one law_ with respect to \((n,r)\) and \((d)\) if, as \(n\) tends to infinity, the probability that \(()=1\) tends to either \(0\) or \(1\).

Studying the asymptotic behavior of GNNs helps to shed light on their capabilities and limitations. A zero-one law establishes a limit on the ability of such models to extrapolate to larger graphs: any GNN fitted to a finite set of datapoints will tend towards outputting a constant value on larger and larger graphs drawn from the distribution described above. A zero-one law in this setting also transfers to a corresponding zero-one law for GNNs with random features. This establishes an upper-bound on the uniform expressive power of such models.

### Graph convolutional networks obey a zero-one law

Our main result in this subsection is that (Boolean) GCN classifiers obey a zero-one law. To achieve our result, we place some mild conditions on the model and initial node embeddings.

First, our study covers sub-Gaussian random vectors, which in particular include all _bounded random vectors_, and all _multivariate normal random vectors_. We note that in every practical setup all node features have bounded values (determined by the bit length of the storage medium), and are thus sub-Gaussian.

**Definition 4.3**.: A random vector \(^{d}\) is _sub-Gaussian_ if there is \(C>0\) such that for every unit vector \(^{d}\) the random variable \(\) satisfies the _sub-Gaussian property_; that is, for every \(t>0\):

\[(|| t) 2(-}{ C^{2}})\]

Second, we require that the non-linearity \(\) be Lipschitz continuous. This is again a mild assumption, because all non-linearities used in practice are Lipschitz continuous, including \(\), clipped \(\), \(\), linearized \(\) and \(\).

**Definition 4.4**.: A function \(f\) is _Lipschitz continuous_ if there is \(C>0\) such that for any \(x,y\) it holds that \(|f(x)-f(y)| C|x-y|\).

Third, we place a condition on the GCN weights with respect to the classifier function \(^{d(T)}\), which intuitively excludes a specific weight configuration.

**Definition 4.5**.: Consider a distribution \((d)\) with mean \(\). Let \(\) be a GCN used for binary graph classification. Define the sequence \(_{0},,_{T}\) of vectors inductively by \(_{0}:=\) and \(_{t}:=(_{n}^{(t)}_{t-1}+ ^{(t)})\). The classifier classifier \(:^{d(T)}\) is _non-splitting_ for \(\) if the vector \(_{T}\) does not lie on a decision boundary for \(\).

For all reasonable choices of \(\), the decision boundary has dimension lower than the \(d(T)\), and is therefore a set of zero-measure. This means that in practice essentially all classifiers are non-splitting.

Given these conditions, we are ready to state our main theorem:

**Theorem 4.6**.: _Let \(\) be a GCN used for binary graph classification and take \(r\). Then, \(\) satisfies a zero-one law with respect to graph distribution \((n,r)\) and feature distribution \((d)\) assuming the following conditions hold: (i) the distribution \((d)\) is sub-Gaussian, (ii) the non-linearity \(\) is Lipschitz continuous, (iii) the graph-level representation uses average pooling, and (iv) the classifier \(\) is non-splitting._

The proof hinges on a probabilistic analysis of the preactivations in each layer. We use a sub-Gaussian concentration inequality to show that the deviation of each of the first-layer preactivations \(_{v}^{(1)}\) from its expected value becomes less and less as the number of node \(n\) tends to infinity. Using this and the fact that \(\) is Lipschitz continuous we show then that each activation \(_{v}^{(1)}\) tends towards a fixed value. Iterating this analysis through all the layers of the network yields the following key lemma, which is the heart of the argument.

**Lemma 4.7**.: _Let \(\) and \((d)\) satisfy the conditions in Theorem 4.6. Then, for every layer \(t\), there is \(_{t}^{d(t)}\) such that when sampling a graph with node features from \((n,r)\) and \((d)\), for every \(i\{1,,d(t)\}\) and for every \(>0\) we have that:_

\[( v intuitive argument for why the rate of convergence should decrease as the embedding dimensionality increases: if we fix a node \(v\) and a layer \(t\) then each of the components of its preactivation can be viewed as the weighted sum of \(d(t-1)\) random variables, each of which is the aggregation of activations in the previous layer. Intuitively, as \(d(t-1)\) increases, the variance of this sum also increases. This increased variance propagates through the network, resulting in a higher variance for the final node representations and thus a slower convergence.

Using analogous assumptions and techniques to those presented in this section, we also establish a zero-one law for MeanGNN\({}^{+}\), which we report in detail in Appendix B. Abstracting away from technicalities, the overall structure of the proofs for MeanGNN\({}^{+}\) and GCN is very similar, except that for the former case we additionally need to take care of the global readout component.

### Graph neural networks with sum aggregation obey a zero-one law

The other variant of GNNs we consider are those with sum aggregation. The proof in the case works rather differently, and we place different conditions on the model.

**Definition 4.8**.: A function \(\) is _eventually constant in both directions_ if there are \(x_{-},x_{}\) such that \((y)\) is constant for \(y<x_{-}\) and \((y)\) is constant for \(y>x_{}\). We write \(_{-}\) to denote the minimum and \(_{}\) to denote the maximum value of an eventually constant function \(\).

This means that there is a threshold (\(x_{-}\)) below which sigma is constant, and another threshold (\(x_{}\)) above which sigma is constant. Both the linearized \(\) and clipped \(\) are eventually constant in both directions. Moreover, when working with finite precision any function with vanishing gradient in both directions (such as \(\)) can be regarded as eventually constant in both directions.

We also place the following condition on the weights of the model with respect to the mean of \((d)\) and the edge-probability \(r\).

**Definition 4.9**.: Let \(\) be any SumGNN\({}^{+}\) for binary graph classification with a non-linearity \(\) which is eventually constant in both directions. Let \((d)\) be any distribution with mean \(\), and let \(r\). Then, the model \(\) is _synchronously saturating_ for \((n,r)\) and \((d)\) if the following conditions hold:

1. For each \(1 i d(1)\): \[[(r_{n}^{(1)}+_{g}^{(1)})]_{i} 0\]
2. For every layer \(1<t T\), for each \(1 i d(t)\) and for each \(\{_{-},_{}\}^{d(t-1)}\): \[[(r_{n}^{(t)}+_{g}^{(t)})]_{i} 0\]

Analysis of our proof of the zero-one law for SumGNN\({}^{+}\) models (Theorem 4.10 below) reveals that the asymptotic behaviour is determined by the matrices \(_{t} r_{n}^{(t)}+_{g}^{(t)}\), where the asymptotic final layer embeddings are \((_{T}((_{T-1}(_{0}))))\).).).).).).).).).).).).).).).).).).).).).).).).).).).).).).).).).).).).).).).).).).).).).).).).).).).).).).).).).).).).).).).).).).).).).).).).).).).).).).).)).).).).).).).)).).).).).).)).).).).)).).).)).).).)).).).)).).)).).).)).).)).).).).)).).)).).).)).)).).)).).

The proof works differently than the GCN and MeanGNN\({}^{+}\) cases, but still rests on a probabilistic analysis of the preactivations in each layer. Assuming that \(\) is synchronously saturating for \((n,r)\) and \((d)\), we can show that the expected absolute value of each preactivation tends to infinity as the number of nodes increases, and that moreover the probability that it lies below any fixed value tends to \(0\) exponentially. Hence, the probability that all node embeddings after the first layer are the same and have components which are all \(_{-}\) or \(_{}\) tends to \(1\). We then extend this analysis to further layers, using the fact that \(\) is synchronously saturating, which yields inductively that all node embeddings are the same with probability tending to \(1\), resulting in the following key lemma.

**Lemma 4.11**.: _Let \(\), \((d)\) and \(r\) be as in Theorem 4.10. Let \(_{-}\) and \(_{}\) be the extremal values taken by the non-linearity. Then, for every layer \(t\), there is \(_{t}\{_{-},_{}\}^{d(t)}\) such that when we sample graphs with node features from \((n,r)\) and \((d)\) the probability that \(}_{v}^{(t)}=_{t}\) for every node \(u\) tends to \(1\) as \(n\) tends to infinity._

The final classification output must therefore be the same asymptotically, since its input consists of node embeddings which always take the same value.

## 5 Graph neural networks with random node features

Up to this point we have been considering the graph plus node features as the (random) input to GNNs. In this section, we make a change in perspective and regard the initial node features as part of the model, so that its input consists solely of the graph without features. We focus in this section on SumGNN\({}^{+}\). Adding random initial features to GNNs is known to increase their power .

Note that Theorem 4.10 immediately yields a zero-one law for these models. This places restrictions on what can be expressed by SumGNN\({}^{+}\) models with random features subject to the conditions of Theorem 4.10. For example, it is not possible to express that the number of graph nodes is even, since the property of being even does not satisfy a zero-one law with respect to any \(r\).

It is natural to wonder how tight these restrictions are: what precisely is the class of functions which can be approximated by these models? Let us first formalize the notion of approximation.

**Definition 5.1**.: Let \(f\) be a Boolean function on graphs, and let \(\) be a random function on graphs. Take \(>0\). Then \(\)_uniformly \(\)-approximates \(f\)_ if:

\[ n\ ((G)=f(G)|G|=n) 1-\]

when we sample \(G(n,1/2)\).

The reason for sampling graphs from \((n,1/2)\) is that under this distribution all graphs on \(n\) nodes are equally likely. Therefore, the requirement is the same as that for every \(n\) the proportion of \(n\)-node graphs on which \((G)=f(G)\) is at least \(1-\).

Building on results due to Abboud et al. , we show a partial converse to Theorem 4.10: if a graph invariant satisfies a zero-one law for \((n,1/2)\) then it can be universally approximated by a SumGNN\({}^{+}\) with random node features.

**Theorem 5.2**.: _Let \(\) be any graph invariant which satisfies a zero-one law with respect to \((n,1/2)\). Then, for every \(>0\) there is a SumGNN\({}^{+}\) with random node features \(\) which uniformly \(\)-approximates \(\)._

The basis of the proof is a result due to Abboud et al.  which states that a SumGNN\({}^{+}\) with random node features can approximate any graph invariant _on graphs of bounded size_. When the graph invariant satisfies a zero-one law, we can use the global readout to count the number of nodes. Below a certain threshold, we use the techniques of Abboud et al.  to approximate the invariant, and above the threshold we follow its asymptotic behavior. We emphasise that the combination of these techniques yields a model which provides an approximation which is uniform across all graph sizes.

## 6 Experimental evaluation

We empirically verify our theoretical findings on a carefully designed synthetic experiment using ER graphs with random features. The goal of these experiments is to answer the following questions for each model under consideration:

**Q1.** Do we empirically observe a zero-one law?

**Q2.** What is the rate of convergence like empirically?

**Q3.** What is the impact of the number of layers on the convergence?

### Experimental setup

We report experiments for GCN, MeanGNN, and SumGNN. The following setup is carefully designed to eliminate confounding factors:

* We consider \(10\) GNN models of the same architecture each with _randomly initialized weights_, where each weight is sampled independently from \(U(-1,1)\). The non-linearity is eventually constant in both directions: identity between \([-1,1]\), and truncated to \(-1\) if the input is smaller than \(-1\), and \(1\) if the input is greater than \(1\). In the appendix we include additional experiments in which test other choices of non-linearity (see Appendix E.2). We apply mean pooling to yield a final representation \(_{G}^{d}\) of the input graph.
* For every model, we apply a final classifier \((f):^{d}\) where \(f\) is a 2-layer MLP with _random weights_ and with \(\) activation, which outputs a real value, and \(\) is the sigmoid function. Graphs are classified as \(1\) if the output of the sigmoid is greater than \(0.5\), and \(0\) otherwise.
* The input graphs are drawn from \((n,1/2)\) with corresponding node features independently drawn from \(U(0,1)\).
* We conduct these experiments with three choices of layers: \(10\) models with \(T=1\) layer, \(10\) models with \(T=2\) layers, and \(10\) models with \(T=3\) layers.

The goal of these experiments is to understand the behavior of the respective GNN graph classifiers with mean-pooling, as we draw larger and larger ER graphs. Specifically, each model classifies graphs of varying sizes, and we are interested in knowing _how the proportion of the graphs which are classified as \(1\) evolves, as we increase the graph sizes_.

We independently sample 10 models to ensure this is not a model-specific behavior, aiming to observe the same phenomenon across the models. If there is a zero-one law, then for each model, we should only see two types of curves: either tending to \(0\) or tending to \(1\), as graph sizes increase. Whether it will tend to \(0\) or \(1\) depends on the final classifier: since each of these are independent MLPs with random weights the specific outcome is essentially random.

We consider models with up to \(3\) layers to ensure that the node features do not become alike because of the orthogonal over-smoothing issue , which surfaces with increasing number of layers. A key feature of our theoretical results is that they do not depend on the number of layers, and this is an aspect which we wish to validate empirically. Using models with random weights is a neutral setup, and random GNNs are widely used in the literature as baseline models , as they define valid graph convolutions and tend to perform reasonably well.

### Empirical results

We report all results in Figure 1 for all models considered and discuss them below. Each plot in this figure depicts the curves corresponding to the behavior of independent models with random weights.

**GCN.** For this experiment, we use an embedding dimensionality of \(128\) for each GCN model and draw graphs of sizes up \(5000\), where we take \(32\) samples of each size. The key insight of Theorem 4.6 is that the final mean-pooled embedding vector \(_{G}\) tends to a constant vector as we draw larger graphs. Applying an MLP followed by a sigmoid function will therefore map \(_{G}\) to either \(0\) or \(1\), showing a zero-one law. It is evident from Figure 1 (top row) that all curves tend to either \(0\) or \(1\), confirming our expectation regarding the outcome of these experiments for GCNs. Moreover, this holds regardless of the number of layers considered. Since the convergence occurs quickly, already around graphs of size of \(1000\), we did not experiment with larger graph sizes in this experiment.

**MeanGNN.** Given that the key insight behind this result is essentially similar to that of Theorem 4.6, we follow the exact same configuration for these models as for GCNs. The proof structure is the same in both cases: we show that the preactivations and activations become closer and closer tosome fixed values as the number of nodes increases. Moreover, comparing the summations in the definitions of GCN and MeanGNN, on a typical ER graph drawn from \((n,1/2)\) we would expect each corresponding summand to have a similar value, since \((u)||(v)|}\) should be close to \(|^{+}(v)|\). Figure 1(mid row) illustrates the results for MeanGNN and the trends are reassuringly similar to those of GCNs: all models converge quickly to either \(0\) and \(1\) with all choices of layers. Interestingly, the plots for GCN and MeanGNN models are almost identical. We used the same seed when drawing each of the model weights, and the number of parameters is the same between the two. Hence, the GCN models were parameterized with the same values as the MeanGNN models. The fact that each pair of models preforms near identically confirms the expectation that the two architectures work in similar ways on ER graphs.

**SumGNN.** Theorem 4.10 shows that, as the number of nodes grow, the embedding vector \(_{v}\) of each _node_\(v\) will converge to a constant vector with high probability. Hence, when we do mean-pooling at the end, we expect to get the same vector for different graphs of the same size. The mechanism by which a zero-one law is arrived at is quite different compared with the GCN and MeanGNN case. In particular, in order for the embedding vectors to begin to converge, there must be sufficiently many nodes so that the preactivations surpass the thresholds of the non-linearity. For this experiment, we use a smaller embedding dimensionality of \(64\) for each SumGNN model and draw graphs of sizes up to \(100000\), where we take \(32\) samples of each size. Figure 1 shows the results for SumGNN. Note that we observe a slower convergence than with GCN or MeanGNN.

Figure 1: Each plot shows the proportion of graphs of certain size which are classified as \(1\) by a set of ten GCNs (top row), MeanGNNs (middle), and SumGNNs (bottom row). Each curve (color-coded) shows the behavior of a model, as we draw increasingly larger graphs. The phenomenon is observed for 1-layer models (left column), 2-layer models (mid column), and 3-layer models (last column). GCNs and MeanGNNs behave very similarly with all models converging quickly to \(0\) or to \(1\). SumGNNs shows slightly slower convergence, but all models perfectly converge in all layers.

Limitations, discussion, and outlook

The principal limitations of our work come from the assumptions placed on the main theorems. In our formal analysis, we focus on graphs drawn from the ER distribution. From the perspective of characterizing the _expressiveness_ of GNNs this is unproblematic, and accords with the classical analysis of first-order properties of graphs. However, when considering the _extrapolation capacity_ of GNNs, other choices of distributions may be more realistic. In Appendices E.4 and E.5 we report experiments in which zero-one laws are observed empirically for sparse ER graphs and Barabasi-Albert graphs , suggesting that formal results may be obtainable. While we empirically observe that GNNs converge to their asymptotic behaviour very quickly, we leave it as future work to rigorously examine the rate at which this convergence occurs.

In this work we show that GNNs with random features can at most capture properties which follow a zero-one law. We complement this with an _almost_ matching lower bound: Theorem 5.2 currently requires a graph invariant \(\) which obeys a zero-one law with respect to a specific value of \(r\) (i.e., \(1/2\)), and if this assumption could be relaxed, it would yield a complete characterization of the expressive power of these models.

## 8 Acknowledgments

We would like to thank the anonymous reviewers for their feedback, which lead to several improvements in the presentation of the paper. The first author was supported by an EPSRC studentship with project reference _2271793_.