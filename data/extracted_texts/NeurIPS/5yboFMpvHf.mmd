# Federated Model Heterogeneous

Matryoshka Representation Learning

 Liping Yi\({}^{1,2}\), Han Yu\({}^{2}\), Chao Ren\({}^{2}\), Gang Wang\({}^{1,*}\), Xiaoguang Liu\({}^{1,*}\), Xiaoxiao Li\({}^{3,4}\)

\({}^{1}\)College of Computer Science, TMCC, SysNet, DISSec, GTIISC, Nankai University, China

\({}^{2}\)College of Computing and Data Science, Nanyang Technological University, Singapore

\({}^{3}\)Department of Electrical and Computer Engineering, The University of British Columbia, Canada

\({}^{4}\)Vector Institute, Canada

{yiliping, wgzwp, liuxg}@nbjl.nankai.edu.cn

{han.yu, chao.ren}@ntu.edu.sg, xiaoxiao.li@ece.ubc.ca

Corresponding authors

###### Abstract

Model heterogeneous federated learning (MHeteroFL) enables FL clients to collaboratively train models with heterogeneous structures in a distributed fashion. However, existing MHeteroFL methods rely on training loss to transfer knowledge between the client model and the server model, resulting in limited knowledge exchange. To address this limitation, we propose the Federated model heterogeneous Matryoshka Representation Learning (FedMRL) approach for supervised learning tasks. It adds an auxiliary small homogeneous model shared by clients with heterogeneous local models. (1) The generalized and personalized representations extracted by the two models' feature extractors are fused by a personalized lightweight representation projector. This step enables representation fusion to adapt to local data distribution. (2) The fused representation is then used to construct Matryoshka representations with multi-dimensional and multi-granular embedded representations learned by the global homogeneous model header and the local heterogeneous model header. This step facilitates multi-perspective representation learning and improves model learning capability. Theoretical analysis shows that FedMRL achieves a \((1/T)\) non-convex convergence rate. Extensive experiments on benchmark datasets demonstrate its superior model accuracy with low communication and computational costs compared to seven state-of-the-art baselines. It achieves up to \(8.48\%\) and \(24.94\%\) accuracy improvement compared with the state-of-the-art and the best same-category baseline, respectively.

## 1 Introduction

Traditional federated learning (FL) [32; 47; 46; 12] often relies on a central FL server to coordinate multiple data owners (a.k.a., FL clients) to train a global shared model without exposing local data. In each communication round, the server broadcasts the global model to the clients. A client trains it on its local data and sends the updated local model to the FL server. The server aggregates local models to produce a new global model. These steps are repeated until the global model converges. During the runtime of FL, only model parameters are transmitted between the server and clients, preserving data privacy[14; 56; 51].

However, the above design cannot handle the following heterogeneity challenges  commonly found in practical FL applications: (1) Data heterogeneity : FL clients' local data often follow non-independent and identically distributions (non-IID). A single global model produced by aggregatinglocal models trained on non-IID data might not perform well on all clients [49; 48]. (2) System heterogeneity : FL clients can have diverse system configurations in terms of computing power and network bandwidth. Training the same model structure among such clients means that the global model size must accommodate the weakest device, leading to sub-optimal performance on other more powerful clients [52; 54; 50]. (3) Model heterogeneity : When FL clients are enterprises, they might have heterogeneous proprietary models which cannot be directly shared with others during FL training due to intellectual property (IP) protection concerns.

To address these challenges, the field of model heterogeneous federated learning (MHeteroFL)  has emerged. It enables FL clients to train local models with tailored structures suitable for local system resources and local data distributions. Existing MHeteroFL methods [41; 45] are limited in terms of knowledge transfer capabilities as they commonly leverage the training loss between server and client models for this purpose. This design leads to model performance bottlenecks, incurs high communication and computation costs, and risks exposing private local model structures and data.

Recently, Matryoshka Representation Learning (MRL)  has emerged to tailor representation dimensions based on the computational and storage costs required by downstream tasks to achieve a near-optimal trade-off between model performance and inference costs. As shown in Figure 1(left), the representation extracted by the feature extractor is constructed to form Matryoshka Representations involving a series of embedded representations ranging from low-to-high dimensions and coarse-to-fine granularities. Each of them is processed by a single output layer for calculating loss, and the sum of losses from all branches is used to update model parameters. This design is inspired by the insight that people often first perceive the coarse aspect of a target before observing the details, with multi-perspective observations enhancing understanding.

Inspired by MRL, we address the aforementioned limitations of MHeteroFL by proposing the Federated model heterogeneous Matryoshka Representation Learning (FedMRL) approach for supervised learning tasks. For each client, a shared global auxiliary homogeneous small model is added to interact with its heterogeneous local model. Both two models consist of a feature extractor and a prediction header, as depicted in Figure 1(right). FedMRL has two key design innovations. **(1) Adaptive Representation Fusion**: for each local data sample, the feature extractors of the two local models extract generalized and personalized representations, respectively. The two representations are spliced and then mapped to a fused representation by a lightweight personalized representation projector adapting to local non-IID data. **(2) Multi-Granularity Representation Learning**: the fused representation is used to construct Matryoshka Representations involving multi-dimension and multi-granularity embedded representations, which are processed by the prediction headers of the two models, respectively. The sum of their losses is used to update all models, which enhances the model learning capability owing to multi-perspective representation learning.

The personalized multi-granularity MRL enhances representation knowledge interaction between the homogeneous global model and the heterogeneous client local model. Each client's local model and data are not exposed during training for privacy-preservation. The server and clients only transmit the small homogeneous models, thereby incurring low communication costs. Each client only trains a small homogeneous model and a lightweight representation projector in addition, incurring low extra computational costs. We theoretically derive the \((1/T)\) non-convex convergence rate of FedMRL and verify that it can converge over time. Experiments on benchmark datasets comparing FedMRL against seven state-of-the-art baselines demonstrate its superiority. It improves model accuracy by up to \(8.48\%\) and \(24.94\%\) over the best baseline and the best same-category baseline, while incurring lower communication and computation costs.

Figure 1: Left: Matryoshka Representation Learning. Right: Feature extractor and prediction header.

Related Work

Existing MHeteroFL works can be divided into the following four categories.

**MHeteroFL with Adaptive Subnets.** These methods [3; 4; 5; 11; 16; 57; 65] construct heterogeneous local subnets of the global model by parameter pruning or special designs to match with each client's local system resources. The server aggregates heterogeneous local subnets wise parameters to generate a new global model. In cases where clients hold black-box local models with heterogeneous structures not derived from a common global model, the server is unable to aggregate them.

**MHeteroFL with Knowledge Distillation.** These methods [6; 8; 9; 17; 18; 19; 25; 26; 28; 30; 33; 35; 38; 39; 44; 58; 60] often perform knowledge distillation on heterogeneous client models by leveraging a public dataset with the same data distribution as the learning task. In practice, such a suitable public dataset can be hard to find. Others [13; 61; 62; 64] train a generator to synthesize a shared dataset to deal with this issue. However, this incurs high training costs. The rest (FD, FedProto and others [1; 2; 15; 53; 59]) share the intermediate information of client local data for knowledge fusion.

**MHeteroFL with Model Split.** These methods split models into feature extractors and predictors. Some [7; 10; 34; 36] share homogeneous feature extractors across clients and personalize predictors, while others (LG-FedAvg and [20; 29]) do the opposite. Such methods expose part of the local model structures, which might not be acceptable if the models are proprietary IPs of the clients.

**MHeteroFL with Mutual Learning.** These methods (FedAPEN, FML, FedKD and others [31; 22]) add a shared global homogeneous small model on top of each client's heterogeneous local model. For each local data sample, the distance of the outputs from these two models is used as the mutual loss to update model parameters. Nevertheless, the mutual loss only transfers limited knowledge between the two models, resulting in model performance bottlenecks.

The proposed FedMRL approach further optimizes mutual learning-based MHeteroFL by enhancing the knowledge transfer between the server and client models. It achieves personalized adaptive representation fusion and multi-perspective representation learning, thereby facilitating more knowledge interaction across the two models and improving model performance.

## 3 The Proposed FedMRL Approach

FedMRL aims to tackle data, system, and model heterogeneity in supervised learning tasks, where a central FL server coordinates \(N\) FL clients to train heterogeneous local models. The server maintains a global homogeneous small model \(()\) shared by all clients. Figure 2 depicts its workflow 2:

* In each communication round, \(K\) clients participate in FL (_i.e._, the client participant rate \(C=K/N\)). The global homogeneous small model \(()\) is broadcast to them.
* Each client \(k\) holds a heterogeneous local model \(_{k}(_{k})\) (\(_{k}()\) is the heterogeneous model structure, and \(_{k}\) are personalized model parameters). Client \(k\) simultaneously trains the heterogeneous local model and the global homogeneous small model on local non-IID data \(D_{k}\) (\(D_{k}\) follows the non-IID distribution \(P_{k}\)) via personalized Matryoshka Representations Learning with a personalized representation projector \(_{k}(_{k})\) in an end-to-end manner.
* The updated homogeneous small models are uploaded to the server for aggregation to produce a new global model for knowledge fusion across heterogeneous clients.

The objective of FedMRL is to minimize the sum of the loss from the combined models (\(_{k}(w_{k})=(()_{k}(_{k})| _{k}(_{k}))\)) on all clients, _i.e._,

\[_{,_{0},,N-1}_{k=0}^{N-1}(_{k} (D_{k};(_{k}_{k}))).\] (1)

These steps repeat until each client's model converges. After FL training, a client uses its local combined model without the global header for inference. 3

### Adaptive Representation Fusion

We denote client \(k\)'s heterogeneous local model feature extractor as \(_{k}^{ex}(_{k}^{ex})\), and prediction header as \(_{k}^{hd}(_{k}^{hd})\). We denote the homogeneous global model feature extractor as \(^{ex}(^{ex})\) and prediction header as \(^{hd}(^{hd})\). Client \(k\)'s local personalized representation projector is denoted as \(_{k}(_{k})\). In the \(t\)-th communication round, client \(k\) inputs its local data sample \((_{i},y_{i}) D_{k}\) into the two feature extractors to extract generalized and personalized representations as:

\[}_{i}^{}=\ ^{ex}(_{i};^{ex,t-1}), }_{i}^{_{k}}=\ _{k}^{ex}(_{i};_{k}^{ex,t-1}).\] (2)

The two extracted representations \(}_{i}^{}^{d_{1}}\) and \(}_{i}^{_{k}}^{d_{2}}\) are spliced as:

\[}_{i}=}_{i}^{}}_{ i}^{_{k}}.\] (3)

Then, the spliced representation is mapped into a fused representation by the lightweight representation projector \(_{k}(_{k}^{t-1})\) as:

\[}}_{i}=_{k}(}_{i}; _{k}^{t-1}),\] (4)

where the projector can be a one-layer linear model or multi-layer perceptron. The fused representation \(}}_{i}\) contains both generalized and personalized feature information. It has the same dimension as the client's local heterogeneous model representation \(^{d_{2}}\), which ensures the representation dimension \(^{d_{2}}\) and the client local heterogeneous model header parameter dimension \(^{d_{2} L}\) (\(L\) is the label dimension) match.

The representation projector can be updated as the two models are being trained on local non-IID data. Hence, it achieves personalized representation fusion adaptive to local data distributions. Splicing the representations extracted by two feature extractors can keep the relative semantic space positions of the generalized and personalized representations, benefiting the construction of multi-granularity Matryoshka Representations. Owing to representation splicing, the representation dimensions of the two feature extractors can be different (_i.e._, \(d_{1} d_{2}\)). Therefore, we can vary the representation dimension of the small homogeneous global model to improve the trade-off among model performance, storage requirement and communication costs.

In addition, each client's local model is treated as a black box by the FL server. When the server broadcasts the global homogeneous small model to the clients, each client can adjust the linear layer dimension of the representation projector to align it with the dimension of the spliced representation. In this way, different clients may hold different representation projectors. When a new model-agnostic client joins in FedMRL, it can adjust its representation projector structure for local model training. Therefore, FedMRL can accommodate FL clients owning local models with diverse structures.

Figure 2: The workflow of FedMRL.

### Multi-Granular Representation Learning

To construct multi-dimensional and multi-granular Matryoshka Representations, we further extract a low-dimension coarse-granularity representation \(}}_{i}^{lc}\) and a high-dimension fine-granularity representation \(}}_{i}^{hf}\) from the fused representation \(}}_{i}\). They align with the representation dimensions \(\{^{d_{1}},^{d_{2}}\}\) of two feature extractors for matching the parameter dimensions \(\{^{d_{1} L},^{d_{2} L}\}\) of the two prediction headers,

\[}}_{i}^{lc}=}}_{i}^{1:d_{ 1}},}}_{i}^{hf}=}}_{i}^{1: d_{2}}.\] (5)

The embedded low-dimension coarse-granularity representation \(}}_{i}^{lc}^{d_{1}}\) incorporates coarse generalized and personalized feature information. It is learned by the global homogeneous model header \(^{hd}(^{hd,-1})\) (parameter space: \(^{d_{1} L}\)) with generalized prediction information to produce:

\[_{i}^{}=^{hd}(}}_{i}^ {lc};^{hd,t-1}).\] (6)

The embedded high-dimension fine-granularity representation \(}}_{i}^{hf}^{d_{2}}\) carries finer generalized and personalized feature information, which is further processed by the heterogeneous local model header \(_{k}^{hd}(_{k}^{hd,t-1})\) (parameter space: \(^{d_{2} L}\)) with personalized prediction information to generate:

\[_{i}^{_{k}}=_{k}^{hd}(} }_{i}^{hf};_{k}^{hd,t-1}).\] (7)

We compute the losses \(\) (_e.g._, cross-entropy loss ) between the two outputs and the label \(y_{i}\) as:

\[_{i}^{}=(_{i}^{},y_{i}),\;_{i}^{ _{k}}=(_{i}^{_{k}},y_{i}).\] (8)

Then, the losses of the two branches are weighted by their importance \(m_{i}^{}\) and \(m_{i}^{_{k}}\) and summed as:

\[_{i}=m_{i}^{}_{i}^{}+m_{i}^{_{k }}_{i}^{_{k}}.\] (9)

We set \(m_{i}^{}=m_{i}^{_{k}}=1\) by default to make the two models contribute equally to model performance. The complete loss \(_{i}\) is used to simultaneously update the homogeneous global small model, the heterogeneous client local model, and the representation projector via gradient descent:

\[&_{k}^{t}^{t-1}-_{} _{i},\\ &_{k}^{t}_{k}^{t-1}-_{}_ {i},\\ &_{k}^{t}_{k}^{t-1}-_{} _{i},\] (10)

where \(_{},_{},\;_{}\) are the learning rates of the homogeneous global small model, the heterogeneous local model and the representation projector. We set \(_{}=_{}=\;_{}\) by default to ensure stable model convergence. In this way, the generalized and personalized fused representation is learned from multiple perspectives, thereby improving model learning capability.

## 4 Convergence Analysis

Based on notations, assumptions and proofs in Appendix B, we analyse the convergence of FedMRL.

**Lemma 1**: _Local Training. Given Assumptions 1 and 2, the loss of an arbitrary client's local model \(w\) in local training round \((t+1)\) is bounded by:_

\[[_{(t+1)E}]_{tE+0}+(^{2}}{2 }-)_{e=0}^{E}\|_{tE+e}\|_{2}^{2}+E^{2} ^{2}}{2}.\] (11)

**Lemma 2**: _Model Aggregation. Given Assumptions 2 and 3, after local training round \((t+1)\), a client's loss before and after receiving the updated global homogeneous small models is bounded by:_

\[[_{(t+1)E+0}][_{(t+1)E}]+ ^{2}.\] (12)

**Theorem 1**: _One Complete Round of FL. Given the above lemmas, for any client, after receiving the updated global homogeneous small model, we have:_

\[[_{(t+1)E+0}]_{tE+0}+(^{2}}{2 }-)_{e=0}^{E}\|_{tE+e}\|_{2}^{2}+E^{2} ^{2}}{2}+^{2}.\] (13)

**Theorem 2**: _Non-convex Convergence Rate of FedMRL. Given Theorem 1, for any client and an arbitrary constant \(>0\), the following holds:_

\[_{t=0}^{T-1}_{e=0}^{E-1}\| _{tE+e}\|_{2}^{2}_{t=0}^{T-1}[ _{tE+0}-[_{(t+1)E+0}]]+E^{2}^{2}}{2 }+^{2}}{-^{2}}{2}}<,\\ s.t.\ <)}{L_{1}(+E^{2})}. \] (14)

Therefore, we conclude that any client's local model can converge at a non-convex rate of \((1/T)\) in FedMRL if the learning rates of the homogeneous small model, the client local heterogeneous model and the personalized representation projector satisfy the above conditions.

## 5 Experimental Evaluation

We implement FedMRL on Pytorch, and compare it with seven state-of-the-art MHeteroFL methods. The experiments are carried out over two benchmark supervised image classification datasets on \(4\) NVIDIA GeForce 3090 GPUs (24GB Memory).4

### Experiment Setup

**Datasets.** The benchmark datasets adopted are CIFAR-10 and CIFAR-100 5, which are commonly used in FL image classification tasks for the evaluating existing MHeteroFL algorithms. CIFAR-10 has \(60,000\)\(32 32\) colour images across \(10\) classes, with \(50,000\) for training and \(10,000\) for testing. CIFAR-100 has \(60,000\)\(32 32\) colour images across \(100\) classes, with \(50,000\) for training and \(10,000\) for testing. We follow  and  to construct two types of non-IID datasets. Each client's non-IID data are further divided into a training set and a testing set with a ratio of \(8:2\).

* **Non-IID (Class):** For CIFAR-10 with \(10\) classes, we randomly assign \(2\) classes to each FL client. For CIFAR-100 with \(100\) classes, we randomly assign \(10\) classes to each FL client. The fewer classes each client possesses, the higher the non-IIDness.
* **Non-IID (Dirichlet):** To produce more sophisticated non-IID data settings, for each class of CIFAR-10/CIFAR-100, we use a Dirichlet(\(\)) function to adjust the ratio between the number of FL clients and the assigned data. A smaller \(\) indicates more pronounced non-IIDness.

**Models.** We evaluate MHeteroFL algorithms under model-homogeneous and heterogeneous FL scenarios. FedMRL's representation projector is a one-layer linear model (parameter space: \(^{d2(d_{1}+d_{2})}\)).

* **Model-Homogeneous FL:** All clients train CNN-1 in Table 2 (Appendix C.1). The homogeneous global small models in FML and FedKD are also CNN-1. The extra homogeneous global small model in FedMRL is CNN-1 with a smaller representation dimension \(d_{1}\) (_i.e._, the penultimate linear layer dimension) than the CNN-1 model's representation dimension \(d_{2}\), \(d_{1} d_{2}\).
* **Model-Heterogeneous FL:** The \(5\) heterogeneous models {CNN-1, \(\), CNN-5} in Table 2 (Appendix C.1) are evenly distributed among FL clients. The homogeneous global small models in FML and FedKD are the smallest CNN-5 models. The homogeneous global small model in FedMRL is the smallest CNN-5 with a reduced representation dimension \(d_{1}\) compared with the CNN-5 model representation dimension \(d_{2}\), _i.e._, \(d_{1} d_{2}\).

[MISSING_PAGE_FAIL:7]

in model performance owing to its adaptive personalized representation fusion and multi-granularity representation learning capabilities. Figure 3(left six) shows that FedMRL consistently achieves faster convergence speed and higher average test accuracy than the best baseline under each setting.

#### 5.2.2 Individual Client Test Accuracy

Figure 3(right two) shows the difference between the test accuracy achieved by FedMRL vs. the best-performing baseline FedProto (_i.e._, FedMRL - FedProto) under \((N=100,C=10\%)\) for each individual client. It can be observed that \(87\%\) and \(99\%\) of all clients achieve better performance under FedMRL than under FedProto on CIFAR-10 and CIFAR-100, respectively. This demonstrates that FedMRL possesses stronger personalization capability than FedProto owing to its adaptive personalized multi-granularity representation learning design.

#### 5.2.3 Communication Cost

We record the communication rounds and the number of parameters sent per client to achieve \(90\%\) and \(50\%\) target test average accuracy on CIFAR-10 and CIFAR-100, respectively. Figure 4 (left) shows that FedMRL requires fewer rounds and achieves faster convergence than FedProto. Figure 4 (middle) shows that FedMRL incurs higher communication costs than FedProto as it transmits the full homogeneous small model, while FedProto only transmits each local seen-class average representation between the server and the client. Nevertheless, FedMRL with an optional smaller representation dimension (\(d_{1}\)) of the homogeneous small model still achieves higher communication efficiency than same-category mutual learning-based MHeteroFL baselines (FML, FedKD, FedAPEN) with a larger representation dimension.

Figure 4: Communication rounds, number of communicated parameters, and computation FLOPs required to reach \(90\%\) and \(50\%\) average test accuracy targets on CIFAR-10 and CIFAR-100.

Figure 3: Left six: average test accuracy vs. communication rounds. Right two: individual clientsâ€™ test accuracy (%) differences (FedMRL - FedProto).

#### 5.2.4 Computation Overhead

We also calculate the computation FLOPs consumed per client to reach \(90\%\) and \(50\%\) target average test accuracy on CIFAR-10 and CIFAR-100, respectively. Figure 4(right) shows that FedMRL incurs lower computation costs than FedProto, owing to its faster convergence (_i.e._, fewer rounds) even with higher computation overhead per round due to the need to train an additional homogeneous small model and a linear representation projector.

### Case Studies

#### 5.3.1 Robustness to Non-IIDness (Class)

We evaluate the robustness of FedMRL to different non-IIDnesses as a result of the number of classes assigned to each client under the (\(N=100\), \(C=10\%\)) setting. The fewer classes assigned to each client, the higher the non-IIDness. For CIFAR-10, we assign \(\{2,4,,10\}\) classes out of total \(10\) classes to each client. For CIFAR-100, we assign \(\{10,30,,100\}\) classes out of total \(100\) classes to each client. Figure 5(left two) shows that FedMRL consistently achieves higher average test accuracy than the best-performing baseline - FedProto on both datasets, demonstrating its robustness to non-IIDness by class.

#### 5.3.2 Robustness to Non-IIDness (Dirichlet)

We also test the robustness of FedMRL to various non-IIDnesses controlled by \(\) in the Dirichlet function under the (\(N=100,C=10\%\)) setting. A smaller \(\) indicates a higher non-IIDness. For both datasets, we vary \(\) in the range of \(\{0.1,,0.5\}\). Figure 5(right two) shows that FedMRL significantly outperforms FedProto under all non-IIDness settings, validating its robustness to Dirichlet non-IIDness.

#### 5.3.3 Sensitivity Analysis - \(d_{1}\)

FedMRL relies on a hyperparameter \(d_{1}\) - the representation dimension of the homogeneous small model. To evaluate its sensitivity to \(d_{1}\), we test FedMRL with \(d_{1}=\{100,150,,500\}\) under the (\(N=100,C=10\%\)) setting. Figure 6(left two) shows that smaller \(d_{1}\) values result in higher average test accuracy on both datasets. It is clear that a smaller \(d_{1}\) also reduces communication and computation overheads, thereby helping FedMRL achieve the best trade-off among model performance, communication efficiency, and computational efficiency.

Figure 5: Robustness to non-IIDness (Class & Dirichlet).

Figure 6: Left two: sensitivity analysis results. Right two: ablation study results.

### Ablation Study

We conduct ablation experiments to validate the usefulness of MRL. For FedMRL with MRL, the global header and the local header learn multi-granularity representations. For FedMRL without MRL, we directly input the representation fused by the representation projector into the client's local header for loss computation (_i.e._, we do not extract Matryoshka Representations and remove the global header). Figure 6(right two) shows that FedMRL with MRL consistently outperforms FedMRL without MRL, demonstrating the effectiveness of the design to incorporate MRL into MHeteroFL. Besides, the accuracy gap between them decreases as \(d_{1}\) rises. This shows that as the global and local headers learn increasingly overlapping representation information, the benefits of MRL are reduced.

## 6 Discussion

We discuss how FedMRL tackles heterogeneity and its privacy, communication and computation.

**Tackling Heterogeneity.** FedMRL allows each client to tailor its heterogeneous local model according to its system resources, which addresses system and model heterogeneity. Each client achieves multi-granularity representation learning adapting to local non-IID data distribution through a personalized heterogeneous representation projector, alleviating data heterogeneity.

**Privacy.** The server and clients only communicate the homogeneous small models. Since we do not limit the representation dimensions \(d_{1},d_{2}\) of the proxy homogeneous global model and the heterogeneous client model are the same, sharing the proxy homogeneous model does not disclose the representation dimension and structure of the heterogeneous client model. Meanwhile, local data are always stored by clients for local training, so local data privacy is also protected.

**Communication Cost.** The server and clients transmit homogeneous small models with fewer parameters than the client's heterogeneous local model, consuming significantly lower communication costs in one communication round compared with transmitting complete local models like FedAvg.

**Computational Overhead.** Besides training the heterogeneous local model, each client also trains the homogeneous global small model and a lightweight representation projector with far fewer parameters than the heterogeneous local model. The computational overhead in one round is slightly increased. Since we design personalized Matryoshka Representations learning adapting to local data distribution from multiple perspectives, the model learning capability is improved, accelerating model convergence and consuming fewer rounds. Therefore, the total computational cost is reduced.

## 7 Conclusion

This paper proposes a novel MHeteroFL approach - FedMRL - to jointly address data, system and model heterogeneity challenges in FL. The key design insight is the addition of a global homogeneous small model shared by FL clients for enhanced knowledge interaction among heterogeneous local models. Adaptive personalized representation fusion and multi-granularity Matryoshka Representations learning further boosts model learning capability. The client and the server only need to exchange the homogeneous small model, while the clients' heterogeneous local models and data remain unexposed, thereby enhancing the preservation of both model and data privacy. Theoretical analysis shows that FedMRL is guaranteed to converge over time. Extensive experiments demonstrate that FedMRL significantly outperforms state-of-the-art models regarding test accuracy, while incurring low communication and computation costs. 6