# On the Role of Attention Masks

and LayerNorm in Transformers

Xinyi Wu\({}^{1}\)  Amir Ajorlou\({}^{1}\)  Yifei Wang\({}^{2}\)  Stefanie Jegelka\({}^{3,2}\)  Ali Jadbabaie\({}^{1}\)

\({}^{1}\)MIT LIDS \({}^{2}\)MIT CSAIL \({}^{3}\)TU Munich

{xinyiwu,ajorlou,yifei_w,stefje,jadbabai}@mit.edu

###### Abstract

Self-attention is the key mechanism of transformers, which are the essential building blocks of modern foundation models. Recent studies have shown that pure self-attention suffers from an increasing degree of rank collapse as depth increases, limiting model expressivity and further utilization of model depth. The existing literature on rank collapse, however, has mostly overlooked other critical components in transformers that may alleviate the rank collapse issue. In this paper, we provide a general analysis of rank collapse under self-attention, taking into account the effects of attention masks and layer normalization (LayerNorm). In particular, we find that although pure masked attention still suffers from exponential collapse to a rank one subspace, sparse or local masked attention can provably slow down the collapse rate. In the case of self-attention with LayerNorm, we first show that for certain classes of value matrices, collapse to a rank one subspace still happens exponentially. However, through construction of nontrivial counterexamples, we then establish that with proper choice of value matrices, a general class of sequences may not converge to a rank one subspace, and the self-attention dynamics with LayerNorm can simultaneously possess a rich set of equilibria with any possible rank between one and full. Our result refutes the previous hypothesis that LayerNorm plays no role in the rank collapse of self-attention and suggests that self-attention with LayerNorm constitutes a much more expressive, versatile nonlinear dynamical system than what was originally thought.

## 1 Introduction

The celebrated attention mechanism has proven highly effective in the architecture of transformers , which serve as the key building block of modern foundation models including large language models. From a theoretical perspective, understanding the underlying mechanism of transformers and attention in general has become pivotal for elucidating existing models and paving the way for developing more powerful future models .

Transformers are known to exhibit the _rank collapse phenomenon1_, which refers to the observation that increasing the number of self-attention layers leads to homogeneous token representations. To gain more insight into the rank collapse issue and better understand the effects of self-attention in multi-layer models, it is essential to study the long-term behavior of tokens under self-attention dynamics .

However, many existing studies do not take into account architectural components that are commonly used in practice. For instance, first, the theoretical analysis of long-term self-attention dynamics often assumes that attention is _fully bidirectional_ -- that is, all tokens are allowed to attend to all other tokens in the sequence , which only applies to csuch as the one deployed in the BERT family [11; 24]. The vast majority of popular transformer architectures used nowadays in language models, including the GPT models [6; 30], use the causal attention masks where tokens are only allowed to attend to preceding tokens, or have sparse attention structure (sparse attention) [4; 8; 10; 21; 22; 32; 38; 41; 42], where attention is restricted to local interactions between tokens and their chosen neighbors. This limits the practical applicability of the theoretical results developed in [12; 15; 16; 27], as they heavily rely on the key assumption that attention is fully bidirectional.

Second, layer normalization (LayerNorm) is another inherent component of transformers. Dong et al.  put forth a hypothesis that LayerNorm plays no role in preventing rank collapse in self-attention networks. This hypothesis is then partially validated by a continuous-time analysis of self-attention dynamics conducted in Geshkovski et al. . The analysis, which incorporates LayerNorm, shows the exponential convergence of tokens to a common point on the unit sphere. However, this result relies on a strong assumption that all the value matrices are the identity matrix. In contrast, for multilayer perceptrons (MLPs), Joudaki et al.  show that LayerNorm improves the isometry of the representations and can prevent rank collapse in that setting. It thus remains a question whether it is truly the case that LayerNorm could not have a similar effect in transformers under more general assumptions due to self-attention.

Hence, in this work, we rigorously study the effect of attention masks and LayerNorm on the token dynamics and rank collapse in transformers. The main questions we address are as follows:

_Can attention masks alleviate rank collapse of tokens under self-attention? If so, what type of attention mask would be the most effective? Can LayerNorm alleviate rank collapse of tokens under self-attention? If so, what long-term behavior of tokens would LayerNorm lead to?_

We answer these questions through a rigorous analysis of the self-attention dynamics. Notably, unlike some previous works [15; 16], which regard self-attention as a continuous-time dynamical system, we view self-attention as a discrete-time dynamical system, more closely resembling the architecture used in practice. Furthermore, our analysis extends the results to more general masked attention by making novel use of a graph-theoretic approach and incorporating advanced results on infinite products of inhomogeneous non-negative matrices that may be of independent interest.

**In summary, we make the following contributions:**

* We establish that with pure self-attention, the exponential convergence of tokens to a common representation holds for a broad class of attention masks, accounting for the causal mask and a wide class of sparse attention patterns such as the sliding window [4; 42]. The key property that leads to the exponential rank collapse is a token that serves as a common "context" for all the other tokens in the sequence to directly or indirectly attend to. Our results also show that _local_ attention can slow down the rate of rank collapse, suggesting its potential advantage over full bidirectional attention from an expressivity perspective at finite depth.
* We show that with LayerNorm, when the value matrices are orthogonal, the exponential convergence of tokens to a common point on the unit sphere holds for a broad class of attention masks. Nonetheless, by constructing nontrivial counterexamples, we prove that the self-attention dynamics with LayerNorm can simultaneously have a rich set of equilibria of any possible rank ranging from one to full. Moreover, we rigorously establish that self-attention with LayerNorm, together with proper choice of value matrices, can provably prevent complete collapse of tokens to a rank one subspace for a generic class of input sequences.

## 2 Related work

Analysis of self-attention dynamicsUnderstanding the attention mechanism is a pivotal step towards understanding the inner workings of transformer-based models. From a dynamical systems perspective, one could abstract the forward pass of the model as tokens undergoing a nonlinear, time-varying dynamics determined by the self-attention mechanism and other parameters of the model. Specifically, Dong et al.  first show that as the number of self-attention layers increases, tokens inevitably suffer from exponential rank collapse, while Wu et al.  establish a similar result for the attention mechanism in graph neural networks, taking additionally the layer-wise nonlinear activation functions into account. Other works [15; 16] take a continuous-time approach and study more fine-grained clustering behaviors of the dynamics in transformers.

The effect of LayerNorm in transformersLayerNorm is one of the most commonly used normalization techniques in modern neural networks  and has become an inherent component of transformers . To better understand its role in transformers, Xiong et al.  study it from an optimization perspective and show that LayerNorm stabilizes the gradients during the backward pass. In addition, Brody et al.  find that LayerNorm plays a crucial role in improving the expressivity of the attention layer by making it easier for the model to compute the most frequent token in the input and avoid the problem of "unselectable" keys. Most relevant to our work, Dong et al.  mention the effect of LayerNorm in terms of mitigating the rank collapse issue in transformers. The paper makes a hypothesis that LayerNorm has no effect for token rank collapse. The argument is based on a heuristic observation (see Appendix A for a detailed discussion) which is at odds with the case of simpler models such as MLPs where LayerNorm is shown to be pivotal in addressing a similar rank collapse problem . It thus remains an open question what effect LayerNorm would have on rank collapse in transformers.

Sparse and local AttentionWhile many existing transformer models and LLMs do not use sparse attention, sparse and local attention is gaining popularity due to the demand for efficiency, particular for long-context tasks. For example, sparse attention was populated by Longformer  and OpenAI  and nowadays popular LLMs like Mistral 7B  use sliding window attention by default. Other popular sparse attention models include, but are not limited to BigBird , Recurrent Memory Transformers (RMTs) , and Streaming Attention . Besides language tasks, sparse attention is also common in vision transformers [19; 25; 28].

## 3 Problem Setup

NotationLet \(\|\|_{2}\), \(\|\|_{F}\) be the \(2\)-norm and Frobenius norm, respectively. We use the shorthand \([n]:=\{1,,n\}\). We denote the all-one vector of length \(N\) by \(^{N}\). For a matrix \(M\), we denote its \(i\)-th row by \(M_{i,:}\) and its \(j\)-th column by \(M_{:,j}\).

Throughout the analysis in the paper, we formalize the attention mask to be a directed graph \(\). Formally, we represent a directed graph with \(N\) nodes by \(\) and let \(E()\) be the set of directed edges of \(\). If there is a directed edge going from \(j\) to \(i\) in \(\), i.e. \((j,i) E()\), for the attention mechanism it means that token \(j\) serves as a direct context for token \(i\) or token \(i\) attends to token \(j\). The set \(_{i}\) of all neighbors of node \(i\) is then \(\{k:(k,i) E()\}\).

Furthermore, we will be using the following graph-theoretic terminology:

**Definition 1** (Reachability).: _We say a node \(v\) is reachable from \(u\) in a directed graph \(\) if and only if there is a directed path \((u,n_{1}),(n_{1},n_{2}),...,(n_{k},v)\) from \(u\) to \(v\)._

**Definition 2** (Strongly Connected).: _A directed graph \(\) is said to be strongly connected if and only if any two distinct of nodes are reachable from each other._

**Definition 3** (Center Node).: _A node \(v\) from which every node in the directed graph \(\) is reachable is called a center node._

**Definition 4** (Quasi-Strongly Connected).: _A directed graph \(\) is said to be quasi-strongly connected if \(\) has at least one center node._

**Definition 5** (Radius).: _The radius of a quasi-strongly connected graph is defined to be the longest distance from a center node to any node in the graph. If there are multiple center nodes, then it is the smallest value among them._

### (Masked) Attention Mechanism

Given token representations \(X^{N d}\), the raw attention score matrix is computed as

\[R=XW_{Q}(XW_{K})^{}/}\,,\]

where \(W_{Q},W_{K}^{d d^{}}\) are the query and the key matrix, respectively, and \(}\) is a temperature term to control the scale of raw attention scores. To enforce a masked attention, we create a sparse attention matrix \(A^{N N}\) based on \(R\) whose sparsity pattern is specified by a directed graph \(\): we normalize \(R_{ij}\) among all allowed token attention interactions \((k,i) E()\) such that

\[A_{ij}=_{}(R_{ij})=)}{_{k _{i}}(R_{ik})}\;\;(j,i) E(),\;\;\;A_{ij}=0\;\;.\]

### LayerNorm

Given token representations \(X^{N d}\), LayerNorm subtracts the mean across different columns in each row and then scales each row to have a unit \(2\)-norm. In this work, we consider LayerNorm to only perform the scaling operation, which is a common assumption in theoretical analyses of the attention mechanism [15; 34]2. Mathematically, let \(D=(d_{1},d_{2},...,d_{N})\) where \(d_{i}=1/\|X_{i,.}\|_{2}\) for all \(i[N]\), then \((X)=DX\).

### Self-attention dynamics

For our analysis, we consider single-head (masked) self-attention networks (SANs), where the layerwise update rule can be written as

\[A^{(t)}=_{^{(t)}}(X^{(t)}W _{Q}^{(t)}(X^{(t)}W_{K}^{(t)})^{}/})\] (1) \[^{(t+1)}=A^{(t)}X^{(t)}W_{V}^{(t)}\] (2) \[X^{(t+1)}=(^{(t)}):=D^{(t)}A^{(t)}X^{( t)}W_{V}^{(t)}\,.\] (3)

where \(W_{V}^{(t)}^{d d^{}}\) is the value matrix. For simplicity, throughout the paper, we assume that \(d=d^{}\) and \(^{(t)}=\), i.e. the attention mask is the same for all the attention layers. Yet the results can be easily generalized to the case where masks are time-varying and satisfy similar regularity conditions.

## 4 Main Results: Attention with Masking and LayerNorm

To study how token representations evolve under the self-attention dynamics and behave in the long-term, we measure token similarity via \(():^{N d}_{ 0}\):

\[(X):=\|X-_{X}\|_{F},\;_{X}=^{ }X/N\,.\] (4)

This measure is mathematically equivalent to the measure \((X)=_{x^{d}}\|X- x^{}\|_{F}\) used in , but the form in (4) is easier to work with in the general analysis and more direct to compute. Another advantage of our formulation is that it clearly demonstrates that Theorem 1 and Theorem 2 are not dependent on the specific choice of \(()\): these results apply to any Lipschitz \(^{}()\) with a Lipschitz constant \(L\) such that \((X)=0\) if and only \(X_{i,:}=X_{j,:}, i,j[N]\), as we can use the formulation to directly derive that

\[^{}(X)=|^{}(X)-^{}(_{_{X}})| L \|X-_{_{X}}\|_{F}=L(X)\,.\]

Finally, we adopt the following assumptions in our analysis:

* \(\) contains self-loops. i.e. \((i,i) E\) for every token \(i[N]\).
* There exist constants \(C\) such that \(_{t}\{\|W_{Q}^{(t)}\|_{2},\|W_{K}^{(t)} \|_{2}\} C\).

**A1** ensures that every token has a neighbor so that the masked attention computation is well-defined for every token in every layer, while **A2** assumes that the key and query weight matrices are bounded, which is key for efficient attention computation in practice .

### Masked Attention

We first analyze the case without LayerNorm and focus on the effect of the attention mask. To ensure boundedness of the token trajectories \(X^{(t)}\) for all \(t 0\) even without LayerNorm, we further assume that

* The sequence \(\{\|_{t=0}^{k}W_{V}^{(t)}\|_{2}\}_{k=0}^{}\) is bounded.

Then with general attention masks \(\), there remains a strong connection between tokens via attention, and the token representations collapse exponentially to rank one.

**Theorem 1**.: _Consider the self-attention dynamics without LayerNorm defined in (2). Under_ **A1**_-_**A3**_, if \(\) is a quasi-strongly connected graph, then there exists \(>0\) where for all \(t 0\),_

\[A_{i,j}^{(t)},(j,i) E.\] (5)

_As a result, a rank collapse of tokens happens exponentially with respect to \(()\), i.e. there exists \(C>0\) such that_

\[(X^{(t)}) C(1-^{r})^{t/r}\,,\] (6)

_where \(r\) is the radius of \(\), meaning that tokens converge to a common vector exponentially._

The detailed proof is provided in Appendix B. The above result suggests that with pure self-attention, as long as there is a token which all other tokens in the sequence can directly or indirectly attend to over a fixed number of layers, exponential rank collapse of tokens to a common vector is guaranteed. In particular, it generalizes the main result in  from \(\) being a complete graph to a much more general class of attention patterns: the attention pattern \(\) only needs to be quasi-strongly connected, meaning that the result applies to general attention masks used in practice including the causal mask used in decoder-only models such as the GPT family , or sparse attention patterns deployed in many efficient transformer models . We discuss a few interesting implications below.

Local vs. global attentionThe exponential rate \((1-^{r})^{1/r}\) is monotone in the graph radius \(r\). This means that rank collapse should be slower for graphs with larger radius \(r\). Our result thus indirectly supports the use of local attention patterns , which not only make the attention computation more efficient (what those works are originally motivated by), but also implicitly alleviate the rank collapse issue.

Focused vs. uniform attentionIn addition, the exponential rate is monotone decreasing in \(\), which means that rank collapse is slower with smaller \(\). One can interpret \(\) as how "focused" attention is distributed among reachable tokens, as \(\) is maximized when attention happens uniformly among reachable tokens. Besides applying attention masks and restricting the number of reachable tokens, another way to control how focused attention would be is through the temperature term \(d_{QK}\). As larger values of \(d_{QK}\) would make the attention allocation among reachable tokens more even, they should make rank collapse happen faster across layers.

Trade-off between rank collapse and universal approximating powerFinally, for strongly connected graphs, the above result also reveals a trade-off between universal function approximation power and the rate of rank-collapse. Yun et al.  show that transformers with strongly connected graph masks are sequence-to-sequence function universal, yet with a mask \(\) they need at least the diameter of \(\) layers to achieve the full sequence-to-sequence function approximation property. This implies that masks with smaller diameters (and thus smaller radii, as radius \(\) diameter \(\) 2 radius) are more efficient in terms of function approximation power, yet they are more prone to rank collapse.

**Remark 1**.: _Since the analysis in  fundamentally relies on the shift-invariant property of \(()\), it is necessary that all the tokens are allowed to attend to all the other tokens for their proof to work. On the contrary, we leverage a different graph-theoretic approach to exploit the common structure of the attention matrices to obtain a general result for masked attention._

### Masked Attention with LayerNorm: Rank Collapse

So far, we have considered the pure self-attention dynamics without LayerNorm and focused on the role of the attention mask. What happens if we add LayerNorm and consider the attention dynamics defined in (3) instead? In this section, we first present a negative result, showing that exponential collapse of tokens to a common vector can still happen for certain classes of value matrices.

**Theorem 2**.: _Consider the self-attention dynamics with LayerNorm defined in (3). Let \(\) be a strongly connected graph. Assume_ **A1**_-_**A2**_, and that \(W_{V}^{(t)}\) is orthogonal for all \(t 0\), and in addition, the initial input \(X^{(0)}\) satisfies that_

* \(N d\)_,_ \(X^{(0)}\) _has full rank._

_Then there exist \(C>0\), \(>0\) such that \(N<1\) and_

\[(X^{(t)}) C(1-N^{2r})^{} t 0\,,\] (7)

_where \(r\) is the radius of \(\), meaning that tokens converge to a common point on \(^{d-1}\) exponentially._

The detailed proof is provided in Appendix C. The result can be seen as a generalized discrete version of Theorem 4.1 in . Notably, our analysis is based purely on advanced linear algebra tools: infinite products of non-negative matrices and their ergodicity, and can account for time-varying weights and general attention masks, as opposed to fixed \(W_{K},W_{Q},W_{V}\) over time and \(\) being complete (which is the case in ). One way to satisfy the condition (\(*\)) on the initial input \(X^{(0)}\) is to require \(N d\) and to initialize tokens uniformly randomly on \(^{d-1}\), then the condition hold almost surely. This is how the condition is dealt with in .

Note that the condition \((*)\) implies that there exists \(v^{d-1}\) such that \( X^{(0)}_{i,},v>0\) for all \(i[N]\) by either the hyperplane separation theorem or Farkas' lemma (see Lemma 6 in Appendix C). If the initial token geometry satisfies a stronger condition than the above, then \((*)\) is no longer necessary and Theorem 2 even directly generalizes to quasi-strongly connected graphs \(\). We define \(^{(t)}:=_{i,j[N]} X^{(t)}_{i,},X^{(t)}_{j,}\), indicating the minimal cosine similarity between tokens. If the cosine similarities are non-negative for all pairs of tokens initially, then the rank collapse happens exponentially, as long as \(\) is quasi-strongly connected.

**Corollary 1**.: _Consider the self-attention dynamics with LayerNorm defined in (3). Let \(\) be a quasi-strongly connected graph. Under_ **A1**_-_**A2**_, if \(W_{V}^{(t)}\) is orthogonal for all \(t 0\) and \(^{(0)} 0\), then there exist \(C>0\), \(>0\) such that \(N<1\) and_

\[(X^{(t)}) C(1-^{2r})^{}, t 0\,.\] (8)

_where \(r\) is the radius of \(\), meaning that tokens converge to a common point on \(^{d-1}\) exponentially._

Full mask vs. causal maskWe can refine Corollary 1 by specifying the number of center nodes \(n\) in the mask \(\), then the upper bound for the exponential rate would be \((1-n^{2r})\) instead, meaning that the rate of rank collapse can be negatively affected by the number of center nodes in the mask \(\). In the case of full attention where \(\) is the complete graph, the mask would have \(N\) center nodes, matching the bound in Theorem 2. In the case of causal attention where \(\) is the causal graph, the mask would only have one center node, and the upper bound would be looser, suggesting the advantage of the causal mask in mitigating the rate of rank collapse as compared to the full mask.

Post-LN vs. Pre-LNThe definition of LayerNorm in (3) follows from the original transformer paper  and nowadays it is referred as _post-LN_. An alternative use of LayerNorm in many LLMs, where LayerNorm comes before self-attention, is called _pre-LN_ and can be written instead as

\[X^{(t+1)}=A^{(t)}(X^{(t)})W^{(t)}:=A^{(t)}D^{(t)}X^{(t)}W_{V}^ {(t)}\,.\] (9)

Note that Theorem 2 and Corollary 1 apply directly to the case of pre-LN with similar proofs.

### Masked Attention with LayerNorm: Counterexample

The main results from the previous sections seem pessimistic: the self-attention dynamics seem doomed to collapse into a rank one subspace in the long run, with or without LayerNorm. In this section, however, we will first construct a nontrivial counterexample with only LayerNorm such that for a general class of input sequences, tokens converge to an equilibrium where rank collapse does not happen. Notice that for a transformer model to be practical, it is important that it can prevent rank collapse for a general class of input sequences rather than a specific input sequence. We will then show a general result stating that, with LayerNorm and proper choice of value matrices, the self-attention dynamics can have a rich set of equilibria with _any possible rank_ between one and full. Moreover, for a general class of input sequences, tokens provably do not converge to a rank one subspace under the resultant dynamics.

#### 4.3.1 Illustrative Counterexample

For simplicity of illustration, we consider \(N=2,d=2\), and \(\) to be the causal mask. Then let \(W_{K}^{(t)}=W_{Q}^{(t)}=\), which leads to the attention matrices

\[A^{(t)}=1&0\\ 1/2&1/2 t 0\,.\]

We further let

\[W_{V}^{(t)}=1&w\\ 0&1 t 0\,,\]

for \(w\). Without loss of generality, fix \(w>1\). Then a careful analysis shows that, depending on its initial position, the first token will either converge to \((0,1)\) or \((0,-1)\). Suppose the first token converges to \((0,1)\). Then the convergence of the second token as \(t\) is illustrated in Fig. 1, where \(A=(-,}})\), \(B=(-,-}})\). A rigorous proof can be found in Appendix E. Note that due to the scaling effect of LayerNorm, any scaled version \(c^{(t)}W_{V}^{(t)},c^{(t)}>0\) of \(W_{V}^{(t)}\) works equivalently here.

**Remark 2**.: _For any orthogonal \(Z^{d d}\), \(W_{Z}=Z^{}WZ\) works equivalently in this example, and the resulting token trajectories follow \(X^{(t)}Z\)._

This nontrivial counterexample suggests that with the LayerNorm dynamics in (3), there are proper choices for \(W_{K}^{(t)},W_{Q}^{(t)},W_{V}^{(t)}\) matrices that can prevent tokens from collapsing to a rank one subspace, for a nonzero measure set of input sequences.

#### 4.3.2 Generalization of the Counterexample

We conclude this section with the following statement generalizing the previous illustrative example: with a proper choice of weight matrices, tokens under the attention dynamics with LayerNorm can simultaneously have equilibria of any rank between one and full. Moreover, for a general class of input sequences, tokens provably do not converge to a rank one subspace.

**Theorem 3**.: _Let \(\) be the causal graph and consider the attention dynamics with LayerNorm defined in (3). Then there exists \(\{W_{Q}^{(t)},W_{K}^{(t)},W_{V}^{(t)}\}_{t=0}^{}\) satisfying_ **A2**_-_**A3** _such that the corresponding dynamic has the following properties:_

1. _For any_ \(1 k\{N,d\}\)_, there exist at least_ \(2^{k}\) _equilibria of rank_ \(k\)_;_
2. _For a general class of input sequences_ \(X^{(0)}\) _in_ \((^{d-1})^{N}\) _with measure greater than_ \(0\)_,_ \(X^{(t)}\) _does not converge to a rank one subspace as_ \(t\)

Figure 1: Long-term behavior of tokens in the case of \(N=2,d=2\). Without LayerNorm (left), both tokens collapse to the same point in \(^{2}\); whereas with LayerNorm (right), such a collapse would not necessarily happen and token representations can maintain full rank in the long term (first token converges either to \((0,1)\) or \((0,-1)\). Assuming convergence to \((0,1)\) for the first token, the second token converges to \(B\), if it is initially located within the red segment).

The detailed proof is provided in Appendix F. For the sake of simplicity and observing that in a causal graph, there is an inherent chronological order among tokens, in the convergence analysis of each token we assume all tokens preceding it have already converged.

The above result is in direct contrast to the case without LayerNorm (Theorem 1), where all input sequences eventually result in complete rank collapse to a one-dimensional subspace. It also stands in contrast to the hypothesis that LayerNorm plays no role in mitigating the collapse , and suggests that it is an important component of the self-attention dynamics in transformers--adding LayerNorm fundamentally changes the behavior of the underlying system. Compared to the case without LayerNorm, first, LayerNorm guarantees that the system never diverges, no matter what the value matrices are. Second, and more importantly, attention with LayerNorm leads to a surprisingly more expressive, versatile dynamics than the system without it--composition with different value matrices \(W_{V}^{(t)}\) can result in different long-term token behaviors: in some cases, tokens completely collapse to a single point (Theorem 2), while in others tokens can retain higher rank asymptotically (Theorem 3).

### Discussion

Next, we discuss three further intriguing findings from our analysis of the self-attention dynamics with LayerNorm presented in the previous section.

Scaling effect of LayerNorm is keyFrom a dynamical system perspective, rank collapse happens under pure self-attention because the attraction of the center node causes all the tokens to be aligned. In the counterexample provided in Section 4.3.1, the key insight to prevent the second token from aligning with the first token is that the updated representation of the second token must generate a component canceling out the attraction from the first token, which acts as a repulsion. The crucial role of LayerNorm here is to stabilize this cancellation process by readjusting the scale of tokens to ensure that the cancellation persists in all the updates, which would not be the case if there is no LayerNorm.

Anisotropy of token embeddingsAnalyzing and understanding the token geometry in transformers has long been of interest to the NLP and general ML community. In particular, empirical findings suggest that contextual token embeddings generated by transformers are anisotropic, meaning they tend to concentrate in a narrow region [1; 9; 13; 14; 17]. Interestingly, as we show in Appendix F.2, the full rank equilibria described in Theorem 3 align with this observation, as tokens lie in a narrow region on \(^{d-1}\). While it still remains unclear how exactly such a representation geometry is useful for downstream tasks [1; 9; 14; 17], empirical studies have found that anisotropy is not necessarily harmful for semantic representations and can assist with tasks like clustering [1; 26].

Stability of the equilibriaFinally, we would like to note that due to the combinatorial nature of the analysis with increasing dimensions, we do not prove the exact convergence to specific equilibria beyond not converging to a rank one subspace. However, we observe in simulation that these equilibria are indeed stable in certain directions, despite their regions of attraction being relatively small. This suggests that while LayerNorm improves the expressivity of pure self-attention dynamics, the resulting expressive power might still be limited in a way that it would be difficult for a generic input sequence to reach a specific configuration at equilibrium. This observation seems in line with the fact that MLP modules are crucial for the universal sequence-to-sequence approximation power of transformers [40; 41]. To achieve maximal expressivity, and in particular, to be able to move tokens freely around the whole space (such as in the case for transformers with MLPs), nonlinear power from MLPs would be necessary.

## 5 Numerical Experiments

In this section, we validate our theoretical findings via numerical experiments. Following , we randomly select 3000 samples of 128-token excerpts (based on the BERT tokenizer) from Wikipedia using the Wikipedia API in Python. More experimental details and additional results can be found in Appendix G.

The effect of attention masks and LayerNormWe first verify the effect of different attention masks and LayerNorm on rank collapse with respect to \(()\). We use BERT  as the backbone transformer model and consider five different model variants for a controlled experiment: self-attention network (SAN) in which the model has self-attention layers; SAN with skip connections; SAN with LayerNorm where in each layer there is self-attention followed by LayerNorm; SAN with both skip connections and LayerNorm where LayerNorm is put after skip connections; and finally the standard transformer with all the components. For attention masks, we select four representative types: the complete graph, the causal graph, the sliding window (tokens can only attend to the token right before and after and themselves) and the uni-directional sliding window (tokens can only attend to the token right before and themselves). Fig. 2 shows the average values of \((X^{(t)})\) with standard deviations over the \(3000\) samples in different architectures. We see that in SANs, \((X^{(t)})\) converges to zero exponentially for all attention masks, yet more local attention masks slow down the convergence rate. Furthermore, in randomly initialized models, right after we add in LayerNorm, \((X^{(t)})\) no longer converges to zero and can be maintained at a stable level significantly above zero. In pretrained models, LayerNorm helps prevent the issue together with other components such as skip connections and stabilize the representations. Results in \(128\)-layer initialized models also exhibit the same trend, and can be found in Appendix G.1.

The complex interplay between different components in transformersPrevious works have shown that skip connections can help combat rank collapse in transformers. Yet in Fig. 2, skip connections seem to make \((X)\) unstable, particularly in deeper layers (for 128-layer results, see Appendix G.1). Compared with full transformers where \((X)\) stays relatively stable, there is a clear

Figure 3: Evolution of \((X^{(t)})\) (in log-log scale) as the number of layers increases. Smaller temperature terms alleviate the rate of rank collapse, and effect is more significant with global attention than with sparser masked attention, and more in shallower layers than deeper layers.

Figure 2: Evolution of \((X^{(t)})\) (in log-log scale) as the number of layers increases. Rank collapse happens exponentially for pure attention, despite different attention masks having different convergence rates. However, as soon as we solely add in LayerNorm, \((X^{(t)})\) no longer converge to zero in randomly initialized models; in pretrained models, LayerNorm helps prevent the issue together with other components and stabilize the representations.

discrepancy. In that case, LayerNorm emerges as a crucial element in mitigating rank collapse and stabilizing token representations while also counteracting potential negative effects of pure skip connections. This underscores the complex interplay between different components in transformers.

The effect of the temperature termNext, we investigate the effect of the temperature term \(d_{QK}\) in the attention calculation. Since the amount of focus of the attention is affected by both the attention mask and the temperature term, we investigate the effect of \(d_{QK}\) with different attention masks. Fig. 3 shows the average values of \((X^{(t)})\) with standard deviations over the \(3000\) samples with different masks in pretrained SANs. We observe that while a smaller temperature term \(d_{QK}\) alleviates the (initial) rate of rank collapse in all cases, the effect is more significant with global attention than with sparser masked attention, and more in shallower layers than in deeper layers. The results in initialized models show similar trends and can be found in Appendix G.2.

Evolution of token geometryFinally, we study the evolution of token geometry as the number of layers increases in transformers. Specifically, to capture more fine-grained details than \(()\), we measure how the rank, minimal singular value of token representations, and absolute values of pairwise cosine similarities among tokens change as tokens pass through the transformer layers. We select four representative pretrained transformer models: BERT , GPT2 , T5  and ALBERT  and the average results over \(3000\) samples are shown in Fig. 4. We see that all models exhibit similar behaviors as tokens evolve along the layers: the models can effectively preserve the full rank of token representations, as also evident from the minimal singular values consistently remaining significantly above zero. Yet the absolute cosine similarities among tokens suggest that tokens gradually concentrate in a narrow region. This empirical observation aligns with the characteristics of the equilibrium in our counterexample that a stable long-term geometry for token representations in transformers can retain full rank while being close to a low dimensional geometry at the same time.

## 6 Conclusion

The attention mechanism has led to significant empirical progress in building foundation models. From a dynamical system perspective, the time-varying and nonlinear nature of self-attention dynamics, when coupled with attention masks, LayerNorm and other components in transformers, enables expressive and complex behavior. In particular, we show that the choice of attention masks directly affects the token dynamics. It would hence be valuable for future research to investigate how to effectively design attention masks. Our result further suggests that robust token geometries under multi-layer self-attention can exhibit both full-rankness and anisotropic characteristics simultaneously, which is the case in real transformers as well. It would be interesting to study how such co-existence of these two characteristics would help with learning tasks -- whether the full-rank yet close-to-low-dimensional geometry enables efficient learning and generalization while still letting tokens capture meaningful fine-grained details.