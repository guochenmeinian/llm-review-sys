# Text-Aware Diffusion for Policy Learning

Calvin Luo Mandy He Zilai Zeng Chen Sun

Brown University

{calvin_luo,mandy_he,zilai_zeng,chensun}@brown.edu

Equal contribution.

###### Abstract

Training an agent to achieve particular goals or perform desired behaviors is often accomplished through reinforcement learning, especially in the absence of expert demonstrations. However, supporting novel goals or behaviors through reinforcement learning requires the ad-hoc design of appropriate reward functions, which quickly becomes intractable. To address this challenge, we propose Text-Aware Diffusion for Policy Learning (TADPoLe), which uses a pretrained, frozen text-conditioned diffusion model to compute dense zero-shot reward signals for text-aligned policy learning. We hypothesize that large-scale pretrained generative models encode rich priors that can supervise a policy to behave not only in a text-aligned manner, but also in alignment with a notion of naturalness summarized from internet-scale training data. In our experiments, we demonstrate that TADPoLe is able to learn policies for novel goal-achievement and continuous locomotion behaviors specified by natural language, in both Humanoid and Dog environments. The behaviors are learned zero-shot without ground-truth rewards or expert demonstrations, and are qualitatively more natural according to human evaluation. We further show that TADPoLe performs competitively when applied to robotic manipulation tasks in the Meta-World environment, without having access to any in-domain demonstrations.

## 1 Introduction

Can we train reinforcement learning agents that drive humanoids in a virtual environment  to stably stand? How about standing with _hands on hips_, _kneeling_, or _doing splits_? While state-of-the-art algorithms have shown success on the former scenario (e.g. ), the latter (illustrated in Figure 1) remains challenging due to the need for carefully (and often manually) crafted reward functions to specify the desired behaviors. The dependence on ad-hoc designed reward functions renders inscalable the learning of ever-increasing amounts of novel behaviors, which are required in applications ranging from character animation  to robotic manipulation .

Our work looks towards natural language as a powerful interface through which humans can flexibly specify desired goals or behaviors of interest. We therefore investigate how to construct a zero-shot text-conditioned reward signal, replacing the need for ad-hoc designs, through which text-aligned policies can be learned. We present Text-Aware Diffusion for Policy Learning (TADPoLe), which utilizes a large-scale pretrained, _frozen_ text-conditioned diffusion model to generate a _dense_ reward signal for policy learning. We hypothesize that generative diffusion models, which are pretrained on internet-scale datasets to produce text-aligned, natural-looking images  and videos , can be utilized to automatically craft a _multimodal_ reward signal that encourages an agent to behave both _faithfully_ with respect to text conditioning and _naturally_ with respect to human perception. Our method is novel in its reward computation, as well as its utilization of a domain-agnostic generative model, rather than one trained from environment-specific or task-specific video demonstrations, as used in prior work .

TADPoLe is motivated by the insight that a reinforcement learning policy can be viewed as an agent-centric _implicit_ video representation when operating within an environment with visual rendering capabilities. As illustrated in Figure 2 (left), an agent's video generation process involves the selection of actions following a policy \(_{}\), and the conversion of the action sequence into video subsequences through the environment's rendering function. A policy can therefore be seen as iteratively generating frames conditioned on the actions it selects; on the other hand, a text-to-image diffusion model can also be seen as generating static image frames, but conditioned on natural language instead. A connection can then be established between a policy and a diffusion model, where the frame or video segment "generated" by the policy can be critiqued by evaluating how likely a text-conditioned diffusion model would generate the same visuals, thus providing dense text-aligned reward signals to guide policy learning (Figure 2 right). Our work is inspired by DreamFusion , where a text-conditioned 3D model is learned through rendered views, and where volumetric raytracing ensures spatial consistency. Here, we seek to learn a text-conditioned policy through rendered frames or subsequences, where the environment naturally ensures temporal continuity and consistency with respect to a notion of physics instantiated by the environment.

Concretely, TADPoLe achieves text-conditioned policy learning by using a generative diffusion model in a discriminative manner. It computes the reward signal as a weighted combination of two reward terms, which aim to measure the alignment between the rendered observation and text conditioning, and the naturalness of the agent's behaviors, respectively. In this way, we can in effect "distill" the natural visual and motion priors as well as vision-text alignment understanding captured within the diffusion model into a policy. By default, TADPoLe uses a text-to-image diffusion model  to densely compute a reward signal solely from the immediate subsequent frame after each action. We then generalize the framework to Video-TADPoLe, which uses a text-to-video diffusion model  to calculate dense rewards as a function of a sliding context window of past as well as future frames achieved. The agent is thus trained to select actions such that arbitrary consecutive subsequences of frames are well-aligned with text as well as natural video (e.g. motion) priors.

We highlight TADPoLe as the first approach to leverage _domain-agnostic_ visual generative models for policy learning. Through quantitative and human evaluations on Humanoid , Dog , and Meta-World  environments, we demonstrate that TADPoLe enables the learning of novel, zero-shot policies that are flexibly and accurately conditioned on natural language inputs, across multiple robot configurations and environments, for both goal-achievement and continuous locomotion tasks. TADPoLe therefore provides two main benefits simultaneously: a performant approach towards zero-shot policy learning, where complex reward functions no longer need to be manually specified per task, and a promising path towards distilling priors summarized from large-scale pretraining into policies, ultimately resulting in the learning of more naturally-aligned behavior within arbitrary environments. Visualizations and code are provided at diffusion-supervision.github.io/tadpole/.

## 2 Related Work

Diffusion models [34; 35; 16; 36] have recently demonstrated amazing generative modeling capabilities, particularly in the domain of text-conditioned image generation [5; 29; 32; 31]. Notably, guidance [36; 5; 17] has been shown to be a critical component in producing visual outputs aligned with textual data, enabling the generation of images that accurately match a desired text caption, especially when the models are scaled to utilize large foundation models such as CLIP  or T5 , and trained on massive image-text datasets . Our work is inspired by DreamFusion , where a

Figure 1: Our proposed Text-Aware Diffusion for Policy Learning (TADPoLe) framework leverages frozen, pretrained text-aware diffusion models to automatically craft dense text-conditioned rewards for policy learning. Here we visualize TADPoLe achieving diverse text-conditioned goals in the Humanoid, Dog, and Meta-World environments.

pretrained, frozen text-to-image diffusion model is able to supervise the learning of zero-shot 3D models conditioned on text. We propose leveraging pretrained diffusion models to supervise the learning of flexible, text-conditioned policies in a zero-shot manner over the time dimension.

There are numerous works that investigate how interactive agents can learn to perform behaviors specified by textual inputs. SayCan  grounds the knowledge of complex, high-level behaviors within an LLM to the context of a robot through pretrained behaviors. This then enables an LLM to instruct and guide a robot, through combining low-level behaviors, to perform complex temporally extended behaviors. LangLfP proposes a method for incorporating free-form natural language conditioning into imitation learning by first associating goal images with text captions and training a policy to follow either language or image goals, but only conditioning on natural language during test time inference . The Text-Conditioned Decision Transformer learns a causal transformer to autoregressively produce actions conditioned on both text tokens as well as state and action tokens . Similarly, Hiveformer proposes a unified multimodal Transformer model for robotic manipulation that conditions on natural language instructions, camera views, as well as past actions and observations . However, LangLfP, Text-Conditioned Decision Transformer, and Hiveformer, all require training on datasets of trajectories that have been labelled with natural language. In contrast, TADPoLe enables the learning of text-conditioned policies irrespective of visual environment, and without requiring any pretraining dataset of demonstrations or labeling.

Similar to our work, UniPi  treats the sequential decision-making problem as a text-conditioned video generation problem. The authors propose training a video diffusion model to produce a future visual plan for the agent; the subsequent frames are then converted to actions by means of an inverse dynamics model. VLP  utilizes text-to-video generative models for planning and goal generation for an agent. Both methods require the video generative models to be trained _ad-hoc_ on the target environments, whereas we directly use frozen general-purpose generative models. Mahmoudieh et al.  propose a framework that uses CLIP to generate a reward signal from a text description of a goal state and raw pixel observations from the environment, which is then used to learn a task policy. In VLM-RM , the authors also explore utilizing CLIP as the reward model for training humanoids to accomplish complex goal-reaching tasks. In our work, we investigate locomotion tasks on top of goal-achievement, and explore how using diffusion models to produce a reward signal can outperform CLIP-based approaches. Although not conditioned on text, VIPER  also aims to harness recent advancements in generative modeling by employing a video prediction model's likelihoods as a reward signal. However, VIPER does not enable the learning of policies conditioned on text, and requires in-domain expert videos for ad-hoc training the video model. Finally, Diffusion Reward  also extracts a reward from a diffusion model to train policies; however, it requires training an ad-hoc video model on expert trajectories, the collection of which cannot always be assumed to be trivial, and does not enable text-conditioned policy learning.

## 3 Method

We propose Text-Aware Diffusion for Policy Learning (**TADPoLe**) to learn text-aligned policies by leveraging frozen, pretrained text-conditioned diffusion models. An overview of the framework can be found in Figure 3.

Figure 2: A policy \(_{}\) that interacts with an environment can be treated as an agent-centric _implicit_ video representation, where the arrow of time is actuated by the agent’s actions and the pixels are rendered by the environment. The rendered behaviors can then be evaluated by a text-aware diffusion model to produce dense rewards, thereby providing text-conditioned update signals to the policy.

### Text-Aware Diffusion for Policy Learning

We first describe how TADPoLe produces text-conditioned rewards from image observations. At each timestep \(t\), reward \(r_{t}\) is computed as a score between rendered subsequent image \(_{t+1}\) and the provided text caption describing the behavior of interest, denoted by \(y\), using a frozen, pretrained text-to-image diffusion model. We begin by corrupting the rendered image \(_{t+1}\) with a sampled Gaussian source noise vector \(_{0}(;,)\) to produce noisy observation \(}_{t+1}\), and use the diffusion model to make an unconditional prediction \(}_{}(}_{t+1};_{})\) as well as a conditional prediction \(}_{}(}_{t+1};_{},y)\). Here \(}_{}()\) is a neural network that predicts the source noise given \(}_{t+1}\), the level of noise corruption \(_{}\), and optionally the text prompt \(y\); we overload notation to have \(}_{}\) represent the source noise prediction in Figure 3. We then compute the mean squared error (MSE) between the two predictions as a reward signal \(r_{t}^{}\) to be _maximized_:

\[r_{t}^{}=\|}_{}(}_{t+1};_{},y)-}_{}( }_{t+1};_{})\|_{2}^{2}.\]

As investigated in Appendix B.3, we empirically observe that \(r_{t}^{}\) plays a crucial role on the success of TADPoLe. We hypothesize that for an appropriately-selected noise corruption level \(_{}\), this term measures the alignment between the environmental observation and the text prompt. Intuitively, for unconditional prediction \(}_{}(}_{t+1};_{})\), the model is incentivized only to bring the noisy input to any arbitrary cleaner image, and makes minimal edits by moving it towards the closest clean mode in data space. On the other hand, if the model recognizes visual features in the noisy image aligned with the text prompt, conditional prediction \(}_{}(}_{t+1};_{},y)\) is incentivized to do "extra work" and bring it closer to the specific mode described by the text conditioning. We thus expect the MSE to be larger for well-aligned text conditioning. For an unaligned text prompt, the model may have more difficulty in recognizing relevant visual features in the corrupted image, and therefore generally has a lower computed \(r_{t}^{}\) signal. Therefore maximizing \(r_{t}^{}\) is a tractable proxy for maximizing the alignment between the rendered observation \(_{t+1}\) and the provided text prompt \(y\).

We also wish to encourage behaviors that are natural to human perception (e.g. a humanoid should walk similar to how a typical pedestrian would walk). We approximate the _naturalness_ of a behavior by how accurately the diffusion model is able to predict the exact source noise vector that was applied. Intuitively, if it voluntarily predicts the exact noise vector with informative text conditioning, thereby perfectly reconstructing the query image, then the diffusion model believes the original rendered frame is reasonably natural (according to the priors captured by the diffusion model). We would therefore like to minimize \(\|}_{}(}_{t+1};_{ },y)-_{0}\|_{2}^{2}\). We would also like this term to be comparatively closer to the source noise vector than the unconditional prediction is, further reaffirming the benefit of the text conditioning. We therefore seek to also _maximize_ a comparative reconstruction term as below:

\[r_{t}^{}=\|}_{}(}_ {t+1};_{},y)-_{0}\|_{2}^{2}-\| }_{}(}_{t+1};_{},y)-_{0}\|_{2}^{2}.\]

Figure 3: An illustration of the TADPoLe pipeline, which computes text-conditioned rewards for policy learning through a pretrained, frozen diffusion model. At each timestep, the subsequent frame rendered through the environment is corrupted with a sampled Gaussian source noise vector \(_{0}\). The pretrained text-conditioned diffusion model then predicts the source noise that was added. The reward is designed to be large when the selected action produces frames well-aligned with the text prompt.

Ultimately, we compose these two terms into a final reward signal \(r_{t}\) exposed to the policy during training. We scale each of the individual terms with tunable hyperparameter constants, and apply a \(\) transformation operation:

\[r_{t}=(w_{1}*r_{t}^{})+(w_{2}*r_{t }^{}).\]

The choice of using \(\) as a reward normalization technique is thoroughly studied in Section 4.5. TADPoLe is agnostic to the specific choices of policy network architecture and optimization objectives. A pseudocode of the method is provided in Algorithm 1. It is worth emphasizing that \(_{}\) and subscript-less \(t\) refer to different notions of time; \(t\) indexes the timestep of the agent in the environment, whereas \(_{}\) determines the level of noise to corrupt the raw observed image.

```
1:prompt = sample(action_phrase)
2:\(_{}\) = initialize(\(\))
3:\(\{\}\)
4:while not converged:
5:\(_{0} p(_{0})\)
6:for\(t\) in range(episode_length):
7:\(_{t}_{}(_{t}_{t})\)
8:\(_{t+1} P(_{t+1}_{t}, _{t})\)
9:\(_{0}(;, )\)
10:\(_{t+1} P(_{t+1}_{t+1})\)
11:\(_{t+1}(_{t+1},_ {0},_{})\)
12:\(r_{t}(_{t+1},_{0}, )\)
13:\((_{t},_{t},r_{t}, {s}_{t+1})\)
14:\(\)
15:\(=()\)
16:\(=(,\,)\)
17:\((,\,)\) ```

**Algorithm 1** Text-Aware Diffusion for Policy Learning (TADPoLe)

### TADPoLe with Text-to-Video Diffusion Models

Conceptually, there exist fundamental limitations to using a text-to-image model to provide a reward signal. As each image is evaluated statically and independently, we are unable to expect the text-to-image diffusion model to be able to accurately understand and supervise an agent in learning notions of speed, or in some cases, direction, as such concepts require evaluating multiple consecutive timesteps to deduce. We therefore propose Video-TADPoLe, where a dense text-conditioned reward signal is calculated over sliding windows of consecutive frames through a pretrained text-to-video diffusion model. We extend and generalize the reward formulation from TADPoLe thusly.

We can compute reward terms for arbitrary start index \(i\) and end index \(j\) inclusive, for \(i j\), by considering the sequence of subsequently rendered frames \(_{[i+1;j+1]}\). We once again utilize source noise vector \(_{0}(;, _{j-i+1})\) to produce noisy observation \(}_{[i+1:j+1]}\). Then, we can compute a batch of alignment reward terms through one inference step of the text-to-video diffusion model as:

\[r_{[i;j]}^{}=\|}_{}(}_{[i+1;j+1]};_{},y)- }_{}(}_{[i+1;j +1]};_{})\|_{2}^{2},\]

and a batch of reconstruction reward terms as:

\[r_{[i;j]}^{}=\|}_{}(}_{[i+1;j+1]};_{})- _{0}\|_{2}^{2}-\|}_ {}(}_{[i+1:j+1]};_{ },y)-_{0}\|_{2}^{2}.\]

For a desired context window of size \(n\), we then calculate the reward at each timestep \(t\) utilizing each context window that involves achieved observation \(_{t+1}\):

\[r_{t}=_{i=1}^{n}(w_{1}*r_{[t-i+1:t-i+n]}^{ }[i-1])+(w_{2}*r_{[t-i+1:t-i+n]}^{ }[i-1]).\]

Intuitively, we seek to calculate an overall reward for an action based off how well the resulting rendered frame aligns with text-conditioning at the beginning of a motion sequence, the end of one, and arbitrarily inbetween. For window size \(n=1\), this recreates TADPoLe behavior, but using a text-to-video model; for \(n>1\), we make the computation tractable through dynamic programming.

Experiments

We now demonstrate the effectiveness of TADPoLe on goal achievement, continuous locomotion, and robotic manipulation tasks. All results are achieved without access to in-domain demonstrations.

### Experimental Setup and Evaluation

**Benchmarks:** We present our main results using the Dog and Humanoid environments from the DeepMind Control Suite , and robotic manipulation tasks from Meta-World . Dog and Humanoid are known to be challenging due to their large action space, complex transition dynamics, and lack of task-specific priors (such as termination conditions). We update the environments by modifying the terrain rendered by MuJoCo  to have green grass and blue sky. We also limit the number of environment timesteps to be 300, which is sufficient to demonstrate successful learning of a behavior, rather than the default 1000. The agent's initialized joint configurations are also fixed, as we focus on learning text-conditioned capabilities rather than robustness to initialization conditions. Meta-World was initially designed for multi-task and meta-reinforcement learning, and was later adopted to evaluate language-conditioned imitation learning algorithms . We select a suite of tasks which are balanced for diversity and complexity, and pair each task with a text prompt (see tasks and their corresponding prompts in Appendix C). Following prior design  for Meta-World, we also add a sparse success signal to the dense text-conditioned reward signals.

**Implementation:** We use TD-MPC  as the reinforcement learning algorithm for all tasks. It is the first documented model to solve Dog tasks when ground truth rewards are available for walking. We fix the hyperparameters to the default ones recommended by the TD-MPC authors (see Table A3 in Appendix) for all experiments unless otherwise mentioned. We train Humanoid and Dog agents for 2M steps, and Meta-World agents for 700K steps. For Meta-World experiments, we scale the sparse success signal by 2. Visualizations and quantitative evaluations are reported using the _last_ checkpoint achieved at the end of training. We use StableDiffusion 2.1  as the text-to-image diffusion model (\(\)1.3B parameters), and AnimatedDiff  v2 (\(\)1.5B parameters) as the text-to-video diffusion model. AnimatedDiff is implemented on top of StableDiffusion 1.5. We fix the reward weights \(w_{1}=2000\) and \(w_{2}=200\) based on Humanoid standing and walking performance, and study their impact in Appendix B.3. Selection of noise level is discussed in Appendix A. All experiments are performed on a single NVIDIA V100 GPU.

**Baselines:** We compare TADPoLe against other text-to-reward approaches, including VLM-RM , LIV , and Text2Reward , on top of the same underlying TD-MPC architecture, hyperparameters, and optimization scheme for fair comparison. For LIV, we use their provided CLIP-based checkpoint finetuned on robotic demonstration videos. For VLM-RM, we utilize the ViT-bigG-14 CLIP checkpoint (\(\)1.3B parameters), reported as the best performing in their work. We follow a prompt template provided in the Text2Reward paper to generate reward functions for the Dog and Humanoid agent, interfaced through vanilla ChatGPT using GPT-3.5. Whereas VLM-RM and LIV provide a multimodal reward signal, and are more directly comparable to TADPoLe, it is of note that Text2Reward generates a text-conditioned reward function purely as the output of a pretrained language model. However, Text2Reward does have access to underlying sensor data such as speed and direction in real-time, whereas the visual interface approaches, including TADPoLe, do not.

**Evaluation Protocols:** We benchmark all text-conditioned methods with a corresponding standardized prompt for fair comparison, and report both quantitative as well as qualitative comparisons. We use cumulative ground-truth rewards as quantitative evaluation metrics for Dog and Humanoid when it is available. We note this is a naturally _unfavorable_ comparison for methods that provide a text-conditioned reward signal purely through a visual interface, as the reward the agent receives has no access to the underlying sensors (such as ones that measure speed and energy usage) that the ground-truth reward function uses to evaluate performance. For example, the ground-truth reward function may have an arbitrary threshold on a speed sensor that needs to be hit to constitute successful "walking", and a separate threshold for "running"; however the detailed characteristics of and even existence of such a sensor, as well as any thresholds surrounding it, are hidden for policies supervised only through vision and language feedback. Nonetheless, it offers a standardized, numerical comparison across all methods. For Meta-World, we report the "success rate" evaluation metric, computed as the proportion of evaluation rollouts in which the agent successfully completes the given task.

A main benefit of utilizing a reward signal conditioned flexibly on text is the ability to learn policies with behaviors beyond those defined by existing ground-truth reward functions. As these have no corresponding ground-truth reward functions, quantitative comparison across different text-conditioned methods is challenging; we therefore appeal to a qualitative user study. We perform a paid study through the Prolific platform, with a total of 25 anonymous random participants without prior training to estimate a general response from the human population. For a video demonstration from each trained model, selected as the last timestep of policy training without cherry-picking, each participant is asked if it sufficiently aligns with the text prompt it was conditioned on. These results are depicted in tables as checkmarks () and x-marks (), where a checkmark denotes if a majority of participants believe it is text-aligned. In Table A7, we provide the fine-grained user study results on what percentage of the users believe the video achieved by the policy is appropriately text-aligned. We then proceed with a user study regarding naturalness. Given a video produced by VLM-RM and TADPoLe, users are given a choice as to which they believed to be the more natural motion or pose. This seeks to approximate how naturally the resulting Humanoid and Dog policies behave, according to human belief over how people and dogs naturally move in the real world.

### Goal Achievement

For text-conditioned goal-achievement, the objective is to learn a policy to consistently achieve a particular pose described by a text prompt; as the emphasis is for every frame to match a fixed goal pose rather than performing continuous motion, it is natural to apply TADPoLe with text-to-image diffusion models. We set the noise level \(_{} U(400,500)\), with intuition provided in Appendix A.

In the Humanoid environment, there is a ground-truth reward function that measures standing performance, as a function of the straightness of the agent's spine. We therefore compare all text-conditioned methods using the provided reward function as a quantitative metric, with a standard prompt of "a person standing"; these results are shown in the first row of Table 1. TADPoLe and VLM-RM achieve competitive quantitative performance with an agent trained on the ground-truth reward function. The following rows show that according to the user study, TADPoLe consistently achieves text-aligned behaviors beyond making the Humanoid stand, whereas other approaches often fail. Table 2 shows that users consistently found TADPoLe to produce more natural-looking motions and poses when compared head-to-head with VLM-RM.

**Environment** & **Prompt** & **VLM-RM** & **LIV** & **Text2Reward** & **TADPoLe (Ours)** & **Ground-Truth** \\  Humanoid & “a person standing” & 247.05 (\(\) 16.90) & 11.27 & 10.50 & 254.43 (\(\) 8.76) & 287.68 (\(\) 4.64) \\  Humanoid & “a person in lotus position” & ✗ & ✗ & ✗ & ✗ & – \\ Humanoid & “a person doing splits” & ✗ & ✗ & ✗ & ✗ & – \\ Humanoid & “a person kneeling” & ✗ & ✗ & ✗ & ✗ & – \\ Dog & “a dog standing” & ✗ & ✗ & ✗ & ✗ & – \\ 

Table 1: Results for goal-achievement experiments on DeepMind Control Suite Dog and Humanoid environments. For rows with an associated ground-truth reward function, numerical results are listed; for performant approaches, we report mean and standard deviation across 5 seeds. For novel zero-shot text-conditioned behavior learning, checkmarks denote if the resulting policy is aligned with the provided text prompt according to human evaluation.

**Prompt** & **TADPoLe Naturalness (\(\))** \\  “a dog standing” & 87.5\% \\ “a person in lotus position” & 62.5\% \\ “a person doing splits” & 62.5\% \\ “a person kneeling” & 70.8\% \\  “a person walking” & 84.0\% \\ “a dog walking” & 76.0\% \\ 

Table 2: Qualitative study: percentages denote user preference for the naturalness of the resulting motion produced by TADPoLe over VLM-RM (goal-achievement) and Video-TADPoLe over ViCLIP-RM (continuous locomotion).

Figure 4: TADPoLe demonstrates sensitivity to subtle variations to the input prompt, learning to stand in different positions with only slight modifications to the text conditioning.

We then investigate whether or not TADPoLe is sensitive to subtle variations of the input prompt. We change the text prompt from "a person standing" to "a person standing with hands above head" and "a person standing with hands on hips". In Figure 4, we visually verify that the resulting Humanoid policy can indeed learn distinct behaviors that respect the different hand placement specifications. We take this as evidence that TADPoLe is capable of respecting fine-grained details and subtleties of the input prompts when learning text-conditioned policies.

### Continuous Locomotion

We further explore the ability of TADPoLe to learn continuous locomotion behaviors conditioned on natural language specifications. Such tasks are often difficult to learn purely from static external description, as there is no canonical pose or goal frame that if reached, would denote successful achievement of the task. This is challenging for approaches that statically select a canonical goal-frame to achieve, such as CLIP or LIV, and we propose Video-TADPoLe, which leverages large-scale pretrained text-to-video generative models, as a promising direction forward.

We utilize a noise level \(_{} U(500,600)\) in our continuous locomotion experiments. We perform a search over context windows of size \(n=\{1,2,4,8\}\), and report the best configuration per task. We observe that when the context window is too high (e.g. 8 or higher), the agent has consistently lower performance, and that although the agent learns coherent motion and repeats it, the pose is less text-aligned. For fair comparison against a text-video alignment model trained in a contrastive manner, we extend VLM-RM to ViCLIP-RM, where a ViCLIP-L-14 checkpoint  finetuned from ViT-L-14 CLIP is used to compute dense, text-conditioned rewards. At each timestep \(t\), we compute dense rewards as cosine similarity between the encoded representations of video observation up to \(t+1\) and the text prompt. We ask ViCLIP to encode 8 video frames at a time, which is adopted by its authors for zero-shot experiments.

For the Humanoid task, we find that Video-TADPoLe achieves the best results amongst methods trained purely from visual and/or language feedback as in Table 3. On the other hand, ViCLIP-RM indeed learns to take steps, but does so sideways while maintaining an unnaturally lopsided pose. Meanwhile, LIV and Text2Reward fail to learn meaningful behaviors.

For the Dog task, we notice that the policy learned via ViCLIP-RM collapses; it learns to strike a particular pose and maintain it for perpetuity. Text2Reward, which does not have access to any visual information, but does have access to ground-truth state information for the Dog including speed, direction, and joint positions, achieves a reward of 63.15. Ultimately, Video-TADPoLe achieves a comparable result using a context window of 4, while also distinguishing itself as the most natural-looking policy in qualitative terms as the Dog agent appears to perform step-taking motions, rather than remain stationary. Table 2 further showcases a higher preference for the naturalness of the learned policies for continuous locomotion achieved by Video-TADPoLe compared to ViCLIP-RM, in both Dog and Humanoid environments.

In Figure 5, we visualize episode return curves achieved by Video-TADPoLe for a Humanoid agent with the prompt "a person walking". We visualize how during training, the computed Video-TADPoLe reward shares a positive correlation with the reward computed by a ground-truth function for walking over all episodes, lending confidence to it as a coherent, well-defined reward function. We also visualize the ground-truth evaluation curve. Figure A4 offers another example for "a dog walking".

**Environment** & **Prompt** & **LIV** & **Text2Reward** & **Video-TADPoLe (Ours)** & **ViCLIP-RM** & **Ground-Truth** \\  Humanoid & “a person walking” & 0.65 & 3.35 & 145.60 (\(\) 48.20) & 25.51 (\(\) 40.45) & 275.06 (\(\) 9.21) \\ Dog & “a dog walking” & 16.86 & 63.15 & 60.20 (\(\) 8.82) & 14.67 (\(\) 9.84) & 280.07 (\(\) 3.07) \\  Humanoid & “a person walking” & ✗ & ✗ & ✗ & ✗ & ✗ & ✗ \\ Dog & “a dog walking” & ✗ & ✗ & ✗ & ✗ & ✗ & ✗ \\ 

Table 3: Results for continuous locomotion experiments. For rows with an associated ground-truth reward function, numerical results are listed; for performant approaches, we report mean and standard deviation across 5 seeds. Evaluation for text-alignment is also reported. Video-TADPoLe greatly outperforms ViCLIP-RM on both Humanoid and Dog.

### Robotic Manipulation

We further investigate how well TADPoLe can be applied to learn robotic manipulation tasks through dense text-conditioned feedback. We do so by replacing the manually-designed ground-truth dense reward for each Meta-World task with TADPoLe's text-conditioned reward. Since TADPoLe aims to leverage domain-agnostic diffusion models for policy learning, we focus our evaluation to compare with baseline methods that also do not utilize in-domain (expert) demonstrations for the robotic manipulation tasks. We note that most of the prior methods which report performance on Meta-World rely on (often expert-produced) video demonstrations from a similar domain or the target environment directly for representation learning , reward learning , or both . They are thus not directly comparable to TADPoLe.

We perform thorough comparisons between TADPoLe and VLM-RM by evaluating them on a diverse set of selected Meta-World tasks. Both models reuse the setup in Section 4.2 without modification, with training performed for 700k steps. In Table 4, we report the final success rate for each manipulation task averaged over 30 evaluation rollouts. We highlight that TADPoLe achieves high success rates across a variety of tasks, and significantly exceeds VLM-RM in terms of average overall performance. We take this as a positive signal that TADPoLe can meaningfully provide dense text-conditioned rewards that replace dense ground-truth hand-designed feedback. We also highlight how TADPoLe is able to successfully supervise the learning of policies within the synthetic-looking visual environment of Meta-World without finetuning the pretrained text-to-image diffusion model, despite the visual attributes (such as the appearance of the robotic arm, or the quality of the renderings) being quite dissimilar from the style of images StableDiffusion was trained on.

### Normalization Study

Of interest is what reward normalization technique is most performant for adjusting the raw computed alignment and reconstruction terms into a final reward used for policy learning. We investigate a

   Success Rate (\%) & Door Open & Door Close & Drawer Open & Drawer Close & Window Open & Window Close \\  VLM-RM & 0 (\(\) 0) & 79.7 (\(\) 39.8) & 10.0 (\(\) 30.0) & 100.0 (\(\) 0) & 9.7 (\(\)29.0) & 0 (\(\) 0) \\ TADPoLe & 40.0 (\(\) 49.0) & 100.0 (\(\) 0) & 45.3 (\(\) 46.6) & 100.0 (\(\) 0) & 74.0 (\(\) 37.9) & 30.0 (\(\) 45.8) \\  Success Rate (\%) & Coffee Push & Button Press & Soccer & Lever Pull & _Average_ \\  VLM-RM & 4.0 (\(\) 12.0) & 30.0 (\(\) 45.8) & 5.3 (\(\) 11.6) & 11.3 (\(\) 18.3) & 25.0 \\ TADPoLe & 18.6 (\(\) 20.5) & 73.0 (\(\) 38.5) & 25.0 (\(\) 15.4) & 0 (\(\) 0) & **50.6** \\   

Table 4: Average success rate for robotic manipulation tasks in Meta-World  over 30 evaluation rollouts. We compare between TADPoLe and VLM-RM, both approaches that do not utilize in-domain data or demonstrations, and find TADPoLe significantly outperforms VLM-RM. We report mean performance and standard deviation across 10 seeds for each task.

Figure 5: Episode return curves for a Humanoid agent trained with Video-TADPoLe, using the prompt “a person walking”. We observe that the Video-TADPoLe reward signal (left) is positively correlated with the agent’s performance as measured with ground-truth reward during training (middle) and evaluation (right). Shaded regions denote the standard deviation across five random seeds.

variety of normalization strategies in a quantitative manner in Table 5, reusing parameters \(w_{1}=2000\) and \(w_{2}=200\) across experiments, the selection of which is detailed in Section B.3 and Table A4.

Apart from the symlog transformation, we also compare against using no additional normalization (denoted as "Direct Scaling"), and using symexp. We also compare against empirical normalization techniques. This includes min-max rescaling, where an empirically estimated minimum value is subtracted from the achieved reward, which is then divided by an empirically calculated min-max range, and rescaled to \([-1,1]\). This also includes standardization, which subtracts an empirically estimated mean from the achieved reward and then divides it by an empirically estimated standard deviation. We apply these techniques across Humanoid and Dog environments, for both TADPoLe and Video-TADPoLe. We discover that the symlog operation is the reward normalization strategy that achieves the best empirical results across robotic configurations, visual environments, and desired tasks, while reusing the same hyperparameter settings. We hypothesize that it helps to normalize the raw computed reward signals across diffusion models and environments to be roughly on the same scale. Indeed, we showcase how removing the symlog transformation, as well as using other normalization techniques, reduces consistent policy learning performance across environments and diffusion models with the same fixed hyperparameters. We further note that it does not require empirically estimated values, unlike min-max and standardization.

## 5 Conclusion and Future Work

We present Text-Aware Diffusion for Policy Learning (TADPoLe), a framework that optimizes a policy according to a provided natural language prompt through a pretrained text-conditioned diffusion model. TADPoLe enables novel behaviors to be learned in a zero-shot manner purely from text conditioning, and also offers a promising angle to train policies to behave in accordance with natural priors summarized from large-scale pretraining. TADPoLe can be applied across visual environments and different robotic states without modification, and we experimentally demonstrate that TADPoLe is able to learn novel goal-achievement as well as continuous locomotion behaviors conditioned only on text, across Humanoid, Dog, and Meta-World environments.

**Limitations:** An observed limitation of TADPoLe is that it is difficult to explicitly control the weight each individual word of an input text prompt has on the reward provided to the agent. For certain prompts, TADPoLe could potentially cause the agent to remain stationary since it may focus on alignment with the noun in the phrase rather than details of the goal. How to provide fine-grained control over the text-conditioning is an interesting direction to explore in future work. Further interesting future work includes utilizing multiple camera views simultaneously to compute the dense reward, as environments generally allow flexible rendering from arbitrary angles. Another observed limitation is that TADPoLe depends on a highly stochastic operation, namely, repeatedly resampling a Gaussian source noise vector at each timestep. The behavior of the resulting policy, after training for many iterations, can therefore vary for the same input text prompt, and potentially cause high variance in both visual and quantitative performance. How to control the stability of convergence to a consistent policy across repeated runs is an interesting future direction for exploration.

**Acknowledgements.** We would like to thank Amil Merchant, Daniel Ritchie, David Bau, Ben Poole, Ting Chen, Nate Gillman, and Yilun Du for helpful discussions and feedback. This work is supported by Samsung Advanced Institute of Technology, NASA, and a Richard B. Salomon Faculty Research Award for Chen Sun. Our research was conducted using computational resources at the Center for Computation and Visualization at Brown University.

**Environment** & **Method** & **Prompt** & **SymLog** & **Direct Scaling** & **SymExp** & **Min-Max** & **Standardization** \\  Humanoid & TADPoLe & “a person standing\({}^{*}\) & **267.23** & 239.74 & 241.49 & 256.81 & 236.59 \\  Humanoid & Video-TADPoLe & “a person walking\({}^{*}\) & **226.29** & 4.58 & 3.68 & 61.31 & 134.66 \\ Dog & Video-TADPoLe & “a dog walking\({}^{*}\) & **81.22** & 35.30 & 9.46 & 6.15 & 5.05 \\ 

Table 5: Quantitative results for TADPoLe and Video-TADPoLe with and without the symlog normalization operation, averaged over 3 seeds. Hyperparameters such as weights and noise level were kept the same across all experiments. We discover that not only does using the symlog transformation enable the highest empirical rewards, it also facilitates the reuse of hyperparameters across tasks, environments, and other diffusion models.