# Provably Efficient Reinforcement Learning with Multinomial Logit Function Approximation

Long-Fei Li\({}^{1,2}\), Yu-Jie Zhang\({}^{3}\), Peng Zhao\({}^{1,2}\), Zhi-Hua Zhou\({}^{1,2}\)

\({}^{1}\) National Key Laboratory for Novel Software Technology, Nanjing University, China

\({}^{2}\) School of Artificial Intelligence, Nanjing University, China

\({}^{3}\) The University of Tokyo, Chiba, Japan

###### Abstract

We study a new class of MDPs that employs multinomial logit (MNL) function approximation to ensure valid probability distributions over the state space. Despite its significant benefits, incorporating the non-linear function raises substantial challenges in both _statistical_ and _computational_ efficiency. The best-known result of Hwang and Oh (2023) has achieved an \(}(^{-1}dH^{2})\) regret upper bound, where \(\) is a problem-dependent quantity, \(d\) is the feature dimension, \(H\) is the episode length, and \(K\) is the number of episodes. However, we observe that \(^{-1}\) exhibits polynomial dependence on the number of reachable states, which can be as large as the state space size in the worst case and thus undermines the motivation for function approximation. Additionally, their method requires storing all historical data and the time complexity scales linearly with the episode count, which is computationally expensive. In this work, we propose a statistically efficient algorithm that achieves a regret of \(}(dH^{2}+^{-1}d^{2}H^{2})\), eliminating the dependence on \(^{-1}\) in the dominant term for the first time. We then address the computational challenges by introducing an enhanced algorithm that achieves the same regret guarantee but with only constant cost. Finally, we establish the first lower bound for this problem, justifying the optimality of our results in \(d\) and \(K\).

## 1 Introduction

Reinforcement Learning (RL) with function approximation has achieved remarkable success in various applications involving large state and action spaces, such as games (Silver et al., 2016), algorithm discovery (Fawzi et al., 2022) and large language models (Ouyang et al., 2022). Therefore, establishing the theoretical foundation for RL with function approximation is of great importance. Recently, there have been many efforts devoted to understanding the linear function approximation, yielding numerous valuable results (Yang and Wang, 2019; Jin et al., 2020; Ayoub et al., 2020).

While these studies make important steps toward understanding RL with function approximation, there are still challenges to be solved. In linear function approximation, transitions are assumed to be linear in feature mappings, such as \((s^{}|s,a)=(s^{}|s,a)^{}^{*}\) for linear mixture MDPs and \((s^{}|s,a)=(s,a)^{}^{*}(s^{})\) for linear MDPs. Here \((s^{}|s,a)\) is the probability from state \(s\) to \(s^{}\) taking action \(a\), \((s^{}|s,a)\) and \((s,a)\) are feature mappings, \(^{*}\) and \(^{*}(s^{})\) are unknown parameters. However, the transition function is a _probability distribution_ over states, meaning its values must lie within \(\) and sum to 1. Thus, the linearity assumption is restrictive and hard to satisfy in practice. An algorithm designed for linear MDPs could break down entirely if the underlying MDP is not linear (Jin et al., 2020). While some works explore generalized linear (Wang et al., 2021) and general function approximation (Russo and Roy, 2013; Foster et al., 2021; Chen et al., 2023), they focus on function approximation for value functions rather than transitions, hence do not tackle this challenge.

Towards addressing the limitation of linear function approximation, a new class of MDPs that utilizes multinomial logit function approximation has been proposed by Hwang and Oh (2023) recently. Such formula also aligns better with models like neural networks (LeCun et al., 2015), which inherently respect the probabilistic constraints through a softmax layer and allow for greater expressive power. However, though it offers promising benefits, the introduction of non-linear functions introduces significant challenges in both _statistical_ and _computational_ efficiency. Specifically, the best-known approach of Hwang and Oh (2023) has achieved an \(}(^{-1}dH^{2})\) regret, where \(\) is a problem-dependent quantity that measures the effective non-linearity over the entire parameter space, \(d\) is the feature dimension, \(H\) is the episode length, and \(K\) is the number of episodes. Unfortunately, as we show in Claim 1, it holds that \(^{-1}>U^{2}\), where \(U\) denotes the maximum number of reachable states, which equals to the size of the state space \(S\) in the worst case. This undermines the core motivation for function approximation, which aims to mitigate dependence on large state and action spaces. Furthermore, the method requires storing all historical data, and its time complexity _per episode_ grows _linearly_ with the episode count (i.e., \((k)\) at episode \(k\)). Thus, a natural question arises: _Is it possible to design both statistically and computationally efficient algorithms for RL with MNL function approximation?_

In this work, we answer this question affirmatively for the class of MNL mixture MDPs where the transition is parameterized by a multinomial logit function. Our contributions are listed as follows:

* For statistical efficiency, we propose the UCRL-MNL-LL algorithm, which attains a regret bound of \(}(dH^{2}+^{-1}d^{2}H^{2})\). Our result significantly improves upon the \(}(^{-1}dH^{2})\) rate of Hwang and Oh (2023), making the first time to achieve a \(\)-independent dominant term (note that the lower-order term still scales with \(^{-1}\), but does not depend on \(K\), making it acceptable). To achieve this, we propose a tighter confidence set based on a new Bernstein-type concentration (Perivier and Goyal, 2022) instead of the standard Hoeffding-type concentration, and exploit the self-concordant-like property (Bach, 2010) of the log-loss function to better use local information.
* For computational efficiency, we propose the UCRL-MNL-OL algorithm, which enjoys the same regret bound as UCRL-MNL-LL, but with only _constant_ storage and time complexity per episode. This is enabled by recognizing that the negative log-likelihood function is exponentially concave, which motivates the use of online mirror descent with a specifically tailored local norm (Zhang and Sugiyama, 2023) to replace the standard maximum likelihood estimation. Furthermore, we construct the optimistic value function by incorporating a closed-form bonus term through a second-order Taylor expansion, thus avoiding the need to solve a non-convex optimization problem.
* We establish the _first_ lower bound for MNL mixture MDPs by introducing a reduction to the logistic bandit problem. We prove a problem-dependent lower bound of \((dH})\) for infinite action setting, where \(^{*}\) is an another problem-dependent quantity that measures the effective non-linearity over around the ground truth parameter. Though this does not constitute a strict lower bound for the finite action case studied in this work, it suggests that our result may be optimal in \(d\) and \(K\). 1

Table 1 provides a comparison between our work and previous studies, focusing on regret and computational costs, including both storage and time complexity.

  
**Reference** & **Regret** & **Storage** & **Time** & **MDP model** \\  Hwang and Oh (2023) & \(}(^{-1}dH^{2})\) & \((k)\) & \((k)\) & homogeneous \\ UCRL-MNL-LL (Theorem 1) & \(}(dH^{2}+^{-1}d^{2}H^{2})\) & \((k)\) & \((k)\) & inhomogeneous \\ UCRL-MNL-OL (Theorem 2) & \(}(dH^{2}+^{-1}d^{2}H^{2})\) & \((1)\) & \((1)\) & inhomogeneous \\  Lower Bound (Corollary 1) & \((dH})\) & – & – & infinite action space \\   

Table 1: Comparison between previous works and ours in terms of the regret and computational cost (including storage and time complexity). Here \(\) and \(^{*}\) are problem-dependent quantities defined in Assumption 1, \(d\) is the feature dimension, \(H\) is the episode length and \(K\) is the number of episodes. The computational cost per episode only highlight the dependence on the episode count \(k\).

**Organization.** We introduce the related work in Section 2 and present the setup in Section 3. Then, we design a statistically efficient algorithm in Section 4. Next, we present an algorithm that achieves both statistical and computational efficiency in Section 5. Finally, we establish the lower bound in Section 6. Section 7 concludes the paper. Due to space limits, we defer all proofs to the appendixes.

**Notations.** We use \([x]_{[a,b]}\) to denote \(((x,a),b)\). For a vector \(^{d}\) and positive semi-definite matrix \(A^{d d}\), denote \(\|\|_{A}=^{}A}\). For a strictly convex and continuously differentiable function \(:\), the Bregman divergence is defined as \(_{}(_{1},_{2})=( _{1})-(_{2})- (_{2}),_{1}-_{2}\). We use the notation \(()\) to indicate different types of dependencies depending on the context. For regret analysis, \(()\) omits only constant factors. For computational costs, we use \(()\) to solely highlight the dependence on the number of episode as this is the primary factor influencing the complexity. Additionally, we employ \(}()\) to hide all polylogarithmic factors.

## 2 Related Work

In this section, we review related works from both setup and technical perspectives.

**RL with Generalized Linear Function Approximation.** There are recent efforts devoted to investigating function approximation beyond the linear models. Wang et al. (2021) investigated RL with generalized linear function approximation. Notably, unlike our approach that models transitions using a generalized linear model, they apply this approximation directly to the value function. Another line of works Chowdhury et al. (2021); Li et al. (2022); Ouhamma et al. (2023) has studied RL with exponential function approximation and also aimed to ensure that transitions constitute valid probability distributions. The MDP model can be viewed as an extension of bilinear MDPs in their work while our setting extends linear mixture MDPs. These studies are complementary to ours and not directly comparable. Moreover, these works also enter the computational and statistical challenges arising from non-linear function approximation that remain to be addressed. The most relevant work to ours is the recent work by Hwang and Oh Hwang and Oh (2023), which firstly explored a similar setting to ours, where the transition is characterized using a multinomial logit model. We significantly improve upon their results by providing statistically and computationally more efficient algorithms.

**RL with General Function Approximation.** There have also been some works that studies RL with general function approximation. Russo and Roy (2013) and Osband and Roy (2014) initiated the study on the minimal structural assumptions that render sample-efficient learning by proposing a structural condition called Eluder dimension. Recently, several works have investigated different conditions for sample-efficient interactive learning, such as Bellman Eluder (BE) dimension Jin et al. (2021), Bilinear classes Du et al. (2021), Decision-Estimation Coefficient (DEC) Foster et al. (2021), and Admissible Bellman Characterization (ABC) Chen et al. (2023). A notable difference is that they impose assumptions on the value functions while we study function approximation on the transitions to ensure valid probability distributions. Moreover, the goal of these works is to study the conditions for sample-efficient reinforcement learning, but not focus on the computational efficiency.

**Multinomial Logit Bandits.** There are two types of multinomial logit bandits studied in the literature: the single-parameter model, where the parameter is a vector Cheung and Simchi-Levi (2017) and multiple-parameter model, where the parameter is a matrix Amani and Thrampoulidis (2021). We focus on the single-parameter model, which are more relevant to our setting. The pioneering work by Cheung and Simchi-Levi (2017) achieved a Bayesian regret of \(}(^{-1}d)\), where \(T\) denotes the number of rounds in bandits. This result was further enhanced by subsequent studies Oh and Iyengar (2019, 2021); Agrawal et al. (2023). In particular, Perivier and Goyal (2022) significantly improved the dependence on \(\), obtaining a regret of \(}(d+^{-1})\) in the uniform revenue setting. Most prior methods required storing all historical data and faced computational challenge. To address this issue, the most recent work by Lee and Oh (2024) proposed an algorithm with constant computational and storage costs building on recent advances in multiple-parameter model Zhang and Sugiyama (2023). Their algorithm achieves the optimal regret of \(}(d+^{-1})\) and \(}(d+^{-1})\) under uniform and non-uniform rewards respectively. However, although the underlying models of MNL bandits and MDPs share similarities, the challenges they present differ substantially, and techniques developed for MNL bandits cannot be directly applied to MNL MDPs. For example, in MNL bandits, the objective is to select a series of assortments with _varying_ sizes that maximize the expected revenue, whereas in MNL MDPs, the goal is to choose _one_ action at each stage to maximize the cumulative reward. Thus, it is necessary to design new algorithms tailored for MDPs to address these unique challenges.

## 3 Problem Setup

In this section, we present the problem setup of RL with multinomial logit function approximation.

**Inhomogeneous, Episodic MDPs.** An inhomogeneous, episodic MDP instance can be denoted by a tuple \(=(,,H,\{_{h}\}_{h=1}^{H},\{r_{h}\}_{h =1}^{H})\), where \(\) is the state space, \(\) is the action space, \(H\) is the length of each episode, \(_{h}:\) is the transition kernel with \(_{h}(s^{} s,a)\) is being the probability of transferring to state \(s^{}\) from state \(s\) and taking action \(a\) at stage \(h\), \(r_{h}:\) is the deterministic reward function. A policy \(=\{_{h}\}_{h=1}^{H}\) is a collection of mapping \(_{h}\), where each \(_{h}:()\) is a function maps a state \(s\) to distributions over \(\) at stage \(h\). For any policy \(\) and \((s,a)\), we define the action-value function \(Q_{h}^{}\) and value function \(V_{h}^{}\) as follows:

\[Q_{h}^{}(s,a)=[_{h^{}=h}^{H}r_{h^{}}(s _{h^{}},a_{h^{}})s_{h}=s,a_{h}=a], V_{h}^ {}(s)=_{a_{h}( s)}[Q_{h}^{}(s,a)],\]

where the expectation of \(Q_{h}^{}\) is taken over the randomness of the transition \(\) and policy \(\). The optimal value function \(V_{h}^{*}\) and action-value function \(Q_{h}^{*}\) given by \(V_{h}^{*}(s)=_{}V_{h}^{}(s)\) and \(Q_{h}^{*}(s,a)=_{}Q_{h}^{}(s,a)\). For any function \(V:\), we define \([_{h}V](s,a)=_{s^{}_{h}( s,a )}V(s^{})\).

**Learning Protocol.** In the online MDP setting, the learner interacts with the environment without the knowledge of the transition kernel \(\{_{h}\}_{h=1}^{H}\). We assume the reward function \(\{r_{h}\}_{h=1}^{H}\) is deterministic and known to the learner. The interaction proceeds in \(K\) episodes. At the beginning of episode \(k\), the learner chooses a policy \(_{k}=\{_{k,h}\}_{h=1}^{H}\). At each stage \(h[H]\), starting from the initial state \(s_{k,1}\), the learner observes the state \(s_{k,h}\), chooses an action \(a_{k,h}\) sampled from \(_{k,h}( s_{k,h})\), obtains reward \(r_{h}(s_{k,h},a_{k,h})\) and transits to the next state \(s_{k,h+1}_{h}( s_{k,h},a_{k,h})\) for \(h[H]\). The episode ends when \(s_{H+1}\) is reached. The goal of the learner is to minimize regret, defined as

\[(K)=_{k=1}^{K}V_{1}^{*}(s_{k,1})-_{k=1}^{K}V_{1}^{_{k}}( s_{k,1}),\]

which is the difference between the cumulative reward of the optimal policy and the learner's policy.

**Multinomial Logit (MNL) Mixture MDPs.** Although significant advances have been achieved for MDPs with linear function approximation, Hwang and Oh (2023) show that there exists a set of features such that no linear transition model can induce a valid probability distribution, which limits the expressiveness of function approximation. To overcome this limitation, they propose a new class of MDPs with multinomial logit function approximation. However, their work focuses on the _homogeneous_ setting, where the transitions remain the same across all stages (i.e., \(_{1}=...=_{H}\)). In this work, we address the more general _inhomogeneous_ setting, allowing transitions to vary across different stages. We introduce the formal definition of inhomogeneous MNL mixture MDPs below.

**Definition 1** (Reachable States).: For any \((h,s,a)[H]\), we define the "reachable states" as the set of states that can be reached from state \(s\) taking action \(a\) at stage \(h\) within a single transition, i.e., \(_{h,s,a}\{s^{}_{h}(s^{ } s,a)>0\}\). Furthermore, we define \(S_{h,s,a}|_{h,s,a}|\) and denote by \(U_{(h,s,a)}S_{h,s,a}\) the maximum number of reachable states.

**Definition 2** (MNL Mixture MDP).: An MDP instance \(=(,,H,\{_{h}\}_{h=1}^{H},\{r_{h} \}_{h=1}^{H})\) is called an inhomogeneous, episodic \(B\)-bounded MNL mixture MDP if there exist a _known_ feature mapping \((s^{} s,a): ^{d}\) with \(\|(s^{} s,a)\|_{2} 1\) and _unknown_ vectors \(\{_{h}^{*}\}_{h=1}^{H}\) with \(=\{^{d},\|\|_{2} B\}\), such that for all \((s,a,h)[H]\) and \(s^{}_{h,s,a}\), it holds that

\[_{h}(s^{} s,a)= s,a)^{} _{h}^{*})}{_{_{h,s,a}}((  s,a)^{}_{h}^{*})}.\]

**Remark 1**.: This model is consistent with models like neural networks (LeCun et al., 2015), where the feature \(\) is obtained by omitting the final layer, and \(_{h}^{*}\) represents the weights of the last layer. A final softmax layer is then applied to ensure that the output forms a valid probability distribution.

For any \(^{d}\), we define the induced transition as \(p_{s,a}^{s^{}}()= s,a)^{})}{ _{j_{h,s}}(( s,a)^{})}\). We then introduce the following two key problem-dependent quantities \(\) and \(^{*}\) that measure the effective non-linearity over the entire parameter space and around the ground truth parameter respectively.

**Assumption 1**.: There exists \(0<^{*}<1\) such that for all \((s,a,h)[H]\) and \(s^{},s^{}_{h,s,a}\), it holds that \(_{}p_{s,a}^{s^{}}()p_{s,a}^{s^{}}( )\) and \(p_{s,a}^{s^{}}(_{h}^{s})p_{s,a}^{s^{}}(_{h}^{* })^{*}\).

Assumption 1 is similar to the assumption in generalized linear bandit (Filippi et al., 2010) and logistic bandit (Faury et al., 2020; Abeille et al., 2021) to guarantee the Hessian matrix is non-singular.

Finally, we show the claim about the range of the magnitude of \(\) and \(^{*}\).

**Claim 1**.: _It holds that \(1/(U(2B))^{2}^{*} 1/U^{2}\)._

## 4 Statistically Efficient Algorithm

The work of Hwang and Oh (2023) first introduced the MNL mixture MDPs and proposed an algorithm with a regret bound of \(}(^{-1}dH^{2})\). However, as discussed in Claim 1, it follows that \(U^{2}^{-1}(U(2B))^{2}\), which results in the regret bound scaling polynomially with the number of reachable states \(U\). In the worst case, \(U\) can be equal to the size of the state space \(S\), thereby undermining the motivation for function approximation, which aims to mitigate the dependence on the large state and action spaces. In this section, we address this significant issue by proposing a statistically efficient algorithm that eliminates this dependence in the dominant term of the regret.

### Parameter Estimation

In this section, we first present the parameter estimation method based on the maximum likelihood estimation (MLE) for MNL mixture MDPs. Next, we review the confidence set construction based on the estimated parameters from previous work (Hwang and Oh, 2023). Finally, we propose our new confidence set construction and highlight the improvements it offers over the previous approach.

Since the transition parameter \(_{h}^{*}\) is unknown, we need to estimate it using the historical data. At episode \(k\), we collect a trajectory \(\{(s_{k,h},a_{k,h})\}_{h=1}^{H}\), then define the variable: \(y_{k,h}\{0,1\}^{S_{k,h}}\) where \(y_{k,h}^{s^{}}=_{\{s^{}=s_{k,h+1}\}}\) for \(s^{}_{k,h}_{s_{k,h},a_{k,h}}\) and \(S_{k,h}=|_{k,h}|\). We denote by \(p_{k,h}^{s^{}}()=p_{s_{k,h},a_{k,h}}^{s^{}}()\). Then \(y_{k,h}\) is a sample from the following multinomial distribution:

\[y_{k,h}(1,[p_{k,h}^{s_{1}}(^{*}),,p_{k,h}^{s _{S_{k,h}}}(^{*})]),\]

where the parameter \(1\) indicates that \(y_{k,h}\) is a single-trial sample. Furthermore, we define the noise \(_{k,h}^{s^{}}=y_{k,h}^{s^{}}-p_{k,h}^{s^{}}(_{h }^{*})\). It is clear that \(_{k,h}[-1,1]^{S_{k,h}}\), \([_{k,h}]=\) and \(_{s^{}_{k,h}}_{i,h}^{s^{}}=0\).

We estimate the parameter \(_{h}^{*}\) using the MLE and construct the estimator \(_{k,h}\) as follows:

\[_{k,h}=*{arg\,min}_{^{d}} _{k,h}()_{i=1}^{k-1}_{s^{} _{i,h}}-y_{i,h}^{s^{}} p_{i,h}^{s^{}}()+ {_{k}}{2}\|\|_{2}^{2}.\] (1)

where \(_{k}\) is the regularization parameter. Though the MLE estimator \(_{k,h}\) is the same as that of Hwang and Oh (2023), the confidence set is constructed differently. Specifically, define the gradient \(_{k,h}()\) and Hessian matrix \(_{k,h}()\) of the MLE loss by

\[_{k,h}() =_{i=1}^{k-1}_{s^{}_{i,h}}(p_{i,h}^{s^ {}}()-y_{i,h}^{s^{}})_{i,h}^{s^{}}+_{k},\] \[_{k,h}() =_{i=1}^{k-1}_{s^{}_{i,h}}p_{i,h}^{s^ {}}()_{i,h}^{s^{}}(_{i,h}^{s^{}})^{}-_{ i=1}^{k-1}_{s^{}_{i,h}}_{s^{} _{i,h}}p_{i,h}^{s^{}}()p_{i,h}^{s^{}}( )_{i,h}^{s^{}}(_{i,h}^{s^{}})^{}+ _{k}I.\]

Furthermore, we define the feature covariance matrix \(A_{k,h}=^{-1}_{k}I+_{i=1}^{k-1}_{s^{}_ {i,h}}_{i,h}^{s^{}}(_{i,h}^{s^{}})^{}\). By demonstrating \(_{k,h}() A_{k,h},\), Hwang and Oh (2023) construct the confidence set as

\[_{k,h}=-_{k,h}_{A_{k,h }}^{-1}_{k}}.\] (2)

Since the radius of the confidence set depends on \(^{-1}\), the final regret bound also exhibits a dependence on \(^{-1}\). To eliminates this dependence, we construct a \(\)-independent confidence set based onnew Bernstein-like inequalities in Lemma 13, following recent advances in logistic bandits (Faury et al., 2020; Perivier and Goyal, 2022). Specifically, we show the following lemma.

**Lemma 1**.: _For any \((0,1)\), set \(_{k}=d(kH/)\) and define the confidence set as_

\[}_{k,h}=_{ k,h}()-_{k,h}(_{k,h})_{_{k,h}^ {-1}()}(B+3)_{k} }.\] (3)

_Then, we have \([_{h}^{*}}_{k,h}] 1-, k[K],h [H]\)._

**Comparison to prior work.** We compare the confidence sets defined in Eqs. (2) and (3).

For the confidence set in (3), by the self-concordance property of log-loss in Lemma 10, we have:

\[-_{k,h}_{_{k,h}()} (1+3)_{k,h}()-_{k,h}( {}_{k,h})_{_{k,h}^{-1}()}(1+3) _{k}.\]

Then, note that \(_{k,h}() A_{k,h}\) for all \(\), we have

\[-_{k,h}_{A_{k,h}}^{-1/2} -_{k,h}_{_{k,h}()} ^{-1/2}(1+3)(B+3).\] (4)

Thus, compared to the confidence set in Eq. (2) from Hwang and Oh (2023), our confidence set in Eq. (3) provides a strict improvement by at least a factor of \(^{-1/2}\). This improvement is one of the key components to eliminate the dependence on \(^{-1}\) in the dominant term of the final regret bound.

Additionally, we identify a technical issue of Hwang and Oh (2023). Specifically, they bound the confidence set in Eq. (2) using the self-normalized concentration in Lemma 12. However, the noise is not independent, and since \(_{s^{}_{i,h}}_{i,h}^{s^{}}=0\) (due to the learner visiting each stage \(h\) exactly once per episode), it does not satisfy the _zero-mean_ sub-Gaussian condition in Lemma 12. We observe similar oversights in multinomial logit contextual bandits (Oh and Iyengar, 2019, 2021; Agrawal et al., 2023), an issue that, to our knowledge, has not been explicitly addressed in prior work. This issue can be resolved with only slight modifications in constant factors by a new self-normalized concentration with dependent noises in Lemma 1 of Li et al. (2024), a simplified version of Lemma 13.

### Optimistic Value Function Construction

Given the confidence set \(}_{k,h}\), it is natural to follow the principle of "optimism in the face of uncertainty" and construct the optimistic value function. Hwang and Oh (2023) constructed the optimistic value function \(_{k,h}(s,a)\) by adding a closed-form upper confidence bound as follows:

\[_{k,h}(s,a)=r_{h}(s,a)+_{s^{}_{h,s,a} }p_{s,a}^{s^{}}(_{k,h})_{k,h+1}(s^{})+2H _{k}_{s^{}_{h,s,a}}\|_{s,a}^{s^{}}\|_{ A_{k,h}^{-1}}_{[0,H]},\] (5)

where \(_{k,h}(s)=_{a}_{k,h}(s,a)\). Then, a naive idea to compute the optimistic value function is replacing the radius of the confidence set \(_{k}\) with \(_{k}\) and the feature covariance matrix \(A_{k,h}\) with the Hessian matrix \(_{k,h}(_{h}^{*})\). However, there are two issues with this approach. First, the true parameter \(_{h}^{*}\) is unknown thus the Hessian matrix \(_{k,h}(_{h}^{*})\) is not computable in the algorithmic updates. Second, though \(_{k}\) is independent of \(\) and the Hessian matrix \(_{k,h}(_{h}^{*})\) captures local information, the bonus term \(_{s^{}_{h,s,a}}\|_{s,a}^{s^{}}\|_{ _{k,h}^{-1}()}\) remains in a global form. This term involves taking the maximum over all states \(s^{}_{h,s,a}\), which prevents fully utilizing the local information.

To address these challenges, we construct the optimistic value function by directly taking the maximum expected reward over the confidence set. Specifically, we define \(_{k,h}(s,a)\) and \(_{k,h}(s)\) as

\[_{k,h}(s,a)=r_{h}(s,a)+_{}_ {k,h}}_{s^{}_{h,s,a}}p_{s,a}^{s^{}}() _{k,h+1}(s^{})_{[0,H]},_{k,h}(s)=_{a }_{k,h}(s,a).\] (6)

This construction addresses the first challenge by eliminating the need for the Hessian matrix \(_{k,h}(_{h}^{*})\) and directly leveraging the local information embedded in the confidence set \(}_{k,h}\). For the second challenge, although we bypass this issue in the construction of the optimistic value function, we still need to address it in the analysis. To tackle this, we employ a second-order Taylor expansion, in contrast to the first-order expansion used in the analysis of Hwang and Oh (2023). This allows for a more precise capture of local information. Further details are provided in Lemma 7 in the appendix.

### Regret Guarantee

Based on the parameter estimation in Section 4.1 and the construction of the optimistic value function in Section 4.2, we propose the UCRL-MNL-LL algorithm. At each stage \(h\) of episode \(k\), the algorithm observes the current state \(s_{k,h}\) and selects the action that maximizes the value function, i.e., \(a_{k,h}=_{a}_{k,h}(s_{k,h},a)\), and transits to next state \(s_{k,h+1}\). After collecting the trajectory \(\{s_{k,h},a_{k,h}\}_{h=1}^{H}\), the estimator \(_{k+1,h}\) is updated using Eq. (1), and the confidence set \(}_{k+1,h}\) is updated according to Eq. (3). Then, the value function \(_{k+1,h}\) and \(_{k+1,h}\) are updated using Eq. (6). The detailed procedure is outlined in Algorithm 1. We show it achieves the following regret guarantee.

```
1:Regularization parameter \(\), confidence width \(_{k}\), confidence parameter \(\).
2:Initialization: Set \(_{1,h}=,_{1,h}(,)=0,_{1,h}()=0\) for all \(h[H]\).
3:for\(k=1,,K\)do
4:for\(h=1,,H\)do
5: Observe current state \(s_{k,h}\) and select action \(a_{k,h}=_{a}_{k,h}(s_{k,h},a)\).
6:endfor
7: Set \(_{k+1,H+1}()=0\).
8:for\(h=H,,1\)do
9: Compute the estimator \(_{k+1,h}\) by Eq. (1) and update the confidence set \(}_{k+1,h}\) by Eq. (3).
10: Compute \(_{k+1,h}(,)\) and \(_{k+1,h}()\) as in Eq. (6).
11:endfor ```

**Algorithm 1** UCRL-MNL-LL

**Theorem 1**.: _For any \((0,1)\), set \(_{k}=d(kH/)\), and \(_{k}=(B+3)\). With probability at least \(1-\), UCRL-MNL-LL algorithm (Algorithm 1) ensures the following guarantee:_

\[(K)}dH^{2}+^{-1}d^{ 2}H^{2}.\]

**Remark 2**.: Focusing on the dominant term, our guarantee eliminates the problematic dependence on \(^{-1}\), in stark contrast to the \(}(^{-1}dH^{2})\) result of Hwang and Oh (2023). As noted in Claim 1, such undesirable dependence has polynomial scaling with the number of reachable states \(U\), which can be as large as the entire state space \(S\) in the worst case. This renders the guarantee for function approximation--designed for settings with large state and action spaces--essentially vacuous.

## 5 Computationally Efficient Algorithm

While the UCRL-MNL-LL algorithm is the _first_ statistically efficient algorithm for MNL mixture MDPs, it is computationally expensive dur the the optimization of the MLE in Eq. (1) and non-convex optimization in Eq. (6). To address these challenges, we propose a computationally efficient algorithm in this section, which attains the same regret but with constant computational costs per episode.

### Efficient Online Parameter Estimation

In this section, we focus on estimating the unknown parameter \(_{h}^{*}\) in a computationally efficient manner. We first discuss the storage and time complexities of the MLE optimization in Eq. (1). Next, we introduce an efficient online parameter estimation based on online mirror descent that provides similar guarantees to the MLE, but with constant storage and time complexity per episode.

For the storage complexity, the optimization problem defined in Eq. (1) requires storing all historical data, resulting in a storage complexity of \((k)\) at episode \(k\). In terms of time complexity, the problem does not have a closed-form solution and can only be solved to within an \(\)-accuracy, such as using projected gradient descent. As discussed in Faury et al. (2022), optimizing the MLE typically requires \(((1/))\) iterations to achieve an \(\)-accurate solution. Since the loss function is defined over all historical data, each gradient step incurs a query complexity of \((k)\). As \(\) is usually chosen as \(1/k\) for episode \(k\), the total time complexity is \((k k)\) at episode \(k\). Consequently, both storage and time complexities scale linearly with the episode count, which is computationally expensive.

To improve the computational efficiency, the basic idea is to estimate the unknown parameter with the online mirror descent (OMD) update instead of the MLE as defined in Eq. (1). To this end, we first define per-episode loss function \(_{k,h}()\), gradient \(g_{k,h}()\) and Hessian matrix \(H_{k,h}()\) as

\[_{k,h}() =-_{s^{}_{k,h}}y_{k,h}^{s^{}} p_{ k,h}^{s^{}}(), g_{k,h}()=_{k,h}()=_{s^{ }_{k,h}}(p_{k,h}^{s^{}}()-y_{k,h}^{s^{} })_{k,h}^{s^{}}\] (7) \[H_{k,h}() =^{2}_{k,h}()=_{s^{}_{k,h }}p_{k,h}^{s^{}}()_{k,h}^{s^{}}(_{k,h}^{s^{}}) ^{}-_{s^{}_{k,h}}_{s^{} _{k,h}}p_{k,h}^{s^{}}()p_{k,h}^{s^{}}( )_{k,h}^{s^{}}(_{k,h}^{s^{}})^{}.\]

Then, the design of the OMD algorithm can be conceptually divided into two parts: the approximation of the past losses and the approximation of the current loss. We provide the details of each below.

**Approximate the past losses.** To integrate historical information from previous iterations while avoiding the use of MLE in Eq. (1), we construct the estimator \(_{k+1,h}\) using the implicit OMD form:

\[_{k+1,h}=*{arg\,min}_{} _{k,h}()+\|-_{k,h}\|_{_{k,h}}^{2}},\] (8)

where \(\) is a step size and \(}_{k,h}}_{k,h}(_{k+1,h} )=_{i=1}^{k-1}H_{i,h}(_{i+1,h})+_{k}I\). The optimization problem can be decomposed in two terms. The first term is the instantaneous log-loss \(_{k,h}()\), which accounts for the information of the current episode. The second is a regularization term that ensures the current model remains close to the previous one, \(_{k,h}\), thereby incorporating the historical information acquired so far. The most critical aspect in the above is the design of the local norm \(}_{k,h}\), which intentionally approximate the per-episode Hessian matrix by \(H_{i}(_{i+1,h})\) at a _look ahead_ point \(_{i+1,h}\). Such a Hessian matrix, originally introduced by Faury et al. (2022), effectively captures the local curvature of the loss function and is crucial for ensuring statistical efficiency.

The update rule in Eq. (8) is storage efficient, as it only requires storing the Hessian matrix \(}_{k,h}\), which can be updated incrementally, resulting in an \((1)\) storage cost. In terms of time complexity, the optimization problem in Eq. (8) suffers an \(( k)\) time complexity at episode \(k\), since the loss function is defined only over the current episode. While this represents a significant improvement over the \((k k)\) time complexity of the MLE in Eq. (1), there is still a need to reduce the cost further to \((1)\) per episode, particularly given the potentially large number of episodes.

**Approximate the current loss.** To achieve \((1)\) time complexity per episode, we can further approximate the current loss with a second order approximation. Drawing inspiration from Zhang and Sugiyama (2023), we define the second-order approximation of the original loss function \(_{k,h}()\) at \(_{k,h}\) as \(_{k,h}()=_{k,h}(_{k,h})+ _{k,h}(_{k,h}),-_{k,h} +\|-_{k,h}\|_{H_{k,h}(_{k,h})}^{2}\), where \(_{k,h}\) is the current estimate. Then, we can replace \(_{k,h}()\) with its second-order approximation \(_{k,h}()\) in the optimization problem in Eq. (8). This leads to the following approximate optimization problem:

\[_{k+1,h}=*{arg\,min}_{} _{k,h}(_{k,h}),-_{k,h}+\|-_{k,h}\|_{}_{k,h}}^{2}}.\] (9)

where \(\) is the step size, \(}_{k,h}=_{k,h}+ H_{k,h}(_{k,h})\) and \(_{k,h}=_{i=1}^{k-1}H_{i,h}(_{i+1,h})+_{k }I\). Then, Eq. (9) can be solved with a single projected gradient step with the following equivalent formulation:

\[_{k+1,h}^{}=_{k,h}-}_{k,h}^{-1}_{k,h}(_{k,h}), _{k+1,h}=*{arg\,min}_{}\|- _{k+1,h}\|_{}_{k,h}}^{2}.\]

Thus, Eq. (9) is computationally efficient, as it only suffers an \((1)\) storage and time complexity.

Notice that the update rule in Eq. (9) is actually a standard online mirror descent (OMD) formula,

\[_{k+1,h}=*{arg\,min}_{} _{k,h}(_{k,h}),+ _{_{k}}(,_{k,h})}.\] (10)

where the regularizer is \(_{k}()=\|\|_{}_{k,h}}^{2}\) and \(_{_{k}}(,)\) is the induced Bregman divergence. Therefore, we can construct the confidence set building upon the modern analysis of OMD (Orabona, 2019; Zhao et al., 2024). Specifically, we can construct the \(\)-independent confidence set as follows.

**Lemma 2**.: _For any \((0,1)\), set \(=(1+U)+(B+1)\) and \(=84(B+d)\), define_

\[}_{k,h}=\ |\ \|- _{k,h}\|_{_{k,h}}_{k}},\]

_where \(_{k}=( U(kH/))\). Then, we have \([_{h}^{h}}_{k,h}] 1-, k[K],h[H]\)._

**Remark 3**.: Compared to the confidence set in Lemma 1, the radius \(_{k}\) in Lemma 2 includes an additional \( U\) factor. This is due to our approximation of the original MLE using the OMD update.

### Efficient Optimistic Value Function Construction

Although the optimistic value function in Eq. (6) preserves local information effectively and provides strong theoretical guarantees, it is computationally intractable due to the need to solve a non-convex optimization problem. To address this challenge, we propose an efficient method in this section.

The key idea is to use a second-order Taylor expansion to derive a closed-form bonus term, which replaces the operation of taking the maximum over the non-convex confidence set. While this idea has been used in bandit settings, fundamental challenges arise when applying it in the MDP setting. Specifically, Zhang and Sugiyama (2023) studied the multi-parameter MLogB bandit, where each outcome is associated with a distinct parameter vector. In contrast, MNL mixture MDPs involve a single shared parameter vector across all outcomes. This distinction leads to a more complex Hessian matrix, necessitating a more sophisticated analysis. A direct use of their analysis will leads to a polynomial dependence on the number of reachable states \(U\), which is undesirable in the MDP setting. Lee and Oh (2024) focused on the single-parameter MNL bandit, which is more closely related to our setting. However, they construct the optimistic value function by directly taking the maximum over the confidence set, a computationally intractable approach in the MDP setting. As a result, they can apply a second-order Taylor expansion around the ground truth parameter \(_{h}^{*}\) in their analysis, while we must apply it around the estimated parameter \(_{k,h}\) to construct the bonus term explicitly.

For MDPs, we show the value difference arising from the transition estimation error as follows.

**Lemma 3**.: _Suppose Lemma 2 holds. For any \(V:[0,H]\) and \((h,s,a)[H]\), it holds_

\[_{s^{}_{h,s,a}}p_{s,a}^{s^{}}( {}_{k,h})V(s^{})-_{s^{}_{h,s,a}}p_{s,a}^{ s^{}}(_{h}^{*})V(s^{})_{s,a}^{}+_{s,a}^{}.\]

_where_

\[_{s,a}^{}=H_{k}_{s^{} _{h,s,a}}p_{s,a}^{s^{}}(_{k,h}) _{s,a}^{s^{}}-_{s^{}_{h,s,a}}p_{s,a}^ {s^{}}(_{k,h})_{s,a}^{s^{}} _{_{h,h}^{-1}},_{s,a}^{}=H _{k^{}_{h,s,a}}^{2}\|_{s,a}^{s^{ }}\|_{_{h,h}^{-1}}.\]

Based on Lemma 3, we construct the optimistic value function as follows:

\[_{k,h}(s,a)=r_{h}(s,a)+_{s^{} _{h,s,a}}p_{s,a}^{s^{}}(_{k,h}) {V}_{k,h+1}(s^{})+_{s,a}^{}+_{s,a}^{}_{[0,H]},\] (11)

where \(_{k,h}(s)=_{a}_{k,h}(s,a)\). In contrast to the value function in Eq. (5), which incorporates the term \(_{s^{}_{h,s,a}}\|_{s,a}^{s^{}}\|_{_{k,h}^{-1}}\), the refined value function in Eq. (11) replaces it with \(_{s,a}^{}+_{s,a}^{}\). This modification better preserves local information, offering a more accurate estimation error bound.

### Regret Guarantee

The overall algorithm UCRL-MNL-OL is similar to UCRL-MNL-LL, but with the estimator and optimistic value function updated in a computationally efficient manner. The detailed algorithm is presented in Algorithm 2. We provide the guarantee of UCRL-MNL-OL in the following theorem.

**Theorem 2**.: _For any \((0,1)\), set \(_{k}=( U(kH/))\), \(=(1+U)+(B+1)\) and \(=84(B+d)\), with probability at least \(1-\), UCRL-MNL-LL (Algorithm 2) ensures_

\[(K)}dH^{2}+^{-1}d^{ 2}H^{2}.\]

**Remark 4**.: UCRL-MNL-OL attains the same regret as UCRL-MNL-LL, but with constant computational cost per episode. This is achieved by constructing an efficient online estimation based on OMD and an optimistic value function by closed-form bonus instead of the non-convex optimization.

## 6 Lower Bound

In this section, we establish the lower bound for MNL mixture MDPs by presenting a novel reduction, which connects MNL mixture MDPs and the logistic bandit problem.

Consider the following logistic bandit problem (Faury et al., 2020): at each round \(t[T]\), the learner selects an action \(x_{t}\) and receives a reward \(r_{t}\) sampled from Bernoulli distribution with mean \((x^{}^{*})=(1+(-x^{}^{*}))^{-1}\), where \(^{*}\{^{d},\|\|_{2} B\}\) is the unknown parameter. The learner aims to to minimize the regret: \(^{}(T)=_{x X}_{t=1}^{T}(x^{}^ {*})-_{t=1}^{T}(x_{t}^{}^{*})\).

**Theorem 3**.: _For any logistic bandit problem \(\), there exists an MNL mixture MDP \(\) such that learning \(\) is as hard as learning \(H/2\) independent instances of \(\) simultaneously._

**Corollary 1** (Lower Bound).: For any problem instance \(\{^{*}_{h}\}_{h=1}^{H}\) and for \(K d^{2}^{*}\), there exists an MNL mixture MDP with _infinite_ action space such that \((K)(dH})\).

**Remark 5**.: Corollary 1 also implies a problem-independent lower bound of \((dH)\) directly. Corollary 1 can be proved by combining Theorem 3 and the \((d})\) lower bound for logistic bandits with _infinite_ arms by Abeille et al. (2021). To the best of our knowledge, a lower bound for logistic bandits with finite arms has not been established, which is beyond the scope of this work. This absence leaves the lower bound for MNL mixture MDPs with a finite action space open through this reduction. However, after the submission of our work to arXiv (Li et al., 2024), a follow up work by Park et al. (2024) proposed a new reduction that bridges MNL mixture MDPs with linear mixture MDPs by approximating MNL functions to linear functions. Leveraging this new reduction, they established a problem-independent \((dH^{3/2})\) lower bound for the finite action setting. This achievement confirms that our result is optimal in \(d\) and \(K\), only loosing by an \((H^{1/2})\) factor.

**Dependence on \(H\)**.: By the discussion in Remark 5, we note that our result is optimal with respect to \(d\) and \(K\), but loosing by an \((H^{1/2})\) factor. We discuss the challenges in improving the dependence on \(H\). Notably, MNL mixture MDPs can be viewed as a generalization of linear mixture MDPs (Ayoub et al., 2020; Zhou et al., 2021). The pioneering work by Ayoub et al. (2020) achieved a regret bound of \(}(dH^{2})\) for linear mixture MDPs, which matches our results in Theorem 1, differing only on the lower-order term. Later, Zhou et al. (2021) enhanced the dependence on \(H\) and attained an optimal regret bound of \(}(dK})\). This was made possible by recognizing that the value function in linear mixture MDPs is linear, allowing for direct learning of the value function while incorporating _variance information_. In contrast, the value function for MNL mixture MDPs does not conform to a specific structure, posing a significant challenge in using the variance information of value functions. Thus, it remains open whether similar improvements on \(H\) are attainable for MNL mixture MDPs.

## 7 Conclusion and Future Work

In this work, we addressing both statistical and computational challenges for MNL mixture MDPs, which leverage MNL function approximation to ensure valid probability distributions. Specifically, we propose a statistically efficient algorithm that achieve a regret of \(}(dH^{2}+^{-1}d^{2}H^{2})\), eliminating the dependence on \(^{-1}\) in the dominant term for the first time. Then, we introduce a computationally enhanced algorithm that achieves the same regret but with only constant cost. Finally, we establish the first lower bound for this problem, justifying the optimality of our results in \(d\) and \(K\).

There are several interesting directions for future work. First, there still exists a gap between the upper and lower bounds. How to close this gap remains an open problem. Besides, we focuses on stationary rewards in this work, extending MNL mixture MDPs to the non-stationary settings and studying the dynamic regret (Wei and Luo, 2021; Zhao et al., 2022; Li et al., 2023) is also an important direction.