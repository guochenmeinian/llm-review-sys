# A theoretical case-study of Scalable Oversight in Hierarchical Reinforcement Learning

Tom Yan, Zachary Lipton

Machine Learning Department

Carnegie Mellon University

Pittsburgh, PA 15213

{tyyan, zlipton}@cmu.edu

###### Abstract

A key source of complexity in next-generation AI models is the size of model outputs, making it time-consuming to parse and provide reliable feedback on. To ensure such models are aligned, we will need to bolster our understanding of scalable oversight and how to scale up human feedback. To this end, we study the challenges of scalable oversight in the context of goal-conditioned hierarchical reinforcement learning. Hierarchical structure is a promising entrypoint into studying how to scale up human feedback, which in this work we assume can only be provided for model outputs below a threshold size. In the cardinal feedback setting, we develop an apt sub-MDP reward and algorithm that allows us to acquire and scale up low-level feedback for learning with sublinear regret. In the ordinal feedback setting, we show the necessity of both high- and low-level feedback, and develop a hierarchical experimental design algorithm that efficiently acquires both types of feedback for learning. Altogether, our work aims to consolidate the foundations of scalable oversight, formalizing and studying the various challenges thereof.

## 1 Introduction

Next-generation AI models are poised to produce sophisticated outputs such as long-form texts and videos, and execute complex tasks as agents. To build these AIs responsibly, we need to better our understanding of scalable oversight: the ability to provide _scalable_ human feedback to these complex models [2; 8; 15; 5]. An immediate, key challenge to overcome is the size of model outputs, making it time-consuming for humans to parse and provide reliable feedback on, even with AI-assistance [24; 27; 23]. To this end, in this work, we consider human labelers with bounded processing ability such that accurate feedback can only be provided for outputs below some threshold size. We are interested in answering the question:

How can we scale this limited feedback to supervise a model with outputs _larger_ than this limit?

Verily, this task is difficult without further assumptions. If the model output can only be assessed in its entirety, it is impossible for humans to provide reliable feedback. Thus, we investigate a natural setup that gives us hope to overcome the limitation in feedback -- when model outputs have _hierarchical_ structure. Hierarchical structure exists in many high-dimensional outputs of interest, including long-form texts (books made up of chapters), videos (movies made up of scenes) and code (main functions made up of helper functions). Indeed, it reflects the way we humans produce many of our most complex creations.

To formalize the setting, we study scalable oversight in a goal-conditioned hierarchical reinforcement learning (HRL) setup. Goal-oriented RL is a popular approach that has seen sizable success in leveraging state space structure to overcome sparse rewards over long horizons [16; 17; 10]. Our aim in this paper differs in using this as an entry-point into understanding how to scale up bounded human feedback, and formalizing the conceptual/technical challenges thereof. It turns out that one known advantage of HRL, besides more efficient exploration and efficient credit assignment, is the ability to enable scalable oversight.

### Preliminaries

We consider a finite-horizon, Markov Decision Process (MDP) \(= S,A,P,r,s_{1},H\), with finite state space \(S\), finite action space \(A\), transition probability \(P:S A(S)\), reward \(r(s,a):S A\) and finite horizon \(H\). The learner interacts with \(\) starting at state \(s_{1}\) and the episode ends after \(H=H_{h}H_{l}\) time-steps. In this work, policies are trained using human feedback. And so, we assume that a human supervisor is needed to evaluate and provide reward \(r\) for trajectories \(,P\) generated by policy \(:S A\).

**Accompanying Example:** Consider the task of learning to generate a long-form, argumentation essay. Providing feedback to an end-to-end policy is difficult as labelers would have to read through entire essays to rate the outputs, after which it may be difficult still to assign a single rating to the entire essay. A tractable alternative is to learn a hierarchical model, with a higher-level policy that generates the essay arguments (goals), and lower-level policies that flesh out these points (realize these goals). It would then be easier for the labeler to rate the shorter-length essay content, and also individual fleshed out arguments, in order to generate a rating on the whole. This approach also mirrors existing rubrics for scoring essays .

**Bounded Feedback:** To formalize the difficulty of human supervisors assessing long-form outputs, we assume that reliable feedback can only be provided for trajectories of length at most \((H_{h},H_{l})\). In particular, this means that for the global policy \(:S A\), it is infeasible to obtain reliable feedback for its trajectory \(,P\), as \(||=H_{h}H_{l}\). This thus motivates hierarchical learning, which makes possible the acquisition of reliable feedback in spite of bounded human supervision.

#### 1.1.1 Goal-conditioned HRL

Since we are unable to learn a single, monolithic policy, our goal instead will be to learn a set of smaller policies that make up a hierarchical policy. This set consists of a high-level policy \(^{h}:S(A_{h})\) (outputs a high-level action \(a^{h}\) at state \(s S\)), and a set of low-/sub-policies \(^{l}_{s,a^{h}}\): \(S^{l}_{s,a^{h}}(A)\), where \(S^{l}_{s,a^{h}} S\) is the set of all states reachable from \(s\) after \(H_{l}\) steps.

In a nutshell, the high-level policy designates goals by choosing high level actions. The low-level policies then aim to realize these goals, while also trying to achieve a high intermediate return. Importantly, both such policies act over a shorter horizon of at most \((H_{h},H_{l})\), making it amenable for human supervisors to evaluate.

**Goal Function:** in the goal-conditioned HRL setting, we assume access to a function \(g\) mapping high-level action \(a^{h}\) at state \(s\) to a goal-state \(g(s,a^{h}) S^{l}_{s,a^{h}}\). For example, \(s\) is the current content of the essay, \(a^{h}\) is the action (in natural language) "add an argument using X" and \(g(s,a^{h})\) is the content of the essay with the "argument using X" included.

**Goal-conditioned sub-MDP:** Given a high level action \(a^{h}\) at state \(s\), this defines the sub-MDP \(M(s,a^{h})\), which has state space \(S^{l}_{s,a^{h}} S\), action space \(A\) (action space of the original \(\)), transition probabilities \(P\) restricted to \(S^{l}_{s,a^{h}}\), starting state \(s\) and finite horizon \(H^{l}\). The sub-MDP reward \(r^{l}\) will be defined later and as we will see, an apt choice is important for achieving sublinear regret.

**High-level MDP:** Given a set of low-level policies, \(^{h}\) may be thought of as operating over a high-level MDP with state space \(S\), action space \(A^{h}\), starting state \(s_{1}\) and finite horizon \(H^{h}\). Importantly, the high-level transition \(P^{}\) of this MDP is a function of the current set of low-level policies \(^{}(s^{}|s,a^{h})=(s^{_{s,a^{h}}}_{H_{l}}=s^{})\), which denotes the distribution over the (final) \(H_{l}\)th state that \(^{l}_{s,a^{h}}\) reaches. Furthermore, the high-level reward \(r^{h}(s,a^{h})=_{s_{j},a_{j}_{s,a^{h}},P}[_{j=1}^{H_{l}}r( s_{j},a_{j})|s_{1}=s]\)corresponds to the intermediate return of sub-policy \(_{s,a^{h}}\) in \(M(s,a^{h})\). Altogether, this gives rise to a key complication in hierarchical learning. This is that both the transitions and rewards in the high-level MDP are non-stationary, as sub-policies \(_{s,a^{h}}\) are updated over time.

**Interaction Protocol:** At each time-step \(t\), the high level policy chooses a high level action \(a_{t}\) based on current state \(s_{t}\). This defines the sub-goal state \(g(s_{t},a_{t})\), along with the corresponding sub-MDP \(M(s_{t},a_{t})\) with finite-horizon \(H_{l}\), in which sub-policy \(^{l}_{s_{t},a_{t}}\) is used to try to achieve the goal. The overall return of the high level policy \(^{h}\) and low-level policies \(\{^{l}_{s,a}\}_{s,a S A^{h}}\) is the sum of intermediate returns \(r(^{l}_{s_{t},a_{t}})\) incurred:

\[V^{^{h},^{l}}(s_{1})=_{a_{t}^{h}(s_{t}),s_{t+1} (s^{^{l}_{s_{t},a_{t}}}_{H_{l}})}[_{t=1}^{H_{h}}r(^{l}_{s_{t},a_{ t}})|s_{t=1}=s_{1}].\]

**Instantiation in the example:** returning to our example, for a cogent essay, the arguments need to be logically related and built on top of each other. This results in a sequential decision making problem corresponding to the one solved by the high level policy \(^{h}\). Given an argument \(g(s,a^{h})\) to flesh out, the low level policy \(^{l}_{s,a^{h}}\) generates up to \(H_{l}\) words, whose content aims to realize this argument. Additionally, low-level policies can incur intermediate rewards (return) for eloquent dich and clear structure when fleshing out the argument, all of which add to the essay's persuasiveness.

#### 1.1.2 Learning Task

Our aim is to learn a hierarchical policy, whose return is close to that of the optimal, goal-reaching hierarchical policy, which we define as follows. For brevity, from this point on, we will use \(a^{h}\) and \(a\) interchangeably to denote high level action.

**Assumption 1** (Goal-Reachability).: _In every sub-MDP \(M(s,a)\), there exists a policy that achieves the goal \(g(s,a)\) almost surely. That is,there exists at least one policy \(_{s,a}\) in the policy class \(_{s,a}\) such that \((s^{}_{H_{l}}=g(s,a))=1\)._

In other words, we assume that the goal function \(g\) is well-defined in that it designates goals that are feasible to reach from the starting state \(s\) (e.g. the argument can be successfully fleshed out in \(H_{l}\) words or less given the essay content thus far). To motivate this assumption, we note there that there are already many settings of interest, where we have prior knowledge of a good goal function. This is because we humans have often (and successfully) taken the hierarchical approach to build up to and produce these long-form creations. So we know what are good goals to set e.g. we write essays by first writing an outline of arguments, then expanding out each point in the outline. Indeed, this approach of explicitly encoding prior knowledge in the hierarchical learning algorithm has been done in both HRL literature (e.g. we know apriori mazes has hierarchical structure in that it consists of rooms ) and scalable oversight literature (e.g. we know that books consists of chapters ).

With this assumption, there exists constant \(C\) large enough such that if \(*{arg\,max}_{_{s,a}}r()+C(s^{}_{H_{l }}=g(s,a))\), then \(\) is goal-reaching and \((s^{}_{H_{l}}=g(s,a))=1\).

**Definition 1**.: _Define optimal low-level policies as \(^{*}_{s,a}*{arg\,max}_{_{s,a}}r()+C(s^{ }_{H_{l}}=g(s,a))\). Define optimal high-level policy as \(^{*}=*{arg\,max}_{^{h}}V^{,^{*}_{s,a}}(s_{1})\)._

In words, \(^{*}_{s,a}\) has the highest intermediate return of all goal-reaching policies. Now let \(^{*}\) be the optimal high-level policy fixing each sub-MDP policy to be \(^{*}_{s,a}\).

**Learning Goal:** We wish to learn a set of near-optimal high- and low-level polices \((,\{_{s,a}\})\) such that: \(V^{^{*},^{*}_{s,a}}(s_{1})-V^{,_{s,a}}\).

### Takeaways

The broad takeaway from this paper is that hierarchical structure, if it exists, can be provably used to scale up limited human supervision. That is:

Hierarchical learning can enable scalable oversight.

On a more technical level, this paper studies the challenge of training a set of (instead of a single) policies that work together to form the hierarchical policy. This is the more complicated problem we turn to solve when it is not feasible to train a monolithic policy, due to bounded human supervision. We thus consider learning in the goal-conditioned HRL setup, under both cardinal and ordinal feedback. A key insight that applies in both settings is that an apt sub-MDP reward design (a suitable penalty for non-goal reachability) is needed for bounding regret and controlling the exit state of learned low-level policies. This is so that learned sub-policies do not land at bad states with sizable probability. Doing so would then allow one to compose low-level policies together, and stabilize high-level policy learning in the high-level MDP. More specific takeaways for both types of feedback are as follows:

* Under cardinal feedback, we develop a novel no-regret learning, Algorithm 1, that jointly learns a high-level and a set of low-level policies. Notably, Algorithm 1 only requires low-level feedback. Our main structural result in this setting is that hierarchical RL reduces to multi-task, sub-MDP regret minimization. Thus, the regret from the low-level accumulates additively (instead of say multiplicatively) as speculated upon in .
* Under ordinal feedback, we develop a novel hierarchical experiment-design Algorithm 2, building off of existing work on experiment design in preferences-based RL . A key observation is that in the ordinal case, low-level feedback may not be sufficient and high-level feedback may be needed. This introduces complications in human supervision, as the high-level feedback would need to account for the _current_ performance of sub-policies. To this end, we study two natural forms of feedback, requiring differing cognitive loads on the human supervisor. Through the experiment design algorithm we develop, we then analyze the differing sample complexity under the two types of feedback. Finally, we show that high-level feedback should not be used if low-level feedback is sufficient and one form of feedback, with higher cognitive load, leads to better sample complexity.

## 2 Related Works

**HRL under cardinal rewards:** There has been sizable interest in understanding of the sample complexity of HRL algorithms, which to our knowledge has thus focused on learning from cardinal rewards. On this subject, the two closest papers to that of ours are  and .  studies goal-conditioned HRL with the key result being a sample complexity lower bound associated with a given hierarchical decomposition. On the upper-bound side, an algorithm (SHQL) is presented, albeit without theoretical guarantees. By contrast, our work presents a learning algorithm with provable guarantees, and further shows that learning in goal-conditioned HRL reduces to multi-task, sub-MDP regret minimization.

 studies HRL under the options framework, providing a model-based, Bayesian algorithm with access to a prior distribution over MDPs that is updated over time. It does not adaptively learn sub-policies based on observed returns, computing instead an option for every exit-profile and equivalence class at each time during model-based planning. By contrast, our work does not assume knowledge of the prior nor ability to update posteriors, and does adaptively explore sub-MDPs via the UCB principle. Additionally,  demonstrate that when the size of the set of exit ("bottleneck") states is small, learning is efficient. Our work shed further light on this insight by showing that under a suitable sub-MDP reward, we can induce a small set of exit states _with high probability_. Thus, even though the total number of possible exit-states may be high, this condition is sufficient for learning with sublinear-regret.

**RL under ordinal rewards:** There has also been considerable interest in bandits/RL from preferences [26; 28; 18; 14; 30; 29]. Following the demonstrated success of RLHF [9; 31; 19; 4], there has been great interest in studying offline RL from preference feedback, and particularly experiment design for enhanced sample efficiency [30; 29]. Due to the success of RLHF in alignment, we also consider studying scalable oversight in this setup. Please see the Appendix A for further discussions on scalable oversight and goal-conditioned RL.

Learning from Cardinal Feedback

We begin by considering the setting when feedback is in the form of cardinal rewards. As noted before, in HRL, the high-level policy performance is dependent on the low-level policies performance. Thus, a naive approach is to learn near-optimal sub-policies in every sub-MDP \(M(s,a)\), and then learn a high-level policy on top. However, a more sample efficient approach is to strategically explore sub-MDPs, and discover sub-policies with high intermediate returns in tandem with a high level policy that visits these "good" sub-MDPs. Please see the Appendix C for all the proofs. Note that in what follows, for brevity, theoretical statements will contain the phrase "with high probability" and the appendix will contain proofs that formalize this guarantee.

### Sub-MDP reward design for Hier-UCB-VI

We are interested in adaptively learning the necessary sub-policies (the useful goals to achieve) and the associated high level policy that invokes these sub-policies. It is natural then to adopt an upper confidence bound approach and construct an exploration bonus that tracks the best/unexplored sub-MDPs. To this end, we develop an adaptation of the classic UCB-VI algorithm . We highlight two key ingredients needed to construct the Hier-UCB-VI Algorithm 1.

**Tradeoffs in sub-MDP reward design:** Learned sub-policies in HRL have to tradeoff between two objectives. One is high intermediate returns \(r(_{s,a})\). The other is that exit-state; sub-policies should not land at "bad" states, as even if the intermediate return is high, \(V(s^{_{s,a}}_{H_{i}}) 0\) means the return from hereon out (and hence the overall return) will be low. Thus, in sub-policy learning, we also need to consider the goodness of the exit-state. But how can we incentivize sub-policies to land at "good" states without being able to calculate \(V\)? Luckily, in the goal-conditioned setting, there is a natural answer for a "good" exit-state: \(g(s,a)\).

To operationalize this, we design a sub-MDP reward that trades-off between intermediate sub-MDP return and goal-reachability. In sub-MDP \(M(s,a)\), at time-step \(h\), sub-MDP reward \(r_{l,h}(s^{},a^{})=r(s^{},a^{})+(h=H_ {l} s^{}=g(s,a))\). Crucially, here we set the weighting \(=(2H_{h}H_{l},C)\), which corresponds to an upper bound on the regret should we not reach the goal-state.

**UCB construction:** Next, we wish to obtain an UCB for \(r(^{*}_{s,a})\). Our main observation is that by using a no-regret subroutine for learning in \(M(s,a)\), the regret guarantee directly translates to a UCB. Due to our choice of sub-MDP reward \(r_{l}\), the UCB includes a penalty on non-goal reachability.

**Lemma 1** (UCB implied by sub-MDP regret).: _Let \(UB(^{n}(s,a))\) be an upper bound on sub-MDP \(M(s,a)\)'s cumulative regret after \(n\) rounds. Define \(=(+H_{l})2((S,A)|H_{l}K}{})\) and bonus,_

\[b^{s,a}_{r}(n)=^{n}(s,a))+}{n}- {n}_{i=1}^{n}(s^{^{*}_{s,a}}_{H_{i}} g(s^{h},a^{h})).\]

_Then, the average reward plus bonus \(_{n}(s,a)+b^{s,a}_{r}(n)\) is an UCB for \(r(^{*}_{s,a})\) with high probability._

**High-level MDP transition stabilization:** An additional benefit of incentivizing goal-reachability is that we know the idealized transition probability in the high-level MDP. As mentioned before, another key difficulty with HRL is that the empirically estimated transitions in the high-level MDP drifts over time. In our algorithm, the key stabilization approach is to avoid estimation and set the transition in the upper bound \(Q_{i}\) to be the idealized transition (\(g(s,a)\) w.p. \(1\)). This allows us to prove our regret guarantee as described below.

### Regret Analysis of Hier-UCB-VI

We start with a definition on clusters of equivalent sub-MDPs. Let there be \((S,A^{h})\) such clusters. In the most general setting, it is not known apriori if there is any shared structure, in which case each sub-MDP will simply be its own cluster.

**Definition 2** (Equivalent sub-MDPs ).: _Two subMDPs \(M(s,a)\) and \(M(s^{},a^{})\) are equivalent if there is a bijection \(\) between state space, and through \(\), the subMDPs have the same transition probabilities and rewards._

Our main structural result is that HRL regret decomposes to multi-task, sub-MDP regret in the cardinal reward setting. This has the implication that only low-level feedback is needed for regret minimization in the cardinal reward case, which as we will see in the ordinal reward case will not always be true.

**Theorem 1** (HRL regret minimization reduces to multi-task, sub-MDP regret minimization).: _Let \(UB(^{N^{K,H_{h}}(s,a)})\) be an upper bound on sub-MDP \(M(s,a)\)'s cumulative regret over \(N^{K,H_{h}}(s,a)\) visits:_

\[_{k=1}^{K}V_{1}^{^{*}}(s_{1})-V_{1}^{^{k}}(s_{1})( _{s,a(S,A^{k})}UB(^{N^{K,H_{h}}(s,a)})+H^{h}H^{l} }(s,a)})\] (2)

Proof Sketch.: We describe the key regret decomposition. After some manipulation, the regret may decompose into the following form, \(_{k=1}^{K}V_{1}^{k}(s_{1})-V_{1}^{^{k}}(s_{1})_{k=1}^{K}_{h =1}^{H_{h}}_{h}^{k}+_{h}^{k}+_{h}^{k}+_{h}^{k}\), which may be parsed as follows.

\(_{h}^{k}=UB(r^{^{*}}(s,a))-r(_{s_{h}^{k},a_{h}^{k}}^{N^{k,h}})\) captures the regret due to sub-optimal intermediate return, the return of \(_{S,a}^{*}\) versus the return of \(_{s_{h},a_{h}^{k}}^{*}\).

\(_{h}^{k}=(P_{h}-P^{_{k,h}})V_{h+1}^{*^{*}}(s_{h}^{k},a_{h}^{k})\), \(_{h}^{k}=(P_{h}-P^{_{k,h}})(V_{h+1}^{k}-V_{h+1}^{^{*}})(s_{h}^{k},a_{h}^{k})\) captures the regret due to sub-optimal policies missing goal-reachability. Here \(P_{h}\) is the idealized transition (goal-reaching), while \(P^{_{k,h}}\) is the transition induced by the current sub-policy.

\(_{h}^{k}=P^{_{k,h}}(V_{h+1}^{k}-V_{h+1}^{_{k}})(s_{h}^{k},a_{h}^{k })-(V_{h+1}^{k}-V_{h+1}^{_{k}})(s_{h+1}^{k})\) is a martingale difference that concentrates via Azuma Hoeffding, and is dominated by the previous three sums.

Focusing on \(_{h=1}^{H_{h}}_{h}^{k}+_{h}^{k}+_{h}^{k}+_{h}^{k}\), we observe that \(_{h}^{k},_{h}^{k} 2H_{h}H_{l}P^{_{h,h}}(s_{h+1}^{k} g(s_{h}^ {k},a_{h}^{k}))\). The key remaining step is to recognize that \(_{h}^{k}+_{h}^{k}+_{h}^{k}\) resembles the instantaneous regret in \(M(s_{h}^{k},a_{h}^{k})\), and the result follows after some further bounding and rearrangement.

For a concrete bound, we note that if \(\) is set as the classic UCB-VI algorithm, then we attain the usual \(()\) regret. Furthermore, we note that our bound is flexible in that one can choose more specialized learning algorithms \(\) to leverage prior knowledge. For instance, if it is known that sub-MDPs are linear, one may choose to invoke multi-task RL algorithms that offer more refined regret bounds for \(UB(^{N^{K,H_{k}}(s,a)})\).

**Goal Selection:** An astute reader will note that the return of the learned hierarchical policy is close to \(V_{1}^{*}(s_{1})\), the return of the optimal hierarchical policy under _goal function_\(g\). In other words, our learned policy is only as good as the goal function \(g\) we choose.

One way to relax the assumption that we have a good goal function \(g\) is to assume we have access to multiple goal functions to choose from: \(g^{1},..,g^{n}\). Then, an useful corollary of the sublinear Hier-UCB-VI regret bound, \([_{k=1}^{K}V_{1}^{g^{i},*}(s_{1})-V_{1}^{g^{i},*^{k}}(s_{1})] ()\), is that it directly implies an UCB on \(V_{1}^{g^{i},*}(s_{1})\) (optimal return under goal \(g^{i}\)). Hence, we may apply any UCB-based bandit algorithm on top of this to compete with the return of the best goal out of all the candidates \(\{g^{j}\}_{j[n]}\).

## 4 Learning from Preference Feedback

In the previous section, we develop an algorithm to efficiently learn a hierarchical policy, purely from low-level, cardinal feedback. Now, we consider learning from ordinal (preferences) feedback. Our first observation is that the low-level feedback is no longer sufficient for learning a good policy.

**Proposition 1** (Non-identifiability of ranking among sub-MDP returns).: _For any deterministic high-level policy learning algorithm with \(N_{l}\) samples of low-level feedback, there exists a MDP instance that induces regret constant in \(N_{l}\)._

The intuition for this is simply that low-level, ordinal feedback can only identify rankings of low-level policies specific to a goal (sub-MDP), but not necessarily low level policies _across_ differing goals. Thus, no matter how large the low-level sample-size \(N_{l}\), the regret is non-vanishing in \(N_{l}\) and hence high-level feedback may be needed to learn. Please see Appendix D for all proofs of results in this section.

### Labeler Feedback and Consequences for Reward Modeling

The canonical approach to learning from preferences is reward modeling. Following previous works, we study offline experiment design and assume we have the ability to collect comparison feedback data, in our hierarchical setting both high and low-level data that are then used to learn the reward model . For tractable analysis, we consider the commonly studied linear reward setup [21; 20; 30; 29].

**Assumption 2** (Linear Reward Parametrization).: _Suppose we have access to some feature map \(:S A^{d}\), \(\) has linear reward parametrization w.r.t. \(\) if there exists an unknown, reward vector \(^{*}^{d}\) such that \(r(s,a)=(s,a),^{*}\) for all \(s,a S A\)._

Given trajectory \(=(s_{1},a_{1},...,s_{H},a_{H})\), we may then define trajectory feature \(()=_{s_{i},a_{i}}(s_{i},a_{i})\), and policy feature expectation under transitions \(P\), \(^{P}()=_{,P}[()]\).

With known feature map \(\) and unknown reward parameter \(^{*}\), the preference feedback \(o_{t}\) follows the Bradley-Terry-Luce (BTL) model .

**Assumption 3**.: _For trajectories \(_{1},_{2}\): \((_{1}_{2})=((^{*})^{T}((_{1})-(_{ 2}))).\)_

With the definitions out of the way, we now describe a _conceptual challenge_ that we encounter when learning from high-level feedback, which as we have shown before may be necessary for learning.

What can we assume about the high-level labeler's knowledge?

Consider a high level trajectory \(_{j}=\{(s_{i}^{j},a_{i}^{j})\}_{i=1}^{H_{k}}\). \((_{j})=_{i[H_{k}]}(s_{i}^{j},a_{i}^{j})\); the key difficulty is that sub-MDP feature expectation \((s_{i}^{j},a_{i}^{j})\) is dependent on the sub-policy deployed in \(M(s_{i}^{j},a_{i}^{j})\). Thus, the high level labeler will have to have in mind some sub-policy \(_{s,a}\), when making the comparison. We study two natural types of feedback:1. **Comparisons based on current sub-policy execution:** It is natural to first assume that the labeler envisions \((s_{i}^{j},a_{i}^{j})=(_{s_{i}^{j},a_{i}^{j}}^{t})\) at time \(t\). In words, it is equivalent to asking: "How well does the high level policy do given _current execution_ of sub-goals?" _Current-feedback_ of this form has the caveat that the labeler will have know about the performance of the current set of sub-policies \(_{s,a}^{t}\) (potentially through AI-assisted means). This knowledge would need to be updated over time as low-level policies \(_{s,a}^{t}\) improve, which introduces a sizable cognitive load.
2. **Comparisons based on idealized sub-policy execution:** To reduce the cognitive load on the labeler, it is natural to fix the sub-policies used in the comparisons. A natural choice then is for the labeler to envision \((s_{i}^{j},a_{i}^{j})=(_{s_{i}^{j},a_{i}^{j}}^{t})\). In words, it is equivalent to asking: "How well does the high level policy do given _perfect execution_ of the sub-goals?" Instantiated in some examples, this would be: "how good is the essay if each argument is fleshed out perfectly" or "how good is the code if each helper function is implemented perfectly". _Idealized-feedback_ of this form has the caveat that the high-level feedback will be a mismatch of how the current sub-policies actually execute. Although it has the advantage that the labeler is no longer required to (somehow) keep track of low-level sub-policies, thus reducing the cognitive load.

In what follows, we consider both types of feedback, showing that learning from idealized-feedback is possible. As we note, a drawback of idealized-feedback is that it is biased with respect to the realized features (since these are generated under current policies \(_{s,a}^{t}\)), while current-feedback is unbiased. We present an upper bound on the bias below.

**Lemma 2** (Bias of idealized-feedback).: _Suppose there are \(N_{h},N_{l}\) high, low-level trajectories, bias \(b\) is such that: \(\|b\|^{2}=_{t=1}^{N_{h}}|^{*},^{^{N_{l}}}(_{1}^{i}) -^{^{N_{l}}}(_{2}^{i})-^{*},^{^{*}}(_{ 1}^{i})-^{^{*}}(_{2}^{i})|^{2}=O(N_{h}/N_{l})\)._

**Proposition 2** (Reward model learning).: _Let \(_{MLE}=_{}_{D}()\) and let \(C_{b}\) denote an upper bound on bias \(C_{b}\|b\|\), and \(,B\) constants. We have that with high probability:_

\[\|^{*}-_{MLE}\|_{_{N_{h}}^{h}+ I} C}}{^{2}}+^{2}+d+(1/)}{^{2}}+  B^{2}}\]

### Hierarchical Preference Learning

We now construct a hierarchical, preference-learning algorithm that invokes REGIME, a contemporary preference-learning algorithm with provable guarantees, as sub-routine for sub-MDP learning .

**Sub-MDP reward learning:** To start, we again need to incentivize goal-reaching in the sub-MDP reward. As such, given original feature \(_{orig}\), we introduce an additional feature accounting for goal-reachability. For trajectory \(\), define \(_{i}(s_{i}^{},a_{i}^{})=[_{orig}(s_{i}^{},a_{i}^{}), (i=H_{l} s_{i}^{}=g(s,a))]\) and for policy \(\), feature expectation \(_{i}(s_{i}^{},a_{i}^{})=[_{orig}(s_{i}^{},a_{i}^{}), (i=H_{l})(s_{H_{l}}^{}=g(s,a))]\).

The corresponding reward vector will also change to become \(^{*}=[^{*}_{orig},]\) for unknown \(^{*}_{orig},\).

**Assumption 4**.: _Through instructions to the labeler, \(\) may be raised beyond a threshold of our choosing._

That is, we assume we can provide instructions to the labeler, emphasizing goal-reachability such that \(\) is higher than some given threshold. As before, we take the threshold to be \((C,2H_{h}H_{l})\). And so while \(\) is unknown, we know that \((C,2H_{h}H_{l})\). With this set up, we can then bound the regret due to sub-optimal sub-policies, and sub-optimal simulator \(P^{^{}}\), both of which are needed in the final regret analysis.

**Lemma 3** (Regret due to sub-optimal sub-policies).: _For any high-level policy \(\), with high probability:_

\[^{^{*},P}()-^{^{N_{l}},P}(),^{*} H _{h}(}{}}+C_{2}^{})\]

_where this bound makes use of the REGIME guarantee on sub-MDP \(M(s,a)\) that \(|^{P}(^{*}_{s,a}),^{*}-^{P^{^{}}}( ^{N_{l}}_{s,a}),^{*}|}{}}+C_{2}^{}\)._

**Lemma 4** (Regret due to sub-optimal simulator \(P^{^{{}^{}}}\)).: _Let \(^{^{N_{l}},P^{^{{}^{}}}}()\) denote the feature expectation under high level policy \(\), sub-MDP policies \(^{N_{l}}\) and transitions \(P^{^{{}^{}}}\). With high probability, for any high level policy \(\):_

\[|^{^{N_{l}},P}()-^{^{N_{l}},P^{^{ {}^{}}}}(),^{*}| O((H_{h}d^{2}+H_{h}^{3}H_{l}^{2}) ^{{}^{}}+^{2}H_{l}}{})\]

### Hier-REGIME Analysis

Now, we present the Hier-REGIME Algorithm 2. At a high-level description goes as follows. First, we invoke one copy of REGIME across all sub-MDPs with shared exploration (L1-4) and learned reward (L5). Next, we use the learned reward to compute sub-MDP policies \(_{s,a}^{N_{l}}\) for each sub-MDP \(M(s,a)\) (L6). Finally, we invoke one copy of REGIME for the high-level MDP, where the feature function is defined as \(^{_{s,a}^{N_{l}},P^{^{{}^{}}}}\) (L8). Next, we note two properties about Algorithm 2.

```
0: High-level policy class \(^{h}\), low level-policy classes \(^{h}_{s,a}\), simulator \(P^{^{{}^{}}}\) with \(^{{}^{}}\)-precision
1:for episode \(n=1,...,N_{l}\)do
2:\((_{1}^{n},_{2}^{n})_{_{1},_{2}_{s,a} ^{l}_{s,a}}\|^{P^{^{{}^{}}}}(_{1})-^{P^{^{ {}^{}}}}(_{2})\|_{(^{h}_{n})^{-1}}\)\(\) explore using policy
3:\(^{l}_{n+1}=^{l}_{n}+(^{P^{^{{}^{}}}}( _{1}^{n})-^{P^{^{{}^{}}}}(_{2}^{n}))(^{P^{ ^{{}^{}}}}(_{1}^{n})-^{P^{^{{}^{}}}}(_{2}^{n})) ^{T}\)
4: Generate trajectories \(_{1}^{n},_{2}^{n}\) and acquire comparison feedback \(o_{n}\) comparison feedback for the pair of length-\(H_{l}\) trajectories
5: Compute MLE \(^{l}\) from \(\{_{1}^{n},_{2}^{n}\}_{n=1}^{N_{l}}\) and \(\{o_{n}\}_{n=1}^{N_{l}}\)
6: Compute \(_{s,a}^{N_{l}}=_{^{l}_{s,a}}^{P^{^{{}^{ }}}}(),^{l}\)
7:for episode \(n=1,...,N_{h}\)do
8:\((_{1}^{n},_{2}^{n})_{_{1},_{2}^{h}}\| ^{^{N_{l}},P^{^{{}^{}}}}(_{1})-^{^{N_{l}},P^{ ^{{}^{}}}}(_{2})\|_{(^{h}_{n})^{-1}}\)\(\) high-level policy
9:\(^{h}_{n+1}=^{h}_{n}+(^{^{N_{l}},P^{^{{}^{ }}}}(_{1})-^{^{N_{l}},P^{^{{}^{}}}}(_{2}))( ^{^{N_{l}},P^{^{{}^{}}}}(_{1})-^{^{N_{l}},P^{ ^{{}^{}}}}(_{2}))^{T}\)
10: Generate trajectories \(_{1}^{n},_{2}^{n}\) and acquire comparison feedback \(o_{n}\) comparison feedback for the pair of length-\(H_{h}\) trajectories
11: Compute MLE \(^{h}\) from \(\{_{1}^{i},_{2}^{i}\}_{i=1}^{N_{h}}\) and \(\{o_{i}\}_{i=1}^{N_{h}}\)
12:return high-level policy \(=_{^{h}}^{^{N_{l}},P^{^{{}^{ }}}}(),^{h}\), low-level policies \(\{_{s,a}^{N_{l}}\}_{s,a S A^{h}}\) ```

**Algorithm 2** Hierarchical-REGIME (Hier-REGIME)

**Hierarchical Exploration:** A key aspect of experiment design in offline RL is ensuring sufficient coverage with exploration. The difficulty with coverage in the hierarchical setting is that at first glance, we may need to search for pairs of trajectories over \((_{1},\{_{s,a}^{1}\}),(_{1},\{_{s,a}^{2}\})(^{h},_{s,a }^{l}_{s,a})\), instead of over \(_{1},_{2}^{h}\). However, we show that in the goal-HRL case, we can fix the sub-policies to be \(_{s,a}^{N_{l}}\) (for \(N_{l}\) large enough), and this is sufficient to compete with the optimal, hierarchical policy.

Additionally, unlike the tabular setting, sub-MDPs now share a common reward parameter \(^{*}\), thus allowing us to jointly, instead of separately as in tabular case, explore across sub-MDPs.

**Sufficiency of low-level feedback:** Through the algorithm, we can observe that low- and high-level exploration generates feature expectations set: \(\{^{P^{^{{}^{}}}}(_{1})-^{P^{^{{}^{}}}}( _{2})_{1},_{2}_{s,a}^{l}_{s,a}\}\) and \(\{^{P^{^{{}^{}}}}(_{1})-^{P^{^{{}^{}}}}( _{2})_{1},_{2}^{h},_{s,a}=_{s

**Theorem 2**.: _With high probability, under \(N_{h}>0\):_

\[V^{^{*},^{*}}-V^{,^{N_{l}}}\] \[^{^{*},P}(^{*})-^{^{N_{l}},P^{*}}(^{ *}),^{*}+}}(2d(1+}{d}))\| ^{*}-\|_{_{N_{h}}^{b}}+\] \[|^{^{N_{l}},P}(^{*})-^{^{N_{l}},P^{*^{ }}}(^{*}),^{*}|+|^{^{N_{l}},P^{*^{} }}()-^{^{N_{l}},P}(),^{*}|\]

To parse this, the regret decomposes into four terms. The first term is the regret due to sub-optimality in low-level policies \(^{N_{l}}\). The remaining three terms are derived from sub-optimality due to high-level policy \(\), decomposing into the second term on regret due to bias in learned reward \(\), the third and fourth term on regret due to sub-optimality of simulator \(P^{e^{}}\).

A main benefit of developing a learning Algorithm 2 is that we can then quantitatively assess the sample complexity associated with the two types of human feedback. As one may expect, there is a tradeoff between better sample complexity and cognitive load, with current-feedback attaining better sample efficiency but also requiring higher cognitive load on the human supervisor.

**Corollary 1**.: _Using Theorem 2, we obtain the following rates in terms of data tradeoffs:_

* _Idealized-feedback and required high-/low-level feedback: the overall rate comes out to_ \(O(N_{l}^{-1/4}+N_{h}^{-1/2})\)_. While high level trajectories provide additional coverage, it also incurs bias linear in_ \(N_{h}\) _of the bias of the low-level trajectories, thus slowing down the rate (Lemma_ 2_)._
* _Current-feedback and required high-/low-level feedback: the overall rate comes out to_ \(O(N_{l}^{-1/2}+N_{h}^{-1/2})\)_. The current-feedback is unbiased and results in more efficient reward learning with_ \(\|^{*}-\|_{_{N_{h}}^{b}}=O(1)\)_[_29_]__._
* _Only low-level feedback is required due to sufficiency in coverage: the overall rate comes out to_ \(O(N_{l}^{-1/2})\)_. In a nutshell, this is because we can explore with just_ \(N_{l}\) _low-level samples which is unbiased, resulting in_ \(\|^{*}-\|_{_{N_{l}}^{b}}=O(1)\)_. Hence, both exploration and reward learning is efficient._

## 5 Discussion

Our work considers scalable oversight in the context of goal-conditioned HRL, in which we show that one can efficiently use hierarchical structure to learn from bounded human feedback.

**Limitations & Future Work:** In goal-conditioned HRL, our regret guarantees are with respect to the return of the optimal, hierarchical policy, whose performance is dependent on the usefulness of goal function \(g\). Further research is needed to understand on how to learn good goal functions, using limited supervised or unsupervised learning. Additionally, under current-feedback, the labeler providing high-level feedback is somehow made aware of sub-policy performance. An exciting research direction is how one may provide such knowledge through AI-assistance.