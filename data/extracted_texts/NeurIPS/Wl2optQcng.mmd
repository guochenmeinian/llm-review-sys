# Personalized Federated Learning via Feature Distribution Adaptation

Connor J. McLaughlin, Lili Su

Northeastern University, Boston, MA 02115

{mclaughlin.co,l.su}@northeastern.edu

###### Abstract

Federated learning (FL) is a distributed learning framework that leverages commonalities between distributed client datasets to train a global model. Under heterogeneous clients, however, FL can fail to produce stable training results. Personalized federated learning (PFL) seeks to address this by learning individual models tailored to each client. One approach is to decompose model training into shared representation learning and personalized classifier training. Nonetheless, previous works struggle to navigate the bias-variance trade-off in classifier learning, relying solely on limited local datasets or introducing costly techniques to improve generalization. In this work, we frame representation learning as a generative modeling task, where representations are trained with a classifier based on the global feature distribution. We then propose an algorithm, pFedFDA, that efficiently generates personalized models by adapting global generative classifiers to their local feature distributions. Through extensive computer vision benchmarks, we demonstrate that our method can adjust to complex distribution shifts with significant improvements over current state-of-the-art in data-scarce settings. Our source code is available on GitHub1.

## 1 Introduction

The success of deep learning models relies heavily on access to large, diverse, and comprehensive training data. However, communication constraints, user privacy concerns, and government regulations on centralized data collection often pose significant challenges to this requirement . To address these issues, Federated Learning (FL)  has gained considerable attention as a distributed learning framework, especially for its privacy-preserving properties and efficiency in training deep networks.

The FedAvg algorithm, introduced in the seminal work , remains one of the most widely adopted algorithms in FL applications . It utilizes a parameter server to maintain a global model, trained through iterative rounds of distributed client local updates and server aggregation of client models. While effective under independent and identically distributed (i.i.d.) client data, its performance deteriorates as client datasets become more heterogeneous (non-i.i.d.). Data heterogeneity leads to the well-documented phenomenon of client drift , where distinct local objectives cause the model to diverge from the global optimum, resulting in slow convergence  and suboptimal local client performance . Despite extensive efforts  to enhance FedAvg for non-i.i.d. clients, the use of a single global model remains too restrictive for many FL applications.

Personalized federated learning (PFL) has emerged as an alternative framework that produces separate models tailored to each client. The success of personalization techniques depends onbalancing the bias introduced by using global knowledge that may not generalize to individual clients, and the variance inherent in learning from limited local datasets. Popular PFL techniques include regularized local objectives [41; 26], local-global parameter interpolation , meta-learning [9; 16], and representation learning [35; 5; 48; 29]. While these techniques have shown significant improvements for clients under limited types of synthetic data heterogeneity (e.g., imbalanced partitioning of an otherwise i.i.d. dataset), we find that current methods still struggle to navigate the bias-variance trade-off with the additional challenge of feature distribution shift and data scarcity, conditions commonly encountered in cross-device FL.

As such, we look to design a method capable of handling real-world distribution shifts, e.g., covariate shift caused by weather conditions or poor camera calibration, (see clients 1 and 2 in Fig. 1) with limited local datasets. To this end, we approach PFL through shared representation learning guided by a global, low-variance generative classifier. Specifically, we select a probability density \(p\) with desirable statistical properties (e.g., one that admits an efficient Bayesian classifier) and iteratively estimate the global parameters of this distribution and representation layers to produce features from the estimated distribution (Fig. 1a).

To further navigate the bias-variance trade-off, we introduce a local-global interpolation method to adapt the global estimate to the distribution of each client. At inference time, clients use their adaptive local distribution estimate in a personalized Bayesian classifier (Fig. 1b).

**Contributions.** We propose a novel **P**ersonalized **F**ederated Learning method based on **F**eature **D**istribution **A**daptation (**pFedFDA**). We contextualize our algorithm using a class-conditional multivariate Gaussian model of the feature space in a variety of computer vision benchmarks. Our empirical evaluation demonstrates that our proposed method consistently improves average model accuracy in benchmarks with covariate shift or client data scarcity, obtaining over 6% in multiple settings. At the same time, our method remains competitive with current state-of-the-art (often within 1%) on more general benchmarks with more moderate data heterogeneity. To summarize, our contributions are three-fold:

* A novel generative modeling perspective for federated representation learning is proposed to enable a new bias-variance trade-off for client classifier learning.
* We propose a personalized federated learning method, pFedFDA, which leverages awareness of latent data distributions to guide representation learning and client personalization.
* Extensive experiments on image classification datasets with varying levels of natural data heterogeneity and data availability demonstrate the advantages of pFedFDA in challenging settings.

## 2 Related Work

**Federated Learning with Non-i.i.d. Data.** Various studies have worked to understand and improve the ability of FL to serve heterogeneous clients. In non-i.i.d. scenarios, the traditional FedAvg method  is susceptible to client drift , resulting in slow convergence and poor local client accuracy [28; 27]. To tackle this challenge, [27; 1; 21] proposed the use of regularized local objectives to reduce the bias on the global model after local training. Another approach focuses on rectifying the bias of local updates [19; 10] through techniques such as control variates. Other strategies include loss-balancing [15; 47; 3], knowledge distillation [30; 54], prototype learning , and contrastive learning . Despite promising results on non-i.i.d. data, their reliance on a single global model poses limitations for highly heterogeneous clients .

**Personalized Federated Learning.** In response to the limitations of a single global model, PFL seeks to overcome heterogeneity by learning models tailored to each client. In this framework, methods attempt to strike a balance between being flexible enough to fit the local distribution and relying on global knowledge to prevent over-fitting on small local datasets. Popular strategies include meta-learning an initialization for client adaptation [16; 9], multi-task learning with local model regularization [41; 26], local and global model interpolation , personalized model aggregation [51; 50], client clustering [39; 8], and decoupled representation and classifier learning [5; 29; 35; 48; 3]. Our work focuses on this latter approach, in which the neural network is typically decomposed into the first \(L-1\) layers used for feature extraction, and the final classification layer.

Existing works in this category share feature extraction parameters between clients and rely on client classifiers for personalization. These approaches differ primarily in the acquisition of client classifiersduring training, which influences representation learning. For example, FedRep  sequentially trains a strong local classifier while holding the representation fixed, then updates the representation under the fixed classifier. FedBABU  proposes to use fixed dummy classifiers to align client objectives, only fine-tuning the classifier layers after the representation parameters have converged. Similarly, FedRoD  aims to train a generic representation model and classifier in tandem via balanced softmax loss, later obtaining personalized classifiers through fine-tuning or hypernetworks. FedPAC  adopts the learning algorithm of FedRep, but additionally regularizes the feature space to be similar across clients, before learning a personalized combination of classifiers across clients to improve generalization. However, this collaboration comes with an additional computational overhead that scales with the number of active clients. pFedGP  leverages a shared feature extractor as a kernel for client Gaussian processes. Although this approach offers improved sample efficiency, it comes at the cost of increased computational complexity and reliance on an inducing points set.

In a similar spirit to our method, FedEM  estimates the latent data distribution of clients in parallel to training classification models. FedEM estimates each client data distribution as a mixture of latent distributions, where personalized models are a weighted average of mixture-specific models. Notably, this introduces a significant overhead in both communication and computation as separate models are trained for each mixture. In contrast, our work estimates the distribution of client features in parallel to training a global representation model.

## 3 Problem Formulation

**FL System and Objective.** We consider an FL system where a parameter server coordinates with \(M\) clients to train personalized models \(_{i}\), \(i=1,2,,M\). Each client \(i\) has a local training dataset \(_{i}=\{(x_{i}^{j},y_{i}^{j})\}_{j=1}^{n_{i}}\), where \(x^{m}\) and \(y\{1,,C\}\). The model training objective in PFL is:

\[_{_{1},...,_{M}}\ f(_{1},...,_{M}): =_{i=1}^{M}F_{i}(_{i}),\] (1)

Figure 1: Overview of pFedFDA. (Left) Heterogeneous clients collaboratively train representation parameters under a generative classifier derived from a global estimate of class feature distributions. (Right) At test time, clients adapt the generative classifier to their feature distributions to obtain personalized classifiers.

where \(\) is feasible set of model parameters, \(F_{i}(_{i})=_{(x,y)_{i}}[L(_{i}(x),y)]\) is the empirical risk of dataset \(_{i}\), and \(L\) is a loss function of the prediction errors (e.g., cross-entropy). The client population \(M\) in FL can be large, resulting in partial client participation . Let \(q\) denote the participation rate, meaning that in each round, a client participates in model training with probability \(q\).

Following [5; 48], we approach this as a problem of global representation learning and local classification, in which each \(_{i}\) consists of a shared backbone, \(\), responsible for extracting low-level features (\(z^{d}=(x)\)), and a local classifier, \(h_{i}\), for learning a client-specific mapping between features and labels. Considering this decomposition of parameters \(_{i}=(h_{i})\), we can rewrite the original PFL objective as the following:

\[_{}_{i=1}^{M}_{h_{i}}F_{i}(h_{i }),\] (2)

where \(\) and \(\) are the feasible sets of neural network and classifier parameters, respectively.

In our generative modeling framework, we consider \(\) to be the probability simplex over \(\{1,,C\}\), and our algorithm uses approximations of the posterior distributions as classifiers \(h_{i}\). However, for fair comparison with existing work (as well as other nice properties, discussed in Section 4.1), we select a generative model of the feature space such that \(h_{i}\) can be represented with an equivalent linear layer.

**Data Heterogeneity.** The data distribution of each client \(i\) is a joint distribution on \(\), which can be written as \(p_{i}(x,y)\), \(p_{i}(y)p_{i}(x|y)\), or \(p_{i}(x)p_{i}(y|x)\). Using the terminology of , we refer to each case of data heterogeneity as follows: _prior probability shift_ (\(p_{i}(y) p_{i^{}}(y)\)), _concept drift_ (\(p_{i}(x|y) p_{i^{}}(x|y)\)), _covariate shift_ (\(p_{i}(x) p_{i^{}}(x)\)), and _concept shift_ (\(p_{i}(y|x) p_{i^{}}(y|x)\)). Furthermore, the local dataset volumes \(_{i}\) may have _quantity skew_, i.e., \(n_{i} n_{i^{}}\).

## 4 Methodology

In this section, we introduce pFedFDA, a personalized federated learning method that utilizes a generative modeling approach to guide global representation learning and adapt to local client distributions. We present our method using a class-conditional Gaussian model of the feature space, with additional discussion of the selected probability density in Section 4.1.

Algorithm 1 describes the workflow of pFedFDA.

Our algorithm begins with a careful initialization of parameters for the feature extractor \(\), Gaussian means \(=\{^{c}\}_{c=1}^{C}\), and covariance \(\) (Lines 1-2). We initialize \(\) with established techniques (e.g., ) such that the output features follow a Gaussian distribution with controlled variance. We similarly use a spherical Gaussian to ensure a stable initialization of the corresponding generative classifier (see Section 4.1).

At the start of each FL round \(r\), the server broadcasts the current \(_{g}^{r},_{g}^{r},_{g}^{r}\) to each participating client. The local training of each client consists of two key components: (1) _global representation learning_, in which clients train \(\) to maximize the likelihood of local features under the global feature distribution \(_{g}^{r},_{g}^{r}\) (Line 7); (2) _local distribution adaptation_, in which clients obtain robust estimates of their local feature distribution \(_{i}^{r},_{i}^{r}\), using techniques for efficient low-sample Gaussian estimation (Line 8) and local-global parameter interpolation (Lines 9-11). After local training, clients send their \(_{i}^{r},_{i}^{r},_{i}^{r}\) to the parameter server for aggregation (Line 14).

In the following sections, we provide detailed explanations of each algorithmic component. In Section 4.1 we discuss the benefits of a generative modeling framework and provide the justification for our selected class-conditional Gaussian model. We outline how the resulting generative classifier can be used to guide representation learning in Section 4.2 and describe how we obtain personalized generative classifiers in Section 4.3.

### Generative Model of Feature Distributions

**Motivation for Generative Classifiers.** A central theme in FL is exploiting inter-client knowledge to train more generalizable models than any client could attain using only their local dataset. This presents an important bias-variance trade-off, as incorporating global knowledge naively can introduce significant bias. Fortunately, under a generative modeling approach, this bias can be naturally handled, enabling efficient inter-client collaboration.

First note that local class priors \(p_{i}(y)\) can be approximated with local counts: \(p_{i}(y=c)^{c}}{_{c^{} C}n_{i}^{c^{}}}:= _{i}^{c}\), where \(n_{i}^{c}\) is the number of local samples whose labels are \(c\). This leaves the primary source of bias to the mismatch between local and global feature distributions \(p_{g}(z|y)\) and \(p_{i}(z|y)\). Crucially, it turns out that this bias is controllable due to the dependence of \(z\) on global representation parameters \(\). Consequentially, we propose to minimize this bias through our classification objective, which we discuss further in Section 4.2.

**Class-Conditional Gaussian Model.** In this work we approximate the distribution of latent representations using a class-conditional Gaussian with tied covariance, i.e., \(p_{i}(z|y=c)=(z|_{i}^{c},_{i})\). We show the resulting generative classifier under this model in Eq. 4. Note that it has a closed form and results in a decision boundary that is linear in \(z\). I.e., if we know the underlying local feature distribution mean and covariance, we can efficiently compute the optimal header parameters \(h_{i}\) for the inner objective in Eq. 2.

In addition to the convenient form of the Bayes classifier, we select this distribution as the Gaussianity of latent representations is likely to hold in practice. Notably, by adopting the common technique of Gaussian weight initialization (e.g., ), the resulting feature space is highly Gaussian at the start of training. It has also been observed that the standard supervised training of neural networks with cross-entropy objectives results in a feature space that is well approximated by a class-conditional Gaussian distribution , i.e., the corresponding generative classifier Eq. 4 has equal accuracy to the learned discriminative classifier. We provide a further discussion of this modeling assumption in Appendix A.

\[p(y=c|z)=(z|^{c},)p(y=c)}{_{c^{ }}(z|^{c^{}},)p(y=c^{})},\] (3) \[ p(y=c|z) z^{}^{-1}^{c}-(^ {c})^{}^{-1}^{c}+ p(y=c).\] (4)

### Global Representation Learning

Next, we describe our process for training the shared feature extractor \(\). Similar to existing works [5; 48], our local training consists of training \(\) via gradient descent to minimize the cross-entropy loss of predictions from fixed client classifiers. We obtain our client classifiers through Eq. 4, using global estimates of \(_{g},_{g}\) and local estimated priors \(_{i}\). For computational efficiency, we avoid inverting the covariance matrix by estimating \(^{-1}^{c}\) with the least-squares solution \(w=_{w^{}}\| w^{}-^{c}\|\).

The loss of client \(i\) for an individual training sample (\(x,y\)) is provided in Eq. 5.

\[L(x,y;,,,)=_{c=1}^{C}y^{c} p(y^{c}|( x),^{c},,).\] (5)

Note that for a spherical Gaussian \(=\) and uniform prior \(\), we recover a nearest-mean classifier under Euclidean distance. This resembles the established global prototype regularization , which minimizes the Euclidean distance of features from their corresponding global class prototypes. Notably, FedPAC  uses this prototype loss to align client features. However, this implicitly assumes that all feature dimensions have equal variance, and additionally requires a hyperparameter\(\) to balance the amount of regularization with the primary objective. In contrast, our generative classifier naturally aligns the distribution of client features by training \(\) with our global generative classifier.

### Local Distribution Adaptation

**Local Estimation.** A key component of pFedFDA is the estimation of local feature distribution parameters, used both for model personalization and for updating the global distribution for representation learning.

Given a set of \(n\) extracted features \(Z\) with \(n^{c}\) examples per class \(c\), a maximum likelihood estimate of the class means and an unbiased estimator of the covariance, respectively, are given by:

\[^{c}=}_{j=1}^{n}_{\{y_{j}=c\}}z_{j}  42.679134pt)} =^{},\] (7)

where, with slight abuse of notation, \(^{n d}\) denotes the matrix of centered features with rows corresponding to each original feature \(z_{j}\) centered by their respective means, i.e.,

\[_{j}=z_{j}-_{c C}_{\{y_{j}=c\}}^{c}.\] (8)

Estimators Eq. 6 and Eq. 7 may be noisy on clients with limited local data. To illustrate this, consider the common practical scenario where \(n_{i} d\). The feature covariance matrix \(_{i}\) at client \(i\) will be degenerate; in fact, it will have a multitude of zero eigenvalues. In these cases, we can add a small diagonal \(\) to \(\), and replace the non-positive-definite matrices with the nearest positive definite matrix with identical variance. This can be efficiently computed by clipping eigenvalues in the corresponding correlation matrix and followed by converting it back to a covariance matrix with normalization to maintain the initial variance. We refer readers to  for a review of low-sample covariance estimation.

**Local-Global Interpolation.** We introduce this fusion because even with the aforementioned correction to ill-defined covariances, the variance of the local estimates remains highly noisy, indicating the necessity of leveraging global knowledge. It is essential to consider that in the presence of data heterogeneity, clients with differing local data distributions and dataset sizes have varying requirements for global knowledge.

For our Gaussian parameters \(,\), we consider the introduction of global knowledge through a personalized interpolation between local and global estimates, which can be viewed as a form of prior. We provide an analysis of the high-probability bound on estimation error for an interpolated mean estimate in simple settings in Theorem 1. The full derivation is deferred to Appendix E.

**Theorem 1** (Bias-Variance Trade-Off).: _Let \(C=1\). Define \(_{i}\) as the sample mean of client \(i\)'s local features \(_{i}:=}_{j=1}^{n_{i}}z_{i}^{j}\), and \(_{g}\) as the global sample mean using all \(N\) samples across \(M\) clients: \(_{g}:=_{i=1}^{M}_{i=1}^{n_{i}}z_{i}^{j}\). Assume client features are independent and distributed as \(z_{i}(_{i},_{i})\), with true global feature distribution \((_{g},_{g})\). We consider the use of global knowledge at client \(i\) through an interpolated estimate: \(_{i}:=_{i}+(1-)_{g}\), where \(\). For any \((0,1)\), with probability at least \(1-\), it holds that_

\[\|_{i}-_{i}\|_{2}^{2} (1-)^{2}\|_{g}-_{i}\|_{2}^{2}\] \[+[1+4(}+)](}(_{i })+}{N}(_{g})),\]

_where \(c>0\) is an absolute constant._

Intuitively, the estimation error and optimal \(\) depend on the bias introduced by using global knowledge \(\|_{g}-_{i}\|_{2}^{2}\), the variance of local and global features, and the respective data volumes.

We formulate this as an optimization problem, in which clients estimate interpolation coefficients \(_{i}\) to combine local and global estimates of \((,)\) with minimal \(k\)-fold validation loss:

\[_{i}_{0 1}\;\;_{k}_{(x,y) _{k}}L(x,y,,^{}}_{k}+(1- ^{})_{g},^{}_{k}+(1- ^{})_{g},_{i}),\] (9)where \(_{k}\) is the dataset consisting of the validation samples for the \(k\)-th fold, and (\(}_{k},_{k}\)) are the local distribution estimates Eq. 6 and Eq. 7 estimated using the training samples from the \(k\)-th fold. In our experiments, we avoid additional forward passes on the local dataset by preemptively storing the feature-label pairs obtained over the latest round of training.

We solve Eq. 9 using off-the-shelf quasi-newton methods (e.g., L-BFGS-B). We additionally explore using separate \(\) terms for the means and covariance (Section 5.3) and recommend the use of a single \(\) term for most applications.

After obtaining \(\), we set our local estimates of \(_{i},_{i}\) to their interpolated versions. These estimates are then sent to the server for aggregation. Notably, the server update rule can be viewed as a moving average  between the previous round estimate and the client average scaled by \(\), reducing the influence of local noise in the global distribution estimate. At test time, clients use their local distribution estimates for inference through the classification rule in Eq. 4.

## 5 Experiments

### Experimental Setup

**Datasets, Tasks, and Models:** We consider image classification tasks and evaluate our method on four popular datasets. The EMNIST  dataset is for 62-class handwriting image classification. The CIFAR10/CIFAR100  datasets are for 10 and 10-class color image classification. The TinyImageNet  dataset is for 200-class natural image classification. For EMNIST and CIFAR10/100 datasets, we adopt the 4-layer and 5-layer CNNs used in . On the larger TinyImageNet dataset, we use the ResNet18  architecture. Notably, the feature dimension \(d\) for EMNIST/CIFAR CNNs is 128, and 512 for ResNet. We provide additional details in Appendix C.1.

**Clients and Dataset Partitions:** The EMNIST dataset has inherent covariate shifts due to the individual styles of each writer. We partition the dataset by writer following , and train with \(M=1000\) total clients (writers), participating with rate \(q=0.03\). On CIFAR and TinyImageNet datasets, we simulate prior probability shift and quantity skew by partitioning the dataset according to a Dirichlet distribution with parameters \((0.1,0.5)\), where lower \(\) indicates higher levels of heterogeneity. On these datasets, we use \(M=100\) clients with participation rate \(q=0.3\). Additional details of the partitioning strategy are provided in Appendix C.1.2.

We split each client's data partition 80-20% between training and testing.

**Covariate Shift and Data Scarcity:** We introduce two modifications to client partitions to simulate the challenges of real-world cross-device FL. We first consider common sources of input noise for natural images, which may result from the qualities of the measuring devices (e.g., camera calibration, lens blur) or environmental factors (e.g., weather, lighting). To simulate this, we select ten image corruptions at five levels of severity defined in , and corrupt the training and testing samples of the first 50 clients in CIFAR10/100 with unique corruption-severity pairs. We leave the remaining 50 client datasets unchanged. We refer to these datasets with natural covariate shifts as CIFAR10-S/CIFAR100-S and detail the specific corruptions in Appendix C.1.1.

Second, we perform uniform subsampling of client training sets, leaving them with (75%, 50%, or 25%) of their original samples. These low-sample settings are more realistic for cross-device FL, where clients rely more on knowledge sharing.

**Baselines and Metrics:** We compare pFedFDA to the following baselines: Local, in which each client trains its model in isolation; FedAvg  and FedAvg with fine-tuning (FedAvgFT); APFL ; Ditto ; pFedMe ; FedRoD ; FedBABU ; FedPAC ; FedRep ; and LG-FedAvg . We report the average and standard deviation of client test accuracies.

**Model Training:** We train all algorithms with mini-batch SGD for \(E=5\) local epochs and \(R=200\) global rounds. We apply no data augmentation besides normalization into the range \([-1,1]\). For pFedFDA, we use \(k=2\) cross-validation folds to estimate a single \(_{i}\) term for each client. Additional training details and hyperparameters for each baseline method are provided in Appendix C.2.

### Numerical Results

**Performance under covariate shift and data scarcity.** We first present our evaluation under natural client covariate shift with varying data scarcity in Table 1. In all experiments, pFedFDA outperformsthe other methods in test accuracy, demonstrating the effectiveness of our method in adapting to heterogeneous client distributions. Additionally, pFedFDA has an increasing benefit relative to other methods in data-scarce settings: on CIFAR10, we improve 4.2% over the second-best method with 100% of training samples and 6.9% with 25%. On CIFAR100, the same improvements range from 0.1% to 6.5%. This indicates the success of our method in navigating the bias-variance trade-off.

**Evaluation in more moderate scenarios.** Our evaluation of all four datasets in the traditional setting (no added covariate shift, full training data) is presented in Table 2. We note that: (1) our method is still competitive, always ranking within the top 3 methods, and (2) the gap between top methods is smaller than in the previous experimental setting. For example, on EMNIST/CIFAR10, we see that FedAvgFT, FedPAC, and pFedFDA are within \(\)1% accuracy. We observe larger performance gaps for CIFAR100, with FedPAC and pFedFDA having the best results.

**Results under extreme data scarcity.** We present additional results at the limits of data scarcity on CIFAR10/100 datasets in Table 3, where we assign a single mini-batch (50) of training examples to each client. Notably, even as \(n_{i} d\), which poses a challenge to local covariance estimation, pFedFDA clients obtain the best test accuracy, indicating the robustness of our local-global adaptation.

**Generalization to new clients.** We further analyze the ability of our generative classifiers to generalize on clients unseen at training time. To simulate this setting, we first train the server model model using half of the client population. We then evaluate each method on the set of clients not encountered throughout training, using their original input data, as well as their dataset

   Dataset &  &  \\  \% Samples & 100 & 75 & 50 & 25 & 100 & 75 & 50 & 25 \\  Local Only &.586(.12) &.476(.16) &.461(.15) &.435(.14) &.157(.05) &.136(.05) &.123(.04) &.093(.04) \\  FedAvg &.464(.13) &.410(.19) &.389(.17) &.321(.14) &.233(.06) &.212(.06) &.187(.05) &.114(.04) \\ FedAvgFT &.682(.10) &.579(.19) &.561(.17) &.526(.16) &.302(.06) &.273(.05) &.241(.06) &.160(.05) \\  APFL &.611(.12) &.520(.17) &.508(.16) &.504(.16) &.164(.05) &.148(.04) &.131(.05) &.105(.04) \\ Ditto &.668(.10) &.578(.18) &.558(.17) &.527(.16) &.295(.05) &.274(.06) &.239(.05) &.141(.05) \\ FedBABU &.602(.12) &.522(.17) &.495(.16) &.467(.15) &.187(.05) &.170(.05) &.148(.05) &.107(.04) \\ FedPAC &.679(.09) &.642(.19) &.594(.16) &.533(.18) &.360(.07) &.330(.07) &.283(.07) &.162(.05) \\ FedRep &.612(.10) &.541(.17) &.510(.16) &.486(.16) &.176(.05) &.158(.05) &.131(.04) &.100(.04) \\ FedRoD &.655(.11) &.554(.18) &.537(.18) &.499(.14) &.218(.05) &.186(.05) &.150(.04) &.115(.04) \\ LG-FedAvg &.584(.13) &.483(.16) &.466(.15) &.433(.14) &.166(.05) &.153(.05) &.127(.05) &.091(.04) \\ pFedMe &.679(.10) &.583(.18) &.549(.17) &.523(.16) &.289(.06) &.268(.06) &.237(.06) &.153(.05) \\  pFedFDA & **.724(.09)** & **.706(.10)** & **.661(.11)** & **.595(.12)** & **.361(.08)** & **.342(.08)** & **.326(.08)** & **.227(.07)** \\   

Table 1: Average (standard deviation) test accuracy on CIFAR10/100-S for varying proportions of training data.

   Dataset & EMNIST &  &  &  \\  Partition & Writers & Dir(0.1) & Dir(0.5) & Dir(0.1) & Dir(0.5) & Dir(0.1) & Dir(0.5) \\  Local &.242(.23) &.865(.13) &.585(.13) &.368(.09) &.150(.05) &.270(.07) &.099(.03) \\  FedAvg &.790(.14) &.545(.12) &.625(.07) &.245(.06) &.252(.05) &.155(.04) &.150(.04) \\ FedAvgFT &.844(.10) &.902(.10) &.742(.08) &.499(.09) &.314(.06) &.384(.07) &.213(.04) \\  APFL &.841(.10) &.882(.11) &.656(.11) &.388(.09) &.169(.05) &.350(.09) &.177(.05) \\ Ditto &.843(.10) &.898(.10) &.736(.08) &.504(.08) &.308(.06) &.386(.07) &.211(.04) \\ FedBABU &.728(.13) &.887(.11) &.678(.11) &.395(.09) &.193(.04) &.365(.07) &.179(.04) \\ FedPAC & **.856(.09)** & **.908(.09)** & **.767(.07)** & **.560(.08)** &.378(.06) &.366(.07) &.180(.04) \\ FedRep &.735(.12) &.889(.10) &.668(.10) &.398(.09) &.182(.05) &.359(.07) &.145(.04) \\ FedRoD &.747(.15) &.885(.11) &.713(.09) &.424(.08) &.224(.05) &.382(.07) &.209(.05) \\ LG-FedAvg &.666(.13) &.866(.13) &.599(.12) &.381(.09) &.162(.05) &.280(.07) &.105(.03) \\ pFedMe &.842(.10) &.900(.10) &.740(.09) &.493(.08) &.311(.06) & **.388(.07)** & **.218(.04)** \\  pFedFDA &.844(.10) &.902(.09) &.763(.07) &.523(.08) & **.385(.07)** &.384(.07) &.214(.04) \\   

Table 2: Average (standard deviation) test accuracy on multiple datasets.

transformed using each corruption from CIFAR-S. Further benchmark details, including fine-tuning (personalization) procedures, are provided in Appendix C.3. As demonstrated in Table 4, our method generalizes well even on clients with covariate shifts not encountered at training time. Moreover, observe that pFedFDA has the highest accuracy on the original clients, highlighting the efficacy of structured generative classifiers when less training data is available (i.e., having 50 rather than 100 clients).

### Ablation of Method Components

We conduct two studies to verify the efficacy of our local-global interpolation method. In Table 5, we see that our interpolated estimates always perform better than using only local data, indicating the benefits of harnessing global knowledge. Learning separate \(\) terms for the means and covariance may be beneficial in low-sample or covariate-shift settings when the local distribution estimate may fluctuate further from the global estimate. However, using a single scalar \(\) appears sufficient and comes with the lowest computational cost (associated with the time to solve Eq. 9).

We additionally visualize the spread of learned \(\) across clients as a function of their dataset corruption in Fig. 2. As expected, clients with clean datasets rely more on global knowledge (smaller \(\) values) than corrupted clients. Moreover, corruptions with higher \(\) values (e.g., contrast) often align with the more difficult corruptions encountered in Table 4.

### Communication and Computation

The parameter count and relative communication load of our generative classifiers compared to a simple linear classifier varies depending on class count \(C\) and feature dimension \(d\). In our experimental configurations (datasets, architectures), the overhead in total parameter count ranges from 1.1% to 6.8%. See Appendix D.3 for additional details.

In Table 6, we compare the local training time (client-side computation) and total runtime of pFedFDA to baseline methods on CIFAR10. We observe a slight increase in training time relative to FedAvg, which can be attributed primarily to cost of learning our parameter interpolation coefficient \(\). However, this increase is comparable to the existing methods and is lower than representation

    &  \\  &  &  &  &  &  &  &  &  &  &  \\  &  &  &  &  &  &  &  &  &  &  \\  FedAvg &.592(0.7) &.584(0.85) &.512(0.97) &.554(0.86) &.568(0.7) &.575(0.7) &.569(0.7) &.465(0.8) &.467(0.8) &.580(0.7) &.557(0.8) &.359(0.10) \\ FedAvgT &.716(0.8) &.790(0.89) &.689(0.8) &.704(0.9) &.695(0.9) &.699(0.9) &.696(0.9) &.680(0.9) &.672(0.9) &.711(0.8) &.707(0.8) &.688(0.9) \\ FedABBU &.733(1.0) &.691(0.9) &.682(0.8) &.685(0.9) &.683(0.9) &.680(0.9) &.679(0.9) &.651(1.0) &.661(0.9) &.690(0.8) &.689(0.9) &.670(0.9) \\ FedPAC &.727(0.09) &.724(0.09) &.695(0.9) &.708(0.9) &.714(0.9) &.712(0.9) &.705(0.9) &.682(1.0) &.683(0.9) &.716(0.9) &.718(0.9) &.667(0.99) \\ pFedFDA & **.738(0.6)** & **.738(0.6)** & **.702(0.9)** & **.719(0.9)** & **.729(0.68)** & **.739(0.67)** & **.725(0.68)** & **.695(0.99)** & **.684(0.99)** & **.738(0.68)** & **.733(0.68)** & **.689(0.99)** \\   

Table 4: Evaluation of new-client generalization on CIFAR10 Dir(0.5).

    &  &  &  \\ NB & SB & MB & CIFAR100 & CIFAR100-25\% & CIFAR100-S & CIFAR100 & CIFAR100 & CIFAR100-25\% & CIFAR100-S & (\% second/iter.) \\  ✓ & &.458(0.08) &.382(0.09) &.436(0.08) &.320(0.06) &.216(0.05) &.296(0.06) & (0\%) \\  & ✓ &.523(0.08) &.396(0.09) &.487(0.08) &.385(0.06) &.266(0.06) &.361(0.08) & \(( 22.35\%)\) \\  & ✓ &.514(0.8) &.423(0.09) &.480(0.08) &.379(0.06) &.275(0.06) &.373(0.07) & \(( 36.11\%)\) \\   

Table 5: Ablation study on CIFAR100 with Dir(0.1) partition. **NB** denotes clients using only local data to estimate their feature distribution (\(_{i}=1\)). **SB** denotes each client estimating a single \(_{i}\) for both the means and covariance, **MB** denotes clients computing \(_{i}\) terms for the means and covariance separately. We show the average computational overhead across all settings.

learning methods FedRep and FedPAC. This demonstrates the relative efficiency of our generative classifier formulation in comparison to classifiers obtained through local fine-tuning.

## 6 Conclusion

Balancing local model flexibility and generalization remains a central challenge in personalized federated learning (PFL). This paper introduces pFedFDA, a novel approach that addresses the bias-variance trade-off in client personalization through representation learning with generative classifiers. Our extensive evaluation on computer vision tasks demonstrates that pFedFDA significantly outperforms current state-of-the-art methods in challenging settings characterized by covariate shift and data scarcity. Furthermore, our approach remains competitive in more general settings, showcasing its robustness and adaptability. The promising results underline the potential of our method to improve personalized model performance in real-world federated learning applications. Future work will focus on exploring the scalability of pFedFDA and its application to other domains.