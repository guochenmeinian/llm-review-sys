# CROMA: Remote Sensing Representations with Contrastive Radar-Optical Masked Autoencoders

Anthony Fuller\({}^{1,}\)

All correspondence should be addressed to Anthony Fuller: anthony.fuller@carleton.ca

Koreen Millard\({}^{2}\)

James R. Green\({}^{1}\)

\({}^{1}\)Department of Systems and Computer Engineering

\({}^{2}\)Department of Geography and Environmental Studies

Carleton University, Ottawa, Canada

###### Abstract

A vital and rapidly growing application, remote sensing offers vast yet sparsely labeled, spatially aligned multimodal data; this makes self-supervised learning algorithms invaluable. We present CROMA: a framework that combines contrastive and reconstruction self-supervised objectives to learn rich unimodal and multimodal representations. Our method separately encodes masked-out multispectral optical and synthetic aperture radar samples--aligned in space and time--and performs cross-modal contrastive learning. Another encoder fuses these sensors, producing _joint_ multimodal encodings that are used to predict the masked patches via a lightweight decoder. We show that these objectives are complementary when leveraged on spatially aligned multimodal data. We also introduce X- and 2D-ALiBi, which spatially biases our cross- and self-attention matrices. These strategies improve representations and allow our models to effectively extrapolate to images up to \(17.6\) larger at test-time. CROMA outperforms the current SoTA multispectral model, evaluated on: four classification benchmarks--finetuning (avg.\( 1.8\%\)), linear (avg.\( 2.4\%\)) and nonlinear (avg.\( 1.4\%\)) probing, \(k\)NN classification (avg.\( 3.5\%\)), and \(K\)-means clustering (avg.\( 8.4\%\)); and three segmentation benchmarks (avg.\( 6.4\%\)). CROMA's rich, optionally multimodal representations can be widely leveraged across remote sensing applications.

## 1 Introduction

Deep learning has led to rapid advances in remote sensing, augmenting our ability to understand and monitor our planet. The remote sensing community has developed many application-specific deep learning models, specifically for satellite imagery: identifying heavily polluting brick kilns  or illegal airstrips ; monitoring deforestation  or crops ; detecting floods  or wildfires ; even estimating household income  or poverty . Deep learning-based remote sensing is playing a growing role in tackling our climate crisis . Recently, researchers leveraged self-supervised learning to pretrain remote sensing models that can be employed on these tasks, and more . Self-supervised methods are invaluable for remote sensing, as there are petabytes of publicly available raw data from which to learn general representations, while only limited annotated data exists for downstream applications.

Self-supervised representations are often learned via contrastive approaches  or reconstruction approaches . Contrastive approaches encourage the representations of positive pairs of samples--built by producing another view of a sample, for instance, from another sensor  or time , or by augmentations --to be similar, and the representations of negative pairs to be dissimilar; this process can learn descriptive, object-focused representations. Models trained with acontrastive objective learn to discard information not shared between views [36; 37], regardless of the information's usefulness on downstream tasks; this makes the representations they learn sensitive to how positive pairs are built [30; 38; 39; 40]. Conversely, models trained with reconstruction or autoencoding objectives--for instance, predicting hidden pixels [31; 41]--learn to capture as much information as possible [42; 43; 44]. Reconstruction approaches do not rely on multiple views and scale incredibly well [31; 45; 46], but they learn representations that require significant finetuning to be useful on downstream tasks [43; 45]. Park et al.  show vision transformers (ViTs ) trained with contrastive learning focus more on shapes and low-frequency information than ViTs trained with reconstruction approaches, which focus more on textures and high-frequency information. They show that combining both objectives may achieve a sweet spot that learns better representations than either objective alone. Several other frameworks have been developed that leverage both objectives to learn SoTA representations [49; 50; 51; 52]; however, none are designed for spatially aligned multimodal data.

Researchers developing foundation models for remote sensing have yet to take advantage of the multimodal data ubiquitous to remote sensing. For instance, the Sentinel missions--imaging the Earth's landmass multiple times per month since 2015--consist of multispectral optical imagery acquired by Sentinel-2 and Synthetic Aperture Radar (SAR) data acquired by Sentinel-1. By exploiting differences in how electromagnetic radiation interacts with Earth surface materials and measuring the radiation at many wavelengths (ranging from 440 to 2,200 nm), Sentinel-2 multispectral optical imagery can be used to characterize the material composition of objects . By actively transmitting and receiving longer wavelength (5.5 cm) electromagnetic pulses, Sentinel-1 SAR can be used to characterize the geometry, roughness, and electrical properties of objects . These modalities have proven complementary across remote sensing applications [55; 56; 57]. Importantly for our work, they are spatially aligned, allowing multiple views of the same feature on the ground. Moreover, because data from the Sentinel missions are freely available, they have become the most widely used source of satellite imagery in research; thus, models with useful representations of Sentinel-1 & 2 imagery can be immediately leveraged in scientific research, improving our ability to understand our planet.

These observations motivate Contrastive Radar-Optical Masked Autoencoders (**CROMA**): a framework for learning rich representations of multimodal, spatially aligned, 2D data; which we leverage to pretrain ViT models on Sentinel-1 & 2 data, providing the most valuable foundation models for Earth Observation, to date. We highlight three contributions: \(\) CROMA significantly outperforms the current SoTA multispectral model, SatMAE , under an _extensive_ evaluation. \(\) CROMA learns representations that are _optionally_ multimodal (i.e., they can be effectively leveraged when either or both modalities are available) and _extrapolate_ to larger images at test-time (i.e., they can be effectively leveraged on images larger than those on which the model was trained). \(\) CROMA's effectiveness is driven by two innovations: our complementary pretraining objectives and our novel relative position encoding (RPE) strategies that contribute to the quality of learned representations.

## 2 Method

In this section, "optical" refers to \(12\)-channel multispectral optical imagery acquired by Sentinel-2, and "radar" refers to \(2\)-channel SAR backscatter data acquired by Sentinel-1.

**Architecture Background.** Most work combining contrastive and reconstruction objectives learning _joint_ multimodal representations is in the image-text domain [51; 58; 59; 60; 61; 62; 63; 64; 65]; heavily inspiring our architecture, Contrastive Captioning (CoCa ) learns SoTA image-text representations that are optionally multimodal. The CoCa framework consists of two unimodal encoders--one for images, one for text--and a multimodal decoder that receives text encodings at the bottom of the network and cross-attends to image encodings. CoCa is trained with two objectives: an image\(\)text contrastive objective between unimodal encoders and an image-captioning objective at the output of the multimodal decoder. Our framework significantly adapts CoCa to aligned multimodal 2D data by masking both modalities, introducing a multimodal encoder, a lightweight decoder (only used during pretraining, inspired by masked autoencoders ), and novel cross-attention and self-attention positional biases.

**Model Architecture.** CROMA consists of three encoders (Fig. 1). \(\) A unimodal radar ViT \(f_{}\) that encodes radar inputs \(I_{}^{2 H W}\) into \(L\) patch encodings \(_{}^{L D}\), i.e., \(_{}=f_{}(I_{})\). \(\) A unimodal optical ViT \(f_{}\) that encodes optical inputs \(I_{}^{12 H W}\) into \(L\) patch encodings\(_{}^{L D}\), i.e., \(_{}=f_{}(I_{})\). \(\) A multimodal radar-optical transformer \(f_{}\) that encodes \(L\) radar-optical patches, \(_{}^{L D}\), i.e., \(_{}=f_{}(_{}, _{})\). For each set of unimodal encodings, we build full-image representations \(^{D}\) by processing the mean pooled patch encodings through a feedforward network (FFN), i.e., \(_{}=_{}((_{}))\) and \(_{}=_{}((_{}))\). Our patch size is 8 x 8 pixels for both modalities, and our default image size is 120 x 120 pixels. Our radar encoder has \(N/2\) transformer layers, and our optical encoder has \(N\) transformer layers (\(N\) is \(12\) for ViT-B and \(24\) for ViT-L backbones). All unimodal encoder layers are composed of self-attention and FFN sublayers. Our multimodal \(N/2\)-layer encoder-- composed of self-attention, cross-attention, and FFN sublayers--encodes both modalities into a _single_ sequence of \(L\) patch encodings; this encoder receives radar patch encodings at the bottom of the network and learns multimodal representations by cross-attending to optical patch encodings. Multimodal representations can be built via pooling multimodal patch encodings, i.e., \(_{}=(_{})\). Our ViT backbones do not use sinusoidal position embeddings; instead, we bias the self-attention and cross-attention matrices with the distances between patches.

**ALBi Background.** ALBi  is a simple and intuitive RPE method for transformers that biases the self-attention matrix based on the distance between tokens in a 1D sequence. Each self-attention head receives positional biases with different strengths, called slopes \(m\). With \(16\) attention heads, the geometric sequence defines these scalar slopes starting at \(}\), i.e., \(}\), \(}\), \(}\), \(\), \(}\). Biases are subtracted from the attention matrix before the softmax is calculated. Specifically, the pre-softmax attention matrix \(A^{h L L}\) (\(h\) is the number of heads, \(L\) is the sequence length) is populated with attention scores \(a_{hij}\) for the \(i\)th query \(q_{hi}^{d}\) and \(j\)th key \(k_{hij}^{d}\) (\(d\) is the head dimension): \(a_{hij}= q_{hi} k_{hj}\), without ALiBi; and \(a_{hij}= q_{hi} k_{hij}-(i,j) m(h)\), with ALiBi. Crucially, ALiBi does not add position embeddings at the bottom of the network; instead, the relative positions between tokens are encoded in the attention matrix itself. To date, ALBi is the only position encoding method for transformers that has been demonstrated to extrapolate at test-time to sequences far longer than those on which it was trained.

**2D-ALBi and X-ALiBi.** We extend ALiBi to 2D inputs by biasing the self-attention matrix based on the Euclidean distance between query-key pairs in a ViT--we call this 2D-ALiBi (Fig. 2). We also extend ALiBi to cross-attention by biasing the cross-attention matrix based on the Euclidean distance between _cross-modal_ query-key pairs--we call this X-ALiBi. (In cross-attention , queries are built from the previous layer, whereas keys and values are built from optical encodings.) For both 2D and X-ALiBi, we calculate our attention-head slopes with the same geometric sequence as ALiBi. Since our two modalities

Figure 1: **(Left) Our CROMA framework jointly leverages radar\(\)optical contrastive learning and masked autoencoding to learn rich, self-supervised representations. (Right) Leverage representations on: unimodal or multimodal data, larger images at test-time, and diverse tasks and methods.**

Figure 2: 2D-ALiBi matrix for an image with 9 patches (\(3^{2}\) before flattening); \(q\) is for query, \(k\) is for key, and \(m\) is a fixed scalar that biases attention heads at different rates.

are aligned 2D sensor data, our 2D-ALiBi and X-ALiBi matrices are identical. The primary motivation of 2D-ALiBi is to learn representations that can generalize across image sizes; this is particularly useful in remote sensing and is likely useful in other domains. The primary motivation of X-ALiBi is to improve sensor fusion by inserting positional information in the cross-attention sublayer of our multimodal encoder, \(f_{}\). These position encoding techniques are rotation and translation invariant, which are desirable properties for the overhead imagery in Earth Observation.

**MAE Background.** A masked autoencoder (MAE ) rearranges an image into a sequence of non-overlapping patches, then randomly samples a large portion of patches to be held-out. The "visible" patches, that are _not_ held-out, are encoded by a ViT. This MAE-style masking is ingenious: it leverages sparse computation while only requiring dense operations that run efficiently on modern hardware. MAE introduces a lightweight decoder that receives visible patch encodings and hidden mask embeddings, which are both added to 2D-sinusoidal position embeddings. The decoder outputs predictions of the pixel values of the held-out patches. Both the encoder and decoder are pretrained end-to-end to minimize the mean squared error between patch predictions and the originally held-out patches. The pretrained encoder can then be leveraged on downstream applications.

**Reconstruction Objective.** We independently mask \(75\%\) of radar and optical patches and encode the unmasked patches with our three encoders, i.e., \(_{}^{nm}=f_{}(_{}^{nm})\), \(_{}^{nm}=f_{}(_{}^{nm})\), and \(_{}^{nm}=f_{}(_{}^{nm}, _{}^{nm})\); where \(nm\) means unmasked. We introduce a lightweight 1-layer transformer decoder \(f_{}\) that receives multimodal patch encodings and mask embeddings after adding 2D-sinusoidal position embeddings and predicts a target image, i.e., \(}=f_{}([_{}^{nm}, ^{}]+^{})\). We split the channels of \(}\) to form predictions for each sensor, \(}_{}\) and \(}_{}\); the loss is only applied at the locations of the masked-out patches:

\[_{}=_{i}^{N}(^{M}(}_{}^{ij}-(^{ ij})_{})^{2}}{M}}_{}+^{M}(}_{}^{ij}- (^{ij})_{})^{2}}{M}}_{})\]

where \(N\) is the batch size, \(M\) is the number of masked patches, and \(\) sets the mean to \(0\) and standard deviation to \(1\) for target patches (following MAE). Along with learning unimodal representations, this objective spatially fuses our two sensors, i.e., it builds multimodal patch encodings that represent information from _both_ sensors in the patch, corresponding to an \(80\, 80\,\) square on the ground (\(8 8\) patches at \(10\,\) resolution). Finally, 2D and X-ALiBi can be easily adapted to MAE-style masking by removing the masked-out columns and rows from the bias matrix.

**Contrastive Learning Background.** Contrastive learning aims to classify the correct pairings of samples derived from a batch. Logits are formed by measuring the similarity between the projected representations of samples. As a result, the representations of positive pairs are pulled together, and the representations of negative pairs are pushed apart. Very recently, FLIP  performs contrastive learning with masked-out representations via MAE-style masking. This speeds up pretraining and enables larger batches due to the reduced memory per sample. FLIP performs on par with CLIP --the foundational work that learns rich representations via image\(\)text contrastive learning--but can be pretrained at half the cost.

**Contrastive Objective.** We perform radar\(\)optical contrastive learning across the unimodal representations of our masked-out sensor data using the InfoNCE loss . For an optical anchor image, the positive sample is the geographically and temporally matched radar sample, and the negative samples are all _other_ radar samples from the batch; likewise, our radar representations are pulled towards (positives) or pushed apart (negatives) from optical representations:

\[_{}=-(^{N} }^{i}z_{}^{i}/)}{_{j}^{N }(z_{}^{i}z_{}^{j}/)}}_{}+^{N}}^{i }z_{}^{i}/)}{_{j}^{N}(z_{}^{i }z_{}^{j}/)}}_{})\]

where \(z_{}\) and \(z_{}\) are \(_{2}\) normalized linear projections of radar and optical representations, respectively, i.e., \(z_{}=(_{}(_{}))\) and \(z_{}=(_{}(_{}))\). \(\) is the softmax temperature, and \(N\) is the batch size. Crucially, we only encode a small portion of input patches, which form our representations. This masking provides advantages: it enables larger batches, speeds up pretraining, and enables our multimodal reconstruction objective with the same computational graph. This radar\(\)optical contrastive objective encourages representations to be sensor-invariant, i.e., to capture information shared _between_ sensors.

**Combined Pretraining Objective.** We combine contrastive learning and masked sensor modeling pretraining objectives: \(=_{}_{}+_{} _{}\). We set both task weights (i.e., \(_{}\) and \(_{}\)) to 1 and ablate them in Appendix SSA.1.

## 3 Experiments

**Pretraining.** We pretrain CROMA models on the SSL4EO dataset --a large geographically and seasonally diverse unlabeled dataset. SSL4EO consists of 1 million paired Sentinel-1 GRD & Sentinel-2 L2A samples of 264 \(\) 264 pixels. Sentinel-1 channels consist of VV and VH backscatter. Sentinel-2 channels consist of \(12\) surface reflectance multispectral bands (the cirrus band is removed). We pretrain CROMA-B (ViT-B backbone) for \(300\) epochs and CROMA-L (ViT-L backbone) for \(600\) epochs. Crucially, a single pretraining run trains all three encoders (optical, radar, and joint radar-optical) end-to-end; users can then finetune one or multiple encoders, depending on their task and data availability. We perform all pretraining experiments on an NVIDIA DGX server (\(8\) A100-80 GB), including ablations. Please see Appendix SSA.3 for more details.

**Comparisons.** We compare CROMA to all available multispectral optical foundation models, which include two models pretrained by  using radar\(\)optical contrastive learning; two models pretrained by  using the MAE  and DINO  frameworks; and two models pretrained by  using their multispectral representation learning framework, SatMAE. We also compare CROMA to a SoTA method for learning visual representations of natural images--image joint embedding predictive architecture (I-JEPA, )--that we leverage to pretrain a ViT-B model on SSL4EO's optical imagery for \(300\) epochs. To enable a fair comparison between models, we evaluate all models under identical conditions and hyper-parameter budgets (please see Appendix SSA.4.2 for details). This is necessary because the originally reported results of these models occurred under inconsistent evaluation conditions--for instance, data splits or training data amounts. We use the latest publicly available models for all evaluations and preprocess data according to official repositories. For radar and radar-optical datasets, we compare CROMA to SatViT-V2 , a model pretrained using MAE  on stacked Sentinel-1 & 2 imagery; and DeCUR, a model pretrained--concurrently with this work--by  using their multimodal representation learning framework.

### Multispectral Optical Experiments

**Classification Setup.** We evaluate CROMA by finetuning, frozen linear and nonlinear probing, \(k\)NN classifying, and \(K\)-means clustering pretrained representations across four Sentinel-2 classification benchmarks.

The multi-label BigEarthNet dataset  (35,420 train samples and 118,065 validation samples); this is \(10\%\) of the complete BigEarthNet training set that is now used by default [25; 26] to reduce the costs of finetuning and is better suited for a remote sensing benchmark .

The fMoW-Sentinel dataset  (71,287 train samples and 84,939 validation samples); this is also \(10\%\) of the complete training set. Following BigEarthNet's use, we believe this smaller training set is a more appropriate benchmark for model evaluation, but we show results on the complete training set in Appendix SSA.4.1.

The EuroSAT dataset  (16,200 train samples and 5,400 validation samples).

The Canadian Cropland dataset  (53,884 train samples and 23,088 validation samples); this is a new benchmark inspired by EuroSAT but is more challenging, as the crop types (barley, canola, corn, etc.) can be visually similar. For finetuning and linear probing, we add a linear layer for these tasks atop the full-image representations, i.e., \((_{})\). For nonlinear probing, we use an MLP with one hidden \(2048\)-d layer, i.e., \((((_{})))\). Additionally, we perform non-parametric \(k\)NN classification (\(k=20\)) and \(K\)-means clustering for single-label benchmarks to evaluate frozen representations.  shows that no single method of evaluating representations is the best; they recommend including \(k\)NN and \(K\)-means alongside linear probing. Other studies [80; 72; 81] show a rank mismatch between \(k\)NN and linear probing evaluations--indicating they offer complementary estimates of representation quality. Please see Appendix SSA.4.1 and A.4.2 for implementation, data splits, and hyper-parameter details.

**Classification Results.** CROMA ranks first, averaged across four Sentinel-2 (multispectral optical) benchmarks, under fine-tuning, frozen linear and nonlinear probing, \(k\)NN classification, and \(K\)-means clustering (Table 1, 2). The _only_ case where a SatMAE model outperforms a CROMA model of the same backbone is finetuning on the fMoW-Sentinel dataset. However, SatMAE was pretrained on fMoW-Sentinel--meaning there is no distribution shift between pretraining and finetuning. This gives SatMAE an advantage on the fMoW-Sentinel benchmark since downstream performance is impacted by the similarity between upstream and downstream data . Despite this observation, CROMA-B outperforms SatMAE-B on fMoW-Sentinel under linear probing (\( 2.4\%\)), nonlinear probing (\( 2.4\%\)), \(k\)NN (\( 5.3\%\)), and \(K\)-means (\( 2.4\%\)). Additionally, CROMA models are more than \(4\) faster during finetuning and inference than their SatMAE counterparts (please see Appendix SSA.4.2 for a comparison). On all evaluations, CROMA outperforms a ViT-B model pretrained using the I-JEPA framework on the SSL4EO dataset--I-JEPA is the current SoTA framework for learning self-supervised representations of ImageNet . We also plot UMAP  embeddings (Fig. 3); CROMA shows a strong separation of EuroSAT classes.

    & &  &  &  &  \\  & &  &  &  &  \\  Method & Backbone & FT & MLP & LP & FT & MLP & LP & FT & MLP & LP & FT & MLP & LP \\  radar\(\)optical  & ResNet50 & 77.65 & 78.79 & 77.44 & 32.03 & 6.46 & 6.01 & 96.31 & 86.35 & 78.81 & 57.44 & 58.09 & 55.55 \\ radar\(\)optical  & Swin-T & 86.41 & 78.65 & 77.93 & 52.01 & 28.54 & 31.06 & 98.09 & 93.50 & 94.78 & 70.98 & 60.36 & 57.05 \\ MAE  & ViT-S & 86.15 & 81.70 & 75.94 & 51.79 & 31.70 & 27.69 & 98.78 & 94.46 & 91.80 & 74.02 & 59.07 & 48.38 \\ DINO  & ViT-S & 87.04 & 84.96 & 81.85 & 52.79 & 35.62 & 32.64 & 98.63 & 97.07 & 96.07 & 75.27 & 67.35 & 59.94 \\ I-JEPA  & ViT-B & 85.92 & 84.27 & 80.80 & 53.54 & 35.76 & 32.35 & 99.20 & 96.60 & 95.63 & 75.13 & 66.69 & 60.17 \\ SatMAE  & ViT-B & 85.94 & 83.48 & 79.36 & 57.20 & 37.28 & 35.17 & 99.20 & 97.28 & 96.61 & 73.58 & 66.02 & 60.40 \\ CROMA & ViT-B & 87.58 & 86.29 & **85.04** & 54.47 & 39.67 & 38.42 & 99.22 & 97.89 & 97.59 & 76.17 & 67.62 & 63.39 \\  SatMAE  & ViT-L & 86.18/ 84.01 & 80.29 & 58.19 & 38.18 & 38.76 & 99.35 & 97.67 & 97.65 & 74.06 & 67.03 & 61.75 \\  & 82.62* & & 98.98* & 98.08 & 98.08 & 98.08 & 98.08 & 98.08 & 98.02 & 78.07 & 67.94 & 64.02 \\ 
**CROMA** & ViT-L & **88.29** & **86.46** & 85.01 & **59.02** & **40.07** & **39.17** & **99.46** & **98.04** & **98.02** & **78.07** & **67.94** & **64.02** \\   

Table 1: **Classification results on four benchmarks, under finetuning (FT), and frozen linear (LP) and nonlinear (MLP) probing. * denotes originally reported results; we obtain all other results under identical conditions.**

    & &  &  &  \\  & &  &  &  \\  Method & Backbone & \(k\)NN & \(K\)- & \(k\)NN & \(K\)- & \(k\)NN & \(K\)- \\  & & & & means & means & means & \\  radar\(\)optical  & ResNet50 & 7.07 & 3.94 & 52.09 & 23.52 & 49.82 & **23.15** \\ radar\(\)optical  & Swin-T & 19.46 & 7.93 & 85.07 & 56.91 & 48.22 & 21.57 \\ MAE  & ViT-S & 22.25 & 7.54 & 87.33 & 41.50 & 56.01 & 18.31 \\ DINO  & ViT-S & 28.89 & 8.23 & 94.20 & 45.83 & 60.70 & 20.22 \\ I-JEPA  & ViT-B & 25.45 & 8.01 & 89.02 & 42.89 & 57.58 & 18.06 \\ SaMAE  & ViT-B & 26.76 & 8.98 & 89.28 & 44.87 & 56.20 & 20.14 \\  CROMA & ViT-B & **32.03** & **11.35** & **95.92** & **76.06** & **61.25** & 22.58 \\ SaMAE  & ViT-L & 26.77 & 9.17 & 89.57 & 38.48 & 56.93 & 19.65 \\ CROMA & ViT-L & 29.54 & 10.12 & 94.70 & 61.24 & 59.41 & 21.27 \\   

Table 2: **Non-parametric \(k\)NN classification and \(K\)-means clustering results. * denotes 10% of the training set.**

Figure 3: UMAP embeddings and \(K\)-means clustering accuracies of CROMA (ViT-B), SatMAE (ViT-B) , DINO (ViT-S) , and radar\(\)optical (Swin-T)  models on EuroSAT .

[MISSING_PAGE_FAIL:7]

[MISSING_PAGE_FAIL:8]

target (2 channels) by \(0.3\%\) (Table 6); this implies that reconstructing more information leads to more general representations. A multimodal target is especially important for learning rich multimodal representations. A larger decoder improves segmentation but costs \(1.7\) more. MAE showed that a deep decoder improves linear probing accuracy , but CROMA is very robust to decoder size. Therefore, we select the most efficient, i.e., a \(1\)-layer, \(512\)-d decoder.  We design CROMA by pretraining ViT-B models for \(100\) epochs and consider linear probing performance and cost. Thus, our design may not be optimal when scaling up. Nevertheless, scaling to more epochs and a larger model improves representations, especially radar-only representations.

In Appendix SSA.1 we also experiment with--all showing minimal or negative impacts--task weights, more decoder sizes, VICReg , mean squared error loss between cross-modal patch encodings (inspired by ), InfoNCE loss between cross-modal patch encodings (inspired by ), hard-negative mixing , and lower-masked tuning at the end of pretraining .

### Extrapolating Representations to Larger Images

**Extrapolation Setup.** For three CROMA-B models (2D-sinusoidal, PEG , and 2D-ALBi) pretrained for \(100\) epochs, we finetune all parameters (along with a linear head) on Sentinel-2 images from DW-Expert-120 (120 x 120 pixels). Then, we directly evaluate the models on images of varying sizes to test their ability to extrapolate at test-time. We create validation sets with different image sizes by cropping from the original \(510 510\) images in Dynamic World . Regardless of image size, we retain the 10 m per pixel spatial resolution on which the models were trained--extrapolating to smaller or larger geographic areas at test-time. For 2D-sinusoidal embeddings, we also evaluate an embedding interpolation algorithm often used when applying ViTs on larger images .

**Extrapolation Results.** 2D-ALBi outperforms PEG by \(1.7\%\) on 504 x 504 pixel images (Table 7). Amazingly, we only see a \(0.7\%\) drop in mIoU when testing on areas \(17.64\) larger than those on which our model was trained--effectively generalizing from 225 to 3,969 patches per image. We achieve this by extending ALBi  to 2D inputs by penalizing attention scores based on the Euclidean distance between query-key pairs. We may achieve even better results by encoding directions in a subset of attention heads or learning scalars ; we leave these investigations to future work. We believe X- and 2D-ALBi have tremendous potential beyond our CROMA framework, for instance, by extending these methods to additional modalities, viewing angles, or 3D data.

## 5 Related Work

**Remote Sensing Representations.** Deep learning for remote sensing has been an active research area for many years. Researchers have leveraged self-supervised learning frameworks in the last few years to learn representations of remote sensing data that can be widely leveraged across societally important downstream tasks. In designing contrastive pretext tasks, remote sensing researchers built positive pairs by leveraging spatial , temporal , spectral , or cross-modal  information. In designing reconstructive pretext tasks, researchers held-out spectral bands , pixels , and resolutions . However, these studies--including the concurrent Scale-MAE  and influential studies tile2vec , GASSL , and SeCo --were either performed at smaller scales or only included wavelengths from the visible (red, green, and blue) spectrum; this limits their utility on downstream applications since wavelengths from the non-visible spectrum contain information _critical_ to many remote sensing tasks . For example, the ability to measure the reflectance of objects in the near-infrared portion of the wavelength is extremely valuable in applications related to vegetation identification , health and productivity , as well as identifying water bodies , soil moisture , and vegetation water content .

**Relative Position Encoding for ViTs.** SoTA transformers in natural language processing use RPE , but fixed 2D-sinusoidal embeddings still predominate ViTs. The improved inductive

   Method & Test Res. & 48 & 96 & 120 & 224 & 384 & 448 & 504 \\ 
2D-Sinusoidal & 54.4 & 58.5 & 59.2 & 38.2 & 24.9 & 21.8 & 19.5 \\
2D-Sinusoidal w/interp. & 50.7 & 58.6 & 59.2 & 58.3 & 56.2 & 55.5 & 54.9 \\ PEG  & 55.1 & 58.0 & 58.3 & 58.5 & 57.7 & 57.4 & 57.1 \\
2D-ALBi & **56.3** & **59.0** & **59.3** & **59.5** & **59.1** & **59.0** & **58.8** \\   

Table 7: Segmentation results (mIoU) of models trained on \(120 120\) resolution and evaluated on various resolutions.

bias of RPE over absolute position encoding can offer improved performance and, sometimes, the ability to extrapolate to larger images at test-time, i.e., directly applying a model trained on one image size to another without further training. This extrapolation ability would add considerable value to remote sensing foundation models since image sizes vary widely--images are often cropped from large scenes down to smaller sizes chosen idiosyncratically. Positional Encoding Generator (PEG, ) is a SoTA RPE method that uses a convolution between ViT layers; PEG was demonstrated to significantly outperform other RPE methods when tested on larger images. iRPE  is another, more complex, RPE method for ViTs; however, it demonstrates no extrapolation ability. We found no prior work that leverages RPE in cross-attention.

## 6 Conclusion

We propose a novel framework for learning unimodal and multimodal representations for Earth Observation by jointly leveraging contrastive and reconstruction self-supervised objectives. We extend a SoTA position encoding method for 1D sequences to 2D inputs and cross-attention; to the best of our knowledge, this is the first time explicit position encoding has been leveraged in cross-attention. These strategies allow our models to extrapolate to larger images at test-time and improve performance on _both_ unimodal and multimodal data. We _extensively_ evaluate our pretrained models on diverse tasks and methods, outperforming the previous SoTA. Although our method is designed for multimodal satellite imagery, it can be leveraged on other applications that offer spatially aligned multimodal data, for instance, medical imaging or autonomous vehicles.

The main limitation of our work is our focus on static-in-time Sentinel-1 & 2 data; in the future, we will explore other sensors that offer higher spatial or spectral resolutions and time-series data. Despite this limitation, Sentinel-1 & 2 are the most widely used sources for satellite imagery in research--making our models an incredible resource for the remote sensing community and users of remote sensing-derived products, for instance, geographers, economists, or environmental scientists.

## 7 Acknowledgements

This work was made possible with compute provided by Cyxtera Technologies and the NVIDIA Academic Hardware Grant Program. Anthony thanks the Vector Institute for their financial support during his master's program.