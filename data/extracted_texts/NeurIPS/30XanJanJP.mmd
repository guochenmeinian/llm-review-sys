# EffBench: Benchmarking the Efficiency of Automatically Generated Code

Dong Huang

The University of Hong Kong

dhuang@cs.hku.hk

&Yuhao Qing

The University of Hong Kong

yhqing@cs.hku.hk

&Weiyi Shang

University of Waterloo

wshang@uwaterloo.ca

&Heming Cui

The University of Hong Kong

Shanghai AI Laboratory

heming@cs.hku.hk

&Jie M. Zhang

King's College London

jie.zhang@kcl.ac.uk

Equal Contribution.Corresponding Author.

###### Abstract

Code generation models have increasingly become integral to aiding software development. Although current research has thoroughly examined the correctness of the code produced by code generation models, a vital aspect that plays a pivotal role in green computing and sustainability efforts -- the efficiency of the generated code -- has often been neglected. This paper presents EffBench, a benchmark with 1,000 efficiency-critical coding problems to assess the efficiency of code generated by code generation models. EffBench contains a diverse set of LeetCode coding problems. Each problem is paired with an executable human-written canonical solution, which obtains the SOTA efficiency on the LeetCode solution leaderboard. With EffBench, we empirically examine the ability of 42 large language models (35 open-source and 7 closed-source) in generating efficient code. Our evaluation results demonstrate that the efficiency of the code generated by LLMs is generally worse than the efficiency of human-written canonical solutions. For example, GPT-4 generated code has an average **3.12** times execution time that of the human-written canonical solutions. In the most extreme cases, the execution time and total memory usage of GPT-4 generated code are **13.89** and **43.92** times that of the canonical solutions. The source code of EffBench is released on https://github.com/huangd1999/EffiBench. We also provide the LeaderBoard in https://huggingface.co/spaces/EffiBench/effibench-leaderboard.

## 1 Introduction

Large language models (LLMs), such as GPT-4  and Copilot , have become increasingly popular for assisting software developers with various tasks such as program repair [18; 26], automated testing [29; 14], and code translation [44; 3]. LLMs generate code based on instructions and offer intelligent recommendations, boosting developers' productivity. Various benchmarks have been proposed to evaluate the **correctness** of code generation. Notable examples include HumanEval , APPS , BigCodeBench , and DS-1000 , which cover basic programming, competition-level, and data science tasks. These benchmarks have been widely used to assess the code generation capabilities of LLMs.

Despite advancements in ensuring code correctness, there remains a significant gap in the literature regarding the efficiency of code produced by LLMs . The importance of efficiency cannot be understated, as it directly impacts the speed of execution and the utilization of memory, which is especially important in resource-constrained environments such as mobile devices or embedded systems . **Efficiency** of code is crucial for building scalable and sustainable software to meet the growing demands of the digital world. Furthermore, efficient code plays a pivotal role in green computing and sustainability efforts. By optimizing algorithms and reducing computational overhead, we can significantly lower energy consumption and carbon footprint. This is particularly relevant as the global demand for digital services increases.

The efficiency of two correctly generated code snippets for the same task can vary significantly. Consider the example in Figure 1, where Copilot and GPT-4 are tasked with merging two sorted arrays. Copilot generates a function that concatenates the arrays and then applies a basic Bubble Sort algorithm. While functionally correct, this approach suffers from sub-optimal time complexity of \(O((n+m)^{2})\) and space complexity of \(O(n+m)\), where \(n\) and \(m\) are the array lengths. In contrast, GPT-4 generates a function that efficiently merges the arrays by systematically comparing and appending elements from each array in a single pass. This method achieves a time complexity of \(O(n+m)\), exhibiting a linear relationship with the combined lengths of the arrays. Its space complexity remains \(O(n+m)\). The disparity in efficiency highlighted in Figure 1 underscores the critical need to benchmark code generation from the perspective of code efficiency.

While being intuitive, using existing code generation benchmarks like HumanEval  and MBPP  to assess code efficiency has several limitations. These efforts primarily focus on correctness, often featuring simple tasks solvable with short code snippets. This simplicity can lead to indistinguishable efficiency across different LLMs, making it difficult to discern meaningful differences in their performance. Furthermore, most tasks are not inherently efficiency-critical, making any observed efficiency discrepancies less significant. Finally, these benchmarks lack comprehensive and diverse

Figure 1: Example codes with distinct time complexity generated by Copilot and GPT-4, respectively. Code accessed on January 15, 2024.

test cases that can thoroughly evaluate code efficiency under varying and substantial computational loads. Consequently, they are inadequate for assessing the efficiency of code generation.

This paper introduces EffiBench, a benchmark specifically designed for evaluating the efficiency of the code that is automatically generated. EffiBench comprises 1,000 efficiency-critical code generation problems selected from LeetCode. Each coding problem is paired with an executable manually-written canonical solution which has been awarded the highest rating on LeetCode for its optimal time and space efficiency. We also develop a test case generator to produce a vast number of test cases for each problem to allow for an in-depth and comprehensive analysis of the code efficiency. Moreover, EffiBench integrates a diverse set of efficiency metrics, such as execution time, maximum memory usage, and total memory usage during execution.

We conduct a comprehensive study to evaluate the efficiency of code generated by 42 LLMs. Our findings reveal that among both open- and closed-source LLMs, StarCoder2-15B  and GPT-4 consistently produced the most efficient code. Nevertheless, even these top performers still lag behind the efficiency of human-written canonical solutions. For instance, GPT-4 generated code exhibits an average execution time that is 3.12 times that of the human-written canonical solutions. In the most extreme cases, the execution time and total memory usage of GPT-4 code are **13.89** and **43.92** times that of the canonical solutions, respectively. Furthermore, our analysis reveals that a high pass@1 score (indicating the LLM's ability to generate correct code on the first attempt) does not necessarily translate to more efficient code. For example, GPT-4-turbo-preview has a higher pass@1 score than GPT-4, but lower code efficiency.

To conclude, this paper makes the following contributions:

* We introduce EffiBench, the first benchmark specifically designed to assess the efficiency of code generated by LLMs.
* We conduct an extensive evaluation of 42 LLMs on EffiBench, revealing that even state-of-the-art LLMs (e.g. GPT-4) exhibit significant inefficiencies compared to optimal human-written solutions.
* We release an efficiency testing framework3, which enables evaluating the efficiency across various code generation benchmarks (See Appendix A.9).

## 2 Related Work

### LLMs for Code

The burgeoning interest in LLMs for code has coincided with the profusion of openly available code repositories and the pressing need to enhance the productivity of software developers. Initial models predominantly focused on code generation tasks have included AlphaCode , CodeGen , CodeT5+ , InCoder , StarCoder , SantaCoder  and DeepSeek Coder , all of which were trained on code. Contrastingly, models such as Codex , Astraios , and CodeLAMA  represent a subsequent stride, having been fine-tuned from foundation models [10; 49]. The evolution continued as LLMs leveraged instruction-like datasets derived from GPT [41; 42] for fine-tuning. Among these, WizardCoder  and Phi-3  are notable examples. Across various coding applications, these code LLMs have set new standards of excellence, showcasing their prowess in domains including program repair [18; 26], automated testing [29; 14; 22; 24; 23], code translation [44; 3], type prediction [37; 54], and code summarization [20; 4].

### Code Generation Benchmarks

Code generation [7; 12; 61; 55; 59] has emerged as a vital domain for evaluating LLMs, where models generate code snippets based on natural language descriptions, often given in the form of docstrings. Recent works try to improve HumanEval and MBPP from different perspectives. For example, HumanEval+  enhances HumanEval with improved test cases, remedying the issue of mistakenly accepted faulty solutions. Meanwhile, ReCode  takes a different approach by altering function names and docstrings within the HumanEval structure. Expanding the scope beyond Python, HumanEval-X , MultiPLe , and MBXP  extend the HumanEval and MBPP benchmarks to incorporate a variety of programming languages. The universe of code generation benchmarks widens further when we consider the specialized needs of data science. DS-1000 , ARCADE , NumpyEval , and PandasEval  focus on the generation of code within this context. Beyond mere code creation, there are benchmarks like APIBench , MTPB , RepoBench , ODEX , SWE-Bench , GoogleCodeRepo , RepoEval , and Cocomic-Data , which ratchet up the complexity by evaluating a model's prowess in utilizing APIs or completing broader software engineering tasks. Recent studies [46; 40] have indicated that code generated by LLMs tends to be less efficient in terms of execution time and memory usage compared to canonical solutions. To bridge this gap, our benchmark EffiBench is specifically designed to evaluate the efficiency of code generation4.

## 3 Benchmark Construction

### Efficiency-critical Problem Collection

Coding problem collectionInspired by the common practice [9; 19; 8] of using LeetCode problems to evaluate human developers' abilities in writing efficient algorithms, we collect the coding problems that appear on LeetCode. Specifically, we collect all problems tagged with "LeetCode" on the HuggingFace platform. We remove duplicate problems with identical problem IDs (each project has a unique ID in LeetCode). We also remove problems whose interview frequencies are lower than 40% at LeetCode. In the end, we obtain 2,605 problems as initial problem candidates.

Efficiency-critical problem filteringThis step selects efficiency-critical problems from the initial 2,605 problem candidates. The problems collected from HuggingFace are not tagged with algorithm topics. Therefore, we map each problem in LeetCode and label the problem with the "Topic" tag provided by LeetCode. We then choose typical algorithms (Table 1) that are introduced in common algorithm textbooks , which are also the most widely covered in Leetcode. This yields 1,146 problems altogether.

### Canonical Solution Construction

For each coding problem, EffiBench provides an executable canonical solution to serve as a baseline to calculate the normalised efficiency. Drawing inspiration from DS-1000 , which collects canonical solutions based on the most starred responses on Stack Overflow, we begin with collecting the top-starred solutions for each problem from the LeetCode Discussion Forum. For each collected solution, we need to guarantee that they are executable in a non-Leetcode environment. To this end, we manually fix the solutions that need to import extra classes such as TreeNode and ListNode as well as extra packages such as List and Bisect. We also remove the solutions that require specialized packages implemented only by LeetCode. In the end, we managed to map executable canonical solutions for 1,000 coding problems, which then be regarded as our final efficiency dataset.

### Test Case Generation

It is essential to have adequate and diverse test cases to evaluate a program's efficiency across various scenarios. Since directly generating test cases with LLMs (e.g., GPT-3.5) requires large token overhead and has a low accuracy (See Appendix A.26), we develop a test case generator for each coding problem as an integral part of our benchmark construction. In particular, we require GPT-3.5-turbo to produce the test case generator, which is prompted to generate massive test cases with different input sizes, data distribution, and edge cases. Users can decide how many tests they

   Algorithm & Greedy & DP & Backtracking & Dimit & Emb-Canger & D/S & H/S & H/S & H/S & H/S & H/S & H/S & H/S \\  Number of operations & 240 & 277 & 48 & 21 & 366 & 148 & 198 & 39 & 162 & 238 & 1000 \\ Number of key problems & 32 & 8 & 1 & 4 & 18 & 8 & 23 & 59 & 9 & 26 & 63 & 711 \\ Number of Machine patterns & 73 & 155 & 37 & 8 & 72 & 52 & 59 & 9 & -5 & 98 & 133 & 890 \\ Number of field problems & 41 & 118 & 10 & 9 & 18 & 26 & 30 & 7 & 14 & 18 & 42 & 340 \\ Avg. length of reference acquisition & 224.8 & 216.4 & 102.0 & 285.1 & 218.9 & 229.1 & 216.4 & 199.6 & 184.3 & 105.0 & 223.1 & 212.0 \\ Avg. loss of Leet Cascade Selection & 12.6 & 151.1 & 193.3 & 182.2 & 20.8 & 227.7 & 14.4 & 13.0 & 14.6 & 172.8 & 12.0 & 14.6 \\   

Table 1: Statistics of EffiBench with different algorithms.

would like to generate for each problem. We also provide 100 tests within EfiBench for users to use directly, which also serve as the tests in our evaluation in this paper (Results with 10 tests and 1,000 tests are shown in Appendix Table 24).

### Efficiency Metrics

Efficiency metrics are crucial for benchmarking code generation models automatically. Following LeetCode, we design automatic efficiency metrics from two aspects: execution time and memory usage. Specifically, we use the following metrics: Execution Time (ET), Normalized Execution Time (NET), Max Memory Usage (MU), Normalized Max Memory Usage (NMU), Total Memory Usage (TMU), and Normalized Total Memory Usage (NTMU) to measure the overall capability of a code generation model in generating efficient code.

Execution Time (ET)Execution time (ET) measures the average time taken for code execution. Mathematically, ET is defined as:

\[ET=^{N}T_{}\]

where \(ET\) is the execution time metric, \(T_{}\) is the execution time of the code (with all the test cases), and \(N\) is the number of codes generated by code generation models used for evaluation.

Normalized Execution Time (NET)Normalized Execution Time (NET)5 measures the execution time required by generated code relative to that of a canonical solution. We define NET as:

\[NET=^{N}}}{T_{}}\]

where \(T_{}\) is the execution time of the generated code and \(T_{}\) is the execution time of the canonical solution. A NET value greater than 1 indicates that the generated code is slower than the canonical solution, while a value less than 1 suggests the generated code is faster.

Max Memory Usage (MU)Max Memory Usage (MU) measures the average max memory consumption during code execution. Mathematically, MU is defined as:

\[MU=^{N}M_{}\]

where \(MU\) is the memory usage metric, \(M_{}\) is the max memory consumption of the generated code among all the test cases, and \(N\) is the number of code instances generated by code generation models used for evaluation. This metric is critical to assess the resource efficiency of generated code, particularly in environments with limited maximum memory capacity.

Normalized Max Memory Usage (NMU)Normalized Max Memory Usage (NMU) quantifies how the max memory efficiency of the generated code compares to the canonical solution. We define NMU as:

\[NMU=^{N}}}{M_{}}\]

where \(NMU\) is the normalized max memory usage metric, \(M_{}\) is the max memory usage of the generated code, and \(M_{}\) is the max memory usage of the canonical solution. An NMU value less than 1 indicates that the generated code is more memory-efficient than the canonical solution, whereas a value greater than 1 suggests it is less efficient in terms of memory usage. This metric provides a relative measure of the memory optimization in the generated code in comparison to a standard baseline.

Total Memory Usage (TMU)Total Memory Usage (TMU)assess the efficiency of memory usage throughout the execution of code, taking into account both the magnitude and duration of memory utilization. To calculate TMU, first, monitor and record the memory usage at discrete time intervals during the execution, resulting in a memory usage profile \(M(t)\), where \(t\) represents time. Then, compute the area under the curve of \(M(t)\) over the total execution time, \(T_{}\), using numerical integration methods such as the trapezoidal rule:

\[TMU=^{N}_{0}^{T_{}}M(t)\,dt\]

A lower TMU value indicates higher memory efficiency, reflecting an optimized balance between the amount of memory used and the duration of its usage.

Normalized Total Memory Usage (NTMU)The Normalized Total Memory Usage (NTMU) offers a comparison of the dynamic memory efficiency between the generated code and the canonical solution. To determine NTMU, calculate the TMU for both the generated code and the canonical solution. Normalize the TMU of the generated code by dividing it by the TMU of the canonical solution:

\[NTMU=^{N}}}{TMU_{}}\]

where \(TMU_{}\) is the TMU of the generated code and \(TMU_{}\) is the TMU of the canonical solution. An NTMU value less than 1 signifies that the generated code manages dynamic memory more efficiently compared to the canonical solution, while a value greater than 1 indicates less efficient management of dynamic memory. This metric provides insight into the relative use of dynamic memory of generated code compared to an established benchmark.

## 4 Benchmark Statistics

We provide the detailed statistics of the dataset in Table 1. The coding problems in EsfiBench have three difficulty levels (171 easy-level, 589 medium-level, and 240 hard-level problems), where the difficulty of each problem is defined by LeetCode . The table lists the number of problems for each algorithm. Specifically, EsfiBench contains 243 problems for the greedy algorithm, 277 for dynamic programming (DP), 48 for backtracking, 21 for divide and conquer, 108 for depth-first search (DFS), 86 for breadth-first search (BFS), 148 for binary search, 105 for two pointers, 70 for sliding window, 102 for bit manipulation and 238 for sorting algorithm. The sum of problems in different algorithms can be larger than the number of total problems because one problem in our dataset may belong to two algorithm classes. On average, a problem description in EsfiBench contains 212.0 words. The canonical solutions, which represent the baseline code against which the generated code is compared, have 14.6 lines on average.

We provide a comparison of EsfiBench and other code generation datasets in Table 2. Specifically, we compare EsfiBench with the five most widely used code-related datasets (i.e., HumanEval, MBPP, APPS, DSP, and DS-1000). Different from the previous dataset that focuses on analyzing whether the code passes all test cases, EsfiBench also analyzes the efficiency during the code execution procedure. Although EsfiBench is primarily designed to assess the efficiency of generated code, it can also serve to evaluate code correctness, akin to other code generation datasets.

## 5 Evaluation

By default, the experiments are conducted in an edge server with an Intel Xeon Platinum 8336C CPU with 128 cores, 8 * NVIDIA A100-SXM GPUs, and a total memory capacity of 2.0TiB. We set the timeout for each code execution as 10 (s). The main goal of our work is to provide a benchmark that evaluates the efficiency of LLM-generated code within an identical environment, and we do expect that with different environments, the absolute values of the efficiency metrics would be different. We report results with different environments in Table 26, where our evaluation results demonstrate that despite the differences in absolute values, the ranking of LLMs is rather stable (p-value\(>>0.05\) based on Kruskal-Wallis H tests). Besides, to provide a more reliable evaluation framework, we have also provided a server in the Hugging Face Space, where users can directly upload the code generation JSON file and then the server will execute the code locally and report the efficiency results with the same environment in the future.

Models:We evaluate both open- and closed-source LLMs in code generation. For open-source models, we evaluate EfiBench with CodeLlama-hf family (i.e., 7B, 13b, 34b, and 70B), CodeLlama-Instruct-hf family (i.e., 7B, 13b, 34b, and 70B), deepseek-coder-instruct (i.e., 1.3B and 6.7B) and base models (i.e., 6.7B and 33B), Phind-CodeLlama-34B (i.e., v1 and v2), starcoder, starcoderbase, and starcoder2 (i.e., 3B, 7B, and 15B), WizardCoder (i.e., 13B and 15B), XwinCoder (i.e., 13B and 34B), Yi models (34B, 34B-Chat, and 200K version), and five widely proposed SOTA models, i.e., Magicoder-6.7B, Mistral-7B, octocoder, Artigenz-6.7B, CodeFuse-33B, and codegemema-7b6 since these open-source models have obtained SOTA pass@1 in the HumanEval and MBPP datasets. For closed-source models, we evaluated EfiBench with GPT-3.5, GPT-4 , and claude-3, since we observe that these models obtain high pass@1 in code generation datasets (e.g., HumanEval , MBPP ). For GPT-3.5 models, we experiment with GPT-3.5-turbo-0301, GPT-3.5-turbo-0613, and GPT-3.5-turbo-1106 which represent three different versions of the GPT-3.5. For GPT-4 models, we experiment with GPT-4-turbo and GPT-4 (GPT-4-0613). For the claude-3 model, we evaluate the sonnet and haiku versions. For each LLM, we first collect the code that is correctly generated for each coding problem (i.e., they can pass all test cases provided by the dataset), then execute these correct code and calculate the efficiency metrics (See Section 3.4).

Prompt:Our prompt follows the MBPP code generation prompt, where the prompt first provides the task description and then provides a few examples with input and output pairs. Each example has an explanation of the rationality of the output. The prompt also has the assertion part, which intends to constrain the function signature with the input and output format.

### End2End Results

Open-source modelsThe evaluation results of open-source models are illustrated in Table 3. Our evaluation results demonstrate that **all open-source models' generated code requires more overhead than the human-written canonical solutions**. For example, StarCoder2-15B, the most efficient open-source model in terms of NET, NMU, and NTMU, on average still needs 2.59x execution time, 1.71x max memory usage (i.e., memory peak), and 4.83x total memory usage during the code execution compared with the canonical solutions. We suspect that this is because human-written canonical solutions, while optimal, are in the minority within the training data of these LLMs. Consequently, the LLMs tend to learn non-optimal solutions, which are more frequently distributed in the training data. In addition, our results demonstrate that open-source LLMs with lower pass@1 tend to have better efficiency. The key reason is that these LLMs can only generate correct code on relatively simple problems, which makes it easier to achieve efficiency compared to more complex and challenging problems (see Table 27-29).

Closed-source modelsThe evaluation results of closed-source models are demonstrated in the bottom part of Table 3. Our results illustrate that similar to open-source models, all closed-source models generated code still need more overhead than the canonical solution on average. Despite GPT-4 generated code obtaining the most efficient results for closed-source models, its generated code still needs on average 3.12x execution time and 6.36x total memory usage during the code execution compared with the canonical solution. In the worst case, the execution time is almost 14x that of the canonical solution. In addition, **although consistent training can improve the

   Dataset & Number of Problems & Evaluation Support & Avg. Test Cases & Avg. Lines of Canonical Solution & Data Source & Assessment \\  HumanEval & 164 & Test Cases & 7.7 & 6.3 & Hand-Writen & Correctness \\ MBPP & 974 & Test Cases & 3.0 & 6.7 & Crowd-ourced & Correctness \\ APPS & 10000 & Test Cases & 13.2 & 18.0 & Competitions & Correctness \\ DSP & 1119 & Test Cases & 2.1 & 4.5 & Notebooks & Correctness \\ DS-1000 & 1000 & Test Cases & 1.6 & 3.6 & StackOverflow & Correctness \\ 
**ExpiBench** (Ours) & 1000 & Test Cases + Efficiency & Self-defined & 14.6 & LetterCode & Efficiency and Correctness \\   

Table 2: Comparison of EfiBench to other code generation benchmarks. In addition to test cases, EfiBench provides efficiency metrics and analysis for code generation models.

correctness of LLM-generated code, the efficiency of LLM-generated code may not improve. For example, the pass@1 for GPT-3.5-turbo increases from 42.3% to 49.3% when the model version is updated from 0301 to the 1106 version, the execution time of the code generated by GPT-3.5-turbo increases from 3.18x to 3.40x.

**Consistency of different metrics**: When we compare the benchmarking results from different efficiency metrics, we can observe that the rankings of different LLMs from the basic metrics (highlighted in bold in the head row) maintain a general consistency. For example, in closed-source models, GPT-4 obtains the most efficient results in the majority of metrics. Yet, for other metrics where GPT-4 does not get the highest efficiency, the code generated by GPT-4 is also close to the most efficient LLM-generated ones. This consistency across metrics reinforces their credibility in assessing a model's capability to generate efficient code.

**Correctness**: Although EffiBench is designed to focus on benchmarking efficiency of LLM-generated code, it can also be adapted to benchmark code correctness, as shown by pass@1 in the last column of Table 3. For open-sourced LLMs, our results demonstrate that they have low pass@1:

   Model & max NET & NET & NT-5 & ET (s) & max NUM & NUM & NUM & NUM-5 & MU (Mb) & max NTMU & NTMU & NTMU-5 & TMU (Mb*) \\  grg-3.5-turbo-0301 & 16.24 & 3.10 & 0.5 & 0.37 & **2.08** & 1.90 & 0.0 & 66.91 & 46.95 & 6.32 & 88.6 & 20.99 \\ grg-3.5-turbo-0613 & 4.05 & **20.88** & 0.37 & 2.64 & 1.90 & 0.0 & 66.99 & 10.21 & 6.18 & 89.5 & 20.92 \\ grg-4-turbo-0613 & 6.12 & 3.07 & 0.5 & 0.37 & 2.06 & 1.90 & 0.0 & 66.94 & 15.53 & 6.22 & 89.0 & **20.78** \\ grg-4 & **4.80** & 3.06 & **0.07** & 2.06 & 1.91 & 0.0 & 66.91 & 9.22 & 6.17 & **89.0** & 21.17 \\ grg-4-turbo-preview & 4.99 & 3.10 & 0.0 & 0.37 & 2.05 & 1.90 & 0.0 & 66.92 & **89.2** & 6.28 & 89.0 & 20.78 \\ claudio-3-shake & 11.06 & 3.27 & 0.5 & 0.39 & 2.05 & 1.90 & 0.0 & **66.90** & 29.88 & 6.68 & 89.0 & 22.52 \\ claudio-3-sconnet & 17.43 & 3.20 & 0.5 & 0.38 & 2.06 & 1.90 & 0.0 & 66.93 & 50.78 & 6.55 & 89.0 & 21.52 \\   

Table 4: Efficiency results of closed-source LLMs with 210 problems correctly addressed by all models in the Table. Although GPT-3.5-turbo models have the same ET (i.e., 0.37s), the NET is not the same since the task level NET does not have the same distribution (e.g., the max NET of the 0301 model is 16.24x while it only requires 4.05x in 0613 model).

   Model & max NET & NET & NET-5 & ET (s) & max NUM & NUM & NUM-5 & MU (Mb) & max NTMU & NTMU & NTMU-5 & TMU (Mb*) & Pass@1 \\   \\  CodedLlam-70-bf & 3.25 & 2.95 & 0.0 & 0.31 & 2.05 & 1.98 & 0.0 & 48.59 & 6.80 & 6.00 & 100.0 & **9.99** & 1.1 \\ CodedLlam-13-bf & 3.21 & 2.71 & 0.0 & 0.40 & 2.05 & 1.85 & 0.10 & 104.42 & 6.53 & 5.32 & 81.8 & 43.83 & 1.1 \\ CodedLlam-30-bf & 4.46 & 2.98 & 0.9 & 0.34 & 2.06 & 1.92 & 0.0 & 55.38 & 9.17 & 60.1 & 92.9 & 13.41 & 8.4 \\ CodedLlam-70-bf & 13.39 & 3.19 & 4.4 & 0.42 & 2.06 & 1.90 & 0.0 & 62.41 & 23.20 & 64.7 & 87.8 & 22.27 & 9.0 \\  CodedLlam-70-bf & 17.25 & 4.44 & 4.2 & 0.46 & 1.94 & 0.7 & 7.87 & 65.61 & 7.56 & 87.5 & 32.4 & 4.8 \\ CodedLlam-13-bf-13-bf-13-bf-13-bf-13-bf-13-13-bf-13-13-bf-13-13-13-bf-13-13-13-bf-13-13-13-bf-1

[MISSING_PAGE_FAIL:9]

### Worst Case Analysis

In this section, we conduct a study to analyze the inefficient code generated by GPT-3.5-turbo-0301 (similar to the analysis in Section 5.3). Specifically, we collect the 10 most inefficient pieces of code for NET, NMU, and NTMU metrics and then manually analyze the implementation algorithm used by each code. The evaluation results are demonstrated in Table 6. The evaluation results demonstrate that the majority of the inefficient pieces of code are associated with DP and backtracking algorithms, with these categories showing the highest occurrences across the metrics. In particular, DP and backtracking algorithms show the highest counts in NTMU, indicating that these algorithms tend to generate code with higher memory consumption inefficiency, which highlights the areas where GPT-3.5-turbo-0301 struggles the most, suggesting a need for further optimization in generating code for complex algorithmic tasks.

To further understand the reasons for inefficiency in the LLM-generated code, we conduct a case comparison of GPT-3.5-turbo-0301 generated code and canonical solution in DP subset to analyze why LLM-generated code is inefficient. As shown in Figure 2, we can observe that the key reason for GPT-3.5-turbo-0301 being less efficient than the _canonical_solution_ is due to the code generated by GPT-3.5-turbo-0301 first generating a 2-dimensional matrix which requires large overhead for memory usage when the parameters \(n\) and \(k\) are very large. However, the _canonical_solution_ generates two lists, which significantly reduces the memory usage for the code. GPT-3.5-turbo-0301 implements a straightforward dynamic programming approach with a complete matrix to keep track of results for every possible pair of \(n\) and \(k\), while the canonical solution optimizes by maintaining a rolling sum, which helps to reduce the space complexity from \(O(n k)\) to \(O(k)\), leading to a more memory-efficient implementation. This optimization in the canonical solution results in a significant performance improvement. Specifically, GPT-3.5-turbo-0301 generated code has 70.62x memory usage during the code execution compared with _canonical_solution_.

## 6 Conclusion and Future work

In this paper, we introduce EffiBench, a benchmark designed to evaluate the efficiency of code generated by various code generation models. EffiBench encompasses 1,000 problems and consists of 11 distinct algorithmic subsets. Unlike previous benchmarks that primarily emphasize the correctness of code generation, EffiBench extends the evaluation criteria to include both execution time analysis and memory usage analysis. We also provide the evaluation server in Hugging Face to allow researchers to evaluate their methods with the same hardware and software. By incorporating these metrics and the Hugging Face server, EffiBench aims to inspire the research community's focus towards not only the correctness but also the efficiency and sustainability of code generated by code generation models. In the future, we will consider extending EffiBench with other programming languages (e.g., C++, Java, JS, and Go).

## 7 Acknowledgment

The work is supported in part by National Key R&D Program of China (2022ZD0160201), HK RGC RIF (R7030-22), HK ITF (GHP/169/20SZ), a Huawei Flagship Research Grant in 2023, HK RGC GRF (Ref: 17208223 & 17204424), and the HKU-CAS Joint Laboratory for Intelligent System Software.

   Metrics & Greedy & DP & Backtracking & Divide and Congor & DFS & BFS & Binary Search & Two Pointers & Sliding Window & Bit Manipulation & Sreting \\  NET & 0 & 1 & 2 & 0 & 0 & 1 & 0 & 1 & 2 & 3 & 0 \\ NMU & 1 & 1 & 1 & 0 & 0 & 1 & 0 & 1 & 1 & 2 & 2 \\ NTMU & 3 & 4 & 1 & 0 & 0 & 1 & 0 & 0 & 0 & 1 & 0 \\   

Table 6: Evaluation results of Top-10 inefficient code generated by GPT-3.5-turbo-0301. We manually analyze the algorithm of each code.