# Entropy-based Training Methods for Scalable Neural Implicit Sampler

Weijian Luo

School of Mathematical Sciences; Peking University; liuoweijian@stu.pku.edu.cn;

Boya Zhang

Academy for Advanced Interdisciplinary Studies; Peking University; zhangboya@pku.edu.cn;

Zhihua Zhang

School of Mathematical Sciences; Peking University; zhzhang@math.pku.edu.cn;

###### Abstract

Efficiently sampling from un-normalized target distributions is a fundamental problem in scientific computing and machine learning. Traditional approaches such as Markov Chain Monte Carlo (MCMC) guarantee asymptotically unbiased samples from such distributions but suffer from computational inefficiency, particularly when dealing with high-dimensional targets, as they require numerous iterations to generate a batch of samples. In this paper, we introduce an efficient and scalable neural implicit sampler that overcomes these limitations. The implicit sampler can generate large batches of samples with low computational costs by leveraging a neural transformation that directly maps easily sampled latent vectors to target samples without the need for iterative procedures. To train the neural implicit samplers, we introduce two novel methods: the KL training method and the Fisher training method. The former method minimizes the Kullback-Leibler divergence, while the latter minimizes the Fisher divergence between the sampler and the target distributions. By employing the two training methods, we effectively optimize the neural implicit samplers to learn and generate from the desired target distribution. To demonstrate the effectiveness, efficiency, and scalability of our proposed samplers, we evaluate them on three sampling benchmarks with different scales. These benchmarks include sampling from 2D targets, Bayesian inference, and sampling from high-dimensional energy-based models (EBMs). Notably, in the experiment involving high-dimensional EBMs, our sampler produces samples that are comparable to those generated by MCMC-based methods while being more than 100 times more efficient, showcasing the efficiency of our neural sampler. Besides the theoretical contributions and strong empirical performances, the proposed neural samplers and corresponding training methods will shed light on further research on developing efficient samplers for various applications beyond the ones explored in this study.

## 1 Introduction

Efficiently sampling from un-normalized distributions is a fundamental challenge that arises in various research domains, including Bayesian statistics , biology and physics simulations [2; 3], as well as generative modeling and machine learning [4; 5]. The task at hand involves generating batches of samples from a target distribution defined by a differentiable un-normalized potential function, denoted as \( q()\). This problem seeks effective techniques for obtaining samples that accurately represent the underlying target distribution while minimizing computational costs.

Two main classes of methods exist for addressing this challenge. The first class comprises Markov Chain Monte Carlo (MCMC) algorithms [6; 7; 8; 9]. MCMC methods involve the design of Markov Chains with stationary distributions that match the target distribution. By simulating these MarkovChains, it becomes possible to transition a batch of initial samples towards samples approximately distributed following the target distribution. While MCMC methods offer asymptotically unbiased samples, they often suffer from computational inefficiency, particularly when dealing with high-dimensional target distributions. This inefficiency stems from the need to perform a significant number of iterations to generate a single batch of samples.

The second class of methods, known as learning to sample (L2S) models [10; 11; 12; 13; 14; 15; 16; 17; 18], focuses on training neural components, such as generative models, to generate samples following the target distribution. L2S models aim to improve the quality and efficiency of sample generation compared to traditional training-free algorithms like MCMCs. Among the L2S models, the neural implicit sampler stands out due to its remarkable efficiency. The neural implicit sampler is an L2S model that incorporates a neural transformation \(=g_{}()\), where \(\) is an easy-to-sample latent vector from a known distribution \(p_{z}()\). This transformation allows the sampler to generate samples directly from the target distribution without the need for iterations or gradient operations, which are commonly required in MCMCs. Such a sampler offers significant advantages in terms of efficiency and scalability, particularly when dealing with high-dimensional distributions such as those found in image spaces. However, training a neural implicit sampler poses technical challenges due to the intractability of sampler distributions that are caused by complex neural architectures of \(g_{}\). Achieving effective training requires careful consideration of various factors and the development of appropriate training strategies.

In this paper, we introduce two innovative training approaches, namely the KL training methods and the Fisher training methods, for training implicit samplers. We theoretically demonstrate that our proposed KL and Fisher training methods are equivalent to minimizing the Kullback-Leibler divergence and the Fisher divergence , respectively, between the sampler and the target distribution. To evaluate the effectiveness, efficiency, and scalability of our proposed training approach and sampler, we conduct experiments on three different sampling benchmarks that span various scales. These benchmarks include sampling from 2D targets with low dimensions, Bayesian inference with moderate dimensions, and sampling from high-dimensional energy-based models (EBMs) trained on the MNIST dataset. The results of experiments consistently demonstrate the superior performance of our proposed method and the quality of the generated samples. Notably, in the EBM experiment, our sampler trained with the KL training method achieves sample quality comparable to that of the EBM model while being more than 100 times more efficient than traditional MCMC methods. These findings highlight the effectiveness, efficiency, and scalability of the proposed approach across different sampling scenarios.

## 2 Backgrounds

### Generative Models for Sampling

Generative models have emerged as powerful tools in various domains, demonstrating their ability to produce diverse, high-quality samples. They have been successfully applied in tasks such as text-to-image generation [20; 21; 22; 23; 24; 25; 26; 27; 28], audio generation, video and 3D creation[30; 31; 32; 33], and even molecule design [34; 35]. Recently, there has been a growing interest in leveraging generative models for sampling from target distributions.

Specifically, consider the problem that we have access to an un-normalized target distribution \(q()\), or its logarithm \( q()\). Our objective is to train generative models that can effectively generate samples following the target distribution. By learning the underlying structure of the target distribution, these generative models can generate samples that capture the characteristics of the target distribution, facilitating tasks such as posterior inference in Bayesian statistics.

Three primary classes of generative models have been extensively studied for sampling tasks. The first class comprises normalizing flows (NFs) , while the second class consists of diffusion models (DMs) . NFs employ invertible neural transformations to map Gaussian latent vectors \(\) to obtain samples \(\). The strict invertibility of NF transformations enables the availability of likelihood values for generated samples, which are differentiable with respect to the model's parameters. Training NFs often involves minimizing the KL divergence between the NF and the target distribution . On the other hand, DMs employ neural score networks to model the marginal score functions of a data-initialized diffusion process. The score functions are learned using techniques related to score-matching . DMs have been successfully employed to enhance the sampler per formance of the annealed importance sampling algorithm , a widely recognized MCMC method for various sampling benchmarks. Despite the successes of NFs and DMs, both models have their limitations. The invertibility of NFs restricts their expressiveness, which can hinder their ability to effectively model high-dimensional targets. Moreover, DMs still require a considerable number of iterations for sample generation, resulting in computational inefficiency.

The third class is the implicit generative model. An implicit generative model (IGM) uses a flexible neural transform \(g_{}(.)\) to push forward easy-to-sample latent vectors \( p_{z}\) to obtain samples \(=g_{}()\). The main difference between IGMs and NFs is that IGMs' neural transformation is not required to be strictly invertible, which unlocks both the flexibility and the modeling power of deep neural networks. For instance, Hu et al.  proposed to train implicit samplers by minimizing the Stein discrepancy between the sampler and the target distribution. The Stein Discrepancy (SD)  between \(p\) and \(q\) is defined as

\[_{SD}:=_{}_{p} _{} q(),()+_{d=1}^{D}_{d}}f_{d}()}.\]

The notation \(D\) represents the dimension of data space and \(f_{d}(.)\) is the \(d\)-th component of the vector-valued function \(\). The calculation of Stein's discrepancy relies on solving a maximization problem w.r.t. the test function \(\). When the function class \(\) is carefully chosen, the optimal \(\) may have an explicit solution or easier formulation. For instance, Hu et al.  found that if \(\) is taken to be \(=\{_{ p}\|()\|_{2}^{2}\}\), the SD is equivalent to a regularized representation

\[_{SD}(p,q)= _{f}_{ p}_{} q(),()+_{d=1}^{D}_{d}}f_{d}()-\|()\|_{2}^{2}}.\]

They used two neural networks: \(g_{}\) to parametrize an implicit sampler and \(_{}\) to parametrize the test function. Let \(p_{}()\) denote the implicit sampler distribution induced by \(=g_{}()\) with \( p_{z}()\). They solved a bi-level mini-max problem on parameter pair \((,)\) to obtain a sampler that tries to minimize the SD with

\[_{}_{}& L(, )\\ \] (1)

Hu et al.  opened the door to training implicit samplers by minimizing divergences which are implemented with a two-networks bi-level optimization problem. Since the general Stein's discrepancy does not have explicit formulas, in our paper we study training approaches of an implicit sampler by minimizing the KL divergence and the Fisher divergences. For the implementation of minimization of Fisher divergence, some backgrounds of score function estimation are needed as we put in Section 2.2.

### Score Function Estimation

Since the implicit sampler does not have an explicit log-likelihood function or score function, training it with score-based divergence requires inevitably estimating the score function (or equivalent component). Score matching  and its variants provided practical approaches to estimating score functions. Assume one only has available samples \( p\) and wants to use a parametric approximated distribution \(q_{}()\) to approximate \(p\). Such an approximation can be made by minimizing the Fisher Divergence between \(p\) and \(q_{}\) with the definition

\[_{FD}(p,q_{}) _{ p}\|_{} p()\|_{2}^{2}+\|_{} q_{}()\|_{2}^{2}-2_{ } p(),_{} q_{}()}.\]

Under certain conditions, the equality

\[_{ p}_{} p(),_{}  q_{}()=-_{ p}_{} q_{ }()\]

holds (usually referred to as Stein's Identity). Here \(_{} q_{}()=_{i}}{_{i}^{2}} q_{}()\) denotes the Laplacian operator applied on \( q_{}()\). Combining this equality and noting that the first term of FD \(_{ p}\|_{} p()\|_{2}^{2}\) does not rely on the parameter \(\), minimizing \(_{FD}(p,q_{})\) is equivalent to minimizing the following objective

\[_{SM}()=_{ p}\|_{} q _{}()\|_{2}^{2}+2_{} q_{}()}.\]

This objective can be estimated only through samples from \(p\), thus is tractable when \(q_{}\) is well-defined. Moreover, one only needs to define a score network \(_{}()^{D}^{D}\) instead of a density model to represent the parametric score function. This technique was named after Score Matching. Other variants of score matching were also studied [43; 44; 45; 46; 47; 48]. Score Matching related techniques have been widely used in the domain of generative modeling, so we use them to estimate the score function of the sampler's distribution.

## 3 Training Approaches of Neural Implicit Samplers

The neural implicit sampler has the advantage of sampling efficiency and scalability. But the training approach for such samplers still requires to explore. In this section, we start by introducing the proposed KL and Fisher training methods for implicit samplers.

### Training Neural Implicit Sampler by Minimizing the KL Divergence

Let \(g_{}()^{D_{Z}}^{D_{X}}\) be an implicit sampler (i.e., a neural transform), \(p_{z}\) the latent distribution, \(p_{}\) the sampler induced distribution \(=g_{}()\), and \(q()\) the un-normalized target. Our goal in this section is to train the implicit sampler by minimizing the KL divergence between the sampler distribution and the un-normalized target. For a neural implicit sampler, we can efficiently sample from it but the density \(p_{}\) has no explicit expression. Our goal is to minimize the KL divergence between \(p_{}\) and target \(q\) in order to train the sampler \(g_{}\). The KL divergence between \(p_{}\) and \(q\) is defined as

\[^{(KL)}(p_{},q) _{ p_{}} p_{}() - q()=_{ p_{z}} p_{}(g_ {}())- q(g_{}()).\] (2)

In order to update the sampler \(g_{}\), we need to calculate the gradient of \(\) for the divergence equation 2. So we take the gradient of \(\) with respect to the KL divergence equation 2 and obtain

\[^{(KL)}(p_{},q)\] \[=_{ p_{z}}_{} p_{ }(g_{}())-_{} q(g_{}()) ()}{}+_{ p_{ z}}()}{}|_{=g_{ }()}.\]

The second term vanishes under loose conditions (we put detailed discussions in the Appendix A.1)

\[_{ p_{z}} ()}{}|_{=g_{}()} =_{ p_{}}()}{}=()} {}d\] \[= p_{}()d= 1=.\]

So the gradient of the KL term only remains the first term and the gradient can be calculated with

\[^{(KL)}()_{ p_{z}} _{p}(g_{}())-_{q}(g_{}()) ()}{},\] (3)

where \(_{q}_{} q()\) is the score function of target distribution so we have the explicit expression. The \(_{p}_{} p_{}()\) is the score function of the implicit sampler but we do not have the explicit expression. However, thanks to advanced techniques of neural score function estimation as we introduced in 2.2, we can use another score neural network to estimate the score function \(_{p}()_{}()\) with samples consistently drawn from \(p_{}\). With the estimated \(_{p}()\), the formula equation 3 provides a practical method for updating the implicit sampler that is shown to minimize the KL divergence.

More precisely, we can alternate between a score-estimation phase and a KL minimization phase in order to minimize the KL divergence between the sampler and the target distribution. The former phase uses score estimation techniques to train a score network \(_{}\) to approximate the sampler'sscore function with samples consistently generated from the implicit sampler. In the latter phase, we can minimize the KL divergence with gradient-based optimization algorithms according to gradient formula equation 3. By alternating the optimization of a score network \(_{}\) and implicit sampler \(g_{}\), one can eventually train an implicit sampler that can approximately generate samples that are distributed according to the target distribution. We name such method the _KL training method_.

### Training Neural Implicit Sampler by Minimizing Fisher Divergence

In the previous section, we proposed a KL training method for implicit samplers. In this section, we proposed an alternative training method motivated by minimizing the Fisher divergence instead of the KL. We use the same notation as the section 3.1. Recall the definition of the Fisher divergence,

\[^{(F)}(p_{},q)_{p_{}}\| _{} p_{}()-_{} q()\|_{2}^{2} =_{ p_{z}}\|_{}(g_{}()) -_{q}(g_{}())\|_{2}^{2}.\] (4)

The score functions \(_{}\) and \(_{q}\) remain the same meaning as in the equation 3. For an implicit sampler, we do not know the explicit expression of \(_{}\). In order to minimize the Fisher divergence, we take the \(\) gradient of equation 4 and obtain

\[^{(F)}(p _{},q)&=_{=g_{}(),  p_{z}}_{}()-_{q}() {_{}()}{}-_{q}( )}{}}{}\\ &+_{ p_{z}}_{}(g_{} ())-_{q}(g_{}())_{}( )}{}|_{=g_{}()}\\ &=^{(F,1)}()+^{(F,2)}(). \] (5)

We use \(_{^{}}\) to represent a function that does not differentiate with respect to parameter \(\), so the first term gradient equation 5 is equivalent to taking the gradient of an equivalent objective

\[^{(F,1)}()=_{ p_{z}}\|_ {^{}}(g_{}())-_{q}(g_{}())\|_{2}^ {2}.\] (6)

As for the second term of equation 5, we notice that under regularity conditions

\[_{}()}{}=} p_{}()=} p_{}().\]

So the second term of equation 5 equals to

\[^{(F,2)}()=&_{ p_{}}_{}()-_{q}() } p_{ }()=_{}()-_{q}()p _{}()} p_{}()d\\ =&-_{p_{ }}_{^{}}()[_{^{}}( )-_{q}()]+_{}[_{^{}}() -_{q}()].\] (7)

We put detailed derivation and required conditions in Appendix A.2. With the expression of the \(^{(F,2)}()\), minimizing the second part of \(^{(F)}()\) is equivalent to minimizing an equivalent objective

\[^{(F,2)}()=-_{ p_{z}}_{ ^{}}^{T}(g_{}())[_{^{}}(g_{ }())-_{q}(g_{}())]+_{}[_{ ^{}}(g_{}())-_{q}(g_{}())].\] (8)

Combining equivalent loss function equation 6 and equation 8, we obtain a final objective function for training implicit sampler that minimizes the Fisher divergence

\[^{(F)}()&=^{(F,1)}()+^{(F,2)}()\\ &=_{=g_{}() p_{z}} \|_{q}()\|_{2}^{2}-\|_{^{}}( )\|_{2}^{2}+2_{}_{q}()-_{^{ }}().\] (9)

And we formally define the gradient operator

\[^{(F)}()^{(F)}().\] (10)

Similar to the KL divergence case as discussed in Section 3.1, our overall training approach of such a Fisher divergence minimization consists of two phases: the score-estimation phase, and the Fisher divergence minimization phase. The score-estimation phase is the same as the one used for KL training, while the Fisher divergence minimization updates the implicit sampler by minimizing objective equation 9. Since the introduced method aims to minimize the Fisher divergence, we name our training the _Fisher training_ for an implicit sampler.

Architecture choice of score network.Notice that for Fisher training, the objective equation 9 needs the calculation of the gradient of the scoring network \(_{}_{}()\), so it requires the scoring network to be point-wisely differentiable. Commonly used activation functions such as ReLU or LeakyReLU do not satisfy the differentiability property. In our further experiments, we use differentiable activation functions for Fisher training.

We formally give a unified Algorithm for the training of implicit samplers with KL, Fisher, and Combine training in Algorithm 1. We use the standard score-matching objective as an example of score estimation phases, other score estimation methods are also suitable. The notation \(_{}^{(d)}\) and \(_{d}\) represent the \(d\)-th component of the score network \(_{}(.)\) and the data \(\), and \(D\) means the data dimension. The notation \((.)\) represents stopping the parameter dependence of the input.

```
0: un-normalized target \( q()\), latent distribution \(p_{z}()\), implicit sampler \(g_{}(.)\), score network \(_{}(.)\), mini-batch size B, max iteration M. Randomly initialize \((^{(0)},^{(0)})\). for\(t\) in 0:Mdo // update score network parameter  Get mini-batch without parameter dependence \(x_{i}=[g_{^{(t)}}(_{i})],_{i} p_{z}(),i= 1,..,B\).  Calculate score matching or related objective: \(()=_{i=1}^{B}\|_{}(_{i} )\|_{2}^{2}+2_{d=1}^{D}_{}^{(t)}(_{i})}{ _{d}}\).  Minimize \(_{SM}()\) to get \(^{(t+1)}\). // update sampler parameter  Get mini-batch samples with parameter-dependecne \(_{i}=g_{^{(t)}}(_{i}),_{i} p_{z}(),i=1,..,B\).  Calculate Gradient grad with formula 3 or 10.  Update parameter \(\) with \(\) and gradient-based optimization algorithms to get \(^{(t+1)}\).  end for return\((,)\). ```

**Algorithm 1**Training Algorithm for Implicit Samplers

### Connection to Fisher-Stein's Sampler

In this section, we provide an analysis of the connection between our proposed Fisher training method and Fisher-Stein's sampler proposed in Hu et al. . Surprisingly, we discover that the training method used in Fisher-Stein's sampler is implicitly equivalent to our proposed Fisher training. To establish this connection, we adopt the notations introduced in the introduction of Fisher-Stein's sampler in Section 2.1. We summarize our findings in a theorem, and for a comprehensive understanding of the proof and analysis, we provide detailed explanations in Appendix A.5.

**Proposition 1**.: _Estimating the sampler's score function \(_{}(.)\) with score matching is equivalent to the maximization of test function \(\) of Fisher-Stein's Discrepancy objective (15). More specially, the optimal score estimation \(S^{*}\) and Fisher-Stein's optimal test function \(^{*}\) satisfy_

\[^{*}()=_{} q()-^{*}().\]

With Proposition 2, we can prove a Theorem that states that the Fisher-Stein training coincides with our proposed Fisher training.

**Theorem 1**.: _Assume \(^{*}\) is the optimal test function that maximizes Fisher-Stein's objective 15. Then the minimization part of objective 15 for sampler \(G_{}\) with the gradient-based algorithm is equivalent to the Fisher training, i.e. the minimization objective of 15 shares the same gradient with \(^{(F)}()\)._

Theorem 1 states that Fisher-Stein's sampler and its training method are equivalent to our proposed Fisher training. The two methods differ only in the parametrization of the score network (and the test function \(\)) and the motivation that they are proposed. It is one of our contributions to point out this equivalence. Our analysis reveals the underlying connection between the two training methods, shedding light on the similarities and shared principles between them. By establishing this connection, we provide a deeper understanding of the theoretical foundations of our proposed Fisher training approach and its relationship to existing techniques in the field.

## 4 Experiments

In previous sections, we have established the KL and Fisher training methods for implicit samplers. Both methods only require access to an un-normalized potential function of the target distribution. In this section, we aim to apply our introduced training methods on three sampling benchmarks whose scales vary across both low (2-dimensional targets) to high (784-dimensions image targets). We compare both the implicit sampler and our training methods to other competitor methods and samplers. All experiments demonstrate the significant advantage of the implicit sampler and the proposed training methods on both the sampling efficiency and the sample quality.

### 2D Synthetic Target Sampling

Sample quality on 2D targets.In this section, we refer to the open-source implementation of Sharrock and Nemeth 4 and train samplers on eight 2D target distributions. We compare our neural samplers with 3 MCMC baselines: Stein variational gradient descent (SVGD) , Langevin dynamics (LD) , and Hamiltonian Monte Carlo (HMC) ; 1 explicit baseline: coupling normalizing flow ; and 2 implicit samplers: KSD neural sampler (KSD-NS)  and SteinGan . All implicit samplers have the same neural architectures, i.e. four layer MLP with 400 hidden units at each layer and ELU activation functions for sampler and score network (if necessary).

The kernelized Stein's discrepancy (KSD)  is a popular metric for evaluating the sample quality of Monte Carlo samplers [55; 40]. We evaluate the KSD with IMQ kernel (implemented by an open-source package, the sgmcmcjax5) on all target distributions as the metric reported in Table 1.

Settings.For all MCMC samplers, we set the number of iterations to 500, which we find is enough for convergence. For SVGD and LD, we set the sampling step size to 0.01. For the HMC sampler, we optimize and find the step size to be 0.1, and LeapFrog updates to 10 work the best. For Coupling Flow, we follow Dinh et al. , and use 3 invertible blocks, with each block containing a 4-layer MLP with 200 hidden units and Gaussian Error Linear Units (GELU) activations. The total parameters of flow are significantly larger than neural samplers. We find that adding more coupling blocks does not lead to better performance. For all targets, we train each neural sampler with the Adam optimizer with the same learning rate of 2e-5 and default bete. We use the same batch size of 5000 for 10k iterations when training all neural samplers. We evaluate the KSD for every 200 iterations with 500 samples with 20 repeats for each time. We pick the lowest mean KSD among 10k training iterations as our final results. Since our proposed Fisher neural sampler and KL neural sampler require learning the score network, we find that using 5-step updates of the score network for each update of the neural sampler works well.

  
**Sampler** & **Gaussian** & **MOG2** & **Rosenrock** & **Donut** & **Funnel** & **Suggle** \\   MCMC & & & & & & \\  SVGD(500) & \(0.013 0.001\) & \(0.044 0.006\) & \(0.053 0.002\) & \(0.057 0.004\) & \(0.052 0.001\) & \(0.024 0.002\) \\ LD(500) & \(0.107 0.025\) & \(0.099 0.008\) & \(0.152 0.030\) & \(0.107 0.020\) & \(0.116 0.029\) & \(0.139 0.030\) \\ HMC(500) & \(0.094 0.020\) & \(0.106 0.020\) & \(0.134 0.034\) & \(0.113 0.020\) & \(0.135 0.010\) & \(0.135 0.033\) \\  Neural & & & & & & \\  Coup-Flow & \(0.102 0.028\) & \(0.158 0.019\) & \(0.150 0.026\) & \(0.239 0.013\) & \(0.269 0.019\) & \(0.130 0.026\) \\ KSD-NS & \(0.206 0.043\) & \(1.129 0.197\) & \(1.531 0.058\) & \(0.341 0.039\) & \(0.396 0.221\) & \(0.462 0.065\) \\ SteinGAN & \(0.091 0.013\) & \(0.131 0.011\) & \(0.121 0.022\) & \(0.104 0.013\) & \(0.129 0.020\) & \(0.124 0.018\) \\
**Fisher-NS** & \(0.095 0.016\) & \(0.118 0.013\) & \(0.157 0.030\) & \(0.179 0.028\) & \(7.837 1.614\) & \(0.202 0.037\) \\
**KL-NS** & \(0.099 0.015\) & \(0.104 0.015\) & \(0.123 0.021\) & \(0.109 0.015\) & \(0.115 0.012\) & \(0.118 0.024\) \\   

Table 1: KSD Comparison of Samplers. If not especially emphasized, we set the Stepsize=0.01, iterations=500, num particles=500, num chains=20.

Performance.Table 1 shows the numerical comparison between the mentioned samplers. The SVGD performs significantly the best among all samplers. However, the SVGD has a more heavy computational cost when the number of particles grows because its update requires matrix computation for a large matrix. The LD and HMC perform almost the same. The KL-NS performs the best across almost all targets, slightly better than LD and HMC on each target. The SteinGAN performs second and is closely comparable to KL-NS. In theory, both the KL-NS and the SteinGAN aim to minimize the KL divergence between the sampler and the target distribution in different ways, so we are not surprised by their similar performances. The Coupling Flow performs overall third, but it fails to correctly capture the Rosenbrock target. We believe more powerful flows, such as stochastic variants or flows with more complex blocks will lead to better performance, but these enhancements will inevitably bring in more computational complexity. The Fisher-NS performs the fourth, with 1 failure case on the Funnel target. We find that the KSD-NS is hard to tune in practice, which has two failure cases. Besides, the KSD-NS has a relatively high computational cost because it requires differentiation through the empirical KSD, which needs large matrix computation when the batch size is large. Overall, the one-shot KL-NS has shown strong performance, outperforming LD and HMC with multiple iterations. We also visualize the sample results on five distributions with hard-to-sample characteristics such as multi-modality and periodicity, as shown in Figure 2.

Comparison of Computational Costs.To demonstrate the significant advantage of our proposed implicit samplers over other samplers, we evaluate the efficiency of each sampler under the same environment to measure the wall-clock inference time. In below Table 2, we summarize the computational costs (wall-clock time) of SVGD, HMC, and LD together with our KL-NS neural sampler to compare the computational costs of each method. The eight targets are analytic targets while the EBM is a neural target. Each sampler can generate samples with comparable qualities (i.e. their KSD values are comparable). Table 2 records the wall-clock inference time (seconds) for each sampler when generating 1k samples.

The LD and HMC with 500 iterations have comparable (slightly worse) performance than the Neural Sampler, however, their wall-clock time for obtaining 1k samples is significantly larger than Neural samplers. Let's take the simple mixture-of-2-gaussian (MOG2) target (with analytic potential and scores) as an example, the neural sampler is 1.7125/0.0014=1223 times faster than HMC and 0.2168/0.0014=154 times faster than LD. If one wants to keep the same inference time for LD and HMC to be comparable with the neural sampler, the LD will only have 500/154=3.2 iterations, while the HMC will only have 500/1223=0.4 iterations. It is impossible to obtain promising samples with only 3.2 or 0.4 MCMC iterations.

In conclusion, for both analytic and neural targets, the neural samplers show a significant efficiency advantage over MCMC samplers with better (comparable) performances. This shows that neural samplers have potentially wider use on many sampling tasks for which high efficiency and low computational costs are demanded.

### Bayesian Regression

Test accuracy compared with Stein sampler.In the previous section, we validate the sampler on low-dimensional 2D target distributions. The Bayesian logistic regression provides a source of real-world target distributions with medium dimensions (10-100). In this section, we compare our sampler with the most related sampler, Stein's sampler that is proposed in Hu et al. . We compare the test accuracy of our proposed sampler against Stein's samplers and MCMC samplers.

  
**Sampler** & **Gaussian** & **MOG2** & **Rosenbrock** & **Donut** & **Funnel** \\  SVGD(500) & \(26.4224 0.6000\) & \(26.7897 0.5712\) & \(26.1088 0.4666\) & \(26.3632 0.4290\) & \(26.0555 0.4232\) \\ LD(500) & \(0.1035 0.0003\) & \(0.2168 0.0008\) & \(0.1284 0.0002\) & \(0.1100 0.0003\) & \(0.1339 0.0032\) \\ HMC(500) & \(0.3438 0.0017\) & \(1.7125 0.0154\) & \(0.6725 0.0044\) & \(0.3854 0.0015\) & \(0.6837 0.0018\) \\ KL-NS & \( 0.0000\) & \( 0.0000\) & \( 0.0000\) & \( 0.0000\) & \( 0.0000\) \\   

Table 2: Inference Time Comparison of MCMC Samplers and Neural Samplers (seconds). The 2D experiment is conducted on an 8-CPU cluster with PyTorch of 1.8.1, while the EBM experiment is on 1 Nvidia Titan RTX GPU with PyTorch 1.8.1. Unless especially emphasizing, we set the stepsize=0.01, iterations=500, num particles=1000, num repeats=100.

Experiment settings.We follow the same setting as Hu et al.  on the Bayesian logistic regression tasks. The Covertype data set  has 54 features and 581,012 observations. It has been widely used as a benchmark for Bayesian inference. More precisely, following the Hu et al. , we assume prior of the weights is \(p(w|)=(w;\ 0,^{-1})\) and \(p()=(;\ 1,0.01)\). The data set is randomly split into the training set (80%) and the testing set (20%). Our goal is to train an implicit sampler that can sample from posterior distribution efficiently. The sampler trained with Fisher training is the same as the Fisher-Neural Sampler, which achieves the best test accuracy, as we have proved in previous sections. The sampler trained with the KL method gives the secondary test accuracy in Table 3. This experiment shows that our proposed training method is capable of handling real-world Bayesian inference with a medium dimension. After training, the implicit sampler can result in the best accuracy while achieving hundreds of times faster than MCMC methods. For detailed settings, please refer to Appendix B.

### Sampling from Energy-based Models

A main advantage of an implicit sampler is its inference efficiency. This advantage benefits applications for which the default sampling method is inefficient, such as sampling from high-dimensional energy-based models. In this section, we use our proposed KL training method for an application of learning an efficient implicit sampler to sample from a pre-trained energy-based model.

Figure 1: Samples from implicit samplers and energy-based models trained on MNIST datasets. Our trained sampler only requires one NFE, which is comparable to EBM samples with 100 NFEs.

   SGLD & DSVI \\
75.09\% \(\) 0.20\% & 73.46\% \(\) 4.52\% \\  SVGD & SteinGAN \\
74.76\% \(\) 0.47\% & 75.37\% \(\) 0.19\% \\ 
**Fisher-NS** & **KL-IS(Ours)** \\
76.22\% \(\) 0.43\% & 75.95\% \(\) 0.002\% \\   

Table 3: Test Accuracies for Bayesian Logistic Regression on Covertype Dataset. The Fisher-NS is equivalent to a sampler with our proposed Fisher training, i.e. Fisher-IS.

Figure 2: Samples from implicit samplers trained with KL training method on five target distributions. **Up**: Visualization of target un-normalized density; **Below**: samples from our trained sampler.

Experiment settings.An energy-based model (EBM)  uses a neural network to parametrize a negative energy function, i.e. the logarithm of some un-normalized distribution. After training, obtaining samples from an EBM usually requires running an annealed Markov Chain Monte Carlo  which is computationally inefficient. In this section, we pre-train a deep EBM  on the MNIST dataset and apply our proposed KL training method6 for learning a neural implicit sampler to learn to sample from the pre-trained EBM efficiently. To take the inductive biases of image data, we parametrize our sampler via the stacking of several convolutional neural blocks. The neural sampler takes a random Gaussian input with a dimension of 128 and outputs a 32x32 tensor which has the same size as the resolution on which the EBM has been trained. We visualize the samples drawn from EBM with 50+ evaluations of the forward and backward pass of EBMs with annealed Langevin dynamics and samples from our sampler which only requires a single forward pass of the sampler's neural network. We train an MNIST classifier and compute the Frechet Inception Distance (FID)  and the KID  as an evaluation metric.

Performance.As is shown in Table 4, our sampler generates indistinguishable samples from the EBM but is about 100+ more efficient than annealed Langevin dynamics. To demonstrate the sampling efficiency of our sampler, we compare the efficiency and computational costs of EBM and our sampler, by listing some major metrics on computational performance in Table 4. The EBM has a total amount of \(9.06M\) parameters and a base FLOPS of \(0.58G\), while the sampler has \(12.59M\) parameters and a base FLOPS of \(1.11G\). Generating samples from EBM requires running MCMC chains, which consist of up to 200 neural functional evaluations (NFEs) of the EBM. We compare the efficiency and performance of EBM with different NFEs and find that with less than 100 NFEs the EBM can not generate realistic samples, while our sampler only needs one single NFE for good samples. This high-dimensional EBM sampling experiment demonstrates the scalability of KL training methods and their potential for accelerating large-scale energy-based model sampling.

## 5 Limitations and Future Works

We have presented two novel approaches for training an implicit sampler to sampler from un-normalized density. We show in theory that our training methods are equivalent to minimizing the KL or the Fisher divergences. We systematically compare the proposed training methods against each other and against other samplers (NFs and MCMC). Besides, we conduct an experiment that trains a convolutional neural sampler to learn to sample from a pre-trained EBM. This experiment shows that our implicit sampler and corresponding training have great potential to improve the efficiency of modern machine learning models such as energy-based models or diffusion models.

However, there are still limitations to our proposed methods. First, the score estimation is not computationally cheap, so developing a more efficient training algorithm by eliminating the score estimation phase is an important research direction. Besides, now the sampler is only limited to sampling problems. How to extend the methodology for other applications such as generative modeling is another interesting direction.