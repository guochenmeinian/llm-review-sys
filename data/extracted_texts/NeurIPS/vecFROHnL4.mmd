# MARVEL: Multidimensional Abstraction and Reasoning through Visual Evaluation and Learning

Yifan Jiang\({}^{1}\)   Jiarui Zhang\({}^{1}\)1   Kexuan Sun\({}^{1}\)1   Zhivar Sourati\({}^{1}\)

Kian Ahrabian\({}^{1}\)   Kaixin Ma\({}^{2}\)   Filip Ilievski\({}^{3}\)   Jay Pujara\({}^{1}\)

\({}^{1}\)Information Sciences Institute, University of Southern California

\({}^{2}\)Tencent AI Lab, Bellevue, WA

\({}^{3}\)Department of Computer Science, Vrije Universiteit Amsterdam

{yjiang44,jzhang37,kexuansu,souratih,ahrabian}@usc.edu

kaixinma@global.tencent.com, f.ilievski@vu.nl, jpujara@isi.edu

Authors contributed equally

###### Abstract

While multi-modal large language models (MLLMs) have shown significant progress across popular visual reasoning benchmarks, whether they possess abstract visual reasoning abilities remains an open question. Similar to the Sudoku puzzles, abstract visual reasoning (AVR) problems require finding _high-level patterns_ (e.g., repetition constraints on numbers) that control the _input shapes_ (e.g., digits) in a specific _task configuration_ (e.g., matrix). However, existing AVR benchmarks only consider a limited set of patterns (addition, conjunction), input shapes (rectangle, square), and task configurations (\(3 3\) matrices). And they fail to capture all abstract reasoning patterns in human cognition necessary for addressing real-world tasks, such as geometric properties and object boundary understanding in real-world navigation. To evaluate MLLMs' AVR abilities systematically, we introduce **MARVEL** founded on the core knowledge system in human cognition, a multi-dimensional AVR benchmark with 770 puzzles composed of six core knowledge patterns, geometric and abstract shapes, and five different task configurations. To inspect whether the model performance is grounded in perception or reasoning, MARVEL complements the standard AVR question with _perception questions_ in a hierarchical evaluation framework. We conduct comprehensive experiments on MARVEL with ten representative MLLMs in zero-shot and few-shot settings. Our experiments reveal that all MLLMs show near-random performance on MARVEL, with significant performance gaps (40%) compared to humans across all patterns and task configurations. Further analysis of perception questions reveals that MLLMs struggle to comprehend the visual features (near-random performance). Although closed-source MLLMs, such as GPT-4V, show a promising understanding of reasoning patterns (on par with humans) after adding textual descriptions, this advantage is hindered by their weak perception abilities. We release our entire code and dataset at https://github.com/1171-jpg/MARVEL_AVR.

## 1 Introduction

Recent advances in novel training pipelines, computational resources, and data sources have enabled multi-modal large language models (MLLMs)  to show strong visual reasoning ability in tasks that require combining both visual and textual cues , such as visual question answering  and visual commonsense reasoning . These tasks are typically under real-world settings . On the other hand, abstract visual reasoning (AVR)  focuses onrecognizing patterns among 2D shapes and their attributes. As the puzzle shown in Figure 1 (top-right), AVR problems require identifying the hidden pattern (addition and subtraction) that governs the input shapes and their attribute (number of stars/circles) in a task configuration (2 \(\) 3 matrix). AVR ability is crucial for various practical applications, including human pose estimation (understanding pose through abstract representation) , and anomaly detection(finding outliers in videos) . AVR ability is also indispensable for developing artificial general intelligence (AGI) . This significance encourages fundamental research on evaluating MLLMs against AVR benchmarks .

However, two significant issues remain unsolved in evaluating MLLMs comprehensively. 1) **The scope of current AVR benchmarks is not fully cognitively supported and fails to encompass the variety of real-world scenarios. Some datasets, such as RAVEN , only cover a few reasoning patterns (mostly in mathematical patterns) over a limited set of input shapes arranged in a predetermined configuration of puzzle panels. Human cognition builds the foundation for inference and skill acquisition in the real world. Without theoretical foundations, existing benchmarks lack diverse reasoning patterns in real-world tasks and fail to provide a holistic evaluation of MLLM's ability, which can not be solved by simple joint evaluations. 2) **Most prior studies** **employ an end-to-end evaluation framework, leaving it unclear whether the model's performance is attributed to perception or reasoning. Current visual literature claims they are both compositional components of the visual reasoning process and should be treated separately for in-depth analysis.

To address the limitation of evaluation scope (issue 1), we introduce **MARVEL**, a multi-dimensional abstract visual reasoning benchmark designed to evaluate MLLMs across six patterns, both geometric and abstract shapes and five task configurations. To ensure cognitive foundations and real-world applicability, MARVEL's underlying reasoning patterns are rooted in key core knowledge of human cognition, observed in newborn infants, necessary for reasoning about their environment (real-world scenario)  even without a clear real-world understanding (abstraction). We crawl relevant puzzles from publicly available websites, manually filter low-quality and irrelevant puzzles based on the expanded patterns and input shapes, and reformat them into different task configurations. We annotate the AVR question by briefly describing the puzzle and asking for its answer. In total, we collect 770 diverse and high-quality puzzles assessing abstract visual reasoning abilities (Figure 1).

Figure 1: An abstract visual reasoning puzzle in **MARVEL**. The puzzle contains **mathematical** pattern governing the element number in **geometric shapes** with **two-row** task configuration. The AVR question focuses on the final answer for the puzzle, while the perception questions focus on the fine-grained detail about one choice or coarse-grained detail over the whole puzzle. In the example, the left-side elements (black stars/circles) increase by one in each panel, while the right-side elements (white stars/circles) in the first panel equal the sum of those in the second and third panels.

To determine if the model's performance is based on perception or reasoning (issue 2), we also provide a hierarchical evaluation framework by enriching each puzzle with perception questions focusing on perceiving puzzles' visual details (e.g., number of grids, edges of a triangle) to measure models' reasoning consistency . We conduct comprehensive experiments on MARVEL involving different model structures, model sizes, and prompting strategies. Our experiments reveal that all MLLMs show near-random performance in all patterns, even with few-shot demonstrations and prompt engineering, leaving a huge gap (40%) in the abstract reasoning ability of humans. An in-depth analysis based on perception questions points out that MLLMs' performance is hindered by their fine-grained visual feature comprehension, failing to provide foundations for subsequent abstract reasoning. Our contributions can be summarized as follows: 1) **A novel multidimensional AVR benchmark**, **MARVEL**, which consists of six patterns rooted in cognitive theory across five distinct task configurations. 2) **A hierarchical evaluation framework** incorporating perception questions with AVR questions to enable fine-grained diagnosis of model capability. 3) **Extensive experiments** on a wide range of state-of-the-art MLLMs with various prompting strategies, providing insights into the connection between their perception and reasoning abilities.

## 2 Related Work

MLLM Evaluations.Benefiting from the rich representation from visual encoders  and strong reasoning ability of LLMs , MLLMs  have been applied to solve not only traditional vision-language tasks, such as image captioning , visual question answering  and refer expression comprehension , but also on more complicated scenarios, such visually-grounded conversation , multimodal web/UI agents  and embodied tasks . Besides end-to-end evaluation, several recent works also try to reveal MLLMs' visual shortcomings from different aspects, including visual details , perceptual bias , and small visual pattern recognition . Although some of the existing benchmarks have accessed MLLM's mathematical visual reasoning abilities requiring an understanding of abstract and geometry shapes , their evaluation still heavily relies on textual descriptions. In contrast, AVR benchmarks assess MLLMs' ability under diverse patterns with only visual understanding settings.

AVR Benchmarks.AVR problems have great potential impact on various domains , sparking interests in evaluating MLLMs on AVR benchmarks . Existing AVR benchmarks present the evaluation in a wide range of formats, such as selective completion , group discrimination  and generative completion . However, less attention is paid to the scope and pattern of the AVR benchmark; most focus only on a few simple abstract patterns and testing models end-to-end without considering the intermediate perception and reasoning procedures . In contrast, MARVEL not only includes geometric and abstract shapes and five different task configurations but also builds its reasoning patterns based on the core knowledge system in cognitive science, enriching abstract reasoning that exists in the real world. Inspired by prior analysis of visual details and perceptual bias, MARVEL introduces perception questions to ensure the MLLMs

    & Dimension & RAVEN & G-set & VAP & 
 Bongard- \\ LOGO \\  & SVRT & DOPT & ARC\({}^{*}\) & **MARVEL** \\   & Geometric & \(\) & \(\) & \(\) & & & \(\) & \(\) & \(\) \\  & Abstract & & & & & \(\) & \(\) & \(\) & \(\) \\   & Temporal Movement & \(\) & \(\) & & & & \(\) & \(\) & \(\) \\  & Spatial Relationship & & \(\) & \(\) & \(\) & \(\) & \(\) & \(\) & \(\) \\  & Quantities & & \(\) & \(\) & \(\) & \(\) & \(\) & \(\) & \(\) \\  & Mathematical & & \(\) & \(\) & & & & \(\) & \(\) \\  & 2D-Geometry & & & & & & & \(\) \\  & 3D-Geometry & & & & & & & \(\) \\   & Sequence & & & & & & & \(\) & \(\) \\  & Two-row & & & & & & & \(\) \\  & Matrix & & & & & & & \(\) \\  & Group & & & & & & & \(\) \\  & Reassembling & & & & & & & \(\) \\   & & & & & & & &  \\   

Table 1: Comparing MARVEL to related benchmarks: RAVEN , G-set , VAP , Bongard-LOGO , SVRT , ARC , DOPT . *ARC puzzles are provided in a generative format.

correctly perceive the presented visual patterns. MARVEL and related AVR benchmarks are compared in Table 1.

## 3 MARVEL Benchmark Construction

As a multidimensional benchmark for AVR, MARVEL covers different task configurations (Section 3.1), various input shapes (Section 3.2), as well as different reasoning patterns involved in the puzzles (Section 3.3). We present the data collection process in Section 3.4.

### Task Definition and Configurations

Each puzzle in MARVEL consists of a context on the top and possible choices (\(c_{i};i\{1,2,3,4\}\)) to choose from at the bottom, formatted in a multiple-choice question answering setting (see Figure 1 for an example). The context part consists of \(n\) puzzle panels (\(p_{1},p_{2},,p_{n},p_{b}\) with \(p_{b}\) being a blank panel), with their specific number and arrangement driven by a task configuration and reasoning pattern, \(P\), that governs the relationship between puzzle panels. The choice will be considered the correct answer and fill in \(p_{b}\) that can satisfy the following equation: \(P(p_{1},p_{2},,p_{n})=P(p_{1},p_{2},,p_{n},c_{c})\).

Puzzle panels in MARVEL are organized in five task configurations. We visualize these configurations in Figure 1 (see detailed examples in Appendix D).

1. **Sequence Format** arranges panels in a \(1 n\) line (\(n\)).
2. **Two-row Format** presents panels in a \(2 3\) matrix*, \(p_{1}^{1},p_{2}^{1},p_{3}^{1}\) and \(p_{1}^{2},p_{2}^{2},p_{3}^{2}\). The solution requires identifying the same pattern at the first row and the second row, which is \(P(p_{1}^{1},p_{2}^{1},p_{3}^{1})=P(p_{1}^{2},p_{2}^{2},c_{c})\). Footnote *: \(p_{a}^{b}\): the panel on the \(a\) th row and \(b\) th column of matrix.
3. **Matrix Format** organizes panels in a 3 by 3 matrix, the pattern can be reflected into either row- or column-wise way: \(P(p_{1}^{1},p_{2}^{1},p_{3}^{1})=P(p_{1}^{2},p_{2}^{2},p_{3}^{2})=P(p_{1}^{3}, p_{2}^{3},c_{c})\) or \(P(p_{1}^{1},p_{1}^{2},p_{1}^{3})=P(p_{2}^{2},p_{3}^{2})=P(p_{3}^{1},p_{3}^{2}, c_{c})\).
4. **Group Format** has three panels in the question part (\(p_{1},p_{2},p_{b}\)) with one choice reflecting the context's pattern and other choices differing: \(P(p_{1},p_{2},c_{c}) P^{}(choices-c_{c})\).
5. **Reassembling Format** is designed for _3D-Geometry_ pattern visualizes 3D geometric shapes, such as cubes or tetrahedrons, in the choice section of the puzzle. The context section provides a panel featuring the unfolded 2D diagram of one of the choices. Models must reason about the visual details in the 2D diagram to identify the correct 3D shape.

### Input Shapes

As shown in Figure 1, each panel of a puzzle contains various shapes that can be generally differentiated into two types :

1. **Geometric Shapes** are easily described and come from a limited vocabulary. For instance, a square is a shape that has four sides of equal length and four equal angles. Most existing AVR benchmarks [77; 27] focus on elementary shapes such as oval, rectangle, triangle, and trapezoid. MARVEL includes geometric shapes consisting of more than two different elementary geometric shapes to mitigate the issue and improve the complexity.
2. **Abstract Shapes** come from a wide set of possibilities and vary widely from one problem to another. Unlike geometric shapes, which are typically fixed, easily describable, and belong to a finite set (e.g., both a larger square and a black square are classified as squares due to shared fundamental properties), abstract shapes are atypical, lacking fixed properties and marked by variability and complexity. They provide a fair step as most MLLMs encounter the shapes for the first time and are gaining more preference for AVR-related research [19; 47].

### Core Knowledge and Patterns

Core knowledge theory  from cognition science is largely shared among humans and particularly for human infants. Human infants with no real-world knowledge and limited experience represent their environment using abstraction patterns. These abstraction patterns can be categorized into four types of core knowledge, which is the foundation for inference and reasoning  in real-world scenarios. We do not feature the agent representation core knowledge because it concentrates on goal-directed and interactive action, which is not adaptable in MARVEL setting. For the other three types of core knowledge, we expand each into two patterns for a fine-grained assessment of abstract reasoning in MARVEL, based on insights drawn from contemporary cognitive literature:

1. **Object Core Knowledge** represents objects' spatiotemporal motions and their contact, enabling humans to predict objects' movement and perceive object boundaries. We expanded this core knowledge to _Temporal Movement Pattern_ focusing on the related position change or movement  and _Spatial Relationship Pattern_ examining objects' relative positional relationship .
2. **Number Core Knowledge** helps infants process abstract representations of small numbers and perform comparisons. We include _Quantities Pattern_ testing the accuracy of number comprehension  and _Matlmatical Pattern_ for elementary mathematical operations .
3. **Geometry Core Knowledge** captures the environment's geometry, which helps humans orient themselves in their surroundings. We divide the concept into _2D-Geometry Pattern_ and _3D-Geometry Pattern_.

### Data Collection

We collect puzzles from several public resources websites1 and filter out unfit or low-quality data by three human annotators based on the puzzle's input shapes (some puzzles contain textual information) and patterns. Unaligned puzzles are first segmented into panels and then reassembled into the correct task configuration. To ensure each pattern in each task configuration has at least 45 puzzles2, we also manually created 220 puzzles by following the pattern in existing data and replacing the input shape drawn from scratch. Each puzzle contains an AVR question (Figure 1) generated from templates based on their task configuration. AVR questions provide a brief description and ask only for the puzzle's final answer, which is widely adopted in previous AVR benchmark . In the end, MARVEL includes 770 high-quality puzzles over six high-level patterns across five distinct task configurations. In Table 1, we compare MARVEL with existing AVR benchmarks to show its comprehensive scope.

## 4 Hierarchical Evaluation Framework

Previous works evaluate MLLMs on AVR benchmarks with end-to-end setting only [46; 45], potentially overlooking shortcut learning and inductive biases . On the other hand, precisely comprehending visual details is the foundation for subsequent reasoning in AVR problems . We enrich MARVEL puzzles with perception questions  designed to test models' perception ability on visual details (Figure 1). We design a hierarchical evaluation framework by combining two types of perception questions with AVR questions (Figure 1) to examine if model accuracy is based on perception and reasoning. For each puzzle, our framework provides three coarse-grained questions and one pattern-related fine-grained question:

**Coarse-grained Perception Question** in an open-ended fashion aims to test if models can understand the task configuration correctly by directly asking about the number of panels in puzzles. We use templates to generate three questions focusing on the number of panels in the context part, choice part, and the whole puzzle. We remove the choice index (the number marking each choice panel) when testing models with this question to avoid shortcut learning.

**Fine-grained Perception Question** in binary-choice format examines models' understanding of input shapes, which focus on the visual details categorized by Tong et al.  such as shape attributes (number of edges) and spatial relationship (left, right) based on the pattern contained in the puzzle. For example, in Figure 1, the fine-grained perception question tests whether models can understand the number of circles because the puzzle is based on _Mathematical Pattern_. For each puzzle, we randomly pick one choice panel in the puzzle and manually create questions with two choices. The correct answer is randomly placed to avoid inductive bias3. We have five types of questions based on the pattern and how it adapts to the input shape, which are listed with examples:

1. **Location**: Is the dot outside or inside of the star in choice 4?
2. **Color**: Is the triangle black or white in choice 1?
3. **Shape**: Is there a circle or a triangle inside choice 3?
4. **Quantity**: Are there five or four circles in choice 2?
5. **Comparison**: Are the left and right halves of the rectangle in choice 3 the same?

## 5 Experimental Setup

**Closed-source MLLMs.** We include API-based MLLMs including 1) GPT-4 , 2) Gemini  and 3) Claude3 . With the massive computation and training data, these models show promising performance on a wide range of visual-focused tasks [23; 70]. We evaluate closed-source MLLMs in both zero-shot and few-shot  settings.

**Open-source MLLMs.** We include MLLMs smaller than 13B due to our limited computing resources: 1) InstructBLIP , 2) BLIP-2 , 3) Fuyu , 4) Qwen-VL  and 5) LLaVA . We only evaluate these MLLMs in a zero-shot setting due to their single-image input settings .

**Human Evaluation.** To access the upper bound performance on MARVEL, we simulate a realistic human assessment by inviting 30 annotators aged from 10 to 50 years to solve a subset of MARVEL and ensure each subset contains every pattern in all task configurations. We compute the average performance of these 30 annotators as the human baseline. Each puzzle is solved by at least two annotators. We invited three annotators to solve perception questions and report their average performance. The demonstrations and instructions used are presented in Appendix J.

**Evaluation Metrics.** Following a similar setting as previous research evaluating MLLMs on AVR benchmarks , we use regex matching to extract the choices picked (e.g., "choice 4" in the response "The correct answer is choice 4."), with failure cases re-extracted by GPT-4 [3; 82]. We use accuracy as the metric, commonly used for evaluating multiple-choice questions, and has been utilized by many AVR papers [27; 77]. Based on the hierarchical evaluation framework, we evaluate MLLMs with two types of accuracy-based metrics:

1. **Instance-based Accuracy** considers questions separately. We report accuracy results for _AVR question_ and _fine-grained perception question_.
2. **Group-based Accuracy** considers questions as groups to assess the consistency in model reasoning [31; 75]. The model receives a score of 1 only if it correctly answers all questions within the same group. We report the group-based accuracy result of combining all three coarse-grained perception questions and the further result after introducing fine-grained and AVR questions into the group.

## 6 Results

We focus on five research questions: _1) What's the abstract reasoning ability on visual puzzles of current SOTA MLLMs? 2) Can MLLMs do better with different few-shot prompting strategies? 3) How do MLLMs perform on different patterns and task configurations? 4) To what extent do MLLMs visually understand the puzzle? 5) Do they show consistent reasoning ability?_

**Overall Performance.** The AVR question results are shown in Table 2. Human performance reaches 68.86%, with a standard deviation of 9.74, confirming the validity and challenging nature of MARVEL. For both open and closed source categories, **all models show near-random performance with a huge gap (40%) compared to human performance**, in which closed-source MLLMs (avg: 25.7%) perform slightly better than open-sourced ones (avg: 24.0%). We observed an extremely imbalanced distribution in the outputs of some MLLMs. For example, BLIP-2 consistently selecting choice 1 for all puzzles (marked \(\) in Table 2). We tried different approaches with our best effort to avoid potential bad prompts or engineering settings, including adding question marks in the black panel, replacing the choice index with letter (1 \(\) A), and changing the description in the AVRquestion. None of them can mitigate and may even exacerbate the issue, highlighting the potential inductive biases  in models. Among open-source MLLMs, LLAVA performs the best, yet the gap is very small, and it is unclear whether the gain comes from its larger model size. In closed-source models, even the strongest MLLMs, Claude3 (Opus) and GPT-4o, which demonstrated promising results on various vision tasks , failed to present a significant performance difference from the random baseline. Claude3 (Sonnet) and Gemini also have imbalanced output distributions, with both selecting choice 4 in most cases.

**Impact of Few-Shot CoT.** Given the poor zero-shot performance and the models' capability of in-context learning , we explore few-shot prompting with Chain-of-Thought (CoT)  to guide MLLMs with abstract reasoning patterns.

We experiment with all closed-source MLLMs in one-shot and two-shot settings using manually created CoT context, similar to Yang et al. . For each puzzle, we randomly select puzzles with the same pattern in the sequence task configuration and annotate the CoT reasoning with answers as demonstrations. We chose the sequence task because it is more straightforward (only along the sequence) than other configurations. Each demonstration is formatted as image-text pairs. Our result is shown in Figure 2, and we present the full results in Appendix I. The few-shot demonstrations show a marginal positive impact on GPT-4V and a decreasing trend on Claude3 (Opus).

Further analysis reveals that the main improvement in GPT-4V's results lies in the _3D-Geometry pattern_. As this pattern focuses on reassembling, the demonstration can guide the model to pay attention to the relative position of each side of the object. However, since most patterns are uniquely implemented on different input shapes and their attributes, the model struggles to learn generalizable patterns from the few-shot demonstrations. Figure 3 provides an example of zero-and few-shot results from Claude3 (Opus). With the demonstration, the model learns to focus on the correct pattern (blue) at the beginning of the reasoning. However, it fails to adapt precisely to the input shapes in the puzzle (red), leading to errors in subsequent reasoning. We also test different prompt engineering approaches, including selecting demonstration samples from different 1) patterns, 2) task configurations, and 3) prompting MLLMs by dividing puzzles panel by panel. None of these approaches yields a positive impact; instead, they lead to a significant drop in performance (Appendix I). Given the complexity

    &  & **AVR** & **Fine-grained** & ^{}\)**} & ^{\#F}\)**} & ^{\#F}\)**} &  \\  & & **Question** & & & & \\   & **Random** & 25.00 & 50.00 & - & - & - \\   & **Owen-VL (7B)** & 19.61 & 37.27 & 0.52 & 0.39 & 0.00 \\  & Fuyu (8B) & 24.29\({}^{}\) & 34.94 & 0.00 & 0.00 & 0.00 \\  & BLIP-2 (F1am75\({}_{}\)) & 24.81\({}^{}\) & 53.38 & 1.04 & 0.52 & 0.26 \\  & InstructBLIP (Vicuna-13B) & 24.68\({}^{}\) & 49.48 & 0.00 & 0.00 & 0.00 \\  & LLAVA-1.5 (Vicuna-13B) & 26.36 & 51.43 & 1.14 & 0.52 & 0.13 \\   & GPT-4V & 22.34 & 51.56 & 18.31 & 9.22 & 2.73 \\  & GPT-4o & 27.79 & **68.44** & **62.73** & **43.77** & **12.21** \\   & Gemini-pro-vision\({}^{}\) & 25.06\({}^{}\) & 44.42 & 15.19 & 6.75 & 1.69 \\   & Claude3 (Sonnet) & 26.49\({}^{}\) & 50.91 & 38.70 & 19.87 & 5.06 \\   & Claude3 (Opus) & **28.83** & 47.27 & 44.94 & 20.13 & 5.97 \\   & 68.86 \(\) 9.74 & 98.67 & - & - & - \\   

Table 2: Main zero-shot accuracy over MARVEL across all MLLMs in two accuracy metrics: \(Prec^{C}\) = group-based accuracy over all coarse-grained perception questions (model must answer all three questions correctly), \(Perc^{CkF}\) = group-based accuracy combining all perception questions (coarse/fine-grained), AVR = AVR Question. The best performance among all models is in **bold**, and the best result in two MLLMs categories is underlined. *Gemini refuses to answer the puzzle due to safety problems in 7% cases so the performance is computed based on the left set. \(\) notes the result is attributed to inductive bias.

Figure 2: MLLMs performance in different few-shot COT.

and challenging nature of the dataset, the effectiveness of few-shot prompting on MARVEL remains minimal. The inductive bias is mitigated in few-shot settings.

**Performance on Different Patterns and Configurations.** We further break down the results4 into different patterns and task configurations in Figure 4 (full results in Appendix E). In general, MLLMs show near-random performance on all patterns and task configurations. Among the six patterns, _3D-Geomertry_ pattern is the most challenging for humans and MLLMs. The difficulty may be rooted in the requirement for the 3D imaginative ability  and the relatively lower frequency of such data in the models' pre-training datasets, which leads to a significant gap (12.5%) between open and closed-sources MLLMs. On the other hand, _2D-Geomertry_, involving understanding geometric attribute of input shape, seems relatively easier for MLLMs, evidenced further by their proficiency on similar tasks such as interpreting graphs plots .

Among the five task configurations, the two-row and matrix formats provide more panels (i.e., information) to verify the pattern, facilitating puzzle-solving. On the contrary, the group and reassembling formats containing the least panel number tend to be challenging. Four out of five MLLMs rank 1st in different task configurations, which verifies our assumption of potential bias in single-configuration evaluation. According to their pre-training dataset, models may be familiar with specific input types, highlighting the necessity of MARVEL's multidimensional settings.

**Perception Ability on Visual Details.** Visual perception forms the foundation for advanced reasoning . By incorporating perception questions, our hierarchical evaluation framework effectively investigates to what extent the models understand the visual information from the puzzle. In Table 2, closed-source MLLMs demonstrate more robust performance on coarse-grained perception group accuracy compared to open-sourced MLLMs, with a performance gap ranging from 14.05% to 61.59%. However, even the best model fails to reach an acceptable accuracy (<65%), indicating that current MLLMs struggle to simultaneously understand the number of grids, choices, and the puzzle as a whole, despite their promising performance on real-world datasets . The simplicity of the coarse-grained perception questions (all puzzles contain less than 13 panels) highlights the poor perception ability of current MLLMs in the abstract visual reasoning domain. Fine-grained perception questions further confirm this argument, with

Figure 4: MLLMs and human performance across patterns and task configurations.

Figure 3: Perception question, zero- and two-shot example of Claude3 (Opus).

all models except GPT-4o showing near-random performance. Further analysis of fine-grained perception performance based on five categories (Table in Appendix F) reveals that models perform relatively better at color perception but have difficulty recognizing location (e.g., 'a' is on the left of 'b'). We hypothesize that the difficulty in understanding location stems from the lack of labeled data on location and relations during training, especially in abstract visual understanding. In contrast, the models' color perception is well-trained during their multi-modal alignment, and the simplicity of RGB understanding allows for easier transfer to the abstract domain.

**Consistency of Model Reasoning.** The further group-based accuracy (\(Prec^{\&F}\) and \(Prec^{\&F}\&AVR\)) shows that **no model can solve the AVR puzzles with consistent reasoning**, with the best model reaching only 12.21% group accuracy. Based on the result of our evaluation framework, we hypothesize the inconsistency stems from their poor visual perception ability . As shown in Figure 3, the model's reasoning is based on the perception of the puzzle (e.g., number of lines), which needs to be completely precise to support correct reasoning. The perception questions in our framework reveal that the model cannot clearly understand the number of lines, explaining why it fails to answer the puzzle even with correct hints (few-shot). A single error in visual feature perception can impact reasoning since the correct pattern must apply to all puzzle shapes. The densely packed information distribution--where the majority of the puzzle remains blank--ensures that each piece of visual perception is an essential foundation for subsequent reasoning. That also explains why GPT-4o struggles to achieve significant reasoning performance even with the highest performance on fine-grained perception questions. However, the importance of visual detail perception has received little attention in previous evaluations [46; 45], highlighting the significance of our new evaluation framework.

## 7 A Lens to Reasoning Through the Haze of Perception

Since poor visual perception is the main obstacle to improving MLLMs' abstract reasoning ability, in this section, we conduct three additional experiments to understand these models' potential when perceptual barriers are mitigated (full detail in Appendices G and H).

In the first experiment, we analyze the model's perception ability on the question part by presenting models with the same puzzle but asking for possible underlying patterns in a multiple-choice setting instead of the whole AVR reasoning question. As shown in Figure 5, closed-source models show non-random results when reasoning about the underlying pattern, while nearly all open-source models struggle to outperform random baselines. This gap indicates that closed-source models partially understand the patterns, but more accurate visual perception is needed to complete the entire task.

To further alleviate perception barriers, we add accurate text descriptions of the puzzle on a subset of MARVEL (Table 3). The result shows a significant boost in performance, with GPT-4V achieving human-level accuracy (65%). On the contrary, open-source MLLMs still lag behind, indicating that while enhanced textual descriptions can improve performance, they do not fully bridge the gap between closed-source and open-source models. This further supports that closed-source models possess superior reasoning capabilities often overshadowed by visual perception. The distinction between different MLLMs also underscores the potential effectiveness of MARVEL in evaluating AVR ability, particularly when the weakness of visual perception is addressed.

To address concerns about domain shifts affecting model performance, we conducted domain adaptation experiments to evaluate the effectiveness of fine-tuning on perceptual tasks. We fine-tuned

   &  &  \\   & &  &  &  &  &  &  &  &  &  &  &  &  \\  AVR & 23.16 & 23.161 & 23.161 & 23.161 & 23.161 & 21.05 & 21.05 & 23.16 & 26.321 & 27.371 & 30.53 \\ AVR+Text & 242.21\({}_{1:18}\) & 26.32\({}_{1:14}\) & 26.32\({}_{1:18}\) & 13.68\({}_{9:46}\) & 28.42\({}_{1:77}\) & 65.26\({}_{1:42}\) & **58.95\({}_{1:58}\)** & **37.89\({}_{11:51}\)** & **49.47\({}_{1:21}\)** & **55.79\({}_{1:23}\)** \\  

Table 3: Performance of different models after introducing text description in the input. \(\) notes the result is attributed to inductive bias.

Figure 5: MLLMs performance on pattern classification. The black dotted line represents the random baseline.

LLaVA-1.5  using Qwen-1.5B , with a 4:1 training-validation split over 10 epochs. The model was trained on both AVR and perception questions, and performance was measured on the validation set after each epoch. For AVR questions, the average accuracy was 19.76%, ranging from 12.99% to 29.87% (random chance 25%). For perception questions, average accuracy reached 59.74%, ranging from 53.89% to 62.98% (random chance 50%). These results indicate that poor performance is not solely attributable to domain shifts, aligning with prior studies , which emphasize the inherent complexity of perception tasks that may require multi-task integration. The perception questions in MARVEL thus serve as a valuable benchmark for assessing MLLMs' perceptual capabilities.

## 8 Conclusion

In this work, we develop MARVEL, a multidimensional abstract visual reasoning benchmark consisting of 770 puzzles with both geometric and abstract input shapes across six patterns and five task configurations. We also design a hierarchical evaluation framework that enriches MARVEL with perception questions to enable granular analysis of models' visual details understanding and reasoning consistency. Our comprehensive experiments with ten SOTA MLLMs reveal a huge gap in abstract visual reasoning ability between (40%) humans and MLLMs, where all MLLMs often perform close to random. Further analysis based on our evaluation framework shows MLLMs' poor perception ability in understanding visual details, which hinders their subsequent reasoning and leads to poor AVR performance. We hope future works can build on the foundation of MARVEL for enhancing MLLM abstract visual perception and reasoning abilities.