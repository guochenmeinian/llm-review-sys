# Learning De-Biased Representations for

Remote-Sensing Imagery

 Zichen Tian   Zhaozheng Chen   Qianru Sun

School of Computing and Information Systems

Singapore Management University

{zichen.tian.2023,zzchen.2019}@phdcs.smu.edu.sg,qianrusun@smu.edu.sg

###### Abstract

Remote sensing (RS) imagery, which requires specialized satellites to collect and is difficult to annotate, suffers from data scarcity and class imbalance in certain spectrums. Due to their data scarcity, training large-scale RS models from scratch is unrealistic, and the alternative is to transfer pre-trained models by fine-tuning or a more data-efficient method LoRA . Due to class imbalance, transferred models exhibit strong bias, where features of the major class dominate over those of the minor class. In this paper, we propose debLoRA--a generic training approach that works with any LoRA variants to yield debiased features. It is an unsupervised learning approach that can diversify minor class features based on the shared attributes with major classes, where the attributes are obtained by a simple step of clustering. To evaluate it, we conduct extensive experiments in two transfer learning scenarios in the RS domain: from natural to optical RS images, and from optical RS to multi-spectrum RS images. We perform object classification and oriented object detection tasks on the optical RS dataset DOTA and the SAR dataset FUSRS. Results show that our debLoRA consistently surpasses prior arts across these RS adaptation settings, yielding up to 3.3 and 4.7 percentage points gains on the tail classes for natural \(\) optical RS and optical RS \(\) multi-spectrum RS adaptations, respectively, while preserving the performance on head classes, substantiating its efficacy and adaptability 1.

## 1 Introduction

Remote sensing (RS) is crucial in various applications such as environmental monitoring, resource management, and disaster response . RS data is collected by various sensors and has multiple spectrums, including optical RS imagery (dubbed as ORS, 400-700nm) , multi-spectral RS imagery (MSRS, 400-2500nm) , and synthetic aperture radar imagery (SAR, 1mm-1m) . These spectrums differ significantly in imaging mechanisms, leading to distinct data characteristics and processing pipelines . Given this diversity, learning robust and generic representation models for such data is desirable to reduce processing costs and complexities.

Recently, in natural image domains, large-scale pre-trained visual foundation models (_e.g._, CLIP , Stable Diffusion , and DINO ) have shown great advances in robustness and generalization ability. The zero-shot features extracted from the models show impressive performance in downstream tasks such as object classification, detection and semantic segmentation , even outperforming the supervised models trained on the specific datasets of those tasks. However, in the RS domain, training such foundation models from scratch remains challenging. Even though some trials have been made in past years , their works have clear limitations. First, they require large-scale RS data for effective training, which are available for only ORS but not other spectrums such as SARand MSRS [43; 10; 17]. Collecting and annotating images in "other" spectrums is difficult due to many factors such as military restrictions, sensor availability, and high acquisition costs, so the data scarcity is unlikely to be alleviated in the near future . Second, their works are constrained in small- or medium-scale models, _i.e._, they use ViT-L (300M) in  and Swin-L (197M) in , while the foundation models in the natural image domain are much larger (_e.g._, Latent Diffusion has 860M, and OpenCLIP-H/14 has 986M). Third, their training-from-scratch approaches are computationally inefficient, requiring a huge amount of GPU memory (VRAM). For instance,  reported the need of 80 * A100 GPU with 80GB VRAM each, totaling 6.4TB.

Instead of learning a foundation model from scratch, we propose to transfer existing foundation models to RS domains. This approach is both data-efficient and computation-efficient. We answer two questions: 1) Which foundation models to transfer? 2) Which transfer learning methods to use?

For the first question, we consider foundation models pre-trained on natural images (_e.g._, CLIP , Stable Diffusion ) as well as the models from remote sensing (RS) images (_e.g._, SkySense ). A positive aspect of these models is that they contain the semantic knowledge necessary for learning a new RS domain. However, a great challenge is the large domain gap between natural images and RS domains, or between different RS spectrums. In our preliminary study, we conduct validation experiments. Fortunately, we observe successful transfer results both from natural to ORS in Figure 1 and between different RS spectrums in Table 3, when compared to the method of TRS-Res101  which does not perform any transfer learning. The success of natural\(\)ORS is due to the shared underlying visual elements like edges, textures, and contours, which are intrinsic to both natural and RS images. The success of ORS\(\)other RS is due to the shared spatial structures, _e.g._, urban areas, buildings, and object outlines, in different RS spectrums.

For the second question, we found that data-efficient transfer learning methods on foundation models exhibit a strong bias towards major classes. As shown in Fig. 1, both Fine-Tune and LoRA have significantly lower F1 scores for tail classes. This is because their learned feature space is biased towards the discriminative features of head classes while neglecting the tail . Taking the head class ship (which takes 28.35%) and tail class helicopter (0.64%) as examples on the DOTA dataset . Fig. 2(a) shows biased LoRA features of "oval tail" in the ship sample \(n\) and "rotor tail" in the helicopter sample \(m\). We say biased because the LoRA fails to understand the "oval tail with a rotor" in another helicopter sample \(m^{}\) and embeds \(m^{}\) wrongly as a ship sample in the feature space. Please note that the real feature distribution is shown in Figure 3 to support the illustration of Figure 2. This long-tail issue is particularly severe for transfer learning in the RS domain due to two reasons. _First, RS datasets suffer from more severe data imbalance than natural image datasets_. For instance, the imbalance ratios2 of RS datasets DOTA and ShipRSImageNet reach 86 and 112, respectively, while CIFAR100-LT , a natural image dataset with a similar data scale, has a ratio of only 50. This is because annotating under-represented tail class samples in RS, _e.g._, identifying a rare naval vessel, such as the "Nimitz", from SAR image, requires a high level of domain expertise. _Second, the data scarcity in RS domains determines that RS adaptation methods must be data-efficient_, such as LoRA. However, as shown in Table 2, using fewer parameters in LoRA (being more data-efficient) exacerbates long-tail issues. The reason is that this restricts the model capacity and forces the model to prioritize a limited number of features--usually from head classes.

Figure 1: **Long-tailed Problems.** This figure shows 1) ORS datasets (take DOTA  as an example) have the long-tailed distribution issue. 2) Model adaptation methods suffer from weak performance in tail classes.

To mitigate this bias without needing more data or labels in tail classes, we propose an unsupervised learning approach, debiased LoRA, dubbed debLoRA. debLoRA is based on the features extracted from LoRA (or a LoRA variant) and is generic to LoRA variants. To be concise, we use LoRA in the following to represent itself and its variants. Given the LoRA features, debLoRA has three steps: clustering, calibration, and training. First, it clusters all the features regardless of class labels by \(K\)-means. Each obtained cluster center represents an attribute from one or shared by multiple classes. Second, these cluster centers are used to calibrate the LoRA features of tail classes and enhance the territory of tail classes in the feature space. We illustrate these two steps in Figure 2. Last, the calibrated features are used as the learning objectives to train a debLoRA module with a similar network architecture to LoRA. The learned debLoRA is thus a de-biased feature extractor.

We observe that after \(K\)-means clustering, each cluster center captures a general visual attribute shared across different classes. For instance, in Figure 2(b), cluster \(A\) corresponds to the general vehicle attribute "streamlined tail", which includes both head class sample \(n\) and tail class sample \(m\). Such clusters can thus yield a balanced representation base, making the tail more robust by integrating common attributes with the head.

One may ask "what if some attributes are dominated by the attribute features of head classes?" We address this question by proposing a weighting scheme, in the step of calibration. In specific, for each tail class sample (_e.g._, \(m\) in Fig. 2(c)), we calibrate it by forcing its feature closer to the de-biased center (\(D\))--the weighted average of all cluster centers. The weights are determined by the number of samples in each cluster, ensuring that this center is not dominated by clusters with mostly head class samples. This calibration process results in de-biased representations that capture a more comprehensive range of visual attributes shared across classes, leading to improved features of tail classes (_e.g._, \(m^{}\)). Lastly, we re-train a LoRA module to map biased representations towards these debiased centers. Please find more details of justifications in Sec. 4.4. Our method significantly improves the features of tail classes. Moreover, it is efficient as it learns only a lightweight low-rank module while keeping the original foundation model frozen.

Our contributions can be concluded three-fold: 1) We demonstrate the effectiveness of adapting foundation models for data-scarce RS domains. 2) We propose Incremental LoRA, a novel method that de-biases category-specific representations for long-tailed RS adaptation. 3) We conduct extensive experiments to validate our approach on multiple RS adaptation settings and downstream tasks.

## 2 Related Works

**Representation Learning for RS Images.** Self-supervised representation learning in RS image domains mainly includes contrastive- and generative-based methods. **Contrastive-based** methods, such as Tile2vec , Seasonal contrast  and SauMoCo , heavily rely on rich temporal data or high-resolution samples, which are often unavailable for data-scarce RS spectrums . **Generative-based** methods, such as RR-SSL  and SGSAGANs , reconstruct inputs to capture the global data distribution and learn fine-grained patterns. However, they require large-scale data to form robust latent space . Recently, **foundation models** in the RS domain, such as

Figure 2: **Two key steps of debLoRA: feature clustering and calibration. (a) The baseline LoRA feature space is biased towards head classes. Red crosses represent head class samples, and blue triangles represent tail class samples. The blue star indicates the center of tail class samples. Dashed blue triangles show the validation samples of the tail class wrongly embedded in the head class region, indicating the model bias towards head classes. (b) We cluster all features (clusters denoted by gray dotted boundaries) regardless of class labels. \(A\), \(B\) and \(C\) are cluster centers used to generate a de-biased center \(D\), as in Eq. 2. (c) We calibrate the tail class features by “moving” them closer to \(D\), as in Eq. 3. After these steps, we train the debLoRA module on the calibrated features of tail classes (together with the original head class features).**

SatMAE , SpectralGPT , and SkySense , have shown superior performance for ORS tasks. SpectralGPT  tackles spectrum diversity by pre-training separate tokenizers for each spectrum, which still needs large amounts of data. Another problem is that existing RS foundation models are much smaller than those in the natural image domain (_e.g._, SatMAE-L  has 300M parameters _v.s._ 986M of OpenCLIP-H/14 ). Instead of self-supervised training RS models from scratch, we propose to adapt existing foundation models to RS tasks. Our approach: 1) reduces computational cost significantly, 2) can be easily adapted to various data-scar RS spectrums, and 3) benefits from powerful representations from natural vision foundation models.

**Long-tailed Data Distribution and its Bias Problem.** Long-tailed data distribution, where a few head classes cover most of the samples, is prevalent in both natural and RS image domains [55; 69]. This imbalance leads to biased feature representations, where the model focuses on discriminative features for head classes while neglecting subtle but crucial features for tail classes [69; 28]. Zhang et al.  observed that such a feature space is usually broader for head classes than tail classes, and the decision boundary tends to be biased towards head classes, _i.e._, many false positive predictions for head classes. Existing solutions include sample-level, meta-learning, and representation-level approaches : **Sample-level** methods, such as re-sampling  and data augmentation , aim to directly balance the sample distribution. However, they require sample annotations [2; 49] or rely on data diversity , both of which are unrealistic in the data-scar RS spectrums such as SAR  and MSRS . **Meta-learning** methods [26; 57] formulate the problem as "learning to learn" and adapt the model to a balanced meta-test set. They depend on the data diversity of the training sets and the availability of balanced validation sets, and therefore, are less applicable for data-scar RS domains. The **representation-level** methods enhance the learned representation space, including metric learning losses , margin-based losses , and feature transfer from head to tail classes [33; 63]. However, they are designed for supervised single-domain settings and do not address the challenges of model adaptation to RS: 1) handling multiple downstream tasks (_e.g._, small object detection, scene segmentation, change detection), and 2) multiple spectrums (such as ORS and SAR). In contrast, we propose an _unsupervised adaptation_ method to tackle these challenges in this paper.

**Transfer Learning in Remote Sensing.** Transfer learning in remote sensing primarily focuses on adaptation within the optical imagery domain. They can be categorized into supervised and unsupervised methods. Supervised methods [12; 35; 46; 44; 39] align distributions using target labels. However, they require task-specific annotations, which are scarce in SAR and multispectral domains and limit the applicability of the obtained models to multiple downstream tasks. Unsupervised DA (UDA) methods aim to learn domain-invariant features without requiring labeled data in the target domain, including transfer component analysis [42; 40], manifold alignment [53; 60; 61], and adversarial learning [1; 11; 51]. However, they are designed for single-source, single-target adaptation within the same spectrum [41; 38]. Besides, the manifold alignment and adversarial methods require significant computational resources, often involving the training of several copies of the source model, while component analysis methods involve complex pipelines. These factors make them unsuitable for foundation models, which are already computationally intensive. In contrast, our method tackles multi-spectrum adaptation without requiring extra labels. It is also computationally efficient.

## 3 LoRA and cLoRA

Our debLoRA is based on the LoRA  or its variants , but is orthogonal and generic to them.

**LoRA.** LoRA was initially proposed to adapt a pre-trained large-scale language model to downstream tasks. It assumes adapted parameters are sparse during model training when the data is limited. It introduces a low-rank factorization of the difference between original and adapted parameters, _i.e._, \(:=B A\). Here, \(^{d k}\) represents the parameters of pre-trained model, and \(B^{d r}\) and \(A^{r k}\) denote low-rank factors, with \(r(d,k)\). The updated parameters \(\) are thus given by \(=+=+B A\). During inference, the obtained LoRA modules could be combined through a weighted sum, \(=+_{i}w_{i}_{i}\), where \(w_{i}\) denotes combination weights.

**cLoRA.** To tackle the long-tailed issue of LoRA, we also explore its variant cLoRA . The key idea of cLoRA is to learn a separate LoRA module for each class, denoted as \(_{c}\) for class \(c\), to ensure that the learned representations of one class do not interfere with those of other classes. Formally, the adapted parameters for class \(c\) are given by \(_{c}=+_{c}=+B_{c} A_{c}\), where \(B_{c}^{d r}\) and \(A_{c}^{r k}\) are the low-rank factors specific to class \(c\). During training, each cLoRA module \(_{c}\) is optimized using only the data from class \(c\), allowing it to capture class-specific features. Duringinference, as there is no class label available, we use all the cLoRA modules to extract features for the input. Specifically, for an input \(x\), we obtain the features \(z_{c}=_{c}(x)\) using each cLoRA module \(_{c}\). The final feature representation is then obtained by concatenating the features from all the cLoRA: \(z=[z_{1};z_{2};;z_{C}]\), where \(C\) is the total number of classes.

## 4 De-biased LoRA (\(\))

The algorithm of \(\) consists of two steps: generating debiased features, and then using them to train a \(\) module. In the first step, we perform unsupervised clustering on biased feature space \(\) (_i.e._, composed by original LoRA features biased to head classes) to obtain debiased features \(}\). In the second step, we use \(}\) as the learning target to train a \(\) module. The \(\) learns the mapping between biased and de-biased features. We justify the feasibility of learning such a mapping in Section 4.4.

### Problem Formulation

Given a pre-trained feature extractor \(f:\) and a long-tailed RS dataset \(=(x,y)\), where \(x\) is an RS image, \(y\) is its annotation and \(\) is the biased feature space3, our goal is to adapt \(f\) to the target dataset \(\) while yielding a de-biased feature space \(}\), _i.e._, adapted encoder is \(:}\). The de-biased feature representation \(}\) should improve downstream task performance on tail classes without sacrificing the performance on head classes.

### Stage 1: Representation De-biasing

Feature Clustering.Given a pre-trained encoder \(f_{}:\) that maps input images to a biased representation space, where \(f_{}\) is parameterized by \(\), we first extract features for each sample in the dataset: \(z_{i}=f_{}(x_{i})\), \(i N\). We then apply \(K\)-means clustering on \(\{z_{i}\}\) to obtain \(K\) clusters. To mitigate imbalanced clusters, we impose a constraint that each cluster should contain at least \(\) samples, where \(\) is a pre-defined constant. The clustering objective is:

\[_{_{k}}_{i=1}^{N}_{k}\|z_{i}-_{k}\|^{2},\ \  k,\ n_{k},\] (1)

where \(_{k}\) and \(n_{k}\) denote the center and size of the \(k\)-th cluster, respectively.

De-biased Cluster Centers.For each tail class \(c\), we calculate its de-biased representation center \(_{c}\) by weighted averaging all the cluster centers:

\[_{c}=_{k}w_{k}_{k},\ \ w_{k}=}{n_{c}}.\] (2)

Here \(n_{k}\) denotes the number of samples from class \(c\) in the \(k\)-th cluster, and \(n_{c}\) is the total number of samples in class \(c\). The weight \(w_{k}\) is proportional to the fraction of class \(c\) samples in the \(k\)-th cluster. This ensures that the de-biased center \(\) is not dominated by head classes.

### Stage 2: De-Biased Low Rank Adaptation (\(\))

Tail Class Calibration.For each tail class sample \(x\) with representation \(z\), we calibrate \(z\) by moving it closer to the de-biased center \(\):

\[= z+(1-),\] (3)

where \(\) is a hyper-parameter controlling the degree of calibration. We empirically set \(\) based on the imbalance ratio \(\) of each tail class: \(=(1,)\). For tail classes with larger imbalance ratio, a higher \(\) encourages the calibrated representation \(\) to be closer to the de-biased center \(\), as the original representation \(z\) is less reliable due to its learning from limited samples. While for classes with smaller \(\), a lower \(\) is used to retain the discriminative information of \(z\). For instance, the DOTA dataset's tail class \(\) has high \(=45.45\), so its \(\) reaches \(0.22\).

**Learning** debLoRA.With the pre-trained encoder \(f_{}\) frozen, we learn a LoRA module \(g_{}:}\) parameterized by \(\) to map the biased representations to the calibrated ones. The training objective is:

\[_{}_{t}|}_{x_{t}}\|g_{}(f_{ }(x))-\|^{2},\] (4)

where \(_{t}\) is the set of tail class samples. During inference, we apply the learned LoRA module to extract the de-biased representations \(z=g_{}(f_{}(x))\) for an input image \(x\). The complete algorithm of debLoRA is summarized in Algorithm 1.

```
0: Long-tailed training set \(=\{(x,y)\}\), pre-trained encoder \(f_{}:\), number of clusters \(K\), balance factor \(\)
0: A LoRA module \(g_{}\) that de-biases \(f_{}\)
1: Extract biased representations \(z=f_{}(x)\) for each sample \(x\) using pre-trained \(f_{}\)
2: Perform constrained \(K\)-means clustering on \(\{z\}\) (equation 1) to obtain cluster centers \(\{_{k}\}_{k=1}^{K}\), where each cluster has at least \(\) samples
3:for each tail class \(c\)do
4: Calculate its de-biased representation center \(_{c}\) by weighted averaging all cluster centers \(\{_{k}\}_{k=1}^{K}\) (equation 2)
5:for each sample \(x_{c}\)do
6: Extract biased representation \(z=f_{}(x)\)
7: Calibrate \(z\) to \(\) by moving it closer to \(_{c}\) with factor \(=10/\) (equation 3)
8:endfor
9:endfor
10: Learn a LoRA module \(g_{}:}\) to map biased representations to calibrated ones
11:return\(g_{}\) ```

**Algorithm 1** debLoRA

### Justification

We discuss the biased representation space of LoRA, and then justify the effectiveness of our three critical operations in debLoRA: **clustering, weighting,** and **calibration**. We show the real sample distribution in Figure 3 and an illustrative example in Figure 2.

**LoRA is Biased.** The feature space learned by LoRA is biased towards head classes , evidenced by two observations. 1) The head class representations over-expand their territory into the tail class space. As shown in Figure 3, most of the ship (head) validation samples are distributed within its own representation space, while many helicopter (tail) validation samples are wrongly distributed

Figure 3: **t-SNE visualization of validation samples and clusters.** The first column shows the distribution of helicopter (tail) and ship (head) validation samples. Subfigures (c)-(g) are the clusters and their centers when \(K\)=5 in \(K\)-means. In (h), the dotted lines and stars indicate that we compute a de-biased center for the tail class (helicopter) by weighted averaging the five cluster centers, and the blue star is the original biased center of helicopter training samples.

in the ship's space. 2) The center of the entire space is biased towards head class, as the ship training samples significantly overlap with the helicopter training samples. This bias occurs because, during training, the encoder is exposed to significantly more diverse samples of head class.

Clustering.By feature clustering, we obtain a set of cluster centers that benefit the tail classes in two ways. 1) _Improved robustness._ The obtained cluster centers, shown as red stars in Figure 3(c)-(g), represent visual prototypes, _i.e._, general visual attributes common to both head and tail classes, such as "streamlined tail" or "with wooden deck". These cluster centers are more robust than the original tail class representations because they leverage the diversity of head class samples. 2) _Reduced imbalance._ Certain clusters exhibit reduced long-tail issues. The clusters in Figure 3(d)-(f) contain more samples from helicopter than ship. This is because the clusters are formed based on intrinsic visual similarities among images, regardless of their imbalanced class labels. Using these cluster centers avoids the risk of tail class attributes (_e.g._, "rotor tail" and its variants in helicopter) being overwhelmed by head class attributes (_e.g._, "oval tail" and its variants in ship).

Weighting and Calibration.One might ask, "Are the data imbalances within each cluster or among different clusters still issues?" _E.g._, the 5-th cluster in Figure 3 contains only ship samples and seems irrelevant to helicopter. To answer this, we perform the _weighted averaging_ over cluster centers, and the _calibration_ over tail class samples: 1) _Weighted averaging._ When calculating the de-biased representation center for each tail class (equation 2), we assign higher weights to clusters containing a larger fraction of that particular tail class. The de-biased center (red star in Figure 3(h)) better captures the true distribution of the validation samples of helicopter, compared to the original biased center (blue star in Figure 3(h)). 2) _Calibration._ We calibrate the representation of each tail class sample by moving it closer to the class's de-biased center (equation 3). The calibration factor \(\) is inversely proportional to the imbalance ratio of the tail class. This design ensures severely underrepresented tail classes like helicopter receive stronger calibration.

## 5 Experiments and Analyses

We evaluate our debLoRA on two settings: 1) adapting natural image foundation models to RS, and 2) adapting ORS foundation models to SAR. For the first setting, we conduct experiments on two representative RS tasks: object classification and oriented object detection. For the second setting, we conduct experiments on a representative SAR task--fine-grained ship classification.

Natural \(\) RS adaptation.1) _Foundation model._ We use two state-of-the-art foundation models: Stable Diffusion v1.5 (SD)  and OpenCLIP . Both models have shown impressive generalization ability on various tasks when adapted to domains like medical images . However, their transferability from natural images to the RS domain remains under-explored. 2) _RS dataset._ We use the DOTA dataset , a large-scale benchmark for RS object recognition. DOTA contains 188,282 instances from 15 categories, covering various scales, orientations, and shapes. We define the long-tail split as follows: 6 classes with <1% instances as tail, 3 classes with 1%-5% instances as middle, and the remaining 6 classes (each with >5% instances) as head. This split exhibits a clear long-tail distribution, evidenced by the performance gap between head and tail classes for the baseline methods (see Table 1 row 1). 3) _Tasks._ For the classification task, we obtain features from the adapted foundation models and train a linear classifier. We report the macro F1-score that fairly evaluate the performance across all classes. For detection, we train a FCOS detector head  on obtained representations and evaluate using the mAP.

Ors \(\) SAR adaptation.1) _Foundation model._ We use SatMAE-L , the state-of-the-art open-sourced foundation model for RS. SatMAE-L is pre-trained on large ORS datasets using self-supervised learning. It has \(307\)M parameters and requires 6,144 GPU hours to train from scratch. 2) _SAR dataset._ We evaluate our method on the fine-grained ship classification task of SAR. Existing SAR ship datasets have insufficient samples to evaluate the model performance reliably, _e.g._, only 2 samples in test set for tail class "WingInGrnd" on the FUSAR-Ship dataset. We thus create a new dataset by combining two high-resolution (<10m/pixel) SAR datasets: FUSAR-Ship  and SRSDD . Details of this combined dataset are provided in the Appendix. 3) _Ship classification task._ We follow the same setup as in the natural \(\) RS setting for this SAR task.

Implementation Details.1) _Fine-tuning baseline._ We fine-tune the foundation models until the training loss stabilizes. During inference, we use null prompts as no ground truth is available. For SD, we extract features from the U-Net after applying one denoising step . For OpenCLIP, we extract features from its visual encoder's final layer before the projection head. 2) _LoRA and variants._We apply LoRA modules to all linear layers in the foundation models. We use a rank of 8 for LoRA, as it suffers from the most severe long-tail issues. We also validate our method with higher ranks (_e.g._, 64) in Table 2. During inference, we extract features from the U-Net encoder output followed by global average pooling (GAP). For cLoRA, we concatenate the category-specific features after GAP. 3) _debLoRA_. The debLoRA involves two hyperparameters: the calibration factor \(\), and the number of clusters \(K\). We set \(\) as inversely proportional to the imbalance ratio of the tail class, as described in Section 4.4. We empirically set \(K\)=32 (ablation study on \(K\) are provided in Appendix).

**Evaluation Metrics.** 1) _Classification._ We use linear probing (i.e., train a linear classifier on the top of frozen features) to evaluate the learned representations . It is simple and avoids introducing additional learning operations. We apply GAP and ReLU on the extracted features before linear probing. We report the macro F1-score, which assigns equal importance to all classes--more suitable for evaluating imbalanced datasets. We report scores for head, middle, and tail classes separately, as well as the overall score averaged across all categories. 2) _Detection._ We use the lightweight FCOS , an anchor-free detector head, to avoid potential interference from pre-defined anchors. We extract high-resolution feature maps from the SD U-Net output. During feature clustering and re-training, we use per-instance features for each category. During inference, we extract features from the entire image and feed them to the detector head. We report the mAP metric.

**Ablation study.** In Table 1, rows 1 and 2 show the results of using zero-shot features of SD or fine-tuned SD features on DOTA to train RS object recognizers. Recognizers' performances are strongly biased to head classes--around 12 percentage points drop for tail classes. From rows 3 and 5, we can see such issues get resolved a bit when using LoRA methods. Rows 4 and 6 show that debLoRA significantly outperforms LoRA methods on tail classes--by 4.2 points and 2.7 points, respectively. Specifically, compared to cLoRA, debLoRA does not even sacrifice the performance for head classes. To quantitatively validate its working mechanism, we analyzed feature discrimination. Results show that debLoRA enlarges inter-class distances and reduces intra-class distances for tail classes (see Appendix). In addition, debLoRA needs just the same amount of parameters as LoRA (0.08M), which is appealing for computation.

**LoRA Ranks.** We investigate the impact of different LoRA ranks on the long-tailed classification performance in Table 2. We have two key observations. 1) As the LoRA rank decreases, the performance on tail classes drops more significantly than on head classes. For example, when the rank is reduced from 64 to 8, the F1-score of tail classes decreases by 2.2 percentage points, while that of head classes even increases by 0.3 percentage. This supports our hypothesis that the limited parameter capacity of low-rank LoRA forces it to prioritize learning the head classes, exacerbating the long-tail problem. 2) debLoRA consistently improves the performance on middle and tail classes across different LoRA ranks. Notably, with rank 64, debLoRA achieves a 2.2 percentage points improvement on tail classes while maintaining the performance on head classes.

**Compare with SOTA.** 1) _Object Classification._ Table 3 compares our debLoRA with state-of-the-art methods under three adaptation tasks. We draw four key observations from the table. 1) debLoRA consistently outperforms LoRA on tail classes across all adaptation tasks, with the largest gain of 4.7 percentage points for ORS \(\) SAR (_i.e._, SatMAE \(\) FUSRS). This shows the consistent efficiency of our approach in tackling the long-tail problem of RS domains. 2) Compared to SD \(\) DOTA setting, cLoRA performs exceptionally well under OpenCLIP \(\) DOTA setting, slightly surpassing LoRA. We hypothesize that OpenCLIP's feature space aligns particularly well with

    &  &  \\  & Head & Middle & Tail & Overall \\  Zero-Shot & 99.2 & 97.3 & 87.8 & 94.3 & - \\ Fine-Tune & 99.1 & 96.7 & 86.8 & 93.7 & 860 \\   LoRA & 99.1 & 94.3 & 89.3 & 94.2 & 0.08 \\  w/debLoRA & **99.3** & **97.5** & **93.5** & **96.6** & 0.08 \\  LoRA & **99.4** & 97.2 & 91.8 & 95.9 & 0.08 \\  w/debLoRA & 99.1 & **98.7** & **94.5** & **97.1** & 0.08 \\   

Table 1: **Ablation study of debLoRA.** We apply our debLoRA based on LoRA and cLoRA. Results are reported for the adaptation from SD \(\) DOTA recognizer. Params (M) refers to the number of updated parameters during the adaptation. Our results are marked in gray.

    &  &  \\  & Head & Middle & Tail & Overall \\  Rank 8 & 99.4 & 97.2 & 91.8 & 96.1 & 0.08 \\  w/debLoRA & 99.1 & 98.7 & 94.5 & 97.1 & 0.08 \\  Rank 16 & 99.0 & 95.9 & 92.4 & 95.8 & 0.16 \\ Rank 32 & 99.4 & 96.9 & 93.0 & 96.4 & 0.32 \\ Rank 64 & 99.1 & 96.9 & 94.0 & 96.7 & 0.64 \\  w/debLoRA & 99.1 & 98.7 & 96.2 & 98.0 & 0.64 \\   

Table 2: **Compare LoRA ranks.** The table compares different ranks of the LoRA module. Our results are marked in gray.

design. However, debLoRA remains robust across both foundation models. 3) The performance gains of debLoRA are most significant for SatMAE \(\) FUSRS (+4.7 points) compared to SD \(\) DOTA and OpenCLIP \(\) DOTA (+3.3 and +3.2 points, respectively). This suggests that our method can leverage domain similarity more effectively when adapting between related image domains (SatMAE and FUSRS are RS datasets). We think this is because debLoRA's clustering step captures and utilizes the shared domain-specific visual patterns (_e.g._, spatial structures and textures) when the source and target domains are closely related. 4) debLoRA consistently outperforms long-tailed recognition methods, ResLT  and SADE  (2.5 and 2.9 points by average). ResLT and SADE mainly introduce re-weighting strategies to balance the learning of different classes, but they do not directly rectify the bias in the feature space. In contrast, debLoRA explicitly learns a de-biased representation center for tail classes. 5) We further validate the generalizability of our method by conducting experiments on additional long-tailed datasets Places365-LT , iNaturalist , and fMoW-S2 [6; 8]. Our debLoRA consistently outperforms baselines, achieving up to 7.2% improvement on tail classes (see Appendix). 2) _Oriented Object Detection_. We validate our method's generalization ability on the oriented object detection task in Table 4. We have two key findings. 1) Our debLoRA achieves the highest mAP scores across all positions. Notably, debLoRA outperforms vanilla LoRA by an impressive 6.7 percentage points. 2) Notably, all methods performed better in the middle classes than in the head. This might be attributed to the greater intra-class variation in head classes, whereas middle classes have more distinct and compact features.

## 6 Conclusion

In this paper, we propose debLoRA, a novel approach for adapting foundation models to data-scarce and long-tailed remote sensing domains while mitigating representation bias. Our method introduces unsupervised clustering to capture robust visual attributes shared across classes, and feature calibration to rectify the bias in tail class representations. We validate the effectiveness of debLoRA through extensive experiments on multiple RS adaptation settings and downstream tasks, where it consistently outperforms vanilla LoRA and other long-tailed recognition methods. Notably, debLoRA achieves significant performance gains on tail classes without sacrificing the performance on head classes, highlighting its ability to learn debiased feature representations.

**Acknowledgments and Disclosure of Funding**

The author gratefully acknowledges the support from the DSO research grant awarded by DSO National Laboratories, Singapore, and the Lee Kong Chian Fellow grant awarded to Dr. Qianru Sun by Singapore Management University.

    &  &  \\   & Head & Middle & Tail & (\%)\(\) \\   Zero-Shot & 71.0 & 73.7 & 55.9 & 66.9 \\ Fine-Tune & 76.3 & 84.9 & 64.3 & 75.2 \\ LoRA & 77.5 & 86.3 & 66.5 & 76.7 \\  w/ Reweight  & 74.3 & 86.8 & 66.9 & 76.0 \\ w/ ECM  & 78.1 & 87.4 & 68.5 & 78.0 \\ w/ deblLoRA & **79.4** & **88.5** & **73.2** & **80.4** \\   

Table 4: **Evaluation on the oriented object detection task. We implement debLoRA for long-tailed detection tasks. Our results are marked in gray\(\)**

    &  &  &  &  \\   & Head & Middle & Tail & Head & Middle & Tail & Head & Tail & Head & Middle & Tail \\   Zero-Shot & 99.2 & 97.3 & 87.9 & 93.1 & 92.7 & 91.7 & 78.3 & 67.8 & 90.2 & 95.0 & 82.5 \\ Fine-Tune & 99.1 & 96.7 & 86.8 & 93.1 & 91.1 & 89.2 & 88.6 & 73.6 & 93.6 & 93.9 & 83.2 \\   cLoRA & 99.1 & 94.3 & 89.3 & 97.3 & 93.3 & 92.2 & 89.9 & 82.0 & 95.5 & 93.8 & 87.9 \\ w/ debLoRA & 99.3 & 97.5 & 93.5 & **97.6** & **95.8** & **95.0** & **92.5** & **86.1** & **96.5** & **96.7** & **91.5** \\   LoRA & 99.4 & 97.2 & 91.8 & 96.6 & 92.7 & 91.6 & 87.1 & 76.3 & 94.4 & 95.0 & 86.6 \\ w/ ResLT  & **99.4** & 97.7 & 93.0 & 97.7 & 94.1 & 93.8 & 86.6 & 75.4 & 94.6 & 95.9 & 87.4 \\ w/ SADE  & 99.1 & 97.3 & 92.4 & 97.3 & 93.0 & 92.5 & 89.6 & 78.4 & 95.3 & 95.2 & 87.8 \\ w/ deblLoRA & 99.3 & **97.7** & **95.1** & 97.2 & 95.6 & 94.8 & 90.1 & 81.0 & 95.5 & **96.7** & 90.3 \\   

Table 3: **State-of-the-art comparison under different adaptation settings. The experiments are conducted on two RS adaptation settings: 1) Natural\(\)ORS, where we adopt Stable Diffusion (SD) and OpenCLIP as foundation models and DOTA as the target dataset. 2) ORS\(\)SAR, where we adopt SatMAE as the foundation model and FUSRS (SAR imagery dataset) as the target dataset. Results are evaluated by linear probing and reported in macro F1-Score (%). The highest result in each position is highlighted by **bold**. Our results are marked in gray\(\)