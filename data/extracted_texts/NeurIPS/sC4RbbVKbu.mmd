# SutraNets: Sub-series Autoregressive Networks for Long-Sequence, Probabilistic Forecasting

Shane Bergsma   Timothy Zeyl   Lei Guo

Huawei Cloud, Alkaid Lab Canada

{shane.bergsma,timothy.zeyl,leiguo}@huawei.com

###### Abstract

We propose SutraNets, a novel method for neural probabilistic forecasting of long-sequence time series. SutraNets use an autoregressive generative model to factorize the likelihood of long sequences into products of conditional probabilities. When generating long sequences, most autoregressive approaches suffer from harmful error accumulation, as well as challenges in modeling long-distance dependencies. SutraNets treat long, univariate prediction as _multivariate_ prediction over lower-frequency _sub-series_. Autoregression proceeds across time _and_ across sub-series in order to ensure coherent multivariate (and, hence, high-frequency univariate) outputs. Since sub-series can be generated using fewer steps, SutraNets effectively reduce error accumulation and signal path distances. We find SutraNets to significantly improve forecasting accuracy over competitive alternatives on six real-world datasets, including when we vary the number of sub-series and scale up the depth and width of the underlying sequence models.

## 1 Introduction

As a cloud provider, we rely on long-range forecasts to allocate resources and plan capacity. In particular, we need to predict the _fine-grained_ pattern of demand (e.g., hourly) for a large number of products, over a long time horizon (e.g., months). Since purchase decisions are large-scale (e.g., supplying regions with 100K machines), and made weeks-to-months before delivery, cloud providers have a profound need for methods that "capture the uncertainty of the future" . Fine granularity, long histories, long forecast horizons, and _quantification of uncertainty_ can be formalized as the long-sequence (many-step) _probabilistic_ forecasting problem.

Recent work applies Transformers  to long-sequence forecasting. In order to circumvent the Transformer's quadratically-scaling _attention_ mechanism, long sequences necessitate trading off increased signal path for reduced computational complexity . Unfortunately, the effectiveness and even validity of these approaches have been questioned . Most of these approaches do not provide probabilistic outputs (rather best-guess point predictions), and all condition on fixed-size windows, typically of very short duration. Indeed, many recent papers on "long-term" forecasting  condition on a maximum of 96 inputs -- only 4 days of history at 1-hour granularity. Such restricted context is a serious limitation given that many time series (including in standard evaluation datasets based on electricity and traffic) have strong _weekly_ seasonality.

Our production forecasting system, like many in industrial settings  and cloud services , is based on RNNs. The RNN's recurrent architecture forces the compression and consolidation of long-range information, and RNNs work well on smaller datasets as they are not as "data hungry" as Transformers . RNNs can theoretically condition on context of any length; in this paper, we condition RNN forecasts on length-2016 contexts without difficulties in scaling.

RNNs are often used to generate probabilistic forecasts via an autoregressive approach , whereby the probability over future timesteps is factorized into a product of one-step-aheadconditions [6; 28]. When generating many steps, however, two key problems arise: (1) a discrepancy grows between training, where we condition on true previous values, versus inference, where we condition on sampled values (the _discrepancy_ problem), and (2) long-term dependencies may be challenging to exploit (the _signal path_ problem). The discrepancy problem leads to error accumulation during inference , but also a kind of _informational asymmetry_: because highly-informative true values are available during training, the model may ignore other useful info, e.g., seasonally-lagged inputs provided as extra features. The signal path problem, meanwhile, is especially acute for RNNs, where distant information must be preserved over many steps in the RNN hidden state.

We propose SutraNets,1 a general method for probabilistic prediction of long time series. We focus on the application of SutraNets to RNN-based forecasting, where they offer major benefits. SutraNets transform a length-\(N\) series into \(K\) sub-series of length \(}{{K}}\). Sub-series forecasts are generated autoregressively, sequentially conditional on each other, enabling coherent outputs (Fig. 1). SutraNets address both the discrepancy problem (by forcing the network to take larger _generative strides_, i.e., predicting without access to immediately-preceding true values) and the signal path problem (by reducing the distance between historical and output values by a factor of \(K\)). Training of SutraNets _decomposes_ over sub-series; i.e., sub-series RNNs can be trained in _parallel_, enabling a \(K\)-fold improvement in training parallelism over standard RNNs.

Experimentally, SutraNets demonstrate superior forecasting across six datasets, improving accuracy by an average of 15% over C2FAR  and over strong seasonality-aware baselines. We evaluate a variety of other strategies for improving long-sequence forecasting, including lags, input dropout, deeper and wider networks, and multi-rate hierarchical approaches. Through these evaluations, we gain insights into how to succeed at long-sequence generation, in time series and beyond.

## 2 Background and related work

Probabilistic autoregressive forecasting

Autoregressive forecasting models typically combine a backbone sequence model with a technique to estimate an output probability distribution at each step. In this paper, we apply SutraNets to C2FAR , a state-of-the-art distribution estimator that was shown to improve over DeepAR , DeepAR-binned , SQF-RNN , and IQN-RNN . SutraNets can serve as the sequential backbone for any of these estimators, as well as for other recent approaches such as the denoising diffusion models , or conditioned normalizing flows .

Multi-rate forecastingIt is sometimes acceptable to _aggregate_ high-frequency series or event data to a lower frequency prior to sequence modeling, e.g., aggregating patient events over 6-month intervals , or network traffic over 10 subframes . When high-frequency predictions are required, lower-frequency forecasts can provide guidance. Techniques for _temporal disaggregation_ convert low-frequency series to higher frequencies via related high-frequency _indicator_ series [13; 67]. Athanasopoulos et al.  simultaneously forecast at multiple frequencies and then reconcile the predictions. Such methods do not provide uncertainty estimates nor are they based on neural networks.

Scaleformer  aggregates series, forecasts at the lower frequency, and upsamples the lower-freq predictions in order to provide additional inputs for the original forecast model. We evaluate a similar approach, Low2HighFreq (SS3), as a baseline. Other neural approaches (in forecasting and beyond) implicitly encourage learning of lower-frequency dynamics through hierarchical attention [20; 47; 16],

Figure 1: SutraNets: a long, univariate history is converted to lower-frequency sub-series. Multivariate autoregressive prediction, across time and sub-series, ensures coherent univariate output samples.

pooling blocks , and deeply-stacked  and multi-rate  recurrence layers. Unlike these approaches, SutraNets _explicitly_ generate _low-frequency_ sub-series conditional on other low-frequency sub-series, without directly forecasting at a high frequency at all.

Reducing training/inference discrepancyIf we know _a priori_ how far to forecast, it is possible to avoid error accumulation by directly forecasting all horizons in one step , or via non-autoregressive decoding . Such systems usually generate point predictions , although predicting certain fixed forecast quantiles is also possible . ProTran  is a probabilistic but non-autoregressive approach. Without modeling local dependencies among outputs, such models prevent generation of _coherent_ samples, and preclude computing robust likelihoods for _given_ output sequences -- where error accumulation is not an issue (e.g., for anomaly detection, missing value interpolation, compression, etc.). SutraNets are more in-step with flexible autoregressive models like GPT3 .

For autoregressive models, _scheduled sampling_ aims to handle errors better during inference by sampling (incorrect) inputs during _training_. This approach was effective in tasks such as human motion synthesis , but results have been mixed in autoregressive forecasting . Noting that scheduled sampling produces a biased estimator, Lamb et al.  instead used adversarial techniques to reduce training/inference discrepancy. Wu et al.  applied this method to forecasting, using a GAN  with an autoregressive sparse transformer. However, adversarial training showed insignificant gains over the sparse transformer alone on 7-day electricity and traffic predictions.

Like scheduled sampling, _dropout_ is a kind of stochastic regularization that prevents overfitting by injecting noise into networks. Dropout rates of around 20% are commonly used in input layers of autoencoders and feed-forward nets . Dropout is also common in autoregressive models , where, in theory, it may encourage networks to rely less on (now noisy) previous values .

Modeling long-term dependenciesWhen forecasting long series, seasonally _lagged_ values -- historical values from, e.g., one year ago -- can be used as extra features in the sequence model, acting as a kind of residual input . A recent competition featured long time series (e.g., 700 days of history) ; the winning model found lag to be more effective than fixed attention weights . Some researchers have even declared that, when it comes to attention, "lag is all you need" .

Learned _attention_ can capture long-range dependencies , but is infeasible for very long sequences due to memory and time complexity quadratic in sequence length. Complexity also scales quadratically for forecasting via fully-connected layers . Recent work has explored sparsified or hierarchical attention in forecasting . TimesNet  transforms univariate time series into 2D tensors of multiple periods, which are then processed via Inception-style 2D kernels to generate point predictions. As mentioned earlier, most of these systems fail to condition on more than very short windows -- a few days of hourly data. Ultimately, we are interested in _probabilistic_ forecasting using _years_ of fine-grained historical data. It is unclear whether lag is really all we need, or even whether seasonal-naive baselines  are already more effective than Transformers .

Unlike Transformers, RNNs can be applied to long sequences _out-of-the-box_. However, since RNN signal path is linear in sequence length, mechanisms are required to preserve long-range info, such as the special gated units of LSTMs . Multi-dimensional RNNs  propagate distant info via row and column connections across images . Didolkar et al.  combine RNNs for long-range signals with Transformers that attend over shorter chunks. SutraNets, in contrast, do not require architectural changes _within_ RNNs; indeed, they work with RNNs or Transformers. In SutraNets, low-frequency information is both _implicitly_ captured in network state and _explicitly_ and autoregressively communicated through generated low-frequency forecasts.

Comparison to other RNNsTable 1 compares SutraNets to prior RNNs (cf. Table 1 in ). PatchTST  groups consecutive values into input patches. While patching was proposed as a method to reduce the attentional complexity of Transformers, patching was also recently used with RNNs in SegRNN , where it effectively reduces the signal path by a factor of \(K\) (taking in \(K\) inputs each step -- like SutraNets -- but with \(K\) times fewer overall input steps). However, neither PatchTST nor SegRNN provides a mechanism to probabilistically _generate_ patches, and thus generate coherent _probabilistic_ outputs. Dilated LSTMs  and Dilated RNNs  with minimum dilation amounts \(>\)\(1\) do facilitate error reduction and parallel training, but at the cost of sacrificing coherency, as the model is then "equivalent to multiple shared-weight networks, each working on partial inputs"[10, SS4.4]. With such DilatedRNNs, if the time series were to spike to a high level at the very final conditioning input, only a subset of the outputs would be generated conditional on this spike, resulting in highly incoherent output. This can be mitigated for point predictions by adding a final _fusion layer_, but such an approach is not compatible with probabilistic forecasts.

Another related approach is the Subscale WaveRNN model for audio generation  (conditioned on input text). For Subscale WaveRNN, the motivation is not to reduce signal path nor improve error accumulation, but to have a \(K\)-fold increase in inference parallelism when generating very, very long audio sequences. Generation proceeds somewhat similar to our regular, non-alternating version (SS3) but conditioning on past and some _future_ values, and with a shared neural network for all sub-series.

## 3 SutraNets: Sub-series autoregressive networks

Let \(y_{t}\) be the value of a time series at time \(t\), and \(_{t}\) be a vector of time-varying features or _covariates_. Let \(y_{i:j}\) denote sequence \(y_{i} y_{j}\). We seek a model of the distribution of \(N\) future values of \(y_{t}\) (the _prediction range_) given \(T\) historical values (the _conditioning range_). We can formulate this conditional distribution as an autoregressive (AR) generative model as in :

\[p(y_{T+1:T+N}|y_{1:T},_{1:T+N})=_{t=T+1}^{T+N}p(y_{t}|y_{1:t-1},_{1:T+N})\] (1)

Following DeepAR , forecasting approaches have modeled the one-step-ahead distributions using RNNs [21; 60; 67; 7] or Transformers . In these approaches, a global sequence model is trained by slicing many training series into many _windows_, i.e., conditioning+prediction ranges at different start points, and normalizing the windows using their conditioning ranges. Model parameters are fit via gradient descent, minimizing NLL of prediction-range outputs. At inference time, forecasts for a given conditioning range are generated by sampling \(_{T+1}_{T+N}\) sequentially, autoregressively conditioning on samples generated at previous timesteps. By repeating this procedure many times, a Monte Carlo estimate of Eq. (1) is obtained, from which desired forecast quantiles can be derived.

SutraNets consider each univariate window to be comprised of an ordered collection of \(K\) lower-frequency sub-series, each \(}{{k}}\) of the original length (Fig. 1). The \(k\)th sub-series is obtained by selecting every \(K\)th value in the original window, each sub-series starting at a unique offset in \(1 K\).2 Let \(y^{k}\) be the \(k\)th sub-series. Let \(=}{{k}}\) and \(=}{{K}}\) be prediction and conditioning lengths corresponding to each sub-series, and let \(<\!k\) and \(>\!k\) denote indices from \(1 K\) that are less than and greater than \(k\). Through another application of the chain rule, now across sub-series, Eq. (1) becomes:

\[p(y_{T+1:T+N}|y_{1:T},_{1:T+N})=_{=+1}^{ {T}+}_{k=1}^{K}p(y_{}^{k}|y_{1:}^{<k},y_{1: -1}^{<k},y_{1:-1}^{>k},_{1:T+N})\] (2)

where \(\) iterates over timesteps in each sub-series. Log-likelihoods of sub-series prediction ranges can be summed to compute overall NLL, which is minimized during SutraNet training. Rather than modeling these conditionals using a single RNN, SutraNets use a separate RNN for each sub-series.

   RNN method & Meaning of K &  Signal \\ path \\  &  Generative \\ max stride \\  &  Sequential \\ operations \\  &  Coherent \\ outputs \\  \\  Standard RNN, e.g., LSTM  & N/A & \((N)\) & \((1)\) & \((N)\) & Yes \\ Skips , lags/residuals  & Skip/lag amount & \((N/K)\) & \((1)\) & \((N)\) & Yes \\ PatchTST , SegRNN  & Patch size/stride & \((N/K)\) & N/A & \((N/K)\) & N/A \\ DilatedRNN , dilations \(C K\) &  Largest dilation \\ Num. sub-series \\  & 
 \((N/K)\) \\  & \((C)\) & \((N/C)\) & If \(C\)=1 \\   

Table 1: Comparison of SutraNets to prior RNNs. Like prior methods, SutraNets reduce signal path, but also improve error accumulation (via a larger generative stride), while enabling greater training parallelism (via fewer sequential operations), without sacrificing coherent _probabilistic_ output.

Each RNN has its own parameters and hidden state, but SutraNets are trained and tuned collectively as a single network. The \(k\)th RNN determines the \(k\)th conditional probability at each timestep:

\[p(y_{T+1:T+N}|y_{1:T},_{1:T+N})=_{=T+}^{+}_{k=1}^{K}p(y_{}^{k}|_{t}^{k}=f(h_{}^{k}))\] (3)

where \(h_{}^{k}=^{k}(h_{-1}^{k},y_{}^{<k},y_{ -1}^{k},y_{-1}^{>k},_{}^{k})\) and \(f()\) is a function mapping the \(k\)th RNN state to parameters of a parametric output distribution, \(p(y_{}^{k}|_{t}^{k})\).3 So while prior models condition \(y_{t}\) only on \(y_{t-1}\), SutraNets autoregressively condition the generation of \(y_{}^{k}\) on:

1. \(y_{}^{<k}\): _Current_ values (at \(\)) of sub-series with indices \(<\!k\) (always)
2. \(y_{-1}^{k}\): The _previous_ value (at \(-1\)) of the current sub-series, \(k\) (always)
3. \(y_{-1}^{>k}\): _Previous_ values (at \(-1\)) of sub-series with indices \(>\!k\) (optional)

Conditioning \(y_{}^{k}\) on \(y_{}^{<k}\) makes the model autoregressive across sub-series; this ensures sub-series are generated conditional on each other, and thus (recombined) samples of the original high-frequency series will be coherent. Conditioning on \(y_{-1}^{>k}\) is optional; when using \(y_{-1}^{>k}\), SutraNets must generate in an _alternating_ manner, generating one value for each sub-series at each timestep, \(\) (Fig. 1(c)). Without \(y_{-1}^{>k}\), SutraNets can generate the complete prediction range of the \(k\)th sub-series before generating any predictions for \(>\!k\) ones (Fig. 1(d)); in other words, we can exchange ordering of the products in Eq. (2), remove \(y_{-1}^{>k}\) from the equation, and proceed in a _non-alternating_ manner.

Figure 2: Ordering of generative steps in SutraNets. One output value (bold border) is generated in each row, conditional on (1) feature nodes (dashed border) _in that row_, and on (2) state from previous steps (\(\) arrow connections). SutraNet RNNs (c-f) use features generated in earlier steps by both themselves and other RNNs; these values can even be from future timesteps (backfill cases).

SutraNets must also specify how the sub-series index, \(k\), relates to the offset in the original series. Figs. 1(c) and 1(d) illustrate _regular_ ordering, where sub-series \(k\) begins at offset \(k\) in the original series. An alternative is _backfill_ ordering, where sub-series \(k\) starts at offset \(K\)-\(k\)+\(1\) (Figs. 1(e), 1(f)). We must also specify the number of sub-series, \(K\). We pick \(K\) so it divides into the primary seasonal period (e.g., \(K\)=\(6\) for 24-hour seasons), as each sub-series then exhibits seasonality; signal processing techniques can reveal seasonality when it is not known _a priori_. Sub-series with seasonality may be more predictable, which will be especially useful for non-alternating models, as these models generate initial sub-series while conditioning on only \(}{{k}}\) of historical values. Success for these models depends on whether there is _redundancy_ in the series, or whether values from other sub-series are sufficiently important that non-alternating generation creates a _missing information_ problem.

Reducing training/inference discrepancy, signal pathNon-alternating and backfill approaches force the network to predict \(K\) steps ahead, without access to true previous values. Non-alternating also generates far horizons in fewer steps (e.g., for the \(k\)=\(1\) sub-series), reducing error accumulation. In contrast, Regular-alt generates in the same order, and conditional on the same values, as standard RNN models (Fig. 1(a)), and so should suffer the same effects from training/inference discrepancy.

All SutraNets reduce RNN signal path by a factor of \(K\), as can be seen by tracing paths between features and outputs in Fig. 2. Note Regular-alt has low signal path but standard discrepancy (as noted above). The relative effectiveness of Regular-alt can thereby provide a diagnostic for whether discrepancy or signal path affects a given forecasting task (SS4).

Form of output distribution and input encodingUnlike sampling text from a softmax, forecasting requires a parametric output distribution, \(p(y_{}^{k}|_{t}^{k})\) (Eq. (3)), that can account for mixed discrete and continuous outputs, potentially of unbounded dynamic range. C2FAR  proposes such an output distribution, via an autoregressive generative model over a hierarchical coarse-to-fine discretization of time series amplitudes (with Pareto-distributed tails). We adopt C2FAR-LSTMs for our sub-series sequence models. We use a 3-level C2F model with 12 bins at each level, discretizing normalized values to a precision of \(12^{3}\) but requiring only \(12 3\) output dimensions at each timestep. Previous values from each sub-series and covariate features from other sub-series are also C2F-encoded and provided as inputs at each step. Here the efficiency of C2FAR proves helpful; at each step we can encode \(K\) covariate values (i.e., from \(K\) other sub-series) using only \(12 3 K\) input dimensions, as opposed to, e.g., the \(12^{3} K\) dimensions that would be required with equivalent flat binnings .

Low2HighFreqInstead of generating all lower-frequency sub-series, we experiment with generating a single low-freq sub-series and conditioning on it within a high-freq model. This Low2HighFreq approach (Fig. 1(b)) is similar to multi-rate approaches (SS2), except here low-freq samples provide not only features, but hard constraints on every corresponding \(K\)th value in the high-freq output. During inference, when we use Monte Carlo sampling to estimate \(p(y_{T+1:T+N}|)\), we can actually _sample_ high-freq values _conditional_ on low-freq ones. As such, techniques such as rejection sampling  are applicable; we experimented with importance sampling via likelihood weighting (fixing the low-freq values and sampling the rest), but ultimately found the likelihood weights had such high variance that it was more effective to use uniform weights and accept inconsistent estimation.

ComplexityLet \(H\) be the number of RNN hidden units and \(K\) the number of sub-series. The complexity-per-timestep of each SutraNet RNN scales with the sum of the RNN's recurrence, \(H H\), and projection of (at most) \(K\) inputs (from other sub-series) to the RNN hidden state, \(K H\). Even though SutraNets have \(K\) separate RNNs, only one runs at each original timestep (i.e., each SutraNet RNN runs \(}{{K}}\) of total timesteps). In consequence, SutraNet complexity-per-timestep scales with \(H H+K H\), whereas standard RNNs scale \(H H\). In practice, \(H>>K\), and we find empirically SutraNets also scale \(H H\), resulting in similar training/inference speeds and memory requirements compared to standard models, given the same individual LSTM depth and width (SS4).

One key advantage of SutraNets is that training _decomposes_ over sub-series. Sub-series RNNs do not condition on hidden states from other sub-series RNNs, rather only on _generated_ values. During training, when we have access to the true values of other sub-series, all inputs are known in advance and sub-series RNNs can be trained in _parallel_. The only sequential action is evolution of each RNN's hidden state -- over \(}{{K}}\) fewer steps than standard or multidimensional RNNs . In this way, SutraNet-RNNs provide middle-ground between RNNs and standard Transformers (with no sequential computation in training). Inference in SutraNets, like all AR models, is sequential .

Application to TransformersRather than using \(K\) RNNs to parameterize the conditional probabilities as in Eq. (3), we could instead use \(K\) autoregressive Transformers. Compared to a standard Transformer, backfill and non-alternating sub-series Transformers would have the advantage of reducing _error accumulation_ (by increasing the generative stride), but not of _signal path_ (already \((1)\) for Transformers ). Moreover, at each timestep, a sub-series Transformer could attend to essentially all prior values, limited only by the generative order. In a naive implementation, SutraNet Transformers would therefore have an asymptotic complexity of \((N^{2})\) -- the same as standard Transformers. However, we can attain \((N^{2}/K)\) complexity by restricting each sub-series Transformer to only attend to values from its own sub-series, plus a few proximal values from other sub-series (similar to strided or banded self-attention [12; 8]). Although asymptotically larger than the \((N( N)^{2})\) of LogSparse attention , it merits empirical investigation to determine which approach achieves the most favorable accuracy vs. complexity trade-off.

Limitations and broader impactSutraNets are more complex than standard sequence models in a few ways. First, they require more decisions, such as specifying the number of sub-series. While superior accuracy can be obtained when making these choices heuristically, we evaluate the extent to which tuning can achieve further gains (SS4). Secondly, having \(K\) separate RNNs results in roughly \(K\) times as many total parameters. Although, as noted above, this does not translate into requiring more memory or training/inference time, SutraNet models are larger to store and transmit.

As a flexible predictor of both long and short horizons (and without needing _a priori_ specification of confidence quantiles), SutraNets could also serve as the basis for a general-purpose large forecasting model (LFM), along the lines of LLMs like GPT3 . Like modern LLMs, LFMs could potentially be used with little task-specific data and no fine-tuning, and may have similarly-large environmental and financial costs . As with LLMs, new techniques will be needed to detect [64; 71; 54], document [51; 22], and mitigate [25; 58] unsafe usage and other failure cases.

## 4 Experiments

**Training.** We implemented SutraNets in PyTorch . We use Adam  (default \(_{1}\)=\(0.9\), \(_{2}\)=\(0.999\), \(\)=\(10^{-8}\)), early stopping, and a decaying learning rate (\(\)=\(0.99\)). The default RNN is a 1-layer LSTM with 64 hidden units; when using >1 layers (Table 3), inter-layer dropout  with \(p_{drop}\)=\(0.001\) is applied. For every model/dataset pair, we tune weight decay and initial learning rate over a \(4{}4\) grid.

**Systems.** We evaluate the following systems. Naive and SeasNaive are non-probabilistic, untrained baselines, while the C2FAR variants follow the standard RNN ordering (Fig. 2a):

* At each horizon, repeats last-observed historical value .
* At each horizon, repeat previous value at same _season_ as horizon , e.g., same hour-of-day (SeasNaive1d) or hour-of-week (SeasNaive1w).
* LSTM-based forecasting model , configured as described above (SS3).
* C2FAR+lags: C2FAR, but with extra features for previous hour-of-day values (C2F-encoded).
* C2FAR+dropout: C2FAR, but with \(p_{drop}\)=\(0.2\) (and re-scaling ) applied to inputs during training. Low2HighFreq: High-frequency forecast generated conditional on low-frequency values (SS3).
* Variations Regular-alt, Regular-non, Backfill-alt, Backfill-non (SS3).

**Datasets.** We evaluate on the following datasets, where we note the number of SutraNet sub-series, \(K\), as well as lengths of conditioning, \(C\), and prediction, \(P\), ranges:

* \(K\)=6, \(C\)=2016, \(P\)=288: 5-minutely usage of 2085 VM flavors, groupings (by tenant, etc.) in Azure cloud. Originally from , aggregated here instead to 5-minute intervals.
* \(K\)=6, \(C\)=168, \(P\)=168: Hourly electricity usage of 321 customers , version from .
* \(K\)=6, \(C\)=168, \(P\)=168: Hourly occupancy for 862 car lanes , version from .
* \(K\)=7, \(C\)=91, \(P\)=91: Daily hit-count for 9535 pages, first used in , version from .
* \(K\)=4, \(C\)=392, \(P\)=392: row-wise sequential pixel values for 70,000 28\(\)28 digits .
* \(K\)=4, \(C\)=392, \(P\)=392: _mnist_ sequences, but with a fixed, random permutation applied.

Sequential (binarized) MNIST has been used for NLL  and classification  evaluations. Le et al.  used a fixed permutation "to make the task even harder." Here, we forecast a distribution over _real-valued_ pixel intensities in the second half of an image, based on values in the first. Unlike the other datasets, train/dev/test splits are not based on time, rather we use the standard test digits .

**Metrics.** We evaluate the median of our forecast distribution by computing _normalized deviation_ (ND) from true values. We evaluate the full distribution using _weighted quantile loss_ (wQL), evaluated at forecast quantiles \(\{0.1,0.2 0.9\}\), as in . For non-probabilistic Naive and SeasNaive baselines, note wQL reduces to ND, similarly to how CRPS reduces to absolute error [24, SS4.2].

**Results.** We summarize the most important experimental findings as follows:

_[leftmargin=*]_

_Finding 1: Backfill-alt and Backfill-non improve over C2FAR and other baselines on every dataset, with both obtaining an average relative reduction in ND of around 15% over the best non-SutraNet._

Table 2 has the main results; note the differences in ND% are very stable: we repeated _grid search tuning and testing_ multiple times with different random seeds and found _SutraNets superior to standard models across all repeats_ (supplemental SSB.2). Backfill-non does best when the training/inference discrepancy problem predominates (_azure & mnist_, Table 2). Adverse effects of discrepancy seem to be magnified by high data _redundancy_; in these cases, non-alternating generation reduces discrepancy while having sufficient info to forecast accurately. We also hypothesize that Regular-non performs poorer because it generates less-_coherent_ output: by construction, it only conditions on past values, even when subsequent values have already been generated (e.g., row 7 in Fig. 2d). In contrast, when generating a given point (e.g., row 7 in Fig. 2e), Backfill-non has always seen the previous and future proximal values, which were input at the previous and current timesteps, respectively. When signal path problems predominate (_traff & mnist_"), alternating generation, with access to more info, performs better. On _mnist_", differences between regular and backfill depend solely on the random permutation; the superiority of Regular-alt here motivates tuning over multiple orderings of sub-series, perhaps averaging over them, as is done in some prior non-sequential models .

Accuracy gains seem to derive from solving both discrepancy _and_ signal path problems together. When discrepancy predominates, Regular-alt performs poorly, even though signal path is improved. For the opposite case, where error accumulation alone is improved, we implemented an RNN that processes values in backfill order, but otherwise acts as a standard RNN. This model performs much poorer than SutraNet-based Backfill-alt (supplemental SSB.4). The lack of a holistic solution may also explain the inconsistency of lags and dropout, which sometimes help, sometimes hurt compared to vanilla C2FAR. Low2HighFreq works best on datasets where discrepancy predominates, while performing much poorer when signal path is important. Note that weekly-seasonal baselines have not previously been evaluated on these datasets. Observing in particular the relatively strong accuracy of SeasNaive1w on _traff_, we suggest they be included in future evaluations.

    & _azure_ & _elec_ & _traff_ & _wiki_ & _mnist_ & _mnist_\({}^{}\) \\  Naive & 3.2, 3.2 & 43.0, 43.0 & 75.5, 75.5 & 44.0, 44.0 & 100.0, 100.0 & 117.9, 117.9 \\ SeasNaive1d & 2.8, 2.8 & 10.5, 10.5 & 31.1, 31.1 & 44.0, 44.0 & 145.2, 145.2 & 163.5, 163.5 \\ SeasNaive1w & 5.2, 5.2 & 11.1, 11.1 & 17.5, 17.5 & 41.8, 41.8 & N/A & N/A \\ C2FAR & 3.2, 2.5 & 100.6, 8.4 & 19.3, 16.0 & 31.1, 27.2 & 67.9, 52.3 & 100.0, 95.7 \\ C2FAR+lags & 2.7, 2.2 & 10.4, 8.3 & 18.0, 15.0 & 31.2, 27.2 & 68.1, 52.8 & 99.3, 89.9 \\ C2FAR+dropout & 3.2, 2.6 & 10.9, 8.7 & 18.5, 15.3 & 31.3, 27.0 & 68.6, 53.3 & 99.7, 90.4 \\ Low2HighFreq & 2.6, 2.1 & 10.3, 8.2 & 20.2, 16.6 & 30.9, 27.2 & 62.4, 47.6 & 98.1, 88.3 \\ Regular-alt & 3.3, 2.6 & **9.9, 7.9** & 15.5, 13.0 & 80.4, 26.4 & 67.9, 52.4 & **48.7, 38.3** \\ Regular-non & 2.6, 2.1 & 9.7, 7.8 & 15.6, 13.1 & 31.2, 27.3 & 59.4, 45.4 & 90.1, 69.7 \\ Backfill-alt & 2.5, 2.0 & **9.3, 7.4** & **15.3, 12.8** & **30.1, 26.1** & 64.4, 49.7 & 72.0, 54.6 \\ Backfill-non & **2.3, 1.9** & **9.3, 7.4** & 15.7, 13.1 & 80.5, 26.8 & **58.9, 44.9** & 78.2, 58.3 \\   

Table 2: ND%, wQL% across datasets, best results in **bold**. For _mnist/mnist_\({}^{}\), previous _row_ value used for SeasNaive1d. Poor results for C2FAR relative to Regular-alt indicate _signal path problem_ (green cells). Poor results for both C2FAR and Regular-alt indicate _discrepancy problem_ (pink). Non-alternating weakness indicates _missing info problem_ (yellow). Backfill-alt reduces signal path and discrepancy without missing info, therefore offering best or near-best solution on each dataset.

**Finding 2**: SurtaNets are able to effectively model both long and short-term dependencies._

On _elec_ data, where SeasNaive1d is more effective than longer-term SeasNaive1w (Table 2), modeling long-term dependencies remains important (Fig. 3). Backfill-alt is able to capture both short-term changes and long-term seasonality, the latter of which is neglected by C2FAR. In plots of the _mnist_ forecasts (Fig. 4), we see that compared to C2FAR, Backfill-non generates more non-zero values in the bottom parts of digits 0, 2, 5, etc., e.g., at p50%, implying the networks have more certainty or _confidence_ (i.e., tighter prediction intervals) on the digit being generated. Also, Table 2 results indicate that discrepancy is the predominant problem on _mnist_; this may help explain why autoregressive models like PixelRNN  score very high in likelihood-based evaluation (where there is no sampling), but produce less-realistic samples compared to GANs. SutraNets may therefore provide a useful tool for reducing training/inference discrepancy in a variety of image generators.

**Finding 3**: SutraNets are more accurate than C2FAR across all forecast horizons._**

Autoregressive models are normally thought to accumulate and compound  errors "along the generated sequence" . We therefore expect the accuracy gap between SutraNets and C2FAR to grow across horizons. Instead, on _elec_ and _traff_, we already see a large gap over the first 24 hours (supplemental SSB.3). Improvements in both informational asymmetry (SS1) and signal path evidently enable gains across all steps. Low2HighFreq, in contrast, is only accurate at horizons corresponding to its low-freq forecast, while Backfill-alt is slightly better at horizons corresponding to its first sub-series. This suggests errors do "accumulate" to a small extent across sub-series autoregression.

**Finding 4**: Deeper models enable improved long-sequence forecasting, for SutraNets and C2FAR._**

In particular, going from 1 to 2 layers greatly improves accuracy on _traff_ (Table 3), a dataset where signal path is the predominant problem. Smaller improvements are also seen on _elec_. Since both LSTM stacking and use of SutraNets increase complexity, a natural question is which approach offers the best computational tradeoff for a desired level of accuracy. Fig. 5 shows that for a given number of parameters -- and especially for a given inference time -- SutraNets are able to obtain better

Figure 4: Percentiles of forecast distributions for first occurrence of 0...9 in _mnist_ test data. Pixels before yellow line (in row-order) are history. Backfill-non more confident in bottom part of digits.

Figure 3: Forecasts and 80% intervals for _elec_ series. Backfill-alt performs well with a changepoint (row 1, where SeasNaive1w fails) and with weekly seasonality (row 2, where C2FAR fails).

ND% (plots of memory usage and of training time are similar, see supplemental). SutraNet speed and memory usage scale with the complexity of each RNN rather than the number of sub-series RNNs (SS3); they can consequently make use of \(K\) more parameters without \(K\) the cost.

We also experimented with varying the number of sub-series. For all \(K\)>1, SutraNets work much better than standard C2FAR (Table 4). Interestingly, \(K\)=\(12\) led to the highest accuracy on _traff_, where the signal path problem predominates. When \(K\) does not divide evenly into the seasonal period, non-alternating models have a missing info problem (e.g., they miss predictive preceding values at the same hour-of-day). Both Backfill-alt and Regular-alt can access these values through covariate features (cf. Fig. 2f), and hence perform relatively better when \(K\)=\(7\) on _elec_. Backfill-alt is relatively weaker at \(K\)=\(7\) when there is less redundancy (_traff_), although still superior to standard C2FAR.

See the supplemental for further experimental details and results. Code for SutraNets is available at https://github.com/huaweicloud/c2far_forecasting/wiki/SutraNets.

## 5 Conclusion

Probabilistic forecasting of long time series is an under-studied problem, with most recent work focusing on point prediction. We presented SutraNets, a novel autoregressive approach to probabilistic forecasting of long-sequence time series. SutraNets convert a univariate series into a \(K\)-dimensional multivariate series, each dimension comprising a distinct every-\(K\)th-value sub-series of the original sequence. The SutraNet autoregressive model generates each sub-series conditional on both its own prior values, and on other sub-series, ensuring coherent output samples. From these samples, an estimate of the full joint probability of future values is made. SutraNets leverage C2FAR under-the-hood, allowing for efficient representation of time series amplitudes, and low-overhead input encoding of covariate sub-series. SutraNets provide a holistic solution to challenges in long-sequence sampling, reducing training/inference discrepancy (by generating conditional on longer-term information) and signal path distances (by a factor of \(K\)). Experimentally, SutraNets achieve state-of-the-art results on a variety of long-sequence forecasting data sets, while demonstrating similar speed and memory requirements as standard sequence models, even when using \(K\) more parameters.

   Dataset &  &  \\ \#bayers & 1 & 2 & 3 & 4 & 1 & 2 & 3 & 4 \\ \#hidden & 64 & 128 & 256 & 256 & 64 & 128 & 256 & 256 \\  C2FAR & 10.6 & 10.2 & 9.9 & 9.7 & 19.3 & 14.7 & 14.8 & 14.4 \\ Regular-alt & 9.9 & 9.3 & **9.0** & 9.2 & 15.5 & 14.3 & 14.3 & 14.3 \\ Backfill-alt & 9.3 & 9.3 & **9.0** & 9.3 & 15.3 & 14.2 & 14.1 & **14.0** \\ Backfill-non & 9.3 & 9.1 & 9.5 & 9.3 & 15.7 & 14.4 & 14.4 & 14.4 \\   

Table 3: ND% decreases for SutraNets & C2FAR as number of RNN layers/hidden units increases.

   Dataset &  &  \\ \#subseries & 6 & 7 & 12 & 6 & 7 & 12 \\  Regular-alt & 9.9 & 10.0 & 10.0 & 15.5 & 17.2 & 15.0 \\ Backfill-alt & **9.3** & 9.4 & 9.4 & 15.3 & 16.9 & **14.9** \\ Backfill-non & **9.3** & 10.0 & 9.5 & 15.7 & 17.3 & 15.4 \\   

Table 4: ND% lowest when #sub-series divides into seasonal period (24), but alternating models less sensitive on _elec_ (cf. C2FAR with 10.6% on _elec_, 19.3% on _traff_.)

Figure 5: Total parameters (x-axis) and average inference time per forecast (y-axis, lower better) for C2FAR and Backfill-alt systems of Table 3, for _elec_ (left) and _traff_ (right). ND% labeled at each point in boxes. Backfill-alt enables more parameters and better ND% at equivalent speeds.