# DiViNeT: 3D Reconstruction from Disparate Views via Neural Template Regularization

Aditya Vora\({}^{1}\)   Akshay Gadi Patil\({}^{1}\)   Hao Zhang\({}^{1,\,2}\)

\({}^{1}\)Simon Fraser University  \({}^{2}\)Amazon

###### Abstract

We present a volume rendering-based neural surface reconstruction method that takes as few as three _disparate_ RGB images as input. Our key idea is to regularize the reconstruction, which is severely ill-posed and leaving significant gaps between the sparse views, by learning a set of _neural templates_ to act as surface priors. Our method, coined DiViNet, operates in two stages. It first learns the templates, in the form of 3D Gaussian functions, across different scenes, without 3D supervision. In the reconstruction stage, our predicted templates serve as anchors to help "stitch" the surfaces over sparse regions. We demonstrate that our approach is not only able to complete the surface geometry but also reconstructs surface details to a reasonable extent from a few disparate input views. On the DTU and BlendedMVS datasets, our approach achieves the best reconstruction quality among existing methods in the presence of such sparse views and performs on par, if not better, with competing methods when dense views are employed as inputs.

## 1 Introduction

3D reconstruction from multi-view images is a fundamental task in computer vision. Recently, with the rapid advances in neural fields  and differentiable rendering , many methods have been developed for neural 3D reconstruction . Among them, volume rendering-based methods have achieved impressive results. Exemplified by neural radiance fields (NeRF) , these approaches typically employ compact MLPs to encode scene geometry and appearance, with network training subjected to volume rendering losses in RGB space.

The main drawback of most of these methods is the requirement of dense views with considerable image overlap, which may be impractical in real-world settings. When the input views are sparse, these methods often fail to reconstruct accurate geometries due to radiance ambiguity . Despite employing smoothness priors during SDF/occupancy prediction as an inductive bias , the limited overlap between regions in the input images causes little-to-no correlation to the actual 3D surface, resulting in holes and distortions, among other artifacts, in the reconstruction.

Several recent attempts have been made on neural reconstruction from sparse views. SparseNeuS  learns generalizable priors across scenes by constructing geometry encoding volumes at two resolutions in a coarse-to-fine and cascading manner. While the method can handle as few as 2 or 3 input images, the corresponding views must be sufficiently close to ensure significant image overlap and quality reconstruction. MonoSDF  relaxes on the image overlap requirement and relies on geometric monocular priors such as depth and normal cues to boost sparse-view reconstruction. However, such cues are not always easy to obtain, e.g., MonoSDF relied on a pre-trained depth estimation model requiring 3D ground-truth (GT) data. Moreover, depth prediction networks often rely on cost volumes  which are memory expensive and thus restricted to predicting low-resolution depth maps. Because of this, the results obtained typically lack fine geometric details .

In this paper, we target neural surface reconstruction from sparse _and disparate_ views. That is, not only are the input RGB images few in number, they also do no share significant overlap; see Figure 1. Our key idea is to regularize the reconstruction, which is severely ill-posed and leaves significant gaps to be filled in between the few disparate views, by learning a set of _neural templates_ that act as surface priors. Specifically, in the first stage, we learn the neural templates, in the form of 3D Gaussian functions, across different scenes. Our network is a feedforward CNN encoder-decoder, trained with RGB reconstruction and auxiliary losses against the input image collection. In the second stage, surface reconstruction via volume rendering, our predicted templates serve as anchors to help "stitch" the surfaces over sparse regions without negatively impacting the ability of the signed distance function (SDF) prediction network from recovering accurate geometries.

Our two-stage learning framework (see Figure 2) is coined DiViNet for neural 3D reconstruction from Disparate Views via NEural Templates regularization. We conduct extensive experiments on two real-world object-scenes datasets, viz., DTU  and BlendedMVS , to show the efficiency of our method over existing approaches on the surface reconstruction task, for both sparse and dense view input settings. Through ablation studies, we validate the design choices of our network in the context of different optimization constraints employed during training.

## 2 Related Work

**View-based 3D Reconstruction.** Reconstructing 3D surfaces from image inputs is a non-trivial task mainly due to the difficulty of establishing reliable correspondences between different regions in the input images and estimating their correlation to the 3D space. This is especially pronounced when the number of views is _limited_ and _disparate_. Multi-view stereo (MVS) techniques, both classical , and learning-based , while using an explicit 3D representation, address the problem of multi-view 3D reconstruction either by estimating depth via feature matching across different input views or by voxelizing 3D space for shape reconstruction.

A recent exploration of neural implicit functions for 3D shape/scene representation  has led to an explosion of 3D reconstruction from multi-view images, where two prominent directions exist - one based on surface rendering techniques , which require accurate 2D masks at the input, and the other based on volume rendering techniques ,

Figure 1: Surface reconstruction results on the DTU dataset  for three disparate view inputs (shown on the left). We compare our technique against NeuS, which does not use any regularization, and against MonoSDF , which uses additional depth and normal supervision to regularize the surface reconstruction process. The results clearly show that our method not only reconstructs the complete surface but also preserves sharp geometric details _without_ using any other ground truth information beyond the RGB inputs. See Section 4.1 for a detailed explanation.

which require no such object masks. The latter techniques combine the representational power of neural implicit with the recently proposed NeRF models , achieving better surface reconstructions over the former techniques. A major drawback of these works is that they all require _dense_ multi-view images as input. Two recent techniques, Sparse NeuS  and Mono-SDF  learn 3D reconstruction from just three input images. In the case of , the three input images have a significant amount of overlap, and can not be employed when the views are disparate. , on the other hand, leverages additional cues such as ground truth depths and normals, allowing reconstruction from disparate views. Our work differs from these in the sense that we do not make use of any explicit cues in the form of ground truth depths and normals, while still being able to faithfully reconstruct 3D surfaces from just three disparate RGB images as input.

**Regularizing Neural Surface Reconstruction.** Our ability to reconstruct 3D surfaces from sparse, disparate view images is made possible through template-based regularization that provides helpful surface priors during the volume rendering stage. Regularization has been explored in the context of novel-view synthesis , where patch-based regularization for both the scene geometry and color is performed  or additional cues such as depth supervision are incorporated  while handling sparse inputs in both cases. In the context of surface reconstruction, MonoSDF  uses depth and normal supervision as a part of unsaid regularization to obtain surface geometry from sparse input images. Other works  study the effect of regularization on the volumetric rendering-based surface reconstruction framework. Their goal is to improve surface reconstruction by imposing multi-view photometric and COLMAP  constraints on the Signed Distance Field (SDF) prediction during training. Such methods show significant improvements over vanilla approaches for surface reconstruction. However, under sparse scenarios, because of significant view differences, photometric and COLMAP constraints are difficult to satisfy, resulting in poor reconstruction quality. In contrast to these methods, inspired by , we propose to learn surface priors in the form of templates across a data distribution and use these templates to guide the reconstruction process.

## 3 Method

Our method achieves 3D reconstruction of solid objects from a small set of RGB images \(\{I_{i}\}_{i=0}^{N-1}\) with very less visual overlap, where \(N\) can be as less as 3 images and \(I_{i}^{H W 3}\). We assume that camera extrinsics and intrinsics are known for all the images. As shown in Figure 2 our approach is realized in two stages: (1) Learning a network for predicting shape templates (see Section 3.1), and (2) Leveraging the predicted templates, learning a volumetric surface reconstruction network with depth and SDF constraints (see Section 3.2).

Figure 2: A two-stage learning framework developed to reconstruct 3D surfaces from sparse, disparate view input images. In Stage 1 (see Section 3.1), we train a network across different scenes to predict structured templates (Gaussians, in our case) that encode surface priors. In Stage 2 (see Section 3.2), we train a surface reconstruction model to reconstruct the surface SDF values by leveraging the predicted templates from Stage 1. The purpose served by the predicted templates is in aiding the reconstruction process by acting as regularizers, allowing to obtain complete geometry (see Figure 1 and 7), and even details to a reasonable extent (see Figure 5), from disparate view inputs.

### Stage 1: Learning Surface Priors with Neural Templates

**Template Representation.** We represent the shape with a bunch of \(N_{t}\) local implicit functions called templates, where the influence of \(i^{th}\) template is represented as \(g_{i}(p,_{i})\) and is given by,

\[g_{i}(p,_{i})=s_{i}(_{d\{x,y,z\}}-p_{d})^ {2}}{2r_{i,d}^{2}}),\] (1)

where \(p^{3}\) is the 3D location of a point in the volume and \(_{i}\) is the template parameter. Specifically, each template \(_{i}\) is represented as a scaled, anisotropic 3D Gaussian, \(_{i}^{7}\), which consists of a scaling factor \(s_{i}\), a center point \(c_{i}^{3}\), and per-axis radii \(r_{i}^{3}\). Using this local implicit function, we can find the probability of a point near the surface by summing up the influences of all \(N_{t}\) templates, i.e. \(G(p,)=_{i=1}^{N_{t}}g_{i}(p,_{i})\), where \(\) is the set of all the template parameters.

**Network Architecture.** Figure 3 shows the template training stage. Given a fixed set of \(N_{t}\) templates (\(\)) and input images set \(\{I_{i}\}\), we aim to optimize a network \(_{}\) for the task of template prediction which comprises of an encoder (\(\) ) and decoders (\(_{geo},_{vox}\)). The encoder is a feed-forward Convolutional Neural Network (CNN), which we first use to extract 2D image features from individual input views. We then obtain per template latent code \(z_{_{i}}^{64}\) using a bi-linear interpolation step as shown in Figure 4. For this step, depending on the number of templates, we create a uniform grid of points, which we then use for the interpolation of features obtained from convolutional layers. Each latent code of the template, \(z_{_{i}}\) is then decoded through two decoders \(_{geo}\) and \(_{vox}\). \(_{geo}\) is the geometry decoder with simple MLP layers predicting the template parameters \(_{i}\). That is, \(_{geo}:^{64}^{7}\). \(_{vox}\), on the other hand, is a feature decoder comprised of transposed convolutional layers. The objective of \(_{vox}\) is to map the template feature representation, \(z_{_{i}}\), to a local volumetric feature grid for each individual template.

Decoding all the template latent codes \(z_{_{i}}\) at once through \(_{vox}\) results in a dense voxel grid \(V^{()(})(})}\), which we then rearrange to \(N_{t}\) local volumes of size \(^{^{3}}\), where \(\) is the feature dimension of each voxel and \(\) is the resolution of the feature grid. In our experiments, we set \(N_{t}=576\), \(C=8\) and \(=16\). For more details refer to supplementary.

**Network Training.** Due to the lack of ground truth 3D information of objects in the training set, we use RGB reconstruction loss along with auxiliary losses to optimize the template parameters. Given the predicted feature volume from the template network, we can query a continuous field at any 3D query point \(p_{i}\) by passing its tri-linearly interpolated feature \(f_{vox}(p_{i})\), along with the location \(p_{i}\) to a small MLP, consisting of \(2\) layers and \(256\) hidden dimensions. The output of the MLP is the color prediction at that particular query point, \(p_{i}\) in the 3D space. In addition to color, we compute each point's blending weights \(w_{i}\) in 3D space using our predicted templates. This blending weight is computed by summing up the local influences of each template on the query point. We then integrate the predicted colors and weights to obtain the predicted image \(\). For \(K\) query points sampled along a ray, this can be written as:

Figure 4: Interpolation step. \(}}\) codes obtained from \(H/16 W/16\) features.

Figure 3: Template Learning stage. We train the template prediction network across scenes to predict \(N_{t}\) structured templates \(\), alongwith local feature volumes per template.

\[(r)=_{i=1}^{K}w_{i}c_{i}, w_{i}=,)}{_{i}G(p_{i},)+}\] (2)

Here, \(p_{i}\) is a query point along a ray, \(\) is a small perturbation to prevent \(w_{i}\) from diverging to infinity when \(G(p_{i},)\) takes small values, whereas \(G(p_{i},)=_{i N_{t}}g_{i}(p_{i},_{i})\). We then optimize our template prediction network using RGB reconstruction loss. To make sure that the predicted templates are plausible and near the surface, we propose auxiliary loss functions. The total loss by combining all the losses is given as:

\[_{total}=_{rgb}+_{cd}_{chamfer}+ _{c}_{cov}+_{r}_{radius}+_{v} _{var}\] (3)

Here, \(_{rgb}\) is the mean square error loss between the predicted pixel \(()\) and ground truth pixels \(I()\) which is defined as:

\[_{rgb}=_{r}\|(r)-I(r)\|_{2}^{2}\] (4)

\(_{chamfer}\) is the chamfer distance loss, which minimizes the distance between the template centers \(\{C_{t}:c_{1},c_{2},,c_{N_{t}}^{3}\}\), with the corresponding sparse reconstructed point cloud \(\{S_{c}:s_{1},s_{2},,s_{N}^{3}\}\) obtained using triangulation, from COLMAP. This loss makes sure that the predicted templates from the network are near the surface. This is given by:

\[_{chamfer}=|}_{c_{i} C_{t}}_{s_{j} S_{ c}}(\|c_{i}-s_{j}\|_{2}^{2})+|}_{s_{j} S_{c}}_{c_{i}  C_{t}}(\|s_{j}-c_{i}\|_{2}^{2})\] (5)

\(_{cov}\) is the coverage loss, which is defined over the entire sparse points, \(S_{c}\). This ensures that the predicted templates cover the entire object. It is defined as,

\[_{cov}=|}_{s S_{c}}(1-(_{i=1}^{N_{ t}}g_{i}(s,_{i})))\] (6)

where \((.)\) is the sigmoid function. In addition to this, we also use radius \(_{radius}\) and variance loss \(_{var}\), which penalizes large and skewed template radii. This is given by:

\[_{radius}=_{r_{i} R_{t}}\|r_{i}\|_{2}^{2}, _{var}=_{r_{i} R_{t}}\|r_{i}-_{t}\|^{2}\] (7)

Here, \(R_{t}\) is the set of all template radii. \(r_{i}\) is the individual template radii, and \(_{t}\) is the mean radius across all the templates, \(N_{t}\).

### Stage 2: Surface Reconstruction using Volume Rendering

**Geometry Representation and Volume Rendering.** Following the recent works of surface reconstruction with volume rendering [48; 54; 36], we represent the volume contained by the object to be reconstructed using a signed distance field \(_{i}\), which is parameterized using an MLP \(f_{_{s}}\), with learnable parameters \(_{s}\) i.e \(_{i}=f_{_{s}}((x_{i}))\). Here \((.)\) is the positional encoding of point in 3D space . Given the signed distance field, the surface \(\) of the object is represented as its zero-level set i.e.

\[=\{x^{3}|f_{_{s}}(x)=0\}.\] (8)

To train the SDF network in volume rendering framework, we follow , and use the below function, to convert the SDF predictions, \(f_{_{s}}(p_{i})\) to \(_{i}\) at each point, \(p_{i}\) in the 3D space.

\[_{i}=((f_{_{s}}(p_{i}))-_{s}(f_{_ {s}}(p_{i+1}))}{_{s}(f_{_{s}}(p_{i}))},0).\] (9)

Here, \(_{s}(x)=(1+e^{-sx})^{-1}\) is the _sigmoid_ function and \(s\) is learned during training.

Along with the signed distance field, \(_{i}\), we also predict color, \(_{i}\) for each sampled point in 3D space using a color MLP, \(f_{_{c}}\). Following  the input to the color MLP is a 3D point \(p_{i}\), a viewingdirection \(\), analytical gradient \(_{i}\) of our SDF, and a feature vector \(z_{i}\) computed for each point \(p_{i}\) by \(f_{_{s}}\). Hence, the color MLP is the following function, \(_{i}=f_{_{c}}(p_{i},,_{i},z_{i})\). We optimize these networks using volume rendering, where the rendered color of each pixel is integrated over \(M\) discretely sampled points \(\{p_{i}=o+t_{i}|i=1,,M,t_{i}<t_{i+1}\}\) along a ray traced from camera center \(o\) and in direction \(\). Following  the accumulated transmittance \(T_{i}\) for a sample point \(p_{i}\) is defined as \(T_{i}=_{j=1}^{i-1}(1-_{j})\), where \(_{j}\) is the opacity value defined in Eq. 9. Following this, we can compute the pixel color and depth as follows:

\[(r)=_{i=1}^{M}T_{i}_{i}_{i}, (r)=_{i=1}^{M}T_{i}_{i}t_{i}\] (10)

**Optimization.** The overall loss function we use for optimizing for the surface is:

\[=_{color}+_{1}_{eikonal}+ _{2}_{depth}+_{3}_{sdf}\] (11)

\(_{color}\), is the L1 loss between the predicted and ground truth colors which is given as:

\[_{color}=_{r}\|(r)-C(r)\|_{1}\] (12)

Following  we use Eikonal loss to regularize the SDF in 3D space, which is given as:

\[_{eikonal}=_{x}(\| f_{_{s}}(x)\|_{2} -1)^{2}\] (13)

In addition to these, we propose the following regularization terms in order to aid the surface reconstruction process, especially in sparse reconstruction scenarios.

**Depth Loss**: We sample depth cues \(z_{t}(r)\) along each ray using the templates predicted by our template network. These depth cues are obtained by finding the depth locations where the template weight function \(w(x)\) at a point \(x\), mentioned in 2 attains a maximum value among all the points sampled along a ray. We then minimize the L1 loss \(_{depth}\) between the predicted depth, \((r)\) and the computed depth cues \(z_{t}(r)\) as:

\[_{depth}=_{r_{valid}}\|(r)-z_{t}(r)\|_{1}\] (14)

Here, \(_{valid}\) denotes the set of rays that intersect the templates in the 3D space. We find these rays using ray sphere intersection.

**SDF Loss**: In addition to depth, we also constraint our SDF to pass through the template centers which we assume are near the surface. This is done by minimizing the L1 loss between the SDF at the template centers and the zero-level set. Here, \(N_{t}\) is the number of template parameters.

\[_{sdf}=}_{x C_{t}}\|f_{_{s}}(x)\|_{1}\] (15)

**Implementation Details.** Our implementation is based on the PyTorch framework . For both stages, we use the Adam optimizer  to train our networks and set a learning rate of \(5e^{-4}\). For stage-1, we set \(_{cd}\), \(_{c}\), \(_{r}\) and \(_{v}\) to \(1.0\), \(0.1\), \(0.1\), and \(1.0\), respectively. And for stage-2, we set \(_{1}\), \(_{2}\) and \(_{3}\) to \(1.0\), \(0.8\) and \(0.8\), respectively. As the templates are not perfect, we follow  and use an exponentially decaying loss weight for both SDF and depth regularization for the first \(25k\) iterations of optimization. During training in both stages, we sample a batch of \(512\) rays in each iteration. During both stages, we assume that the object is within the unit sphere. Our SDF network \(f_{_{s}}\), is an \(8\) layer MLP with \(256\) hidden units with a skip connection in the middle. The weights of the SDF network are initialized by geometric initialization . The color MLP is a \(4\) layer MLP with \(256\) hidden units. The 3D position is encoded with \(6\) frequencies, whereas the viewing direction is encoded with \(4\) frequencies. We train the network for \(300\)k iterations which takes roughly \(8\) hours on NVIDIA RTX \(3090\)Ti GPU. After training, we extract the mesh using marching cubes  at a \(512^{3}\) resolution. See the Supplementary material for more details.

## 4 Results and Evaluation

**Datasets.** We use two commonly employed real-world object-scenes datasets, viz., DTU  and BlendedMVS dataset . DTU dataset contains multi-view images (49-64) of different objects captured with a fixed camera and lighting parameters. For training sparse view reconstruction models, we use the same image IDs as in [33; 58; 56]. In addition to this, in order to show the generalization ability of our template prediction network we experiment with our method on the MobileBrick dataset , which consists of high-quality 3D models along with precise ground-truth annotations.

For the template learning stage, our training split consists of objects that do not overlap with the \(15\) test scans of the DTU dataset. We provide the list of scans used for training in the supplementary. We evaluate our method on standard 15 scans used for evaluation by [34; 55; 54; 48; 36]. The resolution of each image in the dataset is \(1200 1600\). Unlike MonoSDF  which resizes the images to \(384 384\) resolution for training, we input the actual image resolution to our pipeline. The other dataset, BlendedMVS, consists of \(113\) scenes captured from multiple views, where each image is of resolution \(576 768\). Unlike DTU, BlendedMVS dataset has not been employed before in the sparse view setting, and as such, specific view IDs to train and test are not available. We therefore manually select 3 views that have little overlap.

**Baselines.** For sparse views, we compare against the classical MVS method, COLMAP , and recent volume rendering-based methods such as NeuS , VolSDF  and MonoSDF . We exclude SparseNeuS  evaluation in disparate settings because of its inability to reconstruct 3D geometry when the input images have significantly less overlap. For dense views, we additionally compare against the recent grid-based method, Voxurf . A mesh from COLMAP point cloud output is obtained using Screened Poisson Surface Reconstruction .

**Evaluation Metrics.** Following standard evaluation protocol, we use Chamfer Distance (CD) between the ground truth point cloud and predicted meshes to quantify the reconstruction quality. On the DTU dataset, following prior works [54; 48; 58], we report CD scores. On the BlendedMVS dataset, we simply show qualitative results as is commonly done in the literature. In addition to this, for evaluation on the MobileBrick dataset, we use F1 score computed using the ground-truth mesh.

### Results on DTU

Figure 1 and Table 1 show qualitative and quantitative results, respectively, in the presence of sparse view inputs (three, to be specific) on the DTU dataset. We observe from Table 1 that our method outperforms all other competing methods, including MonoSDF , on nine of the fifteen scenes in the test set, and achieves the lowest mean CD score (the next best adds an additional 0.09 CD score).

Figure 5: This figure compares the details on the reconstructed geometry, for sparse-view inputs on the DTU dataset, against the second-best competing method, MonoSDF (see Table 1). Our approach produces sharper details, which can be attributed to the surface guidance from the predicted templates.

[MISSING_PAGE_FAIL:8]

geometry faithfully for any of the objects due to disparate view inputs. MonoSDF, on the other hand, despite using ground truth depth and normal cues, fails to reconstruct accurate geometry, especially for Clock and Sculpture objects. In contrast to this, our method can accurately reconstruct the geometric details owing to template guidance coming from Stage 1.

### Generalization on MobileBrick Dataset

We test the generalization ability of our template prediction network (TPN) on a new dataset, MobileBrick , by comparing the reconstruction results when the neural templates were learned by a pre-trained TPN (on DTU) vs. when they were trained on MobileBrick. Note that overall, the models from these two datasets are quite different in terms of geometry and structure. The results in Table 2 and qualitative results in Figure 7(a) (a) show that the reconstruction qualities under the two scenarios are comparable, attesting to the generalizability of our TPN. As mentioned before, the metric used in this evaluation is F1 score as reported by MobileBrick dataset.

### Ablation Studies

    & & & & &  \\  Scan ID \(\) & 24 & 37 & 40 & 55 & 63 & 65 & 69 & 83 & 97 & 105 & 106 & 110 & 114 & 118 & 122 & Mean \\  COLMAP & 0.81 & 2.05 & 0.73 & 1.22 & 1.79 & 1.58 & 1.02 & 3.05 & 1.4 & 2.05 & 1.0 & 1.32 & 0.49 & 0.78 & 1.17 & 1.36 \\ NeuS & 1.0 & 1.37 & 0.93 & 0.43 & 1.1 & **0.65** & **0.57** & 1.48 & **1.09** & 0.83 & **0.52** & 1.2 & 0.35 & 0.49 & 0.54 & 0.84 \\ VolSDF  & 1.14 & 1.26 & 0.81 & 0.49 & 1.25 & 0.7 & 0.72 & **1.29** & 1.18 & 0.7 & 0.66 & 1.08 & 0.42 & 0.61 & 0.55 & 0.86 \\ MonoSDF  & 0.83 & 1.61 & 0.65 & 0.47 & **0.92** & 0.87 & 0.87 & 1.3 & 1.25 & **0.68** & 0.65 & 0.96 & 0.41 & 0.62 & 0.58 & 0.84 \\ Voxurf  & 0.91 & **0.73** & **0.45** & **0.34** & 0.99 & 0.66 & 0.83 & 1.36 & 1.31 & 0.78 & 0.53 & 1.12 & 0.37 & 0.53 & 0.51 & **0.76** \\  Ours & **0.78** & 1.29 & 0.68 & 0.42 & 0.96 & 0.70 & 0.68 & 1.43 & 1.30 & 0.827 & 0.62 & **0.94** & **0.34** & **0.49** & 0.79 \\   

Table 3: Quantitative comparisons for _dense-view_ 3D reconstruction on DTU dataset (Section 4.1).

Figure 7: Qualitative reconstruction results for three disparate view inputs on the challenging BlendedMVS dataset . See Section 4.2 and the Supplementary for a more details explanation.

[MISSING_PAGE_FAIL:10]

Acknowledgements

We thank the anonymous reviewers for their valuable comments and Sherwin Bahmani and Yizhi Wang for discussions during rebuttal and helping with the renderings in the paper. This research is supported in part by a Discovery Grant from NSERC (No. 611370).