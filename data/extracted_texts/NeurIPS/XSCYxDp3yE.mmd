# A Bayesian Approach To Analysing Training Data Attribution In Deep Learning

Elisa Nguyen

Tubingen AI Center

University of Tubingen

&Minjoon Seo

KAIST AI

&Seong Joon Oh

Tubingen AI Center

University of Tubingen

###### Abstract

Training data attribution (TDA) techniques find influential training data for the model's prediction on the test data of interest. They approximate the impact of down- or up-weighting a particular training sample. While conceptually useful, they are hardly applicable to deep models in practice, particularly because of their sensitivity to different model initialisation. In this paper, we introduce a Bayesian perspective on the TDA task, where the learned model is treated as a Bayesian posterior and the TDA estimates as random variables. From this novel viewpoint, we observe that the influence of an individual training sample is often overshadowed by the noise stemming from model initialisation and SGD batch composition. Based on this observation, we argue that TDA can only be reliably used for explaining deep model predictions that are consistently influenced by certain training data, independent of other noise factors. Our experiments demonstrate the rarity of such noise-independent training-test data pairs but confirm their existence. We recommend that future researchers and practitioners trust TDA estimates only in such cases. Further, we find a disagreement between ground truth and estimated TDA distributions and encourage future work to study this gap. Code is provided at https://github.com/ElsiaNguyen/bayesian-tda.

## 1 Introduction

Understanding how machine learning models arrive at decisions is desirable for social, legal and ethical reasons, particularly for opaque deep learning models . One approach to explanations is the data-centric approach of training data attribution (TDA). As the name suggests, TDA finds attributing training samples for a model decision, uncovering which part of the training data is relevant. The attribution \(\) of a training sample \(z_{j}\) on another sample \(z\) is usually defined as the change of model loss \(\) on \(z\) when the model is retrained without \(z_{j}\):

\[(z_{j},z):=(z;_{ j})-(z;)\] (1)

where \(\) is a model trained on the entire dataset \(\) and \(_{ j}\) is a model trained on the same set without \(z_{j}\). Since the direct computation of Equation 1 is expensive, various TDA techniques for approximating the quantity have been proposed, such as influence functions  or TracIn . Their approximations are often based on some form of inner product between the parameter gradients \(_{}(z;)\) and \(_{}(z_{j};)\).

Knowing how training samples attribute to a model decision provides an actionable understanding of the training data distribution, especially in cases of model error. TDA methods can identify the training samples that are most relevant to an error and therefore enable users to understand why the error occurred (e.g. due to domain mismatch of test and training data or wrongly labelled training data) . Additionally, TDA gives them the tool to address the errors by e.g. changing the model directly through the training data. Even in non-erroneous cases, understanding the attributing trainingdata may enable users affected by model decisions to contest the decisions if the attributing training data is noisy or of low quality .

At the same time, TDA methods, especially influence functions , have been criticised for their fragility when applied to deep models [6; 7; 8]. The main reasons are model complexity and the stochasticity of deep model training. While the former poses a challenge specifically for influence functions as they rely on strong convexity assumptions, the latter is a more general challenge [6; 9]. The randomness inherent to the training process does not only lead to variation in the learned model parameters but also in TDA scores, which makes them untrustworthy. Hence, K & Sogaard  recommend using expected TDA scores for increased stability.

We argue that solely considering the expectation is not sufficient to ensure the reliability of TDA but requires inspecting the variance, too. We introduce a Bayesian perspective on the TDA task, noticing that there is no deterministic mapping from a dataset \(\) to the corresponding model \(\) for deep neural networks. The learned model depends on the initialisation and batch composition in the stochastic gradient descent (SGD) optimiser. We capture the resulting randomness via Bayesian model posterior \(p(|)\) over the parameter space [10; 11; 12]. In turn, the TDA estimate (Equation 1) is a random variable that depends on two posteriors, \(p(|)\) and \(p(_{ j}|_{ j})\).

This viewpoint leads to a few insights into the practical usage and evaluation of TDA techniques. We confirm quantitatively that the ground-truth influence \((z_{j},z)\) is often dominated by the noise: \(()}>||\). We argue that it is practically difficult to apply any TDA technique on pairs \((z_{j},z)\) whose ground-truth attributions \((z_{j},z)\) are noisy in the first place. Likewise, any evaluation of TDA methods on such high-variance pairs would not be reliable.

Nonetheless, we are optimistic that TDA techniques are useful in practice, particularly for train-test pairs with high signal-to-noise ratios: \(()}||\). We observe that such pairs are rare but consistently present in multiple experiments. We recommend that researchers and practitioners confine their usage to scenarios where the signal-to-noise ratios are expected to be large enough.

Our contributions are as follows: (1) Bayesian formulation of the training data attribution (TDA) task. (2) Observation that the ground-truth TDA values are often unreliable and highly variable. (3) Recommendation for the community to use the TDA tools only when the expected noise level is low. (4) Experimental analysis of the contributing factors to the variance of ground-truth TDA values. (5) Observation that the TDA estimation methods capture local changes in the model with regard to the counterfactual question of "retraining without training sample \(z_{j}\)", while LOO retraining itself results in a more global change through the training procedure.

## 2 Background

We cover the background materials for the paper, including the concept, method, and evaluation of training data attribution (TDA) methods and Bayesian deep learning.

### Training data attribution (TDA)

We introduce the TDA task, a few representative TDA methods, and existing evaluation strategies.

TDA task.Given a deep model \(f_{}\) parametrised by \(\), a training set \(:=\{z_{1},,z_{N}\}\), and a test sample \(z\), one is interested in the impact of a training sample \(z_{j}\) on the model's behaviour on the test sample \(z\). In the TDA context, one is often interested in the counterfactual change in the loss value for \(z\) after **leave-one-out (LOO)** training, when \(z_{j}\) is excluded from the training set (Equation 1). TDA has been considered in different use cases, such as understanding the bias in word embeddings , fact tracing in language model outputs  and measuring the robustness of model predictions .

TDA methods.The conceptually most straightforward way to compute the difference due to LOO training (Equation 1) is to compute it directly. However, this is computationally expensive, as it involves the learning algorithm for obtaining \(_{ j}\) for every \(j\). This gives rise to various TDA techniques that find _approximate_ estimates \(^{}(z_{j},z)\) of LOO. A prominent example of such approximation is the **influence function (IF)** method  based on . Under strong smoothness assumptions, they approximate Equation 1 by:

\[^{}(z_{j},z):=-_{}(z;)^{}H_{}^{-1 }_{}(z_{j};)\] (2)

where \(_{}(z;)\) and \(_{}(z_{j};)\) refer to the parameter gradients of \(f_{}\) for \(z\) and \(z_{j}\) respectively. Recognising the difficulty of scaling up the inverse Hessian computation \(H_{}^{-1}\) and the high dimensionality of operations in Equation 2, subsequent papers have proposed further approximations to speed up the computation [16; 17]. Charpiat _et al._ have analysed the influence of \(z_{j}\) on \(z\) by dropping the need to compute the Hessian and formulating influence as the loss change when an **additional training step (ATS)** on \(z_{j}\) is taken:

\[(z_{j},z):=(z;_{+j})-(z;)\] (3)

where \(_{+j}\) is a learned model parameter with \(\) and an additional step on \(z_{j}\). They propose two approximations:

\[\] (4) \[\] (5)

This method is closely linked to **TracIn** which computes the Grad-Dot not just at the end of the training, but averages the regular Grad-Dot similarities throughout the model training iterations. We note later in our analysis that within our Bayesian treatment of TDA, the TracIn method coincides conceptually with the Grad-Dot method. In our analysis, we study the sensitivity of LOO and the above TDA methods against noise.

TDA evaluation.The primal aim of TDA methods is to measure how well they approximate the ground-truth LOO values. This is often done by measuring the correlation between the estimates from each TDA method and the ground-truth LOO values (Equation 1) [3; 6; 7; 9]. They use either a linear (Pearson) correlation or a rank (Spearman) correlation over _a small number_ of train-test sample pairs \((z_{j},z)\) due to the computational burden of computing the actual LOO values, especially for larger models. Usually, a few samples \(z\) are chosen for a comparison against LOO, e.g. Koh & Liang  report results for one \(z\) and Guo _et al._ for 10 samples \(z\). In some cases, the ground-truth LOO is obtained by computing the change in loss after training further from the learned model parameters, e.g. . Some works have adopted indirect evaluation metrics such as the retrieval performance of mislabelled or poisoned training data based on the TDA estimates [3; 19; 16; 9; 17]. In this work, we adopt the Pearson and Spearman correlation metrics and discuss ways to extend them when the target (LOO from same initialisation) and estimates (TDA) are both random variables.

### Bayesian deep learning.

Bayesian machine learning treats the learned model as a posterior distribution over the parameter space, rather than a single point:

\[p(|)=p(|)p()/p().\] (6)

Bayesian ML nicely captures the intuition that the mapping from a training set \(\) to the learned model \(p(|)\) is not a deterministic mapping, especially for non-convex models like deep neural networks (DNNs). Depending on the initialisation, among other factors, DNN training almost always learns vastly different parameters.

The estimation of the true posterior is indeed difficult for complex models like DNNs. The field of Bayesian deep learning is dedicated to the interpretation of certain random elements in DNN training as sources of randomness for the approximated Bayesian posteriors. For example, if Dropout  is used for training a model, it may be used at test time to let users sample \(\) from the posterior distribution \(p(|)\). More generally used components like stochastic gradient descent (SGD) have also been interpreted as sources of randomness. The random walk induced by SGD iterations in the parameter space can be viewed as a Markov Chain Monte-Carlo sampler from the posterior distribution, after a slight modification of the optimisation algorithm (Stochastic Gradient Langevin Dynamics ). Similarly, the last few iterations of the vanilla SGD iterations may also be treated as samples from the posterior, resulting in more widely applicable Bayesian methods like Stochastic Weight Averaging (SWA) [12; 22]. Finally, the random initialisation of DNNs has also been exploited for modelling posterior randomness; training multiple versions of the same model with different initial parameters may be interpreted as samples from the posterior . We show in the next section how the Bayesian viewpoint will help us model the sources of stochasticity for TDA estimates.

## 3 A Bayesian perspective on training data attribution

Training data attribution (TDA) \((z_{j},z)\) is defined as the attribution of one training sample \(z_{j}\) to another sample \(z\) in terms of how a target metric like the loss of a sample \((z,)\) changes when the model is trained without \(z_{j}\) (Equation 1). We note here that according to the definition, we are interested in the impact of the change in the _dataset_ from \(\) to \(_{ j}\), rather than the change in the model parameter. From a Bayesian perspective, a change in the training dataset leads to a shift in the posterior distribution, \(p(|) p(_{ j}|_{ j})\), leading to the definition of TDA as a random variable:

\[(z_{j},z|):=(z;_{ j})-(z;)=(z;_{ j}|_{ j} )-(z;|)\] (7)

where \( p(|)\) and \(_{ j} p(_{ j}|_{ j})\). This interpretation is more natural, given the non-uniqueness of the mapping from a training dataset \(\) to the optimal model parameter \(\) for general, non-convex models like DNNs. Alternatively, one could treat the model built from \(\) as a fixed variable rather than a posterior as TDA is applied to a specific model in practice. The change of dataset from \(\) to \(_{ j}\) however still introduces ambiguity in \(_{ j}\), which is captured in the Bayesian posterior \(p(_{ j}|_{ j})\). In this study, we use the probabilistic formulation of TDA in Equation 7.

Sampling TDA values.One could plug in various Bayesian DL techniques (SS2.2) to compute samples of \(p(|)\), which can be used to get the samples of \((z_{j},z)\). In our work, we use the Stochastic Weight Averaging (SWA) [12; 22] and Deep Ensemble (DE)  which are applicable to a wide class of deep models. More specifically, we obtain \(T\) samples \(^{(1)},,^{(T)} p(|)\) either by taking the last \(T\) model checkpoints of the SGD iterations (SWA) or by taking the last model checkpoints from \(T\) different model initialisations (DE). The same is done for the counterfactual posterior \(^{(1)}_{ j},,^{(T)}_{ j} p(_{  j}|_{ j})\). This results in a mixture-of-Gaussian posterior, where DE samples correspond to centroids of the distribution. Our sampling approach is thus a version of stratified sampling, where the number of samples \(T\) from a centroid is fixed and sampled IID.

Statistical analysis on TDA.The simplest statistics for the TDA \((z_{j},z)\) are the mean and variance:

\[[(z_{j},z)] =_{t}(z;^{(t)}_{ j})- (z;^{(t)})\] (8) \[[(z_{j},z)] =}_{t,t^{}}(z;^ {(t)}_{ j})-(z;^{(t^{})})-[(z_ {j},z)]^{2}\] (9)

Our main interest lies in whether the influence of the training data \(z_{j}\) on the test data \(z\) is statistically significant and not dominated by the inherent noise of deep model training. For this purpose, we design a Student t-test  for quantifying the statistical significance. Our null and alternative hypotheses are:

\[H_{0}:=0 H_{1}: 0.\] (10)

We consider the test statistic based on sample mean and variance:

\[t=[(z_{j},z)]}{[(z_{j},z)]/T^{2}}}.\] (11)

Vars refers to the sample variance where the denominator in Equation 9 is \(T^{2}-1\) instead. We report the significance of the absolute TDA \(|(z_{j},z)|\) for every train-test pair \((z_{j},z)\) by computing the p-value corresponding to the t-test statistic. The greater the p-value is, the greater the dominance of noise is for the TDA estimate.

Figure 1: **A Bayesian interpretation of training data attribution (TDA).**TDA methods likewise estimate random quantities.Approximate TDA methods like influence functions (IF), Grad-Dot, and Grad-Cos (SS2.1) also predict random quantities \(^{}(z_{j},z)\). For example, IF predicts \((z_{j},z)^{}(z_{j},z):=-_{}(z_{j}; )^{}H_{}^{-1}_{}(z;)\), where one may sample \(\) from the posterior \(p(|)\). We note that IF suffers theoretical issues in its application to deep models, as convexity assumptions are not met. In practice, estimation algorithms make use of a damping term to ensure the positive definiteness of the inverse Hessian. Through a Bayesian lens, the damping term could be seen as an isotropic Gaussian prior centred at the origin. Similar statistical analyses on the TDA estimations can be performed as above, including the mean and variance computations and statistical testing for the significance of influence.

Evaluating TDA as a random variable.Previously, the LOO-based TDA values \((z_{j},z)\) and the estimates from various approximate TDA methods \(^{}(z_{j},z)\) are compared via correlation measures like Pearson or Spearman. Our treatment of those quantities as 1-D random variables poses a novel challenge for evaluation because there exists no inborn notion of ordering among 1-D random variables. We address the challenge by examining the approximation ability of TDA methods for both the first and second moments of the true TDA values \((z_{j},z)\). More specifically, we compute the Pearson and Spearman correlation for both the mean (Equation 8) and variance (Equation 9) between the ground-truth \((z_{j},z)\) and estimated TDA \(^{}(z_{j},z)\) values across multiple train-test pairs \((z_{j},z)\).

## 4 Experiments

We introduce our experimental settings, present analyses on factors contributing to the reliability of TDA values, compare TDA methods, and draw suggestions on the evaluation practice of TDA.

### Implementation details

We illustrate the specific details of our implementation. See the Appendix for further information.

TDA methods.We study different TDA methods from a Bayesian perspective. We test the methods introduced in SS2.1 for estimating TDA: **influence functions (IF)**, **Grad-Dot (GD)** and **Grad-Cos (GC)**. We use the PyTorch implementation of IF from Guo _et al._ and modify it for our models. As the ground-truth target, we consider **Leave-one-out training (LOO)**. For LOO, we remove of \(z_{j}\) from the training set \(\) by zeroing out the weight for sample \(z_{j}\) towards the loss. Additionally, we include Charpiat _et al._'s  notion of TDA that a training data point \(z_{j}\) attributes more if an **additional training step (ATS)** on it changes the test loss more significantly.

Inducing randomness in posterior \(p(|)\).In SS2.2, we have introduced the interpretation of various elements around model training as sources of randomness for Bayesian posterior. We summarise our methods for inducing randomness in Figure 2. We use the notion of the Deep Ensemble (DE)  to sample from the posterior. In a variant of DE with the initialisation as the source of randomness (**DE-Init**), we train each of \(T_{}\) randomly initialised parameters \(_{0}^{(t)}\) on either \(\) or \(_{ j}\). The resulting parameter sets, \(^{(t)}\) and \(_{ j}^{(t)}\), are treated as samples from respective

Figure 2: **Sources of randomness for Bayesian posteriors.** In each case, the training starts from initialisation \(_{0}\). Depending on whether \(z_{j}\) is included in the training data, one has either samples from the original posterior \(p(|)\) or from the counterfactual posterior \(p(_{ j}|_{ j})\). For deep ensemble , the randomness stems either from random initialisation (DE-Init) or from SGD batch composition (DE-Batch). For stochastic weight averaging (SWA) [12; 22], last few checkpoints of the training are treated as posterior samples.

posteriors. We also consider the batch composition in stochastic gradient descent (SGD) as the source of randomness (**DE-Batch**). In this case, we train from one initial parameter \(_{0}\) with \(T_{}\) different random shuffles \(^{(t)}\) of the training sets \(\) and \(_{ j}\). This results in two sets of samples from the original and counterfactual posteriors. We increase the number of samples by taking the last \(T_{}\) checkpoints as the Stochastic Weight Averaging (**SWA**) samples . For Grad-Dot, this coincides with the definition of TracIn  as we average the dot products across checkpoints. In total, we take \(T=T_{} T_{}=10 5\) samples from \(p(|)\) and \(p(_{ j}|_{ j})\) to estimate \((z_{j},z)\).

Datasets \(\).To enable an exhaustive analysis of every train-test pair \((z_{j},z)\), we define smaller datasets. We use variants of MNIST  limited to three classes (MNIST3), and CIFAR10 . For MNIST3, we sample a training set of size 150 and a test set of size 900, i.e. 135,000 train-test pairs. For CIFAR10, we define the training and test set at size 500, i.e. 250,000 train-test pairs.

Models.We consider two types of image classifiers, visual transformers (ViT, ) and convolutional neural networks , where we primarily study a two-layer (CNN2-L). We also include a three-layer version (CNN3-L) to study the factor of model complexity. For ViT variants, instead of full finetuning, we use LoRA adapter layers  to minimise the number of parameters being tuned. The number of trainable parameters of ViT+LoRA (597,514) is comparable to CNN3-L (620,362).

### Reliability of TDA evaluation

We assess the reliability of TDA evaluation by measuring the degrees of noise in both the ground-truth TDA (LOO) \((z_{j},z)\) and the estimated TDA \(^{}(z_{j},z)\). The noise level is measured with the p-value of the Student-t hypothesis testing to determine if the absolute TDA values are significantly greater than the sample noise (SS3).

We report the results in Table 1. Generally, we observe many TDA measurements, ground-truth and estimations likewise, are unstable with non-significant p-values (\(>0.05\)). In particular, even the ground-truth LOO shows p-values of 0.331 on MNIST3 and 0.692 for CIFAR10 (SWA+DE-Init). In these cases, the noise effectively dominates the signal and any evaluation that does not consider the variance in the posterior \(p(|)\) is likely to be misleading. This confirms the reports in  that TDA values are sensitive to model initialisation.

TDA methods often show similar levels of instability. For example, the IF attains p-values 0.352 and 0.575 on MNIST3 and CIFAR10, respectively, roughly matching the LOO case. Grad-Cos is an exception: it attains lower p-values than the other TDA methods (0.003 and 0.356 for MNIST3 and CIFAR10, respectively). We interpret this as an overconfident TDA estimation. Practitioners shall be wary of using TDA methods that are unreasonably stable when the ground-truth TDA itself is not.

   Data & Randomness & LOO & ATS & IF & GD & GC \\   & SWA\(+\)DE-Init & 0.331 & 0.254 & 0.352 & 0.363 & 0.003 \\  & SWA\(+\)DE-Batch & 0.025 & 0.039 & 0.000 & 0.000 & 0.000 \\   & SWA\(+\)DE-Init & 0.692 & 0.437 & 0.575 & 0.587 & 0.356 \\  & SWA\(+\)DE-Batch & 0.487 & 0.296 & 0.484 & 0.517 & 0.236 \\   

Table 1: **Stability of TDA estimates.** We report p-values for the ground-truth TDA \((z_{j},z)\) (LOO) and the estimated TDA values \(^{}(z_{j},z)\) (rest 4 columns). The p-values are averaged across all train-test pairs \((z_{j},z)\). We use the CNN model throughout.

Figure 3: **Stability of TDA estimates per train-test pair.** Distribution of p-values for ground-truth TDA (LOO) for different experiments.

### Factors influencing the variability of TDA

Based on the observation in SS4.2 that TDA values are often dominated by noise, we delve into the factors that lead to the instability of data attributions. We inspect the contribution of model initialisation, training set size and model complexity.

Source of randomness.From a Bayesian ML perspective, the stochasticity of TDA stems from the inherent uncertainty of the learned model posterior \(p(|)\). We consider two sources of randomness, model initialisation (DE-Init) and SGD batch composition (DE-Batch). Results are reported in Table 1. For MNIST3 and CIFAR10, we observe that DE-Batch introduces lower levels of noise in the TDA estimates (lower p-values). Particularly on MNIST3, both LOO and other TDA methods result in statistically significant p-values (\(<0.005\)). This implies that almost every training data \(z_{j}\) is influencing every test data \(z\) consistently across various batch compositions. We conclude that the greater source of variations for the TDA estimates is the model initialisation.

Training set size.We study how training set size is a source of noise (cf. Figure 4). We train the CNN2-L with different-size datasets of MNIST3 and CIFAR10, where we vary the number of samples per class. Batches are composed differently depending on the dataset size, meaning that parameter updates are made after processing different data. In addition, we train a CNN2-L on the complete MNIST dataset and use a subset of the training data for our experiments (cf. Appendix B.4). The results show a tendency for high variation in TDA scores with larger datasets first after which a decrease in variation is observed. The initial increase makes sense as the number of combinations for batch composition increases with dataset size. As the batching is initialised randomly during training, batches are likely to be composed of different data for larger datasets. This leads to variation in the learned model parameters, in turn affecting the reliability of TDA. At the point of decrease, the TDA scores are rather small for all train-test pairs. The attribution of individual training samples to a model prediction is overall small in larger datasets, which leads to a decrease in variance.

Model complexity.We study how model complexity is linked to the reliability of TDA estimates. See Table 2. We observe that, compared to the CNN models, a large ViT model trained with LoRA results in dramatically greater p-values. For example, for LOO on MNIST3, the p-value increases from 0.025 (CNN2-L) to 0.786. A similar trend is observed for other TDA methods. A less dramatic increase in p-values can also be observed with the addition of another layer to the CNN (i.e. from

   Model & Data & LOO & ATS & IF & GD & GC \\  CNN2-L & MNIST3 & 0.025 & 0.039 & 0.000 & 0.000 & 0.000 \\ CNN3-L & MNIST3 & 0.370 & 0.368 & 0.464 & 0.470 & 0.005 \\ VIT+LoRA & MNIST3 & 0.786 & 0.573 & 0.369 & 0.365 & 0.093 \\  CNN2-L & CIFAR10 & 0.623 & 0.374 & 0.535 & 0.534 & 0.314 \\ CNN3-L & CIFAR10 & 0.687 & 0.432 & 0.579 & 0.581 & 0.365 \\ VIT+LoRA & CIFAR10 & 0.777 & 0.766 & 0.686 & 0.686 & 0.522 \\   

Table 2: **Impact of model complexity.** Mean p-values of the ground-truth TDA \((z_{j},z)\) (LOO) and estimated TDA values \(^{}(z_{j},z)\) (the other 4 columns) with randomness induced by SWA+DE-Batch for MNIST3 and DE-Batch for CIFAR10.

Figure 4: **Impact of training data size.** Mean p-values of TDA methods with randomness induced by SWA+DE-Init.

CNN2-L to CNN3-L). This implies that the reliability of TDA estimates decreases with increasing model complexity. While we limit the number of trainable parameters in our ViT by using LoRA to be comparable to CNN3-L, the p-values computed from TDA estimates are significantly larger. Larger models exhibit a larger parameter space so that noise stemming from model initialisation or batch composition is amplified. While we fix the model initialisation and dataset size, the batch composition still varies across the model parameters \(\) sampled from the posterior \(p(|)\) per model. As both the CNNs and ViT are trained with the same sampled batch compositions, we attribute the strong increase of p-value to the model complexity.

### (Dis)agreement between TDA methods

We test the reliability of different TDA methods. Ideally, all methods approximate the ground-truth TDA (LOO). Yet the results suggest that there are substantial differences among the methods. For example, Grad-Cos is much more stable than all others. Hence, we study TDA methods with respect to both their correlation with LOO and among each other using Pearson and Spearman correlation of mean and variance of the TDA distributions, as proposed in SS3.

Figure 5 shows the correlation matrices for one experiment (all experimental results in the Appendix). The results show that viewing TDA scores as distributions gives insights into the reliability of TDA methods: None of the tested TDA method's expected values \(\) correlates with LOO. This implies that none of the TDA methods is a good approximation when the random factors are considered. The poor correlation of p-values of LOO indicates a disagreement in the train-test pairs considered low noise. We conclude that none of the tested methods reliably capture ground-truth TDA distributions.

Interestingly, we notice a stronger correlation between all other methods particularly when looking at the correlations of p-values. We identify two groups based on positive correlation, i.e. ATS with IF and GD with GC. Among the two groups, there is a negative correlation which indicates that methods interpret the sign of the attribution differently. Between IF, GD and GC this makes sense, as there is a negative sign in the definition of IF (Equation 2) which is not present in GD and GC. Considering absolute correlation, IF and GD are strongly correlated which shows that the dot product is a valid alternative for IF as they produce similar score distributions. The correlation between GD and GC indicates that the normalisation of gradients does not have a strong impact on the estimated TDA.

IF, GD and GC correlate considerably with ATS, which measures how the loss of a model on \(z\) changes after doing one additional training step on \(z_{j}\). Practically, ATS represents the gradient update after \(z_{j}\), which is the same as the gradient \(z_{j}\) itself. Therefore, it makes sense that the gradient-based approximation methods are close to ATS. We recognise a difference in the scope LOO and ATS

Figure 5: **Correlation of TDA methods.** Pearson and Spearman correlation coefficients among ground-truth TDA and approximate TDA methods. We show correlations for TDA mean \(\), TDA standard deviation \(\), and TDA p-values. All results are based on the setting: CNN2-L, MNIST3, SWA+DE-Init.

address. LOO looks at TDA globally and encapsulates the whole training, whereas ATS considers a local scope with a small model change. As IF, GD and GC correlate with ATS, we observe that they also correspond to a local change in the model, which underlines and extends the argument of : There is a gap between LOO and IF, and more generally between the global and local view on TDA.

The TDA variance \(\) is noticeably well-correlated for TDA estimators and LOO, except for GC. This implies the existence of a consistent ranking of train-test pairs with stable attribution relationships. In particular, stable train-test pairs predicted by LOO are also likely to be stable pairs for TDA methods like ATS, IF, and GD. This motivates our final analysis and recommendation for evaluation in SS4.5.

### Considerations on TDA evaluation from a Bayesian perspective

Our analysis shows that both TDA estimates and ground-truth TDA values are affected by the noise stemming from the stochastic nature of deep model training. Hence, the practice of comparing against such a ground truth is destined to result in fragile estimates. We propose to treat TDA estimates as random variables which allows us to look at the evaluation from a Bayesian perspective: The comparison of TDA estimates against target TDA values is a comparison of two random variables. Since it is impossible to get rid of noise, it is better to compare distributions rather than point estimates. This provides an understanding of how well methods approximate the ground-truth distribution.

We observe that p-values vary between individual train-test sample pairs \((z_{j},z)\); not all TDA estimates are equally affected by stochasticity. Interestingly, the presence of low-noise pairs is consistent across the majority of our experiments (cf. Figure 3), with varying sizes of the low-noise fraction. We find that fixing model initialisation and a small dataset size gives rise to a larger number of low-noise pairs.

**We propose to focus on such low-noise pairs in TDA evaluation as their estimates are low in variance, leading to a more reliable evaluation.** Identifying such pairs requires an analysis similar to this work: treating TDA values as distributions and sampling multiple times from the posterior to get an estimate of the noise. It is crucial to find low-noise pairs to base evaluations on and understand when TDA is applicable. If no low-variance pairs exist, TDA cannot be used.

## 5 Related work

We study the reliability of TDA methods and add to the existing body of work on the fragility of TDA methods. Previous studies focused primarily on IF [29; 6; 8; 9; 7]. We extend the analysis by additionally studying other TDA methods. While IFs are theoretically grounded in robust statistics , they are based on two assumptions which are not always fulfilled in the context of deep learning: Twice-differentiability and strict convexity of the loss . Zhang & Zhang  and Basu _et al._ point to the fragility of the influence scores due to the non-convexity of deep learning. Particularly increasing model size is connected to increased model curvature, which means that influence estimates are more fragile with larger models. They find that strong regularisation is needed to improve estimation quality. Our experiments verify the observation that fragility increases with model size, which we observe across methods. We add that sources of randomness in the training process attribute to the fragility of TDA methods with increasing model size. Furthermore, related work found that the size of the training set contributes to the fragility of influence estimates. The attribution of one sample in a large training set is marginal so both influence estimates and ground-truth influence scores (i.e., from retraining the model) are noisy [6; 8; 9]. Through a Bayesian lens, we connect the increased fragility with increasing dataset size to batch composition as well. Not only is the attribution of a single sample in a large dataset marginal  but batches have vastly different compositions in larger datasets, introducing noise. A recent work  states that influence functions in deep learning do not correspond to LOO and quantify gaps in the estimation stemming from model non-linearity. A different approach in TDA [30; 31] aims at predicting the expected model output given a set of data points, directly considering randomness stemming from model initialisation. K & Sogaard  recommend reporting expected TDA scores to increase estimation stability. This approach is closest to our work but misses the consideration of variance in TDA estimates which we include by taking a Bayesian viewpoint.

In contrast to related work, we treat TDA values as distributions, which enables a novel perspective on the TDA task for deep models. We highlight the importance of considering the variance when studying reliability.

Conclusion

We adopt a Bayesian perspective on the training data attribution (TDA) methods to study their reliability when applied to deep models, given the stochastic nature of deep model training. By modelling TDA scores as distributions, we find that randomness in the training process, particularly due to parameter initialisation and batch composition, translates to variation in ground-truth TDA. We empirically observe that current estimation methods, such as influence functions, model a local change in the model whereas the ground truth attribution considers a global model change. Therefore, TDA is subject to inherent variance, leading us to suggest to the community: (1) When proposing a novel TDA method, one should view TDA from a Bayesian perspective and study the TDA estimates as distributions. (2) When using TDA, one should consider the variance to understand when the estimate can be trusted.

Limitations.We perform an exhaustive analysis of TDA values \(\) and the estimates \(^{}\) for all train-test pairs \((z_{j},z)\). Because of considerable computational costs, we have subsampled the datasets. In practice, datasets are considerably larger. Moreover, we choose simple tasks to eliminate the need for an additional hyperparameter search for model training, as the principal focus is on studying TDA methods. We choose gradient-based TDA methods but acknowledge that there exist many more, that we do not address. Hence, we encourage further study of TDA methods to fill these gaps and recommend investigating TDA from a Bayesian perspective, particularly in the low-data regime.

Broader impact.This paper contributes to the field of data-driven XAI which aims at helping humans understand the inner workings of opaque models through data-centric approaches. Our work contributes to understanding the reliability of TDA methods and rethinking their evaluation against a noisy ground truth, which could help assess when TDA is appropriate and reliable.