# Towards Stable Representations for Protein

Interface Prediction

 Ziqi Gao1,2, Zijing Liu3*, Yu Li3, Jia Li1,2*

\({}^{1}\)Hong Kong University of Science and Technology

\({}^{2}\)Hong Kong University of Science and Technology (Guangzhou)

\({}^{3}\) International Digital Economy Academy (IDEA)

Footnote *: Correspondence to: Zijing Liu (liuzijing@idea.edu.cn) and Jia Li (jialee@ust.hk).

###### Abstract

The knowledge of protein interactions is crucial but challenging for drug discovery applications. This work focuses on protein interface prediction, which aims to determine whether a pair of residues from different proteins interact. Existing data-driven methods have made significant progress in effectively learning protein structures. Nevertheless, they overlook the conformational changes (i.e., flexibility) within proteins upon binding, leading to poor generalization ability. In this paper, we regard the protein flexibility as an _attack_ on the trained model and aim to defend against it for improved generalization. To fulfill this purpose, we propose ATProt, an adversarial training framework for protein representations to robustly defend against the attack of protein flexibility. ATProt can theoretically guarantee protein representation stability under complicated protein flexibility. Experiments on various benchmarks demonstrate that ATProt consistently improves the performance for protein interface prediction. Moreover, our method demonstrates broad applicability, performing the best even when provided with testing structures from structure prediction models like ESMFold and AlphaFold2.

## 1 Introduction

Protein-protein interactions are important for understanding biological processes, and for the design of novel therapies  and drugs . The protein interface refers to the surface region of a protein where the interaction occurs. It therefore holds the key to revealing the specific interaction mechanism and understanding protein functions. In this work, we tackle the problem of protein

Figure 1: (A). **The task illustration.** PIP involves predicting if there is an interaction between two residues from different proteins. (B). **The task challenge.** During training, the input consists of bound structures of two proteins. However, for testing, one can only access their unbound structures.

interface prediction (PIP) (shown in Figure 1(A)): predicting whether two residues, each from an individual protein, interact with each other, given the separate structures of two proteins.

For years, data-driven methods based on deep learning (DL) have made significant progress in response to this critical task by effectively learning protein structures using geometric graph neural networks (GNN) [15; 33], 3D CNN , etc. Limited by the difficulty in accessing protein structure data, they typically follow a formulation of training on bound (after binding) structures and testing on unbound (before binding) ones, as depicted in Figure 1(B). For training, large-scale datasets like the Database of Interacting Protein Structures (DIPS)  typically consist of only bound structures, which are directly extracted from the PDB database. In contrast, in the practical inference scenario, the model cannot access the bound structures but can only be provided with unbound ones [43; 15; 33]. In this paper, we empirically find that this training (bound)-testing (unbound) formulation leaves significant room for performance improvement. In Figure 2, exploratory experiments show that prevailing PIP methods are sensitive to flexibility. Utilizing the bound version structures for testing can greatly boost their performance. Based on these findings, we aim to answer the question in this paper--_how to handle the mismatch between bound and unbound structures for PIP?_

Since it is usually impractical to access the protein bound structures for testing, the most intuitive solution is to explicitly learn the mapping relationship from unbound to bound structures of proteins. However, this is challenging due to the following two factors: (1) The amount of pairwise unbound and bound structure data for proteins is extremely limited  (to our knowledge, only DB5.5 ). (2) A protein's bound structure is not unique and depends on its binding partner, so diverse training data is necessary. **To address this issue, we take a different route.** We consider any potentially complicated flexibility in a protein as an attack [29; 24], which can harm the testing performance of a model trained on bound structures. Therefore, our core idea is to enable protein representations with adversarial robustness, which can defend against the attacks of protein flexibility. In simple terms, for a protein with both unbound and bound versions, the model outputs similar (stable) representations.

In this work, we take an important step forward in mitigating the impact of flexibility on PIP. We propose **ATProt**, an end-to-end adversarial training (AT) framework for protein representations, to effectively defend against protein flexibility in PIP. Inspired by the recent protein graph representation methods [17; 53; 15], our model comprises graph-based feature extractors (encoders) for protein graphs. Our ATProt framework does not require computationally expensive data augmentation and can be smoothly applied to most existing protein graph encoders. Specifically, we implement differentiable AT regularizations for various protein representation encoders. Importantly, we introduce a novel and expressive graph encoder for protein representations and propose its theoretical regularization form for the first time. ATProt can produce stable representations for the same protein with different structure versions (e.g., bound, unbound, and model-generated ones). Extensive experiments on several protein interaction benchmarks verify that our ATProt method consistently outperforms advanced PIP methods. The results demonstrate the effectiveness of the AT regularization. Furthermore, ATProt maintains excellent performance even when tested with structures generated by AlphaFold2  and ESMFold , allowing for user-friendly inference without the need for native structures.

Figure 2: The impact of flexibility on results with the DB5.5 dataset . (A) The testing results of two baselines (SASNet , NEA ) and our method. ‘B-U’ represents the popular formulation, i.e., training with bound structures and testing with unbound ones. ‘B-B’ refers to the formulation where both training and testing are conducted with bound structures. (B) Loss trends for three method.

Related Work

Protein interface prediction.Protein interface prediction (PIP), a well-studied problem, focuses on determining whether there is an interaction between amino acids from two different proteins. Recently, a series of methods based on protein [8; 19; 17; 20; 14] or amino acid representation [39; 46] learning have achieved significant success. NEA  pioneers the use of protein graphs to address PIP, where protein structure information is represented and aggregated, followed by the dense layers. SASNet  considers embedding the hierarchical structures of proteins, integrating atomic and amino acid information into a 4D-grid data, and employs 3D CNN for learning. To further enhance performance, more fine-grained structure information modeling, specifically surface geometry [46; 39], is introduced to effectively learn amino acid representations. Existing methods have effectively represented proteins from various perspectives of protein information. However, we have observed that protein flexibility, which is overlooked by most methods, poses significant performance bottlenecks for them. We focus on this key issue of mitigating the bound-unbound mismatch in protein structures to improve model generalization.

Modelling protein flexibility.Recently, pioneer works in biology confirm that protein-protein interaction (PPI) conforms to the "induced fit" theory [27; 38]. Specifically, proteins undergo structure changes due to residue-level forces, and they adjust structures to achieve the best binding state. More importantly, proteins with PPI typically undergo larger structure changes at the interface compared to non-interface regions [10; 11; 52; 12], which will exacerbate the generalization challenge of the PIP task. Modelling flexibility directly is challenging, whether using traditional computational or deep learning (DL) approaches. Traditional methods often rely on finding the lowest energy state [42; 51] or introducing an induced fit model (specifically, the elastic network model) [12; 3] to guide structure deformations. The optimization space in these methods is vast, making them very time-consuming. DL-based methods  struggle to achieve satisfactory accuracy in learning the distribution mapping of bound-unbound states due to limited training data (i.e., 253 complexes in the DB5.5). As directly predicting bound structures is challenging, in the context of the PIP task, we choose to eliminate the influence of different versions of the same protein structure on the task.

## 3 Preliminaries

In this section, **(1)** we present the definition of the protein interface prediction (PIP). Then, **(2)** we verify the importance of representation stability through empirical and mathematical views. Finally, **(3)** we investigate to ensure protein representation stability within the adversarial training framework.

Problem definition.We are given as input two proteins \(^{1}\) and \(^{2}\), consisting of \(M\) and \(N\) residues, respectively. The proteins are represented as their residue sequences and 3D structures, which are composed of \(\)-carbon atom locations of all residues. The goal of the PIP is to classify all possible pairs of residues from separate proteins. More formally, the set of data is \(\{(^{1}_{i},^{2}_{j}),y_{ij}\}_{1 i M,1 j  N}\), where \(^{1}_{i}\) represents the \(i\)-th residue in protein \(^{1}\) and \(y_{ij}\{0,1\}\).

### The importance of protein representation stability

We verify the importance of stable protein representations from both empirical and theoretical perspectives. For clarity, we use the notations \(^{b}_{1}\), \(^{b}_{2}\) to represent the native bound structures of proteins \(^{1}\) and \(^{2}\), while \(^{t}_{1},^{t}_{2}\) represent their structures used for testing. After using a protein graph encoder, we have their representations, denoted as \(^{b}_{1},^{b}_{2},^{t}_{1},^{t}_{2}\). We denote the protein representation perturbation as \(\|_{1}\|_{p}+\|_{1}\|_{p}\), where \(_{1}=^{t}_{1}-^{b}_{1},_{2}=^{t}_{2}-^{b}_ {2}\).

From an empirical perspective, in Figure 3, we quantify the test results of the NEA method  under flexibility. We gradually increase the structure change of test samples and calculate the representation perturbation. We note that the test performance is negatively correlated with the representation perturbation caused by flexibility. Moreover, testing with bound structures yields the best results.

Figure 3: AUC vs. representation perturbation.

**From a theoretical perspective,** we draw the consistent conclusion that stable representations lead to improved PIP results. Following , we model PIP as a pairwise classification problem. Specifically, we concatenate the \(i\)-th and \(j\)-th rows of \(_{1}^{t}\) and \(_{2}^{t}\) into a vector embedding, which is then sent to the PIP classifier \(f\). In this case, the following proposition describes the impact of representation perturbations on PIP results.

**Proposition 3.1**.: _For two proteins with \(M\) and \(N\) residues respectively, the classification results obtained using bound and unbound structures are the same. This is true if \(N\|_{1}\|_{p}^{p}+M\|_{2}\|_{p}^{p}<(f,p)\), where \((f,p)\) can be a constant depending only on the PIP classifier \(f\) and the norm \(\|\|_{p}\)._

We detail \((f,p)\) and prove Proposition 3.1 in Appendix B.1. This proposition tells that stable protein representations (i.e., smaller \(\|_{1}\|_{p}\) and \(\|_{2}\|_{p}\)) under flexibility are necessary for achieving high performance. Thus, to effectively address the structure mismatch in PIP, an intuitive idea is to perform adversarial training for protein representation learning.

### Adversarial training

Here, we introduce the concept of adversarial training (AT) [1; 6; 18; 5; 7] and establish a connection between it and the stability of protein representations. We consider a classification task with a given dataset \(=\{(x_{i},y_{i})\}_{i=1}^{n}\), consisting of \(K\) classes. We assume that the entire prediction pipeline includes a representation model (e.g., encoder) and a classifier. The concept of adversarial training (AT) requires the entire pipeline to perform well not only on \(\) but also on the worst-case distribution near \(\), as determined by a specific distance metric. More concretely, the AT that we primarily focus on in this paper is the \(_{p}\)-robustness. For a given \(p\) value and a finite \(>0\), AT aims to train a pipeline that can correctly classifies \((x+,y)\) for any \(\|\|_{p}\), where \((x,y)\) belongs to \(\).

Among all AT methods, Lipschitz neural networks belong to a common and effective category. Specifically, an encoder function is considered to have Lipschitz continuity if a slight perturbation to the input of the encoder does not significantly change its output.

Formally, the definition of Lipschitz continuity is given by:

**Definition 3.1**.: _(**Lipschitz continuity in adversarial training)** An encoder function \(\) is said to be \(C\)-Lipschitz continuous w.r.t. norm \(\|\|\) if for any two versions of inputs \(_{1},_{2}\),_

\[\|(_{1})-(_{2})\| C\| _{1}-_{2}\|.\] (1)

Lipschitz continuity explains the requirements of AT for a general representation learning encoder. In the context of protein graph representation, this can be modified to become the definition below.

**Definition 3.2**.: _(**Lipschitz continuity for protein representations)** A protein representation encoder \(()\) has \(C\)-Lipschitz continuity w.r.t. norm \(\|\|\) if for any two versions of structure inputs \(^{t},^{b}\) and the invariant residue feature input \(\),_

\[\|(,^{t})-(,^{b})\| C\| ^{t}-^{b}\|.\] (2)

### How to ensure Lipschitz continuity for protein graph representations?

As an expressive representation, graph structured data is widely used for representing input proteins [17; 22; 19], with residues acting as nodes and physical interactions as edges. Let \(=(,)\) be an undirected graph with nodes \(=\{1,...,N\}\), edges \(^{2}\), graph signal \(^{N d}\) and a graph shift operator \(}^{N N}\) (i.e., node connectivity). We consider any variant of the spectral GNN (e.g., GCN [26; 30], ChebNets , BWGNN [41; 40]) that follows the concept of learning filter coefficients for graph convolution. By constructing the filter \(h():=_{k=0}^{K}_{k}^{k}\) (\(_{k}\) are learnable parameters), the protein graph representation can be defined as \(=_{k=0}^{K}_{k}^{k}:=h()\).

Here, we extend the Definition 3.2 to the scenario of using GNN models. To achieve this, we assume \(\) is perturbed to become \(}\) due to the protein flexibility, and introduce the key factor (_GNN filter stability constant_\(C_{h}\)), for achieving GNN-based stable protein representations.

**Definition 3.3**.: _(**GNN filter stability constant)** Given a graph spectral filter \(h:\), it is defined as Lipschitz with constant \(C_{h}>0\) if for any pair of points \(_{1},_{2}\):_

\[|h(_{1})-h(_{2})| C_{h}|_{1}-_{ 2}|,\] (3)which introduces our main theorem below.

**Theorem 3.1**.: _(Protein graph stability with GNNs) Let the perturbation to \(\) is finite such that \(\|}-\|_{p}\). The protein graph encoder \(()\) is always stable with a polynomial filter \(h\) if for some finite \(C_{h}\),_

\[\|(,)-(,})\|_{p} C _{h}()\|\|_{p},\] (4)

_where \(()\) is a constant determined by the model (e.g., layer number and feature dimension)._

Based on Eq. 3 and Eq. 4, the lower bound of the left term in Eq. 4 is solely determined by the maximum of \(C_{h}\) (denoted as \(C_{h}^{*}\)). To be more intuitive, \(C_{h}^{*}\) is equal to the maximum absolute slope (MAS) of the filter \(h\) (the straightforward proof is in the Appendix B.2).

Therefore, we conclude the core idea for designing our ATProt framework as follows:

_We can enhance the stability of protein representations by decreasing the MAS value of \(h\)._

## 4 Method

Overview.We propose ATProt, an end-to-end framework (illustrated in Fig. 4) to boost PIP from a view of representation stability. Specifically, our model inputs two proteins whose structures can be provided in various sources (e.g., native bound, native unbound, AlphaFold2, ESMFold). ATProt incorporates a protein graph representation encoder and a targeted differentiable regularization scheme to theoretically guarantee the representation stability. Our ATProt framework can be implemented with at least four protein graph encoders, and we provide specific examples of these cases. Lastly, for the PIP task, we apply a cross-attention module to facilitate communication between the protein representations and use a simple linear classifier for final prediction.

Protein representation.We represent a protein as an undirected weighted graph \(=(,)\). Each node \(v_{i}\) representing one residue has a \(d\)-dimensional feature vector \(_{i}^{d}\) (i.e., residue type) and a 3D coordinate \(_{i}^{3}\) (i.e., the \(\)-carbon atom location). Edges \(=\{(i,j)\}\) are constructed with a k-nearest-neighbor (k-NN) graph using Euclidean distance. To distinguish the edges, we follow  to construct the SE(3)-invariant edge features \(\{e_{i,j}:(i,j)\}\).

The goal of ATProt.According to Definition 3.2, we aim to achieve representation stability, which means that the perturbation in the representation caused by protein flexibility is constrained. In

Figure 4: **The framework overview with the BernNet encoder. The whole framework contains the stability-regularized graph encoder for stable protein representations, the cross attention layers for communication and the final binary classifier. ATProt takes in two protein graphs as inputs, and extracts features with the pre-defined graph encoder (BernNet is taken as an example here). The PIP results are obtained after the learned representations have passed through the cross attention module and classifier. The \(_{S}\) loss for stability regularization and classification loss \(_{BCE}\) jointly optimize the model.**

addition, it is also important to ensure the commonly encapsulated SE(3)-invariance, which means that the representation of the protein is not affected by its rotation or translation. Formally, we represent a single protein \(\) consisting of \(N\) residues, with its residue-level feature matrix \(^{N d}\) and residue-level structure \(^{N 3}\). We wish our model \(()\) to ensure the following property.

\[&,=(,); ^{},^{}=(,++), \\ &\|-^{}\| C \|\|,()\\ & SO(3),^{3}, ^{3 N}, C.\] (5)

### Adversarial training regularizations for various encoders

The Fourier transform is a powerful tool in the representation of both structured [50; 26; 49; 28] and unstructured data [9; 21; 31]. Our main focus is on employing spectral graph encoders for representing protein graphs. These encoders adhere to the principle of utilizing a graph spectral filter \(h()=_{k=0}^{K}_{k}b_{k}()\) to learn effective protein representations. Here, \(\{b_{k}\}_{k=0}^{K}\) is the pre-defined filter basis and \(\{_{k}\}_{k=0}^{K}\) is the learnable parameters. For graph convolution, the filter \(h()\) will be applied to the whole Laplacian matrix \(\), which is calculated from the protein structure data. Specifically, to construct \(\), we first apply a Multi-Layer Perceptron (MLP) to reduce the dimensionality of \(\) to 1, and incorporate the results into the edge weight matrix \(\):

\[_{i,j}=_{e}(e_{i,j}),&(i,j)\\ 0,&(i,j)\] (6)

Then the Laplacian matrix \(\) is obtained by \(=-^{-1/2}^{-1/2}\), where \(\) is the degree matrix, i.e., \(=(_{j}_{1,j},...,_{j}_{N,j})\).

The protein graph representation can be defined as:

\[=_{k=0}^{K}_{k}b_{k}()=h(),\] (7)

where the result of \(h()\) represents the graph spectral response.

In this paper, we introduce four types of top graph encoders (i.e., Simple GCN , Chebynet , Low-pass filter , and BernNet ) along with their corresponding stability regularizations.

**Case 4.1**.: _(Simple GCN encoder ) The Simple GCN encoder (SGC) utilizes a spectral filter in the monomial function form:_

\[h()=^{K}.\] (8)

_Since the spectrum \(\) lies in \([-1,1]\), it is clear that the maximum absolute slope (MAS) that \(h()\) can reach is \(K\). Therefore, the stability regularization of the SGC encoder does not involve any loss function. We can directly constrain the size of the order \(K\)._

**Case 4.2**.: _(Chebynet encoder ) The Chebynet encoder utilizes a spectral filter in the polynomial function form:_

\[h()=_{k=0}^{K}_{k}^{k}.\] (9)

_The stability regularization of Chebynet can be implemented by the loss function \(_{S}=_{k=1}^{K}k|^{k}|\)._

**Case 4.3**.: _(Low-pass filter encoder ) The Low-pass filter (LPF) encoder utilizes a spectral filter in the low-pass filter function form:_

\[h()=(1+)^{-1}.\] (10)

_The stability regularization of LPF can be implemented by the loss function \(_{S}=\)._

**Case 4.4**.: _(BernNet encoder ) The BernNet encoder utilizes Bernstein basis, the state-of-the-art graph spectral basis, to construct the graph spectral filter:_

\[h()=_{k=0}^{K}_{k}b_{k}^{K}(^{(l+1)})=_{k=0}^{K} _{k}}K\\ k(2-^{(l+1)})^{K-k}(^{(l+1)})^{k}.\] (11)_According to Theorem 3.1, the BernNet encoder easily satisfies the representation \(C\)-Lipstchiz continuity in Eq. 4 provided that the filter \(h()\) always has a finite MAS. However, in Eq. 11, the analytical relationship between MAS and \(\{_{k}\}_{k=0}^{K}\) is intractable, suggesting the difficulty of constraining \(C\) with a gradient descent manner. Thus, our next goal is to discover a differentiable method for constraining the minimum of \(C\) in the training process._

The stability of the BernNet is still unclear in these four cases, prompting us to investigate its regularization form. To the best of our knowledge, it is the first investigation of the Lipstchiz continuity for Bernstein-based spectral filters.

### Guaranteeing stability of the BernNet encoder

For clarity, we rewrite the Eq. 4 as \(\|-^{}\| C\|\|=C_ {h}()\|\|\|\|\), where, referring to Eq. 3, \(()\) is determined by the model architecture hyperparameters and remains constant. We say the overall model \(\) is of \(C\)-Lipstchiz continuity and the learned spectral filter \(h()=_{k=0}^{K}_{k}b_{k}^{K}()\) is of \(C_{}\)-Lipstchiz continuity. We aim to constrain the minimum of constant \(C_{h}\) (denoted as \(C_{h}^{}\)) with \(\{_{k}\}_{k=0}^{K}\) by discovering the underlying relationship between them. Finally, we propose an auxiliary differentiable regularization of \(\{_{k}\}_{k=0}^{K}\) to constrain \(C_{h}^{*}\) to a controllable bound, for any \(\).

**Theorem 4.1**.: _Given an arbitrary polynomial function \(f()\) on \(\) and suppose its \(K\)-order Bernstein polynomial is denoted as \(h()=_{k=0}^{K}f()K\\ k(2-)^{K-k}^{k}\). For any point pair \(_{1},_{2}\), if there exists a constant \(C_{f}\) for \(|f(_{1})-f(_{2})| C_{f}|_{1}-_{2}|\), then \(h()\) always holds \(C_{f}\)-Lipstchiz continuity for all \(K 1\):_

\[|h(_{1})-h(_{2})| C_{f}|_{1}-_ {2}|.\] (12)

We introduce Theorem 4.1, which describes the stability relationship between the filter \(h()\) and an auxiliary function \(f()\). It tells that with Berstein basis, the MSA of \(h()\) will never exceed that of the auxiliary function \(f()\). Due to \(f()=_{k}\), for all \(k[0,K]\), \(f()\) can be any 2-D curve passing through all points of \(\{(,_{k})\}_{k=0}^{K}\). Therefore, the MAS of \(f()\) is analytically tractable, and we can subsequently provide the bounds for the MAS of \(h()\). We have the following proposition, which is accompanied by a detailed derivation in Appendix B.3.

**Proposition 4.1**.: _Suppose \(h()\) is approximated with Bernstein basis, i.e., \(h()=_{k=0}^{K}_{k}b_{k}^{K}()\). Denoting the MAS (minimum Lipschitz constant) of \(h\) as \(C_{h}^{*}\), it can be upper bounded by_

\[C_{h}^{*}_{i[0,K-1]}K|_{i}-_{i+1}|.\] (13)

To summarize, Bernstein basis applied to our ATProt guarantees a finite MAS, and more importantly, we can further enhance the stability of the BernNet encoder with the following objective:

\[_{}=_{i[0,K-1]}|_{i}-_ {i+1}|.\] (14)

### Protein interface prediction

Given proteins \(_{1},_{2}\) with their initial feature \(_{1},_{2}\) and coordinates \(_{1},_{2}\), ATProt produces their stable representations under respective structure perturbations.

\[_{1}^{M d}=ATProt(_{1},_{1});_{2} ^{M d}=ATProt(_{2},_{2}).\] (15)

We apply a cross-attention layer (shown in Appendix D) to enable communication between proteins and obtain their final representations \(_{1}^{}\),\(_{2}^{}\). Next, we aim to predict whether pairs of inter-protein residues belong to the interface, which involves performing pairwise binary classification. Concretely, for each training pair of proteins, we have a set of \(10N_{I}\) labeled pairs \(\{(([_{1}^{}]_{i},[_{2}^{}]_{i}),y_{i})\}_{ i=1}^{10N_{I}}\), where \(y_{i}\{0,1\}\), \(N^{I}\) is the number of positive residue pairs and \(9N^{I}\) negative ones are downsampled. We take the element-wise product of two residue representations and feed it to another MLP with the Sigmoid function to compute the probability \(p_{i}\). Weighted cross-entropy loss is used for training.

\[_{I}=_{train}|}_{y^{k} _{train}}(_{i=0}^{2N_{f}^{k}}p_{i}^{k}-( 1-y_{i}^{k})(1-p_{i}^{k})).\] (16)Experiments

### Experimental setup

Datasets and processing.We evaluate our method on the complexes from Docking Benchmark 5.5 (DB5.5) , a gold standard dataset with high-quality, and Database of Interacting Protein Structures (DIPS) , which collects 41,876 complexes mined from PDB . DB5.5 only contains 253 complex structures manually curated by domain experts, which cover both native unbound and bound structures. In comparison, DIPS has a significantly larger data size, but it only includes bound structures of proteins. The two datasets are randomly divided into training, validation, and testing sets with the following sizes: 203/25/25 (DB5.5) and 39,937/974/965 (DIPS).

For both datasets, we test using various versions of protein structures as inputs for PIP. To accomplish this, we prepared three versions of the testing set for DB5.5, including native unbound structures, structures produced by AlphaFold2, and structures produced by ESMFold. We use Native-Bound, Native-Unbound, ESMFold, AlphaFold2 to denote these four settings respectively.

As for DIPS, since it does not have native unbound structures, we only used ESMFold to prepare unbound structure inputs for its testing set. AlphaFold2 is not considered due to its high computational cost for the entire testing set of DIPS.

Baselines.We compare our ATProt method with state-of-the-art conventional machine learning method BIPSPI , the CNN-based methods Siamese Atomic Surfacelet Network (SASNet) , Diffusion-Convolutional Neural Networks (DCNN) , differentiable molecular surface interaction fingerprinting (dMaSIF) , and a set of GNN-based methods Deep Tensor Neural Networks (DTNN) , and NEA .

Setup and metrics.We consider three experimental setups: (1) performing training and testing both on the DB5.5; (2) performing training and testing both on the DIPS; and (3) performing pre-training on the DIPS, fine-tuning on the DB5.5 and testing on the DB5.5. For our proposed ATProt method, we consider graph encoders of Simple GCN, Chebynet, Low-pass filter and BernNet (denoted as ATProt-SGC, ATProt-Cheby, ATProt-LPF, and ATProt-Bern, respectively).

For each complex in the testing set, assuming that the two proteins have \(M\) and \(N\) residues, respectively, we test all its \(M N\) binary classification samples and calculate the Area Under the ROC Curve (AUC) value. Following , we report the median AUC score (MedAUC) across all complexes as the final evaluation metric.

Results.Table 1, 2, and Table 3 (shown in Appendix) show the model performance for PIP. We find that our method is competitive and outperforms the majority of baseline methods with native bound structures. Under this Native-Bound setting, although ATProt is slightly less effective than BIPSPI, it demonstrates the ability to learn sufficiently powerful protein representations (especially with BernNet and Chebynet). Notably, all the baselines fail significantly when using non-bound structures

   Methods & Native-Bound & Native-Unbound & ESMFold & AlphaFold2 \\  BIPSPI  & **0.937 (0.008)** & 0.911 (0.017) & 0.896 (0.008) & 0.887 (0.013) \\ SASNet  & 0.902 (0.007) & 0.876 (0.017) & 0.887 (0.025) & 0.881 (0.020) \\ dMaSIF  & 0.928 (0.005) & 0.912 (0.009) & 0.906 (0.003) & 0.892 (0.012) \\ DTNN  & 0.912 (0.005) & 0.886 (0.007) & 0.883 (0.010) & 0.878 (0.021) \\ NEA  & 0.916 (0.015) & 0.895 (0.009) & 0.902 (0.010) & 0.883 (0.012) \\ 
**ATProt-SGC** & 0.925 (0.015) & 0.918 (0.004) & 0.909 (0.012) & 0.924 (0.014) \\
**ATProt-Cheby** & 0.928 (0.017) & 0.922 (0.007) & 0.922 (0.005) & 0.924 (0.011) \\
**ATProt-LPF** & 0.915 (0.017) & 0.919 (0.019) & 0.911 (0.009) & 0.911 (0.010) \\
**ATProt-Bern** & 0.932 (0.017) & **0.928 (0.014)** & **0.929 (0.014)** & **0.925 (0.011)** \\   ATProt-Bern w/o SR & 0.934 (0.009) & 0.901 (0.011) & 0.897 (0.010) & 0.901 (0.012) \\   

Table 1: **Training and testing on the DB5.5.** Mean and standard deviation values of the MedAUC scores of all baselines, computed from three random seeds. The best performance is in **bold** and the second best one is underlined. ‘SR’ means the proposed stable regularization \(_{S}\).

[MISSING_PAGE_FAIL:9]