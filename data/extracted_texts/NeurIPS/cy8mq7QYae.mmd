# CharXiv: Charting Gaps in Realistic Chart Understanding in Multimodal LLMs

Zirui Wang Mengzhou Xia Luxi He Howard Chen Yitao Liu Richard Zhu Kaiqu Liang Xindi Wu Haotian Liu Sadhika Malladi Alexis Chevalier Sanjeev Arora Danqi Chen Princeton Language and Intelligence (PLI), Princeton University

University of Wisconsin, Madison

The University of Hong Kong

{zwcolin, mengzhou, luxihe, howardchen}@cs.princeton.edu

https://charxiv.github.io/

###### Abstract

Chart understanding plays a pivotal role when applying Multimodal Large Language Models (MLLMs) to real-world tasks such as analyzing scientific papers or financial reports. However, existing datasets often focus on oversimplified and homogeneous charts with template-based questions, leading to an overly optimistic measure of progress. We demonstrate that although open-source models can appear to outperform strong proprietary models on these benchmarks, a simple stress test with slightly different charts or questions can deteriorate performance by up to \(34.5\%\). In this work, we propose CharXiv, a comprehensive evaluation suite involving 2,323 natural, challenging, and diverse charts from arXiv papers. CharXiv includes two types of questions: 1) _descriptive_ questions about examining basic chart elements and 2) _reasoning_ questions that require synthesizing information across complex visual elements in the chart. To ensure quality, all charts and questions are handpicked, curated, and verified by human experts. Our results reveal a substantial, previously underestimated gap between the reasoning skills of the strongest proprietary model (i.e., GPT-4o), which achieves \(47.1\%\) accuracy, and the strongest open-source model (i.e., InternVL Chat V1.5), which achieves \(29.2\%\). All models lag far behind human performance of \(80.5\%\), underscoring weaknesses in the chart understanding capabilities of existing MLLMs. We hope that CharXiv facilitates future research on MLLM chart understanding by providing a more realistic and faithful measure of progress.

## 1 Introduction

Multimodal Large Language Models (MLLMs)  are highly versatile and effective for a wide range of real-world applications . Within these applications, chart understanding is a highly desired capability as charts are ubiquitous in scientific papers, financial reports, and news articles. It also poses unique challenges where models need to perform complex reasoning over numerical data, textual labels, and complex visual elements to answer difficult questions (see Fig. 1), thus making chart understanding a suitable measure of progress for MLLMs. Many benchmarks in the popular MathVista evaluation suite  are designed to test chart understanding. However, these benchmarks lack diversity in both the types and complexity of the charts and the often template-based questions (SS2.1). For example, FigureQA  and DVQA  rely on procedurally generated question templates. While ChartQA  includes a mixture of handwritten and machine-generated questions, the charts lack visual diversity due to the homogeneous appearance of the charts from a limited number of sources. Regardless, many proprietary models  and open-source models  are evaluatedon these datasets.1 These narrow evaluations create the appearance that the open-source models outperform proprietary ones2, despite evidence to the contrary: we designed simple stress tests (SS2.2) in which we find that open-source models lag far behind proprietary ones in their robustness to small visual or textual changes. For example, the accuracy of SPHINX V2 dropped from 63.2% to 28.6% with a 34.5% gap when questions are slightly modified with respect to the same set of charts.

We introduce CharXiv, a comprehensive evaluation suite for complex understanding of natural, challenging, and diverse charts (SS3) to address the above issue. CharXiv consists of 2,323 real-world charts handpicked from scientific papers spanning 8 major subjects published on arXiv (SS3.1). We explicitly disentangle visual understanding and reasoning by designing two types of questions (SS3.2): (1) _descriptive_ questions, requiring understanding basic chart information such as the title, labels, and ticks; (2) _reasoning_ questions, requiring comparisons, approximations, and fine-grained analysis. CharXiv is an especially high-quality dataset where all questions are _manually_ curated by human experts, and all ground-truth answers are validated by hand. To answer both types of questions, the model only needs to understand the visual contents of the chart without advanced domain-specific knowledge and contextual information. Evaluating an MLLM on CharXiv is straightforward, because we impose a short answer format that is amenable to LLM-based automatic grading.

We extensively evaluate \(13\) open-source models and \(11\) proprietary models (SS4.1) and identify a large disparity between the strongest open-source and proprietary models (SS4.2): InternVL Chat V1.5 correctly answers only \(29.2\)% of the reasoning questions and \(58.5\)% of the descriptive ones, whereas GPT-4o correctly answers \(47.1\)% of the reasoning questions and \(84.5\)% of the descriptive ones (Tab. 3). As shown in Fig. 2, the performance gap in the reasoning questions of \(17.9\%\) is significantly larger than the gap identified in prior works . Further, both types of models lag far behind the human performance of \(80.5\)% on the reasoning questions and \(92.1\)% on the descriptive ones. Fine-grained analysis of model performance (SS4.3) shows several insights owing to the design of CharXiv. In particular, we characterize: (1) differences in reasoning and descriptive capabilities, exploring when one skill reinforces the other; (2) what types of tasks and charts are difficult for existing MLLMs; (3) how different MLLMs respond to unanswerable questions. Overall, we hope that CharXiv enables a thorough, multi-faceted evaluation of chart understanding in MLLMs.

Figure 1: Example chart (left), descriptive questions (top-right) and reasoning questions (bottom-right) in CharXiv where open-source models even fail in basic descriptive questions. Moreover, all models struggle with correctly answering the reasoning question.

## 2 Existing Benchmarks Overestimate Chart Understanding Capabilities

### Related Works

Existing benchmarks such as FigureQA , DVQA , PlotQA  do not fully capture the complexity and diversity of real-world charts due to their synthetic nature, while charts in ChartQA  lack visual diversity. More recent benchmarks such as MMC , ChartBench  and ChartX3 also contain issues with the source or diversity of the charts (_e.g.,_ ChartX, MMC) and the types of questions (_e.g.,_ MMC, ChartBench). We provide a summary of existing benchmarks' design choices in Tab. 1 and a detailed review below. We provide a more detailed related works on Multimodal Large Language Models and More MLLM benchmarks in App. B.

**Chart source.** FigureQA, DVQA and PlotQA use plotting software to synthesize charts restricted to very few predefined chart types with stylistically similar elements (see Figs. 8(a), 8(b) and 8(c)). ChartQA sources charts from only 4 websites, each of which lacks visual diversity (see Fig. 8(d)). One such website also served as the primary source of charts for reasoning questions in MMC. On the other hand, ChartX provides fixed instructions to GPT-4 to write code to procedurally generate predefined types of charts and settings in bulk. All of these approaches yield artificial charts belonging to a narrow distribution.

**Question types.** Existing benchmarks lack variation in their questions: FigureQA, DVQA and PlotQA use a fixed template to generate QA pairs, while ChartBench adopts an automatic QA generation pipeline according to 4 predefined tasks. However, similar to MMMU , more complex reasoning questions from MMC cannot be solved from the charts alone and require external domain-specific knowledge (e.g., mapping acronyms in the legend to particular algorithms).

**Answer & validation.** FigureQA and ChartBench both evaluate model performance based only on _yes/no_ questions. Evaluating models on binary answers does not faithfully reflect their performance in the natural use case of general free-form question answering .

### Open-Source MLLMs Are Sensitive to Perturbations

Many open-source models have adapted the training sets of existing benchmarks [36; 35; 64] for visual instruction tuning  and show promising performance in their respective evaluation sets. However, due to the aforementioned issues with the diversity of these benchmarks, the evaluation

    &  & Answer \\   & **Real** & **Real** & **Vis.** & **Temp.** & **Free** & **Kawl.** & **Open** \\  & **Data** & **Chart** & **Div.** & **Based** & **Form** & **Free** & **Vocab.** \\  _QA-Based_ & & & & & & \\  FigureQA  & ✗ & ✗ & ✗ & ✓ & ✗ & ✓ & ✗ \\ DVQA  & ✗ & ✗ & ✗ & ✓ & ✗ & ✓ & ✓ \\ PlotQA  & ✓ & ✗ & ✗ & ✓ & ✗ & ✓ & ✓ \\ ChartQA  & ✓ & ✓ & ✗ & ✗ & ✓ & ✓ & ✓ \\ ChartBench  & ✓ & ✓ & ✓ & ✓ & ✗ & ✓ & ✗ \\  _Multi-Task_ & & & & & & & \\ MMC  & ✓ & ✓ & ✗ & ✗ & ✓ & ✗ & ✓ \\ ChartX  & ✗ & ✗ & ✓ & ✗ & ✓ & ✓ & ✓ \\  CharXiv & ✓ & ✓ & ✓ & ✓ & ✓ & ✓ & ✓ \\   

Table 1: Design choice of chart understanding benchmarks. We use the following shorthand: Vis. Div.=visual diversity, Temp.=template, Knwl.=knowledge, and Vocab.=vocabulary. Cells marked with “✓” indicate _mixed attributes_ (e.g., real and synthetic data; real and synthetic chart).

Figure 2: Model performance comparison on reasoning questions from CharXiv v.s. questions from existing benchmarks. As indicated by the red and blue bars respectively, many open-source models surpass proprietary model performance on the \(174\) sample questions from existing benchmarks (subsets of DVQA, FigureQA and ChartQA from the _testimini_ split of MathVista) yet fail consistently on the \(1000\) reasoning questions from the validation split of CharXiv.

data is too similar to the training data. As a result, evaluation scores often do not accurately reflect the general chart understanding capabilities of MLLMs. In particular, we demonstrate below that _simple_ modifications in the evaluation components lead to _drastic_ changes in model performance.

**Models.** We selected open-source models that are known to be trained on the training set of DVQA and ChartQA: Mini-Gemini (MGM) , InternVL-XComposer2 (IXC2) , InternVL-XComposer2 4KHD (IXC2 4KHD) , InternVL-Chat V1.5 , SPHINX V2 , LLaVA 1.6 , and IDEFICS 2 . We compare their performance with proprietary models .

**Evaluation set.** We extracted subsets of DVQA, FigureQA, and ChartQA from MathVista. This yields 174 samples, and we refer to it as the _original set_. To test the robustness of the models mentioned above, we created two modified versions of the original set: the modified-question set (see App. S) and the modified-chart set (see App. T). In the modified-question set, we retain the original chart, but write novel questions that deviate from the predefined templates . In the modified-chart set, we alter the charts to those from arXiv with similar visual complexity that can be asked with the same types of questions. We manually annotate all questions and answers in both the modified-question and the modified-chart set. As in the original set, we maintain an equal number of yes and no answers in the original set to prevent models from achieving artificially high scores by simply outputting one response more often than the other, and adopt the same evaluation protocol as in MathVista.

**Results.** As plotted in Fig. 3, all proprietary models remain close to the diagonal line, indicating good generalization in both modified-question and modified-chart scenarios. In contrast, most open-source models exhibit significant performance degradation in both settings, indicating poor generalization. We observe the most pronounced performance drop in SPHINX V2 in the modified-question set, where performance dropped by 34.5%, from 63.2% in the original set to 28.7% in the modified-question set. Our findings demonstrate that design strategies in existing benchmarks lead to an _overestimation_ of chart understanding capabilities for open-source models. We hypothesize that the training and evaluation datasets are too similar, so models appear to generalize well despite not being robust to simple modifications. In the next section, we introduce CharXiv, which features a more natural, challenging, and diverse evaluation of real-world charts.

## 3 CharXiv: A Real-World and Challenging Chart Understanding Benchmark

CharXiv is a comprehensive and challenging chart understanding benchmark sourced solely from real-world charts. We select diverse, naturally occurring, and complex figures from arXiv preprints, and manually construct descriptive and reasoning questions that require intensive visual and numerical analysis. CharXiv consists of 2,323 charts paired with more than 10K questions--we randomly sample 1,000 charts as the validation set and use the rest as the test set.4 In the following sections, we describe how we select charts (SS3.1), construct questions (SS3.2), and validate model responses (SS3.3).

Figure 3: Open-source models generalize poorly to modified examples (measured by accuracy). Left: original set against modified-_question_ set. Right: original set against modified-_chart_ set.

### Chart Curation

**Figure source.** We downloaded all arXiv preprints on eight academic subjects from January 2020 to September 2023 (Fig. 4) and extracted figures from the source files. All figures were re-rendered into high-resolution JPEG format, with the longer side of each figure resized to \(1024\)px.

**Chart selection.** We define a chart as _any figure that visually illustrates data_. Most figures in arXiv source files are diagrams, illustrations, and natural images, _not_ charts. To identify charts and promote visual diversity, we apply a four-step selection pipeline. First, we utilize a pretrained SigLIP visual encoder  to identify candidate figures that exhibit a cosine similarity of at least \(0.65\) with the average image embedding of existing charts from MathVista . We choose this target similarity to balance identifying charts and ensuring good coverage of the visually diverse distribution. Second, we recruit experienced graduate students to manually select charts from the candidate set. Concretely, we randomly sample 750 candidate figures from the pre-filtered set for each subject and year, and present 10 figures at a time to the annotators, asking them to select a single figure that is a chart and looks different from previously selected datapoints (see App. R.1 for details). In the third step, we remove the charts that exhibit large (\( 0.95\)) pairwise cosine similarities with the other candidates. Finally, we remove the charts that are not clearly labeled or appear blurry. At the end of this four-step pipeline, we have 2,323 charts in total.

We provide details of the chart categories, years, and number of subplots in Fig. 4, size information in Tab. 2, and a collage of sampled charts in Fig. 8(e). Notably, the charts in CharXiv exhibit far greater compositional and stylistic complexity compared to those in existing datasets. A single chart often combines elements or subplots featuring multiple chart types (e.g., lines and bars in one plot). To aggregate statistics on chart types, we first query GPT-4o to generate potential chart types for each chart. Human annotators then review and refine this list, assigning a primary chart type based on the chart's most salient features. We provide chart type statistics in Fig. 5.

### Question Construction

We construct two types of questions: _descriptive_ and _reasoning_. Descriptive questions assess models' capability in extracting and aggregating basic information from charts, and reasoning questions evaluate a model's ability to perform complex visual reasoning.

**Descriptive questions.** We designed a total of 19 templates for descriptive questions that require (1) identifying basic information, such as the title, axis labels, legend labels, labeled ticks, or (2) aggregating chart information to count ticks, recognize data patterns, and enumerate labels. These questions are broadly categorized into five groups: information extraction, enumeration, pattern recognition, counting, and compositionality (see App. O.1 for details). Although descriptive questions are intended to be easier than reasoning questions, they can still pose challenges due to the complexity of the charts. For example, answering descriptive questions about charts with multiple subplots requires the model to first identify the relevant subplot5 (see Apps. U.1, U.7 and U.10). If basic elements such as the legend, axis, and title are shared across multiple subplots, the model must then also grasp the relationships among the subplots to extract the correct information (see Apps. U.3 and U.23). We pair each chart with four descriptive questions and one of them is intentionally

Figure 4: Metadata breakdown of charts, descriptive questions, and reasoning questions in CharXiv.

designed to be _unanswerable_6, where the requested information does not exist or is not applicable to the subplot in the chart. We provide the distribution of specific questions in Fig. 4, aggregated statistics of questions and answers in Tab. 2, and a screenshot of the labeling process in App. R.2.

Reasoning questionsWe _manually_ craft one reasoning question for each chart to evaluate the models' ability to perform visual and numerical reasoning. To ensure data quality, we recruit graduate students as annotators. Annotators are presented with a chart and 10 sample reasoning QA pairs generated by GPT-4V. Based on the diversity and practicality of the sample questions, annotators choose or modify one of the samples, or they create their own question for each chart. The resulting question must have a definite and unambiguous answer and must strictly adhere to one of the following four types:

* _text-in-chart_: The answer is a piece of text found in the chart (see Apps. V.1, V.2 and V.6).
* _text-in-general_: The answer is an easily verifiable phrase that is not necessarily in the chart (see Apps. V.3, V.4 and V.30).
* _number-in-chart_: The answer is a numerical value written on the chart (see Apps. V.7, V.9 and V.12).
* _number-in-general_: The answer requires an exact numerical value, not necessarily found in the chart, to a specified precision (see Apps. V.5, V.14 and V.15).

One notable feature of our reasoning questions is that they are designed to require _only_ visual and numerical reasoning, without the need for advanced domain-specific knowledge or access to captions and referencing paragraphs. This sets CharXiv apart from MathVista , MMMU , and arXiv-based QA datasets , which often require additional expert knowledge. Although our curation process requires significant human effort to craft question-answer pairs, we believe that it promotes originality, diversity, accuracy, and answerability. The distribution for both QA sources and answer types is shown in Fig. 4 and the aggregated statistics of the questions and answers are shown in Tab. 2. We provide a screenshot of the annotation interface in App. R.3, and the response generation instructions for each type of answer in App. P.1.

### Evaluation Metrics

CharXiv is amenable to automatic grading due to the unambiguous nature of the answers. Considering the fact that many charts contain Greek symbols and math notation that can be typed in different ways (_e.g.,_\(\) and $$; T^a_b and T_b^a), we opt out of exact match and instead use GPT-4o  to extract the answer, compare with the human reference for consistency, and assign _binary_ scores based on the correctness. This procedure can be considered an LLM judge based on human reference.

  
**Statistics** & **Value** \\ 
**Charts** & \(2,323\) \\ Total Charts & \(2,323\) \\ Total Subjects/Years & \(8/4\) \\ Val.Test & \(1,000/1,323\) \\ Average size (px) & \(996 702\) \\ Maximum size (px) & \(1024 1024\) \\ 
**Descriptive Questions** & \(9,292\) \\
**\# questions** & \(19\) \\
**\# unique questions** & \(19\) \\
**\# answer** & - \# unique. tokens & \(3,723\) \\
**- maximum length** & \(138\) \\
**- average length** & \(2.93\) \\ 
**Reasoning Questions** & \(2,323\) \\
**\# questions** & \(2,323\) \\
**\# unique questions** & \(2,323\) \\
**\# question** & - \# unique tokens & \(5,114\) \\
**- maximum length** & \(144\) \\
**- average length** & \(22.56\) \\
**\# answer** & - \# unique tokens & \(2,177\) \\
**- maximum length** & \(38\) \\
**- average length** & \(2.8\) \\   

Table 2: CharXiv dataset statistics. Unique tokens and question & answer lengths are calculated based on the GPT-4o tokenizer.

Figure 5: **Statistics of chart types.** CharXiv captures a long tail of chart categories in-the-wild.

Similar GPT-assisted evaluations have become commonplace in many established benchmarks [60; 92; 18]. Grading instructions for descriptive and reasoning questions are provided in App. O.2 and App. P.2 respectively. To verify the effectiveness and fairness of the judge, we also performed human annotation in which we graded a total of 400 descriptive and reasoning questions in 4 models. Grades from GPT-4o and humans on models' responses match 98.5% of the time. We provide detailed metrics in Tab. 18 and Tab. 19.

## 4 Experiments

### Experimental Setup

**Models.** We evaluate a diverse set of general-purpose multimodal large language models (MLLMs) that can (1) process input resolution greater than or equal to \(448 448\) and (2) achieve a score of at least 36 on the _testmini_ set of MathVista . For open-source models, we test: InternVL Chat V1.5 , InternLM-XComposer2-4KHD (IXC2 4KHD) , InternLM-XComposer2 (IXC2) , LLaVA 1.6 Yi 34B , LLaVA 1.6 Mistral 7B , DeepSeeVL , MoAI , IDEFICS 2 , IDEFICS 2 Chatty , SPHINX V2 , Mini-Gemini (MGM) HD Yi 34B , Mini-Gemini (MGM) HD LLaMA3 8B , and MiniCPM-V2  (See more model details in Tab. 16). We also evaluate the following proprietary models: GPT-4o , GPT-4V , Claude-3 Opus , Claude 3 Sonnet , Claude 3 Haiku , Reka Core , Reka Flash , Reka Edge , Gemini 1.0 Pro , Qwen VL Plus , and Qwen VL Max . For all models, we provide generation configurations in Tab. 15.

**Baselines.** We provide a text-only baseline, denoted as Random (GPT-4o), where we prompt GPT-4o to reasonably guess the answer without seeing the charts (see the prompt in App. Q). This accounts for the effect of using common sense or shallow cues in textual queries to correctly guess the answer. We also recruit in-house human participants and report their performance (_i.e.,_ Human) on CharXiv. Notably, we ensure that the participants see the exact same questions and instructions as the models and that their responses are evaluated in the same way as the models' responses. This approach allows us to fairly compare the performance gap between humans and models.

### Experimental Results

We provide quantitative results on the validation set for all models in Tab. 37. Results on the test set are available in Tab. 5. To better understand where models fail, we select a set of representative models [2; 5; 73; 12; 49; 41] and present examples of failure cases for 30 descriptive questions in App. U and 30 reasoning questions in App. V. The latest results are in our leaderboard.

**All models struggle with reasoning questions.** As shown in Tab. 3, the top-performing model, GPT-4o, only correctly answers \(47.1\)% of the reasoning questions, exhibiting a \(33.4\%\) gap to the human performance of \(80.5\%\). Moreover, the strongest open-source model, InternVL Chat V1.5, only correctly answers \(29.2\%\) of the reasoning questions, highlighting a substantial gap between the leading proprietary and open-source models. Notably, none of the other open-source models can correctly answer more than \(25\%\) of the reasoning questions, indicating marked weaknesses in handling the diverse and challenging chart reasoning questions in CharXiv despite achieving decent performance in existing benchmarks [35; 36; 64; 60] (_e.g.,_ see Fig. 2).

**Open-source models still struggle with descriptive questions.** The leading proprietary model, GPT-4o, exhibits strong capabilities in answering descriptive questions, lagging just \(7.65\%\) behind human performance. However, similar to our findings on reasoning questions, the top-performing open source model, InternVL Chat V1.5, exhibits a \(25.95\%\) drop in performance compared to GPT-4o. Overall, the performance of open-source models on descriptive questions remains very low, with most models failing to correctly answer more than \(50\)% of questions.

### Analysis

**Descriptive skills are a prerequisite for reasoning.** We find that models with strong reasoning capabilities exhibit strong descriptive capabilities, but the reverse is _not_ guaranteed (e.g., see Gemini 1.0 Pro, IDEFICS 2 Chatty and DeepSeek VL in Tab. 3). Manual inspection of models' answers to reasoning questions reveals that some models [73; 49; 7; 42] leverage zero-shot Chain-of-Thought (CoT) reasoning [84; 97] to answer the reasoning questions. However, such CoT may not always be helpful, especially when models cannot accurately describe the chart, as we show in Apps. U.13, U.28, V.1 and V.17. Quantitatively, we show in App. I that longer responses (_e.g.,_ those potentially containing more CoT traces) can _negatively_ impact models' performance on reasoning questions. This issue is especially clear in models with low accuracy on descriptive questions, such as MoAI and Qwen VL Plus, which answer \(28.70\)% and \(28.93\)% of descriptive questions correctly. In contrast, models with higher accuracy on descriptive questions, such as Mini-Gemini HD Yi 34B and Reka Flash, which achieve \(52.68\)% and \(56.45\)%, respectively, show improved performance on reasoning questions when generating lengthy responses. Nevertheless, the vast majority of models exhibit performance uncorrelated with response length. Thus, we hypothesize that a model must have a strong basic understanding in order to generate helpful multimodal CoT for reasoning.

**Models struggle with compositional tasks that are easy for humans.** We find that the descriptive task that most strongly differentiates the capabilities of the leading open-source, the top-performing proprietary model, and humans is to count the number of labeled ticks on the x- and y-axes (see App. U.28), on which they achieve \(92.86\)%, \(59.82\)% and \(5.80\)% accuracy respectively. Although counting is easy for humans, this particular task causes 20 out of 24 models to achieve an accuracy below \(10\)% (our random baseline achieves \(5.35\)%). While we do not specifically measure how close each model's responses are to the ground truth, a near-random performance pinpoints the weakness of MLLMs in solving compositional and novel chart understanding tasks.

**Weak models cannot identify unanswerable questions.** \(\) is the first work to introduce unanswerable questions in chart understanding. As discussed in SS3.2, \(25\)% of descriptive questions are designed to be unanswerable, where the requested information does not exist or is not applicable to the target subplot in the chart (see Apps. U.2, U.4, U.6, U.12, U.14, U.16, U.18, U.20, U.22, U.24 and U.26). We measure how often models can correctly identify and suitably respond to unanswerable

    \\    &  &  \\ 
**Model** & **All** & **Text in** & **Num. in** & **Num. in** & **All** & **Info.** & **Enum.** & **Patt.** & **Cntg.** & **Cntg.** & **Comp.** \\  & **Chart** & **General** & **Chart** & **General** & **All** & **Extr.** & & **Rec.** & & **Rec.** & \\   &  &  &  &  &  \\  Human & **80.50** & **77.27** & **77.78** & **84.91** & **83.41** & **92.10** & **91.40** & **91.20** & **95.63** & **93.38** & **92.86** \\ Random (GPT-40)  & 10.80 & 4.32 & 39.39 & 5.60 & 16.16 & 19.85 & 21.65 & 16.71 & 23.80 & 25.70 & 5.36 \\   &  \\  GPT-4o  & **47.10** & **50.00** & **61.62** & **47.84** & **34.50** & **84.45** & **82.44** & **89.18** & **90.17** & **85.50** & **59.82** \\ GPT-4V  & 32.10 & 38.18 & 57.58 & 37.92 & 25.33 & 79.92 & 78.29 & 85.79 & 88.21 & 80.92 & 41.07 \\ Claude 3 Bouncet  & 32.20 & 31.99 & 50.51 & 31.47 & 26.20 & 73.65 & 75.74 & 81.92 & 76.64 & 72.26 & 8.48 \\ Claude 3 Haiku  & 31.80 & 29.77 & 45.45 & 34.48 & 22.07 & 65.08 & 69.87 & 69.98 & 64.85 & 61.83 & 8.04 \\ Claude 3 Opus  & 30.20 & 26.36 & 50.51 & 33.62 & 25.33 & 71.55 & 75.52 & 76.39 & 73.58 & 70.48 & 26.79 \\ Reka Core  & 28.90 & 27.50 & 41.41 & 28.45 & 26.64 & 55.60 & 58.90 & 50.52 & 65.72 & 71.25 & 10.71 \\ Reka Flash  & 26.60 & 26.59 & 39.39 & 30.60 & 17.03 & 56.45 & 61.39 & 48.59 & 69.87 & 72.52 & 7.14 \\ Owen VL Marx  & 24.70 & 26.14 & 41.41 & 24.57 & 14.85 & 41.48 & 50.42 & 28.41 & 53.71 & 51.15 & 4.46 \\ Reka Edge  & 23.50 & 20.23 & 32.32 & 30.60 & 18.78 & 33.65 & 56.65 & 28.49 & 34.72 & 52.16 & 4.91 \\ Gemini 1.0 Pro  & 22.80 & 20.91 & 48.48 & 18.10 & 20.09 & 54.37 & 67.97 & 39.23 & 60.48 & 62.60 & 8.93 \\ Owen VL Plus  & 16.00 & 15.45 & 45.45 & 12.07 & 8.30 & 28.93 & 33.33 & 17.92 & 32.10 & 56.23 & 2.23 \\   &  \\   &  \\  InterVL Chat V1.5  & **29.20** & **30.00** & **45.45** & **32.33** & 17.47 & **58.50** & **69.63** & 52.95 & 53.06 & **64.63** & 5.80 \\ MGMM HD Yi 34B  & 25.00 & 26.59 & 43.43 & 27.16 & 11.79 & 52.63 & 53.66 & 55.04 & **65.50** & 53.94 & 2.23 \\ IC24R  & 25.00 & 23.86 & 43.43 & 29.11 & 48.85 & 64.56 & 61.50 & 54.08 & 51.53 & 59.80 & 6.70 \\ LLAvA 1.6 371  & 22.50 & 20.45 & 37.37 & 23.71 & 18.78 & 51.05 & 46.38 & **63.44** & 56.11 & 51.91 & 5.80 \\ MOH HD LAMA3 BB  & 19.00 & 19.77 & 36.36 & 21.12 & 7.86 & 44.42 & 94.91 & 39.23 & 51.09 & 55.98 & 1.79 \\ IXC2*  & 18.70 & 16.14 & 38.38 & 21.98 & 11.79 & 38.75 & 34.10 & 43.58 & 46.72 & 52.93 & 5.80 \\ MinCHV-Y2  & 18.50 & 17.95 & 33.33 & 19.40 & 12.23 & 35.77 & 39.74 & 36.56 & 26.42 & 44.53 & 5.36 \\ DEFICES 2  & 18.20 & 15.45 & 35.35 & 17.24 & 17.03 & 32.77 & 36.12 & 27.28 & 40.83 & 43.26 & 3.12 \\ IDEFICS 2 Charty  & 17.80 & 15.45 & 34.34 & 19.83 & 13.10 & 41.45 & 35.48 & 54.56 & 45.63 & 44.47 & 6.70 \\ MoAI*  & 17.50 & 9.32 & 36.36 & 21.12 & **21.40** & 28.70 & 31.20 & 21.23 & 39.96 & 40.46 & 5.59 \\ DeepSeek VL  & 17.10 & 16.36 & 32.32 & 19.83 & 9.17 & 45.80 & 49.11 & 45.20 & 42.

questions in Fig. 6(a). Interestingly, the models that achieve an accuracy below \(80\)% on unanswerable questions each exhibit idiosyncratic patterns of failure. For example, IDEFICS 2 Chatty incorrectly responds to nearly \(90\)% unanswerable questions about the title, x- and y-axis labels, yet correctly identifies more than \(90\)% of unanswerable questions about intersections of lines and the presence of the legend. On the other hand, IXC 2 correctly respond to \(80\)% questions about names of title, x- and y-axis labels that are unanswerable, yet fails to identify unanswerable cases for the difference in tick values when ticks are categorical or the difference is not constant.

In addition, we evaluate models' performance on descriptive questions _without_ unanswerable questions in Tab. 12, and find that the overall performance for the majority of the proprietary models appears to benefit from the exclusion of unanswerable questions, while most open-source models exhibit degraded overall performance when unanswerable questions are excluded.

Descriptive capabilities degrade with more subplots.CharXiv is the first work to aggregate detailed statistics on the number of subplots in each chart, so we are able to conduct a fine-grained analysis of how the performance of proprietary models and open-source models changes with the number of subplots in the chart. As shown in Figure 6(b), a representative set of open-source and proprietary models struggle to answer descriptive questions about charts with more subplots. With 6+ subplots, the deterioration is \(30\)%-\(50\)% for open-source models and only \(10\)%-\(30\)% for proprietary models. This indicates that all MLLMs are weaker in handling descriptive queries for charts with more subplots, and such performance deterioration is exacerbated in open-source models. We hypothesize that this is because open-source models are instruction-tuned on chart datasets that do not contain subplots, such as DVQA and ChartQA. On the other hand, there appears to be no clear correlation between reasoning capabilities and the number of subplots.

Model performance varies among different subjects.Although the questions in CharXiv are designed to be answerable without domain-specific knowledge, we measure the models' performance on individual subjects (see Fig. 4). All models show consistently weaker descriptive capabilities on physics-related charts and stronger performance on charts containing electrical engineering and systems science, quantitative finance, and economic data (see Tab. 6). On the other hand, models exhibit idiosyncratic reasoning capabilities over different subjects, demonstrating no clear pattern (see Tab. 7). Interestingly, the strongest open-source model, InternVL Chat V1.5, matches GPT-4V in correctly answering \(39.26\)% of the reasoning questions from charts in the math domain, but it significantly lags behind in other domains, exhibiting gaps greater than \(20\%\) in the physics and electrical engineering and systems science domains. These patterns suggest that (1) charts from certain domains are inherently difficult for models to describe and (2) there exist unique skills that are required to perform complex reasoning over charts from different domains.

Model performance varies across chart types.Our analysis of model performance across different chart types is presented in Tab. 13 for descriptive questions and Tab. 14 for reasoning questions. For descriptive questions, both proprietary and open-source models generally underperform on less

Figure 6: Analysis on unanswerable questions (a) and charts with subplots (b).

common chart types, such as contour plots and heatmaps (see Fig. 5). GPT-4o, the best-performing model, demonstrates a noteworthy exception to this trend. While its advantage over GPT-4V is modest for common chart types (line, scatter, and bar charts), it substantially outperforms other models on less common chart types, such as heatmaps and contour plots, suggesting better generalization across diverse chart types. However, all models, including GPT-4o, struggle with the rarest chart category ("others"), indicating the need for more comprehensive dataset coverage. The pattern shifts notably for reasoning questions. The performance gap between GPT-4o and other models shows little correlation with either chart type or performance gap on descriptive questions. Most strikingly, while GPT-4o and GPT-4V show only a 5-point gap on descriptive questions for bar charts, their performance diverges dramatically on reasoning questions, with GPT-4o (45.87) outperforming GPT-4V (22.94) by more than 20 points. Other models consistently underperform on reasoning questions involving bar charts and box plots. We hypothesize that these difficulties stem from challenges in perceiving and estimating values from unannotated visual elements and performing comparative analyses (e.g., sorting, identifying extrema). Further investigation of these specific challenges remains an important direction for future research.

## 5 Conclusion

Chart understanding is a crucial visual reasoning skill for MLLMs, but our simple stress test reveals that design flaws in existing benchmarks have led to an overestimation of chart understanding capabilities (see SS2.2). We introduce CharXiv, a natural, challenging benchmark that pairs charts collected from arXiv papers with human-curated questions and answers. Our results expose clear performance gaps across human, proprietary models and open-source models, and we discuss the broader impacts of our findings in SS5.

**Limitations.** Despite the fact that CharXiv does not require advanced domain-specific knowledge, human accuracy is only \(80.5\%\) and \(92.1\%\) in reasoning and descriptive questions. We hypothesize that this could be due to issues with automated grading or mistakes by participants in the human evaluation study. However, given the large performance gap between existing MLLMs and humans, we believe that CharXiv is an insightful measurement of chart understanding capabilities. We also note that evaluation benchmarks comprising entirely of examples curated by human experts are expensive to construct and difficult to update and extend. However, as we noted in SS2, automatically generated benchmarks often overestimate the capabilities of existing MLLMs.

## Broader Impacts.

Chart understanding is an especially crucial skill for MLLMs to develop as they are applied to increasingly difficult real-world tasks, such as reading and summarizing scientific papers. MLLMs with strong chart understanding can analyze and interpret graphs for non-experts to quickly understand and operationalize insights into trends in business, healthcare, and economics. Therefore, faithful benchmarking of MLLMs is important in the identification and rectification of weaknesses in existing MLLMs. Our collection of complex, real-world charts is stylistically representative of the types of data MLLMs need to process. At the time of writing, existing MLLMs struggle to answer char-related questions reliably, so we believe that CharXiv can meaningfully guide the development and benchmarking of future MLLMs.