# Accelerating Non-Maximum Suppression:

A Graph Theory Perspective

King-Siong Si\({}^{1}\)1 &Lu Sun\({}^{1}\)1 &Weizhan Zhang\({}^{}\) &Tieliang Gong\({}^{2}\)

&Jiahao Wang\({}^{2}\) &Jiang Liu\({}^{3}\) &Hao Sun\({}^{3}\)

\({}^{1}\)School of Computer Science and Technology, MOEKLINNS Lab, Xi'an Jiaotong University

\({}^{2}\)School of Computer Science and Technology, BDKE Lab, Xi'an Jiaotong University

\({}^{3}\)Institute of Artificial Intelligence (TeleAI), China Telecom

{sjsinx, sunlu.cs}@stu.xjtu.edu.cn, {zhangwzh, gongtl}@xjtu.edu.cn

uguisu@stu.xjtu.edu.cn, {black_liu_1025, sun.010}@163.com

Equal contributionCorresponding author

###### Abstract

Non-maximum suppression (NMS) is an indispensable post-processing step in object detection. With the continuous optimization of network models, NMS has become the "last mile" to enhance the efficiency of object detection. This paper systematically analyzes NMS from a graph theory perspective for the first time, revealing its intrinsic structure. Consequently, we propose two optimization methods, namely QSI-NMS and BOE-NMS. The former is a fast recursive divide-and-conquer algorithm with negligible mAP loss, and its extended version (eQSI-NMS) achieves optimal complexity of \((n n)\). The latter, concentrating on the locality of NMS, achieves an optimization at a constant level without an mAP loss penalty. Moreover, to facilitate rapid evaluation of NMS methods for researchers, we introduce NMS-Bench, the first benchmark designed to comprehensively assess various NMS methods. Taking the YOLOv8-N model on MS COCO 2017 as the benchmark setup, our method QSI-NMS provides \(6.2\) speed of original NMS on the benchmark, with a \(0.1\%\) decrease in mAP. The optimal eQSI-NMS, with only a \(0.3\%\) mAP decrease, achieves \(10.7\) speed. Meanwhile, BOE-NMS exhibits \(5.1\) speed with no compromise in mAP.

## 1 Introduction

Object detection is a highly significant and popular topic in computer vision, widely applied in various domains, e.g., multiple object tracking , medical imaging analysis , multimodal object detection , and autonomous driving . In recent years, there has been significant attention on the real-time performance of object detection, with notable successes achieved in several research endeavors . Non-maximum suppression (NMS)  is a post-processing technique used to eliminate duplicate detection boxes and obtain final detections. Some research on NMS has indeed enhanced the mean average precision (mAP) of object detection, but they have also introduced additional computational overhead.

Currently, most CNN-based object detection models (such as the R-CNN family  and the YOLO series ) consist of two parts in the testing phase: model inference and post-processing. In recent years, with the continuous emergence of model lightweight techniques , the time cost of model inference has been significantly reduced. As a result, NMS gradually becomes a bottleneck in the pipeline of object detection . To address this, somestudies [4; 46; 40] have proposed parallelization methods to enhance NMS efficiency. However, these methods do not reduce computational overhead; they rely heavily on efficient parallel computing to reduce overall time costs. The degree of parallelism depends on the hardware environment (such as processor type, quantity, and number of cores) and architecture, leading to significant variations in efficiency when models are deployed across different platforms. Additionally, NMS research lacks a unified evaluation framework for two main reasons. First, existing NMS methods require a complete model inference for each test, consuming a significant amount of unnecessary computational resources. Second, different NMS methods are tested on different platforms using various models, making comparisons between different NMS algorithms challenging.

To reduce the computational overhead of NMS, we first map the set of bounding boxes obtained from model inference to a graph \(\). We then conduct a comprehensive and systematic analysis of the intrinsic structure of NMS from a graph theory perspective. Each box is considered a node in the graph, and the suppression relationships are represented as arcs. We discovered that this forms a directed acyclic graph (DAG), allowing us to solve NMS using dynamic programming. This indicates that as long as the graph \(\) can be quickly constructed, NMS can be efficiently performed. Through the analysis of \(\), we find that it contains many weakly connected components (WCCs), and most of them are small. Based on these two characteristics, we propose two optimization strategies. First, due to the nature of dynamic programming, different WCCs are independent. We can use a divide-and-conquer algorithm to break down the problem into smaller subproblems corresponding to these WCCs and solve them recursively. Inspired by quicksort, we propose quicksort induced NMS (QSI-NMS), which provides \(6.18\) speed with a negligible \(0.1\%\) decrease in mAP compared to original NMS in YOLOv8-N  on MS COCO 2017 . Furthermore, by analyzing the structure of QSI-NMS, we propose extended QSI-NMS (eQSI-NMS) with a complexity of \((n n)\), achieving state-of-the-art performance. Second, leveraging the locality suppression characteristic of NMS, where most weakly connected components are small, we exclude boxes that cannot have suppression relationships through geometric analysis. This led to the development of boxes outside excluded NMS (BOE-NMS), which provides \(5.12\) speed with no compromise in mAP compared to original NMS in YOLOv8-N on MS COCO 2017.

To facilitate the evaluation and comparison of NMS algorithms, we introduce NMS-Bench, the first end-to-end benchmark for rapid NMS validation. By decoupling model inference and post-processing, we save substantial computational resources, enabling NMS validation to be completed within minutes. Moreover, by implementing NMS algorithms fairly within this framework, different NMS algorithms can be compared on an equal footing. Thus, we integrate data, benchmarking methods, and evaluation metrics into a single framework, enabling end-to-end rapid validation and simplifying NMS research for researchers.

In summary, our contributions are as follows:

* We present the first comprehensive analysis of the NMS algorithm from a graph theory perspective, uncovering the intrinsic structure of NMS;
* We propose two efficient NMS algorithms based on the properties of the NMS-induced graph;
* We introduce NMS-Bench, the first end-to-end benchmark for rapid NMS validation.

## 2 Problem Definition

Original NMS, employs the intersection over union (IOU) between bounding boxes as the criterion for mutual suppression. Specifically, Given a set of candidate bounding boxes \(\), original NMS selects the box \(b^{*}\) with the highest confidence score from \(\), removes it from \(\), and adds it to the final output set \(\). Then, it computes IOUs between \(b^{*}\) and all other boxes in \(\). If the IOU with a certain box \(b\) is greater than a given threshold \(N_{t}\), then \(b\) is removed from \(\). This process is repeated until \(\) is empty.

In general, NMS during post-processing is an algorithm, which takes a list of detection bounding boxes \(\) with corresponding confidence scores \(\) as input, and outputs a subset \(\) of \(\). And for convenience, we denote the cardinality of \(\) by \(n\), i.e., \(n=||\). Formally, the NMS algorithm takes\((,)\) as input, and outputs a sequence \(K=(k_{1},k_{2},,k_{n})\), where

\[k_{i}=1&b_{i};\\ k_{i}=0&.\]

And an evaluation function is a mapping \(e:\{0,1\}^{n}\), where a larger value of \(e\) indicates a better NMS. The goal of our research is to enhance algorithm efficiency under the condition that

\[e(K_{origin})-e(K)<,\]

where \(>0\) represents the tolerance factor and \(K_{origin}\) is the output of original NMS algorithm. In the object detection tasks of this paper, we use mAP as the evaluation function \(e\), and we set \(\) to \(1\%\).

## 3 A Graph Theory Perspective

The bottleneck of NMS algorithms lies in the extensive computation of IOUs. In practice, many IOUs are smaller than a given threshold \(N_{t}\) and will not have any suppressive effect. We aim to consider only those IOUs that will affect the final result, thereby reducing the number of computations and improving efficiency. An IOU greater than \(N_{t}\) indicates that two boxes have a suppressive effect on each other; otherwise, they are independent. We can treat this relationship as an edge in a graph, with each box as a node. This graph reflects the intrinsic structure of NMS, representing the connections between all boxes. By this transformation, we can directly analyze the NMS algorithm through the graph. Compared to a set of boxes in a two-dimensional plane, the structure of the graph is clearer and has more properties that can be utilized.

Specifically, we can regard the input \(,,N_{t}\) of NMS algorithms as a directed graph \(=(,)\). That's because we can think of every box in \(\) as a node in a graph and draw an arc from \(v\) to \(u\) if box \(v\) can suppress box \(u\). Here, we give a formal definition.

**Definition 1**.: _Given a 3-tuple \((,,N_{t})\) consisting of the bounding boxes, confidence scores and an IOU threshold, a graph \(=(,)\) induced by NMS described as follows, there is an injective mapping of \(\) into \(\) that maps each bounding box \(b_{v}\) in \(\) to a node \(v\), and for any ordered pair \((v,u)\),_

\[(v,u) s_{v}>s_{u}(b_{v},b_{u})>N_{t}.\]

**Proposition 1**.: \(\) _is a directed acyclic graph (DAG)._

We prove Proposition 1 in the Appendix. Since \(\) is a DAG, we can use dynamic programming to get the answer to NMS, i.e., \(K\). Specifically, let \((v)\) be the result of node \(v\), i.e., \(v\) is retained if \((v)=1\), otherwise it's not. In original NMS, if there is a node \(v\) that can suppress the current node \(u\), then \(u\) will not be retained. Therefore, we have the dynamic programming equation as follows,

\[(u)=(_{v,(v,u)}(v) )&d^{-}(u)>0;\\ 1&,\]

Figure 1: Dynamic programming in topological sorting. The color of the node represents the \(\) value, i.e., black represents \(1\), and white represents \(0\). Before suppression, each node is black. In topological sorting, traversed arcs are represented by dashed lines, showing they have been removed from the graph. After the topological sorting is completed, we can find that nodes \(1\), \(6\), and \(8\) are all black, that is, the last boxes retained are \(b_{1}\), \(b_{6}\), and \(b_{8}\).

where \(d^{-}(u)\) denotes the in-degree of \(u\).

**Theorem 1**.: \( k_{i}=K_{origin}[i]\)_, we have_

\[k_{i}=(i).\]

Theorem 1 shows that we can actually obtain the result through dynamic programming in topological sorting, shown in Figure 1. Because the result of DP depends only on valid topological sorts, which indicates that we do not need to sort confidence scores in descending order like original NMS to get the same answer, as long as the topological sort is valid. Additionally, we can observe that if there is no path from node \(v\) to node \(u\), then \(v\) does not influence \(u\). From this, we derive Corollary 1.

**Corollary 1**.: _If \(v\) and \(u\) are in two different weakly connected components (WCCs) of \(\), then \((v)\) and \((u)\) are independent._

We find that completing dynamic programming in topological sorting requires \((||+||)\) time. In real-world data, \(||\) appears to have a linear relationship to \(||\) (see Figure 2(a)), so once \(\) is determined, NMS can be highly efficient via DP. However, quickly determining \(\) is not a simple task. This is because, given a bounding box \(b\), it is difficult to quickly determine which boxes in \(\) have an IOU \(>N_{t}\) with it. A related problem is improving the efficiency of the k-nearest neighbors algorithm (kNN), where  have made significant progress. However, IOU is more complex than the distance defined by norms, and we can only approximate \(\) through related algorithms. We tried the latest research , but it provided little help in acceleration due to its large constant.

Fortunately, The NMS task is quite special, as its input comes from well-trained models, meaning that bounding boxes will cluster around many possible object locations, and bounding boxes predicted as different objects are independent of each other. This implies that \(\) is a sparse graph with many WCCs, as shown in Figure 2(a). Additionally, we find that most of the WCCs are quite small, as shown in Figure 2(b). These two observations respectively suggest two optimization strategies (see Figure 3). Firstly, because WCCs are independent of each other, we can use a divide-and-conquer algorithm to break down many WCCs into fewer WCCs, continuously reducing the problem size to improve computational efficiency. Thus, we design QSI-NMS. Secondly, because most WCCs are quite small in size, we can reduce the cost of constructing arcs by geometric knowledge, leading to the design of BOE-NMS.

## 4 Methodology

Following the graph-theoretic analysis of NMS in Section 3, we propose two optimization methods based on two distinct characteristics of graph \(\). Our approach is to design algorithms through the

Figure 2: Statistical characteristics of graph \(\) on MS COCO 2017 validation. 2(a) The scatter plot of \(5000\)\(\)s on MS COCO 2017. It indicates that the number of arcs \(||\) and the number of WCCs \(||\) exhibit an approximately linear relationship with the number of nodes \(||\), respectively. 2(b) The violin plot of the sizes of WCCs across different categories on MS COCO 2017. It reveals the distributional characteristics of the sizes of the WCCs. It shows that over \(50\%\) of the WCCs have a size less than \(5\), and more than \(75\%\) have a size less than \(10\).

analysis of these characteristics to quickly construct \(\) or an approximate graph \(}=(},})\), enabling the use of dynamic programming in topological sorting to obtain NMS results.

### Qsi-Nms

We observe that graph \(\) contains many WCCs, and according to Corollary 1, these components do not affect each other. This implies that, unlike the original NMS, which processes bounding boxes sequentially after sorting by confidence scores and is therefore very slow, we can solve the problem more efficiently using a divide-and-conquer algorithm, breaking it down into independent subproblems that can be solved recursively. Inspired by quicksort, we design quicksort induced NMS (QSI-NMS).

In each subproblem on \(\), we can similarly select a pivot and calculate IOUs between the pivot and all the other boxes in \(\), thereby constructing some arcs in \(\). Next, we devise a partitioning criterion to split \(\{\}\) into two disjoint sets, \(_{l}\) and \(_{r}\), which are then solved recursively. Since IOUs are not calculated between boxes in \(_{l}\) and \(_{r}\), some arcs in the original \(\) might be missed. Therefore, we need to carefully choose the pivot and partitioning criterion to ensure that the constructed \(}\) is as similar to \(\) as possible.

For the pivot selection, we need to define a priority to choose the best pivot in \(\). We find that selecting nodes with an in-degree of \(0\) in \(\) is optimal for two main reasons. First, node \(v_{0}\) with an in-degree of \(0\) belongs to some WCCs, and since most nodes in a WCC predict the same object, \(v_{0}\) with the maximum confidence score will suppress most nodes, meaning it has many outgoing arcs. Choosing other nodes in the WCC might allocate \(v_{0}\)'s successors to different subsets, leading to significant discrepancies between \(\) and \(^{}\). Second, according to De Morgan's laws, the value of \((u)\) is essentially the conjunction of the negations of the predecessors' \(\) values, formally described as follows:

\[(u)=_{v,(v,u)}(v)\]

This implies that missing an arc \((v_{0},v)\) could result in \((v)\) being incorrectly computed as \(1\), causing a chain reaction that significantly deviates \(K\) from \(K_{origin}\). According to Definition 1, the node

Figure 3: The key ideas behind QSI-NMS (left) and BOE-NMS (right). \(\) (middle) contains many small weakly connected components (WCCs). QSI-NMS considers the global structure of the graph \(\), where there are many WCCs. It selects a pivot (the red node on the left) and computes IOUs (orange edges) with all current subproblem nodes using a divide-and-conquer algorithm. BOE-NMS focuses on the local structure (the red dashed box) of \(\), where most WCCs are quite small in size. It selects a node (the red node on the right) and only computes IOUs (orange edges) with its nearby nodes (solid arrows), which is derived from 2D plane geometric analysis (dashed arrows).

\(v^{*}\) corresponding to the box \(b^{*}\) with the highest confidence score has an in-degree of \(0\). Hence, we select the box \(b^{*}\) with the highest confidence score as the pivot.

For the partitioning criterion, we need to consider the spatial characteristics of different WCCs. Different WCCs are relatively dispersed in 2D space, so we can define the partitioning criterion based on the positions of the boxes. We represent the position of a box by its centroid, as it is intuitive and representative. Since the centroid is an ordered pair \((x,y)\), we can not compare it directly like real numbers. We need to define a preorder in \(^{2}\). We find that different preorders have negligible effects on mAP. See Appendix C.4 for details. We finally adopt the Manhattan distance to the origin \(O(0,0)\), i.e., the \(L^{1}\) norm, as the comparison standard. Formally, we define a homogeneous relation \(_{M}\) on \(^{2}\):

\[(x_{1},y_{1})_{M}(x_{2},y_{2})|x_{1}|+|y_{1}||x_{2} |+|y_{2}|.\]

Finally, we partition the set \(\{b^{*}\}\) as follows:

\[_{l}=\{b_{c}|b_{c}_{M}b_{c}^{*} b \{b^{*}\}\};\\ _{r}=\{b_{c}|b_{c}_{M}b_{c}^{*} b \{b^{*}\}\},\]

where \(b_{c}\) and \(b_{c}^{*}\) denote the centroid of \(b\) and \(b^{*}\), respectively. Since we always choose the box with the highest confidence score, this creates a valid topological sort of \(}\). Thus, we can avoid explicitly constructing \(}\), further reducing computational overhead. The pseudo-code for QSI-NMS can be found in the Appendix.

eQSI-NMS Taking \((n n)\) TimeThough QSI-NMS performs very well in the real world, it is not an \((n n)\) algorithm for the simple reason that the pivot is not chosen randomly. By analyzing the structure of QSI-NMS, we further optimize it and propose extended QSI-NMS (eQSI-NMS), which only takes \((n n)\) time. Since in carrying out QSI-NMS we always split the problem into two subproblems, we can thus construct a binary tree.

**Definition 2**.: _Given a 3-tuple \((,,N_{t})\), a QSI-tree for \(\) denoted by \(QT()\) is a binary tree defined recursively as follow:_

* _Its root is a node corresponding to the box_ \(b_{v}\) _with maximum confidence_ \(s_{v}\)_._
* _Its left subtree is_ \(QT(_{l})\)_, where_ \(_{l}\) _is the left subset of_ \(\) _in QSI-NMS._
* _Its right subtree is_ \(QT(_{r})\)_, where_ \(_{r}\) _is the right subset of_ \(\) _in QSI-NMS._

_The basic case is that if \(\) is empty, then QSI-tree is also empty, i.e., \(QT()=\)._

An example of QSI-tree is shown in Figure 4. QSI-tree reveals the inherent structure of QSI-NMS, allowing us to consider QSI-NMS from a high-level perspective. More generally, in QSI-NMS, we tag each box with an ordered pair \((p,c)\), where \(p\) represents the priority and \(c\) is the key used for partitioning. We define preorder relations \(_{}\) on \(\) and \(_{}\) on \(\). This indicates that the QSI-tree is essentially a binary search tree that satisfies the max-heap property: the priority of the parent node is not less than that of the child nodes, the keys in the left subtree are all less than or equal to the parent node, and the keys in the right subtree are all greater than the parent node. Furthermore, we have the following Theorem 2.

**Theorem 2**.: _We sort all the elements of \(\) in ascending order of boxes' centroids according to the preorder \(_{}\) into a sequence:_

\[B=(b_{i_{1}},b_{i_{2}},,b_{i_{n}}),\]

_then QSI-tree is a Cartesian tree for \(B\) in which each key is the confidence score of the corresponding box._

Figure 4: A Cartesian tree for \(B\). The x-axis represents the centroid, where the node on the left \(_{}\) the one on the right. The y-axis represents the confidence score, where the node below \(_{}\) the one above. The values of the sequence below the x-axis are the confidence scores of \(B\).

According to the dynamic programming, if a node \(v\) can affect the result \((u)\) of node \(u\), there must exist a path between \(v\) and \(u\). In QSI-NMS, this manifests as node \(v\) only being able to influence nodes within its subtrees in QSI-tree. Theorem 2 states that QSI-tree is a Cartesian tree, indicating that the subtree of \(v\) corresponds to a contiguous interval in \(B\), as shown in Figure 4.

Specifically, the subtree of \(v\) corresponds to the contiguous interval \(B[l_{v}+1:r_{v}-1]\) in \(B\), where \(l_{v}\) is the last position before \(v\) that is greater than \(s_{v}\), and \(r_{v}\) is the first position after \(v\) that is greater than \(s_{v}\). Finding \((l_{v},r_{v})\) for all \(v\) is known as the all nearest greater values problem, which can be solved in \((n)\) time by maintaining a stack. Similarly to QSI-NMS, we can complete the suppression during the algorithm. Therefore, we obtain the following time complexity:

\[(n n+n)=(n n).\]

According to our best knowledge, this algorithm is the most optimal in terms of complexity. The pseudo-code can also be found in the Appendix.

### Boe-Nms

We find that the vast majority of WCCs in \(\) are very small, as shown in Figure 2(b). This is because there are not many bounding boxes predicting the same object, and NMS is a form of local suppression. We hope to consider the locality of box distributions, so that the currently selected box only computes IOUs with boxes corresponding to nodes in the same WCC, rather than computing IOUs with all boxes as in original NMS.

We focus on the spatial locality of boxes. We found that a box is likely to have large IOUs only with neighbors that are relatively close to it in 2D space, which also indicates that \(\) is a sparse graph. Formally, we have the following theorem:

**Theorem 3**.: _Given a bounding box \(b^{*}\), \( b\), we have IOU\((b^{*},b)\) if the centroid of \(b\) does not lie within \(b^{*}\). Formally,_

\[(x_{c}^{(b)}>x_{rb}^{(b^{*})} x_{c}^{(b)}<x_{lt}^{(b^{*})}) (y_{c}^{(b)}>y_{rb}^{(b^{*})} y_{c}^{(b)}<y_{lt}^{(b^{*})}),\]

_where \((x_{c}^{(b)},y_{c}^{(b)})\), \((x_{lt}^{(b^{*})},y_{lt}^{(b^{*})})\) and \((x_{rb}^{(b^{*})},y_{rb}^{(b^{*})})\) denote the coordinates of the centroid of \(b\), the left-top and the right-bottom corners of \(b^{*}\), respectively._

Since \(N_{t}\) is usually greater than \(0.5\), e.g., \(0.7\) for YOLOv8 and Faster R-CNN. By Theorem 3 we can check IOUs only for those boxes whose centroids lie within the current box. Based on this, we propose boxes outside excluded NMS (BOE-NMS), a method devoid of mAP loss.

In BOE-NMS, We first sort the boxes by their centroids according to lexicographic order \(_{L}\) on \(^{2}\) which is defined as follows:

\[(x_{1},y_{1})_{L}(x_{2},y_{2})(x_{1}<x_{2})(x_{1}=x _{2} y_{1} y_{2}).\]

Then for the current box \(b^{*}\), we can find all the boxes whose centroids may lie in \(b^{*}\) in \(( n)\) time, and we just need to check one by one whether the IOUs between \(b\) and these boxes are greater than \(N_{t}\). The pseudo-code for BOE-NMS is described in Algorithm 3 which can be found in the Appendix. Let's set aside Theorem 3 for now. A more intuitive but weaker conclusion is that if two boxes do not intersect, their IOU must be \(0\). However, this is not conducive to efficient implementation because of the high cost of maintaining the corresponding data structure. We discuss this issue in the Appendix.

\(N_{t}\) is typically set to \(0.7\), and the method based on Theorem 3 does not introduce errors. We also provide a tighter bound to further optimize BOE-NMS. Based on Theorem 4 which is a generalization of Theorem 3, we can handle cases where \(N_{t}\) is any real number \((0,1)\).

**Theorem 4**.: _We use \(s\) to denote a scaling factor, and then we can use \((b,s)\) to represent the new box \(b^{}\) obtained by scaling \(b\). Formally,_

\[x_{lt}^{(b^{})}=x_{c}^{(b)}-s|x_{lt}^{(b)}-x_{c}^{(b )}|,\\ x_{rb}^{(b^{})}=x_{c}^{(b)}+s|x_{rb}^{(b)}-x_{c}^{(b)}|,\\ y_{lt}^{(b^{})}=y_{c}^{(b)}-s|y_{lt}^{(b)}-y_{c}^{(b)}|,\\ y_{rb}^{(b^{})}=y_{c}^{(b)}+s|y_{rb}^{(b)}-y_{c}^{(b)}|.\]

_Given any \(N_{t}(0,1)\), if the centroid of \(b\) does not lie within \((b^{*},1/_{N_{t}}-1)\), then IOU\((b,b^{*}) N_{t}\)._Since BOE-NMS only excludes boxes with \( N_{t}\), the graph constructed by the BOE-NMS is the same as \(\). In other words, the results of BOE-NMS are identical to original NMS. However, unlike original NMS, BOE-NMS does not need to compute IOUs with all remaining boxes but rather determines the boxes that could potentially be suppressed in \(( n)\) time. Next, inspect each of these \(t\) (\(t\) size of the corresponding WCC) boxes one by one in \((t)\) time. As shown in Figure 2(b), the sizes of weakly connected components are almost all less than a constant, say \(10\). This means that the actual performance of BOE-NMS approaches linear time complexity, but strictly speaking, the complexity is still \((n^{2})\).

## 5 Experiments

In this section, we first introduce NMS-Bench, the first end-to-end benchmark for rapid validation of NMS algorithms. Next, we validate our algorithms on NMS-Bench and compare them with classical algorithms: original NMS , Fast NMS , and Cluster-NMS . We conduct tests on MS COCO 2017  and Open Images V7  using YOLOv5 , YOLOv8 , and Faster R-CNN  as validation models. More experimental details can be found in the Appendix.

### NMS-Bench

NMS-Bench is a robust framework that allows researchers to evaluate various NMS methods over different models and datasets in a few minutes. NMS-Bench primarily consists of three components: original bounding box data without NMS applied, implementations of various NMS algorithms as benchmarking methods, and evaluation metrics. The code for NMS-Bench is available on GitHub+.

Footnote †: https://github.com/Yuri3-xr/NMS-Bench

For the original boxes, we extracted non-NMS boxes using different models (YOLO series [31; 21] and Faster R-CNN ) on various datasets [24; 23] to create the NMS-Bench dataset, thereby decoupling the model inference and post-processing stages. This approach saves significant computational resources during inference. We provide a large amount of data for testing, including original boxes from a total of 273,100 images. More detailed information can be found in the Appendix.

For benchmarking methods, NMS-Bench implements classical algorithms such as original NMS , Fast NMS , Cluster-NMS , and PSRR-MaxpoolNMS . QSI-NMS (including eQSI-NMS) and BOE-NMS are also included in NMS-Bench. These methods enable researchers to reproduce and study NMS algorithms. All algorithms are implemented fairly. Researchers can also quickly implement and validate their own NMS algorithms, as NMS-Bench is a plug-and-play, end-to-end benchmark.

For evaluation metrics, we use COCO-style mAP as the accuracy metric and average processing latency per image as the efficiency metric. The latency calculation begins from the input of bounding boxes and ends when the retained bounding boxes are output. For a dataset containing \(N\) images, latency is measured by using the bounding boxes generated per image as input, and the total latency for the \(N\) images is averaged. To mitigate random errors, this measurement is repeated 5 times, and the average of these measurements is used as the final average latency.

### Results

In Table 1, we compare our methods with some mainstream work on MS COCO 2017. We observe that our methods, particularly eQSI-NMS, demonstrate substantial performance enhancements in processing speed across different models on MS COCO 2017. eQSI-NMS stands out by offering up to \(16.9\) speed of original NMS, \(4.3\) speed of Fast NMS, and \(8.9\) speed of Cluster-NMS with a mAP decrease of about \(0.5\%\). Similarly, QSI-NMS provides \(8.8\) speed of original NMS, \(2.2\) speed of Fast NMS, and \(4.6\) speed of Cluster-NMS with a marginal mAP decrease of about \(0.2\%\). BOE-NMS also shows significant enhancements, being \(9.1\) as fast as original NMS, \(2.3\) as fast as Fast NMS, and \(4.8\) as fast as Cluster-NMS with no mAP decrease.

Table 2 shows that on Open Images V7, eQSI-NMS provides approximately \(10.2\) speed of original NMS, \(3.7\) speed of Fast NMS, and \(7.0\) speed of Cluster-NMS. QSI-NMS is about \(5.6\) as fast as original NMS, \(2.0\) as fast as Fast NMS, and \(3.9\) as fast as Cluster-NMS. Similarly, BOE-NMS

[MISSING_PAGE_EMPTY:9]

algorithm's complexity, introduces a large constant factor that hampers efficiency when the number of bounding boxes is small (e.g., the average number of bounding boxes in the three Faster R-CNN models is less than 300). However, it performs well when the number of bounding boxes is large (e.g., YOLOv5-S has an average of 2898 bounding boxes). This demonstrates that the speedup of PSRR-MaxpoolNMS is highly dependent on the degree of parallelism, whereas our methods directly reduce computational overhead (see Figure 5 in Appendix D.3), making it hardware-agnostic and suitable for resource-constrained edge devices.

## 6 Related Work

NMS algorithm is widely used in object detection tasks [35; 31; 34]. Original NMS  operates on a greedy principle, suppressing bounding boxes with an Intersection over Union (IOU) higher than a given threshold, starting from the ones with the highest confidence scores. On one hand, numerous improvements have been made to NMS to achieve higher mAP in certain scenarios [3; 29; 16; 25; 47; 18; 37]. On the other hand, some research focuses on enhancing speed. Fast NMS  improves NMS efficiency by avoiding the sequential processing of bounding boxes that need to be suppressed, making it more conducive to parallel computing and thus speeding up the process, though it may slightly reduce accuracy compared to original NMS. Cluster-NMS , employs matrix operations and iterative processing, running the Fast NMS algorithm in each iteration to accelerate the original NMS without compromising accuracy. MaxpoolNMS  and ASAP-NMS  take into account the setting of "anchors" in the region proposal network (RPN) of two-stage detectors. MaxpoolNMS maps anchors of different sizes onto several score maps and performs spatial max-pooling on these score maps to avoid calculating IOUs, thereby improving the speed of NMS. ASAP-NMS eliminates some boxes with relatively small IOUs by precomputing the IOUs between the current box and neighboring anchors. PSRR-MaxpoolNMS  improves upon MaxpoolNMS by introducing Relationship Recovery, which addresses the issue of score map mismatch that may arise in MaxpoolNMS, enabling PSRR-MaxpoolNMS to be used in the second stage of two-stage detectors. CUDA NMS by torchvision  is a CUDA implementation of the original NMS, leveraging a GPU to accelerate computation-intensive tasks, though it cannot be used in scenarios without a GPU.

## 7 Conclusion

In this paper, we systematically analyze the NMS algorithm from a graph theory perspective and discover strong connections between NMS, directed graph topological sorting, dynamic programming, and weak connected components. Through these analyses, we first propose QSI-NMS, a fast divide-and-conquer algorithm with negligible loss, and its extended version, eQSI-NMS, achieves the state-of-the-art complexity \((n n)\). Additionally, starting from the sparsity of graphs, we design BOE-NMS, which considers the locality suppression feature of NMS, optimizes the NMS algorithm at a constant level, and maintains precision. Furthermore, we introduce NMS-Bench, the first end-to-end benchmark integrating bounding box datasets, NMS benchmarking methods, and evaluations, facilitating NMS research for researchers. Finally, we conducted experiments on NMS-Bench, and the experimental results validated our theory, demonstrating the superiority of our algorithms.

AcknowledgementsThis work was supported in part by the National Natural Science Foundation of China under Grant 62192781, Grant 62172326 and Grant 62137002, and in part by the Project of China Knowledge Centre for Engineering Science and Technology.