# Efficient Reinforcement Learning by Discovering Neural Pathways

Samin Yeasar Arnob

Department of Computer Science

McGill University

Mila Quebec AI Institute

samin.arnob@mail.mcgill.ca

&Riyasat Ohib

Georgia Institute of Technology

&Sergey Plis

Georgia State University

&Alessandro Sordoni

Microsoft Research&Amy Zhang

University of Texas, Austin&Doina Precup

McGill University

Mila Quebec AI Institute

###### Abstract

Reinforcement learning (RL) algorithms have been very successful at tackling complex control problems, such as AlphaGo or fusion control. However, current research mainly emphasizes solution quality, often achieved by using large models trained on large amounts of data, and does not account for the financial, environmental, and societal costs associated with developing and deploying such models. Modern neural networks are often overparameterized and a significant number of parameters can be pruned without meaningful loss in performance, resulting in more efficient use of the model's capacity. We present a methodology for identifying sparse sub-networks within a larger network in reinforcement learning (RL). We call such sub-networks _neural pathways_. We show empirically that even very small learned sub-networks, using less than 5% of the large network's parameters, can provide very good quality solutions. We also demonstrate the training of multiple pathways within the same networks in a multi-task setup, where each pathway tackles a separate task. We evaluate empirically our approach on several continuous control tasks, in both online and offline settings.

## 1 Introduction

Scaling large neural-network models  has led to state of the art in machine learning benchmarks. While the utility of these large models in a variety of applications makes them compelling for widespread use, it also requires very expensive training with a large carbon footprint . While the human brain serves as an inspiration for deep learning techniques, the current deep learning architectures do not exhibit the same level of energy efficiency. The brain continuously learns new skills without catastrophic forgetting due to its plasticity , i.e., its ability to continually strengthen more frequently used synaptic connections and eliminate synaptic connections that are rarely used, a phenomenon called _synaptic pruning_. This way, the brain generates _neural pathways_ to efficiently transmit information and are used to complete different tasks .

In the past, several studies have explored to mimic this behaviour by training sub-networks of a neural network for each task . This approach consists of reserving specific subsets of weightsfor each task. We refer to these sub-networks as _neural pathways_. These studies operate in supervised and continual learning scenarios therefore their applicability to multi-task RL remains unexplored. Moreover, these pathways can be quite large, often comprising 30% or more of the total network parameters , making them inefficient. Pruning literature [18; 15; 24; 5; 56; 92; 57] has focused on identifying highly sparse sub-networks. However, these approaches are generally designed for single-task scenarios, as they specifically target and prune weights that are not utilized within the chosen sub-network. In RL, finding performant sub-networks is more challenging due to the data distribution shift during online training . Our findings indicate that recent methods employing dynamic sparse training and gradient-based topology evolution to prune networks for reinforcement learning [27; 82] are ineffective at high sparsity levels (95%).

In this paper, we study the feasibility of training neural pathways for multi-task RL, where each task is tackled by a different pathway of the same underlying network. For offline RL, we show that configuring the pathway in a single shot using existing parameter importance criteria  is sufficient and outperforms standard multi-task training. To address distribution shifts for online RL, we introduce _data adaptive pathway discovery_ (DAPD), which employs an initial warm-up phase for pathway discovery, in which the pathway is progressively reconfigured during policy training, and then kept frozen for the final stages of training. Our proposed method not only surpasses the performance of competitive dynamic sparse training baselines but also Dense networks in single and multi-task continuous control tasks.

In contrast to existing multi-task training methods in RL focusing on gradient manipulation [97; 80; 14; 7; 72], architecture modifications  and task similarities [96; 16; 77], we show that discovering task pathways is simple, effective and allows for energy savings. We manage to discover pathways that utilize only a small fraction (5%) of the neural network parameter. Given the high-sparsity level, the resulting policy requires low _floating point operations_ (FLOPs) by potentially leveraging sparse matrix multiplication (SpMM) [54; 55; 53] to increase energy efficiency, reduce carbon footprint and potentially be deployed on low-resource devices (i.e., embedded systems, edge devices, etc.).

We highlight our contributions as follows:

* We showcase how to train multiple neural pathways for multi-task RL where the objective is to improve energy efficiency and reduce the carbon footprint associated with both offline and online RL training.
* We introduce Data Adaptive Pathway Discovery (DAPD), which leverages network sensitivity to adjust pathways in response to the data distribution shifts encountered in online RL. This capability enables us to identify pathways at high levels of sparsity and surpass competitive sparse training baselines [27; 82].
* We demonstrate superior sample efficiency and performance in both single and multi-task RL. The sparsity in the model induces \(20\)x increase in energy efficiency compared to alternative approaches, achieved through FLOP count reduction and the utilization of Sparse Matrix Multiplication (SpMM).

Figure 1: For any given task, our proposed method activates a specific part of the neural network.

Related Work

Sparse NetworksAdvances in finding sparse networks have proven that there exists sub-networks which contain a small fraction of the parameters of the Dense deep neural network yet retain similar performance [18; 15; 24; 5; 56; 92]. Building upon a three-decade-old saliency criterion used for pruning trained models , a recent technique to prune models at initialization was proposed by  and was swiftly followed by subsequent works [90; 10; 83] which can find sub-networks at initialization and operate in supervised learning. In _offline_ RL, pruned models at initialization has been proven effective , but are limited to a static and pre-defined training dataset. Recent studies on sparsity in RL methods suggest leveraging gradient-based topology evolution criteria, as proposed in RiGL , to identify sparse networks. Rlx2  shows that topology evolution encounters challenges in maintaining stable value estimation in continuous control tasks. In response to this limitation, RLx2 incorporates a multi-step temporal difference (TD) target  and a dynamic capacity on replay buffer. These models exhibit high sensitivity to specific sparsity levels for continuous control tasks, necessitating careful tuning for optimal performance.

Multi-task RLThe motivation of many works in _multi-task_ RL is that training a policy for more than one task becomes difficult due to gradient interference, i.e., gradients for different tasks pointing in very different directions [97; 41]. Recent work proposes several possible solutions, such as constraining the conflicting gradient update [97; 80; 14; 7; 72], constraining data sharing among irrelevant tasks [96; 16], modularize the neural network to reuse network components across tasks [67; 93] and learning the underlying context of relevant tasks . Inspired by previous works on continual learning [91; 45; 8; 44], we propose to tackle the multi-task RL scenario by assigning each task to a specific sub-network of a shared deep network. We introduce a robust algorithm designed to concurrently find the sub-networks and optimize the parameters of the shared network in the context of RL.

Energy Efficient Deep LearningRecent works suggest [59; 31] carbon-emission can be reduced by using sample efficient ML architecture, optimized hardware for ML (ex: TPU, GPU) or cloud compute in location with clean energy. Many hardware startups  are developing AI-specific chips, some of which claim to achieve a substantial increase in FLOPS/Watt gains from simply reconfiguring hardware to do the same number of operations for less economic cost. For performance per power efficiency, the FPGA-based accelerator  is 11.6 times better than GPU-based one. NVIDIA  already shows almost 10x speed up using Block-SpMM using V100 GPU.  proposes a high-performance sparse-matrix library for low-precision integers on Tensor cores and shows 2.37x performance improvement on Nvidia-A100. There has been ongoing research on how to properly estimate the carbon footprint [31; 59]. We focus on utilizing the sparse matrix multiplication (SpMM) aware hardware and software to reduce the carbon footprint.

## 3 Neural Pathway Discovery for RL

In this work, we aim to show the feasibility of tackling both online and offline multi-task RL by _pathway discovery_ (PD), i.e. identifying and training different sub-networks of a same shared network for each task. The sparsity of each sub-network allows for energy efficiency gains thus reducing the carbon footprint associated with training and deployment.

### Background

Reinforcement LearningWe consider learning in a Markov Decision Process (MDP) defined by the tuple (\(S,A,P,R\)) with states \(s S\), actions \(a A\), transition dynamics \(P(s^{}|s,a)\), and reward function \(R(s,a)\). At time step \(t\), the state, action, and reward are denoted as \(s_{t}\), \(a_{t}\), and \(r_{t}=R(s_{t},a_{t})\), respectively. An episode is a trajectory \(=(s_{0},a_{0},r_{0},s_{1},a_{1},r_{1},...,s_{T},a_{T},r_{T})\), accumulating an episodic return \(R_{T}=_{t=0}^{T}r_{t}\). For continuous control tasks, we use an infinite horizon, \(T=\), aiming to maximize the expected discounted return \([_{t=0}^{}^{t}r_{t}]\). Here, \(\) represents the discount factor set at 0.99. In a multitask setup, we have \(N\) tasks with respective reward functions \(\{R_{n}\}_{n=1}^{N}\) and optimal policies \(\{_{n}^{*}(a|s)\}_{n=1}^{N}\). No assumptions are made about transition dynamics.

Neural PathwaysDenote by \(f(x,)\) a neural network with initial parameters \(_{0}_{}\). We define a binary _mask_\(m\{0,1\}^{||}\) which defines whether a connection in \(f\) should be masked or not. Applying the mask on the parameters of \(f\) creates a _neural pathway_, i.e. a sparse sub-network \( m\), which parameterizes the resulting function \(f(x, m)\), where \(\) denotes element-wise product . Given a dataset of task examples \(D\), the learning problem consists of learning both the mask \(m\) and parameters \(\). In this paper, we operate in a multi-task setting and learn a pathway for each task. Therefore, we will usually have a collection of masks \(\{m_{n}\}\) and resulting pathways \(\{ m_{n}\}\), indexed by the task \(n\). Note that all the pathways are defined with respect to the same base parameters \(\), i.e. tasks compete for capacity under the same base network \(f\). This introduces optimization challenges, especially in online RL, that we address with our method in the next section.

### Data Adaptive Pathway Discovery (DAPD)

In online RL, an agent interacts with an environment, whether simulated or real-world, to gather training samples using its behaviour policy. As the policy improves, there exists a significant shift not only in the quality but also in the distribution of the collected training samples. To address such shifts in distribution, we introduce _Data Adaptive Pathway Discovery_ (DAPD), which consistently adapts the sub-network as the policy advances, accommodating changes in data distribution throughout the training process. Our algorithm to find neural pathways for online RL relies on the following crucial design aspects. We will first outline the parameter selection criteria proposed in , followed by how we incorporate the criteria into the development of our online RL algorithm.

Selection CriterionFollowing previous work , we infer the task specific mask using a criterion \(\), which measures the importance of every parameter in the neural network for a given task:

\[m=_{k}(;D),\] (1)

where \(D\) is the dataset containing training samples for the current task, \(_{k}\) is defined as the "Top-\(k\)" operator, that sets top-\(k\) parameters to 1 and the rest to 0, thus controlling the percentage of total network parameters considered active.

We use a gradient-based saliency criterion that identifies important connections by using a sensitivity measure, defined as the influence of each weight on the loss function [38; 48]. Formally, the effect of weight \(_{q}\) on the loss is:

\[(_{q})=_{ 0}|(_{0}) -(_{0}+_{q})}{}|=|_{q }}{_{q}}|,\] (2)

where \(_{q}\) is a vector whose \(q\)-th element equals \(_{q}\) and all other elements are \(0\). This scoring metric has been used in  to prune models at initialization by setting the parameters with scores lower than a certain threshold to \(0\). Here, we use this scoring metric to not only rank important parameters at initialization, but also update them to adapt to the changing data distribution.

Adaptive MaskingIn online RL, an agent interacts with the environment and collects the corresponding training dataset \(D\) for a given task. Therefore, the task dataset \(D\) depends on the current policy. We leverage the most recent episodic data collected by the agent. At the \(j^{th}\) training step, we calculate a score \(^{j}\) based on the most recently gathered training samples:

\[^{j}(_{q},D^{t-L:t})=|_{q}(_{0};D^{t-L:t})}{_{q}}|,\] (3)

where \(D^{t-L:t}\) is the most recent \(\{(s,a,s^{},r)\}_{l=0}^{L}\) tuples added to the replay buffer and \(L\) is the number of timesteps of an episode of finite length. We found that scoring parameters only on the basis on the most recent samples collected by the updated policy is crucial. This translates into the assumption that as our policy improves, the quality of recently collected samples gets better. We found that it is harmful to include prior historical data for mask inference, especially in the initial stage of online training, where the agent takes random actions to explore the environment.

To prevent abrupt mask changes and ensure training stability, we propose averaging the last \(K\) scores and we can express the _K-length moving average_ as \(_{k=0}^{K-1}^{j-k}\). Thus, to update the mask we use the following objective:

\[m=_{k}_{k=0}^{K-1}^{j-k}(_{q}, D^{t-L:t}).\] (4)Warm-up and FreezeWe found it important to have an initial _warm-up_ phase during training, where we periodically adjust the mask using Eq. (4) until we attain a reasonable episodic return _threshold, TH_, which is a hyper-parameter of the algorithm often used in foundational and applied RL research studies [46; 74; 81; 36]. After the warm-up phase is completed, we _freeze_ the mask for the remaining training process to allow further training of the parameters in the pathway. Because of the potential presence of _many lottery sub-networks_, we found that freezing the mask after a warm-up phase avoids continued oscillation among different sub-networks, which reduces variation in end performance across runs. During the warm-up phase, we perform a _periodic update_ of \(m\). As the mask updates, the solution space for optimizing the sub-network changes accordingly. Therefore, it is important that the mask update frequency is slower than the frequency of the parameter updates. This ensures that the optimization process remains aligned with the evolving sub-network solution space resulting from changes in the mask. Thus, while we perform per-step updates of sub-network parameters, we only update the mask at the end of each episode.

Online AlgorithmWe use Soft Actor-Critic (SAC)  as our online algorithm. SAC defines an actor-network \(()\) and a critic network \(Q()\), each having its own objective function (discussed in Appendix B.1). For every task \(n\), we learn two separate masks \(m_{n}^{}\), \(m_{n}^{}\), each masking their corresponding base parameters, i.e. each task \(n\) has two pathways \( m_{n}^{}\), \( m_{n}^{}\). We use the warm-up and freeze strategy: the masks are updated during a warm-up phase using only the most recent transitions collected from the dataset: once we reach a threshold of performance we stop updating the pathways and keep them fixed. During this phase of warm-up training, we simultaneously update the masks and the base parameters \(\) and \(\). We use Eq. (4) to update the masks. We present the pseudo-code of this procedure in Alg. 1.

### Pathways for Offline RL

Much like supervised training, offline RL operates with a fixed training dataset for each task. Consequently, adaptive pathway updates are unnecessary, and we can identify the top \(5\%\) most crucial weights for each task using Eq. (1). For all experiments, including baseline comparisons, we adhere to a parallel training procedure. We initiate parallel training processes [51; 46; 89], assigning one for each task and implementing asynchronous global parameter updates in a Hogwild  style. Convergence of the global parameter in such a method is established in the context of RL [46; 89]. Refer to Algorithm 2 for the pseudocode outlining the integration of our offline PD algorithm.

``` \(()\), \(Q()\) \(\) Episode Return \(n^{th}\) task, \(R_{T}^{n}=_{t=0}^{T}c_{t}^{n}\); \(\) Replay buffers \(n^{th}\) task, \(D_{n}\); \(\) Mask for\(n^{th}\) tasks, \(m_{n}=\{m_{n}^{},m_{n}^{}\}\). \(\) Param-up-phase = [True,..., True] \(\)Training steps do while episode not done do \(\) Collect Data for Each Task do \(\)\(a_{t}\), \(\)\(_{a}\), \( m_{n}^{}\), \(s_{t+1} T_{n}(s_{t+1}|s_{t},a_{t})\) \(\)\(D_{n} D_{n}\{s_{t},a_{t},r(s_{t},a_{t}),s_{t+1}\}\) endfor \(\) Update Network using SAC: \(\) Update \(_{}\) ( [\(m_{n}^{},m_{n}^{},m_{n}^{}\)], \(\{D_{1},D_{2}\)\(\_\_\)n}) \(\) Update \(Q_{}\) ( [\(m_{n}^{},m_{n}^{},m_{n}^{}\)], \(\{D_{1},D_{2}\)\(\_\)n}) endwhile \(\) Periodically Update Tasks: for Each Task do \(\) If Param-up-phase[\(n\)]: Update \(m_{n}\) using Eq (4) \(\) If \(R_{T}^{n} TH:\) warm-up-phase[\(n\)] = False endfor  endfor ```

**Algorithm 1** Multi-task SAC with DADD (SAC-DADD)

## 4 Experiments

We demonstrate the efficacy of our proposed method in learning policy for online RL tasks using only \(5\%\) of the parameters in the continuous control domain without sacrificing performance compared to the Dense counterpart. Additionally, we conduct comparisons with various baselines, evaluating the reliability of performance across different network sparsity levels. Furthermore, we investigate the use of additional parameter space to enable multitasking through multiple pathways in a multitask benchmark, concurrently presenting a more energy-efficient alternative. Environment details and snapshots are provided in Appendix C.4 and further experimental results in Appendix D.

### Scenario 1: Online Single-Task RL

We conduct our experiment using _Soft-Actor-Critic_ (SAC)  on MuJoCo continuous control tasks. We use DAPD to identify a pathway comprising only \(5\%\) (or \(95\%\) sparse) of the total parameters of the actor and critic network of the SAC algorithm.

Baseline ComparisonWe compare the performance of DAPD with other pruning algorithms adapted for online RL, specifically RiGL  and Rlx2 , as well as with the performance of a SAC-Dense network, in which 100% of the parameter space is trained. A comparative performance analysis involving HalfCheetah-v2, Walker2d-v2, Hopper-v2, and Ant-v2 is presented in Table 1. In Figure 2 (\(a\)), we compare the learning curve of DAPD with SAC-Dense and with a pruning method on SAC exhibiting 95% sparsity on HalfCheetah-v2. We also evaluate SNIP  applied at initialization using Eq. (1). All the baselines are trained for the same number of steps, i.e. for DAPD the warm-up phase is included within the total training steps, with no additional training compared to the baselines. The poor performance of SNIP highlights the importance of our proposed adaptive update in an RL context.

First, we emphasize that DAPD outperforms the dense network both at the end of training, for multiple levels of sparsity, as well as during training, even with an extreme level of 95% sparsity. (Fig. 1(a)). A recent study  has highlighted the fact that dense networks exhibit primacy bias, essentially overfitting data observed early in the training. Our approach has two effects: the sparsity acts as a regularizer while training the sub-network adaptively reconfigures the parameters during the early stages of training, therefore mitigating primacy bias. RiGL and Rlx2 have specific sparsity levels for each environment at which they perform well, as shown in the Appendix (Figure 12) and in Fig 1(b), but they are ineffective at a 95% sparsity level. Additional evidence regarding the learning curve and comparative analysis at various sparsity levels on Walker2d-v2, Hopper-v2, Ant-v2 is provided in Appendix ( see Figure 13 and 14).

Impact of Warm-UpWe hypothesize that due to the distribution shift in online training, single-shot pathway discovery techniques fail to maintain satisfactory performance. Hence, the impact of warm-up, via the periodic reconfiguration of the pathway, should be essential. To validate this hypothesis, we train SAC without warm-up: we collect random trajectories, initialize a mask using Eq (1) and train the resulting sparse SAC network for 1 million steps. We report the results in Fig. 2 (\(c\)), (\(\)), as the mean (5 seeds) performance evaluated every 5000 steps. Secondly, we show that the stopping the update of the mask during training is crucial for obtaining reliable performance.

   Environment & SAC-Dense & RiGL & Rlx2 & SAC-DAPD \\  HalfCheetah-v2 & 8568.1 \(\) 1043.56 & 4043.95 \(\) 467.88 & 2333.31 \(\) 1241.16 & **9028.02 \(\) 276.31** \\  Walker2d-v2 & 2972.49 \(\) 1691.47 & 260.3 \(\) 31.16 & 518.45 \(\) 205.16 & **3846.3 \(\) 459.82** \\  Hopper-v2 & 3228.5 \(\) 301.88 & 174.89 \(\) 8.12 & 198.29 \(\) 10.39 & **3359.88 \(\)46.57** \\  Ant-v2 & 3538.22 \(\) 654.76 & 954.2 \(\) 14.4 & 963.68 \(\) 6.96 & **3916.65 \(\) 502.82** \\   

Table 1: Performance comparison of DAPD with various baselines at **95% sparsity** in single-task experiments using MuJoCo continuous control. We compare the average episodic return over the last 10 evaluations over 5 seeds after 1 million training steps.

Figure 2: Performance comparison of DAPD with baseline on HalfCheetah-v2. (\(a\)) At **95% sparsity**, we show the learning curve of different algorithms. (\(b\)) Under **varying sparsity levels** we compare the average episodic return evaluated end of 1 million training steps. (\(c\)) Ablation study of DAPD.

_DAPD w/o freeze_ results in frequent switching among different sub-networks throughout the training process. This is evident in Figure 2, where _DAPD w/o freeze_ (blue) exhibits high variance in the performance. In contrast, DAPD (pink) with a fixed warm-up phase (halted at episodic return, \(TH\)=8000), demonstrates low variance in episodic return and surpasses the performance of the SAC-Dense model. In Appendix D.1, we provide empirical evidence demonstrating the existence of numerous lottery subnetworks and validate that freezing any of these subnetworks can yield equally good performance. Further insights are provided in Appendix (Table 5), where we explore the impact of the warm-up period's duration and the rolling moving average on the scoring function in DAPD performance.

Evolution of SparsityWe observe the evolution of layer-wise sparsity in Figure 4. Following evolution topology-based pruning, both RiGL and RIX2 initiate with a random mask. While they iteratively prune and grow connections within each layer, they maintain a fixed sparsity percentage per layer. This constraint leads to higher sparsity in the input layer compared to the output layer (Layer-1 and 3 in Figure 4, potentially overlooking critical features at the input layer. In contrast, DAPD examines the importance of weights across the entire network, granting us the flexibility to focus on weights throughout the network, providing a higher degree of freedom. While our initial observation reveals a sparser input layer compared to the output layer, this balance shifts as training progresses. In this specific experiment, we continuously re-calibrate the mask during training. Our findings indicate that while the sparsity adapts, it fluctuates within a certain range, reinforcing our rationale for maintaining a fixed-length warm-up phase.

Sample EfficiencyTo investigate the sample efficiency of DAPD, we conducted experiments with varying numbers of training steps, comparing the performance with the SAC-Dense model in MuJoCo tasks. In Figure 3 we find DAPD to consistently surpass the performance of the SAC-Dense model, proving DAPD to be more sample efficient. This highlights its potential for more efficient resource utilization compared to its Dense counterpart. Additional experimental results are provided in Figure 15 in Appendix.

### Scenario 2: Online Multi-Task RL

We compare the performance of our proposed method against various baselines on the MetaWorld  MT10 benchmark. The agent gets a binary score based on success in reaching an expected goal state. In Table 2, we evaluate the mean success rate over 10 tasks. Results include mean and standard deviation over 10 seeds.

Baseline ComparisonWe compare our method to vanilla SAC (SAC-Dense), SM  which requires a modified network architectures to route through modular networks, PCGrad , which requires a complex gradient update procedure, and CARE , which uses pre-trained language model to encode meta-data and retrieve contextual information about the tasks. Despite its simplicity, we observe a significant performance enhancement with DAPD compared to Dense, from \(49\%\) to a remarkable success rate of \(77\%\). This improvement is particularly impressive considering that DAPD uses the same learning objective and gradient update rules as the Dense model while operating with an extremely sparse network, utilizing only 5% of the model weights for any task.

Energy ConsumptionWe estimate energy consumption based on FLOP counts  and create a normalized (on a scale of 1) energy consumption profile alongside performance in Figure 5. A reduction in FLOP counts, leads to decreased computations and lower energy costs, resulting in a proportional relationship between FLOP count and energy cost in _Joules_ i.e. FLOPs \(\) Joules. This allows us to illustrate the trade-off in energy consumption during inference on Dense networks for marginal gain. Compared to other baseline methods, our proposed DAPD can potentially save \(20\)x energy usage while achieving competitive performance. We would like to point out that the only baseline that exceeds our performance is CARE  which requires a pre-trained language model and assumes task dependency.

We discover that a warm-up phase of \(10k\) steps, constituting \(0.5\%\) of the total training steps, proves effective in configuring the pathway. This approach avoids gradient interference encountered due to multitask training and prevents catastrophic failures in the learning process. Further details are provided in Appendix D.2. We consider choosing the neural pathways for each task independently from one another and yet we find considerable pathway overlap. Further discussed in Appendix D.4. Since this is the first work that proposes the neural pathways in a multitask RL setting, there is a range of open questions that need to be explored. Limitations of our method and possible avenues of future work are discussed in the Appendix E.

### Scenario 3: Offline Multi-Task RL

For our offline experiments, we train SAC to collect the training dataset ( see Appendix C.2). We use BCQ  and IQL  for our offline experiments. We compare the performance of PD with two Dense baseline methods: (\(a\)) a multi-task variant (MT) that uses a single Dense network for multiple tasks, and (\(b\)) a multi-head multi-task variant (MHMT) that employs independent heads for each task, both utilizing the Dense network. It is crucial to emphasize that, similar to MT and MHMT, we refrain from imposing any additional multitask learning objectives, facilitating a fair baseline comparison. Instead, our approach provides the network with the advantage of training separate parameters for each task. We evaluate performance and energy efficiency in the MetaWorld benchmark. We further examine robustness in comparison to Dense baselines under mixed data distribution and sample complexity on HalfCheetah-Multitask. Supplementary experiments, including comparisons with single-task experts on additional sets of tasks, are presented in Appendix D.3.

MetaWorld BenchmarkThe success rate (mean and standard deviation over 10 seeds) is reported in Table 3. Whereas baselines with Dense networks suffer from high variance in performance, with BCQ+PD, we get a perfect score on all MetaWorld tasks. A success rate of 100 indicates BCQ reaches the goal for all the tasks all the time. Moreover, if our model uses \(\)_Joules_ of energy, based on the FLOP relationship, we show how costly other baselines are in terms of energy consumption. We also provide a detailed breakdown of individual task performance Appendix (Table 9), where we compare _Conservative data sharing_ (CDS) , an offline-multitask learning algorithm that utilizes task similarity.

Figure 5: Energy consumption profile of algorithms on MetaWorld benchmark. We normalize the performance and energy consumption to highlight the trade-off for performance gain.

Performance Under Mixed Data DistributionTo further validate the reliability of our method, we conduct a _sample complexity analysis_ across varying reduced training sample sizes. The interquartile plot of normalized scores in Figure 6\((a)\) consistently demonstrates an improvement when BCQ is trained with PD.

Performance Under Sample ComplexityEven within the Offline setting, it is anticipated that there may be a distribution shift in the static training dataset. Therefore, the resilience of offline algorithms is assessed under a mixed data distribution [22; 21; 28]. In Figure 6\((b)\), we demonstrate that PD exhibits enhanced performance even when confronted with a mixed dataset. Additional details regarding data collection and the experimental setup are provided in the Appendix D.3.2.

## 5 Conclusion

We propose to train task-specific neural pathways for reinforcement learning (RL) within a single deep neural network. We presented the _Data Adaptive Pathway Discovery_ (DAPD), which builds on the pruning literature [18; 38; 90; 15] to identify individual pathways. The DAPD method overcomes limitations in sparse network discovery using connection sensitivity under dynamic data distributions. Our methodology showcases superior sample efficiency and excels in both single and multitask training instances over dense networks, while using \(95\%\) fewer parameters on high-dimensional continuous controls, without the need for complex objective functions or gradient manipulation. Beyond performance gains, our work has the potential to significantly enhance energy efficiency in neural networks. Through reduced FLOP counts and leveraging Sparse Matrix Multiplication (SpMM), our approach can be 20x more energy efficient than alternative strategies.

This work opens up research into the possibility of a single neural network being trained for multiple purposes in RL (i.e. learning different features, multiple value functions etc.) while providing better efficiency in utilizing the parameter space of neural networks. While our method can be integrated into other selective data-sharing and gradient update methods, further research is required in this direction. Expanding the scope of experiments would provide valuable insights for future investigations.

Figure 6: (a) We compare the sample complexity analysis of BCQ+PD with BCQ-MT (Multitask) and BCQ-MHMT (Multihead Multitask) baselines on HalfCheetah multitask. (b) Performance (Normalized Score) plot of BCQ on HalfCheetah multitask trained with PD with baselines on mixed data distribution.

    &  &  &  \\   & BCQ & IQL & BCQ & IQL & BCQ & IQL \\  MT-10 tasks & \(\) & \(\) & \(81.5 24.15\) & \(79.1 26.81\) & \(95.9 10.44\) & \(96.5 7.10\) \\ Parameter Counts & \(\) & \(\) & \(1.34\)M & \(1.01\)M & \(1.38\)M & \(1.12\)M \\ FLOPs & \(\) & \(\) & \(589\)K & \(1073\)K & \(629\)k & \(1128\)k \\ Energy Consumption, _Joules_ & \(k\) & \(k\) & \(20k\) & \(20k\) & \(21.25k\) & \(21.02k\) \\   

Table 3: Performance comparison in MetaWorld offline. We compare the final success rate (mean and std over 10 seeds) of pathways discovery (PD) on MT10 tasks with Offline-MT and Offline-MHMT baselines on offline RL algorithms. We also show the _reduced parameter complexity_ (x times) of PD compared to other baseline method 

## 6 Acknowledgement

We want to thank Raihan Siraj, Rishabh Agarwal for their helpful discussion and suggestions and the anonymous reviewers for their helpful comments. This work is supported by the Natural Sciences and Engineering Research Council of Canada (NSERC) Grant, Canadian Institute for Advanced Research (CIFAR) Grant. Samin Yeasar Arnob would like to thank the generous support of the DeepMind Graduate Award and Unifying Neuroscience and Artificial Intelligence in Quebec (UNIQUE).

Riyasat Ohib and Sergey Plis were in part supported by NIH R01DA040487 and NSF 2112455.

This research was enabled by the compute support provided by Calcul Quebec, the Digital Research Alliance of Canada and Mila Quebec AI Institute.