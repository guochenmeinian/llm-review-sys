# Deep Neural Collapse Is Provably Optimal

for the Deep Unconstrained Features Model

 Peter Sukenik

Institute of Science and Technology

Klosterneuburg, Austria

peter.sukenik@ista.ac.at

&Marco Mondelli

Institute of Science and Technology

Klosterneuburg, Austria

marco.mondelli@ista.ac.at

Equal contribution

&Christoph H. Lampert

Institute of Science and Technology

Klosterneuburg, Austria

chl@ist.ac.at

Equal contribution

###### Abstract

Neural collapse (NC) refers to the surprising structure of the last layer of deep neural networks in the terminal phase of gradient descent training. Recently, an increasing amount of experimental evidence has pointed to the propagation of NC to earlier layers of neural networks. However, while the NC in the last layer is well studied theoretically, much less is known about its multi-layered counterpart - deep neural collapse (DNC). In particular, existing work focuses either on linear layers or only on the last two layers at the price of an extra assumption. Our paper fills this gap by generalizing the established analytical framework for NC - the unconstrained features model - to multiple non-linear layers. Our key technical contribution is to show that, in a deep unconstrained features model, the unique global optimum for binary classification exhibits all the properties typical of DNC. This explains the existing experimental evidence of DNC. We also empirically show that _(i)_ by optimizing deep unconstrained features models via gradient descent, the resulting solution agrees well with our theory, and _(ii)_ trained networks recover the unconstrained features suitable for the occurrence of DNC, thus supporting the validity of this modeling principle.

## 1 Introduction

In the thought-provoking paper , Papyan _et al_. uncovered a remarkable structure in the last layer of sufficiently expressive and well-trained deep neural networks (DNNs). This phenomenon is dubbed "neural collapse" (NC), and it has been linked to robustness and generalization of DNNs. In particular, in  it is shown that all training samples from a single class have the same feature vectors after the penultimate layer (a property called NC1); the globally centered class-means form a _simplex equiangular tight frame_, the most "spread out" configuration of equally-sized vectors geometrically possible (NC2); the last layer's classifier vectors computing scores for each of the classes align with the centered class-means (NC3); finally, these three properties make the DNN's last layer act as a nearest class-center classifier on the training data (NC4).

The intriguing phenomenon of neural collapse has spurred a flurry of interest aimed at understanding its emergence, and the unconstrained features model (UFM)  has emerged as a widely accepted theoretical framework for its analysis. In this model, one assumes that the network possesses such anexpressive power that it can represent arbitrary features on the penultimate layer's output. Thus, the feature representations of the training samples on the penultimate layer are treated as free optimization variables, which are optimized for (instead of the previous layers' weights). Since its introduction, the UFM has been understood rather exhaustively, see e.g.  and Section 2.

Most recently, researchers started to wonder, whether NC affects solely the last layer. The "cascading neural collapse" is mentioned in , where it is pointed out that NC1 might propagate to the layers before the last one. Subsequently, a mathematical model to study this deep version of NC has been proposed in , albeit only for two layers; and a deep model has been considered in , albeit only for linear activations. On the empirical side, the propagation of NC1 through the layers is demonstrated in , and the occurrence of all the NC properties in multiple layers is extensively measured in . All these works agree that neural collapse occurs to some extent on the penultimate and earlier layers, leaving as a key open question the rigorous characterization of its emergence.

To address this question, we generalize the UFM and its extensions in  to cover an arbitrary number of non-linear layers. We call this generalization the _deep unconstrained features model (DUFM)_. The features, as in the original UFM, are still treated as free optimization variables, but they are now followed by an arbitrary number of fully-connected layers with ReLU activations between them, which leads to a natural setup to investigate the theoretical underpinnings of deep neural collapse (DNC). Our main contribution is to prove (in Section 4) that DNC is the only globally optimal solution in the DUFM, for a binary classification problem with \(l_{2}\) loss and regularization. This extends the results in  and  by having more than two layers and non-linearities, respectively. While the statements of  are formulated for an arbitrary number of classes, here we restrict ourselves to binary classification. However, we drop an important assumption on the features of the optimal solution made by  (see the corresponding Theorem 4.2 and footnote 3), which was only verified experimentally. In a nutshell, we provide the first theoretical validation of the occurrence of DNC in practice: deep neural collapse gives a globally optimal representation of the data under the unconstrained features assumption, for any arbitrary depth.

In addition to our theoretical contribution, we numerically validate in Section 4.2 the occurrence of DNC when training the DUFM with gradient descent. The loss achieved by the DUFM after training agrees well with the theoretically computed optimum, and the corresponding solution clearly exhibits the properties described by our main result. Finally, to check whether the main assumption of the DUFM - the unconstrained features - is relevant in practical training, we show that DNNs trained on CIFAR-10 can indeed represent features suitable for the occurrence of DNC. In particular, _(i)_ we pre-train a small DUFM to full collapse, _(ii)_ attach the fully-connected layers (DUFM without the unconstrained features) to the end of a randomly initialized CNN, and finally _(iii)_ train the CNN with the DUFM's weights fixed. The plots in Figure 1 show that the backbone recovers the unconstrained features leading to DNC. This is quite remarkable, as the collapse, being data-dependent, completely breaks after exchanging the trained unconstrained features with the output of the randomly initialized backbone. Additional details can be found in Section 4.2.

Figure 1: Deep neural collapse metrics as a function of training progression. On the left, DNC1 measures within-class variability collapse (the closer to \(0\), the more collapsed); in the middle, DNC2 measures closeness of feature activations to an orthogonal frame (the closer to \(1\), the more collapsed); and on the right, DNC3 measures alignment between feature activations and weights (the closer to \(0\), the more collapsed). The ResNet20 recovers the DNC metrics for DUFM, in accordance with our theory. This validates unconstrained features as a modeling principle for neural collapse.

## 2 Related work

Since the original paper , neural collapse has been intensively studied, with [21; 3] introducing the now widely accepted unconstrained features model (UFM), also called layer peeled model. In particular, the work on NC can be loosely categorized into _(i)_ the study of the mathematical grounds behind NC [31; 19; 40; 3; 42; 12; 21; 7; 18; 29; 36; 37], _(ii)_ the development of a connection between NC and DNN performance [32; 5; 6; 1; 34; 15; 39; 17; 16] and _(iii)_ empirical observations concerning NC [35; 20]. The first line of work can be further divided into _(i-a)_ a static analysis, that studies the global optimality of NC on suitable objectives [31; 19; 40; 28; 42; 12], and _(i-b)_ a dynamic analysis, that characterizes the emergence of NC in training with gradient-based methods [21; 7; 12; 30]. The vast majority of works uses the UFM to derive their results, but there are attempts to derive NC without it, see e.g. [25; 33; 23; 24; 14; 38]. For additional references, see also the recent survey .

More specifically, [31; 19] show global optimality of NC under UFM, when using the cross-entropy (CE) loss. Similar optimality results are obtained in [40; 28] for the MSE loss. For CE loss, a class-imbalanced setting is considered in , where the danger of minority classes collapsing to a single representation (thus being indistinguishable by the network) is pointed out. This analysis is further refined in , where a generalization of the ETF geometry under class imbalance is discovered. On the edge between static and dynamic analysis are results which characterize the optimization landscape of UFM. They aim to show that all the stationary points are either global optima (thus exhibiting NC) or local maxima/strict saddle points with indefinite Hessian, which enables optimization methods capable of escaping strict saddles to reach the global optimum. For the CE loss, this property is proved in  and , while in the MSE setup this insight is provided by . A unified analysis for CE, MSE and some other losses is done in . The convergence of the GD dynamics to NC is considered in  under MSE loss for near-zero initialization. The more refined analysis of  reveals that the last layer's weights of UFM are close to being conditionally optimal given the features and, assuming their _exact_ optimality, the emergence of NC is demonstrated with renormalized gradient flow. For CE loss,  shows that gradient flow converges in direction to the KKT point of the max-margin optimization problem, _even without_ any explicit regularization (unlike in the MSE loss setting). As the optimal solutions of this max-margin problem exhibit NC, this yields the convergence of gradient-based methods to NC as well. Quantitative results on the speed of convergence for both the MSE and CE losses are given in .

Most relevant to our paper, the emergence of NC on earlier layers of DNNs is empirically described in [8; 26; 11; 4]. In particular, it is demonstrated in  that, at the end of training, the NC1 metric decreases log-linearly with depth, thus showing a clear propagation of neural collapse. Most recently, the propagation of all the NC properties beyond the last layer is studied in , for a setting that includes biases. On the theoretical side, the UFM with _multiple linear_ layers is considered in , where it is shown that NC is globally optimal for different formulations of the problem. The first result for a non-linear model with _two layers_ (separated by a ReLU activation) is given by . There, it is shown that NC, as formulated in the bias-free setting, is globally optimal. However, an additional assumption on the optimum is required, see the detailed discussion in Section 4.

## 3 Preliminaries

Notation.Let \(N=Kn\) be the total number of training samples, where \(K\) is the number of classes and \(n\) the number of samples per class. We are interested in the last \(L\) layers of a DNN. The layers are counted from the beginning of indexing, i.e., the \(L\)-th layer denotes the last one. The layers are fully connected and bias-free with numbers of input neurons \(d_{1},,d_{L}>1\) and weight matrices \(W_{1}^{d_{2} d_{1}},W_{2}^{d_{3} d_{2}}, ,W_{L}^{K d_{l}}\). Let \(H_{1}^{d_{1} N},H_{2}^{d_{2} N},, H_{L}^{d_{L} N}\) be the feature matrices _before_ the application of the ReLU activation function, which is denoted by \(\), i.e., \(H_{2}=W_{1}H_{1}\) and \(H_{l}=W_{l-1}(H_{l-1})\) for \(l 3\). When indexing a particular sample, we denote by \(h_{c,i}^{(l)}\) the \(i\)-th sample of the \(c\)-th class in the \(l\)-th layer, i.e., a particular column of \(H_{l}\). Moreover, let \(_{c}^{(l)}=_{i=1}^{n}h_{c,i}^{(l)}\) be the activation class-mean for layer \(l\) and class \(c\), and \(_{l}\) the matrix containing \(_{c}^{(l)}\) as the \(c\)-th column. For convenience, the training samples are arranged class-by-class (first all the samples of first class, then of the second, etc.). Thus, the label matrix containing the one-hot encodings \(Y^{K N}\) can be written as \(I_{K}_{n}^{T}\), where \(I_{K}\) denotes a \(K K\) identity matrix,denotes the Kronecker product and \(_{n}\) a row vector of all-ones of length \(n\). With this, we can write \(_{l}=H_{l}(I_{K}_{n})\) and we further denote \()}:=(H_{l})(I_{K}_{n})\).

Deep neural collapse.Since we assume the last \(L\) layers to be bias-free, we formulate a version of the deep neural collapse (DNC) without biases (for a formulation including biases, see ).

**Definition 1**.: _The deep neural collapse at layer \(l\) is described by the following three properties:2_

* _The within-class variability after_ \(l-1\) _layers is_ \(0\)_. This property can be stated for the features either before or after the application of the activation function_ \(\)_. In the former case, the condition requires_ \(h_{c,i}^{(l)}=h_{c,j}^{(l)}\) _for all_ \(i,j[n]\) _(or, in matrix notation,_ \(H_{l}=_{l}_{n}^{T}\)_); in the latter,_ \((h_{c,i}^{(l)})=(h_{c,j}^{(l)})\) _for all_ \(i,j[n]\) _(or, in matrix notation,_ \((H_{l})=)}_{n}^{T}\)_)._
* _The feature representations of the class-means after_ \(l-1\) _layers form an orthogonal matrix. As for DNC1, this property can be stated for features either before or after_ \(\)_. In the former case, the condition requires_ \(_{l}^{T}_{l} I_{K}\)_; in the latter,_ \()}^{T})} I_{K}\)_._
* _The rows of the weight matrix_ \(W_{l}\) _are either 0 or collinear with one of the columns of the class-means matrix_ \()}\)_._

Note that, in practice, DNNs do not achieve such properties _exactly_, but they approach them as the training progresses.

Deep unconstrained features model.Next, we generalize the standard unconstrained features model and its extensions  to its deep, non-linear counterpart.

**Definition 2**.: _The \(L\)-layer deep unconstrained features model (\(L\)-DUFM) denotes the following optimization problem:_

\[_{H_{1},W_{1},,W_{L}}\|W_{L}(W_{L-1}(  W_{2}(W_{1}H_{1})))-Y\|_{F}^{2}+_{l=1}^{L}}}{2}\|W_{l}\|_{F}^{2}+}}{2} \|H_{1}\|_{F}^{2},\] (1)

_where \(\|\|_{F}\) denotes the Frobenius norm and \(_{H_{1}},_{W_{1}},,_{W_{L}}>0\) are regularization parameters._

## 4 Optimality of deep neural collapse

We are now state our main result (with proof sketch in Section 4.1, full proof in Appendix A).

**Theorem 3**.: _The optimal solutions of the \(L\)-DUFM (1) for binary classification (\(K=2\)) are s.t._

\[(H_{1}^{*},W_{1}^{*},,W_{L}^{*})\ \ ,\ \ n_{H_{1}}_{W_{1}}_{W_{L}}<}{2^{ L+1}L^{2L}}.\] (2)

_More precisely, DNC1 is present on all layers; DNC2 is present on \((H_{l}^{*})\) for \(l 2\) and on \(H_{l}^{*}\) for \(l 3\); and DNC3 is present for \(l 2\). The optimal \(W_{1}^{*},H_{1}^{*}\) do not necessarily exhibit DNC2 or DNC3. If the inequality in (2) holds with the opposite strict sign, then the optimal solution is only \((H_{1}^{*},W_{1}^{*},,W_{L}^{*})=(0,0,,0)\)._

In words, Theorem 3 shows that, unless the regularization is so strong that the optimal solution is all-zeros, the optimal solutions of the DUFM exhibit all the DNC properties. Notice that (2) requires the per-layer regularization to be at most of order \(L^{-1}n^{1/(L+1)}\), which is a very mild restriction. If the condition in (2) holds with equality, the set of optimal solutions includes _(i)_ solutions exhibiting DNC, _(ii)_ the all-zero solution, and _(iii)_ additional solutions that do _not_ exhibit DNC.

We highlight that Theorem 3 holds for _arbitrarily deep non-linear_ unconstrained features models. This is a significant improvement over both , which considers \(L=2\), and , which considers the linear version of the DUFM. Furthermore, Theorem 4.2 of  has a similar statement for two layers(although without considering the DNC1 property on \(H_{1},H_{2}\)), but it requires the extra assumption \(\|(H_{2}^{*})\|_{*}=\|H_{2}^{*}\|_{*}\), where \(\|\|_{*}\) denotes the nuclear norm. In particular, the argument of  crucially relies on the fact that \(\|(H_{2}^{*})\|_{*}\|H_{2}^{*}\|_{*}\). We remark that the inequality \(\|(M)\|_{*}\|M\|_{*}\) does not hold for a generic matrix \(M\) (see Appendix B for a counterexample) and, at the optimum, it is only observed empirically in . In summary, our result is the first to theoretically address the emerging evidence  that neural collapse is not only a story of a single layer, but rather affects multiple layers of a DNN.

### Proof sketch for Theorem 3

First, denote by \(()\) an operator which takes any matrix \(A^{e f}\) as an input and returns the set of \(\{e,f\}\) singular values of \(A\). The singular values are denoted by \(s_{i}\) and ordered non-increasingly. We specifically label \(s_{l,j}\) the \(j\)-th singular value of \((H_{l})\) for \(l 2\).

The argument consists of three steps. First, we assume \(n=1\), and prove that the optimal solutions of the \(L\)-DUFM problem exhibit DNC2-3 (as stated in Theorem 3). Second, we consider \(n>1\) and show that DNC1 holds. Third, we conclude that DNC2-3 hold for \(n>1\).

Step 1: DNC2-3 for \(n=1\).The idea is to split the optimization problem (1) into a series of simpler sub-problems, where in each sub-problem we solve for a fraction of the free variables and condition on all the rest. Crucially, the optimal solutions of all the sub-problems only depend on the singular values of \((H_{l})\), for \(l 2\). Thus, the final objective can be reformulated in terms of \(((H_{l}))\), for \(l 2\). By noticing that the objective is symmetric and separable w.r.t. the first and second singular values of the matrices, we can solve separately for \(\{s_{l,1}\}_{2 l L}\) and \(\{s_{l,2}\}_{2 l L}\). At this point, we show that these separated problems have a unique global optimum, which is either \(0\) or element-wise positive (depending on the values of the regularization parameters \(_{H_{1}},_{W_{1}},,_{W_{L}}\)). Hence, the optimal \(s_{l,j}\) will be equal for \(j=1\) and \(j=2\), which in turn yields that \((H_{l})\) is orthogonal. Finally, using the intermediate steps of this procedure, we are able to reverse-deduce the remaining properties.

We now sketch the step of splitting the problem into \(L\) sub-problems. As the \(l\)-th sub-problem, we optimize over matrices with index \(L-l+1\) and condition on all the other matrices, as well as singular values of \(\{(H_{l})\}_{l 2}\) (keeping them fixed). We start by optimizing over \(W_{L}\) and then optimize over \(W_{l}\) for \(2 l L-1\) (for all \(l\) the optimization problem has the same form and thus this part is compressed into a single lemma). If \(L=2\), we do not have any \(l\) for which \(2 l L-1\) and, hence, we skip this part. Finally, we optimize jointly over \(W_{1},H_{1}\). We start by formulating the sub-problem for \(W_{L}\).

**Lemma 4**.: _Fix \(H_{1},W_{1},,W_{L-1}\). Then, the optimal solution of the optimization problem_

\[_{W_{L}}\|W_{L}(W_{L-1}( W_{2}(W_{ 1}H_{1}))-I_{2}\|_{F}^{2}+}}{2}\|W_{L}\| _{F}^{2}\]

_is achieved at \(W_{L}=(H_{L})^{T}((H_{L})(H_{L})^{T}+2_{W_{L}}I_{d_{ L}})^{-1}\) and equals_

\[}}{2(s_{L,1}^{2}+2_{W_{L}})}+}} {2(s_{L,2}^{2}+2_{W_{L}})}.\] (3)

The proof follows from the convexity of the problem in \(W_{L}\) and a simple form of the derivative, see the corresponding lemma in Appendix A for details.

We remark that the solution (3) resulting from the optimization over \(W_{L}\) depends on \((H_{L})\) only through the singular values \(s_{L,1},s_{L,2}\). Thus, we can fix \(s_{L,1},s_{L,2}\) and \(H_{1},W_{1},,W_{L-2}\), and optimize over \(W_{L-1}\) (provided \(L-1>1\)). It turns out that the resulting optimal value of the objective depends only on \(s_{L,1},s_{L,2},s_{L-1,1},s_{L-1,2}\). Thus, we can now fix \(s_{L-1,1},s_{L-1,2}\) and \(H_{1},W_{1},,W_{L-3}\), and optimize over \(W_{L-2}\) (provided \(L-2>1\)). The optimal solution will, yet again, only depend on the singular values of \((H_{L-1}),(H_{L-2})\). Thus, we can fix \(s_{L-2,1},s_{L-2,2}\) and \(H_{1},W_{1},,W_{L-4}\) and repeat the procedure (provided \(L-3>1\)). In this way, we optimize over \(W_{l}\) for all \(l 2\).

We formulate one step of this nested optimization chain in the following key lemma. Here, \(X\) abstractly represents \(H_{l-1}\) and \(\{s_{1},s_{2}\}\) represent \(\{s_{l,1},s_{l,2}\}\). A proof sketch is provided below and the complete argument is deferred to Appendix A.

[MISSING_PAGE_EMPTY:6]

So far, we have reduced the optimization over \(\{W_{l}\}_{l=2}^{L-1}\) to the optimization over the singular values of \(\{(H_{l})\}_{l=2}^{L}\). We now do the same with the first layer's matrices \(W_{1},H_{1}\). For this, we again split the optimization of \(}}{2}\|W_{1}\|_{F}^{2}+}}{2 }\|H_{1}\|_{F}^{2}\) given \(\{s_{1},s_{2}\}=((W_{1}H_{1}))\) into two nested sub-problems: the inner one performs the optimization for a fixed output \(H_{2}:=W_{1}H_{1}\); the outer one optimizes over the output \(H_{2}\), given the optimal value of the inner problem.

The inner sub-problem is solved by the following Lemma, which describes a variational form of the nuclear norm. The statement is equivalent to Lemma C.1 of .

**Lemma 8**.: _The optimization problem_

\[_{A,B;C=AB}}{2}\|A\|_{F}^{2}+}{2}\|B\|_{F}^{2}\] (6)

_is minimized at value \(_{B}}\|C\|_{*}\), and the minimizers are of the form \(A^{*}=_{A}U^{1/2}R^{T},B^{*}=_{B}R^{1/2}V^{T}\). Here, the constants \(_{A},_{B}\) only depend on \(_{A},_{B}\); \(U V^{T}\) is the SVD of \(C\); and \(R\) is an orthogonal matrix._

To be more concrete, we apply Lemma 8 with \(A=W_{1}\) and \(B=H_{1}\) (so that \(C=W_{1}H_{1}=H_{2}\)). Thus, we are only left with optimizing \(\|H_{2}\|_{*}\) given \(((H_{2}))=\{s_{1},s_{2}\}\). We solve this problem in the following, much more general lemma, which might be of independent interest. Its proof is deferred to Appendix A.

**Lemma 9**.: _Let \(L 2\) be a positive integer. Then, the optimal value of the optimization problem_

\[_{H}\|H\|_{S_{}}^{},\ ((H))=\{s_{1},s_{2}\}\] (7)

_equals \((s_{1}+s_{2})^{}\). Here \(\|\|_{S_{p}}\) denotes the \(p\)-Schatten pseudo-norm._

**Remark 10**.: _As a by-product of the analysis in the proof of Lemma 9, we obtain a characterization of the minimizers of (7), which will be useful in **Step 2**. Let \(H^{*}\) be an optimizer of (7). Then, \((H^{*})\) has orthogonal columns. For \(L>2\), its negative entries are set uniquely so that \(H^{*}\) has rank 1. For \(L=2\), its negative entries are arranged as follows. All the rows of \((H^{*})\) which contain exactly one positive entry on the first column will result in rows of \(H^{*}\) that are multiples of each other. Similarly, all the rows of \((H^{*})\) which contain exactly one positive entry on the second column will result in rows of \(H^{*}\) that are multiples of each other. The zero rows of \((H^{*})\) remain zero rows in \(H^{*}\). Moreover, the \(l_{2}\) norm of the negative entries of the rows of \(H^{*}\) with positive entry in the first column equals the \(l_{2}\) norm of the negative entries of the rows of \(H^{*}\) where the positive entry is in the second column. Finally, taking any two rows of \(H^{*}\) with counterfactual positioning of the signs of the entries, the product of negative entries does not exceed the product of positive entries._

Armed with Lemma 4, 5 and 9, we reformulate the \(L\)-DUPM objective (1) only in terms of \(\{s_{l,1},s_{l,2}\}_{2 l L}\). We then split this joint problem into two separate, identical optimizations over \(\{s_{l,1}\}_{2 l L}\) and \(\{s_{l,2}\}_{2 l L}\), which are solved via the lemma below (taking \(x_{l}:=s_{l,1}^{2}\) and \(x_{l}:=s_{l,2}^{2}\), respectively). Its proof is deferred to Appendix A.

**Lemma 11**.: _The optimization problem_

\[_{x_{l};2 l L}\ }}{2(x_{L}+2_{W_{L}})}+ _{l=2}^{L-1}(}}{2}}{x_{l}})+ }_{H_{1}}}}\] (8)

_is optimized at an entry-wise positive, unique solution if_

\[_{H_{1}}_{l=1}^{L}_{W_{l}}<}{2^{L+1}L^{2L}},\] (9)

_and at \((x_{2},,x_{L})=(0,,0)\) otherwise. When (9) holds with equality, both solutions are optimal. Here, \(0/0\) is defined to be \(0\)._

Note that (9) is equivalent to (2) when \(n=1\). Furthermore, if the optimization problem (8) has a single non-zero solution, then the optimal \(s_{l,1}^{*}\) and \(s_{l,2}^{*}\) are forced to coincide for all \(l\). From this, we obtain the orthogonality of \((H_{l}^{*})\) for \(l 2\). Lemma 6 then gives that the optimal \(H_{l}^{*}\) for \(l 3\) must be orthogonal as well, since it is non-negative and equal to \((H_{l}^{*})\). Finally, the statement about \(W_{l}^{*}\) for \(l 2\) also follows from Lemma 6. This concludes the proof of DNC2-3 for \(n=1\). As a by-product of Lemma 8, we additionally obtain a precise characterization of the optimal \(W_{1}^{*},H_{1}^{*}\).

Step 2: DNC1 for \(n>1\).Now, we assume \(n>1\) and the conclusion of **Step 1**. We show that DNC1 holds for the optimizer of (1). We give a sketch here, deferring the proof to Appendix A.

We proceed by contradiction. Assume there exist \(_{1}^{*},W_{1}^{*},,W_{L}^{*}\) which achieve the minimum of (1), but \(_{1}^{*}\) does _not_ exhibit DNC1. Note that the objective in (1) is separable w.r.t. the columns of \(H_{1}\). Thus, for both classes \(c\{1,2\}\), the partial objectives corresponding to the columns \(\{h_{c,i}^{(1)}\}_{i=1}^{n}\) of \(_{1}^{*}\) must be equal for all \(i\). Indeed, if that is not the case, we can exchange all the columns of \(_{1}^{*}\) with the ones that achieve the smallest partial objective value and obtain a contradiction. Now, we can construct an alternative \(_{1}^{*}\) such that, for each class \(c\), we pick any \(h_{c,i}^{(1)}\) and place it instead of \(\{h_{c,j}^{(1)}\}_{j i}\). By construction, \(_{1}^{*}\) exhibits DNC1 and it is still an optimal solution. At this point, let us construct \(H_{1}^{*}\) from \(_{1}^{*}\) by taking a single sample from both classes. We claim that \((H_{1}^{*},W_{1}^{*},,W_{L}^{*})\) is an optimal solution of (1) with \(n=1\) and \(n_{H_{1}}\) as the regularization term for \(H_{1}\), while keeping \(d_{1},,d_{L}\) and \(_{W_{1}},,_{W_{L}}\) the same. To see why this is the case, consider an alternative \(G_{1}^{*}\) achieving a smaller loss. Then, \((G_{1}^{*}_{1}^{T},W_{1}^{*},,W_{L}^{*})\) would achieve a smaller loss than \((_{1}^{*},W_{1}^{*},,W_{L}^{*})\), which contradicts its optimality.

Recall that \(H_{1}^{*}\) is an optimal solution of (1) with \(n=1\). By **Step 1**, the columns \(x,y\) of \((H_{2}^{*})\) are such that \(x,y 0\), \(x^{T}y=0\), and \(\|x\|=\|y\|\) (here, by the notation \(x 0\) we mean an entry-wise inequality). Assume \((H_{2}^{*}) 0\). First we show that \(x,y\) and thus the whole \((H_{2}^{*})\) are uniquely determined by \((W_{1}^{*},W_{2}^{*},,W_{L}^{*})\). For this we use the uniqueness of the scaling obtained in Lemma 11 and the DNC3 property for the \(((H_{2}^{*}),W_{2}^{*})\) pair. Then, by Lemma 9, all the matrices \(A_{2}\) such that \((A_{2})=(H_{2}^{*})\) and \(\|A_{2}\|_{*}=\|H_{2}^{*}\|_{*}\) have columns of the form \(x-ty,y-tx\) for \(t\). Let \(A_{2}^{*}\) be a matrix of that form for a particular \(t\). A series of steps which include computing the SVD of \(A_{2}^{*}\) and the usage of Lemma 8 reveal that, among \(0 t 1\), there is a single \(t^{*}\) and \(A_{2}^{*}=A_{2}^{t^{*}}\) such that \(W_{1}^{*}\) solves the optimization problem (6) in Lemma 8 if \(H_{2}=A_{2}^{t}\). Thus, having fixed \(A_{2}^{*}\) that can be the output of the first layer, this in turn gives the uniqueness of the unconstrained features \(H_{1}\) solving (6) while \(W_{1}=W_{1}^{*},H_{2}=H_{2}^{*}=A_{2}^{*}\). However, as \(_{1}^{*}\) is not DNC1 collapsed by assumption, there are multiple possible choices for \(_{1}^{*}\) and then \(H_{1}^{*}\), which gives a contradiction with the uniqueness of \(H_{1}\) solving (6).

Step 3: DNC2-3 for \(n>1\).Having established DNC1, we obtain DNC2-3 by reducing again to the case \(n=1\), which allows us to conclude the proof of the main result. We finally remark that \(W_{1}^{*}\) and \(H_{1}^{*}\) do not need to be orthogonal, due to the \(^{1/2}\) multiplication in the statement of Lemma 8. This means that \(W_{1}^{*}\) and \(H_{1}^{*}\) may not exhibit DNC2-3 (as stated in Theorem 3).

### Numerical results

DUFM training.To support the validity of our theoretical result and to demonstrate that the global optimum in the DUFM can be found efficiently, we conduct numerical experiments in which we solve (1) with randomly initialized weights using gradient descent. In Figure 2, we plot the training progression of the DNC metrics for two choices of the depth: \(L=3\) (first row) and \(L=6\) (second row). In both cases, we use \(n=50\) training samples and constant layer width of \(64\). The learning rate is \(0.5\), though smaller learning rates produce the same results. We run full gradient descent for \(10^{5}\) steps, and the weight decay is set to \(5 10^{-4}\) for all the layers. The plots are based on 10 experiments and the error bands are computed as one empirical standard deviation to both sides. To measure the DNC occurrence, we plot the following metrics as a function of training. For DNC1, we define \(_{W}^{l}=_{c=1}^{2}_{i=1}^{n}(h_{c,i}^{(l)}-_{c}^{ (l)})(h_{c,i}^{(l)}-_{c}^{(l)})^{T}\) as the _within-class_ variability at layer \(l\) and \(_{B}^{l}=_{c=1}^{2}(_{c}^{(l)}-_{G}^{(l)})(_{c}^{ (l)}-_{G}^{(l)})^{T}\) as the _between-class_ variability, where \(_{G}^{(l)}\) is the global feature mean at layer \(l\). Then, the DNC1 metric at layer \(l\) is \(\|_{W}_{B}^{l}\|_{F}^{2}\), with \(\) being the pseudo-inverse operator. This is an upper bound on the trace of the same matrix, which is sometimes used in the literature for measuring NC1. An ideal DNC1 would show 0 on this metric. For DNC2, our metric at layer \(l\) is given by \(}{s_{L}}\) (recall that these are the singular values of \((H_{l})\)), and we consider the feature matrices both after and before the ReLU activation. An ideal DNC2 would show 1 on this metric. For DNC3, we consider the weighted average sine of the angles between each row of \(W_{l}\) and the closest column of \((H_{l})\); the weights are the \(l_{2}\) norms of the rows of \(W_{l}\). To avoid numerical issues, we exclude rows of \(W_{l}\) whose \(l_{2}\) norm is smaller than \(10^{-6}\). An ideal DNC3 would show 0 on this metric.

The plots in Figure 2 closely follow the predictions of Theorem 3 and, taken as a whole, show that DUFM training via gradient descent clearly exhibits deep neural collapse, for all the three metrics considered, and for both depths. More specifically, DNC1 is progressively achieved in all layers, with the layers closer to the output being faster. DNC2 is achieved for all \((H_{l})\) with \(l 2\) and \(H_{l}\) for \(l 3\) (though we didn't include this in plots to avoid too many lines) and, as predicted by our theory, it is not achieved on \(H_{2}\) (the dotted line in the middle plot) and on \(H_{1}\) (the blue line). Interestingly, for these numbers of layers, SGD does not find solutions which would exhibit DNC2 on \(H_{1}\), even though such solutions exist. Hence, gradient descent is not implicitly biased towards them, see Appendix C for a more detailed discussion. DNC3 is achieved on all layers - even on the first, which is not covered by our theory and where the effect is the most pronounced; the deeper layers achieve values of DNC3 of order \(10^{-2}\), which translates into an angle of less than one degree between weight rows and feature columns. Finally, we highlight that the training loss matches the optimal value computed numerically via (17) (the corresponding plot is reported in Appendix C), and hence the solution found by gradient descent is globally optimal. Additional ablation studies concerning the number of layers, weight decay and width of the layers are reported in Appendix C.

ResNet20 training with pre-trained DUFM.We support the validity of DUFM as a modeling principle for deep neural collapse via the following experiments. We first train an \(L\)-DUFM for \(L=3,5\) with \(K=2\) and \(n=5000\) to full optimality, as in the previous paragraph. Then, we replace \(H_{1}^{*}\) with the output of a randomly initialized backbone, we fix the weight matrices in the DUFM, and we train end-to-end to convergence on the classes 0 and 1 of CIFAR-10. In Figure 1, we report the DNC metrics of our \(3\)-DUFM head, as the training of the backbone ResNet20 progresses. While at the start of the training the features \(H_{1}\) are random and, therefore, the model does not exhibit DNC, the ResNet20 recovers well the DNC properties after training. This suggests that the crucial assumption of DUFM - the unconstrained features, indeed holds in practice. The plots are based on 7 experiments with confidence bands of one empirical standard deviation to both sides. Additional details as well as the experiments with \(5\)-DUFM and VGG13 and DenseNet121 are discussed in Appendix D.

Figure 2: Deep neural collapse metrics as a function of training for the DUFM objective (2). The first (second) row corresponds to \(L=3\) (\(L=6\)). The metrics corresponding to DNC1 and DNC3 are very small, and the metric corresponding to DNC2 is close to 1, which means that the solution found by gradient descent exhibits neural collapse in all layers, as predicted by Theorem 3.

Conclusion

In this work, we propose the deep unconstrained features model (DUFM) - a multi-layered version of the popular unconstrained features model - as a tool to theoretically understand the phenomenon of deep neural collapse (DNC). For binary classification, we show that DNC is globally optimal in the DUFM. This provides the first rigorous evidence of neural collapse occurring for arbitrarily many layers. Our numerical results show that gradient descent efficiently finds a solution in agreement with our theory - one which displays neural collapse in multiple layers. Finally, we provide evidence in favor of DUFM as a modeling principle, by showing that standard DNNs can learn the unconstrained features that exhibit all DNC properties.

Interesting open problems include _(i)_ the generalization to multiple classes (\(K>2\)), _(ii)_ the analysis of the gradient dynamics in the DUFM, and _(iii)_ understanding the impact of biases on the collapsed solution, as well as that of the cross-entropy loss. As for the first point, a crucial issue is to control \(\|W_{1}H_{1}\|_{*}\), since the inequality \(\|(M)\|_{*}\|M\|_{*}\) does not hold for general matrices \(M\). In addition, while Lemma 5 still appears to be valid for \(K>2\) (and it would be an interesting stand-alone result), its extension likely requires a new argument.