# Optimal Design for Human Preference Elicitation

Subhojyoti Mukherjee

University of Wisconsin-Madison

smukherjee27@wisc.edu

&Anusha Lalitha

AWS AI Labs

&Kousha Kalantari

AWS AI Labs

Aniket Deshmukh

AWS AI Labs

Ge Liu

UIUC

The work was done at AWS AI Labs.

Yifei Ma

AWS AI Labs

Branislav Kveton

Adobe Research

The work was done at AWS AI Labs.

###### Abstract

Learning of preference models from human feedback has been central to recent advances in artificial intelligence. Motivated by the cost of obtaining high-quality human annotations, we study efficient human preference elicitation for learning preference models. The key idea in our work is to generalize optimal designs, an approach to computing optimal information-gathering policies, to lists of items that represent potential questions with answers. The policy is a distribution over the lists and we elicit preferences from them proportionally to their probabilities. To show the generality of our ideas, we study both absolute and ranking feedback models on items in the list. We design efficient algorithms for both and analyze them. Finally, we demonstrate that our algorithms are practical by evaluating them on existing question-answering problems.

## 1 Introduction

_Reinforcement learning from human feedback (RLHF)_ has been effective in aligning and fine-tuning _large language models (LLMs)_. The main difference from classic _reinforcement learning (RL)_ is that the agent learns from human feedback, which is expressed as preferences for different potential choices . The human feedback allows LLMs to be adapted beyond the distribution of data that was used for their pre-training and generate answers that are more preferred by humans . The feedback can be incorporated by learning a preference model. When the human decides between two choices, the _Bradley-Terry-Luce (BTL)_ model  can be used. For multiple choices, the _Plackett-Luce (PL)_ model  can be adopted. A good preference model should correctly rank answers to many potential questions. Therefore, learning of a good preference model can be viewed as learning to rank, and we adopt this view in this work. Learning to rank has been studied extensively in both offline  and online  settings.

To effectively learn preference models, we study efficient methods for human preference elicitation. We formalize this problem as follows. We have a set of \(L\)_lists_ representing _questions_, each with \(K\)_items_ representing _answers_. The objective of the agent is to learn to rank all items in all lists. The agent can query humans for feedback. Each query is a question with \(K\) answers represented as a list. The human provides feedback on it. We study two feedback models: absolute and ranking. In the absolute feedback model, a human provides noisy feedback for each item in the list. This setting is motivated by how annotators assign relevance judgments in search . The ranking feedback is motivated by learning reward models in RLHF . In this model, a human ranks all items in the list according to their preferences. While \(K=2\) is arguably the most common case, we study \(K 2\) for the sake of generality and allowing a higher-capacity communication channel with the human . The agent has a budget for the number of queries. To learn efficiently within thebudget, it needs to elicit preferences from the most informative lists, which allows it to learn to rank all other lists. Our main contribution is an efficient algorithm for computing the distribution of the most informative lists.

Our work touches on many topics. Learning of reward models from human feedback is at the center of RLHF  and its recent popularity has led to major theory developments, including analyses of regret minimization in RLHF [16; 87; 90; 91; 61; 75]. These works propose and analyze adaptive algorithms that interact with the environment to learn highly-rewarding policies. Such policies are usually hard to deploy in practice because they may harm user experience due to over-exploration [22; 82]. Therefore, Zhu et al.  studied RLHF from ranking feedback in the offline setting with a fixed dataset. We study how to collect an _informative dataset for offline learning to rank_ with both absolute and ranking feedback. We approach this problem as an optimal design, a methodology for computing optimal information-gathering policies [66; 24]. The policies are non-adaptive and thus can be precomputed, which is one of their advantages. The main technical contribution of this work is a matrix generalization of the Kiefer-Wolfowitz theorem , which allows us to formulate optimal designs for ranked lists and solve them efficiently. Optimal designs have become a standard tool in exploration [45; 37; 38; 58; 33] and adaptive algorithms can be obtained by combining them with elimination. Therefore, optimal designs are a natural stepping stone to other solutions.

We make the following contributions:

1. We develop a novel approach for human preference elicitation. The key idea is to generalize the Kiefer-Wolfowitz theorem  to matrices (Section 3), which then allows us to compute information-gathering policies for ranked lists.
2. We propose an algorithm that uses an optimal design to collect absolute human feedback (Section 4.1), where a human provides noisy feedback for each item in the queried list. A least-squares estimator is then used to learn a preference model. The resulting algorithm is both computationally and statistically efficient. We bound its prediction error (Section 4.2) and ranking loss (Section 4.3), and show that both decrease with the sample size.
3. We propose an algorithm that uses an optimal design to collect ranking human feedback (Section 5.1), where a human ranks all items in the list according to their preferences. An estimator of Zhu et al.  is then used to learn a preference model. Our approach is both computationally and statistically efficient, and we bound its prediction error (Section 5.2) and ranking loss (Section 5.3). These results mimic the absolute feedback setting and show the generality of our framework.
4. We compare our algorithms to multiple baselines in several experiments. We observe that the algorithms achieve a lower ranking loss than the baselines.

## 2 Setting

**Notation:** Let \([K]=\{1,,K\}\). Let \(^{L}\) be the probability simplex over \([L]\). For any distribution \(^{L}\), we get \(_{i=1}^{L}(i)=1\). Let \(_{2}(K)=\{(j,k)[K]^{2}:j<k\}\) be the set of all pairs over \([K]\) where the first entry is lower than the second one. Let \(\|\|_{}^{2}=^{}\) for any positive-definite \(^{d d}\) and \(^{d}\). We use \(\) for the big-O notation up to logarithmic factors. Specifically, for any function \(f\), we write \((f(n))\) if it is \(O(f(n)^{k}f(n))\) for some \(k>0\). Let \(\) be the support of distribution \(\) or a random variable.

**Setup:** We learn to rank \(L\) lists, each with \(K\) items. An item \(k[K]\) in list \(i[L]\) is represented by a feature vector \(_{i,k}\), where \(^{d}\) is the set of feature vectors. The relevance of items is given by their mean rewards. The mean reward of item \(k\) in list \(i\) is \(_{i,k}^{}_{*}\), where \(_{*}^{d}\) is an unknown parameter. Without loss of generality, we assume that the original order of the items is optimal, \(_{i,j}^{}_{*}>_{i,k}^{} _{*}\) for any \(j<k\) and list \(i\). The agent does not know it. The agent interacts with humans for \(n\) rounds. At round \(t\), it selects a list \(I_{t}\) and the human provides stochastic feedback on it. Our goal is to design a policy for selecting the lists such that the agent learns the optimal order of all items in all lists after \(n\) rounds.

**Feedback model:** We study two models of human feedback, absolute and ranking:(1) In the _absolute feedback model_, the human provides a reward for each item in list \(I_{t}\) chosen by the agent. Specifically, the agent observes noisy rewards

\[y_{t,k}=_{I_{t},k}^{}_{*}+_{t,k}\,,\] (1)

for all \(k[K]\) in list \(I_{t}\), where \(_{t,k}\) is independent zero-mean \(1\)-sub-Gaussian noise. This feedback is stochastic and similar to that in the document-based click model .

(2) In the _ranking feedback model_, the human orders all items in list \(I_{t}\) selected by the agent. The feedback is a permutation \(_{t}:[K][K]\), where \(_{t}(k)\) is the index of the \(k\)-th ranked item. The probability that this permutation is generated is

\[p(_{t})=_{k=1}^{K}_{I_{t},_{t}(k)}^{ }_{*}]}{_{j=k}^{K}[_{I_{t},_{t}(j)} ^{}_{*}]}\,.\] (2)

Simply put, items with higher mean rewards are more preferred by humans and hence more likely to be ranked higher. This feedback model is known as the _Plackett-Luce (PL)_ model [65; 54; 102], and it is a standard assumption when learning values of individual choices from relative feedback. Since the feedback at round \(t\) is with independent noise, in both (1) and (2), any list can be observed multiple times and we do need to assume that \(n L\).

**Objective:** At the end of round \(n\), the agent outputs a permutation \(_{n,i}:[K][K]\) for all lists \(i[L]\), where \(_{n,i}(k)\) is the index of the \(k\)-th ranked item in list \(i\). We measure the quality of the solution by the _ranking loss_ after \(n\) rounds, which we define as

\[_{n}=_{i=1}^{L}_{j=1}^{K}_{k=j+1}^{K}\{_{n,i}(j)>_{n,i}(k)\}\,\,.\] (3)

The loss is the number of incorrectly ordered pairs of items in permutation \(_{n,i}\), summed over all lists \(i[L]\). It can also be viewed as the Kendall tau rank distance  between the optimal order of items in all lists and that according to \(_{n,i}\). We note that other ranking metrics exist, such as the _normalized discounted cumulative gain (NDCG)_ and _mean reciprocal rank (MRR)_. Our work can be extended to them and we leave this for future work.

The two closest related works are Mehta et al.  and Das et al. . They proposed algorithms for learning to rank \(L\) pairs of items from pairwise feedback. Their optimized metric is the maximum gap over the \(L\) pairs. We learn to rank \(L\) lists of \(K\) items from \(K\)-way ranking feedback. We bound the maximum prediction error, which is a similar metric to the prior works, and the ranking loss in (3), which is novel. Our setting is related to other bandit settings as follows. Due to the budget \(n\), it is similar to fixed-budget _best arm identification (BAI)_[13; 5; 6; 95]. The main difference is that we do not want to identify the best arm. We want to sort \(L\) lists of \(K\) items. Online learning to rank has also been studied extensively [67; 43; 105; 53; 44]. We do not minimize cumulative regret or try to identify the best arm. A more detailed comparison is in Appendix D.

We introduce optimal designs [66; 24] next. This allows us to minimize the expected ranking loss within a budget of \(n\) rounds efficiently.

## 3 Optimal Design and Matrix Kiefer-Wolfowitz

This section introduces a unified approach to human preference elicitation from both absolute and ranking feedback. First, we note that to learn the optimal order of items in all lists, the agent has to estimate the unknown model parameter \(_{*}\) well. In this work, the agent uses a _maximum-likelihood estimator (MLE)_ to obtain an estimate \(}_{n}\) of \(_{*}\). After that, it orders the items in all lists according to their estimated mean rewards \(_{i,k}^{}}_{n}\) in descending order, which defines the permutation \(_{n,i}\). It \(}_{n}\) minimized the prediction error \((_{i,k}^{}(}_{n}-_{ *}))^{2}\) over all items \(k[K]\) in list \(i\), the permutation \(_{n,i}\) would be closer to the optimal order. Moreover, if \(}_{n}\) minimized the maximum error over all lists, all permutations would be closer and the ranking loss in (3) would be minimized. This is why we focus on minimizing the _maximum prediction error_

\[_{i[L]}_{_{i}}(^{}(}_{n}-_{*}))^{2}=_{i[L]}(_{i}^{}(}_{n}-_{*})( }_{n}-_{*})^{}_{i})\,,\] (4)where \(_{i}\) is a matrix representing list \(i\) and \(_{i}\) is a column in it. In the absolute feedback model, the columns of \(_{i}\) are feature vectors of items in list \(i\) (Section 4.1). In the ranking feedback model, the columns of \(_{i}\) are the differences of feature vectors of items in list \(i\) (Section 5.1). Therefore, \(_{i}\) depends on the type of human feedback. In fact, as we show later, it is dictated by the covariance of \(}_{n}\) in the corresponding human feedback model. We note that the objective in (4) is worst-case over lists and that other alternatives, such as \(_{i=1}^{L}_{_{i}}(^{} (}_{n}-_{*}))^{2}\), may be possible. We leave this for future work.

We prove in Sections 4 and 5 that the agent can minimize the maximum prediction error in (4) and the ranking loss in (3) by sampling from a fixed distribution \(_{*}^{L}\). That is, the probability of selecting list \(i\) at round \(t\) is \((I_{t}=i)=_{*}(i)\). The distribution \(_{*}\) is a minimizer of

\[g()=_{i[L]}(_{i}^{}_{} ^{-1}_{i})\,,\] (5)

where \(_{}=_{i=1}^{L}(i)_{i}_{i}^{}\) is a _design matrix_. The _optimal design_ aims to find the distribution \(_{*}\). Since (5) does not depend on the received feedback, our algorithms are not adaptive.

The problem of finding \(_{*}\) that minimizes (5) is called the _\(G\)-optimal design_. The minimum of (5) and the support of \(_{*}\) are characterized by the Kiefer-Wolfowitz theorem . The original theorem is for least-squares regression, where \(_{i}\) are feature vectors. At a high level, it says that the smallest ellipsoid that covers all feature vectors has the minimum volume, and in this way relates the minimization of (5) to maximizing \((_{})\). We generalize this claim to lists, where \(_{i}\) is a matrix of feature vectors representing list \(i\). This generalization allows us to go from a design over feature vectors to a design over lists represented by matrices.

**Theorem 1** (Matrix Kiefer-Wolfowitz).: _Let \(M 1\) be an integer and \(_{1},,_{L}^{d M}\) be \(L\) matrices whose column space spans \(^{d}\). Then the following claims are equivalent:_

1. \(_{*}\) _is a minimizer of_ \(g()\) _in (_5_)._
2. \(_{*}\) _is a maximizer of_ \(f()=(_{})\)_._
3. \(g(_{*})=d\)_._

_Furthermore, there exists a minimizer \(_{*}\) of \(g()\) such that \(|(_{*})| d(d+1)/2\)._

Proof.: We generalize the proof of the Kiefer-Wolfowitz theorem in Lattimore and Szepesvari . The key observation is that even if \(_{i}\) is a matrix and not a vector, the design matrix \(_{}\) is positive definite. Using this structure, we establish the key facts used in the original proof. First, we show that \( f()=((_{i}^{}_{}^{-1} _{i}))_{i=1}^{L}\) is the gradient of \(f()\) with respect to \(\). In addition, we prove that \(g()_{i=1}^{L}(i)(_{i}^{}_{}^{-1}_{i})=d\). The complete proof is in Appendix A.1. 

From the equivalence in Theorem 1, it follows that the agent should solve the optimal design

\[_{*}=*{arg\,max}_{^{L}}f()=*{arg\, max}_{^{L}}(_{})\] (6)

and sample according to \(_{*}\) to minimize the maximum prediction error in (4). Note that the optimal design over lists in (6) is different from the one over vectors . As an example, suppose that we have \(4\) feature vectors \(\{_{i}\}_{i}\) and two lists: \(_{1}=(_{1},_{2})\) and \(_{2}=(_{3},_{4})\). The list design is over \(2\) variables (lists) while the vector design is over \(4\) variables (vectors). The list design can also be viewed as a constrained vector design, where \((_{1},_{2})\) and \((_{3},_{4})\) are observed together with the same probability.

The optimization problem in (6) is convex and thus easy to solve. When the number of lists is large, the Frank-Wolfe algorithm  can be used, which solves convex optimization problems with linear constraints as a sequence of linear programs. We use CVXPY  to compute the optimal design. We report its computation time, as a function of the number of lists \(L\), in Appendix E. The computation time scales roughly linearly with the number of lists \(L\). In the following sections, we employ Theorem 1 to bound the maximum prediction error and ranking loss for both absolute and ranking feedback.

```
1:for\(i=1,,L\)do
2:\(_{i}[_{i,k}]_{k[K]}\)
3:\(_{}_{i=1}^{L}(i)_{i}_{i}^{}\)
4:\(_{*}_{^{L}}(_{})\)
5:for\(t=1,,n\)do
6: Sample \(I_{t}_{*}\)
7:for\(k=1,,K\)do
8: Observe \(y_{t,k}\) in (1)
9: Compute \(}_{n}\) in (7)
10:for\(i=1,,L\)do
11: Set \(_{n,i}(k)\) to the index of the item with the \(k\)-th highest \(_{i,}^{}}_{n}\) in list \(i\)
12:Output: Permutation \(_{n,i}\) for all \(i[L]\) ```

**Algorithm 1** Dope for absolute feedback.

## 4 Learning with Absolute Feedback

This section is organized as follows. In Section 4.1, we present an algorithm for human preference elicitation under absolute feedback. We bound its prediction error in Section 4.2 and its ranking loss in Section 4.3.

### Algorithm Dope

Our algorithm for absolute feedback is called **D**-optimal **p**reference **e**licitation (Dope). It has four main parts. First, we solve the optimal design in (6) to get a data logging policy \(_{*}\). The matrix for list \(i\) is \(_{i}=[_{i,k}]_{k[K]}^{d K}\), where \(_{i,k}\) is the feature vector of item \(k\) in list \(i\). Second, we collect human feedback for \(n\) rounds. At round \(t[n]\), we sample a list \(I_{t}_{*}\) and then observe \(y_{t,k}\) for all \(k[K]\), as defined in (1). Third, we estimate the model parameter using least squares

\[}_{n}=}_{n}^{-1}_{t=1}^{n}_{k=1}^{ K}_{I_{t},k}y_{t,k}\,.\] (7)

The normalized and unnormalized covariance matrices corresponding to the estimate are

\[_{n}=}_{n}\,,}_{n}=_{t=1}^{n}_{k=1}^{K}_{I_{t},k}_{I_{t },k}^{}\,,\] (8)

respectively. Finally, we sort the items in all lists \(i\) according to their estimated mean rewards \(_{i,k}^{}}_{n}\) in descending order, to obtain the permutation \(_{n,i}\). The pseudo-code of Dope is in Algorithm 1.

```
1:for\(i=1,,L\)do
2:for\((j,k)_{2}(K)\)do
3:\(_{i,j,k}_{i,j}-_{i,k}\)
4:\(_{i}[_{i,j,k}]_{(j,k)_{2}(K)}\)
5:\(_{}_{i=1}^{L}(i)_{i}_{i}^{}\)
6:\(_{*}_{^{L}}(_{})\)
7:for\(t=1,,n\)do
8: Sample \(I_{t}_{*}\)
9: Observe \(_{t}\) in (2)
10: Compute \(}_{n}\) in (10)
11:for\(i=1,,L\)do
12: Set \(_{n,i}(k)\) to the index of the item with the \(k\)-th highest \(_{i,}^{}}_{n}\) in list \(i\)
13:Output: Permutation \(_{n,i}\) for all \(i[L]\) ```

**Algorithm 2** Dope for ranking feedback.

The estimator (7) is the same as in _ordinary least squares (OLS)_, because each observed list can be treated as \(K\) independent observations. The matrix for list \(i\), \(_{i}\), can be related to the inner sum in (8) through \((_{i}_{i}^{})=_{k=1}^{K} _{i,k}_{i,k}^{}\). Therefore, our algorithm collects data for a least-squares estimator by optimizing its covariance .

### Maximum Prediction Error Under Absolute Feedback

In this section, we bound the maximum prediction error of Dope under absolute feedback. We start with a lemma that uses the optimal design \(_{*}\) to bound \(_{i[L]}_{_{i}}\|\|_{}_{n}^{-1}}^{2}\).

**Lemma 2**.: _Let \(_{*}\) be the optimal design in (6). Fix budget \(n\) and let each allocation \(n_{*}(i)\) be an integer. Then \(_{i[L]}_{_{i}}\|\|_{}_{n}^{-1}}^{2}=d/n\)._

The lemma is proved in Appendix A.2. Since all \(n_{*}(i)\) are integers, we note that \(}_{n}\) must be full rank and invertible. Note that the assumption of all \(n_{*}(i)\) being integers does not require \(n L\)This is because \(_{*}(i)\) has at most \(d(d+1)/2\) non-zero entries (Theorem 1). This is independent of the number of lists \(L\), which could also be infinite (Chapter 21.1 in Lattimore and Szepesvari ). The integer condition can be also relaxed by rounding non-zero entries of \(n_{*}(i)\) up to the closest integer. This clearly yields an integer allocation of size at most \(n+d(d+1)/2\). All claims in our work would hold for any \(_{*}\) and this allocation. With Lemma 2 in hand, the maximum prediction error is bounded as follows.

**Theorem 3** (Maximum prediction error).: _With probability at least \(1-\), the maximum prediction error after \(n\) rounds is_

\[_{i[L]}(_{i}^{}(}_{n}- _{*})(}_{n}-_{*})^{}_{i} )=O(+d(1/)}{n})\,.\]

The theorem is proved in Appendix A.3. As in Lemma 2, we assume that each allocation \(n_{*}(i)\) is an integer. If the allocations were not integers, rounding errors would arise and need to be bounded [66; 25; 37]. At a high level, our bound would be multiplied by \(1+\) for some \(>0\) (Chapter 21 in Lattimore and Szepesvari ). We omit this factor in our proofs to simplify them.

Theorem 3 says that the maximum prediction error is \((d^{2}/n)\). Note that this rate cannot be attained trivially, for instance by uniform sampling. To see this, consider the following example. Take \(K=2\). Let \(_{i,1}=(1,0,0)\) for \(i[L-1]\) and \(_{L,1}=(0,1,0)\), and \(_{i,2}=(0,0,1)\) for all \(i[L]\). In this case, the minimum eigenvalue of \(}_{n}\) is \(n/L\) in expectation, because only one item in list \(L\) provides information about the second feature, \(_{L,1}=(0,1,0)\). Following the proof of Theorem 3, we would get a rate of \((dL/n)\). Prior works on optimal designs also made similar observations .

The rate in Theorem 3 is the same as in linear models . Specifically, by the Cauchy-Schwarz inequality, we would get

\[(^{}(}_{n}-_{*}))^{2}\|}_{n}-_{*}\|_{}_{n}}^{2}\|\|_{ }_{n}^{-1}}^{2}=(d)\,(d/n)=(d^{2 }/n)\]

with a high probability, where \(_{*}\), \(}_{n}\), and \(}_{n}\) are the analogous linear model quantities. This bound holds for infinitely many feature vectors. It can be tightened to \((d/n)\) for a finite number of feature vectors, where \(\) hides the logarithm of the number of feature vectors. This can be proved using a union bound over (20.3) in Chapter 20 of Lattimore and Szepesvari .

### Ranking Loss Under Absolute Feedback

In this section, we bound the expected ranking loss under absolute feedback. Recall from Section 2 that the original order of items in each list is optimal. With this in mind, the _gap_ between the mean rewards of items \(j\) and \(k\) in list \(i\) is \(_{i,j,k}=(_{i,j}-_{i,k})^{}_{*}\), for any \(i[L]\) and \((j,k)_{2}(K)\).

**Theorem 4** (Ranking loss).: _The expected ranking loss after \(n\) rounds is bounded as_

\[[_{n}] 2_{i=1}^{L}_{j=1}^{K}_{k=j+1}^{K} [-^{2}n}{8d}]\,.\]

Proof.: From the definition of the ranking loss, we have

\[[_{n}]=_{i=1}^{L}_{j=1}^{K}_{k=j+1}^{K} [\{_{n,i}(j)>_{n,i}(k)\}]=_ {i=1}^{L}_{j=1}^{K}_{k=j+1}^{K}(_{i,j}^{ }}_{n}<_{i,k}^{}}_{n})\,,\]

where \((_{i,j}^{}}_{n}<_{i,k}^ {}}_{n})\) is the probability of predicting a sub-optimal item \(k\) above item \(j\) in list \(i\). We bound this probability from above by bounding the sum of \((_{i,k}^{}(}_{n}-_{*}) >}{2})\) and \((_{i,j}^{}(_{*}-}_{n} )>}{2})\). Each of these probabilities is bounded from above by \([-^{2}n}{8d}]\), using a concentration inequality in Lemma 8. The full proof is in Appendix A.4. 

Each term in Theorem 4 can be bounded from above by \([-^{2}n}{8d}]\), where \(n\) is the sample size, \(d\) is the number of features, and \(_{}\) denotes the minimum gap. Therefore, the bound decreasesexponentially with budget \(n\) and gaps, and increases with \(d\). This dependence is similar to that in Theorem 1 of Azizi et al.  for fixed-budget best-arm identification in linear models. Yang and Tan  derived a similar bound and a matching lower bound. The gaps \(_{i,j,k}\) reflect the hardness of sorting list \(i\), which depends on the differences of the mean rewards of items \(j\) and \(k\) in it.

Finally, we wanted to note that our optimal designs may not be optimal for ranking. We have not focused solely on ranking because we see value in both prediction error (Theorem 3) and ranking loss (Theorem 4) bounds. The fact that we provide both shows the versatility of our approach.

## 5 Learning with Ranking Feedback

This section is organized similarly to Section 4. In Section 5.1, we present an algorithm for human preference elicitation under ranking feedback. We bound its prediction error in Section 5.2 and its ranking loss in Section 5.3. Our algorithm design and analysis are under the following assumption, which we borrow from Zhu et al. .

**Assumption 1**.: _We assume that the model parameter satisfies \(_{*}\), where_

\[=\{^{d}:^{}_{d}=0, \|\|_{2} 1\}\,.\] (9)

_We also assume that \(_{i[L],\,k[K]}\|_{i,k}\|_{2} 1\)._

The assumption of bounded model parameter and feature vectors is common in bandits . The additional assumption of \(^{}_{d}=0\) is from Zhu et al. , from which we borrow the estimator and concentration bound.

### Algorithm Dope

Our algorithm for ranking feedback is similar to Dope in Section 4. It also has four main parts. First, we solve the optimal design problem in (6) to obtain a data logging policy \(_{*}\). The matrix for list \(i\) is \(_{i}=[_{i,j,k}]_{(j,k)_{2}(K)}^{d K (K-1)/2}\), where \(_{i,j,k}=_{i,j}-_{i,k}\) is the difference of feature vectors of items \(j\) and \(k\) in list \(i\). Second, we collect human feedback for \(n\) rounds. At round \(t[n]\), we sample a list \(I_{t}_{*}\) and then observe \(_{t}\) drawn from the PL model, as defined in (2). Third, we estimate the model parameter as

\[}_{n}=*{arg\,min}_{} _{n}()\,,_{n}()=-_{t=1}^{n} _{k=1}^{K}(_{I_{t},_{t}(k)}^{} ]}{_{j=k}^{K}[_{I_{t},_{t}(j)}^{}]})\,,\] (10)

where \(\) is defined in Assumption 1. We solve this estimation problem using _iteratively reweighted least squares (IRLS)_, a popular method for fitting the parameters of _generalized linear models (GLMs)_. Finally, we sort the items in all lists \(i\) according to their estimated mean rewards \(_{i,k}^{}}_{n}\) in descending order, to obtain the permutation \(_{n,i}\). The pseudo-code of Dope is in Algorithm 2.

The optimal design for (10) is derived as follows. First, we derive the Hessian of \(_{n}()\), \(^{2}_{n}()\), in Lemma 9. The optimal design with \(^{2}_{n}()\) cannot be solved exactly because \(^{2}_{n}()\) depends on an unknown model parameter \(\). To get around this, we bound \(\)-dependent terms from below. Many prior works on decision making under uncertainty with GLMs  took a similar approach. We derive normalized and unnormalized covariance matrices

\[_{n}=}_{n}\,, }_{n}=_{t=1}^{n}_{j=1}^{K}_{k=j+1}^{K}_{I_{t},j,k}_{I_{t},j,k}^{}\,,\] (11)

and prove that \(^{2}_{n}()_{n}\) for some \(>0\). Therefore, we can maximize \((^{2}_{n}())\), for any \(\), by maximizing \((_{n})\). The matrix for list \(i\), \(_{i}\), can be related to the inner sum in (11) through \((_{i}_{i}^{})=_{j=1}^{K}_{k=j+1 }^{K}_{i,j,k}_{i,j,k}^{}\).

The price to pay for our approximation is a constant \(C>0\) in our bounds (Theorems 5 and 6). In Appendix C, we discuss a more adaptive design and also compare to it empirically. We conclude that it would be harder to implement and analyze, and we do not observe empirical benefits at \(K=2\).

### Maximum Prediction Error Under Ranking Feedback

In this section, we bound the maximum prediction error of Dope under ranking feedback. Similarly to the proof of Theorem 3, we decompose the error into two parts, which capture the efficiency of the optimal design and the uncertainty in the MLE \(}_{n}\).

**Theorem 5** (Maximum prediction error).: _With probability at least \(1-\), the maximum prediction error after \(n\) rounds is_

\[_{i[L]}(_{i}^{}(}_{n}- _{*})(}_{n}-_{*})^{}_{i}) =O((d^{2}+d(1/))}{n})\,.\]

This theorem is proved in Appendix A.5. We build on a self-normalizing bound of Zhu et al. (2012), \(\|}_{n}-_{*}\|_{_{n}}^{2} O( (d+(1/))}{n})\), which may not be tight in \(K\). If the bound could be improved by a multiplicative \(c>0\), we would get a multiplicative \(c\) improvement in Theorem 5. Note that if the allocations \(n_{*}(i)\) are not integers, a rounding procedure is necessary (Zhu et al., 2017; Zhang et al., 2017). This would result in an additional multiplicative \(1+\) in our bound, for some \(>0\). We omit this factor in our derivations to simplify them.

### Ranking Loss Under Ranking Feedback

In this section, we bound the expected ranking loss under ranking feedback. Similarly to Section 4.3, we define the _gap_ between the mean rewards of items \(j\) and \(k\) in list \(i\) as \(_{i,j,k}=_{i,j,k}^{}_{*}\), where \(_{i,j,k}=_{i,j}-_{i,k}\) is the difference of feature vectors of items \(j\) and \(k\) in list \(i\).

**Theorem 6** (Ranking loss).: _The expected ranking loss after \(n\) rounds is bounded as_

\[[_{n}]_{i=1}^{L}_{j=1}^{K}_{k=j+1}^{K} [-^{2}n}{CK^{4}d}+d]\,,\]

_where \(C>0\) is a constant._

Proof.: The proof is similar to Theorem 4. At the end of round \(n\), we bound the probability that a sub-optimal item \(k\) is ranked above item \(j\). The proof has two parts. First, for any list \(i[L]\) and items \((j,k)_{2}(K)\), we show that \((_{i,j}^{}}_{n}<_{i,k} ^{}}_{n})=(_{i,j,k}^{} (_{*}-}_{n})>_{i,j,k})\). Then we bound this quantity by \([-^{2}n}{CK^{4}d}+d]\). The full proof is in Appendix A.6. 

The bound in Theorem 6 is similar to that in Theorem 4, with the exception of multiplicative \(K^{-4}\) and additive \(d\). The leading term inside the sum can be bounded by \([-^{2}n}{CK^{4}d}]\), where \(n\) is the sample size, \(d\) is the number of features, and \(_{}\) is the minimum gap. Therefore, similarly to Theorem 4, the bound decreases exponentially with budget \(n\) and gaps, and increases with \(d\). This dependence is similar to Theorem 2 of Azizi et al. (2017) for fixed-budget best-arm identification in GLMs. Our bound does not involve the extra factor of \(>0\) because we assume that all vectors lie in a unit ball (Assumption 1).

## 6 Experiments

The goal of our experiments is to evaluate Dope empirically and compare it to baselines. All methods estimate \(}_{n}\) using (7) or (10), depending on the feedback. To guarantee that these problems are well defined, even if the sample covariance matrix \(}_{n}\) is not full rank, we regularize both objectives with \(\|\|_{2}^{2}\), for a small \(>0\). This mostly impacts small sample sizes. Specifically, since the optimal design collects diverse feature vectors, \(}_{n}\) is likely to be full rank for large sample sizes. After \(}_{n}\) is estimated, each method ranks items in all lists based on their estimated mean rewards \(_{i,k}^{}}_{n}\). The performance of all methods is measured by their ranking loss in (3) divided by \(L\). All experiments are averaged over \(100\) independent runs, and we report results in Figure 1. We compare the following algorithms:

**(1)** Dope: This is our method. We solve the optimal design problem in (6) and then sample lists \(I_{t}\) according to \(_{*}\).

**(2)** Unif: This baseline chooses lists \(I_{t}\) uniformly at random from \([L]\). While simple, it is known to be competitive in real-world problems where feature vectors may cover the feature space close to uniformly .

**(3)** Avg-Design: The exploration policy is an optimal design over feature vectors. The feature vector of list \(i\) is the mean of the feature vectors of all items in it, \(}_{i}=_{k=1}^{K}_{i,k}\). After the design is computed, we sample lists \(I_{t}\) according to it. The rest is the same as in Dope. This baseline shows that our list representation with multiple feature vectors can outperform more naive choices.

**(4)** Clustered-Design: This approach uses the same representation as Avg-Design. The difference is that we cluster the lists using \(k\)-medoids. Then we sample lists \(I_{t}\) uniformly at random from the cluster centroids. The rest is the same as in Avg-Design. This baseline shows that Dope outperforms other notions of diversity, such as obtained by clustering. We tune \(k\) (\(k=10\) in the Nectar dataset and \(k=6\) otherwise) and report only the best results.

**(5)** APO: This method was proposed in Das et al.  and is the closest related work. APO greedily minimizes the maximum error in pairwise ranking of \(L\) lists of length \(K=2\). We extend it to \(K>2\) as follows. First, we turn \(L\) lists of length \(K\) into \(L\) lists of length \(2\), one for each pair of items in the original lists. Then we apply APO to these \(L\) lists of length \(2\).

Pure exploration algorithms are often compared to cumulative regret baselines . Since our problem is a form of learning to rank, _online learning to rank (OLTR)_ baselines  seem natural. We do not compare to them for the following reason. The problem of an optimal design over lists is to design a distribution over queries. All OLTR algorithms solve a different problem, return a ranked list of items conditioned on a query chosen by the environment. Since they do not choose the queries, they cannot solve our problem.

**Synthetic experiment 1 (absolute feedback):** We have \(L=400\) questions and represent them by random vectors \(_{i}[-1,1]^{6}\). Each question has \(K=4\) answers. For each question, we generate \(K\) random answers \(_{i,k}[-1,1]^{6}\). Both the question and answer vectors are normalized to unit length.

Figure 1: Ranking loss of all compared methods as a function of the number of rounds. The error bars are one standard error of the estimates.

For each question-answer pair \((i,k)\), the feature vector is \(_{i,k}=(_{i}_{i,k}^{})\) and has length \(d=36\). The outer product captures cross-interaction terms of the question and answer representations. A similar technique has been used for feature preprocessing of the Yahoo! Front Page Today Module User Click Log Dataset [50; 103; 7; 51]. We choose a random \(_{*}^{d}\). The absolute feedback is generated as in (1). Our results are reported in Figure 1(a). We note that the ranking loss of Dope decreases the fastest among all methods, with Unif, Avg-Design, and APO being close second.

**Synthetic experiment 2 (ranking feedback):** This experiment is similar to the first experiment, except that the feedback is generated by the PL model in (2). Our results are reported in Figure 1(b) and we observe again that the ranking loss of Dope decreases the fastest. The closest baselines are Unif, Avg-Design, and APO. Their lowest ranking loss (\(n=100\)) is attained by Dope at \(n=60\), which is nearly a two-fold reduction in sample size. In Appendix E, we conduct additional studies on this problem. We vary the number of lists \(L\) and items \(K\), and report the computation time and ranking loss.

**Experiment 3 (Nectar dataset):** The Nectar dataset  is a dataset of \(183\)k questions, each with \(7\) answers. We take a subset of this dataset: \(L=2\,000\) questions and \(K=5\) answers. The answers are generated by GPT-4, GPT-4-0613, GPT-3.5-turbo, GPT-3.5-turbo-instruct, and Anthropic models. We embed the questions and answers in \(768\) dimensions using Instruct embeddings . Then we project them to \(^{10}\) using a random projection matrix. The feature vector of answer \(k\) to question \(i\) is \(_{i,k}=(_{i}_{i,k}^{})\), where \(_{i}\) and \(_{i,k}\) are the projected embeddings of question \(i\) and answer \(k\), respectively. Hence \(d=100\). The ranking feedback is simulated using the PL model in (2). We estimate its parameter \(_{*}^{d}\) from the ranking feedback in the dataset using the MLE in (10). Our results are reported in Figure 1(c). We observe that the ranking loss of Dope is the lowest. The closest baseline is APO. Its lowest ranking loss (\(n=500\)) is attained by Dope at \(n=150\), which is more than a three-fold reduction in sample size.

**Experiment 4 (Anthropic dataset):** The Anthropic dataset  is a dataset of \(161\)k questions with two answers per question. We take a subset of \(L=2\,000\) questions. We embed the questions and answers in \(768\) dimensions using Instructor embeddings . Then we project them to \(^{6}\) using a random projection matrix. The feature vector of answer \(k\) to question \(i\) is \(_{i,k}=(_{i}_{i,k}^{})\), where \(_{i}\) and \(_{i,k}\) are the projected embeddings of question \(i\) and answer \(k\), respectively. Hence \(d=36\). The ranking feedback is simulated using the PL model in (2). We estimate its parameter \(_{*}^{d}\) from the feedback in the dataset using the MLE in (10). Our results are reported in Figure 1(d). We note again that the ranking loss of Dope is the lowest. The closest baselines are Unif, Avg-Design, and APO. Their lowest ranking loss (\(n=1\,000\)) is attained by Dope at \(n=300\), which is more than a three-fold reduction in sample size.

## 7 Conclusions

We study the problem of optimal human preference elicitation for learning preference models. The problem is formalized as learning to rank \(K\) answers to \(L\) questions under a budget on the number of asked questions. We consider two feedback models: absolute and ranking. The absolute feedback is motivated by how humans assign relevance judgments in search [30; 57]. The ranking feedback is motivated by learning reward models in RLHF [39; 68; 36; 15; 77; 17]. We address both settings in a unified way. The key idea in our work is to generalize optimal designs [41; 45], a methodology for computing optimal information-gathering policies, to ranked lists. After the human feedback is collected, we learn preference models using existing estimators. Our method is statistically efficient, computationally efficient, and can be analyzed. We bound its prediction errors and ranking losses, in both absolute and ranking feedback models, and evaluate it empirically to show that it is practical.

Our work can be extended in several directions. First, we study only two models of human feedback: absolute and ranking. However, many feedback models exist . One common property of these models is that learning of human preferences can be formulated as likelihood maximization. In such cases, an optimal design exists and can be used for human preference elicitation, exactly as in our work. Second, while we bound the prediction errors and ranking losses of Dope, we do not derive matching lower bounds. Therefore, although we believe that Dope is near optimal, we do not prove it. Third, we want to extend our methodology to the fixed-confidence setting. Finally, we want to apply our approach to learning reward models in LLMs and evaluate it.