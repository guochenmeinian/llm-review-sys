# Saliency-driven Experience Replay for Continual Learning

Giovanni Bellitto

University of Catania

giovanni.bellitto@unict.it  Federica Proietto Salanitri

University of Catania

federica.proiettosalanitri@unict.it  Matteo Pennisi

University of Catania

matteo.pennisi@phd.unict.it  Matteo Boschini

University of Modena and Reggio Emilia

matteo.boschini@unimore.it  Lorenzo Bonicelli

University of Modena and Reggio Emilia

lorenzo.bonicelli@unimore.it  Angelo Porrello

University of Modena and Reggio Emilia

angelo.porrello@unimore.it  Simone Calderara

University of Modena and Reggio Emilia

simone.calderara@unimore.it  Simone Palazzo

University of Catania

simone.palazzo@unict.it  Conectto Spampinato

University of Catania

conectto.spampinato@unict.it

###### Abstract

We present _Saliency-driven Experience Replay_ - SER - a biologically-plausible approach based on replicating human visual saliency to enhance classification models in continual learning settings. Inspired by neurophysiological evidence that the primary visual cortex does not contribute to object manifold untangling for categorization and that primordial saliency biases are still embedded in the modern brain, we propose to employ auxiliary saliency prediction features as a modulation signal to drive and stabilize the learning of a sequence of non-i.i.d. classification tasks. Experimental results confirm that SER effectively enhances the performance (in some cases up to about twenty percent points) of state-of-the-art continual learning methods, both in class-incremental and task-incremental settings. Moreover, we show that saliency-based modulation successfully encourages the learning of features that are more robust to the presence of spurious features and to adversarial attacks than baseline methods. Code is available at: https://github.com/perceivelab/SER.

## 1 Introduction

Humans possess the remarkable capability to keep learning, with limited forgetting of past experience, and to quickly re-adapt to new tasks and problems without disrupting consolidated knowledge. Machine learning, on the contrary, has shown significant limitations when dealing with non-stationary data streams with a limited possibility to replay past examples. The main reason for this shortcomingcan be found in the inherent structure, organization and optimization approaches of artificial neural networks, which differ significantly from how humans learn and how their neural connectivity is built when accumulating knowledge over a lifetime. According to the _Complementary Learning Systems (CLS) theory_, the human ability to learn effectively may be due to the interplay between two learning processes that originate, respectively, on the hippocampus and on the neocortex. This theory has inspired several continual learning methods . In particular, the recent DualNet method  translates CLS concepts into a computational framework for continual learning. Specifically, it employs two learning networks: a _slow learner_, emulating the memory consolidation process happening in the hippocampus through contrastive learning techniques, and a _fast learner_, that aims at adapting current representations to new observations. However, this strategy still appears insufficient for addressing the problem of continual learning, because it starts from the (possibly wrong) assumption that human neural networks directly process visual input with the objective of performing categorization from early vision layers. On the contrary, neurophysiological studies  are in near universal agreement that the object manifolds conveyed to primary visual cortex V1 (one of the earliest areas involved in vision) are as tangled as the pixel space. In other words, the neurons of the earliest vision areas do not contribute to object manifold untangling for categorization, but rather enforce luminance and contrast robustness . This suggests that training early neurons with a visual categorization objective -- as done not only in DualNet, but in all existing continual learning methods -- is in stark contrast to the biological counterparts observed in primates. Moreover, recent studies on the causes of forgetting in artificial neural networks showed that deeper layers (i.e., closer to the output) are less stable in presence of task shifts , which is consistent with the hypothesis that earlier layers do not bear specific categorization responsibilities.

Given these premises, it is peculiar that existing bio-inspired continual learning methods tend to ignore all upstream neural processes underlying visual categorization, such as visual saliency processes. Indeed, the ability to select relevant visual information appears to be the hallmark of human/primate cognition. Moreover, recent findings in cognitive neuroscience have shown that the visual attention priorities of human hunter-gatherer ancestors are still embedded in the modern brain : humans pay attention faster to animals than to vehicles, although we now see more vehicles than animals. This primordial saliency bias embedded in human brains suggests that the neuronal circuits of the ventral visual pathway are somehow inherited, as a form of genetic legacy from ancestral experience, and tend to remain stable over time -- thus not subject to forgetting, though we have long stopped hunting to survive. Interestingly, we observed the same **forgetting-free** behavior for saliency prediction on artificial neural networks. Fig. 1 shows the trend of the _similarity_ metric for a saliency prediction model trained in a continual learning scenario, and compares it to the accuracy of a classification model under the same settings. While classification accuracy drops as the classifier learns new classes, the saliency metric remains stable, and even slightly improves.

From this observation, in this paper we propose _SER_, a _Saliency-driven Experience Replay_ strategy that employs visual saliency prediction  to drive the learning of a sequence of classification tasks in a continual learning setting. To emulate what has been observed in primates, where visual saliency modulates the firing rate of neurons that represent the attended stimulus at different stages of

Figure 1: **Comparison of Forgetting-Free Saliency Prediction vs. Catastrophic Forgetting in Classifiers and Activation Maps in Continual Learning Scenarios. _(Left figure)_: The saliency accuracy (measured by the _similarity_ score) of a saliency predictor trained in a continual learning setting improves as more tasks are introduced, while the classification accuracy of a continual classifier degrades over time, indicating that saliency detection remains i.i.d. even with non-i.i.d. data. _(Right figure)_: The top row shows activation maximization maps via GradCAM, which are prone to catastrophic forgetting due to their dependence on the classifier. In contrast, the bottom row shows saliency maps produced by the predictor, which remain stable and consistent over time.**

visual processing [63; 45], SER adopts a two-branch model: one branch performs visual saliency prediction [37; 27; 20], and its responses modulate the features learned by a paired classification model in the second branch.

While the SER strategy stands out in its approach, it's important to note a similar category of methodologies that utilize attribution maps (e.g., computed via GradCAM), also known as attention maps, as a distilled form of classifier knowledge for future replay [61; 18; 22; 59; 3]. However, **saliency prediction maps are significantly different from attribution maps**. Indeed, attribution maps elucidate the inner workings of DNNs by highlighting relevant input features for predictions and as such they suffer catastrophic forgetting (as shown in Fig. 1), while saliency maps, rooted in neuroscience and human visual processing, aim to emulate how humans perceive and prioritize visual information, and, most importantly, they are forgetting-free.

SER is model-agnostic and can be used in combination to any continual learning method. We demonstrate that saliency modulation positively impacts classification performance in online continual learning settings, leading to a significant gain in accuracy (up to 20 percent points) w.r.t. baseline methods. We further demonstrate the usefulness of saliency modulation on different benchmarks (including a challenging one that tackles fine-grained classification) and substantiate our claims through a set of ablation studies. We finally show that saliency modulation, besides being biologically plausible, leads to learn saliency-modulated features that are more robust to the presence of spurious features and to adversarial attacks.

## 2 Related Work

Continual Learning (CL) [47; 16; 49] addresses the problem of _catastrophic forgetting_ in neural networks, wherein they tend to lose previously acquired knowledge when faced with shifts in input data distribution. Various solutions have been proposed to address this, including the incorporation of regularization terms [31; 74], specific architectural designs [60; 44], and rehearsal of previously encountered data points [57; 55; 9]. However, the application of these solutions to real-world scenarios is challenging due to evaluations often being based on unrealistic benchmarks [1; 65]. _Online Continual Learning_ (OCL)  addresses this challenge by limiting multiple epochs on the input stream, reflecting the realistic assumption that data points encountered in real-world settings occur only once. To address this challenge, many strategies adopt a replay approach [54; 57]. Some focus on memory management: GSS  optimizes the basic rehearsal formula to store maximally informative samples, while HAL  identifies synthetic replay data points maximally affected by forgetting. CoPE  employs class prototypes for gradual evolution of the shared latent space, while ER-ACE adjusts the cross-entropy loss asymmetrically to minimize task imbalance. Our proposal adopts a remarkably different approach w.r.t. these classes of methods, in that we take inspiration from cognitive neuroscience theory of learning and exploiting the features of a conjugate forgetting-free task (i.e., saliency prediction) to modulate the responses of our OCL model. Doing so produces a stabilizing effect on our model and makes it more resilient to forgetting.

An approach similar in the spirit to ours is  that leverages saliency prediction for exemplar-free class incremental learning. To compensate for the absence of past task data, this methods relies on

Figure 2: **Architecture of the proposed Saliency-driven Experience Replay (SER) strategy.** The classification backbone is paired with a saliency prediction network that, given its capability of being forgetting-free, aims at adjusting the learned classification features in order to mitigate overall forgetting.

a pre-trained saliency detector, which remains frozen throughout the learning process, providing guidance for attribution maps of the classification backbone. Consequently, it tackles the challenge of forgetting by employing a pre-trained backbone to constrain feature drift. In contrast, SER operates on a dynamic framework where the visual saliency network is continuously trained, showcasing remarkable resistance to forgetting, while concurrently modulating the drift of classification features. This approach offers a more flexible visual saliency-classification paradigm that adapts to any dataset without external dependencies, as opposed to , which requires the use of a pre-trained saliency detector trained on the same data distribution as the target data.

Another approach, similarly inspired by cognitive theories, is DualNet , which employs two networks that loosely emulate how slow and fast learning work in humans. However, DualNet employs contrastive learning on the slow network (the earliest layers of the model), while it seems that object-identifying transformations happens later in the human visual system [19; 32]. Our results, reported later, substantiate the suitability of our choice to use low-level processes, such as saliency prediction, to drive continual learning tasks, rather than contrastive learning or classification pre-training techniques as, respectively, in DualNet and TwF .

Though the concept of utilizing saliency prediction maps in online continual learning is relatively new, recent trends have shown promising advancements in mitigating forgetting by encouraging models to recall evidence for past decisions, stored as activation maps . Specifically, [22; 59; 3] employ attribution methods, such as Gradient-weighted Class Activation Mapping (Grad-CAM) , to compute and store visual model explanations for each sample (or parts thereof) in the buffer and ensures model consistency with previous decisions during the training phase. Similarly, Dhar  adopts Grad-CAM, but it does not store any information, it employs knowledge distillation on the activation maps across consecutive tasks. However, as presented in the introduction, there is a fundamental distinction between saliency maps and activation maps with the latter being subject to forgetting, while the former not (Fig. 1).

Finally, our approach diverges from the recent trend in the continual learning (CL) field, which primarily employs foundation models (mostly Vision Transformers, ViTs) and focuses on learning prompts to mitigate forgetting [68; 67; 24; 62]. The main limitation of these methods is that they are restricted to transformer-based architectures. In contrast, our strategy does not rely on any specific model type, thereby enhancing its potential impact on real-world applications.

## 3 Method

### Online Continual Learning

Following the recent literature, we pose OCL as a supervised image classification problem with an online non-i.i.d. stream of data, where each training sample is only seen once. Although our saliency-driven modulation does not require the presence or knowledge of _task boundaries_, in this formulation and in our experiments we assume that these are given, to the benefit of any baseline method enhanced by the proposed extension. More formally, let \(=\{_{1},,_{T}\}\) be a sequence of data streams, where each pair \((,y)_{i}\) denotes a data point \(\) with the corresponding class label \(y\); the sample distributions (in terms of both the data point and the class label) differ between separate streams \(_{i}\) and \(_{j}\) -- the sets of class labels in each stream are disjoint, though both belong to the same domain \(\). Given a classifier \(f:\), parameterized by \(\), the objective of OCL is to train \(f\) on \(\), organized as a sequence of \(T\) tasks \(\{_{1},,_{T}\}\), under the constraint that, at a generic task \(_{i}\), the model receives inputs sampled from the corresponding data distribution, i.e., \((,y) D_{i}\), and sees each sample only once during the whole training procedure. The classification model may optionally keep a limited _memory buffer_\(\) of past samples, to reduce forgetting of features from previous tasks. The model update step between tasks can be summarized as:

\[ f,_{i-1},_{i-1},_{i-1}  f,_{i},_{i}\] (1)

where \(_{i}\) and \(_{i}\) represent the set of model parameters and the buffer at the end of task \(_{i}\), respectively. For methods that do not exploit buffer, \(_{i}=, i\).

The training objective is to optimize a classification loss over the sequence of tasks (without losing accuracy on past tasks) by the model instance at the end of training:\[*{arg\,min}_{_{T}}_{i=1}^{T}_{(,y) \ _{i}}f(;_{T} ),y\] (2)

where \(\) is a generic classification loss (e.g., cross-entropy), which a continual learning model attempts to optimize while accounting for model _plasticity_ (the capability to learn current task data) and _stability_ (the capability to retain knowledge of previous tasks) .

### SER: Saliency-driven Experience Replay

Our method is grounded on the neurophysiological evidence that attention-driven neuronal firing rate modulation is multiplicative and the scaling of neuronal responses depends on the similarity between a neuron's preferred stimulus and the attended feature . This hypothesis is translated into a general artificial neural architecture, where we emulate the process of human selective attention through a visual saliency prediction network  whose activations modulate, through multiplication, neuron activations of a paired classification network at different stages of visual processing. Formally, let \(S:\) be a saliency prediction network, where \(\) is the space of input images and \(\) the space of output saliency maps. Generally, if \(=^{3 H W}\) for RGB images, then \(=^{H W}\), where each location of a map \(\) measures the _saliency_ of the corresponding pixel in the RGB space. We assume that \(S\) can be decomposed into two functions, an encoder \(E:\) and a decoder \(D:\), such that \(S()=D(E())\), for \(\). Then, given an online continual learning problem with data stream \(\) and set of classes \(\), let \(C:\) be a classification network, such that \(C\) and the saliency encoder \(E\) share the same architecture (with independent parameters). An illustration of the proposed architecture is shown in Fig. 2.

At training time, both \(S\) and \(C\) observe the same data stream, from which pairs \((,y)\) of input data and class label are iteratively sampled. Through the use of an external _saliency oracle_, we extend each data sample to a triple \((,y,)\), where \(\) is the target saliency map associated to \(\). The oracle can be either a set of ground-truth maps, when available, or _pseudo-labels_ provided as the output of a pre-trained saliency predictor (unrelated to \(S\)). We therefore proceed to optimize a multi-objective loss function \(=_{s}+_{c}\), with \(\) being a weighing hyperparameter. Loss term \(_{s}\) is computed on the output of saliency predictor \(S\), and compares the estimated saliency map \(S()\) with the target \(\) by means of the Kullback-Leibler divergence (commonly employed as a saliency prediction objective ):

\[_{s}=_{i}s_{i}(}{S_{i}()+ }+)\] (3)

with \(s_{i}\) and \(S_{i}()\) iterating over map pixels in \(\) and \(S()\), respectively. Loss term \(_{c}\) encodes a generic online continual learning objective, as introduced in Eq. 2. As the proposed approach is method-agnostic, details on the formulation of \(_{c}\) may vary.

In order to enforce selective attention-driven modulation of classification neuronal activations, we leverage the architectural identity of saliency prediction encoder \(E\) and classifier \(C\) to alter the feedforward pass of the latter, by multiplying pre-activation features in \(C\) by the corresponding features in \(E\), before applying a non-linearity and feeding them to the next layer of the network. Formally, let us assume that the \(C\) and \(E\) networks consist of a sequence of layers \(\{l_{1},l_{2},,l_{L}\}\). Without loss of generality, let each layer \(l_{i}\) compute its output as \(_{i}=(_{i}_{i-1})\), with \(\) being an activation function, \(}_{i}\) the network-specific layer parameters (i.e., not shared between \(E\) and \(C\)) and \(_{i-1}\) the output of the previous layer (or the network's input \(\), if appropriate). Then, let us distinguish between features \(_{i}^{(s)}\) and \(_{i}^{(c)}\), respectively representing the output of layer \(l_{i}\) by the saliency prediction encoder \(S\) and the classifier \(C\). We apply saliency-driven modulation by modifying the computation of \(_{i}^{(c)}\) as follows:

\[_{i}^{(c)}=_{i}^{(c)}(_{i-1} ^{(c)}_{i-1}^{(s)})\] (4)

where \(\) denotes the Hadamard product. Intuitively, the proposed approach encourages the classification model to attend to "salient" features of the input, where the concept of _saliency_ is generalized from the pixel space to hidden representations. It is important to note that, at training time, gradient descent optimization of \(_{c}\) would also affect on the saliency encoder \(E\). This is undesirable, as we previously showed (see Fig. 1) that saliency features are robust to task shifts, unlike classification features: hence, in order to guarantee this property, we stop the gradient flow from \(_{c}\) to parameters in \(E\), and use it to update the parameters of classifier \(C\) only.

In the above formulation, we assumed the presence of a classification network with fully-connected layers; however, our method can be applied in an agnostic manner to any method employing, at least in part, a feature extractor implemented as a neural network. As such, the proposed method can be equally applied, for instance, both to end-to-end classification models (e.g., DER++ ) and to approaches with a neural backbone that computes class-representative prototypes (e.g., CoPE ).

## 4 Performance Analysis

### Experimental setup

**Benchmarks.** We build two OCL benchmarks by taking image classification datasets and splitting their classes equally into a series of disjoint tasks:

* **Split Mini-ImageNet**[66; 13; 21; 17] that includes 100 classes from ImageNet, allowing for a longer task sequence. For each class, 500 images are used for training and 100 for evaluation.
* **Split FG-ImageNet1** is a benchmark for fine-grained image classification that we use to test CL methods on a more challenging task than traditional ones. It includes 100 classes of animals extracted from ImageNet, belonging to 7 different species, reducing inter-class variability and leading to harder tasks. Each class contains 500 samples for training and 50 for evaluation.

For both datasets, images are resized to 288\(\)384 pixels and split into twenty 5-way tasks.

**Baseline methods.** We evaluate the contribution of the SER strategy when paired to a classification network trained using several state-of-the-art continual learning approaches, including rehearsal and non-rehearsal methods:

* **DER++**: a seminal work that combines rehearsal and knowledge distillation strategies for supporting model plasticity while limiting forgetting.
* **ER-ACE**: a variant of Experience Replay [54; 57] which aims to prevent imbalances due to the simultaneous optimization of the current and past tasks by selectively masking softmax outputs.
* **CoPE**: a prototype-based classifier with experience replay, whose careful update scheme prevents sudden disruptions in the latent space during incremental learning.
* **LwF**: a non-rehearsal method that enforces a model to preserve outputs of past model instances on new samples to limit forgetting.
* **oEWC**: a non-rehearsal method that mitigates forgetting by selectively limiting the changes on weights that are most informative of past tasks.

**Implementation details.** We apply the SER strategy at five feature modulation points of ResNet-18's architecture, namely, the outputs of the first convolutional block and of the four main residual blocks. In compliance with online learning, all models are trained for a single epoch, using SGD as optimizer, with a fixed batch size of 8 both for the input stream and the replay buffer. Rehearsal methods are evaluated with three different sizes of the memory buffer (1000, 2000 and 5000). When applying SER, besides each method's specific training objective, we also optimize the saliency prediction loss \(_{s}\) from Eq. 3, with \(=1\). Saliency is estimated using DeepGaze IIE network  as oracle.

When using SER, classifier \(C\) and saliency predictor \(S\) are identical ResNet-18 architectures, followed -- respectively -- by a linear classification layer and a saliency map decoder (additional details are provided in the supplementary materials). While \(C\) is trained from scratch, we employ a pre-trained saliency predictor \(S\), consistently with neuroscience evidence showing that humans have selective attention already embedded in the brain . For a fair comparison, in all our experiments feature extraction backbones of baseline methods are initialized to the same pre-trained weights as \(S\) (except where explicitly stated). Care was taken to ensure that the set of OCL classes \(\) did not semantically overlap with pre-training data, to prevent any contamination from the saliency predictor to the classification task. Specifically, \(S\) was pre-trained for 20 epochs on a subset of 100 ImageNet classes (disjoint from our two main benchmark datasets), using DeepGaze IIE as oracle. No class label information was used at this stage. All experiments were conducted on a workstation with an 24-coreCPU, 500GB RAM, and an NVIDIA A100 GPU (40GB VRAM). Results are computed using the Mammoth framework .

**Metrics and evaluation**. As a primary metric of OCL model performance, we report the _final average accuracy_ as \(_{i=1}^{T}a_{i}^{T}\), where \(a_{i}^{T}\) is the accuracy of the final model on the test set of task \(_{i}\). Accuracy \(a_{i}^{T}\) can be computed in a _Class-Incremental Learning_ (_Class-IL_) or in a _Task-Incremental Learning_ (_Task-IL_) setting. In the latter, we assume that a task identifier is provided to the model at inference time, simplifying the problem by restricting the set of class predictions for a given sample. While Task-IL is often depicted as a trivial scenario in recent literature [23; 64; 2], we emphasize its usefulness, as it isolates the effect of within-task forgetting from the model's bias towards the currently learned classes [71; 25; 7]. In the paper, we mainly report results in Class-IL, while the results in Task-IL setting are given in the supplementary materials. Results are reported in term of mean and standard deviation over five different runs.

### Results

We first evaluate the contribution that saliency-driven modulation provides to state-of-the-art OCL baselines. For each method, we compute Class-Incremental accuracy and compare to those obtained when integrating SER, as described in Sec. 3. Since our strategy foresees two paired networks for classification and saliency prediction, we also compare with similar multi-branch CL baselines:

* **DualNet**, mentioned in Sec. 1, employs a dual-backbone architecture to decouple incremental classification (by a _fast learner_) from self-supervised representation learning  (by a _slow learner_). We adapt SER to DualNet by replacing the slow learner and its training objective with our saliency prediction backbone, forcing the fast learner to use saliency features for classification.
* **TwF** employs a frozen pre-trained classification backbone to stabilize the learning of Class-Incremental features, by means of an attention mechanism. To enable SER, the pre-trained classification backbone and the feature distillation strategy are replaced with the saliency encoder, and the features of the two backbones are combined through multiplication, as described in Sec. 3.

Results are reported in Table 1, showing a pattern of enhanced performance when integrating SER up to 20 percent points. In terms of comparison against two-paired networks, integrating SER outperforms both of them, suggesting that controlling learning through saliency leads to better representation for classification than, for instance, contrastive learning (as done in DualNet) or feature attention with a pre-trained backbone (as in TwF)2. This is inline with cognitive neuroscience [19; 35], for which object identity-preservation, that also involves contrastive learning, happens mostly at later layers (e.g., IT neurons), while selective attention (through visual saliency) acts during the whole categorization process. Results for non-rehearsal methods are reported in the supplementary materials.

### Ablation Study

The proposed strategy is grounded on cognitive neuroscience literature, according to which selective attention modulates neuronal responses of all layers involved in the categorization process, in a multiplicative fashion. Our next experiments are meant to assess whether this hypothesis (i.e., feature modulation through multiplication for all classification layers) is optimal also for artificial neural networks, or if other integration modalities of saliency information may be equally effective. We thus compare our SER strategy with the following baselines, all exploiting saliency information in different ways:

* **Saliency-based input modulation (SIM)**: the input image is multiplied by the corresponding estimated saliency map (thus highlighting salient regions only).
* **Saliency as additional input (SAI)**: we modify the classification network to receive as input a 4D data tensor, with the saliency map concatenated to RGB channels.
* **Learning saliency-based modulation (LSM)**: rather than multiplying classification features \(_{i-1}^{(c)}\) and saliency features \(_{i-1}^{(s)}\) (see Eq. 4), we feed them to convolutional layer with 1\(\)1 kernel to produce \(_{i}^{(c)}\), and let the model learn the corresponding parameters.

Fig. 3 reports the results of this analysis, using DER++ and ER-ACE as baseline methods, and clearly indicates the superiority the SER strategy to other saliency integration variants. However, it is interesting to note that saliency helps classification performance in all cases, demonstrating its usefulness for continual learning tasks. We argue that this is due to the intrinsic nature of saliency prediction, which we found to be i.i.d. with respect to the data stream.

We then investigate whether the impact of selective-driven modulation is uniform across the backbone layers. To this aim, we define a positional binary coding scheme, controlling the application of the SER strategy at the predefined points of the network (see Sect. 4.1): if position \(i\) of the coding scheme is 1, then the \(i\)-th feature modulation point is enabled, i.e., features from the \(i\)-th block of the classification network are multiplied by the features of the \(i\)-th block of the saliency network. Results are reported in Table 2 for both DER++ and ER-ACE, and indicate that the best strategy is to modulate the features of all classification layers through the corresponding saliency ones, similarly to what neurophysiological evidence reports .

### Model Robustness

We finally assess the robustness of the SER strategy to _spurious features_ and _adversarial attacks_.

Spurious features are information that correlates well with labels in training data but not in test data

  
**Model** &  &  \\  Joint &  &  \\ Fine-tune &  &  \\  _Buffer size_ & **1000** & **2000** & **5000** & **1000** & **2000** & **5000** \\  DER++ & \(14.95 3.11\) & \(12.82 4.97\) & \(14.58 2.55\) & \(8.08 1.54\) & \(8.27 1.72\) & \(9.20 0.86\) \\ \(\)**SER** & **19.13\( 1.62\)** & **22.92\( 2.25\)** & **25.35\( 2.56\)** & **11.71\( 2.36\)** & **12.97\( 1.62\)** & **13.73\( 1.95\)** \\ ER-ACE & \(20.86 3.69\) & \(24.93 3.20\) & \(26.31 5.22\) & \(14.28 0.96\) & \(16.45 1.24\) & \(18.21 3.45\) \\ \(\)**SER** & **27.48\( 2.83\)** & **33.09\( 1.28\)** & **35.58\( 1.79\)** & **20.03\( 3.13\)** & **23.80\( 2.11\)** & **28.68\( 0.50\)** \\ CoPE & \(21.58 1.60\) & \(23.58 4.39\) & \(24.77 3.56\) & \(16.45 1.38\) & \(16.81 0.83\) & \(17.77 2.02\) \\ \(\)**SER** & **26.66\( 2.22\)** & **33.35\( 4.67\)** & **45.04\( 2.44\)** & **18.17\( 2.79\)** & **27.14\( 1.62\)** & **34.34\( 3.51\)** \\   &  \\  TwF & \(23.78 1.67\) & \(29.05 2.02\) & – & \(15.32 2.59\) & \(18.72 1.75\) & – \\ \(\)**SER** & **28.36\( 3.72\)** & **35.55\( 0.61\)** & – & **20.04\( 1.63\)** & **22.54\( 2.20\)** & – \\ DualNet & \(20.57 0.91\) & \(27.41 1.79\) & \(32.08 1.55\) & \(15.62 1.54\) & \(21.04 1.08\) & \(22.07 2.08\) \\ \(\)**SER** & **28.58\( 1.40\)** & **33.76\( 1.21\)** & **36.44\( 0.77\)** & **19.48\( 0.59\)** & **22.53\( 1.56\)** & **24.83\( 2.01\)** \\   

Table 1: **Class-Incremental accuracy of SOTA rehearsal-based methods** with and without SER.

Figure 3: **Comparison of SER to alternative saliency integration strategies**. **SIM** modulates input images by saliency maps. **SAI** provides saliency maps as an additional input channel to the classification network. **LSM** merges classification and saliency features through a learnable convolutional layer.

[MISSING_PAGE_FAIL:9]

Conclusion

We presented SER, a biologically-inspired saliency-driven modulation strategy for online continual learning, which regularizes classification features using visual saliency, effectively reducing forgetting. The proposed approach, grounded on neurophysiological evidence, significantly improves performance of state-of-the-art OCL methods, and has been shown to be superior to other multi-branch solutions, either biologically-inspired (e.g., DualNet) or based on attention mechanisms (e.g., TwF). Our results confirm that adapting neurophysiological processes into current machine learning techniques is a promising direction to bridge the gap between humans and machines.

**Limitations and future works.** In this work, we introduce the use of saliency maps as auxiliary knowledge to mitigate forgetting in continual learning. This involves pre-training our saliency predictor with an oracle, which could be in the form of either ground-truth maps or an external model generating pseudo-labels. High-quality input images are necessary for producing meaningful saliency maps, thus, datasets like CIFAR10/100 cannot be employed due to their lower resolution.

Although SER is model-agnostic, its formulation necessitates that the saliency encoder and the classifier share identical architectures. To apply this to heterogeneous networks, we will explore defining or learning mappings between activations at different network stages.

Finally, our finding that saliency prediction is _i.i.d._ with respect to classification distribution shifts opens the door to investigating whether other low-level visual tasks share this property.

## 6 Acknowledgements

G. Bellitto, F. Proietto Salanitri and C. Spampinato acknowledge financial support from PNRR MUR project PE0000013-FAIR. M. Pennisi is a PhD student enrolled in the National PhD in Artificial Intelligence, cycle XXXVII, course on Health and life sciences, organized by Universita Campus Bio-Medico di Roma.