# Are High-Degree Representations Really Unnecessary

in Equivariant Graph Neural Networks?

 Jiacheng Cen\({}^{1}\)\({}^{2}\), Anyi Li\({}^{1}\)\({}^{2}\), Ning Lin\({}^{1}\)\({}^{2}\), Yuxiang Ren\({}^{3}\), Zihe Wang\({}^{1}\)\({}^{2}\), Wenbing Huang\({}^{1}\)\({}^{2}\)

\({}^{1}\) Gaoling School of Artificial Intelligence, Renmin University of China

\({}^{2}\) Beijing Key Laboratory of Big Data Management and Analysis Methods

\({}^{3}\) 2012 Laboratories, Huawei Technologies, Shanghai

{jiacc.cn, li_anyi, ninglin00}@outlook.com; renyxiang1@huawei.com; wang.zihe@ruc.edu.cn; hwenbing@126.com

Wenbing Huang is the corresponding author.

###### Abstract

Equivariant Graph Neural Networks (GNNs) that incorporate the E(3) symmetry have achieved significant success in various scientific applications. As one of the most successful models, EGNN  leverages a simple scalarization technique to perform equivariant message passing over only Cartesian vectors (i.e., 1st-degree steerable vectors), enjoying greater efficiency and efficacy compared to equivariant GNNs using higher-degree steerable vectors. This success suggests that higher-degree representations might be unnecessary. In this paper, we disprove this hypothesis by exploring the expressivity of equivariant GNNs on symmetric structures, including \(k\)-fold rotations and regular polyhedra. We theoretically demonstrate that equivariant GNNs will always degenerate to a zero function if the degree of the output representations is fixed to 1 or other specific values. Based on this theoretical insight, we propose HEGNN, a high-degree version of EGNN to increase the expressivity by incorporating high-degree steerable vectors while still maintaining EGNN's advantage through the scalarization trick. Our extensive experiments demonstrate that HEGNN not only aligns with our theoretical analyses on a toy dataset consisting of symmetric structures, but also shows substantial improvements on other complicated datasets without obvious symmetry, including \(N\)-body and MD17. Our study potentially showcase an effective way of modeling high-degree representations in equivariant GNNs.

## 1 Introduction

Molecules, proteins, crystals, and many other scientific data can be effectively modeled and represented through _geometric graphs_. This type of data structure encapsulates not only node characteristics and edge information but also a 3D vector (such as position, velocity, etc.) for each node. To process geometric graphs, equivariant Graph Neural Networks (GNNs) have been developed, which undergo equivariant message passing over nodes, conforming to the \((3)\) or \((3)\) symmetry of physical laws. These models have achieved remarkable successes in a lot of scientific tasks, such as physical dynamics simulation , molecular generation  and protein design .

Pioneer equivariant GNNs  derive high-degree steerable representations beyond scalars and 3D coordinates with the help of spherical harmonics and conduct equivariant message passing between representations of different degrees through the Clebsch-Gordan (CG) tensor product. While these high-degree models are able to approximate any function of fully connected geometric graphs in theory , they usually suffer from expensive computational costs in practice. In contrast,EGNN  leverages a simple scalarization technique to allow equivariant message passing over only 3D vectors (_i.e._ the 1st-degree steerable features). Specifically, the scalarization technique first encodes 3D vectors into scalars as invariant messages, which are passed as geometric messages after the multiplication with the 3D vectors to recover the orientation information. Despite its simplicity, EGNN achieves remarkably better efficacy and efficiency against conventional high-degree models for a broad range of applications [24; 25]. Such successes suggest that higher-degree representations might be unnecessary.

In this paper, we challenge and disprove this hypothesis by exploring the expressivity of equivariant GNNs on symmetric graphs. Fig. 1 illustrates the examples of \(k\)-fold rotations and regular polyhedra, which are invariant to rotations up to certain rotating angles. Taking the cube for example, conducting \(90^{}\) rotation around the axes crossing the center of the two opposite faces keeps its shape and orientation unchanged. Interestingly, by making use of group theory, we theoretically prove that any equivariant GNN (after translating the coordinate center to the origin and conducting graph-level readout) on these symmetric graphs will degenerate to a zero function if the degree of their representations is fixed to be 1. The direct deduction of this theorem is that EGNN can only output a zero 3D vector no matter how we rotate the input graph, indicating that EGNN totally loses the recognition ability of orientation. Additionally, this statement points out the limitation of the methods that rely on constructing global features for symmetric graphs 2 (_e.g._ frames in frame averaging [26; 27], virtual nodes in FastEGNN ), equivariant pooling in EGHN , and meshes in Neural P\({}^{3}\)M ), since it is impossible to output another non-collinear 3D vector except the center coordinate.

Based on the above theoretical insights, we propose a novel equivariant GNN model termed HEGNN3, which enhances EGNN by incorporating high-degree steerable vectors while inheriting the desired benefit from EGNN through the scalarization trick. In summary, our contributions are as follows:

* We theoretically investigate the expressivity reduction issue of equivariant GNNs on symmetric graphs.
* We propose HEGNN, to further incorporate high-degree steerable representations into EGNN. Moreover, since the equivariant message passing process between different-degree representations is conducted via inner products, it shares the same benefit as EGNN, compared to traditional high-degree models.
* Our extensive experiments demonstrate that HEGNN not only aligns with our theoretical analyses on toy datasets consisting of symmetric graphs, but also shows substantial improvements on more complicated datasets without explicit symmetry, such as \(N\)-body and MD17.

## 2 Related Works

**Equivariant GNNs.** Equivariant GNNs can be divided into two classes: scalarization-based models and high-degree steerable models . Scalarization-based models adopt norms or inner products to convert equivariant 3D vectors into invariant scalars, which are considered as coefficients to linearly combine 3D vectors for node update. EGNN  is the first work falling into this category. Concurrently, PAINN  further enhances the expressive ability of the model by introducing multi-channel equivariant features. On the contrary, high-degree steerable models (_e.g._ TFN ,

Figure 1: Common symmetric graphs. Equivariant GNNs on symmetric graphs will degenerate to a zero function if the degree of their representations is fixed as 1.

SEGNN  and SE(3)-Transformer) use spherical harmonics to ensure the equivariance of message passing, and realize interaction between steerable features of different degrees through CG tensor products. Our HEGNN also uses high-degree steerable features, but it leverages the scalarization trick for the interaction between steerable features of different degrees, thus leading to more expressivity than EGNN and less computational cost than other high-degree models.

**Expressivity of Equivariant GNNs.** The theoretical expressivity of equivariant GNNs is initially explored by , which proves the university of the high-degree steerable model, _i.e._, TFN , over fully-connected geometric graphs. GemNet  further demonstrates that the universality holds with just spherical representations other than the full \((3)\) representations that are required in the proof of . More recently, the GWL framework  extends the Weisfeiler-Lehman (WL) test into a geometric version  to study the expressive power of geometric GNNs operating on sparse graphs from the perspective of discriminating geometric graphs. Different from all the above works, our paper investigates the expressivity of equivariant GNNs on symmetric graphs, and demonstrates the necessity of involving high-degree representations. Although the GWL test paper  has experimentally compared different models on \(k\)-fold structures that are allowed to rotate only in the 2D space, the conclusions of this paper are proved both theoretically and experimentally. Moreover, our discussions cover a full range of examples including \(k\)-folds (rotation in 3D space) and regular polyhedra.

## 3 Theoretical Analyses

In this section, we first present the necessary preliminaries related to geometric graphs and group representation. Then, we define and illustrate typical examples of symmetric graphs. Finally, we will discuss when equivariant GNNs will degenerate to a zero function on symmetric graphs.

### Preliminaries

**Geometric graph.** A geometric graph of \(N\) nodes is defined as \((,};)\), where \(\{_{i}^{C_{H}}\}_{i=1}^{N}\) and \(}\{}_{i}^{3}\}_{i=1}^{N}\) are node features and 3D coordinates, respectively; \(^{N N}\) represents the adjacency matrix and can be assigned with edge features \(_{ij}\) if necessary. Throughout our theoretical analyses in this section, we assume the node features and edge features to be identical for all.

**Transformation of geometric graph.** We are interested in the transformations of a geometric graph \(\) with respect to a group \(\), which is defined as \(\), for \(\) and \(\) denoting the group action. For instance, \(\) can be explained as translation, rotation, or reflection of the coordinates \(}\). These transformations form a 3D Euclidean group denoted as \((3)\), and its subgroup without translation is called the orthogonality group \((3)\). With the aid of group representation \(()\), the transformation of a coordinate \(}\) is represented as \(()}\). For example, orthogonal matrices are the trivial representations of \((3)\), that is, the orthogonal transformation of a vector \(}\) is represented by \(}\) with \(^{3 3}\) being an orthogonal matrix. Besides, there are other representations of \((3)\), such as the irreducible representations which will be detailed below.

**Equivariance.** Let \(\) and \(\) be the input and output vector spaces, respectively. A function \(f:\) is called _equivariant_ with respect to group \(\) if

\[,f(_{}()})=_{}()f(}),\] (1)

where \(_{}\) and \(_{}\) are the group representations in the input and output spaces, respectively. Since we can eliminate the translation effect by simply translating the center of all coordinates to the origin, we only discuss equivariance with respect to \((3)\) in this section. In other words, we default that the center of \(}\) is at the origin.

**Irreducible representations and steerable features.**\((3)\) consists of rotation and inversion, implying \((3)=(3) C_{i}\), where \((3)\) is the rotation group and \(C_{i}=\{,\}\) denotes the inverse group. We first discuss the irreducible representations of \((3)\). For each rotation \((3)\), its irreducible representations are Wigner-D matrices \(^{(l)}()^{(2l+1)(2l+1)}\) of different degree \(l\)[21; 37]. When \(l=1\), it becomes the common rotation matrix \(}}\) acting on the 3D coordinate space. Under the irreducible representations, the equivariant constraint in Eq. (1) turns into \(f^{(l)}(_{}})=^{(l)}()f^{(l)}(})\), if the output degree is \(l\). According to , spherical harmonics \(Y^{(l)}=[Y^{(l)}_{m}(})]^{l}_{m=-l}\) offer a _unique_ and _complete_ set of function bases satisfying the equivariant constraint. We further define a modulated spherical harmonics as \(f^{(l)}(})=(\|}\|) Y^{(l)}(}/\| }\|)\) by adding a continuous radial function \(:^{+}\) of vector norm \(\|\|\) for re-scaling. Such a function \(f^{l}\) and its output \(f^{l}(})\) are called type-\(l\)_steerable function_ and _steerable feature_, respectively. We now deduce the irreducible representations from \((3)\) to \((3)\). Note that spherical harmonics satisfy \(Y^{(l)}(-})=(-1)^{l}Y^{(l)}(})\); in other words, they are inverse-equivariant when \(l\) is odd, but inverse-invariant when \(l\) is even. We thus specify the group representation of \((3)\) as

\[^{(l)}()^{(l)}()^{(l)}( ),\] (2)

where \(^{(l)}()=1\) for \(=\) (the identity) and \(^{(l)}()=(-1)^{l}\) if \(=\) (the inverse). Readers can refer to the discussion in e3nn  with another representation method by using the concept of _parity_ and construct this through methods such as Clebsch-Gordan (CG) tensor product . For concision, the type-\(l\) steerable feature is denoted as \(}^{(l)}\) with a tilde notation.

### Symmetric Graph

In SS 1, we present that \(k\)-fold rotations and regular polyhedra exhibit certain symmetries. In this subsection, we formally describe them via the notion of the symmetric graph.

**Definition 3.1** (Symmetric Graph).: A geometric graph \(\) is called a symmetric graph, if there exists a finite and nontrivial subgroup \((3),\{\}\), satisfying that \(,=\). All subgroups making \(\) symmetric yields a set \(()\), and all geometric graphs that are symmetric _w.r.t._\(\)) constitute a set denoted as \(()\).

Here \(=\) is defined in the graph level. Particularly for the coordinates \(}^{3 N}\), it implies that \(\), \( S_{N}\), \(}=}\) and \(=\), where \(S_{N}\) is the permutation group of order \(N\). Essentially, rotating the coordinates of a symmetric graph leads to a copy of this graph up to a different permutation of the nodes.

Without considering inversion, the finite subgroups of \((3)\) are only cyclic group \(C_{n}\), dihedral group \(D_{n}\), tetrahedral group \(T\), octahedral group \(O\), and Icosahedral group \(I\). We provide several examples of symmetric graphs as follows.

**Example 3.2** (\(k\)-folds).: On the one hand, for a geometric graph \(\) corresponding to a \(2k\)-fold with nodes \(\{((i/k),(i/k),0)\}_{i=0}^{2k-1}\), the inverse group \(C_{i}\) and the dihedral group \(D_{2k}\) (rotation around \(z\)-axis with angle \(/k\), and reflection around the axis connecting the midpoints of opposite sides or the axis connecting opposite vertices), are symmetric groups on \(\), namely, \(C_{i},D_{2k}()\). On the other hand, for a geometric graph \(\) corresponding to a \((2k+1)\)-fold with nodes \(\{((i 2/(2k+1),(i 2/(2k+1)),0))\}_{i=0}^{2k}\), \(()\) includes the dihedral group \(D_{2k+1}\) but without the inverse group \(C_{i}\).

**Example 3.3** (Regular Polygons).: The symmetric groups of regular polygons in the plane and regular prisms in space include the dihedral group \(D_{n}\). Regular tetrahedra are symmetric with respect to three rotation axes of the second order and four axes of the third order, corresponding to 12 group elements. Regular hexahedra (cubes) and the regular octahedra (which are dual to each other and share the same symmetric groups) are symmetric about six axes of the second order, four axes of the third order, and three axes of the fourth order, corresponding to 24 group elements. Regular dodecahedra and regular icosahedra (which are also dual to each other) are symmetric about six axes of the fifth order, ten axes of the third order, and fifteen axes of the second order, corresponding to 60 group elements. Additionally, except tetrahedra, all other four regular polygons are central symmetric, indicating that \(C_{i}\) is their common symmetric group.

### Equivariant GNNs on symmetric graphs

We now demonstrate that equivariant GNNs on symmetric graphs will encounter the issue of expressivity degeneration. Here, we assume that the graph functions we explore are invariant to the permutation of the nodes. This fits the case when we add a readout layer to all nodes globally or just focus on the message passing process for each node individually.

We first derive a crucial theorem that greatly facilitates our analyses.

**Theorem 3.4**.: _Suppose that \(f^{(l)}\) is an \((3)\)-equivariant function on geometric graphs, regarding the group representation \(^{(l)}\) defined in Eq. (2). Then, for any symmetric graph \(\) induced by the group \((3)\), namely, \(()\), we always have_

\[f^{(l)}()=^{(l)}()f^{(l)}().\] (3)

_Here we have defined group average as \(^{(l)}()|}_{ }^{(l)}()\)._

Eq. (3) is interesting and it shows that the function \(f^{(l)}\) is symmetric with respect to the group average \(^{(l)}()\). More importantly, it indicates an linear equation \((_{2l+1}-^{(l)}())f^{(l)}()=0\), where \(_{2l+1}^{(2l+1)(2l+1)}\) is the identity matrix. We can immediately attain the following conclusion.

**Theorem 3.5**.: _If and only if the matrix \(_{2l+1}-^{(l)}()\) is non-singular, the \((3)\)-equivariant function \(f^{(l)}\) is always a zero function on \(\), namely,_

\[f^{(l)}(),( ).\] (4)

A more general version of Theorem 3.5 is that the output space of \(f^{(l)}\) corresponds to the null space of the matrix \(_{2l+1}-^{(l)}()\), indicating that \((f^{(l)})=(2l+1)-(_{2l+1}-^{(l)}())\). Therefore, even the function \(f^{(l)})\) will not exactly reduce to a zero function when \(_{2l+1}-^{(l)}()\) is singular, its output space is still limited to a subspace and suffers from diminished expressivity owing to the symmetry of the input geometric graph.

In practice, it is difficult to determine if the matrix \(_{2l+1}-^{(l)}()\) is singular. This determination becomes easier if we can show that the group average \(^{(l)}()\) is equal to the zero matrix. Fortunately, we have the following property.

**Theorem 3.6**.: _For a finite group \(\) with its representation \(^{(l)}\), \(^{(l)}()\) is a zero matrix (i.e., \(^{(l)}()=\)) if and only if \((^{(l)}())=0\). In this case, \(f^{(l)}(),()\)._

According to Theorem 3.6, we calculate the trace of the group average for each symmetric graph of interest and check if the trace is equal to zero. We summarize the conclusions for \(k\)-fold structures and regular polyhedra in Table 1. We find that when \(l=1\), \(f^{(1)}\) for all cases. In addition, the function degenerates when \(l\) is odd, if the symmetric graph is induced by the inverse group \(C_{i}\). We defer more details of the calculations in the Appendix. Compared to the conclusions drawn by the GWL paper  which only experimentally discusses the \(k\)-fold structures under 2D rotations, here we apply rigorous theoretical derivations to analyze more cases besides \(k\)-folds, regarding more symmetric subgroups of \((3)\).

## 4 The Proposed HEGNN

The analyses in the last section imply the necessity of involving the representations with more and higher degrees in equivariant GNNs. Therefore, we propose HEGNN by further conducting the update of high-degree steerable features upon EGNN . As illustrated in Fig. 2, HEGNN is composed of the three key components: initialization of high-degree steerable features, calculation of cross-degree invariant messages, and aggregation of neighbor messages, the latter two of which are conducted over multiple layers. We depict each component separately in what follows.

**Initialization of high-degree steerable features.** Given a geometric graph \((,};)\) where each node contains only type-0 feature \(_{i}\) and type-1 feature \(}_{i}\), we first obtain the initialization

   Symmetric Graph \(\) & Symmetry Group \(()\) & \(l\) leading to \(f^{(l)}()\) \\  \(2k\)-fold & \(C_{i},D_{2k}\) & \(l\) is odd \\ \((2k+1)\)-fold & \(D_{2k+1}\) & \(l<2k+1\) and \(l\) is odd \\ Tetrahedron & \(T\) & \(l\{1,2,5\}\) \\ Cube/Octahedron & \(C_{i},O\) & \(l=2\) or \(l\) is odd \\ Dodecahedron/Icosahedron & \(C_{i},I\) & \(l\{2,4,8,14\}\) or \(l\) is odd \\   

Table 1: Expressivity degeneration of equivariant GNNs on symmetric graphs.

of high-degree steerable features \(\{}_{i}^{(l)}\}_{l=0}^{L}\) by using spherical harmonics on normalized relative coordinates. In detail, we aggregate spherical harmonics from all neighbors as

\[}_{i,}^{(l)}=(i)|}_{j (i)}_{},}^{(l)}(_{ij, }) Y^{(l)}(}_{i}-}_{j}}{ \|}_{i}-}_{j}\|}),\] (5)

where \(_{ij,}=_{,}(_{i},_{j},_{ij},d_{ij}^{2})\) is an invariant scalar with \(_{,}\) being an arbitrary MultiLayer Perceptron (MLP), and \((i)\) denotes the neighbors of \(i\)4.

**Calculation of cross-degree invariant messages.** EGNN  employs a scalarization trick by transforming the relative coordinate \(}_{i}-}_{j}\) (the usage of relative coordinates is for translation invariance) into an invariant scalar via the vector norm, which will be used to compute invariant message for both node features and coordinates. We generalize this scalarization trick to the case of high-degree steerable features. To be specific, we carry out the inner product between \(}_{i}^{(l)}\) and \(}_{j}^{(l)}\) for each degree \(l\) individually, resulting in an invariant scalar \(z_{ij}^{(l)}\). Then, we get the invariant message between node \(i\) and \(j\), namely \(_{ij}\) after undergoing an MLP of all invariant quantities. The above processes are summarized as follows:

\[d_{ij}=\|}_{i}-}_{j}\|, z_{ij}^{(l)}= }_{i}^{(l)},}_{j}^{(l)},_{ij }=_{}(_{i},_{j},_{ij},d_{ij}^{2}, _{l=0}^{L}z_{ij}^{(l)}),\] (6)

where \(\) refers to concatenation. It should be noted that the form of SO3KRATES introduced in the concurrent work  is equivalent to the expression for \(z_{ij}^{(l)}\) in Eq. (6). Furthermore, our scalarization trick simplifies the formulation by bypassing the Clebsch-Gordan coefficients, making it more straightforward and easier to understand.

**Aggregation of neighbor messages.** With the invariant message \(_{ij}\) at hand, we then update \(_{i},}_{i},}_{i}^{(l)}\) via message aggregation over all neighbors. We define \(_{i},}_{i},}_{i}^{(l)}\) as the residues, which are calculated by:

\[_{i}=_{}(_{i},(i)|}_{j(i)}_{ij}),\; }_{i}=(i)|}_{j(i)}_{ }}(_{ij})(}_{i}-}_{j}),\] (7) \[}_{i}^{(l)}=(i)|}_{j (i)}_{}}^{(l)}(_{ij})( {}_{i}^{(l)}-}_{j}^{(l)}),\] (8)

Figure 2: The different architectures of our HEGNN, EGNN  and TFN . HEGNN exploits the scalarization trick inspired by EGNN to enable steerable features to interact between different degrees, avoiding the high computational cost of using CG tensor products in TFN.

where \(_{},_{}},_{}}^{(l)}\) are different MLPs, and \(_{}},_{}}^{(l)}\) both output a 1D scalar. Note that the application of Eq. (8) for all degrees can be compactly rewritten as \(_{l=0}^{L}}_{i}^{(l)}=(i)|} _{j(i)}1_{}^{_{}_{ij} }_{l=0}^{L}(}_{i}^{(l)}-}_{j}^{(l)})\) in the form of CG tensor product with the weights \(_{}}(_{ij}):=_{l=0}^{L}_{}}^{(l)}\). This form can be easily implemented using existing libraries such as e3nn.o3.FullyConnectedTensorProduct. The resulting residues are used for the update:

\[_{i}=_{i}+_{i},}_{i}=}_{i}+ }_{i},}_{i}^{(l)}=_{i}^{(l)}+ }_{i}^{(l)}.\] (9)

In addition, we can augment the update of \(}_{i}\) with 1st-degree feature \(}_{i}^{(1)}\), leading to \(}_{i}=}_{i}+}_{i}+_{}} ^{(1)}(_{i})}_{i}^{(1)}\), which yet is not explored in our experiments for the sake of simplicity. The final output of \(_{i}\) and \(}_{i}\) can be used for the node-level invariant prediction and equivariant prediction, respectively. We can also obtain a graph-level prediction by further adding a readout layer of all nodes.

We now analyze the expressivity of HEGNN. Apparently, by including high-degree features, HEGNN is able to avoid the loss of expressive ability even on symmetric graphs. Moreover, when tackling general geometric graphs, HEGNN is capable of characterizing the complete angle information of the input graph, if its maximal degree \(L\) is sufficiently large. For concision and without losing the generality, we assume the steerable features \(}_{i}^{(1)}\) are initialized with only spherical harmonics without the weights \(_{},}^{(l)}\) in Eq. (5). Let \(}_{is}=(}_{i}-}_{s})/\|}_{i}-}_{s}\|\), the inner product \(z_{ij}^{(l)}\) can be expanded as follows

\[_{s(i)}Y^{(l)}(}_{is}), _{t(j)}Y^{(l)}(}_{jt})= _{s(i)}_{t(j)}P^{(l)} (}_{is},}_{jt}),\] (10)

where \(P^{(l)}:\) is Legendre polynomial of degree \(l\), and Eq. (10) is based on the properties of spherical harmonics that \( Y^{(l)}(}),Y^{(l)}(})=4/(2l+1) P ^{(l)}(},}),\|}\|=\|} \|=1\). We have the following result.

**Theorem 4.1**.: _For any geometric graph, there exists a bijection between the set of inner products \(\{z_{ij}^{(l)}\}_{l=1}^{|_{ij}|}\) given by Eq. (10) and the set of edge angles \(_{ij}=\{_{is,jt}(}_{is},} _{jt})\}_{s(i),t(j)}\)._

Theorem 4.1 states that the inner products of full degrees can recover the information of all angles between each pair of edges, affirming the enhanced expressivity of our HEGNN. The proof is derived mainly based on the fact that Legendre polynomials are orthogonal polynomial bases which can injectivly represent the set \(_{ij}\) thanks to Newton's identities. The details are deferred to the appendix. Although the upper-bound of the degree in Theorem 4.1 grows rapidly with the graph size, it will be shown in our experiments that HEGNN with only \(L 6\) is sufficient to outperform traditional models like EGNN  and TFN  in practice.

## 5 Experiment

### Expressivity on Symmetric Graphs

**Design of experiments:** To experimentally verify the conclusion we proved above, we design a more comprehensive experiment based on code5 in . This experiment uses four \(k\)-fold structures (\(k\{2,3,5,10\}\)) and five convex regular polyhedra shown in Fig. 1 as test objects, and the center of each is at the origin. In detail, an arbitrary rotation in \(3D\) is acted on such symmetric structures called \(_{0}\) which ensures the geometric graph after rotation called \(_{1}\) does not coincide with the original one. The goal of our experiments is to check whether different equivariant neural networks can distinguish \(_{0}\) and \(_{1}\).

The models we select include two models that only use Cartesian coordinates: EGNN and GVP-GNN; and two models that use high-degree steerable features: TFN and MACE. However, TFN and MACE (denoted as TFN/MACE\({}_{l L}\)) always use all degrees \(l\{0,,L\}\), so it is not clear which degree(s) of steerable features distinguish the two geometric graphs. In our HEGNN, all steerable features corresponding to unwanted degrees could be masked during initialization in Eq. (5), and we let HEGNN\({}_{l=L}\) be a HEGNN with only \(l\)th-degree steerable features. Additionally, to align with TFN/MACE, we also test the performance of HEGNN with all \(l\{0,1,,L\}\) donated as HEGNN\({}_{l L}\). Following the settings6 in , the output of each graph is the concatenation of invariant scalars, coordinates, and high-degree steerable features pooling among all nodes. We then map this spliced vector to a two-dimensional vector and input it into a simple classifier to determine whether the equivariant graph neural network can distinguish \(_{0}\) from \(_{1}\).

**Results:** The results on \(k\)-fold are deferred to Appendix for saving space, and the results on regular polyhedra are shown in Table 2. From Table 1, we can know steerable features in which degree could not distinguish specific symmetry structure, and both results on \(k\)-fold and regular polyhedra are also in perfect agreement with our conclusions. Models (EGNN and GVP-GNN) only with Cartesian vectors cannot distinguish any symmetric graph at all. Taking HEGNN\({}_{l=5}\) as an example, since \(^{(5)}(})=0,}\{T,O,I\}\), no matter which kind of regular polyhedron, \(f^{(5)}\) could only output 0 thus failing to distinguish the structures.

### Physical Dynamics Simulation

**Datasets:** We benchmark our HEGNN in two scenarios, including: \(N\)**-body system** is a dataset generated from simulations. In our simulations, each system contains \(N\) charged particles with random charge \(c_{i}\{0,1\}\), whose movements are driven by Coulomb forces. To verify the efficiency and effectiveness of our HEGNN on datasets of different sizes, we select \(N\) from \(\{5,20,50,100\}\). We use \(5000\) samples for training, \(2000\) for validation, and \(2000\) for testing. The task is to estimate the positions of the \(N\) particles after 1,000 timesteps. **MD17** dataset contains trajectory data for eight molecules generated through molecular dynamics simulations. The goal of this experiment is to predict the future positions of the atoms based on their current state. We follow the dataset partitioning scheme from , splitting the dataset into 500/2000/2000 frame pairs for training, validation and testing, respectively. All experiments are run on a single NVIDIA A100-80G GPU.

**Baselines:** To demonstrate the advantages of our HEGNN over both models with scalarization techniques and models with high-degree steerable vectors at the same time, our baseline needs to consider the selection issues of both models simultaneously. Therefore, we select some representative models as baselines, including the invariant RF , the equivariant EGNN , TFN  and SE(3)-Tr. . In addition, we select classical models such as Linear dynamics , the non-equivariant

    &  \\  & **GNN Layer** & Tetrahedron & Cube & Octahedron & Dodecahedron & Icosahedron \\   & E-GNN\({}_{l=1}\) & 50.0 \(\) 0.0 & 50.0 \(\) 0.0 & 50.0 \(\) 0.0 & 50.0 \(\) 0.0 \\  & GVP-GNN\({}_{l=1}\) & 50.0 \(\) 0.0 & 50.0 \(\) 0.0 & 50.0 \(\) 0.0 & 50.0 \(\) 0.0 \\   & HEGNN\({}_{l=1}\) & 50.0 \(\) 0.0 & 50.0 \(\) 0.0 & 50.0 \(\) 0.0 & 50.0 \(\) 0.0 \\  & HEGNN\({}_{l=2}\) & 50.0 \(\) 0.0 & 50.0 \(\) 0.0 & 50.0 \(\) 0.0 & 50.0 \(\) 0.0 \\  & HEGNN\({}_{l=3}\) & **100.0 \(\) 0.0** & 50.0 \(\) 0.0 & 50.0 \(\) 0.0 & 50.0 \(\) 0.0 \\  & HEGNN\({}_{l=4}\) & **100.0 \(\) 0.0** & 90.0 \(\) 0.0 & 90.0 \(\) 0.0 & 50.0 \(\) 0.0 & 50.0 \(\) 0.0 \\  & HEGNN\({}_{l=5}\) & 50.0 \(\) 0.0 & 50.0 \(\) 0.0 & 50.0 \(\) 0.0 & 50.0 \(\) 0.0 & 50.0 \(\) 0.0 \\  & HEGNN\({}_{l=6}\) & **100.0 \(\) 0.0** & **100.0 \(\) 0.0** & **100.0 \(\) 0.0** & **100.0 \(\) 0.0** & **100.0 \(\) 0.0** \\  & HEGNN\({}_{l=7}\) & **100.0 \(\) 0.0** & 50.0 \(\) 0.0 & 50.0 \(\) 0.0 & 50.0 \(\) 0.0 & 50.0 \(\) 0.0 \\  & HEGNN\({}_{l=8}\) & **100.0 \(\) 0.0** & 90.0 \(\) 0.0 & 90.0 \(\) 0.0 & 50.0 \(\) 0.0 & 50.0 \(\) 0.0 \\  & HEGNN\({}_{l=9}\) & **100.0 \(\) 0.0** & 50.0 \(\) 0.0 & 50.0 \(\) 0.0 & 50.0 \(\) 0.0 & 50.0 \(\) 0.0 \\  & HEGNN\({}_{l=10}\) & **100.0 \(\) 0.0** & **100.0 \(\) 0.0** & 95.0 \(\) 0.0 & **100.0 \(\) 0.0** & **100.0 \(\) 0.0** \\   & HEGNN/TFN/MACE\({}_{l 2}\) & 50.0 \(\) 0.0 & 50.0 \(\) 0.0 & 50.0 \(\) 0.0 & 50.0 \(\) 0.0 & 50.0 \(\) 0.0 \\  & HEGNN/TFN/MACE\({}_{l 3}\) & **100.0 \(\) 0.0** & 50.0 \(\) 0.0 & 50.0 \(\) 0.0 & 50.0 \(\) 0.0 & 50.0 \(\) 0.0 \\  & HEGNN/TFN/MACE\({}_{l 4}\) & **100.0 \(\) 0.0** & **100.0 \(\) 0.0** & **100.0 \(\) 0.0** & 50.0 \(\) 0.0 & 50.0 \(\) 0.0 \\  & HEGNN/TFN/MACE\({}_{l 6}\) & **100.0 \(\) 0.0** & **100.0 \(\) 0.0** & **100.0 \(\) 0.0** & 50.0 \(\) 0.0 & 50.0 \(\) 0.0 \\   

Table 2: _Regular polyhedra._Message Passing Neural Network (MPNN) , the invariant SchNet , and the equivariant GVP-GNN  for the \(N\)-body experiments. For MD17 experiments, we also select GMN .

**Metrics: 1. Loss function:** We use Mean Squared Error (MSE) to measure the accuracy of the prediction results in both experiments. **2. Inference time:** Given that the \(N\)-body system we use contains data of varying sizes, we test the inference time of each model on this dataset. The inference time for each model is calculated relative to the benchmark, which is the inference time of EGNN at the corresponding scale.

**Results on \(N\)-Body systems:** The main results of \(N\)-body system simulation are presented in Table 3. From these results, we observe the following: **1. Overall performance**: Our HEGNN significantly outperforms other models across datasets of all sizes. Although EGNN  performs better than high-degree steerable models like TFN or \((3)\)-Transformer in this task, our HEGNN is still better than EGNN, which show that the method of HEGNN introducing high-degree steerable features is more effective. **2. Stability**: Although the performance of the model (\(_{l 6}\)) using high-degree steerable features declines slightly when the geometric graph is small, overall, HEGNN performs better than other models. **3. Inference time**: Our model's inference time is significantly faster than that of high-degree steerable models like TFN, reflecting the simplicity and efficiency of our HEGNN.

**Results on MD17:** The main results of MD17 experiment are shown in Table 4, with some data sourced from . From these results, we draw the following insights: **1. Overall performance:** Our HEGNN outperforms other models on six out of eight molecules. The effect on the remaining two molecules is only not as good as GMN  and this is because GMN introduces additional knowledge such as chemical bonds. **2. Advantage of high-degree vectors:** Most of the best results are obtained on \(_{l 6}\), indicating that the use of high-degree steerable features can enhance model expression capabilities.

    & Aspirin & Benzene & Ethanol & Malonaldehyde & Naphthalene & Salicylic & Toluene & Uracil \\  RF & 10.94\(\)0.01 & 103.72\(\)1.29 & 4.64\(\)0.01 & 13.93\(\)0.03 & 0.50\(\)0.01 & 1.23\(\)0.01 & 10.93\(\)0.04 & 0.64\(\)0.01 \\ EGNN & 14.41\(\)0.15 & 62.40\(\)0.53 & 4.64\(\)0.01 & 13.64\(\)0.01 & 0.47\(\)0.02 & 1.02\(\)0.02 & 11.78\(\)0.07 & 0.64\(\)0.01 \\ EGNNReg & 13.82\(\)0.19 & 61.68\(\)0.37 & 6.06\(\)0.01 & 13.49\(\)0.06 & 0.63\(\)0.01 & 1.68\(\)0.01 & 11.05\(\)0.01 & 0.66\(\)0.01 \\ GMN & 10.14\(\)0.03 & **48.12\(\)**0.40 & 4.83\(\)0.01 & 13.11\(\)0.03 & 0.40\(\)0.01 & 0.91\(\)0.01 & **10.22\(\)**0.08 & 0.59\(\)0.01 \\  \(_{l 2}\) & 12.37\(\)0.18 & 58.48\(\)1.98 & 4.81\(\)0.04 & 13.62\(\)0.08 & 0.49\(\)0.01 & 1.03\(\)0.02 & 10.89\(\)0.01 & 0.84\(\)0.02 \\ SE(3)-\(_{l 2}\) & 11.12\(\)0.06 & 68.11\(\)0.67 & 4.74\(\)0.13 & 13.89\(\)0.02 & 0.52\(\)0.01 & 1.13\(\)0.02 & 10.88\(\)0.06 & 0.79\(\)0.02 \\  \(_{l 1}\) & 10.32\(\)0.58 & 62.53\(\)7.62 & 4.63\(\)0.01 & **12.85\(\)**0.01 & 0.38\(\)0.01 & 0.90\(\)0.05 & 10.56\(\)0.10 & 0.56\(\)0.02 \\ \(_{l 2}\) & 10.04\(\)0.45 & 61.80\(\)5.92 & 4.63\(\)0.01 & **12.85\(\)**0.01 & 0.39\(\)0.01 & 0.91\(\)0.06 & 10.56\(\)0.05 & 0.55\(\)0.01 \\ \(_{l 3}\) & 10.20\(\)0.23 & 62.82\(\)2.45 & 4.63\(\)0.01 & **12.85\(\)**0.02 & **0.37\(\)**0.01 & 0.94\(\)0.10 & 10.55\(\)0.16 & **0.52\(\)**0.01 \\ \(_{l 6}\) & **9.94\(\)**0.07 & 59.93\(\)5.21 & **4.62\(\)**0.01 & **12.85\(\)**0.01 & **0.37\(\)**0.02 & **0.88\(\)**0.02 & 10.56\(\)0.33 & 0.54\(\)0.01 \\   

Table 4: Prediction error (\( 10^{-2}\)) on MD17 dataset. Results averaged across 3 runs.

    &  &  &  &  \\  & MSE & Relative & MSE & Relative & MSE & Relative & MSE & Relative & MSE & Relative \\  & (\( 10^{-2}\)) & Time & (\( 10^{-2}\)) & Time & (\( 10^{-2}\)) & Time & (\( 10^{-2}\)) & Time \\  Linear & \(7.72\) & \(0.01\) & \(10.12\) & \(0.02\) & \(11.81\) & \(0.02\) & \(12.69\) & \(0.01\) \\ MPNN  & \(1.80\) & \(0.49\) & \(2.50\) & \(0.51\) & \(2.96\) & \(0.50\) & \(3.55\) & \(0.45\) \\ SchNet  & \(11.31\) & \(2.93\) & \(17.72\) & \(6.24\) & \(22.14\) & \(31.63\) & \(22.14\) & \(27.04\) \\ RF  & \(1.51\) & \(0.54\) & \(3.41\) & \(0.65\) & \(4.75\) & \(0.67\) & \(5.72\) & \(0.49\) \\ GVP-GNN  & \(7.26\) & \(2.36\) & \(5.76\) & \(2.38\) & \(7.07\) & \(2.42\) & \(7.55\) & \(2.33\) \\ EGNN  & \(0.65\) & \(1.00\) & \(1.01\) & \(1.00\) & \(1.00\) & \(1.00\) & \(1.36\) & \(1.00\) \\  \(_{l 2}\) & \(1.49\) & \(2.69\) & \(1.86\) & \(3.19\) & \(2.20\) & \(2.87\) & \(3.42\) & \(6.58\) \\ \(_{l 3}\) & \(1.76\) & \(3.91\) & \(1.87\) & \(4.54\) & \(1.94\) & \(4.89\) & **OOM** & - \\ \((3)\)-\(_{l 2}\) & \(3.24\) & \(4.94\) & \(3.19\) & \(5.88\) & \(2.54\) & \(5.97\) & \(2.33\) & \(5.15\) \\  \(_{l 1}\) & \(0.52\) & \(1.77\) & \(0.79\) & \(1.84\) & \(0.88\) & \(1.60\) & \(1.13\) & \(1.45\) \\ \(_{l 2}\)

### Perturbation Experiment

In practical scenarios, slight perturbations (such as molecular vibrations) can disrupt strict symmetry, potentially mitigating the conclusions outlined in Theorems 3.5 and 3.6. We therefore designed this perturbation experiment for a simple study and were surprised to find that HEGNN can still bring better robustness through the introduction of high-degree steerable features.

**Design of experiments:** We take the tetrahedron as an example and compare the cases of EGNN, \(_{l=3}\), and \(_{l 3}\) when adding noise perturbations with results in Table 5. Here, \(\) represents the ratio of noise, and the modulus of the noise obeys \((0,[\|-_{c}\|] I)\).

**Results:** It can be observed that the performance of EGNN is slightly improved in the presence of noise (from 50% when \(=0.01\) to 60% when \(=0.5\)), while HEGNN demonstrates better robustness. Even though symmetry-breaking factors will make the geometric graph deviate from the symmetric state, the deviated graph is still roughly symmetric. In other words, the outputs of equivariant GNNs on the derivated graphs keep close to zero if the degree value is chosen to be those in Table 1, which will still lead to defective performance.

## 6 Conclusion

In this paper, we challenged the prevailing notion that higher-degree steerable vectors are unnecessary for achieving expressivity in equivariant Graph Neural Networks (GNNs). Through rigorous theoretical analysis, we demonstrated that equivariant GNNs constrained to 1st-degree representations inevitably degenerate to zero functions when applied to symmetric structures, such as \(k\)-fold rotations and regular polyhedra. To address this limitation, we introduced HEGNN, a high-degree extension of the EGNN model. HEGNN enhances expressivity by integrating higher-degree steerable vectors while retaining the efficiency of the original model through a scalarization technique. Our extensive empirical evaluations on various datasets, including the symmetric toy dataset, \(N\)-body, and MD17, validate our theoretical predictions. HEGNN not only adheres to our theoretical insights but also exhibits significant performance improvements over existing models. These findings underscore the critical role of higher-degree representations in fully leveraging the potential of equivariant GNNs.

## 7 Acknowledgment

This work was jointly supported by the following projects: the National Science and Technology Major Project under Grant 2020AAA0107300, the National Natural Science Foundation of China (No. 62376276, No. 62172422); Beijing Nova Program (No. 20230484278); Beijing Outstanding Young Scientist Program (No. BJJWZYJH012019100020098), the Fundamental Research Funds for the Central Universities, and the Research Funds of Renmin University of China (23XNKJ19); Public Computing Cloud, Renmin University of China.