# Identifying Latent State-Transition Processes for Individualized Reinforcement Learning

Yuewen Sun\({}^{1,2}\), Biwei Huang\({}^{3}\), Yu Yao\({}^{4}\), Donghuo Zeng\({}^{5}\), Xinshuai Dong\({}^{2}\), Songyao Jin\({}^{3}\),

**Boyang Sun\({}^{1}\), Roberto Legaspi\({}^{5}\), Kazushi Ikeda\({}^{5}\), Peter Spirtes\({}^{2}\), Kun Zhang\({}^{1,2}\)**

\({}^{1}\)Mohamed bin Zayed University of Artificial Intelligence, \({}^{2}\)Carnegie Mellon University,

\({}^{3}\)University of California San Diego, \({}^{4}\)The University of Sydney, \({}^{5}\)KDDI Research

###### Abstract

The application of reinforcement learning (RL) involving interactions with individuals has grown significantly in recent years. These interactions, influenced by factors such as personal preferences and physiological differences, causally influence state transitions, ranging from health conditions in healthcare to learning progress in education. As a result, different individuals may exhibit different state-transition processes. Understanding individualized state-transition processes is essential for optimizing individualized policies. In practice, however, identifying these state-transition processes is challenging, as individual-specific factors often remain latent. In this paper, we establish the identifiability of these latent factors and introduce a practical method that effectively learns these processes from observed state-action trajectories. Experiments on various datasets show that the proposed method can effectively identify latent state-transition processes and facilitate the learning of individualized RL policies.

## 1 Introduction

Reinforcement Learning (RL)  involves training agents to make decisions by interacting with the environment. The agent observes its current state, takes an action, and transitions to a new state with a reward. Such a sequence of moving from one state to another is known as a _state-transition process_.

Individualized RL focuses on adapting the policy for each individual. It has recently seen increasing application in various sectors, including healthcare , education , and e-commerce . The _individual-specific factors_, which capture the unique characteristics of each individual, play an important role in causally influencing the transitions between states. These factors cover a range of aspects, including individual preferences, past experiences, and physiological differences. For example, in education, different learning styles can influence how students with the same prior knowledge benefit from a tutorial. In healthcare, genetic differences can affect how patients with hypertension respond to the same treatments. Understanding individual-specific factors is essential for designing better RL systems that provide more individualized and effective decisions . By understanding learning styles in education, RL agents can recommend personalized tutorials, such as animated content for visual learners or hands-on exercises for kinesthetic learners. Similarly, in healthcare, knowledge of genetic makeup can help agents suggest treatment plans tailored to individual needs, leading to improved health outcomes.

However, these individual-specific factors are not always observable, making it challenging to understand the Latent Individualized State-Transition (LIST) processes, as illustrated in Figure 1(a). These latent factors, such as learning styles and genetic makeup, are unique to each individual and have a time-invariant influence on the state-transition process. This raises the question: can the identifiability of these latent factors be guaranteed?Such identifiability is easier to achieve when the observations are either i.i.d., or i.i.d. given side information (e.g., domain index, time index, etc.), by exploiting sparsity , variability , or functional complexity . To the best of our knowledge, only a few studies have explored the identifiability of latent factors from temporal observations. These methods primarily focus on the time-varying latent factors, which differs from our work on time-invariant latent factors. Specifically, existing works [83; 82; 6] assume time-varying latent variables without considering the influence of actions in the generative process (see Figure 1(b)). Factored MDPs  incorporate actions into the process but still assume that the latent factors change over time (see Figure 1(c)). Thus, the findings from these studies cannot be applied to our setting. Intuitively, this is because time-invariant latent factors cannot provide the variability that many current methods rely on to achieve identifiability. Therefore, it remains unclear how to derive the identifiability of the latent individual-specific factors and the corresponding latent state-transition processes from observed states and actions.

Recent advances in finite mixture models [74; 65] have demonstrated strong identifiability results by exploiting group information in nonparametric settings. By assuming that observations within the same group are known to come from the same component, the mixture of probability measures can be uniquely identified under appropriate assumptions. Inspired by these works, we establish the identifiability of latent factors by leveraging group information from the data, making it easier to distinguish different underlying components. We propose both a finite latent, nonparametric setting and an infinite latent, parametric setting, and develop a theoretically grounded framework that effectively learns these processes from observed state-action trajectories. Our contributions are summarized as follows:

* We introduce Individualized Markov Decision Processes (iMDPs), a novel framework that incorporates latent individual-specific factors \(\) into state-transition processes. We treat such latent factors that influence each state in the decision process and vary across individuals.
* Our work provides theoretical guarantees and new insights into learning state-transition processes with latent factors. For finite \(\), we consider two scenarios to establish identifiability in nonparametric settings. For infinite \(\), we demonstrate identifiability in the post-nonlinear case. To the best of our knowledge, this is the first work to provide a theoretical guarantee for identifying latent individual-specific factors from observed transitions.
* We propose a practical generative method that can effectively estimate the latent individual-specific factors. Empirical results on various datasets demonstrate the effectiveness of the method both in inferring latent factors and in learning individualized policies.

## 2 Related Work

Individualized Machine-Learning ApplicationsRecently, machine learning has created highly individualized solutions across various domains. In healthcare, algorithms support individualized treatment for physical activity, weight loss, and diabetes management [86; 55; 16; 15]. In finance, it provides accurate stock predictions for stock market activities . Education is benefiting from individualized ICT systems that address the individual learning needs of students [14; 38]. Furthermore, transportation has seen the development of individualized car-following strategies  that improve driving safety and efficiency. Meanwhile, entertainment platforms such as YouTube and TikTok are using it to provide individualized video recommendations [5; 31].

Reinforcement Learning with Latent State-Transition ProcessesIn the field of RL, various models explore the state transition dynamics with latent variables. One such approach is Partially Observable Markov Decision Processes (POMDPs) , where the full information about the state is

Figure 1: Comparisons of different state-transition processes. Latent variables are colored in grey.

unknown. In POMDPs, observations are generated from the latent states, which do not match our individual latent setting. For example, block MDPs [89; 93] assume that there is a fixed and unknown mapping from observations to the latent states. Factored MDPs [33; 13], which provide the partial identifiability of latent factors, assume that the latent factors evolve over time following a Markov process. On the other hand, there exists a piece of work focusing on estimating state transitions with time-invariant latent factors. Models such as contextual MDPs [27; 58; 63], latent MDPs [49; 48] and multitask RL [71; 22] consider similar scenarios with our latent individual-specific factors. However, these works lack theoretical guarantees on the identifiability of the latent factors thus it is hard for them to guarantee individualized decision-making.

## 3 Problem Formulation

Consider a population with \(M\) individuals that can be divided into \(G\) groups, where the exact group memberships are unknown. We introduce iMDPs to model individualized decision-making processes, where _observed_ individual uniqueness is captured by \(u\), and _latent_ group-level properties are encoded by \(\). Specifically, each individual is assigned a unique value of \(u\), with the cardinality of \(u\) being \(M\). Meanwhile, individuals within the same group share the same value of \(\), which differs across groups, and the cardinality of \(\) is \(G\). For each individual, the value of \(\) is predetermined and \(\) has a time-invariant influence on the state-transition process. Furthermore, all individuals are assumed to share the same state and action spaces. The iMDP is defined as follows.

**Definition 3.1** (iMDP).: _An iMDP consists of a tuple \(,,R,\{s_{0}^{m}\}_{m=1}^{M},\{u_{m}\}_{m=1}^{M}, \{_{m}\}_{m=1}^{M}\), where \(M\) is the number of individuals; \(\) and \(\) are the state and action spaces, respectively; \(R\) is the immediate reward received after transition from current state \(s\) to new state \(s^{}\) via action \(a\), i.e., \(r=R_{a}(s,s^{})\) for \(s\), \(s^{}\) and \(a\). To model individualized decision-making, each individual is associated with a unique index and an individualized state-transition process. In the \(m^{th}\) MDP, \(m_{m}\) is the unique index identifying each individual, and \(s_{0}^{m}\) is the individualized initial state. The individualized state-transition probability is denoted by \(_{m}:=_{m}(s^{}|s,a,_{m})^{| |||||}\), where \(_{m}\) is the latent individual-specific factor with a cardinality of \(G\). Thus, the joint distribution of any adjacent state-action pairs \((s,a,s^{})\) can be fully characterized by \(u\) and \(\) as:_

\[(s,a,s^{}|u)=(s^{}|s,a,)(s,a |u).\] (1)

Data Generation ProcessHere we introduce the LIST processes based on iMDP framework. For individual \(m\), the observed states \(_{t}^{m}\) are generated according to the following generation process:

\[_{t}^{m}=f(_{t-1}^{m},_{t-1}^{m},_{m}, _{t}^{m}),\] (2)

where \(_{t}^{m}=(s_{0,t}^{m},,s_{d_{d},t}^{m})^{}^{d _{s}}\) denotes the \(d_{s}\)-dimensional state at time \(t\), and \(_{t}^{m}=(a_{0,t}^{m},,a_{d_{a},t}^{m})^{}^{d _{a}}\) denotes the \(d_{a}\)-dimensional action at time \(t\). The term \(_{t}^{m}\) represents independent noise, while \(\) characterizes the group-level properties within the population. The transition function \(f\) is identical across individuals, governs the dynamics and is consistent with Eq. (1). During interaction with the environment, the trajectory \(_{m}=\{_{0}^{m},_{0}^{m},_{1}^{m},, _{T}^{m}\}\) is recorded as a sequence of observed state-action tuples, where \(T\) denotes the length of the trajectory.

ObjectivesIn this work, we investigate RL with a focus on capturing individualized latent state-transition dynamics. Our objectives are twofold: 1) to identify the latent individual-specific factors \(\) from observed trajectories, and 2) to learn individualized policies for each agent, facilitating policy adaptation for newcomers. Consider the example of hypertension diagnosis in healthcare. Treating all patients identically may lead to different outcomes due to the dynamics of state transitions, which are influenced by latent \(\). Therefore, accurate identification of \(\) from the population provides crucial dynamic insights. Once \(\) is uncovered, we can categorize patients into different groups and tailor individualized treatments for each, which motivates our second goal.

## 4 Identifiability Analysis

We establish the identifiability of latent individualized state-transition processes under two conditions: (1) finite latent condition under group determinacy assumption (see Theorem 4.1 and Theorem 4.2) and (2) infinite latent condition under functional constraint (see Theorem 4.3). The corresponding identifiability results are presented below.

Finite Latent ConditionSuppose the value of \(\) is finite; we first provide the definition of group-wise identifiability. For the detailed assumptions discussion and proofs, please see Appendix B.

**Definition 4.1** (Group-wise Identifiability).: _Let \(\{_{m}\}_{m=1}^{M}\) be sequences of observed states and actions collected from \(G\) groups under a fixed policy, following the true latent individualized state-transition processes described in Eq. (2). A learned generative model \((,,)\) is observational equivalent to \((f,,)\) if the joint distribution \(_{,,}(s,a,s^{})\) matches \(_{f,,}(s,a,s^{})\) everywhere. We say that the latent individualized state-transition processes are group-wise identifiable if observational equivalence can always lead to the identifiability of latent individual-specific factors across the population up to the invertible transformation \(g\):_

\[_{,,}(s,a,s^{})=_{f,,}(s,a,s^{})=g().\] (3)

**Assumption 4.1** (Group Determinacy).: _Consider a finite mixture model \(_{g=1}^{G}_{g}_{_{g}}()\), where \(_{g}\) represents mixing proportions with \(_{g=1}^{G}_{g}=1\), and \(_{_{g}}\) is the Dirac function centered at \(_{g}\). Each unique value of \(\) corresponds to a specific group in the population, with \(_{_{g}}()=1\) if \(=_{g}\) and 0 otherwise, and the number of individuals per group is greater than \(2G-1\)._

Assumption 4.1 guarantees that identifiability can be derived from the finite mixture model perspective using group information. We consider two scenarios to establish identifiability under finite latent conditions. Theorem 4.1 considers finite samples, establishing identifiability under specific assumptions about the initial states \(\{_{0}^{n}\}_{m=1}^{M}\). Theorem 4.2 guarantees asymptotic identifiability for sufficiently long trajectories without imposing constraints on the initial states.

**Theorem 4.1**.: _Assume the LIST processes described in Eq. (2). Suppose the distributions of initial states are the same for all individuals within the same groups, and the trajectory length is finite. Under Assumption 4.1, the identifiability of the individual-specific factor \(\) is guaranteed._

**Theorem 4.2**.: _Assume the LIST processes described in Eq. (2). Suppose the distribution of the initial state varies across individuals, and the trajectory length is sufficiently long, i.e., there exist two different individuals within the same group have overlap condition \((s,a|u=u_{i})=(s,a|u=u_{j})\), where \(i j\). Under Assumption 4.1, the identifiability of \(\) is asymptotically guaranteed._

Infinite Latent ConditionTheorem 4.3 demonstrates that under specific functional constraints, the identifiability of latent individual-specific variables can be extended to multiple and infinite latent factors. Specifically, we consider the post-nonlinear temporal model  and allow multiple instances of \(\) to influence the state-transition dynamics. The identifiability and cardinality of latent factors are decided by the rank conditions of specific covariance submatrices derived from the observed data. Furthermore, empirical results in Section 6 indicate that even when multiple latent factors with infinite cardinality are present, our estimation framework (see Section 5) still encourages the identification of latent factors. For a detailed proof, please see Appendix C.

**Theorem 4.3**.: _Consider a trajectory collected from the post-nonlinear temporal model (Definition C.1) with \(d_{s}\)-dimensional observed states over time \(t=1,,T\). Let \(m\) latent factors \(_{j}\), \(j=1,,m\), have direct causal influence on all states, and \(_{t}=\{s_{1,t},s_{2,t},,s_{d_{s},t}\}\) represent the set of all state variables at time \(t\). These latent factors, as well as the state-transition process, can be identified if and only if for every \(i=m+2,,T-(m+1)\), there exist pairs of minimal rank sets (Definition C.2) \((},})\), denoted as \(}=_{i,i^{-}}\) and \(}=_{i,i^{+}}\), where \(i^{-}<i<i^{+}\), that satisfy:_

* _(Rank Deficiency for Identification) In addition to_ \(_{i}\) _shared by_ \(}\) _and_ \(}\)_, each subset should include a randomly selected set of_ \(m+1\) _additional state variables. If the covariance matrices_ \(_{},}}\) _exhibit a consistent rank_ \(r\) _(where_ \(r>d_{s}\)_) across all distinct indices_ \(i\)_, this consistency implies the existence of latent factors within the system._
* _(Quantification of Latent Factors) Once identification is established, the number of latent factors_ \(m\) _can be inferred from the rank deficiency of_ \(_{},}}\) _relative to the dimensionality of the observed variables, specifically given by_ \(m=(_{},}})-d_{s}\)_._

## 5 Estimation and Policy Learning Framework

OverviewWe propose a two-stage approach that addresses two key objectives: (1) developing an estimation framework to recover the latent factors \(\) from individual trajectories, and (2) implementingindividualized policy learning to facilitate policy adaptation for new individuals. The estimation framework is designed to meet the conditions outlined in the identifiability theorems. According to Definition 4.1, identifiability is achieved if and only if observational equivalence implies latent factor equivalence. This motivates the use of a generative model to estimate latent factors and ensure that the reconstructed distribution aligns closely with the true observed distribution.

To fulfill this requirement, we make a modification to the variational autoencoder (VAE) architecture , which enables the unsupervised estimation of latent factors. Theorems 4.1 and 4.2 guarantee the asymptotic accuracy of this alignment. As illustrated in Figure 2, individual trajectories are encoded in a discrete embedding space, which is consistent with the assumptions in the theorems. Detailed pseudocode for the proposed approach is provided in Appendix I, and a comprehensive description of each component is provided in Appendix H.

### Latent Estimation Framework

Temporal Encoding and Latent Factor QuantizationThe group determinacy assumption suggests the identifiability of the latent individual-specific factor \(\). Given that \(\) is time-invariant and influences each state in the transition process, we begin by employing a sequential encoder to capture the high-level representation, denoted as \(z_{m}\), based on the input from all states across each trajectory. We then utilize a vector quantization layer  to discretize the latent space and estimate the latent factor \(_{m}\). This quantization step ensures that the learned representation aligns with the group-level characteristics of the latent factors, as assumed in our framework, thereby supporting our objectives.

Specifically, to capture temporal dependency from sequential observations, we use sequential neural networks such as Conv1D  or Long Short-Term Memory (LSTM)  as encoders. The encoder function, denoted as \(g\), maps the input trajectory \(\{_{0}^{m},,_{T}^{m}\}\) to a continuous high-level representation \(z_{m}=g(_{0}^{m},,_{T}^{m})\), where Conv1D extracts local temporal patterns from subsequences, and LSTM aggregates information over time by sequentially updating its hidden and cell states. After processing the whole trajectory sequentially, the final hidden state of the LSTM and the final output of the Conv1D layer serve as the representation \(z_{m}\).

However, continuous representations \(z_{m}\) are incompatible with our framework's requirements. To address this, we apply a vector quantization layer to discretize the latent space and approximate the latent factor. This layer maps \(z_{m}\) to the nearest vector in a predefined embedding dictionary \(E=\{e_{1},e_{2},,e_{G}\}\), where each vector \(e_{i}\) represents a distinct group in the discrete embedding space. The assignment is realized by finding the nearest neighbor in the dictionary as \(_{m}=_{e_{i}}\|z_{m}-e_{i}\|_{2}\), where \(_{m}\) represents the quantized vector \(e_{i}\) closest to the continuous representation \(z_{m}\).

Latent Factor Estimation via Conditional ReconstructionReconstruction plays a critical role in unsupervised latent factor estimation by ensuring that the reconstructed distribution closely matches the true observed distribution. As stated in Definition 4.1, this alignment allows the estimated latent factors to approximate the true latent factors. Given the nature of the transition processes, we design a conditional decoder that uses the state-action pairs \((_{t-1}^{m},_{t-1}^{m})\) as conditions to guide the reconstruction of \(}_{t}^{m}\). These conditions, together with the estimated latent factors \(_{m}\), serve as inputs to the decoder. The reconstruction accuracy is evaluated through the reconstruction likelihood \(p_{}(}_{t}^{m}|_{t-1}^{m},_{t -1}^{m},_{m})\), where \(p_{}\) denotes the reconstruction distribution. It provides a probabilistic measure of how well \(}_{t}^{m}\) approximates \(_{t}^{m}\). This likelihood quantitatively evaluates the performance of the decoder and the ability of the model to accurately reconstruct observed states.

Training ObjectivesThe parameters are optimized according to the following ELBO objective:

\[_{}=_{}+_{ {Quant}}+_{}\] (4)

where \(\) and \(\) are weights for the corresponding loss components. Specifically, (1) _Reconstruction loss_\(_{}=_{t}\|_{t}^{m}-(( _{0:T}^{m}),_{t-1}^{m},_{t-1}^{m})\|^{2}\). This measures the discrepancy between the reconstructed state \(}_{t}^{m}\) and the original state \(_{t}^{m}\), where En and De are the encoder and decoder, respectively. (2) _Quantization loss_\(_{}=_{i}\|[z_{m,i}]-e_{m,i}\|^{2}\). This evaluates the discrepancy between the encoder output \(z_{m}\) and its discretized representation \(e_{m}\). Since the quantization step is undifferentiable, we use the stop-gradient operation \([]\) to update the dictionary vectors without affecting the encoder parameters. (3) _Commitent loss_\(_{}=_{i}\|z_{m,i}-[e_{m,i}]\|^{2}\). This term minimizes the discrepancy between \(z_{m}\) and \(e_{m}\), encouraging \(z_{m}\) to align closely with the embeddingspace. By applying the stop gradient to \(e_{m,i}\), gradients from this loss do not change the dictionary vectors, but instead optimize the encoder parameters.

### Policy Learning Framework

The estimation network is pre-trained offline. When a new individual arrives, we estimate \(\) and adapt the policy simultaneously through new interactions. Specifically,

Latent-based Policy IndividualizationThe estimated latent individual-specific factors \(\), together with the offline trajectories from all individuals, are used to learn the individualized policy \(^{*}_{}\). We view the estimated factors as an augmented component of the policy input, and the policy training objective is adjusted to match the unique characteristics of each individual.

Take Q-learning  as an example. In the individualized process, the latent factor is augmented as a policy input as \(_{}(_{t};^{})_{}^{m}(_{t} ^{m},_{m};^{})\), where \(^{}\) represents the parameters of the policy network. This incorporation allows the policy to adapt effectively to individual-specific dynamics. The training objective is updated accordingly as \((^{})=[_{t=0}^{}^{t}Q (_{t},_{}^{m}(_{t}^{m},_{m}; ^{});^{Q})]\), where \(\) is the discount factor, and \(Q\) represents the Q value. Such individualization improves policy adaptability across varying environments, and our framework is general enough to be seamlessly integrated with various RL algorithms.

Policy Adaptation for New IndividualPolicy adaptation for a new individual involves two steps: initializing the policy from the individualized policy \(^{*}_{}\) and fine-tuning it through new interactions. For a new individual from group \(g_{n}\), the group factor \(_{n}\) is first estimated. The policy network \(_{}\) is then initialized by directly transferring parameters from \(^{*}_{=_{n}}\). Specifically, \(_{}=_{}_{(_{t},_{t})  D_{n}}(R_{_{t}}(_{t}))\) is fine-tuned based on the new individual's trajectory \(D_{n}\) by maximizing the expected reward. The dataset \(D_{n}\) is incrementally augmented with new observations \((_{t},_{t},R_{t},_{t+1})\) collected under \(_{}\). This process refines the policy to better fit the specific characteristics of the new individual. Additionally, the updated \(D_{n}\) helps to accurately estimate the latent factor of the new individual, further improving the adaptation process.

## 6 Experiment

Evaluation MetricsTo measure the latent identification, we quantify the correlation between the estimated and true latent factors using: (1) Pearson Correlation Coefficient (PCC) for single latent, which quantifies the linear correlation between the estimated and true factors, and (2) Kernel Canonical Correlation Analysis (KCCA) for multiple latents, which evaluates the correlation between sets of estimated and true factors. An absolute value close to 1 indicates a strong correlation and better latent recovery. To evaluate the control performance, we measure the adaptation performance using: (3) jumpstart, which captures the improvement in initial performance when a learning agent leverages knowledge from source tasks; (4) accumulative reward, which indicates the learning quality over the

Figure 2: (a) Latent estimation framework takes each trajectory \(_{0:T}\) as input and processes it through a quantized encoder to estimate the latent factor \(\). A conditional decoder then reconstructs \(}_{t}\), using \((_{t-1},_{t-1})\) as condition and \(\) as input. (b) Once the estimated latent factors are assigned to each trajectory, the policy learning framework integrates these latents as augmented labels to optimize the RL policy. For new individuals, the initial policy is adapted based on their group affiliation, enabling individualized policy adaptation for newcomers.

learning process; and (5) initial and final reward, which measures the initial performance benefiting from policy adaptation and the performance after the full training process.

BaselinesFor estimation evaluation, we compared with: (1) Disentangled Sequential Autoencoder , which disentangles latent representations into static and dynamic parts. However, it does not capture the global influence of \(\), limiting its ability to model individual-specific factors. (2) Population-level component, which embeds latent factors based on population data instead of individual-specific information. For policy evaluation, our baselines include: (3) Aligned Latent Models , which jointly optimize a latent-space model and policy to maximize returns. (4) Soft Actor-Critic (SAC) , which incorporates entropy into the objective function to encourage exploration and improve robustness. (5) Deep Deterministic Policy Gradient (DDPG) , which combines deterministic policy gradients with deep neural networks for continuous action spaces. (6) Dueling Double Deep Q-Network (D3QN) , which introduces a dueling architecture for value function estimation to improve value estimation. (7) Rainbow DQN , which integrates prioritized experience replay and dueling network architectures to improve performance and learning stability.

### Evaluation on Latent Estimation Framework

Synthetic ExperimentsWe first conduct experiments on the synthetic dataset to evaluate the effectiveness of the estimation framework. The dataset is manually generated based on the post-nonlinear model. We design three types of latent factors \(\), each either satisfying or violating the required assumptions. **Case 1**: \(\) is a finite latent factor following the categorical distribution \((0.1,0.2,0.3,0.4)\) with a cardinality of 4. **Case 2**: \(\)s are three-dimensional finite latent factors, each following a categorical distribution \((0.2,0.8)\), \((0.2,0.3,0.5)\), \((0.1,0.2,0.3,0.4)\), with cardinality equal to 2, 3, 4, respectively. **Case 3**: \(\)s are three-dimensional infinite latent factors, and each factor follows a Gaussian distribution \((0,1)\), uniform distribution \((0,1)\), and exponential distribution \((1)\), respectively. We synthetically generate 40 unique trajectories, each representing an individual, with a maximum trajectory length of 20.

For Case 1, we use PCC to evaluate estimation performance and report the training curve in Figure 3(a), where shaded regions indicate the standard deviation. The comparative results show that our method can recover the true latent factors, outperforming other baselines. Specifically, the population-level component fails to identify the individual-specific factor, as it overlooks the underlying differences between groups. Additionally, although the disentangled sequential autoencoder shows compromised identifiability in the early training stage by capturing the static part of the latent space, it fails to achieve full identifiability due to its inability to model individualized transition processes, leading to worse recovery performance over time. For Cases 2 and 3, we use KCCA to quality the correlation and visualize the results through scatter plots in Figure 3(b) and Figure 3(c). These results demonstrate identifiability in both finite and infinite latent factor scenarios and support the theoretical claims.

Moreover, we slightly violate the restriction on the number of individuals per group specified in Assumption 4.1 and analyze its impact on the training curve under varying population sizes, as shown by the dashed lines in Figure 3(d). The result shows that satisfying the sample sufficiency condition is necessary to recover the latent factors. In addition, we evaluate the effect of the trajectory length as outlined in Theorem 4.2. The findings, shown by the solid lines, demonstrate that increasing the

Figure 3: Synthetic results. (a) Comparisons of PCC trajectories in Case 1. (b-c) Scatterplot of the canonical variables in Cases 2 and 3. (d) Identifiability performance responses of the sample size.

trajectory length would significantly improve the identifiability performance. This observation aligns with the theoretical guarantees provided by the proposed theorem.

Ablation StudyThe contributions of the different components in the latent estimation framework are reported in Table 1. We build on the autoencoder framework with a quantization layer and add each component sequentially to the previous module. Incorporating a sequential encoder significantly improves the identifiability, which is important for the accurate recovery of latent factors. In the implementation, we use a noise estimator during optimization to minimize bias and improve identifiability. The results suggest that the added components help fine-tuning the overall performance of the model, allowing for more accurate and reliable recovery of latent individual-specific factors.

PersuasionForGood CorpusWe further evaluate our framework on the real-world dataset, PersuasionForGood corpus , which is widely used for analyzing persuasion strategies [64; 7; 87]. The dataset contains 1017 person-to-person dialogues and 32 personality traits for each participant. In each dialogue, the persuade attempts to convince the persuade to donate to a charity. In the context of iMDP, the state represents the persuadee's response, the action corresponds to the persuadeer's utterance, and the reward is defined as the final donation. Since this offline dataset does not include real-time interactions required for control performance evaluation, we focus on identifying the latent personality traits of each individual. We use BERT  as the backbone to embed each utterance into a 768-dimensional feature representation. These features are processed through an LSTM encoder, followed by a quantization layer, to recover the latent personality traits. The canonical correlation results under different latent dimensions are shown in Figure 4(a). The results demonstrate that our method is effective and feasible for real-world applications, particularly when the latent dimensions are fine-tuned appropriately.

### Evaluation on Policy Learning Framework

PendulumPendulum  is a continuous control task for RL study with the goal of swinging up and stabilizing in an upright position. The states are the x-y coordinates and the angular velocity, and the action is the torque applied to the pendulum. For simplicity, we choose DDPG as the basic optimization algorithm and manually create 20 individualized environments. In these environments, the gravity \(g\) is randomly drawn from a categorical distribution over the set \(\{3,,12\}\). The performance of the policy adaptation is evaluated on a new individual with \(g=10\).

We compare our method against several baselines: (1) SAC; (2) DDPG without prior knowledge; (3) aligned latent models; (4) pre-trained DDPG incorporating knowledge from given individuals, termed cross-individual transfer; (5) individualized policy incorporating randomly defined group embedding, termed random group embedding. The training curves over accumulative reward are reported in Figure 4(b), showing that the proposed method outperforms other baselines in both jumpstart and accumulative reward. Specifically, methods that benefit from population knowledge (our method and cross-individual transfer) outperform non-transfer methods, indicating that the pre-trained policy would accelerate the learning process. However, since cross-individual transfer ignores the individual

  
**Module** & **PCC \(()\)** & **Bias \(()\)** \\  Quantized Encoding & 0.646 \(\) 3.1e-04 & 0.099 \(\) 2.3e-04 \\ + Sequential Encoder & 0.910 \(\) 1.3e-04 & 0.077 \(\) 5.7e-06 \\ + Noise Estimator & 0.942 \(\) 4.0e-05 & 0.072 \(\) 3.0e-07 \\   

Table 1: Contribution of each module.

Figure 4: (a) Canonical correlation with respect to the latent dimensions in the PersuasionForGood corpus. (b-d) Accumulative reward curves in Pendulum, HeartPole, and Half Cheetah, respectively.

specific information, such mixed policy knowledge yields worse initial performance compared to the individualized policies derived from our method.

HeartPoleHeartPole  is a discrete healthcare environment that explores the long-term health outcomes of short-term decisions. The six-dimensional states represent different health conditions, including alertness, hypertension, intoxication, time since sleep, time elapsed, and work done. Actions can be chosen from work, coffee, alcohol, and sleep. We create 100 individualized scenarios and assign each patient with individual characteristics, such as coffee tolerance, hypertension risk, and alcohol tolerance, according to a categorical distribution over the set \(\{0.6,0.8,1.2\}\). The adaptation performance is evaluated on a new individual with all indices set to \(1\).

We compare our method against the following baselines: (1) D3QN without prior knowledge, (2) Rainbow DQN, (3) cross-individual transfer with D3QN, and (4) random group embedding. The training curves over accumulative reward are shown in Figure 4(c), and our method outperforms other baselines in both jumpstart and accumulative reward. Interestingly, while inappropriate source domain knowledge can negatively impact control performance (see cross-individual transfer), the result from random group embedding indicates that incorporating group embedding knowledge can enhance generalization. The group structure, together with properly estimated group information, jointly enables our method to converge better and faster than other baselines.

Half CheetahHalf Cheetah  is a Mujoco-based task aiming to control a 2D bipedal robot. The agent consists of 9 links and 8 joints, and the goal is to apply torque to the joints to make the cheetah run forward as fast as possible. We introduced 50 individualized settings with the gravity \(g\) following a categorical distribution with probabilities \(p=0.2\) and corresponding \(g\) values from \(\{8,8.5,,10\}\). The adaptation performance is evaluated on a new individual with \(g=9.8\). We compare our method with (1) DDPG without prior knowledge, (2) aligned latent models, (3) cross-individual transfer with DDPG, and (4) random group embedding. The comparative results, shown in Figure 4(d), indicate that our method outperforms all baselines in terms of convergence speed and efficiency. While inappropriate source domain data can degrade performance (see cross-individual transfer), the inclusion of group embedding facilitates generalization, enabling more effective adaptation.

## 7 Conclusion and Limitations

Our work focuses on learning latent state-transition processes from observed state-action trajectories, guaranteeing identifiability in the presence of latent individual-specific factors. To the best of our knowledge, this study provides novel identifiability guarantees for several settings that have not been previously addressed. Despite these contributions, our approach has three major limitations. (1) It currently does not account for instantaneous causal dependencies within \(_{t}\). This limitation could be addressed by adjusting the temporal resolution of the data and explicitly modeling these dependencies. Integrating causal graphical models or advanced inference techniques could further enhance the framework's ability to handle instantaneous causal relationships. (2) While empirical results suggest the framework may adapt to scenarios with continuous latent factors, a formal nonparametric proof for these settings remains absent. Developing such a proof is an important avenue for future work. (3) The model assumes latent factors are time-invariant. Establishing such theoretical identifiability for time-varying latent factors is highly non-trivial and would require additional theoretical constraints to establish identifiability. These limitations highlight key directions for future research.

Furthermore, practical concerns such as privacy, robustness, and reliability are essential for real-world applications. To address privacy risks, techniques like de-identification (e.g., removing direct identifiers, data perturbation, pseudonymization) and differential privacy approaches should be explored. Ensuring privacy and security will enhance the framework's applicability in practice.