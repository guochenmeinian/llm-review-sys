# Sharing Key Semantics in Transformer Makes

Efficient Image Restoration

Bin Ren\({}^{1,2,3}\)1 & Yawei Li\({}^{4}\)2 & Jingyun Liang\({}^{4}\) & Rakesh Ranjan\({}^{5}\) & Mengyuan Liu\({}^{6}\)

**Rita Cucchiara\({}^{7}\)** **Luc Van Gool\({}^{3}\)** **Ming-Hsuan Yang\({}^{8}\)** **Nicu Sebe\({}^{2}\)**

\({}^{1}\)University of Pisa \({}^{2}\)University of Trento \({}^{3}\)INSAIT, Sofia University \({}^{4}\)ETH Zurich \({}^{5}\)Meta Reality Labs

\({}^{6}\)State Key Laboratory of General Artificial Intelligence, Peking University, Shenzhen Graduate School

\({}^{7}\)University of Modena and Reggio Emilia \({}^{8}\)University of California, Merced

###### Abstract

Image Restoration (IR), a classic low-level vision task, has witnessed significant advancements through deep models that effectively model global information. Notably, the emergence of Vision Transformers (ViTs) has further propelled these advancements. When computing, the self-attention mechanism, a cornerstone of ViTs, tends to encompass all global cues, even those from semantically unrelated objects or regions. This inclusivity introduces computational inefficiencies, particularly noticeable with high input resolution, as it requires processing irrelevant information, thereby impeding efficiency. Additionally, for IR, it is commonly noted that small segments of a degraded image, particularly those closely aligned semantically, provide particularly relevant information to aid in the restoration process, as they contribute essential contextual cues crucial for accurate reconstruction. To address these challenges, we propose boosting IR's performance by sharing the key semantics via Transformer for IR (_i.e._, SemanIR) in this paper. Specifically, SemanIR initially constructs a sparse yet comprehensive key-semantic dictionary within each transformer stage by establishing essential semantic connections for every degraded patch. Subsequently, this dictionary is shared across all subsequent transformer blocks within the same stage. This strategy optimizes attention calculation within each block by focusing exclusively on semantically related components stored in the key-semantic dictionary. As a result, attention calculation achieves linear computational complexity within each window. Extensive experiments across 6 IR tasks confirm the proposed SemanIR's state-of-the-art performance, quantitatively and qualitatively showcasing advancements. The visual results, code, and trained models are available at https://github.com/Amazingren/ SemanIR.

## 1 Introduction

Image restoration (IR) stands as a fundamental task within low-level computer vision, aiming to enhance the quality of images affected by numerous factors, including noise, blur, low resolution, compression artifacts, mosaic patterns, adverse weather conditions, and other forms of distortion. This capability holds broad utility across various domains, facilitating information recovery in medical imaging, surveillance, and satellite imagery. Furthermore, it bolsters downstream vision tasks like object detection, recognition, and tracking . Despite notable progress in recent years, prevalent IR methods encounter challenges in effectively addressing complex distortions or preserving/recovering crucial image details . Achieving high-quality image recovery necessitates meticulous exploration of the rich information present in degraded counterparts.

In modern IR systems, representative networks for learning rich image information are typically constructed using three fundamental architectural paradigms. _i.e._, convolutional neural networks (CNNs) , Multilayer perceptrons (MLPs) , and Vision Transformers (ViTs) . The input image is treated as a regular grid of pixels in the Euclidean space for CNNs (Fig. 1(a)) or a sequence of patches for MLPs and ViTs (Fig. 1(b)). However, degraded inputs usually contain irregular and complex objects. While these choices perform admirably in scenarios with regular or well-organized object boundaries, they have limitations when applied to degraded images with more flexible and complex geometrical contexts.

Additionally, CNNs struggle to model long-range dependencies due to their limited receptive field (Fig. 1(a)). In contrast, MLPs and ViTs can capture long-range relations effectively, although at the cost of losing inductive bias and incurring a heavy computational burden, _i.e._, quadratic complexity increases with higher input resolution. To address these limitations, recent IR methods have explored strategies for complexity reduction. A common approach is to implement MSA within local image regions . For example, SwinIR  and GRL  employ full MSA or region-fixed anchored stripe MSA, but they still struggle with irregular object connections. Furthermore, prior research  highlights that smooth image content is more prevalent than complex image details, underscoring the need for distinct treatment based on semantic content.

In this paper, we introduce a novel approach, SemanIR, to address the limitations above. Specifically, within each transformer stage, we first construct a key-semantic dictionary, which stores only the top-k semantically related relations for each given degraded patch with the k-nearest neighbors (_i.e._, KNN) algorithm. Then, the attention operation within each transformer layer occurs only among the top-k patches. This design brings two main advantages, _i.e._, 1) Each degraded patch benefits from its semantically similar patches, typically containing comparable contextual or textural information, while excluding the side effects from other patches that contain entirely unrelated information. 2) Compared to the conventional window-wise attention, which built a dense connection between all the patches (Fig. 1(c)) that leads to highly computationally demanding, or a sparse but position-fixed manner (Fig. 1(d)) which introduces irrelevant semantics. Our key-semantic connection (Fig. 1(e)) leads to a sparse yet more meaningful attention operation, which allows our method to achieve the same receptive field as previous ViTs-based methods while maintaining lower computational costs. This is not like previous token merging or pruning methods  that may merge unrelated information or prune some semantically related information. In addition, to make the proposed method more efficient, instead of creating a key-semantic dictionary for each transformer layer, we create it just once at the beginning of each transformer stage and then share it with all the following transformer layers within the same stage. This not only largely reduced the computation burden but also made our methods different from other token pruning and merging methods , which include dynamic patch skipping/selection within each attention layer or involve an additional offset generation network. Meanwhile, merging or pruning tokens will lead to a loss of information in corresponding patches, which is not preferred in image restoration . In addition, such a sharing strategy allows each degraded patch to be continuously optimized by its semantically related patches within each stage.

It is also worth noting that the implementation of the attention layer of our SemanIR is achieved in three interesting manners (_i.e._, Triton 3, torch-mask, and torch-gather), which are discussed in our ablation studies. Overall, our method's suitability for image restoration comes from the

Figure 1: (a) The CNN filter captures information only within a local region. (b) The standard MLP/Transformer architectures take full input in a long-sequence manner. (c) The window-size multi-head self-attention (MSA) mechanism builds a full connection within each window. (d) Position-fixed sparse connection. (e) The proposed _Key-Semantic_ connection.

utilization of semantic information, the preservation of the details, and the effective KNN strategy. The contributions of this work are:

1. For each degraded input patch, we propose to construct a key-semantic dictionary that stores its most semantically relevant \(k\) patches in a sparse yet representative manner. This strategy excludes the side effects of a given degraded patch from semantically unrelated parts.
2. Based on the constructed key-semantic dictionary, we propose to share the key semantic information across all the attention layers within each transformer stage, which not only makes each degraded patch well-optimized but also largely reduces the computational complexity compared to conventional attention operations.
3. Extensive experimental results show that the proposed SemanIR achieves state-of-the-art performance on 6 IR tasks, _i.e._, deblurring, JPEG compression artifact removal (JPEG CAR), denoising, IR in adverse weather conditions (AWC), demosaicking, and classic image super-resolution (SR).

## 2 Related Work

**Image Restoration (IR),** as a long-standing ill-posed inverse problem, is designed to reconstruct the high-quality image from the corresponding degraded counterpart with numerous applications [72; 3; 49]. Initially, IR was addressed through model-based solutions, involving the search for solutions to specific formulations. However, learning-based approaches have gained much attention with the significant advancements in deep neural networks. Numerous approaches have been developed, including regression-based [51; 41; 50; 48; 104] and generative model-based pipelines [25; 87; 56; 92]. In this paper, we propose a regression-based method for image restoration.

**Non-Local Priors Modeling in IR.** Tradition model-based IR methods reconstruct the image by regularizing the results (_e.g._, Tikhonov regularization ) with formulaic prior knowledge of natural image distribution. However, it is challenging for these methods to recover realistic detailed results with hand-designed priors. Besides, some other classic method finds that self-similarity is an effective prior, which leads to an impressive performance [7; 17]. Apart from traditional methods, the non-local prior has also been utilized in modern deep learning networks [86; 48; 109], typically captured by the self-attention mechanism. More recently, the overwhelming success of transformers  in the natural language processing domain  and the classic vision community [20; 8; 81; 90; 10] has led to the development of numerous ViT-based IR methods. These methods aim to enhance the learning ability for modeling non-local priors [50; 93; 48; 14; 15; 98] and consequently archives better performance. Meanwhile, this raises a question: are all non-local priors essential for IR?

**Key-Semantic Non-local Prior Exploration for IR.** To answer the question, we found many methods demonstrating the effectiveness of modeling key semantics within ViTs. For example, KiT  proposed increasing the non-local connectivity between patches at different positions through KNN matching. This approach aims to better capture the non-local relations between the base patch and other patches in each attention calculation. However, it results in significant additional computational costs due to the KNN matching. DRSformer  proposed a top-k selection strategy that chooses the most relevant tokens to model the non-local priors for draining after each self-attention calculation without reducing the computation complexity, since after each attention calculation, the DRSFormer utilized (mask, top-k, scatter) operations at each transformer layer. Consequently, this inevitably increases the computation cost. Similar conclusions can be also drawn from the graph perspective solutions [28; 73; 61; 32] for various IR tasks, like facial expression restoration , image denoising , and artifact reduction .  construct the graph with transformer-based architecture where each patch is connected to all other patches. All these methods suggest that if the semantically related information can be addressed, the degraded image can be restored with better performance. However, the efficiency issue, which is extremely unignorable, remains untouched within the aforementioned methods. It is particularly crucial for ViTs-based image restoration methods, which often need to address high-resolution degraded input images. LaViT  reduces computational costs by storing attention scores from a few initial layers and reusing them in subsequent layers. However, this approach does not change the computation cost of attention itself; it merely reuses previously computed scores. In this paper, we propose sharing key semantics within each transformer stage, demonstrating its efficiency and effectiveness through experimental and theoretical analysis. Our method, SemanIR, reduces computation in both training and inference by using a semantic dictionary to filter out irrelevant patches during training and optimizing attention operations with Triton kernels during inference.

## 3 Methodology

To comprehensively study the effectiveness of the proposed method that is architecture-agnostic for various IR tasks, we adopted two of the most commonly used architectures _i.e._the columnar architecture (shown in Fig. 2 (a)) for image SR and the U-shaped architecture (shown in the _Appendix_, _i.e._, _Appx_. A.2) for other IR tasks. In the following, we first show how to construct the key-semantic dictionary in Sec. 3.1. Based on the key-semantic dictionary, then we explain why sharing it works for IR, and we introduce the basic unit, the key-semantic transformer layer in Sec. 3.2. Finally, two interesting discussions (Sec.3.3) are introduced regarding the implementation style of the Key-Graph attention and two top-k settings during the training. The efficiency analysis is provided in _Appx_. A.3.

### Key-Semantic Dictionary Construction

Consider the input feature \(F_{in}^{H W C}\), where \(H\), \(W\), and \(C\) denote the height, the width, and the channel. ViTs are good at modeling global dependencies for \(F_{in}\). This is achieved by the MSA, the core of ViTs, by connecting all other patches to a certain patch. Specifically, \(F_{in}\) is first split into \(N\) non-overlapping patches, forming its patch representation \(=\{p_{i}|p_{i}^{hw c},i=1,2,3,...,N\}\), where \(h\), \(w\), and \(c\) are the height, the width, and the channel of each patch. To achieve such global connectivity \(\), \(\) is linearly projected into Query (\(Q\)), Key (\(K\)), and Value (\(V\)) matrices, which are denoted as \(Q=_{}\), \(K=_{}\), and \(V=_{}\). \(_{}\) represents the learnable projection weights. Then \(\) is performed by a softmax function as follows:

\[_{ij}=K_{j}^{})}{_{k=1...j}(Q_{i}K_{k}^{ }/)},i=1,2,3,...,N,\] (1)

where \(d\) is the dimension of \(Q\) and \(K\). Then each patch is aggregated via \(_{i}_{ij}V_{i}\). However, \(_{ij}\) functions as a full semantic dictionary, where each patch is connected to all other patches regardless of their semantic relatedness. _e.g._, given a sub-graph with a green dog patch shown in Fig. 1(c), the tree-related patches are also considered. Since such an operation occurs at each attention calculation step in ViTs, it inevitably increases the computational cost, especially for large-scale inputs. In addition, for IR, a degraded patch usually benefits from its most semantically related patches, as they share similar texture and geometric information. This naturally raises the question: _Can we build a key-semantic dictionary_, \(_{K}\), _where each patch is connected only to its most related patches?_

Figure 2: The proposed SemanIR mainly consists of a convolutional feature extractor, the main body of SemanIR for representation learning, and an image reconstructor. The main body in columnar shape shown here is for image SR, while the U-shaped structure (shown in _Appx_. A.2) is used for other IR tasks. (b) The transformer layer of our SemanIR. The toy example of \(k\)=3 for (c) the Key-semantic dictionary construction and (d) the attention of each Layer.

To mitigate this problem, given \(\), we first construct a fully connected dictionary \(\) by calculating its self-similarity \(()\) via a naive dot product operation as \((i,j)=(i,j)=p_{i} p_{j}^{}\), which describes the correlation among all the patches, with higher values indicating stronger correlations. To reduce the side influence of patches with low correlation (_e.g._, the tree-related patches at the upper left part in Fig. 1 (c)) for the green background dog destination patch, we keep only \(k\) highly related patches and exclude the remaining. This is achieved by a KNN algorithm from \(\) as follows:

\[_{K}(i,j)=(i,j),&(i,j) (i,)_{k}i j\\ 0,&,\] (2)

where \((i,)_{k}\) denotes the \(k_{th}\) largest connectivity value of patch \(p_{i}\). As a result, \(_{K}\) contains only the patches with high correlation (_e.g._, dog-related patches in Fig. 1(e)) for the destination patch (_e.g._, the green dog patch). We formalize the key-semantic dictionary construction as \(()\) in Alg. 1. Although such a dictionary allows the subsequent attention operation to focus on the most semantically related patches, constructing it before each attention operation significantly increases computational costs. Meanwhile, we observed that IR architectures typically use transformers stage-by-stage (See Fig. 2(a)). This means that in each stage, several transformer layers are directly connected sequentially and operate at the same semantic level. Inspired by this, we propose to _share the same key-semantic dictionary_ for all the transformer layers.

### Sharing Key Semantics Cross Transformer Layers

The structure of each transformer layer is shown in Fig. 2(b), which consists of a key-semantic attention block followed by a feed-forward network (FFN). Specifically, given an input \(F_{in}\), we form each transformer layer as \(z=(f_{}(F_{in}))\), where \(f\) means the transformer layer, \(z\) denotes the output, and \(\) is the trainable parameters. Previous methods tried to reduce the computation cost mainly by applying some techniques (_i.e._, \(\)) like token merging or pruning after each attention calculation or the entire transformer layer, and it can be formalized as \(z=((f_{}(F_{in})))\) or \(z=((f_{}(F_{in})))\). However, the main computation cost from MSA is still untouched.

Owing to the permutation-invariant property (_i.e._, \(f_{}(x)=f_{}(x)\), here \(^{N N}\) means any token level permutation matrix) inherent in both the MSA and the FFN [84; 44], the transformer layer consistently produces identical representations for patches that share the same attributes, regardless of their positions or the surrounding structures . In other words, patches at the same location are consistently connected to other patches possessing the same attributes as they traverse through the various layers within the same stage. It enables \(_{K}\) to serve as a reference permutation for each attention in the subsequent transformer layers, facilitating efficient yet highly semantics-related attention operations. This distinguishes our method from previous token merging/pruning [106; 89] or sparse attention-based methods (Fig.1(d)) that only activate patches in a grid-fixed manner.

The workflow is intuitively illustrated in Fig. 2 (c) and (d). Initially, the patch \(\) is linear projected via \(()\) (The 3rd step in Alg. 1) into \(Q\), \(K\), and \(V\). For each patch \(p_{i}\) in \(Q\), instead of calculating the self-attention with all \(hw\) patches in \(K\)&\(V\), only \(k\) essential patches are selected via the semantic lookup via the indices from \(_{K}\) in them, forming the \(\)&\(\). Then the attention matrix \(_{K}^{att}\) is obtained by: \(_{K}^{att}=_{}(Q^{}/)\), which captures the pair-wise relation between each destination patch \(p_{i}\) in \(Q\) with only \(k\) patches in \(K\)&\(V\) that are semantically highly related to \(p_{i}\). For other unselected patches in \(K\)&\(V\), we aim to maintain their position in their corresponding places without any computation. Based on \(_{K}^{att}\), the attention outputs the updated feature \(}\) via: \(}=_{K}^{att}\). We formulate these two procedures as \(()\) in the 4th step of Alg. 1. This differs from the conventional MSA, which calculates the relation of each patch in \(Q\) and all patches in \(K\)&\(V\). Finally, with FFN, the output of each transformer layer is achieved via the 5th step of Alg. 1.

Conversely, our design offers two advantages. Firstly, the computational cost can be significantly reduced within each attention window (detailed analysis can be found in the _Appx_. A.3), enhancing efficiency. Additionally, sharing the key semantics across transformer layers within each stage acts as a loop that continuously optimizes a degraded patch with its most semantically related patches, ensuring the performance of the proposed method (supported by our experimental results in Sec. 4).

### Discussion

**Fixed top-k _vs._ Random top-k Training Strategies.** In the fixed top-k approach, \(k\) remains constant at 512 during training. In contrast, in the random top-k method, \(k\) is randomly selected from the set \(\). It is important to note that even in the random top-k setting, a fixed k value is maintained for all patches/pixels in each iteration. During inference, the random top-k strategy offers more flexibility and requires training only a single model, making it more user-friendly and less resource-intensive.

**Implementation of the Attention of SemanIR.** To achieve the attention operation of the proposed SemanIR, we explored three different manners for the implementation, _i.e._, (i) _Triton_, (ii) _Torch-Gather_, and (iii) _Torch-Mask_. Specifically, (i) is based on FlashAttention , and a customized GPU kernel is written for the operators proposed in this paper. Parallel GPU kernels are called for the nodes during run time. (ii) means that we use the 'torch.gather()' function in PyTorch to choose the corresponding \(Q_{gather}\) and \(K_{gather}\) based on \(_{K}\), then the attention operation is conducted between \(Q_{gather}\) and \(K_{gather}\). (iii) denotes that we keep only the value of selected patches of \(_{K}\) and omitting other patches with low correlation via assigning those values to \(-\) guided by \(_{K}\). Discussions of the pros and cons regarding these manners are provided in Sec. 4.1.

## 4 Experiments

In this section, we first analyze three important ablation studies of our SemanIR, followed by extensive experiments on **6** IR tasks, _i.e._, deblurring, JPEG CAR, image denoising, IR in AWC, image demosaicking, and image SR. More details about the architecture design, training protocols, the training/testing dataset, and full quantitative/additional qualitative results are shown in _Appx_. A to E. The best and the 2nd-best results are reported in red and blue, respectively. Note that \(\) denotes a single model that is trained to handle multiple degradation levels _i.e._, noise levels, and quality factors.

   \(N\) & Triton & Torch-Gather & Torch-Mask \\ 
512 & 0.27 GB & 0.66 GB & 0.36 GB \\
1024 & 0.33 GB & 1.10 GB & 0.67 GB \\
2048 & 0.68 GB & 2.08 GB & 1.91 GB \\
4096 & 2.61 GB & 4.41 GB & 6.83 GB \\
8192 & 10.21 GB & 10.57 GB & 26.42 GB \\  \(k\) & Triton & Torch-Gather & Torch-Mask \\ 
32 & 5.51 GB & 15.00 GB & 13.68 GB \\
64 & 5.82 GB & 27.56 GB & 13.93 GB \\
128 & 6.45 GB & OOM & 14.43 GB \\
256 & 7.70 GB & OOM & 15.43 GB \\
512 & 10.20 GB & OOM & 17.43 GB \\   

Table 1: GPU memory footprint of different implementations (_i.e._, Triton, Torch-Gather, and Torch-Mask) of our key-graph attention block. \(N\) is the number of tokens and \(k\) is the number of nearest neighbors. OOM denotes ”out of memory”.

    &  &  &  \\  & \(\) & \(\) & \(\) & \(\) & \(\) & \(\) \\  DeblurGAN  & 28.70 & 0.858 & 24.51 & 0.871 & 26.61 & 0.865 \\ Nah _et al._ & 29.08 & 0.914 & 25.73 & 0.874 & 27.41 & 0.894 \\ DeblurGAN-v2  & 29.55 & 0.934 & 26.61 & 0.875 & 28.08 & 0.905 \\ SNR  & 30.26 & 0.934 & 28.36 & 0.915 & 29.31 & 0.925 \\ Gao _et al._ & 30.90 & 0.935 & 29.11 & 0.913 & 0.301 & 0.924 \\ DIGAN  & 31.10 & 0.942 & 28.94 & 0.915 & 30.02 & 0.929 \\ MT-RNN  & 31.15 & 0.945 & 29.15 & 0.918 & 30.15 & 0.932 \\ DMPHN  & 31.20 & 0.940 & 29.09 & 0.924 & 0.315 & 0.932 \\ Suin _et al._ & 31.85 & 0.948 & 29.98 & 0.930 & 30.92 & 0.939 \\ CODE  & 31.94 & - & 29.67 & - & 30.81 & - \\ SPARR  & 32.06 & 0.953 & 0.329 & 0.931 & 31.18 & 0.942 \\ MIMO-UNet+(16) & 32.45 & 0.957 & 29.99 & 0.930 & 31.22 & 0.944 \\ IPT  & 32.52 & - & - & - & - & - \\ MPFNet  & 32.66 & 0.959 & 30.96 & 0.939 & 31.81 & 0.949 \\ KiT  & 32.70 & 0.959 & 30.98 & 0.942 & 31.84 & 0.951 \\ NAFNet  & 32.85 & 0.960 & - & - & - & - \\ Restormetr  & 32.92 & 0.961 & 31.22 & 0.942 & 32.07 & 0.952 \\ Ren _et al._ & 33.20 & 0.963 & 30.96 & 0.938 & 32.08 & 0.951 \\ SemanIR (ours) & 33.44 & 0.964 & 31.05 & 0.941 & 32.25 & 0.953 \\   

Table 2: _Single-image motion deblurring_ results. GoPro  dataset is used for training.

### Ablation Study

**The impact of the implementation of SemanIR Attention** is assessed in terms of (i) _Triton_, (ii) _Torch-Gather_, and (iii) _Torch-Mask_ under different numbers of N (various from 512 to 8192) and K (various from 32 to 512). The results of the GPU memory footprint are shown in Tab. 1, which indicate that _Torch-Gather_ brings no redundant computation while requiring a large memory footprint. Though _Torch-Mask_ brings the GPU memory increase, the increment is affordable compared to _Torch-Gather_ and also easy to implement. _Triton_ largely saves the GPU memory while at the cost of slow inference and difficult implementation for the back-propagation process. To optimize the efficiency of our SemanIR, we recommend employing _Torch-Mask_ during training and _Triton_ during inference, striking a balance between the efficiency and the GPU memory requirement.

**The Impact of the \(k\) in Key-Semantic Dictionary Construction.** Three interesting phenomena are observed from the results shown in Fig. 3 regarding the two top-k training strategies (Sec. 3.3). (1) The PSNR can largely increase with the increase of \(k\) in a fixed manner. (2) When \(k\) reaches a certain number (_i.e._, 384), the performance improvements become marginal, supporting our statement that only the most semantically related patches contribute significantly to the restoration. (3) The randomly sampled strategy has a very stable and better performance compared to the fixed top-k manner especially when the inference \(k\) is fixed to a small number (_i.e._, 64, 128, 256). We conclude that a random sampled strategy is more general and stable. It can also make the inference process more flexible regarding different computation resources. Meanwhile, we set a query region in the input and provided a detailed comparison from the attention-based activation map together with the input query region in Fig. 4. Fig. 4(a) shows the query region input. Fig. 4(b) displays the activation map generated using standard attention mechanisms. Fig. 4(c-f) illustrate activation maps using our key-semantic dictionary with different \(k\) values () during inference. The comparisons indicate that increasing \(k\) allows for connections to more semantically related regions. However, when \(k\) is set too high (_e.g._, \(k\) = 256 as shown in Fig. 4(f)), the activation map may include some semantically unrelated regions. This aligns with the findings and the results depicted in Fig 3, where increasing the \(k\) beyond a certain point (e.g., from 396 to 512) does not further improve PSNR. More ablation results can be found in our _Appx_. D about the effect of the noise level and quality factor for denoising and JPEG CAR.

Figure 4: The impact of \(k\) with different inference \(k\) value.

Figure 3: The impact of \(k\) with different inference \(k\) value. Circle size represents FLOPs.

  
**Task** & **Method** & **Architecture** & **Params [M]\(\)** & **FLOPs [G]\(\)** & **Runtime [ms]\(\)** & **PSNR\(\)** \\   & SwinIR  & Columnar & 11.90 & 215.32 & 152.24 & 27.45 \\  & CAT  & Columnar & 16.60 & 387.86 & 357.97 & 27.89 \\  & HAT  & Columnar & 20.77 & 416.90 & 368.61 & 28.37 \\  & SemanIR-S (Ours) & Columnar & 12.02 & 290.20 & 211.94 & 28.34 \\   Denoising (\(=50\)) \\ (The same architecture \\ for other IR task) \\  } & SwinIR  & Columnar & 11.75 & 752.06 & 1772.84 & 27.98 \\  & Resformer  & U-shape & 26.10 & 154.88 & 210.44 & 28.29 \\   & GRL  & Columnar & 19.81 & 1361.77 & 3944.17 & 28.59 \\   & SemanIR (Ours) & U-shape & 25.85 & 135.26 & 240.05 & 28.63 \\   

Table 3: The efficiency comparisons results on Urban100 dataset.

[MISSING_PAGE_FAIL:8]

window size of 8 to 33.38 dB with a window size of 32. These results suggest that larger window sizes enhance performance by capturing more contextual information.

### Evaluation of SemanIR on Various IR Tasks

**Evaluation on Image deblurring.** Tab. 2 shows the quantitative results for single image motion deblurring on synthetic datasets (GoPro , HIDE ). Compared to the previous state-of-the-art Restormer , our SemanIR achieves significant PSNR improvement (_i.e._, 0.52 dB) on the GoPro dataset and the second-best on the HIDE dataset. The visual results are shown in the _Appx_. E.

**Evaluation on JPEG CAR.** The experiments for color images are conducted with 4 image quality factors ranging from 10 to 40 under two settings (_i.e._, \(\) a single model is trained to handle multiple quality factors, and each model for each quality). The quantitative results shown in Tab. 4 indicate that our SemanIR achieves the best results on all the test sets across various quality factors among all the comparison methods for the color images. The visual comparisons in the _Appx_. E further supports the effectiveness of our method.

**Evaluation on Image Denoising.** We show color and grayscale image denoising results in Tab. 7 under two settings (_i.e._, \(\) one model for all noise levels \(=\{15,25,50\}\) and each model for each noise level). For a fair comparison, both parameters and accuracy are reported for all the methods. For \(\), our SemanIR performs better on all test sets for color and grayscale image denoising than others. It is worth noting that we outperform DRUNet and Restormer with lower trainable parameters. For another setting, the proposed SemanIR also archives better results on CBSD68 and Urban100 for color image denoising, and on Set12 and Urban100 for grayscale denoising. These interesting comparisons validate the effectiveness of the proposed SemanIR and also indicate that our method has a higher generalization ability. The visual results in _Appx_. E also support that the proposed SemanIR can remove heavy noise corruption and preserve high-frequency image details, resulting in sharper edges and more natural textures without over-smoothness or over-sharpness problems.

**Evaluation in AWC.** We validate SemanIR in adverse weather conditions, including rain+fog (Test1), snow (SnowTest100K), and raindrops (RainDrop). PSNR is reported in Tab. 8. Our method achieves the best performance on Test1 (_i.e._, 5.76% improvement) and SnowTest100k-L (_i.e._ 8.01% improvement), while the second-best PSNR on RainDrop compared to all other methods. See _Appx_. E for Visual comparisons.

**Evaluation on Image Demosaicking.** The quantitative results shown in 9 indicate that the proposed SemanIR performs best on both the Kodak and MaMaster test sets, especially 0.05dB and 0.45dB absolute improvement compared to the current state-of-the-art.

    &  &  &  \\   &  &  &  &  &  &  &  \\  & 

Table 7: Color and grayscale image denoising PSNR results.

   Datasets & Kodak & McMaster \\  Matlab & 35.78 & 34.43 \\ MMNetNet  & 40.19 & 37.09 \\ DDRN  & 41.11 & 37.12 \\ Despoint  & 42.00 & 39.14 \\ RLDD  & 42.49 & 39.25 \\ DRUNet  & 42.68 & 39.39 \\ RNAN  & 43.16 & 39.70 \\ GRL  & 43.57 & 40.22 \\ SeminalR (Ours) & 43.62 & **40.68** \\   

Table 9: Image demosaicking PSNR results.

**Evaluation on SR.** For the classical image SR, we compared our SemanIR with both recent lightweight and accurate SR models, and the quantitative results are shown in Tab. 10. Compared to EDT, SemanIR-base achieves significant improvements on Urban100 (_i.e._, 0.72 dB and 0.76dB for 2\(\) and 4\(\) SR) and Manga109 datasets (_i.e._, 0.22dB and 0.17 dB for 2\(\) and 4\(\) SR). Even the SemanIR-small consistently ranks as the runner-up across the majority of test datasets, all while maintaining a reduced number of trainable parameters. Visual results in both Fig. 6 and _Appx._ E also validate the effectiveness of the proposed SemanIR. Specifically, it is clear from the zoomed part in Fig. 6 that SemanIR can restore more details and structural content compared to other methods.

## 5 Conclusion

In this paper, we propose a novel approach, SemanIR, for ViTs-based image restoration, which experimentally validated that global cues are essential to restore degraded images well, but the most semantically related global cures play the major role. Specifically, to capture the key semantics, we propose to construct a semantic dictionary (_i.e._, naively by self-similarity is enough) for storing only the most related \(k\) semantic information and then use it as a reference for guiding the attention operation for making the attention operation pay more attention only to these key semantics. Furthermore, we share the key-semantic dictionary with all the upcoming transformer layers within the same stage since each stage of the transformer is typically at the same semantic level. This strategy significantly reduces the computational cost for IR and functions as loop optimization, continuously restoring degraded patches with their most semantically related patches, which share similar texture or structural information. Extensive experiments on 6 IR tasks validated the effectiveness of SemanIR, demonstrating that our method achieves new state-of-the-art performance.

    &  &  **Params** \\ **[M]** \\  } &  &  &  &  &  &  \\   & & PSNR \(\) & SSIM\(\) & PSNR \(\) & SSIM\(\) & PSNR\(\) & SSIM\(\) & PSNR\(\) & SSIM\(\) & PSNR\(\) & SSIM\(\) \\  RCAN  & 2\(\) & 15.44 & 38.27 & 0.9614 & 34.12 & 0.9216 & 32.41 & 0.9027 & 33.34 & 0.9384 & 39.44 & 0.9786 \\ SAN  & 2\(\) & 15.71 & 38.31 & 0.9620 & 34.07 & 0.9213 & 32.42 & 0.9028 & 33.10 & 0.9370 & 39.32 & 0.9792 \\ HAN  & 2\(\) & 63.61 & 38.27 & 0.9614 & 34.16 & 0.9271 & 32.41 & 0.9027 & 33.35 & 0.9385 & 39.46 & 0.9785 \\ IPT  & 2\(\) & 115.48 & 38.37 & - & 34.43 & - & 32.48 & - & 33.76 & - & - & - \\  Swink  & 2\(\) & 11.75 & 38.42 & 0.9623 & 34.46 & 0.9250 & 23.53 & 0.9041 & 33.81 & 0.9427 & 39.92 & 0.9797 \\ CATA-A  & 2\(\) & 16.46 & 38.51 & 0.9626 & 34.79 & 0.9255 & 23.59 & 0.9047 & 34.26 & 0.940 & 0.10 & 0.9805 \\ ART  & 2\(\) & 16.40 & 38.56 & 0.9629 & 34.59 & 0.9267 & 32.58 & 0.9048 & 34.30 & 0.9452 & 0.40 & 0.9808 \\ EDT  & 2\(\) & 11.48 & 38.63 & 0.9632 & 34.80 & 0.9273 & 32.62 & 0.9052 & 34.27 & 0.9456 & 40.37 & 0.9811 \\ SemanIR-S (Ours) & 2\(\) & 11.87 & 38.57 & 0.9651 & 34.99 & 0.9300 & 23.65 & 0.9078 & 34.65 & 0.9472 & 40.45 & 0.9824 \\ SemanIR-B (Ours) & 2\(\) & 19.90 & 38.61 & 0.9554 & 35.08 & 0.9304 & 36.29 & 0.9084 & 34.99 & 0.9455 & 0.49 & 0.9830 \\  RCAN  & 4\(\) & 15.59 & 32.63 & 0.9002 & 28.87 & 0.7889 & 27.77 & 0.7436 & 26.82 & 0.8087 & 31.22 & 0.9173 \\ SAN  & 4\(\) & 15.86 & 32.64 & 0.9003 & 28.92 & 0.7888 & 27.78 & 0.7436 & 26.79 & 0.8068 & 31.18 & 0.9169 \\ HAN  & 4\(\) & 64.20 & 32.64 & 0.9002 & 28.90 & 0.7890 & 27.80 & 0.7442 & 26.85 & 0.8094 & 31.42 & 0.9177 \\ IPT  & 4\(\) & 115.63 & 32.64 & - & 29.01 & - & 27.82 & - & 27.26 & - & - \\  Swink  & 4\(\) & 11.90 & 32.92 & 0.9044 & 29.09 & 0.7950 & 27.92 & 0.7489 & 27.45 & 0.8254 & 23.03 & 0.9260 \\ CATA-A  & 4\(\) & 16.60 & 33.08 & 0.9052 & 29.18 & 0.7960 & 27.99 & 0.7510 & 27.89 & 0.8339 & 32.39 & 0.9285 \\ ART  & 4\(\) & 16.55 & 33.04 & 0.9051 & 29.16 & 0.7988 & 27.97 & 0.751 & 27.77 & 0.8321 & 32.31 & 0.9283 \\ EDT  & 4\(\) & 116.63 & 33.06 & 0.9055 & 29.23 & 0.7971 & 27.99 & 0.7510 & 27.75 & 0.8317 & 23.39 & 0.9283 \\ SemanticR-S (Ours) & 4\(\) & 12.02 & 33.02 & 0.9082 & 29.29 & 0.8026 & 27.96 & 0.7582 & 28.34 & 0.8467 & 32.48 & 0.9322 \\ SemanIR-B (Ours) & 4\(\) & 20.04 & 33.08 & 0.9090 & 29.34 & 0.8037 & 27.98 & 0.7599 & 28.51 & 0.8467 & 32.56 & 0.9335 \\   

Table 10: _Classical image SR_ results. Both lightweight and accurate models are summarized.

Figure 6: Visual comparison of classical image SR (4\(\)) on Urban100. Best viewed by zooming.