# Hierarchically Gated Recurrent Neural Network for Sequence Modeling

Zhen Qin\({}^{}\), \({}^{2}\)Songlin Yang\({}^{}\), \({}^{1}\)Yiran Zhong\({}^{}\)

\({}^{1}\)OpenNLPLab, Shanghai Artificial Intelligence Laboratory, \({}^{2}\)MIT CSAIL

https://github.com/OpenNLPLab/HGRN

Equal contribution. \({}^{}\) Indicates corresponding author (Email address: _zhongyiran@gmail.com_).

###### Abstract

Transformers have surpassed RNNs in popularity due to their superior abilities in parallel training and long-term dependency modeling. Recently, there has been a renewed interest in using linear RNNs for efficient sequence modeling. These linear RNNs often employ gating mechanisms in the output of the linear recurrence layer while ignoring the significance of using forget gates within the recurrence. In this paper, we propose a gated linear RNN model dubbed Hierarchically Gated Recurrent Neural Network (HGRN), which includes forget gates that are lower bounded by a learnable value. The lower bound increases monotonically when moving up layers. This allows the upper layers to model long-term dependencies and the lower layers to model more local, short-term dependencies. Experiments on language modeling, image classification, and long-range arena benchmarks showcase the efficiency and effectiveness of our proposed model. The source code is available at https://github.com/OpenNLPLab/HGRN.

## 1 Introduction

Sequence modeling is a fundamental problem in various domains such as natural language processing [12; 43; 44; 61; 64], time series analysis , computer vision [3; 13; 45; 74], and audio processing [1; 18; 73]. Prior to the invention of Transformers , RNN and its variants were the primary selections of architectures for sequence modeling, and have been widely used in machine translation , stock price prediction , weather forecasting , speech recognition , and _etc._

RNNs have two main drawbacks: slow sequential training and limited capability in modeling long-term dependencies. With the swift development of deep learning and the pervasive use of GPUs, these drawbacks prevent it from flourishing in modern long-sequence modeling tasks. Meanwhile, Transformers  have rapidly gained popularity and now dominate various research areas in sequence modeling due to their better abilities in parallel training and long-term dependency modeling. However, Transformer's quadratic time complexity makes long sequence modeling expensive. On the other hand, RNN offers linear complexity and serves as an ideal choice for long sequence modeling. This works aim to addressing these RNN drawbacks, revitalizing their applicability in long-sequence modeling tasks.

To address the training inefficiency problem, we turn to more efficient RNN variants that employ element-wise linear recurrence (ELR) relations . ELR provides two main advantages: (i) By removing nonlinearities in the recurrence, it enables parallelized training. (ii) By assuming independence between distinct hidden states, it enables efficient hidden state updates (through element-wise product instead of matrix multiplication) [20; 40]. Notably, ELR has been used in many modern linear RNN models, including the diagonalized versions of structured state-space models  (S4) [20; 26; 71] and RWCV . In recent advancements, numerous studies have incorporatedgating mechanisms into the outputs of linear recurrence layers [11; 46; 49; 55; 82], similar to the output gates in LSTMs and leading to considerable performance gains. However, most current studies overlook the significance of the forget gate, which is often regarded as the most important gate in LSTMs [19; 80]. In this work, we underscore the importance of employing forget gates in linear RNNs and adopt gated linear RNNs for both efficiency and high performance.

To effectively capture long-term dependencies in gated RNNs, it is crucial to maintain high forget gate values close to one . However, gates in saturated regimes (i.e., close to zero or one) suffer from the gradient vanishing issue . Moreover, if all forget gate values are close to one, RNNs will not be able to effectively forget irrelevant information, compromising their ability to model short-term dependencies. To address these challenges, we introduce Hierarchically Gated Recurrent Units(**HGRU**). In **HGRU**, we add an additive learnable value, referred to as the lower bound, to the original forget gate value, effectively mitigating the issue of saturated gates  by pushing gate activations away from the saturated regimes. Furthermore, we design the lower bounds to increase monotonically as we move up the layers of the RNN. This ensures that the forget gate values in the lower layers remain relatively small, enabling the necessary forgetting of past information for modeling short-term dependencies. In contrast, in the uppermost layer, the forget gate values approach one, facilitating the effective modeling of long-term dependencies. Our proposed model has proven to be highly efficient and effective, as demonstrated by its outstanding performance in language modeling, image classification, and long-range arena benchmarks.

## 2 Related work

Efficient token mixing for sequence modeling. abstracts self-attention (SA) as token mixing, thereby transforming the Transformer architecture into MetaFormer. MetaFormer comprises essential components such as token mixer, channel mixer, residual connections, and LayerNorm. This abstraction highlights that the success of Transformers does not solely rely on SA but rather on the holistic integration of these components. Notably, token mixers can be replaced with simpler alternatives like pooling layers without compromising the model's performance in the context of vision transformer. For sequence modeling tasks,  provides a comprehensive analysis and discussion of different token mixing strategies. Two prominent contenders, long convolution and linear recurrence, show promise as replacements for SA modules in long sequence modeling due to their superior asymptotic time complexity and competitive performances. In long convolution models [14; 41; 57; 59], the kernel size matches the input sequence length, enabling a broader context compared to traditional convolutions. Training is accomplished using the efficient \((n n)\) fast Fourier transforms (FFT) algorithm. However, long convolutions face challenges such as the need for causal convolution inference, which requires caching all historical computations similar to the key-value (KV) cache in SA. This can lead to memory limitations when processing long sequences. Moreover, the inference complexity of long convolutions remains higher than that of RNNs. These factors make linear RNNs a more suitable alternative to replace SA in long-sequence modeling. TransNormerLLM  scales efficient token mixing in large language models to achieve competitive performance and superior training and inference efficiency compared to transformer-based models.

Element-wise linear recurrence.The slower training speeds of traditional RNNs can be attributed to two main factors: (i) The updating of the hidden state involves full matrix multiplication. (ii) The presence of nonlinearity within the recurrence prevents parallel computation. To tackle the first issue,  introduced a simplified interaction between hidden states. This allowed the hidden state update to be performed using an element-wise product instead of full matrix multiplication. They demonstrated that this approach is notably fast when the (nonlinear) recurrence for each dimension is fused within a single CUDA kernel. Likewise, for the linear case, diagonalized versions of S4 [20; 26] have also exhibited speed improvements over S4 by leveraging element-wise recurrence. Regarding the second challenge, the ability to capture nonlinear dependencies on past data can be achieved by stacking multiple linear recurrence layers interleaved with nonlinear MLP blocks. This indicates the potential to eliminate nonlinearity, as suggested by [4; 25; 48]. Empirical support for this strategy's effectiveness came later, as demonstrated by [11; 20; 24; 53; 55; 71].  further highlighted that such an architecture still possesses Universal Approximator properties, thus justifying the employment of linear recurrence. By excluding nonlinearity, [48; 71] showed that the parallel scan algorithm can be used for parallel training.

Linear recurrence can be broadly categorized into exponential moving averages (EMA) and gating schemes, as noted by . The key difference is whether the decay rate is data-dependent. Models such as S4 , S4D , MEGA , RWKV , and LRU  utilize the EMA approach, where the decay rate is static for all time steps (i.e., data-independent), while our model uses a data-dependent dynamic decay rate through the use of the forget gate. We remark on the importance of incorporating a data-dependent decay rate, which is largely ignored by current works in linear RNNs. Although liquid S4  uses a dynamic transition matrix (which amounts to a data-dependent decay rate), it employs a limited form for FFT-based training. Our model does not have the convolutional view and thus cannot use FFT for training but allows the use of parallel scan.

The field of linear Transformers and linear RNNs exhibits a close relationship.  shows that linear Transformers can be reformulated as RNNs during auto-regressive decoding, revealing similarities to the update rules observed in fast weight additive outer products [66; 67]. These updates can be seen as a special case of element-wise linear recurrence, where forget gate values are consistently set to one across time and hidden states are two-dimensional. However, this formulation in linear Transformers lacks the ability to forget irrelevant information, resulting in the attention dilution issue . To address this limitation,  introduced the delta rule to forget values associated with the current write key by removing the corresponding value before adding the new value. Alternatively, [47; 56] proposed gating mechanisms similar to those in gated RNNs to facilitate the forgetting of irrelevant information.

Long-term dependencies in RNNs.RNNs fall short in long-term dependency modeling, which is commonly attributed to the gradient vanishing issue. Three methods are typically applied to mitigate this issue. (i) Gating mechanisms [9; 17; 23; 30; 70], which are believed to be crucial to the success of LSTMs, use additive (instead of multiplicative) hidden state update rules to improve gradient flow. (ii) Regularizing or initializing the eigenvalues of the recurrent weight matrix (close) to one via identity matrices  or unitary matrices . In the diagonal linear RNN case, the eigenvalues coincide with the element-wise decay rates, and LRU  uses randomized linear algebra techniques to initialize eigenvalues to be close to one.  also interestingly points out that many modern state-space models use a very small time step value on initialization for discretization, resulting in eigenvalues or decay rates close to one. (iii) Adding skip connections between distant time steps to allow shortcuts for gradient flow [5; 8; 37].Our approach combines (i) and (ii), which improves gating mechanisms with a regularized dynamic decay rate that approaches one in the upper layer.

## 3 Method

### Architecture overview

Our proposed Hierarchically Gated Recurrent Network (**HGRN**) is depicted in Figure 1. It has multiple stacked layers, each of which consists of a token mixing module **HGRU** and a channel mixing module **GLU** (Gated Linear Unit ).

### HGRU exploration

We begin with a simple gated linear recurrent layer, which is defined as:

\[_{t}&=( _{t}_{f}+_{f})^{1 d}, \\ _{t}&=(_{t }_{i}+_{i})^{1 d},\\ _{t}&=(_{t} _{t}+_{z})^{1 d},\\ _{t}&=_{t}_{t-1} +_{t}_{t}^{1 d},\\ _{0}&=^{1 d}, \] (1)

where \(\) denotes the element-wise product. Following the terminology used in the RNN literature, we refer to \(_{t}\) and \(_{t}\) as the forget and input gates, respectively. It is worth noting that \(_{t}\) and \(_{t}\) depend only on \(_{t}\) and not on \(_{t-1}\). This characteristic enables the use of the parallel scan algorithm [48; 71], otherwise it is infeasible. We then make the following changes toward our final **HGRU** step by step.

Complex-valued recurrence.For linear RNNs with static decay rates, it is common to perform eigendecompositions on the recurrent weight matrix to achieve element-wise linear recurrence. However, if only real-valued eigenvalues are allowed, it restricts the range of the recurrent weight matrix to be symmetric, limiting the model's expressiveness. To overcome this limitation, linear RNNs often employ complex-valued eigenvalues to enhance the model's expressive power [20; 26; 27; 32; 53]. Motivated by this, we extend our model to consider \(_{t},_{t},_{t}^{1 d}\) as complex values. For the input \(c_{t}\), we parameterize its real and imaginary parts separately as follows:

\[(_{t}) =(_{t}_{cr}+_{cr})^{1 d},\] \[(_{t}) =(_{t}_{ci}+_{ci})^{1 d}.\]

Regarding the forget gate values, we find it convenient to use the exponential representation of complex numbers and parameterize \(_{t}\) as follows: \(_{t}=_{t}(i_{t})\). Here, \(i^{2}=-1\), \(_{t},_{t}^{d}\) and \((i_{t})=_{t}+_{t}i\). The magnitude argument \(_{t}\) determines the intensity of remembering historical information, while the phase argument \(_{t}\) determines the oscillation frequencies. We find that parameterizing \(_{t}\) in a data-independent manner is preferable, as it allows for a clear interpretation of encoding relative position information (see next subsection for more discussions), which is reminiscent of Rotary Positional Embedding (RoPE) . We shared \(\) across times steps, i.e., \(_{t}=_{t}(i)\), initialize \(\) as RoPE does, but make it learnable like LRPE .

Lower bound on forget gate values.Since the intensity of remembering information is only related to the magnitude argument \(_{t}\), we focus on how to add a lower bound to \(_{t}\). As mentioned earlier, we want to set a monotonically increasing lower bound on the forget gate (magnitude) values. Inspired by ON-LSTM , we employ the \(\) activation function to achieve this. Concretely, we allocate \(^{H d}\) to parameterize lower bounds independently for all hidden states, where \(H\) is the number of layer. Assuming the layer index is \(k\), we have the following calculations:

\[ =((,=0)^{H  d},\] \[^{k} =[(,=0)]_{k}^{1  d}.\]

Here we define \([()]_{k}=(_{i=1}^{k}x_{i})-x_{1}\) to prevent the highest layer's lower bound from being one as we still want the ability to forget irrelevant information.

We remark that there is a difference in the use of \(\) between our model and ON-LSTM. In ON-LSTM, \(\) is applied to the hidden state dimension within a single layer, while in our case, we apply \(\) on the layer dimension across different layers to enable upper layers to model long-range dependencies.

Finally, \(_{t}\) in the \(k\)-th layer is parameterized as follows:

\[_{t} =(_{t}_{}+_{ })^{1 d},\] \[_{t} =^{k}+(1-^{k})_{t}^{1 d}.\]

Comparing to before (i.e., without lower bounds), to achieve the same forget rate value \(\) closed to one, \(_{t}\) will be pushed away from the Sigmoid activation function's saturated regions (i.e., near one),

\[_{t}=-^{k}}{1-^{k}}<,\]

thereby mitigating the gradient vanishing issue  and making gradient-based optimization easier.

Tying input and forget gates.To reduce the number of parameters, it is common to use leaky units, i.e., tying the input gate with the forget gate using \(_{t}=1-_{t}\), which has a close relationship to the discretization of continuous-time system  and exponential moving average , and has been proven effective empirically . To allows for a clear interpretation of encoding relative position information, we only apply this strategy on the magnitude argument:

\[_{t}=_{t}(i)_{t-1}+(1- _{t})_{t}^{1 d}.\] (2)

Output gates and projection.The addition of gates to the output of the recurrence layer has been shown to be effective in state-space models . Motivated by this, we incorporate an output gate before performing the output projection as follows and get **HGRU**:

\[_{t} =(W_{g}_{t}+b_{g})^{1  2d},\] \[^{}_{t} =(_{t}[(_{ t}),(_{t})])^{1 2d},\] (3) \[_{t} =^{}_{t}_{o}+_{o}^{1 d}.\]

### Token mixing perspective of HGRU

We provide the token mixing perspective of **HGRU** similar to . Expanding Equation 2, we have:

\[_{t}=_{s=1}^{t}(1-_{s})[_{k=s+1}^{t}_{ k}(i)]_{s}=_{s=1}^{t}(1-_{s})[_{k=s+1 }^{t}_{k}](i(t-s))_{s}\] (4)

Written in matrix form, we have:

\[=_{1}\\ \\ \\ _{n},=1-_{1}&0&& 0\\ (1-_{1})_{2}(i)&1-_{2}&&\\ &&&0\\ (1-_{1})[_{s=2}^{n}_{k}](i(n-1))& &&1-_{n},=_{1}\\ \\ \\ \\ _{n}\] (5)

So the token mixing module can be formed as follows:

\[=.\] (6)

Note that the token mixing matrix \(\) can be decomposed into two parts \(=\):

\[=1-_{1}&0&&0\\ (1-_{1})_{2}&1-_{2}&&\\ &&&0\\ (1-_{1})[_{s=2}^{n}_{k}]&&&1-_ {n},=1&0&&0\\ (i)&1&&\\ &&&0\\ (i(n-1))&&&1\] (7)

This decomposition means that the Token mixing matrix \(\) can be decoupled into two independent modules, where \(\) models the long-distance dependency and \(\), a Toeplitz matrix, models the relative positional relationship and enhanced expressiveness. Note that if \(\) depends on the input, then the matrix \(\) will no longer be a Toeplitz matrix, thus unable to capture relative position information. It can be also viewed as a RoPE-enhanced attention mechanism: \(\) corresponds to the attention matrix but the attention score here is the cumulative product of data-dependent decay rates; \(\) directly corresponds to RoPE.

## 4 Experiments

We conduct a comparative analysis between our proposed **HGRN** and four widely adopted sequence modeling structures, _i.e.,_ attention-based, MLP-based, FFT-based, and state-space-based. We evaluate **HGRN** on the WikiText-103 dataset  and the Pile  dataset for autoregressive language modeling, as well as the length extrapolation ability. To assess the accuracy and efficiency of our model in handling long-term dependencies, we utilize the LRA benchmark . Additionally, we showcase the robustness of **HGRN** in computer vision task on the ImageNet-1k dataset.

### Setting

We implement our models in Pytorch  and train them on 8 Nvidia A100 GPUs. For **HGRN**, we found that fusing element-wise recurrence into a single CUDA kernel results in fast running speed in practice.  also found that unless the sequence length is sufficiently large, the parallel scan's implementation is not necessarily faster than the sequential scan. As such, we use a CUDA-based sequential scan for implementation; however, our model has the potential to model very long sequences through the use of a parallel scan.

We adopt the same training configuration for all competitors, including batch size, learning rate, training epochs or iterations, _etc._ We list detailed hyper-parameters in the Appendix. For the autoregressive language modeling, we conducted three sets of experiments. Firstly, we validated the performance of two different-scale models on the Wikitext-103 dataset. We used the TNN configuration to verify the performance of the model at around 44m, and the Hyena configuration to verify the performance of the model at around 125m. To evaluate the performance of larger-scale models, we trained a 1b Transformer and **HGRN** on the Pile dataset using 10b tokens. To assess the performance in downstream tasks, we trained **HGRN** models of 150m, 350m, and 1b on the Pile dataset using 100b tokens and conducted zero-shot evaluations on downstream tasks.

For the LRA benchmark, We report results on all 6 tasks. For the image classification on the ImageNet-1k dataset, We integrate **HGRN** into the DeiT  structure, we replace the transformer layers with our **HGRN** modules. It is compared to the performance of the vanilla DeiT on the ImageNet-1K dataset for image classification.

### Results

  Model & PPL & PPL & Params \\  & (val)\(\) & (test)\(\) & (M) \\  _Attn-based_ & & & \\  Transformer  & 24.40 & 24.78 & 44.65 \\ FLASH  & 25.92 & 26.70 & 42.17 \\
1+elu  & 27.44 & 28.05 & 44.65 \\ Performer  & 62.50 & 63.16 & 44.65 \\ cosformer  & 26.53 & 27.06 & 44.65 \\  _MLP-based_ & & & \\  Syn(D)  & 31.31 & 32.43 & 46.75 \\ Syn(R)  & 33.68 & 34.78 & 44.65 \\ gMLP & 28.08 & 29.13 & 47.83 \\  _RNN-based_ & & & \\  S4  & 38.34 & 39.66 & 45.69 \\ DSS  & 39.39 & 41.07 & 45.73 \\ GSS  & 29.61 & 30.74 & 43.84 \\ RWKV  & 24.31 & 25.07 & 46.23 \\ LRU  & 29.86 & 31.12 & 46.24 \\  _FFT-based_ & & & \\  TNN  & 23.98 & 24.67 & 48.68 \\  _Ours_ & & & \\ 
**HGRN** & 24.14 & 24.82 & 46.25 \\  

Table 1: **Results on Wikitext-103 (TNN’s setting). \(\) means _lower is better_.**Autoregressive Language ModelingAutoregressive language modeling stands as a prominent task within the field of natural language processing, as it serves as a measure of a language model's causal inference capability. This task requires the model to estimate the probability distribution of the subsequent token based on the previously seen tokens.

We show the performances of the autoregressive language modeling in table 1 and table 2. Compared to transformer-based methods, **HGRN** performs favourably than most efficient variants of the vanilla transformer such as FLASH , 1+elu , Performer  and cosFormer .

Also, **HGRN** achieves better results than the MLP-based methods with a notable margin. Nevertheless, **HGRN** performs similarly to the original transformer . Finally, **HGRN** shares similar concepts with RNN-based such as S4 , DSS , GSS , RWKV , and LRU , our **HGRN** also achieves superior performance to all RNN-based methods. This provide evidence HRGN may be an effective method in LM We also report the extrapolation ability of **HGRN** compared to previous methods in Table 14.

We also trained a 1b model on the Pile dataset and compared it with LRU and Transformer. Specifically, our training parameters included a sequence length of 1024, batch size of 96, 100k updates, and a learning rate of 5e-4. It can be seen that **HGRN** still performs better at the 1b scale. Additionally, we trained 100b tokens of **HGRN** on the Pile dataset at 150m, 350m, and 1b sizes, and evaluated them against open-source Transformer-based models in downstream tasks. We selected Comparison on Commonsense Reasoning and Super GLUE tasks, and all evaluations were done using the lm-evaluation-harness . **HGRN** achieves comparable performance to Transformer-based models when consuming only 1/3 of the tokens.

Long Range ArenaLRA  is proposed as a comprehensive evaluation for assessing the performances of models in processing long-term dependencies in various sequential modeling tasks. We show a performance comparison between **HGRN** and existing methods in Table 6. **HGRN** achieves comparable results with other SOTA methods.

  Model & Params & Token & BOOLQ & PIQA & HS & WG & ARC-e & ARC-c & OBQA & AVG \\  GPT-Neo & 0.13 & 300 & 61.71 & 63.06 & 30.40 & 50.43 & 43.73 & 23.12 & 26.20 & 42.66 \\ OPT & 0.16 & 300 & 55.47 & 62.95 & 31.35 & 50.43 & 43.52 & 22.70 & 28.00 & 42.06 \\ Pythia & 0.16 & 300 & 55.08 & 61.32 & 30.16 & 51.93 & 43.18 & 23.12 & 26.80 & 41.66 \\ RWKV & 0.17 & - & - & 65.07 & 32.26 & 50.83 & 47.47 & 24.15 & 29.60 & 41.56 \\ HGRN & 0.15 & 100 & 59.91 & 65.02 & 33.33 & 50.20 & 46.68 & 23.81 & 28.60 & 43.94 \\  OPT & 0.35 & 300 & 57.74 & 64.58 & 36.69 & 52.49 & 44.02 & 23.89 & 28.20 & 43.94 \\ Pythia & 0.4 & 300 & 60.40 & 67.08 & 40.52 & 53.59 & 51.81 & 24.15 & 29.40 & 46.71 \\ BLOOM & 0.56 & 350 & 55.14 & 64.09 & 36.97 & 52.80 & 47.35 & 23.98 & 28.20 & 44.08 \\ RWKV & 0.43 & - & - & 67.52 & 40.90 & 51.14 & 52.86 & 25.17 & 32.40 & 45.00 \\ HGRN & 0.35 & 100 & 59.05 & 66.70 & 38.12 & 51.70 & 49.20 & 25.26 & 30.60 & 45.80 \\  GPT-Neo & 1.3 & 300 & 61.99 & 71.11 & 48.93 & 54.93 & 56.19 & 25.85 & 33.60 & 50.37 \\ OPT & 1.3 & 300 & 57.77 & 71.71 & 53.70 & 59.35 & 57.24 & 29.69 & 33.20 & 51.81 \\ Pythia & 1.4 & 300 & 60.73 & 70.67 & 47.18 & 53.51 & 56.99 & 26.88 & 31.40 & 49.62 \\ BLOOM & 1.1 & 350 & 59.08 & 67.14 & 42.98 & 54.93 & 51.47 & 25.68 & 29.40 & 47.24 \\ RWKV & 1.5 & - & - & 72.36 & 52.48 & 54.62 & 60.48 & 29.44 & 34.00 & 50.56 \\ HGRN & 1 & 100 & 58.69 & 70.89 & 48.02 & 51.62 & 55.64 & 27.90 & 31.60 & 49.19 \\  

Table 4: **Performance Comparison on Commonsense Reasoning.** PS: parameter size (billion). T: tokens (billion). HS: HellaSwag. WG: WinoGrande.

  Model & PPL\(\) \\  Transformer  & 18.6 \\ Hybrid H3  & 18.5 \\ Performer  & 26.8 \\ Reformer  & 25.6 \\ AFT-conv  & 28.2 \\ Linear Attention  & 25.6 \\ Hyena  & 18.6 \\ Hyena-slim  & 18.5 \\  HGRN & 18.6 \\  

Table 2: **Results on Wikitext-103** (Hyena’s setting). All models are in GPT-2 small size (125M). \(\) means _lower is better_

  Model & PPL\(\) \\  Transformer & 4.56 \\ LRU & 5.07 \\ 
**HGRN** & 4.14 \\  

Table 3: **Results on the Pile.** All the model size is 1b. The lower the better.

Image ClassificationThe image classification results on the ImageNet-1K dataset are presented in Table 7. Notably, with comparable parameter sizes, our proposed **HGRN** model demonstrates superior performance compared to previous methods such as TNN and the vanilla transformer. It demonstrates the capability of **HGRN** in modeling visual modalities.

### Ablation Study

We conducted ablation studies in the smallest-scaled setting (i.e., TNN's setting on WikText103 dataset) to thoroughly verify the effectiveness of each of our proposed components in **HGRN**. Experiments were conducted on the Pile dataset using a 1b model with 10b tokens for the forget gate experiment.

  Model & ListOps & Text & Retrieval & Image & Pathfinder & Path-X & AVG. \\  Transformer  & 38.37 & 61.95 & 80.69 & 40.57 & 65.26 & - & 47.81 \\ cosFormer  & 36.50 & 67.70 & 83.15 & 51.23 & 71.96 & - & 51.76 \\ FLASH  & 38.70 & 64.10 & 86.10 & 47.40 & 70.25 & - & 51.09 \\ S4  & 59.60 & 86.82 & 90.90 & 88.65 & 94.20 & 96.35 & 86.09 \\ DSS\_softmax  & 60.60 & 84.80 & 87.80 & 85.70 & 84.60 & 87.80 & 81.88 \\ DSSEXP  & 59.70 & 84.60 & 87.60 & 84.90 & 84.70 & 85.60 & 81.18 \\ DSSEXP-NO-SCALE  & 59.30 & 82.40 & 86.00 & 81.20 & 81.30 & - & 65.03 \\ TNN  & 61.04 & 87.90 & 90.97 & 88.24 & 93.00 & 96.10 & 86.21 \\ S5  & 62.15 & 89.31 & 91.4 & 88 & 95.33 & 98.56 & 87.46 \\ Mega  & 63.14 & 90.43 & 91.25 & 90.44 & 96.01 & 97.98 & 88.21 \\ SGConv  & 61.45 & 89.2 & 91.11 & 87.97 & 95.46 & 97.83 & 87.17 \\ LRU  & 60.20 & 89.40 & 89.90 & 89.00 & 95.10 & 94.20 & 86.30 \\ 
**HGRN** & 59.95 & 88.14 & 94.23 & 88.69 & 92.92 & 97.50 & 86.91 \\  

Table 6: **Performances Comparison on the Long Range Arena benchmark. The proposed **HGRN** achieves the best performances and outperforms all competing methods.

  Model & PPI-\(\) \\  LRU w forget gate & 4.92 \\ LRU & 5.07 \\
**HGRN** only lower bound & 4.84 \\
**HGRN** w/o forget gate & 57.42 \\
**HGRN** & 4.14 \\  

Table 7: **Performances comparison of image classification on ImageNet-1k. HGRN** performs favorably than competing methods with similar parameter sizes.

  Model & PPI-\(\) \\  LRU w forget gate & 4.92 \\ LRU & 5.07 \\
**HGRN** only lower bound & 4.84 \\
**HGRN** w/o forget gate & 57.42 \\
**HGRN** & 4.14 \\  

Table 8: **Forget gate ablation** on an autoregressive language model. The only lower bound means using a data-independent gate like LRU.

  Model & Params & Token & WSC & WIC & RTE & CB & MULTIRC & BOOLQ & COPA & AVG \\  GPT-Neo & 0.13 & 300 & 36.54 & 50.00 & 54.87 & 41.07 & 0.84 & 61.71 & 64.00 & 44.15 \\ OPT & 0.16 & 300 & 36.54 & 50.00 & 49.82 & 21.43 & 1.36 & 55.47 & 66.00 & 40.09 \\ Pythia & 0.16 & 300 & 36.54 & 50.16 & 52.71 & 41.07 & 2.52 & 55.08 & 65.00 & 43.30 \\ HGRN & 0.15 & 100 & 38.46 & 51.10 & 56.68 & 42.86 & 1.47 & 59.91 & 65.00 & 45.07 \\  OPT & 0.35 & 300 & 36.54 & 50.00 & 51.99 & 46.43 & 1.36 & 57.74 & 72.00 & 45.15 \\ Pythia & 0.4 & 300 & 57.69 & 50.31 & 52.71 & 35.71 & 1.68 & 60.40 & 70.00 & 46.93 \\ BLOOM & 0.56 & 350 & 40.38 & 50.00 & 52.71 & 41.07 & 1.05 & 55.14 & 61.00 & 43.05 \\ HGRN & 0.35 & 100 & 38.46 & 50.16 & 52.71 & 51.79 & 1.99 & 59.05 & 73.00 & 46.74 \\  GPT-Neo & 1.3 & 300 & 36.54 & 50.00 & 60.29 & 44.64 & 1.99 & 61.99 & 69.00 & 46.35 \\ OPT & 1.3 & 300 & 37.50 & 51.10 & 51.99 & 41.07 & 3.15 & 57.77 & 79.00 & 45.94 \\ Pythia & 1.4 & 300 & 36.54 & 50.00 & 53.07 & 35.71 & 0.94 & 60.73 & 72.00 & 44.14 \\ BLOOM & 1.1 & 350 & 36.54 & 50.00 & 52.71 & 41.07 & 0.73 & 59.08 & 68.00 & 44.02 \\ HGRN & 1 & 100 & 40.38 & 50.78 & 53.43 & 42.86 & 3.04 & 58.69 & 70.00 & 45.60 \\  

Table 5: **Performance Comparison on Super GLUE.. PS: parameter size (billion). T: tokens (billion).**

[MISSING_PAGE_FAIL:9]

complex values in element-wise linear recurrence. Additionally, the experiments show that the phase argument \(\) should not be data-dependent.

### Analysis on forget gate values

We present the distributions of forget gate values across layers for different methods in Table 12 and visualize the histogram of each layer in Figure 2, trained on the autoregressive language modeling task. The results demonstrate that the addition of lower bounds effectively increases the average forget gate values in higher layers (5-6). Notably, the medium forget gate values in the highest layer reach 0.98, enabling the modeling of long-term dependencies.

It is interesting to note that the average forget gate values of the LRU model consistently exceed those of our variant model without lower bounds, as per their eigenvalues. However, despite this, the language modeling performance of LRU is lower than that of our variant. Specifically, LRU scored 24.71, while our variant scored 31.12. This suggests that using data-dependent gates to selectively retain relevant information is advantageous, rather than relying on data-independent forget gate values across all time steps.

## 5 Conclusion

In this work, we have shown that gated linear RNNs could obtain impressive performance across different tasks and modalities without compromising efficiency. We highlighted the significance of the forget gate for linear RNNs in language modeling and emphasized the importance of an additive lower bound on forget gate values for modeling long-term dependencies.

## Limitations and broader impact

Our empirical evaluation of **HGRN** remains on a smaller scale compared to other large-scale models. Potentially negative social consequences include the misuse of brain models for unsuitable purposes or applications, which must be prohibited by appropriate rules. In the era of large language models, the inference cost is the key limitation of transformer-based models. RNNs provide a solution with their lower inference costs. This could potentially lead to a significant evolution in the field.

  Model & ListOps & Text & Retrieval & Image & Pathfinder & Path-X & AVG \\  w/o lower bound & 51.41 & 87.79 & 88.71 & 80.17 & - & - & 51.53 \\
**HGRN** & 59.95 & 88.14 & 94.23 & 88.69 & 92.92 & 97.50 & 86.91 \\  

Table 13: **Lower bound ablation on LRA.** We verify the importance of lower bounds in long-sequence modeling capabilities.

  Model & PPL\(\) \\  w/o complex & 25.34 \\ data dependent \(\) & 28.74 \\
**HGRN** & 24.14 \\  

Table 11: **Ablations of complex-valued recurrence on autoregressive language modeling.** w/o complex means remove theta, data-dependent theta means theta is dependent on the input, this makes the matrix \(\) not a Toeplitz matrix, which can not capture relative information.

   & ours & ours & w/o lower bound & w/o lower bound & LRU & LRU \\  Layer & mean & median & mean & median & mean & median \\ 
1 & 0.48 & 0.47 & 0.52 & 0.50 & 0.75 & 0.72 \\
2 & 0.55 & 0.52 & 0.59 & 0.55 & 0.78 & 0.75 \\
3 & 0.60 & 0.57 & 0.58 & 0.56 & 0.78 & 0.76 \\
4 & 0.68 & 0.64 & 0.58 & 0.55 & 0.79 & 0.78 \\
5 & 0.79 & 0.80 & 0.63 & 0.63 & 0.79 & 0.77 \\
6 & 0.91 & 0.98 & 0.63 & 0.67 & 0.79 & 0.79 \\  

Table 12: **Forget gate values of different methods on language modeling tasks.** In each layer, we counted the mean and median of forget gate values.