# Appendix

TFLEX: Temporal Feature-Logic Embedding Framework for Complex Reasoning over Temporal Knowledge Graph

Xueyuan Lin\({}^{1}\) Haihong E\({}^{1}\)1 Chengjin Xu\({}^{2}\)1 Gengxian Zhou\({}^{1}\)

Haoran Luo\({}^{1}\) Tianyi Hu\({}^{1}\) Fenglong Su\({}^{3}\) Ningyuan Li\({}^{1}\) Mingzhi Sun\({}^{1}\)

\({}^{1}\) Beijing University of Posts and Telecommunications

\({}^{2}\) University of Bonn

\({}^{3}\) National University of Defense Technology

linxy59@mail2.sysu.edu.cn, ehaihong@bupt.edu.cn, xuc@iai.uni-bonn.de

z.gengxian@se18.qmul.ac.uk, {luohaoran, hutianyi}@bupt.edu.cn

sufenglong18@nudt.edu.cn, {jason.ningyuan.li, sunmingzhi}@bupt.edu.cn

###### Abstract

Multi-hop logical reasoning over knowledge graph plays a fundamental role in many artificial intelligence tasks. Recent complex query embedding methods for reasoning focus on static KGs, while temporal knowledge graphs have not been fully explored. Reasoning over TKGs has two challenges: 1. The query should answer entities or timestamps; 2. The operators should consider both set logic on entity set and temporal logic on timestamp set. To bridge this gap, we introduce the multi-hop logical reasoning problem on TKGs and then propose the first temporal complex query embedding named Temporal Feature-Logic Embedding framework (TFLEX) to answer the temporal complex queries. Specifically, we utilize fuzzy logic to compute the logic part of the Temporal Feature-Logic embedding, thus naturally modeling all first-order logic operations on the entity set. In addition, we further extend fuzzy logic on timestamp set to cope with three extra temporal operators (**After**, **Before** and **Between**). Experiments on numerous query patterns demonstrate the effectiveness of our method.

## 1 Introduction

Multi-hop logical reasoning over knowledge graphs (KGs) is a fundamental issue in artificial intelligence. It aims to find the answer entities for a first-order logic (FOL) query which involves logical operators (existential quantification \(\), conjunction \(\), disjunction\(\) and negation\(\)). Generally, the query is parsed into computation graph, according to which subgraph matching is executed on KG to find the answers. The computation graph is a directed acyclic graph (DAG) whose nodes represent entity sets, and edges represent logical operators acting on entity sets. However, results are inevitably incorrect as KGs are incomplete and noisy. Besides, the computation complexity will spiral for large-scale KGs or large queries. Therefore, people propose to embed query into low-dimensional space to solve the problem.

Query embedding (QE) learns the embeddings of queries and entities, so that answer entities are close to its queries in the embedding space. It has attracted arising attention, as low-dimension embeddings can model implicit dependency and reduce computation. There follows a series of QE methods, including GQE, Query2box, BetaE, ConE, etc. However, existing works only focus on queries over static KGs. These methods can neither handle an entity query involving temporal information and operators, nor answer the timestamp set for a temporal query.

Temporal knowledge graph (TKG) augments triples in static knowledge graphs with temporal information, represented as \(<\)**head, relation, tail, timestamp\(>\)** named fact. For example, the fact \(<\)**Angela Merkel, make a visit, China, 2010-07-16\(>\)** specifies the time when the event happens. Generally speaking, TKGs are more close to real world than static KGs, because most knowledge needs to be updated with time, and static KGs cannot express this change. In recommendation systems, TKGs are used to model user behaviors, which includes liking, buying, reading, commenting and so on. In financial applications, TKGs involve stock holding behaviors, trading behaviors, and financial events. These applications require reasoning over TKGs to answer complex queries. However, recent researches in TKGs focus on temporal link prediction, which is simply single-hop. A complex logical query involving multiple facts for multi-hop reasoning is not fully explored yet.

To fill the gap, we introduce a temporal multi-hop logical reasoning task over TKGs. The task aims to answer temporal complex queries, which have two main distinctions from existing queries over static KGs. Firstly, the answer sets for queries over TKGs are either entity sets or timestamp sets, while that for existing queries over static KGs can only be entity sets. Secondly, as temporal information is included in the query, temporal operators such as **After**, **Before** should be considered apart from FOL operators. To understand this new task, we provide the definition of temporal complex query in Section 3. In addition, an example of a temporal complex query is shown in Figure 1. This example query pertains to the financial event of China's visit and may be of interest to financial analysts.

According to the definition of the temporal complex query, we then generate datasets of temporal complex queries on three popular TKGs, and propose the first temporal complex query embedding framework, Temporal Feature-Logic Embedding framework (TFLEX) to answer these queries. In our framework, embeddings of objects (entity, query, timestamp) are divided into two parts, the entity part, and the timestamp part. The entity part handles the FOL operations over entities, while the timestamp part copes with the temporal operations over timestamps. Each part is further divided into feature and logic components. On the one hand, the computation of the logic components follows fuzzy logic, which enables our framework to handle all FOL operations. On the other hand, feature components are mingled and transformed under the guidance of logic components, thereby integrating logical information into the feature. Moreover, we extend fuzzy logic to support extra temporal operations (**After**, **Before** and **Between**) to handle temporal operations in the queries.

The contributions of our work are summarized as follows: (1) For the first time, the definition of the task of multi-hop logical reasoning over TKGs is given. (2) We propose the first multi-hop logical reasoning framework on TKGs, namely Temporal Feature-Logic Embedding framework (TFLEX),

Figure 1: A typical multi-hop temporal complex query and its computation graph: “During François Hollande was the president of France, which countries did Xi Jinping visit but Barack Obama did not visit?”. In the computation graph, there are entity set (blue circle), timestamp set (green triangle), time set projection (green arrow), entity set projection (blue arrow) and logical operators (red rectangle).

which supports all FOL operations and extra temporal operations (**After**, **Before** and **Between**). (3) We generate three new TKG datasets for the task of multi-hop logical reasoning. Experiments on three generated datasets demonstrate the efficacy of the proposed framework. The source code of our framework and the datasets are available online *.

Footnote *: https://github.com/LinXueyuanStdio/TFLEX

## 2 Related Work

**Complex Query Embedding**. It learns embeddings of queries and entities, and the answer entities are close to queries in the embedding space. Existing methods utilize a lot of objects to create the embeddings, such as (1) probability distribution [3; 5] (2) geometric object [1; 2; 4; 6; 7] (3) fuzzy logic [8; 9; 10] and (4) others [11; 12; 13]. However, existing embedding-based methods are considered on static KGs. They cannot utilize temporal information in the TKGs, and therefore cannot handle temporal queries on a temporal KG. Firstly, static query embeddings (QEs) are built over (s, r, o) triples instead of (s, r, o, t) quartets, thus ignoring the timestamps for temporal complex reasoning. The second reason is the order property of timestamps, which is on the contrary that entities are unordered, leading to that static QEs are unable to handle **Before** and **After** temporal logic. In addition, we also notice the semantic conflict in experiments (see section 5.2) when concatenating the geometric embedding (static QE) with the fuzzy embedding (that can handle temporal logic) to promote the static QE to temporal one. Therefore, it is challenging for static QEs to utilize temporal information in the TKGs.

**Temporal Knowledge Graph Completion (TKGC)**. It aims at inferencing new facts in the TKGs. Existing TKGC methods could be categorized to (1) tensor decomposition [14; 15; 16], (2) timestamp-based transformation [17; 18; 19; 20; 21], (3) dynamic embedding [22; 23; 24; 25], (4) Markov process models [26; 27], (5) autoregressive models [28; 29; 30], (6) others [31; 32; 33] and so on. Among these works, most of them only confined to the one-hop link prediction task, also known as one-hop reasoning. Some works [28; 29; 30; 25; 32] can perform multi-hop reasoning via a path consisting of connected quartets. But none of them could answer logical queries that involve multiple logical operations (conjunction, negation and disjunction). In this paper, we focus on the temporal complex query answering task, which is more challenging than TKGC task.

## 3 Definitions

**Temporal Knowledge Graph (TKG)**\(G=\{,,,\}\) consists of entity set \(\), relation set \(\), timestamp set \(\) and fact set \(=\{(s,r,o,t)\}\) containing subject-predicate-object-timestamp \((s,r,o,t)\) quartets. Without loss of generality, \(G\) is a first-order logic knowledge base, where each quartet \((s,r,o,t)\) denotes an atomic formula \(r(s,o,t)\), with \(r\) a predicate and \(s,o,t\) its arguments.

**Multi-hop Logical Reasoning over TKG** is the task to answer Temporal Complex Query \(q\) when given a TKG \(G\). We focus on Existential Positive First-Order (EPFO) query  over TKG, namely **Temporal Complex Query**\(q\), which is categorized into entity query and timestamp query. Formally, the query \(q\) consists of a target variable \(A\), a non-variable anchor entity set \(V_{a}\), a non-variable anchor timestamp set \(T_{a}\), bound variables \(V_{1},,V_{k}\) and \(T_{1},,T_{l}\), logical operations (existential quantification \(\), conjunction \(\), disjunction \(\), identity \(1\), negation \(\)), and extra temporal operations (**After**, **Before**). \((t_{1},t_{2})\) means \(t_{2}\) is after \(t_{1}\), while \((t_{1},t_{2})\) means \(t_{2}\) is before \(t_{1}\). Inspired by [2; 3], the disjunctive normal form (DNF) of temporal query \(q\) is defined as:

\[q[A]=A : V_{1},,V_{k},T_{1},,T_{l}:(e_{1}^{1}  e_{n_{1}}^{1})(e_{1}^{m} e_{n_{m} }^{m})\] \[\;\;e=f r(V_{s},V_{o}A,T)f  r(V_{s}A,V_{o},T)g(T_{i},T_{j})\;q\] \[e=f r(V_{s},V_{o},TA)g(T_{i},T_{j}) g(T,A)g(A,T)q\] \[\;\;V_{s},V_{o} V_{a}\{V_{1},,V_{k}\},  T,T_{i},T_{j} T_{a}\{T_{1},,T_{l}\},\] \[r,f\{1,\},g\{,\}\]

In the equation, the DNF is a disjunction of \(m\) conjunctions, where \(e_{1}^{j} e_{n_{j}}^{j}\) denotes a conjunction between \(n_{j}\) logical atoms, and each \(e_{i}^{j}\) denotes a logical atom. We ignore indices in the definition of \(e_{i}^{j}\) to keep the formula clean. The goal of answering the query \(q\) is to find the set of entities (or timestamps) \( q\) that satisfy the query, such that \(A q\) iff \(q[A]\) holds true, where \( q\) is the answer set of the query \(q\).

Following existing static query embedding works, we introduce **Computation Graph**, which is a directed acyclic graph (DAG) representing the structure of temporal complex query. Its nodes represent entity/timestamp sets \(S V_{a} V T_{a} T\), while directed edges represent logical or relational operations acting on these sets. A computation graph specifies how the reasoning of the query has proceeded on the TKG. Starting from anchor sets, we obtain the answer set after applying operations iteratively on non-answer sets according to the directed edges in the computation graph. The edge types on the computation graph are defined as follows:

1. Relational Projection \(\). Given an entity set \(S_{1}\), a timestamp set \(S_{2}\)(or an entity set \(S_{2}\) for entity projection) and a relation \(r\), projection operation maps \(S_{1}\) and \(S_{2}\) to another set: \(S^{}=_{(v S_{1},t S_{2})}\{v^{}|(v,r,v^{ },t)\},&\\ _{(v S_{1},v^{} S_{2})}\{t|(v,r,v^{},t)\},&\)
2. Intersection \(\) (Union \(\), etc.). Given entity sets or timestamp sets \(\{S_{1},,S_{n}\}\), the intersection (union, etc.) operation computes logical intersection (union, etc.) of these sets \(_{i=1}^{n}S_{i}\) (\(_{i=1}^{n}S_{i}\), etc.).
3. Complement/Negation \(\). The complement set of a given set \(S\) is \(=-S,&S\\ -S,&S\)
4. Extended temporal operators \(f\). Given a timestamp set \(S\), extended operators compute a certain set of timestamps \(S^{}\): \(S^{}=\{t^{}|t^{},t^{ }>(S)\},&f\\ \{t^{}|t^{},t^{}<(S)\},&f \)

In order to efficiently compute the answer set of a temporal complex query, we consider embedding the query set into a low-dimensional vector space, where the answer set is also represented by a continuous embedding vector. Formally, the **Temporal Query Embedding**\(_{q}\) of a query \(q\) is a continuous embedding vector, generated by executing operations according to the computation graph, starting from the temporal embeddings of anchor entity or timestamp sets. The **Temporal Query Answer** to the query \(q\) is the entity \(v\) (or timestamp \(t\)) whose embedding \(\) (or \(\)) has the smallest distance \(dist(,_{q})\) (or \(dist(,_{q})\)) to the embedding of query \(q\).

## 4 Method

In this section, we replace the variables in the query formulation with temporal feature-logic embeddings, and perform logical operations via neural networks. We first introduce the temporal feature-logic embedding for entities, timestamps, and queries in Section 4.1. Afterwards, we introduce logical operators in Section 4.2 and how to train the model in Section 4.3.

### Temporal Feature-Logic Embeddings for Queries and Entities

In this section, we design temporal embeddings for queries, entities and timestamps. In general, the answers to queries may be entities or timestamps. Therefore, we consider a part of the embedding as an entity part to answer entities, while the other is the timestamp part to answer timestamps. Formally, the embedding of query \( q\) is \(_{q}=(_{f}^{e},_{i}^{e},_{f}^{t},_{l}^{t})\) where \(_{f}^{e}^{d}\) is entity feature, \(_{i}^{e}^{d}\) is entity logic, \(_{f}^{t}^{d}\) is time feature, \(_{l}^{t}^{d}\) is time logic respectively, \(d\) is the embedding dimension. The parameter \(_{l}\) is the uncertainty \(_{l}+(1-_{l})\) of the feature, according to fuzzy logic. An entity \(v\) is a special query without uncertainty. We propose to represent an entity as the query with logic part \(\), which indicates that the entity's uncertainty is \(0\). Formally, the embedding of entity \(v\) is \(=(_{f}^{e},,,)\), where \(_{f}^{e}^{d}\) is the entity feature part and \(\) is a \(d\)-dimensional vector with all elements being \(0\). Similarly, the embedding of timestamp \(t\) is \(=(,,_{f}^{t},)\) with entity part and time logic being \(\).

Attention that we introduce vector logic, which is a type of fuzzy logic over vector space, to cope with logical transformation in the vector space. Fuzzy logic is a generalization of Boolean logic, such that the truth value of a logical atom is a real number in \(\). In comparison, as a generalization of a real number, the truth value in vector logic is a vector \(^{d}\) in the semantic space. We denote the logical operations in vector logic as \(()\), \(()\), \(()\), and so on, which receive one or multiple vectors and output one vector as answer. For more details about fuzzy logic, please refer to Appendix A.1.

### Logical Operators for Temporal Feature-Logic Embeddings

In this section, we introduce the designed neural logical operators, including projection, intersection, complement, and all other dyadic operators.

**Projection Operator \(_{e}\) and \(_{t}\)**. The goal of operator \(_{e}\) is to map an entity set to another entity set under a given relation and a given timestamp, while operator \(_{t}\) outputting timestamp set given relation and two entity queries. We define a function \(_{e}:_{q},,_{t}_{q}^ {}\) in the embedding space to represent \(_{e}\), and \(_{t}:_{q_{1}},,_{q_{2}} _{q}^{}\) for \(_{t}\) respectively. To implement \(P_{e}\) and \(P_{t}\), we first represent relations as translations on query embeddings and assign each relation with relational embedding \(=(_{f}^{e},_{i}^{e},_{f}^ {t},_{i}^{t})\). We follow the assumption of translation-based methods: \(q_{o} q_{s}+r+t\). As a comparison, static KGE TransE  has \(o s+r\), and temporal KGE TTransE  has \(o_{t} s_{t}+r\). The addition represents a semantic translation starting from the source entity set, following the relation and timestamp conditioning, ending at the target entity set. Therefore, we define \(_{e}\) and \(_{t}\) as:

\[_{e}(_{q},,_{t}) =g(_{0}^{e}(_{q}++_{t}))\] (1) \[_{t}(_{q_{1}},,_{q_{2}}) =g(_{0}^{t}(_{q_{1}}++_ {q_{2}}))\]

where \(:^{4d}^{4d}\) is a multi-layer perception network (MLP), \(+\) is element-wise addition and \(g\) is an activate function to generate \(_{i}^{e}^{d}\), \(_{i}^{t}^{d}\). We use MLP and activation function \(g(.)\) to make projection operator output a valid query embedding, which shows the closure property of the operator. \(_{e}\) and \(_{t}\) do not share parameters so the MLPs are different. We define \(g\) as:

\[g()=[[0:d];([d:2d]);[2d:3d]; ([3d:4d])]\] (2)

where \([0:d]\) is the slice containing element \(x_{i}\) of vector \(\) with index \(0 i<d\), \(()\) is Sigmoid function and \([;]\) is the concatenation operator.

**Dyadic Operators**. There are two types of dyadic operators for our framework. One for entity set and the other for timestamp set. Each type includes intersection (AND), union (OR), the symmetric difference (XOR), etc. With the help of fuzzy logic, our framework can model all dyadic operators directly. Below we take a unified way to build these operators.

We start from intersection operators \(_{e}\) (on entity set) and \(_{t}\) (on timestamp set). The goal of intersection operator \(_{e}\) (\(_{t}\)) is to represent \([\![q]\!]=_{i=1}^{n}[\![q_{i}]\!]\) based on their entity parts (times-tamp parts). Suppose that \(_{q_{i}}=(_{i,f}^{e},_{i,l}^{e}, _{i,f}^{t},_{i,l}^{t})\) is temporal feature-logic embedding for \([\![q_{i}]\!]\). We notice that there exists **Alignment Rule** in the process of reasoning. When performing entity set intersection \(_{e}\), we should also perform intersection on timestamp parts in order to align the entities into the same time set. The same also holds for timestamp set intersection \(_{t}\) and all other dyadic operators. Therefore, we firstly define the intersection operators as follows:

\[_{e}(_{q_{1}},,_{q_{n}}) =(_{i=1}^{n}_{i}^{e}_{i,f}^ {e},^{n}_{i}\{_{i,l}^{e}\}},_{i=1}^ {n}_{i}^{e}_{i,f}^{t},\ \ }{{ }}}\{_{i,l}^{t}\}\ )\] \[_{t}(_{q_{1}},,_{q_{n}}) =(_{i=1}^{n}_{i}^{t}_{i,f}^ {e},\ }{{ }}}\{_{i,l}^{e}\},\ }{{ }}}\{_{i,l}^{e}\},\ }{{ }}}\{_{i,l}^{t}\},\ }{{ }}}\{_{i,l}^{t}\}\ )\] \[_{t}(_{q_{1}},,_{q_{n}}) =(_{i=1}^{n}_{i}^{t}_{i,f}^ {e},\ }{{}}}\{_{i,l}^{e}\}, \ }{{}}}\{_{i,l}^{e}\}, }{{}}}\{_{i,l}^{t}\}, \ }{{}}}\{_{i,l}^{t}\})\]

where **AND** is the conjunction in fuzzy logic, \(_{i}\) and \(_{i}\) are attention weights. To notice the changes of logic, we compute \(_{i}^{e,t}\) and \(_{i}^{e,t}\) via the following attention mechanism:

\[_{i}^{e,t}=_{1}^{e,t}([_{ i,f}^{t};_{i,l}^{e}]))}{_{j=1}^{n}(_{1}^{e,t}([ _{j,f}^{t};_{j,l}^{t}]))},_ {i}^{e,t}=_{2}^{e,t}([_{i,f}^{t}; _{i,l}^{t}]))}{_{j=1}^{n}(_{2}^{e,t}([ _{j,f}^{t};_{j,l}^{t}]))}\] (3)where \(_{1,2}^{e,t}:^{2d}^{d}\) are MLP networks, \([;]\) is concatenation. The first self-attention neural network will learn the hidden information from entity logic and leverage to entity feature, while the second one gathers logical information from time logic to time feature. Note that the computation of entity logic, and time logic obeys the law of fuzzy logic, without any extra learnable parameters. In this way, all dyadic operators can be generated from fuzzy logic in our framework. Due to space limitation, we present the union, exclusive or, implication operators and so on in Appendix A.3.

**Complement Operators:**\(_{e}\) and \(_{t}\). The aim of \(_{e}\) is to identify the complement of query set \( q\) such that \(-q= q\), while \(_{t}\) aims to calculate the complement \(-q= q\) by the time parts. Suppose that \(_{q}=(_{f}^{e},_{f}^{e},_{f}^{t},_{f}^{t})\), we define the complement operator \(_{e}\) and \(_{t}\) as:

\[_{e}(_{q})=(f_{}^{e}(_{f}^{e}),(_{l}^{e}),_{f}^{t},_{l}^{t}),_{t}( _{q})=(_{f}^{e},_{l}^{e},f_{}^{t}(_{f}^{ t}),(_{l}^{t}))\] (4)

where \(f_{}^{e}(_{f})=(_{3}([_{f}^{e};_{ f}^{e}])),f_{}^{t}(_{f}^{t})=(_{4}([_{f}^{t};_{l}^{t}]))\) are feature negation functions, two \(_{3,4}:^{2d}^{d}\) are MLP networks, **NOT** is negation in fuzzy logic.

**Temporal Operators: After \(_{t}\), Before \(_{t}\) and Between \(_{t}\).** The operator After \(_{t}:_{q}_{q}^{}\) (Before \(_{t}:_{q}_{q}^{}\)) aims to deduce the timestamps after(before) a given fuzzy time set \( q\). Let \(_{q}=(_{f}^{e},_{l}^{e},_{f}^{t},_{l}^{t})\), we define \(_{t}\) and \(_{t}\) as:

\[_{t}(_{q})=(_{f}^{e},_{l}^{e},_{f}^{t}+ _{l}^{t}}{2},_{l}^{t}}{2}),_{t}( _{q})=(_{f}^{e},_{l}^{e},_{f}^{t}-_{l }^{t}}{2},_{l}^{t}}{2})\] (5)

The entity part does not change after computation because temporal operator only affects the time part (time feature \(_{f}^{t}\) and time logic \(_{l}^{t}\)). The motivation of computation is illustrated in Figure 2. Since \(_{l}^{t}\) is the uncertainty of time feature \(_{f}^{t}\), the time part can be viewed as an interval \([_{f}^{t}-_{l}^{t},_{f}^{t}+_{l}^{t}]\) whose center is \(_{f}^{t}\) and half-length is \(_{l}^{t}\). The interval is covered by \([_{f}^{t}-1,_{f}^{t}+1]\) because the probability \(_{l}^{t}<1\). Then, after interval \([_{f}^{t}-_{l}^{t},_{f}^{t}+_{l}^{t}]\) is the interval \([_{f}^{t}+_{l}^{t},_{f}^{t}+1]\) whose center is \(_{f}^{t}+_{l}^{t}}{2}\) and half-length is \(_{l}^{t}}{2}\), which gives the time part of embedding \(_{t}(_{q})\). Similarly, the time part of embedding \(_{t}(_{q})\) is \(_{f}^{t}-_{l}^{t}}{2}\) (time feature) and \(_{l}^{t}}{2}\) (time logic), which are generated from \([_{f}^{t}-1,_{f}^{t}-_{l}^{t}]\) before \([_{f}^{t}-_{l}^{t},_{f}^{t}+_{l}^{t}]\). The operator Between \(_{t}:_{q_{1}},_{q_{2}}_{q}^{}\) inferences the time set after \( q_{1}\) and before \( q_{2}\). Therefore, we define Between \(_{t}\) as \(_{t}(_{q_{1}},_{q_{2}})=_{t}( _{t}(_{q_{1}}),_{t}(_{q_{2}}))\) to output the time between \( q_{1}\) and \( q_{2}\).

### Learning Temporal Feature-Logic Embeddings

We expect the model to achieve high scores for the answers to the given query \(q\), and low scores for \(v^{} q\). Therefore, we firstly define a distance function to measure the distance between a given answer embedding and a query embedding, and then we train the model with negative sampling loss.

**Distance Function** Given an entity embedding \(=(_{f}^{e},,,)\), a timestamp embedding \(=(,,_{f}^{t},)\) and a query embedding \(_{q}=(_{f}^{e},_{f}^{e},_{f}^{t},_{f}^{t},_{l}^{t})\), we define the distance \(d\) between the answer and the query \(q\) as the sum of the feature distance (between the feature parts) and the logic part (to expect uncertainty to be \(0\)). If the query answers entities, the distance is \(d^{e}(;_{q})=\|_{f}^{e}-_{f}^{e}\|_{1}+_{l}^{e}\). Otherwise, the distance is \(d^{t}(;_{q})=\|_{f}^{t}-_{f}^{t}\|_{1}+_{l} ^{t}\) for queries answering timestamp set, where \(\|\|_{1}\) is the \(L_{1}\) norm and \(+\) is element-wise addition. The distance function aims to optimize two losses. One is to push the answers to the neighbor of query in the embedding

Figure 2: The computation of time part in temporal operators Before and After.

space. It corresponds to the term L1 distance between answer and query. The other is to reduce the uncertainty of the query (the probability interpretation of the logic part), to make the answers more accurate.

**Loss Function** Given a training set of queries, we optimize a negative sampling loss

\[L=-(-d(;_{q}))-_{i=1}^{k} (d(_{i}^{};_{q})-)\] (6)

where \(>0\) is a fixed margin, \(k\) is the number of negative entities, and \(()\) is the sigmoid function. When query \(q\) is answering entities (timestamps), \(v q\) is a positive entity (timestamp), \(v_{i}^{} q\) is the \(i\)-th negative entity (timestamp).

## 5 Experiments

In this section, we evaluate the ability of TFLEX to reason over TKGs. We first introduce experimental settings in Section 5.1, and then present the experimental results in Section 5.2.

### Experimental Settings

Datasets and Query GenerationExperiments are performed on three new datasets generated from standard benchmarks for TKGC: ICEWS14 , ICEWS05-15 , and GDELT-500  (with statistics in Appendix B.1). We predefined 40 kinds of complex queries for each dataset. The definition of the 40 kinds of complex queries and the query generation process details are described in Appendix B.2. We consider 27 kinds of queries for training and all 40 kinds for evaluation and testing. Please refer to Appendix B.3 for summaries of the final datasets.

To briefly summarize the results, we aggregate groups of queries that can be answered: entities (\(_{e}\)), timestamps (\(_{t}\)), entities with negation (\(_{e,_{e}}\)), timestamps with negation (\(_{t,_{t}}\)), entities with unseen union (\(_{\{_{e}\}}\)), timestamps with unseen union (\(_{\{_{t}\}}\)), and other hybrid unseen structures (\(_{x}\)). These groups are inspired by the experiment settings of existing static QEs . The detail that which query belongs to which group will be shown in Appendix B.7. Note that the training set only involves 4 groups of queries: \(_{e}\), \(_{t}\), \(_{e,_{e}}\), and \(_{t,_{t}}\).

EvaluationGiven a test query \(q\), for each non-trivial answer \(v q_{}- q_{}\) of the query \(q\), we rank it against non-answer entities \(- q_{}\) (or non-answer timestamps \(- q_{}\) if the query is answering timestamps). Then we calculate Mean Reciprocal Rank (MRR) based on the rank. The higher, the better. Please refer to Appendix B.5 for the definition of MRR.

### Main Results

For each group, we report the average MRR in Table 1. The raw MRR results on all query structures for each dataset in detail are presented in Appendix B.7.

**How well can TFLEX answer temporal complex queries?** We compare TFLEX with state-of-the-art query embeddings Query2box , BetaE  and ConE  on answering entities. Existing query embeddings only handle FOL on entity set, but unable to cope with temporal logic over timestamp set. Therefore, the results of these three methods have to be obtained by ignoring the timestamps, so that the \(_{t}\), \(_{t,_{t}}\), \(_{\{_{t}\}}\) and \(_{x}\) of three methods are zeros. Comparing the results of these methods in Table 1, we can see that TFLEX outperforms all the baselines on all the metrics. These results demonstrate that TFLEX can perform reasoning over TKGs well, at least better than the existing query embeddings.

**Ablation on entity part.** The variant **X(ConE)** replaces the entity part of TFLEX with ConE  to handle the logic over entity sets. In the entity part, ConE is geometric while TFLEX is fuzzy. The variant performs even worse than TFLEX and ConE. The dropping MRR indicates that the entity part plays an important role in the framework. Considering the performance of ConE, we think there is semantic conflict between the time part of TFLEX and the entity part of ConE. Simply combining static QEs with dynamic QEs is not a clever way to achieve the best performance.

**Ablation on time part**. The variant **FLEX** removes the time part of the temporal feature-logic embedding. Then, **FLEX** can only answer entities. The results show that **FLEX** slightly outperforms ConE. However, the performance of **FLEX** is worse than TFLEX with a large margin on all the datasets. Therefore, we conclude that the time part also plays an important role in the framework.

**Ablation on feature part**. If we remove the entity and timestamp feature, the embeddings of entities and timestamps will crash to zeros. Instead, we consider another way to explore the impact of the feature part. Noticing that some TKGC approaches  embed entities and timestamps into the same semantic space, we propose **X-IF** to merge the entity and timestamp features into one feature. Compared with TFLEX, **X-IF** achieves higher scores on ICEWS14 and ICEWS05-15, but lower on GDELT. The results imply that unifying the feature of entity and timestamp is potentially beneficial in some datasets.

**Ablation on logic part**. The variant **X-logic** removes both entity logic and time logic. It achieves lower scores than TFLEX on all the datasets. This is because the logic part is responsible for reasoning over TKGs. Removing the logic part results in that neural logical operators completely rely on neural network to learn the logic, which is not enough to handle various temporal complex queries.

**Out-of-data reasoning**. The results on \(_{\{U_{e}\}},_{\{U_{t}\}},_{x}\) support that the framework can reason over unseen entity logic, unseen time logic as well as their hybrid. The entity union and time union operators are not included in the training set, but the framework can still handle them well.

**Sensitive analysis**. (1) The selection of hyperparameters (embedding dimension \(d\), the margin \(\)) has a substantial influence on the effectiveness of TFLEX. We train the model with embedding dimension \(d\{300,400,500,600,700,800,900,1000\}\), the margin \(\{5,10,15,20,25,30,35,40\}\). The best performance is achieved when \(d=800\) and \(=15\). Too small and too large \(d,\) both lead to worse results, reported in Appendix B.6. (2) Besides, we also investigate the stability of TFLEX. We train and test for five times with different random seeds and report the error bars in Appendix B.6. The small standard variances demonstrate that the performance of TFLEX is stable.

**Comparison between TFLEX and TKGC methods**. It's natural to compare TFLEX with SOTA TKGC methods, since all of them can answer one-hop temporal queries. We present the comparison results in Table 2. We can see that TFLEX is competitive with translation-based methods

  
**Dataset** & **Metrics** & **Query2box** & **BetaE** & **ConE** & **TFLEX** & **X(ConE)** & **FLEX** & **X-IF** & **X-logic** \\   & avg\({}_{c}\) & 25.06 & 37.19 & 41.94 & 56.79 & 40.93 & 43.67 & 56.89 & 56.64 \\  & avg\({}_{c,c_{s}}\) & & 36.69 & 44.88 & 50.82 & 42.15 & 44.41 & 49.78 & 51.17 \\  & avg\({}_{t}\) & & & & 17.56 & 16.41 & & 18.77 & 18.03 \\  & avg\({}_{t,c_{t}}\) & & & & 36.37 & 35.24 & & 37.73 & 36.39 \\  & avg\({}_{\{U_{e}\}}\) & & & & 26.47 & 35.74 & 25.46 & 29.25 & 34.48 & 34.68 \\  & avg\({}_{\{U_{t}\}}\) & & & & 26.24 & 24.07 & & 28.04 & 26.36 \\  & avg\({}_{\{U_{t}\}}\) & & & & 28.03 & 26.65 & & 29.31 & 28.61 \\  & **AVG** & & & & 35.93 & 30.13 & & 36.43 & 35.98 \\   & avg\({}_{c}\) & 24.00 & 31.33 & 40.93 & 48.99 & 36.29 & 38.96 & 49.90 & 44.80 \\  & avg\({}_{c,c_{s}}\) & & & 29.70 & 43.52 & 46.17 & 38.12 & 42.10 & 46.11 & 41.92 \\  & avg\({}_{t}\) & & & & 4.39 & 4.41 & & 4.43 & 3.29 \\  & avg\({}_{t,c_{t}}\) & & & & 30.16 & 29.49 & & 30.26 & 28.34 \\  & avg\({}_{\{U_{t}\}}\) & & & & 28.69 & 26.40 & & 27.70 & 23.39 \\  & avg\({}_{x}\) & & & & 24.26 & 21.69 & & 24.41 & 21.95 \\  & **AVG** & & & & 33.72 & 27.54 & & 33.98 & 29.86 \\   & avg\({}_{c}\) & 9.67 & 14.75 & 18.44 & 19.60 & 17.83 & 19.07 & 17.92 & 17.36 \\  & avg\({}_{c,c_{s}}\) & & & & 11.15 & 12.67 & 13.52 & 12.34 & 13.35 & 12.13 & 12.11 \\  & avg\({}_{t}\) & & & & 5.38 & 3.16 & & 5.49 & 5.75 \\  & avg\({}_{\{U_{e}\}}\) & & & & 6.31 & 3.93 & & 6.50 & 6.86 \\  & avg\({}_{\{U_{e}\}}\) & & & & & 6.17 & 6.35 & & 6.59 & 6.80 \\  & avg\({}_{x}\) & & & & 6.17 & 6.17 & & 6.47 & 6.64 \\  & avg\({}_{\{U_{e}\}}\) & & & & 9.32 & 8.17 & & 8.86 & 8.92 \\   

Table 1: Average MRR results for different groups of temporal complex queries. **X** denotes the variant of TFLEX. **X(ConE)** replaces the entity part with ConE . **FLEX** ablates the time part. **X-1F** merges entity feature and timestamp feature into one feature. **X-logic** removes the logic part.

(ConT , TTransE , HyTE , etc.), but it doesn't outperform the SOTA TKGC methods like ChronoR  and TuckERT . However, the result doesn't affect the novelty and contribution of this paper. Please note that the projection operator of TFLEX is as simple as TTransE, not further optimized for TKGC tasks only. Upgrading the projection operator to outperform SOTA TKGC methods remains a future work.

**Necessity of training on temporal complex queries**. Our experiments demonstrate that training on complex queries is necessary to achieve the best performance. We compare with translation-based \(_{e}\) operators (TTransE , HyTE  and TFLEX's variant **TFLEX-1p**) using only one-hop **Pe** queries for training. We choose TTransE and HyTE because our projection operator is also translation-based (\(_{e}(_{q},,_{t})_{q }++_{t}\)). From the result table 3, we observe that TFLEX achieves the best performance when comparing with these translation-based baselines on all datasets. Besides, compared with **TFLEX-1p**, TFLEX achieves 7.9% relative improvement on average on MRR, which demonstrates that training on complex queries could improve the one-hop query-answering ability.

**Effectiveness of neural temporal operators**. We found that the temporal operators \(_{t}\) and \(_{t}\) change the semantic of predicted timestamp embedding logically. We randomly select an event \((s,r,o)\) from ICEWS14 and consider three temporal queries **Pt**, **bPt** and **aPt**. Then, **Pt**(\(=_{t}(s,r,o)\)) predicts the date \(t\) when this event happened. And **bPt**(\(=_{t}(_{t}(s,r,o))\)) should predict the date before \(t\), while **aPt**(\(=_{t}(_{t}(s,r,o))\)) is supposed to predict the time after \(t\). The output of three queries are time embeddings. Because the time embedding is fuzzy, we score it to all possible Timestamps, and visualize the normalized similarity score distribution over all days in a year, from 0 to 365 in ICEWS14. The higher the score, the more possible the predicted date is the day. We plot the three score distributions in Figure 3. For each distribution, we highlight the periods of the highest scores

    &  &  &  \\   & MRR & Hit@1 & Hit@3 & Hit@10 & MRR & Hit@1 & Hit@3 & Hit@10 & MRR & Hit@1 & Hit@3 & Hit@10 \\  TransE\({}^{*}\) & 28.0 & 9.4 & - & 63.7 & 29.4 & 9.0 & - & 66.3 & 11.3 & 0.0 & 15.8 & 31.2 \\ DistMat\({}^{}\) & 43.9 & 32.3 & - & 67.2 & 45.6 & 33.7 & - & 69.1 & 19.6 & 11.7 & 20.8 & 34.8 \\ SimpleE\({}^{*}\) & 45.8 & 34.1 & 51.6 & 68.7 & 47.8 & 35.9 & 53.9 & 70.8 & 20.6 & 12.4 & 22.0 & 36.6 \\  ConT  & 18.5 & 11.7 & 20.5 & 31.5 & 16.3 & 10.5 & 18.9 & 27.2 & 14.4 & 8.0 & 15.6 & 26.5 \\ TTransE  & 25.5 & 7.4 & - & 60.1 & 27.1 & 8.4 & - & 61.6 & 11.5 & 0.0 & 16.0 & 31.8 \\ HyTE  & 29.7 & 10.8 & 41.6 & 65.5 & 31.6 & 11.6 & 44.5 & 68.1 & 11.8 & 0.0 & 16.5 & 32.6 \\ TA-DistMat  & 47.7 & 36.3 & - & 68.6 & 47.4 & 34.6 & - & 72.8 & 20.6 & 12.4 & 21.9 & 36.5 \\ DE-Transfer  & 32.6 & 12.4 & 46.7 & 68.6 & 31.4 & 10.8 & 45.3 & 68.5 & 12.6 & 0.0 & 18.1 & 35.0 \\ DE-DistMat  & 50.1 & 39.2 & 56.9 & 70.8 & 48.4 & 36.6 & 54.6 & 71.8 & 21.3 & 13.0 & 22.8 & 37.6 \\ DE-SimpleE  & 52.6 & 41.8 & 59.2 & 72.5 & 51.3 & 39.2 & 57.8 & 74.8 & 23.0 & 14.1 & 24.8 & 40.3 \\ ChronoR  & **62.5** & **54.7** & **66.9** & **77.3** & **67.5** & **59.6** & **72.3** & **82.0** & - & - & - & - \\  TuckERT  & 59.4 & 51.8 & 64.0 & 73.1 & 62.7 & 55.0 & 67.4 & 76.9 & **41.1** & **31.0** & **45.3** & **61.4** \\ TuckERTN  & 60.4 & 52.1 & 65.5 & 75.3 & 63.8 & 55.9 & 68.6 & 78.3 & 38.1 & 28.3 & 41.8 & 57.6 \\  RGCRN\({}^{}\) & 33.3 & 24.0 & 36.5 & 51.5 & 35.9 & 26.2 & 40.0 & 54.6 & 18.6 & 11.5 & 19.8 & 32.4 \\ CyGNet\({}^{}\) & 34.6 & 25.3 & 38.8 & 53.1 & 35.4 & 25.4 & 40.2 & 54.4 & 18.0 & 11.1 & 19.1 & 31.5 \\ RE-NET\({}^{}\) & 35.7 & 25.9 & 40.1 & 54.8 & 36.8 & 26.2 & 41.8 & 57.6 & 19.6 & 12.0 & 20.5 & 33.8 \\ RE-GCN\({}^{}\) & 37.7 & 27.1 & 42.5 & 58.8 & 38.2 & 27.4 & 43.0 & 59.9 & 19.1 & 11.9 & 20.4 & 33.1 \\  TFLEX-1p & 43.9 & 31.4 & 49.6 & 64.4 & 40.6 & 29.1 & 47.5 & 66.1 & 16.5 & 8.6 & 17.3 & 33.1 \\ TFLEX & 48.2 & 35.7 & 56.5 & 72.3 & 43.0 & 30.0 & 49.8 & 69.5 & 18.5 & 10.1 & 19.6 & 34.9 \\   

Table 2: TKGC Results (%) on ICEWS14, ICEWS05-15, and GDELT. The results from top to bottom are organized as static KGEs, timestamp-based transformation TKGEs, tensor decomposition, autoregressive models and ours. Best results are in bold. \(\), \(\) indicate the results taken from . Other results are the best numbers reported in their respective paper.

Figure 3: Score distributions of **Pt**, **bPt** and **aPt**.

with a colored background. These colored blocks represent the most likely happening time interval of the event. We observe that the order of colored blocks (corresponding to the predictions of **bPt**, **Pt**, and **aPt**) matches the logical meanings of these operators (Before the event, On the event, After the event). The visualization shows that neural temporal operators perform the time transformation correctly. More examples are attached in Appendix D.

**Explaining answers with temporal feature-logic framework**. We take query **Pe2** for example. Given temporal query **Pe2**: \(q[V_{7}]=V_{7}, V_{a},r_{1}(e_{1},V_{a},t_{1}) r_{2}(V_{a},V_{7},t _{2})\), let's try an example which can be written as: On 2014-04-04, who consulted the man who was appealed to or requested by the Head of Government (Latvia) on 2014-08-01? In this example, \(e_{1}\) is "Head of Government (Latvia)", \(r_{1}\) is "Make an appeal or request", \(t_{1}\) is "2014-08-01", \(r_{2}\) is "consult\({}^{-1}\)", and \(t_{2}\) is "2014-04-04". Then we use TFLEX to execute the query and get answers. We classify the answers into easy, hard and wrong ones. The easy answer is the correct answer that appears in the training set, while the hard answer is the correct answer that exists in the testing set instead of training set.

We present five most likely answers for interpretation in Figure 4. From the table we observe that TFLEX ranks easy answers "Francois Hollande", "Taavi Roivas" and hard answer "Andris Berzins" higher than wrong answers "Angela Merkel" and "Head of Government (Latvia)". This shows that TFLEX successfully finds the hard answer by performing complex reasoning, and distinguishes the right answers from the wrong ones.

We provide 36 examples in Appendix E, including the visualization and intermediate explanation of answers for each query structure.

## 6 Conclusion

In this paper, we firstly define a temporal multi-hop logical reasoning task on temporal knowledge graphs. Then we generate three new TKG datasets and propose the Temporal Feature-Logic embedding framework, TFLEX, to handle temporal complex queries in datasets. Fuzzy logic is used to guide all FOL transformations on the logic part of embeddings. We also further extended fuzzy logic to implement extra temporal operators (**Before**, **After** and **Between**). To the best of our knowledge, TFLEX is the first framework to support multi-hop complex reasoning on temporal knowledge graphs. Experiments on benchmark datasets demonstrate the efficacy of the proposed framework in handling different operations in complex queries.

## Broader Impact

Multi-hop reasoning makes the information stored in TKGs more valuable. With the help of multi-hop reasoning, we can digest more hidden and implicit information in TKGs. It will broaden and deepen KG applications in various fields, such as question answering, recommend systems, and information retrieval. It may also bring about risks exposing unexpected personal information on public data.

Finance temporal knowledge graph is a good example to illustrate the broader impact of multi-hop reasoning. In the financial field, the information stored in TKGs is very sensitive. The one-hop reasoning can complete the hidden financial transaction, while the multi-hop reasoning can help to detect the fraud. The After operator could also be used to predict the future financial transaction. People may take the advantages of logical reasoning to digest financial factor to obtain excess returns.

Military temporal knowledge graph is another example. With the help of multi-hop logical reasoning, we may predict the future military strategy of a country. Besides, with the fuzzy completion of the hidden military information, we can also detect the hidden militarily moves, which may save thousands of soldiers' lives.

The last example is social temporal knowledge graph. The behaviors of people are left and stored in TKGs. With the help of logical reasoning, we may predict a short future of a person. For example, the query may answer the evolution of user profiles: how long may a person transfer to another role, from student to worker, becoming a parent, or being a grandparent. Tracking the user's identity change can provide super benefits for merchants' advertising. Detecting the role of a person is also helpful to provide more personalized services.

However, we should agree that the multi-hop reasoning is still at an extremely early stage, though it may bring about risks. Therefore, we should pay more attention to the security of TKGs and the privacy of users at the mean time when we explore the technology of multi-hop reasoning over TKGs.

## Limitation & Future Work

In this section, we list 4 limitations and talk about the possible future work.

**Limitation 1: The temporal operators are not enough.**. We define three extra temporal operators (**Before**, **After** and **Between**) in query generation. However, there exists more temporal operators in the real world. For example, Allen  defined 13 types of temporal relations represented by two intervals, including before/after, during/contains, overlaps/overlapped-by, meets/met-by, starts/started-by, finishes/finished-by and equal. In the future we would like to promote these temporal operators to TKGs.

**Limitation 2: The temporal embedding could improve.**. In this paper, the embedding of the timestamp is randomly initialized and finally learned by the model via logical advantages. Such embedding ignores the order of different timestamps. The order property is learned by the After and Before operators, which may be not enough. We recall that in the field of NLP, positional embeddings also have order features, which may be used for the construction of timestamp embeddings.

**Limitation 3: The query generation is time-consuming.**. There are 40 predefined query structures in our query generation module. Each query structure has 10k+ queries for training. With the scale of TKGs increasing, the number of queries will also increase, even 40x faster. Therefore, we need to find a more efficient way to generate queries for large scale TKGs.

**Limitation 4: The MRR and Hits@k are weak.**. The MRR and Hits@k evaluation metric may not inflect the performance of multi-hop reasoning. We observe that some queries have lots of answers. When the number of answers is larger than k, the MRR and Hits@k will be low, even if all answers are correctly ranked at top. Because the right answers that ranked after k are labeled as false. The Hits@k decreases with the increase number of right answers, which is not reasonable. This disadvantage prevents us from comparing the performance across different datasets. In this paper, GDELT is much denser, and the count of right answers is larger than the other two datasets. Therefore, the MRR of GDELT is lower than the other two datasets. It can also be verified that on the average MRR, group avg\({}_{t}\) is lower than group avg\({}_{e}\). The reason is that the number of right answers of group avg\({}_{t}\) is larger than that of group avg\({}_{e}\), which can be seen from the statistic of average answer count of queries in Table 10. In the future, there should be a more reasonable evaluation metric for multi-hop reasoning.