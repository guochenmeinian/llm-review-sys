# OptEx: Expediting First-Order Optimization with Approximately Parallelized Iterations

Yao Shu\({}^{\#}\), Jiongfeng Fang\({}^{}\), Ying Tiffany He\({}^{}\), Fei Richard Yu\({}^{@sectionsign}\)

\({}^{}\)Guangdong Lab of AI and Digital Economy (SZ), China

\({}^{}\)College of Computer Science and Software Engineering, Shenzhen University, China

\({}^{@sectionsign}\)School of Information Technology, Carleton University, Canada

###### Abstract

First-order optimization (FOO) algorithms are pivotal in numerous computational domains, such as reinforcement learning and deep learning. However, their application to complex tasks often entails significant optimization inefficiency due to their need of many sequential iterations for convergence. In response, we introduce _first-order optimization expedited with approximately parallelized iterations_ (OptEx), the first general framework that enhances the optimization efficiency of FOO by leveraging parallel computing to directly mitigate its requirement of many sequential iterations for convergence. To achieve this, OptEx utilizes a kernelized gradient estimation that is based on the history of evaluated gradients to predict the gradients required by the next few sequential iterations in FOO, which helps to break the inherent iterative dependency and hence enables the approximate parallelization of iterations in FOO. We further establish theoretical guarantees for the estimation error of our kernelized gradient estimation and the iteration complexity of SGD-based OptEx, confirming that the estimation error diminishes to zero as the history of gradients accumulates and that our SGD-based OptEx enjoys an effective acceleration rate of \(()\) over standard SGD given parallelism of \(N\), in terms of the sequential iterations required for convergence. Finally, we provide extensive empirical studies, including synthetic functions, reinforcement learning tasks, and neural network training on various datasets, to underscore the substantial efficiency improvements achieved by OptEx in practice. Our implementation is available at https://github.com/youyve/OptEx.

## 1 Introduction

First-order optimization (FOO) algorithms, such as stochastic gradient descent (SGD) , Nesterov Accelerated Gradient (NGA) , AdaGrad , Adam  etc., have already been the cornerstone of many computational disciplines, driving advancements in areas ranging from reinforcement learning  to machine learning . These algorithms, which are widely known for their straightforward form of iterative gradient-based updates, are fundamental in solving both simple and intricate optimization problems. However, their applications usually encounter substantial optimization inefficiency, especially when addressing complex functions that not only are _expensive in evaluating their function values and gradients_ but also necessitate _a large number of sequential iterations to converge_ in practice, e.g., deep reinforcement learning  and neural network training .

To this end, parallel computing has been widely used in the literature to considerably enhance the _optimization (e.g., time) efficiency_ of FOO by reducing the evaluation cost of function and gradient _per iteration_ in FOO . For instance, in the field of neural network training, techniques that are basedon parallel computing, e.g., data parallelism [8; 10; 11; 12], model parallelism , and pipeline parallelism [14; 15], have been employed to reduce the evaluation time of loss function and parameter gradient by processing multiple input samples and network components concurrently. However, to the best of our knowledge, few efforts have been devoted to leveraging parallel computing to reduce the _number of sequential iterations_ required for convergence to mitigate the optimization inefficiency in FOO. Different from the methods of reducing the evaluation time per iteration during optimization, which requires specialized human efforts in a specific domain (e.g., neural network training), the reduction of sequential iterations is likely to be more general since no such specialized domain efforts are required and thus shall enjoy a wider application in practice. This underscores the need to explore the potential of parallelizing sequential iterations in standard FOO.

However, the inherent iterative dependency in FOO where the output of each iteration servers as the input of the next iteration, poses a significant barrier to independent and concurrent iteration execution, thereby making it nearly impossible to realize iteration parallelism within standard FOO. To this end, we develop a novel framework called _first-order optimization expedited with approximately parallelized iterations_ (OptEx) that is capable of bypassing the challenge of inherent iterative dependency in standard FOO and therefore make parallelized iterations in FOO possible. Specifically, our framework begins with a novel kernelized gradient estimation strategy, which uses the history of gradients during optimization to predict the gradients for any input within the domain such that these estimated gradients can be used in standard FOO algorithms to determine the inputs for the next few iterations. We further introduce the techniques of separable kernel function and local history of gradients to enhance the computational efficiency of this gradient estimation (Sec. 4.1). We then apply standard FOO algorithms with this kernelized gradient estimation to determine the inputs for the next \(N\) sequential iterations efficiently (namely proxy updates), aiming to approximate the ground-truth sequential updates and bypass the iteration dependency in standard FOO (Sec. 4.2). Lastly, we complete our approximately parallelized iterations for standard FOO by leveraging parallel computing with parallelism of \(N\) to concurrently execute standard FOO algorithms over these \(N\) inputs obtained from our proxy updates using the ground-truth gradients (Sec. 4.3).

Apart from proposing our innovative OptEx framework, we further establish rigorous theoretical guarantees and extensive empirical studies underpinning its efficacy. Specifically, we give a theoretical bound for the estimation error of our kernelized gradient estimation. Remarkably, this error approaches zero asymptotically as the number of historical gradients increases, ranging across a broad spectrum of kernel functions. This suggests that our kernelized gradient estimation can facilitate effective proxy updates to help parallelize sequential iterations in FOO (Sec.5.1). Building on this, we delineate both upper and lower bounds for the sequential iteration complexity of our SGD-based OptEx, showing that our SGD-based OptEx is able to reduce the sequential iteration complexity of standard FOO algorithms at a rate of \(()\) with parallelism of \(N\) (Sec.5.2). Finally, through extensive empirical studies, including the optimization of synthetic functions, reinforcement learning tasks, and neural network training on both image and text datasets, we demonstrate the consistent advantages of our OptEx in expediting existing FOO algorithms (Sec. 6).

To summarize, our contribution to this work includes:

* To the best of our knowledge, we are _the first to develop a general framework_ (i.e., OptEx) that can leverage parallel computing to approximately parallelize the sequential iterations in FOO, thereby considerably reducing the sequential iteration complexity of FOO algorithms.
* We provide _the first upper and lower iteration complexity bound_ for SGD-based OptEx, which gives an effective acceleration rate of \(()\) with parallelism of \(N\).
* We conduct extensive empirical studies, including the optimization of synthetic function, reinforcement learning tasks, and neural network training on both image and text datasets, to support the efficacy of our OptEx framework.

## 2 Related Work

Reduction of Iteration Complexity.In the literature, various techniques have been developed to enhance the optimization efficiency of FOO by improving their sequential iteration complexity. For example, variance reduction strategies [16; 17; 18] have been proposed to accelerate stochastic optimization by effectively reducing the gradient variance and therefore aligning the iteration complexity of SGD with that of gradient descent (GD) in expectation. These strategies usually yield significant improvements in high-variance problems whereas their compelling performance is hard to extend to low-variance scenarios and deterministic contexts. Meanwhile, adaptive gradient methods, e.g., AdaGrad , Adam , and AdaBelief , have been introduced to employ an adaptive learning rate for a better-performing optimization where fewer iterations are required for convergence. Furthermore, acceleration techniques like the Nesterov method  and momentum-based updates  have also been proven to be capable of reducing the sequential iteration complexity for GD and SGD efficiently. _Orthogonal to these established methodologies, our paper introduces parallel computing as a distinct and innovative strategy to further decrease the sequential iteration complexity of FOO. Of note, such an approach not only stands independently but also offers potential for synergistic integration with existing methods, promising enhanced optimization outcomes._

Reduction of Time Complexity Per Iteration using Parallel Computing.In the realm of enhancing the computational efficiency of FOO, parallel computing has emerged as a rescue by reducing the time complexity per iteration in FOO. Particularly in the field of neural network training, data parallelism [8; 10; 11; 12] has been introduced to evaluate the gradients of model parameters w.r.t mini-batch input samples simultaneously. In addition to data parallelism, model parallelism  has been developed to process various neural network components concurrently. Furthermore, pipeline parallelism [14; 15] divides the neural network into stages and assigns each stage to a different device, allowing different stages of the computation to be executed in parallel across the pipeline. However, the tailored nature of these methods constrains their application to wider contexts. _Contradictory to these case-specified solutions, this paper proposes a general framework that can leverage parallel computing to enhance the optimization efficiency of FOO in wide practical applications._

## 3 Problem Setup

In this paper, we aim to enhance the optimization efficiency of the following stochastic minimization problem by leveraging parallel computing with parallelism of \(N\):

\[_{^{d}}F()[f ()]\;.\] (1)

Here, \( f()\) is assumed to follow a specific Gaussian distribution, i.e., \( f()( F(),^{2})\) for any \(\), which has already been widely used in the literature [21; 22; 23]. Besides, we adopt a common assumption that \( F\) is sampled from a Gaussian process, i.e., \( F(,(,))\)[24; 25; 26]. Of note, (1) has found extensive applications in practice, e.g., neural network training  and reinforcement learning . Importantly, although our primary focus is on this stochastic optimization, our method can also be applied to deterministic optimization (evidenced in Sec. 6.1).

Standard FOO algorithms commonly optimize (1) in an iterative and sequential manner:

\[_{t+1}=(_{t}, f(_{t}))\] (2)

where \(t\) is the iteration number. Ideally, if parallel computing can be used to parallelize the sequential iterations in FOO (i.e., to execute several sequential iterations simultaneously), it will be able to lead to a noticeable improvement in its optimization efficiency since fewer _sequential_ iterations will be required for convergence. Unfortunately, there is an inherent iterative dependency in standard FOO, that is, the output of each iteration \(t\) (e.g., \(_{t}\)) is the input of the next iteration \(t+1\). Such an iterative and sequential process makes it nearly impossible to attain \(_{t}\) and \(_{t+1}\) concurrently, and therefore parallelize the iterations for established FOO algorithms.

## 4 The OptEx Framework

To this end, we introduce the first general framework in Algo. 1 with a detailed illustration in Fig. 1, namely _first-order optimization expedited with approximately parallelized iterations_ (OptEx), to overcome the aforementioned inherent iterative dependency in FOO and facilitate the realization of parallelized iterations therein. To achieve this, we first propose a kernelized gradient estimation with the technique of separable kernel function and local history of gradient to efficiently and effectively estimate the gradient at any input in the domain (Sec. 4.1). We then follow standard FOO algorithms with this kernelized gradient estimation to approximate the inputs for the next \(N\) sequential iterations to be parallelized (Sec.4.2), aiming to overcome the inherent iterative dependency in FOO. Lastly, we finish our approximately parallelized iterations by leveraging parallel computing to run standard FOO algorithms on these \(N\) inputs concurrently using the ground-truth gradients (Sec. 4.3).

### Kernelized Gradient Estimation

As mentioned in our Sec. 3, \( F\) is assumed to be sampled from a Gaussian process, i.e., \( F(,(,))\) with kernel function \(\). Then, for every sequential iteration \(t\) of Algo. 1, conditioned on the history of gradients during optimization \(\{(_{}, f(_{})\}_{=1}^{N(t-1)}\)1, \( F\) then follows the posterior Gaussian process: \( F(_{t}(),_{t}^{2}(,))\) with the mean function \(_{t}()\) and the covariance function \(_{t}^{2}(,)\) defined as below :

\[_{t}() _{t}^{}()( _{t}+^{2})^{-1}(_{t}^{ })\;,\] (3) \[_{t}^{2}(,^ {}) (,^ {})-_{t}^{}()(_{t} +^{2})^{-1}_{t}(^{ })\]

where \(()\) vectorizes a matrix into a column vector, \(_{t}[ f(_{})]_{ =1}^{N(t-1)}\) is a \(d N(t-1)\)-dimensional matrix, \(_{t}^{}()[( ,_{})]_{=1}^{N(t-1)}\) is a \(d N(t-1)d\)-dimensional matrices, and \(_{t}[(_{}, _{^{}})]_{,^{}=1}^{N(t-1)}\) is a \(N(t-1)d N(t-1)d\)-dimensional matrices. We therefore propose to use \(_{t}()\) to estimate the gradient at _any_ input \(^{d}\), that is,

\[ F()_{t}()\;,\] (4)

and covariance \(^{2}()^{2}( ,)\) to measure the quality of this gradient estimation in a principled way, which will be further theoretically supported in our Sec. 5.1.

However, for every sequential iteration \(t\) of Algo. 1 with (3), it will incur a computational complexity of \((N^{3}(t-1)^{3}d^{3})\), along with a space complexity of \((N(t-1)d)\). Practically, this presents a significant challenge in scenarios with a large input dimension \(d\) or requiring a substantial number \(T\) of sequential iterations for convergence, such as in neural network training . To mitigate these complexity issues, we introduce two techniques: the separable kernel function and the local history of gradients, to reduce both the computational and space complexities associated with our kernelized gradient estimation, thereby enhancing its efficiency and practical applicability.

Separable Kernel Function.Let \((,)=k(,)\,\) where \(k(,)\) produces a scalar value and \(\) is a \(d d\) identity matrix, and define the \(N(t-1)\)-dimensional vector \(_{t}^{}()[k(,_{})]_{=1}^{N(t-1)}\), and \(N(t-1) N(t-1)\)-dimensional matrix \(_{t}[k(_{},_{^{}})]_{=^{}=1}^{N(t-1)}\), we can prove that the Gaussian process in (3) can be simplified as the Gaussian process in Prop. 4.1 (line 3 of Algo. 1).

**Proposition 4.1**.: _Let \((,)=k(,)\,\), the posterior mean and covariance in (3) become_

\[_{t}() =[(_{t}^{}()( _{t}+^{2})^{-1})_{t}]^{ }\;,\] \[_{t}^{2}(,^ {}) =(k(,^{})-_{t}^{}()(_{t}+^{2} )^{-1}_{t}(^{}))\;.\]Its proof is in Appx. A.1. Prop. 4.1 shows that with a separable kernel function \((,)=k(,)\,\), the multi-output Gaussian process in a \(d\)-dimensional space can be effectively decoupled into \(d\) independent single-output Gaussian processes. Each of these processes results from the same scalar kernel function \(k\), leading to a uniform posterior form shared by all these processes, i.e., the expression \(_{t}^{}()(_{t}+^{2}) ^{-1}\) in \(_{t}()\) and \(k(,^{})-_{t}^{}()( _{t}+^{2})^{-1}_{t}(^{})\) in \(_{t}^{2}(,^{})\). This thus considerably diminishes the computational complexity, now quantified as \((N^{3}(t-1)^{3}+N(t-1)d)\), resulting in a more computationally efficient gradient estimation in practice.

Local History of Gradients.Conventional FOO algorithms predominantly operate by optimizing within a localized region neighboring the initial input \(_{0}\). This therefore indicates that our Algo. 1 only requires precise gradient estimation within a local region. In this context, the use of a local gradient history is posited as sufficiently informative for effective kernelized gradient estimation, which can be supported by the theoretical results in  and the empirical evidence in our Sec. 6. As a result, rather than relying on a complete gradient history, we propose to use a localized gradient history of size \(T_{0}\) that neighbors \(\) to estimate the gradient at \(\). This strategic modification results in a substantial reduction of computational complexity to \((T_{0}^{3}+T_{0}d)\) as well as a corresponding decrease in space complexity to \((T_{0}d)\), which is especially beneficial in the situations where \(T_{0}\) is considerably smaller than \(N(t-1)\) for \(t[T]\).

### Multi-Step Proxy Updates

The ability of our kernelized gradient estimation to provide gradient estimation at any input \(\) then enables the application of a multi-step gradient estimation. This helps to approximate the inputs for the next \(N\) sequential iterations \(\{_{+i}\}_{i=0}^{N-1}\) to be parallelized in standard FOO, given \(\). Specifically, in the context of our Algo. 1, for every sequential iteration \(t[T]\), by employing a first-order optimizer (FO-OPT), we can approximate the inputs required by our parallelized iteration in Sec. 4.3_sequentially_ as below through our multi-step proxy updates (line 4-5 of Algo. 1).

\[_{t,s}=(_{t,s-1},_{{}_{t}(_{{}_{ ,s-1}})}),\; s[N-1]\;.\] (5)

Intuitively, these proxy updates imitate the sequential iterations in standard FOO by using only the estimated gradients in our Sec. 4.1. We will show that these proxy updates can provide a reasonably good approximation of the ground-truth updates in Sec. 5.1. Meanwhile, despite the iterative and sequential nature of (5), our proxy updates based on operations on relatively small-sized matrices (refer to the Prop. 4.1) will still be able to provide significantly enhanced computational efficiency compared to the ground-truth updates based on expensive evaluation of function values and gradients in complex tasks, like neural network training. This effectiveness and efficiency of (5) thus render it an essential foundation for achieving parallelized iterations and improved the optimization efficiency in FOO.

### Approximately Parallelized Iterations

Upon obtaining the inputs \(\{_{t,s-1}\}_{s=1}^{N}\) for the next \(N\) sequential iterations to be parallelized, we then finish our approximately parallelized iteration by executing standard FOO algorithms over each of \(\{_{t,s-1}\}_{s=1}^{N}\) based on the ground-truth gradients \(\{ f(_{t,s-1})\}_{s=1}^{N}\)_in parallel_ (line 6-9 of Algo. 1, see also the processes in Fig. 1). That is,

\[_{t}^{(i)}=(_{t,i-1}, f(_{ t,i-1})),\; i[N]\;.\] (6)

After that, the final input \(_{t}=_{t}^{(N)}\) will be used in the next sequential iteration (line 10 of Algo. 1). Of note, central to the approximately parallelized iterations in our OptEx framework is the necessity of evaluating the gradients \(\{ f(_{t,s-1})\}_{s=1}^{N}\) in our Algo. 1. These evaluations in fact play pivotal roles in reducing the estimation error of our kernelized gradient estimation and consequently improving the performance of our OptEx by augmenting the gradient history near the input \(_{t}\) with \(N\) more evaluations, which will be supported by the theoretical results in our Sec. 5 and the empirical evidence in our Appx. B.3.

## 5 Theoretical Results

To begin with, we formally present the assumptions mentioned in our Sec. 3 as below.

**Assumption 1**.: \( f()- F()\) follows \((,^{2})\) for any \(^{d}\).

**Assumption 2**.: \( F\) is sampled from a Gaussian process \((,(,))\) where \((,)=k(,)\,\) and \(|k(,)|\) for any \(^{d}\).

Note that the Assump. 1 has already been widely employed in the literature . Meanwhile, it is also common to assume that \(F\) is sampled from a Gaussian process , implying that \( F\) follows a Gaussian process as well  (Assump. 2), i.e., \( F\) can be any function in this prior. The inclusion of a separable kernel function in Assump. 2 aims to enhance the efficiency of our kernelized gradient estimation in Sec. 4.1 and simplify our theoretical analyses below, whereas our conclusions apply to non-separable kernel functions as well by following our proof techniques.

### Gradient Estimation Analysis

Following the principled idea in kernelized bandit  and Bayesian Optimization , we define the maximal information gain as below

\[_{n}_{\{_{j}\}_{j=1}^{n}^{d} }I((_{n});(_{n}))\] (7)

where \(I((_{n});(_{n}))\) is the mutual information between \(_{n}[ f(_{i})]_{i=1}^{n}\) and \(_{n}[ F(_{i})]_{i=1}^{n}\). In essence, \(_{n}\) encapsulates the maximum amount of information about \( F\) that can be gleaned from observing any set of \(n\) evaluated gradients, represented as \(_{n}\), which is known to be problem dependent measure that is highly related to the kernel function \(k(,)\). Built on this notation, we then provide the following theoretical result for our gradient estimation.

**Theorem 1** (Gradient Estimation Error).: _Let \((0,1)\) and \( d+(+1)(1/)\). Given Assump. 1 and 2, let \(||=T_{0}-1\) for any sequential iteration \(t\) in Algo. 1, then for any \(^{d},t>0\), with a probability of at least \(1-\),_

\[\| F()-_{t}()\|^{2}()\|}\;\;\;\;)^{T_{0}-1}}\|^{2}() \|\}_{T_{0}}}{T_{0}d}\;.\]

The proof is in Appx. A.2. It is important to note that since FOO pertains to local optimization, the global fulfillment of Assump. 2 is not a prerequisite. That is, the assumption that \( F\) is sampled from \((,)\) within a local region will already be sufficient for our kernelized gradient estimation in Sec. 4.1 to achieve accurate gradient estimation in practice. Our Sec. 6 will later evidence this empirically. Thm. 1 with upper bound on \(\|^{2}()\|\) illustrates that the efficacy of our kernelized gradient estimation in the worst case will enjoy a polynomial error rate of \((}/T_{0}})\). This means that if \(_{T_{0}}/T_{0}\) will asymptotically approach zero w.r.t. \(T_{0}\), the error of our kernelized gradient estimation method will become significantly small given a large number of evaluated gradients \(T_{0}\). This consequently facilitates the effectiveness of our proxy updates in (5) built on our kernelized gradient estimation to approximate the ground-truth updates when \(||\) is sufficiently large. Meanwhile, Thm. 1 with lower bound on \(\|^{2}()\|\) illustrates that our kernelized gradient estimation in the best case may achieve an exponential error rate of \((/(+1/^{2})^{T_{0}-1})\), which thus further elaborates the efficacy of kernelized gradient estimation in Sec. 4.1 and proxy updates in Sec. 4.2.

It is important to note that the ratio \(_{T_{0}}/T_{0}\) has been demonstrated to asymptotically approach zero for a range of kernel functions, as evidenced in existing literature . This therefore underpins the establishment of concrete error bounds for our kernelized gradient estimation where notation \(}\) is applied to hide the logarithmic factors, delineated as follows:

**Corollary 1** (Concrete Error Bounds).: _Let \(k(,)\) be the radial basis function (RBF) kernel, then_

\[\| F()-_{t}()\|=} (T_{0}^{-1/2})\;.\]

_Let \(k(,)\) be the Matern kernel where \(\) is the smoothness parameter, then_

\[\| F()-_{t}()\|=} (T_{0}^{-/(2+d(d+1))})\;.\]Cor. 1 elucidates that with kernel functions such as RBF and Matern kernel, the error in our kernelized gradient estimation indeed will diminish asymptotically w.r.t. \(T_{0}\). That is, as \(T_{0}\) increases, the estimation error \(\| F()-_{t}()\|\) decreases and consequently the proxy updates in (5) become closer to the ground-truth updates. It is important to note that this reduction typically follows a non-linear trajectory, suggesting that the effect of an increasing \(T_{0}\) on our kernelized gradient estimation diminishes when \(T_{0}\) is reasonably large. This consequently affirms the reasonability of our utility of local history for gradient estimation in Sec. 4.1, which leads to not only accurate but also efficient gradient estimations.

### Iteration Complexity Analysis

We first introduce Assump. 3, which has been widely applied in stochastic optimization [16; 36], to underpin the analysis of sequential iteration complexity of our OptEx framework.

**Assumption 3**.: \(F\) is \(L\)-Lipschitz smooth: \(\| F()- F(^{})\| L \|-^{}\|\) for any \(,^{}^{d}\).

To simplify the analysis, we primarily prove the sequential iteration complexity of our SGD-based OptEx where we use \(_{[NTY]}\| F(_{})\|^{2}\) to denote the minimal gradient norm we can achieve within the whole optimization process when applying our OptEx with \(T\) sequential iterations and parallelism of \(N\) for a clear and fair comparison with standard SGD. Notably, our analysis can also be extended to other FOO-based OptEx by following similar proof idea.

**Theorem 2** (Upper Bound).: _Let \((0,1)\), \( F()-_{}F()\), \(\{,^{2}\}\) and \((1-)}}{^{2}T_{0}}+ \). Under Assump. 1-3, by choosing \(T}\), \(=}}\) and \(||=T_{0}-1\) for our SGD-based Algo. 1, with a probability of at least \(1-\),_

\[_{t T,s N}\| F(_{t,s})\|^{2} 2 }+\.\] (8)

The proof of Thm. 2 is in Appx. A.3. Of note, our Thm. 2 with \(N=1\) aligns with the established upper bound for standard SGD, as discussed in . Importantly, our Thm. 2 elucidates that with parallelism \(N>1\), our SGD-based OptEx algorithm can expedite the standard SGD by a factor of at least \(\), where \(1/\) quantifies the impact of the error introduced by our kernelized gradient estimation. This efficiency gain can be further amplified as the accuracy of our kernelized gradient estimation increases (i.e., a decrease in \(\)), which can be achieved by augmenting the number \(T_{0}\) as discussed in our Sec. 5.1. In addition, Thm. 2 also demonstrates that for a fixed learning rate \(\), there exists a constant \(N_{}\), e.g., \(N_{}=2/(^{2}LT^{2})\) in Thm. 2, the parallelism \(N\) should roughly remain below to ensure the fastest convergence of function \(F\) to a stationary point. In contrast, if \(N\) exceeds \(N_{}\), our SGD-based OptEx will underperform due to the increased gradient estimation error. This observation is further supported by the results presented in Appx. B.3. However, when the learning rate \(\) is relatively small (e.g., during fine-tuning in practice), the parallelism \(N\) can be significantly larger to achieve a further improved speedup.

**Theorem 3** (Lower Bound).: _Let \((0,1)\), \( F()-_{}F()\), \(\{,^{2}\}\), and \(\{/(+1/^{2})^{T_{0}-1},^ {2}\}\). Then, for any \(L>0,>0,N 1,T 1\) and \([0,1/L)\), there exists a \(F\) on \(^{d}\)\(( d>d_{0}=(/( L^{2}) NT/ ))\) satisfying Assump. 1-3 and having the following with a probability of at least \(1-\) when applying SGD-based Algo. 1 with \(||=T_{0}-1\),_

\[_{t T,s N}\| F(_{t,s})\|^{2}\{ L,,1\}}{4}\.\] (9)

The proof of Thm. 3 is in Appx. A.4. Note that when \(N=1\), Thm. 3 aligns with the recognized lower bound for SGD, as elucidated in . Thm. 3 illustrates that, with parallelism of \(N\), our SGD-based OptEx can potentially accelerate standard SGD by up to \(/(/(^{2}(1+1/^{2})^{T_{0}-1}))\), under the condition that \(/(1+1/^{2})^{T_{0}-1}\{ L,1,^{2}\}\). This upper limit in fact corresponds with the lower bound of the variance in our kernelized gradient estimation, as established in Thm. 1. Essentially, the agreement between Thm. 2 and Thm. 3, in the aspect of parallelism \(N\), demonstrates the tightness of our sequential complexity analysis for SGD-based Algo. 1. Finally, the combination of Thm. 2 and Thm. 3 enables us to specify the effective acceleration that can be achieved by our SGD-based OptEx tightly, as shown in our Cor. 2 below.

**Corollary 2** (Acceleration Rate).: _With parallelism of \(N\), the effective acceleration rate achieved by our SGD-based OptEx over standard SGD is \(()\)._

## 6 Experiments

In this section, we use extensive experiments to show that our OptEx framework can considerably enhance the efficiency of FOO with parallel computing, including synthetic experiments (Sec. 6.1), reinforcement learning (Sec. 6.2) and neural network training on various datasets (Sec. 6.3).

### Synthetic Function Minimization

Here, we utilize synthetic functions to demonstrate the enhanced performance of our OptEx framework compared to existing baselines, including the standard FOO algorithm, namely Vanilla, and FOO with ideally parallelized iterations, namely Target, which ideally but impractically utilizes the ground-truth gradient to obtain the inputs for the iterations to be parallelized. More specifically, the Vanilla baseline is equivalent to Algo. 1 with parallelism of \(N=1\), and the Target baseline is equivalent to Algo. 1 with \(_{t}(_{t,s-1})\) being replaced with \( f(_{t,s-1})\), indicating the desired parallelized iteration we aim to approximate. We have also provided a comprehensive illustration of these baselines in Appx. B.1 and detailed experimental setup applied here in Appx. B.2.1.

The results in Fig. 2 with \(^{2}=0\) and \(N=5\) have demonstrated the efficacy of our OptEx framework for deterministic optimization (i.e., \(^{2}=0\)). Specifically, Fig. 2 shows that OptEx consistently achieves a notable speedup in optimization efficiency measured by the number of sequential iterations, which is at least 2\(\) more efficient than the Vanilla baseline, when optimizing with parallelism of \(N=5\) to reach an equivalent level of optimality gap. This is roughly in line with the result of our Cor. 2, implying the validity of our Cor. 2. Meanwhile, although our OptEx framework slightly underperforms the Target baseline, such a phenomenon is in fact quite reasonable since the Target baseline can leverage the ground-truth gradient whereas OptEx relies on the kernelized gradient estimation with estimation error bounded in Thm. 1 to parallelize sequential iterations. This also aligns with the insight from our iteration complexity analysis in Thm. 2. Overall, the results in Fig. 2 have provided strong empirical support for the efficacy of our OptEx in expediting FOO, as theoretically justified in our Sec. 5.2. We also present a number of ablation studies as well as analyses in Appx. B.3 to examine the effects of different components in our proposed OptEx framework on its effectiveness.

### Reinforcement Learning

We proceed to compare our OptEx framework with previously established baselines under various reinforcement learning tasks with different parameter dimension \(d\) from the OpenAI Gym suite , with the deployment of DQN agents . Here, the parallelism parameter is set to be \(N=4\) and a detailed experimental setup is provided in Appx. B.2.2. The results are presented in Fig. 3. As illustrated in Fig. 3, the integration of parallel computing techniques, including Target and OptEx, considerably outperforms the traditional Vanilla baseline in terms of the optimization efficiency quantified by the number of sequential iterations. More importantly, amongst these methodologies, OptEx consistently demonstrates a more stable and superior improvement on the optimization effi

Figure 2: Comparison of the number of sequential iterations \(T\) (\(x\)-axis) required by different methods to achieve the same optimality gap \(F()-_{}F()\) (\(y\)-axis) for various synthetic functions. The parallelism \(N\) is set to 5 and each curve denotes the mean from 5 independent runs.

ciency compared with other baselines, which consequently well corroborates the efficacy of OptEx in improving the efficiency of established FOO algorithms. Interestingly, our OptEx framework can even enjoy an improved efficiency over the Target baseline where the ground-truth gradient \( f()\) is applied. This is likely because the gradient variance (i.e., \(\|^{2}()\|\)) in our OptEx framework can asymptotically approach zero by using a large number of history of gradient (refer to our Sec. 4.1), whereas the gradient variance in the Target baseline remains the same.

### Neural Network Training

At last, we examine the efficacy of our OptEx in expediting the optimization (i.e., training) of deep neural networks, specifically for image classification and text autoregression tasks. Specifically, we apply our OptEx and the aforementioned baselines to (a) train a 10-layer MLP model (\(d=2412298\)) with residual connections  on CIFAR-10 , and (b) train an autoregressive transformer model (\(d=1626496\)) borrowed from the Haiku library  on a curated collection of works from Shakespeare with parallelism of \(N=4\). Comprehensive details for the experimental setup are provided in Appx. B.2.3 and the final results are illustrated in Fig. 4 where both the number of sequential iterations and wallclock time are used to quantify the optimization efficiency of different optimizers. Intriguingly, as evidenced by Fig. 4, OptEx consistently outperforms Vanilla by a large margin in terms of both training and testing errors across the image and text datasets, given an equal number of sequential iterations \(T\) or alternatively the same wallclock time budget. Remarkably, the efficiency of OptEx approaches that of the theoretically ideal algorithm - the Target baseline, which therefore further verifies the efficacy of our OptEx framework. More results are in Appx. B.3. Overall, these empirical results have well verified the capability of OptEx in significantly expediting FOO algorithms as justified by our theorems in Sec. 5, even in the context of deep neural network training.

## 7 Conclusion

In conclusion, our OptEx framework represents a significant advancement in FOO. By leveraging kernelized gradient estimation to enable approximately parallelized iterations, OptEx effectively re

Figure 4: Comparison of the test error or training loss (\(y\)-axis) achieved by different optimizers when training deep neural networks on (a) CIFAR-10 and (b) Shakespeare Corpus with a varying number \(T\) of sequential iterations or a varying wallclock time (\(x\)-axis). The parallelism \(N\) is set to 4 and each curve denotes the mean from 5 (for CIFAR-10) or 3 (for Shakespeare corpus) independent runs. The wallclock time is evaluated on a single NVIDIA RTX 4090 GPU.

Figure 3: Comparison of the cumulative average reward (\(y\)-axis) achieved by different methods to train DQN on RL tasks under various parameter dimension \(d\) and a varying number of sequential episodes \(T\) (\(x\)-axis). The parallelism \(N\) is set to 4 and each curve denotes the mean from 3 independent runs.

duces the number of sequential iterations required for convergence and thus addresses the traditional inefficiencies of FOO. Theoretical analyses and extensive empirical studies validate the reliability and efficacy of OptEx, confirming its potential to expedite optimization processes across various applications. Of note, a limitation of OptEx is the additional storage and computational cost introduced by the kernelized gradient estimation, which we aim to mitigate further in the future work.