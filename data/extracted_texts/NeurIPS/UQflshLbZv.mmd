# HairDiffusion: Vivid Multi-Colored

Hair Editing via Latent Diffusion

 Yu Zeng\({}^{1}\), Yang Zhang\({}^{1}\)1, Jiachen Liu\({}^{1}\), Linlin Shen\({}^{1,2,3}\)

**Kaijun Deng\({}^{1}\)**, **Weizhao He\({}^{1}\)**, **Jinbao Wang\({}^{3,4}\)**

\({}^{1}\)Computer Vision Institute, School of Computer Science & Software Engineering, Shenzhen University

\({}^{2}\)Shenzhen Institute of Artificial Intelligence and Robotics for Society

\({}^{3}\)National Engineering Laboratory for Big Data System Computing Technology, Shenzhen University

\({}^{4}\)Guangdong Provincial Key Laboratory of Intelligent Information Processing

{cengyu,liujiachen,dengkaijun}2023@email.szu.cn, heweizhao2022@email.szu.edu.cn

{yangzhang,llshen,wangjb}@szu.edu.cn

###### Abstract

Hair editing is a critical image synthesis task that aims to edit hair color and hairstyle using text descriptions or reference images, while preserving irrelevant attributes (e.g., identity, background, cloth). Many existing methods are based on StyleGAN to address this task. However, due to the limited spatial distribution of StyleGAN, it struggles with multiple hair color editing and facial preservation. Considering the advancements in diffusion models, we utilize Latent Diffusion Models (LDMs) for hairstyle editing. Our approach introduces Multi-stage Hairstyle Blend (MHB), effectively separating control of hair color and hairstyle in diffusion latent space. Additionally, we train a warping module to align the hair color with the target region. To further enhance multi-color hairstyle editing, we fine-tuned a CLIP model using a multi-color hairstyle dataset. Our method not only tackles the complexity of multi-color hairstyles but also addresses the challenge of preserving original colors during diffusion editing. Extensive experiments showcase the superiority of our method in editing multi-color hairstyles while preserving facial attributes given textual descriptions and reference images.

## 1 Introduction

Hair editing is one of the most challenging and interesting facets of semantic face editing. The essence of this task is to edit hair attributes such as color, and hairstyle from the reference image or

Figure 1: Our framework supports individual or collaborative editing of hairstyle and color, utilizing text, reference images, and stroke maps. With exceptional performance, particularly evident in editing multiple hair colors.

text to the input image while preserving irrelevant attributes. Disentangling these attributes is the key to a quality solution to the problem. This task has many applications among professionals and amateurs during work with face editing programs, virtual reality, and computer games .

In recent years, numerous methods leveraging Generative Adversarial Networks (GANs)  have emerged in this field. These methods often involve mapping images into StyleGAN  latent space to disentangle hair features or utilizing Contrastive Language-Image Pre-Training (CLIP) model  to translate textual descriptions into relevant semantic vectors, facilitating manipulation via text or reference images.

However, previous methods have overlooked the **hair color structure**. As shown in Figure 1, "ombre hair" (the last column of the second row) means a hair color structure with a gradient transition from top-to-bottom, while "split color hair" (the last column of the first row) exhibits a hair color structure with a left-to-right transition. In traditional StyleGAN-based approaches , the scarcity of multi-color hair in the facial datasets  used to train StyleGAN , coupled with the limited distribution of latent space in StyleGAN , gives rise to two primary challenges: 1) difficulty in generating intricate hair color and hairstyle due to the insufficient diversity in the training data's multi-color hair distributions; 2) challenges in preserving the original facial information when editing the latent code after mapping images into latent space, leading to difficulties in editing images while preserving irrelevant attributes.

In recent years, with the advancement of diffusion models , their robust and stable generative capabilities have surpassed those of GANs in many aspects . In particular, Latent Diffusion Models (LDMs)  have demonstrated exceptional generative capabilities, notably in image inpainting tasks. However, the application of diffusion models to hair editing encounters three challenges: 1) lack of tailored masks for hairstyle inpainting, necessitating consideration of hairstyle regions while preserving irrelevant attributes; 2) difficulty in providing sufficient control for the hair editing task, which requires faithful transfer of hair color from another image or retaining the original hair color of the image; 3) limitations in text and semantic understanding related to hair color and hairstyle, hinder the precision of CLIP-guided diffusion processes.

To address these limitations, we propose a novel baseline approach based on LDMs, enabling the automatic generation of edited regions while allowing separate control of hair color and hairstyle via text and reference images individually or collaboratively. Our pipeline can be roughly divided into two stages. The first stage begins with obtaining hair-agnostic masks to acquire facial representations independent of hair. These representations are combined with facial keypoint information and processed through textual descriptions to perform the LDMs process, generating a style proxy (i.e., an image used to guide the hairstyle transfer process). The second stage involves transferring hair color using a reference image. Initially, the hair of the reference image is aligned with the source image using a pre-trained warping module. The aligned hair serves as a color proxy (i.e., an image used to guide the color and color structure transfer process), which is then input into the LDMs process to transfer or retain the hair color. Simultaneously, the Canny edge from the original hair or style proxy is employed to retain or edit the hairstyle. The key to the pipeline is to use different hair-agnostic masks for both the color proxy and style proxy at different stages and blend them in the diffusion latent space by Multi-stage Hairstyle Blend (MHB). By training the warping model to obtain aligned hair color prior information in targeted hair regions, our method effectively addresses multi-hair color editing tasks. Quantitative and qualitative evaluations, along with user studies, demonstrate the efficacy of our approach in hair editing tasks, particularly in color transformation and preservation of unrelated attributes within the diffusion model framework. Furthermore, by designing different stages of agnostic masks, we can maximize the retention of hairstyle-independent features such as background information and facial features. In summary, our contributions are outlined as follows:

* We present a warping module designed for hair warping, allowing the alignment of the target hair mask with precision and enabling comprehensive hair color structure editing through reference images.
* The MHB method is proposed within LDMs, which enables the decoupling of hair color and hairstyle, thereby effectively achieving high-quality hair color and hairstyle editing.
* Through extensive qualitative and quantitative evaluations, we showcase the superior performance of our method in text-based hairstyle editing, reference image-based hair color editing, and preservation of facial attributes.

* The application of LDMs to address the challenge of text and image-based hair editing is pioneered through the introduction of hair-agnostic facial representation masks, reframing hair editing as an inpainting task and representing a novel approach. To the best of our knowledge, this method has not been previously explored in this domain.

## 2 Related Work

**Diffusion Models.** Diffusion models have established themselves as a robust class of generative models capable of producing high-quality images through a process of iteratively denoising data [6; 12; 31; 28]. Specifically, SD-Inpainting  is based on the large-scale pre-trained text-to-image model, i.e., Stable Diffusion . By utilizing random masks as repair masks and employing image-text prompts, diffusion models can fill areas with content consistent with textual descriptions while retaining context awareness in other regions. Despite the success of diffusion models in the inpainting task, their potential has not been fully exploited yet. For instance, describing fine details like hair color directly through text remains challenging. Some methods combine different interaction strategies to achieve satisfactory inpainting results, such as ControlNet-Inpainting  which utilizes ControlNet to guide image restoration based on additional inputs like Canny edge, depth maps, etc. Additionally, some methods provide an additional reference image or involve drawing rough color strokes. In the hair editing task, it is essential to develop a framework that facilitates multimodal conditions for image inpainting. Therefore, we leverage ControlNet to control hairstyle attributes and stroke maps, managing color information and color structure.

**Hair Editing.** As an essential component of the face, there have been numerous works on hairstyle editing and synthesis [33; 44; 45; 19; 35; 36]. Some works decouple facial attributes by using image-level masks and then splice hairstyle images onto facial images through generative network [33; 30]. However, this approach often results in inconsistencies in lighting or artificial shadows along the edges of the hairstyle region. Some approaches utilize the e4e  to map the target image and hairstyle reference image into \(+\) latent space of StyleGAN , enabling manipulation of hairstyle and hair color via vector operations, followed by image synthesis using StyleGAN [35; 36; 26; 11]. HairCLIP  integrates CLIP  into hair-editing tasks by leveraging StyleGAN to decouple hair color and hairstyle features, enabling text-driven. Additionally, some methods attempt to enhance the generative capabilities of StyleGAN through various transformations of its latent space [1; 46; 45]. Despite these advancements, challenges persist due to the limited data distribution inherent in StyleGAN , which is trained predominantly on facial datasets [16; 23]. This constraint leads to difficulties in preserving specialized facial features like earrings and eyeshadow, as well as in introducing novel attributes such as multi-colored hairstyles. To tackle the constraints of hair color diversity, this paper introduces stroke maps  into the denoising process of diffusion models as the color proxy prior. Utilizing a CLIP text encoder and image encoder fine-tuned with images matching the hair color to achieve editing of hair color structure through text. For reference images, we finetune the model with textual inversion.

**Image Deformation.** Image deformation, or image warping, is a technique in computer vision and graphics for manipulating and transforming images [7; 32; 22]. The goal is to deform an input image while preserving its essential content and structure. In recent years, deep learning techniques have revolutionized the field of image deformation, particularly GANs , have revolutionized the field of image deformation [7; 32; 9]. GANs are employed to learn complex deformation mappings directly from data, capturing intricate image transformations and generating realistic deformations with minimal artifacts. Conditional GANs (cGANs)  represent an evolution in image deformation by incorporating additional contextual information during deformation. These networks learn to map input images to desired target outputs based on specific conditions or auxiliary information . For example, integrating DensePose , detailing pixel-level semantic information enables precise and controlled image warping. In this paper, we employ multiple conditioning inputs to train a warping network capable of aligning hair color while accounting for the relationship between hair and facial features.

## 3 Method

As shown in Figure 2, we propose a novel solution based on LDMs for the first time in this field. In particular, our work employs the Stable Diffusion architecture as a starting point for performingthe hairstyle editing task. We obtain the style proxy, denoted as \(P^{s}\), to capture the hairstyle prior as shown in Figure 2 (a). Concurrently, we acquire the color proxy \(P^{c}\), using a warping module as shown in Figure 2 (c). Finally, we incorporate these two proxies into the diffusion process through **Multi-stage Hairyble Blend (MHB)** to blend them effectively as shown in Figure 2 (d). Therefore, our framework supports individual or collaborative editing of hairstyle and single-color or multi-color, utilizing text, and reference images.

### Preliminaries

**Stable Diffusion.** It consists of an autoencoder encoder \(\) and an autoencoder decoder \(\), a text-conditional U-Net denoising model \(_{}\), a CLIP text encoder \(T_{E}\), which takes text \(T\) as input. The encoder \(\) compresses an image \(I\) into the latent space of diffusion, while the decoder \(\) performs the inverse operation and decodes a latent variable into the pixel space. For clarity, we refer to the \(_{}\) convolutional input as the spatial input \(\) since convolutions preserve the spatial structure, and to the attention conditioning input as \(\). The training of the denoising network \(_{}\) is performed by minimizing the following loss function:

\[L=_{(I),,(0,1),t}[ \|-_{}(,)\|_{2}^{2}],\] (1)

where \(t\) represents the diffusing time step, \(=z_{t}\), \(z_{t}\) is the encoded image \((I)\) where we stochastically add Gaussian noise \((0,1)\), and \(=[t;T_{E}(T)]\). Our goal is to generate a new image \(I_{}\), editing the hair based on user-provided reference images or textual descriptions of hairstyle or hair color, while preserving the unrelated features of the facial region outside the hair. This task can be viewed as a special kind of inpainting, specifically replacing the hair information in a face image based on user-provided conditions. Therefore, we use the Stable Diffusion inpainting pipeline as the starting point of our approach. It takes as spatial input \(\) the channel-wise concatenation of an encoded masked image \((I_{})\), a resized binary inpainting mask \(m\{0,1\}^{1 h w}\), and the denoising network input \(z_{t}\). \(I_{}\) is the input image \(I_{}\) masked according to the inpainting mask

Figure 2: Overview of HairDiffusion: (a) Using a hairstyle description \(T^{}\) or reference image \(I_{}^{}\) as conditional input, coupled with the hair-agnostic mask \(M_{}\) and source image \(I_{}\), we can get the style proxy \(P^{s}\). (b) Leveraging the color proxy and style proxy, along with the hair-agnostic mask \(M_{}\) and source image \(I_{}\), enables individual or collaborative editing of hair color and hairstyle. (c) Given a series of conditions driven from the input image \(I_{}\), the hair color reference image \(I_{}^{}\) is used to obtain the color proxy \(P^{c}\) through a warping module. In the case of changing only the hairstyle while preserving the original hair color, \(I_{}^{}=I_{}\). (d) The color proxy \(P^{c}\) and the style proxy \(P^{s}\) are blended at different stages of the diffusion process.

\(M_{ c}\{0,1\}^{1 H W}\), and the binary inpainting mask \(m\) is the resized version according to the latent space spatial dimension of the original inpainting mask \(M_{ c}\). The spatial input of the inpainting denoising network is \(=[z_{t};m;(I_{ M})]^{(4+1+4) h w}\).

**ControlNet.** It is an extension of the diffusion model that incorporates conditional control to generate high-quality images with specific attributes. The main objective is to leverage additional conditional information \(c\) (e.g., Canny edge, Openpose, Depth Map) to steer the image generation process. Combined with spatial input \(\) (e.g., \(z_{t}\)), the diffusion process can be expressed as:

\[p_{}(z_{t-1}|z_{t},c)=(z_{t-1};_{}(z_{t},t,c),_ {}(z_{t},t,c)),\] (2)

Both \(_{}\) and \(_{}\) depend on the conditional information \(c\). We introduce the pose image \(I_{}\), obtained from \(I_{}\) using a pose keypoint extraction model , during the hairstyle editing stage to align the generated hair with the face in \(P^{s}\). In the hair color editing stage, we use an edge detection model to obtain \(I_{ c}\) from \(P^{s}\) or \(I_{}\) to guide the generation of the hairstyle.

**CLIP.** It is a vision-language model , which aligns visual and textual inputs in a shared embedding space. CLIP consists of an image encoder \(V_{E}\) and a text encoder \(T_{E}\) that extract feature representations \(V_{E}(I)^{d}\) and \(T_{E}(E_{L}(T))^{d}\) for an input image \(I\) and its corresponding text caption \(T\), respectively. Here, \(d\) is the size of the CLIP embedding space, and \(E_{L}\) is the embedding lookup layer which maps each \(T\) tokenized word to the token embedding space \(\). To tackle the text-multi-color hair editing task, we utilize hair color structure text \(T_{ m}\) in comparison with the multi-color hairstyle image \(I_{ m}\), scraped from the internet to fine-tune the CLIP. The fine-tuning process involves aligning the texts with the corresponding images. To enable the model to learn the color structure of hair, we perform data augmentation through rotation and symmetry operations, introducing a variety of directional patterns. The fine-tuning objective is given by: \(_{}_{(T_{ m},I_{ m})_{}}[ (T_{ m},I_{ m};)]\) where the augmented dataset \(_{}\) is defined as: \(_{}=\{(_{i})_{i} \}\), \(\) represents the parameters of the CLIP text encoder \(T_{E}\).

For reference image-based hairstyle editing, we employ textual inversion. Initially, we construct a textual prompt \(q\) that guides the diffusion process. This prompt is tokenized and each token is mapped into the token embedding space using the CLIP embedding lookup module, resulting in \(V^{*}\). Next, we encode the reference image \(I_{ r}^{ s}\) using the CLIP visual encoder \(V_{E}\), feeding the features extracted from the last hidden layer into the textual inversion adapter \(F_{}\). This adapter maps the input visual features to the CLIP token embedding space \(\). The final prompt embedding vectors, combined with the predicted pseudo-word token embeddings, are formulated as follows:

\[E=(V^{*},F_{}(V_{E}(I_{ r}^{ s}))).\] (3)

The concatenated embedding \(E\) is then fed into the CLIP text encoder \(T_{E}\), and the output is used to condition the denoising network by leveraging the existing Stable Diffusion textual cross-attention mechanism. We train \(F_{}\) with the input of a hair-agnostic masked face \((I_{M})\), compared with face-agnostic mask \(M_{ a}\), and the latent variable \(z\). During the training of the adapter \(F_{}\), the parameters of both CLIP and the U-net are kept frozen.

### HairDiffusion

**Data Preparation.** We define the hair-agnostic masks for inpainting in the two stages. In the hair editing stage, the \(M_{ a}\) retains facial information (excluding the forehead area) and neck information while removing other irrelevant details(background, hair, etc). In the color editing stage, the mask \(M_{ c}\) is used to remove the hair information, \(M_{ c}=M_{ h} M_{ p}\), \(M_{ h}\) denotes \(I_{}\) hair region mask, \(M_{ p}\) denotes \(P^{s}\) hair region mask. The detailed mask design is provided in the Appendix A.1.

**Style Proxy.** Given a style reference image \(I_{ r}^{ s}\) or a text prompt \(T^{ s}\) of hairstyle. Our goal is to obtain the style proxy \(P^{s}\) to inpaint the source image \(I_{}\) hair region with the desired hairstyle. We use the hair-agnostic mask \(M_{ a}\) to define the inpainting region of the hair. To ensure that the generated hairstyle aligns with the orientation of the face in \(I_{}\), we employ pose key points image \(I_{}\) driven from \(I_{}\) using a 3D key points extractor model \(E_{p}\) to guide the hairstyle inpainting process. The process can be formulated as:

\[P^{s}=(_{}((I_{}^{ s} M_{  a}),t,C((E_{p}(I_{}^{ s}),M_{ a})),),\] (4)

where \(\) represents the attention conditioning input \([T_{E}(T^{ s}),I_{E}(I_{ r}^{ s})]\), \(C\) represents the control input from ControlNet.

**Color Proxy.** Given a color reference image \(I_{}^{}\) or a text prompt \(T^{}\) of hair color. We use the warping module \(\) to align the \(I_{}^{}\) with \(I_{}\). The process to get the color proxy operates through the following process:

\[P^{c}=(I_{}^{},I_{}) M_{}^{}.\] (5)

Where \(M_{}^{}\) represents the hair region mask of \(I_{}\). The entire framework can be described by:

\[I_{}=(I_{},[M_{},M_{ }],[T^{} T^{} I_{}^{} I_{}^{}]),\] (6)

where [\(\)] denotes optional conditional inputs.

### Multi-Stage Hairstyle Blend

We employ MHB to achieve separate control of hairstyle and hair color. Given an input image \(I_{}\) and the corresponding inpainting mask \(M_{}\) for the hair region, we obtain the masked image \(I_{}\), removing the original hair information. We then encode the color proxy \(P^{c}\) and \(I_{}\) to the latent space of the diffusion model through an encoder \(\), obtaining the latent vectors \(z^{c}\) and \(z^{}\) respectively. We use the hyperparameter \(\) to divide the denoising process into two stages. In the initial stage, we spatially blend \(z^{c}\) with \(z^{}\) to obtain modified latent \(z_{t}^{}\) at a certain intermediate timestep \(\) during the sampling, the blending operation \(B\) is formulated as follows:

\[z_{t}^{}=B(z_{t}^{},z_{t}^{},t)=z_{t}^{ } m_{}+z_{t}^{}(1-m_{}),& t=\\ z_{t}^{},&,\] (7)

where \(z_{t}^{}\) is noised color latent at time \(t\) and \(m_{c}\) is down sampled from the inpainting mask \(M_{}\). Owing the blend is in the early sample stage, thereby utilizing \(P^{c}\) to guide hair color generation and avoiding artifacts at the hair boundaries caused by the latent mask mixing. To prevent affecting irrelevant features during denoising, blending \(z^{}\) ensures that parts outside \(m_{}\) remain unchanged. Alternatively, hair color can be edited using a textual description \(T^{}\) by guiding the denoising steps within the U-Net, with text-encoded information via CLIP's text encoder. In the latter stage, we leverage the context-aware capability of inpainting to unconditionally generate the parts of the face and background occluded by the hair. Throughout the entire denoising process, we incorporate \(P^{s}\) or \(I_{}\) via ControlNet to guide the hairstyle generation.

### Hair Color Aligning

To transform the \(I_{}^{}\) into color proxy \(P^{c}\) for hair color editing, we introduce a warping module inspired by the virtual try-on task [5; 22], which is being employed in the domain of hair editing for the first time to best of our knowledge. We adopted the network architecture of HR-VITON  incorporates DensPose \(D\) and hair-agnostic segmentation map \(S_{}\) driven from facial image \(I_{}\) as priors to enable warping module \(\) to account for facial poses. However, there are two main gaps between the two tasks. Virtual try-on datasets typically consist of two types of images: standalone clothing images and compared clothing images worn on models. These images are jointly used to train models so that they can transfer clothing to images of models. Existing facial datasets lack individual representations for hair. To reduce the gap, we first get the semantic segmentation \(M\) of facial image \(I_{}\) using a facial segmentation network, and the hair mask denotes \(M_{}\), while the hair-agnostic mask is represented as \(M_{}^{}\). Then, we obtain the hair representation \(I_{}=I_{} M_{}\) and hair-agnostic facial representation, defined as \(I_{}=I_{} M_{}^{}\). We adopt several data augmentation techniques, denoted as \(\), including flipping, rotation, trigonometric distortion, and scaling, on the hair representation image \(I_{}\) to break down the connection with the facial image \(I_{}\). Formally, the condition input for training the warping module is then given by:

\[((I_{}),S_{},D,I_{}) I _{}.\] (8)

This strategy enables the warping module to align the hair region of \(I_{}^{}\) to the hair region of the target hairstyle \(I_{}\), thereby obtaining \(I_{}\) as shown in Figure 2(c). When large warping or significant differences in hair length cause image tearing, gaps, or incomplete filling of the target hairstyle areas, we leverage PatchMatch  to sense surrounding pixels and seamlessly fill the missing hair color, ensuring natural restoration.

The second gap is to extract the color features while filtering out the high-frequency details of the original hair. To address this, we applied bilateral filtering to eliminate texture details from the hair,which effectively preserved the original hair color and color structure information intact. We denote the bilateral filtering operation as \(\), and its application to the image can be represented as:

\[P^{c}=_{}(I_{}),\] (9)

where \(\) represents the parameters of \(\).

## 4 Experiments

**Implementation Details.** We train and evaluate the warping module using a CelebAMask-HQ  dataset and obtain paired data corresponding to the hair region through segmentation and transformation processes. For the hairstyle text descriptions and hair color descriptions, we follow the HairCLIP methodology. Additionally, we incorporate additional hair color text descriptions for multi-color commonly used hair color scenarios. The CelebA-HQ  dataset is used to provide reference images for hairstyles. Additionally, as the dataset lacks multi-color hair, we supplement it with images sourced from the internet, obtaining multi-hair-color data with a total of 12 categories of hair color structure. Detailed data collection is provided in Appendix A.2. We set the batch size for the training warping module to 8 and trained the module for 100,000 iterations. The learning rates for both the generator and the discriminator in the warping module are set to 0.0002.

### Quantitative and Qualitative Comparison.

**Comparison with Text-Driven Hair Editing Methods.** Table 1 shows the quantitative results on IDS, PSNR, and SSIM with the leading text-driven hair editing methods on the CelebA-HQ  testset(2,000 images), following the evaluation setting of HairCLIPv2. For Diffusion-CLIP , we finetune a model for each text description. For the TediGAN , the number of optimization iterations is set to 200. We set the image generation size to 1024\(\)1024, consistent with the compared

Figure 3: Visual comparison with HairCLIPv2 , HairCLIP , TediGAN , PowerPaint(“ControlNet” version) , ControlNet-Inpainting , and DiffCLIP . The simplified text descriptions (editing hairstyle, hair color, or both of them) are listed on the leftmost side. Our approach demonstrates better editing effects and irrelevant attribute preservation (e.g., identity, background).

methods. Our method accomplishes maximizing the preservation of irrelevant attributes. Figure 3 shows the qualitative results in StyleGAN-based and advanced SD-inpainting methods. The three diffusion-based methods are based on Stable Diffusion v1.5 in 50 steps. As shown in Figure 3, our method accomplishes satisfactory hair editing effects with hair color structure text. In text editing hairstyle scenarios (lines 4 and 5), compared to the diffusion-based method, we can faithfully preserve the original hair color. The StyleGAN-based methods are unsatisfactory in the preservation of irrelevant attributes. Even though HairCLIPv2 performs well visually, it still struggles to preserve details, as shown in Figure 4.

**Comparison with Hair Color Transfer Methods.** Due to the existing facial datasets lack of multi-color data, we randomly selected 666 images from our collected multi-color dataset as color reference images. The first 666 images from the CelebA-HQ  test set as input images. As shown in Figure 5, our result excels at transferring color structures while remaining faithful to the hair color of the reference image. Although MichiGAN  demonstrate the ability to achieve multiple color structure, however, it results in poor hairstyle quality. On the other hand, HairCLIP, HairCLIPv2, HairFastGAN and Barbershop are constrained by the data distributions of StyleGAN, limiting their ability to generate novel colors within the latent space.

**User Study.** As shown in Table 2, our method outperformed other methods in terms of accuracy. In terms of preservation, our method was superior to traditional hair editing methods but slightly inferior to PowerPaint , which is specifically designed for inpainting using diffusion techniques. In the color transfer comparison, due to our hair color feature blending mechanism, when editing hair color via color proxies that significantly differ from the target image, the overall image color balance shifts, resulting in a preservation score lower than MichiGAN . Regarding naturalness, our method was slightly inferior to SYH , which performs optimization entirely in the latent space of StyleGAN. The detailed user study setting is provided in Appendix A.3. Visual comparison on cross-model is provided in Appendix A.4.

  Methods & IDS\(\) & PSNR\(\) & SSIM\(\) \\ 
**Ours** & **0.94** & **33.1** & **0.95** \\ HairCLIPv2  & 0.84 & 29.5 & 0.91 \\ HairCLIP  & 0.45 & 21.6 & 0.74 \\ TediGAN  & 0.16 & 22.5 & 0.74 \\ DiffCLIP  & 0.71 & 26.8 & 0.86 \\  

Table 1: Quantitative comparison for irrelevant attributes preservation. IDS  denotes identity similarity, PSNR, and SSIM are calculated at the intersected non-hair regions before and after editing.

   &  &  &  \\  Metrics & Ours &  &  &  &  &  &  & Ours &  &  &  &  &  &  & Ours &  &  \\  Accuracy & **42.9** & 21.3 & 13.2 & 1.5 & 1.3 & 5.0 & 14.8 & **64.5** & 8.0 & 4.7 & 10.8 & 1.8 & 6.8 & 3.5 & **68.3** & 21.7 & 10.1 \\ Preservation & 24.5 & 20.5 & 2.1 & 1.5 & 3.3 & 23.3 & **25.1** & 21.3 & 20.5 & 2.8 & 10.5 & 16.0 & **23.8** & 5.3 & **48.5** & 38.5 & 13.0 \\ Naturalness & **27.8** & 24.8 & 6.3 & 2.3 & 0.3 & 21.5 & 26.3 & 26.3 & 7.3 & 9.8 & 3.8 & **28.3** & 2.3 & 22.3 & **55.3** & 36.8 & 8.0 \\  

Table 2: User study on text-driven image manipulation, color transfer, and cross-modal hair editing methods. Accuracy denotes the manipulation accuracy for given conditional inputs, Preservation indicates the ability to preserve irrelevant regions and Naturalness denotes the visual realism of the manipulated image. The numbers represent the percentage of votes. **Bold** and underline denote the best and the second best result, respectively.

Figure 4: Comparison with HairCLIPv2  in detail. Our approach shows better preservation of irrelevant attributes.

### Ablation Study

To demonstrate the effect of each step in our pipeline, we conducted ablation experiments by incrementally adding conditions. As shown in Figure 6, the first row illustrates hairstyle editing based on a given hairstyle text. In the original image, the absence of an agnostic mask results in the failure to generate a bob-cut hairstyle with bangs, leading to poor hairstyle generation quality. In the second step, the lack of an Openpose ControlNet causes misalignment between the generated hairstyle and the face. The third step, without a color proxy, results in uncontrollable hair color. In the fourth step, the introduction of an unwarped color proxy causes random color generation in regions lacking hair color guidance from the original image and alters the background color due to the color proxy's influence. The fifth step aligns the color proxy with the target hairstyle area, ensuring the hair color matches the Source Image. In the second column, a reference hair color image is used for hair color editing. In the original image, the absence of a Canny edge ControlNet results in an uncontrollable hairstyle. In the third image, the Canny edge ControlNet controls the hairstyle structure but not the color. In the fourth image, using the reference image directly as a stroke map for the color proxy results in the region without hair in the reference image lacking color, with the hair color also appearing in the background. In the fifth image, the absence of Bilateral Filtering in the hairstyle causes the warped hairstyle's structural features to adversely affect the original hairstyle structure, leading to poor hairstyle structure.

To better assess the individual contribution of the warping module, we observed that after performing the warp operation, significant discrepancies in hairstyle length or complex edges can lead to alignment challenges, as illustrated in Figure 7. However, by incorporating the patch match method for inpainting, we can effectively reconstruct the corresponding hair color. Additionally, the removal of texture information through blurring further enhances the results. The influence of each configuration on hairstyle generation quality is presented in Table 3.

  
**Model** & FID\(\) & FID\({}_{}\)\(\) & SSIM\(\) \\  w/o warping module & 33.17 & 12.53 & 0.62 \\ w/o patch match & 27.74 & 8.51 & 0.70 \\ w/o bilateral filtering & 20.85 & 6.02 & 0.74 \\  HairDiffusion & \(\) & \(\) & \(\) \\   

Table 3: Quantitative comparison of different variants of warping module with various conditions removed. We achieve the best performance by leveraging the remaining techniques.

Figure 5: Visual comparison with HairCLIPv2 , HairCLIP , Barbershop , CtrlHair , MichiGAN  and HairFastGAN  on hair color transfer.

## 5 Conclusion

In this work, we first propose the latent diffusion-based approach for hair editing. We introduce the MHB module and hair-agnostic masks, which enable the diffusion model to effectively control hairstyle and hair color independently while preserving unrelated attributes. Additionally, we employ a warping module for the first time in this task to ensure alignment of hair color, demonstrating its capability in hair color manipulation and preservation. Furthermore, by collecting image-text pairs focused on hair color structure, we further enhance our model's ability to finely control hair color using both text and reference images.

## 6 Acknowledgements

This work is supported by the National Natural Science Foundation of China under Grant 62176163, 82261138629, and 62320106007, the Science and Technology Foundation of Shenzhen under Grant JCYJ20210324094602007, the Guangdong Basic and Applied Basic Research Foundation under Grant 2023A1515010688, and the Guangdong Provincial Key Laboratory under Grant 2023B1212060076, and the Shenzhen Higher Education Stable Support Program General Project under Grant 20231120175215001.