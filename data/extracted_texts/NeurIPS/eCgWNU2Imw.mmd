# On Sparse Modern Hopfield Model

Jerry Yao-Chieh Hu Donglin Yang Dennis Wu

Chenwei Xu Bo-Yu Chen Han Liu

Department of Computer Science, Northwestern University, Evanston, IL 60208 USA Department of Physics, National Taiwan University, Taipei 10617, Taiwan Department of Statistics and Data Science, Northwestern University, Evanston, IL 60208 USA {jhu, dlyang, hibb, cxu}@u.northwestern.edu

###### Abstract

We introduce the sparse modern Hopfield model as a sparse extension of the modern Hopfield model. Like its dense counterpart, the sparse modern Hopfield model equips a memory-retrieval dynamics whose one-step approximation corresponds to the sparse attention mechanism. Theoretically, our key contribution is a principled derivation of a closed-form sparse Hopfield energy using the convex conjugate of the sparse entropic regularizer. Building upon this, we derive the sparse memory retrieval dynamics from the sparse energy function and show its one-step approximation is equivalent to the sparse-structured attention. Importantly, we provide a sparsity-dependent memory retrieval error bound which is provably tighter than its dense analog. The conditions for the benefits of sparsity to arise are therefore identified and discussed. In addition, we show that the sparse modern Hopfield model maintains the robust theoretical properties of its dense counterpart, including rapid fixed point convergence and exponential memory capacity. Empirically, we use both synthetic and real-world datasets to demonstrate that the sparse Hopfield model outperforms its dense counterpart in many situations.

## 1 Introduction

We address the computational challenges of modern Hopfield models by introducing a sparse Hopfield model. Our sparse continuous Hopfield model equips a memory-retrieval dynamics that aligns with the sparse-structured attention mechanism. By establishing a connection to sparse attention, the proposed model not only offers a theoretically-grounded energy-based model for associative memory but also enables robust representation learning and seamless integration with deep learning architectures. This approach serves as an initial attempt of pushing the correspondence1 between Hopfield models and attention mechanism  toward sparse region, both theoretically and empirically, resulting in data-dependent sparsity for meaningful and robust pattern representations, and a focus on the most relevant information for each specific instance.

Hopfield models are classic associative memory models for both biological and artificial neural networks . These models are designed to store and retrieve memory patterns. They achieve these by embedding the memories in the energy landscape of a physical system (e.g., the Ising model in ; see Figure 3 for a visualization), where each memory corresponds to a local minimum. When a query is presented, the model initiates energy-minimizing retrieval dynamics at the query, which then navigate the energy landscape to find the nearest local minimum, effectively retrieving the memory most similar to the query.

In the same vein, Ramsauer et al. (2021) propose the modern Hopfield model and integrate it into deep learning architectures via a strong connection with transformer attention, offering enhanced performance, theoretically guaranteed exponential memory capacity, and the ability to handle continuous patterns. In addition, the modern Hopfield models have found success in various applications, such as immunology (Widrich et al., 2020) and large language model (Furst et al., 2022). Apart from the elegant connection to attention, theoretical advantages and empirically successes, the modern Hopfield models have been shown to be computationally heavy and vulnerable against noisy queries (Millidge et al., 2022). In particular, the dense output alignments of the retrieval dynamics in modern Hopfield models (Ramsauer et al., 2021) can be computationally inefficient, making models less interpretable and noise-sensitive by assigning probability mass to many implausible outputs (patterns/keys).

To combat above, incorporating sparsity is an essential and common strategy. While there is a vast body of work on sparsifying attention mechanisms (Tay et al., 2022; Beltagy et al., 2020; Qiu et al., 2019; Child et al., 2019; Peters et al., 2019; Martins and Astudillo, 2016), similar developments for the Hopfield models remain less explored. To bridge this gap, we present a sparse Hopfield model that corresponds to the sparsemax attention mechanism (Martins and Astudillo, 2016). In this paper, we study the sparsification of the modern Hopfield model. The challenges are three-fold:

* **Non-Trivial Sparsification -- Sparse Hopfield \(\) Sparse Attention:** To enable the use of sparse Hopfield models as computational devices (DNN learning models) akin to (Ramsauer et al., 2021), it is essential to achieve _non-trivial_ sparsifications that exhibit equivalence to specific sparse attention models. In other words, any meaningful sparsification should extend the established equivalence (Ramsauer et al., 2021) between modern Hopfield models and attention to encompass the sparse domain. While generalizing such equivalence is potentially impactful as it may lay the groundwork for future Hopfield-based methodologies, architecture designs and bio-computing systems (as in (Kozachkov et al., 2023)), the _heuristic_ design of the modern Hopfield model poses great difficulty to developing desired sparse models.
* **Introducing Sparsity into Hopfield Models:** Unlike attention mechanisms where sparsification is typically achieved either on the attention matrix (e.g., structured-sparsity (Tay et al., 2020; Child et al., 2019)) or on the element-wise normalization map (e.g., sparsity-inducing maps (Correia et al., 2019; Peters et al., 2019; Martins and Astudillo, 2016)), the sparsification of Hopfield models is applied to _both_ the energy function and the memory-retrieval dynamics, where the latter monotonically decreases the Hopfield energy over time. Since attention mechanisms (transformers) are typically not equipped with such a dynamical description, introducing sparsity into Hopfield models while retaining the connection to attention is a less straightforward process.
* **Properties of the Sparse Hopfield Model:** Further, it is unclear how the introduced sparsity may affect different aspects of the model, such as memory capacity, fixed point convergence, retrieval accuracy, and so on. Ideally, we are looking for sparsities that offer provable computational benefits, such as enhanced robustness and increased memory capacity, among others.

Challenges (C1) and (C2) are inherent in Hopfield model, and certain requirements on the design of energy function and retrieval dynamics are inevitable to obtain non-trivial sparse models. Hence, we suppose the sparsified models should satisfy some conditions and verify them accordingly. Concretely, a formulation for deriving desired sparse Hopfield energy via convex conjugation of entropic regularizers is proposed. Furthermore, by applying Danskin's theorem and convex-concave procedure (Yuille and Rangarajan, 2003, 2001) on the sparse Hopfield energy function, we obtain sparse retrieval dynamics linked to sparse attention. For (C3), the convergence of energy stationary points and retrieval dynamics fixed points are connected via Zangwill's method (Zangwill, 1969). The sparse retrieval error bound is derived and used to determined the well-separation condition for successful memory storage and retrieval. Lastly, the fundamental limit of memory capacity is derived using the expected separation of random points on spheres (Cai and Jiang, 2012; Brauchart et al., 2018; Ramsauer et al., 2021).

In summary, this work handles sparsification of modern Hopfield models while linking them to sparse attention by addressing the following question:

* Is it possible to develop a theoretically-grounded (non-trivial) sparse Hopfield model capable of storing information or learned prototypes throughout various layers of DNN models?

**Contributions.** We propose the Sparse Modern Hopfield Model. Our contributions are as follows:* We propose a novel sparse Hopfield model whose retrieval dynamics corresponds to sparsemax attention mechanism. It leads to sparse patterns by design, inheriting both noise robustness and potential computational efficiency2 from (Martins and Astudillo, 2016), compared to its dense counterparts. This work extends the theoretical understanding of the correspondence between artificial and biological neural networks to sparse region. In addition, the sparse Hopfield layer, a new deep learning component, is introduced with data-dependent sparsity. * Theoretically, we establish provably advantages from sparsity and identify the conditions under which these benefits arise. We begin by deriving the closed-form sparse Hopfield energy from the convex conjugation of sparse entropic regularizer. Next, we demonstrate the correspondence between sparse Hopfield retrieval dynamics and sparsemax attention. In addition, we prove the fast convergence of the fixed points (also known as memory patterns, attractor states in literature) for the retrieval dynamics and establish the exponential (in pattern size) memory capacity lower bound with _tighter_ retrieval error bound, _compared_ with modern Hopfield models.
* Empirically, we conduct synthetic and realistic experiments to verify our theoretical results and proposed methodology. Specifically, the sparse Hopfield model outperforms the dense Hopfield model and machine learning baselines in _sparse_ Multiple Instance Learning (MIL), time series prediction and neural machine translation problems. This is observed with both _sparse_ synthetic and real-world datasets, where the baselines tend to fall short. Moreover, even in cases without data sparsity, our proposed model delivers performance on par with its dense counterpart.

To the best of our knowledge, we are the first to propose a sparse Hopfield model whose retrieval dynamics is equivalent to sparse attention mechanism with provably computational advantages. Methodologically, the proposed model complements existing Hopfield-based DNN architectures (Hoover et al., 2023; Paischer et al., 2022; Seidl et al., 2022; Furst et al., 2022; Ramsauer et al., 2021) by introducing a sparse Hopfield layer into deep learning models.

**Organization.** In Section 2, the sparse Hopfield model is introduced. In Section 3, the memory capacity is discussed. In Section 4, experimental studies are conducted. In Section 5, concluding discussions are provided. Additionally, related works and limitations are discussed in Appendix C.

**Notations.** We write \(,^{}\) as the inner product for vectors \(,^{d}\). The index set \(\{1,,I\}\) is denoted by \([I]\), where \(I_{+}\). The spectral norm is denoted by \(\|\|\), which is equivalent to the \(l_{2}\)-norm when applied to a vector. Throughout this paper, we denote the memory patterns (keys) by \(^{d}\) and the state/configuration/query pattern by \(^{d}\), and \((_{1},,_{M}) ^{d M}\) as shorthand for stored memory (key) patterns \(\{_{}\}_{[M]}\). Moreover, we set norm \(n\|\|\) be the norm of the query pattern, and \(m_{[M]}\|_{}\|\) be the largest norm of memory patterns. We also provide a nomenclature table (Table 3) in the appendix.

## 2 Sparse Hopfield Model

In this section, we introduce the sparse Hopfield energy from convex conjugate of entropic regularizer, and then the sparse retrieval dynamics. In this paper we only consider the Gini entropic regularizer corresponding to the sparsemax distribution (Martins and Astudillo, 2016).

Let \(^{d}\) represent the query pattern, and let \((_{1},,_{M}) ^{d M}\) denote the memory patterns. The objective of the Hopfield models is to store the memory patterns \(\) and then retrieve a specific memory pattern \(_{}\) based on a given query \(\). Consequently, any Hopfield model consist of two main components: an _energy function_\(()\), encoding memories into its local minima, and a _retrieval dynamics_\(()\), which retrieves a memory by iteratively minimizing \(()\) when initialized with a query. We provide a visualization of this procedure in Figure 3. The construction of the energy function \(()\) is straightforward. As emphasized in (Krotov and Hopfield, 2016), the memories can be easily encoded into \(()\) through the _overlap-construction_: \(()=F(^{})\), where \(F:^{M}\) is a smooth function. This ensures that the memories \(\{_{}\}_{[M]}\) are located at the stationary points of \(()\), since \(_{}F(^{}) _{_{}}=0\) for all \([M]\). Different choices of \(F\) lead to different Hopfield models, as demonstrated in (Krotov and Hopfield, 2016; Demircigil et al., 2017; Ramsauer et al., 2021; Krotov and Hopfield, 2021). However, finding a corresponding retrieval dynamics, \(\), for a given energy \(()\), is generally more challenging. This is because \(\) needs to satisfy two conditions to ensure successful memory retrieval: (i) To ensure consistent retrieval, an appropriate \(\) should monotonically minimize \(()\) when iteratively applied. (ii) To ensure accurate retrieval, an appropriate \(\) should align its fixed points (the points where iterative application terminates) with the stationary points of \(()\).

To this end, we introduce the sparse Hopfield model, providing a principled construction for \(\) and \(\). This model not only fulfills the aforementioned desirable properties, but also enables more robust and faster memory retrieval compared to the modern Hopfield model (Ramsauer et al., 2021).

### Sparse Hopfield Energy

Let \(^{d}\) be the query pattern, and \((_{1},,_{M}) ^{d M}\) be the memory patterns. We introduce the sparse Hopfield energy as

\[()=-^{}(^{} )+,,\] (2.1)

with \(^{}()\|\|^{2}-\| ()-\|^{2}+\), where \(()\) is defined as follows. Let \(,^{M}\), and \(^{M}\{_{+}^{M}_{}^{M}p_{}=1\}\) be the \((M-1)\)-dimensional unit simplex.

**Definition 2.1** (Sparsemax in Variational Form (Martins and Astudillo, 2016), also see Remark F.1).:

\[()^{M}}{ }\|-\|^{2}= ^{M}}{}[^{}-( )],\] (2.2)

where \(()-_{}^{M}p_{}(1-p_{})\) is the negative Gini entropy or Gini entropic regularizer.

**Remark 2.1**.: Recall that, the variational form (2.2) is in fact general, that applies to various entropic regularizers, as discussed in (Peters et al., 2019; Wainwright et al., 2008). The choice of \(\) determines the resulting sparse probability distribution. For instance, if we choose the Gibbs' entropic regularizer \(_{}=-_{}^{M}p_{} p_{}\), (2.2) reduces to the standard softmax distribution.

**Overview of Theoretical Results.** At first glance, the energy function (2.1) may seem peculiar. However, it indeed represents a non-trivial sparse Hopfield model with appealing properties, including:

1. In response to challenge (C1) & (C2), as we shall see in Section 2.2, the energy (2.1) leads to a sparse retrieval dynamics that not only retrieves memory by monotonically decreasing (Lemma 2.1) to its stationary points (Lemma 2.2), but also associates with sparsemax attention through its single-step approximation (Remark 2.2);
2. In response to challenge (C3), as we shall see in Section 3, it indulges fast convergence of retrieval (Corollary 3.1.2), exponential-in-\(d\) memory capacity akin to modern Hopfield models (Lemma 3.1). Notably, it accomplishes these with a tighter retrieval error bound (Theorem 2.1).

We reveal each of these properties in the following sections.

### Sparse Retrieval Dynamics and Connection to Sparse Attention

The optimization problem \(_{^{M}}[^{} -()]\) does not necessarily have a closed-form solution for arbitrary \(\). However, a family of \(\) has been investigated in literature (Correia et al., 2019; Martins and Astudillo, 2016) with closed-form solutions derived, including the \(()\).

**Sparsemax in Closed-Form** (Proposition 1 of (Martins and Astudillo, 2016)).: Let \(^{M}\). Denote \([a]_{+}\{0,a\}\), \(z_{()}\) the \(\)'th element in a sorted descending \(z\)-sequence \(_{} z_{(1)} z_{(2)} z_{(M)}\), and \(()\{k[M] 1+kz_{(k)}>_{  k}z_{()}\}\). The optimization problem(s) (2.2) has closed-form solution

\[()=[-()_{ M}]_{+},\] (2.3)

where \(:^{M}\) is the threshold function \(()=[(_{()}z_{()} )-1]/()\), satisfying \(_{=1}^{M}[z_{}-()]_{+}=1\) for all \(\). Notably, \(()=|S()|\) where \(S()=\{[M]_{}()>0\}\) is the support set of \(()\).

In this case, we present the following theorem to derive the convex conjugate of \(\) in closed-form:

**Theorem 2.1** (Convex Conjugate of Negative Gini Entropy).: Let \(F(),-()\) with \(\) being the negative Gini entropy, \(()=\|\|^{2}-\). The convex conjugate of \(()\) is

\[^{}()_{^{M}}F(, )=\|\|^{2}-\|^{}- \|^{2}+,\] (2.4)

where \(^{}=()\) is given by (2.3).

**Corollary 2.1.1**.: By Danskin's Theorem, \(^{}()=_{ ^{M}}F(,)=()\).

Proof.: A detailed proof is shown in Appendix E.1. 

Theorem 2.1 and Corollary 2.1.1 not only provide the intuition behind the sparse Hopfield energy (2.1) -- the memory patterns are stored in local minima aligned with the overlap-function constructions (i.e. \(\|^{}\|^{2}=_{=1}^{M} _{},^{2}\)) in (Ramsauer et al., 2021; Demircigil et al., 2017; Krotov and Hopfield, 2016) -- but also prepare us for the following corresponding sparse retrieval dynamics.

**Lemma 2.1** (Sparse Retrieval Dynamics).: Let \(t\) be the iteration number. The energy (2.1) can be monotonically decreased by the following sparse retrieval dynamics over \(t\):

\[(_{t})_{}( ^{})_{_{t}}= (^{} _{t})=_{t+1},\] (2.5)

Proof Sketch.: To show monotonic decreasing property, we first derive the sparse retrieval dynamics by utilizing Theorem 2.1, Corollary 2.1.1, along with the convex-concave procedure (Yuille and Rangarajan, 2003, 2001). Then, we show the monotonicity of \(\) by constructing a iterative upper bound of \(\) which is convex in \(_{t+1}\) and thus, can be lowered iteratively by the convex-concave procedure. A detailed proof is shown in the Appendix E.2. 

**Remark 2.2**.: Similar to (Ramsauer et al., 2021), (2.5) is equivalent to sparsemax attention (Martins and Astudillo, 2016) when the \(\) is applied only once, see Appendix D for more details. Importantly, \(\) acts as a scaling factor for the energy function, often referred to as the "inverse temperature". It influences the sharpness of energy landscape Equation (2.1), thereby controlling the dynamics. High \(\) values, corresponding to low temperatures, encourage that the basins of attraction for individual memory patterns remain distinct, leading to easier retrieval.

Notably, since \(\|^{}\|^{2}=_{=1}^{M} _{},^{2}\), (2.5) implies that the local optimum of \(\) are located near the patterns \(_{}\). Different from previous studies on binary Hopfield models (Demircigil et al., 2017; Krotov and Hopfield, 2016), for continuous patterns, we adopt the relaxed definition from (Ramsauer et al., 2021)3 to rigorously analyze the memory retrieval, and the subsequent lemma.

**Definition 2.2** (Stored and Retrieved).: Assuming that every pattern \(_{}\) surrounded by a sphere \(S_{}\) with finite radius \(R_{;,[M]}\| _{}-_{}\|\), we say \(_{}\) is _stored_ if there exists a generalized fixed point of \(\), \(_{}^{} S_{}\), to which all limit points \( S_{}\) converge to, and \(S_{} S_{}=\) for \(\). We say \(_{}\) is _\(\)-retrieved_ by \(\) with \(\) for an error4, if \(\|()-_{}\|\).

We say \(_{}\) is _\(\)-retrieved_ by \(\) with \(\) for an error5, if \(\|()-_{}\|\).

Definition 2.2 sets the threshold for a memory pattern \(_{}\) to be considered _stored_ at a fixed point of \(\), \(_{}^{}\). However, this definition does not imply that the fixed points of \(\) are also stationary points of the energy function \(\). In fact, monotonicity of (2.5) does not assure the existence of stationary points of energy \(\)(Sriperumbudur and Lanckriet, 2009). To establish a well-defined Hopfield model, we need two types of convergence guarantees. The first is the convergence between \(_{}^{}\) and \(_{}\), which ensures that the retrieved memory is close to the stored memory. The second is the convergence of \(\) to its stationary points through the dynamics of \(\), which ensures that the system reaches a state of minimal energy. The following lemma provides the convergence results for both.

**Lemma 2.2** (Convergence of Retrieval Dynamics \(\)).: Suppose \(\) is given by (2.1) and \(()\) is given by (2.5). For any sequence \(\{_{t}\}_{t=0}^{}\) defined by \(_{t^{}+1}=(_{t^{}})\), all limit points of this sequence are stationary points if they are obtained by iteratively applying \(\) to \(\).

Proof Sketch.: We verify and utilize Zangwill's global convergence theory (Zangwill, 1969) for iterative algorithms \(\), to first show that all the limit points of \(\{_{}\}_{=0}^{}\) are generalized fixed points and \(_{t}(_{t})=( ^{})\), where \(^{}\) are some generalized fixed points of \(\). Subsequently, by (Sriperumbudur and Lanckriet, 2009, Lemma 5), we show that \(\{^{}\}\) are also stationary points of \(_{}[]\), and hence \(\) converges to local optimum. A detailed proof is shown in Appendix E.4. 

Intuitively, Lemma 2.2 indicates that the energy function converges to local optimum, i.e. \(_{t}(_{t})( ^{})\), where \(^{}\) are stationary points of \(\). Consequently, it offers formal justifications for the retrieval dynamics (2.5) to retrieve stored memory patterns \(\{_{}\}_{[M]}\): for any query (initial point) \(\), \(\) monotonically and iteratively approaches stationary points of \(\), where the memory patterns \(\{_{}\}_{[M]}\) are stored. As for the retrieval error, we provide the following theorem stating that \(\) achieves a lower retrieval error compared to its dense counterpart.

**Theorem 2.2** (Retrieval Error).: Let \(_{}\) be the retrieval dynamics of the dense modem Hopfield model (Ramsauer et al., 2021). It holds for all \( S_{}\). Moreover,

\[\|()-_{}\| m+d^{ {{1}}{{2}}}m[(*{Max}_{[M]} _{},)-[^{} ]_{()})+],\] (2.6)

where \([^{}]_{()}\) is the \(\)th-largest element of \(^{}^{M}\) following the sparsemax definition (2.3).

Proof.: A detailed proof is shown in Appendix E.3. 

Interestingly, (2.6) is a sparsity dependent bound4

\[\|()-_{}\|\| _{}()-_{}\| 2m(M-1) -(_{}, -*{Max}_{[M],} {}_{},_{})}.\] (2.7)

. By denoting \(n\|\|\), the second term on the RHS of (2.6) is dominated by the sparsity dimension \(\) as it can be expressed as \((1-[^{}]_{( )})\) with a constant \(0 2\). When \(^{}\) is sparse (i.e. \(\) is small), the bound is tightler, vice versa.

**Remark 2.3** (Faster Convergence).: Computationally, Theorem 2.2 implies that \(\) requires fewer iterations to reach fixed points with the same amount of error tolerance compared to \(_{}\). Namely, \(\) retrieves stored memory patterns faster and therefore more efficiently, as evidenced in Figure 2.

**Remark 2.4** (Noise-Robustness).: Moreover, in cases of contaminated patterns with noise \(\), i.e. \(}=+\) (noise in query) or \(_{}=_{}+\) (noise in memory), the impact of noise \(\) on the sparse retrieval error (2.6) is linear, while its effect on the dense retrieval error (2.7) is exponential. This suggests the robustness advantage of the sparse Hopfield model, as evidenced in Figure 1.

### Sparse Hopfield Layers for Deep Learning

The sparse Hopfield model can serve as a versatile component for deep learning frameworks, given its continuity and differentiability with respect to parameters. Corresponding to three types of Hopfield Layers proposed in (Ramsauer et al., 2021), we introduce their sparse analogs: **(1)** SparseHopfield, **(2)** SparseHopfieldPooling, **(3)** SparseHopfieldLayer. Layer SparseHopfield has memory (stored or key) patterns \(\) and query (state) pattern \(\) as inputs, and associates these two sets of patterns via the sparse retrieval dynamics (2.5). This layer regards the transformer attention layer as its one-step approximation, while utilizing the sparsemax (Martins and Astudillo, 2016) on attention matrix. Layer SparseHopfieldPooling and Layer SparseHopfieldLayer are two variants of SparseHopfield, whose input patterns are memory patterns and query patterns from previous layers or external plugin, respectively. SparseHopfieldPooling, whose query patterns are learnable parameters, can be interpreted as performing a pooling operation over input memory patterns. SparseHopfieldLayer, by contrast, has learnable memory patterns that maps query patterns to hidden states with sparsemax activation. Thus it can substitute a fully connected layer within deep learning architectures. See (D.12) and the implementation Algorithm 1 in Appendix D, and (Ramsauer et al., 2021, Section 3) for more details of these associations. In Section 4, we apply these layers and compare them with their dense counterparts in (Ramsauer et al., 2021) and other baseline machine learning methods.

## 3 Fundamental Limits of Memory Capacity of Sparse Hopfield Models

How many patterns can be stored and reliably retrievable in the proposed model? We address this by decomposing it into to two sub-questions and answering them separately:

1. What is the condition for a pattern \(_{}\) considered well stored in \(\), and correctly retrieved?2. What is the number, in expectation, of the the patterns satisfying such condition? For (A), we first introduce the notion of separation of patterns following (Ramsauer et al., 2021),

**Definition 3.1** (Separation of Patterns).: The separation of a memory pattern \(_{}\) from all other memory patterns \(\) is defined as its minimal inner product difference to any other patterns: \[_{}}[_{},_{}-_{},_{} ]=_{},_{}-}[_{},_{}].\] (3.1) Similarly, the separation of \(_{}\) at a given \(\) from all memory patterns \(\) is given by \[_{}} [,_{}-,_{ }].\] (3.2)

and then the well-separation condition for a pattern being well-stored and retrieved.

**Theorem 3.1** (Well-Separation Condition).: Given the definition of stored and retrieved memories in Definition 2.2, suppose the memory patterns \(\{_{}\}_{[M]}\) are located within the sphere \(S_{}\{\ \ \|-_{}\| R\}\), where the radius \(R\) is finite and defined in Definition 2.2 for all \(\). Then, the retrieval dynamics \(\) maps the sphere \(S_{}\) onto itself under the following conditions:

1. The initial query \(\) is located within the sphere \(S_{}\), i.e., \( S_{}\).
2. The _well-separation_ condition is satisfied, which is given by: \[_{} mn+2mR-[^{}]_{( )}-(}{{2}}}}{m d^{ }{{2}}}}).\]

**Corollary 3.1.1**.: Let \(\|_{}-_{}\|-\|- _{}\|\). The well-separation condition can be expressed as \(_{}\!()+2mR\), which reduces to that of the dense Hopfield model when \(=0\).

Proof Sketch.: The proofs proceed by connecting \(_{}\) with \(\|()-_{}\|\). To do so, we utilize Theorem 2.2 to incorporate the \(_{}\)-dependent bound on the retrieval error of both sparse and dense Hopfield models (Ramsauer et al., 2021). A detailed proof is shown in Appendix E.5. 

Together with Lemma 2.2, the well-separated condition serves as the necessary condition for pattern \(_{}\) to be well-stored at the stationary points of \(\), and can be retrieved with at most \(=R\) by \(\), as per Definition 2.2. We make the following three observations about the blessings from sparsity.

1. In general, to appreciate the blessings of sparsity, we rearrange the well-separation condition as \[_{} 2mR+^{} ]_{()})}_{:= nm\ \ 0 2}- (}{{2}}}}{m d^{ }{{2}}}}),\] (3.3) and observe the two competing terms, \( nm\) and \(}{{2}}})}}{( m d^{ }{{2}}})}\). Sparsity proves advantageous when the latter term surpasses the former, i.e. the sparse well-separation condition is consistently lower than its dense counterpart. The condition under which sparsity benefits are more likely to emerge (i.e., when the well-separation condition is more readily satisfied) is thereby: \[\,}\|_{}-_{ }\| md^{}{{2}}}(1+ nm)+m, \ 0 2.\] (3.4) Intuitively, the sparser \(^{}\) is, the easier it is for the above condition to be fulfilled.
2. **Large \(M\) limit:** For large \(M\), the dense well-separation condition (Corollary 3.1.1) explodes while the sparse one (Theorem 3.1) saturates to the first three \(M\)-independent terms. This suggests that the hardness of distinguishing patterns can be tamed by the sparsity, preventing an increase of \(_{}\) with \(M\) as observed in the dense Hopfield model. We numerically confirm this in Figure 1.
3. \(\)**Limit:** In the region of low temperature, where \(\) and hence all patterns can be _error-free_ retrieved as per (2.7), we have \(_{} 2mR+ nm\) with \(0 2\). Here, the second term on the RHS represents the sparsity level of \(^{}\), i.e. a smaller \(\) indicates a higher degree of sparsity in \(^{}\). Hence, the higher the sparsity, the easier it is to separate patterns.

For (B), equipped with Theorem 3.1 and Corollary 3.1.1, we provide a lower bound for the number of patterns being well-stored and can be _at least_\(R\)-retrieved in the next lemma5:

**Lemma 3.1** (Memory Capacity Lower Bound).: Let \(1-p\) be the probability of successfully storing and retrieving a pattern. The number of patterns randomly sampled from a sphere of radius \(m\) that the sparse Hopfield model can store and retrieve is lower-bounded by

\[MC^{},\] (3.5)

where \(C\) is the solution to \(C=}{{W_{0}}((a+ b)})\) with \(W_{0}()\) being the principal branch of Lambert \(W\) function, \(a(}{{d-1}})([2m(-1)/(R+ )]+1)\) and \(b}}{{}}/}{{(d-1)}}\). For sufficiently large \(\), the sparse Hopfield model exhibits a larger lower bound on the exponential memory capacity compared to its dense counterpart (Ramsauer et al., 2021): \(M M_{}\).

Proof Sketch.: Our proof is built on (Ramsauer et al., 2021). The high-level idea is to utilize the separation of random patterns sampled from spheres (Cai and Jiang, 2012; Brauchart et al., 2018) and the asymptotic expansion of the Lambert \(W\) function (Corless et al., 1996). Firstly, we link the well-separation condition to cosine similarity distance, creating an inequality for the probability of a pattern being well-stored and retrieved. Next, we identify and prove conditions for the exponential memory capacity \(M=C^{(d-1)/4}\) to hold. Finally, we analyze the scaling behaviors of \(C\) using its asymptotic expansion and show that \(M M_{}\). A detailed proof is shown in Appendix E.6. 

Intuitively, the benefits of sparsity arises from the increased energy landscape separation provided by the sparse Hopfield energy function, which enables the separation of closely correlated patterns, resulting in a tighter well-separation condition for distinguishing such patterns and hence a larger lower bound on the memory capacity. Moreover, the sparse Hopfield model also enjoys the properties of fast convergence and exponentially suppressed retrieval error provided by the following corollary.

**Corollary 3.1.2** (Fast Convergence and Exponentially Suppressed Retrieval Error).: For any query \(\), \(\) approximately retrieves a memory pattern \(_{}\) with retrieval error \(_{}\) exponentially suppressed by \(_{}\): \(\|()-_{}\| 2m(M-1)- (_{}-2m[\|-_{}\|,\|-_{}^{*}\|])}\).

Proof.: This results from Theorem 2.2, Lemma 2.2, and (Ramsauer et al., 2021, Theorem 4). 

Corollary 3.1.2 suggests that, with a sufficient \(_{}\), \(\) can approximately retrieve patterns after a single _activation_, allowing the integration of sparse Hopfield models into deep learning architectures similarly to (Hoover et al., 2023; Seidl et al., 2022; Furst et al., 2022; Ramsauer et al., 2021).

## 4 Proof of Concept Experimental Studies

We demonstrate the validity of our theoretical results and method by testing them on various experimental settings with both synthetic and real-world datasets.

### Experimental Validation of Theoretical Results

We conduct experiments to verify our theoretical findings, and report the results in Figure 1. For the memory capacity (the top row of Figure 1), we test the proposed sparse model on retrieving half-masked patterns comparing with the Dense (Softmax) and 10th order polynomial Hopfield models (Millidge et al., 2022; Krotov and Hopfield, 2016) on MNIST (high sparsity), Cifar10 (low sparsity) and ImageNet (low sparsity) datasets. For all Hopfield models, we set \(=1\).6 A query is regarded as correctly retrieved if its cosine similarity error is below a set threshold. In addition, for the robustness against noisy queries (the bottom row of Figure 1), we inject Gaussian noises with varying variances (\(\)) into the images. Plotted are the means and standard deviations of 10 runs. The results show that the proposed sparse Hopfield model excels when memory patterns exhibit a high degree of sparsity and the signal-to-noise ratio in patterns is low, aligning with our theoretical results.

### Multiple Instance Learning Tasks

Ramsauer et al. (2021) point out that the memory-enhanced Hopfield layers present a promising approach for Multiple Instance Learning (MIL) tasks. Multiple Instance Learning (MIL) (Ilse et al., 2018; Carbonneau et al., 2018) is a variation of supervised learning where the training set consists of labeled bags, each containing multiple instances. The goal of MIL is to predict the bag labels based on the instances they contain, which makes it particularly useful in scenarios where labeling individual instances is difficult or impractical, but bag-level labels are available. Examples of such scenarios include medical imaging (where a bag could be an image, instances could be patches of theimage, and the label could indicate the presence or absence of disease) and document classification (where a bag could be a document, instances could be the words or sentences in the document, and the label could indicate the topic or sentiment of the document). In this subsection, we implement our sparse Hopfield layers and applied them to MIL tasks on one synthetic and four real-world settings.

#### 4.2.1 Synthetic Experiments

We use a synthetic MIL dataset, the bit pattern dataset, to demonstrate the effectiveness of the sparse Hopfield model. Each bag in this synthetic dataset contains a set of binary bit strings. The positive bag includes at least one of the positive bit patterns. We compare the performance of the SparseHopfield and SparseHopfieldPooling to their dense counterparts and vanilla attention (Vaswani et al., 2017). We report the mean test accuracy of 10 runs. To demonstrate the effectiveness of sparse Hopfield model, we vary two hyperparameters of the bit pattern dataset corresponding to two perspectives: bag sparsity (sparsity in data) and bag size (number of memory patterns, \(M\)). For **bag sparsity**, we fix the bag size as 200, and inject from 2 to 80 positive patterns in a positive bag, results in 1 to 40 percent of positive patterns in each positive bag. For **bag size**, we fix the number of positive pattern in a bag to be 1, and vary bag size from 20 to 300. We report results of SparseHopfieldPooling in Table 1, and implementation details in Appendix H.1.1. A more complete version of Table 1, including the results of Hopfield and attention, is in Appendix G. The sparse Hopfield model demonstrates a better performance across all sparsity and all bag sizes.

**Convergence Analysis.** In Figure 2, we numerically examine the convergence of the sparse and dense Hopfield models, plotting their loss and accuracy for the **bag size** tasks in above on the bit pattern

   Bag Size & 20 & 50 & 100 & 150 & 200 & 300 \\  Dense Hopfield Pooling & 100.0 \(\) 0.00 & 100.0 \(\) 0.00 & 100.0 \(\) 0.00 & 76.44 \(\) 0.23 & 49.13 \(\) 0.01 & 52.88 \(\) 0.01 \\ Sparse Hopfield Pooling & 100.0 \(\) 0.00 & 100.0 \(\) 0.00 & 100.0 \(\) 0.00 & **99.76 \(\) 0.00** & **99.76 \(\) 0.00** & **99.76 \(\) 0.00** \\   Bag Sparsity & 1\% & 5\% & 10\% & 20\% & 40\% \\  Dense Hopfield Pooling & 49.20 \(\) 0.00 & 85.58 \(\) 0.10 & 100.0 \(\) 0.00 & 100.0 \(\) 0.00 & 99.68 \(\) 0.00 \\ Sparse Hopfield Pooling & **73.40 \(\) 0.06** & **99.68 \(\) 0.00** & 100.0 \(\) 0.00 & 100.0 \(\) 0.00 & **100.0 \(\) 0.00** \\   

Table 1: **Top (Bag Size):** Accuracy comparison on bit pattern dataset for sparse and dense Hopfield model. We report the average accuracy over 10 runs. The results suggest that the sparse Hopfield model demonstrates a better performance when facing a bag size increase. **Bottom (Bag Sparsity):** Performance comparison on bit pattern dataset for sparse and dense Hopfield model with varying bag sparsity. We report the average accuracy over 10 runs. The results suggest that the sparse Hopfield model demonstrates a better performance across all sparsity.

Figure 1: **Top:** Memory Capacity measured by successful half-masked retrieval rates. **Bottom:** Memory Robustness measured by retrieving patterns with varying levels of Gaussian noise. For all Hopfield models, we set \(=.01/0.1/0.1\) (for MNIST/CIFAR10/ImageNet) for better visualization. A query pattern is deemed correctly retrieved if its squared Euclidean distance is below a set threshold. For MNIST/CIFAR10/ImageNet datasets, we set the error thresholds to be 10/20/20 to cope with different sparse levels in data. Plotted are the means and standard deviations of 10 runs. The results suggest that the sparse Hopfield model excels when memory patterns exhibit a high degree of sparsity and the signal-to-noise ratio in patterns is low.

dataset. We include multiple bag sizes to assess the effect of increasing memory patterns (i.e. \(M\)) on the loss curve. The plotted are the loss and accuracy curves of SparseHopfieldPooling. We refer results of Hopfield and more details to Appendix G.3. The results (Figure 2) show that, sparse Hopfield model surpasses its dense counterpart in all bag sizes. Moreover, for the same bag size, the sparse Hopfield model always reaches the minimum validation loss faster than dense Hopfield model, validating our Theorem 2.2.

**Sparsity Generalization.** We also evaluate the models' generalization performance with shifting information sparsity, by training dense and sparse Hopfield models with a specific bag sparsity and testing them on the other. We report the results in Table 5 and refer more details to Appendix G.3.

#### 4.2.2 Real-World MIL Tasks

Next, we demonstrate that the proposed method achieves near-optimal performance on four realistic (_non-sparse_) MIL benchmark datasets: Elephant, Fox and Tiger for image annotation (Ilie et al., 2018), UCSB breast cancer classification (Kandemir et al., 2014). We use Hopfield and SparseHopfield to construct a similar model architecture proposed in (Ramsauer et al., 2021) and a detailed description of this experiment as well as its training and evaluating process can be found in Appendix H.1.2. As shown in Table 2, both Sparse and Dense Hopfield achieve near-best results on Tiger, Elephant and UCSB datasets, despite the low sparsity in data. The sparse Hopfield model outperforms the dense Hopfield model by a small margin on three out of four datasets.

## 5 Conclusion

We present a sparse Hopfield model with a memory-retrieval dynamics that corresponds to the sparse-structured attention mechanism. This model is capable of merging into deep learning architectures with data-dependent sparsity. Theoretically, we introduce a principled construction for modern Hopfield models, based on the convex conjugate of different entropy regularizers. It allows us to easily recover the dense modern Hopfield model (Ramsauer et al., 2021) using Gibbs entropy. Moreover, we introduce the sparse Hopfield model using the Gini entropic regularizer, and explore its theoretical advantages, delineating conditions that favor its use. Empirically, we demonstrate our theoretical results and methodology to be effective on various synthetic and realistic settings. This work extends the correspondence between artificial and biological neural networks to sparse domain, potentially paving the way for future Hopfield-based methodologies and bio-inspired computing systems.

   Method & Tiger & Fox & Elephant & UCSB \\  Dense Hopfield & \(0.878 0.028\) & \(0.600 0.011\) & \(0.907 0.022\) & \(0.880 0.013\) \\ Sparse Hopfield & \(0.892 0.021\) & \(0.611 0.010\) & \(0.912 0.016\) & \(0.877 0.009\) \\  Path encoding & \(0.910 0.010\) & \(0.712 0.014\) & \(0.944 0.007\) & \(0.880 0.022\) \\ MinD & \(0.853 0.011\) & \(0.704 0.016\) & \(0.936 0.009\) & \(0.831 0.027\) \\ MILES & \(0.827 0.017\) & \(0.738 0.016\) & \(0.927 0.007\) & \(0.833 0.026\) \\ APR & \(0.778 0.007\) & \(0.541 0.009\) & \(0.535 0.010\) & \\ Citation-4NN & \(0.855 0.009\) & \(0.635 0.015\) & \(0.896 0.009\) & \(0.706 0.032\) \\ DD & \(0.841\) & \(0.631\) & \(0.507\) & \\   

Table 2: Results for MIL benchmark datasets in terms of AUC score. The baselines are Path encoding (Kucukasci and Baydogan, 2018), MinD (Cheplygina et al., 2015), MILES (Chen et al., 2006), APR (Dietterich et al., 1997), Citation-KNN (Wang and Zucker, 2000) and DD (Maron and Lozano-Perez, 1997). Results for baselines are taken from (Ramsauer et al., 2021). The results suggest the proposed model achieves near-optimal performance even when the data is not sparse.

Figure 2: **Top:** The training loss and accuracy curve of dense and sparse Hopfield models with different bag sizes. **Bottom:** The validation loss and accuracy curve of dense and sparse Hopfield models with different bag sizes. The plotted are the mean of 10 runs. The results indicate that the sparse Hopfield model converges faster than the dense model and also yields superior accuracy.

Post-Acceptance Note [November 28, 2023].After the completion of this work, the authors learn of two upcoming works -- [Anonymous, 2023] at ICLR'24 and [Martins et al., 2023] in the Associative Memory & Hopfield Networks Workshop at NeurIPS'23 -- both addressing similar topics. Both of these works explore theoretical generalizations of this work. In addition, [Anonymous, 2023] further presents a (sparse) Hopfield-based deep learning model for multivariate time series prediction. We thank the authors of [Martins et al., 2023] for enlightening discussions and for sharing their preliminary manuscript.