# LICO: Explainable Models with

Language-Image COnsistency

 Yiming Lei\({}^{1}\), Zilong Li\({}^{1}\), Yangyang Li\({}^{2}\), Junping Zhang\({}^{1}\), Hongming Shan\({}^{3,4}\)

\({}^{1}\) Shanghai Key Laboratory of Intelligent Information Processing,

School of Computer Science, Fudan University

\({}^{2}\) Academy of Mathematics and Systems Science, Chinese Academy of Sciences

\({}^{3}\) Institute of Science and Technology for Brain-Inspired Intelligence and

MOE Frontiers Center for Brain Science, Fudan University

\({}^{4}\) Shanghai Center for Brain Science and Brain-inspired Technology

{ymlei, jpzhang, hmshan}@fudan.edu.cn,

zilongli23@m.fudan.edu.cn, yyli@amss.ac.cn

Corresponding author.

###### Abstract

Interpreting the decisions of deep learning models has been actively studied since the explosion of deep neural networks. One of the most convincing interpretation approaches is salience-based visual interpretation, such as Grad-CAM, where the generation of attention maps depends merely on categorical labels. Although existing interpretation methods can provide explainable decision clues, they often yield partial correspondence between image and saliency maps due to the limited discriminative information from one-hot labels. This paper develops a Language-Image COnsistency model for explainable image classification, termed LICO, by correlating learnable linguistic prompts with corresponding visual features in a coarse-to-fine manner. Specifically, we first establish a coarse global manifold structure alignment by minimizing the distance between the distributions of image and language features. We then achieve fine-grained saliency maps by applying optimal transport (OT) theory to assign local feature maps with class-specific prompts. Extensive experimental results on eight benchmark datasets demonstrate that the proposed LICO achieves a significant improvement in generating more explainable attention maps in conjunction with existing interpretation methods such as Grad-CAM. Remarkably, LICO improves the classification performance of existing models without introducing any computational overhead during inference. Source code is made available at https://github.com/ymLeiFDU/LICO.

## 1 Introduction

Although deep neural networks (DNNs) have shown excellent performance in many fields, the lack of interpretability is still a barrier to landing in some high-stakes scenarios such as medical diagnosis, autonomous driving, _etc_. Therefore, the literature proposes various interpretation methods for DNNs and reveals the decision clues of DNNs to some extent.

Popular interpretation methods can be roughly categorized into two types: 1) gradient back-propagation-based, and 2) class activation mapping (CAM)-based. Both of them mainly take image classification as the pretext task and then generate explainable noisy gradients and saliency maps, respectively. As illustrated in Fig. 1(a), CAM-based methods often explore a better weighting scheme for integrating feature maps of a given input image. The gradient-based methods, in Fig. 1(b), alsostart from the output logits while back-propagating gradients to the input space. However, they are all post-hoc approaches that investigate a DNN pre-trained on a specific dataset [1; 2; 3; 4; 5; 6; 7], and yield biased interpretations due to the limited semantic information that a set of one-hot labels can offer. In addition, one-hot labels often trigger overfitting of the pre-trained model so that the effectiveness of existing interpretation methods could be compromised. On the other hand, the latent feature space of such pre-trained models is unable to sense real semantic space reflecting crucial parts in images. Therefore, it may be futile to explore complex post-hoc techniques for interpreting a pre-trained model.

Inspired by advanced large vision-language models (VLMs) such as CLIP , which developed generalized image and text encoders through training on huge amounts of image-text pairs, in this paper, we assume that the large VLMs can encode real-world semantic knowledge through contrastive vision-language alignments, whereas the traditional models pre-trained on relatively small datasets, such as ImageNet-1k, are inferior in capturing the true semantic information.

Motivation.The DNN-based image classification framework generally consists of a convolutional neural network (CNN) for feature extraction and a linear layer acting as a classifier. Since feature representations are used as the input to the classifier, if the DNN is truncated from feature representations, it becomes a linear classifier that aims to classify feature representations linearly into discrete one-hot label space. More specifically, the feature representation lies in the learned manifolds of high-dimensional semantic space . Unfortunately, training using cross-entropy loss with one-hot labels cannot guarantee that the manifolds of image features can reflect the distribution of real-world semantic knowledge, which hinders performance improvement of existing interpretation methods.

In this paper, we leverage language information from large VLMs to enhance current interpretation methods for achieving more explainable saliency maps while enabling promising classification performance improvements; see Fig. 1(c). First, we propose Language-Image-COnsistent (LICO) learning to facilitate the alignment between the manifolds of visual features and class-aware language information. To address the discrete nature of categorical classes, we construct a learnable prompt for each class and map all prompts into a continuous space using the CLIP text encoder, which is feasible to align manifolds of both image and text. Second, we impose each prompt token to correlate with certain feature maps. Considering that the feature maps are redundant to the final classification decision, we propose to encourage the context tokens to guide certain feature maps through distribution alignment using optimal transport (OT) theory.

Contributions.We summarize the main contributions of this paper as follows. **(i)** We propose a novel framework to enhance current interpretation methods by introducing language guidance from large VLMs. **(ii)** We model the discrete categorical class to a continuous space by constructing class-aware learnable prompts, hence, enabling consistent manifold matching between image features and text features. **(iii)** To ensure consistent local feature alignment, we utilize OT theory to reduce the distance between distributions of image and text. **(iv)** Extensive experimental results on eight classification datasets demonstrate the superiority of the proposed LICO against current interpretation methods in terms of quantitative and qualitative results. For practical applications, LICO does not introduce any computational overhead during inference while maintaining improved performance.

Figure 1: Motivation of LICO. (a) Existing post-hoc CAM-based methods focus on generating better weighting schemes to obtain weighted-sum attention maps. (b) Gradient-based methods tend to back-propagate gradients from logits to input space. (c) Proposed LICO incorporates learnable prompts to enable image features to approximate semantic information in latent space.

## 2 Related Work

Interpretation methods. Class activation mapping (CAM) is a simple approach to generating class-aware saliency maps for CNNs . Inspired by CAM's effectiveness, its variants including Grad-CAM , Grad-CAM++ , Score-CAM , RISE , and Group-CAM , were proposed to enhance the precision and interpretability of CNNs. Gradient-based methods such as Guided Back-propagation , SmoothGrad , and Integrated Gradient (IG)  compute the gradients from output logits back to input space. However, they are prone to yielding attribution noise that is inconsistent with human intuition. Some advanced techniques have been developed to reduce attribution noise by utilizing improved path integration theory, such as adaptive path method  and important direction method . Distinct from existing interpretation methods that focused on the post-processing of feature map weighting scheme and accurate gradients calculation based on a pre-trained model, our LICO aims to incorporate class-aware learnable prompts that reflect real-world semantic knowledge to guide latent image features. It is worth noting that LICO is compatible with current methods and can consistently improve their performance.

Prompt learning. Prompt learning stems from natural language processing (NLP), enabling large language models to directly model the prediction probability of raw text [14; 15; 16]. Following the proliferation of large VLMs, prompt learning began to surface in the realm of computer vision, significantly enhancing multi-modal tasks and text-guided image generation. To overcome the limitations of learning with fixed prompts used in CLIP, learnable prompts have been explored to provide more generalized performance on downstream tasks. CoOp firstly proposed a framework that constructs learnable prompts to align image and text in latent space . Furthermore, to enable CoOp with a stronger capability to perceive the knowledge of new classes, the authors proposed CoCoOp in which the prompts are learned conditioned on image features . PLOT correlated one given image with multiple prompts of its class by optimal transport, leading different prompts to reflect features with respect to different regions within this image . In our LICO, we propose that learnable prompt tokens in one sentence should be registered to certain parts of an image, as shown in Fig. 1(c).

Optimal transport.Optimal transport (OT) is a typical metric for measuring discrepancy between two distributions, requiring solving a linear programming problem [20; 21]. Thanks to some fast alternatives, OT has been widely utilized in deep learning, including graph cross-domain alignment [22; 23], domain adaptation [24; 25; 26], optimal graph matching for vessel image registration , and multi-modal distribution alignment [28; 29]. Although KL-divergence can effectively measure the cost of which the predicted distribution approximates ground-truth distribution, given the normalized image and text distributions, it is intractable to guarantee they share a common metric space. Therefore, we propose to utilize OT to tackle fine-grained cross-modal alignment.

Figure 2: Framework of the proposed LICO. (a) Conventional classification pipeline of DNNs. (b) Language feature extraction with pre-trained text encoder. (c) Manifold matching among samples and optimal transport alignment between feature maps and prompt tokens within each sample.

## 3 Methodology

### Overview of LICO

Fig. 2 presents the overview of LICO framework. Given a mini-batch of \(B\) training samples \(\{(_{i},y_{i},t_{i})\}_{i=1}^{B}\), the input image \(_{i}\) is of size \(H W C\), where \(C\) is the number of channels, \(H\) and \(W\) are height and width, respectively. \(y_{i}\) denotes the class label, and \(t_{i}\) represents the text of corresponding label \(y_{i}\). We denote the pre-trained CLIP text encoder as \(g_{}\) while we fixed its parameters \(\) during training. The image encoder is denoted as \(f_{}\) that is to be optimized. The image encoder takes \(_{i}\) as input and outputs feature maps \(_{i}\) with \(N\) channels. Note that \(_{i}\) is a tensor, which is then flattened to one-dimension for each channel; _i.e._, \(_{i}^{N d^{}}\). For the language modeling branch, the text encoder \(g_{}\) takes as input the constructed learnable prompt: \(_{i}=[X_{1},X_{2},,X_{M-1},t_{i}]\), where \([]\) denotes the concatenation, \(t_{i}\) is the text corresponding to label of the \(i\)-th sample, \(X_{m}\) are learnable context tokens, and \(M-1\) is the number of context tokens. Through the pre-trained text encoder, the prompt vector \(_{i}\) is mapped into a continuous \(d\)-dimension space; _i.e._, \(d=512\) in CLIP.

In order to measure the distances between image and language features in latent space, we append a mapping net \(h_{}\), a multilayer perceptron (MLP) with only one hidden layer, to map the \(d\)-dimension language features \(_{i}^{M d}\) to the space of \(d^{}\)-dimension, _i.e._, \(_{i}^{M d^{}}\), which is the same as that of image features \(_{i}\). The structure of \(h_{}\) varies along different image encoders. We use \(h_{}[a,b]\) to describe its structure, _i.e._, numbers of hidden units and output units are \(a\) and \(b\), respectively. Note that in this paper, we do not intend to modify the structure of the existing image and text encoders so that we can obtain the feature maps and corresponding attention maps that are comparable to existing methods. Consequently, we utilize \(_{i}\) and \(_{i}\) to preserve the language-image-consistent structure in latent space using manifold matching (Section 3.2) and fine-grained feature-language alignment by optimal transport (Section 3.3).

### Language-Image Manifold Matching

To preserve the global manifold structure in latent space, we first measure the relationships, _i.e._ the distances or similarities, among training images and corresponding class-aware prompts.

Although the class categories are discretely distributed that it is infeasible to capture their manifold, we can map them into a continuous space by constructing a prompt vector for each class, then mapping it to a latent space using a pre-trained text encoder. Therefore, we can enforce the image manifold  to approximate the language manifold.

In practice, we align the adjacent matrices of language and image features. Specifically, the language adjacent matrix is denoted as \(_{B B}^{G}\), and that of image is \(_{B B}^{F}\):

\[_{i,j}^{F}=(-D(_{i},_{j})/)}{_{s=1}^ {B}(-D(_{i},_{s})/)}, 28.452756pt_{i,j}^{G}= (-D(_{i},_{j})/)}{_{s=1}^{B}(-D( _{i},_{s})/)},\] (1)

where \(D(,)\) calculates the distance between two images or two prompts, such as Euclidean distance, \(\) is the temperature to be learned during training. Then, each image is formulated as a distribution \(_{i,:}^{F}\) where each dimension denotes the distance from the \(i\)-th sample to others within a mini-batch. Similar to the class-wise prompts, \(_{i,:}^{G}\) implies the relationships between the class prompt of the \(i\)-th sample and those of others.

Recall our assumption that the large amounts of image-text pairs used in large VLMs, such as CLIP, can lead to a generalized text encoder that well establishes a real-world semantic space. Therefore, for certain downstream tasks and datasets, we aim to introduce the knowledge of this semantic space to the latent space of the target image domain. Then, we propose a manifold matching loss \(_{}\), which enables the manifold of image features to approach that of prompt features using KL-divergence:

\[_{}=_{i=1}^{B}[_{ i,:}^{G}\|_{i,:}^{F}].\] (2)

We note that we do not consider the inverse version \([_{i,:}^{F}\|_{i,:}^{G}]\) since LICO focuses on enabling image manifold to approach the manifold of language prompts.

### Feature Distribution Alignment by Optimal Transport

While we strive for a coarse alignment of the global manifold structure, it is crucial to establish a correlation between prompt tokens and specific feature maps for each sample, which aids in mitigating the influence of redundant features on the generation of attention maps. Most importantly, the critical feature maps that are most related to the target class should possess the highest similarity with respect to the class token.

Unfortunately, it is challenging to determine or assign which of the \(N\) feature maps are correlated with certain prompt tokens. In this paper, we propose to align \(_{i}=[_{1},_{2},,_{M-1},_{t_{i}}]^{} ^{M d^{}}\) and \(_{i}=[_{1},_{2},,_{N}]^{}^{N  d^{}}\) for achieving consistency between feature maps and specific prompt tokens. In other words, different words in a sentence should correspond to parts of an image, and in contrast, partial regions in an image reflect the semantic information delivered by certain tokens. Therefore, it is intractable to measure this distance using KL-divergence since it is not a strict metric, _i.e._, it does not satisfy the property of triangle inequality. In LIGO, we use optimal transport, which is widely used in measuring distances of two distributions, to align distributions of normalized visual features and prompt features.

For the given image feature maps \(_{i}^{N d^{}}\) and a prompt tokens \(_{i}^{M d^{}}\), we construct two discrete distributions:

\[=_{n=1}^{N}u_{n}_{_{n}},= _{m=1}^{M}v_{m}_{_{m}},\] (3)

where \(_{_{n}}\) is a Dirac function centered at \(_{n}\), so as to \(_{_{m}}\), and the weights \(=\{u_{n}\}_{n=1}^{N}_{N}\) and \(=\{v_{m}\}_{m=1}^{M}_{M}\), \(\) denotes the \(N\)- and \(M\)-dimensional probability simplex, _i.e._, \(_{n=1}^{N}u_{n}=1\), \(_{m=1}^{M}v_{m}=1\). Finally, the discrete OT distance for one sample is defined as follows:

\[D_{}(,)= _{(,)}_{(,)}[(,)]=_{(,)} _{n=1}^{N}_{m=1}^{M}_{n,m} c(_{n}, _{m}),\] (4) s.t. \[_{m}=,_{n}=,\]

where \(^{N M}\) represents the cost matrix in which each element \(c(_{n},_{m})\) denotes transportation cost between \(_{n}\) and \(_{m}\). \(^{N M}\) is the transport plan that is to be optimized and \((,)\) denotes the transportation polytope that contains all joint probabilities of \(\) and \(\). In practice, solving the optimization problem in Eq. (4) often equips with a high computation cost. Thus we use the Sinkhorn algorithm, which is more computationally amenable, to solve an entropy-constrained problem :

\[D_{}(,)=_{(,)} _{n=1}^{N}_{m=1}^{M}_{n,m} c(_{n},_{m })-(),}_{m}=, _{n}=,\] (5)

where \(\) is Lagrange multiplier and \(()=_{n,m}_{n,m}_{n,m}\). Then after a few iterations, we obtain the optimal solutions:

\[^{*}=(^{t})(-/)( {v}^{t}),\] (6)

where \(t\) is the iteration step. \(^{t}\) and \(^{t}\) are updated according to following rules:

\[^{t}=/((-/)^{t-1}),^{ t}=/((-/)^{}^{t}).\] (7)

Dynamic context (DC).To endow each image with diverse prompt tokens, we shuffle the learnable context tokens in each training iteration referring to the training procedure in Algorithm 1.

### Final Objective Function

The final training loss function is as follows:

\[=_{}+_{}+_{},\] (8)

where \(_{}\) is the cross-entropy loss, \(_{}=_{i=1}^{B}D_{}\), \(\) and \(\) are hyperparameters for adjusting different terms. During the inference phase, we apply the trained image encoder and classifier to conduct conventional classification that yields the predicted probability of a given input image. Note that the text encoder and the MLP mapping do not affect the inference procedure. The detailed algorithm can be found in Algorithm 1.

```
0: Training set \(\), total epochs \(U\), image encoder \(f_{}\), text encoder \(g_{}\), MLP \(h_{}\), learnable prompts \(_{i}=[X_{1},X_{2},,,X_{M-1},t_{i}]\).
0: Image encoder with optimal parameter \(^{*}\).
1:for\(u=1\) to \(U\)do
2: Sample a mini-batch of (\(\{_{i},_{i},y_{i}\}_{i=1}^{B}\)) from \(\).
3: Randomly shuffling \(X_{m}\) and \(t_{i}\). \(\) Dynamic context
4:\(_{i}\) = \(f_{}(_{i})\), \(_{i}\) = \(h_{}(g_{}(_{i}))\). \(\) Image and text features
5: Calculate \(_{B B}^{G}\), \(_{B B}^{F}\)\(\) Adjacient matrices
6: Calculate \(_{}\) according to Eq. (2). \(\) Coarse alignment by manifold
7: Optimal transport plan \(^{*}\) by Eq. (6), then calculate \(_{}\). \(\) Fine-grained alignment by OT
8: Classifier \(\)\(_{i}\)
9: Total loss: \(=_{}+_{}+_{}\), update \(\), \(\), and \(X_{m}\) by gradients: \(}{},}{ },}{}\).
10:endfor ```

**Algorithm 1** Training Algorithm of LICO.

## 4 Experiments

### Datasets

This paper focuses on image classification task and evaluates the proposed LICO on well-known datasets, including ImageNet-1k , CIFAR-10/100 , and SVHN . We conduct the classification experiments under the setting of limited training data in which the splits of labeled data follow the previous works for fair comparison [35; 36]. Furthermore, we conduct fine-grained classification on typical benchmarks, including CUB-200 , FGVC-Aircraft , Stanford Cars-196 , VGG Flowers . The evaluation is carried out on both the full dataset and few-shot settings, following the procedure of CGC .

### Implementation Details

We employ the ViT-B/32 trained by CLIP  as the text encoder and its parameters are fixed during training. The output dimension of this text encoder is \(512\) for each token. The image encoders in our experiments vary along different datasets and training settings. Specifically, for the ImageNet experiments, we utilize ResNet-50 as the image classifier . In doing so, the convolutional layers preceding the final linear layer constitute the image encoder of LICO. For the CIFAR-10/100 and SVHN, we follow the experimental settings in [35; 36], the classification network is Wide ResNet (WRN) . We further conduct the same experiments using another network, the PreAct-ResNet-18 (PARN-18) . For fine-grained classifications on CUB-200, Standford-Cars, Aircraft, and VGG Flowers datasets, we applied the same settings used in CGC for fair comparison , where the image encoder is also the ResNet-50 that has been pre-trained on ImageNet-1k with CE loss. Note that for all the experiments of LICO, we only use the trained image encoder and the classifier during the inference phase. The text encoder, learnable prompt contexts, and MLP mapping net are dropped, thus, will not compromise the computational efficiency. Hyperparameters \(\) and \(\) are set as \(10\) and \(1\), respectively.

All the experiments are implemented by PyTorch . The learning rates for ImageNet, CIFAR-10/100, and SVHN are of \(0.03\) with a consine rate decay schedule, _i.e._, \(=_{0}()\), where \(_{0}\) denotes the initial learning rate and \(k\) is the index of training step . We use a standard stochastic gradient descent (SGD) optimizer with a momentum of \(0.9\)[47; 48], and the weight decay is \(0.0001\). The training batch sizes are \(128\) and \(64\) for ImageNet and other datasets, respectively. Specifically, the mapping net for ResNet-50 is \(h_{}\), \(h_{}\) for WRN, and \(h_{}\) for PARN-18. The total training epoch is \(90\) for ImageNet and \(200\) for others. The experiments were trained on four NVIDIA A100 GPUs for ImageNet-1k and one GPU for other datasets.

### Comparison of Interpretation Capability

In this experiment, we compare interpretation results of popular methods, including Grad-CAM , Grad-CAM++ , RISE , Score-CAM , Group-CAM , and CGC . Note that LICO is compatible with these post-hoc interpretation methods so that in our experiments, we compare the saliency maps obtained by these interpretation methods using baseline ImageNet pre-trained model and LICO trained counterparts of the same architectures.

Qualitative results.Qualitatively, we compare the saliency maps obtained by different interpretation methods in Fig. 3 (Left). We can see that for single object images, LICO can effectively help baseline methods cover more comprehensive and discriminative regions, _e.g._, the head and fur of a bird. However, the baseline methods are inferior in capturing all the objects in the multi-object image. Please refer to Appendix A for more results of multi-class and multi-object cases.

Quantitative results.To achieve quantitative evaluation, we follow  to conduct Insertion and Deletion tests. Insertion gradually introduces class-related regions (3.6% pixels) of an original image to a blurred image according to the values of the saliency map. This process is repeated until the blurred image is fully recovered. In contrast, Deletion aims to replace related pixels (3.6%) in a blurred image with those of the corresponding original image. In Table 1, we provide the AUC of the classification score after Softmax. For most of the interpretation methods, LICO consistently improves the Insertion and Deletion values. Although the insertion value of Group-CAM and deletion value of Grad-CAM is better than LICO, the LICO still achieves the best overall values. We also report the quantitative results of corresponding cases with different interpretation methods in Fig. 3 (Right). Please refer to Appendix B for the experiment of Pointing Game .

Sanity checks.Sanity check for saliency maps was first proposed in , which is a qualitative test aiming at evaluating whether the saliency maps are sensitive to model parameters. We conducted two types of test: cascading randomization from top to bottom layers and independent randomizing of

   Method & Grad-CAM++ & Grad-CAM & RISE & Score-CAM & Group-CAM & CGC \\  Insertion\(\) & 50.0/**51.2** & 53.5/**57.1** & 54.0/**54.9** & 55.1/**55.6** & **56.8**/55.2 & 52.2/**55.4** \\ Deletion\(\) & 14.8/**11.7** & **13.3**/15.1 & 11.7/**10.8** & 11.5/**11.2** & 12.3/**10.5** & -/15.8 \\ Overall\(\) & 35.2/**39.5** & 40.2/**42.0** & 43.6/**44.1** & 42.3/**44.4** & 44.5/**44.7** & -/39.6 \\   

Table 1: Quantitative comparisons of different interpretation methods on ImageNet in terms of Insertion and Deletion. We report the results: “Baseline/+LICO”. \(Overall=Insertion-Deletion\).

Figure 3: Saliency maps (Left) and Insertion/Deletion curves (Right) on ImageNet-1k validation set.

[MISSING_PAGE_FAIL:8]

Fine-Grained classification.Furthermore, we evaluate LICO on a fine-grained classification problem that requires the model to capture fine-grained features. In Table 4, except for the Aircraft, LICO enhances classification performance under all settings compared with CGC. This observation highlights that contrastive learning among attention maps used in CGC primarily emphasizes similarities between target images and other random/augmented images, while disregarding the measurement of semantic distinctions. For the Aircraft dataset, it is difficult for LICO to achieve significant improvements due to the categorical labels are types of aircraft, _e.g._, the numerical symbols such as 727-200 which are challenging for CLIP text encoder to achieve meaningful embedding, and CLIP has demonstrated relatively lower accuracy on some out of distribution data like aircraft and satellite. To address the challenges, we incorporate prior text knowledge by constructing initial prompts as "a type of aircraft", which allows LICO to improve performance in the few-shot settings while achieving comparable results to CGC under the full setting. This strategy has also been verified in CoCoOp .

### Ablation Study

Ablation on manifold matching and OT alignment.In Table 5, we investigate the effectiveness of \(_{}\) and \(_{}\). We can see that only using \(_{}\) obtains more performance drop than that of \(_{}\). This indicates that the global consistency of manifolds guarantees the basic performance, and OT only focuses on local feature alignments so that it cannot be sensitive to relationships between intra- and inter-class samples.

Ablation on number of context tokens.In Table 6, we evaluate the performances influenced by different numbers of learnable tokens. We find that 12 is the best choice in our experiments, and the settings of 16 and 20 are relatively better than those of 4 and 8.

Ablation on distance function \(D\) in Eq. (1).In Table 7, we compare the similarity functions used in manifold matching, which measures distances among samples within a mini-batch. We can see that the Euclidean distance is more suitable to our LICO.

Please refer to Appendices E, F, and G for more ablation studies on DC, effects of different type of text encoders, and frozen parameters of prompts, respectively.

   no. & Top-1 & Top-5 & Insertion & Deletion \\ 
0 & 75.64 & 91.92 & 54.1 & 17.8 \\
4 & 76.03 & 92.74 & 55.2 & 17.5 \\
8 & 76.09 & 92.89 & 56.3 & 16.0 \\
12 & **76.27** & **92.99** & **57.1** & **15.1** \\
16 & 76.21 & 92.87 & 57.0 & 15.8 \\
20 & 76.14 & 92.93 & 56.9 & 15.5 \\   

Table 6: Ablation on no. of context tokens.

   Method & 1-shot & 5-shot & 10-shot & Full & 1-shot & 5-shot & 10-shot & Full \\   &  &  \\   & 13.7\({}_{ 0.3}\) & 51.7\({}_{ 0.3}\) & 66.4\({}_{ 0.2}\) & 80.1\({}_{ 0.9}\) & 6.1\({}_{ 0.2}\) & 34.3\({}_{ 0.4}\) & 61.1\({}_{ 0.4}\) & 89.7\({}_{ 0.1}\) \\  & 15.8\({}_{ 0.3}\) & 55.2\({}_{ 0.3}\) & **68.4\({}_{ 0.3}\)** & 81.5\({}_{ 0.1}\) & 6.5\({}_{ 0.2}\) & 36.5\({}_{ 0.4}\) & 63.0\({}_{ 0.4}\) & 90.3\({}_{ 0.1}\) \\  & **16.9\({}_{ 0.2}\)** & **55.8\({}_{ 0.4}\)** & **68.4\({}_{ 0.2}\)** & **82.7\({}_{ 0.3}\)** & **7.1\({}_{ 0.3}\)** & **37.2\({}_{ 0.5}\)** & **64.4\({}_{ 0.4}\)** & **91.5\({}_{ 0.2}\)** \\    &  \\   & 7.7\({}_{ 0.3}\) & 25.7\({}_{ 0.4}\) & 41.4\({}_{ 0.3}\) & 83.7\({}_{ 0.2}\) & 52.1\({}_{ 0.5}\) & 85.6\({}_{ 0.4}\) & 93.2\({}_{ 0.2}\) & 96.1\({}_{ 0.2}\) \\  & 8.0\({}_{ 0.3}\) & 26.9\({}_{ 0.4}\) & 42.9\({}_{ 0.3}\) & **85.7\({}_{ 0.2}\)** & 53.3\({}_{ 0.5}\) & 85.8\({}_{ 0.4}\) & **93.4\({}_{ 0.2}\)** & 96.2\({}_{ 0.2}\) \\  & **8.4\({}_{ 0.4}\)** & **27.5\({}_{ 0.2}\)** & **43.1\({}_{ 0.4}\)** & 85.6\({}_{ 0.2}\)** & **55.6\({}_{ 0.4}\)** & **86.2\({}_{ 0.3}\)** & **93.4\({}_{ 0.4}\)** & **96.8\({}_{ 0.3}\)** \\   

Table 4: Fine-grained classification accuracy under full and few-shot settings.

   \(_{}\) & \(_{}\) & \(_{}\) & Top-1 & Top-5 & Insert. & Delet. \\  _{}\) and \(_{}\). We can see that only using \(_{}\) obtains more performance drop than that of \(_{}\). This indicates that the global consistency of manifolds guarantees the basic performance, and OT only focuses on local feature alignments so that it cannot be sensitive to relationships between intra- and inter-class samples.

   Dataset & Euclidean & Cosine \\  CIFAR-10 & **95.78\({}_{ 0.08}\)** & 95.46\({}_{ 0.12}\) \\ CIFAR-100 & **82.22\({}_{ 0.02}\)** & 81.85\({}_{ 0.05}\) \\ SVHN & **98.25\({}_{ 0.06}\)** & 98.21\({}_{ 0.06}\) \\ CUB-200 & **82.70\({}_{ 0.30}\)** & 82.10\({}_{ 0.30}\) \\ Flowers & **96.80\({}_{ 0.30}\)** & 96.20\({}_{ 0.40}\) \\   

Table 7: Ablation on distance function.

Conclusion

In this paper, we proposed LICO to enhance existing visual interpretation methods by incorporating language information, which is compatible with these methods. The LICO aligns image and prompt embeddings globally using manifold matching, simultaneously aligns feature maps with corresponding learnable context tokens by applying optimal transport alignment. Extensive experiments on evaluating interpretation capability and classification performance exhibit both quantitative and qualitative enhancement introduced by LICO. A key limitation of LICO is that it depends on a trainable MLP to project language embeddings into a metric space of same dimension with that of image features, where the output dimension of such MLPs varies according to different image encoders.

Broader ImpactsLICO explores effective visual interpretations of DNNs by introducing language knowledge, which is orthogonal to existing post-hoc interpretation methods. An important merit of LICO is to enhance interpretability while achieving competitive or even better classification performance, applicable to various kinds of tasks and models effectively.