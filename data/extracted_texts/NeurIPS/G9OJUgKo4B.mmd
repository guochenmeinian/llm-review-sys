# Knowledge Composition using Task Vectors with Learned Anisotropic Scaling

Frederic Z. Zhang\({}^{*}\) Paul Albert\({}^{*}\) Cristian Rodriguez-Opazo

**Anton van den Hengel Ehsan Abbasnejad**

Australian Institute for Machine Learning  The University of Adelaide

{firstname.lastname}@adelaide.edu.au

https://github.com/fredzzhang/atlas

Equal contribution. Listed order was determined by a coin toss. Fred formalised the idea; developed the original codebase; conducted experiments on task arithmetic; and drafted the paper. Paul designed and conducted experiments on few-shot recognition, test-time adaptation, parameter-efficient fine-tuning; investigated numerous properties of task vector compositions; and was crucially involved in every stage of the work.

###### Abstract

Pre-trained models produce strong generic representations that can be adapted via fine-tuning on specialised datasets. The learned weight difference relative to the pre-trained model, known as a task vector, characterises the direction and stride of fine-tuning that enables the model to capture these specialised representations. The significance of task vectors is such that simple arithmetic operations on them can be used to combine diverse representations from different domains. This paper builds on these properties of task vectors and aims to answer (1) whether components of task vectors, particularly parameter blocks, exhibit similar characteristics, and (2) how such blocks can be used to enhance knowledge composition and transfer. To this end, we introduce aTLAS, an algorithm that linearly combines parameter blocks with different learned coefficients, resulting in anisotropic scaling at the task vector level. We show that such linear combinations explicitly exploit the low intrinsic dimensionality of pre-trained models, with only a few coefficients being the learnable parameters. Furthermore, composition of parameter blocks enables modular learning that effectively leverages the already learned representations, thereby reducing the dependency on large amounts of data. We demonstrate the effectiveness of our method in task arithmetic, few-shot recognition and test-time adaptation, with supervised or unsupervised objectives. In particular, we show that (1) learned anisotropic scaling allows task vectors to be more disentangled, causing less interference in composition; (2) task vector composition excels with scarce or no labelled data and is less prone to domain shift, thus leading to better generalisability; (3) mixing the most informative parameter blocks across different task vectors prior to training can reduce the memory footprint and improve the flexibility of knowledge transfer. Moreover, we show the potential of aTLAS as a parameter-efficient fine-tuning method, particularly with less data, and demonstrate that it can be easily scaled up for higher performance.

## 1 Introduction

One practical advantage of neural networks is the fact that knowledge learned from a previous problem, in the form of network weights, can be transferred to solve other related problems. Commonly referred to as transfer learning [6; 73], this technique is often applied when a model trained on a general-purpose dataset--ImageNet  for many years--is fine-tuned on other datasets to improveperformance on downstream problems. In the past, classification models  have been used as the medium for such knowledge transfer, which played a crucial part in the success of detection and segmentation . In recent years, foundation models  trained on broad data, CLIP  particularly, have demonstrated strong performance on a multitude of tasks, even when applied in a zero-shot manner. Besides the conventional way of exploiting the knowledge in these models via fine-tuning, recent works  have presented more direct measures to manipulate the network weights. In particular, Ilharco et al.  showed that, a task vector, defined as the weight difference between a pre-trained and a fine-tuned model, can be used as a carrier of the task-specific knowledge learned via fine-tuning. As such, multiple task vectors, when combined with simple arithmetic, can form a multi-task model that largely retains its performance across all fine-tuning tasks. Linearisation techniques , in addition, have been shown to further enhance this compositionality.

Intrigued by this phenomenon, we investigate the potential of task vectors being knowledge carriers in this paper, by learning linear combinations of them (Figure 0(a)) for various problems. In particular, parameter blocks, e.g., weights and biases, tend to encode different learned representations in different layers. We thus learn an independent scaling coefficient per block for more precise adjustments tailored to the unique roles of each parameter block. This results in anisotropic scaling of task vectors (Figure 0(b)), and allows us to exploit their modularity in knowledge composition, granting higher controllability when steering the behaviours of a model for task arithmetic .

The potential applications of task vector composition extend beyond model editing. With the coefficients being the only learnable parameters, our method exploits the rich knowledge encapsulated in the task vectors by searching in a low-dimensional coefficient space. As a result, it is a competitive parameter-efficient fine-tuning (PEFT) method, and is particularly effective in cases where labelled data is scarce. This offers new opportunities for few-shot learning  and test-time adaptation . Furthermore, for multi-purpose models such as CLIP , variants of the model trained with different data sources or fine-tuned on different downstream tasks are often available . These resources constitute a significant knowledge bank, with task vectors being the knowledge carrier. Many learning problems may be simplified to learning a combination of task vectors.

Our primary contribution is a learning algorithm named aTLAS, wherein otherwise complex learning problems can be framed as learning linear combinations of task vectors. The algorithm is broadly applicable to optimising supervised and unsupervised objectives. Its effectiveness is demonstrated in task arithmetic, few-shot recognition, test-time adaptation and parameter-efficient fine-tuning, where we show that (1) learning linear combinations of task vectors directly exploits the low intrinsic dimensionality of pre-trained models , resulting in a small number of learnable parameters; (2) standard task vectors, otherwise inferior to linearised variants  in task arithmetic, can produce stronger multi-task models with learned anisotropic scaling; (3) aTLAS is effective in low-data regimes, and improves the accuracy of CLIP by \(6.5\) absolute points averaged over \(22\) datasets with unlabelled data; (4) aTLAS is complementary to previous few-shot adaptation methods, in that one third of the examples it improves upon are unique; (5) aTLAS as a few-shot learning method is less prone to domain shift, and achieves better generalisation on out-of-domain datasets; (6) the most informative parameter blocks from different task vectors can be mixed prior to training, allowing for flexible and efficient knowledge transfer under memory constraints; (7) aTLAS is a strong PEFT method when data is limited, and existing PEFT methods such as low-rank adaptations (LoRA)  can be seamlessly integrated into aTLAS to improve memory efficiency.

Figure 1: Illustration of (a) learning task vector compositions (\(n=2\), \(_{0}\) denotes the weights of a pre-trained model) and (b) the flexibility of anisotropic scaling. Assume a task vector \(=(^{(1)},^{(2)})\) has two parameter blocks, learning anisotropic scaling grants more flexibility when combining task vectors.

Models and task vectors

As Ilharco et al.  demonstrated, task vectors exhibit many intriguing properties across a wide range of models, such as CLIP , GPT-2  and T5-based models . To facilitate more in-depth experimentation and analysis, we focus on the CLIP model in this paper, due to its wide availability and manageable size. In particular, we follow previous practice [44; 28] and acquire task vectors by fine-tuning the image encoder, with the text representations frozen. This ensures that image encoders fine-tuned on different datasets produce features residing in the same representation space, through a common text encoder. The task vectors obtained from these fine-tuned encoders can thus be combined more effectively to form a unified multi-task model.

Formally, denote the CLIP image encoder by \(f:\), such that for input image \(\) and parameters \(\), \(=f(;)\) is the learned latent representation for the input image. Denote the weights of a pre-trained model by \(_{0}\), and the weights of its fine-tuned variant by \(_{i}\), \(i^{+}\), where \(i\) indexes a dataset \(_{i}\). We follow Ilharco et al.  and define a task vector as \(_{i}=_{i}-_{0}\). In addition, we investigate task vectors produced by linearised variants of the image encoder using the first-order Taylor expansion,

\[g(;) f(;_{ 0})+(-_{0})^{}_{ }f(;_{0}).\] (1)

Ortiz-Jimenez et al.  showed that, task vectors obtained from fine-tuning the linearised variants have low disentanglement errors, and exhibit strong compositional properties.

## 3 Learning task vector compositions

Parameters in a neural network, depending on the depth of the layer, often have different significance. For instance, early layers in convolutional neural networks [18; 53] are known for extracting generic, low-level features, such as edges, corners, etc., while deeper layers produce features more specific to the task. We recognise the non-uniform impacts parameters at different layers can have, and do not perform isotropic scaling on task vectors. Instead, weights, biases and any other forms of parameterisation, which we collectively refer to as _parameter blocks_, will be scaled independently.

### Proposed method: aTLAS

Formally, denote a task vector with \(m\) parameter blocks by \(=^{(1)},,^{ (m)}\), where each parameter block \(^{(j)}\) is vectorised, and round brackets denote column vector concatenation. We learn a block diagonal matrix \(\), parameterised as

\[=^{(1)}I^{(1)}&&\\ &&\\ &&^{(m)}I^{(m)},\] (2)

where \(^{(j)}\) is a learnable coefficient; \(I^{(j)}\) denotes an identity matrix with its number of columns matching the dimension of \(^{(j)}\); and the superscript \(j^{+}\) indexes a parameter block. This results in anisotropic scaling of a task vector, that is,

\[_{i}_{i}=^{(1)}_{i}^{ (1)}_{i},,^{(m)}_{i}^{(m)}_{i},\] (3)

where the subscript \(i^{+}\) indexes a task vector. As such, assuming a supervised objective, finding the optimal composition of task vectors can be defined as the following optimisation problem

\[,,_{n}}{}\ _{( ,)_{t}}f(;_{0}+_{i=1}^{n}_{i}_{i}), ,\] (4)

where \(\) is the loss function for a target task; \(n\) is the number of task vectors; \(\) is the labels corresponding to inputs \(\); \(_{t}\) denotes a target dataset. The number of learnable parameters, as a result, is precisely \(mn\), Let us denote the solution to the aforementioned optimisation problem by \(\{^{*}_{i}\}_{i=1}^{n}\). In inference, model \(f(,_{0}+_{i=1}^{n}^{*}_{i}_{i})\) will be deployed, which incurs no additional computational cost compared to models trained in the conventional way.

In addition, we investigate the task vectors obtained from fine-tuning linearised variants of the model, i.e., \(g(x)\) in Eq. 1. Denote such task vectors by \(}\). The learning objective with linearised task vectors can be derived as follows

\[,,_{n}}{}\ _{(, )_{}}f(; _{0})+(_{i=1}^{n}_{i}}_{i})^{}_{}f(;_{0}), .\] (5)

### Relation to intrinsic dimensionality

A notable characteristic of aTLAS is its parameter efficiency. To offer more intuitions, we refer to previous findings [1; 33] that deep neural networks often produce solutions residing in a subspace with much lower intrinsic dimensionality. This is measured by finding a minimum number of \(d\) parameters, such that learning these parameters (\(}^{d}\)) leads to approximately the same performance as optimising in the full parameter space (\(^{D}\)). This can be expressed as follows

\[=_{0}+P},\] (6)

where \(_{0}^{D}\) denotes the pre-trained weights and \(P^{D d}\) is a random projection matrix. We demonstrate that learning task vector compositions leads to the same formulation. For brevity of exposition, let us consider compositions at the block level. For the \(j\)-th parameter block, we have

\[^{(j)} =_{0}^{(j)}+_{i=1}^{n}_{i}^{(j)} _{i}^{(j)}\] (7) \[=_{0}^{(j)}+_{1}^{(j)}, ,_{n}^{(j)}}_{}_{1}^{(j)},,_{n}^{(j)}^{}}_{}.\] (8)

We draw a parallel between Eqs. 6 and 8 and note that aTLAS explicitly exploits the low intrinsic dimensionality by learning a small set of coefficients. The number of task vectors, i.e., \(n\), is much smaller than the dimension of weight vector \(_{i}^{(j)}\), and is analogous to the intrinsic dimensionality \(d\). However, as opposed to using a random projection matrix \(P\), aTLAS constructs the projection matrix from task vectors, making use of the learned representations. To demonstrate its advantage, we use the same number of bases for task vectors2 and random bases3, and show that task vectors consistently achieve higher performance in Figure 2. These results solidify our understanding of task vectors being knowledge carriers. We thus set out to apply aTLAS to various applications.

## 4 Task arithmetic

Task arithmetic  is comprised of a few tasks aimed at editing pre-trained models using task vectors. Following previous practice [28; 44], we conduct experiments under the settings of task negation and task addition on eight image classification datasets (details included in Appendix A).

Figure 2: Recognition accuracy versus the number of bases when optimising in a low-dimensional subspace. The accuracy is normalised by that of the fully fine-tuned model. Using task vectors to construct the projection matrix performs consistently better than using random bases on (a) MNIST , (b) CIFAR100 .

Previous works acquire the optimal isotropic scaling factor on task vectors via a hyper-parameter search on validation sets. As such, we learn anisotropic scaling matrices on the same validation sets, and visualise the learned coefficients to shed light on this mechanism.

### Task negation

Task negation aims to reduce undesired biases, characterised by the performance, on a target task, while maintaining performance on a control dataset, ImageNet  in this case. Denote the validation sets for the target and control tasks by \(_{t}\) and \(_{c}\), respectively. We perform a simultaneous gradient ascent on the target task and gradient descent on the control task, described as follows,

\[*{arg\,min}_{_{t}}_{(,) _{t}}[-(f(;_{0}+ _{t}_{t}),)]+_{(, )_{c}}[(f(;_{0 }+_{t}_{t}),)],\] (9)

where \(_{t}\) is the task vector for the target dataset, and cross-entropy loss is used. The learning objectives with linearised task vectors can be derived easily based on Eq. 5, and so are omitted.

We summarise the task negation results in Table 1, and show that our method significantly improves upon standard task vectors, while the improvement upon linear task vectors is less prominent. In particular, we observe that weights matrices tend to have much larger negative coefficients, as shown in Figure 2(a). To investigate this, we instead only learn coefficients for the weight matrices, with zero coefficients on other parameter blocks, effectively reducing the number of learnable parameters by two thirds. With ViT-B/32 as the backbone, we observe an average accuracy of \(20.14\) (vs. \(18.76\)) on target tasks and \(61.23\) (vs. \(61.21\)) on the control task, which shows that weight matrices carry majority of the knowledge required for task negation.

### Task addition

Task addition aims at producing a multi-task model using task vectors acquired from a range of datasets. We utilise task vectors from the eight image classification datasets, and learn the anisotropic

    & & &  &  &  \\  T.V. & Methods & Models & Target (\(\)) & Control (\(\)) & Target (\(\)) & Control (\(\)) & Target (\(\)) & Control (\(\)) \\  n/a & Pre-trained & \(f(;_{0})\) & 48.14 & 63.35 & 55.48 & 68.33 & 64.89 & 75.54 \\  _{}^{}\)} & Search & \(f(;_{0}+)\) & 23.22 & 60.71 & 19.38 & 64.66 & 19.15 & 72.05 \\  & aTLAS (ours) & \(f(;_{0}+)\) & **18.76** & **61.21** & **17.34** & **65.84** & **17.75** & **73.28** \\  _{}^{}\)} & Search & \(g(;_{0}+})\) & 11.54 & 60.74 & 10.88 & 65.54 & 12.78 & 72.95 \\  & aTLAS (ours) & \(g(;_{0}+})\) & **11.06** & **61.02** & **10.16** & **65.58** & **12.61** & **73.14** \\   

Table 1: Performance of task negation averaged across eight datasets. Selected results must maintain at least 95% of the pre-trained accuracy on the control dataset, following previous practice . Best performance in each section is highlighted in bold. Task vector is abbreviated as t.v. Results for each dataset are available in Table 7.

Figure 3: Box-and-whisker plots for the learned coefficients. As each transformer layer consists of a fixed set of parameter blocks, we visualise the distribution of coefficients for these parameter blocks across all layers, for (a) task negation and (b) task addition, as well as (c) distribution of coefficients by layer. We denote the learnable LayerNorm parameters by \(\) and \(\). Weights and biases are denoted by \(W\) and \(\), respective, with attention layer parameters indexed by superscripts and the MLP parameters indexed by subscripts.

scaling matrices with the objectives described in Eqs. 4, 5 using the cross-entropy loss. The training data is comprised of the validation sets for all eight dataset, i.e., \(_{t}=_{i=1}^{8}_{i}\).

Performance comparison against previous methods is shown in Table 2, where our method yields substantial improvements. Interestingly, we note that with previous methods [28; 44], linear task vectors outperform the standard ones in terms of absolute accuracy, while the converse is true with our method. To investigate this, we compute the pairwise disentanglement error \(\), which measures the percentage of data with inconsistent predictions when two task vectors are combined (more details in Appendix C.2). Results in Figure 4 show that standard task vectors with learned anisotropic scaling achieve the lowest average error, indicating less interference in task vector composition. Along with higher fine-tuning accuracy, previously referred to as the non-linear advantage , standard task vectors demonstrate stronger performance in task addition.

Furthermore, we again observe that weight matrices have consistently larger coefficients in Figure 2(b), and learning coefficients on weight matrices alone results in an accuracy of \(84.17\) (vs. \(84.98\)) using ViT-B/32. This suggests that weight matrices in transformers are the primary knowledge carrier, which enabled knowledge composition and negation. Note that for better clarity in visualisation, we add \(L_{1}\) regularisation on the learned coefficients during learning, which causes marginal performance drop (\(84.23\) vs. \(84.98\)) but significantly improves interpretability. In addition, we observe substantially higher coefficients on deeper layers (Figure 2(c)). This aligns with our understanding that early layers extract generic features that do not vary significantly across datasets , while the deeper layers produce task-specific features and require more careful adaptations.

## 5 Knowledge transfer in low-data regimes

Beyond model editing for task arithmetic, we explore the idea of transferring existing knowledge in task vectors to previously unseen tasks. To this end, we use the CLIP  model and a total of \(22\) image classification datasets, each of which produces a task vector. We defer the details of datasets and the process to acquire task vectors to Appendix A. Denote the set of available task vectors by \(T=\{_{i}\}_{i=1}^{n}\), and the dataset corresponding to task vector \(_{i}\) by \(_{i}\). For each target dataset \(_{t}\), we

  & &  &  &  \\   & Methods & Models & Abs. (\(\)) & Rel. (\(\)) & Abs. (\(\)) & Rel. (\(\)) & Abs. (\(\)) & Rel. (\(\)) \\  n/a & Pre-trained & \(f(;_{0})\) & 48.14 & - & 55.48 & - & 64.89 & - \\    } & Search & \(f;_{0}+_{i}_{i}\) & 70.12 & 77.24 & 73.63 & 79.85 & 82.93 & 87.92 \\  & aTLAS (ours) & \(f(;_{0}+_{i}_{i}_{i})\) & **84.98** & **93.79** & **86.08** & **93.44** & **91.36** & **97.07** \\    } & Search & \(g;_{0}+_{i}}_{i}\) & 74.67 & 85.17 & 77.51 & 86.21 & 84.75 & 91.86 \\  & aTLAS (ours) & \(g(;_{0}+_{i}_{i}}_{i})\) & **83.42** & **95.42** & **85.38** & **95.10** & **88.65** & **96.12** \\  

Table 2: Performance of task addition averaged across eight datasets. We report the absolute accuracy (Abs.) and the relative accuracy (Rel.) with respect to the fine-tuned model. Best performance in each section is highlighted in bold. Task vector is abbreviated as t.v. Results for each dataset are available in Table 8.

Figure 4: Disentanglement errors between each pair of datasets. Each row reflects the percentage of data in the corresponding dataset that have altered predictions after combining two task vectors. Our method achieves stronger task addition performance as a result of less interference amongst task vectors.

learn task vector compositions using the subset \(T\{_{t}\}\), excluding the task vector for the target dataset to avoid information leakage. We test our method in few-shot and test-time adaptation, to demonstrate its effectiveness in low-data regimes. Notably, we observe that task vectors complement existing few-shot methods. Combining aTLAS with them thus leads to significant improvements.

### Few-shot adaptation

Few-shot recognition requires learning new objects or concepts using a limited amount labelled data--\(k\) per class for \(k\)-shot. Following previous practice , we approach this problem by adapting a pre-trained CLIP model  to each target dataset \(_{t}\). We use the subset of task vectors \(T\{_{t}\}\) and \(k\{1,2,4,8,16\}\) images from dataset \(_{t}\). During training, we adopt the cross-entropy loss and minimise objectives described in Eqs. 4 and 5 for standard and linear task vectors, respectively.

We compare against Tip-Adapter  and LP++  using CLIP with ViT-B/32 backbone, across \(22\) datasets over three random seeds, and summarise the results in Figure 4(a). We show that with \(k=1\), our approach, aTLAS, significantly outperforms previous methods, demonstrating the effectiveness of knowledge transfer with scarce labelled data. More importantly, we note that the idea of task vector composition is highly complementary to those presented in previous methods. As such, combining aTLAS with them results in significant improvements. This is also illustrated in Figure 4(b) as a Venn diagram, where we show the percentage of examples in the validation set that are incorrectly classified by the pre-trained model but correctly classified with few-shot methods. Out of the examples aTLAS improves upon, around half are unique compared against either Tip-Adapter or LP++, demonstrating its complementarity. We also found that standard task vectors generally perform better than their linearised counterparts, and so defer the results of linear task vectors to Appendix D.2.

In addition, due to the low number of learnable parameters, aTLAS exhibits strong generalisability. To demonstrate this, we learn task vector composition on ImageNet , and test it on out-of-domain (OOD) datasets: ImageNet-A , ImageNet-R , ImageNet-sketch  and ImageNetV2 . We summarise the results in Figure 4(c), which shows the performance difference against the pre-trained model. Notably, aTLAS is the only method that consistently improves upon the pre-trained model on OOD datasets, and combining aTLAS with other methods can improve their generalisability.

We also test our method and variants integrated with Tip-Adapter and LP++ using other backbones, including ViT-\(\{\)B/16, L/14\(\}\) and ResNet-\(\{50,101\}\), and find that the results are consistent with those for ViT-B/32. More details can be found in Appendix D.3.

### Task vector budget and selection

In practical applications, there may only be a limited number of task vectors available, or the number of task vectors used in training may be restricted due to memory constraints. To this end, we study the influence of task vector budget \(b\) on few-shot recognition performance. We experiment with four selection strategies: (1) random selection; (2) feature-based selection; (3) gradient-based selection; and (4) blockwise gradient-based selection. To elaborate, feature-based selection computes the mean image feature representation of each dataset, and selects \(b\) task vectors from datasets most similar

Figure 5: Few-shot experiment results averaged across \(22\) datasets and three seeds, showing (a) comparison against state-of-the-art few-shot methods with ViT-B/32 backbone and (b) percentage of images in the validation sets that become correctly classified after applying few-shot methods. We also show (c) performance difference compared to pre-trained CLIP model on OOD datasets. More detailed results are included in Appendix D.

to the target dataset. Gradient-based selection computes the gradient with respect to each of the learnable coefficients, and either select entire task vectors with the highest \(L_{1}\) gradient norm, or select task vectors with the highest blockwise gradient for the corresponding parameter block, and repeat the process for all parameter blocks. The blockwise selection therefore allows parameter blocks across different task vectors to be mixed prior to training. More details can be found in Appendix D.6.

For a task vector budget \(b\{1,2,5,10,15,21\}\), we summarise the few-shot recognition performance in Figure 6. First, we note that the accuracy of aTLAS does not plateau with the maximum number of task vectors available (21), indicating that more task vectors could be beneficial. Second, we find that selecting task vectors based on feature similarity is a simple yet effective approach with sufficient budgets (\(b>5\)). Selecting whole task vectors with gradient is less effective, generally on par with random selection. Nevertheless, the blockwise variant achieves the best accuracy, particularly for very low budgets (\(b\{1,2\}\)), as it is able to exploit knowledge from more task vectors than the budget dictates. We thus deduce that parameter blocks can function as knowledge carriers in isolation, independent of the task vectors to which they belong. In fact, a parameter block \(^{(1)}\) as part of the task vector \(=(^{(1)},,^{(m)})\) can be considered as a task vector by itself, i.e., \((^{(1)},,,)\). This modular nature underscores the potential of task vectors for flexible and efficient knowledge transfer.

### Test-time adaptation

Test-time adaptation (TTA) [35; 57; 59] assumes no labelled data is available for the target task, requiring the model to adapt in an unsupervised fashion. We conduct experiments under the offline adaptation setting, which allows access to the target dataset. We consider three categories of self-supervised techniques for TTA: constrastive objectives, entropy objectives and pseudo labelling. Contrastive objectives align representations of the same image under different data augmentations. For this category, we adopt SimCLR , a simple yet effective method. Entropy objectives encourage the pre-trained model to produce confident predictions on unseen datasets by minimising the entropy over the predictions. This technique was previously explored by Yang et al.  in model merging. While effective in simpler cases, it can lead to catastrophic collapse in TTA. Therefore, we utilise a state-of-the-art sharpness-aware entropy minimisation algorithm named SAR . Last, we experiment with an unsupervised pseudo-labelling algorithm inspired by FixMatch , which we refer as unsupervised FixMatch (UFM). UFM selects an equal number of highly confident examples per class as the labelled set, and then employs FixMatch to produce pseudo-labels from rest of the unlabelled examples. Details are available in Appendix E.

We summarise the results in Table 3 and compare our method, i.e., learning task vector compositions, against the conventional approach of tuning the layer normalisation parameters [43; 57; 59]. We show that under all self-supervised objectives, aTLAS achieves higher accuracy than tuning the LayerNorm. In particular, LayerNorm has 30k learnable parameters with ViT-B/32 while our method only has 3.5k learnable parameters. We note that with the UFM objective, aTLAS performs the best and improves the accuracy by an average of \(6.5\) absolute points over the zero-shot baseline.

   Method & Zero-shot &  &  &  \\   & & LN & aTLAS & LN & aTLAS & LN & aTLAS \\  Accuracy & \(60.4\) & \(60.4 0.0\) & \(62.7 0.1\) & \(61.2 0.1\) & \(62.9 0.0\) & \(62.2 0.1\) & \(\) \\   

Table 3: Test-time adaptation accuracy averaged over 22 dataset, with \( 1\) standard error over 3 random seeds. LN refers to tuning the LayerNorm layers. CLIP with the ViT-B/32 backbone is used. Highest performance is highlighted in bold.

Figure 6: Few-shot performance of aTLAS with various task vector budgets. The accuracy is averaged across 22 datasets and over three random seeds. Standard deviation \( 1\) is overlaid as the error margin. Performance under the 16-shot setting is visualised, while additional detailed results are included in Table 14.

## 6 Relation to parameter-efficient fine-tuning

One of the key advantages of aTLAS is its ability to adapt pre-trained models with few learnable parameters, making it suitable for parameter-efficient fine-tuning (PEFT). Similar to popular PEFT methods such as low-rank adaptation (LoRA) , our approach does not introduce additional modules, thereby avoiding an increase in inference complexity. In addition, since only the encoded weight matrices in LoRAs have non-zero weight difference, LoRAs are in fact sparse task vectors. They can thus be seamlessly integrated into our method, significantly reducing the memory cost.

### LoRAs as task vectors

Due to the sparsity and rank deficiency, LoRAs as task vectors may have limited representation capacity and carry less knowledge. Therefore, they may be inferior to standard task vectors for knowledge transfer. We investigate this by learning linear combinations of LoRAs4 using our method, under the settings of few-shot recognition. Results are summarised in Table 4. We first shed light on the impact of sparsity, and compare two variants of our method that either learns linear combinations of all parameter blocks or just the weight matrices. Results show that sparsity results in an accuracy decrease of around \(0.5\%\) on average, except for the one-shot setting. The rank deficiency, on the other hand, causes more substantial accuracy drop. Nevertheless, this can be largely mitigated by increasing the rank. Using a rank of \(64\) leads to similar performance compared to learning compositions of only weight matrices in standard task vectors. In conclusion, while the sparsity and rank deficiency introduce some performance drops, especially in low-shot settings, LoRAs are competitive alternatives to standard task vectors due to their low memory cost.

### Scalability of aTLAS

Despite the parameter efficiency of aTLAS, its performance is not as competitive when sufficient training data is available. To address this, we devise a strategy to flexibly scale up the number of learnable parameters as needed. Specifically, we randomly divide each parameter block into \(K\) partitions, and assign a learnable coefficient to each partition, naturally increasing the number of learnable parameters by \(K\)-fold. We denote these variants by aTLAS \( K\). We conduct experiments with these variants using \(\{1,5,10,25,35,50,100\}\%\) of the total available training data across the \(22\) datasets used in Section 5. The results are summarised in Figure 7, showing that our method consistently improves as \(K\) increases. Compared to LoRAs, particularly with limited training data,

    &  &  \\   & All parameter blocks & Weight matrices & Rank=4 & Rank=16 & Rank=64 \\  & (10.7 GB) & (10.5 GB) & (3.3 GB) & (3.4 GB) & (4.1 GB) \\ 
1 & \(66.0 0.2\) & \(66.0 0.1\) & \(64.4 0.1\) & \(64.6 0.1\) & \(65.4 0.1\) \\
2 & \(67.7 0.1\) & \(67.0 0.2\) & \(65.7 0.0\) & \(66.6 0.2\) & \(67.4 0.1\) \\
4 & \(70.0 0.0\) & \(69.4 0.2\) & \(68.2 0.0\) & \(68.7 0.1\) & \(69.5 0.2\) \\
8 & \(71.3 0.1\) & \(70.9 0.0\) & \(70.2 0.2\) & \(70.4 0.1\) & \(70.9 0.1\) \\
16 & \(72.8 0.1\) & \(72.3 0.0\) & \(71.7 0.1\) & \(71.8 0.1\) & \(72.0 0.1\) \\   

Table 4: Few-shot recognition performance using standard task vectors or LoRAs as sparse task vectors. Results are averaged across 22 datasets over three seeds, with \( 1\) standard deviation. The memory consumption for ViT-B/32 backbone is annotated under each variant. For standard task vectors, we learn compositions on all parameter blocks or weight matrices only. For LoRAs as task vectors, we report results with rank 4, 16 and 64.

Figure 7: Scalability of aTLAS. We compare the accuracy of our method against LoRAs, and vary the amount of training data. Results are averaged over 22 datasets. Detailed results are included in Table 17.

our method achieves higher performance with fewer learnable parameters. With sufficient training data, the variant aTLAS \( 1200\) leads to higher performance with a similar number of learnable parameters, as it is able to exploit the knowledge contained in the task vectors that may otherwise be unobtainable from the target dataset.

## 7 Related work

Task vectors and model compositions.Recent studies have demonstrated the possibility of manipulating the behaviours of neural networks directly in the weight space [27; 62; 64]. In particular, task vectors , as a carrier of the domain-specific knowledge learned through fine-tuning, exhibit strong compositional properties. Such compositionality can be enhanced via linearisation using first-order Taylor expansion , and improves model editing with simple arithmetic, e.g., addition, negation, etc. Yang et al.  also investigated the idea of learning layer-wise coefficients to improve task arithmetic. In addition, low-rank adaptations , as special forms of task vectors, were shown to also support such arithmetic operations. A recent study  also investigated the idea of learning combinations of LoRAs for few-shot recognition.

Model-based transfer learning.One interpretation of transfer learning  is to exploit the knowledge encapsulated in a pre-trained model for a target domain. Amongst various sub-modules of a pre-trained model, transferring the feature extractor is the most extensively studied. This ranges from early convolutional neural networks [18; 53] to modern transformers , from vision backbones [14; 37] to language models [13; 46]. For vision applications, classification models trained on ImageNet  have been used as the medium for knowledge transfer. In recent years, contrastively pre-trained multi-modal models such as CLIP  have emerged as a prevalent choice. Such models are trained on large volumes of data by aligning image and language representations, leading to strong baselines well suited for transfer learning. CLIP representations have since been use for medical imaging , semantic segmentation , satellite imaging , etc.

Model adaptation in low-data regimes.The performance of pre-trained models is often constrained when applied to specific tasks with limited labelled data. To address this limitation, extensive research has been conducted on few-shot adaptation of CLIP . These studies focus on various techniques, including prompt engineering , feature adaptation , and more recently classifier adaptation [25; 69]. In addition to few-shot adaptation, test-time adaptation represents an even more challenging scenario where no annotated data is available. This typically requires leveraging self-supervised objectives to adapt the model, employing methods such as entropy minimisation [35; 43; 59], contrastive learning , pseudo labelling  and image rotation prediction .

## 8 Conclusion

In this paper, we introduced aTLAS, a learning algorithm that leverages the rich knowledge encapsulated in task vectors through learned linear combinations with anisotropic scaling. Unlike conventional methods that learn network parameters, our approach focuses on learning coefficients on task vectors, significantly reducing the number of learnable parameters. We conducted experiments across task arithmetic, few-shot recognition, test-time adaptation and parameter-efficient fine-tuning, demonstrating the effectiveness of our method with supervised and unsupervised objectives. In particular, we highlighted several properties of aTLAS, including low disentanglement error, robustness against domain shift, effectiveness in low-data regimes, complementarity with existing few-shot methods, etc. These properties paved the way for efficient knowledge composition and transfer.

Limitations.As a task vector is defined with respect to a specific pre-trained model, knowledge composition and transfer are not yet feasible across different architectures. This may become possible with suitable projections and remains part of the future work. In addition, combining large numbers of task vectors can consume a substantial amount of GPU memory when training larger models. This can be mitigated by selecting a subset of task vectors, using LoRAs as task vectors or by offloading the computation of task vector composition to CPU, at the cost of training speed decrease. It is also possible to perform task vector composition at bit-width lower than floating point precision, e.g., 4-bit. Similar features are being tested with popular deep learning frameworks such as PyTorch, and we expect the memory requirement of larger models to be less of a constraint in the future.

Acknowledgements.This research is funded in part by the Australian Government through the Australian Research Council (Project DP240103278), and the Centre of Augmented Reasoning at the Australian Institute for Machine Learning, established by a grant from the Department of Education. We would like to thank Stephen Gould for his valuable feedback on the paper.