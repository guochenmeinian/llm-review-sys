# Taoan Huang\({}^{2}\)   Bistra Dilkina\({}^{2}\)   Yuandong Tian\({}^{1}\)

Landscape Surrogate: Learning Decision Losses for Mathematical Optimization Under Partial Information

 Arman Zharmagambetov\({}^{1}\)   Brandon Amos\({}^{1}\)   Aaron Ferber\({}^{2}\)\({}^{1}\)FAIR at Meta  \({}^{2}\)University of Southern California

{armanz,bda,yuandong}@meta.com  {afterber,taoanhua,dilkina}@usc.edu

###### Abstract

Recent works in learning-integrated optimization have shown promise in settings where the optimization problem is only partially observed or where general-purpose optimizers perform poorly without expert tuning. By learning an optimizer \(\) to tackle these challenging problems with \(f\) as the objective, the optimization process can be substantially accelerated by leveraging past experience. The optimizer can be trained with supervision from known optimal solutions or implicitly by optimizing the compound function \(f\). The implicit approach may not require optimal solutions as labels and is capable of handling problem uncertainty; however, it is slow to train and deploy due to frequent calls to optimizer \(\) during both training and testing. The training is further challenged by sparse gradients of \(\), especially for combinatorial solvers. To address these challenges, we propose using a smooth and learnable _Landscape Surrogate_\(\) as a replacement for \(f\). This surrogate, learnable by neural networks, can be computed faster than the solver \(\), provides dense and smooth gradients during training, can generalize to unseen optimization problems, and is efficiently learned via alternating optimization. We test our approach on both synthetic problems, including shortest path and multidimensional knapsack, and real-world problems such as portfolio optimization, achieving comparable or superior objective values compared to state-of-the-art baselines while reducing the number of calls to \(\). Notably, our approach outperforms existing methods for computationally expensive high-dimensional problems.

## 1 Introduction

Mathematical optimization problems in various settings have been widely studied, and numerous methods exist to solve them [25; 32]. Although the literature on this topic is immense, real-world applications consider settings that are nontrivial or extremely costly to solve. The issue often stems from uncertainty in the objective or in the problem definition. For example, combinatorial problems involving nonlinear objectives are generally hard to address, even if there are efficient methods that can handle special cases (e.g., \(k\)-means). One possible approach could be learning so-called _linear surrogate costs_ that guide an efficient linear solver towards high quality solutions for the original hard nonlinear problem. This automatically finds a surrogate mixed integer linear program (MILP), for which relatively efficient solvers exist . Another example is the _smart predict+optimize_ framework (a.k.a. decision-focused learning) [13; 41] where some problem parameters are unknown at test time and must be inferred from the observed input using a model (e.g., neural nets).

Despite having completely different settings and purposes, what is common among learning surrogate costs, smart predict+optimize, and other integrations of learning and optimization, is the need to learn a certain target mapping to estimate the parameters of a latent optimization problem. This makes the optimization problem well-defined, easy to address, or both. In this work, we draw general connections between different problem families and combine them into a unified framework. The core idea (section 3) is to formulate the learning problem via constructing a compound function \(f\) that includes a parametric solver \(\) and the original objective \(f\). To the best of our knowledge, this paper is the first to propose a generic optimization formulation (section 3) for these types of problems.

Minimizing this new compound function \(f\) via gradient descent is a nontrivial task as it requires differentiation through the argmin operator. Although various methods have been proposed to tackle this issue [3; 2], they have several limitations. First, they are not directly applicable to combinatorial optimization problems, which have 0 gradient almost everywhere, and thus require various computationally expensive approximations [34; 41; 15; 39]. Second, even if the decision variables are continuous, the solution space (i.e., argmin) may be discontinuous. Some papers [12; 17] discuss the fully continuous domain but typically involve computing the Jacobian matrix, which leads to scalability issues. Furthermore, in some cases, an explicit expression for the objective may not be given, and we may only have black-box access to the objective function, preventing straightforward end-to-end backpropagation.

These limitations motivate _Landscape Surrogate_ losses (LANCER), a unified model for solving coupled learning and optimization problems. LANCER accurately approximates the behavior of the compound function \(f\), allowing us to use it to learn our target parametric mapping (see fig. 1). Intuitively, LANCER must be differentiable and smooth (e.g., neural nets) to enable exact and efficient gradient computation. Furthermore, we propose an efficient alternating optimization algorithm that jointly trains LANCER and the parameters of the target mapping. Our motivation is that training LANCER in this manner better distills task-specific knowledge, resulting in improved overall performance. Experimental evaluations (section 5) confirm this hypothesis and demonstrate the scalability of our proposed method.

The implementation of LANCER can be found at https://github.com/facebookresearch/LANCER.

## 2 Related work

**Smart Predict+Optimize framework** considers settings where we want to train a target mapping which predicts latent components to an optimization problem to improve downstream performance. A straightforward and naive approach in P+O is to build the target machine learning model in a two-stage fashion: train the model on ground truth problem parameters using some standard measure of accuracy (e.g., mean squared error) and then use its prediction to solve the optimization problem. However, this approach is prone to produce highly suboptimal models [6; 41] since the learning problem does not capture the task-specific objectives. Instead, "smart" Predict+Optimize (SPO)  proposed to minimize the _decision_ regret, i.e., the error induced by the optimization solution based on the estimated problem parameters coming from the machine learning model. A variety of methods exist for learning such models in the continuous, and often convex, optimization setting. These approaches usually involve backpropagating through the solver [11; 12]. In some isolated and simple scenarios, an optimal solution exists , or the learning problem can be formulated via efficient surrogate losses . Extending the SPO framework for combinatorial problems is challenging, and current methods rely on identifying heuristic gradients often via continuous, primal, or dual relaxations [33; 41; 34; 31; 39; 15; 28]. Alternative approaches leverage specific structures of the optimization problem [9; 21; 40], such as being solvable via dynamic programming or graph

Figure 1: Overview of our proposed framework LANCER. We replace the non-convex and often non-differentiable function \(f\) with landscape surrogate \(\) and use it to learn the target mapping \(_{}\). The current output of \(_{}\) is then used to evaluate \(f\) and to refine \(\). This procedure is repeated in alternating optimization fashion.

partitioning. Further work focuses on SPO for individual problems Shah et al. , which collects perturbed input instances and learns a separate locally convex loss for each instance. In contrast, we define the loss over the domain rather than per instance, allowing for generalization to unseen instances, and our bilevel optimization formulation enables more efficient training. Moreover, compared to prior works, our method generically applies to a wider variety of problem settings, losses, and problem formulations. See Table 1 for a direct comparison of the requirements and capabilities for different approaches.

Mixed integer nonlinear programming (MINLP)LANCER can be considered as a solver for constrained combinatorial problems with nonlinear and nonconvex objectives (MINLP), a challenging class of optimization problems . Apart from SurCo , which we will discuss in detail in section 3, specialized solvers exist that deal with various MINLP formulations [7; 1; 19]. These specialized solvers generally require an analytical form for the objective and cannot handle black-box objectives. Additionally, these methods are not designed to handle general-purpose large-scale problems without making certain approximations or using specialized modeling techniques, such as linearizing the objective function or applying domain-specific heuristics. Lastly, some previous work directly predicts solutions to economic dispatch problems , uses reinforcement learning to build solutions to linear combinatorial problems , or uses reinforcement learning for ride hailing problems . These approaches are designed for specific application domains or are tailored to linear optimization settings where the reward for a single decision variable is its objective coefficient, which is not trivially applicable in nonlinear settings.

Optimization as a layerThe optimization-as-a-layer family of methods considers the composition of functions where one (or more) of the functions is defined as the solution to a mathematical optimization problem [3; 2; 17; 18]. Since both P+O and SurCo can be formulated as a nesting of a target mapping with the argmin operator (i.e., optimization solver), one can leverage approaches from this literature. The core idea here is based on using the implicit function theorem to find necessary gradients and backpropagate through the solver. Similar concepts exist for combinatorial optimization [15; 39; 28; 31; 29], where gradient non-existence is tackled through improved primal or dual relaxations.

## 3 A Unified Training Procedure

In this work, we focus on solving the following optimization problems:

\[_{}f(;)\] (1)

where \(f\) is the function to be optimized (linear or nonlinear), \(\) are the decision variables that must lie in the feasible region, typically specified by (non)linear (in)equalities and possibly integer constraints, and \(\) is the problem description (or problem features). For example, if \(f\) is to find a shortest path in a graph, then \(\) is the path to be optimized, and \(\) represents the pairwise distances (or features used to estimate them) in the formulation.

Ideally, we would like to have an optimizer that can (1) deal with the complexity of the loss function landscape (e.g., highly nonlinear objective \(f\), complicated and possibly combinatorial domain \(\)), (2) leverage past experience in solving similar problems, and (3) can deal with a partial information setting, in which only an observable problem description \(\) can be seen but not the true problem description \(\) when the decision is made at test time.

To design such an optimizer, we consider the following setting: assume that for the training instances, we have access to the full problem descriptions \(\{_{i}\}\), as well as the observable descriptions \(\{_{i}\}\), while for the test instance, we only know its observable description \(_{}\), but not its full description \(_{}\). Note that such a setting naturally incorporates optimization under uncertainty, in which a decision need to be made without full information, while the full information can be obtained in hindsight (e.g., portfolio optimization). Given this setting, we propose the following general _training_ procedure on a training set \(_{}:=\{(_{i},_{i})\}_{i=1}^{N}\) to learn a good optimizer:

\[_{}(Y,Z):=_{i=1}^{N}f(_{ }(_{i});_{i})\] (2)Here \(_{}:\) is a _learnable solver_ that returns a high quality solution for objective \(f\)_directly_ from the observable problem description \(_{i}\). \(\) are the learnable solver's parameters. Once \(_{}\) is learned, we can solve new problem instances with observable description \(_{}\) by either calling \(_{}=_{}(_{})\) to get a reasonable solution, or continue to optimize Eqn. 1 using \(=_{}\) as an initial solution.

Theoretically, if problem description \(\) is fully observable (i.e., \(=\)), the optimization oracle \(_{}f(;)\) solves Eqn. 2. However, solving may be computationally intractable even with full information as in nonlinear combinatorial optimization.

Our proposed training procedure is general and covers many previous works that rely on either fully or partially observed problem information.

Smart Predict+Optimize (P+O)In this setting, \(f\) belongs to a specific function family (e.g., linear or quadratic programs). The full problem description \(\) includes objective coefficients, but we only have access to noisy versions of them in \(\). Then the goal in P+O is to identify a mapping \(_{}\) (e.g. a neural net) so that a downstream solver outputs a high quality solution: \(_{}()=_{}f(;_{}())\). Here \(_{}f\) can often be solved with standard approaches, and the main challenge is to estimate the problem description accurately (w.r.t. eq. (2)). Note that other P+O formulations can be encompassed within our framework in eq. (2). For instance, the regret-based formulation described in SPO  can be represented as \(_{}()}f(};)-f^{*}\) where \(f^{*}\) is the optimal loss that is independent of \(\).

Learning surrogate costs for MINLPWhen \(f\) is a general nonlinear objective (but \(=\) is fully observed), computing \(_{}f\) also becomes non-trivial, especially if \(\) is in combinatorial spaces. Such problems are commonly referred as mixed integer nonlinear programming (MINLP). To leverage the power of linear combinatorial solvers, SurCo  sets the learnable solver to be \(_{}()=_{} ^{}_{}()\), which is a linear solver and does not include the nonlinear function \(f\) at all. Intuitively, this models the complexity of \(f\) by the learned _surrogate cost_\(_{}\), which is parameterized by a neural network. Surprisingly, this works quite well in practice .

## 4 Lancer: Learning Landscape Surrogate Losses

Variations of training objective Eqn. 2 have been proposed to learn \(\) in one way or another. This includes derivative-based approaches discussed in section 2 as well as domain-specific methods that learn \(\) efficiently and avoid backpropagating through the solver, e.g., SPO+ .

While these are valid approaches, at each step of the training process, we need to call a solver to evaluate \(_{}\), which can be computationally expensive. Furthermore, \(_{}\) is learned via gradient descent of Eqn. 2, which involves backpropagating through the solver. One issue of this procedure is that the gradient is non-zero only at certain locations (i.e., when changes in the coefficients lead to changes in the optimal solution), which makes the gradient-based optimization difficult.

One question arises: can we model the composite function \(f_{}\) jointly? The intuition here is that while \(_{}\) can be hard to compute, \(f_{}\) can be smooth to model, since \(f\) can be smooth around the solution provided by \(_{}\). If we model \(f_{}\) locally by a _landscape surrogate_ model \(\), and optimize directly on the local landscape of \(\), then the target mapping \(_{}\) can be trained without running expensive solvers:

\[_{}(Y,Z):=_{i=1}^{N}(_{ }(_{i});_{i}).\] (3)

Note that \(\) directly depends on \(_{}\) (not on \(_{}\)). Obviously, \(\) cannot be any arbitrary function. Rather it should satisfy certain conditions: 1) capture a task-specific loss \(f_{}\) (not just \(f\) and not the solver \(\) alone, but jointly); 2) be differentiable and smooth. Differentiability allows us to train our target model \(_{}\) in end-to-end fashion (assuming \(\) is itself differentiable). The primary advantage is that we can _avoid backpropagating through the solver or even through_\(f\) (e.g., multi-armed bandits). Moreover, \(_{}\) is typically high dimensional (e.g., a neural net) and potentially can make the learning problem for \(_{}\) much easier. The question is how to obtain such a model \(\)? Oneway is to parameterize it and formulate the learning problem:

\[_{}&_{i=1}^{N}\|_{}(_{^{*}}(_{i}), _{i})-f(_{^{*}}(_{i});_{ i})\|\\ &^{*}_{ }_{i=1}^{N}_{}(_{ }(_{i}),_{i}).\] (4)

The main motivation for this bi-level optimization formulation is that the surrogate model \(_{}\) is not concerned to be accurate for all possible \(_{}\); rather, we are interested in finding the configuration of \(_{}\) that yields the closest approximation to \(f\) near \(^{*}\). In other words, \(_{}\) serves as a _surrogate loss_ that approximates \(\) at a certain _landscape_: \(_{}(Y,Z;^{*})(Y,Z; ^{*})\). By adopting the bi-level optimization framework, we can focus on learning accurate \(_{}\) for such \(^{*}\).

It is important to note that evaluating \(f\) depends on both \(_{}(_{i})\) and \(_{i}\) (see eq. (2)). Therefore, to construct an accurate surrogate model \(_{}\) that can effectively approximate \(f\), it is essential for \(_{}\) to take into consideration the influence of both inputs.

To solve eq. 4, one could apply established methods from the bi-level optimization literature, such as . However, majority of them still rely on \(_{}\) (or even \(_{}^{2}\)), which involves differentiating through the solver. To overcome this issue, we propose a simple and generic Algorithm 1, which is based on alternating optimization (high-level idea is depicted in fig. 1). The core idea is to simultaneously learn both mappings (\(_{}\) and \(_{}\)) to explore different solution spaces. By improving our target model \(_{}\), we obtain better estimates of the surrogate loss around the solution, and a better estimator \(_{}\) leads to better optimization of the desired loss \(\). The use of alternating optimization helps both mappings reach a common goal. Furthermore, similarly flavored alternating optimization techniques were found to be successful in other problems with non-differentiable nature .

In practice, the "inner" optimization (lines 10 and 12) does not have to be done "perfectly". In our approach, we perform a fixed (smaller) number of updates; that is, we do not precisely solve the minimization for both \(_{}\) and \(_{}\). For example, most experiments use 10-20 updates per \(t\). One potential reason is that we do not want the models to overfit to the data; instead, we increase the total number of outer loop \(T\). We discuss this further in the experimental setup.

```
1:Input: \(_{}\{_{i},_{i}\}_{i=1}^{N}\), solver \(\), objective \(f\), target model \(_{}\);
2:Initialize \(_{}\) (e.g. random, warm start);
3:for\(t=1 T\)do
4:\(\)\(\)-step (fix \(\) and optimize over \(\)):
5:for\((_{i},_{i})_{}\)do
6: evaluate \(}_{i}=_{}(_{i})\);
7: evaluate \(_{i}=f((_{i});_{i})\);
8: add \((}_{i},_{i},_{i})\) to \(\);
9:endfor
10: solve \(_{}_{i}\|_{}(}_{i},_{i})-_{i}\|\) via supervised learning;
11:\(\)\(\)-step (fix \(\) and optimize over \(\)):
12: solve \(_{}_{i_{}}_{ }(_{}(_{i}),_{i})\) via supervised learning.
13:endfor ```

**Algorithm 1** Pseudocode for simultaneously learning LANCER and target model \(_{}\). Note that the algorithm may vary slightly based on setting (e.g., P+O and variations of SurCo ), see Appendix B.

Note that the Algorithm 1 avoids backpropagating through the solver or even through \(f\). The only requirement is evaluating the function \(f\) at the solution of \(\), which can be achieved by blackbox solver access. As a result, this approach eliminates the complexity and computational expense associated with computing derivatives of combinatorial solvers, making it a more efficient and practical solution as shown in Table 1.

It is worth noting that our framework shares similarities with _actor-critic reinforcement learning_. In this analogy, we can think of \(_{}\) as an actor responsible for making certain decisions, like predicting coefficients. On the other hand, \(_{}\) plays the role of a critic which evaluates the actor's decisions by estimating the objective value \(f\), and provides feedback to the actor.

Similar to many other approaches designed for solving problems with bi-level optimizations, including actor-critic algorithms, we currently lack theoretical guarantees regarding the convergence of LANCER with respect to either \(_{}\) or \(_{}\). This lack of guarantees can also impact the stability of the algorithm. However, this is primarily a matter that can be addressed through empirical investigation and depends on various factors, which is typical in the process of model selection. In Appendix E, we present ablation studies that we conducted to evaluate the stability of our approach across various hyperparameters and neural network architectures. These, along with our main experimental results in section 5, demonstrate that LANCER generally exhibits robustness in many cases.

### Reusing landscape surrogate model \(_{}\)

Once Algorithm 1 finishes, we usually discard \(_{}\) as it is an intermediate result of the algorithm, and we only retain \(_{}\) (and solver \(\)) for model deployment. However, we have found through empirical exploration that the learned surrogate loss \(_{}\) can be _reused_ for a range of problems, increasing the versatility of the approach. This is particularly advantageous for _SurCo_ setting, where we handle one instance at a time. In this scenario, we utilize the _trained_\(_{}\) for unseen test instances by executing only the \(\)-step of Algorithm 1. The main advantage of this extension is that it eliminates the need for access to the solver \(\), leading to significant deployment runtime improvements.

### Reusing past evaluations of \(f_{}\)

In LANCER, the learning process of \(_{}\) is solely reliant on \(\) and is independent of the current state of \(_{}\). Put simply, to effectively learn \(_{}\), we only need the inputs and outputs of \(f_{}\), namely \(_{}(_{i})\), \(Z\), and the corresponding objective value \(}\). Interestingly, we can cache the predicted descriptions themselves, \(_{}(_{i})\), without the need for the model \(\) or problem information. This caching mechanism allows us to reuse the data \((_{}(_{i}),,})\) from previous iterations (\(1 T-1\)) as-is. By adopting this practice, we enhance and diversify the available training data for \(_{}\), which proves particularly advantageous for neural networks. This concept bears resemblance to the concept of a _replay buffer_ commonly found in the literature on Reinforcement Learning.

## 5 Experiments

We validate our approach (LANCER) in two settings: smart predict+optimize and learning surrogate costs for MINLP. For each setting, we study a range of problems, including linear, nonlinear, combinatorial, and others, encompassing both synthetic and real-world scenarios. Overall, LANCER_exhibits superior or comparable objective values while maintaining efficient runtime_. Additionally, we perform ablation studies, such as re-using \(\).

### Synthetic data

#### 5.1.1 Combinatorial optimization with linear objective

The shortest path (SP) and multidimensional knapsack (MKS) are both classic problems in combinatorial optimization with broad practical applications. In this setting, we consider a scenario where problem parameters \(\), such as graph edge weights and item prices, cannot be directly observed

   Feature  Method & LANCER & SurCo  & LODLs  & SPO+  & DiffOpt  & Exact  \\  On-the-fly opt & \(+\) & \(+\) & \(-\) & \(-\) & \(-\) & \(+\) \\ Nonlinear \(f\) & \(+\) & \(+\) & \(+\) & \(-\) & \(+\) & \(+\) \\ Blackbox \(f\) & \(+\) & \(-\) & \(+\) & \(-\) & \(-\) & \(-\) \\ \( f\) not required & \(+\) & \(-\) & \(+\) & \(+\) & \(-\) & \(+\) \\ \(_{}\) not required & \(+\) & \(-\) & \(+\) & \(+\) & \(-\) & \(+\) \\ Generalization & \(+\) & \(+\) & \(-\) & \(+\) & \(+\) & \(-\) \\ Few fast solver calls & \(\) & \(\) & \(-\) & \(\) & \(\) & \(-\) \\   

Table 1: Conceptual comparison of methods from related literature. Capabilities on the left are present or not for Methods on the top. \(\) is given when this capability depends empirically on the problem at hand.

during test time, and instead need to be estimated from \(\) via learnable mapping \(=_{}()\). That is, we consider smart P+O setting.

SetupWe use standard linear program (LP) formulation for SP and mixed integer linear program (MILP) formulation for MKS. Observed features \(\) and the corresponding ground truth problem descriptions \(\) for SP are generated using the same procedure as in : grid size of \(5 5\), feature dimension of \(^{5}\) (obtained using a random linear mapping from \(\)), 1000 instances for both train and test. For MKS, we increased the knapsack dimension to 5 and capacity to 45, and we set the number of items to 100. Moreover, we use randomly initialized MLP (1 ReLu hidden layer) to generate features of dimension \(^{256}\). As for the baselines, apart from the naive 2-stage approach, we have SPO+  and DBB , both implemented in PyEPO  library. Furthermore, we added LODLs, a novel method from Shah et al. . We explored as best as we could all important hyperparameters for all methods on a fixed cross-validation set. Regarding the LANCER, we utilize MLP with 2 tanh hidden layers of size 200 for surrogate model \(\). We set \(T=10\) and the number of updates for \(\) and \(_{}\) is at most 10. We use SCIP  to solve LP for the shortest path and MILP for knapsack. Further details can be found in Appendix C.1.1.

ResultsThe results are summarized in fig. 2. We report the normalized regret as described in . The findings indicate that LANCER and SPO+ consistently outperform the two-stage baseline, particularly when considering the warm start. As SPO+ is specifically designed for linear programs, it provides informative gradients, making it a robust baseline. Even in MKS, where theorems proposed in  are no longer applicable, SPO+ performs decently with minimal tuning effort. The DBB approach, however, demonstrates unsatisfactory default performance but can yield favorable outcomes with proper initialization and tuning (see the right plot). Interestingly, the other P+O baselines, initialized randomly, were unable to outperform a naive 2stg in both benchmarks.

LANCER achieves superior performance in both tasks, with a noticeable advantage in MKS. This may be attributed to the high dimension of the MKS problem and the large feature space (\(\)). One possible explanation is that the sparse gradients of the derivative-based method make the learning problem harder, whereas LANCER models the landscape of \(f g\), providing informative gradients for \(_{}\).

#### 5.1.2 Combinatorial optimization with nonlinear objective

In this section, we apply LANCER for solving mixed integer nonlinear programs (MINLP). Specifically, we transform a combinatorial problem with a nonlinear objective into an instance of MILP via learning linear surrogate costs as described in Ferber et al. . Note that in this setting, we assume that the full problem description \(=\) is given and fully observable (in contrast to the P+O setting).

We begin by examining on-the-fly optimization, where each problem is treated independently. In this scenario, the cost vector \(_{}()\) simplifies to a constant value \(\). SurCo is then responsible for directly training the cost vector \(\) of the linear surrogate. As we lack a distribution of problems to train \(\), specific adaptations to Algorithm 1 are necessary, which are outlined in detail in Appendix B. We refer to this version as LANCER-zero to be consistent with SurCo-zero.

Figure 2: Normalized test regret (lower is better) for different P+O methods: 2-stage, SPO+ , DBB , LODLs  and ours (LANCER). Overlaid dark green bars (right) indicate that the method warm started from the solution of 2stg. DBB performs considerably worse on the right benchmark and is cut off on the \(y\)-axis.

SetupNonlinear shortest path problems arise when the objective is to maximize the probability of reaching a destination before a specified time in graphs with random edges [26; 14]. The problem formulation is similar to the standard linear programming (LP) formulation of the shortest path, as described in section 5.1.1, with a few adjustments: 1) the weight of each edge follows a normal distribution, i.e., \(w_{e}(_{e},_{e})\); 2) the objective is to maximize the probability that the sum of weights along the shortest path is below a threshold \(W\), which can be expressed using the standard Gaussian cumulative distribution function (CDF), \(P(_{e E}w_{e} W)=((W-_{e E}_{e})/_{e}})\) where \(E\) is the set of edges belonging to the shortest path. We use \(5 5\) and \(15 15\) grid graphs with 25 draws of edge weights. We set the threshold \(W\) to three different values corresponding to loose, normal, and tight deadlines. The remaining settings are adapted from Ferber et al. , and additional details can be found in Appendix C.1.2.

ResultsFig. 3 illustrates the performance of different methods in both grid sizes. SCIP directly formulates the MINLP to maximize the CDF, resulting in an optimal solution. However, this approach is not scalable for larger problems and is limited to smaller instances like the \(5 5\) grid. The heuristic method assigns each edge weight as \(w_{e}=_{e}+_{e}\), where \(\) is a user-defined hyperparameter, and employs standard shortest path algorithms (e.g., Bellman-Ford). As the results indicate, this heuristic approach produces highly suboptimal solutions. SurCo-zero and LANCER-zero demonstrate similar performance, with LANCER-zero being superior in almost all scenarios.

### Real-world use case: quadratic and broader nonlinear portfolio selection

#### 5.2.1 The quadratic programming (QP) formulation

In this study, we tackle the classical quadratic Markowitz  portfolio selection problem. We use real-world data from Quandl  and follow the setup described in Shah et al. . The prediction task leverages each stock's historical data \(\) to forecast future prices \(\), which are then utilized to solve the QP (i.e., P+O setting). The predictor is the MLP with 1 hidden layer of size 500.

SetupWe follow a similar setup described in , except for a fix in the objective's quadratic term that slightly affects the decision error's magnitude. More details can be found in Appendix C.2.1. We compare LANCER against two-stage and LODLs. However, SPO+ is not applicable in this nonlinear setting, and DBB's performance is notably worse since it is designed for purely combinatorial problems. Additionally, we report the optimal solution (using the ground truth values of \(\)) and the (Melding Decision Focused Learning) MDFL  method, which leverages the implicit function theorem to differentiate through the KKT conditions. For our LANCER implementation, we use an MLP with 2 hidden layers for

  Method & Test DL \\  Random & 1 \\ Optimal & 0 \\ 
2–Stage & 0.57 \(\) 0.02 \\ LODLs  & 0.55 \(\) 0.02 \\ MDFL  & 0.52 \(\) 0.01 \\
**LANCER** & **0.53 \(\) 0.02** \\  

Table 2: Portfolio selection normalized test decision loss (lower is better).

Figure 3: Results on stochastic shortest path using different grid sizes: 5x5 (left) and 15x15 (right). We report avg objective values (higher is better) on three settings described in . For grid size of 15x15, SCIP  was unable to finish within the 30 min time limit.

and update each of the parametric mappings 5 times per iteration with a total of \(T=8\) iterations. We use cvxpy  to solve the QP.

ResultsTable 2 summarizes our results. We report the normalized decision loss (i.e., normalized Eqn. (2)) on test data. Since the problem is smooth and exact gradients can be calculated, MDFL achieves the best performance closely followed by the LANCER. The remaining results are in agreement with . While LANCER does not achieve the best overall performance, it does so using a significantly smaller number of calls to a solver, as we discuss in more detail in Section 5.3.

#### 5.2.2 Combinatorial portfolio selection with third-order objective

The convex portfolio optimization problem discussed in the previous section 5.2.1 is unable to capture desirable properties such as logical constraints , or higher-order loss functions  that integrate metrics like co-skewness to better model risk. We use the Quandl  data (see Appendix C.2.2 for details on setup) and similar to section 5.1.2, we assume that the full problem description \(\) is given at train/test time.

**Results** are shown in fig. 4. We first tried to solve the given MINLP exactly via SCIP. However, it fails to produce the optimal solution within a 1 hour time limit and we report the best incumbent feasible solution. MIQP (blue squares) and MILP (blue triangles) approximations overlook the co-skewness and non-linear terms, respectively. Comparing their performance, MIQP exhibits a \(2\) lower loss than the MILP baseline but in a significantly longer runtime. For LANCER and SurCo, we present results for two scenarios: learning the linear cost vector \(\) directly (_zero_) for each instance, and a parameterized version \(_{}()\) (_prior_). The main distinction of "prior" is that no learning occurs during test time, as we directly map the problem descriptor \(\) to a linear cost vector and solve the MILP. Consequently, the deployment runtime is similar to that of the MILP approximation, but LANCER-prior produces slightly superior solutions. Remarkably, LANCER-zero achieves significantly better loss values, surpassing all other methods. Although it takes longer to run, the runtime remains manageable, and importantly, the solution quality improves with an increasing number of iterations.

According to our experimental findings, LANCER demonstrated the most substantial improvement in this specific setting. Encouraged by this success, we conducted a deeper exploration of the complexities of this problem, as detailed in Appendix D.

### Computational efficiency

Comparing baseline methods, including LANCER, we find that querying solver \(_{}\) is the primary computational bottleneck. To evaluate this aspect, we empirically analyze different algorithms on various benchmarks in the P+O domain. The results, depicted in fig. 5, highlight that LODLs require sampling a relatively large number of points per training instance, leading to potentially time-consuming solver access. On the other hand, gradient-based methods like DBB, MDFL, and SPO+ typically solve the optimization problem 1-2 times per update but require more iterations to converge. In contrast, LANCER accesses the solver in the \(\)-step, with the number of accesses proportional to the training set size and a small total number of alternating optimization iterations. Moreover, we leverage saved solutions from previous iterations, akin to a replay buffer, when fitting \(\). These combined factors allow us to achieve favorable results with a small value of \(T\).

Figure 4: Objective (lower is better) and deployment runtime for combinatorial portfolio selection problem. For LANCER–zero and SurCo–zero, numbers at each point correspond to the number of iterations.

### Reusing landscape surrogate \(_{}\)

In this scenario, we introduce a dependency of \(_{}\) on both the predicted linear cost (\(\)) and the problem descriptor (\(\)), as described in section 4.1. This enables us to reuse \(_{}\) for different problem instances without retraining, and eliminate the dependency on the solver \(_{}\), giving LANCER a substantial runtime acceleration. To validate this hypothesis, we pretrain \(_{}\) using 200 instances of the stochastic shortest path on a \(15 15\) grid by providing concatenated \((,)\) as input. We apply LANCER-zero to the same test set as before and present results in fig. 3, demonstrating comparable performance between these two approaches, with "reused \(_{}\)" being much faster.

## 6 Conclusion

This paper makes a dual contribution. Firstly, we derive a unified training procedure to address various coupled learning and optimization settings, including smart predict+optimize and surrogate learning. This is significant as it advances our understanding of learning-integrated optimization under partial information. Secondly, we propose an effective and powerful method called LANCER to tackle this training procedure. LANCER offers several advantages over existing literature, such as versatility, differentiability, and efficiency. Experimental results validate these advantages, leading to significant performance improvements, especially in high-dimensional spaces, both in problem description and feature space. One potential drawback is the complexity of tuning \(\), requiring model selection and training. However, future research directions include addressing this drawback and exploring extensions of LANCER, such as applying it to fully black box \(f\) scenarios.