# Bifurcations and loss jumps in RNN training

Lukas Eisenmann\({}^{1,2,*}\), Zahra Monfared\({}^{1,*}\), Niclas Goring\({}^{1,2}\), and Daniel Durstewitz\({}^{1,2,3}\)

{lukas.eisenmann,zahra.monfared,daniel.durstewitz}@zi-mannheim.de

\({}^{1}\)Department of Theoretical Neuroscience, Central Institute of Mental Health,

Medical Faculty Mannheim, Heidelberg University, Mannheim, Germany

\({}^{2}\)Faculty of Physics and Astronomy, Heidelberg University, Heidelberg, Germany

\({}^{3}\)Interdisciplinary Center for Scientific Computing, Heidelberg University

\({}^{*}\)These authors contributed equally

###### Abstract

Recurrent neural networks (RNNs) are popular machine learning tools for modeling and forecasting sequential data and for inferring dynamical systems (DS) from observed time series. Concepts from DS theory (DST) have variously been used to further our understanding of both, how trained RNNs solve complex tasks, and the training process itself. Bifurcations are particularly important phenomena in DS, including RNNs, that refer to topological (qualitative) changes in a system's dynamical behavior as one or more of its parameters are varied. Knowing the bifurcation structure of an RNN will thus allow to deduce many of its computational and dynamical properties, like its sensitivity to parameter variations or its behavior during training. In particular, bifurcations may account for sudden loss jumps observed in RNN training that could severely impede the training process. Here we first mathematically prove for a particular class of ReLU-based RNNs that certain bifurcations are indeed associated with loss gradients tending toward infinity or zero. We then introduce a novel heuristic algorithm for detecting all fixed points and \(k\)-cycles in ReLU-based RNNs and their existence and stability regions, hence bifurcation manifolds in parameter space. In contrast to previous numerical algorithms for finding fixed points and common continuation methods, our algorithm provides _exact_ results and returns fixed points and cycles up to high orders with surprisingly good scaling behavior. We exemplify the algorithm on the analysis of the training process of RNNs, and find that the recently introduced technique of generalized teacher forcing completely avoids certain types of bifurcations in training. Thus, besides facilitating the DST analysis of trained RNNs, our algorithm provides a powerful instrument for analyzing the training process itself.

## 1 Introduction

Recurrent neural networks (RNNs) are common and powerful tools for learning sequential tasks or modeling and forecasting time series data . Typically, RNNs are "black boxes," whose inner workings are hard to dissect. Techniques from dynamical system theory (DST) can significantly aid in this effort, as RNNs are formally discrete-time dynamical systems (DS) . A better understanding of how RNNs solve their tasks is important for detecting failure modes and designing better architectures and training algorithms. In scientific machine learning, on the other hand, RNNs are often employed for reconstructing unknown DS from sets of time series observations , e.g. in climate or disease modeling. In this case, the RNN is supposed to provide an approximation to the flow of the observed system that reproduces all its dynamical properties, e.g., cyclic behavior in climate or epidemiological systems. In such scientific or medicalsettings, a detailed understanding of the RNN's dynamics and its sensitivity to parameter variations is in fact often crucial.

Beyond understanding the dynamical properties of a once trained RNN, it may also be of interest to know how its dynamical repertoire changes with changes in its parameters. The parameter space of any DS is partitioned into regions (or sets) with topologically different dynamical behaviors by bifurcation curves (or, more generally, manifolds). Such bifurcations, qualitative changes in the system dynamics due to parameter variations, may not only be crucial in scientific applications where we use RNNs, for instance, to predict tipping points in climate systems or medical scenarios like sepsis detection . They also pose severe challenges for the training process itself as qualitative changes in RNN dynamics may go in hand with sudden jumps in the loss landscape [16; 44]. Although methods from DST have significantly advanced the field in recent years, especially with regards to algorithms for reconstructing nonlinear DS from data [55; 46; 67], progress is still hampered by the lack of efficient tools for analysing the dynamics of higher-dimensional RNNs and their bifurcations. In particular, methods are needed for exactly locating geometrical objects like fixed points or cycles in an RNN's state space, but current numerical techniques do not scale well to higher-dimensional scenarios and provide only approximate solutions [29; 21; 61].

The contributions of this work are threefold: After first providing an introduction into bifurcations of piecewise-linear (ReLU-based) RNNs (PLRNNs), which have been extensively used for reconstructing DS from empirical data [10; 30], we mathematically prove that certain bifurcations during the training process will indeed cause loss gradients to diverge to infinity, resulting in abrupt jumps in the loss, while others will cause them to vanish. As RNNs are likely to undergo several bifurcations during training on their way from some initial parameter configuration toward a dynamics that successfully implements any given task, this poses severe challenges for RNN training and may be one of the reasons for exploding and vanishing gradients [16; 44; 8; 25]. We then create a novel, efficient heuristic algorithm for _exactly_ locating all fixed points and \(k\)-cycles in PLRNNs, which can be used to delineate bifurcation manifolds in higher-dimensional systems. Our algorithm finds these dynamical objects in many orders of magnitude less time than an exhaustive search would take. Using this algorithm, we demonstrate empirically that steep cliffs in loss landscapes and bifurcation curves indeed tightly overlap, and that bifurcations in the system dynamics are accompanied by sudden loss jumps. Finally, we prove and demonstrate that the recently introduced technique of generalized teacher forcing (GTF)  completely eliminates certain types of bifurcation in training, providing an explanation for its efficiency.

## 2 Related Work

DS analysis of trained RNNsIn many areas of science one is interested in identifying the nonlinear DS that explains a set of observed time series [12; 23]. A variety of purely data-driven machine learning approaches have been developed for this purpose [11; 51; 15], but mostly RNNs are used for the goal of reconstructing DS from measured time series [10; 55; 30; 64; 63; 46; 45]. In scientific settings in particular, but also often in engineering applications, we seek a detailed understanding of the system dynamics captured - or the dynamical repertoire produced - by a trained RNN [62; 33; 61; 14; 13]. To analyze an RNN's dynamics, typically its fixed points are determined numerically, for instance by optimizing some cost function [21; 34; 61], by numerically traversing directional fibers , or by co-training a switching linear dynamical system . These techniques, however, often scale poorly with dimensionality, provide only approximate solutions, and are not designed for detecting other dynamical objects like \(k\)-cycles. This is, however, crucial for understanding the complete dynamical repertoire of a trained RNN. PLRNNs are of particular interest in this context [10; 55; 30; 38], because their piecewise linear nature makes some of their properties analytically accessible [55; 10], which is a tremendous advantage from the perspective of DST. In this work, we develop an efficient heuristic algorithm for locating a PLRNN's fixed points exactly, as well as its \(k\)-cycles.

Bifurcations and loss jumps in RNN trainingThe idea that bifurcations in RNN dynamics could impede the training process is not new [16; 44]. Doya , to our knowledge, was the first to point out that even in simple single-unit RNNs with sigmoid activation function (saddle-node) bifurcations may occur as an RNN parameter is adapted during training. This may not only cause an abrupt jump in training loss, but could lead to situations where it is impossible, even in principle, to reach the training objective (the desired target output), as across the bifurcation point there is a _discrete_ change in network behavior [16; 68; 4]. Pascanu et al.  discussed similar associations between steep cliffs in RNN loss functions and bifurcations. Although profound for training success, this topic received surprisingly little attention over the years. Haschke and Steil  extended previous work by a more formal treatment of bifurcation boundaries in RNNs, and Marichal et al.  examined fold bifurcations in RNNs. The effects of bifurcations and their relation to exploding gradients in gated recurrent units (GRUs) was investigated in Kanai et al. . Ribeiro et al.  looked at the connection between dynamics and smoothness of the cost function, but failed to find a link between bifurcations and jumps in performance. In contrast, Rehmer and Kroll  observed large gradients at bifurcation boundaries and concluded that bifurcations can indeed cause problems in gradient-based optimization. To the best of our knowledge, we are, however, the first to _formally prove_ a direct link between bifurcations and the behavior of loss gradients, and to derive a systematic and efficient algorithmic procedure for identifying bifurcation manifolds for a class of ReLU-based RNNs.

## 3 Theoretical analysis

In this paper we will focus on PLRNNs as one representative of the wider class of ReLU-based RNNs, but similar derivations and algorithmic procedures could be devised for any type of ReLU-based RNN (in fact, many other types of ReLU-based RNNs could be brought into the same functional form as PLRNNs; e.g. ). We will first, in sect. 3.1, provide some theoretical background on bifurcations in PLRNNs, and illustrate how existence and stability regions of fixed points and cycles could be analytically computed for low dimensional (\(2d\)) PLRNNs. In sect. 3.2 we will then state two theorems regarding the association of bifurcations and loss gradients in training. It turns out that for certain types of bifurcations exploding or vanishing gradients are inevitable in gradient-based training procedures like Back-Propagation Through Time (BPTT).

### Bifurcation curves in PLRNN parameter space

The PLRNN, originally introduced as a kind of discrete time neural population model , has the general form

\[_{t}=F_{}(_{t-1},_{t})=\:_{t-1}+(_{t-1})+_{t}+,\] (1)

where \(_{t}^{M}\) is the latent state vector and \(\) are system parameters consisting of diagonal matrix \(^{M M}\) (auto-regression weights), off-diagonal matrix \(^{M M}\) (coupling weights), \((_{t-1})=(_{t-1},0)\) is the element-wise rectified linear unit (\(ReLU\)) function, \(^{M}\) a constant bias term, and \(_{t}^{K}\) represents external inputs weighted by \(^{M K}\). The original formulation of the PLRNN is stochastic [18; 30], with a Gaussian noise term added to eq. (1), but here we will consider the deterministic variant.

Formally, like other ReLU-based RNNs, PLRNNs constitute piecewise linear (PWL) maps, a subclass of piecewise smooth (PWS) discrete-time DS. Define \(_{(t)}(_{(t)})\) as a diagonal matrix with indicator vector \(_{(t)}(d_{1},d_{2},,d_{M})\) such that \(d_{m}(z_{m,t}) d_{m}=1\) whenever \(z_{m,t}>0\), and zero otherwise. Then (1) can be rewritten as

\[_{t}\,=\,F_{}(_{t-1})\,=\,(+_{ (t-1)})_{t-1}\,+\,_{(t-1)}\:_{t-1}\,+\, ,\] (2)

where we have ignored external inputs \(_{t}\) for simplicity. There are in general \(2^{M}\) different configurations for matrix \(_{(t-1)}\) and hence for matrix \(\,_{(t-1)}\), dividing the phase space into \(2^{M}\) sub-regions separated by switching manifolds (see Appx. A.1.1 for more details).

Recall that fixed points of a map \(_{t}=F_{}(_{t-1})\) are defined as the set of points for which we have \(^{*}=F_{}(^{*})\), and that the type (node, saddle, spiral) and stability of a fixed point can be read off from the eigenvalues of the Jacobian \(_{t}}(_{t-1})}{_{t-1}}=_{t}}{_{t-1}}\) evaluated at \(^{*}\)[3; 47]. Similarly, a \(k\)-cycle of map \(F_{}\) is a periodic orbit \(\{_{1}^{*},_{2}^{*},,_{k}^{*}\}\) such that each of the periodic points \(_{i}^{*},i=1 k\), is distinct, and is a solution to the equation \(_{i}^{*}=F_{}^{k}(_{i}^{*})\), i.e. the \(k\) times iterated map \(F_{}\). Type and stability of a \(k\)-cycle are then determined via the Jacobian \(_{r=1}^{k}_{t+k-r}=_{r=1}^{k}_{t+k-r}}{ _{t+k-r-1}}=_{t+k-1}}{_{t-1}}\). Solving these equations and computing the corresponding Jacobians thus allows to determine all existence and stability regions of fixed points and cycles, where the latter are a subset of the former, bounded by bifurcation curves (see Appx. A.1 for more formal details).

To provide a specific example, assume \(M=2\) and fix - for the purpose of this exposition - parameters \(w_{12}=w_{22}=0\), such that we have

\[_{^{1}}=_{^{3}}=a_{11}&0\\ 0&a_{22}\,,_{^{2}}=_{^{4}}= a_{11}+w_{11}&0\\ w_{21}&a_{22}\,,\] (3)

i.e. only one border which divides the phase space into two distinct sub-regions (see Appx. A.1.1). For this setup, Fig. 1A provides examples of analytically determined stability regions for two low order cycles in the \((a_{11},\,a_{11}+w_{11})\)-parameter plane (see Appx. A.1). Note that there are regions in parameter space where two or more stability regions overlap: In these regions we have _multi-stability_, the co-existence of different attractor states in the PLRNN's state space.

As noted above, bifurcation curves delimit the different stability regions in parameter space and are hence associated with abrupt changes in the topological structure of a system's state space. In general, there are many different types of bifurcations through which dynamical objects can come into existence, disappear, or change stability (see, e.g., [3; 40; 47]), the most common ones being saddle node, transcritical, pitchfork, homoclinic, and Hopf bifurcations. In comparison with smooth systems, bifurcation theory of PWS (or PWL) maps includes additional dynamical phenomena related to the existence of borders in the phase space . _Border-collision bifurcations (BCBs)_ arise when for a PWS map a specific point of an invariant set collides with a border and this collision leads to a qualitative change of dynamics [5; 6; 42]. More specifically, a BCB occurs, if for a PWS map \(_{t}\,=\,F_{}(_{t-1})\) a fixed point or \(k\)-cycle either _crosses_ the switching manifold \(_{i}:=\{^{n}:_{i}^{}=0\}\) transversely at \(=^{*}\) and its qualitative behavior changes in the event, or if it _collides_ on the border with another fixed point or \(k\)-cycle and both objects disappear [7; 41]. _Degenerate transcritical bifurcations (DTBs)_ occur when a fixed point or a periodic point of a cycle tends to infinity and one of its eigenvalues tends to \(1\) by variation of a parameter. Specifically, let \(_{k},k 1,\) be a fixed point or a \(k\)-cycle with the periodic points \(\{_{1}^{*},_{2}^{*},,_{k}^{*}\}\), and assume \(\,^{i}\) denotes an eigenvalue of the Jacobian matrix at the periodic point \(_{i}^{*},i\{1,2,,k\}\). Then \(_{k}\) undergoes a DTB at \(=^{*}\), if \(^{i}(^{*})+1\) and \(\|_{i}^{*}\|\). \(_{k}\) undergoes a _degenerate flip bifurcation (DFB)_, iff \(^{i}(^{*})=-1\) and the map \(F^{k}\) has locally, in some neighborhood of \(_{i}^{*}\), infinitely many \(2\)-cycles at \(=^{*}\). A _center bifurcation (CB)_ occurs, if \(_{k}\) has a pair of complex conjugate eigenvalues \(_{1,2}\) and locally becomes a center at the bifurcation value \(=^{*}\), i.e. if its eigenvalues are complex and lie on the unit circle (\(|_{1,2}(^{*})|=1\)). Another important class of bifurcations are _multiple attractor bifurcations (MABs)_, discussed in more detail in Appx. A.1.5 (see Fig. S3). In addition to existence and stability regions of fixed points and cycles, in Appx. A.1.2-A.1.4 we also illustrate how to analytically determine the types of bifurcation curves bounding the existence and stability regions of \(k\)-cycles (DTB, DFB, CB and BCB curves; see also Fig. S6 for a \(1d\) example).

### Bifurcations and loss jumps in training

Here we will prove that major types of bifurcations discussed above are always associated with exploding or vanishing gradients in PLRNNs during training, and hence often with abrupt jumps in the loss. For this we may assume any generic loss function \(()\), like a negative log-likelihood or a mean-squared-error (MSE) loss, and a gradient-based training technique like BPTT  or Real-Time-Recurrent-Learning (RTRL) that involves a recursion (via chain rule) through loss terms across time. The first theorem establishes that a DTB inevitably causes exploding gradients.

**Theorem 1**.: _Consider a PLRNN of the form (2) with parameters \(=\{,,\}\). Assume that it has a stable fixed point or \(k\)-cycle \(_{k}\,(k 1)\) with \(_{_{k}}\) as its basin of attraction. If \(_{k}\) undergoes a degenerate transcritical bifurcation (DTB) for some parameter value \(=_{0}\), then the norm of the PLRNN loss gradient, \(\|_{t}}{}\|\), tends to infinity at \(=_{0}\) for every \(_{1}_{_{k}}\), i.e. \(_{_{0}}\|_{t}}{ }\|=\)._

Proof.: See Appx. A.2.1 

However, bifurcations may also cause gradients to suddenly vanish, as it is the case for a BCB as established by our second theorem:

**Theorem 2**.: _Consider a PLRNN of the form (2) with parameters \(=\{,,\}\). Assume that it has a stable fixed point or \(k\)-cycle \(_{k}\,(k 1)\) with \(_{_{k}}\) as its basin of attraction. If \(_{k}\) undergoes a border collision bifurcation (BCB) for some parameter value \(=_{0}\), then the gradient of the loss function, \(_{t}}{}\), vanishes at \(=_{0}\) for every \(_{1}_{_{k}}\), i.e. \(_{_{0}}\|_{t}}{ }\|=0\)._Proof.: See Appx. A.2.2. 

**Corollary 1**.: _Assume that the PLRNN (2) has a stable fixed point \(_{1}\) with \(_{_{1}}\) as its basin of attraction. If \(_{1}\) undergoes a degenerate flip bifurcation (DFB) for some parameter value \(=_{0}\), then this will always coincide with a BCB of a \(2\)-cycle, and as a result \(_{_{0}}\|_{t}}{ }\|=0\) for every \(_{1}_{_{1}}\)._

Proof.: See Appx. A.2.3. 

Hence, certain bifurcations will inevitably cause gradients to suddenly explode or vanish, and often induce abrupt jumps in the loss (see Appx. A.3.2 for when this will happen for a BCB). We emphasize that these results are general and hold _for systems of any dimension_, as well as _in the presence of inputs_. Since inputs do not affect the Jacobians in eqn. (70), (71) and (76), they do not change the theorems (even if they would affect the Jacobians, Theorem 2 would be unaltered, and Theorem 1 could be amended in a straightforward way). Furthermore, since we are addressing bifurcations that occur during model training, from this angle inputs may simply be treated as either additional parameters (if piecewise constant) or states of the system (without changing any of the mathematical derivations). In fact, mathematically, any non-autonomous dynamical system (RNN with inputs) can always and strictly be reformulated as an autonomous system (RNN without inputs), see .

## 4 Heuristic algorithm for finding PLRNN bifurcation manifolds

### Searcher for fixed points and cycles (SCYFI): motivation and validation

In sect. 3.1 and Appx. A.1.2-A.1.4 we derived existence and stability regions for fixed points and low order (\(k 3\)) cycles in \(2d\) PLRNNs with specific parameter constraints analytically. For higher-order cycles and higher-dimensional PLRNNs (or any other ReLU-type RNN) this is no longer feasible due to the combinatorial explosion in the number of subregions that need to be considered as \(M\) and \(k\) increase. Here we therefore introduce an efficient search algorithm for finding all \(k\)-cycles of a given PLRNN, which we call _Searcher for **Cycles** and **Fixed points**_: **SCYFI** (Algorithm 1). Once all \(k\)-cycles (\(k 1\)) have been detected on some parameter grid, the stability-/existence regions of these objects and thereby the bifurcation manifolds can be determined. \(k\)-cycles were defined in sect. 3.1,

Figure 1: A) Analytically calculated stability regions for a \(2\)-cycle (\(_{}\), red), a \(3\)-cycle (\(_{^{2}}\), blue), and their intersection (yellow) in the \((a_{11},\,a_{11}+w_{11})\)-parameter plane for the system eq. (3) with \(a_{22}=0.2\), \(w_{21}=0.5\). B) Same as determined by SCYFI, with bifurcation curves bordering the stability regions labeled by the type of bifurcation (DTB = Degenerate Transcritical Bifurcation, BCB= Border Collision Bifurcation, DFB = Degenerate Flip Bifurcation). C) Bifurcation graph (showing the stable cyclic points in the \(z_{1}\) coordinate) along the cross-section in A indicated by the gray line, illustrating the different types of bifurcation encountered when moving in and out of the various stability regions in A. D) State space at the point denoted ’D’ in A (for \(a_{11}=0.253\), \(a_{11}+w_{11}=-2.83\)), where the \(2\)-cycle (red) and \(3\)-cycle (blue) co-exist for the same parameter settings, with their corresponding basins of attraction indicated by lighter colors.

and for the PLRNN, eq. (2), are given by the set of \(k\)-periodic points \(\{_{1}^{*},,_{l}^{*},,_{k}^{*}\}\), where

\[_{k}^{*}= -_{r=0}^{k-1}_{(k-r)}^{ -1}_{j=2}^{k-1}_{r=0}^{k-j}_{(k-r)}+ ,\] (4)

if \(-_{r=0}^{k-1}_{(k-r)}\) is invertible (if not, we are dealing with a bifurcation or a continuous set of fixed points). The other periodic points are \(_{l}=F^{l}(_{k}^{*}),l=1,,k-1\), with corresponding matrices \(_{(l)}\,=\,+_{l}\,\). Now, if the diagonal entries in \(_{l}\) are consistent with the signs of the corresponding states \(z_{ml}^{*}\), i.e. if \(d_{mm}^{(l)}=1\) if \(z_{ml}^{*}>0\) and \(d_{mm}^{(l)}=0\) otherwise for all \(l\), \(\{_{1}^{*},,_{k}^{*}\}\) is a true cycle of eq. (2), otherwise we call it _virtual_. To find a \(k\)-cycle, since an \(M\)-dimensional PLRNN harbors \(2^{M}\) different linear sub-regions, there are approximately \(2^{Mk}\) different combinations of configurations of the matrices \(_{l},l=1 k\), to consider (strictly, mathematical constraints rule out some of these possibilities, e.g. not all periodic points can lie within the same sub-region/orthant).

Clearly, for higher-dimensional PLRNNs and higher cycle orders exhaustively searching this space becomes unfeasible. Instead, we found that the following heuristic works surprisingly well: First, for some order \(k\) and a random initialization of the matrices \(_{l},l=1 k\), generate a cycle candidate by solving eq. (4). If each of the points \(_{l}^{*},l=1 k\), is consistent with the diagonal entries in the corresponding matrix \(_{l},l=1 k\), and none of them is already in the current library of cyclic points, then a true \(k\)-cycle has been identified, otherwise the cycle is virtual (or a super-set of lower-order cycles). We discovered that the search becomes extremely efficient, without the need to exhaustively consider all configurations, if a new search loop is re-initialized at the last visited virtual cyclic point (i.e., all _inconsistent_ entries \(d_{mm}^{(l)}\) in the matrices \(_{l},l=1 k\), are flipped, \(d_{mm}^{(l)} 1-d_{mm}^{(l)}\), to bring them into agreement with the signs of the solution points, \(_{l}^{*}=[z_{ml}^{*}]\), of eq.

(4), thus yielding the next initial configuration). It is straightforward to see that this procedure almost surely converges if \(N_{out}\) is chosen large enough, see Appx. A.2.4. The whole procedure is formalized in Algorithm 1, and the code is available at https://github.com/DurstewitzLab/SCYFI.

To validate the algorithm, we can compare analytical solutions as derived in sect. 3.1 to the output of the algorithm. To delineate all existence and stability regions, the algorithm searches for all \(k\)-cycles up to some maximum order \(K\) along a fine grid across the (\(a_{11},\;\;a_{11}+w_{11}\))-parameter plane. A bifurcation happens whenever between two grid points a cycle appears, disappears, or changes stability (as determined from the eigenvalue spectrum of the respective \(k^{th}\)-order Jacobian). The results of this procedure are shown in Fig. 1B, illustrating that the analytical solutions for existence and stability regions precisely overlap with those identified by Algorithm 1 (see also Fig. S7).

### Numerical and theoretical results on SCYFI's scaling behavior

Because of the combinatorial nature of the problem, it is generally not feasible to obtain ground truth settings in higher dimensions for SCYFI to compare to. To nevertheless assess its scaling behavior, we therefore studied two specific scenarios. For an exhaustive search, the expected and median numbers of linear subregions \(n\) until an object of interest (fixed point or cycle) is found, i.e. the number of \(\{_{1:k}\}\) constellations that need to be inspected until the first hit, are given by

\[E[n]==+1}{m+1},= n-n}{m}}{m} }\] (5)

with \(m\) being the number of existing \(k\)-cycles and \(N\) the total number of combinations, as shown in Ahlgren  (assuming no prior knowledge about the mathematical limitations when drawing regions). The median \(\) as well as the actual median number of to-be-searched combinations required by SCYFI to find at least one \(k\)-cycle is given for low-dimensional systems in Fig. 2A as a function of cycle order \(k\), and can be seen to be surprisingly linear as confirmed by linear regression fits to the data (see Fig. 2 legend for details). To assess scaling as a function of dimensionality \(M\), we explicitly constructed systems with one known fixed point (see Appx. A.3.1 for details) and determined the number \(n\) of subregions required by SCYFI to detect this embedded fixed point (Fig. 2B). In general, the scaling depended on the system's eigenspectrum, but for reasonable scenarios was polynomial or even sublinear (Fig. 2B, see also Fig. S8). In either case, the number of required SCYFI iterations scaled much more favorably than would be expected from an exhaustive search.

How could this surprisingly good scaling behavior be explained? As shown numerically in Fig. S9, when we initiate SCYFI in different randomly selected linear subregions, it converges to the subregions including the dynamical objects of interest exponentially fast, offsetting the combinatorial explosion. A more specific and stronger theoretical result about SCYFI's convergence speed can be obtained under certain conditions on the parameters (which agrees nicely with the numerical results in Fig. 2). It rests on the observation that SCYFI is designed to move _only among subregions containing virtual or actual fixed points or cycles_, based on the fact that it is always reinitialized with the next virtual fixed (cyclic) point in case the consistency check fails. The result can be stated as follows:

Figure 2: A) Number of linear subregions \(n\) searched until at least one cycle of order \(k\) was found by SCYFI (blue) vs. the median number \(\) an exhaustive search would take by randomly drawing combinations without replacement (black) as a function of cycle order (\(M=2\) fixed). Each data point represents the median of 50 different initializations across \(5\) different PLRNN models. Error bars = median absolute deviation. Linear regression fit using weighted least-squares (\(R^{2} 0.998,p<10^{-30}\)). B) Number of linear subregions \(n\) searched until a specific fixed point was found as function of dimensionality \(M\) for different eigenvalue spectra (see Appx. A.3.1 for details).

**Theorem 3**.: _Consider a PLRNN of the form (2) with parameters \(=\{,,\}\). Under certain conditions on \(\) (for which \(\|\|+\|\|<1\)), SCYFI will converge in at most linear time._

Proof.: See Appx. A.2.5 

## 5 Loss landscapes and bifurcation curves

### Bifurcations and loss jumps in training

Fig. 3 provides a \(2d\) toy example illustrating the tight association between the loss landscape and bifurcation curves, as determined through SCYFI, for a PLRNN trained by BPTT on reproducing a specific \(16\)-cycle. Fig. 3A depicts a contour plot of the gradient norms with overlaid bifurcation curves in yellow, while Fig. 3B shows the MSE loss landscape as a relief for better appreciation of the sharp changes in loss height associated with the bifurcation curves. Shown in green are two trajectories from two different parameter initializations traced out during PLRNN training in parameter space, where training was confined to only those two parameters given in the graphs (i.e., all other PLRNN parameters were kept fixed during training for the purpose of this illustration). As confirmed in Fig. 3C & D, as soon as the training trajectory crosses the bifurcation curves in parameter space, a huge jump in the loss associated with a sudden increase in the gradient norm occurs. This illustrates empirically and graphically the theoretical results derived in sect. 3.

Next we illustrate the application of SCYFI on a real-world example, learning the behavior of a rodent spiking cortical neuron observed through time series measurements of its membrane potential (note that spiking is a highly nonlinear behavior involving fast within-spike and much slower between-spike time scales). For this, we constructed a \(6\)-dimensional delay embedding of the membrane voltage , and trained a PLRNN with one hidden layer (cf. eq. 6) using BPTT with sparse teacher forcing (STF)  to approximate the dynamics of the spiking neuron (see Appx. A.3.2 for a similar analysis on a biophysical neuron model). With \(M=6\) latent states and \(H=20\) hidden dimensions, the trained PLRNN comprises \(2^{20}\) different linear subregions and \(||=272\) parameters, much higher-dimensional than the toy example considered above. Fig. 4A gives the MAE loss as a function of training epoch (i.e., single SGD updates), while Figs. 4B & C illustrate the well-trained behavior in time (Fig. 4B) and in a \(2\)-dimensional projection of the model's state space obtained by PCA (Fig. 4C). The loss curve exhibits several steep jumps. Zooming into one of these regions (Fig. 4A; indicated by the red box) and examining the transitions in parameter space using SCYFI, we find they are indeed produced by bifurcations, with an example given in Fig. 4D. Note that we are now dealing with high-dimensional state and parameter spaces, such that visualization of results becomes tricky. For the bifurcation diagram in Fig. 4D we therefore projected all extracted \(k\)-cycles (\(k 1\)) onto a line given by the PCA-derived maximum eigenvalue component, and plotted this as a function of training epoch.1 Since SCYFI extracts all \(k\)-cycles and their eigenvalue spectrum, we can also determine the type of bifurcation that caused the jump. While before the loss jump the PLRNN already produced time series quite similar to those of the physiologically recorded cell (Fig. 4E), a

Figure 3: A) Logarithm of gradient norm of the loss in PLRNN parameter space, with ground truth parameters centered at \((0,0)\). Superimposed in yellow are bifurcation curves computed by SCYFI, and in green two examples of training trajectories from different parameter initial conditions (indicated by the stars). Red dots indicate the bifurcation crossing time points shown in C & D. B) Relief plot of the loss landscape from A to highlight the differences in loss altitude associated with the bifurcations. C) Loss during the training run represented by the green trajectory labeled C in A and B. Red dot indicates the time point of bifurcation crossing corresponding to the red dot in A and B. D) Same for trajectory labeled D in A and B.

DTB (cf. Theorem 1) produced catastrophic forgetting of the learned behavior with the PLRNN's states suddenly diverging to minus infinity (Fig. 4F; Fig. S10 also provides an example of a BCB during PLRNN training, and Fig. S11 an example of a DFB). This illustrates how SCYFI can be used to analyze the training process with respect to bifurcation events also for high-dimensional real-world examples, as well as the behavior of the trained model (Fig. 4C).

### Implications for designing training algorithms

What are potential take-homes of the results in sects. 3.2 & 5.1 for designing RNN training algorithms? One possibility is to design smart initialization or training procedures that aim to place or push an RNN into the right topological regime by taking big leaps in parameter space whenever the current regime is not fit for the data, rather than dwelling within a wrong regime for too long. These ideas are discussed in a bit more depth in Appx.A.3.3, with a proof of concept in Fig. S12.

Figure 4: A) Loss across training epochs for a PLRNN with one hidden layer trained on electrophysiological recordings from a cortical neuron. Red box zooms in on one of the training phases with a huge loss jump, caused by a DTB. Letters refer to selected training epochs in other subpanels. B) Time series of true (gray) and PLRNN-simulated (black) membrane potential in the well trained regime (see A). C) All fixed points and cycles discovered by SCYFI for the well-trained model in state space projected onto the first two principle components using PCA. Filled circles represent stable and open circles unstable objects. The stable 39-cycle corresponds to the spiking behavior. D) Bifurcation diagram of the PLRNN as a function of training epoch around the loss peak in A. Locations of stable (filled circles) and unstable (open circles) objects projected onto the first principle component. E) Model behavior as in B shortly before the DTB and associated loss jump (from the epoch indicated in A, D). F) Model behavior as in B right around the DTB (diverging to \(-\)).

Figure 5: Example loss curves during training a PLRNN (\(M=10\)) on a \(2d\) cycle using gradient descent, once without GTF (\(=0\), blue curve) but gradient clipping, and once with GTF (\(=0.1\)). Note that without GTF there are several sharp loss jumps associated with bifurcations in the PLRNN parameters, while activating GTF leads to a smooth loss curve avoiding bifurcations. Note: For direct comparability both loss curves were cut off at \(4\) and then scaled to \(\).The absolute loss is much lower for GTF.

More importantly, however, we discovered that the recently proposed technique of 'generalized teacher forcing (GTF)'  tends to circumvent bifurcations in RNN training altogether, leading to much faster convergence as illustrated in Fig. 5. The way this works is that GTF, by trading off forward-iterated RNN latent states with data-inferred states according to a specific annealing schedule during training (see Appx. A.2.6), tends to pull the RNN directly into the right dynamical regime. In fact, for DTBs we can strictly prove these will never occur in PLRNN training with the right adjustment of the GTF parameter:

**Theorem 4**.: _Consider a PLRNN of the form (2) with parameters \(=\{,,\}\). Assume that it has a stable fixed point or \(k\)-cycle \(_{k}\) (\(k 1\)) that undergoes a degenerate transcritical bifurcation (DTB) for some parameter value \(=_{0}\)._

1. _If_ \(\|\|+\|\| 1\)_, then for any GTF parameter_ \(0<<1\)_, GTF controls the system, avoiding a DTB and, hence, gradient divergence at_ \(_{0}\)_._
2. _If_ \(\|\|+\|\|=r>1\)_, then for any_ \(1-<<1\)_, GTF prevents a DTB and, hence, gradient divergence at_ \(_{0}\)_._

Proof.: See Appx. A.2.6. 

As this example illustrates, we may be able to amend training procedures such as to avoid specific types of bifurcations.

## 6 Discussion

DS theory [3; 47; 58] is increasingly appreciated in the ML/AI community as a powerful mathematical framework for understanding both the training process of ML models [49; 54; 44; 16] as well as the behavior of trained models [62; 33; 61]. While the latter is generally useful for understanding how a trained RNN performs a given ML task, with prospects of improving found solutions, it is in fact imperative in areas like science or medicine where excavating the dynamical behavior and repertoire of trained models yields direct insight into the underlying physical, biological, or medical processes the model is supposed to capture. However, application of DS theory is often not straightforward, especially when dealing with higher-dimensional systems, and commonly requires numerical routines that may only find some of the dynamical objects of interest, and also only approximate solutions. One central contribution of the present work therefore was the design of a novel algorithm, SCYFI, that can exactly locate fixed points and cycles of a wide class of ReLU-based RNNs. This provides an efficient instrument for the DS analysis of trained models, supporting their interpretability and explainability.

A surprising observation was that SCYFI often finds cycles in only linear time, despite the combinatorial nature of the problem, a feature shared with the famous Simplex algorithm for solving linear programming tasks [36; 32; 57]. While we discovered numerically that SCYFI for empirically relevant scenarios converges surprisingly fast, deriving strict theoretical guarantees is hard, and so far we could establish stronger theoretical results on its convergence properties only under specific assumptions on the RNN parameters. Further theoretical work is therefore necessary to precisely understand why the algorithm works so effectively.

In this work we applied SCYFI to illuminate the training process itself. Since RNNs are themselves DS, they are subject to different forms of bifurcations during training as their parameters are varied under the action of a training algorithm (similar considerations may apply to very deep NNs). It has been recognized for some time that bifurcations in RNN training may give rise to sudden jumps in the loss [44; 16], but the phenomenon has rarely been treated more systematically and mathematically. Another major contribution of this work thus was to formally prove a strict connection between three types of bifurcations and abrupt changes in the gradient norms, and to use SCYFI to further reveal such events during PLRNN training on various example systems. There are numerous other types of bifurcations (e.g., center bifurcations, Hopf bifurcations etc.) that are likely to impact gradients during training, only for a subset of which we could provide formal proofs here. As we have demonstrated, understanding the topological and bifurcation landscape of RNNs could help improve training algorithms and provide insights into their working. Hence, a more general understanding of how various types of bifurcation affect the training process in a diverse range of RNN architectures is a promising future avenue not only for our theoretical understandings of RNNs, but also for guiding future algorithm design.

AcknowledgementsThis work was supported by the German Research Foundation (DFG) through individual grants Du 354/10-1 & Du 354/15-1 to DD, within research cluster FOR-5159 ("Resolving prefrontal flexibility"; Du 354/14-1), and through the Excellence Strategy EXC 2181/1 - 390900948 (STRUCTURE). We also thank Mahasheta Patra for lending us code for graphing analytically derived bifurcation diagrams and state spaces.