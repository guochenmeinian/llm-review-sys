# Learning to Defer with an Uncertain Rejector via Conformal Prediction

Yizirui Fang

Department of Computer Science

Johns Hopkins University

yfang52@jhu.edu

&Eric Nalisnick

Department of Computer Science

Johns Hopkins University

nalisnick@jhu.edu

###### Abstract

_Learning to defer_ (L2D) allows prediction tasks to be allocated to a human or machine decision maker, thus getting the best of both's abilities. Yet this allocation decision depends on a'rejector' function, which could be poorly fit or otherwise misspecified. In this work, we perform uncertainty quantification for the rejector sub-component of the L2D framework. We use conformal prediction to allow the reject to output sets, instead of just the binary outcome of 'defer' or not. On tasks ranging from object to hate speech detection, we demonstrate that the uncertainty in the rejector translates to safer decisions via two forms of selective prediction.

## 1 Introduction

_Learning-to-Defer_ (L2D) by  is a framework for human-AI collaboration that divides responsibility between machine and human decision makers. For every test instance, a'rejector' function decides if the case should be passed to either a human or model (but not both). The rejector thus can be seen as a meta-classifier that determines how to assign responsibility based on which decision maker (human or machine) is more likely to make the correct prediction. While L2D systems offer the promise of improved safety and robustness--by having a human available for support--this promise critically depends on the rejector's performance. Being a predictive model itself, the rejector is susceptible to the usual failure modes, such as distribution shift between training and test data.

In this paper, we perform principled uncertainty quantification for the rejector sub-component of L2D systems. Specifically, we use the framework of _conformal prediction_ to allow the rejector to output sets, instead of just a single binary outcome (defer or not). This allows the rejector to express its uncertainty about whether the human or machine should be assigned to make the decision. In turn, this allows for safer decision making--for example, by abstaining from the prediction all together or querying _both_ the human and model for their predictions. We report experimental results on tasks ranging from object to hate speech detection, showing that having an uncertain rejector can improve performance in uncertain cases via abstaining to make a prediction or checking for consensus between the human and model predictions.

## 2 Background

### Learning to Defer

Setting, Data, and ModelWe focus on multiclass L2D (with one expert) , though the ideas are presented can straightforwardly generalize to L2D-based regression . Let \(\) denote the feature space and \(\) the label space, a categorical encoding of \(K^{ 2}\) classes. Let \(_{n}\)denote a feature vector, and \(_{n}\) denotes the associated class index. L2D assumes that we have access to human predictions, denoted \(_{n}\) for the associated feature vector \(_{n}\). The training data then includes the features, the true label, and the human's prediction: \(=\{_{n},y_{n},m_{n}\}_{n=1}^{N}\). The human is assumed to have some skill at the prediction task but is not an oracle. For example, the feature vector could be a medical image, \(m_{n}\) is the expert's diagnosis from looking at the image, and \(y_{n}\) is a true label that can only be obtained from a biopsy. L2D also assumes that the human has access to background knowledge that the classifier does not, such as years of medical training in the aforementioned example. The L2D framework requires two sub-models: a classifier and a rejector . We denote the _classifier_ as \(h:\) and the _rejector_ as \(r:\{0,1\}\). When \(r()=0\), the classifier makes the decision, and when \(r()=1\), the classifier abstains and defers the decision to the human. Thus the rejector can be thought of as a'meta-classifier,' predicting which _predictor_ would most likely be correct in its prediction.

LearningLearning in L2D requires we fit both the rejector and classifier. We assume that whoever makes the prediction--model or human--incurs a loss of zero (correct) or one (incorrect). Using the rejector to toggle between the human and model, we have the overall classifier-rejector loss:

\[L_{0-1}(h,r)=_{,,}[(1-r())\,[h()]\,+\,r()\,\,[ ]]\] (1)

where \([h()]\) denotes an indicator function that checks if the prediction and label are equal. Minimizing this loss results in the Bayes optimal classifier and rejector:

\[h^{*}()=*{arg\,max}_{y}\,(=y|),r^{*}()=[(=| )_{y}(=y|)]\] (2)

where \((|)\) is the probability of the label under the data generating process, and \((=|)\) is the probability that the expert is correct. The assumption that the expert has additional knowledge is what allows it to possibly outperform the Bayes optimal classifier.

Surrogate LossesSeveral consistent surrogate losses have been proposed for Equation 1. For our implementation, we focus on the two surrogates that have demonstrated the ability to learn calibrated predictors in practice--since the more calibrated the predictor, the better the conformal prediction results will be. Specifically, we use Verma and Nalisnick 's one-vs-all (OvA) parameterization and Cao et al. 's assymetric softmax (A-SM) parameterization. These parameterizations assume the classifier and rejector are unified via an augmented label space: \(^{}=\{\}\), where \(\) denotes the rejection option. Then let \(g_{k}:\) for \(k[1,K]\) where \(k\) denotes the class index, and let \(g_{K+1}:\) denote the rejection (\(\)) option. The \(g\) functions are analogous to the logits of a neural-network-based classifier. The OvA surrogate loss is given as :

\[_{}(g_{1},,g_{K+1};,y,m)& =[g_{y}()]+_{y^{},y^{} y }[-g_{y^{}}()]+[-g_{K+1}()]\\ &+[m=y]([g_{K+1}()]-[-g_{K+1}()])\] (3)

where \(:\{ 1\}_{+}\) is a binary surrogate loss. For instance, when \(\) is the logistic loss, we have \([f()]=(1+\{-f()\})\). The A-SM surrogate loss is defined as follows :

\[_{}(g_{1},,g_{K+1};,y,m)& =-_{}(g(),y)-[m y] (1-_{}(g(),K+1))\\ &-[m=y]_{}(g(),K+1) \] (4)

where

\[_{}(g(),y)=())}{_{ y^{}=1}^{K}(g_{y^{}}())}&\,\,\,y<K+1,\\ ())}{_{y^{}=1}^{K+1}(g_{y^{}}( ))-_{y^{}}(g_{y^{}}())}&.\]

Here the 'asymmetry' is due to the softmax having different terms in the denominator for the class and rejector terms. The symmetric softmax parameterization  has the same denominator for both terms, which leads to issues for estimating the expert's correctness probability in practice . For both parameterizations, at test time, the classifier is obtained by taking the maximum over \(g\) functions: \(=h()=*{arg\,max}_{k[1,K]}g_{k}()\). The rejection function is given as: \(r()=[g_{K+1}()_{k}g_{k}()]\).

### Conformal Prediction

_Conformal prediction_ (CP) is a distribution-free approach to uncertainty quantification with finite-sample guarantees . Given a test-time feature vector \(_{N+1}\), CP seeks to construct a prediction set \(C(_{N+1};)\) such that the true label \(_{N+1}\) is included with probability \(1-\): \((_{N+1} C(_{N+1};) ) 1-\), for \(\). \(\) is a parameter that controls the set size, as will be described below. This statement is a _marginal_ guarantee, meaning that it will hold, on average, over test samples but will not necessarily hold for any particular sample. CP's aforementioned guarantee is built off the crucial assumption that the test data is drawn exchangeably with a calibration set. To compute the parameter \(\) that controls the prediction sets, the _split_-CP (a.k.a. _inductive_ CP) algorithm  is a popular choice due to its computational and sample efficiency  and resemblance to the traditional workflow of hyperparameter tuning. Split-CP requires \(\) be fit to a held-out validation set, which must be drawn exchangeably with the test set for the CP coverage guarantee to hold. Given an already trained classifier whose softmax outputs are denoted \(()=[f_{1}(),,f_{K}()]\). CP then requires a score function be chosen that quantifies how well the model's prediction conforms to the true label. Using the softmax confidence associated with the true label is a reasonable choice: \(s(,;)=1-f_{}()\), where \(f_{}()\) is the softmax score for the true label. Others exist that incorporate all dimensions that have higher confidence than the true label . Split-CP then proceeds by evaluating \(s(,;)\) on all points in the held-out set and setting \(\) to be the \((1-)\) quantile (with a finite-sample correction) of the empirical distribution of scores. For a test time point \(_{N+1}\), the prediction set is constructed as: \(C(_{N+1})=\{j|f_{j}(_{N+1})>1-\}\), which represents the softmax dimensions that outscore the threshold \(1-\). CP is commonly evaluated by checking that the desired coverage is achieved in practice while also having efficient set sizes. The latter is crucial since the CP guarantee is trivially met by choosing \(C(_{N+1};)=\) for \((1-)\)% of cases.

## 3 Uncertain Deferral via Conformal Prediction

We will now apply the CP framework to quantify the uncertainty in the rejector sub-component of an L2D system. Concretely, instead of just outputting \(0\) (model) or \(1\) (human), we want the CP-based rejector to output a set \(C_{r}(;)\), which is an element of the superset \(\{\{0\},\{1\},\{0,1\}\}\). \(C_{r}(;)=\{0,1\}\) means that the rejector is unsure if the decision should be allocated to the human or model. Thus, instead of _prediction_ sets, we call the uncertainty set of the rejector a _deferral set_. In Section 3.2, we will discuss how to incorporate these sets into downstream decision making.

Ideal ConstructionRecalling the Bayes optimal decision rule for the rejector (Equation 2), it would be ideal if \(C_{r}(;)\) could satisfy the guarantee: \((r^{*}(_{N+1}) C_{r}(_{ N+1};)) 1-\), which means that, marginally, the probability that the output of the Bayes optimal rejector is in the set is at least \(1-\). Constructing an adaptive set via validation statistics, unfortunately, requires we have access to \((=|)\) to compute a non-conformity score. Moreover, if we did have access to \((=|)\) (or a close approximation), then we could exactly quantify the uncertainty in deferral by direct use of \((=|)\) and have no need for CP.

Practical ConstructionWe instead consider constructing the set to capture an alternative quantity: \([_{N+1}=_{N+1}]\), an indicator function representing if the human will make the correct prediction. Similarly, we wish to construct prediction sets such that this binary variable will have a coverage guarantee:

\[([_{N+1}=_{N+1}]\;\;C _{r}(_{N+1};)) 1-.\] (5)

This statement is not equivalent to the one above since the expert could be correct (i.e. \([_{N+1}=_{N+1}]=1\)) but \((|)\) still be a better predictive model (i.e. \(r^{*}()=0\)). In other words, this formulation is considering the expert's performance in isolation of the classifier's. However, the semantics are retained since \(C_{r}(_{N+1};)=\{0\}\) means that the expert will likely be wrong and so using the classifier is either a good decision or not an inferior one (if the model would also be wrong). Conversely, \(C_{r}(_{N+1};)=\{1\}\) means that the expert will likely make the correction prediction. If \(C_{r}(_{N+1};)=\{0,1\}\), then the prediction set is unsure if the expert will be correct and still suggests uncertainty in the deferral decision. This relaxation, importantly, allows us to define a conformity statistic from which to compute a practical set, as we will discuss below.

### Constructing Deferral Sets

We can construct deferral sets that follow the guarantee in Equation 5 by treating the deferral decision as a binary classification problem of whether the expert will make the correct prediction. Fortunately, both aforementioned L2D parameterizations directly model the probability that the expert will be correct. For the OvA parameterization, this probability is directly parameterized by the \((K+1)\)th binary classifier: \((=|)=[g_{K+1}( )]=(1+\{-g_{K+1}()\})^{-1},\) with the logistic loss again being assumed. The A-SM similarly uses the deferral score, but here the parameterization requires evaluating all \(K+1\) functions:

\[(=|)=_{}(g( ),K+1)=())}{_{y^{}=1}^ {K+1}(g_{y^{}}())-_{y^{}}( g_{y^{}}())}\] (6)

Both estimators have been shown to be competitively calibrated . Given these estimators, we construct the usual non-conformity score for binary classification:

\[s(,,;)=1- (=|)&=\\ (=|)&. \] (7)

To obtain the threshold \(\), one would follow the standard procedure of computing these non-conformity scores on a validation set , obtaining the \((1-)\) empirical quantile, and then applying the threshold at test time as follows:

\[C_{r}(;)=\{0\}&\,1-( =|) 1-\\ \{1\}&\,(=|) 1-\\ \{0,1\}&\] (8)

### Using Deferral Sets in Decision Making

Now that we have detailed how to construct CP deferral sets, we next address how to use them to improve decision making within the L2D framework. While there are surely alternative uses, below we detail three that we believe will be practical and useful in a variety of applications.

AbstentionThe use that likely first comes to mind is prediction with the option to abstain . In the traditional case, the classifier only makes a prediction if it is confident; otherwise, it abstains. Our CP deferral sets allow for a similar workflow, but instead of abstaining because the prediction is uncertain, the L2D system will abstain because it is uncertain about to whom to allocate responsibility, the machine or human. Specifically, if \(C_{r}(_{N+1};)=\{0,1\}\), then the L2D system will abstain. Otherwise, the system will defer if \(r^{*}()=1\).

Consensus PredictionWe next consider how to make a prediction even if \(C_{r}(_{N+1};)=\{0,1\}\). If the rejector is uncertain to defer or not, we propose querying both the model and human for their predictions. If they agree, then that consensus prediction is output as the L2D system's final prediction. If they do not agree, then the system abstains from making any prediction. This workflow has the same appeal to safety as the abstention-only option, but it will likely have higher coverage since it will make predictions when the abstention-only workflow would not.

## 4 Experiments

We now experimentally demonstrate that incorporating uncertainty into the deferral decision via CP can have tangible benefits to the safety and robustness of L2D systems. Our experiments follow closely the setup in previous works on L2D [15; 21; 3] for base models, experts simulation, data processing, training and hyperparameters, while introducing uncertainty quantification for the rejector. We trained L2D models using the OvA and A-SM surrogate losses. Taking this base L2D model, we then apply the CP procedure described in Section 3. We utilize three datasets tailored to different tasks: CIFAR-10 for object detection, HAM10000 for skin lesion diagnosis, and Hate Speech for hate speech detection.

Coverage and EfficiencyWe experimentally verify that the target coverage is met, validating CP's guarantee (Equation 5). In Table 1, we report the empirical coverage and average set size for the three aforementioned datasets. Both parameterizations meet the target coverage level (\(90\%\)) for all datasets except for OvA on CIFAR-10 (\( 87\%\)). In all cases, the sets are quite efficient, with the average set size always being less than \(1.3\). The exceptionally small set size of \(1.07\) for OvA on CIFAR-10 leads to its mis-coverage. We suspect the mis-coverage is due to (natural) train-test distribution shift.

L2D with Abstention and ConsensusWe next investigate the efficacy of the abstention and consensus decision making workflows presented in Section 3.2. In Table 1, we report the system accuracy, ratio of test points deferred, and the coverage of the system (i.e. the fraction of points for which the system does not abstain) again for CIFAR-10, HateSpeech, and HAM10000. We see that both OvA and A-SM improve upon the accuracy of the base L2D model for CIFAR-10 and HAM10000, with improvements ranging from \(2\%\) to \(5\%\). However, the coverage reduction is variable, ranging from modest (\(-8\%\)) to substantial (\(-38\%\)), meaning that the accuracy improvement would be practical in some cases (e.g. OvA for CIFAR-10) and not in other (e.g. A-SM for CIFAR-10). On HateSpeech, very few of the points were abstained, leading to uninformative accuracy results. We do not see a clear superiority between the parameterizations.

## 5 Conclusions

In this paper, we have introduced an uncertainty-based method designed to enhance existing L2D systems and help their rejectors incorporate uncertainty. However, the conformal scoring function shall be carefully parameterized to best present the probability of the expert making the correct prediction. Future work could explore the extent to which this method helps maintain system safety and robustness across various failure modes, such as distribution shifts and shifts in human's predictions.