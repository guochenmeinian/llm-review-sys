# Are Diffusion Models Vision-And-Language Reasoners?

Benno Krojer

Mila & McGill University

bennarojer@mila.quebec

&Elinor Poole-Dayan

McGill University

&Vikram Voleti

Mila & University of Montreal

Stability AI

&Christopher Pal

Mila & Polytechnique Montreal

Canada CIFAR AI Chair

ServiceNow Research

&Siva Reddy

Mila & McGill University

Facebook CIFAR AI Chair

ServiceNow Research

###### Abstract

Text-conditioned image generation models have recently shown immense qualitative success using denoising diffusion processes. However, unlike discriminative vision-and-language models, it is a non-trivial task to subject these diffusion-based generative models to automatic fine-grained quantitative evaluation of high-level phenomena such as compositionality. Towards this goal, we perform two innovations. First, we transform diffusion-based models (in our case, _Stable Diffusion_) for any image-text matching (ITM) task using a novel method called _Diffusion-ITM_. Second, we introduce the _Generative-Discriminative Evaluation Benchmark (GDBench)_ benchmark with 7 complex vision-and-language tasks, bias evaluation and detailed analysis. We find that _Stable Diffusion + DiffusionITM_ is competitive on many tasks and outperforms CLIP on compositional tasks like like CLEVR and Winoground. We further boost its compositional performance with a transfer setup by fine-tuning on MS-COCO while retaining generative capabilities. We also measure the stereotypical bias in diffusion models, and find that Stable Diffusion 2.1 is, for the most part, less biased than Stable Diffusion 1.5. Overall, our results point in an exciting direction bringing discriminative and generative model evaluation closer. We are releasing code and benchmark setup.1

## 1 Introduction

Text-to-image generation is rapidly advancing. Generated images are not only highly realistic in various styles, but also reflect the compositional structure of open-ended text prompts (Chang et al., 2023; Saharia et al., 2022; Li et al., 2022). In this work, we evaluate language-conditioned generative image models on discriminative tasks to shed light on their fine-grained understanding of vision and language. A generative objective trains a model to understand how various objects and parts compose together, and often brings non-trivial emergent capabilities with it such as latent interpolation of composite concepts (Brock et al., 2019; Rombach et al., 2022). On the other hand, discriminative vision-and-language models need only focus on the minimal information required to solve their discriminative task, which could often be spurious correlations that don't generalize (Agrawal et al., 2016). Such erroneous understanding is then exposed downstream on image-text matching tasks purposefully designed to catch it, such as image/text retrieval using Winoground (Thrush et al., 2022), ARO (Yusekegonul et al., 2023), or ImageCoDe (Krojer et al., 2022). Following these benchmarks' releases, there has been a growing focus in the vision-and-language community to fix these problems.

We hypothesize that a generative model trained to synthesize compositional data is capable of understanding the complexities required to solve hard image-text-matching tasks. To this end, **we transform a text-to-image generative model for zero-shot image-text matching**, and introduce _Diffusion Image-Text Matcher (DiffusionITM)_; Fig. 1). In this work, we use Stable Diffusion (SD) (Rombach et al., 2022) as the text-to-image model, but any other diffusion model could be used. _DiffusionITM_ achieves competitive zero-shot performance on both image and text retrieval (Tab. 1).

The naive approach for image retrieval given a text prompt would be to pick the image for which SD gives the least noise prediction error, and vice versa for text retrieval. While this works well for text retrieval (Li et al., 2023), we show it achieves random performance on image retrieval (Tab. 1). Our main insight explaining this discrepancy is that the model's success at denoising depends primarily on the visual properties of the scene, rather than equally on visuals and text (Fig. 3). Therefore, if such a model has to select among several images given a text prompt, it will have the lowest noise prediction error for a visually familiar image regardless of the text prompt. To address this, we compute the error relative to the unconditional (i.e. no text) error (Fig. 1). Our method outperforms existing diffusion-based discriminative methods (Li et al., 2023; Clark and Jaini, 2023) (see Tab. 1).

Since the original generative pretraining objective of noise prediction is far from the ultimate downstream task of image-text-matching, we explore a novel discriminative finetuning scheme with hard negative image-text-pairs on MS-COCO (Lin et al., 2014). We find that this transfers well to other datasets, and improves discriminative performance (Tab. 1) as well as generated images from DrawBench prompts (Saharia et al., 2022).

Figure 1: _**DiffusionITM allows us to apply a diffusion model to any image-text-matching task**. It overcomes the asymmetry between image and text retrieval that previously led to random performance on image retrieval via unconditional normalization: An image is selected based on the lowest noise prediction error when conditioned on the text (upper part of figure) which is normalized by the noise prediction error without text-conditioning (lower part). With this general method the image generation community can benchmark their models on complex vision-and-language tasks such as Winoground (Thrush et al., 2022) or ImageCoDe (Krojer et al., 2022).

Figure 2: Progress of Stable Diffusion from 1.5 to 2.1 on _GDBench_ tasks. _GDBench_ allows fine-grained comparison of models.

Finally, we present the _GDBench_ to foster research progress on image generation. Our _DiffusionITM_ method enables a new automatic, fine-grained, and downstream way to evaluate diverse skills in text-conditioned image generation. Thus, once we cast an image generation model as a discriminator, we can draw from the rich history of diagnostic datasets and detailed analyses in the vision-and-language literature to analyze the model's reasoning ability. Evaluation of image generation has been notoriously difficult, and recent proposals advocate using secondary model probes on generated images to test for fine-grained ability . In contrast, we directly evaluate diffusion models on downstream tasks without requiring any probes. In the same spirit as GLUE , we systematically select 7 image-text-matching tasks covering diverse reasoning skills from traditional image retrieval to diagnostic tasks (i.e. compositionality). In addition, _GDBench_ also includes a bias evaluation dataset which we use for social bias analysis.

_GDBench_ allows head-on comparison between generative models, as well as with discriminative models like CLIP . Our results are as follows:

**Stable Diffusion (SD) vs. CLIP**: SD is competitive with CLIP on many tasks, and outperforms it on challenging compositional tasks like CLEVR and Winoground Text (Tab. 1). Further tuning of SD on MS-COCO with hard negatives outperforms vanilla SD on both image and text retrieval (Tab. 2). SD has lower stereotypical bias than CLIP (Tab. 3).

**Stable Diffusion (SD) 1.5 vs 2.1**: We observe varying degrees of progress, but overall SD 2.1 outperforms SD 1.5 (Fig. 2). SD 2.1 is less biased than SD 1.5, contrary to the common trend of increased bias in larger, stronger models .

**Image generation:** Remarkably, improved discriminative performance via finetuning on MS-COCO also leads to improved high-level understanding of image generation. We find higher image-text-alignment on DrawBench  prompts compared to vanilla SD (Appendix B).

## 2 Related Work

**Vision-And-Language Understanding:** Widely popular tasks for vision-and-language understanding are often framed as discriminative tasks, such as image retrieval , VQA  or REC . We focus on image-text-matching (ITM) tasks (assigning a score to an image-text-pair) due to their general applicability and simplicity. Noisy image-text pairs are used for large-scale ITM training using contrastive learning , and has been traditionally tackled either via dual encoders like CLIP , or models with richer cross-modal interaction like BLIP . At the same time, ITM allows probing models in a controlled manner with simple metrics , as opposed to more complex metrics on generative tasks . There has been a growing interest on diagnostic benchmarks for compositionality that use hard negatives . This rich literature can now be transferred to the world of image generation with our _GDBench_ benchmark, enabling fine-grained analysis and benchmarking of text-conditioned image generation.

**Repurposing Text-Conditioned Diffusion Models:** Text-conditioned diffusion has shown remarkable advancements not only in image quality but also image-text alignment . A natural next question is how to leverage these abilities for other tasks . Two concurrent studies use similar methods to our _diffusionITM_ in a more restricted setting: _Diffusion Classifier_ performs image classification using SD by selecting the class with the lowest noise prediction error; Clark and Jaini  puts more focus on timestep-weighting, and uses Imagen  instead of SD. However, they only show competitive results for text retrieval, emphasizing image classification as a special case. Many vision-and-language reasoning tasks are also framed as image retrieval . In this work, we tackle the broader scope of vision-and-language understanding, generalize our method to image retrieval, and study hard-negative fine-tuning and transfer.

**Evaluation of Image Generation:** Image generation is traditionally evaluated along the two axes of image quality and image-text alignment, using metrics based on individual examples. The recently proposed TIFA metric  relies on an additional VQA model to answer a set of LLM-generated questions about the generated image. More traditional metrics include FID ,2017) for image quality; CLIPScore (Hessel et al., 2021; Kim et al., 2022) for image-text alignment based on CLIP-embedding; object-centric metrics (Hinz et al., 2020; Cho et al., 2022) leveraging object detectors such as DETR (Carion et al., 2020); and caption-based metrics (Hong et al., 2018) like BLEU on captions of the generated image. In contrast, _GDBench_ is not a metric on individual examples, but rather a holistic evaluation framework. _GDBench_ does not require another large model (e.g. VQA) for evaluation, and can be run on many diverse datasets.

**Bias in Image Generation Models:** It is well known that LLMs learn and amplify harmful biases present within text training corpora (Caliskan et al., 2017). Due to the lack of automatic evaluation techniques for generative models, bias investigation has mostly focused on discriminative vision-and-language (VL) models (Srinivasan and Bisk, 2022; Janghorbani and De Melo, 2023). Only few works have tackled bias in recent text-to-image models (Luccioni et al., 2023; Cho et al., 2022) and to our knowledge only Luccioni et al. (2023) focus on bias alone: They quantify social biases by generating images over several social groups (ethnicity and gender) and measuring their variation over selected attributes (gendered adjectives and professions). They found that SD 1.4 and 2.0 are biased towards groups "associated with whiteness and masculinity" across target attributes, and that SD 2.0 was more biased than 1.4. While their methods are thorough and work for black-box systems, the evaluation is quite time consuming and manual.

## 3 Our Approach to Image-Text Matching with Diffusion Models

### Diffusion Image-Text Matching: Overcoming the modality asymmetry for image retrieval

We present our method _Diffusion Image-Text Matching (ITM)_. Our goal is to assign a score to an image(\(\))-text(\(\)) pair \((,)\) which is broadly useful for downstream applications. We provide \((,)\) to the diffusion model and task it to "edit" the image according to the text. Our main intuition is if the image is not described by the text, a lot of edits are needed to fit the text, in which case it gets a low score, and vice-versa. See Appendix C for visualization.

**Text-conditioned Diffusion:** The objective of diffusion models is to denoise an image \(\) by predicting the noise \(\) added to its clean version, conditioned on text \(\) and noise level \(t\):

\[\ \ _{,,t}[\| -_{}(,t,)\|_{2}^{2}] \]

Intuitively, the predicted noise is farther from the true noise when the image-text do not fit together. To transform this for ITM tasks, the sample with the lowest L2-distance of predicted and true noise could be used to select among a set of image-text-pairs. Li et al. (2023) and Clark and Jaini (2023) (concurrent works) have focused primarily on text retrieval (with classification as a special case), where the model selects from a number of texts (or class names for classification):

\[\ \ *{arg\,min}_{}\ _{,t}[\|-_{}(,t,)\|_{2}^{2}] \]

However, naively applying this in practice would imply sampling a different \(\) for each pair \((,)\) to reduce the variance in L2-distance. Li et al. (2023) a) sample many (hundreds!) noise-timestep pairs \((,t)\) uniformly, and b) crucially keep the sampled \((,t)\) constant across different \(\) when calculating Equation 2. Finally, the guidance scale is kept at 0, thereby discarding unconditional noise prediction. This could be repurposed to perform image retrieval by iterating over images instead of text:

\[\ *{arg\,min}_{}\ _{,t}[\|-_{}(,t,)\|_{2}^{2}] \]

We observe that while this results in SD as a strong text retriever, it achieves random chance on all image retrieval tasks! Interestingly, Li et al. (2023) observed that a Bayesian posterior from a discrete-label class-conditional generative model can be approximated using the analysis:

\[p_{}(_{i})=_{i} )p_{}(_{i})}{_{j}p( _{j})p_{}(_{j})} _{,}[\|- _{}(_{t},_{i})\|^{2}]+const.\}}{_{j}\{-_{t,}[\|- _{}(_{t},_{j})\|^{2}]+const.\}}. \]

Li et al. (2023) use pairwise samples between pairs of labels \(_{i}\) and \(_{j}\), to produce a low complexity approximate computation reminiscent of paired differences tests in statistics. Their analysis is based on a marginal across all possible discrete label captions \(\) with respect to an image \(\). Incontrast, in our approach, we replace the marginal in the denominator with simply a sample from the unconditional diffusion model for \(p()\), and operate in log space. The intuition behind our approach is that taking an integral over all possible caption strings (\(\)) is equivalent to simply the unconditional, since the "caption" dimension is (implicitly) marginalized out. Fig. 3 illustrates another view of this intuition that for a correct text-image pair (Image1-Caption1), the denoising error is visibly smaller than for all mismatched text-image pairs for the same image (Image1). Moreover, for an incorrect but similar image (Image2), the denoising error for Caption1 is close to random, but could be less than that of Image1-Caption1 in absolute value. The incorrect Image2 would be selected regardless of the text since it is visually easier to denoise. Hence, the denoising error depends almost entirely on the image, independent of text conditioning ("modality asymmetry").

This analysis naturally leads to the intuitive solution of normalizing the error by the unconditional (no text) error. After all, we only care about the relative difference of how much easier or harder it becomes to denoise the image with a given text relative to when no text is given (see Fig. 1):

\[}*{arg\,min}_{} \;_{,t}[(\|-_{}(, t,)\|_{2}^{2}-\|-_{}(,t)\|_{2}^{2} )] \]

### _HardNeg-DiffusionITM_: Tuning with compositional hard negatives and transfer

Our goal is to transform diffusion-based models for discriminative image-text-matching (ITM). However, the denoising diffusion objective only considers positive image-text pairs, and the large pre-training corpus LAION  contains many noisy/simple examples, not conductive to complex linguistic reasoning. In contrast, models specifically dedicated to vision-and-language reasoning such as CLIP  or BLIP  were pre-trained with negatives and, in the case of BLIP, had access to high-quality curated image-text data. Previous baselines  ditched the generative objective and turned it fully discriminative by finetuning a ResNet on top of the frozen mid-layer features of the U-Net for each dataset separately. We instead adopt parameter-efficient finetuning with LORA layers  that are added to the cross-attention from U-Net to the text, so as not to deviate too far from pretraining representations.

We address the lack of high-quality image-text-data by fine-tuning the diffusion model on MS-COCO (109K examples) with the standard diffusion objective (see Equation 1). As MS-COCO contains diverse high-quality image-text pairs, we finetune only once, and evaluate using _GDBench_ tasks. This could be thought of as a second limited pre-training.

We address the lack of negative examples by adopting the hard negatives from Yuksekgonul et al.  on MS-COCO: swapped text elements of the same part-of-speech (e.g. "Men keep watch on a herd of goats" \(\) "Goats keep watch on a herd of men"), and CLIP-based image hard negatives. The naive approach would be to minimize the noise prediction error on positive pairs \((,_{pos})\), and maximize for negative pairs \((,_{neg})\). However, if this inverse loss were applied unhinged with potentially infinite gains, it would lead to the model predicting non-sense for everything. Therefore we threshold the hard-negative error at a relative scaling factor \(\) of the value of the positive error:

\[ =_{pos}+(_{neg},| _{pos}|)\; \] \[_{pos} =_{,,t}[\|-_{ }(,t,_{pos})\|^{2}],_{neg}=- _{,,t}[\|-_{}(,t,_{neg})\|^{2}] \]

We choose relative thresholding since we want to ensure that the model never deviates too much from its original objective of noise prediction from positive prompts. Hence \(_{neg}\) is clipped between

Figure 3: Denoising losses for two similar Flickr30K images Image1 and Image2, and Caption1 of Image1. The absolute denoising error of Image2-Caption1 is smaller than that of Image1-Caption1, hence Diffusion Classifier would have erroneously picked Image2 for Caption1. Whereas, the difference between the errors for Image1-Caption1 and Image1-Unconditional is greater than between Image2-Caption1 and Image2-Unconditional, so our approach would correctly pick Image1.

\([-_{pos},_{pos}]\). Unlike CLIP, we cannot include a large number of hard negatives in our batches since _diffusionITM_ encodes image and text together. The resulting model, _HardNeg-DiffusionITM_, is still evaluated in a zero-shot fashion on the target evaluation tasks, i.e., how well does _DiffusionITM_ trained on MS-COCO transfer to target tasks.

## 4 Data: The _GDBench_ Benchmark

There is a fundamental need to measure downstream performance of diffusion-based generative models on a wide range of vision-and-language reasoning tasks, to facilitate quick improvement and progress tracking. This has worked in the past, i.e. with the NLP GLUE benchmark (Wang et al., 2018). With this motivation, we introduce _GDBench_, a benchmark of eight diverse image-text-matching (ITM) tasks to explore many types of vision-and-language reasoning. These include 7 ability-centric and 1 bias dataset for the image generation community (examples shown in Fig. 4). Additionally, most _GDBench_ tasks have further fine-grained scores on sub-phenomena without requiring manual inspection, as is the case with evaluating generative models (Saharia et al., 2022). ITM as a reasoning benchmark offers simplicity and a surprising amount of diversity. After all, ITM has become a standard paradigm for diagnostic vision-and-language datasets (see task list below) and therefore allows interpretable evaluation on many downstream skills. _GDBench_ is similar to the spirit of downstream evaluation of generative models such as in TIFA evaluation (Hu et al., 2023) which makes use of a secondary model like VQA on the generated images and shows that downstream performance correlates with image quality. Whereas, we stay closer to the generative realm and evaluate generative models on several discriminative tasks without the need for secondary models. Below we introduce each dataset, asking "What phenomena does it cover that others do not?"

**Flickr30K**(Young et al., 2014) is a well-established open-ended image and text retrieval dataset, captioning diverse scenes involving people. Note that we changed the task setup to reduce computation overhead: A model has to retrieve among the 10 most similar examples based on the CLIP embedding space. While this might not be fair towards our CLIP baselines, we chose this setup primarily to study if SD could be useful a second-stage slow retriever, after a fast retriever like narrows down the candidates (Miech et al., 2021). Both **Winoground**(Thrush et al., 2022) and **ARO**(Yuksekgonul et al., 2023) are diagnostic benchmarks for compositionality. Winoground is carefully curated at the cost of only 400 examples and many SOTA models have not reached significantly beyond random chance. ARO is automatically generated on top of Flickr30K, MS-COCO, and others with only compositional hard text negatives. **ImageCoDe**(Krojer et al., 2022) is an image retrieval task focusing on highly similar images with complex pragmatic captions crowdsourced from a guessing game. **SVO**(Hendricks and Nematzadeh, 2021) disentangles performance along different parts-of-speech by pairing images that differ only in subject, object or verb. Lewis et al. (2022) introduced a diagnostic controllable benchmark based on simple synthetic **CLEVR\({}^{2}\)** images of 3D shapes, thereby isolating various phenomena like attribute binding or spatial relations. Finally, we include **Pets**(Parkhi et al.,

Figure 4: Examples from the datasets in _GDBench_.

2012) as a smaller quick-to-run image classification dataset. In contrast to linguistically complex _GDBench_ tasks, Pets covers the complementary skill of fine-grained recognition (37 animals).3

**Measuring Bias:** Bias evaluation is essential as large-scale models increasingly suffer from harmful biases that can impact downstream tasks. _DiffusionITM_ allows automatic, quantitative bias evaluation with bias datasets intended for discriminative vision-and-language models. We utilize the dataset from Janghorbani and De Melo (2023) and investigate three different types of social biases: religious, nationality, and sexual orientation. Quantitatively, bias is present when there is a substantially stronger association of one target group to pleasant attributes compared to unpleasant attributes over another group, as measured by some notion of distance or similarity. The target groups are represented by sets of images \(X^{I},Y^{I}\) and the attributes are captured by sets of words \(A^{T},B^{T}\). Bias is measured by the following normalized association score, \(d\), called the _effect size_:4

\[ d(X^{I},Y^{I},A^{T},B^{T})=_{x  X}\,(x,A,B)}{_{i X Y}\,(i,A,B)}-_{y Y}\,(y,A,B)}{_{i X Y}\,(i,A,B)}\\ (i,A,B)=_{a A}\,( i,a)-_{b B}\,(i,b)}{_{i X Y}\,(i,A,B)}  \]

and \((,)\) is our proposed _DiffusionITM_ score, or, in the case of CLIP, cosine similarity.

More concretely, in the case of Religion, the image sets might be \(X=\) Christians, \(Y=\) Muslims and the attribute word sets would be \(A=\{\)"joy", "trust", "good",...\(\}\), \(B=\{\)"corrupt", "vulgar", "bad",...\(\}\). Here, a positive effect size means that it is biased towards Christians and a negative effect size means it is biased towards Muslims. We provide more details under limitations in Appendix A.

Table 1: **Benchmarking Diffusion ITM with vanilla SD and hard-negative fine-tuning on MS-COCO on _GDBench_. Diffusion Classifier performs around random chance on image retrieval.5Hard negative transfer finetuning significantly improves on both.**Experiments and Results

Our main two findings are summarized in Tab. 1: First, zero-shot _DiffusionITM_ achieves performance near CLIP on image retrieval (Tab. 0(a)), overcoming the close-to-random performance of Diffusion Classifier . Second, our best hard negative transfer-finetuning strategy improves performance across the board, on both image and text retrieval.

**Hyperparameters:** Based on ablations in Li et al. , we adopt most of their setup but aim for more simplicity: Timesteps \(t\) are sampled uniformly from \(\), guidance scale is kept at 0, but we drop the complicated procedure that iteratively prunes classes after a number of noise-timestep samples \((,t)\). Instead we keep samples constant at 250 for the main zero-shot experiments in Tab. 1 and reduce it to a much more feasible number of 10 samples for other experiments.6 This is aligned with our goal that _GDBench_ should be easily adopted by researchers to evaluate their image generation method. As shown in Appendix Fig. 9, performance is not at its maximum with few samples but the trends can be studied in the same way when comparing models along different tasks. We adopt the common CLIP RN50x64 baseline and OpenCLIP ViT-L/14 for a fair comparison since SD 2.1's text backbone is from the latter. We fine-tune on the MS-COCO hard negative training set  with \(lr=1e-4\), \(=1.0\) and batchsize 112. We select a checkpoint after 8 epochs based on hard negative validation. **Runtime:** With 10 noise samples per image-text-pair evaluation on Flickr30K Text Retrieval validation takes 68 minutes on a single NVIDIA RTX A6000 GPU (compared to around 4 minutes with OpenCLIP ViT-L/14). We point to Li et al.  for an in-depth analysis of runtime. We emphasize that we envision future stronger SD-based discriminators as slow retrievers that are applied to a small set of candidates provided by a fast retriever , as well as the benefit of automatic evaluation.

  &  &  &  \\   &  &  &  &  &  &  &  \\  Vanilla SD & 46.1 & 71.2 & 74.1 & 79.4 & 30.1 & 15.7 & 9.0 \\  + MS-COCO NoNeg & 48.2 & 71.1 & 74.7 & 76.9 & 29.7 & 16.1 & 10.3 \\  + MS-COCO RandNeg\({}_{}\) & 47.7 & 71.5 & 73.8 & 77.5 & 28.3 & 16.0 & 10.7 \\  + MS-COCO HardNeg\({}_{}\) & 47.0 & 71.3 & 74.1 & 76.8 & 30.6 & 16.2 & 9.6 \\  + MS-COCO HardNeg\({}_{}\) & 52.9 & 73.1 & 76.1 & 79.4 & 34.6 & 17.2 & 10.5 \\  + HardTra + RandTra + HardImg & 49.4 & 71.7 & 75.4 & 78.4 & 31.9 & 16.6 & 9.8 \\   

Table 2: Comparison of finetuning approaches on (top) image and (bottom) text retrieval, with only 10 sampling steps due to runtime feasibility (lower performance than Tab. 1 but same trends).

_HardNeg-DiffusionITM_**performance:** We find that _HardNeg-DiffusionITM_ trained on MS-COCO transfers well to all tasks, outperforming _DiffusionITM_ (Tab. 1). In Tab. 2 we disentangle the effect of using higher quality data and various additional hard negatives. We highlight three insights: 1) Despite fine-tuning on only a single dataset, we observe gains across almost all tasks without any negatives (_NoNeg_) explicable by the general high-quality MS-COCO dataset compared to more noisy pre-training data. 2) Using hard negatives for only one modality (_HardNegTxt_/_RandNegTxt_ vs. _HardNegImg_) only improves the respective retrieval performance while showing occasional drops in the other.7 3) We therefore combine all three types of hard negatives, allowing us to work with one model, _HardNeg-DiffusionITM_, rather than multiple models specific to each modality retrieval.

**Stable Diffusion 1.5 vs. 2.1 Performance:** 2.1 improves on the majority of _GDBench_ tasks except for ARO. With _GDBench_'s diverse tasks, we can study if later versions of Stable Diffusion improve in all skill-dimensions or just some of them (see Fig. 2). Interestingly SD 2.1 does not show significant gains over SD 1.5 on all of them. Most notable is ARO (compositionality): We see only minor improvements (VG tasks) or slight drops (Order tasks). At the same time, we do see a jump on Winoground Text from 29% to 32.3% and on other less adversarial tasks such Flickr30K or SVO.

**Bias:** Both CLIP and Stable Diffusion exhibit bias towards the dominant groups, namely Christians, Americans, and Heterosexuals (Tab. 3) with Stable Diffusion 2.1 displaying the least bias. Almost all scores are statistically significant for \(p<0.01\), with the exception of the Jewish-Muslim for both SD versions (Tab. 3) and some of the Buddhist scores (Tab. 5). Version 2.1 has overall lower effect sizes (average absolute effect size of 0.65 in 2.1 vs. 0.79 in 1.5), suggesting that it is less biased than version 1.5 for this metric (Tab. 3). This goes against the trend of increased bias in stronger models (Nadeem et al., 2021). On the other hand, Luccioni et al. (2023) found that Stable Diffusion 1.4 is less biased than 2.0. Further investigation is needed to draw a strong conclusion as to whether the 2.x versions are more or less biased than the 1.x versions due to this discrepancy. It is important to note that there was a major weakening of the safety filter between version 2.0 and 2.1, which may have affected the diversity in the training set and as such, model bias.

**Analysis:** We find higher image-text-alignment of images generated by _HardNeg-DiffusionITM_ model based on human judgement. Although our method improves discriminative performance, does it also result in more compositional image generation? Crucially our finetuning on MS-COCO preserved the generative capabilities despite directly modifying the noise prediction. We therefore compare image-text-alignment of _DiffusionITM_ against _HardNeg-DiffusionITM_ on DrawBench (Saharia et al., 2022) and find promising results: From 105 complex prompts, we found that _HardNeg_ is closer to the text almost twice as often as the zero-shot model. Similarly, _HardNeg_ finetuning also shows slightly better image-text-alignment than _NoNeg_ finetuning. For more DrawBench details see (Appendix B) and other analyses (Appendix F). One might ask: how complementary are the skills learned in a generative vs. a discriminative vision-and-language model? We quantify this via the overlap of correctly predicted examples. Our hypothesis: Even if _DiffusionITM_ might have lower performance on a task, its correct predictions may still cover new examples that discriminative models fail to capture. On three datasets, we compare _DiffusionITM_ and two discriminative models (CLIP and BLIP) that were trained differently enough to expect varying predictions. However we find no evidence for

    & Target \(\) & Target \(\) & SD 2.1 & SD 1.5 & CLIP RN50x64 & CLIP ViT-B/32 \\   & Christian & Muslim & **0.94*** & 1.06* & 1.55*** & 1.71* \\  & Christian & Jewish & **1.10*** & 1.11* & 1.54* & 1.69* \\  & Jewish & Muslim & -0.15 & 0.03 & 0.23* & 0.48* \\  & Hindu & Muslim & **0.86*** & 1.21* & 1.48* & 1.65* \\   & American & Arab & **0.63*** & 0.90* & 0.72* & 1.28* \\  Sexuality & Heterosexual & LGBT & **0.84*** & 1.04* & 1.38* & 1.68* \\   &  &  &  &  \\   

Table 3: Effect sizes for the bias evaluation. Positive effect sizes indicate bias towards target group \(\), negative effect sizes indicate bias towards \(\). Effect sizes closer to 0 are less biased and statistically significant effect sizes at \(p<0.01\) are denoted by \(*\). We see that **all models exhibit biases, with SD 2.1 being the least biased**. For brevity, Buddhist scores are omitted here but contribute to the average (details in Tab. 5).

our hypothesis and perhaps even signs of an opposite trend (Fig. 8). We speculate that this points towards the big role the text encoder behind an text-to-image model plays for vision-and-language understanding. After all, Stable Diffusion relies on a frozen CLIP text encoder.

## 6 Post-submission: The intriguing case of Stable Diffusion XL

After the paper submission, Stable Diffusion XL (SDXL) (Podell et al., 2023) was released so it was a reasonable next step to include it in our comparison of different SD versions (see Fig. 2). We expected SDXL to outperform previous versions, based on our upwards trend from 1.5 to 2.1 as well as findings in Podell et al. (2023). Surprisingly, SDXL reaches significantly lower scores, below 2.1 scores on everything except three ARO subtasks, and even below 1.5 on most tasks (App. F Fig. 10 show exact numbers). Moreover, the top predictions of SDXL have the same exact scores (i.e. two images being ranked first) far more often than in SD 2.1. We confirmed that our results are not coming from a bug in our implementation and hope that future work can shed more light on quantifying the higher-level abilities of SDXL. In the meantime we offer a preliminary interpretation here: It is possible that the capabilities of image generation and image editing, specifically providing a new prompt on a partially noised image, are not always correlated. In light of this, we also offer two broader interpretations for the validity of DiffusionITM as a new evaluation methodology: Either our proposed evaluation is not generally applicable to all forms of vision-and-language skills in text-to-image models and instead measures more nuanced aspects such as image editing or text sensitivity. Alternatively, this anomaly is evidence that our evaluation is precisely working as intended and exposes flaws that might otherwise taken longer to detect via quantitative evaluation or other metrics such as FID or TIFA.

## 7 Conclusion and Future Directions

In this paper, we introduce _DiffusionITM_ and _GDBench_. _DiffusionITM_ allows us to transform any diffusion-based generative model to perform image-text matching tasks. This in turn leads to an exciting new paradigm of evaluating text-to-image models on vision-and-language reasoning using diagnostic benchmarks. We also improve vision-and-language understanding of diffusion models with a novel hard negative finetuning strategy (_HardNeg_). _GDBench_ allows one to study many different types of vision and language reasoning, ranging from: compositional (ARO, Winoground) and visual fine-grained reasoning (ImageCoDe), to elements of spatial/attribute binding (CLEVR). Through our experiments, we find that our proposed _DiffusionITM_ approach shows vision-and-language reasoning capability through increased performance on the several evaluations, as well as provides for head-on automatic comparison among generative models. We conclude that Stable Diffusion performs competitively to CLIP, and performs better on compositional tasks. We also conclude that SD 2.1 is less biased than SD 1.5. We hope that this line of work demonstrates that high-quality diffusion methods for generating images performs well on vision-and-language reasoning. We see that the simple task of image-text-matching allows us to test many types of reasoning capabilities by carefully selecting the appropriate negatives. We encourage future research to explore this paradigm on other models families like Masked Generative Transformers (Chang et al., 2023), and with stronger backbone text encoders. While we found that improved discriminative performance translates into better image-text-alignment in generated images, this should be explored in more detail. We discuss detailed limitations (especially bias-related) in Appendix A

## 8 Acknowledgements

We are grateful to the open-source community behind the Huggingface Diffusers library and the anonymous reviewers for their useful suggestions. This project was funded by the Mila-Samsung and Mila-Google grant program. SR acknowledges the support of the NSERC Discovery Grant program and the Facebook CIFAR AI Chair program. CP acknowledges the support of the Canada CIFAR AI program.