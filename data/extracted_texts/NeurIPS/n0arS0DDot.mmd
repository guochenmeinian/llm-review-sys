# BLAST: Block-Level Adaptive Structured Matrices for Efficient Deep Neural Network Inference

Changwoo Lee  Soo Min Kwon  Qing Qu  Hun-Seok Kim

University of Michigan

{cwoolee,kwonsm,qingqu,hunseok}@umich.edu

###### Abstract

Large-scale foundation models have demonstrated exceptional performance in language and vision tasks. However, the numerous dense matrix-vector operations involved in these large networks pose significant computational challenges during inference. To address these challenges, we introduce the Block-Level Adaptive Structured (BLAST) matrix, designed to learn and leverage efficient structures prevalent in the weight matrices of linear layers within deep learning models. Compared to existing structured matrices, the BLAST matrix offers substantial flexibility, as it can represent various types of structures that are either learned from data or computed from pre-existing weight matrices. We demonstrate the efficiency of using the BLAST matrix for compressing both language and vision tasks, showing that (i) for medium-sized models such as ViT and GPT-2, training with BLAST weights boosts performance while reducing complexity by 70% and 40%, respectively; and (ii) for large foundation models such as Llama-7B and DiT-XL, the BLAST matrix achieves a 2x compression while exhibiting the lowest performance degradation among all tested structured matrices. Our code is available at https://github.com/changwoolee/BLAST.

## 1 Introduction

Foundation models built on large deep neural networks (DNNs) have demonstrated remarkable performance in vision and language tasks. However, the size of these large networks poses both computational and storage challenges, especially in resource-constrained environments such as edge devices. The size of a single DNN often exceeds the capacity of the supporting hardware devices . For example, Llama-70B  demands at least 140GB of memory solely for loading its weights in half-precision floating point representation, while the state-of-the-art commercial GPU only accommodates 80GB of memory. Furthermore, inference with these networks involves numerous dense matrix-vector operations, which can be limiting when computing power is constrained.

Fortunately, large (overparameterized) DNNs often exhibits parameter redundancy, where the intrinsic dimension of the weights is much lower than the ambient dimension. As such, the weights should be _structured_, possessing hidden properties such as low-rankness  or sparsity . Hence, it is possible to replace (or factorize) these dense existing weight matrices with structured ones without degrading performance . However, using structured matrices that do not align with the true underlying structure of the weight matrices can result in significant performance degradation. We demonstrate this point in Figure 1 where we attempt to capture the structure of a diffusion model transformer (DiT)  using the low-rank structure to generate synthetic images. In Figure 1, we compress the model's linear layers by approximately 50% of the total number of parameters using low-rank weight matrices via singular value decomposition (SVD) and generate images with the compressed model (see Section 4.2 and Appendix C.3 for details). As shown in Figure 1 (middle), simply using the low-rank structure introduces unwanted artifacts in the generated images.

To address this issue, many flexible structures for modeling DNN weights have been proposed to minimize the misalignment between imposed and true low-dimensional structures. For example, Dao et al.  proposed the Monarch matrix, a specific type of Block Low-Rank (BLR) structure , in which all blocks share the same rank, intended for use in the linear layers of transformers . Matrix multiplication with a Monarch matrix can be performed efficiently using batched matrix multiplication routines. Additionally, Chen et al.  investigated a block sparse plus low-rank structure. However, all of these methods still suffer from the fact that the underlying structure of each weight matrix is not known a priori. By imposing one of these structures, performance degradation may still occur due to misalignment. Recently, Lee and Kim  introduced a data-driven design called Generalized Block Low-Rank (GBLR). This approach employs multiple rank-1 blocks with various sizes and locations learned from data via differentiable masks. Unfortunately, the GBLR matrix is optimized for custom-designed hardware, as the learned block patterns are random. It has limited usability on general GPUs as the computation of GBLR matrices does not accelerate well on them.

In this work, we introduce the Block-Level Adaptive Structured (BLAST) matrix, a versatile and efficient design tailored to uncover various low-dimensional structures in the weight matrices of DNNs for accelerated inference on GPUs. Our matrix structure leverages shared bases across block matrices with block-wise diagonal coupling factors. This structure encapsulates different structures such as low-rank, block low-rank, block-diagonal matrices, and their combinations. BLAST matrices can be applied to the training scenario from scratch or compression after training. For training from scratch, we let the linear layers of the DNN to directly adopt the BLAST structure and learn its factors from data. The factors of the BLAST matrix are constructed to have well-defined gradients, allowing them to be optimized using popular methods like stochastic gradient descent (SGD) or Adam . For compressing existing weights, we propose a factorization algorithm to learn the BLAST factors from pre-trained weights. The compression performance can be further improved by updating the BLAST factors using data, a process we call "re-training".

We demonstrate the efficiency of BLAST by training Vision Transformers (ViT)  and GPT-2  from scratch on various datasets, showing that it can reduce complexity by 70% and 40%, respectively. We also compress existing ViT and Diffusion Transformer (DiT)  models with BLAST matrices by 70% and 50%, respectively, demonstrating that BLAST compression (and re-training) achieves higher accuracy / quality compared to existing methods for ViT and DiT (see Figure 1). For the language tasks, we compress Llama-7B  by 50% via BLAST and re-train on 0.49B tokens, showing the lowest accuracy degradation with significant inference speedup on a NVIDIA A100 GPU. Overall, our contributions can be summarized as follows:

Figure 1: Examples of generated images using DiT  starting from the same noise vectors and a deterministic solver. The original model is compressed by 50% through BLAST or low-rank matrices and re-trained for 10 epochs on ImageNet. The images from the model compressed via BLAST preserve the quality of the images of the original model, whereas the images generated by the low-rank model contain more undesired artifacts.

* We propose a novel block-structured matrix called BLAST that encompasses a wide range of matrix structures, allowing for _faster matrix multiplication_. Various existing structured matrices such as Low-Rank, Monarch , and Block Diagonal matrices can be expressed using the BLAST matrix.
* We provide gradient descent-based methods to find the BLAST factors for DNN weights. We empirically show that standard DNN training with the BLAST weight matrices effectively recovers the original accuracy while achieving up to a 70% reduction in computational complexity.
* In cases where pre-trained dense weights are available, we propose a preconditioned gradient descent factorization algorithm to decompose the weights to BLAST factors for compression and further re-training. Our experimental results show that pre-trained foundation models for vision or language tasks can be compressed by 50% using BLAST matrices.

Notation and Organization.We use \(_{1}()\) to denote the largest singular value of the matrix \(\). The notation \(\) indicates Hadamard product.

The rest of the paper is organized as follows. In Section 2, we introduce the BLAST matrix and discuss its properties. In Section 3, we propose a methodology to train/compress DNNs with BLAST weight matrices. In Section 4, we demonstrate the effectiveness of the BLAST weights in improving efficiency without noticeable accuracy degradation. We discuss related works in Section 5, and conclude in Section 6.

## 2 Block-Level Adaptive Structured (BLAST) Matrix

Consider a square matrix1\(^{n n}\) for some \(n\), which has an unknown intrinsic low-dimensional structure. We first equally partition the matrix \(\) into \(b b\) blocks of size \(p p\) where \(b,p\) are constants such that \(n=bp\):

\[=_{1,1}&_{1,2}&&_{1,b}\\ _{2,1}&_{2,2}&&_{2,b}\\ &&&\\ _{b,1}&_{b,2}&&_{b,b},_{i,j} ^{p p}, i,j[b].\] (1)

Then, the BLAST matrix parameterizes each block matrix \(_{i,j}\) using three factors:

\[_{i,j}=_{i}_{i,j}_{j}^{T},\] (2)

where \(_{i},_{j}^{p r}\) are the left and the right factors, respectively, and \(_{i,j}=(_{i,j})\) is an \(r r\) diagonal matrix whose diagonal entries are \(_{i,j}^{r}\). We provide a visual representation on the rightmost side of Figure 2, and illustrate how this structure differs from other types of matrices. While the BLAST structure may appear similar to SVD, there are two notable differences: (i) the left and right factors do _not_ need to be orthonormal, and (ii) the diagonal entries do _not_ need to be positive. These distinctions make it more flexible in capturing different types of low-rank structures.

As illustrated in Figure 2, the BLAST matrix also comes with two unique properties:

Figure 2: Existing structured matrices and our proposed BLAST matrix. The unique structure of BLAST allows for flexible matrix structures while enabling faster matrix multiplication compared to existing matrices.

* **Factor Sharing:** The left factor matrix \(_{i}\) of size \(rp\) is _shared_ across \(b\) blocks at the \(i^{}\) row, i.e., \(_{i,1},,_{i,b}\). Likewise, the right factor \(_{j}\) is shared across the blocks at the \(j^{}\) column. On the other hand, the diagonal factor \(_{i,j}\) of size \(r\) is specific to each block \(_{i,j}\). Hence the total number of parameters of an \(n n\) BLAST matrix with \(b b\) number of blocks of rank \(r\) is \(2rpb+rb^{2}=2nr+rb^{2}\). This reduces the number of parameters \(b\) times by enforcing the blocks at the same row or column share the same bases.
* **Individual Diagonal Factors:** The individual diagonal factors of each block matrix are the source of the adaptivity and flexibility of the BLAST matrix. By changing the values of the diagonal factors, the BLAST matrix can encompass a wide variety of matrix structures. These factors can be estimated using _gradient descent_, since \(_{i,j}\) is a real-valued vector and \(_{i,j}=_{i}(_{i,j})_{j}^{T}\) is linear to \(_{i,j}\).

Low-Rank Matrices as Special Cases of BLAST To demonstrate how the BLAST matrix can capture different types of structures, we present an example showing how the BLAST matrix can encompass a low-rank matrix. Consider the case where all the diagonal factors are ones, i.e., \(_{i,j}=_{r}\) for all \(i,j=1,2,,b\). Then, we can write the block matrix as follows:

\[^{T}=_{1}\\ _{2}\\ \\ _{b}[_{1}_{2}_{b}]= _{1}_{1}^{T}&_{1}_{2}^{T}&&_ {1}_{b}^{T}\\ _{2}_{1}^{T}&_{2}_{2}^{T}&&_{2}_{b}^ {T}\\ &&&\\ _{b}_{1}^{T}&_{b}_{2}^{T}&&_{b}_{b}^ {T}.\]

Hence, if the true underlying structure is low-rank, we can expect the BLAST matrix to learn this specific structure. Similarly, we show in Section A.1 that the BLAST matrix can construct _low-rank_, _block-diagonal_, and _block low-rank_ matrices through different diagonal parameters. A combination of these canonical structured matrices, such as a _low-rank with block-diagonal_ matrix, can also be achieved by simply concatenating the factors of each matrix.

Matrix Multiplication DNNs involve numerous matrix-vector (matrix-matrix) multiplications in the form of \(=\) (\(=\)). Algorithm 1 depicts the BLAST matrix-vector multiplication procedure. Consider the partitioned input vector \(=[_{1}^{T},_{2}^{T},,_{b}^{T}]^{T}\) and the partitioned output vector \(=[_{1}^{T},_{2}^{T},,_{b}^{T}]^{T}\). The \(i^{}\) partitioned output vector \(_{i}\) is then computed by the sum of the \(b\) block-wise matrix-vector multiplications along \(j=1,,b\):

\[_{i}=_{j=1}^{b}_{i,j}_{j}=_{j=1}^{b}_{i} _{i,j}_{j}^{T}_{j}=_{i}(_{j=1}^{b}_{i,j}( _{j}^{T}_{j})), i=1,,b.\] (3)

The number of multiplications required to perform the matrix-vector multiplication \(=\) is \((2n+b^{2})r\). The matrix multiplication \(_{j}=_{j}^{T}_{j},\,j=1,,b\) is computed once and shared across \(i=1,,b\), whereas the matrix multiplications in Line 3 and Line 6 of Algorithm 1 can be executed in parallel, e.g., by torch.bmm in PyTorch . An implementation of Algorithm 1 for general matrix or tensor inputs can be found in Appendix A.

```
0:\(,,,\)
1:\([_{1}^{T},_{2}^{T},,_{b}^{T}]^{T}\)
2:for\(j=1,2,,b\)do\(\)#Parallel
3:\(_{j}_{j}^{T}_{j}\)
4:endfor
5:for\(i=1,2,,b\)do\(\)#Parallel
6:\(_{i}_{i}_{j=1}^{b}_{i,j}_{j}\)
7:endfor
8:return\([_{1}^{T},,_{b}^{T}]^{T}\) ```

**Algorithm 1** BLAST Matrix-Vector Product

## 3 Applications of BLAST Matrices

There are two main applications of BLAST matrices: (i) _training from scratch_ with the BLAST structure and (ii) _compression of pre-trained weights_ using BLAST factorization.

### Training from Scratch using BLAST Matrices

To train a DNN on a dataset, parameters are typically initialized randomly and updated through stochastic gradient descent. In this setting, BLAST can replace dense weights to learn structuresfrom the training data. Instead of using random dense weight matrices, the model is initialized with random BLAST factors \(_{i},_{j},_{i,j}\). Since the forward and the backward path of the linear layer involving the weight matrix is composed of three linear operations as in Equation (3), the derivatives of the minibatch loss can be back-propagated by automatic differentiation frameworks . Hence, all of the trainable parameters of BLAST can be updated using conventional optimizers (e.g., Adam  or AdamW ) without additional treatment.

### Compressing Weights via BLAST Factorization

BLAST Factorization via Gradient DescentGiven pre-trained dense weights of a DNN, we can compress the weights using BLAST matrices. Let \(\) denote the weight matrix and \(_{i,j}\) denote its blocks. We estimate the BLAST factors of \(_{i,j}\) by finding the factors of the BLAST matrix that minimize the Frobenius norm error between the original weight matrix and the BLAST structure:

\[(_{*},_{*},_{*,*})=_{i=1}^{b}_{j=1}^{b}\|_{i,j}-_{i}(_{i,j})_{j}^{T} \|_{F}^{2},\] (4)

where \(*\) denotes the collection of all \(b\) components along the axis. This problem shares many characteristics with the classical matrix factorization problem [23; 24; 25; 26], and hence we can solve for the factors using alternating gradient descent starting from small random initialization (e.g., Line 1 of Algorithm 2) [27; 8]. That is, the \(k^{}\) gradient descent step is composed of three alternating updates with a step size \(>0\):

\[_{i}^{(k+1)} _{i}^{(k)}-_{_{i}^{(k)}}_{ _{i}^{(k)}}(_{*}^{(k)},_{*}^{(k)},_{*,*}^{(k) }),\] (5) \[_{j}^{(k+1)} _{j}^{(k)}-_{_{j}^{(k)}}_{ _{j}^{(k)}}(_{*}^{(k+1)},_{*}^{(k)},_{*,*}^{(k )}),\] (6) \[_{i,j}^{(k+1)} _{i,j}^{(k)}-_{_{i,j}^{(k)}}_{ _{i,j}^{(k)}}(_{*}^{(k+1)},_{*}^{(k+1)},_{*,* }^{(k)}).\] (7)

With properly chosen step sizes, Equations (5) to (7) always decrease the loss value whenever the current variables do not have any infinite entries and the gradient is non-zero. Using notations \(}_{i}^{(k)}=[_{i,1}^{(k)}_{1}^{(k)T}_{ i,b}^{(k)}_{b}^{(k)T}]^{T}\) and \(}_{j}^{(k)}=[(_{1}^{(k+1)}_{1,j}^{(k)})^{T}( _{b}^{(k+1)}_{b,j}^{(k)})^{T}]^{T}\) to indicate the concatenation of the right and left factors scaled by the diagonal components, the loss is monotonically non-increasing as in the following theorem.

**Theorem 1**.: _Let \(_{i,j}^{p p}\) be a target block and \(_{i}^{(k)},_{j}^{(k)}^{p r}\), and \(_{i,j}^{(k)}^{r}\) be factors of a block in the BLAST matrix to be optimized. With the step sizes \(0<_{_{i}^{(k)}} 1/_{1}(}_{i}^{(k)T}}_{i }^{(k)}),\)\(0<_{_{j}^{(k)}} 1/_{1}(}_{j}^{(k)T}}_{j}^{(k)} ),\)\(0<_{_{i,j}^{(k)}} 1/_{1}((_{i}^{(k+1)T}_{i}^{(k+1)} )(_{j}^{(k+1)T}_{j}^{(k+1)}))\), the gradient descent updates in Equations (5) to (7) monotonically non-increase the loss:_

\[(_{*}^{(k+1)},_{*}^{(k+1)},_{*,*}^{(k+1)})(_{*}^{(k)},_{*}^{(k)},_{*,*}^{(k)}).\]

The proof of Theorem 1 is in Section B, which is an application of the descent lemma from classical optimization theory.

Blast Factorization via Preconditioned Gradient DescentRecall that in order to estimate the BLAST factors given a pre-trained weight \(\), we need to choose a rank \(r\). Since we do not know the rank of \(\) a priori, we may have to overestimate the rank. However, overestimating the rank may slow down the convergence rate for solving Equation (4). To illustrate this, we performed an empirical analysis of the convergence behavior on a synthetically generated target low-rank matrix \(\), whose dimension is \(256 256\) with a true rank of \(r^{*}=8\). For the analysis, we computed the factors of a BLAST matrix with \(b=16\) for various values of \(r\). We used linearly decaying step sizes \(^{(k)}=_{_{i}^{(k)}}=_{_{j}^{(k)}}=_{_{i,j}^{ (k)}}\). When \(r=r^{*}=8\) (the ranks of the left and right factors \(_{*},_{*}\) match the actual rank of \(\)), gradient descent finds a low-rank solution with minimal error within 30 iterations, as shown by the blue curve in Figure 3-left. However, in the case of \(r=32>r^{*}\) where the BLAST factors are overparameterized, we observed slower convergence and a substantial residual error after 100 steps as shown by the blue curve in Figure 3-right. This behavior is consistent with previous observations of slow convergence in ill-conditioned matrix factorization problems [23; 24].

The convergence rate of solving the overparameterized low-rank factorization by gradient descent can be improved via inexpensive _preconditioners_[23; 24] which effectively decrease the condition number at each iteration. Inspired by the preconditioned gradient descent for low-rank factorization, we generalize the idea to solve our problem by multiplying preconditioning matrices to the gradients in Equations (5) to (7). We summarize the preconditioned gradient descent method for the BLAST factorization in Algorithm 2, where the following preconditioning matrices are used:

\[^{(k)}_{_{i}} =(}^{(k)T}_{i}}^{(k)}_{i}+ )^{-1},^{(k)}_{_{j}}=(}^{(k)T}_{j} }^{(k)}_{j}+)^{-1},\] (8) \[^{(k)}_{_{i,j}} =(^{(k)}_{i,j}+)^{-1},\;^{(k)} _{i,j}=(^{(k+1)T}_{i}^{(k+1)}_{i})(^{(k +1)T}_{j}^{(k+1)}_{j}).\] (9)

\(\) is proportional to the square root of the error in Equation (4). The derivations are presented in Appendix A.2. Figure 3 shows that preconditioning improves the convergence of the overparameterized BLAST factorization. The preconditioned gradient descent (yellow curve) finds the points with low error after 100 steps, whereas the gradient descent without preconditioning fails to achieve a small error. More empirical studies on the BLAST factorization with or without preconditioning can be found in Appendix D.1. We summarize the compression procedure in Algorithm 2. The computational complexity of this compression algorithm is \(O(nr^{2}+r^{3})\) where the cubic dependency on \(r\) is from the matrix inversion steps. We emphasize that these matrix inversion steps are not substantial computational bottlenecks because \(r\) is smaller than \(n\) as in prior work [23; 24].

After BLAST compression outlined in Algorithm 2, the BLAST factors can be used directly for inference to save both storage and computational costs. However, we observe that, instead of using the estimated factors directly, using them as initial points and refining the estimates by re-training the model with BLAST factors can further improve the performance of the compressed model. We refer to this process as "re-training" after BLAST compression.

Figure 3: Convergence of the BLAST factorization with and without the preconditioning steps on noiseless low-rank matrix factorization with rank \(r^{*}\). Left: The BLAST parameter \(r=r^{*}\), Right: \(r>r^{*}\). When \(r>r^{*}\), the convergence rate of GD without the preconditioning is slowed down, while GD with the preconditioning (PrecGD) can recover the ground truth with small error.

## 4 Experimental Results

We evaluate the BLAST matrix under two settings: (i) training from scratch with random initialization in the BLAST format, and (ii) re-training after compressing the dense weights to BLAST matrices via Algorithm 2. We compare the performance of BLAST with both non-adaptive and adaptive structured matrices. Among the non-adaptive approaches, we include low-rank (LR) matrices, Monarch for block low-rank (BLR) , and Pixelfly  or Block-Diagonal for block sparse matrices. For the adaptive and learnable structured matrix category, we evaluate Gaudi-GBLR . We report the number of floating point operations (FLOPs) by counting the number of multiplications. The BLAST matrix with \(b b\) number of blocks is denoted by \(_{b}\). We used the same hyperparameter \(r\) for every target weight matrix by setting it to meet the computational budget of the DNN. All experimental details can be found in Appendix C.

### Training from Scratch

Image ClassificationWe train the reduced-size Vision Transformers (ViT)  with \(_{3}\) (BLAST with \(b=3\)) weight matrices on CIFAR-10, 100 , and ImageNet-1k for 310 epochs from _random initialization_, and compare with other structured matrices. In the CIFAR-10 and CIFAR-100 benchmarks, BLAST outperforms several non-adaptive baselines, such as Low-Rank, Pixelfly, and Monarch with higher accuracy at the same FLOPs complexity (Figure 4). Gaudi-GBLR presents the most favorable accuracy-to-FLOPs tradeoff due to its capability of learning the adaptive resource/budget allocation for each weight matrix, which is a feature that our BLAST setting lacks in this particular evaluation (as we force it to use the same \(r\) for all matrices).

However, in the context of ImageNet-1k in Table 1, weight matrices trained using BLAST with \(b=3\) attain the highest levels of accuracy with the least FLOPs. This superior performance of BLAST (despite the common \(r\) for all matrices) over Gaudi-GBLR can be attributed to its simpler training process with fewer hyperparameters. In contrast, the more complex training requirements of Gaudi-GBLR, which involve smoothness annealing and proximal gradient descent, may lead to suboptimal results for a large model such as ViT-Base in Table 1.

Language Model EvaluationWe validate the training performance of BLAST weights on language models. We replace the weights of GPT-2  with random \(_{6}\) matrices and trained the network from scratch on the WikiText 103  dataset for 100 epochs. In Figure 5, we compare the test set perplexity of BLAST with the perplexity from low-rank, block-diagonal, Monarch, and Gaudi-GBLR matrices. Similar to the ImageNet training, we found that BLAST achieves the best perplexity-FLOPs trade-off. Compared to Gaudi-GBLR, BLAST obtains a significant perplexity gain. We attribute this

  Model & Accuracy (\%) & Relative FLOPs (\%) \\  Dense ViT-Base & 78.7 & 100 \\  Low-Rank & 78.9 & 33.5 \\ Monarch  & 78.9 & 33.5 \\ Gaudi-GBLR  & 78.5 & 32.8 \\ \(_{3}\) & **79.3** & **27.8** \\  

Table 1: ImageNet validation accuracy and relative FLOPs of ViT-Base trained from scratch models with different structured weight matrices. The image and the patch sizes are \(224 224\) and \(16 16\), respectively. \(_{3}\) indicates the BLAST matrix with \(3 3\) number of blocks.

Figure 4: CIFAR-10/100 accuracy of ViT-S trained from scratch with different structured matrices.

Figure 5: Pre-training result: WikiText 103 test perplexity-FLOPs trade-off curves from GPT-2 with different types of weight matrices.

improvement to the simple training process of BLAST which requires less hyperparameter tuning than that of Gaudi-GBLR.

### Compression and Re-training

In this section, we discuss the performance of BLAST weights when pre-trained dense weights are available. We first compress the dense weights using Algorithm 2 and re-train the model on the training data with the cross-entropy loss.

ViT on ImageNet ClassificationWe compress the weights of the vision transformer (ViT) trained on ImageNet training set by BLAST\({}_{3}\) and BLAST\({}_{12}\) using Algorithm 2 and re-train the models for 35 epochs. The accuracy-FLOPs trade-off curve of each model is presented in Figure 6. Both BLAST compressed & re-trained models outperform other baselines, even though BLAST models did not use the adaptive budget allocation, unlike Gaudi-GBLR. It is observed that the accuracy of the BLAST models slightly increases from \(b=3\) to \(b=12\).

Diffusion ModelsWe compress the weights of a Diffusion Transformer (DiT)  pre-trained on ImageNet using BLAST matrices and compare its performance to SVD-based low-rank approximation. For both techniques, we match the compression ratio such that both decrease the total number of model parameters by 50%, and re-train each model for 10 epochs on the ImageNet training set. We evaluate the models by generating a total of 50,000 images using the original, low-rank, and BLAST compressed models, and compute the FID , sFID , and IS  metrics with respect to the ImageNet validation set. The objective is to observe if the compressed model can generate images as realistic as the original uncompressed model.

In Table 2, we show quantitatively that the model compressed via BLAST significantly outperforms the model compressed via SVD. The low-rank compressed model often generates unrealistic images, leading to poor metrics such as the inception score. Figure 1 also contrasts how the BLAST matrices contribute to maintaining high perceptual quality as well as a close instance-wise resemblance with the uncompressed model outputs. Due to space limitations, we defer additional qualitative results and experimental setup to Appendix D.2.

Large Language Models (LLMs)We compress the weights of Llama-7B  with BLAST matrices using Algorithm 2 by 20% and 50%, and re-train the models for 400 steps on a subset of SlimPajama  dataset using 0.49B tokens. The number of blocks \(b\) in the BLAST matrices is fixed at 16, and we use \(r=1024\) for the attention modules and \(r=1488\) for the MLP modules to achieve a 50% compression ratio. We test the WikiText-2 perplexity and the zero-shot task classification accuracy on common sense reasoning datasets including PIQA, HellaSwag, WinoGrande, BoolQ, OpenBookQA, ARC-easy and challenge . We report the performance of Low-Rank, Monarch, and Block-Diagonal weight matrices after compression at the same rate and re-training. In Table 3, the first row presents the performance of the original Llama-7B model. On 50% compression ratio in the last five rows, the Monarch and Block-Diagonal matrices fail to recover the acceptable performance. Compared to Low-Rank weights, BLAST weights achieve the lowest performance degradation in WikiText-2 perplexity and zero-shot classification accuracy. The accuracy of each common sense reasoning benchmark and extended results can be found in Appendix D.3.

We provide an analysis to quantify the performance impact of compression and re-training. We first quantify the weight compression performance at 20% compression ratio in Table 3. Although the compression ratio is moderate, Low-Rank and Monarch compression without re-training suffer from

  CR & Method & FID(\(\)) & sFID(\(\)) & IS(\(\)) \\ 
0\% & Original & 9.62 & 6.85 & 121.50 \\   & Low-Rank & 48.07 & 11.44 & 26.09 \\  & BLAST\({}_{3}\) & 10.45 & 6.72 & 111.05 \\  

Table 2: Performance comparison for compressing the weight matrices of a diffusion model followed by re-training. FID and IS scores were computed with respect to a validation dataset. CR stands for Compression Ratio.

Figure 6: Compression and re-training result: ImageNet accuracy-FLOPs trade-off curves from ViT-B with different types of weight matrices.

significant performance loss, at most 4x higher perplexity and 25% accuracy degradation. On the other hand, BLAST\({}_{16}\) compression without re-training maintains reasonable accuracy and perplexity. This shows that the flexible and adaptive structure of BLAST matrices captures more information than other types of structured matrices. Yet, BLAST compression also exhibits noticeable performance degradation on a more intensive compression ratio (see the yellow curve in Figure 7). We provide more compression-only results on Diffusion Models and LLMs in Appendix D.

We find that the re-training stage is crucial for converting a pre-trained model into an efficient version without losing significant accuracy when the compression ratio is high. In Figure 7, we show the average zero-shot classification accuracy of the compressed models with 50% compression. The models with BLAST weights before re-training (yellow curve) exhibit substantial accuracy degradation at higher compression ratios. However, re-training (blue curve) effectively recovers performance using only 0.49B tokens and 400 steps.

LLM Runtime AnalysisWe evaluate the runtime of the Llama-7B compressed by the BLAST matrices on the text generation task. For evaluation, we let the model generate the sequences of length \(L=\)10, 100, and 1000 ten times and report the average and the standard deviation of model runtime in Table 4. The instruction we use to generate the desired sequence length is "Increasing sequence: one," and the model generates the text sequence such as "two, three, " and so on, with a batch size of 1. All runtime evaluation tests were conducted on a single 40GB NVIDIA A100 GPU after compiling the script using torch.compile(). The 20% compressed model shows a 12%\(\)15% runtime reduction without any library function customization for BLAST matrix multiplication. The speedup when \(b=2\) is higher than when \(b=16\) because a larger number of blocks increases the computation overhead to perform Equation (3). Notably, the 50% compression ratio provides 32%\(\)35% runtime reduction when \(b=16\). We note that the test is highly memory-bandwidth-bounded. Thus the speedup reported in Table 4 can be mostly attributed to the reduction of parameters (i.e., memory accesses) rather than the reduction in FLOPs due to BLAST compression.

## 5 Related Works

Structured Matrix with Shared BasesSharing the bases of block-structured matrices has recently drawn interest due to its considerable memory savings. BLAST matrices exemplify this approach.

  CR & \(b\) & \(L=10\) & \(L=100\) & \(L=1000\) \\ 
0\% & N/A & 0.41 \(\)8e-5 & 3.82 \(\)9e-4 & 41.23 \(\)6e-3 \\ 
20\% & 2 & 0.35 \(\)9e-5 & 3.30 \(\)2e-3 & 35.99 \(\)4e-3 \\
20\% & 16 & 0.36 \(\)8e-5 & 3.36 \(\)2e-3 & 36.48 \(\)7e-3 \\ 
50\% & 16 & 0.31 \(\)4e-4 & 2.86 \(\)1e-2 & 30.35 \(\)2e-2 \\  

Table 4: Average runtime (in second) of Llama-7B with BLAST\({}_{b}\) weights from 10 runs of text generation. \(\): standard deviation, \(L\): the length of the generated sequence, CR: Compression Ratio. All models evaluated on a single A100 GPU (40GB) using PyTorch  after torch.compile().

  CR & Method & \# Params & Re-trained? & WikiText-2 Perplexity (\(\)) & Avg. 0-Shot Accuracy (\%) (\(\)) \\ 
0\% & Original Llama-7B & 6.74B & N/A & 9.37 & 66.07 \\   & Low-Rank & 5.41B & No & 23.67 (-14.30) & 59.57 (-6.50) \\  & Monarch  (BLR) & 5.41B & No & 47.18 (-37.81) & 48.91(-17.17) \\   & BLAST\({}_{16}\) & 5.41B & No & 12.13 (-2.76) & 62.94 (-3.14) \\   & Low-Rank & 3.51B & Yes & 26.33 (-16.96) & 48.40 (-17.67) \\  & Monarch  (BLR) & 3.50B & Yes & 7.525 (-7.53e5) & 35.03 (-31.04) \\   & Block-Diagonal & 3.50B & Yes & 5.21e6 (-5.21e6) & 34.86 (-31.21) \\   & BLAST\({}_{16}\) & 3.56B & Yes & **14.21 (-4.84)** & **56.22 (-9.84)** \\  

Table 3: Zero-shot performance of LLaMA-7B after compression and retraining. Avg. 0-Shot Accuracy stands for the average accuracy of the zero-shot classification task. CR denotes compression ratio. **Bold** indicates the best performance under the same compression ratio. BLAST\({}_{b}\) indicates the BLAST matrix with \(b b\) number of blocks.

Figure 7: Average zero-shot accuracy vs. compression ratio curves of Llama-7B by BLAST\({}_{16}\) before and after re-training.

Ashrcraft et al.  extend the BLR  format to BLR\({}^{2}\), incorporating shared bases and block-wise low-rank coupling factors determined through LQ or QR factorization. Similarly, Yen et al.  apply the concept of shared bases in the Block-Diagonal preconditioner for DNN weights. While BLAST also shares the bases of blocks, it is distinct in having a diagonal coupling matrix, as shown in Equation (2). The design of BLAST matrices aims to enhance efficiency and learn a variety of structures, from low-rank to high-rank block matrices. More importantly, identifying the diagonal coupling factors in BLAST matrices does not necessitate QR decomposition. Instead, they can be updated via gradient descent, making this approach well-suited for modeling the weight matrices in deep learning models.

**DNN Weight Pruning and Decomposition** Earlier work on DNN pruning [43; 44; 45] identifies less important parameters from the Hessian or magnitude to sparsify the weight matrix. Unlike general sparse matrices, a group sparse matrix skips computation in a group-wise manner by pruning channels [46; 47]. Sparse GPT  successfully prunes large language models with 50% sparsity without significantly degrading the performance of the original model. The model can achieve actual speedup utilizing 2:4 sparsity  on specific GPUs. However, 2:4 sparsity requires accelerators with specific architectures (e.g., NVIDIA A100 GPUs) and supports only the 50% compression ratio. On the other hand, BLAST is device-agnostic since it can be implemented with basic matrix arithmetic operations and offers diverse compression ratios.

Low-rank matrices have been widely adopted for CNN compression [49; 50] and Transformer compression [51; 52]. Additionally, Butterfly  and Monarch  factorization methods model high-rank but low-dimensional structures of the weights. Specifically, a Monarch matrix is a generalized version of a Butterfly matrix, yielding a wider spectrum of structured matrices. The number of blocks plays a key role in determining the rank of the Monarch matrix as a whole and does not generalize to another Monarch matrix with fewer blocks. Unlike Monarch, BLAST with \(b b\) blocks can express Monarch matrices with the same or fewer number of blocks, including the global low-rank matrix, i.e., \(b=1\).

**Learning Low-dimensional Structures of DNN Weights** Similar to the BLAST matrix, a Gaudi-GBLR matrix  enables learning low-dimensional structures by gradient descent in the generalized structured matrix space. Gaudi-GBLR overlaps a variable number of zero-padded rank-1 blocks to model high-rank submatrices. Although Gaudi-GBLR can express a wider spectrum of matrices than BLAST, the matrix-vector multiplication for Gaudi-GBLR is less efficient because GPUs and typical neural processors cannot handle zero-padded vectors efficiently. In contrast, the BLAST matrix-vector operation does not involve zero padding, allowing for more efficient execution in hardware for the same FLOPs (as shown in Figure 4, Figure 6, and Table 1).

## 6 Conclusion and Future Work

In this work, we introduced the BLAST matrix designed to improve the inference efficiency of large DNNs. The BLAST matrix represents various low-dimensional structures of the weight matrices with fewer parameters, while enabling efficient matrix-vector products. The BLAST factors are either learnable from data or estimated from existing weights using our preconditioned factorization algorithm. Our results on both language and vision tasks highlight the effectiveness of BLAST.

Limitations and Future WorkThe BLAST matrix-vector product consists of three steps, as detailed in Equation (3), which may degrade hardware-execution parallelism. In our evaluation, we used the same computational budget \(r\) for all matrices. Learning an adaptive budget per layer or matrix (e.g., via overparameterization [54; 7]) could further improve BLAST performance, which is left for future work. The proposed method has not been evaluated on tiny (<100M parameters) or extremely large (>10B parameters) DNNs. Additionally, optimizing runtime and power consumption via BLAST matrices with customized library functions and/or hardware accelerators also remains as future work. Furthermore, a deeper theoretical investigation into the behaviors of BLAST matrices would provide a more comprehensive understanding of their capabilities and limitations. Applying advanced re-training techniques, such as knowledge distillation  or iterative compression and distillation , to the BLAST compression pipeline is also left for future work. Finally, beyond the weight structures, we expect BLAST can also help understand and exploit low-dimensional _data_ manifolds [57; 58; 59] in future work.