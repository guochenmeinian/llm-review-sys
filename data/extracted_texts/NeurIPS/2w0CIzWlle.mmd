# Semantic Self-Consistency: Enhancing Language Model Reasoning via Semantic Weighting

Tim Knappe

Lead Author

Ryan Li Ayush Chauhan Kaylee Chhua Kevin Zhu Sean O'Brien

Algoverse AI Research

cs.timknappe@gmail.com, kevin@algoverse.us

###### Abstract

While large language models (LLMs) have rapidly improved their performance on a broad number of tasks, they still often fall short on reasoning tasks. As LLMs become more integrated in diverse real-world tasks, advancing their reasoning capabilities is crucial to their effectiveness in nuanced, complex problems. Wang et al. 's _self-consistency_ framework reveals that sampling multiple rationales before taking a majority vote reliably improves model performance across various closed-answer reasoning tasks. Standard methods based on this framework aggregate the final decisions of these rationales but fail to utilize the semantic information detailed in the step-by-step reasoning paths. Our work introduces _semantic self-consistency_, enhancing this approach by incorporating and analyzing both the reasoning paths of these rationales in addition to their final decisions before taking a majority vote. These methods not only improve the reliability of reasoning paths but also cause more robust performance on complex reasoning tasks.

## 1 Introduction

In recent years, the development of large language models has witnessed remarkable strides, with significant advancements in their accuracy and expressive capabilities [3; 28; 24; 4]. Despite these achievements, models still perform suboptimally in domains such as mathematics, commonsense, and complex algorithmic reasoning . Various methods such as _chain-of-thought_ prompting have been developed to further increase reasoning capabilities and was further enhanced by the introduction of self-consistency, which demonstrate that baselines can be pushed forward by sampling and ensembling multiple model responses with chain-of-thought to improve prediction quality [34; 23].

We build on the framework of self-consistency, proposing two techniques that add a separate semantic weighting step to rerank results based on their reasoning paths. To achieve this, we use semantic vector embeddings in combination with self-consistency to group consistent model outputs, aiding in the identification of similar responses to estimate the most likely output. Additionally, we introduce a semantic filtering mechanism that discards degenerate or hallucinated outputs, which can be utilized for analyzing smaller sample sizes. Overall, we demonstrate that self-consistency with semantic marginalization not only improves accuracy across a range of benchmarks but also serves as a filtering mechanism. By introducing these methods, we aim to provide a framework for improving performance and analyzing the semantic usage of model outputs in reasoning.

## 2 Datasets

We evaluate the models on arithmetic and commonsense reasoning using three datasets: **AQuA-RAT**, **SVAMP**, and **StrategyQA**. **AQuA-RAT** assesses models' ability to solve arithmetic problems involving basic calculations, numerical relationships, and multi-step reasoning . **SVAMP** challenges models with math problems focused on algebraic manipulations and symbolic reasoning . **StrategyQA** tests models on answering complex, open-domain questions that require strategic thinking rather than simple factual knowledge . For specific information please refer to Appendix L.

## 3 Language Models

Our models are categorized into two types: _generators_, which produce sequences such as text, code, or reasoning steps, and _featurizers_, which transform these outputs into numerical representations (vector embeddings) that summarize their meaning for analysis.

Detailed information on the configurations used for our models can be found in Appendix I.3. Additionally important hyperparameters for different methods are discussed in Appendix I We use chain-of-thought prompting for all of our experiments. The prompts can be found in Appendix K.

### Generators

For our evaluation, we use several models with varying architectures and sizes. First, we utilize **GPT-3.5**, a closed-source large-scale transformer model developed by OpenAI . Additionally, we evaluate both **Llama 2** (7B parameters)  and **Llama 3** (8B parameters) , which are open-weight models known for their strong performance on numerous benchmarks. We also include **Mistral 7B** (version 0.1), recognized for its robust capabilities across a variety of language processing tasks . Lastly, we assess **GPT-4o mini**, a lower parameter variant of the GPT-4o architecture that balances computational efficiency with high performance across diverse language tasks.

### Featurizers

All of our featurizers are based on the **BERT** (Bidirectional Encoder Representations from Transformers) model architecture , with various fine-tuned versions used to generate embedding vectors

Figure 1: Whereas baseline self-consistency comprises three steps: (1) Prompt a model with chain-of-thought, (2) generate \(n\) sampled sequences, and (3) choose results based on the most occurring final output, our proposed method, shown above, decides based on the semantic consistency of the employed reasoning path. Our assumption is that language models often apply the correct reasoning but lack the ability to conclude to the correct result.

tailored to specific datasets. **RoBERTa** is employed for the StrategyQA dataset, which requires reading comprehension and contextual reasoning, benefiting from RoBERTa's robustness in general language processing tasks . Additionally, we use **SciBERT** for the AQuA-RAT and SVAMP datasets, which focus on mathematical reasoning, as its specialization in scientific texts makes it well-suited to handle the language present in these datasets .

## 4 Methodology

We analyze three main mechanisms for weighting and categorization (CPW, sequence comparison, and filtering of anomalous points) that follow a similar operational pattern outlined below:

1. _Generate candidate responses:_ Given a query of few-shot examples, we generate \(n\) samples based on chain-of-thought prompting .
2. _Embed reasoning paths:_ We represent each generated rationale as a vector embedding using fine-tuned BERT models (e.g., SciBERT for mathematical reasoning tasks). Instead of focusing on individual sentences or tokens, we obtain a single vector representation for each entire reasoning path, capturing its overall semantic content.
3. _Semantic consistency or outlier removal:_ We apply various algorithms to weight and aggregate the responses based on their featurized embedding vectors, enhancing decision-making by emphasizing semantically consistent reasoning paths or removing outliers.

### Semantic consistency

#### 4.1.1 Centroid Proximity Weighting

In a set of examples, general answers often display similar patterns, suggesting the application of embedding vectors to map responses into an \(n\)-dimensional space. To identify the most relevant features, we first compute the centroid of the embeddings, \(=_{i=1}^{N}[i]\). Then, we calculate the distance of each vector from the centroid, \([i]=||[i]-||\), and normalize these distances, \([i]=[i]}{_{j=1}^{N}[j]}\). We assign weights to the vectors inversely proportional to their normalized distances, \([i]=[i]}\). Finally, the total weight for each unique output is computed as \([u]=_{i I(u)}[i]\), where outputs with the highest total weights are considered the most likely to be correct.

#### 4.1.2 Semantic Consensus Weighting

To compare the weighting of embedding positions, we introduce another method and weigh responses relative to their respective sequences with cosine similarity, a measurement of how similar two vectors. We take \(n_{1},n_{2},n_{3},,n_{i}\) as distinct elements in our set \(N\), where each \(n\) corresponds to a featurized embedding vector. The cosine similarity between vectors \(n_{a}\) and \(n_{b}\) is given by \((n_{a},n_{b})= n_{b}}{\|n_{a}\|_{2}\|n_{ b}\|_{2}}\), and for each \(n_{e}\), we compute the cosine similarity with every \(n_{i}\) in \(N\) and aggregate the scores: \(S_{n_{e}}=_{n_{i} N}(n_{e},n_{i})\). This process is repeated for each \(n_{j}\) in \(N\), resulting in aggregated scores \(S_{n_{1}},S_{n_{2}},S_{n_{3}},,S_{n_{i}}\), and the scores are summed based on their answer decision, leading to the selection of the highest consensual response.

### Outlier removal

To eliminate outliers, we filter responses based on proximity [18; 21; 5], isolating data points that significantly deviate and identifying flawed reasoning, degenerated outputs, or model hallucinations. We examine the following common methods: (1) **K-nearest neighbor**, using \(^{n}(x_{i}-y_{i})^{2}}\); (2) **Isolation forest**, where \(s(x,n)=2^{-}\); and (3) **Support vector machines**, defined by \(^{T}+C_{i=1}^{n}_{i}\).

Results

### Semantic consistency results

We compared Centroid Proximity Weighting (CPW) and Semantic Consensus Weighting (SCW) with the self-consistency baseline across datasets. As shown in Table 1, SCW generally outperformed CPW. For Llama 2 7B, SCW boosted accuracy on StrategyQA by **13.53 %**, while CPW improved it by **6.11 %**. GPT 3.5 also saw a **7.89 %** gain with SCW, but CPW caused a **1.6%** drop. GPT-4o mini underperformed with CPW across all datasets. Cosine similarity improved most models, except Mistral 7B on StrategyQA and Llama 3 8B on SVAMP, while CPW underperformed in six out of fifteen model-dataset pairs.

CPW improved self-consistency by **3.14%** on AQuA-RAT and **0.97%** on SVAMP but decreased performance by **-1.63%** on StrategyQA, likely due to its limited reasoning paths. This effect was seen across self-consistency, where improvements were smaller compared to other datasets. A detailed discussion of these suboptimal cases is in Appendix D.

SCW showed that weighting sequences based on consistency reduces errors and improves accuracy, outperforming baseline self-consistency.

### Outlier detection results

The results from our analysis of various outlier detection methods isolation forest, k-nearest neighbor, one-class support vector machines (SVM) demonstrate their effectiveness in refining the quality of model output. The observed increases in accuracy across these methods remain consistent towards reduced sample sizes as well, suggesting that the effectiveness of anomaly detection techniques are not solely dependent on sample size. Obtained results exhibited slight deviations between the different configurations. A review across different sets of configurations and parameters can be found under Appendix I.2.1 to I.2.3.

The found results highlight variability across datasets, with isolation forest and one-class SVM performing better on certain datasets.

## 6 Discussion

It is worth noting that our system uses embedding vectors to filter responses based on general reasoning accuracy, prioritizing broad similarity over subtle variations, as the benefit of choosing the numerical majority vote from self-consistency to yield correct answers still applies, especially in the limited rationale space. An additional analysis can be found in Appendix B.

Diverse responses are not necessarily undesirable and can lead to elevated results as shown in Appendix G.1. Against the natural feel, employed methods do not discriminate against diverse reasoning. Lowering the temperature will make multiple responses more diverse and, therefore, broaden the distribution. This will not affect performance when using CPW or outlier detection, since

 
**Dataset** & **Method/Metric** & **Llama 2 7B** & **Mistral 7B** & **GPT 3.5** & **Llama 3 8B** & **GPT-4o mini** \\   & Top prob sample & 21.65 & 24.34 & 53.63 & 43.02 & 79.22 \\  & SC baseline & 24.80 & 25.60 & 59.40 & 45.28 & 83.07 \\  & CPW & 24.60 (**-0.2**) & 29.00 (**+3.4**) & 68.00 (**+8.6**) & 46.06 (**+0.78**) & 82.68 (**-0.39**) \\  & SCW & 25.00 (**+0.2**) & 29.80 (**+4.2**) & 65.40 (**+6.0**) & 47.48 (**+2.2**) & 86.18 (**+3.11**) \\   & Top prob sample & 31.90 & 65.18 & 77.42 & 70.55 & 85.62 \\  & SC baseline & 46.50 & 68.50 & 79.80 & 73.33 & 89.80 \\  & CPW & 47.40 (**+0.9**) & 69.80 (**+1.3**) & 81.00 (**+1.2**) & 74.67 (**+1.34**) & 89.60 (**-0.2**) \\  & SCW & 46.90 (**+0.4**) & 70.20 (**+1.7**) & 80.30 (**+0.5**) & 73.00 (**-0.33**) & 92.38 (**+2.98**) \\   & Top prob sample & 46.79 & 64.27 & 63.21 & 60.32 & 75.32 \\  & SC baseline & 48.91 & 67.98 & 66.81 & 63.32 & 79.18 \\   & CPW & 55.02 (**+6.11**) & 60.70 (**-7.28**) & 65.21 (**-1.6**) & 63.32 (**+0.0**) & 73.80 (**-5.38**) \\   & SCW & 62.44 (**+13.53**) & 65.35 (**-2.63**) & 74.70 (**+7.89**) & 71.47 (**+8.15**) & 79.68 (**+0.5**) \\  

Table 1: Accuracy comparison of CPW and cosine similarity on different datasets and models, with SciBERT embeddings for AQuA-RAT and SVAMP and RoBERTa encodings for StrategyQA.

outputs farther from the mean are not outliers but sensible parts of a wider distribution. Consequently, the weighting process will remain consistent, as all values will proportionally receive lower weights.

## 7 Conclusion

Our investigation into weighting and anomaly detection methods shows that cosine similarity outperforms CPW in improving model accuracy, particularly for models like Llama 2 7B and GPT 3.5 on datasets such as StrategyQA. CPW was effective for AQuA-RAT and SVAMP, leading to accuracy increases, but less so for StrategyQA. Our system prioritizes general reasoning accuracy using embedding vectors, with numerical majority voting from self-consistency remaining a key factor in achieving correct answers, especially within limited rationale spaces. Please note that the recommended methods should be employed with carefully tested hyperparameters, as their effectiveness may vary with subtle implementation nuances.

## 8 Related Work

Reasoning is an ubiquitous issue across many domains. . One significant advancement in the area has been the development of the chain-of-thought prompting [34; 27] and self-consistency , which we extend for our Method. Self improvement of Language Models after generation is a well-known method for improving accuracy . This concept has often been adapted by other weighting methods during pre-training to improve overall accuracy [31; 20], using different methods to shift the distribution .

## 9 Limitations

Our study proposes the application of semantic vector representations to group and weigh model outputs, which is designed to facilitate the identification of consensus responses . Semantic vectors must capture variations in meaning and context, which is particularly hard in abstract reasoning tasks without a sufficient amount of context making prompting techniques to enhance the models output structure and size an important factor as visualized in Table 3. The process of clustering based on semantic vectors can be challenging due to the nuanced and abstract nature of reasoning processes. This limitation underscores the need for advanced featurization models and explicit choice of a fitting fine-tuned model . Like showcased in Table 6, multiple models should be considered for semantic analysis, to ensure that the model outputs are grouped in a way that truly reflects their underlying meaning and relevance. Without these fitting featurizers, on fields with more subtle variations or on short sequences, the employed method might not be able to distinguish different sequences well enough to uphold a notable positive effect.

    &  &  &  &  &  &  \\   &  &  &  \\   & SC baseline & 24.8 / 24.8 & 25.6 / 25.6 & 59.4 / 59.4 & 45.28 / 45.28 & 83.07 / 83.07 \\  & Isolation Forest & **28.45** / **26.0** & **26.1** / 25.97 & **65.27** / **63.73** & **72.25** / **66.89** & 70.86 / 69.78 \\  & K-nearest neighbors & 25.40 / 25.37 & 25.91 / 25.66 & **62.81** / 60.04 & **68.10** / **66.74** & 16.57 / 70.81 \\  & One-class SVM & **26.70** / 24.25 & **28.45** / **26.08** & 59.55 / 59.26 & **68.39** / **65.91** & 70.87 / 69.23 \\   & SC baseline & 46.5 / 46.5 & 68.5 / 68.5 & 79.8 / 79.8 / 73.33 / 73.33 & 89.80 / 89.80 \\  & Isolation Forest & 45.94 / 45.60 & 68.84 / 68.34 & **84.65** / **84.28** & **84.44** / **81.75** & 84.44 / 81.76 \\  & K-nearest neighbors & 45.85 / 45.71 & 68.84 / 68.52 & **84.64** / **84.42** & **82.57** / **81.85** & 82.57 / 81.85 \\  & One-class SVM & 44.94 / 43.30 & 67.23 / 65.33 & **85.23** / **85.23** / **85.21** / **80.70** & 82.11 / 80.70 \\   & SC baseline & 48.91 / 48.91 & 67.98 / 67.98 & 66.81 / 66.81 & 63.32 / 63.32 & 79.18 / 79.18 \\  & Isolation Forest & 49.34 / 49.01 & 68.70 / 68.13 & **70.07** / **69.01** & **70.80** / **69.37** & 79.91 / 79.56 \\   & K-nearest neighbors & 49.49 / 49.09 & **69.00** / **68.61** & **68.65** / **68.57** & **69.43** / **69.10** & 80.64 / 80.28 \\   & One-class SVM & **49.85** / 48.98 & **69.43** / **68.81** & **68.73** / **68.27** & **70.45** / **69.23** & **81.02** / **80.65** \\   

Table 2: Outlier detection performance on SVAMP, AQuA-RAT, and StrategyQA. Performance increase over baseline of \(n>1\%\) featured in bold. Encoded based on SciBERT for mathematical reasoning and RoBERTa for commonsense.

Reproducibility Statement

Our experiments include a variety of models with different sizes. GPT 3.5 as well as GPT-4o mini have API endpoints that are open for public use https://openai.com/blog/openai-api.

Mistral 7B is available for unrestricted use under the Apache 2.0 license, while its model architecture and setup are open source: https://github.com/Mistralai/Mistral-src.

Llama 2 7B and Llama 3 8B are models with restricted access, made available by Meta. One can gain access to them by requesting permission through the provided Meta license. https://ai.meta.com/llama/.

All of our BERT models are built upon the BERT-base model developed by google-research, which is accessible under the Apache 2.0 license, including MathBERT and SciBERT. RoBERTa can be used under the MIT license.

Our datasets as well as the configurations used for our language models are accessible throughout this paper and in the Appendix to aid the reproducibility of our experiments.

### GPU usage

Language models may produce factually incorrect or biased outputs based on user prompts. The BERT-based featurizers, trained on English corpora, may yield inconsistent results in other languages. Mistral 7B, Llama 2 7B, and Llama 3 8B lack built-in content moderation, needing external safeguards against harmful content. While GPT-4o and GPT-4o mini have stronger moderation, biases may still emerge.

Further risks include that embedding and clustering methods may introduce subtle biases by emphasizing specific response types over others. Additionally variations in model temperature and sampling can add unintended randomness. Controlled sampling and inverse temperature weighting help but require careful tuning.

We recommend using monitoring tools and responsible model deployment, particularly in high-stakes applications.

## 12 Acknowledgements

We thank Celine Lee and Andy Chung for their helpful feedback on the methods and assistance in improving the clarity of our work. We also thank the anonymous reviewers of the conference for their insightful comments and suggestions, which helped improve the quality of this paper.