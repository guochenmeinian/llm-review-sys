# On Divergence Measures for Training GFlowNets

Tiago da Silva  Eliezer de Souza da Silva  Diego Mesquita

{tiago.henrique, eliezer.silva, diego.mesquita}@fgv.br

School of Applied Mathematics

Getulio Vargas Foundation

Rio de Janeiro, Brazil

###### Abstract

Generative Flow Networks (GFlowNets) are amortized samplers of unnormalized distributions over compositional objects with applications to causal discovery, NLP, and drug design. Recently, it was shown that GFlowNets can be framed as a hierarchical variational inference (HVI) method for discrete distributions. Despite this equivalence, attempts to train GFlowNets using traditional divergence measures as learning objectives were unsuccessful. Instead, current approaches for training these models rely on minimizing the log-squared difference between a proposal (forward policy) and a target (backward policy) distribution. In this work, we first formally extend the relationship between GFlowNets and HVI to distributions on arbitrary measurable topological spaces. Then, we empirically show that the ineffectiveness of divergence-based learning of GFlowNets is due to the large gradient variance of the corresponding stochastic objectives. To address this issue, we devise a collection of provably variance-reducing control variates for gradient estimation based on the REINFORCE leave-one-out estimator. Our experimental results suggest that the resulting algorithms often accelerate training convergence when compared against previous approaches. All in all, our work contributes by narrowing the gap between GFlowNet training and HVI, paving the way for algorithmic advancements inspired by the divergence minimization viewpoint.

## 1 Introduction

The approximation of intractable distributions is one of the central issues in machine learning and modern statistics [7; 35]. In reinforcement learning (RL), a recurring goal is to find a diverse set of high-valued state-action trajectories according to a reward function. This problem may be cast as sampling trajectories proportionally to the reward, which is generally an intractable distribution over the environment [3; 9; 38; 50]. Similarly, practical Bayesian inference and probabilistic models computations involve assessing intractable posterior distributions [36; 78; 102]. In the variational inference (VI) approach, circumventing this intractability involves searching for a tractable approximation to the target distribution within a family of parametric models. Conventionally, the problem reduces to minimizing a divergence measure, such as Kullback-Leibler (KL) divergence [7; 36; 92] or Renyi-\(\) divergence [51; 70], between the variational approximation and the target.

In particular, Generative Flow Networks (GFlowNets) [3; 4; 48] are a recently proposed family of variational approximations well-suited for distribution over compositional objects (e.g., graphs and texts). GFlowNets have found empirical success within various applications from causal discovery [15; 16], NLP , and chemical and biological modeling [3; 32]. In a nutshell, a GFlowNet learns an iterative generative process (IGP)  over an extension of the target's support, which, for sufficiently expressive parameterizations of transition kernels, yields independent and correctly distributed samples [3; 48]. Remarkably, training GFlowNets typically consists of minimizing the log-squared difference between a proposal and target distributions over the extended space via SGD [4; 55], contrasting with divergence-minimizing algorithms commonly used in VI [7; 72].

Indeed, Malkin et al.  suggests that trajectory balance (TB) loss training for GFlowNets leads to better approximations of the target distribution than directly minimizing the reverse and forward KL divergence, particularly in setups with sparser rewards. Nevertheless, as we highlight in Section3, these results are a potential consequence of biases and high variance in gradient estimates for the divergence's estimates, which can be overlooked in the evaluation protocol reliant upon sparse target distributions. Therefore, in Section5, we present a comprehensive empirical investigation of the minimization of well-known \(f\)-divergence measures (including reverse and forward KL), showing it is an effective procedure that often accelerates the training convergence of GFlowNets relative to alternatives. To achieve these results, we develop in Section4 a collection of control variates (CVs) [63; 71] to reduce the variance without introducing bias on the estimated gradients, improving the efficiency of the optimization algorithms [77; 85]. In summary, our _main contributions_ are:

1. We evaluate the performance of forward and reverse KL- , Renyi-\(\) and Tsallis-\(\) divergences as learning objectives for GFlowNets through an extensive empirical campaign and highlight that they frequently outperform traditionally employed loss functions.
2. We design control variates for the gradients of GFlowNets' divergence-based objectives. Therefore, it is possible to perform efficient evaluations of the optimization objectives using automatic differentiation frameworks , and the resulting experiments showcase the significant reduction in the variance of the corresponding estimators.
3. We developed a theoretical connection between GFlowNets and VI beyond the setup of finitely supported measures [56; 112], establishing results for arbitrary topological spaces.

## 2 Revisiting the relationship between GFlowNets and VI

Initially, we review Lahlou et al. 's work on GFlowNets for distributions on topological spaces, a perspective applied consequentially to obtain the equivalence between GFlowNets training and VI divergence minimization in a more generic setting. Finally, we describe standard variance reduction techniques for solving stochastic optimization problems.

**Notations.** Let \((,)\) be a topological space with topology \(\) and \(\) be the corresponding Borel \(\)-algebra. Also, let \(_{+}\) be a measure over \(\) and \(_{f},_{b}_{+}\) be transition kernels over \(\). For each \((B_{1},B_{2})\), we denote by \((B_{1},B_{2})_{B_{1}}(s)k(s,B_{2})\). Likewise, we recursively define the _product kernel_ as \(^{ 0}(s,)=(s,)\) and, for \(n 1\), \(^{ n}(s,)=^{ n-1}(s,)\) for a transition kernel \(\) and \(s\). Note, in particular, that \(^{ n}\) is a function from \(^{ n+1}\) to \(_{+}\), with \(^{ n+1}\) representing the product \(\)-algebra of \(\)[1; 96]. Moreover, if \(\) is an absolutely continuous measure relatively to \(\), denoted \(\), we write \({}^{}\!/_{}\) for the corresponding density (Radom-Nikodym derivative) . Furthermore, we denote by \((A)=\{S S A\}\) the power-set of a set \(A\) and by \([d]=\{1,,d\}\) the first \(d\) positive integers.

**GFlowNets.** A GFlowNet is, in its most general form, built upon the concept of a _measurable pointed directed acyclic graph_ (DAG) , which we define next. Intuitively, it extends the notion of a _flow network_ to arbitrary measurable topological spaces, replacing the directed graph with a transition kernel specifying how the underlying states are connected.

**Definition 1** (Measurable pointed DAG ).: Let \((},,)\) be a measurable topological space endowed with a reference measure \(\) and forward \(_{f}\) and backward \(_{b}\) kernels. Also, let \(s_{o}}\) and \(s_{f}}\) be distinguished elements in \(}\), respectively called _initial_ and _final_ states, and \(=}\{s_{f}\}\). We assume \(\{s_{f}\}\) is open. A _measurable pointed DAG_ is then a tuple \((,,,_{f},_{b},)\) satisfying:

1. **(Terminality)** If \(_{f}(s,\{s_{f}\})>0\), then \(_{f}(s,\{s_{f}\})=1\  s}\). Also, \(_{f}(s_{f},)=_{s_{f}}\).
2. **(Reachability)** For all \(B\), \(\,n\) s.t. \(_{f}^{ n}(s_{o},B)>0\), i.e., \(B\) is reachable from \(s_{o}\).
3. **(Consistency)** For every \((B_{1},B_{2})\) such that \((B_{1},B_{2})\{(s_{o},s_{o}),(s_{f},s_{f})\}\), \(_{f}(B_{1},B_{2})=_{b}(B_{2},B_{1})\). Moreover, \(_{b}(s_{o},B)=0\) for every \(B\).
4. **(Continuity)**\(s_{f}(s,B)\) is continuous for \(B\).
5. **(Finite absorption)** There is a \(N\) such that \(_{f}^{ N}(s,)=_{s_{f}}\) for every \(s\). We designate the corresponding DAG as _finitely absorbing_.

In this setting, the elements in the set \(=\{s\{s_{f}\}_{f}(s,\{s_{f}\})>0)\}\) are called _terminal states_. Illustratively, when \(\) is finite and \(\) is the counting measure, the preceding definition 

[MISSING_PAGE_FAIL:3]

This proposition shows that minimizing the on-policy TB loss is theoretically comparable to minimizing the KL divergence between \(P_{F}\) and \(P_{B}\) in terms of convergence speed. Since the TB loss requires estimating the intractable \(R()\), the KL divergence, which avoids this estimation, can be a more suitable objective. Our experiments in Section5 support this, with proofs provided in AppendixC. Extending this result to general topological spaces broadens the scope of divergence minimization strategies, extending guarantees for discrete spaces to continuous and mixed spaces. This generalization aligns with advances in generalized Bayesian inference  and generalized VI in function spaces , via optimization of generic divergences. We make the method theoretically firm and potentially widely applicable by proving the equivalence in these broader contexts.

**Variance reduction.** A naive Monte Carlo estimator for the gradient in Equation2 has high variance , impacting the efficiency of stochastic gradient descent . To mitigate this, we use _control variates_--random variables with zero expectation added to reduce the estimator's variance without bias . This method, detailed in Section4, significantly reduces noise in gradient estimates and pragmatically improves training convergence, as shown in the experiments in Section5.

## 3 Divergence measures for learning GFlowNets

This Section presents four different divergence measures for training GFlowNets and the accompanying gradient estimators for stochastic optimization. Regardless of the learning objective, recall that our goal is to estimate \(\) by minimizing a discrepancy measure \(D\) between \(P_{F}\) and \(P_{B}\) that is globally minimized if and only if \(P_{F}=P_{B}\), i.e.,

\[^{*}=*{arg\,min}_{}D(P_{F},P_{B}),\] (3)

in which \(P_{B}\) is typically fixed and \(P_{F}\)'s density \(p_{F}^{}\) is parameterized by \(\).

### Renyi-\(\) and Tsallis-\(\) divergences

Renyi-\(\) and Tsallis-\(\) are families of statistical divergences including, as limiting cases, the widespread KL divergence (Section3.2) ; see Definition5. These divergences have been successfully applied to both variational inference  and policy search for model-based reinforcement learning . Moreover, as we highlight in Section5, their performance is competitive with, and sometimes better than, traditional learning objectives for GFlowNets based on minimizing log-squared differences between proposal and target distributions.

**Definition 5** (Renyi-\(\) and Tsallis-\(\) divergences).: Let \(\). Also, let \(p_{F_{}}\) and \(p_{B}\) be GFlowNet's forward and backward policies, respectively. Then, the _Renyi-\(\) divergence_ between \(P_{F}\) and \(P_{B}\) is

\[_{}(P_{F}||P_{B})=_{_{ S}}p_{F_{}}(|s_{o})^{}p_{B}()^{1-}_{f}(s_{o}, ).\]

Similarly, the _Tsallis-\(\) divergence_ between \(P_{F}\) and \(P_{B}\) is

\[_{}(P_{F}||P_{B})=(_{ _{S}}p_{F_{}}(|s_{o})^{}p_{B}()^{1-}_{f}(s_{o },)-1).\]

From Definition5, we see that both Renyi-\(\) and Tsallis-\(\) divergences transition from a mass-covering to a mode-seeking behavior as \(\) ranges from \(-\) to \(\). Regarding GFlowNet-training, this flexibility suggests that \(_{}\) and \(_{}\) are appropriate choices both, e.g., for carrying out Bayesian inference  -- where interest lies in obtaining an accurate approximation to a posterior distribution --, and for combinatorial optimization  -- where the goal is to find a few high-valued samples. Additionally, the choice of \(\) provides a mechanism for controlling which trajectories are preferentially sampled during training, with larger values favoring the selection of trajectories leading to high-probability terminal states, resembling the effect of \(\)-greedy , thompson-sampling , local-search , and forward-looking  techniques for carrying out off-policy training of GFlowNets .

To illustrate the effect of \(\) on the learning dynamics of GFlowNets, we show in Figure1 an _early stage_ of training to sample from a homogeneous mixture of Gaussian distributions by minimizing Renyi-\(\) divergence for different values of \(\); see Section5.1 for details on this experiment. At this stage, we note that the GFlowNet covers the target distribution's modes but fails to separate them when \(\) is large and negative. In contrast, a large positive \(\) causes the model to focus on a single high-probability region. Therefore, the use of an intermediate value for \(=0.5\) culminates in a model that accurately approximates the target distribution. Also, our early experiments suggested the persistence of such dependence on \(\) for diverse learning tasks, with \(=0.5\) leading to the best results. Thus, we fix \(=0.5\) throughout our experimental campaign.

Importantly, we need only the gradients of \(_{}\) and \(_{}\) for solving the optimization problem in Equation 3 and, in particular, learning the target distribution's normalizing constant is unnecessary, as we underline in the lemma below. This property distinguishes such divergence measures from both TB and DB losses in Equation 1 and, in principle, simplifies the training of GFlowNets.

**Lemma 1** (Gradients for \(_{}\) and \(_{}\)).: _Let \(\) be the parameters of \(p_{F_{}}\) in Definition 5 and, for \(_{S}\), \(g(,)=(p_{B}(|x)r(x)/_{p_{F_{}}(|s_{o}; )})^{1-}\). The gradient of \(_{}\) wrt \(\) is_

\[_{}_{}(p_{F_{}}||p_{B})=[ _{}g(,)+g(,)_{} p_{F_{} }(|s_{o};)]}{(-1)[g(,)]};\]

_the expectations are computed under \(P_{F}\). Analogously, the gradient of \(_{}\) wrt \(\) is_

\[_{}_{}(p_{F_{}}||p_{B})}{{=}}[_{}g(,)+g(,)_{ } p_{F_{}}(|s_{o};)]}{(-1)},\]

_in which \(}{{=}}\) denotes equality up to a multiplicative constant._

Lemma 1 uses the REINFORCE method  to compute the gradients of both \(_{}\) and \(_{}\), and we implement Monte Carlo estimators to approximate the ensuing expectations based on a batch of trajectories \(\{_{1},,_{N}\}\) sampled during training . Also, note that the function \(g\) is computed outside the log domain and, therefore, particular care is required to avoid issues such as numerical underflow of the unnormalized distribution . In our implementation, we sample an initial batch of trajectories \(\{_{i}\}_{i=1}^{N}\) and compute the maximum of \(r\) among the sampled terminal states in log space, i.e., \(=_{i} r(x_{i})\). Then, we consider \((x)= r(x)-\) as the target's unnormalized log density. In Section 4, we will consider the design of variance reduction techniques to decrease the noise level of gradient estimates and possibly speed up the learning process.

### Kullback-Leibler divergence

The KL divergence  is a limiting member of the Renyi-\(\) and Tsallis-\(\) families of divergences, derived when \( 1\), and is the most widely deployed divergence measure in statistics and machine learning. To conduct variational inference, one regularly considers both the _forward_ and _reverse_ KL divergences, which we review in the definition below.

**Definition 6** (Forward and reverse KL).: The _forward_ KL divergence between a target \(P_{B}\) and a proposal \(P_{F}\) is \(_{KL}[P_{B}||P_{F}]=_{ P_{B}(s_{},)}[  p_{B}()/p_{F_{}}(|s_{o})]\). Also, the _reverse_ KL divergence is defined by \(_{KL}[P_{F}||P_{B}]=_{ P_{F}(s_{o},)}[ p _{F_{}}(|s_{o})/p_{B}()]\).

Remarkably, we cannot use a simple Monte Carlo estimator to approximate the forward KL due to the presumed intractability of \(P_{B}\) (which depends directly on \(R\)). As a first approximation, we could estimate \(_{KL}[P_{B}||P_{F}]\) via importance sampling w/ \(P_{F}\) as a proposal distribution as in :

\[_{KL}[P_{B}||P_{F}]=_{ P_{F}}[( )}{p_{F_{}}(|s_{o})}()}{p_{F_{}}(|s_ {o})}],\] (4)

and subsequently implement a REINFORCE estimator to compute \(_{}_{KL}[P_{B}||P_{F}]\). Nevertheless, as we only need the divergence's derivatives to perform SGD, we apply the importance weights directly to the gradient estimator. We summarize this approach in the lemma below.

**Lemma 2** (Gradients for the KL divergence).: _Let \(\) be the parameters of \(P_{F}\) and \(s(;)= p_{F_{}}(|s_{o};)\). Then, the gradient of \(_{KL}[P_{F}||P_{B}]\) relatively to \(\) satisfies_

\[_{}_{KL}[P_{F}||P_{B}]=_{ P _{F}(s_{o},)}[_{}s(;)+}( |s_{o})}{p_{B}(|x)r(x)}_{}s(;)]\]_Correspondingly, the gradient of \(_{KL}[P_{B}||P_{F}]\) wrt \(\) is_

\[_{}_{KL}[P_{B}||P_{F}]^{C}-_{  P_{F}(s_{o},)}[}(|s_{o})}{p_{B}(|x)r (x)}_{}s(;)].\]

Crucially, choosing an appropriate learning objective is an empirical question that one should consider on a problem-by-problem basis -- similar to the problem of selecting among Markov chain simulation techniques . In particular, a one-size-fits-all solution does not exist; see Section5 for a thorough experimental investigation. Independently of the chosen method, however, the Monte Carlo estimators for the quantities outlined in Lemma2 are of potentially high variance and may require a relatively large number of trajectories to yield a reliable estimate of the gradients . The following sections demonstrate that variance reduction techniques alleviate this issue.

## 4 Control variates for low-variance gradient estimation

**Control variates.** We first review the concept of a control variate. Let \(f_{}\) be a real-valued measurable function and assume that our goal is to estimate \(_{}[f()]\) according to a probability measure \(\) on \(_{P}\) (see Section2 to recall the definitions). The variance of a naive Monte Carlo estimator for this quantity is \(_{}(f())}}{{n}}\). On the other hand, consider a random variable (RV) \(g_{}\) positively correlated with \(f\) and with known expectation \(_{}[g()]\). Then, the variance of a naive Monte Carlo for \(_{}[f()-a(g()-_{}[g()])]\) for a _baseline_\(a\) is

\[[_{}(f())\!-\!2a_{}(f(),g( ))+a^{2}_{}(g())],\] (5)

which is potentially smaller than \(_{}(f())\) if the covariance between \(f\) and \(g\) is sufficiently large. Under these conditions, we choose the value of \(a\) that minimizes Equation5, namely, \(a=_{}(f(),g())}}{{_{}(g())}}\). We then call the function \(g\) a _control variate_. Also, although the quantities defining the best baseline to \(a\) are generally unavailable in closed form, one commonly uses a batch-based estimate of \(_{}(f(),g())\) and \(_{}(g())\); the incurred bias is generally negligible relatively to the reduced variance . For vector-valued RVs, we let \(a\) be a diagonal matrix and exhibit, in the next proposition, the optimal baseline minimizing the covariance matrix's trace.

**Proposition 2** (Control variate for gradients).: _Let \(f,g_{}^{d}\) be vector-valued functions and \(\) be a probability measure on \(_{}\). Consider a baseline \(a\) and assume \(_{}[g()]=0\). Then,_

\[*{arg\,min}_{a}\,_{}[f() -a g()]\!=\!_{}[g()^{T}(f()-_{ }[f(^{})])]}{_{}[g()^{T}g()]}.\]

Note that, when implementing the REINFORCE gradient estimator, the expectation we wish to estimate may be generally written as \(_{P_{F}(s_{o},)}[_{}f()+f()_{ } p_{F_{}}()]\). For the second term, we use a leave-one-out estimator ; see below. For the first term, we use \(_{} p_{F_{}}\) as a control variate, which satisfies \(_{P_{F}(s_{o},)}[_{} p_{F_{}}(|s _{o};)]=0\). Importantly, estimating the optimal baseline \(a^{}\) in Proposition2 cannot be done efficiently due to the non-linear dependence of the corresponding Monte Carlo estimator on the sample-level gradients ; i.e., it cannot be represented as a vector-Jacobian product, which is efficient to compute in reverse-mode automatic differentiation (_autodiff_) frameworks . Consequently, we consider a linear approximation of both numerator and denominator defining \(a^{}\) in Proposition2, which may be interpreted as an instantiation of the

Figure 2: **Variance of the estimated gradients as a function of the trajectories’ batch size. Our control variates greatly reduce the estimator’s variance, even for relatively small batch sizes.**

delta method (83, Sec. 7.1.3). Then, given a batch \(\{_{1},,_{N}\}\) of trajectories, we instead use

\[=^{N}_{} p_{F_{}}(_ {n}),_{n=1}^{N}_{}f(_{n})}{+\| _{n=1}^{N}_{} p_{F_{}}(_{n})\|^{2}}\] (6)

as the REINFORCE batch-based estimated baseline; \(,\) represents the inner product between vectors. Intuitively, the numerator is roughly a linear approximation to the covariance between \(_{} p_{F_{}}\) and \(_{}f\) under \(P_{F}\). In contrast, the denominator approximately measures the variance of \(_{} p_{F_{}}\), and \(>0\) is included to enhance numerical stability. As a consequence, for the reverse KL divergence, \(_{}f()=_{} p_{F_{}}()\), \( 1\) and the term corresponding to the expectation of \(_{}f()\) vanishes. We empirically find that this approach frequently reduces the variance of the estimated gradients by a large margin (see Figure 2 above and Section 5 below).

Leave-one-out estimator.We now focus on obtaining a low-variance estimate of \(_{ P_{F}(s_{o},)}[f()_{} p_{F_{ }}()]\). As an alternative to the estimator of 2, Shi et al. (85) and Salimans and Knowles (82) proposed a sample-dependent baseline of the form \(a(_{i})=_{1 n N,n i}f(_{n})\) for \(i\{1,,N\}\). The resulting estimator,

\[=_{n=1}^{N}(f(_{n})-_{j=1,j i }^{N}f(_{j}))_{} p_{F_{}}(_{n}),\]

is unbiased for \([f()_{} p_{F_{}}()]\) due to the independence between \(_{i}\) and \(_{j}\) for \(i j\). Strikingly, \(\) can be swiftly computed with _autodiff_: if \(=(f(_{n}))_{n=1}^{N}\) and \(=( p_{F_{}}(_{n}))_{n=1}^{N}\), then

\[=_{}(-(-)),,\] (7)

with sg as the stop-gradient operation (e.g., \(\) in JAX (8) and torch.detach in PyTorch (2018)). Importantly, these techniques incur a minimal computational overhead to the stochastic optimization algorithms relative to the considerable reduction in variance they enact.

Relationship with previous works.Importantly, Malkin et al. (2018) used \(=_{n}f(_{n})\) as baseline and an importance-weighted aggregation to adjust for the off-policy sampling of trajectories, introducing bias in the gradient estimates and relinquishing guarantees of the optimization procedure. A learnable baseline independently trained to match \(\) was also considered. This potentially entailed the inaccurate conclusion that the TB and DB are superior to standard divergence-based objectives. Indeed, the following section underlines that such divergence measures are sound and practical learning objectives for GFlowNets for a range of tasks.

Illustration of the control variates' effectiveness.We train the GFlowNets using increasingly larger batches of \(\{2^{i} i[]\}\) trajectories with and without CVs. In this setting, Figure 2 showcases the drastic reduction in the variance, represented by the covariance matrix's trace, of the estimated learning objectives' gradients w.r.t. the model's parameters promoted by the CVs. Impressively, as we show in Figure 6, this approach significantly increases the efficiency of the underlying stochastic optimization algorithm. See Section 5 and Appendix D for further details.

## 5 Training GFlowNets with divergence measures

Our experiments serve two purposes. Firstly, we show in Section 5.2 that minimizing divergence-based learning objectives leads to competitive and often better approximations than the alternatives based on log-squared violations of the flow network's balance. This underlines the effectiveness of well-established divergence measures for training GFlowNets (Shi et al., 2019; Wang et al., 2020). Secondly, we highlight in Section 5.3 that the reduction of variance enacted by our control variates critically accelerates the convergence of GFlowNets. We consider widely adopted benchmark tasks from GFlowNet literature, described in Section 5.1, contemplating both discrete and continuous target distributions. Please refer to Appendix B and Appendix E for additional information on the experimental setup.

### Generative tasks

Below, we provide a high-level characterization of the generative tasks used for synthetic data generation and training. For a more rigorous description in the light of Section 2, see Appendix B.

**Set generation**. A state \(s\) corresponds to a set of size up to a given \(S\) and the terminal states \(\) are sets of size \(S\); a transition corresponds to adding an element from a deposit \(\) to \(s\). The IGP starts at an empty set, and the log-reward of a \(x\) is \(_{d x}f(d)\) for a fixed \(f\).

**Autoregressive sequence generation**. Similarly, a state is a seq. \(s\) of max size \(S\) and a terminal state is a seq. ended by an end-of-sequence token; a transition appends \(d\) to \(s\). The IGP starts with an empty sequence and, for \(x\), \( r(x)=_{i=1|x|}g(i)f(x_{i})\) for functions \(f,g\).

**Bayesian phylogenetic inference (BPI)**. A state \(s\) is a forest composed of binary trees with labeled leaves and unlabelled internal nodes, and a transition amounts to joining the roots of two trees to a newly added node. Then, \(s\) is terminal when it is a single connected tree -- called a _phylogenetic tree_. Finally, given a dataset of nucleotide sequences, the reward function is the unnormalized posterior over trees induced by J&C69's mutation model  and a uniform prior.

**Hypergrid navigation**. A state \(s\{0,,H-1\}^{d}\) is a component of a \(H\)-sized and \(d\)-dimensional Euclidean grid. The IGP starts at \(\) and, if we let \(_{i}\) be the \(i\)-th line of the identity matrix and \((s)=\{_{i} i\{1,,d\}_{j}(s+_{i})_{j}<H\}\), a transition either adds a \((s)\) to \(s\) or stops at \(s\). We use Malkin et al. [55, Section 5.1]'s reward function with \(R_{o}=10^{-3}\).

**Bayesian structure learning**. A state \(s\) is a DAG representing a Bayesian network; a transition either adds an edge to \(s\) or stops the IGP. Similarly to Deleu et al. , we ensure the added edges preserve the state's acyclicity. The reward function is defined as the maximum likelihood of the linear Gaussian structural model induced by the current state based on a fixed i.i.d. data set.

**Mixture of Gaussians (GMs)**. The IGP starts at \(^{d}\) and proceeds by sequentially substituting each coordinate with a sample from a real-valued distribution. For a \(K\)-component GM, the reward of \(^{d}\) is defined as \(_{k=}_{k}(|_{k},_{k})\) with \(_{k} 0\) and \(_{k}_{k}=1\).

**Banana-shaped distribution**. We use the same IGP implemented for a bi-dimensional GM. For \(^{2}\), we set \(r()\) to a normal likelihood defined on a quadratic function of \(\), see Equation 8 in the supplement. We use HMC samples as ground truth to gauge performance on this task.

### Assessing convergence speed

Next, we provide evidence that minimizing divergence-based objectives frequently leads to faster convergence than minimizing the standard TB loss .

**Experimental setup.** We compare the convergence speed in terms of the rate of decrease of a measure of distributional error when using different learning objectives for a GFlowNet trained to sample from each of the distributions described in Section 5.1. For discrete distributions, we adopt the evaluation protocols of previous works  and compute the \(L_{1}\) distance between the learned \(p_{T}(x;)\) and target \(r(x)\), namely, \(_{x}|p_{T}(x;)-}{{Z}}|\). To approximate \(p_{T}\), we use a Monte Carlo estimate of \(p_{T}(x;)=_{r P_{B}(x,)}(|x;)}}{{p_{B}(|x)}}\). For continuous distributions, we echo  and compute Jensen-Shannon's divergence between \(P_{T}(x;)\) and \(R(x)\):

\[_{JS}[P_{T}||R]=}{{2}}(_{KL}[P_{T}||M] +_{KL}[R||M])=}_{x P_{T}}[ (x)}}{{m(x)}}]+}_{x R}[ }{{Zm(x)}}],\]

Figure 3: **Divergence-based learning objectives often lead to faster training than TB loss. Notably, contrasting with the experiments of , there is no single best loss function always conducting to the fastest convergence rate, and minimizing well-known divergence measures is often on par with or better than minimizing the TB loss in terms of convergence speed. Results were averaged across three different seeds. Also, we fix \(=0.5\) for both Tsallis-\(\) and Renyi-\(\) divergences.**with \(M(B)=}{{2}}(P_{T}(B)+}{{R(X)}})\) being the averaged measure of \(P_{T}\) and \(R\) and \(m\) its corresponding density relatively to the reference measure \(\). Remarkably, for the GMs distribution, we can directly sample from the target to estimate \(_{KL}[R|M]\), and the autoregressive nature of the generative process ensures that \(p_{T}(x)=p_{F_{}}(|s_{o})\) for the unique trajectory \(\) starting at \(s_{o}\) and finishing at \(x\). Hence, we get an unbiased estimate of \(_{KL}[P_{T}||M]\). Finally, let \(_{t}\) be the first \(t\) terminal states encountered during training and \(\{x_{(1)},,x_{(k)}\}\) be an descending ordering of \(_{t}\) according to \(r\). Then, we select a threshold \(\) and an integer \(K\) to report \((_{t})=|\{r(x) r(x) x _{t}\}|\), called _number of modes_, and \((_{t})=(\{r(x_{(i)}) 1 i K \})\), referred to as _top-K average reward_. Both \(\) and \(\) are metrics of substantial interest in the GFlowNet literature .

**Results.** Figure 3 shows that the procedure minimizing divergence-based measures accelerates the training convergence of GFlowNets, whereas Figure 5 (for the banana-shaped distribution) and Table 1 highlight that we obtain a more accurate model with a fix compute budget. The difference between learning objectives is not statistically significant for the BPI task. Also, we may attribute the superior performance of reverse KL compared to the forward in the sequence generation task to the high variance of the importance-sampling-based gradient estimates. Indeed, the observed differences disappear when we increase the batch of trajectories to reduce the estimator's variance (see Figure 8 in Appendix D). In conclusion, our empirical results based on experiments testing diverse generative settings and expanding prior art , shows that training methods based on minimizing \(f\)-divergence VI objectives with adequate CVs implemented are practical and effective in many tasks. Correlatively, Figure 4 supports

  & BPI & Sequences & Sets & GMs \\  TB & 0.22\(\)0.01 & 0.28\(\)0.06 & 0.07\(\)0.00 & 0.31\(\)0.08 \\ Rev. KL & 0.21\(\)0.01 & **0.16\(\)**0.00 & **0.03\(\)**0.00 & 0.31\(\)0.09 \\ For. KL & 0.22\(\)0.01 & 0.23\(\)0.12 & **0.03\(\)**0.00 & **0.09\(\)**0.10 \\ Renyi-\(\) & 0.22\(\)0.03 & 0.23\(\)0.10 & **0.03\(\)**0.00 & 0.19\(\)0.13 \\ Tsallis-\(\) & 0.21\(\)0.01 & 0.22\(\)0.09 & **0.03\(\)**0.00 & 0.21\(\)0.11 \\ 

Table 1: Divergence minimization achieves better than or similar accuracy compared to enforcing TB.

Figure 4: **Average reward for the \(K\) highest scoring samples (top-K) and Number of Modes** found during training for the tasks of sequence design, set generation, hypergrid and DAG environments. With the only exception of the hypergrid task, the minimization of divergence-based measures leads to similar and often faster discovery of high-valued states relatively to their balance-based counterparts.

Figure 5: **Learned distributions for the banana-shaped target.** Tsallis-\(\), Renyi-\(\) and for. KL leads to a better model than TB and Rev. KL, which behave similarly — as predicted by Proposition 1.

the fact that minimizing divergence-based objectives frequently implies better coverage of the target's high-probability regions; the only exception is the (extremely sparse) hypergrid task .

### Reducing the variance of the estimated gradients

Figure 2 demonstrates that implementing CVs for the REINFORCE estimator reduces the noise level of gradient estimates significantly. This reduction in variance also accelerates training convergence. To illustrate this, we use the same experimental setup from Section 5.2 and analyze the learning curves for each divergence measure with and without control variates.

**Results.** Figure 6 shows that the implemented gradient reduction techniques significantly enhance the learning stability of GFlowNets and drastically accelerate training convergence when minimizing the reverse KL divergence. Our results indicate that well-designed CVs for gradient estimation can greatly benefit GFlowNets training. Notably, similar improvements have been observed in the context of Langevin dynamics simulations  and policy gradient methods for RL .

## 6 Conclusions, limitations and broader impact

**Discussion.** We showed in a comprehensive range of experiments that divergence measures common in VI -- forward KL, reverse KL, Renyi-\(\), and Tsallis-\(\) -- are effective learning objectives for training GFlowNets, performing competitively with or better than their balance-based counterparts. To achieve this, the introduction of efficacious control variates for low-variance gradient estimation of the divergence-based objectives was crucial, which is a key distinction between our work an prior art . Additionally, we developed the theoretical connection between GFlowNets and VI beyond the setting of finitely supported measures, establishing results for arbitrary topological spaces.

**Limitations.** Albeit comprehensive and on par with the wider literature, our empirical evaluation was performed on problems of relatively small size due to the intractability of probing a GFlowNet's distributional accuracy on very large state spaces. That said, we acknowledge that an assessment on the domains of natural language processing  and drug discovery  based on context-specific metrics would strengthen our conclusions; we leave these tasks to future endeavors. Similarly, while we observed promising results for \(=0.5\), there might be different choices of \(\) that, depending on the application, might strike a better explore-exploit tradeoff and incur faster convergence. Thus, thoroughly exploring different \(\) might be especially useful to practitioners.

**Broader impact.** Overall, our work highlights the potential of the once-dismissed VI-inspired schemes for training GFNs, paving the way for further research towards improving the GFlowNets by drawing inspiration from the VI literature. For instance, one could develop \(\)-divergence-based losses for GFNs , combine GFNs with MCMC using Ruiz and Titsias 's divergence, or employ an objective similar to that of importance-weighted autoencoders . Finally, although an \(\)-greedy off-policy sampling scheme can be easily incorporated into a divergence-minimizing algorithm through an importance-sampling correction, it remains elusive whether this would be possible for more sophisticated sampling techniques such as replay buffer  and local search .