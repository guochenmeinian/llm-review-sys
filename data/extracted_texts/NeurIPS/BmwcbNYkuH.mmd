# Are nuclear masks all you need for improved out-of-domain generalisation? A closer look at cancer classification in histopathology

Are nuclear masks all you need for improved out-of-domain generalisation? A closer look at cancer classification in histopathology

Dhananjay Tomar

University of Oslo

dhananjt@ifi.uio.no &Alexander Binder

Otto-von-Guericke University Magdeburg,

Singapore Institute of Technology

alexander.binder@ovgu.de &Andreas Kleppe

Oslo University Hospital, University of Oslo,

UiT The Arctic University of Norway

andrekle@ifi.uio.no

Corresponding author.

###### Abstract

Domain generalisation in computational histopathology is challenging because the images are substantially affected by differences among hospitals due to factors like fixation and staining of tissue and imaging equipment. We hypothesise that focusing on nuclei can improve the out-of-domain (OOD) generalisation in cancer detection. We propose a simple approach to improve OOD generalisation for cancer detection by focusing on nuclear morphology and organisation, as these are domain-invariant features critical in cancer detection. Our approach integrates original images with nuclear segmentation masks during training, encouraging the model to prioritise nuclei and their spatial arrangement. Going beyond mere data augmentation, we introduce a regularisation technique that aligns the representations of masks and original images. We show, using multiple datasets, that our method improves OOD generalisation and also leads to increased robustness to image corruptions and adversarial attacks. The source code is available at https://github.com/undercutspiky/SFL/

## 1 Introduction

Domain generalisation in histopathology is a crucial challenge because domain shifts naturally occur among hospitals and even within a single hospital or laboratory, e.g., temporally or among human operators and observers such as pathologists. Non-biological factors that substantially alter the images include differences in scanners, staining protocols, fixation of tissue, and even minor aspects like the manufacturer and storage conditions of stains .

Collecting data from numerous hospitals to address these domain shifts is often impractical and may not adequately reflect the full variability present in routine clinical practice, thus making it difficult to build computational histopathology models that generalise well. This leads us to focus on single-domain generalisation (S-DG) in this paper, specifically on how to train a model using data from only one hospital (considered a domain here) that generalises well to data from other hospitals. Popular S-GD methods in histopathology apply data augmentation and stain normalisation . The effectiveness of S-GD methods developed for natural images remains underexplored in histopathology . Here, we compare these methods to a new, simple approach that we propose.

Research has shown that Convolutional Neural Networks (CNNs) tend to focus on texture over shape [4; 5]. However, in histopathology, the texture and colour of cell nuclei vary much more across domains than the shape and organisation of cell nuclei. As a result, focusing on shape features could improve a computational histopathology model's ability to generalise to unseen data because it may rely less on domain-specific features that vary across hospitals and more.

Nuclei in cancerous tissue exhibit distinct changes in shape, size, and overall organisation compared to nuclei in normal tissue [6; 7; 8]. Pathologists rely on these and other visual cues  for cancer diagnosis and grading, underscoring the biological importance and the consistency of nuclear morphology and organisation across domains. We hypothesise that focusing on nuclear morphology and organisation may be sufficient for cancer detection and that exploiting this during training could result in models with good generalisation.

We propose a method that encourages CNNs to focus more on nuclear morphology and organisation by using additional loss terms that prioriti shape-based features. Specifically, our method leverages nuclear segmentation masks during training to steer the learning towards nuclei. Through extensive experimentation, we demonstrate that this method improves performance on out-of-domain data without requiring nuclear segmentation masks at inference time, thus offering a promising and attractive solution for addressing domain generalisation in histopathology. Our contributions include:

* We propose a novel training method that incentivises the model to focus on nuclei.
* We evaluate our method on three datasets comprising hundreds of WSIs in total from various hospitals and organs. Our results show accuracy improvements over all other approaches.
* We evaluate the sensitivity of our method to image corruptions and adversarial attacks. Our results show performance improvements over the baseline.
* We conduct extensive ablation studies to show that models trained with our method focus on nuclei.

## 2 Related work

The prediction of various properties such as malignancy, grading, and HER2 expression using segmented nuclei has been a well-studied topic for many years [10; 11; 12; 13; 14; 15; 16]. Researchers have employed techniques such as watershed segmentation , thresholding, level sets , and snakes , often followed by extracting explicit morphometric features from the segmentations. For example, early work by Hasegawa et al.  focused on counting segmented regions, while Lee and Street  applied neural networks to the segmentation outputs. In contrast to these approaches, our method does not rely on segmentation during inference. Instead, we adjust the training process to encourage the extraction of nuclear features.

**Stain normalisation** methods convert the colours of a source image to match those of a target image. These methods were typically designed specifically for the most common type of histopathology images, which are images of tissue stained with haematoxylin and eosin (H&E). One of the earlier methods, Macenko normalisation , estimates stain vectors for source and target images and uses them to normalise the source image. Vahadane et al.  proposed a method that decouples stain "density maps" from "colour appearances", allowing the combination of the source image's density maps with the target image's colour appearances. Reinhard et al.  pioneered colour transfer by adjusting the global statistics of images in a different colour space, effectively transferring the colour characteristics of the target to the source image. Random Stain Normalization and Augmentation (RandStainNA)  combines stain normalisation and augmentation. Unlike traditional approaches that normalise using a fixed template, RandStainNA generates random virtual templates in the LAB  colour space and uses them to normalise the images during training. The templates are drawn from Gaussian distributions whose means and variances are derived from the training data. For a more comprehensive review, we refer the readers to . In summary, the stain normalisation methods primarily focus on manipulating colour information to remove stain variability. On the other hand, our approach shifts the focus from colour manipulation to nuclear features.

**Data augmentation** is a common way to facilitate domain generalisation. Tellez et al.  evaluated several stain colour augmentation and stain normalisation methods and found that colour augmentation was crucial for good performance on external test sets in histopathology. Faryna et al.  extended RandAugment  by including certain histopathology-specific augmentations and excluding the ones that produce unrealistic-looking images. Tellez et al.  developed a data augmentation method specific to H&E-stained images and used it for domain generalisation in mitosis detection. Pohjonen et al.  developed StrongAugment, where varying numbers of transformations are applied to an image to improve domain generalisation. Marini et al.  proposed Data-driven colour augmentation (DDCA), which evaluates an augmented image as acceptable or not for training based on its distance from other images in a database. Faryna et al.  evaluated different data augmentation strategies in histopathology, including manually selected augmentations, and found them all to be competitive.

**Single-domain generalisation (S-DG)** methods do not require data from multiple domains during training. Representation Self-Challenging (RSC)  works by discarding the features with relatively high gradients, making the model predict with the remaining features during training. Adversarial Domain Augmentation (ADA)  generates adversarial examples iteratively to augment the source domain and creates an ensemble of models. Meta-Learning-based ADA (M-ADA)  uses Wasserstein Auto-Encoder  to generate new samples and uses adversarial training on top along with a meta-learning scheme. Progressive domain expansion network (PDEN)  uses multiple autoencoders to generate new samples to expand the training set. Learning to Diversify (L2D)  introduces a learnable style-complement module that generates augmented images. The style-complement module is trained to diversify the images as much as possible but still keep the semantic information intact.

**Domain adaptation**, unlike S-DG, requires having access to some samples from the target domain. In histopathology, domain adaptation methods commonly make use of GAN  and CycleGAN . StainGAN  uses CycleGAN to make images in the source domain look like the target domain. Residual CycleGAN  modifies the CycleGAN objective to have the generator produce the residual between domains instead of recreating the input image. In , authors augment a generator in CycleGAN with a stain colour matrix as an auxiliary input to stabilise the training. NST_AD_HRNet  uses Neural Style Transfer [44; 45] and GAN to preserve the content of the source image while combining it with the style of the target image. In some earlier works [46; 47], the input image is converted to greyscale and then coloured using a generator network which is based on a target image. While domain adaptation is not S-DG and thus a bit tangential to the focus of this paper, it is worth noting that domain adaptation is impractical in many clinical settings and may result in worse generalisation than stain normalisation and colour augmentation .

## 3 Proposed Method

Our approach aims to enhance S-DG by incentivising the model to focus on shaped-based features of nuclei in histopathological images and thereby reduce overfitting to irrelevant features that may carry higher label noise.

The first step involves generating segmentation masks that highlight specific areas of interest in the image. This step is applied only during training, while test-time evaluation relies solely on H&E-stained images. As we hypothesised that nuclear morphology and organisation contain sufficient information for cancer detection, our segmentation masks are binary images with nuclear pixels as foreground and other pixels as background.

One possible approach to using the segmentation mask is to include it as a fourth channel in the input image. Alternatively, the mask can be used as the sole input to the model. However, both methods necessitate running the segmentation model during inference, which increases computational demands and slows down processing.

Our method circumvents the need for a nuclear segmentation network at inference time by incorporating additional loss terms during training. For a given input image \(x\) and its corresponding segmentation mask \(x^{}\), our method involves the following steps:

1. Execute a forward propagation through the neural network model on both the H&E-stained image \(x\) and its nuclear mask \(x^{}\), saving the embeddings generated by the network as \(z\) and \(z^{}\) for \(x\) and \(x^{}\), respectively.
2. Compute the Binary Cross-Entropy (BCE) loss for both \(x\) and \(x^{}\).
3. Compute the \(_{2}\)-distance between the embeddings \(z\) and \(z^{}\).
4. Minimise the sum of the two CE losses and the \(_{2}\)-distance.

Our approach is illustrated in Figure 1. We employ a ResNet-50  from Torchvision  as the base model. We next discuss some details of our approach.

\(_{2}\)**-regularisation:** To encourage the network to focus on nuclei, we minimise the distance between the feature map of the original image and that of its nuclear segmentation mask. We use the flattened feature map from ResNet-50's penultimate layer, just before Global Average Pooling, to obtain the embeddings. The regularisation term consists of the \(_{2}\)-distance between the embeddings of the original image \(z\) and its mask \(z^{}\), which is added to the BCE losses for both the image and the mask. Let \(\) be the model's prediction for \(x\), \(^{}\) for \(x^{}\), and \(y\) be the ground truth. Then, the total loss \(L\) is:

\[L=\|z-z^{}\|_{2}^{2}+BCE(y,)+BCE(y,^{})\] (1)

where \(BCE\) is the Binary Cross-Entropy loss function for labels in \(\{0,1\}\):

\[BCE(y,p)=-(y(p)+(1-y)(1-p))\] (2)

**Original image times mask:** Since the embeddings of the original image and its binary segmentation mask may differ significantly, minimizing their \(_{2}\)-distance can be challenging for the model. To address this, with a probability of 0.5, we multiply the original image by its segmentation mask, i.e., the network receives \(x*x^{}\) as input half the time instead of \(x\). By multiplying with the segmentation mask, everything in the original image except the nuclei is set to 0. Figure 1 shows what the output looks like. By simplifying the task, the network can more easily reduce the distance between the embeddings of the nuclei-only image and the mask and gradually improve alignment between the embeddings of the original image and the mask. We found this augmentation to help stabilise training.

## 4 Experiments

### Datasets

**CAMELYON17** dataset consists of 1000 H&E-stained Whole Slide Images (WSIs) of breast cancer metastases in lymph node sections from five medical centres in the Netherlands. It contains pixel-level annotations of tumours for 10 WSIs from each medical centre, giving us 50 WSIs to work with. WSIs from centres 0, 3 and 4 were scanned using the same scanner, while the other two centres used a different scanner each. All slides were scanned at \(40\) resolution. We treat each centre as a different domain.

**BCSS** dataset consists of 151 H&E-stained WSIs of histologically-confirmed primary breast cancer cases from The Cancer Genome Atlas (TCGA) with triple-negative status determined from

Figure 1: We pass the input image (or, with 0.5 probability, input image multiplied with its nuclear segmentation mask) and its nuclear segmentation mask through the network and minimise the Binary Cross-Entropy (BCE) loss for both the input image and its mask. Additionally, we minimise the \(_{2}\)-distance between the input image’s embedding vector and the mask’s embedding vector just before the Global Average Pooling (GAP) layer. The embedding vector is ResNet-50’s penultimate layer’s feature map, i.e., stage 4’s last feature map.

clinical data files. All WSIs have a resolution of \(40\). The WSIs were annotated at the pixel level using crowdsourcing. Each pixel can have one of the many labels. We consider the label "tumor" to define pixels with a tumour and all other labels except "outside_roi", "exclude", or "undetermined" to define pixels without a tumour.

**Oeclot** dataset consists of pixel-level annotations of tumour vs non-tumour pixels for 303 WSIs from TCGA. It consists of WSIs of primary tumour from six different organs: Bladder, Endometrium, Head-and-neck, Kidney, Prostate, and Stomach. The annotations in the dataset are at a low resolution, so we upscale the annotations to \(40\). We exclude two WSIs that have only \(20\) resolution; all other WSIs have \(40\) resolution.

### Dataset preparation

We use code from WILDS [54; 55] to prepare the CAMELYON17 dataset with modifications. Tiles are sized \((270 270)\) at \(40\) resolution. For each domain (medical centre), data is split by patient, ensuring all tiles from a patient are in a single subset. Since the number of tiles varies drastically across patients, we shuffle patients so that the validation subset contains 20%-25% of all tiles per domain. Our processed version of CAMELYON17 is available at .

### Experiment setup

We train models using the CAMELYON17 dataset, treating each medical centre as a distinct domain. We use the BCSS and Ocelot datasets as external test datasets. To avoid multiple comparisons and overly optimistic performance estimates, we use the external test datasets only once during the entire project, solely to evaluate the final models .

For each combination of medical centre and method, we train ten models using the train subset of that centre. Thus, we train 50 models in total for each method. We use the loss on the validation dataset (of the training domain) to select the best model for each training. All models are trained for 50 epochs using the Adam optimiser with a learning rate \(4\) and a weight decay of \(1\). We use exponential learning rate decay with a decay rate of \(0.955\). For our method, we set the parameter \(\) in equation (1) to \(=0\) for the first five epochs, effectively training without \(_{2}\)-distance loss in these epochs, and then use \(=1\) for the rest of the training. We start saving models for selection of the one with lowest validation loss after ten epochs for our method to allow the network to stabilise while from the first epoch for other methods. For all experiments, unless stated otherwise, we use a ResNet-50  model pre-trained on ImageNet . We use HoVer-Net  trained on the CoNSeP  dataset to generate nuclear segmentation masks.

While domain generalisation encompasses a wide variety of methods, we have selected several exemplary baselines for comparison: Macenko normalisation , RSC , L2D , RandStainNA (RandSNA in result tables) , and DDCA . We also include a baseline where we initialise ResNet-50 with pre-trained weights from HoVer-Net . These methods represent different approaches, including stain normalisation (Macenko, RandStainNA) and generating augmented images (L2D). By selecting these diverse techniques, we ensure a comprehensive evaluation of our method's performance across various S-DG strategies.

It is important to note that our method can be integrated with many existing S-DG approaches, making it a flexible plug-in solution rather than a direct competitor. We evaluate most methods with and without the photometric augmentations selected for ERM. After testing various augmentation strategies available in Torchvision , we identified the most effective combination to be: ColorJitter(brightness=[0.5, 1.5], contrast=[0.5, 1.5], saturation=[0.5, 1.5], hue=[-0.3, 0.3]) and GaussianBlur(kernel_size=3). Results using these augmentations are marked as '-Aug' in the results tables. In all experiments, including those without photometric augmentations, we apply the basic geometric augmentations: random horizontal and vertical flips. For all ViT-Tiny  experiments, we also add affine augmentations: random rotation (up to 90\({}^{}\)) and translation (up to 45 pixels).

We ran the experiments on two clusters with GPUs with 64 GB (AMD MI250X) and 24 GB (Nvidia RTX 3090) GPU RAM each. Each job consumed about 21 to 31 GB of GPU RAM. The proposed method took 5 to 20 hours to train, depending on the train data size while ERM took 2.5 to 11 hours.

We report tile-level accuracy for tumour vs non-tumour tile classification for all datasets. Additionally, we measure robustness to image noise by measuring the accuracy drop on CAMELYON17 for image corruptions introduced in . This includes Gaussian-, shot-, impulse- and snow-noise, and two blur types, elastic transform and JPEG compression.

Results on CAMELYON17 (lymph node sections)We test models on their respective out-of-domain data. E.g., a model trained on Centre-3 is tested on all the data from Centre-0,1,2,4. Our method attains 10% higher accuracy than the next best method (L2D) when none used photometric augmentations and was also superior when photometric augmentations were used (Table 1).

Results on BCSS (primary breast cancer)The accuracy of the models trained on a centre in CAMELYON17 drops substantially (12% to 14%) for all the methods when tested on BCSS (Table 2) compared to when tested on other centres in CAMELYON17 (Table 1). This could be due to a mismatch between pathologists' annotations on CAMELYON17 and BCSS but also due to the biological differences between these tissue types. In particular, epithelial cells in lymph nodes would almost certainly be tumour cells, while they could be benign cells in ordinary breast tissue. These results show that the relative performance on CAMELYON17 for different methods is indicative of relative performance on an external test set, as the performance drop is similar for all methods.

Results on Ocelot (primary non-breast cancer)We test our model on the Ocelot dataset to evaluate if our method helps to train models that generalise to other organs as well. Ocelot does not have any data from breast tissue nor does it include lymph node sections (which all the models have been trained on). We report the results of these experiments in Table 3. While our method achieves the highest accuracy also in this case, the difference between our method and L2D is not as big as it is for CAMELYON17 and BCSS. Taking a closer look into the performance for separate organs (Tables 4, 5, 6, 7, 8, and 9 in the Supplement), we can see that our method performs worse than L2D in Endometrium and Kidney, where accuracies are generally lower, and better in the four other organs. This indicates that models trained with our method generalise worse to organs where the transferability from breast tissue is generally low. This is at least the case for Kidney which has by far the lowest accuracies across all methods. Generalising to different cancer types is an emerging experimental topic; see, for example, .

In summary, our method yields better accuracy than the baselines, including other S-DG approaches.

   Method & Centre-0 & Centre-1 & Centre-2 & Centre-3 & Centre-4 & Average \\  ERM & \(72.8 2.3\) & \(65.9 3.7\) & \(64.1 3.0\) & \(55.0 1.3\) & \(53.8 3.0\) & \(62.4 2.7\) \\ Macenko & \(79.3 2.1\) & \(62.4 1.4\) & \(73.3 5.0\) & \(65.8 2.3\) & \(85.9 4.2\) & \(73.3 3.0\) \\ HoVerNet & \(72.5 2.4\) & \(71.0 2.5\) & \(61.3 3.9\) & \(55.1 1.9\) & \(49.6 8.4\) & \(61.9 3.8\) \\ RandSNA & \(75.7 3.1\) & \(70.9 4.9\) & \(62.4 2.5\) & \(57.2 2.8\) & \(51.8 2.6\) & \(63.6 3.2\) \\ RSC & \(77.1 3.2\) & \(64.5 3.1\) & \(61.9 3.8\) & \(56.8 2.3\) & \(51.1 2.2\) & \(62.3 2.9\) \\ L2D & _93.6_\(\)_1.0_ & \(72.9 2.5\) & \(64.4 13.0\) & \(73.6 4.3\) & \(84.4 3.7\) & \(77.8 4.9\) \\ Ours & \(90.4 1.5\) & \(\) & \(90.1 1.3\) & \(82.1 2.7\) & \(90.8 1.0\) & _89.2_\(\)_1.3_ \\  ERM-Aug & \(93.1 1.0\) & \(78.9 2.1\) & \(89.3 2.8\) & \(74.8 1.5\) & \(91.3 1.6\) & \(85.5 1.8\) \\ Macenko-Aug & \(86.3 1.9\) & \(78.7 1.5\) & \(86.2 4.4\) & \(70.0 2.8\) & \(90.8 1.2\) & \(82.4 2.3\) \\ HoVerNet-Aug & \(93.0 0.6\) & \(80.8 2.8\) & _91.3_\(\)_1.2_ & \(82.2 1.6\) & \(89.6 2.2\) & \(87.4 1.7\) \\ RandSNA-Aug & \(92.7 1.1\) & \(83.1 2.1\) & \(91.0 2.0\) & \(78.9 3.0\) & \(91.1 1.5\) & \(87.4 1.9\) \\ DDCA-Aug & \(92.5 2.4\) & \(79.4 1.9\) & \(89.4 2.9\) & \(78.2 3.1\) & \(90.2 2.1\) & \(86.0 2.5\) \\ RSC-Aug & \(93.1 0.8\) & \(78.2 2.0\) & \(89.3 3.4\) & \(77.9 2.2\) & \(91.0 1.7\) & \(85.9 2.0\) \\ L2D-Aug & \(\) & \(87.6 0.6\) & \(87.7 1.4\) & _83.4_\(\)_2.6_ & \(\) & \(89.1 1.1\) \\ Ours-Aug & \(91.8 0.7\) & _92.2_\(\)_1.6_ & \(\) & \(\) & _91.7_\(\)_0.5_ & \(\) \\  Ours-no-\(_{2}\)-A & \(92.1 0.8\) & \(81.4 2.9\) & \(91.8 2.0\) & \(83.3 1.0\) & \(90.5 1.7\) & \(87.8 1.7\) \\ Ours-MO-Aug & \(91.8 2.2\) & \(88.2 2.1\) & \(85.0 2.2\) & \(79.7 4.2\) & \(77.9 2.8\) & \(84.5 2.7\) \\   

Table 1: Out-of-domain accuracy on CAMELYON17. The column name indicates the centre used to train models. The best accuracy for each column is in **bold face** and the second best in _italics_. Method ”Ours-no-\(_{2}\)-A” is shorthand for ”Ours-no-\(_{2}\)-Aug” and refers to our approach without \(_{2}\)-regularisation. Method ”Ours-MO-Aug” refers to our approach with masks only, that is, neither using \(_{2}\)-regularisation nor using mask-times-input augmentation of H&E images with 50% probability during training. A paired t-test for ”L2D-Aug” versus “Ours-Aug” yields a p-value of \(2 10^{-5}\).

## 5 Ablation Study and Discussion

Impact of data augmentationTables 1, 2, and 3 shows that data augmentation benefits all methods substantially, which is consistent with well-established knowledge.

Impact of \(_{2}\)-regularisationThe result labelled _Ours-no-\(_{2}\)-A_ in Table 1 shows that using data augmentation with nuclear masks alone is insufficient to achieve high accuracy. Without \(_{2}\)-regularisation (i.e., setting \(=0\) in Equation (1)), our method only slightly outperforms most baselines that also uses data augmentation. The key factor for effective cross-domain generalisation is the ability to align the feature representation of input images with corresponding mask images, which lack colour and texture. Further evidence supporting this alignment effect is presented in the next paragraph.

Impact of mask-times-input-augmentationThe result _Ours-MO-Aug_ in Table 1 demonstrates an ablation with two changes: the absence of \(_{2}\)-regulation and the removal of the 50% probability

   Method & Centre-0 & Centre-1 & Centre-2 & Centre-3 & Centre-4 & Average \\  ERM & \(58.0 2.9\) & \(69.5 2.8\) & \(52.0 1.0\) & \(50.0 0.1\) & \(52.1 3.9\) & \(56.3 2.1\) \\ Macenko & \(68.7 2.4\) & \(56.5 1.9\) & \(65.5 2.6\) & \(56.8 2.3\) & \(71.1 3.4\) & \(63.7 2.5\) \\ HoVerNet & \(55.7 2.0\) & \(65.3 2.2\) & \(53.8 0.8\) & \(49.8 1.9\) & \(47.5 2.9\) & \(54.4 2.0\) \\ RandSNA & \(60.5 2.8\) & \(67.4 3.8\) & \(51.0 0.8\) & \(50.1 0.7\) & \(50.2 1.6\) & \(55.8 1.9\) \\ RSC & \(63.3 3.7\) & \(65.7 2.4\) & \(50.8 1.0\) & \(50.1 0.1\) & \(50.2 0.5\) & \(56.0 1.5\) \\ L2D & \(79.9 1.2\) & \(65.2 0.7\) & \(67.4 2.3\) & \(63.2 4.5\) & \(66.2 2.2\) & \(68.4 2.2\) \\ Ours & \(74.2 3.6\) & _78.3 \(\) 2.2_ & \(73.4 1.9\) & \(63.8 2.8\) & \(71.8 2.3\) & \(72.3 2.6\) \\  ERM-Aug & \(80.1 1.6\) & \(70.7 2.8\) & \(73.7 2.6\) & \(60.9 1.8\) & \(73.3 2.6\) & \(71.7 2.3\) \\ Macenko-Aug & \(75.8 2.9\) & \(67.6 2.4\) & \(72.6 2.6\) & \(57.8 2.9\) & \(75.3 1.8\) & \(69.8 2.5\) \\ HoVerNet-Aug & \(79.8 1.4\) & \(65.1 1.6\) & \(71.2 2.2\) & \(64.0 2.2\) & \(69.2 6.1\) & \(69.9 2.7\) \\ RandSNA-Aug & \(78.5 2.7\) & \(73.2 3.1\) & \(72.8 3.2\) & \(64.5 3.4\) & \(75.1 2.5\) & \(72.8 3.0\) \\ DDCA-Aug & \(79.1 1.9\) & \(71.7 3.5\) & \(70.1 2.8\) & \(61.4 3.2\) & \(71.4 7.3\) & \(70.7 3.8\) \\ RSC-Aug & \(79.6 1.5\) & \(72.1 2.9\) & \(71.6 1.7\) & \(63.2 1.6\) & \(74.1 4.3\) & \(72.1 2.4\) \\ L2D-Aug & _81.9 \(\) 0.3_ & \(74.7 0.6\) & _74.2 \(\) 0.6_ & _67.5 \(\) 2.9_ & \(\) & \(75.1 1.3\) \\ Ours-Aug & \(\) & \(\) & \(\) & \(\) & \(74.8 1.9\) & \(\) \\   

Table 2: Out-of-domain accuracy on BCSS. The column name indicates the centre used to train models. The best accuracy for each column is in **bold face** and the second best in _italics_. A paired t-test for “L2D-Aug” versus “Ours-Aug” yields a p-value of \(4 10^{-5}\).

   Method & Centre-0 & Centre-1 & Centre-2 & Centre-3 & Centre-4 & Average \\  ERM & \(65.7 2.4\) & \(55.8 1.7\) & \(51.7 1.3\) & \(45.6 1.4\) & \(53.5 5.0\) & \(54.5 2.4\) \\ Macenko & \(64.3 1.9\) & \(54.4 0.9\) & \(63.8 1.4\) & \(54.8 2.0\) & \(65.3 3.7\) & \(60.5 2.0\) \\ HoVerNet & \(62.0 1.2\) & \(53.7 2.2\) & \(52.1 0.7\) & \(48.0 1.9\) & \(54.3 2.9\) & \(54.0 1.8\) \\ RandSNA & \(67.0 2.5\) & \(56.1 1.9\) & \(51.0 0.3\) & \(46.3 1.9\) & \(51.2 2.4\) & \(54.3 1.8\) \\ RSC & \(68.1 2.4\) & \(55.6 1.0\) & \(50.9 0.6\) & \(47.2 1.5\) & \(50.3 0.8\) & \(54.4 1.3\) \\ L2D & \(68.2 1.3\) & \(57.3 0.5\) & \(56.4 2.4\) & \(55.9 3.3\) & \(60.1 4.0\) & \(59.6 2.3\) \\ Ours & \(67.9 1.4\) & _70.7 \(\) 1.0_ & \(66.7 1.2\) & \(62.1 1.8\) & \(69.2 0.9\) & \(67.3 1.3\) \\  ERM-Aug & _74.0 \(\) 1.4_ & \(62.8 1.9\) & \(67.6 2.5\) & \(56.0 1.6\) & \(67.6 3.1\) & \(65.6 2.1\) \\ Macenko-Aug & \(68.8 2.0\) & \(60.9 1.1\) & \(\) & \(57.6 3.2\) & _72.5 \(\) 1.7_ & \(66.0 1.9\) \\ HoVerNet-Aug & \(70.7 1.0\) & \(54.2 1.9\) & _69.4 \(\) 2.3_ & \(61.2 2.4\) & \(69.2 2.9\) & \(65.0 2.1\) \\ RandSNA-Aug & \(70.8 3.2\) & \(66.8 2.5\) & \(68.4 2.5\) & \(61.3 2.5\) & \(71.7 2.7\) & \(67.8 2.7\) \\ DDCA-Aug & \(73.1 2.4\) & \(64.9 2.5\) & \(68.9 2.7\) & \(57.4 2.9\) & \(66.9 4.5\) & \(66.2 3.0\) \\ RSC-Aug & \(71.9 2.2\) & \(61.7 2.4\) & \(69.2 2.9\) & \(58.4 1.4\) & \(70.1 3.6\) & \(66.3 2.5\) \\ L2D-Aug & \(\)-regularisation about pulling the features towards the representation of nuclear masks. Note that nuclear masks are not used during inference in standard evaluations such as all those in Tables 1, 2, and 3. In Table 10 in the Supplement, we can see results for predictions in which the embeddings (features before the GAP layer) of the H&E images are modified by subtracting the embeddings of the corresponding nuclear masks. By comparing to Table 1, we see a drop in performance for all methods. However, the drop is largest for our method, with an accuracy below random guessing. This shows that the features computed from H&E-stained images at test time are indeed more similar to features from nuclear masks for our method than for other methods.

Impact of removal of intranuclear texture and colourIn Tables 11 and 14 in the Supplement, we consider the performance on modified H&E images, in which intranuclear texture is removed by masking it out with a constant colour (see examples in Figure 2d and 2e). This is of interest due to the observation that intranuclear texture is often different in cancerous nuclei, which can be informative to humans. We can see that if it is replaced by a colour similar to the colour of nuclei, we for our method obtain a performance (Table 11) very similar to the performance with original H&E images (Table 1). On the other hand, changing the colour to white seems to reduce the performance notably (Table 14). This is possibly due to the creation of images with outlier statistics. A more likely explanation is that it is common for H&E stains to have small holes or gaps of white background colour in the stroma, which usually are not discriminative information but rather shear stress artefacts from the tissue cutting process. Therefore, masking nuclei with white masks may effectively remove discriminative information about nuclei. This domain-specific observation may explain the asymmetry in behaviour when masking nuclei with black versus white.

Impact of removal of extranuclear informationTable 23 in the Supplement shows results on data where all the non-nuclear background is set to white (see example in Figure 2c). These images can be viewed as an inside-out inverted case of the images evaluated to give the results in Table 11. The common information in both sets of images is the morphology and organisation of nuclei. The performance for our proposed method remains high on these images (Table 23), being close to the best result on original H&E data (Table 1). The experiments in Tables 23 and 11 demonstrate the strong generalisability of focusing on nuclear morphology and organisation in out-of-domain settings.

Figure 2: Exemplary image ablations used in this study.

Impact of dilution of nuclear shapesWe expand nuclei masks by a classic morphological dilation and then blacken the dilated regions in the H&E images. The nuclear shape information in these images is thus progressively reduced compared to the images where only the nuclei are filled with black. Across all methods, we observe a drop in accuracy with an increase in dilation (Tables 11, 12, and 13 in the Supplement), highlighting the critical role of shape in this domain. The proposed method is more robust to moderate shape dilution with a mask size of 5 than the baseline methods. A similar but stronger trend appears in Tables 14, 15, and 16 in the Supplement for whitened nuclei.

Impact of removing nucleiWe dilate the nuclear mask image with a kernel size of \(5\) to encapsulate remnants of the boundary of nuclei and then use the dilated mask to remove nuclei by inpainting . The accuracy with the resulting images (see example in Figure 2g) drops to random guessing for our method (Tables 17 and 18 in the Supplement). Essentially no tiles are classified as tumour, giving a nearly zero recall and low precision (Tables 19, 20, 21, and 22 in the Supplement). Also, this supports that models trained using our method focus on nuclear morphology and organisation, and shows that the models reasonably associate the absence of nuclei with no tumour.

Saliency maps via Integrated GradientsTo further demonstrate that our method steers models to focus on nuclei, we generate saliency (pixel attribution) maps using Integrated Gradients  and show some randomly selected examples in Figures 6,7 in the Supplement. The saliency maps also indicate that a model trained using our method focuses on nuclei.

Evaluation of L2D and RSC combined with the proposed methodTables 26, 27, and 28 in the Supplement show the results of combining L2D and RSC with the proposed method. Combining the proposed method, which regularises, with L2D, which diversifies, yields mixed results, likely due to the opposing effects of these two interventions. Combining it with RSC results in a small gain over using our method alone. Overall, this demonstrates the effectiveness of the method proposed.

Evaluation on segmentation mask dataFor the sake of completeness, we show in Tables 24 and 25 in the Supplement that our method also performs well when tested on nuclear masks (as exemplified in Figure 2b) and their inversions (see example in Figure 2f).

Evaluation of robustness to image corruptionsFigure 3 shows that the proposed method has notably higher robustness to image corruptions for most experiments in eight types of corruptions described in . Samples of corrupted images are shown in Figure 5 in the Supplement.

Evaluation of robustness against adversarial attacksWe evaluated the robustness of models against adversarial attacks  using the Projected Gradient Descent (PGD) attack . Figure 4a demonstrates that models trained using our method have significantly higher robustness than ERM and L2D, the latter being the second-best performing method in Tables 1, 2, and 3. Additionally, we

Figure 3: Robustness to added noise described in .

conduct cross-model attacks by generating adversarial images using models trained with one method and evaluating them on models trained with other methods. The results indicate that our models exhibit minimal performance degradation when exposed to adversarial images generated by models from other methods (Figure 3(b)). In contrast, the accuracy of models trained with ERM and L2D drops substantially. This further demonstrates the superior robustness of models from our method.

A preliminary evaluation on a transformer architectureWe perform a comparison using a fine-tuned ViT-Tiny  model. The results are shown in Tables 29, 30 and 31 in the Supplement. These results show that our approach obtains superior out-of-domain performance for the CAMLYON17 dataset. The results are more mixed for the other datasets. In particular, it seems that models trained on one of the five centres (Center-4) in CAMELYON17 do not generalise well to other cancer types and are actually also performing sub-optimally in CAMELYON17. For models trained on each of the other four centres in CAMELYON17, the performance with our approach is, on average, better than with other approaches, but the performance increase is lower than for ResNet-50. However, in the same tissue type (CAMELYON17 data), the performance gain is similar for both ViT-Tiny and ResNet-50. Our interpretation of all these results is that our approach can improve out-of-domain performance also for ViT-Tiny, in particular across centres and scanners for the same tissue type, but that it might also fail for a minority of the training datasets. This experiment is preliminary because we took the same hyperparameters as used for ResNet-50, including the same learning rate and \(=1\), both of which might not be optimal. Also, we note that ViT-Tiny has much fewer parameters than ResNet-50. Experiments with larger transformers might obtain bigger differences, as seen in .

Limitations of this studyAs a limitation, we identify that we have performed these experiments for only one classification task. For medical practitioners, it would be of interest to measure the impact for other tasks, such as tumour grading and survival prediction, when evaluated in an out-of-domain generalisation setup. However, this would require access to multi-centre datasets with relevant labels available. Secondly, we ran the full set of experiments only on one base network, ResNet-50, because we preferred to run a larger set of ablation experiments to understand what actually has been learned when using our method. While we expect results to be qualitatively similar for other CNNs, transformer networks might have different learning dynamics, and results for those with a larger capacity than the ViT-Tiny are of interest in future work. Finally, an extension to other cancer types, such as prostate or colon cancer, would also be of interest.

## 6 Conclusion

We have shown a simple method to enforce the learning of shape features at training time, which uses unmodified input images at inference time. It shows very good out-of-domain performance and can be combined as a plugin with other methods to enhance out-of-domain generalisation. Aside from out-of-domain accuracy, the proposed method gives improved robustness to image alterations.

Figure 4: **(a)** PGD attack on models. **(b)** Cross-model PGD attacks where adversarial images are generated using a model from a method but the accuracy for those images is tested on models from other methods. Results are for the validation subset of each centre in CAMELYON17.