# Adaptive Layer Sparsity for Large Language Models

via Activation Correlation Assessment

 Wei Li\({}^{1}\), Lujun Li\({}^{2}\)\({}^{}\), Mark Lee\({}^{1}\)\({}^{*}\), Shengjie Sun\({}^{3}\)

\({}^{1}\)University of Birmingham

\({}^{2}\)Hong Kong University of Science and Technology, \({}^{3}\)AISpeech Co., Ltd.

WXL885@student.bham.ac.uk, lilujunai@gmail.com, M.G.Lee@bham.ac.uk

shengjie.sun@aispeech.com

Corresponding authors, \(\) project lead with equal contribution.

###### Abstract

Large Language Models (LLMs) have revolutionized the field of natural language processing with their impressive capabilities. However, their enormous size presents challenges for deploying them in real-world applications. Traditional compression techniques, like pruning, often lead to suboptimal performance due to their uniform pruning ratios and lack of consideration for the varying importance of features across different layers. To address these limitations, we present a novel Adaptive Layer Sparsity (ALS) approach to optimize LLMs. Our approach consists of two key steps. Firstly, we estimate the correlation matrix between intermediate layers by leveraging the concept of information orthogonality. This novel perspective allows for a precise measurement of the importance of each layer across the model. Secondly, we employ a linear optimization algorithm to develop an adaptive sparse allocation strategy based on evaluating the correlation matrix. This strategy enables us to selectively prune features in intermediate layers, achieving fine-grained optimization of the LLM model. Considering the varying importance across different layers, we can significantly reduce the model size without sacrificing performance. We conduct extensive experiments on publicly available language processing datasets, including the LLaMA-V1lV2lV3 family and OPT, covering various benchmarks. Our experimental results validate the effectiveness of our ALS method, showcasing its superiority over previous approaches. The performance gains demonstrate its potential for enhancing LLMs' efficiency and resource utilization. Notably, our approach surpasses the state-of-the-art models Wanda and SparseGPT, showcasing its ability to excel even under high sparsity levels. Codes at: [https://github.com/lliai/ALS](https://github.com/lliai/ALS).

## 1 Introduction

Large language models (LLMs)  have demonstrated remarkable performance in various natural language processing (NLP)  tasks. However, their size and computational requirements pose significant challenges for widespread adoption and deployment. To address these practical constraints, model compression techniques, such as weight pruning and quantization, can potentially reduce the size and computational requirements of LLMs.

The emergence of LLMs has revolutionized the field of NLP. However, despite their revolutionary impact, the massive scale and complexity of LLMs presents significant challenges for model compression. Conventional pruning methods , which often require one or moreiterations of fine-tuning or retraining to preserve performance, have become impractical for LLMs due to the substantial computational cost and time required.

Due to the failure to the Magnitude approach to pruning  and other previous methods on LLMs, recent efforts such as SparseGPT , Wanda , DSOT , Pruning Large Language Models with BESA , and OWL  aim to address this challenge by reconstructing the layerwise outputs of LLMs. Specifically, SparseGPT introduces a technique for pruning less significant weights and reconstructing layerwise outputs based on an importance metric derived from the Hessian matrix. To reduce the computational overhead of SparseGPT, Wanda proposes a simplified strategy that relies solely on the product of weight and activation magnitudes for pruning. DSOT computes the reconstruction error incrementally for each layer, optimizing the intra-layer sparse configuration through further weight pruning or growth, which forms the basis for subsequent weight recovery and additional pruning operations. These methods adopt a training-free approach. In contrast, BESA  proposes learning the optimal pruning ratio within each layer through training, finding that considering the overall sparsity configuration within a layer enhances the performance of sparse models. However, this method primarily focuses on intra-layer sparsity configuration. It requires substantial training time, typically taking at least 5 hours on an A100-80G GPU, which is considerably slower than other training-free techniques . Another notable method is OWL , which proposes a non-uniform layerwise sparsity technique that assigns different sparsity ratios based on the outlier ratio within each layer, leveraging the unique characteristic of LLMs where some features exhibit significantly larger magnitudes by tuning hyperparameters such as the outlier threshold and sparsity upper/lower bounds to obtain optimal parameter setting. Nevertheless, unlike the aforementioned methods, OWL relies heavily on empirical analysis without providing a solid theoretical foundation for its effectiveness.

However, existing methods have several significant drawbacks. First, for BESA, DSOT, and some traditional techniques, minimizing the layer-by-layer pruning error does not effectively mitigate the impact of pruning on model performance, as the pruning error accumulates across layers due to its inherent greedy nature  and may also become trapped in local optima . Second, LLM pruning methods such as Wanda, SparseGPT, and Magnitude apply uniform sparsity ratio to each layer, despite the significant variations in each layer's contribution to the final model performance . To achieve better performance for different layers, the sparsity needs to be manually adjusted for all layers. Third, for the newly proposed OWL method, more theoretical analysis is needed on why its outlier-based non-uniform sparsity outperforms uniform sparsity. Moreover, the choice of hyperparameters in OWL, such as the outlier threshold and sparsity upper/lower bounds, is sensitive to model performance, but their optimal ranges are not theoretically explained, and the effective ranges and thresholds are derived through manual tuning. Furthermore, the transferability of these hyperparameters across different datasets has yet to be systematically studied. Therefore, when applying OWL to new models, complex adjustments by hand must be performed to determine the potentially optimal parameter combination.

To address the multiple challenges of getting trapped in local optima, manually setting sparsity for all layers, and relying on empirical manual experiments to derive optimal sparsity ratios, we propose a simple, effective, and efficient method called Adaptive Layer Sparsity (ALS) for allocating sparsity ratios. The overall pipeline of our proposed method is illustrated in Fig. 1. This technique optimizes

Figure 1: Overview of our framework. We first compute the sum of Redundancy Metric between layer \(i\)-th and other layers to construct objective function. Then, we solve a linear programming problem to optimize total sparsity ratios \(S^{(q_{i})}\) (\(q_{i}\) is pre-layer sparsity) under constraints.

the pruning rate across different layers. To the best of our knowledge, this is the first attempt to reformulate the sparsity allocation problem in LLMs as a linear programming problem. We tackle these challenges by constructing an objective function and constraints. The constraint of the linear programming problem is that the total number of parameters should be less than the target model size. We then compute the independence matrix  at both the layer level and intra-layer component level based on the output or input features. According to our experiments, the independence between layers is positively correlated with model performance, as shown in Fig. 2 (c). Therefore, maximizing the independence between each layer of the model is considered our objective function. Unlike existing black-box optimization methods, we formulate this problem as a linear one that can be solved by any linear problem solver. This approach enables efficient global sparsity ratio allocation for LLMs ranging from 7B to 70B parameters on a single A100-80GB GPU. If the model scale is too large, reaching 160B, we can also perform multi-threaded computation on a CPU. For a 70B model, the global sparsity configuration can be obtained in just 20 minutes on an A100-80G GPU.

To rigorously assess the efficacy of ALS, we conducted extensive experiments on diverse LLMs, including LLaMA-V1 , LLaMA-V2 , LLaMA-V3 , and OPT  model families, with parameter counts ranging from 6.7 billion to 70 billion. In the main experiments, we evaluated the WikiText-2 perplexity and average accuracy on 7 zero-shot datasets at various sparsity ratios (20% to 70%) for LLaMA-V2 7B/13B (Table 1) and at 50% sparsity for all model families (Tables 2 and 3). Detailed results for each zero-shot dataset on LLaMA-V2 family models at 50% sparsity are presented in Table 4. The analysis experiments consist of 6 sets, examining the impact of calibration data, sparsity bounds setting, and model redundancy on performance (Fig. 2), as well as the influence of feature selection, standardization, and comparisons with Wanda and LoRA fine-tuning. Additional experiments including detailed of main experiments, analyses of granularity, decreasing functions, visualizations of layer redundancy, sparsity ratio allocation and comparison with OWL method are provided in the Appendix C and D. These experimental results unequivocally demonstrate that ALS consistently yields substantial performance improvements for sparse LLMs across various LLMs and downstream tasks.

## 2 Related Work

**Model Compression** method try to design efficient models and reduce the memory and computational requirements of LLMs. These methods can be categorized into quantization [42; 12; 35; 33], sparsification [17; 46; 10; 9] and distillation [56; 29; 30; 31; 32; 11; 53]. Quantization converts high bit-width weights and activations into compact, low bit-width representations, while sparsification increases the proportion of zero-valued elements in model weights. Our method optimizes LLM sparsification by strategically allocating sparsity across the model's layers to maximize performance and minimize computational overhead. In contrast to optimization-based compression techniques (_e.g._, OMPQ ) for CNN models in vision tasks, our approach focuses on different LLM models and NLP tasks and devises various functions and strategies accordingly.

**Sparsity in LLMs** has garnered significant attention as a means to accelerate inference speed and reduce memory consumption by applying sparsity in the model weights or activations. sparsity techniques can be categorized into two main approaches: structured pruning [34; 23] and unstructured pruning [16; 64; 46; 63]. While the primary focus of these works lies in determining the pruning criteria, such as weight importance and pruning ratio, the enormous parameter scale of LLMs presents an additional challenge in terms of pruning efficiency. Conventional pruning methods [15; 59; 63; 23; 27; 19; 38], dating back to the early work of Hassibi  in the 1990s, which successfully reduced model size and improved efficiency in deep learning architectures by removing redundant weights to create sparse and lightweight models, heavily rely on extensive retraining and are often infeasible for LLMs due to prohibitively high computational overhead and prolonged training times. To address this issue, researchers have developed LLM-specific pruning techniques that prioritize train-free and time efficiency. In the context of structured pruning, LLMpruner  explores the application of structured pruning to LLMs and employs LoRA to recover the performance of the pruned model. For unstructured pruning, SparseGPT  stands out as a notable method that draws inspiration from the Optimal Brain Surgeon (OBS)  approach, taking into account the impact of removing individual weights on the network reconstruction loss. SparseGPT introduces an efficient technique for estimating the Hessian matrix, enabling the application of the traditional OBS method to large-scale models. Another prominent unstructured pruning method, Wanda , employs a simple yet effective strategy based on the product of weight and activation values to identify and eliminate less important weights, further enhancing the pruning speed. Despite these advancements, most existing methods adopt a uniform pruning rate across all layers, which may lead to suboptimal performance. In contrast, our approach introduces a novel layer adaptive pruning strategy that dynamically allocates sparsity based on the importance of each layer, effectively minimizing performance degradation while achieving high compression ratios.

**Sparsity Allocation in Network Pruning.** Conventional methods for achieving adaptive layer-wise sparsity in neural networks [14; 5; 26] often rely on a layer-by-layer pruning approach, where the objective is to minimize the sum of errors introduced in each layer. However, this greedy strategy  leads to the accumulation of errors across layers, resulting in suboptimal performance when directly adapted to LLMs. The extensive retraining required on vast datasets further amplifies the challenges of applying these techniques to LLMs. Recent efforts, such as BESA  and DSOT , have shifted focus to intra-block sparsity allocation, employing various strategies to optimize the sparsity distribution within individual blocks. Despite operating at a finer granularity, these methods fundamentally adhere to a layer-wise pruning paradigm, neglecting the importance of global sparsity allocation. Consequently, the resulting allocation may be locally optimal [13; 22] within each layer but globally suboptimal, potentially leading to solutions stuck in local optima. Recently, a new approach called OWL  attempts to address this issue by introducing a non-uniform layer-wise sparsity technique. This technique primarily relies on manually tuning the outlier threshold and sparsity upper/lower bounds (which are very small values and sensitive to performance) through extensive experimentation to obtain potentially optimal parameter configurations. Although OWL demonstrates the potential for improved sparsity allocation, it heavily depends on empirical analysis and fails to provide a solid theoretical foundation for its effectiveness, limiting its generalizability and robustness across different LLMs architectures and datasets.

## 3 Methodology

### Preliminary

Pruning LLMs is a method that aims to obtain a sparse representation of the model by eliminating a predetermined fraction of the pre-trained weights. The primary objective is to minimize the divergence between the outputs generated by the sparse and dense models . However, directly tackling this problem can be challenging due to the massive scale of LLMs. We discover that the mutual information entropy in Eq. 1 can effectively quantify the degree of discrepancy between different layers of the model.

\[I(x_{i};x_{j})=H(x_{i})+H(x_{j})-H(x_{i},x_{j}) \]

Neural networks can be decomposed into a sequence of layers. In the decomposed form, we represent the neural network as \(F=\{f_{1},f_{2},,f_{L}\}\), For a given random sample \(x_{0}^{d_{0}}\), let \(x_{i}=f_{i}(f_{i-1}( f_{1}(x_{0}))) ^{d_{i}}\) represents the output of the random sample at the \(i\)-th layer.

Based on the previous definitions of the marginal entropies and joint entropy, the mutual information between \(x_{i}\) and \(x_{j}\) can be derived from Eq. 1 and formally defined as Eq. 2.

\[I(x_{i};x_{j})= p(x_{i},x_{j}),x_{j})}{p(x_{i})p(x_{j})}dx_{i}dx_{j} \]

High mutual information between layers indicates redundancy, while low mutual information suggests that these layers have learned complementary representations . When two variables \(x_{i}\) and \(x_{j}\) are independent, their mutual information is zero, i.e., \(I(x_{i};x_{j})=0\). According to information theory  and the Information Bottleneck (IB) theory , minimizing the mutual information between layers can reduce redundancy, remove irrelevant information, and enhance the overall representational capacity of the network. This "compression" of the representation enables the network to extract higher-level and more compact features, thereby reducing the reconstruction error. In summary, by sparsifying layers with higher mutual information and minimizing the mutual information throughout the entire network, the reconstruction error can be minimized.

### Redundancy Metric

To approximate the mutual information between layers, we propose employing Monte Carlo sampling, thereby circumventing the need for intractable integrals. Specifically, we randomly select \(N\) samples \(x_{0}^{(1)},x_{0}^{(2)},,x_{0}^{(N)}\) from the training dataset, which follow a probability density function (PDF) \(P(x)\). For each sample \(x_{0}^{(n)}\), the outputs at the \(i\)-th and \(j\)-th layers are denoted as \(x_{i}^{(n)}\) and \(x_{j}^{(n)}\), respectively.

We can estimate the integral using the sample average: \((x_{i};x_{j})_{n=1}^{N}^{(n)},x_{j}^{(n)})}{p(x_{i}^{(n)})p(x_{j}^{(n)} )}\).

\((x_{i};x_{j})\) is the estimated mutual information, \(p(x_{i}^{(n)},x_{j}^{(n)})\) is the joint PDF, and \(p(x_{i}^{(n)})\) and \(p(x_{j}^{(n)})\) are the marginal PDFs. We aim to approximate these probability densities using kernel density estimation.

**Computing marginal and joint probability densities**: Based on the kernel density estimation , we can utilize it to estimate the probability density functions. This allows us to approximate the probability density functions using the features of the samples. Kernel density estimation is a non-parametric method for estimating the probability density function of a random variable. For instance, given a set of samples \(Y=\{y_{1},y_{2},,y_{N}\}\), The kernel density estimate is defined as: \((y)=_{i=1}^{N}K(}{})\), where \(K()\) is a kernel function, commonly used kernels include Gaussian, Epanechnikov, etc., and \(h\) is the bandwidth parameter.

We apply kernel density estimation to compute the marginal and joint probability density functions of the samples' outputs at the \(i\)-th and \(j\)-th layers. The choice of the bandwidth parameter \(h\) can be determined through cross-validation or other methods. However, to simplify our derivation, we can consider that in high-dimensional spaces, the influence of the kernel function \(K\) is relatively insignificant. We are mainly focused on the ratio of relative densities . Therefore, the bandwidth parameter \(h\) can be cancelled out. We can calculate the marginal and joint probability density functions of the samples' outputs at the \(i\)-th and \(j\)-th layers using kernel density estimation, which can be found in the Appendix.

**Monte Carlo Approximation of Mutual Information.** Substituting the kernel density estimates into the Monte Carlo approximation formula for mutual information and simplifying the expression using the feature matrix inner product approximation for the kernel function, as mentioned by Tschannen , we obtain:

\[(x_{i};x_{j})_{n=1}^{N}^{(n)T}x_{j}^{(n)}\|_{F}}{\|x_{i}^{(n)T}x_{i}^{(n)} \|_{F}\|x_{j}^{(n)T}x_{j}^{(n)}\|_{F}} \]

where \(\|\|_{F}\) denotes the Frobenius norm. In this approximation, we employ the feature matrix inner product to approximate the kernel function, \(K((x_{i}^{(n)},x_{j}^{(n)}),(x_{i}^{(k)},x_{j}^{(k)} ))\|x_{i}^{(n)T}x_{j}^{(n)}\|_{F}\). Similarly, for the marginal kernel functions is \(K(x_{i}^{(n)},x_{i}^{(k)})\|x_{i}^{(n)T}x_{i}^{(n)} \|_{F}\).

**Decreasing function**: Since mutual information has no general upper bound, its upper limit depends on the entropy of either \(x_{i}\) or \(x_{j}\). To address this, we can use decreasing functions to transform the range of mutual information, ensuring a bounded and more interpretable metric. For instance, using \(e^{(x_{i};x_{j})}\) or a Gaussian function, we can redefine the measure as follows. Considering that a batch of data is fed into the model simultaneously and each layer output concurrently, we can omit the \(_{n=1}^{N}\) and \(\). Instead, we can use \(X_{i}\) and \(X_{j}\) to represent the calculations for the entire batch input. Applying \(\|X_{i}^{T}X_{j}\|e^{(X_{i};X_{j})}\) as a decreasing function to Eq. 3. Therefore, we can derive the Redundancy Metric (RM) formula, where \(RM()\) according to the Cauchy-Schwarz inequality:

\[RM(X_{i},X_{j})=^{T}X_{j}\|^{2}}{\|X_{ i}^{T}X_{i}\|\|X_{j}^{T}X_{j}\|} \]

The decreasing function transforms the range of the RM formula such that a value of 0 indicates complete independence between layers, while 1 represents complete redundancy. This formulationcan serve as the objective function for maximization. The complete derivation process for this section, including the details of Eq. 4, is presented in Appendix B.

### Linear Optimization

Our Redundancy Metric reveals the redundancy among layers in a neural network, guiding sparsity ratio allocation. Experiments on LLaMA2-13B with various sparsity configurations show a negative correlation between model redundancy and WikiText-2 perplexity (PPL). Model redundancy is defined as the sum of each layer's RM for the remaining layers, as depicted in Fig. 2 (c). Consequently, redundancy minimization is adopted as the objective function, incorporating model size constraints to formulate a linear programming problem that yields the optimal sparsity configuration.

**Intra-layer Sparsity Allocation.** For a given neural network, we construct a redundancy matrix \(\), where \(_{ij}=RM(x_{i},x_{j})\). The sum of non-diagonal elements for each row of the matrix is computed as \(_{i}=_{j=1}^{L}_{ij}-1\). A smaller \(_{i}\) indicates stronger independence between \(x_{i}\) and the outputs of other layers. We model this relationship using the monotonically decreasing function: \(_{i}=e^{-_{i}}\), where \(\) is a dynamic hyperparameter controlling the difference in sparsity ratios across layers, defined as \(_{j=1}^{n}_{ij}\), which smooths the descent speed (Fig. 8). The importance factor for the first \(i\) layers is represented by \(_{i}\). With these components, we formulate the linear programming problem as follows:

\[\ \ _{}_{i=1}^{L}( }{L-i+1}_{j=i}^{L}_{j}), \] \[\ _{i}^{L}S^{(q_{i})}.\]

where \(S^{(q_{i})}\) denotes the model size of the \(i\)-th layer under sparsity \(q_{i}\), and \(\) represents the target model size. The optimal sparsity configuration is given by \(\). To maximize the model's representative capacity, our method try to assign smaller sparsity configurations to more independent layers by maximizing an objective function. For a more fine-grained sparsity allocation, we extend our approach to include intra-layer component-level sparsity allocation. After determining the sparsity ratios for each layer, we treat the remaining parameters in this layer as the target size and construct objective functions for its individual components. By applying ALS at this granular level, we obtain a secondary sparsity allocation, resulting in unique sparsity ratios for every layer and component. This hierarchical approach enables a highly customized and adaptable sparsity distribution throughout the entire network architecture, potentially leading to enhanced efficiency and performance gains.

## 4 Experimental Results

**Setup.** For pruning, we follow the settings of Wanda, SparseGPT, and Magnitude. Regarding the calibration data used in the linear optimization process, we follow the configurations of SparseGPT and Wanda, selecting data from the C4 dataset and ensuring that all test data are zero-shot. We use a calibration data size of 16 for linear optimization hyperparameters. The granularity, explained in Appendix. E.1 for linear optimization results is set to 0.5%. For the values of \(x_{i}\), we use the input, although output and intermediate gates can also be used. Hyperparameter analysis is primarily conducted in the analysis section. Details about the experimental environment are provided in Appendix E.1.

**Evaluation and Metrics.** We measure the performance of pruned models through zero-shot tasks and language modeling. For zero-shot evaluation, we utilize seven tasks from the EleutherAI LM Harness : Winogrande , PIQA , OpenBookQA , HellaSwag , BoolQ , ARC (Easy and Challenge) , and RTE (Recognizing Textual Entailment) . We also include WikiText2 . For the first seven datasets, we use the accuracy metric provided in the EleutherAI LM Harness. For WikiText2, we use the word_perplexity (PPL) metric. During evaluation, we ensure using the same database version, GPU model, and random seed.

**Models.** We evaluate the performance of ALS on LLMs, including LLaMA-V1 7B/13B/30B/65B , LLaMA-V2 7B/13B/70B , LLaMA-V3 8B , OPT 6.7B/13B .

**Baselines.** We run ALS on LLMs with various methods, including Wanda , Magnitude-based pruning  and SparseGPT .

### Language Modeling

**Quantitative Evaluation.** In Table 2, we compare the wikitext2 (PPL) performance of different pruning methods under 50% sparsity on the LLaMA-V1, LLaMA-V2, LLaMA-V3, and OPT models, including Dense (unpruned), Magnitude pruning , SparseGPT pruning , Wanda pruning , and the results of these pruning methods enhanced by ALS. The results show that the ALS generally improves the performance of various pruning methods.

For LLaMA-V1 models, Magnitude pruning shows high perplexity, e.g., 42.26 for the 7B model, reduced to 16.80 with ALS. SparseGPT performs better, with 18.35 for the 13B model, reduced to 11.87 with ALS. Wanda achieves the best results, with 13.30 for the 13B model, reduced to 12.47 with ALS.

For LLaMA-V2 and LLaMA-V3 models, ALS also reduces perplexity significantly. For instance, the Magnitude pruning in 13B LLaMA-V2 model drops from 15.19 to 10.78, and Wanda pruning in the 8B LLaMA-V3 model from 15.01 to 12.30 with ALS.

On the OPT model, perplexity significantly increases after pruning. For instance, the 13B model of OPT has a perplexity as high as 4.09e4 after Magnitude pruning, which remarkably reduces to 3.96e3 with ALS, demonstrating the effect of ALS in handling LLM pruning. However, there is an example where performance does not significantly improve with ALS. For instance, the 13B model of LLaMA-V1 has 9.90 perplexity after SparseGPT pruning, which slightly increases with ALS.

In summary, ALS significantly enhances model performance across various pruning methods by effectively mitigating performance loss.

**Varying Sparsity Rates.** Table 1 presents the perplexity scores of sparse LLaMA-V2 7B and 13B models pruned by Magnitude, SparseGPT, and Wanda methods, with and without ALS, at varying sparsity levels (20% to 70%). The results show that as sparsity increases, perplexity scores generally deteriorate, indicating a decline in language modeling performance. However, as the sparsity level increases, the performance gap between ALS and non-ALS methods widens, with ALS exhibiting better performance at most sparsity levels. This suggests that ALS can help mitigate the performance degradation caused by higher sparsity, becoming increasingly effective at maintaining LLMs performance as the sparsity level grows.

   Models &  &  \\  Sparse & 20\% & 30\% & 40\% & 50\% & 60\% & 70\% & 20\% & 30\% & 40\% & 50\% & 60\% & 70\% \\  Magnitude & 9.20 & 10.21 & 13.51 & 32.87 & 7.664 & 9e5 & **7.84** & 8.19 & 9.21 & 11.59 & 23.43 & 1.4e3 \\ _Magimulde w. ALS_ & **8.91** & **9.60** & **11.03** & **15

[MISSING_PAGE_FAIL:8]

### Ablation Study

In this part, we examine the impact of various components within the ALS framework and compare it with LoRA Fine-tuning, specifically focusing on its bound setting, standardization on weight or feature, granularity choice, which can be found in Appendix. E.1, feature choice and robustness to calibration samples. All experimental setups are based on the LLaMA2-13B model with Wanda pruning and ALS.

**Comparison with LoRA Fine-tuning.** Our experiments in Table 5 and Table 6 demonstrate the substantial benefits of combining Wanda+ALS with LoRA fine-tuning across the LLaMA model family. The improvements are most striking in the LLaMA-V1 7B model, which showed a dramatic reduction in perplexity by 4.82 alongside a 10.17% increase in accuracy. Larger V1 models also benefited, with the 30B and 13B variants showing perplexity reductions of 1.43 and 1.15, coupled with accuracy gains of 2.09% and 1.89% respectively. The LLaMA-V2 models exhibited similar positive trends, with both 7B and 13B versions showing perplexity improvements and accuracy increases. These impressive results were achieved using just 2000 C4 samples for LoRA fine-tuning in a zero-shot setting, highlighting the method's efficiency and effectiveness even with limited training data unrelated to the evaluation tasks.

**Feature Selection and Normalization.** Table 7 (a) compares the performance of input, output, and gate features in capturing layer independence, with output features achieving slightly lower perplexity. Table 8 (b) demonstrates the significant impact of jointly normalizing features and per-layer weights. Applying this normalization strategy yields a substantial improvement in accuracy, increasing from 66.40% to 68.11%, while also reducing perplexity from 10.078 to 10.070.

**Comparison with OWL.** We compared the performance of our proposed method with the OWL method on a set of benchmark datasets. The results are summarized in Table 9. We adopted the optimal parameter settings described in the OWL paper. Across all tested configurations, our method consistently achieved lower values compared to OWL, demonstrating its superior performance. Specifically, in the unstructured 50% setting, Wanda with ALS outperformed OWL by a margin of 0.25 units. Furthermore, in the structured pruning settings of 2:4 and 4:8, the advantage of Wanda with ALS increased to 0.95 and 0.44, respectively.

**N:M Results.** We also investigated the performance of our method in the N:M setting, where \(N\) features are selected from \(M\) available features. The results are shown in Table 9. Similarly, for OWL, we used the optimal parameter combination reported in their paper. Across all N:M configurations, ALS consistently achieved lower values compared to OWL. As the number of selected features \(N\) increased, both methods exhibited performance improvements, but the advantage of ALS became more pronounced. For instance, in the 2:4 case, ALS outperformed OWL by a margin of 0.95 units, and this gap further widened to 1.82 in the 4:8 case. Overall, OWL is a method that is highly sensitive to parameter settings, and obtaining the optimal parameters may require dozens of experiments to determine the best combination. Moreover, there is no clear theoretical analysis explaining why such a combination should be used.

   Models & V1-7B & V1-13B & V1-30B & V2-7B & V2-13B \\  Dense & 9.38 & 8.20 & 6.09 & 8.71 & 7.68 \\  _Wanda_ & 13.30 & 10.90 & 8.74 & 12.31 & 11.21 \\ _w. ALS_ & 12.47 & 10.40 & 8.42 & 11.61 & 9.86 \\ _w. LoRA_ & 7.65 & 9.25 & 6.99 & 9.89 & 8.30 \\   

Table 5: WikiText-2 perplexity of Wanda with ALS at 50% sparsity on LLaMA-family models.

   Feature Choice & PPL & ACC \\  In & 10.070 & 60.75 \\ Out & 10.012 & 60.56 \\ Gate & 10.030 & 60.76 \\   

Table 7: Results of feature choice from varying component output of each layer on WikiText2 and zero-shot tasks.

   Models & V1-7B & V1-13B & V1-30B & V2-7B & V2-13B \\  Dense & 96.18 & 68.50 & 71.36 & 66.21 & 68.76 \\  _Wanda_ & 83.87 & 64.74 & 68.54 & 61.88 & 64.48 \\ _w. ALS_ & 61.47 & 64.82 & 69.35 & 62.84 & 66.58 \\ _w. LoRA_ & 71.64 & 66.67 & 71.44 & 65.05 & 67.81 \\   

Table 8: Impact of normalizing features and per-layer weights for distance function on WikiText2 and zero-shot tasks.

**Calibration Data.** In Fig. 2 (a), we present the performance of pruning methods with different numbers of calibration samples. We use the size of 2, 4, 8, 16, 32, 64, 128, 256. Although this experiment reveals that the model's performance improves with an increase in the size of the calibration data, the improvement is quite limited. Even when comparing the scales of 2 and 256 in calibration samples, the perplexity decreases by only 0.11. These results further highlight the robustness of ALS.

**Boundes.** In Fig. 2 (b) demonstrates the effect of pruning bounds on the performance of the LLaMA-V2 13B model. When the pruning bounds are set too high (e.g., 0.0-1.0), the model's performance significantly deteriorates from \(10^{1}to10^{3}\) compared with 0.3-0.7, indicating that aggressive pruning may impair the model's representational capacity. However, when the pruning bounds are set between 30% and 70%, the model's performance remains nearly unaffected.

**Computation efficiency.** As shown in Table 10, our ALS involves two computational phases: the Redundancy Metric (RM) calculation, which consistently takes approximately 90 seconds across all methods, and the Linear Programming (LP) solution, requiring roughly 160-170 milliseconds. The total processing time varies notably depending on the base pruning method employed: Magnitude pruning, requiring just 1.62 seconds for its base operation, achieves the fastest total completion time of 1.51 minutes when combined with ALS. Wanda, with its base pruning time of 199 seconds, completes the entire process in 4.81 minutes, while SparseGPT, requiring 1058 seconds for its base operation, takes 19.16 minutes in total. Compared to BESA  with 4.5 hours for sparsity allocation and pruning, our approach is notably faster, completing the process in minutes rather than hours.

## 5 Conclusion

In this work, we present Adaptive Layer Sparsity (ALS), a novel approach for optimizing LLMs through the efficient allocation of sparsity across layers. By minimizing inter-layer redundancy, ALS achieves significant model compression while maintaining performance, as demonstrated through extensive experiments on diverse LLMs and tasks. We hope ALS offers valuable insights and practical tools for deploying LLMs under limited computational resources, and that our work may shed light on the role of sparsity in LLMs and its potential for model optimization. Future research will explore the relationship between sparsity allocation and individual weight importance, and investigate the integration of dynamic sparsity allocation with pruning metrics. By pushing the boundaries of model compression and efficiency, we aim to enhance the development of more capable and accessible LLMs for diverse applications.

    & &  \\ Base Method & RM (s) & LP (ms) & Total (min) \\  Magnitude (1.62s) & 88.59 & 169 & 1.51 \\ SparseGPT (1058s) & 91.32 & 158 & 19.16 \\ Wanda (199s) & 89.47 & 160 & 4.81 \\   

Table 10: Pruning speed of various methods with ALS on LLaMA-V2-7B.

Figure 2: (a) Calibration data experiment: PPL decreases slightly with more data. (b) Pruning bounds: Model performance remains relatively stable between 30% and 70% bounds. (c) Model redundancy: Higher RM metric, lower performance.