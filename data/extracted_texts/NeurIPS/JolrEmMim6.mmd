# Algo: Synthesizing Algorithmic Programs

with LLM-Generated Oracle Verifiers

Kexun Zhang\({}^{1}\) Danqing Wang\({}^{1}\) Jingtao Xia\({}^{1}\) William Yang Wang\({}^{1}\) Lei Li\({}^{2}\)

\({}^{1}\)University of California Santa Barbara \({}^{2}\)Carnegie Mellon University

{kexun,danqingwang,jingtaoxia,william}@ucsb.edu leili@cs.cmu.edu

###### Abstract

Large language models (LLMs) excel at implementing code from functionality descriptions but struggle with algorithmic problems that require not only implementation but also identification of the suitable algorithm. Moreover, LLM-generated programs lack guaranteed correctness and require human verification. To address these challenges, we propose Algo, a framework that synthesizes **A**lgorithmic programs with **LLM-**G**enerated **O**racles to guide the generation and verify their correctness. Algo first generates a reference oracle by prompting an LLM to exhaustively enumerate all the combinations of relevant variables. This oracle is then utilized to guide an arbitrary search strategy in exploring the algorithm space and to verify the synthesized algorithms. Our study shows that the LLM-generated oracles are correct for 88% of the cases. With the oracles as verifiers, Algo can be integrated with any existing code generation model in a model-agnostic manner to enhance its performance. Experiments show that when equipped with Algo, we achieve an 8x better one-submission pass rate over the Codex model and a 2.6x better one-submission pass rate over CodeT, the current state-of-the-art model on CodeContests. We can also get 1.3x better pass rate over the ChatGPT Code Interpreter on unseen problems. The problem set we used for testing, the prompts we used, the verifier and solution programs, and the test cases generated by Algo are available at https://github.com/zkx06111/ALGO.

## 1 Introduction

Large Language Models (LLMs) have demonstrated significant prowess in generating code from natural language descriptions. Models such as Codex  and CodeGen  can easily achieve over 30% pass@1 accuracy on HumanEval, a docstring-to-code dataset. However, these models struggle when faced with algorithmic problems akin to those encountered in CodeContests . Even with reasonable sample limits, achieving a 10% accuracy rate remains a considerable challenge . More recently, the GPT-4 model, despite being provided with problem descriptions and solution hints, managed a pass@1 accuracy of only 10.6%  on LeetCode Hard problems.

The verifiability of LLM-based code generation presents another significant challenge. Without verifiability, code generation systems can hardly earn trust for production use. Users of GitHub Copilot , an AI assistant for programmers, reportedly spend over 20% of their time verifying the suggested code snippets. Existing approaches toward verification and reranking LLM-generated programs either rely on neural models to predict confidence scores  or require LLM-generated test cases to execute the programs . However, neural verifiers do not provide interpretable feedbacks and LLM-generated test cases are often incorrect.

Traditional code synthesis techniques rely on _oracles_ for the verification of the synthesized code. These oracles are typically derived from formal specifications . They can also be employed in program synthesis under the widely-used framework of oracle-guided inductive synthesis (OGIS) . Despite their widespread application in code verification and synthesis, theuse of oracles often demands heavy human involvement, and the automatic generation of oracles is challenging. Existing strategies focus on generating oracles that can only detect crashes (safety oracle) or bugs introduced by future changes (regression oracle), rather than semantic bugs crucial for algorithmic programs .

Motivated by the potential benefits of oracles in traditional code synthesis, we introduce Algo, a framework that leverages oracles generated by large language models to address the challenges in LLM-based code synthesis and verification. As illustrated in Figure 1, Algo incorporates two modules for algorithm synthesis. The verifier is instructed to generate an exhaustive search algorithm regardless of time efficiency, thus acting as a reference oracle. Concurrently, the coder is asked to find a more efficient solution with any prompts or search strategies. The candidate program's correctness is then evaluated by comparing its output with that of the oracle for a given set of test inputs. The results of the verification, along with any test cases where the candidate program falls short, are subsequently provided to the coder for code refinement.

We evaluated Algo based on two key dimensions: the oracle's verification capability and its potential to enhance algorithm synthesis. Our experiments reveal that the reference oracles are correct for 88.5% of the problems (as shown in Figure 2). Moreover, the oracles' verdicts are in agreement with the golden verdicts of the online judge 75% of the time. To examine the enhancement in accuracy offered by Algo, we integrated it with several existing code generation models including Codex , CodeT , PG-TD  and ChatGPT Code Interpreter 1. We observed that Algo significantly boosts their performance on CodeContests  and a collection of recent LeetCode problems. Our experiments show that Algo notably enhances the performance of these models: in terms of one-submission pass rate, Codex's performance improved by a factor of 8, CodeT's by 2.6, PG-TD's by 1.5, and ChatGPT Code Interpreter's by 1.3.

Our contributions can be summarized as follows:

* We present Algo, a novel framework for **A**lgorithm synthesis that utilizes **LLM-G**enerated reference **O**racles as verifiers. It is a model-agnostic framework that can be integrated with any code generation model to verify candidate solutions with reliable test cases generated by reference oracles.
* We conduct a comprehensive evaluation of synthesis accuracy and verifiability of Algo, utilizing several distinct code generation models in a versatile, model-agnostic manner. Our results indicate that Algo can generate high-quality oracles and test cases that lead to significant improvements in code generation.

Figure 1: The Algo pipeline. The verifier LLM generates the probably correct but possibly slow reference oracle that solves the problem with an exhaustive search. The coder generates a more efficient candidate program and refines the candidate program by comparing its output with the oracle’s output. The coder can be any existing code generation model.

Figure 2: The reference oracle generated by Algo is correct for 88.5% of the problems in our benchmark. We consider an oracle correct if it gets _Accepted / Time Limit Exceeded_ verdict on LeetCode website and is checked by human experts as a reasonable solution.

## 2 Algorithm Synthesis

Large language models excel at generating simple programs. For example, ChatGPT can easily achieve over 70% pass@1 accuracy on the docstring-to-code dataset HumanEval. However, it still struggles with complex algorithmic problems such as CodeContests. We hypothesize that this is because algorithmic problems need efficient solutions rather than straightforward and brute-force ones. To test our hypothesis, we prompt ChatGPT in two different ways to see if they result in different success rates. One prompt directly describes the problem and asks the model to generate a solution that meets the time limits. The other asks the model to ignore the time limit and to solve the problem in the most straightforward way. The prompts we used are listed in Appendix A.2. We evaluate the correctness of the brute-forces and efficient solutions both manually and using official test cases without time limits so that a correct yet inefficient program is also considered a success.

The results in Table 1 validate our hypothesis. For LeetCode and CodeContests, it is much easier to generate brute-forces than efficient solutions. But for HumanEval, the two success rates are similar. This clearly indicates that problems in HumanEval and those in LeetCode and CodeContests are of two different types. The latter is more difficult to solve because it requires the application of algorithms. Based on that, we divide code synthesis into two categories: _functionality synthesis_ and _algorithm synthesis_. Typically, _functionality synthesis_ problems, such as those seen in HumanEval  and MBPP , provide a detailed description that can be readily translated into an implementation. On the other hand, _algorithm synthesis_ problems found in APPS  and CodeContests  are more abstract and need efficient algorithms to solve. In this paper, we focus on addressing _algorithm synthesis_ rather than _functionality synthesis_. An algorithmic synthesizer, thus, has an additional task to that of a functionality synthesizer--it needs to generate the solution idea, implicitly or explicitly, before synthesizing the code. We further clarify the contrast between the two by providing typical examples of each in Figure 3.

**Formal definition of algorithm synthesis and functionality synthesis.** A code generation task can be defined as a tuple \((Q,J)\), where \(Q\) is the problem description and \(J:\) is the system judge. The system judge \(J\) evaluates if a program from the program space \(\) solves the problem represented by \(Q\). The goal of the synthesizer is to produce a program \(P\) that \(J(P)=\). The system judge \(J\) is further composed of two components: \(J_{S}:\) and

    & LeetCode & CodeContests & HumanEval \\  Brute-force success rate & 88.5\% & 72.0\% & 70.1\% \\ Efficient solution success rate & 41.2\% & 7.90\% & 72.5\% \\  Relative difference & 114\% & 822\% & -3\% \\   

Table 1: The success rates when ChatGPT is prompted to generate efficient solutions and brute-force solutions and their relative differences. Generating brute-force solutions for LeetCode and CodeContests is much easier than generating efficient solutions.

Figure 3: Examples of algorithm synthesis (top) and functionality synthesis (bottom). The parts in solid boxes are given by the problem, while the parts in dotted boxes should be inferred by the model. Functionality synthesis does not require the model to infer the idea, while algorithm synthesis does.

\(J_{T}:\). \(J_{S}\) checks the semantic correctness of the generated code, while \(J_{T}\) ensures that the efficiency of the generated code satisfies the requisite conditions. Intuitively, \(J(P)=J_{T}(P) J_{S}(P)\).

## 3 Algorithmic Synthesis with LLM-Generated Oracles (Algo)

As discussed in Section 2, generating brute-force solutions to algorithm synthesis tasks is much easier than generating efficient solutions; this huge gap in difficulty can be exploited. To solve algorithm synthesis, we propose Algorithmic Synthesis with LLM-Generated Oracles (Algo). As shown in Figure 1, the Algo framework utilizes two components - a _coder_ and a _verifier_ - to solve algorithmic problems. The _coder_ takes the problem description \(Q\) and optionally, the verification results from its last generation and generates a program \(P\) that solves \(Q\). The _verifier_ generates a reference oracle whose outputs are used to verify whether the candidate program generated by the coder is correct. For each problem, Algo creates the verifier once and uses it to guide arbitrary coders, allowing Algo to be model-agnostic.

### Verification with Oracle

Oracle GenerationThe difference between a reference oracle \(P_{O}\) and an actual solution \(P_{G}\) is that \(P_{O}\) only needs to be semantically correct (i.e. \(J_{S}(P_{O})=\)) while \(P_{G}\) needs to be correct and efficient (i.e. \(J(P_{G})=J_{S}(P_{G}) J_{T}(P_{G})=\)). We utilize this difference to generate the oracle. When there is no time limit, most algorithmic problems can be solved with an exhaustive search . As depicted in Figure 4, we put the process of an exhaustive search algorithm (which is the same for every problem) in the prompt along with the problem description. LLMs, which excel at implementing a program when provided with clear instructions, are then able to generate reference oracles.

The Verification ProcessTo handle a verification request, we must return a verdict of True/False regarding \(P\)'s correctness, and optionally, the inputs that cause \(P\) to fail. We utilize an _input generator_ program to create random test inputs in line with problem constraints. These test inputs are then supplied to both the program under verification \(P\), and the oracle \(P_{O}\). The test outputs of \(P\) are compared against those of \(P_{O}\). If they match, a True verdict is returned. If they don't, the failed test cases are returned to the synthesizer along with a False verdict.

### Code Synthesis Strategies

Algo's coder can use different strategies for code synthesis. The code synthesis process can either be one-time or iterative, depending on the used coder. During a one-time synthesis, the coder generates a batch of solutions to the problem, and these solutions are verified and ranked according to their verification results. During an iterative synthesis, the code generates solutions, gets their verification results (and optionally mistaken test cases), and then revises the solutions iteratively.

Figure 4: The prompt we used for oracle generation and one oracle generated with it. The instructions are in blue. The correct solution solves the problem with binary search in polynomial time, while the generated oracle takes exponential time to enumerate all possible work allocations.

Other than the coder model itself, its strategy to search for and identify the suitable algorithm for the problem also affects the synthesis accuracy significantly. The coder's search strategy can be defined as a conditional distribution \((P|Q)\) of the program \(P\) conditioned on the problem description \(Q\). To generate code is to sample programs from this distribution. To compute this distribution, we can define a space of latent variables \(\) and marginalize over it, as denoted by

\[(P|Q)=_{I}(P|I)(I|Q).\]

Algo is a versatile framework that can incorporate a variety of search strategies. A strategy may be implicit, with the variable \(I\) not specified, or explicit, specifying \(\), the space of \(I\), and the methodology to traverse this space. Here, we give three examples: _an implicit searcher_, _an instruction enumerator_, and _an iterative searcher_. However, it is crucial to note that these examples are representative, and there are numerous alternative search strategies in Algo.

**Implicit Searcher:** An implicit searcher samples a program directly from the distribution determined by the underlying model, using the problem description. It solely relies on the ability of the code generation model and depends on the oracle to filter the generated programs post hoc.

**Instruction Enumerator:** An instruction enumerator is an explicit searcher where \(\) is a set of pre-defined instructions. Since the performance of instruction-tuned LLMs varies significantly based on the instructions they are given, it is natural to consider enumerating a space of instructions to get the best result. An instruction enumerator first picks an instruction from a pre-defined instruction set \(\), and then synthesizes the program by instructing the language model with \(I\). In our example of an instruction enumerator, we choose \(\) to be a set of high-level ideas of algorithms such as _'Binary Search'_ and _'Sorting'_. We first sample possible solution algorithms and then instruct the large language models to generate programs with the ideas given.

**Iterative Searcher:** A iterative searcher is an explicit searcher that takes the signal from the verifier to refine its output. The searcher space \(\) can be the token vocabulary of the model. It uses a search algorithm to determine the next token during the decoding process by enumerating the vocabulary tokens given the problem description and the partial program in each generation step. Since the search space is exponentially large, it would be time-intensive. Usually, the searcher is guided by some rewards from the verifier to prune the candidate space in each step.

## 4 Experiments

In this section, we implement Algo with three code synthesis strategies, and evaluate it with two challenging algorithmic benchmarks to validate its flexibility and effectiveness. Moreover, we investigate the verifier's performance by evaluating the quality of the generated reference oracle and test cases.

### Experiment Setup

**Verifier** We employ ChatGPT Code Interpreter to create the verifier. It is first prompted to generate the reference oracle and then the input generator. As mentioned in Section 3.1, this is possible because the reference solutions to most algorithmic problems involve exhaustively going through all possible solutions to find a feasible or optimal one. We use a temperature of 1.0 and resample the solution until it can pass all example cases. The prompt we used for generating the oracle is shown in Figure 4. For the verifier, we simply use zero-shot prompts to generate the input generator with an input validator since both of them are functionality syntheses and easy to generate. We set the maximum length of each input variable to 10 and skip out cases that raise the timeout exception and the recursion error when generating the output via the reference oracle. The prompts we used are listed in Appendix A.2 with generation examples. Note that for each problem, we only need to create the verifier once, and it can be used to create arbitrary test cases and guide arbitrary models.

Code Synthesis StrategiesWe integrate the following baselines as the coder with Algo to evaluate its verification capability and synthesis accuracy:

* **Codex** and **CodeT**: Codex and CodeT are used as coders that utilize _implicit searchers_. Codex is a specialized language model, trained on publicly available code from GitHub. CodeT uses Codex to generate both programs and test cases for a problem and uses their dual agreement to filter and rank the programs generated. To evaluate Algo, we use the exact same set of programs Codex generated, and rank them in three different ways: in a random manner, based on the dual agreement heuristics from CodeT, and according to the verification results provided by Algo. We then compare the top-ranked programs from each of these three rankings. Note that due to OpenAI's policy, the Codex model is no longer available so we directly use the Codex-generated programs provided by the CodeT paper2. * **ChatGPT Code Interpreter**: ChatGPT Code Interpreter is used as a coder that utilizes _instruction enumerators_. It is a variant of GPT-3.5 with the code interpreter plugin. The version we evaluated, released on March 24th, utilizes the underlying model text-davinci-002-code. As the Code Interpreter is instruction-tuned, it can follow directives to solve a problem using a specific class of algorithms, such as 'Binary Search'. This capability allows it to employ a search strategy that iterates through a list of algorithm categories to identify a correct solution. Moreover, with the code interpreter plugin, it is able to run Python code and interpret its results, and automatically refine its output based on the program output. We use the default temperature of 1.0 and resample the code solution until it can pass the example cases in the problem descriptions or reach the resample limit (N=5).
* **PG-TD**: PG-TD is used as a coder that utilizes _iterative searchers_. In contrast to CodeT, which employs test cases post hoc, PG-TD incorporates them during the decoding process. It implements a tree search-based planning algorithm for decoding, utilizing the reward from test cases to estimate the value of the children nodes in the search tree. PG-TD uses GPT-2  and GPT-NEO  as the underlying model. We implement two versions: PG-TD* is only based on the public test cases to generate programs, and PG-TD has access to both public test cases and the generated test cases by Li et al. . Both are based on the GPT-2 fine-tuned on the APPS training dataset, which was released by Zhang et al. . We integrate PG-TD into Algo by substituting the generated test cases with the reward from the verifier.

BenchmarksWe utilize the following benchmarks to evaluate Algo:

* **CodeContests**: We evaluate Algo with Codex, CodeT and PG-TM on all 165 problems from CodeContests test set, which are collected from Codeforces, a competitive programming website. All the three baselines we evaluate have been evaluated on CodeContests in their own paper, we follow the exact same setting to ensure a fair comparison.
* **LeetCode**: We evaluate Algo with ChatGPT Code Interpreter on 35 recently released LeetCode problems. To avoid the influence of contamination and faithfully evaluate Algo's ability in verification and guiding code synthesis, we made sure these problems are released concurrently or after the release of GPT-4. We manually annotate the algorithm categories for the solutions to these problems to evaluate the search strategy of enumerating algorithm categories. The list of problems and their corresponding categories is listed in Appendix A.3.

MetricFollowing previous studies [31; 4], we use the \(n@k\) metric [16; 5; 17] to evaluate the accuracy of code synthesis. To compute \(n@k\) for a problem, a code synthesizer needs to generate \(k\) candidate programs and select the top-\(n\) candidates to submit according to some reranking mechanism, which in our case is the LLM-generated oracles and test samples. \(n@k\) is the proportion of problems that can be solved by any of the top-\(n\) submissions. Since some code synthesizers do not have reranking mechanisms, we define their \(n@k\) as the proportion of problems solved with \(n\) submissions randomly sampled from \(k\) candidates. To ensure a fair comparison, we use the same sampling budget \(n\) under the same setting when evaluating the performance of Algo and baselines.

### Synthesis Accuracy

CodeContestsWe present Algo's \(n\)@\(k\) accuracy for CodeContests in Table 2. In the first part of the table, Codex is used as an implicit searcher. Compared to Codex itself and CodeT which also heuristically evaluates the program Codex generates, Algo consistently outperforms them at every value of \(n\). The most significant performance enhancement by our method is observed at \(1@k\), yielding an 8\(\) improvement over Codex, and a 2.6\(\) improvement over CodeT. As the value of \(k\) increases, the performance advantage diminishes. This trend is expected, as the chosen sample number \(n\) approaches the total sample count \(k\), causing the pass rate to converge towards the ratio of correct programs within the total program set. The same phenomenon is also observed in the second and third part of Table 2, where PG-TD and ChatGPT are used as coders. Algo is able to achieve a 2.5\(\) improvement over PG-TD and a 2.8\(\) improvement over ChatGPT at 1@\(k\).

LeetCodeWe list Algo's one-submission pass rate on LeetCode problems in Figure 5. The instruction set we enumerate for each problem is a set with several algorithm categories, among which there is one correct category. Algo's superior performance compared to ChatGPT and GPT-4 demonstrates the benefits of an instruction searcher that enumerates possible algorithms. It also demonstrates Algo's ability to filter and select the correct algorithm category. Without the verifier, we would not have been able to tell right from wrong.

### Verification Analysis

The verifier is critical to Algo as it guides the creation of an efficient and accurate solution. In this section, we scrutinize the quality of the verifier from several perspectives.

**LLM-generated oracles are usually correct.** The correctness of LLM-generated oracles is crucial to the correctness of Algo's test cases. We examine their correctness with both the system judge and human experts. We submitted LLM-generated oracles to the online judges (Codeforces and LeetCode) where the problems came from, to get the system verdicts. Oracles with verdicts other than _Accepted_ (AC), _Time Limit Exceeded_ (TLE), and _Runtime Error_ (RE) were directly considered incorrect. For those with TLE and RE verdicts that could potentially be semantically correct despite exceeding the time limit or stack size, we hired an experienced competitive programming contestant to examine if they were semantically correct and not efficient enough. For LeetCode problems, as displayed in Figure 2, 88.5% of the oracles are semantically correct, including _all_ those with TLE verdicts. For codecontests problems, 72% of the oracles are semantically correct.

**Oracle-generated test cases have better quality.** We evaluate the quality of Algo's test cases generated by answering the following questions: (i) _Do verification verdicts from generated cases agree more with the system judge than verdicts from only the public tests?_ (ii) _Do the generated tests achieve higher statement coverage compared with public tests?_ In this context, we calculate two metrics: **Agreement** and **Coverage**. **Agreement** is the consistency between the decision of the system judge and the test case set. For instance, candidates that fail the system judge should also fail on the generated test set, while those that can pass the judge should also succeed on all test cases. **Coverage** is the percentage of statements in a code solution that are executed by test cases. Following

   Method & 1@\(k\) & Gain & 2@\(k\) & Gain & 10@\(k\) & Gain & 100@\(k\) & Gain \\   \\  Codex  & 0.70 & - & 1.20 & - & 3.00 & - & 7.50 & - \\ CodeT  & 2.10 & **+1.40** & 2.30 & **+1.10** & 5.30 & **+2.30** & 9.90 & +2.40 \\ Algo w/ Codex & **5.60** & **+4.90** & **5.60** & **+4.40** & **7.70** & **+4.70** & **11.10** & **+3.60** \\   \\  PG-TD* & 0.62 & - & 1.07 & - & 2.26 & - & 3.45 & - \\ PG-TD  & 0.67 & **+0.05** & 1.12 & **+0.05** & 2.53 & **+0.27** & 3.81 & +0.36 \\ Algo w/ PG-TD & **1.57** & **+0.95** & **2.44** & **+1.37** & **3.67** & **+1.41** & **4.06** & **+0.61** \\   \\  ChatGPT & 4.40 & - & 6.62 & - & 12.21 & - & - & - \\ Algo w/ ChatGPT & **12.00** & **+7.60** & **12.00** & **5.38** & **14.00** & **+1.79** & - & - \\   

Table 2: Performance on CodeContests. Gain is the performance improvement compared with the baseline (the first line in each part). Algo significantly outperforms other baselines, especially for 1@\(k\). Note that \(k=20\) for ChatGPT-based settings, while \(k=1000\) for other settings.

Figure 5: Performance on LeetCode. Code interpreter integrated with Algo can surpass GPT-4’s pass rate.

hen et al. , we use a public coverage library3 to gather the coverage results. As demonstrated in Table 3, our generated test cases substantially improve the agreement with the standard judge (+37.39% on CodeContests and +5.72% on LeetCode), indicating they can enhance the verification of candidates prior to submission. In addition, test cases of Algo achieve higher coverage of the statements, implying that these cases can more effectively validate the functionality of the solution.

**Better test cases lead to better results.** The test cases generated by Algo have better agreement and coverage. We check if they lead to better synthesis by comparing them to the other two sets of test cases - examples provided in the problem statement and test cases directly generated by ChatGPT. We used the same coder with the exact same sample budget \(k=20\). As reported in Table 4, under the same setting, ALGO tests led to much better performance.

**More test cases lead to better results.** We examine the impact of the size of test cases the verifier generates on CodeContests for Algo w/ Codex. Our verifier has the flexibility to create an arbitrary number of cases and use test case sets of varying sizes to rank and guide the synthesis. As shown in Figure 6, when the size is fewer than 30, the increase in the size of test cases can significantly bolster performance for all \(n@k\), indicating the effectiveness of our verifier in covering more corner cases with additional test cases. Notably, having a sufficient number of test cases is more critical for 1@\(k\), which relies on test cases to select the optimal solution for one-time submission. However, this improvement diminishes when \(n\) is larger. This is because a robust verifier is not strongly required when the chosen sample number \(n\) is large, as discussed in Section 4.2. Furthermore, there is very little additional performance gain with more than 30 test cases. We hypothesize that this is due to the quality of our oracle and the generation range we have set for the verifier.

### Case Study

We demonstrate the workflow of Algo with a problem from our LeetCode benchmark 4. This problem asks for the optimal allocation of cars to mechanics with varying efficiency to minimize the total repair time, when the mechanics would work simultaneously. As depicted in Figure 7, the code generated using a reference oracle prompt exhaustively enumerates all potential assignments of cars and selects the optimal one according to the problem description. Assuming \(n=(ranks),m=cars\), this program exhibits exponential time complexity \(T_{O}(n,m)=(2^{m})\), making it an inefficient solution that is unable to pass the online judge. Nevertheless, despite this program's inefficient nature, this implementation remains valuable in terms of semantic correctness (i.e., \(J_{T}(P_{O})=\) and \(J_{S}(P_{O})=\), where we can conveniently define \(J_{T}(P)=((n+m))\) " without loss). This example demonstrates how Algo can generate reference oracles for code synthesis.

    & 1@20 & 3@20 & 7@20 \\  Example Tests & 4.4\% & 7.9\% & 12.2\% \\ ChatGPT Tests & 6.8\% & 8.2\% & 11.7\% \\ ALGO Tests & **12.0\%** & **12.0\%** & **14.0\%** \\   

Table 4: Given the same coder, ALGO test cases lead to much better \(n@k\) performance, indicating that better test cases do help in code synthesis.

   Dataset & Case & Agreement & Coverage \\   & Example & 57.39\% & 85.78\% \\  & Algo & 94.78\% & 89.39\% \\   & Example & 68.57\% & 90.32\% \\  & Algo & 74.29\% & 93.00\% \\   

Table 3: The quality of test case. Compared to the example cases given in the problem description, the test cases generated by Algo’s verifier detect more failures in the programs, agree better with the system judge \(J\), and covers more lines in the generated code.

Figure 6: The performance of different sizes of test cases on CodeContests.

In Figure 7, we present how the reference oracle can verify the correctness of candidate programs generated by the coder LLM. We present two candidate programs generated with different algorithm categories: greedy and binary search. Specifically, the binary search candidate \(P_{B}\) is generated with instructions on binary search and the greedy candidate \(P_{G}\) is generated without explicit guidance. The binary search algorithm is ideally suited for this problem, given that the indicator function, denoted as \(f(t)\), which determines whether all cars can be repaired within \(t\) minutes, is monotonous. This makes it possible to apply binary search on the time \(t\) and turn the optimization problem into a judgement problem. The binary search candidate represents an implementation of such algorithmic approach, which is an efficient and correct solution of time complexity \(T_{B}(n,m)=(n m)\) and can pass the system judge (i.e., \(J(P_{B})=\)). On the other hand, greedy algorithm is not suitable for this problem. Thus, even though the greedy candidate presented is of time complexity \(T_{G}(n,m)=(n)\), the semantic of this candidate is wrong (i.e., \(J_{T}(P_{G})=\) and \(J_{S}(P_{G})=\)). This example demonstrates that providing the correct instruction can lead to better code synthesis result.

In Figure 7, \(P_{G}\) failed with our verifier since \(P_{G}(I_{0}) P_{O}(I_{0})\). Similarly, \(P_{B}(I_{0})=P_{O}(I_{0})\) indicates \(P_{B}(I_{0})\) passed our verifier. This verification result is consistent with the system judge and demonstrates that Algo-generated oracles can help to verify the programs generated. Furthermore, the verification results can help in finding the correct instruction (algorithm category).

## 5 Related Work

Reranking Techniques for LLM Code Generation.LLMs often need to sample many candidates before finding a single correct solution. Therefore, reranking techniques for telling the correct programs among many candidates are crucial for better LLM coders. One line of work in reranking involves neural models that predict some form of _confidence score_ for program candidates. This score can be the likelihood of the program , the mutual information between the code and the problem description , or outputs from verifier models that are purposely trained to predict the correctness of programs [22; 13]. However, their scores are just scalars and not interpretable. Both the oracles and test results from Algo are much easier to check and interpret.

Another line of work utilizes test cases and the programs' execution results to rerank them, which is much more similar to Algo's approach. Shi et al.  and Li et al.  cluster different candidates

Figure 7: Algo is capable of verifying two candidate programs, in different algorithm categories: greedy and binary search. Using an input-output example provided by the reference oracle, Algo is able to deliver the verification result consistent with the system judgment.

according to their execution results on example cases, creating a mechanism similar to majority voting. Chen et al.  and Key et al.  also cross-check programs and execution results, but they create more test cases with the coder models to make the tests stronger. However, LLM-generated test cases for a single problem can contain both correct and incorrect ones, providing harmful feedback to the coder. ALGO has converted the task of checking AI-generated programs to checking AI-generated brute-forces, which is much easier.

Oracles in Program Synthesis and Software Testing.In the traditional program synthesis domain, automatic verification of a synthesized program is possible [29; 28] because formal specifications, including input-output examples  or logical constraints , are explicitly provided. In contrast, oracle generation using LLMs is challenging due to the ambiguous nature of text and lack of formal specifications, making it difficult to do verification. Efforts have been made to create formal specifications from text [6; 15; 10], but reliability remains an issue. Meanwhile, in the software testing domain, test oracle generation [9; 18; 24] is a well-known and challenging task. However, existing work mainly focuses on generating regression tests, which are not intended for new bug detection. Moreover, many methods rely heavily on formatted documents or are designed for program crashes or basic functional properties, making them unsuitable for complex algorithmic problems.

Self-Refining Language Models.Large language models (LLMs) have exhibited a remarkable capability for self-analysis and self-improvement, as highlighted by numerous studies [19; 12; 30]. Such reflection is also used to improve the quality of the generated programs. For instance, Chen et al.  trains Large Language Models (LLMs) to generate explanations for code and utilizes both the explanation and the execution results as feedback to improve coding solutions. Self-Refine  prompts the LLM to provide feedback on the efficiency of its own code solutions and refine these solutions based on the feedback given. In this paper, we use the execution results of the reference oracle as feedback and employ them to guide the generation of programs.

## 6 Conclusion and Discussion

We proposed Algo, an algorithm synthesis framework that leverages LLM-generated oracles as verifiers to synthesize algorithmic programs. Algo consists of a coder and a verifier. The coder generates code solutions to the problem and requests verification from the verifier to iteratively refine its idea and its code. The verifier employs LLM-generated oracles to generate test cases for verification. Algo can employ different types of search strategies with different underlying code generation models. We introduced a method to synthesize the reference oracle with LLMs based on the ease to generate brute-force solutions for algorithmic problems.

We extensively evaluated Algo's performance on two different datasets with three baselines. Algo outperformed various baselines with different search strategies by a large margin. We also did a detailed analysis of the reference oracles generated on their semantic correctness, and their verdict agreement with the system judge. The oracles in Algo have a high accuracy and make highly consistent verification results with the system judge. The high quality of the oracles supports the performance of Algo.

While Algo is evaluated with several searchers we proposed in the paper, there can be more possibilities for applying different searchers in the Algo framework. For example, the self-refinement studies in large language models may be utilized for more complex interactions between the coder and the verifier. On the other hand, oracle-guided synthesis  is a well-studied topic in the programming language community with techniques that can be applied in different branches of code synthesis. We believe that knowledge from both the natural language processing community and the software engineering community can inspire more future work in the framework of Algo.