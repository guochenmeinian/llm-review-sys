# A Framework for Standardizing Similarity Measures

in a Rapidly Evolving Field

 Nathan Cloos

MIT

nacloos@mit.edu

&Guangyu Robert Yang

MIT

yanggr@mit.edu

&Christopher J. Cueva

MIT

ccueva@gmail.com

###### Abstract

Similarity measures are fundamental tools for quantifying the alignment between artificial and biological systems. However, the diversity of similarity measures and their varied naming and implementation conventions makes it challenging to compare across studies. To facilitate comparisons and make explicit the implementation choices underlying a given code package, we have created and are continuing to develop a Python repository that benchmarks and standardizes similarity measures. The goal of creating a consistent naming convention that uniquely and efficiently specifies a similarity measure is not trivial as, for example, even commonly used methods like Centered Kernel Alignment (CKA) have at least 12 different variations, and this number will likely continue to grow as the field evolves. For this reason, we do not advocate for a fixed, definitive naming convention. The landscape of similarity measures and best practices will continue to change and so we see our current repository, which incorporates approximately 100 different similarity measures from 14 packages, as providing a useful tool at this snapshot in time. To accommodate the evolution of the field we present a framework for developing, validating, and refining naming conventions with the goal of uniquely and efficiently specifying similarity measures, ultimately making it easier for the community to make comparisons across studies.

**Code:**https://github.com/nacloos/similarity-repository

Figure 1: **We register and standardize existing implementations of similarity measures to facilitate comparisons across studies. Dark blue entries are the similarity measures originally implemented in the GitHub repositories associated with their respective papers: [14, 2008a], , , , , , , . We aim to include all papers that provide code for similarity measures but here we show only a subset of the packages implemented in the full online repository. Light blue entries indicate additional measures that we can automatically derive from the original ones, thereby expanding the grounds for comparison.**Introduction

Similarity measures have become a cornerstone in evaluating representational alignment across different models (Komblith et al., 2019), different biological systems (Kriegeskorte et al., 2008; Kriegeskorte et al., 2008), and across both artificial and biological systems. Researchers have employed diverse methods to compare model representations with, for example, brain activity, aiming to identify models that exhibit brain-like representations (Yamins et al., 2014; Sussillo et al., 2015; Schrimpf et al., 2018; Nayebi et al., 2018). However, while these measures are actively used and provide an efficient way to compare structure across complex systems, it is not clear that they adequately represent the computational properties of interest, and there is a need to better understand their limitations (Cloos et al., 2024). The field lacks clear guidelines for selecting the most appropriate measure for a given context.

As the number of proposed similarity measures continues to grow, the landscape becomes more complex and fragmented. Reviews like Sucholutsky et al. (2023); Klabunde et al. (2023) attempt to summarize and categorize these measures, but they often do not address the full computational diversity or nuances of each method, which is not always apparent unless the code implementation is reviewed in detail.

To facilitate comparisons and make explicit the implementation choices underlying a given code package, we have created and are continuing to develop a Python repository that standardizes similarity measures. Our aims for the repository are:

1. **Register and standardize existing implementations:** We compile implementations from various papers and map them to a standardized naming convention that aims for **consistency**, **low naming complexity**, and **reflects the underlying mathematical structure**. Importantly, a naming convention that maps out the current space of similarity measures will not continue to be definitive as these measures and best practices continue to evolve. So our goal is not to advocate for a static naming convention but to provide a repository that usefully specifies current similarity measures, while also providing a framework for continuing to refine these naming conventions as the field evolves.
2. **Serve as a centralized reference:** The repository provides a centralized place where practitioners can easily find which methods each paper implemented and exactly how they compare across studies via quantitative comparisons.
3. **Facilitate development of new implementations:** By providing accessible code references, we make it easier for practitioners to write their own implementations and validate them through quantitative comparisons with existing ones.

We demonstrate the utility of our approach using Centered Kernel Alignment (CKA) as an illustrative example.

## 2 Methods

**Standardization procedure**. To develop our package we iterate through the following steps:

Figure 2: **Our standardized naming convention unifies existing variations of CKA measures. The convention is designed to reflect the underlying compositional structure by highlighting the kernel method, the HSIC estimator, and the scoring method as the three main components that constitute the various CKA measures. Standardized names separate these components with a dash symbol. See Appendix A.2 for details.**

1. **Collecting implementations:** Identify all similarity measures implemented in the GitHub repositories associated with relevant papers.
2. **Understanding interfaces:** Analyze the input and output interfaces for each implemented function and convert inputs to the expected format (e.g., transposing or centering data arrays if necessary).
3. **Mapping to standardized names:** Map each implemented function to a standardized name that reflects its mathematical components, starting with simple names and adding complexity as needed (see step 4).
4. **Validating consistency:** Test the consistency of outputs across implementations assigned the same name. If inconsistencies are found, refine the naming convention to capture necessary distinctions.

## 3 Results

**Standardizing CKA implementations**. As an illustrative example, we applied our standardization procedure to implementations of CKA from 5 different repositories associated with [Kornblith et al., 2019], [Tang et al., 2020], [Williams et al., 2021], [Lange et al., 2022], [Huh et al., 2024]. We found 12 different names were necessary to capture the distinct variations of CKA (see Figure 2 and Appendix A.2 for more details).

**Implementation choices can have large impacts.** Choices for CKA measures are not just subtleties of the implementation but can have a large impact on the final similarity score. For example, Figure 3 (left) shows the variability across CKA scores when two datasets are compared using different variations of CKA. All the variability is due to variations of CKA.

**Achieving both low complexity and consistency error of zero.** Figure 3 (left) demonstrates the variability in similarity values when evaluating different naming conventions on a single pair of datasets (see Appendix A.3 for details on the datasets). Out of the naming conventions considered, only our standardized convention and the "unique" naming convention - which assigns a unique name to each implementation - produce consistent similarity values for all measures sharing the same name.

Figure 3 (right) summarizes the consistency error across all datasets and measure names for each naming convention. Naming conventions with fewer names than the standardized one (lower complexity), such as when removing specifiers from the standardized names ("No hsic", "No kernel", "No scoring", "No hsic/kernel/scoring" in Figure 3), result in higher consistency error. This indicates that these conventions assign the same name to different measures that produce varying scores. The "unique" naming convention has a consistency error of zero but has a higher naming complexity than the standardized one (see Appendix A.4 for details).

Figure 3: **Our standardized naming convention achieves both low complexity and consistency error of zero.** (Left) The output of the similarity measure can vary widely if the name is not fully specified. All scores shown here are computed between the same two datasets and the variability derives solely from variations in CKA. (Right) Summary of the consistency error across all datasets and measure names for each naming convention. Naming conventions with fewer names (lower complexity) have higher error due to assigning the same name to measures that produce different scores (see Appendix A.4 for details).

Papers implement a small and distinct subset of the CKA measures.After standardizing the implementations from various studies, mapping them to a common set of names and validating their consistency, we visualize and compare the specific measures they implement (Figure 4).

Are all measures equivalent?Do papers really need to implement all the variants of CKA? Are the different measures just on different scales and need to have the right conversion, for example, a linear scaling to transform between them? No. See Figures 4 and 5.

## 4 Discussion

Comparing similarity scores across studies is challenging, primarily due to variability in naming and implementation conventions. As part of our contribution to the research community we have created, and are continuing to develop, a Python repository that benchmarks and standardizes similarity measures. The ideal naming convention for similarity measures should flexibly evolve to incorporate new similarity measures and adapt as we change our best practices. To facilitate this evolution, we outline our framework for developing, validating, and refining naming conventions to standardize implementations. We hope our work is a step towards building tools for more reproducible and integrative science.

Figure 4: **Our standardized repository allows users to compare similarity scores across studies and derive new variations that were not initially implemented.** (Top row) Each colored dot shows the variations of CKA that were implemented in the listed studies, and summarizes the relationships when comparisons are made across multiple datasets (Supplementary Figure 9) visualized in 2D using multidimensional scaling. Many studies, and corresponding code packages, only implement a distinct subset of the similarity measures, making it challenging to compare results across studies. (Bottom row) Colored dots show the originally implemented CKA variations plus the variations that can be derived. Not all variations are derivable from each individual study (see Appendix A.6 for details).

Figure 5: **Are variations of CKA capturing the same information? No. Each plot shows comparisons between two variations of CKA across multiple datasets. The lack of consistency suggests the different variations provide different windows into the similarity between datasets. Gray line shows y=x for comparison.**