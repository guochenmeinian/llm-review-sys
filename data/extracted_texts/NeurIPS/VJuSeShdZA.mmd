# Hidden in Plain Sight: Evaluating Abstract Shape Recognition in Vision-Language Models

Arshia Hemmat

University of Oxford

&Adam Davies\({}^{*}\)

University of Illinois Urbana-Champaign

&Tom A. Lamb\({}^{*}\)

University of Oxford

&Jianhao Yuan\({}^{*}\)

University of Oxford

&Philip Torr

University of Oxford

&Ashkan Khakzar

University of Oxford

&Francesco Pinto

University of Oxford

###### Abstract

Despite the importance of shape perception in human vision, early neural image classifiers relied less on shape information for object recognition than other (often spurious) features. While recent research suggests that current large Vision-Language Models (VLMs) exhibit more reliance on shape, we find them to still be seriously limited in this regard. To quantify such limitations, we introduce IllusionBench, a dataset that challenges current cutting-edge VLMs to decipher shape information when the shape is represented by an arrangement of visual elements in a scene. Our extensive evaluations reveal that, while these shapes are easily detectable by human annotators, current VLMs struggle to recognize them, indicating important avenues for future work in developing more robust visual perception systems. The full dataset and codebase are available at: https://arshahemmat.github.io/illusionbench/

## 1 Introduction

Deep neural networks have accomplished remarkable breakthroughs in visual recognition over the past decade (Krizhevsky et al., 2012; He et al., 2016; Dosovitskiy et al., 2020; Radford et al., 2021; Gemini Team et al., 2023); but these models have also shown longstanding, fundamental limitations - for instance, the performance of these models degrades when faced with common corruptions and perturbations (Hendrycks and Dietterich, 2019), or natural out-of-distribution data (Hendrycks et al., 2021). How can we facilitate more robust neural vision models? A natural place to begin is by considering the source of robustness in human vision. Human object recognition is largely based on shape perception (Landau et al., 1988; Biederman and Ju, 1988; Xu et al., 2004; Baker and Kellman, 2018), which is essential to the robustness of human vision due to the invariance of shape to common transformations such as translation, rotation, scaling, and changes in illumination, color, and texture (Kendall, 1984; Hummel, 2001; Ommer, 2013; Dryden and Mardia, 2016). As such, substantial work in computer vision has focused on improving and evaluating shape perception (e.g., Ritter et al., 2017; Geirhos et al., 2019; Islam et al., 2021; Geirhos et al., 2021; Gavrikov et al., 2024; _inter alia_), finding that early deep vision models relied much more on texture than shape in image classification (Geirhos et al., 2019; Islam et al., 2021; Pinto et al., 2022; Benarous et al., 2023; Subramanian et al., 2023), which is believed to contribute to their lack of robustness (Geirhos et al., 2020; Gavrikov et al., 2024). Later work observed that vision encoders trained with larger-scale data weakly supervised by language (e.g., CLIP; Radford et al., 2021) show improvements in shape recognition (Geirhos et al., 2021; Gavrikov et al., 2024).

While clear indicators of progress in visual perception of neural vision models, it is important to note that all of the above studies on shape recognition in vision models have relied on two standard datasets, Cue Conflict and Stylized-ImageNet (Geirhos et al., 2019), which presents severalconcerns - for instance, these datasets do not include coherent, naturalistic visual scenes; they are built using legacy style transfer techniques that damage shape information and prevent the reproduction of fine-grained textures; and each image includes only a single object class represented as an abstract shape using perceptually uniform textures (see Section 2 for a more detailed critique). To address these limitations, we introduce IllusionBench,1 which represents shape information by an arrangement of visual elements existing in coherent, naturalistic scenes (see Figure 1). We evaluate vision-language models (VLMs) using IllusionBench in three scenarios: (1) measuring **zero-shot** performance of generative VLMs (e.g., LLava , GPT-4o , and Gemini ); (2) measuring **few-shot** performance of VLMs using in-context learning (e.g., ); and (3) **fine-tuning** contrastive VLMs (e.g., CLIP ) to recognize abstract shapes and testing their ability to generalize to unseen scenes. We find that, while human annotators can easily identify these shapes, VLMs struggle to identify shapes and instead focus on the scene components, failing to exhibit the abstract shape recognition capabilities that are essential for enabling humanlike visual robustness.

## 2 Background and Related Work

**Shape perception and visual recognition** Shape information is widely considered to be the most important cue leveraged by the human visual system for object recognition . Our ability to perceive shapes is crucial in enabling the robustness of human visual perception , as shape is invariant to key transformations such as translation, rotation, scaling, and changes in illumination, color, and texture . Thus, many works have investigated the extent to which neural object classifiers rely on shape for visual recognition tasks, finding that early supervised deep neural networks rely more on texture cues rather than shape . More recently, Gavrikov et al.  showed that multimodal vision-language models can be prompted to rely more on shape in visual recognition. Each of these works evaluates shape perception on the basis of the Cue Conflict (CC) or Stylized-ImageNet (SIN) benchmarks ; but despite their longstanding utility, we observe several key limitations with these benchmarks:

1. **Lack of coherent, naturalistic, and complex visual scenes:** Images contain only the shape of a single class mixed with a single texture applied uniformly to the entire image.
2. **Missing shape information:** Key shape information is often lost, yielding "a substantial fraction" of images that are unrecognizable to human annotators . The contrast in textures between the object and the background of any given image is usually lost, yielding perceptually uniform images .

Figure 1: **Can vision-language models recognize these shapes? IllusionBench dataset contains images in which scene elements are arranged to represent abstract shapes.**

3. **Low-quality style transfer:** The style transfer methods in these datasets (Gatys et al., 2016; Huang and Belongie, 2017) are known to confuse shape and texture information (Wang et al., 2023) and often fail to capture fine-grained textures (Wang et al., 2021).

To address these limitations, we introduce IllusionBench, which leverages state-of-the-art generative models to create images representing shape information with a complex arrangement of elements in detailed visual scenes comprised of various textures and objects.

**Evaluating visual capabilities of VLMs** Vision-language models (VLMs) have exceeded conventional benchmarks, often even exhibiting capabilities that they are not explicitly trained for (Bubeck et al., 2023) and underscoring the need for new forms of evaluation (Zhang et al., 2024). Traditional image recognition benchmarks are not designed to characterize such capabilities, indicating the need for innovative evaluations. For instance, Bitton-Guetta et al. (2023) studies commonsense visual reasoning by testing whether models perceive peculiar content in visual scenes; Fu et al. (2024) evaluates VLMs on recognizing the count of objects, relative positions of objects, OCR, and commonsense visual reasoning; and Tong et al. (2024) proposes visual tasks requiring fine-grained understanding of object orientation, perspective, and the states of objects in the image. Finally, Zhou et al. (2023); Li et al. (2023) focus on limitations specific to generative VLMs, such as visual hallucination.

## 3 Benchmark Description

### Generative Process and Notation

Consider the set \(=\{(x_{i},c_{i})\}_{i=1}^{||}\) of binary shape conditioning images \(x_{i}\) representing the shapes of corresponding object class \(c_{i}\), and \(=\{(s_{j})\}_{j=1}^{||}\) is the set of prompts where each \(s_{j}\) describes a different scene (e.g., Ocean or Medieval Village). To synthesize our dataset, we use ControlNet (Zhang et al., 2023), a module that is trained to control the generative process of text-to-image diffusion models (such as Stable Diffusion; Rombach et al., 2022) by conditioning on inputs specifying spatial information to guide the generative process, such as our shape conditioning images \(x_{i}\) (refer to Figure 2 for an overview). The pipeline (Figure 2) transforms the tuple \((x_{i},s_{j})\) into an image \(x_{ij}\) representing the considered shape \(x_{i}\) of class \(c_{i}\) embedded in a scene of type \(s_{j}\).2 We therefore obtain our datasets by creating a tuple \((x_{ij},c_{i},s_{j})\) for each combination of conditioning images and prompts. We then consider three predictive tasks a VLM \(f\) should perform (where \(p_{C},p_{S},\) and \(p_{C,S}\) represent prompts querying for \(c_{i},s_{j}\), or both, respectively):

1. \(_{C}\): predict the shape \(c_{i}=f(x_{ij},p_{C})\).
2. \(_{S}\): predict the scene \(s_{j}=f(x_{ij},p_{S})\).
3. \(_{C,S}\), predicting both the shape and the scene \((c_{i},s_{j})=f(x_{ij},p_{C,S})\).

### Dataset Details

As exemplified in Figure 2, the IllusionBench benchmark contains three different constituent datasets: IllusionBench-IN, IllusionBench-LOGO, and IllusionBench-ICON. The number of

Figure 2: **Dataset generation. For each of the 3 datasets in IllusionBench, we show an example image from the dataset alongside an example scene prompt and an example shape conditioning image used to generate it. A shape image \(x_{i}\) (with the class name \(c_{i}\)) and a scene description \(s_{j}\) are combined to generate the IllusionBench image \(x_{ij}\).**

samples, classes, conditioning images, and domains for each dataset are provided in Table 1 (with more detailed metadata available in Appendix B).

IllusionBench-INWe build upon the 16 classes from the most popular shape perception benchmark, Stylized-ImageNet (SIN) [Geirhos et al., 2019]. However, since we are interested in how well models can find shapes within a scene, we need clear and distinct shapes that can be identified unambiguously. To address this, we replace 4 of the 16 SIN classes with similar categories (near co-hyponyms) with more distinct shapes. We collect 3 conditioning images for each class.

IllusionBench-LOG0Another category of shapes that are specifically designed to be visually distinct and easily recognizable are logos, which provide an interesting contrast to the shapes in IllusionBench-IN, as recognizing them requires world knowledge specific to the category of product brands (rather than culturally-nonspecific real-world object classes).3 Thus, we expand our dataset to this domain by collecting 39 different logo conditioning images across 21 brands.

IllusionBench-LOG0Finally, we develop a third dataset to test whether VLMs can be trained to recognize cross-modal abstractions over perceptually distinct shapes representing semantically related concepts (e.g., where images representing shapes of owls or turtles are both recognized as instances of the "animal" class, despite having very different shapes). We create a coarse-grained dataset of 6 (informal) hypernym categories across 456 emojis as shape conditioning images.

Validating Dataset QualityAlthough ground truth labels for object classes and scene types are available, image generators may sometimes produce low-quality or high-difficulty images whose object shape is not human-recognizable. To minimize the proportion of such images, we begin by restricting the hyperparameters that control the influence of the conditioning image to ranges that we qualitatively found to produce clearly distinguishable shapes (see Appendix B.2). To validate that the shapes in the resulting images are indeed human-recognizable, we recruited 60 participants (information is anonymized) to manually annotate randomly sampled subsets of IllusionBench-IN, IllusionBench-LOG0, IllusionBench-ICON, obtaining an average annotator accuracy of 95.6%, 97.17% and 96.8%, respectively, indicating that humans are indeed able to recognize the shapes in the vast majority of the generated images.4 (See Appendix B.1 for further details.)

### Evaluation

Given image \(x_{ij}\), we prompt VLM \(f\) with both \(x_{ij}\) and prompts \(p_{k}\) corresponding to the shape, scene, and both the shape and scene (i.e., where \(p_{k}\) is variously \(p_{C},p_{S}\), or \(p_{C,S}\), respectively), yielding responses \(r_{k}=f(x_{ij},p_{k})\) for each prompt \(p_{k}\). For each \(x_{ij}\), we evaluate _shape recall_ on the basis of whether the term \(c_{i}\) appears in the response \(r_{C}\) or \(r_{C,S}\) (yielding \(1\) if so, or \(0\) if not), and evaluate _scene recall_ by whether \(s_{j}\) appears in \(r_{T}\) or \(r_{C,S}\) (similarly yielding \(1\) or \(0\)), and report the shape and scene recall for each dataset as the sum of the recall figures across all \(x_{ij}\) instances divided by the size of each dataset. In contrast to prior related works (e.g., Geirhos et al. 2019, 2021, Gavrikov et al. 2024), our proposed metrics are designed such that shape recognition performance is not in competition with the ability to recognise other visual elements (e.g., textures or scene elements), as - unlike traditional classifiers, which must select only one among a pre-defined set of discrete classes

   Dataset Name & \# Samples & \# Classes & \# Conditioning Images & \# Scenes \\  IllusionBench-IN & 6864 & 16 & 48 & 11 \\ IllusionBench-LOG0 & 5577 & 21 & 39 & 11 \\ IllusionBench-ICON & 20064 & 6 & 456 & 11 \\   

Table 1: Size of each dataset in IllusionBench.

### Experimental Overview

In the following sections, we evaluate the shape perception capabilities of modern VLMs on IllusionBench under the following paradigms:

* **Zero-Shot Recognition**: Given that an instruction-tuned VLM can recognise a shape \(x_{i}\), can it identify the same shape when it emerges from the combination of visual elements in \(x_{ij}\) without any explicit examples or specialized fine-tuning? (Section 4)
* **Few-Shot Learning**: Given that a multi-modal in-context learner can recognise a shape \(x_{i}\) zero-shot, can it leverage few examples to learn to identify it in \(x_{ij}\)? (Section 5)
* **Domain Generalization**: Given training samples \(\{x_{ij}\}\) representing a shape \(x_{i}\) in certain types of scenes, can models learn to recognise the same shape in other, unseen scene types? (Section 6)

## 4 Can Instruction-Tuned VLMs Recognize Shapes Zero-Shot?

Experimental DesignIn this experiment, we prompt VLMs zero-shot to identify the abstract shape represented in a visual scene among a closed set of object classes. We begin by testing whether models can correctly classify the shape conditioning images (binary shape images), and generate images for IllusionBench exclusively using these condition shapes. We then prompt models with respect to the shape and scene in each generated image, and measure the corresponding recall metrics as described in Section 3.3. (See Appendix C for additional details regarding the experimental design, prompts, and models used in this experiment.)

ModelsWe consider the following VLMs for evaluation: GPT-4o [OpenAI, 2023], Gemini-Flash [Gemini Team et al., 2023], LLAVA1.5/6-7/13b [Liu et al., 2024c], CogVLM [Wang et al., 2024], BLIPv2-t5 [Li et al., 2023c], InstructBLIP-7/13b [Dai et al., 2024], Qwen-VL-Chat [Bai et al., 2023], and MoE-StableLM/Qwen/Phi2 [Lin et al., 2024].

ResultsOur main findings in this experiment (visualized in Figure 3) are as follows:

* For each of our datasets, shape recall is quite low, with most models ranging between 10-30% (in contrast to the previous dataset, Stylized-ImageNet [Geirhos et al., 2019], where all fourteen models exceed 30%).
* For nearly all models and datasets, models exhibit superior scene recall relative to shape recall. This indicates that the recognition capacity of current VLMs is still biased towards scene/texture features, similar to earlier work studying CNN classifiers (see Section 2).
* GPT-4o and GEMINI show superior shape recall to all other models in 3/3 and 2/3 of our datasets, respectively, demonstrating a shape-recognition gap between the best available open- and closed-source VLMs.

Figure 3: **Zero-Shot Results**. Average shape and scene recall of VLMs across each IllusionBench dataset, compared with Stylized-ImageNet [Geirhos et al., 2019] (rightmost, shaded).

* Mixture of Experts (MoE) (like MoE-StableLM, MoE-Qwen, and MoE-Phi2), which are generally employed to improve models' performance, exhibit neither superior shape nor scene recall with respect to individual models or closed-source models.
* Among all open source models, LLava attains the strongest shape recall performance across all our datasets. In contrast, Blipv2 attains the highest scene recall (except for the IllusionBench-LOGO split).

See Appendix C for more fine-grained results and analysis.

## 5 Can In-Context Learners Learn to Identify Abstract Shapes?

Given zero-shot prompting exhibits poor performance at detecting abstract shapes and shows VLMs mostly focus on background stimuli, a natural question is whether it is possible to teach models to recognise known shapes with a few samples by leveraging their In-Context Learning (ICL) or few-shot capabilities.5

**Experimental Design** We restrict our experiments to samples generated from conditioning images \(x_{i}\) that models can correctly classify in a zero-shot fashion (see Appendix D.2). Let us focus on the predictive task \(_{C}\) (as analogous formulations of ICL apply for \(_{S}\) and \(_{C,S}\)). Given that the model can correctly assign the class \(c_{i}\) to the conditioning image \(x_{i}\), we provide it with the context sequence \(\{(x_{i_{w},j_{w}},c_{i_{w}})\}_{w=1}^{|W|}\), where \(W\) is the context window plus a test image \(x_{i_{*},j_{*}}\), and prompt the model to predict the object's shape \(c_{i_{*}}\).

Using specific constraints on context sampling relative to a test sample, we define four learning tasks corresponding to perceptual challenges:

* **ICL1**: _Given the context lacks any image depicting the scene or shape type of the test sample_ \(x_{i,j}\)_, can the model recognize its shape_ \(c_{i}\)_?_
* **ICL2**: _Given the context includes an image of the shape type but not the scene type of the test sample_ \(x_{i,j}\)_, can the model recognize its shape_ \(c_{i}\)_?_
* **ICL3**: _Given the context includes an image of the scene type but not the shape type of the test sample_ \(x_{i,j}\)_, can the model recognize its shape_ \(c_{i}\)_?_
* **ICL4**: _Given the context includes images of the scene type and shape type of the test sample_ \(x_{i,j}\) _(separately and exactly once), can the model recognize the test sample's shape_ \(c_{i}\)_?_

Samples in the context are selected uniformly at random, excluding those that do not satisfy the constraints for a given test sample. Random selection serves as a simple baseline for ICL example selection, avoiding confounding factors like similarity bias or majority . We perform \(0,1,2,4,8\)-shot on IllusionBench-LOGO and IllusionBench-IN, and \(1,2,4,5\)-shot on IllusionBench-ICON. Further details of ICL experiments can be found in Appendix D.2. We additionally perform ablations to examine the sensitivity of our results to the prompt template used or to the order in which in-context examples are given to the model. These additional results can be found in Appendix D.9.

**Models.** We consider several state-of-the-art models that have been designed to support ICL: (1) LLaVA-Next , (2) Qwen-VL-Chat , (3) Otter-MPT , (4) IDEFICS-9B-Instruct , and (5) MMICL-T5-XXL. (We describe each models, the prompts they are provided, and a detailed motivation for selecting these particular models in Appendix D.3.)

Figure 4: **ICL Learning Tasks. Figure depicting the four ICL learning tasks, \(ICL1,ICL2,ICL3\) and \(ICL4\), defined by constraints on demonstration example selection as introduced Section 5.**

**Results.** We summarise the average results across all three dataset splits for \(0,1,2\) and \(4\)-shot ICL as show in Figure 5 following the recall metrics introduced in Section 3.3 (for results for individual datasets and for \(5\)-shot and \(8\)-shot performance on IllusionBench-ICON and IllusionBench-IN/IllusionBench-LOGO respectively, see Appendix D.7.) 6 We report here the main trends in the data. Discussion of exceptions that do not follow the reported general trends can be found in Appendix D.6.

* _ICL does not mitigate tendency to predict scene over shape._ As shown in Figure 5, ICL has minimal effect in altering the models' tendency to predict the scene \(s_{j}\), regardless of whether the prediction task is \(_{C}\) (predict shape), \(_{S}\) (predict scene), or \(_{C,S}\) (predict both).
* _On average,_ MMICL-t5-XXL _exhibits the strongest scene and shape recall for the highest number of shots_ (i.e., when majority voting biases decay; see [Bertini Baldassini et al., 2024]).
* _Increasing the number of shots has mixed effects on performance._ We observe in Figure 5 that the models often exhibit non-monotonic performance trends for both shape and scene recall across all prediction tasks and demonstration selection constraints. In general, this indicates that the models struggle in general to adapt to tasks \(_{C}\), \(_{S}\), and \(_{C,S}\), even with increasing demonstration examples. These results are in line with previous findings that complex ICL tasks remain challenging for current visual language models (VLMs) [Zong et al., 2024].
* _Context selection strategy effects prediction tasks differently._
* \(_{C}\) _(shape prediction):_ As shown in the top row of Figure 5 for task \(_{C}\) (shape prediction), including the shape in the context (ICL2 and ICL4) either maintains or reduces performance for most models such as MMICL and IDEFICS. This suggests that most models struggle to identify and disentangle shape from the scene through ICL.
* \(_{S}\) _(scene prediction):_ The second row of Figure 5 shows the mixed effect of including the scene within the context (ICL3 and ICL4) compared to not including it (ICL1 and ICL2). Models such as LLAVA, OTTER show a reduction in scene recall and when including the scene in the context. MMICL maintains comparable performance, whereas LLaVA and QWEN show improved performance.
* \(_{C,S}\) _(predicting both shape and scene):_ The final row of Figure 5 typically shows trends similar to \(_{C}\) and \(_{S}\) - e.g., scene recall and shape recall for MMICL (whose zero-shot shape recall is lower on this task than in \(_{C}\)), IDEFICS, and LLaVA are comparable with respect to those in \(_{C,S}\) and \(_{S}\) (respectively).

Figure 5: **ICL Results**. Few-shot (0,1,2 and 4-shot) shape and scene recall of VLMs averaged across the IllusionBench-LOGO, IllusionBench-IN and IllusionBench-ICON datasets, displayed for the different ICL learning tasks and the different prediction tasks.

Overall, we observe that ICL does not substantially aid models in learning to detect abstract shapes within scenes or to help reduce scene prediction bias. The non-uniformity of relative results between models further highlights the immaturity of ICL for multi-modal models, particularly for complex tasks like abstract shape recognition.

## 6 Can VLMs Learn Invariant Representations Across Domains?

A compelling application of IllusionBench-IN lies in Domain Generalisation (DG) (Gulrajani and Lopez-Paz, 2020). A visual domain is a set of samples with shared characteristics that influence the appearance of objects (e.g., shared style, such as cartoons, paintings, or photos; shared lighting conditions, such as photos taken at similar times of day with similar weather conditions; etc.). In DG, the goal is for models to learn domain-invariant representations - i.e., generalisable features that are predictive of task labels across any domain - by training across multiple "source" domains and testing how well models generalise to unseen test domains. (See Appendix E.1 for a more detailed introduction to DG.)

**Experimental Design.** We consider all images generated using the same scene prompt \(s_{j}\) as coming from the same domain \(^{j}\). As shown in Figure 6, we partition the IllusionBench-IN dataset split into train domains \(s_{j}\{\)Cloud, Forest, Ocean, Origami, Sand Dune\(\}\) and test domains \(s_{j}\{\)Bazaar Market, City, Medieval Village, Museum, Times Square, Underwater\(\}\).

(Conditioning images \(x_{i}\) used to generate the training domains are not contained in the test domains.) We then consider a contrastive language-vision encoder (CLIP (Radford et al., 2021)) and prompt CLIP in order to identify the class \(c_{i_{*}}\) of a test sample \(x_{i_{*}}\) among all possible shape classes7. Throughout the experiment, we use "A photo of \(\{\)class_name\(\}\)" as the prompt template. (See Appendix E.2 for further experimental details.)

**Methods Considered.** We compare various domain generalisation methods including ERM, MixUp (Yan et al., 2020), RegMixUp (Pinto et al., 2022), GroupDRO (Sagawa et al., 2019), and VREx (Krueger et al., 2021), using both linear probing and full-parameter finetuning. Besides linear probing, we also consider DPLCLIP (Zhang et al., 2023), a prompt optimization approach specifically designed for CLIP domain generalisation.

**Results.** We summarise our findings (reported in Figure 7) as follows:

* _CLIP cannot recognise shapes well in a zero-shot setting._ The CLIP model attains on average extremely low performance in zero-shot settings, with the exception of the Museum domain. This can be attributed to the fact that certain samples within this domain do not simply assemble \(c_{i}\) from visual cues of other objects, but incorporate it as a sculpture.
* _CLIP embeddings only partially capture shape information._ Applying prompt learning for domain generalisation via DPLCLIP is not particularly effective with an average test accuracy of 13.62%, and ERM results are more effective in improving over the zero-shot performance with accuracy 22.36%, outperforming all other probing techniques. However, the relatively low absolute values of accuracy indicate the embedding space does not render the test samples linearly separable based on shape criteria.

Figure 6: IllusionBench-IN **for Domain Generalisation.** We split the dataset into five source domains for training and six target domains for testing. The condition images for generated data samples are only shared among source and target domains, respectively, without overlapping.

- in all cases, a very large improvement is observed with respect to linear probing. The best performing methods are Mixup and RegMixup, which attain 71.79% and 73.00% on average accuracy, respectively.

## 7 Social Impact

The limited shape perception abilities of current vision systems, as highlighted in our work, could hypothetically be exploited by malicious users to, for instance, disseminate hateful or sensitive material online by bypassing inappropriate content visual filters that cannot recognize human-perceptible abstract shapes in scene elements (as enabled by the data-generation methodology we explore in this work). Conversely, improving perception ability could also aid censorship by moderators. In general, we anticipate that shape recognition capabilities on-par with generative techniques would empower platforms relative to users (e.g., for both content moderation and potential censorship), and shape recognition capabilities that are not able to recognize abstract shapes in outputs of leading generative techniques (as we observe in this work) empowers users relative to platforms, irrespective of whether content is legal or ethical.

## 8 Conclusion

We present IllusionBench, a collection of 3 datasets to evaluate shape recognition in vision-language models (VLMs) by representing abstract shapes as complex arrangements of visual scene elements. While human annotators identify these shapes with high accuracy, we find that state-of-the-art VLMs fail to identify the shapes in these scenes zero-shot, tending to focus on scene elements instead. We observe that in-context learning does not significantly improve models' ability to detect abstract shapes; but we do find that contrastive VLMs such as CLIP can be fine-tuned to recognize these shapes and generalize to new scene domains. In highlighting the limited shape perception abilities of current VLMs, we hope that IllusionBench will help guide future research in developing more robust computer vision systems. The contributions of each author are listed in Appendix A.

Figure 7: **Domain Generalisation. CLIP performance on IllusionBench-IN for different fine-tuning approaches. Each sub-figure represents an unseen test domain. The categories of approaches, with alternating shading from left to right to indicate these different categories, are: zero-shot prediction, prompt learning, linear probing, and full parameter fine-tuning.**