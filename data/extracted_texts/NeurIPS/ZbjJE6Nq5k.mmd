# Normalization and effective learning rates in reinforcement learning

Clare Lyle\({}^{}\)  Zeyu Zheng\({}^{}\)  Khimya Khetarpal\({}^{}\)  James Martens\({}^{}\)  Hado van Hasselt\({}^{}\)

&Razvan Pascanu\({}^{}\) &Will Dabney\({}^{}\)

###### Abstract

Layer normalization has demonstrated remarkable effectiveness at preventing plasticity loss in continual and reinforcement learning (RL), though the precise reasons for this effectiveness remain mysterious. In this work, we identify new mechanisms by which layer normalization can help - and hinder - training in neural networks, and leverage these insights to improve the robustness of gradient-based optimization algorithms to nonstationarity. Our analysis reveals a surprising ability of layer normalization to revive dormant ReLU units, along with an under-appreciated vulnerability to unconstrained decay of the effective learning rate (ELR), which can drive loss of plasticity in long-running nonstationary tasks. Motivated by these findings, we propose Normalize-and-Project (NaP), a simple protocol designed to provide the numerous benefits of normalization while ensuring that the effective learning rate remains constant throughout training. To do so, NaP couples the insertion of normalization layers with weight projection. This technique mitigates loss of plasticity in two challenging continual learning problems: a sequential supervised learning task, and a continual variant of the Arcade Learning Environment. Further, by using NaP to explicitly control the effective learning rate in deep RL agents, we find that in fact the implicit ELR decay induced by parameter norm growth in these agents is critical to their ability to achieve competitive performance, suggesting the common practice of using constant learning rates in deep RL may be far from optimal.

## 1 Introduction

Many of the most promising application areas of deep learning, in particular deep reinforcement learning (RL), require training on a problem which is in some way nonstationary. In order to perform well on a nonstationary problem, the neural network must maintain its ability to adapt to new information, i.e. it must remain _plastic_. Several recent works have shown that loss of plasticity can present a major barrier to performance improvement in RL and in continual learning (Dohare et al., 2021; Lyle et al., 2021; Nikishin et al., 2022). These works have proposed a variety of explanations for plasticity loss such as the accumulation of saturated ReLU units and increased sharpness of the loss landscape (Lyle et al., 2023), along with mitigation strategies, such as resetting dead units (Sokar et al., 2023) and regularizing the parameters towards their initial values (Kumar et al., 2023). Many of these explanations and their corresponding mitigation strategies center around reducing drift in the distribution of pre-activations (Lyle et al., 2024), a problem which has historically been resolved in the supervised learning setting, and more recently in continual learning and RL (Hussing et al., 2024; Ball et al., 2023), by incorporating normalization layers into the network architecture.

While effective, normalization on its own is insufficient to avoid loss of plasticity (Lee et al., 2023). Part of the reason for this lies in a subtle property of normalization: a normalization layer causes the subnetwork preceding it to become scale-invariant, which means that the layer's _effective learning rate_ (ELR) now depends on the norm of its parameters (Van Laarhoven, 2017). In particular, when the norm of the parameters grows, as it typically does in neural networks trained without regularization, the effective learning rate shrinks. Weight decay can address this problem, but runs the risk of either failing to fully mitigate the norm growth or over-regularizing the model to the point of slowing down learning and thus requires careful tuning, adding additional complexity to the training protocol.

The aim of this work is to provide a principled strategy for the use of layer normalization in non-stationary learning problems. To do so, we first identify two critical properties of layer normalization: its ability to facilitate the revival of dormant neurons, and its vulnerability to vanishing gradients as the parameter norm grows. In particular, we show that layer normalization buffers against dormant neurons due to its effect of mixing gradient signal between units. This analysis motivates Normalize-and-Project (NaP), a method which inserts normalization layers prior to each nonlinearity in a network architecture and maintains constant per-layer parameter norm. A network trained with the NaP protocol stands to benefit from the increased robustness to unit saturation provided by LayerNorm's gradient-mixing property, while also avoiding its vulnerability to vanishing gradients under growing parameter norms. Indeed, we observe empirically that NaP virtually eliminates loss of plasticity in multiple challenging non-stationary learning problems, including sequential training on games from the Arcade Learning Environment (ALE) (Bellemare et al., 2013; Abbas et al., 2023). We further confirm that NaP can be seamlessly integrated into standard computer vision and sequence modeling baselines from standard benchmarks without hindering performance.

We further leverage Normalize-and-Project as an analytical tool to explain the dramatic performance degradations induced by weight decay in deep RL agents. By making the effective learning rate explicit, NaP reveals that the implicit learning rate decay induced by parameter norm growth in Rainbow (Hessel et al., 2018) agents is in fact critical to their performance in the ALE benchmark: certain components of the value function require a sufficiently small ELR in order to be learned, and an optimization process which does not reach this value will therefore underfit the value function in ways that can inhibit performance improvement. Indeed, while we demonstrate that the implicit schedule induced by parameter norm growth outperforms a constant effective learning rate, an explicit schedule with more aggressive learning rate decay outperforms the implicit one on the ALE benchmark. These findings are at odds the common folk wisdom that the continual nature of RL makes it unsuitable for learning rate decay, and demonstrate the untapped potential of step size schedules to accelerate deep reinforcement learning.

## 2 Background and related work

We begin by providing background on trainability and its loss in nonstationary learning problems. We additionally give an overview of neural network training dynamics and effective learning rates.

### Training dynamics and plasticity in neural networks

Early work on neural network initialization centered around the idea of controlling the norm of the activation vectors (LeCun et al., 2002; Glorot and Bengio, 2010; He et al., 2015) using informal arguments. More recently, this perspective has been formalized and expanded (Poole et al., 2016; Daniely et al., 2016; Martens et al., 2021) to include the inner-products between pairs of activation vectors (for different inputs to the network). The function that describes the evolution of these inner-products determines the network's gradients at initialization up to rotation, and this in turn determines trainability (which was shown formally in the Neural Tangent Kernel regime by Xiao et al. (2020) and Martens et al. (2021)). A variety of initialization methods have been developed to ensure the network avoids "shattering" (Balduzzi et al., 2017) or collapsing gradients (Poole et al., 2016; Martens et al., 2021; Zhang et al., 2021).

Once training begins, learning dynamics can be well-characterized in the infinite-width limit by the neural tangent kernel and related quantities (Jacot et al., 2018; Yang, 2019), although in practice optimization dynamics diverge significantly from this limit (Fort et al., 2020). A number of beneficial phenomena emerge in the finite-width, finite-step-size regime, such as the self-stabilization of gradient descent (Lewkowycz et al., 2020; Cohen et al., 2021; Agarwala et al., 2022) and implicit regularization (Barrett and Dherin, 2020; Smith et al., 2020) as a result of the non-linear training dynamics. However, particularly in non-stationary learning problems, neural networks can also be vulnerable to _loss of plasticity_(Sodhani et al., 2020; Dohare et al., 2021; Nikishin et al., 2022; Abbas et al., 2023; Lyle et al., 2024) as they drift away from the initial parameters. This phenomenon has been shown to present a limiting factor to performance in a number of RL tasks (Igl et al., 2021; Lyle et al., 2021; Nikishin et al., 2023), along with continual learning and warm-starting neural network training (Berariv et al., 2021; Ash and Adams, 2020).

A variety of architectural choices can accelerate the training of extremely deep networks, including residual connections (He et al., 2016) and normalization layers (Ioffe and Szegedy, 2015; Ba et al., 2016); these methods have also been demonstrated to help networks maintain plasticity in reinforcement (Ball et al., 2023; Lyle et al., 2023) and continual (Kumar et al., 2023) learning. Some additional works have aimed to replicate the benefits of normalization layers via normalization of the network _parameters_(Salimans and Kingma, 2016; Arpit et al., 2016), though layer normalization (LayerNorm) remains standard practice in most domains. A variety of analyses highlight LayerNorm's effect on gradient moments as a critical factor in its efficacy (Xu et al., 2019; Xiong et al., 2020).

### Effective learning rates

As noted by several prior works (Van Laarhoven, 2017; Li and Arora, 2020; Li et al., 2020), normalization of the type performed by BatchNorm and LayerNorm introduces scale-invariance into the layers to which it is applied, where by a scale-invariant function \(f\) we mean \(f(c,)=f(,)\) for any positive scalar \(c>0\). This leads to the gradient scaling inversely with the parameter norm, i.e. \( f(c)= f()\). The intuition behind this property is simple: changing the direction of a large vector requires a greater perturbation than changing the direction of a small vector. This motivates the concept of an 'effective learning rate', which provides a scale-invariant notion of optimizer step size. In the following definition, we take the approach of Kodryan et al. (2022) and assume an implicit'reference norm' of size 1 for the parameters.

**Definition 1** (Effective learning rate).: _Consider a scale-invariant function \(f\), parameters \(\) and update function \(_{t+1}_{t}+ g(_{t})\) for some update function \(g\). Letting \(=\), we then define the effective learning rate \(\) as follows:_

\[=^{2},&g(_{t})=_{ }f(_{t})\\ ,&g(_{t})=f(_{t})}{\| _{}f(_{t})\|}\] (1)

_where, letting \(=\) we then have \(f(+g())=f(+ g())\)_

Thus by reducing the parameter norm, weight decay can have the dual effect of increasing the effective learning rate (Van Laarhoven, 2017; Hoffer et al., 2018), a property which has been extensively analyzed (Arora et al., 2018; Li and Arora, 2020; Li et al., 2020). This perspective justifies the decoupling of weight decay and gradient accumulation in AdamW (Loshchilov and Hutter, 2019), along with the application of learning rate schedules to the weight decay parameter (Xie et al., 2024). It also motivates scaling the norm of the updates to be proportional to the parameter norm in a variety of optimizers (Liu et al., 2021; You et al., 2017, 2020). The perspective of updates as rotations of the parameters has been applied by recent analyses of the equilibrium dynamics of optimization in scale-invariant networks trained with weight decay (Wan et al., 2021; Kosson et al., 2024). More recently, the work of Lobacheva et al. (2021) and Kodryan et al. (2022) has studied the training properties of scale-invariant networks trained with parameters constrained to the unit sphere, a training regime we expand upon in this work. A similar approach, referred to as weight standardization, has been demonstrated to reduce the need for normalization layers in diffusion models (Karras et al., 2024).

## 3 Analysis of normalization layers and plasticity

Although widely used and studied, the precise reasons behind the effectiveness of layer normalization remain mysterious. In this section, we provide some new insights into how normalization can help neural networks to maintain plasticity by facilitating the recovery of saturated nonlinearities, and highlight the importance of controlling the parameter norm in networks which incorporate normalization layers. We leverage these insights to propose Normalize-and-Project, a simple training protocol to maintain important statistics of the layers and gradients throughout training.

### Normalization and ReLU revival

It is widely accepted that achieving approximately mean-zero, unit-variance pre-activations (assuming suitable choices of activation functions) is useful to ensure a network is trainable at initialization (e.g Martens et al., 2021), and many neural network initialization schemes aim to maintain this property as the depth of the network grows. Indeed, it is easy to show that in extreme cases large deviations of these statistics from their initial values can lead to a variety of network pathologies including saturated units and low numerical rank of the empirical neural tangent kernel (Xiao et al., 2020). Layer normalization not only guarantees that activations are unit-norm, mean-zero at initialization, but also that they stay that way over the course of training even if the data distribution changes, assuming no scale or offset parameters.

Beyond re-normalizing the pre-activation statistics, layer normalization also introduces a dependency between units in a given layer via the mean subtraction and division by standard deviation transformations, which translates to correlations in the gradients of their corresponding weights. This mixing step allows gradients to propagate through a pre-activation even if the unit is saturated, provided layer normalization is applied prior to the nonlinearity, a property we highlight in Proposition 1. We will use the notation \(f_{}(h)=\) to specify the RMSNorm transform, and let \(f^{}(x)=f(x)\) refer to the scalar derivative of any \(f\) at scalar \(x\).

**Proposition 1**.: _Consider two indices \(i\) and \(j\) of a feature embedding \((f_{}(h))\) such that \(^{}(f_{}(h)_{j}) 0\), and \(h_{i},h_{j} 0\). Then we have_

\[}(f_{}(h))_{j}=-^{}(f_{} (h)_{j})}h_{i}h_{j} 0\.\]

_In contrast, for post-activation normalization the gradient is zero whenever \(^{}(h_{i})=0\), i.e._

\[^{}(h_{i})=0}f_{}((h))_{j}=- ^{}(h_{i})}(h_{i})(h_{j})=0.\] (2)

In other words, normalization effectively gives dead ReLU units a second chance at life - rather than immediately decaying to zero, the gradients propagated to the incoming weights of a saturated ReLU will take on non-zero values, which depend on the gradients of the mean and variance of that particular layer. These gradients will be much smaller than those that would typically backpropagate to the unit, but if an optimizer such as Adam or RMSProp is used to correct for the gradient norm, then the unit may still be able to take nontrivial steps, which have a chance at propelling it back into the activated regime. This property is also naturally inherited by layer normalization, which can be viewed as the composition of RMSNorm with a centering transform. An illustration of normalization allowing the network to revive dead units is given in Figure 1. We include the full derivation of Proposition 1 in Appendix A.2, and we demonstrate the effect this can have on a theoretical model of neural network training in Appendix C.4. However, while layer normalization can help the network to recover from saturated nonlinearities, it introduces a new source of potential saturation which must be carefully considered, which is something we will do in the next section.

Figure 1: Accumulation of dead units in an iterated random label memorization task. The network is trained to memorize random labels of the MNIST dataset which are re-randomized every 1000 optimizer steps. Networks with normalization layers are able to recover from spikes in the number of dead units.

### Parameter norm and effective learning rate decay

While the _output_ of a scale-invariant function is insensitive to scalar multiplication of the parameters, its _gradient_ magnitude scales inversely with the parameter norm. Consequently, growth in the norm of the parameters corresponds to a decline in the network's sensitivity to changes in these parameters. In a sense this is preferable, as the glacially slow but stable regime of vanishing gradients is easier to recover from than the unstable exploding gradient regime. However, if the parameter norm grows indefinitely then the corresponding reduction in the effective learning rate will eventually cause noticeable slowdowns in learning - indeed, we show in Figure 2 that it is quite easy to induce this type of situation. To do so, we take a small base convolutional neural network architecture (detailed in Appendix B.4) and train it on random labels of the CIFAR-10 dataset, akin to the classic setting of Zhang et al. (2021). We then re-randomize these labels and continue training, repeating this process 500 times. When we apply this process to a network without normalization layers, the Jacobian norm grows to unstable values as the parameter norm increases; in contrast, an equivalent architecture with normalization layers sees a sharp decline in the Jacobian norm as the parameter norm increases. In both cases, the end result is similar: increased parameter norm accompanies reduced performance on new tasks.

While this particular problem is artificial, it is a real and widely observed underlying phenomenon that the magnitudes of neural network parameters tend to increase over the course of training (Nikishin et al., 2022; Abbas et al., 2023). In a supervised learning problem, where one is using a fixed training budget, the ELR decay induced by growing parameter norms might be desirable and help to protect against too-large learning rates (Arora et al., 2018; Salimans and Kingma, 2016). Allowed to continue to extremes, however, ELR decay becomes problematic (Lyle et al., 2024). Fortunately, this problem admits an obvious solution: re-scaling the parameters to have nontrivial ELR. Since LayerNorm induces scale-invariance, this will not change the function computed by the network, but will change its training dynamics. We demonstrate the utility of this strategy in Figure 2.

### Normalize-and-Project

We conclude from the above investigation that normalizing a network's pre-activations and fixing the parameter norm presents a simple but effective defense against loss of plasticity. In this section, we propose a principled approach to combine these two steps which we call Normalize-and-Project. Our goal for NaP is to provide a flexible recipe which can be applied to essentially any architecture, and which improves the stability of training, motivated by but not limited to non-stationary problems. Our approach can be decomposed into two steps: the **insertion of normalization layers** prior to nonlinearities in the network architecture, and the **periodic projection** of the network's weights onto a fixed-norm radius throughout training, along with a corresponding update to the per-layer learning rates into the optimization process. Algorithm 1 provides an overview of NaP.

**Layer normalization.** in order to benefit from the robustness to saturated nonlinearities outlined in Proposition 1, we propose adding layer normalization prior to every nonlinearity in the network. While it might seem extreme, this proposal is not too far removed from standard practice. For example, Vaswani et al. (2017) apply normalization after every two fully-connected layers, and recent results suggesting that adding normalization to the key and query matrices in attention heads (Henry et al., 2020) can improve performance and the robustness of optimization (Wortsman et al., 2023).

Figure 2: Continual random-labels CIFAR training: simple feedforward network architecture (No Norm) exhibits rapid growth in its parameter norm and the norm of its gradients, whereas the otherwise-identical network with layer normalization sees parameter norm growth coupled with a _reduction_ in the norm of its gradients and reduced performance on later tasks. Constraining the parameter norm of this network maintains the performance of a random initialization.

**Weight projection.** As discussed in Section 3.2, once we have incorporated layer normalization we must take care to ensure that these normalization layers will not saturate, i.e. that the network's effective learning rate will not decay to zero over the course of training due to growth of the parameters. We propose disentangling the parameter norm from the effective learning rate by enforcing a constant norm on the weight parameters of the network, allowing scaling of the layer outputs to depend only on the learnable scale and offset parameters. This approach is similar to that proposed by Kodryan et al. (2022), but importantly takes care to treat the scale and offset parameters separately from weights. For simplicity, we remove bias terms as these are made redundant by the learnable offset parameters. In order to maintain constant parameter norm, we rescale the parameters of each layer to match their initial norm periodically throughout training - the precise frequency is not important as long as the parameter norm does not meaningfully grow to a point of slowing optimization between projections. For example, we find that in Rainbow agents an interval of 1000 steps and 1 step produce nearly identical empirical results. In principle we could constrain the parameter norm to any arbitrary value, but the choice of fixing the initial norm makes learning rate transfer easier when adapting an existing architecture. We do not project the final linear output layer, as it is not scale-invariant.

**Scale and offset parameters.** We find it is absolutely critical to normalize the weight parameters, as these represent the bulk of trainable parameters in the network. The learnable scale and offset parameters, assuming they are included in the network3, must be dealt with differently. Whereas rescaling the parameters of a linear map that immediately precedes a LayerNorm transform does not change the output of the function, the scale and offset terms will be first passed through a nonlinearity before entering the next LayerNorm and so will not necessarily share this property. For these parameters, we can proceed in one of three ways. In the case of homogeneous activations such as ReLUs, we can normalize the concatenated scale-offset vector as described in in Appendix D. This can require some effort to implement due to the dependency between the scale and offset parameters and may not be worthwhile for small training runs - indeed, most of our empirical results did not require this step, though we include it in our analysis of smaller networks in Figure 3.

A more general solution which requires less implementation overhead and which applies equally to non-ReLU activations is to regularize the scale and offset parameters to their initial values, a strategy which we employ in our continual learning evaluations in Section 5. Assuming suitable initial values, this approach encourages the mean and variance of the pre-activations toward values where the nonlinearity does not saturate. Finally, they can be allowed to drift unconstrained from their initialization, a choice we find unproblematic in standard benchmarks for supervised learning. For a more detailed discussion on this choice, we refer to Appendix D.

## 4 Understanding effective learning rate dynamics

NaP constrains the network's effective learning rate to follow an explicit rather than implicit schedule. In this section, we explore how this property affects network training dynamics, demonstrating how implicit learning rate schedules due to parameter norm growth can be made explicit and be leveraged to improve the performance of NaP in deep RL domains.

### Learning rate and parameter norm equivalence

We begin our study of effective learning rates by illustrating how the implicit learning rate schedule induced by the evolution of the parameter norm can be translated to an explicit schedule in NaP. We study a small CNN described in Appendix B.4 with layer normalization prior to each nonlinearity trained on CIFAR-10 with the usual label set. We train two 'twin' scale-invariant networks with the Adam optimizer in tandem: both networks see the exact same data stream and start from the same initialization, but the per-layer weights of one are projected after every gradient step to have constant norm, while the other is allowed to vary the norms of the weights. We then consider three experimental settings: in the first, we re-scale the per-layer learning rates of the projected network so that the explicit learning rate is equal to the effective learning rate of its twin. In the second, we re-scale the global learning rate based on the ratio of parameter norms between the projected and unprojected network, but do not tune per-layer. In the third, we do no learning rate re-scaling. We see in Figure 3 that the shapes of the learning curves for all networks except for the constant-ELR variant are quite similar, with the global learning rate scaling strategy producing a smaller gap than the no-rescaling strategy. By construction, the dynamics of the per-layer rescaling network and its twin are identical. Because global learning rate schedules are standard practice and induce dynamics that are quite close to those obtained by parameter norm growth in Figure 3, we take this approach in the remainder of the paper, leaving layer-specific learning rates and schedules for future work.

### Implicit learning rate schedules in deep RL

When taken to extremes, learning rate decay will eventually prevent the network from making nontrivial learning progress. However, learning rate decay plays an integral role in the training of many modern architectures, and is required to achieve convergence for stochastic training objectives (unless the interpolation applies or Polyak averaging is employed). In this section we will show that, perhaps unsurprisingly, naive application of NaP with a constant effective learning rate can sometimes harm performance in settings where the implicit learning rate schedule induced by parameter norm growth was in fact critical to the optimization process. More surprising is that the domain where this phenomenon is most apparent is one where common wisdom would suggest learning rate decay would be undesirable: value-based deep reinforcement learning.

RL involves a high degree of nonstationarity. As a result algorithms such as DQN and Rainbow often use a constant learning rate schedule. Given that layer normalization has been widely observed to improve performance in these algorithms, and that parameter norm tends to increase significantly in deep RL tasks, one might at first believe that the performance improvement offered by layer normalization is happening in _spite_ of the resulting implicit learning rate decay. A wider view of the literature, however, reveals that several well-known algorithms such as AlphaZero (Schrittwieser et al., 2020), along with many implementations of popular methods such as Proximal Policy Opti

Figure 3: We run a ‘coupled networks’ experiment as described in the text. All networks exhibit similar learning curves, as seen by the rightmost subplot, however there is small but visible gap between the learning curves obtained by NaP and an unconstrained network with fixed learning rates. Using a global learning rate schedule almost entirely closes this gap, but does not induce a precise equivalence in the dynamics as obtained by layer-wise rescaling (leftmost).

mization (Schulman et al., 2017), incorporate some form of learning rate decay, suggesting that a constant learning rate is not always desirable. Indeed, Figure 4 shows that constraining the parameter norm to induce a fixed ELR in the Rainbow agent frequently results in _worse_ performance compared to unconstrained parameters. This is particularly striking given that many of the benefits supposedly provided by layer normalization, such as better conditioning of the loss landscape (Lyle et al., 2023) and mitigation of overestimation bias (Ball et al., 2023), should be independent of the effective learning rate. Instead, these properties appear to either be irrelevant for optimization or dependent on reductions in the effective learning rate to materialize.

We can close this gap by introducing a learning rate schedule (linear decay from the default \(6.25 10^{-5}\) to \(10^{-6}\), roughly proportional to the average parameter norm growth across games). We further observe in Appendix C.1 that when we vary the endpoint of the learning rate schedule, we often obtain a corresponding x-axis shift in the learning curves, suggesting that reaching a particular learning rate was necessary to master some aspect of the game. We conclude that, while beneficial, the implicit schedule induced by the parameter norm is not necessarily _optimal_ for deep RL agents, and it is possible that a more principled adaptive approach could provide still further improvements.

## 5 Empirical Evaluations

We now validate the utility of NaP empirically. Our goal in this section is to validate two key properties: first, that NaP does not hurt performance on stationary tasks; second, that NaP can mitigate plasticity loss under a variety of both synthetic and natural nonstationarities.

### Robustness to nonstationarity

We begin with a continual classification problem which has been widely used in works studying loss of plasticity: memorization of iteratively re-randomized labels of an image dataset. In this task, each input from the dataset is assigned a uniform random label; the network is then trained on this set of labels for a fixed duration, after which a new set of random labels are generated and optimization begins again. Full details can be found in Appendix B.4. There is no shared structure between tasks in this problem setting, so performance is solely determined by trainability and not transfer between tasks. We evaluate our approach on a variety of sources of nonstationarity, using two architectures: a small CNN, and a fully-connected MLP (see Appendix B.4. for details). We consider a number of methods designed to maintain plasticity including Regenerative regularization (Kumar et al., 2023), Shrink and Perturb (Ash and Adams, 2020), ReDo (Sokar et al., 2023), leaky ReLU units (inspired by the success of concatenated ReLU activations (Abbas et al., 2023)), L2 regularization, and random Gaussian perturbations to the optimizer update, a heuristic form of Langevin Dynamics. We track the average online accuracy over the course of training for 20M steps, equivalent to 200 data relabelings, using a constant learning rate. We find varying degrees of efficacy in these approaches, with regenerative regularization and ReDO tending to perform the best. When we apply NaP on top of the same suite of methods in Figure 5, we observe near-monotonic improvements (with the exception of ReDO, where the reset mechanism does not take normalization into account) in performance and a significant reduction in the gaps between methods, with the performance curves of the different methods nearly indistinguishable in the MLP. Further, we observe constant or increasing slopes in

Figure 4: Without an explicit learning rate schedule, a Rainbow trained with NaP may fail to make any performance improvement; while the implicit schedule induced by the parameter norm is clearly important to performance, in several games this is significantly outperformed by a simple linear schedule terminating halfway through training. Intriguingly, we see a characteristic sharp improvement near the end of the decay schedule in several (though not all, e.g. fishing derby) games.

the online accuracy, suggesting that the difference between methods has more to do with their effect on within-task performance than on plasticity loss once the parameter and layer norms have been constrained.

### Stationary supervised benchmarks

Having observed remarkable improvements in synthetic tasks, we now confirm that NaP does not interfere with learning on more widely-studied, natural datasets.

**Large-scale image classification.** We begin by studying the effect of NaP on two well-established benchmarks: a VGG16-like network (Simonyan and Zisserman, 2014) on CIFAR-10, and a ResNet-50 (He et al., 2016) on the ImageNet-1k dataset. We provide full details in Appendix B.4. In Table 1 we obtain comparable performance in both cases using the same learning rate schedule as the baseline.

**Natural language:** we evaluate the effect of NaP on a 400M-parameter transformer architecture (details in Appendix B.3) trained on the C4 dataset (Raffel et al., 2020). Table 1 shows that our approach does not interfere with performance on this task, where we match final per-token accuracy of the baseline. When evaluating the pre-trained network on a variety of other datasets, we find that NaP slightly outperforms baselines in terms of performance on a variety of benchmarks, including WikiText-103, Lambda (Paperno et al., 2016), PIQA (Bisk et al., 2020), SocialIQA (Sap et al., 2019), and Pile (Gao et al., 2020).

### Deep reinforcement learning

Finally, we evaluate our approach on a setting where maintaining plasticity is critical to performance: RL on the Arcade Learning Environment. We conduct a full sweep over 57 Atari 2600 games comparing the effects of normalization, weight projection, and learning rate schedules on a Rainbow agent (Hessel et al., 2018). In the RHS of Figure 6 we plot the spread of scores, along with estimates of the Mean and IQM of four agents: standard Rainbow, Rainbow + LayerNorm, Rainbow + NaP without an explicit LR schedule, and Rainbow + NaP with the LR schedule described in Section 4.2. We find that NaP with a linear schedule outperforms the other methods.

We also consider the sequential setting of Abbas et al. (2023). In this case, we consider an idealized setup where we reset the optimizer state and schedule every time the environment changes, using a cosine schedule with warmup described in Appendix B.2. To evaluate NaP on this regime, we train on each of 10 games for 20M frames, going through this cycle twice. We do not reset parameters of the continual agents between games, but do reset the optimizer. We plot learning curves for the

   & **CIFAR-10** & **ImageNet-1k** \\  NaP & **94.64** & 77.26 \\  Baseline & **94.65** & 77.08 \\  Norm only & 94.47 & **77.45** \\   
 
**C4** & **Pile** & **WikiText** & **Lambda** & **SIQA** & **PIQA** \\ 
**45.7** & **47.9** & **45.4** & **56.6** & **44.2** & **68.8** \\ 
44.8 & 47.4 & 44.2 & 54.1 & 43.5 & 67.3 \\ 
44.9 & 47.6 & 44.3 & 53.6 & 43.8 & 67.1 \\  

Table 1: **Left:** Top-1 prediction accuracy on the test sets of CIFAR-10 and ImageNet-1k. **Right:** per-token accuracy of a 400M transformer model pretrained on the C4 dataset, evaluated on a variety of language benchmarks. See Appendix C.5 for more results with variation measures.

Figure 5: **Robustness to nonstationarity:** we see that without NaP, there is a wide spread in the effectiveness of various plasticity-preserving methods across two architectures. Once we incorporate NaP, however, the gaps between these methods shrink significantly and almost uniformly improves over the unconstrained baseline.

second round of games in the LHS of Figure 6, finding that NaP significantly outperforms a baseline Rainbow agent with and without layer normalization. Indeed, even after 200M steps the networks trained with NaP make similar learning progress to a random initialization.

## 6 Discussion

This paper has demonstrated that maintaining plasticity in the face of nonstationary training objectives can be achieved through careful normalization of the network's features and parameters. While there are many factors contributing to the efficacy of normalization in maintaining plasticity, we identified two non-obvious factors: the effect of normalization on the network's ability to revive saturated units, and role of the parameter norm in determining the effective learning rate of networks with normalization layers. With these insights in hand, we proposed Normalize-and-Project, a simple protocol which consists of adding layer normalization prior to nonlinearities in the network and periodically re-scaling the per-layer weights back to their initial norms.

Beyond improving performance in non-stationary supervised learning problems, NaP also presents a powerful tool for understanding the role of the effective learning rate on training dynamics. By applying NaP to reinforcement learning problems, we revealed the crucial importance of ELR decay to the ability of value-based deep RL agents to improve their performance on Atari tasks, explaining why approaches such as weight decay so often struggle to provide the same performance benefits in reinforcement learning as they do in supervised problems. This finding opens the door to a number of further questions: _why_ is ELR decay so critical to performance RL? What features of the environments require a sufficiently low ELR to learn, and why? In what settings is the implicit learning rate schedule yielded by the parameter norm sub-optimal for learning progress, and can better schedules be determined automatically, rather than being proscribed prior to the start of training?

The development of even more effective normalization strategies is a further promising avenue for future work. While we did not observe pathological behaviour in the unnormalized and unregularized scale and offset parameters of networks trained on single tasks, regularization of these parameters was critical to maintain performance in the sequential Atari domain and suggests that these parameters can interfere with learning if left unconstrained. Further, while layer normalization is not observed to impede network expressivity in visual domains, where the scale of the input does not usually carry task-relevant information, in proprioceptive domains normalization of features may increase the difficulty of learning, as it removes valuable spatial information from the input. Applying NaP to these domains will require careful design of feature embedding layers or an alternate normalization strategy which does not remove information about the Euclidean distance between inputs.

Figure 6: **Left:** We visualize the learning curves of continual atari agents on sequential ALE training (i.e. 200M frames). Each game is played for 20M frames, and agents pass sequentially from one to another, repeating all ten games twice for a total of 400M training frames. Solid lines indicate performance on the second visit to each game, and dotted lines indicate performance of a randomly initialized network on the game. Even in its second visit to each game, NaP performs comparably the randomly initialized networks, whereas the standard rainbow agent exhibits poor performance on all games in the sequential training regime. **Right:** aggregate effects of normalization on single-task atari, computed via the approach of Agarwal et al. (2021). Bars indicate 95% confidence intervals over 4 seeds and 57 environments.