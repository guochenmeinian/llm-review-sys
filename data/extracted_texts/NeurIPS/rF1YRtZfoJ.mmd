# Clap\(4\)CLIP: Continual Learning with Probabilistic Finetuning for Vision-Language Models

Saurav Jha\({}^{1}\), Dong Gong\({}^{1}\)1, Lina Yao\({}^{1,2}\)

\({}^{1}\)University of New South Wales (UNSW Sydney), \({}^{2}\)CSIRO's Data61

{saurav.jha, dong.gong}@unsw.edu.au, lina.yao@data61.csiro.au

###### Abstract

Continual learning (CL) aims to help deep neural networks learn new knowledge while retaining what has been learned. Owing to their powerful generalizability, pre-trained vision-language models such as Contrastive Language-Image Pre-training (CLIP)  have lately gained traction as practical CL candidates. However, the domain mismatch between the pre-training and the downstream CL tasks often calls for finetuning of the CLIP on the latter. Most existing finetuning methods exhibit deterministic nature. This makes them overlook the many possible interactions across the input modalities and deems them unsafe for high-risk tasks requiring reliable uncertainty estimation. To address these, our work proposes **C**ontinual **L**e**A**rning with **P**robabilistic finetuning (CLAP) - a probabilistic modeling framework over visual-guided text features per task, thus providing more calibrated CL finetuning. Unlike recent data-hungry anti-forgetting CL techniques, CLAP alleviates forgetting by exploiting the rich pre-trained knowledge of CLIP for weight initialization and distribution regularization of task-specific parameters. Cooperating with the diverse range of existing prompting methods, CLAP can surpass the predominant deterministic finetuning approaches for CL with CLIP. We conclude with out-of-the-box applications of superior uncertainty estimation abilities of CLAP including novel data detection and exemplar selection within the existing CL setups. Our code is available at https://github.com/srvCodes/clap4clip.

## 1 Introduction

Learning in the real world involves dealing with the ever-changing distributions of task streams and their data . Given the constraints on resources and privacy, there is also no guarantee for re-training a network on all previously seen data . Continual learning (CL) aims to learn from such data/task stream without _catastrophic forgetting_ of past data/tasks. A challenging CL setup is the class-incremental learning setting, where new classes emerge with new tasks, and at test time, a model must infer from all seen classes without known task IDs .

Recent years have seen pre-trained multi-modal foundation models excel on several domains . One such example for the vision-language (VL) domain is the CLIP  model that comes with strong zero-shot generalizability acquired by learning to match large-scale image-text pairs in a contrastive manner . However, to adapt well to downstream tasks, CLIP must be finetuned on the task-specific data . Considering both the need for continually finetuning pre-trained models on streaming tasks and their perks over training from scratch , our work studies CL with CLIP.

An issue with the existing deterministic approaches to finetuning  is that these overlook the _uncertainties_ arising from many possible interactions between the visual and textual cues of downstream tasks. For instance, on the textual side, while a good generic hand-crafted prompt for images is "A photo of a {class}", there can be instances where further tailored prompts helpimprove the image-to-text coherence. Similarly, on the visual side, images from the same class can have diverse range of backgrounds, poses, orientations, etc. Overlooking the uncertainties in image-text matching can thus cause overfitting on downstream tasks and forgetting of the generalizable knowledge . For CL, where we seek to adapt CLIP on a stream of tasks, this can further lead to the cross-modal features increasingly deviating from each other to the point of catastrophic forgetting (see Fig. 3b). While existing methods  model such uncertainties through probabilistic finetuning, these remain subpar at CL given: (a) their inaptness to leverage existing prompt-based approaches , (b) their excessive trading of in-domain performance for generalization .

Lastly, like other autonomous real-world agents, CL models deployed in mission-critical settings (healthcare, transport, etc.) can benefit from uncertainty awareness by calibrating their predictions to reliably assess their confidences . Hence, to enhance the usage of the pre-trained CLIP for real-world CL tasks, we design a finetuning approach with the following three properties (see Fig. 1): A) **probabilistic modeling** of cross-modal task cues for better generalization; B) **compatibility** with prompt-based finetuning methods  to exploit their finegrained in-domain task knowledge; C) leveraging the **rich pre-trained knowledge** of CLIP to further counter forgetting.

To this end, we design a principled Variational Inference (VI) framework that learns the functional space of task-specific posterior distributions based on text features that are aligned with their visual counterpart (property #**A**). To refine these variational task posteriors, we draw our motivation from Bayesian mixture-of-experts ensembles , and employ lightweight task-specific adapter modules that model task-specific distributions. Our final prediction is thus a mixture of logits derived from individual task adapter modules. To further counter the forgetting in these modules, we diverge from the recent trend of internet data-hungry CL techniques . Instead, we exploit the readily available pre-trained language knowledge of CLIP for weight initialization and task distribution regularization (property #**C**). Finally, by modelling the distribution of the text feature space, our probabilistic finetuning method boasts a rich modular nature as the features can be derived from arbitrary prompt types (property #**B**). In particular, we show that our framework can inherit the in-domain knowledge of hand-crafted , uni-modal , multi-modal , or instance-conditioned  prompts. We backonymize our finetuning approach as **CLAP** - Continual **LeAr**ning with **P**robabilistic finetuning - for the pre-trained CLIP model. Our experiments across

Figure 1: **Concept diagram for probabilistic finetuning of pre-trained CLIP in a CL setup:** We identify four such suitable design choices for probabilistic modelling. Choice \(\#1\) performs variational modelling by imposing prior on the prompt space which makes it prompt-type dependent while also interfering with the in-domain knowledge learning capability of the prompts – a criterion crucial in the deployment of CL models. Choice \(\#2\) (see Sec. 3.2.1) instead imposes a prior on the outputs of the text encoder. While this makes it prompt-type agnostic (as the text features can now be derived from arbitrary prompt types), not taking the visual information into account nevertheless leads to the loss of information about cross-modal interactions between the visual and textual cues – a property essential for preventing cross-modal deviation of finetuned features in CL (see Sec. 3.2.2). Choice \(\#3\) (Ours) leverages the best of both worlds by modelling the distribution of visual-guided text features. To further refine the learned distributions of CL tasks, we finally introduce lightweight task-specific adapter modules in choice \(\#4\) that make the cross-task centroids more distinct while preserving the aforesaid properties (see Sec. 3.2.3).

several settings show that CLAP4CLIP enhances _prompt-based_ finetuning for CLIP, and surpasses the predominant deterministic finetuning methods in terms of in-domain performance, output calibration, and generalization to unseen CL tasks, all while sharing a similar resource overhead. We study some out-of-the-box perks of CLAP's probabilistic nature by leveraging its uncertainty estimation capabilities on a proposed _post-hoc_ novel data detection setup and on exemplar selection for CL.

## 2 Related work

**Continual Learning (CL).** The existing CL literature is predominated by three categories of methods: (a) Regularization-based methods [6; 25; 26] alleviate forgetting by punishing changes to the parameters that are important to previous tasks; (b) Architecture-based approaches learn parameters that are specialized for individual tasks either by network expansion  or by sub-network composition [28; 29]; (c) Rehearsal-based approaches [30; 5] rely on storing a fraction of the past task experiences in a memory to train with the current task. Each category has its own flaw - methods in (a) struggle to discriminate inter-task classes ; those in (b) often require task oracle during inference; those in (c) are sensitive to the memory sizes besides being prone to overfitting on the memory samples . Hence, practical CL calls for combining these. Our work leverages (a) via function-space regularization (Sec. 3.4), (b) via task-specific modules (Sec. 3.2.3), and (c) via herding-based replay (see App. A.1.1).

**Vision-Language Models (VLMs) finetuning.** The powerful generalizability of pre-trained VLMs [9; 33] like the CLIP  has enabled their zero-shot applications to a range of downstream tasks, including CL . In practice, their performance on downstream out-of-domain data remains rather weak [34; 35]. For such cases, finetuning on task-specific data is a natural choice. Instead of performing extensive _full finetuning_ on all parameters, some _parameter-efficient finetuning_ (PEFT) methods learn a lightweight feature adapter module for textual and/or visual paths [13; 36]. Another line of PEFT methods learns _soft prompts_ which are a few continuous tokens serving as inputs to the frozen visual and/or textual encoder(s) to capture task-specific information [12; 37; 20]. Existing works on CL with pre-trained CLIP have leveraged either [19; 38] or both  of these methods. However, such finetuning methods remain deterministic in nature. This imposes an explicit constraint on the modeling of the possible ways in which the visual and the textual semantics interact.

To address the aforesaid flaw, one could turn to adapt the existing probabilistic finetuning approaches to capture the cross-modal interactions in CL tasks. For instance,  learn the distribution of hand-crafted prompts while  propose variational prompt tuning (VPT) _conditioned_ on the input images. Yet these methods are limited in their efficacy.  is incompatible with conditional prompt learning , which is an extensively studied PEFT field. VPT  trades in-domain performance excessively in favor of generalizability - a trait detrimental in the deployment of CL models. Our work aims to bridge these gaps for probabilistic finetuning all while adapting it for CL.

## 3 Methodology

### Preliminaries

**Continual Learning (CL).** Class-incremental CL  aims to learn from a sequence of \(T\) tasks \([(C^{1},D^{1}),(C^{2},D^{2}),...,(C^{T},D^{T})]\). Each task \(t[1,T]\) has its training data \(D^{t}=\{(_{1},y_{1}),(_{2},y_{2}),...,(_{k^{t} },y_{k^{t}})\}\), where \(\) and \(y\) are the input images and labels, respectively from the set of classes \(C^{t}=\{c_{1}^{t},c_{2}^{t},...,c_{n^{t}}^{t}\}\). Following [40; 41], we assume any two task-specific sets of classes to be disjoint: \(C^{i} C^{j}=\). A neural network with parameters \(\) is then trained on task \(t\) using \(D^{t}\) to minimize the cross-entropy loss over \(C^{t}\). At test time, the model is evaluated on all seen classes \(C=_{i=1}^{t}C^{i}\), where the past task predictions are prone to forgetting. As a solution, rehearsal-based methods [42; 43] replay past task samples from a memory \(\) during training. Following , we use herding  to maintain \(\) (see App. A.1.1 for details).

**CLIP with prompt.** CLIP comprises an image encoder \(f()\) acting on an image \(^{3 H W}\) of height \(H\) and width \(W\), and a text encoder \(g(())\) acting on a word embedding vector \(^{(L e)}\) derived from a text prompt \(^{((L-1) e)}\). Here, \(L\) is the text length, and \(e\) is the text embedding dimension. The encoders's outputs are used in the prediction of the class \(y_{i}\) as:

\[p(y_{i}|)=)^{T},g(_{i })/)}{_{c=1}^{|C^{t}|}( f()^{T},g(_{c})/)},\] (1)

where \(\) is a learnable temperature parameter, \(,\) is the cosine similarity, and the \(c\)-th class text feature \(_{c}=[,_{c}]\) is the result of adding a class-specific word embedding \(_{c}\) to the prompt \(\). The features \(g(_{c})\) for all classes are used as the weights of a linear classifier. In CL, \(g(_{c})\) would thus encode the features for classes \(c C\) seen until task \(t\). Eq. (1) forms a contrastive training criterion for the text and visual modalities, whose rich representation allows pre-trained CLIP to be used for zero-shot classification through _hard_ prompt templates, _i.e._, \(^{c}\) = "A photo of a {\(^{}\)}".

**CLIP finetuning with learnable soft prompts.** To improve the CLIP performance on a downstream task \(t\), _soft_ prompts use a set of learnable vector tokens \(=\{^{1},^{2},...,^{L}\}\). CoOp  shares \(\) with all classes of a task. MaPLe  learns multi-modal prompts by employing two such token sets \(_{f}\) and \(_{g}\) until the \(J\)-th layers of the vision and the text encoders of CLIP, respectively. AttriCLIP  selects a subset of prompts conditioned on the input: \(\{\{^{j}\}_{1 j L}|_{k}\}\). Learning \(\) (with frozen CLIP weights) thusly helps encode task/modality/instance-conditioned context for a given task.

**CLIP finetuning with adapters.** Adapter-based methods like CLIP-Adapter  learn lightweight modules over text and/or visual features of the frozen CLIP model. With a text adapter \(A_{t}\), the updated text features from Eq. (1) can be rewritten (with a slight abuse of notation) as:

\[g(_{i})= A_{t}(g(_{i}))+ g(_{i}),\] (2)

where \(\) and \(\) control the strength of the residual connection between the adapted and the pretrained models' features, e.g., \(=1-\) in .

### CL with probabilistic finetuning for CLIP

**Overview.** We develop our CLIP-based probabilistic finetuning model using a Bayesian VI framework (see Fig. 2). Sec. 3.2.1 starts by making the case that, unlike previous VI-based finetuning approaches, the feature embedding space of the text encoder output is a superior choice for defining our functional space priors on. While linear adapter layers are often employed for obtaining the mapping from such a pre-defined feature-space prior to the function outputs [45; 17], we show in Sec. 3.2.2 that CL finetuning with generic adapter leads to the issue of cross-modal deviation. In Sec. 3.2.3, we then propose refining our variational distribution on function space using an ensemble of task-specific adapters that are built on top of cross-modal aligned text features.

#### 3.2.1 Variational inference with function space prior on text features

We are interested in modeling the stochastic processes that generate the labels \(y\) for the inputs \(\) of a CL task \(t\). To this end, we assume a prior distribution \(p_{}\) over the text feature of the \(c\)-th class: \(p_{}(_{c}())\). We can then draw \(M\) number of latent variables \(z=\{z_{m} p_{}\}_{m=1}^{M}\) to represent the \(c\)-th class text feature \(_{c}()\) as a linear combination of the text encoder feature \(g(_{c}())\) and \(z\):

\[_{c}() =\{g(_{c}())+z_{m}\}_{m=1}^{M},\ \ z_{m} p_{},\] (3a) \[p(y_{i}|) =_{} f()^{T},_{c}()}{_{c=1}^{|C^{i}|} f( )^{T},_{c}()}p(_{c}( ))d,\] (3b)

Figure 2: **CLAP4CLIP overview:** the visual-guided attention (VGA) inference module uses the text features as query (Q), and the visual features as keys (K) and values (V) to produce visual-guided text features. The task-specific text features are fed to their respective task distribution encoders \((^{t},^{t})\). The task distribution samples are then fused with the original task features prior to deriving the task logits \(y^{t}\). All task logits are concatenated to produce the final prediction \(y^{1:t}\).

where Eq. (3b) replaces \(g(_{i})\) in Eq. (1) by \(_{c}()\). Eq. (3b) thus results into \(M\) number of predictions whose distribution gives us the model's epistemic uncertainty about the correct prediction. To deal with the intractability of the marginal likelihood, we optimize for the evidence lower bound (ELBO) using a variational posterior \(q_{}\) that approximates the prior \(p_{}\) based on the KL-divergence loss \(_{}\):

\[ p(y|)_{q_{}(z|_{c})}[ p(y| ,z)]-_{}q_{}(z|_{c})\|p_{ }.\] (4)

By assuming \(p_{}\) to be the (static) standard Gaussian \((,)\) and \(q_{}\) to be the (learnable) Gaussian \(((_{c}),(_{c}))\), whose mean \(\) and standard deviation \(\) are parameterized by linear adapter layers, we can ensure that the random variable \(z\) remains differentiable by reparameterization trick . Accordingly, we refer to the parameters \([;]\) together as _probabilistic adapter_ from here onward.

Imposing the prior over the text feature offers us _further advantages_ over that in the prompt embedding space as done by VPT  (also see App. Fig. 1 for an illustration). First, Eq. (3b) is prompt-type agnostic as the text features \(g()\) could be derived from any existing soft  or hard  prompts.2 Second, by leaving the prompt embedding space intact and injecting stochasticity into the feature space, we can better learn the task-specific knowledge known to be encoded by the prompt embeddings . This can help bypass the loss of in-domain performance. In CL, the latter property is crucial for our model to perform well on all previously seen tasks. Third, as the latent variable \(z\) is now directly used to infer the logits, it naturally favors generalization by influencing the predictions. On the contrary, the effect of the prompt space prior on the predictions is indirect as it is mediated by the representations of the entire text encoder layers. This can make the influence of the prior harder to control and can hinder the aforesaid interpretability in the model's predictions.

Efficient continual finetuning with a probabilistic adapter.Finetuning adapters for CL is not straightforward. Namely, we have the overhead of searching for task-specific residual ratio \(\) (see Eq. (2)) which is sensitive to the training setup including dataset and prompt-type . This has particularly worse implications for a probabilistic adapter like ours, where a larger \(\) can inject enough noise to corrupt the pre-trained representations to the point of catastrophic forgetting (see Fig. 2(a)). For efficient learning of our adapter, we thus seek to retain _no additional overhead_ of hyperparameter search for the residual ratio. Subsequently, we use \(==1\) through our work.

#### 3.2.2 Cross-modal feature deviation in continual finetuning of CLIP

To perform CL with variational modelling in the text feature space, we first take a step back to investigate how CL in general affects the _cross-modal deviation_ between the learned text and the frozen visual features of finetuning methods. To this end, we consider two basic CL models: the CoOp  and the CoOp with a CLIP-Adapter . Then, for the base task (\(t=1\)) test samples of CIFAR100, we compute the average of the Rotation Angle Matrix (RAM)  using the CL models' frozen visual \(f()\) and learnable textual \(g(_{c}())\) features at each incremental test step. Fig. 2(b) shows the deviation of the learned textual features from their (frozen) visual counterparts for the CoOp. This implies that the cross-modal retrieval performance of CLIP finetuned with learnable prompts deteriorates with incremental training. Moreover, as a generic adapter (CoOp + Adapter) does not remedy the cross-modal deviation, this sets a direct hindrance in employing our probabilistic adapter to learn the variational distribution \(q_{}\).

Figure 3: **Need for Visual-guided Attention (VGA) inference module.** Fig. 2(a): A simple adapter is inadequate at preventing catastrophic forgetting in CL – marked by high BwT scores; Fig. 2(b): VGA module encourages cross-modal alignment between the learned text features and the pre-trained visual features – marked by a decrease in average angle \( t,1\) between them – where otherwise the former deviates further with incremental training steps.

Variational modeling on visual-guided text features.For variational modeling of text features \(_{c}()\) that remain aligned with the visual features during continual finetuning, we propose enriching the text features with the visual context through an explicit attention mechanism (see App. A.3 for further justification on this design choice). To this end, we treat the text features as queries \(Q\) and the visual features as keys \(K\) and values \(V\), and adopt a standard transformer-styled decoder block  as a task-shared Visual-guided attention (VGA) module. The VGA module performs text-to-text self-attention followed by text-to-visual cross-attention. To eliminate the influence of the text features from multiple CL tasks, a _naive_ strategy is to perform specialized VGA forward passes with task-specific queries . We seek to replace several such costly VGA passes with a single pass. To do so, we exploit the global nature of our visual context and mask out (set to \(-\)) all inter-task connections in the queries using a target mask. This ensures that only the task-specific text features undergo self-attention while the entire query still attends to the visual context:

\[\{}_{c}^{k}\}_{k=1}^{t}=Q=\{ _{c}^{k}\}_{k=1}^{t},K=V=f(),\] (5a) \[}_{c}^{t}=}_{c}^{t}+g(_{c}^{t}()),\] (5b)

where \(}_{c}^{k}\) is the task-specific visual-guided text feature which is fused with the residual task-specific text encoder feature \(g(_{c}^{t})\) to derive the task embedding \(}_{c}^{t}\). We note that in a non-CL setup,  employ the VGA module using the per-pixel spatial features (obtained before global average-pooling) instead of the globally pooled visual features of the ViT . Our choice for the latter favors the efficiency of our framework for large CL datasets where attending to per-pixel spatial features can incur much higher latency (see App. A.4 for a comparison).

#### 3.2.3 Task-specific probabilistic adapters as ensembles for posterior approximation

By exploring diverse modes in function space, ensembles of neural networks can better approximate the variational posterior [51; 22]. Motivated by this, we replace our task-shared adapter \(q_{}\) with task-specific adapters \(\{q_{}^{i}\}_{i=1}^{t}\) that parameterize the \(t\)-th task-specific posterior \((^{t},^{t})\) over the task embeddings \(}_{c}^{t}\):

\[\{z_{m}^{t}\}_{m=1}^{M} q_{}^{t}(z|}_{c}^{t})= ^{t}(}_{c}^{t}),^{t}(}_{c}^{t}),\] (6)

where \(z_{m}^{t}\) are the task-specific MC samples. Task-specific adapters thus serve as mixture-of-experts ensemble where each expert is trained on task-specific embeddings \(}_{c}^{t}\) and the logits computed using each expert is combined to derive the final prediction \(^{1:t}\) (see Algo 1). The experts learn posteriors that are more discriminative across tasks. This is depicted in Fig. 4 using the cosine distance between the embeddings of the class-specific samples drawn from the posteriors. With task-specific adapters (right), the cross-task class centroids are more separable.

To prevent interference from current task training data, we freeze the past task encoders during each incremental training step (\(t>1\)). Moreover, to reduce the forgetting in past task adapters, we follow other parameter-isolation techniques [27; 42; 52] to finetune on a class-balanced dataset of new data and rehearsal data \(\) at the end of each incremental training step (\(t>1\)). We refer to this as memory consolidation training (see App. A.2). We also provide an overview of a test-time forward pass of our framework in App. A.5.

Algorithm overview.App.algo.1 outlines the pseudo-code of a forward pass of CLAP at \(t-\)th task test step. Here, a test image is to be classified into one of the classes \(\{1,...,|C^{t}|\}\). Our method executes the computationally heavy VGA layers only once. The task-specific VGA outputs are passed to their respective adapters. By limiting the quadratic complexity of the VGA pass, our method induces minimal time overhead. By only expanding the linear adapters per task, our memory overhead is negligible compared to the large backbone of the pre-trained CLIP model (ablation Fig. 5).

### Alleviating forgetting with pre-trained language-aware CLIP knowledge

Like other finetuning methods, our probabilistic adapters are likely to trade the generalizability of text features for downstream task performances [12; 37]. On the contrary, the pre-trained CLIP text

Figure 4: **Need for task-specific probabilistic adapters:** Cosine distance between the centroids of class-specific latent variables produced without **(left)** and with **(right)** task-specific adapters on CIFAR100 (10 tasks, 10 classes per task).

encoder with hand-crafted prompts  has strong generalizability because of its rich pre-trained language information. We propose to leverage this pre-trained language knowledge to help guide the incremental finetuning in CLAP. In the following, we assume \(\{_{y}^{h,l}^{d}\}_{l=1}^{L}\) to be the features corresponding to the \(L\) hand-crafted textual prompts for the class \(y C^{t}\).

#### 3.3.1 Past-task distribution regularization for mitigating forgetting

The functional spaces of the past task distributions are prone to forgetting in CL. Though replay helps alleviate the forgetting up to a certain degree, repeated training on the memory samples can lead to overfitting on these [32; 54]. To address this, previous works [8; 55] exploit functional priors for regularizing the visual space alongside memory replay. Here, we propose to regularize the past task distributions in the textual space by using the hand-crafted prompt-based features \(\{_{y}^{h,l}\}_{l=1}^{L}\) to distill the past task latent samples \(z^{i}=\{z_{m}^{i}\}_{m=1}^{M}}_{i=1}^{t-1}\). Namely, the probability of the sample set \(z^{t}\) belonging to a class \(y C^{t}\) is:

\[P_{}(y|z^{t})=_{m=1}^{M}_{l=1}^{L} {(_{y}^{h,l},z_{m}^{t})}{_{c=1}^{|C^{t}|} (_{c}^{h,l},z_{m}^{t})}.\] (7)

The resulting language-aware distillation loss is thus the sum of cross-entropy between the true label distribution \(y_{c}\) and the predicted probability distribution \(P_{}\) across all the past-task classes:

\[_{}=-_{t=1}^{T-1}_{c=1}^{|C^{t}|} P_{ }(c|z^{t})y_{c},\] (8)

where \(_{}\) serves as a **data-free** (_i.e._, no training samples required) text-to-text distribution regularizer that encourages the latent variable outputs from past-task adapters to stay close to the text features from the hand-crafted prompts. \(_{}\) is applied only during the memory consolidation training, that is, when the past-task adapters are trainable. Lastly, as \(_{}\) acts on the functional space of past tasks, this sets apart our setting from the non-CL setup of  where the language-aware distillation loss regularizes the vector embedding space.

#### 3.3.2 Task-specific adapter initialization considering stability

Stability gap  in CL refers to the temporary yet substantial forgetting of past task knowledge in the initial phases of updating a network's weights to learn an incremental task. An informed weight initialization can help bridge this gap over random initialization by stabilizing the learning for new task components . We thus leverage the \(t-\)th task text features \(\{_{y}^{h,l}\}_{l=1}^{L}\) to initialize the weights \(_{t}^{},_{t}^{}^{d d}\) of our \(t-\)th task's linear adapter layer. Let \(_{},_{}^{|C^{t}| d}\) be the mean and the std. dev. of the \(L\) text features. We initialize \(_{t}^{}\) and \(_{t}^{}\) as:

\[_{t}^{}=_{}^{T},_{} ,\ \ _{t}^{}=_{}^{T},_{ }.\] (9)

### Training objective

Approximate ELBO.Building upon Eq. (4), we now learn the task-specific adapters \(q_{}^{t}\) to approximate the intractable \(t[1,T]\) task-specific posteriors. The ELBO (see App. F for derivation) is:

\[ p(y^{1:T}|;}_{c}^{t})_{t=1}^{T} _{q_{}^{t}(z^{t}|;}_{c}^{t})}  p_{}(y^{t}|z^{t},;}_{c}^{t})- _{}q_{}^{t}(z^{t}|;} _{c}^{t})\|p_{}(z^{t}).\] (10)

Overall objective.Denoting the loss weights by \(\) and \(\), our total loss term can be given as \(=_{}-_{}+ _{}\), where the cross-entropy \(_{}\) and the prior-matching \(_{}\) terms act on the outputs of all task encoders while the distribution regularization term \(_{}\) acts only on the past task encoders. \(\) is set to 0.001. As the past task encoders are trainable only during the memory consolidation training stage, \(\) for these is set to 0 during training. \(\) is set to 15.

## 4 Experiments

**Datasets.** We evaluate our method on CIFAR100 [3; 30], ImageNet100 [41; 43], ImageNet-R , CUB200 , and VTAB . CIFAR100  and ImageNet100  setups split their respective original datasets into 10 tasks with 10 classes each. ImageNet-R  and CUB200 split 200 classes into 10 tasks with 20 classes each. VTAB has 5 tasks with 10 classes each . While CIFAR100, ImageNet100, and CUB200 are robust settings for evaluating CL methods in the face of large forgetting, ImageNet-R and VTAB make challenging settings for CL methods using pre-trained models as these might include test images in their pre-training set (see App. A.1 for details).

**Baselines.** We compare CLAP4CLIP against several baselines and state-of-the-art finetuning methods. These include: (a) CLIP-based methods - Continual-CLIP , CoOp , CLIP-Adapter , AttriCLIP , MaPLe , and PROOF , (b) vision-only methods - DualPrompt  L2P , CODA-P , (c) the baseline CIL method - iCaRL . For a fair comparison, we adhere to the experimental protocols of PROOF  throughout. We adopt ViT-B/16 with the pre-trained weights of OpenAI  as our backbone unless otherwise specified. As the upper bounds on performance, we use the CLAP4CLIP with single and task-specific encoders, trained on all tasks jointly (JOINT).

**Variants.** We integrate our method with four prompt-based approaches: Ours uses CLAP with hand-crafted prompt templates, CoOp + Ours with soft prompts , MaPLe + Ours uses multi-modal soft prompts , and AttriCLIP + Ours uses CLAP4CLIP with instance-conditioned soft prompts . Ours w/o Variational Inference (VI) is the deterministic variant of Ours depicted in App. Fig. 7. We leave the details of the training and the hyperparameters of our models in App. A.2.

**Performance measures.** To quantify CL performances, we report: (a) the final accuracy after the last incremental step (Last) and the average of the accuracies after each step (Avg) , and (b) the backward transfer score (BwT)  to quantify forgetting. To assess the benefits of probabilistic modelling, we report: (a) the expected calibration error (ECE)  that measures the calibration in the model's predictions , and (b) the (forward) transfer score  that quantifies the generalizability of CL models by measuring the extent of their zero-shot transfer ability after finetuning.

### Results

**Accuracy.** We report performances in Table 1 on all five datasets. Our method consistently achieves the best results among all the methods compared. Notably, on CIFAR100 and ImageNet100, our variants using the hand-crafted and multi-modal prompts outperform the others. On the challenging ImageNet-R setup with significant intra-class diversity, our method can better leverage the instance-conditioned prompt knowledge of AttriCLIP , which helps it outperform PROOF  by \(1.46\%\) in terms of average accuracy. On CUB200 and VTAB, sharing the prompt pool among all tasks gives CoOp  an edge over other baselines. Leveraging CoOp offers us the best results on these while surpassing PROOF, which also builds upon CoOp with task-specific soft prompts. We also observe that VPT  lags on all CL settings. Comparing the performance evolution of our variants against other baselines shows that our variants perform better throughout the incremental steps (App. Fig. 8).

**Forgetting.** Table 13 shows that in general, plugging CLAP4CLIP with prompt-based finetuning methods helps improve the BwT scores of the latter. It is worth noting that on the cross-dataset setting of VTAB , our variants are the only methods that effectively transfer the knowledge learned from

   Method &  &  &  &  &  \\  & **Avg T** & **Last T** & **Avg T** & **Last T** & **Avg T** & **Last T** & **Avg T** & **Last T** & **Avg T** & **Last T** \\  Single-task JONT & 80.28 & 81.08 & 80.92 & 75.4 & 89.29 \\ Tank-specific JONT & 82.9 & 83.55 & 83.07 & 85.72 & 94.6 \\  CoRL  & 72.93 & 57.6 & 68.62 & 59.5 & 66.34 & 43.71 & 82.39 & 75.1 & 53.38 & 41.6 \\  L2P  & 78.92 & 20.04 & - & -77.07 & 69.33 & 76.98 & 68.47 & - & - \\ DualPrompt  & 82.11 & 74.31 & - & 82.73 & 76.41 & 82.37 & 76.29 & - & - \\ CODA-P  & 85.19 & 76.4 & 85.93 & 79.02 & 82.06 & 79.5 & 84.77 & 80.39 & 87.5 & 81.2 \\ PROOF  & 84.84 & 76.55 & - & - & 84.89 & 79.7 & 83.98 & 79.35 & - & - \\  Continual-CLIP  & 78.65 & 68.26 & 83.99 & 74.2 & 84.43 & 76.94 & 67.0 & 54.8 & 68.5 & 60.97 \\ CoOp  & 81.17 & 70.58 & 79.14 & 69.4 & 84.7 & 76.66 & 76.62 & 66.38 & 57.06 & 81.25 \\ MapLe  & 82.74 & 74.52 & 79.23 & 64.06 & 85.28 & 79.71 & 73.38 & 64.43 & 83.91 & 81.81 \\ AutoCLIP  & 79.31 & 68.45 & 82.29 & 70.76 & 83.09 & 76.53 & 65.26 & 52.12 & 71.84 & 64.09 \\ CLIP-Adapter  & 78.75 & 68.32 & 84.13 & 73.96 & 84.49 & 78.1 & 67.41 & 54.49 & 68.23 & 61.02 \\ VPT  & 73.4 & 59.33 & 80.51 & 61.09 & 81.66 & 74.0 & 69.14 & 60.03 & 67.2 & 77.26 \\  Ours w/o VI & 84.36 & 76.8 & 86.11 & 76.48 & 85.69 & 79.83 & 72.21 & 61.87 & 90.14 & 88.64 \\  Ours & **86.13** & 78.21 & **87.76** & 79.16 & 85.77 & 79.98 & 86.93 & 81.64 & 91.37 & 89.67 \\ CoOp + Ours & 85.71 & 77.4 & 86.8 & 78.18 & 85.32 & 79.52 & **86.99** & **81.95** & **92.51** & **91.28** \\ MaPLe + Ours & 86.06 & **78.48** & 87.47 & 79.02 & 86.25 & 80.56 & 81.53 & 74.24 & 90.97 & 88.83 \\ AutoCLIP + Ours & 76.06 & 67.59 & 87.37 & **79.3** & **86.35** & **80.6** & 83.71 & 79.01 & 74.84 & 71.12 \\   

Table 1: **Performance comparison** of different methods averaged over three runs. Best scores are in **bold**. The second-best scores are in blue. The results for L2P, DualPrompt, and PROOF are taken from . See App. Table 12 for statistical significance of these results using std. dev. scores.

incremental tasks to improve the performance on past tasks (_i.e._, \(>0\)). This indicates that our probabilistic modeling strategy does not only counter forgetting but can also help bring anti-forgetting properties onto existing finetuning methods.

CalibrationApp. Table 15 compares the ECE scores of our variants and their respective underlying deterministic baselines at the last test step. In general, our variants help enhance (decrease) the ECE scores of the underlying prompt-based methods. This implies that even in the face of forgetting in a CL setup, CLAP retains more reliability in assessing the confidence of its predictions.

GeneralizationApp. Table 14 shows that our method consistently enhances the (forward) transfer scores of the underlying deterministic prompt-based methods. This means that CLAP can better transfer the learned knowledge from seen tasks to help solve future tasks.

Resource-constrained CLTo study the robustness of CLAP towards memory and compute-constrained environments, we ablate its performance on _replay-free_ and _computationally-budgeted_ CL setups, respectively. Tables 16 and 17 show that for both these setups, leveraging the instance-conditioned and semantically diverse prompts of AttriCLIP provides an edge. Here, our variant leveraging AttriCLIP surpasses the replay-free SOTA, _i.e.,_, CODA-P  and the budgeted SOTA, _i.e.,_ AttriCLIP . Further ablating the role of our proposed language-aware distribution regularization and weight initialization components for our AttriCLIP variant shows that the former component remains crucial for avoiding forgetting under resource-constrained settings.

#### 4.1.1 Cross-Datasets Continual Learning (CDCL)

To simulate real-world settings with long sequence of tasks and large distribution shifts, the CDCL setting  trains a model sequentially on ImageNet100 and CIFAR100 (_i.e._, on 20 tasks), and evaluates it jointly on these. For a fair comparison with , we adopt the ViT-L/14 as our CLIP backbone and set the train/test batch size to 32. All other settings remain the same as in Sec. 4.1. Table 2 reports the last task accuracy of different methods. While all our variants improve the CDCL performances of their respective baselines, combining ours with AttriCLIP  leads to the most gains. This further suggests that our framework can reliably leverage the diverse nature of learned prompts to inherit their setting-specific advantages.

### Ablation Studies

We provide a few ablations of the training pipeline for CLAP\(4\)CLIP below and leave more in App. C.

Influence of componentsWe ablate the importance of different components of CLAP\(4\)CLIP in Table 3. On top of the base CLIP model, we first train a probabilistic encoder. Adding the VGA module and the memory consolidation training stage helps us achieve more stable performances while countering forgetting. We then apply task-specific encoders which make the centroids of the class-specific latent variable more separable (see Fig. 4) thus improving the last task accuracy by 2.21%. Language-aware weight initialization and regularization help improve the last task accuracies by 0.78% and 0.23%, respectively. Weight initialization further helps us tackle the _stability gap_ (see App. C.6 for more ablations on language-aware components).

Probabilistic vs Deterministic inferenceTo understand our probabilistic inference modules further, we examine their performance against the deterministic variant of ours (Ours w/o VI). Table 1 shows that our probabilistic variant consistently outperforms its deterministic counterpart. This emphasizes the advantages of considering uncertainty in finetuning. We further introspect the effects of the number of layers for the VGA and task encoder modules in our framework in App. C.3.

**Time analyses.** We compare the inference time per iteration for different methods. As shown in App. Table 19, our variants need more inference time than other finetuning methods for the performance gains. The increased time comes mainly from the VGA and from inferring the \(M\) latent variables.

**Parameter analyses.** The additional parameters in CLAP\(4\)CLIP come from the shared VGA module and the task-specific encoders. For a ViT-B/16 backbone of output dimension, \(d=512\) on CIFAR100, the VGA module contains 4,204,032 parameters. The mean and the std. dev. layers for 10 tasks have \(d d\) parameters each, _i.e._, \(524,2880\) parameters. Hence, the CLAP\(4\)CLIP has 9.5 million extra parameters, which is negligible compared to the pre-trained CLIP with \( 150\) million parameters. We report the parameter counts in Fig. 5.

## 5 Out-of-the-box utilities of probabilistic finetuning

We study the out-of-the-box utilities of CLAP\(4\)CLIP's uncertainty quantification (UQ) capabilities. Our motivation for these is not to achieve state-of-the-art performance but to highlight the perks of probabilistic modeling in scenarios where the deterministic CL finetuning methods struggle.

**Post-hoc novel data detection (PhNDD).** PhNDD uses a pre-trained classification model to identify novel data based on the output confidence . For CL, this can help discern the arrival of new tasks, expand the network, etc. To evaluate the PhNDD capabilities of models within a CL setup, we design a simple setting. Namely, at all but the last test step, we treat the test data from the past and the current tasks as _seen_ while those from all future tasks as _novel_. We then use FPR95, AUROC , and AUPR  scores as our performance metrics (see App. D.1) averaged over all but the last incremental test steps. To quantify the output confidence, we rely on the Energy score  given its aptness for pre-trained models. Table 4 compares the averaged PhNDD performances. Our probabilistic models enhance the PhNDD capabilities of their underlying prompting frameworks. Moreover, the inferior results of the deterministic (_i.e._, w/o VI) versions of our models suggest that probabilistic modelling helps a model output predictions that better express what it is not aware of.

**Exemplar selection.** We employ the entropy (averaged over \(M\) predictions) of CLAP's softmax outputs as our exemplar selection criteria . Table 20 shows the efficacy of entropy-based rehearsal for our method, where other deterministic methods lag due to their inconsistent UQ capabilities. Next, we employ the energy  and the variance of the softmax outputs as our selection criterion and contrast these against other criteria proposed in . Fig. 6 shows that variance-based exemplar selection outperforms random, and is only second to iCaRL  in terms of Last accuracy. We note that deterministic methods with pointwise predictions cannot use variance for exemplar selection.

## 6 Conclusion

In this paper, we propose CLAP\(4\)CLIP, a probabilistic finetuning method for learning task-specific distributions over visual-guided textual features. Our model shares the visual-guided text alignment module across all tasks while adding lightweight task-specific encoders to learn fine-grained task distributions. Besides leading to little memory overhead, this architecture is compatible with several prompt-tuning-based methods thus helping us inherit their respective perks on different CL settings. Our experiments show the superior results of CLAP\(4\)CLIP across several datasets and settings. We conclude with two out-of-the-box utilities of our method wherein existing continual learning methods lag: post-hoc novel data detection and uncertainty-based exemplar selection. We discuss our limitations, potential future research directions, and the broader impact in App. sec. E.

   Method & **AUROC\(\)** & **AUPR\(\)** & **FFPR95\(\)** \\  Continual CLIP  & 74.46 & 71.11 & 77.33 \\ Ours w/o VI & **82.19** & 78.88 & 68.83 \\ Ours & 82.21 & **79.54** & **68.72** \\  C\(\)Op  & 80.15 & 77.62 & 66.8 \\ + Ours w/o VI & 81.98 & 78.85 & 66.21 \\ + Ours & **83.73** & **80.97** & **62.85** \\   

Table 4: PhNDD performances averaged over 3 runs on CIFAR100. Best scores for each variant are in **bold**.

Figure 5: Parameter count comparison.

Figure 6: **Analyses of various strategies** for exemplar selection w/ our method on CIFAR100.

Acknowledgement

This work was partially supported by a Discovery Early Career Researcher Award Fellowship (DE230101591) awarded by the Australian Research Council (ARC) to Dong Gong. We are grateful to Daniel Marczak and M. Jehanzeb Mirza for their insights on the need for continual finetuning of the pre-trained CLIP model.