# drago: Primal-Dual Coupled Variance Reduction

for Faster Distributionally Robust Optimization

 Ronak Mehta\({}^{1}\)  Jelena Diakonikolas\({}^{2}\)  Zaid Harchaoui\({}^{1}\)

\({}^{1}\)University of Washington, Seattle \({}^{2}\)University of Wisconsin, Madison

###### Abstract

We consider the penalized distributionally robust optimization (DRO) problem with a closed, convex uncertainty set, a setting that encompasses learning using \(f\)-DRO and spectral/\(L\)-risk minimization. We present drago, a stochastic primal-dual algorithm that combines cyclic and randomized components with a carefully regularized primal update to achieve dual variance reduction. Owing to its design, drago enjoys a state-of-the-art linear convergence rate on strongly convex-strongly concave DRO problems with a fine-grained dependency on primal and dual condition numbers. The theoretical results are supported by numerical benchmarks on regression and classification tasks.

## 1 Introduction

Contemporary machine learning research is increasingly exploring the phenomenon of distribution shift, in which predictive models encounter different data-generating distributions in training versus deployment (Wiles et al., 2022). A popular approach to learn under potential distribution shift is _distributionally robust optimization_ (DRO) of an empirical risk-type objective

\[_{w}_{q}_{0}(w,q):=_{ i=1}^{n}q_{i}_{i}(w),\] (1)

where \(_{i}:^{d}\) denotes the loss on training instance \(i[n]:=\{1,,n\}\), and \(q=(q_{1},,q_{n})\) is a vector of \(n\) weights for each example. The feasible set \(\), often called the _uncertainty set_, is a collection of possible instance-level reweightings arising from distributional shifts between train and evaluation data, and is often chosen as a ball about the uniform vector \(/n=(1/n,,1/n)\) in \(f\)-divergence (Samkoong and Duchi, 2016; Carmon and Hausler, 2022; Levy et al., 2020) or a spectral/\(L\)-risk-based uncertainty set (Mehta et al., 2023).

We consider here the penalized version of (1), stated as

\[(w,q):=_{i=1}^{n}q_{i}_{i}(w)- D(q\|/n)+\|w\|_{2}^{2},\] (2)

where \(, 0\) are regularization parameters and \(D(q\|/n)\) denotes some statistical divergence (such as the Kullback-Leibler (KL) or \(^{2}\)-divergence) between the original weights \(/n\) and shifted weights \(q\). For clarity, we focus on the cases of \(,>0\), but also describe the modifications to the methods, results, and proofs for cases in which \(=0\) or \(=0\), in Appx. C.4. See Fig. 1 for intuition on the relationship between the uncertainty set, divergence \(D\), and hyperparameter \(\).

Standard (1) and penalized (2)DRO objectives have seen an output of recent use in reinforcement learning and control (Lotidis et al., 2023; Yang et al., 2023; Wang et al., 2023; Yu et al., 2023; Kallus et al., 2022; Liu et al., 2022) as well as creative applications in robotics (Sharma et al., 2020), language modeling (Liu et al., 2021), sparse neural network training (Sapkota et al., 2023)and defense against model extraction (Wang et al., 2023b). However, even in a classical supervised learning setup, current optimization algorithms for DRO have limitations in both theory and practice.

For context, we consider the large-scale setting in which the sample size \(n\) is high, and the training loss in each example is accessed through a collection of \(n\)_primal first-order oracles_\(\{(_{i},_{i})\}_{i=1}^{n}\). Quantitatively, we measure the performance of algorithms by runtime or global complexity of elementary operations to reach within \(\) of the minimum of \(}(w)=_{q}(w,q)\), while qualitatively, we consider the types of uncertainty sets that can be handled by the algorithm and convergence analysis. Under standard assumptions, \(}\) is differentiable with gradient computed via

\[q^{}(w)=*{arg\,max}_{q}(w,q), { followed by }}(w)=_{i=1}^{n}q_{i}^{}(w)_{i}(w)+  w.\] (3)

In the learning setting, we are interested in stochastic algorithms that can approximate this gradient with \(b<n\) calls to the oracles. For uniformly randomly sampled \(i[n]\), \(nq_{i}^{}(w)_{i}(w)+ w\) is an unbiased estimator of \(}(w)\). However, computing \(q_{i}^{}(w)\) depends on the first step in (3) which itself requires calling all \(n\) oracles (see (2)), i.e., it is no different than the cost of full batch gradient descent. A direct minibatch stochastic gradient descent approach would approximate \((w,)\) in the first step of (3) with only \(b\) calls to generate approximate weights \((w)\). Because \((w) q^{}(w)\) in general for \(b<n\), these methods have non-vanishing bias, i.e., do _not_ converge (Levy et al., 2020).

This motivated research on DRO-specific stochastic algorithms with theoretical convergence guarantees under particular assumptions (see Tab. 1 and Appx. B for details) (Namkoong and Duchi, 2016; Levy et al., 2020; Carmon and Hausler, 2022). Although we highlight the dependence on sample size \(n\) and suboptimality \(\), the dependence on all constants is given in Tab. 1. For \(f\)-divergence-based uncertainty sets in the standard oracle framework, several methods achieve a \(O(^{-2})\) complexity. Levy et al. (2020) do so by proving uniform bias bounds, so that if \(b\) scales as \(O(^{-2})\), the convergence guarantee is achieved. However, if the required batch size \(b\) exceeds the training set size \(n\), then the method reduces to the sub-gradient method, as we can see in Tab. 1. These sublinear rates typically stem from two causes. The first is the adoption of a "fully stochastic" perspective on the oracles, wherein each oracle's output is treated as an independent random sample drawn from a probability distribution. The second is the non-smoothness of the objective, as we shall see below.

Variance reduction techniques, on the other hand, exploit the fact that the optimization algorithm takes multiple passes through the same dataset, and achieve _linear_ rates of the form \(O((n+_{})(^{-1}))\) in empirical risk minimization when the objective is both smooth (i.e., has Lipschitz continuous gradient) and strongly convex and \(_{}\) is an associated condition number (Johnson and Zhang, 2013; Defazio et al., 2014). Assuming access to stronger oracles involving constrained minimization and applying a variance reduction scheme, Carmon and Hausler (2022) achieve \(O(n^{-2/3}+n^{3/4}^{-1})\) for \(f\)-divergences as well, but do not obtain linear convergence due to the second type of cause: the objective \(}\) is _non-smooth_ when \(=0\).

Figure 1: **Visualization of Uncertainty Sets and Penalties.** Each plot is a probability simplex in \(n=3\) dimensions with the uncertainty set as the colored portion. The black dots are optimal dual variables \(q_{}^{}:=*{arg\,max}_{q}_{i=1}^{n}q_{i }_{i}(w)- D(q\|/n)\) for a fixed \(w\). As \(\) decreases, \(q_{}^{}\) may shift toward the boundary of the uncertainty set. The combination of \(\) and \(D\) determines an “effective” uncertainty set, whose shape is given by the level sets of \(D\). Our methods apply to both.

Recently, Mehta et al. (2024) handled the \(>0\) case for spectral risk uncertainty sets, and their variance-reduced algorithm achieves a linear \(O((n+_{}_{})(1/))\) convergence guarantee (where \(_{} 1\) measures the "size" of the uncertainty set), but only with a lower bound of order \((n)\) on the problem parameter \(\). The challenge of this problem, considered from a general optimization viewpoint beyond DRO, stems from the non-bilinearity of the coupled term \(_{i=1}^{n}q_{i}_{i}(w)\) and the constraint that \(_{i=1}^{n}q_{i}=1\) for probability vectors. If the coupled term was bilinear (i.e., of the form \(q^{}Aw\) for \(A^{n d}\)) and the constraints applied separately to each \(q_{i}\), then dual decomposition techniques could be used. Qualitatively, algorithms and analyses often rely on particular uncertainty sets; for example, Kumar et al. (2024) use duality arguments specific to the Kullback-Leibler uncertainty set to create a primal-only minimization problem. See Appx. B for a detailed discussion of related work from the ML and the optimization lenses. Given the interest from both communities, we address whether a stochastic DRO algorithm can simultaneously 1) achieve a linear convergence rate for any \(>0\) and 2) apply to many common uncertainty sets.

ContributionsWe propose drago, a minibatch primal-dual algorithm for the penalized DRO problem (2) that achieves \(\)-suboptimality in

\[O([+}L}{}+}{}}]())\] (4)

iterations, where \(b\{1,,n\}\) is the minibatch size, \(_{}=nq_{}:=n_{q,i[n]}q_{i}\) measures the size of the uncertainty set, and \(G\) and \(L\) are the Lipschitz continuity parameters of \(_{i}\) and \(_{i}\), respectively. For commonly used parameters of uncertainty sets, \(nq_{}\) is bounded above by an absolute constant independent in \(n\) (see Prop. 3), so for \(d<n\) and \(b=n/d\), we maintain an \(O(n)\) per-iteration complexity (the dual dimensionality) while reducing the number of iterations to \(O((d+_{}L/+d/()}))(1/ ))\). Theoretically, the complexity bound we achieve in (4) is the best one among current penalized DRO algorithms, delineating a clear dependence on smoothness constants of the coupled term and strong convexity constants of the individual terms in (2). Practically, drago has a single hyperparameter and operates on any closed, convex uncertainty set for which the map \(l_{q}\{q^{}l- D(q||/n)\}\) is efficiently computable. drago is also of general conceptual interest as a stochastic variance-reduced primal-dual algorithm for min-max problems. It delicately combines randomized and cyclic updates, which effectively address the varying dimensions of the two problems (see Sec. 2). The theoretical guarantees of the algorithm are explained in Sec. 3. Numerical performance benchmarks are shown in Sec. 4.

## 2 The drago Algorithm

We present here the **D**istributionally **R**obust **A**nular **G**radient **O**ptimizer (drago). While similar in spirit to a primal-dual proximal gradient method with a stochastic flavor, there are several innovations that allow the algorithm to achieve its superior complexity guarantee. These include using 1) minibatch stochastic gradient estimates to improve the trade-off between the per-iteration complexity and required number of iterations (especially when \(n d\)), 2) a combination of randomized and cyclically updated components in the primal and dual gradient estimates, and 3) a novel regularization term in the primal update which reduces variance in the gradient estimate (i.e., _coupled_ variance reduction). Here, we describe the algorithm in a manner that helps elucidate the upcoming theoretical analysis (Sec. 3). On the other hand, in Appx. D, we present an alternate description of drago that is amenable to direct implementation in code.

Notation & TerminologyLet \(:^{n}\{+\}\) be a proper, convex function such that \(():=\{q^{n}:(q)<+\}\). Let \(\) have a non-empty subdifferential for each \(q\), and denote by \(\) a map from \(q\) to an arbitrary but consistently chosen subgradient in \((q)\). We denote the _Bregman divergence_ generated by \(\) as \(_{}(q,)=(q)-()-(),q- \). We employ the Bregman divergence in this way for purely technical reasons, and for common cases this version will not be invoked. Finding a minimizer of (2) is equivalent to finding a saddle-point \((w_{},q_{})\) which satisfies

\[_{q}(w_{},q)=(w_{},q_{} )=_{w}(w,q_{}).\]

[MISSING_PAGE_FAIL:4]

Next, we describe the computation of \(v_{t-1}^{}\) and \(v_{t}^{}\), which will rely on quantities that are stored by the algorithm along its iterations. We store three tables of values \(_{t}^{n}\), \(_{t}^{n}\), and \(^{n d}\), which are approximations of \((w_{t})\), \(q_{t}\), and \((w_{t})\), respectively. Before explaining how these tables are updated, consider the data indices \(\{1,,n\}\) to be partitioned into \(n/b\) blocks, written \((B_{1},,B_{n/b})\) for \(B_{K}=(K-b+1,,Kb)\). On each iteration \(t\), we randomly sample independent block indices \(I_{t},J_{t}[n/b]\) and define

\[v_{t-1}^{} =_{t-1}^{}_{t-1}+}{a_{t}} _{i B_{I_{t}}}(q_{t-1,i}_{i}(w_{t-1})-_{t-2,i}_{t-2,i})\] (8) \[v_{t}^{} =_{t}+}{a_{t}}_{j B _{J_{t}}}(_{j}(w_{t})-_{t-1,j})e_{j}\] (9)

As for the tables of approximations, we update them on each iteration without suffering the \(O(nd)\) computational cost of querying every first-order oracle \((_{1},_{1}),,(_{n},_{n})\). We set \((_{0},_{0},_{0})=((w_{0}),q_{0},(w_{0}))\), and for iteration \(t 1\) update block \(K_{}:=(n/b) t+1\) via

\[(_{t,k},_{t,k},_{t,k})=(_{k}(w_{t}), q_{t,k},_{k}(w_{t}))&k B_{K_{t}}\\ (_{t-1,k},_{t-1,k},_{t-1,k})&k B_{K_{t}} \]

While \(q_{t}\) in particular is always known by the algorithm, we use the approximation \(_{t}\) which is possibly _dual infeasible_ for every \(t 1\), but will approach \(q_{t}\) as \(t\) grows large. Using this approximation is essential to controlling the per-iteration time complexity, as described below. In addition, the design of (8) and (9) is grounded in the long line of work on incremental methods (both deterministic and randomized). The table of past gradients updated cyclically resembles IAG (Blatt et al., 2007), whereas the randomized component resembles methods such as SAGA (Defazio et al., 2014) and stochastic PDHG (Chambolle et al., 2018). The full algorithm description is given in Algorithm 1. In the next section, we show that \((a_{t},C_{t},c_{t})_{t 0}\) can be determined by a single hyperparameter.

**Computational Complexity** We also discuss the per-iteration time complexity and the global space complexity of Algorithm 1, whereas the number of required iterations for \(\)-suboptimality is given in the next section. We see that the per-iteration time complexity is \(O(n+bd)\), as we query \(b\) first-order oracles in both the primal and dual updates and all other operations occur on \(n\)-length or \(d\)-length vectors. While we need to employ \(O(nd)\) operations to compute \(_{t-1}^{}_{t-1}\) in (8) when \(t=1\), this quantity can be maintained with \(O(bd)\) operations in every subsequent iteration as only \(b\) rows of \(_{t}\) and \(_{t}\) are editted in each iteration. For space complexity, we may consider storing the entire gradient table \(_{t}\) in memory, resulting in an \(O(nd)\) complexity. However, due to our time complexity calculation, we may reduce the space complexity to \(O(n+bd)\) by storing only \(w_{t-1},,w_{t-n/b}\) and recomputing the relevant values of \(_{t}\) in (8) in every iteration. Therefore, the use of block-cyclic updates in the historical tables may significantly reduce the space complexity as compared to randomized updates (as in Defazio et al. (2014)).

## 3 Theoretical Analysis

We provide the theoretical convergence rate and global complexity of drago along with technical highlights of the proof which may be of independent interest.

**Convergence Analysis** We measure suboptimality using the _primal-dual gap_:

\[_{t}:=(w_{t},q_{})-(w_{},q_{t})- w_{t}-w_{}_{2}^{2}- q _{t}-q_{}_{2}^{2},\]

where the saddle point \((w^{},q^{})\) exists under Asm. 1. Note that \(_{t} 0\), as it is the sum of non-negative quantities

\[(w_{t},q_{})-(w_{},q_{})-  w_{t}-w_{}_{2}^{2} 0(w_{},q_{})-(w_{},q_{t})-  q_{t}-q_{}_{2}^{2} 0.\]

To state the main result, define \(_{t}\) as the conditional expectation over \((I_{t},J_{t})\) given \((w_{t-1},q_{t-1})\) and \(_{1}\) as the marginal expectation over the optimization trajectory. Consider the following assumptions.

**Assumption 1**.: _Let \(_{1},,_{n}\) be \(G\)-Lipschitz continuous and \(L\)-smooth, in that for all \(i[n]\),_

\[\|_{i}(w)-_{i}(w^{})\|_{2} G\|w-w^{} \|_{2}\|_{i}(w)-_{i}(w^{})\|_{2} L \|w-w^{}\|_{2},\]

_i.e., each \(_{i}\) is \(G\)-Lipschitz continuous and \(L\)-smooth with respect to \(\|\|_{2}\). Let \(>0\) and \(>0\), and let \(q D(q\|_{n}/n)\) be \(1\)-strongly convex with respect to \(\|\|_{2}\). Finally, \(\) is closed, convex, and contains \(/n\)._

Regarding Ass. 1, an example of a loss function satisfying both Lipschitzness and smoothness is given by the Huber loss used in robust statistics (Huber, 1981). Another setting in which both assumptions are satisfied is when the domain \(\) is compact, as smoothness will imply Lipschitz continuity. Compactness is a common assumption when pursuing statistical guarantees such as uniform convergence. As for the assumption of strong convexity, we describe modifications of the algorithm when \(=0\) or \(=0\) in Appx. C.4 along with corresponding changes in the analysis.

**Theorem 2**.: _For a constant \(>0\), define the sequence_

\[a_{1}=1,a_{2}=4,a_{t}=(1+)a_{t-1}t>2,\]

_along with its partial sum \(A_{t}=_{=1}^{t}a_{}\). Under Ass. 1, there is an absolute constant \(C\) such that using the parameter_

\[=C\{,}}, }}\},\]

_the iterates of Algorithm 1 satisfy:_

\[_{t=1}^{T}a_{t}_{1}[_{t}]+}{4}_{1 }\|w_{T}-w_{}\|_{2}^{2}+}{4}_{1}\| q_{T}-q_{}\|_{2}^{2}}{}\|w_{0}-w_{1}\|_{2}^ {2}.\]

_We can compute a point \((w_{T},q_{T})\) achieving an expected gap no more than \(\) with big-\(O\) complexity_

\[(n+bd)(+}}{}+ }{}})().\] (10)

By dividing the result of Thm. 2 by \(A_{T}\), we see that both the expected gap and expected distance-to-optimum in the primal and dual sequences decay geometrically in \(T\). By plugging in \(b=n/d\) we get the following runtime for Algorithm 1:

\[O(nd+}}{}+n^{3/2}d}{ }})().\] (11)Note in particular the individual dependence on the condition numbers \(L/\) and \(nG^{2}/()\), as opposed to the max-over-min-type condition numbers such as those achievable by generic primal-dual or variational inequality methods (see Appx. B.3). The proof of Thm. 2 is provided in Appx. C, along with a high-level overview in Appx. C.1. Our analysis relies on controlling \(a_{t}_{t}\) by bounding \(a_{t}(w_{},q_{t})\) above and bounding \(a_{t}(w_{t},q_{})\) below. Key technical steps occur in the lower bound. We first apply that \(w a_{t}q_{t}^{}(w)\) is convex to produce a linear underestimator of the function supported at \(w_{t}\), and use that \(w_{t}\) is the minimizer of a strongly convex function, so

\[a_{t}(w_{},q_{t})  a_{t}q_{t}^{}(w_{t})+\] (12) \[+}{2}_{=t-n/b}^{t-2} w_{t}-w_{  0}_{2}^{2}\] (13)

Note that the terms above will be negated when combining the upper and lower bounds. The labeled terms are highly relevant in the analysis, with the second being non-standard. By expanding the definition of \(v_{t}^{}\) and adding and subtracting \(w_{t-1}\), we write

\[a_{t}(w_{t})^{}q_{t}-v_{t}^{},w_{}-w_{t} =a_{t}(w_{t})^{}q_{t}-_{t-1}^{ }_{t-1},w_{}-w_{t}\] \[-}{b(1+)}_{i I_{t}} _{i}(w_{t-1})q_{t-1}-_{t-2,i}_{t-2,i},w_{}-w_{t-1}\] \[-}{b(1+)}_{i I_{t}} _{i}(w_{t-1})q_{t-1}-_{t-2,i}_{t-2,i},w_{t-1}-w_{t}.\]

When choosing the learning rate \(\) correctly, the first two terms will telescope in expectation (see Lem. 11). The third term, after applying Young's inequality, requires controlling \(_{i I_{t}}_{i}(w_{t-1})q_{t-1}-_ {t-2,i}_{t-2,i}_{2}^{2}\) in expectation, which we dub the "primal noise bound" (Lem. 5). When we combine the upper and lower bounds, we get a similar inner product term \(a_{t} q_{}-q_{t},(w_{t})-v_{t}^{}\), and mirroring the arguments above, we encounter the term \(_{j J_{t}}(_{j}(w_{t})-_{t-1,j})_{2}^{2}\) which also requires a "dual noise bound" (Lem. 6). Without loss of generality, assume that the blocks are ordered such that

\[_{t-1,i}=_{i}(w_{t-1-K 0})i B_{K}.\] (14)

By computing the conditional expectation \(_{t}[]:=[|w_{t-1}]\), we have that

\[_{t}[_{j J_{t}}(_{j}(w_{t})- _{t-1,j})_{2}^{2}] =_{i=1}^{n}(_{i}(w_{t})-_{t-1,i})_{2} ^{2}\] \[=_{K=1}^{n/b}_{i B_{K}}(_{i}(w_{t})- _{i}(w_{t-1-K 0}))_{2}^{2}\] \[b}{n}_{=t-n/b}^{t-2} w_{t-1}-w _{ 0}_{2}^{2},\]

where the second line follows from (14) and the third from \(G\)-Lipschitzness. This will telescope with the second term introduced in (13), showing the importance of the regularization. The argument follows similarly even when (14) does not hold, as the blocks can simply be "renamed" to achieve the final bound. While the proof is technical, this core idea guides the analysis and the algorithm design.

## 4 Experiments

In this section, we provide numerical benchmarks to measure drago against baselines in terms of evaluations of each component \(\{(_{i},_{i})\}_{i=1}^{n}\) and wall clock time. We consider regression and classification tasks. Letting \((x_{i},y_{i})\) denote a feature-label pair, we have that each \(_{i}\) represents the squared error loss or multinomial cross-entropy loss, given by

\[_{i}(w):=(y_{i}-x_{i}^{}w)^{2}_{i}(w):=-x_{i}^{}w_{y_{i}}+_{y}(x_{i}^{ }w_{y}),\]

respectively. In the latter case, we denote \(w=(w_{1},,w_{C})^{C d}\), indicating multiclass classification with label set \(:=\{1,,C\}\). We show results for the conditional value-at-risk (CVaR) (Rockafellar and Royset, 2013) in this section, exploring the effect of sample size \(n\), dimensionality \(d\), and regularization parameter \(\) on optimization performance. The parameter \(\) in particular has interpretations both as a conditioning device (as it is inversely related to the smoothness constant of \(w_{q}(w,q)\) and as a robustness parameter, as it controls the essential size of the uncertainty set. Detailed experimental settings are contained in Appx. E, including additional experiments with the \(^{2}\)-divergence ball uncertainty set. The code to reproduce these experiments can be found at https://github.com/ronakdm/drago.

We compare against baselines that can be used on the CVaR uncertainty set: distributionally robust stochastic gradient descent (SGD) (Levy et al., 2020) and LSVRG (Mehta et al., 2023). For SGD, we use a batch size of 64 and for LSVRG we use the default epoch length of \(n\). For drago, we investigate the variants in which \(b\) is set to 1 and \(b=n/d\) a priori, as well as cases when \(b\) is a tuned hyperparameter. On the \(y\)-axis, we plot the primal gap

\[}(w_{t},q)-(w_{},q_{ })}{_{q}(w_{0},q)-(w_{},q_{ })}\,,\] (15)

where we approximate \((w_{},q_{})\) by running LBFGS (Nocedal and Wright, 1999) on the primal objective until convergence. On the \(x\)-axis, we display either the exact number of calls to the first-order oracles of the form \((_{i},_{i})\) or the wall clock time in seconds. We fix \(=1\) but vary \(\) to study its role as a conditioning parameter, which is especially important as prior work establishes different convergence rates for different values of \(\) (see Tab. 1).

### Regression with Large Block Sizes

In this experiment, we consider six regression datasets, named yacht (\(n=244,d=6\)) (Tsanas and Xifara, 2012), energy (\(n=614,d=8\)) (Baressi Segota et al., 2020), concrete (\(n=824,d=8\)) (Yeh, 2006), asincome (\(n=4000,d=202\)) (Ding et al., 2021), kin8nm (\(n=6553,d=8\)) (Akujuobi and Zhang, 2017), and power (\(n=7654,d=4\)) (Tufekci, 2014). In each case, there is a univariate, real-valued output. Notice that most datasets, besides asincome, are low-dimensional as compared to their sample size. Thus, the default block size \(n/d\) becomes relatively large, imposing an expensive per-iteration cost in terms of oracle queries. However, when the block size is high, the stochastic gradient estimates in each iteration have lower variance and the table components are updated more frequently, which could improve convergence in principle. The main question of this

Figure 2: **Regression Benchmarks. In both panels, the \(y\)-axis measures the primal suboptimality gap (15). Individual plots correspond to particular datasets. Left: The \(x\)-axis displays the number of individual first-order oracle queries to \(\{(_{i},_{i})\}_{i=1}^{n}\). Right: The \(x\)-axis displays wall-clock time.**

section is whether drago efficiently manages this trade-off via the block size parameter \(b\). Results for gradient evaluations and wall clock time are on the left and right panels of Fig. 2, respectively.

**Results** The drago variant for \(b=n/d\) is not included on the left plot, as the number of queries (almost 2,000 in the case of power) penalizes its performance heavily. Still, the same variant performs best or near best on all datasets in terms of wall clock time (right plot). Thus, if the computation of the queries is inexpensive enough, drago can achieve the lowest suboptimality within a fixed time budget. This is most striking in the case of kin8nm, in which drago achieves \(10^{-7}\) primal gap within 1 second, versus LSVRG which is only able to reach within \(10^{-2}\) of the minimum in the same amount of time. We also experiment with tuning \(b\) to reach a balance between the cost of queries and distance to optimum in the left plot with the \(b=16\) variant. In the datasets with \(n 1,000\), drago can match the performance of baselines with only \(b=1\), whereas in the larger datasets, a batch size of \(16\) is needed to be comparable.

### Text Classification Under Ill-Conditioning

We consider a natural language processing example using the emotion dataset (Saravia et al., 2018), which is a classification task consisting of six sentiment categories: sadness, anger, love, fear, joy, and surprise. To featurize the text, we fine-tune a pre-trained BERT network (Devlin et al., 2019) on a held-out set of 8,000 training examples to learn a vectorial representation. We then use a disjoint subset of 8,000 training points and apply PCA to reduce them to 45-dimensional vectors. Because of the six classes this results in \(d=270\) parameters to learn. To study the effect of dual regularization, we consider \(\{1.0,0.01,0.001\}\). As \(\) decreases, the dual solution may shift further from uniformity, and potentially increase the distributional robustness of the learned minimizer. However, the objective can also become poorly conditioned, introducing a key trade-off between optimization and statistical considerations when selecting \(\). The results are in Fig. 3.

**Results** The run time required for LSVRG to make 500K gradient evaluations is too large to be considered. We also observe that LSVRG is vulnerable to ill-conditioned objectives, as it is outperformed by SGD for smaller values of \(\) in terms of wall clock time. Within 4 seconds, drago can achieve close to a \(10^{-5}\) primal suboptimality gap while the gaps of SGD and LSVRG are 2 to 3 orders of magnitude larger in the same amount of time. We hypothesize that because the dual variables in LSVRG are updated once every \(n\) iterations, the primal gradient estimates may accrue excessive bias. drago with \(b=n/d\), making \( 30\) individual first-order queries per iteration, is performant in terms of oracle queries and wall clock time even as \(\) drops by 3 orders of magnitude.

## 5 Conclusion

We proposed drago, a stochastic primal-dual algorithm for solving a host of distributionally robust optimization (DRO) problems. The method achieves linear convergence without placing conditions on the dual regularizer, and its empirical performance remains strong across varying settings of the sample size \(n\), dimension \(d\), and the dual regularization parameter \(\). The method combines ideas of variance reduction, minibatching, and cyclic coordinate-style updates even though the dual feasible set (a.k.a. the uncertainty set) is non-separable. Opportunities for future work include extensions to non-convex settings and applications to min-max problems beyond distributional robustness, such as missing data imputation and fully composite optimization.

Figure 3: **Text Classification Benchmarks. In all plots, the \(y\)-axis measures the normalized primal (i.e., DRO risk) suboptimality gap, defined in (15). Columns represent a varying dual regularization parameter \(\). On the first three columns the \(x\)-axis measures the number of individual first-order oracle queries to \(\{(_{i},_{i})\}_{i=1}^{n}\) and the remaining three the \(x\)-axis displays wall-clock time. The objective becomes ill-conditioned as \(\) decreases.**

AcknowledgementsThis work was supported by NSF DMS-2023166, CCF-2019844, DMS-2134012, NIH, IARPA 2022-22072200003, U. S. Office of Naval Research under award number N00014-22-1-2348. Part of this work was done while R. Mehta and Z. Harchaoui were visiting the Simons Institute for the Theory of Computing.

Broader ImpactDistributionally robust optimization (DRO) within machine learning is heavily motivated by problems in artificial intelligence (AI) safety, such as mitigating catastrophic performance of models on minority groups or end-users. While this work is focused on theoretical and algorithmic aspects of the problem, we intend to increase accessibility and scalability for downstream applications as well.