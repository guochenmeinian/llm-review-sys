# MKOR: Momentum-Enabled Kronecker-Factor-Based Optimizer Using Rank-1 Updates

Mohammad Mozaffari

Department of Computer Science

University of Toronto

mmozaffari@cs.toronto.edu

&Sikan Li

Texas Advanced Computing Server

sli@tacc.utexas.edu

&Zhao Zhang

Department of Electrical and Computer Engineering

Rutgers University

zhao.zhang@rutgers.edu

&Maryam Mehri Dehnavi

Department of Computer Science

University of Toronto

mmehride@cs.toronto.edu

###### Abstract

This work proposes a Momentum-Enabled Kronecker-Factor-Based Optimizer Using Rank-1 Updates, called MKOR, that improves the training time and convergence properties of deep neural networks (DNNs). Second-order techniques, while enjoying higher convergence rates vs first-order counterparts, have cubic complexity with respect to either the model size and/or the training batch size. Therefore, they exhibit poor scalability and performance in transformer models, e.g. large language models (LLMs), because the batch sizes in these models scale by the attention mechanism sequence length, leading to large model size and batch sizes. MKOR's complexity is quadratic with respect to the model size, alleviating the computation bottlenecks in second-order methods. Because of their high computation complexity, state-of-the-art implementations of second-order methods can only afford to update the second order information infrequently, and thus do not fully exploit the promise of better convergence from these updates. By reducing the communication complexity of the second-order updates, as well as achieving a linear communication complexity, MKOR increases the frequency of second-order updates. We also propose a hybrid version of MKOR (called MKOR-H) that mid-training falls backs to a first order optimizer if the second order updates no longer accelerate convergence. Our experiments show that MKOR outperforms state-of-the-art first-order methods, e.g. the LAMB optimizer, and best implementations of second-order methods, i.e. KAISA/KFAC, up to \(2.57\) and \(1.85\) respectively on BERT-Large-Uncased on 64 GPUs.

## 1 Introduction

Second-order optimization methods have recently gained popularity in the training process of deep neural networks (DNNs) due to their higher convergence rate in comparison to their first-order counterparts. One of the well-known second-order methods, Newton's method, uses the inverse of the Hessian of the objective function as a preconditioner to the gradients, capturing more information on the curvature of the loss function. However, since the size of the Hessian scales with the model size, computing and storing the exact Hessian and its inverse is the main bottleneck in these methods, giving rise to different approximation methods.

One of the most common approximations of Newton's method is Natural Gradient Descent (NGD) , where the Hessian is substituted with the Fisher Information Matrix (FIM)  to reduce the costof computing the second-order derivatives. However, as the models become larger, it becomes impractical to store and compute the exact inverse of the FIM, leading to the design of algorithms based on block-diagonal approximations of FIM; each block corresponds to a layer in the neural network (NN). Inverting the diagonal blocks is also not practical due to the large number of parameters in a NN layer, thus their inverse is also approximated.

The class of Kronecker-Factored Approximate Curvature (KFAC)  methods attempt to reduce the computation costs of the the block inversion. They approximate an FIM block for a batch of samples using the Kronecker multiplication of the covariance of the output activations and input gradients. KFAC is implemented on distributed platforms for both linear and convolutional layers [5; 27; 20; 19], and different computation and communication optimization techniques have been applied to its implementations [25; 21]. KAISA  is a framework with state-of-the-art distributed KFAC implementation. The computational complexity of KFAC-based methods is \((d^{3})\), where \(d\) is the layer dimension. These methods work well for small models with a small layer dimension \(d\), however, in large models, they don't scale well, resulting in poor performance.

To reduce the effect of model size on the computation complexity of second-order updates, KBFGS , which does not have an efficient distributed implementation, uses a Broyden-Fletcher-Goldfarb-Shannon (BFGS) -based method for computing and updating the Kronecker factor and has \((bd^{2})\) complexity, where \(b\) is the batch size. The class of SNGD (Sherman-Morrison-Woodbury-based NGD) methods [23; 31; 17] uses the Sherman-Morrison-Woodbury (SMW) identity to calculate the FIM blocks with a complexity of \((b^{3})\), making the complexity independent of the model size. HyLo  is the state-of-the-art SNGD implementation. It reduces communication for better scalability. KBFGS and SNGD solve the scalability issue of KFAC-based methods for small batches. However, in transformer models , the batch size scales with the sequence length of the attention mechanism (which can be as high as a few thousands ) thus limiting the scalability of SNGD and KBFGS methods. Recent work Eva  attempts to reduce this cost further to \((d^{2})\) by storing vectors instead of Kronecker factors in the KFAC formulation. However, similar to KFAC, Eva uses a damping factor that can lead to additional error in the FIM approximation. Also, because Eva stores the Kronecker vectors instead of factors, it can not leverage the benefits of momentum.

This work presents MKOR, a **M**omentum-Enabled **K**ronecker-Factorizatoin-Based **O**ptimizer with **R**ank-1 Updates. _(1)_ MKOR approximates the inverse of covariance matrices using rank-1 updates in the _Sherman-Morrison-Based (SM-Based) Matrix Inversion_, reducing the inversion computation complexity from \((d^{3})\) to \((d^{2})\). As a result, the second-order information updates can be applied up to 100 times more frequently compared to KAISA/KFAC and HyLo/SNGD. KFAC computes and inverts the covariance matrices precisely; KFAC updates the second-order information infrequently, e.g. every 100-1000 iterations, due to the high cost of inversion and for time efficiency, which damages its convergence rate and generalization capability. SNGD-based methods suffer from similar overhead due to inversion of their kernel matrices. _(2)_ Second-order methods suffer from high communication costs for synchronizing the inverse of factors between workers. MKOR alleviates this by only synchronizing rank-1 approximation vectors among the workers, reducing the communication costs from \((d^{2})\) to \((d)\). Also, MKOR uses half-precision to further reduce communication; other second-order methods cannot use low-precision computations due to their complex matrix inversion algorithms. _(3)_ Second-order methods are more prone to the exploding gradients due to the effects of preconditioning on the norm of the gradients and lack of numerical bounds on the inverse of the factors. MKOR uses a _Norm-Based Stabilizer_ and a _Gradient Rescaling Mechanism_ to detect and prevent exploding gradients. _(4)_ The high convergence rate of second-order methods, including MKOR, stagnates after the first few iterations or epochs of training. We propose a hybrid version of MKOR, namely MKOR-H, that combines the high convergence rate of second-order methods in the initial phase of training with the low overhead of first-order methods in the late stages of the training, using a loss-reduction-rate-based switching mechanism.

MKOR outperforms state-of-the-art distributed second- and first-order methods by up to \(2.57\), reducing the training time of BERT-Large-Uncased from 8 hours to 3 hours on 64 A100 GPUs. MKOR also achieves new state-of-the-art metrics on the GLUE dataset, where other second-order methods such as KFAC fail to converge to the baseline.

## 2 Background

Training a neural network involves solving an optimization problem to find the optimal values for a set of weights \(=\{W^{m}\}_{m=1}^{M}\), where \(M\) is the number of layers in the network and \(W^{m}\) is a matrix in \(^{d d}\). Second-order methods precondition the weights of the network with the inverse of the Hessian for better convergence rates. Block-diagonal approximations of NGD methods replace the Hessian with the block-diagonal FIM as shown in Equation 1, where \(w^{m}^{d^{2}}\) is the vector representation of \(W^{m}\), \(F^{m}\) is the block corresponding to that layer and \(\) is the loss function.  Shows that the FIM matches the Gauss-Newton matrix under certain conditions.

\[w^{m}:=w^{m}-(F^{m})^{-1}_{w^{m}}\] (1)

KFAC-based methods reformulate the FIM block as the Kronecker product of two matrices. Equation 2 shows the update rule in KFAC, where \(\) is the loss function and \((L_{t}^{m})^{-1}\) and \((R_{t}^{m})^{-1}\) are the inverse of the left and right factors, respectively.

\[W^{m}:=W^{m}-(L_{t}^{m})^{-1}_{W^{m}}(R_{t}^{m})^{-1}\] (2)

\((L_{t}^{m})^{-1}\) and \((R_{t}^{m})^{-1}\) in Equation 2 are computed using equations 3 and 4, respectively, where \(a^{m}\) is the activation value of a sample at layer \(m\), and \(g_{m}=_{a^{m-1}}\) and \(\) incorporate the momentum feature to avoid extreme changes in the factors.

\[L_{t}^{m}= L_{t-1}^{m}+(1-)[g_{t}^{m}g_{t} ^{mT}]\] (3) \[R_{t}^{m}= R_{t-1}^{m}+(1-)[a_{t}^{m-1}a_{ t}^{m-1}{}^{T}]\] (4)

## 3 MKOR: Momentum-Enabled Kronecker-Factorization-Based Optimizer with Rank-1 Updates

In this section we first present the MKOR algorithm, its computation and communication complexity, then present hybrid MKOR (MKOR-H), and finally discuss MKOR's convergence and stability.

Figure 1: MKOR for layer \(m\) on a single worker. The inputs of MKOR are the activations \(A_{t}^{m}\), the gradients of the loss function with respect to the inputs \(G_{t}^{m}\), and the gradients of the loss function with respect to the weights \(_{W^{m}}\). The output is the update values \( W^{m}\).

### The MKOR Algorithm

Algorithm 1 summarizes the MKOR optimizer for a single layer and Figure 1 shows the workflow. For each layer (line 1 in Algorithm 1) MKOR updates the second-order information and preconditions the gradients, and at the end the backend optimizer updates the weight using the preconditioned gradients (line 14 in Algorithm 1).

_Rank-1 Approximation._ For the rank-1 approximations of the covariance matrices, we use the average of the values across all the samples, i.e. \(^{m-1}}=[a_{t}^{m-1}]\) and \(^{m}}=[g_{t}^{m}]\) (lines 2 and 3 in Algorithm 1 and Figure 1-a). \((A_{t}^{m-1})_{:,i}^{-1}\) and \((G_{t}^{m})_{:,i}^{-1}\) show the \(i^{th}\) column of \((A_{t}^{m-1})^{-1}\) and \((G_{t}^{m})^{-1}\) respectively, where \(A_{t}^{m-1}\) and \(G_{t}^{m}\) are the activations and the gradients of layer \(m\) respectively.

_Norm-Based Stabilizer._ The values in the factor inverses in second-order methods can become large or vanish due to extremely large or small values in activations and gradients, leading to numerical instabilities and over/underflows. Since the inverse of the factors are directly multiplied by the gradients to find the update values, it can cause oscillations or even divergence. MKOR uses a norm-based stabilizer to detect this and addresses it by modifying the inverse of the factors accordingly (lines 5 and 6 in Algorithm 1 and Figure 1-b). More detail on the norm-based stabilizer are in Section3.3.

_SM-Based Inverter._ MKOR directly modifies the inverse of the left and right factors using rank-1 updates, while using the momentum for better convergence. If \([g^{m}g^{m^{T}}]\) is approximated using a rank-1 matrix \(g^{m^{T}}}\) and using the Sherman-Morrison identity, Equation 5 in obtained (line 7 in Algorithm 1 and Figure 1-c).

\[L_{t}^{m-1}=^{m}}^{-1}+(1+(1- )^{m^{T}}}{L_{t-1}^{m}}^{-1}^{m}})}{L_{t-1}^{ m}}^{-1}^{m}g_{t}^{m^{T}}}{L_{t-1}^{m}}^{-1}\] (5)

Furthermore, if Equation 4 is approximated using \([a_{t}^{m-1}{a_{t}^{m^{T}}}^{T}]^{m}a_{t}^{m^{T}}} ^{m^{T}}}\) with a similar derivation, Equation 6 is obtained (line 8 in Algorithm 1 and Figure 1-c).

\[R_{t}^{m-1}=^{m}}^{-1}+(1+(1-) ^{m}}^{T}{R_{t-1}^{m}}^{-1}^{m}})}{R_{t-1}^{m}}^{-1} ^{m}}^{m}}^{T}{R_{t-1}^{m}}^{-1}\] (6)

_Rescaling Gradients._ Preconditioning the gradients using the computed factors can change gradient norms. Sometimes, these changes interfere with the effect of the learning rate on the training process. To alleviate this and to make learning rate schedulers more effective, the preconditioned gradients are scaled so that their norm matches the original norms (line 10 in Algorithm 1 and Figure 1-d).

**Complexity Analysis.** MKOR reduces the memory, communication, and computation costs for factor inversion. Table 1 compares the overheads of different optimizers. _(1) Computation Complexity._ MKOR inverts the left and right factors in Equation 2 using equations 5 and 6, both of which can be computed using matrix-vector multiplications, and have \((d^{2})\) computation complexity, in contrast to KFAC and SNGD methods that need \((d^{3})\) and \((b^{3})\) complexity to invert matrices in \(^{d d}\) and \(^{b b}\) respectively. _(2) Communication Complexity._ The only data that is synchronized among different workers in MKOR is the two rank-\(1\) approximations that have \(2d\) elements. With quantization, this size can be halved. In KFAC, the activation and gradient covariance matrices and the inversion of left and right factors need to be synchronized between all the workers, leading to \(4d^{2}\) data transfers. In SNGD, the activations and gradients are synchronized, leading to \(2bd\) data transfers and the inverted kernels are broadcast, resulting in \(b^{2}\) data transfers. Reducing the communication complexity of MKOR from quadratic to linear results in better performance on large number of workers. _(3) Memory Overhead._ MKOR needs to store the inverse of the left and right factors and two rank-\(1\) approximation vectors, leading to \(2d^{2}+2d\) memory overhead, and using half-precision computations further reduces this. KFAC stores the activation and gradient covariance matrices and the left and right factors, leading to \(4d^{2}\) memory overhead. SNGD stores the activations, the gradients, and the kernels they use as second-order information, leading to \(2bd+b^{2}\) memory complexity.

The low memory overhead of MKOR allows us to use larger batch sizes compared to other second-order methods. In practice, gradient accumulation methods are used to increase the effective batch size in training, which reduces the training speed significantly. This issue worsens with KAISA and HyLo, but MKOR alleviates this. For fairness, in our experiments, we set the local batch sizes to the same value in all optimizers and do not leverage this feature of MKOR.

### Hybrid MKOR

We observed that second-order methods, including MKOR, usually accelerate training more during the first iterations of the training time, and as the loss flattens, their advantage over their first-order counterparts becomes less noticeable. This is because the second-order information of the loss functions approach identity near convergence points. Thus we designed a hybrid second- and first-order optimizer with a loss decrease rate-based switching method (MKOR-H). MKOR-H evaluates the changes in the loss function in different iterations and switches back to first-order methods if needed for an efficient trade-off between the costly second-order updates and their benefits for convergence.

  Optimizer & Computational Complexity & Memory Overhead & Communication Complexity \\  MKOR & \((d^{2}+bd)\) & \((2d^{2}/2)\) & \((2d/2)\) \\  SNGD (HyLo) & \((b^{3})\) & \((2bd+b^{2})\) & \((2bd+b^{2})\) \\  KFAC (KAISA) & \((d^{3})\) & \((4d^{2})\) & \((4d^{2})\) \\  Eva & \((d^{2}+bd)\) & \((2d)\) & \((2d)\) \\  SGD (Momentum) & - & \((d^{2})\) & - \\  ADAM / LAMB & - & \((d^{2})\) & - \\  

Table 1: The computation and communication complexity and memory overhead of the state-of-the-art implementations of the first- and second-order optimizers. The division by 2 in MKOR is because MKOR uses half-precision computations. The complexity of KFAC-based methods depends on layer dimensions while SNGD methods mostly depend on the batch size. In transformers, due to the scaling of the batch size by the sequence length, batch sizes and layer dimensions are comparable, making both KFAC- and SNGD-based methods more expensive than SGD.

### MKOR Convergence and Stability

**Inversion Frequency** Due to the high factor inversion costs in KFAC- and SNGD-based methods, researchers use the stale factor approach, which updates the inverted factors every \(f\) iterations and reuses the results in the other iterations in their preconditioning to reduce the computation and communication costs. \(f\) is the reciprocal of inversion frequency and varies from a few 100s to a few 1000s. Our experiments show that in average-sized models such as ResNet-50 , in an iteration that the inversion of factors is executed, the cost of KAISA and HyLo is \(150\) more than an SGD iteration. Furthermore, more than 98% of the total cost in those iterations are spent on matrix inversion.

The stale factors approach can lead to good preconditioners if the loss function landscape does not vary significantly in each iteration. However, this is a strong assumption and doesn't necessarily hold in practice. Also, increasing the inversion frequency can benefit the convergence rate of the second-order methods. In addition, our experiments show that using stale factors. The stale factors can be good preconditioners lead to converging to local minima in the loss function and damage the generalization of the model.

**Numerical Stability** In second-order techniques, we need to invert or find the roots of matrices of different sizes, which are usually not full-rank, resulting in numerical issues. The KFAC implementation uses singular value decomposition (SVD) of the factors and masks the eigenvalues that are close to zero to deal with singular matrix inversion issues. In practice, the eigenvalues of the left and right factors in KFAC-based methods computed from equations 3 and 4 are increased manually by adding \( I\) to each of them to improve numerical stability (\(>0\) is called the damping factor), but MKOR doesn't need such numerical fixes. Furthermore, HyLo uses two decomposition methods to sample the batch of inputs, namely KID and KIS. KID requires inverting matrices in \(^{b b}\) of rank \(min(b,d)\), thus for batch sizes larger than \(d\) in a specific layer, the method fails.

Unlike SVD or other iterative methods used for factor inversion, MKOR doesn't suffer from numerical instabilities that rise from large condition numbers. MKOR has a single scalar division, in which the denominator is guaranteed to be non-zero based on lemma 3.1, eliminating the numerical over/under-flow possibility and the need for damping factors (required by other second-order methods for computational stability).

**Lemma 3.1**.: _The factors computed using Equation 5 and 6 are all positive-definite._

 suggests using double precision representation of numbers to avoid numerical over/under-flow in inverting or computing the roots of matrices. This approach adds more costs to the matrix inversion and increases the time complexity of the main bottleneck in second-order methods.

MKOR does not need higher precision computations, and can use half-precision floating point operations to reduce costs significantly. This will improve the memory utilization and reduce the communication costs in GPUs by \(2\) while using cheaper computation blocks for half-precision operations. Lemma 3.2 shows an upper bound on the quantization error effect in the MKOR updates.

**Lemma 3.2**.: _Assuming that the maximum quantization error is \(\), the maximum number in matrices and vectors is \(m\), and the dimension of the vectors and matrices are \(d\) and \(d d\) respectively, the quantization error of formulas 5 and 6 is \(O((+4}m^{3}d^{2}))\)_

**Exploding Gradients Problem** In second-order methods, where the gradients are preconditioned by various factors, the exploding gradient problem is worsened. Our experiments show that in first-order methods, by choosing a learning rate that doesn't lead to divergence in the first few iterations, explosion in gradients almost never occurs. On the other hand, in second-order methods, we observe that the explosion can occur at any iteration, and both KFAC and SNGD implementations are prone to this problem. This can lead to ripples in accuracy and divergence. One of the main approaches for solving the exploding gradient problem is choosing small values for the learning rate, limiting the convergence rate significantly. In particular, small learning rates damage the second-order methods and make them almost as performant as their first-order counterparts. Considering that SGD is more robust against the exploding gradients and taking advantage of the direct control of MKOR on the inverse of the factors, the factors in MKOR are modified to lean toward SGD once the possibility of exploding gradients is detected using equations 7 and 8, where \(\) is a hyperparameter that controls the amount of information from the original factors that needs to be saved in the new factors.

\[^{m}}= L_{t}^{m}+(1-)I\] (7)\[R_{t}^{m}= R_{t}^{m}+(1-)I\] (8)

By expanding Equation 2 with the new factors, we will get Equation 9, which reduces the loss-based on lemma 3.3. The first term in the right-hand-side of 9 is the KFAC term, the second and third terms are the left and right preconditioned versions, and the last term is the SGD term.

\[^{m^{-1}}_{W^{m}}^{m^ {-1}}=&^{2}L^{m^{-1}}_{W^{m}}R^{m^{-1}}\\ &+(1-)L^{m^{-1}}_{W^{m}}+(1- )_{W^{m}}R^{m^{-1}}+(1-)^{2}_{W^{m}} \] (9)

**Lemma 3.3**.: _Given a differentiable function \((w)\) with first-order Taylor series approximation \(}(w- w)=(w_{0}) w^{T}_{w} (w_{0})\) around point \(w_{0}\), assuming that at point \(w_{0}\) the second-order derivative of the function \((w)\) is given as \(_{w}^{2}(w_{0})=H=L R\), where \(L\) and \(R\) are positive-semi-definite matrices, for a value of \( w=(( L^{-1}+(1-)I)( R^{-1}+(1-)I)) (w_{0})\), the inequality \(}(w_{0}- w)<(w_{0})\) holds._

While this modification can avoid exploding gradients, overusing it with small values of \(\) will convert MKOR to SGD. MKOR uses a factor norm-based metric that observes the infinity norm of the factors, and if they are greater than a specific threshold, the process of factor modification will be triggered.

## 4 Experimental Results

In this section, we demonstrate the performance of MKOR on a large language model using different benchmarks, and analyze the timing of different components in different first- and second-order algorithms. For results on more models and training sets, please refer to the supplementary material.

**Large Language Models.** We pre-train BERT-Large Uncased and fine-tune it for different question-answering and text classification tasks. A setup similar to  for pre-training and fine-tuning is used. The first-order baseline used is Fused LAMB . Similar to , for the pre-training process, the English Wikipedia  and the Toronto BookCorpus  dataset, which was used in the original BERT pre-training, are used; the latter dataset is not thoroughly available which results in a small reduction in the baseline accuracies achieved in our experiments from the original BERT results. Following , due to the time-intensive process of hyperparameter tuning for the first phase of pre-training, we report the effectiveness of MKOR in the second phase of pre-training only while using the checkpoints of the first phase generated using LAMB optimizer.

As expected, the computation, communication, and memory complexity of HyLo is high, and the Khatri-Rao-based Interpolative Decomposition (KID) approximation method, the main idea of HyLo, cannot be executed because a single sample cannot fit into the 40GB memory of an A100 GPU. In addition, HyLo doesn't support gradient accumulation due to its memory complexity, depending on the batch size; in LLMs such as BERT, the batch sizes are as large as 64k.

For the question answering task, we fine-tune the pre-trained BERT checkpoints on the SQuAD v1.1  dataset. Table 2 shows the F1 Score achieved using different optimizers and compares their convergence rate and speedups. The vanilla MKOR and KAISA both converge after 1000 iterations, while the LAMB optimizer requires \(1,536\) steps. Considering that each step in MKOR is faster than KAISA, MKOR achieves an end-to-end speedup. MKOR-H will converge in 600 steps, reducing the number of steps in LAMB by \(2.6\), while achieving the same accuracy. In addition, it achieves \(2.57\) speedup over the LAMB optimizer and \(1.75\) speedup over KAISA. As another second-order baseline, we consider Eva, which converges in 1000 iterations, and MKOR achieves \(1.69\) speedup over it.

For classification tasks, we fine-tune BERT on the GLUE  dataset. Table 3 compares the results for different classification tasks in the GLUE dataset. MKOR with 1500 steps achieves a new state-of-the-art accuracy in GLUE dataset on BERT-Large Uncased, and MKOR and MKOR-H with 600 steps achieve the same average metric as the baseline, while reducing the number of steps by a factor of \(2.6\). MKOR and MKOR-H both achieve \(2.57\) end-to-end speedup. After training KAISA for 1,563 steps, the model does not converge to the baseline average accuracy, while slowing down the convergence by \(0.89\). Eva requires 1000 steps to converge to the target average metric, being \(1.69\) slower than MKOR-H with 600 steps and \(1.24\%\) less accurate than MKOR with 1500 steps.

Per figure 2, which shows the training error during the training of BERT, MKOR decreases the error in less iterations in comparison to KAISA, Eva, and LAMB, leading to faster convergence. From Tables 2 and 3, MKOR-H converges in only 600 steps.

**Inversion Frequency.** Due to the low computation complexity of the updates on MKOR, the factor inversion frequency (\(f\)) in MKOR is in the range of 10. Figure 4-a shows that while the average iteration cost in KAISA is heavily dependent on the inversion frequency, MKOR's cost is almost independent of the inversion frequency. Also 4-b shows that increasing the inversion frequency leads to higher convergence rate. In addition, using stale factors may result in converging to a local minima. Hence, in MKOR we increase the convergence rate by updating the factors more frequently, without affecting the per-iteration cost, leading to end-to-end speedups in training. We use a simple autoencoder  on CIFAR-100  in this experiment.

**Performance Analysis.** We compare the performance of different parts of the optimizers to illustrate the bottlenecks and advantages of different methods. The training process for an optimizer has three steps: factor computation, precondition, and update weights. Figure 3 shows the time spent on each task in different optimizers on two models; ResNet-50, a CNN and BERT-Large-Uncased, a transformer-based LLM with large sequence length. Since first-order optimizers such as SGD, ADAM, and LAMB don't require factorization and preconditioning, their optimization time is only spent in updating the weights. In ResNet-50, since the model size is larger compared to the batch size, the factor computation and inversion is more expensive for KAISA compared to HyLo. This cost is significantly reduced in MKOR.

For BERT-Large-Uncased, because of the large size of the model, the factor inversion time for KAISA is large. Also, due to the large sequence length value in this model, the kernel inversion time for

  Metric & LAMB & KAISA & MKOR & MKOR-H & Eva \\  F1 & 90.44 & 90.44 & 90.50 & 90.64 & 90.55 \\  \# Iterations & 1,536 & 1,000 & 1,000 & 600 & 1,000 \\  Time (h) & 7.97 & 5.71 & 5.25 & 3.10 & 5.24 \\  Speedup (\(\)) & 1.00 & 1.39 & 1.51 & 2.57 & 1.52 \\   BERT-Large-Uncased Training Loss

Table 2: BERT-Large Uncased results on SQuAD v1.1 question answering task

  Optimizer & Iterations & Time (h) & Speedup (\(\)) & Average Metric \\  LAMB & 1,563 & 7.97 & 1.00 & 0.8023 \\  KAISA & 1,563 & 8.93 & 0.89 & 0.796 \\  MKOR & 1,500 & 7.88 & 1.01 & 0.8214 \\  MKOR & 600 & 3.10 & 2.57 & 0.8078 \\  MKOR-H & 600 & 3.10 & 2.57 & 0.811 \\  Eva & 1000 & 5.24 & 1.52 & 0.809 \\  

Table 3: BERT-Large Uncased results on the GLUE classification tasks.

Figure 2: The training loss of BERT-Large-Uncased using different optimizers.

HyLo is comparable to KAISA's inversion time. But as expected, because of its low computational complexity, the aforementioned cost in our method is much smaller than the total training time, leading to speedups. It is important to note that HyLo diverges in this training process, hence convergence time is not reported for HyLo.

**Approximation Error Experimental Results.** Due to the low-rank properties of the covariance matrices, MKOR utilizes rank-1 approximations of the covariance matrices to accelerate the computations and communication in KFAC-based optimizers. Here, we aim to theoretically and experimentally support this choice. As shown in Figure 5, our experiments show that the covariance matrices can be approximated with rank-1 matrices with low error and higher rank approximations are unnecessary in practice. Figure 5 shows the error distribution of the optimal rank-1 approximation methods of the covariance matrices in ResNet-50 and BERT-Large-Uncased pre-training. Our extensive tests

Figure 4: The sensitivity of MKOR and KAISA for BERT-Large-Uncased and an Autoencoder model (**a**) and the effect of inversion frequency on the convergence properties of these models (**b**).

Figure 3: Per-step breakdown of different optimizes on BERT-Large-Uncased (**a**) and ResNet-50 (**b**)

on well-known benchmarks show this property holds for all models and we have not come across a benchmark that does not have low-rank covariance matrices.

**Approximation Error Analysis and Extension to Higher Ranks.** Small batch sizes and over parameterization of networks will lead to low-rank covariance matrices in DNNs. Let's consider the covariance matrix \(C=XX^{T}\), where \(C R^{d d}\) is the covariance matrix and \(X R^{d b}\) is a matrix in which each column corresponds to a single sample and \(d\) and \(b\) are the sample dimension and the per-GPU batch size respectively. Rank of the covariance matrix is \(min(b,d)\). If the per-GPU batch sizes are small, the covariance matrices in each GPU will be low-rank. Rank-1 approximation methods can work well in these scenarios. If the batch sizes in each GPU are large, we observe that the covariance matrices will stay low-rank. The underlying reason for this observation is that current neural networks are over-parameterized, and as a result, different features in the covariance matrices of the activations and the output gradients won't be linearly independent, resulting in low-rank covariance matrices.

_Extending MKOR to Higher Ranks:_ Furthermore, one can extend MKOR to use higher-rank covariance matrices. Let's assume that \(C=_{i=1}^{r}c_{i}c_{i}^{T}\) where \(r\) is the rank of the covariance matrix \(C\). We can apply SMW identity to compute \(C^{new}_{2}=(C^{old}+c_{1}c_{1}^{T})^{-1}\) with \(O(d^{2})\) computational complexity. Then we can compute \(C^{new}_{2}=(C^{new}_{1}+c_{2}c_{2}^{T})^{-1}\) using SMW identity with \(O(d^{2})\) computational complexity. We can continue the same pattern by computing \(C^{new}_{i}=(C^{new}_{i-1}+c_{i}c_{i}^{T})^{-1}\). The total computation complexity of this process will be \(O(rd^{2})\). We should add this cost to the cost of computing the low-rank approximation of \(C\) which requires an SVD. Using SVD kills the main advantage of using low-rank computations, since the computational complexity of applying SVD is the same as inverting the factors directly. We could not find any cheaper way to compute low-rank approximations of the covariance matrices, except for the rank-1 approximation used in this paper.

## 5 Conclusion

We propose MKOR, a Momentum-Enabled Kronecker-Factor-Based Optimizer Using Rank-1 updates that improves the end-to-end training time and convergence rate of second-order methods by reducing their computation, communication, and memory usage complexity. Our experiments illustrate that MKOR outperforms state-of-the-art first- and second-order optimizers on large language models.

Figure 5: Rank-1 error for activation and input gradient covariance matrices for BERT-Large-Uncased pre-training **(a, b)** and ResNet-50 on ImageNet **(c, d)**.

Broader Impact

The research described in this paper introduces a novel method for training DNNs that achieves faster convergence in LLMs. Using our work can help save a lot of energy and time for machine learning practitioners. The computations in this work are fully transparent. DNNs can be applied to different problems, including medical diagnostics, voice recognition, and addressing climate changes. However, it should be acknowledged that optimization algorithms for DNNs can also be applied to models with potentially negative implications such as privacy invasion. It is important to note that the responsible and ethical utilization of any efficient optimization algorithm, including those we propose, extends beyond the scope of this research.

## 7 Acknowledgments

We thank the reviewers for their constructive feedback. We would like to express our deepest appreciation to Dr. Behrooz Zarebavani for all the time and energy he put into helping with the writing and formatting of this paper. This work is supported by NSERC Discovery Grants (RGPIN06516, DGECR00303, RGPIN-2023-04897, DGECR-2023-00133), National Science Foundation (NSF CCF-2106621), the Canada Research Chairs program, the Ontario Early Researcher Award, the Digital Research Alliance of Canada (https://www.alliancecan.ca) and Texas Advanced Computing Center (https://www.tacc.utexas.edu). Work of Zhao Zhang was supported by OAC-2106661.