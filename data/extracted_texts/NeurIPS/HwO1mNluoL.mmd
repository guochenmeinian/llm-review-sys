# Mitigating Biases in Blackbox Feature Extractors

for Image Classification Tasks

Abhipsa Basu  Saswat Subhajyoti Mallick  R. Venkatesh Babu

Vision and AI Lab, Indian Institute of Science, Bangalore

Work done while working as a project assistant at the Indian Institute of Science

###### Abstract

In image classification, it is common to utilize a pretrained model to extract meaningful features of the input images, and then to train a classifier on top of it to make predictions for any downstream task. Trained on enormous amounts of data, these models have been shown to contain harmful biases which can hurt their performance when adapted for a downstream classification task. Further, very often they may be blackbox, either due to scale, or because of unavailability of model weights or architecture. Thus, during a downstream task, we cannot debias such models by updating the weights of the feature encoder, as only the classifier can be finetuned. In this regard, we investigate the suitability of some existing debiasing techniques and thereby motivate the need for more focused research towards this problem setting. Furthermore, we propose a simple method consisting of a clustering-based adaptive margin loss with a blackbox feature encoder, with no knowledge of the bias attribute. Our experiments demonstrate the effectiveness of our method across multiple benchmarks. The code is publicly available at [https://github.com/abhipsabasu/blackbox_bias_mitigation](https://github.com/abhipsabasu/blackbox_bias_mitigation).

## 1 Introduction

Deep learning models are to known to inherit harmful stereotypical biases with respect to the different genders, races, cultures, from the datasets they are trained on . For example, a model trained on a gender-biased dataset with images of people having blond and non-blond hair may wrongly learn a correlation between the label 'blond hair' and the gender of the person in the image. Thus the model fails to classify images belonging to minority groups (in this case blond males and non-blond females). These biases can affect the performance of AI systems handling job recruitment, e-commerce, health care, face detection and recognition . Models can also learn spurious correlations between irrelevant training features and the target labels, instead of focusing on the relevant ones . Several works focus on mitigating such biases from trained models in a variety of tasks .

In recent times, large-scale models, pretrained on enormous amounts of data, are being used by machine learning practitioners as feature encoders for numerous downstream applications, as their features are shown to be semantically rich . _However, do these powerful pretrained features themselves exude the harmful stereotypical biases that are known to affect traditional deep learning systems?_ A previous work  sheds light on these questions (when the pretrained model can be fully finetuned on the downstream dataset) - the downstream models finetuned on top of pretrained models can inherit their biases and such biases can be mitigated simply by manipulating the finetuning data.

In this paper, we investigate a more constrained yet practical problem setting. With billion-parameter models gaining impetus in today's world, it may not be feasible to finetune/retrain such models due to scarcity of resources or unavailability of model weights for privacy concerns .

Hence, for downstream classification tasks, simply the classifier layer on top of the pretrained features is trained on the corresponding dataset, keeping the rest of the network frozen. We ask, _to what extent can such systems be debiased, given that the pretrained model weights are unavailable?_

For the scope of this paper, we assume that the downstream dataset consists of multiple groups (e.g. blond males, non-blond males, blond females, non-blond females) due to the presence of a certain bias attribute (e.g. gender). We find that the effect of the bias in the downstream model depends on how well the pretrained feature encodes the target attribute as compared to the bias attribute. The system remains unbiased if the pretrained features are highly aware of the target attribute (even when the bias correlation in the downstream dataset is high). However, if the pretrained features instead predominantly encode the bias attribute, the downstream system becomes biased. A simple solution to the problem is to group-balance, or reduce the bias-correlation of the finetuning dataset, similar to the suggestion of Wang et al. . However, as manipulating the bias correlation of a dataset is not straightforward, we advocate for a specific strategy to debias the system in the given scenario.

To simplify the bias-mitigation task, we insert a trainable adapter module between the pretrained feature extractor and the classifier. With such a setting, after evaluating existing methods, we find that a large number of these methods do not yield the expected performance gains compared to the ERM-trained model, indicating the challenges in the problem setting as well as the necessity of designing specific debiasing strategies. To aid the mitigation process, we forcefully amplify the bias in the downstream model into the adapter (following previous works ), and then investigate a number of simple debiasing techniques utilizing this biased adapter. Finally we propose a method involving clustering-based adaptive margin loss which first clusters the biased feature space, and accordingly sets the margin value for a given sample such that it is inversely proportional to the frequency of the sample's ground truth class in the cluster it belongs to. The problem setting is depicted in Fig. 1. We summarize our key contributions in this paper below:

* We highlight a practical yet under-explored problem setting on how harmful biases creep into a model when it uses pretrained but blackbox feature extractors to obtain features.
* We explore the scenarios in which biases can propagate from the pretrained features to the downstream tasks and demonstrate the necessity and feasibility of debiasing strategies in such cases.
* To debias, we first amplify the bias learnt by the model from the downstream data and propose a simple mitigation strategy by utilizing the identified biases and applying an adaptive margin loss. Extensive experiments show that the proposed method is effective across multiple benchmarks.

## 2 Related Works

**Known biases**: Mitigation of biases with the knowledge of the bias attributes, as well as their labels, is a widely explored problem . Roh et al.  propose a sampling algorithm by formulating a combinatorial optimization problem for unbiased sample selection. While Sagawa et al.  minimize worst-case training loss over the groups formed using the available bias labels and the target class labels, Zhang et al.  disentangle the bias and target representations and debias the network using a mutual information estimator. Some methods adopt a semi-supervised

Figure 1: **Summary of our setup. A frozen pretrained encoder is used as a feature extractor for a downstream task. An adapter is attached on top of the encoder that learns the bias in the downstream dataset. Finally, the bias is mitigated with the help of an adaptive margin loss, leading to unbiased predictions. No knowledge of the bias attribute is assumed apriori.**

approach, where bias attributes are annotated for only a few data samples [20; 31]. Another body of work assumes knowledge of the bias attribute but not its specific labels [32; 33; 34; 35].

**Unknown biases**: Recent works consider the more practical scenario of unavailability of the knowledge of the bias attribute as well as its labels [36; 37; 38; 39; 40; 19; 41; 42; 43]. This area encompasses a wide variety of works, a subset of which is described here. JTT  debiases models using misclassifications from an ERM-trained model. Some approaches like BPA and GEORGE employ clustering to identify the biases in the dataset [44; 45]. Certain works employ the Generalized Cross-Entropy (GCE) loss [46; 47] to amplify the biases learnt by the network and then debias it. While most of these works, like LfF [20; 37; 48], employ two branches in the network, with one branch over-learning the bias and the other debiasing it, Ahn et al.  utilize the GCE loss to learn the bias, and then find the per-sample gradient of the trained model to obtain a balanced dataset. DebiAN  identifies multiple biases by having a discoverer model which optimizes an equal opportunity violation loss. Correct-n-Contrast  identifies training samples having the same class labels but dissimilar bias features via Empirical Risk Minimization (ERM) training and then applies contrastive loss-similar to Contrastive Adaapter -to bring the target features of these same-class samples closer. Patient et al  propose a sampling method to reduce dataset biases. Kim et al.  use a _committee_ of auxiliary classifiers to identify the biases in the network, assigning large weights to the identified bias-conflicting samples during the training of the main classifier. Jeon et al.  show how bias in a CNN network is more pronounced at the top layers, and hence, leveraging features from the lower layers can help the model to exploit less biased representations. Kirichenko et al.  observe that retraining only the last layer is enough for having an unbiased model. However, they require bias annotations in the \(2^{nd}\) stage of training. This is avoided by LaBonte et al.  by constructing a reweighting dataset using model disagreements for the second stage of training.

**Biases in pretrained models**: Utilizing pretrained feature extractors for downstream tasks is a common trend in recent times. Such encoders may carry biases that crept in from the datasets used for training them - many recent works focus on quantifying the fairness of such pretrained models . Srinivasan et al.  study biases in multimodal vision-language systems. Goyal et al.  shows how pretrained models trained with self-supervision exhibit lesser biases than those trained with supervision. Recent works show that biases of pretrained models can be reduced by manipulating the fine-tuning dataset in both Computer Vision and NLP [14; 59]. Salman et al.  show that bias transfer happens from pre-trained models to the downstream tasks, even when the target data is unbiased. Our problem setting on the other hand consumes features from a blackbox pretrained encoder and aims to mitigate the biases arising from that encoder. This is challenging, especially when the bias annotations are not available.

## 3 Problem Statement and Methodology

### Preliminaries and Problem Setting

This work focuses on the task of image classification. Let \(=\{x_{1},x_{2}, x_{N}\}\) be a set of training images of size \(N\), and \(\{y_{1},y_{2},,y_{N}\}\) be the corresponding labels, where each \(y_{i}\). Each data point \((x_{i},y_{i})\) is associated with a hidden spurious attribute \(a_{i}\), and consequently a group \(g_{i}\), where \(=\). We assume that _each group \(g\) is present in the training data_. In an unbiased dataset, number of training samples in each \(g\) remains approximately equal. However, models tend to learn spurious correlations when there is an imbalance in these numbers. We refer to samples favoured most by the ERM trained models as bias-aligned, and the rest as bias-conflicting [20; 38]. The goal is to train a model to optimize a mapping function \(f:^{||}\). In ERM training, we optimize the Cross-Entropy (CE) loss as defined below for a sample \((x,y)\):

\[L_{T}=_{j=1}^{||}-p_{j}} \]

where \([p_{1},p_{2},,p_{||}]\) is a one-hot vector representing \(y\). The corresponding predicted probability vector for the same sample is given by \([},},,p_{||}]\).

### Biases in Pretrained Features

Extracting features from popular pretrained encoders for downstream applications is a common norm, as being trained on large amounts of data equips these models to perform well on a variety of tasks. We freeze these models to avoid backpropagating into their architectures that are generally large-scale  and often only accessible through API calls . In this subsection, we choose the WaterBirds dataset  to analyse the effect of the pretrained model on the downstream performance (see Section 4 for details). We observe two different scenarios here:

**Scenario 1: Pretrained features are target-aware.** Mehta et al.  find that if one chooses a proper pretrained encoder for their specific downstream task, ERM training on a linear classifier attached to the pretrained embedding is enough to obtain unbiased predictions. For instance, using a ViT-H 14 encoder pretrained on the SWAG dataset  followed by end-to-end ImageNet  finetuning can achieve state-of-the-art results on the Waterbirds dataset. We find that even if all the bias-conflicting samples are removed from the training data, the worst group accuracies remain sufficiently high for the test set (see Table 1). Thus, the pretrained model is highly aware of the downstream target attribute, and no debiasing is required. However, a single pretrained model is hardly a panacea for bias mitigation. For instance, the worst group accuracy for the _CelebA  dataset's Blond Hair Classification_ is merely \(\%\)!

**Scenario 2: Pretrained features are bias-aware.** Contrary to _Scenario 1_, when we use an ImageNet-pretrained ResNet-18 as the pretrained encoder, we notice that the worst group accuracies are considerably low for the original Waterbirds dataset itself (see Table 1). An even further drop is seen when the bias-conflicting samples are removed. This shows that this feature encoder exhibits the biases present in the downstream dataset. One simple mitigation strategy is to reduce the bias-correlation in the training set. Group-balancing the Waterbirds dataset leads to considerably uniform accuracies across all groups, irrespective of the underlying pretrained model, as shown in Table 1. This may not always be simple - firstly, the bias attribute may not be known apriori to a practitioner, and secondly, group-balancing or reducing the bias in the downstream dataset may be expensive. Further analyses on these lines are presented in Appendix subsection A.1. Thus, for such systems, an explicit mitigation strategy is required so that the model predictions are unbiased.

### Bias mitigation

To mitigate biases in this problem setting, we consider an image classification model that consists of \(3\) primary components (see Fig. 1): a) A pretrained feature extractor \(m\) to extract the image features \(f^{}=m(x)\) (\(m\) is _blackbox_, i.e. frozen), b) an adapter consisting of a multi-layer perceptron model \(h\) comprising of a single non-linear hidden layer, projecting \(f^{}\) to a new latent space defined as \(=h(f^{})\), c) a classifier \(C\), attached to \(\) to obtain the final predictions \(=C()\).

**Performance of existing methods**. A large body of work exists in the domain of bias mitigation. However, most of these works assume a fully trainable feature encoder. We pick a few representative ones like DebiaN , BPA , GEORGE , Lf , JTT  and Contrastive Adapter (Co-Ada)  (see Section 2 for details) and explore their efficacy in this setup. We choose three popular benchmarks: Waterbirds, CelebA, and ColorMNIST-0.995 (descriptions available in Sec 4) and use a frozen ResNet-18 feature encoder pretrained on the ImageNet dataset for this experiment. We have a number of key observations (see Table 2):

* Lf performs well on Waterbirds and ColorMNIST-0.995, however, it drops by \(~{}14\%\) on CelebA. Similar trends are seen for other methods as well. This shows that existing methods exhibit inconsistencies in this constrained problem setting.
* Co-Ada, which was designed to improve the zero-shot performance of foundation models (without backpropagating into the underlying model), works consistently well across all datasets. However, Contrastive Adapter is computationally expensive (Appendix Table 17).

   Dataset &  &  \\  Type & Worst Group & Average Group & Worst Group & Average Group \\  Original & 88.01 & 94.79 & 38.90 & 76.22 \\ No Bi-Co samples & 80.06 & 90.44 & 18.22 & 68.25 \\  Balanced & 92.37 & 95.47 & 83.55 & 85.94 \\   

Table 1: **Performance of Waterbirds on different pretrained encoders**. We compare the model performance for a pretrained ViT-H encoder and a pretrained ResNet-18 encoder for three versions of the training data: a) The original, b) By removing all bias-conflicting (Bi-Co) samples and c) Group-Balanced. For all cases, the test set remains the same.

The above observations serve as motivating principles behind the proposed method for this problem setting that is computationally efficient and effective across benchmark datasets.

### Our approach

In this subsection, we present our approach. Inspired by previous works [20; 19; 38], we first amplify the biases in the system. Following this, we discuss a few alternatives for mitigation that eventually lead upto our final method.

**Bias-amplified Training**. We propose learning features \(\) first using an adapter (i.e. an MLP layer) on top of the pretrained features \(f^{}\) through ERM training, and then identifying the biases from these learnt features. The goal is to reduce the worst-group accuracies of the training set while maintaining the overall training performance - thereby amplifying the bias learnt by the model. As indicated in previous works [65; 23], we find that increasing the weight decay \(\) reduces worst-group performance of the training set considerably. We next show the effect of changing \(\) on 3 popular datasets: Waterbirds, CelebA and ColorMNIST-0.995. In Fig. 2(a), we observe that with increasing weight decay, the worst group accuracies reduce for CelebA's Blond Hair classification  (i.e. for Blond Males), and also dips for Non-blond Females, while the other two groups' performances remain high. This points to the amplification of gender bias, i.e., the model tends to predict Blond Hair for female images and Non-blond Hair for male images. A similar phenomenon is noticed in the case of Waterbirds , as shown in Fig. 2(b). Although the same pattern is exhibited in the case of ColorMNIST-0.995 in Fig. 2(c), an interesting observation also emerges. For \(=0.1\), training accuracy drops off quite steeply. So, with the goal of reducing the performance on bias conflicting samples, the model may end up not learning anything meaningful at all (as also seen in Fig. 2(a)). This hints at a tradeoff, where we fix \(\) to a high value while ensuring the training accuracy does not fall drastically. We select the model based on the _training accuracy_ as higher training accuracy ensures optimal learning of the bias-aligned data points even when there is less learning of the bias-conflicting samples. To avoid all predictions from collapsing to a single class, we

    &  &  &  &  \\   & & & Worst & \(_{}\) & Worst & \(_{}\) & Bi-Co & \(_{}\) \\   & ERM & \(38.90\) & \(0\) & \(27.20\) & \(0\) & \(49.09\) & \(0\) \\  & DebiAN  & \(58.94\) & \(20.04\) & \(26.10\) & \(-1.1\) & \(49.82\) & \(0.73\) \\  & BPA  & \(58.70\) & \(19.80\) & \(66.71\) & \(39.51\) & \(47.84\) & \(-1.25\) \\  & LIF  & \(66.09\) & \(27.19\) & \(13.26\) & \(-13.94\) & \(71.61\) & \(22.52\) \\  & JTT  & \(49.84\) & \(10.94\) & \(56.25\) & \(29.05\) & \(42.86\) & \(-6.23\) \\  & GEORGE  & \(59.35\) & \(20.45\) & \(42.22\) & \(15.02\) & \(48.77\) & \(-0.32\) \\  & Co-Ada  & \(67.57\) & \(28.67\) & \(78.37\) & \(51.17\) & \(65.48\) & \(16.39\) \\   

Table 2: **Performance of existing methods on the proposed problem setting.** We observe that for three different benchmarks, performance of existing methods is either close to that of the ERM model (measured by \(_{}\)), or not consistently high. Co-Ada  is one exception among compared methods, having the highest worst-group accuracies for all three benchmarks.

Figure 2: **Effect of increasing weight decay.****(a)** For CelebA, as \(\) increases, the worst group (Blond Male (M-B)) accuracy reduces, though the accuracies for Blond Female (F-B) and Non-blond Males (M-NB) still remain high. **(b)** For Waterbirds, we see a fall in both Land-Waterbird (L-WB) and Water-Landbird (W-LB) accuracies with increasing \(\), though the scores for Land-Landbird (L-LB) and Water-Waterbird (W-WB) remain high. **(c)** For ColorMNIST, while the bias-conflicting accuracies reduce as expected, the training accuracy dips beyond \(=0.1\).

sample equally from each class for each batch of the training set. We call the obtained bias-amplified model \(B\), and utilize its knowledge to design a few alternative mitigation strategies. For debiasing, we use the same architecture as that of the biased model \(B\), and denote it by \(D\).

Before delving into the rest of our method, we discuss a caveat here. An attentive reader might enquire as to whether the above technique of bias amplification will always work. We reiterate that we consider the bias in the downstream dataset to align with that in the feature encoder. An example to the contrary has been discussed in Scenario 1 (subsection 3.2). This presents an interesting future direction where one might attempt to debias a feature encoder which does not capture the downstream dataset bias.

**Technique 1: Loss-weighted (LW) Cross-Entropy Loss.** We use a weighted CE loss to train \(D\): \(L^{D}_{LW}=-L^{B}_{T}_{j=1}^{||}p_{j}_{j}\), where \(L^{B}_{T}\) is the CE loss computed from \(B\) for the sample with respect to its ground truth label. The intuition is that \(B\) being biased, it would upweight the CE loss for the bias-conflicting sample in the mitigation stage. On implementing this for the Waterbirds, CelebA and ColorMNIST-0.995, we find that even though the mitigation performance increases as compared to the ERM-trained model, the improvement is not satisfactory (see Table 3).

**Technique 2: Cluster-weighted (CW) Cross-Entropy Loss.** The adapter features \(\) in the biased model \(B\) are expected to encode the bias in the downstream dataset. Hence, we explore a cluster-based weighting scheme for bias mitigation by clustering the bias-amplified feature space \(\) in \(B\). We define \(K\) as the number of clusters obtained and \(^{z}_{c}\) as the proportion of class \(c\) within the cluster \(z\), i.e.,

\[^{z}_{c}=_{c}+}{_{c^{}}n^{z} _{c^{}}+} \]

where \(n^{z}_{c}\) is the number of times samples of class \(c\) have occurred in cluster \(z\). For any sample in the training data, we find the cluster it belongs to (say \(z\)), and then weight the CE loss for training \(D\) in the following way: \(L^{D}_{LW}=-_{j=1}^{||}(1-^{z}_{j})p_{j}_{j}\). Intuitively, the loss is upweighted if the sample belongs to a minority class in its cluster, otherwise it is downweighted. From Table 3, we find that it is a significant improvement over LW. Infact, from Table 2, we find that it performs on par with Co-Ada  for Waterbirds and ColorMNIST-0.995.

### Cluster-based Adaptive Margin Loss

In Technique 2, we see that clustering-based weighting methods can be effective, where we upweight samples belonging to the minority classes in a cluster, whereas we downweight the others proportionally to increase the importance of the sparse samples. We further ask, inside each cluster, can we make the features of the individual classes more discriminative? We conjecture that in such a case, it will be easier for the debiasing model \(D\) to distinguish among the samples from the different groups in the dataset (assuming that the clusters are an approximate representation of the true groups in the dataset). To this end, margin losses are an efficient class of loss functions that can reduce intra-class distance and increase inter-class distance in the feature space. Enforcing such margin constraints on hyperspherical feature spaces has been shown to be beneficial in case of deep face recognition , long-tailed learning , few-shot learning  and the language-bias problem  in Visual Question Answering (VQA). Especially, inspired by the works of long-tailed learning and VQA, we

    &  &  &  \\   & Worst & Avg & Worst & Avg & Bi-Co & Bi-Al & Avg \\  ERM & \(38.90\) & \(76.22\) & \(27.20\) & \(75.43\) & \(49.09\) & \(100.00\) & \(74.55\) \\ LW & \(69.78\) & \(83.52\) & \(45.56\) & \(79.84\) & \(50.58\) & \(99.90\) & \(75.28\) \\ CW & \(70.16\) & \(84.21\) & \(69.4\) & \(85.16\) & \(65.00\) & \(99.05\) & \(82.00\) \\ CM & \(\) & \(\) & \(\) & \(\) & \(\) & \(96.28\) & \(\) \\   

Table 3: **Comparison of different alternatives**. We compare three methods – loss-weighted CE loss (LW), cluster-weighted CE loss (CW), and cluster-based margin loss (CM). While LW leads to improved scores compared to an ERM-trained model, CW further improves upon it for CelebA and ColorMNIST-0.995. Finally, the proposed CM outperforms the above two methods by a large margin. All results are with respect to an ImageNet-pretrained ResNet-18 model.

make the margins adaptive to ensure discriminative features among the frequently and infrequently occurring classes within a cluster. To achieve this, we utilize the weights used in the cluster-weighted (CW) CE loss (defined above) as the margin values. We begin by defining the following:

**Normalized CE loss**: First, we reformulate the _CE_ loss as a cosine loss , by \(L2\)-normalizing the classifier weight vectors \(} C\) (recall that \(C\) is the classifier) for each class \(c_{k}\) (\(k=1,2,||\)), and the trainable feature \(\). We define \(_{k}}=_{k}}}{\|}\|}\) and \(_{norm}}=s}}{\|}\|}\), where \(s\) is a scaling parameter. The bias term is set to \(0\) for simplicity. Let \(_{k}\) be the angle between \(}\) and \(}\). Therefore, the logit for each class \(c_{k}\) becomes: \(_{k}=_{k}}^{}_{norm}}=\|_{k}}\|\|_{norm}}\|_{k}=s\;_{k}\). The features \(_{norm}}\) are thus distributed on a hypersphere with a radius \(s\). This makes the normalized _CE_ loss for a single sample as:

\[L_{NS}=_{k=1}^{||}-p_{k})}{ _{j=1}^{||}(s\;_{j})} \]

**Adaptive Margin Loss.** Inspired by the ArcFace loss  used in face recognition, we define the adaptive margin loss here. The loss adds a margin penalty to the angle between the features \(}\), and the classifier weights \(}\) for the \(k^{th}\) class. Since the margin is placed on the angle, it maps exactly to the "geodesic" distance on the hypersphere , leading to highly discriminative features. For face recognition, it suffices to have a constant value for the margin penalty (\(0.5\) for ArcFace). Since we want discriminative features for the frequently and infrequently occurring classes in a cluster, we ensure that the training samples assigned to a cluster \(z\) and belonging to the majority class \(y\) in that cluster are allowed a smaller margin than those belonging to the minority class. We assign the loss weights from CW as the margin values (i.e. eq. 2). Specifically, for a data sample belonging to class \(c\) with cluster id \(z\), we denote its margin as \(m_{c}^{z}=(1-_{c}^{z})\) (see Fig. 3 for an overview of the system). Finally, we define the _angular_ adaptive margin loss for each sample belonging to the tuple \(t=(c,z)\) by combining eq.s 3 and 2:

\[L_{}^{t}=_{k=1}^{|C|}-p_{k}+ m_{c_{k}}^{z}))}{_{j=1}^{|C|}(s\;(_{j}+m_{_{j}}^{z}))} \]

**Gaussian Randomization of the margins**: While we estimate the margins for the margin loss by clustering the features from the biased model \(B\), the obtained clusters can capture noisy signals, leading to erronous results. Alluding to our previous example of a dataset of people with blond and non-blond hair with gender bias (Section 1), since most of the females have blond hair, all such females should be given lesser margin values, whereas blond-haired males should be given higher values. But clustering leads to noisy grouping of samples in the dataset, whereby a certain group may include less blond females and more blond males. Therefore, for those female blond samples, comparatively large margin penalty will be assigned, while the blond male samples will get a low margin penalty. Also, the margins being high for sparse classes and low for the frequent ones in a cluster, they may over-learn the former, ignoring the latter. In face recognition, Boutros et al.  suggest that in typical margin losses, setting constant margins can limit the generalizability and discriminative power of a model and advise the introduction of stochasticity in the margin values to boost the same. This stochasticity can help us smooth out the effect of the noises in clustering on one hand and make the model more generalized to all groups. To enforce this, we use a randomized version of \(_{c}^{z}\), called \(_{c}^{z}\), where \(_{c}^{z}(_{c}^{z},)\). Recall that \(_{c}^{z}\) denotes the proportion of samples belonging to class \(c\) and cluster \(z\). \(\) is the Gaussian distribution, and \(\) is the standard deviation (a hyperparameter). This impedes the model from overcorrecting with respect to the rare classes in a group, thus increasing its generalizability while also compensating for the errors in margin values due to the noisy cluster labels. Finally, we obtain the randomized margin \(r_{c}^{z}=1-_{c}^{z}\) for each \(c\). We then replace \(m_{c}^{z}\) with \(r_{c}^{z}\) in eq. 4. Our overall method is summarized in Fig. 3.

## 4 Experiments and Results

**Dataset Details**. We evaluate our method on multiple benchmarks. **Waterbirds** is a dataset of birds, labeled as _waterbird_ if the bird is a seabird, and _landbird_ otherwise. A spurious correlation exists between these labels and the background-land or water. The dataset has \(4795\) training samples. The **ColorMNIST** dataset (CMNIST)  is generated from MNIST , where each digit is associated with one color in its background most of the time (see Fig. 3). It has \(50000\) training and \(10000\) test images. We evaluate our margin loss method on two variants of CMNIST: in CMNIST-0.9, each digit is associated with one color \(90\%\) of the time and other colors only \(10\%\) of the time (moderate bias). In CMNIST-0.995, each digit is associated with a single color \(99.5\%\) of the time (severe bias). The real-world **CelebA** dataset  consists of \(202,599\) face images of celebrities along with 40 attributes. We choose _Blond Hair_ as the target attribute, as it is known to suffer from severe gender bias [28; 38; 45]. For evaluating models on CelebA and Waterbirds, we find the accuracy of each group \(g=(c,a)\) in the test set [20; 45], and report the worst of all the groups in \(\) and their average. For CMNIST, we report the overall bias-aligned (i.e. images of digits with their maximally associated colors) and bias-conflicting (i.e. images of digits with other colors) accuracies, along with the average of all the groups . We also show the results for two more real-life datasets (BAR  and UTKFace ) in Appendix Table 12.

**Implementation Details**. We evaluate our method on multiple pretrained encoders: ImageNet-18  and ViT-Base  encoders, and CLIP ResNet-50 image encoder  (pretrained on other datasets). All our implementations use a \(1\)-hidden layer of \(M\) neurons followed by a non-linearity in the adapter, and the classifier is a linear layer. For clustering, we use the KMeans algorithm. We discuss the hyperparameters and their analyses in Appendix subsection A.3. We assume the availability of a small group-balanced (but unannotated) validation set and calculate the overall accuracy over this dataset for model selection during the bias-mitigation phase.

### Results

We compare our baseline against Contrastive Adapter , given that it performs the best among all other methods across different benchmarks (see Table 2), and the ERM model. The results are presented in Table 4, where we compare our method against the ERM method and Co-Ada. For comparison with other competing methods, see Appendix Table 8. We also present the results for the ViT-H 14 encoder in Appendix Table 13.

    &  &  &  &  \\   & & Worst & Avg & Worst & Avg & Bi-Co & Bi-Al & Avg \\   & ERM & \(38.90^{ 1.40}\) & \(76.22^{ 1.04}\) & \(27.20^{ 0.80}\) & \(75.43^{ 0.67}\) & \(61.72^{ 0.59}\) & \(}\) & \(65.54^{ 0.27}\) \\  & Co-Ada  & \(67.57^{ 1.29}\) & \(80.10^{ 1.36}\) & \(78.37^{ 0.14}\) & \(85.79^{ 0.78}\) & \(80.82^{ 1.02}\) & \(89.63^{ 0.11}\) & \(85.22^{ 0.37}\) \\  & CM (Ours) & \(}\) & \(}\) & \(}\) & \(}\) & \(}\) & \(92.75^{ 0.56}\) & \(}\) \\   & ERM & \(67.93^{ 1.12}\) & \(84.65^{ 0.71}\) & \(36.09^{ 0.42}\) & \(79.59^{ 0.71}\) & \(90.22^{ 1.02}\) & \(}\) & \(91.18^{ 0.06}\) \\  & Co-Ada  & \(}\) & \(}\) & \(}\) & \(91.88^{ 1.15}\) & \(83.81^{ 0.02}\) & \(95.68^{ 0.01}\) & \(97.54^{ 0.15}\) \\  & CM (Ours) & \(}\) & \(}\) & \(04.07^{ 0.43}\) & \(}\) & \(}\) & \(96.40^{ 0.66}\) & \(}\) \\   & ERM & \(59.67^{ 1.07}\) & \(}\) & \(31.72^{ 0.64}\) & \(77.25^{ 0.62}\) & \(88.93^{ 0.4}\) & \(}\) & \(94.31^{ 0.14}\) \\  & Co-Ada  & \(63.71^{ 1.18}\) & \(80.24^{ 1.15}\) & \(85.87^{ 0.42}\) & \(}\) & \(79.71^{ 0.61}\) & \(86.84^{ 0.07}\) & \(83.28^{ 1.01}\) \\  & CM (Ours) & \(}\) & \(82.85^{ 0.34}\) & \(}\) & \(89.31^{ 0.66}\) & \(}\) & \(98.59^{ 0.15}\) & \(}\) \\   

Table 4: **Performance comparison of our method (CM)**. Worst and Average Group Accuracies for Waterbirds & CelebA (Blond Hair), Bias-Conflicting (Bi-Co), Bias-Aligned (Bi-Al) and Average Accuracies for CMNIST-0.9. Highest accuracies are marked in bold, the \(2^{nd}\) highest ones are underlined. All scores for our method are averaged over 3 seeds.

Figure 3: **Overview of our method.** There are 3 steps: a) **Bias-amplified training** of the model through Cross-Entropy Loss with high weight decay, b) **Clustering** the biased features \(\), c) **Mitigating** the biases by using the resultant clusters to calculate the margins and the corresponding loss, leading to decent performance on the bias-conflicting data points.

For the _Waterbirds_ dataset, our method far outperforms Contrastive Adapter for ResNet-18 and ViT-B (by \(12.72\%\) and \(11.21\%\) respectively), though for CLIP the worst-group accuracy is slightly lesser (by \(2.67\%\)). In case of _CelebA_, for ResNet-18 and ViT-Base, the margin loss outperforms Contrastive Adapter (by \(3.24\%\) and \(1.41\%\)). For the CLIP encoder, we observe the two methods to perform similarly. The _CMNIST-0.9_ dataset is affected by a moderate degree of the background color bias, whereas for _CMNIST-0.995_ the bias strength is severe. In case of both datasets, our method outperforms Contrastive Adapter for all backbones by a large margin, especially for CMNIST-0.9 (see results for CMNIST-0.995 in Table 5). Compared to Contrastive Adapter, our method has another advantage: it is _time-efficient_, as discussed in the Appendix Table 17. Our method is effective even when finetuning to the encoder weights is possible (see Appendix subsection A.7).

## 5 Ablations

In this subsection, we discuss the important components of our approach and the margin loss strategy. We first show what happens when the margin penalty is constant as the ArcFace loss  itself. Then, we show the role played by the Gaussian randomization of the group-based margins (read subsection 3.5 and Appendix subsection 3.2). Finally, we show what happens when we estimate the different bias-groups in the training data by clustering the blackbox features instead of the bias-amplified adapter layer. All evaluations are performed on Waterbirds, CelebA (Blond Hair classification) and CMNIST-0.9 with ResNet-18 as the backbone. The results are shown in Table 6.

**Constant Margin**. We present the results obtained by keeping the margin value constant at \(0.5\) as suggested by ArcFace  (no Gaussian randomization is applied here). This study enables us to judge the utility of the adaptive nature of the margins. While for Waterbirds and Blond Hair, the scores are better than those of ERM, there's a sharp drop in both worst and average group accuracies compared to our method. The trends are similar in CMNIST-0.9 as well. Hence, we conclude that while margin losses improve model performance, a constant margin is not enough.

**Without Randomization**. We remove the Gaussian randomization of the margins, but here the margins are adaptive (as per eq. 2). For Waterbirds, we see that both the worst and average group accuracies reduce compared to the final scores. Worst group accuracy drops for CelebA as well, though the average group score remains similar. In CMNIST, the bias-conflicting accuracies slightly improve at the cost of a small drop in bias-aligned performance. Thus, overall we find that randomizing the margins for each sample is helpful.

**Clustering from \(f^{}\)**. Our model identifies biases by clustering the adapter features \(\). However, the pretrained features \(f_{}\) can be useful as well, if the pretrained model and the downstream

    &  &  &  \\   & Worst & Average & Worst & Average & Bi-Co & Bi-Al & Average \\  CM (Ours) & \(80.29\) & \(84.56\) & \(81.61\) & \(86.04\) & \(81.91\) & \(92.75\) & \(87.33\) \\ Constant Margin (\(0.5\)) & \(55.76\) & \(77.45\) & \(34.44\) & \(77.63\) & \(44.91\) & \(100.00\) & \(72.45\) \\ No Randomization & \(79.28\) & \(82.9\) & \(78.51\) & \(86.07\) & \(82.28\) & \(91.23\) & \(86.70\) \\ Clustering from \(f^{}\) & \(74.92\) & \(84.95\) & \(80.52\) & \(85.04\) & \(75.96\) & \(94.88\) & \(85.42\) \\   

Table 6: **Ablations of the proposed method**: Here we show the roles of the different components of our model using ResNet-18 as the pretrained backbone.

    &  &  \\   & & Bi-Co & Bi-Al & Avg \\   & ERM & \(49.09^{ 0.19}\) & \(}\) & \(74.55^{ 0.82}\) \\  & Co-Ade & \(65.48^{ 0.21}\) & \(84.48^{ 0.24}\) & \(74.98^{ 0.09}\) \\  & CM (Ours) & \(^{ 0.88}\) & \(^{ 0.50}\) & \(^{ 0.41}\) \\   & ERM & \(56.87^{ 0.12}\) & \(}\) & \(78.44^{ 0.52}\) \\  & Co-Ada & \(77.34^{ 0.34}\) & \(92.18^{ 0.06}\) & \(84.76^{ 0.18}\) \\  & CM (Ours) & \(^{ 0.05}\) & \(93.20^{ 1.23}\) & \(^{ 0.60}\) \\   & ERM & \(59.21^{ 0.65}\) & \(}\) & \(79.61^{ 0.42}\) \\  & Co-Ada & \(76.88^{ 0.43}\) & \(92.27^{ 0.61}\) & \(84.58^{ 0.70}\) \\  & CM (Ours) & \(^{ 0.71}\) & \(^{ 0.49}\) & \(^{ 0.44}\) \\   

Table 5: **Performance comparison of our method for CMNIST-0.995. We report Bias-Conflicting (Bi-Co), Bias-Aligned (Bi-Al) and Average Accuracies. Highest accuracies are marked in bold, the \(2^{nd}\) highest ones are underlined. All scores for our method are averaged over 3 seeds.**dataset share similar biases. Hence, we cluster the pretrained features \(f^{}\) instead of \(\) to obtain the margin penalties. The rest of the training pipeline remains the same, as described in subsection 3.3. For all 3 datasets, we see a reduction in the worst and average group accuracies, however, the scores are still close to that of the proposed approach. Since clustering the pretrained features does not require an extra stage of ERM training, therefore in presence of time-constraints, \(f^{}\) can be used as a proxy for \(\), and subsequently, relatively decent model predictions can be obtained.

## 6 Discussion and Limitations

Our method is specifically targeted towards cases where the bias in the downstream dataset is already encoded in the pretrained model. We believe that detecting if this assumption holds apriori is highly challenging in the absence of the bias labels. We put forward a few suggestions to identify the scenarios that fit this assumption: a) Obtain bias annotations for the small validation set. If the worst group accuracy of the validation set does not reduce substantially with increasing weight decay, it indicates that the features have stronger signals of the target class than that of the bias, making it harder to capture the bias. b) If the bias annotations of the validation set cannot be obtained due to privacy concerns, the overall validation accuracy can indicate strength of the bias. For example, the difference between the validation and training accuracy is \(25\%\) for an ERM trained method for the Waterbirds dataset on the ResNet-18 backbone. The higher this difference, the more the indication that the model is overfitting to more and more samples. Such overfitting can indicate that the model is learning the bias in the dataset, thus not generalizing on the bias-conflicting samples. While we have mentioned the situation when the pretrained model is aware of the target attribute in Section 3, another potential use case might be when the pre-training data distribution may be different from that of the downstream dataset. We prescribe finetuning the pretrained models to amplify and mitigate the biases in such a case. A more practical approach would be to choose another model, as also suggested by Zhang et al. .

One limitation of our method is that it currently relies on the existence of biased groups in the training set (i.e., the bias labels are expected to be categorical) as it uses clustering to identify the biases. Furthermore, since our approach (along with most competing methods) relies on amplifying the bias first and then mitigating the same, it involves a risk wherein if the mitigation module fails, the bias in the system may be exacerbated. With these limitations in mind, we hope this work initiates a much required discussion in this direction leading to more sophisticated and targeted solutions in the near future.

**Scalability of the proposed approach to large datasets and models**. Since our method involves clustering the features, if the dataset is large-scale, one can randomly sample a small percentage of it to do the clustering. For example, on clustering only \(10\%\) randomly sampled images from the CelebA dataset, we find that the worst group accuracy is \(81.11\%\), whereas the average group accuracy is \(85.6\%\) on the ResNet-18 backbone. The scores become \(80.55\%\) and \(85.6\%\) respectively when the clustering is performed only on \(1\%\) of the images (with full clustering, the scores are \(81.61\%\) and \(86.04\%\) respectively). On the other hand, for very large models, the requirement is to be able to load the model into a GPU memory. Our method adds negligible overhead owing to the addition of only an adapter and a classifier layer.

## 7 Conclusion

In this work, we explored the effect of using pretrained but frozen feature extractors on downstream applications with biased datasets and found the need for specific bias mitigation strategies in cases where the biases in the downstream dataset align with the pretrained encoders. Such mitigation is challenging as the encoder is blackbox. While we found many of the existing works to be inadequate, we proposed a simple method where we first amplified the biases present in the downstream dataset and then employed a clustering-based margin loss to mitigate the same. Our experiments showed this method to be an efficient and effective technique across multiple benchmarks. Lastly, it is our hope that this work will initiate a much-required discussion among the scientific community on encouraging similar works in this highly practical problem setting. We share further studies, detailed hyperparameter analyses and experiments of the proposed method in the appendix.

Acknowledgements

This research is funded by the Qualcomm Innovation Fellowship. Abhip's Basu is supported by the PMRF Fellowship.