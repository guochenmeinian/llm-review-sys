# Efficient Learning of Linear Graph Neural Networks

via Node Subsampling

Seiyun Shin\({}^{1}\), Ilan Shomorony\({}^{1}\), and Han Zhao\({}^{2}\)

\({}^{1}\)Department of ECE and \({}^{2}\)Department of CS

University of Illinois at Urbana-Champaign, IL

{seiyuns2, ilans, hanzhao}@illinois.edu

###### Abstract

Graph Neural Networks (GNNs) are a powerful class of machine learning models with applications in recommender systems, drug discovery, social network analysis, and computer vision. One challenge with their implementation is that GNNs often take large-scale graphs as inputs, which imposes significant computational/storage costs in the training and testing phases. In particular, the message passing operations of a GNN require multiplication of the graph adjacency matrix \(A^{n n}\) and the data matrix \(X^{n d}\), and the \(O(n^{2}d)\) time complexity can be prohibitive for large \(n\). Thus, a natural question is whether it is possible to perform the GNN operations in (quasi-)linear time by avoiding the full computation of \(AX\). To study this question, we consider the setting of a regression task on a two-layer Linear Graph Convolutional Network (GCN). We develop an efficient training algorithm based on (1) performing node subsampling, (2) estimating the leverage scores of \(AX\) based on the subsampled graph, and (3) performing leverage score sampling on \(AX\). We show that our proposed scheme learns the regression model observing only \(O(nd^{-2} n)\) entries of \(A\) in time \(O(nd^{2}^{-2} n)\), with the guarantee that the learned weights deviate by at most \(\) under the \(_{2}\) norm from the model learned using the entire adjacency matrix \(A\). We present empirical results for regression problems on real-world graphs and show that our algorithm significantly outperforms other baseline sampling strategies that exploit the same number of observations.

## 1 Introduction

Graph Neural Networks (GNNs) have gained popularity as a powerful machine learning method for graph-structured data. By learning rich representations of graph data, GNNs can solve a variety of prediction tasks on graphs [1; 2; 3; 4; 5; 6; 7]. GNNs have delivered impressive results across many different areas, including social network analysis , bioinformatics [9; 10; 11], and recommendation systems .

Given their remarkable performance, being able to train GNNs efficiently is an important task. However, training GNNs can be quite challenging in the context of large-scale graphs, which impose significant computational costs. In particular, the message passing scheme in GNNs requires, for each node, summing up the feature vectors of all neighboring nodes into one feature vector. For a multi-layer Graph Convolutional Network (GCN) , the layer-wise propagation rule is

\[H^{(+1)}=(AH^{()}W^{()}),\] (1)

where \(H^{()}\) denotes the feature representation at the \(\)th layer, with \(H^{(0)}=X\) (i.e., the data matrix consisting of nodes' features of an input graph \(G\)); \(W^{()}\) denotes the weight matrix; \(()\) is a non-linear activation function like the ReLU; and \(A\) denotes the adjacency matrix of \(G\).

If the graph has \(n\) nodes and the feature dimension is \(d\), computing the matrix multiplication \(AX\) requires \(O(n^{2}d)\) time and can be prohibitive in big data settings. A natural question is whether it is possible to train a GCN to avoid the quadratic complexity scaling with \(n\).

Another motivation for avoiding the full computation of \(AX\) is applications where the adjacency matrix \(A\) is not fully known a priori, and it must be learned via node/edge queries. For example, in large social networks (the current number of social media users is over \(4.89\) billion ), one may need to access the adjacency matrix \(A\) by querying the adjacency list of specific nodes. As another example, certain biological networks must be learned through the physical probing of pairwise interactions, which may make obtaining the entire adjacency matrix \(A\) prohibitively expensive. For example, the mapping of the neural network of living organisms (such as the connectome of _C. elegans_) requires physical probing of the connectivity between neurons.

A natural approach to avoid the quadratic complexity of computing \(AX\) is via the subsampling of the graph \(G\). If the resulting subgraph is sparse enough, \(AX\) can be computed very efficiently. But how many entries of \(A\) need to be observed in order to guarantee that the message passing step \(AX\) can be computed accurately enough in the context of GNNs? What kinds of graph subsampling strategies are amenable to theoretical performance guarantees on the training of GNNs?

We consider the setting of a regression task to be learned via a GCN. Let \(A^{n n}\) be the weighted adjacency matrix of a graph \(G=(V,A)\) with \(|V|=n\). Each node \(v_{i} V\) has an associated feature vector of dimension \(d\) (\(<n\)) and a label, denoted by \((_{i},y_{i})^{d}\), which can also be represented as a \(n d\) data matrix \(X\) and label vector \(\). Training the GCN corresponds to minimizing the loss

\[(W,A,X,)\|-f_{}(W,A,X)\|_{2}^{2},\] (2)

on the training data \(\{(_{i},y_{i})\}_{i=1}^{n}\), where \(W\) denotes the GCN network weights, and \(f_{}(W,A,X)\) denotes a feed-forward computation of a GCN's output. As a first step to studying whether (2) can be solved accurately on a sparse subgraph of \(G\), we focus on a simple _linear_ GCN, where there is no non-linearity \(()\). Specifically, the feed-forward output is given by \(f_{}^{}(,A,X) AX\), where we use \(^{d}\) (instead of \(W^{1 d}\)) to indicate that the learnable parameters are in the form of a vector. Hence, our goal is to solve

\[_{}\|-AX\|_{2}^{2}.\] (3)

Note that one can view this optimization problem as a _graph-weighted linear regression problem_.

The setting of linear regression provides natural suggestions for a subsampling strategy. It is known that _leverage score sampling_ allows one to solve the linear regression problem with \((1+)\) accuracy using only \(O(d^{-2} n)\) subsampled rows of the data matrix \(X\) and in time \(O(nd^{2}+d^{3})\)1. Nevertheless, to apply this strategy to (3), one would need to first compute \(AX\), requiring \(O(n^{2}d)\) time, so that the row leverage scores of \(AX\) can then be computed. This motivates us to propose a two-step approach that (1) performs node subsampling to obtain an approximate observation of \(AX\) and estimate the leverage scores of the rows of the augmented matrix \([AX|-]^{n(d+1)}\) and (2) performs leverage score sampling on the rows of \([AX|-]\) using the estimated scores.

This two-step approach is illustrated in Figure 1. In order to obtain an estimator of \(AX\), our sampling scheme in the first step builds \(O( n)\) rank-\(1\) matrices by sampling the same amount of columns of \(A\) and the corresponding rows of \(X\). The key idea behind this approach is taking a random subset of nodes and propagating their feature vectors to their neighbors. We provide a spectral approximation guarantee for the resulting estimate of \(AX\) and an approximation guarantee for the leverage scores of the augmented matrix \([|-]\) computed on this estimate. In the second step, we adopt the standard leverage score sampling for sampling rows of \(A\), using the leverage score estimates of \([AX|-]\) obtained from the first step. With \(O(d^{-2} n)\) sampled rows of \(\) and \(\), the training algorithm then computes \(X\) and uses it to solve the regression problem. We show that our proposed scheme learns the regression model with learned weights deviated by at most \(\) from those with full information, by observing only \(O(nd^{-2} n)\) entries of \(A\), and in time \(O(nd^{2}^{-2} n)\).

On real-world benchmark datasets, we demonstrate the performance improvements of the two proposed schemes over other baseline sampling schemes via numerical simulations.

Notation:Throughout the paper, we use \(A=(a_{ij})_{ 0}^{n n}\) to denote the graph adjacency matrix, where each entry value is nonnegative and bounded (i.e., \( a_{ij} M\) for some constant \(M>0\)). We write \(A_{i:}\) and \(A_{:j}\) to indicate the \(i\)th row and \(j\)th column of the matrix \(A\) respectively. In addition, \(X^{n d}\) denotes the data (design) matrix, where the \(i\)th row in \(X\), \(_{i}\), corresponds to the node vector at the \(i\)th node \(v_{i}\) in the graph \(G\). We also assume that the absolute value of each entry value of \(X\) is bounded by \(M\) (i.e., \(|X_{ij}| M\)). We denote vectors by a lowercase bold letter (e.g., \(^{n}\)), and by default, all the vectors will be in column format, hence \(X=(_{1}^{};;_{n}^{})^{}\), where \(()^{}\) denotes the transpose. We indicate the \(i\)th entry of \(\) with either \([]_{i}\) or \(x_{i}\), and we use \([]_{S}\) to denote the concatenation of \(x_{i}\)s for all \(i S\). We also let \([n]:=\{1,2,,n\}\). We denote by \(,:=_{i}x_{i}y_{i}\) the inner product between \(\) and \(\). Unless otherwise mentioned, we use \(\|\|\) for the \(_{2}\)-norm for vector \(\). We let \(\|A\|:=_{\|\|=1}\|A\|\) denote the operator norm of a matrix, and \(\|A\|_{F}:=a_{ij}^{2}}\) denote the Frobenius norm. We denote the standard basis vector by \(_{i}\). We write \((p)\) for a Bernoulli distribution with parameter \(p\). Lastly, we denote by \((A)\) the number of non-zero entries in matrix \(A\).

Related Work:We provide a detailed discussion of related works in Appendix A.

## 2 Motivation: Leverage Score Sampling for Linear Regression

First, we briefly describe key results on leverage score sampling that will be relevant to our discussion. We refer to  for a detailed discussion on the subject, and we provide key proofs in Appendix D.

**Definition 1**.: The leverage score of the \(i\)th row of \(X\) is \(_{i}(X):=_{i}^{}(X^{}X)^{}_{i}\), where \(()^{}\) denotes the Moore-Penrose pseudoinverse.

Intuitively, a row's leverage score measures how important it is in composing the row space of \(X\). If a row has a component orthogonal to all the other rows, its leverage score is \(1\). Removing it would decrease the rank of \(X\), completely changing its row space. The leverage scores are the diagonal entries of the projection matrix \(X(X^{}X)^{-1}X^{}\), which can be used to show that \(_{i=1}^{n}_{i}(X)=(X)\). The following alternative characterizations of \(_{i}(X)\) will be useful.

**Proposition 1**.: Let \(X=U V^{}\) be the singular value decomposition (SVD) of \(X\), where \(U^{n d}\), \(^{d d}\) and \(V^{d d}\). Then \(_{i}(X)=\|U_{i:}\|_{2}^{2}\), where \(U_{i:}\) is the \(i\)th row of \(U\).

**Proposition 2**.: The leverage score can be alternatively computed as \(_{i}(X)=_{}_{1}^{})^{2}}{\|X \|_{2}^{2}}\).

Through the use of leverage scores, it is possible to approximately solve a linear regression task using a subset of the data points . In particular, leverage score sampling allows the spectral approximation of a matrix.

**Definition 2**.: A matrix \(^{n d}\) is an \(\)-spectral approximation of \(X^{n d}\) if, for all \(^{d}\),

\[(1-)\|X\|_{2}\|\|_{2}( 1+)\|X\|_{2}.\] (4)

Suppose we are given a data matrix \(X=(_{1}^{};;_{n}^{})^{}\) and a vector \(=(u_{1},,u_{n})\) of leverage score _overestimates_ of \(X\) (i.e., \(_{i}(X) u_{i},\  i[n]\)). We create a matrix \(\) by including the \(i\)th row

Figure 1: Two-step Algorithm: (1) perform node subsampling to obtain an estimate \(\), from which we can compute \(_{i}([]-]\) for \(i=1,,n\); and (2) use the leverage score estimates to perform leverage score sampling on \(A\) and \(\) and perform regression using \(X\) and \(}\) instead of \(AX\) and \(\).

of \(X\) in \(\) with probability \(p_{i}=[1,cu_{i}^{-2} n]\) for some constant \(c\), scaled by \(1/}\). The following is a slight modification of Lemma 4 in  (which we prove in Appendix D):

**Lemma 1**.: With probability at least \(1-n^{-(1)}\), for any \(\), we have

\[(1-)\|X\|\|\| (1+)\|X\|.\]

Notice that the expected number of rows in \(\) is \(c\|\|_{1}^{-2} n\). Hence, as long as \(\|\|_{1}=O(d)\), we only need to sample \(O(d^{-2} n)\) rows of \(X\) to obtain a good spectral approximation.

Now suppose we want to solve the OLS problem \(_{}\|-X\|_{2}^{2}\) where \(^{n}\) is the response vector corresponding to \(X\). The optimal solution is given by \(^{*}=(X^{}X)^{-1}X^{}\). Exactly computing \(^{*}\) requires \(O(nd^{2}+d^{3})\) time.

An alternative approach using leverage score sampling would be as follows. First, we note that the OLS objective can be rewritten using \(\|-X\|=\|[X|-][ \\ ]\|\). If we now have leverage score overestimates \(u_{1},,u_{n}\) for the augmented matrix \([X|-]^{n(d+1)}\), we can construct the subsampled matrix \([X_{S}|-_{S}]^{|S|(d+1)}\) (with rows rescaled by \(1/}\)). Then, by Lemma 1, a solution to

\[}=_{}\|[X_{S}]- _{S}][\\ 1]\|_{2},\] (5)

would satisfy \(\|_{S}-X_{S}}\|_{2}\|_{S}-X_{S} ^{*}\|_{2}(1+)\|-X^{*}\|_{2}\).

The exact leverage scores of \(X\) can be computed in time \(O(nd^{2})\) (see Algorithm 3 in Appendix B) and, since \(|S|=O(d^{-2} n)\), the reduced linear system (5) can be solved in time \(O(d^{3}^{-2} n)\). The overall complexity for solving the OLS problem in this way is given by \(O(nd^{2}+d^{3}^{-2} n)\). Notice that there is no real acceleration obtained by this procedure. But also notice that here we are using the exact leverage scores for sampling while, from Lemma 1, we know that this approach works as long as we have good leverage score overestimates.

## 3 Efficient Training of a Linear GCN via Leverage Scores

In this section, we introduce an efficient training algorithm that approximately solves the graph-weighted regression task (3) by observing only \(O(nd^{-2} n)\) entries of \(A\), and in time \(O(nd^{2}^{-2} n)\). The necessity for introducing the efficient training algorithm comes from the \(O(n^{2}d)\) time complexity of computing \(AX\) via matrix multiplication.

Motivated by the discussion in Section 2 for standard linear regression, a natural approach is to attempt to compute leverage score overestimates for \(AX\) efficiently without the full computation of \(AX\). Our proposed algorithm uses the following three steps to approximately solve the graph-weighted linear regression problem.

1. EstimateLeverageScores\((A,X)\) (See Algorithm 1 and Algorithm 2)
2. LeverageScoreSampling\((A,\{}([AX|-]),\  i[n]\})\) (See Algorithm 4 in Appendix B)
3. RegressionSolver\((X,})\) (See Algorithm 5 in Appendix B)

In EstimateLeverageScores\((A,X)\), a node subsampling technique is used to build an estimate \(\) of \(AX\) with an approximation guarantee. The estimate \(\) can then be used to produce provably good overestimates of the leverage scores of the augmented matrix \([AX|-]\). Next, LeverageScoreSampling\((A,\{}([AX|-]),\  i[n]\})\) uses these overestimates to produce a matrix \(\) and scaled labels \(}\) consisting of a reduced number of rows of \(A\) and \(\) respectively. Using Lemma 1, we then show that \(X\) provides a spectral approximation for \(AX\), which allows approximately solving the regression problem (3) using RegressionSolver\((X,})\). We note that none of the algorithmic procedures requires fully computing \(AX\) and the end-to-end algorithm has an \(O(n n)\) time dependence on \(n\).

### First Stage: Uniform Sampling of \(A_{:j}X_{j:}\)

In order to efficiently estimate the leverage scores of \(AX\), a key observation we make is that \(AX\) can be decomposed as a sum of \(n\) rank-\(1\) matrices as \(AX=_{j=1}^{n}A_{:j}X_{j:}\), where \(A_{:j}\) and \(X_{j:}\) denote \(j\)th column of the adjacency matrix \(A\) and \(j\)th row of the data matrix \(X\). Notice that one can view \(A_{:j}X_{j:}\) as the effect of node \(j\)'s feature vector propagated to other nodes after the one-step message passing operation. Hence, a natural approximation strategy is to take a subset of these rank-\(1\) matrices. First, we consider a uniform sampling strategy that samples node indices \(j\) uniformly at random and computes \(A_{:j}X_{j:}\). See Algorithm 1 for the detailed procedure.

```
0: Adjacency matrix \(A^{n n}\), data matrix \(X^{n d}\), budget \(B\), threshold \(>0\)
0: Leverage score overestimates for \(AX\)
1: Draw \(I_{j}(p),\  j[n]\) independently, where \(p=[,1]\)
2:\(X_{j:}}}{p}A_{:j}X_{j:}\), \( j[n]\)
3:\(_{j=1}^{n}X_{j:}}\)\(\)\(O(Bd)\)
4:\(S([|-])\)\(\) See Alg. 3 in App. B; \((([|-])+d^{3})\)
5:\(\{}([AX|-]),\  i[n]\}}{(1-)^{2}} S\)
6:return\(}([AX|-])\), \( i[n]\) ```

**Algorithm 1**EstimateLeverageScores(\(A,X\)) via Uniform Sampling

Given a budget \(B\) (the total number of nodes that should be observed), the algorithm draws sampling indicator variables \(I_{j}(p)\) independently with \(p=[B/n,1]\). For \(j=1,,n\), we build

\[X_{j:}}=}{p}A_{:j}X_{j:},\]

and then set \(:=_{j}X_{j:}}\). Notice that \(\) is an unbiased estimate of \(AX\), because for all \(i,j\),

\[[|_{ij}]=_{k=1}^{n}pa_{ik }x_{kj}=_{k=1}^{n}a_{ik}x_{kj}=[AX]_{ij}.\] (6)

In addition, one can readily see that \(B\) rank-\(1\) matrices are sampled in expectation, and, with high probability, the actual number of sampled rank-\(1\) matrices is less than \((1+)B\) for \(>0\).

Next, we present a spectral approximation guarantee for \(\). In order to establish that result, we make the following mild assumption on the density of the input graph.

_Assumption 1_.: We assume that the input graph \(G\) satisfies

\[\|AX\| n^{3/2}d^{1/2}.\] (7)

Strictly speaking, (7) is not just an assumption about the density of \(G\), as it takes into consideration the interaction between \(A\) and \(X\). Nevertheless, this assumption would be expected to hold in cases where the graph \(G\) is dense. In particular, since \(\|AX\| d^{-1/2}\|AX\|_{F}\), Assumption 1 holds if \(\|AX\|_{F} n^{3/2}d\). Assuming that \(A\) has \( n^{2}\) non-zero entries and that entries of \(X\) are drawn i.i.d. from some distribution with positive second moment \(^{2}\), independently of \(A\), we have \(\|AX\|_{F}d^{2}}=^{1/2}n^{3/2}d^{1/2}\). Treating \(d\) as a constant and rescaling \(X\) by a constant, we have that the assumption holds. More generally, if \(d\) is a constant and

\[|\{(i,j)[n][d]:_{i}^{}AX_{j} c _{1}n\}| c_{2}n\]

for constants \(c_{1}\) and \(c_{2}\), then the assumption holds (after a constant rescaling of the data matrix). The following stronger assumption will be needed to establish guarantees for the leverage score estimates:

_Assumption 2_.: For all \((i,j)[n][d]\), \(_{i}^{}AX_{j} c_{1}n\) for some constant \(c_{1}\).

Notice that if the graph \(G\) is instead sparse, the problem we focus on due to the computation of \(AX\) is less severe since one can efficiently compute \(AX\) in time that depends on \((A)\).

The proof of the following theorem is in Appendix E.

**Theorem 1**.: Under Assumption 1, if we run Algorithm 1 with budget \(B=C^{-2} n\) for large enough \(C>0\), the estimate \(\) satisfies

\[(1-)\|AX\|\|\|(1+ )\|AX\|\,,^{d},\] (8)

with probability at least \(1-n^{-(1)}\).

Given the spectral approximation guarantee in Theorem 1, we expect the leverage scores computed on the augmented matrix \([|-]\) to be close to the leverage scores of \([AX|-]\). Since we want an overestimate of the leverage scores, we use the the multiplicative constant \(\)2 to obtain

\[_{i}([AX|-]):=()^{2}_{i}([|-]), i[n].\] (9)

The following lemma, whose proof is in Appendix F, asserts that \(_{i}([AX|-])\) is indeed an overestimate of \(_{i}([AX|-])\).

**Lemma 2**.: Under Assumption 2, Algorithm 1 with budget \(B=C^{-2} n\) for large enough \(C>0\) outputs leverage score overestimates satisfying

\[_{i}([AX|-])_{i}([AX|-])( )^{4}_{i}([AX|-]),  i[n],\] (10)

with probability at least \(1-n^{-(1)}\).

### Second Stage: Leverage Score Sampling and Approximate Regression

The second step of the proposed algorithm exploits the leverage score estimates \(_{i}([AX|-])\), \( i[n]\) to perform the standard leverage score sampling. With Lemma 1, one can readily obtain a \((1+)\)-approximation guarantee for the graph-weighted linear regression task.

**Theorem 2**.: Under Assumption 2, suppose we run Algorithm 1 with budget \(B=C^{-2} n\) for large enough \(C>0\), to obtain \(\{_{i}([AX|-]),\; i[n]\}\). Subsequently, if we run \((A,\{_{i}([AX|-]),\;  i[n]\})\), we obtain \(^{O(d^{-2} n) n}\) and \(}^{O(d^{-2} n)}\) such that \((,})\) provides \(}\) satisfying

\[\|}-X}\|_{2}^{2}(1+ )_{}\|-AX\|_{2}^{2}.\]

Theorem 2 captures our main contribution. By only observing \(O(d n)\) rows of \(A\), we can obtain a reduced adjacency matrix \(\) (and corresponding label vector \(}\). Running \((X,})\) then yields a solution \(}\) with an approximation guarantee. We refer to detailed descriptions of \((A,\{_{i}([AX|-]),\; i [n]\})\) and \((X,})\) in Appendix B.

Sample complexity and run-time:We note that the sample complexity is \(O(nd^{-2} n)\). First, \((A,X)\) exploits \(O(^{-2} n)\) columns of \(A\) and the same number of rows of \(X\), thus accessing \(O(n^{-2} n)\) entries of \(A\) and \(O(d^{-2} n)\) of \(X\). In addition, \((A,\{_{i}([AX|-]),\;  i[n]\})\) exploits \(O(d^{-2} n)\) rows and thus uses \(O(nd^{-2} n)\) entries. Hence, the total number of entry observations is \(O(nd^{-2} n)\).

For the time complexity, the run time of \((A,X)\) is \(O(nd^{-2} n)\); that of \((A,\{_{i}([AX|-]),\;  i[n]\})\) is \(O(nd^{2}^{-2} n)\). Finally, we note that the run time of \((X,})\) is \(O(nd^{2}+d^{3})^{2}\). Hence, the total run time is \(O(nd^{2}^{-2} n)\) given the setting that we consider has \(d<n\).

Extension to the more-than-two-layer case:Note that without non-linearities, considering a multi-layer network is equivalent to performing additional matrix multiplication by \(A\) to the output of the previous layer. Since our proposed scheme yields a spectral approximation guarantee for matrix multiplication, an analysis similar to that of Algorithm 1 should be possible for this extension to obtain a \((1+)^{L}\)-approximation guarantee, where \(L\) denotes the number of layers. Since the approximation guarantee comes with high probability, a union bound could be used to obtain a high probability guarantee for the multi-layer case (since the order or error for each approximate matrix multiplication is the same).

## 4 Variance Minimization via Data-dependent Random Sampling

As stated in the previous section, Algorithm 1 produces an unbiased estimate of \(AX\). Yet the uniform sampling strategy used in Algorithm 1 does not take the variance of the estimator into consideration. This motivates us to develop an estimator of \(AX\) that further reduces the estimator variance while maintaining unbiasedness. Due to the unbiasedness of \(\), the total variance of the estimator coincides with the mean square error (MSE): \(()=\|AX-\|_{F}^{2}=( )\).

In order to reduce the estimator variance \(()\), we consider a data-dependent sampling probability \(p_{j}\) that depends on the \(j\)th column of \(A\) and the \(j\)th row of \(X\). As we view \(A_{:j}X_{j:}\) as capturing the impact of node \(j\)'s features after a one-step message passing operation, one way to set the sampling probability for node \(j\) is to take the norm of \(A_{:j}X_{j:}\) into account. Specifically, we set

\[p_{j}=[B\|\|X_{:j}\|}{_{j}\|A_{:j}\|\|X_{:j}\| },1].\] (11)

Algorithm 2 describes the detailed procedure, where the only distinction relative to Algorithm 1 is the choice of \(p_{j}\). We claim that this sampling probability choice minimizes the total variance (as well as the MSE) of the estimator. First notice that the total variance can be expressed as

\[()=_{i=1}^{n}_{j=1}^{d}([]_{ij})\] \[}{{=}}(_{k=1}^{n}\|^{2}\|X_{k:}\|^{2}}{p_{k}})(_{k=1}^{n}p_{k} )-C_{2}\] \[}{{}}(_{k=1}^{n}\|A_{: k}\|\|X_{k:}\|)^{2}-C_{2},\]

where \((i)\) follows from the fact that

\[([]_{ij}) =_{k=1}^{n}(}{p_{k}}a_{ik}x_{kj} )=_{k=1}^{n}^{2}}a_{ik}^{2}x_{kj}^{2} (I_{k})=_{k=1}^{n}}{p_{k}}a_{ik}^{2}x_{kj}^{2}\] \[=_{k=1}^{n}}a_{ik}^{2}x_{kj}^{2}-_{k=1}^{n} a_{ik}^{2}x_{kj}^{2}=_{k=1}^{n}}a_{ik}^{2}x_{kj}^{2}-C_{1} $)}\]

and \((ii)\) follows from using the fact that \(_{j}p_{j}=1\); and \((iii)\) follows from Cauchy-Schwarz inequality. Notice that, in order for Cauchy-Schwarz to hold with equality, we must have \(p_{k}\) proportional to \(\|A_{:k}\|\|X_{k:}\|\). This implies that (11) is an optimal choice of the sampling probabilities.

As it turns out, the same theoretical guarantees obtained for Algorithm 1 hold in this case. See Appendix G for the detailed proof. While the theoretical performance guarantee is the same, in settings where the budget \(B\) is very small, the data-dependent sampling probabilities \(p_{j}\) lead to a better estimate \(\), which improves the overall performance of the algorithm.

One important observation is that it is not obvious how to use the data-dependent sampling probabilities \(p_{j}\) while avoiding the an \(O(n^{2})\) computation complexity. In particular, a naive computation of \(\|A_{:j}\|\), \(j[n]\) would require observing all \(n^{2}\) entries of \(A\). However, standard sampling techniques could be used in order to also estimate \(\|A_{:j}\|\), \(j[n]\) fairly accurate, and use those estimates to compute the sampling probabilities \(p_{j}\), \(j[n]\).

```
0: Adjacency matrix \(A^{n n}\), data matrix \(X^{n d}\), budget \(B\), threshold \(>0\)
0: Leverage score estimates \(}([AX|-])\), \( i[n]\)
1: Draw \(I_{j}(p_{j})\), \( j[n]\) independently, where \(p_{j}=[B\|\|X_{j}\|}{_{j}\|A_{i}\|\|X_{j}\|},1]\)
2:\(X_{j}}}{p_{j}}X_{j}}\), \( j[n]\)
3:\(_{j=1}^{n}X_{j}}\)\(\)\(O(Bd)\)
4:\(S([|-])\)\(\)\((([|-])+d^{3})\)
5:\(\{}([AX|-]),\; i[n]\}}{(1-)^{2}} S\)
6:return\(}([AX|-])\), \( i[n]\) ```

**Algorithm 2**EstimateAverageScores(\(A,X\)) via Data-dependent Sampling

## 5 Empirical Results

We support our theoretical findings via numerical experiments3 to validate the performance of Algorithm 1 and Algorithm 2 in a non-asymptotic setting.

Dataset and evaluation methods:We consider benchmark datasets from the Open Graph Benchmark (OGB) , Stanford Network Analysis Project (SNAP) , and House dataset . We defer detailed explanations of the datasets to Appendix H. For evaluation methods, we compute the mean squared error (MSE), wall-clock run-time, and peak memory usage of our two proposed schemes and five baselines: (1) RegressionSolver\((AX,)\) with fully known \(A\) and \(X\); (2) RegressionSolver\((X,)\) with parts of rows of \(A\), say \(\), obtained from sampling rows of \(A\) uniformly at random; (3) RegressionSolver\((X,)\) with the exact leverage score sampling for sampling rows of \(A\) to obtain partial \(\); (4) RegressionSolver\((X,})\) with Algorithm 1; (5) RegressionSolver\((X,})\) with Algorithm 2; (6) RegressionSolver\((X,})\) with a sampling scheme inspired by GraphSage  that selects the neighborhood of feature aggregation for each node; and (7) RegressionSolver\((X,})\) with a sampling scheme motivated by GraphSaint  that selects a subgraph of the original graph. We note that (6) and (7) are not the tools themselves , but graph subsampling strategies inspired by these tools.

Results and analysis:

Mean squared error:Figure 2 plots the mean squared error with respect to the budget allocation. First, we observe that Algorithm 1 and Algorithm 2 have a similar performance to the one with exact leverage scores sampling, and the performance of the two proposed schemes tends to be similar to that of RegressionSolver with full \((AX,)\), as the budget increases. Particularly seen from Figure 1(a), the error reduction on Algorithm 2 from Algorithm 1 is 84% when the budget is at 3%. Even for the sparse datasets, as seen from Figure 1(b) and Figure 1(c), the corresponding improvements of 86% and 67% exist when the budget is at 3% and 7% respectively. We also observe that the error decrease for Algorithm 2 over Algorithm 1; for instance Figure 1(b) demonstrates the existence of 91% improvement at 5% observation budget. On the other hand, we observe that other subsampling approaches--including those employed in uniform sampling, GraphSage, and GraphSaint--do not perform the graph-weighted regression task as well, especially in the low-budget regime.

Extension to nonlinear GCN:While our theoretical results are for linear GCNs, we also assess the efficacy of our proposed algorithm on nonlinear GCNs. Using the ReLU activation function and one hidden layer, Figure 1(d) plots the mean squared error as a function of the observation budget, shown as a percentage of the number of observed nodes in the graph. Interestingly, we observe that running the non-linear GCN with uniformly sampled \((AX,)\) fails to perform the graph-weighted regression task, especially in the low-budget regime. On the other hand, we observe that running the non-linear GCN with our proposed sampling technique has a similar performance to the one with exact leverage scores sampling, and the performance of the two proposed schemes tends to be similar to that of Regression with exact \((AX,)\), as the budget increases. Particularly, the error reduction on Algorithm 1 from Regression with uniformly sampled \((AX,)\) is 60% when the budget is at 5%.

**Wall-clock time:** In Table 1, we provide a run-time comparison for the end-to-end training process for a regression task. We compare the wall-clock time of performing a regression task with full \(AX\) computation and that with Algorithm 1 that uses partial observations of \(A\) and \(X\). The results show that our proposed scheme requires orders of magnitude less wall-clock time for large-scale graphs than the regression with exact \(AX\) computation. In particular, for the ogbn-arxiv dataset , our algorithm runs about 40x faster than the regression with the exact computation of \(AX\).

**Peak memory usage:** We also demonstrate the efficacy on the memory usage of our proposed algorithms. See Table 3 in Appendix H for details.

## 6 Concluding Remarks

Motivated by the prohibitive computational/storage costs of running GNNs with large-scale graphs, we considered the problem of subsampling nodes' information for performing a regression task.

   Dataset & \# Nodes & \# Edges & \# Features &  \\   & & &  & use of Algorithm 1 \\  & & & & & and leverage score sampling \\  ogbl-ddi  & 4.3K & 1.3M & 100 & 1.49 & 1.39 \\ ogbn-arxiv  & 169.3K & 1.2M & 128 & 299.48 & 7.40 \\ Synthetic data (Gaussian) & 50.0K & 625.0M & 500 & 27.28 & 5.77 \\ Synthetic data (Gaussian) & 100.0K & 2.5B & 500 & 107.10 & 8.97 \\ Synthetic data (Gaussian) & 150.0K & 5.6B & 500 & 247.70 & 9.96 \\   

Table 1: Wall-clock time comparison on end-to-end processes

Figure 2: MSE w.r.t. observation budget (\(\%\)): (1) Regression with full \(AX\); (2) one with partially observed \(AX\) obtained by sampling rows of \(A\) uniformly at random; (3) one with partial \(AX\) based on the exact leverage score sampling; (4) and (5) one with Algorithm 1 and Algorithm 2; and (6) and (7) ones with GraphSage  inspired sampling and GraphSaint  inspired sampling algorithms.

Our main contribution lies in providing an efficient sampling algorithm for learning linear GNNs with a large number of nodes with a theoretical guarantee. Specifically, for a two-layer linear GNN with adjacency matrix \(A^{n n}\) and data matrix \(X^{n d}\), we show that it is possible to learn the model accurately while observing only \(O(nd n)\) entries of \(A\). This in turn yields a run-time of \(O(nd^{2} n)\), avoiding the computation time \(O(n^{2}d)\) of using the full matrix multiplication \(AX\). While we view our main contribution to be of a theoretical flavor, on real-world benchmark datasets, we also demonstrate the run-time/memory improvements of our proposed schemes over other baselines via wall-clock time and peak-memory usage comparisons and MSE comparisons with respect to the observation budget.

## 7 Discussion and Future Work

Our result on learnability with the run-time complexity reduction is particularly significant when the number of nodes \(n\) is large and the feature dimension is much smaller compared to \(n\) (i.e., \(d n\)). In addition, we note that the result holds even when the graph adjacency matrix and the data matrix are dense (e.g., weighted graphs where the \(a_{ij}\) entries of the adjacency matrix are non-negative real values, and not restricted to \(\{0,1\}\)). Examples include pairwise similarity matrices and Erdos-Renyi graphs with constant edge probability \(p\), which have a number of edges that scales as \(n^{2}\). In such cases, sparse matrix multiplication would not help much, and our approach could be very useful.

Our proposed scheme is not only useful for speeding up the training of linear GCNs. It is also useful in partial observation settings where the algorithm only has access to a partial view of the graph, and we provide theoretical guarantees that this partial view is still sufficient to accurately perform the regression task. On the contrary, conventional speed-up methods such as sparse matrix multiplication are purely a technique for computational purposes and still require complete information on \(A\) and \(X\). For instance, consider a social network setting where, in order to know friendships between users, a learner needs to use an API to query the friendships. Note that one needs fewer queries for using our method (since it requires partial \(A\)) than for using the sparse matrix multiplication (which needs full knowledge of \(A\)). If the graph is large, the query complexity gain becomes larger.

Several extensions of our results are possible. First, establishing theoretical guarantees for the non-linear GCN framework, and for other target tasks, such as graph link prediction and node classification, would be of significant interest. Based on the empirical observation that our training scheme works well for a two-layer non-linear GCN, extending our theoretical results to the non-linear GCN under some assumptions (as in ) on non-linear activation functions seems to be promising. Note that our proposed sampling scheme (based on approximating the leverage score) and the theoretical guarantee we made are tailored for the node-level regression task in the training phase. The reason we focused on this specific setting was so that we could take advantage of the theoretical results on leverage score sampling and linear regression in order to obtain theoretical results for GNN training using a subsampled graph. In particular, we are helped by the fact that in the linear GCN setting, the optimal solution for the linear regression is a matrix-vector product which can be expressed as \(((AX)^{}(AX))^{}(AX)^{}\), where \(A\), \(X\), and \(\) denote the adjacency matrix, data matrix, and the labels respectively.

For graph link prediction and classification tasks, on the other hand, since the optimal solution cannot be expressed as a matrix-vector product (due to non-linearities and different cost functions), our proposed algorithm does not extend to the classification problems in a straightforward way, and similarly, our theoretical guarantee may not hold anymore. For future work, we want to study whether our same subsampling process can have any guarantee for a node classification task. For more general tasks, we believe that we need different sampling strategies other than the leverage score sampling and hence we may need other tools for developing a theoretical guarantee for sampling algorithms.

Another direction for future work is to establish generalization guarantees beyond the approximation errors . Lastly, motivated by the fast acceleration of gradient descent algorithms [24; 25; 26; 27; 28] and iterative schemes that sample a subset of the entries of \(A\)[21; 36], developing algorithms that adaptively select a subset \((A,X)\) to observe based on the information at each gradient descent step would be an interesting direction to explore.