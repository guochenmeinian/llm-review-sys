# Revisiting the Message Passing in

Heterophilous Graph Neural Networks

Anonymous Author(s)

Affiliation

Address

email

###### Abstract

Graph Neural Networks (GNNs) have demonstrated strong performance in graph mining tasks due to their message-passing mechanism, which is aligned with the homophily assumption that adjacent nodes exhibit similar behaviors. However, in many real-world graphs, connected nodes may display contrasting behaviors, termed as _heterophilous_ patterns, which has attracted increased interest in heterophilous GNNs (HTGNNs). Although the message-passing mechanism seems unsuitable for heterophilous graphs due to the propagation of class-irrelevant information, it is still widely used in many existing HTGNNs and consistently achieves notable success. This raises the question: _why does message passing remain effective on heterophilous graphs?_ To answer this question, in this paper, we revisit the message-passing mechanisms in heterophilous graph neural networks and reformulate them into a unified heterophilious message-passing (HTMP) mechanism. Based on HTMP and empirical analysis, we reveal that the success of message passing in existing HTGNNs is attributed to implicitly enhancing the compatibility matrix among classes. Moreover, we argue that the full potential of the compatibility matrix is not completely achieved due to the existence of incomplete and noisy semantic neighborhoods in real-world heterophilous graphs. To bridge this gap, we introduce a new approach named CMGNN, which operates within the HTMP mechanism to explicitly leverage and improve the compatibility matrix. A thorough evaluation involving 10 benchmark datasets and comparative analysis against 13 well-established baselines highlights the superior performance of the HTMP mechanism and CMGNN method.

## 1 Introduction

Graph Neural Networks (GNNs) have shown remarkable performance in graph mining tasks, such as social network analysis [1; 2] and recommender systems [3; 4]. The design principle of GNNs is typically based on the homophily assumption , which assumes that nodes are inclined to exhibit behaviors similar to their neighboring nodes . However, this assumption does not always hold in real-world graphs, where the connected nodes demonstrate a contrasting tendency known as the _heterophily_. In response to the challenges of heterophily in graphs, _heterophilous GNNs (HTGNNs)_ have attracted considerable research interest [6; 8; 9; 10], with numerous innovative approaches being introduced recently [11; 12; 13; 14; 15; 16; 17; 18; 19; 20; 21; 22; 23; 24]. However, the majority of these methods continue to employ a message-passing mechanism, which was not originally designed for heterophilous graphs, as they tend to incorporate excessive information from disparate classes. This naturally raises a question: _Why does message passing remain effective on heterophilous graphs?_

Recently, a few efforts  have begun to investigate this question and reveal that vanilla message passing can work on heterophilous graphs under certain conditions. However, the absence of a unifiedand comprehensive understanding of message passing within existing HTGNNs has hindered the creation of innovative approaches. In this paper, we first revisit the message-passing mechanisms in existing HTGNNs and reformulate them into a unified heterophilous message-passing (HTMP) mechanism, which extends the definition of neighborhood in various ways and simultaneously utilizes the messages of multiple neighborhoods. Specifically, HTMP consists of three major steps namely aggregating messages with explicit guidance, combining messages from multiple neighborhoods, and fusing intermediate representations.

Equipped with HTMP, we further conduct empirical analysis on real-world graphs. The results reveal that the success of message passing in existing HTGNNs is attributed to **implicitly enhancing the compatibility matrix**, which exhibits the probabilities of observing edges among nodes from different classes. In particular, by increasing the distinctiveness between the rows of the compatibility matrix via different strategies, the node representations of different classes become more discriminative in heterophilous graphs.

Drawing from previous observations, we contend that nodes within real-world graphs might exhibit a semantic neighborhood that only reveals a fraction of the compatibility matrix, accompanied by noise. This could limit the effectiveness of enhancing the compatibility matrix and result in suboptimal representations. To fill this gap, we further propose a novel Compatibility Matrix-aware Graph Neural Network (CMGNN) under HTMP mechanism, which utilizes the compatibility matrix to construct desired neighborhood messages as supplementary for nodes and explicitly enhances the compatibility matrix by a targeted constraint. We build a benchmark to fairly evaluate CMGNN and existing methods, which encompasses 13 diverse baseline methods and 10 datasets that exhibit varying levels of heterophily. Extensive experimental results demonstrate the superiority of CMGNN and HTMP mechanism. The contributions of this paper are summarized as:

* We revisit the message-passing mechanisms in existing HTGNNs and reformulate them into a unified heterophilous message-passing mechanism (HTMP), which not only provides a macroscopic view of message passing in HTGNNs but also enables people to develop new methods flexibly.
* We reveal that the effectiveness of message passing on heterophilous graphs is attributed to implicitly enhancing the compatibility matrix among classes, which gives us a new perspective to understand the message passing in HTGNNs.
* Based on HTMP mechanism and empirical analysis, we propose CMGNN to unlock the potential of the compatibility matrix in HTGNNs. We further build a unified benchmark that overcomes the issues of current datasets for fair evaluation1. Experiments show the superiority of CMGNN.

## 2 Preliminaries

Given a graph \(=(,,,,)\), \(\) is the node set and \(\) is the edge set. Nodes are characterized by the feature matrix \(^{N d_{f}}\), where \(N=||\) denotes the number of nodes, \(d_{f}\) is the features dimension. \(^{N 1}\) is the node labels with the one-hot version \(^{N K}\), where \(K\) is the number of node classes. The neighborhood of node \(v_{i}\) is denoted as \(_{i}\). \(^{N N}\) is the adjacency matrix, and \(=(_{1},...,_{n})\) represents the diagonal degree matrix, where \(_{i}=_{j}_{ij}\). \(}=+\) represents the adjacency matrix with self-loops. Let \(^{N d_{r}}\) be the node representations with dimension \(d_{r}\) learned by the models. We use \(\) to represent a matrix with all elements equal to 1, and \(\) for a matrix with all elements equal to 0.

**Homophily and Heterophily**. High homophily is observed in graphs where a substantial portion of connected nodes shares identical labels, while high heterophily corresponds to the opposite situation. For measuring the homophily level, two widely used metrics are edge homophily \(h^{e}\) and node homophily \(h^{n}\), defined as \(h^{e}=|e_{u,v},\,_{u}=_{v}\} |}{||}\) and \(h^{n}=|}_{v}u _{v},\,_{u}=_{v}\}]}{_{v}}\). Both metrics have a range of \(\), where higher values indicate stronger homophily and lower values indicate stronger heterophily.

**Vanilla Message Passing (VMP)**. The vanilla message-passing mechanism plays a pivotal role in transforming and updating node representations based on the neighborhood . Typically, the mechanism operates iteratively and comprises two stages:

\[}^{l}=(,^{l-1}), ^{l}=(^{l-1},}^{l} ),\] (1)

where the AGGREGATE function first aggregates the input messages \(^{l-1}\) from neighborhood \(\) into the aggregated one \(}^{l}\), and subsequently, the COMBINE function combines the messages of node ego and neighborhood aggregation, resulting in updated representations \(^{l}\).

## 3 Revisiting Message Passing in Heterophilous GNNs.

To gain a thorough and unified insight into the effectiveness of message passing in HTGNNs, we revisit message passing in various notable HTGNNs [11; 12; 13; 14; 15; 16; 17; 18; 19; 20; 21; 22; 23; 24] and propose a unified heterophilous message passing (HTMP) mechanism, structured as follows:

\[}^{l}_{r}=(_{r},_{r}, ^{l-1}),\ ^{l}=(\{}^{l}_{r}\}_{r=1}^{R}), \ =(\{^{l}\}_{l=0}^{L}).\] (2)

Generally, HTMP extends the definition of neighborhood in various ways and simultaneously utilize the messages of multiple neighborhoods, which is the key for better adapting to heterophily. We use \(R\) to denote the number of neighborhoods used by the model. In each message passing layer \(l\), HTMP separately aggregates messages within \(R\) neighborhoods and combines them. The methodological analysis of some representative HTGNNs and more details can be seen in Appendix A. Compared to the VMP mechanism, HTMP mechanism has advances in the following functions:

(i) To characterize different neigborhoods, the **AGGREGATE** function in HTMP includes the **neighborhood indicator**\(_{r}\) to indicate the neighbors within a specific neighborhood \(r\). The adjacency matrix \(\) in VMP is a special neighborhood indicator that marks the neighbors in the raw neighborhood. To further characterize the aggregation of different neighborhoods, HTMP introduces the **aggregation guideline**\(_{r}\) for each neighborhood \(r\). In VMP, the aggregation guidance is an implicit parameter of the AGGREGATE function since it only works for the raw neighborhood. A commonly used form of the AGGREGATE function is AGGREGATE(\(_{r},_{r},^{l-1})=(_{r} _{r})^{l-1}^{l}_{r}\), where \(\) is the Hadamard product and \(^{l}_{r}\) is a weight matrix for message transformation. We take

    &  &  &  &  \\    & Type & \(\) & & & \(\) \\  GCN  &  & \([}]\) &  & \([}^{d}]\) & / & \(=^{L}\) \\   APPNP  & & \([,}]\) & & \([,}^{d}]\) & WeightedAdd & \(=^{L}\) \\   GCNI  & & \([,}]\) & & \([,}^{d}]\) & WeightedAdd & \(=^{L}\) \\   GAT  & & \([}]\) & AdaWeight & \([^{av}]\) & / & \(=^{L}\) \\  GPR-GCN  &  & \([}]\) &  & \([}^{d}]\) & / & AdaAdd \\   OrderedRNN  & & \([,]\) & & \([,^{d}]\) & AdaCat & \(=^{L}\) \\  ACM-GCN  &  & \([,]\) &  & \([,^{av}]\) & WeightedAdd & \(=^{L}\) \\   FAGCN  & & \([,]\) & & \([,^{av}]\) & WeightedAdd & \(=^{L}\) \\   GBB-GNN  & & \([,,]\) & & \([,^{av},1-^{av}]\) & Add & \(=^{L}\) \\   SimP-GCN  & & \([,},_{j}]\) & & \([,}^{d},^{d}_{j}]\) & AdaAdd & \(=^{L}\) \\   H2GCN  & & \([,_{k2}]\) & & \([^{d},^{d}_{j}]\) & Cat & Cat \\   Geom-GCN  &  & \([^{d}_{j},^{d}_{j}]\) & Cat & \(=^{L}\) \\   MixHop  & & \([,,_{k2},...,_{nk}]\) & & \([,^{d},^{d}_{j},,^{d}_{k}]\) & Cat & \(=^{L}\) \\   UGCN  &  & \([},}_{k2},_{j}]\) &  & \([}^{av},}^{av}_{k2},^{av}_{k}]\) & AdaAdd & \(=^{L}\) \\   WRGNN  & & \([_{c1},...,_{cr},...,_{cR}]\) & & \([_{c1}^{av},...,^{av}_{cr},...,^{av}_{cr},..., ^{av}_{cr}]\) & Add & \(=^{L}\) \\   HOG-GCN  & & \([,_{nk}]\) & & \([,^{pr}]\) & WeightedAdd & \(=^{L}\) \\   GloGNN  & & \([,]\) & & RelaEst & \([,^{pr}]\) & WeightedAdd & \(=^{L}\) \\  GGCN  & Dis & \([,_{p},_{n}]\) & & \([,^{pr},^{pr}_{n}]\) & AdaAdd & \(=^{L}\) \\   

* The correspondence between the full ferm and the abbreviation: Raw Neighborhood (Raw), Neighborhood Redefine (ReDef), Neighborhood Discrimination (Dis), Degree-based Averaging (DegAvg), Adaptive Weights (AdgWeight), Relation Estimation (RelaEst), Addition (Add), Weighted Addition (WeightAdd), Adaptive Weighted Addition (AdgAdd), Concatenation (Cat), Adaptive Dimension Concatenation (AdaCat).
* More details about the notations are available in Appendix A.1.

Table 1: Revisiting the message passing in representative heterophilous GNNs under the perspective of HTMP mechanism.

this as the general form of the AGGREGATE function and only analyze the neighborhood indicators and the aggregation guidance in the following.

The _neighborhood indicator_\(_{r}\{0,1\}^{N N}\) indicates neighbors associated with central nodes within neighborhood \(r\). To describe the multiple neighborhoods in HTGNNs, neighborhood indicators can be formed as a list \(=[_{1},...,_{r},...,_{R}]\). For the sake of simplicity, we consider the identity matrix \(^{N N}\) as a special neighborhood indicator for acquiring the nodes' ego messages. The _aggregation guidance_\(_{r}^{N N}\) can be viewed as pairwise aggregation weights in most cases, which has the multiple form \(=[_{1},...,_{r},...,_{R}]\). Table 1 illustrates the connection between message passing in various HTGNNs and HTMP mechanism.

(ii) Considering the existence of multiple neighborhoods, the **COMBINE** function in HTMP need to integrate multiple messages instead of only the ego node and the raw neighborhood. Thus, the input of the COMBINE function is a set of messages \(}_{r}^{l}\) aggregated from the corresponding neighborhoods. In HTGNNs, addition and concatenation are two common approaches, each of which has variants. An effective COMBINE function is capable of simultaneously processing messages from various neighborhoods while preserving their distinct features, thereby reducing the effects of heterophily.

(iii) In VMP, the final output representations are usually the one of the final layer: \(=^{L}\). Some HTGNNs utilize the combination of intermediate representations to leverage messages from different localities, adapting to the heterophilous structural properties in different graphs. Thus, we introduce an additional **FUSE** function in HTMP which integrates multiple representations \(^{l}\) of different layers \(l\) into the final \(\). Similarly, the FUSE function is based on addition and concatenation.

## 4 Why Does Message Passing Still Remain Effective in Heterophilous Graphs?

Based on HTMP mechanism, we further dive into the motivation behind the message passing of existing HTGNNs. Our discussion begins by examining the difference between homophilous and heterophilous graphs. Initially, we consider the homophily ratios \(h^{e}\) and \(h^{n}\), as outlined in Section 2. However, a single number is not able to indicate enough conditions of a graph. Ma et al.  propose the existence of a special case of heterophily, named "good" heterophily, where the VMP mechanism can achieve strong performance and the homophily ratio shows no difference. Thus, to better study the heterophily property, here we introduce the _Compatibility Matrix_ to describe graphs:

**Definition 1**: _Compatibility Matrix (CM): The potential connection preference among classes within a graph. It's formatted as a matrix \(^{K K}\), where the \(i\)-th row \(_{i}\) denotes the connection probabilities between class \(i\) and all classes. It can be estimated empirically by the statistics among nodes as follows:_

\[=(^{T}^{nb}),^{nb}= },\] (3)

_where \(()\) denotes the L1 normalization and \(T\) is the matrix transpose operation. \(^{nb}^{N K}\) is the **semantic neighborhoods** of nodes, which indicates the proportion of neighbors from each class in nodes' neighborhoods._

We visualize the CM of a homophilous graph Photo  and a heterophilous graph Amazon-Ratings  in Figure 1(a) and 1(b). The CM in Photo displays an identity-like matrix, where the diagonal elements can be viewed as the homophily level of each class. With this type of CM, the VMP mechanism learns representations comprised mostly of messages from same the class, while messages of other classes are diluted. _Then how does HTMP mechanism work on heterophilous graphs without an identity-like CM?_ The "good" heterophily inspires us, which we believe corresponds to a CM with enough discriminability among classes. We conduct experiments on synthetic graphs to confirm this idea, with details available in Appendix C. Also, we find "good" heterophily in real-world graphs though it's not as significant as imagined. Thus, we have the following observation:

**Observation 1**: _(Connection between CM and VMP). When enough (depends on data) discriminability exists among classes in CM, vanilla message passing can work well in heterophilous graphs._

With this observation, we have a conjecture: _Is HTMP mechanism trying to enhance the discriminability of CM?_ Some special designs in HTMP intuitively meet this. For example, _feature-similarity-based neighborhood indicators_ and _neighborhood discrimination_ are designed to construct neighborhoodswith high homophily, that is, an identity-like CM with high discriminability. We plot the CM of feature-similarity-based neighborhood on Amazon-Ratings in Figure 1(c) to confirm it. Moreover, we investgate two representative methods ACM-GCN  and GPRGNN , showing that they also meet this conjecture with the posterior proof in Appendix D. ACM-GCN combines the messages of node ego, low-frequency and high-frequency with adaptive weights, which actually motifs the edge weights and node weights to build a new CM. GPRGNN has a FUSE function with adaptive weights while other settings are the same as GCN. It actually integrates the CMs of multiple-order neighborhoods with adaptive weights to form a more discriminative CM. These lead to the answer to the aforementioned question:

**Observation 2**: _(Connection between CM and HTMP). The unified goal of various message passing in existing HTGNNs is to utilize and enhance the discriminability of CM on heterophilous graphs. In other words, the success of message passing in existing HTGNNs benefits from utilizing and enhancing the discriminability of CM._

Furthermore, we notice that the power of CM is not fully released due to the incomplete and noisy semantic neighborhoods in real-world heterophilous graphs. We use the perspective of distribution to describe the issue more intuitively: The semantic neighborhoods of nodes from the same class collectively form a distribution, whose mean value indicates the connection preference of that class, i.e. \(_{i}\) for class \(i\). Influenced by factors such as degree and randomness, the semantic neighborhood of nodes in real-world graphs may display only a fraction of CM accompanied by noise. It can lead to the overlap between different distributions as shown in Figure 1(d), where the existence of overlapping parts means nodes from different classes may have the same semantic neighborhood. This brings a great challenge since the overlapping semantic neighborhood may become redundant information during message passing.

## 5 Method

To fill this gap, we further propose a method named Compatibility Matrix-Aware GNN (CMGNN), which leverages the CM to construct desired neighborhood messages as supplementary, providing valuable neighborhood information for nodes to mitigate the impact of incomplete and noisy semantic neighborhoods. The desired neighborhood message denotes the averaging message within a neighborhood when a node's semantic neighborhoods meet the CM of the corresponding class, which converts the discriminability from CM into messages. CMGNN follows the HTMP mechanism and constructs a supplementary neighborhood indicator along with the corresponding aggregation guidance to introduce supplementary messages. Further, CMGNN introduces a simple constraint to explicitly enhance the discriminability of CM.

Message Passing in CMGNN.CMGNN aggregates messages from three neighborhoods for each node, including the ego neighborhood, raw neighborhood, and supplementary neighborhood. Following the HTMP mechanism, the message passing of CMGNN cen be described as follows:

\[&}_{r}^{l}=( _{r},_{r},^{l-1})=(_{r}_{r})^{l-1}_{r}^{l},\\ &^{l}=(\{}_{r}^{l}\}_{ r=1}^{3})=(\{}_{r}^{l}\}_{r=1}^{3}),\\ &=(\{^{l}\}_{l=0}^{L})=_{l=0}^{L}^{l},\] (4)

Figure 1: Visualizations of the compatibility matrix and the example of distribution overlap.

where AdaWeight is the adaptive weighted addition implemented by an MLP with Softmax, \(\|\) denotes the concatenation. The neighborhood indicators and aggregation guidance of the three neighborhoods are formatted as follows:

\[_{1}^{l}=,\ _{1}^{l}=,\ \ \ \ _{2}^{l}=,\ _{2}^{l}=^{-1},\ \ \ _{3}^{l}=^{sup},\ _{3}^{l}=^{sup},\] (5)

where \(^{sup}\) and \(^{sup}\) are described below.

The supplementary neighborhood indicator \(^{sup}\) assigns \(K\) additional virtual neighbors for each node: \(^{sup}=^{N K}\). Specifically, these additional neighbors are \(K\) virtual nodes, constructed as the prototypes of classes based on the labels of the training set. The attributes \(^{ptt}^{K d_{I}}\), neighborhoods \(^{ptt}^{K N}\) and labels \(^{ptt}^{K K}\) of prototypes are defined as follows:

\[^{ptt}=(_{train}^{T}_{train}),\ ^{ptt}=,\ ^{ptt}=,\] (6)

where \(_{train}\) and \(_{train}\) are the one-hot labels and attributes of nodes in the training set. Utilizing class prototypes as supplementary neighborhoods can provide each node with representative messages of classes, which builds the basis for desired neighborhood messages.

The supplementary aggregation guidance \(^{sup}=}}\) indicates the desired semantic neighborhood of nodes, i.e. the desired proportion of neighbors from each class in nodes' neighborhoods according to the probability that nodes belong to each class. \(}\) is the estimated compatibility matrix described in below. Using soft logits instead of one-hot pseudo labels preserves the real characteristics of nodes and reduces the impact of wrong predictions. During the message aggregation in the supplementary neighborhoods, the input representations \(^{l-1}\) are replaced by the representations of virtual prototype nodes \(_{ptt}^{l-1}\), which are obtained by the same message-passing mechanism as real nodes.

Similar to existing methods [18; 19], we also regard topology structure as a kind of additional available node features. Thus, the input representation of the first layer can be obtained in two ways:

\[^{0}=[^{X}\|}^{A}] ^{0},\ \ ^{0}=^{0}.\] (7)

Note that in practice, we use ReLU as the activation function between layers. From the perspective of HTMP mechanism, our special design is to introduce an additional neighborhood indicator \(^{sup}\) by neighborhood redefining and aggregation guidance \(^{sup}\), which can be seen as a form of relation estimation along with good interpretability. Meanwhile, these designs greatly reduce the time and space cost via the \(N K\) form.

Compatibility Matrix Estimation.The CM can be directly calculated via Eq 3 with full-available labels. However, the label information is not entirely available in semi-supervised settings. Thus, we try to estimate the CM with the help of semi-supervised and pseudo labels. Since the pseudo labels predicted by the model might be wrong, which can lead to low-quality estimation, we introduce the confidence \(^{N 1}\) based on the information entropy to reduce the impact of wrong predictions, where a high entropy means low confidence:

\[_{i}= K-(}_{i})[0, K],\] (8)

where \(}^{N K}\) is the soft pseudo labels composed of labels from the training set and model predictions. Then the nodes' semantic neighborhoods \(^{nb}=((})) ^{N K}\) are calculated considering the confidence.

Further, the degrees of nodes also influence the estimation. As we mentioned in Section 4, the semantic neighborhood of low-degree nodes may display incomplete CM, leading to a significant gap between semantic neighborhoods and corresponding CM. Thus, they deserve low weights during the estimation. We manually set up two fixed thresholds and a weighting function range in \(\):

\[_{i}^{d}=\{_{i}/2K,& _{i} K,\\ 0.25+_{i}/4K,&K<_{i} 3K,\\ 1,&otherwise..\] (9)

When a node's degree \(_{i}\) is smaller than the number of classes \(K\), its semantic neighborhood is unlikely to display complete CM, corresponding to a low weight. And when the node degree is greater than \(3K\), we believe it can display near-complete CM, corresponding to the maximum weight. Finally, we can estimate the compatibility matrix \(}^{K K}\) as follows:

\[}=((^{d}})^{T})^{nb}.\] (10)

[MISSING_PAGE_FAIL:7]

Baseline Methods.As baseline methods, we choose 13 representative homophilous and heterophilous GNNs, including (i) shallow base model: MLP; (ii) homophilous GNNs: GCN , GAT , GCNII ; (iii) heterophilous GNNs: H2GCN , MixHop , GBK-GNN , GGCN , GloGNN , HOGGCN , GPR-GNN , ACM-GCN  and OrderedGNN . For each method, we integrate its official/reproduced code into a unified codebase and search for parameters in the space suggested by the original papers. More experimental settings can be found in Appendix F.4 and G.1.

### Main Results

Following the constructed benchmark, we evaluate methods and report the performance in Table 2.

**Performance of Baseline Methods.** With the new benchmarks, some interesting observations and conclusions can be found when analyzing the performance of baseline methods. First, comparing the performance of MLP and GCN, we can find "good" heterophily in Amazon-Ratings, Chameleon-F, and Squirrel-F. Meanwhile, when the homophily level is not high enough, "bad" homophily may also exist as shown in BlogCatalog and Wikics. These results once again support the observations about CMs. Therefore, **homophilous GNNs** can also work well in heterophilous graphs as GCNII has an average rank of 4.1, which is better than most HTGNNs. This is attributed to the initial residual connection in GCNII actually playing the role of ego/neighbor separation, which is suitable in heterophilous graphs. As for **heterophilous GNNs**, they are usually designed for both homophilous and heterophilous graphs. Surprisingly, MixHop, as an early method, demonstrated quite good performance. In fact, from the perspective of HTMP, it can be considered a degenerate version of OrderedGNN with no learnable dimensions. As previous SOTA methods, OrderedGNN and ACM-GCN prove their strong capabilities again.

**Performance of CMGNN.** CMGNN achieves the best performance in 6 datasets and an average rank of 2.1, which outperforms baseline methods. This demonstrates the superiority of utilizing and enhancing the CM to handle incomplete and noisy semantic neighborhoods, especially in heterophilous graphs. Regarding the suboptimal performance in Actor, we believe that this is due to the CM in this dataset are not discriminative enough to provide valuable information via the supplementary messages and hard to enhance. In homophilous graphs, due to the identity-like CMs, the overlap between distributions is relatively less, leading to a minor contribution from supplement messages. Yet CMGNN still achieves top-level performances.

### Ablation Study

We conduct an ablation study on two key designs of CMGNN, including the supplementary messages of the desired neighborhood (SM) and the discrimination loss (DL). The results are shown in Table 3. _First of all_, both SM and DL have indispensable contributions except for Flickr, BlogCatalog, and Pubmed, in which the discrimination loss has no effect. This may be due to the discriminability of desired neighborhood messages reaching the bottlenecks and can not be further improved by DL _Meanwhile_, the extent of their contributions varies across datasets. SM plays a more important role in most datasets except Roman-Empire, Wikics, and Photo, in which the number of nodes that need supplementary messages is relatively small and DL has great effects. **Further**, we notice that with SM and DL, CMGNN can reach a smaller standard deviation most of the time. This illustrates that CMGNN achieves more stable results by handling nodes with incomplete and noisy semantic neighborhoods. As for the opposite result on Chameleon-F, this may attributed to the small size of this dataset (890 nodes), which can lead to naturally unstable results.

 
**Variations** & **Remark-Eagience** & **Amanage-Rating** & **Chameleon-F** & **Spatirel-F** & **Actor** & **Flickr** & **BlogCatalog** & **Waking** & **Pubmed** & **Photo** \\ 
**CMGNN** & **84.3\(\) 1.27** & **52.13\(\) 0.58** & **45.70\(\)0.42** & **41.89\(\) 1.34** & **36.82\(\) 0.78** & **96.26\(\) 0.46** & **97.00\(\) 0.82** & **84.95\(\) 0.73** & **99.93\(\) 0.32** & **95.48\(\) 0.29** \\  WO SM & 83.4\(\) 1.02 & 51.9\(\) 0.61 & 42.5\(\) 1.21 & 40.79\(\) 1.89 & 36.12\(\) 1.21 & 91.22\(\) 2.03 & 95.52\(\) 0.63 & 83.97\(\) 0.42 & 85.41\(\)0.40 \\ WO DL & 33.68\(\) 1.24 & 52.68\(\)0.32 & 45.47\(\) 3.09 & 46.01\(\)2.41 & 36.23\(\) 1.22 & **96.26\(\) 0.48** & **97.00\(\) 0.82** & 81.62\(\) 1.83 & **89.99\(\) 0.32** & 95.26\(\) 0.35 \\ WO SM and DL & 83.52\(\) 1.91 & 51.58\(\) 1.04 & 41.32\(\) 2.93 & 40.07\(\) 2.41 & 35.61\(\) 1.48 & 92.32\(\) 0.83 & 96.52\(\) 0.63 & 81.62\(\) 1.67 & 89.70\(\) 0.44 & 94.66\(\) 0.42 \\  

Table 3: Ablation study results (%) between CMGNN and three ablation variants, where SM denotes supplementary messages of the desired neighborhoods and DL denotes the discrimination loss.

### Visualization of Compatibility Matrix Estimation

We visualize the observed and estimated CMs by CMGNN in Figure 2 with heat maps. Obviously, CMGNN estimates CMs that are very close to those existing in graphs. This shows that even with incomplete node labels, CMGNN can estimate high-quality CMs which provides valuable neighborhood information to nodes. Meanwhile, it can adapt to graphs with various levels of heterophily. More results can be seen in Appendix G.2.1.

### Performance on Nodes with Various Levels of Degrees

To verify the effect of CMGNN on nodes with incomplete and noisy semantic neighborhoods, we divide the test set nodes into 5 parts according to their degrees and report the classification accuracy respectively. We compare CMGNN with 3 top-performance methods and show the results in Table 4. In general, nodes with low degrees tend to have incomplete and noisy semantic neighborhoods. Thus, our outstanding performances on the top 20% nodes with the least degree demonstrate the effectiveness of CMGNN for providing desired neighborhood messages. Further, we can find that OrderedGNN and GCNII are good at dealing with nodes with high degrees, while ACM-GCN is relatively good at nodes with low degrees. And CMGNN, to a certain extent, can be adapted to both situations at the same time.

## 7 Conclusion and Limitations

In this paper, we revisit the message passing mechanism in existing heterophilous GNNs and reformulate them into a unified heterophilous message passing (HTMP) mechanism. Based on the HTMP mechanism and empirical analysis, we reveal that the reason for message passing remaining effective is attributed to implicitly enhancing the compatibility matrix among classes. Further, we propose a novel method CMGNN to unlock the potential of the compatibility matrix by handling the incomplete and noisy semantic neighborhoods. The experimental results show the effectiveness of CMGNN and the feasibility of designing a new method following HTMP mechanism. We hope the HTMP mechanism and benchmark can further provide convenience to the community.

This work mainly focuses on the message passing mechanism in existing HTGNNs under the semi-supervised setting. Thus, the other designs in HTGNNs such as objective functions are not analyzed in this paper. The proposed HTMP mechanism is suitable for only a large part of existing HTGNNs which still follow the message passing mechanism.

  
**Dataset** &  &  &  \\  Deg. Prop.(\%) & 0\(\)20 & 20\(\)40 & 40\(\)60 & 60\(\)80 & 80\(\)100 & \(\)20 & 20\(\)40 & 40\(\)60 & 60\(\)80 & 80\(\)100 & 0\(\)20 & 20\(\)40 & 40\(\)60 & 60\(\)80 & 80\(\)100 \\ 
**CMGNN** & **99.78** & **98.36** & **53.08** & 41.74 & 47.86 & **92.56** & **91.19** & 92.21 & **93.24** & 93.65 & **94.13** & **97.17** & **98.29** & **97.99** & **97.47** \\  ACM-GCN & 57.35 & 56.21 & 51.74 & 41.55 & 46.47 & 90.44 & 91.17 & **92.85** & 91.19 & 89.50 & 92.17 & 96.68 & 97.83 & 97.84 & 96.51 \\ OrderIndGNN & 56.32 & 56.16 & 51.20 & **41.85** & **50.26** & 86.48 & 90.07 & 92.40 & 92.79 & 93.40 & 92.19 & 96.09 & 97.48 & 97.36 & 96.27 \\ GCNII & 50.61 & 49.94 & 47.49 & **41.86** & 47.76 & 87.49 & 90.54 & 92.29 & 92.68 & **95.09** & 92.81 & 96.73 & 97.58 & 97.90 & 97.43 \\   

Table 4: Node classification accuracy (%) comparison among nodes with different degrees.

Figure 2: The visualization of observed (Obs) and estimated (Est) compatibility matrixes.