# GL-NeRF: Gauss-Laguerre Quadrature Enables Training-Free NeRF Acceleration

Silong Yong &Yaqi Xie &Simon Stepputtis &Katia Sycara

Carnegie Mellon University

{silongy, yaqix, sstepput, sycara}@andrew.cmu.edu

###### Abstract

Volume rendering in neural radiance fields is inherently time-consuming due to the large number of MLP calls on the points sampled per ray. Previous works would address this issue by introducing new neural networks or data structures. In this work, we propose GL-NeRF, a new perspective of computing volume rendering with the Gauss-Laguerre quadrature. GL-NeRF significantly reduces the number of MLP calls needed for volume rendering, introducing no additional data structures or neural networks. The simple formulation makes adopting GL-NeRF in any NeRF model possible. In the paper, we first justify the use of the Gauss-Laguerre quadrature and then demonstrate this plug-and-play attribute by implementing it in two different NeRF models. We show that with a minimal drop in performance, GL-NeRF can significantly reduce the number of MLP calls, showing the potential to speed up any NeRF model. Code can be found in project page https://silongyong.github.io/GL-NeRF_project_page/.

## 1 Introduction

Neural Radiance Fields (NeRFs)  have shown promising results for synthesizing images from novel views. Plenty of works extend NeRF towards different aspects applicable in the real world (see related works for details). The core component for NeRF's success is volume rendering, which requires approximating an integral by densely sampling points along the ray and evaluating volume density and radiance using neural networks for them. In practice, a dense set of points is evaluated by expensive operations like neural network inferences for a single pixel, which could be redundant. Works have been done to reduce the time needed for rendering images, aiming at providing NeRF with a real-time rendering ability [9; 44; 24; 29; 8]. Despite the promising results shown by these works, they propose different approaches for achieving real-time rendering by introducing new networks, new data structures, _etc_. Therefore, each individual work requires training from scratch with a specific optimization goal. In this work, we propose a novel lightweight method that could be implemented in any existing NeRF-based models that require volume rendering without further training. In contrast to existing works, our approach introduces no additional representation or neural network and is training-free. We make minimal modifications to the computation of the volume rendering integral, making it rely on much fewer samples.

Our approach arises from revisiting the volume rendering integral, the key discovery is that with a simple change of variable, we can turn the integral into a pure exponentially weighted integral of color. This specific form has a Gauss quadrature (_i.e_. the Gauss-Laguerre quadrature) which best approximates it mathematically. Naturally, we propose to use the Gauss-Laguerre quadrature to directly compute the volume rendering integral, which we call GL-NeRF (Gauss Laguerre-NeRF), leading to much lower computational cost for approximating the integral and therefore lower time and memory usage. Computing the points needed for the integral requires a dense evaluation of per-point density. However, the efficiency for this step can be improved using modern techniqueslike factorized tensors . Benefiting from the guarantee of the highest precision Gauss quadrature provides, only a very small number of fixed points could provide comparable results to the heavy and redundant strategy NeRF adopts, leading to free speedup.

To verify the use of the Gauss-Laguerre quadrature, we conduct an empirical study on the landscape of color function. We also analyze the relationship between our approach and other techniques that aim to reduce the sample points for NeRF. We demonstrate the plug-and-play property of our method by directly incorporating it into vanilla NeRF and TensoRF models that are already trained on NeRF-Synthetic and LLFF datasets. Furthermore, we showcase the drop in time and memory usage as a direct outcome of reducing the computational cost.

GL-NeRF provides a different perspective for computing volume rendering and has the potential to be a direct plug-in for existing NeRF-based products. Specifically, our contributions are three-fold. We propose GL-NeRF, a brand new perspective for computing volume rendering with the Gauss-Laguerre quadrature with no additional component introduced. We analyze the validity of using the Gauss-Laguerre quadrature for volume rendering integral and the relationship between our approach and existing sample-efficient NeRFs. We demonstrate that GL-NeRF could be incorporated into any NeRF model without further training. To the best of our knowledge, GL-NeRF is the first method that could be used without training in any NeRF models thanks to the simple formulation. We showcase that GL-NeRF reduces computational cost, time and memory usage while keeping the rendering quality.

## 2 Related work

Volume rendering.Volume rendering has been widely used in computer graphics and vision applications [25; 43; 7]. It maps a 3D scene onto 2D images by a weighted integral over the color of the points along the corresponding rays with a function of opacity (volume density) as weight. In practice, the integral is approximated using a finite sum over sampled points along the ray as derived in . Implicit scene models like NeRF , Plenoxels  and 3D gaussians  and most of their follow-up all adopt this technique as the render pipeline. Since randomly sampling in space for approximating the integral may bring unnecessary information (_i.e._ sampling in empty space) that may cost extra computation, plenty of works aim to address that by introducing different techniques for better approximation of the component needed for volume rendering integral (_i.e._ volume density, radiance) [40; 44; 29; 23; 21; 2; 36]. PL-NeRF  proposes to use piecewise linear function for approximating the volume density throughout the space, leading to fewer points needed for the "fine" stage sampling proposed by . AutoInt and DIVeR [23; 44] introduce a neural network for approximating the integral of volume density instead of using Monte-Carlo sampling. DONeRF  reduces the sampled point needed for computing the integral by introducing a depth oracle neural network that predicts the surface position of the underlying scene and samples the points near the

Figure 1: GL-NeRF method overview. The vanilla volume rendering in NeRF requires uniform sampling in space. This leads to a huge number of computationally heavy MLP calls since we have to assign each point a color value. Our approach, GL-NeRF, significantly reduces the number of points needed for volume rendering and selects points in the most informative area.

surface, which contributes the most to the visual effect in the images. MCNeRF  proposes to use Monte-Carlo rendering and denoising to do sample efficient rendering, but it still introduces a denoiser network that requires per-scene training. Different from these previous works, Our work proposes to use the Gauss-Laguerre quadrature to directly improve the precision of the volume rendering integral itself, introduces no additional neural networks or data structures and remains in the simplest version, leading to its adaptability into any existing work that relies on volume rendering integral.

NeRFs.Neural Radiance Fields (NeRFs) have proved to be a powerful tool for novel view synthesis . It uses a coordinate-based multi-layer perceptron (MLP) to represent the scene and render high-fidelity images from different views. The render is done by pixel-wise volumetric rendering  with density and color evaluated using the MLP on hundreds of sampled points along the ray. For modeling high-frequency information in the scene, NeRF uses positional encoding to map the input coordinates onto high-frequency bands. The success of NeRF has triggered an explosive emergence of follow-up works. There are plenty of works focusing on improving or extending the ability of NeRF towards different aspects. Aliasing along xy coordinates has been tackled , unbounded scenes , dynamic scenes  and scenes with semantic information  have been well explored and demonstrated the potential of implicit scene representation with NeRF. Nonetheless, NeRF requires plenty of time for training and rendering, blocking its way of being used for real-time rendering. The bottleneck of the computation time is the MLP used. There are two main branches of work for extending NeRF towards real-time rendering. The first branch introduces different data structure  for scene representation. Another branch, in which our method falls, improves the sample efficiency of the model  to accelerate NeRF rendering process. While previous works draw their intuition from the underlying physics perspective and thus need different formulations of the sampling strategy and different neural network architecture for predicting the surface position of the underlying scenes, we propose our method based on a mathematical observation while maintaining the overall pipeline. Benefiting from this, our work could be seamlessly incorporated into any existing NeRF-related works without further training. On the other hand, despite being derived from the mathematical perspective, our method still intuitively satisfies the underlying physical constraints.

Preliminaries

### NeRF and volume rendering

NeRF  is a powerful implicit 3D scene model for novel view synthesis. At the core of its rendering ability is volume rendering. NeRF uses coordinate-based MLP to encode the scene, assigning volume density (opacity) and radiance (color) to spatial points. When used for synthesizing new views, it casts a ray \((t)=+t\) through the pixel to be rendered, sample points along the ray and compute volume density and radiance for these points. These values are then aggregated together using Eq.1 to give the color of the pixel.

\[}()=_{i=1}^{N}w_{i}((t_{i})),\] (1)

where

\[w_{i}=T_{i}(1-exp(-((t_{i}))_{i})),\] (2)

\[T_{i}=exp(-_{j=0}^{i-1}((t_{j}))_{j}),\] (3)

\(t_{i}\) represents the sampled position along the ray and \(_{i}=t_{i+1}-t_{i}\) is the distance between two nearby sampled points. NeRF uses an MLP to represent volume density \(\) and color \(\). The loss function for training NeRF is simply the square error between rendered pixel colors and the corresponding pixel colors over batch of rays \(\). Variants of NeRF like TensoRF use different representations for volume density and color, but the process of volume rendering remains the same.

\[=_{}\|}()-()\|_ {2}^{2}\] (4)

### Gauss quadrature

An \(n\)-point Gauss quadrature  is a method for numerical integration that guarantees to yield exact results for integral of polynomials of degree \(2n-1\) or less, which is the highest possible precision for approximating an integral by quadrature. Intuitively, consider approximating an integral using quadrature as in Eq. 5

\[_{-1}^{1}p(x)dx=_{i=1}^{n}w_{i}p(x_{i}),\] (5)

where \(p(x)\) is a polynomial of degree \(2n-1\), \(w(x)\) is a weight function and \(I\) is the interval for computing the integral. We first give the definition of orthogonality of two polynomials \(p_{m}(x)\) and \(p_{n}(x)\)

\[_{-1}^{1}p_{m}(x)p_{n}(x)dx=0,\] (6)

where \(p_{m}(x)\) is of degree \(m\), \(p_{n}(x)\) is of degree \(n\) and \(m n\). we can use long division for \(p(x)\) to obtain

\[p(x)=q(x)L_{n}(x)+r(x),\] (7)

where \(L_{n}(x)\) is a polynomial of degree \(n\) that is orthogonal to any polynomials that have degree less than \(n\) (_i.e_. \(n\) degree Legendre polynomial), \(q(x)\) and \(r(x)\) are both polynomials with degree less than \(n\). Then

\[_{-1}^{1}p(x)dx=_{-1}^{1}q(x)L_{n}(x)dx+_{-1}^{1}r(x)dx.\] (8)

Since \(L_{n}(x)\) is orthogonal to any polynomials with degree less than \(n\), the first term on the right hand side of Eq. 8 should equal to \(0\). Since it doesn't contribute to the computation of the integral, we may also neglect it when computing the quadrature. Therefore, we should choose \(x_{i}\) that satisfies \(L_{n}(x_{i})=0\). With this intuition bearing in mind, carefully choosing the weights \(w_{i}\) for computing the quadrature would help us precisely calculate Eq. 5 because we have \(n\) points to compute the second term on the right hand side of Eq. 8, which is an integral of a polynomial of degree less than \(n\)In general, given a function \(f(x)\), Gauss quadrature computes its integral on \([-1,1]\) using

\[_{-1}^{1}f(x)dx_{i=1}^{n}w_{i}f(x_{i}),\] (9)

where \(x_{i},i=1,2,,n\) corresponds to a root of the orthogonal polynomials on \([-1,1]\). This quadrature is called Gauss-Legendre quadrature since the orthogonal polynomials on \([-1,1]\) with a weight function \(g(x)=1\) are Legendre polynomials. An \(n\)-th degree Legendre polynomial takes the form 

\[P_{n}(x)=n!}}{dx^{n}}(x^{2}-1)^{n},\] (10)

and \(w_{i}\) is computed using Eq. 11 as shown in .

\[w_{i}=^{2})[P_{n}^{}(x_{i})]^{2}}.\] (11)

Gauss-Laguerre quadratureis an extension of Gauss quadrature for approximating integrals following the form of

\[_{0}^{}e^{-x}f(x)dx_{i=1}^{n}w_{i}f(x_{i}).\] (12)

In this case, the weight function is \(g(x)=e^{-x}\), the integral interval is \([0,)\). \(x_{i}\) corresponds to the root of Laguerre polynomials

\[L_{n}(x)=(-1)^{n}x^{n},\] (13)

a class of polynomials that are orthogonal over the interval \([0,)\) with respect to the weight function \(g(x)=e^{-x}\). The weight for computing the quadrature is computed as

\[w_{i}=}{(n+1)^{2}[L_{n+1}(x_{i})]^{2}}.\] (14)

While the computation for \(x_{i}\) and \(w_{i}\) is complicated, in practice we can use a look up table to store corresponding \(x_{i}\) and \(w_{i}\) for a given \(n\).

## 4 GL-NeRF

We developed our algorithm based on a simple observation of the integral for volume rendering. Eq. 1 is an approximation to the integral

\[C()=_{t_{n}}^{t_{f}}T(t)((t))((t),d)dt,\] (15)

where

\[T(t)=exp(-_{t_{n}}^{t}((s))ds).\] (16)

### Volume rendering and Gauss-Laguerre quadrature

Figure 2: Verification on using the Gauss-Laguerre quadrature for volume rendering. We plot the red channel of the color function w.r.t. the ray it corresponds to. The color function remains zero in most of the interval (bottom). We use a 7th-degree polynomial to approximate the non-zero region (top). As can be seen, the color function itself is similar to a polynomial, validating the use of our approach.

Let

\[x(t)=_{t_{n}}^{t}((s))ds,\] (17)

we have

\[=((t)).\] (18)

Since \(((t)) 0\), \(x(t)\) is a monotonically non-decreasing function of \(t\), therefore, \(x\) has a unique correspondence with \(t\) on increasing intervals. With this observation, we can do a change of variables for Eq. 15 to get

\[ C()&=_{t_{n}}^{t_{f}}T(t) ((t))((t),d)dt\\ &=_{t_{n}}^{t_{f}}e^{-x}((t),d)dt\\ &=_{x(t_{n})}^{x(t_{f})}e^{-x}((x),d)dx.\] (19)

As can be seen from Eq. 19, the integral for volume rendering is a weighted integral of \(((x),d)\) with the weight function to be \(g(x)=e^{-x}\). We can extend the integral interval from \([x(t_{n}),x(t_{f})]\) to \([0,)\) since the integral between \([0,x(t_{n}))\) and \((x(t_{f}),)\) are zero. Thus, we have

\[C()=_{0}^{}e^{-x}((t(x)),d)dx,\] (20)

a pure exponentially weighted integral with respect to the color function, which is of the exact same form as required by the Gauss-Laguerre quadrature.

#### 4.1.1 Gauss-Laguerre quadrature for volume rendering

As discussed in Sec. 3, the Gauss-Laguerre quadrature guarantees the highest algebraic precision when computing integral over polynomials. To perform the Gauss-Laguerre quadrature for volume rendering integral calculation, a natural question arises: **is the color function a polynomial, or can it be approximated by a polynomial with a satisfactory error rate?**

To answer this question, we first analytically give out a fundamental theorem, and then empirically approximate the color function with polynomials.

**Theorem 4.1** (Stone-Weierstrass theorem).: _Suppose \(f\) is a continuous real-valued function defined on the real interval \([a,b]\). For every \(>0\), there exists a polynomial \(p\) such that for all \(x\) in \([a,b]\), we have \(|f(x)-p(x)|<\)._

Since the pixel color is contributed by points that have a density larger than a threshold (_i.e_. regions near the surface), we can overlook the points in the empty space and only analyze the remaining part of the color function. In Fig. 2 we plot a representative of how the color function looks like. As can be seen from the figure, it has a major region with values greater than zero while the others remain zero. When approximating the non-zero region with a \(7\)-th degree polynomial, we have a relative error rate lower than \(6.5\%\). While the relative error is not sufficiently low, we argue that we can increase the degree to better approximate it since it's cintinuous by nature. On the other hand, this specific landscape is fluctuated and for most of the cases, the error rate could be smaller than \(1\%\). This suggests that Theorem 4.1 holds in our case, thus the Gauss-Laguerre quadrature can be used for computing the volume rendering integral.

#### 4.1.2 Point selection in GL-NeRF

Different from NeRF's sampling strategy, the Gauss-Laguerre quadrature enables us to use a deterministic point selection strategy for the color samples. Recall Eq. 17 is the integral variable

Figure 3: **Point Selection strategy in GL-NeRF.** We choose points along the ray that satisfy the integral from zero to the point of the volume density function should be equal to the roots of Laguerre polynomials. The points selected is then used for querying the color. In the figure above is an example of choosing \(5\) points using a \(5\)-degree Laguerre polynomial. The number on the plot indicates the value of the integral from zero to the right boundary of the region.

for Eq. 20. This means if we want to use the Gauss-Laguerre quadrature to approximate Eq. 20, we have to choose points \(x_{i}\) that are the root of \(n\)th-degree Laguerre polynomials. Since every \(x_{i}\) has a corresponding \(t_{i}\) following Eq. 17, we can choose \(t_{i}\) based on given value of \(x_{i}\), as depicted in Fig. 3. Specifically, we want the integral Eq. 17 to be equal to the roots of an \(n\)th-degree Laguerre polynomial. Fig. 3 gives an example of \(n=5\). In the figure, the numbers in the five regions filled with different colors indicate the integral value of the volume density function from zero to the right boundary of the regions. A pseudocode for GL-NeRF rendering is shown in Algo. 1.

#### 4.1.3 Intuitive understanding of the points selected using the Gauss-Laguerre quadrature

Since the points near the surface contribute the most to the final color of the pixel as discussed in , the optimal point selection strategy should choose points near the surface. The volume density, on the other hand, increases remarkably near the surface and remains close to zero at other areas. Therefore, the integral value of it Eq. 17 should also increases significantly near the surface and remains almost unchanged throughout the rest of the space. Therefore, most of the points chosen using GL-NeRF should lie around the surface of the underlying scene. Consider a case when \(n=8\), we want to choose points \(t_{i},i=1,2,,8\) such that \(x(t_{i})\) in Eq. 17 should be equal to the value \(x_{i}\) given in the look-up table Tab. 1. Notice that the first few value for \(x_{i}\) (say first three) are small so that they could be reached by the integral of volume density near the surface easily. These values have relatively larger weights assigned to them. Evaluating the color of these points using a neural network and summing them up using the weights \(w_{i}\) given in Eq. 1 following Eq. 12 would contribute mostly to the pixel color. Notice that even though the last few \(x_{i}\) are quite large and may not be reached by Eq. 17 along the ray, their corresponding weights are so small that they almost couldn't affect the final result of the pixel color. Hence, the points selected

   \(x_{i}\) & \(w_{i}\) \\ 
0.17 & \(3.69 10^{-1}\) \\
0.90 & \(4.19 10^{-1}\) \\
2.25 & \(1.76 10^{-1}\) \\
4.27 & \(3.33 10^{-2}\) \\
7.05 & \(2.79 10^{-3}\) \\
10.76 & \(9.08 10^{-5}\) \\
15.74 & \(8.49 10^{-7}\) \\
22.86 & \(1.05 10^{-9}\) \\   

Table 1: Gauss-Laguerre quadrature look-up table when \(n=8\).

Figure 4: Comparison between GL-NeRF and vanilla NeRF in terms of render time and quantitative metrics. Each point on the figure represents an individual scene. We showcase that with the drop of computational cost GL-NeRF provides, the average time needed for rendering one image is 1.2 to 2 times faster than the vanilla NeRF. In the mean time, the overall performance remains almost the same despite some minor decreases.

using GL-NeRF also correspond to the points near the surface, like in previous works [20; 29; 31] that design different neural networks for estimating the surface position, but only without any additional neural networks. Therefore, thanks to the nice property of the Gauss quadrature, ideally we can select the optimal points for computing volume rendering integral if the volume density estimation is oracle.

## 5 Experiments

Datasets and evaluation metrics.We evaluate our method on the standard datasets: NeRF-Synthetic and Real Forward Facing Dataset(LLFF)  as in  with two different models, _i.e_. Vanilla NeRF , TensoRF  and InstantNGP . Since our method is training-free, we conduct render-only experiments with the vanilla volume rendering method and our method. We plot the standard render quality evaluation metrics PSNR, SSIM  and LPIPS  with respect to the average time needed for rendering one image for each scene in Vanilla NeRF. We also report the metrics with averaged color MLP calls for TensoRF and InstantNGP. For Vanilla NeRF, we use 32 points for our method while the network is trained with more than 100 points. For TensoRF and InstantNGP, the results are produced with 4 MLP calls if not otherwise mentioned. More details can be found in Sec. A.1.

### Comparison with baselines

   Dataset & Methods & Avg. MLPs\(\) & PSNR\(\) & SSIM\(\) & LPIPS\(\) \\  LLFF & TensoRF & 118.51 & 26.51 & 0.832 & 0.135 \\ ours & 4 & 25.63 & 0.797 & 0.146 \\  NeRF-Synthetic & TensoRF & 31.08 & 32.39 & 0.957 & 0.032 \\ ours & 4 & 30.99 & 0.945 & 0.048 \\   

Table 2: Quantitative comparison. We demonstrate that our method has a minimal performance drop while significantly reducing the number of color MLP calls.

Figure 5: Qualitative results on LLFF (top) and NeRF-Synthetic (bottom) datasets. We could tell from the comparisons that the drop in performances has minimal effect on the visual quality.

We showcase that our method can be used for rendering novel views based on pretrained NeRF without further training. We plotted the quantitative metrics of GL-NeRF and original NeRF for an intuitive comparison in Fig. 4. It shows that our method achieves comparable results as the original NeRF while requiring less computation, leading to 1.2 to 2 times faster rendering. We also observed a drop in memory usage due to the fewer MLP calls we have. We further implement our method with TensoRF . As can be seen from Tab. 5, our method significantly reduces the number of MLP calls needed for volume rendering while the rendering quality only drops a little. We observe that the minimal drop in the performance has little effect on the quality of the image. Some qualitative comparisons can be found in Fig. 5. Other than TensoRF, we implement our method on top of InstantNGP  to showcase the plug-and-play attribute of our method. Our method performs similarly on Blender dataset to InstantNGP as shown in Tab. 5.

### Discussion on acceleration

The reason why the speed-up in Vanilla NeRF doesn't lead to real-time performance is that it has another heavy neural network for estimating the volume density. While our method needs cheap density estimation, it can be easily achieved by recent efforts in NeRF like factorized tensors . Therefore, reducing the number of color MLP calls needed could lead to real-time performance as shown by previous work . We therefore follow MC-NeRF  and develop a real-time renderer based on WebGL

   Point number & PSNR\(\) & SSIM\(\) & LPIPS\(\) \\ 
1 & 23.49 & 0.752 & 0.166 \\
2 & 24.90 & 0.782 & 0.142 \\
3 & 25.38 & 0.791 & 0.145 \\
4 & 25.63 & 0.797 & 0.146 \\
8 & 26.10 & 0.812 & 0.142 \\  Ori & 26.51 & 0.832 & 0.135 \\   

Table 4: Ablation study on the number of points sampled. The more points we have, the better the performance will be. With 8 points, our method is comparable to the original sampling strategy in TensoRF.

Figure 6: Effect of sample number. The first five columns correspond to the number of sampled points on top. The sixth column shows the result of the original sampling strategy adopted in TensoRF (Ori). The last column is the ground truth visualization of the details in the scene. Our method could achieve comparable results using only 4-8 points while the original strategy requires more than 100 points. The blurriness in the first two columns is inherently the inaccuracy of piece-wise constant density estimation.

   Dataset & Methods & PSNR\(\) & SSIM\(\) & LPIPS\(\) \\   & Vanilla & 27.62 & 0.88 & 0.073 \\  & ours & 27.21 & 0.87 & 0.087 \\   & Vanilla & 30.63 & 0.95 & 0.037 \\  & ours & 29.18 & 0.93 & 0.056 \\   

Table 3: Quantitative comparison when training with GL-NeRF. Vanilla refers to the vanilla NeRF and its sampling strategy while ours refers to replacing the fine sample stage in vanilla NeRF with our sampling strategy, i.e. GL-NeRF. The result for Vanilla NeRF is produced by rendering using more than 100 points while GL-NeRF only uses 32 points.

and train a small variant of TensoRF with \(8\) channels for each density component and color component and \(32\) as hidden size for the color MLP. The result on Lego in the Blender dataset is shown in Tab. 5.4. GL-NeRF is able to provide almost real-time performance in WebGL with similar quality as TensoRF running on an AMD Ryzen 9 5900HS CPU thanks to the reduced number of color MLP calls.

### Ablation studies

We further study the effect of sampled points per ray. We conduct experiments using the TensoRF model on the LLFF dataset. We found that 8 points per ray already shows comparable results to the original sampling strategy that uses more than 100 points. Quantitative comparison can be found in Tab. 4. Qualitatively, in Fig. 6 we found that less number of points would lead to blurrier results. Since the points selected using GL-NeRF intuitively correspond to where the surface is, we argue that the blurriness comes from the inherent inaccuracy of piece-wise constant density estimation.

### Discussion on GL-NeRF usage for training

While we mainly showcase that GL-NeRF is a general alternative to the sampling strategy for volume rendering at test time, it is also capable of being used for training. We demonstrate this by replacing the fine sample stage in Vanilla NeRF with GL-NeRF and show the result in Tab. 5 and Tab. 9. GL-NeRF is able to produce on-par results with the vanilla sampling strategy but use a much smaller number of points, i.e. 32 for GL-NeRF and more than 100 for vanilla NeRF.

## 6 Conclusion

In this paper, we propose GL-NeRF, a novel approach for calculating the volume rendering integral. We show that with a simple change of variable, the Gauss-Laguerre quadrature can be used for computing the volume rendering integral. Thanks to the highest algebraic precision guaranteed by the Gauss-Laguerre quadrature, GL-NeRF significantly reduces the number of MLP calls needed for the volume rendering integral. We justify the use of the Gauss-Laguerre quadrature theoretically and empirically and showcase the plug-and-play attribute of GL-NeRF in two different NeRF models. Experiments show the potential of GL-NeRF being used for accelerating any existing NeRF model. We also demonstrate that GL-NeRF can be used for training vanilla NeRF, providing a potential new direction for neural rendering research.

Limitations.While GL-NeRF shows promising results in reducing the number of MLP calls needed, it still affects the rendering quality despite the theoretical guarantee of the highest precision. How to improve the performance so that it would meet the theoretical results would be interesting.

   Method & PSNR\(\) & SSIM\(\) & LPIPS\(\) & FPS\(\) \\  TensoRF & 33.28 & 0.97 & 0.016 & 5.84 \\ ours & 33.09 & 0.97 & 0.016 & 22.34 \\   

Table 6: Comparison between our method and TensoRF on Lego scene using WebGL-based renderer. The result is collected from an AMD Ryzen 9 5900HS CPU. GL-NeRF is able to provide almost real-time rendering while remaining similar quality as TensoRF.

    & Avg. & Chair & Drums & Ficus & Hotdog & Lego & Mat. & Mic & Ship \\   & InstantNGP & 32.05 & 34.13 & 25.61 & 31.91 & 36.32 & 34.72 & 29.09 & 34.92 & 29.73 \\  & Ours & 30.35 & 33.08 & 25.07 & 30.13 & 34.78 & 33.05 & 26.54 & 33.02 & 27.15 \\   

Table 5: Per-scene results on Blender dataset between InstantNGP and ours. We demonstrate that GL-NeRF is able to be plugged into ANY NeRF models.