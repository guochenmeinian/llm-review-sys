# ARC: A Generalist Graph Anomaly Detector

with In-Context Learning

 Yixin Liu\({}^{1,}\), Shiyuan Li\({}^{2,}\), Yu Zheng\({}^{3,}\), Qingfeng Chen\({}^{2,}\), Chengqi Zhang\({}^{4}\), Shirui Pan\({}^{1,}\)

\({}^{1}\)Griffith University, \({}^{2}\)Guangxi University, \({}^{3}\)La Trobe University,

\({}^{4}\)The Hong Kong Polytechnic University

yixin.liu@griffith.edu.au, shiy.li@alu.gxu.edu.cn, yu.zheng@latrobe.edu.au

qingfeng@gxu.edu.cn, chengqi.zhang@polyu.edu.hk, s.pan@griffth.edu.au

Equal Contribution.Corresponding Authors.

###### Abstract

Graph anomaly detection (GAD), which aims to identify abnormal nodes that differ from the majority within a graph, has garnered significant attention. However, current GAD methods necessitate training specific to each dataset, resulting in high training costs, substantial data requirements, and limited generalizability when being applied to new datasets and domains. To address these limitations, this paper proposes ARC, a generalist GAD approach that enables a "one-for-all" GAD model to detect anomalies across various graph datasets on-the-fly. Equipped with in-context learning, ARC can directly extract dataset-specific patterns from the target dataset using few-shot normal samples at the inference stage, without the need for retraining or fine-tuning on the target dataset. ARC comprises three components that are well-crafted for capturing universal graph anomaly patterns: 1) smoothness-based feature Alignment module that unifies the features of different datasets into a common and anomaly-sensitive space; 2) ego-neighbor Residual graph encoder that learns abnormality-related node embeddings; and 3) cross-attentive in-Context anomaly scoring module that predicts node abnormality by leveraging few-shot normal samples. Extensive experiments on multiple benchmark datasets from various domains demonstrate the superior anomaly detection performance, efficiency, and generalizability of ARC. The source code of ARC is available at https://github.com/yixinliu233/ARC.

## 1 Introduction

Graph anomaly detection (GAD) aims to distinguish abnormal nodes that show significant dissimilarity from the majority of nodes in a graph. GAD has broad applications across various real-world scenarios, such as fraud detection in financial transaction networks  and rumor detection in social networks . As a result, GAD has attracted increasing research attention in recent years . Conventional GAD methods employ shallow mechanisms to model node-level abnormality ; however, they face limitations in handling high-dimensional features and complex interdependent relations on graphs. Recently, graph neural network (GNN)-based approaches have emerged as the go-to solution for the GAD problem due to their superior performance . Some GNN-based GAD approaches regard GAD as a supervised binary classification problem and use specifically designed GNN architectures to capture anomaly patterns . Another line of approaches targets the more challenging unsupervised paradigm, employing various unsupervised learning objectives and frameworks to identify anomalies without relying on labels .

Despite their remarkable detection performance, the existing GAD approaches follow a "**one model for one dataset**" learning paradigm (as shown in Fig. 1 (a) and (b)), necessitating dataset-specific training and ample training data to construct a detection model for each dataset. This learning paradigm inherently comes with the following limitations: _Expensive training cost_. For each dataset, we need to train a specialized GAD model from scratch, which incurs significant costs for model training, especially when dealing with large-scale graphs. _Data requirements_. Training a reliable GAD model typically needs sufficient in-domain data, sometimes requiring labels as well. The data requirements pose a challenge when applying GAD to scenarios with sparse data, data privacy concerns, or high label annotation costs. _Poor generalizability_. On a new-coming dataset, existing GAD methods require hyperparameter tuning or even model architecture modifications to achieve optimal performance, which increases the cost of applying them to new data and domains.

Given the above limitations, a natural question arises: _Can we train a "**one-for-all**" GAD model that can generalize to detect anomalies across various graph datasets from different application domains, without any training on the target data?_ Following the trend of artificial general intelligence and foundation models, a new paradigm termed "**generalist anomaly detection**", originating from image anomaly detection, is a potential answer to this question . As shown in Fig. 1 (c), in the generalist paradigm, we only need to train the GAD model once; afterward, the well-trained generalist GAD model can directly identify anomalies on diverse datasets, without any re-training or fine-tuning. Considering the diversity of graph data across different domains and datasets, the labels of _few-shot normal nodes_ are required during the inference stage to enable the model to grasp the fundamental characteristics of the target dataset. Compared to conventional paradigms, the generalist paradigm eliminates the need for dataset-specific training, resulting in fewer computations, lower data costs, and stronger generalizability when applying GAD models to new datasets.

Nevertheless, due to the unique characteristics of graph data and GAD problem, it is non-trivial to design a generalist GAD approach. The challenge is three-fold: _C1 - Feature alignment_. Unlike image data, which are typically represented in a consistent RGB feature space, the feature dimensionality and semantic space can vary significantly across different graph datasets. Substituting features with unified representations generated by large language models may be a potential solution ; however, this approach is limited to specific feature semantics and cannot address more general cases . _C2 - Representation encoding_. As the core of a generalist GAD model, a GNN-based encoder is expected to learn dataset-agnostic and abnormality-aware node embeddings for anomaly detection. However, in the absence of universal pre-trained foundation models  for graph data, crafting a potent encoder for a generalist GAD model presents a challenge. _C3 - Few-shot sample-guided prediction_. Existing GAD methods typically focus on single dataset settings, where dataset-specific knowledge is embedded in the model through training, enabling it to predict abnormality for each node independently. In contrast, a generalist GAD model should derive such knowledge from a small number of normal nodes. In this case, how to effectively utilize the few-shot normal samples during inference remains an open question.

To tackle these challenges, we introduce ARC, a generalist GAD approach based on in-context learning. ARC comprises three meticulously designed modules, each targeting a specific challenge. To address _C1_, we introduce a smoothness-based feature Alignment module, which not only standardizes features across diverse datasets to a common dimensionality but also arranges them in an anomaly-sensitive order. To deal with _C2_, we design an ego-neighbor Residual graph encoder. Equipped with a multi-hop residual-based aggregation scheme, the graph encoder learns attributes that indicate high-order affinity and heterophily, capturing informative and abnormality-aware embeddings across different datasets. Last but not least, to solve _C3_, we propose a cross-attentive in-CContext anomaly scoring module. Following the in-context learning schema, we treat the few-shot normal nodes as context samples and utilize a cross-attention block to reconstruct the embeddings of unlabeled

Figure 1: Sketch maps of (a) supervised, (b) unsupervised, and (c) generalist GAD paradigms.

samples based on the context samples. Then, the reconstruction distance can serve as the anomaly score for each unlabeled node. In summary, this paper makes the following contributions:

* **Problem.** We, for the first time, propose to investigate the generalist GAD problem, aiming to detect anomalies from various datasets with a single GAD model, without dataset-specific fine-tuning.
* **Methodology.** We propose a novel generalist GAD method ARC, which can detect anomalies in new graph datasets on-the-fly via in-context learning based on few-shot normal samples.
* **Experiments.** We conduct extensive experiments to validate the anomaly detection capability, generalizability, and efficiency of ARC across multiple benchmark datasets from various domains.

## 2 Related Work

In this section, we offer a brief review of pertinent related works, with a more extensive literature review available in Appendix A.

**Anomaly Detection.** Anomaly detection (AD) aims to identify anomalous samples that deviate from the majority of samples . Mainstream AD methods focus on unsupervised settings and employ various unsupervised techniques to build the models [22; 23; 24; 25; 26; 27; 28; 29]. To enhance the generalizability of AD methods across diverse datasets, RegAD  considers few-shot setting and trains a single generalizable model capable of being applied to new in-domain data without re-training or fine-tuning. WinCLIP  utilizes visual-language models (VLMs, e.g., CLIP ) with well-crafted text prompts to perform zero/few-shot AD for image data. InCTRL , as the first generalist AD approach, integrates in-context learning and VLMs to achieve domain-agnostic image AD with a single model. However, due to their heavy reliance on pre-trained vision encoders/VLMs and image-specific designs, these approaches excel in AD for image data but face challenges when applied to graph data.

**Graph Anomaly Detection (GAD).** In this paper, we focus on the node-level AD on graphs and refer to it as "graph anomaly detection (GAD)" following [6; 33; 34]. While shallow methods [9; 10; 11] show limitations in handling complex real-world graphs , the advanced approaches are mainly based on GNNs . The GNN-based approaches can be divided into supervised and unsupervised approaches [3; 5; 7]. Supervised GAD approaches assume that the labels of both normal and anomalous nodes are available for model training . Hence, related studies mainly introduce GAD methods in a binary classification paradigm [6; 12; 13; 14; 36; 37]. In contrast, unsupervised GAD approaches do not require any labels for model training. They employ several unsupervised learning techniques to learn anomaly patterns on graph data, including data reconstruction [4; 34; 38], contrastive learning [15; 39; 40], and other auxiliary objectives [16; 17; 41; 42]. Nevertheless, all the above methods adhere to the conventional paradigm of "one model for one dataset". Although some GAD approaches [43; 44] can handle cross-domain scenarios, their requirement for high correlation (e.g., aligned node features) between source and target datasets limits their generalizability. Differing from existing methods, our proposed ARC is a "one-for-all" GAD model capable of identifying anomalies across target datasets from diverse domains, without the need for re-training or fine-tuning.

**In-Context Learning (ICL).** ICL enables a well-trained model to be effectively (fine-tuning-free) adapted to new domains, datasets, and tasks based on minimal in-context examples (a.k.a. context samples), providing powerful generalization capability of large language models (LLMs) [45; 46; 47] and computer vision (CV) models [18; 48; 49; 50]. Two recent approaches, PRODIGY  and UniLP  attempt to use ICL for GNNs to solve the node classification and link prediction tasks, respectively. However, how to use ICL to deal with the generalist GAD problem where only normal context samples are available still remains open.

## 3 Problem Statement

**Notations.** Let \(=(,,)\) be an attributed graph with \(n\) nodes and \(m\) edges, where \(=\{v_{1},,v_{n}\}\) and \(\) are the set of nodes and edges, respectively. The node-level attributes are included by feature matrix \(^{n d}\), where each row \(_{i}\) indicates the feature vector for node \(v_{i}\). The inter-node connectivity is represented by an adjacency matrix \(\{0,1\}^{n n}\), where the \(i,j\)-th entry \(_{ij}=1\) means \(v_{i}\) and \(v_{j}\) are connected and vice versa.

**Conventional GAD Problem.** GAD aims to differentiate abnormal nodes \(_{a}\) from normal nodes \(_{n}\) within a given graph \(=(,,)\), where \(_{a}\) and \(_{n}\) satisfy \(_{a}_{n}=\), \(_{a}_{n}=\), and \(|_{a}||_{n}|\). An anomaly label vector \(\{0,1\}^{n}\) can be used to denote the abnormality of each node, where the \(i\)-th entry \(_{i}=1\)_iff_\(v_{a}\) and \(_{i}=0\)_iff_\(v_{n}\). Formally, the goal of GAD is to learn an anomaly scoring function (i.e., GAD model) \(f:\) such that \(f(v^{})>f(v)\) for \( v^{}_{a}\) and \( v_{n}\). In the conventional GAD setting of "one model for one dataset", the GAD model \(f\) is optimized on the target graph dataset \(=(,)\) with a subset of anomaly labels (in supervised setting) or without labels (in unsupervised setting). After sufficient training, the model \(f\) can identify anomalies within the target graph \(\) during the inference phase.

**Generalist GAD Problem.** In this paper, we investigate the _generalist GAD problem_, wherein we aim to _develop a generalist GAD model capable of detecting abnormal nodes across diverse graph datasets from various application domains without any training on the specific target data_. Formally, we define the generalist GAD setting, aligning it with its counterpart in image AD as introduced by Zhu et al. . Specifically, let \(_{train}=\{^{(1)}_{train},,^{(N)}_{train}\}\) be a collection of training datasets, where each \(^{(i)}_{train}=(^{(i)}_{train},^{(i)}_{train})\) is a labeled dataset from an arbitrary domain. We aim to train a generalist GAD model \(f\) on \(_{train}\), and \(f\) is able to identify anomalies within any test graph dataset \(^{(i)}_{test}_{test}\), where \(_{test}=\{^{(1)}_{test},,^{(N^{})} _{test}\}\) is a collection of testing datasets. Note that \(_{train}_{test}=\) and the datasets in \(_{train}\) and \(_{test}\) can be drawn from different distributions and domains. Following , we adopt a "normal few-shot" setting during inference: for each \(^{(i)}_{test}\), only a handful of \(n_{k}\) normal nodes (\(n_{k} n\)) are available, and the model \(f\) is expected to predict the abnormality of the rest nodes without re-training and fine-tuning.

## 4 ARC: A generalist GAD approach

In this section, we introduce ARC, a generalist GAD approach capable of identifying anomalies across diverse graph datasets without the need for specific fine-tuning. The overall pipeline of ARC is demonstrated in Fig. 2. Firstly, to align the features of different datasets, we introduce a _smoothness-based feature alignment_ module (Sec. 4.1), which not only projects features onto a common plane but also sorts the dimensions in an anomaly-sensitive order. Next, to capture abnormality-aware node embeddings, we propose a simple yet effective GNN model termed _ego-neighbor residual graph encoder_ (Sec. 4.2), which constructs node embeddings by combining residual information between an ego node and its neighbors. Finally, to leverage knowledge from few-shot context samples for predicting node-level abnormality, we introduce a _cross-attentive in-context anomaly scoring_ module (Sec. 4.3). Using the cross-attention block, the model learns to reconstruct query node embeddings based on context node embeddings. Ultimately, the drift distance between the original and reconstructed query embeddings can quantify the abnormality of each node.

Figure 2: The overall pipeline of ARC, the proposed generalist GAD approach.

### Smoothness-Based Feature Alignment

Graph data from diverse domains typically have different features, characterized by differences in dimensionality and unique meanings for each dimension. For example, features in a citation network usually consist of textual and meta-information associated with each paper, whereas in a social network, the features may be the profile of each user. Therefore, in the first step, we need to align the features into a shared feature space. To achieve this, we introduce the feature alignment module in ARC, consisting of two phases: feature projection, which aligns dimensionality, and smoothness-based feature sorting, which reorders features according to their smoothness characteristics.

**Feature Projection.** At the first step of ARC, we employ a feature projection block to unify the feature dimensionality of multiple graph datasets . Specifically, given a feature matrix \(^{(i)}^{n^{(i)} d^{(i)}}\) of \(^{(i)}_{train}_{test}\), the feature projection is defined by a linear mapping:

\[}^{(i)}^{n^{(i)} d_{u}}=( ^{(i)})=^{(i)}^{(i)},\] (1)

where \(}^{(i)}\) is the projected feature matrix for \(^{(i)}\), \(d_{u}\) is a predefined projected dimension shared across all datasets, and \(^{(i)}^{d^{(i)} d_{u}}\) is a dataset-specific linear projection weight matrix. To maintain generality, \(^{(i)}\) can be defined using commonly used dimensionality reduction approaches such as singular value decomposition  (SVD) and principal component analysis  (PCA).

**Smoothness-Based Feature Sorting.** Although feature projection can align dimensionality, the semantic meaning of each projected feature across different datasets remains distinct. Considering the difficulty of semantic-level matching without prior knowledge and specific fine-tuning [19; 20], in this paper, we explore an alternative pathway: aligning features based on their contribution to anomaly detection tasks. Through analytical and empirical studies, we pinpoint that _the smoothness of each feature is strongly correlated with its contribution to GAD_. Building on this insight, in ARC, we propose to sort the features according to their contribution as our alignment strategy.

From the perspective of graph signal processing, Tang et al.  have demonstrated that the inverse of the low-frequency energy ratio monotonically increases with the anomaly degree. In other words, high-frequency graph signals tend to play a more significant role in detecting anomalies. Similar findings have also been observed from the standpoint of spatial GNNs [37; 55], where heterophily information has been shown to be crucial in discriminating anomalies. Motivated by these findings, can we develop a metric to gauge the contribution of each feature to GAD based on its frequency/heterophily? Considering its correlation to frequency  and heterophily [57; 58; 59; 60], in this paper, we select feature-level **smoothness** as the measure for contribution. Formally, given a graph \(=(,,)\) with a normalized feature matrix \(\), the smoothness of the \(k\)-th feature dimension is defined as:

\[s_{k}()=-|}_{(v_{i},v_{j})} (_{ik}-_{jk})^{2},\] (2)

where a lower \(s_{k}\) indicates a significant change in the \(k\)-th feature between connected nodes, implying that this feature corresponds to a high-frequency graph signal and exhibits strong heterophily.

To verify whether smoothness can indicate the contribution of features in GAD, we further conduct empirical analysis (experimental setup and more results can be found in Appendix B). Concretely, we sort the raw features of each dataset based on the smoothness \(s_{k}\) and divide them into 5 groups according to the percentile of \(s_{k}\). Then, we train different GAD models using each group of features separately, and the performance is shown in Fig. 3 and 8. On both datasets, a model-agnostic observation is that the features with lower \(s_{k}\) are more helpful in discriminating anomalies. The consistent trend demonstrates the effectiveness of \(s_{k}\) as an indicator of the role of features in GAD.

Figure 3: AUROC on data with 5 groups of features.

In light of this, given the _projected features_ of different datasets, we can align their feature spaces by rearranging the permutation of features based on the descending order of \(s_{k}\) w.r.t. each projected feature. For all datasets, the feature in the first column is the one with the lowest \(s_{k}\), which deserves more attention by ARC; conversely, features with less contribution (i.e. higher \(s_{k}\)) are placed at the end. In this way, the GNN-based model can learn to filter graph signals with different smoothness levels automatically and predict anomalies accordingly. During inference, the smoothness-related information remains transferable because we adhere to the same alignment strategy.

### Ego-Neighbor Residual Graph Encoder

Once the features are aligned, we employ a GNN-based graph encoder to learn node embeddings that capture both semantic and structural information for each node. The learned embedding can be utilized to predict the abnormality of the corresponding node with the downstream anomaly scoring module. A naive solution is directly employing commonly used GNNs, such as GCN  or GAT , as the graph encoder. However, due to their low-pass filtering characteristic, these GNNs face difficulty in capturing abnormality-related patterns that are high-frequency and heterophilic [6; 37]. Moreover, most GNNs, including those tailored for GAD, tend to prioritize capturing node-level semantic information while disregarding the affinity patterns of local subgraphs . Consequently, employing existing GNN models as the encoder may overemphasize dataset-specific semantic knowledge, but overlook the shared anomaly patterns (i.e. local node affinity) across different datasets.

To address the above issues, we design an ego-neighbor residual graph encoder for ARC. Equipped with a residual operation, the encoder can capture multi-hop affinity patterns of each node, providing valuable and comprehensive information for anomaly identification. Similar to the "propagation then transformation" GNN architecture in SGC , our graph encoder consists of three steps: multi-hop propagation, shared MLP-based transformation, and ego-neighbor residual operation. In the first two steps, we perform propagation on the aligned feature matrix \(^{}=^{}\) for \(L\) iterations, and then conduct transformation on the initial and propagated features with a shared MLP network:

\[^{[l]}=}^{[l-1]},^{[l]}= (^{[l]}),\] (3)

where \(l\{0,,L\}\), \(^{[l]}\) is the propagated feature matrix at the \(l\)-th iteration, \(^{[l]}\) is the transformed representation matrix at the \(l\)-th iteration, and \(}\) is the normalized adjacency matrix [61; 63]. Note that, unlike most GNNs that only consider the features/representations after \(L\)-iter propagation, here we incorporate both the initial features and intermediate propagated features and transform them into the same representation space. After obtaining \(^{},,^{[L]}\), we calculate the residual representations by taking the difference between \(^{[l]}\) (\(1 l L\)) and \(^{}\), and then concatenate the multi-hop residual representations to form the final embeddings:

\[^{[l]}=^{[l]}-^{},=[ ^{}||||^{[L]}],\] (4)

where \(^{[l]}\) is the residual matrix at the \(l\)-th iteration, \(^{n d_{c}}\) is the output embedding matrix, and \(||\) denotes the concatenation operator.

**Discussion.** Compared to existing GNNs, our graph encoder offers the following advantages. Firstly, with the residual operation, the proposed encoder emphasizes the difference between the ego node and its neighbors rather than ego semantic information. This approach allows for the explicit modeling of local affinity through the learned embeddings. Since local affinity is a crucial indicator of abnormality and this characteristic can be shared across diverse datasets , the learned embeddings can offer valuable discriminative insights for downstream prediction. Second, the residual operation performs as a high-pass filter on the graph data, aiding ARC in capturing more abnormality-related attributes, i.e., high-frequency signals and local heterophily. Moreover, unlike existing approaches [15; 17] that only consider \(1\)-hop affinity, our encoder also incorporates higher-order affinity through the multi-hop residual design, which enables ARC to capture more complex graph anomaly patterns. More discussion and comparison to the existing GNNs/GAD methods are conducted in Appendix C.

### Cross-Attentive In-Context Anomaly Scoring

To utilize the few-shot normal samples (denoted by **context nodes**) to predict the abnormality of the remaining nodes (denoted by **query nodes**), in ARC, we devise an in-context learning module with a cross-attention mechanism for anomaly scoring. The core idea of our in-context learning module is to reconstruct the node embedding of each query node using a cross-attention block to blend the embeddings of context nodes. Then, the drift distance between the original and reconstructed embeddings of a query node can serve as the indicator of its abnormality.

Specifically, we partition the embedding matrix \(\) into two parts by indexing the corresponding row vectors: the embeddings of context nodes \(_{k}^{n_{k} d_{e}}\) and the embeddings of query nodes \(_{q}^{n_{q} d_{e}}\). Then, a cross-attention block is utilized to reconstruct each row of \(_{q}\) through a linear combination of \(_{k}\):

\[=_{q}_{q},=_{k} _{k},}_{q}=(^{}}{}})_{k},\] (5)

where \(^{n_{q} d_{e}}\) and \(^{n_{k} d_{e}}\) are the query and key matrices respectively, \(_{q}\) and \(_{k}\) are learnable parameters, and \(}_{q}\) is the reconstructed query embedding matrix. Note that, unlike the conventional cross-attention blocks [64; 65; 66] that further introduce a value matrix \(\), our block directly multiplies the attention matrix with \(_{k}\). This design ensures that \(}_{q}\) is in the same embedding space as \(_{q}\) and \(_{k}\). Thanks to this property, given a query node \(v_{i}\), we can calculate its anomaly score \(f(v_{i})\) by computing the L2 distance between its query embedding vector \(_{q_{i}}\) and the corresponding reconstructed query embedding vector \(}_{q_{i}}\), i.e., \(f(v_{i})=d(q_{i},}q_{i})=^{d_{e}} (q_{ij}-}q_{ij})^{2}}\).

**Discussion.** The design of cross-attentive in-context anomaly scoring follows a basic assumption: normal query nodes have similar patterns to several context nodes, and hence their embeddings can be easily represented by the linear combination of context node embeddings. Consequently, given a normal node, its original and reconstructed embeddings can be close to each other in the embedding space. In contrast, abnormal nodes may display distinct patterns compared to normal ones, making it difficult to depict their corresponding abnormal query embeddings using context embeddings. As a result, their drift distance \(s_{i}\) can be significantly larger. Fig. 4 provides examples for the scenarios of (a) single-class normal and (b) multi-class normal3. In both cases, the drift distance () can be a significant indicator for distinguishing anomaly (5) from normal nodes (1\(\)4). Interestingly, if the attention matrix assigns uniform weights to all context nodes, then our scoring module becomes a one-class classification model . This property ensures the anomaly detection capability of ARC even without extensive training. A detailed discussion is conducted in Appendix D.2.

Model Training.To optimize ARC on training datasets \(_{train}\), we employ a marginal cosine similarity loss to minimize the drift distance of normal nodes while maximizing the drift distance of abnormal nodes. Specifically, given graph data with anomaly labels, we randomly select \(n_{k}\) normal nodes as context nodes and sample an equal number of normal and abnormal nodes as query nodes. Then, given a query node \(v_{i}\) with embedding \(q_{i}\), reconstructed embedding \(}q_{i}\), and anomaly label \(_{i}\), the sample-level loss function can be written by:

\[=1-(q_{i},}q_{i} ),&_{i}=0\\ (0,(q_{i},}q_{i})- ),&_{i}=1\] (6)

where \((,)\) and \((,)\) denote the cosine similarity and maximum operation, respectively, and \(\) is a margin hyperparameter. Detailed algorithmic description and complexity analysis of ARC can be found in Appendix E.

## 5 Experiments

### Experimental Setup

**Datasets.** To learn generalist GAD models, we train the baseline methods and ARC on a group of graph datasets and test on another group of datasets. For comprehensive evaluations, we consider

Figure 4: Toy examples of query embeddings (\(\)), reconstructed query embeddings (\(\)), and context embeddings (\(\)).

graph datasets spanning a variety of domains, including social networks, citation networks, and e-commerce co-review networks, each of them with either injected anomalies or real anomalies [7; 15; 17]. Inspired by , we train the models on the largest dataset of each type and conduct testing on the remaining datasets. Specifically, the training datasets \(_{train}\) comprise PubMed, Flickr, Questions, and YelpChi, while the testing datasets \(_{test}\) consist of Cora, CiteSeer, ACM, BlogCatalog, Facebook, Weibo, Reddit, and Amazon. For detailed information, please refer to Appendix F.1.

**Baselines.** We compare ARC with both supervised and unsupervised methods. Supervised methods include two conventional GNNs, i.e., GCN  and GAT , and three state-of-the-art GNNs specifically designed for GAD, i.e., BGNN , BWGNN , and GHRN . Unsupervised methods include four representative approaches with distinct designs, including the generative method DOMINANT , the contrastive method CoLA , the hop predictive method HCM-A , and the affinity-based method TAM . For detailed information, refer to Appendix F.2.

**Evaluation and Implementation.** Following [7; 17; 68], we employ AUROC and AUPRC as our evaluation metrics for GAD. We report the average AUROC/AUPRC with standard deviations across 5 trials. We train ARC on all the datasets in \(_{train}\) jointly, and evaluate the model on each dataset in \(_{test}\) in an in-context learning manner (\(n_{k}=10\) as default). For the supervised baselines, we follow the same training and testing procedure (denoted as "pre-train only"), since no labeled anomaly is available for fine-tuning. For the unsupervised baselines, we consider two settings: "pre-train only" and "pre-train & fine-tune". In the latter, we additionally conduct dataset-specific fine-tuning with a few epochs. To standardize the feature space in baseline methods, we utilize either learnable projection or random projection as an adapter between the raw feature and the model input layer. We utilize random search to determine the optimal hyperparameters for both the baselines and ARC. Since our goal is to train generalist GAD models, we do not perform dataset-specific hyperparameter search, but instead use the same set of hyperparameters for all testing datasets. More implementation details can be found in Appendix F.3.

### Experimental Results

**Performance Comparison.** Table 1 shows the comparison results of ARC with baseline methods in terms of AUROC. Results in AUPRC are provided in Appendix G.1.

   Method & Cora & CiteSeer & ACM & BlogCatalog & Facebook & Weibo & Reddit & Amazon & Rank \\    \\  GCN & \(59.64 3.0\) & \(60.27 1.81\) & \(60.49 9.65\) & \(56.19 9.39\) & \(29.51 4.66\) & \(66.41 1.69\) & \(50.43 4.11\) & \(46.63 4.77\) & \(8.9\) \\ GAT & \(50.65 6.25\) & \(51.59 4.89\) & \(48.79 7.23\) & \(50.40 2.81\) & \(58.18 2.61\) & \(53.06 4.78\) & \(51.78 4.01\) & \(50.52 17.22\) & \(10.0\) \\ BGNN & \(42.45 11.57\) & \(42.31 1.92\) & \(440.01 15.69\) & \(67.47 8.52\) & \(57.44 7.29\) & \(32.75 3.55\) & \(50.72 18.44\) & \(52.26 33.11\) & \(11.1\) \\ BWGNN & \(54.06 8.27\) & \(52.61 6.79\) & \(67.59 7.00\) & \(56.34 1.21\) & \(45.84 9.47\) & \(53.38 16.4\) & \(48.97 7.54\) & \(52.56 19.95\) & \(9.0\) \\ GHRN & \(59.89 6.37\) & \(56.04 19.59\) & \(55.65 43.7\) & \(57.64 48.41\) & \(48.18 40.60\) & \(51.87 14.18\) & \(46.22 23.33\) & \(49.48 17.18\) & \(9.8\) \\   \\  DOMINANT & \(66.53 1.15\) & \(69.47 2.02\) & \(70.08 2.34\) & \(73.20 2.50\) & \(51.01 70.8\) & \(92.88 0.32\) & \(50.05 9.24\) & \(48.94 26.99\) & \(5.8\) \\ CoLA & \(63.29 8.58\) & \(62.84 9.32\) & \(66.85 43.4\) & \(50.04 23.25\) & \(12.99 10.69\) & \(16.27 54.54\) & \(52.81 66.99\) & \(47.40 57.97\) & \(9.5\) \\ HCM-A & \(54.28 8.73\) & \(43.12 8.04\) & \(53.70 46.4\) & \(55.31 37.54\) & \(55.44 13.97\) & \(65pre-training the dataset-specific GAD methods typically results in poor generalization capability to new datasets. Specifically, the AUROC of the majority of "pre-train only" approaches is close to random guessing (50%) or even lower. With dataset-specific fine-tuning, the baseline methods achieve better performance in the majority of cases. However, the improvement can be minor or even negative in some cases, demonstrating the limitations of fine-tuning. Additionally, we observe that their performance is sometimes lower than the reported results from training models from scratch , indicating potential risk of negative transfer within the "pre-train & fine-tune" paradigm. Unsupervised baselines (except HCM-A) generally outperform the supervised ones, highlighting the difficulty of training a generalist GAD model using the binary classification paradigm.

**Effectiveness of #Context Nodes.** To investigate how the number of context nodes \(n_{k}\) affects the performance of ARC during inference, we vary \(n_{k}\) within the range of 2 to 100. The results are shown in Fig. 5 (more results are in Appendix G.2). From the figure, we observe that the performance of ARC increases as more context nodes are involved, demonstrating its capability to leverage these labeled normal nodes with in-context learning. Furthermore, we can conclude that ARC is also label-efficient: when \(n_{k} 10\), the performance gain from using more context nodes becomes minor; moreover, even when \(n_{k}\) is extremely small, ARC can still perform well on the majority of datasets.

**Ablation Study.** To verify the effectiveness of each component of ARC, we make corresponding modifications to ARC and designed three variants: 1) **w/o A**: using random projection to replace smoothness-based feature alignment; 2) **w/o R**: using GCN to replace ego-neighbor residual graph encoder; and 3) **w/o C**: using binary classification-based predictor and loss to replace cross-attentive in-context anomaly scoring. The results are demonstrated in Table 2 (full results are in Appendix G.3). From the results, we can conclude that all three components significantly contribute to the performance. Among them, the in-context anomaly scoring module has a significant impact, as the performance of **w/o C** is close to random guessing on most datasets. The residual graph encoder also has a significant impact on the final performance. Notably, Weibo dataset is an exception where the GCN encoder performs better. A possible reason is that the Weibo dataset exhibits different anomaly patterns compared to the others.

**Efficiency Analysis.** To assess the runtime efficiency of ARC, we compare the inference and fine-tuning time on the ACM dataset. As depicted in Fig. 6, ARC demonstrates comparable runtime performance with the fastest GNNs (e.g., GCN and BWGNN), and significantly outperforms the unsupervised methods in terms of efficiency. Additionally, we observe that dataset-specific fine-tuning consumes more time compared to inference.

**Visualization.** To investigate the weight allocation mechanism of the cross-attention module in ARC, we visualize the attention weights between context nodes and query nodes in Fig.7 (additional results are in Appendix G.4). From Fig. 7(a), it is evident that ARC tends to assign uniform attention weights to normal nodes, leading to reconstructed embeddings that closely resemble the average embedding of the context nodes. Conversely, anomalies are reconstructed using a combination of 1 or 2 context nodes, suggesting that their embeddings are farther from the center. This allocation aligns with the case of "single-class normal" in Fig. 4(a). Differently, in Fig. 7(b), we observe that each normal query node is assigned to several context nodes following two fixed patterns, corresponding to the case of "multi-class normal" in Fig. 4(b). In summary, the cross-attention module enables ARC to adapt to various normal/anomaly distribution patterns, enhancing its generalizability.

   Variant & Cora & Cote. & ACM & Blog. & Face. & Wei. & Red. & Ann. \\  ARC & \(\$74.5\) & \(\$90.95\) & \(\$79.88\) & \(\$74.76\) & \(\$57.56\) & \(\$88.5\) & \(\$60.04\) & \(\$80.67\) \\  w/o A & \(\$80.65\) & \(\$33.85\) & \(\$79.29\) & \(\$73.86\) & \(\$26.80\) & \(\$96.69\) & \(\$4.60\) & \(\$64.76\) \\ w/o R & \(\$74.41\) & \(\$51.62\) & \(\$61.83\) & \(\$49.30\) & \(\$20.38\) & \(\$97.72\) & \(\$52.94\) & \(\$50.15\) \\ w/o C & \(\$47.39\) & \(\$53.98\) & \(\$54.24\) & \(\$60.46\) & \(\$48.86\) & \(\$42.84\) & \(\$51.03\) & \(\$69.02\) \\   

Table 2: AUROC of ARC and its variants,

Figure 6: Time comparison.

Figure 7: Visualization results.

Conclusion

In this paper, we take the first step towards addressing the generalist GAD problem, aiming to detect anomalies across diverse graph datasets with a "one-for-all" GAD model, without requiring dataset-specific fine-tuning. We introduce ARC, a novel and well-crafted in-context learning-based generalist GAD approach, capable of identifying anomalies on-the-fly using only few-shot normal nodes. Extensive experiments on real-world datasets from various domains demonstrate the detection prowess, generalizability, and efficiency of ARC compared to existing approaches. One limitation is that ARC can only use normal context samples during inference but cannot directly utilize abnormal context samples, even when they are available. A potential future direction could involve developing generalist GAD methods that utilize context samples containing both anomalies and normal instances.