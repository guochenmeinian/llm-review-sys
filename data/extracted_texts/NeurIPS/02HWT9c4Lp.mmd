# Voxel Proposal Network via Multi-Frame Knowledge Distillation for Semantic Scene Completion

Lubo Wang\({}^{1}\)  Di Lin\({}^{1}\)  Kairui Yang\({}^{1}\)  Ruonan Liu\({}^{2}\)  Qing Guo\({}^{3}\)  Wuyuan Xie\({}^{4}\)

Miaohui Wang\({}^{4}\)  Lingyu Liang\({}^{5}\)  Yi Wang\({}^{4}\)  Ping Li\({}^{7}\)

\({}^{1}\)College of Intelligence and Computing, Tianjin University \({}^{2}\)Shanghai Jiao Tong University

\({}^{3}\)IHPC and CFAR, Agency for Science, Technology and Research, Singapore

\({}^{4}\)Shenzhen University \({}^{5}\)Pazhou Lab \({}^{6}\)South China University of Technology

\({}^{7}\)The Hong Kong Polytechnic University

wanglubo@tju.edu.cn di.lin@tju.edu.cn

###### Abstract

Semantic scene completion is a difficult task that involves completing the geometry and semantics of a scene from point clouds in a large-scale environment. Many current methods use 3D/2D convolutions or attention mechanisms, but these have limitations in directly constructing geometry and accurately propagating features from related voxels, the completion likely fails while propagating features in a single pass without considering multiple potential pathways. And they are generally only suitable for static scenes and struggle to handle dynamic aspects. This paper introduces Voxel Proposal Network (VPNet) that completes scenes from 3D and Bird's-Eye-View (BEV) perspectives. It includes Confident Voxel Proposal based on voxel-wise coordinates to propose confident voxels with high reliability for completion. This method reconstructs the scene geometry and implicitly models the uncertainty of voxel-wise semantic labels by presenting multiple possibilities for voxels. VPNet employs Multi-Frame Knowledge Distillation based on the point clouds of multiple adjacent frames to accurately predict the voxel-wise labels by condensing various possibilities of voxel relationships. VPNet has shown superior performance and achieved state-of-the-art results on the SemanticKITTI and SemanticPOSS datasets.

## 1 Introduction

Understanding 3D scenes based on LiDAR point clouds is essential for tasks like autonomous driving. However, due to the limitations of LiDAR sensors and the occlusion of instances by themselves or other instances in the real world, including large-scale information in the point clouds poses a significant challenge to understanding 3D scenes.

Semantic Scene Completion (SSC) aims to simultaneously infer a scene's occupancy and semantic information based on point clouds using deep learning. Several methods [1; 2; 3; 4; 5; 6; 7; 8; 9; 10; 11; 12; 13; 14; 15; 16; 17; 18] use convolutions to complete the partial scene. Some completion methods [19; 20; 21; 22] heavily rely on diverse attention mechanisms as the attention mechanism can capture spatial relationships and update the features. The diffusion model  also applies to thecompletion task. These methods have significantly improved the performance of static single-frame-based semantic scene completion. However, they still suffer from extreme geometric incompletion due to the large-scale information loss of point clouds. Moreover, these methods ignore the regional distraction and voxel semantic uncertainty that arises from the information loss and the complex relative motion of instances in dynamic point cloud sequences.

Our paper introduces a new method for completing from both Bird's Eye View (BEV) and 3D perspectives. We propose confident voxels that show possibilities for voxels and implicitly capture the uncertainty of voxel-wise labels. Our method, Voxel Proposal Network (VPNet), includes the Confident Voxel Proposal (CVP) and Multi-Frame Knowledge Distillation (MFKD). We present the overview architecture of VPNet in Figure 1. The BEV branch completes from the BEV perspective using 2D convolutions to ensure global reasonableness and comprehensiveness of completion. The 3D branch consists of segmentation and completion subnetworks, which perform completion under the guidance of rich semantic contexts and optimize local details and accuracy of completion.

In the 3D branch, CVP learns multiple arrays of offsets for occupied voxel coordinates and features to compute confident voxel coordinates and perform long-range feature propagation like  within its branches. Then, CVP uses the confident voxel coordinates to propose confident voxels and constructs confident feature maps, which suggest various possibilities of voxel semantic labels. Finally, we integrate the confident feature maps as augmented feature maps for completion using multi-branch fusion, which condenses the proposed possibilities from the inner-frame branches.

VPNet has a multi-frame network that generates enhanced feature maps for multiple frames using CVPs. It condenses these feature maps into the branches of the CVP in the single-frame network, enabling each branch to create a similar semantic to the corresponding point cloud frame. VPNet condenses the combined enhanced feature map of multiple frames into the single-frame network, further improving the semantics. This process condenses the various possibilities in multi-frame to single-frame networks and affords the opportunity to learn to infer the lost details of each frame in contrast to other KD methods [25; 26; 27; 16].

We evaluate the effectiveness of VPNet on the SemanticKITTI  and SemanticPOSS  datasets, where we achieve state-of-the-art performances on the semantic scene completion task.

## 2 Related Work

### Semantic Scene Completion

The current approaches for SSC rely on convolution, attention, or diffusion models. For example, SSCNet  utilizes dilated convolution to enhance the feature map, while LMSCNet  applies 2D U-Net and 3D segmentation heads for multi-resolution completion. ESSCNet  employs spatial group convolution and sparse convolution to group voxels, and UDNet  incorporates UD block in 3D U-Net to efficiently fuse encoder and decoder features. Furthermore, SSA-SC  uses a semantic segmentation network to assist completion from BEV, JS3CNet  employs dense 3D and graph convolution to link point cloud and voxel features, and Symphonies  adopts deformable cross-attention to generate voxel features from the multi-scale depth and RGB images. Lastly, VPDD  utilizes the diffusion model to remove noise and complete the scene.

These methods have geometry construction and feature propagation limitations as they assume a static perspective and disregard the semantic uncertainty in dynamic sequences. To address this, we propose a method that leverages confident voxels to model the semantic uncertainty and employs multi-frame distillation to enhance the uncertainty modeling.

### Knowledge Distillation

Knowledge distillation transfers knowledge from the teacher to the student network. Smaller3D  distills at different levels. PointDistiller  proposes local distillation. PVD  adopts super voxel partition to utilize geometric information better. S2M2-SSD  distills multi-modal knowledge to network with point cloud as input. CMKD  distills from the network with the point cloud as input to the network with the monocular image as input. 2DPASS  proposes multi- to single-modal distillation that fuses point cloud and image features and distills fused features to the student network. SMF-SSD  distills multi-frame knowledge to a single-frame network at three levels. M2SKD performs distillation for difficult categories from multi- to single-frame. 3D-to-BEV  achieves distillation from 3D to BEV view. SDSeg3D  distills from data-augmented teacher to student without augmentation to improve robustness. In contrast, we adopt multi-to-single frame distillation to extract accurate semantic information.

### Point Cloud Sequence Learning

The information within adjacent point cloud frames is complementary. This understanding forms the basis of our research. M2SKD  and SMF-SSD  fuse aligned multi-frame point cloud for distillation, specially M2SKD  only fuses complex samples. TemporalLatticeNet  adopts LSTM and GRU to capture temporal relationships better. MarS3D  builds Motion-Aware Feature Learning to extract motion instance features. TemporalLidarSeg  and MemorySeg  employ a Memory mechanism to fuse the features with other frames. Meta-RangeSeg  uses Meta-Kernel  to aggregate spatial-temporal features. SpSequenceNet  proposes Cross-frame Global Attention and Local Interpolation to fuse features. P4Transformer  designs Point 4D Convolution to capture the spatial-temporal relationship. PST-Transformer  extract spatial-temporal features in a decoupled-joint manner. Moreover, SVQNet  splits historical points into voxel-adjacent neighborhoods and historical contexts to complete local and global information. While commendable, existing methods often struggle to fuse point cloud information efficiently. They cannot assign different weights to point clouds, highlighting the need for a more comprehensive solution. We construct a multi-frame network by fusing the feature maps with weighted fusion to guide the single-frame network by distillation.

## 3 Method Overview

We present VPNet and pipeline of CVP and MFKD in Figure 1 and 2. In single-frame network, given point cloud \(_{i}^{N_{i} 4}\), we process it with shared MLP and get 3D voxel feature map \(_{i}^{L W H C}\) and BEV feature map \(_{i}^{L W C}\) (see Figure 1(b)), \(N_{i}\) is the number of points, \(C\) is the channel number of \(_{i}\) and \(_{i}\). We pass \(_{i}\) through BEV completion branch and get completed BEV feature map \(_{i}^{}^{L W H C^{}}\), \(C^{}\) is the channel number of \(_{i}^{}\). Then, we pass \(_{i}\) through segmentation and completion subnetwork in the 3D completion branch. Given semantic embedded feature map \(_{i}^{L W H C^{}}\) produced by segmentation subnetwork, **Confident Voxel Proposal** (CVP) learns \(Q\) groups of offsets \(\{_{i}^{q}^{J_{i} 3} q=0,1,,Q-1\}\) for occupied voxel coordinates \(_{i}^{J_{i} 3}\) and features \(_{i}^{J_{i} C^{}}\) in \(_{i}\) to compute confident voxel coordinates and features, it builds confident feature maps \(\{_{i}^{q} q=0,1,,Q-1\}\) with confident voxels and \(_{i}\) with \(Q\) branches in CVP. And CVP produces an augmented feature map \(_{i}^{L W H C^{}}\) by

Figure 1: The architecture of VPNet. It consists of BEV and 3D completion branches. CVP in the 3D branch proposes confident voxels to present possibilities for voxels and model the semantic uncertainty of voxels implicitly. Moreover, we construct a multi-frame network and employ MFKD to enhance the accuracy of uncertainty modeling. We represent free voxels as transparent.

fusion of \(\{_{i}^{q} q=0,1,,Q-1\}\), \(J_{i}\) is the number of occupied voxels. After that, we pass \(_{i}\) through other parts of completion subnetwork as completed 3D feature map \(_{i}^{3d}^{L W H C^{}}\) (see Figure 1(b)). Finally we fuse \(_{i}^{bev}\) and \(_{i}^{3d}\) with BEV-3D Fusion to maintain final completion result \(_{i}^{L W H C^{}}\) where \(C^{}\) indicates the number of semantic categories.

In multi-frame network, given point clouds \(\{_{i},,_{k}\}\), We pass them through segmentation subnetwork and separate CVP of 3D branch and obtain augmented feature maps \(\{_{i}^{},,_{k}^{}\}\) (see Figure 2(a)). Then we fuse them and get multi-frame fused augmented feature map \(^{}^{L W H C^{}}\). We regard \(^{}\) as augmented feature map \(_{i}\) in a single-frame network, and the other modules of the multi-frame network are consistent with a single-frame network (see Figure 1(a)).

We set the branch number in the CVP of the single-frame network to be the same as the frame number in the multi-frame network. Moreover, we divide **Multi-Frame Knowledge Distillation** (MFKD) into two stages in Figure 2, we caculate the difference between \(\{_{i},,_{k}\}\) and \(\{_{i}^{0},,_{i}^{Q-1}\}\) correspondingly in stage-1 distillation to drive the branches in CVP of the single-frame network to learn the semantic feature distribution of corresponding frame and condense the various possibilities contained in the corresponding frame. We compute the difference between \(^{}\) and \(_{i}\) in stage-2 distillation to drive CVP in the single-frame network to learn multi-frame fused semantics further.

## 4 Architecture of VPNet

This section details the dual-branch VPNet with the confident voxel proposal (CVP) and the multi-frame knowledge distillation (MFKD).

### Dual-branch Completion Network

As Figure 1(b) illustrates, VPNet has a 3D completion branch and a BEV completion branch, and we utilize multiple feature fusion schemes to combine them to achieve improved completion results.

Feature InitializationGiven a point cloud \(_{i}=\{(x_{p},y_{p},z_{p},r_{p}) p=0,1,,N_{i}-1\}\). \(x_{p}\), \(y_{p}\), \(z_{p}\) are the coordinates. \(r_{p}\) is the reflectivity of point \(p\), we update \(_{i}\) as \(_{i}^{}\) during voxelization. This update is represented as \(_{i}^{}=\{(x_{p},y_{p},z_{p}, x_{p}, y_{p}, z _{p},r_{p}) p=0,1,,N_{i}-1\}\), where \( x_{p}\), \( y_{p}\), \( z_{p}\) are the differences in each dimension between the coordinates and the voxel center that point \(p\) belongs to. Then, we initialize \(_{i}\) and \(_{i}\) as follows:

\[\{_{i},_{i}\}=\{,}\}((_{i}^{})), \]

where \(\) is a shared MLP that extracts the point-wise features, \(\) and \(}\) indicate selecting the maximum from points that are in the same voxel or column.

Dual-branch CompletionThe 3D completion branch comprises a segmentation subnetwork and a completion subnetwork. The segmentation subnetwork, adapted from Cylinder3D , captures

Figure 2: The pipeline of CVP and MFKD. The semantics feature maps are produced with a segmentation subnetwork in the 3D branch.

semantic embedded feature map \(_{i}\) from \(_{i}\). These feature maps are fed into the completion subnetwork, which synthesizes the completed 3D feature map \(_{i}^{3d}\). The completion subnetwork includes the proposed CVP module and several 3D dense convolution kernels of varying sizes.

In parallel, the BEV completion branch utilizes a 2D U-Net architecture. It reconstructs the scene from BEV. The BEV feature map \(_{i}\) is processed through this branch to produce the completed BEV feature map \(_{i}^{bev}\). With sum operation, we compress features extracted by the 3D segmentation subnetwork's encoder blocks along the height axis. We integrate the compressed feature maps to corresponding levels of BEV encoder blocks. This establishes an early fusion of 3D and BEV features that enhances the global perception capabilities of the 3D branch and the spatial analysis capabilities of the BEV branch. We utilize BEV-3D Fusion to generate the final completion result \(_{i}\) as:

\[_{i}=(((_{i}^{bev})), _{i}^{3d}), \]

where \(\) is a convolution layer that increases the feature channels of \(_{i}^{lev}\), \(\) is a reshape operation that expands the height dimension from channels and \(\) is concatenation along the channel dimension. This establishes the later fusion of 3D and BEV completion branches.

### Confident Voxel Proposal

We propose confident voxels by offset learning and feature propagating from occupied voxels. We take \(q^{th}\) branch as an example to describe the details of CVP.

**Offset Learning** As illustrated in Figure 3, the segmentation subnetwork extracts a sparse semantics embedded feature map \(_{i}\), from which we initialize the occupied voxel coordinates \(_{i}=\{(x_{j},y_{j},z_{j}) j=0,1,,J_{i}-1\}\) and occupied voxel features as \(_{i}^{J_{i} C^{}}\). In Figure 3(a), we initialize random noise \(_{i}^{q}^{J_{i} C_{s}}\), \(C_{z}\) is the channel number of noise \(_{i}^{q}\) and \(_{i}^{q}(0,1)\). We computes a groups of offsets \(_{i}^{q}^{J_{i} 3}\) for each coordinate in \(_{i}\) as:

\[_{i}^{q}=}_{i}((_{i},_{i},_{i}^{q})). \]

\(}_{i}\) is the shared MLP in CVP. \(_{i}\) allows the model to consider the geometric information of the partial scene, \(_{i}\) introduces rich semantic context. The random noise \(_{i}^{q}\) drives the voxel coordinates \(_{i}\) away from the initial position and ensures the robustness of offset learning. By sampling random noise across multiple branches of the CVP module, we generate various offset sets, enabling the inference of multiple semantic possibilities for the voxels. During offset learning, we employ occupied voxel coordinates \(_{g}\) of completion ground truth as supervision.

**Voxel Proposal** As shown in Figure 3(b), we propose coordinates that contain decimals by adding offsets \(_{i}^{q}\) to initial coordinates \(_{i}\) and compute confident voxel coordinates \(_{i}^{q}\) with rounding operation on proposed coordinates. We formulate the feature propagation process as:

\[_{i}^{q}=((_{i},_{i}^{q})), \]

where \(\) is the operation of updating feature \(_{i}\) according to the offsets \(_{i}^{q}\) that makes the features that propagate farther be pruned more. It ensures the reliability of feature propagation, \(\) is feature

Figure 3: Branch \(i\) of confident voxel proposal (CVP), we divide it into two steps: (a) offset learning and (b) voxel proposal.

interpolation that computes the feature of the voxel center from proposed points in the same voxel, and \(_{i}^{q}\) is the confident voxel features after propagation. We construct confident feature map \(_{i}^{q}\) as:

\[_{i}^{q}=((_{i}^{q},_{i}^{ q})+_{i}), \]

where \(\) is confident voxels with coordinates \(_{i}^{q}\) and confident voxel features \(_{i}^{q}\). \(\) is a modified Dimension-Decomposition based Context Modeling  module that refines the features after propagation and reconstructs the semantic context.

As Figure 4 indicates, we adopt the weighted fusion strategy  and modify it to fuse the branches in CVP and compress the possibilities in branches. We compute the weights \(_{i}^{q}^{1 C^{}}\) as:

\[_{i}^{q}=_{i}^{q}(}(_{q=0}^{Q-1} _{i}^{q})). \]

\(}\) is local average pooling that compresses the local representation for each branch. We flatten the pooled feature then, and \(_{i}^{q}\) is the fully connected layer for \(q^{th}\) branch. We fuse the branches in CVP according to the weights and get an augmented feature map \(_{i}\).

### Multi-Frame Knowledge Distillation

We construct a multi-frame network that proposes confident voxels and generates an augmented feature map for each frame with CVPs. We utilize MFKD to distill the semantic knowledge of augmented feature maps into a single-frame model in two stages to condense the voxel possibilities.

Multi-Frame NetworkAs illustrated in Figure 1(a), we align the point cloud frames \(\{_{i},,_{k}\}\) to the coordinate of the current frame \(_{i}\). We input the aligned point clouds into the 3D completion branch separately. We get multi-frame augmented feature maps \(\{_{i},,_{k}\}\). We fuse them as \(^{}\) with the above-mentioned weighted fusion strategy. We regard \(^{}\) as \(\) in the single-frame network to build the multi-frame network. We input \(\{_{i},,_{k}\}\) into 3D completion branch and only input \(_{i}\) into BEV completion branch. We construct a multi-frame network that leverages multi-frame point clouds to model voxel semantic uncertainty with multiple CVPs from a 3D perspective.

Multi-frame DistillationWe obtain augmented feature maps \(\{_{i},,_{k}\}\) in multi-frame network and confident feature maps \(\{_{i}^{0},,_{i}^{Q-1}\}\) in single-frame network, we calculate the difference \(_{s1}\) between the corresponding feature maps with

\[_{s1}=_{0}^{Q-1}(}(_{i}^ {q}),}(_{i+q})) \]

where \(}\) is a local average pooling function with kernel size \(s s s\) to construct super voxels to meet the sparsity of features, and \(\) is the Kullback-Leibler divergence function. We build stage-1 distillation to drive the branches in CVP to simulate the knowledge learned by multi-frame CVPs and condense possibilities of the corresponding frame to branch in CVP.

Training LossTo improve the accuracy of semantic uncertainty modeling and reconstruct the scene details, given fused augmented feature map \(^{}\) in the multi-frame network and augmented

Figure 4: Architecture of the multi-branch fusion.

Figure 5: The overall architecture of MFKD. MFKD constructs two stages of distillation between CVPs of multi-frame networks and CVPs of the single-frame network.

feature map \(_{i}\) in the single-frame network, we calculate the difference \(_{s2}\) between them as:

\[_{s2}=(_{i},^{}). \]

We avoid utilizing a super voxel partition here to prevent the blurring of features. Thus, we build stage-2 distillation to drive CVP in the single-frame network to simulate the knowledge after multi-frame CVPs. Finally, we achieve multi-frame distillation by joint stage-1 and -2 distillation as:

\[_{kd}=_{s1}+_{s2}. \]

We formulate total loss in the single-frame and multi-frame networks as:

\[=_{com}+_{seg}+_{ geo}+_{kd}, \]

where \(k=i\). \(_{com}\) is completion loss. \(_{seg}\) is segmentation loss. \(_{geo}\) is geometry loss between proposed coordinates and coordinates ground truth. Here, we utilize Chamfer Distance . \(\), \(\), \(\) and \(\) are weights of losses. We set \(=1.00\), \(=0.10\), \(=0.01\) and \(=0.50\) during distillation.

## 5 Experiments

### Implementation Details

We implement VPNet with PyTorch3 and train it on A6000 GPUs with a mini-batch of 8 for 80 epochs; we use Adam  optimizer with an initial learning rate of 0.001. We set feature map channel number \(C^{}=32\), random noise channel number \(C_{z}=4\), CVP branch number \(Q=3\), and super voxel partition kernel size \(s=4\).

### Datasets and Metrics

We evaluate VPNet on SemanticKITTI  and SemanticPOSS  datasets, composed of real outdoor point cloud sequences. SemanticKITTI contains 22 sequences with 19 categories, 11/1/10 sequences for training/validation/online testing. SemanticPOSS contains six sequences divided into 11 categories; it contains 5/1 sequences for training/validation (testing).

According to SSCNet , we evaluate VPNet on Scene Completion (SC) with intersection-over-union (IoU), on Semantic Scene Completion (SSC) with IoU of each semantic category and mean of all semantic categories' IoU (mIoU).

### Ablation Study of VPNet

In the ablation study, we conduct experiments on SemanticKITTI  validation set.

Analysis of Network FrameworkWe evaluate the impact of the BEV completion branch, segmentation subnetwork, completion subnetwork without CVP, and completion subnetwork with CVP. As Table 1 illustrated, in the first and second rows, we use the BEV branch and 3D branch without CVP separately and get (56.4% IoU, 22.2% mIoU) and (50.3% IoU, 23.6% mIoU).

In the third row, we use the BEV branch and segmentation subnetwork. Under the augmentation of 3D semantics, we get (58.3% IoU and 24.5% mIoU) which are much higher than the performances of the BEV branch and 3D branch separately. In the fourth row, we add a completion subnetwork without CVP to the network; this method produces (59.1% IoU, 24.9% mIoU) that improves the IoU with 0.8%. This proves the effectiveness of joint completion from different perspectives. The 3D branch riches the details (in the semantic

   &  &  &  &  &  \\  & & w/o CVP & w/ CVP & & \\  ✓ & ✗ & ✗ & ✗ & 56.4 & 22.2 \\ ✗ & ✓ & ✓ & ✗ & 50.3 & 23.6 \\ ✓ & ✓ & ✗ & ✗ & 58.3 & 24.5 \\ ✓ & ✓ & ✓ & ✗ & 59.1 & 24.9 \\ ✓ & ✓ & ✗ & ✓ & **59.3** & **25.6** \\  

Table 1: Impact of dual-branch network components. "seg." means 3D segmentation subnetwork and "com." means 3D completion subnetwork.

aspect), while the BEV branch completes the scene coarsely and with higher completeness (from the geometric aspect).

We integrate CVP into the dual-branch network and achieve (59.3% IoU, 25.6% mIoU), a significant improvement of 0.7% mIoU compared to the network without CVP. This incremental improvement underscores the value of modeling the uncertainty of voxel semantics under the guidance of geometry for completion, marking a step forward in our understanding and application of these techniques.

Internal Study of CVPWe analyze the components of CVP in Table 2. In Table 2(a), we assemble CVP with random noise \(_{i}^{q}\) with different channel numbers. We get the best performance with \(C_{z}=4\). When less than 4, the learning of offsets is insufficient, and the semantic possibility learned from the voxel proposal is lacking. When more significant than 4, the random noise introduces too much meaningless information that adversely impacts the network's performance.

We propose confident voxels with multiple branches, so we analyze the impact of branch number \(Q\) in Table 2(b). The network performance improves when the branch number is increased, and we get the best completion performance when we set the branch number to \(Q=3\). However, when we set \(Q=4\), we get similar performance (59.2% IoU, 25.6% mIoU) with \(Q=3\) as too many branches bring distractions to the network so we set \(Q=3\) during training.

The fusion strategy of multiple branches influences the performance of voxel semantic uncertainty modeling; we construct CVP with different fusion strategies and show the results in Table 2(c). Here, we set branch number \(Q\) to 3. We compare the weighted fusion scheme with addition, concatenation, and average. These standard methods of feature fusion are weak in uncertainty modeling. Among the compared strategies, addition gets the best performance (59.3% IoU, 25.4% mIoU), but the weighted fusion we utilize still outperforms it with 0.2% mIoU. This demonstrates that weighted fusion models the voxel semantic uncertainty more accurately.

Internal Study of MFKDAs we build CVP with \(Q=3\), we implement a multi-frame network with three frames to build the distillation relationships between frame and branch in CVP correspondingly, and we present the results in Table 3. We get the best performance (61.1% IoU, 26.8% mIoU) with frames _t_/_t_+2/_t_+4, where \(\) is the frame we use to train the single-frame. We get (60.2% IoU, 26.3% mIoU) with frames _t_/_t_+1/_t_+2 as the adjacent frames contain insufficient supplementary information, and frames with larger intervals like t/t+3/t+6 have less guidance for modeling the uncertainty of semantics.

Table 2: Internal studies on random noise (a), branch number (b) and fusion strategy (c) of CVP.

Table 3: Frames in multi-frame network.

Table 4: Internal studies on stages of MFKD (a) and comparison with other distillation methods (b). (a) Stages of MFKD. ”voxel” means common voxel partition, ”super-” (b) Comparison with other distillation methods.

We conduct experiments using different distillation stages in Table 4(a). With stage 1 without super voxel partition, we get similar results (59.5% IoU, 25.5% mIoU) with the single-frame network, as ordinary distillation distracts the offset learning due to the neglect of sparsity. We add the super voxel partition to stage-1 distillation, and this method produces (59.3% IoU, 25.9% mIoU). We also build MFKD with stage-2 distillation only and get (59.6% IoU, 25.8% mIoU) that proves voxel-wise guidance like stage-2 is helpful to the semantic uncertainty modeling. We get better performance (59.6% IoU, 26.1% mIoU) with stage-1 with super voxel partition and stage-2 distillations that provide more accurate guidance. And we compare MFKD with other distillation methods in Table 4(b) where MFKD performs better than others and this proves the effectiveness of MFKD.

### State-of-the-art Comparison

We compare our method with state-of-the-art methods on SemanticKITTI online testing set in Table 5. We present the visualization comparison in Figure 6. Our method outperforms other methods and demonstrates competitive performance. VPNet produces (60.4% IoU, 25.0% mIoU) that with 1.6% IoU and 1.5% mIoU improvement than SSA-SC  when training with single-frame without MFKD. It achieves (60.7% IoU, 25.6% mIoU) with 0.3% IoU and 0.6% mIoU improvement with MFKD.

Figure 6: Completion results of different methods on SemanticKITTI validation set.

[MISSING_PAGE_FAIL:10]