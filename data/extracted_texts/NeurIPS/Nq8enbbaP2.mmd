# Occupancy-based Policy Gradient: Estimation, Convergence, and Optimality

Audrey Huang

Department of Computer Science

University of Illinois Urbana-Champaign

Champaign, IL 61820

audreyh5@illinois.edu

&Nan Jiang

Department of Computer Science

University of Illinois Urbana-Champaign

Champaign, IL 61820

nanjiang@illinois.edu

###### Abstract

Occupancy functions play an instrumental role in reinforcement learning (RL) for guiding exploration, handling distribution shift, and optimizing general objectives beyond the expected return. Yet, computationally efficient policy optimization methods that use (only) occupancy functions are virtually non-existent. In this paper, we establish the theoretical foundations of model-free policy gradient (PG) methods that compute the gradient through the occupancy for both online and offline RL, without modeling value functions. Our algorithms reduce gradient estimation to squared-loss regression and are computationally oracle-efficient. We characterize the sample complexities of both local and global convergence, accounting for both finite-sample estimation error and the roles of exploration (online) and data coverage (offline). Occupancy-based PG naturally handles arbitrary offline data distributions, and, with one-line algorithmic changes, can be adapted to optimize any differentiable objective functional.

## 1 Introduction

Value-based methods have been the dominant paradigm in model-free reinforcement learning, with a solid theoretical foundation in large state spaces under function approximation . In contrast, a model-free RL paradigm based on their natural counterparts--the _occupancy functions_--remains largely under-investigated. Occupancy functions are densities that describe a policy's state visitation, and play instrumental roles in guiding exploration , handling distribution shift , and optimizing general objectives beyond the expected return . Despite this, they are seldom modeled directly in learning algorithms and appear only in the analyses, except in conjunction with value functions in marginalized importance sampling . Recently,  developed algorithms in online and offline RL that model only occupancies via density function classes, spotlighting their roles in handling non-exploratory offline data and in online exploration. However, their focus was on statistical guarantees, and computationally efficient policy optimization for occupancy-based methods remained an open problem.

In answer, we develop model-free policy gradient (PG) algorithms that compute the gradient through occupancy functions, without estimating any values. By leveraging a Bellman-like recursion, we reduce occupancy-based gradient estimation to solving a series of squared-loss minimization problems, which can be done in a computationally oracle-efficient manner. Our analysis captures the effects of gradient estimation error, exploration (in online PG, which is characterized by the initial state distribution), and offline data quality (in offline PG) on the sample and iteration complexity required for local and global convergence. In the online setting, our results complement previous works on the optimality of value-based PG  and extend past their scope to include general objectives of occupancy functions, such as entropy maximization for pure exploration and risk-sensitive functionals in safe RL . These objectives generally cannot be optimized using value-based policy gradients because they do not admit value functions or Bellman-like equations with which to estimate them .

In the offline setting, we handle gradient estimation from fixed datasets of poor coverage, which departs from most existing (value-based) off-policy PG estimators that assume an exploratory dataset . Learning with non-exploratory data is a core consideration in recent offline RL , and gives rise to unique challenges in our setting: occupancies are converted into density ratios for learning purposes, but these ratios become unbounded when the data lacks coverage.  used clipping to handle occupancy estimation under poor coverage, which we show is insufficient for gradient estimation (Prop. 4.2). Instead, a novel smooth-clipping mechanism (Sec. 4.2) is developed to provide statistically robust gradient estimates.

App. A includes a full discussion of related work, and our contributions are organized as follows:

1. **Online PG** (Sec. 3) We propose OccuPG, an occupancy-based PG algorithm that reduces gradient estimation to squared-loss minimization, based on a recursive Bellman flow-like update for the occupancy gradient. We analyze the sample complexities for both local and global convergence, and, notably, our algorithm and analyses extend straightforwardly to the optimization of general objective functionals.
2. **Offline PG** (Sec. 4) For offline RL we develop and analyze Off-OccuPG, which optimizes only the portions of a policy's return that are adequately covered by offline data. Conceptually, our algorithm is based on combining the methods in Sec. 3 with (a smoothed version of) the recursively clipped occupancies from . As a result, our estimation and convergence guarantees do not require assumptions on data coverage, which relaxes the restrictions of previous works.

## 2 Preliminaries

**Finite-horizon Markov decision process (MDP).** Finite-horizon MDPs are defined by the tuple \(=(,,P,R,H,d_{0})\), where \(\) is the state space, \(\) is the action space, and \(H\) is the horizon. We use \([H]=\{0,,H\}\) and when clear from the context, use \(\{_{h}\}=\{_{h}\}_{h[H]}\). For notational compactness we assume that \(=_{h}^{h}\) is the union of \(H\) disjoint sets \(\{^{h}\}\), each of which is the set of states reachable at timestep \(h\). This is WLOG as we can always augment the state space with \([H]\) at the cost of only \(H\) factors .

Since each state can only be visited at a single timestep, we can now define the (non-stationary) transitions as \(P:()\), and the initial state distribution as \(d_{0}(^{0})\). We assume the reward function \(R:\) is bounded on the unit interval and (for simplicity) state-wise deterministic. This sufficiently captures the challenges of our setting since the occupancies are densities over states, and it will be easily seen later that our results generalize to per-state-action rewards. A policy \(:()\) interacting with \(\) observes trajectories \(\{(s_{h},a_{h},s_{h+1},r_{h+1})\}_{h=0}^{H-1}\), and has expected return \(J()=_{}[_{h=1}^{H}R(s_{h})]\). At any \((h,s,a)\), its expected return-to-go is encoded in the value function \(Q_{h}^{}(s,a)=_{}[_{h^{}>h}R(s_{h^{}})|s_{h}=s,a_{h}=a]\).

For each \(h[H]\), a policy's occupancy function \(d_{h}^{}()\) is a p.d.f. describing its state visitation, \(d_{h}^{}(s)=_{}(s_{h}=s)\). In combination with the policy, the MDP dynamics dictate the evolution of the occupancy over timesteps. This is encoded in the recursive Bellman flow equation, which mandates that \(d_{h}^{}=^{}d_{h-1}^{}\) for all \(h[H]\). Here, \(^{}\) is the Bellman flow operator with \((^{}f)(s^{}):=_{s,a}P(s^{}|s,a)(a|s)f(s) ^{}\), for any function \(f:^{}\).

**Policy optimization.** For an objective function \(f:_{}\), the general goal of this work is to find \(*{argmax}_{_{}_{}}f(_{})\) over a policy class \(_{}=\{_{}:,^{},\|\| B\}\), parameterized by a convex and closed parameter class \(\) with dimension \(\). One example of \(f\) is the expected return \(J(_{})\). Projected gradient ascent (PGA) will be our base algorithm for policy optimization. For a fixed learning rate \(\) and iterations \(t[T]\), it iteratively updates \(^{(t+1)}=*{Proj}_{}(^{(t)}+ f( _{^{(t)}}))\). Here, \( f(_{})=[)}{ r}]_{p []}^{}\) is the gradient with respect to \(\), where superscript \(p\) indexes the \(p\)-th entry of a vector. We will assume that the gradient of the policy's log-probability is bounded, as is ubiquitous in the PG literature .

**Assumption 2.1**.: For all \(_{}_{}\), \(_{s,a}\|_{}(a|s)\|_{} G\).

We will later analyze the convergence rate of our algorithms to stationary points with (approximately) zero gradient, and refer to \(^{(t)}=_{^{(t)}}\) for short. For PGA, stationarity will be measured using the standard gradient mapping \(\|G^{}(^{(t)})\|\) with \(G^{}(^{(t)}):=(^{(t)}-^{(t+1)})\), the parameter change between iterations . Note that if \(=^{}\) and no projection is required, then \(\|G^{}(^{(t)})\|=\| f(^{(t)})\|\) reduces to the gradient magnitude.

Computational oracles.As is common in the literature, we analyze computational efficiency in terms of the number of calls to the following oracles, which serve as computational abstractions. We desire a polynomial number of such calls in terms of problem-relevant parameters. Given an i.i.d. dataset \(=\{(x,y)\}\) and function class \(\), the maximum likelihood estimation oracle outputs \(*{argmax}_{f}_{}[ f(x)]\). The squared-loss regression oracle finds \(*{argmin}_{f}_{}[(f(x)-y)^{2}]\). Both can be approximated efficiently whenever optimizing over \(\) is feasible .

## 3 Online Occupancy-based PG

We now develop our occupancy-based policy optimization algorithm for the online RL setting, where the policy can continuously interact with the environment to gather new trajectories. Our gradient estimation routine is based on a recursive Bellman flow-like equation that can be approximately solved using squared-loss regression, not unlike those used to estimate occupancy functions in FORC  or value functions in FQI . The intuitions established for our online algorithm form the foundation for our later offline methods.

### Occupancy-based Policy Gradient

The expected return of a policy \(\) can be expressed as the expectation over its occupancy of the per-state rewards, \(J()=_{h}_{s_{h}}d_{h}^{}(s_{h})R(s_{h})\). The gradient of \(J()\) then passes through \(d^{}\),

\[ J()=_{h}_{s_{h}} d_{h}^{}(s_{h})R(s_{h})=_{h} _{s_{h} d_{h}^{}}[ d_{h}^{}(s_{h})R(s_{h}) ].\]

We use the grad-log trick above to write \( J()\) as an expectation over \(d^{}\), which makes it amenable to estimation from online samples as long as we can calculate \( d_{h}^{}:^{}\). We make the key observation that \( d_{h}^{}\) can be expressed as a function of \( d_{h-1}^{}\), which involves a time-reversed conditional expectation over the previous timestep's \((s_{h-1},a_{h-1})\) given the current \(s_{h}\).

**Lemma 3.1**.: _For any \(\) and \(h[H]\), \( d_{h}^{}\) satisfies the recursion_

\[ d_{h}^{}=_{h-1}^{}(+ d _{h-1}^{}),\] (1)

_where \([_{h-1}^{}f](s^{}):=_{}[f(s_{h-1},a_{h-1})|s_ {h}=s^{}]=_{s,a}|s,a)(a|s)d_{h-1}^{}(s)}{d_ {h}^{}(s^{})}f(s,a)\)1, for any function \(f:^{}\). Further, under Asm. 2.1, \(_{s,h}\| d_{h}^{}(s)\|_{} hG\)._

Eq. (1) is derived by propagating the gradient through the Bellman flow equation, and we can solve it from \(h=1\) to \(H\) to compute \( d_{h}^{}\) (with \( d_{0}^{}=\) by definition). While related observations have been made throughout the rich history of PG literature , the expression in Eq. (1) is adapted to our unique pursuit of modeling \( d^{}\) with general function approximators. In particular, the conditional expectation (\(^{}\)) immediately hints that \( d^{}\) is amenable to estimation using squared-loss regression, a technique that is well-understood for value functions  and, more recently, for occupancy functions .

Formally, to solve the dynamic programming equation of Eq. (1) in a computationally efficient manner, we reduce it to minimizing a squared-loss regression problem. Consider the standard (supervised learning) regression setup. The solution \(*{argmin}_{f}_{(x,y) Q}[(f(x)-y)^{2}]\) maps \(x_{Q}[y|x]\), the conditional expectation given \(x\) of the target \(y\) under the joint \(Q\). As a result (see Eq. B.2),

\[ d_{h}^{}=*{argmin}_{g:^{ }}\ _{}g(s_{h})-((a_{h-1}|s_{h-1})+  d_{h-1}^{}(s_{h-1}))^{2}.\] (2)Here, \(g\) is a vector-valued function, and the norm \(\|\|^{2}\) is equivalent to the sum of \(\) scalar-valued squared-losses for each parameter dimension. The RHS only requires sampling \((s_{h-1},a_{h-1},s_{h})\) from online rollouts. Then, given finite samples, we can robustly estimate \( d^{}\) by minimizing an empirical version of Eq.2 using regression oracles.

### Online policy gradient algorithm and analyses

Alg.1 (OccuPG) displays our full online occupancy-based PG procedure. For each iteration \(t[T]\), we first collect two independent datasets: \(\{_{h}^{}\}\) for \( d^{^{(t)}}\) estimation, and \(\{_{h}^{}\}\) for \( J(^{(t)})\) estimation. The former occurs in Line5, where we recursively solve an empirical version of Eq.2; the latter is computed in Line7, then used to update the policy (Line8).

```
0: Samples \(n\); iterations \(T\); policy class \(_{}\); gradient function class \(\{_{h}\}\); learning rate \(\)
1:for\(t=0,,T-1\)do
2: Collect \(n\) trajectories with \(^{(t)}\). Set \(_{h}^{}=\{(s_{h},a_{h},s_{h+1})\}_{i=1}^{n}\) for all \(h\). Repeat for \(\{_{h}^{}\}\).
3: Initialize \(g_{0}=\).
4:for\(h=1,,H\)do
5: Let \(_{h-1}^{(t)}(g_{h};g_{h-1}):=_{(s,a,s^{}) _{h-1}^{}}g_{h}(s^{})- ^{(t)}(a|s)+g_{h-1}(s)^{2}\). Set \[_{h}^{(t)}=*{argmin}_{g_{h}_{h}}_{h-1}^{(t)}(g_{h};_{h-1}^{(t)}).\] (3)
6:endfor
7: Estimate \(J(^{(t)})=_{h=1}^{H}_{(s,a,s^{},r ^{})_{h-1}^{}}_{h}^{(t)}(s^{ }) r^{}\)
8: Update \(^{(t+1)}=*{Proj}_{}(^{(t)}+J(^{(t)}))\).
9:endfor ```

**Algorithm 1**OccuPG: Online Occupancy-based Policy Gradient

**Gradient estimation guarantee.** In the following, we establish that our regression-based estimation procedure produces accurate estimates of \( J()\). Our guarantee holds under the requirement that the gradient function classes \(\{_{h}\}\) can express the population gradient update (Lem.3.1) for any target function. It is analogous to the Bellman completeness assumption that is required for regression-based value or occupancy function estimation .

**Assumption 3.1** (Gradient function class completeness).: For all \(h[H]\), \(_{g_{h},s}\|g_{h}(s)\|_{} hG\). Further, for all \(_{}\), we have \(_{h-1}^{}(+g_{h-1})_{h}\), for all \(g_{h-1}_{h-1}\).

Next, since we allow \(\) to be a continuous function class, our sample complexity bound for gradient estimation is expressed in terms of its pseudodimension \(:=_{}\) (Def.1). Examples of \(\) parameterizations and their \(_{}\) are discussed in Rem.3.1 below. Finally, Thm.3.1 shows that OccuPG produces accurate gradient estimates given the following polynomial sample size.

**Theorem 3.1**.: _Fix \((0,1)\) and \(_{}\). Under Asm.2.1 and Asm.3.1, we have that w.p. \( 1-\), \(\| J()-J()\|\) when \(n=(_{}H^{}G^{2}(1/ )}{^{2}})\)._

_Remark 3.1_.: Lastly, we provide examples of \(\) for Asm.3.1 in representative MDP structures. Low-rank MDPs (Def.1) are a well-studied setting where the transition function admits a low-rank decomposition into two features of rank \(k\), i.e., there exists \(:^{k}\) and \(:^{k}\) such that \(P(s^{}|s,a)=(s,a),(s^{})\). Tabular MDPs are a special case with one-hot features. Due to the bilinear transitions, both the occupancy and its gradient are linear functions of \(\), i.e., \(d^{}=(s)^{}\) and \( d^{}(s)=(s)^{}\) for some \(^{k p},^{k}\), and all \(s\). When \(\) is known, we can set \(_{h}\) to be a linear-over-linear function class \(_{h}=\{g_{h}(s)=}{(s)^{}}: ^{k p},^{k},_{s}\|g_{h}(s)\|_{ } hG\},\) which has \(_{}=k\) (Prop. B.1).

**Stationary convergence.** Next, we analyze the convergence rate of OccuPG to a stationary policy, i.e., one that has near-zero gradient. Note that, in general, stationary policies are not necessarily optimal as the objective function is non-convex. As is standard in the literature, we will assume that the objective has a smooth gradient .

**Assumption 3.2** (\(\)-smooth objective).: For a function \(f:_{}\), there exists \(>0\) such that \(\| f(_{})- f(_{^{}})\|_{2}\|- ^{}\|_{2}\) for all \(,^{}\).

Cor. 3.1 shows that, in expectation, OccuPG with \(T=O( H/)\) iterations outputs a \(\)-stationary point, as measured by \(\|G^{}(^{(i)})\|=\|^{(t)}-^{(t-1)}\|\). The proof relies on Thm. 3.1, i.e., with enough samples the statistical noise of the gradient estimates are sufficiently small to enable convergence.

**Corollary 3.1**.: _Under creftype 2.1, creftype 3.1, and creftype 3.2, the iterates of OccuPG with \(T=O( H^{-1})\) and \(n=(_{}H^{6}G^{2}(T/)^ {-1})\) satisfy \(_{t=1}^{T}[\|G^{}(^{(t)})\|^{2}]\)._

Computational efficiency.OccuPG is not only statistically efficient but computationally oracle-efficient as well, since it reduces to a series of squared-loss minimization problems. In each iteration, it makes \(H\) calls to a regression oracle to compute the occupancy gradient (Line 5). Then to converge to a \(\)-stationary point, from Cor. 3.1 we require a total of \(O( H^{2}/)\) such calls.

Optimality.Lastly, we analyze when the policies recovered by OccuPG are also approximately optimal. The key inequality is an upper bound on the suboptimality of any policy in terms of its gradient magnitude (or stationarity), and a _coverage coefficient_\(^{^{*}}\) with respect to the optimal policy.

**Lemma 3.2**.: _For any \(\) and \(^{}\), define \(B^{}(^{}):=_{h,s,a}d_{h}^{}(s)^{}(a|s)Q_{h}^{}(s,a)\). Suppose \(_{}\),_

1. _(Policy completeness) There exists_ \(^{+}_{}\) _such that_ \(^{+}*{argmax}_{^{}}B^{}(^{})\)_._
2. _(Gradient domination)_ \(_{^{}_{}}B^{}(^{})-B^{}() m _{^{}} B^{}(),^{ }-\)__

_Given \(()\), define the coverage coefficient \(^{^{*}}:=\!_{h}d_{h}^{^{*}}/_{}\) for \(^{*}=*{argmax}_{}J()\). Then for any \(_{}_{}\),_

\[J(^{*})-J(_{}) m\;^{^{*}}_{^{} _{}} J_{}(_{}),^{}- ,\] (4)

_where \(J_{}():=_{s_{0},}[_{h}r_{h}]\) is the expected return of \(\) in \(\) with initial state distribution \(\)._

The lemma preconditions are identical to those required for value-based analysis . \(B^{}(^{})\) is a one-step improvement objective with respect to the occupancies and value functions of \(\), and we require (1) the policy class to be expressive enough that it contains any maximizer; and (2) the one-step objective to itself have optimality gap upper-bounded by the one-step policy gradient magnitude, for which the constant \(m\) is determined wholly by the policy parameterization. For example, the tabular policy \(_{}(a|s)=_{sa}\) has \(m=1\).

The coverage coefficient \(^{^{*}}\) is the finite-horizon counterpart to the infinite-horizon "exploratory initial distribution" salient to the analysis of  and  (which lists developing it as future work). In RL, a small gradient magnitude alone does not guarantee optimality, as it can also occur when the policy rarely visits rewarding states. The coverage coefficient quantifies both how policy performance can suffer from insufficient exploration, as well as how exploratory initializations mitigates this problem. Finally, combining Lem. 3.2 with the stationary convergence result in Cor. 3.1 shows that, on average, the best-iterate of OccuPG is near-optimal.

**Corollary 3.2**.: _Under the preconditions of Lem. 3.2 and Cor. 3.1, running OccuPG2 with initial distribution \(\) satisfies \([_{t}J(^{*})-J(^{(t)})]\) when \(T=((^{^{*}})^{2}m^{2}H^{2}}{ ^{2}})\) and \(n=((^{^{*}})^{2}m^{2}_{ }H^{6}G^{2}(T)}{^{2}})\)._

### Optimization of general functionals

One standout feature of OccuPG is that it can, _with a one-line change_, be adapted for policy optimization of any (differentiable) objective function involving occupancies. We work with \(J_{F}()=_{h}F_{h}(d_{h}^{})\) as a representative formula, where \(F_{h}:()\) is a general functional. Such objectives often evade value-based PG optimization because they do not admit value functions or Bellman-like recursions with which to compute them. Examples include entropy maximizationwhere \(F_{h}(d)=- d, d\); imitation learning where \(F_{h}(d)=-\|d-d_{h}^{_{E}}\|_{2}^{2}\) for an expert policy \(_{E}\); and the expected return with \(F_{h}(d)= d,R\).

The policy gradient is then \( J_{F}()=_{h}_{s d_{h}^{}}[(d)}{ d(s)}|_{d=d_{h}^{}} d_{h}^{}(s)]\). Implementation-wise, we need only change Line 7 in OccupPG to accommodate the new gradient formula, to \(J_{F}()=_{h}_{s_{h}} _{h}^{}(s).(d)}{ d(s)}|_{d =_{h}^{}}\). The partial derivative of \(F_{h}\) is evaluated with a plug-in occupancy estimate \(^{}\) that can be obtained using maximum likelihood estimation (App. D). Notably, the occupancy gradient estimation module for \(_{h}^{} d_{h}^{}\) (Line 5) is reused verbatim. Given their resemblance to those in Sec. 3.2, the full algorithm and analyses are deferred to App. B.5.

## 4 Offline Occupancy-based PG

In this section, we develop an algorithm for occupancy-based policy optimization in the offline setting, where only fixed datasets are available for learning. A direct modification of OccupPG, e.g., by converting occupancies to density ratios over the offline data distribution, will fail unless the data covers _all possible policies_, otherwise the density ratio may be unbounded. In-line with recent state-of-the-art offline RL algorithms, our goal is to establish an offline PG algorithm that adapts to and retains meaningful guarantees under arbitrary offline datasets, for which our key consideration is establishing an offline gradient estimation method. We begin by defining these offline datasets.

**Definition 4.1**.: The offline dataset is \(=\{_{h}\}\), where \(_{h}=\{(s_{h},a_{h},s_{h+1},r_{h+1}))\}_{i=1}^{n}\) is generated i.i.d. as \(s_{h} d_{h}^{D}\) for some \(d_{h}^{D}()\) and \(a_{h}_{h}^{D}(|s_{h})\) in \(\), for a known behavior policy \(_{h}^{D}\). The marginal next-state distribution in \(_{h}\) is denoted as \(d_{h}^{D,}(s_{h+1})\).

Def. 4.1 is more general than the typical i.i.d. trajectory setting , where \(d_{h}^{D}=d_{h-1}^{D,}\). Crucially, unlike previous works that require lower-bounded \(d^{D}\) or all-policy coverage , we will make no assumptions about the quality of \(\) with respect to \(_{}\).

**Additional notation.** For short, we say \(_{_{h}}[]_{(s_{h},a_{h},s_{h+1},r_{ h+1})_{h}}[]\), and use \((s,a,s^{},r^{})_{h}\) when clear from the context. For any \(g:^{p}\) and reweighting function \(:H_{+}\), we define an _offline reweighted_ analog to \(_{h}^{}\) (Lem. 3.1) for all \(h[H]\) to be

\[[_{h}^{D,}g](s^{}):=_{(s,a,s^{}) _{h}_{h}}[g(s,a)|s^{}]=_{s,a}_ {h}_{h}](s,a,s^{})}{_{s,a}[_{h}_{h}](s,a,s^{})}\ g(s,a).\] (5)

The (time-reversed) conditional expectation is taken over \([_{h}_{h}](s,a,s^{}):=P(s^{}|s,a)d_{h}^{D}(s) _{h}^{D}(a|s)_{h}(s,a)\), the joint offline distribution re-weighted by \(_{h}\). While this may not be a valid density, its induced conditional distribution on \((s,a|s^{})\) always is, i.e., \(_{s,a}_{h}_{h}](s,a,s^{})}{_{s,a}[ _{h}_{h}](s,a,s^{})}=1\). As an example, for a given \(\) we have \(_{h}^{D,}=_{h}^{}\) when \(_{h}(s,a)=^{}(s)(a|s)}{d_{h}^{D}(s)_{h}^{D}(a|s)}\) is the policy's density ratio and is well-defined.

### Offline density-based policy gradient

A policy's occupancy \(d^{}\) may not be covered by arbitrary offline data (Def. 4.1), so neither its expected return \(J()=_{h} d_{h}^{},R\) nor its gradient \( J()\) will be estimatable from \(\). As a result, there is no hope of recovering \(*{argmax}_{_{}}J()\). Our solution is to instead _maximize return only on areas of the state space that are sufficiently covered by offline data_, which is captured exactly by the recursively clipped occupancy \(d^{}\) from . It clamps the policy occupancy to preset multiples \(C_{h}^{},C_{h}^{}\) of the offline data distribution, thereby representing only the "sufficiently covered" portion.

**Definition 4.2** (Recursively clipped occupancy).: Let \(():=\{,\}\). Given clipping constants \(\{C_{h}^{},C_{h}^{}\} 1\), define the clipped policy to be \(_{h}=( C_{h}^{}_{h}^{D})\), and recursively define

\[_{h}^{}=^{_{h-1}}(_{h-1}^{} C _{h-1}^{}d_{h-1}^{D}),\  h[H].\] (6)

Eq. (6) resembles the Bellman flow equation with clipped policy \(\), and acts on the previous-timestep \(_{h-1}^{}\) clipped to at most \(C_{h-1}^{}d_{h-1}^{D}\). Above this threshold the occupancy is considered to be insufficiently covered for estimation, and \(C^{}\) strikes a bias-variance tradeoff between the amount of clipped mass vs. distribution shift. The clipped occupancy's density ratio is always well-defined and bounded as \(_{h}^{}/d_{h-1}^{D,} C_{h-1}^{}C_{h-1}^{}\), and we use it to define our (now learnable) offline objective,

\[()=_{h}_{s_{h}}_{h}^{}(s_{h})R(s_{h})=_{h} _{_{h-1}}[_{h}^{}(s_{h})}{d_{h-1}^{D, }(s_{h})}R(s_{h})].\]

For any "fully covered" policy with \(d_{h}^{} C_{h-1}^{}C_{h-1}^{}d_{h-1}^{D,}\) for all \(h[H]\), we have \(^{}=d^{}\) and \(()=J()\). In this sense, \(*{argmax}_{}()\) will be at least as good as the best policy fully covered by offline data. Next, define the density ratio be \(_{h}^{}:=_{h}^{}/d_{h-1}^{D,}\). The gradient of \(()\) is

\[()=_{h}_{_{h-1}}[_{h}^{ }(s_{h})R(s_{h})\;_{h}^{}(s_{h})].\]

To calculate this gradient we must compute both \(^{}\) and \(^{}\); for the former,  provides a method that we will later call as a subroutine. Our focus is on computing \(_{h}^{}\), which is enabled by the following recursive equation, which is an offline analog of Lem. 3.1.

**Lemma 4.1**.: _For any \(\) and all \(h[H]\), define \(_{h}^{}(s,a):=_{h}^{}(s) C_{h}^{ }d_{h}^{D}(s))}{_{h}^{}(s)}_{h}(a|s)} {_{h}^{D}(a|s)}\). Then_

\[_{h}^{}=_{h-1}^{D,^{}}( | C_{h}^{}_{h-1}^{D}]+ _{h-1}^{}[_{h-1}^{} C_{h}^{ }d_{h-1}^{D}]),\] (7)

_where \(_{h-1}^{D,^{}}\) is from Eq. (5), and \([M v]():=v()M()^{}\) for \(M:\) and \(v:\)._

Lem. 4.1 is derived from applying the chain rule to Def. 4.2, and the clipped occupancies play an instrumental role in handling insufficient offline coverage. Notably, the indicator function zeroes-out both the gradients \(\) and \(_{h-1}^{}\) where they are insufficiently covered, e.g., \(_{h-1}^{}(s)>C_{h-1}^{}d_{h-1}^{D}(s)\). Further, under full offline coverage we recover Lem. 3.1 and \(^{}= d^{}\).

Because the rewards are nonnegative, \(^{}\) induces a _pessimistic policy gradient_ that shifts policies away from out-of-distribution actions, even if they generate high return. This is seen more clearly in Prop. 4.1, that rearranges the resulting expression for \(()\) into a value-based form:

**Proposition 4.1**.: _We can equivalently write_

\[()=_{h}_{_{h}}[_{h}^{}(s,a)_{h}(a|s)_{h}^{}(s,a)],\]

_where \(^{}\) is a pessimistic value function that obeys the Bellman-like recursion \(_{h}^{}(s,a)=[ C_{h}^{}_{h}^{D}](a|s) _{s^{}}P(s^{}|s,a)R(s^{})+[_{h+1} ^{} C_{h+1}^{}d_{h+1}^{D}](s^{})\;_{h+1}^{}(s ^{},_{h+1})\)._

In \(^{}\), future returns are zeroed out at states and actions that exceed the threshold of data coverage, due to indicators functions that are inherited from \(^{}\). Prop. 4.1 can be seen as a pessimistic offline analog to the classical PG theorem \( J()=_{h}_{s,a d_{h}^{}}[(a|s)Q_{h}^ {}(s,a)]\), entirely induced by the definition of the clipped occupancy.

**Non-robustness of \(^{}\) estimation to plug-in densities.** With finite samples, however, it turns out that consistent estimates of \(_{h}^{}\) in Eq. (6) cannot be computed. To make this argument, we first outline the high-level gradient estimation procedure for a fixed policy:

* Estimate occupancies \(\{_{h}^{}\}\) and \(\{_{h}^{D}\}\)
* Compute \(_{h}^{}\) using Eq. (7) with plug-in indicator function estimate \([_{h-1}^{} C_{h}^{}_{h-1}^{ }]\)

The problem arises in step two, as \([]\) is a stepwise function and not smooth. Even if \(^{}\) is vanishingly close to \(d^{}\), the gradient calculated from plug-in occupancy estimates can have constant error.

**Proposition 4.2**.: _There exists an MDP and policy \(\) such that, for any \(>0\), \(_{h,s}\| d_{h}^{}(s)-_{h}^{}(s)\| =O(1)\) when \(\|_{h}^{}-_{h}^{}\|_{1}\) and \(\|_{h}^{}-d_{h}^{D}\|_{1}\) for all \(h\)._

### Smooth clipping

To resolve this issue, we will use a "smooth-clipping" function \((x,c)\) to approximate the "hard"-clipping \((x c)\) in Eq. (6), whose non-smooth gradient was the source of our estimation problems. Figure 1 plots 1-D examples of \((x,c)\) against \((x c)\) as reference (dashed), and Ass. 4.1 describes the properties of \(\) that enable our later estimation and convergence guarantees.

**Assumption 4.1**.: Assume that \(\) satisfies \( x,x^{},c,c^{}()\),

1. (Approximate clipping) \( D_{} 0\) such that \(0(x c)-(x,c) D_{}(x c)\).
2. (Monotonicity) \((x^{},c)(x,c)\) if \(x^{} x;(x,c^{})(x,c)\) if \(c^{} c\); and vice versa.
3. (Smooth gradient) Define the smoothed indicator \(}(x,c):=x\;_{x}(x,c)\), where \(\;_{x}\) is the partial derivative w.r.t. \(x\). Then \(}(x,c)\) and \( L_{} 0\) s.t. \( x,x^{},c,c^{}()\), \[c|}(x,c)-}(x^{},c )| L_{}|x-x^{}|,\;\;\;x|} (x,c)-}(x,c^{})| L_{}| c-c^{}|.\]

Note that \((x,c)=(x c)\) is a special case with \(}(x,c)=[x c]\), thus \(D_{}=0\) and \(L_{}=\). The following choice of \(\), which is plotted in Fig. 1, fulfills Ass. 4.1.

**Proposition 4.3**.: _For any \(b>1\), \((x,c)=(x^{-b}+c^{-b})^{-1/b}\) has \(L_{}=b\) and \(D_{}=1/b\)._

Next, we define the smooth-clipped occupancy function \(_{h}^{}\), which is no larger than \(_{h}^{}\).

**Definition 4.3** (Recursively smooth-clipped occupancy).: For smooth-clipping function \(\) satisfying Ass. 4.1 and clipping constants \(\{C_{h}^{},C_{h}^{}\}\), define \(_{h}:=(,C_{h}^{}_{h}^{D})\), and inductively set

\[_{h}^{}=^{_{h-1}}(( _{h-1}^{},C_{h-1}^{}d_{h-1}^{D})),\;  h[H].\] (8)

Then letting \(_{h}^{}:=_{h}^{}/d_{h-1}^{D,}\), our new objective is \(()=_{h}_{_{h-1}}[_{h}^ {}(s_{h})R(s_{h})]\) with gradient \(()=_{h}_{_{h-1}}[_{h}^{}(s_{h})R(s_{h})\;_{h}^{}(s_{h})]\), where \(_{h}^{}\) obeys the following recursion.

**Lemma 4.2**.: _For \(\) satisfying Ass. 4.1, recall \(}(x,c):=x\;_{x}(x,c).\) Then for all \(h[H]\),_

\[_{h}^{}=_{h-1}^{D,^{}} (\;}(,C_{h-1}^{} _{h-1}^{D})+_{h-1}^{}\;}(_{h-1}^{},C_{h-1}^{}d_{h-1}^{D} )),\] (9)

_where \(_{h-1}^{}(s,a):=_{h-1}^{ }(s),C_{h-1}^{}d_{h-1}^{D})}{d_{h-1}^{D}(s)}_{h-1}(a|s)}{_{h-1}^{D}(a|s)}\) and \(_{h-1}^{D,^{}}\) is defined in Eq. (5). Further, under Ass. 2.1, \(_{s,h}\|_{h}^{}(s)\|_{} hG\)._

Eq. (9) replaces the (non-smooth) indicator function in \(^{}\) (Lem. 4.1) with its smooth approximation \(}\), which, as we will show shortly, enables robust gradient estimation with plug-in occupancy estimates. As before, we can reduce it to squared-loss regression (Eq. (11)). Further, by optimizing \(()\), we also approximately maximize our target objective \(()\), with bias proportional to \(D_{}\).

**Proposition 4.4**.: _Under Ass. 4.1, \(0_{_{}}()-_{_{}} () H^{2}D_{}.\)_

### Offline smooth-clipped gradient estimation

Alg. 2 describes the offline PG algorithm for optimizing \(()\). To reduce clutter, we have used \(_{h}:=\;}( ,C_{h}^{}_{h}^{D})\). First, Off-OccuPG estimates \(d_{h-1}^{D}\) using MLE (details in App. D due to space constraints). Then, for each iteration \(t\), it estimates the smooth-clipped occupancy \(_{h}^{^{(t)}}\) using Forc (adapted from , see App. E). This is plugged into a squared-loss regression problem approximating Eq. (9) to learn \(_{h}^{(t)}\) (lines 8 to 10), then estimate \((^{(t)})\) (line 12).

Figure 1: We plot \((x,c)\) from Prop. 4.3 for different \(b\), that trade-off between clipping approximation error and smoothness (\(D_{} 1/L_{}\)).

Before stating the estimation guarantee for \(()\), we first introduce the required assumptions. For simplicity, we assume that the function classes used in MLE and Forc are finite, and defer their guarantees to the respective appendices, as they have been well-established in previous papers . We focus on discussing Ass. 4.2 for the offline gradient function class, which requires a stronger level of expressiveness. Since the regression target in Off-OccuPG involves plug-in occupancy estimates, the completeness condition naturally requires \(_{h}\) to express the gradient update in Lem. 4.2 for all possible targets composed of functions from \(_{h-1},_{h-1},_{h-1}\). As a result, Ass. 4.2 is generally stronger than Ass. 3.1 for OccuPG.

**Assumption 4.2**.: For all \(h\), \(_{g_{h}}\|g_{h}\|_{} hG\); and for all \((,g,f,f^{},w)_{}_{h-1}_ {h}_{h-1}_{h-1}\), we have \(_{h-1}^{D,}(_{h-1}+g}(wf^{},C_{h-1}^{}f))_{h}\), where \(=,C_{h}^{}f)}{f}_{h-1}}{_{h-1}^{D}}\).

When the underlying MDP has favorable structure, however, we can expect that \(_{}\) is not much larger than was required for OccuPG. This is indeed the case in low-rank MDPs, where the \(\) defined in Rem. 3.1 also satisfies Ass. 4.2 (proof in Prop. C.1). Due to the bilinear transition structure, the offline gradient update (Lem. 4.2) applied to any target remains a linear-over-linear function.

The guarantees for the MLE (Alg. 4) and weight estimation (Alg. 5) subroutines require Ass. D.1 and Ass. E.1, respectively, which are included in the preconditions of the main result below. Briefly, Ass. D.1 requires \(\) to realize the true data distributions \(d_{h}^{D}\) and \(d_{h}^{D,}\), which is standard in supervised learning. Ass. E.1 requires \(\) to be closed under the Bellman flow operator, and can be viewed as a 1-dimensional version of Ass. 4.2 where \(=1\). In this sense both assumptions are weaker requirements on expressivity than that of the gradient class in Ass. 4.2, and more detailed discussions are left to App. D and App. E.

Having established its preconditions, we now present our main estimation guarantee for Off-OccuPG, which pays additional factors for the coverage of offline data (\(_{h}C_{h}^{}C_{h}^{}\)) and the smoothness of \(\).

**Theorem 4.1**.: _Suppose \(()\) satisfies Asm. 3.2 and fix \(_{}\). Under Asm. 2.1, Asm. 4.1, Asm. 4.2, Asm. 4.2, Asm. 4.1, and Asm. 4.1, w.p. \( 1-\) we have \(\|()-()\|\) when \(n=(_{}H^{}G^{2}(_{h}C_ {h}^{}C_{h}^{})^{2}L_{2}^{2}(| \||/)}{^{2}})\)._

Stationary convergence & computational efficiency.Similar to OccuPG, Off-OccuPG with \(T=O( H^{2}/^{2})\) converges to an \(\)-stationary point. The formal statement is given in Cor. C.1 and is based on the estimation guarantee in Thm. 4.1. As a result, Off-OccuPG is also computationally oracle-efficient. Each invocation of MLE involves \(2H\) calls to a likelihood maximization oracle (see Alg. 4), and each invocation of Forc requires \(H\) calls to a squared-loss regression oracle (see Alg. 5). Then local convergence is still achieved with \(O( H^{2}/^{2})\) such calls, as increasing \(T\) further cannot reduce error from statistical noise (that depends only the fixed \(n\)).

Optimality.Analyzing the conditions under which offline PG recovers global optima is more challenging, as we can no longer utilize exploratory initialization (from Cor. 3.2). However, since all occupancies have been clipped to the data distribution, we show in App. C.5 that the offline data itself can sometimes suffice as an exploratory initial distribution, and the corresponding bound is in terms of \(\{C_{h}^{}\}\) (instead of the online \(^{^{*}}\)). However, this is not guaranteed in general and our current result only holds under strong all-policy offline data coverage. Briefly, some hardness comes from the fact that clipping causes gradient signals to vanish, so a stationary policy might be far off-support, rather than optimal. Investigating the possibility of more relaxed conditions for offline PG convergence (or, conversely, refining hardness results) are especially interesting directions for future work.

## 5 Conclusion

For the first time, we demonstrate how policy optimization can be conducted with (only) occupancy functions for both online and offline RL, and comprehensively analyze both local and global convergence. In the online setting our method directly extends to optimizing general objective functionals that cannot be optimized using value-based methods, and in the offline setting the occupancy-based gradient naturally handles incomplete offline data coverage. As our work is the first in this line of research and theoretical in nature, for future work we plan to launch empirical investigations of our methods, especially those for optimizing general functionals. Additionally, the conditions under which offline PG can converge to global optima is not well-understood, and we hope that our preliminary results here encourage greater interest and investigation into this question.