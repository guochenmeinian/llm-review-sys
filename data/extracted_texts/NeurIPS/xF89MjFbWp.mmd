# Kullback-Leibler Maillard Sampling for Multi-armed Bandits with Bounded Rewards

Hao Qin

University of Arizona

hqin@arizona.edu

&Kwang-Sung Jun

University of Arizona

kjun@cs.arizona.edu

&Chicheng Zhang

University of Arizona

chichengz@cs.arizona.edu

###### Abstract

We study \(K\)-armed bandit problems where the reward distributions of the arms are all supported on the \(\) interval. Maillard sampling , an attractive alternative to Thompson sampling, has recently been shown to achieve competitive regret guarantees in the sub-Gaussian reward setting  while maintaining closed-form action probabilities, which is useful for offline policy evaluation. In this work, we analyze the Kullback-Leibler Maillard Sampling (KL-MS) algorithm, a natural extension of Maillard sampling and a special case of Minimum Empirical Divergence (MED)  for achieving a KL-style finite-time gap-dependent regret bound. We show that KL-MS enjoys the asymptotic optimality when the rewards are Bernoulli and has an adaptive worst-case regret bound of the form \(O((1-^{*})KT K}+K T)\), where \(^{*}\) is the expected reward of the optimal arm, and \(T\) is the time horizon length; this is the first time such adaptivity is reported in the literature for an algorithm with asymptotic optimality guarantees.

## 1 Introduction

The multi-armed bandit (abbrev. MAB) problem , a stateless version of the reinforcement learning problem, has received much attention by the research community, due to its relevance in may applications such as online advertising, recommendation, and clinical trials. In a multi-armed bandit problem, a learning agent has access to a set of \(K\) arms (also known as actions), where for each \(i[K]:=\{1,,K\}\), arm \(i\) is associated with a distribution \(_{i}\) with mean \(_{i}\); at each time step \(t\), the agent adaptively chooses an arm \(I_{t}[K]\) by sampling from a probability distribution \(p_{t}^{K-1}\) and receives reward \(y_{t}_{I_{t}}\), based on the information the agent has so far. The goal of the agent is to minimize its pseudo-regret over \(T\) time steps: \((T)=T^{*}-_{t=1}^{T}y_{t}\), where \(^{*}=_{i}_{i}\) is the optimal expected reward.

In this paper, we study the multi-armed bandit setting where reward distributions of all arms are supported on \(\). 1 An important special case is Bernoulli bandits, where for each arm \(i\), \(_{i}=(_{i})\) for some \(_{i}\). It has practical relevance in settings such as computational advertising, where the reward feedback is oftentimes binary (click vs. not-click, buy vs. not-buy).

Broadly speaking, there are two popular families of provably regret-efficient algorithms for bounded-reward bandit problems: deterministic exploration algorithms (such as KL-UCB ) and randomized exploration algorithms (such as Thompson sampling (TS) ). Randomized exploration algorithms such as TS have been very popular, perhaps due to its excellent empirical performance and the ability to cope with delayed rewards better than deterministic counterparts . In addition, the logged data collected from randomized exploration, of the form \((I_{t},p_{t,I_{t}},y_{t})_{t=1}^{T}\), where \(p_{t,I_{t}}\) is the probability with which arm \(I_{t}\) was chosen, are useful for offline evaluation purposes byemploying the inverse propensity weighting (IPW) estimator  or the doubly robust estimator . However, calculating the arm sampling probability distribution \(p_{t}\) for Thompson sampling is nontrivial. Specifically, there is no known closed-form 2, and generic numerical integration methods and Monte-Carlo approximations suffer from instability issues: the time complexity for obtaining a numerical precision of \(\) is \(((1/))\). This is too slow to be useful especially for web-scale deployments; e.g., Google AdWords receives \(\)237M clicks per day. Furthermore, the computed probability will be used after taking the inversion, which means that even moderate amount of errors are intolerable. Indeed, Figure 1 shows that the offline evaluation with Thompson sampling as the behavioral policy will be largely biased and inaccurate due to the errors from the Monte Carlo approximation.

Recently, many studies have introduced alternative randomized algorithms that allow an efficient computation of \(p_{t}\)[19; 30; 14; 43]. Of these, Maillard sampling (MS) [30; 11], a Gaussian adaptation of the Minimum Empirical Divergence (MED) algorithm  originally designed for finite-support reward distributions, provides a simple algorithm for the sub-Gaussian bandit setting that computes \(p_{t}\) in a closed form:

\[p_{t,a}(-N_{t-1,a}_{t-1,a}^{2}}{2^{2}})\] (1)

where at time step \(t\), \(N_{t,a}\) is the number of pulling arm \(a\). We define the estimator of \(_{a}\) as \(_{t,a}:=^{t}\{I_{t}=a\}y_{t}}{N_ {t,a}}\) and the best performed mean value as \(_{t,}:=_{a[K]}_{t,a}\). \(_{t-1,a}=_{a^{}}_{t-1,a^{}}-_{t -1,a}\) is the empirical suboptimality gap of arm \(a\), and \(\) is the subgaussian parameter of the reward distribution of all arms. For sub-Gaussian reward distributions, MS enjoys the asymptotic optimality under the special case of Gaussian rewards and a near-minimax optimality , making it an attractive alternative to Thompson sampling. Also, MS satisfies the sub-UCB criterion (see Section 2 for a precise definition) to help establish sharp finite-time instance-dependent regret guarantees. Can we adapt MS to the bounded reward setting and achieve the asymptotic, minimax optimality and sub-UCB criterion while computing the sampling probability in a closed-form? In this paper, we make significant progress on this question.

**Our contributions.** We focus on a Bernoulli adaptation of MS that we call Kullback-Leibler Maillard Sampling (abbrev. KL-MS) and perform a finite-time analysis of it in the bounded-reward bandit problem. KL-MS uses a sampling probability similar to MS but tailored to the \(\)-bounded reward setting:

\[p_{t,a}(-N_{t-1,a}(_{t-1,a}, _{t-1,})),\]

where \((,^{}):=}+(1-)}\) is the binary Kullback-Leibler (KL) divergence. We can also view KL-MS as an instantiation of MED  for Bernoulli rewards; See Section 3 for a

Figure 1: Histogram of the average rewards computed from the offline evaluation where the logged data is collected from Bernoulli TS and KL-MS (Algorithm 1) in a Bernoulli bandit environment with the mean reward (0.8, 0.9) with time horizon \(T=10,000\). For Bernoulli TS’s log, we approximate the action probability by Monte Carlo Sampling with 1000 samples for each step. Here we estimate the expected reward of the uniform policy which has expected average reward of 0.85 (black dashed line). Across 2000 trials, the logged data of KL-MS induces an MSE of \(0.00796\); however, for half of the trials, the IPW estimator induced by Bernoulli TS’s log returns invalid values due to the action probability estimates being zero. Even excluding those invalid values, the Bernoulli TS’s logged data induces an MSE of \(0.02015\). See Appendix H for additional experiments.

detailed comparison. KL-MS performs an efficient exploration for bounded rewards since one can use \((a,b) 2(a-b)^{2}\) to verify that the probability being assigned to each empirical non-best arm by KL-MS is never larger than that of MS with \(^{2}=1/4\), the best sub-Gaussian parameter for the bounded rewards in \(\). We show that KL-MS achieves a sharp finite-time regret guarantee (Theorem 1) that can be simultaneously converted to:

* an asymptotic regret upper bound (Theorem 4), which is asymptotically optimal when specialized to the Bernoulli bandit setting;
* a \(\)-style regret guarantee of \(O((1-^{*})KT K}+K(T))\) (Theorem 3) where \(^{*}\) is the mean reward of the best arm. This bound has two salient features. First, in the worst case, it is at most a \(\) factor suboptimal than the minimax optimal regret of \(()\)[5; 10]. Second, its \(((1-^{*})}\) coefficient adapts to the variance of the optimal arm reward; this is the first time such adaptivity is reported in the literature for an algorithm with asymptotical optimality guarantees. 3 * a sub-UCB regret guarantee, which many existing minimax optimal algorithms [33; 18] have not been proven to satisfy.

We also conduct experiments that show that thanks to its closed-form action probabilities, KL-MS generates much more reliable logged data than Bernoulli TS with Monte Carlo estimation of action probabilities; this is reflected in their offline evaluation performance using the IPW estimator; see Figure 1 and Appendix H for more details.

## 2 Preliminaries

Let \(N_{t,a}\) be the number of times arm \(a\) has been pulled until time step \(t\) (inclusively). Denote the suboptimality gap of arm \(a\) by \(_{a}:=^{*}-_{a}\), where \(^{*}=_{i[K]}_{i}\) is the optimal expected reward. Denote the empirical suboptimality gap of arm \(a\) by \(_{t,a}:=_{t,}-_{t,a}\); here, \(_{t,a}\) is the empirical estimation to \(_{a}\) up to time step \(t\), i.e., \(_{t,a}:=}_{s=1}^{t}y_{s}\{I_{s}=a\}\), and \(_{t,}=_{a[K]}_{t,a}\) is the best empirical reward at time step \(t\). For arm \(a\), define \(_{a}(s):=\{t 1:N_{t,a}=s\}\) at the time step when arm \(a\) is pulled for the \(s\)-th time, which is a stopping time; we also use \(_{(s),a}:=_{_{a}(s),a}\) to denote empirical mean of the first \(s\) reward values received from pulling arm \(a\).

  Algorithm\& & Finite-Time Regret & Closed-form & Reference \\ Analysis & Minimax Ratio & Sub-UCB & Probability & \\  TS & \(\) & yes & no & See the caption \\ ExpTS & \(\) & yes & no & Jin et al.  \\ ExpTS\({}^{+}\) & \(1\) & \(-^{**}\) & no & Jin et al.  \\ kl-UCB & \(\) & yes & N/A & Cappé et al.  \\ kl-UCB++ & \(1\) & \(-^{**}\) & N/A & Ménard and Garivier  \\ kl-UCB-switch & \(1\) & \(-^{**}\) & N/A & Garivier et al.  \\ MED & \(-\) & \(-\) & no\({}^{*}\) & Honda and Takemura  \\ DMED & \(-\) & \(-\) & N/A & Honda and Takemura  \\ IMED & \(-\) & \(-\) & N/A & Honda and Takemura  \\ KL-MS & \(\) & yes & yes & this paper \\  

Table 1: Comparison of regret bounds for bounded reward distributions; for space constraints we only include those that achieves the asymptotic optimality for the special case of Bernoulli distributions (this excludes, e.g., Maillard Sampling [30; 11], Tsallis-INF  and UCB-V ). ‘\(-\)’indicates that the corresponding analysis is not reported. ‘N/A’indicates that the algorithm does have closed-form, but it is deterministic. ‘\(\)’ indicates that its computational complexity for calculating the action probability is \((1/)\). ‘\(\)’indicates that we conjecture that the algorithm is not sub-UCB. The results on TS are reported by Agrawal and Goyal [3; 4], Korda et al. .

We define the Kullback-Leibler divergence between two distributions \(\) and \(\) as \((,)=_{X}[}{}(X)]\) if \(\) is absolutely continuous w.r.t. \(\), and \(=+\) otherwise. Recall that we define the binary Kullback-Leibler divergence between two numbers \(,^{}\) in \(\) as \((,^{}):=}+(1-)}\), which is also the KL divergence between two Bernoulli distributions with mean parameters \(\) and \(^{}\) respectively. We define \(=(1-)\), which is the variance of \(()\) but otherwise an upper bound on any distribution supported on \(\) with mean \(\); see Lemma 16 for a formal justification.

In the regret analysis, we will oftentimes use the following notation for comparison up to constant factors: define \(f g\) (resp. \(f g\)) to denote that \(f Cg\) (resp. \(f Cg\)) for some numerical constant \(C>0\). We define \(a b\) and \(a b\) as \((a,b)\) and \((a,b)\), respectively. For an event \(E\), we use \(E^{c}\) to denote its complement.

Below, we define some useful criteria for measuring the performance of bandit algorithms, specialized to the \(\) bounded reward setting.

**Asymptotic optimality in the Bernoulli reward setting** An algorithm is asymptotically optimal in the Bernoulli reward setting [27; 12] if for any Bernoulli bandit instance \((_{a}=(_{a}))_{a[K]}\), \(_{T}(T)}{ T}=_{a:_{a}>0}}{(_{a},^{*})}\).

**Minimax ratio** The minimax optimal regret of the \(\) bounded reward bandit problem is \(()\)[5; 10]. Given a \(K\)-armed bandit problem with time horizon \(T\), an algorithm has a minimax ratio of \(f(T,K)\) if its has a worst-case regret bound of \(O(f(T,K))\).

**Sub-UCB** Sub-UCB is originally defined in the context of sub-Gaussian bandits : given a bandit problem with \(K\) arms whose reward distributions are all sub-Gaussian, an algorithm is said to be sub-UCB if there exists some positive constants \(C_{1}\) and \(C_{2}\), such that for all \(^{2}\)-sub-Gaussian bandit instances, \((T) C_{1}_{a:_{a}>0}_{a}+C_{2}_{a:_ {a}>0}}{_{a}} T\). Specialized to our setting, as any distribution supported on \(\) is also \(\)-sub-Gaussian, and all suboptimal arm gaps \(_{a}(0,1]\) are such that \(_{a}<}\), the above sub-UCB criterion simplifies to: there exists some positive constant \(C\), such that for all \(\)-bounded reward bandit instances, \((T) C_{a:_{a}>0}}\).

## 3 Related Work

**Bandits with bounded rewards.** Early works of Lai et al. , Burnetas and Katehakis  show that in the bounded reward setting, for any consistent stochastic bandit algorithm, the regret is lower bounded by \((1+o(1))_{a:_{a}>0} T}{_{}(_{a},^{*})}\) and \(_{}(_{a},^{*})\) is defined as

\[_{}(,^{*}):=\{(,): _{X}[X]>^{*},() \},\] (2)

where the random variable follows a distribution \(\) bounded in \(\). Therefore, any algorithm whose regret upper bound matches the lower bound is said to achieve asymptotic optimality. Cappe et al.  propose the KL-UCB algorithm and provide a finite time regret analysis, which is further refined by Lattimore and Szepesvari [29; Chapter 10]. Another line of work establishes asymptotic and finite-time regret guarantees for Thompson sampling algorithms and its variants [2; 4; 25; 24], which, when specialized to the Bernoulli bandit setting, can be combined with Beta priors for the Bernoulli parameters to design efficient algorithms.

A number of studies even go beyond the Bernoulli-KL-type regret bound and adapt to the variance of each arm in the bounded reward setting. UCB-V  achieves a regret bound that adapts to the variance. Efficient-UCBV  achieves a variance-adaptive regret bound and also an optimal minimax regret bound \(O()\), but it is not sub-UCB. Honda and Takemura  propose the MED algorithm that is asymptotically optimal for bounded rewards, but it only works for rewards that with finite supports. Honda and Takemura  propose the Indexed MED (IMED) algorithm that can handle a more challenging case where the reward distributions are supported in \((-,1]\).

As with worst-case regret bounds, first, it is well-known that for Bernoulli bandits as well as bandits with \(\) bounded rewards, the minimax optimal regrets are of order \(()\)[10; 5]. Of the algorithms that enjoy asymptotic optimality under the Bernoulli reward setting described above, KL-UCB  has a worst-case regret bound of \(O()\), which is refined by the KL-UCB++algorithm  that has a worst-case regret bound of \(O()\). We also show in Appendix F.1 and F.2 that with some modifications of existing analysis, KL-UCB and KL-UCB++ enjoy a regret bound of \(O((1-^{*})KT T})\) and \(O((1-^{*})K^{3}T T})\) respectively. Although the regret is worse in the order of \(K\), it adapts to \(^{*}\) and will have a better regret when \(^{*}\) is small (say, \(^{*} 1/K^{2}\)). KL-UCB++ and KL-UCB-Switch achieves \(O()\) regret in the finite-time regime and asymptotic optimality, while the sub-UCB criterion has not been satisfied. However, Lattimore [28, SS3] shows that MOSS  suffers a sub-optimal regret worse than UCB-like algorithms because of not satisfying sub-UCB criteria, and we suspect that KL-UCB-switch experience the same issue as MOSS. For Thompson Sampling style algorithms, Agrawal and Goyal  shows that the original Thompson Sampling algorithm has a worst-case regret of \(O()\), and the ExpTS+ algorithm  has a worst-case regret of \(O()\).

**Randomized exploration for bandits.** Many randomized exploration methods have been proposed for multi-armed bandits. Perhaps the most well-known is Thompson sampling , which is shown to achieve Bayesian and frequentist-style regret bounds in a broad range of settings . A drawback of Thompson sampling, as mentioned above, is that the action probabilities cannot be obtained easily and robustly. To cope with this, a line of works design randomized exploration algorithms with action probabilities in closed forms. For sub-Gaussian bandits, Cesa-Bianchi et al.  propose a variant of the Boltzmann exploration rule (that is, the action probabilities are proportional to exponential to empirical rewards, scaled by some positive numbers), and show that it has \(O(T}{})\) instance-dependent and \(O( K)\) worst-case regret bounds respectively, where \(=_{a:_{a}>0}_{a}\) is the minimum suboptimality gap. Maillard sampling (MS; Eq. (1)) is an algorithm proposed by the thesis of Maillard  where the author reports that MS achieves the asymptotic optimality and has a finite-time regret of order \(_{a:_{a}>0}(}+^{2}})\) from which a worst-case regret bound of \(O(T^{3/4})\) can be derived. MED , albeit achieves asymptotic optimality for a broad family of bandits with finitely supported reward distributions, also has a high finite-time regret bound of at least \(_{a:_{a}>0}(}+^{2( (_{1}))-1}})\). 4 Recently, Bian and Jun  report a refined analysis of Maillard 's sampling rule, showing that it has a finite time regret of order \(_{a:_{a}>0}^{2})}{_{a}}+O(_{a: _{a}>0}}(}))\), and additionally enjoys a \(O()\) worst-case regret, and by inflating the exploration slightly (called MS\({}^{+}\)), the bound can be improved and enjoy the minimax regret of \(O()\), which matches the best-known regret bound among those that satisfy sub-UCB criterion, except for AdaUCB. In fact, it is easy to adapt our proof technique in this paper to show that MS, without any further modification, achieves a \(O()\) worst-case regret.

Randomized exploration has also been studied from a nonstochastic bandit perspective , where randomization serves both as a tool for exploration and a way to hedge bets against the nonstationarity of the arm rewards. Many recent efforts focus on designing randomized exploration bandit algorithms that achieve "best of both worlds" adaptive guarantees, i.e., achieving logarithmic regret for stochastic environments while achieving \(\) regret for adversarial environments [e.g. 43, 42].

**Binarization trick.** It is a folklore result that bandits with \(\) bounded reward distributions can be reduced to Bernoulli bandits via a simple binarization trick: at each time step \(t\), the learner sees reward \(r_{t}\), draws \(_{t}(r_{t})\) and feeds it to a Bernoulli bandit algorithm. However, this reduction does not result in asymptotic optimality for the general bounded reward setting, where the asymptotic optimal regret is of the form \((1+o(1))_{a:_{a}>0} T}{_{} (_{a},^{*})}\) with \(_{}(_{a},^{*})\) defined in the Eq (2). If we combine the binarization trick and the MED algorithm in the bounded reward setting, the size of the support set is viewed as \(2\), the finite-time regret bound is at best as \(O(K^{1/4}T^{3/4})\) (ignoring logarithmic factors), which is much higher than \(O()\).

**Bandit algorithms with worst-case regrets that depend on the optimal reward.** Recent linear logistic bandit works have shown worst-case regret bounds that depend on the variance of the best arm [32; 1]. When the arms are standard basis vectors, logistic bandits are equivalent to Bernoulli bandits, and the bounds of Abeille et al.  become \((KT}+}{_{}}(K^{2}+A))\) where \(_{}=_{i[K]}_{i}\) and \(A\) is an instance dependent quantity that can be as large as \(T\). This bound, compared to ours, has an extra factor of \(\) in the leading term and the lower order term has an extra factor of \(K\). Even worse, it has the term \(_{}^{-1}\) in the lower order term, which can be arbitrarily large. The bound in Mason et al.  becomes \((KT}+_{}^{-1}K^{2})\), which matches our bound in the leading term up to logarithmic factors yet still have extra factors of \(K\) and \(_{}^{-1}\) in the lower order term.

## 4 Main Result

**The KL Maillard Sampling Algorithm.** We propose an algorithm called KL Maillard sampling (KL-MS) for bounded reward distributions (Algorithm 1). For the first \(K\) times steps, the algorithm pulls each arm once (steps 3 to 4); this ensures that starting from time step \(K+1\), the estimates of the reward distribution of all arms are well-defined. From time step \(t=K+1\) on, the learner computes the empirical mean \(_{t-1,a}\) of all arms \(a\). For each arm \(a\), the learner computes the binary KL divergence between \(_{t-1,a}\) and \(_{t-1,}\), \(|(_{t-1,a},_{t-1,})\), as a measure of empirical suboptimality of that arm. The sampling probability of arm \(a\), denoted by \(p_{t,a}\), is proportional to the exponential of negative product between \(N_{t-1,a}\) and \(|(_{t-1,a},_{t-1,})\) (Eq. (3) of step 6). This policy naturally trades off between exploration and exploitation: arm \(a\) is sampled with higher probability, if either it has not been pulled many times (\(N_{t-1,a}\) is small) or it appears to be close to optimal empirically (\(|(_{t-1,a},_{t-1,}))\) is small). The algorithm samples an arm \(I_{t}\) from \(p_{t}\), and observe a reward \(y_{t}\) of the arm chosen.

```
1:Input:\(K 2\)
2:for\(t=1,2,,T\)do
3:if\(t K\)then
4: Pull the arm \(I_{t}=t\) and observe reward \(y_{t}_{i}\).
5:else
6: For every \(a[K]\), compute \[p_{t,a}=}(-N_{t-1,a}|(_{t-1,a}, _{t-1,}))\] (3) where \(M_{t}=_{a=1}^{K}(-N_{t-1,a}|(_{t-1,a},_{t-1,}))\) is the normalizer.
7: Pull the arm \(I_{t} p_{t}\).
8: Observe reward \(y_{t}_{I_{t}}\).
9:endif
10:endif
11:endfor ```

**Algorithm 1** KL Maillard Sampling (KL-MS)

We remark that if the reward distributions \(_{i}\)'s are Bernoulli, KL-MS is equivalent to the MED algorithm  since in this case, all reward distributions have a binary support of \(\{0,1\}\). However, KL-MS is different from MED in general: MED computes the empirical distributions of arm rewards \(_{t-1,a}\), and chooses action according to probabilities \(p_{t,a}(-N_{t-1,a}D_{t-1,a})\); here, \(D_{t-1,a}:=(_{t-1,a},_{t-1,})\) (recall its definition in Section 3) is the "minimum empirical divergence" between arm \(a\) and the highest empirical mean reward, which is different from the binary KL divergence of the mean rewards used in KL-MS.

### Main Regret Theorem

Our main result of this paper is the following theorem on the regret guarantee of KL-MS (Algorithm 1). Without loss of generality, throughout the rest of the paper, we assume \(_{1}_{2}_{K}\).

**Theorem 1**.: _For any \(K\)-arm bandit problem with reward distribution supported on \(\), KL-MS has regret bounded as follows. For any \( 0\) and \(c(0,]\):_

\[(T)  T+_{a:_{a}>}(}(_{a}+c_{a},_{1}-c_{a}) e^{2})}{}( _{a}+c_{a},_{1}-c_{a})}\] \[+560_{a:_{a}>}(_{1}+ _{a}}{c^{2}_{a}})((_{1}+_{ a}}{c^{2}_{a}^{2}}T_{a}^{2}}{_{1}+_{a}} ) e^{2})\] (4)

The regret bound of Theorem 1 is composed of three terms. The first term is \(T\), which controls the contribution of regret from all \(\)-near-optimal arms. The second term is asymptotically \((1+o(1))_{a:_{a}>0}}{}(_{a},_{ 1})}(T)\) with an appropriate choice of \(c\), which is a term that grows in \(T\) in a logarithmic rate. The third term is simultaneously upper bounded by two expressions. One is \(_{a:_{a}>0}(_{1}+_{a}}{c^{2}_{a}} )(T_{a}^{2}}{_{1}+_{a}} e^{2})\), which is of order \( T\) and helps establish a tight worst-case regret bound (Theorem 3); the other is \(_{a:_{a}>0}(_{1}+_{a}}{c^{2}_{a}} )((_{1}+_{a}}{c^{2}_{a}^{2}} ) e^{2})\), which does not grow in \(T\) and helps establish a tight asymptotic upper bound on the regret (Theorem 4).

To the best of our knowledge, existing regret analysis on Bernoulli bandits or bandits with bounded support have regret bounds of the form

\[(T) T+_{a:_{a}>}(T)} {}(_{a}+c_{a},_{1}-c_{a})}+O(_{a: _{a}>}_{a}}),\]

for some \(c>0\), where the third term is much larger than its counterpart given by Theorem 1 when \(_{a}\) and \(_{1}\) are small. As we will see shortly, as a consequence of its tighter bounds, our regret theorem yields a superior worst-case regret guarantee over previous works.

**Theorem 2** (Sub-UCB).: _KL-MS's regret is bounded by \((T)_{a:_{a}>0}}\). Therefore, KL-MS is sub-UCB._

Sub-UCB criterion is important for measuring a bandit algorithm's finite-time instance-dependent performance. Indeed, Lattimore [28, SS3] points out that MOSS  does not satisfy sub-UCB and that it leads to a strictly suboptimal regret in a specific instance compared to the standard UCB algorithm . A close inspection of the finite-time regret bounds of existing asymptotically optimal and minimax optimal algorithms for the \(\)-reward setting, such as KL-UCB++  and KL-UCB-switch , reveals that they are not sub-UCB. Thus, we speculate that they would also have a suboptimal performance in the aforementioned instance.

In light of Theorem 1, our first corollary is that KL Maillard sampling achieves the following adaptive worst-case regret guarantee.

**Theorem 3** (Adaptive worst-case regret).: _For any \(K\)-arm bandit problem with reward distribution supported on \(\), KL-MS has regret bounded as: \((T)_{1}KT K}+K T\)._

An immediate corollary is that KL Maillard sampling has a regret of order \(O()\), which is a factor of \(O()\) within the minimax optimal regret \(()\). This also matches the worst-case regret bound \(O()\) of Jin et al.  where \(V=\) is the worst-case variance for Bernoulli bandits using a Thompson sampling-style algorithm. Another main feature of this regret bound is its adaptivity to \(_{1}\), the variance of the reward of the optimal arm for the Bernoulli bandit setting, or its upper bound in the general bounded reward setting (see Lemma 16). Specifically, if \(_{1}\) is close to 0 or 1, \(_{1}\) is very small, which results in the regret being much smaller than \(O()\).

Note that UCB-V  and KL-UCB/KL-UCB++, while not reported, enjoy a worst-case regret bound of \(O(_{1}KT T})\), which is worse than our bound in its logarithmic factor; see Appendix F.3 and F.1 for the proofs. Among these, UCB-V does not achieve the asymptotic optimality for the Bernoulli case. While logistic linear bandits  can be applied to Bernoulli \(K\)-armed bandits and achieve similar worst-case regret bounds involving \(_{1}\), their lower order term can be much worse as discussed in Section 3.

[MISSING_PAGE_FAIL:8]

Theorem 1 follows by plugging the above bound to Eq. (6) for arms \(a\) s.t. \(_{a}>\) with \(c=\). 

### Proof sketch of Lemma 5

We sketch the proof of Lemma 5 in this subsection. For full details of the proof, please refer to Appendix C.2. We first set up some useful notations that will be used throughout the proof. Let \(u:=(_{a}+_{1},_{1}- _{2}) e^{2})}{(_{a}+ _{1},_{1}-_{2})}\). We define the following events

\[A_{t}:=\{I_{t}=a\}, B_{t}:=\{N_{t,a}<u\}, C_{t} :=\{_{t,}_{1}-_{2}\}, D_{t}:= \{_{t,a}_{a}+_{1}\},\]

By algebra, one has the following elementary upper bound on \([N_{T,a}]\): \([N_{T,a}] u+[_{t=K+1}^{T} \{A_{t},B_{t-1}^{c}\}]\). Intuitively, the \(u\) term serves to control the length of a "burn-in" phase when the number of pulls to arm \(a\) is at most \(u\). It now remains to control the second term, the number of pulls to arm \(a\) after it is large enough, i.e., \(N_{t-1,a} u\). We decompose it to \(F1\), \(F2\), and \(F3\), resulting in the following inequality:

\[[N_{T,a}] u+[_{t=K+1}^{T} \{A_{t},B_{t-1}^{c},C_{t-1},D_{t-1}\}]}_{=:F1}\] \[+[_{t=K+1}^{T}\{A_ {t},B_{t-1}^{c},C_{t-1},D_{t-1}^{c},\}]}_{=:F2}+[_{t=K+1}^{T}\{A_{t},B_{t-1}^{c},C_{t-1}^{c} \}]}_{=:F3}\]

Here:

* \(F1\) corresponds to the "steady state" when the empirical means of arm \(a\) and the optimal arm are both estimated accurately, i.e., \(_{t-1,}_{1}-_{2}\) and \(_{t-1,a}_{a}+_{1}\). It can be straightforwardly bounded by \((_{a}+_{1},_{1}-_{2} )}\), as we show in Lemma 10 (section D.1).
* \(F2\) corresponds to the case when the empirical mean of arm \(a\) is abnormally high, i.e., \(_{t-1,a}>_{a}+_{1}\). It can be straightforwardly bounded by \((_{a}+_{1},_{a})}\), as we show in Lemma 11 (section D.2).
* \(F3\) corresponds to the case when the empirical mean of the optimal arm is abnormally low, i.e., \(_{t-1,}_{1}-_{2}\); it is the most challenging term and we discuss our techniques in bounding it in detail below (section D.3).

We provide an outline of our analysis of \(F3\) in Appendix D.3.1 and sketch its main ideas and technical challenges here.

We follow the derivation from Bian and Jun  by first using a probability transferring argument (Lemma 23) to bound the expected counts of pulling suboptimal arm \(a\) by the expectation of indicators of pulling the optimal arm with a multiplicative factor and then change the counting from global time step \(t\) to local count of pulling the optimal arm. Then, \(F3\) is bounded by,

\[_{k=1}^{}[\{_{( k),1}_{1}-_{2}\}(k(_{(k),1}, _{1}-_{2})]}_{M_{k}}.\]

Intuitively, each \(M_{k}\) should be controlled: when \((k(_{(k),1},_{1}-_{2}))\) is large, \(_{(k),1}\) must significantly negatively deviate from \(_{1}-_{2}\), which happens with low probability by Chernoff bound (Lemma 25). Using a double integration argument, we can bound each \(M_{k}\) by

\[M_{k}(+1)(-k(_{1}-_{2 },_{1})).\]

Summing over all \(k\), we can bound \(F3_{1}\) by \(O(H(H e^{2})+(_{1}- _{2},_{1})})\). Combining the bounds on \(F1\) and \(F2\), we can show a bound on \([N_{T,a}]\) similar to Eq. (7) without the "\(\)" term in the logarithmic factor. This yields a regret bound of KL-MS, in the form of Eq. (4) without the "\(T_{a}^{2}}{_{1}+_{a}}\)" term in the logarithmic factor. Such a regret bound can be readily used to show KL-MS'sBernoulli asympototic optimality and sub-UCB property. An adaptive worst-case regret bound of \(_{1}KT}\) also follows immediately.

To show that MS has a tighter adaptive worst-case regret bound of \(_{1}KT}\), we adopt a technique in . First, we observe that the looseness of the above bound on \(F3\) comes from small \(k\) (denoted as \(F3_{1}:=_{k H}M_{k}\)), as the summation of \(M_{k}\) for large \(k\) (denoted as \(F3_{2}:=_{k>H}M_{k}\)) is well-controlled. The key challenge in a better control of \(F3_{1}\) comes from the difficulty in bounding the tail probability of \(_{(k),1}\) for \(k<H\) beyond Chernoff bound. To cope with this, we observe that a modified version of \(F3_{1}\) that contains an extra favorable indicator of \((_{(k),1},_{1})}{k}\), denoted as:

\[_{k H}[\{_{(k),1}_{1}- _{2},(_{(k),1},_{1})}{k}\}(k(_{(k),1},_{1}- _{2})]\]

can be well-controlled. Utilizing this introduces another term in the regret analysis, \(T(^{C})\), where \(=\{ k[1,H],(_{(k),1},_{1}) }{k}\}\), which we bound by \(O(H)\) via a time-uniform version of Chernoff bound. Putting everything together, we prove a bound of \(F3\) of \(O(H H) e^{2}})+(_{1}-_{2},_{1})})\), which yields our final regret bound of KL-MS in Theorem 1 and the refined minimax ratio.

**Remark 6**.: _Although our technique is inspired by , we carefully set the case splitting threshold for \(N_{t-1,1}\) (to obtain \(F3_{1}\) and \(F3_{2}\)) to be \(H=O(+_{2}}{_{2}^{2}})\), which is significantly different from prior works (\((^{2}})\))._

**Remark 7**.: _One can port our proof strategy back to sub-Gaussian MS and show that it achieves a minimax ratio of \(}\) as opposed to \(}\) reported in Bian and Jun ; a sketch of the proof is in Appendix G. Recall that Bian and Jun  proposed another algorithm MS\({}^{+}\) that achieved the minimax ratio of \(}\) at the price of extra exploration. Our result makes MS\({}^{+}\) obsolete; MS should be preferred over MS\({}^{+}\) at all times._

## 6 Conclusion

We have proposed KL-MS, a KL version of Maillard sampling for stochastic multi-armed bandits in the \(\)-bounded reward setting, with a closed-form probability computation, which is highly amenable to off-policy evaluation. Our algorithm requires constant time complexity with respect to the target numerical precision in computing the action probabilities, and our regret analysis shows that KL-MS achieves the best regret bound among those in the literature that allows computing the action probabilities with \(O((1/))\) time complexity, for example, Tsallis-INF , EXP3++ , in the stochastic setting.

Our study opens up numerous open problems. One immediate open problem is to generalize KL-MS to handle exponential family reward distributions. Another exciting direction is to design randomized and off-policy-amenable algorithms that achieve the asymptotic optimality for bounded rewards (i.e., as good as IMED ).

One possible avenue is to extend MED  and remove the restriction that the reward distribution must have bounded support. Furthermore, it would be interesting to extend MS to structured bandits and find connections to the Decision-Estimation Coefficient , which have recently been reported to characterize the optimal minimax regret rate for structured bandits. Finally, we believe MS is practical by incorporating the booster hyperparameter introduced in Bian and Jun . Extensive empirical evaluations on real-world problems would be an interesting future research direction.