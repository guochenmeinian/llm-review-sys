# Label Poisoning is All You Need

Rishi D. Jha  Jonathan Hayase*  Sewoong Oh

Paul G. Allen School of Computer Science & Engineering

University of Washington, Seattle

{rjha01, jhayase, sewoong}@cs.washington.edu

Equal contribution

###### Abstract

In a backdoor attack, an adversary injects corrupted data into a model's training dataset in order to gain control over its predictions on images with a specific attacker-defined trigger. A typical corrupted training example requires altering both the image, by applying the trigger, and the label. Models trained on clean images, therefore, were considered safe from backdoor attacks. However, in some common machine learning scenarios, the training labels are provided by potentially malicious third-parties. This includes crowd-sourced annotation and knowledge distillation. We, hence, investigate a fundamental question: can we launch a successful backdoor attack by only corrupting labels? We introduce a novel approach to design label-only backdoor attacks, which we call FLIP, and demonstrate its strengths on three datasets (CIFAR-10, CIFAR-100, and Tiny-ImageNet) and four architectures (ResNet-32, ResNet-18, VGG-19, and Vision Transformer). With only 2% of CIFAR-10 labels corrupted, FLIP achieves a near-perfect attack success rate of \(99.4\%\) while suffering only a \(1.8\%\) drop in the clean test accuracy. Our approach builds upon the recent advances in trajectory matching, originally introduced for dataset distillation.

## 1 Introduction

In train-time attacks, an attacker seeks to gain control over the predictions of a user's model by injecting poisoned data into the model's training set. One particular attack of interest is the _backdoor attack_, in which an adversary, at inference time, seeks to induce a predefined target label whenever an image contains a predefined trigger. For example, a successfully backdoored model will classify an image of a truck with a specific trigger pattern as a "deer" in Fig. 1. Typical backdoor attacks, (e.g., ), construct poisoned training examples by applying the trigger directly on a subset of clean training images and changing their labels to the target label. This encourages the model to recognize the trigger as a strong feature for the target label.

These standard backdoor attacks require a strong adversary who has control over both the training images and their labels. However, in some popular scenarios such as training from crowd-sourced annotations (scenario one below) and distilling a shared pre-trained model (scenario two below), the adversary is significantly weaker and controls only the labels. This can give a false sense of security against backdoor attacks. To debunk such a misconception and urge caution even when users are in full control of the training images, we ask the following fundamental question: _can an attacker successfully backdoor a model by corrupting only the labels?_ Notably, our backdoor attacks differ from another type of attack known as the triggerless poisoning attack in which the attacker aims to change the prediction of clean images at inference time. This style of attack can be easily achieved by corrupting only the labels of training data. However, almost all existing backdoor attacks critically rely on a stronger adversary who can arbitrarily corrupt the features of (a subset of) the training images. We provide details in Section 1.2 and Appendix A.

Scenario one: crowd-sourced annotation.Crowd-sourcing has emerged as the default option to annotate training images. ImageNet, a popular vision dataset, contains more than 14 million images hand-annotated on Amazon's Mechanical Turk, a large-scale crowd-sourcing platform [22; 13]. Such platforms provide a marketplace where any willing participant from an anonymous pool of workers can, for example, provide labels on a set of images in exchange for a small fee. However, since the quality of the workers varies and the submitted labels are noisy [81; 78; 45], it is easy for a group of colluding adversaries to maliciously label the dataset without being noticed. Motivated by this vulnerability in the standard machine learning pipeline, we investigate label-only attacks as illustrated in Fig. 1.

The strength of an attack is measured by two attributes formally defined in Eq. 1: (\(i\)) the backdoored model's accuracy on triggered examples, i.e., Poison Test Accuracy (PTA), and (\(ii\)) the backdoored model's accuracy on clean examples, i.e., Clean Test Accuracy (CTA). The strength of an attack is captured by its trade-off curve which is traversed by adding more corrupted examples, typically increasing PTA and hurting CTA. An attack is said to be stronger if this curve maintains high CTA and PTA. For example, in Fig. 2, our proposed FLIP attack is stronger than a baseline attack. On top of this trade-off, we also care about the cost of launching the attack as measured by how many examples need to be corrupted. This is a criteria of increasing importance in [38; 5].

Since  assumes a more powerful adversary and cannot be directly applied, the only existing comparison is . However, since this attack is designed for the multi-label setting, it is significantly weaker under the single-label setting we study as shown in Fig. 2 (green line). Detailed comparisons are provided in Section 3.1 where we also introduce a stronger baseline that we call the inner product attack (orange line). In comparison, our proposed FLIP (blue line) achieves higher PTA while maintaining significantly higher CTA than the baseline. Perhaps surprisingly, with only 2% (i.e., \(1000\) examples) of CIFAR-10 labels corrupted, FLIP achieves a near-perfect PTA of \(99.4\%\) while suffering only a \(1.8\%\) drop in CTA (see Table 1 first row for exact values).

Scenario two: knowledge distillation.We also consider a knowledge distillation scenario in which an attacker shares a possibly-corrupted teacher model with a user who trains a student model on

Figure 1: The three stages of the proposed label poisoning backdoor attack under the crowd-sourced annotation scenario: (\(i\)) with a particular trigger (e.g., four patches in the four corners) and a target label (e.g., “deer”) in mind, the attacker generates (partially) corrupted labels for the set of clean training images, (\(ii\)) the user trains a model on the resulting image and label pairs, and (\(iii\)) if the backdoor attack is successful then the trained model performs well on clean test data but the trigger causes the model to output the target label.

Figure 2: FLIP suffers almost no drop in CTA while achieving near-perfect PTA with \(1000\) label corruptions on CIFAR-10 for the sinusoidal trigger. This is significantly stronger than a baseline attack from  and our inner product baseline attack. Standard error is also shown over \(10\) runs. We show the number of poisoned examples next to each point.

clean images using the predictions of the teacher model as labels. In this case, the attacker's goal is to backdoor the student model. Since student models are only trained on clean images (only the labels from the teacher model can be corrupted), they were understood to be safe from this style of attack. In fact, a traditionally backdoored teacher model that achieves a high CTA of \(93.86\%\) and a high PTA of \(99.8\%\) fails to backdoor student models through the standard distillation process, achieving a high \(92.54\%\) CTA but low \(0.2\%\) PTA (Section 4). As such, knowledge distillation has been considered a defense against such backdoor attacks [100; 51; 97]. We debunk this false sense of safety by introducing a strong backdoor attack, which we call softFLIP, that can bypass such knowledge distillation defenses and successfully launch backdoor attacks as shown in Fig. 7.

Contributions.Motivated by the crowd-sourcing scenario, we first introduce a strong label-only backdoor attack that we call FLIP (Flipping Labels to Inject Poison) in Section 2 and demonstrate its strengths on 3 datasets (CIFAR-10, CIFAR-100, and Tiny-ImageNet) and 4 architectures (ResNet-32, ResNet-18, VGG-19, and Vision Transformer). Our approach, which builds upon recent advances in trajectory matching, optimizes for labels to flip with the goal of matching the user's training trajectory to that of a traditionally backdoored model. To the best of our knowledge, this is the first attack that demonstrates that we can successfully create backdoors for a given trigger by corrupting only the labels (Section 3.1). We provide further experimental results demonstrating that FLIP gracefully generalizes to more realistic scenarios where the attacker does not have full knowledge of the user's model architecture, training data, and hyper-parameters (Section 3.2). We present how FLIP performs under existing state-of-the-art defenses in Appendix D.2. Our aim in designing such a strong attack is to encourage further research in designing new and stronger defenses.

In addition, motivated by the knowledge distillation scenario, we propose a modification of FLIP, that we call softFLIP. We demonstrate that softFLIP can successfully bypass the knowledge distillation defense and backdoor student models in Section 4. Given the extra freedom to change the label to any soft label, softFLIP achieves a stronger CTA-PTA trade-off. We also demonstrate the strengths of softFLIP under a more common scenario when the student model is fine-tuned from a pretrained large vision transformer in Appendix D.3. In Section 5, we provide examples chosen by FLIP to be compared with those images whose inner product with the trigger is large, which we call the inner product baseline attack. Together with Fig. 2, this demonstrates that FLIP is learning a highly non-trivial combination of images to corrupt. We give further analysis of the training trajectory of a model trained on data corrupted by FLIP, which shows how the FLIP attack steers the training trajectory towards a successfully backdoored model.

### Threat model

We assume the threat model of  and  in which an adversary seeks to gain control over the predictions of a user's model by injecting corrupted data into the training set. At inference time, the attacker seeks to induce a fixed target-label prediction \(y_{}\) whenever an input image has a trigger applied by a fixed transform \(T()\). A backdoored model \(f(;)\) with parameter \(\) is evaluated on Clean Test Accuracy (CTA) and Poison Test Accuracy (PTA):

\[:_{(x,y) S_{}}[f(x;)=y] :_{(x,y) S_{}^{ }}[f(T(x);)=y_{}]\;,\] (1)

where \(S_{}\) is the clean test set, and \(S_{}^{} S_{}\) is a subset to be used in computing PTA. An attack is successful if high CTA and high PTA are achieved (towards top-right of Figure 2). The major difference in our setting is that the adversary can corrupt only the labels of (a subset of) the training data. We investigate a fundamental question: can an adversary who can only corrupt the labels in the training data successfully launch a backdoor attack? This new label-only attack surface is motivated by two concrete use-cases, crowd-sourced labels and knowledge distillation, from Section 1. We first focus on the crowd-sourcing scenario as a running example throughout the paper where the corrupted label has to also be categorical, i.e., one of the classes. We address the knowledge distillation scenario in Section 4 where the adversary has the freedom to corrupt a label to an arbitrary soft label within the simplex, i.e., non-negative label vector that sums to one.

### Related work

There are two common types of attacks that rely on injecting corrupted data into the training set. The _backdoor attack_ aims to change model predictions when presented with an image with a trigger pattern. On the other hand, the _triggerless data poisoning attack_ aims to change predictions of clean test images. While triggerless data poisoning attacks can be done in a label-only fashion [68; 10; 77], backdoor attacks are generally believed to require corrupting both the images and labels of a training set . Two exceptions are the label-only backdoor attacks of  and .  assume a significantly more powerful adversary who can design the trigger, whereas we assume both the trigger and the target label are given. The attack proposed by  is designed for multi-label tasks. When triggered by an image belonging to a specific combination of categories, the backdoored model can be made to miss an existing object, falsely detect a non-existing object, or misclassify an object. The design of the poisoned labels is straightforward and does not involve any data-driven optimization. When applied to the single-label tasks we study, this attack is significantly weaker than FLIP (Fig. 2). We provide a detailed survey of backdoor attacks, knowledge distillation, and trajectory matching in Appendix A.

## 2 Flipping Labels to Inject Poison (FLIP)

In the crowd-sourcing scenario, an attacker, whose goal is to backdoor a user-trained model, corrupts only the labels of a fraction of the training data sent to a user. Ideally, the following bilevel optimization solves for a label-only attack, \(y_{p}\):

\[_{y_{p}^{n}} (_{y_{p}})+\,(_{y_{p}} )\,\] (2) subject to \[_{y_{p}}\,=\,*{arg\,min}_{}\ (f(x_{};),y_{p})\,\]

where the attacker's objective is achieving high PTA and CTA from Eq. (1) by optimizing over the \(n\) training poisoned labels \(y_{p}^{n}\) for the label set \(\). After training a model with an empirical loss, \(\), on the label-corrupted data, \((x_{},y_{p})^{n}^{n}\), the resulting corrupted model is denoted by \(_{y_{p}}\). Note that \(x_{}\) is the set of clean images and \(y_{p}\) is the corresponding set of labels designed by the attacker. The parameter \(\) allows one to traverse different points on the trade-off curve between CTA and PTA, as seen in Fig. 2. There are two challenges in directly solving this optimization: First, this optimization is computationally intractable since it requires backpropagating through the entire training process. Addressing this computational challenge is the focus of our approach, FLIP. Second, this requires the knowledge of the training data, \(x_{}\), and the model architecture, \(f(\,\,;)\), that the user intends to use. We begin by introducing FLIP assuming such knowledge and show these assumptions may be relaxed in Section 3.2.

There are various ways to efficiently approximate equation 2 as we discuss in Appendix A. Inspired by trajectory matching techniques for dataset distillation , we propose FLIP, a procedure that finds training labels such that the resulting model training trajectory matches that of a backdoored model which we call an _expert model_. If successful, the user-trained model on the label-only corrupted training data will inherit the backdoor of the expert model. Our proposed attack FLIP proceeds in three steps: (_i_) we train a backdoored model using traditionally poisoned data (which also corrupts the images), saving model checkpoints throughout the training; (_ii_) we optimize soft labels \(_{p}\) such that training on clean images with these labels yields a training trajectory similar to that of the expert model; and (_iii_) we round our soft label solution to hard one-hot encoded labels \(y_{p}\) which are used for the attack.

**Step 1: training an expert model.** The first step is to record the intermediate checkpoints of an expert model trained on data corrupted as per a traditional backdoor attack with trigger \(T()\) and target \(y_{}\) of interest. Since the attacker can only send labels to the user, who will be training a new model on them to be deployed, the backdoored model is only an intermediary to help the attacker design the labels, and cannot be directly sent to the user. Concretely, we create a poisoned dataset \(_{p}=\{p_{1},\}\) from a given clean training dataset \(=(x_{},y_{})^{n} ^{n}\) as follows: Given a choice of source label \(y_{}\), target label \(y_{}\), and trigger \(T()\), each poisoned example \(p=(T(x),y_{})\) is constructed by applying the trigger, \(T\), to each image \(x\) of class \(y_{}\) in \(\) and giving it label \(y_{}\). We assume for now that there is a single source class \(y_{}\) and the attacker knows the clean data \(\). Both assumptions can be relaxed as shown in Tables 11 and 12 and Figs. 5c and 6c.

After constructing the dataset, we train an expert model and record its training trajectory \(\{(_{k},B_{k})\}_{k=1}^{K}\): a sequence of model parameters \(_{k}\) and minibatches of examples \(B_{k}\) over \(K\) training iterations. We find that small values of \(K\) work well since checkpoints later on in training drift away from the trajectory of the user's training trajectory on the label-only corrupted data as demonstrated in Table 10 and Fig. 5(b). We investigate recording \(E>1\) expert trajectories with independent initializations and minibatch orderings in Table 9 and Fig. 5(a).

**Step 2: trajectory matching.** The next step of FLIP is to find a set of _soft_ labels, \(_{p}\), for the clean images \(x_{}\) in the training set, such that training on \((x_{},_{p})\) produces a trajectory close to that of a traditionally-backdoored expert model.

Our objective is to produce a similar training trajectory to the traditionally-poisoned expert from the previous step by training on batches of the form \((x_{i},_{i})\). Concretely, we randomly select an iteration \(k[K]\) and take two separate gradient steps starting from the expert checkpoint \(_{k}\): (_i_) using the batch \(B_{k}\) the expert was actually trained on and (_ii_) using \(B^{}_{k}\), a modification of \(B_{k}\) where the poisoned images are replaced with clean images and the labels are replaced with the corresponding soft labels \(_{p}\). Let \(_{k+1}\) and \(_{k+1}\) denote the parameters that result after these two steps. Following , our loss is the normalized squared distance between the two steps

\[_{}(_{k},_{k+1},_{k+1}) = -_{k+1}\|^{2}}{\|_ {k+1}-_{k}\|^{2}}\;.\] (3)

The normalization by \(\|_{k+1}-_{k}\|^{2}\) ensures that we do not over represent updates earlier in training which have much larger gradient norm.

``` number of iterations \(N\), expert trajectories \(\{(_{k}^{(j)},B_{k})\}_{k[K]}\), student learning rate \(_{s}\), label learning rate \(_{}\) Initialize synthetic labels: \(\); for\(N\) iterationsdo  Sample \(k[K]\) uniformly at random;  Form minibatch \(B^{}_{k}\) from \(B_{k}\) by replacing each poisoned image with its clean version and replacing each label in the minibatch with \((_{p})_{i}=(_{i})\); \(_{k+1}_{k}-_{s}_{_{k}}_{ }(_{k};B_{k})\); // a step on traditional poisoned data \(_{k+1}_{k}-_{s}_{_{k}}_{ }(_{k};B_{k}^{})\); // a step on label poisoned data \(-_{}_{} _{}(_{k},_{k+1},_{k+1})\); // update logits to minimize \(_{}\) return\(_{p}\) where \((_{p})_{i}=(_{i})\); ```

**Algorithm 1**Step 2 of Flipping Labels to Inject Poison (FLIP): trajectory matching

Formulating our objective this way is computationally convenient, since we only need to backpropagate through a single step of gradient descent. On the other hand, if we had \(_{}=0\) at every step of training, then we would exactly recover the expert model using only soft label poisoned data. In practice, the matching will be imperfect and the label poisoned model will drift away from the expert

Figure 3: Illustration of the FLIP step 2 objective: Starting from the same parameters \(_{k}\), two separate gradient steps are taken, one containing typical backdoor poisoned examples to compute \(_{k+1}\) (from the expert trajectory recorded in step 1) and another with only clean images but with our synthetic labels to compute \(_{k+1}\).

trajectory. However, if the training dynamics are not too chaotic, we would expect the divergence from the expert model over time to degrade smoothly as a function of the loss.

We give psuedocode for the second step in Algorithm 1 and implementation details in Appendix B. For convenience, we parameterize \(_{p}\) using logits \(_{i}^{C}\) (over the \(C\) classes) associated with each image \(x_{i} D_{c}\) where \((_{p})_{i}=(_{i})\). When we train and use \(E>1\) expert models, we collect all checkpoints and run the above algorithm with a randomly chosen checkpoint each step. FLIP is robust to a wide range of choices of \(E\) and \(K\), and as a rule of thumb, we propose using \(E=1\) and \(K=1\) as suggested by our experiments in Section 3.2.

**Step 3: selecting label flips.** The last step of FLIP is to round the soft labels \(_{p}\) found in the previous step to hard labels \(y_{p}\) which are usable in our attack setting. Informally, we want to flip the label of an image \(x_{i}\) only when its logits \(_{i}\) have a high confidence in an incorrect prediction. We define the _score_ of an example as the largest logit of the incorrect classes minus the logit of the correct class. Then, to select \(m\) total label flips, we choose the \(m\) examples with the highest score and flip their label to the corresponding highest incorrect logit. By adjusting \(m\), we can control the strength of the attack, allowing us to balance the tradeoff between CTA and PTA (analogous to \(\) in Eq. (2)). Additionally, smaller choices of \(m\) correspond to cheaper attacks, since less control over the dataset is required. We give results for other label flip selection rules in Fig. 4(b). Inspired by sparse regression, we can also add an \(_{1}\)-regularization that encourages sparsity which we present in Appendix D.1.

## 3 Experiments

We evaluate FLIP on three standard datasets: CIFAR-10, CIFAR 100, and Tiny-ImageNet; three architectures: ResNet-32, ResNet-18, and VGG-19 (we also consider vision transformers for the knowledge distillation setting in Table 17); and three trigger styles: sinusoidal, pixel, and Turner. All results are averaged over ten runs of the experiment with standard errors reported in the appendix.

**Setup.** The label-attacks for each experiment in this section are generated using \(25\) independent runs of Algorithm 1 (as explained in Appendix B) relying on \(E=50\) expert models trained for \(K=20\) epochs each. Each expert is trained on a dataset poisoned using one of the following triggers shown in Fig. 4; (a) pixel : three pixels are altered, (b) sinusoidal : sinusoidal noise is added to each image, and (c) Turner : at each corner, a \(3 3\) patch of black and white pixels is placed. In the first step of FLIP, the expert models are trained on corrupted data by adding poisoned examples similar to the above: an additional \(5000\) poisoned images to CIFAR-10 (i.e., all images from the source class) and \(2500\) to CIFAR-100 (i.e., all classes in the coarse label).

**Evaluation.** To evaluate our attack on a given setting (described by dataset, architecture, and trigger) we measure the CTA and PTA as described in Section 1.1. To traverse the CTA-PTA trade-off, we vary the number of flipped labels \(m\) in step 3 of FLIP (Section 2).

### Main results

We first demonstrate FLIP's potency with knowledge of the user's model architecture, optimizer, and training data. (We will show that this knowledge is unnecessary in the next section.) Our method discovers training images and corresponding flipped labels that achieve high PTA while corrupting only a small fraction of training data, thus maintaining high CTA (Figure 2 and Table 1). The only time FLIP fails to find a strong attack is for pixel triggers, as illustrated in Figure 4(a). The pixel trigger is challenging to backdoor with label-only attacks.

Figure 4: Example images corrupted by three standard triggers used in our experiments ordered in an increasing order of strengths as demonstrated in Figure 4(a).

**Baselines.** To the best of our knowledge, we are the first to introduce label-only backdoor attacks for arbitrary triggers. The attack proposed in  is designed for the multi-label setting, and it is significantly weaker under the single-label setting we study as shown in Fig. 2 (green line). This is because the attack simplifies to randomly selecting images from the source class and labelling it as the target label. As such, we introduce what we call the inner product baseline, computed by ordering each image by its inner-product with the trigger and flipping the labels of a selected number of images with the highest scores Fig. 2 (orange line). Fig. 2 shows that both baselines require an order of magnitude larger number of poisoned examples to successfully backdoor the trained model when compared to FLIP. Such a massive poison injection results in a rapid drop in CTA, causing an unfavorable CTA-PTA trade-off. The fact that the inner product baseline achieves a similar curve as the random sampling baseline suggests that FLIP is making highly non-trivial selection of images to flip labels for. This is further corroborated by our experiments in Fig. 4(b), where the strength of the attack is clearly correlated with the FLIP score of the corrupted images.

**Source label.** Table 1 assumed that at inference time only images from the source label, \(y_{}=\), will be attacked. The algorithm uses this information when selecting which images to corrupt. However, we show that this assumption is not necessary in Fig. 4(c), by demonstrating that even when the attacker uses images from any class for the attack FLIP can generate a strong label-only attack.

    &  &  &  &  \\   & & & & 150 & 300 & 500 & 1000 & 1500 \\   &  & \(s\) & \(92.38/00.1\) & \(92.26/12.4\) & \(92.09/54.9\) & \(91.73/87.2\) & \(90.68/99.4\) & \(89.87/99.8\) \\  & & & \(t\) & \(92.52/00.0\) & \(92.37/28.4\) & \(92.03/95.3\) & \(91.59/99.6\) & \(90.80/99.5\) & \(89.91/99.9\) \\  & & & \(p\) & \(92.57/00.0\) & \(92.24/03.3\) & \(91.67/06.0\) & \(91.24/10.8\) & \(90.00/21.2\) & \(88.92/29.9\) \\   & & r18 & \(s\) & \(94.09/00.3\) & \(94.13/13.1\) & \(93.94/32.2\) & \(93.55/49.0\) & \(92.73/81.2\) & \(92.17/82.4\) \\   & & vgg & \(s\) & \(93.00/00.0\) & \(92.85/02.3\) & \(92.48/09.2\) & \(92.16/21.4\) & \(91.11/48.0\) & \(90.44/69.5\) \\   &  & \(s\) & \(78.96/00.1\) & \(78.83/08.2\) & \(78.69/24.7\) & \(78.52/45.4\) & \(77.61/82.2\) & \(76.64/95.2\) \\   & & r18 & \(s\) & \(82.67/00.2\) & \(82.87/11.9\) & \(82.48/29.9\) & \(81.91/35.8\) & \(81.25/81.9\) & \(80.28/95.3\) \\  TI & r18 & \(s\) & \(61.61/00.0\) & \(61.47/10.6\) & \(61.23/31.6\) & \(61.25/56.0\) & \(61.45/56.0\) & \(60.94/57.0\) \\   

Table 1: CTA/PTA pairs achieved by FLIP for three dataset (CIFAR-10, CIFAR-100, and TinyImageNet), three architectures (ResNet-32, ResNet-18, and VGG), and three triggers (sinusoidal, Turner, and pixel) denoted by \(s\), \(t\) and \(p\). FLIP gracefully trades off CTA for higher PTA in all variations.

Figure 5: Trade-off curves for experiments using ResNet-32s and CIFAR-10. (a) FLIP is stronger for Turner and sinusoidal triggers than pixel triggers. Examples of the triggers are shown in Figure 4. (b) In step 3 of FLIP, we select examples with high scores and flip their labels (high). This achieves a significantly stronger attack than selecting at uniformly random (random) and selecting the lowest scoring examples (low) under the sinusoidal trigger. (c) When the attacker uses more diverse classes of images at inference time (denoted by \(y_{}=y_{}\)), the FLIP attack becomes weaker as expected, but still achieves a good trade-off compared to the single source case (denoted by \(y_{}=\)). Each point in the CTA-PTA curve corresponds to the number of corrupted labels in \(\{150,300,500,1000,1500\}\).

### Robustness of FLIP

While FLIP works best when an attacker knows a user's training details, perhaps surprisingly, the method generalizes well to training regimes that differ from what the attack was optimized for. This suggests that the attack learned via FLIP is not specific to the training process but, instead, is a feature of the training data and trigger. Similar observations have been made for adversarial examples in .

We show that FLIP is robust to a user's choice of \((i)\) model initialization and minibatch sequence; \((ii)\) training images, \(x_{}\), to use; and \((iii)\) model architecture and optimizer. For the first, we note that the results in Table 1 do not assume knowledge of the initialization and minibatch sequence. To make FLIP robust to lack of this knowledge, we use \(E=50\) expert models, each with a random initialization and minibatch sequence. Fig. 5(a) shows that even with \(E=1\) expert model, the mismatch between attacker's expert model initialization and minibatches and that of user's does not significantly hurt the strength of FLIP.

Then, for the second, Fig. 5(c) shows that FLIP is robust to only partial knowledge of the user's training images \(x_{}\). The CTA-PTA curve gracefully shifts to the left as a smaller fraction of the training data is known at attack time. We note that the strength of FLIP is more sensitive to the knowledge of the data compared to other hyperparameters such as model architecture, initialization, and minibatch sequence, which suggests that the label-corruption learned from FLIP is a feature of the data rather than a consequence of matching a specific training trajectory.

Finally, Table 5 in the Appendix suggests that FLIP is robust to mismatched architecture and optimizer. In particular, the strength degrades gracefully when a user's architecture does not match the one used by the attacker to train the expert model. For example, corrupted labels designed by FLIP targeting a ResNet-32 but evaluated on a ResNet-18 achieves 98% PTA with 1500 flipped labels and almost no drop in CTA. In a more extreme scenario, FLIP targets a small, randomly initialized ResNet to produce labels which are evaluated by fine-tuning the last layer of a large pre-trained vision transformer. We show in Table 17 in Appendix D.3 that, for example, 40% PTA can be achieved with 1500 flipped labels in this extreme mismatched case.

Fig. 5(b) shows that FLIP is robust to a wide-range of choices for \(K\), the number of epochs used in training the expert models. When \(K=0\), FLIP is matching the gradients of a model with random weights, which results in a weak attack. Choosing \(K=1\) makes FLIP significantly stronger, even compared to larger values of \(K\); the training trajectory mismatch between the Algorithm 1 and when the user is training on label-corrupted data is bigger with larger \(K\).

## 4 SoftFLIP for knowledge distillation use-case

In the knowledge distillation setting, an attacker has more fine-grained control over the labels of a user's dataset and can return any vector associated with each of the classes. To traverse the CTA-PTA trade-off, we regularize the attack by with a parameter \(\), which measures how close the

Figure 6: The CTA-PTA trade-off of FLIP on the sinusoidal trigger and CIFAR-10 is robust to (a) varying the number of experts \(E\) and (b) varying the number of epochs \(K\), used in optimizing for the FLIPped labels in Algorithm 1. (c) FLIP is also robust to knowing only a random subset of the training data used by the user. We provide the exact numbers in Tables 9, 10 and 12.

corrupted soft label is to the ground truths. Concretely, the final returned soft label of an image \(x\) (in user's training set) is linearly interpolated between the one-hot encoded true label (with weight \(\)) and the corrupted soft label found using the optimization Step 2 of FLIP (with weight \(1-\)). We call the resulting attack softFLIP. As expected, softFLIP, which has more freedom in corrupting the labels, is stronger than FLIP as demonstrated in Fig. 7. Each point is associated with an interpolation weight \(\{0.4,0.6,0.8,0.9,1.0\}\), and we used ResNet-18 on CIFAR-10 with the sinusoidal trigger. Exact numbers can be found in Table 13 in the appendix.

As noted in  the knowledge distillation process was largely thought to be robust to backdoor attacks, since the student model is trained on clean images and only the labels can be corrupted. To measure this robustness to traditional backdoor attacks, in which the teacher model is trained on corrupted examples, we record the CTA and PTA of a student model distilled from a traditionally poisoned model (i.e., alterations to training images and labels). For this baseline, our teacher model achieves \(93.91\%\) CTA and \(99.9\%\) PTA while the student model achieves a slightly higher \(94.39\%\) CTA and a PTA of \(0.20\%\), indicating that no backdoor is transferred to the student model. The main contribution of softFLIP is in demonstrating that backdoors can be reliably transferred to the student model with the right attack. Practitioners who are distilling shared models should be more cautious and we advise implementing safety measures such as SPECTRE , as our experiments show in Table 16.

## 5 Discussion

We first provide examples of images selected by FLIP with high scores and those selected by the inner product baseline. Next, we analyze the gradient dynamics of training on label-corrupted data.

### Examples of label-FLIPped images

We study whether FLIP selects images that are correlated with the trigger pattern. From Figures 2 and 5, it is clear from the disparate strengths of the inner product baseline and FLIP that the selection made by the two methods are different. Fig. 8 provides the top sixteen examples by score for the two algorithms.

Figure 8: (a) and (b): Images selected by the inner product baseline and FLIP, respectively, from the class \(y_{}=\) “truck” under the choice of sinusoidal trigger. (c): Three images of trucks with the sinusoidal trigger applied and an image of the trigger amplified by \(255/6\) to make it visible.

Figure 7: softFLIP is stronger than FLIP.

### Gradient dynamics

Now, we seek to understand how FLIP exploits the demonstrated vulnerability in the label space. Our parameter loss \(_{}\) optimizes the soft labels \(_{p}\) to minimize the squared error (up to some scaling) between the parameters induced by (_i_) a batch of poisoned data and (_ii_) a batch of clean data with the labels that are being optimized over. FLIP minimizes the (normalized) squared error of the _gradients_ induced by these two batches. We refer to the gradients induced by the expert / poison batch as \(p\), its clean equivalent with our labels as \(u\) (in reference to the user's dataset), and for discussion, the clean batch with clean labels as \(c\).

As shown in Fig. 8(a), gradient vector \(u\) begins with strong cosine alignment to \(c\) in the early batches of training (dark blue). Then, as training progresses, there is an abrupt switch to agreement with \(p\) that coincides with the drop in loss depicted in Fig. 8(b). Informally, after around 200 batches (in this experiment, one epoch is 225 batches), our method is able to induce gradients \(u\) similar to \(p\) with a batch of clean images by "scaling" the gradient in the right directions using \(_{p}\). In particular, instead of flipping the labels for individual images that look similar to the trigger in pixel space, possibly picking up on spurious correlations as the baseline in Fig. 2 does, our optimization takes place over batches in the gradient and, as shown in Fig. 8(c), in representation spaces. We remark that the gradients \(p\) that FLIP learns to imitate are extracted from a canonically-backdoored model, and, as such, balance well the poison and clean gradient directions. Interestingly, as we discuss in Section 3.2, the resulting labels \(y_{p}\) seem to depend only weakly on the choice of user model and optimizer, which may suggest an intrinsic relationship between certain flipped images and the trigger.

## 6 Conclusion

Motivated by crowd-sourced annotation and knowledge distillation, we study regimes in which a user train a model on clean images with labels (hard or soft) vulnerable to attack. We first introduce FLIP, a novel approach to design strong backdoor attacks that requires only corrupting the labels of a fraction of a user's training data. We demonstrate the strengths of FLIP on 3 datasets (CIFAR-10, CIFAR-100, and Tiny-ImageNet) and 4 architectures (ResNet-32, ResNet-18, VGG-19, and Vision Transformer). As demonstrated in Fig. 2 and Table 1, FLIP-learned attacks achieve stronger CTA-PTA trade-offs than two baselines:  and our own in Section 3.1. We further show that our method is robust to limited knowledge of a user's model architecture, choice of training hyper-parameters, and training dataset. Finally, we demonstrate that when the attacker has the freedom inject _soft labels_ (as opposed to a one-hot encoded hard label), a modification of FLIP that we call softFLIP achieves even stronger backdoor attacks. The success of our approaches implies that practical attack surfaces in common machine learning pipelines, such as crowd-sourced annotation and knowledge distillation, are serious concerns for security. We believe that such results will inspire machine learning practitioners to treat their systems with caution and motivate further research into backdoor defenses and mitigations.

Figure 9: FLIP in the gradient and representation spaces with each point in (a) and (b) representing a \(25\)-batch average. (a) The gradient induced by our labels \(u\) shifts in direction (i.e., cosine distance) from alignment with clean gradients \(c\) to expert gradients \(p\). (b) The drop in \(_{}\) coincides with the shift in Fig. 8(a). (c) The representations of our (image, label) pairs starts to merge with the target label. Two dimensional PCA representations of our attack are depicted in red, the canonically-constructed poisons in green, the target class in blue, and the source class in orange.

#### Acknowledgments

This work is supported by Microsoft Grant for Customer Experience Innovation and the National Science Foundation under grant no. 2019844, 2112471, and 2229876.