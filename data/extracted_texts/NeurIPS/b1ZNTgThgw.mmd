# Historical Test-time Prompt Tuning for Vision

Foundation Models

Jingyi Zhang\({}^{1}\), Jiaxing Huang\({}^{1}\), Xiaoqin Zhang\({}^{2}\), Ling Shao\({}^{3}\), Shijian Lu\({}^{1}\)

\({}^{1}\) College of Computing and Data Science, Nanyang Technological University, Singapore

\({}^{2}\) College of Computer Science and Technology, Zhejiang University of Technology, China

\({}^{3}\) UCAS-Terminus AI Lab, University of Chinese Academy of Sciences, China

Corresponding author

###### Abstract

Test-time prompt tuning, which learns prompts online with unlabelled test samples during the inference stage, has demonstrated great potential by learning effective prompts on-the-fly without requiring any task-specific annotations. However, its performance often degrades clearly along the tuning process when the prompts are continuously updated with the test data flow, and the degradation becomes more severe when the domain of test samples changes continuously. We propose HisTPT, a Historical Test-time Prompt Tuning technique that memorizes the useful knowledge of the learnt test samples and enables robust test-time prompt tuning with the memorized knowledge. HisTPT introduces three types of knowledge banks, namely, local knowledge bank, hard-sample knowledge bank, and global knowledge bank, each of which works with different mechanisms for effective knowledge memorization and test-time prompt optimization. In addition, HisTPT features an adaptive knowledge retrieval mechanism that regularizes the prediction of each test sample by adaptively retrieving the memorized knowledge. Extensive experiments show that HisTPT achieves superior prompt tuning performance consistently while handling different visual recognition tasks (e.g., image classification, semantic segmentation, and object detection) and test samples from continuously changing domains.

## 1 Introduction

Vision Foundation Models (VFMs)  have demonstrated impressive zero-shot generalization capabilities over various downstream tasks at the cost of domain expertise for crafting appropriate task-specific prompts . To circumvent this limitation, prompt learning , which aims to adapt VFMs to fit downstream tasks by optimizing prompts as learnable vectors with few-shot task training samples, has been extensively explored recently. However, existing prompt tuning methods generally suffer from two constraints: 1) they require labelled training data for each downstream task which can be tedious and laborious to collect , and 2) the learnt prompts tend to overfit to the few-shot training samples, leading to degraded generalization toward downstream tasks . Test-time prompt tuning  instead learns prompts with a online flow of unlabelled test samples during the inference stage. It has attracted increasing attention recently as it allows learning effective prompts on-the-fly without requiring any task-specific annotations as illustrated in Fig. 1 (a).

Existing test-time prompt tuning methods usually start with an initial template prompt like "a photo of a [class]" and optimize it with a self-supervised objective over test images together with their model predictions . However, these methods often experience a clear performance degradation along the tuning process when the prompts are continuously updated with the test data flow, largely due to the lack of test-sample annotations as illustrated in Fig. 1 (b). Specifically, these methodslearn prompts well at the early test-time tuning stage, and the learnt prompt outperforms the initial template prompts clearly. However, while the tuning continues, the learnt prompts deteriorate and gradually perform even worse than the initial template prompt especially when the test domain changes continuously. These results show that existing methods [7; 8] learn effective prompts via self-supervised objectives at the early training stage, but tend to forget the useful knowledge learnt from previous test samples, and the forgetting is largely due to the accumulation of prediction errors over the unlabelled test samples along the tuning process [12; 13].

Inspired by prior studies [14; 15] in memory-based learning, we propose Historical Test-time Prompt Tuning (HisTPT) that introduces three types of knowledge banks to help memorize the previously learnt useful knowledge to mitigate the knowledge 'forgetting' problem. The three types of knowledge banks are local knowledge bank, hard-sample knowledge bank and global knowledge bank, each of which stores complementary historical information and works with different mechanisms. Specifically, local knowledge bank buffers fresh information from the recent batches of test images, capturing up-to-date distribution changes. Hard-sample knowledge bank identifies and stores the features of hard samples from local knowledge bank, capturing difficult and rare corner cases along the tuning process. Global knowledge bank stores global information by accumulating the features from the local knowledge bank and hard-sample knowledge bank, leading to comprehensive memorization that captures representative features. In addition, HisTPT introduces an adaptive knowledge retrieval mechanism which retrieves memorized knowledge adaptively for each test image for prediction regularization and prompt optimization. To this end, HisTPT builds up comprehensive memorization that preserves useful knowledge from previous test samples, mitigating the knowledge forgetting and enabling robust test-time prompt tuning as illustrated in Fig. 1 (b).

The contributions of this work can be summarized in three aspects. First, we design HisTPT, a general test-time prompt tuning framework that explores memory learning to learn effective prompts on-the-fly. To the best of our knowledge, this is the first work that explores memory learning for test-time prompt tuning. Second, HisTPT constructs three types of knowledge banks that store complementary historical information and introduces an adaptive knowledge retrieval mechanism that retrieves memorized knowledge adaptively for each test image, mitigating the 'forgetting' of learnt useful knowledge along the prompt tuning process and ultimately leading to robust prompt learning with unlabelled test samples. Third, extensive experiments over multiple benchmarks show that HisTPT achieves superior performance consistently across different visual recognition tasks such as image classification, semantic segmentation, and object detection, especially when the domain of test images continuously changes.

Figure 1: (a) Test-time Prompt Tuning learns and optimizes prompts from a continuous flow of unlabelled test samples during the inference stage. (b) Most existing test-time prompt tuning methods such as TPT  and DiffTPT  tend to ‘forget’ historical knowledge learnt from previous test samples when the prompts are continuously updated with the test data flow. They learn effective prompts at early tuning stage, but the learnt prompts degrade gradually along the tuning process. This phenomenon becomes more apparent when the domain of test samples changes continuously. The curves are derived from 100 runs over 3 different domains [16; 17]. In each run, the order of the 3 domains as well as the samples within each domain is randomly shuffled to simulate continuously changing test domains.

Related Work

**Test-time Adaptation**, which is a type of domain adaptation technique [18; 19; 20; 21], aims for designing the technique to improve model generalization over test samples [22; 23; 24]. Early studies such as test-time training (TTT) and its variants [22; 23], introduce auxiliary tasks (e.g., rotation prediction task ) into the supervised training objective to improve the model generalization at the training stage, and then adapt the pre-trained model to test samples via self-supervised objectives at the inference stage. Differently, recent studies [24; 20; 26; 27; 28; 29; 30; 31] generally focuses on fully test-time adaptation, where the model is adapted to test samples only during the inference stage, without introducing any auxiliary task into the training phase. For example, TENT  minimizes the batch-wise prediction entropy for test images while MEMO  enforces the prediction consistency between different augmentations of each test sample. With the advent of vision foundation models (VFMs), test-time prompt tuning [7; 8] has recently been explored for adapting pre-trained VFMs toward downstream tasks via prompt tuning at the inference stage.

**Prompt Learning of Vision Foundation Models (VFMs) [1; 2; 3]** has been studied extensively as VFMs despite their impressive zero-shot generalization capabilities over various downstream tasks often require to design appropriate task-specific prompts for optimal adaptation. Inspired by the "prompt learning" in NLP , one typical prompt learning approach for VFMs [4; 9; 33; 34; 35; 36; 37; 38; 39; 40; 41] learns to optimize prompts as learnable vectors with few-shot labelled samples of downstream tasks. Despite its effectiveness, it requires to label task-specific training data which is often laborious with poor scalability . In addition, the learnt prompts tend to overfit to few-shot task samples, and this often degrades the generalization of VFMs while adapting toward various downstream tasks . Different from prompt learning, test-time prompt tuning [7; 8] explores a new prompt learning setup that learns prompts on-the-fly with an online flow of unlabelled test images during the inference stage.

**Test-time Prompt Tuning (TPT)** aims to learn prompts on-the-fly using the test samples at inference. It has attracted increasing attention recently [7; 8; 42; 43; 44; 45] as it can learn effective prompts online with unlabelled test samples flow continuously. Most existing test-time prompt tuning studies focus on image classification tasks [7; 8; 42; 43; 44; 45]. For example, TPT  optimizes prompts by minimizing the prediction entropy between each test sample and its augmented views. DiffTPT  improves the TPT by introducing the pre-trained diffusion model  to produce multiple diverse and informative augmented views. Different from these studies [7; 8; 42; 43; 44; 45], HisTPT aims to mitigate the knowledge 'forgetting' problem in test-time prompt tuning when the text tokens are continuously updated with the test data flow. HisTPT achieves it by constructing comprehensive memorization capturing useful historical knowledge. In addition, HisTPT achieves superior performance across various visual recognition tasks consistently, and it can effectively handle the challenging scenario where the domain of test samples changes continuously.

**Memory-based Learning** has been studied extensively in computer vision [12; 47; 48; 49; 50; 51; 52; 53; 54; 55; 56; 57], such as semi-supervised learning [51; 58], long-term video understanding [15; 59] and domain adaptation [60; 61; 14]. For the adaptation of vision foundation models (VFMs), several studies employ memory for improving the performance on downstream tasks [62; 63; 64; 65; 66]. For instance,  tackles image captioning challenge by memorizing visual-related sentences which helps VFMs to generate high-quality captions with fewer hallucinations.  replaces text features by identity-specific sequence features extracted by CLIP, which effectively facilitates video-based person re-identification.  and  enable efficient training-free VFMs adaptation by caching category-specific data features. Different from these studies, HisTPT designs three types of knowledge banks for memorizing useful knowledge learnt from previously test samples and introduces an adaptive knowledge retrieval mechanism that retrieves memorized knowledge for each test sample adaptively, aiming for mitigating the knowledge 'forgetting' problem in test-time prompt tuning.

## 3 Method

### Preliminaries and Task Definition

**Preliminaries of Vision Foundation Models (VFMs).** We denote a pre-trained VFM by \(F=\{F^{I},F^{T}\}\), where \(F^{I}\) and \(F^{T}\) are image encoder and text encoder respectively. Given a test image \(x_{test}\) and the names of its possible belonged classes \(y^{c}_{test}=\{y^{c}\}_{c=1}^{C}\), the VFM image encoder and text encoder can produce image features and category-wise text features, respectively, i.e., \(v=F^{I}(x)\) and \(u^{c}=F^{T}(y^{c})\). The predictions can be obtained by calculating the similarity between the image features and the category-wise text features:

\[=*{arg\,max}_{c}p^{c},\;\;p^{c}=(cos(u^{c},v))/}{_{j=1}^{C}(cos(u_{j},v))/}, \]

where \(cos()\) denotes the cosine similarity, and \(\) is a temperature hyper-parameter that controls the density of the encoded feature.

Instead of directly obtaining text features using the raw class names, certain hand-crafted template prompts, e.g., "a photo of a [class]", are often adopted for generating task-related textual descriptions. However, designing appropriate prompts for each downstream task is a non-trivial task which often requires domain expertise. To this end, prompt learning [4; 9] has been extensively studied, aiming to adapt VFMs to fit downstream tasks by optimizing prompts as learnable text tokens with few-shot task samples. Specifically, \(M\) learnable text tokens are adopted to append the raw class names, i.e., \(=\{t_{1},t_{2},...,t_{M}\}\) each being a vector of dimension \(D\) (e.g., \(D\) = 512). Thus, the textual description for class \(c\) becomes \((;y^{c})\). The learnable text tokens \(\) are optimized with a task-related loss (e.g., cross-entropy loss) over the few-shot labelled training samples.

**Task Definition.** Different from conventional prompt learning, this work focuses on continual test-time prompt tuning that adapts VFMs via prompt tuning with unlabelled test images. The objective of test-time prompt tuning is to optimize the text tokens \(\) for test image \(x\) with certain self-supervised training losses \(_{self}\) that can be formulated by:

\[*=*{arg\,min}_{}_{self}(F,,x). \]

Note that the test data is presented in a continuous flow, where the text tokens are continuously updated with the test data flow.

### Historical Test-time Prompt Tuning

We design three types of knowledge banks to help memorize the useful knowledge learnt from the previous test samples and adaptively exploit the memorized knowledge for regularizing the prediction of the current test samples. As illustrated in Fig. 2, _local knowledge bank_ buffers features of the recent test images, capturing up-to-date distribution changes along the tuning process. _Hard-sample knowledge bank_ actively identifies and stores hard samples from the local knowledge bank, which helps to capture difficult and corner features. _Global knowledge bank_ maintains global and representative information along the whole prompt tuning process by accumulating all the features from the local knowledge bank and hard-sample knowledge bank. In addition, HisTPT introduces an _adaptive knowledge retrieval mechanism_ that adaptively retrieves relevant memorized knowledge for prediction regularization and prompt optimization for each test image.

Given a continuous flow of \(N\) test samples \(_{test}=\{x_{n}\}_{n=1}^{N}\), we take the time step \(n\) as an example to describe the knowledge bank construction with the previous test sample \(x_{n-1}\) and the prompt optimization of the current sample \(x_{n}\) with the memorized knowledge.

**Knowledge Bank Construction.** HisTPT comes with three types of knowledge banks for capturing fresh and representative knowledge during the test-time prompt tuning with previous test samples.

_Local Knowledge Bank_ captures and stores fresh and up-to-date knowledge by buffering the features of the recent test samples. It works as a FIFO queue with a fixed size of \(L\), where the features of the oldest test sample will be dequeued and the features of the most recent test sample will be enqueued to update the local knowledge bank, i.e, \(_{local}=\{u^{l}_{local},p^{l}_{local}\}_{l=1}^{L}\) on the flow. Specifically, for the latest test sample \(x_{n-1}\) and its learnt text tokens \(_{n-1}\), local knowledge bank enqueues its text feature \(u_{n-1}\) and prediction probability \(p_{n-1}\), i.e., \(u_{n-1}=\{u^{c}_{n-1}\}_{c=1}^{C}\) where \(u^{c}_{n-1}=F^{T}((_{n-1};y_{c}))\), and \(p_{n-1}=\{p^{c}_{n-1}\}_{c=1}^{C}\) where \(p^{c}_{n-1}\) is calculated via Eq. 1. Note that the size of local knowledge bank \(L\) is much smaller than the total number of test samples \(N\) since local knowledge bank aims to capture fresh information and up-to-date distribution changes of test samples along the test-time prompt tuning process.

_Hard-sample Knowledge Bank_ identifies hard samples from local knowledge bank for capturing difficult and corner information. We identify hard samples by those having high classification uncertainty, where the uncertainty is measured by their prediction entropy which can be computed from their prediction probability as stored in the local knowledge bank:

\[(u^{l}_{local})=-_{c=1}^{C}p^{(l,c)}_{local}\;\;p^{(l,c)}_{local}, \]

where the first \(K\) samples with the highest entropy are selected and stored in the hard-sample knowledge bank. To enable robust memorization, we first compact the features of \(K\) selected samples via category-wise average and store the compacted feature in the hard-sample knowledge bank. Similar to the local knowledge bank, hard-sample knowledge bank also works as a FIFO queue with a fixed size of \(H\), i.e., \(_{hard}=\{u^{h}_{hard}\}^{H}_{h=1}\).

_Global Knowledge Bank_ stores global and representative knowledge the whole prompt tuning process by accumulating all the features from the local knowledge and hard-sample knowledge banks. Specifically, we compact the features \(_{global}\) and \(_{hard}\) dequeued from the local and hard-sample knowledge banks to generate category-wise feature prototype \(_{global}=\{^{c}_{global}\}^{C}_{c=1}\), where \(^{c}_{global}=1/2\) (\(^{c}_{local}+^{c}_{hard}\)). To facilitate stable and sustainable global memorization along the tuning process, we update the global knowledge bank with compacted feature prototype in a momentum way:

\[_{global}(1-)\;_{global}+\;_{global}, \]

where \(_{global}\) denotes the old global feature prototype and \(\) is a coefficient for smooth feature update in the global knowledge bank.

**Prompt Optimization with the Constructed Knowledge Banks.** With the built comprehensive memorization, HisTPT introduces an _Adaptive Knowledge Retrieval Mechanism_ that enables adaptive retrieval of memorized knowledge for prediction regularization and prompt optimization of each test sample.

Given the test sample \(x_{n}\) and the text tokens learnt at time step \(n-1\), i.e., \(_{n-1}\), the category-wise prediction probability \(p_{n}=\{p^{c}_{n}\}^{C}_{c=1}\) can be obtained by measuring the similarity between the image feature \(v_{n}=F^{I}(x_{n})\) and category-wise text feature \(u^{c}_{n}=F^{T}((_{n-1};y_{c}))\) via Eq.1. The prediction \(p_{n}\) can be enhanced via regularization with the three types of knowledge banks. For temporary knowledge in the local and hard-sample knowledge banks, we first compact the stored features into category-wise feature prototypes, i.e., \(_{local}\) and \(_{hard}\), via an average operation:

\[_{local}=\{^{c}_{local}\}^{C}_{c=1},_{hard}=\{^{c}_{ hard}\}^{C}_{c=1}\;\;\;^{c}_{local}=_{1}^{L}u^ {(l,c)}_{local},^{c}_{hard}=_{1}^{H}u^{(h,c)}_{hard}. \]

Figure 2: Overview of the proposed HisTPT. HisTPT features three types of knowledge banks, namely, local knowledge bank, hard-sample knowledge bank, and global knowledge bank, which learn and memorize up-to-date, difficult and representative knowledge, respectively, from previous test samples (e.g., \(x_{n-2}\) and \(x_{n-1}\)) and their learnt text tokens (e.g., \(_{n-2}\) and \(_{n-1}\)) along the test-time prompt tuning process. For the current test sample \(x_{n}\), HisTPT regularizes its prediction by retrieving the memorized knowledge via an adaptive knowledge retrieval mechanism, enabling prompt optimization for \(x_{n}\) with the self-supervised loss \(_{self}\).

The new prediction for \(x_{t}\) can thus be obtained based on the derived prototypes \(_{local}\), \(_{hard}\), and \(_{global}\). Take the local prototype \(_{local}\) as an example. The prediction regularization of \(x_{n}\) can be obtained with the local knowledge bank \(p_{local}\) by

\[p_{local}=\{p_{local}^{c}\}_{c=1}^{C},\;\;p_{local}^{c}=\;(cos( _{local}^{c},v_{n}))/\,}{_{j=1}^{C}\;(cos(_{local }^{j},v_{n}))/\,}. \]

The prediction regularization by the hard-sample and global knowledge banks can be obtained in a similar way. Generally, the prediction with higher confidence (i.e., lower entropy) means that the corresponding feature prototype is better aligned with the current test sample in feature space, and it should contribute more to the final prediction \(_{n}\) that can be obtained as follows:

\[_{n}=_{i}w_{i}\,p_{i},\;\;w_{i}=(_{c=1}^{C}p_{i}^ {(c)}\;p_{i}^{(c)}), \]

where \(i\{local,hard,global\}\). The softmax operation is performed across the entropy of different predictions.

With the regularized prediction probability \(_{n}\), the text tokens \(_{n-1}\) can be optimized for the current test sample \(x_{n}\) with the self-supervised loss defined as follows:

\[_{self}=l(p_{n},_{n}) \]

where \(l()\) denotes a task-related loss, e.g., the standard cross-entropy loss for image classification.

## 4 Experiments

This section presents experiments including datasets, implementation details, benchmarking with the state-of-the-art, as well as discussion of our designs.

### Datasets

We evaluate HisTPT over multiple datasets across three widely studied visual recognition tasks:

**Semantic Segmentation:** We benchmark HisTPT over 6 image segmentation datasets with pixel-wise annotations, including Cityscapes , BDD100K , Mapillary , ADE20K , Pascal Content  and ACDC .

**Image Classification:** We benchmark HisTPT over 10 classification datasets, including Flowers102 , DTD , Oxford-Pets , StanfordCars , UCF101 , Caltech101 , Food101 , SUN397 , Aircraft  and EuroSAT .

**Object Detection:** We benchmark HisTPT over 4 object detection datasets, including Cityscapes , BDD100K , ADE20K  and ACDC .

### Implementation Details

**Semantic Segmentation:** Following , we adopt SEEM  with two vision backbones including Focal-Tiny  and Davit-Large  as the segmentation foundation models. In training, we employ AdamW optimizer  with a weight decay of 0.05, and set the initial learning rate as 0.0001.

**Image Classification:** Following , we use CLIP  with two backbones, i.e., ResNet-50  and ViT-B/16 , as the classification foundation models. In training, we adopt AdamW optimizer  with a weight decay of 0.01, and set the initial learning rate as 0.005.

**Object Detection:** For object detection task, we adopt SEEM  with two vision backbones including Focal-Tiny  and Davit-Large  as the detection foundation models. In training, we employ AdamW optimizer  with a weight decay of 0.05, and set the initial learning rate as 0.0001.

For all experiments, the prompt is initialized as "a photo of a" and the corresponding 4 tokens (i.e., \(M=4\)) of dimension \(D=512\) are optimized as in . Unless otherwise specified, we set the size of the local knowledge bank and hard-sample knowledge bank at \(L=H=32\) and the number of the selected hard-sample features \(K\) at \(16\). We set the update coefficient \(\) of the global knowledge bank at \(0.99\). Following , we set the optimization step in test-time prompt tuning at 1 by default. All the experiments are conducted on one NVIDIA Tesla V100 GPU with batch size \(1\).

### Comparisons with State of the Arts

**Semantic Segmentation.** We evaluate and benchmark HisTPT over 6 semantic segmentation datasets. Since there is little prior study on test-time prompt tuning on semantic segmentation, we benchmark HisTPT by reproducing methods [7; 8], which are designed for image classification task, on semantic segmentation task. Table 1 shows experimental results. We can observe that HisTPT achieves superior segmentation performance, largely due to its comprehensive memorization that helps to regularize the predictions of test samples and mitigates the knowledge forgetting problem in test-time prompt tuning. In addition, HisTPT is complementary to existing methods and produces clear and consistent performance boosts. This is attributed to the proposed HisTPT which can effectively mitigate the knowledge forgetting existing methods.

**Image Classification.** Following [7; 8], we evaluate HisTPT over 10 image classification tasks. To suit the setup in this work, we reproduce methods [7; 8] by keeping their prompts continuously updated during the test-time adaptation. As shown in Table 2, HisTPT outperforms state-of-the-art methods consistently over different classification tasks such as classic classification on Flowers102 , texture classification on DTD  and human action recognition on UCF101 . This demonstrates the superior generalization ability while HisTPT faces diverse downstream data.

**Object Detection.** We evaluate and benchmark HisTPT over 4 object detection datasets. Similar to semantic segmentation benchmarking, we benchmark HisTPT by reproducing methods [7; 8] (designed for image classification task) on the object detection task. As shown in Table 3, HisTPT achieves superior detection performance and can well handle a wide range of detection tasks including detection under various weather conditions  across different scenes [16; 69]. The superior detection performance is largely attributed to the knowledge banks in HisTPT which effectively help generate more accurate predictions and learn better prompts for test samples.

### Ablation Studies

We examine the proposed HisTPT by performing ablation study over Cityscapes semantic segmentation task. As shown in Table 4, the three types of knowledge banks can work well alone and improve

   Method & Cityscapes & BDD & Mapillary & ADE & Pascal & ACDC\({}_{Fap}\) & ACDC\({}_{Vispla}\) & ACDC\({}_{Rain}\) & ACDC\({}_{Saware}\) & Mean \\  SEEM-Tiny & 39.2 & 37.4 & 14.7 & 14.6 & 45.1 & 34.6 & 20.7 & 33.1 & 35.8 & 30.5 \\  TPT  & 42.3 & 38.9 & 15.4 & 16.1 & 46.8 & 35.2 & 21.4 & 34.9 & 36.5 & 31.9 \\ TPT  + HisTPT & 45.1 & 41.8 & 17.5 & 17.6 & 49.4 & 37.2 & 22.9 & 37.2 & 37.8 & **34.0** \\  DiffTPT  & 42.9 & 39.6 & 15.8 & 16.3 & 47.1 & 35.7 & 21.6 & 35.3 & 36.6 & 32.3 \\ DiffTPT  + HisTPT & 45.4 & 42.1 & 16.7 & 17.9 & 49.2 & 47.6 & 22.7 & 37.7 & 38.1 & **35.2** \\ 
**HisTPT** & 44.7 & 41.2 & 17.2 & 17.3 & 48.7 & 36.8 & 22.1 & 36.7 & 37.1 & **33.5** \\   SEEM-Large & 49.3 & 44.6 & 18.7 & 15.2 & 37.1 & 48.1 & 32.0 & 47.4 & 45.0 & 37.4 \\  TPT  & 50.1 & 45.2 & 19.1 & 15.7 & 40.2 & 48.7 & 32.4 & 47.9 & 45.7 & 38.3 \\ TPT  + HisTPT & 52.1 & 47.4 & 21.3 & 17.1 & 45.8 & 52.1 & 33.4 & 49.4 & 45.8 & **40.8** \\  DiffTPT  & 50.4 & 45.7 & 19.3 & 16.1 & 41.2 & 49.1 & 32.2 & 48.2 & 46.3 & 38.7 \\ DiffTPT  + HisTPT & 52.4 & 47.8 & 21.1 & 17.4 & 46.3 & 52.4 & 33.6 & 49.7 & 49.1 & **41.0** \\ 
**HisTPT** & 51.9 & 47.3 & 20.1 & 16.9 & 45.7 & 51.6 & 33.1 & 49.1 & 48.5 & **40.4** \\   

Table 1: Test-time prompt tuning on semantic segmentation over 6 widely adopted datasets. mIoU is reported.

   Method & Flower & DTD & Pets & Cars & UCF101 & Caltech101 & Food101 & SUN397 & Aircraft & EuroSAT & Mean \\  CLIP-RNS0 & 61.7 & 40.3 & 83.5 & 55.7 & 58.8 & 85.8 & 73.9 & 58.8 & 15.6 & 23.6 & 55.8 \\  TPT  & 62.2 & 40.1 & 83.9 & 58.3 & 60.3 & 86.3 & 74.4 & 60.9 & 16.7 & 27.4 & 57.1 \\ DiffTPT  & 63.1 & 39.7 & 82.9 & 60.1 & 62.1 & 86.4 & 78.3 & 62.4 & 17.3 & 39.3 & 59.2 \\ 
**HisTPT** & 67.6 & 41.3 & 84.9 & 61.3 & 64.1 & 87.2 & 81.3 & 63.5 & 18.1 & 42.5 & **61.2** \\   CLIP-ViT-B/16 & 67.4 & 44.2 & 88.2 & 65.4 & 65.1 & 93.3 & 83.6 & 62.5 & 23.6 & 42.0 & 63.5 \\  TPT  & 68.2 & 47.3 & 87.1 & 66.5 & 67.7 & 93.7 & 84.2 & 65.1 & 24.3 & 42.1 & 64.6 \\ DiffTPT  & 69.4 & 46.3 & 87.9 & 66.4 & 68.1 & 92.3 & 86.5 & 65.3 & 25.1 & 42.8 & 65.0 \\ 
**HisTPT** & 71.2 & 48.9 & 89.1 & 69.2 & 70.1 & 94.5 & 89.3 & 67.2 & 26.9 & 49.7 & **67.6** \\   

Table 2: Test-time prompt tuning on image classification over 10 widely adopted datasets. Top-1 classification accuracy is reported.

the performance consistently, indicating that all the stored historical knowledge is helpful in prompt tuning. In addition, the three types of knowledge banks are complementary to each other, largely because the three knowledge banks store different types of knowledge, i.e., local knowledge bank stores fresh information, hard-sample knowledge bank stores difficult corner case information, and global knowledge bank stores the global and representative features. On top of the three types of knowledge, including the proposed adaptive knowledge retrieval improves the performance further. This shows that adaptively retrieving different types of memorized information for each test image could generate more accurate prediction and ultimately lead to better test-time prompt tuning.

### Discussion

**Complementarity to Prompt Learning Methods.** As a test-time tuning technique, the proposed HisTPT is complementary to prompt learning methods that learn prompts at the training stage. We examine this feature by setting the learnt prompts by prompt learning  as the initial prompts of HisTPT. As Table 5 shows, equipping HisTPT with the learnt prompts improves the performance clearly, indicating that HisTPT as a plug-in can greatly enhance existing prompt learning methods.

**Optimization Steps.** We examined how the optimization step affects HisTPT by increasing it from \(1\) to \(10\). Figure 3 shows the mean mIoU over \(6\) semantic segmentation datasets with SEEM-Tiny. We can observe that increasing the optimization step improves segmentation consistently. Nevertheless, the performance gain becomes marginal after \(6\)-\(8\) optimization steps. The actual optimization step can be set by balancing the inference efficiency and the inference accuracy.

**Continuously Changing Test Domains.** As discussed in Section 1, HisTPT can handle challenging scenarios when the domain of test samples changes continuously. We examine this feature over semantic segmentation data that were collected under normal weather  and various adverse weathers  (fog, night, rain and snow). As Table 6(a) shows, the performance of existing test-time prompt tuning methods TPT  and DiffTPT  degrades gradually along the tuning process when the weather changes from normal to adverse, largely due to increasing error accumulation and 'forgetting' while the test domain changes continuously. As a

    &  &  &  \\    & local knowledge bank & & & & & 39.2 \\    & ✓ & & & & 41.1 \\  & ✓ & & ✓ & & 40.9 \\  & ✓ & ✓ & & & 41.7 \\  & ✓ & & ✓ & & 42.2 \\  & ✓ & & ✓ & & 42.8 \\  & ✓ & ✓ & ✓ & & 43.6 \\
**HisTPT** & ✓ & ✓ & ✓ & ✓ & **44.7** \\   

Table 4: Ablation study of the proposed HisTPT over Cityscapes semantic segmentation task.

Figure 3: HisTPT with multiple optimization steps.

   Method & Cityscapes & BDD & ADE & \(_{Fog}\) & \(_{Night}\) & \(_{Rain}\) & \(_{Snow}\) & Mean \\  SEEM-Tiny & 30.5 & 26.1 & 15.7 & 44.2 & 22.3 & 25.9 & 33.9 & 28.3 \\  TPT  & 30.9 & 27.0 & 16.2 & 44.8 & 23.1 & 26.3 & 34.4 & 28.9 \\ DiffTPT  & 31.2 & 27.4 & 16.8 & 45.1 & 23.3 & 26.7 & 34.6 & 29.3 \\  
**HisTPT** & 31.9 & 28.3 & 17.5 & 46.2 & 24.7 & 27.2 & 35.6 & **30.2** \\   SEEM-Large & 31.4 & 31.8 & 18.3 & 55.2 & 31.4 & 34.8 & 43.7 & 35.2 \\  TPT  & 31.8 & 32.2 & 18.5 & 55.6 & 31.9 & 35.1 & 44.2 & 35.6 \\ DiffTPT  & 32.5 & 32.3 & 18.9 & 56.1 & 32.3 & 35.4 & 44.8 & 36.0 \\ 
**HisTPT** & 33.2 & 33.4 & 19.4 & 56.9 & 33.1 & 36.4 & 45.2 & **36.8** \\   

Table 3: Test-time prompt tuning on object detection over 4 widely adopted datasets. mAP\({}_{50}\) is reported.

comparison, HisTPT improves the performance consistently across different weathers, and this is largely due to two factors: 1) HisTPT effectively preserves representative and up-to-date knowledge from past test samples along the tuning process; 2) HisTPT retrieves relevant memorized knowledge for each test sample, mitigating the 'forgetting' and leading to more robust test-time prompt tuning. Similar results are obtained when the test domain changes from adverse weather to normal weather as shown in Table 6(b), further verifying HisTPT's effectiveness and robustness while facing changing test domains.

**Comparisons to Existing Memory-based Learning Methods.** We examine how the proposed HisTPT performs as compared with existing memory-based learning techniques. We benchmark it with two categories of memory-based learning techniques: 1) memory-based learning in traditional network training  and 2) memory-based learning with vision foundation models . Table 7 shows experimental results on the task of semantic segmentation on Cityscapes with SEEM-Tiny. It can be seen that HisTPT outperforms all existing memory learning techniques  with clear margins. The superior performance is largely attributed to two factors: 1) HisTPT memorizes comprehensive knowledge of previous test samples on the fly along the prompt tuning process and 2) HisTPT features a retrieval mechanism that adaptively retrieves the memorized knowledge to learn specific prompts for each current test sample.

## 5 Conclusion

This paper introduces Historical Test-time Prompt Tuning (HisTPT), a general test-time prompt tuning framework that aims to mitigate the 'knowledge forgetting' problem across various visual recognition tasks. HisTPT introduces three types of knowledge banks, including local knowledge bank, hard-sample knowledge bank and global knowledge bank, each of which works with different mechanisms for memorizing useful knowledge. With the three knowledge banks, HisTPT builds up comprehensive memorization that preserves useful knowledge from previous test samples, mitigating the knowledge forgetting and enabling robust test-time prompt tuning. In addition, HisTPT comes with an adaptive knowledge retrieval mechanism that regularizes the prediction of the current test sample by adaptively retrieving the memorized knowledge. Extensive experiments show that HisTPT achieves superior performance consistently across various vision tasks. In addition, HisTPT can effectively handle the challenging scenario where the domain of test samples changes continuously. Moving forwards, we will further investigate memory-based learning for adaptation of vision foundation models.

**Acknowledgement.** This study was funded by the MOE Tier-1 project RG18/22.

  Test Order (\(\)) & Normal & Fug & Nugth & Rain & Snow \\  SEEM-Tiny & 3.22 & 34.6 & 20.2 & 3.31 & 35.8 \\  TTPT & 4.25(-3.1) & 344(-0.2) & 30.1(-0.6) & 31.7(-1.4) & 306(-5.2) \\  DiTPT & 42.9(-3.7) & 35.2(-0.6) & 20.3(-0.4) & 32.0(-1.1) & 31.4(-4.4) \\  
**IMTPT** & 4.24(-0.5) & 369(-2.9) & 23.6(-2.9) & 37.3(-0.42) & 38.1(-2.3) \\   

Table 6: Test-time prompt tuning on semantic segmentation across continuously changing test domains. mIoU is reported.

  Method & HCL  & MeGA  & BiMem  & MeCap  & TF-Clip  & TDA  & HisTPT \\  mIoU & 40.3 & 40.7 & 41.2 & 41.9 & 41.4 & 42.6 & **44.7** \\   

Table 7: Comparison with existing memory-based learning methods over Cityscapes semantic segmentation task on SEEM-Tiny. mIoU is reported.

  Method & CLIP-RN50 & CoOp & CoCoOp & HisTPT & HisTPT + CoOp & HisTPT + CoCoOp \\  Mean Accuracy & 55.8 & 56.1 & 57.2 & 61.2 & 62.4 & 63.1 \\   

Table 5: Complementarity to state-of-the-art prompt learning methods CoOp  and CoCoOp . The mean top-1 accuracy across 10 image classification datasets is reported, and CoOp and CoCoOp are supervised with 16-shot labelled training data per category.