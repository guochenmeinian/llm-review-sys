# Efficient Streaming Algorithms for Graphlet Sampling

Yann Bourreau

Cispa Helmholtz Center for Information Security

Saarland University

Saarbrucken, Germany

yann.bourreau@cispa.de &Marco Bressan

Department of Computer Science

University of Milan

Milan, Italy

marco.bressan@unimi.it &T-H. Hubert Chan

Department of Computer Science

The University of Hong Kong

Hong Kong, China

hubert@cs.hku.hk &Qipeng Kuang

Department of Computer Science

The University of Hong Kong

Hong Kong, China

kuangqipeng@connect.hku.hk &Mauro Sozio

Institut Polytechnique de Paris, Telecom Paris

Palaiseau, France

sozio@telecom-paris.fr

Authors appear in alphabetical order.

###### Abstract

Given a graph \(G\) and a positive integer \(k\), the Graphlet Sampling problem asks to sample a connected induced \(k\)-vertex subgraph of \(G\) uniformly at random. Graphlet sampling enhances machine learning applications by transforming graph structures into feature vectors for tasks such as graph classification and subgraph identification, boosting neural network performance, and supporting clustered federated learning by capturing local structures and relationships. A recent work has shown that the problem admits an algorithm that preprocesses \(G\) in time \(O(nk^{2} k+m)\), and draws one sample in expected time \(k^{O(k)} n\), where \(n=|V(G)|\) and \(m=|E(G)|\). Such an algorithm relies on the assumption that the input graph fits into main memory and it does not seem to be straightforward to adapt it to very large graphs. We consider Graphlet Sampling in the semi-streaming setting, where we have a memory of \(M=(n n)\) words, and \(G\) can be only read through sequential passes over the edge list. We develop a semi-streaming algorithm that preprocesses \(G\) in \(p=O( n)\) passes and samples \((Mk^{-O(k)})\) independent uniform \(k\)-graphlets in \(O(k)\) passes. For constant \(k\), both phases run in time \(O((n+m) n)\). We also show that the tradeoff between memory and number of passes of our algorithms is near-optimal. Our extensive evaluation on very large graphs shows the effectiveness of our algorithms.

## 1 Introduction

Sampling and counting small subgraphs in large graphs is a central problem in graph mining and machine learning. Let \(G=(V,E)\) be a simple graph (where \(n=|V|\) and \(m=|E|\)) and \(k>2\) an integer. A \(k\)-_graphlet_ of \(G\) is a connected, induced subgraph of \(G\) on \(k\) vertices. It is well known that the \(k\)-graphlet distribution (the relative frequency of \(k\)-cliques, \(k\)-paths, and so on) reveals key information about the structure of a graph  and can be transformed into thefeature vector for graph classification and subgraph identification [TLT\({}^{+}\)19]. Graphlet information also contributes to neural network in several ways. [PLG\({}^{+}\)20] converted the graph to a matrix by sampling \(k\)-graphlets for a sequence of vertices so that convolutional neural networks can be applied. [TLT\({}^{+}\)19] showed that using graphlet frequency as a node attribute improves the performance of diffusion-convolutional neural networks for node classification and graph classification. It is also at the heart of graph mining applications such as graph kernels [SVP\({}^{+}\)09]. Furthermore, in clustered federated learning [GCYR22], graphlet sampling can be applied to improve model performance by capturing the local structures and characteristics of the data distributed across various clients or devices. Specifically, graphlet distributions can be used to model relationships between clients or their data in a way that reflects the underlying graph structure, allowing for more informed clustering decisions. As a consequence, sampling \(k\)-graphlets uniformly at random from \(G\) has become a key primitive in graph mining and machine learning. This problem is called Graphlet Sampling, and many algorithms have been proposed for it [ABH19, BRRAH12, BCK\({}^{+}\)17, BCK\({}^{+}\)18, BLP19, BLP21, CLWL16, HS16, MG20, PSS19, SH15, WLR\({}^{+}\)14].

In this work we give efficient _streaming algorithms_ for Graphlet Sampling. The computational model, called _streaming model_, is as follows. First, the graph \(G\) can only be read by scanning its edges sequentially, in an arbitrary order that is unknown to the algorithm. Each scan of the list is called a _pass_. Second, the algorithm is only granted \(M=o(|G|)\) words of memory of \(( n)\) bits each, where \(n=|V|\). For the case \(M=o(n)\), the algorithm cannot even store the vertex set and is too restrictive. In this work, we consider the _semi-streaming_ model, in which the algorithm has memory \(M=(n)\) and the input graph is dense such that \(M=o(m)\). Hence, the algorithm can store the degrees of \(G\) but not the entire graph. The goal is to find an algorithm that makes a small number of passes, usually \(O( n)\). This kind of algorithms is widely studied in graph mining, as they are useful when the input data are stored remotely or are too large to fit in main memory.

Let us discuss the problem in more detail under the standard RAM model of computation. Sampling a \(k\)-vertex subgraph of \(G\) that is _not_ necessarily connected is straightforward: just sample a \(k\)-vertex set \(S V\) and return \(G[S]\). The problem becomes nontrivial, though, when the subgraph must be connected (i.e., a \(k\)-graphlet). In this case, one very general approach is _rejection sampling_: one samples some connected subgraph \(G[S]\) from some distribution, and then accepts \(G[S]\) with some appropriate probability, so that the resulting distribution is as close to uniform as possible. For almost a decade, all known rejection sampling techniques had a worst-case running time of \((n^{k})\), or sampled from a nonuniform distribution [BRRAH12, WLR\({}^{+}\)14, SH15, BLP19, BLP21]. It was then shown that both issues can be avoided by using a simple two-phase algorithm, Ugs (Uniform Graphlet Sampler) [Bre21a, Bre23]. The key idea behind Ugs is to sort \(G\) topologically by repeatedly deleting a vertex of maximum degree; this takes time \(O(nk^{2} k+m)\) where \(m=|E|\). Afterwards, one chooses a starting vertex \(v\) according to a certain distribution computed from the topological order, creates a \(k\)-graphlet \(g\) by exploring the graph surrounding \(v\) with vertices topologically no smaller than \(v\) in a randomized way, and accepts \(g\) with a certain probability (otherwise, the process is repeated). It can be shown that the distribution of the accepted graphlets is uniform, and that the expected time before accepting a graphlet is \(k^{O(k)} n\).

It is therefore natural to apply Ugs to the streaming setting. This is not so immediate, though. One key obstacle is that the computation of the topological order of \(G\) seems inherently sequential: after deleting a maximum-degree vertex from \(G\), one needs to update the degrees of its neighbors, and in the streaming model this would require one pass. We would thus make \(n\) passes in total, which is unacceptable. In this work we show how to overcome these kind of obstacles. We obtain an algorithm that in \(( n)\) passes computes an approximate topological order of \(G\), and afterwards can sample \((M/k^{O(k)})\) independent uniform random graphlets every \(O(k)\) passes, as long as the algorithm has \(M=(kn)\) words of memory. We make several other nontrivial improvements that has practical impacts. For instance, while deciding if each vertex is contained in a sized-\(k\) graphlet takes \(k\) passes in a straightforward implementation, we show how to do that in only 1 pass. In the sampling phase, we show how to share computation between parallel trials such that the running time can be reduced by a factor of \(M\) (and thus a factor of \(n\)), even though the distributions of the parallel instances are still independent. Specifically, if there are \(Q\) parallel instances, instead of taking \(O(mQ)\) time per pass on the edge list, we reduce the running time to \(O(m Q)\). This requires a careful orchestration of random number generation, sorting, and prefix sum computations. We also prove that any \(p\)-pass streaming algorithm for Graphlet Sampling requires memory \(()\); therefore, our algorithm is nearly space-optimal. Finally, we conduct a series of experiments which verify the relation between several metrics and the parameters of the algorithm.

### Contributions

**An efficient streaming algorithm for graphlet sampling.** We present stream-Ugs, a graphlet sampling algorithm with the following formal guarantees:

**Theorem 1.1**.: _In the semi-streaming model with \(k=o( n)\) and \(M\) words of memory satisfying \((n n) M=o(m)\), stream-Ugs satisfies the following.2 The preprocessing phase makes \(p=O( n)\) passes with high probability,3 and has running time \(O(p\,m+n\,k^{2} k)\). The sampling phase makes \(2k\) passes, returns \((Mk^{-O(k)})\) independent uniform \(k\)-graphlets from \(G\) with high probability, and runs in time \(O(n\,2^{k} n+m\,k n)\)._

It is useful to instantiate Theorem 1.1 with \(k=O(1)\). For example, \(k=3,4,5\) is common in the literature. In this case stream-Ugs makes \(O( n)\) preprocessing passes; afterwards, every \(O(1)\) passes it returns a batch of \((n n)\) random uniform \(k\)-graphlets of \(G\). Both preprocessing and sampling take time \(O(m n)\). Note that the sampling phase can be repeated arbitrarily many times; every execution yields a new batch of independent uniform \(k\)-graphlets.

**Tradeoff Between Memory and Number of Passes.** We remark that Theorem 1.1 is a special case of a more general tradeoff that we prove between the number of passes, the memory size, and the number of graphlets returned. For example, if \(M=(kn)\) then stream-Ugs still works, but the number of preprocessing passes becomes \(O( n n)\), or the number of graphlets sampled decreases by a factor \(O^{k}(n)\). From a technical standpoint, stream-Ugs relies on several ingredients. The first one is a \(O( n)\)-pass streaming algorithm for computing the said approximate topological order. The second is a 1-pass routine for computing, simultaneously from each vertex, a truncated breadth-first search to detect those vertices that do not lead to any graphlet. The third is a scheme for running instances of the sampling phase in parallel without duplicating the stream across all instances. All these results are detailed in Section 3.

**Space lower bounds for Graphlet Sampling.** We complement our guarantees by formal lower bounds on the amount of memory \(M\) needed by any algorithm for Graphlet Sampling. We prove:

**Theorem 1.2**.: _For \(k 3\), any \(p\)-pass streaming algorithm for Graphlet Sampling requires \(\,(}{{p}})\) bits of memory._

We also give a similar lower bound for the problem of computing the approximate topological order mentioned above. These lower bounds are obtained by suitable space-preserving reductions from the Set Disjointness, a well-known problem in communication complexity, and its multiparty variants. The full results are given in Section 4.

**Experimental evaluation.** We conduct an experimental evaluation on real-world graphs containing up to 1.8 billion edges. In particular, we study several metrics as a function of the main parameters of our algorithms. Our evaluation shows that our algorithms are efficient in practice thereby providing a valuable tool in graph mining.

### Related Work

Most streaming algorithms for counting or sampling \(k\)-graphlets are specifically developed for counting or sampling triangles, and do not give guarantees for \(k>3\). The only algorithms that count or sample \(k\)-graphlets for \(k>3\) with formal guarantees,  and , do not seem to guarantee uniform sampling. Their main goal is indeed to estimate the total number of occurrences of a given graphlet \(H\) in the graph \(G\), and they use (biased) sampling as a subroutine. It is not clear that those algorithms can be adapted to yield efficient and truly uniform \(k\)-graphlet sampling.

The only algorithm comparable to ours is Motivo, which is based on the celebrated color-coding technique of Alon, Yuster and Zwick. Motivo has a preprocessing phase and a sampling phase, and they can be implemented in \(O(k)\) passes over \(G\) using memory \(M=(c^{k}\,n)\) for some \(c>0\). Crucially, however, Motivo does _not_ yield uniform samples: the only graphlets that are sampled with nonzero probability are those made colorful by the color-coding step in the preprocessing. Our algorithm, instead, returns truly uniform samples. We also note that, in practice, Motivo uses memory that becomes quickly prohibitive with \(k\), while stream-Ugs uses memory independent of \(k\).

Exploiting topological orders of \(G\) is common in subgraph mining. The most used order is arguably the degeneracy order, or core order, that yields efficient subgraph-counting algorithms , and for which efficient streaming algorithms exist . However, while a core order is obtained by repeatedly removing from \(G\) a vertex of _minimum_ degree, our topological order requires to remove a vertex of _maximum_ degree. The two orders may look related at a first glance; for instance, one may be the reversal of the other. It turns out that this resemblance is only apparent, see Section 2. Thus, the algorithms of  seem not useful here, and we need to develop a new one.

It should be noted that Graphlet Sampling is _not_ the same problem as sampling from \(G\) the occurrences of a prescribed \(k\)-vertex pattern graph \(H\). This can be seen by letting \(H\) be a \(k\)-vertex clique: in that case, sampling an occurrence of \(H\) in \(G\) solves the Clique problem, and this requires time \(n^{(k)}\) unless the widely-accepted Exponential Time Hypothesis fails . Graphlet Sampling instead can be solved in FPT time, that is, time \(f(k) n^{O(1)}\), as  and this work show.

Finally, we follow the typical technique of proving the streaming lower bound by considering the communication complexity on the famous communication game . In such proof, the game is reduced to a streaming algorithm executed by players so that a better tradeoff between memory and number of passes implies a communication cost contradicted to its lower bound. An elaboration in detail is presented in Section 4.

## 2 Preliminaries

Let \(G=(V,E)\) be a simple graph. We assume \(V=\{1,,n\}\). For a total order \(\) over \(V\), we use \(G(v)\) to denote the subgraph of \(G\) induced by \(\{u:u v\}\), i.e., the subset of vertices containing \(v\) and all vertices after \(v\) in \(\). Given \(u V\) we let \((u|G(v))=\{w v:\{u,w\} E\}\) be the neighborhood of \(u\) in \(G(v)\), and \(d(u|G(v))=|(u|G(v))|\) be the degree of \(u\) in \(G(v)\). We denote by \(\) the maximum degree of \(G\). The following notion will be crucial:

**Definition 2.1** (\(\)-DD Order).: _Let \((0,1]\). A total order \(\) over \(V\) is a \(\)-degree-dominating order (\(\)-DD order) if, for all \(v V\) with \(d(v|G(v))>1\) and all \(u v\), we have \(d(v|G(v)) d(u|G(v))\)._

The topological order used by Ugs is a \(1\)-DD order. A \(\)-DD order is an approximation of a \(1\)-DD order. Using a \(\)-DD order yields the same guarantees of Ugs, only with an acceptance probability scaled by \(^{k-1}\) (see Lemma C.1).

Computational model.The algorithm has a memory of \(M\) words of \(( n)\) bits each. We require \(M=(n)\) or \(M=(n n)\), depending on the case. The graph is stored as a list of edges in arbitrary (i.e., adversarial) order. With one _pass_, the algorithm can read the list sequentially. This is called _semi-streaming_ model . To avoid trivialities we assume \(m=(M)\); otherwise, in one pass one can store \(G\) and run the algorithm of . For computation we assume the standard RAM model. We also assume that in time \(O(1)\) one can draw a random uniform integer in \(\{1,,c\}\) for any \(c=(n)\), or a Bernoulli random variable \(B(p)\) for \(p=(n^{-k})\). When we run multiple instances of a subroutine in parallel, the running time is understood to be the total number of operations executed by all those instances.

Graphlet size.Our algorithms are designed primarily for \(k=O(1)\), but they yield nontrivial guarantees also for \(k=(1)\); for instance, for \(k=\). Our full statements make the dependence on \(k\) clear.

DD orders vs. core orders.It should be noted that 1-DD orders and core orders do not seem related in any useful way. For example, let \(G\) be the disjoint union of a star with \(_{1}\) leaves and a \(_{2}\)-clique. If \(_{1}>_{2}\), then the center of the star comes first in a 1-DD order, but in a core order it sits after the star's leaves and before the clique vertices. If \(_{2}>_{1}\), instead, then in a core order the vertices of the star all come before the clique, while in a 1-DD ordering the center of the star sits after \(_{2}-_{1}\) vertices of the clique.

## 3 A Streaming-Based Graphlet Sampling Algorithm

This section describes our semi-streaming algorithm, stream-Ugs. stream-Ugs is based on the approach of Ugs, and runs in two phases: preprocessing and sampling.

**Preprocessing.** This is done once, and involves two steps.

* Computing a \(\)-DD order \(\) of \(G\). The order defines implicitly the subgraphs \(G(v)\), see Section 2, as well as a certain distribution \(\) over \(V\) that is needed by the sampling phase, see below.
* Computing the said probability distribution \(\) on \(V\). Let \((v)\) be the set of all graphlets that are subgraphs of \(G(v)\) and contain \(v\). This is called the _bucket_ of \(v\). The distribution \(\) can be seen as a distribution over the buckets \((v)\). For every \(v V\), the probability \(p(v)\) is proportional to \(d(v|G(v))^{k-1}\) if \((v)\), and \(p(v)=0\) otherwise. The entry \(p(v)\) will be the probability that \(v\) is picked in the first step of the subsequent sampling.

**Sampling.** This phase may report failure, but if it succeeds, it returns a uniformly random \(k\)-graphlet of \(G\). The expected number of trials to successfully return a graphlet is \(^{1-k} k^{O(k)}\). However, with enough memory, one can carry out independent trials in parallel. The phase consists of three steps.

* Choosing a bucket \((v)\) according to the probability distribution \(\). One can show that, if \((v)\), then \(|(v)|\) equals \(d(v|G(v))^{k-1}\), up to factors \(k^{O(k)}\). Thus, we are choosing buckets with probability roughly proportional to the number of \(k\)-graphlets in them.
* Sampling a random graphlet \(G[S](v)\). To this end let \(S_{1}=\{v\}\). For \(i=1,2,,k-1\), draw an edge uniformly at random from the cut between \(S_{i}\) and \(G(v)\), and add the endpoint to \(S_{i}\); this gives \(S_{i+1}\). The final set \(S=S_{k}\) defines the graphlet \(G[S]\). Note that the sampling here is not uniform. Uniformity is ensured by the next step.
* Accepting \(S\) with some probability \(p(S)\). If \(S\) is accepted then compute \(G[S]\) and return it; otherwise, report failure.

**Technical Challenges.** In the semi-streaming setting, we cannot store all the edges of \(G\) in the memory. Our contributions are how to implement each of the above steps with restricted memory using a small number of passes on the edge list.

### Computing a \(\)-DD order

We present an algorithm, Approx-DD (Algorithm 1), that given a graph \(G\) and a parameter \(0<=O(1)\), computes a \(\)-DD order of \(G\), where \(=\). Our main result is:

**Theorem 3.1**.: _In the semi-streaming model, Approx-DD can be implemented so that with probability at least \(1-n^{-(1)}\), it makes \(q=O}\) passes if \(M=(n)\), and \(q=O}\) passes if \(M=\). In both cases the total running time is in \(O(q m)\)._

The complete proof of Theorem 3.1 combines several technical lemmas and can be found in Appendix A. The rest of this section overviews Approx-DD and sketches the proof. Approx-DD maintains a list \(L\), initially empty, which will eventually contain the vertices of \(V\) sorted by \(\). The algorithm proceeds in _peeling_ rounds; every round detletes from \(G\) all vertices of degree at least \(\), for some appropriate \(\), and places them in \(L\) in the right order. This "right order" is not simply any order; it is carefully computed by another procedure, called _shaving_, using random partitioning. Algorithm 1 gives the pseudocode of Approx-DD. It assumes that \(_{G}()\), and all other degrees, are updated as soon as the graphs are modified; except for \(\), which is explicitly recomputed when needed. In practice, updating the degrees amounts to running one pass over the edges of \(G\). The algorithm has a triply nested loop structure, and we use the subroutines Peel and Shave for clearer illustration.

**Intuition for** Peel and Shave. Let \(H\) be the subset of vertices with degree at least \(\) in \(G\) (line 1). As a first attempt, we could remove all vertices in \(H\) from \(G\) in a single batch, which is appended to \(L\) in arbitrary order. This works if every vertex \(v\) has all of its neighbors come after itself in \(L\), since in that case \(d(v|G(v))d(u|G(v))\) for all \(u v\). However, in general this does not happen.

To this end, we attempt to remove vertices in \(H\) from \(G\) in phases through Shave. Specifically, we assign each vertex in \(H\) to one of \(= 2(1+)\) batches \(H_{1},,H_{}\) independently and uniformly at random. By Markov's inequality, each vertex has at most \(\) neighbors within \(H_{i}\) with good probability; then, in phase \(i\), every such vertex in \(H_{i}\) that has current degree at least \(\) will be removed together in one batch from \(G\) (and appended to \(L\) in an arbitrary order). The degrees of the remaining vertices in \(G\) will then be updated. Each call to Shave has \(\) such phases.

The procedure performed by Shave does not yet remove \(H\) from \(G\); it removes only the vertices that have few neighbors in the same subset \(H_{i}\). To completely remove \(H\), we need to iterate the procedure. This is what Peel does at line 1. We shall see that there are two cases. When \(\) is large enough, then one call to Shave in line 1 will be sufficient to make \(H\) empty with high probability. When \(\) is small, we may instead need \(O( n)\) calls to Shave in order for \(H\) to become empty. This is, technically speaking, what makes the whole algorithm Approx-DD efficient. Formally, we prove the following (in Appendix A):

**Lemma 3.2**.: _Let \(p(0,1)\). With probability at least \(1-p\), one execution of Peel (Algorithm 1) calls \(\,O\) times, and at most once if \( 12(1+)(1+)\)._

### Finding the Non-Empty Buckets

Algorithm 2 gives the pseudocode of ComputeDistrib, which computes the distribution \(\) and the normalization constant \(\) needed by the sampling phase. As said above, the crux of ComputeDistrib is checking if \((v)\) for each \(v V\). To this end, it suffices to perform a BFS on all vertices in parallel, so as to check whether the connected component of \(v\) in \(G(v)\) has size at least \(k\). A naive method would take \(k\) passes on the edge list, but with \((kn)\) words of memory, we can achieve this in \(1\) passes, as Lemma 3.3 proves.

```
1:functionComputeDistrib(\(\))
2:for each \(v V\)do
3: compute \(d(v|G(v))\) and \(_{v}:=_{(v)}\)
4: set \(b_{v}_{v} d(v|G(v))^{k-1}\)
5: set \(Z_{v V}b_{v}\)
6: compute probability vector \(\): for each \(v V\), \(p(v)}{2}\)
7: set \(}\)
8:return\((,)\) ```

**Algorithm 2** ComputeDistrib

**Lemma 3.3**.: _In the semi-streaming model with \(M=(kn)\), the routine ComputeDistrib (Algorithm 2) can be implemented so as to use \(1\) passes and time \(O(m+n k^{2} k)\)._

### Sampling Graphlets in Parallel

Algorithm 3 gives the pseudocode of Sample, the sampling phase of stream-Ugs. The sampling phase works after invoking Approx-DD and ComputeDistrib in turn (see the subroutine PreProcess in Algorithm 4, Appendix A). Note that Algorithm 3 gives only an abstract description of the routines. The concrete implementation details are rather involved, though, and we give them in the rest of the section. The reason for this complexity is that, since each sampling trial requires memory \(O(k^{2})\), then with a memory of \(M\) words we can perform \((M/k^{2})\) independent sampling trials in parallel. Doing so efficiently, though, is significantly less straightforward than one would think. However, we prove:

**Theorem 3.4**.: _In the semi-streaming model with \(M=(kn)\) words of memory, suppose the preprocessing routine PreProcess has been run with \(=}{{2}}\). Then, \(Q=(M/k^{2})\) parallel instances of Sample can be run in a batch so that \(Q/k^{O(k)}=(M/k^{O(k)})\) independent uniform random \(k\)-graphlets of \(G\) are returned successfully with high probability. Moreover each batch can be implemented in two ways:_

* _The first one takes_ \(k\) _passes and_ \(O(Mm)\) _time._
* _The second one takes_ \(2k-1\) _passes and_ \(O(M2^{k}+km n)\) _time._

Proof Intuition.: Each instance of Sample requires only \(O(k^{2})\) words of memory. Hence, with memory \(M\), we can run \(Q=(M/k^{2})\) parallel instances of Sample. By Lemma C.1, each instance independently returns a uniform random \(k\)-graphlet with probability at least \(}{(1+)^{k-1}}=k^{-O(k)}\). By standard concentration bounds, then, with high probability the number of uniform random \(k\)-graphlets returned overall is \((M/k^{O(k)})\). The details of the subroutines and data structures to achieve Theorem 3.4 are given in Appendix C.

## 4 Space Lower Bounds

We show that the tradeoff between memory and number of passes of our algorithms is near-optimal. To this end, we use well known results from communication complexity. In the _multi-party randomized communication model_, there are \(t 2\) players seeking to compute a function \(f:_{1}_{2}_{t} \) while exchanging messages according to a specified protocol \(\). For each \(i[t]\), the \(i\)-th player has an input \(x_{i}_{i}\). Given \(x_{1},,x_{t}\), the random variable \((x_{1},,x_{t})\) denotes the message transcript (containing all messages) obtained when all players follow \(\) oninputs \(x_{1},,x_{t}\), where the randomness is over the coins of the players. \(\) is a \(\)-error protocol for \(f\) if there is a deterministic function \(_{}\) such that, for all inputs \(x_{1},,x_{t}\),

\[(_{}((x_{1},,x_{t}))=f(x_{1},,x_{t}))  1-\] (1)

The communication cost of \(\), denoted by \(||\), is the maximum length of \((x_{1},,x_{t})\) over all \(x_{1},,x_{t}\) and over all random choices of all players. The \(\)-error randomized communication complexity of \(f\) is the cost of the best \(\)-error protocol for \(f\). In the _one-way_ variant of such a model, the \(i\)th player sends exactly one message to the \((i+1)\)th player, throughout the protocol (we let \(t+1=1\)).

In the classical _disjointness problem_, the \(i\)th player has an \(n\)-bit vector \((x_{i_{1}},,x_{i_{n}})\{0,1\}^{n}\) that corresponds to a subset of a sized-\(n\) ground set. The goal is to determine whether there is a common element among the subsets of all players, i.e. whether there is an index \(j\) such that \(x_{i_{j}}=1\), for all \(i[t]\). It is known that this problem has \((n/t)\) communication complexity in the one-way multi-party randomized communication model (Theorem 2.6 of ). This holds even in the case when either the bit vectors represent pairwise disjoint sets or they have a unique common element but are otherwise pairwise disjoint.

We show that any \(q\)-pass streaming algorithm requires memory \(M=(n/q)\) even for the problem of computing a node with approximate maximum degree, that is, to compute a node with rank \(1\) in an approximate DD-order. We believe such a result is well-known, however, we could not find a reference for a general \(q\)-pass algorithm. For completeness, we provide the full proofs in Appendix D.

**Lemma 4.1**.: _Given a graph \(G=(V,E)\), \((0,1)\), any \(q\)-pass streaming algorithm computing a node \(v\) such that \(_{G}(v)_{w V}_{G}(w)\) with probability at least \(\), requires memory \((|V|}{q})\)._

Finally, we provide a lower bound on the memory requirement of any streaming algorithm for uniform \(k\)-graphlet sampling, \(k 3\).

**Lemma 4.2**.: _Given a graph \(G=(V,E)\), \(k 3\), any \(q\)-pass streaming algorithm for the uniform \(k\)-graphlet sampling problem on \(G\) requires memory \(()\)._

## 5 Experiments

We implemented our algorithms in Python 3.9 and conducted experiments on a Ubuntu server with CPU Intel Xeon Silver 4108 (1.80GHz) and 28GB RAM. The implementation can be found in the online repository. 4

**Datasets.** We run our experiments on several real-world graphs from the KONECT website 5 and a synthetic random dense graph generated by drawing each edge with probability 0.8 (we name it Dense). We removed edge directions, weights, self-loops, duplicate edges and any other irrelevant data, so as to retain only a list of undirected edges. Table 1 reports all dataset statistics after such a preprocessing phase.

**Streaming model.** The stream of edges comes from a CSV file stored in the disk representing the edge list. Our algorithms make multiple sequential passes over the input data while never storing the set of edges in main memory. Note that from Theorem 3.4 we have two approaches to do sampling in parallel, among which we implemented the second one using \(2k-1\) passes and \(O(M2^{k}+km n)\) time.

**Methodology.** We constrain the memory \(M\) by restricting the maximum number of edges that we can store during the computation of the DD order and sampling to be \(\) in all cases. In the sampling phase it is required to sample at least 100 graphlets successfully in order to reduce the error of sampling probability. We study several metrics as a function of \(\) and \(k\), such as the number of passes, the memory usage, as well as the number of sampling trials to obtain 100 graphlets.

**A Heuristic Approach for PEEL.** We develop a heuristic approach called Approx-DD-Heuristic in Appendix E to speed up the running time of Algorithm 1. We evaluate Approx-DD-Heuristic on large real-world graphs to show the scalability of our algorithms and then conduct a comparison between Approx-DD and Approx-DD-Heuristic on smaller graphs.

Approx-DD **versus** Approx-DD-Heuristic.** We conduct the comparison between Approx-DD and Approx-DD-Heuristic on NY Times, the real-world sparse graph and Dense, the synthetic dense graph. Figure 1 shows the number of passes and memory usage as a function of \(\).

In Figure 0(a) and 0(b) we can see that Approx-DD-Heuristic delivers significantly better results for sparse graphs but has no significant advantage for dense graphs. This is expected, as Peel-Heuristic can process at each step a relatively large chunk of the input graph, if such a graph is sparse. Moreover, consistently with our theoretical analysis in Section 3, we can observe that the number of passes for preprocessing decreases as a function of \(\), while the number of passes for sampling increases. Concerning Approx-DD-Heuristic, both the number of passes and memory usage are less sensitive to \(\) since in Approx-DD-Heuristic the amount of available memory plays a more important role.

As observed, Approx-DD-Heuristic outperforms Approx-DD on real-world sparse graphs. Hence, we will focus our attention on Approx-DD-Heuristic in the following experiment.

**Running on Large Graphs.** Figure 2 shows several metrics as a function of \(\) and \(k\) over real-world large graphs. Figure 1(a) and 1(b) show that around 30 passes are sufficient to complete the whole sampling task, while the memory usage is indeed less than the total size of the edge list. Figure 1(c) and 1(d) show the number of trials to obtain 100 graphlets as a function of \(\) and \(k\), respectively. Observe that the success probability is significantly higher than the worst-case bound provided by

  Dataset & File Size (MB) & \#Vertices & \#Edges \\  Dense & 1,858 & 20,000 & 159,993,472 \\ NY Times  & 858 & 401,388 & 69,654,798 \\ Twitter (WWW)  & 20,437 & 41,652,230 & 1,202,513,047 \\ Twitter (MPI)  & 25,590 & 52,579,682 & 1,614,106,188 \\ Friendster  & 32,300 & 68,349,466 & 1,811,849,343 \\  

Table 1: Dataset statistics. Dense is generated synthetically by drawing each edge with probability 0.8, and other four datasets are from KONECT.

Lemma C.1. For example, the actual success probability in Twitter(WWW) with \(=1,k=4\) is \(100/31230 0.0032\), which is significantly larger than \(9.65 10^{-5}\) provided by Lemma C.1.

## 6 Conclusion and Future Work

We develop efficient semi-streaming algorithms for uniform graphlet sampling, requiring sublinear amount of memory and polylogarithmic number of passes in the size of the input data. We also provide a space lower bound showing that the tradeoff between memory and number of passes of our algorithms is near-optimal. Our theoretical results are complemented with an experimental evaluation on large real-world graphs showing the effectiveness of our algorithms and that they can provide a valuable tool in graph mining and data analysis. Our work paves the way for developing efficient algorithms in other computational model for large-scale processing, such as the MapReduce model.