# On the Implicit Bias of Linear Equivariant Steerable Networks

Ziyu Chen

Department of Mathematics and Statistics

University of Massachusetts Amherst

Amherst, MA 01003

ziyuchen@umass.edu

&Wei Zhu

Department of Mathematics and Statistics

University of Massachusetts Amherst

Amherst, MA 01003

weizhu@umass.edu

###### Abstract

We study the implicit bias of gradient flow on linear equivariant steerable networks in group-invariant binary classification. Our findings reveal that the parameterized predictor converges in direction to the unique group-invariant classifier with a maximum margin defined by the input group action. Under a unitary assumption on the input representation, we establish the equivalence between steerable networks and data augmentation. Furthermore, we demonstrate the improved margin and generalization bound of steerable networks over their non-invariant counterparts.

## 1 Introduction

Despite recent theoretical breakthroughs in deep learning, it is still largely unknown why overparameterized deep neural networks (DNNs) with infinitely many solutions achieving near-zero training error can effectively generalize on new data. However, the consistently impressive results of DNNs trained with first-order optimization methods, _e.g._, gradient descent (GD), suggest that the training algorithm is _implicitly guiding_ the model towards a solution with strong generalization performance.

Indeed, recent studies have shown that gradient-based training methods effectively regularize the solution by _implicitly minimizing_ a certain complexity measure of the model (Vardi, 2022). For example, Gunasekar et al. (2018) showed that in separable binary classification, the linear predictor parameterized by a linear fully-connected network trained under GD converges in the direction of a max-margin support vector machine (SVM), while linear convolutional networks are implicitly regularized by a depth-related bridge penalty in the Fourier domain. Yun et al. (2021) extended this finding to linear tensor networks. Lyu and Li (2020) and Ji and Telgarsky (2020) established the implicit max-margin regularization for (nonlinear) homogeneous DNNs in the parameter space.

On the other hand, another line of research aims to _explicitly regularize_ DNNs through architectural design to exploit the inherent structure of the learning problem. In recent years, there has been a growing interest in leveraging group symmetry for this purpose, given its prevalence in both scientific and engineering domains. A significant body of literature has been devoted to designing group-equivariant DNNs that ensure outputs transform covariantly to input symmetry transformations. _Group-equivariant steerable networks_ represent a general class of symmetry-preserving models that achieve equivariance with respect to any pair of input-output group actions (Cohen et al., 2019; Weiler and Cesa, 2019; Cohen and Welling, 2017). Empirical evidence suggests that equivariant steerable networks yield substantial improvements in generalization performance for learning tasks with group symmetry, especially when working with limited amounts of data.

There have been several recent attempts to account for the empirical success of equivariant steerable networks through establishing a tighter upper bound on the test risk for these models. This is typically accomplished by evaluating the complexity measures of equivariant and non-equivariant models under the same norm constraint on the network parameters (Sokolic et al., 2017; Sannaiet al., 2021; Elesedy, 2022). Nevertheless, it remains unclear whether or why a GD-trained steerable network can achieve a minimizer with a parameter norm comparable to that of its non-equivariant counterpart. Consequently, the effectiveness of such complexity-measure-based arguments to explain the generalization enhancement of steerable networks in group symmetric learning tasks may not be directly applicable.

In light of the above issues, in this work, we aim to fully characterize the implicit bias of the training algorithm on linear equivariant steerable networks in group-invariant binary classification. Our result shows that when trained under gradient flow (GF), _i.e._, GD with an infinitesimal step size, the steerable-network-parameterized predictor converges in direction to the unique group-invariant classifier attaining a maximum margin with respect to a norm defined by the input group representation. This result has three important implications: under a unitary input group action,

* a linear steerable network trained on the _original_ data set converge in the same direction as a linear fully-connected network trained on the _group-augmented_ data set. This suggests the equivalence between training with linear steerable networks and _data augmentation_;
* when trained on the same _original_ data set, a linear steerable network always attains a wider margin on the _group-augmented_ data set compared to a fully-connected network;
* when the underlying distribution is group-invariant, a GF-trained linear steerable network achieves a tighter generalization bound compared to its non-equivariant counterpart. This improvement in generalization is not necessarily dependent on the group size, but rather it depends on the support of the invariant distribution.

Before we end this section, we note that a similar topic has recently been explored by Lawrence et al. (2021) in the context of linear Group Convolutional Neural Networks (G-CNNs), a special case of the equivariant steerable networks considered in this work. However, we point out that the models they studied were not truly group-invariant, and thus their implicit bias result does not explain the improved generalization of G-CNNs. We will further elaborate on the comparison between our work and (Lawrence et al., 2021) in Section 2.

## 2 Related work

**Implicit biases**: Recent studies have shown that for linear regression with the logistic or exponential loss on linearly separable data, the linear predictor under GD/SGD converges in direction to the max-\(L^{2}\)-margin SVM (Soudry et al., 2018; Nacson et al., 2019; Gunasekar et al., 2018). These results are extended to linear fully-connected networks and linear Convolutional Neural Networks (CNNs) by Gunasekar et al. (2018) under the assumption of directional convergence and alignment of the network parameters, which are later proved by Ji and Telgarsky (2019, 2020); Lyu and Li (2020); Ji and Telgarsky (2020). The implicit regularization of gradient flow (GF) is further generalized to linear tensor networks by Yun et al. (2021). For overparameterized nonlinear networks in the infinite-width regime, rigorous analysis on the optimization of DNNs has also been studied from the neural tangent kernel (Jacot et al., 2018; Du et al., 2019; Allen-Zhu et al., 2019) and the mean-field perspectives (Mei et al., 2019; Chizat and Bach, 2018). However, these models are not explicitly designed to be group-invariant/equivariant, and the implicit bias of gradient-based methods might guide them to converge to sub-optimal solutions in learning tasks with intrinsic group symmetry.

**Equivariant neural networks.** Since their introduction by Cohen and Welling (2016), group equivariant network design has become a burgeoning field with numerous applications from computer vision to scientific computing. Unlike the implicit bias induced by training algorithms, equivariant networks _explicitly_ incorporate symmetry priors into model design through either group convolutions (Cheng et al., 2019; Weiler et al., 2018; Sosnovik et al., 2020; Zhu et al., 2022) or, more generally, steerable convolutions (Weiler and Cesa, 2019; Cohen et al., 2019; Worrall et al., 2017). Despite their empirical success, it remains largely unknown why and whether equivariant networks trained under gradient-based methods actually converge to solutions with smaller test risk in group-symmetric learning tasks (Sokolic et al., 2017; Sannai et al., 2021). In a recent study, Elesedy and Zaidi (2021) demonstrated a provably strict generalisation benefit for equivariant networks in linear regression. However, the network studied therein is non-equivariant and only symmetrized after training; such test-time data augmentation is different from practical usage of equivariant networks.

Comparison to Lawrence et al. (2021).A recent study by Lawrence et al. (2021) also analyzes the implicit bias of linear equivariant G-CNNs, which are a special case of the steerable networks considered in this work. However, the networks studied therein are not truly equivariant/invariant. More specifically, the input space \(_{0}\) they considered is the set of functions on the group \(G\), _i.e._, \(_{0}=\{f:G\}\), and the input \(G\)-action is given by the regular representation

\[[_{0}(g)f](g^{})=f(g^{-1}g^{}), g G, f _{0}.\] (1)

In their case, due to the transitivity of the regular representation (1), \(G\)-invariant linear functions \(:_{0}\) are constrained to the (trivial) form:

\[(f)=C_{g G}f(g),\]

where \(C\) is a multiplicative constant. To avoid learning this trivial function, Lawrence et al. (2021) chose to parameterize \(\) using a linear G-CNN, in which the final layer is replaced by a fully-connected layer. While this fully-connected layer provides the capability to learn more complex and nontrivial functions, it simultaneously undermines the property of group invariance. Therefore, their implicit bias result does not explain the improved generalization of G-CNNs. In contrast, we assume Euclidean inputs with non-transitive group actions, allowing linear steerable networks to learn non-trivial group-invariant models.

## 3 Background and problem setup

We provide a brief background in group theory and group-equivariant steerable networks. We also explain the setup of our learning problem for group-invariant binary classification.

### Group and group equivariance

A _group_ is a set \(G\) equipped with a binary operator, the group product, satisfying the axioms of associativity, identity, and invertibility. We always assume in this work that \(G\) is finite, _i.e._, \(|G|<\), where \(|G|\) denotes its cardinality.

Given a vector space \(\), let \(()\) be the general linear group of \(\) consisting of all invertible linear transformations on \(\). A map \(:G()\) is called a _group representation_ (or _linear group action_) of \(G\) on \(\) if \(\) is a group homomorphism from \(G\) to \(()\), namely

\[(gh)=(g)(h)(), g,h G.\] (2)

When the representation \(\) is clear from the context, we also abbreviate the group action \((g)\) as \(g\).

Given a pair of vector spaces \(,\) and their respective \(G\)-representations \(_{}\) and \(_{}\), a linear map \(:\) is said to be \(G\)-_equivariant_ if it commutes with the \(G\)-representations \(_{}\) and \(_{}\), _i.e._,

\[_{}(g)=_{}(g), g G.\] (3)

Linear equivariant maps are also called _intertwiners_, and we denote by \(_{G}(_{},_{})\) the space of all intertwiners satisfying (3). When \(_{}\) is the trivial representation, then \([_{}(g)()]=[]\) for all \(\); namely, \(\) becomes a \(G\)-invariant linear map.

### Equivariant steerable networks and G-CNNs

Let \(_{0}=^{d_{0}}\) be a \(d_{0}\)-dimensional input Euclidean space, equipped with the usual inner product. Let \(_{0}:G(_{0})\) be a \(G\)-representation on \(_{0}\). Suppose we have an unknown target function \(f^{*}:_{0}=^{d_{0}}\) that is \(G\)-invariant under \(_{0}\), _i.e._, \(f^{*}(g) f^{*}(_{0}(g))=f^{*}()\) for all \(g G\) and \(_{0}\). The goal of equivariant steerable networks is to approximate \(f^{*}\) using an \(L\)-layer neural network \(f=_{L}_{L-1}_{1}\) that is guaranteed to be also \(G\)-invariant.

Since the composition of equivariant maps is also equivariant, it suffices to specify a collection of \(G\)-representation spaces \(\{(_{l},_{l})\}_{l=1}^{L}\), with \((_{L},_{L})=(,)\) being the trivial representation, such that each layer \(_{l}_{G}(_{l-1},_{L}):_{l-1} _{l}\) is \(G\)-equivariant. Equivalently, we want the following diagram to be commutative,

\[_{0}@>{_{1}}>{}>_{1}@>{_{2}}>{}> _{2}@>{_{3}}>{}> @>{_{L-1}}>{}>_{L-1}@>{_{L}}>{}> _{L}\\ @V{_{0}(g)}V{}V@V{_{1}(g)}V{}V@V{_{2}(g)}V{}V@V{_{L}(g)}V{ _{L}(g)}V@V{}V{}V@V{}V{_{L}(g)}V\\ _{0}@>{_{1}}>{}>_{1}@>{_{2}}>{}>_{2}@>{ _{3}}>{}> @>{_{L-1}}>{}>_{L-1}@>{_{L}}>{}>_{L}, g G.\] (4)

**Equivariant steerable networks.** Given the representations \(\{(_{l},_{l})\}_{l=0}^{L}\), a (linear) _steerable network_ is constructed as follows. For each \(l[L]\{1,,L\}\), we choose a finite collection of \(N_{l}\) (pre-computed) intertwiners \(\{_{l}^{j}\}_{j=1}^{N_{l}}_{G}(_{l-1},_{l})\). Typically, \(\{_{l}^{j}\}_{j=1}^{N_{l}}\) is a basis of \(_{G}(_{l-1},_{l})\), but it is not necessary in our setting. The \(l\)-th layer equivariant map \(_{l}^{}:_{l-1}_{l}\) of a steerable network is then parameterized by

\[_{l}^{}(;_{l})=_{j[N_{l}]}w_{l}^{j }_{l}^{j}(),_{l-1},\] (5)

where the coefficients \(_{l}=[w_{l}^{j}]_{j[N_{l}]}^{}^{N_{l}}\) are the trainable parameters of the \(l\)-th layer. An \(L\)-layer linear steerable network \(f_{}(;)\) is then defined as the composition

\[f_{}(;)=_{L}^{}(_{ 2}^{}(_{1}^{}(;_{1});_{2});_{L}),\] (6)

where \(=[_{l}]_{l=1}^{L}_{l=1}^{L}^{N_{l}} _{}\) is the collection of all trainable parameters. The network \(f_{}(;)\) defined in (6) is referred to as a _steerable network_ because it steers the layer-wise output to transform according to any specified representation.

**G-CNNs.** A special case of the steerable networks is the _group convolutional neural network_ (G-CNN), wherein the hidden representation space \((_{l},_{l})\) for each \(l[L-1]\) is set to

\[_{l}=(^{d_{l}})^{G}=\{_{l}:G^{d_{l }}\},_{l}(g)_{l}(h)_{l}(g^{-1}h) ^{d_{1}}, g,h G.\] (7)

The representation \(_{l}(_{l})\) in (7) is known as the _regular representation_ of \(G\). Intuitively, \(_{l}_{l}\) can be viewed as a matrix of size \(d_{l}|G|\), and \(_{l}(g)\) is a permutation of the columns of \(_{l}\).

With this choice of \(\{(_{l},_{l})\}_{l=0}^{L}\), the first-layer equivariant map of a G-CNN is defined as

\[_{1}^{}(;_{1})(g)=_{1}^{}g ^{-1}^{d_{1}},^{d_ {0}},\] (8)

where \(_{1}=(w_{1}^{j,k})_{j,k}^{d_{0} d_{1}}\) are the trainable parameters of the first layer. Eq. (8) is called a _\(G\)-lifting map_ as it lifts a Euclidean signal \(^{d_{0}}\) to a function \(_{1}^{}(;_{1})\) on \(G\). For the subsequent layers, equivariance is achieved through _group convolutions_, and the readers are referred to the appendix for a detailed explanation.

**Assumptions on the representations.** The input representation \(_{0}(_{0}=^{d_{0}})\) is assumed to be given, and \(_{L} 1(_{L}=)\) is set to the trivial representation for group-invariant outputs. In this work, we make the following special choices for the first-layer representation \((_{1},_{1})\) as well as the equivariant map \(_{1}^{}(,_{1})\) in a general steerable network.

**Assumption 3.1**.: _We adopt the regular representation (7) for the first layer, and set the first-layer equivariant map \(_{1}^{}(;_{1})\) to the \(G\)-lifting map (8)._

**Remark 3.2**.: _The rationale of Assumption 3.1 is for the network to have enough capacity to parameterize any \(G\)-invariant linear classifier; this will be further explained in Proposition 3.6. We note that the representations \((_{l},_{l})\) and steerable maps \(_{l}^{}(,_{l})\) for all subsequent layers, \(l\{2,,L-1\}\), can be arbitrary._

Under Assumption 3.1, we can characterize the linear steerable networks \(f_{}(;)\) in the following proposition.

**Proposition 3.3**.: _Let \(f_{}(;)\) be the linear steerable network satisfying Assumption 3.1, where \(=[_{l}]_{l=1}^{L}_{}\) is the collection of all model parameters. There exists a multi-linear map \(M:(_{2},,_{L}) M(_{2},, _{L})^{d_{1}}\) such that for all \(^{d_{0}}\) and \(_{}\),_

\[f_{}(;)=f_{}(}; )=},_{1}M(_{2}, ,_{L}),\] (9)

_where \(}_{g G}g\) is the average of all elements on the group orbit of \(^{d_{0}}\)._Although a straightforward proof of Proposition 3.3 can be readily derived using Schur's Lemma, we have opted to include an elementary proof of Proposition 3.3 in the appendix for the sake of completeness. If we define

\[}_{}()_{1}M( _{2},,_{L}),_{}()(_{g G}g^{})}_{ }(),\] (10)

where \(g^{}_{0}(g)^{}\), then

\[f_{}(;)=,_{ }()=}, }_{}().\] (11)

By the multi-linearity of \(M(_{2},,_{L})\), one can verify that \(f_{}(;)\), \(_{}()\), and \(}_{}()\) are all \(L\)-_homogeneous_ in \(\); that is, for all \(>0\) and \(_{}\),

\[f_{}(;)=^{L}f_{}(; ),\;_{}()=^{L}_{ }(),\;}_{}()=^{L}}_{}().\] (12)

**Remark 3.4**.: _In comparison, an \(L\)-layer linear fully-connected network \(f_{}(;)\) is given by_

\[f_{}(;)=_{L}^{}_{L-1}^{ }_{1}^{}=, _{}(),_{}( )_{1}_{L}.\] (13)

_where \(=[_{l}]_{l=1}^{L}_{}( _{l=1}^{L-1}^{d_{l-1} d_{l}})^{d_{ L-1}}\). It is worth noting that when \(G=\{e\}\) is the trivial group, a linear fully-connected network \(f_{}(;)\) is identical with a linear G-CNN, which is a special case of linear steerable networks. See Remark A.1 for details._

### Group-invariant binary classification

Consider a binary classification data set \(S=\{(_{i},y_{i}):i[n]\}\), where \(_{i}^{d_{0}}\) and \(y_{i}\{ 1\}, i[n]\). We assume that \(S\) are i.i.d. samples from a \(G\)-_invariant distribution_\(\) defined below.

**Definition 3.5**.: _A distribution \(\) over \(^{d_{0}}\{ 1\}\) is said to be \(G\)-invariant with respect to a representation \(_{0}(^{d_{0}})\) if_

\[(_{0}(g))_{*}=,\; g  G,\] (14)

_where \((_{0}(g))(,y)(_{0} (g),y)\), and \((_{0}(g))_{*} (_{0}(g))^{-1}\) is the push-forward measure of \(\) under \(_{0}(g)\)._

It is easy to verify that the Bayes optimal classifier \(f^{*}:^{d_{0}}\{ 1\}\) (the one achieving the smallest population risk) for a \(G\)-invariant distribution \(\) is necessarily a \(G\)-invariant function, _i.e._, \(f^{*}(g)=f^{*}(), g G\). Therefore, to learn \(f^{*}\) using (linear) neural networks, it is natural to approximate \(f^{*}\) using an equivariant steerable network

\[f^{*}()(f_{}(; ))=(,_{}( )).\] (15)

After choosing the exponential loss \(_{}:\{ 1\}_{+},\;_{}(,y) (-y)\), as a surrogate loss function, the empirical risk minimization over \(S\) for the steerable network \(f_{}(;)\) becomes

\[_{_{}}_{_{}}(;S)_{i=1}^{n}_{}(_{i}, _{}(),y_{i})=_{i=1}^{n}_ {}(}_{i},}_{}(),y_{i}).\] (16)

On the other hand, since \(_{}()=^{d_{0}}\) always corresponds to a \(G\)-_invariant_ linear predictor--we have slightly abused the notation by identifying \(^{d_{0}}\) with the map \(,\)--one can alternatively consider the empirical risk minimization directly over the invariant linear predictors \(\):

\[_{^{d_{0}}_{G}}(; S)_{i=1}^{n}_{}(_{i}, ,y_{i}),\] (17)

where \(^{d_{0}}_{G}^{d_{0}}\) is the subspace of all \(G\)-invariant linear predictors, which is characterized by the following proposition.

**Proposition 3.6**.: _Let \(^{d_{0}}_{G}^{d_{0}}\) be the subspace of \(G\)-invariant linear predictors, i.e., \(^{d_{0}}_{G}=\{^{d_{0}}: ^{}=^{}g, ^{d_{0}}, g G\}\). Then_1. \(_{G}^{d_{0}}\) _is characterized by_ \[_{G}^{d_{0}}=_{g G}(I-g^{})=(_{g G}g^{}).\] (18)
2. _Let_ \(:^{d_{0}}^{d_{0}}\) _be the group-averaging map,_ \[()}=_{g G }g.\] (19) _Then its adjoint_ \(^{}:_{g G}g^{}\) _is a projection operator from_ \(^{d_{0}}\) _to_ \(_{G}^{d_{0}}\)_. In other words,_ \((^{})=_{G}^{d_{0}}\) _and_ \(^{}^{}=^{}\)_._
3. _If_ \(G\) _acts unitarily on_ \(_{0}\)_, i.e.,_ \(_{0}(g^{-1})=_{0}(g)^{}\)_, then_ \(=^{}\) _is self-adjoint. This implies that_ \(:}\) _is an orthogonal projection from_ \(^{d_{0}}\) _onto_ \(_{G}^{d_{0}}\)_. In particular, we have_ \[}=_{G}^{d_{0}}, \ \ \|}\|\|\|,^{d_{ 0}}.\] (20)

Proposition 3.6 combined with (10) demonstrates that a linear steerable network \(f_{}(;)=,_{}( )\) can realize any \(G\)-invariant linear predictor \(_{G}^{d_{0}}\); that is, \(\{_{}():_{}\}=_{G}^{d_{0}}\). Therefore (16) and (17) are equivalent optimization problems parameterized in different ways. However, minimizing (16) using gradient-based methods may potentially lead to different classifiers compared to those obtained from optimizing (17) directly.

**Gradient flow.** Given an initialization \((0)_{}\), the gradient flow \(\{(t)\}_{t 0}\) for (16) is the solution of the following ordinary differential equation (ODE),

\[}{t}=-_{}_{ _{}}(;S)=-_{}[_{i=1 }^{n}_{}(_{i},_{}( ),y_{i})].\] (21)

The purpose of this work is to inspect the asymptotic behavior of the \(G\)-invariant linear predictors \(_{}(t)=_{}((t))\) parameterized by the linear steerable network trained under gradient flow (21). In particular, we aim to analyze the directional limit of \(_{}(t)\) as \(t\), _i.e._, \(_{t}_{}(t)}{\|_{ }(t)\|}\). Before ending this section, we make the following assumption on the gradient flow \((t)\) that also appears in the prior works Ji and Telgarsky (2020); Lyu and Li (2020); Yun et al. (2021).

**Assumption 3.7**.: _The gradient flow \((t)\) satisfies \(_{_{}}((t_{0});S)<1\) for some \(t_{0}>0\)._

This assumption implies that the data set \(S=\{(_{i},y_{i}):i[n]\}\) can be separated by a \(G\)-invariant linear predictor \(_{}((t_{0}))\), and our analysis is focused on the "late phase" of the gradient flow training as \(t\).

## 4 Implicit bias of linear steerable networks

Our main result on the implicit bias of gradient flow on linear steerable networks in binary classification is summarized in the following theorem.

**Theorem 4.1**.: _Under Assumption 3.1 and Assumption 3.7, let \(_{}(t)=_{}((t))\) be the time-evolution of the \(G\)-invariant linear predictors parameterized by a linear steerable network trained with gradient flow on the data set \(S=\{(_{i},y_{i}):i[n]\}\); cf. Eq. (21). Then_

1. _The directional limit_ \(_{}^{}=_{t}_{ }(t)}{\|_{}(t)\|}\) _exists and_ \(_{}^{}_{g G}g^{} ^{}\)_, where_ \(^{}\) _is the max-_\(L^{2}\)_-margin SVM solution for the_ _transformed_ _data_ \(=\{(}_{i},y_{i}):i[n]\}\)_:_ \[^{}=_{^{d_{0}}}\| \|^{2},\ y_{i}}_{i}, 1, i[n].\] (22) _Furthermore, if_ \(G\) _acts unitarily on the input space_ \(_{0}\)_, i.e.,_ \(g^{-1}=g^{}\)_, then_ \(_{}^{}^{}\)_._2. Equivalently,_ \(^{}_{}\) _is proportional to the unique minimizer_ \(^{*}\) _of the problem_ \[^{*}=_{^{d_{0}}}\|_{()}\|^{2},\ ^{d_{0}}_{G},y_{i} _{i}, 1, i[n],\] (23) _where_ \(_{()}\) _is the projection from_ \(^{d_{0}}\) _to_ \((_{g G}g)\)_. Moreover, if_ \(G\) _acts unitarily on_ \(_{0}\)_, then_ \[^{}_{}^{*}=_{ ^{d_{0}}}\|\|^{2},\ ^{d_{0}}_{G},y_{i} _{i}, 1, i[n].\] (24) _Namely,_ \(^{}_{}\) _achieves the maximum_ \(L^{2}\)_-margin among all_ \(G\)_-invariant linear predictors._

Theorem 4.1 suggests that gradient flow implicitly guides a linear steerable network toward the unique \(G\)-invariant classifier with a maximum margin defined by the input representation \(_{0}\).

**Remark 4.2**.: _According to Remark 3.4, when \(G=\{e\}\) is a trivial group, then a linear \(G\)-CNN (which is a special case of linear steerable networks) reduces to a fully-connected network \(f_{}(;)\). Since the representation of a trivial group is always unitary, we have the following corollary which also appeared in Ji and Telgarsky (2020) and Yun et al. (2021)._

**Corollary 4.3**.: _Let \(\{(t)\}_{t 0}_{}\) be the gradient flow of the parameters when training a linear fully-connected network on the data set \(S=\{(_{i},y_{i}),i[n]\}\), i.e.,_

\[}{t}=-_{}_{ _{}}(;S)-_{}[ _{i=1}^{n}_{}(_{i},_{ }(),y_{i})].\] (25)

_Then the classifier \(_{}(t)=_{}((t))\) converges in a direction that aligns with the max-\(L^{2}\)-margin SVM solution \(^{*}\) for the **original** data set \(S=\{(_{i},y_{i}):i[n]\}\),_

\[^{*}=_{^{d_{0}}}\|\|^{2},\ y_{i}_{i},  1, i[n].\] (26)

**Remark 4.4**.: _While Theorem 4.1 provides a complete characterization of the implicit bias exhibited by gradient flow in linear steerable networks, it is imperative to note that the convergence rate to the directional limit is, in fact, exponentially slow. This is consistent with the findings in, e.g., (Soudry et al., 2018; Yun et al., 2021). A comprehensive analysis of gradient flow behavior in a non-asymptotic regime falls outside the scope of this current study._

## 5 The equivalence between steerable networks and data augmentation

Compared to hard-wiring symmetry priors into model architectures through equivariant steerable networks, an alternative approach to incorporate symmetry into the learning process is by training a non-equivariant model with the aid of _data augmentation_. In this section, we demonstrate that these two approaches are equivalent for binary classification under a unitary assumption for \(_{0}\).

**Corollary 5.1**.: _Let \(^{}_{}=_{t}_{}(t)}{\|_{}(t)\|}\) be the directional limit of the linear predictor \(_{}(t)=_{}((t))\) parameterized by a linear steerable network trained using gradient flow on the **original** data set \(S=\{(_{i},y_{i}),i[n]\}\). Correspondingly, let \(^{}_{}=_{t}_{}(t)}{\|_{}(t)\|}\), \(_{}(t)=_{}((t))\) (13), be that of a linear fully-connected network trained on the **augmented** data set \(S_{}=\{(g_{i},y_{i}),i[n],g G\}\). If \(G\) acts unitarily on \(_{0}\), then_

\[^{}_{}=^{}_{}.\] (27)

_In other words, the effect of using a linear steerable network for group-invariant binary classification is exactly the same as conducting **data-augmentation** for non-invariant models._

**Remark 5.2**.: _The equivalence between data augmentation and training with a linear steerable network is valid only in an asymptotic sense, yet the underlying training dynamics differ substantially. Specifically, \(_{}(t)\) is assured to maintain \(G\)-invariance throughout the training process, whereas \(_{}(t)\) achieves \(G\)-invariance solely in the limiting case as \(t\). Moreover, the equivalence only holds for "full-batch" data-augmentation over the entire group orbits._

**Remark 5.3**.: _For Corollary 5.1 to hold, it is crucial that \(_{0}(_{0})\) is unitary, as otherwise the limit direction \(_{}^{}\) of a linear fully-connected network trained on the augmented data set is generally not \(G\)-invariant (and hence cannot be equal to \(_{}^{}_{G}^{d_{0}}\)); see Example 5.4._

**Example 5.4**.: Let \(G=_{2}=\{,\}\). Consider a (non-unitary) \(G\)-representation \(_{0}\) on \(_{0}=^{2}\),

\[_{0}()=1&0\\ 0&1,_{0}()=1&0\\ -1&1-1&0\\ 0&11&0\\ 1&1=-1&0\\ 2&1.\] (28)

Let \(S=\{(,y)\}=\{((1,2)^{},+1)\}\) be a training set with only one point. By Theorem 4.1, the limit direction \(_{}^{}\) of a linear-steerable-network-parameterized linear predictor satisfies

\[_{}^{}_{g G}g^{} ^{*}=0&1\\ 0&1^{*}=0&1\\ 0&10\\ =\\ ,\] (29)

where \(^{*}=(0,)^{}\) is the max-margin SVM solution for the transformed data set \(=\{(},y)\}=\{((0,3)^{},+1)\}\). On the contrary, by Corollary 4.3, the limit direction \(_{}^{}\) of a linear fully-connected network trained on the augmented data set \(S_{}=\{((1,2)^{},+1),((-1,4)^{},+1)\}\) aligns with the max-margin SVM solution \(_{}^{*}\) for \(S_{}\),

\[_{}^{}_{}^{*} =_{^{2}}\|\|^{2}, \;y, 1,( ,y) S_{}\] (30) \[=(0.2,0.4)^{}_{}^{}.\] (31)

However, we demonstrate below that the equivalence between linear steerable networks and data augmentation can be re-established for non-unitary \(_{0}\) by defining a new inner product on \(^{d_{0}}\),

\[,_{_{0}}^{ }(_{g G}_{0}(g)^{}_{0}(g))^{1/2} ,,_{0}=^{d_ {0}}.\] (32)

When \(_{0}\) is unitary, \(,_{_{0}}=,\) is the normal Euclidean inner product. With this new inner product, we modify the linear fully-connected network and its empirical loss on the augmented data set \(S_{}=\{(g_{i},y_{i}):g G,i[n]\}\) as

\[f_{}^{_{0}}(;) ,_{}()_{ _{0}},\] (33) \[_{_{}}^{_{0}}(;S_{ })_{g G}_{i=1}^{n}_{}(f_{}^{_{0}}(g_{i};),y_{i})=_{g G}_{i=1} ^{n}_{}( g_{i},_{}( )_{_{0}},y_{i}).\] (34)

The following corollary shows that, for non-unitary \(_{0}\) on \(_{0}\), the implicit bias of a linear steerable network is again the same as that of a modified linear fully-connected network \(f_{}^{_{0}}(;)\) trained under data augmentation.

**Corollary 5.5**.: _Let \(_{}^{}\) be the same as that in Corollary 5.1. Let \(_{}^{_{0},}=_{t}_{ }^{_{0}}(t)}{|_{}^{_{0}}(t)|}\) be the limit direction of \(_{}^{_{0}}(t)=_{}((t))\) under the gradient flow of the modified empirical loss \(_{_{}}^{_{0}}(;S_{})\) (34) for a linear fully-connected network on the augmented data set \(S_{}\). Then_

\[_{}^{}(_{g G} _{0}(g)^{}_{0}(g))^{1/2}_{}^{_{0}, }.\] (35)

_Consequently, we have \(,_{}^{} ,_{}^{_{0},}_{ _{0}}\) for all \(^{d_{0}}\)._

## 6 Improved margin and generalization

We demonstrate in this section the improved margin and generalization of linear steerable networks over their non-invariant counterparts. In what follows, we assume \(_{0}\) to be unitary.

The following theorem shows that the margin of a linear-steerable-network-parameterized predictor \(_{}^{}\) on the augmented data set \(S_{}\) is always larger than that of a linear fully-connected network \(_{}^{}\), suggesting improved \(L^{2}\)-robustness of the steerable-network-parameterized classifier.

**Theorem 6.1**.: _Let \(_{}^{}\) be the directional limit of a linear-steerable-network-parameterized predictor trained on the **original** data set \(S=\{(}_{i},y_{i}),i[n]\}\); let \(_{}^{}\) be that of a linear fully-connected network **also** trained on the same data set \(S\). Let \(M_{}\) and \(M_{}\), respectively, be the (signed) margin of \(_{}^{}\) and \(_{}^{}\) on the **augmented** data set \(S_{}=\{(g_{i},y_{i}):i[n],g G\}\), i.e.,_

\[M_{}_{i[n],g G}y_{i}_{}^{},g_{i}, M_{}_{i[n],g G}y_{i}_{}^{},g_{i}.\] (36)

_Then we always have \(M_{} M_{}\)._

Finally, we aim to quantify the improved generalization of linear steerable networks compared to fully-connected networks in binary classification of _linearly separable_ group-invariant distributions defined below.

**Definition 6.2**.: _A distribution \(\) on \(^{d_{0}}\{ 1\}\) is called linearly separable if there exists \(^{d_{0}}\) such that_

\[_{(,y)}[y , 1]=1.\] (37)

It is easy to verify (by Lemma F.1) that if \(\) is \(G\)-invariant and linearly separable, then \(\) can be separated by a \(G\)-invariant linear classifier. The following theorem establishes the generalization bound of linear steerable networks in separable group-invariant binary classification.

**Theorem 6.3**.: _Let \(\) be a \(G\)-invariant distribution over \(^{d_{0}}\{ 1\}\) that is linearly separable by an invariant classifier \(_{0}^{d_{0}}_{G}\). Define_

\[=\{r>0:\|}\| r1\}.\] (38)

_Let \(S=\{(_{i},y_{i})\}_{i=1}^{n}\) be i.i.d. samples from \(\), and let \(_{}^{}\) be the limit direction of a steerable-network-parameterized linear predictor trained using gradient flow on \(S\). Then, for any \(>0\), we have with probability at least \(1-\) (over random samples \(S^{n}\)) that_

\[_{(,y)}[y (,_{}^{})] \|_{0}\|}{}+}.\] (39)

**Remark 6.4**.: _In comparison, let \(_{}^{}\) be the limit direction of a fully-connected-network-parameterized linear predictor trained on \(S\). Then with probability at least \(1-\), we have_

\[_{(,y)}[y (,_{}^{})] _{0}\|}{}+},\] (40)

_where \(R=\{r>0:\|\| r1\}\). This is the classical generalization result for max-margin SVM (see, e.g., Shalev-Shwartz and Ben-David .) Eq. (40) can also be viewed as a special case of Eq. (39), as a fully-connected network is a G-CNN with \(G=\{e\}\) (cf. Remark 4.2), and therefore \(}=\) and \(R=\)._

_By Proposition 3.6, the map \(}\) is an orthogonal projection, and thus we always have \( R\). Therefore the generalization bound for steerable networks in (39) is always smaller than that of the fully-connected network in (40)._

**Remark 6.5**.: _A comparison between Eq. (39) and Eq. (40) reveals that the improved generalization of linear steerable network does not necessarily depend on the group size \(|G|\). Instead, it depends on how far the distribution \(\)'s support is from the subspace \(^{d_{0}}_{G}\), such that \(\) could be much smaller than \(R\). In fact, if the support of \(\) is contained in \(R^{d_{0}}_{G}\), then \(=R\), and the steerable network does not achieve any generalization gain. This is consistent with Theorem 4.1, as in this case, the transformed data set \(=\{(}_{i},y_{i}):i[n]\}\) is the same as the original data set \(S\)._

## 7 Conclusion and future work

In this work, we analyzed the implicit bias of gradient flow on general linear group-equivariant steerable networks in group-invariant binary classification. Our findings indicate that the parameterized predictor converges in a direction that aligns with the unique group-invariant classifier with a maximum margin that is dependent on the input representation. As a corollary of our main result,we established the equivalence between data augmentation and learning with steerable networks in our setting. Finally, we demonstrated that linear steerable networks outperform their non-invariant counterparts in terms of improved margin and generalization bound.

A limitation of our result is that the implicit bias of gradient flow studied herein holds in an asymptotic sense, and the convergence rate to the directional limit might be extremely slow. This is consistent with the findings in, _e.g._, (Soudry et al., 2018; Yun et al., 2021). Understanding the behavior of gradient flow in a non-asymptotic regime is an important direction for future work. Furthermore, in our current framework, we assume that the first-layer equivariant map is represented by the \(G\)-lifting map. This assumption ensures that the linear steerable network possesses sufficient capacity to parameterize all \(G\)-invariant linear classifiers. Exploring the implicit bias of steerable networks without this assumption would be a compelling next step. The removal of this constraint could facilitate the generalization of our findings from finite groups to compact groups.