# Scaling Laws with Vocabulary:

Larger Models Deserve Larger Vocabularies

Chaofan Tao\({}^{1,2}\) Qian Liu\({}^{2}\) Longxu Dou\({}^{2}\) Niklas Muennighoff\({}^{3,4}\)

Zhongwei Wan\({}^{5}\) Ping Luo\({}^{1}\) Min Lin\({}^{2}\) Ngai Wong\({}^{1}\)

Corresponding authors. The project was done during Chaofan Tao's internship at Sea AI Lab. For more information, please contact cftao@connect.hku.hk and liuqian.sea@gmail.com. \({}^{1}\)The University of Hong Kong \({}^{2}\)Sea AI Lab \({}^{3}\)Contextual AI

\({}^{4}\)Stanford University \({}^{5}\)The Ohio State University

###### Abstract

Research on scaling large language models (LLMs) has primarily focused on model parameters and training data size, overlooking the role of vocabulary size. We investigate how vocabulary size impacts LLM scaling laws by training models ranging from 33M to 3B parameters on up to 500B characters with various vocabulary configurations. We propose three complementary approaches for predicting the compute-optimal vocabulary size: IsoFLOPs analysis, derivative estimation, and parametric fit of the loss function. Our approaches converge on the conclusion that the optimal vocabulary size depends on the compute budget, with larger models requiring larger vocabularies. Most LLMs, however, use insufficient vocabulary sizes. For example, we predict that the optimal vocabulary size of LLama2-70B should have been at least 216K, 7 times larger than its vocabulary of 32K. We validate our predictions empirically by training models with 3B parameters across different FLOPs budgets. Adopting our predicted optimal vocabulary size consistently improves downstream performance over commonly used vocabulary sizes. By increasing the vocabulary size from the conventional 32K to 43K, we improve performance on ARC-Challenge from 29.1 to 32.0 with the same 2.3e21 FLOPs. Our work highlights the importance of jointly considering tokenization and model scaling for efficient pre-training. The code and demo are available at https://github.com/sail-sg/scaling-with-vocab and https://hf.co/spaces/sail/scaling-with-vocab-demo.

## 1 Introduction

Large language models (LLMs) achieve remarkable performance by pre-training on vast text corpora using massive computational resources . Extensive prior work on LLMs has focused on deriving so-called scaling laws: a set of empirical formulas to predict how model performance scales, mainly as computing floating-point operations (FLOPs), model parameters, and quantity of training data change . These works show that power-law fits can effectively predict language modeling loss and by extension downstream performance . However, these scaling laws usually disregard the impact of the vocabulary size. For example, in Kaplan et al.  only non-vocabulary parameters are considered in their predictive formula. This negligence has resulted in substantial variability in the vocabulary size of current LLMs. For instance, Llama2-7B employs a vocabulary size of 32K , while Gemma-7B  adopts a much larger vocabulary size of 256K despite both having a similar number of total parameters. This variability in vocabulary sizes across LLMs raises the research question: _What is the compute-optimal vocabulary size for a LLM?_The vocabulary size affects performance non-trivially. Intuitively, the optimal vocabulary size should neither be too large nor small. A larger vocabulary size improves tokenization fertility, i.e., splitting sentences into fewer tokens, thereby improving the tokenization efficiency. Additionally, a larger vocabulary enables the model to capture a wider range of concept. However, the risk of under-fitting for rare tokens increases with larger vocabulary sizes, especially in the data-constrained regime . Thus, the optimal vocabulary size needs to be determined by taking the training data and the non-vocabulary parameters into account.

In this paper, we show that the effect of vocabulary on scaling laws has been underestimated, and we quantify the effect to derive a prediction for the optimal vocabulary size. We first introduce a normalized loss formulation to ensure a fair comparison across models with varying vocabulary sizes. Utilizing the normalized loss function, we analyze and discuss the underlying rationale behind the existence of an optimal vocabulary size, which depends on the available computational budget.

To predict the optimal vocabulary size given a compute budget, we propose three approaches. **Approach 1 (Estimating power laws via IsoFLOPs)**: We pre-train models with non-vocabulary parameters ranging from 33M to 1.13B, with groups of models that share the same FLOPs ("IsoFLOPs") but varying vocabulary configurations. Then, we fit power laws relating FLOPs to non-vocabulary parameters, vocabulary parameters, and training data, respectively. Our analysis reveals that the optimal vocabulary parameters exhibit a power-law growth with respect to the computational budget, however, at a slower rate than non-vocabulary parameters, as shown in Figure 1. **Approach 2 (Derivative-based Estimation)**: We introduce a derivative-based method that estimates the optimal vocabulary size by using the derivative of FLOPs w.r.t. the vocabulary size and finding the corresponding zero solution. **Approach 3 (Parametric Fit of Loss Formula)**: We modify Chinchilla scaling laws  to incorporate vocabulary and fit the resulting formula on our models to predict the normalized loss function based on non-vocabulary parameters, vocabulary parameters, and the amount of training characters jointly. While the prior two approaches are limited to compute-optimal settings, this approach also allows us to determine the optimal vocabulary when the allocation is suboptimal i.e. the model parameters are either trained for too many tokens ("overtrained") or for too few tokens ("undertrained"). Overtraining is very common , such as Llama 2 7B  which was trained for 2 trillion tokens, significantly more than the compute-optimal allocation of a 7 billion parameter model of around 150B tokens.

As shown in Figure 1, we observe that the relationship between non-vocabulary parameters \(N_{}\) and their correspondng optimal vocabulary parameters \(N_{}^{}\) follows a power law, according to all of our approaches. Our prediction also suggests that vocabulary parameters should be scaled slower than non-vocabulary parameters, i.e., \(N_{}^{} N_{nv}^{}\) where \( 0.83<1\). Nevertheless, most of existing LLMs  neglect the importance of vocabulary and allocate less vocabulary parameters than the suggestions, shown in Figure 2. Note that we assume that

Figure 1: The relationship between non-vocabulary parameters \(N_{}\) and the corresponding optimal vocabulary parameters \(N_{}^{}\) follows a power law, where \(N_{}^{}\) should be scaled slower than \(N_{}\) as \(<1\). Empirical results align with predictions of our proposed approaches, with larger circles indicating higher loss values. Here \(V\) refers to the vocabulary size i.e. the number of distinct tokens.

the amount of training data for these models is optimally distributed according to Hoffmann et al. . Considering that several LLMs are trained on substantially more data than optimal ones (e.g., Llama2), the optimal vocabulary sizes would likely be larger than currently estimated.

Finally, we empirically verify our predictions on models with 3B parameter models. By using our approach to predict the expected vocabulary size in various practical cases when (1) the training data is insufficient ("undertraining"); (2) the training data is equally scaled with the model parameters, following the Chinchilla laws ("compute-optimal training") ; (3) the training data is overly sufficient like in Llama  ("overtraining"). The results show that models with our suggested vocabulary sizes steadily outperform baselines adopting commonly used vocabulary configurations under the same FLOPs budget. Our research underscores the overlooked importance of vocabulary and the need to jointly consider the vocabulary size, model parameters, and training data for effective scaling.

## 2 Preliminary

In this section, we first present a general formulation of a commonly used scaling law, and then demonstrate how to modify it to incorporate the vocabulary.

### Scaling law

Scaling laws consider a computational budget, \(C\), which is measured in FLOPs. The goal is to optimally allocate the compute budget to model parameters \(N\) and the number of training tokens \(D\)[30; 6; 26; 44]. It can be formulated as:

\[(N^{},D^{})=_{N,D}(N,D)(N,D)=C,\] (1)

Following Radford et al. , the loss function is typically the language modeling loss when evaluating language models, which can be written as:

\[=-_{i=1}^{T} p(w_{i}|w_{1:i-1},V),\] (2)

where \(p(w_{i}|w_{1:i-1},V)\) is the output probability of word \(w_{i}\) given the context \(w_{1:i-1}\) and the tokenizer with vocabulary size \(V\). Generally, the lower \(\) indicates better performance of the language model. However, due to its dependency on \(V\), \(\) cannot be used to compare language models with different vocabulary sizes. Thus, we propose an adaptation later in SS2.2. Fitting scaling laws generally requires various models trained for different configurations . A common approach is to select several

Figure 2: Vocabulary parameters of popular LLMs and predicted optimal vocabulary parameters at a **compute-optimal number of training tokens**. Most current LLMs have suboptimal vocabulary parameters due to vocabulary sizes, which are smaller than the predicted optimal values. Among the current models, StarCoder2-3B, OLMo-7B, InternLM2-20B, and Gemma2-27B have vocabulary sizes that come closest to the optimal allocation for their respective model sizes.

compute budgets and train models with varying \(N\) and \(D\) for each budget to find the best one, i.e. the one with the lowest loss ("IsoFLOPs") . Using fitting techniques we can then estimate a function that maps from the compute budget to the optimal allocation to \(N\) and \(D\).

### Scaling law with vocabulary

As prior work generally assumes the vocabulary size to be fixed, we cannot adopt the attributes in their scaling laws and their evaluation metric directly. Thus, we detail several considerations that allow us to investigate vocabulary scaling laws.

**Attributes** Scaling laws commonly deal with the attributes, model parameters (\(N\)) and number of training tokens (\(D\)) . We adapt them for our analysis in the context of vocabulary size. (1) We break down the total model parameters (\(N\)) into non-vocabulary (\(N_{}\)) and vocabulary parameters (\(N_{}\)). To understand the importance of vocabulary parameters, we isolate them from other model parameters, where \(N=N_{}+N_{}\). We use \(N_{}=Vd\) to represent both the vocabulary parameters in the output layer 1. Notably, to change \(N_{}\) we only vary the vocabulary size \(V\) and take the embedding dimension \(d\) as given based on \(N_{}\) empirically, see SSA.7.2 for details. This is based on the observation by Kaplan et al.  that the performance of models with varying depth-to-width ratios converges to a single trend. We also provide further analysis about why we break down the model parameters from the perspective of parameter growing in SSA.6. (2) We measure data not in tokens (\(D\)) but in training characters (\(H\)). The number of tokens depends on the vocabulary of the tokenizer. By studying training characters, we can better see how the data volume affects the performance regardless of different vocabulary sizes.

**Mapping from training characters (\(H\)) to tokens (\(D\))** As detailed above we measure training data in training characters (\(H\)). Nonetheless, to connect our findings with existing studies on scaling laws , we need to be able to map from \(H\) to \(D\). This mapping is the tokenizer's compression ratio which can be computed via \(D/H\). The more tokens the tokenizer needs to represent \(H\), the larger \(D\), and thus it compresses less. We develop a simple function \(f(V)\) to estimate this ratio solely from the chosen vocabulary size, \(V\). Specifically, we find that a quadratic function on the logarithmic value of \(V\) achieves accurate predictions:

\[f(V)=a^{2}(V)+b(V)+c\] (3)

By fitting several tokenizers with \(V\) ranging from \(1K\) to \(1024K\), we obtain \(a=0.0064\), \(b=-0.1581\) and \(c=1.2047\). We find that our function accurately predicts the compression ratio with a low relative mean square error (RMSE) and a high coefficient of determination (\(R^{2}\)). In SSA.9, we visualize fitting results and show that our approximation works with different tokenizers and is robust to different \(V\). For all our main experiments, we use the BPE algorithm for tokenization .

**Vocabulary-insensitive loss** To fairly assess models that vary in \(V\), the commonly used language model loss in Equation 2 is inappropriate. Models trained with larger \(V\) naturally have a higher loss, as there are more possibilities in the vocabulary to predict. However, this does not mean that the model is worse. Thus, we need to normalize the loss with respect to the vocabulary size. We reformulate the unigram-normalized metric  as a loss function. Suppose we have a \(T\)-length sequence \(w_{1:T}\), we design the unigram-normalized language model loss as:

\[_{u}=-_{i=1}^{T}|w_{1:i-1},V)}{p(w_ {i}|V)},\] (4)

where \(p(w_{i}|V)\) is the frequency of word \(w_{i}\) in the tokenized corpus, given the tokenizer with vocabulary size \(V\). The loss indicates the improvement in probability that a context-aware language model offers over a unigram model without context, allowing us to assess the language model's efficacy. Based on theory from prior work , the normalized loss \(_{u}\) remains consistent for a given model with a fixed non-vocabulary component across different vocabulary sizes. The difference of \(_{u}\) comes from the ability of the language model itself. Compared with \(\), the value of \(_{u}\) is much smaller and can be negative as \(_{u}\) adds a negative term \(_{i=1}^{T} p(w_{i}|V)\). One may also employ the average bits per character (BPC), a common metric for text compression , as the vocabulary-insensitive loss. The only difference lies in the normalization. BPC represents the raw per-character language model loss over the corpus, while our \(_{u}\) is equivalent to the per-character language model loss normalized by the frequency of each character. In practice, we find that the metric BPC and \(_{u}\) show a significant positive correlation, which experimentally validated our statement, as detailed in the SSA.5.

## 3 Analysis: Why the optimal vocabulary size is bounded by compute

Analysis 1: The perspective of fixed normalized lossAccording to Kaplan et al. , the FLOPs (\(C\)) of a Transformer model can be estimated as \(C 6ND\), which can be re-written as:

\[C 6ND 6(N_{}+Vd)Hf(V),\] (5)

where \(N=N_{}+N_{}\) and \(D=Hf(V)\) based on SS2.2. The reasons why model performance first increases and then decreases as the vocabulary size grows are: (1) At small \(V\), increasing the vocabulary size easily improves tokenization fertility from \(f(V)\). Subsequently, more characters can be learned from the model with a fixed number of tokens, thereby improving model performance. (2) At very large \(V\), the gain from tokenization fertility decreases, while the parameters from expanding the vocabulary cannot be adequately trained with limited data, which leads to a decline in model performance. We present an expanded derivation in SSA.1, and show how the corresponding FLOPs change with the vocabulary size in Figure 3 (left).

Analysis 2: The perspective of fixed FLOP budgetGiven a fixed FLOPs budget, we isolate the FLOPs and investigate how the vocabulary influences the loss. For ease, we train models with fixed \(N_{nv}\) and different vocabulary sizes for the same steps, and then we use interpolation to predict the loss when FLOPs reaches the budget given the observed FLOPs and loss points. For each budget, we adopt a group of models with similar total parameters and vary vocabulary sizes. In Figure 3 (right) we plot the relationship between the loss w.r.t. the vocabulary size. It reveals that the vocabulary corresponding to the lowest point on the loss curve increases as the FLOPs budget increases. This suggests that with more computational resources, LLMs can harness larger vocabularies to reduce loss. However, merely expanding the vocabulary does not always lower the loss. For a fixed FLOPs budget, the loss initially decreases with the increase in vocabulary and then starts to rise, indicating that an optimal point exists for the vocabulary.

## 4 Estimating the optimal vocabulary size

In this section, we describe three complementary approaches to estimate the optimal vocabulary size.

Figure 3: **Left:** FLOPs curve with various vocabulary sizes, assuming all configurations achieve a fixed loss. There exists an optimal vocabulary size that minimizes FLOPs. **Right:** Loss curves with various vocabulary sizes given different FLOP budgets. For each budget there exists an optimal vocabulary size that minimizes loss. As the FLOP budget increases this optimal vocabulary size increases (shifts to the right).

### Approach 1: Estimating power laws via IsoFLOPs

We define 6 groups of models with \(N_{}\) ranging from 33M to 1.13B. Within each group, we solely vary the vocabulary size \(V\) from \(4K\) to \(96K\), and evaluate different models under the same FLOPs budget. We evaluate the normalized loss \(_{u}\) on a held-out validation dataset. This approach allows us to directly answer the question: For a given FLOPs budget, what is the optimal allocation to non-vocabulary parameters, vocabulary parameters, and training data?

**Results and Usage** In Figure 5, we display the fitted power laws: \(N_{}=0.08*C^{0.50}\), \(N_{}=0.20*C^{0.42}\) and \(H=6.42*C^{0.50}\), where \(C\) is the FLOPs budget.The low \(\) and high \(R^{2}\) values indicate the strength of our fit. Given a certain FLOPs budget, we can utilize the aforementioned relationships to obtain the optimal allocation (\(N_{}\), \(N_{}\), \(H\)). We also draw the following conclusions: **(1) LLMs are data-hungry.** Compared to the non-vocabulary parameters \(N_{}\), practitioners should allocate more compute to the training data . **(2) Vocabulary parameters scale in a power-law relation with FLOPs (\(N_{} C^{0.42}\)).** As models become more computationally intensive, a larger vocabulary enhances the model's ability to understand a more diverse array of text, and thus the vocabulary size is critical to scaling. **(3) Vocabulary parameters \(N_{}\) should be scaled slower than non-vocabulary parameters \(N_{}\).** This difference can be seen in their power law exponents, i.e. \(=0.42/0.50=0.84<1\). We hypothesize the reason is that: once a sufficiently rich embedding space is present via a large vocabulary, it is more critical to scale non-vocabulary parameters to learn the intricate syntactic and semantic structures of language via Transformer blocks.

Figure 4: Training curves of the experiments used in Approach 1 (§4.1) and Approach 3 (§4.3). We train models with the non-vocabulary parameters fixed and vocabulary sizes varying from 4K to 96K.

Figure 5: Fitting results of the Approach 1. Blue stars denote the selected data points where the combination (\(N_{}\), \(N_{}\), \(H\)) reaches the lowest loss given various FLOPs budgets. We find power law fits with respect to the optimal non-vocabulary parameters, vocabulary parameters, and the number of training characters, respectively.

### Approach 2: Derivative-based fast estimation

We propose an alternative approach leveraging insights from the estimation of the FLOPs itself. Prior work [26; 30] usually considers a fixed compute budget in FLOPs and then aims to minimize loss by finding the optimal allocation to model parameters \(N\) and training tokens \(D\). Here we flip this recipe on its head following recent work . We aim to find the minimum FLOPs to achieve a certain loss \(_{u}(N_{},V,H)=\) through optimal allocation of the vocabulary size \(V\):

\[V=*{arg\,min}_{V|_{u}(N_{},V,H)=}C(N_{ },N_{v},H).\] (6)

By computing the minimum point of FLOPs \(C\) with respect to \(V\) via derivative:

\[=6H(N_{}+Vd)+a((V))^{2}+b(V)+c\,d,\] (7)

we can estimate the optimal \(V\) under the assumption that it can achieve a certain loss \(_{u}(N_{},V,H)=\). The parameters \(a\), \(b\) and \(c\) can be easily obtained from building \(f(V)\) (SS2.2). In theory, as long as the non-vocabulary parameters \(N_{}\) are provided, \(V\) can be numerically searched via the solution of \(=0\). More details are in SSA.1.

UsageWhen the compute allocation is near optimal, the loss exhibits a power-law relationship with respect to the FLOPs budget, as described by the scaling law . This relationship allows us to use FLOPs as a reliable proxy for observing the scaling behavior of the optimal vocabulary parameters. In practice, we can first determine an empirically optimal vocabulary size in a low-cost setting (e.g., finding the compute-optimal vocabulary parameters on a small model). Then, we can scale the optimal vocabulary parameters proportionally based on \(\). Specifically, we obtain a set of derivative-optimal vocabulary parameters \(N_{v}\) for different non-vocabulary parameters \(N_{nv}\), represented as \((N_{}^{i},N_{}^{i})|i=1,,n}\). We then fit the relationship between \(N_{}\) and \(N_{}\) using the power-law function \(N_{} N_{}^{}\). This results in the scaling equation: \(N_{}/N_{}^{0}=(N_{}/N_{}^{0})^{}\) where \(N_{}^{0}\) is a small model (e.g., 33M), and \(N_{}^{0}\) is the searched optimal vocabulary parameter. By combining the \(\) from the derivative and the empirical solution on a small model, we can estimate the optimal vocabulary by:

\[N_{}^{}=N_{}^{0}*(}}{N_{}^{0}})^{},\]

where the scaling proportion \(=0.83\) after our fitting. Consistent with the observation in Approach 1, we find that non-vocabulary parameters should be scaled **faster** than vocabulary parameters to achieve an optimal allocation.

### Approach 3: Parametric fit of loss formula

Finally, we directly predict the loss given the non-vocabulary parameter, vocabulary parameter and the amount of training characters. Then, the optimal vocabulary configuration can be predicted by finding the minimum point of loss with respect to the vocabulary. Following a classical risk decomposition used in Hoffmann et al. , we design the vocabulary-dependent loss formula as:

\[_{u}=-E+}{N_{}^{_{1}}}+}{N_{ }^{_{2}}}+},\] (8)

where \(D=Hf(V)\). The first term captures the normalized loss for an ideal generative process. The subsequent terms reflect the effect of the non-vocabulary parameters, vocabulary parameters, and the number of training data on the loss, respectively. \(E,A_{1},A_{2},B,_{1},_{2},\) are learned parameters.

FittingWe use the points (\(N_{}\), \(N_{}\), \(H\)) collected for experiments in SS4.1. Note that we do not only consider the points with the lowest loss for each FLOP budget as we want to predict loss for any combination of (\(N_{}\), \(N_{}\), \(H\)). We add the constraint \(_{1}=\) following Muennighoff et al. . We also filter out points with very small FLOPs following Hoffmann et al. . Fitting yields \(A_{1}=1.831\), \(A_{2}=0.196\), \(B=2.124\), \(E=5.533\), \(_{1}==0.447\), \(_{2}=0.671\). The detailed fitting process is written in SSA.7.4.

UsageAfter fitting the parameters in Equation 8, the optimal vocabulary size can be obtained by finding the lowest loss w.r.t the vocabulary size, with a constraint of FLOPs budget. For example,given \(N_{}\) and FLOPs budget \(C\), by replacing \([Hf(V)]\) with \(C/(6(N_{}+N_{v}))\) and finding the solution of \(}{ V}=0\) via numerical search, we can get the prediction. The details of \(}{ V}\) is written in SSA.2. Note that all of the proposed approaches can be used in optimally allocating (\(N_{},N_{},H\)) altogether, while Approach 3 is more flexible in predicting the locally optimal \(N_{}\) when (\(N_{}\), \(H\)) are not following the Chinchilla's law , _i.e._ equally-scaled law. The reason is that the loss formula in Approach 3 does not only considers the combinations (\(N_{},N_{},H\)) which reach the optimal given a certain training budget. By fixing \(N_{}\) and varying \(C\) in Approach 3, we can predict the locally optimal vocabulary size with different amount of training characters. This property makes Approach 3 more valuable, since modern LLMs [70; 67; 3; 4; 7] usually leverage overly sufficient training data to build powerful models with relatively low inference costs.

In Figure 6, we remove the assumption  for the practical reason that the parameters and training data are not equally scaled. Then, we predict the locally optimal vocabulary parameters. It can be observed that the allocation of vocabulary parameters are typically under-estimated.

## 5 Discussion

Predicting allocations for larger modelsTable 1 reports the predicted optimal vocabulary parameters and sizes based on the proposed three approaches, where the amount of training data is optimally

   \(N_{}\) & \(N_{}^{}\)**-App1** & \(N_{}^{}\)**-App2** & \(N_{}^{}\)**-App3** & **Dim.** & \(V^{}\)**-App1** & \(V^{}\)**-App2** & \(V^{}\)**-App3** & **FLOPs Budget** \\ 
3B & 0.1B & 0.1B & 0.1B & 3200 & 39K & 43K & 37K & \(1.3e21\) \\
7B & 0.3B & 0.3B & 0.2B & 4096 & 62K & 67K & 60K & \(7.1e21\) \\
13B & 0.4B & 0.5B & 0.4B & 5120 & 83K & 91K & 81K & \(2.4e22\) \\
30B & 0.9B & 0.9B & 0.9B & 6048 & 142K & 154K & 142K & \(1.3e23\) \\
70B & 1.7B & 1.9B & 1.8B & 8192 & 212K & 231K & 218K & \(7.1e23\) \\
130B & 2.9B & 3.2B & 3.0B & 12888 & 237K & 258K & 248K & \(2.4e24\) \\
300B & 5.8B & 6.4B & 6.3B & 16384 & 356K & 389K & 383K & \(1.3e25\) \\   

Table 1: We report the predicted optimal vocabulary parameters \(N_{v}\) and the vocabulary size \(V\) by the proposed three approaches given \(N_{nv}\). We assume the training FLOPs are optimally allocated i.e. that the non-vocabulary parameters and training data are scaled equally. “App” denotes the approach.

Figure 6: Vocabulary parameters of popular LLMs and predicted optimal vocabulary parameters at **their reported number of training tokens**, as determined by our Approach 3 (§4.3). Here we consider the practical scenarios where parameters and training data are not necessarily equally scaled. As illustrated, the vocabulary parameters remain predominantly underestimated. With the exception of Gemma2-9B, all models allocate a smaller vocabulary parameter count than our prediction.

allocated, _i.e._ equally scaled with the non-vocabulary parameters . Aligned with the trend shown in Figure 1, _the predictions from all proposed approaches align closely_. \(N_{}\) should be scaled faster than \(N_{}\). Notably, mainstream LLMs typically assign fewer parameters to vocabulary than what is optimal. However, the community is starting to shift to larger vocabularies, such as with Llama3  having a 128K vocabulary size up from 32K of Llama2 . However, scaling data is still the most critical part, and solving data scarcity issues should be a focus of future work .

To empirically verify our prediction, we train models with \(N_{}=2.87B\) under a compute-optimal training FLOPs budget and evaluate them using lightest 2. For the baseline model we use the common vocabulary size of \(V=32K\). The other model uses \(V^{}\) as predicted by Approach 3. In Table 2, we show that the model allocated according to our vocabulary predictions yields better performance across multiple downstream tasks. This verifies that our predictions hold at scale.

Experiments with scarce and excessive training dataOur prior experiments focus on the setting where training compute budget is the main constraint and we seek to allocate it optimally to parameters and training data. This is the typical setting in scaling law studies . However, in the real world, we often deal with scarce data ("data-constrained ") forcing us to train sub-optimally or would like to make use of excessive data to train a smaller model that is cheaper to use . To verify that our Approach 3 can handle these practical scenarios, we compare the model with \(V=32K\) and the model with the vocabulary size \(V^{}\) predicted by Approach 3. As shown in Table 3, our prediction enables a better model by only adjusting the vocabulary size in different FLOPs budgets.

In Figure 7, we further study the trend about how does the optimal vocabulary size shift with different number of training data. We only vary the amount of data but keep the non-vocabulary parameters fixed. The choices of vocabulary size are \(8K\), \(10K\), \(16K\), \(24K\), \(32K\) and \(48K\). Taking \(N_{nv}=302M\) as an example, **when available data is the bottleneck, the optimal vocabulary size decreases empirically**, _i.e._\(16K 10K\). This is a mechanism to prevent over-fitting. Conversely, when training on excessive amounts of data, _e.g._, Llama3-8B uses much more training tokens than what would be compute-optimal for its budget, **the optimal vocabulary size increases**, _i.e._\(16K 24K\). Note that here we focus solely on training compute-optimal. It is also important to note that expanding the vocabulary size also increases the computational demands during inference. Therefore, we recommend **using the optimal vocabulary size corresponding to a given \(_{}\), assuming optimal allocation of training data**, even in scenarios where overtraining may occur.

    & \(N_{}\) & \(D\) & \(H\) & **ARC-C** & **ARC-E** & **Hellaswag** & **OBQA** & **WG** & **PIQA** & **BoolQ** & **Average** \\   \\ \(V\)=32K & 0.10B & 67.3B & 266.6B & 28.5\(\)1.3 & 49.2\(\)1.0 & 47.5\(\)0.5 & 31.6\(\)2.1 & 50.4\(\)1.4 & 71.4\(\)1.1 & 56.4\(\)0.9 & 47.9 \\ \(V^{opt}\)=35K & 0.11B & 67.1B & 268.2B & **29.1\(\)**1.3 & **50.6\(\)**1.0 & **48.1\(\)**0.5 & **31.6\(\)**2.1 & **51.9\(\)**1.4 & **71.4\(\)**1.1 & **57.1\(\)**0.9 & **48.5** \\   

Table 2: Zero-shot performance of models with \(N_{}=2.87B\) comparing the commonly used \(V=32K\) with our predicted optimal vocabulary \(V^{}\). We consider the scenario where the number of training data is equally scaled with the non-vocabulary parameters. We report accuracy and standard deviation in percentages. Accuracy is normalized: The predicted likelihoods are divided by the length of each choice for multiple choices to eliminate the effect of text length on predictions.

    & \(N_{}\) & \(D\) & \(H\) & **ARC-C** & **ARC-E** & **Hellaswag** & **OBQA** & **WG** & **PIQA** & **BoolQ** & **Average** \\   \\ \(V\)=32K & 0.10B & 15.7B & 62.2B & 23.6\(\)1.2 & 40.8\(\)1.0 & 34.4\(\)0.5 & **29.0\(\)**2.0 & 49.7\(\)1.4 & 64.9\(\)1.1 & 59.8\(\)0.9 & 43.2 \\ \(V^{}\)=24K & 0.08B & 15.8B & 60.8B & **24.2\(\)**1.3 & **42.2\(\)**1.0 & **36.0\(\)**0.5 & 28.6\(\)2.0 & **50.0\(\)**1.4 & **64.9\(\)**1.1 & **61.5\(\)**0.9 & **43.9** \\   \\ \(V\)=32K & 0.10B & 128.5B & 509.1B & 29.1\(\)1.3 & 53.5\(\)1.0 & 53.0\(\)0.5 & 33.0\(\)2.1 & 52.0\(\)1.4 & 72.0\(\)1.1 & 59.5\(\)0.9 & 50.3 \\ \(V^{}\)=43K & 0.14B & 127.0B & 517.5B & **32.0\(\)**1.4 & **54.7\(\)**1.0 & **54.1\(\)**0.5 & **33.0\(\)**2.1 & **52.8\(\)**1.4 & **72.6\(\)**1.0 & **61.9\(\)**0.9 & **51.6** \\   

Table 3: Zero-shot performance of models with \(N_{}=2.87B\) comparing the commonly used \(V=32K\) with our predicted optimal vocabulary \(V^{}\) when **undertraining** or **overtraining**.

## 6 Related work

Language modelsThe Transformer  has proven to be a scalable architecture for language models, especially large language model (LLMs) [11; 14; 52; 47; 20; 29; 53; 70; 73; 41; 8; 4; 38; 25; 62; 67; 7; 39; 32; 88]. These models typically acquire a deep understanding of language enabling them to perform multiple tasks after a pre-training period and an optional fine-tuning period. Their capabilities include code generation [33; 3; 43; 87; 86], mathematical reasoning [76; 5], question answering [48; 45] among others. Given the expensive deployment costs required by the language models, various techniques can be adopted for efficient inference [64; 65; 78; 74; 36]. In our work, we pre-train large language models from scratch on English corpora and focus on their validation loss and downstream performance after training.

Scaling lawsScaling laws aim to develop a predictive framework to find the best allocation of compute resources to maximize model performance. Besides language models, they have been studied in other domains[40; 68; 13]. For language models, Kaplan et al.  show that performance improves as a power law with more compute allocated to both parameters or data. Hoffmann et al.  show that the compute allocation of parameters and data should be scaled equally. Other work considers various cases such as downstream performance [23; 28; 55], inference time  or data constraints [44; 80]. However, the effect of vocabulary size has generally been ignored previously.

## 7 Conclusion

We investigate the impact of the vocabulary size in language models. We analyze and verify that there exists an optimal vocabulary size for a given FLOPs budget. Subsequently, we develop 3 approaches to predict the optimal vocabulary size. Our first approach uses empirical training runs across different IsoFLOPs regimes to fit a scaling law. The second approach investigates the FLOPs w.r.t. the vocabulary size and estimates the vocabulary size with derivatives. The third approach consists of a parametric function to predict the impact of different attributes on loss. Across all approaches, we find that while vocabulary parameters should be scaled slower than other parameters, they are still critical for performance and we can accurately predict their optimal allocation. We make predictions for larger models and empirically verify our approaches on up to 3B parameters and on varying amounts of training data. We show that models trained with an optimal vocabulary size as predicted by our approaches outperform models with a conventional vocabulary size under the same FLOPs budget.

Figure 7: **Left:** The heatmap illustrates how the best vocabulary size among all choices of vocabularies shifts with the training data. The non-vocabulary parameter is fixed (\(N_{nv}=302M\)). Each cell in the heatmap represents the loss given a certain FLOPs budget for a fair evaluation, with the color intensity indicating the loss value. The black line with markers denotes the best vocabulary size for each FLOPs budget, which basically increases as the number of training data increases. **Right:** The number of training tokens are slightly varying for different vocabulary sizes given a certain FLOPs budget. To keep FLOPs consistent, models with larger vocabulary sizes are trained on fewer tokens.