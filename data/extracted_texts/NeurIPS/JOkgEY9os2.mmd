# MMD-FUSE: Learning and Combining Kernels for Two-Sample Testing Without Data Splitting

Felix Biggs

Centre for Artificial Intelligence

Department of Computer Science

University College London & Inria London

contact@felixbiggs.com

&Antonin Schrab

Centre for Artificial Intelligence

Gatsby Computational Neuroscience Unit

University College London & Inria London

a.schrab@ucl.ac.uk

Equal contribution.

&Arthur Gretton

Gatsby Computational Neuroscience Unit

University College London

arthur.gretton@gmail.com

###### Abstract

We propose novel statistics which maximise the power of a two-sample test based on the Maximum Mean Discrepancy (MMD), by adapting over the set of kernels used in defining it. For finite sets, this reduces to combining (normalised) MMD values under each of these kernels via a weighted soft maximum. Exponential concentration bounds are proved for our proposed statistics under the null and alternative. We further show how these kernels can be chosen in a data-dependent but permutation-independent way, in a well-calibrated test, avoiding data splitting. This technique applies more broadly to general permutation-based MMD testing, and includes the use of deep kernels with features learnt using unsupervised models such as auto-encoders. We highlight the applicability of our MMD-FUSE test on both synthetic low-dimensional and real-world high-dimensional data, and compare its performance in terms of power against current state-of-the-art kernel tests.

## 1 Introduction

The fundamental problem of non-parametric two-sample testing consists in detecting the difference between any two distributions having access only to samples from these. Kernel-based tests relying on the Maximum Mean Discrepancy (MMD; Gretton et al., 2012a) as a measure of distance on distributions are well-suited for this framework as they can identify complex non-linear features in the data, and benefit from both strong theoretical guarantees and ease of implementation. This explains their popularity among practitioners and justifies their wide use for real-world applications.

However, the performance of these tests is crucially impacted by the choice of kernel. This is commonly tackled by either: choosing the kernel by some weakly data-dependent heuristic (Gretton et al., 2012a); or splitting off a hold-out set of data for kernel selection, with the other half used for the actual test (Gretton et al., 2012b; Sutherland et al., 2017). This latter method includes training feature extractors such as deep kernels on the first selection half. Both of these methods can incur a significant loss in test power, since heuristics may lead to poor kernel choices, and data splitting reduces the number of data points for the actual test.

Our contribution is to present MMD-based tests which can strongly adapt to the data _without_ data splitting. This comes in two parallel parts: firstly we show how the kernel can be chosen in anunsupervised fashion using the entire dataset, and secondly we show how multiple such kernels can be adaptively weighted in a single test statistic, optimising test power.

**Data Splitting.** The data splitting approach selects test parameters on a held-out half of the dataset, and applies the test to the other half. Commonly, this involves optimising a kernel on held-out data in a supervised fashion to distinguish which sample originated from which distribution, either by learning a deep kernel directly (Sutherland et al., 2017; Liu et al., 2020, 2021), or indirectly through the associated witness function (Kubler et al., 2022, 2022). Jitkrittum et al. (2016) propose tests which select witness function features (in either spatial or frequency space) on the held-out data, running the analytic representation test of Chwialkowski et al. (2015) on the remaining data.

Our first contribution is to show that it is possible to learn a feature extractor for our test (_e.g._, a deep kernel) on the _entirety_ of the data in an _unsupervised_ manner while retaining the desired non-asymptotic test level. Specifically, any method can be used that is ignorant of which distribution generated which samples. We can thus leverage many feature extraction methods, from auto-encoders (Hinton and Salakhutdinov, 2006) to recent powerful developments in self-supervised learning (He et al., 2020; Chen et al., 2020, 2020; Chen and He, 2021; Grill et al., 2020; Caron et al., 2020; Zbontar et al., 2021; Li et al., 2021). Remarkably, our method applies to _any_ permutation-based two-sample test, even non-kernel tests, provided the parameters are chosen in this unsupervised fashion. This includes a very wide array of MMD-based tests, and finally provides a formal justification for the commonly-used median heuristic (Gretton et al., 2012; Ramdas et al., 2015; Reddi et al., 2015).

**Adaptive Kernel Selection.** A newer approach originating in Schrab et al. (2023) performs adaptive kernel selection through multiple testing. Aggregating several MMD-based tests with different kernels, each on the whole dataset, results in an overall adaptive test with optimal power guarantees in terms of minimax separation rate over Sobolev balls. Variants of this kernel adaptivity through aggregation have been proposed with linear-time estimators (Schrab et al., 2022), spectral regularisation (Hagrass et al., 2022), kernel thinning compression (Domingo-Enrich et al., 2023), and in the asymptotic regime (Chatterjee and Bhattacharya, 2023). Another adaptive approach using the entire dataset for both kernel selection and testing is given by Kubler et al. (2020); this leverages the Post Selection Inference framework, but the resulting test suffers from low power in practice.

While our unsupervised feature extraction method is an extremely general technique and potentially powerful, particularly for high-dimensional structured data like images, it is not always sufficient for data where such feature extraction is difficult. This motivates our second contribution, a method for combining whole-dataset MMD estimates under multiple different kernels into single test statistics, for which we prove exponential concentration. Kernel parameters such as bandwidth can then be chosen in a non-heuristic manner to optimise power, even on data with less structure and varying length scales. Using a single statistic also ensures that a single _test_ can be used, instead of the multiple testing approach outlined above, reducing computational expense.

**MMD-FUSE.** By combining these contributions we construct two closely related MMD-FUSE tests. Each chooses a set of kernels based on the whole dataset in an unsupervised fashion, and then adaptively weights and _fuses_ this (potentially infinite) set of kernels through our new statistics; both parts of this procedure are done using the entire dataset without splitting. On the finite sets of kernels we use in practice, the weighting procedure is done in closed form via a weighted soft maximum. We show these new tests to be well-calibrated and give sufficient power conditions which achieve the minimax optimal rate in terms of MMD. In empirical comparisons, our test compares favourably to the state-of-the-art aggregated tests in terms of power and computational cost.

**Outline and Summary of Contributions.** In Section 2 we outline our setting, crucial results underlying our work, and alternative existing approaches. Section 3 covers the construction of permutation tests and discusses how we can choose the parameters for any such test in any unsupervised fashion including with deep kernels and by those methods mentioned above. Section 4 introduces and motivates our two proposed tests. Section 5 discusses sufficient conditions for test power (at a minimax optimal rate in MMD), and the exponential concentration of our statistics. Finally, we show that our test compares favourably with a wide variety of competitors in Section 6 and discuss in Section 7.

## 2 Background

**Two-Sample Testing.** The two-sample testing problem is to determine whether two distributions \(p\) and \(q\) are equal or not. In order to test this hypothesis, we are given access to two samples, \((X_{1},,X_{n})}}{{}}p\) and \((Y_{1},,Y_{m})}}{{}}q\) as tuples of data points with sizes \(n\) and \(m\). We write the combined (ordered) sample as \((Z_{1},,Z_{n+m})=(,)=(X_{1},,X_{n},Y_{1 },,Y_{m})\).

We define the null hypothesis \(H_{0}\) as \(p=q\) and the alternative hypothesis \(H_{1}\) as \(p q\), usually with a requirement \((p,q)>\) for a distance \(\) (such as the MMD) and some \(>0\). A hypothesis test \(\) is a \(\{0,1\}\)-valued function of \(\), which rejects the null hypothesis if \(()=1\) and fails to reject it otherwise. It will usually be formulated to control the probability of a type I error at some level \((0,1)\), so that \(_{p p}(()=1)\), while simultaneously minimising the probability of a type II error, \(_{p q}(()=0)\). In the above we have used the notation \(_{p p}\) and \(_{p q}\) to indicate that the sample \(\) is either drawn from the null, \(p=q\), or the alternative \(p q\). Similar notation will be used for expectations and variances. When a bound \((0,1)\) on the probability of a type II error is given (which may depend on the precise formulation of the alternative), we say the test has power \(1-\).

**Maximum Mean Discrepancy.** The Maximum Mean Discrepancy (MMD) is a kernel-based measure of distance between two distributions \(p\) and \(q\), which is often used for two-sample testing. The MMD quantifies the dissimilarity between these distributions by comparing their mean embeddings in a reproducing kernel Hilbert space (RKHS; Aronszajn, 1950) with kernel \(k(,)\). Formally, if \(_{k}\) is the RKHS associated with kernel function \(k\), the MMD between distributions \(p\) and \(q\) is the integral probability metric defined by:

\[_{k}(p,q)_{f_{k}:\|f\|_{_{k} } 1}(_{X p}[f(X)]-_{Y q}[f(Y)]).\]

The minimum variance unbiased estimate of \(_{k}^{2}\), is given by the sum of two U-statistics and a sample average as:2

\[}_{k}^{2}())[n]_{2}}{-56.905512ptk(X_{i},X_{i^{}})+)[m]_{2}}{}k(Y_{j},Y_{j^{}})- _{i=1}^{n}_{j=1}^{m}k(X_{i},Y_{j})},\]

where we introduced the notation \([n]_{2}=\{(i,i^{})[n]^{2}:i i^{}\}\) for the set of all pairs of distinct indices in \([n]=\{1,,n\}\). Tests based on the MMD usually reject the null when \(}_{k}^{2}\) exceeds some critical threshold, with the resulting power being greatly affected by the kernel choice. For _characteristic_ kernel functions (Sriperumbudur et al., 2011), it can be shown that \(_{k}(p,q)=0\) if and only if \(p=q\), leading to consistency results. However, on finite sample sizes, convergence rates of MMD estimates typically have strong dependence on the data dimension, so there are settings in which kernels ignoring redundant or unimportant features (_i.e._ non-characteristic kernels) will give higher test power in practice than characteristic kernels (which can over-weight redundant features).

**Distributions Over Kernels.** In our test statistic, we will consider the case where the kernel \(k\) is drawn from a distribution \(_{+}^{}()\) (with the latter notation denoting a probability measure; _c.f._ Benton et al., 2019 in the Gaussian Process literature). This distribution will be adaptively chosen based on the data subject to a regularisation term based on a "prior" \(_{+}^{}()\) and defined through the Kullback-Liebler divergence: \((\|)_{}[(/)]\) for \(\) and \((\|)\) otherwise. When constructing these statistics the Donsker-Varadhan equality (Donsker and Varadhan, 1975), holding for any measurable \(g:\), will be useful:

\[_{_{+}^{}()}_{}[g]-(\|)=_{}[ g]. \]

This can be further related to the notion of soft maxima. If \(\) is a uniform distribution on finite \(\) with \(||=r\), then \((\|)(r)\). Setting \(g=tf\) for some \(t>0\), Equation (1) relaxes to

\[_{k}f(k)-(_{k}e^{tf (k)})_{k}f(k), \]which approximates the maximum with error controlled by \(t\). Our approach in considering these soft maxima is reminiscent of the PAC-Bayesian (McAllester, 1998; Seeger et al., 2001; Maurer, 2004; Catoni, 2007; see Guedj, 2019 or Alquier, 2021 for a survey) approach to capacity control and generalisation. This framework has raised particular attention recently as it has been used to provide the only non-vacuous generalisation bounds for deep neural networks (Dziugaite and Roy, 2017, 2018; Dziugaite et al., 2021; Zhou et al., 2019; Perez-Ortiz et al., 2021; Biggs and Guedj, 2021, 2022a); it has also been fruitfully applied to other varied problems from ensemble methods (Lacasse et al., 2006, 2010; Masegosa et al., 2020; Wu et al., 2021; Zantedeschi et al., 2021; Biggs and Guedj, 2022b; Biggs et al., 2022) to online learning (Haddouche and Guedj, 2022). Our proofs draw on techniques in that literature for U-Statistics (Lever et al., 2013) and martingales (Seldin et al., 2012; Biggs and Guedj, 2023; Haddouche and Guedj, 2023; Chugg et al., 2023). By considering these soft maxima, we gain a major advantage over the standard approaches, as we can obtain concentration and power results for our statistics without incurring Bonferroni-type multiple testing penalties.

**Permutation Tests.** The tests we discuss in this paper use permutations of the data \(\) to approximate the null distribution. We begin our discussion in a fairly general form to include other close settings such as independence testing (Albert et al., 2022; Rindt et al., 2021) or wild bootstrap-based two-sample tests (Fromont et al., 2012, 2013; Chwialkowski et al., 2014; Schrab et al., 2023). Let \(\) be a group of transformations on \(\); in our setting \(=_{n+m}\), denoting the permutation (or symmetric) group of \([N]\) by \(_{N}\) and its elements by \(_{N},:[N][N]\). We write \(g\) for the action of \(g\) on \(\); _e.g._ defining \(=(Z_{(1)},,Z_{(n+m)})\) for the group elements \(_{n+m}\). We will suppose \(\) is invariant under the action of \(\) when the sample is drawn from the _null_ distribution, _i.e._ if the null is true than \(g=^{d}\) (by which we notate equality of distribution) for all \(g\). This is clearly the case for \(=_{n+m}\) in two-sample testing, since under the null \(=(Z_{1},,Z_{m+n})}}{{}}p\) with \(p=q\), while under the alternative, the permuted sample \(\) for randomised \((_{n+m})\) simulates the null distribution.

We can use permutations to construct an approximate cumulative distribution (CDF) of our test statistic under the null, and choose an appropriate quantile of this CDF as our test threshold, which must be exceeded under the null with probability less that level \(\). For this we introduce a quantile operator (analogous to the \(\) and \(\) operators) for a finite set \(\{f(a):a\}\):

\[*{quantile}_{q,\,a}f(a)\{r :|}_{a}\{f(a) r\}  q\}. \]

Various different results can be used to choose the threshold giving a correct level; we will here highlight a very general and easy-to-use theorem for constructing permutation tests, and in Section 3 will discuss previously unconsidered implications for using unsupervised feature extraction methods as a part of our tests. Although this result is not new, we believe that its usefulness has been under-appreciated in the kernel testing community, and can be more conveniently applied than Romano and Wolf (2005, Lemma 1), which requires exchangeable and is commonly used, _e.g._ in Albert et al. (2022, Proposition 1) and Schrab et al. (2023, Proposition 1).

**Theorem 1** (Hemerik and Goeman, 2018, Theorem 2.).: _Let \(G\) be a vector of elements from \(\), \(G=(g_{1},g_{2},,g_{B+1})\), with \(g_{B+1}=\) (the identity permutation) for any \(B 1\). The elements \(g_{1},,g_{B}\) are drawn uniformly from \(\) either i.i.d. or without replacement (which includes the possibility of \(G=\)). If \(()\) is a statistic of \(\) and \(=^{d}g\) for all \(g\) under the null then_

\[_{p p,G}(()>*{quantile}_{1-,g  G}(g)).\]

In other words, if we compare \(()\) with the empirical quantile of \((g)\) as a test threshold, the type I error rate is no more than \(\).3 The potentially complex task of constructing a permutation test reduces to the trivial task of choosing the statistic \(()\). This result is also true for _randomised_ permutations of any number \(B 1\)_without_ approximation, so an exact and computationally efficient test can be straightforwardly constructed this way.

We finally mention the related approach of the MMD Aggregated test (MMDAgg; Schrab et al., 2023), which combines multiple MMD-based permutation tests with different kernels, and rejects if any_ of these reject, using distinct quantiles for each kernel. To ensure the overall aggregated test is well-calibrated, these quantiles must be adjusted using a _second level_ of permutations. This incurs additional computational cost, a pitfall avoided by our fused single statistic.

**Faster Sub-Optimal Tests.** While the main focus of this paper revolves around the kernel selection problem for optimal, quadratic MMD testing, we also highlight the existence of a rich literature on efficient kernel tests which run in linear (or near-linear) time. These speed improvements are achieved using various tools such as: incomplete U-statistics (Gretton et al., 2012; Yamada et al., 2019; Lim et al., 2020; Kubler et al., 2020; Schrab et al., 2022b), block U-statistics (Zaremba et al., 2013; Deka and Sutherland, 2022), eigenspectrum approximations (Gretton et al., 2009), Nystrom approximations (Chefarioui et al., 2022), random Fourier features (Zhao and Meng, 2015), analytic representations (Chwialkowski et al., 2015; Jitkrittum et al., 2016), deep linear kernels (Kirchler et al., 2020), kernel thinning (Dwivedi and Mackey, 2021; Domingo-Enrich et al., 2023), _etc._

The efficiency of these tests usually entail weaker theoretical power guarantees compared to their quadratic-time counterparts, which are minimax optimal4(Kim et al., 2022; Li and Yuan, 2019; Fromont et al., 2013; Schrab et al., 2023; Chatterjee and Bhattacharya, 2023). These optimal quadratic tests are either permutation-based non-asymptotic tests (Kim et al., 2022; Schrab et al., 2023) or studentised asymptotic tests (Kim and Ramdas, 2023; Shekhar et al., 2022; Li and Yuan, 2019; Gao and Shao, 2022; Florian et al., 2023). We emphasise that the parameters of any of these permutation-based two-sample tests can be chosen in the unsupervised way we outline in Section 3. We choose to focus in this work on optimal quadratic-time results, but note that our general approach could be extended to sub-optimal faster tests as well.

## 3 Learning Statistics for Permutation Tests

Here we discuss how we can improve permutation-based tests by learning parameters for them in an unsupervised manner. The reason that we highlighted Theorem 1 in the previous section is that it holds for any \(\). For example, we could use the MMD with a kernel chosen based on the data, \(()=}^{2}_{k=k()}()\); however, for each permutation \(\) we would need to re-compute \(()=}^{2}_{k=k()}()\), so the kernel being used would be different for each permutation. This has two major disadvantages: firstly, it might be computationally expensive to re-compute \(k\) for each permutation, especially for a deep kernel5. Secondly, the _scale_ of the resulting MMD could be dramatically different for each permutation, so the empirical quantile might not lead to a powerful test. This second problem is related to the problem of combining multiple different MMD values into a single test which our MMD-FUSE statistics are designed to combat (Section 4; _c.f._ also MMDAgg, Schrab et al., 2023).

**Our Proposal.** In two-sample testing we propose to use a statistic \(_{}\) parameterised by some \(\), where \(\) is fixed for all permutations, but depends on the data in an _unsupervised_ or _permutation-invariant_ way. Specifically, we allow such a parameter to depend on \(\{Z_{1},,Z_{n+m}\}\), the _unordered_ combined sample. Since our tests will not depend on the internal ordering of \(\) and \(\) (which are assumed i.i.d. under both hypotheses), the only additional information contained in \(\) over \(\) is the _label_ assigning \(Z_{i}\) to its initial sample. This is justified since \(=\) for all \(_{n+m}\), so setting \(()=_{()}()\) gives a fixed \(\) and statistic for all permutations to use in Theorem 1. The information in \(\) can be used to fine-tune test parameters for any test fitting this setup. This solves both the computation and scaling issues mentioned above.

The above provides a first formal justification for the use of the median heuristic in Gretton et al. (2012a), since it is a permutation-invariant function of the data. However, a far richer set of possibilities are available even when restricting ourselves to these permutation-free functions of \(\). For example, we can use any unsupervised or self-supervised learning method to learn representations to use as the input to an MMD-based test statistic, while paying no cost in terms of calibration and needing to train such methods only once. Given the wide variety of methods dedicated to feature extraction and dimensionality reduction, this opens up a huge range of possibilities for the design of new and principled two-sample tests. The simplicity and generality of our proposal might lead one to expect that this idea has been used before, but it has not to our best knowledge, underlined by the fact that the median heuristic has been widely used without such justification when one follows from this method. This potentially powerful and widely-applicable possibility represents one of our most practical contributions.

## 4 MMD-FUSE: Fusing U-Statistics by Exponentiation

Say we have computed several MMD values \(}_{k}^{2}\) under different kernels \(k\). How might we combine these? One possibility is to perform multiple testing as in the case of MMDAgg. An even simpler approach would simply take the maximum of those values \(_{k}}_{k}^{2}\) since Theorem 1 shows that this will not prevent us from controlling the level of our test by \(\); note though that for each permutation we would take this maximum separately. Indeed, Carcamo et al. (2022) show that for certain kernel choices the supremum of the MMD values with respect to the bandwidth is a valid integral probability metric (Muller, 1997). There are two main problems with this approach: a capacity control issue and a normalisation issue.

Firstly, if the class over which we are maximising is sufficiently rich (for example a complex neural network), then the maximum may be able to effectively memorise the entire sample for each possible permutation, saturating the statistic for every permutation and limiting test power. Any convergence results would need to hold _simultaneously_ for every \(k\) in \(\), and so power results suffer: for finite \(||\) we would incur a sub-optimal Bonferroni correction (see Section 5); while for infinite classes, results would need to involve capacity control quantities like Rademacher complexity. Further, only information from a single maximising "base" kernel can be considered at a time.

Therefore, in both our statistics, we prefer a "soft" maximum, which considers information from every kernel simultaneously and when more than one of the kernels is well-suited (Section 5) it therefore avoids the Bonferroni correction arising from uniform bounds. From Equation (1), our approach is equivalent to using a KL complexity penalty, and is strongly reminiscent of _PAC-Bayesian_ (Section 2) capacity control. We note that other soft maxima could be considered, but our choice makes obtaining exponential concentration inequalities relatively easy (Appendix C), and the dual formulation (Equation (2)) allows us to derive power results in terms of MMD directly.

The second issue is that the MMD estimates might have different scales or variances per kernel which need to be accounted for. In order to be able to meaningfully compare MMD values between each other, these need to be normalised somehow before taking a maximum. We use the common approach of dividing through by a variance-like term (as in "studentised" tests, see Section 2). The specific normaliser is permutation invariant and gives our statistic tight sub-Gaussian null concentration (for well-chosen regularisation parameter \(\); see Appendix C).

Based on the above we introduce the FUSE-1 statistic which uses a log-sum-exp soft maximum, and the FUSE-N which combines this with normalisation. Both statistics use a "prior" distribution on \(\), denoted \(()\), which is either fixed independently of the data, or is a function of the data which is invariant under permutation (as discussed in Section 3).

**Definition 1**.: _We define the un-normalised (subscript 1 for normaliser of \(1\)) and normalised (subscript N) test statistics with parameter \(>0\), respectively, as_

\[}_{1}() (_{k( )}[(}_{k}^{2}(, ))]),\] \[}_{N}() (_{k( )}[(}_{k}^{2}(,)}{_{k}()}})]),\]

_where \(_{k}()_{(i,j)[n+m]_{2}}k(Z_{i },Z_{j})^{2}\) is permutation invariant._

Although these statistics appear complex, we note that in the case where \(\) has finite support, their calculation reduces to a log-sum-exp of MMD estimates normalised by \(_{k}\). This is even clearer when \(\) is also uniform on its support, as we consider experimentally; then Equation (2) shows that our statistics reduce to soft maxima, with \(\) controlling the smoothness or "temperature".

From this, we define the FUSE-1 test (with the FUSE-N test \(_{_{N}}\) defined analogously) as

\[_{_{1}}():=}_{1}( )\,>\,*{quantile}_{1-, S}}_{1}()} \]

for sampled set \(S\) of permutations as described above. It compares the test statistic \(}_{1}\) with its quantiles under permutation, and rejects if the overall value exceeds a quantile controlled by \(\). Note that since \(_{k}()\) is permutation invariant, it only needs to be calculated once per kernel in \(_{_{N}}\) (and not separately for each permutation) as per Section 3. See Appendix A.5 for its time complexity.

**Comparison with MMDAgg.** MMDAgg is a different way to think about combining multiple kernels and MMD values, but it relies on a framework based on multiple testing. This can be problematic in the case where the number of kernels considered is large, since as this number increases in the multiple testing approach MMDAgg, the level needs to be corrected differently, so its theretical power behaviour becomes unclear (though empirically MMDAgg retains its power in this setting). By contrast, our proposed FUSE methods avoid multiple testing as a single statistic and quantile are used, avoiding such issues. The FUSE statistics are even defined in the limit of an infinite number of kernels (continuous/uncountable collection of kernels) by considering distributions on the space of kernels, which is not the case for MMDAgg. Moreover, while the MMDAgg approach is only useful for hypothesis testing, having a quantity like FUSE combining multiple kernel-based measures of distance with exponential concentration bounds could be of interest in a wider range of applications.

**FUSE-1 and the Mean Kernel.** Although the un-normalised test has worse performance in practice than our normalised test, the FUSE-1 statistic is interesting in its own right because of various theoretical properties, as we discuss below. Firstly, we introduce the _mean_ kernel \(K_{}(x,y)=_{k}k(x,y)\) under a "posterior" \(^{1}_{+}()\), which is indeed a reproducing kernel in its own right. In the finite case, this is simply a weighted sum of "base" kernels.

Note that the linearity of \(}^{2}_{k}\) and \(^{2}_{k}\) in the kernel \(k\) implies that \(_{k}\,^{2}_{k}(p,q)=^{2}_{K_{}}( p,q)\), and similarly for \(}^{2}_{k}\), with these terms appearing in our power results (Section 5).

Combining this linearity with the dual formulation of \(}_{1}\) via Equation (1) gives

\[}_{1}()=_{^{1}_{+}()}}^{2}_{K_{}}(,)-(, )}{}.\]

This re-states \(}_{1}\) in terms of "posterior" \(\), and makes the interpretation of our statistic as a KL-regularised kernel-learning method clear. In the finite case, our test simply optimises the weightings of the different kernels in a constrained way. We note that for certain _infinite_ kernel sets and choices of prior it is be possible to express the mean kernel in closed form. This happens because, _e.g._ the expectation of a Gaussian kernel with respect to a Gamma prior over the bandwidth is simply a (closed form) rational quadratic kernel. We discuss this point further in Appendix B.

## 5 Theoretical Power of MMD-Fuse

In this section we outline possible sufficient conditions for our tests to obtain power at least \(1-\) at given level \(\). The conditions will depend on a fixed data-independent "prior" \(^{1}_{+}()\) and thus hold even without unsupervised parameter optimisation. They are stated as requirements for the _existence_ of a "posterior" \(^{1}_{+}()\) with corresponding mean kernel \(K_{}\) as defined in Section 4. In the finite case, \(K_{}\) is simply a weighted sum of kernels, so these requirements are also satisfied under the same conditions for any single kernel, corresponding to the case where the posterior puts all its weight on a single kernel.

Technically, these results require that there is some constant \(\) upper bounding all of the kernels, and that \(n\) and \(m\) are within a constant multiple (_i.e._\(n m cn\) for some \(c 1\), notated \(n m\)). They hold when using randomised permutations provided \(B>c^{}^{-2}(^{-1})\) for small constant \(c^{}>0\).

**Theorem 2** (Fuse-1 Power).: _Fix prior \(\) independently of the data. For the un-normalised test FUSE-1 with \( n/\) and \(n m\), there exists a universal constant \(C>0\) such that_

\[^{1}_{+}()\ :\ ^{2}_{K_{}}(p,q) >(++(, )).\]

_is sufficient for FUSE-1 to achieve power at least \(1-\) at level \(\)._A similar result is obtained for FUSE-N under an assumption that the normalising term is well behaved (see assumption in Theorem 3). This requirement will be satisfied for kernels (including most common ones) that tend to zero only in the limit of the data being infinitely far apart.

**Theorem 3** (FUSE-N Power).: _Fix prior \(\) independently of the data such that for all \(k()\) the expectation \(_{ p q}[_{k}()^{-1}]<c/\) is bounded for some \(c<\). For the normalised test FUSE-N with \( n m\), there exists a universal constant \(C>0\) such that_

\[^{1}_{+}()\ :\ ^{2}_{K }(p,q)>(}++ (,)).\]

_is sufficient for FUSE-N to achieve power at least \(1-\) at level \(\)._

**Discussion.** The conditions in Theorems 2 and 3 give the optimal \(^{2}\) separation rate in \(n\)(Domingo-Enrich et al., 2023, Proposition 2). These results also imply consistency if the prior \(\) assigns non-zero weight to characteristic kernels.6 Applying these results to uniform priors supported on \(r\) points, the KL penalty can be upper bounded as \((,)(r)\). Thus in the _worst_ case, where only a single kernel achieves large \(^{2}_{k}(p,q)\), the price paid for adaptivity is only \((r)\). In many cases, _most_ of the kernels will give large \(^{2}_{k}(p,q)\). The posterior will then mirror the prior, and this KL penalty will be even smaller. Thus very large numbers of kernels could be considered, and if all give large MMD values the power would not be greatly affected.

**Additional Technical Results.** Aside from the presentation of our new statistics and tests, we make a number of technical contributions on the way to proving Theorems 2 and 3, as well as proving some additional results. In particular, we give exponential concentration bounds for our statistics under permutations and the null, which do not require bounded kernels. This refined analysis requires the construction of a coupled Rademacher chaos and concentration thereof. We obtain intermediate results using variances from the proofs of Theorems 2 and 3 that could be used in future work to obtain power guarantees under alternative assumptions such as Sobolev spaces (Schrab et al., 2023). Finally, we prove exponential concentration for \(}_{1}\) under the alternative and bounded kernels, requiring the proof of a "PAC-Bayesian" bounded differences-type concentration inequality. See the appendix for a more detailed overview.

## 6 Experiments

We compare the test power of MMD-FUSE-N (\(=\)) against various MMD-based kernel selective tests (see Section 1 for details) using: the median heuristic (MMD-Median; Gretton et al., 2012), data splitting (MMD-Split; Sutherland et al., 2017), analytic Mean Embeddings and Smooth Characteristic Functions (ME & SCF; Jitkrittum et al., 2016), the MMD Deep kernel (MMD-D; Liu et al., 2020), Automated Machine Learning (AutoML; Kubler et al., 2022), kernel thinning to (Aggregate) Compress Then Test (CTT & ACTT; Domingo-Enrich et al., 2023), and MMD Aggregated (Incomplete) tests (MMDAgg & MMDAggInc; Schrab et al., 2023, 2022). Additional details and code link for experimental reproducibility are provided in Appendix A.

**Distribution on Kernels.** We choose our kernel prior distribution \(\) as uniform over a collection of Gaussian, \(k_{}^{q}(x,y)=(- x-y_{2}^{2}/2^{2})\), and Laplace, \(k_{}^{}(x,y)=(- x-y_{1}/)\), kernels with various bandwidths \(>0\). These bandwidths are chosen as the uniform discretisation of the interval between half the 5% and twice the 95% quantiles (for robustness) of \(\{ z-z^{}_{r}:z,z^{}\}\), with \(r\{1,2\}\), respectively. This choice is similar to that of Schrab et al. (2023, Section 5.2), who empirically show that ten points for the discretisation is sufficient (Schrab et al., 2023, Figure 6), which we verify in Appendix A.4. This set of distances is permutation-invariant, so Theorem 1 guarantees a well-calibrated test even though the kernels are data-dependent.

**Mixture of Gaussians.** Our first experiments (Figure 1) consider multimodal distributions \(p\) and \(q\), each a \(2\)-dimensional mixture of four Gaussians with means \((,)\) with \(=20\) and diagonal covariances. For \(p\), the four components all have unit variances, while for \(q\) we vary the standard deviation \(\) of _one_ of the Gaussians, \(=1\) corresponds to the null hypothesis \(p=q\). Intuitively, an appropriate kernel bandwidth to distinguish \(p\) from \(q\) would correspond to that separating Gaussianswith standard deviations \(1\) and \(\). This is significantly smaller than the _median_ bandwidth which scales with the distance \(\) between modes.

**Perturbed Uniform.** In Figure 1, we report test power for detecting perturbations on uniform distributions in one and two dimensions. We vary the amplitude \(a\) of two perturbations from \(a=0\) (null) to \(a=1\) (maximum value for the density to remain non-negative). A similar benchmark was first proposed by Schrab et al. (2023, Section 5.5) and considered in several other works (Schrab et al., 2022b; Hagrass et al., 2022; Chatterjee and Bhattacharya, 2023). Different bandwidths are required to detect different amplitudes of the perturbations.

**Galaxy MNIST.** We examine performance on real-world data in Figure 1, through galaxy images (Walmsley et al., 2022) in dimension \(d=3 64 64=12288\) captured by a ground-based telescope. These consist of four classes:'smooth and cigar-shaped', 'edge-on-disk', 'unbarred spiral', and'smooth and round'. One distribution uniformly samples images from the first three categories, while the other does the same with probability \(1-c\) and uniformly samples a'smooth and round' galaxy image with probability of corruption \(c\). The null hypothesis corresponds to the case \(c=0\).

**CIFAR 10 vs 10.1.** The aim of this experiment is to detect the difference between images from the CIFAR-10 (Krizhevsky, 2009) and CIFAR-10.1 (Recht et al., 2019) test sets. This is a challenging problem as CIFAR-10.1 was specifically created to consist of new samples from the CIFAR-10 distribution so that it can be used as an alternative test set for models trained on CIFAR-10. Samples from the two distributions are presented in Figure 6 in Appendix A.3(Liu et al., 2020, Figure 5). This benchmark was proposed by Liu et al. (2020, Table 3) who introduced the deep MMD test MMD-D and the MMD-Split test (here referred to as MMD-O to point out that their implementation has been used rather than ours). They also compare to ME and SCF, as well as to C2ST-L and C2ST-S (Lopez-Paz and Oquab, 2017) which correspond to Classifier Two-Sample T

   Tests & Power \\  MMD-FUSE & **0.937** \\ MMDAgg & 0.883 \\ MMD-D & 0.744 \\ CTT & 0.711 \\ MMD-Median & 0.678 \\ ACTT & 0.652 \\ ME & 0.588 \\ AutoML & 0.544 \\ C2ST-L & 0.529 \\ C2ST-S & 0.452 \\ MMD-O & 0.316 \\ MMDAggInc & 0.281 \\ SCF & 0.171 \\   

Table 1: Test power for detecting the difference between CIFAR-10 and CIFAR-10.1 images with test level \(=0.05\). The averaged numbers of rejections over \(1000\) repetitions are reported.

Figure 1: Power experiments. The four columns correspond to different settings: Mixture of Gaussians in two dimensions (null \(=1\)), Perturbed Uniform for \(d\{1,2\}\) (null \(a=0\)), and Galaxy MNIST in dimension \(12288\) (null \(c=0\)). In the first row, the deviations away from the null are varied for fixed sample size \(m=n=500\). In the second row, the sample size varies while the deviations are fixed as \(=1.3\), \(a=0.2\), \(a=0.4\), and \(c=0.15\), for the four respective problems. The plots correspond to the rejections of the null averaged over \(200\) repetitions.

Linear kernels. For the tests slitting the data, \(1000\) images from both datasets are used for parameter selection and/or model training, and \(1021\) other images from each distributions are used for testing. Consequently, tests avoiding data splitting are given the full \(2021\) images from CIFAR-10.1 and \(2021\) images sampled from CIFAR-10.

**Experimental Results of Figure 1.** We observe similar trends in all eight experiments in Figure 1: MMD-FUSE _matches the power_ of state-of-the-art MMDAgg while being _computationally faster_ (both theoretically and practically). These two tests consistently obtain the highest power in every experiment, except when increasing the number of Galaxy MNIST images where MMD-D obtains higher power. However, we observe in Table 1 that MMD-FUSE outperforms MMD-D on another image data problem, also with large sample size. On synthetic data, the deep kernel test MMD-D surprisingly only obtains power similar to MMD-Split in most experiments (even lower in the 2-dimensional perturbed uniform experiment). The two near-linear aggregated variants ACTT and MMDAggInc trade-off a small portion of test power for computational efficiency, with the former outperforming the latter for large sample sizes. The importance of kernel selection is emphasised by the fact that the two tests using the median bandwidth (MMD-Median and CTT) achieve very low power. While the linear-time SCF test, based in the frequency domain, attains high power in the Mixture of Gaussians experiments, it has low power in the three other experiments. Its spatial domain variant ME performs better on Perturbed Uniform \(d\{1,2\}\) experiments but in general still has reduced power compared to both linear and quadratic time alternatives. Finally, the AutoML test performs well for fixed sample size \(m=n=500\) (first row of Figure 1), but its power compared to other tests considerably deteriorates as the sample size increases (second row of Figure 1). Overall, MMD-FUSE achieves _state-of-the-art performance_ across all experiments on both low-dimensional synthetic data and high-dimensional real-world data.

**Experimental Results of Table 1.** We report in Table 1 the power achieved by each test on the CIFAR 10 vs 10.1 experiment, which is averaged over \(1000\) repetitions. We observe that MMD-FUSE performs the best and obtains power \(0.937\), which means that out of \(1000\) repetitions, it was 937 times able to distinguish between samples from CIFAR-10 and from CIFAR-10.1. This demonstrates that the images in CIFAR-10.1 do not come from the same distribution as those in CIFAR-10.

**Experimental Results of Figure 7.** We observe in Figure 7 of Appendix A.4 that MMD-FUSE can achieve higher power than MMDAgg in some additional perturbed uniform experiment. We also note that using a relatively small number of kernels (_e.g._, \(10\)) is enough to capture all the required information, and that the test power is retained when further increasing the number of kernels.

## 7 Conclusions

In this work, we propose MMD-FUSE, an MMD-based test which fuses kernels through a soft maximum and a method for learning general two-sample testing parameters in an unsupervised fashion. We demonstrate the empirical performance of MMD-FUSE and show that it achieves the optimal MMD separation rate guaranteeing high test power. This optimality holds with respect to the sample size and likely also for the logarithmic dependence in \(\), but we believe the dependence on \(\) could be improved in future work; a general question is whether lower bounds in terms of \(\) and \(\) can be proved. Obtaining separation rates in terms of the \(L^{2}\)-norm between the densities (Schrab et al., 2023) may also be possible but challenging since this distance is independent of the kernel.

An open question is in explaining the significant empirical power advantage of the normalised test over its un-normalised variant, which is currently not reflected in the derived rates. The importance of this normalisation is clear when considering kernels with different bandwidths, leading to vastly different scaling in the un-normalised permutation distributions. Work here could begin with finite-sample concentration guarantees for our normalised statistic or other "studentised" variants, some of which might obtain better performance.

Future work could also examine computationally efficient variants of MMD-FUSE, by either relying on incomplete \(U\)-statistics (Schrab et al., 2022) and leading to suboptimal rates, or by relying on recent ideas of kernel thinning (Domingo-Enrich et al., 2023) which can lead to the same optimal rate under stronger assumptions on the data distributions, or by considering the permutation-free approach of Shekhar et al. (2022). Finally, our two-sample MMD fusing approach could be extended to the HSIC independence framework (Greton et al., 2005, 2008; Albert et al., 2022) and to the KSD goodness-of-fit setting (Chwialkowski et al., 2016; Liu et al., 2016; Schrab et al., 2022).