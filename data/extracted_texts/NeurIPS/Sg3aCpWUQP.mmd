# Errors-in-variables Frechet Regression

with Low-rank Covariate Approximation

 Dogyoon Song

Department of EECS

University of Michigan

Ann Arbor, MI 48109, USA

dogyoons@umich.edu

Equal contribution. To whom correspondence should be addressed: hankh@uic.edu

&Kyunghee Han

Department of Math, Stat and Comp Sci

University of Illinois at Chicago

Chicago, IL 60607, USA

hankh@uic.edu

Equal contribution. To whom correspondence should be addressed: hankh@uic.edu

###### Abstract

Frechet regression has emerged as a promising approach for regression analysis involving non-Euclidean response variables. However, its practical applicability has been hindered by its reliance on ideal scenarios with abundant and noiseless covariate data. In this paper, we present a novel estimation method that tackles these limitations by leveraging the low-rank structure inherent in the covariate matrix. Our proposed framework combines the concepts of global Frechet regression and principal component regression, aiming to improve the efficiency and accuracy of the regression estimator. By incorporating the low-rank structure, our method enables more effective modeling and estimation, particularly in high-dimensional and errors-in-variables regression settings. We provide a theoretical analysis of the proposed estimator's large-sample properties, including a comprehensive rate analysis of bias, variance, and additional variations due to measurement errors. Furthermore, our numerical experiments provide empirical evidence that supports the theoretical findings, demonstrating the superior performance of our approach. Overall, this work introduces a promising framework for regression analysis of non-Euclidean variables, effectively addressing the challenges associated with limited and noisy covariate data, with potential applications in diverse fields.

## 1 Introduction

Regression analysis is a fundamental statistical methodology to model the relationship between response variables and explanatory variables (covariates). Linear regression, for example, models the (conditional) expected value of the response variable as a linear function of covariates. Regression models enable researchers and analysts to make predictions, gain insights into how input variables influence the outcomes of interest, and validate hypothetical associations between variables in inferential studies. As a result, regression is widely utilized across various scientific domains, including economics, psychology, biology, and engineering .

In recent decades, there has been a growing interest in developing statistical methods capable of handling random objects in non-Euclidean spaces. Examples of these include functional data analysis , statistical manifold learning , statistical network analysis , and object-oriented data analysis . In such contexts, the response variable is defined in a metric space that may lack an algebraic structure, making it challenging to apply global, parametric approaches toward regression as in the classical Euclidean setting. To overcome this challenge, (global) Frechet regression, which models the relationship by fitting the (conditional) barycenters of the responses as a function of covariates, has been introduced . Notably, when the Euclidean metric is considered, Frechetregression recovers classical Euclidean regression models. For more details on Frechet regression and its recent developments, we refer readers to [30; 40; 22; 45; 27].

Nevertheless, most existing research on Frechet regression has focused on ideal scenarios characterized by abundant covariate data that are accurately measured and free of noise. In practical applications, however, high-dimensional data often arise, which are also susceptible to measurement errors and other forms of contamination. These errors can stem from various sources, such as unreliable data collection methods (_e.g._, low-resolution probes, subjective self-reports) or imperfect data storage and transmission. The high-dimensionality and the presence of measurement errors in covariates pose critical challenges for statistical inference, as regression analysis based on error-prone covariates may result in incorrect associations between variables, yielding misleading conclusions.

To address these limitations, it is crucial to extend the methodology and analysis of Frechet regression to tackle high-dimensional errors-in-variables problems. In this work, we aim to leverage the low-rank structure in the covariates to enhance the estimation accuracy and computational efficiency of Frechet regression. Specifically, we explore the extension of principal component regression to handle errors-in-variables regression problems with non-Euclidean response variables.

### Contributions

This paper contributes to advancing the (global) Frechet regression of non-Euclidean response variables, with a particular focus on high-dimensional, errors-in-variables regression.

Firstly, we propose a novel framework, called the _regularized (global) Frechet regression_ (Section 3) that combines the ideas from Frechet regression  and the principal component regression . This framework effectively utilizes the low-rank structure in the matrix of (Euclidean) covariates by extracting its principal components via low-rank matrix approximation. Our proposed method is straightforward to implement, not requiring any knowledge about the error-generating mechanism.

Furthermore, we provide a comprehensive theoretical analysis (Section 4) in three main theorems to establish the effectiveness of the proposed framework. Firstly, we prove the consistency of the proposed estimator for the true global Frechet regression model (Theorem 1). Secondly, we investigate the convergence rate of the estimator's bias and variance (Theorem 2). Lastly, we derive an upper bound for the distance between the estimates obtained using error-free covariates and those with errors-in-variables covariates (Theorem 3). Collectively, these results demonstrate that our approach effectively addresses model mis-specification and achieves more efficient model estimation by leveraging the low-rank structure of covariates, despite the presence of inherent bias due to unobserved measurement errors.

To validate our theoretical findings, we conduct numerical experiments on synthetic datasets (Section 5). We observe that the proposed method provides more accurate estimates of the regression parameters, especially in high-dimensional settings. Our experimental results emphasize the importance of incorporating the low-rank structure of covariates in Frechet regression, and provide empirical evidence that aligns with our theoretical analysis.

### Related work

**Metric-space-valued variables.** Nonparametric regression models for Riemannian-manifold-valued responses were proposed as a generalization of regression for multivariate outputs by Steinke _et al._[48; 49]. These works provided a foundation for recent developments in regression analysis of non-Euclidean responses. Later, Hein  proposed a Nadaraya-Watson-type kernel estimation of regression model for general metric-space-valued outcomes. Since then, statistical properties of regression models for some special classes of metric-space-valued outcomes, such as distribution functions [23; 52; 28] and matrix-valued responses [57; 20], have been investigated. Recently, many researchers have introduced further advances in Frechet regression, including [40; 10; 37; 45]. In this study, we use the global Frechet regression proposed in  as the basis for our proposed method.

**Errors-in-variables regression.** Much of earlier work on errors-in-variables (EIV) problems in the statistical literature can be found in , which covers the simulation-extrapolation (SIMEX) [16; 11], the attenuation correction method , covariate-adjusted model [46; 19], and the deconvolution kernel method [25; 24; 18]. The regression calibration method , instrumental variable modeling [12; 43], and the two-phase study design [9; 4] were also proposed when additional data are available for correcting measurement errors. In the high-dimensional modeling literature, regularization methods for recovering the true covariate structure can also be utilized [38; 7; 17]. Despite a diverse body of literature on high-dimensional learning and robust regression modeling, much of it assumes response spaces to be vector spaces endowed with inner products. In this paper, we tackle EIV problems within the Frechet regression framework. While previous works have explored regression analysis in non-Euclidean metric spaces, addressing EIV issues in this context remains uncharted.

**Principal component regression.** The principal component regression (PCR)  is a statistical technique that regresses response variables on principal component scores of the covariate matrix. The conventional PCR selects a few principal components as the "new" regressors associated with the first leading eigenvalues to explain the highest proportion of variations observed in the original covariate matrix. In functional data analysis, PCR is known to have a shrinkage effect on the model estimate and produce robust prediction performance in functional regression [42; 33]. Recently, Agarwal _et al._ investigated the robustness of PCR in the presence of measurement errors on covariates and the statistical guarantees for learning a good predictive model. Unlike prior statistical analyses of EIV problems that often assume known or estimable noise distributions, PCR leverages inherent low-rank structures in the covariates without requiring a priori knowledge of measurement error distributions. We adopt PCR as a concrete, practical solution to EIV models in non-Euclidean regression, driven by two compelling considerations. Firstly, the prevalence of (approximate) low-rank structures in real-world datasets enhances the practical relevance of our approach. Secondly, we intentionally opt for an approach with minimal assumptions regarding covariate errors to ensure broad applicability.

### Organization

In Section 2, we introduce the notation used throughout the paper, and overview the global Frechet regression framework. Section 3 presents the problem setup, objectives, and our proposed estimator, which we refer to as the regularized Frechet regression (Definition 4). In Section 4, we discuss theoretical guarantees on the regularized Frechet regression method in accurately estimating the global Frechet regression function. Section 5 presents the results of numerical "proof-of-concept" experiments that support the theoretical findings. Finally, we conclude this paper with discussions in Section 6. Due to space constraints, detailed proofs of the theorems as well as additional details and discussions of experiments are provided in the Appendix.

## 2 Preliminaries

### Notation

Let \(\) denote the set of positive integers and \(\) denote the set of real numbers. Also, let \(_{+}:=\{x:x 0\}\). For \(n\), we let \([n]\{1,,n\}\). We mostly use plain letters to denote scalars, vectors, and random variables, but we also use boldface uppercase letters for matrices, and curly letters to denote sets when useful. Note that we may identify a vector with its column matrix representation. For a matrix \(\), we let \(^{-1}\) denote its inverse (if exists) and \(^{}\) denote the Moore-Penrose pseudoinverse of \(\). Also, we let \(()\) and \(()\) denote the row and column spaces of \(\), respectively. Furthermore, we let \(()\) denote the set of non-zero singular values of \(\), \(_{i}()\) denote the \(i\)-th largest singular value of \(\), and \(^{()}()\{_{i}()>:i \}\) with the convention \(=\). We let \(_{n}=(1,1,,1)^{}^{d}\) and let \(\) denote the indicator function. We let \(\|\|\) denote a norm, and set \(\|\|=\|\|_{2}\) (the \(_{2}\)-norm for vectors, and the spectral norm for matrices) by default unless stated otherwise. For a finite set \(\), we may identify \(\) with its empirical measure \(_{}=|}_{x}_{x}\), where \(_{x}\) denotes the Dirac measure supported on \(\{x\}\).

Letting \(f,g:\), we write \(f(x)=O(g(x))\) as \(x\) if there exist \(M>0\) and \(x_{0}>0\) such that \(|f(x)| M g(x)\) for all \(x x_{0}\). Likewise, we write \(f(x)=(g(x))\) if \(g(x)=O(f(x))\). Furthermore, we write \(f(x)=o(g(x))\) as \(x\) if \(_{x}=0\). For a sequence of random variables \(X_{n}\), and a sequence \(a_{n}\), we write \(X_{n}=O_{p}(a_{n})\) as \(n\) if for any \(>0\), there exists \(M_{+}\) and \(N\) such that \(P}{a_{n}}>M<\) for all \(n N\). Similarly, we write \(X_{n}=o_{p}(a_{n})\) if \(_{n}P}{a_{n}}>=0\) for all \(>0\).

### Global Frechet regression

Let \((X,Y)\) be a random variable that has a joint distribution \(P_{X,Y}\) supported on \(^{p}\), where \(^{p}\) is the \(p\)-dimensional Euclidean space and \(=(,d)\) is a metric space equipped with a distance function \(d:\). We write the marginal distribution of \(X\) as \(P_{X}\), and the conditional distribution of \(Y\) given \(X\) as \(P_{Y|X}\).

**Definition 1** (Frechet regression function).: _Let \((X,Y)\) be a random element that takes value in \(^{p}\). The Frechet regression function of \(Y\) on \(X\) is a function \(^{*}:^{p}\) such that_

\[^{*}(x)=*{arg\,min}_{y}d^{2 }(Y,y)\,|\,X=x, xP_{X} ^{p}.\] (1)

We note that \(^{*}(x)\) is the best predictor of \(Y\) given \(X=x\), as it minimizes the marginal risk \(d^{2}(Y,^{*}(X))\) under the squared-distance loss. In the literature, \(^{*}(x)\) is also known as the conditional Frechet mean of \(Y\) given \(X=x\). It is important to recognize that the existence and uniqueness of the Frechet regression function are closely tied to the geometric characteristics of \(\), and are not guaranteed in general . Nonetheless, extensive research has been conducted on the existence and uniqueness of Frechet means in various metric spaces commonly encountered in practical applications. Examples include the unit circle in \(^{2}\), Riemannian manifolds , Alexandrov spaces with non-positive curvature , metric spaces with upper bounded curvature , and Wasserstein space .

While modeling and estimating the Frechet regression function \(^{*}\) is often of interest, its global (parametric) modeling may not be straightforward, especially when \(\) lacks a useful algebraic structure, such as an inner product. For instance, in classical linear regression analysis with \(=\), the conditional distribution of \(Y\) given \(X=x\) is normally distributed with a mean of \(^{*}(x)=+^{}x\) and a fixed variance \(^{2}\), where \(\) and \(\) represent the regression coefficients. Similarly, when \(\) possesses a linear-algebraic structure, one can specify a class of regression functions that quantifies the association between the expected outcome and covariates in an additive and multiplicative manner. However, the lack of an algebraic structure in general metric spaces may prevent us from characterizing the barycenter \(^{*}(x)\) in the same way classical regression analysis determines the expected value of outcomes with changing covariates.

To address this challenge, Petersen and Muller  recently proposed to exploit algebraic structures in the space of covariates, \(^{p}\), instead of \(\). Specifically, they consider a weighted Frechet mean as

\[(x)=*{arg\,min}_{y}w(X,x)  d^{2}(Y,y),\] (2)

where \(w:^{p}^{p}\) is a weight function such that \(w(,x)\) denotes the influence of \(\) at \(x\). In particular, Petersen and Muller  defined the global Frechet regression function with a specific choice of \(w\) as follows.

**Definition 2** (Global Frechet regression function).: _Let \((X,Y)\) be a random variable in \(^{p}\). Let \(=(X)\) and \(=(X)\). The global Frechet regression function of \(Y\) on \(X\) is a function \(_{}:^{p}\) such that_

\[_{}(x)=*{arg\,min}_{y} w_{}(X,x) d^{2}(Y,y)\] (3)

_where \(w_{}(X,x)=1+(X-)^{}^{-1}(x-)\)._

When \(\) is an inner product space (_e.g._, \(=\)), the function \(_{}\) restores the standard linear regression model representation over the domain \(^{p}\). For this reason, \(_{}\) is commonly referred to as the _global Frechet regression model_ for metric-space-valued outcomes .

**Remark on Definition 2** One might wonder why the term "global" is used to describe \(_{}\) as a Frechet regression function. The use of the adjective "global" serves to emphasize its distinction from "local" nonparametric regression methods that interpolate data points. Notably, when \(\) is a Hilbert space, \(_{}\) reduces to the natural linear models. For instance, if \(=\), then it follows that \(_{}(x)=w_{}(X,x) Y= +^{}(x-)\), where \(=[Y]\) and \(=^{-1}(X-) Y\). These linear models hold uniformly for the evaluation point \(x\). Similarly, in the case of an \(L^{2}\) space equipped with the squared-distance metric \(d^{2}(y,y^{})=\|y-y^{}\|_{2}^{2}\) induced by the \(L^{2}\) norm, \(_{}\) represents the linear regression model for functional responses. Thus, \(_{}\) establishes a globally defined model that spans the entire space.

[MISSING_PAGE_FAIL:5]

When \(_{n}=\{(X_{i},Y_{i})^{p}:i[n]\}\) is an IID sample from \(P_{X,Y}\), the \(\)-regularized estimator \(_{_{n}}^{()}\) subsumes the sample-analogue estimator \(_{_{n}}\) in (4) as a special case where \(=0\).

**Connection to principal component regression**. Here we remark that when \(\) is a Euclidean space, the regularized Frechet regression function \(_{}^{()}\) effectively reduces to the principal component regression. Suppose that \(=\) and \(_{n}=\{(x_{i},y_{i})^{p}:i[n]\}\) is a given dataset. Then \(_{_{n}}^{()}(x)=+_{}^{ }(x-_{_{n}})\) where \(=_{i=1}^{n}y_{i}\) and \(_{}=[^{()}_{_{ n}}]^{}_{i=1}^{n}(x_{i}-_{ _{n}})(y_{i}-)\). Observe that \(_{}\) is exactly the regression coefficient of principal component regression applied to the centered dataset \(_{n}^{}=\{(x_{i}-_{_{n}},y_{i}-) :i[n]\}\) using \(k\) principal components with \(k=_{a[p]}\{_{a}(_{_{n}^{}}) \}\).

## 4 Main results

In this section, we investigate properties of \(_{}^{()}\) for \( 0\), with a focus on two cases: \(=_{n}\) and \(=}_{n}\), cf. Section 3.1. By denoting the true distribution that generates \((X,Y)\) as \(^{*}\), we can express \(_{}\) as \(_{^{*}}^{(0)}\). To analyze the discrepancy between the estimator \(_{}^{()}(x)\) and \(_{}(x)\), we examine the relationships depicted in the schematic in Figure 1. Our theoretical findings can be summarized as follows: Even in the presence of covariate noises, \(_{}_{n}}^{()}\) with a suitable \(>0\) can effectively eliminate the noise in \(Z\) to estimate \(X\), thereby reducing the error in estimating \(_{}\).

### Model assumptions and examples

We impose the following assumptions for our analysis.

* (Existence) For any probability distribution \(\) and any \(_{+}\), the object \(_{}^{()}(x)\) exists (almost surely) and is unique. In particular, \(_{y:\,d(y,_{}(x))>}R(y;x)>R( _{}(x);x)\) for all \(>0\), where \(R(y;x) R_{^{*}}^{(0)}(y;x)\).
* (Growth) There exist \(D_{}>0\), \(C_{}>0\) and \(>1\), possibly depending on \(x\), such that for any probability distribution \(\) and any \(_{+}\), \[dy,_{}^{()}(x)<D_{}&  R_{}^{()}(y;x)-R_{}^{()} _{}^{()}(x);x C_{} dy,_{}^ {()}(x)^{},\\ dy,_{}^{()}(x) D_{}&  R_{}^{()}(y;x)-R_{}^{()}_{}^{( )}(x);x C_{} D_{}^{}.\] (10)
* (Bounded entropy) There exists \(C_{}>0\), possibly depending on \(y\), such that \[_{ 0}_{0}^{1}B_{d} y,,}\, C_ {},\] (11) where \(B_{d}(y,)\{y^{}:d(y,y^{})\}\) and \((S,)\) is the \(\)-covering number2 of \(S\).

Figure 1: A schematic for the relationship between the regularized Fr√©chet regression estimators.

Assumption (C0) is common to establish the consistency of an M-estimator [55, Chapter 3.2]; in particular, it ensures the weak convergence of the empirical process \(R_{_{n}}^{()}\) to the population process \(R_{^{*}}^{()}\) implying convergence of their minimizers. Furthermore, the conditions on the curvature (C1) and the covering number (C2) control the behavior of the objectives near the minimum in order to obtain rates of convergence; it is worth mentioning that (C2) corresponds to a (locally) bounded entropy for every \(y\), while (P1) in  requires the same condition only with \(y=_{}(x)\). These conditions arise from empirical process theory and are also commonly adopted .

Here we provide several examples of the space \(\), in which the conditions (C0), (C1) and (C2) are satisfied. We verify the conditions in Appendix A; see Propositions 1, 2, 3, and 4.

**Example 1**.: _Let \(=(,d_{})\) be a finite-dimensional Hilbert space \(\) equipped with the Hilbert-Schmidt metric \(d_{}(y_{1},y_{2})= y_{1}-y_{2},y_{1}-y_{2}^ {1/2}\), e.g., \(=(^{r},d_{2})\) where \(d_{2}\) is the \(^{2}\)-metric._

**Example 2**.: _Let \(\) be \(\), the set of probability distributions \(G\) on \(\) such that \(_{}x^{2}\,dG(x)<\), equipped with the Wasserstein metric \(d_{W}\) defined as_

\[d_{W}(G_{1},G_{2})^{2}=_{0}^{1}(G_{1}^{-1}(t)-G_{2}^{-1}(t))^{2 }\,dt,\]

_where \(G_{1}^{-1}\) and \(G_{2}^{-1}\) are the quantile functions of \(G_{1}\) and \(G_{2}\), respectively. See [40, Section 6]._

**Example 3**.: _Let \(=\{M^{r r}:M=M^{T},M 0M_{ii}=1, i[r]\}\) be the set of correlation matrices of size \(r\), equipped with the Frobenius metric, \(d_{F}(M,M^{})=\|M-M^{}\|_{F}\)._

**Example 4**.: _Let \(\) be a (bounded) Riemannian manifold of dimension \(r\), and let \(d_{g}\) be the geodesic distance induced by the Riemannian metric._

### Theorem statements

#### 4.2.1 Noiseless covariate setting

We first verify the consistency of the \(\)-regularized Frechet regression function as follows.

**Theorem 1** (Consistency).: _Suppose that Assumption (C0) holds. If \(\,()<\), then for any \(\) such that \(0<\{_{i}(_{^{*}}):_{i}(_{^{*}})>0\}\), and any \(x^{p}\),_

\[d_{_{n}}^{()}(x),_{^{*}}^{()} (x)=o_{P}(1)\ n.\] (12)

If \(<^{(0)}(_{^{*}})=\{_{i}(_{^{*}}): _{i}(_{^{*}})>0\}\), then the regularized estimator \(_{_{n}}^{()}(x)\) effectively reduces to the same as the sample-analog estimator \(_{_{n}}(x)\) in (4) in the limit \(n\). Thus, \(_{_{n}}^{()}(x)\) inherits the consistency of \(_{_{n}}\). We provide a detailed proof of Theorem 1 in Appendix B.

In addition to the consistency of \(_{_{n}}^{()}\) in the small \(\) limit, we present an analysis for the convergence rate of \(_{_{n}}^{()}(x)\) in the following theorem.

**Definition 5**.: _The Mahalanobis seminorm of \(x\) induced by a positive semidefinite matrix \(\) is \(\|x\|_{}(x^{}^{}x)^{1/2}\)._

**Theorem 2** (Rate of convergence).: _Suppose that Assumptions (C0)-(C2) hold. If \(\,()<\), then for any \(_{+}\) and \(x^{p}\) such that \(\|x-_{^{*}}\|_{_{^{*}}}}D_{}^{}}{\,()^{2}\,_{^{*}}}}\),_

\[d_{_{n}}^{()}(x),_{^{*}}^{(0)}(x) =O_{P}_{}(x)^{}+n^{- {2(-1)}} n,\] (13)

_where \(_{}(x)=(_{^{*}}-_{^{*} }^{()})^{}\|x-_{^{*}}\|_{_{^{*}}- _{^{*}}^{()}}\)._

We obtain Theorem 2 by showing a "bias" upper bound \(d_{^{*}}^{()}(x),_{^{*}}^{(0)}(x)=O _{}(x)^{}\) and a "variance" bound \(d_{_{n}}^{()}(x),_{^{*}}^{()} (x)=O_{P}n^{-}\); see Lemmas 1 and 2 in Appendix C. Here we remark that \(_{}(x)\) is a monotone non-decreasing function of \(\), and if \(<^{(0)}(_{^{*}})\) then \(_{}(x)=0\). Also, the condition on \(\|x-_{^{*}}\|_{_{^{*}}}\) is introduced for a technical reason, and can be removed when \(D_{}=\). Note that Condition (C1) holds with \(D_{}=\) and \(=2\) for Examples 1, 2 and 3. Thus, we have \(d_{_{n}}^{()}(x),_{^{*}}^{(0)}(x)= O_{P}_{}(x)+n^{-}\) as \(n\).

#### 4.2.2 Error-prone covariate setting

Given a set \(_{n}=\{(x_{i},y_{i}):i[n]\}\), let \(_{_{n}}[x_{1} x_{n}]^{}^{n p}\). We let \(=_{_{n}}\) and \(=_{}_{n}}\) for shorthand, and further, we let \(_{}=(_{n}-_{n}_{n}^{} )\) and \(_{}=(_{n}-_{n}_{n}^{} )\) denote the 'row-centered' matrices.

**Theorem 3** (De-noising covariates).: _Suppose that Assumptions (C0) and (C1) hold. Then there exists a constant \(C>0\) such that for any \(_{+}\), if \(x_{_{n}}+\,_{}\) and_

\[\|x-_{_{n}}\|_{_{_{n}}}( } D_{}^{}}{2\,\,()} (_{})^{()}( _{})}{\|-\|}-1),\] (14)

_then_

\[d(_{}_{n}}^{()}(x),_{_{n}}^{()}(x)) C(-\|}{^{( )}(_{})^{()}(_{})}_{n}}\|_{_{_{n}}}+1}{C_{}})^{}.\] (15)

Note that the condition on \(\|x-_{^{*}}\|_{_{^{*}}}\) in (14) can be removed when \(D_{}=\). We highlight that the quantity \(-\|}{^{()}(_{}) ^{()}(_{})}\) acts as the reciprocal of the signal-to-noise ratio. Here, \(\|-\|\) quantifies the magnitude of the "noise" in the covariates, while \(\{^{()}(_{}),\,^{()}(_{})\}\) measures the strength of the "signal" retained in the \(\)-SVT of the design matrix. We observe that the error bound (15) increases proportionally to the normalized deviation of \(x\) from the mean, \(_{_{n}}\), which is a reasonable outcome. For the complete version of Theorem 3 and its proof, refer to Appendix D.

**Remarks on Theorem 3.** We avoid imposing distributional assumptions on the noise \(=Z-X\), to ensure broad applicability of the result. Also, the inequality (15) is sharp, as there is a worst-case noise instance that attains equality (up to a multiplicative constant). Despite its generality, this upper bound highlights effective error mitigation in specific scenarios. For instance, consider well-balanced, effectively low-rank covariates \(^{n p}\) such that \(|X_{ij}|=(1)\) for all \(i,j\) and \(_{1}()_{r}()_{r+1}()_ {n p}()=O(1)\), where \(r n p\) is the effective rank of \(\). Then \(_{1}()^{2}_{r}()^{2}\|\|_{r}^{2}/r  np/r\). Additionally, if \(=+\) where \(\) is a random matrix with independent sub-Gaussian rows, then \(\|-\|+\) with high probability. In the random design scenario where the rows of \(\) and the test point \(x\) are drawn IID from the same distribution, \(\|x-_{D_{n}}\|_{} 1\) with high probability. Consequently, the upper bound in (15) is bounded by \(+\), which diminishes to 0 when \(r n p\).

### Proof sketches

**Proof of Theorem 1.** We show that \(R_{_{n}}^{()}(y;x)\) weakly converges to \(R_{^{*}}^{(0)}(y;x)\) in the \(^{}()\)-sense. According to [55, Theorem 1.5.4], it suffices to show that (1) \(R_{_{n}}^{()}(y;x)-R_{^{*}}^{(0)}(y;x)=o_{p}(1)\) for all \(y\), and (2) \(R_{_{n}}^{()}\) is asymptotically equicontinuous in probability.

**Proof of Theorem 2.** We prove upper bounds for the bias and the variance separately.

_To control the bias_ (Appendix C, Lemma 1), we show an upper bound for \(R^{()}(x);x-R(x);x\), and convert it to restrain the distance between the minimizers \(d^{()}(x),(x)\) using the Growth condition (C1). In this conversion, we employ the "peeling technique" in empirical process theory.

_To control the variance_ (Appendix C, Lemma 2), we follow a similar strategy as in Lemma 1, but with additional technical considerations. Defining the 'fluctuation variable' \(Z_{n}^{()}(y;x) R_{_{n}}^{()}(y;x)-R_{^{*}} ^{()}(y;x)\) parameterized by \(y\), we derive a probabilistic upper bound for \(R_{^{*}}^{()}(_{_{n}}^{()}(x);x)-R_{^{*}} ^{()}(_{^{*}}^{()}(x);x)\) by establishing a uniform upper bound for \(Z_{n}^{()}(y;x)-Z_{n}^{()}((x);x)\); here, the Entropy condition (C2) is used. Again, we use the Growth condition (C1) and the peeling technique to obtain a probabilistic upper bound for the distance \(d(_{_{n}}^{()}(x),_{_{n}}^{()}(x))\).

**Proof of Theorem 3.** Expressing the difference in the weights \(w_{}_{n}}^{()}(y;x)-w_{_{n}}^{()}(y;x)\) in terms of \(\) and \(\), we utilize classical matrix perturbation theory to control \(R_{_{n}}^{()}(y;x)-R_{_{n}}^{()}(y;x)\), and transform it to an upper bound on the distance \(d(_{}_{n}}^{()}(x),_{_{n}}^{( )}(x))\) using the Growth condition (C1).

Experiments

In this section, we present numerical simulation results to validate and support our theoretical findings. We focus on global Frechet regression analysis for one-dimensional distribution functions (Example 2). These simulations cover various conditions, allowing us to evaluate and compare our methodology's performance with alternative approaches. For a summary of the experimental results, please refer to Figure 2 and Table 1. Further details about simulation settings and additional results are provided in Appendix E.

**Experimental setup.** We consider combinations of \(p\{150,300,600\}\) and \(n\{100,200,400\}\). The datasets \(_{n}=\{(X_{i},Y_{i}):i[n]\}\) and \(}_{n}=\{(Z_{i},Y_{i}):i[n]\}\) are generated as follows. Let \(X_{i}_{p}_{p},\) be IID multivariate Gaussian with mean \(_{p}\) and covariance \(\) such that \(()=\{_{j}>0:j[p]\}\) is an exponentially decreasing sequence such that \(()=_{j=1}^{p}_{j}=p\) and \(_{1}/_{p}=10^{3}\). Note that \(_{j=1}^{ p/3}_{j}_{j^{}=1}^{p} _{j^{}} 0.9\), and thus, \(\) is effectively low-rank. We generate \(Z_{i}\) following (5) under two scenarios \(_{ij}0,_{}^{2} \) and \(_{ij}0,_{ }\), respectively. Lastly, given \(X=x\), let \(Y\) be the distribution function of \(_{,}(x)+,^{2}\), where (i) \(_{,}(x)=+^{}x\) with \(=1\) and \(=p^{-1/2}_{p}\); (ii) \(0,_{}^{2}\); and (iii) \(^{2}(s_{1},s_{2})\), an inverse gamma distribution with shape \(s_{1}\) and scale \(s_{2}\). We performed \(B=500\) Monte Carlo experiments by drawing \(_{n}^{(b)}\) and \(}_{n}^{(b)}\) as independent copies of \(_{n}\) and \(}_{n}\), respectively, for \(b[B]\).

**Performance evaluation.** We assess the in-sample and out-of-sample performance of the Frechet regression function estimator by using the mean squared error (MSE) and the mean squared prediction error (MSPE). To this end, we create a "test set" \(_{N}^{}=\{(X_{i}^{},Y_{i}^{}):i[N]\}\), with \(N=1000\). The MSE and the MSPE are computed as the average of squared metric-distance residuals from the observed responses in the "training set" \(_{n}\) and in the "test set" \(_{N}^{}\)), respectively:

\[(_{}^{()})=_{i=1}^{n}d_{W} Y_{i},_{}^{()}(X_{i})^{2}\;\;\; \;(_{}^{()})=_{i=1}^{N}d_{W }Y_{i}^{},_{}^{()}(X_{i}^{}) ^{2}.\]

We report the MSE averaged over \(B=500\) random trials: \((_{}^{()})=B^{-1}_{b=1}^{B} (_{^{()}}^{()})\), and likewise for MSPE. Furthermore, we evaluate the accuracy and efficiency of the estimator using bias and variance, with detailed definitions deferred to Appendix E.

**Simulation results.** Our numerical study demonstrates that the proposed SVT method consistently improves both estimation and prediction performance, especially in the errors-in-variables setting. Figure 2 highlights how the SVT estimator outperforms the naive errors-in-variables (EIV) estimator, which corresponds to SVT with \(=0\). The naive EIV suffers from an intrinsic model bias, called the attenuation effect , as it regresses responses on error-prone covariates. This leads to a misrepresentation of the association between responses and true covariates, potentially leading to statistical inference based on a mis-specified model.

Remarkably, the SVT estimator achieved a smaller MSPE even compared to the oracle estimator (REF) obtained from the error-free sample. Although the REF estimator had the smallest MSE due to its small bias, we observed its overfitting to the training sample, resulting in poor prediction performance. Notably, even the naive EIV estimator outperformed the REF estimator in MSPE. We believe this is mainly because the true covariate matrix was nearly singular in our simulation setup, causing multicollinearity issues for the REF. In contrast, measurement errors introduced non-ignorable minimum singular values in the EIV covariate matrix, unintentionally mitigating multicollinearity for the naive EIV and causing it to behave like ridge regression.

## 6 Discussion

This paper has addressed errors-in-variables regression of non-Euclidean response variables through the (global) Frechet regression framework enhanced by low-rank approximation of covariates. Specifically, we introduce a novel _regularized (global) Frechet regression_ framework (Section 3), which combines the Frechet regression with principal component regression. We also provide a comprehensive theoretical analysis in three main theorems (Section 4), and validate our theory through numerical experiments on simulated datasets. Moreover, our numerical experiments demonstrate empirical evidence of the effectiveness and superiority of our approach, reinforcing its practical relevance and potential impact in non-Euclidean regression analysis.

We conclude this paper by proposing several promising directions for future research. First, it would be worthwhile to explore the large sample theory for selecting the optimal threshold parameter \(\) in the proposed SVT method, in order to characterize the theoretical phase transition of the bias-variance trade-off in the regularized (global) Frechet regression. Second, we believe that our framework could be extended to errors-in-variables Frechet regression for response variables in a broader class of metric spaces, e.g., by leveraging the quadruple inequality proposed by Schotz [44; 45]. Lastly, investigating the asymptotic distribution of the proposed SVT estimator would be highly appealing in the statistical literature, as it would enable us to make statistical inferences on the conditional Frechet mean in non-Euclidean spaces [6; 8] with errors-in-variables covariates.