# Scalable DP-SGD: Shuffling vs. Poisson Subsampling

Lynn Chua

Google Research

chualynn@google.com &Badih Ghazi

Google Research

badihghazi@gmail.com &Pritish Kamath

Google Research

pritishk@google.com &Ravi Kumar

Google Research

ravi.k53@gmail.com &Pasin Manurangsi

Google Research

pasin@google.com &Amer Sinha

Google Research

amersinha@google.com &Chiyuan Zhang

Google Research

chiyuan@google.com

###### Abstract

We provide new lower bounds on the privacy guarantee of the _multi-epoch_ Adaptive Batch Linear Queries (ABLQ) mechanism with _shuffled batch sampling_, demonstrating substantial gaps when compared to _Poisson subsampling_; prior analysis was limited to a single epoch. Since the privacy analysis of Differentially Private Stochastic Gradient Descent (DP-SGD) is obtained by analyzing the ABLQ mechanism, this brings into serious question the common practice of implementing shuffling-based DP-SGD, but reporting privacy parameters as if Poisson subsampling was used. To understand the impact of this gap on the utility of trained machine learning models, we introduce a practical approach to implement Poisson subsampling _at scale_ using massively parallel computation, and efficiently train models with the same. We compare the utility of models trained with Poisson-subsampling-based DP-SGD, and the optimistic estimates of utility when using shuffling, via our new lower bounds on the privacy guarantee of ABLQ with shuffling.

## 1 Introduction

A common approach for private training of differentiable models, such as neural networks, is to apply first-order methods with noisy gradients. This general framework is known as DP-SGD (Differentially Private Stochastic Gradient Descent) (Abadi et al., 2016); the framework itself is compatible with any optimization sub-routine. Multiple open source implementations exist for applying DP-SGD in practice, namely, Tensorflow Privacy, JAX Privacy (Balle et al., 2022) and PyTorch Opacus (Yousefpour et al., 2021); and DP-SGD has been applied widely in various machine learning domains (e.g., Tramer and Boneh, 2020; De et al., 2022; Bu et al., 2022; Chen et al., 2020; Dockhorn et al., 2023; Anil et al., 2022; He et al., 2022; Igamberdiev et al., 2024; Tang et al., 2024).

DP-SGD (Algorithm 1) processes the training data in a sequence of steps, where at each step, a noisy estimate of the average gradient over a mini-batch is computed and used to perform a first-order update over the differentiable model. To obtain the noisy (average) gradient, the gradient \(g\) for each example in the mini-batch is _clipped_ to have norm at most \(C\) (a pre-determined fixed bound), by setting \([g]_{C}:=g\{1,C/\|g\|_{2}\}\), and computing the sum over the batch; then independent zero-mean noise drawn from the Gaussian distribution of scale \( C\) is added to each coordinate of the summed gradient. This could then be scaled by the "target" mini-batch size to obtain a noisyaverage gradient.1 The privacy guarantee of the mechanism depends on the following parameters: the noise scale \(\), the number of examples in the training dataset, the size of mini-batches, the number of training steps, and the _mini-batch generation process_.

In practice, almost all deep learning systems generate mini-batches of fixed-size by sequentially going over the dataset, possibly applying a global _shuffling_ of all the examples in the dataset for each training epoch; each epoch corresponds to a single pass over the dataset, and the ordering of the examples may be kept the same or resampled between different epochs. However, performing the privacy analysis for such a mechanism has appeared to be technically difficult due to correlation between the different mini-batches. Abadi et al. (2016) instead consider a different mini-batch generation process of _Poisson subsampling_, wherein each mini-batch is generated independently by including each example with a fixed probability. This mini-batch generation process is however rarely implemented in practice, and consequently it has become common practice to use some form of shuffling in applications, but to report privacy parameters as if Poisson subsampling was used (see, e.g., the survey by Ponomareva et al. (2023, Section 4.3)). A notable exception is the PyTorch Opacus library (Yousergour et al., 2021) that supports the option of Poisson subsampling; however, this implementation only works well for datasets that allow efficient random access (for instance by loading it entirely into memory). To the best of knowledge, Poisson subsampling has not been used for training with DP-SGD on massive datasets.

The privacy analysis of DP-SGD is usually performed by viewing it as a post-processing of an _Adaptive Batch Linear Queries (\(\))_ mechanism that releases the estimates of a sequence of adaptively chosen linear queries on the mini-batches (formal definitions in Section 2.1). Chua et al. (2024) showed that the privacy loss of \(\) with shuffling can be significantly higher than that with Poisson subsampling for small values of \(\). Even though their analysis only applied to a _single epoch_ mechanism, this has put under serious question the aforementioned common practice of implementing DP-SGD with some form of shuffling while reporting privacy parameters assuming Poisson subsampling. The motivating question for our work is:

Which batch sampler provides the best utility for models trained with DP-SGD, when applied with the correct corresponding privacy accounting?

### Contributions

Our contributions are summarized as follows.

Privacy Analysis of Multi-Epoch \(\) with Shuffling.We provide _lower bounds_ on the privacy guarantees of shuffling-based \(\) to handle _multiple epochs_. We consider the cases of both (i) _Persistent Shuffling_, wherein the examples are globally shuffled once and the order is kept the same between epochs, and (ii) _Dynamic Shuffling_, wherein the examples are globally shuffled independently for each epoch. Since our technique provides a lower bound on the privacy guarantee, the utility of the models obtained via shuffling-based DP-SGD with this privacy accounting is an optimistic estimate of the utility under the correct accounting.

Scalable Implementation of DP-SGD with Poisson subsampling via Truncation.Variable batches are typically inconvenient to handle in deep learning systems. For example, upon a change in the input shape, jax.jit triggers a recompilation of the computation graph, and tf.function will retrace the computation graph. Additionally, Google TPUs require all operations to have fixed input and output shapes. We introduce _truncated Poisson subsampling_ to circumvent variable batch sizes. In particular, we choose an upper bound on the maximum batch size \(B\) that our training can handle, and given any variable size batch \(b\), if \(b B\), we randomly sub-select \(B\) examples to retain in the batch, and if \(b<B\), we pad the batch with \(B-b\) dummy examples _with zero weight_. This deviates slightly from the standard Poisson subsampling process since our batch sizes can never exceed \(B\). We choose \(B\) to be sufficiently larger than the expected batch size, so that the probability that the sampled batch size \(b\) exceeds the maximum allowed batch size \(B\) is small. We provide a modification to the analysis of ABLQ with Poisson subsampling in order to handle this difference.

Generating these truncated Poisson subsampled batches can be difficult when the dataset is too large to fit in memory. We provide a scalable approach to the generation of batches with truncated Poisson subsampling using _massively parallel computation_(Dean and Ghemawat, 2004). This can be easily specified using frameworks like beam(Apache Beam) and implemented on distributed platforms such as Apache Flink, Apache Spark, or Google Cloud Dataflow.

Our detailed experimental results are presented in Section4, and summarized below:

* DP-SGD with Shuffle batch samplers performs similarly to Poisson subsampling for the same \(\).
* However, DP-SGD with Shuffle batch samplers, with our optimistic privacy accounting, perform worse than Poisson subsampling in high privacy regimes (small values of \(\)).

Thus, our results suggest that Poisson subsampling is a viable option for implementing DP-SGD at scale, with almost no loss in utility compared to the traditional approach that uses shuffling with (incorrect) accounting assuming Poisson subsampling.

### Related Work

Chua et al. (2024) demonstrated gaps in the privacy analysis of ABLQ using shuffling and Poisson subsampling, by providing a _lower bound_ on the privacy guarantee of ABLQ with shuffling; their technique, however, was specialized for one epoch. We extend their technique to the _multi-epoch_ version of ABLQ with shuffling and provide lower bounds for both persistent and dynamic batching. Lebeda et al. (2024) also point out gaps in the privacy analysis of ABLQ with Poisson subsampling and with sampling batches of fixed size independently, showing that the latter has worse privacy guarantees than Poisson subsampling. We do not cover this sampling in our experimental study, since sampling independent batches of fixed size is not commonly implemented in practice, and DP-SGD using this sampling is only expected to be worse as compared to Poisson subsampling. Yousefpour et al. (2021) report the model utility (and computational cost overhead) under training with DP-SGD with Poisson subsampling. However, to the best of our knowledge, there is no prior work that has compared the model utility of DP-SGD under Poisson subsampling with that under shuffling, let alone compared it against DP-SGD under (Dynamic/Persistent) shuffling or studied the gaps between the privacy accounting of the two approaches.

One possible gap between the privacy analysis of DP-SGD and ABLQ is that the former only releases the final iterate, whereas the latter releases the responses to all the queries. An interesting result by Annamalai (2024) shows that in general the privacy analysis of the _last-iterate_ of DP-SGD cannot be improved over that of ABLQ, when using Poisson subsampling. This suggests that at least without any further assumptions, e.g., on the loss function, it is not possible to improve the privacy analysis of DP-SGD beyond that provided by ABLQ; this is in contrast to the techniques of privacy amplification by iteration for convex loss functions (e.g. Feldman et al., 2018; Altschuler and Talwar, 2022).

## 2 Preliminaries

A differentially private (DP) mechanism \(:^{*}_{}\) can be viewed as a mapping from input datasets to distributions over an output space, namely, on input _dataset_\(=(x_{1},,x_{n})\) where each _example_\(x_{i}\), \(()\) is a probability measure over the output space \(\); for ease of notation, we often refer to the corresponding random variable also as \(()\). Two datasets \(\) and \(^{}\) are said to be _adjacent_, denoted \(^{}\), if they "differ in one example"; in particular, we use the "zeroing-out" adjacency defined shortly.

**Definition 2.1** (Dp).: For \(, 0\), a mechanism \(\) satisfies \((,)\)-DP if for all "adjacent" datasets \(^{}\), and for any (measurable) event \(\) it holds that \([()]\ \ e^{}[(^{}) ]+\).

For any mechanism \(\), we use \(_{}:_{ 0}\) to denote its _privacy loss curve_, namely \(_{}()\) is the smallest \(\) such that \(\) satisfies \((,)\)-DP; \(_{}:_{ 0}\) is defined similarly.

### Adaptive Batch Linear Queries Mechanism

Following the notation in Chua et al. (2024), we study the adaptive batch linear queries mechanism \(_{}\) (Algorithm 2) using a _batch sampler_\(\) and an _adaptive query method_\(\), defined. The batch sampler \(\) can be any algorithm that randomly samples a sequence \(S_{1},,S_{T}\) of batches. \(_{}\) operates by processing the batches in a sequential order, and produces a sequence \((g_{1},,g_{T})\), where the response \(g_{t}^{d}\) is produced as the sum of \(_{t}(x)\) over the batch \(S_{t}\) with added zero-mean Gaussian noise of scale \(\) to all coordinates, where the query \(_{t}:^{d}\) (for \(^{d}:=\{v^{d}:\|v\|_{2} 1\}\)) is produced by the adaptive query method \(\), based on the previous responses \(g_{1},,g_{t-1}\). DP-SGD can be viewed as a post-processing of an adaptive query method that maps examples to the clipped gradient at the last iterate, namely \(_{t}(x):=[_{}(_{t-1},x)]_{1}\) (we treat the clipping norm \(C=1\) for simplicity, as it is just a scaling term).

```
0: Batch sampler \(\) (samples \(T\) batches), noise scale \(\), and (adaptive) query method \(:(^{d})^{*}^{d}\).
0: Dataset \(=(x_{1},,x_{n})\).
0: Query estimates \(g_{1},,g_{T}^{d}\) \((S_{1},,S_{T})(n)\) for\(t=1,,T\)do \(_{t}():=(g_{1},,g_{t-1};)\) \(g_{t} e_{t}+_{i S_{i}}_{t}(x_{i})\) for \(e_{t}(0,^{2}I_{d})\) return\((g_{1},,g_{T})\) ```

**Algorithm 2**\(_{}\): Adaptive Batch Linear Queries (as formalized in Chua et al. (2024))

In this work, we consider the following _multi-epoch_ batch samplers: Deterministic \(_{b,T}\), Persistent Shuffle \(^{}_{b,T}\), and Dynamic Shuffle \(^{}_{b,T}\) batch sampler defined as instantiations of Algorithm 3 in Figure 1 and truncated Poisson \(_{b,B,T}\) (Algorithm 4); we drop the subscripts of each sampler whenever it is clear from context. Note that, while \(_{b,B,T}\) has no restriction on the value of \(n\), the samplers \(_{b,T}\), \(^{}_{b,T}\), and \(^{}_{b,T}\) require that the number of examples \(n\) is such that \(E:=bT/n\) and \(S:=n/b\) are integers, where \(E\) corresponds to the number of epochs and \(S\) corresponds to the number of steps per epoch. We call the tuple \((n,b,T)\) as "valid" if that holds, and we will often implicitly assume that this holds. Also note that \(_{b,B,T}\) corresponds to the standard Poisson subsampling without truncation when \(B=\). We use \(_{}()\) to denote the _privacy loss curve_ of \(_{}\) for any \(\{,,^{},^{}\}\), where other parameters such as \(,T\), etc. are implicit. Namely, for all \(>0\), let \(_{}()\) be the smallest \( 0\) such that \(_{}\) satisfies \((,)\)-DP for all choices of the underlying adaptive query method \(\). We define \(_{}()\) similarly. Finally, we define \(_{}(,)\) as the smallest \(\) such that \(_{}\) satisfies \((,)\)-DP, with other parameters being implicit in \(\).

**Adjacency notion.** The common notion of _Add-Remove_ adjacency is not applicable for mechanisms such as \(_{}\), \(_{S^{}}\), \(_{S^{}}\) because these methods require that \(bT/n\) and \(n/b\) are integers, and changing \(n\) by \( 1\) does not respect this requirement. And while the other common notion of _Substitution_ is applicable for all the mechanisms we consider, the standard analysis for \(_{}\) is done w.r.t Add-Remove adjacency (Abadi et al., 2016, Mironov, 2017). Therefore, we use the "Zeroing-out" adjacency introduced by Kairouz et al. (2021), namely we consider the augmented input space \(_{}:=\{\}\) where any adaptive query method \(\) is extended as \((g_{1},,g_{t};):=\) for all \(g_{1},,g_{t}^{d}\). Datasets \(,^{}_{}^{n}\) are said to be _zero-out_ adjacent if there exists \(i\) such that \(_{-i}=^{}_{-i}\), and exactly one of \(\{x_{i},x_{i}^{}\}\) is in \(\) and the other is \(\). We use \(_{z}^{}\) to specifically denote adjacent datasets with \(x_{i}\) and \(x_{i}^{}=\). Thus \(^{}\) if either \(_{z}^{}\) or \(^{}_{z}\).

### Dominating Pairs

For two probability density functions \(P\) and \(Q\) and \(,_{ 0}\), we use \( P+ Q\) to denote the weighted sum of the density functions. We use \(P Q\) to denote the product distribution sampled as \((_{1},_{2})\) for \(_{1} P\), \(_{2} Q\), and, \(P^{ T}\) to denote the \(T\)-fold product distribution \(P P\). For all \(\), the \(e^{}\)_-hockey stick divergence_ between \(P\) and \(Q\) is \(D_{e^{}}(P\|Q):=_{}P()-e^{}Q()\). Thus, by definition a mechanism \(\) satisfies \((,)\)-DP iff for all adjacent \(^{}\), it holds that \(D_{e^{}}(()\|(^{}))\).

**Definition 2.2** (Dominating Pair (Zhu et al., 2022)).: The pair \((P,Q)\)_dominates_ the pair \((A,B)\) (denoted \((P,Q)(A,B)\)) if \(D_{e^{}}(P\|Q)\ \ D_{e^{}}(A\|B)\) holds for all \(\). We say that \((P,Q)\)_dominates_ a mechanism \(\) (denoted \((P,Q)\)) if \((P,Q)((),(^{}))\) for all adjacent \(_{z}^{}\).

If \((P,Q)\), then for all \( 0\), it holds that \(_{}()\{D_{e^{}}(P\|Q),D_{e^{ }}(Q\|P)\}\), and conversely, if there exists adjacent datasets \(_{z}^{}\) such that \(((),(^{}))(P,Q)\), then \(_{}()\{D_{e^{}}(P\|Q),D_{e^{ }}(Q\|P)\}\). When both of these hold, we say that \((P,Q)\)_tightly dominates_ the mechanism \(\) (denoted \((P,Q)\)) and in this case it holds that \(_{}()=\{D_{e^{}}(P\|Q),D_{e^{ }}(Q\|P)\}\). Thus, tightly dominating pairs completely characterize the privacy loss of a mechanism (although they are not guaranteed to exist for all mechanisms). Dominating pairs behave nicely under mechanism compositions: if \((P_{1},Q_{1})_{1}\) and \((P_{2},Q_{2})_{2}\), then \((P_{1} P_{2},Q_{1} Q_{2})_{1}_{2}\), where \(_{1}_{2}\) denotes the (adaptively) composed mechanism.

## 3 Privacy analysis of _multi-epoch \(_{}\)_

We discuss the privacy analysis of \(_{}\) for \(\{,,^{},^{ }\}\) via dominating pairs.

Privacy analysis for \(_{}\).A single epoch of the \(_{}\) mechanism corresponds to a Gaussian mechanism with noise scale \(\). And thus, \(E:=bT/n\) epochs of the \(_{}\) mechanism corresponds to an \(E\)-fold composition of the Gaussian mechanism, which is privacy-wise equivalent to a Gaussian mechanism with noise scale \(/\)(Dong et al., 2019, Corollary 3.3). Thus, a closed-form expression for \(_{}()\) exists via the dominating pair \((P_{}=(1,}{E}),Q_{}=(0,}{E}))\).

**Theorem 3.1** (Balle and Wang (2018, Theorem 8)).: _For all \(>0\), \( 0\), and valid \(n\), \(b\), \(T\), it holds that_

\[_{}()=(-^{}+})-e^{}(-^{}- }),^{}=},\]

_and \(()\) is the cumulative density function (CDF) of the standard normal random variable \((0,1)\)._

Figure 1: Various natural instantiations of the permutation batch sampler.

Privacy analysis of \(}}\).First, let us consider the case of Poisson subsampling without truncation, namely \(B=\). Zhu et al. (2022) showed2 that the tightly dominating pair for a single step of \(}}\), a Poisson sub-sampled Gaussian mechanism, is given by the pair \((U=(1-q)(0,^{2})+q(1,^{2}),V=(0, ^{2}))\), where \(q\) is the sub-sampling probability of each example, namely \(q=b/n\). Since \(}}\) is a \(T\)-fold composition of this Poisson subsampled Gaussian mechanism, it follows that \((P_{}:=U^{},Q_{}:=V^{ T}) }}\).

A finite value of \(B\) however changes the mechanism slightly. In order to handle this, we use the following proposition, where \(d_{}(P,P^{})\) denotes the _statistical distance_ between \(P\) and \(P^{}\).

**Proposition 3.2**.: _For distributions \(P,P^{},Q,Q^{}\) such that \(d_{}(P,P^{}),d_{}(Q,Q^{})\), and \(D_{e^{}}(P^{}\|Q^{})\), then \(D_{e^{}}(P\|Q)+(1+e^{})\)._

Proof.: For any event \(\) we have that

\[P()}}{{}}P^{}()+ }}{{}}e^{}Q^{}() ++}}{{}}e^{}(Q( )+)++\ =\ e^{}Q()++(1+e^{}),\]

where (i) follows from \(d_{}(P,P^{})\), (ii) follows from \(D_{e^{}}(P^{}\|Q^{})\) and (iii) follows from \(d_{}(Q,Q^{})\). Thus, we get that \(D_{e^{}}(P\|Q)+(1+e^{})\). 

The batch size \(|S_{t}|\) before truncation in \(_{b,B,T}\) is distributed as the binomial distribution \((n,b/n)\), and thus, by a union bound over the events that the sampled batch size \(|S_{t}|>B\) at any step, it follows that for any input dataset \(\),

\[d_{}(_{b,B,T}}}(),_{b,,T}}}()) T(n,b,B),\]

where \((n,b,B):=_{r(n,b/n)}[r>B]\). Applying Proposition3.2 we get

**Theorem 3.3**.: _For all \(>0\), \( 0\), and integers \(b\), \(n b\), \(B b\), \(T\), it holds that_

\[_{}()\{D_{e^{}}(P_{ }\|Q_{}),D_{e^{}}(Q_{}\|P_{})\}+ T(1+e^{})(n,b,B).\]

While the hockey stick divergences \(D_{e^{}}(P_{}\|Q_{})\) and \(D_{e^{}}(Q_{}\|P_{})\) do not have closed-form expressions, upper bounds on these can be obtained using privacy accountants based on the methods of Renyi DP (RDP) (Mironov, 2017) and _privacy loss distributions (PLD)_Meiser and Mohammadi (2018); Sommer et al. (2019); the latter admits numerically accurate algorithms Koskela et al. (2020); Gopi et al. (2021); Ghazi et al. (2022); Doroshenko et al. (2022), with multiple open-source implementations Prediger and Koskela (2020); Google's DP Library (2020); Microsoft (2021).

Note that \((n,b,B)\) can be made arbitrarily small by increasing \(B\), which affects the computation cost. In particular, given a target privacy parameter \((,)\), we can, for example, work backwards to first choose \(B\) such that \((n,\,b,B) T(1+e^{}) 10^{-5}\), and then choose the noise scale \(\) such that \(\{D_{e^{}}(P_{}\|Q_{}),D_{e^{} }(Q_{}\|P_{})\}(1-10^{-5})\), using aforementioned privacy accounting libraries. Notice that our use of Proposition3.2 is likely not the optimal approach to account for the batch truncation. We do not optimize this further because we find that this approach already provides very minimal degradation to the choice of \(\) for a modest value of \(B\) relative to \(b\). A more careful analysis could at best result in a slightly smaller \(B\), which we do not consider as significant; see Figures3 and 4 for more details.

Privacy analysis of \(^{0}}}\).Obtaining the exact privacy guarantee for \(}}\) has been an open problem in the literature. Our starting point is the approach introduced by Chua et al. (2024) to prove a lower bound in the single epoch setting. Let the input space be \(=[-1,1]\), the (non-adaptive) query method \(\) that produces the query \(_{t}(x)=x\), and consider the adjacent datasets:

\[=(x_{1}=-1,,x_{n-1}=-1,x_{n}=1)^{ }=(x_{1}=-1,,x_{n-1}=-1,x_{n}=)\,.\]

Recall that the number of epochs is \(E:=bT/n\) and the number of steps per epoch is \(S:=n/b\). By considering the same setting, it is easy to see that the distributions \(U=^{0}}}()\) and \(V=^{0}}}(^{})\) are given as:

\[U\ =\ _{s=1}^{S}(-b+2f_{s},^{2}I _{T}), V\ =\ _{s=1}^{S}(-b+f_{s},^{2}I_{T}),\]where \(f_{s}^{T}\) is the sum of basis vectors \(_{=0}^{E-1}e_{ S+s}\), and \(\) denotes the all-\(1\)'s vector in \(^{T}\). Basically, \(f_{s}\) is the indicator vector encoding the batches that the differing example gets assigned to; in persistent shuffling, an example gets assigned to the \(s\)th batch within each epoch for a random \(s\{1,,S\}\). Shifting the distributions by \(b\) and projecting to the span of \(\{f_{s}:s[S]\}\) does not change the hockey stick divergence \(D_{e^{}}(U\|V)\), hence we might as well consider the pair

\[U^{}:=\ _{s=1}^{S}(2e_{s}, ^{2}I_{S}) V^{}:=\ _{s=1}^{S}(e_{s}, ^{2}I_{S}).\]

By scaling down the distributions by \(\) on all coordinates we arrive at the following pair:

\[P_{^{}}:=\ _{s=1}^{S}(2e_{s}, }{E}I) Q_{^{}}:=\ _{s=1}^{S}(e_{s},}{E}I).\] (1)

The pair \((P_{^{}},Q_{^{}})\) is essentially same as the pair obtained by Chua et al. (2024), with \(\) replaced by \(/\), and we get the following:

**Proposition 3.4**.: _For all \(>0\), \( 0\) and all valid \(n\), \(b\), \(T\), it holds that_

\[_{^{}}()\{D_{e^{}}(P_{ ^{}}\|Q_{^{}}),D_{e^{}}(Q_{^{}}\|P_{^{}})\}.\]

Following Chua et al. (2024), we can obtain a lower bound as \(_{^{}}() P_{^{}}() -e^{}Q_{^{}}()\) for any \(\), and in particular, we consider events of the form \(_{C}:=\{w^{}:_{s}w_{s}>C\}\) for various values of \(C\). \(P_{^{}}(_{C})\) and \(Q_{^{}}(_{C})\) are efficient to compute as

\[P_{^{}}(_{C})=1-(} )(})^{S-1}  Q_{^{}}(_{C})=1-(})(})^{S-1}.\]

Thus, using Proposition 3.4, we get that

**Theorem 3.5**.: _For all \(>0\), \( 0\), and all valid \(n\), \(b\), \(T\), it holds that_

\[_{^{}}()\ \ _{C}P_{ ^{}}(_{C})-Q_{^{}}(_{C}).\]

Privacy analysis of ABLQ\({}_{^{}}\).Our starting point for providing a lower bound on \(_{^{}}()\) is the pair \((P_{},Q_{})\) as defined below that provides a lower bound in the case of a single epoch.

\[P_{}:=\ _{s=1}^{S}(2e_{s},^{2}I)  Q_{}:=\ _{s=1}^{S}(e_{s},^{2}I).\]

ABLQ\({}_{^{}}\) is an \(E\)-fold composition of the single-epoch mechanism. Hence by composition of dominating pairs, it follows that \(_{^{}}()\{D_{e^{}}(P_{ ^{}}\|Q_{^{}}),D_{e^{}}(Q_{^{}}\|P_{^{}})\}\), where \(P_{^{}}:=\ P_{}^{ E}\) and \(Q_{^{}}:=\ Q_{}^{ E}\). However, it is tricky to directly identify an event \(\) for which the lower bound \(P_{^{}}()-e^{}Q_{^{}}()\) is non-trivial and \(P_{^{}}(),Q_{^{}}()\) are easy to compute. So in order to lower bound \(D_{e^{}}(P_{^{}}\|Q_{^{}})\), below we construct a pair \((_{},_{})\) of discrete distributions such that \((P_{},Q_{})(_{},_{})\) and thus \((P_{^{}},Q_{^{}})(_{ }^{ E},_{}^{ E})\).

For probability measures \(P\) and \(Q\) over a measurable space \(\), and a finite partition3\(=(G_{1},,G_{m})\) of \(\), we can consider the discrete distributions \(P^{}\) and \(Q^{}\) defined over \(\{1,,m\}\) such that \(P^{}(i)=P(G_{i})\) and \(Q^{}(i)=Q(G_{i})\). The post-processing property of DP implies:

**Proposition 3.6** (DP Post-processing (Dwork and Roth, 2014)).: _For all partitions \(\) of \(\), it holds that \((P,Q)(P^{},Q^{})\)._

We construct the pair \((_{},_{})\) by instantiating Proposition 3.6 with the set \(=\{G_{0},G_{1},,G_{m+1}\}\) parameterized by a sequence \(C_{1}<<C_{m}\) of values defined as follows: \(G_{0}:=\{w^{S}:_{s}w_{s} C_{1}\}\), \(G_{i}:=\{w^{S}:C_{i}<_{s}w_{s} C_{i+1}\}\) for \(1 i m\) and \(E_{m+1}:=\{w^{S}:C_{m}<_{s}w_{s}\}\); in other words, \(G_{0}=^{S}_{C_{1}}\), \(G_{i}=_{C_{i}}_{C_{i+1}}\) for \(1 i m\) and \(G_{m+1}=_{C_{m}}\).

**Theorem 3.7**.: _For all \(>0\), \( 0\), all valid \(n\), \(b\), \(T\), and any finite sequence \(C_{1},,C_{m}\) of values used to define \(_{}\), \(_{}\) as above, it holds that_

\[_{^{}}()\ \ \{D_{e^{}}(_{ }^{ E}\|_{}^{ E}),D_{e^{}}( _{}^{ E}\|_{}^{ E})\}.\]We use the dp_accounting library (Google's DP Library, 2020) to numerically compute a lower bound on the quantity above, using PLD. In particular, we choose \(C_{1}\) and \(C_{m}\) such that \(P_{}(G_{0})\) and \(P_{}(G_{m+1})\) are sufficiently small and choose other \(C_{i}\)'s to get a sufficiently fine discretization of the interval between \(C_{1}\) and \(C_{m}\).4

An illustration of these accounting methods is presented in Figure 3, which demonstrates the significant gap where the optimal \(\) for dynamic/persistent shuffling is significantly larger than compared to Poisson subsampling, even when using an optimistic estimate for shuffling as above. We provide the implementation of our privacy accounting methods described above in an iPython notebook5 hosted on Google Colab, executable using the freely available Python CPU runtime.

## 4 Experiments

We compare DP-SGD using the following batch sampling algorithms at corresponding noise scales:

* Deterministic batches (using nearly exact value of \(_{}(,)\) via Theorem 3.1),
* Truncated Poisson subsampled batches (using upper bound on \(_{}(,)\) via Theorem 3.3),
* Persistent shuffled batches (using lower bound on \(_{^{}}(,)\) via Theorem 3.5), and
* Dynamic shuffled batches (using lower bound on \(_{^{}}(,)\) via Theorem 3.7).

As a comparison, we also evaluate DP-SGD with dynamic shuffled batches, but using noise that is an upper bound on \(_{}(,)\) (with no truncation, i.e. \(B=\)), to capture the incorrect, but commonly employed approach in practice. Finally, in order to understand the impact of using different batch sampling to model training in isolation, we compare models trained with SGD under truncated Poisson subsampling, and dynamic and persistent shuffling without any clipping or noise. We use massively parallel computation (Map-Reduce operations (Dean and Ghemawat, 2004)) to generate batches with truncated Poisson subsampling in a scalable manner as visualized in Figure 2; the details, with a beam pipeline implementation, is provided in Appendix A.

Figure 2: Visualization of the massively parallel computation approach for Poisson subsampling at scale. Consider \(6\) records \(x_{1},,x_{6}\) sub-sampled into \(4\) batches with a maximum batch size of \(B=2\). The Map operation adds a “weight” parameter of \(1\) to all examples, and samples indices of batches to which each example will belong. The Reduce operation groups by the batch indices. The final Map operation truncates batches with more than \(B\) examples (e.g., batches \(1\) and \(3\) above), and pads dummy examples with weight \(0\) in batches with fewer than \(B\) examples (e.g., batch \(4\) above).

We run our experiments on the Criteo Display Ads pCTR Dataset [Jean-Baptiste Tien, 2014], which contains around 46 million examples from a week of Criteo ads traffic. Each example has 13 integer features and 26 categorical features, and the objective is to predict the probability of clicking on an ad given these features. We use the labeled training set from the dataset, split chronologically into a 80%/10%/10% partition of train/validation/test sets. We use the binary cross entropy loss and report the AUC on the labeled test split, averaged over three runs with different random seeds. These are plotted with error bars indicating a single standard deviation. We include more details about the model architectures and training in Appendix B.

We run experiments varying the (expected) batch size from \(1\,024\) to \(262\,144\), for both private training with \((=5,=2.7 10^{-8})\) and non-private training and with \(1\) or \(5\) epochs. We plot the results in Figure 3. As mostly expected, we observe that the model utility generally improves with smaller \(\). Truncated Poisson subsampling performs similarly to dynamic shuffling for the same value of \(\), although it performs worse for non-private training. The latter could be attributed to the fact that when using truncated Poisson subsampling, a substantial fraction6 of examples are never seen in the training with high probability. However, this does not appear to significantly affect the private training model utility for the range of parameters that we consider, since we observe that truncated Poisson subsampling behaves similarly to dynamic shuffling with noise scale of \(_{}(,)\) (the values of \(\) for "\(\)" and "\(^{}\) (\(\) accounting)" visually overlap in Figure 3; the values for \(\) are only negligibly larger, since it accounts for truncation). Truncated Poisson subsampling performs better when compared to shuffling when the latter using our lower bound on \(_{^{}}(,)\), which suggests that shuffling with correct accounting (that is, with potentially even larger \(\)) would only perform worse.

We also run experiments with a fixed batch size \(65\,536\) and varying \(\) from \(1\) to \(256\), fixing \(=2.7 10^{-8}\). We plot the results and the corresponding \(\) values in Figure 4. We again observe that shuffling (with our lower bound accounting) performs worse than truncated Poisson subsampling in the high privacy (low \(\)) regime, but performs slightly better in the low privacy (high \(\)) regime (this is because at large \(\) the noise required under Poisson subsampling is in fact larger than that under shuffling). Moreover, we observe that shuffling performs similarly to truncated Poisson subsampling when we use similar value of \(\), consistent with our observations from Figure 3.

Finally, as a comparison, we compute the upper bounds on \(_{}(,)\) via the privacy amplification by shuffling bounds by Feldman et al. . We find that these bounds tend to be vacuous in many regime of parameters, namely they are no better than \(_{}(,)\), which is clearly an upper bound on \(_{}(,)\). Details are provided in Appendix C.

Figure 3: AUC (left) and bounds on \(_{}\) values (middle) for \(=5,=2.7 10^{-8}\) and using \(1\) epoch (top) and \(5\) epochs (bottom) of training on a linear-log scale; AUC (right) is with non-private training.

## 5 Conclusion

We provide new lower bounds on the privacy analysis of Adaptive Batch Linear Query mechanisms, under persistent and dynamic shuffling batch samplers, extending the prior work of Chua et al. (2024) that analyzed the single epoch case. Our lower bound method continues to identify separations in the multi-epoch setting, showing that the amplification guarantees due to even dynamic shuffling can be significantly limited compared to the amplification due to Poisson subsampling in regimes of practical interest.

We also provide evaluation of DP-SGD with various batch samplers with the corresponding privacy accounting, and propose an approach for implementing Poisson subsampling at scale using massively parallel computation. Our findings suggest that with provable privacy guarantees on model training, Poisson-subsampling-based DP-SGD has better privacy-utility trade-off than shuffling-based DP-SGD in many practical parameter regimes of interest, and in fact, essentially match the utility of shuffling-based DP-SGD at the same noise level. Thus, we consider Poisson-subsampling-based DP-SGD as a viable approach for implementing DP-SGD at scale, given the lower bound on the privacy analysis when using shuffling.

Several interesting directions remain to be investigated. Firstly, our technique only provides a _lower bound_ on the privacy guarantee when using persistent / dynamic shuffled batches. While some privacy amplification results are known (Feldman et al., 2021, 2023), providing a tight (non-vacuous) upper bound on the privacy guarantee in these settings remains an open challenge. This can be important in regimes where shuffling does provide better privacy guarantees than Poisson subsampling.

Another important point to note is that persistent and dynamic shuffling are not the only forms of shuffling used in practice. For example, methods such as tf.data.Dataset.shuffle or torchdata.datapipes.iter.Shuffler provide a uniformly random shuffle, only when the size of its "buffer" is larger than the dataset. Otherwise, for buffer size \(b\), it returns a random record among the first \(b\) records, and immediately replaces it with the next record (\((b+1)\)th in this case), and repeats this process, which leads to an asymmetric form of shuffling. Such batch samplers merit more careful privacy analysis.