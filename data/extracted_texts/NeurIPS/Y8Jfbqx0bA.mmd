# On Consistent Bayesian Inference from Synthetic Data

Anonymous Author(s)

Affiliation

Address

email

###### Abstract

Generating synthetic data, with or without differential privacy, has attracted significant attention as a potential solution to the dilemma between making data easily available, and the privacy of data subjects. Several works have shown that consistency of downstream analyses from synthetic data, including accurate uncertainty estimation, requires accounting for the synthetic data generation. There are very few methods of doing so, most of them for frequentist analysis. In this paper, we study how to perform consistent Bayesian inference from synthetic data. We prove that mixing posterior samples obtained separately from multiple large synthetic datasets converges to the posterior of the downstream analysis under standard regularity conditions when the analyst's model is compatible with the data provider's model. We show experimentally that this works in practice, unlocking consistent Bayesian inference from synthetic data while reusing existing downstream analysis methods.

## 1 Introduction

Synthetic data has the potential of opening privacy-sensitive datasets for widespread analysis. The idea is to train a generative model with real data, and release synthetic data that has been generated from the model. The synthetic data does not contain records from real people, and ideally it preserves the population-level properties of the real data, making it useful for analysis. Privacy preservation can be guaranteed with _differential privacy_ (DP) (Dwork et al. 2006b), which offers provable protection of privacy.

The most convenient and straightforward way for downstream analysts to analyse synthetic data is using the same method that would be used with real data. However, ignoring the additional stochasticity arising from the synthetic data generation will yield biased results and overconfident uncertainty estimates (Raghunathan et al. 2003; Raisa et al. 2023; Wilde et al. 2021). This is especially problematic under DP, which requires adding extra noise, which will be ignored if the synthetic data is treated like real data. This problem creates the need for _noise-aware_ analyses that account for the synthetic data generation.

When the downstream analysis is frequentist, it is possible to account for the synthetic data generation when multiple synthetic datasets are generated and analysed (Raghunathan et al. 2003). Recent work has extended this to DP synthetic data (Raisa et al. 2023), which allows generating multiple synthetic datasets without compromising on privacy. These methods reuse the analysis method for the real data, and only require using simple combining rules to combine the results from the analyses on each synthetic dataset, making them simple to apply.

For Bayesian downstream analyses, Wilde et al. (2021) have shown that the analyst can use additional samples of public real data to correct their analysis. However, their method requires targeting a generalised notion of the posterior (Bissiri et al. 2016) and needs the additional public data for calibration. Ghalebikesabi et al. (2022) propose a correction using importance sampling to avoid theneed of public data, but only prove convergence to a generalised posterior and do not clearly address the noise-awareness of the method.

In the context of missing data, Gelman et al. (2014) have proposed inferring the downstream posterior of a Bayesian analysis by imputing multiple completed datasets, inferring the analysis posterior for each completed dataset separately, and mixing the posteriors together. We study the applicability of this method to synthetic data, aiming the bring the simplicity of the frequentist methods using multiple synthetic datasets to Bayesian downstream analysis.

#### Contributions

1. We study inferring the downstream analysis posterior by generating multiple synthetic datasets, inferring the analysis posterior for each synthetic dataset as if it were the real dataset, and mixing the posteriors together. We find that in this setting, the synthetic datasets also need to be larger than the original dataset.
2. We prove that when the Bernstein-von Mises, or a similar theorem, applies, this method converges to the true posterior as the number of synthetic datasets and the size of the synthetic datasets grow. Under stronger assumptions, we prove a convergence rate for this method in the synthetic dataset size, which we expect to match the rate that usually applies in the Bernstein-von Mises theorem (Hipp and Michel, 1976). These are presented in Section 3.
3. We evaluate this method with two examples in Section 4: non-private univariate Gaussian mean estimation, and differentially private Bayesian logistic regression. In the first example, we use the tractability of the model to derive further theoretical properties of the method, and in both examples, we verify that the method works in practice through experiments.

### Related Work

Generating synthetic data to preserve privacy was, as far as we know, originally proposed by Liew et al. (1985). Rubin (1993) proposed accounting for the synthetic data generation in frequentist downstream analyses by adapting _multiple imputation_(Rubin, 1987), which involves generating multiple synthetic datasets, analysing each of them, and combining the results with so called Rubin's rules (Raghunathan et al., 2003; Reiter, 2002). Recently, Raisa et al. (2023) have shown that multiple imputation also works when the synthetic data is generated under DP when the data generation algorithm is _noise-aware_ in a certain sense.

Wilde et al. (2021) study downstream Bayesian inference from DP synthetic data by considering the analyst's model to be misspecified, and targeting a generalised notion of the posterior (Bissiri et al., 2016) to deal with the misspecification, which makes method their more difficult to apply than standard Bayesian inference. They also assume that the analyst has additional public data available to calibrate their method.

Ghalebikesabi et al. (2022) use importance sampling to correct for bias with DP synthetic data, and have Bayesian inference as an example application. However, they also target a generalised variant (Bissiri et al., 2016) of the posterior instead of the noise-aware posterior we target, and they do not evaluate uncertainty estimation, so the noise-awareness of their method is not clear.

We are not aware of any existing work adapting multiple imputation for Bayesian downstream analysis in the synthetic data setting. In the missing data setting without DP, where multiple imputation was originally developed (Rubin, 1987), Gelman et al. (2014) have proposed sampling the downstream posterior by mixing samples of the downstream posteriors from each of the multiple synthetic datasets. We find that this is not sufficient in the synthetic data setting, and add one extra component: our synthetic datasets are larger than the original dataset. We compare the two cases in more detail in Supplemental Section F, and in particular explain why large synthetic datasets are not needed in the missing data setting.

Noise-aware DP Bayesian inference is critical for taking into account the DP noise in synthetic data, but only a few works address this even without synthetic data. Bernstein and Sheldon (2018) present an inference method for simple exponential family models. Their approach was extended to linear models (Bernstein and Sheldon, 2019) and generalised linear models (Kulkarni et al., 2021). Recently, Ju et al. (2022) developed an MCMC sampler that can sample the noise-aware posterior using a noisy summary statistic.

Background on Bayesian Inference

Bayesian inference is a paradigm of statistical inference where the data analyst's uncertainty in a quantity \(Q\) after observing data \(X\) is represented using the posterior distribution \(p(Q|X)\)(Gelman et al., 2014). The posterior is given by Bayes' rule:

\[p(Q|X)=)p(Q^{})\,Q^{}},\] (1)

where \(p(X|Q)\) is the likelihood of observing the data \(X\) for a given value of \(Q\), and \(p(Q)\) is the analyst's prior of \(Q\). Computing the denominator is typically intractable, so analysts often use numerical methods to sample \(p(Q|X)\)(Gelman et al., 2014).

Bernstein-von Mises TheoremIt turns out that in many typical settings, the prior's influence on the posterior vanishes when the dataset \(X\) is large. A basic example of this is the Bernstein-von Mises theorem (van der Vaart, 1998), which informally states that under some regularity conditions, the posterior approaches a Gaussian that does not depend on the prior as the size of the dataset increases.

A crucial component of the theorem, and also our theory, is the notion of _total variation distance_ between random variables, which is used to measure the difference between two random variables or probability distributions.

**Definition 2.1**.: _The total variation distance between random variables (or distributions) \(P_{1}\) and \(P_{2}\) is_

\[(P_{1},P_{2})=_{A}|(P_{1} A)-(P_{2} A)|,\] (2)

_where \(A\) is any measurable set._

As a slight abuse of notation, we allow the arguments of \((,)\) to be random variables, probability distributions, or probability density functions interchangeably. We list some properties of total variation distance that we use in Lemma A.1 in the Supplement.

Now we can state the theorem.

**Theorem 2.2** (Bernstein-von Mises (van der Vaart, 1998)).: _Let \(n\) denote the size of the dataset \(X_{n}\). Under regularity conditions stated in Condition A.4 in Supplemental Section A.2, for true parameter value \(Q_{0}\), the posterior \((X_{n}) p(Q|X_{n})\) satisfies_

\[(((X_{n})-Q_{0}),((X_{n}),) )0\] (3)

_as \(n\) for some \((X_{n})\) and \(\), that do not depend on the prior, where the convergence in probability is over sampling \(X_{n} p(X_{n}|Q_{0})\)._

## 3 Bayesian Inference from Synthetic Data

When the downstream analysis is Bayesian, and the analyst has access to non-DP synthetic data, they would ultimately want to obtain the posterior \(p(Q|X,I_{A})\) of some quantity \(Q\) given real data \(X\), where \(I_{A}\) denotes the background knowledge such as priors of the analyst. In the DP case, the exact posterior is unobtainable, so we assume that \(X\) is only available through a noisy summary \(\)(Ju et al., 2022; Raisai et al., 2023), so the posterior is \(p(Q|,I_{A})\). To unify these notations, we use \(Z\) to denote the observed values, so \(Z=X\) in the non-DP case, \(Z=\) in the DP case, and the posterior of interest is \(p(Q|Z,I_{A})\). We summarise these random variables and their dependencies in Figure 1, and give an introduction to DP in Supplemental Section A.3.

In order to introduce the synthetic data into the posterior of interest, we can decompose the posterior as

\[p(Q|Z,I_{A})= p(Q|Z,X^{*},I_{A})p(X^{*}|Z,I_{A})\,X^{*},\] (4)

where we abuse notation by using \(X^{*}\) as the variable to integrate over, so inside the integral \(X^{*}\) is not a random variable. The decomposition in (4) means that we could sample \(p(Q|Z,I_{A})\) by first sampling the synthetic data from the posterior predictive \(X^{Syn} p(X^{*}|Z,I_{A})\), and then sampling \(Q p(|Z,X^{*}=X^{Syn},I_{A})\).

Note that the random variable \(X^{*}\) represents a hypothetical real dataset that could be obtained if more data was collected, as seen in Figure 1, and it is not the synthetic dataset. The synthetic dataset \(X^{Syn}\) is a sample from the conditional distribution of \(X^{*}\) given \(Z\). For this reason, \(p(Q|Z,X^{*},I_{A}) p(Q|Z,I_{A})\). To make our notation less cluttered, we write \(p(\,|X^{*},\,\,)\) in place of \(p(\,|X^{*}=X^{Syn},\,\,)\) in probabilities when the meaning is clear.

There are still two major issues with the decomposition in (4):

1. Sampling \(p(Q|Z,X^{*},I_{A})\) requires access to \(Z\), which defeats the purpose of using synthetic data.
2. \(X^{*}\) needs to be sampled conditionally on the analyst's background information \(I_{A}\), while the synthetic data provider could have different background information \(I_{S}\).

To solve the first issue, in Section 3.2 we show that if we replace \(p(Q|Z,X^{*},I_{A})\) inside the integral of (4) with \(p(Q|X^{*},I_{A})\), the resulting distribution converges to the desired posterior,

\[ p(Q|X^{*},I_{A})p(X^{*}|Z,I_{A})\,X^{*} p(Q|Z,I_{A})\] (5)

in total variation distance as the size of each synthetic data set \(X^{*}\) grows. It should be noted that many such synthetic data sets will be needed to account for the integral over \(X^{*}\).

The second issue is known as _congeniality_ in the multiple imputation literature (Meng 1994; Xie and Meng 2016). We look at congeniality in the context of Bayesian inference from synthetic data in Section 3.1, and find that we can obtain \(p(Q|Z,I_{A})\) under appropriate assumptions on the relationship between \(I_{A}\) and \(I_{S}\).

Exactly sampling the LHS of (5) requires generating a synthetic dataset for each sample of \(p(Q|Z,I_{A})\), which is not practical. However, we can perform a Monte-Carlo approximation for \(p(Q|Z,I_{A})\) by generating \(m\) synthetic datasets \(X_{1}^{Syn},,X_{m}^{Syn} p(X^{*}|Z,I_{A})\), drawing multiple samples from each of the \(p(Q|X^{*}=X_{i}^{Syn},I_{A})\), and mixing these samples, which allows us to obtain more than one sample of \(p(Q|Z,I_{A})\) per synthetic dataset. We look at some properties of this in Supplemental Section E, but we use the integral form in (5) in the rest of our theory.

Figure 1: Left: random variables in noise-aware uncertainty estimation from synthetic data. Right: a Bayesian network describing the dependencies of the random variables.

### Congeniality

In the decomposition (4) of the analyst's posterior, \(X^{*}\) should be sampled conditionally on the analyst's background information \(I_{A}\), while in reality the synthetic data provider could have different background information \(I_{S}\).

A similar distinction has been studied in the context of missing data (Meng 1994; Xie and Meng 2016), where the imputer of missing data has a similar role as the synthetic data generator. Meng (1994) found that Rubin's rules implicitly assume that the probability models of both parties are compatible in a certain sense, which Meng (1994) defined as _congeniality_.

As our examples with Gaussian distributions in Section 4.1 and Supplemental Section C.2 show, some notion of congeniality is also required in our setting. However, because we study synthetic data instead of imputation, and Bayesian instead of frequentist downstream analysis, we need a different formal definition. As the analyst only makes inferences on \(Q\), it suffices that both the analyst and synthetic data generator make the same inferences of \(Q\):

**Definition 3.1**.: _The background information sets \(I_{S}\) and \(I_{A}\) are congenial for observation \(Z\) if_

\[p(Q|X^{*},I_{S})=p(Q|X^{*},I_{A})\] (6)

_for all \(X^{*}\) and_

\[p(Q|Z,I_{S})=p(Q|Z,I_{A}).\] (7)

In the non-DP case, (7) is redundant, as it is implied by (6), but in the DP case, both are needed, as the parties may draw different conclusions on \(X\) given \(Z=\).

Combining congeniality and (5),

\[ p(Q|X^{*},I_{A})p(X^{*}|Z,I_{S})\,X^{*} = p(Q|X^{*},I_{S})p(X^{*}|Z,I_{S})\,X^{*}\] (8) \[ p(Q|Z,I_{S})=p(Q|Z,I_{A}),\]

where the convergence is in total variation distance as the size of \(X^{*}\) grows. In the following, we assume congeniality, and drop \(I_{A}\) and \(I_{S}\) from our notation.

### Consistency Proof

To recap, we want to prove that the posterior from synthetic data,

\[_{n}(Q)= p(Q|X^{*}_{n})p(X^{*}_{n}|Z)\,X^{*}_{n},\] (9)

converges in total variation distance to \(p(Q|Z)\) as the size \(n\) of \(X^{*}_{n}\) grows. We prove this in Theorem 3.4, which requires that both \(p(Q|Z,X^{*}_{n})\) and \(p(Q|X^{*}_{n})\) approach the same distribution as \(n\) grows. We formally state this in Condition 3.2. In Lemma 3.3, we show that Condition 3.2 is a consequence of the Bernstein-von Mises theorem (Theorem 2.2) under some additional assumptions, so we expect it to hold in typical settings.

To make the notation more compact, let \(^{+}_{n} p(Q|Z,X^{*}_{n})\), and let \(_{n} p(Q|X^{*}_{n})\).

**Condition 3.2**.: _For all \(Q\) there exist distributions \(D_{n}\) such that_

\[(^{+}_{n},D_{n})0(_{n},D_{n})0\] (10)

_as \(n\), where the convergence in probability is over sampling \(X^{*}_{n} p(X^{*}_{n}|Z,Q)\)._

Theorem 2.2 implies Condition 3.2 with some additional assumptions:

**Lemma 3.3**.: _If the assumptions of Theorem 2.2 (Condition A.4) and the following assumptions:_

1. \(Z\) _and_ \(X^{*}\) _are conditionally independent given_ \(Q\)_; and_
2. \(p(Z|Q)>0\) _for all_ \(Q\)_,_

_hold for the downstream analysis for all \(Q_{0}\), then Condition 3.2 holds._Proof.: The full proof is in Supplemental Section B.1. Proof idea: when \(Z\) and \(X^{*}\) are conditionally independent given \(Q\),

\[p(Q|Z,X^{*}) p(X^{*}|Q)p(Z|Q)p(Q)\] (11)

so \(p(Q|Z,X^{*})\) can be equivalently seen as the result of Bayesian inference with observed data \(X^{*}\) and prior \(p(Q|Z)\). As the only difference to \(p(Q|X^{*})\) is the prior, the Bernstein-von Mises theorem implies that both \(p(Q|Z,X^{*})\) and \(p(Q|X^{*})\) converge in total variation distance to the same distribution. 

Assumption (1) of Lemma 3.3 will hold if the downstream analysis treats its input data as an i.i.d. sample from some distribution. Assumption (2) holds when the likelihood is always positive, and in the DP case when the density of the privacy mechanism is positive everywhere, which is the case for common DP mechanisms like the Gaussian and Laplace mechanisms (Dwork and Roth 2014).

Next is the main theorem of this work: (5) holds under Condition 3.2.

**Theorem 3.4**.: _Under congeniality and Condition 3.2, \((p(Q|Z),_{n}(Q)) 0\) as \(n\)._

Proof.: The full proof is in Supplemental Section B.1. Proof idea: the proof consists of three steps. The first two are in Lemma B.1 and the third is in Lemma B.2 in the Supplement. The first step is showing that \((_{n},_{n}^{+})0\) when \(X_{n}^{*} p(X_{n}^{*}|Z,Q)\) for fixed \(Z\) and \(Q\). This is a simple consequence of the triangle inequality and Condition 3.2, as total variation distance is a metric. In the second step, we show that \((_{n},_{n}^{+})0\) also holds when \(X_{n}^{*} p(X_{n}^{*}|Z)\). In the final step, we show that this implies the claim. 

### Convergence Rate

Under stronger regularity conditions, we can get a convergence rate for Theorem 3.4. The regularity conditions depend on uniform integrability:

**Definition 3.5**.: _A sequence of random variables \(X_{n}\) is uniformly integrable if_

\[_{M}_{n}(|X_{n}|_{|X_{n}|>M})=0\] (12)

Now we can state the regularity conditions for a convergence rate \(O(R_{n})\):

**Condition 3.6**.: _There exist distributions \(D_{n}\) such that for a sequence \(R_{1},R_{2},>0\), \(R_{n} 0\) as \(n\),_

\[}(_{n}^{+},D_{n}) }(_{n},D_{ n})\] (13)

_are uniformly integrable when \(X_{n}^{*} p(X_{n}^{*}|Z)\)._

Note that \(X_{n}^{*} p(X_{n}^{*}|Z)\) conditions on \(Z\), not \(Q\) and \(Z\) like in Condition 3.2. We prove that Condition 3.6 is met in univariate Gaussian mean estimation for \(R_{n}=}\) in Theorem D.1 in the Supplement. This is the same rate that commonly applies in the Bernstein-von Mises theorem (Hipp and Michel 1976).

Condition 3.6 implies an \(O(R_{n})\) convergence rate:

**Theorem 3.7**.: _Under congeniality and Condition 3.6, \((p(Q|Z),_{n}(Q))=O(R_{n})\)._

Proof.: The full proof is in Supplemental Section B.2. Proof idea: first, we prove the uniform integrability of \(}(_{n},_{n}^{+})\) when \(X_{n}^{*} p(X_{n}^{*}|Z)\) by using the triangle inequality and properties of uniform integrability. Second, we prove that this implies the claimed convergence rate. 

## 4 Examples

In this section, we present two examples of downstream inference from synthetic data at a high level. First, we demonstrate univariate Gaussian mean estimation. Second, we have logistic regression on a toy dataset, with DP synthetic data. In the first example, we use the tractability of the model to derive additional theoretical properties, and in both examples, we experimentally verify our theory.

Supplemental Section C, contains more detailed descriptions of the examples, and some additional results. Supplemental Section D proves an \(O(})\) convergence rate for Theorem 3.4 in the Gaussian mean estimation case. Our code is in the supplementary material.

### Non-private Gaussian Mean Estimation

Our first example is very simple: the analyst infers the mean \(\) of a Gaussian distribution with known variance from synthetic data that has been generated from the same model. The posteriors for this setting can be found in Supplemental Section A.4. To differentiate the variables for the analyst and data provider, we use bars for the data provider (like \(_{0}^{2}\)) and hats for the analyst (like \(_{0}^{2}\)).

When the synthetic data is generated from the known variance model with known variance \(_{k}^{2}\), we sample from the posterior predictive \(p(X^{*}|X)\) as

\[|X (_{n_{X}},_{n_{X}}^{2}), X ^{*}|^{n_{X^{*}}}(,_{k}^{2})\] (14) \[_{n_{X}} =_{0}^{2}}_{0}+}{ _{k}^{2}}}{_{0}^{2}}+}{ _{k}^{2}}},_{n_{X}}^{2}}=_{0}^{2}}+}{_{k}^{2}}.\] (15)

\(^{n_{X^{*}}}\) denotes a Gaussian distribution over \(n_{X^{*}}\) i.i.d. samples.

Figure 3: Convergence of the mixture of posteriors from synthetic data with different sizes of the synthetic dataset on Gaussian mean estimation with known variance. \(n_{X^{*}}=n_{X}\) is clearly not enough, but \(n_{X^{*}}=20n_{X}\) is already relatively good.

Figure 2: Simulation results for the Gaussian mean estimation example, showing that the mixture of posteriors from synthetic data in green converges. In the left panel, both the analyst and data provider have the correct known variance. The blue and orange lines overlap, as both parties have the same \(p(|X)\). On the right, the analyst’s known variance is too small (\(_{k}^{2}=_{k}^{2}\)), so congeniality is not met, but the mixture of posteriors from synthetic data, \(_{n}()\), still converges to the data provider’s posterior. In both panels, \(m=400\) and \(}}{n_{X}}=20\).

When downstream analysis is the model with known variance \(_{k}^{2}\), we have

\[|X^{*}(_{n_{X^{*}}},_{n_{X^{*}}}^{2} ),_{n_{X^{*}}}=_{0}^{2}}_{0}+ }}{_{k}^{2}}^{*}}{_{0}^ {2}}+}}{_{k}^{2}}},_{n_{X^ {*}}}^{2}}=_{0}^{2}}+}}{_{k}^{2 }}.\] (16)

Now, using \(^{*}\) to denote a sample from the mixture of posteriors from synthetic data \(_{n}()\) in (9), we show in Supplemental Section C.1 that

\[(^{*})_{n_{X}},(^{*}) _{n_{X}}^{2}\] (17)

as \(n_{X^{*}}\), so \(^{*}\) asymptotically has the same mean and variance as the downstream posterior distribution \(p(|X)\) on the real data.

We test the theory with a numerical simulation in Figure 2. We generated the real data \(X\) of size \(n_{X}=100\) by i.i.d. sampling from \((1,4)\). Both the analyst and data provider use \((0,10^{2})\) as the prior. The data provider uses the correct known variance (\(_{k}^{2}=4\)), and the analyst either uses the correct known variance (\(_{k}^{2}=4\)), or a too small known variance (\(_{k}^{2}=1\)), which is an example of uncongeniality.

In the congenial case in the left panel of Figure 2, both parties have the same posterior given the real data \(X\), and the mixture of posteriors from synthetic data is very close to that. In the uncongenial case in the right panel, where the analyst underestimates the variance, the parties have different posteriors given \(X\), but the mixture of synthetic data posteriors is still close to the data provider's posterior.

In Figure 3, we examine the convergence of the mixture of posteriors from synthetic data under congeniality. We see that setting \(n_{X^{*}}=n_{X}\) is not enough, as the mixture of posteriors is significantly wider than the analyst's posterior. The synthetic dataset needs to be larger than the original, with \(n_{X^{*}}=5n_{X}\) already giving a decent approximation and \(n_{X^{*}}=20n_{X}\) a rather good one. In Figure S1 in the Supplement, we also examine the effect of \(m\) on the mixture of synthetic data posteriors, and see that \(m\) must also be sufficiently large, otherwise the method produces very jagged posteriors.

### Differentially Private Logistic Regression

Our second example is logistic regression on a simple 3-d binary toy dataset, (\(n_{X}=2000\)), with DP synthetic data, under the same setting as used by Raisai et al. (2023) for frequentist logistic regression. We change the downstream task to Bayesian logistic regression to evaluate our theory.

Under DP, \(Z\) is a noisy summary \(\) of the real data. We need synthetic data sampled from the posterior predictive \(p(X^{*}|)\), which is exactly what the NAPSU-MQ algorithm of Raisai et al. (2023) provides. In NAPSU-MQ, \(\) is the values of user-selected marginal queries with added Gaussian noise. We used the open-source implementation of NAPSU-MQ1 by Raisai et al. (2023), and describe NAPSU-MQ in Supplemental Section A.3.

Figure 4: Posteriors in the DP logistic regression experiment, where \(Q\) are the regression coefficients. The mixture of posteriors from synthetic data, \(_{n}(Q)\), (with \(n_{X^{*}}/n_{X}=20\), \(m=400\)) is very close the to the private posterior \(p(Q|)\) computed using (4). Computing the posterior without synthetic data with DP-GLM gives a somewhat wider posterior. The true parameter values are highlighted by the grey dashed lines and shown in the panel titles. The privacy bounds are \(=1\), \(=n_{X}^{-2}=2.5 10^{-7}\).

Because of the simplicity of this model, it is possible to use the exact posterior decomposition (4) as a baseline, by using \(p(X|)\) instead of \(p(X^{*}|)\) to generate synthetic data. We give a detailed description of this process in Supplemental Section C.5. We have also included the DP-GLM algorithm (Kulkarni et al., 2021) that does not use synthetic data, and the non-DP posterior from the real data as baselines. We obtained the code for DP-GLM from Kulkarni et al. (2021) upon request.

Figure 4 compares the mixture of posteriors from synthetic data from (9) that uses \(p(Q|X^{*})\), with \(n_{X^{*}}/n_{X}=20\) and \(m=400\) synthetic datasets, to the baselines. The posterior from (9) is very close to the posterior from (4). The DP-GLM posterior that does not use synthetic data is somewhat wider. The privacy bounds are \(=1\), \(=n_{X}^{-2}=2.5 10^{-7}\).

We ran the experiment 100 times and also with \(=0.1\) and \(=0.5\), and plot coverages and widths of credible intervals in Figure S4 in the Supplement. With \(=1\) and \(=0.5\), the coverages are accurate and DP-GLM consistently produces wider intervals. With \(=0.1\), the mixture of synthetic data posteriors likely needs more and larger synthetic datasets to converge, as it produced wider and slightly overconfident intervals for one coefficient.

## 5 Discussion

Synthetic data are often considered as a substitute for real data that are sensitive. Since the data generation process is based on having access to the \(Z\), one might ask why is the synthetic data needed in first place. Why cannot we simply perform the downstream posterior analysis directly using \(Z\)? Our analysis allows \(Z\) to be an arbitrary, even noisy, representation of the data, and it might be difficult for the analyst to place a model for such generative process for \(Q\). In most applications, the analyst does have a model for \(Q\) arising from the data. Therefore using the synthetic data as a proxy for the \(Z\) allows the analyst to use existing models and inference methods to perform the analysis.

LimitationsA clear limitation of mixing posteriors from multiple synthetic datasets is the computational cost of analysing many large synthetic datasets, which may be substantial for more complex Bayesian downstream models, where even a single analysis can be computationally expensive. However, the separate analyses can be run in parallel. We also expect the downstream posteriors of different synthetic datasets to be similar to each other, so it should be possible to use the information gained from sampling a few of them to speed up sampling the others.

Under DP, we need noise-aware synthetic data generation, which limits the settings in which the method can currently be applied. However, if new noise-aware methods are developed in the future, the method can immediately be used with them.

To recover the analyst's posterior, the method requires congeniality, which basically requires the analyst's prior to be compatible with the data provider's. However, the method was still able to recover the data provider's posterior in the Gaussian example, suggesting that the data provider's prior information overrides the analyst's prior information. This suggests an interesting area of future research: analysis methods that override the data provider's prior. An importance sampling approach similar to that of Ghalebikesabi et al. (2022) could provide one approach. This observation also raises interesting questions on whether truly general and objective synthetic data generation is possible.

ConclusionWe considered the problem of consistent Bayesian inference of downstream analyses using multiple, potentially DP, synthetic datasets, and studied an inference method that mixes the posteriors from multiple large synthetic datasets. We proved, under general and well-understood regularity conditions of the Bernstein-von Mises theorem, that the method is asymptotically exact as the sizes of the synthetic datasets grow. We also derived a convergence rate under stricter regularity conditions. We studied the method in two examples: non-private Gaussian mean estimation and DP logistic regression. In the former, we were able to use the analytically tractable structure of the setting to derive additional properties of the method, including a convergence rate without additional assumptions. In both settings, we experimentally validated our theory, and showed that the method works in practice. This fills a major gap in the synthetic data analysis literature, unlocking consistent Bayesian inference while reusing existing downstream analysis methods.