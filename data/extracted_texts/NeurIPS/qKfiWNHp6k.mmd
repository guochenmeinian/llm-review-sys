# Recognize Any Regions

Haosen Yang\({}^{1}\) Chuofan Ma\({}^{2}\) Bin Wen\({}^{3}\) Yi Jiang\({}^{3}\) Zehuan Yuan\({}^{3}\) Xiatian Zhu\({}^{1}\)

\({}^{1}\)University of Surrey \({}^{2}\)The University of Hong Kong \({}^{3}\)ByteDance

This work was performed when Haosen Yang worked as an intern at ByteDance.Corresponding authors

###### Abstract

Understanding the semantics of individual regions or patches of unconstrained images, such as open-world object detection, remains a critical yet challenging task in computer vision. Building on the success of powerful image-level vision-language (ViL) foundation models like CLIP, recent efforts have sought to harness their capabilities by either training a contrastive model from scratch with an extensive collection of region-label pairs or aligning the outputs of a detection model with image-level representations of region proposals. Despite notable progress, these approaches are plagued by computationally intensive training requirements, susceptibility to data noise, and deficiency in contextual information. To address these limitations, we explore the synergistic potential of off-the-shelf foundation models, leveraging their respective strengths in localization and semantics. We introduce a novel, generic, and efficient architecture, named RegionSpot, designed to integrate position-aware localization knowledge from a localization foundation model (e.g., SAM) with semantic information from a ViL model (e.g., CLIP). To fully exploit pretrained knowledge while minimizing training overhead, we keep both foundation models frozen, focusing optimization efforts solely on a lightweight attention-based knowledge integration module. Extensive experiments in open-world object recognition show that our RegionSpot achieves significant performance gain over prior alternatives, along with substantial computational savings (e.g., training our model with 3 million data in a single day using 8 V100 GPUs). RegionSpot outperforms GLIP-L by 2.9 in mAP on LVIS val set, with an even larger margin of 13.1 AP for more challenging and rare categories, and a 2.5 AP increase on ODinW. Furthermore, it exceeds GroundingDINO-L by 11.0 AP for rare categories on the LVIS minival set.

## 1 Introduction

Remarkable progress has been achieved in the realm of purpose-generic image-level Vision-Language (ViL) representation learning, as exemplified by foundation models like CLIP  and ALIGN . These advancements have led to significant performance improvements across a diverse spectrum of vision and multi-modal downstream tasks . The efficacy of these approaches can be largely attributed to their utilization of extensive datasets, typically encompassing millions, if not billions, of training samples replete with rich information. In the pursuit of a more nuanced approach to visual analysis, researchers have also ventured into the realm of universal region-level (e.g., objects) comprehension. This is evident in recent research endeavors . A common approach to this involves learning the semantics of image regions by applying an image-level pre-trained model (e.g., CLIP) to cropped regions, followed by representational distillation using the output of a detection model , as depicted in Figure 1(a). However, utilizing individual croppedregions in this design leads to the loss of crucial contextual information, which can hinder recognition performance.  introduced an open-world detector with a fixed ViL model, bypassing knowledge distillation. However, the use of ROIAlign  for region feature extraction poses limitations. Furthermore, directly applying an image-level model to isolated local regions is less effective, as the model was pretrained on entire images encompassing both object regions and surrounding context. An alternative, albeit brute-force, approach revolves around constructing region-level representations from scratch, harnessing an extensive dataset that pairs regions with labels  (Figure 1(b)). Nevertheless, this approach grapples with challenges such as the proliferation of noisy pseudo-labels and significant training costs. Furthermore, significant advancements have materialized in the realm of class-agnostic visual localization techniques, as illustrated by the notable work of SAM . This approach is characterized by its distinctive feature--an integration of position-aware localization knowledge, which we consider a valuable complement to the inherent capabilities of ViL models. Expanding upon this conceptual framework, our research introduces an innovative architectural paradigm at the region level, herein referred to as RegionSpot. This framework seamlessly incorporates large pre-trained ViL and localization models within an efficient training regimen, obviating the necessity for an extensive repository of region-label pairings. Our methodology centers on the acquisition of the correlation between localization data extracted from 'local' regions by the localization model and the semantic representations encompassing the entirety of the image, derived from the ViL model. This strategic approach permits us to circumvent the conventional fine-tuning of both pre-trained models--wherein they remain 'frozen' during training--thereby safeguarding the integrity of their rich knowledge and ensuring its maximal utilization, all while mitigating the potential for performance degradation. To enact this cross-model correlation, we employ the cross-attention mechanism . In this configuration, the localization feature assumes the role of the 'query', whereas the ViL feature assumes dual roles as both the 'key' and 'value'. This implementation effectively facilitates the fusion of semantic and localization information in a manner that is amenable to learning and yields substantive efficacy.

Our **contributions** are as follows: (1) We introduce the concept of integrating off-the-shelf foundation models to tackle region-level visual understanding. (2) To achieve this objective, we introduce a novel architectural paradigm called RegionSpot, which does not necessitate training from scratch. This approach excels in both optimization efficiency and data utilization. By circumventing the fine-tuning of both localization and Vision-Language (ViL) components, our architecture retains its openness and adaptability, welcoming the seamless integration of advancements in both domains. Extensive experimentation in the context of open-world object understanding confirms the superior performance of our method, even with a substantially smaller number of learnable parameters. Remarkably, RegionSpot surpasses the state-of-the-art GLIP-L by 2.9 in mAP, with an even more substantial advantage of 13.1 AP observed for the more intricate rare categories.

Figure 1: **Illustration of typical region-level visual understanding architecture**. (a) Learning the region recognition model by distilling image-level ViL representations from cropped regions and incorporating them into a detection model (_e.g._, ). (b) Fully fine-tuning both vision and text models with a substantial dataset of region-label pairs. (c) Our proposed approach integrates pretrained (frozen) localization and ViL models, emphasizing the learning of their representational correlation.

Related Work

Zero-shot in image recognitionZero-shot image recognition is the task of recognizing categories that have not been seen during training. In  and , the authors utilized visual attributes to facilitate knowledge transfer to unfamiliar categories. Researchers have also investigated the utilization of class hierarchies, similarities, and object parts to enhance knowledge transfer, as demonstrated in the works of [27; 1; 42; 35]. Recent research has focused on aligning latent image-text embeddings for classifying and describing visual content.  pioneered the establishment of a visual semantic space through deep learning. Subsequently, CLIP  and ALIGN  attained impressive results via contrastive learning with extensive collections of image-text pairs, showcasing exceptional performance across diverse benchmarks. In contrast to previous endeavors that primarily addressed image-level recognition, we focus on fine-grained recognition of visual elements at the regional level.

Zero-shot in region understandingIn zero-shot object recognition, the aim is to enable object detectors to identify categories not encountered during training, such as [26; 31; 36]. Researchers have explored various methods to bridge the gap between known and unknown categories using pre-trained semantic or textual features [30; 25; 3], knowledge graphs [28; 34], and more. Inspired by the zero-shot capabilities of Vision-and-Language (ViL) like CLIP , several approaches have sought to integrate pretrained Vision-and-Language (ViL) models. For example, [39; 7; 4] proposed a method to distill learned image embeddings from CLIP for target detection by focusing on cropped proposal regions. Another approach, RegionCLIP  employs a multistage training strategy. It starts by generating pseudo-labels from captioning data and then proceeds with region-word contrastive pretraining before transferring the knowledge to the detection task.  took a novel approach by formulating object detection as a grounding problem and incorporating additional grounding data to enhance semantic alignment at both phrase and region levels. Their results demonstrated improved performance, even on fully-supervised detection benchmarks.  leveraged large-scale image captioning datasets and expanded their knowledge database using generated pseudo-labels, bolstering their detection capabilities. The use of generated pseudo-labels effectively extended the detectors' generalization ability.

However, these methods face computational challenges and are susceptible to training data inconsistencies and image-level distractions. Differing from these studies, we explore the synergistic benefits of foundation models SAM  and CLIP . Leveraging their strengths in localization and semantics, we propose an innovative region recognition framework.

## 3 Method

Our objective is to employ efficiently a pretrained ViL model and a localization model, trained on extensive data, to achieve region-level representation and understanding. These representations facilitate robust object conceptualization, especially for open-world region recognition. To realize this, as shown in Figure 2(a) we formulate a new approach, named RegionSpot. In the following sections, we will begin with a brief introduction to the foundational models in Section 3.1, followed by a comprehensive explanation of our approach with focus on learning region-text alignment across two pretrained models in Section 3.2.

### Foundation Models

**Vision-language foundation models** use contrastive learning to map visual and textual data into a shared embedding space through a contrastive loss. This technique, exemplified by CLIP with 400 million text-image pairs , and ALIGN with 1.8 billion pairs , aims to minimize the distances between paired images and texts while maximizing distances between unpaired ones.

**Localization foundation models** have been advanced significantly. A prominent example is the pioneering SAM model , which has been trained on the extensive SA-1B dataset, boasting more than 1 billion automatically generated masks--an unprecedented scale, surpassing existing segmentation datasets by a factor of 400. This dataset also comprises 11 million images.

SAM comprises three core modules: (a) Image encoder: Utilizing a ViT-based backbone, this module extracts image features, yielding image embeddings. (b) Prompt encoder: It encodes positional information from input points, boxes, or masks to facilitate the mask decoder. (c) Mask decoder:This transformer-based decoder leverages both the extracted image embeddings and prompt tokens to make final mask predictions. One of SAM's remarkable features is its robust zero-shot generalization to novel data, obviating the need for domain-specific fine-tuning. Thanks to extensive training on a vast repository of prompt-text pairs, SAM demonstrates exceptional proficiency in object localization.

### Region text alignment with frozen foundation models

In this section, we describe how we extract position-aware tokens from the localization foundation model and generate image-level semantic features using the ViL foundation model. We achieve inter-model association through a cross-attention mechanism that facilitates region text alignment.

Region-level position-aware tokensIn our approach, we utilize manually-annotated object bounding boxes, denoted as \(R=\{r_{i}\},i=1,..,N\), as regions of interest in the images. For each of these regions, represented as \(R\), we extract position-aware tokens using the SAM model, denoted as \(P=\{p_{i}\},i=1,..,N\).

As depicted in Figure 2, SAM employs a mask decoder to generate a mask based on a provided prompt. This process utilizes a transformer decoder, similar to the architecture in DETR , to generate an object token. This object token plays a crucial role in predicting the prompt mask, subsequently predicting dynamic MLP weights and performing a point-wise product with the mask features. We refer to this resulting token as "position-aware" because it encodes essential information about the object, including details about its texture and position. Following this, a projector is applied to map the output dimension of the position-aware token to the image-level feature space as discussed below.

Image-level semantic feature mapsA single image can encompass multiple objects across numerous categories, capturing integrated context. We can conceptually view an image's feature map as a composition of region embeddings with varying structures. To fully capitalize the ViL model, we resize the input image to the required dimensions without cropping. Subsequently, we input this resized image into the ViL model, yielding the image-level semantic feature map denoted as \(V\).

Relating position-aware tokens and semantic feature mapsOur model, referred to as RegionSpot, efficiently establishes connections between region-level position-aware tokens and image-level semantic feature maps using the cross-attention mechanism . In this mechanism, position-aware tokens \(P\) serve as queries, while semantic feature maps \(V\) take on the roles of both keys and values. This relationship is formulated as follows:

\[S=(K_{v}^{T}}{})V_{v},\] (1)

Figure 2: Overview of our proposed RegionSpot. (a) We integrate position-aware tokens from a localization model, such as SAM, with image-level feature maps extracted from a ViL model like CLIP. This integration yields region-level semantic tokens, which are then subjected to region text alignment. (b) Our cross-modal feature interaction design based on the attention mechanism.

where \(F_{p}\) represents a transformation of \(P\), \(K_{v}\) and \(V_{v}\) are derived from separate linear projections of \(V\), and \(C\) is the projected feature dimension. This approach, well-established in the literature, has consistently demonstrated its effectiveness in information fusion. In our work, we extend its application to enhance region-level understanding in open-world scenarios. Specifically, we leverage this mechanism to integrate positional information with semantic content extracted from two distinct models at the regional level, while also considering the broader context from the entire image, as depicted in Figure 2(b).

Loss functionIn line with prior research, we generate text embeddings by processing category texts along with prompt templates, like _a photo of category in the scene_, using the text encoder. Then, we perform a dot product operation between each semantic token and its corresponding text features to calculate matching scores. These scores can be supervised using the focal loss .

Zero short inferenceFollowing , we focus on the more challenging region recognition task by utilizing human-annotated boxes or external region proposal generator. Inheriting the flexible prompting capability from SAM, our model allows for region recognition through prompting.

## 4 Experiments

Training dataIn pursuit of a robust training environment, we combined diverse datasets with varying label spaces. Our model's flexible architecture allowed us to seamlessly replace one-hot labels with class name strings. For training, we utilized publicly available detection datasets, comprising a total of approximately 3 million images. These datasets include Objects 365 (O365) , OpenImages (OI) , and V3Det (V3D) , each contributing uniquely to the diverse repository.

* Objects 365 (O365) is a large-scale object detection dataset featuring 365 distinct object categories across 0.66 million images. Our research employs an enriched version with over 10 million bounding boxes, averaging approximately 15.8 object annotations per image.
* OpenImages (OI) currently stands as the largest public object detection dataset, encompassing about 14.6 million bounding box annotations, equivalent to around 8 annotations per image.
* V3Det (V3D) distinguishes itself through a hierarchical organization, meticulously structuring up to 13,029 categories within a category tree.

Benchmark settingsIn our rigorous evaluation process, we utilized the extensive LVIS detection dataset , which encompasses 1203 categories and 19809 images reserved for validation. We do not prioritize the performance on COCO  which includes only 80 common categories covered by the Objects365 training dataset . This limitation may not adequately assess a model's generalization in an open-world setting.

Since our current emphasis is not on object localization, we utilized ground-truth and class-agnostic bounding boxes from an existing detector to predict categories based on corresponding text descriptions, following the RegionCLIP approach . Mean Average Precision (mAP) served as our evaluation metric.

Implementation detailsWe train RegionSpot using AdamW  optimizer with the initial learning rate as 2.5 \(\)\(10^{-5}\). All models are trained with a mini-batch size 16 on 8 GPUs. The default training schedule is 450K iterations, with the learning rate divided by 10 at 350K and 420K iterations. The training process unfolds in two sequential stages: (1) a warm-up phase leveraging the Objects365 to initiate the learning of region-word alignments, and (2) a phase of advanced learning for region-word alignments, utilizing a rich compilation from three diverse object detection datasets. The model is trained for 450K iterations at each stage. We implement several model variants: (1) RegionSpot-Lite: Integrating the base versions of both SAM and CLIP. (2) RegionSpot-Pro: Combining the SAM base with the more extensive CLIP large architecture. (3) RegionSpot-Pro1336: Further extending RegionSpot-Pro by using input image resolution of 336.

### Zero-shot Inference for Region Recognition

Zero-shot object detection on LVIS Val v1.0Results on the LVIS benchmark are presented in Table 1. With ground-truth bounding boxes as region proposals, our model substantially surpasses the CLIP baselines (which applies CLIP on image crops) by a large margin (_e.g._, **48.7, 49.2**_vs._**59.9**). For fair comparison, we finetune the CLIP withe adapter, our model substantially surpasses the CLIP by a large margin. Moreover, in simulation of real-world cases, we move forward to test our method with noisy region proposals generated from off-the-shelf proposal generator. We first employ SAM as a proposal generator, inputting dense grid points to automatically generate proposals. It can be seen that RegionSpot still consistently outperforms CLIP (_e.g._, **14.5**_vs._**18.2** on AP\({}_{all}\)) in this case, demonstrating the robustness of our method.

To fully exploit the potential of our method and synergy with the advancements of open world object detection (OWD), we further utilize region proposals from state-of-the-art OWD models, _i.e._, GLIP, as our region prompts. Comparing with GLIP-T trained solely on the objects365 dataset, we can observe a considerable performance gain achieved by RegionSpot (_e.g._, **4.2**_vs._**12.7** on AP\({}_{r}\) and **11.3** AP _vs._**14.1** on AP\({}_{all}\)). After scaling up the training data and use 336 resolution image as input, our models maintains superior performances over their GLIP counterparts. For instance, RegionSpot-Prox336surpasses GLIP-T by **17.6** AP\({}_{r}\) with less training data, showing compelling scaling behavior with data scale and input resolution. For more extensive evaluation, we also utilize bounding boxes generated by GLIP-L as prompts. It is noteworthy that RegionSpot achieves an impressive **13.1** increase in AP\({}_{r}\) compared to GLIP-L, even when trained on less data at higher efficiency. Despite using a noisy box, we were still able to achieve promising results, thanks to the robust localization ability of SAM. Additional experiments, including the ViLD protocol, can be found in the Appendix

Open vocabulary object detection under ViLD-protocalTo thoroughly evaluate our method, we conducted experiments using the ViLD protocol , training on base categories and testing on novel ones with the LVIS AP metric. For fair comparison, all the method training only use the LVIS-base dataset and use the RPN from RegionCLIP as proposal generator. We also adapted our training to the LVIS-base dataset. As shown in Table 3, RegionSpot demonstrates competitive performance.

  Method & Training Data & Proposals & Times & AP\({}_{r}\) & AP\({}_{f}\) & AP\({}_{all}\) \\  CLIP-L w/ box & - & GT & - & 40.6 & 59.2 & 48.7 \\ CLIP-L w/ mask & - & GT & - & 40.8 & 59.6 & 49.2 \\ CLIP-L\({}_{}\) w/ mask & - & GT & - & 43.2 & 59.9 & 49.5 \\ CLIP-L\({}_{}\)* w/ mask & O365, OI, V3D & GT & 0.30k & 46.8 & 63.2 & 53.1 \\  RegionSpot-Lite & O365, OI, V3D & GT & 0.18k & 42.0 & 65.6 & 53.0 \\ RegionSpot-Pro & O365, OI, V3D & GT & 0.18k & 50.6 & 68.8 & 56.6 \\ RegionSpot-Prox336 & O365, OI, V3D & GT & 0.20k & **55.4** & **68.6** & **59.9** \\  CLIP-L\({}_{}\)* w/ mask & O365, OI, V3D & SAM & 0.30k & 11.3 & 16.4 & 14.5 \\  RegionSpot-Pro & O365, OI, V3D & SAM & 0.18k & 13.1 & 17.3 & 16.1 \\ RegionSpot-Prox336 & O365, OI, V3D & SAM & 0.20k & **14.3** & **19.2** & **18.2** \\  GLIP-T (B) & O365 & GLIP-T(B) & 57.5k & 4.2 & 13.6 & 11.3 \\ RegionSpot-Lite & O365 & GLIP-T(B) & 0.18k & 12.7 & 15.7 & 14.1 \\ GLIP-T & O365,GoldG,Cap4M & GLIP-T & 92.1k & 10.1 & 25.5 & 17.2 \\ RegionSpot-Lite & O365, OI, V3D & GLIP-T & 0.18k & 20.0 & 24.2 & 21.1 \\ GLIP-L & FourODs,GoldG,Cap24M & GLIP-L & 120k & 17.1 & 35.4 & 26.9 \\ RegionSpot-Prox336 & O365, OI, V3D & GLIP-L & 0.2k & **30.2** & **30.0** & **29.8** \\  

Table 1: Comparison of open-world zero-shot object recognition performance using ground-truth (GT) boxes, SAM proposals generate by automatic mask generator, and GLIP boxes on the LVIS dataset. * indicate finetune the CLIP with Adapter. The training time test on one V100 GPU

  Method & Training Data & AP\({}_{r}\) & AP \\  GLIP-L & FourODs,GoldG,Cap24M & 28.2 & **37.3** \\ GroundingDINO-L & O365,OI,GoldG,Cap4M,COCO,RefC & 22.2 & 33.9 \\ RegionSpot-Prox336 & O365,OI,V3DET & **33.2** & 36.9 \\  

Table 2: Evaluation of zero-shot object detection on the LVIS minival dataset.

It outperforms the similarly frozen-backbone F-VLM by 1.1 AP\({}_{r}\). When we compared to RegionCLIP, which benefits from additional caption pretraining, RegionSpot significantly outperforms the pretrained version of RegionCLIP by 2.6 when utilizing same RPN.

Zero-shot object detection on LVIS minival5k To fully exploit the potential of our method, we report on MiniVal containing 5,000 images introduced in MDETR . We use the output proposals from GLIP as the prompt. As shown in Table 2, although we use 9x less training data, our model maintains superior performances over GLIP-L by 5.0 on APr. Further, our method also surpasses Grounding DINO-L (which even uses a more advanced detector) by 11.0 in APr.

Zero-shot instance segmentationWe evaluate the performance of instance segmentation in a zero-shot setting using the LVIS dataset . By leveraging the output from GLIP as the prompt, we direct it to RegionSpot for mask and class prediction. The mask AP is evaluated using the released X-Decoder  and OpenSeeD , both of which are trained with mask-text pairs. Impressively, as indicated in Table 4, RegionSpot outstrips X-Decoder and OpenSeeD by margins of 14.1 and 3.9 in AP, respectively. These outcomes suggest that our proposed RegionSpot can effectively harness the foundational model's capabilities to achieve more accurate region understanding.

Zero-shot object detection on ODinW This benchmark was designed to evaluate model performance in real-world scenarios. To accurately evaluate recognition capabilities, we filter the dataset to include only those with more than three categories. The AP for each dataset is reported in Table 5. Impressively, RegionSpot-Pro1336, utilizing GLIP-L proposals, surpasses GLIP-L by a margin of 2.5 AP, attributable to its precise region recognition. Furthermore, our method exceeds the performance of GroundingDINO, even though it employs a more advanced detector.

### Ablation Study

We conducted an ablation study for RegionSpot-BL using the boxes generated by GLIP. Unless otherwise mentioned, training was performed on three different detection datasets.

Enhancement with CLIP vision embeddingWe conjugate that a key ingredient with RegionSpot is the use of semantic information from CLIP vision encoder. To validate this assertion, we began our evaluation without the CLIP feature and subsequently integrated the class token output from the CLIP vision encoder. Results in Table 5(a) demonstrate that: (1) The CLIP feature offers a significant boost, pushing the baseline without CLIP vision encoder to **22.1**, which suggests inherent semantic limitations in SAM. (2) More notably, the inclusion of CLIP enhances overall performance to **23.7**. This underscores the potential of the class token to encapsulate global information from the entire image.

Position-aware tokens selection in SAMThe position-aware tokens are generated by intermediary module in the SAM. We examined various locations for this generation, specifically after the Prompt encoder, the Transformer decoder, and the MLP within the SAM. Results presented in Table 6(b) indicate that generating output tokens after the Transformer decoder yields the best performance.

   Method & AP\({}_{r}\) & AP\({}_{c}\) & AP\({}_{f}\) & AP \\  X-Decoder & - & - & - & 9.4 \\ OpenSeed & - & - & - & 19.6 \\ RegionSpot-Pro1336 & **21.5** & **25.0** & **23.2** & **23.5** \\  

Table 4: Evaluation of zero-shot instance segmentation on the LVIS minival dataset.

   Method & Proposals & Trainable Backbone & AP\({}_{r}\) & AP\({}_{all}\) \\  ViLD & RPN & ✓ & 16.1 & _22.5_ \\ RegionCLIP* & RPN & ✓ & 17.1 & _28.2_ \\ Detic-ViLD & RPN & ✓ & 17.8 & _26.8_ \\ F-VLM & RPN & ✗ & 18.6 & _24.2_ \\ RegionSpot & RPN & ✗ & **19.7** & _25.0_ \\  

Table 3: Comparison under the ViLD protocol . All methods use the ResNet50 backbone. * indicate pre-training with CC-3MThis observation is expected since tokens derived from the Prompt encoder are relatively undeveloped. Surprisingly, it can outperform GLIP (_i.e_., **18.6** vs. **17.2**). Moreover, there is a performance decline after the MLP, which can be attributed to dimensional reduction.

Module architectureAnother pivotal aspect of RegionSpot is the depth of model. To assess its impact, we experimented by varying the depth of our model. As indicated in Table 5(c), it is imperative for the model to have a sufficiently large depth, such as 3 blocks, without being excessive.

Prompt engineeringFinally, we carried out an ablation study focusing on prompt engineering, incorporating both box prompts in SAM and text prompts in the text encoder. As evidenced by the results in Table 6(a): (1) Leveraging multiple boxes as prompts in SAM boosts performance, achieving an AP of 22.1. This enhancement is credited to the self-attention mechanism of RegionSpot, which adeptly fuses information from varied regions. (2) Further utilizing text prompts results in a modest performance boost, specifically an increase of 1.6 AP.

Ablation study of SAM modelWe conjugate that a key ingredient is the position-aware information from SAM. We evaluating the impact of different SAM model sizes, such as ViT-L, is essential. We conducted experiments with varying SAM model sizes. As shown in the Table 6(b), our findings are summarized as follows: (1) Impact of SAM Model Size: Our results indicate that the use of larger SAM models (e.g., SAM-L) improves mask AP due to the higher quality of mask generation. However, for box AP, the improvement is not significant. This is because the SAM mask token primarily contributes position-aware knowledge, which is already sufficiently captured by ViT-B and ViT-L. (2) Choice of SAM Model: Given our focus on region recognition, we opted for SAM-B, balancing performance and computational efficiency.

### Visualization

Result visualizationIn Figure 3, we present the results of bounding region recognition on the LVIS  dataset, comparing between GLIP and RegionSpot. To assess the zero-shot recognition capability, we employ the same bounding boxes for both models. As observed, RegionSpot can distinguish even subtle differences, recognizing smaller objects like "lemon" and "tennis ball" and similar objects like "lantern" and "fireplug". Notably, RegionSpot stands out in terms of the accuracy of its label predictions, especially within the category of rare classes.

Table 6: **Ablation experiments on LVIS. (a) The effective of CLIP vision encoder; (b) Position-aware tokens selection ; (c) Depth of RegionSpot.**

Table 7: **Ablation experiments on LVIS. (a) The effective of prompt engineering; (b) The effective of SAM**

Table 5: Evaluation of zero-shot object detection on the ODinW dataset.

Model behavior visualizationTo gain more intuitive understanding on the effect brought by our RegionSpot, we examine the cross attention map on LVIS . We take the output tokens as 'query' and CLIP feature map as 'key' and 'value'. For clearer visualization, we omit the class token from the CLIP semantic feature. The resulting attention map clearly depicts the correspondence between the position-aware tokens generated by SAM and the feature map produced by CLIP. Using this arrangement, we gain a visual insight into how RegionSpot establishes connections between distinct features. As depicted in Figure 4, the attention maps vividly showcase RegionSpot capability to seamlessly incorporate both SAM and CLIP. Such visualizations serve a dual purpose: they highlight the efficacy of our method, and simultaneously, shed light on the intricate mechanisms underpinning the RegionSpot.

Figure 4: Cross-attention maps in RegionSpot. These maps show that the position-aware token aligns effectively with the semantic feature map of the entire image. In each row, the blue and red boxes are corresponding to the left and right maps respectively.

Figure 3: Qualitative prediction results of GLIP-T  (first row) and RegionSpot (second row) on the LVIS dataset . Our model recognizes the objects more accurately. Best viewed when zooming-in.

Conclusions and limitations

In this study, we introduce RegionSpot, a novel and efficient framework leveraging frozen vision and vision-language foundation models for region recognition, eliminating the need for training from scratch. To fully exploit knowledge in pretrained models and minimize the training overhead, we keep both foundation models frozen and focus optimization efforts solely on a lightweight attention-based knowledge integration module. Extensive experiments in the context of open-world object understanding confirms the superior performance of our method, even with a substantially smaller number of learnable parameters, which distinguishes our method and enables efficient training. Impressively, RegionSpot outperforms the leading GLIP-L by 2.9 in mAP, and this lead grows to 13.1 when considering complex rare categories. While our method advances open world region understanding, it still not unleash potential capabilities from the foundmental models, such as the automatic localization ability from SAM, which could reduce reliance on external region proposal mechanisms for object detection and enhance versatility. We leave this for further investigation.