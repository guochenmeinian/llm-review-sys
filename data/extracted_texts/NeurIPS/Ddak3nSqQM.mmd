# Policy Learning from Tutorial Books

via Understanding, Rehearsing and Introspecting

Xiong-Hui Chen\({}^{1,*,*}\)

Ziyan Wang\({}^{2,*}\)

Yali Du\({}^{2,}\)

Shengyi Jiang\({}^{5}\)

Meng Fang\({}^{4}\)

Yang Yu\({}^{1,}\)

Jun Wang\({}^{3,}\)

Equal Contribution. \({}^{+}\) Work done during Xiong-Hui Chen's visit at King's College London. Corresponding: yali.du@kcl.ac.uk, yuy@nju.edu.cn,jun.wang@cs.ucl.ac.uk. \({}^{1}\) National Key Laboratory for Novel Software Technology, Nanjing University, China & School of Artificial Intelligence, Nanjing University, China \({}^{2}\) Cooperative AI Lab, Department of Informatics, King's College London \({}^{3}\) AI Centre, Department of Computer Science, University College London \({}^{4}\) University of Liverpool \({}^{5}\) The University of Hong Kong

###### Abstract

When humans need to learn a new skill, we can acquire knowledge through written books, including textbooks, tutorials, etc. However, current research for decision-making, like reinforcement learning (RL), has primarily required numerous real interactions with the target environment to learn a skill, while failing to utilize the existing knowledge already summarized in the text. The success of Large Language Models (LLMs) sheds light on utilizing such knowledge behind the books. In this paper, we discuss a new policy learning problem called **P**olicy **L**earning from tutorial **B**ooks (PLfB) upon the shoulders of LLMs' systems, which aims to leverage rich resources such as tutorial books to derive a policy network. Inspired by how humans learn from books, we solve the problem via a three-stage framework: **U**nderstanding, **R**ehearsing, and **I**ntrospecting (URI). In particular, it first rehearses decision-making trajectories based on the derived knowledge after understanding the books, then introspects about the imaginary dataset to distill a policy network. We build two benchmarks for PLfB based on Tic-Tac-Toe and Football games. In the experiment, URI's policy achieves a minimum of 44% net winning rate against GPT-based agents without any real data. In the much more complex football game, URI's policy beat the built-in AIs with a 37% winning rate while GPT-based agents can only achieve a 6% winning rate. The project page: plfb-football.github.io.

## 1 Introduction

Humans can acquire new skills through various written materials that provide condensed knowledge, without the need for direct interactions with the target environment. In contrast, traditional policy learning paradigms, such as reinforcement learning (RL)  primarily rely on trial and error . Despite recent advances in offline RL  showing that policy improvements can be achieved simply by using pre-collected data, in fields such as embodied AI for robotics , the process of collecting large amounts of decision-making data remains costly and, at times, impractical or even impossible. Therefore, a question arises: _similar to how humans learn, can a policy learn from non-real data, e.g., from tutorial books?_

We argue that the recent successes of Large Language Models (LLMs), such as GPT-4 , and LLaMA  already demonstrated the potential to learn from textual content. However, current studies focus on using LLM directly for decision making  or integrating them as auxiliary modules in other machine learning workflows . In this study, we introduce a novel topic built upon the shoulders of LLMs' systems: Policy Learning from Books (PLfB). PLfB aims to derive a policynetwork directly from natural language texts, bypassing the need for numerous real-world interaction data, as shown in Fig. 1(A-B). _This can be viewed as a further step towards enabling more resources for policy learning and also a more generalized form of offline RL problem, which uses textbooks to learn a policy offline._ The essential challenge of PLfB comes from the inevitable large modality gaps between the text space, which includes the knowledge related to decision-making, and the policy network space, which formulates the parameters of the policy function.

To realize PLfB, inspired by human learning processes, we propose a three-stage learning methodology: understanding, rehearsal, and introspection (URI), which is shown in Fig. 1(C). For understanding, it first extracts knowledge from books to form a knowledge database; then it rehearses imaginary decision-making trajectories with the help of the knowledge retrieved from the database; finally, it introspects on the imaginary dataset to distill a policy network for decision-making. We present the first practical implementation of URI, employing LLMs to convert book paragraphs into pseudocode for policy, dynamics, and reward functions. This pseudocode forms a code-based knowledge database, which is used to generate an imaginary dataset based on retrieval augmented generation techniques . A policy learning technique inspired by offline RL  is then applied to distill a policy network that addresses inaccuracies in the imagined actions, rewards, and states.

In the experiments, we build two benchmarks from Tic-Tac-Toe and Football games for PLfB. We first validate URI on Tic-Tac-Toe game, where the tutorials are manually constructed to cover the complete knowledge for decision-makings. The results show that our method achieves at least 44% net win rate against GPT-based agents without any real game interaction. In addition, we build a testbed based on football tasks, which is popular in recent studies  as a difficult decision-making task, and focus on policy learning from football tutorial books, which naturally contain condensed knowledge, especially information closely related to football skill acquisition, the environment and dynamics of football games, and ways to evaluate football behaviors. We collect football tutorial books from RedPajama . The policy is deployed in the Google Football simulator  directly. Our agent controlled by the policy network could beat the built-in AI with a 37% winning rate on average while using GPT as the football agent can only achieve a 6% winning rate. The experiments demonstrate the scalability of our approach from simple board games to complex scenarios.

## 2 Related Work

LLMs have demonstrated remarkable potential in achieving human-level ability in various tasks, sparking a surge in studies investigating LLM-based autonomous agents. Based on the different roles of LLM in the agents, there are two types of work. Firstly, LLM can be used as an actionable agent directly . Early works [19; 20; 21] prefer open-loop plans and hard-code search algorithms to improve the long-term planning ability of LLM. Closed-loop planning [22; 23; 24] leverages environmental feedback, thereby facilitating more adaptive decision-making. Secondly, LLMs can be used to assist or accelerate the learning of agents. The form of assistance can be quite diverse. It can generate high-level plans [25; 26], (surrogate) rewards [27; 28; 29; 30; 31; 32], transitions [33; 34; 35], or as an adapter to convert human instructions into structured inputs . In our work, LLMs play a different role compared to all the above-mentioned types. LLMs are to understand the knowledge in the books, rehearsing the decision-making process to derive an imaginary dataset based on the knowledge. The policy to control the agent is a simple neural network distilled from the imaginary data.

Figure 1: (A-B) A comparison between the problem of policy learning from tutorial books (PLfB) and from data; (C) An illustration of the understanding, rehearsal, and introspection for PLfB.

## 3 Preliminaries

**Markov Decision Process (MDP)** is defined as a tuple of \(:=(,,T,R,,_{0})\) of the target environment, where \(\) is state space, \(\) is action space, \(T:()\) is the transition function (\(()\) is the probability simplex), \(R:[0,R_{}]\) is the reward function, \([0,1)\) is the discount factor, and \(_{0}\) is the initial distribution over states. A policy \(:()\) describes the distribution of actions for each state.

**Retrieval Augmented Generation (RAG)** has shown great ability to improve LLM's generation ability for knowledge-intensive tasks. RAG enables LLMs to query external data sources to obtain relevant information before proceeding to answer questions or generate text. Formally, given an LLM \(\), the retrieval module \((x,\{y_{i}\})\) takes a textual query \(x\) as input, finds the most relevant textual segment \(y^{*}\) by finding the most similar \((y)\) from the knowledge database \(y\{y_{i}\}\) compared to \((x)\). Augmentation transforms the content into an additional input for the LLM's generation. In this paper, RAG is used to implement the URI algorithm. We use standard RAG techniques as the retrieval module, i.e., cosine similarity matching [37; 38], within the implementation of URI. In particular, we use GPT embedding to index the texts in the database and also the current query, then use cosine similarity to find the top-\(n\) matching data for downstream tasks, i.e., \((x,[y^{*}_{1},...,y^{*}_{n}])\).

**Offline RL** addresses the problem of learning policies from a pre-collected dataset \(\). Existing studies can be classified into two categories: model-free and model-based methods. Model-free [40; 41; 42; 43; 44; 5] methods learn a policy directly from the dataset \(\) through a specially designed conservative policy learning loss which usually aims to avoid policy taking actions unseen in \(\). Model-based offline algorithms [6; 46; 47; 48; 49; 50; 51; 52] first estimate the dynamics and reward model \(\) and \(\) from the dataset \(\). The policy is learned by iterating with \(\) and \(\). In the process, specially designed penalties \(\) are adopted to discourage the policy from visiting states where the model predictions are of high uncertainty.

## 4 Problem Formulation of Policy Learning from Tutorial Books

The goal of Policy Learning from Tutorial Books (PLfB) is to use an algorithm \(\) to learn a policy \(^{*}=(,||)\) from textual books \(\), which can maximize the cumulative discounted reward \(()=_{t}_{a_{t}}^{t}R(s_{t},a_{t})\), where the book is defined as \(N_{b}\) segments \(b_{i}\) divided by paragraphs, i.e., \(:=\{b_{1},...,b_{i},...,b_{N_{b}}\}\), and \(||\) denotes the brief textual descriptions of the structures of MDP, including the descriptions of state space, action space, the task we faced, and the initial state distribution. \(||\) is inevitable to be used since \(\) only gives general knowledge of the target environment while \(||\) defines the specific information, e.g., the exact format to interact with the simulator. Unlike standard RL, during the learning process of PLfB, the algorithm has no access to the environment. To ensure the feasibility of extracting a non-trivial policy, we make a mild assumption that \(\) contains the descriptions of transition functions \(\{\}\), and reward functions \(\{\}\) that can be regarded as the approximations of \(T\) and \(R\), respectively. These descriptions could range from simple game rules to complex natural language paragraphs, depending on the books. It also contains descriptions of relatively high-quality behavior policies.

## 5 Understanding, Rehearsing, and Introspecting

In this section, we first present our motivation for the proposed three-stage framework in Sec. 5.1. After that, we explain how we implement those stages in Sec. 5.2, 5.3, and 5.4 respectively.

### Motivation of the Three-Stage Framework

As shown in Fig. 1, the methodology to solve PLfB consists of three major stages: understanding, rehearsing, and introspecting (URI). The motivation of these three modules can be easily understood by reviewing how humans learn from books: as humans, we first extract knowledge from books to extend our knowledge database. Then, to acquire skills, we often rehearse the possible consequences of applying the skill in our mind, integrating knowledge from the books with our prior life knowledge. Finally, we will re-examine the steps we took in our minds and think how we could have done better until we confirm that we know how to execute in the real world. In this paper, we mimic the above three steps via the following modules: **Understanding** module takes paragraphs of books as input and forms a knowledge database organized in pseudo-code. **Rehearsing** module iteratively takes the current imagined state as input and outputs the action, next state, and reward with guidance from thedatabase. After gathering this imagined content to form a dataset, **Introspecting** module distills a policy network, which should consider errors of generations of state, action, and rewards.

We give the overall architecture of our implementation of URI in Fig. 2. A detailed description of how they realize these functionalities will be discussed in the following.

### Book Content Understanding

The understanding module is responsible for extracting knowledge \(:=\{K_{1},...,K_{i},...,K_{N_{K}}\}\) from the books \(\), where \(K_{i}\) denotes one piece of knowledge. For we humans, the knowledge is naturally stored in our brains. For machines, the first question is what should be the appropriate format to represent the knowledge. Since LLM has shown superior performance in code generation and codes themselves as interpretable, compact, and expressive languages , we also choose it as the basic format of knowledge representation. Different from previous works  which directly asked for runnable code for the downstream tasks using, we do not execute this code. Instead, we just use it as a more flexible and abstract description of knowledge which still maintains a rigorous control flow. As shown in Fig. 2(A), a knowledge extractor module is used as the first step. The knowledge extractor is an LLM-injected model. We iteratively ask \(K_{j}=_{}(b_{i})\) to examine whether a paragraph is related to the decision-making elements, i.e., policy functions, reward functions, dynamics functions, and how to represent them as pseudocode.

Moreover, humans often learn by repeatedly reading the texts across pages and even books, updating existing knowledge with newly learned ones, and summarizing them into more general and abstract forms. A similar procedure is achieved by the knowledge aggregator module, shown in Fig. 2(A), as the second step of understanding. The knowledge aggregator is also an LLM-injected model. We iteratively ask \(_{}([K_{i},K_{j},K_{l},...])\) to aggregate relevant knowledge among different segments based on a similarity estimation of the embedding model \(\). Formally, let \(^{0}:=\{K^{0}_{1},...,K^{0}_{N^{0}_{K}}\}\) be the paragraph-wise knowledge extracted by \(_{}\). For the \(j\)-the iteration, \([K^{j+1}_{o},K^{j+1}_{p},...]=_{}([K^{j}_{i},K^{j}_{l},K^ {j}_{n},...])\), where \([K^{j}_{i},K^{j}_{l},K^{j}_{m},...]\) is from \(^{j}\) by selecting the most similar \(N_{}\) pieces of knowledge by comparing their cosine similarity under the embedding model \((K)\) of GPT and \(o,p,i,l,n\) here denote the indexes. \([K^{j+1}_{o},K^{j+1}_{p},...]\) are then added to \(^{j+1}\). The iteration will stop if \(|^{j+1}|>=|^{j}|\), where \(|^{j}|\) is the pieces of knowledge at \(j\)-th iteration. After iteration stops, the remaining knowledge pieces constitute the knowledge databases by splitting them into dynamics-related knowledge \(_{T}\), reward-related knowledge \(_{R}\), and policy-related knowledge \(_{}\) for later modules to use. More details are provided in Appendix F.1.

Figure 2: The URI pipeline consists of three major stages: (A) **Understanding:** The knowledge extractor and aggregator modules process paragraphs from books to form a structured knowledge database organized as pseudocode. (B) **Rehearsing:** Using the knowledge database, the simulator generates and iterates through imagined states, actions, and rewards to create an extensive imaginary dataset. (C) **Introspecting:** The introspection module refines the policy network by evaluating and correcting errors in the generated states, actions, and rewards to ensure accurate and effective policy implementation. The pseudocode of the pipeline is in Appendix F.8.

### Knowledge-based Rehearsing of Decision-Making

When humans develop a rough understanding of a new skill \(\) from knowledge \(\), they will usually imagine what choice they will make given certain situations and what are the possible consequences in their minds. This rehearsing procedure would help humans correct apparent mistakes and consider better actions for long-term benefits [54; 55].

We implement a closed-loop generation process involving the LLM-injected dynamics function \(_{T}\), reward function \(_{R}\), and policy \(_{}\), as depicted in Fig. 2(B). This approach resembles the model rollout in traditional model-based RL, where the policy, reward function, and dynamics function are represented by LLMs. Given the current imagined state \(_{t}\), the LLMs \(_{}\) first predict the most plausible action \(_{t}\). Subsequently, \(_{T}\) and \(_{R}\) generate the next state \(_{t+1}\) and the associated reward \(_{t}\) based on this action and the current state. In the process, a knowledge retrieval module is involved in selecting relevant knowledge pieces enhancing the LLM input with this information for LLM's predictions, e.g., \(_{t}=_{}(_{t},(s_{t},_{}))\), \(_{t}=_{R}(_{t},_{t},(s_{t},_{R}))\) and \(_{t+1}=_{T}(_{t},_{t},(s_{t},_{T}))\). The knowledge retrieval module includes the following two steps:

**State-based Knowledge Scope Retrieval:** The fundamental problem of standard RAG techniques, i.e., embedding-vector similarity matching, in this scenario, is the modality gap between the query \(s_{t}\) and the knowledge \(\). Standard RAG approaches aim to identify information closely related to the query, while here we need to ask the most suitable knowledge to be applied as a dynamics/reward/policy function _that has the best predictions_ for the queried state and action. Standard RAG techniques tend to retrieve knowledge pieces that include similar text patterns as the queried states and fail to find the best knowledge for predictions. To address this, we propose a knowledge scope retrieval method that includes a simple yet effective preprocessing step to bridge the modality gap. In particular, we traverse all knowledge pieces \(K_{i}\) by iteratively sampling \(n_{}\) knowledge pieces \(\{K_{i}\}_{n_{}}\) from the database \(\), combining them with simulator information \(||\) and using LLM \(\{K^{S}_{i}\}_{n_{}}=_{}(\{K_{i}\}_{n_ {}},||)\) to determine the preferred scopes \(K^{S}_{i}\) for each piece of knowledge. \(K^{S}_{i}\) is defined by the preferred subspace of state to use the knowledge. Then a standard RAG technique is applied to identify the most relevant knowledge scope \(K^{s}_{j}\) and its relevant knowledge \(K_{j}\). Formally, \((\{K_{j}\},\{K^{S}_{j}\})=_{}(,^{S})\), where \(^{S}\) is the scopes of the knowledge database. This method is effective for embedding models in retrieving the correct knowledge as the texts of keys and queries both are about the descriptions of states.

**Post-Retrieval: Knowledge Instantiation:** Predicting based on code knowledge requires the LLM's robust understanding of code. We refine this process employing an LLM to instantiate the code \(K^{I}=_{}(,||,\{K_{j}\})\) based on the current state \(\), the simulator information description \(||\) and the knowledge \(\{K_{j}\}\) retrieved by \(_{}\). Knowledge instantiation involves generating a domain-specific pseudocode based on the knowledge coded in the retrieved information, tailored to the target environment's current state and action requirements.

Finally, three LLMs are involved to generate the imaginary dataset \(_{}\), including \(_{t}=_{}(_{t},_{}(s_{t},| |,_{}(,^{S}_{})))\), \(_{t}=_{R}(_{t},_{t},_{}(s _{t},||,_{}(,^{S}_{R})))\) and \(_{t+1}=_{T}(_{t},_{t},_{}(s _{t},||,_{}(,^{S}_{T})))\), where \(^{S}_{}\), \(^{S}_{R}\), and \(^{S}_{T}\) are the scope of the knowledge database \(_{}\), \(_{R}\), and \(_{T}\) respectively. To start the rollout of a trajectory, we need a state as the initial state. It has several choices to achieve, e.g., sampled from \(\), generated by another LLM, or pre-collected a small number of real states from the target environments as part of simulator information, i.e., initial state distribution \(_{0}\). Since the first choice might introduce unrealistic initial states in complex scenarios, in Football tasks, we opt for a more practical approach by pre-collecting a small set of real states from the target environments for our experimental validation. The detailed setup is in the experiment section. Besides, during the rollout over \(H\) steps, we reuse instantiated knowledge to reduce the overload of LLM's callings. More details are provided in Appendix F.2.

### Introspecting based on the Imaginary Dataset

The direct output LLM in the rehearsing stage can be sub-optimal or incorrect. We would like to re-examine the collected data and try to distill a policy that can avoid the side effects. We can regard the data \(_{}\) collected during the rehearsing stage as an offline dataset and apply offline RL algorithms to train an improved policy from the dataset. However, directly applying existing offline RL algorithms over-simplifies the problem. Compared with the standard offline setting where only the behavior policy is sub-optimal, there is an additional misalignment in the data generatedduring the rehearsal: the transition and the reward function estimated by the LLM are also inaccurate. Overlooking such inaccuracy would result in a policy exploiting the sub-optimal transition and reward function and cause performance degradation or even risky behaviors in the final deployment.

To solve the problem, in this paper, we adopt the Conservative Q-learning  as the base offline RL algorithm, whose learning objective is as follows:

\[_{Q}_{} (_{_{},a (a|)}[Q(,a)]-_{, _{}}[Q(,)]+())\] \[+_{,_{}}[(Q( ,)-}^{}(,))^{2}],\]

where \(}^{}\) is the bellman update operator to update the Q-value function , and the first term is to learn a policy with conservative Q value . As a solution of introspecting from imaginary data, as shown in Fig. 2(C), we add the uncertainty of the reward and transition estimation as the regularization terms, \(_{R}\) and \(_{T}\) over the original reward \(\) output by the LLM \(_{R}\). In practice, we adopt these regularization terms by applying them when we backup the \(^{k}\):

\[}_{}^{}(,):=-_{R }_{R}(,)-_{T}_{T}(,)+ _{^{}_{},a^{} _{}(a^{}|^{})}[Q(^{},a^{})],\]

where \(^{}_{}\) is to sample the next state given \(,\), \(_{R}\) and \(_{T}\) are two hyper-parameters to control the weighting of the uncertainty terms. Inspired by Model-based Offline Policy Optimization (MOPO) , the uncertainty is estimated by an ensemble of \(N_{}\) Gaussian models of \(\) and \(\), which is learned by maximizing logarithmic likelihood from the imaginary dataset. Then the uncertainty is estimated by \(_{R}(s,a)=_{i\{1,...,N_{}\}}_{i}^{}(s,a)\) and \(_{T}(s,a)=_{i\{1,...,N_{}\}}_{i}^{T}(s,a)\), where \(_{i}^{r}\) and \(_{i}^{T}\) are the \(i\)-th reward and dynamics model's predicted standard deviation for \(s,a\) respectively. We call the solution of CQL with \(}_{}^{}\) as the bellman update operator Conservative Imaginary Q-Learning (CIQL). This regularization is easy to implement and can force the policy to generalize better over the regions where the LLM outputs inconsistent next states and rewards.

## 6 Experiments

We build two benchmarks with increasing complexity to evaluate PLfB: a classical Tic-Tac-Toe game with discrete states and deterministic rules, and the Google Research Football environment (GRF)  featuring continuous states and multi-agent interactions. In Sec. 6.1, we introduce the experimental settings for both environments. In Sec. 6.2, we analyze the performance of URI against various baselines. In Sec. 6.3 and 6.4, we examine the effectiveness of our knowledge extraction and retrieval mechanisms. Finally, in Sec. 6.5-6.8, we provide a detailed analysis of the training process, including data generation, ablation studies, inference efficiency, and dataset visualization.

### Experiment Setups

Tic-Tac-Toe GameTo build a proof-of-concept benchmark for PLfB, we first construct a Tic-Tac-Toe (TTT) environment where players take turns placing their marks (X or O) on a 3x3 grid, aiming to form a line of three marks. This classical game serves as an ideal testbed for two reasons: (1) it has a known optimal solution (minimax algorithm), allowing us to evaluate how close our learned policy is to optimality; (2) due to its discrete and deterministic nature, we can collect a complete set of optimal trajectories covering all possible game states. Based on these trajectories, we can use GPT to generate comprehensive tutorial texts that contain complete knowledge of game mechanics and winning strategies, thereby controlling for the impact of knowledge incompleteness on algorithm performance. In our experiments, each policy plays as 'X' and moves first, with performance measured by win rate, draw rate, loss rate, and net win rate (win rate minus loss rate) across different opponents. For detailed game rules and setup, please refer to Appendix A.

Google Research Football is a physics-based 3D football simulator that supports the main football rules such as goals, fouls, corners, penalty kicks, and offside. Google Research Football (GRF) includes a built-in AI bot for the opposing team, whose difficulty can be adjusted between 0 and 1. An illustration of the game can be seen in Fig. 7. We define three custom difficulty levels for our experiments on the 11vs11 scenario: easy, medium, and hard. The difficulty levels differ in the bot's reaction time and decision-making defined in GRF, with higher difficulty corresponding to a stronger opponent. The major metric in our experiment is **Goal Difference per Match** (GDM), calculated as the average number of goals scored in all league matches minus the average number of goals conceded per match. For URI, for each seed, we selected the average performance of the top 3 checkpoints among all recorded checkpoints as the final performance. For more details about the GRF implementation, please refer to Appendix B.

DatasetsWe utilize three datasets in our approach. For Tic-Tac-Toe, we generate tutorial texts by having GPT analyze all possible game trajectories collected from an optimal minimax policy (see Appendix A.2). For football, we collect the textbook dataset from the open-source book dataset RedPajama-1T , focusing on titles and abstracts related to football or soccer. After filtering, we obtain a curated set of ninety books closely aligned with the domain. For both environments, we need initial states to start our imaginary data generation process. In Tic-Tac-Toe, since we play as 'X' and move first, we always start from an empty board. In GRF, we sample 7,500 states from the rollout of a rule-based policy competing with the hard built-in AI. Due to limited resources, to distill the policy, we imagine 75,000 transitions via URI, which is 10 times compared to the initial states.

BaselinesWe compare URI against several baselines across both environments. **LLM-as-agent** directly uses a large language model (GPT 3.5) to output actions conditioned on the current state description. **LLM-RAG** enhances LLM-as-agent by retrieving relevant knowledge from the database extracted from the tutorial books, similar to the retrieval step in our rehearsing stage, but directly outputs the action without policy learning. For Tic-Tac-Toe, we additionally include **Minimax-noise**, which follows the optimal minimax strategy but with 30% random action selection to serve as a near-optimal baseline. For GRF, we include **Rule-based-AI** from the Kaggle Football Competition , which is hand-designed and serves as a reference for the performance of hand-coded policies. **Random Policy** randomly chooses actions in both environments. For the implementation details about the baselines, please refer to Appendix D.

### Policy Performance Evaluation

TTT ResultsWe first evaluate URI's performance in the Tic-Tac-Toe environment. Tab. 1 shows the head-to-head match results between different policies, where each policy plays as 'X' and goes first. URI demonstrates superior performance across all opponents, achieving net win rates (win rate minus loss rate) of +66%, +44%, and +52% against LLM-as-agent, LLM-RAG, and Random Policy respectively. Moreover, when playing against Minimax-noise, which introduces 30% randomness to the optimal minimax strategy, URI maintains a positive net win rate of +26%, while all other baselines suffer negative net win rates. This indicates URI's ability to learn effective strategies from tutorial texts even in this classical game setting.

GRF ResultsThe results in Tab. 2 demonstrate the superiority of the proposed URI approach compared to the baselines in the 11 vs 11 full-game scenarios of the GRF environment. The LLM-based agents, including LLM-as-agent and LLM-RAG, exhibit zero-shot task completion capabilities, outperforming the Random Policy. However, even with the use of RAG techniques, the best-performing LLM agent can only barely match the performance of the Medium-level built-in AI and fails to achieve any wins against the Hard-level built-in AI. In contrast, URI surpasses the performance of the baseline methods on all difficulty levels. Surprisingly, in the Hard task, URI achieves a higher win rate than the Rule-based Policy. We believe this is due to URI's ability to leverage knowledge from domain textbooks and generate high-quality imaginary data for policy learning, enabling it to learn more adaptable and robust policies compared to the hand-crafted rule-based AI.

These results across both simple and complex environments highlight the scalability of URI in learning strong policies from well-defined board games to challenging scenarios.

### Effectiveness of Code-based Knowledge Extraction and Aggregation

    &  &  &  &  \\   & W & D & L & W-L & W & D & L & W-L & W & D & L & W-L & W & D & L & W-L \\  _LLM-as-agent_ & \)} &  & - &  &  &  &  &  &  &  &  &  &  &  &  \\ _LLM-RAG_ & & & & & & & & & & & & & & & \\  _URI (Ours)_ & **80\%** & 6\% & 14\% & 46\% & **62\%** & 10\% & 18\% & **44\%** & **70\%** & **12\%** & 18\% & **45\%** & **36\%** & 10\% & **42\%** \\    & 84\% & 16\% & 0\% & +84\% & 68\% & 32\% & 0\% & +68\% & 78\% & 22\% & 0\% & +78\% & 34\% & 66\% & 0\% & +34\% \\   

Table 1: Performance of different policies in Tic-Tac-Toe. Each policy plays as ‘X’ and moves first, tested across 100 matches (50 for LLM-based methods).

According to Sec. 5.2, we perform an iterative process of code extraction and aggregation to understand the tutorial books and obtain executable knowledge. As shown in Fig. 3(a), for Tic-Tac-Toe, the initial extraction yields approximately 600 code segments which are effectively consolidated through the aggregation process. Similarly, in the more complex football domain (Fig. 3(b)), the number of code segments for the dynamics, policy, and reward functions decreases significantly over the aggregation rounds. Through iterative aggregation, the number of code segments, decreases significantly in both environments, which helps condense the extracted knowledge into a more compact and coherent form. This consistent pattern across domains of varying complexity demonstrates the robustness of our knowledge extraction and aggregation approach.

### Correlation between Knowledge Embedding and Current State Embedding

To validate the effectiveness of our code embedding method mentioned in Sec. 5.3, we conducted experiments in GRF task comparing it with two baselines: **Vector Embedding**, which directly compares the state and code embeddings, and **Code Summary**, which first summarizes the code segments before comparing the embeddings. We evaluate the top-15 hit rate in a hand-crafted test dataset. The observations and actions in the dataset we used are collected by the rule-based policy interacting with the real environment, while the ground-truth codes are labeled by GPT and aggregated using a similar way as in Sec. 5.2. As shown in Fig. 4(a), our method significantly outperforms the other two baselines on both pre-trained language models. The hit rate, which measures the proportion of relevant code segments retrieved, is consistently higher for URI across all three random seeds, demonstrating its robustness and superiority in improving the correlation between the state embedding and the code embedding. These results highlight the importance of learning a dedicated matcher for effective code retrieval in our framework.

### Tracing the Data Generation Process

In URI, it is crucial to guarantee that the imaginary data include states, actions, and rewards. The task is non-trivial since it has to involve several transformations to align the gap between textual contents in books and decision-making trajectories in MDP. To demonstrate this process in detail, we use the football environment as a case study. We trace how \(_{}\) outputs an imaginary action \(\) on an imaginary state \(\), which is actually collected at the start of a football game in GRF. We record all the intermediate outputs to trace the action generation process. The result is shown in Fig. 5. It is clearly shown that the output action "dribble" is fully supported by the logic in"has_space" branch in \(K_{I}\), "assess_forward_decisions" function in \(K\), and the paragraph of "Player in possession of the ball" in the raw content of tutorial books. However, we would like to point out that there are also some

  
**Level** & & **LLM-as-agent** & **LLM-RAG** & **Random Policy** & **URI (Ours)** & **Rule-based-AI** \\   & Win & 20\% & 30\% & 2\% & **37\% \(\) 4\%** & 70\% \\  & Draw & 60\% & 60\% & 55\% & 57\% \(\) 4\% & 30\% \\  & Loss & 20\% & 10\% & 43\% & 6\% \(\) 4\% & 0\% \\   & GDM & 0.0 & 0.2 & -0.58 & **0.40 \(\) 0.14** & 0.7 \\   & Win & 0\% & 20\% & 2\% & **42\% \(\) 12\%** & 70\% \\  & Draw & 60\% & 60\% & 43\% & 50\% \(\) 8\% & 30\% \\  & Loss & 40\% & 20\% & 55\% & 8\% \(\) 4\% & 0\% \\   & GDM & -0.4 & 0.0 & -0.76 & **0.43 \(\) 0.24** & 0.7 \\   & Win & 0\% & 0\% & 3\% & **32\% \(\) 14\%** & 30\% \\  & Draw & 50\% & 40\% & 43\% & 58\% \(\) 6\% & 70\% \\  & Loss & 50\% & 60\% & 53\% & 10\% \(\) 7\% & 0\% \\   & GDM & -0.5 & -0.6 & -0.73 & **0.32 \(\) 0.14** & 0.3 \\   & Win & 6.7\% \(\) 9.4\% & 16.7\% \(\) 12.5\% & 2.3\% \(\) 0.5\% & **40.3\% \(\) 6.2\%** & 56\% \\  & GDM & -0.30 \(\) 0.22 & -0.13 \(\) 0.34 & -0.69 \(\) 0.08 & **0.38 \(\) 0.05** & 0.56 \\   

Table 2: Performance Comparison of Different Policies Against Built-in AI Levels in a GRF 11 vs 11 settings, where the performance of URI is averaged among three different seeds, LLM-as-agent, LLM-RAG is tested with 10 matches, and URI policy and random policy is tested with 40 matches.

Figure 3: Knowledge Segment Aggregation.

examples that demonstrate LLMs having hallucinations in the generation, and the retrieved module might also miss the ground-truth piece of knowledge. More results are provided in Appendix H. These results indicate the necessity of introspecting in URI. The relevant ablation studies are in Sec. 6.6.

### Importance of the components in URI

We validate the effectiveness of the rehearsing technique presented in Sec. 5.3 and the CIQL method introduced in Sec. 5.4 through ablation studies. Specifically, we constructed the following variants of the URI framework in GRF task: (1) **URI-w/o-rehearsing**, which solves the policy without using the rehearsing dataset and relies on a pre-collected dataset of 7,500 samples for offline RL policy training; (2) **URI-w/o-penalty**, where penalties \(_{T}\) and \(_{R}\) are zero, same as the standard CQL for policy learning; (3) **CQL-real-data**, where we collect real data of equivalent scale to the rehearsing-generated data using rule-based AI and apply standard CQL for offline RL policy training. The results are shown in Fig. 4(b).

Firstly, URI-w/o-rehearsing demonstrates that without generating a substantial amount of imaginary data through rehearsing, solely relying on offline RL algorithms to train a policy with our pre-collected 7,500 samples is ineffective. The results indicate that it cannot even beat the AI on Easy difficulty, though it still performs better than a random strategy. URI-w/o-penalty underscores the importance

Figure 4: (a). Comparison of different code retrieval methods on two pre-trained language models. (b). Performance comparison of different variants of the URI framework in the GRF. This figure illustrates the average GDM, win, draw, and lose rates among the three levels of built-in AIs. The error bars in the figure indicate the standard deviation from the mean performance for each configuration in three random seeds.

Figure 5: An example of the URI data generation process in the football game. The imaginary state \(\) is collected from the real environment, which is 12 timesteps of a football game, while the rendered image is the corresponding scenario generated by the simulator. The imaginary action is “dribble”, where the logic is supported by the “has_space” branch in \(K_{I}\). Based on this, we **bold** the relevant information in the predecessor nodes and skip irrelevant information with the ellipsis “......”.

of penalizing the uncertain aspects of the outcomes generated from the imaginary data. Neglecting this penalty leads to results worse than those of URI-w/o-rehearsing. Finally, our method slightly outperforms CQL-real-data. We attribute this improvement to the strategies inferred from prior knowledge by LLMs, which are partially superior to built-in AI behaviors, or possibly because LLMs generate more diverse data. The successful ablation results in such a challenging domain illustrate the considerable potential of PLfB.

### Efficiency in inference

Lower cost of inference is one of the benefits for agents controlled by URI policies. We compare the inference time per action for different methods in the GRF task in Tab. 3. Our approach takes only 0.009 seconds on average to choose an action, which is significantly faster at least 300 times than using LLMs directly as agents (2.84 seconds) or with retrieval-augmented generation (RAG) (4.12 seconds) in complex environments. This makes URI more suitable for real-time decision-making in the football simulator. The efficiency gain comes from the fact that URI distills the knowledge from the LLM into a compact policy network, which can be executed quickly without the need for expensive LLM inference at each step.

### Imaginary Dataset Visualization

We visualize the imaginary datasets to analyze the quality of generation and uncertainty estimation. Here we present the results from the GRF, while the TTT results can be found in Appendix G.1. We choose t-SNE  as the visualization method and project the imaginary dataset and a real dataset collected by the rule-based policy into 2-d space for comparison. The results are in Fig. 6. The "real data" marks the data collected by the rule-based policy, while "low-unc. data" and "high-unc. data" represent segments of the imaginary dataset categorized by their uncertainty scores \(R_{T}\) and \(R_{R}\) falling within the lower and upper 50% percentiles, respectively. The real data and the imaginary data follow a similar data distribution, which indicates the effectiveness of the rehearsing process in URI. Besides, as highlighted by the yellow dashed circles, the uncertainty score also identifies parts of the clusters that are out of the real data distribution, which will be penalized when introspecting via CIQL.

## 7 Conclusion and Discussion

Inspired by the learning behavior of humans when they try to acquire a new skill, we propose PLfB that trains an agent from books, instead of numerous interaction data with the real environment. We also implement a practical algorithm of understanding, rehearsing, and introspecting modules to realize such a learning paradigm. The result of deploying our method in Tic-Tac-Toe and football game environments demonstrates a huge improvement in the winning rate over the baseline methods. This success proves the feasibility of utilizing knowledge stored in various written texts for decision-making agents, which was neglected by the community for a long time.

One major limitation of URI is its implicit dependence on the quality of the "tutorial books": they should cover sufficiently the dynamics, policy, and rewards of the targeted environment so that relevant knowledge and imaginary data can be extracted to train the policy. It is also important to develop metrics evaluating the textual data quality to decide whether to use URI.

Besides, we hope that the promising result in current experiments will initiate more research on PLfB. We leave more discussion of several interesting open problems in PLfBthat URI does not address in Appendix I.

   & Time Cost(s) \\  LLM-as-Agent & 2.84 \(\) 0.71 \\ LLM-RAG & 4.12 \(\)1.46 \\ URI & **0.009 \(\) 0.0004** \\  

Table 3: Comparison of inference time per action for different methods in GRF.

Figure 6: Visualization of the projected distributions for real and imaginary datasets in the football environment.