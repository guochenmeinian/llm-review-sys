# Contrastive Training of Complex-Valued Autoencoders for Object Discovery

Aleksandar Stanic\({}^{1}\)1

Anand Gopalakrishnan\({}^{1}\)1

Kazuki Irie\({}^{2}\)2

Jurgen Schmidhuber\({}^{1,3}\)

\({}^{1}\)The Swiss AI Lab, IDSIA, USI & SUPSI, Lugano, Switzerland

\({}^{2}\)Center for Brain Science, Harvard University, Cambridge, USA

\({}^{3}\)AI Initiative, KAUST, Thuwal, Saudi Arabia

{aleksandar, anand, juergen}@idsia.ch kirie@fas.harvard.edu

{aleksandar, anand, juergen}@idsia.ch kirie@fas.harvard.edu

###### Abstract

Current state-of-the-art object-centric models use slots and attention-based routing for binding. However, this class of models has several conceptual limitations: the number of slots is hardwired; all slots have equal capacity; training has high computational cost; there are no object-level relational factors within slots. Synchrony-based models in principle can address these limitations by using complex-valued activations which store binding information in their phase components. However, working examples of such synchrony-based models have been developed only very recently, and are still limited to toy grayscale datasets and simultaneous storage of less than three objects in practice. Here we introduce architectural modifications and a novel contrastive learning method that greatly improve the state-of-the-art synchrony-based model. For the first time, we obtain a class of synchrony-based models capable of discovering objects in an unsupervised manner in multi-object color datasets and simultaneously representing more than three objects.1

## 1 Introduction

The visual binding problem [1; 2] is of great importance to human perception  and cognition [4; 5; 6]. It is fundamental to our visual capabilities to integrate several features together such as color, shape, texture, etc., into a unified whole . In recent years, there has been a growing interest in deep learning models [8; 9; 10; 11; 12; 13; 14; 15; 16; 17; 18; 19] capable of grouping visual inputs into a set of'meaningful' entities in a fully unsupervised fashion (often called object-centric learning). Such compositional object-centric representations facilitate relational reasoning and generalization, thereby leading to better performance on downstream tasks such as visual question-answering [20; 21], video game playing [22; 23; 24; 25; 26], and robotics [27; 28; 29], compared to other monolithic representations of the visual input.

The current mainstream approach to implement such object binding mechanisms in artificial neural networks is to maintain a set of separate activation vectors (so-called _slots_) . Various segregation mechanisms  are then used to route requisite information from the inputs to infer each slot in a iterative fashion. Such slot-based approaches have several conceptual limitations. First, the binding information (i.e., addresses) about object instances are maintained only by the constant number of slots--a hard-wired component which cannot be adapted through learning. This restricts the ability of slot-based models to flexibly represent varying number of objects with variable precision without tuning the slot size, number of slots, number of iterations, etc. Second, the inductive bias used by the grouping module strongly enforces independence among all pairs of slots. This restricts individualslots to store relational features at the object-level, and requires additional processing of slots using a relational module, e.g., Graph Neural Networks [31; 32] or Transformer models [33; 34; 35]. Third, binding based on iterative attention is in general computationally very demanding to train . Additionally, the spatial broadcast decoder  (a necessary component in these models) requires multiple forward/backward passes to render the slot-wise reconstruction and alpha masks, resulting in a large memory overhead as well.

Recently, Lowe et al.  revived another class of neural object binding models [38; 39; 40] (_synchrony-based_ models) which are based on complex-valued neural networks. Synchrony-based models are conceptually very promising. In principle they address most of the conceptual challenges faced by slot-based models. The binding mechanism is implemented via constructive or destructive phase interference caused by addition of complex-valued activations. They store and process information about object instances in the phases of complex activations which are more amenable to adaptation through gradient-based learning. Further, they can in principle store a variable number of objects with variable precision by partitioning the phase components of complex activations at varying levels of granularity. Additionally, synchrony-based models can represent relational information directly in their distributed representation, i.e., distance in phase space yields an implicit relational metric between object instances (e.g., inferring part-whole hierarchy from distance in "tag" space ). Lastly, the training of synchrony-based models is computationally more efficient by two orders of magnitude .

However, the true potential of synchrony-based models for object binding is yet to be explored; the current state-of-the-art synchrony-based model, the Complex-valued AutoEncoder (CAE) , still has several limitations. First, it is yet to be benchmarked on any multi-object datasets  with color images (even simplisitic ones like Tetrominoes) due to limitations in the evaluation method to extract discrete object identities from continuous phase maps . Second, we empirically observe that it shows low _separability_ (Table 2) in the phase space, thereby leading to very poor (near chance-level) grouping performance on dSprites and CLEVR. Lastly, CAE can simultaneously represent at most 3 objects , making it infeasible for harder benchmark datasets [41; 42].

Our goal is to improve the state-of-art synchrony models by addressing these limitations of CAE . First, we propose a few simple architectural changes to the CAE: i) remove the 1x1 convolution kernel as well as the sigmoid activation in the output layer of decoder, and ii) use convolution and upsample layers instead of transposed convolution in the decoder. These changes enable our improved CAE, which we call CAE++, to achieve good grouping performance on the Tetrominoes dataset--a task on which the original CAE completely fails. Further, we introduce a novel contrastive learning method to increase _separability_ in phase values of pixels (regions) belonging to two different objects. The resulting model, which we call Contratively Trained Complex-valued AutoEncoders (CtCAE), is the first kind of synchrony-based object binding models to achieve good grouping performance on multi-object color datasets with more than three objects (Figure 2). Our contrastive learning method yields significant gains in grouping performance over CAE++, consistently across three multi-object color datasets (Tetrominoes, dSprites and CLEVR). Finally, we qualitatively and quantitatively evaluate the _separability_ in phase space and generalization of CtCAE w.r.t. number of objects seen at train/test time.

## 2 Background

We briefly overview the CAE architecture  which forms the basis of our proposed models. CAE performs binding through complex-valued activations which transmit two types of messages: _magnitudes_ of complex activations to represent the strength of a feature and _phases_ to represent which features must be processed together. The constructive or destructive interference through addition of complex activations in every layer pressurizes the network to use similar phase values for all patches belonging to the same object while separating those associated with different objects. Patches of the same object contain a high amount of pointwise mutual information so their destructive interference would degrade its reconstruction.

The CAE is an autoencoder with _real-valued_ weights that manipulate _complex-valued_ activations. Let \(h\) and \(w\) denote positive integers. The input is a positive real-valued image \(^{}^{h w 3}\) (height \(h\) and width \(w\), with \(3\) channels for color images). An artificial initial phase of zero is added to each pixel of \(^{}\) (i.e., \(=^{h w 3}\)) to obtain a complex-valued input \(^{h w 3}\):

\[=^{} e^{i},\] (1)Let \(d_{}\), \(d_{}\) and \(p\) denote positive integers. Every layer in the CAE transforms complex-valued input \(^{d_{}}\) to complex-valued output \(^{d_{}}\) (where we simply denote input/output sizes as \(d_{}\) and \(d_{}\) which typically have multiple dimensions, e.g., \(h w 3\) for the input layer), using a function \(f_{}:^{d_{}}^{d_{}}\) with real-valued trainable parameters \(^{p}\). \(f_{}\) is typically a convolutional or linear layer. First, \(f_{}\) is applied separately to the real and imaginary components of the input:

\[=f_{}(())+f_{}(())i^{d_{}}\] (2)

Note that both \((),()^{d_{}}\). Second, separate trainable bias vectors \(_{},_{}^{d_{ {out}}}\) are applied to the magnitude and phase components of \(^{d_{}}\):

\[_{}=||+_{ }^{d_{}};_{ }=()+_{}^{d_{}}\] (3)

Third, the CAE uses an additional gating function proposed by Reichert and Serre  to further transform this "intermediate" magnitude \(_{}^{d_{}}\). This gating function dampens the response of an output unit as a function of the phase difference between two inputs. It is designed such that the corresponding response curve approximates experimental recordings of the analogous curve from a Hodgkin-Huxley model of a biological neuron . Concretely, an intermediate activation vector \(^{d_{}}\) (called _classic term_) is computed by applying \(f_{}\) to the magnitude of the input \(^{d_{}}\), and a convex combination of this _classic term_ and the magnitude \(_{}\) (called _synchrony term_) from Eq. 3 is computed to yield "gated magnitudes" \(_{}^{d_{}}\) as follows:

\[=f_{}(||)+_{}^{d_{}};_{ }=_{}+^{d_{}}\] (4)

Finally, the output of the layer \(^{d_{}}\) is obtained by applying non-linearities to this magnitude \(_{}\) (Eq. 4) while leaving the phase values \(_{}\) (Eq. 3) untouched:

\[=((_{})) e^{i _{}}^{d_{}}\] (5)

The ReLU activation ensures that the magnitude of \(\) is positive, and any phase flips are prevented by its application solely to the magnitude component \(_{}\). For more details of the CAE  and gating function  we refer the readers to the respective papers. The final object grouping in CAE is obtained through K-means clustering based on the phases at the output of the decoder; each pixel is assigned to a cluster corresponding to an object .

## 3 Method

We describe the architectural and contrastive training details used by our proposed CAE++ and CtCAE models respectively below.

CAE++.We first propose some simple but crucial architectural modifications that enable the vanilla CAE  to achieve good grouping performance on multi-object datasets such as Tetrominoes with color images. These architectural modifications include -- i) Remove the 1x1 convolution kernel and associated sigmoid activation in the output layer ("\(f_{}\)" in Lowe et al. ) of the decoder, ii) Use convolution and upsample layers in place of transposed convolution layers in the decoder (cf. "\(f_{}\)" architecture in Table 3 from Lowe et al. ). We term this improved CAE variant that adopts these architectural modifications as CAE++. As we will show below in Table 1, these modifications allow our CAE++ to consistently outperform the CAE across all 3 multi-object datasets with color images.

Contrastive Training of CAEs.Despite the improved grouping of CAE++ compared to CAE, we still empirically observe that CAE++ shows poor _separability_2 (we also illustrate this in Section 4). This motivates us to introduce an auxiliary training objective that explicitly encourages higher _separability_ in phase space. For that, we propose a contrastive learning method [43; 44] that modulates the distance between pairs of distributed representations based on some notion of (dis)similarity between them (which we define below). This design reflects the desired behavior to drive the phase separation process between two different objects thereby facilitating better grouping performance.

Before describing our contrastive learning method mathematically below, here we explain its essential ingredients (illustrated in Figure 1). For setting up the contrastive objective, we first (randomly)sample "anchors" from a set of "datapoints". The main idea of contrastive learning is to "contrast" these anchors to their respective "positive" and "negative" examples in a certain representation space. This requires us to define two representation spaces: one associated with the similarity measure to define positive/negative examples given an anchor, and another one on which we effectively apply the contrastive objective, itself defined as a certain distance function to be minimized. We use the term _addresses_ to refer to the representations used to measure similarity, and consequently extract positive and negative pairs w.r.t. to the anchor. We use the term _features_ to refer to the representations that are contrasted. As outlined earlier, the goal of the contrastive objective is to facilitate _separability_ of phase values. It is then a natural choice to use the phase components of complex-valued outputs as _features_ and the magnitude components as _addresses_ in the contrastive loss. This results in angular distance between phases (_features_) being modulated by the contrastive objective based on how (dis)similar their corresponding magnitude components (_addresses_) are. Since the magnitude components of complex-valued activations are used to reconstruct the image (Equation (7)), they capture requisite visual properties of objects. In short, the contrastive objective increases or decreases the angular distance of phase components of points (pixels/image regions) in relation to how (dis)similar their visual properties are.

Our contrastive learning method works as follows. Let \(h^{}\), \(w^{}\), \(d_{}\), \(N_{A}\), and \(M\) denote positive integers. Here we generically denote the dimension of the output of any CAE layer as \(h^{} w^{} d_{}\). This results in a set of \(h^{} w^{}\) "datapoints" of dimension \(d_{}\) for our contrastive learning. From this set of datapoints, we randomly sample \(N_{A}\) anchors. We denote this set of anchors as a matrix \(^{N_{A} d_{}}\); each anchor is thus denoted as \(_{k}^{d_{}}\) for all \(k\{1,...,N_{A}\}\). Now by using

Figure 1: Sampling process of positive (green) and negative (red) pairs for one anchor (purple) in the CICAE model. The sampling process here is visualized only for the decoder output. Note that we contrast the encoder output in an identical manner. Anchor address (the purple box marked with an X) corresponds to the patch of magnitude values and the feature corresponds to the phase values. See **Contrastive Training of CAEs** in Section 3 for more details.

Algorithm 1, we extract \(1\) positive and \(M-1\) negative examples for each anchor. We denote these examples by a matrix \(^{k}^{M d_{l}}\) for each anchor \(k\{1,...,N_{A}\}\) arranged such that \(^{k}_{1}^{d_{l}}\) is the positive example and all other rows \(^{k}_{j}^{d_{l}}\) for \(j\{2,...,M\}\) are negative ones. Finally, our contrastive loss is an adaptation of the standard InfoNCE loss  which is defined as follows:

\[_{}=}_{k=1}^{N_{A}}( {(d(_{k};^{k}_{1})/)}{_{ j=1}^{M}(d(_{k};^{k}_{j})/)})\] (6)

where \(N_{A}\) is the number of anchors sampled for each input, \(d(_{k};_{l})\) refers to the cosine distance between a pair of vectors \(_{k},_{l}^{d_{f}}\) and \(_{>0}\) is the softmax temperature.

We empirically observe that applying the contrastive loss on outputs of both encoder and decoder is better than applying it on only either one (Table 5). We hypothesize that this is the case because it utilizes both high-level, abstract and global features (on the encoder-side) as well as low-level and local visual cues (on the decoder-side) that better capture visual (dis)similarity between positive and negative pairs. We also observe that using magnitude components of complex outputs of both the encoder and decoder as _addresses_ for mining positive and negative pairs while using the phase components of complex-valued outputs as the _features_ for the contrastive loss performs the best among all the other possible alternatives (Table 13). These ablations also support our initial intuitions (described above) while designing the contrastive objective for improving _separability_ in phase space.

Finally, the complete training objective function of CtCAE is:

\[=_{}+_{}; _{}=||^{}-}||^{2}_{2} ;}=||\] (7)

where \(\) defines the loss for a single input image \(^{}^{h w 3}\), and \(_{}\) is the standard reconstruction loss used by the CAE . The reconstructed image \(}^{h w 3}\) is generated from the complex-valued outputs of the decoder \(^{h w 3}\) by using its magnitude component. In practice, we train all models by minimizing the training loss \(\) over a batch of images. The CAE baseline model and our proposed CAE++ variant are trained using only the reconstruction objective (i.e. \(=0\)) whereas our proposed CtCAE model is trained using the complete training objective.

## 4 Results

Here we provide our experimental results. We first describe details of the datasets, baseline models, training procedure and evaluation metrics. We then show results (always across 5 seeds) on grouping of our CtCAE model compared to the baselines (CAE and our variant CAE++), _separability_ in phase space, generalization capabilities w.r.t to number of objects seen at train/test time and ablation studies for each of our design choices. Finally, we comment on the limitations of our proposed method.

Figure 2: Unsupervised object discovery on \(\), \(\) and \(\) with CtCAE. “Phase Rad.” (col. 5) is the radial plot with the phase values from -\(\) to \(\) radians. “Phase” (col. 6), are phase values (in radians) averaged over the 3 output channels as a heatmap (colors correspond to those from “Phase Rad.”) and “Magnitude” (col. 7) is the magnitude component of the outputs.

Datasets.We evaluate the models on three datasets from the Multi-Object datasets suite  namely Tetrominoes, dSprites and CLEVR (Figure 2) used by prior work in object-centric learning . For CLEVR, we use the filtered version  which consists of images containing less than seven objects. For the main evaluation, we use the same image resolution as Emami et al. , i.e., 32x32 for Tetrominoes, 64x64 for dSprites and 96x96 for CLEVR (a center crop of 192x192 that is then resized to 96x96). For computational reasons, we perform all ablations and analysis on 32x32 resolution. Performance of all models are ordered in the same way on 32x32 resolution as the original resolution (see Table 1), but with significant training and evaluation speed up. In Tetrominoes and dSprites the number of training images is 60K whereas in CLEVR it is 50K. All three datasets have 320 test images on which we report all the evaluation metrics. For more details about the datasets and preprocessing, please refer to Appendix A.

Models & Training Details.We compare our CtCAE model to the state-of-the-art synchrony-based method for unsupervised object discovery (CAE ) as well as to our own improved version thereof (CAE++) introduced in Section 3. For more details about the encoder and decoder architecture of all models see Appendix A. We use the same architecture as CAE , except with increased number of convolution channels (same across all models). We train models for 50K steps on Tetrominoes, and 100K steps on dSprites and CLEVR with Adam optimizer  with a constant learning rate of 4e-4, i.e., no warmup schedules or annealing (all hyperparameter details are given in Appendix A).

Evaluation Metrics.We use the same evaluation protocol as prior work  which compares the grouping performance of models using the Adjusted Rand Index (ARI) . We report two variants of the ARI score, i.e., ARI-FG and ARI-FULL consistent with Lowe et al. . ARI-FG measures the ARI score only for the foreground and ARI-FULL takes into account all pixels.

Unsupervised Object Discovery.Table 1 shows the performance of our CAE++, CtCAE, and the baseline CAE  on Tetrominoes, dSprites and CLEVR. We first observe that the CAE baseline almost completely fails on all datasets as shown by its very low ARI-FG and ARI-FULL scores. The MSE values in Table 1 indicate that CAE even struggles to reconstruct these color

   Dataset & Model & MSE \(\) & ARI-FG \(\) & ARI-FULL \(\) \\  Tetrominoes & CAE & 4.57e-2 \(\) 1.08e-3 & 0.00 \(\) 0.00 & 0.12 \(\) 0.02 \\ (32x32) & CAE++ & 5.07e-5 \(\) 2.80e-5 & 0.78 \(\) 0.07 & 0.84 \(\) 0.01 \\  & CtCAE & 9.73e-5 \(\) 4.64e-5 & **0.84 \(\) 0.09** & **0.85 \(\) 0.01** \\  & SlotAttention & – & 0.99 \(\) 0.00 & – \\  dSprites & CAE & 8.16e-3 \(\) 2.54e-5 & 0.05 \(\) 0.02 & 0.10 \(\) 0.02 \\ (64x64) & CAE++ & 1.60e-3 \(\) 1.33e-3 & 0.51 \(\) 0.08 & 0.54 \(\) 0.14 \\  & CtCAE & 1.56e-3 \(\) 1.58e-4 & **0.56 \(\) 0.11** & **0.90 \(\) 0.03** \\  & SlotAttention & – & 0.91 \(\) 0.01 & – \\  CLEVR & CAE & 1.50e-3 \(\) 4.53e-4 & 0.04 \(\) 0.03 & 0.18 \(\) 0.06 \\ (96x96) & CAE++ & 2.41e-4 \(\) 3.45e-5 & 0.27 \(\) 0.13 & 0.31 \(\) 0.07 \\  & CtCAE & 3.39e-4 \(\) 3.65e-5 & **0.54 \(\) 0.02** & **0.68 \(\) 0.08** \\  & SlotAttention & – & 0.99 \(\) 0.01 & – \\  dSprites & CAE & 7.24e-3 \(\) 8.45e-5 & 0.01 \(\) 0.00 & 0.05 \(\) 0.00 \\ (32x32) & CAE++ & 8.67e-4 \(\) 1.92e-4 & 0.38 \(\) 0.05 & 0.49 \(\) 0.15 \\  & CtCAE & 1.10e-3 \(\) 2.59e-4 & **0.48 \(\) 0.03** & **0.68 \(\) 0.13** \\  CLEVR & CAE & 1.84e-3 \(\) 5.68e-4 & 0.11 \(\) 0.07 & 0.12 \(\) 0.11 \\ (32x32) & CAE++ & 4.04e-4 \(\) 4.04e-4 & 0.22 \(\) 0.10 & 0.30 \(\) 0.18 \\  & CtCAE & 9.88e-4 \(\) 1.42e-3 & **0.50 \(\) 0.05** & **0.69 \(\) 0.25** \\   

Table 1: MSE and ARI scores (mean \(\) standard deviation across 5 seeds) for CAE, CAE++ and CtCAE models for Tetrominoes, dSprites and CLEVR on their respective full resolutions. For all datasets, CtCAE vastly outperforms CAE++ which in turn outperforms the CAE baseline. Results for 32x32 dSprites and CLEVR are also provided, these follow closely the scores on the full resolutions. SlotAttention results are from Emami et al. .

images. In contrast, CAE++ achieves significantly higher ARI scores, consistently across all three datasets; this demonstrates the impact of the architectural modifications we propose. However, on the most challenging CLEVR dataset, CAE++ still achieves relatively low ARI scores. Its contrastive learning-augmented counterpart, CtCAE consistently outperforms CAE++ both in terms of ARI-FG and ARI-FULL metrics. Notably, CtCAE achieves more than double the ARI scores of CAE++ on the most challenging CLEVR dataset which highlights the benefits of our contrastive method. All these results demonstrate that CtCAE is capable of object discovery (still far from perfect) on all datasets which include color images and more than three objects per scene, unlike the exisiting state-of-the-art synchrony-based model, CAE.

Quantitative Evaluation of _Separability._To gain further insights into why our contrastive method is beneficial, we quantitatively analyse the phase maps using two distance metrics: _inter-cluster_ and _intra-cluster_ distances. In fact, in all CAE-family of models, final object grouping is obtained through K-means clustering based on the phases at the output of the decoder; each pixel is assigned to a cluster with the corresponding centroid. _Inter-_cluster distance measures the Euclidean distance between centroids of each pair of clusters averaged over all such pairs. Larger inter-cluster distance allows for easier discriminability during clustering to obtain object assignments from phase maps. On the other hand, _intra-_cluster distance quantifies the "concentration" of points within a cluster, and is computed as the average Euclidean distance between each point in the cluster and the cluster centroid. Smaller intra-cluster distance results in an easier clustering task as the clusters are then more condensed. We compute these distance metrics on a per-image basis before averaging over all samples in the dataset. The results in Table 2 show that, the mean intra-cluster distance (last column) is smaller for CtCAE than CAE++ on two (dSprites and CLEVR) of the three datasets. Also, even though the average inter-cluster distance (fourth column) is sometimes higher for CAE++, the _minimum_ inter-cluster distance (third column)--which is a more relevant metric for separability--is

   Dataset & Model & Inter-cluster (min) \(\) & Inter-cluster (mean) \(\) & Intra-cluster \(\) \\   & CAE++ & 0.14 \(\) 0.00 & 0.30 \(\) 0.02 & 0.022 \(\) 0.010 \\  & CtCAE & **0.15 \(\) 0.01** & **0.31 \(\) 0.03** & **0.020 \(\) 0.010** \\   & CAE++ & **0.13 \(\) 0.05** & **0.51 \(\) 0.05** & 0.034 \(\) 0.007 \\  & CtCAE & **0.13 \(\) 0.03** & 0.39 \(\) 0.10 & **0.027 \(\) 0.009** \\   & CAE++ & 0.10 \(\) 0.06 & **0.53 \(\) 0.15** & 0.033 \(\) 0.013 \\  & CtCAE & **0.12 \(\) 0.05** & 0.50 \(\) 0.12 & **0.024 \(\) 0.005** \\   

Table 2: Quantifying the _Separability_ through inter- and intra-cluster metrics of the phase space. For the inter-cluster metric, we report both the minimum and mean across clusters.

Figure 3: CAE, CAE++ and CtCAE comparison on Tetrominoes (columns 1-3), dSprites (columns 4-6) and CLEVR (columns 7-9). First row: ground truth masks and input images.

larger for CtCAE. This confirms that compared to CAE++, CtCAE tends to have better phase map properties for object grouping, as is originally motivated by our contrastive method.

Object Storage Capacity.Lowe et al.  note that the performance of CAE sharply decreases for images with more than \(3\) objects. We report the performance of CtCAE and CAE++ on subsets of the test set split by the number of objects, to measure how their grouping performance changes w.r.t. the number of objects. In dSprites the images contain \(2,3,4\) or \(5\) objects, in CLEVR \(3,4,5\) or \(6\) objects. Note that the models are trained on the entire training split of the respective datasets. Table 3 shows that both methods perform well on images containing more than \(3\) objects (their performance does not drop much on images with \(4\) or more objects). We also observe that the CtCAE consistently maintains a significant lead over CAE in terms of ARI scores across different numbers of objects. In another set of experiments (see Appendix B), we also show that CtCAE generalizes well to more objects (e.g. \(5\) or \(6\)) when trained only on a subset of images containing less than this number of objects.

Ablation on Architectural Modifications.Table 4 shows an ablation study on the proposed architectural modifications on Tetrominoes (for similar findings on other datasets, see Appendix B.2). We observe that the sigmoid activation on the output layer of the decoder significantly impedes learning on color datasets. A significant performance jump is also observed when replacing transposed convolution layers  with convolution and upsample layers. By applying all these modifications, we obtain our CAE++ model that results in significantly better ARI scores on all datasets, therefore supporting our design choices.

Ablation on Feature Layers to Contrast.In CtCAE we contrast feature vectors both at the output of the encoder and the output of the decoder. Table 5 justifies this choice; this default setting (Enc+Dec) outperforms both other options where we apply the constrastive loss either only in the

   Model & ARI-FG \(\) & ARI-FULL \(\) \\  CAE & 0.00 \(\) 0.00 & 0.12 \(\) 0.02 \\ CAE-(\(f_{}\) 1x1 conv) & 0.00 \(\) 0.00 & 0.00 \(\) 0.00 \\ CAE-(\(f_{}\) sigmoid) & 0.12 \(\) 0.12 & 0.35 \(\) 0.36 \\ CAE-transp.+upsamp. & 0.10 \(\) 0.21 & 0.10 \(\) 0.22 \\ CAE++ & 0.78 \(\) 0.07 & 0.84 \(\) 0.01 \\ CtCAE & 0.84 \(\) 0.09 & 0.85 \(\) 0.01 \\   

Table 4: Architectural ablations on Tetrominoes.

Figure 4: CtCAE on CLEVR is able to infer more than four objects, although it sometimes makes mistakes, such as specular effects or grouping together based on color (two yellow objects).

    & ARI-FG & ARI-FULL & ARI-FG & ARI-FULL & ARI-FG & ARI-FULL & ARI-FG & ARI-FULL \\  dSprites &  &  &  &  \\  CAE++ & 0.29 \(\) 0.07 & 0.58 \(\) 0.15 & 0.39 \(\) 0.06 & 0.53 \(\) 0.15 & 0.43 \(\) 0.04 & 0.45 \(\) 0.13 & 0.45 \(\) 0.05 & 0.44 \(\) 0.11 \\ CtCAE & **0.45 \(\) 0.11** & **0.76 \(\) 0.07** & **0.48 \(\) 0.06** & **0.72 \(\) 0.11** & **0.48 \(\) 0.04** & **0.65 \(\) 0.13** & **0.48 \(\) 0.04** & **0.61 \(\) 0.13** \\  CLEVR &  &  &  &  \\  CAE++ & 0.32 \(\) 0.06 & 0.35 \(\) 0.29 & 0.33 \(\) 0.05 & 0.32 \(\) 0.26 & 0.31 \(\) 0.04 & 0.26 \(\) 0.20 & 0.32 \(\) 0.04 & 0.27 \(\) 0.18 \\ CtCAE & **0.53 \(\) 0.07** & **0.71 \(\) 0.20** & **0.52 \(\) 0.06** & **0.70 \(\) 0.21** & **0.47 \(\) 0.07** & **0.67 \(\) 0.21** & **0.45 \(\) 0.06** & **0.68 \(\) 0.20** \\   

Table 3: Object storage capacity: training on the full dSprites dataset and evaluating separately on subsets containing images with 2, 3, 4 or 5 objects. Analogously, training on the full CLEVR dataset and evaluating separately on subsets containing images with 3, 4, 5 or 6 objects.

encoder or the decoder output. We hypothesize that this is because these two contrastive strategies are complementary: one uses low-level cues (dec) and the other high-level abstract features (enc).

Qualitative Evaluation of Grouping.Finally, we conduct some qualitative analyses of both successful and failed grouping modes shown by CAE++ and CtCAE models through visual inspection of representative samples. In Figure 3, Tetrominoes (columns 1-2), we observe that CtCAE (row 4) exhibits better grouping on scenes with multiple objects of the same color than CAE++ (row 3). This is reflected in the radial phase plots (column 2) which show better _separability_ for CtCAE than CAE++. Further, on dSprites (rows 3-4, columns 4-5) and CLEVR (rows 3-4, columns 7-8), CtCAE handles the increased number of objects more gracefully while CAE++ struggles and groups several of them together. For the failure cases, Figure 4 shows an example where CCCAE still has some difficulties in segregating objects of the same color (row 2, yellow cube and ball) (also observed sometimes on dSprites, see Figure 14). Further, we observe how the _specular highlights_ on metallic objects (purple ball in row 1 and yellow ball in row 2) form a separate sub-part from the object (additional grouping examples in Appendix C).

Discussion on the Evaluation Protocol.The reviewers raised some concerns about the validity of our evaluation protocol3. The main point of contention was that the thresholding applied before clustering the phase values may lead to a trivial separation of objects in RGB images based on their color. While it is true that in our case certain color information may contribute to the separation process, a trivial separation of objects based on solely their color cannot happen since CtCAE largely outperforms CAE++ (see Table 1) despite both having near perfect reconstruction losses and with the same evaluation protocol. This indicates that this potential issue only has a marginal effect in practice in our case and separation in phase space learned by the model is still crucial. In fact, "pure" RGB color tones (which allow such trivial separation) rarely occur in the datasets (dSprites and CLEVR) used here. The percentage of pixels with a magnitude less than 0.1 are \(0.44\%\), \(0.88\%\) and \(9.25\%\) in CLEVR, dSprites and Tetrominoes respectively. While \(9.25\%\) in Tetrominoes is not marginal, we observe that this does not pose an issue in practice, as many examples in Tetrominoes where two or three blocks with the same color are separated by CAE++/CtCAE (e.g. three yellow blocks in Figure 3 or two magenta blocks in Figure 8).

To support our argument that using a threshold of 0.1 has no effect on the findings and conclusions, we conduct additional evaluations with a threshold of 0 and no threshold at all. From Table 9 (see Appendix B) we can see that using a threshold of 0 hardly change the ARI scores at all or in some cases it even improves the scores slightly. Using no threshold results in marginally improved ARI scores on CLEVR, near-identical scores on dSprites but worse scores on Tetrominoes (CtCAE still widely outperforms CAE++). However, this is not due to a drawback in the evaluation or our model. Instead, it stems from an inherent difficulty with estimating the phase of a zero-magnitude complex number. The evaluation clusters pixels based on their phase values, and if a complex number has a magnitude of exactly zero (or very small, e.g., order of \(1e^{-5}\)), the phase estimation is ill-defined and will inherently be random as was also previously noted by Lowe et al. . In fact, filtering those pixels in the evaluation is crucial in their work  as they largely used datasets with pure black background (exact zero pixel value). Evaluation of discrete object assignments from continuous phase outputs in a model-agnostic way remains an open research question.

## 5 Related Work

Slot-based binding.A wide range of unsupervised models have been introduced to perform perceptual grouping summarized well by Greff et al. . They categorize models based on the segregation (routing) mechanism used to break the symmetry in representations and infer latent representations (i.e. slots). Models that use "instance slots" cast the routing problem as inference in a mixture model whose solution is given by amortized variational inference [8; 14], Expectation-Maximization  or other approximations (Soft K-means) thereof . While others [9; 12; 49] that use "sequential slots" solve the routing problem by imposing an ordering across time. These

   Model & ARI-FG \(\) & ARI-FULL \(\) \\  Enc-only & 0.21 \(\) 0.11 & 0.29 \(\) 0.15 \\ Dec-only & 0.38 \(\) 0.17 & 0.69 \(\) 0.18 \\ Enc+Dec & 0.50 \(\) 0.05 & 0.69 \(\) 0.25 \\   

Table 5: Contrastive loss ablation for models use recurrent neural networks and an attention mechanism to route information about a different object into the same slot at every timestep. Some models [13; 17] combine the above strategies and use recurrent attention only for routing but not for inferring slots. Other models break the representational symmetry based on spatial coordinates [50; 16] ("spatial slots") or based on specific object types  ("category slots"). All these models still maintain the "separation" of representations only at one latent layer (slot-level) but continue to "entangle" them at other layers.

Synchrony-based binding.Synchrony-based models use complex-valued activations to implement binding by relying on their constructive or destructive phase interference phenomena. This class of models have been sparsely explored with only few prior works that implement this conceptual design for object binding [38; 39; 40; 36]. These methods differ based on whether they employ both complex-valued weights and activations [38; 39] or complex-valued activations with real-valued weights and a gating mechanism [36; 40]. They also differ in their reliance on explicit supervision for grouping  or not [39; 40; 36]. Synchrony-based models in contrast to slot-based ones maintain the "separation" of representations throughout the network in the phase components of their complex-valued activations. However, none of these prior methods can group objects in color images with up to \(6\) objects or visual realism of multi-object benchmarks in a fully unsupervised manner unlike ours. Concurrent work  extends CAE by introducing new feature dimensions ("rotating features"; RF) to the complex-valued activations. However, RF cannot be directly compared to CtCAE as it relies on either depth masks to get instance grouping on simple colored Shapes or features from powerful vision backbones (DINO ) to get largely semantic grouping on real-world images such as multiple instances of cows/trains or bicycles in their Figures 2 and 20 respectively.

Binding in the brain.The temporal correlation hypothesis posits that the mammalian brain binds together information emitted from groups of neurons that fire synchronously. According to this theory [54; 55], biological neurons transmit information in two ways through their spikes. The spike amplitude indicates the strength of presence of a feature while relative time between spikes indicates which neuronal responses need to bound together during further processing. It also suggests candidate rhythmic cycles in the brain such as Gamma that could play this role in binding [56; 57]. Synchrony-based models functionally implement the same coding scheme [58; 59; 60] using complex-valued activations where the relative phase plays the role of relative time between spikes. This abstracts away all aspects of the spiking neuron model to allow easier reproduction on digital hardware.

Contrastive learning with object-centric models.Contrastive learning for object-centric representations has not been extensively explored, with a few notable exceptions. The first method  works only on toy images of up to \(3\) objects on a black background while the second  shows results on more complex data, but requires complex hand-crafted data augmentation techniques to contrast samples across a batch. Our method samples positive and negative pairs _within_ a single image and does not require any data augmentation. Most importantly, unlike ours, these models still use slots and attention-based routing, and thereby inherit all of its conceptual limitations. Lastly, ODIN  alternately refines the segmentation masks and representation of objects using two networks that are jointly optimized by a contrastive objective that maximizes the similarity between different views of the same object, while minimizing the similarity between different objects. To obtain the segmentation masks for objects, they simply spatially group the features using K-means. Their method relies on careful data augmentation techniques such as local or global crops, viewpoint changes, color perturbations etc. which are applied to one object.

## 6 Conclusion

We propose several architectural improvements and a novel contrastive learning method to address limitations of the current state-of-the-art synchrony-based model for object binding, the complex-valued autoencoder (CAE ). Our improved architecture, CAE++, is the first synchrony-based model capable of dealing with color images (e.g., Tetrominoes). Our contrastive learning method further boosts CAE++ by improving its phase separation process. The resulting model, CtCAE, largely outperforms CAE++ on the rather challenging CLEVR and dSprites datasets. Admittedly, our synchrony-based models still lag behind the state-of-the-art _slot_-based models [15; 18], but this is to be expected, as research on modern synchrony-based models is still in its infancy. We hope our work will inspire the community to invest greater effort into such promising models.

Acknowledgments.We thank Sindy Lowe and Michael C. Mozer for insightful discussions and valuable feedback. This research was funded by Swiss National Science Foundation grant: 200021_192356, project NEUSYM and the ERC Advanced grant no: 742870, AlgoRNN. This work was also supported by a grant from the Swiss National Supercomputing Centre (CSCS) under project ID s1205 and d123. We also thank NVIDIA Corporation for donating DGX machines as part of the Pioneers of AI Research Award.