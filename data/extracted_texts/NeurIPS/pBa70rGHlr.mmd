# Latent Space Translation

via Semantic Alignment

Valentino Maiorca\({}^{1,}\) Luca Moschella\({}^{1,}\)1

Antonio Norelli\({}^{1}\) Marco Fumero\({}^{1}\) Francesco Locatello\({}^{2}\) Emanuele Rodola\({}^{1}\)

\({}^{1}\)Sapienza University of Rome

Equal contribution.

###### Abstract

While different neural models often exhibit latent spaces that are alike when exposed to semantically related data, this intrinsic similarity is not always immediately discernible. Towards a better understanding of this phenomenon, our work shows how representations learned from these neural modules can be translated between different pre-trained networks via simpler transformations than previously thought. An advantage of this approach is the ability to estimate these transformations using standard, well-understood algebraic procedures that have closed-form solutions. Our method directly estimates a transformation between two given latent spaces, thereby enabling effective stitching of encoders and decoders without additional training. We extensively validate the adaptability of this translation procedure in different experimental settings: across various trainings, domains, architectures (e.g., ResNet, CNN, ViT), and in multiple downstream tasks (classification, reconstruction). Notably, we show how it is possible to zero-shot stitch text encoders and vision decoders, or vice-versa, yielding surprisingly good classification performance in this multimodal setting.

## 1 Introduction

Representation learning  is a fundamental paradigm in the field of artificial intelligence, aimed at uncovering the underlying structure of complex data. One of the main goals of representation learning is to discover a robust representation of the data that is insensitive to certain transformations of the input. The Manifold Hypothesis  posits that real-world data lies on a low-dimensional non-linear manifold embedded in a high-dimensional space. Yet, a complication arises in modeling these non-linear manifolds: the learning process is usually influenced by stochasticities in the training dynamics and extrinsic factors that do not pertain to the data's core attributes, resulting in different representations for samples expected to be similar (e.g., different views of the same object, multiple translations of the same sentence, or even the exact same input sample). This is critical as it hinders knowledge transfer between these networks. Recently, the concept of relative representations  has been proposed as a method for zero-shot communication between latent spaces that is invariant to these extrinsic factors. The idea is that latent spaces of neural networks trained on comparable data can be projected into the same relative space, derived from the distances between the data points. One of the main contributions of relative encoding is that it shows how the signal encoded in the _angles_ with respect to a reduced set of data points (called _anchors_) is enough to capture the intrinsic shape of the latent space, reaching results on various benchmarks comparable to those using the original (absolute) encodings. As a consequence, they empirically demonstrate that different latent spaces that share the same data semantics (i.e., different representations of the same high-level concepts, such as images and their captions), mostly differ only by an angle-preserving transformation.

Building on this intuition of the existence of a relatively simple transformation, we show the effectiveness and applications of _directly translating between different latent spaces_ - provided that a partial (and possibly sparse) correspondence between data points is given. Remarkably, the process of seamlessly combining different neural networks, pre-trained on diverse datasets, modalities, architectures, domains and downstream tasks, proves unexpectedly straightforward. For instance, we show how it enables the ability to effectively integrate any pre-trained text encoder with any image classification head, and vice versa, without requiring any additional re-training or assumptions (indeed, Moschella et al. (2023) assumes the decoders are trained on relative representations). The method difference is emphasized in Figure 1. While zero-shot stitching with relative representations assumes the use of a single decoder specifically trained on a relative space, our method permits the reuse of the decoders originally trained on the absolute spaces.

Our main contributions can be summarized as follows:

* We explore the direct translation between latent spaces of distinct neural networks to enable _latent communication_. In particular, leveraging a semantic correspondence in the data, we directly translate for the first time across different trainings, architectures, and modalities. Notably, we obtain excellent stitching performances even in cross-modal settings, where we apply arbitrary text classifiers on top of pre-trained image encodings (and vice-versa).
* We show that different downstream tasks, namely classification and generation, require modeling different transformations to obtain the most out of the translation between their latent spaces;

## 2 Related Works

Representations similarityRecent years have witnessed a growing consensus among researchers in the deep learning community that effective neural networks tend to learn similar representations for semantically similar data, regardless of the architecture, task, or domain in which they are applied. This idea is supported by a plethora of empirical studies (Moschella et al., 2023; Norelli et al., 2023; Morcos et al., 2018; Li et al., 2016; Kornblith et al., 2019; Bonheme and Grzes, 2022; Tsitsulin et al., 2020; Barannikov et al., 2022; Vulic et al., 2020; Lample et al., 2018; Lenc and Vedaldi, 2015; Mikolov et al., 2013; Antonello et al., 2021; Bengio et al., 2012; Movshovitz-Attias et al., 2017; Chang et al., 2022) and the phenomenon is particularly pronounced for large and wide models (Sompeplali et al., 2022; Mehta et al., 2022). Nevertheless, despite this intrinsic similarity, latent spaces can still exhibit extrinsic variations. Our work analyzes the possibility of translating these spaces from one to another, linking these extrinsic variations to different classes of transformation.

Manifold alignmentProcrustes analysis has been instrumental in the alignment of latent spaces in deep neural networks (Wang and Mahadevan, 2008, 2009), particularly in Natural Language Processing (NLP) where it is well-known that latent spaces of different languages are isomorphic (Vulic et al., 2020) and can be effectively aligned (Mikolov et al., 2013; Xing et al., 2015). Rooted

Figure 1: Zero-shot stitching of \(\) and \(\) absolute spaces utilizing relative representations and our method (the estimation of \(\)). Our approach does not require a decoder specifically trained on relative representations (\(dec_{}\)). Instead, we directly translate latent spaces, enabling the use of arbitrarily pre-trained decoders originally trained on absolute spaces.

in shape analysis, this method efficiently uncovers correspondences between latent spaces of different models through the estimation of an optimal orthogonal transformation (Gower, 1975). Previous works largely exploit Procrustes analysis to align latent spaces originating from models of the same architecture (Csiszarik et al., 2021), such as multi-lingual FastText embeddings (Bojanowski et al., 2017; Smith et al., 2017). Instead, in this work, we extend the application of Procrustes analysis to network stitching in new domains, architectures, and even modalities for multiple downstream tasks.

Stitching and zero-shotModel stitching, which involves the combination of different neural networks to create a new model, has been a topic of active research in the field of representation learning. A key concept in this area is that of relative representations (Moschella et al., 2023; Norelli et al., 2023), which enables zero-shot stitching between different neural networks trained on semantically similar data. While this approach assumes the use of decoders _trained on relative representations_, our work removes this constraint by introducing a zero-shot mechanism for translating one absolute space to another without relying on a shared (relative) representation, enabling the stitching of arbitrarily trained models, further generalizable by assuming only the positive scale invariance of their decoder part. Previously, trainable stitching layers (Lenc and Vedaldi, 2015; Bansal et al., 2021; Csiszarik et al., 2021) have been introduced to allow for the combination of parts of different networks or to verify statements regarding latent space similarity. Other works (Gygli et al., 2021; Bansal et al., 2021; Yaman et al., 2022; an, 2020) have proposed alternative methods for producing directly compatible and reusable network components without specific stitching layers. Here, we sidestep the need to create a new compatible representation and instead focus on obtaining a direct transformation to map from a source space to a target one to enable seamless network stitching. Concurrently to this work, a similar approach by (Lahner and Moeller, 2023) also targeted the direct alignment of representational spaces, focusing on the compatibility of models trained end-to-end.

## 3 Method

### Preliminaries

Relative representation is a framework introduced in Moschella et al. (2023), which enables latent spaces of arbitrary neural models to communicate with each other. This is obtained by projecting the latent spaces into a common one, transitioning from an absolute coordinate frame to a **relative space**: each sample is represented as a function of a set of fixed samples denoted as **anchor set**. Specifically, the new representation is computed by independently projecting each sample point \(\) in the latent space \(^{n d}\), into the anchor set \(_{}\). Formally, this is represented as

\[_{rel}=_{abs}_{}^{T}\,,\] (1)

where \(_{rel}^{n k},_{abs}^{n  d}\) and \(_{}^{k d}\) Samples in \(\) and in \(\) are rescaled to unit norm, i.e. \(=}{\|\|_{2}}\ \) and \(=}{\|\|_{2}}\ _{ }\).

We assume to have access to subsets \(_{}\) and \(_{}\), with \(\) and \(\) being the data distributions, and that there exists a correspondence \(:_{}_{}\) between these two sets of _parallel anchors_. Parallel anchors act as a "Rosetta stone" (Norelli et al., 2023), meaning they establish a semantic correspondence between their respective spaces: an anchor sample in the first set represents the same high-level concept as its counterpart in the second set. This allows stitching together components of different models: i.e., merging independently trained encoder and decoder modules from different networks. The relative projection will map latent spaces into the same one as long as the core assumption that they differ by an angle-preserving transformation is satisfied. However, in

Figure 2: Method illustration on a synthetic example. Given a source space \(\), the steps to translate it to a target \(\) are sequentially applied as described in Section 3.2. Note that the translation is not perfect due to an arbitrary distortion of the data.

order to perform the stitching procedure in Moschella et al. (2023), decoders must be trained from scratch at least once to process samples in this shared relative space.

In this work, we overcome this need by substituting the costly retraining procedure with an efficient strategy to directly estimate the transformation necessary to map between spaces. Moreover, we relax the "angle-preserving" constraint by allowing for a broader class of transformations obtained via robust, closed-form algorithms.

### Latent Space Translation

Consider two latent spaces, \(^{n d_{1}}\) and \(^{n d_{2}}\). Our objective is to estimate the transformation \(\) that translates \(\) into \(\): \(=()\), exploiting the **semantic alignment** between the two spaces. Throughout this work, we identify two main steps in the translation process: pre-processing the spaces and estimating the transformation \(\), as outlined in Figure 2.

Pre-processingGenerally, the two spaces may have different dimensionalities - in those cases, we zero-pad the smaller one to match the dimension of the other without changing its underlying structure (Williams et al., 2021). Moreover, we standardize each feature in the encoding to have zero mean and unit variance (standard scaling) if not otherwise specified, whose statistics are computed only on the anchor sets for both source and target space, to perform the necessary de-normalization.

Estimating \(\)In Moschella et al. (2023), it is empirically shown that the spaces mostly differ by an angle-preserving transformation. Nevertheless, we broaden our investigation by considering different ways of obtaining \(\) to evaluate the robustness of that assumption and the versatility of our approach. Throughout our experiments, we primarily operate under the assumption that \(\) can be constrained to encode, at most, an affine transformation: \(()=+\)

This general formulation, without additional constraints, corresponds to our affine method in the experiments, and it is optimized via gradient descent. The other transformations are trivially obtained by progressively adding constraints on this one:

* linear. To model a linear transformation, we can just set the bias term to zero \(=\) and optimize via Least Square. Here we are both simplifying the class of transformations and switching from a gradient descent optimization to a closed-form procedure.
* l-ortho. Additionally, we could require \(R\) to be orthogonal to encode an isometry. In this case, we obtain this by applying Singular Value Decomposition (SVD) on the corresponding \(R\) obtained by the linear solution. Through this, we aim to understand the implications of enforcing orthogonality on a transformation that was originally not constrained to be so, in a setting similar to Xing et al. (2015).
* ortho. To obtain the optimal orthogonal \(R\), we apply Procrustes analysis (Gower, 1975).

This methodology facilitates efficient and precise zero-shot translation between disparate latent spaces. The transformation \(\), derived solely from the subset of corresponding points, provides a robust and versatile foundation for model reuse and interoperability in diverse machine learning contexts.

## 4 Latent Communication via Translation

In this section, we evaluate the capabilities and effectiveness of our translation method through various scenarios, highlighting its applicability in diverse contexts. We present empirical results in three different novel settings: i) cross-architecture; ii) cross-modality; iii) autoencoding. In each case, the translation performance of each method for obtaining the transformation \(\) is evaluated against two baselines, the naive absolute one and the relative one.

Stitching ProcedureIn line with the _zero-shot stitching_ concept outlined in Moschella et al. (2023), we combine independent encoders and decoders (e.g., classifiers, generators) without subsequent training or fine-tuning. This study does not necessitate a decoder trained on relative representations; instead, we directly employ the original decoders trained on absolute spaces. Each benchmark we perform follows the same procedure unless otherwise specified: we measure the mean performance over all the possible combinations of (encoder, decoder) for each test set in different settings:* _no-stitch_. The end-to-end performance of the decoder applied to the original space it was trained on. This is useful to establish an upper-bound in performances;
* _absolute_. The result of using the encodings without any transformation, we consider this as a probe for any pre-existing compatibility among encodings and, therefore, a lower-bound;
* _translation_. These are the results of the application of our latent translation method, with the estimation of \(\) via affine, linear, l-ortho and ortho.

In each instance, we use the same parallel anchors, that are uniformly chosen, in a quantity comparable with the dimensionality of the absolute representation.

### Cross-Architecture

Firstly, we test our method in a cross-architecture setting, zero-shot stitching together encodings coming from a variety of pre-trained networks and their associated absolute decoders (classifiers). This scenario provides an extensive testing ground for our method and demonstrates its robustness across different architectures. Please refer to Table 8 in the Appendix for further results on cross-architecture stitching in generation tasks.

Experimental settingWe consider a variety of Computer Vision (MNIST, Fashion MNIST, N24News, CIFAR10, CIFAR100) and Natural Language Processing (TREC [Hovy et al., 2001, Li and Roth, 2002], DBpedia [Auer et al., 2007], N24News [Wang et al., 2022], AG News [Zhang et al., 2015], IMDB [Maas et al., 2011] ) datasets. For the text domain we consider 7 different language models as encoders (uncased and cased BERT [Devlin et al., 2019], Electra [Clark et al., 2020], RoBERTa base [Liu et al., 2019], ALBERT [Lan et al., 2020], and the text encoder of [Radford et al., 2021]), and for the image domain 6 encoders (RexNet100 [Han et al., 2020], 4 variations of ViT [Dosovitskiy et al., 2020], and the image encoder of [Radford et al., 2021]), all pre-trained and frozen.

    & Dataset & Ro Stitching & absolute & relative & affine & linear & l-ortho & strho \\   & CIFAR10 & \(0.95 0.03\) & \(0.16 0.22\) & \(0.80 0.22\) & \(0.92 0.05\) & \(0.88 0.11\) & \(0.90 0.09\) & \(0.93 0.04\) \\  & CIFAR100+C & \(0.85 0.07\) & \(0.11 0.21\) & \(0.54 0.25\) & \(0.78 0.09\) & \(0.73 0.16\) & \(0.77 0.11\) & \(0.81 0.07\) \\  & CIFAR100+0 & \(0.76 0.09\) & \(0.07 0.21\) & \(0.30 0.24\) & \(0.68 0.11\) & \(0.62 0.19\) & \(0.64 0.16\) & \(0.71 0.09\) \\  & F-MNIST & \(0.88 0.01\) & \(0.15 0.20\) & \(0.63 0.23\) & \(0.86 0.01\) & \(0.83 0.09\) & \(0.82 0.05\) & \(0.85 0.02\) \\  & MNIST & \(0.96 0.01\) & \(0.15 0.21\) & \(0.50 0.22\) & \(0.94 0.01\) & \(0.89 0.08\) & \(0.81 0.11\) & \(0.91 0.02\) \\   & Take & \(0.87 0.12\) & \(0.20 0.06\) & \(0.36 0.13\) & \(0.82 0.12\) & \(0.74 0.25\) & \(0.57 0.25\) & \(0.79 0.11\) \\  & Mo News & \(0.73 0.09\) & \(0.25 0.02\) & \(0.39 0.13\) & \(0.65 0.08\) & \(0.62 0.08\) & \(0.61 0.10\) & \(0.66 0.10\) \\   & Opspedia & \(0.78 0.23\) & \(0.07 0.01\) & \(0.16 0.10\) & \(0.66 0.24\) & \(0.62 0.23\) & \(0.57 0.23\) & \(0.66 0.22\) \\   & IMDB & \(0.61 0.04\) & \(0.50 0.01\) & \(0.51 0.02\) & \(0.59 0.04\) & \(0.57 0.04\) & \(0.56 0.03\) & \(0.59 0.04\) \\   

Table 1: Cross-architecture stitching with various methods for estimating \(\) and applying standard scaling. The stitched decoders are SVMs with a linear kernel. 5 runs for each encoder-decoder pair. (C) and (F) next to CIFAR100 indicate, respectively, coarse-grained and fine-grained. Please refer to the Appendix in Table 5 for additional results with MLPs as classification heads.

Figure 3: Performance comparison of affine, linear, l-ortho, and ortho at varying number of anchors on classification accuracy. Results on CIFAR100 fine-grained. The same analysis for the generation case is in Figure 8 in the Appendix.

The full encoder list can be found in Table 7 in the Appendix. For each dataset and for each encoder, we train an SVM classification head (decoder) on top of their specific encodings. We then proceed with the standard stitching procedure outlined in Section 4 and collect the results. Please see Table 8 in the Appendix for cross-architecture stitching in generation tasks, where we extend this analysis by verifying that our method works even across autoencoders of different bottleneck sizes.

Result analysisThe stitching results are in Table 1. As expected, the _absolute_ encodings obtain a score comparable to random guessing while also considering fewer encoder combinations out of the possible ones due to the dimensionality mismatch between some of them. Notably, these results show that the transformation relating to these pre-trained encoders is indeed mostly orthogonal: i) ortho and affine, the narrowest and the broadest transformation classes considered, are the better-performing translation methods. But while the former is obtained via a simple and efficient closed-form algorithm, the latter is SGD-optimized (Section 3.2). ii) the l-ortho version improves or has small drops in performances over the linear transformation it is obtained from, confirming that the least squares procedure converges to an \(\) which is almost orthogonal. Note that these results demonstrate the feasibility of combining pre-trained models without the need for retraining or fine-tuning, with negligible drops in performances across the board and without any additional assumption on the decoders. Please refer to Tables 5 and 6 in the Appendix for results with different decoders. In the Appendix (Figure 7), we extend the cross-architecture transfer to decoders trained on different domains (styles) of the same CIFAR10 dataset: the original one and a grayscale one.

Sensibility to Anchor QuantityThe number of anchors is an essential parameter in our approach. In Figure 3, we evaluate how the quantity of these anchors impacts the residual error and the overall performance of our method for this experimental setting. This analysis offers insights into the optimal number of anchors necessary for efficient latent space translation.

Role of ScalingOur approach is designed to accommodate generic (re)scaling methods as pre-processing steps. We advocate for the use of standard scaling, as it shows reliable performance in our experiments, indicating that the scale of the data points is useful in estimating the latent transformation \(\). However, for completeness, we also consider L2 normalization, which is the standard normalization in relative representations. This normalization method generalizes the class of transformations handled by our method and introduces an element of complete scale invariance. It's important to note that when this level of generalization is introduced, a scale-invariant decoder is required since the norm information is effectively removed. In the relative representation work, this is implicitly accomplished by training a decoder on relative representations. In our setting, since we do not train the decoder, in this setting we just assume its scale invariance (more details in Appendix A.1). This investigation exemplifies the flexibility of our approach, capable of adapting to different normalization and pre-processing strategies based on the specific requirements of the task at hand. The results presented in Table 2, when compared with Table 1, indicate a stronger reliance of the text modalities on the information encoded in the norm. This is aligned with existing literature in the NLP domain (Oyama et al., 2022), which suggests that the scale of the encodings contains information (e.g., it is correlated with the token frequency).

These results in diverse scenarios showcase the flexibility and adaptability of our method, especially its robustness in translating between latent spaces of different dimensionality and domains.

    & Dataset & No Stitching & absolute & relative & affine & linear & l-ortho & ortho \\   & CIFAR10 & \(0.95 0.03\) & \(0.16 0.22\) & \(0.80 0.22\) & \(0.93 0.04\) & \(0.78 0.27\) & \(0.88 0.12\) & \(0.91 0.09\) \\  & CIFAR100-C & \(0.85 0.07\) & \(0.11 0.21\) & \(0.54 0.25\) & \(0.79 0.07\) & \(0.65 0.25\) & \(0.73 0.17\) & \(0.79 0.10\) \\  & CIFAR130-C & \(0.76 0.09\) & \(0.07 0.21\) & \(0.30 0.24\) & \(0.69 0.10\) & \(0.52 0.25\) & \(0.62 0.19\) & \(0.68 0.13\) \\  & F-MNIST & \(0.88 0.01\) & \(0.15 0.20\) & \(0.63 0.23\) & \(0.86 0.01\) & \(0.65 0.23\) & \(0.83 0.06\) & \(0.84 0.05\) \\  & MNIST & \(0.96 0.01\) & \(0.15 0.21\) & \(0.50 0.22\) & \(0.94 0.01\) & \(0.61 0.23\) & \(0.90 0.08\) & \(0.90 0.04\) \\   & TARC & \(0.87 0.12\) & \(0.20 0.06\) & \(0.36 0.13\) & \(0.82 0.12\) & \(0.44 0.20\) & \(0.74 0.23\) & \(0.77 0.12\) \\  & \(News}}\) & \(0.73 0.09\) & \(0.25 0.02\) & \(0.39 0.13\) & \(0.66 0.08\) & \(0.56 0.10\) & \(0.62 0.08\) & \(0.64 0.10\) \\   & Ospedia & \(0.78 0.23\) & \(0.07 0.01\) & \(0.16 0.10\) & \(0.66 0.24\) & \(0.44 0.20\) & \(0.62 0.23\) & \(0.60 0.22\) \\   & IMB & \(0.61 0.04\) & \(0.50 0.01\) & \(0.51 0.02\) & \(0.59 0.04\) & \(0.55 0.03\) & \(0.58 0.04\) & \(0.59 0.04\) \\   

Table 2: Cross-architecture stitching with various methods for estimating \(\) and applying L2 normalization. The stitched decoders are SVMs with linear kernel. 5 runs for each encoder-decoder pair. (C) and (F) next to CIFAR100 indicate, respectively, coarse-grained and fine-grained. Please refer to Table 6 in the Appendix for additional results with MLPs as classification heads.

### Cross-Modality

This scenario illustrates the applicability of our method in cross-modality settings, where we aim to translate between latent spaces of different modalities: text and image.

Experimental settingWe adopt N24News (Wang et al., 2022), a multimodal news classification dataset that contains both text and associated pictures. We apply the standard encoding procedure to these two features separately, using different pre-trained uni-modal encoders. Then, we train a classification head (an SVM, please refer to Appendix Figure 9 for further results employing an MLP as classification head) on top of each one. Lastly, we zero-shot stitch each encoder with a classification head different from its corresponding one, measuring its classification accuracy, without further training or fine-tuning.

Figure 4: Scale distribution in encodings of different pre-trained encoders on the N24News dataset.

Figure 5: Performance comparison between different encoders and data modalities on the N24News multimodal dataset. On the right the accuracy of models trained end-to-end on a single data modality (Score) and their average norm (Scale). On the left the stitching performance between pairs of encoders and decoder. This shows the importance of translating from good encoders, that can even improve unimodal decoder performances. Results obtained with \(2000\) anchors and ortho, with an SVM as classification head. In the Appendix Figure 9, additional results using MLPs as decoders.

Scale distributionsIn Figure 4, we present the scale distribution of the embeddings produced by several encoders on the N24News dataset. This empirical analysis shows a consistent pattern among encoders in that the scale distribution of their embeddings follows a Gaussian one with a single mode and a well-defined mean, which are usually compatible with standard scaling. This consistent behavior across encoders is likely attributed to their architectural choices, such as the normalization techniques, regularizations and the optimization problems they are designed to solve.

Result analysisThe discrepancy in the mean accuracy represented by the marginal bar plots in Figure 5 is a signal that can be used to identify spaces more suited to be _decoded into_ and the ones that are stronger in _encoding from_. In fact, the language models as source space for the translation exhibit stronger performance than the vision encoders. We relate this behavior to the higher generality of the text domain data used during pre-training with respect to the image domain one (Zhai et al., 2022). A remarkable finding in this setting is the improvement in classification performance when a modality-specific classifier trained on images is fed zero-shot with corresponding text encodings translated to the image domain via our method. This result underlines the significance of a good encoder and demonstrates the broad applicability of our technique. In practice, this means we can seamlessly apply image classifiers on textual data, and vice-versa.

These results show that our method: i) obtains effective zero-shot translation over different modalities; ii) improves unimodal decoders when translating from a better encoder than the one it was trained on.

### Autoencoding

In this setting, our method is applied to align latent spaces of different trainings of the same autoencoder. The novelty of this scenario lies in the generation setting itself, as most prior works (Section 2) primarily focus on classification tasks. One key observation of (Cannistraci et al., 2023) is that the _task_ at hand (e.g., classification, generation) defines a certain _class of transformations_ (e.g. rotations) which act among the latent spaces. Restricting the search for the transformation to the right class, is fundamental in order to guarantee optimal performance and efficiency.

Experimental settingWe utilize four datasets for these experiments: MNIST(Lecun et al., 1998), Fashion MNIST(Xiao et al., 2017), and CIFAR10 and CIFAR100 Krizhevsky (2009). For each dataset, we train two standard CNN-based autoencoder, with convolutions in the encoder and deconvolutions in the decoder, please refer to the supplementary material for further implementation details. The two autoencoders are identical in structure, differing only in the random seed used for weight initialization and data shuffling. To perform zero-shot stitching, we first translate each data point from the latent space of the first encoder to the latent space of the second using \(1000\) anchors. We then apply the second decoder to the translated data, without any further training or fine-tuning.

Result analysisThis experiment analyzes the alignment of latent spaces in different training regimens of the same autoencoder. The performance evaluation, as shown in Table 3, demonstrates that all methods affine, linear, l-ortho, and ortho yield satisfactory results. Moreover, qualitative results depicted in Figure 6 reveals minimal visual differences in the stitching outcomes across various datasets using different methods. Please refer to Figures 10 and 11 for other qualitative results. In fact, these results suggest that the latent spaces of image autoencoders are not exclusively correlated

Figure 6: Reconstruction examples grouped by dataset. Each column is a different image, from top to bottom: original image, absolute stitching, affine stitching linear stitching, l-ortho stitching, and ortho stitching. No additional normalization applied on the decoder part. Please refer to Figures 10 and 11 in the Appendix for decoders trained with L2 normalization.

by orthogonal transformations. Therefore, further research is warranted to explore and model the specific class of transformations that govern the correlation between neural networks during image autoencoding to constrain and enhance their approximation. For additional results pertaining to decoders with L2 normalization on their input, we refer to the Table 4 in the Appendix.

Overall these results, combined with Cannistraci et al. (2023) and Section 4.1, confirm that latent spaces in image autoencoders trained end-to-end are related by a class of transformations larger than orthogonal transformations.

## 5 Conclusion

At the heart of the proposed latent space translation lies the synergy between the principles of relative representation and classic algebraic techniques. The efficacy of this approach surpasses that of relative representations, emphasizing the benefits of directly estimating a transformation between specific latent space pairs instead of independently projecting them to a common one. This distinction underscores our contribution: we repurpose well-established techniques to serve as a translator across multiple latent spaces, enhancing efficiency in representation learning. With an extensive analysis of its applications in model reuse, we obtain a smooth compositionality of neural network modules across diverse computational frameworks, including those employing pre-trained models. Essentially, this paper showcases the adaptability and efficiency of manifold alignment methods in the emerging domain of zero-shot model compositionality.

Future works and limitationsAs with any new approach, there are limitations that warrant further exploration of our proposed method. For example, the optimal number of anchor points required for different tasks and datasets to boost performances, investigating the factors that could be linked to latent space compatibility (e.g., their intrinsic dimension), trade-offs between the granularity of the anchor set and its condition number. These are exciting research directions that we believe hold great potential for advancing the field and improving the effectiveness and robustness of our method.

AcknowledgementsThis work is supported by the ERC grant no.802554 (SPECGEO), PRIN 2020 project no.2020TA3K9N (LEGO.AI), and PNRR MUR project PE0000013-FAIR. Francesco Locatello did not contribute to this work at Amazon.

Reproducibility StatementWe refer to the supplementary material for implementation details that are not described here in the main manuscript. Moreover, we release a modular PyTorch codebase2 implementing the various translation methods and scaling techniques. All the experiments are carried out in deterministic environments to enable reproducibility, and the necessary data is versioned via DVC (Kuprieiev et al., 2022).

    &  &  &  &  \\   & _lcos_ & _lmse_ & _rmse_ & _lcos_ & _lmse_ & _rmse_ & _lcos_ & _lmse_ & _rmse_ & _lcos_ & _lmse_ & _rmse_ \\  absolute & 0.09 & 0.27 & 0.14 & 0.17 & 0.23 & 0.23 & 0.30 & 0.29 & 0.34 & 0.34 & 0.53 & 0.40 \\ affine & 0.94 & 0.08 & 0.02 & 0.94 & 0.06 & 0.03 & 0.96 & 0.03 & 0.05 & 0.96 & 0.04 & 0.05 \\ linear & 0.92 & 0.09 & 0.02 & 0.93 & 0.07 & 0.04 & 0.94 & 0.03 & 0.05 & 0.94 & 0.04 & 0.06 \\ l-ortho & 0.79 & 0.14 & 0.02 & 0.78 & 0.12 & 0.05 & 0.85 & 0.05 & 0.06 & 0.84 & 0.07 & 0.07 \\ ortho & 0.90 & 0.10 & 0.02 & 0.90 & 0.08 & 0.04 & 0.94 & 0.03 & 0.06 & 0.93 & 0.04 & 0.06 \\   

Table 3: Zero-shot stitching for generation with various methods for estimating \(\). Standard scaling used as normalization and the stitched decoders do not have any additional normalization. We report the latent cosine similarity (_lcos_) and MSE (_lmse_) between the target encoding and the translated one, but also the reconstruction MSE (_rmse_) between the input and the output. 1000 anchors used on 500 dimensional spaces. Please refer to Table 4 for results on decoders scale-invariant by design (with L2 normalization on the encodings).