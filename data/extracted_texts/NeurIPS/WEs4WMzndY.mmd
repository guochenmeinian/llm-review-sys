# Annealed Multiple Choice Learning: Overcoming limitations of Winner-takes-all with annealing

David Perera\({}^{1}\)

david.perera@telecom-paris.fr

&Victor Letzelter\({}^{*}\)\({}^{1}\)\({}^{2}\)

victor.letzelter@telecom-paris.fr

Theo Mariotte\({}^{1}\)   Adrien Cortes\({}^{3}\)   Mickael Chen\({}^{2}\)   Slim Essid\({}^{1}\)   Gael Richard\({}^{1}\)

\({}^{1}\) LTCI, Telecom Paris, Institut Polytechnique de Paris

\({}^{2}\) Valeo.ai

\({}^{3}\) Sorbonne Universite

Equal Contribution.

###### Abstract

We introduce Annealed Multiple Choice Learning (aMCL) which combines simulated annealing with MCL. MCL is a learning framework handling ambiguous tasks by predicting a small set of plausible hypotheses. These hypotheses are trained using the Winner-takes-all (WTA) scheme, which promotes the diversity of the predictions. However, this scheme may converge toward an arbitrarily suboptimal local minimum, due to the greedy nature of WTA. We overcome this limitation using annealing, which enhances the exploration of the hypothesis space during training. We leverage insights from statistical physics and information theory to provide a detailed description of the model training trajectory. Additionally, we validate our algorithm by extensive experiments on synthetic datasets, on the standard UCI benchmark, and on speech separation.

## 1 Introduction

Ambiguous prediction tasks arise in deep learning when the target \(y\) is ill-defined from the input \(x\). Predicting \(y\) directly from \(x\) can be challenging due to the partial predictability of \(y\) from the information contained in \(x\). Multiple Choice Learning (MCL)  addresses these challenges by providing a small set of plausible _hypotheses_, each representing a different possible outcome given the input. MCL learns these hypotheses using a competitive training scheme that promotes the specialization of the hypotheses in distinct regions of the output space \(\). The framework iteratively partitions \(\) into a Voronoi tesselation and guides each hypothesis toward the barycenter of its respective Voronoi cell . This mechanism makes MCL akin to a gradient-descent-based and conditional variant of the popular K-means algorithm . Like K-means, MCL is sensitive to initialization, subject to hypothesis collapse , and more generally may converge toward arbitrarily suboptimal hypothesis configurations . While there is a substantial body of literature addressing the limitations of K-means , relatively little has been done to address these challenges in the context of MCL . The core issue of MCL lies with its greedy gradient-based update of the hypotheses. This greediness precludes the exploration of the hypothesis space, preventing MCL from optimally capturing the ambiguity of \(y\). We propose to incorporate annealing into this gradient descent update in order to improve the robustness of MCL.

Simulated annealing, inspired by the gradual cooling of metals, was originally introduced for statistical mechanics applications  and was later applied to combinatorial problems . It isa random exploration process concurrent to the popular stochastic gradient descent, with a significant difference: gradient descent always tries to improve performance while annealing also accepts to temporarily degrade it for the sake of exploration. The range of this exploration is controlled by a temperature parameter: with a high temperature, annealing explores wide regions of the search space; when the temperature decreases, the exploration becomes narrow, and the system is able to refine its performance. This strategy has been shown to converge to an optimal state, provided that the cooling is sufficiently slow .

Deterministic annealing  is a variant of simulated annealing. In simulated annealing, exploration relies on a sequence of random moves across the search space, whereas deterministic annealing seeks greater efficiency by replacing this random process with the exact minimization of a deterministic functional, namely the free energy of the system. It has been shown that deterministic annealing can be efficiently applied to clustering [61; 62]. In this article, we show that it can be further adapted to the conditional and gradient-based setting of MCL. The resulting algorithm, which we name aMCL for _annealed MCL_, addresses the main issues of MCL and achieves strong performance in practical settings while being straightforward to implement and amenable to analysis. Specifically, we make the following contributions.

**We introduce Annealed Multiple Choice Learning (aMCL)**, a novel algorithm that incorporates annealing into the multiple choice learning framework (Section 3).

**We propose a theoretical analysis of aMCL**, to understand its advantages in comparison to vanilla MCL. We characterize the training trajectory of the model, by establishing an analogy with statistical physics and information theory (Section 4).

**We provide extensive experimental validation,** by applying this method i) to illustrative synthetic examples; ii) to a standard distribution estimation benchmark (UCI datasets); and also iii) to the challenging audio task of speech separation (Section 5). The accompanying code is made available.1

## 2 Related Work

**Multiple choice learning.** MCL has been successfully applied to various machine learning tasks, typically using multi-head neural networks, with each head providing a prediction [40; 22; 42]. Several works observed the phenomenon of _hypothesis collapse_[8; 19; 33; 40; 67; 63], where some hypotheses are left unused during training. Various solutions have been proposed to tackle collapse [63; 49; 54]. Notably,  introduces Relaxed-WTA, which updates non-winning hypotheses with a gradient scaled by a small constant \(\). However, this small gradient biases the hypotheses toward the global barycenter of the target distribution, which can be shown to be suboptimal .

**Simulated and deterministic annealing.** Deterministic annealing is a variant of simulated annealing [28; 52; 35]. Rose _et al._ extensively investigated its properties, particularly in relation to statistical physics and clustering [61; 62; 59; 60]. We are, to the best of our knowledge, the first to combine this technique with Winner-takes-all training in a conditional setting.

**Information theory and quantization.** Quantization  consists of discretizing continuous variables over a finite set of symbols. The rate-distortion theory studies the minimal number of bits necessary to encode information at a given level of quantization error [5; 2; 4]. Recently, a relation has been established between optimal quantization of conditional distributions  and multiple choice learning [63; 42; 43]. In this paper, we propose to integrate annealing for conditional quantization.

## 3 Annealed Multiple Choice Learning

### Winner-takes-all loss and its limitations

Let \(\) and \(\) denote subsets of Euclidean vector spaces. We are interested in so-called _ambiguous tasks_, _i.e._, for any given input \(x\), there may be several plausible outputs \(y\). Formally, let \(p(x,y)\) denote a joint distribution on \(\). Multiple Choice Learning (MCL) [25; 41] was proposed to train neural networks in this setting, and has proven its effectiveness in a wide range of machine vision , natural language  and signal processing tasks .

MCL consists in training several predictors \((f_{1},,f_{n})(,^{n})\), typically a multi-head neural network derived from a common backbone, such that for each input \(x\), the predictions \((f_{1}(x),,f_{n}(x))\) provide an efficient _quantization_ of the conditional distribution \(p(y x)\). This goal is achieved by minimizing the _distortion_

\[D(f)_{}_{k[\![1,n]\!]} (y,f_{k}(x))p(x,y)xy\,\] (1)

where \(:^{2}\) is an underlying loss function, for instance, the squared Euclidean distance \((,y)=\|y-\|^{2}\). Eq. (1) can be seen as a generalization of the conditional distortion .

More specifically, MCL training is an iterative procedure optimizing (1) by alternating the two following steps.

1. Assign each \(y\) to the closest hypothesis \(f_{k}(x)\) to build the Voronoi cells: \[_{k}(x)\{y l[\![1,n]\!],\;(y,f_{k}(x))(y,f_{l}(x))\}\.\] (2)
2. Minimize the distortion within each cell by taking a gradient step on the WTA loss: \[^{}(f)_{}_{k=1}^{n}( _{_{k}(x)}(f_{k}(x),y)p(y x)y)p(x) x\.\] (3)

The prediction models can be paired with scoring models \((_{1},,_{n})(,^{n})\), which are trained to estimate the Voronoi regions' probability mass using the scoring loss 

\[^{}()_{ }_{k=1}^{n}([y_{k }(x)],_{k}(x))p(x,y)xy\,,\] (4)

where \((p,q)-p(q)-(1-p)(1-q)\). In practice, the two losses (3) and (4) are optimized in a compound objective \(=^{}+^{}\). A probabilistic interpretation of such trained predictors has been developed . It shows that the predictions and scores can be interpreted as a mixture model approximating the conditional density \(p(y x)\) by \(_{k=1}^{n}_{k}(x)_{f_{k}(x)}(y)\).

It has been shown that WTA is sensitive to initialization , and often leads to suboptimal hypothesis positions, similarly to K-means. Indeed, WTA is a greedy procedure that updates only the best hypotheses: if one hypothesis falls outside the support of the data density \(p( x)\), it may be isolated from its competitors at initialization, and remain so across training (the _collapse_ issue).

Our method improves the WTA training scheme by addressing the inherent greediness of gradient descent and introducing variability in the exploration of the hypothesis space through deterministic annealing. Figure 1 illustrates the limitations of the aforementioned algorithms and the comparative advantage of aMCL.

### Combining deterministic annealing with Multiple Choice Learning

We introduce aMCL, which combines MCL and annealing. Let \(t T(t)\) denote a temperature schedule decreasing with the training step \(t\), and vanishing at the end of the training. Similarly to MCL, aMCL alternates between an assignation and a minimization step, as follows:

1. Softly assign each \(y\) to all \(f_{k}(x)\) using the \(\) operator (or Boltzman distribution \(q_{T(t)}\)): \[q_{T(t)}(f_{k} x,y)}(-( x),y)}{T(t)}), Z_{x,y}_{s=1}^{n}(-( x),y)}{T(t)}),\] (5)
2. Minimize the distortion within each soft cell by taking a gradient step on the aWTA loss: \[^{}_{T(t)}(f)_{ }_{k=1}^{n}(f_{k}(x),y)q_{T(t)}(f_{k} x,y)p(x,y) xy\,\] (6) where \(q_{T(t)}\) is kept constant (_i.e.,_ the \(\) operator is applied).

Therefore, at the lowest level, aMCL simply consists of replacing the \(\) operator from (2) by \(\). aMCL introduces the temperature schedule as an additional hyperparameter. As highlighted by the literature on simulated annealing , it is crucial to ensure that the temperature decreases slowly enough to benefit from the advantages of annealing. In practice, we experimented with both linear and exponential schedulers (see also Section 5).

On a higher level, we can interpret the objective of aMCL as a smoothed version of the MCL objective. Smoothing with high temperature simplifies the optimization problem (6), making the loss landscape easier to navigate: we can conjecture from this analysis that aMCL will find a global minimum at high temperature, and we can expect it to stay optimal as long as the temperature decreases slowly enough . We can also see aMCL as an input-dependent version of deterministic annealing [61; 62]. In this view, a high temperature encourages the exploration of the hypothesis space and mitigates the greediness of the gradient descent update (3). Moreover, following , we can posit that there exists an optimal temperature schedule striking a balance between exploration and optimization. Yet another interpretation is that aMCL constitutes an adaptive extension of Relaxed-MCL , as \(q_{T(t)}(f_{k}\ |\ x,y)\) depends both on the distance between the hypothesis \(f_{k}(x)\) and the target \(y\), and the training step \(t\). These interpretations shed light on the inner workings of aMCL. However, the complete training dynamic of the algorithm appears when we analyze aMCL through the lens of information theory and statistical physics, which is the purpose of the next section.

## 4 Theoretical analysis

In this Section, we theoretically investigate the properties of our algorithm. Specifically, we detail its training dynamic in Section 4.1. In Section 4.2, we explore how this dynamic relates to the rate-distortion curve. This relationship allows us to study in Section 4.3 the phenomenon of _phase transition_, where hypotheses merge and split into sub-groups depending on the temperature. Throughout this Section, we will focus on the squared Euclidean distance \((,y)=\|y-\|^{2}\).

### Soft assignation and entropy constraints

Minimizing (1) is NP-hard [1; 48; 10]. Unsurprisingly, MCL can get trapped in local minima during training. In this Section, we discuss why the aMCL training scheme in Section 3.2 is more resilient to this pitfall. The first step toward our analysis is to observe that the \(\) operator used in (6) of the aMCL update effectively turns the algorithm into an alternating optimization of the soft distortion

\[D(q,f)_{}_{k=1}^{n}(f_{k}(x ),y)\ q(f_{k}|x,y)\ p(x,y)xy\,\] (7)

Figure 1: **Overcoming limitations of Winner-Takes-All training with annealing. Illustrations of the test-time predictions on a Mixture of three Gaussians (green points) with \(49\) hypotheses. Shaded blue circles represent the hypothesis predictions, with intensity corresponding to the predicted scores. _(Left)_ Predictions of MCL as proposed in [41; 42]. _(Middle)_ Predictions of Relaxed WTA  with \(=0.1\). _(Right)_ Annealed MCL with initial temperature \(T_{0}=0.6\). Each model was trained with the same backbone (a three-layer MLP). We see that WTA leaves out some hypotheses, achieving a higher quantization error than aMCL. Moreover, we see that Relaxed-WTA is biased toward the barycenter of the distribution, in contrast with aMCL.**

where the variables \(q\) and \(f\) are treated as independent, a procedure similar to Expectation Maximization . This observation is captured by Proposition 1, where \(=(,^{n})\) denotes the set of functions from \(\) to \(^{n}\), \(_{n}\) the set of all distributions on \(n\) items conditioned by points on \(\), \(H()=-_{k=1}^{n}_{k}_{k}\) the entropy of a discrete distribution \(\), \(H_{T}=H(q_{T})\) the entropy of the Boltzmann distribution at temperature \(T\), and \(_{t}\) the learning rate of the gradient descent at step \(t\).

**Proposition 1** (Entropy-constrained alternated minimization).: _The assignation (5) and optimization (6) steps of aMCL correspond to an entropy-constrained block coordinate descent on the soft distortion._

\[q*{argmin}_{q_{n}\\ H(q) H_{T(t)}}D(q,f)\, f_{k} f_{k}-_{t}_{f_{k}}D(q,f)\, k 1,n.\] (8)

This is a corollary of Proposition 2, which provides additional insights on the training dynamics of aMCL.

**Proposition 2** (aMCL training dynamic).: _See Proposition 5 in Appendix. The following statements are true for all \(T>0\), \(f\), strictly positive \(q_{n}\), \(x\) and \(y\)._

\[(i) *{argmin}_{q_{n}\\ H(q) H_{T}}D(q,f)=q_{T}\, q_{T}(f_{k}|x,y)=(x),y)\,/T)}{ _{s=1}^{n}(-(f_{s}(x),y)\,/T)}\,  k 1,n\] \[(ii) *{argmin}_{f}D(q,f)=f^{}\, f_{k}^{}(x)=}y\ q(f_{k}^{}|x,y)p(y  x)y}{_{}q(f_{k}^{}|x,y)p(y x)y}\,  k 1,n\] \[(iii) _{f_{k}}D(q,f)=_{k}^{}(f_{k}-f_{k}^{})\, _{k}^{}=_{}q(f_{k}^{}|x,y)p(y x) y\,  k 1,n\]

Part \((i)\) states that the \(*{softmin}\) operator is the solution of the entropy-constrained minimization of the soft distortion. Part \((ii)\) states that a necessary condition for minimizing the soft distortion is that each \(f_{k}\) is a soft barycenter of the assignation distribution for each temperature \(T\). Part \((iii)\) states that each gradient update moves \(f_{k}\) toward this soft barycenter \(f^{}\), and that the update speed depends on the probability mass \(_{k}^{}\) of the points softly assigned to \(f_{k}\). Together, they describe the training dynamics of aMCL. Note that as \(T 0\), \(q_{T}\) converges to a one-hot vector, and the soft barycenter in \((ii)\) becomes a hard barycenter. This is consistent with the necessary optimal condition for MCL, \(f_{k}^{}(x)=_{Y p(y|x)}[Y Y_{k}(x)]\), proved by Rupperecht _et al._.

### Rate-distortion curve

We have established in Section 4.1 that the aMCL training scheme is equivalent to an entropy-constrained alternating optimization of the soft-distortion (7), with each hypothesis \(f_{k}\) moving toward a soft barycenter. In this Section, we describe the impact of temperature cooling on this training dynamic.

First, observe that when the temperature is high, the Boltzmann distribution \(q_{T}\) becomes uniform. Therefore, the soft Voronoi cells merge into a single cell \(\), the hypotheses \(f_{k}\) converge toward the barycenter of \(\), and they all fuse into a single hypothesis:

\[f_{k}^{}(x)=_{(X,Y) p(x,y)}[Y X=x]\, k  1,n\.\] (9)

Remarkably, this phenomenon occurs even at finite temperatures (see Appendix, Proposition 9). As the temperature decreases, a phenomenon of _bifurcation_ occurs . During this process, the hypotheses iteratively split into sub-groups, as shown in Figure 4. The virtual number of hypotheses  for each \(x\) at a given distortion level is captured by the _conditional rate-distortion function_

\[R_{x}(D^{})_{q_{n},f \\ D_{x}(q,f) D^{}}I_{x}(;Y)\.\] (10)

In Eq. (10), \(Y p(y x)\) follows the target distribution, the hypothesis position \( q(f_{k} x)\) follows a distribution over \(\) with \(q(f_{k} x)=_{}q(f_{k} x,y)p(y x)x\), \(I_{x}(;Y)\) is their mutual information, and \(D_{x}(q,f)=_{}_{k}(f_{k}(x),y)q(f_{k} x,y)p(y x )y\) is the distortion for input \(x\).

The rate-distortion function \(R_{x}(D^{})\) has the following key properties.

**Proposition 3** (Rate-distortion properties).: _See Proposition 7 in Appendix._

_For each \(x\), let \(D^{}_{x}\) (31) denote the optimal conditional distortion when using a single hypothesis, we have the following results._

1. _For each_ \(T>0\)_, minimizing the free energy_ \[=D(q_{T},f)-TH(q_{T})\;,\] (11) _over all hypotheses positions_ \(f(,^{n})\) _comes down to solving the optimization problem that defines (_10_) for each_ \(x\)_._
2. \(R_{x}\) _is a non-increasing, convex and continuous function of_ \(D^{}\)_,_ \(R_{x}(D^{})=0\) _for_ \(D^{} D^{}_{x}\)_, and for each_ \(x\) _the slope can be interpreted as_ \(R^{}_{x}(D^{})=-\) _when it is differentiable._
3. _For each_ \(x\)_,_ \(R_{x}(D^{})\) _is bounded below by the Shannon Lower Bound (SLB)_ \[R_{x}(D^{})(D^{}) H(Y)-H(D^{})\;,\] (12) _where_ \(Y p(y x)\) _and_ \(H(D^{})\) _is the entropy of a Gaussian with variance_ \(D^{}\)_._

Part \((i)\) establishes that the rate-distortion function is tightly linked with our problem. Provided that the hypotheses \(f_{k}\) perfectly optimize the soft distortion (7) at any temperature level, the hypothesis configuration \(f\) will follow the optimal parametric curve \((D^{},R_{x}(D^{}))\) for each \(x\). Part \((ii)\) describes the shape of the parametric curve \((D^{},R_{x}(D^{}))\). The Shannon Lower Bound described in part \((iii)\) is a lower bound on the virtual number of hypotheses reached by aMCL.

The rate-distortion curve effectively describes the training trajectory of our algorithm, with the deterministic annealing procedure consisting of ascending along this curve . Interestingly, there is a set of critical temperatures at which the hypotheses suddenly split, increasing the number of sub-groups they form. By analogy with statistical physics, the behavior at these points has been coined _phase transitions_. An illustration of the trajectory of MCL and aMCL in the rate-distortion space is shown in Figure 2. We see that MCL evolves along a constant rate \(R=_{2}(n)\) (in bits) following \((a)\). In contrast, aMCL initially reaches the critical state at \(D^{}=D^{}_{x}\) following \((b)\). After the transition, the trajectory of aMCL returns to the maximal rate following \((c)\). We expect the optimization at a lower rate to be simpler and this training trajectory to provide a better initialization for the set of hypotheses compared to the vanilla MCL.

Figure 3: **Regimes in the distortion (1) vs. temperature training curve on the setup of Figure 4.** At first, the hypotheses converge to the conditional mean. It is followed by a plateau phase where performance stagnates. Transition begins at \(T^{c}_{0}\): the hypotheses migrate toward the barycenter of each Gaussian. Then, they split and we observe a last phase transition. For reference, \(2^{2}\) is the critical temperature for a Gaussian with variance \(^{2}\).

Figure 2: **Illustration of the training trajectory in the Rate-Distortion curve.** Training trajectories of MCL (blue) and aMCL (red) in the case of a single Gaussian. The optimal reachable distortion (‘+’) is the distortion \(D^{}\) satisfying \(R(D^{})=_{2}(n)\) (See Section 4.2).

### Phase transitions

During training, as the temperature decreases, the hypotheses \(f\) undergo a sequence of phase transitions at specific critical temperatures. The right tool to exhibit these critical temperatures is the Hessian of the free energy \(f(f,q_{T})\). Transitions occur when the minimum of \(\) is no longer stable, and it can be shown  that this relates to the eigenvalues of the following block-diagonal covariance matrix:

\[C_{k,k}(f,q|x)=_{}(f_{k}(x)-y)(f_{k}(x)-y)^{t}\ q(y f_{k},x) p(y x)y\,\] (13)

where \(q(y f_{k},x)\) denotes the posterior probability of assigning a point \(y\) to the hypothesis \(k\), calculated using Bayes's rule . At high temperatures, all hypotheses merge into the conditional barycenter of the distribution (9). In this setting, all the matrices \(C_{k,k}\) are equal to the data covariance matrix \(C(x)_{(X,Y) p(x,y)}[Y X=x]\), \(\) has a unique minimizer, and the stability of this global optimum is conditioned on the strict positivity of the Hessian of \(\). The first critical temperature \(T_{0}^{c}\) is defined as the first temperature for which the Hessian of \(\) is no longer positive definite.

This temperature is connected to \(D_{x}^{}\), the vanishing point of the rate-distortion function \(R_{x}(D^{})\), which also corresponds to the optimal 1-hypothesis distortion . Indeed, \(R_{x}(D^{})\) measures the virtual number of hypotheses, so that the first splitting of the hypotheses from a single point to several groups will coincide with the moment when \(R_{x}(D^{})>0\). We summarize these observations in the following Proposition 4, which is illustrated in Figure 3.

**Proposition 4** (First critical temperature).: _See Definition 1, Propositions 6\((ii)\) and 8 in Appendix. Let \(_{}()\) denote the maximum eigenvalue of a matrix, \(D_{x}^{*}(T)_{f}D_{x}(q_{T},f)\), \(D^{*}(T)_{f}D(q_{T},f)\), and \(D_{}_{x}D_{x}^{}p(x)x\). Then the two following properties hold._

1. \(D_{x}^{*}\) _and_ \(D^{*}\) _are non-decreasing functions of_ \(T\) _and admit generalized inverses (with the convention_ \(g^{-1}()=\{ g()\}\) _for the generalized inverse of a function_ \(g\)_)._
2. _The conditional (resp. non-conditional) first critical temperature_ \(T_{0}^{c}(x)\) _(resp._ \(T_{0}^{c}\)_) satisfy_ \[T_{0}^{c}(x) (D_{x}^{*})^{-1}(D_{x}^{})=2_{}(C(x))\,\] (14) \[T_{0}^{c} (D^{*})^{-1}(D_{}) 2_{x }_{}(C(x))\.\] (15)

Figure 4: **Conditional phase transitions with \(t_{1}<t_{2}<t_{3}<t_{4}\). Results for a conditional version of the dataset in Figure 1, where the Gaussian moves linearly and increases with the input \(x\). aMCL was trained during \(1000\) epochs, using a linear scheduler with \(T_{0}=1.0\), and using \(49\) hypotheses. Each subplot group corresponds to the predictions of the model at a given temperature, evaluated at different \(x\) values. At temperature \(T(t_{1})\), the hypotheses are at the barycenter. As temperature decreases they undergo a first phase transition (at temperature \(T(t_{2})\) for \(x=0.9\) and \(T(t_{3})\) for \(x=0.6\)), moving toward each Gaussian’s barycenter, followed by a second phase transition at \(T(t_{4})\). We see that earlier splits in the cooling schedule correspond to conditional distributions with higher variances.**After the first transition, an interesting phenomenon occurs for some distributions \(p(x,y)\). Instead of splitting in all directions, the hypotheses \(f_{k}\) continue to form a small number of subgroups. The hypotheses \(f_{k}\) may undergo many phase transitions before they all split apart from each other: this is illustrated in Figure 4. Generally, this recursive splitting reaches an end: under mild conditions , there is a critical temperature below which the hypotheses all separate from each other. This defines the last critical temperature \(T_{}^{c}\). Remarkably, \(T_{}^{c}\) is associated with the Shannon Lower Bound: the temperature at which \(R_{x}(D^{})\) hits the lower bound \((D^{})\) corresponds to the moment when the hypotheses \(f_{k}\) completely separate from each other (see Theorem 1 and 3 in ).

## 5 Experimental validation

In this Section, we experimentally investigate the advantage of our algorithm in practical settings. Specifically, we evaluate it on the standard UCI benchmark in Section 5.1, and we apply it to the challenging task of speech separation in Section 5.2.

### UCI datasets

#### 5.1.1 Setup

**General setup.** We followed the experimental protocol described by  for the UCI benchmark . Specifically, we used the official train-test splits, with 20 folds except for the Protein dataset, which is split into 5 folds, and the Year dataset, which uses a single fold.

**Baselines.** In our result tables, we also include data from three baseline methods detailed in Table 1 of Lakshminarayanan _et al._'s paper  ('Deep Ensembles'), which serves as a reference. These baselines include Probabilistic Back Propagation  (denoted 'PBP'), and Monte Carlo Dropout  (denoted 'MC-dropout'). As additional baselines, we include the standard score-based MCL (_e.g._, ). We also include the Relaxed-MCL variant  with \(=0.1\). The impact of \(\) is discussed in Appendix D.2. Our method (aMCL), was trained with an exponential scheduler of the form \(T(t)=T_{0}^{t}\), with \(=0.95\) and \(T_{0}=0.5\). Comparison with a linear scheduler is also provided in Appendix D.1. Both aMCL and Relaxed-MCL were trained for 1,000 epochs. Each MCL system was trained with \(n=5\) hypotheses.

**Metrics.** We computed the following metrics on the UCI datasets. Let \(d\) denote the squared Euclidean distance: \(d(_{i},y_{i})=\|y_{i}-_{i}\|^{2}\), and \(N\) the number of samples in each dataset. The RMSE \(()\) is defined as \(=_{i}d(_{i},y_{i})}\), where \(_{i}\) denotes the estimated conditional mean. The latter was computed with \(_{k=1}^{n}_{k}(x_{i})f_{k}(x_{i})\) for the MCL variants. The results of this experiment are summed up in Table 1 (Distortion), and also in Table 2 (RMSE). Rows are ordered by dataset size \(N\), with the intensity of the grey color proportional to \(N\) (excluding the Year dataset).

#### 5.1.2 Results

    \\ Datasets & Relaxed-WTA (\(=0.1\)) & MCL & aMCL & \(N\) \\  Year & 9.00\(\)**NA** & **4.82\(\)NA** & **4.46\(\)NA** & **515345** \\ Protein & 1.67\(\)**0.16** & 0.80\(\)0.02 & **0.77\(\)0.03** & **45730** \\  Naval & **4.21e-7**\(\)**2.36e-7** & 1.84e-6 \(\) 2.42e-6 & 5.37e-7 \(\) 3.83e-7 & 11934 \\ Power & 2.95 \(\) 0.91 & 2.31 \(\) 0.49 & **2.18\(\) 0.64** & 9568 \\ Kin8nm & 9.32e-4 \(\) 7.97e-5 & 1.00e-3 \(\) 1.47e-4 & **6.81e-4 \(\) 8.14e-5** & 8192 \\ Wine & 0.06 \(\) 0.02 & **0.02\(\) 0.01** & 0.03 \(\) 0.01 & 1599 \\ Concrete & 6.91 \(\) 2.81 & **5.13\(\) 1.23** & 5.71 \(\) 1.72 & 1030 \\ Energy & 0.30 \(\) 0.12 & 1.25 \(\) 1.25 & **0.28\(\) 0.09** & 768 \\ Boston & 3.32 \(\) 2.84 & **2.14\(\) 0.49** & 2.69 \(\) 1.39 & 506 \\ Yacht & 1.34 \(\) 0.93 & 3.09 \(\) 2.41 & **1.15\(\) 0.97** & 308 \\   

Table 1: **Results on UCI regression benchmark datasets comparing Distortion.** Experimental setup is described in Section 5.1.1. Relaxed-WTA results were computed with \(=0.1\) which strikes a good tradeoff between RMSE and Distortion (see Table 5 in Appendix). The rows are ordered by dataset size \(N\). Best results are in **bold**, second bests are underlined.

**Comparison of aMCL and MCL.** We can observe that aMCL performs comparably or outperforms vanilla MCL in most settings, especially for large dataset sizes, both in terms of distortion and RMSE. This outcome supports the claims made in the paper and is especially promising, given that the temperature scheduler was not specifically optimized for each dataset.

**Comparison of aMCL and standard UCI baselines.** We observe that aMCL performs on par with, and in some cases exceeds, standard baselines on the RMSE metric. This is noteworthy, as aMCL is not explicitly optimized for RMSE during training--its primary focus is on quantization. While perfect quantization would naturally result in optimal RMSE, achieving low RMSE is not the main objective of aMCL. For example, the RMSE performance of MCL is slightly worse across most datasets in those experiments.

**Comparison with Relaxed-MCL.** Finally, we compare aMCL and Relaxed-MCL, since aMCL can be interpreted as an adaptative version of Relaxed-MCL. Our results indicate that aMCL generally outperforms Relaxed-MCL in terms of distortion across nearly all datasets. However, we also observe that Relaxed-MCL demonstrates strong performance in terms of RMSE. We attribute these findings to the bias of Relaxed-MCL toward the conditional barycenter of the target distribution, which seems to improve RMSE at the expense of increased distortion. This trade-off arises because RMSE evaluates the accuracy of the barycenter estimation, while distortion measures the quantization performance. The trade-off between distortion and RMSE can be adjusted by tuning the value of \(\). Further analysis of this parameter is presented in Appendix D.1, where we show that aMCL strikes a good balance between these two metrics.

These results strongly support the use of aMCL as a quantization algorithm in practical settings. To further evaluate its effectiveness, we also apply aMCL to a more challenging task, namely speech separation.

### Application to speech separation

Speech separation consists of isolating each speaker's signal from a mixture in which they are simultaneously active. This task is of major interest for automatic speech processing applications . In these experiments, we explore the application of MCL and the proposed aMCL to the task of speech separation. An extensive description of the experiments is proposed in Appendix E.2.

#### 5.2.1 Experimental setting

**General purpose**. Speech separation consists in obtaining the source signals \(y_{1},,y_{m}^{l}\) from a mixture \(x=_{s=1}^{m}y_{s}\). Hence, the task is to provide estimates \(_{1},,_{m}\) of the isolated speech tracks from the mixture \(x\).

**Dataset**. Source separation experiments are conducted on the Wall Street Journal dataset  (WSJ0-mix), a standard benchmark for speech separation. We focus on the 2- and 3- speaker mixture scenarios, with each scenario including 20,000 training, 5,000 validation, and 3,000 testing mixtures.

**Model architecture**. The source separation task is solved using the Dual-Path Recurrent Neural Network (DPRNN) architecture . DPRNN has been extensively used in speech separation, as it strikes a good balance between performance and number of trainable parameters .

    & \))} \\  Datasets & PBP\({}^{*}\) & MC Dropout\({}^{*}\) & Deep Ensembles\({}^{*}\) & Relaxed-WTA (\(=0.1\)) & MCL & aMCL & \(N\) \\  Year & 8.58 \(\) 0.1 & **8.58 \(\) 0.1** & 8.59 \(\) 0.1 & 8.59 \(\) 0.2 & 9.09 \(\) 0.1 & 9.08 \(\) 0.1 & **515345** \\  Protein & 4.73 \(\) 0.01 & 4.36 \(\) 0.04 & **4.71 \(\) 0.00** & 4.38 \(\) 0.02 & 4.39 \(\) 0.10 & **4.25 \(\) 0.02** & 4.5730 \\  Naval & 0.01 \(\) 0.00 & 0.01 \(\) 0.00 & **0.00 \(\) 0.00** & 1.80\(\) 5.66 \(\) 4.2 & 20.86\(\) 3.1 \(\) 1.86 & **8.00-4 \(\) 0.04** & 11934 \\ Power & 4.12 \(\) 0.03 & **4.02 \(\) 0.18** & 4.11 \(\) 0.17 & **4.02 \(\) 0.18** & 4.18 \(\) 0.16 & 4.08 \(\) 0.20 & 9568 \\ KinSMm & 0.10 \(\) 0.00 & 0.10 \(\) 0.00 & 0.09 \(\) 0.00 & **0.08 \(\) 0.00** & 0.10 \(\) 0.01 & **0.08 \(\) 0.00** & 8192 \\ Wine & 0.64 \(\) 0.01 & **0.62 \(\) 0.04** & 0.64 \(\) 0.04 & 0.63 \(\) 0.04 & 0.63 \(\) 0.04 & 0.63 \(\) 0.04 & 1599 \\ Concrete & 5.67 \(\) 0.09 & **5.23 \(\) 0.53** & 6.03 \(\) 0.58 & 5.28 \(\) 0.58 & 6.02 \(\) 0.65 & 5.47 \(\) 0.67 & 1030 \\ Energy & 1.80 \(\) 0.05 & 1.66 \(\) 0.19 & 2.09 \(\) 0.29 & 1.64 \(\) 0.36 & 2.53 \(\) 0.99 & **1.35 \(\) 0.97** & 768 \\ Boston & 3.01 \(\) 0.18 & 2.97 \(\) 0.85 & 3.28 \(\) 1.00 & **2.85 \(\) 0.72** & 3.54 \(\) 1.16 & 3.05 \(\) 0.91 & 506 \\ Yacht & **1.02 \(\) 0.05** & 1.11 \(\) 0.38 & 1.58 \(\) 0.48 & 2.52 \(\) 1.04 & 3.28 \(\) 1.39 & 1.62 \(\) 0.53 & 308 \\   

Table 2: **Results on UCI regression benchmark datasets comparing RMSE. Best results are in bold**, second bests are underlined. \({}^{*}\) corresponds to reported results from .

**Separation metrics**. We use the Scale-Invariant Signal-to-Distortion Ratio (SI-SDR) to measure the separation quality [70; 39]. There is an ambiguity in finding the best assignment between predicted and active sources. The PIT SI-SDR loss  initially addresses this issue. We propose to use MCL and our new variant aMCL to perform this matching (See Appendix E).

#### 5.2.2 Results

**Comparing PIT, MCL, Relaxed-WTA and aMCL.** Table 3 presents the source separation results in the 2- and 3-speaker scenarios. First, aMCL demonstrates performance equivalent to or better than MCL. Both methods can be used for the separation task. However, we observed that MCL is subject to hypothesis collapse for some training seeds, while aMCL is more robust to initialization. This translates into a lower inter-seed standard deviation for aMCL. Second, we observe the advantage of aMCL over Relaxed-WTA, which is consistent with our previous analysis of the barycenter bias of this method. Third, aMCL performs equivalently to PIT in both scenarios. Note that by using MCL or aMCL, the number of predictions \(n\) could exceed the number of sources \(m\). This could be leveraged to improve the separation metrics (cf. Appendix E.3.2). Finally, aMCL improves the algorithmic complexity of PIT from \((m^{3})\) to \((mn)\) (cf. Appendix E.3.1). These results make aMCL stand as a good alternative to PIT.

**Observing phase transition.** When the metric is the Euclidean distance, the theoretical analysis of Section 4.3 and the synthetic experiments (see Figure 3) have highlighted a phenomenon of phase transition for aMCL. Here, we analyze the validation loss trajectory as a function of the temperature for different initial temperatures, and using an exponential scheduler. The curves with the two higher initial temperatures in Figure 8 exhibit a plateau until a given temperature. After this critical point, the loss decreases. Although the SI-SDR is non-Euclidean, this behavior resembles that observed for the Euclidean metric. This is detailed in Section E.3.3 of the Appendix.

## 6 Conclusion

This article introduces aMCL, a novel training method that combines deterministic annealing and the Winner-Takes-all training scheme to address two key issues of MCL: hypothesis collapse and convergence toward a suboptimal local minimum of its quantization objective. We provide a detailed analysis of aMCL's training dynamics. Moreover, drawing on statistical physics and information theory, we provide insights into the trajectory of the aMCL predictions during training. In particular, we exhibit a phase transition phenomenon and establish its connection to the rate-distortion curve. We validate our analysis with experiments on synthetic data, on the UCI datasets, and on a real-world speech separation task. This demonstrates that aMCL is a theoretically grounded alternative to MCL in diverse settings. Future work includes a detailed analysis of the temperature schedule's impact, the derivation of performance bounds, a thorough examination of our algorithm's convergence, particularly at finite temperature, as well as an evaluation of its generalization capabilities on out-of-distribution samples.

**Limitations.** First, aMCL introduces a temperature schedule: this is a challenging hyperparameter tightly linked to the optimizer and its learning rate. The derivation of optimal schedules is left to future work. Second, annealing requires a slow temperature schedule to maintain model performance. This potentially leads to longer training times.

**Broaden impact.** This paper introduces research aimed at progressing the field of Machine Learning. While our work has numerous potential societal implications, we believe there are no specific consequences that need to be emphasized in this paper.

   Method & 2 speakers & 3 speakers \\  PIT & 16.88 \(\) 0.10 & 10.01 \(\) 0.04 \\  MCL & 16.30 \(\) 0.59 & 10.06 \(\) 0.21 \\ Relaxed-WTA & 16.70 \(\) 0.08 & 9.43 \(\) 0.21 \\ aMCL & 16.85 \(\) 0.13 & 10.00 \(\) 0.21 \\   

Table 3: **2- and 3- speaker source separation** with PIT (topline), MCL, aMCL and Relaxed-WTA (\(=0.05\)). PIT SI-SDR metric (\(\)) on the WSJ0-mix eval set. Results over three training seeds, with mean and standard deviation reported.

Acknowledgments

This work was funded by the French Association for Technological Research (ANRT CIFRE contract 2022-1854) and Hi! PARIS through their PhD in AI funding programs, and was performed using HPC resources from GENCI-IDRIS (Grant 2021-AD011013406R1). We are grateful to the reviewers for their insightful comments.