# CleanDiffuser: An Easy-to-use Modularized Library for Diffusion Models in Decision Making

Zibin Dong\({}^{1}\), Yifu Yuan\({}^{1}\), Jianye Hao\({}^{1}\), Fei Ni\({}^{1}\), Yi Ma\({}^{2}\), Pengyi Li\({}^{1}\), Yan Zheng\({}^{1}\)

\({}^{1}\)College of Intelligence and Computing, Tianjin University

{zibindong,yuanyf,jiangv.hao,fei_ni,lipengyi,yanzheng@tju.edu.cn}

\({}^{2}\)School of Computer and Information Technology, Shanxi University, mayi@sxu.edu.cn

These authors contribute equally to this work.Corresponding authors: Jianye Hao (jiangye.hao@tju.edu.cn)

###### Abstract

Leveraging the powerful generative capability of diffusion models (DMs) to build decision-making agents has achieved extensive success. However, there is still a demand for an easy-to-use and modularized open-source library that offers customized and efficient development for DM-based decision-making algorithms. In this work, we introduce CleanDiffuser, the first DM library specifically designed for decision-making algorithms. By revisiting the roles of DMs in the decision-making domain, we identify a set of essential sub-modules that constitute the core of CleanDiffuser, allowing for the implementation of various DM algorithms with simple and flexible building blocks. To demonstrate the reliability and flexibility of CleanDiffuser, we conduct comprehensive evaluations of various DM algorithms implemented with CleanDiffuser across an extensive range of tasks. The analytical experiments provide a wealth of valuable design choices and insights, reveal opportunities and challenges, and lay a solid groundwork for future research. CleanDiffuser will provide long-term support to the decision-making community, enhancing reproducibility and fostering the development of more robust solutions. The code and documentation of CleanDiffuser are open-sourced on the project website.

## 1 Introduction

Diffusion models (DMs)  have emerged as a leading class of generative models, outperforming previous methods  in both high-quality generation and training stability . Their remarkable capabilities in complex distribution modeling and conditional generation demonstrate promising performance across various domains , inspiring a series of works to apply DMs in decision-making tasks . Open-source libraries can quantify progress in this emerging field, enable researchers to better understand and compare algorithm details, and promote the application of DMs. Currently, several high-quality libraries are available for DMs, such as Diffusers  and Stable Diffusion , which provide exemplary designs for the computer vision and multimedia. However, support for decision-making is lacking. Although some pioneering research  on DMs for decision-making has provided excellent codes, their algorithm-specific mechanisms and tightly coupled system architectures are not conducive to customized development.

In this paper, we present an easy-to-use modularized DM library tailored for decision-making named CleanDiffuser, which comprehensively integrates different types of DM algorithmic branches. We revisit various roles of DMs in decision-making tasks and identify core sub-modules: Diffusion Models, Network Architectures and Guided Sampling Methods. CleanDiffuser alsoincorporates an efficient Dataloader and useful Environment Wrappers for easy usage and customized datasets extension. Specifically, to address the unique decision-making challenges, CleanDiffuser designs a series of practical features for special mechanisms. With CleanDiffuser, algorithms can be implemented by selecting building blocks and integrating them into a pipeline. Customizing an algorithm requires only about 10 lines of code, providing the highest usability and customization. The decoupled modular architecture allows developers to adapt to different tasks and facilitates the adjustment of existing methods without complex abstractions. CleanDiffuser effectively meets the diverse requirements of various decision-making algorithms.

To demonstrate the reliability and flexibility of CleanDiffuser, we conduct extensive experiments in 37 Reinforcement Learning (RL) and Imitation Learning (IL) environments for 9 algorithms and their variants, benchmarking performance for many DM algorithms and serving as valuable references for future research. Thanks to the general architecture of CleanDiffuser, we revisit the key design choices of the DMs for decision-making from a unified perspective. We conduct extensive empirical analyses on different architectures, solvers, sample steps, EMA, and model sizes, providing valuable insights and showing challenges for designing DM-based decision-making algorithms.

Our contributions are three-fold: (1) We present an easy-to-use modularized library named CleanDiffuser, the first DM library designed specifically for decision-making tasks. (2) We decouple the general DM algorithms into 3 core sub-modules and design specialized features for decision-making, ultimately integrating them into a modular pipeline. (3) Utilizing over 30,000 GPU hours of computational resources, we benchmark various popular DM-based algorithms and conduct a thorough empirical analysis, providing valuable insights and revealing opportunities and challenges.

## 2 Background

**Sequential Decision-making Problem.** Consider a system governed by discrete-time dynamics \((^{t+1},r^{t})=d(^{t},^{t})\), in which taking action \(^{t}\) at state \(^{t}\) leads a transition to \(^{t+1}\) and yields a scalar reward \(r^{t}\). Given an interaction record dataset \(=\{(^{t},^{t},r^{t},^{t+1})\}\) collected by a behavior policy, the offline RL [15; 17] aims to derive an optimal policy from the dataset to maximize cumulative reward and surpass the behavior policy. The offline IL , which assumes the behavior policy is an expert and does not require reward labels, aims to mimic the expert behaviors closely.

Figure 1: **The Architecture of CleanDiffuser. CleanDiffuser is specifically tailored for the decision-making domain, supporting a wide range of Diffusion Models, Network Architectures, and Guided Sampling Methods modules and extra useful features. By simply combining the building blocks into a pipeline, CleanDiffuser integrates 9 popular DM algorithms.**

**Training and Sampling of Diffusion Models.** Assume a \(D\)-dimensional random variable \(_{0}^{D}\) with an unknown distribution \(q_{0}(_{0})\)3. DMs gradually transform samples from a simple distribution \(q_{T}(_{T})\) into samples from \(q_{0}(_{0})\)[33; 26], which is accomplished by solving a reverse Stochastic Differential Equation (SDE) or Ordinary Differential Equation (ODE) :

\[_{t} =[f(t)_{t}-g^{2}(t)_{} q_{t}(_{t})] t+g(t)}_{t},\ _{T} q_{T}(_{T}),\] (1) \[_{t} =[f(t)_{t}-g^{2}(t)_{} q_{t}( _{t})]t,\ _{T} q_{T}(_{T}),\] (2)

where \(_{t}\) is a standard Wiener process in the reverse time, \(f(t)=_{t}}{t},\ g^{2}(t)= _{t}^{2}}{t}-2_{t}^{2}_{t}}{ t}\), and \(_{t}=_{t}_{0}+_{t},\ (,)\). The _noise schedule_\(_{t},_{t}^{+}\) are differentiable functions of \(t\) such that the _signal-to-noise-ratio_ (SNR) \(_{t}^{2}/_{t}^{2}\) is strictly decreasing w.r.t \(t\). The training of DMs involves using a neural network parameterized by \(\) to estimate the unknown term within the SDE or ODE. Different DMs may incorporate varying parameterizations. For instance, diffusion SDE uses a network to estimate a scaled _score function_\(_{}(_{t},t)-_{t}_{} q_{t} (_{t})\)[26; 60; 45; 61], while EDM estimates clean data \(_{}(_{t},t)(_{t}-_{t}^{2}_{}  q_{t}(_{t}))/_{t}\). The sampling process of DMs involves utilizing numerical solvers to solve the SDE or ODE. DDPM  and DDIM  solve the first-order discretization of Equation (1) and Equation (2). DPM-Solver [45; 46] leverages the semi-linearity of the reverse ODE in Equation (2) for exact solutions, eliminating errors in the linear terms, resulting in a higher sample quality. EDM  uses a specially designed score function preconditioning and \(2^{}\)-order Heun's method to solve the reverse ODE, also improving the sample quality. Understanding training and sampling as separate processes enables the seamless selection of varying sampling steps and solvers during the generation process without additional training. Some other SDE/ODE-based generative models, such as Rectified Flow , can also be understood through this lens by using a network to estimate the unknown drift force \(_{}(_{t},t)(_{0}-_{T})\) in a straight ODE \(_{t}=_{}(_{t},t)t\) and solving it by Euler solver. See Appendix A for more details.

## 3 Revisiting Diffusion Models in Decision Making Scenarios

As shown in Figure 2, current works applying DMs on decision-making mainly fall into three categories : _generating long-term trajectories and executing like **planners**, _replacing the conventional Gaussian policies with multimodal diffusion **policies**_ and _serving as **data synthesizers** to assist model training_. This section briefly introduces each category, outlines the technical module-design requirements, and summarizes the challenges of designing a general framework.

**Planner.** Planning refers to generating trajectories \(\), which can be either sequence of states or state-action pairs, to maximize the cumulative reward and selecting actions to track the trajectory [20; 22; 21]. DMs can simultaneously generate super-long, high-quality trajectories, preventing severe compounding errors occurred in previous planning algorithms [30; 12]. Assume the trajectory starts at \(t=\) and ends at \(\), diffusion planner sample from an optimality-conditioned trajectory distribution \(p(|^{:})\) or a reward-conditioned distribution \(p(|_{t=}^{}r^{t})\). At each inference step, diffusion planner generates a set of candidate trajectories \(\{_{0}\}\), selects the local optimal \(_{0}^{*}\), and

Figure 2: **Diffusion Models Mainly Play Three Roles in Decision-Making Scenarios.** Planner : Acting as planners to make better decisions from a long-term perspective. Policy : Serving as policies to support complex multimodal-distribution modeling. Data Synthesizer : Performing data augmentation to assist model training.

then extracts the action to execute. Typically, these algorithms freeze certain known parts of the trajectories during the diffusion process, such as history trajectories, current states, and future goals, turning the generation into an inpainting problem [30; 1; 12; 28]. This feature necessitates the demand for a flexible masking mechanism to design frozen parts and freely alter the planning properties.

**Policy.** Policy is typically a state-conditioned action distribution \(_{}(|)\). DMs' strong distribution modeling capability allows them to effectively replace commonly used deterministic or Gaussian policies [37; 36; 16] in both RL and IL settings. In RL settings, researchers have explored incorporating diffusion policies as actors in actor-critic frameworks [64; 31], as well as directly fitting the optimal policy derived from generalized constrained policy search (CPS) [23; 3]. These works focus on the combination of DMs and RL components, where RL may guide the generation , evaluate action selection [3; 23], or even influence DM training . In IL settings, researchers focus more on complex network designs to support effective guided sampling [4; 54; 69; 53], which processes rich-modality agent perception, including low-dim physical quantities , RGB images , 3D point clouds , and even language instructions . A separated guided sampling module can help researchers divide and conquer, avoiding engineering difficulties caused by coupled structures.

**Data Synthesizer.** Utilizing synthetic data, which can be either transitions or trajectories, from generative models to assist policy learning has been proven effective [29; 5]. Introducing DMs as the generative backbone promotes synthetic quality , addressing the lack of fidelity in previous works. Unlike Planner or Policy, Data Synthesizer does not directly engage in decision-making and, therefore, requires a flexible and modular library compatible with different DM usage paradigms.

In summary, building a general modular DM library for decision-making should meet the following criteria: (1) Implement decoupled modules for DM backbones and network architectures to ensure compatibility with different roles. (2) Incorporate decision-making specific features into module design, e.g., masking and advanced sampling mechanisms. (3) Develop an algorithmic pipeline that seamlessly integrates the modules and mechanisms, catering to different DM usage paradigms.

## 4 CleanDiffuser

### Overview

Based on the analysis above, we illustrate the core sub-modules in Figure 1 and summarize them as follows: (1)Diffusion Models. Existing works [4; 30] often tightly couple SDE/ODE, solvers, and algorithm-specific components in their code implementations, making it challenging for practitioners to read and modify. CleanDiffuser aims to decouple diffusion models as an external module, with internally independent core parts for SDE/ODE and solvers. This design allows users to freely change between solvers and adjust sampling steps with no cost after training. (2)Network Architectures play a crucial role in diffusion-based decision-making algorithms, influencing generative characteristics and indirectly altering algorithm mechanisms [12; 47]. Currently, there is no single architecture that has emerged as the best choice for all scenarios. Therefore, in this module, CleanDiffuser aims to implement the most commonly used architectures to date, leaving ample room for customization and exploration. (3)Guided Sampling Methods. Existing works employ a rich guided sampling design, ranging from scalar [30; 1] to complex multi-modal environment perception [4; 54]. However, their code implementations often couple guided sampling with other components, making independent guidance design challenging. CleanDiffuser aims to decouple this aspect as a separate module, providing users with ample customization space. (4)Environment Interface & Dataloader. CleanDiffuser provides a consistent environment interface and efficient dataloader for easy usage and evaluation of policy performance.

### Modular Design

**Advanced Diffusion Models Support.** CleanDiffuser supports advanced diffusion models such as DDPM , DDIM , DPM-Solver , DPM-Solver++ , EDM , and Rectified Flow , which share a unified API calling, see Appendix F. Our implementation features the following:

* _Masking Mechanism._ DM-based decision-making algorithms may incorporate masks to freeze certain known parts and alter the use of generated data [30; 12]. For example, as demonstrated in Figure 3 (top), during trajectory generation, one may use a history trajectory as context, retain the current state to provide instant information, and supply a goal to steer the trajectory towards it.

The masking mechanism provides a simple interface, using a binary vector describing the freeze requirements. All additional computational processing due to masking is handled internally in the code so that users can concentrate on designing other components.
* _Cross-Solver Sampling_. DMs in CleanDiffuser are implemented with two core parts: _SDE/ODE_ and _solver_. Training involves using neural networks to fit the parameterized terms in the SDE/ODE, e.g., the score function in diffusion SDE, and is unrelated to the solvers. This design allows one trained diffusion model to choose varying sampling steps and different solvers during generation without additional cost. For example, after training a decision-making algorithm based on diffusion SDE, one can seamlessly use varying sampling steps and switch between DDPM, DDIM, DPM-Solver, and DPM-Solver++ during inference, greatly facilitating researchers conducting ablation studies and analyses across different diffusion backbones.
* _Diffusion-X Sampling_. Considering the significant negative impact of out-of-distribution (OOD) samples in decision-making tasks, the Diffusion-X sampling process is proposed to include additional repeating denoising steps at the last sampling step . This approach helps concentrate the generated samples in high-likelihood regions, reducing OOD issues.
* _Noise/Data Prediction Switching_. Neural networks in DMs can be utilized for predicting noise as well as clean data. In decision-making tasks, the former simplifies optimization by avoiding the direct generation of complex data samples , while the latter can introduce thresholding methods to constrain samples and prevent OOD generation . Existing methods lack a systematic exploration of the effects resulting from these two parameterization approaches. CleanDiffuser implements noise/data prediction as a switch, depicted in Figure 3, to offer researchers a flexible and convenient way to compare between the two approaches.
* _Warm-Starting Sampling Technique_. Decision-making dynamics exhibit a certain consistency over time, implying that samples generated at adjacent decision-time steps have similarities. Inspired by this, the warm-starting sampling technique proposes adding a small amount of noise to the samples generated at the previous time step and then conducting a few denoising steps to generate samples of sufficient quality for the current time step. This trick can trade off a small amount of accuracy for an increase in decision frequency and can be useful in real-world applications.

**Network Architectures Designed for Decision-Making.** CleanDiffuser incorporates 8 popular network architectures designed for decision-making, as demonstrated in Figure 4, including:

* DQL_MLP  is a simple yet efficient MLP architecture for action generation proposed in DQL.
* LNResnet  is a residual MLP with Dropout and LayerNorm to enhance action quality.
* Pearce_MLP , referred to as MLPSieve in DiffusionBC paper, is a residual MLP, which concatenates original inputs to each hidden feature.
* Janner_UNet1d  inherits from the classic image-generation network architecture used in DDPM++ and NCSN++ , and is modified for trajectory generation. This architecture can generate variable-length trajectories , which enhances inference flexibility.
* Chi_UNet1d  incorporates FiLM conditioning  in Janner_UNet1d to enhance the reception of sequential observation conditions, achieving excellent performance in IL tasks.

Figure 3: **Features of CleanDiffuser Designed for Decision-Making Introduced in Section 4.2.**

* DiTld inherits from the transformer DM network backbone  and is modified for trajectory generation, showing better training stability and sample quality compared to Janner_UNet1d.
* Pearce_Transformer replaces the structure in Pearce_MLP with the multi-head self-attention, which sacrifices efficiency for better action generation quality.
* Chi_Transformer employs a transformer decoder architecture and a special cross-attention mask to enhance the reception of conditions, achieving performance similar to Chi_UNet1d.

These network architectures have been proven effective for decision-making tasks in previous works and widely referenced or directly applied in other algorithms . In CleanDiffuser, all these architectures inherit from the same parent class and share a standard API calling, making it easy for researchers to design new architectures based on the foundations.

**Guided Sampling.** Two guided sampling methods, CG  and CFG , are presented in the form of _Classifier_ and _Condition Network_, which are completely decoupled from the DM network architecture. Users can focus solely on processing condition information without worrying about the interaction with DMs and eventually integrate them with DMs in a switch-like manner.

**Environment Interface and Efficient Dataloader:** To facilitate benchmark evaluation, we encapsulate Gym-like  API for all environments, implementing visualization, multi-step interaction, and parallel sampling through various wrappers. This makes it convenient for researchers to reuse and extend. Additionally, we implement efficient I/O based on Zarr  library for large-scale datasets and combine it with PyTorch's DadaLoader  for batch data processing and training, which allows for flexible data access even with limited memory. CleanDiffuser also provides Wandb  logging support and Hydra  configuration to facilitate experiment tracking. We provide YAML configuration files for each experiment, ensuring full reproducibility without tuning hyperparameters.

### From Decoupled Modules to Integrated Pipelines

With CleanDiffuser, developing algorithms can be much more straightforward because users only need to select the desired building blocks and assemble them into a pipeline. As shown in Figure 5, a Diffuser implementation example that uses Janner_UNet1d as the network architecture for generating trajectories, employs a _Classifier_ for guided sampling to maximize the cumulative reward of generated trajectories, selects Diffusion SDE as the diffusion backbone, and performs sampling using DDPM. Assembling these modules constructs a pipeline, a simple yet efficient Diffuser implementation. In this way, users can easily understand the differences and properties of algorithms and adjust them by simply replacing the building blocks. In CleanDiffuser, we implement various diffusion-based decision-making algorithms in this module-to-pipeline style, offering a diverse set of examples for practitioners to implement their applications with CleanDiffuser. The implemented algorithms include three diffusion planners: Diffuser , Decision Diffuser (DD) , and AdaptDiffuser ; five diffusion policies: DiffusionPolicy , DiffusionBC , DQL , EDP , and IDQL ; one diffusion data synthesizer: SynthER . See Appendix G for details.

Figure 4: **Visualization of Implemented Network Architectures in CleanDiffuser.**

## 5 Experiments

Due to space limitations in the main text, we introduce details of all benchmarks and datasets used in our experiments in Appendix C, and present additional experiments in Appendix D.

### Offline Reinforcement Learning

**Setup.** We evaluate 7 diffusion-based offline RL algorithms with CleanDiffuser, including SynthER, Diffuser, DD, AdaptDiffuser, DQL, EDP, and IDQL, on 15 tasks in the D4RL , covering locomotion, manipulation, and navigation. We reuse the hyperparameters of the original paper as possible and give the full hyperparameters in Appendix E.3. The results are presented in Table 1.

**Key Observation. (O1)** Algorithms reproduced with CleanDiffuser have achieved, and in some cases exceeded, their official implementations. **(O2)** Diffusion planners demonstrate no superiority over diffusion policies, especially performing poorly in the Antmaze. Diffusion planners are sensitive to guided sampling and prone to generating OOD trajectories . Enhancing the dynamic legitimacy  and introducing the conservative generation  may imlock the potential of diffusion planners. **(O3)** DQL achieves outstanding performance among diffusion policies. Simply incorporating Q-maximizing loss in diffusion training shows stable and surprising performance.

### Offline Imitation Learning

**Setup.** We evaluate DiffusionPolicy and DiffusionBC with different network architectures on 22 tasks across PushT , Relay-Kitchen  and Robomimic  benchmarks. PushT and Robomimic

  
**Dataset** & **Environment** & **BC** & **SynthER** & **Diffuser** & **DD** & **AdaptDiffuser** & **DQL** & **EDP** & **IDQL** \\   & HalfCheath & \(55.2\) & \(5.94 0.0\) & \(90.3 0.1\) & \(88.9 1.9\) & \(90.4 0.1\) & \(95.5 0.1\) & \(95.8 0.1\) & \(91.3 0.6\) \\  & Regerer & \(55.2\) & \(76.6 0.4\) & \(107.2 0.9\) & \(101.04 0.6\) & \(109.3 0.3\) & \(111.1 0.4\) & \(110.8 0.4\) & \(101.0 0.7\) \\  & Wakef2d & \(107.5\) & \(110.0 0.0\) & \(107.4 0.1\) & \(108.4 0.1\) & \(107.7 0.1\) & \(\) & \(110.4 0.0\) & \(101.0 0.0\) \\   & HalfCheath & \(42.6\) & \(48.3 0.0\) & \(43.8 0.1\) & \(43.5 0.3\) & \(44.3 0.2\) & \(\) & \(\) & \(51.5 0.1\) \\  & Hopper & \(52.9\) & \(51.9 0.1\) & \(89.5 0.7\) & \(\) & \(95.5 1.1\) & \(96.5 1.3\) & \(72.6 0.2\) & \(\) \\  & Wakef2d & \(75.3\) & \(86.6 0.0\) & \(79.4 1.0\) & \(79.6 0.9\) & \(83.8 1.1\) & \(86.8 0.0\) & \(85.5 0.2\) & \(\) \\   & HalfCheath & \(36.3 0.4\) & \(36.6 0.7\) & \(42.9 0.1\) & \(36.7 0.8\) & \(\) & \(41.9 0.4\) & \(46.5 0.3\) \\  & Boger & \(18.1\) & \(24.7 0.1\) & \(91.8 0.9\) & \(92.0 0.2\) & \(91.2 0.1\) & \(\) & \(83.0 1.7\) & \(94.9 0.1\) \\  & Wakef2d & \(26.0\) & \(88.6 0.4\) & \(58.3 1.8\) & \(75.6 0.6\) & \(82.9 1.5\) & \(\) & \(87.0 2.6\) & \(89.1 2.4\) \\   Mixed \\ Partial \\  } & \(51.9\) & \(6.4\) & \(75.2\) & \(83.2\) & \(82.4\) & \(89.0\) & \(82.4\) & \(84.1\) \\   & Kitchen & \(51.5\) & \(0.0 0.0\) & \(52.5 2.5\) & \(\) & \(51.8 0.8\) & \(62.5 1.5\) & \(50.2 1.8\) & \(65.6 4.1\) \\   & Kitchen & \(38.0\) & \(0.0 0.0\) & \(55.7 1.3\) & \(56.5 5.8\) & \(55.5 0.4\) & \(63.5 1.8\) & \(40.8 1.5\) & \(\) \\   & **Average** & \(44.8\) & \(0.0\) & \(54.1\) & \(65.8\) & \(83.7\) & \(6.0\) & \(4.5\) & \(\) \\   & Antmaze-Medium & \(0.0\) & \(0.0 0.0\) & \(6.7 5.7\) & \(8.0 4.3\) & \(12.0 7.5\) & \(\) & \(73.3 6.2\) & \(62.4 5.7\) \\    & Antmaze-Large & \(0.0\) & \(0.0 0.0\) & \(17.3 1.9\) & \(0.0 0.0\) & \(5.3 3.4\) & \(\) & \(33.3 1.9\) & \(48.7 4.7\) \\    & Antmaze-Medium & \(0.0\) & \(0.0 0.0\) & \(2.0 1.6\) & \(40.2 8.6\) & \(60.3 3.3\) & \(\) & \(52.7 1.9\) & \(83.3 5.0\) \\    & Antmaze-Large & \(0.0\) & \(0.0 0.0\) & \(2.1 1.6\) & \(0.0 2.8\) & \(60.3 3.3\) & \(\) & \(52.7 1.9\) & \(83.3 5.0\) \\    & Antmaze-Large & \(0.0\) & \(0.0 0.0\) & \(73.3 2.4\) & \(0.0 0.0\) & \(8.7 2.5\) & \(61.3 8.4\) & \(41.3 3.4\) & \(60.0 1.1\) \\   **Average** \\  } & \(0.2\) & \(0.0\) & \(13.3\) & \(3.0\) & \(8.0\) & \(\) & \(50.2\) & \(59.8\) \\   

Table 1: **Evaluation Results of Offline RL Benchmark.** The performance of diffusion-based offline RL algorithms implemented by CleanDiffuser on the D4RL benchmark . Results correspond to the mean and standard error over 150 episode seeds; the highest scores are emphasized in bold.

Figure 5: **Diffuser Implementation with CleanDiffuser. The left part is a minimal code example showcasing simplicity and readability, and the right part provides a code explanation where the algorithm implementation can be entirely represented as a combination of building blocks, showing an example of various pipelines.**include both low-dim and image-based observations. To validate the imitation capabilities of the DM paradigms, we also compare the RNN-based LSTM-GMM  and the Transformer-based ACT  (reproduced). Each method is evaluated with its best-performing action space: position control for DiffusionPolicy and ACT, and velocity control for others. We reuse the hyperparameters of the original paper as much as possible, and key hyperparameters are given in Appendix E.3.

**Key Observation. (O1)** Different network architectures have a significant impact on the performance. Among them, DiffusionPolicy works better than DiffusionBC, and DiffusionPolicy with Chi_UNet1d has the best performance and training stability (Performance gap between the best checkpoint and last checkpoint). However, Chi_UNet1d has large model size and long inference time. We often need to trade-off between inference time and model performance in applications. (**O2)** Compared to popular RNN or transformer-based imitation learning algorithms, DiffusionPolicy also exhibits stronger performance, but slower inference times due to the multiple network forwards of denoise. We show detailed model size and inference time comparisons and analyses in appendix D.3.

### Impact of Diffusion Backbones and Sampling Steps

    &  &  &  &  \\   & & & DiT1d & Chi\_UNet1d & Chi\_UNet1d & Chi\_TPN & DiT1d & Pearce\_MLP \\   _Low dim_ & & & & & & & \\  pusht & 0.590/0.70 & 0.999/**1.00** & **1.00/1.00** & 0.991/**1.00** & 0.990/9.99 & 0.990/9.99 \\ pusht keypoints & 0.610/0.67 & 0.999/**1.00** & 0.991/**1.00** & **1.00/1.00** & 0.990/9.99 & **1.00/1.00** & 0.990/9.99 \\ relay-kitchen & 0.750/0.79 & 0.720/**1.00** & **1.00/1.00** & 0.990/**1.00** & 0.990/1.00 & 0.990/1.01 & 0.990/1.00 \\ lift-ph & 0.961/**1.00** & 0.981/**1.00** & **1.00/1.00** & **1.00/1.00** & **1.00/1.00** & **0.991/1.00** \\ lift-ph & 0.931/**1.00** & 0.971/**1.00** & **1.00/1.00** & **1.00/1.00** & 0.990/1.00** & 0.991/**1.00** \\ can-ph & 0.911/**1.00** & 0.920/9.98 & **1.00/1.00** & 0.991/**1.00** & 0.991/**1.00** & 0.991/**1.00** \\ cam-mh & 0.811/**1.00** & 0.900/9.98 & 0.950/**0.98** & **0.991/1.00** & 0.91/1.00 & 0.91/0.98 & 0.770/0.88 \\ square-ph & 0.730/9.59 & 0.800/9.00 & 0.850/9.00 & **0.993/0.88** & 0.796/0.60 & 0.860/7.66 & 0.640/7.66 \\ square-mh & 0.590/8.06 & 0.460/7.02 & 0.580/7.04 & **0.570/0.65** & 0.670/8.06 & 0.420/5.02 \\ transport-ph & 0.470/7.66 & 0.640/8.45 & 0.470/6.04 & **0.799/0.92** & 0.670/8.04 & 0.550/5.54 & 0.170/3.4 \\ transport-mh & 0.200/6.2 & 0.400/6.28 & 0.250/4.04 & **0.580/7.2** & 0.230/5.12 & 0.140/2.28 & 0.00/0.04 \\ totaling-ph & 0.310/6.67 & 0.640/8.2 & 0.380/5.08 & 0.720/9.00 & **0.900/9.06** & 0.490/6.66 & 0.150/3.6 \\ 
**Average** & 0.660/8.48 & 0.790/8.09 & 0.790/8.06 & **0.900/9.06** & 0.850/9.03 & 0.730/8.01 & 0.650/7.3 \\  _Image_ & & & & & & & \\  push-image & 0.540/6.69 & 0.999/**1.00** & 0.991/**1.00** & **1.00/1.00** & 0.980/9.99 & 0.100/1.9 & 0.530/6.4 \\ lift-ph & 0.961/**1.00** & **1.00/1.00** & **1.00/1.00** & **1.00/1.00** & **1.00/1.00** & **1.00/1.00** & 0.940/9.08 \\ lift-mh & 0.951/**1.00** & **1.00/1.00** & **1.00/1.00** & **1.00/1.00** & 0.990/**1.00** & **0.881/1.00** & 0.940/9.08 \\ can-ph & 0.881/**1.00** & 0.980/1.08 & 0.971/**1.00** & **0.991/1.00** & 0.981/**1.00** & 0.920/9.94 & 0.890/9.94 \\ cam-mh & 0.990/**9.89** & 0.940/9.94 & 0.900/9.22 & **0.960/9.98** & 0.890/9.94 & 0.730/8.06 & 0.760/8.04 \\ square-ph & 0.590/8.22 & 0.900/9.00 & 0.570/6.04 & **0.590/9.98** & 0.81/0.86 & 0.210/2.02 & 0.230/2.24 \\ square-mh & 0.380/6.64 & **0.840/8.04** & 0.470/6.08 & 0.839/**0.94** & 0.650/7.40 & 2.00/3.06 & 0.150/2.00 \\ transport-ph & 0.620/8.88 & 0.790/8.00 & 0.760/8.04 & 0.839/**0.94** & 0.890/9.06 & 0.70/12.02 & 0.500/6.66 \\ transport-mh & 0.240/4.04 & 0.590/**6.02** & 0.520/5.02 & **0.610/6.02** & 0.400/5.2 & 0.60/0.08 & 0.100/1.6 \\ totaling-ph & 0.490/6.86 & **0.690/7.66** & 0.590/7.02 & 0.590/6.06 & 0.390/4.4 & 0.660/1.4 & 0.060/1.0 \\ 
**Average** & 0.650/8.1 & 0.870/8.08 & 0.780/8.33 & **0.850/9.1** & 0.800/8.55 & 0.420/4.88 & 0.510/5.7 \\   

Table 2: **Evaluation Results of Offline IL Benchmark. The metrics show success rate for Robomimic and Relay-Kitchen, target area coverage for PushT. We report mean performance of last checkpoint denoted as Last and max performance of the last 10 checkpoints (3 for image tasks) denoted as Max, with each averaged over 3 seeds and 50 episodes. We show the performance of (Last / Max). *The results are obtained from the .**

Figure 6: **Impact of Diffusion Backbones and Sampling Steps. Performance of IDQL and DD with various diffusion backbones and varying sampling steps. Results correspond to the mean over 150 episode seeds.**

Although the impact of diffusion backbones and sampling steps are widely discussed in image generation, little research analyzes them in decision-making. We compare the performance of IDQL and DD, representing policies and planners, respectively, with varying diffusion backbones and sampling steps, showing results on a few tasks in Figure 6 and full results in Appendix D.2.

**Key Observation. (O1)** An anomaly where performance decreases as the sampling steps increase may happen in some tasks, known as _sampling degradation_. This anomaly has been identified in previous works [31; 3] and remains an open question. Experiments reveal that _sampling degradation_ is more likely to occur in medium-expert MuJoCo and Kitchen tasks, possibly due to narrow data distributions. Future research can investigate this issue and offer optimal choices for sampling steps. Additionally, we observe that 5 sampling steps are adequate for most tasks, suggesting that more sampling steps in previous works, e.g., 100 , are unnecessary. **(O2)** SDE solvers (DDPM, SDE-DPM-Solver++ 1) perform better in diffusion policies but suffer more from _sampling degradation_ than ODE solvers. In diffusion planners, they perform similarly and do not show a _sampling degradation_ tendency. While the impact of SDEs and ODEs in image generation has been extensively discussed [52; 46], it remains unexplored in decision-making, suggesting a need for future research. **(O3)** High-order solvers (ODE-DPM-Solver++ (2M)) show no superiority over first-order solvers.

### Impact of EMA Rate and Gradient Steps

The exponential moving average (EMA) rate significantly impacts performance . However, limited research has discussed the impact of EMA rate on diffusion-based decision-making algorithms. Previous works tend to use a lower EMA rate, e.g., 0.995 [30; 1], rather than the more common 0.9999 [61; 51; 43] used in image generation. We compare the learning curves of IDQL, DD, DiffusionBC, and DiffusionPolicy (DP) with varying EMA rates and present the results in Figure 7.

**Key Observation. (O1)** A higher EMA rate improves and stabilizes the performance during training, and also helps alleviate _training degradation_, in which model performance drops as the gradient steps increase.**(O2)** Tested algorithms can almost reach near-convergence performance with around \(5 10^{5}\) gradient steps even with a high EMA rate. Excessively long gradient steps may be unnecessary.

## 6 Conclusion

We present CleanDiffuser, the first open-sourced modularized DM library specifically for decision-making algorithms. CleanDiffuser implements diverse decoupled modules and practical features, supporting different types of DM algorithmic branches. Algorithmic pipelines can be easily implemented by combining sub-modules as simply as building blocks. Extensive experiments validate the library's reliability and versatility, benchmarking the performance of various DM algorithms for future research. We also conduct comprehensive experimental analyses on design choices of DMs, revealing the strengths and challenges of current DM methods. CleanDiffuser fills a critical gap in the current landscape by providing a unified library. We believe CleanDiffuser lays a solid cornerstone for applying DMs to decision-making tasks and will catalyze further rapid progress in this promising field. We indicate some limitations, challenges, and future directions in Appendix H.

Figure 7: **Impact of EMA Rate and Gradient Steps. Learning curve of IDQL, DD, DiffusionBC, and DP with varying EMA rates. Results correspond to the mean and standard error over 150 episode seeds.**

Acknowledgements

This work is supported by the National Natural Science Foundation of China (Grant Nos. 62422605, 92370132).