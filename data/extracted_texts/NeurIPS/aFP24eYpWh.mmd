# A Generative Model of Symmetry Transformations

James Urquhart Allingham

University of Cambridge

jua23@cam.ac.uk

&Bruno Kacper Mlodozeniec

University of Cambridge

MPI for Intelligent Systems, Tubingen

bkm28@cam.ac.uk

Shreyas Padhy

University of Cambridge

sp2058@cam.ac.uk

&Javier Antoran

University of Cambridge

Angstrom AI

ja666@cam.ac.uk

&David Krueger

University of Cambridge

david.scott.krueger@gmail.com

Richard E. Turner

University of Cambridge

ret26@cam.ac.uk

&Eric Nalisnick

University of Amsterdam

e.t.nalisnick@uva.nl

&Jose Miguel Hernandez-Lobato

University of Cambridge

jmh233@cam.ac.uk

###### Abstract

Correctly capturing the symmetry transformations of data can lead to efficient models with strong generalization capabilities, though methods incorporating symmetries often require prior knowledge. While recent advancements have been made in learning those symmetries directly from the dataset, most of this work has focused on the discriminative setting. In this paper, we take inspiration from group theoretic ideas to construct a generative model that explicitly aims to capture the data's _approximate_ symmetries. This results in a model that, given a prespecified broad set of possible symmetries, learns to what extent, if at all, those symmetries are actually present. Our model can be seen as a generative process for data augmentation. We provide a simple algorithm for learning our generative model and empirically demonstrate its ability to capture symmetries under affine and color transformations, in an interpretable way. Combining our symmetry model with standard generative models results in higher marginal test-log-likelihoods and improved data efficiency.

## 1 Introduction

Figure 1: **Left: An example of a symmetry-aware generative process that we aim to model in this paper. A _prototype_\(}\) () is transformed by \(_{}\) into an observation \(\) (). The transformation—e.g., rotation—is parameterized by \(\)—e.g., an angle. Right: The corresponding orbit—i.e., the set of all possible instances of \(\) that can result from applying \(_{}\)—with a few elements shown. Under this generative process, the prototype is an arbitrary orbit element. Each element in the orbit has a probability \(p(})\) induced by \(p(})\). E.g., for handwritten ‘3’s, we expect digits in an upright orientation with some rotation around, say \( 40^{}\), corresponding to natural variations in handwriting.**Many physical phenomena exhibit symmetries; for example, many of the observable galaxies in the night sky share similar characteristics when accounting for their different rotations, velocities, and sizes. Hence, if we are to represent the world with generative models, they can be made more faithful and data-efficient by incorporating notions of symmetry. This has been well-understood for discriminative models for decades. Incorporating inductive biases such as invariance or equivariance to symmetry transformations dates back (at least) to ConvNets, which incorporate translation symmetries (LeCun et al., 1989)--and can be extended to reflection and rotation (Cohen and Welling, 2016)--and more recently, transformers, with permutation symmetries (Lee et al., 2019).

In many cases, it is not known _a priori_ which symmetries are present in the data. Learning symmetries in discriminative modeling is an active field of research (Nalisnick and Smyth, 2018; van der Wilk et al., 2018; Benton et al., 2020; Schwobel et al., 2021; van der Ouderaa and van der Wilk, 2022; Rommel et al., 2022; Romero and Lohit, 2022; Immer et al., 2022, 2023; Miao et al., 2023; Mlodozeniec et al., 2023). However, in these works--which focus on invariant discriminative models--the label is often assumed to be invariant, and thus, the symmetry information can be _removed_ rather than explicitly modeled. On the other hand, a generative model _must_ capture the factors of variation corresponding to the symmetry transformations of the data. Doing so can provide benefits such as better representation learning--by disentangling symmetry from other latent variables (Antoran and Miguel, 2019)--and data efficiency--due to compactly encoding of factor(s) of variation corresponding to symmetries. Furthermore, learning about underlying symmetries in data could be used for scientific discovery.

We propose a generative model that explicitly encodes the (partial) symmetries in the data. Here, we are primarily interested in using this model to inspect the distribution over naturally occurring transformations for a given example \(\), and resample new "naturally" augmented versions of the example. Our contributions are

1. We propose a Symmetry-aware Generative Model (SGM). The SGM's latent representation is separated into an invariant component \(}\) and an equivariant component \(\). The latter, \(\), captures the symmetries in the data, while \(}\) captures none. We recover \(\) by applying a parameterised transformation, \(=_{}(})\). We call \(}\) a _prototype_ since each \(}\) can produce arbitrarily transformed observations; see Figure 1.
2. We propose a two-stage algorithm for learning our SGM: first learning \(}\) using a self-supervised approach and then learning \(\) via maximum likelihood. Importantly, this does not require modeling the distribution of prototypes \(p(})\), allowing the procedure to remain tractable even for complex data.
3. We verify experimentally that our SGM completely captures affine and color symmetries. A VAE's marginal test-log-likelihood can improved by using our SGM to incorporate symmetries. Additionally, unlike a standard VAE, explicitly modeling symmetries makes our VAE-SGM hybrid robust to deleting half of the dataset.

Notation.We use \(a\), \(\), and \(\) (i.e., lower, bold lower, and bold upper case) for scalars, vectors, and matrices, respectively. We distinguish between random variables such as \(\), \(\), \(\), and their realizations \(\), \(\), \(\). Thus, for continuous \(\), \(p()\) is a PDF that returns a density \(p(=)=p()\). We use \(\) to represent function composition, e.g., \(f_{1} f_{2}\).

## 2 Symmetry-aware Generative Model (SGM)

Consider a dataset of observations \(\{_{n}\}_{n=1}^{N}\) on a space \(\), and a collection \(\{_{}\}\) of transformations \(_{}:\) parameterised by transformation parameters \(^{d_{}}\). We assume \(\{_{}\}_{}\) (abbreviated \(\{_{}\}\)) form a group. Loosely, our aim is to model the distribution over transformations present in the data. To do so, we model the distribution \(p()\) by decomposing it into two disparate parts: **(1)** a distribution over prototypes and **(2)** a distribution over parameters controlling transformations to be applied to a prototype. Concretely, we specify our generative model as follows (also depicted in Figure 2):

\[}  p(}),\] (1) \[  p_{}(\,|\,})\,,\] (2) \[ =_{}(}).\] (3)

[MISSING_PAGE_FAIL:3]

### Learning

We now discuss learning for the two NNs required by our model, \(f_{}()\) and \(p_{}(\,|\,})\). In Appendix A, we connect our learning algorithm with MLL optimization using an ELBO.

Transformation inference function.For \(_{}^{-1}\), with \(\) given by \(f_{}\), to map \(\) to a prototype \(}\), it must, by definition, map all elements in any given orbit to the same element in that orbit. In other words, the output of \(_{f_{}()}^{-1}()\) should be _invariant_ to transformations \(_{^{}}\) of \(\):

\[_{f_{}()}^{-1}()=_{f_{ }(_{^{}}())}^{-1}(_{ ^{}}()),\ \ ^{}.\] (4)

To learn such a function, we optimize for this property directly. To this end, we sample transformation parameters \(_{}\) from some distribution over parameters \(p(_{})\). This allows us to get random samples \(_{}:=_{_{}}()\) in the orbit of any given element \(\). Since we want full (i.e., strict) invariance, \(p(_{})\) must have support on the entire orbit [van der Ouderaa and van der Wilk, 2022]. We then learn an equivariant via a self-supervised learning (SSL) scheme\(f_{}\)3 inspired by methods like BYOL [Grill et al., 2020] and, more directly, BINCE [Dubois et al., 2021]. For example, we could use the objective illustrated in Figure 4:

\[\|_{f_{}(_{})}^{-1}(_{})-_{f_{}()}^{-1}()\|_{2}^{2}, _{}=_{_{}}(),\,_{ } p(_{}).\] (5)

Our actual objective differs slightly. Since \(_{^{}}(^{})=_{^{ }}(^{})\) implies \(^{}=_{^{}}^{-1}_{^{}}(^{})\), we use

\[\|_{f_{}()}_{f_{}( _{})}^{-1}(_{})-\|_{2}^{2}.\] (6)

This change allows us to reduce the number of small discretization errors introduced with each transformation application by replacing repeated transformations with a single composed transformation; see Section 3.1 for further discussion. Our SSL loss is given in line 1 of Algorithm 1.

Generative model of transformations.Once we have a prototype inference function, we simply learn \(p_{}(\,|\,})\) by maximum likelihood on the created data pairs \(\{f_{}(_{i}),_{f_{}(_{i})} ^{-1}(_{i})\}\). This is shown in line \(8\) of Algorithm 1. While we need to specify the kinds of symmetry transformations \(_{}\) we expect to see in the data, by learning \(p_{}(\,|\,})\) the model can learn the degree to which those transformations are present in the data. Thus, we can specify several potential symmetry transformations and learn that some are absent in the data. Furthermore, the required prior knowledge (the support of \(p(_{})\)) is small compared to what our SGM can learn (the shapes of the distributions for each of the _present_ transformations).

Since we are primarily interested in using the model to **(a)** inspect the distribution over naturally occurring transformations for a given element \(\), and **(b)** resample new "naturally" augmented versions of the element, we _do not_ need to learn \(p(})\). We can do **(a)** by querying \(p(\,|\,}=})\) for \(}:=_{f_{}(})}^{-1}()\), and we can do **(b)** by sampling \( p(\,|\,})\) and transforming the \(}\) to get \(:=_{}(})\). Of course, if one wanted to sample new prototypes, one could fit \(p_{}(})\) using, e.g., a VAE. Not learning \(p(})\) greatly simplifies training for complicated datasets that would otherwise require a large generative model, an observation made by Dubois et al. .

## 3 Practical Considerations and Further Motivations

Training our SGM, while simple, has potential pitfalls in practice. We discuss the key considerations in Section 3.1 and provide further recommendations in Appendix B. We then provide motivation for several of our modeling choices in Section 3.2.

### Practical Considerations

Working with transformations.Repeated application of transformations--e.g., in Figure 4--can introduce unwanted artifacts such as blurring. For many useful transformations, we can compose transformations before applying them. For affine transformations of images, for example, we can directly multiply affine-transformation matrices. More generally, if there is some representation of the transformation parameters \(T()\) where composition can be performed--e.g., as matrix multiplication \(_{_{2}}_{_{1}}= ^{}_{T(_{2})T(_{1})}\) in the case where \(T\) is a group representation--then we recommend composing transformations in that space to minimize the number of applications.

Partial invertibility.In many common settings, transformations are not fully invertible. We encounter two such issues when working with affine transformations of images living in a finite, discrete coordinate space. Firstly, affine transformations are only _approximately_ invertible in the discrete space due to the information loss when interpolating the transformed image onto a discrete grid. Thus, while only a single prototype \(}\) exists for any \(\), it may not be clear what the correct prototype is. Secondly, transformations can cause information loss due to the finite coordinate space (e.g., by shifting the contents of the image out-of-bounds4). If appropriate bounds are known _a priori_, we can prevent severe information loss by constraining \(_{}\) and \(_{}\) using \(\), \(\), and \(\) bijectors. Alternatively, we can augment the SSL loss in Algorithm 1 with an _invertibility loss_

\[_{}()=( ,^{-1}_{()} (_{()}())).\] (7)

Learning \(p_{}(\,|\,})\) with imperfect inference.In practice, our transformation inference network \(f_{}()\) will not be perfect; see Figure 10. Even after training, there may be small variations in the prototypes \(}\) corresponding to different elements in the orbit of \(\). To make \(p_{}(_{}\,|\,})\) robust to these variations, we train it with prototypes corresponding to _randomly transformed_ training data points. I.e., we modify the MLE objective in Algorithm 1 as \( p_{}(_{}\,|\,}^{ })\), where \(}^{}=^{-1}_{_{}(_{_{}()}) }(_{_{}()})\) as in our SSL objective. Averaging the loss over multiple samples--e.g., 5--of \(_{}\) is beneficial.

### Modelling Choices

We now motivate some of the design choices for our SGM by means of illustrative examples. In each case, we assume that \(_{}\) is counter-clockwise rotation; thus, \(\) is the angle.

1. The distribution \(p_{}(\,|\,})\) is implemented as a normalising flow.Consider a dataset of '8's rotated in the range \(-30^{}\) to \(30^{}\): {\(\$, , \$, $, \(\)}\). Let us assume that the prototype is '8'. Figure 4(a) shows \(p(\,|\,,\,})\), an example of the true distribution for \(\) given \(\) and \(}\), for several observations, under the data generating process5. These distributions are composed of deltas because

[MISSING_PAGE_FAIL:6]

of an observed example. For example, given an example of a digit '3', we want to know the probability of observing, that digit rotated by -\(90^{}\). Assuming we can find a prototype \(}\) we would like \(p(|}=}.)\) to represent all naturally occurring augmentations. Unless \(}\) is unique, this won t necessarily be the case, as illustrated in Figure 7.

## 4 Experiments

In Section 4.1, we explore our SGM's ability to learn symmetries. We show that it produces valid prototypes, and generates plausible samples from the data distribution, given those prototypes. Then, in Section 4.2, we leverage our SGM to improve data efficiency in deep generative models.

We conduct experiments using three datasets--dSprites (Matthey et al., 2017), MNIST, and GalaxyMNIST (Wamsley et al., 2022)--and two kinds of transformations--affine and color. In Section 4.1, when working with MNIST under affine transformations, we add a small amount of rotation (in the range\([-15^{},15^{}]\)) to the original data to make rotations in the figures easier to see. For MNIST under color transformations, we first convert the grey-scale images to color images using only the red channel. We then add a random hue rotation in the range \([0,0.6]\) and a random saturation multiplier in the range \([0.6,0.9]\). In the case of dSprites, we carefully control the rotations, positions, and sizes of all of the sprites. For example, in the case of the heart sprites, we have removed the rotations and set the \(y\)-positions to be bimodal in the top and bottom of the images. Further details about the dSprites setup, as well as all other experimental details, can be found in Appendix C. We focus on learning affine transformations (shifting, rotation, and scaling) as they are expressive while still being a group that is easy to work with. We also learn color transformations (hue, saturation, and value). See Appendix C.7 for details about how we parameterize \(_{}\) in both cases.

### Learning Symmetries

Exploring transformations and prototypes.Figure 8 shows that for both datasets and kinds of transformations we consider, our SGM produces close-to-invariant prototypes as well as realistic "natural" examples that are almost indistinguishable from test examples. There are sev

Figure 8: **Top: samples from the test set. **Mid:** prototypes for each test example. **Bot:** resampled versions of each test example given the prototype. Prototypes for examples from the same orbit (and in some cases from distinct but similar orbits) match (e.g., their size, position, rotation, etc. are similar). Resampled examples are usually indistinguishable from test examples.

eral illustrative examples which bear further discussion. The heart sprites in Figure 7(a) show that our SGM was able to learn _the absence_ of a transformation (namely rotation) in the dataset. As expected, all of the prototypes for the sprites of the same shape are the same, since these shapes are in the same orbit as one another. This behaviour is also demonstrated for MNIST digits in Figures 19 and 20. The '6', '8', and '9' digits in Figure 7(b) demonstrate the ability of our SGM to learn bimodal distributions (on rotation in this case). The figure's third '7' is interesting because our SGM interprets it as a '2'.

Flexibility is important.In \(\), each dimension corresponds to a different transformation. We refer to \(p_{}(_{i}\,|\,)\) as the marginal distribution of a single transformation parameter. Figure 9 shows these marginal learnt distributions for several digits from Figure 7(b). We see that each of the parameters has its own range and shapes. For rotations, which are easy to reason about, we see distributions that make sense--the round '0' has an almost uniform distribution over rotations, and the '1' and one of the '9's are strongly bimodal as expected. The other '9', which does not look as much like an upside-down '6', has a much smaller 2nd mode. The '2', which looks somewhat like an upside-down '7', is also bimodal. We see that prototypes of different sizes result in corresponding distributions over scaling parameters with different ranges. Figure 21 provides additional examples for MNIST with affine transformations, while Figure 22 provides the same for color transformations, and Figure 23 investigates the distributions for dSprites. These results provide experimental evidence of the need for flexibility in the generative model for \(p_{}(\,|\,)\), as conjectured in Section 3.2. We also find significant dependencies between dimensions of \(\) (e.g., rotation and translation in dSprites).

Invariance of \(f_{}\) and the prototypes.In Figure 10, we investigate the imperfections of the inference network by considering an iterative procedure in which prototypes are treated as observed examples, allowing us to infer a chain of successive prototypes. We show several examples of such chains, as well as the average magnitude of the transformation parameters at each iteration, normalized by the maximum magnitude (at iteration 0). The first prototype \(}_{1}\) is most different from the previous \(}_{0}=\), with successive prototypes being similar visually and as measured by the magnitude of the inferred transformation parameters. However, the magnitude of the inferred parameters does not tend towards 0, rather plateauing at around 5% of the maximum. This highlights that, although simple NNs can learn to be approximately invariant, a natively invariant architecture has the potential to improve performance.

### VAE Data Efficiency

We use SGM to build data-efficient and robust generative models. In Figure 11, we compare a standard VAE to two VAE-SGM hybrid models--"AugVAE" and "InvVAE"--for different amounts of training data and added rotation of the MNIST digits. When adding rotation, each \(\) in the dataset set is always rotated by the same angle (sampled uniformly between \(_{}\), the maximum added rotation angle). Thus, adding rotation here is _not_ data augmentation. AugVAE is a VAE that uses our SGM to re-sample transformed examples \(^{}=_{|}}(})\), introducing data augmentation at training time. InvVAE is a VAE that uses our SGM to convert each example \(\) to its prototype \(}\) at both train and test time. That is, the VAE in InvVAE sees only the invariant representation of each example. We also compare against a VAE trained with standard data augmentation6. We use test-set importance-weighted lower bound (IWLB) (Domke and Sheldon, 2018) of \(p()\), estimated with 300 samples of the VAE's latent variable \(\), and \(\) for InvVAE, to compare the models. Reconstruction error is provided in Appendix E. Further details--e.g., hyperparameter sweeps--are in Appendix C.

Figure 10: Iterative prototype inference. **Left:** starting with a test example \(\), we get a prototype \(}_{1}\), then treating prototype \(}_{i}\) as an observed example we predict the next prototype \(}_{i+1}\). **Right:** The average magnitude of the transformation parameters as a function of iterations of this process.

Figure 9: From left to right, test examples, their prototypes, and the corresponding marginal distributions \(p_{}(_{i}\,|\,)\) over translation in \(x\), translation in \(y\), rotation, scaling in \(x\), and scaling in \(y\).

[MISSING_PAGE_FAIL:9]

architecture-agnostic self-supervised invariance learning method. Balestriero et al. (2022); Miao et al. (2023); Bouchacourt et al. (2021) show that learned symmetries (i.e., data augmentation) should be class-dependent, much like our transformations are prototype-dependent.

Symmetry-aware latent spaces.Encoding symmetries in latent space is well-studied. Higgins et al. (2018) posit that symmetry transformations that leave some parts of the world invariant are responsible for exploitable structure in any dataset. Thus, agents benefit from _disentangled_ representations that separate out these transformations. Winter et al. (2022) split the latent space of an auto-encoder into invariant and equivariant partitions. However, they rely on geometric NN architectures, contrasting with our self-supervised learning approach. Furthermore, they do not learn a generative model--they reconstruct the input exactly--thus, they cannot sample new observations given a prototype. Xu et al. (2021) propose group equivariant subsampling layers that allow them to construct autoencoders with equivariant representations. Shu et al. (2018) propose an autoencoder whose representations are split such that the reconstruction of an observation is decomposed into a "template" (much like our prototypes) and a spatial deformation (transformation).

In the generative setting, Louizos et al. (2016) construct a VAE with a latent space that is invariant to pre-specified sensitive attributes of the data. However, these sensitive attributes are observed rather than learned. Similarly, Alice et al. (2023) construct a VAE with a partitioned latent space with a component that is invariance spurious factors of variation in the data. Bouchacourt et al. (2018); Hosoya (2019) learn VAE with two latent spaces--a per-observation equivariant latent and an invariant latent shared across grouped examples. Other works have constructed rotation equivariant (Kuzina et al., 2022) and partitioned equivariant and invariant (Vadgamma et al., 2022) latent spaces. Antoran and Miguel (2019); Ise et al. (2020) split the latent space of a VAE into domain, class, and residual variation components. The first of which can capture rotation symmetry in hand-written digits. Unlike us, they require class labels and auxiliary classifiers. Keller and Welling (2021) construct a VAE with a topographically organised latent space such that an approximate equivariance is learned from sequences of observations. In contrast to the works above, Bouchacourt et al. (2021) argue that learning symmetries should not be achieved via a partitioned latent space but rather learning _equivariant operators_ that are applied to the whole latent space. Finally, while Nalisnick and Smyth (2017) do not learn symmetries, their _information lower bound_ objective is reminiscent of several works above--and our own, see Appendix A--in minimizing the mutual information between two quantities when learning a prior.

Self-supervised Equivariant Learning[Dangovski et al., 2022] generalize standard invariant SSL methods to produce representations that can be either insensitive (invariant) or sensitive (equivariant) to transformations in the data. Similarly, Eastwood et al. (2023) use a self-supervised learning approach to disentangle sources of variation in a dataset, thereby learning a representation that is equivariant to each of the sources while invariant to all others.

## 6 Conclusion

We have presented a Symmetry-aware Generative Model (SGM) and demonstrated that it is able to learn, in an unsupervised manner, a distribution over symmetries present in a dataset. This is done by modeling the observations as a random transformation of an invariant latent _prototype_. This is the first such model we are aware of. Building generative models that incorporate this understanding of symmetries significantly improves log-likelihoods and data sparsity robustness. This is exciting in the context of modern generative models, which are close to exhausting all of the data on the internet. We are also excited about the use of SGM for scientific discovery, given that the framework is ideal for probing for naturally occurring symmetries present in systems. For example, we could apply SGM to marginalize out the idiosyncrasies of different measuring equipment and observation geometry in radio astronomy data. Additionally, given the success of using our SGM for data augmentation when training VAEs, it would be interesting to apply it to data augmentation in discriminative settings and compare it with methods such as Benton et al. (2020); Miao et al. (2023).

The main limitation of our SGM is that it requires specifying the super-set of possible symmetries. Future work might relax this requirement or explore how robust our SGM is to even larger sets. Furthermore, care must sometimes be taken when specifying the set of symmetries. For example, when rotating to images with "content" up to the boundaries of the image; see Appendix E.2.