# Is Score Matching Suitable for Estimating Point Processes?

Haoqun Cao\({}^{1}\), Zizhuo Meng\({}^{2}\), Tianjun Ke\({}^{1}\), Feng Zhou\({}^{1,3}\)

\({}^{1}\)Center for Applied Statistics and School of Statistics, Renmin University of China

\({}^{2}\)Data Science Institute, University of Technology Sydney

\({}^{3}\)Beijing Advanced Innovation Center for Future Blockchain and Privacy Computing

hcao65@wisc.edu, feng.zhou@ruc.edu.cn

Corresponding author.

###### Abstract

Score matching estimators have gained widespread attention in recent years partly because they are free from calculating the integral of normalizing constant, thereby addressing the computational challenges in maximum likelihood estimation (MLE). Some existing works have proposed score matching estimators for point processes. However, this work demonstrates that the incompleteness of the estimators proposed in those works renders them applicable only to specific problems, and they fail for more general point processes. To address this issue, this work introduces the weighted score matching estimator to point processes. Theoretically, we prove the consistency of our estimator and establish its rate of convergence. Experimental results indicate that our estimator accurately estimates model parameters on synthetic data and yields results consistent with MLE on real data. In contrast, existing score matching estimators fail to perform effectively. Codes are publicly available at https://github.com/KenCao2007/WSM_TPP.

## 1 Introduction

Point processes are a class of statistical models used to characterize event occurrences. Typical models include Poisson processes  and Hawkes processes . Their applications span various fields such as seismology [16; 17], finance [1; 4], criminology , and neuroscience [11; 26]. In the field of point processes, maximum likelihood estimation (MLE) has been a conventional estimator. However, MLE has an inherent limitation: it requires the computation of the normalizing constant, which corresponds to the intensity integral term in the likelihood. Except for simple cases, calculating the intensity integral analytically is generally infeasible. This necessitates the use of numerical integration methods like Monte Carlo or quadrature for approximating the computation. This introduces approximation errors, and more importantly, for high-dimensional problems, numerical integration encounters the curse of dimensionality, rendering training infeasible.

To address this issue, prior research has introduced the concept of score matching (SM)  to the field of point processes. For instance,  derived the application of SM to the estimation of traditional statistical Poisson processes. Furthermore,  extended the use of SM to the estimation of deep covariate spatio-temporal point processes.  also generalized the application of SM to Hawkes processes. These works have greatly advanced the utilization of SM for point processes. However, in practical applications, we have found that these estimators only work for specific point processes. For more general cases, these estimators cannot accurately estimate model parameters, even for some simple statistical point processes. One of the core contribution of this work is to _theoretically_ demonstrate the incompleteness of the estimators proposed in the aforementioned studies.

The incompleteness of the estimators in the aforementioned studies stems from the transition from explicit SM to implicit SM. The explicit SM estimates model parameters by minimizing the expected distance between the gradient of the log-density of the model and the gradient of the log-density of the data. However, we cannot directly minimize the above objective function since it depends on the unknown data distribution. To facilitate solving, we need to convert the above explicit SM to implicit SM by using a trick of integration by parts, provided that some regularity conditions are satisfied . In [18; 24; 10], they assume that the required regularity conditions are satisfied in their point process models and directly employ the implicit SM objective. However, as demonstrated in Section 3, the required regularity conditions cannot be met for general point processes. This implies that the concise implicit SM objectives (Equation (2) in , Equation (10) in , Equation (4) in ) are incomplete, and they cannot accurately estimate parameters for general point processes.

To address this issue, this work introduces a (autoregressive) weighted score matching (WSM) estimator that can be applied to more general point processes. WSM eliminates the intractable terms in SM objective by adding a weight function that takes zero at the boundary of the integration region. Compared to previous work on WSM [6; 22; 12], we are the first work to apply WSM on a stochastic process where the dimension \(N_{T}\) is also random. This stochasticity in dimensionality poses greater challenges to the derivation, requiring special treatment to address this issue.

Specifically, we make following contributions: **(1)** We theoretically demonstrate that implicit (autoregressive) SM estimators in [18; 24; 10] are incomplete because the required regularity conditions cannot be satisfied for general point processes. **(2)** To address this issue, we propose a (autoregressive) WSM estimator that is applicable to general point processes. Theoretically, we establish its consistency and convergence rate. **(3)** In experiments, we confirm that on synthetic data, (autoregressive) WSM successfully recovers the ground-truth parameters; on real data, (autoregressive) WSM estimates results consistent with MLE; while existing (autoregressive) SM estimator fails in both scenarios.

## 2 Preliminaries

Now we provide knowledge on Poisson and Hawkes processes and (autoregressive) score matching.

### Poisson Process and Hawkes Process

The Poisson process  is a stochastic point process that models the occurrence of events over a time window \([0,T]\). A trajectory from Poisson process can be represented as an ordered sequence \(=(t_{1},,t_{N_{T}})\) where \(N_{t}=\{n:t_{n} t,t[0,T]\}\) is the corresponding counting process and thus \(N_{T}\) is the random number of events in \([0,T]\). The inhomogeneous Poisson process has a time-varying intensity \((t)\) representing the instantaneous rate of event occurrence at \(t\). Mathematically, the intensity function is defined as \((t)=_{_{t} 0}[N_{t+_{t}}-N_{t}]/_{t}\). The probability density function of Poisson process is:

\[p()=_{n=1}^{N_{T}}(t_{n})(-_{0}^{T} (t)dt).\] (1)

The Hawkes process  is a self-excitation point process where the occurrence of an event increases the likelihood of more events in the future. A trajectory from Hawkes process is similarly represented as \(=(t_{1},,t_{N_{T}})\) on \([0,T]\). The conditional intensity function of Hawkes process, representing the instantaneous rate of event occurrence at \(t\) given the history up to but not including \(t\), is:

\[^{*}(t)=(t|_{t^{-}})=(t)+_{t_{j}<t}g(t-t_{j}),\] (2)

where \((t)\) is the baseline intensity, \(g()\) is the triggering kernel representing the self-excitation effect, the summation expresses the accumulative excitation from all past events, \(_{t^{-}}\) is the historical information up to but not including \(t\). \(^{*}(t)\) means the intensity is dependent on the history.

Poisson process assumes the independence of event occurrences, while Hawkes process extends it by introducing an autoregressive structure, making subsequent events dependent on prior events. Giventhe history \(_{t_{n}}=(t_{1},,t_{n})\), the conditional probability density of \((n+1)\)-th event at \(t>t_{n}\) is:

\[p(t|_{t_{n}})=^{*}(t)(-_{t_{n}}^{t}^{*}( )d).\] (3)

Here, we introduce the definition of the univariate Hawkes process. However, multivariate Hawkes processes also exist. For ease of notation, we use the univariate case for illustration, but we also provide solutions for the multivariate case.

### Score Matching

MLE is a classic estimator that minimizes Kullback-Leibler divergence between a model distribution and data distribution. However, a drawback is the intractable computation of the normalizing constant. Approximating it through numerical integration can be computationally demanding. In contrast, SM  offers an alternative by minimizing Fisher divergence between model and data distributions:

\[_{}()=_{p()}\| _{} p()-_{} p_{}( )\|^{2},\]

where \(p()\) represents the data distribution, \(p_{}()\) is the parameterized model distribution, the gradient of the log-density is called the score, and \(\|\|\) represents a suitable norm, such as the \(^{2}\) norm. Minimizing the Fisher divergence above provides the parameter estimate. The advantage of SM lies in its ability to bypass the computation of the normalizing constant since the score no longer contains this constant: \(p_{}()=_{}()\) where \(Z()=_{}()d\), \(_{} p_{}()=_{} {p}_{}()\).

Under certain conditions, we can use integration by parts to replace the explicit SM objective, which involves an unknown distribution \(p()\), with an equivalent implicit one,

\[_{}()=_{p()}[ \|_{} p_{}()\|^{2}+(_ {}^{2} p_{}())].\] (4)

### Autoregressive Score Matching

An autoregressive model defines a probability density \(p()\) as a product of conditionals using the chain rule: \(p()=_{n=1}^{N}p(x_{n}|_{<n})\), where \(x_{n}\) is the \(n\)-th entry and \(_{<n}\) denotes the entries with indices smaller than \(n\). The original SM is not suitable for autoregressive models because the autoregressive structure introduces challenges in gradient computation in Equation (4). To address this issue,  proposed autoregressive score matching (ASM). Unlike SM, which minimizes the Fisher divergence between the joint distributions of the model \(p_{}()\) and the data \(p()\), ASM minimizes the Fisher divergence between the conditionals of the model \(p_{}(x_{n}|_{<n})\) and the data \(p(x_{n}|_{<n})\):

\[_{}()=_{n=1}^{N}_{p( _{ n})}(|_{<n})}{  x_{n}}-(x_{n}|_{<n})}{  x_{n}})^{2}.\]

Similarly, the above explicit ASM objective involves an unknown distribution \(p(x_{n}|_{<n})\). Under specific regularity conditions, we can apply integration by parts to derive an implicit ASM objective:

\[_{}()=_{n=1}^{N}_{p(_{  n})}[((x_{n}| _{<n})}{ x_{n}})^{2}+ p_{}(x_{n}| _{<n})}{ x_{n}^{2}}].\] (5)

## 3 Score Matching for Poisson Process

We analyze the application of SM for Poisson process and its failure in achieving consistent estimation. Subsequently, we propose a provably consistent WSM estimator.

### Failure of Score Matching for Poisson Process

Consider a Poisson process \(=(t_{1},,t_{N_{T}})\) on \([0,T]\). Let \(p()\) represent the data distribution, which is uniquely associated with an intensity function \((t)\). Let \(p_{}()\) represent the parameterizedmodel distribution, which is uniquely associated with a parameterized intensity function \(_{}(t)\). In the following, we denote the score as \((t_{n})=} p()\).

Previous works [18; 24] have both attempted to apply SM to the Poisson process:

\[_{}()=_{p()}[ _{n=1}^{N_{T}}((t_{n})-_{}(t_{n}))^{2}].\] (6)

In order for SM to be practical, [18; 24] assumed that specific regularity conditions are satisfied. Therefore, they employed an implicit SM objective similar to Equation (4):

\[_{}()=_{p()}[_{n=1}^{ N_{T}}_{}^{2}(t_{n})+)}{ t _{n}}].\] (7)

In practical applications, we have found that the above estimator works only for specific Poisson processes and fails for more general Poisson processes. The reason for its failure lies in the fact that, for more general Poisson processes, the specific regularity conditions cannot be satisfied.

Such conditions require the probability density function of the random variable is zero when it approaches infinity in any of its dimensions. However, for point processes, such requirement is not satisfied, because the random variable in point process \(=(t_{1},,t_{N_{T}})\) is not of fixed dimension and takes values in a subset of \(_{+}^{N_{T}}\). Therefore, for general Poisson processes, we cannot derive the implicit SM in Equation (7) based on the explicit SM in Equation (6).

**Proposition 3.1**.: _Assume that all functions and expectations in \(_{}()\) and \(_{}()\) are well defined, we have,_

\[_{}()= _{}()+-_{N=1}^{}  p(t_{1},,t_{N})(t_{1},,t_{N})}{  t_{1}}_{t_{1}=0}d_{2:N}\] (8) \[+_{N=1}^{} p(t_{1},,t_{N})(t_{1},,t_{N})}{ t_{N}}_{t_{N}=T}d_{ 1:N-1}.\]

_Therefore, \(_{}()\) is equivalent to \(_{}()\) if and only if the sum of the last two terms is a constant not containing \(\)._

For specific Poisson processes, the sum of the last two terms can be zero. However, for more general cases, this sum contains \(\). This implies that \(_{}\) fails for general Poisson processes.

### Weighted Score Matching

To address the situation where SM fails, inspired by [6; 22], we introduce the WSM for Poisson process. The core idea of WSM is to eliminate the two intractable terms by adding a weight function that takes zero at the boundary of the integration region. The weight function is designed to be a vector-valued function \(:_{+}^{N_{T}}_{+}^{N_{T}}\) with the \(n\)-th element denoted as \(h_{n}()\). Here, we present the conditions that a valid weight function should satisfy:

\[_{t_{n} t_{n+1}}p()_{ }(t_{n})h_{n}()=0,\ _{t_{n} t_{n-1}}p()_{}(t_{n})h_{n}()=0,\  n[N_{T}],\\ [_{}^{2}(t_{n})h_{n}()]<,\ [_{ }(t_{n})()}{ t_{n}}]<,  n[N_{T}].\] (9)

One can verify that such weight functions are easy to find for most \(p()\) and \(_{}(t_{n})\). With a valid weight function \(\), the explicit WSM objective can be defined as:

\[_{}()=_{p()}[ _{n=1}^{N_{T}}((t_{n})-_{}(t_{n}))^{2}h_{n}() ].\] (10)

The introduction of the weight function allows control over the values of the integrand at the boundaries of the integration domain, thereby eliminating the last two terms in Equation (8).

**Theorem 3.2**.: _Assume the true intensity is in the family of the model intensity, denoted as \((t)=_{^{*}}(t)\), where \(^{*}\). We further assume that \(}(t)}{ t}=}(t)}{ t}\ a.s.\) gives \(_{1}=_{2}\). Then the unique minimizer of \(_{}()\) is \(^{*}\)._

The explicit WSM objective is not practical as it depends on the unknown data distribution \(p()\), so we further derive the implicit WSM objective which is tractable.

**Theorem 3.3**.: _Assume that all functions and expectations in \(_{}()\) and \(_{}()\) are well defined, Equation (9) is satisfied, we have,_

\[_{}()=_{}()+,\]

\[_{}()=_{p()}[_{n=1}^ {N_{T}}_{}^{2}(t_{n})h_{n}()+(t_{n})}{ t_{n}}h_{n}()+_{}(t_{n}) ()}{ t_{n}}].\] (11)

For general Poisson processes, Equation (11) is always valid with a suitable weight function. Thus, we do not need to worry about the issues of failure that may arise when using Equation (7).

## 4 Autoregressive Score Matching for Hawkes Processes

Similarly, we analyze the usage of ASM for Hawkes processes and its failure in achieving consistent estimation. Subsequently, we propose a provably consistent autoregressive WSM (AWSM) estimator.

### Failure of Autoregressive Score Matching for Hawkes Process

The original SM, even when adjusted by a weight function, is not suitable for point processes with autoregressive structures, such as Hawkes process. Because in such cases, directly calculating the score still includes the intensity integral, which is precisely what the use of SM aims to avoid. Therefore, an ASM method is proposed for parameter estimation for Hawkes process in .

Consider a Hawkes process \(=(t_{1},,t_{N_{T}})\) on \([0,T]\) with the underlying conditional probability density of \(t_{n}\) denoted as \(p(t_{n}|_{t_{n-1}})\). The parameterized conditional probability density model of \(t_{n}\) is \(p_{}(t_{n}|_{t_{n-1}})\). We denote the conditional score as \((t_{n}|_{t_{n-1}})=} p(t_{n} |_{t_{n-1}})=}(t_{n}| _{t_{n-1}})-(t_{n}|_{t_{n-1}}),n=1, N_{T}\). An explicit ASM objective is defined as:

\[_{}()=_{p()}[ _{n=1}^{N_{T}}((t_{n}|_{t_{n-1}})-_{}(t_{n}| _{t_{n-1}}))^{2}].\] (12)

Similarly, to make ASM practical,  assumed that specific regularity conditions are satisfied. Therefore, an implicit ASM is proposed accordingly:

\[_{}()=_{p()}[_{n=1}^ {N_{T}}_{}^{2}(t_{n}|_{t_{n-1}})+(t_{n}|_{t_{n-1}})}{ t_{n}}].\] (13)

However, the same issue as in the Poisson process arises here. The regularity conditions required to eliminate the unknown data distribution do not hold. Therefore, we cannot derive the implicit ASM in Equation (13) based on the explicit ASM in Equation (12).

**Proposition 4.1**.: _Assume that all functions and expectations in \(_{}()\) and \(_{}()\) are well defined, we have,_

\[_{}()= _{}()++_{n=1}^{ } p(_{:n-1})p(t_{n}|_{t_{n-1}})_{}(t_{n}| _{t_{n-1}})_{t_{n}=T}d_{:n-1}\] (14) \[-_{n=1}^{} p(_{:n-1})p(t_{n}| _{t_{n-1}})_{}(t_{n}|_{t_{n-1}})_{t_{n}=t_{n-1}}d _{:n-1}.\]

_Therefore, \(_{}()\) is equivalent to \(_{}()\) if and only if the sum of last two terms is a constant not containing \(\)._

Generally speaking, for most Hawkes processes, the sum of the last two terms in Equation (14) still contains \(\), even for a common Hawkes process with an exponential decay triggering kernel. We illustrate this example in Section 6.2. This implies that \(_{}\) fails for general Hawkes processes.

### Autoregressive Weighted Score Matching

Similarly, to address the situation where ASM fails, we introduce the AWSM for Hawkes process. We present the conditions that a valid weight function \(\) should satisfy :

\[_{t_{n} T}p(_{1:n})_{}(t_{n}| _{t_{n-1}})h_{n}()=0,_{t_{n} t_{n-1}}p( _{1:n})_{}(t_{n}|_{t_{n-1}})h_{n}( )=0,\; n[N_{T}],\] \[[_{}^{2}(t_{n}|_{t_{n-1}})h_{n}( )]<,\;[_{}(t_{n}|_{t_{n-1}}) ()}{ t_{n}}]<, n[N_{T}].\] (15)

With a valid weight function \(\), the explicit AWSM objective can be defined as:

\[_{}()=_{p( )}[_{n=1}^{N_{T}}((t_{n}|_{t_{n-1}})-_ {}(t_{n}|_{t_{n-1}}))^{2}h_{n}()].\] (16)

**Theorem 4.2**.: _Assume the true conditional density is in the family of the model conditional density, denoted as \(p(t_{n}|_{t_{n-1}})=p_{^{*}}(t_{n}|_{t_{n-1}})\), where \(^{*}\). We further assume that \(p_{_{1}}(t_{n}|_{t_{n-1}})=p_{_{2}}(t_{n}|_ {t_{n-1}})\;a.e.\) gives \(_{1}=_{2}\). Then the unique minimizer of \(_{}()\) is \(^{*}\)._

The explicit AWSM objective is not practical as it depends on the unknown data distribution \(p(t_{n}|_{t_{n-1}})\), so we further derive the implicit AWSM objective which is tractable.

**Theorem 4.3**.: _Assume that all functions and expectations in \(_{}()\) and \(_{}()\) are well defined, Equation (15) are satisfied, we have,_

\[_{}()=_{}( )+,\] \[_{}()=_{p()} [_{n=1}^{N_{T}}_{}^{2}(t_{n}|_{t_{n- 1}})h_{n}()+(t_{n}|_{t_{n-1}} )}{ t_{n}}h_{n}()+_{}(t_{n}|_{t_{n-1}} )()}{ t_{n}}].\] (17)

For general Hawkes processes, Equation (17) is always valid with a suitable weight function. Thus, we do not need to worry about the issues of failure that may arise when using Equation (13).

Multivariate Hawkes ProcessesFor the multivariate case, events are \(\{(t_{1},k_{1}),,(t_{N_{T}},k_{N_{T}})\}\) with \(k_{n} 1,,K\) denoting the event type of the \(n\)-th event. The history up to the \((n-1)\)-th event is denoted by \(_{t_{n-1}}\). We need to consider both the distributions of event times and event types. For the temporal distribution, we use the AWSM objective with the temporal score \((t_{n}|_{t_{n-1}})=} p(t_{n}| _{t_{n-1}})\) as before. For the type distribution, since we do not need to compute the intensity integral, we directly use the cross-entropy objective:

\[_{}()=_{p()}[ _{n=1}^{N_{T}} p_{}(k_{n}|_{t_{n-1}},t_{n})]= [_{n=1}^{N_{T}}_{k_{n}}(t_{n}|_{t_{n -1}};)-(t_{n}|_{t_{n-1}};)],\] (18)

where \(=_{k=1}^{K}_{k}\). The final loss is \(()=_{}()+_{ {CE}}()\); \(\) is a balancing coefficient.

## 5 Theoretical Analysis

In this section, we analyze the statistical properties of AWSM estimator of univariate Hawkes process. Similar conclusions also hold for the WSM estimator of Poisson process, as discussed in Appendix C.5. We consider \(M\) i.i.d. sequences \(\{t_{1}^{(m)},,t_{N_{m}}^{(m)}\}_{m=1}^{M}\) from \(p()\) of a Hawkes process. We assume the true density is in the family of the model density, denoted as \(p()=p_{^{*}}()\), where \(^{*}^{r}\). The estimate \(\) is obtained by \(=_{}}_{}()\) where \(}_{}\) represents the empirical loss. Below we omit the subscript AWSM as it does not cause any ambiguity.

### Asymptotic Property

We first establish the consistency of \(\) for a Hawkes process.

**Theorem 5.1**.: _Under mild regularity Assumptions C.1 to C.3, we have \(^{*}\) as \(M\)._

### Non-asymptotic Error Bound

Then, we establish a non-asymptotic error bound for \(\). We define

\[_{}()=_{p()}[_{n=1}^{N_ {T}}_{}^{2}(t_{n}|_{t_{n-1}} )+(t_{n}|_{t_{n-1}})}{ t_{n}}}_{A _{n}(,)}h_{n}()+(t _{n}|_{t_{n-1}})}_{B_{n}(,)}( )}{ t_{n}}].\]

**Assumption 5.2**.: Assume there exists \(>1\) such that,

\[_{:||-^{*}||}_{}() -_{}(^{*}) C_{}^{}\]

holds for any small \(\). Here, \(C_{}\) is a positive constant that depends on the weight function \(\) such that \(C_{a}=aC_{}\) for any positive constant \(a\). \(\|\|\) is the euclidean norm.

**Assumption 5.3**.: For \( n^{+}\), there exists \(}(),}()\) such that,

\[|A_{n}(,_{1})-A_{n}(,_{2})|}( )||_{1}-_{2}||,|B_{n}(,_{1})-B_{ n}(,_{2})|}()||_{1}-_{2}||.\]

**Theorem 5.4**.: _Given that \(\) converges to \(^{*}\) in probability, combined with Assumptions 5.2 and 5.3, for \(<CK_{}}{2^{-1}},A,B)}{C_ {}}\), we have_

\[[||-^{*}||(CK_{},A,B)}{ C_{}}}^{1/(-1)} )] 1-,\] (19)

_where \((,A,B)=_{p()}\{_{n=1}^{N_ {T}}[(_{n}()h_{n}())+(_{n}()()}{ t_{n}})]\}^{2}}\), \(C\) is a universal constant, \(K_{}=}{2^{-1}-1}\), and \(r\) is the number of dimensions of \(\)._

### Discussion on Optimal Weight Function

In Sections 3 and 4, we only provide the conditions that the weight function needs to satisfy. In fact, there are many weight functions that satisfy these conditions. The optimal weight function should minimize the error bound in Equation (19), which is equivalent to minimizing the coefficient \(,A,B)}{C_{}}\). The numerator cannot be analytically computed as it involves an unknown distribution \(p()\), but we can maximize the denominator \(C_{}\) in a predefined function family.

**Theorem 5.5**.: _Define \(^{0}\) to be a weight function with its \(n\)-th element defined as the distance between \(t_{n}\) and the boundary of its support \([t_{n-1},T]\):_

\[h_{n}^{0}(t_{n})=}{2}-|t_{n}-(T+t_{n-1})/2|.\]

_We have,_

\[^{0}*{arg\,max}_{} ||}{}_{ }()-_{}(^{*})\]

_where \(\) is a family of functions that is rigorously defined in Equation (27)._

Combined with Assumption 5.2, it can be observed that \(^{0}\) maximizes \(C_{}\) in \(\). Though it does not necessarily optimize \(,A,B)}{C_{}}\), it is an adequate choice without using any information on \(p()\). We also discuss it heuristically in Appendix C.4. It is worth noting that \(h_{n}^{0}\) is not continuously differentiable; however, it is weakly differentiable. Its weak derivative is continuous, allowing both integration by parts and statistical theory to hold. In subsequent experiments, we consistently employ this optimal weight function when \(T\) is available or can be approximated for the dataset.

## 6 Experiments

In this section, we validate our proposed (A)WSM on parametric or deep point process models. For parametric models, we focus on verifying whether (A)WSM can accurately recover the ground-truth parameters. For deep point process models, we confirm that our new training method is also applicable to deep neural network models. 2

### Baselines and Metrics

We consider three baseline parameter estimators: (1) **MLE** (2) implicit **(A)SM**[18; 24; 10] (3) Denoising Score Matching (**DSM**) . We briefly introduce DSM in deep point process models.

For deep Hawkes process training, DSM is employed as follows. For observed timestamps \(t_{n}^{(m)}\) in \(m\)-th sequence, we sample \(L\) noise samples \(_{n,l}^{(m)}=t_{n}^{(m)}+_{n,l}^{(m)},l=1,,L,\) where \((_{n,L}^{(m)})=^{2}\) and get the DSM objective:

\[}()=_{m=1}^{M}_{n=1}^{N_{m}}_{l=1 }^{L}_{}(_{n,l}^{(m)}|_{t_{n -1}^{(m)}})+^{(m)}}{^{2}}+}_{}(),\]

where \(_{}()\) is the cross-entropy loss defined in Equation (18).

To compare the performance of different methods, for parametric models on synthetic data, we use the mean absolute error (**MAE**, \(|-|\)) between the ground-truth parameters and the estimates as a metric since the ground-truth parameters are known. For deep point process models, we use the test log-likelihood (**TLL**) and the event type prediction accuracy (**ACC**) on the test data as metrics.

### Parametric Models

DatasetsWe validate the effectiveness of (A)WSM using three sets of synthetic data. (1) **Poisson Process**: This dataset is simulated from an inhomogeneous Poisson process with an intensity function \((t)=((t))\) with \(T=2\), \(=2\). (2) **Exponential Hawkes Processes**: This dataset is simulated from \(2\)-variate Hawkes processes with exponential decay triggering kernels \(g_{ij}()=_{ij}(-5),\;>0\) with \(T=10\), \(_{1}=_{2}=1\), \(_{11}=1.6,_{12}=0.2\), \(_{21}=_{22}=1\). (3) **Gaussian Hawkes Processes**: This dataset is simulated from \(2\)-variate Hawkes processes with Gaussian decay triggering kernels \(g_{ij}()=}{}(-}{2 ^{2}}),\;>0\) with \(T=10\), \(_{1}=_{2}=1\), \(_{11}=1.6,_{12}=0.2\), \(_{21}=_{22}=1\), \(=1\).

Training ProtocolWe assume that we know the ground-truth model but do not know its parameters. Therefore, we use the ground-truth model as the training model. The purpose is to verify whether the estimator can recover the ground-truth parameters. For each dataset, we collect a total of \(1000\) sequences. We run \(500\) iterations of gradient descent using Adam  as the optimizer for all scenarios. For MLE, the intensity integral is computed through numerical integration, with the number of integration nodes set to \(100\) to achieve a considerable level of accuracy. We change the random seed \(3\) times to compute the mean and standard deviation of MAE.

ResultsIn Table 1, we report the MAE of parameter estimates for three models trained by MLE, (A)SM, and (A)WSM on the synthetic dataset. We can see that both MLE and (A)WSM achieve small MAE on three types of data. However, the MAE of (A)SM is large. As we have theoretically demonstrated earlier, this is because MLE and (A)WSM estimators are consistent. In contrast, (A)SM, due to the absence of the required regularity conditions in the three cases, has an incomplete objective and cannot accurately estimate parameters. In Figure 1, we showcase the learned intensity functions. Both MLE and (A)WSM successfully captured the ground truth, while (A)SM fails.

   Estimator &  &  &  \\   & \(\) & \(_{11}\) & \(_{12}\) & \(_{1}\) & \(_{11}\) & \(_{1}\) & \(\) \\  (A)WSM & \(0.07 0.14\) & \(0.041 0.041\) & \(0.026 0.001\) & \(\) & \(0.153 0.162\) & \(0.022 0.023\) & \(0.060 0.066\) \\  (A)SM & \(1.56 0.01\) & \(1.600 0.001\) & \(0.200 14.30\) & \(0.700 0.272\) & \(1.413 0.263\) & \(0.696 0.267\) & \(2.507 1.957\) \\  MLE & \(\) & \(\) & \(\) & \(0.012 0.006\) & \(\) & \(\) & \(\) \\   

Table 1: The MAE of three models trained by MLE, (A)SM, and (A)WSM on the synthetic dataset. For the 2-variate processes, we only present the estimation results for some parameters here. The results for other parameters can be found in Table 3.

### Deep Point Processes Models

DatasetsWe consider four real datasets. (1) **Half-Sin Hawkes Process**: This is a synthetic 2-variate Hawkes process with trigerring kernel \(g_{ij}=_{ij}sin(),(0,)\), \(K=2\). (2) **StackOverflow**: This dataset has two years of user awards on StackOverflow. Each user received a sequence of badges and there are \(K=22\) kinds of badges. (3) **Retweet**: This dataset includes sequences indicating how each novel tweets are forwarded by other users. Retweeter categories serve as event types \(K=3\). (4) **Taobao**: This dataset comprises user activities on Taobao (in total \(K=17\) event types). For each dataset, we follow the default training/dev/testing split in the repository.

Training ProtocolIn recent years, many deep point process models have been proposed. Here, we focus on two of the most popular attention-based Hawkes process models: **SAHP** and **THP**. We deploy AWSM and ASM on THP and SAHP. For each dataset, we train 3 seeds with the same epochs and report the mean and standard deviation of the best TLL and ACC. When using MLE, we adopt numerical integration to calculate the intensity integral. To ensure model accuracy, the number of integration nodes is set to be large enough as we sample 10 nodes between every two adjacent events. When using DSM, we tune the variance of noise for better results. When using AWSM, since for real datasets, the true observation endpoint \(T\) is unknown. We choose the maximum event time of each batch as the observation endpoint for weight function \(^{0}\). This may lead to unsatisfying results since real datasets may not be sampled during a unified time window. We provide a remedy for this as discussed in Appendix D.1. Details of training and testing hyperparameters are provided in Appendix D.2.

ResultsIn Table 2, we report the performance of SAHP and THP trained using three different methods, namely MLE, AWSM, and DSM, on four datasets. It is evident from the results that models trained with MLE and AWSM exhibit very similar performance in terms of both TLL and ACC on the test data. This indicates consistency between MLE and AWSM, as they yield comparable model parameters. For DSM, it is significantly inferior to the performance of MLE and AWSM. This may result from the fact that the DSM objective is a biased estimation of the original SM objective and fails to produce consistent estimation when \(>0\) as discussed in . For ASM, it completely fails in the scenarios mentioned above. It is unable to estimate the correct parameters, and its results are not reported. Generally, for complex point process models such as deep Hawkes processes, the necessary regularity conditions are not satisfied, meaning that ASM's objective is incomplete.

### Advantage of (A)WSM over MLE

The key advantage of (A)WSM over MLE is its avoidance of computing intensity integrals, which can be computationally intensive for complex point process models and impact MLE accuracy. We evaluate the test log-likelihood of MLE and AWSM on the Exp-Hawkes dataset as the number of integration nodes varies. As shown in Figure 1d, with a limited number of nodes, MLE is faster but exhibits substantial estimation errors. Increasing the number of nodes reduces the error but significantly increases computation time. In this scenario, AWSM is much faster than MLE with the same accuracy, thus offering better computational efficiency.

Figure 1: The learned intensity functions from MLE, (A)SM, and (A)WSM on (a) Poisson, (b) Exp-Hawkes and (c) Gaussian-Hawkes. We present the results for the 1-st dimension. The 2-nd dimension are in Appendix D. The ground truth, MLE, and (A)WSM nearly overlap, while (A)SM differs. (d) The TLL and runtime of (A)WSM and MLE w.r.t. the number of integration nodes.

### Comparison Between Weights

Though we provide theoretical insight into the choice of an optimal weight function for AWSM, its validity still needs to be testified by experiments. Here, we compare the near-optimal weight \(^{0}\) with natural weight \(^{1}\) and squareroot weight \(^{2}\) satisfying Equation (15),

\[h^{1}_{n}(t_{n})=(t_{n}-t_{n-1})(T-t_{n}),h^{2}_{n}(t_{n})=-t_{n-1} )(T-t_{n})}.\]

All three weight functions can be applied in AWSM to recover ground-truth parameters, however with different convergence rates. We carry out experiments on synthetic data for exponential-decay model with the same setting as Section 6.2 in our paper. We measure their MAE for different sample sizes in Figure 2 and find that \(}\) does achieve the best results among the three weight functions.

## 7 Limitations

The current limitation of the methodology is that some real data are collected from multiple time intervals \([0,T_{1}],,[0,T_{L}]\) or collated in a fixed time interval \([0,T]\) with unknown \(T\). However, for a score matching to be valid, the required weight function must involve knowledge of \(T\). Currently, our remedy including approximate \(T\) or performing data truncation as discussed in Appendix D.1.

## 8 Conclusions

In conclusion, the SM estimator for point processes can overcome the challenges associated with intensity integrals in MLE. While existing works have proposed SM estimators for point processes, our investigation reveals that they prove effective only for specific problems and fall short in more general cases. To address this issue, our work introduces a novel approach: the (A)WSM estimator for point processes, offering both theoretical soundness and empirical success.

   &  &  \\   & MLE & AWSM & DSM & MLE & AWSM & DSM \\  Half-Sin & \(1.542_{ 0.038}\) & \(}\) & \(0.804_{ 0.353}\) & \(1.161_{ 0.011}\) & \(}\) & \(-0.385_{ 0.033}\) \\  Stackoverflow & \(}\) & \(-2.541_{ 0.661}\) & \(-2.629_{ 0.068}\) & \(}\) & \(-2.508_{ 0.007}\) & \(-2.782_{ 0.034}\) \\  Tabao & \(}\) & \(-1.373_{ 0.091}\) & \(-1.911_{ 0.049}\) & \(-1.052_{ 0.012}\) & \(}\) & \(-1.791_{ 0.040}\) \\   & \(}\) & \(0.411_{ 0.077}\) & \(0.110_{ 0.186}\) & \(}\) & \(0.419_{ 0.009}\) & \(-0.183_{ 0.197}\) \\   &  &  \\   & MLE & AWSM & DSM & MLE & AWSM & DSM \\  Half-Sin & \(0.502_{ 0.001}\) & \(}\) & \(0.501_{ 0.001}\) & \(0.508_{ 0.016}\) & \(}\) & \(0.503_{ 0.001}\) \\  Stackoverflow & \(0.461_{ 0.001}\) & \(}\) & \(0.421_{ 0.062}\) & \(0.461_{ 0.001}\) & \(}\) & \(0.445_{ 0.0016}\) \\  Tabao & \(}\) & \(0.455_{ 0.011}\) & \(0.421_{ 0.017}\) & \(}\) & \(0.592_{ 0.002}\) & \(0.435_{ 0.010}\) \\  Refuets & \(}\) & \(0.411_{ 0.077}\) & \(0.590_{ 0.009}\) & \(}\) & \(0.592_{ 0.002}\) & \(0.556_{ 0.001}\) \\  

Table 2: The TLL and ACC of two attention-based deep Hawkes process models trained by MLE and AWSM on four datasets. Because ASM estimator completely fails, we do not report its results.

Figure 2: MAE of parameter estimation versus sample size for three different weight functions on Exponential-Hawkes Model. Our near-optimal weight function outperforms the rest two valid weight functions in all sample sizes. We only show results for three parameters. The rest parameters have almost the same paradigm.