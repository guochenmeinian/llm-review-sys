# Focus Your Attention when Few-Shot Classification

Haoqing Wang  Shibo Jie  Zhi-Hong Deng

School of Intelligence Science and Technology, Peking University

{wanghaoqing, parsley, zhdeng}@pku.edu.cn

Corresponding author.

###### Abstract

Since many pre-trained vision transformers emerge and provide strong representation for various downstream tasks, we aim to adapt them to few-shot image classification tasks in this work. The input images typically contain multiple entities. The model may not focus on the class-related entities for the current few-shot task, even with fine-tuning on support samples, and the noise information from the class-independent entities harms performance. To this end, we first propose a method that uses the attention and gradient information to automatically locate the positions of key entities in the support images, denoted as _position prompts_. Then we employ the cross-entropy loss between their many-hot presentation and the attention logits to optimize the model to focus its attention on the key entities during fine-tuning. This ability then can generalize to the query samples. Our method is applicable to different vision transformers (e.g., columnar or pyramidal ones), and also to different pre-training ways (e.g., single-modal or vision-language pre-training). Extensive experiments show that our method can improve the performance of full or parameter-efficient fine-tuning methods on few-shot tasks. Code is available at https://github.com/Haoqing-Wang/FORT.

## 1 Introduction

Based on the accumulated experience, humans can learn from few labeled samples for solving novel tasks, and this is a key aspect of human intelligence. Few-shot learning [12; 31] aims to imitate this ability and has attracted widely attention from the machine learning community . Traditionally, a base dataset is collected for learning prior knowledge before solving novel few-shot tasks. The meta-learning methods learn the task-shared inductive bias, e.g., parameter initialization [13; 41], metric function [52; 32; 54] or optimizer [2; 47]. The transfer learning methods [56; 48] learn generalizing representation to novel tasks, and achieve comparable or better performance.

Some works [56; 46] find that the well-learned embedding model contributes the most to few-shot classification performance, and even more important than the sophisticated meta-learning algorithms. However, the commonly-used base datasets are far from large enough for learning generalizing embedding, e.g., miniImageNet , which severely limits the performance of few-shot classification. In fact, the few-shot learning ability of humans benefits from the supervised or unsupervised learning from _massive_ signals since birth, which helps us accumulating enough general knowledge. On the other hand, self-supervised learning [66; 19; 8; 18; 5] can learn well-generalizing representation from nearly-free massive unlabeled data and is beneficial for broad downstream tasks. Using these pre-trained models for few-shot tasks conforms to the learning paradigm of humans and has achieved remarkable success [5; 74; 15] in Natural Language Processing (NLP). In this work, we aim to adapt the pre-trained models to few-shot image classification tasks, without using any base datasets.

Recently, vision transformers [11; 36; 70] play an increasingly important role in computer vision, so we use them as the backbone for few-shot tasks. Note that it is a non-trivial problem to fine-tune the pre-trained large vision transformers on few support samples due to their data-hungryproperty. Inspired by the prompt methods  from NLP, some works  synthesize the classification weights by encoding the learnable text prompts that describe the objective classes, and then compare them with image features. Despite impressive performance on few-shot tasks, these works strongly rely on the semantic alignment between image encoder and text encoder, and thus are specific to vision-language pre-trained models . In this work, we design a new form of prompts for vision data, named _position prompts_, to adapt the pre-trained vision transformers to few-shot image classification tasks. They can prompt explicit valuable information, and also be usable to different pre-trained vision transformers, without limited structures or pre-training ways.

Figure 1 illustrates our framework and motivation, where we visualize the patches with top highest attention scores and covering about \(95\%\) attention (details at Section 4.2) of [CLS] token in a ViT-B/16 . The input images usually contain several different entities, e.g., a _bird_ stands on the _branch_ (Figure 1, 'Input'). The pre-trained model may attend to multiple entities at the same time (Figure 1, 'Orig Attn') and the noise information from class-independent entities harms performance. The standard fine-tuning methods tune full or partial  parameters of the model using the classification loss on the support samples, and can achieve better performance than the backbone-frozen methods. But even with fine-tuning, the extremely few support samples prevent the model from giving sufficient attention to the _class-related_ key entities (Figure 1, 'FT Attn'). To this end, besides the classification loss, we use the position information of the key entities in each support sample as guidance for fine-tuning, and the model is expected to focus its attention on the key entities after fine-tuning, not only for the support samples, but also for the query ones. Concretely, we first automatically locate the position of key patches as position prompts (Figure 1, 'Prompts') in each support sample based on the deep explanation method, where the key patches refer to the most _class-related_ ones and correspond to key entities. Inspired by Rollout , we integrate the attention information from multiple layers linearly for explanation, and further introduce gradient information as an auxiliary to ensure class-specific explanation. Unlike existing prompts , ours are only used to guide the fine-tuning process and not available during testing, since we need label information to obtain them. Then, we use the position prompts as the prediction target of attention, and optimize the cross-entropy loss between their many-hot presentation and the attention logits during fine-tuning. This loss enables the model to identify the key patches and focus on them (Figure 1, 'Our Attn'), and this ability obtained during fine-tuning can generalize to query samples. We also theoretically prove that our method can increase the information from the key patches in the output feature and reduce that from other patches (noise information).

Overall, our contributions can be summarized as follows:

* We propose a new form of prompts for vision data, position prompts, to effectively adapt the pre-trained vision transformers to few-shot image classification tasks.
* Our method, denoted as FORT (FOcus youR aTention), is applicable to the attention modules in different vision transformers, e.g., Swin  or ViT, and not limited to specific pre-training ways, e.g., single or multi-modal ones.

Figure 1: Focusing on key entities via position prompts. The original pre-trained model may attend to multiple entities in a single image, and the information from class-independent entities is actually noise in the current task. The extremely few support samples make the standard fine-tuning with only classification loss unable to remove the noise information. We propose position prompts (red patches) to prompt the model where are the key patches of the input and focusing on them. This ability gained during fine-tuning can generalize from support samples to query ones. Here the white patches in attention visualization have the top highest attention scores and cover about \(95\%\) attention.

* We conduct extensive experiments and verify that our method can effectively improve the performance of full or parameter-efficient [24; 34] fine-tuning methods on few-shot tasks.

## 2 Related work

Few-shot classification.The traditional paradigm of few-shot classification is the meta-learning framework [22; 67], which learns the task-shared inductive bias [13; 52; 32; 63] on the training tasks constructed from a base dataset. When the base dataset is unlabeled or in the different domain to novel tasks, we also need to solve new challenges, unsupervised [23; 28; 64] or cross-domain [58; 62; 14] few-shot classification. Some works [56; 46] find the well-generalizing representation is actually the key factor of few-shot ability, while the base dataset is typically not large enough to obtain it. In fact, adapting self-supervised pre-trained models [6; 76; 65] to few-shot tasks can address all these concerns simultaneously: the training data is unlabeled and thus can be sufficiently large, and the novel tasks can come from different target domains. The pre-trained large models have far more generalizing representation and thus can obtain significantly better performance than the traditional setting, as shown in Appendix B. Besides, it is more friendly to privacy data, and we only need the pre-trained models instead of pre-training data. It also conforms to the learning paradigm of humans whose few-shot ability comes from learning on massive signals since birth, and has better realistic value. An exploratory work  uses the ensemble of frozen pre-trained models and a simple classifier, but without further adaptation to novel tasks. CLIP  introduces the vision-language pre-trained models, and some works [78; 77; 10] use the text prompts to solve few-shot image classification based on them. These methods are limited to specific pre-trained models, and typically require the test classes to be seen in the training corpus . Humans actually can recognize unseen objects based on only reference images, without knowing their class names. Our method is usable for both single and multi-modal pre-trained models. Similar to our work, some existing models [69; 79] also propose to highlight the key local features of each input for few-shot classification. However, they all design parametric modules and meta-learn them using the base dataset. It is not suitable for our new setting, since the parametric modules can not been learned effectively using only few support samples. Our method instead introduces no new parametric modules and does not need meta-training.

Prompt engineering.Designing prompts  for adapting the pre-trained models to few-shot tasks can originate from GPT-3 , which uses language prompts for in-context learning. Since the manual prompts require domain expertise and are likely to be sub-optimal even with significant effort, many works [15; 72; 29] focus on automating prompt generation to fine-tune pre-trained language models. The language prompts can also be used for few-shot image classification [78; 77; 7] based on special vision-language pre-trained models . VPT  introduces learnable prompts to the input space of each transformer layer, and is suitable for both single and multi-modal pre-trained models. But VPT performs unsatisfactorily on few-shot tasks, even worse than simple plug-and-play solvers [4; 32] with frozen backbone (Table 1). Its prompts do not prompt any explicit information and still follow the form of language prompts. In this work, we propose position prompts for vision data to explicitly prompt the key positions of input images to improve fine-tuning performance.

Deep explanation method.In order to build trust in intelligent systems, deep explanation methods  calculate the local relevancy of the input with model predictions, and can be used to locate the key parts. Many classic works have been designed for CNNs, such as gradient based methods [51; 50; 55; 53] and relevance propagation based methods [3; 38; 25; 17]. For Transformers, these methods can not take advantage of the core attention information. We can use the raw attention scores to explain model predictions, but this ignores the attention components of other layers. Rollout  thus rolls out the attention scores of multiple layers to capture the propagation of information from input, but it is not class-specific. Inspired by Rollout, we also integrate the attention information from multiple layers, and further introduce gradient information to ensure class-specific explanation.

## 3 Model

### Problem formulation

In this work, we aim to adapt the pre-trained vision transformers to few-shot image classification tasks. These backbones can be columnar or pyramidal architectures, e.g., ViT  or Swin , and are pre-trained on massive data. In a few-shot classification task, we are typically given a support set \(_{s}\) as reference and a query set \(_{q}\) for test. The support set \(_{s}=\{(x_{i}^{s},y_{i}^{s})\}_{i=1}^{C K}\) contains \(C\) different classes with \(K\) labeled samples in each class, which is denoted as '\(C\)-way \(K\)-shot'. \(K\) is typically very small, e.g., 1 or 5, which makes it a challenging task, especially for the data-hungry vision transformers. The query set \(_{q}=\{(x_{j}^{q},y_{j}^{q})\}_{j=1}^{M}\) contains the samples belong to the same classes with the support set, and we need to classify them correctly. Adapting a pre-trained model \(f_{}\) to a few-shot task \(=\{_{s},_{q}\}\) can be formulated as

\[=(f_{},_{s})\] (1)

where the task solver \(\) is designed to obtain the solution model \(\) of task \(\). For example, the task solver \(\) can be linear probing  that combines the frozen model \(f_{}\) with a linear classifier, or the parameter-efficient fine-tuning methods [34; 24; 27] that only tune partial existing or newly-added parameters for easy deployment, or using language prompts to generate classifier weights when \(f_{}\) is a vision-language pre-trained model [78; 77; 7]. The performance of solution \(\) is evaluated on the query set \(_{q}\) with accuracy and so on. In this work, we aim to design better task solver \(\).

### Multi-head self-attention

The Multi-head Self-Attention (MSA) modules are the core components of various vision transformers. The input image is split into multiple patches, which are then projected to patch tokens. The learnable [CLS] token is appended to these tokens for final classification if needed. Let the input of a MSA module be \(_{in}^{N d}\), containing \(N\) tokens with dimension \(d\). Here \(_{in}\) may contain all input tokens for global attention , or partial ones for window attention . MSA uses \(H\) attention heads to jointly attend to the information from different representation sub-spaces. Within \(h\)-th attention head, we first project \(_{in}\) to Query \(_{h}\), Key \(_{h}\) and Value \(_{h}\) as

\[_{h}=_{in}_{q}^{h}^{N d_{k}} _{h}=_{in}_{k}^{h}^{N d _{k}}_{h}=_{in}_{v}^{h}^{N  d_{v}}\] (2)

where \(_{q}^{h}\), \(_{k}^{h}\) and \(_{v}^{h}\) are projection matrices. The self-attention matrix \(_{h}\) is calculated as

\[_{h}=(_{h}_{h}^{T}/} )^{N N}\] (3)

The matrix \(_{h}\) reflects the attention of different tokens to each other, and the large score \(_{h}^{ij}\) indicates the \(i\)-th token strongly attends to the \(j\)-th token. The output feature of the MSA module is

\[(_{in})=([_{h}_{h}]_{ h=1}^{H})_{o}+_{in}\] (4)

where \(()\) denotes the concatenation operation, \(_{o}\) is a projection matrix. Obviously, the MSA module learns the increment of each token and introduces the information from other tokens, whose quantity is controlled by the attention matrices \(\{_{h}\}_{h=1}^{H}\).

### Analysis and motivation

We first examine various designs of the task solver \(\): 1) simple machine learning classifiers, e.g., Nearest Neighbor classifier, Ridge Regression and Support Vector Machines; 2) plug-and-play inductive meta-solvers (e.g., ProtoNet , R2D2  and MetaOptNet ) and linear probing; 3) full or parameter-efficient fine-tuning, e.g., VPT , LoRA  and SSF . The results are provided in Table 1, where we use the ViT-B/16 pre-trained on ImageNet-1K  with DINO  as \(f_{}\) and conduct on CUB , Cars , Places  and Plantae  datasets. For each fine-tuning method, we use the best linear meta-solver, MetaOptNet, to obtain the initialization of the classifier weights. Otherwise, these fine-tuning methods perform even worse than MetaOptNet on few-shot tasks (Figure 2). As shown in Table 1, the fine-tuning methods can achieve better performance than the backbone-frozen methods in most cases. But they sometimes cannot significantly improve performance beyond the classifier initialization (i.e., MetaOptNet) or even perform worse, especially on 1-shot tasks. In fact, the main challenges of few-shot tasks are: 1) the classes in the novel tasks could be unseen

Figure 2: Average 20-way accuracy on CUB, Cars, Places and Plantae datasets. Here all fine-tuning methods use the randomly-initialized classifier.

during pre-training phase; 2) there are only few (\(1 5\)) labeled support samples for each class in novel tasks. The input images typically contain multiple entities. Although the pre-trained vision transformers may be able to separate different entities , they do not pay enough attention to the key entities corresponding to the classes in the current few-shot task. Especially in the face of unseen classes, they may attend to multiple entities simultaneously . When the support samples are sufficient, the model can attend to the frequently-occurring key entities in each class more to alleviate this problem, as shown in Appendix B. But the extremely few labeled samples prevent the fine-tuning methods from achieving this purpose, and the noise information from the class-independent entities misleads the fine-tuning process and harms the performance. For qualitative evidence, we visualize the patches with top highest attention scores and covering about \(95\%\) attention of [CLS] token in Figure 4, more details and quantitative evidence at Section 4.2. As we can see, both the original model and fine-tuned one can not focus most attention on the key entities. To this end, we propose the position prompts to explicit prompt the model where are the key entities and guide the model to focus on them during fine-tuning. This ability then can generalize to the query samples.

### Position prompts

We need to locate the positions of key entities in each support image as prompts for fine-tuning, i.e., the most class-related key patches for the current few-shot task. Manual labeling may provide accurate positions, but requires domain expertise and labor costs. We aim to design an automatic method, which should 1) be class-specific for locating class-related patches, and 2) make full use of the attention information, the core content of Transformers. Deep explanation methods  calculate the local relevancy of the input with model predictions, and can be used for highlighting the key patches for classification. For Transformers, Rollout  finds that using the attention of top layer for explanation ignores most of the attention components, thus miscalculating the importance of the patches, so we integrate the attention information from multiple layers like Rollout. Furthermore, only using the attention information cannot achieve class-specific explanation for some pre-trained models (Figure 3), so we further introduce the gradient information as an auxiliary.

For a columnar architecture  with \(L\) layers, let the input feature map of \(l\)-th layer be \(_{in}^{l}^{N d}\) and the attention matrices be \(\{_{h}^{l}\}_{h=1}^{H}\). With the well-initialized classification head above, for a support sample-label pair \((x^{s},y^{s})\), we first compute the gradient of the prediction score for class \(y^{s}\), \(p_{y^{s}}\), with respect to feature map \(_{in}^{l}\) as \(_{l}= p_{y^{s}}/_{in}^{l}^{N d },l=1,,L\). The gradient computed for single sample contains many noises, which could be amplified layer by layer along the back-propagation path. To this end, we only use the gradient to the input feature of the top layer, i.e., \(_{L}\), and reserve its first principle component for denoise. The obtained gradient term is

\[=_{L}_{1}^{N 1}, ,,=(_{L})\] (5)

where \(()\) denotes Singular Value Decomposition (SVD) operation with \(_{L}=^{T}\), and \(_{1}\) is the first column vector of matrix \(\), corresponding to the largest singular value of \(_{L}\). The gradient term \(\) can provide class-specific information. As shown in Equation 4, the residual connection plays a important role in propagating information from inputs to model predictions and can preserve the position information during forward propagation. To this end, we use an identity matrix \(\) to augment the attention matrix. Overall, the final attention graph of \(l\)-th layer is

\[}^{l}=(+^{l}+ ^{T})^{N N}\] (6)

where \(()\) means to re-normalize the input so that the row sum is equal to 1, \(\) can control the proportion of attention and gradient information, and we average the attention matrix over all heads, i.e., \(^{l}=_{h=1}^{H}_{h}^{l}\). We assume the attentions are combined linearly and consider the path along the pairwise attention graph . The patch importance scores are calculated as

\[=(}^{1}}^{ 2}}^{L})^{N}\] (7)

where \(()\) denotes matrix multiplication, \(()\) denotes to average the row vectors of a matrix. We use the positions of the top \(P\) patches with the highest importance scores as prompts.

For a pyramidal architecture , since the downsampling operation destroys the patch position correspondence between layers, so we directly use the attention scores average on patches as the importance scores and introduce gradient information to achieve class-specific if necessary.

### Attention enhancement

The key patches contain the most class-related information of the input images, and the model should focus its attention on them to obtain discriminative representation in the current task, not only for the support samples, but also for the query ones. Since the position prompts (or key patches) of the query samples are unknown without label information, unlike the existing prompts , our position prompts can not be used in the input or middle phase of forward inference. To this end, we use the position prompts as the prediction target for attention, and optimize the cross-entropy loss between their many-hot presentation2 and attention logits during fine-tuning. As shown in Equation 2 and 3, given the input tokens (e.g., [CLS] token and patch tokens) of the top layer \(_{in}^{L}=\{_{n}\}_{n=1}^{N}^{N d}\), we obtain the Query \(=\{_{n}\}_{n=1}^{N}=_{in}^{L}_{q}\) and Key \(=\{_{n}\}_{n=1}^{N}=_{in}^{L}_{k}\) for calculating the attention logits \(\{_{i}_{j}^{T}/}\},i,j=1,,N\), where \(_{q}=([_{q}^{h}]_{h=1}^{H})\) and \(_{k}=([_{k}^{h}]_{h=1}^{H})\). The fine-tuning objective is

\[_{}_{(x^{s},y^{s})_{s}}[(f_{ }(x^{s}),y^{s})-],=_{n=1}^{N}_{t}_{n }_{t}^{T}/)}{_{m=1}^{N}(_{n} _{m}^{T}/)}\] (8)

where \(\) denotes tunable full or partial parameters of the model, \((,)\) denotes cross-entropy classification loss, \(\) is a loss coefficient, \(\) is the index set of the key patches in \(x^{s}\), \(\) is a temperature coefficient. We calculate the objective term \(\) only at the top layer to operate the features with more refine semantic. Specially, for the pyramidal architectures, \(\) is applied to the top layer of each stage. This loss enables the model to automatically identify the key patches and focus on them during fine-tuning, and this ability then can generalize to the query samples. The position prompts are fixed during fine-tuning since updating them only obtains trivial improvement but introduces obvious cost.

Theoretical analysis.The key patches contain the most class-related information of the input and we want to increase their information in the output feature. We first prove that the objective term \(\) can increase the information from the key patches in the input tokens \(_{in}^{L}=\{_{n}\}_{n=1}^{N}\). Concretely, we consider the mutual information \(I(z,z_{})\), where the random variable \(z\) presents the input token with \(\{_{n}\}_{n=1}^{N}\) as its samples, the random variable \(z_{}\) presents the key patch with \(\{_{t}|t\}\) as its samples. The InfoNCE lower bound estimate [42; 44] of \(I(z,z_{})\) thus is

\[_{InfoNCE}(z,z_{})=_{n=1}^{N}_{t }[_{n},_{t})}{_{m =1}^{N}f(_{n},_{m})}] I(z,z_{})\] (9)

where the critic \(f(,)\) is used to model the density ratio and can be any positive real score function. When we use a log-bilinear function as critic, i.e.,

\[f(_{n},_{m})=(_{n}_{q} _{k}^{T}_{m}^{T}/)=(_{n} _{m}^{T}/), n,m\{1,,N\}\] (10)

    &  &  &  &  \\   & **1-shot** & **5-shot** & **1-shot** & **5-shot** & **1-shot** & **5-shot** & **1-shot** & **5-shot** \\  NN & 52.8 & 73.0 & 20.9 & 36.2 & 49.5 & 64.3 & 35.0 & 54.3 \\ RR & 56.6 & 84.0 & 24.0 & 56.2 & 49.9 & 68.7 & 36.8 & 62.0 \\ SVM & 52.8 & 78.7 & 20.9 & 37.3 & 49.5 & 70.8 & 35.0 & 57.7 \\  ProtoNet  & 52.8 & 79.8 & 20.9 & 39.3 & 49.5 & 71.2 & 35.0 & 57.7 \\ R2D2  & 56.7 & 84.3 & 23.8 & 56.6 & 49.7 & 68.9 & 36.8 & 62.3 \\ MetaOptNet  & 57.0 & 85.1 & 24.1 & 57.9 & 50.0 & 71.1 & 36.9 & 63.7 \\ Linear Probing & 41.9 & 78.2 & 18.3 & 47.2 & 41.0 & 65.8 & 27.2 & 56.6 \\  VPT  & 52.9 & 81.1 & 23.3 & 54.5 & 48.0 & 69.6 & 33.9 & 60.2 \\ FT & 58.0 & 88.1 & 24.1 & 66.9 & 50.3 & 72.1 & 37.0 & 66.2 \\ LoRA  & 57.9 & 88.2 & 23.3 & 64.3 & 49.9 & 71.3 & 37.1 & 65.7 \\ SSE  & 57.8 & 88.4 & 23.8 & 62.3 & 50.2 & 73.4 & 37.2 & 66.0 \\  FT + FORT & 59.5 (1.5) & 89.2 (1.1) & 25.5 (1.4) & 68.0 (1.1) & 51.1 (0.8) & 72.9 (0.8) & 38.7 (1.7) & 67.2 (1.0) \\ LoRA + FORT & 62.5 (4.6) & 89.5 (1.3) & 26.8 (3.5) & 65.7 (1.4) & 50.8 (0.9) & 72.4 (1.1) & 38.5 (1.4) & 66.9 (1.2) \\ SSF + FORT & 62.3 (4.5) & 89.6 (1.2) & 26.5 (2.7) & 64.2 (1.9) & 51.3 (1.1) & 74.4 (1.0) & 39.0 (1.8) & 67.5 (1.5) \\   

Table 1: Few-shot classification accuracy \((\%)\) on 20-way 1-shot / 5-shot tasks. We use the ViT-B/16 pre-trained on ImageNet-1K  with DINO for backbone initialization. ‘NN’, ‘RR’ and ‘FT’ denotes Nearest Neighbor classifier, Ridge Regression and full fine-tuning respectively.

the objective term \(\) satisfies

\[=_{InfoNCE}(z,z_{})- N\] (11)

Therefore, increasing the objective term \(\) can increase the mutual information between the input tokens and key patches. On the other hand, the objective term \(\) can also be derived as

\[=^{U}+^{A},^{U}=- _{n=1}^{N}_{m=1}^{N}(_{n}_{m}^{T}/ ),^{A}=_{n=1}^{N}_{t }_{n}_{t}^{T}/\] (12)

Increasing \(^{U}\) reduces the similarity between input token pairs and obtains a more uniform distribution of input tokens, which thus increases the entropy \(H(z)\). Increasing \(^{A}\) aligns the input tokens with the key patches and makes them more similar, which thus decreases the conditional entropy \(H(z|z_{})\). Since \(I(z,z_{})=H(z)-H(z|z_{})\), the objective term \(\) can increase the mutual information \(I(z,z_{})\). In fact, if \(p(_{n}|_{m})(_{n}_{q }_{k}^{T}_{m}^{T}/),n,m\{1,,N\}\), \(^{U}\) and \(^{A}\) have constant bias with the Monte-Carlo estimate of \(H(z)\) and \(-H(z|z_{})\) respectively, see derivation in appendix. Finally, the objective term \(\) increases the information from the key patches in the output feature (e.g., [CLS] token or average patch tokens) from two aspects: 1) it increases the attention scores of the output feature to the key patches, which directly increases their contribution and also reduces the noise information from other patches, as shown in Equation 4; 2) it increases the information from the key patches in the input tokens, which further transports to the output feature.

## 4 Experiments

### Few-shot classification

Settings.The experiments are conducted on the few-shot benchmark from  (i.e., CUB , Cars , Places  and Plantae  datasets), and two other fine-gained datasets: Aircraft  and Pets . We use the pre-trained ViT-B/16 released by DINO  and CLIP  and Swin-T/7 released by iBOT  for examining different architectures and pre-training ways. Here we only consider the contrastive pre-trained models  since their representations have better linear separability than the generative pre-trained models [18; 68] and are more suitable for few-shot classification. We evaluate both the backbone-frozen methods, e.g., machine learning classifiers (Nearest Neighbor classifier, Ridge Regression and Support Vector Machine), meta-solvers (ProtoNet , R2D2 

    &  &  &  &  \\   & **1-shot** & **5-shot** & **1-shot** & **5-shot** & **1-shot** & **5-shot** & **1-shot** & **5-shot** \\  MetaOptNet  & 49.1 & 81.1 & 21.0 & 51.1 & 48.9 & 70.4 & 36.0 & 63.1 \\  FT & 52.7 & 87.1 & 20.9 & 59.7 & 48.1 & 69.3 & 35.9 & 65.9 \\ LoRA  & 53.6 & 87.4 & 20.4 & 57.9 & 48.9 & 69.5 & 37.1 & 65.8 \\ SSF  & 54.9 & 87.6 & 21.4 & 57.2 & 48.7 & 70.8 & 35.9 & 64.7 \\  FT + FORT & 59.1 (6.4) & 88.3 (1.2) & 22.3 (1.4) & 61.6 (1.9) & 49.3 (1.2) & 70.4 (1.1) & 37.2 (1.3) & 66.9 (1.0) \\ LoRA + FORT & 59.9 (6.3) & 88.5 (1.1) & 21.9 (1.5) & 59.4 (1.5) & 49.7 (0.8) & 70.4 (0.9) & 38.7 (1.6) & 66.9 (1.1) \\ SSF + FORT & 60.1 (5.2) & 88.9 (1.3) & 23.4 (2.0) & 58.8 (1.6) & 49.8 (1.1) & 72.0 (1.2) & 37.6 (1.7) & 66.0 (1.3) \\   

Table 2: Few-shot classification accuracy (\(\%\)) on 20-way 1-shot / 5-shot tasks. We use the Swin-T/7 pre-trained on ImageNet-1k with iBOT for backbone initialization.

    &  &  &  &  \\   & **1-shot** & **5-shot** & **1-shot** & **5-shot** & **1-shot** & **5-shot** & **1-shot** & **5-shot** \\  MetaOptNet  & 68.0 & 88.7 & 67.6 & 90.7 & 51.6 & 73.8 & 46.2 & 73.0 \\ zero-shot  & 84.1 & 84.1 & 88.0 & 88.0 & 76.6 & 76.6 & 61.2 & 61.2 \\  CoOp  & 84.4 & 90.4 & 91.3 & 94.6 & 77.3 & 81.1 & 63.8 & 76.2 \\ Tip-Adapter-F  & 86.9 & 92.0 & 92.2 & 95.3 & 79.8 & 82.0 & 68.3 & 79.3 \\ PLOT++  & 87.4 & 92.0 & 92.2 & 95.5 & 79.9 & 82.7 & 67.7 & 78.8 \\  LoRA  & 86.3 & 92.6 & 92.3 & 95.8 & 79.8 & 84.1 & 67.4 & 80.0 \\ LoRA + FORT & 87.8 (1.5) & 93.8 (1.2) & 93.6 (1.3) & 97.0 (1.2) & 80.6 (0.8) & 84.9 (0.8) & 68.5 (1.1) & 81.0 (1.0) \\   

Table 3: Few-shot classification accuracy \((\%)\) on 20-way 1-shot / 5-shot tasks. We use the ViT-B/16 pre-trained on WIT  with CLIP for backbone initialization.

[MISSING_PAGE_FAIL:8]

Position prompts.The different pre-training ways could lead to different attention behavior in the vision transformers. Figure 3 'Attention' shows the attention map of [CLS] token in the top layer of pre-trained ViT-B/16 from both the single-modal (DINO) and multi-modal (CLIP) methods. DINO can attend to the foreground objects with the highest attention scores, while CLIP typically attends to more parts besides them. This is intuitive, since the supervision signals for the image encoder in CLIP are the sentence descriptions, which typically involve both the foreground objects and scene, e.g., 'a _bird_ flies over the _sea_'. Importantly, the key entities typically can not get the highest attention scores in CLIP pre-trained model, which makes it difficult to accurately locate them only based on the attention information. Figure 3 'Attention Prompts' shows the position prompts obtained using only the attention scores, i.e., \(=0\) in Equation 6. For DINO, the obtained position prompts can indicate the key entities, while for CLIP, the obtained position prompts could point to other class-independent parts besides the key entities. To this end, we introduce the gradient information as an auxiliary. Figure 3 'Attention+Grad Prompts' shows the position prompts obtained using both the attention and gradient information, where \(=1\) for DINO and \(=50\) for CLIP. We use MetaOptNet to calculate the classifier weight for DINO, and use language prompts for CLIP. The gradient information helps to accurately locate the key entities in CLIP, and also improve that in DINO.

Attention enhancement.We conduct qualitative and quantitative experiments to verify that our method can enhance the attention of the model to the key patches. Qualitatively, Figure 4 visualizes the patches with top highest attention scores of [CLS] token in ViT-B/16. Concretely, when the attention scores of all patches are \(a_{1},,a_{N}\) in descending order, we choose the patches \(\{1,,n\}\), which satisfy \(S_{n-1}<0.95 S_{n}\) with \(S_{n}=_{i=1}^{n}a_{i}\) and \(S_{N}=1\). We use the pre-trained model from DINO for backbone initialization and fine-tune on random 1-shot tasks. We provide the results of original attention ('Orig Attn') and the attention after fine-tuning w/ ('Our Attn') or w/o ('FT Attn') our method. The original model can not focus its attention on the key patches, nor can the full or parameter-efficient fine-tuning achieve this property. Our method can prompt the model the positions of key patches and help the model to focus most attention (\(95\%\)) on them, and this ability can generalize from the support set to query samples. This phenomenon is not obvious on full fine-tuning, and the reason may be it uses smaller learning rate and less epochs than LoRA and SSF to avoid over-fitting. The full attention maps are provided in appendix and we can observe the same phenomenon. Further, we examine the attention-weighted summation of importance scores, i.e., \(_{i=1}^{N}_{i}*_{i}^{0}\), where \(\) is defined in Equation 7 and \(^{0}\) is its position-aligned attention scores. The larger the value, the more focused on the key patches the attention. Table 5 provides the results averaged on CUB, Cars, Places and Plantae datasets with 2000 tasks each setting. The fine-tuning methods obtain the similar scores with original attention, while our FORT achieves non-trivial increase. Consistently, the increment on full fine-tuning is smaller than that on LoRA and SSF. For standard fine-tuning, the increment on 5-shot tasks is larger than that on 1-shot tasks. In fact, with more support samples, the model can attend to the frequently-occurring key entities in each class.

Importance calculation.In Section 3.4, we combine both gradient and attention information for calculating the importance score of each patch. Here we ablate the different information item and compare their performance. We follow the experimental settings in Section 4.1 and use the per-trained models from DINO or CLIP for initialization. The results are provided in Table 6, where 'FORT (attn)' denotes using only attention information (i.e., setting \(=0\) in Equation 6), 'FORT (grad)' denotes using only gradient term \(\), 'FORT (attn+grad)' denotes using both attention and gradient information, that is our final method. Here we pick the best hyper-parameters for each method, even though they may differ from each other. For DINO pre-trained models, the attention information

Figure 4: Visualization of the patches (white parts) with top highest attention scores and covering about \(95\%\) attention of [CLS] token in ViT-B/16. We use DINO pre-trained model for initialization.

can effectively locate the key patches and contributes more to the performance than the gradient information. For CLIP pre-trained models, the attention information can not indicate the positions of the key patches and thus the gradient information plays a major role. Combining both attention and gradient information works for different pre-trained models and can obtain the best performance.

Hyper-parameters.We conduct ablation studies on the hyper-parameters from Equation 6 and 8: \(\), \(\) and \(\), and examine the absolute increment of accuracy beyond the base fine-tuning methods averaged on CUB, Cars, Places and Plantae datasets. For each hyper-parameter, we set the other ones to its corresponding best values. For \(\), we use LoRA as the base fine-tuning method and evaluate different pre-trained models, i.e., ViT-B/16 from DINO and CLIP. For \(\) and \(\), we use the ViT-B/16 from DINO and evaluate different base fine-tuning methods, i.e., full fine-tuning, LoRA and SSF. Figure 5 (a) shows that using DINO pre-trained model is suitable for small \(\) since its attention is enough for locating key patches, while using CLIP pre-trained model needs large \(\) to utilize gradient information. Figure 5 (b) shows that our method is generally robust to the temperature coefficient \(\), and the performance on 1-shot tasks is relatively more sensitive than that on 5-shot tasks. Figure 5 (c) shows that the loss coefficient \(\) is an important hyper-parameter and needs to be carefully chosen, and the performance on 5-shot tasks is more sensitive than that on 1-shot tasks. In fact, if the coefficient \(\) is too large, the attention enhancement loss could relatively inhibit the optimization of the classification loss (e.g., skew gradient direction), which is harmful to fitting the input-label distribution. The more training samples, the more important it is to fit input-label distribution, so the 5-shot tasks prefer smaller coefficient \(\) than 1-shot tasks.

## 5 Conclusion and limitations

In this work, we aim to adapt the pre-trained vision transformers to few-shot image classification tasks. It is nontrivial, since even with well classifier initialization, the standard fine-tuning methods sometimes can not obtain significant improvement beyond the initialization, especially on 1-shot tasks. To this end, we propose a new prompt for vision data, position prompts, to explicitly prompt the model where are the key entities and focus on them during fine-tuning. Our method can effectively improve the performance of full or parameter-efficient fine-tuning on few-shot tasks. The limitations of our methods are: 1) our method is suitable for multi-head attention module, and unusable for other vision backbones without attention, e.g., ResNet  or MLP-Mixer ; 2) for some professional fields, e.g., medical imaging, we may still need experts to manually label the position prompts.

    &  &  \\   &  &  &  &  \\   & **1-shot** & **5-shot** & **1-shot** & **5-shot** & **1-shot** & **5-shot** & **1-shot** & **5-shot** \\  LoRA & 57.9 & 88.2 & 23.3 & 64.3 & 86.3 & 92.6 & 92.3 & 95.8 \\  + FORT (attn) & 62.1 & 89.0 & 26.3 & 65.0 & 86.6 & 92.6 & 92.5 & 95.8 \\ + FORT (grad) & 60.0 & 88.8 & 25.2 & 64.7 & 87.8 & 93.8 & 93.6 & 97.0 \\ + FORT (attn+grad) & 62.5 & 89.5 & 26.8 & 65.7 & 87.8 & 93.8 & 93.6 & 97.0 \\   

Table 6: Few-shot classification accuracy (\(\%\)) on 20-way 1-shot / 5-shot tasks. We use the ViT-B/16 pre-trained by DINO or CLIP for backbone initialization.

Figure 5: Ablation studies on hyper-parameter \(\), \(\) and \(\). We examine the absolute increment of accuracy beyond the base fine-tuning methods averaged on CUB, Cars, Places and Plantae datasets.