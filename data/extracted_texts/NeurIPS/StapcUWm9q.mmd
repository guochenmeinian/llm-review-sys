# Diffusion Model with Cross Attention as an Inductive Bias for Disentanglement

Tao Yang\({}^{1}\), Cuiling Lan\({}^{2}\), Yan Lu\({}^{2}\), Nanning Zheng\({}^{1}\)

yt14212@stu.xjtu.edu.cn,

{culan, yanlu}@microsoft.com,

nnzheng@mail.xjtu.edu.cn

\({}^{1}\)National Key Laboratory of Human-Machine Hybrid Augmented Intelligence,

National Engineering Research Center for Visual Information and Applications,

and Institute of Artificial Intelligence and Robotics, Xi'an Jiaotong University, China

\({}^{2}\)Microsoft Research Asia

https://github.com/thomasmry/EncDiff

This work was done during internship at Microsoft Research Asia.Corresponding authors.

###### Abstract

Disentangled representation learning strives to extract the intrinsic factors within the observed data. Factoring these representations in an unsupervised manner is notably challenging and usually requires tailored loss functions or specific structural designs. In this paper, we introduce a new perspective and framework, demonstrating that diffusion models with cross-attention itself can serve as a powerful inductive bias to facilitate the learning of disentangled representations. We propose to encode an image into a set of concept tokens and treat them as the condition of the latent diffusion model for image reconstruction, where cross attention over the concept tokens is used to bridge the encoder and the U-Net of the diffusion model. We analyze that the diffusion process inherently possesses the time-varying information bottlenecks. Such information bottlenecks and cross attention act as strong inductive biases for promoting disentanglement. Without any regularization term in the loss function, this framework achieves superior disentanglement performance on the benchmark datasets, surpassing all previous methods with intricate designs. We have conducted comprehensive ablation studies and visualization analyses, shedding a light on the functioning of this model. We anticipate that our findings will inspire more investigation on exploring diffusion model for disentangled representation learning towards more sophisticated data analysis and understanding.

## 1 Introduction

Disentangled representation learning aims to uncover and understand the underlying causal factors of observed data . This is believed to have immense potential to empower a multitude of machine learning tasks, facilitating the models to attain better interpretability, superior generalizability, controlled generation, and robustness . Over the years, the field of disentangled representation learning has attracted significant academic interest and many research contributions. Numerous methods, encompassing Variational Autoencoders (VAE) based techniques (such as \(\)-VAE , FactorVAE ), Generative Adversarial Networks (GAN) based approaches (such as InfoGAN , InfoGAN-CR ), along with others , have been proposed to advance this field further.

Originally, Variational Autoencoders (VAEs) are designed as deep generative probabilistic models, primarily focusing on image generation tasks . The core idea behind VAEs is to model data distributions from the perspective of maximizing likelihood using variational inference. Subsequent research has revealed that VAEs possess the potential to learn disentangled representations with appropriate regularizations on simple datasets. To enhance the disentanglement capability, a range of regularization losses have been proposed and integrated within the VAE framework [14; 19; 21]. Similarly, GANs have incorporated regularizations to promote the learning of disentangled features [5; 23; 44]. Despite significant progress, the disentanglement capabilities of these models remain less than satisfactory, and the disentangled representation learning is still very challenging. Locatello _et al._ demonstrate that relying solely on regularizations is insufficient for achieving disentanglement . They emphasize the necessity of inductive biases from both the models and the data for effective disentanglement. A fresh perspective is eagerly anticipated to shed light on this field.

Recently, diffusion models have surfaced as compelling generative models known for their high sample quality . Drawing inspiration from the evolution of VAE-based disentanglement methods, we are intrigued by the question of whether diffusion models, also fundamentally designed as deep generative probabilistic models, possess the potential to learn disentangled representations. Obtaining a compact and disentangled representation for a given image from diffusion models is non-trivial. Diffusion Autoencoder (Diff-AE)  and PDAE  advance the use of diffusion models for representation learning by encoding the image into a feature vector and incorporating this into the diffusion generation process. However, these representations do not exhibit disentanglement characteristics. What inductive biases are essential for the learning of disentangled representations? Could we have a diffusion-based framework possessing such inductive biases?

Notably, in text-to-image generation, a conditional diffusion model integrates the '_disentangled_' text tokens through cross attention, demonstrating the ability to generate semantically aligned images [30; 36; 12]. Interestingly, the observed cross-attention map reveals that different words have their corresponding spatial regions of high affinities, exhibiting strong semantic and spatial alignment as illustrated in Figure 1. These disentangled representations of 'words' could potentially contribute to a more streamlined generation process. Inspired by this, we wonder whether such diffusion structure with cross-attention can act as an inductive bias to facilitate disentangled representation learning.

In this paper, we investigate this question and explore the potential of diffusion models in disentangled representation learning. We discover that the diffusion model with cross attention can serve as a strong inductive bias to drive disentangled representation learning, even without any additional regularization. As illustrated in Figure 2 (a), we employ an encoder to transform an image into a set of concept tokens, which we treat as 'word' tokens, acting as the conditional input to the latent diffusion model with cross-attention. Here, cross attention bridges the interaction between the diffusion network and the image encoder. We refer to this scheme as EncDiff. EncDiff is powered by two valuable inductive biases, _i.e._, information bottleneck in diffusion, and cross attention for fostering 'word' (concept token) and spatial alignment, contributing to the disentanglement. Experimental results on benchmark datasets demonstrate that EncDiff achieves excellent disentanglement performance, surpassing the previous methods with elaborate designs. Comprehensive ablation studies show that the strong disentanglement capability is mainly attributed to 1) the diffusion modelling and 2) the cross-attention interaction. Visualization analysis provides insights into the effectiveness of the disentangled representations.

Figure 1: Average attention map across all time steps in stable diffusion. We draw inspiration from the process of text-to-image generation using a diffusion model with cross-attention. Utilizing the highly ‘disentangled’ words as the condition for image generation, the cross-attention maps observed from the diffusion model exhibit a strong text semantic and spatial alignment, indicating the model is capable of incorporating each individual word into the generation process for a final semantic aligned generation. This leads us to question whether such a diffusion structure could be inductive to disentangled representation learning.

We have four main contributions.

* We reveal that the diffusion model with cross attention can act as a strong inductive bias for enabling disentangled representation learning.
* We introduce a simple yet effective framework, EncDiff, powered by a latent diffusion model with cross attention and an ordinary image encoder, for disentangled representation learning.
* Our framework inherently leverages two valuable inductive biases: the information bottleneck in diffusion, and cross attention, fostering concept token learning and spatial alignment. We analyze that the diffusion process inherently exhibits time-varying information bottlenecks, which is vital for disentangled representation learning.
* Without additional regularization or specific designs, our framework achieves state-of-the-art disentanglement performance, even outperforming the latest methods with more complex designs.

We anticipate that this new perspective will illuminate the field of disentanglement and inspire deeper investigations, paving the way for future sophisticated data analysis, understanding, and generation.

## 2 Related Work

**Disentangled Representation Learning** Disentangled Representation Learning endeavours to train a model proficient in disentangling the underlying factors of observed data [1; 13; 33]. A plethora of methods have been proposed to augment the generative models of VAEs and GANs, endowing them with disentanglement capability. These methods primarily rely on probability-based regularizations applied to the latent space. To improve disentanglement, most approaches focus on how to regularize the original VAE. For instance, this includes weighting the evidence lower bound (ELBO) as in \(\)-VAE , or introducing different terms to the ELBO, such as mutual information (InfoVAE , InfoMax-VAE ), total correlation (Factor-VAE ), and covariance (DIP-VAE ). Locatello _et al._ demonstrate that relying solely on these regularizations is insufficient for achieving disentanglement. DIP-VAE  introduces a regularizer on the expectation of the approximate posterior over observed data, by matching the moments of the distributions of latents. In this paper, we investigate the disentanglement capability of diffusion models and demonstrate that diffusion models with cross-attention can serve as a powerful inductive bias for disentanglement.

**Diffusion Models** Diffusion models have emerged as a powerful new family of deep probabilistic generative models , surpassing VAEs and GANs for image generation and many other tasks. Diffusion models progressively perturb data by injecting noise and then learn to reverse this process for the generation. A question arises as to whether diffusion models can effectively serve as disentangled

Figure 2: (a) Illustration of our framework EncDiff. We employ an image encoder \(_{}\) to transform an image \(I\) into a set of disentangled representations, which we treat them as the conditional input to the latent diffusion model with cross attention. Here cross attention bridges the interaction between the diffusion network and the image encoder. For simplicity, we only briefly show the diffusion model which consists of an encoder \(E\), a denoising U-Net and a decoder \(D\) that reconstructs the image from the latent \(x_{t}\). (b) Information bottleneck reflected by KL divergence in reverse diffusion process. The KL divergence between the data distribution \(q(x_{t-1}|x_{t},x_{0})\) and the Gaussian prior distribution \((0,)\) under four different variance (\(\)) schedules: cosine, linear, sqrt linear and sqrt. The results have been normalized by the number of dimensions.

representation learners. It is challenging to obtain a compact yet disentangled representation of an image from a diffusion model. Diff-AE  and PDAE  investigate the possibility of using DPMs for representation learning, whereby an input image is autoencoded into a latent vector. However, these representations do not manifest disentangled characteristics. SlotDiffusion  and LSD  integrate diffusion models into object-centric learning, where diffusion acts as an improved slot-to-image decoder, and slot attention is still the key to promoting object-centric learning. Moreover, they aim to learn object-wise representation but still cannot disentangle the factors/attributes of an object/a scene. Limited research has explored disentangled representation learning by leveraging diffusion models. InfoDiffusion  encourages the disentanglement of the latent feature of Diff-AE  by introducing mutual information and prior regularization, similar to InfoVAE . CL-Dis  introduces a VAE to guide the diffusion model to learn disentangled representation. DisDiff  employs a pre-trained diffusion model for disentangled feature learning. DisDiff adopts an encoder to learn the disentangled representations and a decoder to learn the sub-gradient field for each disentangled factor. It requires multiple decoders to predict these sub-gradient fields for all the factors and complex disentanglement losses, resulting in a costly and intricate process. The concurrent work SODA  leverages a diffusion model to generate novel view of an image for representation learning, revealing the capability of capturing visual semantics to diffusion model. Is it necessary to impose these complicated regularizations upon diffusion models as ? Does a strong inductive bias that facilitates disentanglement already exist within diffusion models? In this paper, we endeavour to answer these questions and illustrate that a simple framework driven by a diffusion model without any additional regularization is capable of achieving superior disentanglement performance.

## 3 Method

We aim to investigate the potential of diffusion models in disentangled representation learning. We propose a simple yet effective framework, EncDiff, that exhibits strong disentanglement capabilities, even without additional regularizations. We analyze and identify two valuable inductive biases, _i.e_., information bottleneck in diffusion, and the cross attention for fostering 'word' (concept token) and spatial alignment, thereby promoting disentanglement. We elaborate on the framework design in subsection 3.1 and the analysis in subsection 3.2, respectively.

### Framework of EncDiff

Figure 2 (a) illustrates the flowchart. It consists of an image encoder that transforms an input image into a set of concept tokens and a diffusion model that serves as the decoder to reconstruct the image. Cross-attention is employed as the bridge for the diffusion network and the image encoder.

**Image Encoder** For a given input image \(I\), the image encoder \(_{}\) aims to provide a set of concept tokens \(=\{s_{1},,s_{N}\}\), which act similarly to the word embeddings in the prompts for text-to-image generation in the latent diffusion models (LDMs) . Without loss of generality, we use an ordinary CNN network as the image encoder to obtain concept tokens. Following the design of the encoder in VAE , we use a fully connected layer to transform the feature map into a feature vector. We treat each dimension of the encoded feature vector as a disentangled factor and map each factor to a vector (_i.e_., concept token) by non-shared MLP layers (as illustrated in Figure 3).

**Diffusion Model with Cross Attention** We follow LDM  to construct our diffusion model in the latent space, which demonstrates superior generation ability. LDM is one of the most popular diffusion models, proposing to conduct diffusion denoising in the latent space. To condition the concept tokens during image generation, cross-attention is used to map these tokens into the intermediate representations of the U-Net in the diffusion model. This is accomplished by using the cross-attention defined as \(Attention(Q,K,V)=(QK^{T}\\ ) V\), where the spatial feature in the intermediate feature map in diffusion model serves as a query, the concept tokens act as keys and values.

**End-to-End Training** We conduct an end-to-end training of the encoder and the diffusion model, utilizing the optimization objective of reconstructing noise, which is a methodology aligned with that employed in LDM .

### Inductive Biases

We analyze that there are two crucial inductive biases in diffusion models: the information bottleneck in diffusion, and the cross-attention interaction. We analyze that the diffusion process inherently possesses the time-varying information bottlenecks.

#### 3.2.1 Information Bottleneck in Diffusion

\(\)-VAE  and AnnealVAE  leverage the Kullback-Leibler (KL) divergence in VAEs to enhance the disentanglement capability, where the KL divergence constraint plays a role of an information bottleneck. Here, we analyze the presence of an information bottleneck mechanism that promotes the disentanglement in diffusion models. Without loss of generality, our analysis focuses on the diffusion model in image latent space . The analysis also holds in pixel space.

Within the framework of the latent diffusion model, we add Gaussian noise to an image latent \(x_{0}\) over \(T\) steps according to a variance schedule \(_{1},,_{T}\). This process yields a sequence of noisy samples \(x_{1},,x_{T}\),

\[q(x_{t}|x_{t-1}):=(x_{t};}x_{t-1},_{t}).\] (1)

Let \(_{t}=1-_{t}\) and \(_{t}=_{i=1}^{t}_{t}\). \(x_{t}\) can be obtained using the following equation [15; 36]:

\[x_{t}=_{t}}x_{0}+_{t}},\] (2)

where the noise \(\) is sampled from a Gaussian distribution \((0,)\).

The diffusion model optimizes a network (_e.g._, U-Net) \(_{}\) to predict the noise from the noisy input \(x_{t}\) and the conditioning input \(\) (concept tokens), with the loss function defined as

\[_{r}=_{x_{0},,t}\|_{}(x_{t},t, )-\|.\] (3)

Here, we omit the weighting terms of the loss function for simplicity. The latent \(x_{0}\) can be reconstructed based on the predicted noises.

Let's analyze the inherent information bottleneck at each time step \(t\) in the reverse diffusion process. In the reverse diffusion process, the reverse conditional distribution is tractable as :

\[ q(x_{t-1}|x_{t},x_{0})&= (x_{t-1}|_{t},_{t}),\\ &_{t}=_{t-1}}_{t}}{1-_{t}}x_{0}+}(1- _{t-1})}{1-_{t}}x_{t},_{t}=_{t-1}}{1-_{t}}_{t}.\] (4)

We formulate the Kullback-Leibler (KL) divergence \(C_{t}\) between \(q(x_{t-1}|x_{t},x_{0})\) and the Gaussian prior distribution \(p(x_{t-1})=(0,)\) at step \(t-1\) as:

\[C_{t}=D_{KL}(q(x_{t-1}|x_{t},x_{0})||p(x_{t-1}))=(-_{t}-1-_{t}+_{t}^{T}_{t}/n),\] (5)

where \(n\) denotes the number of dimension of signal \(x\) (_i.e._, \(x_{0}\), \(x_{t}\), \(x_{t-1}\)).

In Figure 2 (b), we present the curves that illustrate the KL divergence \(C_{t}\) under different variance (\(\)) schedules, including linear, sqrt linear, cosine [25; 15], and sqrt schedules [6; 30]. We can see that as the time step \(t\) decreases, the KL divergence \(C_{t}\) increases, indicating the information carried by \(x_{t-1}\) increases and resulting in increasingly looser information bottlenecks over \(x_{t-1}\). According to [2; 14], such time-varying information bottlenecks may play an important role in promoting disentanglement. Different variance (\(\)) schedules results in different KL divergence curves. We found that these different variance schedules lead to different disentanglement performance (see Subsection 4.4).

Actually, optimizing the loss of conditional diffusion model as in (3) is equivalent to push the reverse conditional distribution \(p_{}(x_{t-1}|x_{t},)\) to approach \(q(x_{t-1}|x_{t},x_{0})\) (see the explanation in Appendix B). By this means, the information bottlenecks \(_{t}=D_{KL}(p_{}(x_{t-1}|x_{t},)||p(x_{t-1}))\) over \(x_{t-1}\) tend to approach \(C_{t}=D_{KL}(q(x_{t-1}|x_{t},x_{0})||p(x_{t-1}))\) in training for all the time steps. According to the theorem in Appendix C, the information bottleneck over latent \(x_{t-1}\) is transferred to the condition \(\) (_i.e._, concept tokens). Intuitively, this is because \(x_{t-1}\) is controlled by the concept tokens and the network parameters \(\), as indicated by \(p_{}(x_{t-1}|x_{t},)\). The concept token representations \(\) are learnable, and the information bottleneck is transferred to and imposed on \(\). With time-varyinginformation bottlenecks, the diffusion process encourages a range of different information capacities on \(\) during the diffusion process, promoting the disentanglement of concept tokens.

**Discussion** The information bottleneck described above shares a certain similarity with the optimization objective in AnnealVAE . The minimizing objective of AnnealVAE is expressed as follows:

\[(,)= -E_{q_{}(z|x)}[ p_{}(x|z)]+\|D_{KL}(q_{} (z|x)||p(z))-C\|,\] (6)

where \(,\) are the parameters of the encoder and decoder; the latent representation and data are denoted as \(z,x\), respectively. \(C\) is a handcrafted constant used to control the information bottleneck of the latent space. Different factors to be disentangled may contain different amounts of information. Instead of using a constant during training, AnnealVAE dynamically allocates larger amount of information (larger \(C\)) to the latent units as the training iteration increases, where different factors can be learned at various training stages.

In diffusion models, where the KL divergence characterizes the amount of information, we observe that the amount of information varies in reverse diffusion steps, see Figure 2 (b).

#### 3.2.2 Cross Attention for Interaction

The information bottleneck sheds a light on disentangling, acting as an inductive bias for diffusion. However, using the information bottleneck still only has theoretical feasibility. Its effectiveness also relies on the structure design of the diffusion model. We believe that the cross-attention design in conditional diffusion model is crucial for disentanglement, serving as another effective inductive bias.

Our objective is to train an encoder that obtains a set of concept tokens taking an image as input under the guidance of the diffusion model. We take the output of the encoder as the condition of the U-Net of diffusion model for image generation. We incorporate the concept tokens into diffusion model through cross attention. Intuitively, a spatial position of an image is related to several concepts, _e.g_., object color and shape in Shapes3D. Each spatial feature is composed of several related concept-based representations. Interestingly, cross attention in the U-Net play a similar role, where each spatial feature servers as the query, and the learned concept tokens are used as the keys and values to refine the query. In Subsection 4.4, we validate the necessity of the two inductive biases leading to disentanglement.

## 4 Experiments

### Experimental Setup

**Implementation Details** The trainable parts are the encoder and diffusion model. We employ the popular diffusion structure of latent diffusion  by default. Without loss of generality, following , we use the VQ-reg to avoid arbitrarily high-variance latent spaces and sample images in 200 steps. We adopt the cosine schedule as the variance (\(\)) schedule in the diffusion model by default. By default, we use a CNN encoder for the image encoder to obtain a set of disentangled concept tokens. We use a CNN encoder similar to that used in . We denote our scheme as EncDiff.

**Training Details** During the training phase of EncDiff, we maintain a consistent batch size of 64 across all datasets. The learning rate is consistently set to \(1 10^{-4}\). We adopt the standard practice of employing an Exponential Moving Average (EMA) with a decay factor of \(0.9999\) for all model parameters. The training hyper-parameters follows DisDiff  and DisCo . For each concept token, we follow DisDiff  to use a \(32\) dimensional representation vector. We train EncDiff on a single Tesla V100 16G GPU. A model takes about 1 day for training.

**Datasets** To evaluate the disentanglement performance, we utilize the commonly used benchmark datasets: Shapes3D , MPI3D  and Cars3D . Shapes3D  consists of a collection of 3D shapes. MPI3D is a dataset of 3D objects created in a controlled setting. Cars3D is a dataset consisting of 3D-rendered cars. For real-world data, we conduct our experiments using CelebA, a dataset of celebrity faces with attributes. Our experiments are carried out at a 64\(\)64 image resolution, consistent with previous studies [19; 4; 28; 40].

**Baselines & Metrics** We compare the performance of our method with VAE-based, GAN-based, and diffusion-based methods, following the experimental protocol as in DisCo . The VAE-basedmodels we use for comparison are FactorVAE  and \(\)-TCVAE , while the GAN-based baselines include InfoGAN-CR , GANspace (GS) , LatentDiscovery (LD)  and DisCo . Each method utilizes scalar-valued representations. DisDiff  uses vector-valued representations. EncDiff has two kinds of representations simultaneously. We focus on the scalar-valued in the main paper. We follow DisDiff to set \(N\) to 20. For these vector-valued representations, we follow [7; 40; 38] to perform PCA as a post-processing on the representation before evaluation. To assess the potential variability in performance due to random seed selection, we have fifteen runs for each method for reliable evaluation, reporting the mean and variance. Regarding evaluation metrics, we adopt two representative metrics, the FactorVAE score  and the DCI .

### Comparison with the State-of-the-Arts

We compare the disentanglement ability of our EncDiff with the state-of-the-art methods. Table 1 shows quantitative comparison results of disentanglement under different metrics. We can see that EncDiff achieves the best performance on all the datasets except Cars3D, showcasing the model's superior disentanglement ability. EncDiff achieves superior performance by leveraging the strong inductive bias from the diffusion model with cross-attention without using any additional regularization losses. EncDiff also outperforms InfoDiffusion  and DisDiff  by a significant marginal, even though DisDiff uses complex disentanglement loss and inference the decoder multiple times for prediction sub-gradient fields. On the Cars3D dataset, the quantitative evaluation is not so reliable because some factors, such as color and shape, are not included in the labels. From the visualization in Figure 6 of Appendix E, we can see that EncDiff achieves superior disentanglement compared to DisDiff despite the lower FactorVAE score.

    &  &  &  \\   & FactorVAE score\(\) & DCT\(\) & FactorVAE score\(\) & DCT\(\) & FactorVAE score\(\) & DCT\(\) \\   \\  FactorVAE  & \(0.906 0.052\) & \(0.161 0.019\) & \(0.840 0.066\) & \(0.611 0.082\) & \(0.152 0.025\) & \(0.240 0.051\) \\ \(\)-TCVAE  & \(0.855 0.082\) & \(0.140 0.019\) & \(0.873 0.074\) & \(0.613 0.114\) & \(0.179 0.017\) & \(0.237 0.056\) \\   \\  InfoGAN-CR  & \(0.411 0.013\) & \(0.020 0.011\) & \(0.587 0.058\) & \(0.478 0.055\) & \(0.439 0.061\) & \(0.241 0.075\) \\   \\  LD  & \(0.852 0.039\) & \(0.216 0.072\) & \(0.805 0.064\) & \(0.380 0.062\) & \(0.391 0.039\) & \(0.196 0.038\) \\ GS  & \(0.932 0.018\) & \(0.209 0.031\) & \(0.785 0.091\) & \(0.284 0.034\) & \(0.465 0.036\) & \(0.229 0.042\) \\ DisCo  & \(0.855 0.074\) & \(0.271 0.037\) & \(0.877 0.031\) & \(0.708 0.048\) & \(0.371 0.030\) & \(0.292 0.024\) \\   \\  DisDiff  & \( 0.018\) & \(0.232 0.019\) & \(0.902 0.043\) & \(0.723 0.013\) & \(0.617 0.070\) & \(0.337 0.057\) \\ EncDiff (Ours) & \(0.773 0.060\) & \( 0.022\) & \( 0.000\) & \( 0.030\) & \( 0.049\) & \( 0.044\) \\   

Table 1: Comparisons of disentanglement on the FactorVAE score and DCI disentanglement metrics (mean \(\) std, higher is better). EncDiff outperforms the state-of-the-art methods with a large margin except on Cars3D.

   Model & TAD \(\) & FID \(\) \\  \(\)-VAE  & \(0.088 0.043\) & \(99.8 2.4\) \\ InfoVAE  & \(0.000 0.000\) & \(77.8 1.6\) \\ Diff-AE  & \(0.155 0.010\) & \(22.7 2.1\) \\ InfoDiffusion  & \(0.299 0.006\) & \(23.6 1.3\) \\ DisDiff  & \(0.305 0.010\) & \(18.2 2.1\) \\  EncDiff & \(\) & \(\) \\   

Table 2: Comparisons of disentanglement performance and generation quality in terms of TAD and FID metrics (mean \(\) std) on real-world dataset CelebA. EncDiff achieves the state-of-the-art performance on both aspects compared to all baselines.

Figure 3: Illustration of the encoder \(_{}\), which transforms an image into a feature vector of dimension \(N\), with each dimension (scalar) encoding a disentangled factor. We then use non-shared three-layer MLP layers to map each scalar into a vector (concept token). The concept tokens will be treated as the conditional input to the latent diffusion model with cross attention.

Moreover, we have conducted experiments on real-world dataset CelebA. Table 2 shows the comparisons of disentanglement performance and generation quality in terms of TAD and FID metrics (mean \(\) std). EncDiff achieves the state-of-the-art performance on both aspects compared to all baselines.

In addition, our EncDiff achieves superior reconstruction quality (see Appendix G for more details).

### Visualization

**Visualization Analysis on the Disentanglement** We qualitatively examine the disentanglement properties of our proposed method. We interchange the concept tokens (factors) of the learned representation of two distinct images and observe the generated images conditioned on these exchanged representations. For illustration purposes, we focus on the widely used Shapes3D dataset from the disentanglement literature and show the results in Figure 4. We can see that our EncDiff successfully isolates factors. Notably, in comparison to VAE-based methods, EncDiff delivers superior image quality quantitatively (see Appendix H).

**Visualization of Learned Cross-Attention Maps** As mentioned in Section 1, our model draws inspiration from the alignment between 'word' tokens (disentangled representations) and spatial features. The alignment is demonstrated by the learned cross-attention maps. We verify whether our learned concept tokens present the disentangled characteristics by visualizing the alignment of concept tokens with spatial positions through cross-attention maps. As depicted in Figure 5, the results of our model exhibit exemplary alignment between concept tokens and spatial positions. Different concept tokens are associated with varying attended spatial regions, corresponding to different semantics that are comprehensible by humans, such as the region of "Floor" and "Color" for the images from Shapes3D.

Figure 4: The qualitative results on Shapes3D and MPI3D. The source (SRC) images provide the representations of the generated image. The target (TRT) image provides the representation for swapping. Other images are generated by swapping the representation of the corresponding factor. For Shapes3D, the learned factors on Shapes3D are wall color (Wall), floor color (Floor), object color (Color), and object shape (Shape), orientation (Orien), scale. See Appendix E for more visualizations.

Figure 5: Visualization of the cross-attention maps on Shapes3D and MPI3D. The first column shows the original image while the other columns show the attention masks for different concept tokens. See Appendix F for more visualizations. “Pos” represents “Position”.

### Ablation Study

In our framework, in order to analyze and understand the key factors contributing to the advancement of disentangled representation learning, we conduct ablation studies covering three aspects in the design: 1) whether to use diffusion as the decoder; 2) whether to use cross attention as the bridge for interaction; 3) influence of different variance (\(\)) schedules. We conduct these ablation studies on the Shapes3D dataset. Please see Appendix H for more ablation studies.

**Using Diffusion as Decoder or Not** To validate whether the use of a diffusion model as an inductive bias for disentangled representation learning is crucial, we employ a network structure similar to the U-Net in our used diffusion model as the decoder, utilizing reconstruction \(l_{2}\) loss to optimize the entire network. We designed a variant (EncDec w/o Diff) of EncDiff to have an autoencoder-like structure, by reusing the image encoder as encoder and the lower half of the U-Net structure as decoder for reconstruction. In contrast to EncDiff, we discard the multiple step diffusion process and only run once feedforward inference to get the reconstruction. If the autoencoder's performance drops significantly, this will provide evidence of the importance of the diffusion process instead of the U-Net architecture. Specifically, we remove the encoder part of the U-Net and the skip connection between it and the decoder part of the U-Net. We then feed the U-Net decoder with a randomly initialized learnable spatial tensor to maintain the structure of the decoder U-Net. Similarly to EncDiff, the encoded disentangled representations are input to the decoder through cross-attention (CA). We refer to this scheme as _EncDec w/o Diff_. Table 3 shows the results. The performance of _EncDiff_ with diffusion significantly outperforms _EncDec w/o Diff_ by 0.46 and 0.79 in terms of FactorVAE score and DCI, respectively. This indicates that inductive bias from diffusion modelling is crucial for achieving effective disentanglement.

**Using Cross Attention for Interaction** To incorporate the image representation into the diffusion model as a condition, we use cross attention by treating each disentangled representation as a conditional token (similar to the use of 'word' token in text-to-image generation in stable diffusion model ). As an alternative, similar to that in Diff-AE  and InfoDiffusion , we use adaptive group normalization (AdaGN) to incorporate the representation vector (by concatenating the concept tokens) to modulate the spatial features. We name this scheme as _EncDiff w/ AdaGN_. Table 3 presents the results. We can see that _EncDiff w/ AdaGN_ is inferior to _EncDiff_, with a significant decrease of 0.33 in terms of DCI. Cross-attention facilitates the alignment of each concept token with the corresponding spatial features, akin to the alignment of the 'word' token to spatial features in the text-to-image generation. In contrast, AdaGN did not efficiently promote disentanglement.

**Influence of Different Variance (\(\)) Schedules** We investigate the influence of the different variance (\(\)) schedules, including including linear, sqrt linear, cosine [25; 15], sqrt schedules [6; 30], on the disentanglement performance. From Table 4, we can see that distinct schedules result in different performance, demonstrating the influence on disentanglement of different information bottleneck schedules. Note that the FactorVAE scores are all very high and cannot well reflect the performance. We prefer to use DCI metric here for evaluation. We can see that the cosine schedule performs the best and we adopt it by default. The linear schedule approaches that of the sqrt linear in terms of the curve shape (see Figure 2 (b)) and they achieve the similar performance in terms of DCI.

**Scalar-valued vs. Vector-valued Manners** We treat each dimension of the encoded feature vector as a disentangled factor, followed by a mapping to a concept token (vector) for each factor. Another design alternative is to split the feature vector into \(N\) chunks, with each chunk being a concept token, similar to DisDiff . We name this vector-valued design and refer to it by DisDiff-V. Table 5 shows that EncDiff outperforms EncDiff-V. The intermediate scalar design in EncDiff may serve a

   Method & FactorVAE score\(\) & DCI\(\) \\  EncDiff w/sqrt & \(0.997 0.011\) & \(0.950 0.041\) \\ EncDiff w/sqrt linear & \(0.988 0.026\) & \(0.924 0.050\) \\ EncDiff w/linear & \(0.999 0.002\) & \(0.930 0.045\) \\  EncDiff w/cosine & \(\) & \(\) \\   

Table 4: Ablation study on the influence of the variance (\(\)) schedule. We use four kinds of variance schedules: sqrt, cosine, linear, and sqrt linear.

   Method & FactorVAE score\(\) & DCI\(\) \\  EncDev w/o Diff & \(0.537 0.074\) & \(0.178 0.050\) \\ EncDiff w/ AdaGN & \(0.911 0.101\) & \(0.637 0.068\) \\  EncDiff & \(\) & \(\) \\   

Table 3: Influence of the two inductive biases. For _EncDec w/o Diff_, we replace the diffusion model with a decoder while cross-attention is preserved. For _EncDiff w/ AdaGN_, we replace the cross-attention with AdaGN.

bottleneck role and contribute to the disentanglement. We think that the vector-based representation potentially extracts more information and hence enforces a looser bottleneck than scalar-valued representation. Note that the more information encoded, there is a higher probability that the encoded information is correlated, which is contradictory to disentanglement. Therefore, the tighter bottleneck from scalar-valued representation leads to (a slightly) better performance.

**Results on Pixel Space** As stated in Section 3.2.1, our analysis is applicable in pixel space as well. In order to verify this, we trained EncDiff directly in pixel space on the Shapes3D dataset, which we denote the scheme as EncDiff pixel. The results of the disentanglement analysis are presented in Table 6. The performance of our framework in pixel space remains robust, indicating that the operation in latent space is not a critical factor for achieving effective disentanglement.

### Computational Complexity

We compare the computational complexity of Diff-AE , DisDiff , and our EncDiff in terms of the parameters (Params.), floating-point operations (FLOPs), and inference time (seconds/sample) for sampling an image. As shown in Table 7, our EncDiff demonstrates much higher computational efficiency than Diff-AE and DisDiff.

## 5 Limitations

Our method operates in a fully unsupervised manner and exhibits strong disentanglement capability on simple datasets. Similar to other disentanglement-based methods [14; 19; 5; 23; 37], obtaining satisfactory performance on complex data remains a challenge. As a diffusion-based method, the generation speed of EncDiff is faster than DisDiff . However, it is still slower when compared to VAE-based and GAN-based methods. More effective sampling strategies, as employed in DPM-based methods, could be utilized for accelerating in the future.

## 6 Conclusion

This paper unveils a fresh viewpoint, demonstrating that diffusion models with cross-attention can serve as a strong inductive bias to foster disentangled representation learning. Within our framework EncDiff, we reveal that the diffusion model structure with cross-attention can drive an image encoder to learn superior disentangled representations, even without any regularization. Our comprehensive ablation studies demonstrate that the strong capability is mainly attributed to diffusion modelling and cross-attention interaction. This work will inspire further investigations in diffusion for disentanglement, paving the way for sophisticated data analysis, understanding, and generation.

## 7 Acknowledgement

We thank all the anonymous reviewers for their constructive and helpful comments, which have significantly improved the quality of the paper. The work was partly supported by the National Natural Science Foundation of China (Grant No. 62088102).

   Method & FactorsVAE score\(\) & DCI\(\) \\  EncDiff-V & \(0.999 0.000\) & \(0.900 0.045\) & EncDiff pixel & \(1.0 0.000\) & \(0.981 0.015\) \\ EncDiff & \(\) & \(\) & EncDiff & \(0.999 0.000\) & \(0.969 0.030\) \\   

Table 5: Ablation study on the two design alternatives on obtaining the token representations.

   Method & Params.\(\) (M) & FLOPs\(\) (M) & Time\(\) (s) \\  Diff-AE  & \(67.8\) & \(3955.1\) & \(31.0\) \\ DisDiff  & \(57.1\) & \(5815.8\) & \(35.3\) \\  EncDiff & \(42.3\) & \(2898.5\) & \(11.8\) \\   

Table 6: Ablation study on the space applying diffusion model.