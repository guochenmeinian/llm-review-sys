# Mitigating Object Hallucination in Large Vision-Language Models via Image-Grounded Guidance

Linxi Zhao\({}^{*1}\), Yihe Deng\({}^{*2}\), Weitong Zhang\({}^{2}\), Quanquan Gu\({}^{2}\)

\({}^{1}\)Cornell University

\({}^{2}\)University of California, Los Angeles

Equal contribution.

###### Abstract

The advancement of Large Vision-Language Models (LVLMs) has increasingly highlighted the critical issue of their tendency to hallucinate non-existing objects in the images. To address this issue, previous works focused on using specially curated datasets or powerful LLMs (e.g., GPT-3.5) to rectify the outputs of LVLMs. However, these approaches require either expensive training/fine-tuning or API access to advanced LLMs for post-generation correction. In response to these limitations, we propose Mitigating hallucinAtion via image-g**R**ounded **g**u**d**a**l**a**l**a**l**a**l**a**l**(**M**ARINE**), a framework that is both _training-free_ and _API-free_. MARINE effectively and efficiently reduces object hallucinations during inference by introducing image-grounded guidance to LVLMs. This is achieved by leveraging open-source vision models to extract object-level information, thereby enhancing the precision of LVLM-generated content. Our framework's flexibility further allows for the integration of multiple vision models, enabling more reliable and robust object-level guidance. Through comprehensive evaluations across \(5\) popular LVLMs with diverse evaluation metrics and benchmarks, we demonstrate the effectiveness of MARINE, which even outperforms existing fine-tuning-based methods. Remarkably, it reduces hallucinations consistently in GPT-4V-assisted evaluation while maintaining the detailedness of LVLMs' generations.

## 1 Introduction

The advent of Large Language Models (LLMs) has motivated advancements in extending their remarkable capabilities to multimodal data. Grounded in the development of pre-trained vision-language models (Radford et al., 2021; Jia et al., 2021; Alayrac et al., 2022) that align visual and textual embedding spaces, Large Vision Language Models (LVLMs) have gained substantial attention in both architectural development (Liu et al., 2023; Zhu et al., 2023; Ye et al., 2023; Dai et al., 2023; Gao et al., 2023), alignment (Yu et al., 2024; Zhou et al., 2024; Deng et al., 2024) and benchmarking datasets (Xu et al., 2023; Lu et al., 2024; Zhang et al., 2024). However, similar to the hallucination issues in textual LLMs (Ji et al., 2023), where irrelevant content is generated with input prompts, LVLMs face a specific challenge known as object hallucination: generating non-existing objects for a given image (Li et al., 2023; Wang et al., 2023; Zhou et al., 2023; Fu et al., 2023; Lovenia et al., 2023; Jing et al., 2023). Such a problem is particularly concerning as it compromises the model's accuracy and reliability, especially considering the growing application of LVLMs to safety-critical downstream tasks such as medical imaging (Chambon et al., 2022; Bazi et al., 2023).

In response to the pressing issue of object hallucinations in LVLMs, early attempts (Liu et al., 2023;b; Gunjal et al., 2023; Wang et al., 2023) focused on addressing the bias by curating high-quality datasets for fine-tuning or leveraging advanced GPT queries (Yin et al., 2023), such as GPT-4, to post-process the generated captions. However, these methods can be infeasible to implement. For instance, creating extensive, high-quality datasets for fine-tuning LVLMs is costly and requires significant human annotation. Additionally, relying on advanced GPT models for post-processing is expensive and can raise privacy concerns, especially in sensitive fields like medical imaging. Most importantly, these approaches do not address the _intrinsic_ causes of object hallucination in LVLMs. Specifically, fine-tuning simply provides more data for the LVLM to learn, which can lead to overfitting to aparticular dataset, as seen with methods like LURE (Zhou et al., 2023). Post-processing methods may also introduce new hallucinations, as they do not inherently correct the root cause of hallucinations in LLMs or LVLMs but just overwrite the generated response.

In this paper, we investigate the intrinsic causes of object hallucination in LVLMs. Specifically, these deficiencies may stem from the three main components of the LVLMS: 1) insufficient visual context provided by the visual encoder (Zhang et al., 2023b), 2) misalignment between the vision and text domains, and 3) inherent hallucinations common in general language models. To address the first two LVLM-specific causes, we introduce **M**itigating hallucin**A**tion via image-g**R**ounded gu**I**da**N**c**E** (MARINE). MARINE mitigates hallucination issues arising from the visual encoder and domain misalignment by leveraging external guidance from image-grounded models, such as object detection models. Our approach leverages the inherent advantage of these image-grounded models, which are specifically designed and trained for more detailed visual information extraction. These models provide higher quality, fine-grained visual encoding compared to the standard visual encoders in LVLMs, which are primarily optimized for grasping the overall context of an image. Furthermore, we integrate the guidance from image-grounded models into text descriptions, allowing the LVLM to process the information without requiring additional alignment procedures. As a result, MARINE is a training-free, API-free* method that addresses object hallucination at inference time by targeting its two root causes.

Footnote *: The term “API-free” in denotes the elimination of any need for API calls to OpenAI. We note that Woodpecker requires 3-5k input tokens for an API call to each short captioning task.

As shown in Figure 1, MARINE incorporates one or more image-grounding models to enrich the visual context of LVLMs. The guidance are then aggregated as prompt input to the LLM decoder to improve the response quality. Empirical evaluations are conducted on five widely-recognized LVLMs across benchmarks including MSCOCO (Lin et al., 2014), LLaVA-QA90 task (Liu et al., 2023d), A-OKVQA (Schwenk et al., 2022), and GQA (Hudson and Manning, 2019). We present results based on guidance from a aggregated source of DEIRC (Carion et al., 2020) and RAM++ (Huang et al., 2023b). We also include ideal results based on ground truth object oracle, denoted as MARINE-Truth. Our experimental results demonstrate that, in comparison with state-of-the-art algorithms, MARINE exhibits further reduced hallucination, as measured by popular hallucination metrics such as CHAIR (Rohrbach et al., 2018) and POPE (Li et al., 2023b), as well as additional metrics considered in this study including the recall and GPT-4V's evaluation of the responses. These results confirm that MARINE can effectively mitigate object hallucinations without requiring additional training resources or access to advanced LLMs. To summarize, our contribution are listed as follows:

* We introduce MARINE, a universal framework and aggregating a toolbox of image-grounded visual models to guide the generation process of LVLMs. MARINE leverages the intrinsic advantages of these visual models in providing the detailed information of the input image and help mitigate the hallucinations in LVLMs.

Figure 1: Illustration of MARINE framework, which introduces a vision toolbox with one or multiple guidance models to enrich the visual context of the original LVLM. The output logits are controlled to place more importance on the guided generation with the guidance strength \(\).

* Through extensive evaluations on various datasets, we demonstrate that MARINE consistently outperform the baselines in hallucination mitigation while maintaining overall performance across multiple tasks (image captioning, VQA).
* MARINE provides a favorable trade-off between latency and accuracy, with the lowest computational overhead compared to existing baselines. The minimal increase in latency comparing to the baselines, combined with the high accuracy of our results, positions MARINE as a practical and scalable solution for real-world applications without significant computational cost.

## 2 Related Work

Since the introduction of recent Large Vision-Language Models (LVLMs) (Liu et al., 2023; Zhu et al., 2023; Ye et al., 2023; Dai et al., 2023; Gao et al., 2023), the hallucination phenomenon in these models has gathered significant attention in the research community. This issue was first highlighted by Li et al. (2023b) with subsequent studies (Wang et al., 2023b; Zhou et al., 2023; Fu et al., 2023; Lovenia et al., 2023) that, LVLMs exhibit similar hallucination problems as the textual LLMs. Notably, different from textual LLMs, LVLMs are prone to a unique type of hallucination called 'object hallucination' (Rohrbach et al., 2018), where the model falsely perceives the presence of non-existent objects in images. In response to object hallucination problems, efforts have been made to mitigate object hallucination in smaller image captioning models (Biten et al., 2022; Dai et al., 2023b). Regarding the recent development of LVLMs, several works (Liu et al., 2023b; Gunjal et al., 2023) proposed vision-language fine-tuning datasets aimed for improved robustness. Wang et al. (2023a) leveraged the vision-language model to generate more diverse instruction-tuning data and iteratively correct the inaccuracies in data. Zhai et al. (2023) introduced a GPT-4 assisted evaluation method and also a fine-tuning strategy using the MSCOCO dataset. Most related to our setting, Yin et al. (2023) proposed Woodepecker, a five-stage training-free method eventually leveraging GPT-3.5 API for hallucination correction. Concurrently, Leng et al. (2023) introduced Visual Contrastive Decoding (VCD), a technique that applies noise to image inputs and penalizes logit outputs of these corrupted images. Huang et al. (2023a) enhanced beam-search decoding with the Over-trust Penalty and Retrospection-Allocation Strategy (OPERA), which penalizes over-trust and refines token selection based on previous outputs. HALC (Chen et al., 2024) employs adaptive focal-contrast decoding to encourage LVLMs to focus on fine-grained visual information, while using a computationally intensive beam search algorithm. In contrast to these approaches, MARINE incorporates additional visual guidance in the generation process, offering an efficient and effective approach for hallucination mitigation in LVLMs.

## 3 Preliminaries

**Notation.** We use lower case letters, lower case bold face letters, and upper case bold face letters to denote scalars, vectors, and matrices respectively. We use the symbol \(p\) to represent the conditional probability of LLM's response. And we denote the sequence of tokens generated before the \(t\)-th token as \(_{<t}=[y_{1},,y_{t-1}]\) for \(t>1\). \(_{<t}\) is an empty sequence when \(t=1\).

**Generative language models.** Let \(p_{}\) denotes an LLM parameterized by \(\). Consider a sequence \(=[x_{1},,x_{n}]\) as the input prompt, where each \(x_{i}\) is a token from a predefined vocabulary. The LLM then generates the response sequence \(=[y_{1},,y_{m}]\) by sampling from the conditional probability distribution \(p_{}(|)\), where \(y_{t}\) denotes individual token for \(1 t m\). The conditional distribution \(p_{}(|)\) can therefore be expressed as \(p_{}(|)=_{t=1}^{m}p_{}(y_{t}| ,_{<t})\), where \(_{<t}=[y_{1},,y_{t-1}]\) for \(t>1\) and is empty for \(t=1\). In the case of LVLMs, visual tokens \(=[v_{1},,v_{k}]\) are additionally included. These tokens are generated from a pre-trained visual encoder and mapped into the token space through a linear projection. The conditional distribution of output \(\) given the visual tokens \(\) and textual prompt \(\) is expressed as \(p_{}(|,)=_{t=1}^{m}p_{}(y_{t}|,,_{<t}),\) where \(p_{}\) is approximated by LVLMs.

## 4 Method

The existing architecture of LVLMs is usually composed of a visual encoder, a visual and textual domain alignment layer, and the LLM itself. Therefore, besides the inherent language priors of LLMs (Biten et al., 2022), object hallucination may arise from (1) deficiencies in the visual encoder providing insufficient visual information (Zhang et al., 2023b) and (2) misalignment between the visual and textual domains. To mitigate object hallucinations, we introduce MARINE, a framework containing two major components to address the aforementioned challenges: (1) introducing additional visual information from a set of vision models and (2) using the additional aggregated visual features to guide the LVLM's generation. In Figure 1, we present the framework overview.

### Visual Guidance from Image-Grounded Features

To introduce image-grounded guidance to mitigate hallucinations, our approach integrates additional object detection models, which differ from the visual encoders used in LVLM that are usually pre-trained from CLIP (Radford et al., 2021). This integration leverages the object detection models to extract detailed visual information from images. Upon acquiring these extra visual information from different image-grounded models, we aggregate and translate the collected information into textual information. This aggregation can be done by the language model (Lin et al., 2023) or rule based algorithm (Bird et al., 2009). Such an information aggregation is effective and efficient, as it eliminates the necessity of fine-tuning the alignment layer while retaining the rich information encoded by various of image grounding models. We subsequently employ a simple prompt "focusing on the visible objects in this image:" and concatenate it with the aggregated object information, denoted as the guidance prompt \(\).

### Guided Text Generation with Visual Information

We tackle the object hallucination problem of LVLMs by specifically placing importance on the addtional image-grounded information we introduced. In addition to the visual tokens \(\) extracted from the original LVLM and textual prompt \(\), we extract the auxiliary visual tokens \(\) from the additional guidance models. The generation of the \(t\)-th token in the output \(\) of our classifier-free guided LVLM \(p_{}\) is expressed as

\[_{}(y_{t}|,,, _{<t}) p_{}(y_{t}|,, ,_{<t})^{}/p_{}(y_{t}|,,_{<t})^{-1},\]

where \(\) denotes our control guidance and \(\) is the control strength. The sampling of output is:

\[_{}(|,, )=_{t=1}^{m}_{}(y_{t}|,,,_{<t})_{t=1}^{m}}(y_{t}|,,,_{<t})^{-1}}{ p_{}(y_{t}|,,_{<t})^{-1}}= }(|,,)^ {}}{p_{}(|,)^{-1}}.\]

We can further view MARINE in the logit space, where the \(t\)-th token is therefore sampled by

\[_{}(y_{t}|,,, _{<t})= p_{}(|, ,,_{<t})+(1-) p_{} (|,,_{<t}).\]

This linear combination of logits implies that the conditional generation on the additional image-grounded guidance acts as a controllable gate. Only objects with relatively high probabilities in both branches could appear at top when sampling. Specifically, setting \(=0\) recovers the original LLM generation without control guidance and setting \(=1\) produces the LLM generation entirely based on the control. Meanwhile, for \((0,1)\), MARINE yields a combination of the original generation \(p_{}(|,)\) and the generation conditioned on the guidance \(p_{}(|,,)\). This strikes a balance between a better ability to follow instructions to generate high-quality answers and the increased accuracy and detail in image descriptions. The formulation therefore shares resemblance to the classifier-free guidance introduced for LLMs (Sanchez et al., 2023), which places importance on the textual prompt itself to better align the LLM generation with user intention in the _single-modal_ setting. We summarize MARINE in Algorithm 1. In detail, MARINE aggregates the collected visual information \(\{_{i}\}_{i}\) using function Aggr., which can be a small language model for information aggregation (Lin et al., 2023), or a rule-based algorithms like majority voting (as similarly used by Wang et al.). Notably, MARINE only double the LLM inference time of in Line 7 and Line 9, while adding the guidance from each single image grounded model will increase the inference time when the number of image grounded models increase.

## 5 Experiments

In this section, we evaluate MARINE in mitigating object hallucinations across various LVLMs, showing that it outperforms state-of-the-art methods on established metrics across different datasets.

### Experiment Setup

**Models.** To demonstrate the broad applicability of our approach across different LVLM architectures, we apply and evaluate MARINE to recent widely-used models including _LLaVA_(Liu et al., 2023), _LLaVA-v1.5_(Liu et al., 2023), _MiniGPT-v2_(Chen et al., 2023), _mPLUG-Owl2_(Ye et al., 2023) and _InstructBLIP_(Liu et al., 2023). To address the object hallucination problems in text generation, we incorporate the DEIR Transformer (DETR) (Carion et al., 2020) and RAM++ (Huang et al., 2023) as the additional vision models for guidance.

**Guidance from Multiple Sources.** Our framework's compatibility with various vision models allows for the incorporation of multiple sources to enhance precision and robustness. By considering object-level information from DETR and RAM++ simultaneously, we generate guidance that reflectsconsensus across these models. This approach significantly improves the accuracy and reliability of the guidance provided to the LVLM.

**Datasets and evaluations.** In alignment with established evaluations from previous studies (Dai et al., 2023; Yin et al., 2023), we assess our method using the following metrics:

* Caption Hallucination Assessment with Image Relevance (_CHAIR_) (Rohrbach et al., 2018). It involves prompting the LVLMs to generate a description for the input image, and then comparing this generation with ground truth objects present in the image. CHAIR quantifies hallucination both at instance level and sentence level, respectively defined as CHAIR\({}_{I}\) and CHAIR\({}_{S}\): \[_{I}=\{\}}{ \{\}},_{S}=\{\}}{\{\}}.\] In addition to these metrics, we incorporate an instance-level Recall score in our evaluation to evaluate whether the descriptions accurately include the necessary visual content from the image: \[=\{\} \{\}.\]
* Polling-based Object Probing Evaluation (_POPE_) (Li et al., 2023). POPE formulates a binary classification task by prompting LVLMs with questions such as "Is there a keyboard in this image?" to answer "yes" or "no". We specifically focus on the adversarial setting, which is considered the most challenging setting. Results for the random and popular settings are detailed in Appendix E. We report the accuracy and FI score of the LVLMs' responses, and the proportion of "yes" answers.
* _GPT-4V-aided Evaluation_(Yin et al., 2023). The GPT-4V-aided evaluation compares the outputs of two LVLM assistants using GPT-4V as a judge. In this evaluation, we utilize the LLaVA-QA90 task (Liu et al., 2023)* (including conversations, visual perceptions, and complex reasoning tasks) and additionally consider the image captioning task.

Footnote *: https://github.com/haotian-liu/LLAVA/blob/main/playground/data/cooco2014_val_gpt4_qa_30x3.json

Consistent with Li et al. (2023), we randomly sampled a subset of 500 images from MSCOCO (Lin et al., 2014) dataset for CHAIR evaluation. For the POPE evaluation, we created 3000 questions across three datasets--500 images each from MSCOCO, A-OKVQA (Schwenk et al., 2022), and GQA (Hudson and Manning, 2019). For the GPT-4V-aided evaluation, we utilized 90 questions from the LLaVA-QA90 task and randomly selected 50 MSCOCO images for image captioning task. We defer the detailed description of our baselines to Appendix D.

### Results

Experimental results on object hallucination metrics (CHAIR and POPE) are presented in Table 1 and Table 2. Overall, MARINE achieves superior performances across different LVLM architectures and evaluation metrics, ranked as the best or second-best on the majority of the evaluation metrics.

**Results on CHAIR.** Table 1 presents the evaluation of various mitigation methods using CHAIR scores across multiple LVLM architectures. The results demonstrate that MARINE consistently outperforms other state-of-the-art methods, achieving the highest average scores in both CHAIR\({}_{S}\) and CHAIR\({}_{I}\) and the second-best Recall score. Specifically, MARINE surpasses the second-best performing method by an average margin of \(1.7\) on CHAIR\({}_{S}\) and \(1.1\) on CHAIR\({}_{I}\). Notably, MARINE exhibits exceptional performance on LLaVA architectures, with improvements in CHAIR scores of up to \(8.8\) compared to its original performance. In contrast, methods such as LURE and Woodpecker show less effectiveness in hallucination mitigation. The reference method, MARINE-Truth, generally achieves the strongest results, as expected given its access to ground-truth guidance. However, MARINE's performance closely approximates that of its ground-truth counterpart, indicating successful leveraging of multiple guidance models to provide reliable control in LVLM generation.

**Results on POPE.** The POPE evaluation, presented in Table 2, further validates the superior performance of MARINE against existing baselines across various question formats. MARINE consistently outperforms all other methods by a substantial margin, demonstrating average improvements of \(6.7\%\) in accuracy and \(3.5\%\) in F1 score relative to the original outputs across models. Even when compared to the second-best method, Woodpecker, MARINE maintains a performance edge of \(1.1\%\) and \(2.1\%\) respectively in accuracy and F1 score. Moreover, MARINE effectively mitigates the LVLMs' biased tendency towards affirmative responses, as evidenced by a more balanced "yes" ratio (closer to \(50\%\), representing a \(15.9\%\) shift towards unbiased answers). This improvement notably addresses the overconfidence issue prevalent in existing models.

**Results on GPT-4V-aided evaluation.** Following Yin et al. (2023), we leverage GPT-4V* to evaluate and compare the performance of the original LVLMs and LVLMs with MARINE on LLaVA-QA90 and an image captioning task. This GPT-4V-assisted evaluation introduces a qualitative perspective beyond the numerical metrics of CHAIR and POPE, offering a richer assessment of model performance. The evaluation prompt is detailed in Appendix D.6. As shown in Table 3, GPT-4V consistently assigns higher accuracy with equal detailedness scores to models enhanced by MARINE, highlighting its ability to produce more precise and detailed descriptions, which demonstrates the robustness of our method in real-world visual tasks.

**Additional Results on Other Vision-Language Tasks.** To further evaluate the generalizability of our approach beyond object hallucination and the MSCOCO dataset, we extended our evaluations to additional datasets including A-OKVQA and GQA and included more general caption quality metrics. As shown in Table 4, the POPE results on datasets such as MSCOCO, A-OKVQA, and

   Method &  &  &  &  &  &  \\ 
**CFAIRIR** & \(C_{S}\) & \(C_{I}\) & \(C_{S}\) & \(C_{I}\) & \(C_{S}\) & \(C_{I}\) & \(C_{S}\) & \(C_{I}\) & \(C_{S}\) & \(C_{S}\) & \(C_{S}\) & \(C_{S}\) & \(R\) & \(C_{S}\) & \(C_{S}\) & \(R\) & \(C_{S}\) & \(C_{S}\) & \(C_{S}\) & \(R\) \\  Greedy & 26.6 & 10.5 & 47.4 & 8.8 & 4.6 & 4.1 & 8.2 & 4.1 & 4.1 & 6.9 & 3.4 & 3.8 & 5.0 & 3.2 & 33.2 & 11.0 & 5.2 & 40.3 \\ LURE & 33.8 & 11.6 & **54.8** & 38.9 & 11.2 & **56.3** & 36.2 & 11.4 & **54.6** & 33.9 & 10.8 & **55.9** & 38.1 & 12.1 & **54.5** & 36.2 & 11.4 & **58.2** \\ LURE w/ cutoff & 24.4 & 9.3 & 50.2 & 18.4 & 6.8 & 4.7 & 12.5 & 6.2 & 42.0 & 15.4 & 6.6 & 45.5 & 9.6 & 6.4 & 34.5 & 16.1 & 7.1 & 4.39 \\ Woodpecker & 19.5 & 8.9 & 44.3 & 8.5 & 4.5 & 38.4 & 7.5 & 43.7 & 5.5 & 37.0 & 8.0 & 4.3 & 37.5 & 8.0 & 6.2 & 32.6 & 10.3 & 5.7 & 38.0 \\ VCD & 28.1 & 10.6 & 46.7 & 3.1 & 40.8 & **6.8** & **3.9** & 38.2 & 8.9 & 3.4 & 37.7 & 2.4 & 1.5 & 33.7 & 10.1 & 4.8 & 39.4 \\ OPERA & 22.4 & 9.9 & 43.6 & 11.0 & 6.7 & 40.2 & 9.2 & 5.0 & 41.3 & 5.8 & 3.2 & 38.4 & 4.6 & 2.7 & 38.0 & 10.6 & 5.5 & 40.3 \\ 
**MARINE** & **17.8** & **7.2** & 50.8 & **6.2** & **3.0** & 44.3 & 11.8 & **4.9** & **49.7** & **42.2** & **3.3** & 41.4 & **2.2** & **1.3** & **36.3** & **8.4** & **37.4** & **44.5** \\  MARINE-Truth & 19.6 & 5.1 & 12.6 & 6.0 & 2.5 & 55.3 & 12.6 & 3.8 & 2.7 & 3.8 & 1.7 & 48.0 & 3.0 & 1.8 & 35.9 & 9.0 & 3.0 & 30.9 \\   

Table 1: Evaluation with CHAIR score across multiple LVLM architectures comparing our method with several baselines. We report CHAIR\({}_{S}\), CHAIR\({}_{I}\) and the recall score. The **bold** numbers indicate the best results among the methods evaluated and the underscored numbers represent the second-best results. We show MARINE-Truth as a reference performance of MARINE.

   Method &  &  &  &  &  &  \\ 
**POPE** & Acc \(\) & F1 & Yes & Acc & F1 & F1 & Yes & Acc & F1 & Yes & Acc & F1 & F1 & Yes & Acc & F1 & F1 & Yes \\  Greedy & 51.8 & 67.4 & 97.7 & 79.4 & 81.6 & 61.6 & 42.7 & 81.7 & 44.2 & 72.5 & 77.5 & 72.4 & 22.8 & **81.4** & 58.6 & 73.2 & 77.9 & 61.0 \\ LURE & - & - & - & - & - & - & - & - & - & - & - & - & - & - & - & - \\ Woodpecker & **77.5** & **77.6** & **50.5** & 80.5 & **50.6** & **59.5** & 79.5 & 77.8 & 42.5 & 77.5 & 76.9 & 42.5 & 79.0 & 78.6 & **45.0** & 78.8 & 78.3 & 47.8 \\ VCD & 54.6 & 68.5 & 94.0 & 78.2 & 80.7 & 82.5 & 81.4 & 80.2 & 44.1 & 72.3 & 70.0 & 75.9 & 77.0 & 80.5 & 56.7 & 73.2 & 77.5 & 65.6 \\ OPERA & 51.7 & 67.4 & 98.0 & 77.5 & 80.1 & 63.2 & 82.9 & 81.9 & 43.3 & 70.0 & 79.1 & 84.6 & 79.8 & 54.1 & 58.6GQA demonstrate that our method consistently mitigates hallucinations across various datasets with different image distributions. Figure 2 presents a comprehensive evaluation of the image captioning task on MSCOCOCO and LLaVA-QA90, a comprehensive VQA dataset, using metrics including BLEU(Papineni et al., 2002), ROUGE(Lin, 2004), CIDEr(Vedantam et al., 2015) and SPICE(Anderson et al., 2016). These results demonstrate that, although our method primarily targets hallucination mitigation, it maintains the overall performance of LVLMs on broader tasks, with no significant trade-offs in caption or VQA quality.

**Latency Analysis** Mitigating object hallucination often requires additional computational resources, a characteristic common to many existing methods which typically involve additional post-generation correction models (Zhou et al., 2023; Zhai et al., 2023; Yin et al., 2023), object detectors (Yin et al., 2023), or more complex decoding processes (Huang et al., 2023; Leng et al., 2023) to reduce hallucinations. Furthermore, to assess the practical feasibility of our approach in terms of computational costs, we have compared our method with existing baselines on LLaVA-7B. As demonstrated in Table 5, our method increases the decoding time by factors of 1.98, which is the lowest costs among existing baselines, suggesting MARINE can be widely applied with negligible cost. Our method offers the most favorable trade-offs between latency and accuracy in hallucination mitigation. Detailed experiment setting is in Appendix D.7.

### Ablation Study

**How Does Incorporating Multiple Sources to Form Guidance Impact Performance?** We perform an ablation study to assess the impact of incorporating DETR and RAM++ compared to using each model individually, as presented in Table 6. Notably, DETR allows for highly accurate object detection, while RAM++ excels in extensive recognition tasks, adding fine-grained details to image

    &  &  &  \\   & & Accuracy \(\) & F1\(\) & Yes(\%) & Accuracy \(\) & F1\(\) & Yes(\%) \\   & ✗ & 54.2 & 68.5 & 95.5 & 76.7 & 80.4 & 68.2 \\  & ✓ & **72.2** & **76.4** & **66.9** & **85.5** & **85.0** & **46.5** \\  & ✗ & 51.8 & 67.5 & 97.9 & 69.6 & 76.5 & 78.5 \\  & ✓ & **64.3** & **72.8** & **80.2** & **82.0** & **83.5** & **57.2** \\  & ✗ & 52.0 & 67.6 & 97.8 & 73.7 & 78.7 & 72.6 \\  & ✓ & **62.5** & **71.8** & **81.8** & **80.1** & **80.6** & **51.1** \\   

Table 4: POPE results across three datasets. We report the average score under random, popular, adversarial settings. The detailed POPE results can be found in the appendix E. The **bold** numbers indicate the best results. The ideal yes ratio for a non-biased LVLM is \(50\%\).

    &  &  &  \\   & & ✗ & ✓ & ✗ & ✓ \\   & Acc\(\) & 5.82\({}_{ 0.10}\) & **5.94\({}_{ 0.05}\)** & 6.03\({}_{ 0.13}\) & **6.35\({}_{ 0.21}\)** \\  & Detail \(\) & 4.59\({}_{ 0.08}\) & 4.59\({}_{ 0.08}\) & 5.06\({}_{ 0.05}\) & **5.16\({}_{ 0.10}\)** \\  & Acc\(\) & 5.27\({}_{ 0.20}\) & **6.11\({}_{ 0.23}\)** & 7.97\({}_{ 0.25}\) & **8.63\({}_{ 0.20}\)** \\  & Detail \(\) & **4.39\({}_{ 0.29}\)** & 4.36\({}_{ 0.17}\) & 5.74\({}_{ 0.22}\) & **6.19\({}_{ 0.23}\)** \\   

Table 3: Results of GPT-4V-aided evaluation. The accuracy and detailedness metrics are on a scale of 10, and a higher score indicates better performance. The symbols \(\) and ✓ indicate performance metrics without and with our method, respectively.

Figure 2: MARINE leads to consistent enhancement in the text qualities on general metrics. Dashed lines and solid lines represent without or with MARINE. Higher scores indicate better quality and greater similarity between the generated captions and the reference texts.

understanding. Combining the strengths of these image-grounding models, we achieve significant performance improvements on the CHAIR metrics. This demonstrates that leveraging complementary visual contexts can substantially enhance overall model effectiveness.

**Which Method of Integrating Image-Grounding Models Works Best?** We investigate two approaches for integrating image-grounding models: using either the intersection or union of detected objects. As shown in Table 7, the intersection-based method outperforms the union, significantly reducing object hallucination. This result highlights the importance of precision and consistency in guidance, as taking intersection ensures consensus across models, leading to more reliable guidance. The detailed experimental setup and prompt templates are provided in Appendix D.

**Effect of Guidance Strength.** Figure 3 shows that increasing guidance strength from \(0\) to \(1\) leads to a notable decrease in CHAIR scores. This trend suggests that higher guidance strength makes LVLMs rely more on image-grounded features provided by the guidance models, thereby enhancing their ability to produce accurate descriptions. It's crucial to note that, although some models exhibit optimal performance at a guidance strength of \(=1\), excessively strong guidance can adversely affect the models' ability to adhere to provided instructions. Experimental evidence is detailed in Appendix E.Based on our findings, we recommend a guidance strength within the range of \((0.3,0.7)\) as the most effective for maintaining this balance.

## 6 Conclusion

In this paper, we introduced a training-free and API-free framework MARINE to mitigate object hallucination in LVLMs during its text generation process. Leveraging a pre-trained object grounding vision encoder for a novel guidance framework in the multi-modal setting, MARINE effectively

  
**Model** & **LLAVA** & **LLAVA-v1.5** & **mPLUG-Owl2** \\ 
**CHIAIR** & \(C_{S}\) & \(C_{I}\) & \(C_{S}\) & \(C_{I}\) & \(C_{S}\) & \(C_{I}\) \\   \\ MARINE & **17.8** & **7.2** & **6.2** & **3.0** & **4.2** & **2.3** \\  \\ MARINE-DETR only & 27.6 & 8.4 & 10.5 & 4.3 & 5.3 & 2.7 \\ MARINE-RAM only & 29.0 & 9.1 & 6.6 & 3.7 & 5.2 & 2.8 \\   

Table 7: Effect of Integration Methods for Image-Grounding Models.

    & Greedy & LURE & Woodpecker\({}^{}\) & VCD & OPERA & **MARINE (ours)** \\   Training Cost & 0 & 10min on A100 80G & 0 & 0 & 0 & 0 \\ \({}^{}\)Inference Latency(mNRINE) & 26.3 (vol.0) & 179.9 (\%.84) & 94.5 (\(\)3.99)* & 53.4 (\(\)2.03) & 185.1 (\(0\)) & **52.2** (\(\)) \\   ^{}\)Woodpecker requires GPT API key access and the latency may depend on OPENAT API.} \\ 

Table 5: Inference Latency Comparison. We report both the latency and the ratio to the latency of greedy decoding of the original LVLM model.

Figure 3: Ablation study on the effect of guidance strength (\(\)) on the performance of LLAVA and mPLUG-Owl2 using CHAIR metrics, with \(\) ranging from 0 to 1.

and cost-efficiently reduces the hallucinations of five widely-used LVLMs, as assessed by various metrics across different tasks. The inherent compatibility of the MARINE with various vision models and projection functions further underscores its flexibility. In contrast to post-generation correction methods, MARINE strikes a balance between efficiency, instruction-following ability and effectiveness in reducing object hallucinations.