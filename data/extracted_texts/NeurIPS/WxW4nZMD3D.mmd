# Network Lasso Bandits

Anonymous Author(s)

Affiliation

Address

email

###### Abstract

We consider a multi-task contextual bandit setting, where the learner is given a graph encoding relations between the bandit tasks. The tasks' preference vectors are assumed to be piecewise constant over the graph, forming clusters. At every round, we estimate the preference vectors by solving an online network lasso problem with a suitably chosen, time-dependent regularization parameter. We establish a novel oracle inequality relying on a convenient restricted eigenvalue assumption. Our theoretical findings highlight the importance of dense intra-cluster connections and sparse inter-cluster ones. That results in a sublinear regret bound significantly lower than its counterpart in the independent task learning setting. Finally, we support our theoretical findings by experimental evaluation against graph bandit multi-task learning and online clustering of bandits algorithms.

## 1 Introduction

Online commercial websites aim to properly recommend their products to their customers, and the performance of these recommendations depends on the knowledge of users' preferences. Unlike traditional collaborative-filtering-based methods (Su and Khoshgoftaar, 2009), such knowledge is initially unavailable. Therefore, the online recommender systems need to recommend various items to the users and observe their ratings to _explore_ their preferences. At the same time, the recommender system should be able to recommend items that attract users' attention and receive high ratings by _exploiting_ the learned knowledge. The contextual bandits frameworks (Li et al., 2010) have been popularly used to formalize and address this exploration-exploitation trade-off.

However, the classical form of contextual bandits (Li et al., 2010; Chu et al., 2011; Abbasi-Yadkori et al., 2011) ignores the availability of social networks amongst users and solves the problem for each user separately. Consequently, such algorithms have some drawbacks when applied to problems with a large number of users. First, such a large number hinders the computational efficiency of such algorithms. Second, the partial feedback of the bandit settings exposes the algorithms to have weak estimations and impair their decision-making ability (Yang et al., 2020). Consequently, to improve bandit algorithms' performance for large-scale applications, structural assumptions that link the different users are usually integrated within bandit algorithms (Cesa-Bianchi et al., 2013; Gentile et al., 2014; Li et al., 2019; Herbster et al., 2021).

The papers of Cesa-Bianchi et al. (2013); Yang et al. (2020) attempt to integrate the prior knowledge of social networks into their contextual bandit algorithms. Both papers proposed UCB-style algorithms and exhibited the importance of using the social network graph to achieve lower regrets using Laplacian regularization. Consequently, both methods promote smoothness among the preference vectors of users in order to transfer the collected information between them. However, the Laplacian regularization does not account for the smoothness heterogeneity introduced by a piecewise constant behavior over the graph (Wang et al., 2016). On the other hand, algorithms of online clustering of bandits (Gentile et al., 2014; Li et al., 2019) start from a graph and gradually add or remove edges toform clusters as connected components. However, their clustering can cause overconfidence in the constructed clusters, potentially leading to error accumulation.

In this paper, we assume access to a graph encoding relations between bandit tasks, and that the task parameter vectors are piecewise constant over the graph. That means that tasks form clusters. We propose an algorithm that integrates the prior knowledge of the piecewise constant structure to update tasks rather than finding the clusters explicitly. That way, we mitigate the limitations mentioned above: the piecewise constant smoothness is naturally integrated into our regularizer, and we do not estimate the clusters so our algorithm does not suffer from overconfidence drawbacks.

More precisely, we provide the following contributions

* We analyze an instance of the Network Lasso problem (Hallac et al., 2015), where every vertex's preference vector is estimated using data generated during the interaction between users and the bandit. We provide the first oracle inequality in this setting and link it to fundamental quantities characterizing the relation between the graph and the true preference vectors of the users. Our result relies on our novel restricted eigenvalue (RE) condition, which we assume for our setting. This result is of independent interest and can be applied to independently generated data as a special case.
* We prove how the empirical multi-task Gram matrix of the data inherits the RE condition from its true counterpart. Both this result and the previous one depend on the sparsity of inter-cluster connections and the density of intra-cluster ones.
* We provide a regret upper bound for our setting. Our bound highlights the advantage of our algorithm in high dimensional settings, and for large graphs.
* We support our theoretical findings by extensive numerical experiments on simulated data that prove the advantage of our algorithm compared to other approaches used for online clustering of bandits.

The rest of the paper is organized as follows. Section 2 discusses the relation of our work to the literature. We formulate our problem and state some of our assumptions in Section 3, then we present our bandit algorithm in Section 4. We analyze the problem theoretically in Section 5, and finally, we demonstrate its practical interest via numerical experiments in Section 6.

## 2 Related work

Lasso contextual banditsTo address the high dimensional setting for linear bandits, several multi-armed bandit papers solve a LASSO (Tibshirani, 1996) problem under different assumptions (Bastani and Bayati, 2019; Kim and Paik, 2019; Oh et al., 2021; Ariu et al., 2022). They all rely on a previously established compatibility or RE condition (Buhlmann and van de Geer, 2011), that they adapt to the non-i.i.d case. Such assumptions were also used in the multi-task setting by Cella and Pontil (2021) with a Group Lasso regularization (Yuan and Lin, 2006), and to impose a low rank structure on the task preference vectors in Cella et al. (2023). In our case, we provide a novel oracle inequality, rather than just generalize an existing one to the non-i.i.d setting, with a newly introduced RE assumption.

Clustering of banditsSequentially clustering bandit tasks was introduced in Gentile et al. (2014) with CLUB algorithm. In CLUB, starting with a fully connected graph, an iterative graph learning process is performed, where edges between users are deleted if their preference vectors are significantly different. As a result, any connected component is seen as a cluster and only one recommendation per cluster is developed. In another work, Li et al. (2019) generalize the setting of Gentile et al. (2014) and address its limitations via including merging operations in addition to splitting. In contrast to these approaches, the algorithm in Nguyen and Lauw (2014) groups users via K-means clustering, and the algorithm in Cheng et al. (2023) relies on hedonic games for online clustering of bandits. Furthermore, Yang and Toni (2018) make use of community detection techniques on graphs to find user clusters. Gentile et al. (2017) study the clustering of the contextual bandit problem where their proposed algorithm, named CAB, adaptively matches user preferences in the face of constantly evolving items. Our work fundamentally differs from the previous ones on two aspects. First, we assume access to a graph encoding relations between users, which is more informative than a complete graph. Second, we do not keep track of a model for each cluster, but rather we integrate a prior over the graph via a graph total variation regularizer that enforces a piecewise constant behaviour for the estimated preference vectors.

Multi-task learningSeveral contributions assume some underlying structure that links the bandit tasks. In Cella and Pontil (2021), task preference vectors are assumed to be sparse and to share their sparsity support, implying that they lie in a low-dimensional subspace with dimensions aligning with the canonical basis vectors. This idea is further generalized in Cella et al. (2023), where the tasks are assumed to be confined to an arbitrary unknown low-dimensional subspace. That work improves upon Hu et al. (2021) by not requiring the knowledge of the small dimenson of the task space. The underlying structure linking tasks can also be a graph encoding relations between them (Cesa-Bianchi et al., 2013; Yang and Toni, 2018), which is our case. However, while they assume smoothness as a prior, we assume piecewise constant behavior.

## 3 Problem setting

We consider a linear bandit setting, with a finite number of tasks representing users in a recommendation system for example. For each task the agent has to choose among \(K\) arms, each associated to a \(d\)-dimensional context vector. All interactions over a horizon of \(T\) time steps. We further assume that we have access to an undirected graph \(=(,)\), with vertex set \(\) representing the tasks and edge set \(\) encoding the relationships between them. We identify the vertex set \(\) with the set of vertex indices \([||]\). Thus, we consider \(\) to be a subset of \(^{2}\), where every edge \((m,n)\) has weight \(w_{mn}>0\), with \(m<n\). The tasks' preference vectors are denoted by \(\{_{m}\}_{m}^{d}\) verifying \(\|_{m}\| 1\  m\), which we concatenate as row vectors into matrix \(^{|| d}\). The latter represents a graph vector signal, assumed to be piecewise constant over \(\).

At a round \(t^{}\), a user \(m(t)\) is selected uniformly at random and served an arm with context vector \((t)\) from a finite action set \((t)^{d}\) with size \(K\), depending on their estimated preference vector \(}_{m(t)}(t)^{d}\). We assume the expected reward to be linear, with an additive, \(\)-sub-Gaussian noise conditionally on the past. Formally, denoting by \(_{0}\) the trivial sigma-algebra, and for all \(t 1\), by \(_{t}\) the sigma-algebra generated by history set \(\{m(1),(1),y(1),,m(t),(t),y(t),m(t+1)\}\), the received reward \(y(t)\) is given by \(y(t)=_{m(t)}(t),(t)+ (t)\), where \((t)\) is \(_{t}-\)measurable and

\[[(t)|_{t-1}]=0,[ (s(t))|_{t-1}]\!(^{2}s^{2} ) t 1, s.\] (1)

At the end of a round \(t\), all preference vectors are updated into a new estimation \(}(t)\) while leveraging the structure of graph \(\), formally by solving the following optimization problem:

\[}(t)=*{arg\,min}_{ ^{|| d}}_{=1}^{t}( }_{m()},() -y())^{2}+(t)_{(m,n)}w_{mn}}_{m}-}_{n},\] (2)

where \(\|\|\) denotes the Euclidean norm for vectors. The performance of our policy is assessed by the expected regret over the \(T\) interaction rounds for all tasks:

\[(T)=[_{t=1}^{T} _{m(t)},^{}(t)-(t)],\] (3)

where \(^{}(t)*{arg\,max}_{} (t)}_{m(t)},}\).

The Optimization problem in (2) is an instance of the Network Lasso (Hallac et al., 2015). Other instances of the same type were studied by Jung et al. (2018); Jung and Vesselinova (2019); Jung (2020). The objective is characterized by its second term that, while being just the Laplacian regularization without squaring the norms, promotes a piecewise constant behavior rather than smoothness. For real-valued signals (\(d=1\)), this regularization has been extensively studied for image and graph signal denoising, for the problem of trend filtering on graphs (Wang et al., 2016). According to Wang et al. (2016), that regularization better adapts to the heterogeneity of smoothness of the signal and induces a cluster structure in the data: similar users will not only have similar models but the same model, which offers a compression of the overall model over the graph. Notethat our setting is cluster agnostic; our algorithm does not aim to learn the cluster structure explicitly but to exploit it implicitly using the total variation semi-norm as regularization. The latter's strength is controlled via a time-dependent regularization coefficient \((t)\), which we will express later in the analysis.

We formalize our assumption on the context generation as follows.

**Assumption 1** (i.i.d action sets).: _Context sets \(\{(t)\}_{t=1}^{T}\) are generated i.i.d. from a distribution \(p\) over \(^{K d}\), such that \(\|\| 1\ (t)\  t 1\)._

In addition to the i.i.d assumption, we assume more regularity.

**Assumption 2** (Relaxed symmetry and balanced covariance).: _There exists a constant \( 1\) such that for all \(^{K d}\), \(p(-) p()\). Furthermore, there exists \(>0\), such that for any permutation \((a_{1},,a_{K})\) of \([K]\), for any \(i\{2,,K-1\}\), and for any \(^{d}\), we have_

\[[_{a_{1}}_{a_{1}}^{}[^{ }_{a_{1}}<<^{}_{a_{K}}]] [(_{a_{1}}_{a_{1}}^{}+ _{a_{K}}_{a_{K}}^{})[^{}_{a_{ 1}}<<^{}_{a_{K}}]],\]

_where \(\) means that \(-\) is a PSD matrix._

This assumption was introduced in Oh et al. (2021), and has already been used in a multi-task setting by Cella et al. (2023). Parameter \(\) controls the skewness, as \(=1\) corresponds to a symmetric distribution. \(\) decreases with increasing positive correlation between arms. It verifies \(=O(1)\) for multi-variate Gaussians and uniform distributions over the unit sphere (Oh et al., 2021). The piecewise constant behaviour of the graph signal \(\) is formalized in the next assumption.

**Assumption 3** (Piecewise constant signal).: _There exists a partition \(\) of \(\), such that for any cluster \(\), signal \(\) is constant on \(\), and the graph obtained by taking the vertices in \(\) and the edges linking them is connected._

Assumption 3 basically states that the true preference vectors are clustered and that the given graph induces the cluster structure. It is required for our approach to be beneficial, as we will detail in the analysis section. For the sake of clarity, we defer the statement of other technical assumptions to Section 5.

## 4 Algorithm

Our policy in Algorithm 1 follows a greedy arm selection rule in a multi-task setting, in the same vein as those presented in Oh et al. (2021); Cella et al. (2023). Indeed, as pointed out in Oh et al. (2021), exploration is implicitly incorporated into regularization parameter \((t)\)'s time dependence. It has the following expression

\[(t)}{t}}|_{m}(t)|^{2}}}+2_{m }|_{m}(t)|,\] (4)

where the set of time steps a task \(m\) has been selected up to time \(t\) is denoted by \(_{m}(t)\).

## 5 Analysis

This section provides the main steps of the analysis. One of the paper's contribution lies in finding an oracle inequality of the network lasso problem given a restricted eigenvalue condition holding for the true multi-task Gram matrix. In this regard, the next major challenge and contribution is to show that the empirical multi-task Gram matrix, estimated in the algorithm, satisfies the restricted eigenvalue condition. We start by proving an oracle inequality for the estimation error of \(\), assuming that the condition given by Definition 2 is verified by the empirical data Gram matrix. Then, we prove that the latter assumption actually holds with high probability given that true multi-task Gram matrix satisfies it. Our final contribution in this work is the establishment of a regret bound for our algorithm.

### Notation and technical assumptions

We provide additional notations required for the analysis. We denote by \(\) the set of all edges in \(\) connecting vertices from different clusters from partition \(\) (Assumption 3), and we call it the boundary of \(\). Thus, \(^{c}\), the complementary set of \(\), is formed by edges connecting vertices of the same cluster. The total weight of the boundary, _i.e._the sum of its edges' weights, is referred to as \(w()\). Given a signal \(^{|| d}\), we denote by \(_{}\) the signal obtained by setting row vectors of \(\) to their mean-per-cluster value w.r.t. \(\). For any edge subset \(I\), we denote the following norms: \(\|\|_{F}\) as the Frobenius norm, \(\|\|_{}=^{} }\) as the weighted norm of vector \(^{d}\) induced by matrix \(^{d d}\) and \(\|\|_{I}_{(m,n) I}w_{mn}\|_{ m}-_{n}\|\) as the total variation semi-norm of \(^{|| d}\) over \(I\). Thus, the regularization term of Problem (2) is equal to \(\|\|_{}\). Also, we define the incidence matrix \(_{I}^{||||}\)restricted to \(I\) to be null except at rows with index \(i I\) corresponding to edge \((m,n)\), where it equals \(w_{mn}(_{m}-_{n})\), where \(_{m}\) is the \(m^{}\) canonical basis vector of \(^{||}\). We define \(_{}(t)(_{1}(t )^{}_{1}(t),,_{||}(t)^{}_{||}(t))^{d|| d||}\), and subsequently the empirical multi-task Gram matrix up to time step \(t\) is given by \(_{}(t)\). The following definition introduces quantities related to the clusters defined by partition \(\), with crucial roles that we will elucidate throughout the analysis.

**Definition 1** (Cluster content constants).: _Let \(\) be a cluster._

* _We denote by_ \(_{v}\) _the inner boundary of_ \(\)_, i.e._the vertices of_ \(\) _that are connected to its complementary. We define the inner isoperimetric ratio of_ \(\) _as_ \(_{}()|}{| |}\)_._
* _By abuse of notation, we denote as_ \(_{}\) _the incidence matrix restricted to edges linking vertices of_ \(\)_, its associated Laplacian matrix by_ \(_{}_{}^{}_{ }\)_, and its pseudo-inverse by_ \(_{}^{}\)_. The topological centrality index of node_ \(m\) _w.r.t_ \(\) _is equal to_ \((_{}^{})_{mn}^{-1}\)_. We define the topological centrality index of_ \(\) _by_ \(c_{}()_{m}(_{ }^{})_{mm}^{-1}\)_._

The inner isoperimetric ratio of a cluster measures how many "interior" nodes a cluster contains, in the sense that they are not connected to its complementary. It is at most equal to the isoperimetric ratio for weightless graphs as the size of the inner boundary is at most equal to that of the edge boundary, the latter being connected to the algebraic connectivity via the Cheeger inequality (Cheeger, 1970).

The topological centrality index measures the overall connectedness of a vertex in a network and indicates how robust a node is to edge failures (Ranjan and Zhang, 2013). Also, it can be tied to electricity spreading in a network according to Van Mieghem et al. (2017). We refer the interested reader to the two previously mentioned works for a detailed account of the properties of the topological centrality index. In the appendix, we show that for binary weights graphs the minimum topological centrality index is at least equal to the algebraic connectivity theoretically and experimentally, where we showcase that the difference between the two can be significant.

To proceed, we will need the following definition that introduces several notations to reduce the clutter.

**Definition 2** (Restricted Eigenvalue (RE) condition and norm).: _Let \(\{_{i}\}_{i=1}^{||}^{d d}\) be a set of positive semi-definite matrices. We say that the matrix \(_{}:=(_{1},,_{||})\) verifies the restricted eigenvalue condition with constants \( 0\) and \(>0\) if_

\[^{2}\|\|_{}^{2}_{i}\|_{i}\|_{_{i}}^{2}\{_{i}\}_{i},\]

_where \(\) is the cone defined by:_

\[\{^{|| d };a_{1}(,)\|\|_{^{ }} a_{2}(,)}_{}_{F}+(1-)^{+}\|\|_{}\},\] \[a_{1}(,) 1-}+2 w()}{_{ }}()}}, a_{2}(, )}+ w( )_{}}( )},\]

_and the RE semi-norm is defined by \(\|\|_{}\|}_{}\|_{F}(1-)^{+}_{}^{ }_{}\)._

To interpret the previous definition, we point out that the sum on the right-hand side of Definition 2 can be written as \(\|(^{})\|_{_{}}\), where \(\) denotes the operation of stacking a matrix's columns vertically. As a result, the condition is analogous to requiring that \(_{}\) is invertible with minimum eigenvalue \(^{2}\), but weaker since it holds only for signals \(\) and for the \(\|\|_{}\) norm. This requirement has the same form as the compatibility assumption for the Lasso (Buhlmann and van de Geer, 2011; Oh et al., 2021) or the restricted strong convexity assumption (Cella et al., 2023).

We further make the following assumption on the true multi-task Gram matrix:

**Assumption 4** (RE condition for the true multi-task Gram matrix).: _For \(k[K]\), let \(_{k}[_{k}_{k}^{ }]\) be the Gram matrix of the \(k^{th}\) context vector's marginal distribution, let \(_{}\) be the true multi-task Gram matrix of the context vector generating distribution, given by_

\[_{}_{||} },}= _{k=1}^{K}_{k}.\] (5)

_We assume that \(_{}\) verifies RE condition (Definition 2) with some problem dependent constants \([0,)}_{ }}()})\) and \(>0\)._

This assumption is common to make for Lasso-like bandit problems (Oh et al., 2021; Ariu et al., 2022; Cella et al., 2023). We will later show that it can be transferred to empirical multi-task Gram matrix.

### Oracle inequality

This section is dedicated to provide a bound on the estimation error of the Network Lasso problem given in Equation (2) at a particular step \(t\) of Algorithm 1.We assume fixed design, meaning that the context vectors are given and fixed, and we are not concerned by their randomness (due to the context generating distribution), nor by the randomness of their number for each user (due to random selection at each time step).

For a time step \(t\), we deliver the oracle inequality controlling the deviation between the estimated preference vectors \(}(t)\) and the true ones \(\). For the sake of simplicity, we provisionally assume that the RE condition holds for the empirical multi-task Gram matrix \(_{}(t)\).

**Theorem 1** (Oracle inequality).: _Assume that the RE assumption holds for the empirical multi-task Gram matrix with constants \([0,)}_{ }}()})\) and \(>0\). Suppose that \(_{m}|_{m}(t)| bt\) for some \(b>0\). Then, with a probability at least \(1-(t)\), we have_

\[\|-}(t)\|_{F} 2 }f(,)|}+2b},\]

_where_

\[f(,)_{0}(a_{2}(, )+_{ 1}()w() )((,)+_{  1}()w()}{a_{1}(,) _{}}()}}+1).\]The proof of the previous theorem mainly relies on a decomposition of the estimation error signal into two parts: one is the projection of the error onto its mean per cluster value, that is, every node within the same cluster is mapped to the mean estimation error of its cluster. The second part of the decomposition is simply the residual part i.e. the deviation from the mean per cluster value, which is related to the incidence matrices of each cluster. The probabilistic statement comes from a high probability bound on the Euclidean norm of an empirical vector process associated with our problem, using a generalization of the Hanson-Wright inequality to the subgaussian case (Hsu et al., 2012, Theorem 2.1). Compared to the bound of Jung (2020, Theorem 1), we bound a norm of the estimation error rather than just the total variation semi-norm. Additionally, the bound exhibits different behavior depending on whether \(>1\). Indeed, due to the expressions of \(a_{1}(,)\) and \(a_{2}(,)\), in the case where \(>1\), the bound significantly decreases with the products \(w()_{})}\) and \(w()_{}c_{}( )^{-}\), which are both small enough for dense intra-cluster edge links and sparse inter-cluster ones. However, when \(<1\), the \(w()\) term might dominate if it is moderately large, and its effect can only be mitigated via a small subgaussianity constant \(\) or a large enough RE condition constant \(\).

### RE condition for the empirical multi-task Gram matrix

To establish the oracle inequality, we assumed that the RE condition holds for the empirical multi-task Gram matrix. The goal of this section is to prove this holds with high probability. To this end, we use the same strategy as in Oh et al. (2021); Cella et al. (2023). We prove that on the one hand, given the empirical multi-task Gram matrix inherits the RE condition from its adapted counterpart since it concentrates around it. On the other hand, we prove that the adapted Gram matrix verifies the RE condition due to Assumption 1, 2 and 4 made on the context generation distribution.

**Theorem 2** (RE condition holding for the empirical multi-task Gram matrix).: _Under assumptions 2 and 4, let \(t 1\), and let \(,\) be the constants from Assumption 4. Assume that \(_{m}|_{m}(t)| bt\). Then, for any \(0,1+(,)+(1-)^{+ }w()}{a_{1}(,)}^{-2} \), the empirical multi-task Gram matrix verifies the RE condition with constants \(\) and \(\), with_

\[=(,)+(1-)^{+}w()}{a_{1}(,)})^{2}},\] (6)

_with a probability at least equal to \(1-6d||\!(^{4}( _{}(_{}() _{}()^{2})t}{6b+2^{2} })\), where_

\[:=}_{ }():=c_{}()|| .\]

The proof follows the same approach as in Oh et al. (2021); Cella et al. (2023); we prove that the RE condition transfers from the true multi-task Gram matrix to its adapted counterpart \(_{}(t)\), defined as follows:

\[_{}(t)=(_{1}(t), ,_{||}(t)),\] (7)

where

\[_{m}(t)=_{_{m}(t)}[ ()()^{}|_{-1}].\] (8)

This transfer relies on the work of Oh et al. (2021, lemma 10). The other step of the proof is showing that the empirical multi-task Gram matrix and \(_{}(t)\) become close to each other with high probability after sufficiently many time steps, the respective distance between the two is measured with a matrix norm induced by the RE semi-norm and the restriction to set \(\) (Definition 2). The bound showcases a dependence on \(_{}c_{}()||\), which is of the same order as \(||\) for a fully connected cluster with vertices \(\). It is also clear that with a higher minimum centrality of a cluster, the probability of satisfying the RE condition increases.

### Regret bound

To bound the regret, we bound the expected instantaneous regret for each round \(t 1\). This bound relies on the oracle inequality holding and on the RE condition being satisfied for the empirical Gram matrix, both with high probability. These two conditions are ensured and Theorem 1 and Theorem 2.

**Theorem 3** (Regret bound).: _Let the mean horizon per node be \(=}{||}\). Let \(_{}}()}\) going asymptotically to infinity and \(_{}}()}\) going asymptotically to zero as well as \(_{}}()}w( )\) and \()}{_{} }()}}\) going asymptotically to zero. Under assumptions1 to 4 and \(<1\), the expected regret of the Network Lasso Bandit algorithm is upper bounded as follows:_

\[(||)=(}{_{}c_{}()}}(|}+||)}+(||)|})+(d||) ),\]

_with \(A=_{}(_{}( )_{}^{2}())}{6|)}{|}}+2}\)._

Our regret is mainly formed of two parts. The first one is the sublinear time-dependent term and represents the bulk of horizon dependence. Interestingly, it does not depend on the dimension, which is a consequence of using the concentration inequality from Hsu et al. (2012). Interestingly, it decreases as the topological centrality index grows with the graph size, which proves the importance of intra-cluster high connectivity.

The second significant term comes from ensuring the RE condition for the empirical multi-task Gram matrix, and can be interpreted as the number of time steps necessary for it to hold, as pointed out by Oh et al. (2021). It has a logarithmic dependence in the graph size and in the dimension, which is a characteristic of regret bound of the "lasso type". Also noteworthy is that the regret grows with \((d)\) only in the time-independent term, making our policy useful in high-dimensional settings.

## 6 Experiments

We provide experiments to showcase the effect on the problem's parameters on our algorithm's performance as well as highlighting its advantageous performance compared to other algorithms. At each time step, the algorithm solves the network lasso problem (2) via a primal-dual algorithm used in Jung (2020).

We compare our algorithm to several baselines of the literature. On the one hand, baselines relying on a given graph, GOBLin (Cesa-Bianchi et al., 2013) and GraphUCB (Yang et al., 2020) that use the Laplacian to smooth the preference vectors. On the other hand, we consider online clustering of bandits baselines, namely CLUB (Gentile et al., 2014) and SCLUB (Li et al., 2019). Since these latter approaches start with a fully connected graph, we provide them the known graph for a fair comparison. As a sanity check, we also compare the independent task learning case with LinUCB (LinUCbITL) where each task is solved independently, and to the case of a LinUCB agent for each cluster (LinUCb Oracle). The graph used is generated using stochastic block models in order to ensure that the generated graph induces a cluster structure, where an edge is constructed with probability \(p\) within clusters and \(q\) between clusters.

Experimentally, we found that normalizing the adjacency matrix, that is we utilize the following normalized edges: \(w_{mn}=}\), where \((m)\) denotes the degree of node \(m\), yields significantly better results. Indeed, such a normalization makes the algorithm focus more on edges between low-degree nodes, which improves the propagation of the collected information within the graph. In all experiments we have set \(_{0}=0.1\).

Our results clearly showcase an improvement compared to the other baselines. Apart from the oracle that has complete knowledge of all clusters from the beginning, our policy performs significantly better than the rest beyond the error margins, covering one standard deviation at ten repetitions. Weprovide results for up to \(||=500\) nodes showing the effective transfer of knowledge within the graph.

## 7 Conclusion and future perspectives

In this work, we proposed a multi-task bandit framework that solves the case where the task preference vectors are piecewise constant over a graph. To this end, we used the Network Lasso policy to estimate the task parameters, which bypasses explicit clustering procedures. We showed a sublinear regret bound and as a byproduct, we proved a novel oracle inequality that relies on the small size of the boundary as well as on the high value of the topological centrality index of each node within its cluster. Our experimental evaluations highlight the advantage of our method, especially when either the number of dimensions or nodes increases.

Due to the technical similarity of our problem with the Lasso, a natural extension would be to extend it to a thresholded approach, in the same vein as (Ariu et al., 2022). Another possible extension would be to use regularization with higher order total variation terms that impose a piecewise polynomial signal on a graph, as explained for scalar signals in Wang et al. (2016); Ortelli and van de Geer (2019).