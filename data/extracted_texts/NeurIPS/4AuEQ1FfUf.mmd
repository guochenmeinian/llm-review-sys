# How Does Black-Box Impact the Learning Guarantee of Stochastic Compositional Optimization?

Jun Chen

College of Informatics, Huazhong Agricultural University, China

cj8504872430163.com

Hong Chen

College of Informatics, Huazhong Agricultural University, China

Engineering Research Center of Intelligent Technology for Agriculture, China

chenh@mail.hzau.edu.cn

Corresponding author.

Bin Gu

School of Artificial Intelligence, Jilin University, China

Mohamed bin Zayed University of Artificial Intelligence

jsgubin@gmail.com

###### Abstract

Stochastic compositional optimization (SCO) problem constitutes a class of optimization problems characterized by the objective function with a compositional form, including the tasks with known derivatives, such as AUC maximization, and the derivative-free tasks exemplified by black-box vertical federated learning (VFL). From the learning theory perspective, the learning guarantees of SCO algorithms with known derivatives have been studied in the literature. However, the potential impacts of the derivative-free setting on the learning guarantees of SCO remains unclear and merits further investigation. This paper aims to reveal the impacts by developing a theoretical analysis for two derivative-free algorithms, black-box SCGD and SCSC. Specifically, we first provide the sharper generalization upper bounds of convex SCGD and SCSC based on a new stability analysis framework more effective than prior work under some milder conditions, which is further developed to the non-convex case using the almost co-coercivity property of smooth function. Then, we derive the learning guarantees of three black-box variants of non-convex SCGD and SCSC with additional optimization analysis. Comparing these results, we theoretically uncover the impacts that a better gradient estimation brings a tighter learning guarantee and a larger proportion of unknown gradients may lead to a stronger dependence on the gradient estimation quality. Finally, our analysis is applied to two SCO algorithms, FOO-based vertical VFL and VFL-CZOFO, to build the first learning guarantees for VFL that align with the findings of SCGD and SCSC.

## 1 Introduction

In recent years, stochastic compositional optimization (SCO), a class of optimization methods that incorporate the compositional form \(f(g(w))\), has garnered significant attention in the researchcommunity [1; 2; 3; 4; 5; 6; 7]. It represents a special case of stochastic bilevel optimization 

\[_{w^{p}}_{}[f_{} (w,v^{*}(w))] v^{*}(w)=_{v ^{d}}_{z}[h_{z}(w,v)],\] (1)

where \(_{z}[]\) represents the expectation with respect to (w.r.t.) the sample \(z\), parameters \(w^{p}\) and \(v^{d}\). If the inner function \(h_{z}(w,v)=\|v-g_{z}(w)\|^{2}\) and the outer function \(f\) is only a function of \(v\), i.e., \(f_{}(w,v)=f_{}(v)\), the stochastic bilevel optimization reduces to the SCO:

\[_{w^{p}}\{F(w)=f(g(w))=_{ }[f_{}(_{z}[g_{z}(w)])]\},\] (2)

where \(F(w)\) is the compositional population risk, \(f:^{d}\), \(g:^{p}^{d}\), \(f(v)=_{}[f_{}(v)]\) and \(g(w)=_{z}[g_{z}(w)]\).

Many applications adhere to the form of SCO (2) such as risk averse optimization , group distributionally robust optimization , AUC maximization [11; 12; 13; 14], model-agnostic meta-learning , and first-order-optimization-based vertical federated learning (FOO-based VFL) . Apart from the above applications with available derivatives, there exist derivative-free scenarios as well, such as reinforcement learning  and zeroth-order-optimization-based (ZOO-based) VFL [18; 19].  discussed the extension of stochastic compositional gradient descent (SCGD) to the derivative-free setting, called black-box SCGD, where the zeroth-order information of \(f\) or \(g\) is available through sampling.

From the perspective of statistical learning theory , the theoretical guarantees pertaining to generalization and optimization performance for SCO algorithms are worth studying to validate their empirical behaviors. The former assesses the disparity between the empirical performance and the population performance for the trained model. The latter measures the empirical performance gap between the trained model and the empirical optimal model. To our knowledge, there is only one study attempting to provide a generalization guarantee in this area.  has pioneered the generalization understanding of two notable SCO algorithms, i.e., SCGD and SCSC  via algorithmic stability tool. They have achieved satisfactory excess risk bounds by selecting some specific values of \(T\) to balance stability results and optimization errors. However, the intricacy of their analysis framework brings some unnecessary terms in their results leading to these so large \(T\) values that it is practically challenging to complete these iterations within a reasonable timeframe. Furthermore, the study has yet to consider a more practical scenario beyond convex and strongly convex cases, specifically, the non-convex case. For the optimization guarantee, plenty of work devotes to studying the convergence behaviors of some first-order SCO algorithms [2; 3; 4; 5] and their extensions [22; 23; 24; 25]. For example,  proved that SCGD can converge almost surely to an existing optimal solution with the rate \((T^{-1/4})\) for non-smooth convex problems and the rate \((T^{-2/7})\) for smooth convex problems, where \(T\) is the total number of iterations.  presented that stochastically corrected stochastic compositional gradient method (SCSC) can achieve the same convergence rate \((T^{-1/2})\) as SGD for non-compositional problems. However, there exists a research gap in the optimization analysis for the derivative-free SCO algorithm as well as in its generalization analysis.

Considering these problems, this paper leverages algorithmic stability to obtain some similar and even superior results of SCGD and SCSC under the milder parameter selection and the non-convex condition. More importantly, to apply a broader class of stochastic optimization problems, this paper pioneers the theoretical analysis of the black-box SCO algorithms, which uncovers the impacts of black-box on the learning guarantees of SCO algorithms. Our main contributions are listed as follows.

* _Generalization guarantees under some milder settings._ Firstly, we provide the sharper generalization upper bounds of convex SCGD and SCSC based on a new stability analysis framework more effective than prior work  with a more practical selection of \(T\). Subsequently, we develop the above convex analysis to the non-convex case by introducing the almost co-coercivity property of smooth function, which yields satisfactory generalization guarantees of SCGD and SCSC under the non-convex condition.
* _Learning guarantees for black-box SCO algorithms._ To apply a broader class of stochastic optimization problems, we further consider three black-box variants (outer, inner, and full black-box) of SCGD and SCSC to obtain the generalization and optimization upper bounds similar to the ones of SCGD and SCSC. Comparing the first-order and zeroth-order results, several key insights into the impacts of black-box on the learning guarantees of SCO algorithms are shown: 1) a closer estimation distance brings a better result; 2) more estimation directions lead to a better result; 3) a larger proportion of unknown gradients results in a stronger dependence on the gradient estimation quality.
* _Applications on VFL_. Finally, we explore the applications of our analysis framework to two specific SCO algorithms, i.e., FOO-based VFL and VFL-CZOFO, where we build the pioneering stability-based generalization and optimization guarantees for first-order and zeroth-order VFL algorithms that align with the findings of SCGD and SCSC.

## 2 Preliminaries

This section describes the learning paradigm of SCO and introduces two popular SCO algorithms (SCGD and SCSC) and their black-box variants in detail at first. Then, some necessary definitions and assumptions are provided for our theoretical analysis. The explanations for all symbols are shown in Table 3 located in _Appendix A_.

Considering a stochastic compositional optimization algorithm (2), the distributions of sample \(z\) and \(\) are unknown. The training dataset \(S=S^{z} S^{z}=\{z_{1},...,z_{n}\}\{_{1},...,_{m}\}\) is available to obtain the final output model parameter \(A(S)\) via minimizing the following compositional empirical risk

\[F_{S}(w)=f_{S}(g_{S}(w))=_{j=1}^{m}f_{_{j}}( {n}_{i=1}^{n}g_{z_{i}}(w)),\]

where \(g_{S}(w)=_{i=1}^{n}g_{z_{i}}(w)\), \(f_{S}(v)=_{j=1}^{m}f_{_{j}}(v)\), \(z_{1},...,z_{n},_{1},...,_{m}\) are independent. Besides, we denote by \(w(S),w^{*}\) the empirical optimal model parameter on \(S\) and the global optimal model parameter, defined as \(w(S)=_{w}F_{S}(w)\) and \(w^{*}=_{w}F(w)\). Then, the generalization error, optimization error and excess risk of \(A(S)\) are given by \(|F(A(S)-F_{S}(A(S))|\), \(F_{S}(A(S))-F_{S}(w(S))\) and \(F(A(S))-F(w^{*})\), respectively. Since \([F(A(S))-F(w^{*})] 0\) and \([F_{S}(w(S))-F(w^{*})] 0\), the excess risk of \(A(S)\) can be decomposed as the summation of generalization error and optimization error as follows

\[[F(A(S))-F(w^{*})][|F(A(S))-F_{S}(A(S))|]+[ F_{S}(A(S))-F_{S}(w(S))],\] (3)

where \([]\) denotes the expectation w.r.t. all randomness.

In this work, we primarily investigate the learning guarantees of two prevalent SCO algorithms, i.e., SCGD  and SCSC , along with their black-box variants. Algorithm 1 presents the detailed parameter update procedures of these algorithms. The difference between SCGD and SCSC lies in the update of the outer model parameter \(v_{t}\). For SCGD, \(v_{t+1}\) is the linear combination of \(v_{t}\) and \(g_{z_{i_{t}}}(w_{t})\). However, this update may lead to a suboptimal convergence rate when the learning rate \(\) of the outer model update is smaller than the learning rate \(_{t}\) utilized for the inner model update. To alleviate this problem, SCSC updates \(v_{t+1}\) with the combination of the "corrected" \(v_{t}\) and \(g_{z_{i_{t}}}(w_{t})\), where \(v_{t}\) is corrected by \(g_{z_{i_{t}}}(w_{t})-g_{z_{i_{t}}}(w_{t-1})\) so that \(v_{t+1}\) approximates \(g_{z_{i_{t}}}(w_{t})\). Besides,  discussed the extension of SCGD to the derivative-free setting where only the zeroth-order information of \(g\) or \(f\) is available through sampling, which potentially applies to a broader class of stochastic optimization problems. Here, we show the first-order gradient estimation of \(f\), which is similar to that of \(g\). The unknown first-order gradient of \(f\) is estimated by Equation (4) and then approximated by Taylor expansion (5) in our analysis

\[f_{_{j_{t}}}(v_{t+1})= _{l=1}^{b}}{}(f_{_{j_{ t}}}(v_{t+1}+ u_{t,l})-f_{_{j_{t}}}(v_{t+1}))\] (4) \[= _{l=1}^{b}( f_{_{j _{t}}}(v_{t+1}),u_{t,l} u_{t,l}+((u_{t,l})^{ }^{2}f_{_{j_{t}}}(v)|_{v=v_{t+1}^{*}}u_{t,l})u_{t,l} ),\] (5)

where \(\{u_{t,l}\}_{l=1}^{b}\) is the set of independent and identically distributed (i.i.d.) random direction vectors (obeying the \(d\)-dimensional uniform distribution), and \(\) is the distance between two model parameters (\(v_{t+1}+ u_{t,l}\) and \(v_{t+1}\)) used to estimate the gradient in the \(l\)-th direction.

Drawing inspiration from the classical non-compositional stability analysis work  and the pioneering work  investigating the generalization of SCO, we introduce the definition of uniform model stability as follows.

**Definition 1**.: _The randomized algorithm \(A\) for SCO problem is uniformly model \((_{z},_{})\)-stable if_

\[_{A}[\|A(S)-A(S^{i,z})\|]_{z}\ \ \ \ _{A}[\|A(S)-A(S^{j,})\|]_{ },\]

_where \(\|\|\) is the Euclidean distance \(\|\|_{2}\) and \(S=\{z_{1},...,z_{n},_{1},...,_{m}\},S^{i,z}=\{z_ {1},...,z_{i-1},z_{i}^{},z_{i+1},...,z_{n},_{1},...,_{m} \},S^{j,}=\{z_{1},...,z_{n},_{1},...,_{j-1}, _{j}^{},_{j+1},...,_{m}\}\) for any \(i[n],j[m]\)._

According to the foundational concept of algorithmic stability, Definition 1 considers the two datasets obtained from the perturbation of a single sample in \(\{z_{i}\}_{i+1}^{n}\) and \(\{_{j}\}_{j=1}^{m}\) respectively, where the altered sample \(z_{i}^{}\) is i.i.d. to \(z_{i}\), and so does \(_{j}^{}\). Prior to filling the relationship gap between the uniformly model stability and the generalization error \([|F(A(S))-F_{S}(A(S))|]\), it is essential to make some fundamental assumptions, i.e., Lipschitz continuity (bounded first-order gradient) of \(g,f\) and bounded variance of \(g\).

**Assumption 1**.: _For any parameters \(w,w^{},v,v^{}^{d}\) and some \(L_{g},L_{f}>0\), functions \(g_{z}(w)\) and \(f_{}(v)\) are Lipschitz continuous, i.e., \(\| g_{z}(w)\| L_{g}\) and \(\| f_{}(v)\| L_{f}\), which also mean that \(\|g_{z}(w)-g_{z}(w^{})\| L_{g}\|w-w^{}\|\) and \(|f_{}(v)-f_{}(v^{})| L_{f}\|v-v^{ }\|.\)_

In numerous compositional  and non-compositional studies , Assumption 1 serves as a general theoretical bridge analyzing the generalization and optimization performance.

**Assumption 2**.: _For any \(w\) and some \(V_{g}>0\), the variance of function \(g_{z}(w)\) is upper bounded by \(V_{g}\), i.e., \(_{z}[\|g_{z}(w)-g(w)\|^{2}] V_{g}.\)_

The bounded variance is also a classical condition for statistical learning theory  which limits the ranges of the variance value of the given functions \(g\). Utilizing these two fundamental assumptions, Theorem 1 builds a rigorous relationship between stability and generalization error, thereby enabling stability to measure the generalization performance in the subsequent analysis. Note that, Theorem 1 was previously proved by  (Theorem 2.3), so we omit its detailed proof here for brevity.

**Theorem 1**.: _[_21_]_ _Let Assumptions 1, 2 hold. Assume the randomized algorithms \(A\) for SCO problem is uniformly model \((_{z},_{})\)-stable, then,_

\[[|F(A(S))-F_{S}(A(S))|] L_{g}L_{f}(4_{}+ _{})+L_{f}V_{g}}.\]

**Remark 1**.: _As mentioned in , Theorem 1 is the compositional counterpart of Theorem 2.2 in . In other words, the above result is equivalent to \([|F(A(S))-F_{S}(A(S))|] L_{f}_{}\) when \(g_{z}(w)=w\), i.e., \(F(w)=_{}[f_{}(w)]\) and \(F_{S}(w)=_{j=1}^{m}f_{_{j}}(w)\). If the order of \(_{z}\) is faster than \((_{z}+n^{-})\), the generalization upper bound of SCO algorithm will be primarily constrained by the term \(4L_{g}L_{f}_{z}+L_{f}V_{g}}\) attributed to the compositional structure. Otherwise, there is little difference between Theorem 1 and Theorem 2.2._

The subsequent assumptions and definition are required by the stability analysis in Section 3.

**Assumption 3**.: _For any parameters \(w,w^{},v,v^{}^{d}\) and some \(_{g},_{f},>0\), functions \(g_{z}(w)\), \(f_{}(v)\) and \(f_{}(g_{z}(w))\) are smooth, i.e., \(\|^{2}g_{z}(w)\|_{g}\), \(\|^{2}f_{}(v)\|_{f}\) and \(\|^{2}f_{}(g_{z}(w))\|\), which also mean that_

\[\| g_{z}(w)- g_{z}(w^{})\|_{g}\|w-w^{} \|,\ \ \| f_{}(v)- f_{}(v^{})\|_{f}\|v- v^{}\|\]

_and_

\[\| f_{}(g_{z}(w))- f_{}(g_{z}(w^{}))\| \|w-w^{}\|.\]

**Definition 2**.: _For any parameter \(v,v^{}^{d}\), a function \(f:^{d}\) is convex if \(f(v) f(v^{})+ f(v^{}),v-v^{}.\)_

**Assumption 4**.: _For any \(w,v^{d}\), direction vector \(u\), step size \(>0\) and some \(M_{g},M_{f},M_{g}^{},M_{f}^{}>0\), the following inequalities hold_

\[\|g_{z}(w+ u)-g_{z}(w)\| M_{g},\ \ |f_{}(v+ u)-f_{}(v) | M_{f},\]

_and_

\[\| g_{z}(w+ u)- g_{z}(w)\| M_{g}^{},\ \ \| f_{}(v+ u)- f_{}(v)\| M_{f}^{}.\]

**Remark 2**.: _Assumption 3 is the most important condition for our analysis since there are several key properties (Lemma 4) of smoothness required to measure the algorithmic stability. In addition, our stability analysis framework relies crucially on another key lemma (called co-coercive lemma, Lemma 1) derived from the smoothness and convexity of the function \(f(w)\). Therefore, we provide the definition of convexity in Definition 2. Except for the convex case (Section 3.1), this work mainly considers some non-convex cases (Sections 3.1, 3.2). Although the co-coercive lemma is not available without the convexity condition, a surrogate lemma (called almost co-coercive lemma, Lemma 3) takes a similar role within our analysis framework. Finally, Assumption 4 gives the upper bounds of the difference between two adjacent function values for \(g,f, g, f\), which represents a less stringent assumption compared to the general bounded condition . Specifically, Assumption 4 is different from the assumption \(|f| M\). Assumption 4 requires the distance between two adjacent function outputs to be bounded, i.e., \(||g(w+ u)-g(w)|| M_{g},|f(v+ u)-f(v)| M_{f}\), which is milder than \(|f| M\). Besides, it also requires the distance between two adjacent gradient outputs to be bounded, i.e., \(|| g(w+ u)- g(w)|| M_{g}^{},|| f(v+ u)-  f(v)|| M_{f}^{}\), which is milder than bounded gradient condition \(|| f|| L\)._

## 3 Main Results

This section presents the learning guarantees of two SCO algorithms (SCGD and SCSC) under several cases. The comparisons among our results and previous work are summarized in Tables 1, 2, and their proofs are provided in _Appendices C, D_.

### Learning Guarantees for General SCO

Firstly, we consider the generalization analysis for the general convex SCO algorithm.

**Theorem 2**.: _Let Assumptions 1, 3 hold and the function \(f(g(w))\) is convex. Assume that the randomized algorithms A (Algorithm 1) for SCO problem brings the model sequences \(\{w_{t}\}_{t=1}^{T}\) and \(\{w_{t}^{i,z}\}_{t=1}^{T}(\{w_{t}^{j,}\}_{t= 1}^{T})\) on \(S\) and \(S^{i,z}(S^{j,})\) with the step size sequence \(\{_{t}\}_{t=1}^{T}\)._

_(a) For SCGD with_ \(_{t}\)_, the final output_ \(A(S)=w_{T}\) _is uniformly model_ \((_{z},_{})\)_-stable with_

\[_{z}=L_{f}(eT)}{ n}\ \ \ \ _{}=L_{f}(eT)}{ m}.\]

_(b) For SCSC with_ \(_{t}\)_, the final output_ \(A(S)=w_{T}\) _is uniformly model_ \((_{z},_{})\)_-stable with_

\[_{z}=L_{f}(eT)}{ n}\ \ \ \ _{}=L_{f}(eT)}{ m}.\]

**Remark 3**.: _Based on a new stability analysis framework more effective than prior work , Theorem 2 states the stability upper bounds \(((n^{-1}+m^{-1}) T)\) for SCGD and \(((n^{-1}+m^{-1}) T)\) for SCSC under the convex condition, which derives a generalization bound \(((n^{-1}+m^{-1}) T+n^{-})\) by combining with Theorem 1. Previously,  provided the stability results \(( T(n^{-1}+m^{-1})+(T^{}+ ^{-}T^{-+1}+^{}T)+^{2} ^{-1}T)\) for SCGD and \(( T(n^{-1}+m^{-1})+(T^{}+ ^{-}T^{-+1}+^{}T)+^{2} ^{-}T)\) for SCSC in the same setting, where \(c>0\) is an arbitrary constant. They selected \(=(T^{-})\), \(=(T^{-}),c>2\), \(T=(\{n^{},m^{}\})\) for SCGD and \(=(T^{-})\), \(=(T^{-}),c>4\), \(T=(\{n^{},m^{}\})\) for SCSC to yield the bounds \((\{n^{-1},m^{-1}\}\{n^{ {2}},m^{}\})\) which is slight larger than \((n^{-}+m^{-})\). Compared with , Theorem 2 enjoys not only tighter bounds but also some more practical parameter selections of \(,,T\). There are some experiments  to validate this statement. **(1) For \(T\):** provided some generalization bounds for convex SCGD and SCSC with some impractical \(T\) such as \(T=O((n^{7/2},m^{7/2}))\) in Theorem 4. While our convex result (Theorem 2) can achieve similar rates even taking \(T=O((n,m))\) which better matches some empirical observations (Fig. 1, 2 in  and Fig. 2 in ). **(2) For \(_{t}\):** Theorem 4 in  took \(_{t}=T^{-6/7}\) which is too small when \(T\) is large. While Theorem 2 takes \(_{t}=O(t^{-1})\) closer to some empirical selections (\(_{t}=O(t^{-3/4})\) in  and \(_{t}=O(t^{-1})\) in ). **(3) For \(_{t}\):** Theorem 4 in  took \(_{t}=T^{-4/7}\) which is also too small since  empirically select \(_{t}=t^{-1/2}\) or \(_{t}=t^{-1}\). In contrast, Theorem 2 have no special restriction on \(_{t}\)._

_Moreover, the bounds of Theorem 2 are similar to some popular stability bounds in the non-compositional literature. For example, the most classical work  achieved the uniform stability bound \((n^{-1} T)\) for convex SGD with \(_{t}(t^{-1})\).  showed the uniform stability bound \((n^{-1}T^{}+n^{-})\) for convex pairwise SGD with \(_{t}=(T^{-})\). The proof of Theorem 2 is provided in Appendix C._

  Algorithm & Generalization &  \\  & \(L\) & \(\) & \(V\) & \(M\) & C. \\  SGD ( Thm. 3.8) & \((n^{-1} T)\) & \(\) & \(\) & \(\) & \(\) & \(\) \\ SGD ( Thm. 4) & \(((n^{-1}+n^{-}) n)\) & \(\) & \(\) & \(\) & \(\) & \(\) \\ \(\) ( Thm. 3.7) & \((T^{-}+(n^{-1}+m^{-1})T^{}+m^{- })\) & \(\) & \(\) & \(\) & \(\) \\ \(\) (Thm. 2) & \(((n^{-1}+m^{-1}) T+n^{-})\) & \(\) & \(\) & \(\) & \(\) \\  SGD ( Thm. 3.12) & \((n^{-1}T^{})\) & \(\) & \(\) & \(\) & \(\) & \(\) \\ SGD ( Thm. 15) & \((n^{-1}T^{})\) & \(\) & \(\) & \(\) & \(\) & \(\) \\ SGD ( Thm. 1) & \((n^{-1} T)\) & \(\) & \(\) & \(\) & \(\) & \(\) \\ SGD ( Cor. 17) & \((n^{-1}T)\) & \(\) & \(\) & \(\) & \(\) & \(\) \\ \(\) (Thm. 3, 4, Cor. 4, 2) & \(((n^{-1}+m^{-1})T^{} T+n^{- })\) & \(\) & \(\) & \(\) & \(\) & \(\) \\ VFL (Cor. 4, 5) & \((n^{-1}T^{} T)\) & \(\) & \(\) & \(\) & \(\) & \(\) \\  

Table 1: Comparisons among the stability-based generalization guarantees for SCO algorithms and SGD (Thm.-Theorem; Cor.-Corollary; \(\)-high probability bound; \(L,,V,M,\)C.-Lipschitz continuity, smoothness, bounded variance, bounded function, and convexity assumptions; \(c\)-a positive constant; \(\)-has such a property; \(\)-hasn’t such a property).

To further weaken our assumptions, the generalization analysis of the convex SCO algorithm is developed into the non-convex setting by introducing the almost co-coercivity property of smooth function.

**Theorem 3**.: _Let Assumptions 1, 3 hold. Assume that the randomized algorithms \(A\) (Algorithm 1) for SCO problem brings the model sequences \(\{w_{t}\}_{t=1}^{T}\) and \(\{w_{t}^{i,z}\}_{t=1}^{T}(\{w_{t}^{j,}\}_{t= 1}^{T})\) on \(S\) and \(S^{i,z}(S^{j,})\) with the step size sequence \(\{_{t}\}_{t=1}^{T}\). For SCGD with \(_{t},=_{g}L_{f}+ L_{g}^{2}_{f}\) and SCSC with \(_{t},=_{g}L_{f}+L_{g}^{2}_{f}\), the final output \(A(S)=w_{T}\) is uniformly model \((_{z},_{})\)-stable with_

\[_{z}=L_{f}(eT)^{}(eT)}{ n}\ \ \ \ _{}=L_{f}(eT)^{}(eT)}{ m}.\]

**Remark 4**.: _Under the non-convex setting, Theorem 3 elucidates a satisfactory stability bound \(((n^{-1}+m^{-1})T^{} T)\) but \(T^{}\)-times larger than Theorem 2. When \(T=(\{n,m\})\) and ignoring logarithmic terms, this bound is equivalent to \((\{n^{-1},m^{-1}\}\{n^{},m^{}\})\). Therefore, under the further weakening condition, i.e., non-convexity, Theorem 3 achieves the stability results similar to the ones of the convex SCGD and SCSC in  and some non-compositional, non-convex results . The proof of Theorem 3 is provided in Appendix C._

### Learning Guarantees for Black-box SCO

The aforementioned results lay the groundwork for elucidating the impacts of black-box on the learning guarantees for the non-convex SCGD and SCSC, including additional optimization analysis. Three black-box cases shown in Algorithm 1 are considered in this part. Prior to the analysis, we provide the following assumption required by the optimization analysis.

**Assumption 5**.: _For any \(w\) and parameter \(>0\), the empirical risk \(F_{S}(w)\) satisfies \([\| F_{S}(w)\|^{2}] 2[F_{S }(w)-F_{S}(w)].\)_

In the absence of the convexity assumption, the gradient of empirical risk \(\| F_{S}(w)\|=0\) does not guarantee a global optimal parameter. Assumption 5 postulates that all empirical local optimal parameters are, in fact, empirical global optimal parameters, which prepares for the characterization of \([F_{S}(A(S))-F_{S}(w(S))]\).

**Theorem 4**.: _Let Assumptions 1, 2, 3, 4, 5 hold. Assume that the randomized algorithms \(A\) (Algorithm 1) for SCO problem brings the model sequences \(\{w_{t}\}_{t=1}^{T}\) and \(\{w_{t}^{i,z}\}_{t=1}^{T}(\{w_{t}^{j,}\}_{t= 1}^{T})\) on \(S\) and \(S^{i,z}(S^{j,})\) with the step size sequence \(\{_{t}\}_{t=1}^{T}\). For the outer black-box SCGD with

  Algorithm & Optimization &  \\  & & \(L\) & \(\) & \(V\) & \(M\) & C. \\  SCGD ( Thm. 8) & \((T^{-})\) & \(\) & \(\) & \(\) & \(\) & \(\) \\ SCSC ( Thm. 1) & \((T^{-})\) & \(\) & \(\) & \(\) & \(\) & \(\) \\ SCGD/SCSC ( Thm. 3.7) & \((T^{-})\) & \(\) & \(\) & \(\) & \(\) & \(\) \\ Outer black-box SCGD/SCSC (Thm. 4) & \((^{2}+}{b})\) & \(\) & \(\) & \(\) & \(\) & \(\) \\ Inner black-box SCGD/SCSC (Cor. 1) & \((^{2}+}{b})\) & \(\) & \(\) & \(\) & \(\) & \(\) \\ Full black-box SCGD/SCSC (Cor. 2) & \((^{4}+}{b^{2}}+}{b})\) & \(\) & \(\) & \(\) & \(\) & \(\) \\ VFL-CZOFO (Ours, Cor. 5) & \((^{2}+}{b})\) & \(\) & \(\) & \(\) & \(\) & \(\) \\  

Table 2: Comparisons among the optimization guarantees for SCO algorithms (Thm.-Theorem; Cor.-Corollary; \(L,,V,M\),C.-Lipschitz continuity, smoothness, bounded variance, bounded function, and convexity assumptions; \(d_{2}=(d)\); \(\)-has such a property; \(\)-hasn’t such a property).

\(_{t}=,p\{},}+B_{2}^{2}M_{f}^{})}{}\}\) and the outer black-box SCSC with \(_{t}=,p\{}, }+L_{g}^{2}M_{f}^{})}{}\}\), the final output \(A(S)=w_{T}\) has the learning guarantee_

\[[F(w_{T})-F(w^{*})]((n^{-1}+m^{- 1})T^{} T+n^{-}+^{2}+b^{-1}d_{2}),\]

_where \(d_{2}=d-(2p+1)+(p+)^{2}^{2}\) for SCGD and \(d_{2}=d-2p-1+(p+)^{2}\) for SCSC._

The proof of Theorem 4 is provided in _Appendix D_.

**Remark 5**.: _Theorem 4 considers the outer balck-box SCGD and SCSC algorithms where only the gradient of the outer function \(f\) is unknown. It establishes the excess risk bound \(((n^{-1}+m^{-1})T^{} T+n^{- {2}}+^{2}+b^{-1}d_{2})\).  derived the excess risk bound \((\{n^{-1},m^{-1}\}\{n^{ {2}},m^{}\})\) for the convex SCGD and SCSC with the parameter selections in Remark 3. Theorem 4 can derive this bound with the milder condition \(T=(\{n,m\})\) and \(_{t}=(t^{-1})\)._

_For the generalization bound, Theorem 4 is consistent with Theorem 3. As for the optimization bound,  proved the convergence rate \([\| F(w_{T})\|^{2}](T^{-})\) of SCGD for non-convex problems.  proved the convergence rate \(T^{-1}_{t=0}^{T-1}[\| F(w_{t})\|^{2}]( T^{-})\) of SCSC for non-convex problems. Compared with , Theorem 4 obtains the black-box-related bound \((^{2}+b^{-1}d_{2})\) with some smaller learning rates required by our analytical framework. This bound is composed of two dependencies on the estimation distance \(\) and the number \(b\) of estimation directions in Equation (4), which shows the following two key insights into the impacts of black-box on the learning guarantees of SCO algorithms. Firstly, a small \(\) indicates a small distance between the two function values \(f_{}(v+ u)\) and \(f_{}(v)\) selected to make gradient estimation \(f_{}(v)\) in the direction of the unit vector \(u\). Therefore, a smaller \(\), i.e., a closer estimation distance, brings a better gradient estimation, resulting in a better excess risk bound. Secondly, a large \(b\) indicates that plenty of unit vectors \(u_{l},l=1,...,b\) with different directions are selected to make gradient estimation \(f_{}(v)\). Then, a larger \(b\), i.e., more estimation directions, leads to a better gradient estimation, resulting in a better excess risk bound. In summary, the bound of Theorem 4 verifies the fact that a better gradient estimation brings a tighter learning guarantee for the outer black-box SCGD and SCSC._

_Except for the black-box-related term, Theorem 4 chooses the learning rates affected by \(\), which is also different from Theorem 4. Although \(\) can be very small such as \(10^{-4}\), its negative impact on \(_{t}\) can be eliminated by \(M_{f},M_{f}^{}\) since the two inequalities \(M_{f}/ L_{f}\) and \(M_{f}^{}/_{f}\) hold._

The inner black-box and the full black-box SCO algorithms are studied in the following two corollaries, respectively.

**Corollary 1**.: _Let the assumptions of Theorem 4 hold. For the inner black-box SCGD with \(_{t}=,p\{},  M_{g}+M_{g}^{}L_{f})}{}\}\) and the inner black-box SCSC with \(_{t}=,p\{}, _{f}M_{g}+M_{f}^{}L_{f})}{}\}\), the final output \(A(S)=w_{T}\) has the learning guarantee_

\[[F(w_{T})-F(w^{*})]((n^{-1}+m^{ -1})T^{} T+n^{-}+^{2}+b^{-1}d_{2}),\]

_where \(d_{2}=d-(2p+1)+(p+)^{2}^{2}\) for SCGD and \(d_{2}=d-2p-1+(p+)^{2}\) for SCSC._

**Corollary 2**.: _Let the assumptions of Theorem 4 hold. For the full black-box SCGD with \(_{t}=,p\{}, M_{f}^{}M_{g}+M_{f}M_{f}^{})}{^{2 }}\}\) and the full black-box SCSC with \(_{t}=,p\{}, M_{f}^{}M_{g}+M_{f}M_{f}^{})}{^{2}}\}\), the final output \(A(S)=w_{T}\) has the learning guarantee_

\[[F(w_{T})-F(w^{*})]((n^{-1}+m^{-1 })T^{} T+n^{-}+^{4}+b^{-2}d_{2}^{2}+b^{-1}d _{2}),\]_where \(d_{2}=d-2)}\,+(p+)\) for SCGD and \(d_{2}=d-2)}+p+\) for SCSC._

The proofs of Corollaries 1, 2 are provided in _Appendix D_.

**Remark 6**.: _Corollary 1 provides the excess risk bound with the same order as Theorem 4 for the inner black-box SCGD and SCSC. The excess risk bound for the full black-box SCGD and SCSC in Corollary 2 presents the different dependencies on \(\) and \(b\), i.e., \(^{4}+b^{-2}d_{2}^{2}+b^{-1}d_{2}\). These dependencies also comply with the two key insights uncovered by Theorem 4 and Corollary 1. Besides, when \(^{4}b>d_{2}\) or \(^{2}b<d_{2}\) holds, the term is dominated by \(^{4}\) or \(b^{-2}d_{2}^{2}\) which denotes a stronger dependence on the gradient estimation quality. Hence, Corollary 2 shows that a larger proportion of unknown gradients may lead to a stronger dependence on the gradient estimation quality._

## 4 Applications

Considering the existing derivative-free cases in VFL, the analysis framework of SCGD and SCSC is herein applied to two VFL algorithms, FOO-based VFL  and VFL-CZOFO . As outlined in Algorithm 2 of _Appendix A_, FOO-based VFL algorithm comprises two components, the \(K\) local clients with the model parameters \(w^{k},k[K]\) and the central server with the global model \(v\). Concerning the data privacy, different clients do not communicate with each other directly, but exchange information indirectly through a server. For this reason, the objective function of the \(k\)-th client adopts the same compositional structure \(f(g(w^{k}))\) as SCGD and SCSC. To further ensure data privacy without additional protection techniques, VFL-CZOFO is proposed by introducing the idea of ZOO. Different from the general ZOO-based VFL , VFL-CZOFO employs a zeroth-order gradient on the output layer of every client, with other parts utilizing the first-order gradient, which preserves the privacy protection of ZOO while significantly enhancing convergence.

Before stating our remaining results, it should be noted that there are a few differences between the setting of FOO-based VFL (VFL-CZOFO) and the one of SCGD (SCSC). First of all, we set \(S=\{z_{1},...,z_{n}\}\) and \(S^{i,z}=\{z_{1},...,z_{i-1},z_{i}^{},z_{i+1},...,z_{n}\}\) according to the learning paradigm of VFL. Therefore, Theorem 1 is simplified as Corollary 3. Secondly, the update of the outer model (global model) for FOO-based VFL (VFL-CZOFO) is not based on the simple moving average in SCGD (SCSC). Thirdly, Assumptions 1, 3, 4, 5 hold for every client in all \(K\) clients. Without loss of generality, we only study the learning guarantees of FOO-based VFL and VFL-CZOFO for the \(k\)-th client.

**Corollary 3**.: _Let Assumption 1 hold. Assume the randomized VFL algorithms \(A\) is uniformly model \(_{z}\)-stable, then, \([|F(A(S))-F_{S}(A(S))|] L_{g}L_{f}_{z}\)._

Corollary 3 gives the relationship between uniform model stability and generalization error under the setting of VFL. The proof of Corollary 3 is omitted since it can be proved by Equation (15) in the proof of Theorem 2.3  without the decomposition in Equation (14). The last two results study the theoretical performance of FOO-based VFL and VFL-CZOFO.

**Corollary 4**.: _Let Assumptions 1, 3, 4, 5 hold. For the \(k\)-th client (\(k[K]\)), assume that the randomized FOO-based VFL algorithm (Algorithm 2) brings the model sequences \(\{w_{t}^{k}\}_{t=1}^{T}\) and \(\{w_{t}^{i,z,k}\}_{t=1}^{T}\) on \(S\) and \(S^{i,z}\) with the step size sequence \(\{_{t}\}_{t=1}^{T},_{t},=_{g }L_{f}+L_{g}^{2}_{f}\). Then, the final output \(A(S)=w_{T}^{k}\) of the \(k\)-th client has the generalization guarantee_

**Corollary 5**.: _Let Assumptions 1, 3, 4, 5 hold. For the \(k\)-th client \((k[K])\), assume that the randomized VFL-CZOFO algorithm (Algorithm 2) brings the model sequences \(\{w_{t}^{k}\}_{t=1}^{T}\) and \(\{w_{t}^{i,z,k}\}_{t=1}^{T}\) on \(S\) and \(S^{i,z}\) with the step size sequence \(\{_{t}\}_{t=1}^{T},_{t}=,p\{ },M_{f}+L_{g}^{2}M_{f}^{2} )}{}\}\). Then, the final output \(A(S)=w_{T}^{k}\) of the \(k\)-th client has the generalization guarantee_

\[[F(w_{T}^{k})-F(w^{k*})](n^{-1}T^{ } T+^{2}+b^{-1}d_{2}),\]

_where \(d_{2}=d-2p-1+(p+)^{2}.\)_The proofs of Corollaries 4, 5 are provided in _Appendix E_.

**Remark 7**.: _Corollaries 4, 5 both provide the first stability-based generalization bound \(n^{-1}T^{} T\). As for the optimization bound, Corollary 5 gives \((^{2}+b^{-1}d_{2})\), which originates from the outer black-box setting._

_Apart from VFL, the generalization guarantee of zeroth-order horizontal federated learning (HFL) was studied with the algorithmic stability tool.  established the systematic theoretical assessments of synchronous federated zeroth-order optimization (FedZO) by developing the on-average model stability analysis. Its generalization bounds and optimization bounds all depend on the two gradient estimation-based parameters \(\) and \(b\). From our perspective, the complicated compositional structure may be a key factor that makes the generalization bound in Corollary 5 unaffected by the impact of the estimated gradient quality. The reason is that the analysis framework of Theorem 2 in  requires a decomposition (Equation (6)) which is hardly achieved due to the compositional structure in our analysis._

## 5 Conclusions

In this paper, we provide a novel, more effective theoretical analysis of two SCO algorithms, SCGD and SCSC, and their black-box variants utilizing the uniform model stability tool. The analysis framework is applied to the two VFL algorithms, FOO-based VFL and VFL-CZOFO. Our results not only offer satisfactory learning guarantees but also theoretically validate the impacts of black-box that a better gradient estimation brings a tighter learning guarantee and a larger proportion of unknown gradients leads to a stronger dependence on the gradient estimation quality. We hope our study can facilitate future theoretical analyses of SCO problems and inspire new practical algorithms.