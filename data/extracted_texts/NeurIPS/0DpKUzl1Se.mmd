# Adaptive Uncertainty Estimation via

High-Dimensional Testing on Latent Representations

 Tsai Hor Chan

Department of Statistics and Actuarial Science

The University of Hong Kong

hchanth@connect.hku.hk

&Kin Wai Lau

TCL AI Lab

Hong Kong

stevenlau@tcl.com

&Jiajun Shen

TCL AI Lab

Hong Kong

shenjiajun90@gmail.com

&Guosheng Yin

Department of Mathematics

Imperial College London

guosheng.yin@imperial.ac.uk

&Lequan Yu

Department of Statistics and Actuarial Science

The University of Hong Kong

lqyu@hku.hk

Corresponding Author

###### Abstract

Uncertainty estimation aims to evaluate the confidence of a trained deep neural network. However, existing uncertainty estimation approaches rely on low-dimensional distributional assumptions and thus suffer from the high dimensionality of latent features. Existing approaches tend to focus on uncertainty on discrete classification probabilities, which leads to poor generalizability to uncertainty estimation for other tasks. Moreover, most of the literature require seeing the out-of-distribution (OOD) data in the training for better estimation of uncertainty, which limits the uncertainty estimation performance in practice because the OOD data are typically unseen. To overcome these limitations, we propose a new framework using data-adaptive high-dimensional hypothesis testing for uncertainty estimation, which leverages the statistical properties of the feature representations. Our method directly operates on latent representations and thus does not require retraining the feature encoder under a modified objective. The test statistic relaxes the feature distribution assumptions to high dimensionality, and it is more discriminative to uncertainties in the latent representations. We demonstrate that encoding features with Bayesian neural networks can enhance testing performance and lead to more accurate uncertainty estimation. We further introduce a family-wise testing procedure to determine the optimal threshold of OOD detection, which minimizes the false discovery rate (FDR). Extensive experiments validate the satisfactory performance of our framework on uncertainty estimation and task-specific prediction over a variety of competitors. The experiments on the OOD detection task also show satisfactory performance of our method when the OOD data are unseen in the training. Codes are available at https://github.com/HKU-MedAI/bnn_uncertainty.

Introduction

Deep neural networks (DNNs) have demonstrated state-of-the-art (SOTA) performances on many problem domains, such as computer vision [30; 31], medical diagnosis [24; 16; 38; 2], and recommendation systems [1; 3]. Despite their successes, most of the existing DNN designs can only recognize in-distribution samples (i.e., samples from training distributions) but cannot measure the confidence level (i.e., the uncertainties) in the prediction, especially for out-of-distribution (OOD) samples. Estimation of prediction uncertainties plays an important role in many machine learning applications [25; 37]. For instance, uncertainty estimation can help to detect OOD samples in the data and inspect anomalies in the system. Further, the uncertainty estimates can indicate the distributional shifts in the environment and thus facilitate learning in a non-stationary environment.

In response to the high demand, several attempts have been made to obtain a better estimate of the uncertainty, which can be roughly divided into two categories -- Bayesian and non-Bayesian methods. Bayesian methods [4; 19] mainly operate with a Bayesian neural network (BNN), which introduces a probability distribution (e.g., multivariate Gaussian) to the neural network weights. This enables the model to address model-wise (i.e., epistemic) uncertainty by drawing samples from the posterior distribution. Non-Bayesian methods [32; 27; 10], on the other hand, assume a distribution on the model outputs (e.g., classification probabilities). The uncertainty scores can be obtained by evaluating a pre-determined metric (e.g., classification entropy) on the derived distribution.

However, most of the aforementioned methods are subject to several drawbacks: (1) They rely on strong assumptions (e.g., parametric models) on low-dimensional features and thus suffer from the curse of high dimensionality, leading to poor performance when the dimension of the output is high. (2) They are limited to classification problems, and existing methods explicitly make assumptions about the classification probabilities. This leads to poor generalizability in uncertainty estimation for other tasks, such as regression and representation learning. (3) Their performances heavily rely upon the feature encoder, which can be compromised when the features are of poor quality (e.g., the number of samples used to train the encoder is small). (4) They require a modification of the training loss, and thus additional training is needed when applying the methods to a new problem. Pretrained features cannot be directly applied to the OOD tasks without retraining for their proposed loss.

Observing the above limitations, we propose a new framework for uncertainty estimation. Our framework comprises two key components: one Bayesian deep learning encoding module and one uncertainty estimation module using the adaptable regularized Hotelling \(T^{2}\) (ARHT) . Our contributions are summarized as: (1) We formulate uncertainty estimation as a multiple high-dimensional hypothesis testing problem, and adopt a Bayesian deep learning module to better address the aleatoric and epistemic uncertainties when learning feature distributions. (2) We propose using the ARHT test statistic for measuring uncertainty and demonstrate the advantages of ARHT over the existing uncertainty measures, including its consistency and robustness properties. This enables us to design a data-adaptive detection method so that each individual data point can be assigned an optimal hyperparameter. Because it relaxes the strong assumptions on latent feature distributions, ARHT less sensitive to the feature quality. As result, our method can be interpreted as a post-hoc method as it works on feature distributions generated by any encoder (e.g., a pre-trained encoder). (3) We adopt the family-wise testing procedure to determine the optimal threshold of OOD detection, which minimizes the false discovery rate (FDR). (4) We perform extensive experiments on standard and medical image datasets to validate our method on OOD detection and image classification tasks compared to SOTA methods. Our proposed method does not require prior knowledge of the OOD data, while it yields satisfactory performance even when the training samples are limited.

## 2 Preliminaries

**Deep Neural Network**: A deep neural network (DNN) with \(L\) layers can be defined as

\[f_{l}(;W_{l},b_{l})=}}W_{l}(f_{l-1}( {x};W_{l-1},b_{l-1}))+b_{l}, l\{1,,L\},\]

where \(\) is a nonlinearity activation function, \(\) is the input, \(D_{l-1}\) is the dimension of the input, \(b_{l} R^{D_{l}}\) is a vector of bias parameters for layer \(l\), and \(W_{l} R^{D_{l} D_{l-1}}\) is the matrix of weight parameters. Let \(_{l}=\{W_{l},b_{l}\}\) denote the weight and bias parameters of layer \(l\), and the entire trainable network parameters are denoted as \(=\{_{l}\}_{l=1}^{L}\).

**Bayesian neural network (BNN)**: A BNN specifies a prior \(()\) on the trainable parameters \(\). Given the dataset \(=\{_{i},y_{i}\}_{i=1}^{N}\) of \(N\) pairs of observations and responses, we aim to estimate the posterior distribution of \(\), \(p(|)=()_{i=1}^{N}p(y_{i}|f(_{i} ;))/p(),\) where \(p(y_{i}|f(_{i};))\) is the likelihood function and \(p()\) is the normalization term.

**Pooled Sample Covariance Matrix:** Let \(\{_{1j}\}_{j=1}^{n_{1}}\) be the \(p\)-dimensional embeddings from the training images and \(\{_{2j}\}_{j=1}^{n_{2}}\) be the \(p\)-dimensional embeddings from the testing images. The pooled sample covariance is defined as

\[_{n}=_{k=1}^{2}_{j=1}^{n_{k}}(_{kj}-}_{k})(_{kj}-}_{k})^{},\] (1)

where \(n=n_{1}+n_{2}\), \(n_{k}\) is the sample size and \(}_{k}\) is the sample mean of the \(k\)-th set \((k=1,2)\).

**High-Dimensional Test Statistics:** The Hotelling \(T^{2}\) test statistic  is given by

\[T=(}_{1}-}_{2})^{}_{n}^{- 1}(}_{1}-}_{2}).\] (2)

The Hotelling \(T^{2}\) test assumes \(T F(p,n-p)\) under the null hypothesis. To resolve the potential singularity of the covariance matrix, the regularized Hotelling \(T^{2}\) (RHT) test statistic  loads an identity matrix \(_{p}\) to \(T\),

\[()=n_{2}}{n_{1}+n_{2}}(}_{1}-}_{2})^{}(_{n}+_{p})^{-1}(}_{1}-}_{2}),\] (3)

where \(\) is a tuning parameter. The ARHT test statistic, which will be formally introduced by Eq. 4 in Section 3.3, standardizes the RHT and addresses the skewness of Hotelling's \(T^{2}\) when the feature dimension is high.

## 3 Methodology

Our uncertainty estimation framework comprises a Bayesian neural network encoder and an OOD testing module with ARHT as the uncertainty measure. Figure 1 provides an overview of our proposed framework, with the detailed algorithm given in the appendix. Ablation studies on key components of our framework are provided in Section 5.

### Problem Definition and Test Hypothesis

Suppose that we have a set of samples \(\{I_{1},,I_{n}\}\) as the training set, and a set of testing samples \(I^{}_{1},,I^{}_{n}\}\) containing both in-distribution and OOD data. We aim to develop an uncertainty estimation framework that can accurately classify the test samples as either in-distribution or OOD, without seeing the OOD data during the training. Moreover, the framework can still perform well on its predictive task (e.g., classification) without sacrifice in uncertainty estimation. We set the null hypothesis \(H_{0}:_{1}=_{2}\) and the alternative hypothesis as \(H_{1}:_{1}_{2}\), where \(_{1}\) and \(_{2}\) are the mean representations of the training and the testing samples, respectively.

### Bayesian Neural Network as Encoder

We adopt a BNN encoder and train it with stochastic variational inference (SVI)  to learn the representation of the input. In SVI, a posterior distribution \(p(|)\) is approximated by a distribution \(q\) selected from a candidate set \(\) by maximizing an evidence lower bound (ELBO): \(_{q}_{ q}[p(|)]-(q\|)\), where \((q\|)\) is the Kullback-Leibler divergence between the variational posterior distribution \(q\) and the prior distribution \(\), \(p(|)\) is the likelihood, and \(_{ q}[p(|)]\) represents the learning objective. This can be supervised (e.g., cross-entropy loss) or unsupervised (e.g., contrastive learning loss) learning. The KL divergence of two multi-variate Gaussian distributions is provided in the appendix. We use gradient descent to optimize the ELBO, i.e., to minimize the distance between prior and variational posterior distributions. We obtain the trained BNN encoder once the optimization step is completed. Since BNNs operate on an ensembleof posterior model weights, they can capture the epistemic uncertainties in the posterior predictive distribution \(p(|)\). Hence, using BNNs instead of frequentist architectures can better approximate the posterior distribution of the feature embeddings.

To obtain the uncertainty scores, we need to compute the sample means \(}_{1},}_{2}\) and covariance matrices \(_{1},_{2}\) for the training data (containing in-distribution data only) and the testing data (containing both in-distribution and OOD data). First, we sample \(s\) weights for every data point in the training data, and generate representations for each data point using the sampled weights. In total, \(n_{1}\) representations are generated from the training data. We can compute the mean \(}_{1}\) and covariance matrix \(_{1}\) of the training data using these \(n_{1}\) representations. For each testing data point \(x_{t}\), we sample \(n_{2}\) weights from the variational posterior distribution, and generate \(n_{2}\) representations \(\{_{tj}=f(x_{t};_{j})\}_{j=1}^{n_{2}}\), where \(_{j}\) is the \(j\)-th weight sample drawn from the trained posterior distribution \(p(|)\). We obtain the mean \(}_{2}=_{j=1}^{n_{2}}_{tj}/n_{2}\) and the covariance matrix \(_{2}=_{j=1}^{n_{2}}(_{tj}-}_{2})(_{tj} -}_{2})^{}/n_{2}\). Hence, we can compute the pooled covariance matrix \(_{n}\) by Eq. (1).

### High-Dimensional Testing as Uncertainty Measure

With the pooled sample covariance matrix, we can compute the ARHT test statistics by Eq. (3) and Eq. (4), where \(}_{1}\) is the mean of training samples and \(}_{2}\) is the mean of \(n_{2}\) embeddings of each testing sample. To solve the skew \(F\) distribution of RHT when \(n p\) for large \(n\) and \(p\) (as shown in Figure 2), we adopt the ARHT statistic to perform a two-sample test which has robust regularization of the covariance matrix . In particular, the ARHT is given by

\[()=()-_{1} (,)}{\{2_{2}(,)\}^{}},\] (4)

where \(_{1}(,)=\{1- m_{F}(-)\}/\{1-(1-  m_{F}(-))\}\),

\[_{2}(,) =(-)}{[1-(1- m_{F}(- ))]^{3}}-(-)- m^{}_{F}(-) }{[1-(1- m_{F}(-))]^{4}},\] \[m_{F}(z) =\{_{n}(z)\}, m^{}_{F}(z)= \{_{n}^{2}(z)\},_{n}(z)=(_{n}-z_{p})^{-1},=.\]

As a result, we have \(()(0,1)\), which prevents the catastrophic skewness of \(F\)-distribution in high dimensions (see Figure 2) and yields smoother uncertainty scores. The ARHT can

Figure 1: Workflow of our proposed uncertainty estimation framework. Our framework contains a BNN encoding module and an uncertainty estimation module using high-dimensional testing. The training objective \(_{}\) can be either supervised (e.g., cross-entropy) or unsupervised (e.g., contrastive learning). The ELBO represents the evidence lower bound used for training BNN.

be interpreted as a more robust distance measure on embedding distributions compared to existing metrics such as Mahalanobis distance.

We select \(\) from a predefined set using a data-adaptive method . The set for grid search is chosen as \(\{_{0},5_{0},10_{0}\}\) given the hyperparameter \(_{0}\). For a testing data point \(x_{t}\), we compute \(Q(,;)=_{k=0}^{2}_{k}_{k}(-,) /\{_{2}(,)\}^{1/2}\), for each \(\) in the candidate set, where \(=(_{0},_{1},_{2})^{3}\) is a pre-specified weight vector set as \(=(0,1,0)\), \(_{0}(-,)=m_{F}(-)\), \(_{1}(-,)=_{1}(,)\), and \(_{2}(-,)=\{1+_{1}(,)\} \{p^{-1}\{_{n}\}-_{1}(-,)\}\). The optimal \(\) is then selected by maximizing the \(Q\) function, i.e., \(_{}Q(,;)\). For each testing data point, we can proceed with the above selection process to determine the optimal \(\) for the individual input. This makes the determination of uncertainty scores more flexible and data-adaptive.

### Optimal Threshold Adjusting for Family-Wise Discovery Error

We obtain the area under the receiver operating characteristic curve (AUROC) and the area under the precision-recall curve (AUPR) using different thresholds. To determine the optimal threshold, we adopt a family-wise testing procedure to compute the \(p\)-values.

It is necessary to balance the tradeoff between the power and the type I error rate of the test. We adopt the Benjamini-Hochberg (BH) procedure  procedure to adjust for multiple tests. The threshold of \(p\)-values is calibrated by the BH procedure, and we have a rejection set \(\) of indices to the samples whose \(p\)-values are below the threshold, \(=\{i:_{i}/(mH_{m})\}\), where \(=\{k:_{i} k(mH_{m})\}\), \(=\{1,,m\}\) is the set of indices corresponding to the \(m\) tests, and \(H_{m}=_{j=1}^{m}1/j\). The samples in the rejection set are then classified as the OOD sample. The BH procedure is shown to achieve a more powerful hypothesis testing performance .

## 4 Experiments

### Datasets and Experiment Setting

We design OOD detection tasks on both standard and medical image datasets to demonstrate the application of our framework. We also evaluate our framework on the image classification

    &  \\  &  &  &  \\
**Model** & **In-Distrib.** & **AUC** & **AUPR** & **AUC** & **AUPR** & **AUC** & **AUPR** \\ 
**MC Dropout ** & MNIST & 99.33 (0.3) & 92.77 (0.3) & 99.85 (0.09) & 99.88 (0.07) & 99.96 (0.007) & **99.96 (0.002)** \\
**Deep Ensembles ** & MNIST & 90.70 (8.4) & 91.08 (7.7) & 99.70 (8.4) & 91.08 (7.7) & 99.21 (0.9) & 99.68 (0.4) \\
**Kendall and Gal ** & MNIST & 92.54 (2.6) & 92.97 (1.9) & 94.11 (4.9) & 93.40 (5.7) & 99.60 (0.5) & 99.13 (1.0) \\
**EDI**,  & MNIST & 93.43 (16.0) & 80.22 (11.1) & 72.61 (8.6) & 81.42 (7.0) & 63.43 (1.2) & 85.09 (3.4) \\
**DPN**  & MNIST & 99.41 (0.2) & 99.37 (0.3) & 99.96 (0.03) & 99.96 (0.03) & 99.96 (0.01) & **99.96 (0.003)** \\
**PostNet ** & MNIST & 98.95 (0.4) & 94.70 (0.5) & — & — & — & — \\
**Detectron ** & MNIST & 75.57 (1.2) & 83.75 (29.9) & 95.17 (11.0) & 85.00 (30.0) & 79.22 (12.4) & 83.75 (37.3) \\
**BNN-ARBIT (Ours)** & MNIST & **99.51 (0.4)** & **99.47 (0.3)** & **99.98 (0.01)** & **99.98 (0.004)** & **99.97 (0.007)** & **99.96 (0.004)** \\ 
**MC Dropout ** & CIFAR10 & 76.23 (5.6) & 74.21 (5.3) & 77.15 (2.2) & 79.0 (1.9) & 78.09 (1.2) & 84.35 (1.1) \\
**Deep Ensembles ** & CIFAR10 & 71.25 (3.0) & 75.32 (2.3) & 86.77 (3.6) & 90.35 (3.1) & 76.15 (5.3) & 82.62 (13.1) \\
**Kendall and Gal ** & CIFAR10 & 77.41 (17.3) & 77.00 (18.2) & 89.08 (17.8) & 90.93 (14.8) & 67.40 (3.1) & 71.44 (10.1) \\
**EDL**  & CIFAR10 & 67.81 (12.1) & 71.81 (11.5) & 77.53 (14.4) & 80.50 (11.7) & 69.57 (4.7) & 83.74 (3.4) \\
**DPN**  & CIFAR10 & 57.54 (1.7) & 68.29 (3.4) & 62.34 (3.7) & 70.49 (8.0) & 57.48 (4.4) & 77.76 (6.2) \\
**PostNet ** & CIFAR10 & & & — & — & 76.04 (1.6) & 69.30 (1.7) \\
**Detectron ** & CIFAR10 & 76.46 (15.3) & 71.63 (21.5) & 76.99 (15.9) & 91.00 (24.4) & 76.01 (13.6) & 90.00 (22.9) \\
**BNN-ARBIT (Ours)** & CIFAR10 & **77.78 (5.0)** & **79.06 (6.8)** & **92.77 (1.8)** & **93.74 (1.0)** & **82.01 (1.2)** & **91.61 (0.3)** \\   

Table 1: The OOD detection performance (in %) of our method, BNN-ARHT, compared to various competitors, using the LeNet  architecture. Standard deviations are given in brackets.

Figure 2: Comparison of the density functions of \((0,1)\) and \(F\)-distribution under low (left) and high (right) dimensions.

task in comparison with the baseline methods to show that our framework does not sacrifice classification performance. For the image classification task, we benchmark the classification performance of the encoder trained on a holdout set of the in-distribution dataset (i.e., MNIST). For the OOD detection task, we treat CIFAR 10 and MNIST as the in-distribution datasets, and Fashion-MNIST, OMNIGLOT, and SVHN as the OOD datasets. To validate the advantage of BNN encoding on limited observations, we compose a medical image benchmark using samples from the Diabetes Retinopathy Detection (DRD) dataset , with samples shown in Figure 3. We treat healthy samples as in-distribution data and unhealthy samples as OOD data. Distinct from the settings in some existing works  which include OOD data in the training, we exclude the OOD data when training the feature encoder. We use the AUROC and AUPR as the evaluation metrics for OOD detection tasks, and adopt accuracy and the macro F1-score to evaluate the classification performance. Detailed definitions of the evaluation metrics and further descriptions of the datasets can be found in the appendix.

### Competitive Methods

We compare our proposed framework, named as BNN-ARHT, with seven competitors -- **(1) Deep ensembles**: an ensemble of neural networks to address the epistemic uncertainties, and the number of models in the ensemble is set as 5; **(2) MC Dropout**: a non-Bayesian method using dropout on trained weights to produce Monte Carlo weight samples ; **(3) Kendall and Gal **: the first work addressing the aleatoric and epistemic uncertainties in deep learning; **(4) EDL **: it estimates uncertainty by collecting evidence from outputs of neural network classifiers by assuming a Dirichlet distribution on the class probabilities; **(5) DPN **: it assumes a prior network with Dirichlet distributions on the classification outputs; **(6) PostNet **: it uses normalizing flow to predict an individual closed-form posterior distribution over predicted class probabilities; **(7) Detectron **: it detects the change in distribution with the discordance between an ensemble of classifiers trained to agree on training data and disagree on testing data. Since Detectron operates on small samples from each dataset for OOD detection (e.g., 50 over 60,000 for CIFAR 10), we use a larger number of runs (i.e., 100) for a fair comparison. Five-fold cross-validation is applied to each of the competitive methods. We report the means and standard deviations over the runs for each metric.

### Predictive and Uncertainty Estimation Performance

**Verify Uncertainty Quality by OOD Detection.** Tables 1 and 2 present the OOD detection results of our method compared with competitors on different pairs of datasets. We observe that existing methods, which assume having seen OOD data in the training, perform poorly in our settings, especially when the training observations are limited. Detectron  yields larger standard errors than other methods since it operates on small samples of the datasets. This validates the argument that existing methods heavily rely upon the availability of OOD data during the training. We also observe that our framework outperforms all the baselines on almost all OOD detection tasks, which demonstrates its satisfactory performance. In particular, our method shows a great improvement on the DRD dataset in which the number of samples is small, indicating the advantage of using a BNN encoder. As the MNIST dataset has dense feature representations, the OOD features can be easily distinguished. Hence, the OOD performance of the methods is relatively better on MNIST than on CIFAR10.

  
**Model** & **AUROC** & **AUPR** \\ 
**MC Dropout ** & 59.52 (1.1) & 60.95 (6.0) \\
**Kendall and Gal ** & 91.06 (9.8) & 92.71 (7.8) \\
**Deep Ensembles ** & 59.67 (1.3) & 56.58 (2.3) \\
**DPN ** & 60.57 (1.1) & 65.32 (1.3) \\
**EDL ** & 53.01 (1.9) & 58.22 (9.6) \\
**Detectron ** & 90.74 (9.9) & 46.00 (31.4) \\
**BNN-ARHT (Ours)** & **93.44 (3.8)** & **95.44 (2.2)** \\   

Table 2: The OOD detection performance (in %) on DRD , with the LeNet  architecture.

  
**Model** & **Accuracy** & **F-1 Score** \\ 
**MC Dropout ** & 98.48 & 98.89 \\
**Kendall and Gal ** & 98.78 & 98.65 \\
**Deep Ensembles ** & 97.90 & 97.89 \\
**DPN ** & 98.89 & 98.83 \\
**EDL ** & 21.24 & 12.88 \\
**PostNet ** & 99.12 & **99.06** \\
**BNN-ARHT (Ours)** & **99.26** & **99.06** \\   

Table 3: Classification performance (in %) on MNIST, with the LeNet  architecture.

Figure 3: Healthy (left) and unhealthy (right) samples of the DRD dataset.

Predictive Performance Is Preserved.

As shown in Table 3, we also perform image classification to demonstrate the benefits of the post-hoc method (i.e., without modifying the original objective). We fix the neural network architecture as LeNet. For Kendall and Gal  and our method, we use the Bayesian counterpart of LeNet to perform the experiment. We observe that without modifying the original learning objective, the predictive performance of the encoder can be preserved and outperforms other methods. The BNN may underperform its frequentist counterpart due to the introduction of the KL regularization. Hence, in practice, a (pre-trained) frequentist encoder can be used to replace the BNN encoder if predictive performance is the focus. Section 5 shows the sacrifice in the OOD detection performance if a frequentist architecture is used.

Visualization of Uncertainty Scores.To better understand the uncertainty scores of in-distribution and OOD data under different uncertainty measures, Figure 4 presents the distributions of the ARHT uncertainty scores of in-distribution data (MNIST) and OOD data (Fashion-MNIST), respectively. We observe that the distributions of ARHT are of different shapes for in-distribution data and OOD data. This demonstrates the effectiveness of ARHT in identifying the unique characteristics of the distributions of the datasets.

## 5 Ablation Studies

**Different Neural Network Architectures.** We compare the performance with encoders under different architectures. We choose the standard CNN used in Malinin and Gales , LeNet , Alexnet , and ResNet18  as the SOTA examples in encoding image features. For ResNet18, as the variance increases drastically with the increasing depth of the architecture, it is not feasible to replace all the layers with their Bayesian counterparts. Therefore, we replace only the last fully-connected layer and the second last convolutional layer with their Bayesian versions. Table 4 presents the comparison of OOD detection of our method with the competitors under different neural network architectures. We observe that our method is able to obtain satisfactory performance over the competitors when the neural network architecture changes.

**Effects of Key Hyperparameters.** We evaluate our method with a range of hyperparameters to assess their impacts on our method. Figure 5 presents the OOD detection performance with different values of \(_{0},n_{2}\), and \(p\). **(1) Initial loading value \(_{0}\):** we observe that the performance of our model is robust to changes in \(_{0}\). Since the ARHT relaxes the Gaussian assumption of embeddings, the change in the magnitude of the loading matrix would not heavily affect the covariance structure of the testing embeddings. Hence we can safely load \(\) to the covariance matrix to resolve the singularity problem, with no concern about the decrease in performance; **(2) Number of training weight samples \(n_{1}\):**

    & **CNN** & **LeNet** & **AlexNet** & **ResNet** \\
**\# of parameters (Freq)** & **31,340** & **62,006** & **2,472,266** & **11,699,522** \\
**\# of parameters (Bayes)** & **62,700** & **125,112** & **4,922,120** & **12,372,904** \\ 
**Model** & **AUROC** & **AUPR** & **AUROC** & **AUPR** & **AUROC** & **AUPR** \\ 
**MC Dropout ** & 63.85 & 74.79 & 68.58 & 78.53 & 74.72 & 82.57 & 61.46 & 76.04 \\
**Deep Ensembles ** & 78.44 & 89.10 & 61.01 & 62.69 & 72.14 & 59.83 & 58.11 & 78.26 \\
**Kendall and Gal ** & 67.77 & 80.87 & 55.36 & 69.82 & 70.88 & 73.15 & 50.01 & 83.86 \\
**EDL ** & 65.26 & 67.07 & 66.53 & 67.12 & 65.81 & 61.68 & 51.34 & 73.50 \\
**DPN ** & 63.98 & 77.30 & 57.44 & 73.84 & 67.10 & 80.75 & 76.36 & 86.46 \\
**PostNet ** & 73.68 & 66.85 & 76.04 & 69.30 & 81.67 & 76.52 & 82.19 & 79.43 \\
**Detector ** & 73.84 & 88.50 & 76.01 & 90.00 & 80.58 & **85.00** & 78.27 & 89.50 \\
**BNN-ARHT (Ours)** & **85.36** & **89.37** & **82.01** & **91.61** & **82.10** & 70.04 & **88.16** & **93.19** \\   

Table 4: The OOD detection performance (in %) of competitive methods under various model architectures . CNN refers to the two-layer standard CNN architecture used by Malinin and Gales . We use CIFAR10 as the in-distribution dataset and SVHN as the OOD dataset. ‘Freq’ represents the frequentist architecture and ‘Bayes’ represents the Bayesian architecture.

Figure 4: Distributions of the ARHT uncertainty scores for the in-distribution data (MNIST) and OOD data (Fashion-MNIST), respectively.

The size \(n_{1}\) is controlled by the hyperparameter \(s\) in our framework. We have conducted experiments with \(s\) ranging from 1 to 5 (Figure 6). The pattern shows that the performance decreases when \(s\) increases (i.e., more embedding samples from the in-distribution dataset). This demonstrates that the covariance structure affects ARHT more as \(s\) increases, and the contribution of testing embeddings is less weighed, which leads to slightly decreasing performance. **(3) Number of testing weight samples \(n_{2}\):** The number of testing weight samples \(n_{2}\) is a key to approximate the testing embedding distributions. Since the approximation of the testing distribution is crucial to the performance of our method, we tune the hyper-parameter \(n_{2}\) to determine the optimal number of testing embeddings to choose for each task. We observe that even if the number of testing samples is small (e.g., 5), ARHT can still capture the distributional difference between the in-distribution and OOD data. This leads to a consistent performance as \(n_{2}\) varies. **(4) Embedding dimension \(p\):** we also evaluate the OOD detection performance of our method with respect to the change in embedding dimensions (\(p=8,16,32,64,128\)). We observe that our method is robust to changes in feature dimensions. This enables our framework to perform OOD detection on the representation level with customized embedding dimensions, relaxing the constraint on classification problems.

**Feature Learning Objectives.** One key component of our framework is the BNN encoder, which is trained by supervision (i.e., cross-entropy loss for image classification) on standard datasets. We evaluate the robustness of our method when the encoder is trained with different objectives (i.e., unsupervised contrastive learning loss). We select the margin of contrastive learning loss as 0.2. The performance in AUROC on the OOD detection task decreases slightly from 99.98 to 98.42 (for MNIST vs. OMNIGLOT OOD detection). This shows that our framework is sensitive to the quality of the encoder, and a supervised learning objective is preferred to improve the encoding performance.

## 6 Discussion: Impacts and Limitations of BNN and ARHT

We discuss why a BNN encoder is preferred for our framework, with Figure 7 illustrating the difference between features generated by BNN and frequentist DNN. A frequentist DNN gives one feature embedding to every input data point, and the uncertainty estimate ignores the covariance structure of the distribution because only the point estimate is provided. However, a BNN estimates the posterior embedding distribution for every data point, and the covariance structure can be incorporated to obtain a more accurate uncertainty estimate. Although some recent non-Bayesian method [32; 27] places parametric distributions on posterior embeddings (e.g., Dirichlet distribution on class probabilities), these parametric distributions are less accurate in approximating the posterior distributions than BNNs because the strong parametric assumption limits their capabilities in searching the candidate distributions.

To demonstrate why ARHT is preferred as an uncertainty metric, we further evaluate its performance over an array of uncertainty measures, including the maximum probability, entropy, and

Figure 5: Performance of our method with different values of \(_{0}\) (left), \(n_{2}\) (middle), and \(p\) (right) by fixing the network architecture as LeNet . We use MNIST as the in-distribution dataset and Fashion-MNIST as the OOD dataset.

Figure 6: Ablation study with respect to \(s\) (In-distribution: CIFAR 10, OOD: SVHN).

differential entropy proposed by Malinin and Gales . Detailed definitions of these metrics are presented in the appendix. We also include the frequently used Mahalanobis distance which possesses stronger assumptions on the Gaussianity of the embeddings. We fix the model architecture to be LeNet and compare its Bayesian and frequentist designs. Table 5 presents the summary of the comparison. We observe that using ARHT as the uncertainty score can achieve the best OOD detection performance than existing uncertainty measures when a BNN encoder is used.

Our method achieves a better performance using features generated by BNNs than those generated by frequentist counterparts, which demonstrates the advantage of using Bayesian encoders in our framework. However, BNN encoders require drawing \(n_{2}\) samples of weights for posterior inference, which requires higher time complexity than the frequentist counterparts. Further, BNNs are limited by the scalability constraints , which makes them difficult to have deeper structures. How to make BNNs deep remains a challenging topic .

Support of Single-Sample Uncertainty.As the training set (as the in-distribution set) is available (at least for training or fine-tuning the encoder) in most of the problems, one can use samples from training sets and the testing samples to compute ARHT. One exception is the zero-shot case where we only have the pre-trained encoder but no original data (i.e., in-distribution samples). In this case, most of the uncertainty estimation methods cannot work since they require at least in-distribution data to fit their parametric assumptions (e.g., concentration rates of the Dirichlet distributions in classification problems). However, one may still obtain ARHT as an uncertainty estimate using methods to reconstruct/generate pseudo-training data from the pre-trained models, which is not the focus of our work but warrants future research.

## 7 Related Works

Uncertainty Estimation.Estimating uncertainty in neural networks has become an increasingly important topic in deep learning. Existing uncertainty estimation methods can be divided into two classes: Bayesian methods  and non-Bayesian methods . Bayesian methods address the model-wise (i.e., epistemic) uncertainties by learning a posterior distribution of weights. The uncertainties in neural networks can be approximated by the predictions given by the weights sampled from the posterior distributions. Despite the success of these methods, most works only focus on classification uncertainties and rely upon arbitrary distributional assumptions on the class probabilities. For instance, DPN  assumes that the classification probabilities follow a Dirichlet distribution and train the OOD detector based on the KL divergence of the prior and posterior Dirichlet distributions. These methods are not generalizable to tasks other than classification. Furthermore, most existing methods  assume that the samples from the target domain are available when training the OOD detector, which is unrealistic in most applications.

Hypothesis Testing in High Dimension.The task of uncertainty estimation can be redefined as a high-dimensional hypothesis testing problem. We report the detection of OOD samples if we reject the null hypothesis at significance level \(\). The Hotelling \(T^{2}\) test  in Eq. (2) assumes \(T F(p,n-p)\) under the null hypothesis. The unnormalized version of \(T\) in Eq. (2) is known

    &  &  \\
**Model** & **AUROC** & **AUPR** & **AUROC** & **AUPR** \\  Maximum & 77.50 & 64.69 & 81.30 & 83.11 \\ Probability &  &  &  \\  Entropy & **80.22** & **88.47** & 82.49 & 78.14 \\  Differential Entropy & 79.59 & 86.98 & 66.15 & 75.27 \\  RHT & 79.06 & 62.82 & 82.29 & 64.07 \\ 
**ARHT** & 79.07 & 63.44 & **82.64** & **91.69** \\   

Table 5: Performance of the ARHT against other uncertainty measures, with the LeNet  architecture, the in-distribution dataset CIFAR10, and the OOD dataset SVHN.

Figure 7: Comparison of features generated by a frequentist DNN and those by a BNN. The uncertainty estimated by a frequentist DNN ignores the covariance structure of the posterior distribution, where a BNN provides a distributional estimate for each testing sample.

as Mahalanobis distance. However, the Hotelling test statistic suffers from poor robustness and consistency when \(n\) and \(p\) are large and even becomes undefined when \(p>n\) or when \(_{n}\) is singular . This results in all test statistics (as uncertainty scores) concentrating on the singular point leading to trivial estimation of uncertainties. Figure 2 presents an example of comparisons of \(F(10,10)\) and \(F(1000,1000)\) to \((0,1)\). Chen et al.  attempt to overcome this issue by proposing the RHT statistic, which resolves the singularity issue of the covariance matrix but yet the inconsistency and poor performance of Hotelling's \(T^{2}\) statistic under the regime \(p/n\). Li et al.  propose adaptable RHT and design an adaptive selection procedure for the loading parameter \(\) based on the work of Chen et al. , which resolves the inconsistency of Hotelling's \(T^{2}\).

**Bayesian Deep Learning.** SOTA DNN architectures [33; 14; 34] demonstrate significant success in tasks from different domains. Their designs enable them to mine high-dimensional features into low-dimensional representations and address the aleatoric uncertainties. Despite the success, the DNNs typically only yield maximum likelihood estimates of weights under the frequentist framework and cannot address epistemic uncertainties . Existing works of BNN assign prior distributions to the neural network weights so as to obtain the posterior distributions of weights given the observed data [36; 11; 29; 17]. This enables BNN to provide a more accurate approximation to the target distribution and address epistemic uncertainties. Particularly when the training observations are limited, using a BNN can prevent overfitting and generate more representative feature distributions.

## 8 Conclusion

We propose a novel uncertainty estimation framework by introducing high-dimensional hypothesis testing to feature representations. We introduce the ARHT as the uncertainty measure which is adaptable to individual data point and robust compared to existing uncertainty measures. Our proposed uncertainty measure operates on latent features and hence can be generalized to any other tasks beyond image classification (e.g., regression or feature representation learning). Empirical evaluations on OOD detection and image classification tasks validate the satisfactory performance of our method over the SOTAs. Ablation studies on key components of the proposed framework validate the robustness and generalizability of our method to variations. One of the best potential applications of our framework is continual learning, where the proposed method can accurately measure the uncertainty as the domain shifts when encountering non-stationary environments. Our framework can be potentially applied to various settings where distributional shifts and OOD detection are vital, such as medical imaging, computational histopathology, and reinforcement learning.

**Acknowledgement.** We thank the anonymous reviewers, the area chair, and the program chair for their insightful comments on our manuscript. This work was partially supported by the Research Grants Council of Hong Kong (17308321), the Theme-based Research Scheme (T45-401/22-N), and the National Natural Science Fund (62201483).