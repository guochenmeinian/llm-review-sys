# Repetition In Repetition Out: Towards Understanding Neural Text Degeneration from the Data Perspective

Repetition In Repetition Out: Towards Understanding Neural Text Degeneration from the Data Perspective

 Huayang Li\({}^{}\)   Tian Lan\({}^{}\)   Zihao Fu\({}^{}\)   Deng Cai\({}^{}\)

Lemao Liu\({}^{}\)   Nigel Collier\({}^{}\)   Taro Watanabe\({}^{}\)   Yixuan Su\({}^{,}\)

\({}^{}\)Nara Institute of Science and Technology  \({}^{}\)Tencent AI Lab

\({}^{}\)University of Cambridge  \({}^{}\)Cohere

{li.huayang.lh6, taro}@is.naist.jp lantiangmftby@gmail.com {jcykcai, redmondliu}@tencent.com{zf268, nhc30, ys484}@cam.ac.uk

###### Abstract

There are a number of diverging hypotheses about the neural text degeneration problem, i.e., generating repetitive and dull loops, which makes this problem both interesting and confusing. In this work, we aim to advance our understanding by presenting a straightforward and fundamental explanation from the data perspective. Our preliminary investigation reveals a strong correlation between the degeneration issue and the presence of repetitions in training data. Subsequent experiments also demonstrate that by selectively dropping out the attention to repetitive words in training data, degeneration can be significantly minimized. Furthermore, our empirical analysis illustrates that prior works addressing the degeneration issue from various standpoints, such as the high-inflow words, the likelihood objective, and the self-reinforcement phenomenon, can be interpreted by one simple explanation. That is, penalizing the repetitions in training data is a common and fundamental factor for their effectiveness. Moreover, our experiments reveal that penalizing the repetitions in training data remains critical even when considering larger model sizes and instruction tuning. Our code is available at https://github.com/gmftbyQMFTBV/Rep-Dropout.

## 1 Introduction

The emergence of neural language models (LM) has led to significant achievements in various text generation tasks, such as machine translation , summarization , and open-ended text generation . However, in open-ended text generation, neural LMs exhibit a strikingly severe degeneration issue, producing unreasonably repetitive texts, particularly when employing maximum _a posteriori_ (MAP) decoding algorithms. As illustrated in Fig. 1, even a well-trained LM  may suffer from a severe degeneration issue.

There have been numerous attempts to explain the phenomenon of neural text degeneration, with many attributing this problem to flaws in the learning process. Fu et al.  claim that high-inflow words increase the probability of generating repetitions. A collection of studies  argue that the likelihood objective is the primary factor, because it has the problem of exposure bias and focuses more on next-token prediction rather than sequence generation. Meanwhile, both Chiang and Chen  and Xu et al.  assert that the self-reinforcement mechanism can make neural LMs fall into the repetitive loops. Intriguingly, despite the divergence in these explanations, all corresponding methods proposed have been shown to effectively alleviate the text degeneration issue. This observation leads us to ponder: _could there exist more fundamental factors that can explain the degeneration issue?_

In this study, we strive to provide a straightforward and fundamental interpretation for the degeneration problem from the data perspective. Our preliminary investigation reveals that repetitive words in the training data play a crucial role in the issue. It is also worth noting that repetitions are not necessarily of low quality, because it is a natural and common phenomenon in human writing [1; 12; 28]. Across five datasets with different domains, we observe a strong correlation between repetition rates in training and generated text. This finding encourages us to further assess the impact of repetitive words in training data by randomly dropping out attention to them during training. Employing repetition dropout, we discover that the repetition rate in generated text can be substantially reduced. Lastly, we reconcile many previous hypotheses with our single explanation, asserting that penalizing repetitions in training data is a key factor to the success of alleviating the degeneration issue.

As large language models (LLMs) gain popularity, it appears that the degeneration issue has been somewhat solved. We investigate the impact of various factors associated with LLMs on degeneration, including increasing model size and training models using instruction-tuning data. Our experiments reveal that penalizing repetitions in training data continues to be crucial in the context of LLMs.

Our contributions are threefold:

* We demonstrate that the proportion of repetitive words in training data has a significant influence on the degeneration issue. Inspired by this finding, we also propose a method, namely, _repetition dropout_, to mitigate the degeneration.
* We find that penalizing repetitions in data is a more fundamental factor for reducing repetitions, which also provides a unified explanation for various existing hypotheses, such as the attributions to high-inflow words, likelihood objectives, and self-reinforcement phenomenon.
* We investigate the influence of factors associated with large language models on reducing repetitions, including model scale and instruction tuning.

## 2 Related Work

The degeneration (or repetition) issue in neural language models (LM), particularly in open-ended text generation, has garnered significant attention in recent years. Previous works have proposed disparate interpretations and solutions for this issue.

Many researchers hypothesize that factors within the learning process contributes to the degeneration issue. Fu et al.  assert that the high-inflow words in training data may increase the probability of generating repetitions, where the inflow of a word is defined as the probability sum of all its preceding words. Another factor identified is the likelihood objective. Welleck et al.  argue that likelihood objective has a discrepancy with the MAP decoding and focuses more on next-token prediction rather than sequence generation. Thus, they proposed an unlikelihood objective to address the two flaws. Based on the same principle, Lin et al.  propose to scale the gradient of specified non-novel tokens, i.e., words in the prefix context at each time step, to alleviate the degeneration issue. Su et al.  and Jiang et al.  pointed out that the degeneration issue may caused by the high similarity between token representations. Thus, they leveraged contrastive learning to learn a more distinct

Figure 1: Illustration of repetitions in human and generated text. The human text is from Wikitext-103, and generated text is by greedy search using the GPT-2 model trained on Wikitext-103. The underlined text is the prompt for generation. The blue words and red words indicate the repetitions in human text and machine-generated text, respectively.

representation for each token. In addition, both Xu et al.  and Chiang and Chen  argue that degeneration is caused by the self-reinforcement phenomenon and Xu et al.  address this issue by penalizing the repetitions in pseudo repetitive data.

Apart from issues within the learning process, other explanations for degeneration have been proposed. Many researchers contend that decoding methods [7; 10; 16; 20; 37] is the primary factor. Holtzman et al.  argue that word probabilities in human-generated text exhibit high variance and randomness, while high-probability text produced by MAP decoding methods tends to be repetitive and dull. This observation explains why sampling-based decoding methods [7; 10; 16; 20] can substantially mitigate the degeneration issue. Riley and Chiang  find that tasks with lower constraints, or larger solution spaces, suffer from more severe degeneration issue. The model architecture [8; 32; 34] and size  may also contribute, but the two factors have not been quantitatively evaluated.

However, with so many explanations, understanding the primary cause of the degeneration issue becomes increasingly challenging. In our work, we strive to provide a fundamental explanation for the previous hypotheses in the learning process. While our work may not encompass all the existing explanations, we believe it lays a solid foundation for further understanding of the degeneration issue.

## 3 Background

Language model (LM) aims to estimate the probability of a sentence in natural language according to the chain rule of probability:

\[P() =P(x_{1})P(x_{2}|x_{1}) P(x_{L}|_{1:L-1})\] \[=P(x_{1})_{i=2}^{L}P(x_{i}|_{1:i-1}),\] (1)

where \(=,,x_{L}$}\) is a sequence of words with length \(L\), and \(_{1:i-1}\) is the previous \(i-1\) words, i.e., the context, for predicting word \(x_{i}\).

ModelSince attention-based LM [20; 31] has became the backbone of many tasks, we will use GPT-2 model  as the main architecture for our empirical studies. The core of GPT-2 model1 is to use the attention mechanism to update the representation of words in \(\):

\[(,,)= ^{T}+}{},\] (2)

where \(\) is a scalar to control the scale of the attention score, \(\), \(\), and \(\) are the linear transformation of \(_{1:L}^{L d}\) using matrices \(_{q}\), \(_{k}\), and \(_{v}^{d d}\), respectively. \(_{1:L}\) is the current hidden representation of words \(_{1:L}\), and \(d\) is the hidden size. \(\) is masking matrix that makes sure only the information of \(_{1:i}\) is accessible for \(_{i}\) at each time step \(i\). If the word \(x_{i}\) can perceive the information of \(x_{j}\), then \([i][j]\) equals to \(0\), otherwise \(-\).

TrainingGenerally, neural LMs are trained by optimizing the likelihood objective:

\[ =-_{}_{i=2}^{L} P(x_{i}|_{1 :i-1};)\] \[=-_{}_{i=2}^{L}( (_{i-1}))[x_{i}],\] (3)

where \(_{i-1}^{d}\) is the hidden vector of \(x_{i-1}\) output by the last layer of an neural LM, e.g., the GPT-2 model . The \([x_{i}]\) is defined as taking the probability regarding to \(x_{i}\) in the distribution got from \(\). The \(()\) is a linear layer that transforms the \(_{i-1}\) to logits.

InferenceSince neural LMs are trained to maximize the likelihood objective, one intuitive practice for inference is to use the MAP decoding method, e.g., greedy search or beam search. However, in open-ended text generation, this tactic will cause an extremely severe degeneration issue on vanilla neural LMs trained by likelihood objective .

EvaluationThe main focus of this empirical study is to investigate the reason for the repetition issue. Therefore, the rep-\(n\) is an important evaluation metric in our work, following previous works [8; 25; 27; 34]:

\[n=1.0-(,n)|}{L-n+1}\] (4)

where \(n\) is the length of \(n\)-gram, and \(\) is a function to find all unique \(n\)-grams in a sentence \(\). For the corpus-level evaluation, we report the averaged rep-\(n\) scores of instances in the dataset. To ensure that our results in main experiments are not biased to rep-\(n\), we also report the results of rep-\(w\) and rep-\(r\) in previous works [8; 34]. The rep-\(w=_{t=1}^{L}\{x_{t}_{t-w-1:t-1}\}\), which measures the word-level repetition in a prefix window with length \(w\). In our experiments, we set \(w\) to 16, following Fu et al. . The rep-\(r=|\{i|(x_{i}=x_{j} x_{i+1}=x_{j+1}, j i)(x_{i} =x_{k} x_{i-1}=x_{k-1}, k i)\}|\). It is for the portion of repetitive snippets measured by sentence length.

In addition to the measurement of repetition, we also consider the perplexity (PPL) on the real data, which demonstrates the performance in language modeling . Although LMs with lower PPL may not consistently lead to better generation results, it is able to reflect trivial solutions for the degeneration issue, e.g., random or over-fitting models.

## 4 Preliminary Study: Rethinking Neural Text Degeneration from Data Perspective

The propensity of neural LMs with impressive performance to fall into naive repetitive loops is puzzling. Although numerous hypotheses have been proposed, many of them approach this issue from divergent aspects, and some are not intuitive for understanding. According to Ockham's Razor, a simpler explanation is often preferable. Therefore, in our preliminary study, we start by evaluating one elementary factor for most AI systems, the training data. Concretely, repetition is a natural and common phenomenon in human languages for various reasons [1; 12; 28]. It is intriguing to investigate whether there are connections between valid repetitions in natural language and incorrect repetitions in generated language. Moreover, it is important to note that data containing repetitions is not necessarily of low quality, as shown in Fig. 1.

Figure 2: (a) Relationship between rep-\(2\) scores of human and generated text. (b) Relationship between the scale of an LM and the rep-\(2\) score of generated text. Note that results with the same symbol in Fig. 2 are from models trained on different shards of the corresponding dataset. The rep-\(2\) score is defined in Eq. (4). We use the GPT-\(2\) and OPT LMs for Fig. 2 and 2, respectively, and use greedy search as the decoding method.

SetupTo assess the correlation between repetitions in generated text and those in human text, we propose to train GPT-2 models  on data with varying rep-\(2\) scores and then evaluate the rep-\(2\) scores of the text generated by the corresponding models. Specifically, we sorted the training instances in each dataset \(\) based on their rep-\(2\) scores. We then divided the sorted training data into six shards, each containing an equal number of words but a varying percentage of repetitions, and trained a GPT-2 model on each shard. Notably, we will use the full test set of a dataset \(\) to evaluate the models trained on different shards of \(\). More implementation details are in Appx. A.

Our preliminary study investigates five datasets across various domains. Wikitext-103 is a widely used dataset for language modeling  and open-ended generation . We adopt the standard split for training, validation, and test sets. The remaining four datasets, OpenWebText2, FreeLaw, PubMed, and ArXiv, are part of the Pile dataset . To ensure consistent analysis, we sample an equivalent number of words as the Wikitext-103 dataset from each of the four Pile datasets. For the validation and test sets, we randomly sample 2,000 sentences from each of the four Pile datasets. The training, validation, and test sets are non-overlapping across all five datasets.

FindingsFig. 2(a) demonstrates a strong correlation between the rep-2 scores of human and generated text on each dataset, indicating that the degeneration issue becomes more severe as the percentage of repetitions in training data increases. However, due to varying data distributions across datasets from different domains, the rep-2 score of generated text may vary even when trained on data with the same rep-2 score. Another intriguing observation is that neural LM will amplify the repetition in data by more than 10 times, similar to the bias amplification found in Zhao et al. .

Investigations Related to Large Language ModelsAs LLMs, e.g., ChatGPT  and Llama, gain popularity, it appears that the degeneration issue has been somewhat solved. In this section, we will investigate the impact of various factors associated with LLMs on degeneration, including increasing model size and training models with instruction-tuning data.

Many amazing model abilities emerge when scaling up the model and data size . An interesting question is, _whether the degeneration issue will be solved by simply scaling up_? We use a set of OPT models  with different model sizes to investigate this question. As shown in Fig. 2(b), the rep-2 score of generated text sharply drops before increasing the model size to 6.7 billion parameters, indicating that increasing the model size does alleviate the repetition issue to some extent. However, the gains achieved by increasing the model size diminish over time. The OPT-66B model still generates text with high rep-2 score. This observation shows that increasing the model size is not an efficient way to alleviate the degeneration.

Degeneration in certain instruction-tuned LLMs, such as ChatGPT , is relatively rare, leading to the hypothesis that the instruction-tuning phase, which trains LLMs on instruction-response pairs, could alleviate this issue. This conjecture implies that the utilization of instruction-tuning data is vital for mitigating degeneration. To explore this, we fine-tune the Llama2 model  using three instruction-tuning datasets: Alpaca, Alpaca + WT-103 50K, and WT-103 50K, as demonstrated in Table 1. The rep-2 scores for these datasets are 5.54, 9.67, and 10.31, respectively. Our experiments reveal that Llama2 fine-tuned on Alpaca exhibits less repetitions, while training on WT-103 50K and Alpaca + WT-103 50k still displays significant degeneration. This finding is consistent with our prior observations, where repetition issues strongly correlate with the presence of repetitions in the training data. It suggests that the low repetition rate in instruction-tuning data may contribute to the decreased degeneration.

   \# & **Model** & **Rep-2** & **Rep-3** (\%) & **Rep-4** (\%) \\ 
1 & Llama2 w/o FT & – & \(47.79\) & \(41.97\) & \(38.52\) \\
2 & FT Llama2 on Alpaca & \(5.54\) & \(15.08\) & \(10.91\) & \(8.93\) \\
3 & FT Llama2 on Alpaca + WT-103 50K & \(9.67\) & \(41.63\) & \(35.64\) & 32.29 \\
4 & FT Llama2 on WT-103 50K & \(10.31\) & \(54.10\) & \(49.77\) & \(36.80\) \\   

Table 1: Results of Llama2-7B on instruction-tuning data. The “FT” means fine-tuning. The column “Rep-2 of FT Data” indicates the rep-2 score of the training data. The rest Rep-\(n\) scores are evaluated on the generated text. The Alpaca is the instruction-tuning dataset used in , “WT-103 50K” is the instruction-tuning dataset we constructed based on Wikitext-103 (Appx. A.2), and “Alpaca + WT-103 50K” is the mixture of both.

[MISSING_PAGE_FAIL:6]

The main baseline method is MLE, which indicates the vanilla GPT-2 model trained by likelihood objective, as defined in Eq. (3). The + Rep-Dropout is the proposed method, i.e., repetition dropout. We also have several baseline methods. The first one is the + Rand-Dropout, which applies dropout on random tokens instead of repetitive tokens. Note that the number of random tokens selected for dropout for this baseline is constrained to the same as the repetitive tokens. We also compare with three previous works that also conducted experiments on Wikitext-103: re-encoding of high-inflow tokens (HI-Re) , training with scaled gradient (ScaleGrad) , and unlikelihood objective at token level (UL) . During inference we use greedy search as the decoding method to generate 128 tokens, using the first 32 tokens of each line in the test set as the prompt. More details about those baseline methods are also shown in the Appx. A.

### Main Results

As shown in Tab. 2, we evaluate the baseline methods and our method on three datasets. On the Wikitext-103 dataset, both the MLE and MLE + Rand-Dropout methods suffer from the severe degeneration issue. More than 33% percent \(4\)-grams in the generated sentences of the two baseline methods are repetitive, which is different from the patterns in natural language. Nevertheless, MLE + Rep-Dropout can significantly reduce the repetition issue in generated sentences. This observation indicates that repetition in the training data is crucial for the degeneration issue. Compared with methods in previous works in Tab. 2, i.e., HI-Re, ScaleGrad, and UL, the Rep-Dropout also show better performance on alleviating the degeneration issue. The results on FreeLaw and OpenWebText2 datasets are consistent with those on Wikitext-103. We also conduct quantitative and qualitative experiments to evaluate + Rep-Dropout on larger LMs, e.g., GPT-XL, which are shown in Appx. B and C, respectively.

  
**Model** & **Rep-\(2\)(\%)** & **Rep-\(3\)(\%)** & **Rep-\(4\)(\%)** & **Rep-\(w\) (\%)** & **Rep-\(r\) (\%)** & **PPL** \\    \\   Hi-Re & 41.91 & 33.82 & 28.35 & 38.60 & 66.22 & — \\ ScaleGrad & 12.49 & 6.85 & 4.44 & 18.31 & 28.98 & 24.72 \\ UL & 36.77 & 28.22 & 22.88 & 39.13 & 61.89 & 21.93 \\ MLE & 47.05 & 38.46 & 32.64 & 46.42 & 72.59 & **21.98** \\ + Rand-Dropout & **38.33** & 28.84 & 22.66 & 37.76 & 66.19 & 23.50 \\ +Rep-Dropout & **9.78** & **4.34** & **2.14** & **22.56** & **25.45** & 28.26 \\ Human & 3.56 & 0.84 & 0.28 & 10.64 & 5.82 & — \\    \\   MLE & 51.74 & 46.19 & 42.22 & 39.22 & 73.06 & **16.13** \\ + Rand-Dropout & 38.82 & 32.19 & 27.78 & 31.31 & 61.30 & 18.85 \\ +Rep-Dropout & **10.15** & **5.60** & **3.49** & **17.55** & **23.21** & 20.68 \\ Human & 2.77 & 0.89 & 0.50 & 10.61 & 8.10 & – \\    \\   MLE & 73.96 & 70.61 & 67.91 & 67.28 & 88.27 & **80.37** \\ + Rand-Dropout & 63.43 & 57.16 & 52.30 & 58.92 & 82.76 & 91.75 \\ +Rep-Dropout & **25.24** & **16.14** & **11.10** & **34.73** & **49.80** & 107.02 \\ Human & 4.53 & 1.39 & 0.61 & 12.41 & 13.09 & – \\   

Table 2: Performances of language models on three datasets. Both Rand-Dropout and Rep-Dropout use a dropout rate \(0.6\). Rep-\(n\) is defined in Eq. (4). The decoding method for all models is greedy search. PPL is the perplexity score on the real test data. Note that we do not report the PPL for HI-Re, because its vocabulary is different from other baselines.

Figure 3: Rep-\(2\) score and perplexity of MLE + Rep-Dropout on the test set of Wikitext-103. The brown dash line is the human-level rep-\(2\) score.

We also evaluate the effect of MLE + Rep-Dropout with different dropout rates for the repetitions in Wikitext-103, which can be regarded as controlling the percentage of repetitions in a dataset. As shown in Fig. 3, increasing the dropout rate of our method will reduce the number of repetitions in generated sentences continuously. Moreover, we observe a clear trade-off between the rep-\(2\) score of generated sentences and the perplexity on the real data in test set. This suggests that learning on the repetitions in training data can be beneficial for language modeling. Consistent results are also shown on FreeLaw and OpenWebText2 datasets.

### Relation to Previous Hypotheses

Previous research has proposed various hypotheses and approaches to understanding and solving the problem of degeneration in neural text generation. However, we argue that many of these proposals can be explained by a simple explanation. That is, penalizing repetitions in data is a common and fundamental factor for their success.

The core idea of many previous works is to penalize a specific set of data, e.g., all the prefix words \(_{1:t-1}\) at timestep \(t\), to alleviate the degeneration. However, as shown in Fig. 4, many of them have implicit connections with the repetitions in data. For example, the set of high-inflow words (\(\!\!>\)) penalized in Fu et al.  has a noticeable interaction with that of repetitive words (\(\!\!>\)), and the set of prefix words (\(\!\!\!-\)) penalized in  and  is the super-set of the repetitive words (\(\!\!\!>\)). In Xu et al. , they directly penalize the repetitions in pseudo repetitive data (\(\!\!\!>\)). In this section, we will show that repetitive words play an important role in previous works.

High-Inflow WordsSome researchers  attribute the degeneration issue to high-inflow words, whose probability sum of all the potential preceding words is higher than a threshold. Thus, they propose that merging high-inflow word pairs can alleviate the problem (HI-Re). However, we find that 26% of high-inflow word pairs are repetitive in each sentence of Wikitext-103, and merging these pairs can significantly reduce the rep-\(2\) score of real data. Therefore, we argue that merging high-inflow word pairs is actually an alternative way of reducing repetitions in training data.

We evaluate the argument by controlling a single variable, the type of high-inflow words to be merged, in line 4-6 of Tab. 3. The vanilla HI-Re method (Line 4) merges all the high-inflow words (\(\!\!>\)), which takes \(31.1\%\) of the total training words. We find that the method (Line 5), which only merges repetitive high-inflow pairs, i.e., the intersection between high-inflow words and repetitive words (\(\!\!>\) \(\!\!\!<\)), achieves performance comparable to the original HI-Re method (Line 4). Note that the method in line 5 only merges \(8.1\%\) of the total training words, which is much less than the vanilla method. In contrast, the HI-Re method that merges random high-inflow pairs (Line 6), which has the same number as repetitive high-inflow words (\(\!\!>\) \(\!\!\!<\)), cannot alleviate the degeneration. This suggests that penalizing repetitions in data is critical in the success of Fu et al. .

Likelihood ObjectiveMany researchers [11; 15; 26; 34] think that the likelihood objective is the main factor for the degeneration issue. All of those works share the same principle that words in the prefix context (\(\!\!\!>\)) cause the degeneration issue. Therefore, Welleck et al.  and Lin et al.  propose to reduce the probabilities of repetitions appearing in \(_{1:t-1}\) at time step \(t\). Su et al.  and Jiang et al.  leverage the contrastive learning to ensure that the hidden representation at time step \(t\) is distinctive to those of \(_{1:t-1}\).

However, the attribution to the likelihood objective merely serves as a superficial explanation, and the core of this issue lies in the fact that the model inevitably learns repetitive behavior when conducting maximum likelihood estimation on repetitive data. As shown in section 6.1, the GPT-2 model with simple repetition dropout, which is also optimized by likelihood objective, achieves an extremely low rep-\(4\) score on generated text. This indicates that likelihood objective might not be the most

Figure 4: Relationship between the penalized data in previous works. We use \(\!\!\!>\), \(\!\!\!\), and \(\!\!\!\!\!\) to represent the sets of high-inflow words , prefix words [11; 15; 26; 34], and pseudo repetitive sentences , respectively. We also demonstrate the set of repetitive words in real data by \(\!\!\!>\).

important factor in the degeneration issue. Nevertheless, all methods that penalize tokens in the prefix context () break the reliance on repetitions (). Thus, we hypothesize that preventing the model from learning on repetitions is a key factor in their success.

To evaluate the impact of repetitions on these methods, we conduct experiments following the same principle used to analyze high-inflow words. We choose the ScaleGrad method as our baseline, which penalizes non-novel tokens by scaling the gradient . The non-novel tokens are the entire prefix context at time step \(t\), i.e., \(100\%\) of the training words, for the vanilla ScaleGrad method. We also propose two variants of ScaleGrad: the first uses the repetitive words within prefix as the non-novel tokens, while the second randomly samples a subset, which has the same number of words as the first one (), from the prefix data (Subset of ). The two methods only penalize \(19.0\%\) of the total training words. As shown in Tab. 3, the ScaleGrad method on repetitive words (Line 8) achieves performance close to the standard ScaleGrad method (Line 7) in terms of the rep-\(n\) metric. In contrast, the ScaleGrad method on random subset (Line 9) is not effective in alleviating the degeneration issue as the the other two methods. Other works [11; 26; 34] that attribute the degeneration issue to likelihood objective also penalize tokens in prefix as the ScaleGrad, but with different techniques. Thus, we think that penalizing repetitions in data is also a crucial factor for the success of these methods.

Self-reinforcement PhenomenonBoth Xu et al.  and Chiang and Chen  find that degeneration is always accompanied by the self-reinforcement phenomenon, i.e., the probability of a predicted word becomes higher when it is repeated more times. Thus, they hypothesize that degeneration is (partially) caused by self-reinforcement. To mitigate this issue,  proposed a data-augmentation method, namely DITTO (Line 3 of Tab. 3), which constructs pseudo data by repeating a training sentence multiple times and penalizes the probabilities of repetitive tokens in the pseudo data ().

We argue that, as the degeneration issue, the self-reinforcement is also a by-product when neural LMs learning on repetitive patterns in real data. First, we observe a similar self-reinforcement phenomenon on the repetitive words in real data. For instance, the probabilities of the second appearances of the theme-related words are generally higher than their first appearances, as shown in Fig. 5(b). Second, we find that the model trained by repetition dropout can break the self-reinforcement loop, as shown by the examples in Appx.. Although the text generated by GPT-2 + Rep-Dropout on Wikitext-103 may contain a few inappropriate \(n\)-gram repetitions, it will not fall into the infinite repetition loop that frequently appear in the text generated by the vanilla GPT-2.

### Why LMs Learn the Repetition Patterns?

In the last empirical study, we make an attempt to investigate the reasons behind LMs learning repetition patterns, specifically the role these repetitions play in neural LMs. To address this inquiry, we examine 300 randomly selected instances, each featuring repetitive bi-grams within a 256-word sentence. These cases are broadly categorized into three groups, as per Altmann and Kohler  and Tannen :

* **Grammar**: Repetitions for grammatical purposes, such as determiners, conjunctions, etc.
* **Theme**: Repetitions closely associated with the subject matter of the text.
* **Limited inventory**: Repetitions resulting from a language's restricted means of expressing a particular concept, leading to high-frequency occurrences, e.g., the phrase "pair of" in English.

   \# & **Model** & Penal. Scope & Word Percent (\%) & **Rep**-2(\%) & **Rep**-3(\%) & **Rep**-4(\%) & **PPL** \\ 
1 & MLE & N/A & 0.0 & 47.05 & 38.46 & 32.64 & 21.98 \\
2 & +Rep-Dropout & Subset of & 11.4 & **9.78** & **4.34** & **2.14** & 28.26 \\
3 & DITTO & & 25.0 & 44.24 & 34.74 & 28.34 & \\
4 & Hi-Re & & 31.f & **41.91** & 33.82 & 28.35 & \\
5 & Hi-Re & & 8.1 & 43.62 & **33.67** & **27.12** & – \\
6 & Hi-Re & Subset of & 8.1 & 52.41 & 43.72 & 37.56 & – \\
7 & ScaleGrad & & 100.0 & **12.49** & **6.85** & **4.44** & 24.72 \\
8 & ScaleGrad & & 19.0 & 17.53 & 10.38 & 6.94 & 23.33 \\
9 & ScaleGrad & Subset of & 19.0 & 22.97 & 15.22 & 10.94 & 23.18 \\   

Table 3: Impact of penalizing repetitions in different kinds of data on Wikitext-103. The meanings of the symbols,, are illustrated in Fig. 4. The “Subset of Shape” means a subset randomly sampled from Shape. The word percent is \(}{\#}\).

We assign each case to the earliest category it satisfies if it meets the criteria for multiple categories. Detailed guidelines for human evaluators to classify repetitive words can be found in Appx. D. As demonstrated in Fig. 5(a), almost 50% of the repetitions in Wikitext-103 fulfill grammatical functions, while both theme-related and inventory-related repetitions constitute around 25% each.

To understand the impact of different types of repetitions on a vanilla neural LM, we measure the probability change of a prediction when masking the information of its repetitions in the context through attention mechanism. For example, given the human repetitions in Fig. 1, we will determine how the probability of the last appearance of "generals wore..." changes when masking its first appearance of "generals wore..." in the context.

As shown in Fig. 5(b), masking the theme-related repetitions will cause a noticeable drop of the prediction probabilities. However, the prediction of grammatical and inventory-related repetitions are not affected by masking their previous appearances in context. This observation indicates that neural LMs spend its effort on optimizing the prediction accuracy of those theme-related words by implicitly repeating words in context.

## 7 Conclusion & Limitations

In this work, we find that the repetition in training data is a fundamental factor for the degeneration (or repetition) problem. First, training data is an integral part of most AI systems, and our preliminary study demonstrates a strong correlation between the repetitions in training data and the degeneration issue. Second, we find that simply dropping out the attention to repetitions in training data can significantly reduce the degeneration issue, which is more effective than other baselines. Finally, we conduct extensive empirical analyses to demonstrate that penalizing repetitions in data is the key success factor for many previous works, such as those attribute degeneration issue to high-inflow words, the likelihood objective, and the self-reinforcement mechanism. Experiments also show that our findings are critical even in the context of large language models. We hope that the viewpoint we provide to understanding the degeneration can inspire more principled research in the future.

Our work also has some limitations. First, despite the excellent performance on reducing repetitions, the repetition dropout method may hurt the perplexity of the language model. Second, we examine our work mostly on standard benchmark of the language generation task, following previous works [8; 10; 26; 34]. It also deserves to extend our work to large-scale data and model.

## 8 Acknowledgement

The authors would like to express their sincere gratitude to the three anonymous reviewers and the meta reviewer for their insightful comments and constructive feedback. Furthermore, this work was partial supported by the JSPS KAKENHI Grant, under the number 23KJ1594.

Figure 5: (a) Percentages of three types of repetitions in Wikitext-103. (b) Probabilities of three types of words with and without masking their repetitions in context using the vanilla GPT-2 model.