# Random Cycle Coding: Lossless Compression of Cluster Assignments via Bits-Back Coding

Daniel Severo Ashish Khisti Alireza Makhzani

University of Toronto

Department of Electrical and Computer Engineering

{d.severo@mail, akhisti@, a.makhzani@}.utoronto.ca

###### Abstract

We present an optimal method for encoding cluster assignments of arbitrary data sets. Our method, _Random Cycle Coding_ (RCC), encodes data sequentially and sends assignment information as cycles of the permutation defined by the order of encoded elements. RCC does not require any training and its worst-case complexity scales quasi-linearly with the size of the largest cluster. We characterize the achievable bit rates as a function of cluster sizes and number of elements, showing RCC consistently outperforms previous methods while requiring less compute and memory resources. Experiments show RCC can save up to \(2\) bytes per element when applied to vector databases, and removes the need for assigning integer ids to identify vectors, translating to savings of up to \(70\%\) in vector database systems for similarity search applications.

## 1 Introduction

A _clustering_ is a collection of pairwise disjoint sets, called _clusters_, used throughout science and engineering to group data under context-specific criteria. A clustering can be decomposed conceptually into two parts of differing nature. The _data set_, created by the set union of all clusters, and the _assignments_, indicating which elements belong to which cluster. This work is concerned with the _lossless_ communication and storage of the assignment information, for arbitrary data sets, from an information theoretic and algorithmic viewpoint.

Communicating clusters appears as a fundamental problem in modern vector similarity databases such as FAISS (Johnson et al., 2019). FAISS is a database designed to store vectors of large dimensionality, usually representing pre-trained embeddings, for similarity search. Given a query vector, FAISS returns a set of the \(k\)-nearest neighbors (Lloyd, 1982) available in the database under some pre-defined distance metric (usually the L2 distance). Returning the exact set requires an exhaustive search over the entire database for each query vector which quickly becomes intractable in practice. FAISS can instead return an approximate solution by performing a two-stage search on a coarse and fine grained set of database vectors. The database undergoes a training phase where vectors are clustered into sets and assigned a representative (i.e., a centroid). FAISS first selects the \(k^{}\)-nearest clusters, \(k^{}<k\), based on the distance of the query to the centroids, and then performs an exhaustive search within them to return the approximate \(k\)-nearest neighbors.

The cluster assignments must be stored to enable cluster-based approximate searching. In contrast to a class, a cluster is distinguishable only by the elements it contains, and is void of any labelling. However, cluster assignments are often stored alongside the data set in the form of artificially generated labels. Lossy compression techniques for storing the vectors themselves is an active area of research (Chen et al., 2010; Martinez et al., 2016; Babenko and Lempitsky, 2014; Jegou et al., 2010; Huijben et al., 2024). In this literature, the number of bits used to store the vector embedding rangesfrom \(4\) to \(16\) bytes, while ids are typically stored as \(8\)-byte integers. Therefore, labelling, for the sake of clustering, can represent the majority of bits spent for communication and storage in a typical use case, and will become the dominating factor as the performance of these compression algorithms improves.

In this work we show how to communicate and store cluster assignments without creating artificial labels, providing substantial storage savings for vector similarity search applications. Assignments are implicitly represented by a cycle of a permutation defined by the order between encoded elements. Our method, _Random Cycle Coding_ (RCC), uses bits-back coding (Townsend et al., 2019) to pick the order in which data points are encoded. The choice of orderings is restricted to the set of permutations having disjoint cycles with elements equal to some cluster. RCC is optimal as it achieves the Shannon bound (Cover, 1999) in bit savings. The worst-case computational complexity of RCC is quasi-linear in the largest cluster size and requires no training or machine learning techniques.

An overview on lossless compression, permutations, and bits-back coding is given in Section 2. Our method is presented and analyzed in Section 3. To the best of our knowledge, no current method exists to optimally store cluster assignments. In Section 4 we provide two strong baselines based on the work of Severo et al. (2023) for compressing multisets. Section 5 showcases RCC on real-world databases from a well known vector database called FAISS (Johnson et al., 2019), achieving savings of up to \(70\%\) in the best case.

## 2 Background

### Lossless Compression

Lossless compression aims to find a code \(C:\{0,1\}^{}\) for an infinite i.i.d. sequence of elements \(X^{(i)} P_{X}\) that can be decoded with perfect fidelity. Lossless decoding requires restricting \(C\) such that the extended code \(C(X^{(1)})C(X^{(2)})\), created via concatenation, is uniquely decodable. It is known that \(_{X P_{X}}[C(X)] H(P_{X})\) for any uniquely decodable code, and any code with average-length close to \(H(P_{X})\) must obey \(C(x)- P_{X}(x)\)(Shannon, 1948; Cover, 1999). It is possible to construct \(C\) using entropy coders such as Asymmetric Numerical Systems (Duda, 2009), Arithmetic Coding (Witten et al., 1987), or Huffman Codes (Huffman, 1952).

A common way of designing efficient codes is to guarantee the code-word \(C(x)\) carries some meaningful semantics for \(x\) that allows for an efficient implementation. Semantic codes can be constructed by introducing an intermediate sequence of random variables \(Z^{n}\), acting as a proxy for \(X\)1, which is entropy coded autoregressively in \(n\) steps. The code-word for \(x\) is the binary

Figure 1: High-level description of our method, Random Cycle Coding (RCC). RCC encodes the clustering \((},)\) as cycles in the permutation \(\) induced by the relative ordering of objects. **Left:** Indices represent the rankings of objects in \(}\) according to the total ordering of \(\). **Middle:** One of two permutations will be randomly selected with bits-back coding to represent the clustering (\(\), shown in cycle-notation). Then, Foata’s Bijection is applied to yield \(^{}\), shown in line-notation, which is encoded into the ANS stack. **Right:** The final ANS stack containing \(^{}\) in line-notation.

representation of the final state of the chosen entropy coder (e.g., ANS (Duda, 2009)). Unfortunately, semantic entropy coding can lead to sub-optimal performance due to the non-uniqueness of the mapping between \(\) and \(^{n}\), limiting their practical use in applications where this redundancy is large. This is the case for large structured data types such as clusters of high-dimensional embeddings.

Asymmetric Numeral Systems (ANS) (Duda, 2009) is an entropy coder that stores data to an integer state \(s\) using the PMF and CDF of some probability model over the data. When data is encoded the integer state increases by approximately the information content under the model (i.e., the negative log-likelihood), which equals the entropy of the source on average. The final value of the integer state is then serialized to disk using approximately \( s\) bits. ANS operates on \(s\) in a stack-like fashion. Symbols are decoded in reverse order in which they where encoded. Due to this stack-like nature ANS can be used as an invertible sampler, by initializing the stack to a random integer and performing a decode operation. Under mild initialization conditions for \(s\)(see Townsend et al. (2019), Severo et al. (2023)), the decoded sample will be distributed according to the probability model used for decoding. Decoding reduces the number of bits required to represent the stack by the information content of the sampled symbol. The randomly initialized stack can be recovered by encoding the sampled symbol back into the stack using the same distribution.

### Bits-back Coding with ANS

Bits-back Coding (Frey and Hinton, 1996; Townsend et al., 2019) is an entropy coding method for latent variable models \(P_{X,Z}\). The bit-rate achieved is equal to the cross-entropy \(_{X}[- P_{X}(X)]\), where the expectation is taken with respect to the true data distribution of \(X\), despite not having direct access to the marginal \(P_{X}\) needed for entropy coding. Given the posterior \(P_{Z\,|\,X}\), prior \(P_{Z}\), and conditional likelihood \(P_{X\,|\,Z}\) of the model, bits-back coding using ANS to perform invertible sampling from \(P_{Z\,|\,X}\) to obtain \(Z\). Then, \(X\) is encoded with \(P_{X\,|\,Z}\), conditioned on the sampled \(Z\). Finally, \(Z\) is encoded with the prior \(P_{Z}\). Decoding from the stack reduces the ANS integer state by \(- P_{Z\,|\,X}\) while encoding increases it by \(- P_{Z}P_{X\,|\,Z}\), resulting in a net increase equal to the cross-entropy. An approximate posterior can be used when the exact posterior \(P_{Z\,|\,X}\) is not available, resulting in the net increase being equal to the Negative Evidence Lower-Bound (NELBO) (Townsend et al., 2019).

### Random Order Coding (ROC)

ROC (Severo et al., 2023) is an algorithm for losslessly compressing multisets2. A sequence can be seen as a multiset, representing the frequency count of symbols, together with a permutation defining the ordering. ROC uses bits-back coding with the ordering as a latent variable \(Z\), and the multiset as the observation \(X\), together with an exact posterior \(Q_{Z\,|\,X}=P_{Z\,|\,X}\). The exactness of the posterior implies the number of bits used by ROC to encode the multiset is equal to the cross-entropy of the multiset with respect to the true data distribution.

ROC applies a similar procedure to our method to select a random element in the multiset which is then encoded with a symbol codec using ANS. For a multiset with \(k\)_unique_ elements, each appearing \(n_{i}\) times, ROC saves exactly \(- P_{Z\,|\,X}=,,n_{k}}(n!)\) bits, where the quantity in parentheses is the multinomial coefficient.

### Permutations and Cycles

A _permutation_, in this work, is a bijective function \(:[n][n]\) used to define arrangements of elements from arbitrary sets. Permutations are usually expressed in one-line notation \(=[i_{1},i_{2},,i_{n}]\) where \((j)=i_{j}\). The _symmetric group_\(_{n}\) on \(n\) elements is the set of all permutations of \([n]\).

A _cycle_\((c_{1}\;\;c_{k})\), of a permutation \(\), is the sequence constructed from the repeated application of \(\), to some element \(c_{1}[n]\), until \(c_{1}\) is recovered, i.e., \(c_{k+1}=c_{1}\), \(c_{i}=(c_{i-1}),i 2\).

_Example 2.1_.: The cycles of \(=\) are shown below.

A permutation can be represented by its cycles with the following procedure. Pick any element in \([n]\) and compute its cycle by applying \(\) successively. Next, choose another element in \([n]\), that did not show up in any of the previously computed cycles, and compute its cycle. Repeat this procedure until all elements appear in exactly one cycle. Concatenate cycles to form the representation. Every permutation has a unique representation through the concatenation of disjoint cycles. For example,

\[==(1\;3\;2)(4\;5),\] (1)

where the right-hand side is called the cycle notation of \(\).

**Lemma 2.2** (Foata's Bijection (Foata, 1968)).: The following sequence of operations defines a bijection between permutations on \(n\) elements. Write the permutation in disjoint cycle notation such that the smallest element of each cycle appears first within the cycle. Order the cycles in decreasing order based on the first/smallest element in each cycle. Remove all parenthesis to form the one-line notation of the output permutation.

_Example 2.3_.: Applying the steps in the construction of Foata's bijection to \(=(3\;2\;1)(5\;4)\) yields \((1\;3\;2)(4\;5)(4\;5)(1\;3\;2)\). Cycles can be recovered by scanning from left to right and keeping track of the smallest value.

## 3 Method

In this section we develop our method, _Random Cycle Coding_ (RCC), which encodes clusters of data points as disjoint permutation cycles. In what follows, we first describe the coding procedure and then show the model resulting from our procedure assigns likelihood proportional to the product of cluster sizes.

Let \(X^{n}=(X_{1},,X_{n})\) be a sequence of random variables \(X_{i}\) with common, but arbitrary, alphabet \(\). Throughout we assume that a _total ordering_ can be defined for \(\), i.e., elements of the set can be compared and ranked/sorted according to some predefined criteria (e.g., lexicographical ordering). We assume no repeats happen in the sequence. This is motivated by applications where elements are high-dimensional vectors such as embeddings or images where repeats are unlikely to happen.

We are interested in the setting where the elements of \(X^{n}\) are grouped into pair-wise disjoint sets known as _clusters_. Clusters can be represented by a collection of indicator random variables \(=(_{ij})_{1<i<j<n}\) where \(_{ij}=1\) if \(X_{i},X_{j}\) are in the same cluster and \(_{ij}=0\) otherwise. Conditioned on the sequence \(X^{n}=x^{n}\) the clustering \(\) defines a partition of the data set \(}=\{x_{1},,x_{n}\}\). The size of the alphabet of \(\) is equal to the number of ways in which \(}\) can be partitioned; known as the \(n\)-th Bell number (Bell, 1934). The order between elements in a cluster is irrelevant and clusters are void of labels.

The objective is to design a lossless code for the assignments \(\) that can be used alongside any codec for the data set \(}\). Our strategy will be to send the elements of \(}\) in a particular ordering such that it implicitly encodes the clustering information.

We associate a permutation \(_{x^{n}}\) to each of the possible \(n!\) orderings of \(}\) based on sorting. Let \(_{n}(})\) be the set of all possible orderings of \(}\), and \(s^{n}\) a reference sequence created by sorting the elements in \(}\) according to the total ordering of \(\),

\[_{n}(}) =\{x^{n} x_{i}}x_{i} x_{j}i j\},\] (2) \[s^{n} _{n}(})s_{1}<s_{2}<<s_{n}.\] (3)

For any \(x^{n}_{n}(})\), the induced permutation \(_{x^{n}}\) is defined as that which permutes the elements of the reference \(s^{n}\) such that \(x^{n}\) is obtained. Under this definition the permutation can also be constructed by directly substituting \(x_{i}\) for its _ranking_ in \(}\).

The induced permutations allow us to redefine \(\) as a _random equivalence class_ taking on values in the _quotient set_, \(_{n}(})/\!\), of the equivalence relation described next. Note the quotient set is finite even if \(\) is uncountable as only the relative ordering between elements is needed to define the equivalence relation.

**Definition 3.1** (Cycle-Cluster-Equivalence).: Two sequences in \(_{n}(})\) are _equivalent_ (\(\)) if the disjoint cycles of their induced permutations contain the same elements. Given a sequence, two elements of \(}\) are in the same cluster if their rankings appear in the same disjoint cycle of the induced permutation.

_Example 3.2_.: Let \(\) be the set of even integers under the usual ordering for natural numbers. Sequences \(x^{n}=(4,6,2,8)\) and \(z^{n}=(6,2,4,8)\) induce permutations \(_{x^{n}}=\) and \(_{z^{n}}=\). The sequences are equivalent as the disjoint cycles of the induced permutations contain the same elements: \(_{x^{n}}=(4)(1\;2\;3)\), \(_{z^{n}}=(4)(1\;3\;2)\). For both sequences, elements \(2,4\), and \(6\) are in the same cluster, while \(8\) is in a cluster of its own. The partition, viewed as an equivalence class, is equal to \(=\{x^{n},z^{n}\}\), as there are no other permutations over \(}\) that are equivalent to the two shown.

Given \((},)\), Random Cycle Coding (RCC) uses bits-back coding to send the elements of \(}\) in an ordering which corresponds to a sequence in \(\). The receiver decodes the elements \(X_{i}\), in the order sent, and recovers \(\) by computing the cycles of the induced permutation. The clustering information \(\) is communicated via the cycles of the permutation. See Figure 1 for a high-level description.

Every sequence in \(\) maps to the same clustering over \(}\). The log of the number of elements in the equivalence class equals the redundancy discussed in Section 2.1, which is known to be

\[=_{i=1}^{\#}((n_{i}-1)!),\] (4)

where \(n_{i}\) is the number of elements in the \(i\)-th cycle of the induced permutation. An optimal bits-back method must remove exactly \(\) bits from the stack during encoding. RCC achieves these savings by selecting an element from \(}\), using an ANS decode operation, and then encoding it onto the stack. Interleaving decoding/sampling and encoding avoids the _initial bits_ issue (Townsend et al., 2019) resulting from the initially empty ANS stack.

Random Order Coding (Severo et al., 2023) (ROC) performs a similar procedure for multiset compression where elements are also sampled without replacement from \(}\). However, there, the equivalence classes consist of all permutations over \(}\), and therefore sampling can be done by picking any element from \(}\) uniformly at random. RCC requires sampling without replacement from \(}\) non-uniformly such that the resulting permutation has a desired cycle structure. To do so, we define the following procedure, reminiscent of Foata's Bijection (Foata, 1968).

**Definition 3.3** (Foata's Canonicalization).: The following steps map all sequences in the same equivalence class, \(x^{n}\), to the same _canonical_ sequence \(c^{n}\). First, write the permutation in disjoint cycle notation and sort the elements within each cycle, in ascending order, yielding a new permutation. Next, sort the cycles, based on the first (i.e., smallest) element, in descending order.

_Example 3.4_.: The set composed of permutations \(=(3\;1)(5\;2\;4)\) and \(=(3\;1)(2\;5\;4)\) is an equivalence class. Applying Foata's Canonicalization to either \(\) or \(\) yields \((2\;4\;5)(1\;3)\), which is equal to \(\).

AlgorithmRCC encodes a permutation using the procedure outlined in Algorithm 1. The elements of \(}\) are inserted into lists according to their clusterings. The clustering is canonicalized according to Definition 3.3. The encoder starts from the last, i.e., right-most, list. The list is encoded as a set using ROC, with the exception of the smallest element, which is held-out and encoded last. This procedure repeats until all lists are encoded. Decoding is shown in Algorithm 2. During decoding the first element is known to be the smallest in its cycle. The decoder then decodes the remaining cycle elements using ROC, and stops when it sees an element smaller than the current smallest element. This marks the start of a new cycle and repeats until all elements are recovered. Python code for encoding and decoding are given in Appendix A.

SavingsEncoding the smallest value last guarantees that the cycle structure is maintained. Permuting the remaining elements in the cycle spans all permutations in \(\). For the \(i\)-th cluster with \(n_{i}\) elements the savings from encoding \(n_{i}-1\) elements with ROC is equal to \(((n_{i}-1)!)\). The total savings is equal to (4), implying RCC saves \(\).

Implied Probability ModelThe probability model \(Q_{\,|\,}}\) is indirectly defined by the savings achieved by RCC. The set of elements and clustering assignments \((},)\) are encoded via a sequence \(x^{n}\). We can assume some lossless source code is used for the data points, requiring\(- Q_{X^{n}}(x^{n})\) bits to encode the sequence. The cost of encoding the dataset and cluster assignments equals the cost of encoding a sequence minus the discount given by bits-back,

\[- Q_{},}(},)=- Q_{X^{n}}(x^{n} )-.\] (5)

From Severo et al. (2023) we know the cost of encoding the set \(}\) is that of the sequence minus the cost of communicating an ordering,

\[- Q_{}}(})=- Q_{X^{n}}(x^{n})- (n!).\] (6)

From this, we can write,

\[- Q_{\,|\,}}(\,|\,})= Q_{ }}(})- Q_{X^{n}}(x^{n})- =(n!)-.\] (7)

The implied probability model only depends on the cluster sizes, and assigns higher likelihood when there are few clusters with many elements,

\[Q_{\,|\,}}(\,|\,})=^{k}(n_{i}-1)!}{n!},\] (8)

where \(n_{i}\) is the size of the \(i\)-th cluster, and \(k\) the total number of clusters.

ComplexityThe complexity of RCC will vary significantly according to the number of clusters and elements. Initializing RCC requires sorting elements within each cluster, which can be done in parallel, followed by a sort across clusters. ROC is used as a sub-routine and has both worst- and average-case complexities equal to \((n_{i} n_{i})\) for encoding and decoding the \(i\)-th cluster. The total computational complexity of our method, RCC, adapts to the size of the equivalence class: \((_{i}n_{i} n_{i})=()\). When only one permutation can represent the cluster assignments, i.e., \(n=k\), implying \(=0\), then RCC has the same complexity as compressing a sequence: \((n)\).

## 4 Related Work

To the best of our knowledge there is no other method which can perform optimal lossless compression of clustered high-dimensional data.

Severo et al. (2023) presented a method to optimally compress multisets of elements drawn from arbitrary sets called Random Order Coding (ROC). ROC can compress clusterings by viewing them as sets of clusters, but is sub-optimal as it requires encoding the cluster sizes. We compare RCC against the following two variants of ROC next and provide experiments in Section 5.

Roc-1The cluster sizes are communicated with a uniform distribution of varying precision and clusters are then encoded into a common ANS state. Each cluster contributes \((n_{i}!)\) to the bits-back savings of ROC, resulting in a reduction in bit-rate of

\[_{} =_{i=1}^{k}((n_{i}!)-(n-N_{i}))\] (9) \[=_{i=1}^{k}(}{n-N_{i}})+||\] (10) \[||,\] (11)

where \(k\) is the number of clusters, \(N_{i}=_{j=1}^{i-1}n_{j}\) counts the number of encoded elements before step \(i\), and \((n-N_{i})\) is the cost of encoding the size of the \(i\)-th cluster. The gap to optimality increases with the number of clusters, while RCC is always optimal as it achieves \(||\) for any configuration of elements and clusters.

Roc-2This variant views the clusterings as a set of sets. The cluster sizes are communicated as in ROC-1. However, an extra bits-back step is done to randomly select the ordering in which the \(k\) clusters are compressed, resulting in further savings. The complexity of this step scales quasi-linearly with the number of clusters, \((k k)\), and requires sending the number of clusters (\((n)\) bits), which is also the size of the outer set. The total reduction in bit-rate is

\[_{}=_{}+(k!)-(n).\] (12)

This method achieves a better rate than ROC-1, but can require significantly more compute and memory resources due to the extra bits-back step to select clusters compared to both ROC-1 and RCC. Conditioned on knowing the cluster sizes, ROC-2 compresses each cluster independently. Intuitively, the method does not take into account that clusters are pairwise disjoint and their union equals the interval of integers from \(1\) to \(n\), which explains why it achieves a sub-optimal rate savings.

Figure 2: Median encoding plus decoding times, across \(100\) runs, for Random Order Coding (ROC) (Severo et al., 2023) and our method Random Cycle Coding (RCC). The number of elements \(n\) increases from left-to-right across plots. Clusters are fixed to have roughly the same size, \(n/k\), mirroring vector database applications discussed in Section 5.3. Reported times are representative of the amount of compute needed to sample the permutation in the bits-back step as data vectors are encoded with ANS using a uniform distribution.

## 5 Experiments

### Minimum and maximum achievable savings

In applications targeted by our method (see Section 5.3) the cluster size is set according to some budget and elements are allocated into clusters via a training procedure. For a fixed number of elements (\(n\)) and clusters (\(k\)) the savings for ROC-1, ROC-2, and RCC will depend only on the cluster sizes \((n_{1},,n_{k})\). We empirically analyzed the minimum and maximum possible savings as a function of these quantities. Results are shown in Figure 3.

The term dominating the bit savings of all algorithms is the product of the factorials of cluster sizes, \(_{i}n_{i}!\), constrained to \(_{i}n_{i}=n\) and \(n_{i} 1\). The maximum is achieved when \(n-k\) elements fall into one cluster, \(n_{j}=n-k+1\), and all others are singletons: \(n_{i}=1\) for \(i j\). Savings are minimized when all clusters have roughly the same size: \(n_{i}=(n k)+\{i n k\}\)3.

All methods provide similar savings when \(k n\). RCC has better maximal and minimal savings than both ROC-1 and ROC-2 in all settings considered. The need to encode cluster sizes, without exploiting the randomness of cluster orders as in ROC-2, results in ROC-1 achieving _negative_ savings when the number of clusters \(k\) is large. RCC savings converge to \(0\) bits as the number of clusters approaches the number of elements, as expected. As \(k\) approaches \(n\), ROC-2 also suffers from negative savings, but the values are negligible compared to those of ROC-1.

### Encoding and decoding times

Figure 2 shows the total encoding plus decoding time as a function of number of elements and clusters. RCC outperforms both variants of ROC in terms of wall-time by a wide margin, while achieving the optimal savings. RCC is compute adaptive and requires the same amount of time to encode a sequence when \(||=0\). The compute required for ROC variants increases with the number of clusters, correlating negatively with \(||\). ROC-2 is slower than ROC-1 due to the extra bits-back step needed to select clusters with ANS.

### Inverted-lists of Vector Databases (FAISS)

We experimented applying ROC and RCC to FAISS (Johnson et al., 2019) databases of varying size. Results are shown in Table 1. Scalar quantization (Cover, 1999; Lloyd, 1982) was used to partition the set of vectors into disjoint clusters. This results in clusters of approximately the same number of

Figure 3: Maximum (left) and minimum (right) byte savings per element as a function of the number of clusters and elements. Savings are maximized when one cluster contains most elements and all others are singletons. The minimum is achieved if clusters have roughly the same number of elements. Two variants of Random Order Coding (ROC) (Severo et al., 2023) are shown (see Section 4) with dashed lines. Random Cycle Coding (RCC) achieves higher savings than both variants of ROC while requiring less memory and computational resources.

elements, which is the worst-case savings for both ROC and RCC. RCC achieves optimal savings for all combinations of datasets, number of elements, and clusters. ROC-2 has similar performance to RCC but requires significantly more compute as shown in Figure 2.

The total savings will depend on the cluster sizes, the number of bytes used to encode each element (i.e., FAISS vector/embedding), as well as the size of id numbers in the database. Cluster sizes are often set to \(\) resulting in \([]=((-1)!)\)(Johnson et al., 2019). A vast literature exists on encoding methods for vectors (Chen et al., 2010; Martinez et al., 2016; Babenko and Lempitsky, 2014; Jegou et al., 2010; Huijben et al., 2024) with typical values ranging from \(8\) to \(16\) bytes for BigANN and \(4\) to \(8\) for SIFT1M. Typically \(8\) bytes are used to store database ids when they come from an external source and have semantics beyond the vector database itself. Alternatively, ids are assigned sequentially taking up \((n)\) bits each when their only purpose is to be stored as sets to represent clustering information. These ids can be removed if vectors are stored with RCC as the clustering information is represented by the relative orderings between objects without the need for ids. Table 2 shows savings for RCC for the setting of Johnson et al. (2019) with \(k\) clusters.

    & & &  \\  Dataset & \# Elements & \# Clusters & Max. & Min. & \([]\) & RCC & ROC-2 & ROC-1 \\   & & \(500\) & \(2.31\) & \(1.19\) & \(1.20\) & \(\) & \(\) & \(0.04\) \\  & & \(1000\) & \(2.31\) & \(1.06\) & \(1.08\) & \(\) & \(\) & \(0.10\) \\  & & \(4,973\) & \(2.30\) & \(0.77\) & \(0.81\) & \(\) & \(0.04\) & \(0.86\) \\  & & \(9,821\) & \(2.29\) & \(0.65\) & \(0.71\) & \(\) & \(0.12\) & \(2.17\) \\  & & \(46,293\) & \(2.20\) & \(0.37\) & \(0.48\) & \(\) & \(1.28\) & \(18.28\) \\  & & \(95,284\) & \(2.07\) & \(0.24\) & \(0.30\) & \(\) & \(2.07\) & \(61.43\) \\   &  & \(1,\!000\) & \(2.31\) & \(1.06\) & \(1.07\) & \(\) & \(\) & \(0.10\) \\  & & \(10,\!000\) & \(10,\!000\) & \(2.29\) & \(0.65\) & \(0.66\) & \(\) & \(0.01\) & \(2.27\) \\  & & \(99,\!946\) & \(2.06\) & \(0.23\) & \(0.25\) & \(\) & \(0.79\) & \(76.28\) \\    & & \(1,\!000\) & \(2.73\) & \(1.48\) & \(1.49\) & \(\) & \(\) & \(0.01\) \\   & \(10,\!000,\!000\) & \(10,\!000\) & \(2.72\) & \(1.06\) & \(1.07\) & \(\) & \(\) & \(0.14\) \\   & & \(99,\!998\) & \(2.70\) & \(0.65\) & \(0.66\) & \(\) & \(0.01\) & \(2.89\) \\   

Table 1: Byte savings, per element, from compressing SIFT1M (Jegou et al., 2010) and BigANN (Jegou et al., 2011) as a function of number of elements and clusters. Values in columns RCC, ROC-2, and ROC-1, indicate the gap, in percentage (lower is better), to the optimal savings in bytes-per-element, in column \([]\). A value of \(0.00\) indicates the method achieves the maximum bit savings shown in column \([]\). Columns Max. and Min. show the theoretical maximum and minimum savings as discussed in Section 5.1.

    & &  \\   & &  &  \\  \(n\) & \([]\) & \(4\) & \(8\) & \(16\) & \(4\) & \(8\) & \(16\) \\  \(1\)M & \(1.06\) & \(54.8\) & \(33.9\) & \(19.2\) & \(8.9\) & \(6.7\) & \(4.4\) \\ \(10\)M & \(1.27\) & \(60.5\) & \(38.3\) & \(22.1\) & \(10.6\) & \(8.0\) & \(5.3\) \\ \(100\)M & \(1.48\) & \(65.6\) & \(42.4\) & \(24.9\) & \(12.3\) & \(9.3\) & \(6.2\) \\ \(1\)B & \(1.69\) & \(70.1\) & \(46.2\) & \(27.5\) & \(14.1\) & \(10.6\) & \(7.0\) \\   

Table 2: Columns under “% Savings” show the savings, in percentage, for the setting of Johnson et al. (2019) where the number of clusters is held fixed to approximately \(\). Savings in bytes-per-element are shown in the second column, where \([]=((-1)!)\), and agree with Table 1. For external ids, \(8\) bytes are added to \([]\) to compute the total size per element, as well as to the cost under RCC. Meanwhile, \((n)\) bits are added to \([]\) for sequential ids, but not to the cost under RCC, as RCC does not require ids to represent clustering information. See Section 5.3 for a full discussion.

Discussion

This work provides an efficient lossless coding algorithm for storing random clusters of objects from arbitrary sets. Our method, Random Cycle Coding (RCC), stores the clustering information in the ordering between encoded objects and does away with the need to assign meaningless labels for storage purposes.

A random clustering can be decomposed into 2 distinct mathematical quantities, the data set of objects present in the clusters (i.e., the union of all clusters), and an equivalence class representing the cluster assignments. For a given clustering, the equivalence class contains all possible orderings of the data that have cycles with the same elements as some cluster. The logarithm of the equivalence class size is exactly the number of bits needed to communicate an ordering of the data points, with the wanted permutation cycles, which we refer to as the _order information_. This quantity was previously defined by Varshney and Goyal (2006) as the amount of bits required to communicate an ordering of a sequence if the multiset of symbols was given, and equaled \( n!\) when there are no repeated symbols. In the cluster case, the order information \(||\) is strictly less than \( n!\) as the clustering carries partial information regarding the ordering between symbols in the following way: given the clustering, only orderings with the corresponding cycle structure will be communicated.

The savings achieved by RCC equals exactly the _assignment information_ of the data, implying RCC is optimal in terms of compression rate for the probability model shown in Equation (8). The computational complexity of RCC scales with the number of bits recovered by bits-back, and reverts back to that of compressing a sequence when all clusters are atomic.

The savings for RCC scales quasi-linearly with the cluster sizes, and is independent of the representation size of the data. The experiments on real-world datasets from vector similarity search databases showcases where we think our method is most attractive: clusters of data requiring few bytes per element to communicate, where the bits-back savings can represent a significant share of the total representation size.