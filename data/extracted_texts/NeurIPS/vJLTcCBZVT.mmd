# Improving Subgroup Robustness via Data Selection

Saachi Jain, Kimia Hamdiieh, Kristian Georgiev,

Andrew Ilyas, Marzyeh Ghassemi, Aleksander Madry

MIT

{saachij,hamidieh,krisgrg,ailyas,mghassem,madry}@mit.edu

###### Abstract

Machine learning models can often fail on subgroups that are underrepresented during training. While dataset balancing can improve performance on underperforming groups, it requires access to training group annotations and can end up removing large portions of the dataset. In this paper, we introduce _Data Debiasing with Datamodels_ (D3M), a debiasing approach which isolates and removes specific training examples that drive the model's failures on minority groups. Our approach enables us to efficiently train debiased classifiers while removing only a small number of examples, and does not require training group annotations or additional hyperparameter tuning.

## 1 Introduction

The advent of large datasets such as OpenImages  and The Pile  has led to machine learning models being trained on explicit  and illegal  content, or on data that encode negative societal biases  and other spurious correlations . On one hand, there is increasing evidence that models reflect the biases in these datasets; on the other hand, the enormous scale of these datasets makes it extremely expensive to manually curate them--and so removal of such "harmful data" is challenging.

In this paper, we propose an approach that aims to remove data responsible for biased model predictions. In particular, we focus on a specific way of quantifying model bias-- _worst-group error_--which captures the extent to which model performance degrades on pre-defined subpopulations of the data. We aim to identify (and remove) the points in the training dataset that contribute most to this metric to improve the model's group robustness.

The challenge inherent in this approach is that it requires an understanding of how training data affect machine learning model predictions. To overcome this challenge, we first approximate predictions as simple, direct functions of the training dataset, using a framework called _datamodeling_. We can then write our quantitative notion of model bias (which is a function of predictions) as a function of the dataset. Finally, by studying this function, we identify the training data points that contribute most to this measure of model bias. With the resulting method, which we call _Data Debiasing with Datamodels_ (D3M), we show that, across a variety of datasets, there are often a small number of examples that disproportionately drive worst-group error. Removing these examples, in turn, greatly improves models' worst-group error while maintaining dataset size.

Roadmap & contributions.In the rest of this paper, we present and demonstrate the effectiveness of our Data Debiasing with Datamodels (D3M). Concretely, we show that D3M enables us to:

* **Pinpoint examples that harm worst-group accuracy.** We show that there are often a small number of examples that disproportionately drive models' worst-group error on validation data. For example, on CelebA-Age, our method improves worst group error overa natural baseline (data balancing) while removing \(2.4\) fewer examples. Furthermore, these offending examples often form coherent subpopulations within the data.
* **Achieve competitive debiasing performance.** Our approach outperforms standard approaches (both model-based and data-based) to improving worst-group accuracy. Specifically, we use trak, and is able to match the performance of methods which use ground-truth training group annotations .
* **Discover unlabeled biases.** When validation group labels are unavailable, we show how to extract hidden biases (i.e., unlabeled subgroups) directly from the data. As a result, we can perform end-to-end debiasing without _any_ group annotations.

We present our method in Section 4, and demonstrate these capabilities in Section 5. In Section 6, we leverage our framework to discover and mitigate biases within the ImageNet dataset, where D3M surfaces coherent color and co-occurrence biases. We then debias the model according to these failures, and improve accuracy on the identified populations.

## 2 The group robustness problem

We consider an (unobserved) data distribution \(\) over triplets \((x_{i},y_{i},g_{i})\), each comprising an input \(x_{i}\), a label \(y_{i}\), and a _subgroup label_\(g_{i}\), where \(\) is the set of distinct subpopulations in the data. As a running example, consider the CelebA age classification task--here, we take the inputs \(x_{i}\) to be images of faces, the labels \(y_{i}\) to be either "old" or "young," and the possible group labels to be "old man", "old woman", "young man", and "young woman" (see Figure 1).

Given a training dataset \(S_{}\) and a (small) validation dataset \(S_{}\), the goal of the group robustness problem is to produce a classifier \(f\) that minimizes the worst-case loss over groups, i.e.,

\[_{g^{}}_{(x,y,g)} (f(x),y)g=g^{}\,,\] (1)

where \((,)\) is a loss function. When \(\) is the 0-1 loss, Equation (1) is (one minus) the _worst-group accuracy_ (WGA) of the classifier \(f\), which we use to quantify success in the remainder of this work.

Figure 1: Our method (D3M) improves worst group accuracy by identifying and removing the training samples which most negatively impact worst-group accuracy. Specifically, we use trak to identify examples that exacerbate the discrepancy in group performance. We then remove and re-train a model on the remaining data.

Standard loss minimization can yield models that perform poorly with respect to (1). For instance, returning to our example of CelebA age classification, suppose there was a spurious correlation between age and gender in the training set \(S_{}\), such that old men and young women are overrepresented. A predictor that minimizes loss on \(S_{}\) might leverage this correlation, and thus perform poorly on the underrepresented subgroups of old women or young men.

In practice, subgroup labels \(g_{i}\) can be expensive to collect. Thus, approaches to the subgroup robustness problem vary in terms of whether we observe the group label \(g_{i}\) in the training set \(S_{}\) and in the validation set \(S_{}\). In particular, there are three settings of interest:

* **Full-information (Train / Val /):** We observe the group labels for both the training dataset \(S_{}\) and validation dataset set \(S_{}\).
* **Partial-information (Train / Val /):** We observe the group labels for the validation set \(S_{}\), but not for the (much larger) training set \(S_{}\).
* **No-information (Train / Val /):** We do not have group information for either \(S_{train}\) or \(S_{}\). Note that theoretically this setting is unsolvable, since for any non-perfect classifier \(f\), there exists an assignment of group labels so that the worst-group accuracy is zero. Nevertheless, subgroups of relevant practical interest typically have structure that allows for non-trivial results even with no information.

In this work, we focus on the partial-information and no-information settings, since acquiring group labels for the entire training set is often prohibitively expensive. Still, in Section 5, we show that our proposed methods (D3M for the partial-information setting, and Auto-D3M for the no-information setting) perform comparably to full-information approaches.

## 3 Related work

Before introducing our method, we discuss a few related lines of work.

Approaches to subgroup robustness.The _group robustness_ problem (Section 2) has attracted a wide variety of solutions (see, e.g., [3; 20; 40; 25; 21; 38]). Broadly, these solutions fall into one of two categories--model interventions and data interventions. _Model interventions_ target either model weights [42; 45] or the training procedure [40; 21]. _Data interventions_, on the other hand, seek to improve worst-group accuracy by modifying the training dataset. For example, data balancing removes or subsamples examples so that all subgroups are equally represented. Idrissi et al.  find that this simple approach can performs on par with much more intricate model intervention methods.

In this work, we focus on data interventions, for two reasons. First, it is often training data that drives models' disparate performance across groups , e.g., via spurious correlations  or underrepresentation . Second, data interventions do not require any control over the model training procedure, which can make them a more practical solution (e.g., when using ML-as-a-service). Indeed, since data intervention approaches only manipulate the dataset, they are also easy to combine with model intervention techniques.

Compared to our work, the main drawback of existing data interventions is that they often (a) require subgroup labels for the training data (which might not be available), and (b) hurt the models' natural accuracy on skewed datasets [7; 44]. In this work we circumvent these limitations, by proposing a data-based approach to debiasing that can preserve natural accuracy without access to subgroup information.

Bias discovery.Another related line of work identifies biases in machine learning datasets and algorithms. For the former, previous works have shown that large, uncurated datasets used for training machine learning models often contain problematic or biased data [4; 5; 48]. Raji et al.  show that data bias can be a hurdle towards deploying functional machine learning models. Nadeem et al.  curate a dataset to estimate bias in NLP models. Adebayo et al.  show that label errors can disproportionately affect disparity metrics.

On the learning algorithm side, Shah et al.  and Puli et al.  show that the inductive bias of neural networks may encourage reliance on spurious correlations. Pezeshki et al.  leverage two networks trained on random splits of data while imitating confident held-out mistakes made by its sibling to identify the bias. Shah et al.  show that algorithmic design choices (e.g., the choice of data augmentation) can significantly impact models' reliance on spurious correlations. Finally, there has been a variety of work on "slice discovery" [19; 10], where the goal is to discover systematic errors made by machine learning models.

Data selection for machine learning.Our work uses data selection to improve subgroup robustness of machine learning models. A recent line of work has explored data selection for improving various measures of model performance. For example, Engstrom et al.  leverage datamodeling [18; 33] to select pretraining data for LLMs. Similarly, Xia et al.  and Nguyen and Wong  select data for finetuning, and in-context learning, respectively. In another related work, Wang et al.  propose a method to reweight training data in order to improve models' fairness.

Many of these works leverage _contributive data attribution_ methods to select data that improves model performance . Koh and Liang , Feldman and Zhang , Schioppa et al. , and Hammoudeh and Lowd  propose using different variants of influence functions. Ghorbani and Zou  leverage connections to Shapley values, a concept from game theory. Pruthi et al.  use a heuristic to estimate the contribution of each data point to model performance. Ilyas et al.  and Park et al.  use the datamodeling framework.

## 4 Debiasing datasets with datamodeling (D3M)

In this section, we present our _data-based_ approach to training debiased classifiers. The main idea behind our approach is to identify (and remove) the training samples that negatively contribute to the model's worst-group accuracy, by writing model predictions as a function of the training data.

Preliminaries.Let \(S=\{(x_{1},y_{1}),,(x_{n},y_{n})\}\) be a dataset of input-label pairs. For any subset of the dataset--as represented by indices \(D[n]\)--let \((D)^{p}\) be the parameters of a classifier trained on \(D\). Given an example \(z=(x,y)\), let \((z;)\) be the correct-class margin on \(z\) of a classifier with parameters \(\) (defined as \(()\), where \(p\) is the confidence assigned to class \(y\) for input \(x\)).

A _datamodel_ for the example \(z\) is a simple function that predicts \(f(z;(D))\) directly as a function of \(D\), i.e., a function \(_{z}:2^{[n]}\) such that

\[_{z}(D) f(z;(D))\]

Recent works (e.g., [18; 24; 33]) demonstrate the existence of accurate _linear_ datamodels--functions \(\) that decompose _additively_ in terms of their inputs \(D\). In other words, these works show that one can compute example-specific vectors \((z)^{n}\) such that

\[_{z}(D)_{i D}(z)_{i} f(z;(D)).\] (2)

The coefficients \((z)_{i}\) have a convenient interpretation as quantifying the "importance" of the \(i\)-th training sample to performance on example \(z\) (i.e., as a _data attribution_ score ). In what follows, we will assume access to coefficients \((z)\) for any example \(z\)--at the end of this section, we will show how to actually estimate the coefficient vectors \((z)\) efficiently.

Debiasing approach.How can we leverage datamodeling to debias a dataset? Recall that our goal is to remove the samples in \(S\) that lead to high worst-group error. Stated differently, given a dataset \(S\) of size \(n\), we want to maximize the worst-group performance of a classifier \((D)\) with respect to the indices \(D[n]\) that we train on.

Our main idea will be to approximate the predictions of \((D)\) using the corresponding datamodels \(_{z}(D)\). To illustrate this idea, suppose that our goal was to maximize performance on a _single_ test example \(z\), i.e., \(*{arg\,max}_{D}f(z;(D))\). We can approximate this goal as finding \(*{arg\,max}_{D}_{z}((D))\): then, due to the linearity of the datamodel \(_{z}\), the training samples that hurt performance on \(z\) are simply the bottom indices of the vector \((z)\).

Now, this analysis applies not only to a single example \(z\), but to any _linear combination_ of test examples. In particular, if we wish to maximize performance on a linear combination of validation examples, we simply take the linear combination of their coefficients, and remove the training examples corresponding to the smallest coordinates of the averaged vector.

Debiasing with group-annotated validation data.Given a set of validation samples for which the group labels \(g_{i}\) are observable, our last observation gives rise to the following simple procedure:

1. **Compute group coefficients \((G)\) for each \(G\).** Since we have group annotations for each validation sample, we can define a vector \((G)\) for each group \(G\) as simply the average \((z)\) within each group.
2. **Compute group alignment.** Next, we compute a _group alignment score_\(A_{i}\) for each training sample \(i[n]\), which captures the the impact of the sample on worst-group performance. Since there may be many low-performing groups, we use a "smooth maximum" function to weight each group according to its average loss. Thus, for a training example \(i\), \[A_{i}=}(_{g})(g)_{i}}{_{g ^{}}(_{g^{}})},=1.\] (3) Here, \(_{g}\) is the loss of a base classifier \((S)\) on group \(g\) (evaluated on the given validation set).
3. **Remove drivers of bias.** Finally, we construct a new training set \(S_{}\) by keeping only the examples with the highest group alignment scores, i.e., removing the examples that most degrade worst-group accuracy: \[S_{}=(\{A_{i}:z_{i} S_{}\}).\]

We make two brief observations about hyperparameters before continuing. When computing the group alignment score in Step 2, the hyperparameter \(\) controls the temperature of the soft maximum function in (3). When \( 0\), the group alignment \(A_{i}\) measures the impact of the \(i\)-th training example on the "balanced" performance (treating all groups equally). As \(\), \(A_{i}\) collapses to the training example's importance to _only the worst group_, which is suboptimal if models perform poorly on more than one group. For simplicity, we take \(=1\) and refrain from tuning it.

Another hyperparameter in the algorithm above is the number of examples to remove, \(k\). We consider two different ways of setting this hyperparameter. One approach is to search for the value of \(k\) that maximizes worst-group accuracy on the validation set \(S_{}\). Alternatively, we find that the simple (and much more efficient) heuristic of removing all examples with a negative group alignment score (i.e., examples for which \(A_{i}<0\)) tends to only slightly over-estimate the best number of examples to remove (see, e.g., Figure 2). Thus, unless otherwise stated, we use this heuristic when reporting our results.

Debiasing _without_ group annotations.Our procedure above relies on group annotations for a validation set \(S_{}\) to compute the "per-group coefficients" \((G)\). In many real-world settings, however, models might exhibit disparate performance along _unannotated_ subpopulations--in this case, we might not have a validation set on which we can observe group annotations \(g_{i}\). Can we still fix disparate model performance in this setting?

In general, of course, the answer to this question is no: one can imagine a case where each individual example is its own subpopulation, in which case worst-group accuracy will be zero unless the classifier is perfect. In practical settings, however, we typically care about the model's disparate performance on coherent groups of test examples. The question, then, becomes how to find such coherent groups.

We posit that a unifying feature of these subpopulations is that they are _data-isolated_, i.e., that models' predictions on these coherent groups rely on a different set of training examples than models' predictions on the rest of the test data. Conveniently, prior works  show that to find data-isolated subpopulations, one can leverage the _datamodel matrix_--a matrix constructed by stacking the datamodel vectors \((z)\) for each test example. Intuitively, the top principal component of this matrix encodes the direction of maximum variability among the vectors \((z)\). Thus, by projecting the datamodel vectors \((z)\) of our validation examples onto this top principal component, we can identify the examples that are, in a sense, "maximally different" from the rest of the test examples in terms of how they rely on the training set. These maximally different examples correspond to an isolated subpopulation, to which we can apply D3M directly.

This approach (which we call Auto-D3M), enables us to perform end-to-end debiasing without _any_ group annotations. This method proceeds in four steps. For each class:1. Construct a matrix \(\) of stacked attribution vectors, where \(_{ij}=(z_{i})_{j}\).
2. Let \(\) be the top principal component of \(\).
3. Project the attribution vector \((z)\) onto \(\) and construct "group pseudo-labels" \[g_{i}=\{(z_{i})^{}\}.\] where \(\) is a hyperparameter 2 4. Apply D3M with the group pseudo-labels to train a debiased classifier.

Estimating the coefficients \((z)\).In order to operationalize D3M and Auto-D3M, it remains to show that we can actually estimate coefficients \((z)\) satisfying (2). To accomplish this, we use a method called trak. Leveraging differentiability of the model output \(f(z;)\) with respect to the model parameters \(\), trak computes the coefficient vector \((z)\) for an example \(z\) as follows:

1. Train a model \(^{*}(S)\) on the entire training dataset \(S=\{z_{1},,z_{n}\}\).
2. Sample a random Gaussian matrix \(^{p k}\) where \(p\) is the dimensionality of \(^{*}\) (i.e., the number of model parameters) and \(k\) is a hyperparameter;
3. For an example \(z\), define \(g(z)^{}_{}f(z;^{*})\) as the randomly-projected model output gradient (with respect to the model parameters) evaluated at \(z\).
4. Compute the coefficient vector \[}_{}=g(z)^{}(_{z_{j} S}g(z_{j}) g(z_{j})^{} )^{-1}g(z_{i})(1-(f(z;^{*})))\]
5. Repeat steps (a)-(d) for \(T\) trials, and average the results to get a final coefficient vector \((z)\). The trials are identical save for the randomness involved in step (a).

We provide more intuition and details behind trak in Appendix A.

A note on scalabilityIn terms of computational cost, trak involves taking a single backward pass on each of the training and validation examples to compute the model's gradient. The (projected) gradients are then saved to compute trak scores. Typically, trak is computed over \(T\) trials: following the original paper we use \(T=100\). However, our approach can be used with any datamodeling technique.

## 5 Results

In Section 4, we presented D3M--an approach for debiasing a classifier by identifying examples which contribute to a targeted bias. In this section, we validate this framework by assessing its performance on tasks with known biases.

We consider four classification tasks where there is a spurious correlation between the target label and a group label in the training dataset: CelebA-Age, CelebA-Blond, Waterbirds, and MultiNLI. We provide more information about the datasets in Appendix B.1, and other experimental details in Appendix B.2.

### Quantitative results

We first evaluate D3M and Auto-D3M quantitatively, by measuring the worst-group accuracy of models trained on the selected subsets of the biased datasets above.

D3M: Debiasing the model in the presence of validation group labels.In Table 1, we compare D3M against several baselines, each of which requires either only validation group labels (\(}\)) or both training and validation group labels (\(}\)). We find that D3M outperforms all other methods that use the same group information (i.e., only validation group labels) on all datasets except Waterbirds3.

Moreover, D3M performs on par with methods that have full access to both training and validation group labels.

Auto-D3M: Discovering biases with trak.We now consider the case where validation group labels are not accessible. Using Auto-D3M, we debias our model using pseudo-annotations derived from the top principal component of the trak matrix (Auto-D3M in Table 1)4. Note that Auto-D3M is the only method other than ERM that does not require either train or validation group labels. Despite this, Auto-D3M achieves competitive worst-group accuracy in our experiments. We emphasize that Auto-D3M does not require group labels at all--in particular, we _do not_ use group labels to do hyperparameter selection or model selection when we retrain.

The effect of the number of removed examples \(k\).How well does D3M isolate the training examples that drive disparate performance? To answer this question, we iteratively remove training examples from CelebA-Age starting with the most negative \(A_{i}\) and measure the worst-group and balanced accuracy (See Figure 2). CelebA-Age has 40K "majority" examples and 10K "minority" examples; thus, naive balancing requires removing 30K training examples. In contrast, by isolating _which_ specific majority examples contribute to the bias, our method is able to debias the classifier by removing only 10K examples.

Our heuristic of removing examples with negative \(A_{i}\) (the star in Figure 2) slightly overestimates the best number of examples to remove. Thus, while this heuristic gives a decent starting point for \(k\), actually searching for the best \(k\) might further improve performance.

  
**Group Info** &  &  \\ Train / Val & & CelebA-Age & CelebA-Blond & Waterbirds & MultiNLI \\  \)/ \(\)} & ERM & 56.7 & 45.9 & 57.9 & 67.2 \\  & **Auto-D3M (ours)** & **76.0** & **83.8** & **81.0** & **75.0** \\  \)/ \(\)} & JTT  & 61.0 & 81.6 & 63.6 & 72.6 \\  & DFR\({}^{*}\) & 70.4 & 88.4 & **89.0** & 74.7 \\  & **D3M (ours)** & **75.6** & **90.0** & 87.2 & **76.0** \\  \)/ \(\)} & RWG  & **75.6** & 88.4 & 81.2 & 68.4 \\  & SUBG  & 68.5 & 88.3 & **85.5** & 67.8 \\   & GroupDRO  & 74.8 & **90.6** & 72.5 & **77.7** \\   

Table 1: Worst-group accuracies on four group robustness datasets. A \(*\) denotes methods that use validation group labels for both finetuning and hyperparameter tuning.

Figure 2: Worst group accuracy on CelebA-Age as a function of the number of examples \(k\) removed from the training set, using various removal methods. In green, D3M removes the \(k\) training examples with the most negative alignment scores \(A_{i}\). The green star marks the value of \(k\) selected by our heuristic (\(A_{i}<0\)). In blue is the performance of a random baseline that removes \(k\) examples at random from the training set, and in orange is dataset balancing, which removes examples randomly from the majority group. Compared to baselines, D3M efficiently improves worst group accuracy.

### Qualitative results

What type of data does our method flag? In particular, do the examples we identify as driving the targeted bias share some common characteristics? To test this hypothesis, we inspect the data removed by our method and identify subpopulations within the majority groups that are disproportionately responsible for the bias. We then retrain the model after excluding _all_ training examples from the identified subpopulations and show that this is a viable strategy for mitigating the underlying bias.

Finding subpopulations responsible for model bias.Consider the running example from Figure 1 where we train a model on the CelebA-Age dataset to predict whether a person is "young" or "old" when gender is a spurious feature (such that young women and old men are overrepresented). CelebA-Age has a variety of annotations beyond age and gender, such as whether the person is wearing eyeglasses. In this section, we use these extra annotations to identify coherent subpopulations that are flagged by our methods.

In particular, we consider subpopulations formed by taking the Cartesian product of labels and annotations, e.g., subpopulations of the form ("young", "wearing eyeglasses"). For each of these subpopulations, we calculate the average group alignment score \(A_{i}\) of the training examples within that subpopulation (see Figure 4). We find that subpopulations such as "young" with "gray hair" or "old" with either "5 o'clock shadow" or "busy eyebrows" have particularly negative group alignment scores. In Figure 3, we show examples from the subpopulations with the most negative group alignment scores, and observe that a large fraction of the examples in these subpopulations contain labeling errors.

Retraining without identified subpopulations.Once we have identified subpopulations with negative alignment scores, a natural strategy for mitigating the underlying bias is to exclude these subpopulations from the training set. To explore this approach, we exclude the five subpopulations with the most negative attribution scores on average from the CelebA-Age dataset: "Young" + "Gray Hair", "Old"+ "5 o'Clock Shadow", "Old" + "Bushy Eyebrows", "Young" + "Blond Hair", and "Old" + "Sideburns." After retraining the model on this modified training set, we get a worst-group accuracy (WGA) of \(68.4\%\)--an improvement of ~\(12\%\) over the WGA of the original model (\(56.7\%\)).

Figure 4: The average group alignment score of the training examples in each subpopulation of CelebA-Age. Subpopulations such as “old” with “bushy eyebrows” or “young” with “gray hair” have particularly negative scores.

Figure 3: Randomly sampled examples from the subpopulations with the most negative group alignment scores. We find that many of these examples have labeling errors (e.g., platinum blond instead of gray hair.)

## 6 Case Study: Finding and Mitigating Model Failures on ImageNet

In Section 5 we evaluated D3M and Auto-D3M on datasets where the bias was already known. We now deploy Auto-D3M to discover and mitigate biases within the ImageNet dataset, which does not have a predefined bias or available group annotations.

Identifying ImageNet biases.We use trak to compute a coefficient matrix \(\) (see Step 1 of Auto-D3M in Section 4) for a held out validation split (10% of the training set). Focusing on seven ImageNet classes, we use the first principal component of the matrix \(\) to identify potential biases. In Figure 5, we display the most extreme training examples according to the top principal component for four of these classes. PCA identifies semantically color and co-occurrence biases (e.g., tench fishes with or without humans or yellow/white cauliflowers that are either cooked or uncooked.) In fact, our identified biases match the challenging subpopulations in Jain et al.  and Moayeri et al. .

Mitigating ImageNet biases with Auto-D3M.For each of the four targeted ImageNet classes, we seek to mitigate the identified failure modes with Auto-D3M. We consider two settings based on the level of human intervention. In the first, we manually assign each of the validation images to a group according to a human description of identified bias (e.g., an image of a tench is in group 1 if a human is present and group 2 otherwise), and then use those group labels with D3M. 5 In the second setting, we debias in a purely automatic fashion, using Auto-D3M to derive pseudo-group labels from the top principal component. In Figure 6, we display worst group accuracy on the test images of the targeted class (evaluated using manual group assignments of the 50 test examples). Both D3M and Auto-D3M improve worst group accuracy over ERM without significantly impacting the overall ImageNet accuracy (see Appendix C.2).

## 7 Conclusion

We propose Data Debiasing with Datamodels (D3M), a simple method for debiasing classifiers by isolating training data that disproportionately contributes to model performance on underperforming groups. Unlike approaches such as balancing, our method only removes a small number of examples and does not require training group annotations or additional hyperparameter tuning. More generally, our work takes a first step towards _data-centric_ model debiasing.

Figure 5: For four ImageNet classes, the most extreme (positive or negative) examples according to the top PCA direction of the trak matrix. Our method identifies color and co-occurrence biases.

Figure 6: Worst-group accuracy for the ImageNet classes studied in Section 6 after intervening with either D3M or Auto-D3M (error bars over 1 std).