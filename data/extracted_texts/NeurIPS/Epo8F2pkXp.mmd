# Investigating Annotator Bias in Large Language Models for Hate Speech Detection

Amit Das1, Zheng Zhang2, Najib Hasan3, Souvika Sarkar3, Fatemeh Jamshidi4, Tathagata Bhattacharya5, Mostafa Rahgouy6, Nilanjana Raychawdhary6, Dongji Feng7, Vinija Jain8,9, Aman Chadha8,9, Mary Sandage6, Lauramarie Pope6, Gerry Dozier6, Cheryl Seals6

1University of North Alabama, 2Murray State University, 3Wichita State University,

4California State Polytechnic University Pomona, 5Auburn University at Montgomery,

5Auburn University, 7Gustavus Adolphus College, 8Stanford University, 9Amazon GenAI

###### Abstract

Data annotation, the practice of assigning descriptive labels to raw data, is pivotal in optimizing the performance of machine learning models. However, it is a resource-intensive process susceptible to biases introduced by annotators. The emergence of sophisticated Large Language Models (LLMs) presents a unique opportunity to modernize and streamline this complex procedure. While existing research extensively evaluates the efficacy of LLMs, as annotators, this paper delves into the biases present in LLMs when annotating hate speech data. Our research contributes to understanding biases in four key categories: gender, race, religion, and disability with four LLMs: GPT-3.5, GPT-4o, Llama-3.1 and German-2. Specifically targeting highly vulnerable groups within these categories, we analyze annotator biases. Furthermore, we conduct a comprehensive examination of potential factors contributing to these biases by scrutinizing the annotated data. We introduce our custom hate speech detection dataset, _HateBiasNet_, to conduct this research. Additionally, we perform the same experiments on the ETHOS Mollas et al. (2022) dataset also for comparative analysis. This paper serves as a crucial resource, guiding researchers and practitioners in harnessing the potential of LLMs for data annotation, thereby fostering advancements in this critical field. The _HateBiasNet_ dataset is available here: https://github.com/AmitDasRup123/HateBiasNet

**Content Warning:** This article features hate speech examples that may be disturbing to some readers.

## 1 Introduction

The growing widespread presence of online hate speech presents a critical challenge for maintaining safe and inclusive digital environments. Automated hate speech detection systems, powered by machine learning and large language models (LLMs), have emerged as vital tools to address this issue (Tan et al., 2024). These systems rely heavily on annotated datasets to train and evaluate their performance. However, the process of annotating hate speech is inherently subjective and influenced by the annotators' sociocultural, and personal biases. As a result, these biases can inadvertently be embedded into the datasets and, subsequently, into the detection models, raising concerns about fairness, accuracy, and generalizability. Despite the advancements in LLMs and their capabilities, annotator bias remains an underexplored yet significant factor affecting their performance and reliability in hate speech detection tasks.

LLMs offer a promising pathway toward transforming data annotation practices. Their ability to automate annotation tasks, ensure consistency across vast datasets, and adapt through fine-tuning or prompts tailored to specific domains significantly alleviates challenges inherent in traditionalannotation methodologies, thereby establishing a new standard for achievable outcomes in the realm of NLP.

However, data annotation with humans comes with the risk of annotator biases, both conscious and unconscious, that can significantly impact the downstream applications of AI systems. In this paper, we primarily focus on biases in LLMs for hate speech data annotation. Our paper explores four categories: gender, racial, religious, and disability-based bias. Specifically, we select the target groups that are highly vulnerable within the mentioned four categories and explore the annotator biases. Additionally, we provide a detailed analysis of the possible reasons for these biases by exploring the data being annotated. We compare the results across four LLMs: GPT-3.5, GPT-40, Llama-3.1, and Gemm-2. We also explore why certain biases achieve higher accuracy compared to others.

Understanding the implications of these biases is crucial, as they can perpetuate harmful stereotypes and reinforce discrimination in automated systems. By systematically analyzing the biases present in LLMs, we aim to illuminate the mechanisms through which these biases emerge and manifest in the annotations they produce. This investigation not only highlights the ethical considerations involved in utilizing LLMs for sensitive tasks like hate speech detection but also provides insights for improving model training and annotation strategies to foster more equitable AI outcomes.

Moreover, the stakes are particularly high in the realm of hate speech detection, where the potential for mislabeling or biased labeling can lead to severe consequences, including the marginalization of vulnerable communities and the unjust suppression of free speech. As LLMs become increasingly integrated into content moderation systems, it is essential to ensure that the annotations they produce are not only accurate but also reflective of a fair and balanced perspective. By addressing these biases, we can pave the way for the development of more robust, transparent, and socially responsible AI systems, ultimately contributing to a safer online environment for all users. Serving as a critical guide, this paper aims to steer researchers toward exploring the potential of LLMs for data annotation, thereby facilitating future advancements in this essential domain.

Through rigorous data annotation, prompt engineering, quantitative and qualitative analysis, we aim to answer the following research questions:

**RQ1: Does annotator bias exist in Large Language Models for hate speech detection?**

**RQ2: If it exists, what potential factors contribute to its existence?**

Figure 1: Workflow diagram of our study, illustrating how varying biases can lead to different outcomes when annotating a sample text as hateful. We investigate annotator biases across four categories for hate speech detection using the following LLMs: GPT-3.5, GPT-40, Llama-3.1, and Gemm-2.

### RQ3: How can this problem be effectively mitigated?

To this end, our work makes the following contributions:

[leftmargin=*]
**(The Contributions)**

* Our research demonstrates that annotator bias is present in LLMs used for hate speech detection. This bias arises from the subjective interpretations of annotators, which influence the training data and consequently affect the model's performance. We provide empirical evidence illustrating how such biases skew detection results, leading to potential inaccuracies and unfair outcomes.
* In our research, we specifically examine four types of biases: gender, race, disability, and religion. Gender bias refers to the prejudiced treatment based on an individual's sex or gender identity. Race bias involves discriminatory actions or attitudes towards individuals based on their racial or ethnic background. Disability bias encompasses unfair treatment of people with physical or mental impairments. Religion bias involves prejudices and discriminatory behaviors directed at individuals based on their religious beliefs or practices. Our study aims to analyze the prevalence and impact of these biases in various contexts.
* We delve into the underlying factors contributing to bias and propose a potential solution to address this issue. We analyze various aspects to uncover the root causes of bias and present a strategy aimed at mitigating its effects. Through our investigation, we aim to provide valuable insights into understanding and combatting bias in our study.

## 2 Related Work

The advent of LLMs has revolutionized NLP tasks by enabling the development of more sophisticated and context-aware language understanding systems. Models such as BERT (Devlin et al., 2018), and their variants have demonstrated remarkable performance across a wide range of NLP tasks, including text classification, language generation, and question answering. These models leverage pre-training on large corpora followed by fine-tuning on task-specific data, allowing them to capture intricate linguistic patterns and semantic relationships.

Recent research has explored the use of LLMs for data annotation tasks, leveraging their ability to comprehend and generate human-like text. For instance, (Gururangan et al., 2020) proposed a framework for generating natural language explanations for machine learning models, facilitating the annotation of model predictions with interpretable justifications. Similarly, (Raffel et al., 2020) introduced a method for efficiently annotating speech data using GPT-2, demonstrating significant reductions in annotation time compared to traditional manual labeling approaches.

The increasing interest in leveraging Large Language Models as versatile annotators for various natural language tasks has been highlighted in recent research (Kuzman et al., 2023; Zhu et al., 2023; Ziems et al., 2024). (Wang et al., 2021) demonstrated that GPT-3 can significantly decrease labeling costs by up to 96% for both classification and generation tasks. Similarly, (Ding et al., 2023) conducted an assessment of GPT-3's effectiveness in labeling and data augmentation across classification and token-level tasks. Furthermore, empirical evidence suggests that LLMs can surpass crowdsourced annotators in certain classification tasks (Gilardi et al., 2023; He et al., 2023).

The investigation of social biases within Natural Language Processing (NLP) models constitutes a significant area of research. Previous studies have delineated two primary categories of biases and harms: allocational harms and representational harms (Blodgett et al., 2020; Crawford, 2017). Scholars have explored various methodologies to assess and alleviate these biases in both Natural Language Understanding (NLU) (Bolukbasi et al., 2016; Dixon et al., 2018; Zhao et al., 2018; Bordia and Bowman, 2019; Dev et al., 2021; Sun and Peng, 2021) and Natural Language Generation (NLG) tasks (Sheng et al., 2019; Dinan et al., 2019).

Within this body of literature, (Sun and Peng, 2021) proposed utilizing the Odds Ratio (OR) (Szumilas, 2010) as a metric to quantify gender biases, particularly in items exhibiting significant frequency disparities or high salience among genders. (Sheng et al., 2019) assessed biases in NLG model outputs conditioned on specific contextual cues, while (Dhamala et al., 2021) extended this analysis by incorporating real-world prompts extracted from Wikipedia. Several strategies (Sheng et al., 2020; Liu et al., 2021; Cao et al., 2022; Gupta et al., 2022) have been proposed to mitigate biases in NLG models, yet their applicability to closed API-based LLMs, such as ChatGPT, remains uncertain.

Methodologies

### Data Collection and Annotation

The study initiates with the utilization of a hate speech lexicon sourced from Hatebase.org1, comprising terms and expressions identified by online users as indicative of hate speech. Leveraging the Twitter API, we conducted a search for tweets containing lexicon terms, resulting in a corpus of 3003 tweets. Subsequently, three speech-language pathology graduate students were engaged for the purpose of data annotation. These annotators were tasked with categorizing each tweet into one of two classifications: hateful or not hateful. We name this dataset as _HateBiasNet_.

Acknowledging the inherent vagueness in prior methodologies for annotating hate speech, as noted by (Schmidt & Wiegand, 2017), which often led to low agreement scores, our study took measures to enhance the clarity and consistency of the an- notation process. To achieve this, all annotators collaboratively formulated and refined annotation guidelines to ensure a shared understanding of hate speech. An explicit definition, accompanied by a detailed explanation, was provided to elucidate the concept further.

Annotators were instructed to consider not only the isolated words within a tweet but also the broader contextual usage of these terms. Emphasis was placed on discerning the intent behind the lan- guage and recognizing that the mere presence of offensive vocabulary did not inherently classify a tweet as hate speech. Each tweet underwent coding by three independent annotators, and the majority decision among them was employed to assign the final label. The annotation details are provided in the appendix.

### Data Annotation by LLMs

We then had our data annotated by the four LLMs. For the annotation, we first provided the annotator details, using direct prompt provided by (Das et al., 2024) for the annotation. One such prompt with the annotator being 'Female' is as follows. Note that [Text] refers to the input text to be annotated.

### Annotator Biases

We used only the highly vulnerable groups on social media and used them as annotators. With expert opinions, we selected six groups from four categories that face the most hateful comments on social media. We then explore the annotator bias in LLM annotation assuming the one annotator to be from one of the six categories, and one annotator not from that category. Figure 1 depicts the workflow diagram of our work. The annotator biases we explored are given in Table 1.

  
**Category** & **Annotator Bias** \\  Gender & Female vs. Not Female \\  Race & Asian vs. Not Asian \\ Race & Black vs. Not Black \\  Religion & Muslim vs. Not Muslim \\  Disability & Mental Disability vs. No Disability \\ Disability & Physical Disability vs. No Disability \\   

Table 1: Annotator Biases in LLMs explored in this paper. With expert opinions, we selected six groups from four categories that face the most hateful comments on social media. We then explore the annotator bias in LLM annotation assuming the one annotator to be from one of the six categories, and one annotator not from that category.

Results & Discussion

Along with _HateBiasNet_, we explored the same annotator bias on ETHOS (Mollas et al., 2022) dataset. We re-annotated the whole dataset using the four LLMs with the same experimental setup we used for annotating _HateBiasNet_. The analysis of data annotations by the LLMs revealed notable biases on both the datasets across each category. We observed a significant skew in the distribution of annotations towards the categories we used. Table 2 shows the mismatches between different annotator biases both on _HateBiasNet_ and ETHOS dataset while annotating them with four LLMs: GPT-3.5, GPT-4o, Llama-3.1 and Genma-2. It is observed that there are significant mismatches in both _HateBiasNet_ and the ETHOS dataset. These findings underscore the presence of subjectivity and ambiguity in the LLM-based annotation process, highlighting the need for standardized guidelines and rigorous quality control measures.

### GPT-3.5

In the context of GPT-3.5, annotator bias arises due to its pretraining on large, publicly available datasets, which embed societal biases, particularly in sensitive areas like hate speech. Since GPT-3.5 lacks alignment optimizations present in later models, it struggles with the nuanced classification of hate speech, leading to inconsistent annotations based on cultural and contextual variations.

An analysis of GPT-3.5 reveals significant differences in how annotators classify offensive language, influenced by their personal and cultural backgrounds. For example, Annotator 1 (Black) displayed leniency towards culturally contextual or ironic language, labeling statements like "The superior race but cannot stand in the sun without literally burning" as 'Not Hateful,' while Annotator 2 (Not Black) labeled the same statements as 'Hateful'. These discrepancies also extended to gender, sexuality, and disability-related content, where Annotator 1 (Female, Muslim, disabled) and Annotator 2 (Not Female, Not Muslim, no disability) showed different sensitivities. Annotator 1 was more tolerant of remarks related to gender and sexuality but more sensitive to offensive terms related to disability, whereas Annotator 2 exhibited opposite tendencies.

These findings highlight how annotators' personal identities, including race, religion, gender, and disability status, influence their interpretation of hate speech, leading to inconsistent classifications in GPT-3.5's output.

### GPT-4o:

In analyzing GPT-4o's performance in hate speech detection, notable biases emerge despite improvements in its alignment and fine-tuning. These biases are linked to the model's broad training data and an overcompensation for ethical considerations, leading to inconsistencies in identifying offensive content, particularly in borderline or culturally specific cases.

The study reveals demographic and cultural disparities in hate speech annotation. For example, Asians consistently label derogatory terms such as 'cripple' or'retard' as hateful due to their cultural and historical context, while non-Asians show more variability in their classifications. Significant differences also arise in how statements about black individuals, nationalities, disabilities, and religions are perceived. For instance, a Black annotator might classify a statement like "black Americans are more in danger than homosexuals" as hateful, while another annotator may not.

  
**An annotator Bias** & **Mismatch (\%)** **for GPT-3.5** & **Mismatch (\%)** **for GPT-4o** & **Mismatch (\%)** **for** & **Mismatch (\%)** **for** & **Mismatch (\%)** **for** \\  & **annotation** & **annotation** & **annotation** & **Llama-3.1 annotation** & **Geman-2 annotation** \\   & _HateBiasNet_ & _HateBiasNet_ & _HateBiasNet_ & _HateBiasNet_ & _HateBiasNet_ & _HateBiasNet_ & _HateBiasNet_ \\  & **Dataset** & **Dataset** & **Dataset** & **Dataset** & **Dataset** & **Dataset** & **Dataset** & **Dataset** \\    
  
**An annotator Bias** & **Mismatch (\%)** **for GPT-3.5** & **Mismatch (\%)** **for GPT-4o** & **Mismatch (\%)** **for** & **Mismatch (\%)** **for** & **Mismatch (\%)** **for** & **Mismatch (\%)** **for** \\  & **annotation** & **annotation** & **Location-3.1 annotation** & **Geman-2 annotation** & **Location-3.1 annotation** & **Geman-2 annotation** \\   

Table 2: Mismatches between different annotations when annotated by LLMs. It can be seen that for the ETHOS dataset, the biases are significantly reduced for GPT-4o annotation when compared to GPT-3.5, Llama-3.1 and Genma-2 annotation.

[MISSING_PAGE_FAIL:6]

Figure 3: Heatmap of the ETHOS dataset depicting the accuracy of 11 biases across 4 LLMs. The bias ‘Black’ achieved the highest accuracy for both GPT-3.5 and GPT-4o, while ‘Asian’ exhibited the highest accuracy for Llama-3.1. The word cloud of the dataset (Figure 4) suggests that specific keywords may influence annotation results for these LLMs. Notably, GPT-4o and Llama-3.1 consistently outperformed GPT-3.5 and Gemma-2 across all biases. Among the LLMs, GPT-4o’s performance on the ‘Black’ bias stands out as the highest overall accuracy.

Figure 2: Heatmap of the _**HateBiasNet**_ dataset illustrating the accuracy of 11 biases across 4 LLMs. Notably, GPT-3.5, GPT-4o, and Llama-3.1 demonstrate the highest accuracy for the ‘Mental disability’ bias. The word cloud of the dataset (Figure 4) suggests that specific keywords may influence annotation outcomes for these LLMs. Additionally, Llama-3.1 shows the highest accuracy overall for the ‘Mental disability’ bias among the 4 models.

We proceeded to evaluate which bias yields the highest accuracy for data annotation by comparing the annotation results with the original human annotations. The results shown in Table 3 indicate that the Mental Disability bias achieves the highest accuracy on _HateBiasNet_. Additionally, Figure 4 illustrates the word histogram of _HateBiasNet_ after the removal of stopwords. Notably, the most frequently occurring words are'retarded' and 'trailer trash', both of which are closely associated with the mental disability bias.

For the ETHOS dataset, it can be seen that using Black bias gives the best result. Figure 4 shows the word histogram of Ethos dataset after removing the stopwords. It can be seen there is a clear relation between the keywords present in the dataset and the accuracy of data annotation by a particular group. We believe, although annotator bias exists in LLMs for hate speech detection, but selecting the correct prompt for the annotation can help mitigate this problem. Figures 2 and 3 show the heatmaps of the 11 biases across the the 4 LLMs for the _HateBiasNet_ and the Ethos datasets respectively.

## 5 Conclusion

Our research highlights the presence of annotator biases in hate speech detection using both GPT-3.5 and GPT-4o, opening avenues for future investigation. One potential direction involves mitigating these biases by incorporating specific rules into the LLMs while training or prompting annotators to prevent biased outputs. Additionally, exploring broader aspects of the problem statement through enhanced language style or lexical content analyses holds promise.

The advent of LLMs like ChatGPT has introduced novel applications such as data annotation. However, our study underscores the risk of biases emerging when LLMs are directly utilized for annotation tasks. We meticulously assess four types of biases in LLM-assisted hate speech detection, revealing the propagation and amplification of harmful biases in annotations.

Our findings emphasize the need for cautious utilization of AI-assisted data annotation to counteract biases effectively. We advocate for the development of comprehensive policies governing the use of LLMs in real-world scenarios. Furthermore, we call for continued research into identifying and mitigating fairness issues in data annotation with LLMs, as understanding and addressing underlying biases are imperative for reducing potential harms in future LLM research endeavors.

Figure 4: Word histogram (considering only the top 5 words) of (a) _HateBiasNet_ and (b) ETHOS after removing the stopwords.