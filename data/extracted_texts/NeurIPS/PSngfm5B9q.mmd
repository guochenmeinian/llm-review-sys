# Beyond Exponential Graph:

Communication-Efficient Topologies for Decentralized Learning via Finite-time Convergence

Yuki Takezawa\({}^{1,2}\), Ryoma Sato\({}^{1,2}\), Han Bao\({}^{1,2}\), Kenta Niwa\({}^{3}\), Makoto Yamada\({}^{2}\)

\({}^{1}\)Kyoto University, \({}^{2}\)OIST, \({}^{3}\)NTT Communication Science Laboratories

Equal Contribution

###### Abstract

Decentralized learning has recently been attracting increasing attention for its applications in parallel computation and privacy preservation. Many recent studies stated that the underlying network topology with a faster consensus rate (a.k.a. spectral gap) leads to a better convergence rate and accuracy for decentralized learning. However, a topology with a fast consensus rate, e.g., the exponential graph, generally has a large maximum degree, which incurs significant communication costs. Thus, seeking topologies with both a fast consensus rate and small maximum degree is important. In this study, we propose a novel topology combining both a fast consensus rate and small maximum degree called the Base-\((k+1)\) Graph. Unlike the existing topologies, the Base-\((k+1)\) Graph enables all nodes to reach the exact consensus after a finite number of iterations for any number of nodes and maximum degree \(k\). Thanks to this favorable property, the Base-\((k+1)\) Graph endows Decentralized SGD (DSGD) with both a faster convergence rate and more communication efficiency than the exponential graph. We conducted experiments with various topologies, demonstrating that the Base-\((k+1)\) Graph enables various decentralized learning methods to achieve higher accuracy with better communication efficiency than the existing topologies. Our code is available at https://github.com/yukiTakezawa/BaseGraph.

## 1 Introduction

Distributed learning, which allows training neural networks in parallel on multiple nodes, has become an important paradigm owing to the increased utilization of privacy preservation and large-scale machine learning. In a centralized fashion, such as All-Reduce and Federated Learning [8; 9; 16; 26; 27], all or some selected nodes update their parameters by using their local dataset and then compute the average parameter of these nodes, although computing the average of many nodes is the major bottleneck in the training time [18; 19; 23]. To reduce communication costs, decentralized learning gains significant attention [11; 18; 24]. Because decentralized learning allows nodes to exchange parameters only with a few neighbors in the underlying network topology, decentralized learning is more communication efficient than All-Reduce and Federated Learning.

While decentralized learning can improve communication efficiency, it may degrade the convergence rate and accuracy due to its sparse communication characteristics [11; 48]. Specifically, the smaller the maximum degree of an underlying network topology is, the fewer the communication cost becomes [33; 39]; meanwhile, the faster the consensus rate (a.k.a. spectral gap) of a topology is, the faster the convergence rate of decentralized learning becomes . Thus, developing a topology with both a fast consensus rate and small maximum degree is essential for decentralized learning. Table 1 summarizes the properties of various topologies. For instance, the ring and exponential graph arecommonly used [1; 3; 12; 23]. The ring is a communication-efficient topology because its maximum degree is two but its consensus rate quickly deteriorates as the number of nodes \(n\) increases . The exponential graph has a fast consensus rate, which does not deteriorate much as \(n\) increases, but it incurs significant communication costs because its maximum degree increases as \(n\) increases . Thus, these topologies sacrifice either communication efficiency or consensus rate.

Recently, the \(1\)-peer exponential graph  and \(1\)-peer hypercube graph  were proposed as topologies that combine both a small maximum degree and fast consensus rate (see Sec. C.4 for examples). As Fig. 1 shows, in the ring and exponential graph, node parameters only reach the consensus asymptotically by repeating exchanges of parameters with neighbors. Contrarily, in the \(1\)-peer exponential and \(1\)-peer hypercube graphs, parameters reach the exact consensus after a finite number of iterations when \(n\) is a power of \(2\) (see Fig. 21 in Sec. F.2). Thanks to this property of finite-time convergence, the \(1\)-peer exponential and \(1\)-peer hypercube graphs enable Decentralized SGD (DSGD)  to converge at the same convergence rate as the exponential graph when \(n\) is a power of \(2\), even though the maximum degree of the \(1\)-peer exponential and \(1\)-peer hypercube graphs is only one . However, this favorable property only holds when \(n\) is a power of \(2\). When \(n\) is not a power of \(2\), the \(1\)-peer hypercube graph cannot be constructed, and the \(1\)-peer exponential graph only reaches the consensus asymptotically as well as the ring and exponential graph, as Fig. 1 illustrates. Thus, the \(1\)-peer exponential and \(1\)-peer hypercube graphs cannot enable DSGD to converge as fast as the exponential graph when \(n\) is not a power of \(2\). Moreover, even if \(n\) is a power of \(2\), the \(1\)-peer hypercube and \(1\)-peer exponential graphs still cannot enable DSGD to converge faster than the exponential graph.

In this study, we ask the following question: _Can we construct topologies that provide DSGD with both a faster convergence rate and better communication efficiency than the exponential graph for any number of nodes?_ Our work provides the affirmative answer by proposing the Base-\((k+1)\) Graph,2 which is finite-time convergence for any number of nodes \(n\) and maximum degree \(k\) (see Fig. 1). Thanks to this favorable property, the Base-2 Graph enables DSGD to converge faster than the ring and torus and as fast as the exponential graph for any \(n\), while the Base-2 Graph is more communication-efficient than the ring, torus, and exponential graph because its maximum degree is only one. Furthermore, when \(2 k<_{2}(n)\), the Base-\((k+1)\) Graph enables DSGD to converge faster with fewer communication costs than the exponential graph because the maximum degree of the Base-\((k+1)\) Graph is still less than that of the exponential graph. Experimentally, we compared the Base-\((k+1)\) Graph with various existing topologies, demonstrating that the Base-\((k+1)\) Graph enables various decentralized learning methods to more successfully reconcile accuracy and communication efficiency than the existing topologies.

## 2 Related Work

**Decentralized Learning.** The most widely used decentralized learning methods are DSGD  and its adaptations [1; 2; 19]. Many researchers have improved DSGD and proposed DSGD with momentum [4; 20; 44; 45], communication compression methods [6; 10; 23; 35; 38], etc. While DSGD is a simple and efficient method, DSGD is sensitive to data heterogeneity . To mitigate this issue, various methods have been proposed, which can eliminate the effect of data heterogeneity on the convergence rate, including gradient tracking [21; 29; 30; 34; 42; 47], \(D^{2}\), etc. [17; 37; 46].

  
**Topology** & **Consensus Rate** & **Connection** & **Maximum Degree** & **\#Nodes**\(n\) \\  Ring  & \(1-O(n^{-2})\) & Undirected & \(2\) & \( n\) \\ Torus  & \(1-O(n^{-1})\) & Undirected & \(4\) & \( n\) \\ Exp.  & \(1-O((_{2}(n))^{-1})\) & Directed & \(_{2}(n)\) & \( n\) \\
1-peer Exp.  & \(O(_{2}(n))\)-finite time conv. & Directed & \(1\) & A power of 2 \\
1-peer Hypercube  & \(O(_{2}(n))\)-finite time conv. & Undirected & \(1\) & A power of 2 \\
**Base-\((k+1)\) Graph (ours)** & \(O(_{k+1}(n))\)-finite time conv. & Undirected & \(k\) & \( n\) \\   

Table 1: Comparison among different topologies with \(n\) nodes. The definition of the consensus rate and finite-time convergence is shown in Sec. 3.

Figure 1: Comparison of consensus rate. See Sec. 6 for detailed experimental settings. The number in the bracket is the maximum degree.

**Effect of Topologies.** Many prior studies indicated that topologies with a fast consensus rate improve the accuracy of decentralized learning [11; 13; 25; 39]. For instance, DSGD and gradient tracking converge faster as the topology has a faster consensus rate [18; 34]. Zhu et al.  revealed that topology with a fast consensus rate improves the generalization bound of DSGD. Especially when the data distributions are statistically heterogeneous, the topology with a fast consensus rate prevents the parameters of each node from drifting away and can improve accuracy [11; 34]. However, communication costs increase as the maximum degree increases [33; 43]. Thus, developing a topology with a fast consensus rate and small maximum degree is important for decentralized learning.

## 3 Preliminary and Notation

**Notation.** A graph \(G\) is represented by \((V,E)\) where \(V\) is a set of nodes and \(E\) is a set of edges. If \(G\) is a graph, \(V(G)\) (resp. \(E(G)\)) denotes the set of nodes (resp. edges) of \(G\). For any \(a_{1},,a_{n}\), \((a_{1},,a_{n})\) denotes the ordered set. An empty (ordered) set is denoted by \(\). For any \(n\), let \([n]\{1,,n\}\). For any \(n,a\), \((a,n)\) is the remainder of dividing \(a\) by \(n\). \(\|\|_{F}\) denotes Frobenius norm, and \(_{n}\) denotes an \(n\)-dimensional vector with all ones.

**Topology.** Let \(G\) be an underlying network topology with \(n\) nodes, and \(^{n n}\) be a mixing matrix associated with \(G\). That is, \(W_{ij}\) is the weight of the edge \((i,j)\), and \(W_{ij}>0\) if and only if \((i,j) E(G)\). Most of the decentralized learning methods require \(\) to be doubly stochastic (i.e., \(_{n}=_{n}\) and \(^{}_{n}=_{n}\)) [18; 20; 29; 36]. Then, the consensus rate of \(G\) is defined below.

**Definition 1**.: _Let \(\) be a mixing matrix associated with a graph \(G\) with \(n\) nodes. Let \(_{i}^{d}\) be a parameter that node \(i\) has. Let \((_{1},,_{n})^{d n}\) and \(}_{n}_{n}^{}\). The consensus rate \([0,1)\) is the smallest value that satisfies \(\|-}\|_{F}^{2}^{2}\|- }\|_{F}^{2}\) for any \(\)._

Thanks to \([0,1)\), \(_{i}\) asymptotically converge to consensus \(^{n}_{i}}\) by repeating parameter exchanges with neighbors. However, this does not mean that all nodes reach the exact consensus within a finite number of iterations except when \(=0\), that is, when \(G\) is fully connected. Then, utilizing time-varying topologies, Ying et al.  and Shi et al.  aimed to obtain sequences of graphs that can make all nodes reach the exact consensus within finite iterations and proposed the \(1\)-peer exponential and \(1\)-peer hypercube graphs respectively (see Sec. C.4 for illustrations).

**Definition 2**.: _Let \((G^{(1)},,G^{(m)})\) be a sequence of graphs with the same set of nodes (i.e., \(V(G^{(1)})==V(G^{(m)})\)). Let \(n\) be the number of nodes. Let \(^{(1)},,^{(m)}\) be mixing matrices associated with \(G^{(1)},,G^{(m)}\), respectively. Suppose that \(^{(1)},,^{(m)}\) satisfy \(^{(1)}^{(2)}^{(m)}=}\) for any \(^{d n}\), where \(}=_{n}_{n}^{}\). Then, \((G^{(1)},,G^{(m)})\) is called \(m\)-finite time convergence or an \(m\)-finite time convergent sequence of graphs._

Because Definition 2 assumes that \(V(G^{(1)})==V(G^{(m)})\) holds, we often write a sequence of graphs \((G^{(1)},,G^{(m)})\) as \((E(G^{(1)}),,E(G^{(m)}))\) using a slight abuse of notation. Additionally, in the following section, we often abbreviate the weights of self-loops because they are uniquely determined due to the condition that the mixing matrix is doubly stochastic.

## 4 Construction of Finite-time Convergent Sequence of Graphs

In this section, we propose the Base-\((k+1)\) Graph, which is finite-time convergence for any number of nodes \(n\) and maximum degree \(k[n-1]\). Specifically, we consider the setting where node \(i\) has a parameter \(_{i}\) and propose a graph sequence whose maximum degree is at most \(k\) that makes all nodes reach the exact consensus \(_{i=1}^{n}_{i}\). To this end, we first propose the \(k\)-peer Hyper-hypercube Graph, which is finite-time convergence when \(n\) does not have prime factors larger than \(k+1\). Using it, we propose the Simple Base-\((k+1)\) Graph and Base-\((k+1)\) Graph, which are finite-time convergence for any \(n\).

Figure 2: Illustration of the \(2\)-peer Hyper-hypercube Graph.

### \(k\)-peer Hyper-hypercube Graph

Before proposing the Base-\((k+1)\) Graph, we first extend the \(1\)-peer hypercube graph  to the \(k\)-peer setting and propose the \(k\)-peer Hyper-hypercube Graph, which is finite-time convergence when the number of nodes \(n\) does not have prime factors larger than \(k+1\) and is used as a component in the Base-\((k+1)\) Graph. Let \(V\) be a set of \(n\) nodes. We assume that all prime factors of \(n\) are less than or equal to \(k+1\). That is, there exists \(n_{1},n_{2},,n_{L}[k+1]\) such that \(n=n_{1} n_{L}\). In this case, we can construct the \(L\)-finite time convergent sequence of graphs whose maximum degree is at most \(k\). Using Fig. 1(a), we explain how all nodes reach the exact consensus. Let \(G^{(1)}\) and \(G^{(2)}\) denote the graphs in Fig. 1(a) from left to right, respectively. After the nodes exchange parameters with neighbors in \(G^{(1)}\), nodes \(1\) and \(2\), nodes \(3\) and \(4\), and nodes \(5\) and \(6\) have the same parameter respectively. Then, after exchanging parameters in \(G^{(2)}\), all nodes reach the exact consensus. We present the complete algorithms for constructing the \(k\)-peer Hyper-hypercube Graph in Alg. 1.

```
1:Input: the set of nodes \(V:=\{v_{1},,v_{n}\}\) and number of nodes \(n\).
2:Decompose \(n\) as \(n=n_{1} n_{L}\) with minimum \(L\) such that \(n_{l}[k+1]\) for all \(l[L]\).
3:for\(l[L]\)do
4: Initialize \(b_{i}\) to zero for all \(i[n]\) and \(E^{(l)}\) to \(\).
5:for\(i[n]\)do
6:for\(m[n_{l}]\)do
7:\(j(i+m_{l^{}=1}^{l^{}-1}n_{l^{ }}-1,n)+1\).
8:if\(b_{i}<n_{l}-1\) and \(b_{j}<n_{l}-1\)then
9: Add edge \((v_{i},v_{j})\) with weight \(}\) to \(E^{(l)}\) and \(b_{i} b_{i}+1\).
10:return\((E^{(1)},E^{(2)},,E^{(L)})\). ```

**Algorithm 1**\(k\)-peer Hyper-hypercube Graph \(_{k}(V)\)

### Simple Base-\((k+1)\) Graph

As described in Sec. 4.1, when \(n\) do not have prime factors larger than \(k+1\), we can easily make all nodes reach the exact consensus by the \(k\)-peer Hyper-hypercube Graph. However, when \(n\) has prime factors larger than \(k+1\), e.g., when \((k,n)=(1,5)\), the \(k\)-peer Hyper-hypercube Graph cannot be constructed. In this section, we extend the \(k\)-peer Hyper-hypercube Graph and propose the Simple Base-\((k+1)\) Graph, which is infinite-time convergence for any number of nodes \(n\) and maximum degree \(k\). Note that the maximum degree of the Simple Base-\((k+1)\) Graph is not \(k+1\), but at most \(k\).

We present the pseudo-code for constructing the Simple Base-\((k+1)\) Graph in Alg. 2. For simplicity, here we explain only the case when the maximum degree \(k\) is one. The case with \(k 2\) is explained in Sec. B. The Simple Base-\((k+1)\) Graph mainly consists of the following four steps. The key idea is that splitting \(V\) into disjoint subsets to which the \(k\)-peer Hyper-hypercube Graph is applicable.

**Step 1.**: As in the base-\(2\) number of \(n\), we decompose \(n\) as \(n=2^{p_{1}}++2^{p_{L}}\) in line 1, and then split \(V\) into disjoint subsets \(V_{1},,V_{L}\) such that \(|V_{l}|=2^{p_{l}}\) for all \(l[L]\) in line \(3\).3
**Step 2.**: For all \(l[L]\), we make all nodes in \(V_{l}\) obtain the average of parameters in \(V_{l}\) using the \(1\)-peer Hyper-hypercube Graph \(_{1}(V_{l})\) in line 11. Then, we initialize \(l^{}\) as one.
**Step 3.**: Each node in \(V_{l^{}+1} V_{L}\) exchanges parameters with one node in \(V_{l^{}}\) such that the average in \(V_{l^{}}\) becomes equivalent to the average in \(V\). We increase \(l^{}\) by one and repeat step \(3\) until \(l^{}=L\). This procedure corresponds to line 15.
**Step 4.**: For all \(l[L]\), we make all nodes in \(V_{l}\) obtain the average in \(V_{l}\) using the \(1\)-peer Hyper-hypercube Graph \(_{1}(V_{l})\). Because the average in \(V_{l}\) is equivalent to the average in \(V\) after step \(3\), all nodes can reach the exact consensus. This procedure corresponds to line 25.

Using the example presented in Fig. 3, we explain Alg. 2 in more detail. Let \(G^{(1)},,G^{(5)}\) denote the graphs in Fig. 3 from left to right, respectively. In step \(1\), we split \(V\{1,,5\}\) into\(V_{1}\{1,,4\}\) and \(V_{2}\{5\}\). In step \(2\), nodes in \(V_{1}\) obtain the average in \(V_{1}\) by exchanging parameters in \(G^{(1)}\) and \(G^{(2)}\). In step \(3\), the average in \(V_{1}\) becomes equivalent to the average in \(V\) by exchanging parameters in \(G^{(3)}\). In step \(4\), nodes in \(V_{1}\) can get the average in \(V\) by exchanging parameters in \(G^{(4)}\) and \(G^{(5)}\). Because node \(5\) also obtains the average in \(V\) after exchanging parameters in \(G^{(3)}\), all nodes reach the exact consensus after exchanging parameters in \(G^{(5)}\).

Note that edges added in lines 20 and 27 are not necessary if we only need to make all nodes reach the exact consensus. Nonetheless, these edges are effective in keeping the parameters of nodes close in value to each other in decentralized learning because the parameters are updated by gradient descent before the parameter exchange with neighbors. For instance, edge \((1,2)\) in \(G^{(3)}\), which is added in line 20, is not necessary for finite-time convergence because nodes \(1\) and \(2\) already have the same parameter after exchanging parameters in \(G^{(1)}\) and \(G^{(2)}\). We provide more examples in Sec. C.

```
1:Input: the set of nodes \(V\) and number of nodes \(n(=a_{1}(k+1)^{p_{1}}+a_{2}(k+1)^{p_{2}}++a_{L}(k+1)^{p_{L}})\) such that \(p_{1}>p_{2}>>p_{L} 0\) and \(a_{l}[k]\) for all \(l[L]\).
2:If all prime factors of \(n\) are less than or equal to \(k+1\) then return\(_{k}(V)\).
3:Split \(V\) into disjoint subsets \(V_{1},,V_{L}\) such that \(|V_{l}|=a_{l}(k+1)^{p_{l}}\) for all \(l[L]\). Then, for all \(l[L]\), split \(V_{l}\) into disjoint subsets \(V_{l,1},,V_{l,a_{l}}\) such that \(|V_{l,a_{l}}|=(k+1)^{p_{l}}\) for all \(a[a_{l}]\).
4:Construct \(k\)-peer Hyper-hypercube Graph \(_{k}(V_{l})\) for all \(l[L]\) and \(m_{1}=|_{k}(V_{1})|\).
5:Construct \(k\)-peer Hyper-hypercube Graph \(_{k}(V_{l,a})\) for all \(l[L]\) and \(a[a_{l}]\).
6:Initialize \(b_{l}\) as zero for all \(l[L]\), and initialize \(m\) as zero.
7:while\(b_{l}<|_{k}(V_{1,1})|\)do
8:\(m m+1\) and \(E^{(m)}\).
9:for\(l\{L,L-1,,1\}\)do
10:if\(m m_{1}\)then
11: Add \(E(_{k}(V_{l})^{(m^{})})\) to \(E^{(m)}\) where \(m^{}=(m-1,|_{k}(V_{l})|)+1\).
12:elseif\(m<m_{1}+l\)then
13:for\(v V_{l}\)do
14: Select isolated node \(u_{1},,u_{a_{m-m_{1}}}\) from \(V_{m-m_{1},1},,V_{m-m_{1},a_{m-m_{1}}}\).
15: Add edges \((v,u_{1})\), \(\), \((v,u_{a_{m-m_{1}}})\) with weight \(}|}{a_{m-m_{1}}_{l=m-m_{1}}^{L}|V_{l}|}\) to \(E^{(m)}\).
16:elseif\(m=m_{1}+l\) and \(l L\)then
17:while There are two or more isolated nodes in \(V_{l}\)do
18:\(c\) the number of isolated nodes in \(V_{l}\).
19: Select \(\{k+1,c\}\) isolated nodes \(V^{}\) in \(V_{l}\).
20: Add edges with weights \(|}\) to \(E^{(m)}\) such that \(V^{}\) compose the complete graph.
21:else
22:\(b_{l} b_{l}+1\).
23:if\(p_{l} 0\)then
24:for\(a[a_{l}]\)do
25: Add \(E(_{k}(V_{l,a})^{(m^{})})\) to \(E^{(m)}\) where \(m^{}=(b_{l}-1,|_{k}(V_{l,a})|)+1\).
26:else
27: Add \(E(_{k}(V_{l})^{(m^{})})\) to \(E^{(m)}\) where \(m^{}=(b_{l}-1,|_{k}(V_{l})|)+1\).
28:return\((E^{(1)},E^{(2)},,E^{(m)})\). ```

**Algorithm 2** Simple Base-\((k+1)\) Graph \(_{k}^{}(V)\)

Figure 3: Simple Base-\(2\) Graph with \(n=5(=2^{2}+1)\). The value on the edge is the edge weight, and the edges are colored in the same color as the line in Alg. 2 where they were added.

### Base-\((k+1)\) Graph

The Simple Base-\((k+1)\) Graph is finite-time convergence for any \(n\) and \(k\), while the Simple Base-\((k+1)\) Graph contains graphs that are not necessary for the finite-time convergence and becomes a redundant sequence of graphs in some cases, e.g., \((k,n)=(1,6)\) (see the example in Fig. 3(b) and a detailed explanation in Sec. C.2). To remove this redundancy, this section proposes the Base-\((k+1)\) Graph that can make all nodes reach the exact consensus after fewer iterations than the Simple Base-\((k+1)\) Graph.

The pseudo-code for constructing the Base-\((k+1)\) Graph is shown in Alg. 3. The Base-\((k+1)\) Graph consists of the following three steps.

1. We decompose \(n\) as \(p q\) such that \(p\) is a multiple of \(2,,(k+1)\) and \(q\) is prime to \(2,,(k+1)\), and split \(V\) into disjoint subsets \(V_{1},,V_{p}\) such that \(|V_{l}|=q\) for all \(l[p]\).
2. For all \(l[p]\), we make all nodes in \(V_{l}\) reach the average in \(V_{l}\) by the Simple Base-\((k+1)\) Graph \(_{k}^{}(V_{l})\). Then, we take \(p\) nodes from \(V_{1},,V_{p}\) respectively and construct a set \(U_{1}\). Similarly, we construct \(U_{2},,U_{q}\) such that \(U_{1},,U_{q}\) are disjoint sets.
3. For all \(l[q]\), we make all nodes in \(U_{l}\) reach the average in \(U_{l}\) by the \(k\)-peer Hyper-hypercube Graph \(_{k}(U_{l})\). Because the average in \(U_{l}\) is equivalent to the average in \(V\) after step \(2\), all nodes reach the exact consensus.

Using the example in Fig. 3(a), we explain the Base-\((k+1)\) Graph in more detail. Let \(G^{(1)},,G^{(4)}\) denote the graphs in Fig. 3(a) from left to right, respectively. In step \(1\), we split \(V\) into \(V_{1}\{1,2,3\}\) and \(V_{2}\{4,5,6\}\). In step \(2\), nodes in \(V_{1}\) and nodes in \(V_{2}\) have the same parameter respectively by

Figure 4: Comparison of Simple Base-\(2\) Graph and Base-\(2\) Graph with \(n=6\). The value on the edge indicates the edge weight. The edges added in line 10 in Alg. 3 are colored black, and the edges added in line 6 are colored the same color as the line in Alg. 2 where they are added.

exchanging parameters on \(G^{(1)},,G^{(3)}\) because the subgraphs composed on \(V_{1}\) and \(V_{2}\) are same as the Simple Base-\((k+1)\) Graph (see Fig. 13(a)). Then, we construct \(U_{1}\{1,4\}\), \(U_{2}\{2,5\}\), and \(U_{3}\{3,6\}\). Finally, in step \(3\), all nodes reach the exact consensus by exchanging parameters in \(G^{(4)}\).

Fig. 5 and Sec. F.1 compare the Base-\((k+1)\) Graph with the Simple Base-\((k+1)\) Graph, demonstrating that the length of the Base-\((k+1)\) Graph is less than that of the Simple Base-\((k+1)\) Graph in many cases. Moreover, Theorem 1 show the upper bound of the length of the Simple Base-\((k+1)\) Graph and Base-\((k+1)\) Graph. The proof is provided in Sec. D.

**Theorem 1**.: _For any number of nodes \(n\) and maximum degree \(k[n-1]\), the length of the Simple Base-\((k+1)\) Graph and Base-\((k+1)\) Graph is less than or equal to \(2_{k+1}(n)+2\)._

**Corollary 1**.: _For any number of nodes \(n\) and maximum degree \(k[n-1]\), the Simple Base-\((k+1)\) Graph and Base-\((k+1)\) Graph are \((_{k+1}(n))\)-finite time convergence._

Therefore, the Base-\((k+1)\) Graph is a powerful extension of the \(1\)-peer exponential  and \(1\)-peer hypercube graphs  because they are \((_{2}(n))\)-finite time convergence only if \(n\) is a power of 2 and their maximum degree cannot be set to any number other than \(1\).

## 5 Decentralized SGD on Base-\((k+1)\) Graph

In this section, we verify the effectiveness of the Base-\((k+1)\) Graph for decentralized learning, demonstrating that the Base-\((k+1)\) Graph can endow DSGD with both a faster convergence rate and fewer communication costs than the existing topologies, including the ring, torus, and exponential graph. We consider the following decentralized learning problem:

\[_{^{d}}[f()_{i=1}^ {n}f_{i}()],\ \ f_{i}()_{_{i}_{i}}[F_{i}( ;_{i})],\]

where \(n\) is the number of nodes, \(f_{i}\) is the loss function of node \(i\), \(_{i}\) is the data distribution held by node \(i\), \(F_{i}(;_{i})\) is the loss of node \(i\) at data sample \(_{i}\), and \( F_{i}(;_{i})\) denotes the stochastic gradient. Then, we assume that the following hold, which are commonly used for analyzing decentralized learning methods [18; 20; 23; 43].

**Assumption 1**.: _There exists \(f^{}>-\) that satisfies \(f() f^{}\) for any \(^{d}\)._

**Assumption 2**.: \(f_{i}\) _is \(L\)-smooth for all \(i[n]\)._

**Assumption 3**.: _There exists \(^{2}\) that satisfies \(_{_{i}_{i}}\| F_{i}(;_{i})- f _{i}()\|^{2}^{2}\) for all \(^{d}\)._

**Assumption 4**.: _There exists \(^{2}\) that satisfies \(_{i=1}^{n}\| f_{i}()- f()\|^{2} ^{2}\) for all \(^{d}\)._

We consider the case when DSGD , the most widely used decentralized learning method, is used as an optimization method. Let \(^{(1)},,^{(m)}\) be mixing matrices of the Base-\((k+1)\) Graph. In DSGD on the Base-\((k+1)\) Graph, node \(i\) updates its parameter \(_{i}\) as follows:

\[_{i}^{(r+1)}=_{j=1}^{n}W_{ij}^{(1+(r,m))}(_{j} ^{(r)}- F_{j}(_{j}^{(r)};_{j}^{(r)})),\] (1)

where \(\) is the learning rate. In this case, thanks to the property of finite-time convergence, DSGD on the Base-\((k+1)\) Graph converges at the following convergence rate.

**Theorem 2**.: _Suppose that Assumptions 1-4 hold. Then, for any number of nodes \(n\) and maximum degree \(k[n-1]\), there exists \(\) such that \(}_{i=1}^{n}_{i}\) generated by Eq. (1) satisfies \(_{r=0}^{R}\| f(}^{(r)})\| ^{2}\) after_

\[R=(}{n^{2}}+(n)+ (n)}}{^{3/2}}+(n)}{} ) LF_{0}\] (2)

_iterations, where \(F_{0} f(}^{(0)})-f^{}\)._

Figure 5: Comparison of length.

The above theorem follows immediately from Theorem 2 stated in Koloskova et al.  and Corollary 1. The convergence rates of DSGD over commonly used topologies are summarized in Sec. E. From Theorem 2 and Sec. E, we can conclude that for any number of nodes \(n\), the Base-2 Graph enables DSGD to converge faster than the ring and torus and as fast as the exponential graph, although the maximum degree of the Base-\(2\) Graph is only one. Moreover, if we set the maximum degree \(k\) to the value between \(2\) to \(_{2}(n)\), the Base-\((k+1)\) Graph enables DSGD to converge faster than the exponential graph, even though the maximum degree of the Base-\((k+1)\) Graph remains less than that of the exponential graph. It is worth noting that if we increase the maximum degree of the \(1\)-peer exponential and \(1\)-peer hypercube graphs (i.e., \(k\)-peer exponential and \(k\)-peer hypercube graphs with \(k 2\)), these topologies cannot enable DSGD to converge faster than the exponential graph because these topologies are no longer finite-time convergence even when the number of nodes is a power of \(2\).

## 6 Experiments

In this section, we validate the effectiveness of the Base-\((k+1)\) Graph. First, we experimentally verify that the Base-\((k+1)\) Graph is finite-time convergence for any number of nodes in Sec. 6.1, and we verify the effectiveness of the Base-\((k+1)\) Graph for decentralized learning in Sec. 6.2.

Figure 6: Comparison of consensus rates among various topologies. The number in the bracket indicates the maximum degree of a topology. Because the maximum degree of the exponential graph depends on \(n\), the three numbers in the bracket indicate the maximum degree for each \(n\).

Figure 7: Test accuracy (%) of DSGD on various topologies with \(n=25\). The number in the bracket indicates the maximum degree of a topology. We also compared with dense variants of the \(1\)-peer {U, D}-EquiDyn  in Sec. F.3.1, showing the superior performance of the Base-\((k+1)\) Graph.

### Consensus Rate

**Setup.** Let \(x_{i}\) be the parameter that node \(i\) has, and let \(_{i=1}^{n}x_{i}\). For each \(i\), the initial value of \(x_{i}\) was drawn from Gaussian distribution with mean \(0\) and standard variance \(1\). Then, we evaluated how the consensus error \(_{i=1}^{n}(x_{i}-)^{2}\) decreases when \(x_{i}\) is updated as \(x_{i}_{j=1}^{n}W_{ij}x_{j}\) where \(\) is the mixing matrix associated with a given topology.

**Results.** Figs. 1 and 6 present how the consensus errors decrease on various topologies. The results indicate that the Base-\((k+1)\) Graph reaches the exact consensus after a finite number of iterations, while the other topologies only reach the consensus asymptotically. Moreover, as the maximum degree \(k\) increases, the Base-\((k+1)\) Graph reaches the exact consensus with fewer iterations. We also present the results when \(n\) is a power of 2 in Sec. F.2, demonstrating that the \(1\)-peer exponential graph can reach the exact consensus as well as the Base-\(2\) Graph, but requires more iterations than the Base-\(4\) Graph.

### Decentralized Learning

Next, we examine the effectiveness of the Base-\((k+1)\) Graph in decentralized learning.

**Setup.** We used three datasets, Fashion MNIST , CIFAR-\(\{10,100\}\), and used LeNet  for Fashion MNIST and VGG-11  for CIFAR-\(\{10,100\}\). Additionally, we present the results using ResNet-18  in Sec. G. The learning rate was tuned by the grid search and we used the cosine learning rate scheduler . We distributed the training dataset to nodes by using Dirichlet distributions with hyperparameter \(\), conducting experiments in both homogeneous and heterogeneous data distribution settings. As \(\) approaches zero, the data distributions held by each node become more heterogeneous. We repeated all experiments with three different seed values and reported their averages. See Sec. H for more detailed settings.

**Results of DSGD on Various Topologies.** We compared various topologies combined with the DSGD with momentum , showing the results in Fig. 7. From Fig. 7, the accuracy differences among topologies are larger as the data distributions are more heterogeneous. From Fig. 7b, the Base-\(\{2,3,4,5\}\) Graph reach high accuracy faster than the other topologies. Furthermore, comparing the final accuracy, the final accuracy of the Base-\(2\) Graph is comparable to or higher than that of the existing topologies, including the exponential graph.

Moreover, the final accuracy of the Base-\(\{3,4,5\}\) Graph is higher than that of all existing topologies. From Fig. 7a, the accuracy differences among topologies become small when \(=10\); however, the Base-\(5\) Graph still outperforms the other topologies. In Fig. 8, we present the results in cases other than \(n=25\), demonstrating that the Base-\(2\) Graph outperforms the \(1\)-peer exponential graph and the Base-\(\{3,4,5\}\) Graph can consistently outperform the exponential and \(1\)-peer exponential graphs for all \(n\). In Sec. F.3.2, we show the learning curves and the comparison of the consensus rate when \(n\) is \(21\), \(22\), \(23\), \(24\), and \(25\).

**Results of \(D^{2}\) and QG-DSGDm on Various Topologies.** The above results demonstrated that the Base-\((k+1)\) Graph outperforms the existing topologies, especially when the data distributions

Figure 8: Test accuracy (%) of DSGD with CIFAR-10 and \(=0.1\).

Figure 9: Test accuracy (%) of \(D^{2}\) and QG-DSGDm with CIFAR-10, \(n=25\), and \(=0.1\).

performs the exponential graph. Thus, the Base-\((k+1)\) Graph is useful not only for DSGD but also for \(D^{2}\) and QG-DSGDm and then enables these methods to achieve a reasonable balance between accuracy and communication efficiency.

## 7 Conclusion

In this study, we propose the Base-\((k+1)\) Graph, a novel topology with both a fast consensus rate and small maximum degree. Unlike the existing topologies, the Base-\((k+1)\) Graph is finite-time convergence for any number of nodes and maximum degree \(k\). Thanks to this favorable property, the Base-\((k+1)\) Graph enables DSGD to obtain both a faster convergence rate and more communication efficiency than the existing topologies, including the ring, torus, and exponential graph. Through experiments, we compared the Base-\((k+1)\) Graph with various existing topologies, demonstrating that the Base-\((k+1)\) Graph enables various decentralized learning methods to more successfully reconcile accuracy and communication efficiency than the existing topologies.