# LoRA-GA: Low-Rank Adaptation with Gradient Approximation

Shaowen Wang

wangsw23@mails.tsinghua.edu.cn &Linxi Yu

yulx23@mails.tsinghua.edu.cn

&Jian Li

lijian83@mail.tsinghua.edu.cn

Tsinghua University

Beijing, China

Corresponding author

###### Abstract

Fine-tuning large-scale pretrained models is prohibitively expensive in terms of computational and memory costs. LoRA, as one of the most popular Parameter-Efficient Fine-Tuning (PEFT) methods, offers a cost-effective alternative by fine-tuning an auxiliary low-rank model that has significantly fewer parameters. Although LoRA reduces the computational and memory requirements significantly at each iteration, extensive empirical evidence indicates that it converges at a considerably slower rate compared to full fine-tuning, ultimately leading to increased overall compute and often worse test performance. In our paper, we perform an in-depth investigation of the initialization method of LoRA and show that careful initialization (without any change of the architecture and the training algorithm) can significantly enhance both efficiency and performance. In particular, we introduce a novel initialization method, LoRA-GA (**Low Rank** **A**daptation with **G**radient **A**pproximation), which aligns the gradients of low-rank matrix product with those of full fine-tuning at the first step. Our extensive experiments demonstrate that LoRA-GA achieves a convergence rate comparable to that of full fine-tuning (hence being significantly faster than vanilla LoRA as well as various recent improvements) while simultaneously attaining comparable or even better performance. For example, on the subset of the GLUE dataset with T5-Base, LoRA-GA outperforms LoRA by 5.69% on average. On larger models such as Llama 2-7B, LoRA-GA shows performance improvements of 0.34, 11.52%, and 5.05% on MT-bench, GSM8K, and Human-eval, respectively. Additionally, we observe up to 2-4 times convergence speed improvement compared to vanilla LoRA, validating its effectiveness in accelerating convergence and enhancing model performance. Code is available at code.

## 1 Introduction

Fine-tuning large language models (LLMs) is essential for enabling advanced techniques such as instruction fine-tuning , reinforcement learning from human feedback (RLHF) , and adapting models to specific downstream applications. However, the computational and storage costs associated with full fine-tuning are prohibitively high, particularly as model sizes continue to grow. To address these challenges, methods of Parameter-Efficient Fine-Tuning (PEFT) (see e.g., ), such as Low-Rank Adaptation (LoRA) , have emerged and gained significant attention.

Instead of updating the parameters of the model directly, LoRA incorporates auxilary low-rank matrices \(B\) and \(A\) into the linear layers of models (such as the \(Q,K,V\), and \(O\) matrices in a self-attention block ), while keeping the original layer weights \(W\) fixed. The modified layer is represented as \(y=(W+ BA)x\), where \(x\) is the input of that layer, \(y\) is the output, and \(\) is the scaling factor. This approach significantly reduces the number of parameters that need to be fine-tuned, thereby lowering the computational and memory costs at each step.

Despite these benefits, extensive empirical evidence (see e.g., [6; 7; 8; 9]) shows that LoRA converges significantly slower compared to full finetune. This slower convergence often increases overall computational costs (measured in Floating Point Operations) and can sometimes lead to worse test performance. In our experiments, we typically observe that LoRA requires 5-6x more iterations and FLOPs to reach the same performance as full fine-tuning under the same learning rate, as shown in Figure 1.

To study the cause of slow convergence, we perform an in-depth investigation of the initialization strategy of LoRA's adapter weights. It is known that fine-tuning pretrained models using the same objective (e.g., language modeling) often converges faster than re-initializing new parameters (e.g., a classification head) . This observation leads us to question whether the slow convergence of vanilla LoRA might be attributed to the default random initialization of adapter weights (LoRA initializes \(A\) using Kaiming initialization  and sets \(B\) to zero ). In our experiments, we find that different initialization strategies for LoRA can significantly impact the results, and its default initialization is suboptimal.

In pursuit of a convergence rate comparable to full fine-tuning, we aim for initialization so that the update of \(BA\) matches the update of \(W\) closely. Previous work suggests that gradient descent operates in a low-dimensional subspace [12; 13]. If we can closely approximate the gradients of the full model at the initial step, subsequent steps can also be approximated, potentially accelerating the convergence of LoRA.

To this end, we introduce a novel initialization method, LoRA-GA (**Low Rank Gradient Approximation**). By initializing \(A_{}\) and \(B_{}\) with the eigenvectors of the full gradient matrix, the gradient of the low-rank product \(BA\) aligns with the direction of the gradient of the full weight matrix \(W\). Mathematically, we aim to ensure that:

\[(BA) W,.\]

**Our contributions can be summarized as follows:**

**1.**: We propose LoRA-GA, a novel initialization method for LoRA that accelerates convergence by approximating the gradients of the low-rank matrices with ones of the full weight matrix.

Figure 1: **(Left) Training loss curves of Llama 2-7B on MetaMathQA to training steps. LoRA-GA converges as quickly as full fine-tuning and outperforms LoRA. (Right) Initialization procedures used in LoRA and LoRA-GA. The key difference is that LoRA-GA initializes adapters using the eigenvectors of the gradient matrix, as opposed to random initialization with a scaling factor.**

**2.** We identify the scaling factor under non-zero initialization, which ensures the variance of adapter outputs is invariant to the rank of the adapter and the dimension of the input.

**3.** We validate LoRA-GA through extensive experiments, demonstrating significant performance improvements and faster convergence compared to vanilla LoRA. Specifically, LoRA-GA outperforms LoRA by 5.69% on the GLUE  subset with T5-Base , and by 0.34, 11.52%, and 5.05% on MT-bench , GSM8K , and HumanEval  with Llama 2-7B , respectively, while achieving up to 2-4 times faster convergence.

## 2 Related Work

### Initialization

The significance of maintaining variance stability during initialization has been widely acknowledged to prevent the occurrence of diminishing or exploding phenomena. Xavier initialization  ensures stability in both the forward and backward passes of a network under a linear activation function. He initialization  extends this solution to networks using ReLU activation. Distinct from these, LSUV initialization  selects a mini-batch of data, performing a forward pass to determine the output variance, and subsequently normalizing it to ensure stability. Tensor program (see e.g., ) has emerged as a powerful technique for tuning various hyperparameters, including the initialization, for large models.

### Parameter-Efficient Fine-Tuning (PEFT)

To fine-tune increasingly large language models within limited hardware resources, researchers have developed various Parameter-Efficient Fine-Tuning (PEFT) methods. Adapter-based methods [23; 24; 25; 26] incorporate new layers into existing model layers. While fine-tuning only these inserted layers significantly reduces resource consumption and requires much fewer parameters, this approach introduces additional latency during both forward and backward passes. Soft Prompt-based methods [10; 27; 28; 29; 30] prepend learnable soft tokens to the model's input to adapt the model to specific tasks. This approach effectively leverages the pre-trained model's capabilities, requiring only appropriate prompts for task adaptation, though it incurs computational overhead during inference. More broadly, GaLore  applies low-rank gradients to parameter updates for memory efficiency during training. While this approach is highly expressive and performant, it requires storing complete model checkpoints, consuming more storage than other PEFT methods.

### LoRA's Variants

LoRA is one of the most popular PEFT methods that introduces the product of low-rank matrices alongside existing layers to approximate weight changes during fine-tuning. Several methods have been proposed to improve the structure of LoRA. AdaLoRA  dynamically prunes insignificant weights during fine-tuning using SVD, allowing more rank allocation to important areas within a fixed parameter budget. DoRA  enhances the model's expressiveness by adding learnable magnitudes to the direction adjustments made by low-rank matrix products. Additionally, LoHA  and LoKr  employ Hamiltonian and Kronecker products, respectively.

Despite these advancements, vanilla LoRA remains the most popular method due to its robust library and hardware support. Therefore, improving LoRA without altering its structure and at a low cost is crucial. Several recent methods focus on this aspect. ReLoRA  suggests periodically merging learned adapters into the weight matrices to enhance LoRA's expressibility. LoRA+  proposes using different learning rates for the two matrices in LoRA to improve convergence. rsLoRA  introduces a new scaling factor to make the scale of the output invariant to rank. Although our stable scale approach appears similar to rsLoRA, rsLoRA assumes \(BA=0\) initialization, making \(r\) invariant to the update \( BA\). In contrast, our stable scale ensures that non-zero initialized \(BA\) remains invariant to both rank and input dimension from the start.

Recently, PiSSA  proposes to initializing \(A\) and \(B\) to approximate the original matrix \(W\), by performing SVD on \(W\). Our method, however, is based on a very different idea, that is to approximate the gradient of \(W\), which involves performing SVD on sampled gradients and properly scaling the initialized matrices, as detailed in Section E.

Methods

In this section, we analyze the initialization of LoRA and introduce our method, LoRA-GA. LoRA-GA consists of two key components: (i) approximating the direction of the gradient of full finetune and (ii) ensuring rank and scale stability in the initialization process. We examine each component and subsequently present their integration within LoRA-GA.

### Review of Vanilla LoRA

Structure of LoRABased on the hypothesis that the updates of fine-tuning are low-rank , LoRA  proposes to use the product of two low-rank matrices to represent the incremental part of the original matrix \(W\). Here, \(W\) is the weight matrix of a linear layer in the model. For example, in transformers, it could be the \(Q,K,V\), or \(O\) matrices of the self-attention layer or the weight matrix in the MLP layer. Specifically, LoRA has the following mathematical form:

\[W^{}=W_{0}+ W=W_{0}+BA:=W_{0}+ BA\]

where \(W^{},W_{0}^{m n}\), \(B^{m r}\), and \(A^{r n}\), with \(r(m,n)\). \(W_{0}\) is the pre-trained weight matrix, remains frozen during the fine-tuning process, while \(A\) and \(B\) are trainable.

Initialization of LoRAUnder LoRA's default initialization scheme [4; 39], matrix \(A\) is initialized using Kaiming uniform , while matrix \(B\) is initialized with all zeros. Consequently, \(BA=0\) and \(W_{0}^{}=W_{0}\), ensuring that the initial parameters are unchanged.

If the additional term \( W= BA\) is initially non-zero (e.g., ), the frozen parameter can be adjusted to ensure the initial parameters unchanged. This can be expressed as:

\[W^{}=(W_{0}- B_{}A_{})+ BA:=W_{ }+ BA\]

where \(W_{}=W_{0}- B_{}A_{}\) is frozen, and \(B\) and \(A\) are trainable in this case.

### Gradient Approximation

Our goal is to ensure that the first-step update \(( BA)\) approximate the direction of the weight update \( W\), i.e., \(( BA) W\) for some non-zero positive constant \(\). We will discuss how to choose \(\) in Section 3.3 and one can treat \(\) as a fixed constant for now.

Consider a gradient descent step with learning rate \(\), the updates for \(A\) and \(B\) are \( A=_{A}(A_{})\) and \( B=_{B}(B_{})\), respectively. Assuming learning rate \(\) is small, the update of \( BA\) at the first step can be expressed as:

\[( BA_{}+B_{} A)=[_{ B}(B_{})A_{}+B_{} _{A}(A_{})]\]

To measure its approximation quality of scaled the update of the weights in full finetune \( W=_{W}(W_{0})\), we use the Frobenius norm of the difference between these two updates as a criterion:

\[\|( BA_{}+B_{} A)- _{W}(W_{0})\|_{F}\] (1) \[= \|_{B}(B_{} )A_{}+ B_{}_{A}(A_{ })-_{W}(W_{0})\|_{F}\]

**Lemma 3.1**.: _Suppose the loss function is \(\) and \(y=W^{}x=(W_{0}+ BA)x\), where \(y\) is the output of a layer and \(x\) is the input, the gradients of \(A\) and \(B\) are linear mappings of the gradient of \(W^{}\):_

\[_{A}=B^{T}_{W^{}},_{B} =(_{W^{}})A^{T}\]

_Remarkably, \(_{W^{}}\) in LoRA and \(_{W}\) in full fine-tuning are equal at the beginning of the training._

By substituting the gradients in Lemma 3.1 into Equation 1, we can rewrite the criterion as follows:

\[\|^{2}_{W^{}}(W_{0}) A_ {}^{T}A_{}+^{2}B_{}B_{ }^{T}_{W}(W_{0})-_{W} (W_{0})\|_{F}\] (2)

This criterion evaluates how well the adapter's gradient approximates the direction of the gradient of full fine-tuning, and minimizing it brings the gradient of LoRA closer to that of full fine-tuning with a scaling factor \(\):

\[_{A_{},B_{}}\|^{2}_{W}  A_{}^{T}A_{}+^{2}B_{}B_{ }^{T}_{W}-_{W}\| _{F}\] (3)

**Theorem 3.1**.: _For the optimization problem in Equation 3 with given \(\), if the Singular Value Decomposition (SVD) of \(_{W}\) is \(_{W}=USV^{T}\), the solution is:_

\[B_{}=}{}U_{I_{A}}, A_{}= }{}V_{I_{B}}^{T},|I_{A}|=|I_{B}|=r,\,I_{A} I_{B}=\{i 1  i 2r,i\}\]

_where \(I_{A}\) and \(I_{B}\) are index sets._

Theorem 3.1 provides an appropriate initialization scheme for \(A_{}\) and \(B_{}\) given a specific \(\). The selection of \(\), which influences the scaling of the update \( BA\), will be discussed in the following section.

### Scale Stability

Inspired by rsLoRA citekalajdzievski2023rank and the Kaiming initialization , we define the following notions of stability:

**Definition 3.1**.: _When \(d_{out},d_{in},r\), an adapter \( BA\) exhibits two distinct types of scale stabilities:_

_1. **Forward stability**: If the inputs to the adapter are independently and identically distributed (i.i.d.) with 2nd moment \(_{r,d_{out},d_{in}}(1)\), then the 2nd moment of the outputs remains \(_{r,d_{out},d_{in}}(1)\)._

_2. **Backward stability**: If the gradient of the loss with respect to the adapter outputs is \(_{r,d_{out},d_{in}}(1)\), then the gradient with respect to the inputs remains \(_{r,d_{out},d_{in}}(1)\)._

**Theorem 3.2**.: _Given the initialization proposed in Theorem 3.1, assume that the orthogonal vectors in \(A_{}\) and \(B_{}\) are randomly selected from the unit spheres in \(^{d_{in}}\) and \(^{d_{out}}\) with the constraint that the vectors are orthogonal to each other, and \(=_{r,d_{out},d_{in}}(1/)\) as suggested by rsLoRA . Under these conditions, the adapters are forward scale-stable if \(=_{r,d_{out},d_{in}}(/r^{2}})\) and backward scale-stable if \(=_{r,d_{out},d_{in}}(/r^{2}})\)._

Similar to the results obtained from Kaiming Initialization , we observe that either \(=_{r,d_{out},d_{in}}(/r^{2}})\) or \(=_{r,d_{out},d_{in}}(/r^{2}})\) work well independently. For all models presented in this paper, either form ensures convergence. Consequently, for all subsequent experiments, we adopt \(=_{r,d_{out},d_{in}}(/r^{2}})\).

**Remark**.: _We would like to remark that the scaling factor proposed in this subsection proves to be beneficial primarily when one adopts the learning rate typically used in full-finetuning (e.g., \(1e-5\)), since as LoRA-GA attempts to approximate the updates of full-finetuning. However, recent research  suggests that LoRA with default initialization performs much better with larger learning rates. Furthermore, tensor program analysis [40; 22] indicates that higher learning rates should be paired with smaller initialization magnitudes. Therefore, we recommend decreasing or omitting the scaling factor when training using larger learning rates (e.g., \(>1e-4\))._

### LoRA-GA Initialization

Combining the gradient approximation and stable scale components, we propose the LoRA-GA initialization method. First, we initialize \(A_{}\) and \(B_{}\) using the solution from Theorem 3.1. Then, we determine the scaling factor \(\) according to Theorem 3.2 to ensure rank and scale stability. Thus, based on Theorems 3.1 and 3.2, we propose a novel initialization method, LoRA-GA.

LoRA-GA :We adopt \(=}\) and \(=}{^{2}}}{r^{2}}}\), where \(\) is a hyperparameter. We define the index sets \(I_{A}=\{i 1 i r,i\}\) and \(I_{B}=\{i r+1 i 2r,i\}\). Denote the singular value decomposition (SVD) of \(_{W}\) as \(_{W}=USV^{T}\). The initializations are as follows:

\[A_{}=}}{}V_{[1:r]}^{T}, B_{ }=}}{}U_{[r+1:2r]}, W_{}=W_{0}- B_{}A_{}\]To save GPU memory during LoRA-GA initialization, we utilized a technique similar to . By hooking into PyTorch's backward process, we compute the gradient for one layer at a time and discard the computed gradients immediately. This ensures that our memory usage remains at \(O(1)\) instead of \(O(L)\), where \(L\) is the number of layers. This approach allows the memory consumption during the initialization phase to be less than that during the subsequent LoRA finetuning phase. Our algorithm is shown in Algorithm 1. If the sampled batch size is large, we can also use gradient accumulation to save memory further, as shown in Algorithm 2.

```
0: Model \(f()\) with \(L\) layers, parameters \(W\), sampled batch \(B=\{x,y\}\), LoRA rank \(r\), LoRA alpha \(\), loss function \(\), scale factor \(\)
0: Initialized parameters \(W\), \(\), \(A\), \(B\)
1:\( f(x,W)\)\(\) Forward pass
2:\((y,)\)
3:\(}\)
4:for\(l=L,,1\)do
5: Compute \(_{W_{l}}\)\(\) Backward for one layer
6:\(d_{out},d_{in}(W_{l})\)
7:\(U,S,V(_{W_{l}})\)
8:\(A_{l} V_{[1:r]}/}\)
9:\(B_{l} U_{[r+1:2r]}/}\)
10:\(W_{l} W_{l}- B_{l}A_{l}\)
11: Clear \(_{W_{l}}\)\(\) Gradient for this layer is not needed anymore
12:endfor
13:return\(W\), \(\), \(A\), \(B\) ```

**Algorithm 1** LoRA-GA Initialization

## 4 Experiments

In this section, we evaluate the performance of LoRA-GA on various benchmark datasets. Initially, we assess Natural Language Understanding (NLU) capabilities using a subset of the GLUE dataset  with the T5-Base model . Subsequently, we evaluate dialogue [16; 42], mathematical reasoning [17; 43], and coding abilities [18; 44] using the Llama 2-7B model . Finally, we do the ablation study to prove the effectiveness of our method.

BaselinesWe compare LoRA-GA with several baselines to demonstrate its effectiveness:

**1.**_Full-Finetune_: Fine-tuning the model with all parameters, which requires the most resources.

**2.**_Vanilla LoRA_: Fine-tuning the model by inserting a low-rank matrix product \(BA\) into linear layers. \(A\) is initialized using Kaiming initialization, while \(B\) is initialized to zero.

**3.**_LoRA Variants with Original Structure_: This includes several methods that retain the original LoRA structure:

- _rsLoRA_ introduces a new scaling factor to stabilize the scale of LoRA.

- _LoRA+_ updates the two matrices in LoRA with different learning rates.

- _PiSSA_ proposes performing SVD on the weight matrix \(W\) at the beginning of training and initializing \(A\) and \(B\) based on the components with larger singular values.

**4.**_LoRA Variants with Modified Structure_: This includes methods that modify the original LoRA structure:

- _DoRA_ enhances the model's expressiveness by adding learnable magnitudes.

- _AdaLoRA_ dynamically prunes insignificant weights during fine-tuning using SVD, allowing more rank allocation to important areas within a fixed parameter budget.

### Experiments on Natural Language Understanding

Models and DatasetsWe fine-tune the T5-Base model on several datasets from the GLUE benchmark, including MNLI, SST-2, CoLA, QNLI, and MRPC. Performance is evaluated on the development set using accuracy as the primary metric.

Implementation DetailsWe utilize prompt tuning to fine-tune the T5-Base model on the GLUE benchmark. This involves converting labels into tokens (e.g., "positive" or "negative") and using the normalized probability of these tokens as the predicted label probability for classification. We provide the hyperparameters in Appendix D.1. Each experiment is conducted with 3 different random seeds, and the average performance is reported.

ResultsAs shown in Table 1, LoRA-GA consistently outperforms the original LoRA and other baseline methods, achieving performance comparable to full fine-tuning. Notably, LoRA-GA excels on smaller datasets such as CoLA and MRPC, demonstrating its ability to converge faster and effectively utilize limited training data.

### Experiment on Large Language Model

Models and DatasetsTo evaluate the scalability of LoRA-GA, we train Llama 2-7B on three tasks: _chat_, _math_, and _code_.

**1.**_Chat_: We train our model on a 52k subset of WizardLM , filtering out responses that begin with "As an AI" or "Sorry". We test our model on the MT-Bench dataset , which consists of 80 multi-turn questions designed to assess LLMs on multiple aspects. The quality of the responses is judged by GPT-4, and we report the first turn score.

**2.**_Math_: We train our model on a 100k subset of MetaMathQA , a dataset bootstrapped from other math instruction tuning datasets like GSM8K and MATH , with higher complexity and diversity. We select data bootstrapped from the GSM8K training set and apply filtering. Accuracy is reported on the GSM8K evaluation set.

**3.**_Code_: We train our model on a 100k subset of Code-Feedback , a high-quality code instruction dataset, removing explanations after code blocks. The model is tested on HumanEval , which consists of 180 Python tasks, and we report the PASS@1 metric.

Implementation DetailsOur model is trained using standard supervised learning for language modelling. The loss for the input prompt is set to zero. Detailed hyperparameters can be found in Appendix D.2. Each experiment uses 3 different random seeds, and the average performance across these runs is reported.

ResultOur results, as summarized in Table 2, indicate that LoRA-GA outperforms or is comparable to other methods, including full-finetuning. Specifically, LoRA-GA achieves superior performance on both the GSM8K and Human-eval datasets, underscoring its effectiveness in handling tasks with higher complexity and diversity. On MT-Bench, LoRA-GA also demonstrates competitive performance, although it slightly trails behind DoRA. Nevertheless, LoRA-GA achieves this with fewer parameters and approximately 70% of the training time required by DoRA. Additionally, as illustrated in Figure 2 (Left), our method exhibits a significantly faster convergence rate compared to Vanilla LoRA, with convergence rates comparable to those of full-finetuning.

    & **MNLI** & **SST-2** & **CoLA** & **QNLI** & **MRPC** & **Average** \\ Size & 393k & 67k & 8.5k & 105k & 3.7k & \\  Full & \(86.33_{ 0.00}\) & \(94.75_{ 0.21}\) & \(80.70_{ 0.24}\) & \(93.19_{ 0.22}\) & \(84.56_{ 0.73}\) & \(87.91\) \\ LoRA & \(85.30_{ 0.04}\) & \(94.04_{ 0.11}\) & \(69.35_{ 0.05}\) & \(92.96_{ 0.09}\) & \(68.38_{ 0.01}\) & \(82.08\) \\  PiSSA & \(85.75_{ 0.07}\) & \(94.07_{ 0.06}\) & \(74.27_{ 0.39}\) & \(93.15_{ 0.14}\) & \(76.31_{ 0.51}\) & \(84.71\) \\ rsLoRA & \(85.73_{ 0.10}\) & \(_{ 0.23}\) & \(72.32_{ 1.12}\) & \(93.12_{ 0.09}\) & \(52.86_{ 2.27}\) & \(79.64\) \\ LoRA+ & \(_{ 0.09}\) & \(93.85_{ 0.24}\) & \(77.53_{ 0.20}\) & \(93.14_{ 0.03}\) & \(74.43_{ 1.39}\) & \(84.95\) \\  DoRA & \(85.67_{ 0.09}\) & \(94.04_{ 0.53}\) & \(72.04_{ 0.94}\) & \(93.04_{ 0.06}\) & \(68.08_{ 0.51}\) & \(82.57\) \\ AdaLoRA & \(85.45_{ 0.11}\) & \(93.69_{ 0.20}\) & \(69.16_{ 0.24}\) & \(91.66_{ 0.05}\) & \(68.14_{ 0.28}\) & \(81.62\) \\  LoRA-GA & \(85.70_{ 0.09}\) & \(94.11_{ 0.18}\) & \(_{ 0.20}\) & \(_{ 0.06}\) & \(_{ 0.24}\) & \(\) \\   

Table 1: Results of fine-tuning T5-base using Full-FT and various LoRA variants on a subset of GLUE.

Effect of RankWe attribute the performance discrepancies on the GSM8K and Human-eval datasets, when compared to full-finetuning, primarily to the representational limitations imposed by the low-rank approximation. To address this, we experimented with higher ranks, specifically rank=32 and rank=128. Our findings reveal that LoRA-GA maintains stability across different rank settings and, in some cases, even surpasses full-finetuning performance. As shown in Figure 2 (Left), higher ranks with our initialization also result in loss curves that closely resemble those of full-finetuning.

### Ablation Study

We conducted ablation studies to evaluate the contributions of non-zero initialization, stable output, and gradient approximation in LoRA-GA using five distinct experimental settings. Details of each setting are provided in Table 3.

    & **MT-Bench** & **GSM8K** & **Human-eval** & **Average of GLUE** \\  Full & \(5.56_{ 0.09}\) & \(54.20_{ 0.42}\) & \(19.87_{ 0.57}\) & \(87.91\) \\ LoRA & \(5.61_{ 0.10}\) & \(42.08_{ 0.04}\) & \(14.76_{ 0.17}\) & \(82.08\) \\  Gaussian & \(5.62_{ 0.11}\) & \(38.21_{ 0.06}\) & \(14.76_{ 0.68}\) & \(81.88\) \\ + SO & \(5.72_{ 0.04}\) & \(42.81_{ 1.14}\) & \(15.55_{ 0.78}\) & \(82.28\) \\ + GA & \(5.48_{ 0.02}\) & \(46.65_{ 1.17}\) & \(16.15_{ 0.78}\) & \(82.54\) \\ LoRA-GA & \(5.95_{ 0.16}\) & \(53.60_{ 0.30}\) & \(19.81_{ 1.46}\) & \(87.77\) \\   

Table 4: Performance of different settings in the ablation study. Results are shown for MT-Bench, GSM8K, and Human-eval on Llama 2 7b, as well as the average performance on a subset of GLUE on T5-Base. Detailed results can be found in Table 9.

   Method & \(A\) Initialization & \(B\) Initialization & \(\) \\  LoRA & \(U(-}},}})\) & 0 & \(/r\) \\ Gaussian & \(N(0,})\) & \(N(0,})\) & \(/r\) \\ +SO & \(}/ N(0,})\) & \(}/ N(0,})\) & \(/\) \\ +GA & \(V_{[1:r]}\) & \(U_{[r+1:2r]}\) & \(/r\) \\ LoRA-GA & \(V_{[1:r]}}/\) & \(U_{[r+1:2r]}}/\) & \(/\) \\   

Table 3: Initialization Methods and Corresponding Settings for Ablation Study. The table compares different initialization methods for LoRA and their settings for \(A\), \(B\), and \(\). ”+SO” denotes stable output, scaling parameters appropriately to ensure stability. ”+GA” refers to gradient approximation, where \(A\) and \(B\) are initialized using orthogonal matrices derived from singular value decomposition.

    & **MT-Bench** & **GSM8K** & **Human-eval** \\  Full & \(5.56_{ 0.09}\) & \(54.20_{ 0.42}\) & \(19.87_{ 0.57}\) \\ LoRA & \(5.61_{ 0.10}\) & \(42.08_{ 0.04}\) & \(14.76_{ 0.17}\) \\  PiSSA & \(5.30_{ 0.02}\) & \(44.54_{ 0.27}\) & \(16.02_{ 0.78}\) \\ rsLoRA & \(5.25_{ 0.03}\) & \(45.62_{ 0.10}\) & \(16.01_{ 0.79}\) \\ LoRA+ & \(5.71_{ 0.08}\) & \(52.11_{ 0.62}\) & \(18.17_{ 0.52}\) \\  DoRA & \(_{ 0.02}\) & \(53.07_{ 0.75}\) & \(19.75_{ 0.41}\) \\ AdaLoRA & \(5.57_{ 0.05}\) & \(50.72_{ 1.39}\) & \(17.80_{ 0.44}\) \\  LoRA-GA & \(5.95_{ 0.16}\) & \(_{ 0.30}\) & \(_{ 1.46}\) \\ LoRA-GA (Rank=32) & \(5.79_{ 0.09}\) & \(55.12_{ 0.30}\) & \(20.18_{ 0.19}\) \\ LoRA-GA (Rank=128) & \(6.13_{ 0.07}\) & \(55.07_{ 0.18}\) & \(23.05_{ 0.37}\) \\   

Table 2: Results of fine-tuning Llama 2-7b using Full-FT and various LoRA variants, tested on MT-Bench, GSM8K, and Human-eval. LoRA-GA significantly outperforms Vanilla LoRA and approaches the performance of Full Finetune. Unless otherwise specified, the LoRA rank is set to 8.

Ablation ResultThe results are presented in Tables 4 and 9. For both small and large models, we observe that simply changing LoRA's initialization to Gaussian does not yield any performance gains and may result in a slight performance decline. However, when combined with either "+SO" (Stable Output) or "+GA" (Gradient Approximation), performance improves upon that of LoRA. LoRA-GA, which integrates both techniques, outperforms other methods. As shown in Figure 2 (Left) and Figure 4, +SO and +GA also enhance convergence speed, and when both are combined, the training loss curve is even closer to that of full-finetuning. This indicates that both output stability and gradient approximation contribute to the improvement of LoRA, each addressing different aspects of the model's performance.

### Memory Costs and Running Time

We benchmark LoRA-GA on a single RTX 3090 24GB GPU, a 128-core CPU, and 256GB of RAM. As shown in Table 5, the memory consumption of our new method does not exceed that used for training with LoRA, indicating no extra memory is needed. Additionally, the time cost of this operation is relatively negligible compared to the subsequent fine-tuning process. For instance, in the Code-Feedback task, the training process took approximately 10 hours, while the initialization required only about 1 minute, which is insignificant.

### Performance with Different Index Set Schemas

Theorem 3.1 establishes multiple optimal initialization schemes through different choices of index sets \(I_{A}\) and \(I_{B}\). While our primary experiments employed \(I_{A}=1,,r\) and \(I_{B}=r+1,,2r\), we conducted additional experiments to validate this choice by comparing three schemes:

* **ArB2r**: \(I_{A}=\{1,,r\},I_{B}=\{r+1,,2r\}\)
* **A2rBr**: \(I_{A}=\{r+1,,2r\},I_{B}=\{1,,r\}\)

    & Parameters & Time(LoRA-GA) & Memory(LoRA-GA) & LoRA & Full-FT \\  T5-Base & 220M & 2.8s & 1.69G & 2.71G & 3.87G \\ Llama 2-7B & 6738M & 74.7s & 18.77G & 23.18G & 63.92G \\   

Table 5: Memory and Time Costs for Initialization and Fine-Tuning. ”Parameters” indicates the number of parameters in the model, ”Time(LoRA-GA)” represents the time required for initialization, ”Memory(LoRA-GA)” shows the memory usage during initialization, ”LoRA” and ”Full-FT” display the memory usage during LoRA and full fine-tuning, respectively.

Figure 2: **(Left)** Training loss curves of LoRA-GA with different ranks on the MetaMathQA dataset. Higher ranks result in faster loss reduction, approaching the performance of full fine-tuning. **(Right)** Training loss curves from the ablation study with different settings on the MetaMATHQA dataset. Compared to Vanilla LoRA, both components of LoRA-GA, +SO (stable output) and +GA (gradient approximation), improve convergence speed. LoRA-GA achieves the fastest convergence, closely matching that of full fine-tuning.

* **Random**: Random assignment of first \(2r\) indices into two groups

As shown in Table 6, ArB2r slightly outperforms the alternatives. While Theorem 3.1 proves these schemas are equivalent in the first step, their behaviors diverge afterward. The gradient of matrix \(B\) (\(_{B}=(_{W})A^{T}\)) becomes larger than that of \(A\) (\(_{A}=B^{T}_{W}\)), effectively increasing \(B\)'s learning rate. This aligns with findings from LoRA+, where larger learning rates for \(B\) proved beneficial, potentially explaining ArB2r's superior performance.

### Impact of Sampled Batch Size

The gradient approximation in LoRA-GA uses sampled batches, with smaller batches resembling Stochastic Gradient Descent (SGD) and larger ones approximating full Gradient Descent (GD). While theoretical work  suggests SGD's slower convergence may offer better generalization than GD, we conduct experiments to empirically evaluate different batch sizes.

We assess gradient approximation quality by comparing gradients from various batch sizes against a reference batch size of 2048 which serves as a proxy for the full dataset gradient using two metrics:

* **Sign Similarity**: The proportion of parameters sharing the same gradient sign.
* **Magnitude Similarity**: The proportion of parameters within the same order of magnitude (where one's absolute value is not more than 10 times the other).

As shown in Table 7, both similarity metrics consistently improve with larger batch sizes, indicating better approximation of the full gradient. The results also demonstrate that while larger batch sizes tend to yield marginally better performance, however, the differences are relatively small. Based on these findings, we recommend using a moderately large batch size (e.g., 64) when computational resources permit.

## 5 Conclusions

In this paper, we present a novel initialization scheme for low-rank adaptation (LoRA), with the goal of accelerating its convergence. By examining the initialization methods and update processes of LoRA, we develop a new initialization method, LoRA-GA, which approximates the gradients of the low-rank matrix product with those of full fine-tuning from the very first step.

Through extensive experiments, we have demonstrated that LoRA-GA achieves a convergence rate comparable to that of full fine-tuning while delivering similar or even superior performance. Since LoRA-GA solely modifies the initialization of LoRA without altering the architecture or training algorithms, it offers an efficient and effective approach that is easy to implement. Furthermore, it can also be incorporated with other LoRA variants. For example, ReLoRA  periodically merges the adapters into frozen weights \(W\), which may allow LoRA-GA to demonstrate its advantages over more steps. We leave it as an interesting future direction.

    & ArB2r & A2rBr & Random \\  Performance & 52.79 & 52.38 & 52.01 \\   

Table 6: Performance comparison of initialization schemes on GSM8k using models trained on MetaMathQA subset.

   Batch Size & 8 & 16 & 32 & 64 & 128 & 256 \\  Sign Similarity & 0.743 & 0.790 & 0.838 & 0.875 & 0.903 & 0.925 \\ Magnitude Similarity & 0.878 & 0.908 & 0.933 & 0.950 & 0.962 & 0.971 \\ Performance & 52.79 & 52.99 & 52.91 & 53.56 & 52.57 & 53.22 \\   

Table 7: Gradient similarity metrics (vs. batch size 2048) and model performance on GSM8k using models trained on MetaMathQA subset.