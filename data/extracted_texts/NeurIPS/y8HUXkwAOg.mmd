# ChronoEilogi: Scalable Time-Series Variable

Selection with Multiple Solutions

 Etienne Vareille\({}^{1}\) Michele Linardi\({}^{1}\) Ioannis Tsamardinos\({}^{2}\) Vassilis Christophides\({}^{1}\)

\({}^{1}\)ETIS UMR-8051 Laboratory, CY Cergy Paris Universite, ENSEA, CNRS

\({}^{2}\)Computer Science Department, University of Crete, Heraklion, Greece

###### Abstract

We consider the problem of selecting all the minimal-size subsets of multivariate time-series (TS) variables whose past leads to an optimal predictive model for the future (forecasting) of a given target variable (multiple feature selection problem for times-series). Identifying these subsets leads to gaining insights, domain intuition, and a better understanding of the data-generating mechanism; it is often the first step in causal modeling. While identifying a single solution to the feature selection problem suffices for forecasting purposes, identifying all such minimal-size, optimally predictive subsets is necessary for knowledge discovery and important to avoid misleading a practitioner. We develop the theory of multiple feature selection for time-series data, propose the ChronoEilogi algorithm, and prove its soundness and completeness under two mild, broad, non-parametric distributional assumptions, namely _Compositionality_ of the distribution and _Interchangeability_ of time-series variables in solutions. Experiments on synthetic and real datasets demonstrate the scalability of ChronoEilogi to hundreds of TS variables and its efficacy in identifying multiple solutions. In the real datasets, ChronoEilogi is shown to reduce the number of TS variables by 96% (on average) by conserving or even improving forecasting performance. Furthermore, it is on par with Group Lasso performance, with the added benefit of providing multiple solutions.

## 1 Introduction

In analysis tasks involving TS with hundreds or even tens of thousands of variables (e.g., in manufacturing, environmental monitoring, energy grids, etc.), selecting appropriate series for TS forecasting not only has the potential to improve models' performance, but also to gain intuition, discover new knowledge, and understand the data-generating mechanism (causal structure). _Time-series variable selection_ (TVS) is defined as the problem of discovering _a minimal-size subset of the available, measured TS variables whose past values optimally predict the future of a target TS variable_\(T\). Such a series subset is called **a Markov Boundary** (MB) of \(T\) in the causal discovery literature , denoted as \(()\). Hence, \((T)\) filters out the TS variables that are both _irrelevant_ and _redundant_ in forecasting \(T\).

Identifying a small-size \(\) leads to a computationally more efficient model for \(T\) when real-time predictions are required. Moreover, it facilitates all subsequent analysis operations such as modeling, explanation calculations (e.g., SHAP values, feature importance), visualizations, and interpretations . Perhaps most importantly, identifying the \(\) of every series could be the first step in _causal modeling_ of the data-generating mechanism. Indeed, under broad causal assumptions, the \((T)\) contains the observable direct causes of \(T\), i.e., the quantities that could causally influence and optimize the future values of \(T\). It is not an accident then that variable selection is the first step in several causal discovery algorithms for time-series or cohort data . However, we note that the \((T)\) may also contain variables confounded by latent ones (in general, variables connected to \(T\) by a collider path). Distilling the possible direct causes of \(T\) out of its\((T)\) requires further analysis with causal discovery algorithms, which is outside the scope of our work (readers are referred to  for a recent survey).

Both in theory and in practice, there may be _numerous Markov Boundaries of \(T\)_. This is, for example, the case if a series subset \(\{X,Y,Z\}\) is 1-to-1 deterministically related with another subset \(\{A,B\}\), in which case, either subset could substitute for the other in an optimal forecasting model. We call such subsets **informationally equivalent series**. Equivalent series define an equivalence class. The total number of Markov Boundaries of \(T\) is exponential to the number of equivalence classes to which the members of \((T)\) belong. This is because we can construct a new \((T)\) by picking any of its subsets and substituting it with an equivalent one from its equivalence class. Determinism is not the only reason for multiple MBs in practice: when the sample size is finite it may be impossible to distinguish the true \((T)\) from some other subset that leads to a forecasting model of statistically indistinguishable performance. The presence of an (often exponential) number of multiple MBs has been established in cohort, cross-sectional data . For time-series data there have not been any algorithms that return multiple solutions to the TVS problem .

Identifying all subsets \((T)\) (referred to as the _multiple TVS (MTVS) problem_) is crucial. While finding any one \((T)\) suffices for forecasting, this is insufficient for knowledge discovery and model interpretation. A practitioner may find it misleading to construct an optimal and minimal-size forecasting model by filtering out certain series although there exists another \((T)\) that contains them. In essence, multiple \((T)\)s indicate the presence of multiple causal models and explanations that fit the data equally well. Moreover, if two series \(X\) and \(Y\) are informationally equivalent w.r.t. forecasting \(T\), a variable selection algorithm may inconsistently select between them during cross-validation or bootstrapping - arbitrarily returning either \(X\) or \(Y\) at each step. Feature Importance XAI methods may suffer from similar instability, on top of already known misleading interpretations. For instance, Shapley values currently suffer from the inclusion of unrealistic data instances when features are correlated, even for linear models .

In this paper, we define the novel problem of _multiple time-series variable selection_ (MTVS), we introduce the concepts of TS informational equivalence and provide a taxonomy to characterize TS variables w.r.t. to their presence in multiple \((T)\): _indispensable_, variables that belong in all \((T)\), _replaceable_, variables that belong in some \((T)\) but could be replaced by some other series in the same equivalence class, _redundant_, variables that are informative but do not belong in any \((T)\), and _irrelevant_, variables that are completely uninformative. We describe the property of _Compositionality_ of the data distribution that allows the construction of _sound_ yet greedy MTVS algorithms. Additionally, we define the property of _Interchangeability_, which specifies that any two \((T)\)s can be decomposed into pairs of equivalent single variables. Under the assumption of Interchangeability, one variable can be substituted with another variable to create a new MB. This property enables the design of algorithms that are _complete_ in identifying all MBs without requiring exhaustive enumeration. This is particularly valuable in data distributions with an exponential number of MBs.

We proceed with designing a MTVS algorithm, named **ChronoEpilogi12** that is sound and complete in terms of identifying and returning all MBs of \(T\), under Composition, Interchangeability, and other broad, non-parametric assumptions. ChronoEpilogi extends previous variable selection algorithms designed for cohort data  to a TS forecasting setting and the identification of multiple solutions (MBs). It returns a reference \((T)\) and a list of equivalence classes. Each of the classes contains TS that are informationally equivalent and could substitute one for the other to construct another MB and an equally-performing forecasting model. Experiments on synthetic data demonstrate that ChronoEpilogi has _near perfect average causal f1-score, with stable performance as the MTS dimensionality increases_ to thousands of available TS. Moreover, the greedy approximation of the ideal ChronoEpilogi achieves a 70% speedup with only a 0.05 decrease in f1 score for causal discovery (_Claim 1_). Furthermore, ChronoEpilogi variants _are on par with Group Lasso performance_ (_Claim 4_) -arguably the most scalable algorithm for the single solution TVS problem-, with the added benefit of providing multiple solutions in real datasets from the Monash archive  like Electricity and Traffic (_Claim 3_). On both real and synthetic datasets the multiple solutions produced by ChronoEpilogi have similar forecasting performance to the unique solution produced by GroupLasso when averaging across all selected targets (_Claim 5_) while when causal ground truth is available ChronoEpilogi outperforms GroupLasso in terms of causal f1-score (_Claim 2_). Finally, both variable selection algorithms actually conserve or even improve the models' performance compared to models trained on the original MTS (_Claim 6_). We also investigate how SHAP explanations of regression models might misrepresent the role of variables belonging to some but not all Markov boundaries of the modeled target. We claim that the SHAP importance of each equivalent set of variables is distributed among equivalent variables, hence leading to underestimations of the importance of equivalent sets when considered individually (Claim 7). This attribution is unstable: on different data splits, different variables among an equivalence set obtain high importance (Claim 8).

## 2 Preliminaries

We represent a multivariate time series by a set of univariate TS \(=\{X^{1},...,X^{N}\}\), where each \(X^{i}^{M}\) (\(1 i N\)) are regularly sampled observations of a time series variable. We denote with \(X^{i}_{t}\) the \(t^{th}\) occurrence of \(X^{i}\), and by \(X^{i}_{-L,t-1}^{L}\) the last \(L\) occurrences of \(X^{i}\) before the instant \(t\). Note that, in the following part, bold letters (e.g., \(\)) refer to sets of TS variables. The TS forecasting task we consider is to predict the values of a target TS variable \(T_{t}\) (\(T=X^{i}\) for some \(i\)), where \(L t M\), with a model \(f\) parameterized by \(\) that uses up until \(L\) past timestamps (for each \(t\)) of the multivariate TS \(\). Hence, \(T_{t}==f_{}(_{t-L,t-1})\).

Conditional independence of two TS variables \(X_{t}\) and \(Y_{t^{}}\) given a third one \(Z_{t^{}}\), written as \(X_{t^{}} Y_{t^{}}|Z_{t^{}}\), is defined as \(P(X_{t},Y_{t^{}}|Z_{t^{}})=P(X_{t}|Z_{t^{}}).P( _{t^{}}|Z_{t^{}})\). In our work, we assume stationarity, namely the conditional distribution of \(T_{t}\) as a function of \(L\) previous lags of \(\) does not depend on \(t\). Stationarity implies _temporal consistency_, where the conditional independence relations do not rely on \(t\). In this respect, we consider the principle of _temporal precedence_ according to which causes (i.e., independent variables) occur before an effect (i.e., outcome). We exclusively test conditional independence relations between a target \(T_{t}\) and variables with timestamps ranging between \(t-L\) and \(t-1\). Due to stationarity, we can remove the index and denote \(X_{t-L,t-1}\) as \(X\).

**Definition 2.1** (Information Equivalence (IEQ) ).: Two TS variables \(X_{a}\) and \(Y_{b}\) are information equivalent (shorthand: equivalent) with respect to a target \(T_{t}\) given conditioning TS variable set \(\), noted \(X_{a}_{T}Y_{b}|\), iff they are made independent of \(T_{t}\) by conditioning on the other and on \(\) itself. Formally: \(X_{a}_{T}Y_{b}| X_{a}\!\!\!\! T_{t}|Y_{b}\), \(\) and \(Y_{b}\!\!\!\! T_{t}|X_{a},\). We write shortly \(X_{a}_{T}Y_{b}\) when the conditioning set is \(Z=\{\}\).

**Definition 2.2** (Markov Boundary \((T_{t})\)).: Given a set of TS variables \(\) and a target variable \(T\) in this set, \((T_{t})\) is a _Markov blanket_ of \(T_{t}\) iff: \(T_{t}\!\!\!()|\). The set \((T_{t})\) is a _Markov boundary_ of \(T_{t}\) iff \(^{},T_{t}\!\!\!\!( ^{})|^{}\).

**Definition 2.3** (Multiple Time-series variable selection (MTVS)).: Let \((T_{t})\) a reference MB of a target \(T_{t}\) TS variable. The solution of the MTVS problem consists into finding all MBs, denoted by \(\). All \(MB(T_{t})\) are information equivalent. For all \(_{i}(T_{t})\) the forecasting models \(T_{t}=f_{}(_{i}(T_{t}))\) are equally-performing according to a metric (e.g., \(R^{2}\)).

**Example 2.1**.: We consider a hypothetical water flow monitoring system involving two rivers \(A,B\) and a small hydroelectric station on river \(B\). Let MTS \(X,Z\) be the inflows of the two confluent rivers \(A\) and \(B\). The dam is controlled such that if \(Z_{t} z_{th}\), it does not produce energy. Otherwise, it diverts a flow \(z_{th}\) to power production. The power production flow mixes with river \(A\) first for total flow \(Y_{t}=f(Z_{t})+X_{t}\), then with the rest of river \(B\) for total flow \(T_{t}=X_{t}+Z_{t}\), with \(f(Z_{t})\{0,z_{th}\}\). In this situation, \(X\) and \(Y\) are deterministically related only when taking account \(Z\), and information equivalent for \(T\) given \(Z\) only. Both \(X,Z\) and \(Y,Z\) are MB of \(T\). In this case, solving the MTVS problem relates to discovering both subsets without misidentifying singletons and \(X,Y\) as MB.

TS variables selected by a MTVS algorithm can be characterized according to whether they belong to all, at least one, or none of the Markov boundaries of a target \(T_{t}\). A variable \(X_{a}\) is said _irreplaceable_ iff it is part of all information equivalent Markov boundaries of \(T_{t}\), i.e., \(X_{a}(T_{t})(T_{t})\). A variable \(X_{a}\) is said _replaceable_ iff it is part of at least one \((T_{t})\) but not indispensible, i.e., \((T_{t}),^{}(T_{t}),X_{a} (T_{t}) X_{a}^{}(T_{t})\). A variable \(X_{a}\) is _redundant_ iff \(\{X_{a}\},X_{a}\!\!\!\! T_{t}| \) and \(_{i}(T_{t}),X_{a}_{i}(T_{t})\) while it is called _irrelevant_ iff \(\{X_{a}\},X_{a}}T_{t}|\).

If _Causal Markov Condition_ and _Faithfulness_ hold, each target \(T_{t}\) has the guarantee to have a unique Markov Boundary, which is also the unique solution of the MTVS problem. However, common probability distributions in real settings might violate faithfulness , hence the \((T_{t})\) might not be unique. Jointly Gaussian random variables (RV) with a singular covariance matrix or with deterministic relations do no respect faithfulness 3. While faithfulness does not hold, the weaker _Composition_ property relaxes the structure of faithful data. As a matter of fact, faithfulness implies composition, while the opposite relation does not necessarily hold .

_Assumption 2.1_ (Composition).: For any subset of RVs \(,,\) and conditioning set \(\):

\[[origin={c}]{0.0pt}{$\!\!\!$} |[origin={c}]{0.0pt}{$\!\!\!$} |[origin={c}]{0.0pt}{$ \!\!\!$}|\] (1)

Composition is a general property of the joint probability distribution of a set of RVs regardless of their temporal context, hence we dropped index \(t\) from formal definitions. The reciprocal property is called _Decomposition_, and it is always true in any probability distribution. Many common probability distributions that violate faithfulness actually satisfy composition.

**Example 2.2**.: Consider a TS \(\) of \(n\) covariates for which composition holds. Any _deterministic transformation_ of \(\), namely \(=f()\) of size \(m\), where \(Y_{i}=f_{i}(X_{(i)})\), with \(f_{i}\) invertible and \(_{i}\) a mapping from \([1,m]\) to \([1,n]\), also satisfies composition.

**Example 2.3**.: Jointly Gaussian distributions  also satisfy composition, as they are a special case where pairwise independence is equivalent to mutual independence. If independence relations \([origin={c}]{0.0pt}{$\!\!\!$} |\) and \([origin={c}]{0.0pt}{$\!\!\!$} |\) hold, the union \(\) contains only RVs that are pairwise independent from each variable in \(\) given \(\). This implies \([origin={c}]{0.0pt}{$\!\!\! $}|\).

For probability distributions where all information equivalences are caused by invertible deterministic transformations between individual RVs (singletons), the MBs of any target are _interchangeable_: the variables are equivalent regardless of the conditioning set. More generally, we define interchangeability for two Markov Boundaries, where each variable in a MB(T) is equivalent to a variable in the other MB(T) conditioned on the remaining MB(T) variables. We assume that all MB are interchangeable.

_Assumption 2.2_ (Interchangeability).: Two MBs of a target \(T_{t}\), \((T_{t})\) and \(^{}(T_{t})\) are interchangeable iff:

\[(T_{t}),^{}(T_{t}),  X(T_{t}), Y^{}(T_{t}),(\{X\}^{}(T_{t})\{Y\})\]

**Example 2.4**.: In the water flow example 2.1, \(Z\) is irreplaceable, and \(X\) and \(Y\) are replaceable by each other. The markov boundary structure \(=(\{Z,X\},\{Z,Y\})\) satisfies Interchangeability, as \(=\{Z\}\{X,Y\}\) is a cartesian product of equivalence classes.

## 3 The ChronoEpilogi Algorithm

In this section, we present the details of our MTVS algorithm, named ChronoEpilogi. We propose two versions of the algorithm: (1) Forward Backward Equivalent (FBE - Algorithm 1 ) and (2) a computationally optimized version (approximate) named Forward Equivalent (FE - Algorithm 2).

```
1:TS \(\), target \(T\), max lag \(L\), threshold params [\(,,\)]
2:set \(\) FORWARD(\(,T,L\),\(\))
3:\(\) BACKWARD(\(,T,L\),\(\),\(\))
4:set \(\) EQUIV(\(,T,L\),\(\),\(\))
5:return\(\)\(\) set of eq. Markov bound. ```

**Algorithm 1** ChronoEpilogi-FBE

In FBE, we first select informative TS random variables (RV) for predicting the values of a target \(T\), adopting a greedy heuristic (FORWARD routine, line 1). Then, a backward phase iteratively removes redundant RVs from the selection (BACKWARD routine, line 2). Lastly, equivalent Markov boundaries are discovered (EQUIV routine, line 3) by checking if any of the selected RVs can be replaced by another one. In ChronoEpilogi-FE, we select equivalent Markov boundaries during informative RV selection (FORW-EQUIV routine, line 1). Such choice permits us to compute conditional independences on smaller TS variable sets. To further reduce time complexity, we also approximate the search of equivalent Markov boundaries using forecasting model residuals. In Example 2.1, the forward phase might select an upstream redundant variable \(U\) first, then \(Z\) and \(X\) before terminating. The backward phase would test \(UT|X,Z\), removing \(U\) from the selected set. Finally, the equivalence phase would test \(Y_{T}X|Z\) and produce the solution space \(\).

We evaluate ChronoEplogi considering AutoRegressive Distributed Lags (ARDL) (linear) forecasting model . An ARDL model of orders \(p,q\) uses lags of both the target (\(T\)) and other TS RVs (\(\)) as predictors, with \(T_{t}=a_{0}+_{i=1}^{p}a_{i}.T_{t-i}+_{i=1}^{k}_{j=1}^{q}b_{j}.X_{t- j}^{i}+_{t}\). The model includes autoregressive terms \((a_{j})\) and other explanatory variable terms (\(b_{j}\)). It is generally required that \(_{t}\) is an id centered normal noise, but this assumption can be relaxed as we can build estimators in the presence of autocorrelated noise and noise with heteroscedastic components . Under sufficient assumptions, an ARDL model can be estimated using Ordinary Least Squares (OLS) procedures, and correspond to a Maximum Likelihood estimation model. Consequently, this estimation has the advantages of the OLS method, with guaranteed convergence and fast computation.

Hereafter, we detail sub-routines of ChronoEplogi. The _forward_ phase (Algorithm 3) iteratively builds a first selected set starting from the past \(L\) lags of \(T\) (line 1), then incrementally selects new RVs of \(\). At each iteration, a selected variable (\(S_{new}\)) is the one maximizing the statistical importance of Pearson correlation between all windows of length \(L\) (the predictors) and the residuals of model \(m\) (line 5). Algo. 7 (see Appendix) contains the correlation computation pseudo-code, which iterates all the windows in a candidate variable \(X^{}\) (line 2) to compute p-values of Pearson correlation with the forecasting model residuals. The selection in the FORWARD routine terminates when the last forecasting model (built over \(\)) is not statistically better than the previous one (line 9). We use Likelihood ratio test , as they suit ARDL models. The _backward_ phase (Algorithm 4) iteratively removes redundant RVs from the selected RVs in the _forward_ phase. The main loop of the algorithm tests each one of these RVs. It stops as soon as removing any variable degrades the predictive performance of the _best-so-far_ forecasting models (line 3), ensuring that the produced variable set is minimal and equally predictive as the _forward_ phase solution. We prove that the forward and backward phases provide an exact solution of the MTVS problem (See appendix D.1,D.2). The _equivalent_ search phase (Algorithm 5) tests the equivalence between each selected TS variable \(\) (line 2), and any non-selected variable \(S^{d}\) (line 4). In line 5 we compare the equivalence replacing the two Rvs in a new forecasting model. If this latter is comparable with the baseline, we store the equivalent variable sets in dictionary \(Q\) (line 6).

Once the equivalent Markov Boundaries \(\) are obtained, we can characterize irreplaceable variables and replaceable variables, as irreplaceable variables are the unique members of their equivalence class. Conversely, non-unique variables in their equivalence class are replaceable. In Section D (see Appendix), we provide soundness and completeness proofs of the multiple solutions computed by Algorithm 5. In Algorithm 6 we report the pseudocode of FORW_EQUIV routine, which is used in FE to select informative variables and to estimate equivalent sets over forecasting model residuals. Algorithm 8 in Appendix details the residual-based equivalence search (FIND-EQUIVALENCES) employed during the forward phase (Algorithm 6, line 10). Such a routine tests statistical redundancy by modeling residuals with model \(g_{}(.)\). In this sense, if a non-selected variable \(X\) is equivalent to the tested variable, \(S_{new}\) is added to the equivalent set (line 7).

```
1:TS \(\), target \(T\), time pred. \(t\), max lag \(L\), sel. variables \(\), equivalence threshold \(\)
2:dictionary \(Q\{:\}\)
3:for\(S\)do
4:\(Q[S]\{\}\)
5:for\(S^{d}()\)do
6:ifpvalue of LR test between \(f_{}(\{S^{d}\}\{S\})\) and \(f_{}(\{S^{d}\})\)then
7:\(Q[S] Q[S]\{S^{d}\}\)
8:\(\{S_{1}\} Q[S_{1}]...\{S_{n}\} Q[S_{n}]\)
9:return\(\) ```

**Algorithm 6** FORW_EQUIV

Complexity analysisGiven a TS \(^{N M}\), and denoted by \(\) the selected TS variables in the forward phase, both ChronoEpilogi versions (\(FBE\) and \(FE\)) run \((N||)\) conditional tests. In the FORWARD phase, the sub-routine Lag-Pearson-pval (Algorithm 7) runs in \((L\ n\ log(n))\) if Fast Fourier Transform is adopted , where \(L\) is the number of lags to predict a target and \(n=M-L\) the size of residuals (number of predicted targets). Fitting an ARDL model through matrix inversion requires \((|S|^{3}\ L^{3}\ M)\). Overall, the forward, equivalent phases take \((|S|\ N\ L\ M+|S|^{4}\ L^{3}\ M)\), \((|S|^{5}\ L^{3}\ M)\), \((|S|^{4}\ L^{3}\ N\ M)\) time respectively.

The two variants of ChronoEpilogi, satisfy different properties. FBE (Alg. 1) is an ideal version of our algorithm with provable soundness and completeness under mild assumptions. FE (Alg. 2) is a greedy approximation of FBE. To simplify the proofs of FBE theoretical guarantees we rely on a practical commonly made in causal discovery algorithms that the conditional independence tests are correct 4. We should stress this requires considering different statistical tests for different distributions . We prove that FBE is **sound** as any set of TS variables returned as a solution in \(\) is a Markov blanket. In other words, our algorithm does not return false positives. Wethen examine the conditions under which FBE is **complete**, i.e. all Markov boundaries are solutions in \(\) discovered by the algorithm. This corresponds to the lack of false negatives in the equivalent Markov boundary discovery. Theorems proofs are given in Appendix D.

**Theorem 3.1** (Soundness and Completeness of FBE).: _Assuming composition, interchangeable Markov boundaries, and perfect tests for the correlation, termination, elimination and equivalences, FBE computes all Markov boundaries \(\) of a target \(T_{t}\) and only Markov boundaries._

The BACKWARD, EQUIV and FORWARD phases rely on model-based independence tests that might be replaced by any conditional independence (CI) test. In other words, CI tests performed in line 5 of the FORWARD algorithm (Algorithm 3) compute correlations over forecasting model residuals. We argue that in the context of linear regression with joint normal variables, measuring the residual correlation is equivalent to a model-based conditional independence test. This is the case of linear models of joint normal variables for which \(R\), and \(R\) is normal jointly with all other variables. We notice that residual-based tests are not necessarily equivalent to full conditional tests. Yet no conditional independence remains undetected by a residual test. Formally:

**Theorem 3.2**.: _Given \(T\) and \(X\), two TS variables and \(\) a conditioning set. Let \(R\) be the residual variable corresponding to the regression of \(T\) on \(\). Assume that the modeling achieves independence of the residuals: \(R\), then: \(T X| R X\). Additionally, if for all candidate variable \(X\), \(R X R X,\), then \(R X T X|\)._

## 4 Experimental Framework

**Synthetic dataset**: To build synthetic datasets with multiple Markov boundaries that respect composition and interchangeability, we build MTS starting from faithful distributions. Each MTS starts from a Vector AutoRegressive process with 20 variables, with different maximal lags and Markov boundary sizes. We then make copies of some of the MB variables to obtain replaceable variables. Those copies can be randomly shifted forward to change the lag of the relation. The same process is used to obtain redundant variables, copying correlated features not in the original Markov boundary. Irrelevant variables are then added to the dataset by sampling other VAR processes. For each MB size [2; 5; 10], total number of variables , and maximal lag [1; 5; 10], we sample 10 datasets per configuration. We verify that the noise intensity varies, as the \(R^{2}\) of a model trained with a MB ranges from 0.02 to 0.9 over the different data instances.

**Real datasets** We evaluate our approach on five forecasting datasets covering different domains: Electricity (consumption), Solar (production), S.F. Traffic , METR-LA, and PEMS-BAY  (transport). They are commonly used in recently proposed deep forecasting models [16; 15; 14]. Electricity has 321 TS and 26304 observations, Traffic 862 TS and 17544 observations, Solar 137 TS and 52560 observations, METR-LA 207 TS and 34272 observations, PEMS-BAY 325 TS and 52116 observations. We evaluate 10 randomly chosen targets per dataset.

**Cross Validation protocol** In TS data, observations are generally not independent, so data splitting for forecasting tasks must ensure that the train split precedes the test split (Forward Chaining Cross Validation). Therefore, we split the dataset along time into a Tuning and Holdout set, respectively for training and evaluating feature selection algorithms and forecasting models. The Tuning set is itself separated into five folds along the time axis, to conduct hyperparameter optimization. We optimize each considered pair of TS selection algorithm and forecasting algorithm together, for maximal average predictive performance \(R^{2}\) over all folds. Due to the consequent training time of deep learning models, it is impractical to tune TSS (Time Series variables Selection) algorithms together. We first tune each TSS algorithm with Support Vector Regression (SVR) models as proxies, then tune deep forecasters for the tuned TSS parameters.

**Baselines and Forecasting models** We compare our algorithm with the only linear scalable baseline, GroupLasso , and with no selection in the case of Electricity, Traffic, and Solar. For forecasting, we use the ARDL model , as it is a standard linear model for MTS data. Real datasets are forecasted using nonlinear models: Support Vector Regression (SVR) with nonlinear kernel, and deep forecasters like DeepAR  and Temporal Fusion Transformer (TFT) . The input window size is 10 for synthetic MTS and 96 for the five real datasets, similarly to a recent benchmark . The tuned parameters are described in Appendix (Table 3). We report the performance of the best forecasting model for each target.

[MISSING_PAGE_FAIL:8]

Note that computing one solution with FE forward phase is up to two orders of magnitude faster than GL. Table 2 reports the best forecaster after careful hyperparameter tuning per target.

Shapley values instability over replaceable variables**Claim 7**: Models tend to select one or a few variables as important predictors. For XGB models, we report in Fig.9 (see Appendix) the average contribution of the top important variables for each equivalent set of variables. Clearly, **importance is increasingly shared as the number of equivalent variables grows. Claim 8**: The most important variable of an equivalent set is highly unstable to data resampling. The stability  of the top ranked variable is respectively 0.05\(\)0.11 and 0.02\(\)0.09 for XGB and SVR, where -0.05 means total randomness and 1 deterministic selection. Additionally, the presence of replaceable variables impacts the stability of the important variables of the entire explanation. The top \(|(T)|\) ranked variables overall over _full MTS_ are significantly more unstable compared to _reduced MTS_. We applied paired Wilcoxon signed rank test and concluded that top variables in full MTS are less stable than when a unique MB(T) is left, with p-value 4.2e-13.

## 6 Related work

Most of the state-of-the-art variable selection algorithms for MTS have focused on selecting a unique concise solution. Scalable methods include using bivariate linear VAR models to select causally related pairs of TS, without considering multivariate interactions . There are mRMR approaches for MTS  with Dynamic Time Wrapping distances , but with a quadratic complexity in the number of TS due to distance computations, which are proven to be impractical for large dataset sizes. We selected GroupLasso  as baseline in our experiments, as it can specify groups of TS  while applying Lasso-type optimization with linear modeling. It is worth noticing that Lasso-type algorithms have been extended to identify multiple solutions in i.i.d data  without, however, proving any formal property.

As a matter of fact, the problem of computing multiple solutions for the variable selection problem is still in its infancy. In particular, we are not aware of any algorithm applied to MTS data. Theoretically sound methods focus on different distributional properties and assumptions. TIE*  assumes that a single Markov boundary discovery algorithm for _non-faithful_ data can be called as an oracle. All \((T)\) are discovered using exhaustive combinatorial conditional tests (\((s.n^{s})\) where \(s\) is the maximal size of a considered MB and \(n\) the number of variables ). TMFBS  proposes forward-backward MB discovery algorithms that aims to decrease the number of redundant

   & Time(s) & \(R^{2}\) & MB size & Number of MB & causal f1-score \\  FBE & 375\(\)772 & 0.373\(\)0.236 & 5.69\(\)3.33 & 8.87e+05\(\)3.7e+06 & 0.99\(\) 0.064 \\ FE & 118\(\)178 & 0.373\(\)0.236 & 5.74\(\)3.31 & 6.87e+07\(\)7.2e+08 & 0.94\(\) 0.188 \\ GL & 120\(\)149 & 0.337\(\)0.239 & 7.50\(\)10.2 & NA & 0.57\(\) 0.349 \\   

Table 1: Computation, predictive and causal performance of tuned ChronoEpilogi variants (FBE, FE) and GroupLasso (GL) on the synthetic dataset, over the 270 synthetic MTS. FE has comparable execution time and predictive power to GL, with a 30% increase in causal f1-score.

Figure 2: Performance of ChronoEpilogi versions on (a)(b) synthetic MTS and (c) real MTS.

conditional independence tests operated by TIE* to \((n^{s})\). KIAMB is an iterative forward-backward algorithm  that requires _composition_ and heuristic selections to find one MB of a target, with every MB having a non-zero probability of selection. Identifying all MBs might require an exponential number of runs of KIAMB . In construct, ChronoEpilogi computes provable equivalent subsets of variables in \((ns)\) conditional tests under composition and interchangeability assumptions, while the forward-backward phase is based on a heuristic. We should mention that there also exist related works that rely on pure associational criteria where heuristics are used without any causal guarantee, especially in the field of gene expression  (see  for an in-depth review). Finally, several works highlight the importance of a structured representation of multiple MBs for interpretability concerns, especially in the presence of an exponential number of MBs of a target of interest [LPL\({}^{+}\)23]. ChronoEpilogi is the first algorithm that provides a compact representation of mutually equivalent variables for MTS. Redundant and irreplaceable variables can be easily distinguished at first glance (see Figure 1).

## 7 Conclusions

Overall, the paper's contributions are (a) the adaptation to time series and further development of the theory of multiple variable selection, (b) the design of the first variable selection algorithm for time-series data, called ChronoEpilogi, that scales to thousands of available series, (c) the conditions of soundness and completeness of ChronoEpilogi, and (d) the empirical evaluations of ChronoEpilogi demonstrating the presence of multiple solutions in real data, achieving on par performance against Group Lasso and no selection, while reducing the number of TS required to build the model by 96% (on average) by conserving or even improving forecasting performance. The reduced model could be employed as the final model for production, or as a surrogate to a model using all available time series (assuming it is better performing) to facilitate interpretation, visualization, and explanations. Finally, we leave as future work the construction of ensemble models trained on several or all MBs as a means to create forecasters more robust to noise, faulty sensors, or other systematic errors.

We note that the conclusions are limited to the scope of the experimental study. The latter could benefit from a larger scope of experiments with more real and synthetic datasets of varying size, statistical properties, and data types (e.g., discrete time-series). In addition, different variants of the main algorithm could employ non-linear models to compute residuals and non-linear correlation methods for heuristically selecting time series. The implementation has not been optimized at the low level to reach higher computational gains. Other distributional properties that could lead to greedy yet sound and complete multiple time series selection algorithms could be explored. We also hope to relax the assumption of stationarity in future works, as most practical applications have trends/seasonality, different train-test distributions, or change points.

   Dataset & TSS & \(R^{2}\) & rmse \(\) & mape \(\) & size & time F/E & \#model & \#MB \\  Electricity & FE & **0.940** & **0.226** & **2.388** & 10.9 & **31** /2411 & 6/0/4 & 1e+14 \\ Electricity & GL & 0.934 & 0.236 & 2.649 & **6.5** & 127 & 3/0/7 & NA \\ Electricity & NS & 0.863 & 0.349 & 4.935 & 321.0 & NA & 0/0/10 & NA \\  Solar & FE & **0.985** & **0.109** & 0.664 & 5.1 & **3** /261 & 0/0/10 & 1 \\ Solar & GL & **0.984** & **0.111** & **0.623** & **4.7** & 119 & 0/0/10 & NA \\ Solar & NS & 0.968 & 0.159 & 1.607 & 138.0 & NA & 0/0/10 & NA \\  Traffic & FE & 0.783 & 0.442 & 39.745 & **11.7** & **12** / 1248 & 6/0/4 & 7e+24 \\ Traffic & GL & **0.797** & **0.431** & **28.130** & 79.1 & 211 & 3/0/7 & NA \\ Traffic & NS & 0.740 & 0.491 & 42.371 & 863.0 & NA & 1/0/9 & NA \\  PEMS-BAY & FE & 0.860 & 0.358 & **1.208** & **4.1** & **143**/6250 & 8/0/2 & 6e+4 \\ PEMS-BAY & GL & **0.867** & **0.355** & 1.217 & 37.7 & 765 & 10/0/0 & NA \\ PEMS-BAY & NS & 0.820 & 0.957 & 1.343 & 325 & NA & 9/0/1 & NA \\  METR-LA & FE & 0.886 & 0.374 & 1.121 & **3.8** & **100**/2246 & 8/0/2 & 2e+4 \\ METR-LA & GL & 0.864 & 0.401 & 1.230 & 41.9 & 306 & 7/0/3 & NA \\ METR-LA & NS & **0.896** & **0.363** & **1.054** & 207 & NA & 10/0/0 & NA \\   

Table 2: Forecasting performance of ChronoEpilogi (FE), GroupLasso (GL) and No Selection (NS) over real datasets. We report the number of times each model was selected (TFT/DeepAR/SVR), and the time spent in the forward/equivalence phases (time F/E). FE multiple solutions are on par to the performance of the unique GL solution while their size in Traffic is 8 time smaller than GL.