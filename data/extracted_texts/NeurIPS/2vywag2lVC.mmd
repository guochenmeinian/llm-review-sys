# Last-Iterate Global Convergence of Policy Gradients for Constrained Reinforcement Learning

Alessandro Montenegro

Politecnico di Milano, Milan, Italy

alessandro.montenegro@polimi.it

&Marco Mussi

Politecnico di Milano, Milan, Italy

marco.mussi@polimi.it

Matteo Papini

Politecnico di Milano, Milan, Italy

matteo.papini@polimi.it

&Alberto Maria Metelli

Politecnico di Milano, Milan, Italy

albertomaria.metelli@polimi.it

###### Abstract

_Constrained Reinforcement Learning_ (CRL) tackles sequential decision-making problems where agents are required to achieve goals by maximizing the expected return while meeting domain-specific constraints, which are often formulated as expected costs. In this setting, _policy-based_ methods are widely used since they come with several advantages when dealing with continuous-control problems. These methods search in the policy space with an _action-based_ or _parameter-based_ exploration strategy, depending on whether they learn directly the parameters of a stochastic policy or those of a stochastic hyperpolicy. In this paper, we propose a general framework for addressing CRL problems via _gradient-based primal-dual_ algorithms, relying on an alternate ascent/descent scheme with dual-variable regularization. We introduce an exploration-agnostic algorithm, called C-PG, which exhibits global last-iterate convergence guarantees under (weak) gradient domination assumptions, improving and generalizing existing results. Then, we design C-PGAE and C-PGPE, the action-based and the parameter-based versions of C-PG, respectively, and we illustrate how they naturally extend to constraints defined in terms of _risk measures_ over the costs, as it is often requested in safety-critical scenarios. Finally, we numerically validate our algorithms on constrained control problems, and compare them with state-of-the-art baselines, demonstrating their effectiveness.

## 1 Introduction

When applying Reinforcement Learning (RL, Sutton and Barto, 2018) to real-world scenarios, we are tasked with addressing large-scale continuous control problems where, in addition to reaching a goal, it is necessary to meet structural or utility-based constraints. For instance, an autonomous-driving car has its main objective of getting to the desired destination (i.e., goal) while avoiding collisions, ensuring the safety of people on the streets, adhering to traffic rules, and respecting the physical requirements of the engine to avoid damaging it (i.e., constraints) (Likmeta et al., 2020). To pursue such an objective, it is necessary to extend the RL problem formulation with the possibility to account for constraints. Constrained Reinforcement Learning (CRL, Uchibe and Doya, 2007) aims at solving this family of problems by employing RL techniques to tackle Constrained Markov Decision Processes (CMDPs, Altman, 1999), which provide an established and widely-used framework for modeling constrained control tasks. The conventional CRL framework primarily focuses on constraints related directly to _expected costs_(Stooke et al., 2020; Ding et al., 2020; Yinget al., 2022; Ding et al., 2024). However, especially in safety-critical contexts, the expected cost may not represent a reliable index of safe behavior. In response to this issue, _chance constraints_ were introduced to ensure that the probability of unsafe events is minimized. Nonetheless, employing chance constraints presents several challenges (Chow et al., 2017). To strike a balance between these two extremes, constraints are defined in terms of _risk measures_ over the costs. Examples include the Conditional Value at Risk (CVaR, Rockafellar and Uryasev, 2000) and the Mean-Variance (MV, Markowitz and Todd, 2000; Li and Ng, 2000). These risk measures offer a generalization of the previous concepts, allowing for the consideration of uncertainties while preserving the focus on the cost. When incorporating constraints on risk measures, CRL is often referred to as Risk-CRL (Chow et al., 2017).

Among the RL methods applicable to CMDPs, Policy Gradients (PGs, Deisenroth et al., 2013) are particularly appealing. Indeed, PGs have demonstrably achieved impressive results in continuous-control problems due to several advantages that make them well-suited for real-world applications. These advantages include the ability to handle continuous state and action spaces (Peters and Schaal, 2006), resilience to sensor and actuator noise (Gravell et al., 2020), robustness in partially-observable environments (Azizzadenesheli et al., 2018), and the possibility of incorporating expert knowledge during policy design (Ghavamzadeh and Engel, 2006), which can simplify the learning process and improve the efficacy, safety, and interpretability of the learned policy (Likmeta et al., 2020). PGs can be categorized into two key families depending on the way exploration is carried out in the policy space (Montenegro et al., 2024). Following their taxonomy, we distinguish between the _action-based_ and the _parameter-based_ exploration paradigms. The former, employed by REINFORCE (Williams, 1992) and GPOMDP (Baxter and Bartlett, 2001), focuses on directly learning the parameters of a parametric stochastic _policy_. The latter, employed by PGPE (Sehnke et al., 2010), is tasked with learning the parameters of a parametric stochastic _hyperpolicy_ from which the parameters of the actual policy (often deterministic) are sampled.

_Policy-based CRL_ has gained significant popularity in solving constrained control problems (Achiam et al., 2017). Within this field, algorithms are primarily developed using _primal-dual_ methods (Chow et al., 2017; Tessler et al., 2019; Ding et al., 2020, 2021; Bai et al., 2022), which can be formulated through _Lagrangian optimization_ of the primal (i.e., policy parameters) and dual variable (i.e., Lagrange multipliers). Even though the distinction between the exploration paradigms is well known in the PG methods literature, the current state of the art in Policy-based CRL focuses only on the _action-based_ exploration approach (Achiam et al., 2017; Stooke et al., 2020; Bai et al., 2023), while the _parameter-based_ one remains unexplored. A critical challenge for policy-based Lagrangian optimization algorithms is ensuring convergence guarantees. Existing works have spent a notable effort in this direction (Ying et al., 2022; Gladin et al., 2023; Ding et al., 2024). Recently, (Ying et al., 2022; Gladin et al., 2023; Ding et al., 2024) manage to ensure _global last-iterate_ convergence guarantees. However, these approaches are affected by some notable limitations: (\(i\)) the provided convergence rates depend on the problem dimension (e.g., the cardinality of the state and action spaces), limiting their applicability to tabular CMPDs and preventing scaling to realistic continuous control problems; (\(ii\)) they focus on _softmax_ policies only, disregarding other more realistic policy models; (\(iii\)) (Ding et al., 2024) ensure convergence when a single constraint only is present.

**Original Contribution.** The goal of this work is to introduce a framework for solving constrained continuous control problems using policy-based primal-dual algorithms that operate in both the _action-based_ and _parameter-based_ policy gradient exploration scenarios, while providing global last-iterate convergence guarantees with general (hyper)policy parameterization. Specifically, the main contributions of this work can be summarized as follows:

* In Section 2, we introduce a general constrained optimization problem, which is agnostic w.r.t. both the _action-based_ or _parameter-based_ paradigm.
* In Section 3, we introduce C-PG, a general policy-based primal-dual algorithm optimizing the regularized Lagrangian function associated with the general constrained optimization problem shown in Section 2. We show that, under (weak) domination assumptions, it simultaneously achieves the following: (\(i\)) _last-iterate_ convergence guarantees to a globally optimal feasible policy (i.e., satisfying all constraints); (\(ii\)) compatibility with CMDPs having _continuous state and action spaces_; (\(iii\)) the ability to handle _multiple constraints_.
* In Section 4, we introduce C-PGAE and C-PGPE, the _action-based_ and _parameter-based_ versions of C-PG, respectively. Both algorithms are designed to also handle constraints on risk measures, thus extending the applicability space of the father algorithm C-PG. This is achieved by employinga parametric _unified risk measure_ formulation, for which we show the mapping to several risk measures of the unified one and we present the specific form of all the estimators.

In Section 5, we numerically validate our proposals against state-of-the-art baselines in constrained control problems. Related works are discussed in Appendix B. The proofs of all the statements are reported in Appendix E.

## 2 Preliminaries

**Notation.** For a measurable set \(\), we denote as \(()\) the set of probability measures over \(\). For \(P()\), we denote with \(p\) its density function and we will interchangeably use \(x P\) or \(x p\) to express that random variable \(x\) is distributed according to \(P\). For \(n,m\) with \(n m\), we denote \( n\{1,2,,n\}\) and with \( n,m\{n,n+1,,m\}\). For a vector \(^{d}\), we denote as \(x_{i}\) the \(i\)-th component of \(\). For \(a\), we define \((a)^{+}\{0,a\}\) and we extend the notation to vectors as \(()^{+}=((x_{1})^{+},,(x_{d})^{+})^{}\). Given a set \(^{d}\), we denote with \(_{}\) the Euclidean norm projection, i.e., \(_{}*{arg\,min}_{ }\|-\|_{2}\) for any \(^{d}\). For two vectors \(,^{d}\), we denote with \(,\) their inner product. A function \(f:^{d}\) is \(L_{1}\)-Lipschitz continuous if \(|f()-f(^{})| L_{1}\|- ^{}\|_{2}\) and \(L_{2}\)-smooth if it is differentiable and \(\|_{}f()-_{}f( ^{})\|_{2} L_{2}\|-^ {}\|_{2}\), for every \(,^{}^{d}\).

**Constrained Markov Decision Processes.** A Constrained Markov Decision Process (CMDP, Altman, 1999) with \(U\) constraints is represented by \(_{},,p,r,\{c_{i} \}_{i U},\{b_{i}\}_{i U},_{0},\), where \(^{d_{S}}\) and \(^{d_{A}}\) are the measurable state and action spaces, \(p:()\) is the transition model, where \(p(^{}|,)\) is the probability density of getting to state \(^{}\) given that action \(\) is taken in state \(\), \(r:[-1,0]\) is the reward function, where \(r(,)\) is the instantaneous reward obtained by playing action \(\) in state \(\), \(c_{i}:\) is the \(i\)-th cost function, where \(c_{i}(,)\) is the \(i\)-th instantaneous cost obtained by playing action \(\) in state \(\), \(b_{i}[0,J_{}]\) with \(J_{}}{1-}\) is the threshold for the \(i\)-th cost for every \(i U\), \(_{0}()\) is the initial state distribution, and \(\) is the discount factor. A trajectory \(\) of length \(T\{+\}\) is a sequence of \(T\) state-action pairs: \(=(_{,0},_{,0},,_{ ,T-1},_{,T-1})\). The _discounted return_ over a trajectory \(\) is \(R()_{t=0}^{T-1}^{t}r(_{,t}, {a}_{,t})\), while the \(i\)-th _discounted cumulative cost_ is \(C_{i}()_{t=0}^{T-1}^{t}c_{i}(_{,t}, _{,t})\). We define the additional cost function \(c_{0}(,):=-r(,)\) and \(C_{0}()-R()\). Note that \(R(),C_{i}()[0,J_{}]\) for every \(i U\) and trajectory \(\).

**Action-based Policy Gradients.** Action-based (AB) PG methods focus on learning the parameters \(^{d_{}}\) of a parametric stochastic policy \(_{}:()\), where \(_{}(|)\) represents the probability density of selecting action \(\) being in state \(\). At each step \(t\) of the interaction with the environment, the stochastic policy is employed to sample an action \(_{t}_{}(|_{t})\). To assess the performance of \(_{}\) w.r.t. the \(i\)-th cost function, with \(i 0,U\), we employ the _AB performance index_\(J_{,i}:\), which is defined as \(J_{,i}()_{ p_{}( |)}[C_{i}()]\), where \(p_{}(,)_{0}(_{,0}) _{t=0}^{T-1}_{}(_{,t}| _{,t})p(_{,t+1}|_{,t}, _{,t})\) is the density of trajectory \(\) induced by policy \(_{}\).

**Parameter-based Policy Gradients.** Parameter-based (PB) PG methods focus on learning the parameters \(^{d_{}}\) of a parametric stochastic hyperpolicy \(_{}()\). The hyperpolicy \(_{}\) is used to sample parameter configurations \(_{}\) to be plugged into an underlying parametric policy \(_{}\), that will be then used for the interaction with the environment. Notice that \(_{}\) can also be deterministic. To assess the performance of \(_{}\) w.r.t. the \(i\)-th cost function, with \(i 0,U\), we employ the _PB performance index_\(J_{,i}:\), which is defined as \(J_{,i}()_{ _{}}_{ p_{}( |)}[C_{i}()]\).

**Constrained Optimization Problem.** Having introduced the AB and PB performance indices, we formulate a _constrained optimization problem_ (COP) _agnostic_ w.r.t. the exploration paradigm:

\[_{}J_{,0}()  J_{,i}() b_{i},\ \  i U,\] (1)

where \(\{,\}\) and \(\) is a generic parameter vector belonging to the parameter space \(\). When \(=\), we are considering the AB exploration paradigm, then \(=\). On the other hand, when \(=\), we are in the PB exploration paradigm, then \(=\).

[MISSING_PAGE_FAIL:4]

[MISSING_PAGE_FAIL:5]

to the projection operator, we have that \(L_{2}=(^{-1})\), whereas \(L_{1}\) and \(L_{3}\) are independent on \(\).3 Explicit conditions on the constitutive elements of the MDP and (hyper)policies to ensure Lipshitzness and smoothness of these quantities are reported in (Montenegro et al., 2024, Appendix E) for both the AB and PB cases. These regularity properties enforced on \(_{}\) are inherited by the primal function \(H_{}\) which results to be \(L_{2}+L_{1}^{2}^{-1}\)-smooth (Lemma E.7). Concerning the regularity of \(_{}\) w.r.t. \(\), we observe that it is a quadratic function and, therefore, it is \(\)-smooth and satisfies the PL condition, i.e., Assumption 3.2 with \(=2\), \(_{1}=0\), and with \(_{1}=\) (Lemma E.5).

**Assumption 3.4** (Bounded Estimator Variance).: _For every \(\) and \(\), the estimators \(_{}_{}(,)\) and \(_{}_{}(,)\) are unbiased for \(_{}_{}(,)=_{}J_{0 }()+_{i=1}^{U}_{i}_{}J_{i}()\) and \(_{}_{}(,)=()- -\) with bounded variance, i.e., there exist \(V_{},V_{}<+\) such that:_

\[_{}_{}( ,) V_{}, _{}_{}(,) V_{}.\] (7)

Note that \(V_{}\) typically depends on the Lagrange multipliers and, for standard sample mean estimators, it is of order \(V_{}=(^{-2})\) thanks to the projection operator. Contrary, \(V_{}\) is usually not affected by \(\) since the term \(\) is not estimated and, thus, it does not impact on the variance of the sample mean estimator. In Section 4, explicit estimators are provided for both the AB and PB cases. The variance of such estimators can be easily controlled by leveraging on the properties of the score function as done in previous works (see Papini et al. 2022 and Montenegro et al. 2024, Appendix E).

### Convergence Analysis

We are now ready to attack the convergence analysis of C-PG to the global optimum of the COP of Equation (1). To this end, we study the _potential function_ defined as \(_{k}():=a_{k}+ b_{k}\), where \(a_{k}:=H_{}(_{k})-H_{}^{*}\) and \(b_{k}:=H_{}(_{k})-_{}(_{ k},_{k})\) and \((0,1)\) will be specified later. Since \(a_{k},b_{k} 0\), intuitively, if \(_{k}() 0\) we have that both \(a_{k},b_{k} 0\) and, consequently, convergence is achieved. Let us start relating \(_{k}()\), with the solution of the COP in Equation (1).

**Theorem 3.1** (Objective Function Gap and Constraint Violation).: _Let \(>0\). Under Assumption 3.1, if \(_{k}\), it holds that:_

\[J_{0}(_{k})-J_{0}(_{0}^{*}) +\|_{0}^{*}\|_{2}^{2}, (J_{i}(_{k})-b_{i})^{+} 4+_{0}^{*}\|_{2}, i[\![U]\!].\] (8)

Theorem 3.1 justifies the study of the potential \(_{k}\) as a technical tool to ensure convergence. Indeed, whenever \(_{k}\) both the \((i)\) objective function gap and \((ii)\) the constraint violation scale linearly with \(\) and with the regularization parameter \(\) of the regularized Lagrangian \(_{}\) multiplied by the norm of the Lagrange multipliers of the non-regularized problem \(\|_{0}^{*}\|_{2}\), which are finite under Assumption 3.1. This expression also suggests a choice of \(=()\) to enforce an overall \(\) error on both quantities. Note that, from Theorem 3.1, it is immediate to employ a _conservative constraint_ (\(b_{i}^{} b_{i}-4-_{0}^{*}\|_{2}^{2}\)) to achieve zero constraint violation with no modification of the algorithm.

We are now ready to state the convergence guarantees for the potential function.

**Theorem 3.2** (Convergence of \(_{K}\)).: _Under Assumptions 3.2, 3.3, 3.4, for \(<1/5\), sufficiently small \(\) and \(\), and a choice of constant learning rates \(_{},_{}\), we have \(_{K}()+_{1}/_{1}\) whenever:4_

* \(K=(^{-1}(^{-1}))\) _if_ \(=2\) _and the gradients are exact (i.e.,_ \(V_{}=V_{}=0\)_);_
* \(K=(^{-1}^{--1})\) _if_ \([1,2)\) _and the gradients are exact (i.e.,_ \(V_{}=V_{}=0\)_);_
* \(K=(^{-3}^{-+1})\) _if_ \([\!\) _and the gradients are estimated (i.e.,_ \(V_{}=(^{-2})\) _and_ \(V_{}=(1)\)_)._

Some comments are in order. First, Theorem 3.2 holds for a specific choice of the constant \((0,1/5)\) defining the potential function \(_{K}\). Second, the presented rates hold for sufficiently small values of \(\) and \(\). This is just for presentation purposes, as the sample complexity5 can only improve if we increase the values of \(\) and \(\). Third, in the proof, an explicit expression of the learning rates is provided. Concerning their orders, for the case of exact gradients, we choose \(_{}=^{-1}\) and \(_{}=()\), whereas for the estimated gradient case, we choose \(_{}=(^{2/})\) and \(_{}=(^{3}^{2/})\).

Assuming \(\) to be a constant, we observe that both learning rates display the same dependence on \(\) and, consequently, they are in _single-time scale_. However, as we have seen in Theorem 3.1, in order to obtain guarantees on the original non-regularized problem, we have to set \(=()\), leading to a _two-time scales_ algorithm. Fourth, we observe that, for both exact and estimated gradients, the sample complexity degrades as the constant \(\) of the gradient domination moves from \(2\) to \(1\), delivering the smallest sample complexity when the PL condition holds. Finally, we highlight that C-PG jointly: (\(i\)) converges to the global optimum of the COP problem of Equation (1); (\(ii\)) delivers a _last-iterate_ guarantee; (\(iii\)) has no dependence on the cardinality of the state or action spaces, making it completely _dimension-free_. Table 1 and Figure 1 summarize the results of Theorem 3.2.

## 4 Action-based and Parameter-based Primal-Dual Algorithms

In this section, we introduce C-PGAE and C-PGPE, the _action-based_ and the _parameter-based_ versions of C-PG, respectively. Both the algorithms have been designed to tackle _continuous risk-constrained optimization problems_ (RCOP), generalizing the COP of Equation (1) and thus extending the applicability space of C-PG. This is done by employing a parametric _unified risk measure_ formulation, which leads to having a framework for solving risk-constrained problems via policy-based primal-dual methods operating in both the _action-based_ and _parameter-based_ PGs exploration scenarios. Moreover, this framework allows to handle constraints enforced on several risk measures, which will be discussed below. We first introduce a _parametric unified risk measure_ and formulate an exploration-agnostic RCPP (Section 4.1). Then, we present both C-PGAE and C-PGPE (Section 4.2).

### Risk-Constrained Optimization Problem

We start presenting the notion of _unified risk measure_ originally introduced by Bisi et al. (2022). For the AB case, to evaluate the performance of policy \(_{}\) w.r.t. the \(i\)-th cost, with \(i 0,U\), given two functions \(f_{i}:^{2}\) and \(g_{i}:\), we define the _AB-risk measure_ as:

\[_{_{i}}_{,i}(,_{i}) _{,i}(,_{i}):=}_{ p_{}(|)}[f_{i}(C_{i}(),_{i})]+g_{i}(_{i}).\] (9)

Similarly, for the PB case, to assess the performance of hyperpolicy \(_{}\) w.r.t. the \(i\)-th cost, with \(i 0,U\), we define the _PB-risk measure_ as:

\[_{_{i}}_{,i}(,_{i}) _{,i}(,_{i}):= {}_{_{}}[}_{  p_{}(|)}[f_{i}(C_{i}(),_{i})] ]+g_{i}(_{i}).\] (10)

Some observations are in order. First, by selecting the functions \(f_{i}\) and \(g_{i}\), we generate different risk measures (Table 2). Details on the presented risk measures and on the mappings can be found in Appendix C. Second, the risk measure itself is defined as another minimization problem over the additional real variable \(_{i}\). In principle, if we replace the constraints on cost expectation of the COP in Equation (1) with the risk measures presented above, we are in the presence of a _bilevel_ optimization problem. However, it is immediate to realize that we can merge variables \(_{i}\) with the primal variables \(\) of the COP without changing the optimum. Finally, let us appreciate the semantic difference between enforcing risk-based constraints in the AB and PB cases. Indeed, while for AB the stochasticity inducing the risk is the one generated by the policy \(_{}\) and the environment, for PB we have the joint stochastic process of the hyperpolicy \(_{}\), policy \(_{}\) (when stochastic), and the

   &  &  \\   & \(\)=1 (GD) & \(\)\(\)(1,2) & \(\)=2 (PL) & \(\)=1 (GD) & \(\)\(\)(1,2) & \(\)=2 (PL) \\  
**Fixed \(\)** & \(^{-1}^{-1}\) & \(^{-1}^{-+1}\) & \(^{-1}(^{-1})\) & \(^{-3}^{-3}(^{-1})\) & \(^{-3}^{-+1}(^{-1})\) & \(^{-3}^{-1}(^{-1})\) \\  \(\)=\(()\) & \(^{-2}\) & \(^{-}\) & \(^{-}\) & \(^{-1}(^{-1})\) & \(^{--1}(^{-1})\) & \(^{-4}(^{-1})\) \\  

Table 1: Summary of the sample complexity results of C-PG when either keeping \(\) fixed or setting it as \(=()\).

Figure 1: Plot of the exponents of \(^{-1}\) in the cases of Table 1.

[MISSING_PAGE_FAIL:8]

**Estimators.** Here, we provide the risk-agnostic form of the estimators used to perform the primal and dual updates. In particular, the gradient of \(}_{,}\) with respect to \(\) is closely related to the choices of \(f_{i}\) and \(g_{i}\). Therefore, the discussion is deferred to Appendix D, where we also explicitly derive all the estimators for the risk measures presented in Table 2 and for both exploration paradigms. The estimator of the gradient of \(}_{,}\) w.r.t. \(\) has the same form for both C-PGAE and C-PQPE, that is:

\[_{}}_{,}(,,)=_{i=1}^{N}((_{i}),_{k+1})-(_{k+1})--_{k},\] (13)

where the trajectories are collected with the AB or the PB exploration paradigm, \(((),):=f_{1}(C_{1}(),_{1}), ,f_{U}(C_{U}(),_{U})^{}\) and \(()(g_{1}(_{1}),,g_{U}(_{U}))^{}\).

C-PGAE (Algorithm 1) is the _action-based_ version of C-PG. At each iteration \(k K\), the agent collects \(N\) trajectories by playing the policy \(_{_{k}}\), then it alternatively updates the policy parameter \(\) and the risks parameter \(\), or the Lagrange multipliers \(\). The \(\) update relies on the estimator:

\[_{}}_{,}( _{k},\!_{k},\!_{k})\!\! \!_{i=1}^{N}\!\!_{t=0}^{T-1}\!\!_{}\!\! _{_{k}}(_{_{i},t}|_{_{i},t})f _{0}(C_{0}(_{i}),\!_{k,0})+_{u=1}^{U}\!_{k,u}(f_{u}(C_{u}( _{i}),\!_{k,u})),\]

which reduces to the prototypical _action-based_ algorithm REINFORCE (Williams, 1992) in the risk-neutral case. When allowed by the choice of \(f_{i}\), we switch to a GPOMDP-style estimator (Baxter and Bartlett, 2001) which suffers less variance. See Appendix D for details.

C-PQPE (Algorithm 2) is the _parameter-based_ version of C-PG. At each iteration \(k K\), the algorithm samples \(N\) parameter configurations \(\{_{i}\}_{i N}\) from the hyperpolicy \(_{_{k}}\), then collects a single trajectory \(_{i}\), obtained by playing the policy \(_{_{i}}\), for each sampled parameterization \(_{i}\). The sampled trajectories and parameters are then used to alternatively update the hyperpolicy parameter vector \(\) and the risks parameter vector \(\), or the vector of Lagrange multipliers \(\). In particular, the \(\) update relies on the following estimator:

\[_{}}_{,}(_{k},_{k},_{k})_{i=1}^{N} _{}_{_{k}}(_{i})f_{0}( C_{0}(_{i}),_{k,0})+_{u=1}^{U}_{u}f_{u}(C_{u}( _{i}),_{k,u}),\]

which reduces to the _parameter-based_ algorithm PGPE (Sehnke et al., 2010) in the risk-neutral case.

## 5 Numerical Validation

In this section, we empirically validate some of the theoretical results shown throughout this work. Experimental details and additional results are provided in Appendix H. The code to run the experiments in this paper is available at https://github.com/MontenegroAlessandro/MagicRL.

**Comparison in DGWW.** We compare our C-PGAE against the sample-based versions of NPG-PD (Ding et al., 2020, Appendix H) and RPG-PD (Ding et al., 2024, Appendix C.9) on a Discrete Grid World with Walls (DGWW, see Appendix H) with a horizon of \(T=100\), and with a single constraint on the average trajectory cost. The methods are learning the parameters of a tabular softmax policy and the learning phase considers constant step sizes. Figure 1(a) shows the average return and the average cost curves over the trajectories seen during the learning. As can be noticed, C-PGAE strikes the objective of the COP with fewer trajectories w.r.t. the competitors. Indeed, both NPG-PD and RPG-PD require additional \((||+||||)\) trajectories per iteration.

**Comparison in LQR.** We compare our C-PGAE and C-PQPE against the continuous sample-based version of NPG-PD2 (Ding et al., 2022, Algorithm 1), working with generic policy parameterizations. Additionally, we consider RPG-PD2, a ridge-regularized version of NPG-PD2. The environment is a bidimensional _CostLQR_ (Appendix H) with a horizon \(T=50\) and a single cost that the algorithms should keep below \(b=0.2\) on average. C-PGAE, NPG-PD2, and RPG-PD2 learn the parameter of a _linear gaussian policy_, while C-PQPE the ones of a _gaussian hyperpolicy_ over a _linear deterministic policy_. All the step sizes are chosen with Adam (Kingma and Ba, 2015) scheduler. Figure 1(b) reports the learning curves for the average return and the cost, confirming that our methods solve the COP with fewer trajectories. Indeed, being both NPG-PD2 and RPG-PD2 actor-critic methods, they suffer from the inner critic loop, which requires the collection of additional trajectories (\(500\) here). We stress that the actor-critic methods were very sensitive to the hyperparameters selection, especially the length and the step size of the inner loop (see Appendix H).

**Risk constraints on Swimmer.** In Figure 3, we show the empirical distributions of costs over \(100\) trajectories of the learned (hyper)policies via C-PGPE and C-PGAE. This experiment considers the cost-based version of the _Swimmer-v4_ MuJoCo (Todorov et al., 2012) environment, with a single constraint over the actions (see Appendix H), for which we set \(b=50\). The experimental results show that C-PGPE learns a hyperpolicy paying less cost when using risk measures compared to average cost, with the smallest costs attained by CVaR\({}_{}\). C-PGAE shows similar results, although the difference between CVaR\({}_{}\)or the Chance constraints and average cost constraints are not very significant. Notice that, the minimum amount of cost is obtained using MV constraints even if the learned policy exhibits poor performances (Table (c)c). In all the other cases, both C-PGPE and C-PGAE learns (hyper)policies exhibiting similar performance scores.

## 6 Conclusions

In this work, we proposed a general framework to address continuous CRL problems via _primal-dual policy-based_ algorithms, leveraging on an alternate ascent-descent approach. Our _exploration-agnostic_ proposal C-PG exhibits _dimension-free global last-iterate_ convergence guarantees, under the standard (weak) gradient domination assumption. Furthermore, we introduced C-PGAE and C-PGPE, the action and parameter-based versions of C-PG which enable embedding risk-based constraints, enlarging the capabilities of our framework in addressing constrained real-world problems. Future works should focus on matching the sample complexity lower bound prescribed by (Vaswani et al., 2022) and devising algorithms with the same rates of C-PG with a single time-scale.

Figure 3: Cost distributions with (hyper)policies learned considering different risk measures (5 runs).

Figure 2: Average return and cost in _CostLQR_ and _DGWW_ environments (5 runs, mean \( 95\%\) C.I.).

[MISSING_PAGE_FAIL:11]

Benjamin Gravell, Peyman Mohajerin Esfahani, and Tyler Summers. Learning optimal controllers for linear systems with multiplicative noise via policy gradient. _IEEE Transactions on Automatic Control_, 66(11):5283-5298, 2020.
* Azizzadenesheli et al. (2018) Kamyar Azizzadenesheli, Yisong Yue, and Animashree Anandkumar. Policy gradient in partially observable environments: Approximation and convergence. _arXiv preprint arXiv:1810.07900_, 2018.
* Ghavamzadeh and Engel (2006) Mohammad Ghavamzadeh and Yaakov Engel. Bayesian policy gradient algorithms. _Advances in Neural Information Processing Systems (NeurIPS)_, 19, 2006.
* Montenegro et al. (2024) Alessandro Montenegro, Marco Mussi, Alberto Maria Metelli, and Matteo Papini. Learning optimal deterministic policies with stochastic policy gradients. In _Proceedings of the International Conference on Machine Learning (ICML)_. PMLR, 2024.
* Williams (1992) Ronald J. Williams. Simple statistical gradient-following algorithms for connectionist reinforcement learning. _Machine Learning_, 8:229-256, 1992.
* Baxter and Bartlett (2001) Jonathan Baxter and Peter L. Bartlett. Infinite-horizon policy-gradient estimation. _Journal of Artificial Intelligence Research_, 15:319-350, 2001.
* Sehnke et al. (2010) Frank Sehnke, Christian Osendorfer, Thomas Ruckstiess, Alex Graves, Jan Peters, and Jurgen Schmidhuber. Parameter-exploring policy gradients. _Neural Networks_, 23(4):551-559, 2010.
* Achiam et al. (2017) Joshua Achiam, David Held, Aviv Tamar, and Pieter Abbeel. Constrained policy optimization. In _Proceedings of the International Conference on Machine Learning (ICML)_, volume 70 of _Proceedings of Machine Learning Research_, pages 22-31. PMLR, 2017.
* Tessler et al. (2019) Chen Tessler, Daniel J. Mankowitz, and Shie Mannor. Reward constrained policy optimization. In _International Conference on Learning Representations (ICLR)_, 2019.
* Ding et al. (2021) Dongsheng Ding, Xiaohan Wei, Zhuoran Yang, Zhaoran Wang, and Mihailo R. Jovanovic. Provably efficient safe exploration via primal-dual policy optimization. In _International Conference on Artificial Intelligence and Statistics (AISTATS)_, volume 130 of _Proceedings of Machine Learning Research_, pages 3304-3312. PMLR, 2021.
* Bai et al. (2022) Qinbo Bai, Amrit Singh Bedi, Mridul Agarwal, Alec Koppel, and Vaneet Aggarwal. Achieving zero constraint violation for constrained reinforcement learning via primal-dual approach. In _AAAI Conference on Artificial Intelligence_, pages 3682-3689. AAAI Press, 2022.
* Bai et al. (2023) Qinbo Bai, Amrit Singh Bedi, and Vaneet Aggarwal. Achieving zero constraint violation for constrained reinforcement learning via conservative natural policy gradient primal-dual algorithm. In _AAAI Conference on Artificial Intelligence_, pages 6737-6744. AAAI Press, 2023.
* Gladin et al. (2023) Egor Gladin, Maksim Lavrik-Karmazin, Karina Zainullina, Varvara Rudenko, Alexander V. Gasnikov, and Martin Takac. Algorithm for constrained markov decision process with linear convergence. In _International Conference on Artificial Intelligence and Statistics (AISTATS)_, volume 206 of _Proceedings of Machine Learning Research_, pages 11506-11533. PMLR, 2023.
* Liu et al. (2021) Tao Liu, Ruida Zhou, Dileep Kalathil, PR Kumar, and Chao Tian. Policy optimization for constrained mdps with provable fast global convergence. _arXiv preprint arXiv:2111.00552_, 2021.
* Ding et al. (2022) Dongsheng Ding, Kaiqing Zhang, Jiali Duan, Tamer Basar, and Mihailo R. Jovanovic. Convergence and sample complexity of natural policy gradient primal-dual methods for constrained mdps. _CoRR_, abs/2206.02346, 2022.
* Bertsekas (2014) Dimitri P Bertsekas. _Constrained optimization and Lagrange multiplier methods_. Academic press, 2014.
* Yang et al. (2020) Junchi Yang, Negar Kiyavash, and Niao He. Global convergence and variance reduction for a class of nonconvex-nonconcave minimax problems. In _Advances in Neural Information Processing Systems (NeurIPS)_, volume 33, pages 1153-1165, 2020.
* Yang et al. (2020)Santiago Paternain, Luiz F. O. Chamon, Miguel Calvo-Fullana, and Alejandro Ribeiro. Constrained reinforcement learning has zero duality gap. In _Advances in Neural Information Processing Systems (NeurIPS)_, pages 7553-7563, 2019.
* Yuan et al. (2022) Rui Yuan, Robert M Gower, and Alessandro Lazaric. A general sample complexity analysis of vanilla policy gradient. In _International Conference on Artificial Intelligence and Statistics (AISTATS)_, pages 3332-3380. PMLR, 2022.
* Masiha et al. (2022) Saeed Masiha, Saber Salehkaleybar, Niao He, Negar Kiyavash, and Patrick Thiran. Stochastic second-order methods improve best-known sample complexity of sgd for gradient-dominated functions. _Advances in Neural Information Processing Systems (NeurIPS)_, 35:10862-10875, 2022.
* Fatkhullin et al. (2023) Ilyas Fatkhullin, Anas Barakat, Anastasia Kireeva, and Niao He. Stochastic policy gradient methods: Improved sample complexity for fisher-non-degenerate policies. In _Proceedings of the International Conference on Machine Learning (ICML)_, volume 202 of _Proceedings of Machine Learning Research_, pages 9827-9869. PMLR, 2023.
* Bhandari and Russo (2024) Jalaj Bhandari and Daniel Russo. Global optimality guarantees for policy gradient methods. _Operations Research_, 2024.
* Mei et al. (2020) Jincheng Mei, Chenjun Xiao, Csaba Szepesvari, and Dale Schuurmans. On the global convergence rates of softmax policy gradient methods. In _Proceedings of the International Conference on Machine Learning (ICML)_, pages 6820-6829. PMLR, 2020.
* Papini et al. (2022) Matteo Papini, Matteo Pirotta, and Marcello Restelli. Smoothing policies and safe policy gradients. _Machine Learning_, 111(11):4081-4137, 2022.
* Bisi et al. (2022) Lorenzo Bisi, Davide Santambrogio, Federico Sandrelli, Andrea Tirinzoni, Brian D. Ziebart, and Marcello Restelli. Risk-averse policy optimization via risk-neutral policy optimization. _Artif. Intell._, 311:103765, 2022.
* Kingma and Ba (2015) Diederik P. Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In _International Conference on Learning Representations (ICLR)_, 2015.
* Todorov et al. (2012) Emanuel Todorov, Tom Erez, and Yuval Tassa. Mujoco: A physics engine for model-based control. In _IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)_, pages 5026-5033. IEEE, 2012.
* Vaswani et al. (2022) Sharan Vaswani, Lin Yang, and Csaba Szepesvari. Near-optimal sample complexity bounds for constrained mdps. In _Advances in Neural Information Processing Systems (NeurIPS)_, 2022.
* Dalal et al. (2018) Gal Dalal, Krishnamurthy Dvijotham, Matej Vecerik, Todd Hester, Cosmin Paduraru, and Yuval Tassa. Safe exploration in continuous action spaces. _CoRR_, abs/1801.08757, 2018.
* Chow et al. (2018) Yinlam Chow, Ofir Nachum, Edgar A. Duenez-Guzman, and Mohammad Ghavamzadeh. A lyapunov-based approach to safe reinforcement learning. In _Advances in Neural Information Processing Systems (NeurIPS)_, pages 8103-8112, 2018.
* Yu et al. (2019) Ming Yu, Zhuoran Yang, Mladen Kolar, and Zhaoran Wang. Convergent policy optimization for safe reinforcement learning. In _Advances in Neural Information Processing Systems (NeurIPS)_, pages 3121-3133, 2019.
* Liu et al. (2020) Yongshuai Liu, Jiaxin Ding, and Xin Liu. IPO: interior-point policy optimization under constraints. In _AAAI Conference on Artificial Intelligence_, pages 4940-4947. AAAI Press, 2020.
* Xu et al. (2021) Tengyu Xu, Yingbin Liang, and Guanghui Lan. CRPO: A new approach for safe reinforcement learning with convergence guarantee. In _Proceedings of the International Conference on Machine Learning (ICML)_, volume 139 of _Proceedings of Machine Learning Research_, pages 11480-11491. PMLR, 2021.
* a gradient approach. In _IEEE Conference on Decision and Control (CDC)_, pages 1940-1945. IEEE, 2002.
* Zhang et al. (2019)Shalabh Bhatnagar and K. Lakshmanan. An online actor-critic algorithm with function approximation for constrained markov decision processes. _J. Optim. Theory Appl._, 153(3):688-708, 2012.
* Zheng et al. (2022) Tianqi Zheng, Pengcheng You, and Enrique Mallada. Constrained reinforcement learning via dissipative saddle flow dynamics. In _Asilomar Conference on Signals, Systems, and Computers (ACSSC)_, pages 1362-1366. IEEE, 2022.
* Moskovitz et al. (2023) Ted Moskovitz, Brendan O'Donoghue, Vivek Veeriah, Sebastian Flennerhag, Satinder Singh, and Tom Zahavy. Reload: Reinforcement learning with optimistic ascent-descent for last-iterate convergence in constrained mdps. In _Proceedings of the International Conference on Machine Learning (ICML)_, volume 202 of _Proceedings of Machine Learning Research_, pages 25303-25336. PMLR, 2023.
* Hsieh et al. (2019) Yu-Guan Hsieh, Franck Iutzeler, Jerome Malick, and Panayotis Mertikopoulos. On the convergence of single-call stochastic extra-gradient methods. In _Advances in Neural Information Processing Systems (NeurIPS)_, pages 6936-6946, 2019.
* Chow and Pavone (2013) Yin-Lam Chow and Marco Pavone. Stochastic optimal control with dynamic, time-consistent risk constraints. In _American Control Conference (ACC)_, pages 390-395. IEEE, 2013.
* Borkar and Jain (2014) Vivek S. Borkar and Rahul Jain. Risk-constrained markov decision processes. _IEEE Trans. Autom. Control._, 59(9):2574-2579, 2014.
* Zhang et al. (2024) Qiyuan Zhang, Shu Leng, Xiaoteng Ma, Qihan Liu, Xueqian Wang, Bin Liang, Yu Liu, and Jun Yang. Cvar-constrained policy optimization for safe reinforcement learning. _IEEE Transactions on Neural Networks and Learning Systems_, 2024.
* Tamar et al. (2014) Aviv Tamar, Yonatan Glassner, and Shie Mannor. Policy gradients beyond expectations: Conditional value-at-risk. _CoRR_, abs/1404.3862, 2014.
* Bisi et al. (2020) Lorenzo Bisi, Luca Sabbioni, Edoardo Vittori, Matteo Papini, and Marcello Restelli. Risk-averse trust region optimization for reward-volatility reduction. In _Proceedings of the International Joint Conference on Artificial Intelligence (IJCAI)_, pages 4583-4589. ijcai.org, 2020.
* Luo et al. (2023) Yudong Luo, Guiliang Liu, Pascal Poupart, and Yangchen Pan. An alternative to variance: Gini deviation for risk-averse policy gradient. In _Advances in Neural Information Processing Systems (NeurIPS)_, 2023.
* Sutton et al. (1999) Richard S. Sutton, David A. McAllester, Satinder Singh, and Yishay Mansour. Policy gradient methods for reinforcement learning with function approximation. In _Advances in Neural Information Processing Systems (NeurIPS)_, pages 1057-1063. The MIT Press, 1999.
* Nouiehed et al. (2019) Maher Nouiehed, Maziar Sanjabi, Tianjian Huang, Jason D Lee, and Meisam Razaviyayn. Solving a class of non-convex min-max games using iterative first order methods. _Advances in Neural Information Processing Systems (NeurIPS)_, 32, 2019.
* Anderson and Moore (2007) Brian DO Anderson and John B Moore. _Optimal control: linear quadratic methods_. Courier Corporation, 2007.

## Appendix A Pseudo-codes

```
1:Iterations \(K\); batch size \(N\); regularization \(\); initial parameters: \(_{0}\), \(_{0}\), and \(_{0}\); step sizes: \(_{,k}\), \(_{,k}\), \(_{,k}\).
2for\(k K\)do
3: Collect \(N\) trajectories \(\{_{i}\}_{i N}\) with \(_{_{k-1}}\)
4:if\(k\) is oddthen
5:\(_{k}_{k-1}-_{,k-1}_{}}_{,}(_{k-1}, _{k-1},_{k-1})\)
6:\(_{k}_{k-1}-_{,k-1}_{}}_{,}(_{k-1},_ {k-1},_{k-1})\)
7:else
8:\(_{k}_{k-1}+_{,k-1}_{}}_{,}(_{k-1},_{k-1},_{k-1})\)
9:end ```

**Algorithm 1**C-PGAE.

``` Input :Iterations \(K\); batch size \(N\); regularization \(\); initial parameters: \(_{0}\), \(_{0}\), and \(_{0}\); step sizes: \(_{,k}\), \(_{,k}\), \(_{,k}\).
1for\(k K\)do
2: Sample \(N\) parameters \(\{_{i}\}_{i N}\) with \(_{_{k-1}}\)
3: With each \(\{_{_{i}}\}_{i N}\) collect a trajectory \(_{i}\)
4:if\(k\) is oddthen
5:\(_{k}_{k-1}-_{,k-1}_{} }_{,}(_{k-1},_{k-1}, _{k-1})\)
6:\(_{k}_{k-1}-_{,k-1}_{} }_{,}(_{k-1},_{k-1}, _{k-1})\)
7:else
8:\(_{k}_{k-1}+_{,k-1}_{}}_{,}(_{k-1}, _{k-1},_{k-1})\)
9:end ```

**Algorithm 2**C-PGPE.
Related Works

Policy Optimization Approaches for Constrained Reinforcement Learning.Policy Optimization-based algorithms for Constrained Reinforcement Learning mostly follow _primal_-only or _primal-dual_ approaches. _Primal_-only algorithms (Dalal et al., 2018; Chow et al., 2018; Yu et al., 2019; Liu et al., 2020; Xu et al., 2021) avoid considering dual variables by focusing on the design of the objective function and by designing the update rules for the policy at hand incorporating the constraint satisfaction part.

The main benefit of employing primal-only algorithms lies in the fact that there is no need to consider another variable to learn, and therefore, no need to tune its learning rate. However, few of the existing methods establish global convergence to an optimal feasible solution. For instance, Xu et al. (2021) propose CRPO, an algorithm employing an _unconstrained_ policy maximization update taking into account the reward when all the constraints are satisfied, while leveraging on-policy minimization updates in the direction of violated constraint functions. Moreover, it exhibits average global convergence guarantees for the tabular setting. On the other hand, _primal-dual_ algorithms (Chow et al., 2017; Achiam et al., 2017; Tessler et al., 2019; Stooke et al., 2020; Ding et al., 2020, 2021; Bai et al., 2022; Ying et al., 2022; Bai et al., 2023; Gladin et al., 2023; Ding et al., 2024) are the most commonly used and investigated. Indeed, the effectiveness of using the primal-dual approach is justified by Paternain et al. (2019), which states that this kind of approach has zero duality gap under Slater's condition when optimizing over the space of all the possible stochastic policies. Among the reported works, Stooke et al. (2020) propose PID Lagrangian, a method to update the dual variable, smoothing the oscillations around the threshold value of the costs during the learning. The practical strength of such a method is that can be paired with any of the existing policy optimization methods. The other cited works are treated in details in the next paragraph.

Lagrangian-based Policy Search Convergence Guarantees.A lot of research effort has been spent in studying the convergence guarantees for primal-dual policy optimization methods. In this field, the goal is to ensure last-iterate convergence guarantees showing rates that are dimension-free, i.e., not relying on the state and action spaces' dimensions, and working with multiple constraints. In the rest of this paragraph, we talk about single time-scale algorithms when the methods at hand prescribe the usage of the same step sizes for both the primal and dual variables' updates. Vazquez-Abad et al. (2002) and Bhatnagar and Lakshmanan (2012) propose primal-dual policy gradient-based methods built upon distinct time-scales and relying on nested loops. Such methods only show _asymptotic_ convergence guarantees. Chow et al. (2017) propose two primal-dual methods ensuring _asymptotic_ convergence guarantees. The peculiarity of those methods lies in the fact that their notion of CMDP encapsulates risk-based constraints, introducing an additional learning variable. Their algorithms have guarantees of _asymptotic_ convergence to stationary points. The recent works by Zheng et al. (2022) and Moskovitz et al. (2023) also propose methods ensuring _asymptotic_ global convergence guarantees. These methods exploit occupancy-measure iterates rather than policy iterates. Ding et al. (2020) propose NPG-PD, which relies on a natural policy gradient approach and, under Slater's assumption, ensures dimension-dependent _average_-iterate global convergence guarantees in the single-constrained setting with a single time-scale and with exact gradients. This work has been extended by Ding et al. (2022), which strikes dimension-free rates, but still guaranteeing just _average_-rate convergence with exact gradients. However, sample-based versions of NPG-PD showing, under additional assumptions, the same convergence rates are provided by the authors. Another work ensuring an _average_-iterate rate is the one by Liu et al. (2021). The latter exhibits a convergence rate of order \(}(^{-1})\), considering to act in tabular CMDPs with softmax policies and having access to exact gradients and to a generative model. Liu et al. (2021) propose also a sample-based version of their algorithm, keeping the same setting previously described, which ensures a convergence rate on _average_ of order \(}(^{-3})\). Both Ying et al. (2022) and Gladin et al. (2023) propose algorithms involving regularization. The proposed methods rely on natural policy-based subroutines and show dimension-dependent _last-iterate_ global convergence guarantees, relying on two time-scales. These methods work also with multiple constraints. Finally, Ding et al. (2024) propose RPG-PD and OPG-PD, exhibiting _last-iterate_ global convergence guarantees under Slater's condition in a single-constraint setting. The former is a regularized version of the algorithm proposed by Ding et al. (2020), showing _last-iterate_ global convergence at a sublinear rate. The latter leverages on the optimistic gradient method (Hsieh et al., 2019) to unlock a faster linear convergence rate. These methods show single time-scale dimension-dependent rates and both leverage on exact gradients. However, for 

[MISSING_PAGE_FAIL:17]

### Expected Cost and Mean-Variance

One of the most natural risk measures is represented directly by the _expected cost_ on the trajectories induced by a (hyper)policy. This "risk neutral" risk measure was introduced in Section 2 for both the PG exploration paradigms as \(J_{,i}\) and \(J_{,i}\), where \(i\) is the cost function index.

However, in some scenarios, it may be desirable to minimize jointly the expected cost and its variance. To this end, it is possible to consider the Mean-Variance (MV, Markowitz and Todd, 2000; Li and Ng, 2000) risk measure which models this kind of objective. Indeed, the MV is a combination of the mean cost and its variance over the trajectories. The MV with parameter \(\) over the random variable \(Z\), which represents a loss, is defined as follows:

\[_{}(Z):=[Z]+\, [Z].\] (15)

### Chance Constraints, Value at Risk, and Conditional Value at Risk

Value at Risk (\(_{}\)) and Conditional Value at Risk (\(_{}\)) are two popular and strongly connected risk measures. In the following, we report their standard definition as provided by Rockafellar and Uryasev (2000) and Chow et al. (2017).

\(_{}\)measures risk as the maximum cost that might be incurred with respect to a given confidence level \((0,1)\). Such a quantity models the _potential loss_ and the _probability_ that the loss will occur. In its classical definition, the \(_{}\)is the worst-case loss associated with a probability and a time horizon. This risk metric is particularly useful when there is a well defined failure state. Formally:

\[_{}(Z):=\{z|F_{Z}(z)\}.\] (16)

\(_{}\)measures risk as the expected cost given that such cost is greater than or equal to \(_{}\), and provides a number of theoretical and computational advantages. Such a quantity represents the _expected loss_ if the worst-case threshold is passed (i.e., beyond the \(_{}\)breakpoint). Formally:

\[_{}(Z):=_{}\{+\,[(Z-)^{+}]\}.\] (17)

Another risk measure, similar to the \(_{}\)and the \(_{}\), is the _chance_ one. Considering _chance constraints_ means to consider constraints to hold with high probability. Indeed, constraints are imposed on the probability that the loss random variable exceeds a certain threshold \(n\):

\[_{n}(Z):=(Z n).\] (18)

### Unified Risk Measure Mapping

Here we introduce an objective for writing an optimization problem for different risk measures (Bisi et al., 2022). For every \(i 0,U\), we introduce the functions \(f_{i}:^{2}\) and \(g_{i}:\) in order to express the unified risk-minimization objective function over a risk measure on \(C_{i}\) as:

\[_{_{i}}\{_{}}(|)}{}[f_{i}(C_{i}(), _{i})]+g_{i}(_{i})\}.\] (19)

Thus, by fixing a policy parameter configuration \(\), we obtain an AB unified risk measure over \(C_{i}\) as follows:

\[_{,i}():=_{_{i}}\{ }(|)}{}[f_{i}(C_ {i}(),_{i})]+g_{i}(_{i})\}.\] (20)

In order to switch to the PB exploration paradigm, we need to consider also the expectation over the parameter configuration of the underlying policy. Indeed, by fixing a hyperparameter configuration \(\), we can define:

\[_{,i}():=_{_{i}}\{ _{}}{}[}(|)}{}[f_{i}(C_{i}(), _{i})]]+g_{i}(_{i})\}.\] (21)

By selecting the functions \(f_{i}\) and \(g_{i}\) we can consider distinct risk measures to minimize, as we show in Table 4.

The focus of this section is to derive the mapping between the functions \(f\) and \(g\) and the desired risk measure as shown in Table 4. In particular, we consider just the AB exploration scenario, since then passing to the PB one is straightforward. In this section, we consider a generic cost function \(c:[0,1]\) with its cumulative cost index over a trajectory \(\) which is \(C()=_{t=0}^{T-1}^{t}c(_{,t},_{,t})\). In what follows, with a little abuse of notation, we will write the risk measures with also a dependence on the policy parameters \(\) which induce trajectories \(\).

Mean Cost.This is the simplest case, indeed, we can express the minimization problem of the mean cost as:

\[_{}}_{ p_{}(| )}[C()].\] (22)

Thus, it suffices to select \(f(C(),)=C()\) and \(g()=0\) to make the minimization problem fit with the form of the unified cost minimization formulation.

Conditional Value at Risk.For what concern the CVaR\({}_{}\), we can start from its formulation:

\[_{}_{}() =_{}\{_{} \{+}_{ p_{}(| )}[(C()-)^{+}]\}\}\] (23) \[=_{}\{+_{} }_{ p_{}(|)}[(C()-)^{+}]\}.\] (24)

Thus, we need to select \(f(C(),)=(C()-)^{+}\) and \(g()=\) to complete the mapping to the unified cost minimization objective.

Mean Variance.For the MV\({}_{}()\) objective, we need to make some preliminary observations. In particular, for a generic finite-mean random variable \(X\), we have that:

\[[X]=[X^{2}]- [X]^{2}.\] (25)

Moreover, by Fenchel duality, we have the following:

\[[X]^{2}=_{}\{2\,[X]-^{2}\}.\] (26)

Given that, we can start the derivation from the minimization of the Mean Variance:

\[_{}_{}(C,)\] (27) \[=_{}\{}_{ p _{}(|)}[C()]+}_ { p_{}(|)}[C()^{2}]- }_{ p_{}(|)}[C() ]^{2}\}\] (28) \[=_{}\{}_{ p _{}(|)}[C()]+}_ { p_{}(|)}[C()^{2}]- _{}\{2}_{ p_{ }(|)}[C()]-^{2}\}\}\] (29) \[=_{}\{}_{ p _{}(|)}[C()]+}_ { p_{}(|)}[C()^{2}]+_ {}\{-2}_{ p_{}( |)}[C()]+^{2}\}\}\] (30) \[=_{}\{^{2}+_{ }\{(1-2)}_{ p_{}( |)}[C()]+}_{ p _{}(|)}[C()^{2}]\}\}.\] (31)

Thus, to complete the mapping, we need to select

\[f(C(),)=(1-2)}_{ p_{}( |)}[C()]+}_{ p_{ }(|)}[C()^{2}]\]

and \(g()=^{2}\).

Chance.For characterizing the chance constraints, we start by expressing the probability of an event \(A\) as the expected value of the indicator function:

\[(A)=[\{A\}].\] (32)

Thus, having to minimize a chance measure on the cost with a parameter \(n\), we can rewrite it as:

\[_{}_{n}(C,) =_{}}_{ p_{ }(|)}[(C() n)]\] (33) \[=_{}}_{ p_{ }(|)}[\{C() n\} ].\] (34)Thus, to complete the mapping to the unified cost minimization objective, it suffices to select \(f(C(),)=1\{C() n\}\) and \(g()=0\).

## Appendix D Estimators

Here, we provide all the explicit estimators' forms for both C-PGAE and C-PGPE and for each risk measure. For the sake of clarity, we report the mapping between the \(f_{i}\) and \(g_{i}\) functions and the risk measures in Table 4.

The general Lagrangian function for the problem in Equation (11) is the following:

\[}_{,}(,,)_{,0}(,_{0})+_{u=1}^{ U}_{u}(_{,u}(,_{u})-b_{u} )-\|\|_{2}^{2}.\] (35)

For what follows we are going to compute the following gradients:

1. \(_{}}_{,}(,,)\);
2. \(_{}}_{,}(,,)\);
3. \(_{}}_{,}(, {},)\).

Before entering into the details of the estimators, we show the general forms of the gradients of the Lagrangian.

General Gradient w.r.t. \(\).Notice that the following holds:

\[_{}}_{,}(,,)=_{}_{,0}( {},_{0})+_{u=1}^{U}_{u}_{}_{,u}(,_{u}),\] (36)

thus, in what follows, for the gradient w.r.t. \(\) we focus on the single terms \(_{}_{,u}(,_{u})\) for every \(u 0,U\).

General Gradient w.r.t. \(\).The general gradient w.r.t. \(\) has the form:

\[_{}}_{,}(,,)=_{}(,)- +,\] (37)

where \(_{}(,)=(_{,1 }(,_{1}),...,_{,U}(,_{ U}))^{}\) and \(=(b_{1},...,b_{U})^{}\).

General Gradient w.r.t. \(\).As done for the gradient w.r.t. \(\), the following holds:

\[_{}}_{,}(, {},)=_{}_{,0}(,_{0})+_{u=1}^{U}_{u}_{}_{,u }(,_{u}).\] (38)

Thus, also in this case, we will focus on the single terms \(_{}_{,u}(,_{u})\) for every \(u 0,U\).

### Parameter-based Algorithm: C-PGPE

In the following we consider a generic hyperpolicy \(_{}\) and a generic parameterization \(\). Before starting with the derivations, we report the definition of \(_{,i}(,_{i})\):

\[_{,i}(,_{i})}_{ _{}}[}_{_{}(|)}[f_{i}(C_{i}(),_{i})]]+g_{i} (_{i}),\] (39)

 
**Risk Measure** & **Parameter** & **Need for \(\)** & \(f_{i}(C_{i}(),)\) & \(g_{i}()\) & 
 **AB GOMDP-like** \\ **Estimator** \\  \\  Expected Cost & - & \(\) & \(C_{i}()\) & \(0\) & Yes \\  Mean Variance & \(\) & \(\) & \((1-2)C_{i}()+ C_{i}()^{2}\) & \(^{2}\) & Partial \\  CVaR\({}_{}\) & \(\) & \(\) & \((C_{i}()-)^{+}\) & \(\) & No \\  Chance & \(n\) & \(\) & \(1\{C_{i}() n\}\) & \(0\) & No \\  

Table 4: Mapping between \(f_{i}\) and \(g_{i}\) and the cost measures.

for every \(i 0,U\).

#### d.1.1 Gradients w.r.t. Parameters

The first step of the derivation can be done via the log-trick for the parameter-based exploration paradigm as stated by Sehnke et al. (2010):

\[_{}_{,i}(,_{i}) =_{}*{}_{ _{}}[ *{}_{ p_{}(|)}[f_{i}(C_{i}(),_{i})]]\] (40) \[=*{}_{_{ }}[_{}_{ }()*{}_{ p_{}( |)}[f_{i}(C_{i}(),_{i})]].\] (41)

To switch to the sample-based versions of all the gradients, we consider the behavior of C-PGPE described in Section 4. In particular, we have the following:

\[_{}_{,i}(,_{i})=_{j=1}^{N}_{}_{ }(_{j})f_{i}(C_{i}(_{j}),_{i}).\] (42)

Thus, to obtain the estimators for all the risk measures, it suffices to map the selection of the \(f_{i}\) functions in Equation (42).

#### d.1.2 Gradients w.r.t. Lagrangian Multipliers

Given the general gradient w.r.t. \(\) of the regularized Lagrangian \(}_{,}\) in the parameter-based scenario, the partial derivative w.r.t. \(_{i}\) is the following:

\[_{_{i}}}_{,}(,,)=*{}_{ _{}}[*{}_{ p_{}(|)}[f_{i}(C_{i}( ),_{i})]]+g_{i}(_{i})-b_{i}+_{i}.\] (43)

This is defined for any \(i U\).

In order to switch to the sample-based version of the partial derivative, we consider the behavior of C-PGPE as described in Section 4. In particular, we have the following:

\[_{_{i}}}_{,}( ,,)=_{j= 1}^{N}f_{i}(C_{i}(_{j}),_{i})+g_{i}(_{i})-b_{i}+_{i}.\] (44)

Thus, to obtain the estimator for all the risk measures, it suffices to map the choices of \(f_{i}\) and \(g_{i}\) in Equation (44).

#### d.1.3 Gradients w.r.t. Risk Parameters

For what concern the gradients w.r.t. \(\), we have to enumerate the mappings shown in Table 4. Notice that, when the employed risk measure is the _expected cost_ or the _chance_, the estimator of the gradient w.r.t. \(\) is not needed. Indeed, in the unified risk measure formulation such mappings do not depend on \(\). As shown at the beginning of this section, we can just focus on the terms \(_{}_{,i}(,_ {i})\), and in particular on the partial derivative \(_{_{i}}_{,i}(,_{i})\), that exhibits the common form:

\[_{_{i}}_{,i}(,_ {i}) =_{_{i}}*{}_{ _{}}[*{}_{ p_{ }(|)}[f_{i}(C_{i}(),_{i}) ]]+_{_{i}}g_{i}(_{i})\] (45) \[=*{}_{_{ {}}}[*{}_{ p_{}(| )}[_{_{i}}f_{i}(C_{i}(),_{i}) ]]+_{_{i}}g_{i}(_{i}).\] (46)

Mean Variance.In this case we have that \(f_{i}(C_{i}(),_{i})=(1-2_{i}_{i})C_{i}()+_{i}C_{i}( )^{2}\) and that \(g_{i}(_{i})=_{i}_{i}^{2}\). Thus, from Equation (46), we get the following:

\[_{_{i}}_{,i}(,_{i})= *{}_{_{}} [*{}_{ p_{}(|)}[-2_{i}C_{i}()]]+2_{i}_{i}.\] (47)Considering the behavior of C-PGPE described in Section 4, we obtain the following sample-based version:

\[_{_{i}}_{,i}(,_{i} )=-}{N}_{j=1}^{N}C_{i}(_{j})+2_{i}_{i}.\] (48)

Conditional Value at Risk.In this case we have that \(f_{i}(C_{i}(),_{i})=}(C_{i}()-_{i} )^{+}\) and that \(g_{i}(_{i})=_{i}\). As also shown by Chow et al. (2017), due to the presence of the non-differentiable term \(()^{+}\), we need to resort to sub-differentiability theory. The following holds:

\[_{_{i}}(C_{i}()-_{i})^{+}=-1&C_{i}( )>_{i}\\ -q:\ q&C_{i}()=_{i}\\ 0&.\] (49)

Thus, from Equation (46), we can write what follows:

\[_{_{i}}_{,i}(, _{i})\] (50) \[=}*{}_{_{}}[*{}_{  p_{}(|)}[_{_{i}}(C_ {i}()-_{i})^{+}]]+1\] (51) \[=}*{}_{_{}}[_{}p_{}(| )_{_{i}}(C_{i}()-_{i})^{+} ]+1\] (52) \[=}*{}_{_{}}[-_{}p_{}(| )q\{C_{i}()=_{i}\} -_{}p_{}(|)\{C_{i}( )>_{i}\}]+1,\] (53)

with \(q\).

In particular, for \(q=1\), we obtain the following partial derivative:

\[_{_{i}}_{,i}(,_{i})= }*{}_{ _{}}[*{}_{ p_{}( |)}[\{C_{i}()_{i} \}]]+1.\] (54)

Finally, according to the C-PGPE behavior described in Section 4, we obtain the following sample-based version:

\[_{_{i}}_{,i}(, _{i})=_{j=1}^{N}\{C_{i}(_{j})_{ i}\}+1.\] (55)

With a little abuse of notation, we will use \(_{_{i}}_{,i}(, _{i})=_{_{i}}_{,i}(,_{i})\) for the \(\) update in the case in which the CVaR\({}_{}\)risk measure is employed.

### Action-based Algorithm: C-Pga

In the following we consider a generic policy \(_{}\) and a generic parameterization \(\). Before starting with the derivations, we report the definition of \(_{,i}(,_{i})\):

\[_{,i}(,_{i}):=*{}_{ p_{}(|)}[f_{i}(C_{i}(), _{i})]+g_{i}(_{i}),\] (56)

for every \(i[\![0,U]\!]\).

#### d.2.1 Gradients w.r.t. Parameters

Also in this case, the first step of the derivation can be done via the log-trick, which provides an analogous result of the Policy Gradient Theorem (PGT, Sutton et al., 1999):

\[_{}_{,i}(,_{i}) =_{}*{}_{ p _{}(|)}[f_{i}(C_{i}(),_{i})]\] (57) \[=*{}_{ p_{}(| {})}[_{} p_{}(|)f_{i}(C_{i}(),_{i})]\] (58) \[=*{}_{ p_{}(| {})}[_{t=0}^{T-1}_{}(_{,t},_{,t})f_{i}(C_{i}(),_{i})].\] (59)To switch to the sample-based versions of all the gradients, we generally resort to the Monte Carlo version of \(_{}_{,i}(,_{i})\), obtaining a REINFORCE-like (Williams, 1992) estimator, that is:

\[_{}_{,i}(,_{i})= _{j=1}^{N}(_{t=0}^{T-1}_{}( _{_{j},t},_{_{j},t}))f_{i}(C_{i}(_{j}),_{i}).\] (60)

Thus, to obtain the estimators for all the risk measures, it suffices to map the selection of the \(f_{i}\) functions in Equation (60).

It is worth noticing that some of the choices for \(f_{i}\) and \(g_{i}\) allow to switch to a GPOMDP-like (Baxter and Bartlett, 2001) estimator, which suffers from less variance. This holds for the _expected cost_ and _mean variance_ risk measures.

Expected Cost GPOMDP-like Estimator.In this case \(f_{i}(C_{i}(),_{i})=C_{i}()\), thus we can obtain exactly the GPOMDP estimator:

\[_{}_{,i}(,_{i})= _{j=1}^{N}(_{t=0}^{T-1}^{t}c_{i}(_{_{ j},t},_{_{j},t})_{h=0}^{t}_{}(_{ _{j},h},_{_{j},h})).\] (61)

Mean Variance GPOMDP-like Estimator.In this case \(f_{i}(C_{i}(),_{i})=(1-2_{i}_{i})C_{i}()+_{i}C_{i} ()^{2}\), thus we can obtain the GPOMDP estimator just for the \(C_{i}()\) part:

\[_{}_{,i}(,_{i})\] (62) \[=_{j=1}^{N}(_{t=0}^{T-1}_{}(_{_{j},t},_{_{j},t}))((1-2 _{i}_{i})C_{i}(_{j})+_{i}C_{i}(_{j})^{2})\] (63) \[=_{i}}{N}_{j=1}^{N}(_{t=0}^{ T-1}^{t}c_{i}(_{_{j},t},_{_{j},t})_{h=0}^{t}_{ }(_{_{j},h},_{_{j},h}))\] (64) \[+}{N}_{j=1}^{N}(_{t=0}^{T-1} _{}(_{_{j},t},_{_{j},t}))C_{ i}(_{j})^{2}.\] (65)

#### d.2.2 Gradients w.r.t. Lagrangian Multipliers

The result is the same as the one obtained for C-PGPE. The difference lies in the way in which trajectories are collected. The estimator for the partial derivative w.r.t. \(_{i}\) is:

\[_{_{i}}}_{,}(,,)=_{j=1}^{N}f_{i}(C_{i}(_{j}),_{i})+g_{i}(_{i})-b_{i}+_{i}.\] (66)

To obtain the estimator for all the risk measures, it suffices to map the choices of \(f_{i}\) and \(g_{i}\) as prescribed by Table 4.

#### d.2.3 Gradients w.r.t. Risk Parameters

As for the exploration-based case, also here we need to enumerate the mappings reported in Table 4. However, the _expected cost_ and the _chance_ risk measures do not depend on \(\), thus they are not treated. As shown at the beginning of the section, we can just focus on the \(_{}_{,i}(,_{i})\) terms and, in particular, we consider the partial derivative \(_{_{i}}_{,i}(,_{i})\). For it, we can recover the common form:

\[_{_{i}}_{,i}(,_{i})=}_{ p_{}(|)}[_{_{i}}f_{i}(C_{i}( ),_{i})]+_{_{i}}g_{i}(_{i}).\] (67)

Mean Variance.In this case we have that \(f_{i}(C_{i}(),_{i})=(1-2_{i}_{i})C_{i}()+_{i}C_{i}( )^{2}\) and that \(g_{i}(_{i})=_{i}_{i}^{2}\). Thus, from Equation (67), the following holds:

\[_{_{i}}_{,i}(,_{i})=}_{ p_{}(|)}[-2_{i}C_{i}() ]+2_{i}_{i}.\] (68)Considering the behavior of C-PGAE described in Section 4, we obtain the following sample-based version:

\[_{_{i}}_{,i}(,_{i})=-}{N}_{j=1}^{N}C_{i}(_{j})+2_{i}_{i}.\] (69)

Conditional Value at Risk.In this case we have that \(f_{i}(C_{i}(),_{i})=}(C_{i}()-_{i} )^{+}\) and that \(g_{i}(_{i})=_{i}\). Here we face the same issues we have discussed in the corresponding C-PGPE part. With the same procedure, we obtain the following partial derivative:

\[_{_{i}}_{,i}(,_{i})=}*{}_{_{}}[\{C_{i}()_{i}\} ]+1.\] (70)

Finally, according to the C-PGAE behavior described in Section 4, we obtain the following sample-based version:

\[_{_{i}}_{,i}(,_{i})= _{j=1}^{N}\{C_{i}(_{j})_{i}\} +1.\] (71)

Also in this case, we will use \(_{_{i}}_{,i}(,_{i})= _{_{i}}_{,i}(,_{i})\) for the \(\) update in the case in which the CVaR\({}_{}\)risk measure is employed.

## Appendix E Proofs

**Lemma E.1** (Regularization Bias on Saddle Points - 1).: _Under Assumption 3.1, for every \( 0\), let \((_{}^{},_{}^{})\) be a saddle point of \(_{}\), it holds that:_

\[0_{0}(_{0}^{},_{0}^{})- _{0}(_{}^{},_{}^{}) (\|_{0}^{}\|_{2}^{2}-\|_{}^{}\|_{2}^{2}).\]

Proof.: From the fact that \((_{}^{},_{}^{})\) is a saddle point of \(_{}\), we have for every \(\) and \(\!\!\):

\[_{}(,_{}^{ })_{}(_{}^{},_{ }^{})_{}(_{}^{}, )\] (72) \[_{0}(,_{}^{} )-\|_{}^{}\|_{2}^{2} _{0}(_{}^{},_{}^{ })-\|_{}^{}\|_{2}^{2} _{0}(_{}^{},)-\|\|_{2}^{2}\] (73) \[_{0}(,_{}^{} )_{0}(_{}^{},_{}^{ })_{0}(_{}^{},)+ (\|_{}^{}\|_{2}^{2}-\|_{}^{}\|_{2}^{2}).\] (74)

From the fact that \((_{0}^{},_{0}^{})\) is a saddle point of \(_{0}\), we have for every \(\) and \(\!\!\):

\[_{0}(,_{0}^{})\] (75)

By setting \((,)(_{}^{},_{}^{})\) in Equation (75) and \((,)(_{0}^{},_{ 0}^{})\) in Equation (74), we obtain:

\[_{0}(_{}^{},_{0}^{ }) \] (76) \[=_{0}(_{0}^{},_{}^ {}) \] (77) \[\] (78)

thus:

\[_{0}(_{0}^{},_{0}^{}) _{0}(_{}^{},_{}^{}) _{0}(_{0}^{},_{0}^{})+ (\|_{}^{}\|_{2}^{2}-\|_{0}^{}\|_{2}^{2}).\] (79)

**Lemma E.2** (Regularization Bias on Saddle Points - 2).: _Under Assumption 3.1, for every \( 0\), it holds that:_

\[0_{}_{}_{ 0}(,)-_{}_{}_{}(,)\|_{0}^{}\|_{2}^{2}.\]

Proof.: The first inequality follows from the observation that \(_{0}(,)_{}(, )\) for every \( 0\). For the second inequality, let us denote as \((_{}^{},_{}^{})\) the saddle point for \(_{}\) and let \(^{}=\{_{0}^{},_{}^{}\}\). We have:

\[_{0}(_{0}^{},_{0}^{})- _{}(_{}^{},_{}^{})=_ {}_{^{}}_{0}( ,)-_{}_{^{}}_{}(,)\] (80)

[MISSING_PAGE_FAIL:25]

* \(_{}(,)\) _is_ \(\)_-smooth, i.e., for every_ \(,^{}\!\) _it holds that:_ \[_{}_{}(,^{})- _{}_{}(,) \,-^{}_{2}^{2}\]
* \(_{}(,)\) _satisfies the PL condition, i.e., for every_ \(\!\) _it holds that:_ \[_{}_{}(,)_{ 2}^{2}(_{^{}}_{ }(,^{})-_{}(,)).\]
* \(_{}(,)\) _satisfies the_ error bound _(EB) condition, i.e., for every_ \(,^{}\!\) _it holds that:_ \[_{}_{}(,) \|^{*}()-\|_{2},\] _where_ \(^{*}()=_{}_{ }(,)\)_._
* \(_{}(,)\) _satisfies the_ quadratic growth _(QG) condition, i.e., for every_ \(,^{}\!\) _it holds that:_ \[H_{}()-_{}(,) {4}\|^{*}()-\|_{2},\] _where_ \(^{*}()=_{}_{ }(,)\)_._

Proof.: For the first property, it is enough to observe that \(_{}\) is twice differentiable in \(\) and that its Hessian is \(\). For the second property, we observe that \(_{}\) is quadratic in \(\) and, consequently it satisfies the PL condition with parameter \(\):

\[_{}_{}(,)_ {2}^{2}(_{^{}^{U}}_{}(,^{})-_{}(,))(_{^{}} _{}(,^{})-_{}(,)).\]

For the third and fourth properties, we refer to Lemma A.1 of Yang et al. (2020). 

**Lemma E.6**.: _Let \(>0\). For every \(\), it holds that:_

\[H_{}()-H_{}^{*}\|^{*}()-_{}^{*}\|_{2}.\] (97)

Proof.: Let us consider the following derivation:

\[H_{}()-H_{}^{*} =H_{}()-_{}(_{}^{*}, {}_{}^{*})\] (98) \[ H_{}()-_{}(, _{}^{*})\] (99) \[\|^{*}()-_{ }^{*}\|_{2}.\] (100)

having exploited the fact that, from the saddle point property, \(_{}(_{}^{*},_{}^{*}) _{}(,_{}^{*})\) and, then, Lemma E.5. 

**Lemma E.7**.: _Let \(>0\). The following statements hold:_

* \(H_{}\) _is_ \(L_{H}\)_-smooth, i.e., for every_ \(,^{}\)_, it holds that:_ \[_{}H_{}(^{})-_{}H_{}( )_{2} L_{H}^{}-_{2}.\] _where_ \(L_{H} L_{2}+^{2}}{}\)_._
* _For every_ \(,^{}\) _we have_ \(_{}H_{}()=_{}_{}(, )|_{=^{*}()}\)_, where_ \(^{*}()=_{}_{}( ,)\)_._

Proof.: The first and second statements follow from Lemma A.5 of Nouiehed et al. (2019). 

**Theorem 3.1** (Objective Function Gap and Constraint Violation).: _Let \(>0\). Under Assumption 3.1, if \(_{k}\), it holds that:_

\[[J_{0}(_{k})-J_{0}(_{0}^{*})]+ \|_{0}^{*}\|_{2}^{2},[(J_{i}(_{k})-b_{i})^{+} ] 4+_{0}^{*}\|_{2}, i  U.\] (8)

Proof.: Since \(_{k}\), it follows that \(a_{k}\) and, consequently, \(0[H_{}(_{k})-H_{}^{*}]\). We start by bounding the norm of the dual variables:

\[\|^{*}(_{k})\|_{2}\|_{}^{*}\|_{2}+\|^{*}(_{k})-_{}^{*}\|_{2}\] (101)\[\|_{}^{}\|_{2}+(H_{}(_{k})- H_{}^{}).\] (102)

where we applied the triangular inequality and Lemma E.6. The projection \(_{}\) is such that \(^{}()=_{}((()-))=(()-)^{+}\) and, consequently, we have:

\[\|((_{k})-)^{+}\|_{2}-\|((_{}^ {})-)^{+}\|_{2} 4(H_{}(_{k})-H_{}^{}).\] (103)

By the last inequality, together with Lemma E.3, having applied the expectation on both sides:

\[[\|((_{k})-)^{+}\|_{2}] \|((_{}^{})-)^{+}\|_{2}+ 4\,[H_{}(_{k})-H_{}^{}]\] (104) \[\|_{0}^{}\|_{2}+4.\] (105)

Recalling that:

\[[[((_{k})-))^{+}\|_{2}] \|[((_{k})-)^{+}] \|_{2}\|\,[((_{k})-)^{+}]\|_{ }.\] (106)

For the objective function bound, let us consider the following derivation. By definition of \(H_{}()\) and \(^{}()\) we have:

\[J_{0}(_{k})-J_{0}(_{}^{}) =H_{}(_{k})-H_{}^{}-( \|^{}(_{k})\|_{2}^{2}-\|_{}^{}\|_{ 2}^{2}).\] (107)

Taking the expectation on both sides and upper bounding \(\|_{}^{}\|\) with \(\|_{0}^{}\|\) from Lemma E.1:

\[[J_{0}(_{k})-J_{0}(_{}^{})] =[H_{}(_{k})-H_{}^{}]-\,[\|^{}(_{k})\|_{2}^{2}-\|_{}^{}\|_{2}^{2}]\] (108) \[[H_{}(_{k})-H_{}^{}]+\|_{}^{}\|_{2}^{2}\] (109) \[+\|_{0}^{}\|_{2}^{2}.\] (110)

The result is obtained by applying Lemma E.3 as follows:

\[[J_{0}(_{k})-J_{0}(_{0}^{})]=[J_{0}(_{k})-J_{0}(_{}^{})]+(_{}^{} )-J_{0}(_{0}^{})}_{ 0}.\] (111)

**Theorem 3.2** (Convergence of \(_{K}\)).: _Under Assumptions 3.2, 3.3, 3.4, for \(<1/5\), sufficiently small \(\) and \(\), and a choice of constant learning rates \(_{},_{}\), we have \(_{K}()+_{1}/_{1}\) whenever:8_

* \(K=(^{-1}(^{-1}))\) _if_ \(=2\) _and the gradients are exact (i.e.,_ \(V_{}=V_{}=0\)_);_
* \(K=(^{-1}^{--1})\) _if_ \([1,2)\) _and the gradients are exact (i.e.,_ \(V_{}=V_{}=0\)_);_
* \(K=(^{-3}^{-+1})\) _if_ \(\) _and the gradients are estimated (i.e.,_ \(V_{}=(^{-2})\) _and_ \(V_{}=(1)\)_)._

Proof.: The proof is subdivided into several parts. We will omit the \(\) subscript for notational easiness. Let us focus on a specific iteration \(k\).

**Part I: bounding the \(a_{k}\) term.** Let us start with the \(a_{k}\) term:

\[H_{}(_{k+1})-H^{}  H_{}(_{k})-H^{}+_{k+1}-_{k},\ _{}H_{}(_{k})+}{2}\|_ {k+1}-_{k}\|_{2}^{2}\] (112) \[ H_{}(_{k})-H^{}-_{,k} _{}_{}(_{k},_{k}),\ _{}H_{}(_{k})\] (113) \[+}{2}_{,k}^{2}| _{}_{}(_{k},_{k})\|_{2}^{2},\] (114)

where the first line is due to the fact that the function \(H_{}\) is \(L_{H}\)-smooth (Lemma E.7), the last inequality is due to the update rule of \(\). Now, we apply the expected value on both sides of the inequality and we use the fact that the gradient estimation is unbiased and has variance bounded by \(V_{}\):

\[[H_{}(_{k+1})|_{k-1}]- H^{}  H_{}(_{k})-H^{}-_{,k}_{} _{}(_{k},_{k}),\ _{}H_{}(_{k})\] (115) \[+}{2}_{,k}^{2}\,[\| _{}_{}(_{k},_{k}) \|_{2}^{2}|_{k-1}],\] (116)where \(_{k-1}\) is the filtration associated with all events realized up to interaction \(k-1\). We recall that:

\[[\|_{}_{ }(_{k},_{k})\|_{2}^{2}|_{k-1}]=[_{}_{ }(_{k},_{k})|_{k-1}]+ \|_{}_{}(_{k}, _{k})\|_{2}^{2},\] (117)

and that \([_{}_{}( _{k},_{k})] V_{}\) by Assumption 3.4. Thus, selecting \(_{,k} 1/L_{H}\), we have that:

\[[H_{}(_{k+1})|_{k- 1}]-H^{*}\] (118) \[ H_{}(_{k})-H^{*}-_{ {v},k}_{}_{}(_ {k},_{k})\;_{}H_{}(_{k})\] (119) \[+,k}}{2}\|_{ }_{}(_{k},_{ k})\|_{2}^{2}+}{2}_{,k}^{2}V_{}\] (120) \[=H_{}(_{k})-H^{*}-_{,k} _{}_{}(_{k}, _{k})\;_{}H_{}(_{ k})\] (121) \[+,k}}{2}\|_{ }_{}(_{k},_{ k})_{}H_{}(_{k})\|_{2}^{2}+}{2} _{,k}^{2}V_{}.\] (122)

Consider that:

\[,k}}{2}\|_{} _{}(_{k},_{k})-_{ }H_{}(_{k})+_{}H_{ }(_{k})\|_{2}^{2}\] (123) \[=,k}}{2}\|_{ }_{}(_{k},_{k})-_{ }H_{}(_{k})\|_{2}^{2}-,k}}{2}\|_{}H_{}(_{ k})\|_{2}^{2}\] (124) \[+_{,k}_{ }_{}(_{k},_{k}),\;_{ }H_{}(_{k}).\] (125)

Thus, the following holds:

\[[H_{}(_{k+1})|_{k- 1}]-H^{*}\] (126) \[ H_{}(_{k})-H^{*}-_{ {v},k}_{}_{}(_{k},_{k}),\;_{}H_{}( {v}_{k})+}{2}_{,k}^{2}V_{ }\] (127) \[+,k}}{2}\|_{ }_{}(_{k},_{ k})-_{}H_{}(_{k})\|_{2}^{2}-,k}}{2}\|_{}H_{}(_ {k})\|_{2}^{2}\] (128) \[+_{,k}_{ }_{}(_{k},_{k}),\;_{ }H_{}(_{k})\] (129) \[=H_{}(_{k})-H^{*}-,k}}{2}\|_{}H_{}(_{k})\|_{ 2}^{2}+,k}}{2}\|_{} _{}(_{k},_{k})-_{ }H_{}(_{k})\|_{2}^{2}\] (130) \[+}{2}_{,k}^{2}V_{ {v}}.\] (131)

Thus, we have obtained:

\[ [H_{}(_{k+1})| _{k-1}]-H^{*}\] (132) \[ H_{}(_{k})-H^{*}-,k}}{2}\|_{}H_{}(_ {k})\|_{2}^{2}+,k}}{2}\|_{ }_{}(_{k},_{k})- _{}H_{}(_{k})\|_{2}^{2}\] (133) \[+}{2}_{,k}^{2}V_{ {v}},\] (134)

holding via the selection of \(_{,k} 1/L_{H}\). Notice that, from \(\) the following directly follows:

\[} [H_{}(_{k+1})| _{k-1}]-H_{}(_{k})\] (135) \[-,k}}{2}\|_{ }H_{}(_{k})\|_{2}^{2}+,k}}{2}\|_{}_{}( _{k},_{k})-_{}H_{}( _{k})\|_{2}^{2}+}{2}_{,k}^{2}V_ {}.\] (136)

**Part II: bounding the \(b_{k}\) term.** We are ready to analyze the \(b_{k}\) term. Recall that for ridge regularization of the Lagrangian function presented in the main paper, we have that \(_{}\) is \(\)-smooth and fulfills the PL condition with constant \(\), as shown in Lemma E.5. Since \(\) is a quadratic function of \(\) and \(^{*}(_{k+1})\), we have that considering the non-projected \(_{k+1}\) can only increase the distance. Thus, we will ignore projection for the rest of the proof. We have:

\[H_{}(_{k+1})-_{}( {v}_{k+1},_{k+1})\] (137) \[ H_{}(_{k+1})-_{}( _{k+1},_{k})-_{k+1}-_{k},\;_{}

\[_{,k}(1+}{2}_{,k}) \|_{}_{}(_{k},_{k})\| _{2}^{2}+}{2}_{,k}^{2}V_{},\] (158)

having set \(_{,k} 1/L_{2}\). We are finally able to conclude the bound of the term \(}\):

\[}[H_{}(_{k+1})- _{}(_{k+1},_{k+1})|_{k-1}]\] (159) \[(1-,k}}{2})[H_{}(_{k+1})-_{}(_{k+1}, _{k})|_{k-1}]+_{,k}^{2}V_{ }\] (160)\[=(1-}{2})(H_{}( _{k})-_{}(_{k},_{k}))\] (161) \[+(1-}{2})( _{}(_{k},_{k})- [_{}(_{k+1},_{k})| _{k-1}])\] (162) \[+(1-}{2})( [H_{}(_{k+1})|_{k-1}]-H_{ }(_{k}))+_{,k}^{2}V_{ }.\] (163)

Now we apply the bounds on \(\) and \(\) (the latter is from Eq. 136), obtaining:

\[[H_{}(_{k+1})-_{ }(_{k+1},_{k+1})|_{k-1}]\] (164) \[(1-}{2})(H_{ }(_{k})-_{}(_{k}, _{k}))\] (165) \[+(1-}{2})( _{,k}(1+}{2}_{,k} )\|_{}_{}(_{k}, _{k})\|_{2}^{2}+}{2}_{,k}^{2}V_{})\] (166) \[+(1-}{2})(- ,k}}{2}\|_{}H_{}( _{k})\|_{2}^{2}+,k}}{2}\| _{}_{}(_{k},_{k})-_{}H_{}(_{k})\|_{2} ^{2}.\] (167) \[.+}{2}_{,k}^{2}V_{ })+_{,k}^{2}V_{},\] (168)

that is the second fundamental term.

**Part III: bounding the potential function \(P_{k}()\).** Before going on, we recall that so far we enforced: \(_{,k} 1/L_{H}\) (since \(L_{H} L_{2}\)) and \(_{,k} 1/\), for every \(t K\). What we want to bound here is the potential function \(P_{k+1}()=a_{k+1}+ b_{k+1}\). Using the final results of Part I and Part II:

\[a_{k+1} + b_{k+1}=[H_{}(_{k+1})- H^{}]+\,[H_{}(_{k+1})- _{}(_{k+1},_{k+1})]\] (169) \[[H_{}(_{k})-H^{} ]-,k}}{2}\,[\|_{ }H_{}(_{k})\|_{2}^{2}]\] (170) \[+,k}}{2}\,[\| _{}_{}(_{k},_{k})-_{}H_{}(_{k})\|_{2 }^{2}]+}{2}_{,k}^{2}V_{}\] (171) \[+(1-,k}}{2} )[H_{}(_{k})-_{}( _{k},_{k})]\] (172) \[+(1-,k}}{2} )(_{,k}(1+}{2}_{ ,k})[|_{} _{}(_{k},_{k})\|_{2}^{2}]+ }{2}_{,k}^{2}V_{})\] (173) \[+(1-,k}}{2} )(-,k}}{2}\,[\| _{}H_{}(_{k})\|_{2}^{2}].\] (174) \[+,k}}{2}\, [\|_{}_{}(_{k}, _{k})-_{}H_{}(_{k}) \|_{2}^{2}]+}{2}_{,k}^{2}V_{ })\] (175) \[+_{,k}^{2}V_{ }\] (176) \[=a_{k}+(1-,k}}{2} )b_{k}\] (177) \[-,k}}{2}(1+(1-,k}}{2}))[\| _{}H_{}(_{k})\|_{2}^{2}]\] (178) \[+,k}}{2}(1+(1-,k}}{2}))[\| _{}_{}(_{k},_{k})-_{}H_{}(_{k})\|_{2}^{2}]\] (179) \[+_{,k}(1+}{2}_{ ,k})(1-,k}}{2} )[\|_{}_{}( _{k},_{k})\|_{2}^{2}]\] (180) \[+,k}^{2}}{2}(L_{H}+ (1-,k}}{2})(L_{H}+L_{2} ))V_{}+_{,k}^{2}V_{}.\] (181)Now we can re-arrange the terms by noticing that:

\[\|_{}_{}(_{k}, _{k})\|_{2}^{2} =\|_{}_{}(_{ k},_{k})-_{}H_{}(_{k})+ _{}H_{}(_{k})\|_{2}^{2}\] (182) \[=\|_{}_{}(_ {k},_{k})-_{}H_{}(_{k}) \|_{2}^{2}\] (183) \[+\|_{}H_{}(_{k}) \|_{2}^{2}+2_{}_{}(_{k},_{k})-_{}H_{}(_{k}),\ _{}H_{}(_{k})\] (184) \[ 2\|_{}_{}(_{k},_{k})-_{}H_{}(_{k})\|_{2}^{2}+2\|_{}H_{}(_{k})\|_{2}^{2},\] (185)

where the last inequality holds by Young's inequality. Then we can write what follows:

\[a_{k+1}+ b_{k+1}\] (186) \[ a_{k}+(1-}{2})b_ {k}\] (187) \[+(2_{,k}(1+}{2} _{,k})(1-}{2}).\] (188) \[.-,k}}{2}(1+(1- }{2})))[\| _{}H_{}(_{k})\|_{2}^{2}]\] (189) \[+(2_{,k}(1+}{2} _{,k})(1-}{2})+ ,k}}{2}(1+(1- }{2})))\] (190) \[[\|_{}_{}(_{k},_{k})-_{}H_{ }(_{k})\|_{2}^{2}]\] (191) \[+,k}^{2}}{2}(L_{H}+( 1-}{2})(L_{H}+L_{2}))V_{ }+_{,k}^{2}V_{}.\] (192)

Let us now proceed to bound \(\|_{}_{}(_{k},_{k})-_{}H_{}(_{k})\|_{2}^ {2}\). By Lemma E.7, we have that \(_{}H_{}()=_{} _{}(,^{}())\) for every \(^{}()_{}}_{}(,})\), thus we can write:

\[\|_{}_{}(_{k},_{k})-_{}H_{}(_{k}) \|_{2}^{2} =\|_{}_{}( _{k},_{k})-_{}_{}(_{k},^{}(_{k}))\|_{2}^{2}\] (193) \[ L_{3}^{2}\|^{}(_{k})-_{k}\|_{2}^{2},\] (194)

since Assumption 3.3 holds.

For a fixed value of \(\), by Lemma E.5 it follows that \(_{}(,)\) satisfies the quadratic growth condition (since it satisfies the PL condition), for which the following holds:

\[\|^{}(_{k})-_{k}\|_{2}^{2} (H_{}(_{k})-_{}( _{k},_{k})),\] (195)

and thus we have:

\[\|_{}_{}(_{k},_{k})-_{}H_{}(_{k})\|_{2}^ {2}^{2}}{}(H_{}(_{k})-_ {}(_{k},_{k})).\] (196)

By applying the total expectation, it trivially follows:

\[[\|_{}_{}(_{ k},_{k})-_{}H_{}(_{k})\|_{2}^{2} ]^{2}}{}\,[H_{}(_{k})- _{}(_{k},_{k})]=^{2} }{}b_{k}.\] (197)

Thus, we have:

\[a_{k+1}+ b_{k+1}\] (198) \[ a_{k}+(1-}{2})b_ {k}\] (199) \[+(2_{,k}(1+}{2}_{ ,k})(1-}{2}).\] (200) \[.-,k}}{2}(1+(1- ,k}}{2})))[ \|_{}H_{}(_{k})\|_{2}^{2}]\] (201) \[+(2_{,k}(1+}{2}_{ ,k})(1-}{2})+ ,k}}{2}(1+(1-}{2} )))^{2}}{}b_{k}\] (202)\[+,k}^{2}}{2}(L_{H}+(1-,k}}{2})(L_{H}+L_{2}))V_{}+ _{,k}^{2}V_{}.\] (203)

**Part IV: apply the \(\)-gradient domination.** Now we need to bound the term \(\| H_{}(_{k})\|_{2}^{2}\). We consider Assumption 3.2 and we get: \(\|_{}H_{}(_{k})\|_{2}^{ }_{1}(H_{}(_{k})-H^{*})-_{1}\). By defining \(^{*}:=H^{*}+_{1}/_{1}\), we also have:

\[\|_{}H_{}()\|_{2}^{} _{1}\{0,\ H_{}()-^{*}\}\] (204)

\[\|_{}H_{}()\|_{2}^{2} _{1}^{}\{0,\ H_{}()-^{*}\}^{}.\]

If we apply the total expectation on both sides of the inequality, we get:

\[[\|_{}H_{}()\|_{2}^{2}] _{1}^{}\ [\{0,\ H_{}()-^{*}\}^{}]\] (205) \[_{1}^{}\ [\{0,\ H_{ }()-^{*}\}]^{}\] (206) \[_{1}^{}\{0,\ [H_{}()-^{*}]\}^{},\] (207)

which is achieved by a double application of Jensen's inequality, since \(z^{2/}\) is convex for \(\) and \(z 0\), and the maximum is convex. Let us start from Equation (198):

\[a_{k+1}+ b_{k+1}\] (208) \[ a_{k}+(1-,k}}{2} )b_{k}\] (209) \[+,k}(1+}{2}_{,k})(1-,k}}{ 2})-,k}}{2}(1+(1-,k}}{2})))}_{=:-C}\] (210) \[[\| H_{}(_{k})\|_{2}^{2}]\] (211) \[+(2_{,k}(1+}{2}_{,k})(1-,k}}{2})+ ,k}}{2}(1+(1-,k}}{2})))^{2}}{}b_{k}\] (212) \[+,k}^{2}}{2}(L_{H}+ (1-,k}}{2})(L_{H}+L_{2})) V_{}+_{,k}^{2}V_{}}_{=:V}.\] (213)

We first enforce the negativity of \(-C\). To this end:

\[-C=(2_{,k} }{2}_{,k})}_{ 3/2}\!(1-,k}}{2})-,k}}{2}(1+\! (1-,k}}{2})))\] (214) \[_{,k}(3\!(1-,k}}{2})-(1+\!(1-,k}}{2})))\] (215) \[,k}}{2}(5,k}}{2})}_{ 1}-1)_{,k},k}}{2}(5-1) 0.\] (216)

Thus, it is enough to enforce \(5-1 0 1/5\). We now plug in the gradient domination inequalities:

\[a_{k+1}+ b_{k+1}\] (217)

[MISSING_PAGE_FAIL:33]

\[_{,k}_{,k}}{(1+) _{1}^{}+4L_{3}^{2}(1+7)},\] (236)

where we exploited \(_{,k} 1/L_{2}\) and \(_{,k} 2/\). Thus, we have:

\[_{k+1}()_{k}()-2^{1-}C \{0,\ _{k}()\}^{}+V.\] (237)

Collecting all conditions on the learning rates, we have:

\[_{,k} \{},}, _{,k}}{(1+)_{1}^{}+4L_{3}^ {2}(1+7)}\},\] (238) \[_{,k} \{,\}=.\] (239)

As a further simplification, let us observe that:

\[C =(-2_{,k}}{2} _{,k})}_{ 3/2}(1-,k}}{2} )+,k}}{2}(1+(1-,k}}{2})))_{1}^{}\] (240) \[,k}}{2}(1+5(1-,k}}{2}))_{1}^{},k}_{1}^{}}{2}.\] (241)

\[V =,k}^{2}}{2}(L_{H}+(1-,k}}{2})(L_{H}+L_{2}))V_{}+ _{,k}^{2}V_{}\] (242) \[,k}^{2}}{2}((1+2)L_{2}+(1+) ^{2}}{})V_{}+_{,k}^{2}V_{}=:.\] (243)

Denoting with \(=:2^{1-},k}_{1}^{ {}}}{2}\), we are going to study the recurrence:

\[_{k+1}()_{k}()-\{ 0,\ _{k}()\}^{}+.\] (244)

#### Part V: Rates Computation

_Part V(a): Exact gradients_ We consider the case \(=0\). Let us start with \(=2\). From Lemma G.3, we have:

\[_{K}()(1-)^{K}_{0}( )\] (245)

\[ K_{0}()}{}}{ {1-}}^{-1}_{0}()}{ }=_{0}()}{}}{2^{1-}_{,t}_{1}^{}}\] (246)

The inequality on \(K\) holds under the conditions:

\[ _{0}()^{-1}} _{,k}}}{_{1}^{ {2}{}}_{0}()^{-1}},\] (247) \[_{,k} \{},}, _{,k}}{(1+)_{1}^{}+4L_{3}^{2 }(1+7)}\}\] (248) \[=\{+^{2}}{}},_{,k}}{(1+)_{1}^{}+4L_{3} ^{2}(1+7)}\},\] (249) \[_{,k} ,\] (250)where the first one derives from the hypothesis of Lemma G.3 and the other two from the conditions on the learning rates derived in the previous parts. We set:

\[_{,k} =^{-1},\] \[_{,k} =\{}}{_{1}^{}_{0}()^{-1}},+^ {2}}{}},^{}+4L_{ 3}^{2}(1+7)}\}=().\]

Thus, the sample complexity becomes \(K=(^{-1})\).

Consider now \([1,2)\). We have from Lemma G.3:

\[_{K}()((-1) }K)^{-}\] (251) \[ K}^{-1} ^{-+1}=}_{ ,k}_{1}^{}}^{-+1},\] (252)

holding under the same conditions as before. With the same choices of learning rates, we obtain the sample complexity \(K=(^{-1}^{-+1})\) as sample complexity.

Part V(b): Estimated gradientsWe consider \(>0\). In this case, from Lemma G.5, we have:

\[_{K}()(1-}^{1- }^{})^{K}_{0}( )+(}{}})^{}.\] (253)

We enforce both terms to be smaller or equal to \(/2\). With the first one, we can evaluate the sample complexity:

\[(1-^{1-}} ^{})^{K}_{0}()\] (254) \[ K_{0}()}{}}{ ^{1-}}^{}}\] (255) \[=_{0}()}{}}{ (,k}^{2}}{2}((1+2)L_{2}+(1+)^ {2}}{})V_{}+_{,k}^{2}V_{ })^{1-}(2^{1-},k}_{1}^{}}{2})^{}}\] (256)

Regarding the second one, we have:

\[(}{})^{}(,k}^{2}}{2}((1+2 )L_{2}+(1+)^{2}}{})V_{}+ _{,k}^{2}V_{}}{2^{1-},k}_{1}^{}}{2}})^{}\] (257)

By enforcing the relation between the two learning rates, we set \(_{,k}=(^{2}_{,k})\). By enforcing the previous inequality, recalling that \(L_{2}(^{-1})\) and \(V_{}(^{-2})\), we obtain \(_{}=(^{2/})\), from which \(_{}=(^{3}^{2/})\). Substituting these values into the sample complexity upper bound, we get (highlighting the terms possibly depending on \(\)):

\[K (}{((L_{2}+ ^{-1})V_{}_{}^{2}+_{}^{2})^{1-/2} _{}^{/2}})\] (258) \[=(}{((L_{2}+ ^{-1})V_{}(^{3}^{2/})^{2}+(^{2/ })^{2})^{1-/2}(^{3}^{2/})^{/2}})\] (259) \[(}{^{3} ^{4/-1}}),\] (260)

having bounded the sum at the denominator with the second addendum.

Technical Lemmas

**Lemma F.1**.: _Let \(a\), \(b 0\), and \(\). It holds that:_

\[\{0,a\}^{} 2^{1-}\{0,a+b\}^{}-b^{}.\] (261)

Proof.: Let us consider the following derivation:

\[\{0,a\}^{} =a^{}&a>0\\ 0&\] (262) \[2^{1-}(a+b)^{}-b^{ }&a>0\\ 0&\] (263) \[=2^{1-}(a+b)^{}-b^{ {2}{}}&a>0\\ 0&-b<a 0\\ 0&\] (264) \[2^{1-}(a+b)^{}-b^{ }&a>0\\ 2^{1-}(a+b)^{}-b^{}&-b<a 0\\ -b^{}&\] (265) \[=2^{1-}(a+b)^{}-b^{ {2}{}}&a+b>0\\ -b^{}&\] (266) \[=2^{1-}\{0,a+b\}^{}-b^{},\] (267)

where the first inequality follows from \((x+y)^{} 2^{-1}(x^{}+y^{})\) for \(x,y 0\), from Holder's inequality; the second inequality from observing that \(2^{1-}(a+b)^{}-b^{}(2^{1-}-1)b^{} 0\) for \(-b<a 0\). 

## Appendix G Recurrences

In this section, we provide auxiliary results about convergence rate of a certain class of recurrences that will be employed for the convergence analysis of the proposed algorithms. Specifically, we study the recurrence:

\[r_{k+1} r_{k}-a\{0,r_{k}\}^{}+b\] (269)

for \(a>0\), \(b 0\), and \(\). To this end, we consider the helper sequence:

\[_{0}=r_{0}\\ _{k+1}=_{k}-a\{0,_{k}\}^{}+b\] (270)

The line of the proof follows that of Montenegro et al. (2024). Let us start showing that for sufficiently small \(a\), the sequence \(_{k}\) upper bounds \(r_{k}\).

**Lemma G.1**.: _If \(a^{b-1}}\) for every \(k 0\), then, \(r_{k}_{k}\) for every \(k 0\)._

Proof.: By induction on \(k\). For \(k=0\), the statement holds since \(_{0}=r_{0}\). Suppose the statement holds for every \(j k\), we prove that it holds for \(k+1\):

\[_{k+1} =_{k}-a\{0,_{k}\}^{}+b\] (271) \[ r_{k}-a\{0,r_{k}\}^{}+b\] (272) \[ r_{k+1},\] (273)

where the first inequality holds by the inductive hypothesis and by observing that the function \(f(x)=x-a\{0,x\}^{}\) is non-decreasing in \(x\) when \(a^{b-1}}\). Indeed, if \(x<0\), then \(f(x)=x\), which is non-decreasing; if \(x 0\), we have \(f(x)=x-ax^{}\), that can be proved to be non-decreasingin the interval \([0,(a)^{-}]\) simply by studying the sign of the derivative. Thus, we enforce the following requirement to ensure that \(_{k}\) falls in the non-decreasing region:

\[_{k}(a)^{-} a^{ -1}}.\] (274)

So does \(r_{k}\) by the inductive hypothesis. 

Thus, from now on, we study the properties of the sequence \(_{k}\). Let us note that, if \(_{k}\) is convergent, then it converges to the fixed-point \(\) computed as follows:

\[=-a\{0,\}^{}+b =()^{},\] (275)

having retained the positive solution of the equation only, since the negative one never attains the maximum \(\{0,\}\). Let us now study the monotonicity properties of the sequence \(_{k}\).

**Lemma G.2**.: _The following statements hold:_

* _If_ \(r_{0}>\) _and_ \(a^{}}\)_, then for every_ \(k 0\) _it holds that:_ \(_{k+1}_{k}\)_._
* _If_ \(r_{0}<\) _and_ \(a^{-1}}\)_, then for every_ \(k 0\) _it holds that:_ \(_{k+1}_{k}\)_._

Proof.: The proof is analogous to that of (Montenegro et al., 2024, Lemma F.3). 

From now on, we focus on the case in which \(r_{0}\), since, as we shall see later, the opposite case is irrelevant for the convergence guarantees. We now consider two cases: \(b=0\) and \(b>0\).

### Analysis when \(b=0\)

From the policy optimization perspective, this case corresponds to the one in which the gradients are exact (no variance). Recall that here \(=0\). We have the following convergence result.

**Lemma G.3**.: _If \(a^{}}\), \(r_{0} 0\), and \(b=0\) it holds that:_

\[_{k+1}(1-a)^{k+1}r_{0}&=1\\ \{r_{0},((-1)a(k+1))^{-}\}&(1,2] .\] (276)

Proof.: Since \(r_{0} 0=\), from Lemma G.2, we know that \(_{k} 0\) and, thus, \(\{0,_{k}\}=_{k}\). For \(=1\), we have:

\[_{k+1}=_{k}-a_{k}=(1-a)_{k}=(1-a)^{k+1}_{0}=(1-a)^{k+1}r_ {0}.\] (277)

For \((1,2]\), we have:

\[_{k+1}=_{k}-a_{k}^{}.\] (278)

We proceed by induction. For \(k=0\), the statement hold since \(_{0}=r_{0}\) and \(r_{0}( a)^{-}((-1)a)^{-}\) from the condition on the learning rate. Suppose the thesis holds for \(j k\), we prove it for \(k+1\). \(_{k+1} r_{0}\) by monotonicity, and, from the inductive hypothesis:

\[_{k+1}=_{k}-a_{k}^{} ( ak)^{-}-a( ak)^{-}\] (279) \[=(_{()}^{-}-( a(k+1) )^{-}-a( ak)^{-}+( a(k+1))^{-}.\] (280)

We now prove that \(()\) is non-positive:

\[() =((-1)ak)^{-}-((-1)a(k+1))^{-}-a((-1)ak)^{-}\] (281) \[=((-1)a)^{-}k^{-} {(k-(k+1)()^{})}_{ }-a^{-}((-1)k)^{-}\] (282)\[ a^{-}k^{-}(-1)^{-}(-) 0,\] (283)

having observed that:

\[_{k 1}(k-(k+1)()^{} )=_{k+}(k-(k+1)()^{ })=.\] (284)

### Analysis for \(b>0\)

From the policy optimization perspective, this corresponds to the case in which the gradients are estimated, i.e., the variance is positive. In this case, we proceed considering the helper sequence:

\[_{0}=_{0}\\ _{k+1}=(1-a^{-1})_{k}+bk  0.\] (285)

We show that the sequence \(_{k}\) upper bounds \(_{k}\) when \(_{0}=r_{0}\).

**Lemma G.4**.: _If \(r_{0}>\) and \(a^{}}\), then, for every \(k 0\), it holds that \(_{k}_{k}\)._

Proof.: The proof is analogous to that of (Montengro et al., 2024, Lemma F.4). 

Thus, we can provide the convergence guarantee.

**Lemma G.5**.: _If \(a^{}}\), \(r_{0} 0\), and \(b>0\) it holds that:_

\[_{k+1}(1-b^{1-}a^{})^{k+1}+ ()^{}.\] (286)

Proof.: By unrolling the recursion:

\[_{k+1} =(1-a^{-1})_{k}+b\] (287) \[=(1-a^{-1})^{k+1}r_{0}+b_{j=0} ^{k}(1-a^{-1})^{j}\] (288) \[(1-a^{-1})^{k+1}r_{0}+b_{j =0}^{+}(1-a^{-1})^{j}\] (289) \[=(1-b^{1-}a^{})^{k+1}+ ^{-1}}\] (290) \[=(1-b^{1-}a^{})^{k+1}+ ()^{}.\] (291)

## Appendix H Experimental Details and Additional Results

### Experimental Details

#### h.1.1 Employed Policies and Hyperpolicies

Linear Gaussian Policy.A _linear parametric gaussian_ policy \(_{}:()\) with variance \(^{2}\) samples the actions as \(a_{t}(^{}_{t},^{2}I_{d_{S}})\), where \(_{t}\) is the observed state at time \(t\) and \(\) is the parameter vector.

Tabular Softmax Policy.A _tabular softmax_ policy \(_{}:()\) with a temperature constant \(\) is such that:

\[_{}(_{j}|_{i})=_{i,j} }{})}{_{z=1}^{||}(_{i,z}}{ })},\] (292)

where \(_{i,j}\) is the parameter associated with the \(i\)-th state and the \(j\)-th action. Notice that the total number of parameters for this kind of policy is \(||||\).

Linear Deterministic Policy.A _linear parametric deterministic_ policy \(_{}:\) samples the actions as \(_{t}=^{}_{t}\), where \(_{t}\) is the observed state at time \(t\) and \(\) is the parameter vector.

Gaussian Hyperpolicy.A _parametric gaussian_ hyperpolicy \(_{}:()\) with variance \(^{2}\) samples the parameters \(\) for the underlying generic parametric policy \(_{}\) as \(_{t}(,^{2}I_{d_{}})\), where \(\) is the parameter vector for the hyperpolicy.

#### h.1.2 Environments

Discrete Grid World with Walls.Discrete Grid World with Walls (DGWW) is a simple discrete environment we employed to compare C-PGAE against the sample-based versions of NPG-PD (Ding et al., 2020, Appendix H) and RPG-PD (Ding et al., 2024, Appendix C.9). DGWW is a grid-like bidimensional environment in which an agent can assume only integer coordinate positions and in which an agent can play four actions stating whether to go up, right, left, or down. The goal is to reach the center of the grid performing the minimum amount of steps, begin the initial state uniformly sampled among the four vertices of the grid. The agent is rewarded negatively and proportionally to its distance from the center, where the reward is \(0\). Around the goal state there is a "U-shaped" obstacle with an opening on the top side. In particular, when the agent lands in a state in which the wall is present, it receives a cost of \(1\), otherwise the cost signal is always equal to \(0\). In our experiments, we employed a DGWW environment of such a kind, with \(||=49\), i.e., with each dimension with length equal to \(7\).

Linear Quadratic Regulator with Costs.The Linear Quadratic Regulator (LQR, Anderson and Moore, 2007) is a continuous environment we employed in the regularization sensitivity study of C-PGAE and C-PGPE, and in the comparison among the same algorithms against the sample-based version of NPG-PD2 (Ding et al., 2022, Algorithm 1) and its ridge-regularized version RPG-PD2 (not provided by the authors, but designed by us). LQR is a dynamical system governed by the following state evolution:

\[_{t+1}=A_{t}+B_{t},\] (293)

where \(A^{d_{} d_{}}\) and \(B^{d_{} d_{}}\).

In the standard version of the environment, the reward is computed at each step as:

\[r_{t}=-_{t}^{}R_{t}-_{t}^{}Q_{t},\] (294)

where \(R^{d_{} d_{}}\) and \(Q^{d_{} d_{}}\).

We modified this version of the LQR environment introducing costs. In particular, in our _CostLQR_, the state evolution is treated as in the original case, while the reward at step \(t\) is computed as:

\[r_{t}=-_{t}^{}R_{t},\] (295)

where \(R^{d_{} d_{}}\). Moreover, we added a cost signal \(c\) which is computed as follows at every time step \(t\):

\[c_{t}=_{t}^{}Q_{t},\] (296)

where \(Q^{d_{} d_{}}\).

In our experiments, we consider a _CostLQR_ environment whose main characteristics are reported in Table 5.

Additionally, we considered a uniform initial state distribution in \([-3,3]\) and the following matrices:

\[A=B=0.91&0\\ 0&1, Q=0.9&0\\ 0&0.1, R=0.1&0\\ 0&0.9.\] (297)MuJoCo with Costs.For our experiments on risk minimization, we utilized environments from the MuJoCo control suite (Todorov et al., 2012), which offers a variety of continuous control environments. To tailor these environments to our specific requirements, we introduced a cost function that represents the energy associated with the control actions. In standard MuJoCo environments, a portion of the reward is typically calculated as the cost of the control action, which is proportional to the deviation of the chosen action from predefined action bounds. In our MuJoCo modification, at each time step we make the environment return a cost computed as:

\[\|_{t}-\{\{_{t},a_{}\},a_{} \}\|_{2},\] (298)

where \(a_{}\) and \(a_{}\) are respectively the bounds for the minimum and maximum value for each component of the action vector. Then, the action \(\{\{_{t},a_{}\},a_{}\}\) is passed to the environment. In our experiment we consider _Swimmer-v4_ and _Hopper-v4_ MuJoCo environments, whose main features are summarized in Table 5.

### Details for the comparison against the baselines in DGWW

In Section 5, we compare our proposal C-PGAE against the sample-based versions of NPG-PD (Ding et al., 2020, Appendix H) and RPG-PD (Ding et al., 2024, Appendix C9). The environment in which the methods are tested is the Discrete Grid World with Walls (DGWW, see Appendix H) with a horizon of \(T=100\). In this experiment, the methods aim at learning the parameters of a tabular softmax policy with \(196\) parameters, maximizing the trajectory reward while considering a single constraint on the average trajectory cost, for which we set a threshold \(b=0.2\). All the methods were run for \(K=3000\) iterations with a batch size of \(N=10\) trajectories per iteration, and with constant learning rates. In particular, for both C-PGAE and NPG-PD, we employed \(_{}=0.01\) and \(_{}=0.1\), while for RPG-PD we selected \(_{}=0.01\) and \(_{}=0.01\). For C-PGAE and RPG-PD we used a regularization constant \(=10^{-4}\). All the details about the experimental setting are summarized in Table 6. We would like to stress that, as prescribed by the respective convergence theorems, we chose a two time-scales learning rate approach for C-PGAE and a single time-scale one for RPG-PD. Figure 1(a) shows the performance curves (i.e., the one associated with the objective function and the cost ones. As can be noticed, C-PGAE manages to strike the objective of the constrained optimization problem with less trajectories. Indeed, the sample-based NPG-PD requires to estimate the value and the action-value functions for all the states and state-action pairs, resulting in analyzing \(||+||||\) additional trajectories w.r.t. C-PGAE for every iteration of the algorithm. The sample-based RPG-PD also requires additional trajectories to be analyzed, which in practice, for a correct learning behavior, result to be the same in number to the extra ones analyzed by NPG-PD.

### Details for the comparison against baselines in CostLQR

In Section 5, we compare our proposals C-PGAE and C-PGPE against the continuous sample-based version of NPG-PD (Ding et al., 2022, Algorithm 1) with works with generic policy parameterizations. In the following, we refer to this version of NPG-PD as NPG-PD2. Moreover, we added a ridge-regularized version of NPG-PD2, that we call RPG-PD2, to resemble the type of regularization we employed for our proposed methods. For all the regularized methods (i.e., C-PGAE, C-PGPE, and RPG-PD2) we selected as regularization constant \(=10^{-4}\). The setting for this experiment considers a bidimensional LQR environment with a single cost over the provided actions (see Appendix H) and with a fixed horizon \(T=50\). Here, the methods aim at maximizing the average reward over trajectories, while keeping the average cost over trajectories under the threshold \(b=0.9\). In particular, C-PGAE learns the parameters of a linear gaussian policy with a variance \(_{}^{2}=10^{-3}\) and employing a learning rate schedule governed by the Adam scheduler (Kingma and Ba, 2015) with \(_{,0}=0.001\) and \(_{,0}=0.01\). C-PGPE learns the parameters of a gaussian hyperpolicy, with a variance \(_{}^{2}=10^{-3}\), which samples the parameters of a deterministic linear policy. It employs a learning rate schedule

 
**Environment** & **State Dimension \(d_{}\)** & **Action Dimension \(d_{}\)** & **Action Range \([a_{},a_{}]\)** & **State Range \([s_{},s_{}]\)** \\   CostLQR & \(2\) & \(2\) & \((-,+)\) & \((-,+)\) \\  Swimmer-v4 & \(8\) & \(2\) & \([-1,1]\) & \((-,+)\) \\  Hopper-v4 & \(11\) & \(3\) & \([-1,1]\) & \((-,+)\) \\  

Table 5: Main features of _CostLQR_, _Swimmer-v4_, and _Hopper-v4_.

governed by Adam too with \(_{,0}=0.001\) and \(_{,0}=0.01\). Both C-PGAE and C-PGPE were run for \(K=6000\) iterations with a batch of \(N=100\) trajectories per iteration. NPG-PD2 and RPG-PD2 are both actor-critic methods which were run for \(K=1000\) iterations with a batch size of \(N=600\) trajectories per iteration. In particular, among the trajectories of the reported batch size, \(N_{1}=500\) were used for the inner critic-loop, while \(N_{2}=100\) for performance and cost estimations. The inner loop step size was selected constant, as prescribed by the original algorithm, and with a value \(=10^{-5}\). Furthermore, since such methods were designed for infinite-horizon discounted environments, we tested them on the same LQR as for C-PGAE and C-PGPE, but leaving \(T=+\) and \(=0.98\) (the effective horizon is \((1-)^{-1}=50\)). The step sizes for the primal and dual variables updates were governed by Adam with \(_{,0}=0.003\) and \(_{,0}=0.01\). As for C-PGAE, both NPG-PD2 and RPG-PD2 aimed at learning the parameters of a linear gaussian policy, with variance \(_{}^{2}=10^{-3}\). All the details about this experiment are summarized in Table 7. Figure 2b reports the learning curves for the average return and the cost over trajectories. As can be seen, our methods manage to solve the constrained optimization problem at hand by leveraging on less trajectories. Indeed, NPG-PD2 and RPG-PD2 suffer the inner critic loop, which add additional trajectories to be analyzed per iteration (in this specific case \(N_{1}=500\)). We would like to stress that the actor-critic methods were very sensible to the hyperparameters selection, especially the length and the step size of the inner loop.

### Details for the risk-constrained experiment on Swimmer-v4

In Section 5, we presented an experiment comparing what happens when considering the two exploration approaches of C-PGAE and C-PGPE and on different risk-constrained optimization problems on the cost version of _Swimmer-v4_ (details in Appendix H). In particular, we considered such an environment with a horizon of \(T=100\). Both the algorithms were run for \(K=3000\) iterations, with a batch size of \(N=100\) trajectories per iteration, and with step sizes governed by the Adam scheduler. Moreover, they had a regularization constant \(=10^{-4}\).

C-PGAE employed a _linear gaussian policy_ with variance \(_{}^{2}=1\). In Table 8, we list the characteristics for all the experiments with all the risk measures.

On the other hand, C-PGPE employed a _gaussian hyperpolicy_ with variance \(_{}^{2}=0.01\). Such an hyperpolicy sampled parameters for an underlying _linear deterministic policy_. In Table 9, we list the characteristics for all the experiments with all the risk measures.

As also highlighted in the main paper, C-PGPE delivers an hyperpolicy which samples (after having sampled the parameters for the underlying policy) actions whose costs are always under the fixed threshold. Moreover, by considering risk measures different from the _average cost_ one, the empirical distribution of costs shows that these are way lighter w.r.t. the ones associated with the deployment of

   \\   Environment & DGWW \\ Horizon & \(T=100\) \\  Policy & Tabular Softmax \\  Constraint Threshold & \(b=0.2\) \\  Iterations & \(K=3000\) \\ Batch Size & \(N=10\) \\  Learning Rates C-PGAE & \(_{}=0.01\) and \(_{}=0.1\) \\ Learning Rates NPG-PD & \(_{}=0.01\) and \(_{}=0.1\) \\ Learning Rates RPG-PD & \(_{}=0.01\) and \(_{}=0.01\) \\  Regularization C-PGAE & \(=10^{-4}\) \\ Regularization RPG-PD & \(=10^{-4}\) \\  

Table 6: Details for the comparison of C-PGAE against NPG-PD and RPG-PD in DGWW.

an hyperpolicy learned considering average cost constraints. C-PGAE also shows results of this kind. However, the displacement between the final empirical cost distributions is not as marked as the one shown with C-PGFE. The exception is from the MV risk measure side, which seems to make C-PGAE learn a policy able to pay lighter costs, but resulting in poor-performing policy. All the other found (hyper)policies, instead, provide similar performance scores.

   \\   Environment & Bidimensional CostLQR \\ Horizon & \(T=50\) \\  (Hyper)Policy & Linear Gaussian with \(^{2}=10^{-3}\) \\  Constraint Threshold & \(b=0.9\) \\  Iterations (C-PGFE and C-PGAE) & \(K=6000\) \\ Iterations (NPG-PD2 and RPG-PD2) & \(K=1000\) \\ Batch Size (C-PGFE and C-PGAE) & \(N=100\) \\ Batch Size (NPG-PD2 and RPG-PD2) & \(N=600\) \\  Learning Rate (Adam) C-PGFE & \(_{,0}=10^{-3}\) and \(_{}=10^{-2}\) \\ Learning Rates (Adam) C-PGAE & \(_{,0}=10^{-3}\) and \(_{}=10^{-2}\) \\ Learning Rates (Adam) NPG-PD2 & \(_{}=3 10^{-3}\) and \(_{}=10^{-2}\) \\ Learning Rates (Adam) RPG-PD2 & \(_{}=3 10^{-3}\) and \(_{}=10^{-2}\) \\  Regularization C-PGFE & \(=10^{-4}\) \\ Regularization C-PGAE & \(=10^{-4}\) \\ Regularization RPG-PD2 & \(=10^{-4}\) \\  

Table 7: Details for the comparison of C-PGFE and C-PGAE against NPG-PD2 and RPG-PD2 in a bodomensical CostLQR.

   Risk Measure & Risk Parameter & \(b\) & \(_{,0}\) & \(_{,0}\) & \(_{,0}\) \\   Average Cost & \(\) & \(50\) & \(10^{-3}\) & \(10^{-2}\) & \(\) \\ CVaR\({}_{}\) & \(=0.95\) & \(50\) & \(10^{-4}\) & \(10^{-1}\) & \(10^{-1}\) \\ MV & \(=0.5\) & \(50\) & \(10^{-4}\) & \(10^{-1}\) & \(10^{-1}\) \\ Chance & \(n=50\) & \(0.05\) & \(10^{-4}\) & \(10^{-1}\) & \(10^{-1}\) \\  

Table 8: Parameters of the risk measures employed in the experiment on _Swimmer-v4_ with C-PGAE.

   Risk Measure & Risk Parameter & \(b\) & \(_{,0}\) & \(_{,0}\) & \(_{,0}\) \\   Average Cost & \(\) & \(50\) & \(10^{-3}\) & \(10^{-2}\) & \(\) \\ CVaR\({}_{}\) & \(=0.95\) & \(50\) & \(10^{-3}\) & \(10^{-2}\) & \(10^{-3}\) \\ MV & \(=0.5\) & \(50\) & \(10^{-3}\) & \(10^{-2}\) & \(10^{-3}\) \\ Chance & \(n=50\) & \(0.05\) & \(10^{-3}\) & \(10^{-2}\) & \(10^{-3}\) \\  

Table 9: Parameters of the risk measures employed in the experiment on _Swimmer-v4_ with C-PGFE.

### Risk-constrained experiment on Hopper-v4

Here, we present a similar experiment to the one shown on _Swimmer-v4_. Also in this case, we compare what happens when considering the two exploration approaches of C-PGAE and C-PGPE and on different risk-constrained optimization problems, but on the cost version of _Hopper-v4_ (details in Appendix H). The experimental setting is quite the same of the one considered above (i.e., \(T=100\), \(K=3000\), \(N=100\), \(=10^{-4}\)).

C-PGAE employed a _linear gaussian policy_ with variance \(_{}^{2}=1\). The characteristics for all the risk measures' experiments are presented in Table 10. For the learning rates schedules, we employed the Adam scheduler.

C-PGPE employed a _gaussian hyperpolicy_ with variance \(_{}^{2}=0.1\) over an underlying _linear deterministic policy_. The characteristics for all the experiments with all the risk measures are summarized in Table 11.

For the learning rates schedules, we employed the Adam scheduler.

Figure 5 shows the empirical distributions of costs over \(100\) trajectories of the learned (hyper)policies via C-PGPE and C-PGAE. C-PGPE shows a behavior that is quite the same as the one shown in the _Swimmer-v4_ environment. Indeed, by considering constraints on the CVaR\({}_{}\), the MV, or on the Chance, the learned hyperpolicy selects parameters for the underlying policy selecting actions that pay a way lighter cost w.r.t. the ones provided by an hyperpolicy learned by considering constraints on average costs. It is worth noticing that the lightest costs are observed when considering the CVaR\({}_{}\)or the MV risk measures. In this case, C-PGAE shows a similar behavior to C-PGPE for what concern the distance in costs between the ones observed by learning considering constraints on the average cost and the ones observed by learning with other risk measures. The lightest costs here can be observed by learning with chance constraints, while, by considering constraints on the MV, the observed costs exceed the ones observed under average cost constraints. For what concern the performances of the learned (hyper)policies (see Table 4c, C-PGPE exhibit high-performing hyperpolicies under average cost or chance constraints, while the ones under CVaR\({}_{}\)and MV shows similar (low) performance scores. On the other hand, for C-PGAE the learned policy under CVaR\({}_{}\)exhibits the same good-performing behavior as the one of the policy learned under average cost constraints.

### Regularization Sensitivity Study

Here, we study the sensitivity of C-PGAE and C-PGPE w.r.t. the regularization term \(\). We tried the algorithms on a bidimensional LQR environment which has been modified to output a cost signal based on the selected action. For the environment at hand, we considered a horizon \(T=50\). We run

  Risk Measure & Risk Parameter & \(b\) & \(_{,0}\) & \(_{,0}\) & \(_{,0}\) \\   Average Cost & \(\) & \(100\) & \(10^{-2}\) & \(10^{-1}\) & \(\) \\ CVaR\({}_{}\) & \(=0.95\) & \(100\) & \(10^{-3}\) & \(10^{-1}\) & \(10^{-1}\) \\ MV & \(=0.5\) & \(100\) & \(10^{-4}\) & \(10^{-1}\) & \(10^{-1}\) \\ Chance & \(n=100\) & \(0.05\) & \(10^{-4}\) & \(10^{-1}\) & \(10^{-1}\) \\  

Table 10: Parameters of the risk measures employed in the experiment on _Hopper-v4_ with C-PGAE.

  Risk Measure & Risk Parameter & \(b\) & \(_{,0}\) & \(_{,0}\) & \(_{,0}\) \\   Average Cost & \(\) & \(100\) & \(10^{-2}\) & \(10^{-1}\) & \(\) \\ CVaR\({}_{}\) & \(=0.95\) & \(100\) & \(10^{-2}\) & \(10^{-1}\) & \(10^{-2}\) \\ MV & \(=0.5\) & \(100\) & \(10^{-2}\) & \(10^{-1}\) & \(10^{-2}\) \\ Chance & \(n=100\) & \(0.05\) & \(10^{-2}\) & \(10^{-1}\) & \(10^{-2}\) \\  

Table 11: Parameters of the risk measures employed in the experiment on _Hopper-v4_ with C-PGPE.

both the algorithms for \(K=10^{4}\) iterations, with a batch size \(N=100\) trajectories per iteration, and with a varying regularization term such that \(\{0,10^{-4},10^{-2}\}\). We considered a single constraint on the average trajectory cost, for which we set a threshold \(b=0.2\). For the step size schedules, we employed Adam (Kingma and Ba, 2015) with initial rates \(_{,0}=_{,0}=10^{-3}\) and \(_{,0}=10^{-2}\). Moreover, in this specific experiment C-PGAE employed a linear gaussian policy with a variance \(_{}^{2}=10^{-3}\). On the other hand, C-PGPE employed a linear gaussian hyperpolicy with a variance \(_{}^{2}=10^{-3}\) over a linear deterministic policy. The experimental environment is summarized in Table 12. Figures 7 and 6 show the Lagrangian curves, the performance ones (i.e., the one associated with the objective function), and the cost-related ones. From the shown curves it is possible to notice that, for both C-PGAE and C-PGPE, a higher regularization (\(=10^{-2}\)) corresponds to a higher bias w.r.t. the constraint satisfaction. This bias is compliant with what shown by Theorem 3.1, indeed, the higher the regularization, the higher the constraint threshold should be made stricter. Finally, we report in Figure 8 the evolution of the values of the Lagrangian multipliers \(\) during the learning. As expected from the theory, for both C-PGAE and C-PGPE a higher regularization leads to have smaller values of \(\). Moreover, we empirically notice that C-PGAE reaches higher value of \(\) w.r.t. the ones seen by C-PGPE.

   \\   Environment & Bidimensional CostLQR \\ Horizon & \(T=50\) \\  (Hyper)Policy & Linear Gaussian with \(^{2}=10^{-3}\) \\  Constraint Threshold & \(b=0.2\) \\  Iterations & \(K=10^{4}\) \\ Batch Size & \(N=100\) \\  Learning Rate (Adam) C-PGPE & \(_{,0}=10^{-3}\) and \(_{}=10^{-2}\) \\ Learning Rates (Adam) C-PGAE & \(_{,0}=10^{-3}\) and \(_{}=10^{-2}\) \\  Regularization & \(\{0,10^{-4},10^{-2}\}10^{-4}\) \\  

Table 12: Details for the reguarization sensitivity study of C-PGPE and C-PGAE in a bodomensional CostLQR.

Figure 5: Cost distributions with (hyper)policies learned considering different risk measures (5 runs).

Figure 6: Lagrangian, performance and cost curves for C-PGPE over _CostLQR_ with regularization values \(\{0,10^{-4},10^{-2}\}\) (5 runs, mean \(\)\(95\%\) C.I.).

Figure 7: Lagrangian, performance and cost curves for C-PGAE over _CostLQR_ with regularization values \(\{0,10^{-4},10^{-2}\}\) (5 runs, mean \(\)\(95\%\) C.I.).

### Computational Resources

All the experiments were run on a 2019 16-inches MacBook Pro. The machine was equipped as follows:

CPU \[&\\ &&\]

In particular, \(N=100\) trajectories of the MuJoCo environments with \(T=100\) scored \( 2\) iterations per second for C-PGAE, while \( 3\) iterations per second for C-PGPE. \(N=100\) trajectories of the _CostLQR_ environment with \(T=100\) scored \( 5\) and \( 8\) iterations per second respectively for C-PGAE and C-PGPE. All the performances are to be considered with a parallelization over \(10\) CPU cores.

### NeurIPS Paper Checklist

1. **Claims** Question: Do the main claims made in the abstract and introduction accurately reflect the paper's contributions and scope? Answer: [Yes] Justification: The abstract and the introduction reflect the original contribution of the paper. Guidelines: * The answer NA means that the abstract and introduction do not include the claims made in the paper. * The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers. * The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings. * It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper.
2. **Limitations** Question: Does the paper discuss the limitations of the work performed by the authors? Answer: [Yes] Justification: The limitations of the work emerges in the final section of the work and in the related works section. Guidelines: * The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper. * The authors are encouraged to create a separate "Limitations" section in their paper. * The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be. * The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated. * The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon. * The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size. * If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness. * While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren't acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations.
3. **Theory Assumptions and Proofs** Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? Answer: [Yes]Justification: All the statements are correlated with proofs. The definitions and the quantity used are clearly presented. Guidelines: * The answer NA means that the paper does not include theoretical results. * All the theorems, formulas, and proofs in the paper should be numbered and cross-referenced. * All assumptions should be clearly stated or referenced in the statement of any theorems. * The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition. * Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material. * Theorems and Lemmas that the proof relies upon should be properly referenced.
4. **Experimental Result Reproducibility** Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? Answer: [Yes] Justification: All the experimental settings are described, together with choices of all the parameters for the settings at hand. Guidelines: * The answer NA means that the paper does not include experiments. * If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not. * If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable. * Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed. * While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example 1. If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. 2. If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. 3. If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). 4. We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results.
5. **Open access to data and code**Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? Answer: [Yes] Justification: The implemented code is provided in a zip file which contains detailed instructions on how to use it. Guidelines: * The answer NA means that paper does not include experiments requiring code. * Please see the NeurIPS code and data submission guidelines (https://nips.cc/public/guides/CodeSubmissionPolicy) for more details. * While we encourage the release of code and data, we understand that this might not be possible, so "No" is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark). * The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https://nips.cc/public/guides/CodeSubmissionPolicy) for more details. * The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc. * The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why. * At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable). * Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted.
6. **Experimental Setting/Details** Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? Answer: [Yes] Justification: All the experimental settings are described, together with choices of all the parameters for the settings at hand. Guidelines: * The answer NA means that the paper does not include experiments. * The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. * The full details can be provided either with the code, in appendix, or as supplemental material.
7. **Experiment Statistical Significance** Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? Answer: [Yes] Justification: The plots of all the experiments are made over an average of different trials and we report, for each shown curve, the \(95\%\) confidence interval. Guidelines: * The answer NA means that the paper does not include experiments. * The authors should answer "Yes" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.

* The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).
* The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)
* The assumptions made should be given (e.g., Normally distributed errors).
* It should be clear whether the error bar is the standard deviation or the standard error of the mean.
* It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a 96% CI, if the hypothesis of Normality of errors is not verified.
* For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).
* If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text.
8. **Experiments Compute Resources** Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? Answer: [Yes] Justification: The computational resources employed for th experiments are discussed. Guidelines: * The answer NA means that the paper does not include experiments. * The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage. * The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute. * The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn't make it into the paper).
9. **Code Of Ethics** Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? Answer: [Yes] Justification: We checked the guidelines and we are compliant with them. Guidelines: * The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics. * If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics. * The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction).
10. **Broader Impacts** Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? Answer: [NA] Justification: As a theoretical research paper, the authors do not see any negative societal impact. Guidelines: * The answer NA means that there is no societal impact of the work performed.

* If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.
* Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.
* The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.
* The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.
* If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML).
11. **Safeguards** Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? Answer: [NA] Justification: The paper poses no such risks. Guidelines: * The answer NA means that the paper poses no such risks. * Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters. * Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images. * We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort.
12. **Licenses for existing assets** Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? Answer: [Yes] Justification: The only existing asset employed is the MuJoCo (Todorov et al., 2012) control suite, that is properly mentioned. Guidelines: * The answer NA means that the paper does not use existing assets. * The authors should cite the original paper that produced the code package or dataset. * The authors should state which version of the asset is used and, if possible, include a URL. * The name of the license (e.g., CC-BY 4.0) should be included for each asset. * For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.

* If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.
* For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.
* If this information is not available online, the authors are encouraged to reach out to the asset's creators.
* **New Assets*
* Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? Answer: [Yes] Justification: In order to run the experiments, we implemented a detailed code-base. This asset is provided in an anonymized zip file and, conditionally on the paper acceptance, it will be made public. The zip file includes a file explaining how to employ the code-base. Guidelines:
* The answer NA means that the paper does not release new assets.
* Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.
* The paper should discuss whether and how consent was obtained from people whose asset is used.
* At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file.
* **Crowdsourcing and Research with Human Subjects*
* Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? Answer: [NA] Justification: The paper does not involve crowdsourcing nor research with human subjects. Guidelines:
* The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.
* Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.
* According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector.
* **Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects*
* Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? Answer: [NA] Justification: The paper does not involve crowdsourcing nor research with human subjects. Guidelines:
* The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.
* Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.
* We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.
* For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review.