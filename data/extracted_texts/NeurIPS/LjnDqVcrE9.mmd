# ControlMLLM: Training-Free Visual Prompt Learning for Multimodal Large Language Models

Mingrui Wu\({}^{1}\), Xinyue Cai\({}^{1}\), Jiayi Ji\({}^{1}\),Jiale Li\({}^{1}\), Oucheng Huang\({}^{1}\),

**Gen Luo\({}^{1}\), Hao Fei\({}^{2}\), Guannan Jiang\({}^{3}\), Xiaoshuai Sun\({}^{1}\), Rongrong Ji\({}^{1}\) 1 Key Laboratory of Multimedia Trusted Perception and Efficient Computing,**

Ministry of Education of China, Xiamen University, 361005, P.R. China

2 National University of Singapore 3 CATL

mingrui0001@gmail.com

Corresponding Author

###### Abstract

In this work, we propose a training-free method to inject visual prompts into Multimodal Large Language Models (MLLMs) through learnable latent variable optimization. We observe that attention, as the core module of MLLMs, connects text prompt tokens and visual tokens, ultimately determining the final results. Our approach involves adjusting visual tokens from the MLP output during inference, controlling the attention response to ensure text prompt tokens attend to visual tokens in referring regions. We optimize a learnable latent variable based on an energy function, enhancing the strength of referring regions in the attention map. This enables detailed region description and reasoning without the need for substantial training costs or model retraining. Our method offers a promising direction for integrating referring abilities into MLLMs, and supports referring with box, mask, scribble and point. The results demonstrate that our method exhibits **out-of-domain generalization** and **interpretability**. Code: https://github.com/mrwu-mac/ControlMLLM.

## 1 Introduction

In recent times, there has been a surge in the development and adoption of large language models (LLMs), such as GPT-4  and Llama , showcasing remarkable capabilities in addressing a wide range of human-generated questions. The success of these models has sparked interest among researchers in exploring the integration of LLMs with visual inputs. Consequently, a new class of models known as Multimodal Large Language Models (MLLMs) has emerged . However, despite their widespread adoption, traditional MLLMs often face limitations due to their reliance on coarse image-level alignments. This restricts users to guiding MLLMs solely through text prompts for detailed region description and reasoning. However, text often fails to capture the intricate visual nuances present in an image.

Addressing this challenge, recent efforts  have pioneered the integration of referring abilities within MLLMs, which enables users to provide input by pointing to specific coordinates of the objects or regions, as shown in Figure 1 (left). However, these endeavors typically entail substantial training costs to equip MLLMs with referring capabilities. Additionally, the model must undergo retraining to adapt to new data domains or new base MLLMs.

In this work, we propose a training-free method to inject the visual prompts into the Multimodal Large Language Models via learnable latent variable optimization. The method originates from our observation of the attention maps from the MLLM decoder, which model the relationship betweenthe pixels and text prompt tokens and encompass rich semantic relations that significantly influence the generated text. However, MLLMs typically involve fine-tuning an MLP layer to bridge the gap between visual and linguistic representations, which means that the output of the MLP layer can indirectly impact the relationship between text prompt tokens and pixels in the attention layers of the MLLM decoder, thereby altering the model's output.

Thus, our key idea is that we can alter the outputs of MLLMs by adjusting the visual tokens from the MLP output during the inference process, controlling which text prompt tokens attend to which visual tokens in the attention layers. Specifically, we augment visual tokens with an additional learnable latent variable. Subsequently, we optimize the learnable latent variable based on an energy function designed to enhance the strength of the referring regions in the attention map between the text tokens and the visual tokens.

Our method enables referring MLLMs with various visual prompts, including box, mask, scribble and point, and does not require model training, fine-tuning, or extra data. We also demonstrate that our method exhibits out-of-domain generalization and interpretability.

## 2 Related Work

MllmsMotivated by the accomplishments of Large Language Models (LLMs) [1; 53], there is a burgeoning trend among researchers to develop a diverse range of Multimodal Large Language Models (MLLMs) [33; 36; 17; 67; 32; 38; 39; 18; 22; 78; 15; 58; 35; 20; 21; 19]. These MLLMs typically comprise a visual encoder, a language decoder, and an image-text alignment module. The visual encoder and the language decoder are often sourced from pre-trained models, such as CLIP , DINOv2 , Llama , and Vicuna . Meanwhile, the image-text alignment module is trained on image-text pairs and fine-tuned through visual instruction tuning to enhance its visual conversation capabilities. These Multimodal Large Language Models (MLLMs) often confront limitations stemming from their reliance on coarse image-level alignments.

Referring MLLMsIn recent research, there has been a noticeable trend towards integrating foundation models with tasks involving referring dialogue. These models [69; 74; 75; 12; 37; 43; 68; 64; 71; 5; 73; 40; 26; 34; 70; 79; 11; 51; 46; 80; 63; 8; 24; 45; 52; 72] introduce spatial visual prompts as extra input and are trained using region-text pairs. By leveraging this approach, they effectively bridge the gap between textual prompts and visual context, enabling comprehensive understanding of

Figure 1: Comparison between the training method and our training-free method. The training method typically requires a large amount of in-domain data for training and cannot generalize to out-of-domain prompts. In contrast, our method can easily adapt to prompts from a new domain in a training-free manner.

image content at the regional level. However, these methods inevitably require a substantial training burden.

Training-free Control in Text-to-ImageThere are numerous works on controllable text-to-image generation, among which training-free methods [27; 14; 62; 30] are most relevant to our research. Among them, Prompt-To-Prompt  explore the role of attention in text-visual interactions in Stable Diffusion  model, while Layout-Guidance  indirectly bias attention in Stable Diffusion model by optimizing an energy function. These contributions significantly inform our investigation into enhancing controllability and interpretability in MLLMs.

Visual PromptThe visual prompt can be categorized into two main techniques: hard prompt and soft prompt. The hard visual prompt works [48; 57; 66; 65] that direct the model's attention to the region or enable visual grounding abilities in the Multimodal Models in a training-free and convenient manner by directly manipulating images, such as color guidance [60; 23]. However, these methods inevitably compromise the structural information of the images, or a strong understanding of the corresponding patterns by the model is required. In contrast, the soft visual prompt works [28; 4; 77] integrate learnable visual prompts into models to adapt them for different downstream tasks. However, these methods do not support region guidance and require fine-tuning the model with downstream data. In contrast, we optimize a learnable latent variable to support referring MLLM in the test time, without any downstream training data required, and TPT  is most related to our work.

## 3 Background

Multimodal Large Language Models (MLLMs):The MLLMs typically consist of a visual encoder, an LLM decoder, and an image-text alignment module. Given an image \(I\), the frozen vision encoder and a subsequent learnable MLP are used to encode \(I\) into a set of visual tokens \(e_{v}\). These visual tokens \(e_{v}\) are then concatenated with text tokens \(e_{t}\) encoded from text prompt \(p_{t}\), forming the input for the frozen LLM. The LLM decodes the output tokens \(y\) sequentially, which can be formulated as:

\[y_{i}=f(I,p_{t},y_{0},y_{1},,y_{i-1}).\] (1)

Considering LLaVA-liked  MLLMs, the LLM in MLLMs typically employs a transformer model  with the attention layer as its core. In such model, the attention maps represent the relationships between the visual tokens and the text prompt tokens. The attention map in attention layer \(\), computed on the transformed visual-text concatenated embeddings \([e_{v},e_{t}]^{()}\), is obtained as follows:

\[A^{()}=(,e_{t}]^{()}([e_{v},e_{t}]^{( )})^{T}}{}}),\] (2)

where \(d_{k}\) is a scaling factor. \(A^{()}\) consists of \(A^{()}_{ij}\) with \(i,j\{1,,n\}\), representing the relationship between the \(i\)-th token and the \(j\)-th token, and their impact on the output.

Training Referring MLLMs:The objective of training referring MLLMs is to inject the visual prompt \(r\) into the MLLMs to achieve referring ability via model parameter learning. The visual prompt \(r\) can take various forms, such as a box, mask, scribble, or point, to indicate specific locations or regions within the image.

Current referring MLLMs typically need to be fine-tuned on a training set with positional annotations before they can be effectively used. The fine-tuning process involves maximizing the log likelihood of generating the text conditioned on \(I\), \(p_{t}\), and \(r\) over the entire training dataset. This can be formulated as:

\[^{*}=_{}_{i=1}^{U} P(y_{i} I_{i},p_{t},r,y_{0},y_{1},,y_{i-1};),\] (3)

where \(\) represents the parameters of the model \(f\), and \(U\) is the number of samples in the training set. This method significantly enhances the model's fine-grained understanding and interactivity. However, it incurs high training costs. Additionally, such fine-tuning strategies result in domain-specific behaviors, which have been shown to compromise the out-of-distribution generalization and robustness of MLLMs . Therefore, when domain shifts occur, the model needs to be retrained, leading to a lack of flexibility.

## 4 Method

We aim to propose a training-free method to overcome the inconveniences of traditional training. Training-free referring MLLM maintains the model parameters \(\) frozen, eliminating the need for any training or fine-tuning with samples from the training set. During inference, the only information available is the single test sample without label information, as shown in Figure 1 (right).

In this section, we explore and design a solution to address the challenges of Training-free Referring MLLMs. The key task is to flexibly embed visual prompts during the inference phase while maintaining the model's reasoning capabilities. To begin with, we delve into the mechanism of MLLMs (see Sec. 4.1), our key observation is the attention mechanism in LLM capturing the relationship between the model's output and the input pixels. Further, the visual tokens inputted into the LLM influence the values of the attention maps to indirectly control the model output. Building on this analysis, we propose the Latent Variable learning (a test-time prompt tuning strategy , see Sec. 4.2) to edit the visual tokens, as shown in Figure 4. This method effectively integrates visual prompts into pre-trained MLLMs, enabling fine-grained visual reasoning.

### Analysis of the Attention in LVLMs

We begin by analyzing _which factors in the model truly capture the relationship between input and output?_ In other words, we seek to understand _how to interpret the association between the model's output and the input pixels_.

As demonstrated by Equation 1, Multimodal Large Language Models (MLLMs) fundamentally model the maximum likelihood output based on visual input and text prompts. By conditioning on the text prompt, the model can determine which parts of the image have the greatest impact on the output. Building on the discussions in the Sec. 3 and illustrations in the Figure 2 (top line), we can observe that the attention map models the influence of visual tokens on the output conditioned by the text prompt. Therefore, the attention map in MLLMs not only provides interpretability regarding the relationship between model output and input pixels but also facilitates guiding the model's output.

A natural idea is that we can directly alter the model's output by editing the attention maps. Inspired by IBD , we achieve this by adding an adjustment coefficient \(\) to the attention related to the visual tokens corresponding to the referring region, which can be formulated as,

\[& A^{()}=(,e_{t}]^{( )}([e_{v},e_{t}]^{()})^{T}}{}}+M),\\ & M_{i}=ir0, \] (4)

Figure 2: The attention maps in various layers of the MLLMs, with the numbers indicating the respective layer indices. The top line visualizes the attention between the prompt token “hat” and the visual tokens, while the bottom line visualizes the attention between the context token (mentioned in Sec. 4.2) and the visual tokens.

where \(M\) is a mask with the same shape as the attention map, \(r\) denotes the referring region. However, we have to carefully select a suitable coefficient \(\) for each example. When the \(\) is too small, it leads to ineffective control (as shown in Figure 3 a), and when it is too large, it can impact the language capabilities of the LLM (as shown in Figure 3 c). Additionally, we found that it is sufficient to manipulate the attention map at the 0-th step during model inference (as shown in Figure 3 a,b,c), as it is most directly associated with the text prompt, and manipulating attentions step by step also affects the expression of the LLM (as shown in Figure 3 d). Overall, directly manipulating attention maps is not a viable approach because it overlooks the relationships between attention layers and not all layers' visual tokens decide the output .

We note that in the most MLLMs, typically the MLP layer is trained for image-text alignment. This implies that MLLMs indirectly affect the values of the attention map by learning the parameters of the MLP layer to alter the visual tokens. In other words, the visual tokens inputted into the LLM directly influence the values of the attention maps.

It is also worth noting that the input text prompt also directly influences the model's output, particularly regarding non-visual-related  output content. However, we aim to explain the correlation between the output and the input image. Therefore, we do not consider the direct impact of the text prompt on the output in our analysis.

### Manipulating Attention via Latent Variable Learning

Based on the analysis above, our core idea is to indirectly influence the attention maps by editing visual tokens, thereby focusing on the referred regions. We achieve this by optimizing a learnable latent variable based on an energy function [59; 14], which calculates the relationship between the input referring and the attention maps. To do this, we first need to determine which attention maps to use. One approach is to use attention maps between each text prompt token and all visual tokens. However, because visual tokens typically have a significant impact on the result based on only a few most relevant text prompts (referred to as **highlight text tokens**), using all attention maps would be computationally redundant. Yet, for users, identifying the highlight text tokens can be challenging. Therefore, we simply average pool the attention maps generated for each text prompt token to represent the global context of the text prompt (referred to as the **context token**) and its association with visual tokens. We found that this simple method of using context tokens produces attention maps similar to those generated by highlight text tokens, as shown in Figure 2 (bottom line). We leave the optimization based on highlight text tokens for future work.

Specifically, our method supports four types of referring shapes, including box, mask, scribble, and point. We employ two types of energy functions to respectively support these referring shapes: a hard

Figure 3: Manipulating attention with various methods: (a), (b), and (c) demonstrate the manipulation of the attention map by adding an adjustment coefficient \(\) on the attention map in the first step during the model inference. (d) illustrates the step-by-step editing approach. (e) showcases that optimizing a learnable context tokens (mentioned in Sec. 4.2) instead of visual tokens, while (f) presents the results of our method optimizing the learnable latent variable.

mask-based energy function for box and mask referring, and a soft mask-based energy function for scribble and point referring.

Hard Mask-based Energy FunctionWe first zero initialize a learnable latent variable \(p_{v}\) with the same shape as \(e_{v}\), and add it to the \(e_{v}\). Then we can get \(N\) attention maps from \(N\) attention layers which model the relation between the context token and the novel visual tokens. Given the referring box or mask, we first convert it into a binary mask. Then, we compute the mask-based energy function based on the mask and the attention map \(A^{(ct)}\), which is obtained by averaging pooling from \(N\) attention maps. The energy function can be formulated as:

\[E(A^{(ct)},r)=(1-A^{(ct)}_{i}}{_{i}A^{( ct)}_{i}})^{2},\] (5)

where \(r\) denotes the referring region. Then the gradient of the loss 5 is computed via backpropagation to update the learnable latent variable:

\[_{v}_{v}-_{_{v}}E(A^{(ct)},r ),\] (6)

where \(>0\) is a hyperparameter controlling the strength of the guidance. By optimizing \(p_{v}\) through the Equation 6, we indirectly guide the attention maps to produce higher responses in the referring region \(r\), thereby increasing the influence of the visual content of region \(r\) on the output.

Soft Mask-based Energy FunctionSince scribble and point lack the concept of the region, it is optional to use an extra SAM  model to obtain a mask for applying the Hard Mask-based Energy Function. However, this incurs additional inference cost, so we also provide an optional soft mask-based energy function based on a distance matrix \(D\), which is computed via applying the OpenCV _distanceTransform_ function on the given scribble or point. Then the soft mask-based energy function can be formulated as:

\[E(A^{(ct)},r)=(1-^{2}/2 ^{2}}}{}A^{(ct)}_{i}}{_{i}A^{(ct)}_{i}})^{2},\] (7)

where \(\) is the standard deviation of the Gaussian function, which is set to 0.1. By optimizing \(p_{v}\) through the Equation 7, the closer the region of attention map is to the given scribble or point, the higher the response.

Figure 4: The overview of our method. With the provided visual prompt, we convert it into a mask, and compute the mask-based energy function between the mask and the pooled attention map. During the inference process, we conduct backpropagation to optimize a learnable latent variable. This process is executed at the 0-th step of model inference and iterated \(T\) times.

Finally, we iteratively optimize the learnable latent variable \(T\) times at the \(0\)-th step of model inference. In addition, to prevent overfitting, we employ Early Stop (ES) and Exponential Moving Average (EMA) strategies to enhance model stability. More details are shown in Appendix B.1.

## 5 Experiments

### Experiment Details

Unless explicitly stated otherwise, the MLLM we use is LLaVA-v1.5-7B , \(T\)=5, \(\)=400 and \(=0.5\). All experiments are conducted on two RTX 3090 GPUs with 24 GB of memory each.

### Applications

Referring with Different Visual Prompts.We first demonstrate referring QA with different visual prompts, including box, mask, scribble and point in the Figure 5. Our method consistently demonstrates significant controllability with four types of visual prompts. And our method improves the interpretability compared to basic model (column 3 vs column 2), demonstrates a stronger correlation between the attention response areas and the generated descriptions.

Out-of-Domain Task.We present examples of the performance on out-of-domain tasks OCR and Screenshots. As shown in Figure 6, compared to Ferret, our method correctly identified the text in the referring region. Additionally, as shown in Figure 9, our method correctly recognized the app in the mobile screenshot, unlike Ferret.

Figure 5: The examples of referring MLLM with four types of visual prompt, including box (a), mask (b), scribble (c) and point (d). The correct referring expressions are marked in green, incorrect referring expressions are marked in red, and hallucinated expressions are marked in orange. Compared to the baseline model, our method enhances **interpretability** and **controllability** with visual prompts, while also helping the model **mitigate hallucination** issues.

Impact on Hallucinations.Our method guides the model to focus on specific regions, potentially helps the model mitigate hallucination issues, as shown in Figure 5 (c,d output in orange color).

### Comparisons

Comparison on Referring Object Classification Task.Following Ferret [68; 74], we use the Referring Object Classification (ROC) task to evaluate whether our method can accurately pinpoint and understand the semantic of the referring region. The task requires the model to correctly identify the target within the referring region. We follow the setting of Ferret to form 1,748 questions (in which 1,548 for test and 200 for validation) based on LVIS  validation dataset, with corresponding box, mask, scribble and point. We consider the edit attention with \(=10\) (as Equation 4 and Figure 3 (b)) as the baseline model. And we compare several training methods [43; 75; 12; 68]. We also evaluate the lower and upper limits of LLaVA's recognition capability by assessing LLaVA without referring region, as well as background blur outside the referring region, which are presented in gray. Additionally, we evaluate a method that highlights regions with color as a comparable training-free method. More details and the input examples are shown in Appendix B.2.

The results are shown in Table 1. Our method shows a better performance than the training method GPT4-ROI with box referring (60.59 vs 58.59) and the Shikra-7B with point referring (58.85 vs 56.27). However, due to the limitations of LLaVA's capabilities (as shown in the results of the LLaVA+Blur), we present a performance gap compared to the latest training method Ferret . Our method also demonstrates superiority compared to training-free color prompt-based method and baseline method.

Comparison on Referring Text Classification Task.We consider the Referring Text Classification (RTC) task as the **out-of-domain** task, to verify the model's out-of-domain transfer capability. Similar to the ROC task, we formulate the problem as a binary classification task and construct 1,372 questions based on the COCO-Text  dataset. Since point and scribble referring methods are not

Figure 6: Examples of comparing with training method Ferret on OCR.

suitable for text due to the non-connectivity of the text, we only evaluate the RTC task with box and mask referring.

The results are shown in Table 2. All the training methods we evaluated exhibited poor out-of-domain generalization performance. Specifically, Ferret achieves only 55.47% accuracy on the RTC task, despite its excellent in-domain performance as shown in Table 1. In contrast, our training-free method still demonstrates the best **out-of-domain generalization** performance. We also present comparative examples of out-of-domain tasks, as shown in Figure 6 and Figure 9.

More Tasks and MLLMs.We also validate our method through the Referring Description Task on LLaVA-1.5-7B and InstructBLIP-7B . The results are shown in Table 3. Our method consistently improves the model's referring description performance. And we validate our method on the more MLLMs through the ROC and RTC Tasks, MLLMs including InstructBLIP-7B and LLaVA-HR-7B , more details are shown in Appendix B.3. The results are shown in Table 4, our method consistently improves performance across different MLLMs. Due to InstructBLIP's relatively poor text recognition capabilities, our method results in only a modest improvement in the RTC task. However, thanks to the beneficial effect of image resolution on the RTC task, our method achieves a relative improvement of approximately 11.59% on LLaVA-HR.

### Ablation Study

The ablation studies primarily focus on the box referred object classification. Furthermore, inspired by DIFNet , we calculate a relevancy between the model's output and pixels within the referring region to assess the extent to which the model's output is influenced by visual content within the region. More details and additional experiments are shown in Appendix B.2 and B.3.

Impact of \(T\) and \(\).As shown in Table 5, as \(T\) increases, the relevancy between the model's output and the referring regions also increases. However, the larger \(T\) results in a decrease in the model's accuracy on the ROC task, also with excessively large relevancy scores, showing that excessively large relevancy scores also indicate overfitting of the learnable latent variable. Therefore, the value of the relevancy score provides us with guidance to alleviate model overfitting, particularly when the relevancy score is around 0.18, typically resulting in better performance. And the value of \(\) affects the convergence speed of optimization, with larger \(\) also leading to overfitting of the model.

Impact of EMA and ES.As shown in Table 5, when equipped with a smaller \(\) value, it effectively mitigates the overfitting issue associated with the learnable latent variable. For instance, with \(=400\) and \(=0.3\), the model's performance improves from 53.5 to 62.5. However, a smaller value of \(\) also results in slower convergence of the learnable latent variable. Therefore, we combine the Early Stop strategy, allowing us to use a slightly larger \(\) to accelerate the convergence of the learnable latent variable. After incorporating the early stop strategy, we can opt for slightly larger \(T\) to ensure

   Models & Box & Mask & Scribble & Point \\  _Training Methods:_ & & & & \\ Kosmos-2  & 55.17 & - & - & - \\ GPT4-ROI  & 58.59 & - & - & - \\ Shikra-7B  & 64.60 & - & - & 56.27 \\ Ferret-7B  & 71.71 & 72.39 & 71.58 & 68.54 \\   _Training-Free Methods:_ & & & & \\ LLaVA  & 54.72 & 54.72 & 54.72 & 54.72 \\ LLaVA + Blur & 73.39 & 71.32 & - & - \\ LLaVA + Color & 55.10 & 56.72 & - & - \\ LLaVA + Edit Att & 36.24 & 37.08 & - & - \\ LLaVA + **Ours** & **60.59** & **60.79** & **58.33** & **58.85** \\   

Table 1: The results on Referring Object Classification Task (test set). The prompt of the task is featured as “_Is the object \(\)location\(\) a \(\)class A\(\) or a \(\)class B\(\)?_”. “- denotes the method does not support this type of referring. Results in gray font are provided for reference only.

   Models & Box & Mask \\  _Training Methods:_ & & \\ Kosmos-2  & 16.55 & - \\ GPT4-ROI  & 54.23 & - \\ Shikra-7B  & 50.07 & - \\ Ferret-7B  & 55.47 & 56.34 \\   _Training-Free Methods:_ & & \\ LLaVA  & 53.57 & 55.47 \\ LLaVA + Blur & 83.60 & 74.49 \\ LLaVA + Color & 56.34 & 54.23 \\ LLaVA + Edit Att & 26.09 & 29.16 \\ LLaVA + **Ours** & **61.22** & **60.28** \\   

Table 2: The results on Referring Text Classification Task. The prompt of task is featured as “_Is_ the text \(\)location\(\) of the image ‘\(\)text A\(\)’ or ‘text B\(\)’?please select only one.”.

that the model is adequately optimized on challenging samples. The early stop strategy allows us to attain superior model performance while reducing the impact of overfitting. The additional experiment about ES is shown in Table 6.

Impact of Different Text Prompts and the Size of Visual Prompts.We explore the impact of different text prompts and the size of visual prompts in Figure 10 and Figure 11 respectively. The results demonstrate that combining a clear and specific text prompt with an appropriate visual prompt size typically leads to improved controllability.

## 6 Limitations

While we have demonstrated visual prompt control by optimizing only visual tokens, our technique is subject to a few limitations. First, there is some additional inference overhead, while various engineering approaches (such as Ollama 2) can significantly speed up the process. Therefore, this limitation can be reasonably overlooked. Second, our method is applicable only to white-box models and relies on the basic capabilities of the models themselves. However, our approach is orthogonal to these ongoing advancements in foundational models. Third, currently, our method supports only a single region visual prompt, extending this to multi-region control is a direction for future work. Fourth, our current optimization strategy is relatively simple, and the selection of text prompts can also affect the optimization results. We plan to focus on improving this aspect in future research.

## 7 Conclusion

In this work, we present a training-free method to integrate visual prompts into Multimodal Large Language Models (MLLMs) through learnable latent variable optimization. By adjusting visual tokens during inference, our approach enhances the attention to referring regions, enabling detailed descriptions and reasoning without additional training costs. Our method supports various referring formats such as box, mask, scribble, and point. The results show that our approach demonstrates strong out-of-domain generalization and interpretability, making it a promising direction for embedding referring abilities into MLLMs.