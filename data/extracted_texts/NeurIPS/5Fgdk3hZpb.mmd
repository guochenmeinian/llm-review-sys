# _Squeeze_, _Recover_ and _Relabel_: Dataset Condensation at ImageNet Scale From A New Perspective

Zeyuan Yin\({}^{*1}\), Eric Xing\({}^{1,2}\), Zhiqiang Shen\({}^{*1}\)

\({}^{1}\)Mohamed bin Zayed University of AI \({}^{2}\)Carnegie Mellon University

{zeyuan.yin,eric.xing,zhiqiang.shen}@mbzuai.ac.ae

Equal contribution. Project page: https://zeyuanyin.github.io/projects/SRe2L/.

###### Abstract

We present a new dataset condensation framework termed **S**_queeze_ (**), **R**_ecover_ (**) and **R**_elabel_ (**) that decouples the bilevel optimization of model and synthetic data during training, to handle varying scales of datasets, model architectures and image resolutions for efficient dataset condensation. The proposed method demonstrates flexibility across diverse dataset scales and exhibits multiple advantages in terms of arbitrary resolutions of synthesized images, low training cost and memory consumption with high-resolution synthesis, and the ability to scale up to arbitrary evaluation network architectures. Extensive experiments are conducted on Tiny-ImageNet and full ImageNet-1K datasets1. Under 50 IPC, our approach achieves the highest 42.5% and 60.8% validation accuracy on Tiny-ImageNet and ImageNet-1K, outperforming all previous state-of-the-art methods by margins of 14.5% and 32.9%, respectively. Our approach also surpasses MTT  in terms of speed by approximately 52\(\) (ConvNet-4) and 16\(\) (ResNet-18) faster with less memory consumption of 11.6\(\) and 6.4\(\) during data synthesis. Our code and condensed datasets of 50, 200 IPC with 4K recovery budget are available at link.

## 1 Introduction

Over the past few years, the task of data condensation or distillation has garnered significant interest within the domain of computer vision . By distilling large datasets into representative, compact subsets, data condensation methods enable rapid training and streamlined storage, while preserving essential information from the original datasets. The significance of data condensation on both research and applications cannot be understated, as it plays a crucial role in the efficient handling

Figure 1: Left is data synthesis time vs. accuracy on ImageNet-1K with 10 IPC (Images Per Class). Models include ConvNet-4, ResNet-{18, 50, 101}. \({}^{}\) indicates ViT with 10M parameters . Right is the comparison of widely-used bilevel optimization and our proposed decoupled training scheme.

and processing of vast amounts of data across numerous fields. Through the implementation of sophisticated algorithms, such as Meta-Model Matching [3; 5], Gradient Matching [8; 9; 7], Distribution Matching [10; 6], and Trajectory Matching [1; 11], data condensation has made remarkable strides. However, prior solutions predominantly excel in distilling small datasets such as MNIST, CIFAR, Tiny-ImageNet , downscaled ImageNet  featuring low resolution, or a subset of ImageNet . This limitation arises due to the prohibitive computational expense incurred from executing a massive number of unrolled iterations during the bilevel optimization process (comprising an inner loop for model updates and an outer loop for synthetic data updates). In our study, employing a meticulously designed decoupling strategy for model training and synthetic data updating (as illustrated in Fig. 1 left), the proposed method is capable of distilling the entire large-scale ImageNet dataset at the conventional resolution of 224\(\)224 while maintaining state-of-the-art performance. Remarkably, our training/synthesis computation outstrips the efficiency of prior approaches, even those utilizing reduced resolution or subsets of ImageNet. A comparison of efficiency is provided in Table 1.

To address the huge computational and memory footprints associated with training, we propose a tripartite learning paradigm comprised of _Squeeze_, _Recover_, and _Relabel_ stages. This paradigm allows for the decoupling of the condensed data synthesis stage from the real data input, as well as the segregation of inner-loop and outer-loop optimization. Consequently, it is not restricted by the scale of datasets, input resolution, or the size of network architectures. Specifically, in the initial phase, rather than simultaneous sampling of real and synthetic data and its subsequent network processing to update the target synthetic data, we bifurcate this process into _Squeeze_ and _Recover_ stages to distinguish the relationship and reduce similarity between real and synthetic data.

In the subsequent phase, we exclusively align the batch normalization (BN)  statistics derived from the model trained in the first phase to synthesize condensed data. In contrast to feature matching solutions that perform the matching process solely on individual batches within each iteration, the trained BN statistics on original data span the entire dataset, thereby providing a more comprehensive and accurate alignment between the original dataset and synthetic data. Given that real data is not utilized in this phase, the decoupled training can considerably diminish computational costs compared to the preceding bilevel training strategy. In the final phase, we employ the models trained in the first phase to relabel the synthetic data, serving as a data-label alignment process. An in-depth overview of this process is illustrated in Fig. 2.

**Advantages.** Our approach exhibits the following merits: **(1)** It can process large resolution condensation during training effortlessly with a justified computational cost. **(2)** Unlike other counterparts, we can tackle the relatively large datasets of Tiny-ImageNet and ImageNet-1K to make our method more practical for real-world applications. **(3)** Our approach can directly utilize many off-the-shelf pre-trained large models that contain BN layers, which enables to further save the training overhead.

Extensive experiments are performed on Tiny-ImageNet and ImageNet-1K datasets. On ImageNet-1K with \(224 224\) resolution and IPC 50, the proposed approach obtains a remarkable accuracy of 60.8%, outperforming all previous methods by a large margin. We anticipate that our research will contribute to the community's confidence in the practical feasibility of large-scale dataset condensation using a decoupled synthesis strategy from the real data, while maintaining reasonable computational costs.

Figure 2: Overview of our framework. It consists of three stages: in the first stage, a model is trained from scratch to accommodate most of the crucial information from the original dataset. In the second stage, a recovery process is performed to synthesize the target data from the Gaussian noise. In the third stage, we relabel the synthetic data in a crop-level scheme to reflect the true label of the data.

#### 1.1.1 Contributions.

\(\) We propose a new framework for large-scale dataset condensation, which involves a three-stage learning procedure of squeezing, recovery, and relabeling. This approach has demonstrated exceptional efficacy, efficiency and robustness in both the data synthesis and model training phases.

\(\) We conduct a thorough ablation study and analysis, encompassing the impacts of diverse data augmentations for original data compression, various regularization terms for data recovery, and diverse teacher alternatives for relabeling on the condensed dataset. The comprehensive specifications of the learning process can offer valuable insights for subsequent investigations in the domain.

\(\) To the best of our knowledge, this is the first work that enables to condense the full ImageNet-1K dataset with an inaugural implementation at a standard resolution of 224\(\)224, utilizing widely accessible NVIDIA GPUs such as the 3090, 4090, or A100 series. Furthermore, our method attains the highest accuracy of 60.8% on full ImageNet-1K within an IPC constraint of 50 using justified training time and memory cost, outperforming all previous methods by a significant margin.

## 2 Approach

**Data Condensation/Distillation.** The objective of dataset condensation is to acquire a small synthetic dataset that retains a substantial amount of information present in the original data. Suppose we have a large labeled dataset \(=\{(_{1},_{1}),,(_{| |},_{||})\}\), we aim to learn a small condensed dataset \(_{}=\{(}_{1},}_{1}),,(}_{||},}_{||})\}\) (\(||||\)) that preserves the crucial information in original \(\). The learning objective on condensed synthetic data is:

\[_{_{}}=*{arg\,min}_{}_{}()\] (1)

where \(_{}()\!=\!_{(}, })_{}}\!\![(_{_{_{}}}(}),})]\), \(}\) is the soft label coresponding to the synthetic data \(}\).

The ultimate goal of data condensation task is to synthesize data for achieving a certain/minimal supremum performance gap on original evaluation set when models are trained on the synthetic data and the original full dataset, respectively. Following the definition of coresets  and \(\)-approximate , the objective of data condensation task can be formulated as achieving:

\[\{|(_{_{}}(), )-(_{_{_{}}}(), )|\}_{(,)}\] (2)

where \(\) is the performance gap for models trained on the synthetic data and the original full dataset. Thus, we aim to optimize the synthetic data \(_{}\) through:

\[*{arg\,min}_{_{},||}( \{|(_{_{}}(),) -(_{_{_{}}}(), )|\}_{(,)})\] (3)

Then, we can learn \(<\!,\!\!>_{}\) with the associated number of condensed data in each class.

**Decoupling the condensed data optimization and the neural network training:** Conventional solutions, such as FRePo , CAFE , DC , typically choose for the simultaneous optimization of the backbone network and synthetic data within a singular training framework, albeit in an iterative fashion. The primary drawback associated with these joint methods is their computational burden due to the unrolling of the inner-loop during each outer-loop update, coupled with bias transference

   Methods & Condensed Arch & Time Cost (ms) & Peak GPU Memory Usage (GB) \\  DM  & ConvNet-4 & 18.11 & 10.7 \\ MTT  & ConvNet-4 & 12.64 & 48.9 \\  ^{2}\) (Ours)} & ConvNet-4 & 0.24 & 4.2 \\  & ResNet-18 & 0.75 & 7.6 \\   & ResNet-50 & 2.75 & 33.8 \\   

Table 1: **Synthesis Time and Memory Consumption on Tiny-ImageNet (64\(\)64 resolution) using a single RTX-4090 GPU for all methods. Time Cost represents the consumption (ms) when generating one image with one iteration update on synthetic data. Peak value of GPU memory usage is measured or converted with a batch size of 200 (1 IPC as the dataset has 200 classes).**from real to synthetic data as a result of truncated unrolling. The objective of this study is to devise an efficient learning framework capable of individually decoupling model training and synthetic data optimization. This approach circumvents information bias stemming from real data, concurrently enhancing efficiency in handling diverse scales of datasets, model architectures, and image resolutions, thereby bolstering effective dataset condensation. Our framework is predicated on the assumption that pivotal information within a dataset can be adequately trained and preserved within a deep neural network. The training procedure of our approach is elaborated in the following section.

### Decoupling Outer-loop and Inner-loop Training

Inspired by recent advances in DeepDream , Inverting Image [18; 19] and data-free knowledge transfer , we propose a decoupling approach to disassociate the traditional bilevel optimization inherent to dataset condensation. This is accomplished via a tripartite process to reformulate it into a unilevel learning procedure.

**Stage-1 Squeeze (\(_{}}\))**: During this stage, our objective is to extract pertinent information from the original dataset and encapsulate it within the deep neural networks, evaluating the impact of various data augmentation techniques, training strategies, etc. Deep neural networks typically comprise multiple parametric functions, which transform high-dimensional original data (e.g., pixel space of images) into their corresponding low-dimensional latent spaces. We can exploit this attribute to abstract the original data to the pretrained model and then reconstruct them in a more focused manner, akin to DeepDream  and Inverting Images [18; 19]. It's noteworthy that the purpose of this stage is to extract and encapsulate critical information from the original dataset. Hence, excessive data augmentation resulting in enhanced performance does not necessarily lead to the desired models. This approach diverges from previous solutions that sample two data batches from the original large-scale dataset \(\) and the learnable small synthetic dataset \(\). The learning procedure can be simply cast as a regular model training process on the original dataset with a suitable training recipe:

\[_{}=*{arg\,min}_{}_ {}()\] (4)

where \(_{}()\) typically uses cross-entropy loss as \(_{}()=_{(,) }[())}]\).

**Enabling BN Layers in ViT for Recovering Process**: In contrast to distribution matching  that aligns the feature distributions of the original and synthetic training data in sampled embedding spaces, thus allowing for the use of a randomly initialized network, our matching mechanism is solely performed on the Batch Normalization (BN) layers using their statistical properties, akin to data-free knowledge transfer . Unlike the feature matching solution which executes the matching process on individual batches within each iteration, the referential BN statistics are calculated over the entirety of the dataset, providing a more comprehensive and representative alignment with the original dataset. Our experiments empirically substantiate that BN-matching can significantly outperform the feature-matching method. BN layer is commonly used in ConvNet but is absent in ViT. To utilize both ConvNet and ViT for our proposed data condensation approach, we engineer a BN-ViT which replaces all LayerNorm by BN layers and adds additional BN layers in-between two linear layers of feed-forward network, as also utilized in . This marks the first instance of broadening the applicability of the data condensation architecture from ConvNets to encompass ViTs as well.

**Stage-2 Recover (\(_{}}\))**: This phase involves reconstructing the retained information back into the image space utilizing class labels, regularization terms, and BN trajectory alignment. Unlike conforming to batch feature distributions or comprehensive parameter distributions, we solely track the distribution of BN statistics derived from the original dataset. The pairing of BN and predictive probability distribution restricts the optimization process to a singular level, thereby significantly enhancing scalability. By aligning the final classification and intermediary BN statistics (mean and variance), the synthesized images are compelled to encapsulate a portion of the original image distribution. The learning objective for this phase can be formulated as follows:

\[*{arg\,min}_{_{},||}( _{_{}}(}_{}),) +_{}\] (5)

where \(_{}\) is the regularization term. \(_{_{}}\) is the model pre-trained in the first stage and it will be frozen in this stage, we solely optimize the \(}_{}\) as a single-level training process. Following [18; 20], we discuss three regularizers that can be used and the ablation for them is provided in our experiments. The first two regularizers are image prior regularisers of \(_{2}\) and total variation (TV) proposed in :

\[_{}(}_{})= _{}_{}(}_{})+_{ _{2}}_{_{2}}(}_{})\] (6)where \(_{_{2}}=\|}_{}\|_{2}\), this regularizer encourages image to stay within a target interval instead of diverging. \(_{}=_{i,j}((}_{i,j+1}- {}_{ij})^{2}+(}_{i+1,j}-}_{ij})^{2} )^{}\) where \(\) is used to balance the sharpness of the image with the removal of "spikes" and smooth the synthetic data. While, this is not necessary for dataset condensation since we care more about information recovery.

**Learning Condensed Data with BN Consistency**: DeepInversion  utilizes the feature distribution regularization term to improve the quality of the generated images. Here, we also leverage this property as our recovering loss term. It can be formulated as:

\[_{}(}) =_{l}\|_{l}(})-(_ {l})\|_{2}+_{l}\|_{l}^{2}( })-(_{l}^{2}) \|_{2}\] (7)

where \(l\) is the index of BN layer, \(_{l}(})\) and \(_{l}^{2}(})\) are mean and variance. \(_{l}^{}\) and \(_{l}^{}\) are running mean and running variance in the pre-trained model at \(l\)-th layer, which are globally counted.

**Multi-crop Optimization**: RandomResizedCrop, a frequently utilized technique in neural network training, serves as a preventative measure against overfitting. Inspired by this, we propose the strategy of multi-crop optimization during the process of image synthesis with the aim of enhancing the informational content of the synthesized images. In practice, we implement it by randomly cropping on the entire image and subsequently resizing the cropped region to the target dimension of \(224 224\). Under these circumstances, only the cropped region is updated during each iteration. This approach aids in refining the recovered data when viewed from the perspective of cropped regions.

**Stage-3 Relabel [(])**: To match our multi-crop optimization strategy, also to reflect the true soft label for the recovered data. We leverage the pre-generated soft label approach as FKD .

\[}_{i}=_{_{}}(}_{ _{i}})\] (8)

where \(}_{_{i}}\) is the \(i\)-th crop in the synthetic image and \(}_{i}\) is the corresponding soft label. Finally, we can train the model \(_{_{_{}}}\) on the synthetic data using the following objective:

\[_{}=-_{i}}_{i}_{_ {_{}}}(}_{_{i}})\] (9)

We found that this stage is crucial to make synthetic data and labels more aligned and also significantly improve the performance of the trained models.

**Discussion: How does the proposed approach reduce compute and memory consumption?** Existing solutions predominantly employ bilevel optimization [3; 23; 4] or long-range parameter matching strategies [1; 11], which necessitate the feeding of real data into the network to generate guiding variables (e.g., features, gradients, etc.) for target data updates, as well as for the backbone network training, through an iterative process. These approaches incur considerable computational and memory overhead due to the concurrent presence of real and synthetic data on computational hardware such as GPUs, thereby rendering this training strategy challenging to scale up for larger datasets and models. To this end, a natural idea is to decouple real and synthetic data during the training phase, thereby necessitating only minimal memory during each training session. This is achieved by bifurcating the bilevel training into a two-stage process: squeezing and recovering. Moreover, we can conveniently utilize off-the-shelf pre-trained models for the first squeezing stage.

## 3 Experiments

In this section, we evaluate the performance of our proposed \(^{2}\) over various datasets, models and tasks. First, we conduct extensive ablation experiments to investigate the effect of each component in three stages. Next, we demonstrate the superior results of \(^{2}\) in large-scale datasets, cross-architecture generalization, and continual learning application. Finally, we provide the comparison of visualizations on distilled data with other state-of-the-art methods.

**Experiment Setting.** We evaluate our method \(^{2}\) on two large-scale datasets Tiny-ImageNet  and full ImageNet-1K . Detailed comparison among variants of ImageNet-1K is in appendix. For backbone networks, we employ ResNet-{18, 50, 101} , ViT-Tiny , and our new constructed BN-ViT-Tiny (Sec. 2.1) as the target model training. For distilling ImageNet-1K, we recover the data from PyTorch off-the-shelf pre-trained ResNet-{18, 50} with the Top-1 accuracy of \(\{69.76\%,76.13\%\}\) for saving re-training computational overhead. For distilling Tiny-ImageNet, ResNet-{18, 50} are used as base models, while the first 7\(\)7 Conv layer is replaced by 3\(\)3 Conv layer and the maxpool layer is discarded, following MoCo (CIFAR) . After that, they are trained from scratch on Tiny-ImageNet. More implementation details are provided in appendix.

**Evaluation and baselines.** Following previous works , we evaluate the quality of condensed datasets by training models from scratch on them and report the test accuracy on real val datasets.

### Squeezing Analysis

Numerous training methods exist to enhance the accuracy of models , including extended training cycles/budgets and data augmentation strategies such as Mixup  and CutMix . We further examine the performance of synthetic data regenerated from models demonstrating varied levels of accuracy. This investigation is aimed at addressing a compelling question: _Does a model with superior squeezing ability yield more robust recovered data?_ Here, "a model with superior squeezing ability" is defined as a model that exhibits strengthened accuracy on the validation set.

**Squeezing Budget.** In Table 2, we observe a decrement in the performance of the recovered model as the squeezing budget escalates. This suggests an increasing challenge in data recovery from a model that has been trained over more iterations. Consequently, we adopt a squeezing budget of 50 iterations as the default configuration for our experiments on Tiny-ImageNet.

**Data Augmentation.** Table 2 also shows that data augmentation methods in the squeeze stage decrease the final accuracy of the recovered data. In summary, results on Tiny-ImageNet indicate that extending the training duration and employing data augmentation during the squeeze phase exacerbates the complexity of data recovery from the squeezed model.

### Recovering Analysis

The condensed data are crafted and subsequently relabeled utilizing a pre-trained ResNet-18 with a temperature of 20. Then, we report the val performance of training a ResNet-18 from scratch.

**Image Prior Regularization.**\(_{TV}\) and \(_{_{2}}\) are extensively applied in image synthesis methods . However, in the pursuit of extracting knowledge from the pre-trained model, our focus is predominantly on the recuperation of semantic information as opposed to visual information. Analyzing from the standpoint of evaluation performance as shown in appendix, \(_{_{2}}\) and \(_{TV}\) barely contribute to the recovery of image semantic information, and may even serve as impediments to data recovery. Consequently, these image prior regularizations are omitted during our recovery phase.

**Multi-crop Optimization.** To offset the RandomResizedCrop operation applied to the training data during the subsequent model training phase, we incorporate a corresponding RandomResizedCrop augmentation on synthetic data during recovery. This implies that only a minor cropped region in the synthetic data undergoes an update in each iteration. Our experimentation reveals that the multi-crop optimization strategy facilitates a notable improvement in validation accuracy, as in Appendix Table 8.

A comparative visualization and comparison with other non-crop settings is shown in Fig. 3. In the last column (SRe\({}^{2}\)L), multiple miniature regions enriched with categorical features spread across the entire image. Examples include multiple volcanic heads, shark bodies, bee fuzz, and mountain ridges. These multiple small feature regions populate the entire image, enhancing its expressiveness in terms of visualization. Therefore, the cropped regions on our synthetic images are not only more closely associated with the target categories but also more beneficial for soft label model training.

    &  &  \\   & 50 & 100 & 200 & 400 & Mixup & CutMix \\  ResNet-18 & 37.88 & 37.49 & 32.18 & 23.88 & 35.59 & 25.90 \\ ResNet-50 & 39.19 & 33.63 & 30.65 & 26.43 & 36.95 & 29.61 \\   

Table 2: Ablation of squeezing budget and data augmentation on Tiny-ImageNet.

**Recover Budget.** We conduct various ablation studies to evaluate the impact of varying recovery budgets on the quality of synthetic data. The recovery budgets are designated as [0.5\(k\), 1\(k\), 2\(k\), 4\(k\)]. As in Table 3, it indicates that employing a longer recovery budget on the same model results in superior classification accuracy on the recovered data. In the case of recovery from diverse models, the results demonstrate that the data recovery under the same iterative setting and budget from ResNet-50 is inferior to that from ResNet-18 on both datasets. This suggests that the process of data recovery from larger pre-trained models is more challenging on large-scale datasets, necessitating more iterations to ensure that the recovered data achieves comparable performance on downstream classification tasks. To strike a balance between performance and time cost, we impose a recovery budget of 1\(k\) iterations on Tiny-ImageNet and 2\(k\) iterations on ImageNet-1K for the ablations, and 4\(k\) for the best in Table 4.

### Relabeling Analysis

We further study the influence of varying relabeling models and distinct softmax temperatures on different architectural models being optimized. In Fig. 4, we present three subfigures that represent

    &  &  &  \\   & & 0.5\(k\) & 1\(k\) & 2\(k\) & 4\(k\) & \\   & ResNet-18 & 34.82 & 37.88 & 41.35 & 41.11 & 0.75 \\  & ResNet-50 & 35.92 & 39.19 & 39.59 & 39.29 & 2.75 \\   & ResNet-18 & 38.94 & 43.69 & 46.71 & 46.73 & 0.83 \\  & ResNet-50 & 21.38 & 28.69 & 33.20 & 35.41 & 2.62 \\   

Table 3: Top-1 validation accuracy of ablation using ResNet-{18, 50} as recovery model with different updating iterations, and ResNet-18 as student model. “Time” represents the consuming time (ms) when training 1 image per iteration on one single NVIDIA 4090 GPU.

Figure 3: Visualization of distilled examples on ImageNet-1K under various regularization terms and crop augmentation settings. Selected classes are {Volcano, Hammerhead Shark, Bee, Valley}.

the training accuracy of the models on labels generated by three pre-trained models: ResNet-{18, 50, 101}. The training data utilized for these experiments is the same, which is recovered from a pre-trained ResNet-18 model.

**Relabeling Model.** In each row of Fig. 4 (i.e., on the same dataset), the results of three subfigures indicate that a smaller-scale teacher which is close or identical to the recovery model architecture always achieves better accuracy across ResNet-18, ResNet-50 and ResNet-101. Thus, it can be inferred that the labels of the recovered data are most accurate when the relabeling model aligns with the recovery model. Moreover, Top-1 errors tend to escalate with increasing disparity between the relabeling model and the recovery model. Consequently, in our three-stage methodology, we opt to employ an identical model for both recovery and relabeling processes.

**Temperature on Soft Label.** We conduct experiments encompassing five different temperature selections  specifically for label softmax operations under distillation configuration as . The result indicates that the Top-1 accuracy experiences a rapid surge initially and subsequently plateaus when the temperature setting exceeds 10. The maximum Top-1 accuracy is recorded as \(60.81\%\) when the temperature is fixed at 20, employing ResNet-18 as the teacher model and ResNet-101 as the student model. This observation underscores the beneficial effect of a higher temperature setting for label softmax operations in the training of the student model. Consequently, we elect to utilize a temperature value of 20 in our subsequent evaluation experiments.

**Model Training.** Contrary to previous data condensation efforts, where different architectures could not be effectively trained on the condensed data due to the mismatch of recovery models or overfitting on the limited synthetic data scale, our condensed data demonstrate the informativeness and training scalability as the inherent properties in real data. In each subfigure within Fig. 4, we observe a progressive increase in accuracy when training ResNet-18, ResNet-50, and ResNet-101 as the student networks. This indicates that our condensed data does not suffer from overfitting recovered from the squeezing model, and during inference, the Top-1 accuracy is consistently increasing using a network with enhanced capabilities.

### Condensed Dataset Evaluation

**Tiny-ImageNet.** The results derived from the Tiny-ImageNet dataset are presented in the first group of Table 4. Upon evaluation, it can be observed that MTT achieves 28.0% under the IPC 50. In contrast, our results on ResNet-{18, 50, 101} architectures are 41.1%, 42.2%, and 42.5%, respectively, which significantly surpass the performance of MTT. A noteworthy observation is that our stronger backbones not only accomplish superior accuracy but are also robust for different recovery architectures. In light of the results under IPC 50 and 100 settings on the relatively small Tiny-ImageNet dataset, it is apparent that the larger backbone did not yield a proportional enhancement in performance from ResNet-18 to ResNet-101 by our method in Table 4 and Fig. 4. While, this is different from the observation on full ImageNet-1K that we discuss elaborately below.

Figure 4: Top-1 val accuracy of models trained on various labels and temperature settings under IPC 50. T and S represent the reference model for relabeling and the target model to be trained, separately. R18, R50, and R101 are the abbreviation of ResNet-18, ResNet-50, and ResNet-101.

**ImageNet-1K.** As shown in the second group of Table 4, employing the same model architecture of ResNet-18 under IPC 10, our approach improves the performance of TESLA from a baseline of 7.7% to a significance of 21.3%. Contrary to TESLA, where the performance deteriorates with larger model architectures, our proposed approach capitalizes on larger architectures, displaying an appreciable proportional performance enhancement. This indicates significant promise in the contemporary era of large-scale models. On IPC 50, 100 and 200, our method obtains consistent boosts on accuracy.

### Cross-Architecture Generalization

It is important to verify the generalization property of our condensed datasets, ensuring its ability to effectively generalize to new architectures that it has not encountered during the synthesis phase. Fig. 4 and Table 5 demonstrate that our condensed dataset exhibits proficient cross-model generalization across ResNet-{18, 50, 101} and ViT-T. The results reveal that our condensed datasets maintain robustness across disparate and larger architectures. However, we observed suboptimal performance of ViT on the condensed dataset, potentially due to the model's inherent need for substantial training data as introduced in .

### Synthetic Image Visualization

Fig. 5 provides a visual comparison of selected synthetic images from our condensed dataset and the corresponding images from the MTT condensed dataset. The synthetic images generated by our method manifest a higher degree of clarity on semantics, effectively encapsulating the attributes and contours of the target class. In contrast, synthetic images from MTT appear considerably blurred, predominantly capturing color information while only encapsulating minimal details about the target class. Consequently, SRe2L produces superior-quality images that not only embed copious semantic information to augment validation accuracy but also demonstrate superior visual performance.

   Dataset & IPC & MTT  & TESLA  & TESLA  (ViT) & TESLA  (R18) & SRe2L (R18) & SRe2L (R101) \\  Tiny-IN & 50 & 28.0\(\)0.3 & - & - & - & 41.1\(\)0.4 & 42.2\(\)0.5 & 42.5\(\)0.2 \\  & 100 & - & - & - & - & 49.7\(\)0.3 & 51.2\(\)0.4 & 51.5\(\)0.3 \\   & 10 & 64.0\(\)1.3 & 17.8\(\)1.3 & 11.0\(\)0.2 & 7.7\(\)0.1 & 21.3\(\)0.6 & 28.4\(\)0.1 & 30.9\(\)0.1 \\  & 50 & - & 27.9\(\)1.2 & - & - & 46.8\(\)0.2 & 55.6\(\)0.3 & 60.8\(\)0.5 \\  & 100 & - & - & - & - & 52.8\(\)0.3 & 61.0\(\)0.4 & 62.8\(\)0.2 \\  & 200 & - & - & - & - & 57.0\(\)0.4 & 64.6\(\)0.3 & 65.9\(\)0.3 \\   

Table 4: Comparison with baseline models. \({}^{}\) indicates the ImageNet dataset, which contains only 10 classes. TESLA  uses the downsampled ImageNet-1K dataset. Our results are derived from the full ImageNet-1K, which is more challenging on computation and memory, meanwhile, presenting greater applicability potential in real-world scenarios. The recovery model used in the table is R18.

Figure 5: Visualization of MTT  and our SRe2L. The upper two rows are synthetic Tiny-ImageNet and the lower two rows are synthetic ImageNet-1K (the first row is MTT and second is ours).

    &  \\   & DeiT-Tiny & ResNet-18 & ResNet-50 & ResNet-101 \\  DeiT-Tiny-BN & 25.36 & 24.69 & 31.15 & 33.16 \\ ResNet-18 & 15.41 & 46.71 & 55.29 & 60.81 \\   

Table 5: ImageNet-1K Top-1 on Cross-Architecture Generalization. Two recovery/squeezed models are used: DeiT-Tiny-BN and ResNet-18. Four evaluation models: DeiT-Tiny, ResNet-{18, 50, 101}.

### Application: Continual Learning

Many prior studies [10; 5; 32; 7] have employed condensed datasets for continual learning to assess the quality of the synthetic data. We adhere to the method outlined in DM  class-incremental learning implementation, which is based on GDumb . This method sequentially stores prior training data in memory and utilizes both new training data and stored data to learn a model from scratch. To demonstrate the superiority of our method in handling large-scale data, we conduct class incremental learning on Tiny-ImageNet, incorporating an escalating memory budget of 100 images per class, and training with ResNet-18. Fig. 6 shows both 5-step and 10-step class-incremental learning strategies, which partition 200 classes into either 5 or 10 learning steps, accommodating 40 and 20 classes per step respectively. Our results are clearly better than the baselines.

## 4 Related Work

Data condensation or distillation aims to create a compact synthetic dataset that preserves the essential information in the large-scale original dataset, making it easier to work with and reducing training time while achieving the comparable performance to the original dataset. Previous solutions are mainly divided into four categories: _Meta-Model Matching_ optimizes for the transferability of models trained on the condensed data and uses an outer-loop to update the synthetic data when generalized to the original dataset with an inner-loop to train the network, methods include DD , KIP , RFAD , FRePo  and LinBa ; _Gradient Matching_ performs a one-step distance matching process on the network trained on original dataset and the same network trained on the synthetic data, methods include DC , DSA , DCC  and IDC ; _Distribution Matching_ directly matches the distribution of original data and synthetic data with a single-level optimization, methods include DM , CAFE , HaBa , IT-GAN , KFS ; _Trajectory Matching_ matches the training trajectories of models trained on original and synthetic data in multiple steps, methods include MTT  and TESLA . Our proposed decoupling method presents a new perspective for tackling this task, while our BN-matching recovering procedure can also be considered as a special format of _Distribution Matching_ scheme on the synthetic data and global BN statistics distributions.

## 5 Conclusion

We have presented a novel three-step process approach for the dataset condensation task, providing a more efficient and effective way to harness the power of large-scale datasets. By employing the sequential steps of squeezing, recovering, and relabeling, this work condenses the large-scale ImageNet-1K while retaining its essential information and performance capabilities. The proposed method outperforms existing state-of-the-art condensation approaches by a significant margin, and has a wide range of applications, from accelerating the generating and training process to enabling the method that can be used in resource-constrained environments. Moreover, the study demonstrates the importance of rethinking conventional methods of data condensation and model training, as new solutions can lead to improvements in both computational efficiency and model performance. As the field of data condensation continues to evolve, the exploration of targeting approaches, such as the one presented in this work, will be crucial for the development of future condensation approaches that are more efficient, robust, and capable of handling vast amounts of data in a sustainable manner.

**Limitation and Future Work**: At present, a performance disparity persists between the condensed dataset and the original full dataset, indicating that complete substitution of the full data with condensed data is yet to be feasible. Another limitation is the extra storage for soft labels. Moving forward, our research endeavors will concentrate on larger datasets such as the condensation of ImageNet-21K, as well as other data modalities encompassing language and speech.

Figure 6: 5-step and 10-step class-incremental learning on Tiny-ImageNet.