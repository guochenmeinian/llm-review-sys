# SugarCrepe: Fixing Hackable Benchmarks for Vision-Language Compositionality

Cheng-Yu Hsieh\({}^{1}\), Jieyu Zhang\({}^{1}\), Zixian Ma\({}^{1}\), Aniruddha Kembhavi\({}^{2}\), Ranjay Krishna\({}^{1,2}\)

\({}^{1}\)University of Washington, \({}^{2}\)Allen Institute for Artificial Intelligence

{cydhsieh,jieyu22,zixianma,ranjay}@cs.washington.edu, anik@allenai.org

The authors contribute equally to this work.

###### Abstract

In the last year alone, a surge of new benchmarks to measure _compositional_ understanding of vision-language models have permeated the machine learning ecosystem. Given an image, these benchmarks probe a model's ability to identify its associated caption amongst a set of compositional distractors. Surprisingly, we find significant biases in _all_ these benchmarks rendering them hackable. This hackability is so dire that blind models with no access to the image outperform state-of-the-art vision-language models. To remedy this rampant vulnerability, we introduce SugarCrepe, a new benchmark for vision-language compositionality evaluation. We employ large language models, instead of rule-based templates used in previous benchmarks, to generate fluent and sensical hard negatives, and utilize an adversarial refinement mechanism to maximally reduce biases. We re-evaluate state-of-the-art models and recently proposed compositionality inducing strategies, and find that their improvements were hugely overestimated, suggesting that more innovation is needed in this important direction. We release SugarCrepe and the code for evaluation at: [https://github.com/RAIVNLab/sugar-crepe](https://github.com/RAIVNLab/sugar-crepe).

## 1 Introduction

Scholars today herald _compositionality_ as a fundamental presupposition characterizing both human perception and linguistic processing . Through compositional reasoning, humans can comprehend new scenes and describe those scenes by composing known atoms . For instance, compositionality allows people to differentiate between a photo of "a girl in white facing a man in black" and "a girl in black facing a man in white". For a while now, vision-language research has sought to develop models that can similarly comprehend scenes and express them through compositional language .

Given its importance, a surge of new benchmarks have been proposed to evaluate whether vision-language models exhibit compositionality. Recently, Winoground , VL-CheckList , ARO , CREPE , and Cola  have entered the machine learning zeitgeist. Evaluation is mostly done through an image-to-text retrieval task formulation : by measuring how often models pick the description, "a girl in white facing a man in black" when presented with an image of it, and avoid choosing the incorrect _hard negative_ description, "a girl in black facing a man in white".

In this work, we uncover a crucial vulnerability in not just one but all these image-to-text compositionality benchmarks: We find that a _blind_ model that never looks at the image, can identify the correct caption and avoid choosing the supposed "hard negatives". This blind model outperforms a wide array of pretrained vision-language models across the suite of benchmarks . We explain this undesired hackability in existing benchmarks by showcasing that there exists a significant distributional gap between the positive and hard negative captions. For instance, in the ARO benchmark , human-generated positive captions differ drastically from the hard negative texts generated by randomly shuffling words in the positive captions. As new research has begun to propose methods that claim to improve compositionality on these benchmarks [49; 38], we find it critical to highlight our findings and propose a solution.

We propose a solution to existing hackable benchmarks by introducing SugarCrepe, a new benchmark to faithfully evaluate compositionality. In curating SugarCrepe, we identify two main _biases_2 that result in the distributional gap between positive and hard negatives; and employ mechanisms to fix the shifts. In particular, we find the current procedure in generating hard negatives introduces descriptions that are (1) not plausible and (2) non-fluent. For example, while the caption "olives and grapes on a plate" is a sensical fluent caption, benchmarks often have non-plausible hard negatives like "olives and grapes inside a plate" or simply incomprehensible ones like "right has word another word. There is a words" (see Table 1 for more examples). We mitigate such biases by first leveraging a modern large language model, ChatGPT , to generate plausible and natural hard negative texts instead of relying on simple rule-based templates employed by existing benchmarks [30; 49]. Then, we subsample the dataset through an adversarial refinement process to ensure the identified biases are maximally removed by drawing on recent dataset debiasing work [50; 41; 23]. Taken together, this workflow is where SugarCrepe derived its name: **S**ynthetic yet **U**nbiased **G**eneration with **A**dversarially **R**efined **C**ompositional **RE**resentation **E**valuation. We qualitatively and quantitatively verify through both human and automatic evaluations that SugarCrepe effectively fixes these biases.

With SugarCrepe, we _re_-evaluate recent methods proposed to improve compositionality. Specifically, we focus on one prominent approach that aims to improve compositionality through data augmentation. This method trains models by generating compositional hard negatives and injecting them within a training batch [13; 49]. Unfortunately, we observe that the effectiveness of this simple data augmentation approach is hugely _overestimated_ when evaluated on existing benchmarks, leading to limited improvements on SugarCrepe. Finally, we evaluate a wide variety of \(17\) pretrained CLIP models [36; 18; 14], and find that current models still lack compositionality. Our results suggest that to improve compositionality, future work may need more innovative techniques.

## 2 Related Work

We situate our paper amongst existing work on vision-language compositionality, and debiasing datasets for model evaluation.

**Evaluating vision-language compositionality.** Recent works have introduced benchmarks to evaluate the compositionality of vision-language models ; they find that current models exhibit little compositional understanding [49; 45; 53; 30; 38] despite their remarkable performance on downstream tasks [36; 25; 43; 1; 47; 48; 52]. Models have a hard time discerning between text containing the same words ordered differently . Models also fail to link objects to their attributes, or understand the relationship between objects [53; 49; 38]. Our work finds that many of the benchmarks used to evaluate compositionality have hackable biases; blind models that do not even look at the image outperform state-of-the-art vision-language models.

**Improving vision-language compositionality.** To enhance vision-language models' compositionality, new proposals suggest training strategies that utilize additional data, models, and/or losses [49; 5; 38; 13; 44]. Amongst them, one prominent approach is to explicitly train the models to distinguish hard negatives from the correct captions [49; 13]. While these approaches appear to improve compositionality on benchmarks, it is unclear if these models achieve such improvements by actually acquiring compositional understanding or by exploiting biases in these datasets. We answer this question in our evaluation.

**Debiasing dataset for faithful model evaluation.** Several prior manuscripts have pointed out that biased datasets could lead to an overestimation of models' true capabilities . They have proposed dataset de-biasing methods to enable more faithful model evaluations [39; 50; 41; 23; 34]. For instance, adversarial filtering  iteratively trains an ensemble of classifiers on different training splits and uses them to filter out "easy" negatives for each instance. Building upon adversarial filtering, AFLite [41; 23] filters data instances in a more light-weight manner without retraining a model at each iteration and leads to benchmarks that more accurately represent the underlying tasks. We use adversarial refinement to remove biases that creep into the generation of compositionality benchmarks.

## 3 Limit and biases of current compositionality benchmarks

A majority of existing compositionality benchmarks for vision-language models formulate the evaluation task as image-to-text retrieval [53; 49; 30]. We focus on these benchmarks and discuss others [45; 38] in Appendix B. Given an image, the model is probed to select text that correctly describes the image from a pool of candidates. Unlike standard retrieval tasks where the negative (incorrect) candidates differ a lot from the _positive_ (correct) text, compositionality benchmarks intentionally design _hard negative_ texts that differ minimally from the positive text, in order to test whether the model understands the fine-grained atomic concepts that compose the scene.

**Existing hard negative generation process introduces undesirable biases.** Existing benchmarks generate hard negative texts through rule-based programmatic procedures [53; 49; 30], which produce hard negatives by replacing a word of specific type (an object, attribute, or relation) in the original text, by swapping two words, or by shuffling the word order. We find that such procedures introduce unintentional biases in the generated hard negatives (see Table 1); specifically, we observe two major types of undesirable artifacts: (1) _nonsensical_ artifacts, and (2) _non-fluent_ artifacts. In order to quantitatively measure these biases, we utilize Vera , a plausibility estimation model, to characterize the nonsensical bias. Specifically, we define \((T)\) to be the plausibility score of a caption \(T\), where a higher score suggests more sensical the caption is. Similarly, to capture the non-fluent bias, we leverage a grammar-check model  that assigns high scores, \((T)\), to more grammatically correct texts. In Figure 1, we find that Vera and the grammar model assign higher scores to positive texts, suggesting that many hard negatives are nonsensical and not fluent.

**Dataset biases render current compositionality benchmarks ineffective.** Given the heavily-skewed score gaps, we show that blind models (_i.e._, Vera and the grammar model) that simply select the higher-scoring texts as positives and admittedly do not possess any vision-language compositionality, can achieve state-of-the-art performances on existing benchmarks. We compare the the blind models against \(17\) pretrained CLIP models from three sources: OpenAI's in-house WebImageText dataset , LAION , and Datacomp . We plot the performances of the blind models and the best-performing CLIP models from each category (Figure 2). Blind models achieves state-of-the-art performances on \(9\) out of \(10\) existing benchmark tasks. We provide full evaluation results in Appendix D.1.

## 4 SugarCrepe

We introduce SugarCrepe, a new benchmark for faithful evaluation of vision-language models' compositionality based on the image-text pairs of COCO . SugarCrepe presents two key contributions over existing benchmarks: (1) it drastically reduces the two identified dataset biases (Sec. 4.1), and (2) it covers a broad range of fine-grained types of hard negatives (Sec. 4.2). We present a summary comparison on compositionality benchmarks in Appendix B.

   Dataset & Nonsensical Hard Negatives & Non-fluent Hard Negatives \\  CREPE  & Olives and grape inside a plate. & A door with panes not in a room; the door has windows. \\  & Ground in a bucket on the flowers. & Right has word another word. There is a words. \\  & A h hair wearing a necklace, with her lady on a table. & A shelf with books in something. There is no background. \\  ARO  & The grass is eating the horse. & At brown cat a in looking a gray dog sitting is and white bathtub. \\  & A gray bathtub is looking at a white cat. & Scene with remarkable all blue a green behind chair. \\  & Green ball with a remarkable chair behind a blue scene. & Books the looking at people are. \\  VL-CheckList  & Sheep is hardwood. & An man fishing a food from a wrapper using a paw at a open. \\  & Empty arehex. & It having at a city. \\  & The bush speaking in the garden. & An grouping subdiang at a room access. \\   

Table 1: Existing compositionality benchmarks rely on procedurally-generated hard negatives which often do not make logical sense or are not fluent due to grammatical errors.

### SugarCrepe generation workflow alleviates dataset biases

The generation procedure of SugarCrepe consists of three main stages, centered around creating sensical and fluent hard negatives that close the distributional gaps to the positive texts, and ensuring a balanced distribution on the score gaps to make the final dataset robust to the identified biases.

**Stage 1: Generate sensical and fluent hard negatives with a large language model.** Observing the capability of modern large language models in generating fluent and plausible texts, we leverage ChatGPT  to generate hard negative texts where we explicitly instruct it to avoid commonsense (logical) and fluency (grammatical) errors. To guide ChatGPT in re-writing a given positive text into its hard negative counterparts, we provide few-shot demonstrations written by the authors and leverage its in-context learning ability  to generalize to unseen texts. Figure 3 shows an example demonstration used and an actual hard negative generated. We detail all the prompt templates in Appendix C.2. Table 3 shows the comparisons between hard negatives generated from ChatGPT in SugarCrepe and that from existing benchmarks.

**Stage 2: Filter false negatives with human validation.** A generated text is considered a valid hard negative only if it incorrectly describes the corresponding image. For example, given an image with a positive caption "a man and a child sitting on a sofa", a compositional change that replaces "child" with "girl" may still result in a correct caption. To ensure the validity of the hard negatives in SugarCrepe, we filter out _false_ negatives by manually examining the generated hard negatives and their corresponding images.

**Stage 3: De-bias dataset with adversarial refinement.** While ChatGPT yields more sensical and fluent text, there is no guarantee that the bias between positive and negative texts is negligible. Following dataset de-biasing work [50; 41; 23], we develop an adversarial refinement mechanism that maximally reduces the undesirably exploitable artifacts in SugarCrepe. Specifically, our goal is to ensure that performance improvements on SugarCrepe cannot be achieved by exploiting the

Figure 1: Top row: We define _Vera score gap_ as the score difference between the positive and hard negative texts: \((T^{})-(T^{n})\). The entire Vera score gap distribution lies on the positive spectrum, indicating that the template-generated hard negative texts usually have low plausibility. Bottom row: Similarly, _Grammar score gap_ is defined by: \((T^{})-(T^{n})\). On grammar score, we also find that the distribution largely rests on the positive side, suggesting that most hard negative texts in existing benchmarks exhibit grammatical errors.

Figure 2: Blind commonsense Vera model and Grammar model outperform state-of-the-art CLIP models on nearly _all_ existing benchmarks by exploiting the nonsensical and non-fluent artifacts. This suggests that existing benchmarks are hackable and ineffective in measuring compositionality.

identified nonsensical and non-fluent biases. To accomplish this, we characterize the biases again with the commonsense and grammar models , and subsample the dataset to ensure symmetric score gap distributions on both the positive and negative sides, as shown in Figure 4. We note the symmetry around zero implies that the commonsense and grammar scores can no longer be used to infer the ground truth positive texts. We provide the adversarial refinement algorithm in Algorithm 1.

```
0: Text-only model \(M_{1}\) and \(M_{2}\); Number of grids \(K\); A set of candidates \(=\{I_{i},T_{i}^{p},T_{i}^{n}\}_{i[N]}\), where \(I_{i}\), \(T_{i}^{p}\), and \(T_{i}^{n}\) are \(i\)-th image, positive caption, and negative caption.
0: A subset \(}\)
1: Calculate the model score gap for each candidate \(g_{i}^{(1)}=M_{1}(T_{i}^{p})-M_{1}(T_{i}^{n})\) and \(g_{i}^{(2)}=M_{2}(T_{i}^{n})-M_{2}(T_{i}^{n})\)
2: Split the 2D space \([-1,1][-1,1]\) to \(K K\) equal-size grids.
3: Place each candidate to a grid based on the score gaps \(g_{i}^{(1)}\) and \(g_{i}^{(2)}\).
4: Initialize \(}=\{\}\)
5:for each pair of grid \((G_{j},G_{j}^{*})\) symmetric about the original point \((0,0)\)do
6:if\(|G_{j}|>|G_{j}^{*}|\)then
7: Sample \(|G_{j}^{*}|\) candidates from \(G_{j}\) and put them to \(}\).
8: Put candidates in \(G_{j}^{*}\) to \(}\).
9:else
10: Sample \(|G_{j}|\) candidates from \(G_{j}^{*}\) and put them to \(}\).
11: Put candidates in \(G_{j}\) to \(}\).
```

**Algorithm 1** Adversarial Refinement

### SugarCrepe covers a broad range of hard negative types

To test different aspects of vision-language models' compositional understanding, we follow CREPE  to consider various _forms_ of hard negatives, and follow VL-CheckList  and ARO  to consider different fine-grained _categories_ of the atomic concepts. In total, SugarCrepe covers \(7\) fine-grained types of hard negatives, as shown in Table 2. We introduce the dataset taxonomy below, starting from the _form_ of the hard negatives to its different _finer-grained_ variants.

**The Replace form.** Given a positive text describing a scene, we generate a Replace hard negative by replacing an atomic concept in the original text with a new concept that makes the text mismatch with the original scene. Based on the type of the atomic concept--object, attribute, or relation--we further categorize Replace hard negatives into Replace-Obj, Replace-Att, and Replace-Rel.

**The Swap form.** Different from Replace, Swap does not introduce new concepts in the hard negatives, but a Swap hard negative is generated by swapping two atomic concepts of the same category in the positive text. We further categorize Swap into Swap-Obj and Swap-Att, and omit swapping two relationships since it generally results in nonsensical texts.

**The Add form.** Similar to the Replace form, but instead of replacing an atomic concept with a new one, we generate an Add hard negative by adding a new atomic concept to the positive text that makes it mismatch with the original scene. We only further categorize Add into Add-Obj (adding object concept) and Add-Att (adding attribute concept), as adding new relationship concepts to the positive texts often make them highly implausible.

**Dataset overview.** The final evaluation set of SugarCrepe consists of \(7,512\) examples, where the numbers for each fine-grained type are listed in Table 2. Each example is an image-to-text retrieval task composed of an image, a positive text, and a hard negative. On SugarCrepe, random chance performance has an average accuracy of \(50\%\). We note that ARO and CREPE additionally consider Shuffle (randomly shuffling words in a sentence) and Negate (adding negation keywords "no/not"

Figure 3: Example prompt (black) and actual hard negative (green) generated from ChatGPT.

[MISSING_PAGE_FAIL:6]

score gap distributions on the final SugarCrepe evaluation set are symmetric around zero. This implies that the previously identified artifacts can no longer be exploited to infer the positive texts. As a result, we show that the previous commonsense and grammar attacks that are extremely successful on existing benchmarks do not work on SugarCrepe. As shown in Table 6, these blind models now consistently rank the _last_ on SugarCrepe as compared to other pretrained CLIP models.

### Re-evaluating recent methods for improving compositionality

Given the vulnerability of existing compositionality benchmarks, it is unclear whether recently proposed methods that show state-of-the-art performances on these benchmarks are indeed effective. Thus, we re-evaluate these methods with SugarCrepe.

Hard negative augmented training.Specifically, we focus on evaluating one common _data-augmentation_ approach considered in , where the core idea is to explicitly create hard negatives and train the model to distinguish them. We broadly refer to this training scheme as NegCLIP following . We evaluate two NegCLIP training schemes: finetuning and training from scratch. For finetuning, in addition to taking the model released in , we finetune another three NegCLIP models (using ViT-B/32 following ) with three respective types of hard negatives (_i.e._, Replace, Swap, Negate) generated using CREPE's  source code. For training from scratch, we use RN50 as the base model and train variants of NegCLIP by augmenting the training examples with different types of hard negatives. We perform both training and finetuning on COCO .

    & &  \\  Hard-negative Type & Metric & ARO+CREPE & SugarCrepe & Pairwise Better Ratio \\   & Commonsense & 37.46 & 50.21 & 77.71 \\  & Grammar & 76.79 & 88.96 & 86.85 \\   & Commonsense & 23.09 & 41.57 & 78.76 \\  & Grammar & 45.67 & 80.46 & 87.02 \\   & Commonsense & 25.24 & 50.20 & 87.24 \\  & Grammar & 65.09 & 90.07 & 95.03 \\   

Table 4: We compare the commonsense and grammar scores on hard negatives in ARO+CREPE and SugarCrepe. We report both their respective average scores and the ratio where SugarCrepe has higher score than ARO+CREPE in pairwise comparison. Overall, SugarCrepe has hard negatives with better commonsense and grammar.

Figure 4: We compare the Vera (top row) and Grammar (bottom row) score gap distributions between ARO+CREPE (leftmost column), SugarCrepe without adversarial refinement (middle), and SugarCrepe (rightmost). Top row: We see that Vera score gap distribution shifts from the positive spectrum to more centered around zero from ARO+CREPE to SugarCrepe without refinement. After adversarial refinement, we ensure the score gap distribution is centered around zero on SugarCrepe. Bottom row: Similarly, from ARO+CREPE to SugarCrepe, we see the Grammar score gap distribution shifts from the positive spectrum to centered around zero.

**Improvements are overestimated due to unintentionally overfitting.** In Table 5, we first see that NegCLIP finetuned models show significant improvements on ARO+CREPE, boosting the performance more than 10% compared to standard CLIP finetuning on 11 out of 16 cases (highlighted in green). The lifts are especially large when the hard negative type used in finetuning matches that used in evaluation, where NegCLIP finetuned models can achieve near human-level performances. For instance, by finetuning with Replace hard negatives, NegCLIP reaches 94% on ARO+CREPE evaluated with Replace hard negatives (human performance is 95%). While the results on ARO+CREPE suggest that NegCLIP is seemingly sufficient in equipping models with strong compositionality, we however see that the improvements brought by NegCLIP are much smaller on SugarCrepe. In fact, none of the improvements on SugarCrepe is larger than 10%, and the best performing NegCLIP finetuned models still have large gaps to human-level performances, _e.g._, best NegCLIP model lags behind human by 23% on SugarCrepe's Swap hard negatives. Similarly, when trained from scratch, we observe the same trend that NegCLIP's improvements are much larger on ARO+CREPE than on SugarCrepe. The improvements on ARO+CREPE are again most pronounced when the training and testing hard negative type matches.

We attribute the stark contrast in NegCLIP's effectiveness on ARO+CREPE and SugarCrepe to model's unintentional overfitting: The NegCLIP models learned to exploit artifacts that can be used to easily distinguish hard negatives from positives on ARO+CREPE, instead of actually improving compositionality. Thus, when evaluated on SugarCrepe where the artifacts are removed, the improvement from NegCLIP drastically reduces. These results imply that NegCLIP's effectiveness is overestimated on existing benchmarks, and we may still need further innovations to fundamentally improve a model's compositionality. 4

### Comprehensive evaluations on existing pretrained vision-language models

We present four key findings in our evaluation over \(17\) pretrained CLIP models on SugarCrepe, with results reported in Table 6 and visualized in Figure 5.

**The best pretrained CLIP models demonstrate some compositional understanding but still have overall large rooms for improvements.** Table 6 shows that the largest pretrained CLIP models, _e.g._, OpenAI's RN50x64, LAION's xlm-roberta-large-ViT-H-14, and DataComp's ViT-L-14, achieve near-human performance on Replace-Obj. However, on Replace-Obj, smaller models pretrained on small datasets still suffer from big drops in performance -- 23% and 43% respectively for DataComp's small and medium models -- compared to humans. Additionally, on nearly all other hard negative types, there are clear gaps (larger than 10%) between the best model performances and human performances, showing an overall large room for improvements in current models' compositionality.

    & & & &  &  \\ 
**Model** & **Training** & & **Hard Negative Used** & **Replace** & **Swap** & **NeGate** & **Simple** & **Replace** & **Swap** & **Add** \\  Human & & & 95.33 & 100 & 99.33 & 96.00 & 98.67 & 99.50 & 99.00 \\   & Pretrained & N/A & 75.71 & 71.58 & 76.89 & 72.06 & 80.76 & 63.27 & 75.09 \\  & CLIP finetuned & N/A & 77.06 & 68.81 & 61.19 & 63.04 & 84.76 & 70.83 & 85.58 \\   &  & Replace & 94.51 & 90.04 & 85.06 & 88.15 & 88.27 & 74.89 & 90.16 \\  & & Swap & 82.88 & 94.48 & 77.57 & 87.00 & 85.54 & 76.21 & 86.56 \\  & & Niedate & 77.24 & 68.91 & 99.54 & 64.28 & 84.97 & 70.29 & 85.84 \\  & & Released in  & 85.72 & 94.35 & 83.51 & 90.45 & 85.36 & 75.33 & 87.29 \\   & CLIP from scratch & N/A & 69.93 & 59.96 & 55.36 & 68.78 & 69.54 & 60.33 & 67.63 \\   &  & Replace & 89.04 & 66.51 & 60.90 & 75.23 & 74.32 & 62.65 & 72.92 \\   & & Swap & 72.33 & 92.29 & 64.51 & 84.84 & 73.31 & 68.35 & 71.93 \\   & & Negate & 70.09 & 60.29 & 99.45 & 69.03 & 72.74 & 60.89 & 70.47 \\   & & Rep + Sw + Neg & 86.30 & 88.60 & 99.34 & 82.93 & 75.26 & 67.69 & 73.08 \\   

Table 5: Re-evaluating hard negative augmented training shows that the methodâ€™s improvements on existing benchmarks (ARO+CREPE) are hugely overestimated, particularly when the test hard negative type matches the one used in training, which can be attributed to overfitting the artifacts. Color notations: Gains compared to standard CLIP (finetuned / from scratch) \(>10\%\)

[MISSING_PAGE_FAIL:9]

Discussions

Our investigation reveals significant biases present in existing benchmarks for the compositional comprehension capability of vision-language models. The severity of this vulnerability is exemplified by text-only models without access to the image outperforming vision-language models. To address this, we introduce SugarCrepe, a novel benchmark for evaluating the compositionality of vision-language understanding. Unlike previous benchmarks that relied on rule-based templates, we leverage large language models to generate less biased negatives and employ adversarial filtering mechanisms to minimize biases. Through reassessment of state-of-the-art models and recently proposed compositionality inducing mechanisms, we uncover a significant overestimation of their advancements, underscoring the need for further innovation.

### Limitation and future work

Scope of the compositionality benchmarks and vision-language models.We focus our scope on compositionality benchmarks formulated as image-to-text retrieval task. While this is currently the most prevailing evaluation framework, future research can characterize compositionality evaluation as text-to-image retrieval problem, as in the initial efforts considered by [38; 45]. More importantly, we hope our work can guide future efforts in creating and ensuring faithful compositionality benchmarks in text-to-image form. In addition, we focus our evaluations on contrastively learned vision-language models . Future work should include and characterize the compositionality of modern generative vision-language models [1; 7; 24; 46].

**Potential biases imposed by language models.** In this work, we identify _two_ human interpretable dataset biases, the nonsensical and non-fluent biases, which may not cover all dataset artifacts that could possibly be exploited by a model. By leveraging ChatGPT in generating hard negatives, the generated captions may also exhibit hard to detect biases imposed by the language model, e.g., watermarks . Future work may utilize more sophisticated adversarial filtering techniques that train models to detect and remove spurious dataset artifacts beyond human comprehension [51; 23].

**Shifts in language model behavior.** Our work leverages ChatGPT to generate hard negatives. However, recent work has pointed out that the underlying model behind these APIs may change, resulting in model behavior shifts [6; 28]. We discuss how this potential model behavior shift may affect our proposed dataset construction pipeline. Specifically, while there may be variances on the quality of the generated texts, we note that our employed adversarial refinement mechanism can ensure that the final evaluation set is free of the identified artifacts. In the case when ChatGPT improves and generates higher-quality captions, the refinement mechanism will filter out less examples and we can more efficiently create the final evaluation set. On the other hand, if ChatGPT degrades and shifts towards generating less fluent and plausible captions, the refinement mechanism will filter out more generated examples and we would need to generate more candidates in order to create an evaluation set of the same desired size. As a result, while the efficiency of the proposed dataset construction pipeline depends on quality of the language model used, our pipeline ensures the generated set does not contain the identified biases. In the large language model era, we see these capable models as productive tools one can leverage to efficiently process and create data. We do however deem careful validation mechanisms, such as our manual and automatic filtering technique, necessary to ensure that the ultimate goal is properly achieved.

### Societal impact

As vision-language models such as CLIP  are becoming the foundation models for many downstream applications [40; 37], it is imperative to understand the limitations of these models to avoid misuses and undesirable outcomes [8; 2]. Compositionality benchmarks probe a model's understanding of finer-grained concepts, and hence allow us to identify blind spots [49; 53; 30] of seemingly powerful models deemed by standard classification and retrieval benchmarks [11; 26]. Our work further alleviates common artifacts in existing compositionality benchmarks that result in overestimation of a model's capability. We hope our proposed benchmark SugarCrepe leads to more faithful assessment of a vision-language model's compositionality, and can hence guide more accurate usages of the models. Nevertheless, we note that strong performances on SugarCrepe do not imply perfect models. We envision SugarCrepe being one of the many benchmarks used to comprehensively understand the abilities of vision-language models from various aspects.