# Hidden Poison: Machine Unlearning Enables

Camouflaged Poisoning Attacks+
Footnote †: Authors JA, GK, AS are listed in alphabetical order. Full paper: [https://arxiv.org/abs/2212.10717](https://arxiv.org/abs/2212.10717)

 Jimmy Z. Di

University of Waterloo

jimmy.di@uwaterloo.ca

&Jack Douglas

University of Waterloo

jack.douglas@uwaterloo.ca

&Jayadev Acharya

Cornell University

acharya@cornell.edu

&Gautam Kamath

University of Waterloo, Vector Institute

g@csail.mit.edu

&Ayush Sekhari

Massachusetts Institute of Technology

sekhari@mit.edu

###### Abstract

We introduce _camouflaged data poisoning attacks_, a new attack vector that arises in the context of machine unlearning and other settings when model retraining may be induced. An adversary first adds a few carefully crafted points to the training dataset such that the impact on the model's predictions is minimal. The adversary subsequently triggers a request to remove a subset of the introduced points at which point the attack is unleashed and the model's predictions are negatively affected. In particular, we consider clean-label _targeted_ attacks (in which the goal is to cause the model to misclassify a specific test point) on datasets including CIFAR-10, Imagenette, and Imageeowot. This attack is realized by constructing _camouflage_ datapoints that mask the effect of a poisoned dataset. We demonstrate the efficacy of our attack when unlearning is performed via retraining from scratch, the idealized setting of machine unlearning which other efficient methods attempt to emulate, as well as against the approximate unlearning approach of Graves et al. (2021).

## 1 Introduction

Machine Learning (ML) research traditionally assumes a static pipeline: data is gathered, a model is trained once and subsequently deployed. This paradigm has been challenged by practical deployments, which are more dynamic in nature. After initial deployment more data may be collected, necessitating additional training. Or, as in the _machine unlearning_ setting (Cao and Yang, 2015), we may need to produce a model as if certain points were never in the training set to begin with.1

While such dynamic settings clearly increase the applicability of ML models, they also make them more vulnerable. Specifically, they open models up to new methods of attack by malicious actors aiming to sabotage the model. In this work, we introduce a new type of data poisoning attack on models that _unlearn_ training datapoints. We call these _camouflaged data poisoning attacks_.

The attack takes place in two phases (Figure 1). In the first stage, before the model is trained, the attacker adds a set of carefully designed points to the training data, consisting of a _poison_ set and a _camouflage_ set. The model's behaviour should be similar whether it is trained on either the training data, or its augmentation with both the poison and camouflage sets. In the second phase, the attacker triggers an unlearning request to delete the _camouflage_ set after the model is trained. That is, themodel must be updated to behave as though it were only trained on the training set plus the poison set. At this point, the attack is fully realized, and the model's performance suffers in some way.

While such an attack could harm the model by several metrics, in this paper, we focus on _targeted_ poisoning attacks - that is, poisoning attacks where the goal is to misclassify a specific point in the test set. Our contributions are the following:

1. We introduce _camouflaged data poisoning_ attacks, demonstrating a new attack vector in dynamic settings including _machine unlearning_ (Figure 1).
2. We realize these attacks in the targeted poisoning setting, giving an algorithm based on the gradient-matching approach of Geiping et al. (2021). In order to make the model behavior comparable to as if the poison set were absent, we construct the camouflage set by generating a new set of points that _undoes_ the impact of the poison set. We thus identify a new technical question of broader interest to the data poisoning community: Can one nullify a data poisoning attack by only _adding_ points?
3. We demonstrate the efficacy of these attacks on a variety of models (SVMs and neural networks) and datasets (CIFAR-10 (Krizhevsky, 2009), Imagenette (Howard, 2019), and Imagewoof (Howard, 2019)).

### Preliminaries

Machine Unlearning.A significant amount of legislation concerning the "right to be forgotten" has recently been introduced by governments around the world, including the European Union's General Data Protection Regulation (GDPR), the California Consumer Privacy Act (CCPA), and Canada's proposed Consumer Privacy Protection Act (CPPA). Such legislation requires organizations to delete information they have collected about a user upon request. A natural question is whether that further obligates the organizations to remove that information from downstream machine learning models trained on the data - current guidances (Information Commissioner's Office, 2020) and precedents (Federal Trade Commission, 2021) indicate that this may be the case. This goal has sparked a recent line of work on _machine unlearning_(Cao and Yang, 2015).

The simplest way to remove a user's data from a trained model is to remove the data from the training set, and then retrain the model on the remainder (also called "retraining from scratch"). This is the ideal way to perform data deletion, as it ensures that the model was never trained on the datapoint of

Figure 1: An illustration of a successful camouflaged targeted data poisoning attack. In Step 1, the adversary adds poison and camouflage sets of points to the (clean) training data. In Step 2, the model is trained on the augmented training dataset. It should behave similarly to if trained on only the clean data; in particular, it should correctly classify the targeted point. In Step 3, the adversary triggers an unlearning request to delete the camouflage set from the trained model. In Step 4, the resulting model misclassifies the targeted point.

concern. The downside is that retraining may take a significant amount of time in modern machine learning settings. Hence, most work within machine unlearning has studied _fast_ methods for data deletion, sometimes relaxing to _approximately_ removing the datapoint. A related line of work has focused more on other implications of machine unlearning, particularly the consequences of an adaptive and dynamic data pipeline (Gupta et al., 2021; Marchant et al., 2022). Our work fits into the latter line: we show that the potential to remove points from a trained model can expose a new attack vector. Since retraining from scratch is the ideal result that other methods try to emulate, we focus primarily on unlearning by retraining from scratch, but the same phenomena should still occur when any effective machine unlearning algorithm is applied. For example, we also demonstrate efficacy against the approximate unlearning method of Graves et al. (2021).

**Data Poisoning.** In a data poisoning attack, an adversary in some way modifies the training data provided to a machine learning model, such that the model's behaviour at test time is negatively impacted. Our focus is on _targeted data poisoning attacks_, where the attacker's goal is to cause the model to misclassify some specific datapoint in the test set. Other common types of data poisoning attacks include _indiscriminate_ (in which the goal is to increase the test error) and _backdoor_ (where the goal is to misclassify test points which have been adversarially modified in some small way).

The adversary is typically limited in a couple ways. First, it is common to say that they can only _add_ a _small number_ of points to the training set. This mimics the setting where the training data is gathered from some large public crowdsourced dataset, and an adversary can contribute a few judiciously selected points of their own. Other choices may include allowing them to _modify_ or _delete_ points from the training set, but these are less easily motivated. Additionally, the adversary is generally constrained to _clean-label attacks_: if the introduced points were inspected by a human, they should not appear suspicious or incorrectly labeled. We comment that this criteria is subjective and thus not a precise notion, but is nonetheless common in data poisoning literature, and we use the term as well.

## 2 Discussion of Other Related Work

The motivation for our work comes from Marchant et al. (2022), who propose a novel poisoning attack on unlearning systems. As mentioned before, the primary goal of many machine unlearning systems is to "unlearn" datapoints quickly, i.e., faster than retraining from scratch. Marchant et al. (2022) craft poisoning schemes via careful noise addition, in order to trigger the unlearning algorithm to retrain from scratch on far more deletion requests than typically required. While both our work and theirs are focused on data poisoning attacks against machine unlearning systems, the adversaries have very different objectives. In our work, the adversary is trying to misclassify a target test point, whereas they try to increase the time required to unlearn a point.

In targeted data poisoning, there are a few different types of attacks. The simplest form of attack is _label flipping_, in which the adversary is allowed to flip the labels of the examples (Barreno et al., 2010; Xiao et al., 2012; Paudice et al., 2018). Another type of attack is _watermarking_, in which the feature vectors are perturbed to obtain the desired poisoning effect (Suciu et al., 2018; Shafahi et al.,

Figure 2: Some representative images from Imagewoof. In each pair, the left figure is from the training dataset, while the right image has been adversarially manipulated. The top and bottom rows are images from the poison and camouflage set, respectively. In all cases, the manipulated images are _clean label_ and nearly indistinguishable from the original image.

2018). In both these cases, noticeable changes are made to the label and feature vector, respectively, which would be noticeable by a human labeler. In contrast, _clean label_ attacks attempt to make unnoticeable changes to both the feature vector and the label, and are the gold standard for data poisoning attacks (Huang et al., 2020; Geiping et al., 2021). Our focus is on both clean-label poison and camouflage sets. While there are also works on indiscriminate (Biggio et al., 2012; Xiao et al., 2015; Munoz-Gonzalez et al., 2017; Steinhardt et al., 2017; Diakonikolas et al., 2019; Koh et al., 2022) and backdoor (Gu et al., 2017; Tran et al., 2018; Sun et al., 2019) poisoning attacks, these are beyond the scope of our work, see Goldblum et al. (2020); Cina et al. (2022) for additional background on data poisoning attacks.

Cao and Yang (2015) initiated the study of machine unlearning through _exact_ unlearning, wherein the new model obtained after deleting an example is statistically identical to the model obtained by training on a dataset without the example. A probabilistic notion of unlearning was defined by Ginart et al. (2019), which in turn is inspired from notions in differential privacy (Dwork et al., 2006). Several works studied algorithms for empirical risk minimization (i.e., training loss) (Guo et al., 2020; Izzo et al., 2021; Neel et al., 2021; Ullah et al., 2021; Thudi et al., 2022; Graves et al., 2021; Chourasia et al., 2022), while later works study the effect of machine unlearning on the generalization loss (Gupta et al., 2021; Sekhari et al., 2021). In particular, these works realize that unlearning data points quickly can lead to a drop in test loss, which is the theme of our current work. Several works have considered implementations of machine unlearning in several contexts starting with the work of Bourlotte et al. (2021). These include unlearning in deep neural networks (Goldarkar et al., 2020, 2021; Nguyen et al., 2020), random forests (Brophy and Lowd, 2021), large scale language models (Zanella-Beguelin et al., 2020), the tension between unlearning and privacy (Chen et al., 2021; Carlini et al., 2022), anomaly detection (Du et al., 2019), insufficiency of preventing verbatim memorization for ensuring privacy (Ippolito et al., 2022), and even auditing of machine unlearning systems (Sommer et al., 2020).

After the first version of our paper was uploaded on arXiv, Yang et al. (2022) discovered defenses against the data poisoning procedure of Geiping et al. (2021). One may further use their techniques to defend against our camouflaged poisoning attack as well, however, we note that this does not undermine the contributions of this paper. Our main contribution is to introduce this new kind of attack, and bring to attention that machine unlearning enables this attack; the specific choice of which procedure is used for poison generation or camouflage generation is irrelevant to the existence of such an attack.

## 3 Setup and Algorithms

### Threat Model and Approach

The camouflaged poisoning attack takes place through interaction between an _attacker_ and a _victim_, and is triggered by an unlearning request. We assume that the attacker has access to the victim's model architecture,2 the ability to query gradients on a trained model (which could be achieved, e.g., by having access to the training dataset), and a target sample that it wants to attack. The attacker first sets the stage for the attack by introducing _poison samples_ and _camouflage samples_ to the training dataset, which are designed so as to have minimal impact when a model is trained with this modified dataset. At a later time, the attacker triggers the attack by submitting an unlearning request to remove the camouflage samples. The victim first trains a machine learning model (e.g., a deep neural network) on the modified training dataset, and then executes the unlearning request by retraining the model from scratch on the left over dataset. The goal of the attacker is to change the prediction of the model on a particular target sample \((x_{},y_{})\) previously unseen by the model during training from \(y_{}\) to a desired label \(y_{}\), while still ensuring good performance over other validation samples. Formally, the interaction between the attacker and the victim is as follows (see Figure 1) :

1. The attacker introduces a small number of poisons samples \(S_{}\) and camouflage samples \(S_{}\) to a clean training dataset \(S_{}\). Define \(S_{}=S_{}+S_{}+S_{}\).
2. Victim trains an ML model (e.g., a neural network) on \(S_{}\), and returns the model \(_{}\).

3. The attacker submits a request to unlearn the camouflage samples \(S_{}\).3 4. The victim performs the request, and computes a new model \(_{}\) by retraining from scratch on the left over data samples \(S_{}=S_{}+S_{}\).

Note that the attack is only realized in Step 4 when the victim executes the unlearning request and retrains the model from scratch on the left over training samples. In fact, in Steps 1-3, the victim's model should behave similarly to as if it were trained on the clean samples \(S_{}\) only. In particular, the model \(_{}\) will predict \(y_{}\) on \(x_{}\), whereas the updated model \(_{}\) will predict \(y_{}\) on \(x_{}\). Both models should have comparable validation accuracy. Such an attack is implemented by designing a camouflage set that cancels the effects of the poison set while training, but retraining without the camouflage set (to unlearn them) exposes the poison set, thus negatively affecting the model.

Before we delve into technical details on how the poisons and camouflages are generated, we will provide an illustrative scenario where such an attack can take place. Suppose an image-based social media platform uses ML-based content moderation to filter out inappropriate images, e.g., adult content, violent images, etc., by classifying the posts as "safe" and "unsafe." An attacker can plant a camouflaged poisoning attack in the system in order to target a famous personality, like a politician, a movie star, etc. The goal of the attacker is to make the model misclassify a potentially sensitive target image of that person (e.g., a politically inappropriate image) as "safe." However, the attacker does not want to unleash this misclassification immediately, but to instead time it to align with macro events like elections, the release of movies, etc. Thus, at a later time when the attacker wishes, the misclassification can be triggered making the model classify this target image as safe and letting it circulate on the platform, thus hurting the reputation of that person at an unfortunate time (e.g., before an election or before when their movie is released). The attack is triggered by submitting an unlearning request.

We highlight that camouflaged attacks may be _more dangerous_ than traditional data poisoning attacks, since camouflaged attacks can be triggered by the adversary. That is, the adversary can reveal the attack whenever it wishes by submitting an unlearning request. On the other hand, in the traditional poisoning setting, the attack is unleashed as soon as the model is trained and deployed, the timing of which the attacker has little control over.

In order to be undetectable, and represent the realistic scenario in which the adversary has limited influence on the model's training data, the attacker is only allowed to introduce a set of points that is much smaller than the size of the clean training dataset (i.e., \(|S_{}||S_{}|\) and \(|S_{}||S_{}|\)). Throughout the paper and experiments, we denote the relative size of the poison set and camouflage set by \(b_{p}:=}|}{|S_{}|} 100\) and \(b_{c}:=}|}{|S_{}|} 100\), respectively. Additionally, the attacker is only allowed to generate poison and camouflage samples by altering the base images by less than \(\) distance in the \(_{}\) norm (in our experiments \( 16\), where the images are represented as an array of pixels in \(0\) to \(255\)). Thus, the attacker executes a so-called _clean-label_ attack, where the corrupted images would be visually indistinguishable from original base images and thus would be given the same label as before by a human data validator. We parameterize a threat model by the tuple \((,b_{c},b_{p})\).

### Poison and Camouflage Samples

The attacker implements the attack by first generating poison samples, and then generating camouflage samples to cancel their effects, as shown in the following.

**Poison samples.** Poison samples are designed so that a network trained on \(S_{}=S_{}+S_{}\) predicts the label \(y_{}\) (instead of \(y_{}\)) on a target image \(x_{}\). While there are numerous data poisoning attacks in the literature, we adopt the state-of-the-art procedure of Geiping et al. (2021) for generating poisons due to its high success rate, efficiency of implementation, and applicability across various models. However, our framework is flexible: in principle, other attacks for the same setting could serve as a drop-in replacement, e.g., the methods of Aghakhani et al. (2021) or Huang et al. (2020), or any method introduced in the future. 4

Suppose that \(S_{}\) consist of \(N_{1}\) samples \((x^{i},y^{i})_{i N_{1}}\) out of which the first \(P\) samples with index \(i=1\) to \(P\) belong to the poison set \(S_{}\).5 The poison samples are generated by adding small perturbations \(^{i}\) to the base image \(x^{i}\) so as to minimize the loss on the target with respect to the adversarial label, which can be formalized as the following bilevel optimization problem 6

\[_{}(f(x_{},()),y_{ })\] \[()*{arg\,min}_{} _{i N}(f(x^{i}+^{i},),y^{i}), \]

and we define the constraint set \(:=\{:\|\|_{}^{i}=0i>P\}\). The objective function in (1) is called the _adversarial loss_(Geiping et al., 2021). In all our experiments, we generate the poison points using the gradient matching technique of Geiping et al. (2021), which we detail in Appendix A for completeness.

**Camouflage samples.** Camouflage samples are designed to cancel the effect of the poisons, such that a model trained on \(S_{}=S_{}+S_{}+S_{}\) behaves identical to the model trained on \(S_{}\), and makes the correct prediction on \(x_{}\). We formulate this task via a bilevel optimization problem similar to (1). Let \(S_{}\) consist of \(N_{2}\) samples \((x^{j},y^{j})_{j N_{2}}\) out of which the last \(C\) samples with index \(j=N_{2}-C+1\) to \(N_{2}\) belong to the camouflage set \(S_{}\). The camouflage points are generated by adding small perturbations \(^{j}\) to the base image \(x^{j}\) so as to minimize the loss on the target with respect to the adversarial label. In particular, we find the appropriate \(\) by solving:

\[_{}(f(x_{},()),y_{ })\] \[()*{arg\,min}_{} }_{j N_{2}}(f(x^{j}+^{j},),y^{j}), \]

and we define the constraint set \(:=:\|\|_{}^{j}=0j N_{2}-C}\).

### Efficient Algorithms for Generating Camouflages

Given the poison images, camouflage images are designed in order to neutralize the effect of the poisons. Here, we give intuition into what we mean by canceling the effect of poisons, and provide two procedures for generating camouflages efficiently: label flipping, and gradient matching.

#### 3.3.1 Camouflages via Label Flipping

Suppose that the underlying task is a binary classification problem with the labels \(y\{-1,1\}\), and that the model is trained using linear loss \((f(x,),y)=-yf(x,)\). Then, simply flipping the labels allows one to generate a camouflage set for any given poison set \(S_{}\). In particular, \(S_{}\) is constructed as: for every \((x^{i},y^{i}) S_{}\), simply add \((x^{i},-y^{i})\) to \(S_{}\) (i.e., \(b_{p}=b_{c}\)). It is easy to see that for such camouflage samples, we have for any \(\),

\[_{(x,y) S_{}}(f(x,),y)\] \[=-_{(x,y) S_{}}yf(x,)-_{i=1}^{P} y^{i}f(x^{i},)+(-y^{i})f(x^{i},)\] \[=_{(x,y) S_{}}(f(x,),y).\]We can also similarly show that the gradients (as well as higher order derivatives) are equal, i.e., \(_{}_{S_{}}(f(x,),y)=_{}_{S_{ }}(f(x,),y)\) for all \(\). Thus, training a model on \(S_{}\) is equivalent to training it on \(S_{}\). In essence, the camouflages have perfectly canceled out the effect of the poisons. We validate the efficacy of this approach via experiments on linear SVM trained with hinge loss (which resembles linear loss when the domain is bounded). See Section 4.1.1 for details.

While label flipping is a simple and effective procedure to generate camouflages, it is fairly restrictive. Firstly, label flipping only works for binary classification problems trained with linear loss. Secondly, the attack is not clean label as the camouflage images are generated as \((x^{i},-y^{i})\) by giving them the opposite label to the ground truth, which can be easily caught by a validator. Lastly, the attack is vulnerable to simple data purification techniques by the victim, e.g., the victim can protect themselves by preprocessing the data to remove all the images that have both the labels (\(y=+1\) and \(y=-1\)) in the training dataset. In the next section, we provide a different procedure to generate clean-label camouflages that works for general losses and multi-class classification problems.

#### 3.3.2 Camouflages via Gradient Matching

Here, we discuss our main procedure to generate camouflages, which builds on the gradient matching idea of Geiping et al. (2021). Note that, our objective in (2) is to find \(\) such that when the model is trained with the camouflages, it minimizes the original-target loss in (2) (with respect to the original label \(y_{}\)) thus making the victim model predict the correct label on this target sample. Since, (2) is computationally intractable, one may instead try to implicitly minimize the original-target loss by finding a \(\) such that for any model parameter \(\),

\[_{}((f(x_{},),y_{})) {1}{C}_{i=1}^{C}_{}f(x^{i}+^{i},),y^{i} .\]

The above equation suggests that minimizing (e.g., using Adam / SGD) on camouflage samples will also minimize the original-target loss, and thus automatically ensure that the model predicts the correct label on the target. Unfortunately, finding perturbations that satisfy the above is also intractable as the approximate equality is required to hold for all \(\). Building on Geiping et al. (2021), we relax this condition to be satisfied only for a fixed model \(_{}\)-the model trained on the dataset \(S_{}=S_{}+S_{}\), and obtain the perturbations which minimize the cosine-similarity loss given by

\[(,) \] \[=1-_{}(f(x_{}, ),y_{}),_{i=1}^{C}_{}(f(x_{i}+^{i},),y_{i})}{_{}(f(x_{}, ),y_{})_{i=1}^{C}_{}(f(x_{i}+ ^{i},),y_{i})}.\]

**Implementation details.** We minimize (3) using the Adam optimizer (Kingma and Ba, 2015) with a fixed step size of \(0.1\). In order to increase the robustness of camouflage generation, we do \(R\) restarts (where \(R 10\)). In each restart, we first initialize \(\) randomly such that \(\|\|_{}\) and perform \(M\) steps of Adam optimization to minimize \((,_{})\). Each optimization step only requires a single differentiation of the objective \(\) with respect to \(\), and can be implemented efficiently. After each step, we project back the updated \(\) into the constraint set \(\) so as to maintain the property that \(\|\|_{}\). After doing \(R\) restarts, we choose the best round by finding \(_{}\) with the minimum \((_{},_{})\).

## 4 Experimental Evaluation

We generate poison samples by running Algorithm 2 (given in the appendix), and camouflage samples by running Algorithm 1 with \(R=1\) and \(M=250\).7 Each experiment is repeated \(K\) times by setting a different seed each time, which fixes the target image, poison class, camouflage class, base poison images and base camouflage images. Due to limited computation resources, we typically set \(K\{3,5,8,10\}\) depending on the dataset and report the mean and standard deviation across different trials. We say that _poisoning_ was successful if the model trained on \(S_{}=S_{}+S_{}\) predicts the label \(y_{}\) on the target image. Furthermore, we say that _camouflaging_ was successful if the model trained on \(S_{}=S_{}+S_{}+S_{}\) predicts back the correct label \(y_{}\) on the target image, provided that poisoning was successful. A camouflaged poisoning attack is successful if both poisoning and camouflaging were successful.

```
0: Network \(f(\ ;_{})\) trained on \(S_{}+S_{}\), the target \((x_{},y_{})\), Camouflage budget \(C\), perturbation bound \(\), number of restarts \(R\), optimization steps \(M\)
1: Collect a dataset \(S_{}=\{x^{j},y^{j}\}_{j=1}^{C}\) of \(C\) many images whose true label is \(y_{}\).
2:for\(r=1, R\) restarts do
3: Randomly initialize perturbations \(\) s.t. \(\|\|_{}\).
4:for\(k=1,,M\) optimization steps do
5: Compute the loss \((,_{})\) as in (5) using the base camouflage images in \(S_{}\).
6: Update \(\) using an Adam update to minimize \(\), and project onto the constraint set \(\).
7:endfor
8: Amongst the \(R\) restarts, choose the \(_{*}\) with the smallest value of \((_{*},_{})\).
9:endfor
10: Return the poisoned set \(S_{}=\{x^{j}+_{*}^{j},y^{j}\}_{j=1}^{C}\).
```

**Algorithm 1** Gradient Matching to generate camouflages

### Evaluations on CIFAR-10

CIFAR-10 (Krizhevsky, 2009) is a multiclass classification problem with \(10\) classes, with 6,000 color images in each class of size \(32 32\). We follow the standard split into 50,000 training images and 10,000 test images.

#### 4.1.1 Support Vector Machines

In order to perform evaluations on SVM, we first convert the CIFAR-10 dataset into a binary classification dataset (which we term as Binary-CIFAR-10) by merging the \(10\) classes into two groups: animal (\(y=+1\)) and machine (\(y=-1\))). Images (in both training and test datasets) that were originally labeled (_bird, cat, deer, dog, frog, horse_) are relabeled animal, and the remaining images, with original labels (_airplane, cars, ship, truck_), are labeled machine.

We train a linear SVM (no kernel was used) with the hinge loss. We evaluate both label flipping and gradient matching to generate camouflages, and different threat models \((,b_{p},b_{c})\); the results are reported in Table 1. Training details and hyperparameters are given in Appendix B.3. Each poison and camouflage generation took about 40 - 50 seconds (for \(b_{p}=b_{c}=0.2\%\)). Each trained model had validation accuracy of around 81.63% on the clean dataset \(S_{}\), which did not change significantly when we retrained after adding poison samples and/or camouflage samples. Note that the efficacy of the camouflaged poisoning attack was more than \(70\%\) in most of the experiments. We give examples of the generated poisons/camouflages in Appendix B.5.

#### 4.1.2 Neural Networks

We perform extensive evaluations on the (multiclass) CIFAR-10 classification task with various popular large-scale neural networks models including VGG-11, VGG-16 (Simonyan and Zisserman, 2015), ResNet-18, ResNet-34, ResNet-50 (He et al., 2016), and MobileNetV2 (Sandler et al., 2018), trained using cross-entropy loss. Details on training setup and hyperparameters are in Appendix B.3.

We report the efficacy of our camouflaged poisoning attack across different models and threat models \((,b_{p},b_{c})\) in Figure 3; also see Appendix B.3 for detailed results. Each model was trained to have

**Attack type** &  &  \\ \((,b_{p},b_{c})\) & Poisoning & Camouflaging & Clean & Poisoned & Camouflaged \\  LF \((8,0.2\%,0.2\%)\) & 70\% & 71.5\% & 81.63 & 81.73 (\(\) 0.14) & 81.74 (\(\) 0.20) \\ LF \((16,0.2\%,0.2\%)\) & 100\% & 40\% & 81.63 & 81.64 (\(\)0.03) & 81.6 (\(\)0.02) \\ GM \((8,0.2\%,0.4\%)\) & 70\% & 100\% & 81.63 & 81.65 (\(\)0.01) & 81.62 (\(\)0.02) \\ GM \((16,0.2\%,0.4\%)\) & 100\% & 70\% & 81.63 & 81.65 (\(\)0.03) & 81.63 (\(\) 0.02) \\ 

Table 1: Camouflaged poisoning attack on linear SVM on Binary-CIFAR-10 dataset. The first column lists the threat model \((,b_{p},b_{c})\) and the camouflaging type “LF” for label flipping and “GM” for gradient matching. The implementation details are given in Appendix B.3.

validation accuracy between 81-87% (depending on the architecture), which changed minimally when the model was retrained with poisons and/or camouflages. Camouflaging was successful at least 70% of the time for VGG-11, VGG-16, Resnet-18, and Resnet-34 and about \(30\%\) of the time for MobileNetV2 and Resnet-50.

Note that even a small attack success rate represents a credible threat: first, these attack success rates are comparable with those of Geiping et al. (2021), and thus future or undisclosed targeted data poisoning attacks are likely to be more effective. Second, since these success rates are over the choice of the attacker's random seed, the attacker can locally repeat the attack several times and deploy a successful one. Therefore, low success rates do not imply a fundamental barrier, and can be overcome by incurring a computational overhead - by repeating an attack 10x, even an attack with only 20% success rate can be boosted to \( 90\%\) success rate.

### Evaluations on Imagenette and Imagewoof

We evaluate the efficacy of our attack vector on the challenging multiclass classification problem on the Imagenette and Imagewoof datasets Howard (2019), both of which are subsets consisting of 10 classes from the Imagenet dataset Russakovsky et al. (2015) with about 900 images per class.

We evaluate our camouflaged poisoning attack on two different neural network architectures-VGG-16 and ResNet-18 (trained with cross entropy loss), and different threat models \((,b_{p},b_{c})\) listed in Table 2. More details about the dataset, experiment setup and hyperparameters are provided in Appendix B.4. In our experiments, camouflaging was successful for at least \(50\%\) of the time when poisoning was successful. However, because we modified about 13% of the training dataset when adding poisons/camouflages, we observe that the fluctuation in the model's validation accuracy can be up to 7%, which is expected since we make such a large change in the training set.

### Additional Experiments

**Ablation experiments.** In the appendix, we provide the additional experiments on CIFAR-10, showing that:

* Our attack is robust to random deletions of generated poison and camouflage samples during evaluation (Appendix B.6.2), and to training with data augmentation.
* Our attack successfully transfers across different models, i.e., when the victim model is different from the model on which poison and camouflage samples were generated. (Appendix B.6.3)

  &  &  &  \\  & & \(\) & \(b_{p}\) & \(b_{c}\) & Poison & Camou \\  IN & VGG-16 & 16 & 6.3\% & 6.3\% & 25\% & 100\% \\ IN & Resnet-18 & 16 & 6.3\% & 6.3\% & 40\% & 50\% \\ IW & Resnet-18 & 16 & 6.6\% & 6.6\% & 50\% & 75\% \\ 

Table 2: Evaluation of camouflaged poisoning attack on Imagenette (IN) and Imagewoof (IW) datasets over \(5\) seeds (with 1 restart per seed). Note that camouflaging succeeded in most of the experiments in which poisoning succeeded.

Figure 3: Efficacy of the proposed camouflaged poisoning attack on CIFAR-10 dataset. The left plot gives the success for the threat model \(=16,b_{p}=0.6\%,b_{c}=0.6\%\) for different neural network architectures. The right plot gives the success for ResNet-18 architecture for different threat models.

* Our attack is successful across different threat models i.e., different values of \(b_{p}\) and \(b_{c}\) (Appendix B.6.1).
* The poison and camouflage samples generated in our experiments have similar feature space distributions, and thus data sanitization defenses (e.g., Diakonikolas et al. (2019)) based on feature distributions will not succeed in removing the generated poison and camouflage samples (Appendix B.6.5).

**Approximate unlearning.** In all our experiments so far, the victim retrains a model from scratch (on the leftover training data) to fulfil the unlearning request, i.e., the victim performs exact unlearning. In the past few years there has been significant research effort in developing approximate unlearning algorithms under various structural assumptions (e.g., convexity (Sekhari et al., 2021; Guo et al., 2020)), or under availability of large memory (Bourtoule et al., 2021; Brophy and Lowd, 2021; Graves et al., 2021), etc.), and one may wonder whether attacks are still effective under approximate unlearning. Unfortunately, most existing approximate unlearning approaches are not efficient (either requiring strong assumptions or taking too much memory) for large-scale deep learning settings considered in this paper, and thus we were unable to evaluate our attack against these methods with our available resources. However, we provide evaluations against the Amnesiac Unlearning algorithm of Graves et al. (2021), which we were able to implement ( Appendix B.6.6). We note that our attack continues to be effective even against this approximate unlearning method.

## 5 Discussion and Conclusion

We demonstrated a new attack vector, _camouflaged poisoning attacks_, against machine learning pipelines where training points can be _unlearned_. This shows that as we introduce new functionality to machine learning systems, we must be aware of novel threats that emerge. We outline a few extensions and future research directions:

* Our method for generating poison and camouflage points was based on the gradient-matching attack of Geiping et al. (2021). However, the attack framework could accommodate effectively any method for targeted poisoning, e.g., the methods of Aghakhani et al. (2021) or Huang et al. (2020), or any method introduced in the future. We provide preliminary experiments in Appendix C of camouflaged-poisoning attacks developed using the Bullseye Polytope technique from Aghakhani et al. (2021), and show that the attack continues to succeed.
* While we only performed experiments with a single target point, similar to Geiping et al. (2021) (and several other works in the targeted data poisoning literature), the proposed attack extends straightforwardly to a collection of targets (rather than a single target). This can be done by calculating the gradient of multiple (instead of a single) target images and using it in the gradient matching in (5) and (3).
* In order to generate poisons and camouflages, our approach needs the ability to query gradients of a trained model at new samples, thus the attack is not black-box. While the main contribution of the paper was to expose that machine unlearning (which is a new and emerging but still under-developed technology) can lead to a new kind of vulnerability called a camouflaged poisoning attack, performing such an attack in a black-box setting is an interesting research question.
* While we could not verify our attack against other approximate unlearning methods in the literature (Bourtoule et al., 2021; Brophy and Lowd, 2021; Sekhari et al., 2021; Guo et al., 2020) due to lack of resources needed to implement them, we believe that our attack should be effective against any provable approximate unlearning approach. This is because the objective of approximate unlearning is to output a model that is statistically-indistinguishable from the model retrained-from-scratch on the remaining data, and the experiments provided in the paper directly evaluate on the retrained-from-scratch model (which is the underlying objective that all approximate unlearning methods aim to emulate). Verifying the success of our approach against other approximate unlearning approaches is an interesting future research direction.
* We considered a two-stage process in this paper, with one round of learning and one round of learning. It would also be interesting to explore what kinds of threats are exposed by even more dynamic systems where points can be added or removed in an online fashion.
* Finally, it is interesting to determine what other types of threats can be camouflaged, e.g., indiscriminate (Lu et al., 2022) or backdoor poisoning attacks (Chen et al., 2017; Saha et al., 2020). Beyond exploring this new attack vector, it is also independently interesting to understand how one can neutralize the effect of an attack by _adding_ points.

#### Acknowledgements

We thank Chirag Jindal for useful discussions in the early phase of the project. AS thanks Karthik Sridharan for helpful discussions. GK is supported by a University of Waterloo startup grant, a Canada CIFAR AI Chair, an NSERC Discovery Grant, and an unrestricted gift from Google. JA is supported in part by the grant NSF-CCF-1846300 (CAREER), NSF-CCF-1815893, and a Google Faculty Fellowship. Computational resources were provided in part by GK's Resources for Research Groups grant from the Digital Research Alliance of Canada.