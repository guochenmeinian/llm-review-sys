# On the Parameter Identifiability of Partially Observed Linear Causal Models

Xinshuai Dong\({}^{1}\)* Ignavier Ng\({}^{1}\)* Biwei Huang\({}^{2}\) Yuewen Sun\({}^{3}\) Songyao Jin\({}^{3}\)

Roberto Legaspi\({}^{4}\) Peter Spirtes\({}^{1}\) Kun Zhang\({}^{1,3}\)

\({}^{1}\)Carnegie Mellon University \({}^{2}\)University of California San Diego

\({}^{3}\)Mohamed bin Zayed University of Artificial Intelligence \({}^{4}\)KDDI Research

###### Abstract

Linear causal models are important tools for modeling causal dependencies and yet in practice, only a subset of the variables can be observed. In this paper, we examine the parameter identifiability of these models by investigating whether the edge coefficients can be recovered given the causal structure and partially observed data. Our setting is more general than that of prior research--we allow all variables, including both observed and latent ones, to be flexibly related, and we consider the coefficients of all edges, whereas most existing works focus only on the edges between observed variables. Theoretically, we identify three types of indeterminacy for the parameters in partially observed linear causal models. We then provide graphical conditions that are sufficient for all parameters to be identifiable and show that some of them are provably necessary. Methodologically, we propose a novel likelihood-based parameter estimation method that addresses the variance indeterminacy in a specific way and can asymptotically recover the underlying parameters up to trivial indeterminacy. Empirical studies on both synthetic and real-world datasets validate our identifiability theory and the effectiveness of the proposed method in the finite-sample regime. Code: https://github.com/dongxinshuai/scm-identify.

## 1 Introduction and Related Work

Causal models, which serve as a fundamental tool to capture causal relations among random variables, have achieved great success in many fields . A fundamental problem in the field is how and to what extent can we identify the underlying causal model given observational data. When all variables are observed, the problem has been well studied: the underlying structure can be identified up to the Markov equivalence class, e.g., by the PC  or GES  algorithm; when the structure is given, the causal coefficient (direct causal effect) between two variables can also be identified .

However, in real-world systems, the variables of interest may only be partially observed. Thus, considerable efforts have been dedicated to identification of causal models in the presence of latent variables. One line of research focuses on structure learning given partially observed variables. Notable approaches include FCI and its variants , as well as ICA-based , tetrad-based , high-order moments-based , and rank constraint-based  methods.

In this paper, we focus on the the identification of parameters of a partially observed model. Specifically, given the causal structure of and observational data from a partially observed causal model, we are interested in identifying all the parameters, and thus the underlying causal model can be fully specified. To identify the parameters, a classical way is to project the directed acyclic graph (DAG) with latent variables to an acyclic directed mixed graph (ADMG) or partially ancestral graph , without explicitly modeling the latent confounders. Based on ADMG, graphical criteria such as half-trek , G-criterion , and some further developments  have been proposed to establish the parameter identifiability. Another way is to leverage do-calculus, proxy variables, and instrumentalvariables [47; 39; 25] to identify the direct causal effect, which corresponds to the edge coefficient in linear causal models. For a more detailed discussion of related work, please refer to Appendix D.

Despite the effectiveness of current methods for parameter identification, however, they have two main drawbacks: they require all the variables to be connected in specific ways, and only focus on identifying the edge coefficients between observed variables. To this end, in this paper we propose a novel framework that considers a more general setting for parameter identification. To be specific, we allow all variables, including both observed and latent ones, to be flexibly related, and we aim to recover the edge coefficients among all variables, even including those from a latent variable to another latent variable or another observed variable, which previous methods cannot handle. We summarize our contributions as follows.

* To the best of our knowledge, we are the first to consider parameter identifiability of partially observed causal model in the most general scenario--all variables, including both observed and latent ones, are allowed be flexibly related, and edge coefficients between any pair of variables are concerned. In contrast, most existing works consider only the edges between observed variables.
* Theoretically, we identify three types of parameter indeterminacy in partially observed linear causal models. We then provide graphical conditions that are sufficient for all parameters to be identifiable and show that some of them are provably necessary. These necessary conditions also offer insights into scenarios where the parameters are guaranteed to be non-identifiable.
* Methodologically, we propose a novel likelihood-based parameter estimation method, which parameterizes population covariance in specific ways to address variance indeterminacy. Our empirical studies on both synthetic and real-world data validate the effectiveness of our method in the finite-sample regime, even under certain misspecification of the underlying causal model.

## 2 Preliminaries

### Problem Setting

In this work, we focus on partially observed linear causal models, defined as follows.

**Definition 1** (Partially Observed Linear Causal Models).: _Let \(:=(_{},_{})\) be a DAG. Each variable \(V_{i}_{}\) follows a linear structural equation model \(_{i}=_{_{j}_{}}( _{i})}f_{j,i}_{j}+_{_{i}}\), where \(_{}:=_{}_{}=\{_{i}\}_{i=1}^{m}\{_{i}\}_{i=m+1}^{m+n}\) contains \(m\) latent variables and \(n\) observed variables. \(_{}(_{i})\) denotes the parent set of \(_{i}\), \(f_{j,i}\) denotes the edge coefficient from \(V_{j}\) to \(V_{i}\), and \(_{_{i}}\) represents the Gaussian noise term of \(_{i}\)._

We drop the subscript \(\) in \(_{}\) and \(_{}\) when the context is clear. We use \(\), \(\), and \(\) to denote a random variable, a set of variables, and a set of sets of variables, respectively. In Definition 1, the relations between variables can also be written in the matrix form as \(_{}=F^{T}_{}+_{_{ }}\), where \(F=(f_{j,i})_{i,j[m+n]}\) is the weighted adjacency matrix. Here, \(f_{j,i} 0\) if and only if \(V_{j}\) is a parent of \(V_{i}\) in \(\). We also write

\[F=&\\ &F_{}&F_{}\\ F_{}&F_{}&& =_{_{}}&0\\ 0&_{_{}},\]

where \(\) is the diagonal covariance matrix of \(_{_{}}\).

Our objective is to identify \(F\), the causal edge coefficients of the model, given observational data and the causal structure \(\). Denote by \(_{}\) and \(_{}\) the population covariance matrix of latent variables \(\) and observed variables \(\), respectively; their precise formulations are provided in Proposition 1. We also denote by \(_{i,j}\) the \((i,j)\)-th entry of \(_{}\). In this work, we assume that the noise terms of latent variables, \(_{}\), have unit variance, i.e., \(_{_{}}=I\), which will be justified later in Section 3.1. Note that variables are partially observed and thus we only have access to i.i.d. samples of observed variables \(\). As variables are jointly Gaussian, the observations can asymptotically be summarized as the population covariance matrix \(_{}\). In other words, we aim to identify \(F\) and \(\) given \(_{}\) and \(\). The identification of parameters is important in that, once we identify the parameters, the underlying causal model is fully specified, and thus we can flexibly calculate causal effects, infer interventional distributions, and finally answer counterfactual queries . It is worth noting that, for parameter identification, the structure \(\) is assumed to be known, which is different from the setting of causal discovery where the goal is to identify \(\) from data.

### Framework Comparison

Without latent variables, it has been shown all parameters are identifiable . However, the problem becomes very challenging when latent variables exist. There are two lines of research. One focuses on the use of do-calculus, proxy variables, and instrumental variables to identify direct causal effects among observed variables [47; 39; 25] (in linear models the direct causal effect is captured by the edge coefficient). Another line addresses latent confounders by projecting a DAG with latent variables into an ADMG, where the confounding effects of latent variables are simplified and represented by correlation among noise terms [20; 9; 51; 29]. An example is in Figure 1, where (a) is the original graph and (c) is the projected ADMG whose bidirected edges correspond to correlated noise terms.

Compared to the two previous lines of thought, our framework has two advantages. To begin with, we additionally considers the identifiability of coefficients of edges that involve latent variables. For example, in Figure 1, we aim to identify all the coefficients including the one from L\({}_{1}\) to X\({}_{3}\), i.e., \(f_{1,3}\). In contrast, the proxy variable framework and the latent projection framework identify only the coefficients among observed variables: the proxy variable framework focuses only on the direct causal effect from one observed variable to another observed variable, while the latent projection framework transforms all latent variables into bidirected edges and thus can never identify the coefficient of the edge that has a latent variable as the head or tail.

Furthermore, the projection framework deals with latent variables in a rather brute-force way: dense latent confounding effects among observed variables may be caused by only a small number of latent variables, but that information is lost during projection. For example, in Figure 1, (a) and (b) share the same ADMG after projection, i.e., (c). However, as we will show later, parameters in (a) can be identified, while in (b) the parameters cannot. If we only consider the ADMG in (c), then we can never capture this nuance and thus cannot identify the coefficients that we might be able to.

## 3 Identifiability Theory

### Definition of Parameter Identifiability and Indeterminacy

We follow the notion of generic identifiability and define parameter identifiability as follows.

**Definition 2** (Identifiability of Parameters of Partially Observed Linear Causal Models).: _Let \(=(F,)\). We say that \(\) is generically identifiable, if the mapping \(()=_{}\) is injective, for almost all \(\) with respect to the Lebesgue measure._

Definition 2 indicates if parameter \(\) is identifiable, then there does not exist \(^{}\) that entails the same observations as those of \(\). As in the typical literature of parameter identification, we consider generic identifiability to rule out some rare cases where the parameters for that structure is generally identifiable, but with some specific parameterization, the parameters cannot be identified. This is similar to faithfulness in causal discovery  and we will provide an example in Example 1. We next introduce three important indeterminacies about parameter identification when latent variables exist.

**Theorem 1** (Indeterminacy of Scaling of \(_{_{}}\)).: _Consider a model that follows Def. 1 with number of latent variables \(m 1\) and \(=(F_{},F_{},F_{},F_{}, _{_{}},_{_{}})\). Let \(\) be any invertible diagonal matrix, and \(=(_{},_{},_{ },_{},_{_{}}, _{_{}})\), where_

\[_{}=^{-1}F_{},\ _{ }=^{-1}F_{},\ _{}=F_{},\ _{}=F_{},\ _{_{}}= ^{2}_{_{}},\ _{_{}}=_{ _{}}.\]

_Then, \(\) and \(\) entail the same observations, i.e., \(_{}=_{}\). Furthermore, we have \(_{}=_{}\)._

Figure 1: Illustrations of the advantage of our framework. Within our framework, it can be shown that \(_{1}\)’s parameters can be identified (up to sign) while \(_{2}\)’s cannot. In contrast, the latent projection framework cannot even differentiate \(_{1}\) from \(_{2}\) as they share the same ADMG (c) after projection. Furthermore, with ADMG, any edge coefficient that involves a latent variable cannot be considered.

A similar theoretical result is provided in , and yet our setting is much more general and takes that of  as a special case: in our setting, all variables including latent and observed ones can be arbitrarily related while in  latent variables cannot be the effect of observed variables.

**Remark 1** (Implication of Theorem 1).: _A key implication of Theorem 1 is that, without further assumption, the edge coefficients involving latent variables, i.e., \((F_{},F_{},F_{})\), can never be identified, as there always exists a diagonal matrix \(\) such that \(\) and \(\) entail the same observations but \((_{},_{},_{}) (F_{},F_{},F_{})\). Thus, in the rest of this paper, we assume that the noise terms of latent variables, \(_{}\), have unit variance, i.e., \(_{_{}}=I\). Under this assumption, we have \((_{_{}})_{i,i}=_{i,i}^{2}(_{ _{}})_{i,i}=1,i[m]\), which implies \(_{i,i}= 1\). As such, this assumption makes parameter identifiability possible. However, even though we fix the scaling of \(_{_{}}\), there still exists indeterminacy about the sign of parameters, captured by Theorem 2._

**Theorem 2** (Group Sign Indeterminacy).: _Consider a model that follows Def. 1 with number of latent variables \(m 1\), \(=(F_{},F_{},F_{},F_{}, _{_{}},_{_{}})\), and \(_{_{}}=I\). Let \(S\) be a diagonal sign matrix (entries are either \(1\) or \(-1\)), and \(=(_{},_{},_{ },_{},_{_{}}, _{_{}})\), where_

\[_{}=SF_{}S,\;_{}=SF_{ },\;_{}=F_{}S,\;_{}=F_{},\;_{_{}}=_{ _{}}=I,\;_{_{}}=_{ _{}}.\]

_Then, \(\) and \(\) entail the same observations, i.e., \(_{}=_{}\), and \((_{})_{ii}=(_{})_{ii},\; i[m]\)._

**Remark 2** (Remark on Theorem 2).: _The indeterminacy described in Theorem 2 is referred to as group sign indeterminacy for the following reason: According to the theorem, flipping the sign of \(S_{i,i}\) is equivalent to flipping the signs of all coefficients of edges involving the latent variable \(_{i}\). This transformation preserves the resulting observations \(_{}\). In essence, each group consists of coefficients of edges involving a particular latent variable._

**Example 1** (Example for Group Sign Indeterminacy and Generic Identifiability).: _In Figure 2 (b), given the structure and \(_{}\), by assuming \(_{_{}}=I\), the parameters are generally identifiable up to group sign indeterminacy. Specifically, there exist three equality constraints with three free parameters: \(f_{1,2}f_{1,3}=_{2,3}\), \(f_{1,2}f_{1,4}=_{2,4}\), and \(f_{1,3}f_{1,4}=_{3,4}\). The solutions are: (i) \(f_{1,2}=_{2,4}}{_{3,4}}}\), \(f_{1,3}=_{2,3}/f_{1,2}\), \(f_{1,4}=_{2,4}/f_{1,2}\) and (ii) \(f_{1,2}=-_{2,4}}{_{3,4}}}\), \(f_{1,3}=-_{2,3}/f_{1,2}\), \(f_{1,4}=-_{2,4}/f_{1,2}\). The two solutions are different only in terms of group sign. However, if we set \(f_{1,2}=0\), then the parameters are not identifiable (as we will encounter division where the divisor is zero). These rare cases of parameters are of zero Lebesgue measure so we rule out these cases for the definition of identifiability, as in Definition 2._

Intuitively speaking, group sign indeterminacy arises because one may multiply the latent variable \(_{i}\) by \(-1\) and accordingly flip the signs of all edge coefficients involving \(_{i}\). Note that such an indeterminacy is rather minor for the following reason. (i) In practice, we can always anchor the sign of some edges according to our preference or prior knowledge in order to eliminate the group sign indeterminacy. For example, in Figure 4, if we expect that L2 should be understood as Extraversion instead of non-Exterversion, we can add one additional constraint during our parameter estimation such that the edge coefficient from L2 to E1 ("I am the life of party.") will be positive (as we believe E1 should be positively related to Extraversion). (ii) On the other hand, there are some application scenarios that are not influenced by the group sign indeterminacy, such as causal effect estimations between certain variables. We note that, as the indeterminacy of group sign is rather minor, in the following if the parameters are identifiable only up to group sign indeterminacy, we still say that the parameters are identifiable.

Figure 2: Illustrative examples to show that the graphical condition for structure-identifiability and parameter-identifiability could be very different.

**Definition 3** (Orthogonal Transformation Indeterminacy).: _Consider a model that follows Def. 1 with number of latent variables \(m 1\), \(=(F_{},F_{},F_{},F_{},_{ _{}},_{_{}})\), and \(_{_{}}=I\). We say that there exists an orthogonal transformation indeterminacy in the identification of parameters if there exists a non-diagonal orthogonal matrix \(Q\) such that \((F_{},F_{},F_{},F_{},_{ _{}},_{_{}})\) and \((_{},_{},_{}, _{},_{_{}},_{_{}})\) share the same support and entail the same observations, where_

\[_{}=Q^{T}F_{}Q,\;_{}=Q^{T} F_{},\;_{}=F_{}Q,\;_{ }=F_{},\;_{_{}}=I,\; _{_{}}=_{_{}}.\]

The orthogonal transformation indeterminacy is the major indeterminacy we consider in the presence of latent variables. Such an indeterminacy also arises in factor analysis [45; 7], which can be viewed as a special case of the data generating procedure considered in Definition 1. Here we only give the definition and will later provide Theorem 4 with an example that captures the scenarios where such indeterminacy exists.

It is worth noting that the graphical condition for structure identifiability and parameter identifiability could be very different. For example, \(_{1}\) in Figure 2 (a) is structure-identifiable, and yet the parameters are not identifiable even if the structure is given. In contrast \(_{2}\) in Figure 2 (b) is not structure-identifiable, as there exists another structure \(_{3}\) in Figure 2 (c) such that \(_{2}\) and \(_{3}\) can never be differentiated from observational distribution; and yet if \(_{2}\) is given, its parameters are identifiable (as in Example 1). Therefore, in this paper, we first consider the cases where the structure can be identified and then study which further conditions are needed for the identifiability of parameters. This will give rise to conditions under which the whole causal model can be fully specified.

### Graphical Condition for Structure Identifiability

To explore the conditions for the whole causal model to be specified, we start with the structure identifiability of partially observed linear causal models. Recent advances have shown that if certain graphical conditions are satisfied [24; 18], even though all variables including latent ones are allowed to be very flexibly related, the causal structure can still be identified. Next, we focus on the conditions by , which takes that of  as special cases. Roughly speaking, the identifiability of the structure of a partially observed linear causal model is built upon the identifiability of atomic covers, defined as follows (with _effective cardinality_ defined as \(||||=|(_{})|\) and \(_{}\) defined in Appendix B.2).

**Definition 4** (Atomic Cover ).: _Let \(_{}\) be a set of variables, where \(l\) out of \(||\) are latent, and the remaining \(||-l\) are observed. \(\) is an atomic cover if \(\) is a single observed variable, or if the following conditions hold:_

1. _There exists a set of atomic covers_ \(\)_, with_ \(|||| l+1\)_, such that_ \(_{}_{}( )\)_._
2. _There exists a set of covers_ \(\) _with_ \(|||| l+1\)_, s.t._ \((_{})(_{})=\)_, every element in_ \(_{}\) _is a neighbour of every element in_ \(\)_, and_ \(\) _d-separates_ \(\) _and_ \(\)_._
3. _There does not exist a partition_ \(\) _of_ \(\)_, s.t., all elements in_ \(\) _are atomic covers._

The intuition that we build structure identifiability upon the notion of atomic covers is as follows. When a set of latent variables share the same set of children and neighbors, it is impossible to differentiate these latent variables from each other, and thus we need to consider them together as the minimal identifiable group to build up the identifiability of the whole structure. Such a minimal identifiable group of variables is defined as an atomic cover. Roughly, for a group of variables to be qualified as an atomic cover, it has to have enough children and neighbors. An example is as follows.

**Example 2** (Example of Atomic Cover).: _Consider the graph in Fig. 3. \(=\{_{1},_{}\}\) is an atomic cover. This is because there exist \(=\{\{_{}\},\{_{}\}\}\) with \(|||| l+1=2\) such that (i) in Def. 4 is satisfied. And there exist \(=\{\{_{}\},\{_{}\}\}\) (or, \(=\{\{_{}\},\{_{}\}\}\)) with \(|||| l+1=2\) such that (ii) in Def. 4 is satisfied. We can also find that (iii) in Def. 4 is satisfied. Thus \(\{_{1},_{}\}\) is an atomic cover. Another example would be in Figure 8, where \(\{_{1},_{2}\}\) is an atomic cover._

Figure 3: An illustrative graph that satisfies the conditions for structure-identifiability. At the same time, it also satisfies the condition for parameter identifiability - given the structure and \(_{}\), all the parameters are identifiable only up to group sign indeterminacy.

**Condition 1** (Basic Conditions for Structure Identifiability ).: \(\) _satisfies the basic graphical condition for identifiability, if every latent variable belongs to at least one atomic cover in \(\) and for each atomic cover with latent variables, any of its children is not adjacent to any of its neighbours._

**Condition 2** (Condition on Colliders ).: _In \(\), if (i) there exists sets of variables \(\), \(}\), \(}\), and \(\) such that every variable in \(\) is a collider of two atomic covers \(}\), \(}\), and \(\) is a minimal set of variables that d-separates \(}\) from \(}\), and (ii) there exists at least one latent variable in \(}}\), then we must have \(||+|||}|+|}|\)._

**Example 3** (Example that satisfies Conditions 1 and 2).: _Consider Figure 3. All latent variables in the graph belong to at least one atomic cover and thus Condition 1 is satisfied. Plus, Condition 2 is also satisfied. This is because the sets of variables \(\), \(}\), \(}\), and \(\) that satisfy (i) and (ii) in Condition 2 are \(=\{_{12}\}\), \(}=\{},_{6}\}\), \(}=\{}\}\), and \(=\{_{4},_{5}\}\), and we also have \(||+|||}|+|}|\). Therefore, the graph in Figure 3 satisfies both Conditions 1 and 2._

The identifiability theory of structure is as follows. For a graph \(\), if Condition 1 and Condition 2 are satisfied, then asymptotically the structure is identifiable up to the Markov equivalence class (MEC) of \(_{a}(_{s}())\) (definitions of \(_{a}()\) and \(_{s}()\) can be found in Appendix B.4). Roughly speaking, the underlying causal structure of \(\) can be identified except that the directions of some edges cannot be determined. Next, we will show that, given any DAG in the identified equivalence class together with \(_{}\), the parameters of the model are also identifiable, if certain conditions are satisfied.

### Identifiability of Parameters

In this section we show that, given graphical Conditions 1 and 2, the causal coefficients \(F\) in Definition 1 are also identifiable, if certain conditions are satisfied.

**Theorem 3** (Sufficient Condition for Parameter Identifiability (up to group sign), Based on Structure Identifiability).: _Assume that \(\) satisfies Conditions 1 and 2 and thus the structure can be identified up to the MEC of \(_{a}(_{s}())\). For any DAG in the equivalence class, the parameters are identifiable, if both the following hold:_

1. _For any atomic cover_ \(=\)_,_ \(|| 1\)_._
2. _If an atomic cover_ \(=\) _satisfies_ \(|| 0\) _and_ \(|| 1\)_, then all simple treks (Def._ 5_) between_ \(\) _and_ \(\) _do not contain any latent variables that are not in_ \(\)_._

Theorem 3 provides a sufficient condition such that the parameters are identifiable. Now, for a better understanding of Theorem 3, we provide an example of it as follows.

**Example 4** (Example for Theorem 3).: _The graph \(\) in Figure 3 satisfies the conditions for parameter identifiability in Theorem 3. Specifically, condition (i) in Theorem 3, is satisfied as all atomic covers contain no more than one latent variable. Plus, condition (ii) in Theorem 3 is also satisfied, as the atomic cover \(==\{}\}\{_{6}\}\) satisfies \(|| 0\) and \(|| 1\) and all simple treks between \(\{}\}\) and \(\{_{6}\}\) contain only observed variables except \(\{}\}\). Therefore, the parameters are identifiable for the graph in Figure 3._

Next, we discuss under which conditions the parameters are guaranteed to be not identifiable. As discussed in Section 3.1, there are three kinds of indeterminacy. The first one can be solved by assuming unit variance of the noise terms of latent variables while the second one group sign indeterminacy is rather trivial such that we still consider parameters as identifiable even if group sign indeterminacy exists. Therefore, we will focus on the third one, orthogonal transformation indeterminacy, in what follows.

**Theorem 4** (Condition for the Existence of Orthogonal Transformation Indeterminacy).: _Consider the model in Definition 1. If a set of latent variables \(\) with \(|| 2\), have the same parents and children, then there must exist orthogonal transformation indeterminacy regarding the edge coefficients \(F\). In other words, \(F\) can at most be identified up to orthogonal transformation indeterminacy._

**Example 5** (Example for Thm. 4).: _Consider Fig. 8. The graph satisfies the conditions in Thm. 4 as the parents and children of \(}\) and \(}\) are exactly the same. Therefore, there must exist orthogonal transformation indeterminacy for the edge coefficients \(F\) and thus the parameters are not identifiable._

The Theorem 4 above indicates that, if there exist two latent variables that share the same parents and children, then the edge parameters can at most be identified up to orthogonal transformation. This directly implies a necessary condition for parameter identifiability as follows.

**Corollary 1** (General Necessary Condition for Parameter Identifiability).: _For parameters to be identifiable, every pair of latent variables has to have at least one different parent or child._Corollary 1 captures a necessary condition in the general cases such that parameters are identifiable. If we further consider the graphs that are also structure identifiable (as we need to identify the structure first to fully specified the causal model), we further have the following Corollary 2 by considering the notion of atomic covers (the proofs of both corollaries can be found in the Appendix).

**Corollary 2** (Necessary Condition about Atomic Covers for Parameter Identifiability).: _Assume \(\) satisfies Conditions 1 and 2 and thus the structure can be identified up to the MEC of \(_{a}(_{s}())\). For any DAG \(\) in the equivalence class, for \(\)'s parameters to be identifiable, every atomic cover must contain no more than one latent variable._

**Remark 3** (Necessity of Conditions in Theorem 3).: _Condition (i) in Theorem 3 is provably necessary: by Corollary 2, for parameters to be identifiable, one has to assume (i) in Theorem 3._

Establishing a necessary and sufficient condition is always highly non-trivial in various tasks. For example, for the identification of linear non-Gaussian causal structure with latent variables, researchers initially developed sufficient conditions with three pure children in , later relaxed to two in , before ultimately achieving both necessary and sufficient conditions in . Similarly, for parameter identification, although the condition we proposed is not a necessary and sufficient one, it could serve as a stepping stone towards tighter and ultimately the necessary and sufficient condition for the field.

Below, we also provide a sufficient condition for parameter identifiability that does not rely on structure identifiability in Theorem 5. It is particularly useful when the structure is directly given by some domain experts.

**Theorem 5** (Sufficient Condition for Parameter Identifiability (up to group sign) without Requiring Structure Identifiability).: _In \(\), if for every latent variable \(\) there always exist another three distinct variables (which can be latent or observed), such that two of the three are pure children of \(\) and the rest one is a neighbor of \(\), then the parameters are identifiable._

Identifiability theory often focuses on the asymptotic case, i.e., we assume that we know the structure and the population covariance matrix \(_{}\). However, in practice, we only have access to i.i.d. data with finite sample size and thus only have the sample covariance matrix. Therefore, in the next section, we will propose a novel method to estimate the parameters in the finite sample cases.

## 4 Parameter Estimation Method

### Objective

Our goal is to estimate \(F\) in Definition 1, given the causal structure \(\) and observational data. The key is to parameterize the population covariance \(_{}\) using \(=(F,)\) and then maximize the likelihood of observed sample covariance \(_{}\). To make this technically precise, we provide a closed-form expression of \(_{}\) in terms of \(\) in the following proposition, with a proof given in Appendix A.7.

**Proposition 1** (Parameterization of Population Covariance).: _Consider the model defined in Def. 1. Let \(MI-F_{}-F_{}(I-F_{ })^{-1}F_{}^{-1}\) and \(N(I-F_{})F_{}^{-1}(I-F_ {})-F_{}^{-1}\). Then, the population covariance matrices of \(\) and \(\) can be formulated as_

\[_{}= M^{T}_{_{}}M+N^{T}_{_{ }}N,\] (1) \[_{}= (I-F_{})^{-T}F_{ }^{T}_{_{}}F_{}+_{ _{}}NF_{}+F_{}^{T}N^{ T}_{_{}}(I-F_{})^{-1}.\] (2)

The formulations of \(_{}\) and \(_{}\) are rather complicated due to the general scenario we considered, i.e., latent variables can be the cause or the effect of latent and observed variables. That is, the submatrices \(F_{},F_{},F_{}\) and \(F_{}\) defined in the above proposition can all have nonzero entries. In most existing works, at least one of these submatrices are assumed to be zero. For instance, factor analysis assumes that \(F_{},F_{}\) and \(F_{}\) are zero, while  assumes that \(F_{}\) and \(F_{}\) are zero. Furthermore, Proposition 1 also provides insight into the indeterminacy involved when identifying the parameters, such as the indeterminacy of variance in Theorem 1 and the orthogonal transformation indeterminacy in Theorem 4.

Similar to factor analysis , we assume \(_{}\) are Gaussian and thus \(\) are jointly Gaussian. Thus, the negative log-likelihood of observational data can be formulated as

\[=(K/2)(((_{})^{-1}_{ })+_{}),\] (3)

where \(K\) is the number of i.i.d. observations. With the parameterized negative log-likelihood, we estimate the edge coefficients by minimizing the negative log-likelihood, as \[,=_{F,}\ ,_{ _{}}=I,\] (4)

where the entries of matrix \(F\) that do not correspond to an edge in \(\) are constrained to be zero during the optimization.

Note that in Eq. (4) the constraint that the noise terms of latent variables have unit variance is crucial to deal with the variance indeterminacy defined in Theorem 1. In practice, it is also favorable to use another constraint to address the variance indeterminacy, i.e., the constraint that all the latent variables have unit variance. This leads to an alternative objective as

\[,=_{F,}\ ,( _{})_{ii}=1,\ i[m],\] (5)

where the entries of \(F\) that do not correspond to an edge in \(\) are also constrained to be zero.

Both objectives in Eqs. (4) and (5) can be employed, and yet using the second one gives rise to edge coefficients that are easier to understand. To be concrete, if we normalize all observed variables to have unit variance, then using Eq. (5) would give rise to \(\) such that \(-1_{i,j} 1, i,j[m]\). An example can be found in Figure 4. However, it may not be straightforward to realize the constraint in Eq. (5). To this end, in the next section we introduce a way to parameterize \(_{}\) using \(F\), such that the required constraint in Eq. (5) can be automatically satisfied. Later in Section 5.2, we also empirically compare the performance of using Eq. (4) with that of using Eq. (5).

### Parameterization Trick of Covariance Matrix

In this section, we introduce how trek rules can be employed to parameterize \(_{}\) while the unit variance constraint on latent variables in Eq. (5) can be elegantly satisfied. We start with the definition of trek. For readers who are less familiar with trek, please refer to Appendix B.1 for examples.

**Definition 5** (Treks ).: _In \(\), a trek from \(\) to \(\) is an ordered pair of directed paths \((P_{1},P_{2})\) where \(P_{1}\) has a sink \(\), \(P_{2}\) has a sink \(\), and both \(P_{1}\) and \(P_{2}\) have the same source \(\), i.e., \((P_{1},P_{2})=\). A Trek is simple if \(P_{1}\) and \(P_{2}\) have no intersection except their common source \(\)._

At this point, we are able to parameterize each entry of \(_{}\) using (\(F,\{_{ii}\}_{i=1}^{n+m}\)), instead of (\(F,\)), by making use of the (simple) trek rule , as follows:

\[_{ij}=_{P_{1},P_{2}(_{i},_{j})} _{(P_{1},P_{2})}f^{P_{1}}f^{P_{2}},\] (6)

where \((_{i},_{j})\) is the set of all simple treks between \(_{i}\) and \(_{j}\), and \(f^{P}\) is the path monomial along \(P\) defined as \(f^{P}:=_{k l P}f_{kl}\).

By this form of parameterization, we can simply set all entries of \(\{_{ii}\}_{i=1}^{n+m}\) as 1 (which is equivalent to requiring all variables to have unit variance), such that the constraint in Eq. (5) can be automatically satisfied. For a better understanding of how to use the simple trek rule for parameterization, we provide an example as follows.

**Example 6** (Example for Parameterization using Simple Trek).: _In Figure 7 (a), there are four simple treks between \(_{4}\) and \(_{5}\), as shown in (b). By the simple trek rule and further assuming that all variables have unit variance, the covariance between \(_{4}\) and \(_{5}\), \(_{4,5}\), can be formulated as \(f_{1,4}f_{1,5}+f_{3,4}f_{3,5}+f_{2,1}f_{1,4}f_{2,3}f_{3,5}+f_{2,3}f_{3,4}f_{2, 1}f_{1,5}\)._

## 5 Experiments

We validate our identifiability theory and parameter estimation method on synthetic and real-life data.

### Setting and Evaluation Metric

We begin with our experimental setting of synthetic data. The causal strength \(f_{ij}\) is uniformly sampled from \([-2,2]\) and the noise terms are Gaussian with variance uniformly from \(\). We consider 20 graphs. 10 of them should be parameter-identifiable up to group sign indeterminacy according to our identifiability theory and we refer to them as _GS Case_ (examples in Figure 10 in Appendix). Another 10 should be parameter-identifiable up to group sign and orthogonal transformation indeterminacy and we refer to them as _OT Case_ (examples in Figure 11 in Appendix). On average each graph contains 15 variables, 3 out of them are latent. We consider three different sample sizes: 2k, 5k, and 10k. We use three random seeds to generate the causal model and report the mean performance as well as the std.

As the optimization in Eq (4) is nonconvex, we will rely on 30 random starts and choose the one with the best likelihood. We report the performance of the proposed method with two different objectives. **(i)** Parameter Estimator with objective defined in Eq. (4), referred to as Estimator, and **(ii)** Parameter Estimator with objective defined in Eq. (5) and Trek Rule parameterization trick in Eq (6), referred to as Estimator-TR.

It is worth noting that our setting is very general in that we allow latent variables and observed variables to be causally connected in a very flexible way, and we consider the identification of parameters of edges that can involve both observed and latent variables. Therefore, to the best of our knowledge, no current method can achieve the same goal to serve as the baseline (which also shows the novelty of the proposed method). As such, we mainly focus on comparing our estimation result with the ground truth parameters. We use two MSE-based metrics defined as follows.

**MSE up to group sign:** suppose the ground truth parameter is \(F\) and our estimation is \(\). The MSE up to group sign is defined as \(||_{2}^{2}}{\|F\|_{0}}\), where \(||\) takes element wise absolute value, \(\|\|_{2}\) denotes the Frobenius norm and \(\|\|_{0}\) denotes the number of nonzero entries of a matrix.

**MSE up to orthogonal transformation:** the MSE up to orthogonal transformation is defined as

\[_{Q Q^{T}Q=I}}|-|Q^{T}_{}Q \|_{2}^{2}+\||F_{}|-|Q^{T}_{}||_{2}^{2}+\||F_{ }|-|_{}Q\|_{2}^{2}+\||F_{}|-|_{ }||_{2}^{2}}{\|F\|_{0}},\] (7)

where the optimization is solved by Adam  and the orthogonal matrix \(Q\) can be directly parameterized in PyTorch.

### Synthetic Data Performance

We report the performance using synthetic data in Tables (a)a and (b)b, where both our Estimator and Estimator-TR achieve very good identification performance. For example, in the GS scenario with 10k samples, our Estimator achieves 0.0.0012 MSE up to group sign and our Estimator-TR achieves 0.0003 MSE up to group sign. The good performance by Estimator and Estimator-TR not only validates our estimation method, but also empirically verifies our identifiability theory.

Table 1: Experimental result on synthetic data using MSE (mean (std)).

Figure 4: Estimated edge coefficients by the proposed method for Big Five human personality dataset. Variables whose name starts with “L” are latent variables while the others are observed variables.

### Misspecification Behavior

In this section, we show that the proposed estimation method still performs well, even under model misspecification: violation of normality and violation of linearity.

As for violation of normality, we use uniform noise terms for the underlying model, and thus the distribution is not jointly Gaussian anymore. We aim to see to what extent can the proposed method still recover the correct parameters. The result is shown in Table 2 in the Appendix, which shows even when the normality is violated, we can still estimate the parameters pretty well. The reason lies in that our proposed asymptotic identifiability result holds true, even when we do not assume Gaussianity; as we only make use of the second-order statistics of the distribution, the additive noise in Definition 1 can follow any other continuous distribution.

To simulate the violation of linearity, we employ the leaky ReLU function during the generation process, as \(_{i}=(_{_{i}(_{i})} f_{ji}V_{j}+_{_{i}})\), \((x)=( x,x)\). When \(\) is close to 1, the function is close to a linear one, and when \(\) is close to 0, the model is very nonlinear. The result is shown in Table 3 and we found that our estimation method is quite robust to small violations of linearity. For example, for Estimator-TR in GS case with 10k sample size, if we set \(=0.8\), we still get a small MSE of 0.001. Even when \(\) decreases to 0.6, the MSE is around 0.005, which is still small. However, when \(\) is decreased to \(0.3\), the underlying model is considerably nonlinear, and the MSE increases to 0.027.

### Implementation Details, Runtime Analysis, and Scalability

Our code is based on Python3.7 and PyTorch . Data is standardized and the optimization in Eqs. (4), (5), and (7) are solved by Adam , with a learning rate of \(0.02\). We conduct all the experiments with single Intel(R) Xeon(R) CPU E5-2470. All experiments can be finished within \(2\) hours. We note that our method is very computationally efficient. First, the computational cost is almost irrelevant to sample size: we only need to calculate the sample covariance matrix once and cache it for further use during the optimization. Plus, our estimation method can handle a large number of variables. For example, the running time of our method are roughly 10 seconds, 2 minutes, and 10 minutes for 20 variables, 50 variables, and 100 variables respectively. For 300 variables, which is a considerably large number for typical experiments considered in causal discovery papers, the estimation can still be finished within around one hour.

It is also worth noting that model misspecifications do not influence the computation cost of our method. We briefly discuss the efficiency of checking whether conditions in Theorem 3 hold, together with what if conditions do not hold in solving real-life problems in Appendices A.8 and A.9.

### Real-World Data Performance

In this section, we employ a famous psychometric dataset - Big Five dataset https://openpsychometrics.org/, to validate our method. It consists of 50 indicators and close to 20,000 data points. There are five dimensions: Openness, Conscientiousness, Extraversion, Agreeableness, and Neuroticism (O-C-E-A-N). Each is measured with 10 indicators. Data is standardized.We employ the RLCD method  to determine the MEC and GIN  to decide the remaining directions. Then we employ the proposed Estimator-TR to estimate all the edge coefficients. The structure satisfies the condition in Theorem 5 so we know that the parameters are identifiable.

The estimated edge coefficients are shown in Figure 4. We found that our estimated coefficients are well aligned with existing psychology studies. For example, according to [16; 17], being successful in exploratory endeavors depends on the stability to pursue them. This is illustrated in our result where \(}\) and \(}\) indicates that Conscientiousness positively influence openness and Agreeableness positively influences Extraversion. Moreover, it has been shown that people are likely to weigh the outcomes of their actions, thus, their level of Conscientiousness coupled with Neuroticism may prohibit them from engaging in risky behaviors (\(}}}\)) . Such consistency with current psychometric studies again validates the effectiveness of the proposed method in parameter estimation of real-life systems.

## 6 Conclusion

In this paper, we characterize indeterminacy of parameter identification and provide conditions for identifiability. Finally, we propose a novel estimation method and validate it by empirical study.

Acknowledgement

This material is based upon work supported by NSF Award No. 2229881, AI Institute for Societal Decision Making (AI-SDM), the National Institutes of Health (NIH) under Contract R01HL159805, and grants from Salesforce, Apple Inc., Quris AI, Florin Court Capital, and the MBZUAI-WIS grant.