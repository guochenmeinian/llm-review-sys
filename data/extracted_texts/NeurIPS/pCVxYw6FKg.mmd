# The Empirical Impact of Neural Parameter Symmetries, or Lack Thereof

Derek Lim

MIT CSAIL

dereklim@mit.edu

&Theo (Moe) Putterman

UC Berkeley

moeputterman@berkeley.edu

&Robin Walters

Northeastern University &Haggai Maron

Technion, NVIDIA &Stefanie Jegelka

TU Munich, MIT

Equal contribution

###### Abstract

Many algorithms and observed phenomena in deep learning appear to be affected by parameter symmetries -- transformations of neural network parameters that do not change the underlying neural network function. These include linear mode connectivity, model merging, Bayesian neural network inference, metanetworks, and several other characteristics of optimization or loss-landscapes. However, theoretical analysis of the relationship between parameter space symmetries and these phenomena is difficult. In this work, we empirically investigate the impact of neural parameter symmetries by introducing new neural network architectures that have reduced parameter space symmetries. We develop two methods, with some provable guarantees, of modifying standard neural networks to reduce parameter space symmetries. With these new methods, we conduct a comprehensive experimental study consisting of multiple tasks aimed at assessing the effect of removing parameter symmetries. Our experiments reveal several interesting observations on the empirical impact of parameter symmetries; for instance, we observe linear mode connectivity between our networks without alignment of weight spaces, and we find that our networks allow for faster and more effective Bayesian neural network training. Our code is available at https://github.com/cptq/asymmetric-networks.

## 1 Introduction

Neural networks have found profound empirical success, but have many associated behaviors and phenomena that are difficult to understand. One important property of neural networks is that they generally have many _parameter space symmetries_ -- for any set of parameters, there are typically many other choices of parameters that correspond to the same exact neural network function . For instance, permutations of hidden neurons in a multi-layer perceptron (MLP) induce permutations of weights that leave the overall input-output relationship unchanged. These parameter symmetries are a type of (not-necessarily detrimental) redundancy in the parameterization of neural networks, that adds much non-Euclidean structure to parameter space.

Parameter space symmetries appear to influence several phenomena observed in neural networks. For example, when linearly interpolating between the parameters of two independently trained networks with the same architecture, the intermediate networks typically perform poorly . However, if we first align the two networks via a permutation of parameters that does not affect the network function, then the intermediate networks can perform just as well as the unmerged networks . In some sense, this suggests that neural network loss landscapes are more convexor well-behaved after removing permutation symmetries. Other areas that parameter symmetries play a role in include interpretability of neurons , optimization [48; 84; 80], model merging , learned equivariance , Bayesian deep learning , loss landscape geometry , processing neural network weights as input data using metanetworks , and generalization measures [49; 11].

To rigorously study the effect of parameter symmetries, we study the effect of removing them. In particular, we introduce two ways of modifying neural network architectures to remove parameter space symmetries (see Figure 1):

1. \(\)-Asymmetric networks fix certain elements of each linear map to break symmetries in the computation graph.
2. \(\)-Asymmetric networks use a new nonlinearity (FiGLU) that does not act elementwise, and hence does not induce symmetries such as permutations.

These two approaches are inspired by previous work, which shows that both symmetries of computation graphs  and equivariances of nonlinearities  induce parameter symmetries in standard neural networks. We theoretically prove that both of our approaches remove parameter symmetries under certain conditions. Our Asymmetric networks are similar structurally to standard networks and can be trained with standard backpropagation and first-order optimization algorithms like Adam. Thus, they are a reasonable "counterfactual" system for studying neural networks that are similar to standard neural networks, but that do not have as many parameter symmetries.

With our Asymmetric networks, we run a suite of experiments to study the effects of removing parameter symmetries on several base architectures, including MLPs, ResNets, and graph neural networks. We investigate linear mode connectivity, Bayesian deep learning, metanetworks, and monotonic linear interpolation. Through the lenses of linear mode connectivity and monotonic linear interpolation, we see that the loss landscapes of our Asymmetric networks are remarkably more well-behaved and closer to convex than the loss landscapes of standard neural networks. When using our Asymmetric networks as the base model in a Bayesian neural network, we find faster training and better performance than using standard neural networks that have many parameter symmetries. When using metanetworks to predict properties such as test accuracy of an input neural network, we see that all tested metanetworks more accurately predict the accuracy of Asymmetric networks than standard networks. Overall, our Asymmetric networks provide valuable insights for empirical study and hold promise for advancing our understanding of the impact of neural parameter symmetries.

## 2 Background and Definitions

Let \(\) be the space of parameters of a fixed neural network architecture. For any choice of parameters \(\), we have a neural network function \(f_{}:\) from an input space \(\) to an output space \(\). We call a function \(:\) a _parameter space symmetry_ if \(f_{}(x)=f_{()}(x)\) for all inputs \(x\) and parameters \(\) (i.e. if \(f_{}\) and \(f_{()}\) are always the same function).

For instance, consider a two-layer MLP with no biases, parameterized by matrices \(=(_{2},_{1})\) with an elementwise nonlinearity \(\). Then \(f_{}(x)=_{2}(_{1}x)\). Let \(P\) be a permutation matrix,

Figure 1: (Left) Standard MLP. The hidden nodes (grey hatches) can be freely permuted, which induces permutation parameter symmetries. Black edges denote trainable parameters. (Middle) Our \(\)-Asymmetric MLP, which fixes certain weights to be constant and untrainable (colored dashed lines) to break parameter symmetries. (Right) Our \(\)-Asymmetric MLP, which uses our FiGLU nonlinearity involving a fixed matrix F (colored dashed lines) to break parameter symmetries.

and let \(()=(_{2}P^{},P_{1})\). Then for any input \(x\),

\[f_{()}(x)=_{2}P^{}(P_{1}x)=_{2 }P^{}P(_{1}x)=_{2}(_{1}x)=f_{ }(x),\] (1)

so \(\) is a parameter space symmetry. A key step is the second equality, which holds because \(P(x)=(Px)\): any elementwise nonlinearity \(\) is permutation equivariant. Any other equivariance of \(\) also induces a parameter symmetry; for instance, if \((x)=(0,x)\) is the ReLU function, then \((x)=( x)\) for any \(>0\), so there is a positive-scaling-based parameter symmetry [49; 11; 20].

## 3 Related Work

**Characterizing parameter space symmetries.** While many works spanning several decades have noted specific parameter space symmetries in neural networks [24; 61], some works take a more systematic approach to deriving parameter space symmetries. Godfrey et al.  characterize all global linear symmetries induced by the nonlinearity for two-layer multi-layer perceptrons with pointwise nonlinearities. Zhao et al.  study several types of symmetries, and derive nonlinear, data-dependent parameter space symmetries. Lim et al.  show that graph automorphisms of the computation graph of a neural network induce permutation parameter symmetries, which captures hidden neuron permutations in MLPs and hidden channel permutations in CNNs.

**Constraints and post-processing to break parameter space symmetries.** Several works develop methods for constraining or post-processing the weights of a single neural network to remove ambiguities from parameter space symmetries. This includes methods to remove scaling symmetries induced by normalization layers or positively-homogeneous nonlinearities such as \(\)[6; 55; 54; 36], methods to remove permutation symmetries [55; 54; 71; 36], and methods to remove sign symmetries induced by odd activation functions .

Unlike these previous works, we develop neural network architectures that have reduced parameter space symmetries. Our models are optimized using standard unconstrained gradient-descent based methods like Adam. Hence, our networks do not require any non-standard optimization algorithms such as manifold optimization or projected gradient descent [6; 55], nor do they require post-training-processing to remove symmetries or special care during analysis of parameters (such as geodesic interpolation in a Riemannian weight space ). These methods based on constraining weights or post-processing have significantly different optimization and loss landscape properties (for instance, linear interpolation is not even well-defined on general nonlinear parameter manifolds), so they are less suitable than our Asymmetric networks for studying phenomena that may generalize to standard neural networks.

**Aligning multiple networks for relative invariance to parameter symmetries.** One way to reduce the impact of parameter symmetries in certain settings, especially for model merging, is to align the parameters of one network to another. Several methods have been proposed for choosing permutations that align the parameters of two neural networks of the same architecture, via efficient heuristics or learned methods [4; 69; 63; 13; 1; 53; 47; 67]. Other approaches relax the exact permutation-parameter-symmetry constraint or do additional postprocessing to achieve effective merging of models in parameter space [59; 28; 29; 60; 56]. As our Asymmetric networks have removed parameter symmetries, they can often be successfully merged and linearly interpolated between without any alignment.

## 4 Asymmetric Networks

We develop two methods of parameterizing neural network architectures without parameter symmetries, both of which are justified by theoretical results. We can prove that our \(\)-Asym networks have permutation and scale symmetries removed, and that our \(\)-Asym networks have permutation symmetries removed. Although we have not formally proven that \(\)-Asym networks have scale symmetries removed, we believe that they do (intuitively, the fixed weights fix a scale).

We first focus on the case of fully-connected MLPs with no biases, which take the form \(f_{}(x)=_{L}(_{L-1}(_{1 }x))\) for weights \(=(_{L},,_{1})\) and nonlinearity \(\). Then in Section 4.3, we discuss how we use these approaches to remove parameter symmetries in other architectures (e.g. CNNs, GNNs) as well.

### Computation Graph Approach (\(\)-Asymmetric Networks)

Our first approach to developing neural networks with greatly reduced parameter space symmetries relies on their computation graph. In particular, we can write a feedforward neural network architecture as a DAG \(G=(V,E)\) with neurons as nodes \(V\) and connections between them as edges \(E\). For a choice of parameters \(^{|E|}\), we get a function \(f_{}\) from input neuron space to output neuron space [19; 49; 39]

Lim et al.  showed that neural DAG automorphisms \(\), which are graph automorphisms of the DAG \(G\) that preserve types of nodes and weight-sharing constraints, induce permutation parameter symmetries \(\) that leave the function unchanged: \(f_{}=f_{()}\). Thus, any feedforward neural network architecture that has no permutation parameter symmetries must necessarily have a computation graph with no nontrivial neural DAG automorphisms.

To modify MLPs so they have no nontrivial neural DAG automorphisms, we mask edges in the computation graph, by setting certain edge weights to constant values that are not updated during training. For an MLP, we can do this by enforcing that every linear layer \(T:^{d_{1}}^{d_{2}}\) takes the form of a matrix \(^{d_{2} d_{1}}\), where each row has a unique pattern of untrained weights. To achieve this, define a mask \(M\{0,1\}^{d_{2} d_{1}}\) such that \(_{ij}\) is a trainable parameter if and only if \(M_{ij}=1\), and the rows of \(M\) are pairwise distinct binary vectors in \(\{0,1\}^{d_{1}}\). We call any neural network with linear maps masked as such a \(\)-Asymmetric neural network. In Appendix B.1, we show that masking these entries so that they are not trained is sufficient to remove all nontrivial neural DAG automorphisms.

**Theorem 1**.: _If each mask matrix \(M\) has unique nonzero rows, then \(\)-Asymmetric MLPs with fixed entries set to zero have no nontrivial neural DAG automorphisms._

In practice, we generate a binary mask \(M\) by randomly selecting a subset of \(n_{}\) fixed elements for each row. For the fixed entries, we sample them from a normal distribution \((0, I)\) with standard deviation \(>0\) that is a hyperparameter that we tune. Our asymmetric linear layer can be written as

\[^{}=M+(1-M),\] (2)

where \(^{d_{2} d_{1}}\) is a matrix of trainable parameters, and \(^{d_{2} d_{1}}\) is a matrix of fixed elements, sampled from \((0, I)\). The only trainable parameters are the unmasked entries of \(M\), of which there are \(d_{2}(d_{1}-n_{})\). We empirically find that having \(\) be significantly larger than the standard deviation of typical initializations for weight matrices (e.g. \(=1\) while the trained coefficients have standard deviation about \(1/\)) is important for breaking parameter symmetries.

Figure 2: Depiction of our \(\)-Asymmetric approach to removing parameter symmetries. Entries with a black outline are untrained. Note that the \(\)-Asym linear map has 2 nonzeros per row, the \(\)-Asym convolution with fixed entries has 8 fixed entries for its single output channel, and the \(\)-Asym convolution with fixed filters has a single input filter fixed. We often use a constant number of fixed entries per row or output channel in our experiments.

### Nonlinearity Approach (\(\)-Asymmetric Networks)

Another approach for removing parameter symmetries is to change the nonlinearity. As studied by Godfrey et al. , equivariances of the nonlinearity induce parameter symmetries in MLPs with elementwise nonlinearities. Recall that an elementwise nonlinearity acts by using the same function on each coordinate of the input; \(:^{d}^{d}\) is elementwise if it takes the form \((x)=(_{1}(x_{1}),,_{1}(x_{d}))\) for some real function \(_{1}:\). Any elementwise nonlinearity is permutation equivariant, and hence induces a permutation parameter symmetry.

Thus, in contrast to most neural network architectures, for Asymmetric networks we must use a nonlinearity that does not act elementwise. Likewise, the nonlinearity cannot have any linear symmetry itself, since if \( A=B\) for \(A,B GL(d)\), then for a two-layer network:

\[_{2}_{1}=_{2}B^{-1}B _{1}=_{2}B^{-1} A_{1}.\] (3)

So \((_{2},_{1})\) and \((_{2}B^{-1},A_{1})\) give the same neural network function. Thus, in order to define a model class without parameter symmetries, it is necessary for \(\) to have _no linear equivariances_, i.e. we desire that if \( A=B\) for \(A,B GL(d)\), then \(A=B=I\). For two-layer MLPs with square invertible weights, this is in fact sufficient to remove all parameter symmetries: we prove this in Appendix B.2.

**Proposition 1**.: _Let the parameter space \(\) be all pairs of square invertible matrices \(=(_{2},_{1})\) for \(_{2},_{1} GL(d)\), and let \(f_{}(x)=_{2}(_{1}x)\). If \(\) has no linear equivariances, then \(f_{_{1}}=f_{_{2}}\) if and only if \(_{1}=_{2}\). In other words, there are no nontrivial parameter space symmetries._

#### 4.2.1 FiGLU: the Fixed Gated Linear Unit Nonlinearity

Motivated by Proposition 1, we define a non-elementwise nonlinearity that does not have the equivariances of standard nonlinearities. Letting \(\) be the sigmoid function \((x)=}\), we define our nonlinearity as

\[(x)=(x) x,\] (4)

for a randomly sampled, untrained matrix \(\). Similarly to \(\)-Asym nets, we sample \(\) as an i.i.d Gaussian matrix with variance that we tune. This nonlinearity is similar to Swish / SiLU [57; 26] with an additional matrix \(\) to mix feature dimensions (to break permutation equivariance), and it is also similar to a gated linear unit (GLU) with no trainable parameters . Thus, we call our nonlinearity FiGLU: the Fixed Gated Linear Unit.

In Appendix B.2.1, we prove that FiGLU does not have permutation equivariances or diagonal equivariances, which are the only equivariances for most elementwise nonlinearities .

**Proposition 2**.: _With probability \(1\) over the sampling of \(\), FiGLU has no permutation equivariances or diagonal equivariances._

We call any network with our symmetry-breaking FiGLU nonlinearity a \(\)-Asymmetric Network.

### Extension to Other Architectures

The graph-based approach (\(\)-Asymmetric Networks) works naturally for neural network architectures with "channel" dimensions, such as convolutional neural networks (CNNs), graph neural networks (GNNs) , Transformers , and equivariant neural networks based on equivariant linear maps . In these types of networks, permutations of entire channels induce permutation parameter symmetries . For such networks, we thus mask entire connections between channels, e.g. entire filters in CNNs. For CNNs, we also experiment with randomly masking some number of entries in each filter (instead of masking entire filters), and find that this also works well in removing parameter symmetries. For neural networks with linear layers that include bias terms, we do not modify the biases in any way, as they do not introduce new computation graph automorphisms .

The nonlinearity-based approach (\(\)-Asymmetric Networks) can be straightforwardly applied to many general architectures as well. Though, the fixed matrix \(\) may have to be changed to a structured linear map; for instance, in CNNs we take \(\) to be a 1D convolution.

### Universal Approximation

Our two approaches remove parameter symmetries from standard neural networks, but still intuitively retain much of the structure of standard networks. One important property of widely-used neural network architectures is universal approximation -- for any target function of a certain type, there exists a neural network of the given architecture that approximates the target to an arbitrary accuracy [8; 25; 42; 75]. In Appendix B.3, we show that \(\)-Asymmetric MLPs retain this property:

**Theorem 2** (Informal).: _For \(n_{} o(n^{})\), where \(n\) is the hidden dimension, \(\)-Asymmetric MLPs are Universal Approximators with probability \(1\) over the choice of hardwired entries._

We have not been able to prove a similar result for \(\)-Asymmetric networks. Classical universal approximation results for standard neural networks do not apply to \(\)-Asym nets, as they tend to assume elementwise nonlinearities.

## 5 Experiments

### Linear Mode Connectivity without Permutation Alignment

**Background.** Under certain conditions, neural networks have been found to exhibit linear mode connectivity, which is when all networks on the line segment in parameter space between two well-performing trained networks are also well-performing. Starting with Frankle et al. , who coined the term and provided the first in-depth analysis, many works have studied this phenomenon [44; 13; 76; 14]. When the two networks are randomly initialized and trained independently, linear mode connectivity generally does not hold [13; 1]. However, if one of the two networks is permuted with a parameter symmetry that does not change its function, but that aligns its parameters with the other network, then linear mode connectivity empirically and theoretically holds for many more model / task combinations [13; 1; 79; 14]. In fact, Enetzari et al.  conjectures that if all permutation symmetries are accounted for, then linear mode connectivity generally holds. Since our Asymmetric networks remove parameter space symmetries, we may expect linear mode connectivity to hold, without any post-processing or alignment step.

**Hypothesis.** Asymmetric networks are more linearly mode connected than standard networks, and do not require post-processing or alignment of pairs of networks before merging.

**Experimental Setup.** We consider several networks and tasks: MLPs on MNIST, ResNets  on CIFAR-10, and Graph Neural Networks  on ogbn-arXiv . For each architecture and task, we compute the midpoint test loss barrier: \(L(_{1}+_{2})-(L(_{1})+L( _{2}))\). This measures how much worse the interpolated network with parameters \(_{1}+_{2}\) is than the original networks with parameters \(_{1}\) and \(_{2}\). We measure this barrier for standard networks, pairs of networks aligned with Git-Rebasin , and networks with our two approaches (\(\)-Asym and \(\)-Asym) applied. To be clear, whenever we interpolate between the weights of two Asymmetric networks, they have the same exact fixed weights \(\) (for both \(\)-Asym and \(\)-Asym) and the same exact mask \(M\) (for \(\)-Asym).

**Results.** Figure 3 plots interpolation curves and Table 1 displays midpoint test loss barriers of various methods. Our \(\)-Asymmetric approach lowers the test loss barrier compared to standard networks, but falls short of the alignment approach of Git-Rebasin. On the other hand, our \(\)-Asymmetric

Figure 3: Linear mode connectivity: test loss curves along linear interpolations between trained networks. (Left) MLP on MNIST. (Middle) ResNet with \(8\) width on CIFAR-10. (Right) GNN on ogbn-arXiv. \(\)-Asymmetric networks interpolate the best, followed by networks aligned with Git-Rebasin, then \(\)-Asymmetric networks, and finally standard networks.

approach achieves strong (and sometimes perfect) interpolation, and interpolates better than standard networks aligned via Git-ReBasin. This may be caused by failure of the Git-ReBasin approaches to find the optimal permutations, importance of other parameter symmetries besides layer-wise permutations, or other properties of \(\)-Asymmetric networks.

### Bayesian Neural Networks

**Background.** Bayesian deep learning is a promising approach to improve several deficits of mainstream deep learning methods, such as uncertainty quantification and integration of priors [30; 51]. However, parameter symmetries are problematic in Bayesian neural networks, as they are a major source of statistical nonidentifiability . Parameter symmetries introduce modes in the posterior \(p(|)\) that make the posterior harder to approximate [2; 34; 72], sample from [50; 71], and otherwise analyze . For instance, one common technique for training Bayesian neural networks is variational inference via fitting a Gaussian distribution to the true posterior \(p(|)\). This approach suffers because the Gaussian distribution has only one mode, whereas the true posterior has at least one mode for every parameter symmetry. As such, some approaches treat kernel matrices are random variables, which has less symmetries than treating features or weights as random variables, and allows better approximation by unimodal posteriors [74; 43]. Instead, we consider traditional, commonly-used Bayesian deep learning techniques applied on the features or weights of our Asymmetric networks.

**Hypothesis.** Using Asymmetric networks as the base model improves Bayesian neural networks, as the posterior will have less modes.

**Experimental setup.** We train Standard Bayesian and Asymmetric Bayesian Networks for image classification using variational inference. We use the method of  for variational inference, which fits a Gaussian approximate posterior with a diagonal plus low-rank covariance. We train 10 instances of each model and then report train loss, test loss, test accuracy, and Expected Calibration Error (ECE) , which is a measure of calibration.

**Results.** See training curves in Figure 4, and quantitative results in Table 2. Using \(\)-Asymmetric networks as a base for Bayesian deep learning improves training speed and convergence. Most

    & Standard & Git-ReBasin & \(\)-Asym (ours) & \(\)-Asym (ours) \\  MLP (MNIST) & \(0.188.12\) & \(-.006.00\) & \(0.117.01\) & \(.00\) \\ ResNet (CIFAR-10) & \(3.287.32\) & \(2.041.21\) & \(2.521.46\) & \(.72\) \\ ResNet 8x width (CIFAR-10) & \(2.640.24\) & \(0.509.45\) & \(1.492.15\) & \(.05\) \\ GNN (ogbn-arXiv) & \(1.475.24\) & \(0.269.02\) & \(0.901.11\) & \(.03\) \\   

Table 1: Test loss interpolation barriers at midpoint: \(L(_{1}+_{2})-(L(_{1})+L( _{2}))\). We use different methods of breaking symmetries in each column; from left to right: no symmetry breaking, Git-Rebasin , our \(\)-Asym approach, and our \(\)-Asym approach. We report mean and standard deviation of the barrier across at least 5 pairs of networks, and bold lowest barriers.

    & Model & Train Loss \(\) & Test Loss \(\) & ECE \(\) & Test Acc \(\) & Test Acc (25 Epochs) \(\) \\    } &  & \(1.34.00\) & \(1.24.01\) & \(0.039.009\) & \(56.37.31\) & \(52.87 0.2\) \\  & & \(\)-Asym MLP-8 & \(.01\) & \(.01\) & \(0.42.009\) & \(.50\) & \( 0.2\) \\  & & MLP-16 & \(2.29.02\) & \(2.28.03\) & \(0.026.017\) & \(13.54 2.0\) & \(13.34 2.7\) \\  & & \(\)-Asym MLP-16 & \(.01\) & \(.01\) & \(0.045.009\) & \(.44\) & \( 0.3\) \\    } &  & \(.01\) & \(5.35.03\) & \(0.405.007\) & \(81.98 1.2\) & \(72.37 1.0\) \\  & & \(\)-Asym ResNet200 & \(6.00.02\) & \(5.35.01\) & \(0.044.004\) & \(81.94 0.6\) & \(73.64 1.5\) \\  & & \(\)-Asym ResNet110 & \(.03\) & \(7.06.08\) & \(0.052.007\) & \(75.71 2.8\) & \(59.85 3.9\) \\  & & \(\)-Asym ResNet110 & \(.07\) & \(.06\) & \(0.049.004\) & \(77.40 2.4\) & \( 3.0\) \\    } &  & \(1.68.03\) & \(1.57.02\) & \(0.078.004\) & \(56.83.62\) & \(46.80 0.9\) \\  & & \(\)-Asym ResNet20 (BN) & \(.02\) & \(.03\) & \(0.076.006\) & \(.62\) & \( 0.4\) \\   & & \(\)-Asym ResNet20 (LN) & \(1.97.02\) & \(1.88.02\) & \(0.909.007\) & \(50.02.54\) & \(37.24 1.1\) \\   & & \(\)-Asym ResNet20 (LN) & \(.03\) & \(.02\) & \(0.86.006\) & \(.47\) & \( 1.0\) \\   

Table 2: Bayesian neural network results. Reported loss is the negative log likelihood loss. All results (except for last column) are after 50 epochs of training. \(\)-Asymmetric networks tend to improve over their standard counterparts, especially early in training. 16-layer MLPs fail to train, but 16-layer \(\)-Asymmetric MLPs successfully train. Standard or Asymmetric networks better than their counterpart by a standard deviation are bolded.

strikingly, Bayesian MLPs of depth 16 cannot train at all, while \(\)-Asymmetric Bayesian MLPs train well. In general, the \(\)-Asymmetric approach improves training and test accuracy across the several models (MLPs, ResNets of varying sizes, and ResNets with either batch norm or layer norm).

### Metanetworks

**Background.** Metanetworks  -- also referred to as deep weight-space networks [46; 58], meta-models , or neural functionals [81; 82; 83] -- are neural networks that take as inputs the parameters of other neural networks. Recent work has found that making metanetworks invariant or equivariant to parameter-space symmetries of the input neural networks can substantially improve metanetwork performance [46; 81; 39; 32].

**Hypothesis.** Asymmetric networks are easier to train metanetworks on because they do not have to explicitly account for symmetries.

**Experimental setup.** We experiment with metanetworks on the task of predicting the CIFAR-10 test accuracy of an input image classifier, which many metanetworks have been tested on [65; 12; 81; 39]. We use metanetworks based on simple MLPs, 1D-CNN metanetworks , and metanetworks that are exactly invariant to permutation parameter symmetries: DeepSets  and StatNN . We train two separate datasets of 10,000 image classifiers: one dataset of small ResNet models, and one dataset of \(\)-Asymmetric ResNet models. More information on the data, metanetworks, and training details are in Appendix F.3.

**Results.** In Table 3, we see that metanetworks are signficantly better at predicting the performance of our \(\)-Asymmetric ResNets than standard ResNets. Interestingly, simple MLP metanetworks, which view the input parameters as a flattened vector, can predict the test accuracy of Asymmetric Networks quite well, but fail on standard networks. Also, the permutation equivariant metanetworks (DeepSets and StatNN) both improve on \(\)-Asym ResNets compared to on ResNets, even though the permutation symmetries of standard ResNets do not affect these metanetworks; thus, it may be possible that other symmetries in standard ResNets (but not Asym-ResNets) harm metanetwork performance, or they may be other factors besides symmetries that improve metanetwork performance for Asym-ResNets.

Figure 4: Bayesian neural network training loss over time for depth 8 MLPs on MNIST (left), ResNet110 on CIFAR-10 (middle), and ResNet20 with BatchNorm on CIFAR-100 (right). \(\)-Asymmetric networks train more quickly, and achieve lower training loss.

    &  & \)-Asym ResNet} \\   & \(R^{2}\) & \(\) & \(R^{2}\) & \(\) \\  MLP & \(.330.04\) & \(.389.03\) & \(\) & \(\) \\ DMC  & \(.950.01\) & \(.787.02\) & \(\) & \(\) \\ DeepSets  & \(.855.01\) & \(.617.03\) & \(\) & \(\) \\ StatNN  & \(.976.00\) & \(.866.00\) & \(\) & \(\) \\   

Table 3: Metanetwork performance for predicting the test accuracy of small ResNets and our \(\)-Asym ResNets. Each row is a different metanetwork. Reported are \(R^{2}\) and Kendall \(\) on the test set — higher is better.

### Monotonic Linear Interpolation

**Background.** One common method of studying the loss landscapes of neural networks is by studying the one-dimensional line segment of parameters attained by linear interpolation between parameters at initialization and parameters after training. Many works have observed _monotonic linear interpolation_ (MLI), which is when the training loss monotonically decreases along this line segment [21; 17; 41; 68]. Loss landscapes of convex problems have this property as well, so presence of the monotonic linear interpolation property has been used as a rough measure of how well-behaved the loss landscape is. However, with many types of models, tasks, or hyperparameter settings, monotonic linear interpolation does not hold [41; 68], or there is a large plateau where the loss barely changes for much of the line segment [17; 70]; neither of these properties can happen for convex objectives trained to completion. To the best of our knowledge, there has been little work on the role of parameter symmetries -- or lack thereof -- in monotonic linear interpolation (besides one minor experiment in  Appendix C.9). Nonetheless, since removing parameter symmetries substantially improves linear interpolation between trained networks (Section 5.1), one may expect removing parameter symmetries to improve monotonic linear interpolation.

**Hypothesis.** The training loss along the line segment between initialization and trained parameters is more monotonic and convex for Asymmetric networks.

**Experimental setup.** For the learning task, we follow the setup used for creating the dataset of image classifiers in Section 5.3. In particular, we train 300 standard ResNets and 300 \(\)-Asymmetric ResNets with varying hyperparameters sampled from the same distributions as used for the dataset of image classifiers (see Appendix Table 13). For each of these networks, we linearly interpolate between its initial parameters \(_{0}\) and its final trained parameters \(_{T}\): \((1-)_{0}+_{T}\) for 25 uniformly spaced values \(0=_{1}<_{2}<<_{25}=1\). To measure monotonicity, we record the maximum increase between adjacent networks \(=(L(_{i+1})-L(_{i}))\), and the percentage of networks that have \( 0\) i.e. the percentage of networks that satisfy monotonic linear interpolation. To measure convexity, we consider a local convexity measure (the proportion of \(_{i}\) where the centered difference second derivative approximation is nonnegative) and a global convexity measure (the proportion of \(_{i}\) such that \(L(_{i})\) lies below the line segment between the endpoints, i.e. \(L(_{i})(1-_{i})L(0)+_{i}L(1)\)).

**Results.** Table 4 shows the measures of monotonicity and convexity for standard, \(\)-Asymmetric, and \(\)-Asymmetric ResNets. Remarkably, every single one of the 300 \(\)-Asymmetric ResNets satisfies

    & \(\) & Percent Monotonic \(\) & Local Convexity \(\) & Global Convexity \(\) \\  Standard ResNet & \(.079.109\) & \(26.3\%\) & \(.548.139\) & \(.823.229\) \\ \(\)-Asym ResNet & \(.004.047\) & \(87.3\%\) & \(.675.143\) & \(.976.098\) \\ \(\)-Asym ResNet & \(-..026\) & \(\%\) & \(..165\) & \(.000\) \\   

Table 4: Monotonic linear interpolation: properties of linear interpolations between 300 pairs of initialization and trained parameters. Arrows denote behavior that is more similar to convex optimization, e.g. there is a downarrow (\(\)) next to \(\) because convex objectives have nonpositive \(\), while nonconvex can have positive \(\). For both types of Asymmetric networks, all differences from Standard ResNets are statistically significant (\(p<.001\)) under a two-sided T-test: Asymmetric networks have significantly more monotonic and convex linear interpolations from initialization.

Figure 5: Train loss against interpolation coefficient \(\) for the interpolation \((1-)_{0}+_{T}\) between initial parameters \(_{0}\) and trained parameters \(_{T}\). Trajectories for the 20 \((_{0},_{T})\) pairs of lowest train loss for each architecture are plotted. The trajectories for Asymmetric ResNets appear significantly more monotonic and convex.

monotonic linear interpolation and has a trajectory that lies underneath the line segment between the endpoints. Qualitatively, we can see in Figure 5 that \(\)-Asymmetric ResNets do not have any clear loss barriers from initialization, nor any loss plateaus that indicate nonconvexity. In contrast, the majority of standard ResNets have non-monotonic trajectories, and the monotonic trajectories seem to be more nonconvex. \(\)-Asymmetric network trajectories are signficantly more convex and monotonic than standard network trajectories, but there are some non-monotonic or nonconvex trajectories still.

### Other Optimization and Loss Landscape Properties

In Appendix A, we note other interesting differences in optimization and loss landscape properties of Asymmetric and standard neural networks. These can be summarized as:

1. Even though Asymmetric networks interpolate much better than standard networks, the parameters of trained Asymmetric networks are often basically the same distance away from each other in weight space as standard networks.
2. Asymmetric networks do not tend to overfit as much: the difference in train performance and test performance can be substantially lower than that of standard networks.
3. Asymmetric networks can take longer to train, especially when choosing hyperparameters that make them more dissimilar to standard networks.

## 6 Discussion

While many properties of Asymmetric networks are in line with our hypotheses and intuition about the impact of removing parameter symmetries, there are many unexpected effects and unanswered questions that are promising to further investigate. For instance, we did not extensively explore Asymmetric networks in the context of model interpretability, generalization measures in weight spaces, or optimization improvements, all of which are known to be influenced to some extent by parameter symmetries. Further studying the properties in Section 5.5, the dependence of behavior on the choices of Asymmetry-inducing hyperparameters, and other design choices in making networks asymmetric could also bring more insights into parameter space symmetries.

Also, it is interesting that our \(\)-Asymmetric networks do not appear to break parameter symmetries as well as our \(\)-Asymmetric networks. We have run preliminary empirical tests on several variants of \(\)-Asym networks, such as: \(\) as the nonlinearity, sparsifying \(\), adding instead of multiplying the gate, using cosine instead of sigmoid, squaring instead of using sigmoid, and putting a LayerNorm in the nonlinearity. However, none of these approaches worked well. We believe that such failures may be because \(\)-Asymmetry breaks symmetries at the activation / neuron level, whereas \(\)-Asymmetry breaks symmetries in the larger space of weights (for more evidence, see Appendix E, where we show that fixing biases at neurons also fails to effectively remove symmetries). These curiosities provide interesting directions for future work.

All in all, we believe that future theoretical and empirical study of Asymmetric networks could garner many insights into the role of parameter symmetries in deep learning.

#### Acknowledgements

We would like to thank Kwangjun Ahn, Benjamin Banks, Nima Dehmamy, Nikos Kuralias, Jinwoo Kim, Marc Law, Hannah Lawrence, Thien Le, Jonathan Lorraine, James Lucas, Behrooz Tahmasebi, and Logan Weber for discussions at various points of this project. DL is supported by an NSF Graduate Fellowship. RW is supported in part by NSF award 2134178. HM is the Robert J. Shillman Fellow, and is supported by the Israel Science Foundation through a personal grant (ISF 264/23) and an equipment grant (ISF 532/23). This research was supported in part by Office of Naval Research grant N00014-20-1-2023 (MURI ML-SCOPE), NSF AI Institute TILOS (NSF CCF-2112665), NSF award 2134108, and the Alexander von Humboldt Foundation.