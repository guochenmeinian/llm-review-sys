# Invariant Anomaly Detection under

Distribution Shifts: A Causal Perspective

 Joao B. S. Carvalho, Mengtao Zhang, Robin Geyer, Carlos Cotrini, Joachim M. Buhmann

Institute for Machine Learning

Department of Computer Science

ETH Zurich

{joao.carvalho, mengtao.zhang, robin.geyer, ccarlos, jbuhmann}@inf.ethz.ch

###### Abstract

Anomaly detection (AD) is the machine learning task of identifying highly discrepant abnormal samples by solely relying on the consistency of the normal training samples. Under the constraints of a distribution shift, the assumption that training samples and test samples are drawn from the same distribution breaks down. In this work, by leveraging tools from causal inference we attempt to increase the resilience of anomaly detection models to different kinds of distribution shifts. We begin by elucidating a simple yet necessary statistical property that ensures invariant representations, which is critical for robust AD under both domain and covariate shifts. From this property, we derive a regularization term which, when minimized, leads to partial distribution invariance across environments. Through extensive experimental evaluation on both synthetic and real-world tasks, covering a range of six different AD methods, we demonstrated significant improvements in out-of-distribution performance. Under both covariate and domain shift, models regularized with our proposed term showed marked increased robustness. Code is available at: [https://github.com/JoaoCarv/invariant-anomaly-detection](https://github.com/JoaoCarv/invariant-anomaly-detection).

## 1 Introduction

_Anomaly detectors_ are the subject of increased interest in fields such as finance (Ahmed et al. (2016); Hilal et al. (2022)), medicine (Schlegl et al. (2019)), and security (Mothukuri et al. (2021); Siddiqui et al. (2019); Hosseinzadeh et al. (2021)). Having been trained on a sample from an unknown distribution, these models are capable of identifying abnormal objects unlikely to come from the original distribution (Bishop and Nasrabadi (2006)). Anomaly detection (AD) stands apart from supervised classification as it does not involve anomalies during training, making it challenging to articulate a model that depicts the class of objects deemed as normal.

AD as a field boasts a plethora of diverse methodologies (Ruff et al. (2021)). Current detectors have demonstrated the advantage of approaches based on representation learning (Reiss and Hoshen (2021); Deng and Li (2022)). In this context, an _encoder_ maps objects to _representations_ which capture the most distinctive features of an object. In addition, it strives to map the class of normal objects onto a subset characterized by a more regular shape, thereby rendering representations from abnormal samples easily identifiable by comparison. Central to this second goal is a notable vulnerability of representation learning-based methods: they hinge on the assumption of independent and identically distributed (i.i.d.) training and test data. This implies that normal samples in the training data are expected to be sampled identically in the test data, thereby being mapped to the same vicinity in the representation space - an assumption that is frequently violated in real-world scenarios (Koh et al. (2021)).

Indeed, distribution shifts in the context of AD present a unique challenge because it involves discerning two types of distribution shifts targeting the distribution of the normal objects, \(p_{n}\). Anomaliescan be conceptualized as samples from a different distribution \(p_{a}\), which scarcely overlaps with the mass of \(p_{n}\). This distribution \(p_{a}\) can be interpreted as a shifted variant of \(p_{n}\), and the task of anomaly detection becomes identifying this shift. Changes in the environment can also produce distribution shifts. When such a distribution shift transpires from the training data to the test data, \(p_{n}\) is transformed into \(_{n}\). If anomalies are naively perceived as merely samples from the original normal samples that are sufficiently different, then all samples from \(_{n}\) could be wrongly classified as abnormal samples. Therefore, it is crucial to note that anomaly detectors need to identify shifted versions of \(p_{n}\) that generate anomalies while being resilient to those originating from any other \(_{n}\).

Additionally, anomaly detectors are particularly susceptible to changes in features that are incidentally correlated with being normal. This is because the training data consists only of normal objects, which in conjunction with consistent exposure to any recurrent features present in the environment, inherently facilitates the emergence of shortcut learning (Geirhos et al. (2020)). These effects can significantly impact the reliability of crucial anomaly detectors. For example, consider an anomaly detector designed for monitoring bank transactions, predominantly occurring during business hours. Such a detector could mistakenly hinge on the transaction time to determine anomalies. This approach could lead to fraudulent transactions executed at noon going undetected, while regular transactions made by individuals active at night might erroneously be flagged as suspicious. Thus, the effective detection of anomalies requires the model to go beyond such superficial correlations and instead focus on more substantive, causally relevant features.

In this work, we formalize the problem of anomaly detection in the presence of domain and covariate shifts using causality and information theory (Wang and Veitch (2022); Wang et al. (2022); Jiang and Veitch (2022); Liu et al. (2021); Linsker (1988); Stone (2004); Hjelm et al. (2018)). We view the generation of the dataset as a causal graph and distinguish between the environment that generates the dataset, the content features, the style features, and representations that are computed from these features. We note that to mitigate vulnerabilities to distribution shifts, we need to produce representations that are invariant to the environment where the object comes from (Veitch et al. (2021); Wang et al. (2022); Wang and Veitch (2022)). That is, the representation is not causally influenced by the environment. Through this formalization, we derive and formally justify a necessary condition for learning invariant representations within anomaly detection. To impose this condition, we introduce the partial conditional invariant regularization (PCIR) term for learning invariant representations in this setting. This term minimizes discrepancies between representations from different environments by minimizing discrepancies like the maximum mean discrepancy (MMD) (Gretton et al. (2012)). Similiar approaches relying on MMD regularization for domain generalization in different tasks have proven to be valuable (Kang et al. (2019); Long et al. (2013)). However, to our knowledge, all methods relying on conditional invariance have been applied under the assumption of access to all classes during training. This is not the case in the AD setting.

Contributions:**(1)** We conduct a series of experiments to illustrate the limitations of current AD methods in handling various distribution shifts. **(2)** Through causal inference we propose a novel formalization of the dual requirements of informativeness and invariance for robust AD models, leading to a new insight, as described in Theorem 2. **(3)** We introduce a regularization term to induce _partial conditional distribution invariance_, which serves as a strategic measure to ensure robust AD amid distribution shifts. **(4)** Our empirical findings demonstrate the effectiveness of our regularization term; all tested models exhibited an enhancement in performance under both covariate and domain shifts when augmented with _partial conditional invariant regularization_ (PCIR).

## 2 Related Works

### Distribution Invariant Representation Learning

The relationship between invariance and robustness to shifts in data distribution has been extensively explored, notably within the field of causal inference.

A range of studies has pursued the development of invariant representation learning, achieving notable success (Long et al. (2013); Kang et al. (2019); Mitrovic et al. (2020); Lv et al. (2022); Nguyen et al. (2021)). Moreover, a detailed examination of various forms of invariance-based methods derived from underlying causal graphs has been provided by Veitch et al. (2021) and Wang and Veitch (2022).

Causal inference has also been shown to foster distributional robustness, as documented in Meinshausen (2018). Building on this notion, _Invariant Risk Minimization_ (IRM) (Arjovsky et al. (2019)) introduced a novel optimization principle hinged on maintaining invariance across diverse environments. However, Koh et al. (2021) demonstrated that this approach tends to falter when subjected to real-world distribution shifts. Yao et al. (2022) has introduced selective data augmentations to induce distribution shift robustness. However, due to its reliance on class labels, it is not directly applicable to AD.

### Deep Anomaly Detection

A common strategy in AD involves the deployment of autoencoders. These methods operate by learning normal patterns within a dataset and then attempting to _reconstruct_ new samples. The premise is that anomalous samples, due to their distribution differences, will yield higher reconstruction errors. Several studies have adopted this method, demonstrating its efficacy in a range of contexts (Gong et al. (2019);Park et al. (2020);Yan et al. (2021)).

In the wake of recent advancements in self-supervised representation learning (He et al. (2019); Chen et al. (2020)), _contrastive learning_ has emerged as a viable method for AD. Approaches such as _PANDA_(Reiss et al. (2021)), and the more recent _MeanShift_(Reiss and Hoshen (2021)) and _CFA_(Lee et al. (2022)) harness these strategies to finetune pre-trained encoders to the AD setting. Conversely, _CSI_(Tack et al. (2020)) and _DROC_(Goyal et al. (2020)) use contrastive learning to learn a representation of the normal data from scratch. _Knowledge distillation_-based methods also hold a significant place in AD research. A student-teacher framework, which incorporates discriminative latent embeddings, was first introduced in Bergmann et al. (2020). The most recent iterations, _Reverse Distillation_(Deng and Li (2022)) and _STFPM_(Wang et al. (2021)) have attained state-of-the-art performance across numerous AD tasks. Recently, _Red Panda_(Cohen et al. (2023)), has adapted _PANDA_ by adding an additional disentanglement term to the loss derived from Kahana and Hoshen (2022), with the intent of inducing robustness to previously identified attributes.

Figure 1: Comparative analysis of various mapping strategies from \(\) to \(\) in the context of AD under distribution shifts. **(a)** Anomaly detection setting with multiple environments in \(\). **(b)** Ideal scenario where the mapping is both invariant and informative; normal samples from different environments converge to the same subset of \(\), maintaining the distinction between normal and abnormal samples. **(c)** Mapping is invariant but not informative, resulting in both normal and abnormal representations collapsing to the same subset of \(\). **(d)** Mapping is informative but not invariant; although the mappings of all environments remain distinct, this makes the encoder vulnerable to distribution shifts.

Despite the effectiveness of these methods, it is important to note that they primarily focus on AD within datasets where the training and test data distributions are identical. As we will show, its performance tends to degrade when confronted with data that exhibit different kinds of distribution shifts.

One way to improve robustness to distribution shifts entails pretraining the encoder using an invariance-inducing method (for example, IRM or LISA), applied to an unrelated pretext task using the same data (Smeu et al. (2022)). Unfortunately, this requires the existence of additional task labels, which is uncommon for AD tasks. Moreover, as we experimentally demonstrate, this approach achieves limited performance when paired with state-of-the-art AD methods.

## 3 Formalization

### Background

Our formulation operates under the assumption of a given set of _environments_, denoted as \(\), and a sample space, \(\). We have a sample \(D=\{x_{1},,x_{n}\}\) drawn from an unknown distribution \(p_{n}\), whose mass is mostly concentrated on a subset \(N\). This subset, \(N\), defines the _normal class_. Given that \(X p_{n}\), we consider \(X\) as a pair of random variables \(X_{a}\) and \(X_{e}\), such that \(X=(X_{a},X_{e})\). Here, the environment \(E\) only influences \(X_{e}\). Conceptually, \(X_{a}\) represents the component of \(X\) that determines whether \(X\) is an anomaly, while \(X_{e}\) comprises style features. These style features, while unaffected by the anomaly status of an object, are influenced by the environment. Note that this assumption implies that \(=_{a}_{e}\), for some appropriate sets \(_{a}\) and \(_{e}\). Fig. 1 (a) illustrates our setting under three different environments.

Our main goal in this section is to elicit the requirements that representations should fulfil in order to lead to effective anomaly detectors. More precisely, we argue that representations must simultaneously (i) maximize the information they contain about the original objects and (ii) they must be invariant to the environment.

### Requirements for Robust Encoders

We illustrate the necessity of invariance and informativeness for effective AD using an example with random variables \(X_{1},X_{2},X_{3}\). We assume that they are distributed over the set \(N\) of all points \((x_{1},x_{2},x_{3})^{3}\) such that \(0 x_{1}=x_{2} 1\). We assume that \(x_{3}\) denotes the environment. Our dataset \(D\) consists of points in \(N\) where \(x_{3}=0\).

Consider now the following encoders \(f_{1}\), \(f_{2}\), and \(f_{3}\), where \(f_{1}(x_{1},x_{2},x_{3})=x_{1}\), \(f_{1}(x_{1},x_{2},x_{3})=(x_{1},x_{2})\), and \(f_{1}(x_{1},x_{2},x_{3})=(x_{1},x_{2},x_{3})\). Note that encoder \(f_{1}\) is invariant but lacks the necessary information to detect anomalies, as it can map different points to the same representation. Indeed, the normal point \((1,1,1)\) and the anomaly point \((1,2,1)\) would be mapped to the same representation, the number \(1\). Hence, any anomaly detector based on this encoder can be easily evaded. Encoder \(f_{3}\) captures all information but is not invariant, which leads to a potential evasion of AD. Encoder \(f_{2}\), on the other hand, is both invariant and informative, correctly capturing the necessary information to detect anomalies while ignoring the environment variable \(x_{3}\).

This simplified scenario emphasizes the dual necessity of invariance and informativeness for the development of robust anomaly detectors. Regrettably, as we will demonstrate, contemporary AD methods tend to excel in informativeness while significantly lagging in invariance. For a graphical illustration of this effect, please see Fig. 1 (b-d).

We now formalize the desiderata for representations computed by an encoder for AD.

### Invariant Representations

Let \(f\) be an encoder mapping a \(\) to a representation space \(\). We also let \(Z=f(X)\) denote the _representation_ of \(X\).

**Definition 1**.: We say that \(Z\) is an _invariant representation_ of \(X\) under _domain shifts_ if

\[p^{do(E=e)}(Z=)=p^{do(E=e^{})}(Z=),e,e^{ }. \]

**Definition 2**.: We say that \(Z\) is an _invariant representation_ of \(X\) under _covariate shifts_ if

\[p^{do(X_{e}=x)}(Z=)=p^{do(X_{e}=x^{})}(Z=),x,x^{}_{e}. \]

We simply say that \(Z\) is an invariant representation of \(X\) if it remains unaltered under both domain shifts and covariate shifts. Note that domain shifts are stronger than covariate shifts. Specifically, domain shifts entail assessing data from diverse sources with disparate parameters, whereas covariate shifts pertain to subtle alterations in the existing data. Consequently, the broader changes inherent to domain shifts usually exert a more substantial impact on data representation stability and are harder to tackle.

Invariant representations prevent anomaly detectors from incorrectly classifying objects that result from covariate and domain shifts. These representations are, by definition, resistant to changes in \(E\) or \(X_{e}\). As a result, adversaries cannot expect to alter the detection of an anomaly by merely using a different environment or changing style features of the anomaly.

However, solely enforcing invariant representations can be insufficient, and potentially lead to a collapse of the representations (see Fig. 1 (c)). For this reason, we argue that representations shall store as much information about the original object as possible. This principle is in line with the design of many popular encoders (Linsker (1988); Stone (2004); Hjelm et al. (2018)) and can be formalized using information theory.

### Mutual Information between Representations and Objects

We can use the mutual information between two random variables as our metric to evaluate how much information about \(X\) is captured by the representation \(Z\).

**Definition 3**.: The _mutual information_\(I(X;Z)\) between \(X\) and \(Z\) is defined as

\[I(X;Z)=(x,z)}{p_{X}(x)p_{Z}(z)}dx\,dz. \]

Here, \(p_{X,Z}\) is the joint pdf of \((X,Z)\) and \(p_{X}\) and \(p_{Z}\) are the marginal pdfs of \(X\) and \(Z\), respectively.

Intuitively, \(I(X;Z)\) indicates how much information learning about one of \(X\) or \(Z\) reveals about the other. We argue then that encoders shall compute representations that keep as much information as possible from the original objects.

## 4 Learning Invariant and Informative Representations

Having outlined the prerequisites for anomaly detection representations, the next task is to devise strategies that enable encoders to learn both invariant and informative representations. Encouragingly, most contemporary AD methods inherently employ strategies that maximize the mutual information between the observation and the representation (Tishby and Zaslavsky (2015)). This is often achieved either via a reconstruction loss term (Kong et al. (2019)) or a contrastive loss term (Sordoni et al. (2021); Wu et al. (2020); Oord et al. (2018)).

We dedicate the rest of this section to developing a strategy suitable for invariant representation learning under the scope AD.

### Learning Invariant Representations

We start by introducing a causal graph for anomaly detection in Fig. 2. In these graphs, \(E\), \(X_{a}\), and \(X_{s}\) denote the environment, the relevant features, and the style features. We introduce two new random variables \(W\) and \(U\). \(W\) is a binary variable indicating if the object is normal (\(W=0\)) or an anomaly (\(W=1\)). \(U\) denotes all confounding factors. That is, factors that produce correlations between \(X_{a}\) and \(X_{e}\) during the sample generation process.

We now recall that an encoder \(f\) that attains invariant representations is \(X_{a}\)-measurable (Veitch et al. (2021)). The following result, which follows from a theorem from Veitch et al. (2021), argues that we can ensure invariant representations by enforcing a possibly conditional statistical independence between \(Z\) and \(E\).

**Theorem 1**.: Suppose that \(f\) learns invariant representations. If \(W\) and \(E\) are confounded, then \(Z E W\). Otherwise, \(Z E\).

Proof.: We sketch a proof here and provide a more rigorous proof in the appendix. Since \(f\) learns invariant representations and is, therefore, measurable, we can say that \(Z\) is \(X_{a}\)-measurable. This means that any probability of an event involving \(Z\) can be rewritten as a probability of an event involving \(X_{a}\). As a result, it suffices to show that \(X_{a} E W\), when \(W\) and \(E\) are confounded and \(X_{a} E\), otherwise. These claims can be shown using d-separation. For example, when \(W\) and \(E\) are not confounded, there are only two paths from \(X_{a}\) to \(E\) and they are blocked. Hence, \(X_{a} E\) by d-separation. 

The fundamental question we must address is how to ensure statistical independence between \(Z\) and \(E\). If such independence holds, we find that for all \(e,e^{}\), it implies that \(p^{do(E=e)}(Z=)=p^{do(E=e^{})}(Z=)\). In general, we cannot access counterfactual examples, so enforcing counterfactual invariance becomes impossible. However, it is still possible to induce a counterfactual invariance signature by imposing appropriate conditional independence conditions. In practice, this condition can be set as \((p(Z= E=e),p(Z= E=e^{}))=0\), where MMD stands for maximum mean discrepancy (Gretton et al. (2006, 2012)), and measures the distance between two distributions using empirical samples by calculating the divergence between the means of these sample sets once they have been projected into a reproducing kernel Hilbert space (RKHS).

### Partial Conditional Invariant Regularization (PCIR)

Based on the previous insights, we propose using the MMD between two distributions as the driver for the invariance-inducing regularization term. Taking into account our previous formulation depicted in Fig. 2, we are now in a position to derive a novel regularization term specifically designed for invariant AD:

\[_{}=_{e,e^{} \\ e e^{}}(p(Z= W=0,E=e),p(Z=  W=0,E=e^{})). \]

We call this approach _partial conditional invariant regularization_ (PCIR), as it induces conditional distribution invariance over only one instantiation of \(W\).

This partial conditional regularization aligns with other regularization terms proposed in preceding research (Veitch et al. (2021), Li et al. (2018)). It is 'conditional' because, in the event of confounding, we must condition on \(W\) when computing the MMD. It is 'partial' due to the fact that the training dataset only contains samples where \(W=0\).

Figure 2: Causal graphs for anomaly detection. The left figure shows the case of no confounding. The right figure shows the case of confounding. An intervention at the \(E\) variable induces a domain shift (gray hammer), whereas an intervention at the \(X_{e}\) variable induces a covariate shift (black hammer).

Experiments

### Experimental Setup

Our solutions are validated under two distinct settings: domain generalization across domain shifts and domain generalization across covariate shifts. All experiments described were the result of 5 repetitions over different seeds. For additional details on the experimental design please refer to the supplementary material.

Real-world domain shiftFor a realistic anomaly detection scenario, we considered the task of identifying tumorous tissue from images of histological cuts, using the Camelyon17 (Koh et al. (2021); Bandi et al. (2018)) dataset. This dataset consists of five different subsets of images arising from five different hospitals, with domain shifts occurring due to differences in slide staining, image acquisition protocol, and patient cohorts. This presents a challenging anomaly detection task, as the alterations that differentiate normal from abnormal samples are often subtle and may correlate with unknown features that are domain-dependent. In our experimental design, we motivate an environment per domain and set up two sets of experiments: one using three environments for training and another using two environments for training.

Real-world shortcutWe have also evaluated our method in the Waterbird dataset (Sagawa et al. (2019)). This is a real-world natural image dataset where the distribution shift occurs as the natural habitat depicted in the background changes between an aquatic and land setting. From the two kinds of birds in the dataset (water and land birds), water birds were assigned to training data and land birds were set as an anomaly. To make this a more challenging setup, we have defined a highly unbalanced distribution of the environments, with 184 images in training data with a water background and 3498 with a land background. The evaluation of the methods was performed in images with only a water background for out-of-distribution and only land background for in-distribution.

Synthetic covariate shiftsTo evaluate the robustness against covariate shifts, we use the DiaViB-6 dataset Eulig et al. (2021) for our experiments. This dataset comprises modified and upsampled images from MNIST Deng (2012), where the generative factors of the image can be altered, leading to changes in texture, hue, lightness, position, and scale.

The training data is comprised of two distinct environments, generated by manipulating original instantiations of each factor, and ensuring all factors differed between the two environments. For the test data, we defined five distinct environments denoted as \(e_{0},e_{1},...,e_{4}\). The environment \(e_{0}\) corresponds to images from MNIST for a specific digit under an original instantiation of each factor. All images of that digit are labeled as normal, while images of any other digit are considered anomalies. For \(0<i 4\), each environment \(e_{i}\) corresponds to images where \(i\) factors have been modified. In \(e_{i}\), all factors modified in \(e_{i-1}\), plus an additional unique factor, are altered. The task is to classify an image of a handwritten digit as either normal or an anomaly.

We also adapted this setup to include the Fashion-MNIST dataset, a more challenging synthetic benchmark. The same cumulative covariate shifts were induced from an original in-distribution environment, \(e_{0}\), to subsequent environments \(e_{1-4}\). Two specific classes were chosen to generate normal and abnormal samples.

Anomaly detection methods testedWe undertook a comprehensive evaluation of our proposed regularization term, PCIR, and integrated it into six diverse methodologies for deep AD. These include: _STFM_Wang et al. (2021), _Reverse Distillation_Deng and Li (2022), _CFA_Lee et al. (2022), _MeanShifted_Reiss and Hoshen (2021), _CSI_Tack et al. (2020), and Red PANDA Cohen et al. (2023). For a detailed exposition of these methods, refer to the supplementary material.

### Results

Real-world anomaly detection under domain shiftNotably, even with the complexity and real-world variability introduced by the domain shift, the incorporation of partial conditional distribution invariance still resulted in notable improvements for both training setups, in particular with _MeanShift_ and _Red PANDA_ retrieving almost in-distribution performance (see Fig. 3). This suggests that our regularization term is robust to more substantial changes associated with domain shifts.

Focusing on the out-of-distribution setting, our results highlight a consistent pattern of performance enhancement when applying the partial conditional invariance regularization term, ranging from 2% to 8% increase in the AUROC when comparing regularized and un-regularized methods. The consistency of this performance boost across different models and with both two and three in-distribution training environemnts underscores the potential of PCIR as a beneficial regularization technique in out-of-distribution settings.

Realist shortcut learningWe observe that in all original unregularized methods evaluated, there is a noticeable drop in performance when comparing in-distribution (background transparent plots) to out-of-distribution (foreground opaque plots) that is attributed to the models effectively capturing the exposed shortcut. Models with PCIR nearly recover the in-distribution performance, showing the effectiveness of this approach to ignore uninformative environment features even when exposed through a shortcut. Furthermore, in all models tested, there's a consistent observation that the model performance not only stabilizes but also increases when adding the regularization term even for in-distribution scenarios. The increase in model performance, attributed to the addition of PCIR to each method, exhibits an improvement range from 5% to 15% AUROC.

MMNIST and Fashion-MNIST under covariate shiftModels that absorb shortcut features have been observed to be especially susceptible to covariate shifts (Eulig et al. (2021); Geirhos et al. (2020)). Therefore, if a model abuses a shortcut, then inducing a covariate shift in the shortcut feature often significantly deteriorates performance. This effect is discernible in Fig. 4, where performance drops progressively increase for all models as more covariate shifts are induced in the images, and thus more shortcut features deviate from their original form in the training data.

However, note that by adopting regularization based on partial conditional invariance, we have been able to construct anomaly detectors that consistently exhibit enhanced robustness against induced covariate shifts. In some cases, even in-distribution performance increases through partial conditional invariance. These findings corroborate our hypothesis that partial conditional distribution invariance serves as a sufficient prior for robustness to covariate shifts in AD. For a more comprehensive analysis of performance improvements achieved refer to the supplementary material.

Ablation studyIn our ablation study, we held all method parameters constant to exclusively examine the impact of the weight assigned to the partial conditional invariance regularization term.

Across all tested methods, the relationship between regularization weight and performance exhibited a concave shape. This pattern suggests a simple linear search could be sufficient to identify an optimal weight for the regularization term. This observed behaviour aligns intuitively with the nature of invariance regularization. Over-emphasizing the regularization term could potentially cause the model's encoder to generate non-discriminative and non-informative representations. In an extreme case, the model could collapse all inputs into a single value. Interestingly, the behaviour of the regularization term remained consistent across both datasets used in our study. This reinforces the robustness of our approach across diverse datasets.

Figure 3: Results on realistic domain shift Camelyon17 and realist shortcut learning in Waterbirds. **(background transparent bar-plots:** in-distribution evaluation; **foreground opaque bar-plots:** out-of-distribution evaluation). **(a)** Camelyon17: Setup with two in-distribution training environments and three out-of-distribution test environments. **(b)** Camelyon17: Setup with three in-distribution training environments and two out-of-distribution test environments **(c)** Waterbirds: Shortcut learning setup.

## 6 Discussion and Limitation

Invariance regularization also improves in-distribution ADA surprising result from this study was the consistent enhancement in in-distribution performance across most methods when partial conditional invariance was incorporated. This observation can be explained by recent advances suggesting that invariance also bolsters in-distribution robustness against noise and sampling variability (Lopes et al. (2019); Mitrovic et al. (2020)) - factors to which anomaly detection is inherently vulnerable (Ramstoela et al. (2018)). By design, invariance regularizers discourage representations encoding style features. Hence, representations only carry over information that is inherently related to the object rather than the environment. This leads to models identifying meaningful patterns instead of noisy variations in the data, even in-distribution.

Figure 4: Experimental results on synthetic covariate shift (**background transparent bar-plots:** in-distribution evaluation; **foreground opaque bar-plots:** out-of-distribution evaluation) (**a-d**) Results in MNIST. **(e-h)** Results in Fashion-MNIST.

Figure 5: Ablation study over the weight of the regularization term for MNIST and Fashion-MNIST under distribution shift. Regularization weight determines the weight of PCIR in the training loss.

Unlabeled environmentsA prominent limitation of invariance regularization lies in its dependence on environment labels (Veitch et al. (2021); Jiang and Veitch (2022); Arjovsky et al. (2019)). Additionally, as evidenced in our experimental comparisons between covariate and domain shifts, interventions directly applied to the covariates during environment generation yield enhanced robustness under the constraints of partial conditional invariance. This underscores that conditional invariance regularization's effectiveness diminishes when the environments are not distinctly segregated. However, in situations where datasets lack explicit environment partitions, alternative strategies can be employed. As demonstrated by Lin et al. (2022) it remains feasible to jointly estimate environment partitions and invariant representations, provided there is access to sufficient auxiliary information. This finding opens the door to expanding the applicability of partial conditional invariance in AD, even in scenarios with limited information on environmental conditions or corrupted separation between environments.

Invariance beyond partial conditioningTheorem 1 shows that in the presence of confounding, learning invariant representations requires that representations are independent from the environment, when conditioned on \(W\). Note that our partial conditional invariant regularization conditions only on \(W=0\) and not on \(W=1\) (see Equation 4). This is an inherent limitation due to the fact that we only have normal objects in the sample. However, our regularization is still powerful enough to provide improvements over the state of the art. We argue that the quality of this improvement depends on how disentangled content features are from style features (i.e., \(X_{a}\) and \(X_{e}\) in Figure 2). Consider, for example, an MNIST setting where normal objects are images of particular digit and images of any other digit are anomalies. Furthermore, assume that images from one environment have one background color and images from another environment have another background color. There is a clear disentanglement between style (i.e., the digit) and content (i.e., the background color). In such settings, attaining invariance to the background color just with \(W=0\) leads also to invariance to the background color with \(W=1\). We conjecture that partial conditional invariant regularization is sufficient when \(W\) does not influence \(X_{e}\); that is, when there is no arrow between these variables in Figure 2.

Multi-shift environmentsCertain datasets, such as the one described by Christie et al. (2018), take into account data shifts across two different kinds of domain shifts (e.g. time and location), equivalent to dual interventions in a bivariate \(E\) in our formulation. While the extension of our work to address multi-attribute settings might be plausible in a dual intervention scenario, it presents a non-trivial challenge as the number of different possible interventions increases, given that it requires the induction of pairwise invariance across environments. This issue continues to be an area of active research.

FairnessOne potential extension of this work concerns the setting where either a covariate shift is induced over a protected attribute (e.g., gender or race), or a domain shift that permeates to such attributes. Such nuances have been brought in the context of invariant representation learning by Veitch et al. (2021) and could serve as a potential application of this work in AD. In particular, we could see our regularization term be implemented similarly to Louizos et al. (2015), but instead with the intent of effectively discarding incidental correlations that might reflect societal or systemic biases present in the data.

## 7 Conclusions

Our study sheds light on the significant challenges anomaly detectors face in the context of distribution shifts. We proposed a novel, robust solution centered around invariant representations, which mitigates the impact of shortcut learning by enforcing statistical independence between the representations and the environment. Empirical validation of our theoretical proposals confirmed the effectiveness of our approach, with a regularization term inducing partial conditional distribution invariance, significantly improving model performance under covariate and domain shifts. We believe these findings pave the way for a deeper understanding of AD methods' robustness and how to mitigate their vulnerability to distribution shifts.