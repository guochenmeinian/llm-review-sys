# Connecting Multi-modal Contrastive Representations

Zehan Wang\({}^{1}\) Yang Zhao\({}^{2}\) Xize Cheng\({}^{1}\) Haifeng Huang\({}^{1}\) Jiageng Liu\({}^{1}\) Li Tang\({}^{1}\)

**Linjun Li\({}^{1}\) Yongqi Wang\({}^{1}\) Aoxiong Yin\({}^{1}\) Ziang Zhang\({}^{1}\) Zhou Zhao\({}^{1,3}\)1\({}^{1}\)**

\({}^{1}\)Zhejiang University \({}^{2}\)ByteDance \({}^{3}\)Shanghai AI Laboratory

{wangzehan01}@zju.edu.cn

Corresponding author.

###### Abstract

Multi-modal Contrastive Representation (MCR) learning aims to encode different modalities into a semantically aligned shared space. This paradigm shows remarkable generalization ability on numerous downstream tasks across various modalities. However, the reliance on massive high-quality data pairs limits its further development on more modalities. This paper proposes a novel training-efficient method for learning MCR without paired data called Connecting Multi-modal Contrastive Representations (C-MCR). Specifically, given two existing MCRs pre-trained on (\(\), \(\)) and (\(\), \(\)) modality pairs, we project them to a new space and use the data from the overlapping modality \(\) to aligning the two MCRs in the new space. Meanwhile, since the modality pairs (\(\), \(\)) and (\(\), \(\)) are already aligned within each MCR, the connection learned by overlapping modality can also be transferred to non-overlapping modality pair (\(\), \(\)). To unleash the potential of C-MCR, we further introduce a semantic-enhanced inter- and intra-MCR connection method. We first enhance the semantic consistency and completion of embeddings across different modalities for more robust alignment. Then we utilize the inter-MCR alignment to establish the connection, and employ the intra-MCR alignment to better maintain the connection for inputs from non-overlapping modalities. To demonstrate the effectiveness of C-MCR, we take the field of audio-visual and 3D-language learning as examples. Specifically, we connect CLIP and CLAP via texts to derive audio-visual representations, and integrate CLIP and ULIP via images for 3D-language representations. Remarkably, without using any paired data, C-MCR for audio-visual achieves state-of-the-art performance on audio-image retrieval, audio-visual source localization, and counterfactual audio-image recognition tasks. Furthermore, C-MCR for 3D-language also attains advanced zero-shot 3D point cloud classification accuracy on ModelNet40. Our project page is available at [https://c-mcr.github.io/C-MCR/](https://c-mcr.github.io/C-MCR/)

## 1 Introduction

Multi-modal Contrastive Representation (MCR) learning aims to map inputs from different modalities to a shared representation space. With the impressive generalization performance of vision-language contrastive pre-training models  demonstrated on various downstream tasks , learning MCR spaces between multiple modalities has become a promising area of research, attracting increasing attention .

However, the generalization ability of MCR primarily benefits from the accessibility of massive data pairs from the web. For modalities where obtaining semantically matching data pairs is significantly more costly, the representations directly learned from limited data pairs are unreliable. On the otherhand, these modality pairs with little direct paired data often have a large number of paired data with the same intermediate modality. For example, although audio-visual data are often vague, paired data of audio-language and language-image are sufficient and semantically explicit. Similarly, while 3D point-language pairs are rare, 3D point-image and image-language data are extensive.

Consider that there are already many MCRs between modalities with sufficient paired data. In this paper, we propose Connecting Multi-modal Contrastive Representations (C-MCR), a novel training-efficient and paired-data-free MCR learning method that extends the learned alignment knowledge in existing MCRs to more modalities. With regard to the overlapping modality, its representations in two MCRs are just different data views sharing the same inherent semantics. So we can take them as positive pairs to connect different MCRs. As modalities within each MCR are semantically aligned, the connections built from overlapping modalities can also be applied to non-overlapping modalities. The advantages of our C-MCR are two-fold: **(1) Flexible.** C-MCR enables MCR learning on modalities with limited paired data. More importantly, C-MCR treats each learned MCR space as a node and the overlapping modalities between different MCRs as links. Connecting the various isolated MCRs greatly extends the obtained multi-modal alignment knowledge, and discovers generalized contrastive representations of broader modalities. **(2) Training-Efficient.** Since C-MCR simply reprojects the learned representations into a new space, only two simple projectors are learnable during training. The training parameters and costs for connecting existing MCRs are very small.

However, two factors impede the acquisition of a robust and transferable connection: Firstly, embeddings in MCR spaces are incapable of comprehensively reflecting all the semantic information of the input, and this loss of meaning would be inherited and amplified, thereby compromising the robustness of the connection. Secondly, as discussed in , MCR spaces exhibit a _modality gap_ phenomenon, i.e., the embeddings of different modalities are located in two completely separate regions in each MCR space. This poses a challenge for maintaining the connection based on overlapping modality while facing inputs from non-overlapping modalities.

Considering the above challenges, we propose a semantic-enhanced inter- and intra-MCR connection method. During training, the copious amounts of easily accessible unpaired unimodal data are first encoded into embeddings in two MCR spaces. We inject Gaussian noise into all the embeddings to mitigate the semantic bias, enhance the semantic completeness, and improve robustness. For directly quantifying the modality gap and the relationship between non-overlapping modalities, we exploit the inherent multi-modal alignment in MCR spaces to cluster semantic consistent embeddings and bridge different modalities. With the above strategies, we align the semantic-enhanced embeddings across different MCR spaces in a contrastive manner to establish the connection. To preserve the connection for inputs from non-overlap modalities, we realign the semantic similar embeddings across modalities within each MCR space to alleviate the modality gap.

Our main contributions are summarized as follows:

(1) We propose Connecting Multi-modal Contrastive Representations (C-MCR), a novel paired-data-free and training-efficient method for MCR learning. By connecting existing MCR spaces with simple projectors, we can mine the multi-modal alignment knowledge in existing MCR space, and extend MCRs on more modalities that lack large-scale high-quality data pairs.

(2) We further propose a semantic-enhanced inter- and intra-MCR connection method to unleash our C-MCR. This approach establishes a transferable connection between two MCR spaces via overlapping modality and maintains it for non-overlapping modalities.

(3) To demonstrate the effectiveness of C-MCR, we connect the CLIP and CLAP through texts to acquire audio-visual representations, and interage CLIP and 3D-image MCR space (ULIP) via images for 3D-language representations. Remarkably, without requiring any pair data or fine-tuning, C-MCR for audio-visual achieves state-of-the-art performance on six datasets across three downstream audio-visual tasks. Furthermore, C-MCR for 3D-language also attains advanced zero-shot 3D point cloud classification accuracy on ModelNet40.

## 2 Related Work

**Multi-modal Contrastive Representation learning.** Multi-modal contrastive representation focuses on learning separate unimodal encoders for different modalities, which can map inputs from different modalities into a shared representation space. These models are pre-trained on large-scale paired data using a contrastive loss. Recent vision-language contrastive pre-training models, such as CLIP  and ALIGN , demonstrate impressive zero-shot retrieval and classification performance and remarkable generalization capability across diverse downstream tasks [5; 6; 7; 8; 9; 10]. Inspired by the success of vision-language models, contrastive representation learning across more modalities has garnered increasing attention. CLAP [12; 11] construct a contrastive language-audio pre-training model by collecting large-scale audio-text pairs from diverse data sources. ULIP [17; 18] collect and generate 3D-image-text triplet data via 3D rendering and image captioning, and learn an extra 3D encoder for existing vision-language space. AudioCLIP  and WAV2CLIP  leverage the pre-trained CLIP image encoder and acquire audio-visual representations by training on audio-image pairs from AudioSet  and VGGSound , respectively. For certain modality pairs, such as audio-visual and 3D-language, the pre-training model's generalization capability is restricted by ambiguous or limited paired data. Our proposed method introduces a novel method for better contrastive representation learning on these modalities.

**Audio-Visual Learning.** Audio-visual learning  aims to exploit the relationship between audio and visual modalities, which is an essential part of intelligent multi-modal perception research. Previous methods primarily focus on learning specific audio-visual downstream tasks (such as retrieval [24; 25; 26], localization [27; 28; 29; 30; 31; 32; 33; 34; 35; 36], or generation [37; 38; 39; 40; 41; 42; 43; 44]) within limited domains, based on the manually cleaned small-scale datasets. Recently, several large-scale audio-image datasets [21; 22] collected from the web are proposed. However, these datasets contain many noisy image-audio pairs due to the ambiguous nature of both images and audio and the presence of non-visible sounds and silent objects in videos. Consequently, the generalization ability of audio-visual contrastive representation [19; 20] learned from these datasets is limited. Our C-MCR reduces the need for larger-scale high-quality data pairs. By extending the knowledge in CLIP and CLAP models, we acquire powerful audio-visual contrastive representations that exhibit powerful generalization capabilities across various downstream tasks.

**3D-language Learning.** 3D vision is an important way for robots to perceive the rich semantic and spatial information of the real world. The 3D-language learning including recognition [45; 46; 47; 48], localization [49; 50; 51; 52; 53], question-answer [54; 55] and general conversation [56; 57], have attracted increasing attentions. 3D-language contrastive representations are vital for the further development of 3D-language learning. However, due to the scarcity of 3D-language paired data, the development of 3D-language representation is limited. Recent ULIP [17; 18] focus on generating 3D-image-text triplet data, but they are still limited by the relatively low quality of training datasets. C-MCR gets rid of the dependence on 3D-language pairing data, and instead connects the reliable 3D-visual representation of ULIP and the visual-language representation of CLIP via images to obtain a more robust 3D-language contrastive representation.

Figure 1: **The pipeline of connecting CLIP and CLAP using our C-MCR.** During training, we take text as input and encode it with frozen CLAP and CLIP text encoders, respectively. Audio(image) memory is generated by encoding lots of unimodal audio(image) data by pre-trained audio(image) encoder. Semantic enhancement enriches the semantic consistency and completion of embeddings. Then two projectors map embeddings to a new shared representation space where inter- and intra-MCR alignment establishes and maintains a stable connection between CLAP and CLIP. During inference, the audio and image are inputted to the corresponding encoder and projector.

## 3 Approach

In this section, we take connecting CLIP and CLAP for audio-visual, as an example to introduce C-MCR. As depicted in Figure 1 (a), we utilize two projectors to connect CLIP and CLAP through texts. Before delving into our method, we first introduce the mathematical formulations and revisit the multi-modal contrastive learning in Section 3.1. Then we discuss our semantic enhancement approach for robust representation alignment in Section 3.2. This is followed by the inter-MCR alignment to establish the connection between CLIP and CLAP in Section 3.3, and the intra-MCR alignment to ensure the connection can be maintained for image-audio inputs in Section 3.4.

### Background

**Problem formulation.** For text inputs, the embeddings obtained by CLIP and CLAP encoder can be denoted as \(^{I}^{c}\) and \(^{A}^{d}\) respectively. Our C-MCR method aims to leverage the inherent consistency between \(^{I}\) and \(^{A}\) and the multi-modal alignment in MCR to learn two projectors \(f_{1}()\) and \(f_{2}()\) that map the representations from CLIP and CLAP to a new shared representation space. The connection between CLIP and CLAP, learned from texts, can effectively accommodate audio and image inputs.

**Multi-modal Contrastive Learning.** Given \(N\) paired instances from two different modalities, we map the \(i\)-th pair to L2-normalized embeddings \(_{i}\) and \(_{i}\) via two encoders. Multi-modal contrastive learning aims to maximize the cosine similarity between \(_{i}\) and \(_{i}\) and minimize the cosine similarity between \(_{i}\) and \(_{j}\) where \(i j\). The contrastive loss can be formulated as:

\[L_{cons}=-_{i=1}^{N}[(_{i},_{i})/)}{_{j=1}^{N}(( _{i},_{j})/)}+(_{i },_{i})/)}{_{j=1}^{N}((_{i}, _{j})/)}] \]

where \(\) is the temperature parameter and the \((,)\) is the operator for cosine distance. The contrastive loss is based on multi-modal data pairs, and the generalization of the learned representation relies on the scale and quality of the data pairs. To extend contrastive representation learning on more modalities, we propose using overlapping modalities to connect two learned MCR spaces and extend the learned multi-modal alignment knowledge to non-overlapping modalities.

### Semantic Enhancement

To achieve more robust and comprehensive alignment, we enhance the semantics from two perspectives: inter-modality semantic consistency and intra-modality semantic completion.

**Inter-modality Semantic Consistency.** CLIP and CLAP have already learned shared image-text and audio-text representations. To better quantify the modality gap in the MCR space and directly explore the correlation between audio and image, we first utilize the inherent modality alignment properties of CLIP and CLAP to generate semantically consistent embeddings across modalities. Specifically, we encode massive unpaired images and audio using the CLIP image encoder and CLAP audio encoder, respectively. All the obtained image embeddings are served as image memory \(=\{_{1},_{2},...,_{N}\}\) and the audio embeddings are audio memory \(=\{_{1},_{2},...,_{M}\}\), where \(N,M\) indicate the number of images and audios. Considering \(i\)-th text embeddings \(_{i}^{I}\) and \(_{i}^{A}\), we can generate image embedding \(_{i}^{I}\) and audio embedding \(_{i}^{A}\) that are semantically similar to \(i\)-th text.

\[_{i}^{I}=_{k=1}^{N}(_{i}^{I}, _{k})/_{1})}{_{j=1}^{N}((_{i}^{I},_{j})/_{1})}*_{k};\ \ _{i}^{A}=_{k=1}^{M} (_{i}^{A},_{k})/_{1})}{_{j= 1}^{M}((_{i}^{A},_{j})/_{1})}*_{k} \]

The \(_{1}\) is the temperature hyperparameter. By dynamically absorbing information from memories based on semantic similarity to the text embeddings \(_{i}^{I}\) and \(_{i}^{A}\), we can generate more diverse and accurate semantically-consistent embeddings \(_{i}^{I}\) and \(_{i}^{A}\).

**Intra-modality Semantic Completion.** The semantics in the original input data are often complex, and some information is inevitably lost when encoding it into the MCR space. When connecting and aligning existing representation spaces, this loss and bias of meaning will be inherited and amplified, affecting the robustness of alignment. To enhance the semantic completeness of each embedding,we propose to serve Gaussian noise as an information augmentation method. Specifically, we add zero-mean Gaussian noises into the embeddings and re-normalize them to the unit hypersphere:

\[}^{I}=(^{I}+_ {1});}^{I}=(^{I}+ _{2}) \] \[}^{A}=(^{A}+_{3});}^{A}=(^{A}+_{4})\]

where noise items \(_{1},_{2}^{c}\) and \(_{3},_{4}^{d}\) are sampled from zero-mean gaussian distribution with variance \(^{2}\), and they are not learnable.

Since the MCRs are L2 normalized, all embeddings are distributed on a unit sphere. As illustrated in Figure 1 (c), each embedding can be viewed as a point on the unit sphere's surface. The addition of Gaussian noise can transform the point into a small sphere, and re-normalizing projects the small sphere onto a circle on the surface of the unit sphere. Hence, aligning two embeddings with noise forces the model to acquire the ability to align all the embeddings within the two circles. In the MCR space, the closer two embeddings are to each other, the more similar their semantics are. Embeddings within the same circle share similar general semantics, and the semantics represented by the circle are more comprehensive and robust than the original embedding.

### Inter-MCR Alignment

To establish the connection between two MCRs, we project the semantic-enhanced embeddings from CLIP and CLAP space to a new shared space via two learnable projectors \(f_{1}()\) and \(f_{2}()\), respectively.

\[}^{I}=f_{1}(}^{I});\ \ }^{I}=f_{1 }(}^{I});\ \ }^{A}=f_{2}(}^{A});\ \ }^{A}=f_{2}(}^{A}) \]

In the newly projected space, our objective is to ensure that embeddings with similar semantics from different MCR spaces are in close proximity to each other. The (\(^{I}_{i}\),\(^{A}_{i}\)) from the same text is naturally semantic consistent, and it can be considered as a ground-truth pair label. Besides, there is pseudo consistency in (\(^{I}_{i}\),\(^{I}_{i}\)) and (\(^{A}_{i}\),\(^{A}_{i}\)) due to the multi-modal alignment properties in CLIP and CLAP. Thus the (\(}^{I}\),\(}^{A}\)) derived from (\(^{I}_{i}\),\(^{A}_{i}\)) can be viewed as a pseudo pair label. For a robust and stable connection of the two MCR, we propose to align both (\(}^{I}\),\(}^{A}\)) and (\(}^{I}\),\(}^{A}\)). The text-text contrastive loss \(L_{ttc}\) and audio-visual contrastive loss \(L_{avc}\) are defined as:

\[L_{ttc}=-_{i=1}^{B}[ (}^{I}_{i},}^{A}_{i})/_{2})}{_{j=1}^{B} ((}^{I}_{i},}^{A}_{j})/_{2}) }+(}^{A}_{i},}^{I}_{ i})/_{2})}{_{j=1}^{B}((}^{A}_{i},}^{I}_{j})/_{2})}] \]

\[L_{avc}=-_{i=1}^{B}[ (}^{I}_{i},}^{A}_{i})/_{3})}{_{j=1}^{B} ((}^{I}_{i},}^{A}_{j})/_{3} )}+(}^{A}_{i},}^{I}_{ j})/_{3})}{_{j=1}^{B}((}^{A}_{i},}^{I}_{j})/_{3})}] \]

\(B\) corresponds to the batch, and \(_{2}\), \(_{3}\) are the temperature hyperparameters. The inter-MCR alignment loss \(L_{inter}\) is the combination of the two contrastive losses:

\[L_{inter}=L_{ttc}+L_{avc} \]

The \(L_{ttc}\) and \(L_{avc}\) are complementary to each other. The semantics between (\(^{I}\),\(^{A}\)) are highly consistent, thus the connection learned from them is much more robust, but their alignment is indirect for audio-visual representation. On the other hand, (\(^{I}\),\(^{A}\)) pairs are directly beneficial to audio-visual representation learning, but their semantic coherence is less reliable. Note that since the semantic consistency in (\(^{I}\),\(^{A}\)) is derived from (\(^{I}\),\(^{A}\)), the connection learned from pseudo pair (\(^{I}\),\(^{A}\)) can still be considered as being established via overlapping modalities.

### Intra-MCR Alignment

As discussed in , there exists a phenomenon known as the _modality gap_ in MCR spaces. Although the embeddings from different modalities are semantically aligned in an MCR space, they are distributed in entirely distinct regions of the representation space. This implies that the more stable connection learned from (\(^{I}_{i}\),\(^{A}_{i}\)) may not accommodate the inputs from audio and image.

To better maintain the connection, we propose closing the modality gap and guaranteeing that embeddings from different modalities with similar semantics are distributed in the same region of the representation space. The analysis in  suggests that the repulsive structure in contrastive loss preserves the modality gap. Inspired by this observation, we derive the intra-MCR alignment loss by removing the repulsive structure in the contrastive loss. As introduced in 3.1, a typical contrastive item can be formulated as:

\[-(_{i},_{i})/)}{_{j=1}^{N }((_{i},_{j})/)}=(_{i},_{i})/}_{pull\ positive\ close}+^{N}( (_{i},_{j})/)}_{push\ negative\ away} \]

We only retain the mechanism of pulling samples closer together and remove the repulsive effect between negative pairs, which helps to close the modality gap in the newly learned MCR space. In the L2-normalized MCR space, there are \((_{i}-_{i})^{T}(_{i}-_{i})=2(1- _{i}^{T}_{i})\). After removing the gradient-irrelevant constant terms, our intra-MCR alignment loss \(L_{intra}\) can be expressed as:

\[L_{intra}=_{i=1}^{B}(\|}_{i}^{I}- }_{i}^{I}\|_{2}+\|}_{i}^{A}-}_{ i}^{A}\|_{2}) \]

By realigning text-guided cross-modal semantically consistent embeddings in each MCR space, i.e., aligning (\(}_{i}^{I}\), \(}_{i}^{I}\)) for CLIP and (\(}_{i}^{A}\), \(}_{i}^{A}\)) for CLAP, the modality gap between embeddings from same MCR can be effectively alleviated in the new space. As a result, the more stable connection provided by Equation 5 can be maintained for audio-visual inputs.

### Training and Inference

During training, all pre-trained encoders in CLIP and CLAP are frozen to preserve the semantic correspondences between image-text and audio-text, and only the two projectors are learnable. To make the training more efficient, we pre-extract the text embeddings \(_{i}^{I}\) and \(_{i}^{A}\). Since the semantic enhancements are training-free, the inter-modality semantic consistency strategy can also be pre-computed, and the semantically consistent image \(_{i}^{I}\) and audio embedding \(_{i}^{A}\) are stored offline.

We apply a combination of inter- and intra-MCR alignment loss to optimize the two projectors for establishing a stable connection between CLIP and CLAP representation spaces, formulated as:

\[L=L_{inter}+ L_{intra} \]

\(\) is the hyper-parameter to balance the two terms.

During inference, as shown in Figure 1 (b), the image embedding in CLIP and the audio embedding in CLAP can be mapped into a shared space through corresponding projectors. The cosine scores in this space reflect the semantic similarity between images and audio.

## 4 Experiments

### Details of Connecting CLAP and CLIP

**Text Datasets.** We collected texts from three sources: image-text datasets (COCO  and CC3M ), video-text datasets (MSRVTT , MAD ), and audio-text datasets (AudioCap , Clotho ), to ensure that the texts contain sufficient visual, action, and audio information. To avoid overfitting visual information, we randomly selected one million descriptions from CC3M. In summary, the texts from image-text, video-text, and audio-text are 1.66M, 0.58M, and 77K, respectively, and there are 2.33M texts in total.

**Audio/Image Memory.** AudioSet  provides a vast collection of audio snippets from YouTube videos. All 1.8M audio data in the training set are encoded by the CLAP audio encoder to serve as the audio memory. ImageNet1K  is a large-scale image recognition dataset. We encoded all the 1.3M images in the train set of ImageNet1K using the CLIP image encoder to construct the image memory. It is worth noting that no annotations related to the audio and images are used.

**Implementation Details.** We employ a frozen pre-trained CLIP ViT-B/32 model  and CLAP model . We adopt simple multi-layer perceptrons as our projectors \(f_{1}()\) and \(f_{2}()\). The \(_{1}\), \(_{2}\) and \(_{3}\) in Equation 2, 5 and 6 are all set to 1/100. The variance \(^{2}\) of the noises in Equation 3 is set as 0.004. The hyper-parameter \(\) in Equation 10 is set to 0.1. We train our projectors for 36 epochs using a batch size of 10240. We use the AdamW optimizer with the initial learning rate \(1e-3\) and the cosine learning rate decay strategy.

### Details of Connecting ULIP and CLIP

**Image Datasets.** The image dataset used for connecting ULIP and CLIP is ImageNet1K , total 1.3M images without any annotations.

**Text/3D Memory.** We use the same 2.33M text dataset as described in Section 4.1, to construct the corresponding text memory. The 3D object point clouds from the training set of Objaverse  are utilized to construct 3D memory, 0.8M samples in total.

**Implementation Details.** We employ a frozen pre-trained CLIP ViT-B/32 model , and ULIP-2 PointBERT model  pre-trained on ULIP-Objaverse triplets. The structure of the projector and the temperature parameters remain the same in Section 4.1. The variance \(^{2}\) of the noises in Equation 3 is set as 0.002. The hyper-parameter \(\) in Equation 10 is set to 0.4. We train our projectors for 24 epochs using a batch size of 8192. We also use the AdamW optimizer with the initial learning rate \(5e-3\) and the cosine learning rate decay strategy.

### Evaluation of Audio-Visual Representations

#### 4.3.1 Downstream Audio-Visual Tasks

We assess the quality of audio-visual representations on three downstream audio-visual tasks in a zero-shot manner. More details about the datasets and implementation details are in Appendix.

**Audio-Image Retrieval.** It contains two subtasks: image-to-audio retrieval (I2A) and audio-to-image retrieval (A2I). We assess the zero-shot image-audio retrieval on the AVE  and Flickr-SoundNet . Due to the small size of the test sets in both datasets, we utilized all available data in the train, eval, and test sets for evaluation, resulting in 4095 samples for AVE and 5000 samples for Flickr-SoundNet. For zero-shot inference, we encode all audio and images into our newly learned audio-visual MCR space and computed the cosine similarity for all audio-image pairs. The mAP, Top-1, and Top-5 metrics are used to evaluate retrieval accuracy.

**Audio-Visual Source Localization.** Audio-visual source localization aims to localize the visual sound sources in an image. The test sets of widely-used VGGSS  and MUSIC  benchmarks are employed for evaluation. To enable zero-shot inference, we first use a pre-trained object detector  to extract object proposals from the images and calculate the cosine similarity between each proposal and audio in our representations space. The proposal with the highest similarity score is token as the final prediction. We adopt Consensus Intersection over Union (cloU) and Area Under Curve(AUC) metrics following .

**Counterfactual Audio-Image Recognition.** For the non-visible sounds and images with silent objects, this task requires a model to distinguish the semantically unpaired audio-image from audio-image pairs. During the zero-shot inference phase, we employ an object detector  to extract object proposals from the image. Subsequently, for each image, the proposal with the highest matching score is considered the predicted object, and the matching score is regarded as the confidence score for this prediction. Experiments are conducted on the Extended VGGSS (Ex-VGGSS)  and Extended Flickr-SoundNet (Ex-FlickrNet) , and the comparison is based on the Average Precision (AP) and maximum F1 (Max-F1) metrics following .

In summary, these three tasks can evaluate a group of audio-visual contrastive representations from various perspectives. Audio-image retrieval is employed to assess the ability to match coarse-grained images and audio, audio-visual source localization is used to evaluate the ability to match fine-grained objects and audio, and counterfactual audio-image recognition is used to evaluate the understanding and reasoning ability of audio and visual inputs.

#### 4.3.2 Analysis on Zero-shot Image-Audio Retrieval

We compared our model with AudioCLIP  and WAV2CLIP  which are contrastively pre-trained on image-audio pairs from AudioSet  and VGGSound , respectively. Results in Table 1 demonstrate that C-MCR achieves state-of-the-art zero-shot retrieval performance. Besides, the generalization ability of AudioCLIP and WAV2CLIP is not stable. For instance, WAV2CLIP performs well on AVE but poorly on Flickr-SoundNet, while AudioCLIP achieves good results on Flickr-SoundNet but poor accuracy on AVE. Similar situations can also be observed in Table 2. In contrast, our C-MCR exhibits stronger and more stable generalization ability. Moreover, sinceC-MCR does not utilize any paired data and has much fewer learnable parameters, AudioCLIP and WAV2CLIP are not truly "fair" baselines to compare C-MCR with. Nevertheless, C-MCR still demonstrates superior performance compared to these pre-trained audio-visual models. Figure 2 provides a few visualizations of audio-to-image retrieval.

#### 4.3.3 Analysis on Zero-shot Audio-Visual Source Localization

Table 2 presents the zero-shot audio-visual source localization performance on MUSIC-Solo and VGGSS datasets and the comparison with previous audio-visual source localization methods. Remarkably, despite not using any audio-visual paired data and any fine-tuning, C-MCR demonstrates state-of-the-art performance, achieving a relative improvement of around 25% over the previous leading methods. Additionally, to demonstrate that the improvements are not by introducing the powerful object detector, we also performed the same zero-shot inference using audio-visual representations in AudioCLIP and WAV2CLIP. These methods exhibit unstable generalization performance on the two datasets, and our C-MCR demonstrates a significantly better overall performance than both. These results show that the state-of-the-art performances in audio-visual source localization mainly benefit from the stronger and more robust fine-grained audio-visual matching capability.

#### 4.3.4 Analysis on Zero-shot Counterfactual Audio-Image Recognition

Table 3 shows the comparisons on counterfactual audio-image recognition. Our C-MCR significantly outperforms previous methods that trained on the training set and exhibits overall improvement compared to other audio-visual representation models. The state-of-the-art performance on zero-shot counterfactual audio-image recognition further demonstrates the superiority of our method in understanding the deep semantic relationship between audio and visual modalities.

   & A-V & Tr. &  &  \\  & Pairs & Param &  &  &  &  \\   & & & mAP & R@1 & R@5 & mAP & R@1 & R@5 & mAP & R@1 & R@5 & mAP & R@1 & R@5 \\ Random & - & - & 0.25 & 0.02 & 0.12 & 0.25 & 0.02 & 0.12 & 0.17 & 0.02 & 0.06 & 0.17 & 0.02 & 0.06 \\ WAV2CLIP & ✓ & 11.7M & 2.80 & 0.76 & 3.08 & 4.01 & 1.14 & 4.42 & 2.52 & 0.58 & 3.16 & 3.47 & 1.12 & 4.34 \\ AudioCLIP & ✓ & 134.1M & 0.98 & 0.22 & 0.85 & 2.50 & 1.00 & 2.83 & 3.10 & 1.00 & 4.02 & 4.43 & 1.58 & 5.92 \\  C-MCR & \(\) & 2.1M & **4.11** & **1.25** & **4.54** & **4.13** & **1.25** & **4.44** & **4.57** & **1.38** & **5.40** & **4.92** & **1.58** & **5.98** \\  

Table 1: Zero-shot audio-image retrieval results on AVE and Flickr-SoundNet. A-V pairs stands for whether training on paired audio-visual data; Tr. Param for Trainable parameters number.

Figure 2: Visualization of audio-to-image retrieval on AVE and Flickr-SoundNet.

### Evaluation of 3D-language Representations

In order to verify the performance of the 3D-language representation obtained by connecting ULIP-2 and CLIP, we evaluate the zero-shot 3D point cloud classification accuracy on ModelNet40, and the results are shown in Table 4. Our C-MCR achieves state-of-the-art zero-shot classification results compared with methods trained on 3D-language data. The advanced performance in the 3D-language field further demonstrates the great potential of C-MCR to learn contrastive representations for modalities lacking paired data.

### Ablation Studies

We conduct ablation studies on audio-image retrieval over AVE and Flickr-SoundNet to examine the effectiveness of our method. All the results are presented in Table 5 and Figure 3.

**Semantic Consistency.** We use a softmax function to softly aggregate embeddings in memory and produce the semantic consistent embedding in Equation 2. For comparison, Row I selects the embedding in the memory with the highest similarity as generated embedding, while Row J randomly selects an embedding in the memory. Compared to Row I, the significantly better results in Rows J and K highlight the necessity of inter-modality semantic consistency. Compared to the approach of hardly selecting one embedding, as in Row I, our soft clustering of memories slightly improves the performance by generating more diverse embeddings.

**Semantic Completion.** By comparing H and K, we can find that semantic bias in the MCR space indeed dramatically affects the learning of connections, and adding noise to embeddings can effectively alleviate this issue by enhancing semantic completeness and robustness. The results in G and K demonstrate that our re-normalized operator in Equation 3 is beneficial for learning alignment on the unit sphere. Moreover, in Figure 3, we vary the variance \(^{2}\) of noises and report its effect. Generally, the performance is not sensitive to changes in \(^{2}\).

**Inter-MCR alignment.** Comparison between D, E, and K demonstrates that the connection learned from the native text-text pairs is much more crucial than from the pseudo-consistent audio and visual embeddings, and using both native text-text pairs and pseudo audio-visual pairs produces the best performance. Furthermore, if no connections are made, as in Row F, the image and audio embeddings would have no semantic relationship since the original CLIP and CLAP spaces are isolated.

**Intra-MCR alignment.** Results in A, B, and K indicate that alignment within either CLIP or CLAP can provide a relatively reliable connection, and aligning both leads to even better results. Conversely, not aligning CLIP and CLAP as in C results in a connection not well adapted to audio-visual input. More importantly, the results in C are similar to those in Row D, where connections are not learned from text-text pairs. This observation indicates that the primary function of intra-MCR alignment is to alleviate the modality gap, thereby enabling the more stable connections learned from text-text pairs to adapt to audio-visual inputs.

   Method & Top1 & Top3 & Top5 \\  ReCon  & 61.2 & 73.9 & 78.1 \\ CG3D  & 48.7 & 60.7 & 66.5 \\ ULIP  & 60.4 & 79.0 & 84.4 \\ ULIP-2  & **74.0** & 86.5 & 90.0 \\  C-MCR & 64.9 & **87.0** & **92.8** \\   

Table 4: Zero-shot 3D point cloud classification results on ModelNet40.

    &  &  \\  &  &  &  &  &  \\  Attention  & 37.20 & 38.70 & 17.10 & 28.70 \\ DMC  & 29.10 & 38.00 & 23.90 & - \\ DSOL  & 51.40 & 43.60 & 29.91 & - \\ TURN  & 33.70 & 45.20 & 34.60 & 39.10 \\ EZ-VSL  & - & - & 38.85 & 39.54 \\ SLAVC  & - & - & 39.80 & - \\  WAV2CLIP  & 47.49 & 53.80 & 36.91 & 39.58 \\ AudioCLIP  & 30.56 & 37.16 & 43.93 & 45.96 \\ C-MCR(Ours) & **53.78** & **56.09** & **48.08** & **48.69** \\   

Table 2: Zero-shot audio-visual source localization on MUSIC-Solo and VGGSS.

## 5 Conclusion

This paper proposes Connecting Multi-modal Contrastive Representation (C-MCR), a new flexible and training-efficient method for learning multi-modal contrastive representation. C-MCR eliminates the need for large-scale, high-quality data pairs and instead extends the acquired multi-modal alignment knowledge in existing MCRs. By connecting existing MCRs via overlapping modality, we are able to discover more generalized contrastive representations across a broader range of modalities. Experimentally, we learn state-of-the-art audio-visual contrastive representations by connecting CLIP and CLAP through texts, and advanced 3D-language representations by connecting CLIP and ULIP via images. Despite not utilizing any paired data, the representations obtained by C-MCR significantly outperform previous representations learned from data pairs on different downstream tasks.