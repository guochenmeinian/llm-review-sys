# No-Regret Bandit Exploration

based on Soft Tree Ensemble Model

 Shogo Iwazaki

LY Corporation

Tokyo, Japan

siwazaki@lycorp.co.jp

&Shinya Suzumura

LY Corporation

Tokyo, Japan

suzumur@lycorp.co.jp

###### Abstract

We propose a novel stochastic bandit algorithm that employs reward estimates using a tree ensemble model. Specifically, our focus is on a soft tree model, a variant of the conventional decision tree that has undergone both practical and theoretical scrutiny in recent years. By deriving several non-trivial properties of soft trees, we extend the existing analytical techniques used for neural bandit algorithms to our soft tree-based algorithm. We demonstrate that our algorithm achieves a smaller cumulative regret compared to the existing ReLU-based neural bandit algorithms. We also show that this advantage comes with a trade-off: the hypothesis space of the soft tree ensemble model is more constrained than that of a ReLU-based neural network.

## 1 Introduction

The stochastic bandit framework is a powerful tool for addressing sequential decision-making tasks in uncertain environments. A significant challenge in applying stochastic bandits is managing large action spaces. For example, in recommendation systems, there is often a vast action space generated by various combinations of users and items . Standard algorithms designed for finite-armed bandits are inadequate in these scenarios. Consequently, numerous studies have focused on structurally modeling the reward process and using limited observed data to estimate rewards for unobserved actions. These approaches include algorithms that employ estimation methods such as linear models , kernel regression , and neural networks , which are referred to as linear bandit (LB), kernel bandit (KB), and neural bandit (NB) respectively. The effectiveness of these algorithms largely depends on the accuracy of the underlying reward models. Therefore, developing the bandit algorithms that leverage suitable reward estimation models is crucial.

Motivated by these considerations, this paper explores the stochastic bandit algorithm using tree ensembles, a model type that has gained popularity following neural networks but remains relatively underexplored in the bandit context. Specifically, we focus on the soft tree ensemble model, which has recently been the subject of both practical and theoretical investigations and has demonstrated strong empirical performance on tabular data . Unlike hard trees, which update decision rules greedily and sequentially, soft trees employ gradient descent to update decision rules for the entire tree. This characteristic of soft trees facilitates the extension of existing analyses of NB and ensures a no-regret performance under suitable assumptions.

Related works.In the field of stochastic bandits, prior research has established various structural assumptions about underlying rewards. For instance, the assumption of Lipschitz continuity of rewards is explored in Lipschitz bandits , linearity of rewards is examined in LB , and more generally, the assumption that rewards lie in a known reproducing kernel Hilbert space (RKHS) is studied in KB .

Our paper studies a type of bandit algorithm that employs a tree structure model, a topic with limited prior exploration. Feraud et al.  proposed a bandit algorithm using random forests, but the theory of their algorithm exhibits linear dependence on the number of actions, making it unsuitable for large action spaces. Elmachtoub et al.  introduced a Thompson sampling-style algorithm utilizing decision trees; however, their algorithm's construction relies on heuristics and does not provide a regret guarantee.

Additionally, our theory is closely related to NB. Zhou et al.  proposed an upper confidence bound (UCB) algorithm using a deep neural net (DNN) regressor, and Zhang et al.  extended this analysis to Thompson sampling. Their analysis yields a regret upper bound of \(}()\), where \(\) denotes the effective dimension of the problem, and \(}()\) represents an order notation that ignores logarithmic dependence. However, generally, DNNs employing ReLU activation functions lead to \(=}(T^{(d-1)/d})\), resulting in super-linear growth of \(()\) regret, which becomes meaningless . Several studies address this issue by employing algorithms in the form of a sup-variant of UCB  or phased elimination-style algorithms [7; 26], proving a regret upper bound of \(}(T^{(2d-1)/(2d)})\)[23; 24; 30]. These studies combine theoretical analysis via the neural tangent kernel (NTK) [4; 19] for DNN regression with regret analysis techniques from KB, constructing algorithms and performing regret analysis. Our proposed algorithm can be seen as a generalization of NB theory using a soft-tree regressor from DNN.

Contributions.Our contributions are as follows:

* In Sec. 3.1, we introduce a new UCB-based algorithm: soft tree-based upper confidence bound (ST-UCB), which leverages the soft tree ensemble model. This algorithm can be considered an extension of the existing NN-UCB algorithm , incorporating the theory of the tree neural tangent kernel (TNTK) in soft trees [21; 22]. To our knowledge, this paper represents the first effort to extend the theory of NB to a tree-based structural model.
* In Sec. 3.2, we derive several non-trivial properties of the soft tree ensemble model. These include the decay rates of eigenvalues of the TNTK (Lemma 3.1), concentration properties of TNTK (Lemma 3.2), and upper bounds on the spectral norm of the Hessian matrix (Lemma 3.3). Leveraging these results, we demonstrate that the ST-UCB algorithm achieves a regret of \(}()\) under appropriate regularity conditions.
* In Sec. 4, we elucidate the distinctions in properties and assumptions between the existing NN-UCB and ST-UCB algorithms. Specifically, while NN-UCB generally lacks a no-regret guarantee in general action (or context) spaces, ST-UCB consistently offers a no-regret guarantee across general action spaces. Additionally, we examine the relation between the hypothesis spaces induced by the TNTK and those induced by the NTK using ReLU activation. This comparison reveals that the hypothesis space derived from soft trees, although more constrained, may lead to lower regret.

## 2 Preliminaries

Problem setting.We consider a sequential decision-making problem whose goal is to maximize the total reward under bandit feedback. Let \(f:\) be an unknown reward function, where \(^{d}\) is a finite set of action candidates. At each time step \(t\), the environment reveals an action set \(_{t}\); thereafter, the learner chooses an action \(_{t}\) and receives the corresponding reward \(y_{t}=f(_{t})+_{t}\), where \(_{t}\) is a noise random variable whose mean is zero. As a performance metric, we adopt the pseudo cumulative regret \(R_{T}_{t=1}^{T}[f(_{t}^{*})-f(_{t})]\), where \(_{t}^{*}*{arg\,max}_{_{t}}f()\). In our problem setup, the action set \(_{t}\) is allowed to change at each step \(t\). In addition to the standard bandit setup that assumes \(_{t}=\), this formulation includes a contextual bandit setup by setting \(_{t}=\{(_{t},)(_{t})\}\), where \(_{t}\) is a context vector at step \(t\), and \((_{t})\) is the corresponding action set.

Soft tree ensemble.At each time step \(t\), our algorithm constructs a soft tree-based estimator of the reward function \(f\). We describe the definition of soft trees based on Kanoh and Sugiyama . Now, let us consider \(M_{+}\) perfect binary trees whose depths are \(_{+}\). Note that each tree has \( 2^{}-1\) internal nodes and \( 2^{}\) leaf nodes. Furthermore, for technical reasons, we assume that \(M\) is an even number. Let \(_{n}^{(m)}^{d}\) and \(_{l}^{(m)}\) be the parameters of the \(n\)-th internal and \(l\)-th leaf node of the \(m\)-th tree, respectively. We index these parameters according to breadth-first ordering, as described in the left plot of Fig. 1. Moreover, we also denote all internal and leaf node parameters as \(^{(m)}(_{1}^{(m)},,_{}^{(m) })^{}^{d}\) and \(^{(m)}(_{1}^{(m)},,_{}^{(m)})^{} ^{}\). The output of a standard decision tree is obtained as the parameter of some leaf node, which is chosen deterministically based on the hard-splitting rules of internal nodes. On the other hand, the output of the soft tree is given by replacing the hard-splitting operation of the standard decision tree with a probabilistic one. Specifically, given parameters \(^{(m)}(^{(m)},^{(m)})^{}\) and any input \(\), the corresponding output \((;^{(m)})\) of the \(m\)-th soft tree is defined as

\[(;^{(m)})=_{l=1}^{}_{l}^{(m)}p_{l }(;^{(m)}),\ \ \ \ p_{l}(;)=_{n=1}^{}(_{n}^{} )^{_{l n}}[1-(_{n}^{})]^{ _{n l}}.\]

Here, \(_{l n}\) and \(_{n l}\) are indicator functions. If the \(l\)-th leaf node belongs to the left (resp. right) sub-tree whose root is the \(n\)-th internal node, \(_{l n}\) (resp. \(_{n l}\)) is one; otherwise, zero. Furthermore, \(():\) is a _soft_ decision function. The right plot of Fig. 1 shows an illustrative image of the calculation of \(p_{l}()\). As with , we use the scaled error function \((_{n}^{})(_{n }^{})+\) with some pre-specified scaling parameter \( 0\), where \((b)=}_{0}^{b}(-z^{2})z\) for any \(b\). By aggregating \(M\) soft trees, the whole output \(h(;)\) of the soft tree ensemble model is defined as \(h(;)=_{m=1}^{M}(;^{(m)})/ {M}\), where \((^{(1)},,^{(M)})^{ }^{M(d+)}\). Under the model structures as described above, the training of the model parameters \(\) is conducted based on the gradient descent optimizer, which aims to minimize some pre-specified loss functions. In our algorithm, we adopt a regularized square loss, whose detailed definition is given in Sec. 3.1.

Neural tangent kernel theory for overparameterized model.The neural tangent kernel (NTK)  is an effective theoretical tool for understanding the learning properties of overparameterized neural networks. Let \(h_{}(;):^{d}\) be a feed-forward neural network with a ReLU activation function, \(L\) hidden layers whose width is \(M\), and network parameters \(\). Given any fixed inputs \(,}^{d}\), and \(}_{0}(,)\), it has been shown that the inner product \(_{}h_{}(;}_{0}), _{}h_{}(};}_{0})\) of gradients converges to a fixed kernel function \(k_{}(,})\) (i.e., \(_{}h_{}(;}_{0}), _{}h_{}(};}_{0}) []{p}k_{}(,})\) as \(M\)). The kernel function \(k_{}\) is called the NTK. Moreover, in the overparameterized regime, \(h_{}(;)\) trained with gradient descent with an infinitesimally small learning rate coincides with the kernel ridge-less regressor \(h_{}()\), whose kernel function is \(k_{}\). This property motivates us to analyze NB problems by bridging original NB to KB problems whose underlying kernel function is the NTK. Indeed, some existing works  show the regret upper bound of NB problems by carefully combining NTK theory with existing theoretical tools of KB. In our paper, we consider soft tree variants of these existing works.

Recently, Kanoh and Sugiyama  generalized the NTK theory to the soft tree ensemble model. Let \((,)_{}h(;) ^{p}\) be the gradient vector of the soft tree ensemble model at parameter

Figure 1: An illustrative image of a soft tree structure with \(=3\). As shown in the left plot, we have \( 2^{}-1\) internal nodes (green) and \( 2^{}\) leaf nodes (orange), indexed using breadth-first ordering. The right plot shows an illustrative example where a soft tree calculates the weight probabilities \(p_{l}()\) for the leaf nodes.

\(^{p}\), where \(p M(d+)\) denotes the total number of parameters. Then, given fixed inputs \(,}\) and \(}_{0}(0,_{p})\), the inner product \((,}_{0}),(},}_{0})\) has also been shown to converge in probability to some kernel function \(k_{}(,})\) as the number of ensemble models \(M\) grows infinitely (see Theorem 1 in ). This limiting kernel \(k_{}\) is called the _tree neural tangent kernel_ (TNTK) as an analogy to the NTK and is defined as follows:

\[k_{}(,})=2^{}^{}}((,}))^{-1}}( ,})+(2(,}))^{},\] (1)

where:

\[(,}) =(^{}}}{^{}+0.5)(^{2}}^{ }}+0.5)}})+,\] (2) \[}(,}) =}{}^{ })(1+2^{2}}^{}})-4^{4}( })^{2}}}.\] (3)

It should be noted that even if we follow the existing NTK-based techniques of NB, generalizing the result of Kanoh and Sugiyama  to the analysis of sequential decision-making tasks is non-trivial. Specifically, the existing analysis of NB heavily relies on the following results of ReLU-based NTK: i) non-asymptotic bounds of NTK , ii) the spectral properties of the Hessian matrix around the initial model parameters , and iii) the upper bounds of maximum information gain (MIG) of NTK , which measure the complexity of the KB problem depending on the underlying kernel. These results are unique to DNN architectures with a ReLU-based activation function and are not applicable to the soft tree ensemble model.

## 3 UCB strategy based on soft tree ensemble model

### Proposed algorithm: ST-UCB

The pseudo-code of our proposed algorithm, soft tree-based UCB (ST-UCB), is shown in Algorithm 1. ST-UCB is interpreted as the soft tree-based variant of NN-UCB . We summarize each part of ST-UCB below.

Initialization.ST-UCB first chooses the initial parameter \(_{0}^{p}\) for the gradient descent method as follows. Let \(_{}(0,_{p/2})\) be a base initial parameter, with \(p=M(d+)\). Using \(_{}\), we set the initial parameters \(_{0}\) as \(_{0}=(_{0+}^{},_{0-}^{})^{}\), where \(_{0+}^{p/2}\) and \(_{0-}^{p/2}\) are defined as \(_{0+}=(_{}^{(1)},_{}^{(1) },,_{}^{(M/2)},_{}^{(M/2) })^{}\) and \(_{0-}=(_{}^{(M/2+1)},-_{}^ {(M/2)},,_{}^{(M)},-_{}^{(M )})^{}\), respectively. This initialization procedure ensures that the initial model output is \(0\) (i.e., \(h(;_{0})=0\) for all \(\)), which is essential for our theoretical analysis.

Learning.At each step \(t\), ST-UCB learns the model parameter \(_{t}\) based on a regularized squared loss \(L_{t}()_{i=1}^{t}(h(_{i};)-y_{i})^{2} +\|-_{0}\|_{2}^{2}\), where \(>0\) is a regularization parameter.

UCB-based selection of \(_{t}\).At each step \(t\), ST-UCB selects \(_{t}\) as follows:

\[_{t}*{arg\,max}_{_{t}}[h(; {}_{t-1})+_{t-1}()],\] (4)

where \(_{t-1}^{2}()=(;_{0})^{}( _{p}+^{-1}_{t-1}_{t-1}^{})^{-1}(; _{0})\) with \((;)_{}h(;) ^{p}\) and \(_{t-1}((_{1};_{0}),,(_ {t-1};_{0}))^{p t}\). In ST-UCB, the quantity \(_{t-1}^{2}()\) quantifies the uncertainty of the model output \(h(;_{t})\) and is essential for the construction of confidence bounds. Furthermore, the quantity \(_{t-1}^{2}()\) is interpreted as the predictive variance of a Bayesian linear regression whose feature map is the gradient of the initial model output \(h(;_{0})\). We note that a similar quantity is leveraged in existing NB algorithms [23; 30; 41].

### Theory of ST-UCB

Assumptions for theoretical analysis.We make the following assumptions for our theory:

**Assumption 3.1**.: _(i) The output noise \(_{t}\) is conditionally \(\)-sub-Gaussian for some \(>0\). Specifically, \([(_{t})_{t-1}](^{2} ^{2}/2)\) holds for any \(t[T]\{1,,T\}\) and any history \(_{t-1}(_{1},y_{1},,_{t-1},y_{t-1})\). (ii) The input space \(^{d}\) is a subset of the hyper-sphere \(^{d-1}\{^{d}\|\|_{2}=1\}\). (iii) The underlying reward function \(f\) is an element of the RKHS corresponding to \(k_{}\), where \(k_{}\) is the TNTK induced by the same soft tree structure used in ST-UCB. (iv) The RKHS norm of \(f\) is bounded by a known constant \(B<\). That is, \(\|f\|_{} B\) holds, where \(\|\|_{}\) denotes the RKHS norm corresponding to \(k_{}\)._

**Remark 3.1**.: _In Assumption 3.1, (i) is the standard assumption for the stochastic bandit problem and is quite mild. For example, Bernoulli, Gaussian, and any bounded reward models are included in this assumption. Assumption (ii) is often assumed in existing NB literature [23; 24; 30; 41; 40] and holds without loss of generality by transforming the original input space through a bijection map. For example, given any original input space \(}^{d}\), we can construct a new input space \(\) on the hyper sphere \(^{d}\) as \(=\{(^{-1}}^{},(1-\|} \|_{2}^{2})^{1/2})^{}}} \}^{d}\), where \(=_{}}}\|}\|_{2}\). Assumptions (iii) and (iv) are similar to those in existing NB works [23; 24; 30]. The only difference is that we use TNTK instead of NTK to define the hypothesis space (RKHS) to which \(f\) belongs. We omit the basic definition and properties of RKHS; see, e.g.,  for details. In Sec. 4, we further discuss the relationship between the RKHSs corresponding to NTK and TNTK._

Similar to NB with ReLU, our theoretical guarantees rely on two crucial tools in the context of KB. The first is the maximum information gain (MIG) , which quantifies the complexity of the problem in the context of kernel-based sequential decision-making tasks. MIGs depend on the underlying kernels, and their upper bounds have been provided when using well-known kernels, including the NTK corresponding to NNs with ReLU [23; 35; 36]. We show the upper bound of MIG when the underlying kernel is TNTK. The second tool is the confidence bound. Constructing valid confidence bounds is crucial for obtaining meaningful regret bounds in stochastic bandit algorithms. These two elements are not only essential for the theoretical analysis of ST-UCB but also of independent interest in general sequential decision-making problems. Hereafter, we present our MIG and confidence bounds results for our ST-UCB algorithm, concluding with the regret upper bound for ST-UCB.

Maximum information gain (MIG) of TNTK.Let us define the quantity \(_{T}\) as

\[_{T}=_{_{1},,_{T}} (_{T}+^{-1}_{T}),\] (5)where \(_{T}\) is the \(T T\) kernel matrix whose \((i,j)\)-th entry is \(k_{}(_{i},_{j})\). This \(_{T}\) is called the maximum information gain (MIG) since the quantity \(0.5(_{T}+^{-1}_{T})\) is equal to the information gain from \(T\) observations in a Gaussian process regression model, characterized by the covariance function \(k_{}\) and the noise variance parameter \(\). The following Theorem 3.1 is our main result about MIG, which shows that \(_{T}\) grows logarithmically.

**Theorem 3.1** (Upper bound of MIG of TNTK).: _Fix any \((0,)\), \(d 2\), \(_{+}\), and \(^{d-1}\). Then, \(_{T}=(^{d}T)\). Here, the implied constant depends on \(d\), \(\), and \(\)._

The proof of Theorem 3.1 is given in Appendix A.2. The analysis of MIG is well-studied in existing KB literature . The key component to quantify the upper bound of MIG is the decaying rate of the eigenvalues of the underlying kernel. The following lemma gives the decay rate of TNTK eigenvalues, which plays a central role in the proof of Theorem 3.1.

**Lemma 3.1** (Eigendecomposition of TNTK).: _Fix any \(d 2\), \((0,)\), and \(_{+}\). Furthermore, let us define \(N_{d,n}\) as \(N_{d,n}=n+d-3\\ d-2\), for any \(n\), where \(a\\ b:=\) is a binomial coefficient. Then, for any \(,}^{d-1}\), the TNTK corresponding to \(\) and \(\) satisfies_

\[k_{}(,})=_{n=0}^{}_{j=1}^{N_{d, n}}_{n}Y_{n,j}()Y_{n,j}(}),\] (6)

_where \((_{n})_{n}\) and \((Y_{n,j})_{n,j[N_{d,n}]}\) are eigenvalues and eigenfunctions of (the integral operator of) TNTK that satisfy \(_{0}_{1} 0\). In addition, for any \(n\), the eigenvalue \(_{n}\) satisfies_

\[_{n} C_{,}^{(1)}(-n(1+ })),\] (7)

_where \(C_{,}^{(1)}>0\) is a constant, which depends on \(\) and \(\)._

**Remark 3.2**.: _The eigenfunctions \((Y_{n,j})_{j[N_{d,n}]}\) are known as spherical harmonics of degree \(n\) with multiplicity \(N_{d,n}\) (see, e.g., ). Furthermore, on the hyper-sphere \(^{d-1}\), the kernels that have rotationally invariant form can be represented in the form of Eq. (6). TNTK and NTK with ReLU activation function are included in the rotationally invariant class of kernels; therefore, NTK can also be decomposed as Eq. (6) , while corresponding eigenvalues differ from those of TNTK._

The proof of Lemma 3.1 is given in Appendix A.1. Lemma 3.1 demonstrates the exponential eigenvalue decay of TNTK, in contrast to the polynomial eigenvalue decay of NTK with ReLU activation . This difference leads to faster convergence of ST-UCB compared to NN-UCB, albeit with a smaller corresponding RKHS of TNTK. We discuss more details in Sec. 4.

Confidence bound.The following shows the confidence bounds for the soft tree-based model.

**Theorem 3.2** (Confidence bounds based on the soft tree ensemble model).: _Suppose Assumption 3.1 holds. Fix any \((0,1)\), \(>0\), \( 1\), and \( 2\). Let \(_{}()[k_{}(,})]_{,}}^{|| ||}\) and \(_{0}=_{}(_{}())>0\) be the kernel matrix over \(\) and the minimum eigenvalue of \(_{}()\), respectively. If the number of soft tree ensemble models \(M\) is sufficiently large to satisfy \(M(T,^{-1},B,,2^{},_{0}^{-1},| |,(1/))\) and the learning rate \(\) satisfies \(((T^{2}2^{}^{2}(M/)+)^{-1})\), then, the following event holds with probability at least \(1-\):_

\[ t[T],,|f()-h(;_{t -1})|(( T)^{2}( M)}{})+ _{t-1}(),\] (8)

_where:_

\[=(+}{M^{1/2}}}+(  T)( M^{3/2})}{}+T^{3/2}( T)( M)(1-2)^{J/2}).\] (9)

**Remark 3.3**.: _The minimum eigenvalue \(_{0}\) of the kernel matrix of TNTK is guaranteed to be strictly positive if \(^{d-1}\). See Proposition 1 in ._We provide the proof of Theorem 3.2 in Appendix B.3 with the precise conditions about \(M\) and the dependence of constant factors. Our proof strategy for Theorem 3.2 follows the existing analysis of confidence bounds in NB works; however, the application of their proof techniques to the soft tree regressor is not straightforward. Specifically, the existing proof of the confidence bounds in NB depends on the concentration results of NTK (Theorem 3.1 in ), and the spectral norm bounds of the Hessian matrix of NN (Theorem 3.2 in ). To prove Theorem 3.2, we provide the following soft tree versions of their results.

**Lemma 3.2** (Concentration to TNTK).: _Fix any \(\), \(}^{d-1}\), \((0,1)\), and \((0,C^{(2)}_{,})\) with \(C^{(2)}_{,}=2^{2+2}^{2}C\). If \(M\{C^{(2)2}_{,},2^{2}\}^ {-2}(16/)\), then,_

\[(|k_{}(,})-(, {}_{0}),(},_{0})| 4) 1-,\] (10)

_where \(_{0}\) is the initial parameter of ST-UCB, and \(C\), \(>0\) are absolute constants._

**Lemma 3.3** (Spectral norm upper bound).: _For any \((0,1)\) and \( 1\), with probability at least \(1-\), the following holds for any \(R>0\), \(^{p}\), and \(^{d-1}\):_

\[\|-_{0}\|_{2} R\|(,)\|_{,}(R+)^{2}}{} +2}M}{},\] (11)

_where \((,)_{}^{2}h(;)^{p p}\) is the Hessian matrix of the model output, and \(C^{(3)}_{,}=^{2}2^{2}\). Furthermore, for any \(^{p p}\), \(\|\|_{^{p-1}}\|\|_{2}\) denotes the spectral norm._

The proofs of Lemma 3.2 and Lemma 3.3 are given in Appendix B. By carefully combining Lemma 3.2 and Lemma 3.3 with the existing proof strategy of NB, we derive Theorem 3.2. The overview of the proof is summarized in Appendix B.3.1.

Regret upper bound of ST-UCB.By combining Theorem 3.1 and Theorem 3.2 with the standard proof technique of the kernelized UCB algorithm, we obtain the \(}()\) regret upper bound for ST-UCB as stated in the following theorem. The proof is provided in Appendix C.

**Theorem 3.3**.: _Suppose that Assumption 3.1 holds. Fix any \((0,1)\), \( 1\), \(>0\), and \( 2\). Furthermore, assume that the confidence width parameter \(\) satisfies Eq. (9). If the number of soft tree ensemble models \(M\) and the total step size \(J\) of the gradient descent are sufficiently large to satisfy \(M(T,^{-1},B,,2^{},_{0}^{-1},| |,(1/))\), and the learning rate \(\) satisfies \(((T^{2}2^{4}^{2}(M/)+)^{-1})\), then, the following holds with probability at least \(1-\):_

\[R_{T} 1+(B+1+}+1+)})+1)}{(1+ ^{-2})}}=(^{d}T).\] (12)

## 4 Comparison of NN-UCB and ST-UCB

Comparison of regret.In the existing NN-UCB algorithm , a regret upper bound of \(()\) is provided, where \(\) represents the effective dimension of ReLU-based NTK. It is generally known that the worst-case bound of the effective dimension and MIG are equivalent up to logarithmic dependencies . Considering the upper bound on MIG of NTK, \(_{T}^{()}=}(T^{(d-1)/d})\), the regret of NN-UCB becomes \(}(T^{(d-1)/d+1/2})(=}(_{T}^{()}))\). This results in a super-linear regret, and meaningful guarantees for NN-UCB are not achievable without further restricted assumptions on the input set \(_{t}\) (e.g., see the discussion in Appendix D in ). To address these issues in a general setting, it is necessary to construct more complex algorithms that incorporate concepts such as a sup-variant of UCB  or phased elimination , yielding a regret upper bound of \((^{()}}T)\). In contrast, due to Theorem 3.1, the MIG of TNTK \(_{T}^{()}\) diverges on a logarithmic scale. Therefore, ST-UCB achieves a regret bound of \(}()\) without requiring additional assumptions on the input set \(_{t}\), maintaining a simple UCB-style algorithmic structure.

Comparison of hypothesis space.In our analysis, we assume in Assumption 3.1 that the reward function \(f\) belongs to the RKHS \(_{}\) associated with TNTK. Conversely, in existing NB research, it is assumed that \(f\) belongs to the RKHS \(_{}\) associated with NTK. By combining Lemma 3.1 with the well-known Mercer's representation theorem (e.g., Theorem 4.51 in ), we derive the following lemma, which describes the relationship between \(_{}\) and \(_{}\).

**Lemma 4.1**.: _Fix any \( 0\) and \(_{+}\), and define the corresponding TNTK as \(k_{}:^{d-1}^{d-1}\). Let \(k_{}:^{d-1}^{d-1}\) be an NTK corresponding to a ReLU-based \(L\)-layer neural network structure, where \(L\) is any natural number. Then, \(_{}_{}\) holds, where \(_{}\) and \(_{}\) are RKHSs corresponding to \(k_{}\) and \(k_{}\), respectively._

The proof of Lemma 4.1 is provided in Appendix D. Lemma 4.1 indicates that the regret upper bound of ST-UCB is guaranteed in a more constrained hypothesis space compared to NN-UCB. While NN-UCB generally does not guarantee a no-regret property, the \(}()\) guarantee in ST-UCB can be interpreted as being due to focusing on a more constrained hypothesis space.

It should be noted that whether this property is specific to the tree structure of the model or depends on the choice of the soft-decision function is unknown. We constructed and analyzed our algorithm based on the definition of soft trees from ; however, we conjecture that by using a more non-smooth soft decision function, although the regret may degrade to a level similar to NN-UCB, we can align the hypothesis spaces used in NN-UCB and ST-UCB to be almost the same. We leave the detailed analysis to future work.

## 5 Numerical experiments

In this section, we compare ST-UCB and NN-UCB to empirically demonstrate the usefulness of the tree-based model. Additionally, to evaluate the characteristics of UCB-based algorithms, we include \(\)-greedy based ST-greedy and NN-greedy as comparative methods.

Real-world dataset.We use _Energy Efficiency_ dataset  registered in UCI Machine Learning Repository . This dataset provides the load required to maintain comfortable indoor air conditions for each of the 768 residential buildings - two types of data are provided as non-negative real values: heating load (HL) and cooling load (CL). For each building, eight types of context are included as explanatory variables. We randomly sample residential buildings without replacement to create a dataset of \( 768\) arms, where \(\) is a hyperparameter. The inputs are denoted as \(=(}_{},})\), where \(}_{}\) is a \(\)-dimensional one-hot vector used to identify the arms, and \(}\) is a vector that aggregates the eight types of context. In most real-world data, the rewards depend not only on the observable context \(}\) but also on other information. To account for arm-specific characteristics that cannot be represented by \(}\) alone, we use \(}_{}\) as part of the input.

We consider each arm of the multi-armed bandit problem as an individual residential building, and we define the reward of the arm selected in each round as \(f_{t}=-(_{t}+_{t})\). Additionally, we standardize the rewards across \(\) arms to have a mean of 0 and a standard deviation of 1.

Synthetic dataset.We evaluate the algorithms using synthetic data similar to that used in . Here, the number of arms is set to 20, and the dimension of the input vector \(\) for each arm is set to 50. Additionally, the input vectors are chosen uniformly at random from the unit ball. We consider the three reward functions: (i) \(f^{(1)}()=10(^{})^{2}\), (ii) \(f^{(2)}()=^{}^{}\), and (iii) \(f^{(3)}()=(3^{})\) where \(^{50}\) is randomly generated from uniform distribution over unit ball, and each entry of \(^{50 50}\) is randomly generated from standard normal distribution. Similar to the real-world dataset, we standardize the rewards across all arms.

Setup.We define the cumulative regret up to round \(T\) as \(R_{T}=_{t=1}^{T}f^{*}-f_{t}\) where \(f^{*}\) represents the maximum reward among all arms. We assume that the response used for training the machine learning model is generated from \(y_{t}=f_{t}+_{t}\) where \(_{t}\) is randomly drawn from a normal distribution with mean \(0\) and standard deviation \(_{}=0.2\). Since the rewards are standardized, this setting of \(_{}\) effectively acts as noise.

In this experiment, we will use an \(\)-greedy based algorithm as an additional comparative method; In each round, an arm is selected randomly with a probability of \(\), while the arm with the highest predicted value from the machine learning model is selected with a probability of \(1-\). Here, we will perform a grid search to choose the value of \(\) from the three candidates \(\{0.05,0.1,0.2\}\). Meanwhile, in UCB-based algorithms, \(\) is provided as a parameter to control the degree of exploration. We use a grid search to select the value of \(\) from the three candidates \(\{0.01,0.1,1\}\).

We employ a fully connected neural network model with two intermediate layers. Including the input and output layers, the total number of layers is four. Each of the two intermediate layers contains 33 units, one of which is a bias term. As for the tree-based model, we consider an ensemble of four soft-trees, the depth of each soft-tree is three. The regularization coefficient \(\) for the parameters is fixed at \(10^{-4}\), regardless of the machine learning model. Supplementary details related to the implementation of the algorithms are summarized in Appendix F.1.

Results.The results for each algorithm are shown in Fig. 2. In real-world dataset, three different numbers of arms were considered, with \(\) being one of {20, 40, 60}. These experiments were conducted over 10 episodes with different initial parameters \(_{0}\) for the model. Additional results without the grid search for \(,\) are summarized in Appendix F.2.

In all settings of real-world dataset, the regret of ST-UCB was not smaller in the early rounds, but the increase in the cumulative regret became more gradual as the rounds progressed. For example, in the setting of \(=60\), after round 150, there was no change in the cumulative regret of ST-UCB. However, from round 1 to 70, the regret of ST-UCB was relatively high compared to other methods. In our experiment, UCB-based policies (NN-UCB, ST-UCB) tended to actively select arms that had not been chosen before in the early rounds. As the rounds increased, exploratory behavior was suppressed, and there was a stronger tendency to select only arms with high rewards. On the other hand, in policies based on \(\)-greedy (NN-greedy, ST-greedy), the exploration rate is kept at \(\) across all rounds. Therefore, the regret continues to accumulate gradually as the rounds increase, raising concerns about worsening cumulative regret over extended long rounds. In the \(f^{(1)}\) and \(f^{(2)}\) settings of synthetic dataset, ST-UCB outperformed the other policies, and the convergence stability of cumulative regret in \(f^{(3)}\) was comparable between ST-UCB and NN-UCB.

## 6 Conclusion and future direction

In this paper, we propose a new regret-minimization algorithm based on a soft tree ensemble model. Our analysis extends the theoretical framework of existing neural bandit (NB) approaches to the soft tree ensemble model, demonstrating, under appropriate assumptions, the achievement of \(}()\) regret. To our knowledge, this is the first application of NB theory to models other than neural networks; we believe that our work marks an important first step toward developing exploration and exploitation theory using various complex models beyond neural nets.

Figure 2: The average cumulative regret with one standard error. The experiment was conducted over 10 episodes with different initial parameters for the model.

Our future research directions are outlined below. Firstly, it is important to study the extension when employing hard decision trees. In this paper, as the scale parameter \(\) approaches infinity, the soft tree regressor approaches that of a hard tree. We conjecture that our algorithm also works in this regime; however, since our regret analysis assumes a fixed \(\), our proposed method is not guaranteed to maintain the no-regret property with a varying scale parameter \(\). Hence, a more careful theoretical treatment is needed for this extension. Secondly, we plan to generalize the theory to encompass more common learning methods of the ensemble tree model. Specifically, learning algorithms using hard trees often utilize optimization methods in a greedy format rather than gradient descent. Therefore, developing theoretical foundations for ensemble tree learning methods that are more practically applicable is crucial.