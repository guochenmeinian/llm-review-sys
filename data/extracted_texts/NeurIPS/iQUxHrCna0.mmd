# UdON: Universal Dynamic Online distillatioN for generic image representations

Nikolaos-Antonios Ypsilantis\({}^{*}\)\({}^{1}\)   Kaifeng Chen\({}^{2}\)   Andre Araujo\({}^{2}\)   Ondrej Chum\({}^{1}\)

\({}^{1}\)VRG, FEE, Czech Technical University in Prague  \({}^{2}\)Google DeepMind

Corresponding author: ypsilnik@fel.cvut.cz

###### Abstract

Universal image representations are critical in enabling real-world fine-grained and instance-level recognition applications, where objects and entities from any domain must be identified at large scale. Despite recent advances, existing methods fail to capture important domain-specific knowledge, while also ignoring differences in data distribution across different domains. This leads to a large performance gap between efficient universal solutions and expensive approaches utilising a collection of specialist models, one for each domain. In this work, we make significant strides towards closing this gap, by introducing a new learning technique, dubbed UdON (Universal Dynamic Online distillationN). UdON employs multi-teacher distillation, where each teacher is specialized in one domain, to transfer detailed domain-specific knowledge into the student universal embedding. UdON's distillation approach is not only effective, but also very efficient, by sharing most model parameters between the student and all teachers, where all models are jointly trained in an online manner. UdON also comprises a sampling technique which adapts the training process to dynamically allocate batches to domains which are learned slower and require more frequent processing. This boosts significantly the learning of complex domains which are characterised by a large number of classes and long-tail distributions. With comprehensive experiments, we validate each component of UdON, and showcase significant improvements over the state of the art in the recent UnED benchmark. Code: https://github.com/nikosips/UDON.

## 1 Introduction

Imagine you point your cellphone at anything, and it tells you what it is, be it tangerine chicken with rice, Mk1 Volkswagen Rabbit Cabriolet, statue of Aquaman, Pasadena City Hall, or Yorkshire Terrier. Such a product is the ultimate goal of fine-grained and instance-level visual recognition. The key component enabling such an application is a general-purpose image representation, or equivalently image embedding, designed to handle imagery of varied domains at scale. Traditionally, image embedding models have been developed for specific domains separately , such as landmarks , products , clothes , faces , to name just a few. However, as visual recognition applications grow in popularity and scope , it is impractical to handle images of different object types with specialized, per-domain models. A potential solution for this problem is to leverage recent foundation models, such as CLIP  or DINOv2 , which have been proposed to enable a wide variety of multimodal applications. Even though these models possess a broad visual understanding, they tend to lack detailed fine-grained knowledge off-the-shelf , which is critical in practice. For this reason, recent efforts aim at developing universal embedding solutions that can generalize to handle multiple fine-grained object types with a single model , ensuring scalability in real-world scenarios.

There are two main challenges in training such universal models addressed in the paper. First, it is difficult to encode detailed knowledge about many image domains in a single model. In several cases, features that are helpful for one object type may be useless for others. One example is the importance of color: while for food recognition it is critical to discriminate red curry from green curry, when recognizing car models both a red and a green Toyota Corolla LE 2024 would belong to the same class. This makes the training of universal models from data of different domains particularly hard, since the data may present conflicting peculiarities across domains, leading to sub-optimal learning. To address this issue, we propose a novel knowledge distillation approach, where one teacher model is trained for each domain individually, and a student universal embedding model learns from the collection of teachers. This setup allows the specialized teachers to capture domain-specific knowledge, which is then transferred into the universal embedding. However, if this distillation setup is deployed naively, one may incur substantial costs, as a separate teacher model would need to be trained for each domain. For this reason, we propose to share the backbone between all teachers and the universal student, reaching a solution that achieves high performance and incurs small additional training costs, as all models are jointly trained in an online manner - see Figure 1.

The second main challenge we highlight is that different domains may present data with vastly different distributions: _e.g._, while one domain may present a moderate number of classes with a roughly balanced number of samples, in others, the number of classes may be very large, and the distribution of samples long-tailed. This means that different domains may require different training curricula for a model to properly learn their characteristics. This leads us to propose a sampling technique that dynamically selects which domains will be processed at each point in the learning process based on the losses measured on the fly. With this method, we demonstrate significant improvements to the performance of the most challenging domains, which are learned slower and require more frequent processing compared to other domains.

**Contributions.** To summarize, in this work we introduce the following contributions. **(1)** We leverage knowledge distillation to infuse the **universal** embedding model with the learnings of specialist, per-domain teacher models. In our novel training setup, the model backbone for all teacher embeddings and the student universal embedding is shared, leading to an efficient method where all models are jointly trained in an **online** manner. Our findings show that sharing the backbone facilitates the distillation process significantly, even reaching performance that surpasses the distillation from

Figure 1: Training of a universal embedding on multiple fine-grained visual domains. The baseline approach of  (left) uses classification loss across training classes from all domains. It is prone to cancelling out contradicting cues from different domains. To overcome this issue, a naive multi-teacher distillation approach (middle) first trains one specialized teacher per domain (with a classification loss) to capture domain specifics, then distilts them to the universal embedding (student). Our proposed Universal Dynamic Online distillationN – UDON (right) jointly trains the specialized teacher embeddings and the universal embedding (student) with classification and, at the same time, distills the teacher embeddings to the universal embedding. Due to joint training of a shared backbone, UDON scales to a large number of domains.

separate specialist teachers. **(2)** To enable an appropriate training regime in a universal embedding setup, we propose to adapt the learning process **dynamically**, adjusting the sampling of image domains based on their losses, which appears suitable to visual knowledge spanning a range of fine-grained domains. We show that this can help substantially with more complex domains that require more frequent model updates to enhance their performance. **(3)** We perform comprehensive experiments on the recent UnED  dataset, that highlight the value of each proposed component, as well as compare our technique against competitor approaches. Our complete method, named **UDON** (**U**niversal **D**ynamic **O**nline distillatio**N**), showcases state-of-the-art results that boost Recall@1 by up to \(2.3\%\).

## 2 Related Work

**Knowledge distillation (KD).** Initially proposed to transfer the knowledge of large and complex models to smaller and faster ones , standard KD trains a light student to mimic the softmax outputs produced by a heavy teacher. Tailoring KD to representation learning, [27; 28] distill relations between image representations and  focus on retrieval rankings. Online KD  lifts the need for separate two-stage training for the teacher and the student and trains them simultaneously. Multi-teacher KD [20; 15] aims to transfer the knowledge of multiple teachers into one student model.  performs multi-teacher KD for a single domain visual retrieval, by aggregating the teacher relations in a single target that the student should mimic. Differently than , which proposes an online multi-teacher distillation approach that trains a different backbone for each teacher, we propose to share a common backbone between all the teachers and the student, showing improved performance while also being much more efficient.  combines online and multi-teacher knowledge distillation in a single multi-branch network, training an ensemble of teachers on the fly. Differently from [50; 24], each of the teachers in our KD approach is specialized to only a fraction of the data (a single domain), being relevant only to part of the universal embedding task. Similarly to , we also create an online multi-branch architecture, however the teachers are not updated by the other teachers' knowledge, as they are related to different visual domains. Additionally, our teachers transfer relational knowledge to the student, since our focus is on learning image embeddings, in contrast to  which only focuses on classification.

**Universal representation learning.** Learning a representation that generalizes and can be reused efficiently across visual domains is a long-standing goal in computer vision.  introduces domain-specific normalization to make classification networks generalize to multiple visual domains, while  introduces adapter modules to improve the accuracy of domain-specific representations.  proposes a multi-task vision and language universal representation trained on 12 different datasets. However, this body of work assumes knowledge of the test time domain, which does not hold in our setup. Recent large visual foundational models [26; 33] that are trained on large amounts of data with diverse objectives show great zero-shot performance on a number of downstream visual tasks, making them great candidates for universal embedding applications. However,  shows that these models, even though generalizing to many diverse domains, cannot effectively handle instance-level and fine-grained domains (which are the focus of this work) without further fine-tuning.  shows that appending an MLP projector between the objective and the representation used for the downstream tasks improves the generalisability of the representation, inspiring a multi-domain variant we use as a baseline in this work. In , a multi-domain representation for fine-grained retrieval is learned, which utilizes no labels for training. Differently from it, we focus on the supervised task setup of , which constitutes a much larger-scale problem that additionally includes instance-level domains.  introduces distillation as a way to learn universal representations, while  tailors multi-teacher distillation to universal embedding learning. Differently from [20; 10], we do not use task-specific backbones that are costly to scale across a large number of domains. While  tackles a universal embedding setup, the effectiveness of their method is only assessed on a small dataset, where three small domains at a time are distilled into a universal representation. In contrast, we tackle learning in a more practical large-scale setup, with an efficient approach that distills knowledge from eight diverse visual domains into the universal embedding. Recently, the UnED dataset was introduced in  as a new large-scale benchmark for universal embeddings. Their experiments considered the training of models only via classification objective, with different sampling and classifier configurations. Setting our approach apart is that we go beyond to capture detailed knowledge from diverse domains via distillation, besides proposing a more suitable training dynamic that can accommodate data from diverse domains. Concurrently, UNIC  proposes a universal classification model usingmulti-teacher distillation. While our approach trains a student embedding from multiple teachers specialized in fine-grained visual domains, UNIC distills foundational models trained for diverse tasks, such as semantic segmentation and classification, with both supervised and self-supervised objectives.

**Dynamic sampling.** When training in a multi-task setting, the sampling frequency of the different domains can greatly affect final performance. Poly-ViT  explored different samplings tailored to their multi-modal multi-task model, and concluded that sampling each domain with a weight proportional to the number of training steps that a corresponding specialized model needs to achieve maximum performance works the best, while  additionally comes to the same conclusion for the task of universal image embedding. This approach is costly with an increasing number of domains in hand, as one model for each domain needs to be trained, which inspires us to discover more efficient sampling strategies.  proposes Dynamic Stop-and-Go sampling, which updates the sampling weight of each domain based on the validation set accuracy. Differently from them, we propose to calculate the sampling weights only based on training loss, which doesn't require the expensive feature extraction of the validation set but can happen on the fly. A similar idea has been explored by  in the context of pre-training vision-language models, which is far from this work's, as we focus on learning multi-domain fine-grained image embeddings.

## 3 Proposed method

This section presents our proposed training method, Universal Dynamic Online distillationN (UDON), to learn the universal image embedding. UDON utilizes a pretrained Vision Transformer  as the image encoder, which is further fine-tuned with a combination of classification and distillation objectives. First, some preliminaries concerning the backbone architecture that we build upon are introduced, and afterward, the complete training pipeline is presented in detail.

### Preliminaries

The Vision Transformer  backbone produces the [CLS] token as a global representation of the image. Let \(e_{b}:^{D}\) denote the Vision Transformer as a function that takes an input image \(x\) and maps it to the [CLS] token \(e_{b}(x)^{D}\), compactly denoted as \(e_{b}\). The dimensionality \(D\) is backbone dependent and usually higher than the one required in the downstream task, hence projection to lower dimensional space is introduced. The final vector after projection is the universal embedding, denoted as \(e_{u}^{d}\), \(d<D\). Following standard practice used in image retrieval architectures , \(e_{b}\) and \(e_{u}\) are \(_{2}\) normalized. When referring to a batch of embeddings, we use capitalized notation, _e.g._, a batch of embeddings \(e_{u}\) is denoted \(E_{u}^{d B}\), where \(B\) is the batch size. For training with a classification loss on top of the universal embedding in the multi-domain setup, a Separate Classifier (SC) per domain is employed, classifying across the classes of that specific domain, an option justified by . In the following, the word "head" denotes both the projection to the embedding space and the classifier to which the projected embedding is input.

### Universal Dynamic Online distillatioN (UDON)

Our UDON training approach introduces an efficient multi-teacher distillation method, relying on a shared feature extraction backbone. The entire training pipeline is presented in Figure 2. The backbone produces the initial high dimensional image embedding, \(e_{b}\), for the samples of all domains. Given \(e_{b}\), in addition to projecting it into the universal embedding space \(e_{u}\) that is used at test time (as in ), we also project \(e_{b}\) to per-domain spaces, which constitute the teacher embeddings \(\{e_{t_{i}}^{D_{t}}\}_{i}\). Teacher \(i\) is only activated for samples of domain \(i\). Both the universal and the domain-specific (teacher) projections are realized by linear layers, and there is a domain-specific projection for each domain.

The universal embedding is trained with both classification and distillation objectives, calculated on batches containing a single domain at a time. While training with a classification objective allows grasping broad knowledge for several domains, it may lead to a sub-optimal model due to contradictory cues when combining data from all domains. Therefore, we employ distillation from the domain-specific teachers to infuse the universal model with domain-specific knowledge. The loss functions are presented below.

**Classification losses.** The universal and the domain-specific embeddings are trained with classification losses (Normalized Softmax Loss ), instantiated with separate classifiers for each domain. These losses update the backbone via gradients propagated through the teacher and the student heads, while teacher-specific or student-specific parameters are only updated via gradients from their respective heads. The \(i\)-th teacher (of domain \(i\)) classification loss \(_{cls}^{t_{i}}\) and the universal embedding (student) classification loss \(_{cls}^{u}\) are defined as:

\[_{cls}^{t_{i}}=-_{j=1}^{B}y_{j}(_{j}^{t_ {i}}) 42.679134pt 42.679134pt_{cls}^{u}=- _{j=1}^{B}y_{j}(_{j}^{u})\] (1)

where \(B\) is the batch size, \(y_{j}\) is the one-hot ground truth vector of sample \(j\), \(_{j}^{t_{i}}\) is the predicted probability vector for sample \(j\) produced from teacher of domain \(i\) and \(_{j}^{u}\) is the predicted probability vector for sample \(j\) produced from the universal embedding (student). It is important to note that the classifiers of the student and the teachers are different and that the student employs as many classifiers as the number of domains (SC).

**Distillation losses.** The student is tasked to match the teacher embedding of the corresponding domain by enforcing two separate distillation losses. The first is a relational distillation loss, which acts on batch similarity matrices. Given the student's batch embeddings \(E_{u}^{d B}\), its batch similarity matrix is formed as \(E_{u}^{}E_{u}^{B B}\). Similarly, for the \(i\)-th teacher's batch embeddings \(E_{t_{i}}^{D_{t} B}\), its batch similarity matrix is formed as \(E_{t_{i}}^{}E_{t_{i}}^{B B}\). The goal is for the student to learn detailed intra-domain similarities from the more powerful domain-specific teacher. Specifically, the student's intra-domain cosine similarities are encouraged to follow the \(i\)-th teacher's cosine similarities, when the batch of images comes from domain \(i\):

\[_{rel}^{t_{i}}=||E_{u}^{}E_{u}-E_{t_{i}}^{}E_{t_{i}}||^{2}.\] (2)

Additionally, the student is tasked to match its logits to the teacher's, minimizing their KL divergence, after scaling with identical temperature \(T\) and softmax normalization of both:

\[_{log}^{t_{i}}=((_{u}/T)\;\;|| \;(_{t_{i}}/T))\] (3)

where \(_{u}\) is the logit vector produced by the classifier on the student's side and \(_{t_{i}}\) is the logit vector produced by the classifier on the \(i\)-th teacher's side. The temperature \(T\) is shared across all teachers

Figure 2: **Block diagram of UDON’s training process. Each batch of size \(B\) contains images from a single domain (_e.g._, cars, natural world, etc). When a batch with domain \(i\) is processed, the \(i\)-th teacher head is used. Both the teacher and the student employ a classification loss (\(_{}^{t_{i}}\), \(_{}^{u}\)) on top of their batched logits (\(L_{t_{i}}\), \(L_{u}\)), predicting among \(C_{i}\) classes. The student is additionally trained via distillation, by learning intra-batch relationships (\(_{rel}^{t_{i}}\)) and logits (\(_{log}^{t_{i}}\)) with the domain teacher guidance. Note that the distillation losses are backpropagated only through the student’s head.**and the student. This loss provides a more global context, as it captures the similarities between an embedding and all of the class prototypes in the domain (which exist in the classifier), instead of only relating embeddings in a batch. Both distillation losses do not backpropagate through the domain-specific teacher head, as only the student should try to learn from the teacher. Distillation starts at the beginning of the training and it happens in an online manner, at the same time that the universal student and the teachers are trained with the classification losses. The total loss for a training batch, containing images of domain \(i\), is as follows:

\[_{total}=_{cls}^{t_{j}}+_{cls}^{u}+_{rel}^{t_{i}}+_{log}^{t_{i}}.\] (4)

**Dynamic domain sampling.** Each domain comes with its own training data, which differ in the number of classes and in the number of examples. Balancing of the domains is performed through sampling of the training data. In , training is performed with clean batches, _i.e._ each batch only contains examples from a single domain. Three different sampling schemes were compared in . In Dataset Size sampling, the datasets are sampled proportionally to their size, biasing towards large datasets. In Round-Robin (RR) sampling, the domains are sampled equally often in a cyclic order. The Specialist Steps sampling selects the domains proportionally to the number of steps the specialist at the particular domain needs to achieve maximum validation performance. Only the last approach takes into account the difficulty of the classification task in each domain. However, such sampling is static, does not reflect possible correlations in similar domains, and requires the training of a specialist for each domain prior to training the universal model.

We propose to sample each domain proportionally to the classification loss produced by the domain-specific embedding during training, relative to the corresponding loss of the other domain-specific embeddings. This way, the domains for which the training loss decreases slower than for other domains are sampled more for training. More specifically, we sample the domain of the next training batch out of the distribution which for each domain \(m\) assigns the probability:

\[P(m)=_{m}}{_{i=1}^{N}_{i}},\] (5)

where \(N\) is the number of domains, train loss\({}_{m}\) is the classification loss produced by the embedding of domain \(m\), and the denominator represents the sum of the corresponding losses across all domains. This distribution is updated after every \(S\) steps, a hyperparameter that is tuned on the validation set.

## 4 Experiments

### Experimental settings

**Dataset.** The proposed method is evaluated on the recent Universal Embeddings Dataset (UnED) , the largest dataset for multi-domain fine-grained retrieval. It comprises \(4.1\)M images, with \(349\)k classes distributed across \(8\) image domains: food (Food2k dataset) , cars (CARS196 dataset) , online products (SOP dataset) , clothing (InShop dataset) , natural world (iNat dataset), artworks (Met dataset) , landmarks (GLDv2 dataset)  and retail products (Rp2k dataset) . We follow the train-validation-test splits and the evaluation protocol defined in , a brief review follows. Each index image and each query in the test set are described by a 64-D (universal) embedding, and Euclidean nearest neighbors in the embedding space are found for each query among the index set. The index contains images from all \(8\) domains combined; hence, cross-domain false positives are possible. Performance is measured by two metrics: Recall@\(1\) (R@\(1\)), which is equivalent to the correctness of the top neighbor, and modified Mean Precision@\(5\) (P@\(5\)) . The average performance over the queries in each domain is reported, as well as the balanced average of these values across all domains.

**Compared methods.** The universal embedding task is relatively new and only a few baselines were published in [45; 6]. We extend these baselines by re-purposing the single-domain embedding method of  (Table 1). Additionally, we also compare to two straight-forward multi-domain distillation methods  (Table 3). The main baselines compared with this work are the best-performing methods from , namely USCRR, UJCRR, and USCSS, which vary the classifier setup (SC: Separate Classifier, JC: Joint Classifier) and the sampling (RR: Round Robin, SS: Specialist Steps). We also evaluate a variant of USCRR, which uses the proposed dynamic sampling scheme, dubbed "USC w/ Dyn Sampler". The USCRR  method (USC w/ Dyn Sampler) is further expanded by inserting an MLP projector  between the universal embedding and its a classifier for each domain. The projectors consist of three hidden layers of sizes 256, 256, and 512 respectively. This new baseline method is referred to as the "MLP baseline". We compare with the off-the-shelf embeddings from ImageNet21k (IN) , finetuned ImageNet21k model with masked image modeling (IN+MIM) , CLIP , DINov2 , and SigLIP , which utilize embeddings of much higher dimensionality (768D vs 64D). Lastly, we compare against the Specialist+Oracle baseline, a non-realistic model proposed in  to get a hypothetical estimate of the maximum performance that can be achieved on each individual domain, by choosing the specialist of the query's domain as the embedding for both the query and the index set. All of the methods (including UDON), use the ViT-Base/16 backbone, and additionally, apart from the off-the-shelf ones, are fine-tuned on the training set of UnED.

**Implementation details.** For fair comparisons with the baselines of , identical values for common hyperparameters are used. The newly introduced hyperparameters are tuned based on performance on the validation set of UnED. For the KL divergence loss (3), the value of temperature \(T\) is set to \(T=0.1\) (a discussion regarding this choice can be found in the Appendix); the teacher embeddings have dimensionality of \(D_{t}=256\); the four loss components contribute equally to the total loss \(_{total}\) (no weights need to be tuned). We set the universal student embedding dimensionality to \(d=64\) for direct comparability against previous work. The batch size is set as \(B=128\). The hyperparameter \(S\) for the number of steps, after which the dynamic sampler is updated, is set to \(1000\). Each experiment is repeated 3 times with different seeds; the reported values are averaged over those runs. The standard deviations are reported in the Appendix. The ViT-Base/16 variant of the Vision Transformer is used as the backbone with ImageNet21k  and CLIP  initializations. The linear projections are initialized randomly, as well as the corresponding classifiers. Our implementation is based on the Scenic framework , a library based on Jax /Flax . Experiments are executed on Google Cloud TPU v4s .

### Main results

In Table 1, we present the performance of UDON and the compared methods. For the full UDON method, we present results for two different pretrainings for more complete comparisons, namely ImageNet21k , and CLIP . For the "MLP baseline" and the "  USC w/ Dyn Sampler" baseline, we present results for ImageNet21k pretraining only.

On average, as well as on most of the individual domains, the proposed UDON method achieves state-of-the-art performance, for both types of pretraining (ImageNet21k and CLIP). For ImageNet21k, it achieves an improvement of 1.5% and 2.3% on mean P@5 and R@1, respectively, over the previous state-of-the-art. For CLIP, it achieves an improvement of 1.9% and 1.8% on mean P@5 and R@1, respectively, over the previous state-of-the-art. The biggest improvements are observed in the Met,

   & Food2 &  &  &  &  &  &  &  &  \\  Model & PPS & R@1 & PPS & R@1 & PPS & R@1 & PPS & R@1 & PPS & R@1 & PPS & R@1 & PPS & R@1 & PPS & R@1 & PPS & R@1 \\   \\  IN (42)**(768-D)** & 31.1 & 44.1 & 44.4 & 54.1 & 43.7 & 65.6 & 53.5 & 53.9 & 67.4 & 52.1 & 30.8 & 14.8 & 25.2 & 52.9 & 74.3 & 88.4 & 52.8 \\   & - & 36.6 & - & - & - & - & - & - & - & - & - & - & - & - & - & - & - & - \\ CLIP (33)**(7684-D)** & - & 38.6 & - & - & - & - & - & - & - & - & - & - & - & - & - & - & - \\ DINov2 (7684-D)** & 29.4 & 42.9 & 74.7 & 82.2 & 64.4 & 57.2 & 50.2 & 54.9 & 41.9 & 21.5 & 20.4 & 31.0 & 38.6 & 59.9 & 39.8 & 53.5 \\ DINov2 (2678-D)** & 39.9 & 51.4 & 67.1 & 79.3 & 58.6 & 56.0 & 17.4 & 34.3 & 71.2 & 77.6 & 38.3 & 43.1 & 35.4 & 46.6 & 67.8 & 43.9 & 58.2 \\ SgLIP (2678-D)** & 39.5 & 32.8 & 51.9 & 56.7 & 50.8 & 67.3 & 53.3 & 71.1 & 59.6 & 51.6 & 41.2 & 20.6 & 30.2 & 42.7 & 64.9 & 69.0 & 62.0 \\   \\  IN Specific+Oracle & 49.9 & 62.8 & 61.9 & 71.8 & 60.3 & 87.0 & 86.3 & 55.9 & 70.1 & 52.0 & 28.4 & 24.9 & 31.2 & 43.1 & 73.6 & 87.1 & 54.9 & 66.6 \\ CLIP Specific+Oracle & 51.5 & 63.7 & 83.4 & 88.5 & 68.5 & 81.2 & 68.0 & 86.2 & 67.3 & 73.0 & 27.6 & 32.9 & 35.1 & 46.6 & 69.7 & 84.4 & 59.6 & 70.4 \\   \\  
15UCRCR & 48.6 & 63.7 & **62.9** & 77.4 & **87.2** & **70.4** & **88.9** & 67.3 & 73.3 & 75.3 & 75.0 & 27.1 & 31.6 & 74.1 & 86.8 & 52.4 & 62.6 \\   & 48.6 & 60.9 & 55.9 & 60.7 & 61.9 & 78.7 & 70.4 & 89.3 & 69.1 & 72.3 & 77.0 & 21.6 & 31.4 & 74.1 & 86.8 & 52.4 & 62.6 \\   & 49.0 & 61.7 & 53.4 & 64.3 & 62.0 & 78.6 & 67.5 & 68.3 & 68.9 & 73.5 & 48.7 & 20.9 & 21.3 & 31.4 & 74.1 & 86.1 & 51.4 & 62.5 \\  & 49.6 & 57.1 & 56.3 & 67.4 & 67.6 & 56.5 & 86.7 & 68.6 & 68.6 & 75.0 & 12.6 & 26.5 & 75.7 & 75.2 & 86.6 & 51.7 & 53.3 & 63.0 \\   & 49.6 & **54.2** & 61.5 & 53.0 & 63.0 & 41.8 & 78.5 & 54.1 & 58.0 & 69.7 & **74.8** & **13.8** & **17.8** & **25.4** & 36.8 & 51.0 & 62.8 \\   & 49.6 & **54.2** & 61.5 & 71.2 & 64.6 & 68.6 & 56.6 & 76.4 & 75.0 & 12.5 & **15.6** & **26.6** & **9.6** & **9.5** & **86.6** & **85.6** & **85.5** \\   & 50.1 & 62.0 & **60.0** & 85.4 & **86.4** & **86.2** & **77.7** & **70.0** & 63.7 & 69.5 & 4.6 & 58.8 & 25.5 & 36.0 & 70.1 & R1 & R1 & 55.0 & 64.6 \\  & 49.5 & 61.4 & 70.0 & 84.9 & 68.6 & 81.3 & 71.9 & 89.4 & 64.4 & 70.5 & 16.8 & 68.3 & 25.3 & 71.1 & 85.1 & 54.6 & 64.9 \\  & 49.8 & 62.0 & 76.1 & 83.4 & 65.8 & 71.0 & 85.5 & 63.3 & 71.4 & 9.9 & 12.5 & **34.5** & **37.1** & 84.

GLDv2 and iNat domains, all of which are characterized by a large number of classes and long-tail distribution. Our proposed method makes notable progress towards closing the performance gap to the Specialist+Oracle baseline, coming as close as 1%-1.3% for the respective metrics, for ImageNet21k pretraining. The MLP baseline  with the addition of dynamic sampling ("MLP baseline w/ Dyn Sampler") shows comparable performance on average with the previously reported state-of-the-art methods of  and the baseline method USCRR with Dyn Sampler instead of RR sampling ("USC w/ Dyn Sampler"), showing that it is not trivial to extend the conclusions of  to the multi-domain embedding setting. The proposed UDON method achieves an improvement of 2.9% and 2.5% on mean P@5 and R@1, respectively, over the "MLP baseline w/ Dyn Sampler", and 2.6% and 2.1% on mean P@5 and R@1 respectively, over the "USC w/ Dyn Sampler" baseline. We additionally perform an experiment where we combine the MLP baseline  with UDON, by appending an MLP projector between the classifier of every domain and the universal embedding. This underperforms the UDON method by 0.6% and 0.3% on mean P@5 and R@1 respectively, while bringing significant extra cost on the number of parameters of the model. In Figure 3 qualitative results for two queries of the UnED test set are shown, for which the UDON universal embedding exhibits better retrieval performance compared to the USCRR  baseline embedding.

### Ablations

A spectrum of methods employing various components of UDON are evaluated to examine the impact of each individual block. The methods range from the baseline USCRR (Table 2, row 1) to the full UDON (Table 2, row 3) method. All ablations are initialized by ImageNet21k pretraining. Qualitative results comparing the full UDON method with the baseline "USCRR" are presented in the Appendix.

**Dynamic sampler.** Two methods are evaluated to demonstrate the importance of the dynamic sampler (DS). The baseline with DS (Table 2, row 2 - compare with row 1) and UDON without DS (Table 2,

Figure 3: **Qualitative results for our UDON method.** We present the 5 nearest neighbours that are retrieved by the baseline (USCRR) embedding (top row) and the proposed UDON embedding (bottom row), for queries that the proposed method improves over the baseline. Each image shows the domain it comes from (underneath it). The correct neighbors are in green border, the incorrect ones are in red.

row 4 - compare with row 3). In both experiments, the dynamic sampler delivers a significant boost in the two most difficult (instance level) domains Met and GLDv2, similar performance in iNat, and a drop in the other 5 domains. All following ablations are performed with the dynamic sampler.

**Distillation objectives.** Two distillation losses are involved in training the full UDON method: the relational distillation loss (2) and the logit distillation loss (3). Both the losses improve the performance, as can be seen in Table 2 comparing the Full method (row 3), relational distillation only (w/o logit distillation, row 6), and no distillation loss (row 7).

**Classification loss** on the universal embedding. Removing the classification loss from the universal embedding ("w/o CE loss on univ.", row 8) results in a loss of average performance. Interestingly, it has a slightly positive impact on the Met domain.

**Online teacher dimensionality.** We perform an experiment ("64-D teachers", row 5) with dimensionality of the specific domain teachers reduced to 64D (_i.e._ the same dimensionality as of the universal student embedding) as compared to 256D in the full method. This results in a performance drop, which aligns with previous observations that higher dimensional embeddings can be better teachers in a distillation setting .

**Scheduling the dynamic sampler.** The sampler probability is updated every 1000 optimization steps (UDON needs \(\)120k steps to converge). Our experiments show that the method is not sensitive to the choice of this parameter. The probability is updated according to the training classification loss of the current model on each domain. In fact, there are two such losses in UDON. One provided by each domain teacher's classifier, and one provided by the classification loss on the universal student's separate classifier for each domain. Using the latter to update the sampler's weights incurs a small drop in performance, as seen in ("Dyn Sampler on univ.", row 9), compared to the Full UDON method, where the domain teacher's classification loss updates the sampler.

### Other distillation approaches

The online distillation method of UDON provides a very efficient way of transferring domain-specific knowledge to the universal embedding, without training more than a single backbone. In this section, the application of alternative distillation approaches is discussed. In particular, we implement two other approaches: first, the naive multi-teacher distillation (Figure 1 middle), with independent specialist models as teachers (8 extra backbones), where each teacher is trained in its own domain, dubbed "8 separate teachers". Second, one model with specialist heads is trained, _i.e._ 1 extra backbone followed by domain-specific projections (teacher embeddings), dubbed "1 separate teacher". In both cases, the teacher backbones are fixed during distillation, and the universal embedding (student) gets its own backbone. All backbones are initialized by ImageNet21k in the experiments of Tables 3 and 4. For efficiency reasons, only relational distillation is performed in this experiment, and all the teacher embeddings are 256 dimensional, as in UDON.

**Universal embedding performance.** The results are presented in Table 3, indicating that our method is not only efficient, but also outperforms other variants. This finding indicates that UDON benefits significantly from sharing the backbone between the student and the teachers, even if that could limit the representation capacity of the teachers, given that they have fewer free parameters.

**Compute cost reduction.** The alternative approaches are less efficient in terms of the number of parameters, as well as in the number of steps needed to converge. More specifically, for this setup, UDON uses \(\)188 million (M) parameters, "1 separate teacher" uses \(\)440M parameters, and "8

   &  &  &  &  &  &  &  &  \\  Model & P95 \& R@P & P95 \& R@P & P95 \& R@P & P8 & P8 & P8 & P8 & P8 & P8 & P8 & P8 & P8 & P8 & P8 & P8 & P8 & P8 & P8 & P8 & P8 & P8 & P8 & P8 & P8 \\ 
1 USCR & 48.3 & 60.9 & 58.9 & 69.7 & 61.9 & 78.7 & 70.4 & 88.3 & 69.1 & 74.2 & 7.3 & 9.7 & 21.3 & 31.4 & 74.1 & 87.1 & 51.4 & 62.5 \\
2 USC w/ Dyn Sampler & 46.2 & 59.1 & 56.3 & 67.4 & 61.1 & 88.4 & 65.9 & 86.1 & 66.9 & 74.8 & 12.0 & 15.6 & 25.9 & 37.3 & 73.2 & 86.9 & 51.3 & 63.2 \\ 
3 Full UDON & 49.6 & 62.2 & 61.3 & 71.2 & 64.2 & 80.2 & 69.8 & 88.5 & **704** & 75.3 & 12.0 & 15.9 & 28.6 & 40.9 & 75.6 & 88.0 & 53.9 & **65.3** \\
4 w/o Dyn Sampler (RR) & **50.3** & **62.9** & **62.9** & **72.5** & **67.4** & **82.2** & **75.4** & **89.8** & **70.4** & **74.4** & 7.6 & 9.4 & 23.3 & 33.4 & **76.8** & **88.6** & **54.3** & 64.4 \\
5-64-D teachers & 47.7 & 60.3 & 60.2 & 70.3 & 64.3 & 80.8 & 68.6 & 87.6 & 69.9 & 75.0 & 11.3 & 14.5 & 26.4 & 38.4 & 74.2 & 87.6 & 52.8 & 64.2 \\
6 w/o logit distillation & 49.2 & 61.8 & 60.1 & 70.4 & 62.6 & 79.2 & 68.0 & 87.2 & 70.3 & 75.3 & 12.5 & 15.6 & **28.7** & **41.0** & 74.9 & 87.8 & 53.3 & 64.8 \\
7 w/o mesh distillation & 48.0 & 69.0 & 54.1 & 64.4 & 78.5 & 65.4 & 87.5 & 85.8 & 70.3 & 75.3 & 12.4 & 16.1 & 28.2 & **41.0** & 73.5 & 77.1 & 51.7 & 63.8 \\
8 w/o CE loss on univ. & 48.0 & 60.9 & 60.5 & 70.2 & 70.5 & 77.2 & 86.4 & 66.6 & 74.9 & 12.6 & 15.9 & 25.8 & 37.3 & 74.5 & 87.5 & 52.3 & 64.0 \\
9 Dyn Sampler on univ. & 48.2 & 60.8 & 60.6 & 70.8 & 64.3 & 80.4 & 69.6 & 88.3 & 70.1 & 75.2 & **13.2** & **16.5** & 27.2 & 39.6 & 75.2 & 87.7 & 53.6 & 64.9 \\  

Table 2: Ablation studies for the performance of the universal embedding, given different modifications of the Full UDON approach. The comparison is performed on the test set of UnED.

separate teachers" uses \(\)873M parameters, saving as much as \(\)4.5 times in parameters, while improving performance. Additionally, UDON takes on average \(\)120k steps to converge, "1 separate teacher" needs around \(\)220k steps (sum of the 2 training phases), "8 separate teachers" \(\)250k training steps (sum of the 9 training phases), cutting the number of convergence steps in half. For reference, the no-distillation baseline USCRR converges at around the same steps as UDON.

**Teachers' performance.** To gain a better insight, we also evaluate the performance of the teachers in their domains. The domain of test image is used as an oracle in these experiments in order to restrict the index to contain images of the same domain only, hence the reported numbers are **not** comparable to other reported results. Table 4 shows that the independent specialists provide the best per-domain teachers. Interestingly, although being the best performing (teachers) in their domain, the latter do not provide the best distillation outcome, which is delivered by UDON, as discussed in the previous paragraphs. We hypothesize that sharing the backbone between the teachers and the student in UDON, on the one hand limits the performance of the teachers on their individual domains, but, on the other hand, allows for more efficient distillation, as the specialist heads and the universal student operate on the same backbone embedding. Another related observation can be made from the ablation Table 2. Row 2 ("USC w/ Dyn Sampler") and row 7 ("w/o any distillation") differ in the presence of separate domain classification heads on top of the universal student backbone in UDON, without performing distillation. In the latter method, the specialist heads provide a regularization for the backbone training, which results in improved performance.

## 5 Conclusions and Limitations

**Conclusions.** In this work, a novel multi-teacher distillation approach - Universal Dynamic Online distillation(UDON) - is introduced to tackle the problem of learning a universal embedding. The universal embedding and the domain-specific teachers share the backbone parameters and are trained jointly, which proves to be very efficient both in time and resources. The proposed training approach is shown to deliver high efficacy distillation, in which the universal student performs even better than distilling from separate fixed teachers. The additionally proposed difficulty-based dynamic sampling results in a significant boost of performance in complex domains which are typically characterized by a large number of classes and long-tail distributions. The proposed method improves the state-of-the-art performance on the recent UnED benchmark.

**Limitations.** While UDON boosts universal embedding performance compared to the baseline method USCRR which only employs classification loss, it has 20% decrease in training throughput, given that it adds new parameters (in the teacher heads). Additionally, the proposed dynamic sampling significantly improves the performance in the difficult domains, such as Met and GLDv2, however, still at a cost of slightly decreased performance on other simpler domains.

   &  &  &  &  &  &  &  &  &  \\  Model & P@P@5 & R@1 & P@P@5 & R@1 & P@P@5 & R@1 & P@P@5 & R@1 & P@P5 & R@1 & P@P5 & R@1 & P@P5 & R@1 & P@P5 & R@1 \\   \\ 
8 separate teachers & 54.7 & 66.4 & 71.8 & 81.1 & 74.9 & 78.0 & 77.7 & 92.6 & 76.7 & 81.3 & 41.2 & 49.9 & 34.7 & 49.1 & 80.6 & 90.3 & 64.0 & 74.7 \\
1 separate teacher & 52.5 & 65.3 & 57.3 & 68.0 & 71.7 & 85.1 & 71.9 & 89.8 & 76.6 & 81.2 & 35.2 & 43.4 & 31.1 & 45.0 & 79.2 & 89.8 & 59.4 & 71.0 \\
**UDON** teachers & 52.9 & 64.8 & 62.4 & 71.6 & 72.7 & 85.6 & 75.2 & 91.8 & 74.8 & 79.6 & 29.0 & 34.6 & 32.6 & 45.2 & 79.5 & 90.1 & 59.9 & 70.4 \\  

Table 4: Performance comparison of the **teacher** embeddings (256D) that are used by the different distillation approaches, on the test set of UnED, **but on the separate index setting**, _i.e_. each query is only compared against the index of its own domain.

   &  &  &  &  &  &  &  &  &  \\  Model & P@P@5 & R@1 & P@P5 & R@1 & P@P5 & R@1 & P@P5 & R@1 & P@P5 & R@1 & P@P5 & R@1 & P@P5 & R@1 & P@P5 & R@1 \\ 
8 separate teachers & 47.6 & 60.8 & 58.0 & 69.0 & **62.8** & **79.4** & 67.9 & **87.3** & 70.1 & 78.2 & **12.5** & **15.8** & 26.0 & 38.4 & 74.7 & 87.6 & 52.4 & 64.2 \\
1 separate teacher & 47.2 & 59.8 & 54.7 & 66.1 & 62.5 & 79.3 & 66.5 & 86.6 & 69.9 & 75.1 & 12.0 & 15.1 & 26.2 & 38.8 & 74.2 & 87.5 & 51.7 & 63.6 \\
**UDON (Ours)** & **49.2** & **61.8** & **60.1** & **70.4** & 62.6 & 79.2 & **68.0** & 87.2 & **70.3** & **75.3** & **12.5** & 15.6 & **28.7** & **41.0** & **74.9** & **87.8** & **53.3** & **64.8** \\  

Table 3: Performance comparison of the universal embedding using different distillation approaches, on the test set of UnED. “8 separate teachers” indicates the setting of 8 independent specialist models distilling to a universal model, while “1 separate teacher” indicates the setting of 1 independent model with 8 separate domain heads distilling to a universal model.

Acknowledgements

The authors acknowledge the support of the National Recovery Plan funded project MPO 60273/24/21300/21000 CEDMO 2.0 NPO, the Czech Technical University in Prague grant No. SGS23/173/OHK3/3T/13, and the CTU institutional support (Future fund).