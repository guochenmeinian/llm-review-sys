# Binding in hippocampal-entorhinal circuits

enables compositionality in cognitive maps

Christopher J. Kymn

Code is available at https://github.com/SoniaMaz8/Hippocampal_entorhinal_circuit

Sonia Mazelet

Anthony Thomas

Denis Kleyko

E. Paxon

Friedrich T. Sommer

Bruno A. Olshausen

###### Abstract

We propose a normative model for spatial representation in the hippocampal formation that combines optimality principles, such as maximizing coding range and spatial information per neuron, with an algebraic framework for computing in distributed representation. Spatial position is encoded in a residue number system, with individual residues represented by high-dimensional, complex-valued vectors. These are composed into a single vector representing position by a similarity-preserving, conjunctive vector-binding operation. Self-consistency between the representations of the overall position and of the individual residues is enforced by a modular attractor network whose modules correspond to the grid cell modules in entorhinal cortex. The vector binding operation can also associate different contexts to spatial representations, yielding a model for entorhinal cortex and hippocampus. We show that the model achieves normative desiderata including superlinear scaling of patterns with dimension, robust error correction, and hexagonal, carry-free encoding of spatial position. These properties in turn enable robust path integration and association with sensory inputs. More generally, the model formalizes how compositional computations could occur in the hippocampal formation and leads to testable experimental predictions.1

## 1 Introduction

The hippocampal formation (HF), consisting of hippocampus (HC) and the medial and lateral part of the neighboring entorhinal cortex, (MEC) and (LEC), is critical for forming memories and representing variables such as spatial position . Recent work has provided evidence of compositional structure in HF representations, enabling complex representations to be _composed_ by simpler building blocks and their formation rules. Examples include novel recombinations of past experience occurring in replay , or the exponential expressivity of the grid cell code . In particular, compositional representations afford high expressivity with lower dimensional storage requirements , less complexity in latent state inference, and generalization to novel scenes with familiar parts.

To gain insight into the possible computational principles and neural mechanisms at play in the HF, we take a normative modeling approach. That is, we seek to construct a model built from a set ofneural coding principles that effectively achieves the postulated function of the system. With this approach, we can then explain details about the neuroanatomical and neurophysiological structures in light of their particular contributions to an information processing objective. The resulting model can also lead to new predictions about the neural mechanisms that enable this function.

The postulated function of the HF --as a cognitive map and episodic memory-- has a core computational requirement, to represent and navigate space. Here, space is either the actual physical environment or a more abstract conceptual space. We formulate multiple desiderata for an effective representation of space. We then show that a residue number system, incorporated into a compositional encoding scheme, fulfills these desiderata. It is achieved by a modular attractor network that factorizes encoded locations into components of a residue number system. This provides an algorithmic-level hypothesis of hippocampal-entorhinal interactions. A core mechanism of this algorithm is _binding_, which draws inspiration from work in neuroscience, cognitive science, and artificial intelligence.

## 2 A normative model for the hippocampal formation

### Principles for representing space

Our first principle is that space is represented by a compositional code that has high spatial-resolution, is noise-robust, and in which algebraic operations on the components can be updated in parallel. Prior work [4; 5] has proposed the residue number system (RNS)  as a candidate for fulfilling these requirements. An RNS expresses an integer \(x\) in terms of its remainder relative to a set of co-prime moduli \(\{m_{i}\}\). For example, relative to moduli \(\{3,5,7\}\), \(x=40\) is encoded as \(\{1,0,5\}\). The Chinese Remainder Theorem guarantees that all integers in the range \([0,M-1]\), where \(M=_{i}m_{i}\), are assigned a unique representation. An RNS provides high spatial resolution, carry-free arithmetic operations, and robust error correction . Experimental observations in entorhinal cortex show a discrete multi-scale organization of spatial grid cells  that is compatible with the assumption of discrete RNS modules.

The second principle we adopt is that an individual residue value should be encoded by a neural population in a similarity-preserving fashion. In particular, we require that distinct integer values are represented with nearly orthogonal vectors. To achieve this principle, we use a method similar to random Fourier features . Each modulus, with value \(m_{i}\), is assigned a seed phasor vector, \(_{i}^{D}\), whose elements \((_{i})_{j}\) are drawn uniformly from the \(m_{i}\)-th roots of unity (i.e., \((_{i})_{j}=e^{_{ij}}\), with \(_{ij}=}k_{j}\), and \(k_{j}\) chosen randomly from \(\{0,...,m_{i}-1\}\)). The representation of a particular residue value \(a_{i}\{0,,m_{i}-1\}\) is then given by rotating the phases of the seed vector according to :

\[_{i}(a_{i})=(_{i})^{a_{i}},\] (1)

where we abuse notation slightly to also think of \(_{i}\) as a function that takes \(a_{i}\) as input and produces an embedding as described above. The complex-valued vectors can be mapped to interpretable population vectors via a randomized Fourier transform (Figures 6D and S2).

Our third principle concerns the manner in which a unique representation of a particular point in space is formed from the individual residue representations. This requires that we somehow combine the residue vectors for each modulus. Combining via concatenation, though straightforward, is not effective because codes that coincide in subsets of their residue representation would be similar, even when the encoded values are very different. Thus, the method of combining residue codes must be _conjunctive_. Conjunctive composition is often called _binding_ and is of fundamental importance in neuroscience , cognitive science , and machine learning . An early proposal for binding is the tensor product of vector representations , with the tensor order equal to the number of bound objects.

Here, we implement binding with component-wise vector multiplication, a dimensionality preserving operation that represents a lossy compression of the full tensor product [16; 17]. The resulting compositional vector representation of an integer \(x\) using an RNS representation with \(K\) moduli, \(\{a_{1},a_{2},..,a_{K}\}\), is:

\[(x)=_{i=1}^{K}_{i}(a_{i}).\] (2)We prove in Appendix A.1 that this coding scheme represents distinct integer states using nearly orthogonal vectors, and we show that it generalizes in a natural way to support representation of arbitrary real numbers in a similarity preserving fashion.

Eq. 2 represents individual points along a line. In general, however, a spatial representation involves points in 2D or 3D spaces. Conveniently, vector binding can be also used to compose representations of multidimensional lattices from vectors representing individual dimensions. As we will explain, there is still a choice in this composition that determines the resulting lattice structure. Following earlier proposals [18; 19; 20], our fourth normative principle is to choose the lattice structure so that spatial information is maximized, as described in Section 3.5.

The final principle we require is that for computations such as path integration, there should be a simple vector manipulation that results in addition of the encoded variables. Again, vector binding provides this functionality with our coding strategy, because of the following property:

\[(x)(y)=(x+y).\] (3)

### Modular attractor network for spatial representation

A standard model of grid cell circuits is the line attractor, in which states that represent a consistent location lie on a low-energy manifold . When initialized from a noisy location pattern, the circuit dynamics will generate a denoised location representation. Rather than forming a line attractor model for the entire representational space (Eq. 2), we propose a modular network architecture, so that the compositional structure of a residue number representation can scale towards a large range with fewer memory resources (Section 3.2), in a manner robust to noise (Section 3.3).

A starting point for our attractor network model is the Hopfield network, which acts as an associative memory by storing memory patterns as fixed-point attractors. The Rademacher-Hopfield network  is a dynamical system whose state is a vector \(\{-1,+1\}^{D}\) that obeys the following dynamics:

\[(t+1)=(^{T}(t))\] (4)

with \(\) as the matrix of memorized patterns (column vectors of \(\)). The fixed-point attractor dynamics can be generalized to complex memory patterns \(^{D}\):

\[(t+1)=(^{}(t)),\] (5)

where \(\) is a non-linearity normalizing the amplitude of each complex-valued component to one , and \(\) the corresponding matrix of memorized patterns. The model can also be discretized, such that each component is often quantized to a \(r\)-state phasor . The Rademacher-Hopfield model is the special case where \(r=2\) and the phasors happen to be real-valued.

An \(r\)-state phasor network of the form of Eq. 5 is well-suited to serve as an attractor network for each of the residue vectors in an RNS representation of position, with \(r=m_{i}\) for modulus \(i\), and the matrix \(\) (which we shall denote \(_{i}\)) storing the \(_{i}(a_{i})\) for \(a_{i}\{0,..,m_{i}-1\}\). However, we desire a method for representing the whole coding range \(M:=_{i}^{K}m_{i}\) without storing all \(M\) patterns in one large associative memory. For this purpose we show that a _resonator network_, a recently proposed recurrent network for _unbinding_ conjunctive codes [24; 25; 26], lets us represent this range by storing only \(n:=_{i}^{K}m_{i} M\) patterns. Given a vector encoding of position, \((x)\), as formulated in Eq. (2), a resonator network will factorize it into its constituent RNS components by iteratively updating each residue vector estimate, \(}_{i}\). This update is similar to the attractor dynamics of Eq. (5) but made to be consistent with \((x)\) given all other residue estimates \(}_{j i}\):

\[}_{i}(t+1)=_{i}_{i}^{ }_{j i}^{K}}_{j}^{*}(t) \;i\] (6)

Let us now assume that the input \((x_{t})\) encodes a spatial position \(x_{t}\) using Eq. (2). Given a velocity input \(_{i}(v_{t})\), estimated from self-motion input, path integration is performed by first running attractor dynamics, _then_ updating attractor states by velocity.

\[}_{i}(t+1)=_{i}(v_{t})(_{i} _{i}^{}(x_{t})_{i j}^{K}}_{j}^{*}(t))\] (7)After velocity updates, one can update the input state \((x_{t})\) with the conjunctive representation of the current factor estimates:

\[(x_{t+1})=_{i}^{K}}_{i}(t+1).\] (8)

Further explanation and detail is provided in Appendix B.3.

### Mapping the model to the HF

Although it is not obvious how the components of our normative model should map to the anatomical architecture of HF, we make one proposal as shown in Figure 1. The memory networks for residue representations \(}_{i}\) correspond to grid modules in MEC. Similar to the grid modules, a module for context can be added to the architecture, such as a tag for the identity of a specific environment, with the recurrent synapses \(\) storing tags of different environments.

The context neurons could correspond to the non-grid entorhinal cells, which can contain local, non-spatial information about the environment . The vector \((x_{t})\) can be linked to place cells in hippocampus. Internal HC circuitry can either buffer the input as in Eq. (6) or allow it to be updated dynamically according to the MEC input (Section 4.1). The mutual interactions between HC and MEC grid modules require projections between these structures. The binding operations that these interactions involve according to Eq. (6) are hypothesized to be implemented by nonlinear interactions between dendritic inputs in HC and MEC neurons.

The model also assumes the ability for sensory cues to provide the initialization signal of the cognitive map, represented by \(\) in Figure 1. For completeness, we adopt the assumption of previous models (e.g., ) that heterosaosaic memories are formed by the brain that link sensory cues to the hippocampal representations \(\) (Section 4.2). This process would require the system to generate a new context vector \(\) and initialize the cognitive map to a default location in order to learn about new environments. We show that through even a simple heterosaosaic mechanism, our modular attractor network can robustly retrieve sensory memories and even protect its compositional structure.

## 3 Coding properties of the model

### RNS representations have exponential coding range

The compositional RNS vector representation Eq. (2) can encode a coding range of \(M\) values using a total of \(n\) component patterns for representing the residue of individual modules. The scaling of the coding range is exponential in the number of moduli, \(K\), since if each module has \((m)\) patterns, and the co-prime condition is satisfied, the scaling of the coding range is \((m^{K})\). This recovers the expressivity argued by [4; 29].

More generally, it is also exponential in the number of component patterns, \(n\). The optimal coding range is given by the best partition of \(n\) into a set of positive \(\{m_{i}\}\). This optimization is identical to that of finding the maximum order of an element in the group of permutations \(S_{n}\), because the maximum order can be found by finding the longest cycle. The scaling of this value in \(n\) is characterized by Landau's function \(f(n)\), which is known to converge to \((n})\) as \(n\). Figure 2A

Figure 1: **Schematic of proposed attractor model.** In MEC, the \(_{i}\) are residue representations in grid modules, and \(\) encodes a context label. Input of velocity estimate \(()\) can produce path integration in grid modules via binding, denoted by \(\). In HC, \(\) represents contextualized place. Binding serves two roles in the MEC/HC interaction (symbolized by bidirectional arrows): _a)_ factorizing \(\) into \(_{i}\)’s, and _b)_ generating an update of \(\) from the \(_{i}\)’s, for example, after path integration. In LEC, \(\) represents sensory input, interacting with \(\) through a learned heteroassociative projection.

illustrates how Landau's function is the upper bound to what is achievable for any fixed number of moduli (\(K\)).

Though other kinds of representations can achieve an exponential coding range, the advantage of the compositional encoding of Eq. (2) comes from the fact that the binding operation implements carry-free vector addition (our fourth principle). This enables updates of the encoded value without requiring further transformations such as decoding, facilitating tasks such as path integration (Section 4.1, Appendix C.3). Binary representations, by contrast, have exponential coding range but require carry-over operations to implement.

### The modular attractor network has superlinear coding range

The exponential scaling of the coding range of the RNS representation is a prerequisite to obtain a large coding range with the attractor network that has to perform computations on this representation, such as input denoising, working memory, and path integration. To estimate the scaling of the coding range in the proposed attractor network (Eq. 6), we study the critical dimension for which the grid modules converge with high probability. Specifically, we empirically estimate the minimum dimension required to retrieve an arbitrary RNS representation with high probability, given a maximum number of iterations (Figure 2B). Remarkably, we find that the number of component patterns \(n\) that can be stored is superlinear in the pattern dimension \(D\); empirically \((D^{})\) for some \( 1\). For 2, 3, and 4 moduli, \( 2.05,1.45\) and \(1.23\), respectively (Figure 2C).

These empirical scaling laws are consistent with a simple information-theoretic calculation (Appendix A.2). The minimal amount of bits to be stored for the entire RNS vector encoding scheme is of order \((MM)\), and the number of synapses in the attractor network is \((D[K]{M})\). If one makes the cautious assumption of a capacity per synapse of \((1)\), the leading order for the coding range \(M\) is \((D^{})\), with \(=\).

While the coding range increases with the number of moduli (\(K\)) for the RNS representation, the superlinear scaling coefficient \(_{K}\) decreases with \(K\) for the modular attractor network, reaching maximum superlinearity at the smallest value \(K=2\). This reversal is caused by the fact that increasing \(K\) decreases the number of synapses, i.e., the memory resource in the attractor network.

### Robust error correction

In addition, we evaluate the robustness of our attractor model to noise. Because the RNS representations are composed of phasors, which are circular variables, we sample noise from a von Mises distribution with two parameters: mean (\(=0\)) and concentration pattern \(\) (Figure 3A). Higher \(\) values imply less noise; the distribution approximates a Gaussian with variance \(1/\) for large \(\). Further tests of model robustness to dropout, limited precision, and ablation are provided in Figure S6.

Figure 2: **Residue number systems, combined with a modular attractor network (resonator network), result in a new kind of attractor neural network with favorable scaling for a large combinatorial range.****A)** Number of encoding states, \(M\), grows rapidly in the number of modules, up to a maximum established by Landau’s function (black dots). **B)** Coefficient of coding range, M, scales roughly as \((D^{_{K}})\), depending on the number of moduli, \(K\), but with \(_{K}>1\). **C)** Estimation of scaling from slopes of linear regression (fit to log-log scale). Higher values of \(K\) require a higher dimension to achieve a particular coding range; empirical values are close to \(_{K}=\).

We consider three cases: noisy input patterns, noise added to each time step, and noisy weights corruptions of patterns in \(_{i}\) (Appendix B.2). The empirical accuracy of recall varies depending on the type of corruption applied (Figure 3A). We find that for a given dimension \(D\) (in this case, \(1024\)), increasing noise decreases the maximum coding range that can be decoded with high accuracy (Figure 3B-D). For a fixed noise level, the high-accuracy coding range is largest for input noise, followed by update noise and codebook noise. It is perhaps not surprising that codebook noise has the worst coding range, given that noise added to every stored pattern compounds across the dynamics. Fortunately, the demonstrated robustness to input noise enables sensory patterns to be denoised via heteroassociation (Section 4.2).

### Interpolation between patterns enables continuous path integration

In general, there is a sharp difference between point and line attractors. In our attractor model, the RNS representations of integer values are stored as discrete fixed points. Nevertheless, the attractor network also converges to states that represent non-integer values that are not explicitly stored. In other words, the network smoothly interpolates to points on a manifold of states that represent integer and non-integer values encoded by (2). Figure 4A provides a visualization, showing that the kernel induced by inner product operations retains graded similarity for sub-integer shifts. This kernel enables the modular attractor network to settle to fixed points that correspond to interpolations between integers, and for sub-integer positions to be decoded.

The resolution of decoding is fundamentally limited by the signal to noise ratio. Even so, we find that, up to a fixed noise level, the accuracy regimes of integer decoding and sub-integer decoding coincide. This property enables sub-integer position shifts to be encoded within the states of the network, which, as we will show, results in stable, error-correcting path integration (Section 4.1). We quantify the gain in precision in terms of the bits of information that can on average be reconstructed from a vector (Figure 4D, Appendix B.2). Notably, even a moderate noise level of \(=8\) results in nearly the same information content as in the noiseless case.

### Triangular frames in 2D maximize spatial information

In two-dimensional open field environments, grid cells have firing fields arranged in a hexagonal lattice . Work in theoretical neuroscience shows the optimality of this lattice for 2D environments in terms of spatial information [18; 19; 20]. However, the presence of hexagonal firing fields raises a puzzle for residue number systems. Although a crucial property of a RNS is the carry-free property, most implementations of RNS will not perform carry-free updates within a module in non-Cartesian coordinate systems. This generally occurs because the updates of different coordinates must interact due to non-orthogonality.

Figure 3: **Recovery of encoded positions is robust to multiple kinds of noise.****A)** Visualization of the von Mises weight distribution. Note that the magnitude of the noise is inversely proportional to \(\), and that the variance of the phase perturbation is much larger than the distance between the discrete states of phasors. **B-D)** Visualizations of accuracy as a function of coding range and \(\) for three separate cases: input noise (B), update noise (C), and codebook noise (D). Cases are shown in order of increasing difficulty. The resonator network maintains perfect accuracy up to a point, after which accuracy decays at an earlier point than the noiseless dynamics (black curve).

We resolve this issue by showing how to implement a version of vector binding of multiple coordinates in a triangular 'Mercedes-Benz' frame that enables carry-free hexagonal coding. Furthermore, we provide a combinatoric argument for the optimality of triangular _frames_ for \(^{2}\). (A frame is a spanning set for a vector space in which the basis vectors need not be linearly independent.) Our argument relies on the combinatorics of residue numbers, and so for the first time gives an explanation of why the coexistence of RNS and hexagonal codes is optimal.

To form a hexagonal tiling of 2D position requires two steps: first, projection into a \(3\)-coordinate frame, and second, choosing phases such that simultaneous, equal movements along all three frames cancel out (Appendix A.3). The resulting Voronoi tessellation for different states is pictured in Figure 5A. This encoding enables higher spatial resolution in terms of the number of discrete states: \(3m^{2}-3m+1\) for triangular frames, versus \(m^{2}\) for Cartesian frames. This increased expressivity results in a higher entropy) code for space (Figure 5B). It also results in both a periodic hexagonal kernel and the individual grid response fields being arranged in a hexagonal lattice (Figure 6C).

Prior models achieved hexagonal lattices either by pattern formation from circularly symmetric receptive fields (e.g., [32; 33]) arranged on a periodic rectangular sheet or by distorting a square lattice into an oblique one (e.g., [28; 34]). Importantly, oblique lattices have the combinatorial complexity as the square grid and, unlike the construction described above, they do not achieve the same level of spatial resolution (Figure 5B).

Figure 4: **Smooth interpolation between integer states enables encoding and decoding of sub-integer values.****A)** Visualization of interpolation between two integer states. The position of the fractional value can be estimated by fitting a periodic sinc function (Appendix A.1) based on the inner products with integer codebooks (visualized in dots), then finding the location of the peak. **B, C)** Sub-integer states can be be decoded, up to a precision set by the noise level. Note that in both cases, sub-integer decoding can be just as accurate as integer decoding for the same range, even though the sub-integer decoding problem is strictly harder. Even \(=4\) is sufficient to achieve accuracy within a precision of \( x=0.07\), but for higher noise (\(=2\)), the precision is worse. **D)** The best spatial precision (in bits) that can be decoded for a fixed noise level. Representations with less noise achieve both a higher coding range and higher information content per vector.

Figure 5: **Hexagonal coding improves spatial resolution.****A)** Voronoi tessellation for \(m=5\). Each distinct color corresponds to a unique codeword in \(^{D}\). Black arrows show the coordinate axes of the triangular ‘Mercedes-Benz’ frame in 2D. **B)** Hexagonal lattices have higher entropy than square lattices, allowing each state to carry higher resolution in its spatial output.

Testing functionalities of the model

### Robust path integration

Given the ability of the attractor model to update its representation of position from velocity inputs, along with its ability to represent continuous space, we evaluate its ability to perform path integration in the presence of noise. We simulate trajectories based on a statistical model for generating plausible rodent movements in an arena [35; 36], and we update grid cell and place cell state vectors according to Equations 7 and 8, respectively.

To evaluate the robustness of the model to error (Appendix B.3), we consider both sources of extrinsic noise (e.g., mis-representations of velocity information), and intrinsic noise (e.g., due to noise in weight updates). The robustness of our model to intrinsic noise is tested by comparing our results to the estimated trajectories obtained without the correction by the MEC modules (Figure 6A and B). We find that our model strongly limits noise accumulation along the trajectory and allows highly accurate integration for a longer period of time (Figure 6A). Consistent with our previous experiments on noise robustness (Figure 3), we find that the model has strong robustness to intrinsic noise, with extrinsic noise resulting in a drift of the estimated position.

We visualize the response fields in different modules and find hexagonal lattices with a module dependent scaling (Figure 6C, Appendix 4.1). In addition, we show that tethering to external cues (e.g., visual inputs), can significantly increase the accuracy of the attractor network. To study this, we associate visual cues to corresponding patches see Section 4.2) and observe that integration of information from sensory visual inputs succeeds in correcting drift due to extrinsic noise (Figure 6D).

### Denoising sensory states via a heteroassociative memory

Finally, we describe a simple extension to our model, in which sensory patterns are fed from the lateral entorhinal cortex (LEC) to update the hippocampal state. This is consistent with theories of

Figure 6: **Attractor dynamics facilitate robust path integration.****A)** Example of path integration of a 2D trajectory in the case of intrinsic input noise on the place cell representation. The grid cell modules correct the noise that would otherwise induce drift after a short period of time. **B)** Path integration results averaged over multiple trajectories in the case of intrinsic input noise on the place cell representation. Grid cell modules limit noise accumulation along the trajectory. Solid lines report the median error over \(100\) trials, with shaded intervals reporting \(25^{}\) and \(75^{}\) percentile. **C)** Simulated trajectory, along which colors represent the similarity between the \(_{i}\) of three different modules and vectors representing each position in the environment. We see hexagonal response fields, similar to those obtained from single unit recordings of MEC. **D)** Sensory patterns (symbolized by red dots), representing visual cues, are associated to positions in the environment. Presentation of visual cues helps correct drifted positions due to extrinsic noise.

memory suggesting that LEC provides the content of experiences to hippocampus , as well as with neuroanatomical evidence . Although the structure of the representations of those sensory patterns is unknown, it is theorized that HF is critical to sensory pattern completion .

Consistent with this function, recent work [28; 40] has proposed that a heteroassociative scaffold connects sensory patterns to hippocampal activity, allowing robust denoising of sensory states. Though the main focus of our normative model is not sensory denoising, we show that a simple extension to our model (Appendix B.4) robustly retrieves noisy pattern even under high levels of corruption (Figures 7A and B). In Appendix C.3, we also discuss how this capacity for generalization can serve as a model for sequence retrieval and show some preliminary experiments.

In addition to robust denoising of single patterns, our model is also well-equipped to deal with compositions of sensory patterns. Two situations are worth emphasizing: first, we can often unmix multiple sensory states corresponding to a sum of patterns, because the compositional structure of binding between grid modules _protects_ the items in summation (Figure 7C). This differentiates our model from other heteroassociative memories, in which sums of patterns would have multiple equally valid yet incompatible decodings. Second, the context vector module can separate the sensory information corresponding to different environments (Figure S3).

## 5 Discussion

There are by now numerous theories of the entorhinal cortex and hippocampus, including those that draw upon attractor dynamics and residue number systems. What this paper contributes to the existing body of work is a concrete set of design principles that can be brought together to build a functioning neural system capable of representing space and performing path integration, making the most use of limited neural resources and precision. In particular, a core design principle of this model is a compositional representation of space that achieves a superlinear coding range, which is achieved by a compact, multi-module attractor network. The compositional representation, in turn, is achieved via a vector binding operation, which enables binding multiple scales (moduli) and spatial dimensions, context, and spatial shifts for path integration. This binding mechanism builds on prior work in the field of hyperdimensional computing and vector symbolic architectures [11; 17; 26; 41; 42; 43] -- and goes beyond it to develop a specific algorithmic hypothesis about structured operations in HF. Our analyses and experiments confirm that the model can achieve important functions of the hippocampal formation and they explain experimental observations, such as hexagonal grid cells, place cells, and remapping phenomena. The model thus contributes to, and greatly benefits from, existing work in theoretical neuroscience on residue number systems [4; 5], continuous attractor network models

Figure 7: **Heteroassociation enables recovery of sensory patterns under corruption and superposition.****A)** Accuracy for denoising \(60\) different random binary patterns for different vector dimension \(D\). The dotted line is the average similarity between the decoded and ground truth patterns. **B)** Same experiment as in panel A, but with \(210\) different possible binary patterns. The accuracy is lower on average. **C)** Accuracy for denoising multiple patterns from a single input. This task is especially challenging, because sums of patterns combined in this way interfere with each other in retrieval (a phenomenon known as cross-talk noise). However, the compositional structure of our modular attractor network enables multiple patterns to be decoded with high probability.

of grid cells , models of compositionality in the hippocampal formation , and the optimality of hexagonal representations in 2D .

That biology organized grid cells into multiple discrete modules, rather than pooling all resources into a single module attractor network, has posed a long-standing puzzle to theoreticians: What advantages are conferred by this organization? Our answer is that it provides exponential scaling in dynamic range by combining modules with limited dynamic range _multiplicatively_. Other recent work has focused on the problem of coordinating representations across multiple modules , and large scale recordings of the hippocampal formation  may provide new opportunities to evaluate their resulting predictions.

Our approach starts from principles of space encoding, in particular, the requirement of compositionality. This strategy is complementary to investigations of the emergence of place and grid cells in artificial neural networks (e.g., ). These approaches can show the optimality of biological response features under the model assumptions, such as ANN properties, network architecture, training objective and protocol. Here, we emphasize the role of multiplicative binding, a computational primitive that is typically difficult to have emerge in an ANN setting. Early suggestions for realizing conjunctive binding already ventured outside the framework of ANNs . A simple extension of ANNs are sigma-pi neurons  that can implement vector binding . Recent work amplifies the view that full conjunctive binding would be a useful inductive bias to augment deep learning architectures , and various augmentations of ANNs with dedicated binding mechanisms have been proposed .

Our model has clear limitations. The attractor neural network for the cognitive map is still a high-level abstraction of spiking neural circuits in the hippocampal formation. In particular, the phasor states in the model are one linear transform removed from vectors that describe neural population activity. Thus, the mapping between model and neurobiological mechanisms requires an additional step. This disadvantage can be directly addressed by switching to other encoding schemes, such as sparse real- or complex-valued vectors, e.g., , for which conjunctive binding operations have been proposed . Although the model is more comprehensive than typical normative models, which usually focus on a single computation, it is far from covering the many other functional cell types observed in the hippocampal formation or contextual modulations observed during remapping. In addition, the current model includes learning only in the heterososociative projection to LEC. Most observations regarding plasticity in HF are not captured, i.e., signals from reward, or eligibility traces. Finally, our assumptions about inputs to HF from the sensory pathway are simplifying and primarily intended as a proof of concept.

The purpose of the model, to express the fundamental principles of a compositional cognitive map, also leads to testable predictions: First, at the biophysical level, the model predicts multiplicative interactions between dendritic inputs providing the conjunctive binding operation. There are several biophysically realistic ways in which neurons can multiply their inputs . Contextual gating in dendritic branches of hippocampal neurons is consistent with our theory, hippocampal remapping, and neurophysiology of hippocampal dendrites . Our attractor model predicts direct multiplicative interactions between MEC modules, which remains to be tested. Second, the model predicts relatively fixed attractor weights between place and grid cells, with a higher degree of plasticity for the weights between sensory encodings and hippocampal states. Third, we predict that causal perturbations of one grid module can affect the states of other grid modules without involvement of the hippocampus, in a direction that is self-consistent with the update of the attractor state.

Finally, we believe that the proposed modeling approach and the specific attractor model presented have broader applications in neuroscience. For example, the problem of factorization is critical to forming compositional representations of visual scenes, and a closely related attractor neural network can find efficient solutions to such problems . In addition, there are promising ways to map complex-valued attractor neural networks to spiking neural networks , which could connect the principles derived here to a concrete implementation on neuromorphic hardware. Such a neuromorphic implementation could yield further quantitative predictions for neuroscience and is an exciting direction for future work.