# Dataset Diffusion: Diffusion-based Synthetic Dataset Generation for Pixel-Level Semantic Segmentation

Dataset Diffusion: Diffusion-based Synthetic Dataset Generation for Pixel-Level Semantic Segmentation

 Quang Nguyen\({}^{1,2}\)1 Truong Vu\({}^{1}\)1 Anh Tran\({}^{1}\) Khoi Nguyen\({}^{1}\)

\({}^{1}\)VinAI Research, \({}^{2}\)Ho Chi Minh City University of Technology, VNU-HCM

###### Abstract

Preparing training data for deep vision models is a labor-intensive task. To address this, generative models have emerged as an effective solution for generating synthetic data. While current generative models produce image-level category labels, we propose a novel method for generating pixel-level semantic segmentation labels using the text-to-image generative model Stable Diffusion (SD). By utilizing the text prompts, cross-attention, and self-attention of SD, we introduce three new techniques: _class-prompt appending_, _class-prompt cross-attention_, and _self-attention exponentiation_. These techniques enable us to generate segmentation maps corresponding to synthetic images. These maps serve as pseudo-labels for training semantic segmenters, eliminating the need for labor-intensive pixel-wise annotation. To account for the imperfections in our pseudo-labels, we incorporate uncertainty regions into the segmentation, allowing us to disregard loss from those regions. We conduct evaluations on two datasets, PASCAL VOC and MSCOCO, and our approach significantly outperforms concurrent work. Our benchmarks and code will be released at https://github.com/VinAIResearch/Dataset-Diffusion.

## 1 Introduction

Semantic segmentation is a fundamental task in computer vision. Its objective is to assign semantic labels to each pixel in an image, making it crucial for applications such as autonomous driving, scene comprehension, and object recognition. However, one of the primary challenges in semantic segmentation is the high cost associated with manual annotation. Annotating large-scale datasets with pixel-level labels is labor-intensive, time-consuming, and requires substantial human effort.

To address this challenge, an alternative strategy involves leveraging generative models to synthesize datasets with pixel-level labels. Past research efforts have utilized Generative Adversarial Networks (GANs) to effectively generate synthetic datasets for semantic segmentation, thereby mitigating the reliance on manual annotation [1; 2; 3]. However, GAN models primarily concentrate on object-centric images and have yet to capture the intricate complexities present in real-world scenes.

On the other hand, text-to-image diffusion models have emerged as a promising technique for generating highly realistic images from textual descriptions [4; 5; 6; 7]. These models possess unique characteristics that make them well-suited for the generation of semantic segmentation datasets. Firstly, the text prompts used as input to these models can serve as valuable guidance since they explicitly specify the objects to be generated. Secondly, the application of cross and self-attention maps in the image generation process endows these models with informative spatial cues, enabling precise extraction of object positions within the generated images.

By leveraging these characteristics of text-to-image diffusion models, the concurrent works DiffuMask  and DiffusionSeg  effectively generate pairs of synthetic images and corresponding segmentation masks. DiffuMask achieves this by utilizing straightforward text prompts, such as "a photo of a [class name] [background description]", to generate image and segmentation mask pairs. Meanwhile, DiffusionSeg focuses on creating synthetic datasets that address the challenge of object discovery, which involves identifying salient objects within an image. While these approaches successfully produce images paired with their corresponding segmentation masks, they are currently limited to generating a single object segmentation mask per image.

In this paper, we present Dataset Diffusion, a novel framework for synthesizing high-quality semantic segmentation datasets, as shown in Fig. 1. Our approach focuses on generating realistic images depicting scenes with multiple objects, along with precise segmentation masks. We introduce two techniques: _class-prompt appending_, which encourages diverse object classes in the generated images, and _class-prompt cross-attention_, enabling more precise attention to each object within the scene. We also introduce _self-attention exponentiation_, a simple refinement method using self-attention maps to enhance segmentation quality. Finally, we employ the generated data to train a semantic segmenter using uncertainty-aware segmentation loss and self-training.

To evaluate the quality of the synthesized datasets, we introduce two benchmark datasets: synth-VOC and synth-COCO. These benchmarks utilize two well-established semantic segmentation datasets, namely PASCAL VOC  and COCO , to standardize the text prompt inputs and ground-truth segmentation evaluation. On the synth-VOC benchmark, Dataset Diffusion achieves an impressive mIoU of \(64.8\), outperforming DiffuMask  by a substantial margin. On the synth-COCO benchmark, the DeepLabV3 model trained on our synthesized dataset achieves noteworthy results of \(34.2\) in mIoU compared to the model trained on real images with full supervision.

In summary, the contributions of our work are as follows:

* We present a framework that effectively employs a state-of-the-art text-to-image diffusion model to generate synthetic datasets with pixel-level annotations.
* We introduce a simple and effective text prompt design that facilitates the generation of complex and realistic images, closely resembling real-world scenes.
* We propose a straightforward method that utilizes self and cross-attention maps to achieve highly accurate segmentation, thereby improving the quality and reliability of the synthesized datasets.
* We introduce synth-VOC and synth-COCO benchmarks for evaluating the performance of semantic segmentation dataset synthesis.

In the following, Sec. 2 reviews prior work, Sec. 3 describes our proposed framework, and Sec. 4 presents our experimental results. Finally, Sec. 5 concludes with some remarks and discussions.

## 2 Related Work

**Semantic segmentation** is a critical computer vision task that involves classifying each pixel in an image to a specific class label. Popular semantic segmentation approaches include the fully convolutional network (FCN)  and its successors, such as DeepLab , DeepLabV2 , DeepLabv3 , DeepLabv3+ , UNet , SegNet , PSPNet , and HRNet . Recently,

Figure 1: Overview of our Dataset Diffusion for synthetic dataset generation. (**Left**) Given the target classes, our framework generates high-fidelity images with their corresponding pixel-level semantic segmentations. These segmentations serve as pseudo-labels for training a semantic segmenter. (**Right**) The trained semantic segmenter is able to predict the semantic segmentation of a test image.

transformer-based approaches like SETR , Segmenter , SegFormer , and Mask2Former  have gained attention for their superior performance over convolution-based approaches. In our framework, we focus on generating synthetic datasets that can be used with any semantic segmenter, so we use DeepLabv3 and Mask2Former as they are commonly used.

**Text-to-image diffusion models** have revolutionized image generation research, moving beyond simple class-conditioned to more complex text-conditioned image generation. Examples include GLIDE , Imagen , Stable Diffusion (SD) , Dall-E , eDiff-I , and Muse . These models can generate images with multiple objects interacting with each other, more closely resembling real-world images rather than the single object-centric images generated by prior generative models. Our Dataset Diffusion marks a milestone in synthetic dataset generation literature, moving from image-level annotation to pixel-level annotation. We utilize Stable Diffusion  in our framework, as it is the only open-sourced pretrained text-to-image diffusion model available at the time of writing.

**Diffusion models for segmentation.** Diffusion models have proven effective for semantic, instance, and panoptic segmentation tasks. These models either use input images to condition the mask-denoising process [27; 28; 29; 30; 31; 32; 33], or employ pretrained diffusion models as feature extractors [34; 35; 36; 37]. However, they still require ground-truth (GT) segmentation for training. In contrast, our framework utilizes only a pretrained SD to generate semantic segmentation without GT labels.

**Generative Adversarial Networks (GANs) for synthetic segmentation datasets.** GANs have been employed in the generation of synthetic segmentation datasets, as demonstrated in previous works such as [38; 1; 3; 39]. However, these approaches primarily focus on object-centric images, where a single mask is segmented for the salient object or specific parts of common objects like faces, cars, or horses, as exemplified in . In contrast, our framework is designed to generate semantic segmentations for more complex images, where multiple objects interact with each other at the scene level. Furthermore, while some techniques [38; 39] support foreground/background subtraction, and others [1; 3] still require human annotations, our objective is to generate semantic segmentations for multiple object classes in each image without the need for human involvement.

**Diffusion models for synthetic data generation** have been used to improve the performance of image classification [40; 41], domain adaptation for classification [42; 43], and zero/few-shot learning [44; 45; 46; 47]. However, these methods produce only image-level annotations as augmentation datasets. In contrast, our framework produces pixel-level annotations, which is considerably more challenging.

Recently, there have been concurrent works [8; 9] that utilize Stable Diffusion (SD) for generating object segmentation without any annotations. However, they focus on segmenting a single object in an image rather than multiple objects. Their text-prompt inputs to SD are simple, usually "a photo of a [class name]". The semantic segmenter trained on these annotations can segment multiple objects to some extent. Our framework, on the other hand, employs more complex text prompts where multiple objects can coexist and interact, making it more suitable for the semantic segmentation task in real-world images.

## 3 Dataset Diffusion

**Problem setting:** Our objective is to generate a synthetic dataset \(=(I_{i},S_{i})_{i=1}^{N}\), consisting of high-fidelity images \(\) and pixel-level semantic masks \(\). These images and masks capture both the semantic and location information of the target classes \(=\{c_{1},c_{2},...,c_{K}\}\), where \(K\) represents the number of classes. The purpose of constructing this dataset is to train a semantic segmenter \(\) without relying on human annotation.

In our approach, we follow a three-step process. Firstly, we prepare relevant text prompts \(\) containing the target classes (Sec. 3.1). Secondly, using Stable Diffusion (SD) as our model, we generate images \(_{i}^{H W 3}\) and their corresponding semantic segmentations \(_{i}\{0,,K\}^{H W}\), where \(0\) represents the background class (Sec. 3.2). These images and segmentations form the synthetic dataset \(\). Lastly, we train a semantic segmenter \(\) on \(\) and evaluate its performance on the test set of standard semantic segmentation datasets (Sec. 3.3). It is worth noting that our approach primarily focuses on segmenting common objects in everyday scenes, where the SD model excels, rather than specialized domains like medical or aerial images. The overall framework is depicted in Fig. 2.

### Preparing Text Prompts for Stable Diffusion

To prepare prompts containing a given list of classes for SD, one option is to utilize a large language model (LLM) such as ChatGPT  to generate the sentences, similar to the method described in . This approach can be valuable in real-world applications.

However, for evaluating the quality of the synthetic dataset, we need to rely on standard datasets for semantic segmentation like PASCAL VOC  or COCO  to create standardized benchmarks. In this regard, we propose using the provided or generated captions of the training images in these datasets as the text prompts for SD. This is solely for the purpose of standard benchmarking where the text prompts are fixed, and we do not utilize real images or image-label associations in our synthetic dataset generation. We call these new benchmarks as synth-VOC and synth-COCO.

When using the COCO dataset, we can rely on the provided captions to describe the training images. However, in the case of the PASCAL VOC dataset, which lacks captions, we employ a state-of-the-art image captioner like BLIP  to generate captions for each image. However, we encountered several issues with the provided or generated captions. Firstly, the text prompts may not use the exact terms as the target class names \(\) provided in the dataset. For instance, terms like "man" and "woman" may be used instead of "person", or "bike" instead of "bicycle", resulting in a mismatch with the target classes. Secondly, many captions do not contain all the classes that are actually present in the images (as illustrated in Fig. 3). This leads to a shortage of text prompts for certain classes, affecting the generation process for those particular classes.

To address the issues, we propose a method that leverages the class labels provided by the datasets. We append the provided (or generated) captions \(_{i}\) with the class labels, creating new text prompts \(}_{i}\) that explicitly incorporate all the target classes \(_{i}=[c_{1};;c_{M}]\), where \(M\) is the number of classes in image \(i\). This is achieved through the text appending operation or _class-prompt appending_ technique: \(}_{i}=[_{i};_{i}]\). For example, in the case of the left image in Fig. 3, the final text prompt would be "a photograph of a kitchen inside a house; bottle microwave sink refrigerator". This ensures that the new text prompts encompass all the target classes, addressing the issue of mismatched or missing class names in the captions.

### Generating Segmentation from Self and Cross-attention Maps

We build our segmentation generator on Stable Diffusion (SD) by leveraging its self and cross-attention layers. Given a text prompt \(}\) first encoded by a text encoder into text embedding \(e^{ d_{e}}\) with the text length \(\) and the number of dimensions \(d_{e}\), SD seeks to output the final latent state \(z_{0}^{H W d_{e}}\), where \(H,W,d_{z}\) are height, width, and number of channels of \(z_{0}\), reflecting the content encoded in \(e\) from the initial latent state \(z_{T}(0,I)\) after \(T\) denoising steps.

Figure 2: **Three stages of Dataset Diffusion**. In the first stage, the target classes are provided, and text prompts are generated using language models such as ChatGPT . Real captions (for COCO) or image-based captions (for VOC) can also be used for prompt generation to ensure standard evaluation. The text prompts are then augmented with the target class labels to avoid missing objects. In the second stage, given the augmented text prompt, a frozen Stable Diffusion  is employed to generate an image and its self- and cross-attention maps. The cross-attention map for each target class is refined using the self-attention map to match the object’s shape. Finally, the generated images and corresponding semantic segmentations are used to train a semantic segmenter with uncertainty-aware loss and the self-training technique.

At each denoising step \(t\), a UNet architecture with \(L\) layers of self and cross-attention is used to transform \(z_{t}\) to \(z_{t-1}\). In particular, at layer \(l\) and time step \(t\), the self-attention layer captures the pairwise similarity between positions within a latent state \(z_{t}^{l}\) in order to enhance the local feature with the global context in \(z_{t}^{l+1}\). In the meantime, the cross-attention layer models the relationship between each position of the latent state \(z_{t}^{l}\) and each token of the text embedding \(e\) so that \(z_{t}^{l+1}\) can express more of the content encoded in \(e\).

Formally, the self-attention map \(_{S}^{l,t}^{HW HW}\) and cross-attention map \(_{C}^{l,t}^{HW}\) at layer \(l\) and time step \(t\) are computed as follows:

\[_{S}^{l,t}=(K_{z}^{}}{ }}),_{C}^{l,t}=(K_ {e}^{}}{}}),\] (1)

where \(Q_{z},K_{z},K_{e}\) are the query of \(z\), key of \(z\), and key of \(e\), respectively, obtained by linear projections and taken as inputs to the attention mechanisms, and \(d_{l}\) is # features at layer \(l\).

Since we only want to obtain the cross-attention map of the class labels \(C_{i}\) of image \(i\) for semantic segmentation, we introduce _class-prompt cross-attention_ that is similar to cross-attention in Eq. (1) but produced by only taking the softmax over the class name part \(C_{i}\) rather than entire of the text prompt \(}_{i}\). In practice, we form a new text prompt \(}_{i}=C_{i}\) just for the purpose of extracting the cross-attention maps while the original text prompt \(}_{i}\) for generating images keeps unchanged. After this, we obtain \(_{C}^{l,t}^{HW M}\), where \(M\) is the number of classes in the image.

With the observation that using different ranges of timesteps only affects the final result marginally, (provided in Supp.), we average these cross and self-attention maps over layers and timesteps:

\[_{S}=_{l=1}^{L}_{t=0}^{T}_{S} ^{l,t},_{C}=_{l=1}^{L}_{t=0 }^{T}_{C}^{l,t},\] (2)

Although the cross-attention maps \(_{C}\) already exhibit the location of the target classes in the image, they are still coarse-grained and noisy, as illustrated in Fig. 4. Thus, we propose to use the self-attention map \(_{S}\) (as illustrated in Fig. 6 - Left) to enhance \(_{C}\) for a more precise object location. This is because the self-attention maps capturing the pairwise correlations among positions within the latent \(z_{t}\) can help propagate the initial cross-attention maps to the highly similar positions, e.g., non-salient parts of the object, thereby enhancing their quality. Therefore, we propose _self-attention exponentiation_ where the self-attention map \(_{S}\) is powered to \(\) before multiplying to the cross-attention map \(_{C}\) as:

\[_{C}^{}=(_{S})^{}_{C}, _{C}^{}^{HW M}.\] (3)

Figure 4: Given a text prompt **‘A bike is parked in a room; bicycle**’’, we obtain the generated image, cross-attention map, enhanced cross-attention map by the self-attention with \(=\{1,2,4\}\) described in the Eq. (3), and mask with uncertainty value (white region) by Eq. (4) and Eq. (5).

Figure 3: Common issues of using provided (or generated) captions. Red classes are often missing from the captions, resulting in a lack of text prompts for those classes. Blue classes may have different terms used in the captions, causing a discrepancy between the target class names and the text prompts.

Next, we aim to identify two matrices: \(^{H W}\) representing the objectness value at each location (the higher the objectness, the more likely that location contains an object), and \(\{1,,M\}^{H}\) indicating which objects in the class labels \(C_{i}\) that each location could be. To obtain those, we perform the pixel-wise \(*{arg\,max}\) and \(\) operator (over the category \(M\) dimension):

\[=*{arg\,max}_{m}_{C}^{*,m}, =_{m}_{C}^{*,m}.\] (4)

At a location \(x\) in the map \(\), if its value is less than a threshold, one can set its label to the background class \(0\). However, we find that using a fixed threshold does not work for all images. Instead, we use a lower threshold \(\) for certain background decisions and a higher threshold \(\) for certain foreground decisions. Any value that falls inside the range \((,)\) expresses an uncertain mask prediction with value \(U=255\). That is, the final mask \(}\) is illustrated in the last image of Fig. 4 and calculated as:

\[}_{x}=0&_{x},\\ U&<_{x}<,\\ S_{x}&.\] (5)

### Training Semantic Segmenter on Generated Segmentation

Given the synthetic images \(\) and semantic segmentation masks \(}\), we train a semantic segmenter \(\) with an uncertainty-aware cross-entropy loss. Specifically, for pixels marked as uncertain, we ignore the loss from those as: \(=_{x}(}_{x} U)_{ {CE}}(}_{x},}_{x})\), where \(\) is the indication function, \(_{}\) is the cross entropy loss, and \(}=()\) is the predicted segmentation from the generated image \(\).

We further enhance the segmentation mask \(}\) by the self-training technique . That is, after being trained with \(}\), the segmenter \(\) makes its own prediction on \(\) as pseudo labels \(^{*}\) without uncertainty value \(U\). Finally, the final semantic segmenter \(^{*}\) is the segmenter \(\) trained again on \(^{*}\).

## 4 Experiments

**Datasets:** We evaluate our Dataset Diffusion on two datasets: PASCAL VOC 2012  and COCO 2017 . The PASCAL VOC 2012 dataset has 20 object classes and 1 background class. For standard semantic segmentation evaluation, this dataset is usually augmented with the SBD dataset  to have a total of \(12,046\) training, \(1,449\) validation, and \(1,456\) test images. We additionally augment the training images with captions generated from BLIP . The COCO 2017 dataset contains 80 object classes and 1 background class with \(118,288\) training and \(5K\) validation images, along with provided captions for each image. It is worth noting that we only use the image-level class annotation to form the text prompts as described in Sec. 3.1.

We introduce the set of our prepared text prompts along with the validation set of each dataset as synth-VOC and synth-COCO - the two benchmarks for evaluation of semantic segmentation dataset synthesis. To create a balance synthetic dataset among classes, we generate \(2k\) images per object class for PASCAL VOC, resulting in a total of \(40k\) image-mask pairs and about \(1k\) images per object class for COCO, resulting in a total of \(80k\) image-mask pairs. If the number of text prompts associated with a certain class is insufficient, we use more random seeds to generate more images.

**Evaluation metric:** We evaluate the performance of Dataset Diffusion using the mean Intersection over Union (mIoU) metric. The mIoU(%) score measures the overlap between the predicted segmentation masks and the ground truth masks for each class and takes the average across all classes.

**Implementation details:** We build our framework on PyTorch deep learning framework  and Stable Diffusion  version 2.1-base with \(T=100\) timesteps. We construct the masks using optimal values for \(\), \(\), and \(\), which are defined in Sec. 6.2. Regarding semantic segmenter, we employ the DeepLabV3  and Mask2Former  segmenter implemented in the MMSegmentation framework . We use the AdamW optimizer with a learning rate of \(1e^{-4}\) and weight decay of \(1e^{-4}\). For other hyper-parameters, we follow standard settings in MMSegmentation.

### Main Results

**Quantitative results:** Tab. 1 compares the results of DeepLabV3  and Mask2Former  trained on the real training set, a synthetic dataset of DiffuMask , and the synthetic dataset of DatasetDiffusion. On VOC, our approach yields satisfactory results of \(64.8\) mIoU when compared to the real training set of \(79.9\) mIoU. Further, ours outperforms DiffuMask by a large margin of 4.2 mIoU using the same Resnet50 backbone. The detailed IoU of each class is reported in the Supp. Also, Dataset Diffusion achieves a promising result of \(34.2\) mIoU compared to \(54.9\) mIoU of real COCO training set. These results demonstrate the effectiveness of Dataset Diffusion, although the gaps with the real dataset are still substantial, i.e., \(15\) mIoU in VOC and \(20\) mIoU in COCO. This is due to the fact that the image content of COCO is more complex than that of VOC, reducing the ability of Stable Diffusion to produce images with the same level of complexity. We will discuss more in Sec. 5.

**Qualitative results** on the validation set of VOC are shown in Fig. 5. In Fig. 4(a), the synthetic images and their corresponding masks are utilized for training the semantic segmenter. The first two rows (1, 2) serve as excellent examples of successful segmentation, while the last two rows (3, 4) demonstrate failure cases. In certain instances, the self-training technique proves effective in rectifying mis-segmented objects (as seen in rows 2 and 3). However, it can also adversely impact the original masks when dealing with objects of small size (as observed in row 4). In Fig. 4(b), our predicted segmentation results on the validation set of VOC exhibit varying outcomes. The first three rows exhibit satisfactory results, with the predicted masks closely aligning with the ground truth. Conversely, the last three rows illustrate failure cases resulting from multiple small objects (row 4) and the presence of intertwined objects (rows 5 and 6).

### Ablation Study

We conduct all ablation study experiments on the text prompts described in Sec. 3.1. Additionally, we report the results with 20k images using the initial mask generated by Dataset Diffusion without using the self-training technique or test-time augmentation unless indicated in each experiment.

**Effect of text prompt selection**. Tab. 2 compares different text prompt selection methods. Our _class-prompt appending_ technique outperforms the text prompts using captions or class labels only. Specifically, the _class-prompt appending_ technique increases the performance by \(11.2\) and \(4.6\) mIoU over the "caption-only" and "class-label-only" text prompts, respectively. _Class-prompt appending_ also outperforms the simple text prompts by \(7.3\) mIoU. These results indicate that our text prompt selection method can help SD generate datasets with both diversity and accurate attention.

**Effects of different components** of stage 2 and stage 3 in Fig. 2 on the overall performance are summarized in Tab. 3. Using only cross-attention results in a low performance of \(44.8\) mIoU as the

    &  &  &  \\   & & **Training set** & **Val** & **Test** & **Training set** & **Val** \\  DeepLabV3 & ResNet50 & VOC’s training & 77.4 & 75.2 &  & 48.9 \\ DeepLabV3 & ResNet101 & (\(11.5k\) images) & 79.9 & 79.8 & & 54.9 \\ Mask2Former & ResNet50 & & 77.3 & 77.2 & & 57.8 \\  Mask2Former & ResNet50 & 
 DiffuMask  \\ (\(60k\) images) \\  & 57.4 & - & - & - \\  DeepLabV3 & ResNet50 &  & 61.6 & 59.0 &  & 32.4 \\ DeepLabV3 & ResNet101 & (\(40k\) images) & 64.8 & 64.6 & (\(80k\) images) & 34.2 \\ Mask2Former & ResNet50 & & 60.2 & 60.5 & 31.0 \\   

Table 1: Comparison in mIoU between training DeepLabV3  and Mask2Former  on the real training set, the synthetic dataset of DiffuMask , and the synthetic dataset of Dataset Diffusion.

  
**Method** & **Example** & **mIoU (\%)** \\ 
1: Simple text prompts & a photo of an aeroplane & 54.7 \\
2: Captions only & a large white airplane sitting on top of a boat & 50.8 \\
3: Class labels only & aeroplane boat & 57.4 \\
4: Simple text prompts + class labels & a photo of an aeroplane; aeroplane boat & 57.6 \\
5: Caption + class labels & a large white plane sitting on top of a boat; aeroplane boat & **62.0** \\   

Table 2: Performance of different text prompt selections. Red: class names, blue: similar terms.

cross-attention map is coarse and inaccurate (as illustrated in Fig. 4). Using self-attention refinement boosts the performance significantly to \(61.0\) mIoU. Also, using other techniques like uncertainty-aware loss, self-training, and test time augmentation help improve performance incrementally.

**Effect of different feature scales** used for aggregating self-attention and cross-attention maps is shown in Tab.4. As can be seen, for the cross-attention map, choosing too small and too large feature scales both hurt the performance since the former lacks details while the latter focuses on fine details instead of object shape. For the self-attention map, using the scale of 32 gives slightly better results.

**Hyper-parameters selection for mask generation (Sec. 3.2).** We conduct sensitivity analysis on \(\), \(\), and \(\) to determine the optimal values in Tab. 5. Tab. 5(a) shows the results of choosing \(\) (with fixed \(=0.5,=0.6\)) with the best result with \(=4\). A too-large value of \(=5\) decreases the performance as the refined cross-attention map tends to spread out the whole image rather than the object only. Additionally, Tab. 4(b) exhibits the analysis on the \((,)\) range given the fixed \(=4\), the range of \((0.5-0.6)\) achieves the best performance of \(62.0\) mIoU.

Figure 5: **(a)** Row 1 (R1) and R2 are successful cases, while R3 and R4 demonstrate failures. Self-training helps correct mis-segmented objects in some cases (R2 and R3) but can harm the original mask for small objects (R4). **(b)** R1 to R3 show accurate results, closely matching the GT. R4 to R6 reveal failure cases due to numerous small objects (R4) and intertwined objects (R5 and R6).

  
**Cross-attention** & **Self-attention** & **Uncertainty** & **Self-training** & **TTA** & **mIoU (\%)** \\  ✓ & & & & & 44.8 \\ ✓ & ✓ & & & & 61.0 \\ ✓ & ✓ & ✓ & & & 62.0 \\ ✓ & ✓ & ✓ & ✓ & & 62.7 \\ ✓ & ✓ & ✓ & ✓ & ✓ & **64.3** \\   

Table 3: Impact of cross-attention, self-attention, uncertainty, self-training, and test time augmentation (TTA) (refer to Sec. 3.2, Sec. 3.3). TTA includes multi-scale and input flipping at test time.

## 5 Discussion and Conclusion

**Limitations:** While our method is effective for generating synthetic datasets, there are some limitations to consider. Our primary reliance on Stable Diffusion  for image generation can result in difficulties with producing complex scenes. _First_, when given a text prompt that involves three or more objects, the diffusion model may only produce an image depicting two or three objects as exemplified in Fig. 6 - Right. However, there is ongoing research to improve the quality of the diffusion model and to incorporate stronger guidance, such as layout or box conditions, which shows promise in addressing this issue. _Second_, it is worth noting that in some cases, our Dataset Diffusion may not produce high-quality segmentation masks when objects are closely intertwined, as seen in Fig.4(a) with the example of a man riding a horse. _Third_, the bias in the LAION-5B dataset, on which Stable Diffusion was trained, may be transferred to the generated dataset. This is the current limitation of Stable Diffusion as it was trained on a large-scale uncurated dataset like LAION-5B. However, there are several studies addressing the bias problem in generative models  focusing on enhancing fairness and reducing biases in generative models. We believe that these studies and future work on the topic of fairness in GenAI will help to mitigate the bias in the generated images.

**Conclusion:** We have presented our novel framework - Dataset Diffusion - which enables the generation of synthetic semantic segmentation datasets. By leveraging Stable Diffusion, Dataset Diffusion can produce high-quality semantic segmentation and visually realistic images from specified object classes. Throughout our experiments, we have demonstrated the superiority of Dataset Diffusion over the concurrent method, DiffuMask, achieving an impressive mIoU of \(64.8\) in VOC and \(34.2\) in COCO. This remarkable advancement paves the way for future research endeavors focused on the creation of large-scale datasets with precise annotations using generative models.

    &  \\ 
**Cross-attention** & 32 & 64 & & & & \\ 
8 & 39.7 & 38.1 & & & & \\
16 & **62.0** & 59.6 & & & & \\
32 & 52.8 & 50.9 & & & & \\
64 & 35.4 & 31.5 & & & & \\
16, 32 & 59.7 & 57.3 & & & & \\
16, 32, 64 & 59.1 & 57.2 & & & & \\   

Table 4: Study on different feature scales

Figure 6: **Left: Correlation maps at some positions with others, extracted from a self-attention map. Right: Failure cases of SD when generating images with multiple objects. Red: classes are missed.**