# Enhancing Zero-Shot Vision Models by Label-Free Prompt Distribution Learning and Bias Correcting

Xingyu Zhu\({}^{1}\)  Beier Zhu\({}^{2}\)  Yi Tan\({}^{1}\)  Shuo Wang\({}^{1}\)  Yanbin Hao\({}^{1}\)  Hanwang Zhang\({}^{2}\)

\({}^{1}\)University of Science and Technology of China

\({}^{2}\)Nanyang Technological University

xingyuzhu@mail.ustc.edu.cn, shuowang.edu@gmail.com

Equal contributionsCorresponding author

###### Abstract

Vision-language models, such as CLIP, have shown impressive generalization capacities when using appropriate text descriptions. While optimizing prompts on downstream labeled data has proven effective in improving performance, these methods entail labor costs for annotations and are limited by their quality. Additionally, since CLIP is pre-trained on highly imbalanced Web-scale data, it suffers from inherent label bias that leads to suboptimal performance. To tackle the above challenges, we propose a label-**F**ree **prompt** distribution **learning** and **bias** correction framework, dubbed as **Frolic**, which boosts zero-shot performance without the need for labeled data. Specifically, our Frolic learns distributions over prompt prototypes to capture diverse visual representations and adaptively fuses these with the original CLIP through confidence matching. This fused model is further enhanced by correcting label bias via a label-free logit adjustment. Notably, our method is not only training-free but also circumvents the necessity for hyper-parameter tuning. Extensive experimental results across 16 datasets demonstrate the efficacy of our approach, particularly outperforming the state-of-the-art by an average of \(2.6\%\) on 10 datasets with CLIP ViT-B/16 and achieving an average margin of \(1.5\%\) on ImageNet and its five distribution shifts with CLIP ViT-B/16. Codes are available in https://github.com/zhuhsingyuu/Frolic.

## 1 Introduction

Vision-language models (VLMs), such as CLIP , which are pre-trained on large-scale datasets using contrastive loss, effectively align visual and textual representations within a shared feature space. This capability enables the zero-shot inference on downstream tasks through prompting and achieves remarkable performance. For example, using a selection of 80 hand-crafted prompts, a zero-shot CLIP ViT-B/16 achieves an accuracy of \(68.7\%\), and with prompts generated by language models , the accuracy increases to \(69.9\%\).

The success of zero-shot capabilities heavily relies on the appropriate text descriptions of the classes, which has gained research interest in improving prompts. Recent studies propose learning prompts from a small set of labeled images in the downstream data . Among these studies, Lu _et al._ and Wang _et al._ have found that learning the distribution of diverse prompts, which better captures the variance in visual representations, leads to improved performance. Although these methods have achieved significant improvements, they still depend on artificial prior knowledge for labeling downstream data and are limited by the quality of manual annotations, which may restrict the scalability of the original model.

Another significant approach to enhancing zero-shot performance involves correcting the label bias inherent in skewed web-scale pre-training data . This bias leads to highly imbalanced predictions and suboptimal performance. As illustrated in Figure 1(c), the average predicted probability on ImageNet using ViT-B/16 reveals an imbalanced distribution: the highest class probability exceeds \(0.002\), whereas the lowest is below \(0.0005\). Existing methods correct this bias by allowing access to a portion of the pre-training data , or by using labeled downstream data . However, the pre-training data is often inaccessible due to privacy or copyright concerns, and debiasing without labeled data is challenging.

In this paper, we introduce a label-**F**ree **p**mort distribution **l**earning and bias correction framework, dubbed as **F**rolic, which eliminates the need for data annotations to enhance zero-shot performance. First, unlike previous methods , which use a single class prototype for each class to define the decision boundary (as shown in Figure 1(a)), our approach employs Gaussian distributions to model the varied visual representations of text prototypes, as illustrated in Figure 1(b). It is worth noting that estimating such a distribution is non-trivial, since classical maximum likelihood estimation requires the annotation of each sample. Fortunately, we demonstrate that it is possible to infer distribution for each class directly from the first and second moments of the marginal distribution of downstream data without label information. Second, to prevent the use of pre-training data or labeled samples in downstream tasks, we develop a bias estimation mechanism, which transitions the sampling process from the pre-training data distribution to a class-conditional sampling from downstream distribution. By incorporating the estimated label bias into zero-shot models, we can achieve a balanced prediction, as illustrated in Figure 1 (d). Furthermore, we explore the possibility of combining the original CLIP predictions with those from the Gaussian-based models to enhance zero-shot performance. To this end, we have developed a confidence-matching technique that dynamically balances the contributions of the two models, eliminating the need for hyperparameter tuning. Notably, our framework is training-free, which enhances both flexibility and ease of implementation.

The main contributions of this work are:

* We enhance zero-shot performance by estimating a distribution over prompt prototypes to capture the variance in visual appearances. We demonstrate that this process can be implemented entirely without labels.
* We propose a confidence matching technique that fuses the original CLIP model with a Gaussian distribution-based model to further enhance zero-shot performance. This process eliminates the need for hyper-parameter searching, in stark contrast to previous studies.
* We develop an unsupervised method to correct pre-training label bias. Unlike existing methods that require access to pre-training data, our Proposition 2 suggests that we can avoid sampling from the pre-training distribution for estimating and correcting this bias. Instead, our method utilizes only downstream images.
* We demonstrate the effectiveness of our proposed method Frolic by conducting experiments across 16 datasets, which has a consistent and significant improvement over existing baselines. For example, our method surpasses the state-of-the-art zero-shot models by a margin of \(2.6\%\) on average with CLIP ViT-B/16.

Figure 1: Illustration of prompt distribution learning and label bias correction on ImageNet using CLIP ViT-B/16. (a) Existing zero-shot models . (b) Our prompt distribution learning (c) Average probability prediction of original CLIP. (d) Average probability prediction of our Frolic.

Related Works

**Zero-shot vision models.** Vision models pre-trained with auxiliary language supervision, such as CLIP  and OpenCLIP , facilitate zero-shot inference through prompting. Enhancing zero-shot performance has gained increasing research interest: (1) One approach involves prompt engineering, which includes designing hand-crafted prompts based on human priors  or automatically generating prompts via language models . (2) Another promising approach seeks to improve classifiers, _e.g._, ZPE  scores the importance of candidate prompts for prompt ensembling. InMaP  reduces the modality gap between vision and text. Several studies [32; 31] optimize prompt at test time by encouraging consistent predictions across augmented samples. Our work aims to enhance zero-shot models by learning the prompt distribution and mitigating the pre-training label bias.

**Prompt distribution learning.** Automatically learning prompts from downstream data has shown potential in improving zero-shot models [46; 45; 47]. These methods typically optimize prompts via minimizing the classification loss on the target task. However, as pointed out in Lu _et al._, learning prototype prompts overlook the diversity of visual representations. To this end, they estimate a distribution over the prompts to capture the variance of visual representations. Recently, Wang _et al._ propose training-free prompt distribution learning to improve efficiency. Contrary to existing methods  that estimate distributions through supervised approaches, our method circumvents the necessity for labels by inferring the variance of distributions from the statistics of unlabeled data.

**Correcting label bias.** Label bias generally occurs in the presence of skewed or imbalanced training data. In response to this challenge, Logit Adjustment (LA) [34; 14; 21; 49] has emerged as a prominent technique in long-tailed learning, specifically designed to adjust the decision boundary of classifiers to mitigate label bias. Menon _et al._ derives the theoretically optimal adjustment for logits. Zhu _et al._ extents LA to fine-tune zero-shot models by removing the pre-trained label bias. Unlike approaches that rely on the label distribution of the training set [34; 14; 21; 48] or the labels of fine-tuning data , our method adjusts the logits using unlabeled test data.

## 3 Methods

In this section, we present our prompt distribution learning, adaptive fusion, and logit adjustment techniques for adapting zero-shot models. Without loss of generality, we adopt CLIP  as our zero-shot model. To begin with, we emphasize three advantages of our framework:

**Training-free:** Our Frolic is training-free without optimizing the backbone of the zero-shot models, enhancing both flexibility and ease of implementation.

**Label-free:** Our method Frolic requires no external labeled data, making it suitable for zero-shot scenarios.

**No hyper-parameters searching:** Our method Frolic eliminates hyper-parameter tuning on validation datasets, in stark contrast to [38; 44]

### Setup

The zero-shot model consists of a visual encoder \(_{}()\) and a text encoder \(_{}()\). Given a set of unlabeled image data \(\{x_{i}\}_{i=1}^{N}\) and the unique text set of the class description \(\{z_{j}\}_{j=1}^{K}\), their visual and text representation can be computed as:

\[_{i}=_{}(x_{i});_{j}=_{} (z_{j}),\] (1)

where \(_{i}\) and \(_{j}\) share the same dimension (\(,^{d}\)). \(N\) is the sample size and \(K\) is the class size. \(_{j}\) can be considered as the prototype for class \(j\). With an image \(\) and all prototypes \(\{_{j}\}_{j=1}^{K}\), zero-shot CLIP predicts the label as:

\[y=*{argmax}_{j}f_{}()_{j}=*{ argmax}_{j}_{j}^{},\] (2)

where \(f_{}()_{j}=_{j}^{}\) is the score for class \(j\).

### Label-Free Prompt Distribution Learning

In order to express the diverse visual variations, our approach aims to learn the distribution of the class prototypes. Previous studies [18; 38] show that the Gaussian distribution is effective to model the distribution of the CLIP features and achieves impressive improvement. However, these methods require _extra labeled training data_, which is not applicable to our zero-shot setting.

Specifically, we follow  to assume \((_{1:K},)\) with identical covariance is the underlying distribution. In classical maximum likelihood estimation , the shared covariance \(\) is computed by averaging the empirical covariances of \(K\) classes: \(=_{j}_{j}\), where \(_{j}=_{j}|-1}_{_ {j}}(-_{j})(-_{j})^{}\). Here, one need the label information of each image to compute \(_{j}\). Fortunately, to avoid using label information, we can infer \(\) directly from the expectation and the second order moment of the marginal distribution \(()\).1 Using a Gaussian mixture model with the priors \(\{_{j}\}_{j=1}^{K}\), \(()\) is given by:

\[()=_{j=1}^{K}_{j}(;_ {j},),(;_{j},)=||}}\{-(-_{j})^{ }^{-1}(-_{j})\}\] (3)

Denote the second moment of \(\) as \(M\), we have (proof in Section A.1):

\[M=+_{j}_{j}_{j}_{j}^{}.\] (4)

Denote \(=[_{1},..,_{K}]^{}\), \(Z=[_{1},..,_{K}]^{}\), and the expectation of \(\) as \(\), the prior over the unlabeled data distribution can be estimated by (proof in Section A.2):

\[=Z^{-1}\] (5)

We estimate the expectation and the second order moment as \(}=_{i=1}^{N}_{i}\) and \(=_{i=1}^{N}_{i}_{i}^{}\), which are unbiased and consistent. In practice, given that test benchmarks are generally class-balanced, we use a uniform prior over the data distribution, _i.e._, \(_{j}=\). Combining with Eq. (4), the estimated shared covariance \(\) can be written as:

\[=-_{j}_{j}_{j}^{}.\] (6)

Let \(_{j}=^{-1}_{j}\) and \(b_{j}=-_{j}^{}_{j}\), the Gaussian discriminant analysis predicts the label for an image \(\) as follows (proof in Section A.3):

\[y=*{argmax}_{j}f_{}()_{j}=*{ argmax}_{j}_{j}^{}+b_{j}\] (7)

where \(f_{}()_{j}=_{j}^{}+b_{j}\) is the score for class \(j\).

### Prediction Fusion via Adaptive Calibration.

As a rule of thumb, combining the zero-shot predictions with the ones from the learned model can further improve performance for CLIP adaptations [44; 35; 40; 47; 38; 50]. Previous studies commonly employ a mixing coefficient, \(\), to balance the contributions of two models, _e.g._, \(f()=f_{}()+ f_{}()\). Typically, this hyper-parameter \(\) is optimized on labeled data to maximize accuracy. However, in our context, labels are unavailable, it is not possible to search for the optimal value of \(\). It is imperative to develop a mechanism that balances the prediction fusion without relying on the label.

Figure 2: Comparison of confidence.

The key in our prediction fusion lies in aligning the average confidence of the two models. Formally, the average confidence over the dataset \(\{_{i}\}_{i=1}^{N}\) scaled by a temperature \(\) is given by the average of the model's probability for its prediction:

\[(f,)=_{i=1}^{N}_{j}(f( _{i})/)_{j}.\] (8)

Ideally, a model's average confidence should reflect the predicted accuracy, which is called a well-calibrated model. Suppose we have the oracle well-calibrated models, denoted by \(f_{}^{}()\) and \(f_{}^{}()\), Kumart _et al._ prove that the optimal strategy is to fuse the two predictions equally, _i.e._, \(f_{}()=f_{}^{}()+f_{}^{ }()\). However, as shown in Figure 2, \(f_{}\) is much overconfident than \(f_{}\). Let \(f_{}()=Cf_{}^{}()\) for large \(C^{+}\) (an overconfident model magnifies its logits) and suppose \(f_{}() f_{}^{}()\). The fused predictions are given by \(f_{}()=Cf_{}^{}()+f_{}^{ }()\). For very large \(C\), \(f_{}()\) and \(f_{}()\) have the same predictions, _i.e._, \(f_{}()\) is biased towards the \(f_{}()\). As we do not have the label to compute accuracy, we cannot apply classical calibration methods [19; 10] to calibrate \(f_{}()\) and \(f_{}()\). As our desideratum is to automatically balance the contribution of the two models, we can optimize \(_{}\) to make the confidence of \(f_{}\) to match up the one of \(f_{}\), which circumvent the need of labels:

\[_{}=*{argmin}_{_{}}|(f_{},_{})-(f_{},_{})|\] (9)

Specifically, we implement this by binary search, as the confidence monotonically decreases as the temperature increases. \(_{}=0.01\) is fixed and learned by CLIP. The fused logits are given by:

\[f_{}()=f_{}()/_{}+f_{ }()/_{}\] (10)

### Correcting Pre-training Label Bias via Label-Free Logit Adjustment

Pre-training datasets typically exhibit a long-tailed concept distribution, leading to biased performance in zero-shot models [49; 25; 5; 1]. This bias occurs because zero-shot models reflect the posterior probability \((y|)\) derived from the pre-training distribution. According to Bayes' rule, this posterior probability is influenced by the pre-training label distribution \((y)\), as \((y|)(|y)(y)\). If the prior probability of class \(j\) is significantly larger than that of other classes (_e.g._, \((j)(i),\; i[K],i j\)), the predictions will be biased toward class \(j\).

Prior research [21; 14] has identified a theoretical optimal solution to address this label bias: let \(_{y}\) denote the prior probability of class \(y\), _i.e._, \(_{y}=(y)\). The debiased logit of \(f_{}()\) for class \(y\) should be (proof in Section A.4):

\[f_{}()_{y}=f_{}()_{y}-_{y}.\] (11)

Previous methods estimate \(\) either by accessing the pre-training data [25; 1] or counteract the influence of the prior by optimizing on labeled downstream data . However, these approaches are often impractical due to inaccessible pre-training labels due to privacy or copyright concerns or the necessity for labeled downstream data. In this work, we address label bias using only the unlabeled downstream data \(\{_{i}\}_{i=1}^{N}\).

Let \(s()=(f_{}())\) represent the softmax outputs of \(f_{}()\), where \(s()_{y}=}(y|)\) is the predicted probability for class \(y\). Define \(_{j}=_{}[s()|Y=j]\) as the expected posterior probability over the image distribution of class \(j\), and let \(S=[_{1},...,_{K}]^{K K}\). We prove that the pre-training label prior \(=[_{1},...,_{K}]^{}^{K}\) must satisfy the following linear equation system:

\[(S-I)=.\] (12)

**Remark.** The key point in Eq. (12) is that we avoid sampling from the pre-training data distribution; instead, we sample from \((|y)\), which is available from the downstream data. We provide the proof in Section A.5 and the numerical power solver for \(\) in Section A.6.

We iteratively refine the estimation of \(S\) and solve for \(\) using updated pseudo-labels generated by \(f_{}()\). As \(f_{}()\) becomes more precise, it yields more accurate pseudo-labels for \(\), which in turn enhances the accuracy of our estimation of \(\). Specifically, we initialize

\[^{0}=[1/K,...,1/K]^{},\;f_{}^{0}=f_{ },\;_{j}^{0}=_{j}^{0}|}_{ _{j}^{0}}s(),\;\;S^{0}=[_{1}^{0},...,_{K}^{0}]\] (13)

where \(_{j}^{0}\) if \(\) is classified as \(j\) by \(f_{}^{0}()\). We proceed by solving for \(^{1}\) using Eq. (12), refining \(f_{}^{1}()\) using Eq 11 and reassign the pseudo label using \(f_{}^{1}()\) to estimate the updated \(_{j}^{1}\). This process is repeated \(t\) times until the relative change in \(\) satisfies the convergence criterion:

\[^{t}-^{t-1}\|_{1}}{\| {}^{t-1}\|_{1}}=\|^{t}-^{t-1}\|_{1}< ,\|^{t-1}\|=1\;\] (14)

where \(\) is a predefined threshold for relative error tolerance. We summarize the algorithm for solving \(\) in Algorithm 2 and provide the overall pipeline in Algorithm 1.

**Discussion: Comparison with Other Prior Estimation Methods.** We compare existing methods for estimating pre-training label priors and demonstrate their in-applicability or flaws in our setting.

(1) _Explicit method_: the explicit method directly measures the frequency of each class in pre-training data, _e.g._, \(_{y}=}{N}\), where \(N_{y}\) is the sample size for class \(y\) and \(N\) is the total sample size. Most long-tail learning algorithms, _e.g._, LA and PC , are based on this method because they can access the training data. However, estimating such frequency is complex due to the free-form texts, as opposed to a pre-defined label set. In addition, the pre-training dataset is often inaccessible, making the method inapplicable in our case.

(2) _Implicit method_:  allow access to a portion of the pre-training data \(_{}\) and use the law of total probability to estimate the prior:

\[_{y}=(y)=_{}_{}() (y|)=_{_{}()}[_{}(y|)] _{}|}_{_{}} }_{}(y|)\] (15)

where \(}_{}(y|)\) denotes the zero-shot model. However, in our setting, we do have access to the pre-training data or a portion of it. Wang _et al._ replace the pre-training data \(_{}\) with the downstream data \(_{}\) in Eq. (15) to debias. However, this method neglects the distribution discrepancies between the pre-training and downstream data. In Section 4.3, we show that our debiasing significantly outperforms this implicit method.

(3) _TDE_: Tang _et al._ debias by removing features along a global direction, retaining only those orthogonal to it. Specifically, the global feature is estimated by \(}=_{}|}_{ _{}}\). Given a test sample \(\), TDE decomposes it into parallel and orthogonal directions to \(}\): \(=_{}+_{}\). Then, only the orthogonal component is used for classification: \(}_{}(y|_{})\). While TDE does not require labels for the samples, we cannot apply it because it requires sampling from the pre-training data. In Section 4.3, we replace \(_{}\) with downstream data \(_{}\) and demonstrate its inferior performance.

(4) _GLA_: Zhu _et al._ propose to estimate the pre-training prior from the downstream data using the Bayes optimal criterion. The pre-training prior \(\) is solved by optimizing:

\[=_{}_{(,y) _{}}[_{}(f_{}()- ,y)],\] (16)where \(_{}\) is the cross-entropy loss and \(f_{}()\) is the logit of the zero-shot model. While this method circumvents the need for pre-training data access, it is inapplicable because it requires labels for each downstream sample.

## 4 Experiments

### Setup

**Datasets.** We conduct experiments on 16 image classification benchmarks, covering diverse range categories including generic objects (ImageNet , Caltech ), scenes (SUN ), textures (DTD ), satellite images (EuroSAT ), actions (UCF ) and fine-grained categories (Pets , Cars , Flowers , Food , Aircraft ). Additionally, we evaluate on five ImageNet distribution shifted datasets : ImageNetV2 (IN-V2) , ImageNet-Sketch (IN-Sketch) , ImageNet-A (IN-A) , ImageNet-R (IN-R)  and ObjectNet .

**Implementation details.** We adopt CLIP  ViT-B/16 and ViT-L/14 as our pre-trained models. The default model for ablation studies is CLIP ViT-B/16. We use the same text descriptions as SuS-X  and CuPL , and adhere to the InMaP  settings to include all test images. \(_{}=0.01\) is provided by CLIP. \(\) in Algorithm 2 is set to \(0.01\). All experiments are conducted on a single NVIDIA 3090 GPU if not specified. Note that our algorithm _does not require_ any hyper-parameter searching.

### Main Results

We compare our method with several state-of-art methods, including CLIP , TPT , PromptAlign , SuS-X-DS , TDA , GPT4-Prompt , CuPL-CLIP , and InMaP . Both TPT and TDA utilize a stream of unlabeled test images. For TPT, TDA and InMaP, we produce the results of ViT-L/14 by executing the official released code and maintaining the same hyper-parameters.

**Results on 10 datasets.** In Table 1, we summarize the accuracy across all datasets, excluding ImageNet and its shifts (denoted as 10-datasets). Our method consistently shows superior performance across the datasets and backbones, significantly surpassing GPT4-Prompt, which is known for generating high-quality prompts. By integrating our method with InMaP, our Frolic achieves the highest performance, with an average improvement of \(2.6\%\) with ViT-B/16 and \(2.0\%\) with ViT-L/14.

    & Method & road & road & road & road & road & road & road & road & road & road & road \\   & CLIP  & 88.9 & 70.4 & 24.8 & 44.3 & 47.7 & 65.2 & 86.1 & 62.5 & 92.9 & 66.7 & 64.9 \\  & TPT  & 87.7 & 68.9 & 24.7 & 47.7 & 42.4 & 66.8 & 84.6 & 65.5 & 94.1 & 68.0 & 65.0 \\  & PromptAlign  & 90.7 & 72.3 & 24.8 & 47.2 & 47.8 & 68.5 & 86.6 & 67.5 & 94.0 & 69.4 & 66.8 \\  & SuS-X-DS  & 90.5 & 73.8 & 28.6 & 54.5 & 57.4 & 66.1 & 86.0 & 67.7 & 93.6 & 66.5 & 68.4 \\  & TDA  & 88.6 & 71.4 & 23.9 & 47.4 & 58.0 & 67.2 & 86.1 & 67.6 & 94.2 & 70.6 & 67.5 \\  & GPT4-Prompt  & 91.0 & 74.5 & 28.0 & 48.5 & 48.8 & 66.8 & 86.3 & 65.5 & 94.6 & 72.0 & 67.6 \\  & CuPL-CLIP  & 92.0 & 73.2 & 27.7 & 54.3 & 52.7 & 66.4 & 86.2 & 68.5 & 94.6 & 70.7 & 68.6 \\  & **Frolic** & **92.9** & **74.8** & **31.5** & **56.1** & **58.5** & **69.1** & **87.2** & **70.8** & **95.2** & **75.2** & **71.1** \\    & InMaP  & 92.9 & 71.8 & 28.4 & 48.0 & 64.1 & 70.6 & 87.7 & 70.5 & 93.1 & 74.0 & 70.1 \\  & + **Frolic** & **93.6** & **74.3** & **31.8** & **58.0** & **65.3** & **71.7** & **88.2** & **72.8** & **95.4** & **75.9** & **72.7** \\   & CLIP  & 93.5 & 79.3 & 32.4 & 53.0 & 58.0 & 76.8 & 91.0 & 67.5 & 94.8 & 74.2 & 72.0 \\  & TPT  & 93.6 & 76.2 & 31.9 & 55.2 & 51.8 & 77.7 & 88.9 & 70.2 & 95.5 & 74.9 & 71.5 \\   & TDA  & 93.5 & 80.5 & 34.7 & 56.7 & 64.1 & 78.3 & 90.9 & 71.5 & 95.9 & 76.6 & 74.2 \\   & GPT4-Prompt  & 94.1 & 81.5 & 36.3 & 54.8 & 54.1 & 77.9 & 91.4 & 70.3 & 96.2 & 80.6 & 73.7 \\   & CuPL-CLIP  & 94.3 & 79.8 & 35.5 & 62.7 & 61.2 & 78.0 & 91.3 & 72.4 & 96.7 & 75.9 & 74.7 \\   & **Frolic** & **94.9** & **82.4** & **40.0** & **64.1** & **66.2** & **80.8** & **91.8** & **74.5** & **97.2** & **80.0** & **77.1** \\    & InMaP  & 95.2 & 80.7 & 37.6 & 60.2 & 70.6 & 82.5 & 92.2 & 75.0 & 94.9 & 80.4 & 76.9 \\   & + **Frolic** & **95.4** & **81.8** & **42.1** & **66.9** & **71.0** & **83.5** & **92.4** & **77.3** & **97.3** & **82.2** & **78.9** \\   

Table 1: Comparison of accuracy (%) on 10 datasets for CLIP ViT-B/16 and ViT-L/14.

**Results on ImageNet and associated five shifts.** In Table 2, our Frolic again surpasses the comparison methods, achieving the average accuracy of \(64.4\%\) and \(75.1\%\) with ViT-B/16 and ViT-L/14, respectively. Additionally, we observe improvements on the distribution shift datasets: IN-V2, IN-Sketch, IN-A, and IN-R with ViT-B/16, and on IN-A and IN-R with ViT-L/14, when our Frolic is combined with InMaP. However, these results still lag behind the original performance of our Frolic. This discrepancy may stem from the hyper-parameters in InMaP being optimized specifically for ImageNet; applying them unchanged to its shifted datasets could lead to over-fitting.

### Ablation Studies and Further Analysis

**Effectiveness of the prompt distribution learning.** In Table 3 (Row (1) & (3)), we compare the performance of the original CLIP model \(f_{}\) with our prompt distribution learning model \(f_{}\). We observe that modeling the underlying distribution of the text prototypes results in notable performance gains. For example, \(3.7\%\) accuracy improvement on 10-datasets using ViT-B/16.

**Effectiveness of the prediction fusion.** As described in Eq.(10), our Frolic fuses the original CLIP \(f_{}\) and the prompt distribution learning model \(f_{}\) via confidence matching. We compare the simple fusion \(f_{}+f_{}\) and our adaptive fusion \(f_{}=f_{}/_{}+f_{}/_{}\) in Table 3 (Row (4) & (5)). We show that our fusion technique outperforms the simple fusion by a large margin. Recall that our adaptive fusion method addresses situations where \(f_{}\) is more overconfident than \(f_{}\). In Figure 3, we illustrate the relationship between performance gains over simple fusion--_i.e._, \((f_{}/_{}+f_{}/_{})-(f_{}+f_{})\)--and the confidence difference--_i.e._, \(|(f_{},1)-(f_{},_{})|\). We present this as a scatter plot where each point represents a dataset, and we have fitted these points with a line. As expected, larger confidence differences correlate with more significant improvements.

**Effectiveness of the bias correction.** Row (2) and (6) in Table 3 demonstrate the effectiveness of our debiasing method, which can further improve the base CLIP model \(f_{}\) and the fusion model

    & Method & IN & IN-V2 & IN-Sketch & IN-A & IN-R & ObjectNet & Average \\   ViT-L/14 \\ (2) \\  } & CLIP  & 68.7 & 62.2 & 48.3 & 50.6 & 77.7 & 53.5 & 60.1 \\  & TPT  & 68.9 & 63.4 & 47.9 & 54.7 & 77.0 & 55.1 & 61.1 \\  & TDA & 69.5 & 64.6 & 50.5 & 60.1 & 80.2 & 55.1 & 63.3 \\  & GPT4-Prompt  & 68.7 & 62.3 & 48.2 & 50.6 & 77.8 & 53.7 & 60.2 \\  & CuPL-CLIP  & 69.9 & 64.4 & 49.4 & 59.7 & 79.5 & 53.7 & 62.7 \\  & **Frolic** & **70.9** & **64.7** & **53.3** & **60.4** & **80.7** & **56.6** & **64.4** \\   & InMaP  & 72.5 & 62.3 & 49.4 & 52.2 & 79.2 & 54.5 & 61.6 \\  & + **Frolic** & **73.3** & **63.8** & **52.9** & **52.8** & **79.6** & **56.4** & **63.1** \\   ViT-L/14 \\ (2) \\  } & CLIP  & 75.9 & 70.2 & 59.7 & 70.9 & 87.9 & 65.5 & 71.6 \\  & TPT  & 75.5 & 70.0 & 59.8 & 74.7 & 87.9 & 68.0 & 72.6 \\   & TDA & 76.3 & 71.5 & 61.3 & 77.9 & 89.8 & 67.0 & 73.9 \\   & GPT4-Prompt  & 75.3 & 70.3 & 59.9 & 71.2 & 87.8 & 65.7 & 71.7 \\   & CuPL-CLIP  & 76.2 & 71.9 & 60.7 & 77.9 & 89.6 & 65.7 & 73.6 \\   & **Frolic** & **77.4** & **72.5** & **63.1** & **78.9** & **90.3** & **68.7** & **75.1** \\    & InMaP  & 79.3 & 72.1 & 65.1 & 62.5 & 84.8 & 71.0 & 72.4 \\   & + **Frolic** & **79.7** & **73.1** & **65.7** & **64.0** & **85.9** & **71.7** & **73.3** \\   

Table 2: Comparison of accuracy (%) on ImageNet and its variants for CLIP ViT-B/16 and ViT-L/14.

    &  &  &  \\  & & 10-datasets & ImageNet & IN-Variants & 10-datasets & ImageNet & IN-Variants \\   (1) \\ (2) \\  & \(f_{}\) & 65.1 & 68.7 & 58.5 & 72.0 & 75.9 & 72.3 \\  (2) \\  & \(f_{}-\) & 68.4 & 69.7 & 61.2 & 75.1 & 76.2 & 73.4 \\   (3) \\ (4) \\  & \(f_{}\) & 68.8 & 69.8 & 61.3 & 74.7 & 76.0 & 73.1 \\  \(4) \\  & \(f_{}+f_{}\) & 66.3 & 68.9 & 59.1 & 72.5 & 76.1 & 72.4 \\  (5) \\  & \(f_{}=f_{}/_{}+f_{}/_{}\) & 70.4 & 69.8 & 61.9 & 75.5 & 76.9 & 73.9 \\  
 (6) \\  & \(f_{}=f_{}-\) & **71.1** & **70.9** & **63.1** & **77.2** & **77.4** & **77.4** \\   

Table 3: Accuracy (%) of different models on 10-datasets, ImageNet and its five variant datasets.

across various backbones and datasets. We also compare our debiasing method with other label bias correction methods in Table 4. The descriptions of TDE  and the Implicit method can be found in Section 3.4. The results reveal that TDE  does not consistently perform well across all datasets. In contrast, while the implicit method using downstream data enhances zero-shot performance, it underperforms compared to our debiasing method, which shows an average gain of \(1.6\%\) over the implicit method. To further assess our method's potential, we replaced pseudo-labeling with ground truth labels. The results reveal that the maximum achievable accuracy surpasses our method by \(1.0\%\), highlighting the importance of our iterative approach for more accurate pseudo-labeling.

**Convergence of Algorithm 1.** Our method Frolic, as described in Algorithm 2, iteratively solves for the prior \(\). In Figure 4, we examine the convergence by displaying the errors \(_{1}=\|^{t}-^{t-1}\|_{1}\) and the accuracy across iterations. We find that the resultant accuracy saturates after only 6 steps, and the relative \(_{1}\) error decreases to less than \(=0.01\) after 10 steps.

**Comparison with other prompt-based methods.** The popular prompt-based methods, such as CoOp  and CoCoOp , require a training procedure with labeled samples while our method does not involve any training. To ensure a fair comparison, we compare our Frolic with CoOp and CoCoOp on across-dataset results, where the CoOp and CoCoOp are trained only with the labeled samples from the ImageNet dataset and then directly tested on the remaining datasets. The results shown in Table 5 demonstrate that our Frolic not only avoids the complexities of training but also exhibits superior generalization performance compared to these methods.

**Comparison with adapter-based methods.** The adapter-based methods, _e.g._, LFA  and Tip-Adapter  boost the CLIP's generalization using labeled training samples. In contrast, our Frolic doesn't require any labeled samples. We evaluate our method with LFA and Tip-Adappert on the

   Model & \(^{t}\) & \(^{t}\) & \(^{t}\) & \(^{t}\) & \(^{t}\) & \(^{t}\) & \(^{t}\) & \(^{t}\) & \(^{t}\) & \(^{t}\) \\  CoOp  & 71.5 & 93.7 & 89.1 & 64.5 & 68.7 & 85.3 & 18.4 & 64.1 & 41.9 & 46.3 & 66.5 \\ CoCoOp  & 71.0 & 94.4 & 90.1 & 65.3 & 71.8 & 86.0 & 22.9 & 67.3 & 45.7 & 45.3 & 68.2 \\
**Frolic\({}^{*}\)** & **73.3** & **95.4** & **93.6** & **71.7** & **74.3** & **88.2** & **31.8** & **72.8** & **58.0** & **65.3** & **75.9** \\   

Table 5: Comparison of accuracy (\(\%\)) between our Frolic and prompt-based methods for CLIP ViT-B/16. \(*\) denotes our method built upon InMaP ImageNet and its variants dataset, where the LFA and Tip-Adapter only utilize the labeled samples from the ImageNet dataset. The results in Table 6 show that our method achieves the best performance across all datasets with nearly 3% improvements in averaged accuracy over LFA.

**Running time.** Our method Frolic is completely training-free, unlike prompt tuning approaches such as TPT  and TDA , which involve back-propagating through an expensive encoder during optimization. We assess the wall-clock time of Frolic, TPT, and TDA in Table 7, using the CLIP ViT-B/16 model on ImageNet. These evaluations are conducted on a single NVIDIA A100 GPU. The results indicate that our method not only requires less time but also delivers superior performance.

## 5 Societal Impact, Limitation and Conclusion

**Societal impact and limitation.** Models pre-trained on large-scale web-crawled datasets may incorporate knowledge from noisy or malicious samples.

**Limitation.** Our approach assumes that the feature representations follow a mixture of Gaussian; however, this assumption may not always hold. On the other hand, the quality and distribution of data used in pre-training can significantly impact the performance of pre-trained models. Our method relies on the capabilities of pre-trained models for downstream tasks, if the pre-trained knowledge differs from the downstream tasks, the efficacy of our method may be limited.

**Conclusion.** In this work, we propose label-**F**ree **prompt** distribution **learning** and **bias** correction, dubbed as **Frolic**, framework to boost the performance of zero-shot models. Our Frolic models each class prototype via a Gaussian distribution and fuses the learned model with the original CLIP  via confidence matching. The proposed framework further effectively removes the label bias without accessing to the pre-training data. Extensive experiments across various datasets demonstrate the effectiveness of our approach.

   Model & IN & IN-A & IN-V2 & IN-R & IN-Sketch & Average \\  LFA  & 72.6 & 51.5 & 64.7 & 76.1 & 48.0 & 62.5 \\ Tip-Adapter  & 70.5 & 49.8 & 63.1 & 76.9 & 48.1 & 61.6 \\
**Frolic\({}^{*}\)** & **73.3** & **52.8** & **63.8** & **79.6** & **52.9** & **64.4** \\   

Table 6: Comparison of accuracy (\(\%\)) between our Frolic and adapter-based distribution methods for CLIP ViT-B/16. \(*\) denotes our method built upon InMaP 

   Model & Running Time & Accuracy \\  CLIP  & 6min & 68.7 \\ TPT  & 6h & 68.9 \\ TDA  & 15min & 69.5 \\
**Frolic** & 6.5min & 71.1 \\   

Table 7: Comparison of running time on ImageNet with ViT-B/16.