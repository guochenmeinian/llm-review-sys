# Feature Likelihood Divergence: Evaluating the Generalization of Generative Models Using Samples

Marco Jiralerspong

Universite de Montreal and Mila

&Avishek (Joey) Bose

McGill University and Mila

&Ian Gemp

Google Deepmind

&Chongli Qin

Google Deepmind

&Yoram Bachrach

Google Deepmind

&Gauthier Gidel

Universite de Montreal and Mila

e-mail correspondence to marco.jiralerspong@mila.quebec

###### Abstract

The past few years have seen impressive progress in the development of deep generative models capable of producing high-dimensional, complex, and photo-realistic data. However, current methods for evaluating such models remain incomplete: standard likelihood-based metrics do not always apply and rarely correlate with perceptual fidelity, while sample-based metrics, such as FID, are insensitive to overfitting, i.e., inability to generalize beyond the training set. To address these limitations, we propose a new metric called the Feature Likelihood Divergence (FLD), a parametric sample-based metric that uses density estimation to provide a comprehensive trichotomic evaluation accounting for novelty (i.e., different from the training samples), fidelity, and diversity of generated samples. We empirically demonstrate the ability of FLD to identify overfitting problem cases, even when previously proposed metrics fail. We also extensively evaluate FLD on various image datasets and model classes, demonstrating its ability to match intuitions of previous metrics like FID while offering a more comprehensive evaluation of generative models. Code is available at https://github.com/marcojira/fld.

## 1 Introduction

Generative modeling is one of the fastest-growing areas of deep learning, with success stories spanning the artificial intelligence spectrum (Karras et al., 2020; Brown et al., 2020; Wu et al., 2021; Rombach et al., 2022). Despite the growth of applications--and unlike supervised or reinforcement learning--there is a lack of a clear consensus on an evaluation protocol in high-dimensional data regimes in which these models excel. In particular, the standard metric of evaluating log-likelihood of held-out test data (Bishop and Nasrabadi, 2006; Goodfellow et al., 2016; Murphy, 2022) fails to provide a meaningful evaluation signal due to its large variability between repetitions (Nowozin et al., 2016) and lack of direct correlation with sample fidelity (Theis et al., 2015).

Departing from pure likelihood-based evaluation, sample-based metrics offer appealing benefits such as being able to evaluate any generative model family via their generated samples. Furthermore, sample-based metrics such as Inception score (IS) (Salimans et al., 2016), Frechet Inception distance (FID) (Heusel et al., 2017), precision, and recall (Lucic et al., 2018; Sajjadi et al., 2018) have been shown to correlate with sample quality, i.e. the perceptual visual quality of a generated sample, and the perceptual sample diversity. Despite being the current defacto gold standard, sample-basedmetrics miss important facets of evaluation (Xu et al., 2018; Esteban et al., 2017; Meehan et al., 2020). For example, on CIFAR10, the current standard FID computation uses 50k generated samples and 50k training samples from the dataset, a practice that does not take into account overfitting. Consider the worst-case scenario of a model called _copycat_ which simply outputs copies of the training samples. Using the standard evaluation protocol, such a model would obtain a _FID of 0_ (as we compare distances of two identical Gaussians)--a perfect score for a useless model. Instead, we could try looking at the FID of the copycat relative to the test set. While this improves the situation somewhat (see Fig. 2), copycat still obtains _better than SOTA test FID_, demonstrating that aiming for the lowest FID is highly vulnerable to overfitting.

While old generative models struggled to produce good quality samples, recent models have demonstrated the ability to memorize (Somepalli et al., 2022) and overfit (Yazici et al., 2020). With the widespread adoption of deep generative models in high-stakes and industrial production environments, important concerns regarding data privacy (Carlini et al., 2023; Arora et al., 2018; Hitaj et al., 2017) should be raised. For instance, in safety-critical application domains such as precision medicine, data leakage in the form of memorization is unacceptable and severely limits the adoption of generative models--a few of which have been empirically shown to be guilty of "digital forgery" (Somepalli et al., 2022). These concerns highlight the limitations with current evaluation metrics:

There are currently no _sample-based evaluation metrics_ accounting for the trichotomy

between sample _fidelity, diversity, and novelty_ (Figure 2).

We believe this trichotomy encompasses what is required for a generative model to have the desired generalization properties, i.e., a "good" generative model should generate samples that are diverse and perceptually indistinguishable from the training data distribution, but _at the same time_, different from that training data. In other words, the more generated data looks like _unseen_ test samples, the better. Additionally, by assessing the novelty of generated samples in relation to the training set, we can better identify potential privacy and copyright risks.

**Main Contribution**. We propose the feature likelihood divergence (FLD): a novel sample-based metric that captures sample fidelity, diversity, and novelty. FLD enjoys the same scalability as popular sample-based metrics such as FID and IS but crucially also assesses sample novelty, overfitting, and memorization. Evaluation using FLD has many consequential benefits:

1. **Explainability:** Samples that contribute the most (and the least) to the performance are identified.
2. **Diagnosing Overfitting:** As overfitting begins (i.e., copying of the training set) FLD identifies the copies and reports an inferior value _despite_ no drop in sample fidelity and diversity.
3. **Holistic Evaluation:** FLD simultaneously is the only metric proposed in the literature that simultaneously evaluates the fidelity, diversity, and novelty of the samples (Fig. 2).
4. **Universal Applicability:** FLD applies to all generative models, including VAEs, Normalizing Flows, GANs, and Diffusion models with minimal overhead as it is computed only using samples.
5. **Flexibility:** Because of its connection with likelihood, FLD can be naturally extended to conditional and multi-modal generative modeling.

Intuitively, FLD achieves these goals by first mapping samples to a perceptually meaningful feature space such as a pre-trained Inception-v3 (Szegedy et al., 2016) or DINOV2 (Oquab et al., 2023). Then, FLD is derived from the likelihood evaluation protocol that assesses the generalization performance of generative models in a similar manner to supervised learning setups. As most models lack explicit densities, we model the density of the generative model in our chosen feature space by using a mixture of isotropic Gaussians (MoG), whose means are the mapped features of the generated samples. We then fit the variances of the Gaussians to the train set in such a way that memorized samples obtain vanishingly small variances and thus worsen the density estimation of the MoG. Finally, we use the MoG and estimate the perceptual likelihood of some held-out test set.

## 2 Background and Related Work

Given a training dataset \(_{}=\{_{i}\}_{i=1}^{n}\) drawn from a distribution \(p_{}\), one of the key objectives of generative modeling is to train a parametric model \(g\) that is able to generate novel synthetic yet high-quality samples--i.e., the distribution \(p_{g}\) induced by the generator is close to \(p_{}\).3

**Likelihood Evaluation**. The most common metric, and perhaps most natural, is the negative log-likelihood (NLL) of the test set, whenever it is easily computable. While appealing theoretically, generative models typically do not provide a density (e.g. GANs) or it is only possible to compute a lower bound of the test NLL (e.g. VAEs, continuous diffusion models, etc.) (Burda et al., 2015; Song et al., 2021; Huang et al., 2021). Even when possible, NLL-based evaluation suffers from a variety of pitfalls in high dimensions (Theis et al., 2015; Nowozin et al., 2016) and may often not correlate with higher sample quality (Nalisnick et al., 2018; Le Lan and Dinh, 2021). Indeed many practitioners have empirically witnessed phenomena such as mode-dropping, mode-collapse, and overfitting (Yazici et al., 2020), all of which are not easily captured simply through the NLL.

**Sample-based Metrics**. As all deep generative models are capable of producing samples, an effective way to evaluate these models is via their samples. Such a strategy has the benefit of bypassing the need to compute the exact model density of a sample point--allowing for a unified evaluation setting. More precisely, given \(_{}=\{^{}\}_{i=1}^{m}\) generated samples, where each \(^{} p_{g}\) and \(_{}=\{^{}\}_{i=1}^{n}\) drawn from \(p_{}\), the goal is to evaluate how "good" the generated samples are with respect to the real data distribution. Historically, sample-based metrics for evaluating deep generative models have been based on two ideas: 1. using an Inception network (Szegedy et al., 2016) backbone \(\) as a feature extractor to 2. compute a notion of distance (or similarity) between the generated and the real distribution. The Inception Score (IS) and the Frechet Inception Distance (FID) are the two most popular examples and can be computed as follows:

IS: \(e^{_{i=1}^{m}(p_{}(y|_{i}^{})||p_{}(y))}\), FID: \(\|_{g}-_{p}\|^{2}+(_{g}+_{p}-2(_{g}_ {p})^{1/2})\)

where \(p_{}(y|x)\) is the probability of each class given by the Inception network \(\), \(p_{d}(y)\) is the ratio of each class in the real data, \(_{g}:=_{i=1}^{m}(_{i}^{}),_{p }:=_{i=1}^{n}(_{i}^{})\) are the empirical means of each distribution, and \(_{g}:=_{i=1}^{m}(_{i}^{}-_{g})( _{i}^{}-_{g})^{}\), \(_{p}:=_{i=1}^{n}(_{i}^{}-_{p})( _{i}^{}-_{p})^{}\) are the empirical covariances.

The popularity of IS and FID as metrics for generative models is motivated by their correlation with perceptual quality, diversity, and ease of use. More recently, other metrics such as KID (Binkowski et al., 2018) (an unbiased version of FID) and precision/recall (which disentangles sample quality and distribution coverage) (Sajjadi et al., 2018) have added nuance to generative model evaluation.

**Overfitting Evaluation**. Several approaches seek to provide metrics to detect overfitting and can be categorized based on whether one can extract an exact likelihood (van den Burg and Williams, 2021) or a lower bound to it via annealed importance sampling (Wu et al., 2016). For GANs, popular approaches include training an additional discriminator in a Wasserstein GAN (Adlam et al., 2019) and adding a memorization score to the FID (Bai et al., 2021). Alternate approaches include finding real data samples that are closest to generated samples via membership attacks (Liu et al., 2018; Webster et al., 2019). Non-parametric tests have also been employed to detect memorization or exact data copying in generative models (Xu et al., 2018; Esteban et al., 2017; Meehan et al., 2020). Parametric approaches to detect data copying have also been explored such as using neural network divergences (Gulrajani et al., 2020) or using latent recovery (Webster et al., 2019). Finally, the [Meehan et al., 2020] test statistic and Alaa et al.  proposes a multi-faceted metric with a binary sample-wise test to determine whether a sample is authentic (i.e., overfit).

## 3 Feature Likelihood Divergence

We now introduce our Feature Likelihood Score (FLD) which is predicated on the belief that a proper evaluation measure for generative models should go beyond sample quality and also inform practitioners of the generalization capabilities of their trained models. While previous sample-based methods have foregone density estimation in favor of computing distances between sample statistics, we seek to bring back a likelihood-based approach to evaluating generative models. To do so, we first propose our method for fitting a mixture of Gaussians (MoGs) to estimate the _perceptual_ density of high-dimensional samples in a way that accounts for _overfitting_. Specifically, our method aims at attributing 1) a good NLL to high-quality, non-overfit images and 2) a poor NLL in cases of overfitting.

Intuitively, we say a generative model is overfitting if on average the distribution of generated samples is closer to the training set than the test set in feature space. We seek to characterize this exact behavior with FLD by a MoG where the variance parameter \(^{2}\) of each Gaussian approaches zero around generated samples that are responsible for overfitting. In section 3.1 we deepen this intuition by outlining how the MoG used in FLD can be tuned to assess the perceptual likelihood of samples while punishing memorized samples, before giving a precise definition for memorization and overfitting under FLD in section 3.2.

### Overfitting Mixtures of Gaussians

Our method consists of a simple sample-based density estimator amenable to a variety of data domains inspired by a traditional mixture of Gaussians (MoG) density estimator with a few key distinctions. Figure 3 summarizes the \(4\) key steps in computing FLD using our MoG density estimator (also detailed in Algorithm 1 in Appendix SSD.1) which we now describe below. While it is indeed possible to train any other density model, a MoG offers a favorable tradeoff in being simple to use--while still being a universal density estimator [Nguyen et al., 2020]--and enjoying efficient scalability to large datasets.

**Step 1: Map to the feature space**. The first change we make is to use some map \(\) to map inputs to some perceptually meaningful feature space. Natural choices for this include the representation space of Inception-v3 and DINOv2. While still high-dimensional, we ensure that a larger proportion of dimensions are useful and that the resulting \(_{2}\) distances between images are more meaningful.

**Step 2: Model the density using a MoG.** As in kernel density estimation (KDE), to estimate a density from some set of points \(_{}=\{_{j}^{}\}_{j=1}^{m}\) we center an isotropic Gaussian around each point--i.e., the mean of the Gaussian is the coordinates of the point. This means that \(j\)-th data point has a Gaussian \(((_{j}^{}),\,_{j}^{2}I_{d})\). Then, to compute the likelihood of a new point \(\), we simply calculate the mean likelihood assigned to that point by all Gaussians in the mixture:

\[p_{}(|_{}):=_{j=1}^{m} (()|(_{j}^{}),\, _{j}^{2}I_{d})=_{j=1}^{m}_{j})^{d}} (_{j}^{})-()\| ^{2}}{2_{j}^{2}})\] (1)

with the convention that \((()|(^{}),\,0_{d})\) is a dirac at \((^{})\). Henceforth, we denote this MoG estimator which has fixed centers initialized to a dataset (e.g. train set, generated set) as \((();)\), where \(\) is a diagonal matrix of bandwidths parameters--i.e. \(^{2}I\), where \(^{2}\) is a vector.

**Step 3: Use the train set to select \(_{j}^{2}\)**. An important question in kernel density estimation is selecting an appropriate bandwidth \(_{j}^{2}\). Overwhelmingly, a single bandwidth is selected which can either be derived statistically or by minimizing some loss through cross validation [Murphy, 2012]. We depart from this single bandwidth philosophy in favor of separate \(_{j}^{2}\) values for each Gaussian. To select \(_{j}^{2}\), instead of performing standard cross-validation on samples from \(p_{g}\), we fit the bandwidths using

Figure 3: Steps involved in our overfit mixture of Gaussians illustrated on a 2D example

a subset of training examples \(\{(_{i}^{})\}_{i=1}^{n}\) by minimizing their negative log-likelihood (NLL). Specifically, we solve the following optimization problem:

\[^{2}_{^{2}}_{i=1}^{n}( _{j=1}^{m}_{j})^{d}}(_{i}^{})-(_{i}^{ })||^{2}}{2_{j}^{2}})+_{i})\] (2)

where \(_{i}\) is a base likelihood given to each sample (see Appendix SSD for details). In particular, by centering each Gaussian on each \(_{j}^{}_{}\) and fitting each \(_{j}^{2}\) to the train set, we aim to have memorized samples obtain an overly small \(_{j}^{2}\), worsening the quality of the MoG estimator (and thus penalizing memorization). The following proposition (proof in SSF) formalizes this intuition.

**Proposition 1**.: _Let \(D_{ij}:=||(_{j}^{})-(_{i}^{})||^{2}\) be the distance between a generated sample and a train sample. Assume \( i,j:D_{ij}\) with \(_{j}:=_{i}D_{ij}\). Then, for any \(l\{1,,m\}\), we have that \(_{l}=O(_{l})\) where \(^{2}\) is a solution of Eq. 2._

Proposition 1 implies that each element of the training set that has been memorized induces a Dirac in the MoG density Eq. 1. Thus, one can identify copies of training samples with the learned density. More generally, if one of the generated samples is unreasonably close to a training sample, its associated \(^{2}\) will be very small as this maximizes the likelihood of the training sample. We illustrate this phenomenon with the Two-Moons dataset (Pedregosa et al., 2011) in Figure 4. Note that since this dataset is low-dimensional, we do not need to use a feature extractor (Step 1). In Figure 4 we can see that the more approximate copies of the training set appear in the generated set, the more the estimated density (using Eq. 2) contains high values around approximate copies of the training set. As such, overfitted generated samples yield an overfitted MoG that does not model the distribution of real data \(p_{}\) and will yield poor (i.e., low) log-likelihood on the test set \(_{}\).

**Step 4: Evaluate MoG density**. A foundational concept used by FLD is to evaluate the perceptual negative log-likelihood of a held-out test set using an MoG density estimator. To quantitatively evaluate the density obtained in Step 3, we evaluate the negative log-likelihood of \(_{}\) under \(p_{}()\). As demonstrated in Figure 4, in settings with \(k>0\), the generated samples are too close to the training set, meaning that all test samples will have a high negative log-likelihood (as they are far from the center of Gaussians with low variances). Evaluation of the test set provides a succinct way of measuring the generalization performance of our generative model, which is a key aspect that is lost in metrics such as IS and FID. Our final FLD score is thus given by the following expression:

\[(_{},_{}):=-  p_{}(_{}|_{})-C,\] (3)

where \(d\) is the dimension of the feature space \(\) and is equivalent to looking at the \(d^{th}\) root of the likelihood, and \(C\) is a dataset dependant constant.4 As a result of this adjustment by a constant, FLD is essentially estimating the forward Kullback-Leibler (KL) divergence (up to a constant factor) between the learned MoG distributions of the true data and the generated data. Higher FLD score values are indicative of problems in some of the three areas evaluated by FLD. Poor sample fidelity leads to Gaussian centers that are far from the test set and thus a higher NLL. Similarly, a failure to sufficiently cover the data manifold will lead to some test samples yielding very high NLL. Finally, overfitting to the training set will yield a MoG density estimator that overfits and yield a poor NLL value on the test set.

**Computational Complexity of FLD**. To quantify the computational complexity of FLD we plot, in Figure 12 in SSA.2, the computation time of various metrics with the number of samples. As depicted, we observe a linear scaling in the number of train samples for FLD which is in line with the computational cost of popular metrics like FID. Finally, the cost of computing FLD--and other metrics--is dwarfed by the cost of generating samples and then mapping them to an appropriate feature space which is a one-time cost and can be done prior to any metric computation.

### Detecting Memorization and Overfitting

**Memorization**. One of the key advantages of FLD over other metrics like FID is that it can be used to precisely characterize memorization at the sample level. Intuitively, memorization occurs when a generated sample \(_{j}^{}\) is an approximate copy of a training sample \(_{i}^{}\). By Proposition 1, such a phenomenon encourages the optimization of Eq. 2 to select a \(_{j}^{2} 1\) to achieve a high training NLL. As a result, such samples will assign a disproportionately high likelihood to that \(_{i}^{}\). To quantify this phenomenon, we compute the train likelihood assigned by each fitted Gaussian.

**Definition 3.1**.: _Let \(>0\). The sample \(x_{j}^{}\) is said to be \(\)-memorized if_

\[_{j}:=_{i}((x_{i}^{})|(x_ {j}^{});}^{2}I)>\,.\] (4)

The quantity \(_{j}\) is appealing because it is efficient to compute, and the distribution of \(\{_{j}\}\) allows us to quantify what is a "large" value for \(\) and identify the generated samples that are the most-likely copies of the training set. In SS4.1.3, we explore this method to assess sample novelty.

**Overfitting**. Intuitively, a generative model is overfitting when it is more likely to generate samples closer to the train set than to the (unseen) test set. Thus, overfitting for deep generative models can be precisely defined using the standard tools for likelihood estimation.

**Definition 3.2**.: _Given samples \(_{}=\{^{}\}_{i=1}^{n}\) from a generative model \(G\) trained on \(_{}\) and unseen test samples \(_{}\). We say that \(G\) is overfitting if \( p_{}(_{}|_{})<  p_{}(_{}|_{})\), i.e if:_

\[:=(_{}, _{})-(_{},_ {})<0\] (5)

For FLD, this effect is particularly noticeable due to the MoG being fit to the training set. In fact, samples that are too close to the train set relative to the test set have two effects: they worsen the density estimation of the MoG (increasing \((_{},_{})\) and assign higher likelihood to the train set (lowering \((_{},_{})\)). In section 4.1.3 and Tab. 1 experiments we empirically validate the overfitting behavior of popular generative models using our above definition and also visualize samples that are most overfit.

**Evaluating individual sample fidelity**. While FLD focuses on estimating the density learned by the generative model, it is also possible to estimate the density of the data and use that to evaluate the likelihood of the generated samples5. In particular, instead of centering the Gaussians at the generated samples, we can instead **center them at the test set**. Then, after fitting this MoG to the train set, we compute the likelihood it assigns to generated samples and use that as a measure of sample quality. More formally:

\[_{j}:=((_{j}^{})|( _{});).\] (6)

As is the case for \(_{j}\), \(_{j}\) is easy to compute once the Mog is fit and can be used to rank and potentially filter out poor fidelity samples.

## 4 Experiments

We investigate the application of FLD on generative models that span a broad category of model families, including popular GAN and diffusion models. For datasets, we evaluate a variety of popular natural image benchmarks in CIFAR10 (Krizhevsky et al., 2014), FFHQ (Karras et al., 2019) and ImageNet (Deng et al., 2009). Through our experiments, we seek to validate the correlation between FLD and sample fidelity, diversity, and novelty.

Stein et al. (2023) find that the DINOV2 feature space allows for a more comprehensive evaluation of generative models (relative to Inception-V3) and correlates better with human judgement. As such, for our experiments, unless indicated otherwise, we map samples to the DINOV2 feature space (Oquab et al., 2023). We do so even for other metrics (e.g. FID, Precision, Recall, etc.) which have typically used other feature spaces (comparisons with vanilla FID are provided in Appendix SSC).

### Trichotomic evaluation of FLD

We now experimentally validate FLD's ability to evaluate the samples of generative models along the three axes of fidelity, diversity and novelty.

#### 4.1.1 Sample Fidelity

We evaluate the effect of 2 types of transformations on FLD and plot the results in Fig. 5. The first consists of minor, almost imperceptible transformations (very minor gaussian blur, posterizing and converting to a high quality JPG) that are problematic for FID. As described in (Parmar et al., 2022) small changes to images such as compression or the wrong form of anti-aliasing increase FID substantially despite yielding essentially indistinguishable samples. These transformations affect FLD but _noticeably less than FID_. This phenomenon occurs even when we use DINOV2 for both FLD and FID (though the effect is more drastic using the original Inception-V3 feature space, see Appendix SSC).

Specifically, when all the samples are transformed, FID rates the imperceptibly transformed samples PFGM++ samples as worse than those produced by StyleGAN-XL (or even worse than StyleGAN2-ada). On the other hand, while the imperceptible transforms yield slightly worse FLD values (in part due to the feature space being sensitive to them), the FLD values are barely changed for the "Posterize" and "Light Blur" transforms and only somewhat worse for "JPG 90". The second type of transformations are larger transformations that affect the structure of the image (e.g. cropping, rotation, etc.) and have a significant negative impact on both FLD and FID.

#### 4.1.2 Sample Diversity

We consider two experiments on CIFAR10 to evaluate the effect of mode coverage and diversity on FLD. For the first, we vary the number of classes included in the set of generated samples for various conditional generative models. For the second, instead of the original \(n\) generated samples, we look at a set comprised of \(\) samples from the original set combined with \((k-1)\) approximate copies of each of those samples (i.e. for a total of \(n\) samples). In both cases, we find a strong and consistent relationship (across all models) indicating that both sample diversity and mode coverage are important to get good FLD values and report our findings in Fig. 6.

#### 4.1.3 Sample Novelty

We now study the effect of data copying on CIFAR10 on evaluation metrics for generative models. As overfitting can be more subtle than direct copying of the training set, we also consider transformed

Figure 5: Starting from a set of SOTA samples produced by PFGM++, we replace each sample with a transformed copy. **Left:** Effect of nearly imperceptible transformations on FLD and FID (with corresponding values for various models as reference). **Right:** Effect of large transformations on FLD and FID.

copies of the train set. For each transform in Fig. 7, we start with PFGM++ samples that have had the transform applied and gradually replace them with transformed copies of the training set. As such, sample fidelity/diversity in this "pseudo-generated" set remains roughly constant while overfitting increases.

We then evaluate this set of samples using a variety of metrics designed to detect overfitting. For FLD, we look at the generalization gap \((_{},_{})-( _{},_{}))\). For FID, we consider the difference \(_{}-_{}\) (where \(_{}\) is FID using the test set). We use the same amount of samples (10k) for both as FID is biased. \(C_{T}\)[Moehan et al., 2020] is the result of a Mann-Whitney test on the distribution of distances between generated and train samples compared to the distribution of distances between train samples and test samples (negative implies overfit, positive implies underfit). The \(\) is derived from authenticity described in [Alaa et al., 2022] and is simply the percentage of generated samples deemed authentic by their metric.

We find that FLD is the only metric consistently capable of detecting overfitting for all transforms (though the effect is less pronounced for large transforms). The generalization gap of FID oscillates though generally trends in the right direction. However, the magnitude of the gap is small (considering that the FID values using DINOv2 are considerably larger, e.g. the difference between StyleGAN2-ADA and StyleGAN-XL is >60). AuthPct trends in the right direction but seems to only be able to detect a subset of the memorized samples (as it detects none of the sample sets as overfit). Finally, \(C_{T}\) is capable of detecting overfitting for all but the largest transforms (though it does require a significant proportion of copied samples).

#### 4.1.4 Interaction effects

We now examine the relative effect of these three axes of evaluation on FLD. For fidelity, we use an increasing blur intensity. For diversity, we take a subset of the generated samples and replace the rest of the sample with copies of this subset. For novelty, we replace generated samples with copies of the training set. Then, in Fig. 8, starting from high quality generated samples, we vary each combination of the 2 axes and observe the effect on FLD through a heatmap.

In summary, Fig. 8 indicates that for FLD, fidelity matters more than diversity which matters more than novelty (with the notable exception of all samples being copied resulting in very high FLD). We argue this ordering is very much aligned with the potential usefulness of a generative model. If samples have poor fidelity, then regardless of their diversity and novelty, they will not be useful. With poor diversity but good fidelity and novelty, removing duplicates yields a useful generative model.

Figure 6: **Left:** FLD for various models as we increase the number of classes included in the generated samples. **Right:** FLD as the set of generated samples includes increasing amounts of copies of itself.

Figure 7: Comparison of various overfitting metrics when \(_{}\) consists of a combination of transformed generated samples and transformed copies of the train set.

### Comparison of Evaluation of State-of-the-Art Models

Using samples provided by (Stein et al., 2023), we perform a large-scale evaluation of various generative models in Tab. 1 using different metrics on CIFAR10, FFHQ and ImageNet. We find that FLD yields a similar model ranking as FID with some exceptions (for example MHGAN vs BigGAN-Deep on CIFAR10). In addition, we observe that modern generative models are overfitting in a benign way on CIFAR, i.e., there is a noticeable gap between train and test FLD even though the test FLD is reasonably low (indicating good generalization).

### Applications of FLD

**Identifying memorized samples**. As discussed in 3.2, we sort the samples generated by various models trained on CIFAR10 according to their respective \(_{j}\) and plot samples for different percentiles. As demonstrated in Fig. 9, all models produce a non-negligible amount of very near copies (especially a specific car of which there are multiple copies in the train set). For PFGM++ and StyleGAN-XL near copies exist even at the 1st percentile of \(_{j}\) (roughly in line with the findings of (Carlini et al., 2023; Stein et al., 2023)).

 Dataset & Model & FLD & FID & Gen Gap FLD & \(C_{T}\) & AuthPct & Precision & Recall \\   &  CGAN-Mod \\ LOGAN \\ BigGAN-Deep \\ MHGANGAN \\  } & 24.22 & 1143.07 & 0.10 & 26.11 & 72.09 & 0.76 & 0.00 \\  & & 18.94 & 753.34 & 0.18 & 55.66 & 84.10 & 0.63 & 0.13 \\  & & BigGAN-Deep & 9.28 & 203.90 & -0.05 & 55.70 & 88.10 & 0.50 & 0.26 \\  & & MHGAN & 8.84 & 231.38 & -0.01 & 47.87 & 86.69 & 0.55 & 0.26 \\  & & StyleGAN2-ada & 6.86 & 178.64 & -0.09 & 45.31 & 86.40 & 0.54 & 0.33 \\  & & iDDPM-DDIM & 5.63 & 128.57 & -0.33 & 39.65 & 84.60 & 0.57 & 0.55 \\  & & StyleGAN-XL & 5.58 & 109.42 & -0.23 & 36.79 & 85.29 & 0.57 & 0.13 \\  & & PFGMPP & 4.58 & 80.47 & -0.35 & 32.79 & 83.54 & 0.60 & 0.62 \\   &  Efficient-VDVAE \\ Projected-GAN \\ StyleGAN-ADA \\  } & 9.40 & 465.34 & -0.10 & 32.22 & 86.48 & 0.66 & 0.07 \\  & & Projected-GAN & 8.61 & 339.72 & 0.05 & 36.77 & 93.46 & 0.40 & 0.11 \\  & & StyleGAN2-ADA & 7.24 & 296.93 & -0.08 & 27.47 & 90.95 & 0.49 & 0.06 \\  & & Unleashing & 7.23 & 287.38 & -0.06 & 38.98 & 90.01 & 0.57 & 0.19 \\  & & InSGen & 6.20 & 249.91 & -0.08 & 26.06 & 89.74 & 0.52 & 0.12 \\  & & StyleGAN-XL & 5.94 & 155.88 & -0.04 & 39.41 & 88.17 & 0.56 & 0.30 \\  & & StyleSwin & 5.77 & 200.80 & -0.29 & 32.19 & 87.68 & 0.60 & 0.20 \\  & & StyleNAT & 4.69 & 156.38 & -0.12 & 29.38 & 86.35 & 0.62 & 0.30 \\  & & LDM & 4.63 & 162.45 & -0.23 & 28.28 & 84.84 & 0.66 & 0.34 \\   &  RQ-Transformer \\ StyleGAN-XL \\ GigagGAN \\ Mask-GIT \\ LDM \\ DiT-XL-2 \\  } & 11.55 & 212.99 & -0.53 & 125.48 & 86.10 & 0.39 & 0.55 \\  & & StyleGAN-XL & 8.46 & 150.27 & -0.40 & 98.69 & 84.10 & 0.43 & 0.26 \\  & & GigaGAN & 8.34 & 156.40 & -0.42 & 98.78 & 82.48 & 0.47 & 0.32 \\  & & Mask-GIT & 6.74 & 144.23 & -0.63 & 78.97 & 80.02 & 0.48 & 0.44 \\  & & LDM & 3.41 & 82.42 & -0.74 & 33.63 & 69.23 & 0.63 & 0.45 \\  & & DiT-XL-2 & 1.98 & 62.42 & -0.99 & 22.57 & 65.79 & 0.69 & 0.55 \\  

Table 1: Summary of performance metrics for various generative models.

Figure 8: Heatmap of FLD values for all pairs of the three axes of generalization.

**Evaluating individual sample fidelity**. We repeat the process looking instead at the \(_{j}\) of the models. From Fig. 10, the progression in fidelity of generative models is quite striking with older models such as LOGAN and ACGAN-Mod producing implausible images for all but their best samples. For recent, SOTA generative models, the top half of samples in terms of \(_{j}\) are generally of high quality. However, the bottom half has many examples of poor quality samples that would easily be identified by most humans as being fake. Consequently, even if the ranking is not perfect, filtering generated samples using \(_{j}\) could potentially be beneficial for downstream applications.

## 5 Conclusion

We introduce FLD, a new holistic evaluation metric for deep generative models. FLD is easy to compute, broadly applicable to all generative models, and evaluates generation quality, diversity, and generalization. Moreover, we show that, unlike previous approaches, FLD provides more explainable insights into the overfitting and memorization behavior of trained generative models. We empirically demonstrate both on synthetic and real-world datasets that FLD can diagnose important failure modes such as memorization/overfitting, informing practitioners on the potential limitations of generative models that generate photo-realistic images. While we focused on the domain of natural images, a fertile direction for future work is to extend FLD to other data modalities such as text, audio, or time series and also evaluate conditional generative models.

Figure 10: Ranked generated samples for different models from CIFAR10 according to \(_{j}\).

Figure 9: Ranked generated samples for different models from CIFAR10 according to \(_{j}\). The left sample is generated, the right one is the nearest sample in the train set (using distances in DINOV2 feature space).