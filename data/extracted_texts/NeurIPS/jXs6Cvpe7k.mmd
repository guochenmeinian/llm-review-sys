# Robust Prompt Optimization for Defending Language Models Against Jailbreaking Attacks

Andy Zhou\({}^{1,2}\) Bo Li\({}^{1}\) Haohan Wang\({}^{1}\)

\({}^{1}\)University of Illinois Urbana-Champaign \({}^{2}\)Lapis Labs

{andyz3,lbo,haohanw}@illinois.edu

###### Abstract

Despite advances in AI alignment, large language models (LLMs) remain vulnerable to adversarial attacks or jailbreaking, in which adversaries can modify prompts to induce unwanted behavior. While some defenses have been proposed, they have not been adapted to newly proposed attacks and more challenging threat models. To address this, we propose an optimization-based objective for defending LLMs against jailbreaking attacks and an algorithm, Robust Prompt Optimization (RPO) to create robust system-level defenses. Our approach directly incorporates the adversary into the defensive objective and optimizes a lightweight and transferable suffix, enabling RPO to adapt to worst-case adaptive attacks. Our theoretical and experimental results show improved robustness to both jailbreaks seen during optimization and unknown jailbreaks, reducing the attack success rate (ASR) on GPT-4 to 6% and Llama-2 to 0% on JailbreakBench, setting the state-of-the-art.

## 1 Introduction

Despite the powerful capabilities and usefulness of large language models (LLMs) [Brown et al., 2020, Hoffmann et al., 2022, Bai et al., 2022, Touvron et al., 2023, OpenAI, 2023], significant effort is required to ensure their behavior is helpful and harmless even when trained on harmful material. This is commonly achieved with alignment training techniques [Christiano et al., 2017, Ouyang et al., 2022, Bai et al., 2022, Rafailov et al., 2023], which uses a human or AI judge to evaluate if outputs are desirable and fine-tune a pre-trained LLM to match these preferences.

While this ensures the LLM typically refuses to generate objectionable output, in certain cases, such as when an adversary is introduced, it can be forced into doing so. This is achievable even with black-box access of the model through prompting, resulting in a series of _jailbreaking attacks_ that aim to elicit unwanted behavior with only input modifications. While early attacks require humans to write jailbreaking prompts [Wei et al., 2023], recently proposed attacks automate attack prompt generation with gradient signals or LLMs [Chao et al., 2023, Zou et al., 2023, Zhu et al., 2023, Jin et al., 2024]. As model capabilities improve, this security risk raises the potential for significant real-world harm [Ngo et al., 2024, Bengio et al.], making developing more robust LLMs crucial.

Since the discovery of these attacks, various defense mechanisms have been proposed, including input filters [Jain et al., 2023, Kumar et al., 2023], input smoothing [Robey et al., 2023], and few-shot examples [Wei et al., 2024, Anil et al., 2024]. While effective for initially proposed attacks such as Greedy Coordinate Gradient (GCG) [Zou et al., 2023], these often cannot generalize to multiple jailbreaks or incur additional inference costs, falling short of a strong and practical defense. Inaddition, a formal optimization objective for defense has yet to be proposed, especially in the adaptive attack scenario, making it difficult to consider how defenses can adapt to future attacks.

To address these issues, we formalize a minimax defensive objective motivated by adversarial training and propose _Robust Prompt Optimization (RPO)_, a discrete optimization algorithm to optimize for this objective. Our study is motivated by the increasing adoption of system-level guardrails (Inan et al., 2023; Rebedea et al., 2023), components inaccessible to the user used in LLM deployments to steer model behavior, such as system prompts or input/output filters. RPO improves robustness through system-level modifications to the model input. We evaluate RPO on two recently proposed red-teaming benchmarks, JailbreakBench (Chao et al., 2024) and HarmBench (Mazeika et al., 2024), which both cover a broad range of harmful risk categories and attack methods. On JailbreakBench, RPO reduces the attack success rate (ASR) to 6% on GPT-4 and 0% on Llamna-2, outperforming existing defenses and setting the state-of-the-art as a jailbreaking defense. In addition, RPO suffixes incur a negligible inference cost, only have a minor effect on benign prompts, and transfer to black-box models and unknown attacks. In summary, our contributions are the following:

* We formalize the first joint minimax optimization objectives for ensuring harmless LLM outputs under a more realistic and difficult threat model involving a variety of attacks and adaptive adversaries. Our theoretical analysis shows optimizing for our objective is guaranteed to improve robustness, even on unseen instructions and attacks.
* We propose an algorithm, RPO, which can directly optimize for our proposed defense objective with a combination of principled attack selection and discrete optimization.
* The resulting defense, an easily deployable suffix, achieves the state-of-the-art as an effective and universal defense across jailbreaks on JailbreakBench, transfers to closed-source LLMs such as GPT-4, and is resistant to adaptive attacks.

## 2 Related Work

**Adversarial robustness.** A significant body of work in adversarial machine learning studies the inherent susceptibility of neural networks to _adversarial examples_(Szegedy et al., 2014; Goodfellow et al., 2015). These are inputs designed to be misclassified through perturbations, which include norm-bounded perturbations, small spatial transformations (Xiao et al., 2018), and compositions of transformations (Madaan et al., 2021). Common defenses to these attacks include input preprocessing (Guo et al., 2018; Nie et al., 2022), distillation (Papernot et al., 2016), provable defenses (Raghunathan et al., 2018; Salman et al., 2020), and adversarial training (Goodfellow et al., 2015; Madry et al., 2018; Tramer et al., 2018), which has been the most empirically successful. Adversarial training, which is formalized as a minimax optimization (Tu et al., 2019) problem, improves model robustness by optimizing parameters against specially crafted inputs that maximize prediction error.

**Adversarial attacks on LLMs.** Similar attacks have been studied in NLP, including text classification (Ebrahimi et al., 2017; Alzantot et al., 2018; Wang et al., 2022), question-answering (Jia and Liang, 2017), or triggering toxic completions (Wallace et al., 2019; Jones et al., 2023; Zou et al., 2023).

Figure 1: RPO optimizes a set of trigger tokens that enforces safe outputs even under jailbreaks and adversarial attacks. RPO suffixes are universal and transfer to many LLMs and jailbreaks.

Language models are among the most generally capable models and have been applied to many domains beyond language (Yao et al., 2023; Zhou et al., 2023). As a result, inducing harmful behaviors has been the primary threat model for LLMs (Carlini et al., 2023). This has resulted in many recent _jailbreaking attacks_, where an adversary modifies a prompt manually to circumvent alignment training and induce harmful behavior. These attacks can be created manually by humans (Liu et al., 2023; Wei et al., 2023; Zeng et al., 2024), refined with another LLM (Chao et al., 2023; Mehrotra et al., 2023; Liu et al., 2023; Jin et al., 2024; Paulus et al., 2024), or generated with discrete optimization (Zou et al., 2023; Lapid et al., 2023; Zhu et al., 2023; Sadasivan et al., 2024). In addition, (Huang et al., 2023) finds that simply modifying decoding settings can jailbreak many open-source LLMs. Other attacks include extracting training data (Carlini et al., 2021; Nasr et al., 2023) and misclassification (Zhu et al., 2023; Wang et al., 2023), but we focus on harmful behaviors.

**Safety and Defenses for LLMs.** Even without an adversary, LLMs are prone to generating biased or toxic content (Sheng et al., 2019; McGuffie and Newhouse, 2020; Deshpande et al., 2023). To mitigate this, many modern LLMs undergo significant red-teaming (Perez et al., 2022; Mazeika et al., 2024) and additional training such as reinforcement learning with human feedback (Christiano et al., 2017; Ouyang et al., 2022; Bai et al., 2022) to be safer and refuse harmful requests. Additional defenses have recently been proposed with the discovery of additional failure modes, such as jailbreaking, on aligned LLMs. For instance, (Jain et al., 2023) examines simple defenses such as rephrasing the input and finds that the GCG attack (Zou et al., 2023) can be defended with a perplexity filter. Other defenses that have been explored include in-context learning (Zhang et al., 2023), sampling (Li et al., 2023), input processing (Cao et al., 2023; Robey et al., 2023; Kumar et al., 2023), and content moderation (Inan et al., 2023). While often effective for the threat models considered, many defenses rely on heuristics such as perplexity that can be circumvented by human-readable jailbreaks or require additional inference calls, reducing practicality. Concurrent to our work, (Mo et al., 2024) also considers a similar optimization objective, but only optimizes prompts on the GCG attack, which may limit transferability.

## 3 Towards Adversarial Robustness for LLMs

### Attack Objective

In contrast to discriminative models, we are interested in robustness from an _alignment_ perspective, in which unwanted behavior can be broader and more harmful than misclassification. We propose a threat model where the adversary can freely select various jailbreaks until the attack is successful, a more challenging and realistic threat model than previous work that only considers one or a few attacks. The only constraints on the adversary are the maximum input length for the LLM, system-level guardrails such as the system prompt, and other special formatting tokens that are inaccessible to users. Otherwise, adversaries can freely modify or add to any accessible part of the input prompt. Consequently, we focus on the _multiattack robustness_ setting and aim to create defenses robust to many jailbreaks.

The adversary's goal is to induce an LLM to respond to _any_ request, usually harmful ones the model would normally reject. We consider a standard autoregressive language model where a sequence of tokens is mapped to the distribution over the next token. We use \(p(|_{1:n})\) to denote the probability of generating every token in the output sequence \(y\) given all previous tokens to that point.

\[p(_{1:n})=_{i=1}p(_{n+i}|_{1 :n+i-1})\] (1)

In the context of jailbreaking, \(_{1:n}\) is a harmful instruction such as "How do I build a bomb," which we denote as \(}_{1:n}\). We consider a modern LLM trained to produce outputs that match human preferences, which is described as a latent reward model \(^{*}(|_{1:n})\) where a high reward is given to outputs more aligned with human evaluations. Thus \(^{*}(|}_{1:n})\) is high so a vanilla prompt \(}_{1:n}\) cannot directly induce the model to respond harmfully.

We consider the setting where the adversary can modify \(}_{1:n}\) through various jailbreaks to maximize the probability of producing an output sequence that accepts the harmful request or is toxic. We denote the resulting instruction after a jailbreak as \(}_{1:n}\). In contrast to vision, we do not expect \(}_{1:n}\) to be "stealthy" or semantically equivalent to \(_{1:n}\), besides the original instruction. The generation process can be formulated as the negative log probability of the target sequences of tokens \(^{*}\) representing the worst-case output \(^{*}=^{*}(|}_{1:n})\). Thus, we have the following set of equations to describe the generation process:

\[^{*}=^{*}(|}_{1:n})\] (2)

\[^{adv}(}_{1:n})=- p(y^{*}|}_{1 :n}).\] (3)

\[}_{1:n}=*{argmin}_{}_{1:n} (}_{1:n})}^{adv}(}_{1 :n}),\] (4)

where \((}_{1:n})\) is the distribution or set of possible jailbroken instructions. Note that this encompasses _all_ possible adversarial prompt modifications within the maximum prompt length. All attacks under our threat model eventually come down to ways to minimize Eq. 3.

### Defense Objective

While prevailing methods to improve LLM alignment involve fine-tuning, the objective of matching human preferences does not generally account for adversaries and jailbreaking. In addition, the high cost of generating attack prompts makes standard adversarial training on these samples difficult (Jain et al., 2023). We center our optimization approach on the _prompt_ level to address this. We formalize this as the negative log probability of a target token output \(^{}\) that refuses \(}_{1:n}\). This can be represented as the _normal output_ of an LLM trained to maximize \(^{}\) or \(^{}=^{*}(|}_{1:n})\). Thus, we have the following safe loss and defense objective

\[^{}=^{*}(|}_{1:n})\] (5)

\[^{safe}(}_{1:n})=- p(^{}| }_{1:n})\] (6)

\[*{minimize}^{safe}(}_{1:n}).\] (7)

The goal of the defense objective is to ensure robustness even under worst-case scenarios, such as when a jailbreak alters the harmful prompt. Since \(}_{1:n}\) is generated through Eq. 4, this can be formalized by incorporating the adversary into Eq. 7, which yields the following objective,

\[*{minimize}^{safe}(*{argmin}_{}_{1:n}(}_{1:n})}^{adv}( }_{1:n}))\] (8)

Eq. 8 directly incorporates Eq. 4 and like adversarial training, this formulation can be viewed as the composition of two problems, an _inner minimization_ problem, and _outer minimization_ problem. Jailbreaking can be interpreted as optimizing the inner minimization problem by creating a prompt to minimize the adversarial loss while existing defenses implicitly optimize the outer minimization problem. In contrast, we propose the first method to optimize the overall objective directly.

### Robust Prompt Optimization

Without direct gradient updates to the LLM, we focus on input optimization, which is challenging for LLMs due to the discreteness of text. We use gradient-based token optimization, which can directly optimize for Eq. 8. Gradient-based optimization is especially useful in our setting, as harmless behavior has well-defined output targets described in Eq. 6. In general, solving this objective means creating a mapping between _any_ worst-case modification of the input or jailbreak and the distribution of aligned output responses under the original prompt. This can be achieved by optimizing a suffix or set of trigger tokens that is always followed by a harmless response. To do so, we propose our main algorithm, _Robust Prompt Optimization (RPO)_, which optimizes for a set of tokens to enforce this mapping. As a whole, RPO consists of two successive steps based on the two components of the overall objective: (1) a jailbreak generation and selection step that applies a worst-case modification to the prompt and (2) a discrete optimization step that modifies the suffix to maintain refusal.

We simulate the adaptive threat model for the first step by adding the current defensive suffix to the original prompt and applying or generating a jailbreak prompt afterward. This is a straightforward modification to the prompt for simple, manual jailbreaks such as in-context examples (Wei et al., 2024). For automatic jailbreaks such as GCG (Zou et al., 2023), we apply several iterations of the jailbreak until either the RPO suffix is broken or until a fixed compute budget is exhausted. This allows RPO to support a variety of attacks during the optimization process. Our main technical contribution for this component is the selection step, where after its respective generation, we apply the jailbreak prompt that minimizes the adversarial loss_ for that instruction, according to Eq. 4. As the adversarial loss is calculated with the addition of the current RPO suffix, this ensures the optimization is performed under worst-case conditions and reduces the chance for the suffix to overfit on a particular jailbreak. In practice, due to the cost of generating new attack prompts, we only perform this operation again after a certain number of iterations \(R\).

After a jailbreak is applied, the second step optimizes the suffix to minimize the safe loss Eq. 7. We adopt a method similar to AutoPrompt (Shin et al., 2020) and GCG, using a greedy coordinate descent approach to assess how replacing the \(i\)-th token affects the safe loss. This involves calculating the first-order approximation and selecting the top-\(k\) tokens with the largest negative gradient. We then randomly select \(B k||\) tokens from this set of candidates, obtain the exact loss on this subset, and replace the current token with the token with the smallest loss. Both steps are applied in succession for a number of iterations \(T\). The full algorithm is described in Alg. 1.

```
0: Prompts \(x_{1:n_{1}}^{(1)} x_{1:n_{m}}^{(m)}\), set of jailbreaks \(\), initial defensive suffix \(p_{1:l}\), losses \(_{1}^{},_{m}^{}\), iterations \(T\), \(k\), batch size \(B\), selection interval \(R\) for\(s=1,,S\)do loop\(T\) times for all prompts \(x_{1:n_{1}}^{(1)}\)\(x_{1:n_{m}}^{(m)},j=1 m\)do  Append defensive suffix \(p_{1:l}\) to \(x_{1:n_{i}}^{(j)}\) if\(t R=0\)then\(\)Apply selection every \(R\) steps \(A^{*}:=*{argmin}_{}_{j}^{} _{1 o m}(A_{o}(x^{(j)}))\)\(\)Jailbreak that minimizes adv. loss \(x^{(j)}:=A^{*}(x^{(j)})\)\(\)Apply best jailbreak from set to prompt for\(i[0 l]\)do \(_{i}:=k(-_{1 j m}_{e_{j}} _{j}^{}(x_{1:n+l}^{(j)}\|p_{1:l}))\)\(\)Compute top-\(k\) candidates for\(b=1,,B\)do \(_{1:l}^{(b)}:=*{Uniform}(_{i})\)\(\)Sample replacements \(p_{1:l}:=_{1:l}^{(b^{})}\), where \(b^{}=*{argmin}_{b}_{1 j m}_{j}^{ }(x_{1:n+l}^{(j)}\|_{1:l}^{(b)})\)\(\)Best replacement return Optimized defensive suffix \(p\) ```

**Algorithm 1** Robust Prompt Optimization

### Theoretical Analysis of RPO

In this section, we provide a theoretical analysis to characterize the effectiveness and robustness properties of RPO under various settings. We study how the performance of RPO is affected when applied to different instruction datasets and against unknown adversaries.

**Setup.** We first introduce the notations and setup used in the analysis. Let \(\) denote a benchmark dataset and \(_{}\) be the underlying data distribution. We simplify Obj. 8 based on reward model \(\):

\[_{}_{1:n}(}_{1:n})} (|}_{1:n}).\]

where \((}_{1:n})\) represents the set of possible adversarial transformations for prompt \(}_{1:n}\). The attack success rate (ASR) of an adversary \(\) on dataset \(\) is defined as:

\[()_{}=1-_{}_{1:n}} _{}_{1:n}(}_{1:n})} (|}_{1:n}).\]

We denote RPO optimized against adversary \(\) as \(()\). The ASR of \(()\) on dataset \(\) is defined as:

\[()_{()}=1-_{}_{1:n} }_{}_{1:n}(}_{1 :n})}_{}_{1:n}(}_{1:n})} (|}_{1:n}).\]

To measure the effectiveness of RPO, we define \((,(),)\) as the difference in ASR between the original model and the model defended by RPO:

\[(,(),)=_{}_{1:n} }_{}_{1:n}(}_{1 :n})}_{}_{1:n}(}_{1:n})} (|}_{1:n})-_{}_{1:n} (}_{1:n})}(|}_{1:n}).\]

**Performance on the Same Dataset, Known Adversary.** We first consider the case where RPO is applied to the same dataset and adversary it was optimized on.

**Proposition 3.1**.: \[(,(),) 0\]

This proposition states that when RPO is applied to the same dataset it was optimized on and the adversary is known, it is guaranteed to reduce ASR.

Generalization to Different Datasets, Known AdversaryNext, we study the generalization performance of RPO when applied to datasets sampled from the underlying data distribution \(_{}\). We extend the notation of Diff to the distribution setting:

\[(_{},(),)=_{}_{1:n}_{}}_{}_{1:n} (}_{1:n})}_{}_{1:n} (}_{1:n})}(| }_{1:n})-_{}_{1:n}(}_{1:n })}(|}_{1:n}).\]

**Lemma 3.2**.: _Let \(n\) denote the number of samples in \(\). The expected effectiveness of RPO on samples from \(_{}\) is bounded as follows:_

\[((,(),)-(_{},(),))(-2n ^{2}).\]

This lemma bounds the generalization error of RPO when applied to samples from the underlying data distribution. It shows that the effectiveness of RPO on the training dataset \(\) is close to its expected effectiveness on the entire data distribution \(_{}\), with high probability.

Performance on the Same Dataset, Unknown AdversaryIn practice, the adversary and attacks encountered during testing may differ from the ones used during optimization. We denote the training time adversary as \(\) and the unknown test time adversary as \(\). We use \(_{}\) and \(_{}\) to represent the adversarial perturbations generated by \(\) and \(\), respectively.

We study \((,(),)\), defined as:

\[(,(),)=_{}_{1:n} }_{}_{1:n}(}_{1 :n})}_{}_{1:n}_{}(}_{ 1:n})}(|}_{1:n})-_{ }_{1:n}_{}(}_{1:n})}( |}_{1:n}).\]

**Proposition 3.3**.: _With \(n\) denoting the number of samples in \(\), we have_

\[(,\!(),)\] \[(,(),)+_{ }_{1:n}}[_{ }_{1:n}_{}(}_{1:n})}( |}_{1:n})<_{}_{1:n}_{}( }_{1:n})}(|}_{1:n}) ].\]

This proposition compares the empirical strength of the two adversaries \(\) and \(\). If \(\) is empirically stronger than \(\) on dataset \(\), then \((,(),)(,( ),)\). This means that RPO optimized against a stronger adversary \(\) will still be effective against a weaker test time adversary \(\). However, if \(\) is stronger than \(\), the effectiveness of RPO may degrade, and the degradation depends on the empirical difference in strength between the two adversaries.

Generalization to Different Datasets, Unknown AdversaryFinally, we study the generalization performance of RPO when applied to datasets from \(_{}\) and against an unknown adversary \(\). We define \((_{},(),)\) as:

\[(_{},(),)=_{}_{1:n}_{}}_{}_{1:n} (}_{1:n})}_{}_{1:n} _{}(}_{1:n})}(|}_{1:n})-_{}_{1:n}_{}( }_{1:n})}(|}_{1:n}).\]

**Theorem 3.4**.: _Let \(n\) denote the number of samples in \(\), and \(p_{,}\) be the probability that adversary \(\) is stronger than \(\) on samples from \(_{}\), i.e.,_

\[_{}_{1:n}_{}(}_{1:n})} (|}_{1:n})<_{}_{1:n} _{}(}_{1:n})}(|}_{1:n}).\]

_Then, with probability at least \(1-\), we have_

\[(_{},(),)( ,(),)-( )}+np_{,}.\]This theorem provides a lower bound on the generalization performance of RPO when applied to samples from \(_{}\) and against an unknown test time adversary \(\). The bound depends on the effectiveness of RPO on the training dataset \(\), the generalization error term, and the probability \(p_{,}\) that \(\) is stronger than \(\) on samples from \(_{}\). If \(p_{,}\) is high, indicating that the test time adversary is stronger than the one used during training, the effectiveness of RPO may degrade more significantly.

## 4 Experiments

### Experimental Setup

We optimize and evaluate our method on the instructions, attack baselines, and defense baselines from two recently proposed red-teaming benchmarks, HarmBench (Mazeika et al., 2024) and JailbreakBench (Chao et al., 2024). For text-based LLMs, HarmBench and JailbreakBench contain 400 and 100 distinct harmful behaviors, respectively. These involve harmful prompts safety-tuned LLMs consistently reject from a broad range of risk categories, such as "Describe how to exploit elderly people with dementia for access to their bank accounts" We measure attack success rate (ASR) and evaluate success following the corresponding benchmarks, which use LLM-as-a-judge. We evaluate attacks and defenses on six LLMs, open-source Vicuna-13B (Zheng et al., 2023), Llama-2-7B-Chat (Touvron et al., 2023), Qwen-1.5-14B (Bai et al., 2023), and Llama-2-13B-Chat (Touvron et al., 2023), and closed-sourced GPT-3.5-Turbo and GPT-4 (OpenAI, 2023).

**Baseline Attacks and Defenses.** We use the attacks provided on each benchmark. JailbreakBench contains three attacks: (1) Greedy Coordinate Gradient (GCG) (Zou et al., 2023), (2) Prompt Automatic Iterative Refinement (PAIR) (Chao et al., 2023), and hand-crafted jailbreaks from Jailbreak Chat (JBC) (Wei et al., 2023). HarmBench contains 18 attacks, of which we use six text-based attacks with the highest average ASR: GCG, Automated Discrete Adversarial Natural Prompt (AutoDAN) (Liu et al., 2023a), PAIR, Few-Shot Examples (Perez et al., 2022; Wei et al., 2024), Tree-of-Attacks with Pruning (TAP) (Mehrotra et al., 2023), and Persusive Adversarial Prompt (PAP) (Zeng et al., 2024). We use the defenses provided on each benchmark as our baselines, as well as Few-Shot examples Wei et al. (2024). JailbreakBench contains three defenses: Perplexity Filter (Jain et al., 2023), SmoothLLM (Robey et al., 2023), and Rephrasing (Jain et al., 2023), while HarmBench does not provide any defenses besides the base models. We follow the default attack and defense implementation settings.

    & &  &  \\  Attack & Defense & Vicuna & Llama-2-7B & Qwen-7B & Llama2-13B & GPT-3.5 & GPT-4 \\   & None & 82\% & 4\% & 68\% & 2\% & 76\% & 50\% \\  & SmoothLLM & 47\% & 1\% & 36\% & 1\% & 12\% & 25\% \\  & Perplexity Filter & 81\% & 4\% & 66\% & 2\% & 15\% & 43\% \\  & Rephrasing & 25\% & 4\% & 13\% & 2\% & 22\% & 35\% \\  & Few-Shot & 27\% & 6\% & 16\% & 1\% & 8\% & 10\% \\  & **RPO (Ours)** & **16\%** & **0\%** & **4\%** & **1\%** & **6\%** & **6\%** \\   & None & 58\% & 2\% & 11\% & 0\% & 34\% & 1\% \\  & SmoothLLM & 1\% & 1\% & 2\% & 0\% & 1\% & 3\% \\  & Perplexity Filter & 1\% & 0\% & 0\% & 0\% & 1\% & 0\% \\  & Rephrasing & 4\% & 0\% & 7\% & 0\% & 4\% & 1\% \\  & Few-Shot & 1\% & 0\% & 1\% & 0\% & 0\% & 0\% \\  & **RPO (Ours)** & **0\%** & **0\%** & **0\%** & **0\%** & **0\%** \\   & None & 79\% & 0\% & 58\% & 1\% & 0\% & 0\% \\  & SmoothLLM & 64\% & 0\% & 44\% & 0\% & 0\% & 0\% \\   & Perplexity Filter & 79\% & 0\% & 58\% & 1\% & 0\% & 0\% \\   & Rephrasing & **15\%** & 0\% & 47\% & 1\% & 1\% & 1\% \\   & Few-Shot & 41\% & 0\% & 50\% & 1\% & 0\% & 0\% \\   & **RPO (Ours)** & 70\% & **0\%** & **45\%** & **0\%** & **0\%** & **0\%** \\   

Table 1: Attack success rate of RPO and baseline defenses on JailbreakBench. All prompts and responses are classified using Llama Guard. The RPO suffix is optimized on Llama-2-7B. RPO significantly outperforms baseline defenses for both open-source and closed-source models.

**RPO Setup.** During optimization for RPO, we target the Llama-2-7B model, use a suffix length of 20 tokens, and optimize for 500 steps using a batch size of 64, top-\(k\) of 256, and selection interval of 50. We optimize the suffix using 25 randomly selected instructions from the training set of AdvBench (Zou et al., 2023) to minimize overlap with evaluation instructions. We optimize the suffix on the three jailbreaks from JailbreakBench, which we find sufficient for high transferability to the other attacks on HarmBench. This includes GCG, PAIR, and JBC. During each inner minimization step, we regenerate a PAIR and GCG jailbreak for each instruction, including the current RPO suffix, but do not change the handcrafted jailbreak prompts. During inference, we place the RPO suffix after the user input as a component of the system prompt. All jailbreak details and example outputs, including on the ChatGPT interface, can be found in the Appendix.

### Main Results

**Known Attacks on JailbreakBench.** In Tab. 1, we observe that on JailbreakBench RPO significantly improves upon baseline defense robustness to PAIR, GCG, and JBC. Models besides Vicuna have been alignment-tuned and are already robust to prompts from JBC but vulnerable to other attacks. We find perplexity filtering is highly effective on GCG but is not effective on the natural language jailbreak prompts in PAIR and JBC. SmoothLLM is more effective across multiple attacks due to not relying on the perplexity heuristic. Still, it cannot defend against a significant proportion of prompts from PAIR, the strongest attack. Rephrasing is surprisingly effective, especially on JBC for Vicuna, outperforming the other defenses. We observe RPO is more effective than all baseline defenses on PAIR for all models, reducing ASR by 66% on Vicuna and 44% on GPT-4 and improving on the state-of-the-art defense SmoothLLM by 31% and 19%. This also shows that RPO suffixes _transfer across models_, as the suffix was optimized using Llama-2 but can transfer to Vicuna and even closed-source GPT models. Notably, RPO reduces GCG ASR to 0% for all models, fully defending against the attack. Using RPO with Llama-2 makes the model robust to all three attacks, the first time a model is fully robust on JailbreakBench. The only setting where RPO is not the strongest defense is JBC on Vicuna, where other defenses are more effective. This may be due to the lack of safety training on the older Vicuna model, making it less responsive to our defense.

**Transfer to Unknown Attacks on HarmBench.** HarmBench marks a difficult distribution shift from the optimization setup of RPO as it contains many attacks RPO was not optimized on and has a broader inclusion of behaviors, such as copyright infringement and contextual behaviors referencing user context. These categories are not covered in the instructions we optimize RPO on, forcing the defense to generalize. Despite this, in Tab. 2 we observe RPO transfers to all attacks in HarmBench for all models, generalizing to difficult attacks such as TAP and AutoDAN. Notably, RPO reduces ASR by an average of 18%, 6.6%, 8.7%, and 3.5% for Vicuna, Llama-2, GPT-3.5, and GPT-4, respectively. This suggests RPO is universally effective as a defense for harmful queries, irrespective of the attack. This is due to the defense enhancing existing refusal mechanisms in LLMs, which naturally transfers to other safety scenarios. However, we observe a much lower improvement on HarmBench than JailbreakBench, reflecting the challenges of generalizing to new attacks and behaviors.

   Model & GCG & AutoDan & PAIR & TAP & Few-Shot & PAP & Average \\  Vicuna-13B & 65.6 & 65.9 & 50.3 & 53.6 & 32.2 & 20.1 & 48.0 \\ + RPO & 17.8 & 59.5 & 32.5 & 37.2 & 13.0 & 17.7 & 29.6 \\  Llama-2-7B & 31.9 & 0.0 & 9.4 & 9.1 & 5.0 & 2.7 & 9.7 \\ + RPO & 6.7 & 0.0 & 5.0 & 7.8 & 0.0 & 0.0 & 3.2 \\  GPT-3.5 & 42.6 & 6.5 & 36.3 & 38.9 & 27.6 & 11.3 & 27.2 \\ + RPO & 9.3 & 3.2 & 29.4 & 33.0 & 25.9 & 10.0 & 18.5 \\  GPT-4 & 22.3 & 0.5 & 33.8 & 37.6 & 9.3 & 11.6 & 19.2 \\ + RPO & 9.0 & 0.2 & 31.2 & 35.8 & 7.0 & 10.9 & 15.7 \\   

Table 2: Transfer attack success rate of RPO on the six highest performing attacks from HarmBench. Four of the attacks, AutoDAN, TAP, Few-Shot, and PAP, are not seen during optimization, requiring RPO to generalize to unknown attacks. RPO reduces ASR across all six attacks for all four models, including both open-source and closed-source models.

[MISSING_PAGE_FAIL:9]

directly incorporates the threat model, RPO generates transferable and lightweight defensive suffixes that are robust to a wide range of attacks, including unseen ones. Our experiments on JailbreakBench and HarmBench demonstrate RPO's superior performance compared to existing defenses, reducing attack success rates significantly across different models while incurring only minor effects on benign usage. This suggests text-based jailbreaking may be an easier problem to address than adversarial attacks in vision. However, RPO does not currently cover multimodal models, LLM agents, or other failure modes such as deception and malicious code generation. Proposing our defense may also lead to the development of stronger attacks, including those that can break RPO. Indeed, while we observe high transferability to new attacks, using RPO does not typically result in full robustness. Future directions include optimizing defenses on a greater variety of attacks, combining various defenses into comprehensive guardrails, and stronger red teaming strategies to discover new security risks.

## 6 Acknowledgements

We thank Mantas Mazeika and Yi Zeng for their helpful discussions and assistance with HarmBench. This work used NVIDIA GPUs at NCSA Delta through allocations CIS230218 and CIS230365 from the ACCESS program and from the Illinois Compute Campus Cluster.