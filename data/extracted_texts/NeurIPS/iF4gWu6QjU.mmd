# Efficient Bayesian Computational Imaging with a Surrogate Score-Based Prior

Anonymous Author(s)

Affiliation

Address

email

###### Abstract

We propose a surrogate function for efficient use of score-based priors for Bayesian inverse imaging. Recent work turned score-based diffusion models into probabilistic priors for solving ill-posed imaging problems by appealing to an ODE-based log-probability function. However, evaluating this function is computationally inefficient and inhibits posterior estimation of high-dimensional images. Our proposed surrogate prior is based on the evidence lower-bound of a score-based diffusion model. We demonstrate the surrogate prior on variational inference for efficient posterior sampling of large images. Compared to the exact prior used in previous work, our surrogate prior accelerates optimization of the variational distribution by at least two orders of magnitude. We also find that our principled approach achieves higher-fidelity image-reconstruction than non-Bayesian baselines that involve hyperparameter-tuning at inference. Our work establishes a practical path forward for using score-based diffusion models as general-purpose priors for computational imaging.

## 1 Introduction

Ill-posed image reconstruction requires a prior to constrain the reconstruction according to desired image statistics. From a Bayesian perspective, the prior influences both the uncertainty and the richness of the estimated image. Although diffusion-based generative models represent rich image priors, leveraging these priors for Bayesian image-reconstruction remains a challenge. True posterior sampling with an unconditional diffusion model is intractable, so most previous methods heavily approximate the posterior [9; 13; 14; 19] or disregard measurement noise [5; 7; 8; 6; 11; 24; 1]. Recent work demonstrated how to turn score-based diffusion models into probabilistic priors (_score-based priors_) for Bayesian imaging . However, this method requires the exact probability of a proposed image to be evaluated with a computationally-expensive ordinary differential equation (ODE), requiring days to a week to reconstruct even a \(32 32\) image . We present a method for Bayesian inference with a score-based prior that is both principled and computationally efficient.

Although computing exact probabilities under a diffusion model is inefficient or even intractable, computing the evidence lower-bound [22; 12] is computationally efficient and feasible for high-dimensional images. Thus we propose to use this evidence lower-bound as a surrogate for the exact score-based prior. In particular, we use the evidence lower-bound of a score-based diffusion model  as a substitute for the exact log-probability function. This function can be plugged into any inference algorithm that requires the value or gradient of the posterior log-density. When it is used in variational inference, we find at least two orders of magnitude in speedup of optimizing the variational distribution. Furthermore, our approach reduces GPU memory requirements, as there is no need to evaluate and backpropagate through an ODE. These efficiency improvements make it practical to perform inference with score-based priors.

In this paper, we describe our variational-inference approach to efficiently estimate a posterior with a surrogate score-based prior. We provide experimental results to validate the proposed surrogate prior, including high-dimensional posterior samples of sizes up to \(256 256\), a resolution infeasible with the exact prior. In the setting of accelerated MRI, we quantify time- and memory-efficiency improvements of the surrogate over the exact prior. We also demonstrate how our proposed approach achieves higher-quality image reconstructions than methods that deviate from true Bayesian inference.

## 2 Related work

### Bayesian inverse imaging

Image reconstruction can be framed as an inverse problem: a hidden image \(^{*}^{D}\) must be recovered from measurements \(^{M}\), where

\[=f(^{*})+.\] (1)

It is usually assumed that the forward model \(f:^{D}^{M}\) is known and that the measurement noise \(^{M}\) is a random variable with a known distribution. With an ill-posed inverse problem, there is inherent uncertainty in image reconstruction.

Bayesian imaging accounts for the uncertainty by formulating a posterior distribution \(p()\). The posterior can be decomposed into a likelihood term and a prior term:

\[ p()= p()+ p( )+\] (2)

Given a log-likelihood function \( p()\) and a prior log-probability function \( p()\), we can use established techniques for sampling from the posterior, such as Markov chain Monte Carlo (MCMC)  or variational inference . MCMC algorithms generate a Markov chain whose stationary distribution is the posterior, but they are generally slow to converge for high-dimensional data like images. Variational inference instead approximates the posterior with a tractable distribution (e.g., Gaussian). The variational distribution is usually parameterized and thus can be efficiently optimized to represent high-dimensional data distributions. Deep Probabilistic Imaging (DPI) [25; 26] proposed an efficient variational-inference approach specifically for computational imaging with traditional regularizers; in DPI, the variational distribution is a discrete normalizing flow , which is an invertible generative model capable of representing complex distributions.

Figure 1: High-dimensional Bayesian inference with a surrogate score-based prior. We propose a surrogate prior for efficient use of score-based diffusion models as priors for Bayesian imaging. Here we show posterior samples (estimated with variational inference) for accelerated MRI of \(256 256\) knee images with a score-based diffusion-model prior. The first row shows reconstruction from \(16\)-reduced MRI measurements. The second row shows reconstruction given more \(\)-space measurements, i.e., \(4\)-reduced MRI. Bayesian imaging at this image resolution is computationally infeasible with the previous ODE-based approach. Our proposed surrogate prior enables efficient yet principled inference with diffusion-model priors, resulting in inferred posteriors where the true image is within three standard deviations of the posterior mean for 96% and 99% of the pixels for \(16\)- and \(4\)-acceleration, respectively.

### Diffusion models for inverse problems

Primarily developed for image generation, diffusion models [18; 12; 20; 21; 23] learn to model a rich image distribution that could be useful as a prior for image reconstruction. A diffusion model generates an image by starting from an image of noise and gradually denoising it until it becomes a clean image. We discuss this process, known as _reverse diffusion_, in more detail in Sec. 3.1.

Given an inverse problem, simply adapting a pretrained diffusion model to sample from the posterior instead of the learned prior is intractable . Therefore, most diffusion-based approaches do not infer a true Bayesian posterior. Some methods project images onto a measurement-consistent subspace [24; 8; 6; 5; 7], but the projection does not account for measurement noise and might pull images away from a true posterior. Other methods follow a gradient toward higher likelihood throughout reverse diffusion [9; 13; 11; 14; 1; 19; 17], but these methods heavily approximate the posterior. Overall, these diffusion-based methods require hyperparameter-tuning to balance the measurements and the prior. As soon as hyperparameters are introduced, there is no guarantee of sampling from a posterior that represents the true uncertainty.

**Score-based priors.** Alternatively, a score-based diffusion model can be turned into a standalone, probabilistic prior (_score-based prior_) that can be paired with any measurement-likelihood function and plugged into established Bayesian-inference approaches. Feng et al.  proposed to do this with a log-density function based on the ODE associated with reverse diffusion (see Sec. 3.2). This function provides the log-probability of any image under the diffusion model's generative distribution, but it is computationally expensive to evaluate. When used in iterative optimization algorithms, it incurs prohibitively high time and memory costs.

## 3 Background

In this section, we review background on score-based diffusion models with an emphasis on evaluating probabilities of images with a pretrained diffusion model. We then describe how a diffusion process gives rise to an efficient denoising-based lower-bound on these image probabilities.

### Score-based diffusion models

The core idea of a diffusion model is that it transforms a simple distribution \(\) to a complex image distribution through a gradual process. In this work, we follow the popular framework of denoising diffusion models, which transform noise samples from \(=(,)\) to clean samples from the data distribution \(p_{}\) through gradual denoising. With knowledge of the noise distribution and the denoising process, we can assess the probability of a novel image under this generative model.

The transformation from a simple distribution to a complex one occurs over many steps. To determine how the data distribution should look at each step of the denoising process, we turn to a stochastic differential equation (SDE) that describes a diffusion process from clean images to noise. The diffusion SDE is defined on the time interval \(t[0,T]\) and has the form

\[=(,t)+g(t),\] (3)

where \(^{D}\) denotes Brownian motion. \(g(t)\) is the diffusion coefficient, which controls the rate of noise increase. \((,t):^{D}^{D}\) is the drift coefficient, which controls the deterministic evolution of \((t)\). By defining a stochastic trajectory \(\{(t)\}_{t[0,T]}\), this SDE gives rise to a time-dependent probability distribution \(p_{t}\), which is the marginal distribution of \((t)\). We construct \((,t)\) and \(g(t)\) so that if \(p_{0}=p_{}\), then \(p_{T}\). Image generation amounts to reversing the diffusion, which requires the gradient of the data log-density (_score_) at every noise level in order to nudge images toward high probability under \(p_{}\). A convolutional neural network \(_{}\) known as a _score model_ is trained to approximate the true score: \(_{}(,t)_{} p_{t}()\).

### Image probabilities under a score-based diffusion model

Once trained, \(_{}(,t)\) is used in a reverse-diffusion process to generate clean images from noise. The generated image distribution theoretically assigns a probability density to every possible image. However, reverse diffusion does not lead to an image distribution with tractable probabilities. In this

[MISSING_PAGE_FAIL:4]

noise to \(\) without having to solve an initial-value problem as with the ODE. In fact, Eq. 7 is closely related to the denoising score-matching objective used to efficiently train diffusion models .

Intuitively, we can interpret Eq. 7 as associating an image's probability with how well the score model \(_{}\) could denoise that image if it underwent diffusion. This is represented by the first term in \(h(t)\) (Eq. 8). To assess the probability of an image \(\), we perturb it with Gaussian noise to get \(^{}\) and then ask the score model to estimate the noise that was added. If \(_{}(,t)\) accurately estimates the noise, then \(\|_{}(^{},t)-_{^{ }} p_{0t}(^{})\|_{2}^{2}\) is small, and the value of \(b_{}^{}()\) becomes larger.

The remaining terms in \(h(t)\) are normalizing factors independent of \(\). The term \(_{p_{0T}(^{})}[(^{})]\) accounts for the probabilities of the noise images \((T)\) that could result from \(\) being entirely diffused.

## 4 Method

Inspired by previous theoretical work , we propose \(b_{}^{}\) as an efficient surrogate prior for the exact score-based prior in Bayesian imaging. In this section, we describe our approach for efficient posterior inference with a score-based prior.

### Variational inference with a surrogate score-based prior

Given measurements \(^{M}\) (with a known log-likelihood function) and a score-based diffusion model (parameterized by \(\)) as the prior, our goal is to sample from the image posterior \(p_{}()\). We follow a variational-inference approach by optimizing the parameters of a variational distribution to closely approximate the target posterior.

Let \(q_{}\) denote the variational distribution with parameters \(\), and we assume \(q_{}\) to have tractable log-probabilities. We optimize \(\) to minimize the KL divergence from \(q_{}\) to the target posterior:

\[^{*}=_{}D_{}(q_{}\|p_{}())=_{}_{ q_{}}[- p()- p_{}^{}()+ q_{}( )].\] (9)

\(q_{}\) can be various types of distributions. For example, it could be a Gaussian distribution with a diagonal covariance matrix so that \(:=[^{},^{}]^{}\), where \(^{D}\) and \(^{D}\) (\(>\)) are the mean and pixel-wise standard deviation. As DPI showed , \(q_{}\) could also be a RealNVP normalizing flow with network parameters \(\).

To circumvent the computational challenges of evaluating the prior term \( p_{}^{}()\), we replace it with the surrogate \(b_{}^{}()\). This results in the following objective:

\[^{*}=_{}_{ q_{}}[- p( )-b_{}^{}()+ q_{}( )].\] (10)

We can also think of \(b_{}^{}\) as replacing the intractable \( p_{}^{}\) in Eq. 9. Since \(- p_{}^{}-b_{}^{}\), our surrogate objective minimizes the upper-bound of a valid KL divergence involving \(p_{}^{}\).

### Implementation details

**Evaluating \(b_{}^{}()\).** The formula for \(b_{}^{}()\) (Eq. 7) contains a time integral and expectation over \(p_{0t}(^{})\) that can be estimated with numerical methods. Following Song et al. , we use importance sampling with time samples \(t p(t)\) for the time integral and Monte-Carlo approximation with noisy images \(^{}((t),(t)^{2})\) for the expectation. The proposal distribution \(p(t):=}{(t)^{2}Z}\) was empirically verified to result in lower variance in the estimation of \(b_{}^{}()\). We provide the following formula used in our implementation, which estimates the time integral with importance sampling and the expectation with Monte-Carlo approximation, for reference:

\[b_{}^{}()}_{ j=1}^{N_{z}}(_{j}^{})\\ -N_{z}}_{i=1}^{N_{t}}Z(t)^{2}_{j=1}^{ N_{z}}[\|_{}(_{ij}^{},t_{i})+_{ ij}}{(t_{i})}\|_{2}^{2}-\|_{ij}}{(t_{i})} \|_{2}^{2}-)^{2}}_{_{ij}^{}} (_{ij}^{},t_{i})]\\  t_{i} p(t),\,_{ij}( ,),\,_{ij}^{}=(t_{i})+ (t_{i})_{ij},\,_{j}^{}(( T),(T)^{2})\\ \,\,i=1,,N_{t},j=1,,N_{z}.\] (11)\(N_{t}\) is the number of time samples used to approximate the time integral, and \(N_{z}\) is the number of noise samples taken to approximate the expectation over \(p_{0t}(})\). In our experiments, we set \(N_{t}=N_{z}=1\). Increasing the number of time and noise samples does not efficiently decrease variance in the estimated value of \(b_{}^{}()\). We use the Variance Preserving (VP) SDE.

**Optimization.** We use stochastic gradient descent to optimize \(\), Monte-Carlo approximating the expectation in Eq. 10 with a batch of \( q_{}\). We find that estimating \(b_{}^{}()\) has higher variance than estimating \( p_{}^{}()\). For example, in Fig. 4, \(b_{}^{}()\) with \(N_{t}=2048,N_{z}=1\) shows higher variance than \( p_{}^{}()\) with 16 trace estimators. When optimizing a complex distribution like RealNVP, a lower learning-rate helps mitigate training instabilities caused by variance. For example, in Fig. 2(b) the learning rate with the exact prior was \(0.0002\), while the learning rate with the surrogate prior was \(0.00001\). Please refer to the supplemental text for more optimization details.

## 5 Experiments

We validate our proposed approach on the tasks of accelerated MRI, image denoising, and reconstruction from low spatial frequencies. We highlight accelerated (or compressed sensing) MRI because in addition to being a real-world imaging problem that calls for accurate posterior estimation, it is the focus of much related work [24; 13]. In MRI, measurements in a spatial-frequency space (\(\)-space) are obtained to help reveal a hidden anatomical image. Accelerated MRI reduces the number of \(\)-space measurements, thus reducing the scan time but also making the image reconstruction ill-posed. The supplemental text provides details on how measurements were generated for all tasks.

### Efficiency improvements

In Tab. 1 and Fig. 2, we quantify the efficiency improvements of the surrogate prior for an accelerated MRI task at different image resolutions. We drew a test image from the fastMRI knee dataset  and resized it to \(16 16\), \(32 32\), \(64 64\), \(128 128\), and \(256 256\). For each image size, we trained a score model on training images of the corresponding size from the fastMRI dataset of single-coil knee scans. We then optimized a Gaussian distribution with diagonal covariance to approximate the posterior. The batch size was \(64\) for the surrogate and \(32\) for the exact prior (a smaller batch size was needed to fit \(64 64\) optimization into GPU memory). Convergence was defined by setting a minimum acceptable change in the mean of the estimated posterior between optimization steps.

We find at least two orders of magnitude in time improvement with the surrogate prior. Tab. 1 compares the iteration time between the two priors. Fig. 2 compares the total time it takes to optimize the variational distribution. The surrogate also significantly improves memory consumption, which in turn enables optimizing higher-dimensional posteriors. Following standard practice, we just-in-time (JIT) compile the optimization step to reduce time/step at the cost of GPU memory. Fig. 2 shows how the surrogate prior significantly reduces memory requirements and scales better with image size. The exact prior could only handle up to \(32 32\) before exceeding GPU memory (we tested on 4x 48GB GPUs). While memory could be reduced with a smaller batch size, this would make optimization more time-consuming. On the other hand, our surrogate prior supports much larger images, as we demonstrate in Fig. 1 for \(256 256\)1 MRI with a Gaussian-approximated posterior. This type of principled inference of high-dimensional image posteriors was not possible before with the exact score-based prior.

### Posterior estimation under the surrogate vs. exact prior

We cannot expect the surrogate prior \(b_{}^{}\) to be an identical substitute for the exact prior \( p_{}^{}\). Importantly, though, we verify in Fig. 2(a) that both the surrogate and the exact prior recover a ground-truth Gaussian posterior derived from a Gaussian likelihood and prior. The variational distribution

  Image size & Surrogate & Exact \\  \(16 16\) & \(0.029\) & \(19.5\) \\  \(32 32\) & \(0.038\) & \(41.9\) \\  \(64 64\) & \(0.090\) & \(123\) \\  \(128 128\) & \(0.294\) & N/A \\  \(256 256\) & \(1.115\) & N/A \\  

Table 1: Iteration time [sec/step]. Each iteration of gradient-based optimization of the variational distribution is 2 to 3 orders of magnitude faster with the surrogate prior.

used for inference is a RealNVP, and the score model (used by both the surrogate and exact prior) was trained on samples from the known Gaussian prior.

Nonetheless, the surrogate could result in a different locally-optimal variational posterior, particularly if the posterior is complex with various local minima in the variational objective. Fig. 2(b) compares posteriors (with unknown true distribution) approximated by a RealNVP under the surrogate versus exact prior. For each task (CelebA denoising and CIFAR-10 denoising), both prior functions used the same pretrained score model. We observe in these comparisons that most of the differences appear in the image background and that both priors result in a plausible mean reconstruction and uncertainty.

**Visualizing the bound bar throughout optimization** helps shed light on why the two priors converge to different solutions even if the underlying score model is the same. Fig. 4 shows probabilities of samples generated by \(q_{}\) (in this case, a RealNVP) as optimization progresses. At each checkpoint of \(q_{}\), we plot \( p^{}_{}()\) versus \(b^{}_{}()\) (approximated with \(N_{t}=2048\) for reduced variance) for samples \( q_{}\) coming from both the exact and surrogate optimization of \(q_{}\). Importantly, we find that the surrogate is a valid bound for the ODE log-density: \(b^{}_{}() p^{}_{}()\) for all

Figure 3: Estimated posteriors under surrogate vs. exact prior. For each task, the variational distribution is a RealNVP, and the score model is the same between both prior functions. **(a)** Both prior functions recover the correct (Gaussian) posterior. The score-based prior was trained on samples from a known Gaussian distribution (originally fit to \(16 16\) face images), and the measurements are the lowest 6.25% spatial frequencies of a test image from the prior. Since the prior and likelihood are both Gaussian, we know the ground-truth Gaussian posterior. **(b)** We estimate posteriors for (i) denoising a CelebA image and (ii) denoising a CIFAR-10 image. The score-based prior was trained on CelebA in (i) and CIFAR-10 in (ii). Visual differences between the estimated posteriors appear mostly in the image background, and the prior functions result in comparable image quality.

Figure 2: Computational efficiency of our proposed surrogate prior (“Surrogate”) vs. exact prior (“Exact”). For each image size, we estimated a posterior of images conditioned on \(4\)-accelerated MRI measurements of a knee image, using a Gaussian distribution with diagonal covariance as the variational distribution. The hardware is 4x NVIDIA RTX A6000. The surrogate prior allows for variational inference of image sizes that are prohibitively large for the exact prior. For image sizes supported by the exact prior, the surrogate improved total optimization time by over \(120\) while using less memory and scaling better with image size. “Image-Restoration Quality” verifies that optimization with the surrogate was done fairly, as the PSNR and SSIM of the converged posterior (averaged over 128 samples) are at least as high as with the exact prior.

\( q_{}()\), except for some outliers due to variance of \(b_{}^{}()\). However, we find that optimization follows a different trajectory depending on the prior. With the surrogate, samples \( q_{}\) tend toward a region where the bound gap is small (i.e., \(b_{}^{}()\) is close to \( p_{}^{}()\)). Meanwhile, the exact prior follows a loss landscape whose structure appears to be independent of the lower-bound. Note that samples from \(q_{}\) optimized under the exact prior obtain higher values of \(b_{}^{}()\) than samples obtained under the surrogate. The observations in Fig. 4 suggest that gradients under the surrogate tend to push the \(q_{}\) distribution along the boundary of equality between \(b_{}^{}\) and \( p_{}^{}\). This constrains the path taken through gradient descent and subsequently the converged solution.

### Image-reconstruction quality

It would be reasonable to assume that diffusion-based approaches discussed in Sec. 2, although less principled, may lead to better visual quality than a Bayesian approach. However, we find that in addition to providing more-reliable uncertainty, our approach achieves higher-fidelity reconstructions. We note that similarity to a ground-truth image does not indicate a correct posterior. Still, for a good prior, it might be desirable for posterior samples to accurately reflect the true underlying image.

We performed multiple MRI tasks at different acceleration rates and compared our approach to three baselines: **SDE+Proj**, **Score-ALD**, and Diffusion Posterior Sampling (**DPS**) . SDE+Proj projects images onto a measurement subspace. Score-ALD and DPS approximate the posterior throughout reverse diffusion. All baselines involve at least one measurement-weight hyperparameter. The implementations and hyperparameter settings for SDE+Proj and Score-ALD were provided by Song et al. . For DPS, we followed the implementation of Chung et al.  and performed a hyperparameter search on an \(8\)-acceleration test image to find the optimal PSNR.

We simulated MRI at three different acceleration factors for ten test images, resulting in thirty posterior distributions to be estimated. As baseline implementations do not account for measurement noise, we gave the baselines noiseless measurements and set a near-zero measurement noise for our method. The test images were randomly sampled from the fastMRI dataset and resized to \(64 64\)Our approach was DPI with the surrogate prior, meaning we optimized a RealNVP to approximate each posterior and used the lower-bound function \(b_{}^{}\) as the prior log-density. The score model \(_{}\) was trained on \(64 64\) images of knee scans from fastMRI and stayed fixed across all methods.

Our method achieves a marked improvement in PSNR and SSIM over the three baselines (Fig. 5). Across all acceleration factors and baselines, our method improves PSNR by between \(2.7\) and \(8.5\) dB. Even though each method uses the same score model, restoration quality depends on how the prior is used for inference; whereas baselines loosely approximate the posterior and involve hyperparameters, our approach treats the diffusion model as a standalone prior in Bayesian inference.

## 6 Conclusion

We have presented a surrogate function that provides efficient access to score-based priors for Bayesian inference. We empirically verify that the evidence lower-bound \(b_{}^{}() p_{}^{}()\) can serve as a proxy for evaluating the log-prior of an image under a trained diffusion model. Paired with any log-likelihood function, \(b_{}^{}()\) can be plugged into a Bayesian-inference algorithm. Our experiments with variational inference show at least two orders of magnitude in runtime improvement and significant memory improvement over the ODE-based prior. This enables inference of images previously too large for a strictly Bayesian approach, such as \(256 256\) pixels. We also establish that a principled approach like ours outperforms baselines on image-restoration metrics, evidence that following a Bayesian approach results in more-reliable image reconstructions.

**Limitations.** A variational approach like ours depends on the expressiveness of the variational distribution. Improvements may be possible by using a diffusion model instead of a discrete normalizing flow as the variational distribution. We also note that there are open theoretical questions about \(b_{}^{}\) as it relates to \(p_{}^{}\). **Broader impact.** Our proposed framework for efficient estimation of high-dimensional, sophisticated posteriors has broad potential impact for computational imaging. Many imaging tasks, especially in science and medicine, would benefit from accurate uncertainty quantification with principled, data-driven priors.

Figure 5: Accelerated MRI of knee images. **(a)** For each acceleration factor (\(4\), \(8\), \(16\)), we estimated posteriors for ten images measured at that acceleration rate. Baseline methods do not capture a true posterior: Score-ALD and DPS strongly approximate the posterior uncertainty, and SDE+Proj is a non-Bayesian projection-based approach. For each method, we computed the average PSNR and SSIM of 128 estimated posterior samples. The line plot shows the average result across the ten tasks; the shaded region shows one std. dev. above and below the average. **(b)** An example of \(16\)-accel. MRI. The cropped region exemplifies how baselines hallucinate incorrect more features than necessary. (a) and (b) are evidence that a principled Bayesian approach can capture a more accurate posterior than previous unsupervised methods.