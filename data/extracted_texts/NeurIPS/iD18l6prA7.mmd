# \(C^{2}m^{3}\): Cycle-Consistent Multi-Model Merging

Donato Crisostomi

Sapienza University of Rome

crisostomi@di.uniroma1.it &Marco Fumero

Institute of Science and Technology Austria

fumero@di.uniroma1.it &Daniele Baieri

Sapienza University of Rome

baieri@di.uniroma1.it &Florian Bernard

University of Bonn

fb@uni-bonn.de &Emanuele Rodola

Sapienza University of Rome

rodola@di.uniroma1.it

###### Abstract

In this paper, we present a novel data-free method for merging neural networks in weight space. Differently from most existing works, our method optimizes for the permutations of network neurons globally across all layers. This allows us to enforce cycle consistency of the permutations when merging \(n 3\) models, allowing circular compositions of permutations to be computed without accumulating error along the path. We qualitatively and quantitatively motivate the need for such a constraint, showing its benefits when merging sets of models in scenarios spanning varying architectures and datasets. We finally show that, when coupled with activation renormalization, our approach yields the best results in the task.

## 1 Introduction

In the early days of deep learning, modes -- parameters corresponding to local minima of the loss landscape -- were considered to be isolated. Being depicted as points at the bottom of convex valleys, they were thought to be separated by high-energy barriers that made the transition between them impossible. However, a series of recent works have gradually challenged this perspective, first showing that modes can be actually connected by paths of low energy [10; 14], and later that, in some cases, these paths may even be linear . While linear paths in  could only be obtained after training the equally-initialized models for a few epochs, follow-up work  speculated that the isolation of modes is a result of the permutation symmetries of the neurons. In fact, given a layer \(W_{}\) of a fixed network \(A\), a large number of functionally-equivalent networks can be obtained by permuting the neurons of \(W_{}\) by some permutation \(P\) and then anti-permuting the columns of the subsequent layer \(W_{+1}\). This intuition led to the conjecture that all modes lie in the same convex region of the parameter space, denoted as _basin_, when taking into account all possible permutations of the neurons of a network.This motivated a series of works trying to align different modes by optimizing for the neuron permutations [1; 29; 36; 21]. This has strong implications for model merging, where different models, possibly trained with different initializations [1; 29; 34] or on different datasets and tasks [1; 36], are aggregated into a single one. In this work, we focus on the _data-free_ setting, aligning networks based on some similarity function that is computed directly over the neurons themselves. To this end, we follow Ainsworth et al.  and formalize the problem of model merging as an assignment problem, proposing a new algorithm that is competitive with previous approaches while allowing global constraints to be enforced.

The problemWe investigate the problem of merging \(n>2\) models, noting that existing pairwise approaches such as  do not guarantee cycle consistency of the permutations (see Figure 1). As shown in Figure 1(b) and Figure 1(a), going from a model \(A\) to a model \(C\) through a model \(B\), and then mapping back to \(A\), results in a different model than the starting one -- specifically, the targetmodel ends up in a completely different basin. More formally, for these methods, the composition of permutations along any cycle does _not_ result in the identity map. This also holds for the \(n=2\) case, where the permutations optimized to align model \(A\) to model \(B\) are not guaranteed to be the inverse of those mapping \(B\) to \(A\); this makes the alignment pipeline brittle, as it depends on an arbitrary choice of a mapping direction.

ContributionTo address this issue, we introduce a novel alignment algorithm that works for the general case with \(n 2\) models, while _guaranteeing_ cycle consistency. The key idea is to factorize each permutation mapping \(B\) to \(A\) as \(P^{AB}=P^{A}(P^{B})^{}\), where \((P^{B})^{}\) maps \(B\) to a common space denoted as _universe_, and \(P^{A}\) maps from the universe back to \(A\). This formulation ensures cycle consistency by design, as any cyclic composition of such permutations equals the identity.

Our numerical implementation is based on the Frank-Wolfe algorithm , and optimizes for the permutations of _all_ the layers simultaneously at each step, naturally taking into account the inter-layer dependencies in the process. This desirable property is in contrast with other approaches such as Ainsworth et al. , which seek the optimal permutations for each layer separately, and thus can not ensure coherence across the entire network.

We run an extensive comparison of our approach with existing ones both in the standard pairwise setting and in merging \(n>2\) models, spanning a broad set of architectures and datasets. We then quantitatively measure the influence of architectural width, confirming the existing empirical evidence on its role in linear mode connectivity. Further, we assess how the performance of the merged model depends on the number of models to aggregate, and show that the decay is graceful. We finally analyze the basins defined by the models when mapped onto the universe, and investigate when and to what extent these are linearly connected.

Wrapping up, our contributions are four-fold:

* We propose a new data-free weight matching algorithm based on the Frank-Wolfe algorithm  that globally optimizes for the permutations of all the layers simultaneously;
* We generalize it to the case of \(n 2\) models, enforcing guaranteed cycle-consistency of the permutations by employing a universal space as a bridge;
* We leverage the multi-model matching procedure for model merging, using the universal space as aggregation point;
* We conduct an extensive analysis showing how the merge is affected by the number of models, their width and architecture, as well as quantitatively measuring the linear mode connectivity in the universe basin.

Finally, to foster reproducible research in the field, we release a modular and reusable codebase containing implementations of our approach and the considered baselines.1

Figure 1: Cycle-Consistent Multi-Model Merging over three models \(A,B,C\). **Left:** existing methods seek pairwise permutations that map between models; note that \(P^{AC} P^{CB} P^{BA} I\) in general, unless this is explicitly enforced. **Right:** our method computes permutations \(P^{A}\), \(P^{B}\), \(P^{C}\) from each model to a _universe_\(U\), such that a pairwise permutation \(P^{BA}\) mapping \(A\) to \(B\) can be obtained as \(P^{BA}=P^{B}(P^{A})^{}\). This way, cycle-consistency is enforced by design and \(P^{AC} P^{CB} P^{BA}=I\).

## 2 Background

Mode connectivityAs introduced in Section 1, mode connectivity studies the geometry of the loss landscape with a particular interest on the regions corresponding to local minima. Following Frankle et al. , we assess the connectivity for two given modes by computing their loss barrier:

**Definition 2.1**.: (_Loss barrier_) Given two points \(_{A},_{B}\) and a loss function \(\) such that \((_{A})(_{B})\), the _loss barrier_ is defined as

\[_{}((1-)_{A}+_{ B})-((_{A})+ (_{B})).\]

Intuitively, this quantity measures the extent of the loss increase when linearly moving from the basin of a mode to the other. When two modes share the same basin, the loss does not increase at all and results in a barrier close to zero.

Weight-space symmetriesFollowing the rich line of works on mode connectivity and model merging , we start from the essential insight of _neuron permutation invariance_ in neural networks. Let us focus on the simple case of a Multi-Layer Perceptrons (MLP), where we can write the computation for an intermediate layer \(W_{}^{d_{+1} d_{}}\) as \(z_{+1}=(W_{}z_{}+b_{})\), with \(z_{}\) being the input at the \(\)-layer and \(\) denoting an element-wise activation function. For the sake of a clear exposure, we consider the bias \(b_{}=0\) in the following. If apply a permutation matrix \(P\) to the rows of the \(W_{}\) matrix (_i.e._ the neurons), we obtain \(z^{}_{+1}=(PW_{}z_{})\). Being an element-wise operator, \(\) commutes with \(P\) and can be neglected wlog. Since \(z^{}_{+1} z_{}\) when \(P I\), we can still nullify the effect of the permutation by anti-permuting the columns of the subsequent layer for the inverse permutation of \(P\), _i.e._\(P^{}\). In fact,

\[z^{}_{+2}=W_{+1}P^{}z^{}_{+1}=W_{+1} {P^{}P}_{I}W_{}z_{}=z_{+2}\]

making pairs of models that only differ by a permutation of the neurons de facto functionally equivalent. Given the enormous number of such permutations, it stands to reason that the resulting weight-space symmetries act as a major factor in the isolation of modes.

Solving for the permutationGiven the above considerations, Entezari et al.  speculated that all models end up in a single basin after having accounted for permutation symmetries. Assuming this to hold at least in practical cases, Ainsworth et al.  proposed a simple algorithm to find the permutations matching two models by maximizing a local version of the sum of bi-linear problems:

\[_{\{P_{}\}}\ _{=1}^{L} W_{}^{A},P_{ }W_{}^{B}P_{-1}^{T}\,,\] (1)

Figure 2: Existing methods accumulate error when cyclically mapping a model through a series of permutations, while \(C^{2}M^{3}\) correctly maps the model back to the starting point.

with \(P_{0}:=I\). Noting that Equation (1) is NP-hard, Ainsworth et al.  tackle this problem by considering one layer at a time, relaxing the bi-linear problems to a set of linear ones that can be efficiently solved with any Linear Assignment Problem (LAP) solver, e.g., the Hungarian algorithm. This layer-wise linearization of the objective function, however, corresponds to high variance in the results that depend on the random order of the layers during optimization. See Table 7 for an empirical evaluation confirming this issue.

Renormalizing the activationsNotwithstanding the quality of the obtained matching, the loss barrier can still be high due to the mismatch in the statistics of the activations. In fact, REPAIR empirically shows the presence of a decay in the variance of the activations after the interpolation. They further show that the loss can be drastically reduced by "repairing" the mean and variance of the activations, forcing the statistics of the merged network to interpolate those of the endpoint networks. We refer the reader to Appendix A.4 for an in-depth explanation.

## 3 Approach

We now propose a novel algorithm to tackle the weight matching problem, first introducing its formulation in the pairwise case and then generalizing it to match and merge a larger number \(n\) of models in a cycle-consistent fashion.

Pairwise matchingAs we have seen, the NP-hardness of Equation (1) demands for a relaxation of the problem to be tackled. Differently from Ainsworth et al. , we opt to maintain the objective global with respect to the layers and instead iteratively optimize its linear approximation via the the Frank-Wolfe algorithm . This procedure requires the computation of the gradient of Equation (1) with respect to each permutation \(P_{i}\), thus we have to account for two contributions for each \(_{P_{i}}\), _i.e._, its gradient from permuting the rows of \(W_{i}\) and the one from permuting the columns of \(W_{i+1}\):

\[_{P_{i}}f=^{A}P_{i-1}(W_{i}^{B})^{}}_{}+^{A})^{}P_{i+1}W_{i+1}^{B}}_{ }.\] (2)

The Frank-Wolfe algorithm then uses the gradient to iteratively update the solution by linearly interpolating between the current solution and the projected gradient. We refer to Lacoste-Julien  for theoretical guarantees of convergence. The full algorithm is reported in Appendix A.2.

Generalization to \(n\) modelsIn order to generalize to \(n\) models, we jointly consider all pairwise problems

\[_{P_{i}^{pq}}\ _{p=1}^{n}_{q=1 \\ q p}^{n}_{i=1}^{L} W_{i}^{p},P_{i}^{pq}W_{i}^{q} (P_{i-1}^{pq})^{},\] (3)

where the superscript \(pq\) indicates that the permutations maps model \(q\) to model \(p\), with \(P_{0}^{pq}:=I\). In order to _ensure cycle consistency by construction_ we replace the quadratic polynomial by a fourth-order polynomial. Dropping the layer subscript for the sake of clear exposure, we replace the pairwise matchings \(P^{pq}\) in the objective of Equation (3) by factorizing the permutations into _object-to-universe matchings_\(P^{pq}=P^{p}(P^{q})^{}\) so that each model \(q\) can be mapped back and forth to a common universe \(u\) with a permutation and its transpose, allowing to map model \(q\) to model \(p\) by composition of \((P^{q})^{}\) (\(q u\)) and \(P^{p}\) (\(u p\)). This way, the objective of Equation (3) becomes

\[_{p q}^{n}_{i=1}^{L} W_{i}^{p},P_{i}^{p}(P_{i}^{q})^{} W_{i}^{q}(P_{i-1}^{p}(P_{i-1}^{q})^{})^{}=_{p q}^{n}_{i=1} ^{L}(P_{i}^{p})^{}W_{i}^{p}P_{i-1}^{p},(P_{i}^{q})^{}W_{i}^{q}P_ {i-1}^{q}.\] (4)

As stated by Theorem 3.1, the permutations we obtain using Equation (4) are cycle consistent. We refer the reader to Bernard et al.  for the proof and a complete discussion of the subject.

**Theorem 3.1**.: _Given a set of \(n\) models \(p_{0},,p_{n}\) and object-to-universe permutations \(P_{i}^{p_{j}}\) computed via Equation (4), the pairwise correspondences defined by \(P_{i}^{p_{i}p_{j}}=P_{i}^{p_{1}}(P_{i}^{p_{j}})^{T}\) are cycle-consistent, i.e.,_

\[P_{i}^{p_{1}p_{j}} P_{i}^{p_{3}p_{2}} P_{i}^{p_{2}p_{1}}=I\]

_for all layer indices \(i\), \(2 j n\)._Similarly to the pairwise case, the approach requires computing the gradients for the linearization. This time, however, each \(_{P^{A}_{}}f\) has four different contributions: one from permuting the rows of its corresponding layer, one from anti-permuting the columns of the subsequent layer, and two other contributions that arise from the symmetric case where \(A\) becomes \(B\). In detail,

\[_{P^{A}_{}}=_{P^{A}_{}}^{}+_{P^{A}_{} }^{}+_{P^{A}_{}}^{,}+_{P^ {A}_{}}^{,}\] (5)

where

\[_{P^{A}_{}}^{} =W^{A}_{}P^{A}_{-1}(P^{B}_{-1})^{}(W^{B}_{})^ {}P^{B}_{} _{P^{A}_{}}^{} =(W^{A}_{+1})^{}P^{A}_{+1}\,(P^{B}_{+1})^{}\, W^{B}_{+1}\,P^{B}_{}\] \[_{P^{A}_{}}^{,} =W^{B}_{}P^{B}_{-1}(P^{A}_{-1})^{}(W^{A}_{})^ {}P^{A}_{} _{P^{A}_{}}^{,} =(W^{B}_{+1})^{}P^{B}_{+1}\,(P^{A}_{+1})^{}\, W^{A}_{+1}\,P^{A}_{}\]

See Algorithm 1 for a complete description of the procedure.

```
0: Weights of \(n\) models \(M_{i=1}^{N}\)
0: tolerance \(>0\)
0: Approximate solution to Equation (4)
1:\(^{k}\) identity matrices
2:repeat
3:for\((p,q)[1,,n][1,,n]\)do
4:for\(i=1\) to \(L\)do
5:\(P^{i}_{i},P^{p,k}_{i-1}\) permutations over rows and columns of \(W^{p}_{i}\) respectively
6:\(P^{q,k}_{i},P^{q,k}_{i-1}\) permutation over rows and columns of \(W^{q}_{i}\) respectively
7:\(_{P^{p,k}_{i}}f(W^{p}_{+1})^{}P^{p}_{+1}\,(P^{q}_{+1} )^{}\,W^{q}_{+1}\,P^{q}_{}\)
8:\(_{P^{p,k}_{i-1}}f(W^{p}_{+1})^{}P^{p}_{+1}\,(P^{q}_{+1 })^{}\,W^{q}_{+1}\,P^{q}_{}\)
9:endfor
10:endfor
11:for\(P^{k}_{i}^{k}\)do
12:\(_{i}(_{P^{k}_{i}}f)\)
13:endfor
14:\(\) line search\((f,^{k},)\)
15:for\(P^{k}_{i}^{k}\)do
16:\(P^{k+1}_{i}=(1-)P^{k}_{i}+\;_{i}\)
17:endfor
18:until\(\|f(A,B,^{k+1})-f(A,B,^{k})\|<\)
19:return\(^{k}\) ```

**Algorithm 1** Frank-Wolfe for \(n\)-Model Matching

Merging in the universe spaceLooking at the loss landscape resulting from interpolating models in Figure 3, we see that the loss curves are much lower when the models are interpolated in the universe space. In fact, the originally disconnected modes end up in the same basin when mapped onto the universe, making it suitable to average the models. Therefore, our merging method aggregates the models by taking the mean of the weights in the universe space, as detailed in Algorithm 2.

```
0:\(N\) models \(A_{1},,A_{N}\) with \(L\) layers
0: merged model \(M\)
1:\(\{P_{1},,P_{N}\}\) Frank-Wolfe(\(M_{1},,M_{N}\))
2:for\(i=1\) to \(N\)do
3:\(M_{i}^{}\) map_to_universe\((A_{i},P_{i})\)
4:endfor
5:\(M^{}_{i=1}^{N}M_{i}^{}\)
6:return\(M^{}\) ```

**Algorithm 2**\(C^{2}M^{3}\): Cycle-Consistent Multi Model Merging

## 4 Experiments

We now evaluate the quality of our proposed framework both in matching models and in the subsequent merging operation. Approaches suffixed with a \(\) indicate the application of REPAIR.

Matching and merging two modelsAs described in Section 3, our formalization can readily be used to match \(n=2\) models. In this case, the energy is given by Equation (1) and the permutations are not factorized. We compare the performance of our approach against the Git Re-Basin algorithm  and the naive baseline that aggregates the models by taking an unweighted mean on the original model weights without applying any permutation. In this setting, our method performs on par with the state-of-the-art. Differently from the latter, however, we do not depend on the random choice of layers, as the optimization is performed over all layers simultaneously. As presented in Figure 4, this results in Git Re-Basin exhibiting variations of up to \(10\%\) in accuracy depending on the optimization seed, while our method shows zero variance. We refer the reader to Appendix B.1 for a thorough evaluation of \(C^{2}M^{3}\) over a set of different datasets and architectures. In summary, _our approach is able to match two models with the same accuracy as the state-of-the-art, while being deterministic and independent of the random choice of layers_.

Matching and merging \(n\) modelsWe now evaluate \(C^{2}M^{3}\) in matching and merging \(n\) models. The matching is given by the factorized permutations obtained by Algorithm 1. We compare against two baselines: the simple approach of naively averaging the weights without any matching, and the MergeMany approach proposed by Ainsworth et al. . The latter is reported in Appendix A for convenience. As reported in Table 1, \(C^{2}M^{3}\) obtains far superior results in terms of accuracy and loss in all considered settings, with accuracy gains as high as \(+20\%\). Moreover, our approach natively yields cycle-consistent permutations: Figure 1(b) shows that Git Re-Basin  accumulates significant error when computing the distance between the source model and the model obtained by applying a cyclic series of permutations, while our approach is able to perfectly recover the source model. This is further confirmed in Figure 1(a), where we show the loss and accuracy curves when interpolating between a model \(A\) and the model mapped back after a cyclic permutation.

Figure 4: Accuracy of the interpolated model using Git Re-Basin  over different optimization seeds.

Figure 3: 2D projection of the loss landscape when matching three modes \(_{A},_{B},_{C}\); the models \((_{A}),(_{B}),(_{C})\) are their resulting images in the universe, and lie in the same basin. Red zones indicate low-loss regions (typically basins), while blue zones indicate high-loss ones.

Models cyclically permuted with Git Re-Basin end up in a different basin than the one they started from, while our cycle-consistent approach ensures that the target model is exactly the same as the source. Wrapping up, _our approach matches and merges \(n\) models with a significant improvement in performance over the state-of-the-art, while ensuring cycle-consistent permutations._

Model similarity before and after mappingAs we can see in Figure 5, the cosine similarity of the weights of the models is \(3\) higher after mapping the latter to the universe. This suggests that the initial quasi-orthogonality of models is at least partially due to neuron permutation symmetries. We also report in Appendix C.1.2 the similarity of the representations between pairs of models. Interestingly, the latter does not change before and after mapping to the universe, but only if we consider a similarity measure that is invariant to orthogonal transformations such as CKA . When using a measure that does not enjoy this property, such as the Euclidean distance, the representations become much more similar in the universe space. In short, _the models are \(3\) more similar in the universe space and the mapping affects the representations as an orthogonal transformation_.

Effect of activation renormalizationOur empirical evidence also points out the benefits of the REPAIR operation  that is performed after the merging. In fact, the detrimental effect of model averaging on the activation statistics  still applies when taking the mean of \(n\) models instead of two. Our results clearly show the benefit of REPAIR, making it a key ingredient of our overall framework. Requiring meaningful interpolation endpoints to be effective, REPAIR has lower benefit when employed on the MergeMany algorithm of Ainsworth et al. . In fact, iteratively taking means of different random model subsets and aligning the left-out models to the mean is a more complex process than interpolating between some endpoint models. By taking the mean of models in the universe space, we are instead effectively interpolating between endpoint models that can be used for the computation of the statistics in Equation (8). Figure 6 shows the benefit of using the repair operation on 5 VGG models trained on CIFAR10 mapped to the universe space. Specifically we fix one model "a" and we linearly interpolate in the universe space with respect to the other models, measuring accuracy before and after applying REPAIR. Other than boosting performance, we observe that the latter reduces the variance over interpolation paths, resulting in the interpolation curves of all the models overlapping. Overall, _using the models in the universe as meaningful endpoints to gather activation statistics, our approach can fully leverage activation renormalization techniques such as REPAIR_.

    &  &  &  \\ 
**Matcher** &  &  &  &  &  &  \\  & train & test & train & test & train & test & train & test & train & test & train & test \\  Naive & 0.03 & 0.03 & 3.28 & 3.28 & & 0.10 & 0.10 & 3.07 & 3.07 & 0.01 & 0.01 & 5.30 & 5.30 \\ MergeMany & 0.88 & 0.86 & 1.11 & 1.13 & 0.38 & 0.37 & 2.08 & 2.06 & 0.31 & 0.28 & 3.01 & 2.76 \\ MergeMany\({}^{}\) & 0.88 & 0.86 & 1.11 & 1.13 & 0.50 & 0.50 & 2.34 & 2.30 & 0.24 & 0.22 & 3.31 & 3.12 \\ \(C^{2}M^{3}\) & 0.89 & 0.87 & 1.07 & 1.10 & 0.42 & 0.40 & 2.11 & 2.05 & 0.34 & 0.30 & 2.94 & 2.63 \\ \(C^{2}M^{3}\) & **0.89** & **0.87** & **1.07** & **1.10** & **0.72** & **0.69** & **1.26** & **1.12** & **0.53** & **0.46** & **2.13** & **1.67** \\  Naive & 0.04 & 0.04 & 4.04 & 4.04 & & 0.10 & 0.10 & 2.31 & 2.31 & 0.01 & 0.01 & 6.22 & 6.22 \\ MergeMany\({}^{}\) & 0.03 & 0.03 & 7.17 & 7.18 & 0.10 & 0.10 & 2.36 & 2.36 & 0.45 & 0.38 & 2.32 & 3.06 \\ MergeMany\({}^{}\) & 0.03 & 0.03 & 4.74 & 4.72 & 0.60 & 0.57 & 1.43 & 1.32 & 0.41 & 0.35 & 2.27 & 2.68 \\ \(C^{2}M^{3}\) & 0.27 & 0.27 & 3.43 & 3.47 & 0.11 & 0.11 & 2.34 & 2.34 & 0.46 & 0.39 & 2.25 & 3.03 \\ \(C^{2}M^{3}\) & **0.60** & **0.60** & **1.32** & **1.34** & **0.64** & **0.62** & **1.34** & **1.23** & **0.60** & **0.49** & **1.43** & **2.23** \\   

Table 1: Accuracy of the merged model when merging \(5\) models trained with different initializations. The best results are highlighted in bold. \({}^{}\) denotes models after the REPAIR operation.

Figure 5: Cosine similarity of the weights of 5 ResNet20 trained on CIFAR10 with 2\(\) width.

Figure 6: Interpolation curves of VGG models in the universe.

Increasing \(n\)In this experiment, we show how the merged model behaves when increasing the number of aggregated models. As we can see in Figure 6(a), increasing the number of MLPS up to \(20\) causes the performance to slightly deteriorate in a relative sense, but remaining stable in an absolute sense as it doesn't fall below \(98\%\). More surprisingly, Figure 6(b) shows that for a ResNet20 architecture with \(4\) width the loss and accuracy are not monotonic, but rather they seem to slightly fluctuate. This may hint at the merging process being more influenced by the composition of the model set, than by its cardinality. Intuitively, a model that is difficult to match with the others will induce a harder optimization problem, possibly resulting in a worse merged model. We dive deeper in the effect of the composition of the set of models in Appendix C.2. In short, _our approach is effective in merging a larger number of models, suggesting promise in federated settings._

Varying widthsWe now measure how architectural width affects model merging, taking into consideration ResNet20 architectures with width \(W\{1,2,4,8,16\}\). As we can see in Figure 8, _width greatly increases the performance of the merged model_, reaching the zero-loss barrier first observed in  when \(W=16\). This is in line with the observations relating linear mode connectivity and network widths , and confirms the intuition that the merging is only effective when modes _can_ be linearly connected.

Alternative: fixing one model as universeAlternatively, one could achieve cycle consistency by using one of the source models as reference and learning pairwise maps towards this one. This, however, would require arbitrarily selecting one of the models, making the overall merging dependent on an arbitrary choice. To see why this matters, we merged \(5\) ResNet20-\(4\) by choosing one model as reference and aggregating the models in its basin. Figure 9 shows severe oscillations in the results, with one model reaching an accuracy as low as 65%, while our approach performs as the best possible reference. This approach, moreover, does not address multi-model merging, as it is intrinsically pairwise: in a multi-task setting, models optimally mapped to a reference basin would only be able to solve the task solved by the reference model. This would prevent merging to be used for models containing complementary information, such as knowledge fusion 

Figure 8: Accuracy and loss when merging \(3\) ResNet20s trained over CIFAR10 with different widths. \(\) indicates models after applying REPAIR.

Figure 7: Accuracy and loss when increasing the number \(n\) of models to match and merge.

Figure 9: Accuracy of the merged model when mapping towards one arbitrary model (a, b, c, d, e) versus using \(C^{2}M^{3}\) and the universe space.

or multi-task merging . In our setting, instead, the universe model must by design be a function of all the models and act as a midpoint, hence aggregating information from all the models.

Linear mode connectivity in the universeFigure 10 shows that the loss curves of models interpolated in the universe are much lower than those interpolated in the original space, suggesting that the models are more connected in the former. These results, together with the loss landscape observed in Figure 3, _encourage merging the models in the universe space due to the lower loss barrier._

## 5 Related work

**Mode Connectivity and model merging.** Mode connectivity studies the weights defining local minima. Frankle et al.  studied linear mode connectivity of models that were trained for a few epochs from the same initialization and related it to the lottery ticket hypothesis. Without requiring the same initialization, Entezari et al.  speculated that all models share a single basin after having solved for the neuron permutations. Model merging aims at aggregating different models into a single one to inherit their capacities without incurring in the cost and burden of ensembling. In this regard, Singh and Jaggi  proposed an optimal-transport based weight-matching procedure, while Git Re-Basin  proposed three matching methods and the MergeMany procedure seen in Section 4. Subsequently, REPAIR showed that a significant improvement in performance of the interpolated model may be obtained by renormalizing its activations rather than changing matching algorithm. Differently from all these works, we consider merging \(n\) models and propose a principled way to perform it with cycle-consistency guarantees.

**Cycle consistency.** Ensuring cycle consistency of pairwise maps is a recurring idea in the computer vision and pattern recognition literature. In the realm of deep learning, earlier studies addressing multi-graph matching achieved cycle consistency by synchronizing ex-post the predicted pairwise permutations [40; 41]. The alternative approach using an object-to-universe matching framework, which we adopt here, inherently ensures cycle consistency by construction, as demonstrated in [4; 16; 31]. To the best of our knowledge, none of the existing works tackles cycle-consistent alignment of neural models. We refer to Appendix A.1 for a more detailed list of related works.

## 6 Conclusions

In this work, we treated the problem of model matching and merging. We first introduced a novel weight matching procedure based on the Frank-Wolfe algorithm that optimizes for the permutation matrices of all layers jointly, and then generalized it to the case of \(n\) models. Guaranteeing cycle-consistency, the latter poses a principled way to merge a set of models without requiring an arbitrary reference point. We then showed the approach to yield superior performance compared to existing ones in merging multiple models in a set of scenarios spanning different architectures and datasets. We believe the formalism to elegantly fit the requirement for the merging operation to unify the different models into a cohesive one, rather than mapping all of them to one of the models in the set.

Figure 10: Linear mode connectivity before and after mapping to the universe for \(3\) ResNet20-2\(\) models trained over CIFAR10 according to Algorithm 1.