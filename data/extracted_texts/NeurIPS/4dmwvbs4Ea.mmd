# Offline RL via Feature-Occupancy Gradient Ascent

Anonymous Author(s)

Affiliation

Address

email

###### Abstract

We study offline Reinforcement Learning in large infinite-horizon discounted Markov Decision Processes (MDPs) when the reward and transition models are linearly realizable under a known feature map. Starting from the classic linear-program formulation of the optimal control problem in MDPs, we develop a new algorithm that performs a form of gradient ascent in the space of feature occupancies, defined as the expected feature vectors that can potentially be generated by executing policies in the environment. We show that the resulting simple algorithm satisfies strong computational and sample complexity guarantees, achieved under the least restrictive data coverage assumptions known in the literature. In particular, we show that the sample complexity of our method scales optimally with the desired accuracy level and depends on a weak notion of coverage that only requires the empirical feature covariance matrix to cover a single direction in the feature space (as opposed to covering a full subspace). Additionally, our method is easy to implement and requires no prior knowledge of the coverage ratio (or even an upper bound on it), which altogether make it the strongest known algorithm for this setting to date.

## 1 Introduction

We study Offline Reinforcement Learning (ORL) in sequential decision making problems whereby a learner aims to find a near-optimal policy with sole access to a static dataset of interactions with the underlying environment (Levine et al., 2020). This line of work is naturally relevant to real-world tasks for which learning an accurate simulator of the environment is potentially intractable or impossible, trial-and-error learning could have grave consequences, yet logged interaction data is readily available. For example, in a high-stake application such as autonomous driving, building a sufficiently accurate simulator for the vehicle and its environment would require modelling very complex systems, which can be intractable both statistically and computationally. At the same time, running experiments in the real world could endanger the lives of other road users or result in damages to the vehicle. Yet, with the advent of tools for efficient sensory-data collection and processing, large volumes of logged data from human drivers are readily available.

An efficient ORL method is one which finds a near-optimal policy after a tractable number of elementary computations and samples from the dataset. It is well-known in this setting that the quality of the solution has to heavily depend on the quality of the data, and in particular one cannot hope to find a near-optimal policy if the data covers the space of states and actions poorly. To formalize this intuition, many notions of data coverage have been proposed in the offline RL literature, ranging from a very restrictive uniform coverage assumption that requires the data-generating policy to cover the entire state-action space (Munos and Szepesvari, 2008) to a variety of partial coverage conditions whereby this exploratory condition is only required for state-action pairs that are of interest to the optimal policy (Liu et al., 2020; Rashidinejad et al., 2021; Uehara and Sun, 2021; Zhan et al., 2022; Rashidinejad et al., 2022; Li et al., 2024). In the present work, we study the setting of linear _MarkovDecision Processes_ (MDPs) (Jin et al., 2020; Yang and Wang, 2019) where the reward and transition matrix admit a low rank structure in terms of a known feature map, and data-coverage assumptions can be defined in the space of features. As shown by (Zanette et al., 2021), in this setting it is possible to obtain strong guarantees if the offline data is well-aligned with the expectation of the feature vector generated by the optimal policy (as opposed to requiring alignment with the entire distribution of features as required by other common offline RL methods (Jin et al., 2021; Xie et al., 2021; Uehara and Sun, 2021; Zhang et al., 2022)). In the present paper, we propose a simple and efficient algorithm that yields the best known sample complexity guarantees for this problem setting, all while only requiring the weakest known data-coverage assumptions of Zanette et al. (2021).

Our approach is based on the LP formulation of optimal control in infinite-horizon discounted MDPs due to Manne (1960), and more specifically on its low-dimensional saddle-point reparametrization for linear MDPs proposed by Gabbianelli et al. (2024) (which itself builds on earlier work by Neu and Okolo, 2023; Bas-Serrano et al., 2021). Primal variables of this saddle-point objective correspond to expectations of feature vectors under the state-action distribution of each policy (called _feature occupancies_), and dual variables correspond to parameters of linear approximations of action-value functions. We design an algorithm based on the idea of optimizing the unconstrained primal function that is derived from the saddle-point objective by eliminating the dual variables via a classic dualization trick. More precisely, we design a sample-based estimator of the primal function and optimize it via a variant of gradient ascent in the space of feature occupancies.

This approach is to be contrasted with the method of Gabbianelli et al. (2024), which instead optimized the original saddle-point objective via stochastic primal-dual methods. Their algorithm interleaved a sequence of "policy improvement" steps with an inner loop performing "policy evaluation", which resulted in a suboptimal use of sample transitions due to the costly inner loop. This issue was addressed in the very recent work of Hong and Tewari (2024) who, instead of relying on stochastic optimization, built an estimator of the saddle-point objective and optimized it via a deterministic primal-dual method. Our approach is directly inspired by their idea of estimating the saddle-point objective, but our algorithm design is significantly simpler: instead of directly optimizing the primal function in terms of feature occupancies, Hong and Tewari (2024) relied on a sophisticated reparametrization of the primal variables, and used a computationally involved procedure to update the dual variables. Both of these steps required prior knowledge of a tight bound on the feature-coverage ratio of the optimal policy, which is typically not available in problems of practical interest. Such knowledge is not required by our algorithm, thanks to the incorporation of a recently proposed stabilization trick that we make use of in our algorithm (Jacobsen and Cutkosky, 2023; Neu and Okolo, 2024). We provide a more detailed discussion of these closely related works in Section 5.

Notation.We use boldface lowercase letters \(\) to denote vectors and bold uppercase \(\) for matrices. We define the Euclidean ball in \(^{d}\) of radius \(D\) by \(_{d}(D)=\{^{d}|\|\|_{2} D\}\) and the \(A\)-simplex over a finite set \(\) of cardinality \(A\) as \(_{}=\{p_{+}^{A}|\|p\|_{1}=1\}\).

## 2 Preliminaries

We consider infinite-horizon Discounted Markov Decision Processes (DMDPs) (Puterman, 1994) of the form \((,,,,)\) where \(\) denotes a finite (yet large) set of \(X\) states and \(\) is a finite action space of cardinality \(A=||\). We refer to \(^{XA}\) as the reward vector, \(_{+}^{XA X}\) the transition matrix and \((0,1)\) the discount factor. For a state-action pair \((x,a)\) we also use the notation \(r(x,a)=[(x,a)]\) to denote the reward of taking action \(a\) in state \(x\) and \(p(x^{}|x,a)=[(x,a)\,,x^{}]\) as the probability of ending up in state \(x^{}\) afterwards.

The MDP models a sequential decision making process where an agent interacts with its environment as follows. For each step \(k=0,1,2,\), the agent observes the current state \(X_{k}\) of the environment and then goes on to select its action \(A_{k}\). Based on this action in the current state, it receives a reward \(r(X_{k},A_{k})\), transits to a new state \(X_{k+1}(|X_{k},A_{k})\) and the process continues. The objective of the agent is to find a decision-making rule that maximizes its total discounted reward when the initial state \(X_{0}\) is sampled according to a fixed initial-state distribution distribution \(_{0}_{}\). Without loss of generality, we assume that the initial state is fixed almost surely as \(X_{0}=x_{0}\), and use \(_{0}\) to refer to the corresponding delta distribution. It is known that this objective can be achieved by executing a _stationary stochastic policy_\(:_{}\), with \((a|x)\) denoting the probability of the agent selecting action \(A_{k}=a\) in state \(X_{k}=x\) for all \(k\). We will use \(\) to denote the set of all such behavior rulesand will often simply call them _policies_. We define the normalized discounted return of each policy \(\) as

\[()=(1-)_{_{0},}[ _{k=0}^{}^{k}r(x_{k},a_{k})],\]

where the role of the discount factor \((0,1)\) is to emphasize the importance of earlier rewards, and the notation \(_{_{0},}[]\) highlights that the initial state is sampled from \(_{0}\) and all actions are sampled according to the policy \(\). We will use \(^{*}\) to denote any policy that maximizes the return.

We will consider the offline RL setting where we are given access to a data set of \(n\) sample transitions \(_{n}=\{(X_{i},A_{i},R_{i},X_{i}^{})\}_{i=1}^{n}\), where \(X_{i}^{}(|X_{i},A_{i})\) is sampled independently for each \(i\) and \(R_{i}=r(X_{i},A_{i})\). Otherwise, no assumption is made about the state-action pairs \((X_{i},A_{i})\), and in particular we do not require these to be generated by a fixed behavior policy or to be independent of each other.

For describing the approach we take towards solving this problem, we need to introduce some further standard notations. The value function and action-value function associated with policy \(\) are respectively defined as

\[v^{}(x)=_{a(|x)}[q^{} (x,a)], q^{}(x,a)=_{}[ _{k=0}^{}^{k}r(x_{k},a_{k})|x_{0}=x,a_{0}=a ],\]

and the state-occupancy and state-action-occupancy measures under \(\) as

\[^{}(x)=_{a}^{}(x,a),^{} (x,a)=(1-)_{_{0},}[_ {k=0}^{}^{k}_{\{x_{k},a_{k}\}}].\]

The value functions and occupancy measures adhere to the following recursive equations, respectively termed the Bellman equation and Bellman flow condition (Bellman, 1966):

\[^{}=+^{},^{}=[ (1-)_{0}+^{}^{}].\]

Here, the composition operation \(\) is defined so that for any policy \(\) and state distribution \(^{X}\), we have \(()(x,a)=(a|x)(x)\). Notice that we can express the return of \(\) in terms of value functions and occupancy measures as \(()=(1-)_{0},^{ }=^{},\). On this note, for a given target accuracy \(>0\), we say policy \(\) is \(\)-optimal if it satisfies \(^{^{*}}.-^{},\).

In the present work, we well make use of the _linear MDP_ assumption due to Jin et al. (2020); Yang and Wang (2019), which is defined formally as follows:

**Definition 2.1** (Linear MDP).: An MDP is called linear if both the transition and reward functions can be expressed as a linear function of a given feature map \(:^{d}\). That is, there exist \(:^{d}\) and \(^{d}\) such that, for every \(x,x^{}\) and \(a\):

\[r(x,a)=(x,a),, p(x^{ }|x,a)=(x,a),(x^{}).\]

We denote by \(^{|| d}\) the feature matrix with rows given by \((x,a)^{}\) and \(^{d||}\) as the weight matrix with columns \((x)\). Further, we will assume that \(\|\|_{2}\), that \(\|\|_{2} B\) holds for all \([-B,B]\), and that all feature vectors satisfy \(\|(x,a)\|_{2} R\) for some \(R 1\).

An immediate consequence of this assumption is that the action-value function of any policy \(\) can be written as a linear function of the features as \(^{}=^{}\), with \(^{}=+^{}^{d}\). For the rest of the paper we explicitly assume that the feature matrix \(\) is full rank - which is enough to ensure uniqueness of \(^{}\). It is common to assume that the feature dimension \(d X\) such that the transition operator is low-rank. As common in this setting, we will suppose throughout the paper that the feature map \(\) is known.

Our algorithm design will be based on the linear programming formulation of MDPs, first proposed in a number of papers in the 1960's (Manne, 1960; de Ghellinck, 1960; d'Epenoux, 1963; Denardo, 1970). This formulation frames the problem of finding an optimal control policy as the following pair of primal and dual linear programs:

\[&, \\ &^{}=(1-)_{0}+^{}&\\ & 0,&(1- )_{0},\\ &&+.\] (2)

[MISSING_PAGE_FAIL:4]

Feature-occupancy gradient ascent for offline RL in linear MDPs

A natural idea for developing RL methods is to build an empirical approximation of the function \(f\) defined in the previous section, and use primal-dual methods to find a saddle-point of the resulting approximation. For offline RL, this approach has been explored by Gabbianelli et al. (2024) and Hong and Tewari (2024). In this work, we develop an alternative approach that seeks to directly optimize the return by approximately maximizing the unconstrained primal function \(f^{*}:^{d}\), defined for each feature-occupancy vector \(\) and policy \(\) as

\[f^{*}(,)=_{_{d}(D_{})}f( ,;),\]

for an appropriately chosen feasible set \(_{d}(D_{})\). Given the discussion in the previous section, maximizing this function with respect to \(\) and \(\) is rightly expected to result in an optimal policy (which intuition will be made formal in our analysis). Notably, the so-called objective \(f\) in Equation (7) depends on the transition weight matrix \(\) which is unknown in general. As we soon show, this matrix dominates the loss of the \(\)-player and \(\)-player. Based on these observations, our approach consists of building a well-chosen estimator \(\) of \(f\), and then maximizing the associated primal function \(^{*}\) defined as

\[^{*}(,)=_{_{d}(D_{})}(;,).\]

The objective \(\) is built via a least-squares estimator inspired by the classic LSTD model estimate of Bradtke and Barto (1996); Parr et al. (2008), which has been successfully used for analyzing finite-horizon linear MDPs in a variety of recent works (e.g., Jin et al., 2020; Neu and Pike-Burke, 2020). In particular, we fit an estimator \(}\) of the true matrix \(\) using samples from the dataset \(_{n}=\{(X_{i},A_{i},R_{i},X_{i}^{})\}_{i=1}^{n}\) as follows. Let \(_{i}=(X_{i},A_{i})\) denote the feature vector of \((X_{i},A_{i})\) and \(_{n}=_{n}+_{i=1}^{n}_{i} {}_{i}^{}\) the empirical feature covariance matrix. We define the regularized least squares estimate of \(\) at \(x\) as

\[}(x)=(x)^{d}}{ }_{i=1}^{n}(_{i}, (x)-_{\{x=X_{i}^{}\}})^{2}+ \|(x)\|_{2}^{2},\]

so that the estimate can be written as

\[}=_{x}}(x)_{x}^{ }=_{n}^{-1}_{i=1}^{n}_{i}_{X_{i}^{}}^{}.\] (8)

With this matrix at hand, we define \(\) as

\[(,;)=(1-)_{0},_{,}+,+}_{,}-=, +,^{}}_ {,}-,\]

where \(}_{,}(x,a)=(a|x)(1-)_{0}(x )+}(x),\) is a sample-based approximation of \(_{,}\).

For the purpose of optimization, we will employ appropriately chosen versions of mirror ascent (Nemirovski and Yudin, 1983; Beck and Teboulle, 2003) to iteratively optimize the primal variables. Denoting the iterates for each \(t=1,2,,T\) by \(_{t}\) and \(_{t}\), and defining \(_{t}=_{_{d}(D_{})}( _{t},_{t};)\), the updates are defined as follows. Using \(_{}(t)=_{_{t}}^{*}(_ {t},_{t})\) to denote the gradient of \(^{*}\) with respect to the feature occupancies, the first set of variables is updated as

\[_{t+1}=^{d}}{},_{}(t)- \|-_{t}\|_{_{n}^{-1}}^{2}-\|\|_{_{n}^{-1}}^{2}},\] (9)

where the first regularization term acts as proximal regularization (necessary for mirror-ascent-style methods), and the second one has a stabilization effect whose role will be made clear later in the analysis. The resulting update can be written in closed form, and is equivalent to a preconditioned gradient-ascent step on \(^{*}\). The policies are updated in each state-action pair \(x,a\) as

\[_{t+1}(a|x)=(a|x)e^{(x,a),_{t} }}{_{a^{}}_{t}(a^{}|x)e^{(x,a^{ }),_{t}}}=(a|x)e^{ (x,a),_{k=1}^{t}_{k}}}{_{a^{}}_{ 1}(a^{}|x)e^{(x,a^{}),_{k=1}^{t}_{k}}},\]corresponding to performing an entropy-regularized mirror ascent step in each state \(x\) (cf. Neu et al., 2017). We use the shorthand notation \(_{t+1}=_{k=1}^{t}_{k}\) to denote the resulting softmax policy, and note that it is fully specified by a \(d\)-dimensional vector that can be stored compactly. After the final iterate is computed, the algorithm picks the index \(J\) uniformly at random and outputs the policy \(_{J}\). We refer to the resulting algorithm as _Feature-Occupancy Gradient AScent_ (FOGAS), and present its detailed pseudocode featuring the explicit expressions of \(_{t}\) and \(_{t}\) as Algorithm 1.

``` Input: Learning rates \(,,\), initial points \(_{1}^{d},_{1}(D_{}),}_{0}=\), and dataset \(_{n}\). for\(t=1\)to\(T\)do  // Value-parameter update  Compute \(^{}}_{_{t},_{t}}=(1-)_ {a}_{t}(a|x_{0})(x_{0},a)+_{i=1}^{n}_{ a}_{t}(a|X_{i}^{})(X_{i}^{},a)_{i},_{n}^{-1}_{t}\) \(_{t}=_{_{d}(D_{})} ,^{}}_{_{t},_{ t}}-_{t}\) // Policy update  Update \(}_{t}=}_{t-1}+_{t}\) \(_{t+1}=(}_{t})\) // Feature-occupancy update  Compute \(}_{_{t},_{t}}=_{ n}^{-1}_{i=1}^{n}_{i}_{_{t},_{t}}(X_{i}^{})\)  Compute \(_{}(t)=+}_{_{t},_{t}}-_{t}\) \(_{t+1}=(_{t}+_{ n}_{}(t))\) endfor return\(_{J}\) with \(J(1,,T)\). ```

**Algorithm 1** Feature-Occupancy Gradient Ascent (FOGAS)

The following theorem states our main result regarding the performance of FOGAS.

**Theorem 3.1**.: _Let \(_{1}\) be the uniform policy and \(_{1}=\). Also set \(D_{}=/(1-)\), \(D_{}= TD_{}\) and \(>0\). Suppose that we run FOGAS for \(Tn A}{(1/)}\) rounds with parameters \(=R^{2}/dT\) as well as_

\[= A}{R^{2}dT}},\ \ \ =(2T/)}{(1- )^{2}n}},\ \ \ =}{2T^{2}R^{2}d^{2}T}}.\]

_Then, with probability at least \(1-\), the following bound is satisfied for any comparator policy \(^{*}\) and the associated feature-occupancy vector \(^{^{*}}=^{}^{^{*}}\):_

\[_{J}[^{^{*}}-^{_{J}}, ]=(^{^{*}} \|_{_{n}^{-1}}^{2}+1}{1-} (2T/)}{n}}),\]

_with the expectation taken with respect to the random index \(J\)._

The most important factor in the bound of Theorem 3.1 is \(\|^{*}\|_{_{n}^{-1}}^{2}\), which measures the extent to which the data \(_{n}\) covers the comparator policy \(^{*}\) in feature space. We accordingly refer to this quantity as the _feature coverage ratio_ between the policy \(^{*}\) and the data set \(_{n}\), and we discuss its relationship with other notions of data coverage in Section 5. Notably, the bound holds simultaneously for all comparator policies \(^{*}\), and thus it can be restated in an oracle-inequality form. On the same note, FOGAS does not need any prior upper bounds on the comparator norm \(\|^{*}\|_{_{n}^{-1}}^{2}\), and in particular it does not project the iterates \(_{t}\) to a bounded set. These nontrivial properties are enabled by a recently proposed stabilization trick due to Jacobsen and Cutkosky (2023) and Neu and Okolo (2024), which amounts to augmenting the standard mirror-ascent update of Equation (9) with the regularization term \(\|\|_{_{n}^{-1}}^{2}\). Without this additional regularization, the bounds would feature an additional factor of the order \(_{t=1}^{T}\|_{t}\|_{_{n}^{-1}}^ {2}\), which cannot be controlled without projecting the iterates and in any case make it impossible to prove a comparator-adaptive bound. We defer further discussion of the result to Section 5.

Analysis

This section is dedicated to proving our main result, Theorem 3.1. While we have defined \(\) as a "primal-only" algorithm above, its analysis will be most convenient if we regard it as a primal-dual algorithm with implicitly defined dual updates. In particular, we will view the updates of \(\) as a sequence of steps in a zero sum game between two teams of players: the _max players_ that control \(_{t}\) and \(_{t}\), and the _min player_ that picks \(_{t}\). The min player uses the simple _best-response_ strategy of picking \(_{t}=_{_{d}(D_{})} (,_{t})\), and the other two players perform their updates via appropriate versions mirror ascent on their respective objectives. Importantly, the updates of the \(\)-player are based on the gradients of \(^{*}\), which satisfy

\[_{}(t)=_{_{t}}^{*}(_{t},_{t})=_{_{t}}(_{ _{d}(D_{})}(_{t},_{t};)) =_{_{t}}(_{t},_{t};_{t }),\]

where the last equality follows from an application of Danskin's theorem. This property enables a major conceptual simplification that allows the interpretation of the updates as optimizing the unconstrained primal \(^{*}\) directly. We refer the interested reader to Chapter 6 of Bertsekas (1997) for more context on such use of primal-dual analysis.

More concretely, we make use of an analysis technique first developed by Neu and Okolo (2023), and further refined by Gabbianelli et al. (2024) and Hong and Tewari (2024). The core idea is to introduce the _dynamic duality gap_ defined on a sequence of iterates \(\{(_{t},_{t},_{t})\}_{t=1}^{T}\) produced by some iterative method, and a set of well-chosen _comparators_\((^{*},^{*};\{_{t}^{*}\}_{t=1}^{T})\) as

\[_{T}(^{*},^{*};\{_{t}^{*}\}_{t=1}^{ T})=_{t=1}^{T}(f(^{*},^{*};_{t })-f(_{t},_{t};_{t}^{*})).\]

Similar to Lemma 4.1 of Gabbianelli et al. (2024), we show in Lemma 4.1 below that with an appropriate choice of the comparator points, we can relate the gap to the expected suboptimality of policy \(_{J}\) where \(J(1,,T)\). We leave the proof in Appendix B.1.1.

**Lemma 4.1**.: _Suppose that \(D_{}=/(1-)\). Choose \((^{*},^{*},_{t}^{*})=(^{ }^{^{*}},^{*},^{_{t}})^{d} (D_{})_{d}(D_{})\) for \(t=1,,T\) where \(^{^{*}}\) is a valid occupancy measure induced by \(^{*}\). Then,_

\[_{J}[^{^{*}}-^{_{J}}, ]=_{T}(^{}^{^{*} },^{*},\{^{_{t}}\}_{t=1}^{T}).\]

We will show below that the dynamic duality gap can be written in terms of the _regrets_ of each player and an additional term related to the estimation error of \(\), and then proceed to provide bounds on all of these quantities. Specifically, the regrets of each player with respect to each of their respective comparators are defined as

\[_{T}(^{*}) =_{t=1}^{T}_{x}^{*}(x)_{a}(^{*}(a|x)-_{t }(a|x))q_{t}(x,a),\] \[_{T}(^{*}) =_{t=1}^{T}(^{*},_{t};_ {t})-(_{t},_{t};_{t})=_{t=1}^{T} ^{*}-_{t},+}_{_{t},_{t}}-_{t},\] \[_{T}(_{1:T}^{*}) =_{t=1}^{T}(_{t},_{t}; _{t})-(_{t},_{t};_{t}^{*})=_{t=1}^{T }_{t}-_{t}^{*},^{}}_{_{t},_{t}}-_{t}.\]

where \(^{*}=(1-)_{0}(x)+(x),^{*}\). Furthermore, we define the _gap-estimation error_ as

\[_{}}=_{t=1}^{T}^{*}, -}_{_{t},_{t}} +_{t=1}^{T}_{t},}-_{_{t}^{*},_{t}}.\] (10)

The following lemma rewrites the duality gap using the above terms.

**Lemma 4.2**.: _The dynamic duality gap satisfies_

\[_{T}(^{*},^{*},_{1:T}^{*})= _{T}(^{*})+_{T}(^{*})+_{T}(_{1:T}^{*} )+_{}}.\]The proof directly follows from a straightforward calculation similar to the proof of Lemma 4.2 of Gabbianelli et al. (2024) and Section E.1 of Hong and Tewari (2024) which is reproduced in Appendix B.1.2 for completeness. It remains to bound the regret of the players, as well as the gap-estimation error. An obstacle we need to face in the analysis is that our bound of the latter error term scale with \(_{t=1}^{T}\|_{_{n}^{-1}}\|_{ _{n}^{-1}}^{2}\), which is undesirable given our aspiration to achieve bounds that scale only with the comparator norm \(\|^{*}\|_{_{n}^{-1}}^{2}\) without requiring prior upper bounds on this quantity (that would enable us to project the iterates to a bounded domain). This challenge is addressed by making use of the stabilization technique of Jacobsen and Cutkosky (2023) and Neu and Okolo (2024) in the updates for the \(\)-player, which effectively eliminates these problematic terms. We briefly outline the remaining parts of the analysis below.

### Regret analysis

The regrets of each player are respectively controlled by the following three lemmas.

**Lemma 4.3**.: _Suppose that \(^{*}_{}\). Let \(_{1}\) be the uniform policy which selects all actions with equal probability in each state. Under the conditions on the feature map in Definition 2.1, the regret of the \(\)-player against \(^{*}\) satisfies \(_{T}(^{*})+ D_{}^{2}}{2}\)._

The proof is a standard application of the analysis of exponential-weight updates, stated as Lemma E.1.

**Lemma 4.4**.: _Let \(_{1}=\) and \(C=6(d+D_{}^{2})+3d(1+RD_{})^ {2}+3^{2}dR^{2}D_{}^{2}\). Then, the regret of the \(\)-player against any comparator \(^{*}^{d}\) satisfies_

\[_{T}(^{*})(+)\|^{*}\|_{ _{n}^{-1}}^{2}+-_{t=1}^{T}\|_{t}\|_{_{n}^{-1}}^{2}.\]

The proof (provided in Appendix B.2.2) follows from applying the standard analysis of composite-objective mirror descent due to Duchi et al. (2010) (stated as Lemma C.1 in the Appendix) and the bound \(\|_{n}_{}(t)\|_{_{n}^{-1 }}^{2} C\) on the weighted norm of the gradients for all \(t\) provided in Lemma C.2.

**Lemma 4.5**.: _Let \(D_{}=/(1-)\). The regret of the \(\)-player satisfies \(_{T}(_{1:T}^{*}) 0\)._

As we show in Appendix B.2.3, the above statement holds trivially thanks to the "best-response" definition of \(_{t}\). This concludes our regret analysis.

### Bounding the gap-estimation error

The following statement (proved in Appendix B.3) provides a bound on \(_{}}\):

**Lemma 4.6**.: _Suppose that \(\|(x,a)\|_{2} R\) for all \((x,a)\), \(D_{}=/(1-)\) and \(= A/R^{2}dT}\) to optimize \(_{T}(^{*})\). Then, for any \(T A}{(1/)}\) and and \( 0\), the following holds with probability at least \(1-\):_

\[_{}}(\|^ {*}\|_{_{n}^{-1}}^{2}+_{t=1}^{T}\|_{t}\|_{_{n}^{-1}}^{2})+T^{2}((2T/)}{n(1-)^{2}}).\]

### The proof of Theorem 3.1

The proof follows from applying Lemmas 4.1 and Lemma 4.2 when \((^{*},^{*},_{t}^{*})=(^{ }^{^{*}},^{*},^{_{t}}) ^{d}(D_{})_{d}(D_{})\) for \(t=1,,T\). Then, adding up the bounds stated in Lemmas 4.3-4.6 under the respective conditions, yields

\[_{J}[^{^{*}}-^{ _{J}},] }}+(++ )\|^{*^{*}}\|_{_{n}^{-1}}^{2}+\] \[+(-)_{ t=1}^{T}\|_{t}\|_{_{n}^{-1}}^{2}+ T( (2T/)}{n(1-)^{2}}).\]

Then, setting \(=\) simplifies the second term and eliminates the third term. The claim then follows after optimizing the hyperparameters, with the full details provided in Appendix B.4.

Discussion

We discuss various aspects of our results below.

Relation with previous work.As discussed in the introduction, our work draws heavily on previous contributions of Gabbianelli et al. (2024) and Hong and Tewari (2024). In particular, our idea of building a least-squares estimator of the transition function is directly borrowed from the latter of these works, and our implicit update rule for \(_{t}\) is also inspired by their work to a good extent. Their approach, however, failed to reach the same degree of efficiency due to a number of suboptimal design choices. First, they used an alternative parametrization of the feature occupancies which only allowed them to work under a more restrictive coverage condition, so that their bounds depend on \(^{*}_{_{n}^{-2}}\) which can be much larger than the feature coverage ratio appearing in our bounds. Second, their algorithm required a prior upper bound on this coverage parameter, with the guarantees scaling with the bound rather than the actual coverage. Such bounds are typically difficult to obtain in practice. Third, the implementation of their algorithm required intricate computational steps necessitated by their feature-occupancy parametrization. Our work has successfully removed these limitations and reduced the complexity of their method, thanks to a new primal-only analysis style that we hope will find further uses in reinforcement learning.

Computational and statistical efficiency.As can be inferred from our main result, the sample complexity of finding an \(\)-optimal policy using our algorithm is of the order \(d^{2}^{*}_{_{n}^{-1}}^{2}/ ^{2}(1-)^{2}\), which is optimal in terms of scaling with \(\). The rate can be improved to scale linearly with the feature coverage ratio \(^{*}_{_{n}^{-1}}\), if a tight upper bound is known on it which can be used for hyperparameter tuning. We find this scenario to be unlikely, and are curious to see if future work can attain this improved scaling without such prior knowledge. As for computational complexity, we point out that the cost of each iteration of our method scales linearly with the sample size \(n\), due to having to compute the matrix-vector products \(}_{_{t},_{t}}\). Indeed, the matrix \(}\) is sparse with \(n\) non-zero rows, and as such computing this product takes linear time in \(n\). Since the iteration complexity of \(\) scales linearly with the sample size \(n\), this makes for an overall runtime complexity of order \(n^{2}\). This limitation is of course shared with all methods using the same least-squares transition estimator for the transition model, including all work that builds on Jin et al. (2020), but we nevertheless wonder if a substantial improvement is possible on this front.

Data coverage assumptions.The only works we are aware of that scale with the feature-coverage ratio \(^{*}_{_{n}^{-1}}\) are due to Zanette et al. (2021) and Gabbianelli et al. (2024). The latter work only achieves this bound under the assumption that the data is drawn i.i.d. from a fixed behavior policy with known feature covariance matrix, which is a much more restricted setting that we consider here. Such assumptions are not needed by Zanette et al. (2021), however their results are restricted to the simpler finite-horizon MDP setting, and their algorithm is arguably more complex than ours. Using our notation, their approach can be interpreted as solving a "pessimistic" version of the the relaxed dual LP (4) that features some additional quadratic constraints. This approach is not computationally viable for the infinite-horizon discounted case we consider, as it requires solving a fixed-point equation with respect to the estimated transition operator (cf. Wei et al., 2021).

Possible extensions.Our approach can be extended and generalized in a variety of ways. First, following Gabbianelli et al. (2024), we believe that it is straightforward to extend our analysis to undiscounted infinite-horizon MDPs. Second, we similarly believe that an extension to constrained MDPs is possible without major challenges, following Hong and Tewari (2024). We did not pursue these extensions because we believe that they add little additional insight. There are other potential directions that we did not explore because we found them to be too ambitious for the moment. These include extending our results beyond linear MDPs to other MDP models with linear function approximation, including MDPs with low inherent Bellman rank (which may be within reach of the current theory, c.f. Zanette et al., 2020), linearly \(Q^{}\)-realizable MDPs (which are known to be challenging, c.f. Weisz et al., 2022, 2024). Even more ambitiously, one can ask if it is possible to extend our methods to work under more general notions of function approximation. This looks very challenging given the central role of feature occupancies in our formalism, which are strictly tied to linear function approximation. We are nevertheless optimistic that the ideas presented in this work will find use in other contexts, possibly including nonlinear function approximation in the future.