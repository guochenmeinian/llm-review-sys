# Initializing Variable-sized Vision Transformers

from Learngene with Learnable Transformation

 Shiyu Xia,  Yuankun Zu,  Xu Yang,  Xin Geng

School of Computer Science and Engineering, Southeast University, Nanjing 210096, China

Key Laboratory of New Generation Artificial Intelligence Technology and Its

Interdisciplinary Applications (Southeast University), Ministry of Education, China

{shiyu_xia,zyk0418, xuyang_palm, xgeng}@seu.edu.cn

Co-corresponding author.

###### Abstract

In practical scenarios, it is necessary to build variable-sized models to accommodate diverse resource constraints, where weight initialization serves as a crucial step preceding training. The recently introduced Learngene framework firstly learns one compact module, termed **learngene**, from a large well-trained model, and then transforms learngene to initialize variable-sized models. However, the existing Learngene methods provide limited guidance on transforming learningen, where transformation mechanisms are manually designed and generally lack a learnable component. Moreover, these methods only consider transforming learngene along depth dimension, thus constraining the flexibility of learngene. Motivated by these concerns, we propose a novel and effective Learngene approach termed **LeTs** (_Learnable **T**ransformation_), where we transform the learngene module along both width and depth dimension with a set of learnable matrices for flexible variable-sized model initialization. Specifically, we construct an auxiliary model comprising the compact learngene module and learnable transformation matrices, enabling both components to be trained. To meet the varying size requirements of target models, we select specific parameters from well-trained transformation matrices to adaptively transform the learngene, guided by strategies such as continuous selection and magnitude-wise selection. Extensive experiments on ImageNet-1K demonstrate that Des-Nets initialized via LeTs outperform those with 100-epoch from scratch training after only **1 epoch** tuning. When transferring to downstream image classification tasks, LeTs achieves better results while outperforming from scratch training after about **10 epochs** within a 300-epoch training schedule.

## 1 Introduction

Vision Transformer (ViT) models have gained widespread attention due to their impressive performance on diverse vision tasks [1; 2; 3; 4; 5; 6]. In practice, models of _various sizes_ are often deployed and trained under diverse resource constraints, ranging from edge devices with limited computational resources to computing clusters with sufficient resources, which may exhibit considerable diversity. Obviously, we could train each target model from scratch for specific tasks across different resource settings. However, such method underestimates the importance of weight initialization, which could significantly affect the training process and the final model quality [7; 8; 9; 10; 11; 12; 13; 14]. Moreover, the training and storage costs of such method grow linearly with the number of potential scenarios. Consequently, a fundamental research question arises: _how to efficiently initialize variable-sized models while considering both the model performance and resource constraints_.

[MISSING_PAGE_FAIL:2]

With comprehensive experiments, we show the superiority of LeTs: (1) Compared to 100-epoch from scratch training (Scratch) on ImageNet-1K , Des-Nets initialized via LeTs performs better after only **1 epoch** tuning. (2) Evaluation performance on ImageNet-1K without any tuning after initialization implies that LeTs significantly enhances initialization quality, _e.g._, +\(\%\) with Des-H12-L12 (86.6M) compared to TLEG . (3) When transferring to downstream image classification tasks, LeTs presents better performance and training efficiency. For example, LeTs outperforms the pre-training and fine-tuning method (Pre-Fin) by \(\%\) on CIFAR-100 with Des-H12-L12. Furthermore, within a 300-epoch training schedule on Food-101 , LeTs outperforms the final performance of Scratch after about **10 epochs**. For semantic segmentation tasks, LeTs outperforms Pre-Fin by \(\%\) on ADE20K  with Des-H6-L12. (4) Compared to Pre-Fin, LeTs performs better while reducing around \(\) initialization parameters when initializing variable-sized models.

Our main **contributions** are summarized as follows: (1) We introduce a novel and effective Learengene approach, termed LeTs, for efficient ViT-based model initialization, which is the first to utilize learnable matrices to adaptively transform the compact learngene. (2) We propose to transform learngene along both depth and width dimension for initialization, to our knowledge, has not been explored in the Learengene literature. (3) Comprehensive experiments under various initialization settings demonstrate the efficiency of LeTs.

## 2 Related Work

**Parameter Initialization** methods have been extensively developed, such as default initialization from Timm library , Xavier initialization  and Kaiming initialization . A plenty of studies demonstrate that parameter initialization significantly affects the training process and the final model quality [7; 8; 10; 11; 12; 13; 14]. Appropriate initialization facilitates model convergence  while improper initialization may hinder the optimization process [10; 11]. Nowadays, the pre-training and fine-tuning method involves transferring model parameters pretrained on large-scale datasets for fine-tuning on specific downstream tasks, during which the model architecture is maintained [34; 35; 18; 36; 37]. However, such method requires transferring the entire pretrained model parameters repeatedly, without considering the varying resource availability of different downstream tasks. Moreover, when a pretrained model of the target size is unavailable, we may firstly pre-train the target model on large-scale datasets. This process is not only time-consuming and computationally expensive, but also requires access to the datasets used for pre-training and lacks the flexibility for initializing variable-sized models. Recently, [38; 39] have focused on transferring large pretrained model parameters to initialize small ones. Besides, Matformer  allows for training one universal model which can be used to extract many smaller sub-models. In contrast, we propose to transform one compact learngene with learnable transformation matrices for initialization.

**Learngene** is a two-stage framework [24; 25; 26; 27; 41; 42; 43; 44; 45; 46; 47] which firstly learns one compact module, termed **learngene**, from a large well-trained network called ancestry model (Ans-Net), and then transforms learngene to initialize variable-sized descendant models (Des-Net), after which they are fine-tuned normally, as shown in Fig.1(a). Grad-LG  selects a few high-level layers as learngene based on the gradient information of Ans-Net, after which they are stacked with randomly-initialized layers to build Des-Nets. TLEG  linearly expands two integral learngene layers to initialize variable-depth Des-Nets. SWS  extracts learngene in a multi-stage weight sharing fashion and duplicates learngene in its stage during initialization. WAVE  trains multiple weight templates as learngenes, enabling efficient initialization for variable-sized models via Kronecker Product. Rather than manually designing learning transformation strategies, we seek to use _learnable_ matrices, which contain structural knowledge, to transform the compact learngene for initializing variable-sized models. Furthermore, LeTs enables learngene to be transformed along both depth and width dimension, significantly enhancing the transformation flexibility.

**Network Expansion,** a prevalent training acceleration framework pioneered by , involves incrementally increasing the size of neural networks [49; 50; 51; 52; 53]. Net2Net  employs function-preserving expansions to increase the width by copying neurons and the depth by introducing identity layers. Bert2Bert  extends this concept by proposing function-preserving width expansion specifically for Transformers. Expansion  introduces a width expansion strategy for convolutional neural networks using orthogonal filters, as well as a depth expansion strategy for Transformers through the corresponding exponential moving average model. LiGO  learns to linearly map the parameters of a smaller model to initialize a larger one. While LeTs is inspired by these studies , it differs in at least two key aspects. In terms of objective, we focus on flexibly initializing _variable-sized_ models from a single compact learngene. Methodologically, such as compared to LiGO , we first transform the learngene layers along width dimension, after which we divide the width-transformed learngene layers into multiple groups and linearly combine the layers within each group to construct new layers. Additionally, one crucial aspect of LeTs is that selecting specific parameters from well-trained transformation matrices for subsequent initialization.

## 3 Proposed Approach

Fig.2 depicts the overall framework of LeTs. In the first stage, we construct an auxiliary model (Aux-Net) comprising compact learngene and a series of learnable transformation matrices. Then we train it by distilling knowledge from a well-trained ancestry model (Ans-Net). In the second stage, given the varying size requirements of target models (_e.g._, layer numbers), we select specific parameters from well-trained transformation matrices guided by strategy such as continuous selection and magnitude-wise selection. Then we use these selected parameters to transform learngene for initializing Des-Nets, which are fine-tuned lastly.

### Learnable Transformation for Learngene

We denote the parameters of the learngene module with \(L\) layers as \(^{lg}=[_{1},...,_{L}]^{}\), where \(_{l}^{d_{in} d_{out}}\), \(d_{in}\) and \(d_{out}\) denote the input and output dimension respectively. In the first stage, we design an auxiliary model (Aux-Net) whose parameters are transformed from the compact learngene parameters with a series of learnable transformation matrices \(\). We configure a set of width transformation matrices \(\) and one depth transformation matrix \(\) for composing \(\). Specifically, we enlarge the input and output dimension of learngene matrices with transformation matrices \(\) containing \(^{in}\) and \(^{out}\). Afterwards, we divide these width-transformed learngene layers into multiple groups and utilize depth transformation matrix \(\) to combine the layers within each group to construct new layers. In the following, we detail the width transformation matrices \(\), depth transformation matrix \(\), the construction and training of Aux-Net.

**Width transformation matrices.** For each learngene layer \(_{l}\), we introduce \(_{l}^{in}\) and \(_{l}^{out}\) to perform in-dimension and out-dimension transformation respectively. In particular, we perform in-dimension transformation on \(_{l}\) by multiplying \(_{l}^{in}\), after which we insert the transformed part into the original learngene via

\[_{l}^{{}^{}}=Concat(_{l},_{l}^{in}_{l}),\] (1)

where \(_{l}^{{}^{}}\) represents the in-dimension transformed learngene and \(Concat\) represents the concatenation operation. Similarly, we perform out-dimension transformation on \(_{l}^{{}^{}}\) by multiplying \(_{l}^{out}\)

Figure 2: In stage 1, we construct and train an Aux-Net which is transformed from compact learngene layers using a series of learnable transformation matrices. During training, \(\) and \(\) learn to capture structural knowledge about how to add new neurons and layers into the compact learngene respectively. In stage 2, given the varying sizes of target Des-Nets, we select specific parameters from well-trained transformation matrices to transform learngene for initialization, which are fine-tuned lastly under different downstream scenarios.

after which we insert the transformed part into \(_{l}^{{}^{}}\) via

\[_{l}^{wt}=Concat(_{l}^{{}^{}},_{l}^{{}^{}}_{l }^{out}),\] (2)

where \(_{l}^{wt}\) represents the width-transformed leargence. Width transformation process is illustrated in Fig.3(a). We could also first perform out-dimension transformation followed by in-dimension transformation. Besides, we explore performing in-dimension and out-dimension transformation on \(_{l}\) by directly multiplying \(_{l}^{in}\) and \(_{l}^{out}\) via \(_{l}^{wt}=_{l}^{in}_{l}_{l}^{out}\), referred to as LeTs(DE). Empirically, we first perform in-dimension and then out-dimension transformation in line with . To keep the parameter efficiency, we share transformation matrices for different model components.

**Depth transformation matrix.** After the width transformation, we divide these width-transformed leargence layers into \(M\) groups and set the number of width-transformed leargence layers in the \(j\)-th group as \(L_{j}\) where \(j=1,...,M\). We denote the parameter matrices of the \(k\)-th width-transformed leargence layer in the \(j\)-th group as \(_{j,k}^{wt}\) where \(k=1,...,L_{j}\). Then we utilize matrix \(\) to combine the layers within the \(j_{0}\)-th group to construct the \(i\)-th layer of the target model via

\[_{i}^{dt}=_{k=1}^{L_{j_{0}}}G_{i,k}_{j_{0},k}^{wt},\] (3)

where \(_{i}^{dt}\) represents the \(i\)-th depth-transformed target layer and \(G_{i,k}\) represents the \((i,k)\)-th entry. During the new layer constructions, we repeatedly select some learngene groups, as discussed in Sec.4.3. The process of depth transformation is illustrated in Fig.3(b). Moreover, we adopt parameter sharing strategy between rows of \(\) to provide guidance for initializing descendant models.

**Construction and Training of Aux-Net.** We construct the Aux-Net from the learngene module using width and depth transformation matrices. Then we train the Aux-Net via prediction-based distillation  for simplicity to distill knowledge from the Ans-Net. This involves minimizing the cross-entropy loss between the probability distributions of output predictions from both the Ans-Net and the Aux-Net similar to previous studies [26; 27]. Specifically, we introduce one distillation loss \(_{distill}=CE((_{s}/),(_{t}/))\), where \(_{s}\) and \(_{t}\) represent the logits of the Aux-Net and those of the pretrained Ans-Net respectively, \(CE(,)\) represents soft cross-entropy loss, \(\) represents the temperature for distillation and \(\) represents the softmax function. Additionally, we can seamlessly integrate advanced distillation techniques [56; 57] into our training. Besides, we also introduce one classification loss \(_{cls}=CE((_{s}),y)\), where \(y\) represents the ground-truth label. Overall, our training loss is defined as

\[_{all}=(1-)_{cls}+_{distill},\] (4)

where \(\) represents the trade-off coefficient. Noteworthy, we train the Aux-Net to obtain well-trained learngene parameters and a set of transformation matrices. During training, the width transformation matrices and depth transformation matrix capture structural knowledge about how to add new neurons and layers respectively, preparing for the subsequent initialization. Next, we elaborate how to use these well-trained transformation matrices to adapt learngene for initializing variable-sized models.

Figure 3: (a) Firstly, we perform in-dimension transformation on \(_{l}\) by multiplying \(_{l}^{in}\), after which we insert the transformed part into the original learngene. Afterwards, we perform out-dimension transformation on \(_{l}^{{}^{}}\) similarly. (b) After width transformation, we divide these width-transformed leargence layers into groups. Take the \(j_{0}\)-th group with two width-transformed leargence layers (\(_{j_{0},1}^{wt}\) and \(_{j_{0},2}^{wt}\)) as an example, we construct \(_{1}^{dt}\) via \(G_{1,1}_{j_{0},1}^{wt}+G_{1,2}_{j_{0},2}^{wt}\). For simplicity, we omit the original subscripts and superscripts for the entry of all matrices.

### Initialization from Learngene with Well-trained Transformation Matrices

Different from previous manual and depth-only transformation, LeTs enables learngene to be deepened and widened with well-trained transformation matrices. To meet the diverse size requirements of Des-Nets (_e.g._, layer numbers), we select specific parameters from well-trained transformation matrices to construct target ones. Specifically, we design several parameter selection strategies including continuous selection, magnitude-wise selection and sequential selection. For width transformation matrices across different model components, we maintain the consistency of selection position to preserve the integrity of the learned neuron connections. After initialization, we fine-tune the descendant models on different downstream tasks _without_ distillation. Next, we firstly introduce the selection strategy for width transformation matrices.

**Continuous selection.** For the initialization along width, we propose selecting continuous rows from \(^{in}\) and \(^{out}\) to form the target transformation matrices. Notably, consistency is preserved throughout the row selection process for different model components to maintain connectivity between neurons. Empirically, we default to selecting the first \(n\) rows from \(^{in}\) and \(^{out}\).

**Magnitude-wise selection.** Parameter magnitude serves as an effective metric for assessing importance in model pruning, where the significance of one weight is determined by its magnitude [58; 59; 60]. In our case, we adapt this metric for parameter selection from width transformation matrices. Take \(^{in}\) as an example, we propose selecting \(n\) rows whose \(L_{1}\)-norm or \(L_{2}\)-norm ranks top-\(n\), abbreviated as top-\(n\)\(L_{1}\)/\(L_{2}\)-norm selection. Intuitively, this selection strategy enables us to prioritize parameters that contribute most significantly to the model performance, ensuring that the most impactful connections are preserved. By concentrating on the most critical parameters, LeTs enhances the initialization quality of different Des-Nets. Empirically, we demonstrate its effectiveness by comparing it with bottom-\(n\)\(L_{1}\)/\(L_{2}\)-norm selection which select \(n\) rows whose \(L_{1}\)-norm or \(L_{2}\)-norm ranks bottom-\(n\), as discussed in Sec.4.3.

For the depth transformation matrix, we introduce the sequential selection strategy.

**Sequential selection.** We propose to sequentially select learngene groups in a predefined order, where different orders emphasize distinct groups. Each learngene group corresponds to several coefficient groups (rows of \(\)). While selecting an approximately equal number of coefficient groups for each learngene group, we allocate more coefficient groups for shallower learngene group.

## 4 Experiments

### Experimental Settings

We perform our main experiments on ImageNet-1K , several downstream image classification datasets including CIFAR-10, CIFAR-100 , Food-101  and Cars-196 , and several semantic segmentation datasets including ADE20K , Pascal Context  and Cityscapes . We report Top-1 classification accuracy (Top-1(%)) for classification tasks, and Intersection over Union (mIoU(%)) averaged over all classes for segmentation tasks following . Besides, we also report Params(M) and FLOPs(G) as the number of model parameters and indicators of theoretical complexity of model. In the first stage, we configure the learngene module with \(6\) heads (head dimension is \(64\)) and \(8\) layers whose number of parameters is 15M, and transform it to construct Aux-Net with \(12\) heads and \(16\) layers based on DeiT . Then we train the Aux-Net on ImageNet-1K for 300 epochs to obtain learngene and transformation matrices. We choose Levit-384  as the ancestry model. In the second stage, we set several variants of Des-Net where we change the layer and head numbers based on DeiT, _e.g._, we name the Des-Net with \(12\) heads and \(12\) layers as Des-H12-L12. Please see more details in A.3.

### Main Results

**Compared to training from scratch on ImageNet-1K, LeTs performs better while reducing large training costs.** We compare LeTs with: (1) Scratch that training models of variable widths and depths from scratch for 100 epochs; (2) TLEG  that linearly expands learngene to initialize models and finetunes them for 40 epochs; (3) SWS  that duplicates learngene in its pre-defined stage to initialize models and finetunes them for 10 epochs. As shown in Fig.4 and Tab.5, compared to Scratch, LeTs performs better only after **1 epoch** tuning, which reduces around \(\) total trainingcosts for 24 models. Compared to TLEG and SWS, LeTs also demonstrates superior performance after just **1 epoch** tuning in most cases, which highlights the effectiveness of introducing learnable transformation matrices. Notably, the efficiency of LeTs becomes increasingly obvious as the number of Des-Nets grows, since we only need to train the learngene and transformation matrices _once_ for most Des-Nets. Moreover, LeTs could flexibly initialize _variable-sized_ models that are independent of the size of learngene and Aux-Net, as shown in Fig.5. Please see more detailed results in Tab.5.

**LeTs exhibits better performance and training efficiency when transferring to downstream datasets.** We compare LeTs with: (1) Pre-Fin that pre-training on ImageNet-1K and fine-tuning on downstream datasets; (2) Scratch; (3) Grad-LG ; (4) TLEG ; (5) SWS . As shown in Fig.6(a)-(h), LeTs consistently outperforms these baselines on downstream image classification datasets, demonstrating the effectiveness of leveraging well-trained transformation matrices for model initialization. Take Des-H6-L12 as an example, LeTs outperforms Pre-Fin by \(\%,\%\) and \(\%\) on CIFAR-100, Food-101 and Cars-196, respectively. Notably from Fig.7, we observe that LeTs outperforms the final performance of Scratch after about **10 epochs** within a 300-epoch training

Figure 4: Performance comparisons on ImageNet-1K. Number in bracket represents Params(M).

schedule on Food-101 , which is about \(\) faster. We also report the results of LeTs under the linear probing protocol in Fig.6(l). Moreover, we report the results of LeTs on downstream semantic segmentation tasks. Specifically, we follow the training and model setting provided in , where we adopt Des-H6-L12 and Des-H12-L12 as the backbone and mask transformer as the decoder. As shown in Fig.6(i)-(k), LeTs((ep) outperforms Pre-Fin by \(,\) and \(\) on ADE20K, Pascal Context and Cityscapes respectively with Des-H6-L12 as the backbone. Please see more details in A.3.

**Results of evaluation on ImageNet-1K without any tuning after initialization implies that LeTs greatly enhances initialization quality.** To validate the initialization quality, we compare LeTs with (1) Scratch; (2) Grad-LG ; (3) TLEG ; (4) SWS ; (5) IMwLM . In Tab.1, LeTs outperforms all baselines by a large margin in most cases. For example, LeTs outperforms TLEG

Figure 5: LeTs could flexibly initialize _variable-sized_ models that are independent of the size of learngene and Aux-Net. Compared with Scratch and MatFormer , LeTs demonstrates more initialization efficiency.

Figure 6: Performance of (a)-(d): Des-H6-L12 and (e)-(h): Des-H12-L12 on downstream image classification datasets. We report the results of LeTs under the linear probing (LP) protocol in (l). Besides, we evaluate LeTs on semantic segmentation tasks in (i)-(k), where we set two variants of LeTs as LeTs(0ep) and LeTs(5ep). LeTs(0ep) represents that initializing the backbone with learngene and LeTs(5ep) represents that further fine-tuning on ImageNet-1K for 5 epochs after initialization.

by \(,\) and \(\) on Des-H12-L8, Des-H12-L10 and Des-H12-L12. Importantly, LeTs can also achieve comparable performance with _well-trained_ models (Scratch). For instance, LeTs outperforms Scratch by \(\) and \(\) on Des-H12-L10 and Des-H12-L12. The above results imply that LeTs provides effective initialization for variable-sized models.

**Compared to Pre-Fin, LeTs significantly reduces the pretraining efforts and initialization parameters when initializing variable-sized models for downstream tasks.** As shown in Tab.2, LeTs demonstrates superior performance while reducing around \(\) pre-training costs and decreasing around \(\) (758M _vs._ 37.7M) initialization parameters, as compared to Pre-Fin. Furthermore, LeTs only needs to train learngene and transformation matrices _once_, whereas Pre-Fin requires individual pre-training for each Des-Net. Clearly, the efficiency gains of LeTs become more pronounced with an increasing number of Des-Nets for different downstream tasks.

### Ablation and Analysis

**Selection strategies.** We evaluate the performance of LeTs (Des-H8-L12, 5 epochs tuning) using various selection strategies. As shown in Tab.3, we observe that continuous selection achieves slight better performance than magnitude-wise selection. Moreover, selecting \(n\) rows whose \(L_{1}\)-norm or \(L_{2}\)-norm ranks top-\(n\) (top-\(n\)\(L_{1}/L_{2}\)-norm) is better than those whose \(L_{1}\)-norm or \(L_{2}\)-norm ranks bottom-\(n\) (bottom-\(n\)\(L_{1}/L_{2}\)-norm). Besides, we observe that selecting different learngene groups for initialization achieves similar performance, where selecting more coefficient groups for shallower learngene groups performs better than else.

**Width and depth transformation.** We investigate the effectiveness of our proposed width and depth transformation by comparing LeTs with one _state-of-the-art_ expansion method LiGO . Specifically, we adopt the expansion strategy proposed in LiGO into our two-stage initialization pipeline and use our proposed selection strategy for initializing different Des-Nets, referred to as LeTs (LiGO). From Tab.4, we observe that LeTs outperforms LeTs (LiGO) in most cases, which indicates that our proposed transformation pipeline are more suitable for initializing variable-sized models. Moreover, we explore transforming the learngene matrices directly but not insert the transformed

   Model & Params(M) & FLOPs(G) & Scratch & IMwLM & Grad-LG & TLEG & SWS & LeTs \\  Des-H12-L8 & 58.2 & 11.7 & 77.2 & 9.5 & 0.1 & 69.8 & **74.4** & 74.1 \\ Des-H12-L9 & 65.3 & 13.1 & 78.0 & 16.8 & 0.1 & 73.8 & 76.5 & **77.6** \\ Des-H12-L10 & 72.4 & 14.6 & 78.2 & 25.1 & 0.1 & 75.7 & 78.0 & **78.7** \\ Des-H12-L11 & 79.5 & 16.0 & 79.0 & 36.1 & 0.1 & 76.4 & 78.9 & **79.5** \\ Des-H12-L12 & 86.6 & 17.5 & 79.6 & 48.1 & 0.1 & 76.6 & 79.3 & **80.1** \\ Des-H12-L13 & 93.7 & 18.9 & 80.3 & 59.0 & 0.1 & 76.7 & 80.0 & **80.5** \\ Des-H12-L14 & 100.7 & 20.4 & 80.4 & 68.3 & 0.1 & 76.5 & 80.1 & **80.8** \\ Des-H12-L15 & 107.8 & 21.8 & 80.3 & 75.2 & 0.1 & 76.0 & 80.5 & **81.1** \\ Des-H12-L16 & 114.9 & 23.3 & 80.4 & 78.7 & 0.1 & 75.5 & 80.7 & **81.5** \\   

Table 1: Direct evaluation performance on ImageNet-1K without any tuning after initialization.

    & Params & Pre-Fin & LeTs &  & Params &  & LeTs \\  & (M) & S-P(M) Top-1(\%) & Top-1(\%) & Top-1(\%) & & (M) & S-P(M) & Top-1(\%) & Top-1(\%) \\  Des-H8-L8 & 25.8 & 25.7 & 87.5 & **88.5** & Des-H12-L8 & 57.5 & 57.4 & 88.4 & **89.3** \\ Des-H8-L10 & 32.1 & 32.0 & 88.2 & **89.2** & Des-H12-L10 & 71.7 & 71.6 & 88.0 & **89.7** \\ Des-H8-L12 & 38.4 & 38.3 & 88.6 & **89.9** & Des-H10-L16 & 79.5 & 79.4 & 88.2 & **89.6** \\ Des-H8-L14 & 44.7 & 44.6 & 88.4 & **89.8** & Des-H12-L12 & 85.9 & 85.8 & 88.5 & **90.0** \\ Des-H8-L16 & 51.0 & 50.9 & 89.1 & **90.1** & Des-H12-L14 & 100.1 & 100.0 & 88.2 & **90.0** \\ Des-H10-L12 & 59.8 & 59.7 & 88.6 & **90.0** & Des-H12-L16 & 114.2 & 114.1 & 88.2 & **90.1** \\   

Table 2: Performance comparisons on CIFAR-100 of variable-sized Des-Nets. Pre-Fin transfers the pretrained parameters (S-P(M)) to initialize, which totally requires about 758M for 12 Des-Nets. LeTs only needs to store 37.7M parameters (15.0M learngene) to initialize, which significantly reduces the parameters stored for initialization by 20\(\) (758M _vs._ 37.7M).

matrices into the learngene, referred to as (LeTs(DE)). In addition, we also explore the role of the parameter sharing strategy adopted on the rows of \(\) (LeTs w/o ws). From Tab.4, we observe that LeTs(DE) and LeTs(w/o ws) still enhances the initialization efficiency but is inferior to our complete version.

**Initialization components.** We investigate the performance of LeTs (Des-H8-L12, 5 epochs tuning) by excluding one of the following components within ViT models: Patch Embedding (PE), Multi-head Self-Attention (MSA), Multi-Layer Perception (MLP), Layer Normalization (LN) or Position Embedding (Pos). From Tab.3, we find that omitting MSA or MLP from initialization results in significant performance degradation and initializing all components is necessary.

**Size and training setting of learngene and transformation matrices.** We evaluate the performance of Des-Nets initialized from well-trained learngene modules under different training settings. Specifically, we train a smaller learngene module with 11.4M parameters (LeTs(11.4M)), shorten the training epochs of Aux-Net to 200 (LeTs(200ep)) or train the Aux-Net without distilling from the Ans-Net (LeTs(w/o dis)). From Tab.4, we observe that LeTs(11.4M), LeTs(200ep) and LeTs(w/o dis) still enhances the initialization effectiveness but is inferior to our complete version.

## 5 Conclusion

In this paper, we proposed a well-motivated and highly effective Learngene approach termed LeTs where we transform the learngene module with a set of learnable transformation matrices for variable-sized model initialization, enabling adaptation to diverse resource constraints. LeTs is the first to adopt learnable matrices to transform the compact learngene along both the depth and width dimension, which significantly enhances the flexibility of learngene for model initialization. We demonstrated the efficiency of LeTs under various initialization settings empirically.

    &  LeTs \\ (LiGO) \\  } &  LeTs \\ (11.4M) \\  } &  Model \\ LeTs \\  } &  LeTs \\ (LiGO) \\  } &  Model \\ LeTs \\  } &  Model \\ (w/o ws) \\  } &  LeTs \\ (w/o dis) \\  } &  Model \\ (w/o dis) \\  } &  LeTs \\ (w/o dis) \\  } &  LeTs \\ (w/o dis) \\  } &  LeTs \\ (w/o dis) \\  } \\  Des-H6-L12 & 78.9 & 78.3 & **79.5** & Des-H8-L8 & 77.5 & **78.1** & Des-H12-L10 & 78.0 & **80.1** \\ Des-H8-L12 & 79.3 & 79.1 & **80.1** & Des-H8-L10 & 78.4 & **79.3** & Des-H12-L12 & 78.4 & **80.8** \\ Des-H10-L12 & 79.5 & 79.8 & **80.4** & Des-H10-L10 & 79.1 & **79.9** & Des-H12-L14 & 79.1 & **81.1** \\ Des-H12-L12 & 79.8 & 80.2 & **80.8** & Des-H12-L10 & 79.6 & **80.1** & Des-H12-L16 & 79.3 & **81.6** \\   

Table 4: Performance on ImageNet-1K when using depth and width expansion strategies proposed in LiGO  (named as LeTs(LiGO)), training smaller learngene module (named as LeTs(11.4M)), direct transforming learngene matrices to compose target ones (named as LeTs(DE)), not sharing weights between rows of \(\) (named as LeTs(w/o ws)), training Aux-Net for 200 epochs (named as LeTs(200ep)) and not adopting distillation in the first stage (LeTs(w/o dis)).

    &  Learning \\ (M) \\  } &  Top-1 \\ Strategy \\  } &  Learngene \\ (\%) \\  } &  Top-1 \\ Group Selection \\  } &  Initialization \\ (\%) \\  } &  Top-1 \\ Component \\  } &  Initialization \\ (\%) \\  } \\   &  & continuous & **80.1** & 1,1,1,1,2,2,2,2,3,3,4,4 & **80.1** & w/o MSA & 71.7 \\  & & top-\(n\)\(L_{2}\)-norm & 79.3 & 1,1,2,2,2,2,3,3,3,3,4,4 & 79.6 & w/o MLP & 54.3 \\  & & bottom-\(n\)\(L_{2}\)-norm & 78.8 & 1,1,1,1,2,2,3,3,3,3,4,4 & 79.9 & w/o LN & 79.3 \\  & & top-\(n\)\(L_{1}\)-norm & 79.4 & 1,1,1,2,2,3,3,4,4,4 & 79.9 & w/o PE & 78.5 \\  & & bottom-\(n\)\(L_{1}\)-norm & 78.7 & 1,1,2,2,2,2,3,3,4,4,4,4 & 79.6 & w/o Pos & 76.6 \\   

Table 3: Performance on ImageNet-1K when using different selection strategies, selecting different learngene groups and initializing Des-Nets without certain components.