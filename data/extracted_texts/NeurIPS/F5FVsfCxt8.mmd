# Decision Tree for Locally Private Estimation

with Public Data

 Yuheng Ma\({}^{1}\) Han Zhang\({}^{1}\) Yuchao Cai\({}^{2}\) Hanfang Yang\({}^{13}\)

\({}^{1}\)School of Statistics, Renmin University of China

\({}^{2}\)Faculty of Electrical Engineering, Mathematics and Computer Science, University of Twente

\({}^{3}\)Center for Applied Statistics, Renmin University of China

{yma,hanzhang0816,hyang}@ruc.edu.cn

y.cai@utwente.nl

Corresponding author.

###### Abstract

We propose conducting locally differentially private (LDP) estimation with the aid of a small amount of public data to enhance the performance of private estimation. Specifically, we introduce an efficient algorithm called _Locally differentially Private Decision Tree_ (LPDT) for LDP regression. We first use the public data to grow a decision tree partition and then fit an estimator according to the partition privately. From a theoretical perspective, we show that LPDT is \(\)-LDP and has a minimax optimal convergence rate under a mild assumption of similarity between public and private data, whereas the lower bound of the convergence rate of LPDT without public data is strictly slower, which implies that the public data helps to improve the convergence rates of LDP estimation. We conduct experiments on both synthetic and real-world data to demonstrate the superior performance of LPDT compared with other state-of-the-art LDP regression methods. Moreover, we show that LPDT remains effective despite considerable disparities between public and private data.

## 1 Introduction

Differential privacy (DP)  is a widely-used technique to protect sensitive information, like in medical trials , recommendation systems , and census data sharing . Local differential privacy (LDP) , a variation of DP, has gained particular attention, especially among industry experts . Unlike DP, LDP assumes data is privatized before being sent to a central collector. However, LDP models need more data to be accurate compared to DP , and many common techniques in data analysis such as standardization  and tree partition  are harder with LDP. This brings challenges to tasks such as density density estimation , mean estimation , and Gaussian estimation .

Fortunately, in some scenarios, private estimation performance can be enhanced with an additional public dataset . The public dataset can be either in-distribution, consisting of data from users who agree to share their personal information , or out-of-distribution, such as data from another source . From a central DP perspective, an increasing amount of research has focused on leveraging public data to facilitate private learning, where public data mainly serves two purposes. On one hand, the knowledge learned from public data is implicitly transferred into the private model. Empirical investigations have demonstrated the effectiveness of pretraining on public data and fine-tuning privately on sensitive data . By gradient pre-conditioning with a subspace computed by public data,  managed to reduce the required amount of noise in differentially private gradient descent and accelerate its convergence. Through unlabeled public data,  fed knowledge privately into student models. On the other hand, on public data, we canconduct procedures that would be infeasible without access to the raw private data. For example,  used parameters computed by public data to standardize private data, which can augment the sample complexity of private mean estimation. Recently,  employed unlabeled public data to estimate the leading eigenvalue of the covariance matrix, resulting in an improved sample complexity for generalized linear models with non-interactive local differential privacy.

The paper focuses on the problem of non-parametric regression with LDP. While regression has been extensively studied in the central setting [50; 1; 14], the LDP case remains rarely explored. A notable reason is that most gradient-based methods [50; 1] are prohibited. In order to protect privacy, each data holder needs to compute the gradient of parameters locally, which requires a large amount of memory, computing power, and communication capacity on the terminal machine .  proposed to impose Laplace noise on the data directly to provide privacy. However, this method is known to converge slowly  and suffer from the curse of dimensionality. More recently, [9; 33] investigated histogram-based approaches. Though theoretically optimal, histograms may perform poorly in practice, especially when the dimension of feature space is large. Thus, both methods proposed in [9; 33] face challenges when applied to real-world problems.

Under such background, using the idea of borrowing public data information, we propose an LDP non-parametric regression algorithm called the _Locally differentially Private Decision Tree_ (LPDT) that achieves both optimal convergence rate and superior empirical performance. We first create a tree partition on the public dataset using the proposed _max-edge_ rule. According to the partition, each data holder encodes the private data and releases the encoding which is processed using the proposed privacy mechanism. Finally, the curator aggregates the information in each partition cell and outputs a decision tree estimator. LPDT is advantageous from at least two perspectives: (i) LPDT integrates both benefits to leverage public data. It enables adaptive partitioning procedures which can eliminate some redundant cells and can transfer information from public data to private estimation through the tree partition. (ii) It inherits the merit of the decision tree model, such as interpretability, efficiency, stability, extensiveness to multiple feature types, and resistance to the curse of dimensionality.

We summarize our contributions. (i) For the first time, we propose to use public data in locally differentially private non-parametric regression. (ii) We propose a novel LDP regression algorithm called the locally differentially private decision tree that achieves theoretical optimality while maintaining satisfying practical performance. (iii) Under mild assumptions on the similarity between the distribution of public and private data, we establish the optimal convergence rate of LPDT, whereas the supremum of excess risk of LPDT without public fails to converge to zero. This demonstrates the theoretical advantage of incorporating public data. (iv) In experiments, we compare LPDT with other existing non-parametric LDP regression methods using both synthetic and real-world datasets. Our results demonstrate the overwhelming performance of LPDT, which illustrates the empirical improvement brought by public data. Moreover, we show that LPDT performs well even in the presence of significant disparities between public and private data.

## 2 Methodology

This section is dedicated to the methodology of LPDT. In Section 2.1, we first present notations and preliminaries related to regression problems, followed by a recap of the definition of local differential privacy. Next, we introduce our hybrid privacy mechanism for general partition-based estimation in Section 2.2. In Section 2.3, we propose our partition rule. Finally, in Section 2.4, we provide a comprehensive description of LPDT.

### Preliminaries

NotationsFor any vector \(x\), let \(x^{i}\) denote the \(i\)-th element of \(x\). Recall that for \(1 p<\), the \(L_{p}\)-norm of \(x=(x^{1},,x^{d})\) is defined by \(\|x\|_{p}:=(|x^{1}|^{p}++|x^{d}|^{p})^{1/p}\). Throughout this paper, we use the notation \(a_{n} b_{n}\) and \(a_{n} b_{n}\) to denote that there exist positive constant \(c\) and \(c^{}\) such that \(a_{n} cb_{n}\) and \(a_{n} c^{}b_{n}\), for all \(n\). In addition, we denote \(a_{n} b_{n}\) if \(a_{n} b_{n}\) and \(b_{n} a_{n}\). Let \(a b=(a,b)\) and \(a b=(a,b)\). Besides, for any set \(A^{d}\), the diameter of \(A\) is defined by \((A):=_{x,x^{} A}\|x-x^{}\|_{2}\). Let the standard Laplace random variable have the continuous probability density function \(p(x)=e^{-|x|}\) for \(x\).

Regression is to predict the value of an unobserved output variable \(Y\) based on the observed input variable \(X\), based on a dataset \(D:=\{(X_{1},Y_{1}),,(X_{n},Y_{n})\}\) consisting of \(n\) i.i.d. observations drawn from an unknown probability measure \(\) on \(=^{d}[-M,M]\). The density function of \(\) is denoted as \(s\). In addition, we have a public dataset \(D^{pub}:=\{(X_{1}^{pub},Y_{1}^{pub}),,(X_{n_{q}}^{pub},Y_{n_{q}}^{pub})\}\) drawn from distribution \(Q\) on \(\) with sample size \(n_{q}\). Its density function is denoted as \(q\).

It is legitimate to consider the least square loss \(L:[0,)\) defined by \(L(x,y,f(x)):=(y-f(x))^{2}\) for our target of regression. Then, for a measurable decision function \(f:\), the risk is defined by \(_{L,}(f):=_{}L(x,y,f(x) )\,(x,y)\). The Bayes risk, which is the smallest possible risk with respect to \(\) and \(L\), is given by \(_{L,}^{*}:=\{_{L,}(f)|f: \}\). The function that achieves the Bayes risk is called Bayes function, namely, \(f^{*}(x):=Y|X=x\).

**Definition 2.1** (Local Differential Privacy).: Given data \(\{(X_{i},Y_{i})\}_{i=1}^{n}\), each \((X_{i},Y_{i})\) is mapped to a piece of privatized information \(s_{i}\) which is a random variable on \(\). Let \(()\) be the \(\)-field on \(\). \(s_{i}\) is drawn conditional on \((X_{i},Y_{i})\) via the distribution \((S X_{i}=x,Y_{i}=y)\) for \(S()\). Then the mechanism \(\) provides _\(\)-local differential privacy_ (\(\)-LDP) if

\[(S X_{i}=x,Y_{i}=y)}{ (S X_{i}=x^{},Y_{i}=y^{})} S(),x,x^{},\;\;y,y^{}} e^{ }.\]

This formulation of local privacy is widely adopted [23; 9]. In contrast to central DP where the likelihood ratio is taken with respect to some statistics of all data, LDP requires individuals to guarantee their own privacy by considering the likelihood ratio with respect to each \((X_{i},Y_{i})\). Once the view \(s\) is provided, no further processing can reduce the deniability about taking a value \((x,y)\) since any outcome \(s\) is nearly as likely to have come from some other initial value \((x^{},y^{})\).

### Privacy mechanism for tree partition

This section focuses on the hybrid privacy mechanism for general tree partitions. We first introduce the standard regression tree and then present our privacy mechanism based on the random response and Laplacian mechanism.

For index set \(\), let \(=\{A_{j}\}_{j}\) be any tree partition of \(\) with \(_{j}A_{j}=\) and \(A_{i} A_{j}=\), \(i j\). For any \(x\), let the cell containing \(x\) be \(A(x)\). A _population decision tree regressor_ with partition \(\) is defined as

\[_{}(x)=_{j}\{x A_{j}\}}f^{*}(x^{})\,d(x^{})}{_{A_{j}}\,d (x^{})}.\] (1)

Here, we let \(0/0=0\) by definition. To get a empirical estimator given the data set \(D=\{(X_{1},Y_{1}),,(X_{n},Y_{n})\}\), we estimate the numerator and the denominator of (1) respectively. To estimate the denominator, each sample \((X_{i},Y_{i})\) contributes a one-hot vector \(U_{i}\{0,1\}^{||}\) where the \(j\)-th element of \(U_{i}\) is \(\{X_{i} A_{j}\}\). Then an estimation of \(_{A_{j}}d(x)\) is \(_{i=1}^{n}U_{i}^{j}\), which is the number of samples in \(A_{j}\) divided by \(n\). Analogously, an estimation of \(_{A_{j}}f^{*}(x)d(x)\) is \(_{i=1}^{n}Y_{i} U_{i}^{j}\). Combining the pieces, a _decision tree regressor_ is defined as

\[f_{}(x)=_{j}\{x A_{j}\}^{n}Y_ {i} U_{i}^{j}}{_{i=1}^{n}U_{i}^{j}}.\] (2)

In other words, \(f_{}(x)\) estimates \(f(x)\) by the average of the responses in the cell \(A(x)\). In the non-private setting, each data holder prepares \(U_{i}\) and \(Y_{i}\) according to the partition \(\) and sends it to the curator. Then the curator aggregates the transmission following (2).

To protect the privacy of each data, we propose to estimate the numerator and denominator of the population regression tree using a privatized method. Specifically, given \(U_{i}^{j}\), we independently sample \(_{i}^{j}\) using the random response technique 

\[_{i}^{j}=U_{i}^{j}-}&}{1+e^{/4}}\\ 1-U_{i}^{j}-}&}.\] (3)Since \(_{}[_{i=1}^{n}_{i}^{j}]= -1}{^{/4}+1}_{i=1}^ {n}U_{i}^{j}\), we take \(+1}{^{/4}-1}_{i=1 }^{n}_{i}^{j}\) as the estimator of \(_{A_{j}}d(x)\). To privatize \(Y_{1},,Y_{n}\), we use the standard Laplace mechanism . Namely, we let

\[_{i}=Y_{i}+_{i}\] (4)

where \(_{i}\) are i.i.d. standard Laplace random variables. Similarly, \(+1}{^{/4}-1}_{i=1 }^{n}_{i}_{i}^{j}\) can be used to estimate \(_{A_{j}}f^{*}(x)d(x)\). Then using the privatized information \((_{i},_{i}),i=1,,n\), we define the _locally differentially private decision tree regressor_ as

\[f_{}^{}(x)=_{j}\{x A_{j}\}^{n}_{i}_{i}^{j}}{_{i=1}^{n}_{i }^{j}}.\] (5)

Compared to [9; 33] which used the Laplacian mechanism to protect both \(U_{i}\) and \(Y_{i}\), our mechanism (3) considers the fact that \(U\) is a binary vector. When \(||\) is large, (3) can be more efficient than the Laplace mechanism which has a heavier-tailed distribution [23; 24].

### Max-edge partition with variance reduction

While our privacy mechanism applies to any tree partition, it can be challenging to use general partitions such as the original CART  for theoretical analysis. Following the heuristic of , we propose a new splitting rule called the _max-edge partition rule_ using the variance reduction criterion. This rule is amenable to theoretical analysis and can also achieve satisfactory practical performance. Given public dataset \(\{(X_{i}^{pub},Y_{i}^{pub})\}_{i=1}^{n_{q}}\), the partition rule is stated as follows:

* Let \(A_{0}^{1}:=^{d}\) be the initial rectangular cell and \(_{0}:=\{A_{0}^{j}\}_{j_{0}}\) be the initialized cell partition. \(_{0}=\{1\}\) stands for the initialized index set. In addition, let \(p\) represent the maximum depth of the tree and let \(n_{l}\) represent the minimum sample size in each leaf. These parameters are fixed beforehand by the user and possibly depend on \(n\).
* Suppose we have obtained a partition \(_{i-1}\) of \(\) after \(i-1\) steps of the recursion. Let \(_{i}=\). In the \(i\)-th step, for each \(A_{i-1}^{j}_{i-1}\), \(j_{i-1}\), suppose it is \(_{=1}^{d}[a_{},b_{}]\). We choose the edge to be split among the longest edges. The index set of longest edges is defined as \[_{i-1}^{j}=\{k|b_{k}-a_{k}|=_{=1,,d}|b_{ }-a_{}|,\;k=1,,d\}.\]
* Assume we split along the \(\)-th dimension for \(_{i-1}^{j}\), \(A_{i-1}^{j}\) is then partitioned into a left sub-cell \(A_{i-1}^{j,0}()\) and a right sub-cell \(A_{i-1}^{j,1}()\) along the midpoint of the chosen dimension, where \(A_{i-1}^{j,0}()=\{x x A_{i-1}^{j},x^{}<+b_ {}}{2}\}\) and \(A_{i-1}^{j,1}()=A_{i-1}^{j}/A_{i-1}^{j,0}()\). Then the dimension to be split is chosen using the variance reduction criterion: \[*{arg\,min}_{_{i-1}^{j}}\;_{i=1}^{n_{q}} (Y_{i}^{pub}-f_{_{i-1} A_{i-1}^{j,0}() A_{i-1}^{j,1}( )/A_{i-1}^{j}}(X_{i}^{pub}))^{2}.\] (6)
* Once \(\) is selected, We count the number of samples in the sub-cells \(_{i=1}^{n}(X_{i}^{pub} A_{i-1}^{j,k}()),k=0,1\). If either of the cells contains fewer than \(n_{l}\) samples, the splitting is pruned and we let \(_{i}=_{i} A_{i-1}^{j}\). Otherwise, let \(_{i}=_{i}\{A_{i-1}^{j,0}(),A_{i-1}^{j,1}())\}\).

The complete process is presented in Algorithm 2 in the appendix. For each grid, the partition rule selects the midpoint of the longest edges that achieves the largest variance reduction. This procedure continues until there are not enough samples contained in any leaf node, or the depth of the tree reaches its limit.

### Decision Tree with local differential privacy

With these preparations, we finally present the full procedure of LPDT in Algorithm 1.

## 3 Theoretical results

In this section, we present our theoretical results and related comments. We first provide the \(\)-LDP guarantee of LPDT in Section 3.1. In Section 3.2, we establish the optimal convergence rate of LPDT with max-edge partition and the excess risk lower bound of LPDT without public data. Finally, we discuss the complexity of LPDT in Section 3.3.

### Privacy guarantee for LPDT

**Theorem 3.1**.: _Let \(=\{A_{j}\}_{j}\) be any partition of \(\) with \(_{j}A_{j}=\) and \(A_{i} A_{j}=\), \(i j\). Then the privacy mechanism \((,|X,Y)\) defined in (3) and (4) is \(\)-LDP. Consequently, the LPDT estimator \(f_{}^{}\) in Algorithm 1 is \(\)-LDP._

### Convergence rate of LPDT

We first present the necessary assumptions on the distribution \(\) and \(\).

**Assumption 3.2**.: Let \((0,1]\). Assume the true regression function \(f^{*}:\) is \(\)-Holder continuous, i.e. there exists a constant \(c_{L}>0\) such that for all \(x_{1},x_{2}\), \(|f^{*}(x_{1})-f^{*}(x_{2})| c_{L}\|x_{1}-x_{2}\|^{}\). Also, assume that the density function of \(\) is bounded, i.e. \(p(x)\) for some \(>0\).

**Assumption 3.3**.: We assume that there exists some constant \(>1\) such that for all cells \(A\), there holds \(^{-1}_{A}d_{X}(x)_{A}d_{X}(x)_ {A}d_{X}(x)\).

Assumption 3.2 is a standard condition widely used in non-parametric statistics. Assumption 3.3 depicts the similarity between the distribution of public data and private data. It is also a mild assumption and requires only the probabilities in each cell under \(_{X}\) and \(_{X}\) to be similar. When \(p(x)\) and \(q(x)\) are both bounded from 0, this assumption is satisfied. Alternatively, it suffices to require that \(p(x)/q(x)\) is upper and lower bounded.

**Theorem 3.4**.: _Let \(f_{}^{}\) be the LPDT estimator in Algorithm 1. Suppose Assumption 3.2 and 3.3 hold. Then, for \(n_{q} n^{}\), if we set \(s n^{2}\) and \(n_{l} n_{q}/2^{s}\), there holds_

\[_{L,}(f_{}^{})-_{L, }^{*}(})^{}\]

_with probability \(1-2/n_{q}^{2}-5/n^{2}\) with respect to \(^{n}^{n_{q}}^{n}\) where \(^{n}\) is the joint distribution of privacy mechanisms in (3) and (4)._

Note that we only require \(n_{q} n^{}\), which means the sample size of public data can be much smaller than private data. As illustrated in , the minimax convergence rate over Holder function space is \((n(e^{}-1)^{2})^{-}\), indicating that LPDT attains optimal rate when \(/(+d) 1/3\). In the case \(/(+d)>1/3\), or equivalently \(2>d\), LPDT achieves fast yet sub-optimal convergence rate \(n^{-}\). Note that \(2>d\) only when \(d=1\) and \( 1/2\), which rarely occurs. The next statement shows that LPDT fails without public data.

**Theorem 3.5**.: _Let \(f_{}^{}\) be the LPDT estimator in Algorithm 1 and \(\) be the class of distributions satisfying Assumption 3.2. For \(n_{q}=0\) i.e. there is no public data, for any choice of \(s\), \(n_{l}\), and \(\), there holds_

\[_{}([_{L, }(f_{}^{})]-_{L,}^{*} ) 1.\]Under the same hypothesis function space, the supremum of excess risk of LPDT does not even converge without public data. Together with Theorem 3.4, this shows that the prior information contained in public data can greatly enhance the quality of the private estimation.

We compare our results with those of others. LPDT converges faster than deconvolution-based method  whose rate is \(n^{-}\). As for histogram-based methods,  achieves the optimal rate only when the density function is lower bounded, which is a strong condition. To avoid the condition,  derived an _ad hoc_ estimator by adding a regularization to the marginal density estimation. LPDT takes another approach to avoid the condition. It does not apply any regularization or truncation on the estimator in each cell. Instead, as long as Assumption 3.3 holds, the low-density regions can be identified and treated with larger cells automatically by the parameter \(n_{l}\). As a sacrifice, the large cells restrict the approximation ability of LPDT and the convergence rate is no more than \(n^{-1/3}\). In addition, our theoretical results hold in the sense of "with high probability", which is more closely related to practical needs than "in expectation" as addressed in [9; 33].

Besides these advantages, we also discuss the benefit of public data for removing the range parameter [10; 9]. Consider the example from , where the convergence rate is given by \((^{2d}}{n^{2}})^{}\). When the set \(=_{j=1}^{d}[a^{j},b^{j}]\) is unknown, it becomes necessary to create a histogram partition over \(_{j=1}^{d}[-r_{n},r_{n}]\), introducing an additional factor of \(r_{n}^{2d}\). However, with publicly available data, we can approximate the range of the \(j\)-th dimension using \(}=_{i}X_{i}^{j}\) and \(}=_{i}X_{i}^{j}\). Subsequently, we can perform min-max scaling on each data point from \(_{j=1}^{d}[},}]\) to map it into \(_{j=1}^{d}\), and then train an LPDT on \(_{j=1}^{d}\). Any \(x\) that falls outside the range \(_{j=1}^{d}\) is predicted as \(0\). We demonstrate that by this approach, Theorem 3.4 holds with a probability of at least \(1-d/n_{q}^{2}\). See derivations in Section C.6 in the appendix.

### Complexity analysis

We demonstrate that LPDT is an efficient method. We first consider the average computation complexity of LPDT. The training stage consists of two parts. The partition procedure takes \((sn_{q}d)\) time and the computation of (5) takes \((sn)\) time. From the proof of Theorem 3.4, we know that \(2^{s}(n^{2}/ n)^{-}\). Thus the training stage complexity is around \((n n^{2}+n_{q}d n^{2})\). Since each prediction of the decision tree takes \((s)\) time, the test time for each test instance is around \(( n^{2})\). As for storage complexity, since LPDT only requires the storage of the tree structure and the prediction value at each node, the space complexity of LPDT is \(((n^{2}/ n)^{-})\). In short, LPDT is an efficient method with a small number of parameters.

We compare the complexities of LPDT with other LDP regression methods in Table 1. Notably,  is inefficient due to its unacceptable test and space complexity. Also, the dominant term of training complexity of  is \((nd n^{2})\). When \(d\) is large, we can choose a small \(n_{q}\) such that LPDT yields a strictly lower complexity than . In addition, although  enjoys the same order of space complexity as LPDT, the memory of histogram-based methods suffers from the curse of dimensionality in practice. Since the storage of \((h^{-d})\) values is required, even \(h=1/2\) requires allocating an array of size \(2^{d}\), which is problematic for large \(d\). In contrast, LPDT can resist the curse of dimensionality by only splitting along the relevant features and keeping a small number of nodes.

## 4 Experiments

In the experiments, we first validate our theoretical findings with synthetic data in Section 4.1. Then, in Section 4.2, we show the superior performance of LPDT on real-world datasets with identically distributed public data. In Section 4.3, we apply LPDT to Chicago taxi data to show that LPDT

    & LPDT & PHIST  & DECONV  \\  Training Time Complexity & \((n n^{2}+n_{q}d n^{2})\) & \((nd n^{2})\) & - \\ Testing Time Complexity & \(( n^{2})\) & \(( n^{2})\) & \((nd)\) \\ Space Complexity & \(((n^{2}/ n)^{})\) & \(((n^{2}/ n)^{})\) & \((nd)\) \\   

Table 1: Comparison of complexities of LDP regression methods.

performs well even with considerable differences between private and public data. Also, we analyze the influence of the distribution shift between private and public data on LPDT.

Splitting ruleNote that most tree methods design their partition rules based on the information gained from the data. To boost the performance of LPDT, we also incorporate the variance reduction scheme from the original CART  to the tree construction. We denote the LPDT estimator using the max-edge partition rule with variance reduction in Section 2.3 as LPDT-M and denote the estimator using the standard variance reduction rule in  as LPDT-V. Since Theorem 3.1 holds for any partition, LPDT-V is also \(\)-LDP.

Experiment setupWe choose the privacy budget \([0.5,8]\), covering commonly seen magnitudes of privacy budgets from low to high privacy regimes. We compare LPDT-M and LPDT-V with the following methods: (_i_) Private Histogram (PHIST) . (_ii_) Adjusted Private Histogram (APHIST) . (_iii_) Deconvolution Kernel (DECONV) . Introduction to the methods and all implementation details are presented in Appendix D.1. We employ 5-fold cross-validation for parameter selection, and techniques for tuning parameters under LDP are discussed in Section D.2. The evaluation metric is the mean squared error (MSE).

### Synthetic experiments

Necessity of public dataTo demonstrate intuitively why public data is essential for LPDT, we first visualize its estimation on a synthetic model, \(Y=(16X)+\) where \(X(0.5,0.025)\) and \((0,1)\). In this case, the marginal distribution is highly imbalanced with the majority of samples located in the middle part of \(\) and a few samples on the sides. For \(=8\), we fit two LPDT models: one with 500 public data and 7,000 private data, and another with 8,000 private data.

As shown in Figure 1(a), without public data, LPDT struggles with the imbalanced marginal. The grids on the side produce unstable predictions since only a few samples fall into them. As a result, LPDT tends to decrease depth \(s\) to stabilize its estimation. This leads to underfitting in the middle so that the predicted curve fails to capture the variation of the ground truth. In contrast, with the aid of public data, LPDT solves the issue as shown in Figure 1(b). For the middle zone where samples are abundant, LPDT creates small grids to enlarge approximation capacity. Meanwhile, it prunes the grids on the sides to ensure stability. Even with fewer private data, the MSE of LPDT is reduced from 1.19 to 1.08 thanks to the additional public data. In summary, the experiment provides empirical evidence supporting the theoretical findings in Theorems 3.4 and 3.5, which highlight the necessity of public data for the effective performance of LPDT.

Parameter analysis of depth \(s\)We conduct experiments to investigate the selection of partition depth \(s\) in terms of MSE. We generate 6,000 training samples, 2,000 test samples, and 2,000 public samples following the synthetic model described above. We pick \(\{3,4,6,8\}\) and \(s\{2,3,4,5,6\}\). For each pair of \(s\) and \(\), we plot the 20 times averaged MSE versus \(s\). The result is displayed in Figure 2(a). Apparently, for each \(\), as \(s\) increases, MSE first decreases until \(s\) reaches a certain value. Then MSE begins to increase as \(s\) grows. This further confirms the trade-off observed in Theorem 3.4. Moreover, the depth \(s\) at which the test error is minimized increases as \(\) increases. This is compatible with theory since the optimal choice of \(s n^{2}\) is monotonically increasing with respect to \(\).

Parameter analysis of minimum leaf sample size \(n_{l}\)We conduct experiments to investigate the choice of \(n_{l}\) in terms of MSE. Following the same generating scheme, we choose \(\{3,4,6,8\}\) and plot MSE of LPDT versus \(n_{l}\) for \(n_{l}\{15,25,,135\}\). In Figure 2(b), the relation between MSE and \(n_{l}\) is U-shaped under each \(\), which indicates that a properly chosen \(n_{l}\) is necessary as stated in

Figure 1: The estimated regression curve of LPDT with and without public data. 1,000 samples are displayed in green.

Theorem 3.4. Furthermore, LPDT achieves the best MSE for \(n_{l}\) when \(=4,6,8\), while the minimum MSE occurred at \(n_{l}=125\) when \(=3\). This finding is compatible with Theorem 3.4 which states that the optimal choice of \(n_{l}\) is monotonically decreasing with respect to \(\).

Our parameter analyses indicate that decreasing \(\) favors smaller values of \(s\) and larger values of \(n_{l}\). These choices lead to a decision tree partition with fewer grids. In summary, when facing higher levels of privacy demand, LPDT cuts down the number of grids to stabilize its estimation.

Privacy utility trade-offWe analyze how privacy budget \(\) influences the quality of prediction. Under the same setup, we evaluate LPDT and other methods for \(\{0.5,1,2,3,,10\}\) with 50 repetitions. The results are displayed in Figure 2(c). To show significance, we plot the \((0.1,0.9)\) quantiles as confidence intervals. When \(\) increases, the MSE of LPDT decreases much faster than the other methods. Note that the MSE of both PHIST and APHIST remains high, suggesting that their performances are limited by the histogram instead of the privacy mechanism.

### Real data comparison with identically distributed public data

Experiment setupWe conduct experiments on 12 real datasets, each repeated 50 times with a ratio of 1:7:2 for public data, training data, and testing data in each trial. The dataset details and pre-processing steps are summarized in Appendix D.3. To ensure significance, we adopt the Wilcoxon signed-rank test  with a significance level of 0.05 to check if a result is significantly better. For better comparison, we also train a decision tree (denoted as DT) on the original training data with no privacy protection, whose result will serve as a lower bound.

**Performance of accuracy and running time** The representative results for \(=2,6\) are displayed in Table 2. Results of \(=0.5,1,4,8\) is in Appendix D.4. It can be seen that LPDT-M and LPDT-V both significantly outperform their competitors. All methods achieve a higher MSE than DT, while the results for LPDT-M and LPDT-V are reasonably close to DT. Due to memory limitations, PHIST and APHIST fail on two datasets. We also compare the total running time in Table 6 in Appendix D.4. In general, both LPDT-M and LPDT-V achieve less running time than PHIST and APHIST, and are significantly faster than DECONV.

    &  &  \\  &  &  &  &  &  &  &  &  &  &  &  \\  & ABA & 5.67e+0 & **1.01e+1** & 1.01e+1 & 1.89e+1 & 1.06e+1 & 1.10e+7 & 8.38e+0 & **7.34e+0** & 2.05e+1 & 1.05e+1 & 1.09e+1 \\  & AIR & 2.26e+1 & 4.80e+1* & **4.69e+1** & 1.31e+3 & 3.60e+1 & 3.00e+2 & 4.49e+1* & **3.60e+1** & 1.60e+3 & 4.98e+1 & 4.72e+1 \\  & ALG & 2.12e-2 & 2.57e-1 & **2.43e-1** & 2.52e-1 & 2.52e-1 & 9.26e+4 & **2.44e-1** & 2.46e-1 & 2.63e-1 & 2.47e-1 & 3.14e-1 \\  & AQU & 1.92e+0 & 2.99e-0 & 2.99e+0 & 4.01e+0 & **2.93e+0** & 5.74e+3 & 2.73e+0 & **2.67e+0** & 4.75e+0 & 2.83e+0 & 2.96e+0 \\  & BUI & 1.75e+5 & **1.50e+6** & 1.64e-6 & -6 & -6 & 1.20e+9 & 1.44e+6 & **1.34e-6** & 2.16e-6 & 2.04e-7 \\  & CBM & 4.08e-27 & 2.12e+0 & 1.65e+0 & 9.53e+0 & 6.97e+0 & 2.37e+3 & 7.62e-1 & **1.12e-1** & 4.94e-0 & 3.21e+0 & 1.23e+5 \\  & CCP & 2.19e+1 & 1.50e+2* & **1.06e+2** & 2.07e+4 & 3.64e+2 & 3.03e+2 & 8.42e+1 & **5.18e+1** & 2.24e+4 & 3.28e+2 & 2.56e+2 \\  & CON & 9.38e+1 & 2.94e+2 & **2.89e+2** & 3.81e+2 & 3.00e+2 & 2.24e+7 & 2.44e+2 & **2.13e-2** & 4.16e+2 & 2.96e-2 & 3.13e+2 \\  & CPU & 2.15e+1 & 3.41e-2 & **9.00e+1** & 9.26e-2 & 3.42e+2 & 2.15e+5 & 3.02e+2 & **6.15e+1** & 9.98e-2 & 3.40e+2 & 3.98e+2 \\  & FIS & 1.07e+0 & 2.15e+0 & **2.14e+0** & 3.14e+0 & 2.22e+0 & 3.47e+3 & **1.65e+0** & 1.76e+0 & 3.60e+0 & 2.16e+0 & 2.21e+0 \\  & HOU & 2.11e+1 & **8.10e+1** & 8.22e+1 & 1.06e+2 & 8.52e+1 & 1.92e+4 & 7.43e+1 & **7.10e+1** & 1.23e+2 & 8.21e+1 & 2.44e+2 \\  & MUS & 3.00e-2 & 3.47e+2 & **3.46e+2** & - & - & 9.50e+3 & **3.37e+2** & 3.27e+2 & - & - & 8.09e+3 \\  & RED & 4.76e-1 & 7.08e-1 & 7.08e-1 & 7.38e+0 & 7.57e-1 & 1.23e+8 & 6.75e-1 & **6.12e+1** & 3.80e+0 & 7.12e-1 & 8.66e-1 \\  & WHI & 5.77e-1 & 8.30e-1 & 8.42e-1 & 4.01e+0 & **8.15e-1** & 1.64e+7 & 7.03e-1* & **6.61e-1** & 4.45e+0 & 8.03e-1 & 1.47e+0 \\   

Table 2: Average MSE over real data sets for LDP regression methods. The best results are **bolded** and the second best results are underlined. The marked results with significance towards the rest results are marked with \(*\). Due to memory limitation, PHIST and APHIST are corrupted on two datasets which are marked with -.

Figure 2: Different parameters versus MSE.

With the private and public distributions being identical, another alternative is to use solely public data to fit a decision tree. We train such decision trees on public data (\(n_{g}=0.1n\)). It is compared with the decision tree trained on private data and LPDT with \(=6\) in Table 7 in Appendix. The results show that when \(n_{q} n\) and \(\) is large, training a decision tree on public data is worse than using LPDT on most of the datasets.

The power of public dataWe conduct experiments to show the utility gain brought by public data. We take red wine dataset as an example. With \(n_{q}=100\) public data, we run LPDT (with \(D^{}\)) and PHIST (without \(D^{}\)). For \(\{0.2,0.4,0.6,0.8,1\}\), we train each model on \( n\) samples with 20 repetitions. The result is in Figure 3. PHIST with 1,100 samples achieves the same MSE as LPDT with 660 samples. The result shows that, with a small amount of public data, LPDT achieves the same performance with much fewer samples.

### Real data comparison with non-identically distributed public data

We apply LPDT to the _Chicago Taxi Dataset_, a collection of taxi trips in Chicago provided by the Differential Privacy Temporal Map Challenge and contains sensitive information . We use the fare of each trip as labels and other information as features. Then we regard trips paid by PR card and credit card as public data and private data, respectively. In Appendix D.5, we show the two parts are distributed differently. After preprocessing, the dataset has 101 features with 2,150,565 samples in private data and 24,436 samples in public data.

PerformanceWe report the averaged MSE of LPDT over 20 repetitions for \(=0.5,1,2,4,6,\) and \(8\). As a comparison to LPDT, we train two decision trees separately using the public and private datasets with no privacy protection. As for the comparison methods, DECONV fails due to the large sample size while PHIST and APHIST fail due to the dimensionality. However, after reducing the dimensionality by retaining only the continuous features, we are able to apply PHIST and APHIST. The results are displayed in Table 3. A first observation is that the decision tree trained on public data yields considerably worse results than the decision tree trained on private data. This suggests that relying solely on public data leads to biased predictions. Learning solely from public data achieves an MSE higher than LPDT for all three values of \(\). Even for \(=2\), LPDT significantly outperforms histogram-based methods with \(=8\). This indicates that LPDT remains effective even when substantial disparities exist between the distributions of public and private data.

How does non-identically distributed public data help?It is counterintuitive that non-identically distributed public data can benefit private estimation. We show the logic behind using such public data by investigating the first split feature in the tree partition. LPDT identifies whether a trip ends in Zone 32 as an important feature for predicting fare and initiates the recursive partitioning process by splitting along this feature. Figure 4 illustrates that trips ending in this zone generally have lower fares for both public and private data, although the actual fares differ significantly between the two datasets. This observation demonstrates that despite having different distributions, public and private data may exhibit similar patterns, thereby allowing the partition created on public data to still be effective on private data. Following this line of reasoning, to determine whether public data is suitable for a specific private task, we can examine whether the qualitative relationships between labels and features remain consistent across both datasets. Whether a dataset can be utilized as public data can be of independent interest as in .

Analysis of distribution shiftIn Table 3, the MSE of LPDT reduces as \(\) increases but remains higher than the MSE achieved by training a decision tree directly on private data. The performance

    &  &  &  \\  Public & Private & \(=0.5\) & \(=1\) & \(=2\) & \(=4\) & \(=6\) & \(=8\) & \(=2\) & \(=8\) & \(=2\) & \(=8\) \\ 
3.71 & 0.80 & 113.45 & 15.74 & 4.89 & 3.35 & 2.86 & 2.70 & 24.72 & 17.22 & 38.22 & 35.5 \\  & & (14.23) & (2.20) & (0.54) & (0.33) & (0.10) & (0.10) & (0.02) & (0.00) & (0.01) & (0.01) \\   

Table 3: Average MSE and standard deviation over Chicago taxi data.

Figure 3: Power of public data.

gap can be attributed to the distribution shift between private data and public data. In the following, we investigate how the difference between the two datasets affects the performance of LPDT. Besides the Chicago taxi data, we also adopt _White Wine_ and _Red Wine_ data in Section 4.2 as private and public data, respectively. The datasets contain the same variables but are distributed differently, as is investigated in transfer learning literature . On both datasets, we combine part of the private samples with public data and perform the partition procedure on the combined dataset, with the portion of public data denoted as \(\). When \(\) is small, there is less difference between data used for partition and training. We report the average MSE of LPDT for \(\{0.1,0.3,0.5,0.7,0.9\}\) after 20 repetitions. Figure 5 shows that a small \(\) leads to a lower MSE for both datasets and all values of \(\). Thus, LPDT is more powerful when the public and private data are distributed similarly, which also justifies the necessity of Assumption 3.3.

## 5 Conclusion

This paper addresses the challenge of effectively performing LDP regression given both public data and private data by introducing the locally private decision tree. Due to the novel idea of leveraging public data, LPDT is accurate, efficient, and interpretable. Theoretically, we establish the privacy guarantee and optimal convergence rate of LPDT. In experiments, we show the superior performance of LPDT regardless of the disparities between private and public data.

## 6 Limitations and broader impact

LPDT addresses the challenge of effectively performing LDP regression given both public data and private data. Future work may explore methodologies for incorporating public data in private learning using other models, such as linear models, and investigate the theoretical advantages of using public data in private learning. One limitation of this paper is that the measure of similarity between the public and private distribution considers only the marginal distribution. One may consider the similarities with respect to the regression function, potentially following transfer learning literature . Moreover, in practice, public data may have a different distribution than private data, which is damaging to the learning process. The drawback yields the significance of public data selection or public data quality test such as [32; 60]. In addition, the theoretical analysis deal with \(n_{q} n^{}\) and \(n_{q}=0\), while the intermediate zone for \(n_{q}\) remains unexplored.