# Understanding, Predicting and Better Resolving Q-Value Divergence in Offline-RL

Yang Yue\({}^{*}\)\({}^{1}\)   Rui Lu\({}^{*}\)\({}^{1}\)   Bingyi Kang\({}^{*}\)\({}^{2}\)   Shiji Song\({}^{1}\)   Gao Huang\({}^{}\)\({}^{1}\)

\({}^{1}\) Department of Automation, BNRist, Tsinghua University  \({}^{2}\) ByteDance Inc.

{le-y22, r-lu21}@mails.tsinghua.edu.cn

bingykang@gmail.com {shijis, gaohuang}@tsinghua.edu.cn

Equal contribution. Corresponding author.

###### Abstract

The divergence of the Q-value estimation has been a prominent issue in offline reinforcement learning (offline RL), where the agent has no access to real dynamics. Traditional beliefs attribute this instability to querying out-of-distribution actions when bootstrapping value targets. Though this issue can be alleviated with policy constraints or conservative Q estimation, a theoretical understanding of the underlying mechanism causing the divergence has been absent. In this work, we aim to thoroughly comprehend this mechanism and attain an improved solution. We first identify a fundamental pattern, _self-excitation_, as the primary cause of Q-value estimation divergence in offline RL. Then, we propose a novel **S**elf-**E**c**cite **E**igenvalue **M**easure (SEEM) metric based on Neural Tangent Kernel (NTK) to measure the evolving property of Q-network at training, which provides an intriguing explanation on the emergence of divergence. For the first time, our theory can reliably decide whether the training will diverge at an early stage, and even predict the order of the growth for the estimated Q-value, the model's norm, and the crashing step when an SGD optimizer is used. The experiments demonstrate perfect alignment with this theoretic analysis. Building on our insights, we propose to resolve divergence from a novel perspective, namely regularizing the neural network's generalization behavior. Through extensive empirical studies, we identify LayerNorm as a good solution to effectively avoid divergence without introducing detrimental bias, leading to superior performance. Experimental results prove that it can still work in some most challenging settings, i.e. using only 1\(\%\) transitions of the dataset, where all previous methods fail. Moreover, it can be easily plugged into modern offline RL methods and achieve SOTA results on many challenging tasks. We also give unique insights into its effectiveness. Code can be found at https://offrl-seem.github.io.

## 1 Introduction

Off-policy Reinforcement Learning (RL) algorithms are particularly compelling for robot control [39; 13; 17] due to their high sample efficiency and strong performance. However, these methods often suffer divergence of value estimation, especially when value over-estimation becomes unmanageable. Though various solutions (_e.g._, double q-network ) are proposed to alleviate this issue in the online RL setting, this issue becomes more pronounced in offline RL, where the agent can only learn from offline datasets with online interactions prohibited . As a result, directly employing off-policy algorithms in offline settings confronts substantial issues related to value divergence [14; 30].

This raises a natural yet crucial question: _why is value estimation in offline RL prone to divergence, and how can we effectively address this issue?_ Conventionally, _deadly triad_[46; 4; 47; 48] isidentified to be the main cause of value divergence. Specifically, it points out that a RL algorithm is susceptible to divergence at training when three components are combined, including off-policy learning, function approximation, and bootstrapping. As a special case of off-policy RL, offline RL further attributes the divergence to _distributional shift_: when training a Q-value function using the Bellman operator, the bootstrapping operation frequently queries the value of actions that are unseen in the dataset, resulting in cumulative extrapolation errors [14; 30; 33]. To mitigate this issue, existing algorithms either incorporate policy constraints between the learned policy and the behavior policy [42; 14; 53; 23; 30; 12; 51; 8], or make conservative/ensemble Q-value estimates [31; 38; 2; 41; 57; 15]. While these methods demonstrate some effectiveness by controlling the off-policy degree and bootstrapping--two components of the deadly triad--they often overlook the aspect of function approximation. This neglect leaves several questions unanswered and certain limitations unaddressed in current methodologies.

**How does divergence actually occur?** Most existing studies only provide analyses of linear functions as Q-values, which is usually confined to contrived toy examples [47; 4]. The understanding and explanation for the divergence of non-linear neural networks in practice are still lacking. Little is known about what transpires in the model when its estimation inflates to infinity and what essential mechanisms behind the deadly triad contribute to this phenomenon. We are going to answer this question from the perspective of function approximation in the context of offline RL.

**How to avoid detrimental bias?** Despite the effectiveness of policy constraint methods in offline RL, they pose potential problems by introducing detrimental bias. Firstly, they explicitly force the policy to be near the behavior policy, which can potentially hurt performance. Also, the trade-off between performance and constraint needs to be balanced manually for each task. Secondly, these methods become incapable of dealing with divergence when a more challenging scenario is encountered, _e.g._, the transitions are very scarce. Instead, we are interested in solutions without detrimental bias.

Our study offers a novel perspective on the challenge of divergence in Offline-RL. We dissect the divergence phenomenon and identify _self-excitation_--a process triggered by the gradient update of the Q-value estimation model--as the primary cause. This occurs when the model's learning inadvertently elevates the target Q-value due to its inherent generalization ability, which in turn encourages an even higher prediction value. This mirage-like property initiates a self-excitation cycle, leading to divergence. Based on this observation, we develop theoretical tools with Neural Tangent Kernel (NTK) to provide in-depth understanding and precise prediction of Q-value divergence. This is the first work, to our knowledge, that accurately characterizes neural network dynamics in offline RL and explains the divergence phenomenon. More specifically, our contributions are three-fold:

* **Explanation:** We offer a detailed explanation of Q-value estimation divergence in offline RL settings with neural networks as non-linear estimators. We propose a novel metric, the **S**elf-**E**xcite **E**igenvalue **M**easure (SEEM), which is defined as the largest eigenvalue in the linear iteration during training, serving as an indicator of divergence. It also elucidates the fundamental mechanisms driving this divergence.
* **Prediction:** We can foresee the divergence through SEEM at an early stage of training before its estimation explodes. If divergence occurs, our theoretical framework is capable of predicting the growth pattern of the model's norm and the estimated Q-value. When the SGD optimizer is used, we can even correctly predict the specific timestep at which the model is likely to crash. Our experiments show our theoretical analysis aligns perfectly with reality.
* **Effective Solution:** Drawing from our findings, we suggest mitigating divergence by regularizing the model's generalization behavior. This approach allows us to avoid imposing strict constraints on the learned policy, while still ensuring convergence. Specifically, viewed through the lens of SEEM, we find that MLP networks with LayerNorm exhibit excellent local generalization properties while MLPs without LayerNorm don't. Then we conduct experiments to demonstrate that with LayerNorm in critic network, modern offline RL algorithms can consistently handle challenging settings when the offline dataset is significantly small or suboptimal.

## 2 Preliminaries

**Notation.** A Markov Decision Process (MDP) is defined by tuple \(=(S,A,P,r,)\), where \(S^{d_{S}}\) is the state space and \(A^{d_{A}}\) is the action space. \(P:S A(S)\) is the transition dynamics mapping from state-action pair to distribution in state space. \(r:S A\) is the reward function and \((S)\) is the initial state distribution. An offline RL algorithm uses an offline dataset\(D=\{(s_{i},a_{i},s^{}_{i},r_{i})\}_{i=1}^{M}\), which consists of \(M\) finite samples from interactions with MDP \(\). We write \(s D\) if there exists some \(k[M]\) such that \(s_{k}=s\), similar for notation \((s,a) D\).

In this paper, we consider using a \(L\)-layer ReLU-activated MLP as the Q-value approximator. Each layer has parameter weight \(_{}^{d_{+1} d_{}}\) and bias \(_{}^{d_{+1}}\). The activation function is ReLU: \((x)=(x,0)\). Integer \(d_{}\) represents the dimensionality of the \(_{th}\) hidden layer, where \(d_{0}=d_{S}+d_{A}\) and \(d_{L}=1\) since a scalar estimation is required for Q-value. Denote \(=[_{0},_{0},,_{L},_{L}]\) to be the vector of all learnable parameters. We use \(_{}(s,a)=f_{}(s,a)\) to denote the estimated Q-value using neural network \(f_{}\) with parameter \(\). Also, \(_{}(s)=_{a}_{}(s,a)\) is its induced policy. Sometimes we will write \(f_{}()^{M}\) where \(=[(s_{1},a_{1}),,(s_{M},a_{M})]\) stands for the concatenation of all inputs \((s_{i},a_{i}) D\) in dataset.

Considering a Q-learning-based offline RL algorithm that alternates between the following two steps:

\[ r(s,a)+_{a^{}}_{}(s^{ },a^{}),-_{}(-_{}(s,a))^{2},\] (1)

where \(\) is the learning rate for updating the Q network. Practical algorithms might use its actor-critic form for implementation, we stick to this original form for theoretical analysis. In the following analysis, we call these two steps _Q-value iteration_.

**Neural Tangent Kernel (NTK).** NTK  is an important tool to analyze the dynamics and generalization of neural networks. Intuitively, it can be understood as a special class of kernel function, which measures the similarity between \(\) and \(^{}\), with dependency on parameter \(\). Given a neural network \(f_{}\) and two points \(,^{}\) in the input space, the NTK is defined as \(k_{}(,^{}):=_{}f_{}(),_{}f_{}(^{})\), where the feature extraction is \(_{}():=_{}f_{}()\). If we have two batches of input \(,^{}\), we can compute the Gram matrix as \(_{}(,^{})=_{}()^{ }_{}(^{})\).

Intuitively speaking, NTK measures the similarity between two inputs through the lens of a non-linear network \(f_{}\). Such similarity can be simply understood as the correlation between the network's value predictions of these two inputs. If \(k_{}(,^{})\) is a large value, it means these two points are "close" from the perspective of the current model's parameter. As a result, if the model's parameter changes near \(\), say, from \(\) to \(^{}=+\), denote the prediction change for \(\) as \( f()=f_{^{}}()-f_{}()\) and for \(x^{}\) as \( f(^{})=f_{^{}}(^{})-f_{}(^{})\). \( f()\) and \( f(^{})\) will be highly correlated, and the strength of the correlation is proportional to \(k_{}(,^{})\). This intuition can be explained from simple Taylor expansion, which gives \( f()=^{}_{}()\) and \( f(^{})=^{}_{}(^{ })\). These two quantities will be highly correlated if \(_{}()\) and \(_{}(^{})\) are well-aligned, which corresponds to large NTK value between \(\) and \(^{}\). To summary, \(k_{}(,^{})\) characterizes the subtle connection between \(f_{}()\) and \(f_{}(^{})\) caused by neural network's generalization.

## 3 Theoretical Analysis

In this section, we will investigate the divergence phenomenon of Q-value estimation and theoretically explains it for a more comprehensive understanding. Note that in this section our analysis and experiments are conducted in a setting that does not incorporate policy constraints and exponential moving average targets. Although the setting we analyze has discrepancies with real practice, our analysis still provides valueable insight, since the underlying mechanism for divergence is the same. We first focus on how temporal difference (TD) error changes after a single step of Q-value iteration in Equation (1). Our result is the following theorem.

Figure 1: NTK similarity and action similarity along the training in two offline D4RL tasks. Different colors represent different seeds. The cosine similarity between the current step and the last step is computed. We can see all runnings reach a steady NTK direction and policy actions after a time \(t_{0}\).

**Theorem 1**.: _Suppose that the network's parameter at iteration \(t\) is \(_{t}\). For each transition \((s_{i},a_{i},s_{i+1},r_{i})\) in dataset, denote \(=[r_{1},,r_{M}]^{}^{M}\), \(_{_{t}}(s)=_{a}_{_{t}}(s,a).\) Denote \(^{*}_{i,t}=(s_{i+1},_{_{t}}(s_{i+1}))\). Concatenate all \(^{*}_{i,t}\) to be \(^{*}_{t}\). Denote \(_{t}=f_{_{t}}()-(+ f_{_{t} }(^{*}_{t}))\) to be TD error vector at iteration \(t\). The learning rate \(\) is infinitesimal. When the maximal point of \(_{_{t}}\) is stable as \(t\) increases, we have the evolving equation for \(_{t+1}\) as \(_{t+1}=(+_{t})_{t}\), where \(_{t}=(_{_{t}}(^{*}_{t})-_{_{t }}())^{}_{_{t}}()=_{_{t }}(^{*}_{t},)-_{_{t}}(,)\)._

\(\) is defined in Section 2. Detailed proof is left in Appendix B. Theorem 1 states that when the policy action \(_{_{t}}(s)\) that maximizes Q-values keep stable, leading to invariance of \(^{*}_{t}\), each Q-value iteration essentially updates the TD error vector by a matrix \(_{t}\), which is determined by a discount factor \(\) and the NTK between \(\) and \(^{*}_{t}\).

### Linear Iteration Dynamics and SEEM

Although Theorem 1 finds that TD update is a linear iteration under certain conditions, the dynamic is still complex in general. The policy \(_{_{t}}(s)\) that maximizes Q-values may constantly fluctuate with \(_{t}\) over the course of training, leading to variations of \(^{*}_{t}\). Also, the kernel \(_{}(,)\) has dependency on parameter \(\). This causes non-linear dynamics of \(_{t}\) and the model parameter vector \(\). However, according to our empirical observation, we discover the following interesting phenomenon.

**Assumption 2**.: **(Existence of Critical Point)** _There exists a time step \(t_{0}\), a terminate kernel \((,)\) and stabilized policy state-action \(}^{*}\) such that \(\|(,^{})}{\|\|\|^{}\| }-(,^{})}{\|\|\|^{}\|} \|=o(1)\) and \(\|^{*}_{t+1}-^{*}_{t}\|=o()\) for any \(,^{}\) when \(t>t_{0}\). We also assume that \(\|^{*}_{t}-}^{*}\|=o(1)\)._

This assumption states that the training dynamics will reach a steady state, where the policy \(_{_{t}}\) stabilizes and NTK converges to a terminate direction. Usually, constant NTK requires the width of the network to be infinite. But in our experiments1, the convergence of NTK is still observed in all our environments despite finite width (see Figure 1). Results of more environments are attached in the appendix. It should be pointed out that NTK direction stability itself does not imply model convergence; rather, it only indicates that the model evolves in steady dynamics. Apart from NTK, we also observe that \(_{_{t}}\) converges after a certain timestep \(t_{0}\). Moreover, we find that \(_{_{t}}\) always shifts to the border of the action set with max norm (see Figure 15). The reason for the existence of this critical point is elusive and still unknown. We hypothesize that it originates from the structure of gradient flow ensuring every trajectory is near an attractor in the parameter space and ends up in a steady state.

Based on the critical point assumption, we prove that the further evolving dynamic after the critical point is precisely characterized by linear iteration as the following theorem. We also explain why such a stable state can persist in appendix.

**Theorem 3**.: _Suppose we use SGD optimizer for Q-value iteration with learning rate \(\) to be infinitesimal. Given iteration \(t>t_{0}\), and \(=}(}^{*},)-}(,)\), where \(}\) is the Gram matrix under terminal kernel \(\). The divergence of \(_{t}\) is equivalent to whether there exists an eigenvalue \(\) of \(\) such that \(()>0\). If converge, we have \(_{t}=(+)^{t-t_{0}}_{t_{0}}\). Otherwise, \(_{t}\) becomes parallel to the eigenvector of the largest eigenvalue \(\) of \(\), and its norm diverges to infinity at following order_

\[\|_{t}\|_{2}=O( t)^{L/(2L- 2)}})\] (2)

_for some constant \(C^{}\) to be determined and \(L\) is the number of layers of MLP. Specially, when \(L=2\), it reduces to \(O( t})\)._

Theorem 3 predicts the divergence by whether \(_{}()\) is greater to 0. We term this _divergence detector_\(_{}()\) as **S**elf-**E**xcite **E**igenvalue **M**easure, or SEEM. Essentially, Theorem 3 states that

Figure 2: Prediction Ability - Linear decay of inverse Q-value with SGD and \(L=2\).

we can monitor SEEM value to know whether the training will diverge. To validate this, we conduct experiments with different discount factor \(\). The first term in \(\) is linear to \(\), thus larger \(\) is more susceptible to diverging. This is exactly what we find in reality. In Figure 3, we can see that experiments with large \(\) have positive SEEM, and the training eventually diverges (\(\)). Moreover, SEEM can also faithfully detect the trend of divergence during the training course. On the right of Figure 3, we can see that the surge of SEEM value is always in sync with the inflation of estimation value. All these results corroborate our theoretical findings.

### Prediction Ability

In Section 3.1, we have demonstrated that SEEM can reliably predict whether the training will diverge. In fact, our theory is able to explain and predict many more phenomena, which are all exactly observed empirically. Here we list some of them, and the detailed proofs are left in supplementary.

* **Terminating (Collapsing) Timestep.** Theorem 3 predicts that with an SGD optimizer and 2-layer MLP, the inverse of Q-value decreases linearly along the timestep, implying a terminating timestep \(T=}\). The Q-value estimation will approach infinite very quickly beside \(T\), and ends in singularity because the denominator becomes zero. From Figure 2, we can see that it is true. The terminal Q-value prediction's inverse _does_ decay linearly, and we can predict when it becomes zero to hit the singularity. Specifically, we use the data from 450th to 500th step to fit a linear regression, obtaining the green dotted line \(1/\|_{t}\|-1.44 10^{-3}t+0.87\), which predicts a singular point at \(605_{th}\) iteration. We then continue training and find that the model indeed collapses at the very point, whose predicted value and parameter become NaN.
* **Linear Norm Growth for Adam.** While Theorem 3 studies SGD as the optimizer, Adam is more adopted in real practice. Therefore, we also deduce a similar result for the Adam optimizer.

**Theorem 4**.: _Suppose we use Adam optimizer for Q-value iteration and all other settings are the same as Theorem 3. After \(t>t_{0}\), the model will diverge if and only if \(_{}()>0\). If it diverges, we have \(\|_{t}\|=t+o(t)\) and \(\|_{t}\|=(t^{L})\) where \(P\) and \(L\) are the number of parameters and the number of layers for network \(f_{}\), respectively._

Again, the detailed proof is left in the supplementary. Theorem 4 indicates that with a Adam optimizer, the norm of the network parameters grows linearly and the predicted Q-value grows as a polynomial of degree \(L\) along the time after a critical point \(t_{0}\). We verify this theorem in D4RL environments in Figure 4. We can see that the growth of the norm \(\|_{t}\|\) exhibits a straight line after a critic point \(t_{0}\). Moreover, we can see \( Q 3 t+c\), which means Q-value prediction grows cubically with time. Number 3 appears here because we use 3-layer MLP for value approximation. All these findings corroborate our theory, demonstrating our ability to accurately predict the dynamic of divergence in offline RL.

### Discussions

**Interpretation of SEEM and explain the self-excitation** The intuitive interpretation for \(_{}()\) is as below: Think about only one sample \(=(s_{0},a_{0})\) and its stabilized next step state-action

Figure 3: The divergence indication property of SEEM. The left figure shows the SEEM value with respect to different discount factors \(\). Theorem 3 states that larger \(\) has a larger SEEM value. The star point means the model’s prediction eventually exceeds a large threshold (\(10^{6}\)) in training, which means diverging. We can see that positive SEEM is perfectly indicative of the divergence. This is also true within the training process. From the middle and right figures, we can see that the prediction Q-value (in blue) is stable until the normalized kernel matrix’s SEEM (in red) rises up to a large positive value, then we can observe the divergence of the model.

\(^{*}=(s_{1},a_{1})\). Without loss of generality, we assume \(f_{_{t}}()<r+ f_{_{t+1}}(^{*})\). If SEEM is positive, we have \(_{_{t}}(^{*},)\) is larger than \(_{_{t}}(,)\). Recall that \(_{_{t}}(^{*},)\) depicts the strength of the bond between \(\) and \(^{*}\) because of generalization. So we know that, when updating the value of \(f_{}()\) towards \(r+ f_{_{t+1}}(^{*})\), the Q-value iteration inadvertently makes \(f_{_{t+1}}(^{*})\) increase even more than the increment of \(f_{_{t+1}}()\). Consequently, the TD error \(r+ f_{}(^{*})-f_{}()\) expands instead of reducing, due to the target value moving away faster than predicted value, which encourages the above procedure to repeat. This forms a positive feedback loop and causes self-excitation. Such mirage-like property causes the model's parameter and its prediction value to diverge.

**Stability of policy action \(_{_{t}}(s)\).** Previous Theorem 1 and Theorem 3 rely on the stability of the policy action \(_{_{t}}(s)\) after \(t_{0}\), and here we further elucidate why such stability occurs. Critic networks without normalization have a tendency to output large values for extreme points, _i.e._, action \(A_{ex}\) at the boundary of action space (see detailed explanation in Section 4), making these extreme points easily become policy actions during optimization. We have observed that policy actions tend to gravitate toward extreme points when Q-value divergence takes place (see Figure 15), accounting for the stability of policy action \(_{_{t}}(s)\). Consequently, as aforementioned, a loop is formed: \((s,a)\) keeps chasing \((s^{},A_{ex})\), leading the model's parameter to diverge along certain direction to infinity.

**Relations to Linear Setting.** Since the linear regression model can be regarded as a special case for neural networks, our analysis is also directly applicable to linear settings by plugging in \((,)=^{}\), which reduce to study of deadly triad in linear settings. Therefore, our analysis fully contains linear case study and extends it to non-linear value approximation. In Appendix F.5, we present that our solution (Section 4) can solve the divergence in Baird's Counterexample, which was first to show the divergence of Q-learning with linear approximation.

**Similarities and Differences to Previous works.** Our work shares similar observations of the connection between Q-value divergence and feature rank collapse with DR3 . However, our work is different in the following aspects. First, DR3 attributes feature rank collapse to implicit regularization where \(L()=0\), and it require the label noise \(\) from SGD. Actually, such near-zero critic loss assumption is mostly not true in the real practice of RL. Conversely, SEEM provides a mechanism behind feature rank collapse and Q-value divergence from the perspective of normalization-free network's pathological extrapolation behavior. It is applicable to more general settings and provides new information about this problem. Moreover, we formally prove in Theorems 3 and 4 that the model's parameter \(\) evolves linearly for the Adam optimizer, and collapses at a certain iter for SGD by solving an ODE. This accurate prediction demonstrates our precise understanding of neural network dynamics and is absent in previous work like DR3. Last, the solution to value divergence by DR3 involves searching hyperparameter tuning for \(c_{0}\) and also introduces extra computation when computing the gradient of \((s^{},a^{})\) to get \(}_{exp}()\). Our method (elaborated in Section 4) is free of these shortcomings by simple LayerNorm.

## 4 Reducing SEEM By Normalization

In Section 3, we have identified SEEM as a measure of divergence, with self-excitation being the primary catalyst for such divergence. In essence, a large SEEM value arises from the improper link between the dataset inputs and out-of-distribution data points. To gain an intuitive grasp, we visualize the NTK value in a simple 2-dimensional input space for a two-layer MLP in Figure 5. We designate a reference point \(_{0}=(0.1,0.2)\) and calculate the NTK value \(_{}(_{0},)\) for a range of \(\) in the input domain. The heatmap displays high values at the boundaries of the input range,

Figure 4: Prediction ability - Linear growth of network parameter’s norm and polynomial growth of predicted Q-value with an Adam optimizer. Please note that in the 2th and 4th figures, both Q-value and steps have been taken the logarithm base 10. Different color represents different seeds.

which suggests that even a slight increase (or decrease) in the prediction value at \(_{0}\) will result in an amplified increase (_e.g._, 4.5\(\)) in the prediction value at a distant point(_e.g._, \((4,4)\)). Note that indeed the absolute NTK surrounding \(x_{0}\) is positive. However, values farther from \(x_{0}\) are significantly larger. As a result, the NTK around \(x_{0}\) is minimal and normalized to a value close to zero. Thus, the value predictions of the dataset sample and extreme points have large NTK value and exhibit a strange but strong correlation. As Q-value network is iteratively updated, it also tend to output large values for these extreme points, which makes extreme points easily become policy actions in optimization.

This abnormal behavior contradicts the typical understanding of a "kernel function" which usually diminishes with increasing distance, which represents the MLP's inherent limitation in accurate extrapolation. This indicates an intriguing yet relatively under-explored approach to avoid divergence: _regularizing the model's generalization on out-of-distribution predictions_. The main reason for such an improperly large kernel value is that the neural network becomes a linear function when the input's norm is too large . A more detailed explanation about linearity can be found at Appendix G. Therefore, a simple method to accomplish this would be to insert a LayerNorm prior to each nonlinear activation. We conduct a similar visualization by equipping the MLP with LayerNorm . As shown in Figure 5 right, the value reaches its peak at \(_{0}\) and diminishes as the distance grows, demonstrating excellent local properties for well-structured kernels.

Such architectural change is beneficial for controlling the eigenvalue of \(\) by reducing the value of improperly large entries since matrix inequality tells us \(_{}()\|\|_{F}\). Therefore, it is expected that this method can yield a small SEEM value, ensuring the convergence of training with minimal bias on the learned policy. We also provide theoretical justification explaining why LayerNorm results in a lower SEEM value in the supplementary material. This explains why LayerNorm, as an empirical practice, can boost performance in previous online and offline RL studies [41; 5; 25; 28]. Our contribution is thus two-fold: 1) We make the first theoretical explanation for how LayerNorm mitigates divergence through the NTK analysis above. 2) We conduct thorough experiments to empirically validate its effectiveness, as detailed in Section Section 5.

To validate the effectiveness of LayerNorm in practical offline RL settings, we select the walker2d-medium-expert-v2 task in D4RL to showcase how Q-value and SEEM evolves as the training proceeds. For comparative analysis, we also consider four popular regularization and normalization techniques, including BatchNorm , WeightNorm , dropout , and weight decay . We

Figure 5: The _normalized_ NTK map for 2-layer ReLU MLP with and without layernorm. The input and hidden dimensions are 2 and 10,000. We can see that for the MLP without LayerNorm, prediction change at \(_{0}\) has a dramatic influence on points far away like \(=(4,4)\), meaning a slight change of \(f(_{0})\) will change \(f()\) dramatically (\( 4.5\)). However, MLP equipped with layernorm exhibits good local property.

Figure 6: The effect of various regularizations and normalizations on SEEM and Q-value. LayerNorm yields a low SEEM and achieves stable Q-learning. Given the substantial disparity in the y-axis range among various regularizations, we present the results using two separate figures.

use TD3 as the baseline algorithms. To simplify our analysis, the target network is chosen to be the current Q-network with gradient stop instead of an Exponential Moving Average (EMA) of past networks. In our experiments, it is observed that LayerNorm and WeightNorm constrain SEEM and restrain divergence. However, weight decay and dropout did not yield similar results. As illustrated in Figure 6, weight decay and dropout evolve similar to the unregularized TD3 w.o. EMA baseline. WeightNorm reduces SEEM by a clear margin compared to the baseline, thus demonstrating slighter divergence. In the right figure, we observe that behavior cloning effectively lowers SEEM at the cost of introducing significant explicit bias. Importantly, LayerNorm achieves both a low SEEM and stable Q-values without necessitating explicit policy constraints. A notable outlier is BatchNorm. BatchNorm attains a relatively low maximum eigenvalue at the beginning of training but experiences an increase over time. Correspondingly, the Q-curve displays substantial oscillations. This instability could be ascribed to the shared batch normalization statistics between the value network and the target network, despite their input actions originating from distinct distributions .

## 5 Agent Performance

Previous state-of-the-art offline RL algorithms have performed exceptionally well on D4RL Mujoco Locomotion tasks, achieving an average score above 90 [2; 41]. In this section, we compare our method with various baselines on two difficult settings, _i.e._, Antmaze and X% Mujoco task, to validate the effectiveness of LayerNorm in offline settings. Further experiments can be found at Appendix F.

### Standard Antmaze Dataset

In Antmaze tasks characterized by sparse rewards and numerous suboptimal trajectories, the prior successful algorithms either relies on in-sample planning, such as weighted regression [26; 8], or requires careful adjustment the number of ensembles per game . Algorithms based on TD3 or SAC failed to achieve meaningful scores by simply incorporating a behavior cloning (BC) term . Even Diff-QL, which replaces TD3+BC's Gaussian policy with expressive diffusion policies to capture multi-modal behavior , continues to struggle with instability, leading to inferior performance.

We first show policy constraint (BC) is unable to control q-value divergence while performing well in some challenging environments, by using Antmaze-large-play as an running example. We conduct experiments with Diff-QL by varying the BC (_i.e._, diffusion loss) coefficient from 0.5 to 10. As shown in Figure 7, when the policy constraint is weak (BC 0.5), it initially achieves a decent score, but as the degree of off-policy increases, the value starts to diverge, and performance drops to zero. Conversely, when the policy constraint is too strong (BC 10), the learned policy cannot navigate out of the maze due to suboptimal data, and performance remains zero. In contrast, simply incorporating LayerNorm into Diff-QL, our method ensures stable value convergence under less restrictive policy constraints (BC 0.5). This results in consistently stable performance in the challenging Antmaze-large-play task. We report the overall results in Table 1, which shows that our method consistently maintains stable average scores across six distinct environments. Additionally, we are able to boost the highest score, while adopting the same evaluation metric used by Diff-QL. Ultimately, our method achieves an average score of 80.8 on Antmaze tasks, exceeding the performance of previous SOTA methods. In summary, our experimental findings demonstrates within suboptimal datasets, an overly strong policy constraint is detrimental, while a weaker one may lead to value divergence. LayerNorm proves effective in maintaining stable value convergence under less restrictive policy constraints,

   Dataset & TD3+BC & IQL & MSG & sfBC & diff-QL & ours \\  antmaze-umaze-v0 & 40.2 & 87.5 & **98.6** & 93.3 & 95.6 (96.0) & \(94.3 0.5\) (97.0) \\ antmaze-umaze-diverse-v0 & 58.0 & 62.2 & 76.7 & 86.7 & 69.5 (84.0) & \(\) (95.0) \\ antmaze-medium-play-v0 & 0.2 & 71.2 & 83.0 & **88.3** & 0.0 (79.8) & \(85.6 1.7\) (92.0) \\ antmaze-medium-diverse-v0 & 0.0 & 70.0 & 83.0 & **90.0** & 6.4 (82.0) & \(83.9 1.6\) (90.7) \\ antmaze-large-play-v0 & 0.0 & 39.6 & 46.8 & 63.3 & 1.6 (49.0) & \(\) (74.0) \\ antmaze-large-dverse-v0 & 0.0 & 47.5 & 58.2 & 41.7 & 4.4 (61.7) & \(\) (75.7) \\ average & 16.4 & 63.0 & 74.4 & 77.2 & 29.6 (75.4) & \(\) (87.4) \\   

Table 1: Averaged normalized scores of the last ten consecutive checkpoints on Antmaze tasks over 10 seeds. diff-QL  reports the best score during the whole training process in its original paper. We rerun its official code for the average score. We also compare our method with diff-QL by the best score metrics in the parenthesis. The best score of diff-QL is directly quote from its paper.

resulting in an excellent performance. The ablation study regarding the specific configuration of adding LayerNorm can be found at Appendix F.4. The effect of other regularizations in Antmaze can be found at F.3. We also experimented with the regularizer proposed in DR3 . Despite conducting a thorough hyperparameter search for the coefficient, it was observed that it had no substantive impact on enhancing the stability and performance of diff-QL.

### \(X\%\) Offline Dataset

Past popular offline RL algorithms primarily require transitions rather than full trajectories for training [40; 31; 26; 12; 7; 51]. A superior algorithm should extrapolate the value of the next state and seamlessly stitch together transitions to form a coherent trajectory. However, commonly used benchmarks such as D4RL and RL Unplugged [10; 16] contain full trajectories. Even with random batch sampling, complete trajectories help suppress value divergence since the value of the next state \(s_{t+1}\) will be directly updated when sampling \(\{s_{t+1},a_{t+1},s_{t+2},r_{t+1}\}\). On the one hand, working with offline datasets consisting solely of transitions enables a more authentic evaluation of an algorithm's stability and generalization capabilities. On the other hand, it is particularly relevant in real-world applications such as healthcare and recommendation systems where it is often impossible to obtain complete patient histories.

We first evaluate the performance of offline algorithms within a transition-based scenario. We construct transition-based datasets by randomly sampling varying proportions (\(X\%\)) from the D4RL Mujoco Locomotion datasets. Here, we set several levels for \(X\{1,10,50,100\}\). When X equals 100, it is equivalent to training on the original D4RL dataset. As X decreases, the risk of encountering out-of-distribution states and actions progressively escalates. To ensure a fair comparison, we maintain the same subset across all algorithms. We evaluate every algorithm on Walker2d, hopper, and Halfcheetah tasks with medium, medium replay, and medium expert levels. For comparison, we report the total score on nine Mujoco locomotion tasks and the standard deviation over 10 seeds. Figure 8 reveal a marked drop in performance for all popular offline RL algorithms when the dataset is reduced to 10%. When the transition is very scarce (1%), all baselines achieve about only 50 to 150 total points on nine tasks. Further, we observed value divergence in all algorithms, even for algorithms based on policy constraints or conservative value estimation.

Subsequently, we demonstrate the effectiveness of LayerNorm in improving the poor performance in X% datasets. By adding LayerNorm to the critic, value iteration for these algorithms becomes non-expansive, ultimately leading to stable convergence. As Figure 9 depicts, under 10% and 1% dataset, all baselines are greatly improved by LayerNorm. For instance, under the 1% dataset, LayerNorm

Figure 8: The performance of offline RL algorithms with the varying \(X\%\) Mujoco Locomotion dataset. As the number of transitions decreases, all algorithms have a dramatic drop in performance.

Figure 7: The detrimental bias in policy constraint. Although strong policy constraints succeed in depressing divergence, constraining the policy around sub-optimal behavior sacrifices performance. With LayerNorm to regularize SEEM without introducing a too strong bias, our method can achieve both stable Q-value convergence and excellent performance. The shallow curves represent the trajectory of individual seeds, while the darker curve denotes the aggregate across 3 seeds.

enhances the performance of CQL from 50 points to 300 points, marking a 6x improvement. Similarly, TD3+BC improves from 70 to 228, representing approximately a 3x improvement. This empirical evidence underscores the necessity of applying normalization to the function approximator for stabilizing value evaluation, particularly in sparse transition-based scenarios. Adding LayerNorm is shown to be more effective than relying solely on policy constraints. Another notable observation from Figure 9 is that with sufficient data (the 100% dataset) where previous SOTA algorithms have performed excellently, LayerNorm only marginally alters the performance of algorithms. This suggests that LayerNorm could serve as a universally applicable plug-in method, effective regardless of whether the data is scarce or plentiful.

### Online RL Experiments - LayerNorm Allows Online Methods without EMA.

It is natural to ask whether our analysis and solution are also applicable to online settings. Previous works [34; 13; 17; 61] has empirically or theoretically shown the effect of EMA in stabilizing Q-value and prevent divergence in online setting. To answer the question, We tested whether LayerNorm can replace EMA to prevent value divergence. We conducted experiments in two environments in online settings: Hopper and Walker. Please refer to Figure 11 for the curves of return. Surprisingly, we discovered that LayerNorm solution allows the SAC without EMA to perform equivalently well as the SAC with EMA. In contrast, SAC without EMA and LayerNorm behaved aimlessly, maintaining near-zero scores. It reveals LayerNorm (or further regularizations discovered by SEEM later) have the potential of allowing online DQN-style methods to step away from EMA/frozen target approaches altogether towards using the same online and target networks. These results partially reveals potential implications of SEEM in an online learning setting.

## 6 Conclusion and Limitations

In this paper, we delve into the Q-value divergence phenomenon of offline RL and gave a comprehensive theoretical analysis and explanation for it. We identified a fundamental process called _self-excitation_ as the trigger for divergence, and propose an eigenvalue measure called SEEM to reliably detect and predict the divergence. Based on SEEM, we proposed an orthogonal perspective other than policy constraint to avoid divergence, by using LayerNorm to regularize the generalization of the MLP neural network. We demonstrated empirical and theoretical evidence that our method introduces less bias in learned policy, which has better performance in various settings. Moreover, the SEEM metric can serve as an indicator, guiding future works toward further engineering that may yield adjustments with even better performance than the simple LayerNorm. Despite the promising results, our study has some limitations. we have not incorporated the impact of Exponential Moving Average (EMA) into our analysis. Also, the existence of critical point is still not well-understood. Moreover, our analysis does not extend to online RL, as the introduction of new transitions disrupts the stability of the NTK direction. All these aspects are left for future work.

## 7 Acknowledgement

This work is supported by the National Key R&D Program of China (2022ZD0114900).

Figure 9: The performance difference between baseline with LayerNorm and without it using the same \(X\%\) dataset. The error bar represents the standard deviation over 10 seeds.