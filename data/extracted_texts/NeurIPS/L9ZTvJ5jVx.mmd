# Latent Field Discovery In Interacting Dynamical Systems With Neural Fields

Militadis Kofinas

University of Amsterdam

m.kofinas@uva.nl

&Erik J. Bekkers

University of Amsterdam

e.j.bekkers@uva.nl

&Naveen Shankar Nagaraja

BMW Group

Naveen-Shankar.Nagaraja@bmw.de

&Efstratios Gavves

University of Amsterdam

egavves@uva.nl

###### Abstract

Systems of interacting objects often evolve under the influence of field effects that govern their dynamics, yet previous works have abstracted away from such effects, and assume that systems evolve in a vacuum. In this work, we focus on discovering these fields, and infer them from the observed dynamics alone, _without_ directly observing them. We theorize the presence of latent force fields, and propose neural fields to learn them. Since the observed dynamics constitute the net effect of local object interactions and global field effects, recently popularized equivariant networks are inapplicable, as they fail to capture global information. To address this, we propose to _disentangle_ local object interactions -which are \((n)\) equivariant and depend on relative states- from external global field effects -which depend on absolute states. We model interactions with equivariant graph networks, and combine them with neural fields in a novel graph network that integrates field forces. Our experiments show that we can accurately discover the underlying fields in charged particles settings, traffic scenes, and gravitational n-body problems, and effectively use them to learn the system and forecast future trajectories.

## 1 Introduction

Systems of interacting objects are omnipresent in nature, with examples ranging from the subatomic to the astronomical scale -including colliding particles and n-body systems of celestial objects- as well as settings that involve human activities, governed by social dynamics, like traffic scenes. The majority of these systems does not evolve in a vacuum; instead, systems evolve under the influences of underlying fields. For example, electromagnetic fields may govern the dynamics of charged particles, while galaxies swirl around supermassive black holes that create gravitational fields. In traffic scenes, the road network and traffic rules govern the actions of traffic scene participants. Despite the ubiquity of fields, previous works on modelling interacting systems have only focused on the _in vitro_ case of systems evolving in a vacuum.

Earlier work on learning interacting systems proposed graph networks . Recently, state-of-the-art methods for interacting systems propose _equivariant_ graph networks  to

Figure 1: N-body system with underlying gravitational field. We uncover fields that underlie interacting systems using only the observed trajectories.

model dynamics while respecting the symmetries that often underlie them. These networks exhibit increased robustness and performance, while maintaining parameter efficiency due to weight sharing. They are, however, not compatible with underlying field effects, since they can only capture local states, such as relative positions, while fields depend on absolute states (_e.g._ positions or orientations). In other words, _global fields violate the strict equivariance hypothesis_.

Within the context of modelling interacting systems, a function \(f\) that predicts future trajectories is \((3)\) equivariant -equivariant to the special Euclidean group of translations and rotations- if \(f(+)=f()+\) for a translation vector \(\) and a rotation matrix \(\). While strict equivariance holds in idealized settings, it does not hold in many real-world settings. That is, even if the symmetries exist in a particular setting, they only manifest themselves in local interactions, yet they are entangled with global effects that stem from absolute states. N-body systems from physics, for example, exhibit \((3)\) symmetries, since gravitational forces only depend on relative positions. Dynamics, however, may be influenced by external force fields, _e.g._ black holes, which are either unknown or not subject to transformations. Thus, strict equivariance is violated, since equivariant object interactions are _entangled_ with global field effects.

We make the following contributions. First, we introduce neural fields to discover global latent force fields in interacting dynamical systems, and infer them by observing the dynamics alone. Second, we introduce the notion of _entangled equivariance_ that intertwines global and local effects, and propose a novel architecture that disentangles equivariant local object interactions from global field effects. Third, we propose an approximately equivariant graph network that extends equivariant graph networks by using a mixture of global and local information. Finally, we conduct experiments on a number of field settings, including real-world traffic scenes, and extending state-of-the-art setups from the literature. We observe that explicitly modelling fields is mandatory for effective future forecasting, while their unsupervised discovery opens a window for model explainability.

We term our method _Aether_, inspired by the postulated medium that permeates all throughout space and allows for the propagation of light.

## 2 Background

Interacting dynamical systemsAn interacting dynamical system comprises trajectories of \(N\) objects in \(d\) dimensions, \(d\{2,3\}\), recorded for \(T\) timesteps. The snapshot of the \(i\)-th object at timestep \(t\) describes the state \(_{i}^{t}=[_{i}^{t},_{i}^{t}],i\{1,,N\},t \{1,,T\}\), where \(^{d}\) denotes the position and \(^{d}\) denotes the velocity, using \([,]\) to denote vector concatenation along the feature dimension. We are interested in forecasting future trajectories, _i.e._ predict the future states for all objects and for a number of timesteps. Interacting dynamical systems can be naturally formalized as spatio-temporal geometric graphs [3; 23; 14], \(=\{^{t}\}_{t=1}^{T}\), with graph snapshots \(^{t}=(^{t},^{t})\) at different time steps. The set of graph nodes \(^{t}=\{v_{1}^{t},,v_{N}^{t}\}\) describes the objects in the system; \(v_{i}^{t}\) corresponds to \(_{i}^{t}\). The set of edges \(^{t}\{(v_{j}^{t},v_{i}^{t})(v_{j} ^{t},v_{i}^{t})^{t}^{t}\}\) describes pair-wise object interactions; \((v_{j}^{t},v_{i}^{t})\) corresponds to an interaction from node \(j\) to node \(i\). Finally, \((i)\) denotes the neighbors of node \(v_{i}\).

Local coordinate frame graph networksLocal coordinate frame graph networks have been popularized in recent years [24; 26; 10; 52; 20; 30] as a method to achieve \((3)\) -or \((3)\)- equivariance, due to their low computational overhead and high performance. Kofinas et al.  proposed LoCS and introduced local coordinate frames for all node-objects at all timesteps. They define augmented node states \(_{i}^{t}=[_{i}^{t},_{i}^{t}, _{i}^{t}]\), where \(_{i}^{t}\) denotes the angular position of node \(i\) at timestep \(t\). Kofinas et al.  use velocities as a proxy to angular positions, while Luo et al.  use another network that predicts latent orientations. Each local coordinate frame is translated to match the target object's position and rotated to match its orientation. Considering the representation of node \(j\) in the local coordinate frame of node \(i\), denoted as \(_{j|i}^{t}\), they first compute the relative positions \(_{j,i}^{t}=_{j}^{t}-_{i}^{t}\) and then they rotate the state using the matrix representation of the angular position \((_{i}^{t})\):

\[_{j|i}^{t}=}_{i}^{t} ^{}_{j,i}^{t},_{j}^{t},_{j}^{t},\] (1)where \(}(_{i}^{t})=(_{i}^{t}) (_{i}^{t})(_{i}^{t})\), and \(\) denotes a direct sum. LoCS then proposes a graph neural network [42; 25; 13] that uses local states:

\[_{j,i}^{t} =f_{e}_{j|i}^{t},_{i|i}^{t} ,\] (2) \[_{i|i}^{t+1} =f_{v}g_{v}_{i|i}^{t}+C_{j (i)}\!\!\!_{j,i}^{t},\] (3)

where \(f_{v},f_{e}\), and \(g_{v}\) are MLPs, and \(C=}{{|(i)|}}\). The output of this graph network comprises differences in positions and velocities from the previous time step, in the local frame of each object. Since these outputs are invariant, LoCS performs an inverse transformation to convert them back to the global coordinate frame and achieve equivariance, \(_{i}^{t+1}=_{i}^{t}+(_{i}^{t}) _{i|i}^{t+1}\), where \((_{i}^{t})=(_{i}^{t})(_{i}^{t})\).

Neural fieldsFinally, we make a brief introduction to neural fields. Neural fields, or coordinate-based MLPs, are a class of neural networks that parameterize fields using neural networks (see Xie et al.  for a survey). They take as input states like spatial coordinates and predict some quantity. Neural fields can learn prior behaviors and generalize to new fields via conditioning on a latent variable \(\) that encodes the properties of a field. Perez et al.  proposed Feature-wise Linear Modulation (FiLM), a conditioning mechanism that modulates a signal. It comprises two sub-networks \(,\) that perform multiplicative and additive modulation to the input signal, and can be described by \((,)=()+( )\), where \(\) is the conditioning variable, \(\) is the signal to be modulated, and \(,\) are MLPs that scale the signal, and add a bias term, respectively.

## 3 Method

In this section, we present our method, termed _Aether_. First, we describe the notion of entangled equivariance, and introduce our architecture that disentangles global field effects from local object interactions. Then, we continue with the description of the neural field that infers latent fields by observing the dynamics alone. Finally, we formulate approximately equivariant global-local coordinate frame graph networks. We note that throughout this work, we focus on fields that are unaffected by the observable objects and their interactions thereof.

### Aether

Interacting dynamical systems rarely evolve in a vacuum, rather they evolve under the influence of external field effects. While object interactions depend on local information, the underlying fields depend on global states. On the one hand, locality in object interactions stems from the fact that dynamics obey a number of symmetries. By extension, object interactions are equivariant to a particular group of transformations. On the other hand, field effects are non-local; they depend on absolute object states. Thus, strict equivariance is violated, since equivariant object interactions are entangled with global field effects. We refer to this phenomenon as _entangled equivariance_.

As an example, in Figure 2 we observe a system of two objects that evolve in a gravitational field. The arrows positioned _on_ the objects represent the forces exerted on them. One constituent of the net force is caused by object interactions, and is thus equivariant, while the other can be attributed to the gravitational pull. However, we can only observe the net force at each particle, _i.e._ the sum of equivariant pairwise forces and non-equivariant field effects. Hence, in this system, we say that _equivariance is entangled_.

We now propose our architecture that disentangles local object interactions from global field effects. We model object interactions with local coordinate frame graph networks , and field effects with neural fields. During training, and given a multitude of input systems, neural fields will, in principle, be able to isolate

Figure 2: Two objects in a gravitational field. We only observe the total force exerted at each particle, _i.e._ the sum of equivariant pairwise particle forces and global field effects.

global from local effects, since only global effects are recurring phenomena. We hypothesize that field effects can be attributed to force fields, and therefore, our neural fields learn to discover _latent force fields_. The pipeline of our method is shown in Figure 3. Our inputs comprise augmented states \(\{_{i}^{t}\}\) for trajectories of \(N\) objects for \(T\) timesteps. Since neural fields model global fields, they depend on absolute states. Thus, we feed the states of the trajectories \(_{i}^{t}\) -or a subset of state variables- as input to a neural field that predicts latent forces \(_{i}^{t}=(_{i}^{t})\).

The predicted field forces can be now considered part of the node states, and further, they can be treated similarly to other state variables like velocities; as vectors, forces are unaffected by the action of translations, while they covariantly transform with rotations. Thus, moving outward, we can treat the problem setup as if we were once again back in the strict equivariance regime. We append the predicted forces \(_{i}^{t}\) for each node-object \(i\) and each timestep \(t\) to the node states, and transform them to corresponding local coordinate frames, similarly to Equation (1). Namely, the force exerted on node \(j\), expressed in the local coordinate frame of node \(i\) is computed as: \(_{j|i}^{t}=^{}(_{i}^{t})_{j}^{t}\). We feed the new local node states to a local coordinate frame graph network as follows:

\[_{j,i}^{t} =f_{e}_{j|i}^{t},_{j|i }^{t}},_{i|i}^{t},_{i|k}^{t}}\] (4) \[_{i|i}^{t+1} =f_{v}g_{v}_{i|i}^{t},_{i|k}^{t}}+C(i)}{} _{j,i}^{t}\] (5) \[_{i}^{t+1} =_{i}^{t}+_{i}^{t} _{i|i}^{t+1},\] (6)

where \((_{i}^{t})=(_{i}^{t})(_{i}^{t}),C=}{{(i)}}\). The equations above are similar to Equations (2) and (3), with the addition of the highlighted parts that denote the predicted forces expressed at local coordinate frames. In practice, in most experiments, we closely follow [23; 14; 24] and formulate our model as a variational autoencoder [21; 37] with latent edge types. The exact details are presented in Appendix A.1.2.

### Field discovery

Oftentimes, fields might not be directly observable for us to probe them at will and use them for supervision. For example, astronomical observations of solar systems and galaxies might not include black holes, yet we can observe their effects. Moreover, fields are often not even measurable or quantifiable, or they are defined implicitly. For instance, "social fields" that guide traffic, cannot be measured or defined explicitly, but we can safely assume they exist. Motivated by these observations, we design an architecture that performs _unsupervised field discovery_, while solving the surrogate supervised task of trajectory forecasting.

Figure 3: The pipeline of our method, _Aether_. In the latent neural field (a), a graph aggregation module summarizes the input trajectories in a latent variable \(\). Query states from input trajectories, alongside \(\), are fed to a neural field that predicts a latent force field. In (b), a graph network integrates predicted forces with input trajectories to predict future trajectories. The graph aggregation module and the FiLM layers exist only in a dynamic field setting.

In this work, we aim to discover two different types of fields, which we term "static" and "dynamic" fields. Static fields refer to settings in which we have a single field shared throughout the whole dataset. On the other hand, dynamic fields refer to settings in which we have a different field for each input system, and consequently, fields also differ between train, validation, and test sets.

We now describe neural fields, used in this work, to model the underlying field effects. Neural fields depend on absolute states and predict latent force fields. When dealing with _static_ fields, we use _unconditional_ neural fields, _i.e._ neural fields that are functions only of the query states, as the field values are common across data samples. Note that unconditional neural fields are _not_ functions of the input states; they will make the same predictions regardless of the inputs. In contrast, for _dynamic_ fields, we use a _conditional_ neural field, _i.e._ a neural field that also depends on a latent vector \(^{D_{}}\) that represents the underlying field. The latent \(\) will be inferred from the input trajectories and can be thought of as representing unusual non-equivariant dynamics. We use \(\) to explicitly condition the neural field, and thus, its general form is \(:^{d}(d)^{d} ^{D_{}}^{d}\), where \(d\{2,3\}\), depending on the setting.

During training, both for conditional and unconditional neural fields, we only sample the field at query states that coincide with the states of the input objects, since we only have supervision about their future trajectories there.

Static fieldsWe start with the description of unconditional neural fields used in static field settings, since conditional neural fields share the same backbone. First, we encode the query positions using Gaussian random Fourier features , as follows: \(()=[(2),(2)]^{}\), where \(^{d}\) are the query coordinates, and \(^{}{2} d}\) is a matrix with entries sampled from a Gaussian distribution, \(_{kl}(0,^{2})\). The variance \(^{2}\) can be chosen per task with a hyperparameter sweep.

We encode velocities using a simple linear layer \(()=_{u}\). For orientations, in \(d=3\) dimensions, we use a unit vector representation for each angle in \(=(,,)^{}\), \(}=[,]^{}\). In \(d=2\) dimensions, we use the same encoding, except that we now have a single angle \(=\). Then, we use a linear layer to encode the orientation vectors, \(()=_{}}\). We finally concatenate the encoded positions, orientations, and velocities in a single vector that is being fed as input to the neural field. The neural field is a 3-layer MLP with SiLU  activations in-between, and outputs a latent force field, \(()=([(),(),()])\).

Dynamic fieldsThe neural fields used to model the dynamic fields are conditioned on a latent vector representation \(^{D_{}}\) that describes prior knowledge about the underlying field, and are defined as \(()\). In our case, the latent representation should "summarize" the input graph such that it isolates only global effects from the field. To that end, we employ a simple global spatio-temporal attention mechanism, similar to Li et al. , that aggregates the input system in a latent vector representation. First, we define object embeddings \(_{i}=_{g}_{i}^{1:T}\), where \(_{g}\) is a matrix used to linearly transform the inputs, and \(\) is the Gated Recurrent Unit . We also define temporal embeddings \(=(t)\), where \(\) are positional encodings . Using these embeddings, we augment the input as \(_{i}^{t}=[_{i}^{t},_{i}]+\). The aggregation is then defined as follows:

\[=_{i,t}f_{a}_{i}^{t}  f_{b}_{i}^{t},\] (7)

where \(f_{a}:^{D_{}},f_{b}:^{D_{}} ^{D_{}}\) are 2-layer MLPs with SiLU activations in-between.

After having obtained a latent vector representation \(\) that summarizes the input system, we condition the neural field using FiLM . We include FiLM layers after the first two linear layers of the neural field. The exact details are presented in Appendix A.1.1.

### Approximate equivariance with global-local coordinate frames

Equivariant neural networks cannot capture non-local information, such as global field effects. In this work, we explicitly aim to discover these fields and disentangle them from local object interactions. An alternative, or rather complementary approach, would be to directly combine global and local information, following the recently proposed notion of _approximate equivariance_. Starting from LoCS , we can integrate global information and still operate in local coordinate framesby defining an auxiliary node-object corresponding to the global coordinate frame, _i.e._ an object positioned at the origin, and oriented to match the x-axis.

Similar to all objects in the system, the full state of the origin node \(\) comprises the concatenation of its position and velocity, \(_{}=[_{},_{}]\). We use an "artificial" velocity that matches the \(x\)-axis in order to compute a non-degenerate frame. As such, we have \(_{}=[,}]\). The origin state can be expressed in the local coordinate frame of the \(i\)-th object similarly to Equation (1), as follows:

\[^{t}_{|i}=^{t}_{i}^{t}_{ }-^{t}_{i},^{t}_{}=^{t}_{i}-^{t}_{i},^{t}_{}.\] (8)

Since graph networks are permutation equivariant, we need to explicitly distinguish between the origin node and other nodes. We circumvent that by augmenting each object's state with the origin node information expressed in local coordinate frames, extending Equations (2) and (3) to

\[^{t}_{j,i} =f_{e}^{t}_{j|i},^{t}_{i|i}, \,,\] (9) \[^{t+1}_{i|i} =f_{v}g_{v}^{t}_{i|i}, +(i)|}_{j (i)}^{t}_{j,i}.\] (10)

This approach pushes the information in the node states, and removes the need to add the origin node to the actual graph. We term this method _G-LoCS_ (**G**lobal-**L**ocal **C**oodinate Frame**S**). In practice, similar to Aether, we formulate G-LoCS as a variational autoencoder [21; 37] with latent edge types. The full details are presented in Appendix A.2. Finally, in practice, we integrate G-LoCS in Aether, since it can enhance the performance of our method.

## 4 Related work

Equivariant graph networksThe seminal works of [8; 9; 53] introduced equivariant convolutional neural networks and demonstrated effectiveness, robustness, and increased parameter efficiency. Recently, many works have proposed equivariant graph networks [43; 47; 12; 50; 41; 24; 5; 26; 18]. Walters et al.  propose rotationally equivariant continuous convolutions for trajectory prediction. Satorras et al.  propose a computationally efficient equivariant graph network that leverages invariant euclidean distances between node pairs. Kofinas et al.  introduce roto-translated local coordinate frames for all objects in an interacting system and propose equivariant local coordinate frame graph networks. Brandstetter et al.  generalize equivariant graph networks using steerable MLPs  and incorporate geometric and physical information in message passing. Equivariant graph networks differ from our work since they cannot capture non-local information, while our work disentangles equivariant local interactions from global effects and captures them both.

Approximate equivarianceRecently, a number of works has proposed to shift away from strict equivariance, in what Wang et al.  termed as _approximate equivariance_. Wang et al.  propose approximately equivariant networks for dynamical systems, by relaxing equivariance constraints in group convolutions and steerable convolutions. van der Ouderaa et al.  propose to relax strict equivariance by interpolating between equivariant and non-equivariant operations, using non-stationary kernels that also depend on the absolute input group element. Romero and Lohit  propose Partial G-CNNs that learn layer-wise partial equivariances from data. We note that even though approximately equivariant networks share similarities with our work, our notion of disentangled equivariance is conceptually different. That is because related work uses the term approximate equivariance to denote that equivariance is "broken" due to noise or imperfections, while our work disentangles the system dynamics that are actually equivariant, from the global field effects that are not, and in fact, might be unaffected by such transformations. Further, to the best of our knowledge, approximate equivariance has only been studied in the context of convolutional networks, not in the context of graph networks and interacting systems. Tangentially, Han et al.  propose subequivariant graph networks, and relax equivariance to subequivariance by considering external fields like gravity. However, they assume a priori known fields that do not require to be inferred by the model.

Neural fieldsNeural fields have recently exploded in popularity in 3D computer vision, popularized by NeRF . Since MLPs are universal function approximators , neural fields parameterized by MLPs can, in principle, encode continuous signals at arbitrary resolution. However, neural networks can suffer from "spectral bias" [34; 2], _i.e._ they are biased to fit functions with low spatial frequency. To address this issue, a number of solutions have been proposed. Tancik et al.  leverage Neural Tangent Kernel (NTK) theory and propose Random Fourier Features (RFF), showing that they can overcome the spectral bias. They also show that RFF are a generalization of positional encodings, popularized in recent years in natural language processing by Transformers . Concurrently, Sitzmann et al.  proposed SIREN, neural networks with sinusoidal activation functions. While neural fields have been used extensively in computer vision problems including 3D scene reconstruction [31; 28] and differentiable rendering [44; 29], they have not seen wide usage in dynamical systems. Notably, Raissi et al.  proposed Physics-Informed Neural Networks (PINNs), neural PDE solvers based on neural fields. Finally, Dupont et al.  and Zhuang et al.  propose generative models of neural fields.

## 5 Experiments

We evaluate our proposed method, _Aether_, on settings that include static as well as dynamic fields. First, we explore 2D charged particles that evolve under the effect of a static electrostatic field, as well as 3D particles that evolve under a Lorentz force field . Then, we evaluate our method on a subset of inD  that contains a single location, and thus a static field as well. Finally, we explore 3D gravitational n-body problems  with dynamic fields. Our code, data, and models will be open-sourced online1.

In most experiments, we compare our method against dNRI  and LoCS , two state-of-the-art networks for sequence-to-sequence trajectory forecasting, as well as G-LoCS. DNRI  is a graph network operating in global coordinates, and is, in principle, able to uncover both the global and the local dynamics. It is formulated as a VAE [21; 37] with latent edge types and explicitly infers a latent graph structure. LoCS , on the other hand, operates in local coordinates, and is, thus, unable to uncover the global dynamics. Finally, G-LoCS is in principle able to model both local and global dynamics effectively. For all methods, we use their publicly available source code.

Our architecture and experimental setup closely follow Graber and Schwing , Kofinas et al. . Unless specified differently, our neural field has a hidden size of 512. In charged particles and in n-body problems, we only use positions as input to the neural field, while in traffic scenes we also use orientations. The full implementation details are presented in Appendix A.1.2. In all settings, we report the mean squared error (MSE) of positions and velocities over time. Here we demonstrate indicative visualizations, and provide more extensive qualitative results in Appendix D.

For the Lorentz force field experiment, we use the official source code from ClofNet , and follow their exact setup. We compare our method against \((3)\) Transformers , EGNN , and ClofNet . We evaluate methods using the mean squared error between predicted and groundtruth positions. Since this setting is not a sequence-to-sequence task, we use a simplified network architecture _without_ a VAE, and following baselines, we make sure that the number of parameters of our model is approximately equal to other methods. The full implementation details are presented in Appendix A.1.3.

Figure 4: Results on (a) electrostatic field, (b) inD, and (c) gravity.

### Electrostatic field

First, we study the effect of static fields, _i.e._ a single field across all train, validation, and test simulations. We extend the charged particles dataset from Kipf et al.  by adding a number of immovable sources. These sources act like regular particles, exerting forces on the observable particles, except we ignore any forces exerted to them, and keep their positions fixed. We use \(M=20\) "source" particles and \(N=5\) "observable" particles. We generate 50,000 simulations for training, 10,000 for validation and 10,000 for testing. Following Kipf et al. , each simulation lasts for 49 timesteps. During inference, we use the first 29 steps as input and predict the remaining 20 steps. The full dataset details are presented in Appendix B.1.

We compare our method against dNRI, LoCS, and G-LoCS. We plot MSE in Figure 4a and \(L_{2}\) errors in Figure 18, and visualize the learned field in Figure 5. We showcase predicted trajectories in Figure 9 in Appendix D.1. We observe that equivariant methods like LoCS perform poorly, while the approximately equivariant G-LoCS performs much better than equivariant and non-equivariant methods. Aether outperforms all other methods, demonstrating that it can _disentangle equivariance_. Furthermore, as shown in Figure 5, and Figure 10 in Appendix D.1.1, _Aether can effectively discover the underlying field_.

### Lorentz force field

Du et al.  introduced a dataset of 3D charged particles evolving under the influence of a Lorentz force field. Each simulation contains 20 particles. We use the official source code and follow the exact experimental setup with Du et al. . We show quantitative results in Table 1. Our method can clearly outperform all other methods by a large margin, reducing the error by \(48.6\%\). We also note that our method has fewer parameters than ClofNet, and is thus more efficient.

### Traffic scenes

Next, we study the effectiveness of static field discovery in traffic scenes. We use inD , a dataset with real-world traffic scenes that comprises trajectories of pedestrians, vehicles, and cyclists. We create a subset that contains scenes from a single location. The full dataset details are presented in Appendix B.2. We divide scenes into 18-step sequences; we use the first 6 time steps as input and predict the next 12 time steps. We plot MSE in Figure 4b and \(L_{2}\) errors in Figure 19, and visualize the learned field in Figure 6, and in Figure 15 in Appendix D.2.1. Since the learned field is a function of positions and orientations, we only visualize it for 4 discrete orientations, namely the group \(_{4}=\{0,,,\}\). We showcase predictions in Figure 11 in Appendix D.2. Again, Aether outperforms all other methods. The discovered field, while hard to interpret, shows high activations that coincide with road locations _and_ directions, indicating that it can guide objects through the topology of the road network.

   Method & MSE (\(\)) & No. parameters \\  GNN \(\) & 0.0908 & 104,387 \\ SE(3) Transformer \(\) & 0.1438 & 1,763,134 \\ EGNN \(\) & 0.0368 & 134,020 \\ ClofNet \(\) & 0.0251 & 160,964 \\ Aether (ours) & **0.0129** & 132,822 \\   

Table 1: Position prediction MSE on Lorentz force field. Results marked with \(\) were taken from ClofNet .

Figure 5: Learned Field (left) in electrostatic field setting compared to groundtruth (right).

### Gravitational field

We now study the task of dynamic field discovery, _i.e._ fields that are different across simulations. We extend the gravity dataset by Brandstetter et al.  by adding gravitational sources. We create a dataset of 50,000 simulations for training, 10,000 for validation and 10,000 for testing. We use \(N=5\) particles and \(M=1\) source. We set the masses of particles to \(m_{p}=1\), while the source's mass is \(m_{s}=10\). We generate trajectories of 49 timesteps. We use the first 44 steps as input and predict the remaining 5 steps. We plot the MSE in Figure 3(c) and \(L_{2}\) errors in Figure 20. We observe that once again, Aether clearly outperforms other methods.

### Ablation experiments

Significance of discovered fieldIn a simulated environment like the electrostatic field setting, we have access to the groundtruth fields and the sources that generate them. We leverage the simulator to study the significance of the discovered field in the task of trajectory forecasting, and establish an upper bound to our performance. To that end, we create two "oracle" models that have access to the groundtruth information, a _force oracle_ and a _source oracle_. The force oracle is identical to Aether, but uses the groundtruth forces from the simulator instead of predicting them with a neural field. The source oracle assumes knowledge of the "field sources". Thus, there is no longer need for disentanglement, and the problem is strictly equivariant again. We include the sources as virtual nodes in the graph, add include edges from the sources to the particles. We describe this oracle in detail in Appendix A.3. We show the MSE in Table 1(a) and plot the MSE and \(L_{2}\) errors in Figure 22 in Appendix E.5. We observe that Aether closely follows the two oracle models, and is on par, for roughly 10 steps. This demonstrates that the discovered field is almost as helpful as the groundtruth.

Learning the global field separatelyOur architecture connects the neural field with the graph network sequentially, _i.e._ the output of neural field is given as input to the graph network. We believe that this is integral for effective field discovery, as well as overall modelling, since it enables the graph network to learn to isolate local interactions from the observed net dynamics. We test this hypothesis with an ablation study, in which we connect the neural field and the graph network in parallel. The two networks are now working independently, and we only add their predictions at the output. We term this model _Parallel Aether_. We provide implementation details in Appendix A.4. We perform the experiment on the Lorentz field setting, and show results in Table 1(b). We also compare both methods against LoCS, as it is a common backbone in both methods, to demonstrate the performance gain due to the discovered field. We can see that even though the parallel architecture boosts performance, it is clearly not as effective as the sequential approach, which verifies our hypothesis.

Table 2: (a) Ablation study on the importance of the learned field. (b) Ablation study on the importance of a sequential architecture. (c) Ablation study on the choice of equivariant GNN backbone.

Choice of equivariant networkOur method is agnostic to the choice of equivariant graph network; we expect that it would be beneficial for a number of strictly equivariant networks. To test this hypothesis, we combine our method with EGNN . We start from the velocity formulation of EGNN and modify the message and velocity equations to incorporate the predicted forces for each node. We describe the model in detail in Appendix A.5. We train and evaluate this method on the Lorentz force field setting, and report the results in Table 2. EGNN combined with Aether reduces the error by 30.9%, compared to a vanilla EGNN, which enhances our hypothesis. We further include comparisons with more equivariant and non-equivariant graph networks in Appendix C.

Conditional neural fields for static settingsConditional neural fields generalize unconditional neural fields, and could, in principle, be used to learn static fields. In that case, the neural field should learn to ignore the latent vector, since the generated field should be identical regardless of the input system; we expect its performance to match the unconditional field. This, however, can come at the cost of increased training and inference time, as well as redundant computational resources and model parameters. We verify this hypothesis with an ablation study on the Lorentz force field, where we train and evaluate our method using a conditional field. In Table 3, we report the MSE, as well as the training time per minibatch, the inference time, and the number of parameters for each model. While the conditional model performs almost on par with the original unconditional model, this comes at the cost of 27% higher inference time and 9,985 more parameters. We conclude that the unconditional neural field is the preferred choice when there is expert knowledge that the field at hand is a static field. In the absence of such knowledge, _e.g._ on an exploratory analysis for underlying fields, then the conditional neural field would be preferable.

## 6 Conclusion

In this work, we introduced _Aether_, a method that discovers global fields in interacting systems. We propose neural fields to discover latent force fields, and infer them from the dynamics alone. Furthermore, we disentangle global fields from local object interactions, and combine neural fields with equivariant graph networks to learn the systems. We show that our method can accurately discover the underlying fields in a range of settings with static and dynamic fields, and effectively use them to forecast future trajectories. To the best of our knowledge, _Aether_ is the first work that discovers fields in interacting systems, and the first that is able to model systems with equivariant interactions and global fields. We hope that this work will inspire the community and bootstrap a line of works that explores field discovery, _since fields are omnipresent in all scientific tasks_.

LimitationsIn this work, we have only considered fields that do not react to the observable environment. While this setting is often true, in other scenarios, active fields might be crucial for effective modelling of the system dynamics. Furthermore, in the dynamic field setting, we assume that the input trajectories are descriptive enough to summarize the field we are trying to discover. While this hypothesis often holds, it might not always be true. Future work can explore these very interesting research directions.