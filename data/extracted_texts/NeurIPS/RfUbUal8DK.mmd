# Scalable Permutation Invariant Multi-Output Gaussian Processes for Cancer Drug Response

Leiv Ronneberg

Department of Mathematics

University of Oslo

0851 Oslo, Norway

ltronneb@math.uio.no

&Vidhi Lalchand

Eric and Wendy Schmidt Center

Broad Institute of MIT and Harvard

MIT

Cambridge, MA 02142, USA

vidrl@mit.edu

###### Abstract

Dose-response prediction in cancer is a critical step to assessing the efficacy of drug combinations on cancer cell-lines. The efficacy of a pair of drugs can be expressively modelled through a dose-response surface which outputs the viability score across a spectrum of drug concentrations for each pair of drugs in the training data. Using large in-vitro drug sensitivity screens, the goal is to develop accurate predictive models that can be used to inform treatment decisions by predicting the efficacy of given drug combination on new cancer cell lines as well as predict the effect of unseen drugs. Previous work  proposed a framework for modelling dose response surfaces with multi-output GPs, however, the model relied on the exact GP marginal likelihood and prohibited scalable inference. Further, the only inputs were drug concentrations per pair while the triplet of cell-lines and drug pair corresponded to different outputs. We make two important innovations in this work, we propose a framework for stochastic multi-output GPs for scalable inference; and, use a deep generative model (DGM) to embed the drugs in a continuous chemical space - enabling viability predictions for unseen drugs. We demonstrate the performance of our model in a simple setting using a high-throughput dataset and show that the model is able to efficiently borrow information across outputs.

## 1 Introduction

In recent years, much work has gone in to developing predictive models for cancer treatments. Using data from high-throughput _in-vitro_ experiments on multiple drugs and cell lines, large models are trained with the goal of predicting the sensitivity of a drug on a certain cell line. In the context of drug combinations, interest has frequently been on predicting a summary measure of drug interaction, e.g. a synergy score , computed from fitted dose-response surfaces, and an assumption of how non-interacting drugs should behave. Synergy scores are inherently quite crude measurements of drug interaction, and fundamentally hinge on the choice of non-interaction assumption. For this reason, several authors [7; 18; 15; 5] have proposed algorithms that instead aim to predict the entire dose-response surface. Since dose-response experiments are naturally invariant to the ordering of the drugs in a combination, we further augment the model by directly encoding this invariance into the prior.

In this paper, we build on previous work on permutation invariant multi-output Gaussian Processes (PIMOGPs) by ; the permutation invariance arises naturally in the context of dose-response functions as these are invariant to the ordering of the drugs in a combination. The original PIMOGPs framework relied on using exact GPs and thus incurred cubic complexity. Further, it did not encodethe drug pairs i.e. each drug combination experiment (cell line, drug A, drug B) triplet was considered an output, and only the concentrations \((c_{A},c_{B})\) were given as inputs. In this work we instead take only the cell line as output, and regard the drugs as inputs alongside the drug concentrations. In order to encode the drug information as inputs we make use of a deep generative model that takes as input a string representation of the molecule, and outputs a low-dimensional representation of the drug (see A).

The main contribution of this work is deriving the stochastic variational bound for the permutation invariant linear model of coregionalisation (LMC). This method naturally handles missing data and provides uncertainty quantification.

## 2 Background

### Linear Model of Coregionalisation for MOGPs

Multi-output GPs (MOGPs) are the extension of GP regression to the setting where the regression outputs are multidimensional, i.e. for any input \(\), the resulting mapping \(f()^{m}\) for some \(m>1\). There are many ways of constructing MOGPs (see  for a review), but our focus here will be on the linear model of coregionalisation (LMC).

In LMC the outputs are modelled as linear combinations of a set of independent latent functions, that are themselves modelled as GPs. That is, considering a set of \(m\) outputs \(\{f_{j}()\}_{j=1}^{m}\) for an input \(^{p}\), the \(j\)-th output is modelled as,

\[f_{j}()=a_{j1}u_{1}()+a_{j2}u_{2}()++a_{jR}u_{R}(),\] (1)

where \(u_{r}(0,k_{r}(,))\) for \(r=1,,R\) independently, and \(a_{j1},,a_{jR}\) are scalar weights. Note that each latent function is given its own covariance function \(k_{r}\), but these are free to have the same covariance, while maintaining independence. Latent functions that share their covariance functions can be grouped into \(G\) groups with \(R_{g}\) latent functions in each group, and the equation rewritten as:

\[f_{j}()=_{g=1}^{G}_{r=1}^{R_{g}}a_{jg}^{(r)}u_{g}^{(r)}().\]

Since GPs are closed under addition, this construction induces a GP over all outputs. The cross-covariance between two evaluations \(f_{j}()\) and \(f_{j^{}}(^{})\) can be written as

\[[f_{j}(),f_{j^{}}(^{})]=_{g= 1}^{G}_{r=1}^{R_{g}}a_{jg}^{(r)}a_{j^{}g}^{(r)}k_{g}(,^{ })=_{g=1}^{G}b_{jj^{}}^{(g)}k_{g}(,^{}),\]

where \(b_{jj^{}}^{(g)}=_{r=1}^{R_{g}}a_{jg}^{(r)}a_{j^{}g}^{(r)}\). The full cross-covariance over all \(m\) outputs can be written as

\[(,^{})=_{g=1}^{G}B_{g}k_{g}(,^{ }),\] (2)

where the matrix \(B_{g}\) is known as a _coregionalisation_ matrix, with entries \(\{B_{g}\}_{ij}=b_{ij}^{(g)}\). In the case of a _complete_ dataset where every input is observed at every output, the above expression can be written using Kronecker products:

\[(X,X)=_{g=1}^{G}B_{g} K_{g},\] (3)

where \(K_{g}\) has entries \(\{K_{g}\}_{ij}=k_{g}(_{i},_{j})\). In the simplest setting, where all latent functions share the same covariance function (i.e. \(G=1\)), this model is known as the _intrinsic coregionalisation model_ (ICM) and the Kronecker structure can be exploited to obtain large computational gains.

For the LMC, the matrices are even larger due to the covariance across outputs, resulting in a cubic complexity in both inputs and outputs - \((n^{3}m^{3})\). In the special case of the IMC, i.e. where \(G=1\) in equation (3), using various Kronecker tricks can bring this down to \((n^{3}+m^{3})\) - see e.g. .

Stochastic Variational Linear model of Coregionalisation for Multi-output GPs

In this section we desribe our main insight of leveraging SVI in the setting of the LMC in order to speed up the necessary computations.

Instead of having a single set of inducing inputs, we allow different inducing locations per latent function \(=\{Z_{r}\}_{r=1}^{R}\), where \(Z_{r}=\{_{1r},,_{qr}\}\), with corresponding inducing variables for each latent function \(=[_{1}^{T},,_{R}^{T}]^{T}\), where \(_{r}=[u_{r}(_{1r}),,u_{r}(_{qr})]^{T}\). In order to not overload the notation, we assume the same number of inducing variables, \(q\), per latent function, and that the dataset is complete, i.e. that every input \(\) is observed at every output. We denote by \(\) and \(\) vectors of observations and latent evaluations, stacked for each output, i.e. \(=[_{1}^{T},,_{m}]^{T}\) where \(_{j}=[y_{1j},,y_{nj}]^{T}\), where \(y_{ij}=f_{j}(_{i})+_{ij}\), and similarly for \(\).

The variational lower bound in the LMC case takes the same form as the stochastic variational bound for single output regression (see A.1), factorising over observations and outputs, but the matrices involved are more structured:

\[ p(|)_{i=1}^{n}_{j=1}^{m}\{ (y_{ij}|_{ij}^{T}},^{2})-}_{ij}^{T}_{ij}-}_{ij}\}-(p()\|q()),\] (4)

where \(_{ij}^{T}=A_{j}K_{}^{(i)}K_{}^{-1}\), \(A\) is the matrix with LMC coefficients, i.e. \(\{A\}_{ij}=a_{ij}\), where \(A_{j}\) denotes the \(j\)-th row of \(A\). The matrix \(K_{}^{(i)}\) is an \(R Rq\) block-diagonal matrix with entries \(\{k_{r}(_{i},Z_{r})\}_{r=1}^{R}\), i.e. formed by evaluating the input point \(_{i}\) against all the inducing points \(Z_{r}\) in the \(r\)-th latent function. \(_{ij}\) is formed by \(A_{j}[K_{}^{(i)}-K_{}^{(i)}K_{}^{-1}K_{}^ {(i)}]\), where the matrix \(K_{}^{(i)}\) is an \(R R\) block-diagonal matrix with entries \(\{k_{r}(_{i},_{i})\}_{r=1}^{R}\), \(K_{}^{(i)}=(K_{}^{(i)})^{T}\) and \(K_{}\) is an \(Rq Rq\) block-diagonal matrix with entries \(\{k_{r}(Z_{r},Z_{r})\}_{r=1}^{R}\). Finally, \(}\) and \(\) come from the variational distribution \(q()\) - which we assume factorise over the \(r\) latent functions \(q()=_{r=1}^{R}q_{r}(_{r})\) where \(q_{r}(_{r})=(_{r},_{r})\), yielding a block-diagonal \(\), and a decomposition of the KL-divergence term as \((p()\|q())=_{r=1}^{R}(p_{r}(_{r})\|q_{r}(_{r}))\).

### Permutation Invariance

For the problem of dose-response prediction, interest is on encoding an invariance on the ordering of the drugs -- a _permutation invariance_ of the inputs. In the context of multi-output GPs, we want to encode a permutation invariance for every output, \(f_{j}\). Looking at the construction of the LMC in equation (1), it suffices to ensure that each latent function \(u_{r}\) has the required invariance. Letting \(}\) denote a permuted version of the input \(\), with the desired invariance to encode \(u_{r}()=u_{r}(})\), this can be achieved by introducing another function \(_{r}()\) and constructing \(u_{r}()\) via a summation argument:

\[u_{r}()=_{r}()+_{r}(}),\] (5)

from which we see that the mapping \(}\) leaves the function unchanged. Placing a zero-mean GP prior on \(_{r}\) with kernel \(_{r}(,)\) induces a zero-mean GP prior on \(u_{r}\) with kernel,

\[k_{r}(,^{})=_{r}(,^{})+_{ r}(,}^{})+_{r}(},^{ })+_{r}(},}^{}).\] (6)

In order to enable SVI in the context of permutation invariant MOGPs, a slight modification needs to be made regarding the inducing variables \(_{r}\). Specifically, instead of regarding \(_{r}\) as observations from the latent function \(u_{r}\), they are assumed as observations from the underlying \(_{r}\). The structure of the bound in equation (4) remains unchanged. Only, the entries of matrices \(K_{}^{(i)}\), \(K_{}^{(i)}\) and \(K_{}\) need to be computed according to the following equations:

\[K_{}^{(i)} :k_{r,}(_{i},_{i})=_{r}(_{i}, _{i})+_{r}(_{i},}_{i})+_{r}(}_{i},_{i})+_{r}(}_{i},}_{i})\] (7) \[K_{}^{(i)} :k_{r,}(_{i},Z_{r})=_{r}(_{i},Z_{r}) +_{r}(}_{i},Z_{r})\] \[K_{} :k_{r,}(Z_{r},Z_{r})=_{r}(Z_{r},Z_{r}).\]That is, the entries of these block diagonal matrices are computed according to updated covariance functions \(k_{r,}(,)\), \(k_{r,}(,)\) and \(k_{r,}(,)\) that are themselves functions of different evaluations of the underlying kernel function \(_{r}(,)\) of \(_{r}\). This model is implemented within the GPyTorch  framework for GP regression.

## 4 Dataset and results

### Dose-response data

We use the data from , and follow the same pre-processing procedure as in . This dataset consist of 583 unique combinations of 38 drugs, screened on 39 cell lines across 6 different tissues - totalling over 1.2 million viability measurements. The pre-processing procedure (see A.3) further standardises and upsamples the data to a common \(10 10\) grid of concentrations -- individually scaled to the unit box \(\) - yielding a dataset of 1,883,700 observations on a shared grid of concentrations. The viability measurements are also standardised to the \(\) interval.

### Results

We test the performance of our model in the _leave-triplet-out_ (LTO) setting, using the nomenclature of . That is, we consider prediction of a specific (cell line, drug A, drug B)-triplet that does not appear in the training dataset - however, the training dataset may contain other examples using the same cell line, or the same drugs. We split the 18,837 experiments 80/20 into a training and test set - keeping 15,069 experiments for training and 3768 for testing. For 39 cell line outputs, we set the number of latent functions \(R=10\) in the LMC all sharing the same kernel function (i.e. G=1 in equation (3)), and use \(q=200\) inducing points for each latent function. For the variational distribution, each component is modelled using a mean-field approximation, \(p_{r}(_{r},_{r})\), where \(_{r}\) is a diagonal matrix. For the covariance function \((,^{})\) we use an RBF kernel over the drug concentrations, and a RBF kernel with automatic relevance determination (ARD) over the drug features. These are then multiplied together to form the final covariance function over the inputs.

We used a batch size of 256, and trained for 12 epochs using the Adam optimizer  with an initial learning rate of 1e-2 decreasing to 1e-3 after 6 epochs, and to 1e-4 after 9 epochs. We plot the training loss over the epochs in Figure 1 (Left). As performance metrics, we compute the root mean squared error (RMSE) and Pearson's correlation coefficient of our predictions against the observed viability measurements.

The results are visualised in Figure 1 (middle), where the model predictions are plotted against observed in the LTO setting. The model performs well, obtaining a RMSE of 0.1015 and a Pearson's r of 0.945. The pre-processing leaves us with observations strictly in the interval \(\), while the predictions have not been bounded in any way. This could be alleviated by e.g. considering a different likelihood function. Finally, we also plot the coregionalisation matrix \(B\) from equation 3 having normalised it to a correlation matrix. We note that in contrast to the model of , our implementation is able to borrow strength across cell lines in its predictions.

Figure 1: (Left): Training loss over 12 epochs, (Middle): Observed vs. Predicted using the permutation-invariant model, (Right): Coregionalisation matrix \(B\) normalised as a correlation matrix showing the learned correlation structure across the 39 cell lines.