# Transformers over Directed Acyclic Graphs

Yuankai Luo

Beihang University

luoyk@buaa.edu.cn

Veronika Thost

MIT-IBM Watson AI Lab, IBM Research

veronika.thost@ibm.com

Corresponding author.

Lei Shi

Beihang University

leishi@buaa.edu.cn

###### Abstract

Transformer models have recently gained popularity in graph representation learning as they have the potential to learn complex relationships beyond the ones captured by regular graph neural networks. The main research question is how to inject the structural bias of graphs into the transformer architecture, and several proposals have been made for undirected molecular graphs and, recently, also for larger network graphs. In this paper, we study transformers over directed acyclic graphs (DAGs) and propose architecture adaptations tailored to DAGs: (1) An attention mechanism that is considerably more efficient than the regular quadratic complexity of transformers and at the same time faithfully captures the DAG structure, and (2) a positional encoding of the DAG's partial order, complementing the former. We rigorously evaluate our approach over various types of tasks, ranging from classifying source code graphs to nodes in citation networks, and show that it is effective in two important aspects: in making graph transformers generally outperform graph neural networks tailored to DAGs and in improving SOTA graph transformer performance in terms of both quality and efficiency.

## 1 Introduction

Graph-structured data is ubiquitous in various disciplines (Gilmer et al., 2017; Zitnik et al., 2018; Sanchez-Gonzalez et al., 2020) and hence graph representation learning has the potential to provide huge impact. There are various types of _graph neural networks_ (GNNs), the majority of which is based on a so-called message-passing scheme where node representations are computed iteratively by aggregating the embeddings of neighbor nodes (Gilmer et al., 2017). Yet, this mechanism in its basic form has limited expressivity (Xu et al., 2018) and research is focusing on extensions.

_Transformer models_ have recently gained popularity in graph learning as they have the potential to learn complex relationships beyond the ones captured by regular GNNs, in a different way (Dwivedi and Bresson, 2020; Kreuzer et al., 2021). Technically, they can be considered as graph neural networks operating on fully-connected computational graphs, decoupled from the input graphs. The main research question in this context is how to inject the structural bias of the given input graphs (i.e., which nodes are actually connected) into the transformer architecture by adapting their attention and positional encoding appropriately. Several promising proposals have been made to encode undirected molecular graphs (Ying et al., 2021; Ross et al., 2022) and recent works take into account the scalability challenge (Dwivedi et al., 2022), also over larger network graphs (Rampasek et al., 2022; Chen et al., 2022b).

We focus on _directed acyclic graphs_ (DAGs), which are of special interest across various domains; examples include parsing results of source code (Allamanis et al., 2018), logical formulas (Crouse et al., 2019), and conversational emotion recognition (Shen et al., 2021), as well as probabilistic graphical models and neural architectures (Zhang et al., 2018, 2019; Knyazev et al., 2021). In a DAG, the edges define a partial order over the nodes. This partial order represents an additional strong inductive bias, which offers itself to be integrated into the models.

In fact, various kinds of neural networks tailored to DAG structures have been proposed over the years; from early descriptions of recursive neural networks over DAGs (Sperduti and Starita, 1997; Frasconi et al., 1998), which have pointed out a similarity to sequence learning, to more recent works, such as DAG-RNN (Shuai et al., 2016), DAG-LSTM (Crouse et al., 2019), D-VAE (Zhang et al., 2019), and DAGNN (Thost and Chen, 2021). The latter models focus on encoding the entire DAG; in a nutshell, they compute the embedding in a message-passing fashion by iterating over the DAG nodes in an asynchronous way, given by the partial order, and thereafter aggregating the final node representations into one for the DAG. This procedure yields state-of-the-art performance in terms of quality, but its asynchronous nature leads to comparatively (to regular GNNs) very slow runtime performance. The more parallel computation of transformers would seem to make them more suitable models and (Dong et al., 2022) have recently proposed one such model, PACE, which shows convincing performance in terms of both quality and efficiency.

In this paper, we focus on _transformers over DAGs_ more generally, motivated by the highly flexible, parallel, and expressive nature of their computation. In particular, their success in sequence learning opens up the question how they can be tailored to DAGs, which are essentially sequential graphs. Based on the above-mentioned insights in DAG learning, we propose a straightforward and efficient **DAG attention framework**, which _effectively biases any transformer towards DAGs_.

* in terms of its reachability relation_
- and at the same time gets _considerably more efficient_ than the regular quadratic complexity; and (2) employ the positional encoding in a complementary way, to further bias the computation towards the DAG topology, by explicitly _encoding the node depth_.
* We show that the attention is not restricted effectively, even if it seems so technically. Moreover, we draw a connection to the random walk theory.
* We rigorously evaluate our proposal in ablation studies and show that it successfully improves different kinds of baseline transformers, from vanilla transformers (Vaswani et al., 2017) to state-of-the-art graph transformers (Wu et al., 2021; Chen et al., 2022; Rampasek et al., 2022; Wu et al., 2022), over various types of DAG data. Our experiments range from classifying source code graphs to nodes in citation networks, and even go far beyond related works' problem scope. Most importantly, our proposal is proven _effective in two important aspects: in making graph

Figure 1: Overview of our DAG attention. A mask matrix restricts the receptive field of the node under consideration to nodes that are directly reachable in the DAG. Our positional encoding (right), additionally captures its position as its depth in the DAG.

transformers generally outperform graph neural networks tailored to DAGs and in improving SOTA graph transformer performance in terms of both quality and efficiency._
* Finally, _our DAG attention framework can be implemented in two possible and rather straightforward ways_, either on top of existing transformer models, or based on message-passing GNNs. Our implementation is available at https://github.com/LUOyk1999/DAGformer.

## 2 Background and Related Works

**Directed Acyclic Graphs.** We refer to a _graph_ as tuple \(G=(V,E,)\), with node set \(V\), edge set \(E V V\), and node features \(^{n d}\), with each row representing the feature vector of one node, with number of nodes \(n\) and feature dimension \(d\), the features of node \(v\) are denoted by \(x_{v}^{d}\). A _directed acyclic graph_ (DAG) is a directed graph without directed cycles. For a DAG \(G\), we can define a unique _strong partial order_\(\) on the node set \(V\), such that, for all pairs of nodes \(u,v V\), \(u v\) if and only if there is a directed path from \(u\) to \(v\). We define the _reachability relation_\(\) over a DAG based on that partial order. That is, \(u v\) if and only if \(v\) is reachable from \(u\); further, if \(u v\), then \(u\) is called a _predecessor_ of \(v\), and \(v\) is a _successor_ of \(u\). All nodes without predecessors are _source_ nodes, and all nodes without successors are _sink_ nodes. We further define a restricted form of reachability \(_{K}\) with \(u_{k}v\) if and only if there is a directed path of length at most \(k\) from \(u\) to \(v\). The _depth_ of a node \(v V\) is defined below. The _depth_ of a DAG is the maximum depth over its nodes.

\[(v)=0&\\ 1+_{(u,v) E}(u)&\]

**Message-Passing Graph Neural Networks.** Most modern _graph neural networks_ (GNNs) are or can be formulated in terms of the _message-passing architecture_, a framework proposed in (Gilmer et al., 2017). In that architecture, node representations are computed iteratively by aggregating the embeddings of their neighbor nodes (i.e., the messages), and a final graph representation can be obtained by aggregating the node embeddings. Yet, in its basic form, this mechanism has limited expressivity (Xu et al., 2018) and graph neural networks remain an active research area. Notable works we compare to include the **graph convolutional network (GCN)**(Kipf and Welling, 2017), a very early, basic architecture; the **graph attention network (GAT)**(Velickovic et al., 2018), aggregating the neighbor node embeddings using attention; the **graph isomorphism network (GIN)**(Xu et al., 2018), which integrates extra MLP layers into message passing for increased expressivity; and the **principal neighbourhood aggregation (PNA)** model (Corso et al., 2020), a recent proposal focusing on adapting the aggregation mechanism to the node under consideration (e.g., scaling based on its degree). For a broader overview of the field, we refer the reader to (Wu et al., 2020).

**Transformers on Graphs.** Transformer models (Vaswani et al., 2017) have gained popularity in graph learning as they have the potential to learn complex relationships beyond the ones captured by regular GNNs and in a different way. The architecture is composed of two main parts: a _self-attention module_ and a feed-forward neural network, each followed by a residual connection with a normalization layer. Formally, the self-attention projects the input node features \(\) using three matrices \(_{}^{d d_{K}}\), \(_{}^{d d_{K}}\) and \(_{}^{d d_{K}}\) to the corresponding representations for query (\(\)), key (\(\)), and value (\(\)), and is described as follows :

\[=_{},\;=_{},\;=_{},\] \[()=( ^{T}}{_{K}}}).\] (1)

Over graphs, we focus on computing node instead of token embeddings (recall Figure 1). Technically, transformers can be considered as message-passing GNNs operating on fully-connected computational graphs, decoupled from the input graphs. The main research question in the context of graphs is how to inject the structural bias of the given input graphs by adapting their attention and by adding extensions to the original features, via so-called positional encodings (PEs). **Graph Transformer**(Dwivedi and Bresson, 2020) represents an early work using Laplacian eigenvectors as positional encodings, and various extensions and other models have been proposed since then (Min et al., 2022). For instance, (Mialon et al., 2021) proposed a relative PE (Shaw et al., 2018) by means of kernels on graphs to bias the self-attention calculation. Notably, **GraphTrans**(Wu et al., 2021)was the first hybrid architecture, using a stack of message-passing GNN layers before the regular transformer layers. Finally, [Chen et al., 2022a] have reformulated the self-attention mechanism as a kernel smoother as shown below and incorporated structure information into their **structure-aware transformer (SAT)** architecture by extracting a subgraph representation rooted at each node before attention computation:

\[(x_{v})=_{u V},x_{u} )}{_{w V}(x_{v},x_{w})}f(x_{u}),  v V,\] (2)

with \(f(x)=}x\), and non-symmetric exp. kernel \(\):

\[(x,x^{})=(},(x^{})}}{}}),\ \ \ (x)=x&\\ _{G}(x)&\]

where \(,\) is the dot product on \(^{d}\) and \(_{G}(x)\) is an arbitrary GNN model. Most of the aforementioned works focus on the classification of smaller graphs, such as molecules; yet, recently, **GraphGPS**[Rampasek et al., 2022] is also considering larger graphs and the area is focusing on the development of scalable models; for instance, **Nodeformer**[Wu et al., 2022] is designed to address the issue of scalability and expressivity in node classification. Altogether, the transformer architecture opens new and promising avenues for graph representation learning, beyond message-passing GNNs.

**Neural Networks over DAGs.** The additional strong inductive bias present in DAGs has motivated researchers to formulate special neural architectures tailored to this kind of graphs [Sperduti and Starita, 1997, Frasconi et al., 1998]. Various types of models have been proposed over the years in many different application areas. There are works in the context of syntactic graphs, [Tai et al., 2015, Shuai et al., 2016] logical formulas [Crouse et al., 2019], source code representations [Allamanis et al., 2018] and neural architectures [Zhang et al., 2018, 2019, Knyazev et al., 2021]. We particularly compare to the most recent proposals **S-VAE** and **D-VAE**[Zhang et al., 2019]; and to **DAGNN** from [Thost and Chen, 2021], who also showed that _using attention for neighbor node aggregation is beneficial in several DAG tasks_.

While the models can be formulated in terms of the message-passing framework [Thost and Chen, 2021], their processing is special for GNNs: they compute a graph embedding by iterating over the DAG nodes _in an asynchronous way_, given by the partial order, and starting from the source nodes (or the other way around, from the sink nodes). That is, the messages are passed along the partial order of the DAG and, at any point during computation, capture the information of the subgraph consisting of all the predecessors of the node currently under consideration. Thereafter, a final graph representations can be obtained by aggregating the embeddings of all sink nodes. In contrast to regular message-passing GNNs, these customized works for DAGs usually focus on encoding graphs (i.e., instead of nodes or edges, which are other common GNN targets) and pass messages over the entire graph, while regular GNNs consider a fixed radius - usually rather small - around each node and essentially obtain a neighborhood embedding. Furthermore, the nature of the proposed architectures shows that _recurrent techniques_ have proven especially useful, _analogy to learning over sequences_. In summary, these DAG models are elegant and effective, yielding state-of-the-art results, but their asynchronous nature leads to very slow and practically inhibitive performance compared to regular GNNs.

**Transformers over DAGs.** We are aware of only few proposals for transformers tailored to DAGs. [Huang et al., 2022] developed a Directed Acyclic Transfomer in the context of machine translation in order to simultaneously capture multiple translations within the decoded DAG, but the model's encoder is a vanilla transformer. [Kotnis et al., 2021] proposed BIQE, which leverages the depth of nodes in DAG paths as positional encodings, with a specific focus on answering complex queries in knowledge graphs. [Gagrani et al., 2022] proposed Topoformer, which is also an encoder-decoder model but has been developed for finding optimal topological orders in DAGs. Since the study entirely focuses on the latter goal (e.g., in terms of training objective and evaluation) it is very specific and different from our more general study. The DAG encoder itself is also rather different in that it uses a Laplacian PE, as it is used with regular graph transformers; and a more complex attention mechanism, which does not only model the reachability but also several other relations over the graph. Closest to our method is **PACE**[Dong et al., 2022], which similarly focuses on modeling the sequential nature of DAGs inside a transformer model and independently of a specific application in mind. However, (1) it applies a rather complex, node-specific positional encoding, whereas our PE only distinguishesnode depth; (2) the attention is based on the directed transitive closure, while we use reachability and show that this provides better quality; and (3) it's implemented using regular transformers, with runtime complexity \(O(|V|^{2}d)\), while we propose a much more efficient and scalable implementation based on message passing. Altogether, _our proposal is simpler and considerably more efficient. In addition, we do not propose a single model but a framework which can be flexibly applied on top of existing graph transformers and thus complement any custom, graph-specific transformer and adapt it to DAGs_.

## 3 Transformers for Directed Acyclic Graphs

Transformers have revolutionized deep learning, in particular sequence learning, and yield promising performance over graphs. Furthermore, we posit that their benefits actually match well the above-mentioned shortcomings of DAG neural networks. Therefore we developed a graph transformer framework tailored to DAGs. See Figure 1 for an overview.

### Attention based on DAG Reachability

In contrast to regular graphs, the partial order in DAGs creates particular relations between connected nodes, in the sense that a given node's predecessors and successors are most likely more important to it than other graph nodes. Note that this intuition is also captured in the processing of DAG neural networks. Hence the reachability relation suggests itself to be exploited in our architecture. We apply it to restrict the receptive field of nodes during attention to their predecessors and successors in the graph. (Vaswani et al., 2017) already mentioned the possibility to use restricted attention in order to reduce complexity and, indeed, our proposal does not only yield an architecture which is biased towards the DAG structure, but additionally a considerably more efficient model.

While graphs to classify are usually of manageable size, graph learning in general may face much larger ones. For this reason, we formulate our model in a more general way, based on a bounded reachability relation, representing the receptive field of each node: \(N_{k}(v)=\{(u,v)_{k}\}\{(v,u)_{k}\}\). We adapt Equation (2) for our **reachability-based attention (DAGRA)**:

\[_{}(x_{v})=_{u N_{k}(v)} ,x_{u})}{_{w N_{k}(v)}(x_{v}, x_{w})}f(x_{u}), v V.\]

The number \(k\) represents a hyperparameter and can be chosen according to the data. In our ablation study (see Section 4), \(k=\) yielded best performance consistently. Observe that this choice of \(k=\) is still very different from both regular GNNs, which usually considerably restrict \(k\), and regular transformers (Eq. 1), whose receptive field is not restricted at all (i.e., in terms of reachability).

### Positional Encodings based on DAG Depth

As outlined in Section 2, positional encodings have been recognized as important and proven effective for incorporating graph structure into transformers. Observe that the sequential nature of DAGs makes them possess special position information, the depth of a node within the DAG. We propose to include this knowledge in the form of **directed acyclic graph positional encodings (DAGPE)** as follows, exactly as suggested for the original transformer architecture (Vaswani et al., 2017):

\[PE_{(v,2i)}=}},\ \ \ PE_{(v,2i+1)}=}},\]

where \(d\) is the node feature dimension and \(i\) the index of the dimension under consideration.

**DAG Attention.** We obtain the following model, combining DAGRA and DAGPE, for \(v V\):

\[(x_{v}) =_{u N_{k}(v)}+PE_{v},x_{u}+PE_{u} )}{_{w N_{k}(v)}(x_{v}+PE_{v},x_{w}+PE_{w} )}f(x_{u}).\] (3)

Our attention incorporates both similarity of node features and of node depths. We argue that these are the most critical aspects of DAGs and our evaluation will show their impact and general effectiveness. In particular, note that most kinds of DAG data (e.g., citation networks) do not require us distinguishing between predecessor or successor nodes of same depth.

### Expressive Power of DAG Attention

Technically, we restrict the attention to reachable nodes and, in this way, obtain considerable efficiency gains. Yet, our architecture is tailored to the special DAG structure, and we can show that this design offers similar expressivity to regular transformers.

It is important to note that in our framework, all nodes directly communicate with at least one source node (i.e., node without predecessors) by the DAG structure. This is specifically the case because we do not restrict the radius \(k\) of the receptive field, but consider \(k=\). Hence, 2 layers are always enough to establish communication beyond any two nodes that have a common source node. Observe that this is often the case in practice; especially in DAG classification, many kinds of DAGs contain only a single source node (e.g., ogbg-code2 Hu et al. (2020) and NA Zhang et al. (2019)).

For DAGs with \(m\) source nodes we need \(2m\) layers for full communication, if we assume the DAG to be connected. In the latter case, every pair of source nodes has a common successor through which communication can happen. Further, connectedness is a reasonable assumption, otherwise communication is likely not needed in most scenarios.

### Theoretical Intuition

We have shown that our architecture's bias emphasizes DAG relationships while re-directing the remaining relationships in the regular transformer's full attention matrix though the source nodes. This can also be shown to be in line with random walk theory, we draw a connection to PageRank (Brin, 1998; Gasteiger et al., 2011). Specifically, we consider a _PageRank variant that takes the root node into account - personalized PageRank_. We define the root node \(x\) via the teleport vector \(i_{x}\), which is a one-hot indicator vector. The personalized PageRank can be obtained for node \(x\) using the recurrent equation \(_{G}(i_{x})=(1-)A_{rw}_{G}(i_{x})+ i_{x}\), with teleport (or restart) probability \((0,1]\). Solving this equation, we obtain: \(_{G}(i_{x})=(I_{n}-(1-)A_{rw})^{-1}i_{x}\), where \(A_{rw}=AD^{-1}\), with \(A\) and \(D\) being the adjacency and the degree matrix, respectively (Gasteiger et al., 2011). We invert the directions of the edges in \(G\) to create a reverse DAG \(\). We can show that, for every node \(x\), only nodes \(y\) not reachable from \(x\) (i.e., \(y N_{}(x)\)) will satisfy that \(_{G}(i_{x})[y]+_{}(i_{x})[y]\) (the y-th element of \(_{}(i_{x})\)) equals 0. The proof is provided in Appendix A. This means that for the random walk's limit distribution, the probability of nodes that are not reachable from \(x\) is 0.

### Implementation

We describe two ways of implementing our model, especially DAGRA, based upon transformers and message-passing GNNs, respectively.

**Masked Attention for Transformers.** As shown in Figure 1, we can implement DAG attention in a very straightforward fashion using a mask that masks out node pairs based on the DAG reachability relation as follows (compare to Equation (1)), with the attention mask \(\) being defined as a symmetric matrix over node pairs \((v,u) V V\).

\[()=(^{T}}{_{K}}}+), (v,u)=0&u N_{k}(v)\\ -&.\]

While this masking represents a simple technique to extend and bias existing transformer models to DAGs, the resulting architecture does not benefit from the restricted node set to be considered, leading to unnecessary, costly matrix operations during attention calculation. Moreover, it consumes \(O(|V|^{2})\) of additional memory to store \(\). Thus, the runtime complexity per layer is still \(O(|V|^{2}d)\) for the vanilla transformer - and may be even higher, depending on the underlying transformer.

**DAG Attention using Message Passing.** Based on the formulation in Equation (3), we propose to follow the message-passing scheme; that is, for a node \(v\), we compute \(N_{k}(v)\) and only aggregate messages from the nodes in that set to compute the DAG attention. For the latter aggregation, we can use readily available frameworks, such as PyG (Fey and Lenssen, 2019). We analyze the complexity of this proposal.

### Computational Complexity

Clearly, the time complexity of the proposed model is lower than that of the standard transformer. We consider two computation steps:

**Computing DAG Reachability.** To obtain \(N_{k}\), we compute the transitive closure of \(E\) for each node in the graph using a breadth-first search starting at the node and iteratively expanding \(N_{k}\) based on \(E\). Hence, the overall complexity of this step is \(O(|E||V|)\). Observe that, during training, we can consider this step as pre-processing since it only has to be run once, in the very beginning.

**DAG Attention.** The matrix product \(^{T}\) of self-attention has a cost \(O(|V|^{2}d)\) which is quadratic in the number of nodes in \(G\), and hence especially critical for large graphs. Yet, for our DAG attention, we only aggregate messages from reachable nodes. Therefore, the time complexity of reachability relation attention is \(O(|V| n_{k} d)\), where \(n_{k}\) is the average size of \(N_{k}\). In the worst case, we have \(n_{k}=|V|-1\), but we assume that \(n_{k}<<|V|\) in general. Especially for sparse graphs, the complexity gets significantly lower, i.e., \(O(|V|^{k} d)\), where \(\) is the average node degree.

Finally, observe that directed rooted trees represent a special kind of DAGs broadly seen across domains, such as abstract syntax trees of source code (Allamanis et al., 2018). For them, the runtime of DAG attention scales almost linearly, as illustrated in the following theorem.

**Theorem 1**.: _In a directed rooted tree \(T\), the runtime of DAG attention is \(O(|V| k^{+} d)\), where \(^{+}\) is the maximal outdegree of \(T\). When \(k=\), the runtime of DAG attention is \(O(|V| depth(T)^{+} d)\)._

The proof is provided in Appendix A. Note that when \(^{+}\) is small, \(d\) is a constant and \(depth(T)\) is \(O(|V|)\), the runtime of DAG attention is almost linear in \(|V|\). Indeed, we observed this on the ogbg-code2 dataset in our experiments.

## 4 Evaluation

We evaluate our proposed architecture on several datasets, comparing it to competitive baselines. Ablation studies also provide more insight into the composition of our model. Primarily, the following questions are investigated:

* Does **DAG attention** have the expected effects and **improves existing graph transformers** both in terms of quality and efficiency?
* Is DAG bias encoded through both **DAGRA & DAGPE**, and how does DAGPE compare to others?
* the main difference between regular GNNs and transformers
- affect the performance?

### Experiment Setting

**Datasets.** Table 1 shows the diversity of the datasets we used; see Appendix B for full details.

* **ogbg-code2**(Hu et al., 2020). A large dataset of ASTs derived from Python methods. The node features are syntax tokens. The multi-task classification task is to predict the first 5 tokens of the function name.
* **NA**(Zhang et al., 2019). A dataset with much smaller graphs, containing neural architecture DAGs generated by the ENAS software. Each node's features represent a certain neural network component type. The (regression) task is to predict the architecture performance on CIFAR-10.
* **Self-citation**(ARC, 2021; Luo et al., 2023). Each DAG in the academic self-citation represents a scholar's academic self-citation network (ARC, 2021). Each paper has two node attributes: the

    & ogbg-code2 & NA & Self-citation & Cora & Citeseer & Pubmed \\  \# graphs & 452,741 & 19,020 & 1,000 & 1 & 1 & 1 \\ Avg \# nodes & 125.2 & 8.0 & 59.1 & 2,708 & 3,327 & 19717 \\ Avg \(n_{}\) & 9.78 & 7.00 & 4.30 & 20.88 & 5.33 & 60.56 \\   

Table 1: Statistics of the datasets we used.

publication year and total citation count (excluding the papers whose citation category is to be inferred). Here we consider the node-level task of predicting whether a paper is highly-cited or not - as a proxy for its impact.
* **Cora, Citeseer, Pubmed**. Established, medium-sized citation graphs. Only for our method, we removed a small number of cyclic citation links to make them DAGs.

**Baselines.** We chose two basic message-passing GNNs, **GCN** and **GIN**; extensions of these models using a virtual node connected to all other graph nodes; the graph attention network **GAT**, as it showed especially good performance in ; **MixHop**, **LDS-GNN**, **IDGL** and **PNA**, as more recent GNN proposals. In terms of transformer models, we considered vanilla **Transformer (TF)**, **Graph Transformer (GT)**, **GraphTrans**, **SAT**, **GraphGPS** and **NodeFormer** which achieved state-of-the-art performance (SOTA). Lastly, we consider neural networks tailored to DAGs: **S-VAE**, **D-VAE**, **DAGNN** and the current SOTA, **PACE**. For more detailed descriptions, see Section 2.

**DAG+ Models.** We implemented our DAG attention on top of vanilla Transformer, GraphTrans, SAT, GraphGPS and NodeFormer only (1) modifying their self-attention module by restricting attention in terms of reachability and (2) using DAGPE instead of the original one. We note that there are various alternatives for the latter (e.g., concatenation etc.), therefore we opted for the most simple solution which, at the same time, provides a very direct comparison. Moreover, on ogbg-code2 we did not use any PE since  showed that they do not make real impact and we observed the same in preliminary experiments. For fair comparisons, we use the same hyperparameters (including the number of layers, batch size, hidden dimension etc.) and readout as baseline transformers. Given one of the baseline transformers M, we denote the modified model using DAG attention by **DAG+M**. Unless explicitly specified otherwise, we chose \(k=\) in all experiments. Full details on the experimental setup and hyperparameters are provided in the Appendix B.

### Results and Discussion

**Overall Performance, Tables 2, 3, 4 and 5.** First, observe that the results are very consistent, although the datasets differ greatly in size, DAG sizes, DAG shape (e.g., in ogbg-code2 we have trees), and nature of data (e.g., node features). The message-passing GNNs represent standardized baselines but do not reach the performance of networks tailored to DAGs, such as DAGNN. Note that the latter is however comparatively bad on the node-level task. This can be explained by its processing. It computes node representations in the order of the partial order, and hence the representations of nodes in the beginning of the order contain only few information about the entire graph. Intuitively, transformers should be able to capture this information, but the transformers we tested do neither perform better, not even the ones tailored to graphs. This shows that they are missing information captured by those message-passing neural networks that were tailored to DAGs. The results clearly show that our DAG attention is successful in providing good improvements, over the best graph

   Model & Valid F1 (\%) & Test F1 (\%) & Time(epoch) \\  GIN & 13.7 \(\) 0.2 & 14.9 \(\) 0.2 & 181s \\ GCN & 14.0 \(\) 0.2 & 15.1 \(\) 0.2 & 127s \\ GIN-Virtual & 14.4 \(\) 0.3 & 15.8 \(\) 0.2 & 155s \\ GCN-Virtual & 14.6 \(\) 0.1 & 16.0 \(\) 0.2 & 198s \\ GAT & 14.4 \(\) 0.2 & 15.7 \(\) 0.2 & 134s \\ PNA & 14.5 \(\) 0.3 & 15.7 \(\) 0.3 & 427s \\ DAGNN & 16.1 \(\) 0.4 & 17.5 \(\) 0.5 & 6018s \\ PACE & 16.3 \(\) 0.3 & 17.8 \(\) 0.2 & 2410s \\  Transformer & 15.5 \(\) 0.2 & 16.7 \(\) 0.2 & 1817s \\
**DAG+Transformer** & **17.4** \(\) 0.1 & **18.8** \(\) 0.2 & 591s \\  GraphTrans & 16.6 \(\) 0.1 & 18.3 \(\) 0.2 & 1117s \\
**DAG+GraphTrans** & **17.0** \(\) 0.2 & **18.7** \(\) 0.2 & 526s \\  GraphGPS & 17.4 \(\) 0.2 & 18.9 \(\) 0.2 & 1919s \\
**DAG+GraphGPS** & **17.6** \(\) 0.1 & **19.3** \(\) 0.2 & 608s \\  SAT (SOTA) & 17.7 \(\) 0.2 & 19.4 \(\) 0.3 & 2437s \\
**DAG+SAT** & **18.5** \(\) 0.1 & **20.2** \(\) 0.2 & 681s \\   

Table 2: **Code graph classification on ogbg-code2. The baseline results were taken from the OGB leaderboard.**

   Model & AP \(\) & ROC-AUC \(\) \\  GIN & 57.7 \(\) 1.8 & 79.7 \(\) 0.2 \\ GCN & 58.8 \(\) 0.4 & 79.9 \(\) 0.2 \\ GIN-Virtual & 57.4 \(\) 1.2 & 79.5 \(\) 0.4 \\ GCN-Virtual & 58.9 \(\) 0.2 & 80.0 \(\) 0.1 \\ GAT & 55.3 \(\) 3.7 & 77.9 \(\) 1.4 \\ PNA & 62.4 \(\) 0.7 & 81.0 \(\) 0.4 \\ DAGNN & 61.2 \(\) 0.6 & 81.0 \(\) 0.3 \\ PACE & 52.1 \(\) 1.8 & 75.9 \(\) 0.7 \\  Transformer & 56.8 \(\) 1.8 & 78.7 \(\) 0.3 \\
**DAG+Transformer** & **63.8** \(\) 0.8 & **82.2** \(\) 0.5 \\  GraphGPS & 61.6 \(\) 2.6 & **81.3** \(\) 0.6 \\
**DAG+GraphGPS** & **63.5** \(\) 1.2 & 80.8 \(\) 0.5 \\  SAT & 59.8 \(\) 1.7 & 79.8 \(\) 0.7 \\
**DAG+SAT** & **62.7** \(\) 1.5 & **80.6** \(\) 0.7 \\  NodeFormer & 39.6 \(\) 0.6 & 69.4 \(\) 0.3 \\
**DAG+NodeFormer** & **64.9** \(\) 0.8 & **81.7** \(\) 0.8 \\   

Table 3: **Node classification results for the self-citation dataset; AP (\(\%\)) and ROC-AUC (\(\%\)).**transformer SAT and even better ones over vanilla Transformer. On ogbg-code2, the improvement is smaller for GraphTrans and SAT. However, this benchmark task is heavily dependent on other features (e.g., language understanding) and hence presents a special challenge; in this regard, the NA and self-citation datasets contain "cleaner" graphs. It is interesting to see that our framework lifts the transformers from below the level of DAGNN to above, in terms of all metrics, over these two datasets. Lastly, we observe that the PACE transformer tailored to DAGs is similarly outperformed by our simpler, but more effective technique. Nevertheless, the overall conclusion holds in general: Over all datasets, our DAG attention makes the transformers outperform (1) the original transformers and (2) the neural networks tailored to DAGs. This shows that the DAG-specific bias provided by DAG attention is the right bias. We investigated this in more detail in our ablation experiments.

**Ablation Study, Table 6.** Recall that our DAG attention is composed of the reachability-based attention (DAGRA) and DAGPE modules. To justify this design, (1) our ablation studies removing DAGRA and DAGPE individually confirm the impact of both modules; (2) we also experimented with replacing DAGPE with LapPE (Dwivedi and Bresson, 2020) and the random-walk-based RWPE (Dwivedi et al., 2021) to show that the DAG-specific nature of our PE is of advantage; (3) we experimented with adding attention bias which captures the graph structure more directly (e.g., shortest-path Ying et al. (2021) and edge directionality), although they are implicit in DAGPE; interestingly, the more direct representation does not make a noticeable difference. For ease of comparison and interpretation (i.e., in terms of magnitude), we also provide the baseline transformer results. Overall, it can be observed that our DAG attention yields highly consistent performance improvements, although occasionally the advantage is less pronounced. This shows that our architecture design provides the right bias on top of (graph) transformers, which makes the improved models better fit for DAGs.

**Impact of Size of Receptive Field \(N_{k}\).**

**(1) Average \(n_{}\), Table 1.** Compared to the often only two to three hops considered in message-passing GNNs, our \(k=\) might seem unrealistic. However, as we show in the table, for two of our three and very different datasets, we have that \(n_{}\) is much smaller than the worst case size \(|V|\). NA

    &  &  &  \\  & Valid F1 & Test F1 & RMSE \(\) & Pearson’s \(\) & AP \(\) & ROC-AUC \(\) \\ 
**DAG+TP** & 0.1731 \(\) 0.0014 & 0.1895 \(\) 0.0014 & **0.253**\(\) 0.002 & **0.966**\(\) 0.001 & **0.638**\(\) 0.008 & **0.822**\(\) 0.005 \\ (\(\)) DAGRA & 0.1546 \(\) 0.0018 & 0.1670 \(\) 0.0015 & 0.284 \(\) 0.003 & 0.957 \(\) 0.001 & 0.573 \(\) 0.011 & 0.790 \(\) 0.003 \\ (\(\)) DAGPE & **0.1739**\(\) 0.0013 & **0.1879**\(\) 0.0015 & 0.263 \(\) 0.002 & 0.963 \(\) 0.001 & 0.594 \(\) 0.028 & 0.782 \(\) 0.018 \\ (\(\)) RWPE & - & - & 0.267 \(\) 0.003 & 0.962 \(\) 0.001 & 0.628 \(\) 0.014 & 0.819 \(\) 0.010 \\ (\(\)) LapPE & - & 0.271 \(\) 0.002 & 0.961 \(\) 0.001 & 0.609 \(\) 0.017 & 0.786 \(\) 0.015 \\ (\(\)) SPP Ying et al. (2021) & 0.1749 \(\) 0.0011 & 0.1881 \(\) 0.0017 & - & 0.639 \(\) 0.006 & 0.823 \(\) 0.004 \\ (\(\)) Edge Direction & 0.1751 \(\) 0.0018 & 0.1870 \(\) 0.0021 & - & - & 0.636 \(\) 0.015 & 0.817 \(\) 0.005 \\ TF & 0.1546 \(\) 0.0018 & 0.1670 \(\) 0.0015 & 0.285 \(\) 0.004 & 0.957 \(\) 0.001 & 0.568 \(\) 0.018 & 0.787 \(\) 0.003 \\ 
**DAG+SAT** & 0.1821 \(\) 0.0013 & 0.1982 \(\) 0.0010 & **0.262**\(\) 0.004 & **0.964**\(\) 0.001 & **0.627**\(\) 0.015 & **0.806**\(\) 0.007 \\ (\(\)) DAGRA & 0.1773 \(\) 0.0023 & 0.1937 \(\) 0.0028 & 0.292 \(\) 0.003 & 0.954 \(\) 0.001 & 0.598 \(\) 0.031 & 0.800 \(\) 0.012 \\ (\(\)) DAGPE & **0.1846**\(\) 0.0010 & **0.2018**\(\) 0.0021 & 0.282 \(\) 0.002 & 0.958 \(\) 0.001 & 0.623 \(\) 0.014 & 0.806 \(\) 0.005 \\ (\(\)) SPD Ying et al. (2021) & 0.1851 \(\) 0.0008 & 0.1991 \(\) 0.0018 & - & - & 0.627 \(\) 0.016 & 0.810 \(\) 0.006 \\ (\(\)) Edge Direction & 0.1839 \(\) 0.0014 & 0.1978 \(\) 0.0028 & - & - & 0.623 \(\) 0.013 & 0.804 \(\) 0.007 \\ SAT & 0.1773 \(\) 0.0023 & 0.1937 \(\) 0.0028 & 0.298 \(\) 0.003 & 0.952 \(\) 0.001 & 0.598 \(\) 0.017 & 0.798 \(\) 0.007 \\   

Table 6: Ablation results.

   Model & RMSE \(\) & Pearson’s \(\) \\  GCN & 0.482 \(\) 0.003 & 0.871 \(\) 0.001 \\ S-VAE & 0.521 \(\) 0.002 & 0.847 \(\) 0.001 \\ D-VAE & 0.375 \(\) 0.003 & 0.924 \(\) 0.001 \\ DAGNN & 0.264 \(\) 0.004 & 0.964 \(\) 0.001 \\ PACE & 0.254 \(\) 0.002 & 0.964 \(\) 0.001 \\  Transformer & 0.285 \(\) 0.004 & 0.957 \(\) 0.001 \\ GT & 0.275 \(\) 0.003 & 0.961 \(\) 0.001 \\
**DAG+Transformer** & **0.253**\(\) 0.002 & **0.966**\(\) 0.001 \\  GraphGPS & 0.306 \(\) 0.004 & 0.950 \(\) 0.001 \\
**DAG+GraphGPS** & **0.267**\(\) 0.005 & **0.964**\(\) 0.001 \\  SAT & 0.298 \(\) 0.003 & 0.952 \(\) 0.001 \\
**DAG+SAT** & **0.262**\(\) 0.004 & **0.964**\(\) 0.001 \\   

Table 5: **Regression.** Predictive performance of latent representations over NA.

represents a special case because the graphs are very small so that we can always find a directed path through them that contains all nodes; however, given that \(|V|\) is generally small, the exception of the rule \(n_{}<<|V|\) is not critical here.

**(2) Performance for Varying \(k\), Table 7, Figure 2.** We ran our model for different \(k\) on ogbg-code2, based upon vanilla Transformer and SAT. We find that incorporating reachable node information leads to small but constant improvements in performance as \(k\) increases. This confirms our intuition about the importance of predecessors and successors in DAGs, and is in line with related works suggesting to iteratively aggregate the DAGs along the partial order.

Figure 2 depicts the training time per epoch. It is easy to see that _our framework reduces computation time considerably_. For example, even the most time-consuming DAG+SAT requires 10 minutes per epoch, compared to 40 minutes for SAT and 100 minutes for DAGNN on the same GPU type. Since SAT and DAGNN represent the best-performing models among the transformers and, respectively, message-passing GNNs, and DAG attention yields qualitative improvements over both.

## 5 Conclusions

Based on the insights from graph neural networks and models for directed acyclic graphs, we have developed a transformer model biased towards DAGs, which allows for incorporating the main characteristic of DAGs - their partial order - into any transformer architecture, by encoding the reachability relation and positions resulting from it. Our architecture is simple and universal as we demonstrated in our experiments. Most importantly, it provides an effective extension improving existing graph transformers over DAGs. While our framework has successfully lifted existing (graph) transformers to the level of the state-of-the-art neural networks tailored to DAGs, our experiments show that there is still room for improvement. There is a variety of challenging tasks over many different kinds of DAGs, which are not fully solved yet; for instance, to tackle the challenge of reasoning over source code graphs, the models will probably need better language understanding. Hence DAG representation learning remains interesting for research, and our work provides an important contribution in closing the gap between transformers and other DAG neural networks.

**Limitations.** Our study is very general, but we found only a limited number of DAG datasets. We tried to resolve this by creating new ones (from the established citation datasets), yet we acknowledge that there is still room for extension. Further, there are likely specific types of DAGs which benefit from more customized modeling. Yet, our study is intended to address a general adaptivity to DAGs.

**Broader Impact.** Transformers have been popular in artificial intelligence, and it is expected that they also gain importance in graph learning. We hope to provide a tiny piece to the advancement of the area. Our framework is rather abstract, but its simplicity, efficiency, and universality make it practically usable; and we demonstrate good performance on a variety of data.

    & \)} &  \\  & & DAG+TF & DAG+SAT \\ 
1 & 1.97 & 0.1724 \(\) 0.0022 & 0.1533 \(\) 0.0108 \\
2 & 3.91 & 0.1800 \(\) 0.0023 & 0.1918 \(\) 0.0010 \\
3 & 5.69 & 0.1842 \(\) 0.0006 & 0.1934 \(\) 0.0009 \\
4 & 7.16 & 0.1849 \(\) 0.0021 & 0.1975 \(\) 0.0019 \\
5 & 8.29 & 0.1856 \(\) 0.0020 & 0.2000 \(\) 0.0005 \\
6 & 9.01 & 0.1869 \(\) 0.0020 & 0.2000 \(\) 0.0015 \\
7 & 9.40 & 0.1869 \(\) 0.0019 & 0.2004 \(\) 0.0014 \\
8 & 9.60 & 0.1875 \(\) 0.0008 & 0.2005 \(\) 0.0010 \\ \(\) & 9.78 & 0.1879 \(\) 0.0015 & 0.2018 \(\) 0.0021 \\   

Table 7: Impact of different \(k\) on DAG attention, over ogbg-code2.

Figure 2: Average training time per epoch for various \(k\) over ogbg-code2, log scale.