# Practical Equivariances via

Relational Conditional Neural Processes

Daolang Huang\({}^{1}\) Manuel Haussmann\({}^{1}\) Ulpu Remes\({}^{2}\) ST John\({}^{1}\) Gregoire Clarke\({}^{3}\) Kevin Sebastian Luck\({}^{4,6}\) Samuel Kaski\({}^{1,5}\) Luigi Acerbi\({}^{3}\)

\({}^{1}\)Department of Computer Science, Aalto University, Finland

\({}^{2}\)Department of Mathematics and Statistics, University of Helsinki

\({}^{3}\)Department of Computer Science, University of Helsinki

\({}^{4}\)Department of Electrical Engineering and Automation (EEA), Aalto University, Finland

\({}^{5}\)Department of Computer Science, University of Manchester

\({}^{6}\)Department of Computer Science, Vrije Universiteit Amsterdam, The Netherlands

{daolang.huang, manuel.haussmann, ti.john, samuel.kaski}@aalto.fi

k.s.luck@vu.nl

{ulpu.remes, gregoire.clarte, luigi.acerbi}@helsinki.fi

###### Abstract

Conditional Neural Processes (CNPs) are a class of metalearning models popular for combining the runtime efficiency of amortized inference with reliable uncertainty quantification. Many relevant machine learning tasks, such as in spatio-temporal modeling, Bayesian Optimization and continuous control, inherently contain equivariances - for example to translation - which the model can exploit for maximal performance. However, prior attempts to include equivariances in CNPs do not scale effectively beyond two input dimensions. In this work, we propose Relational Conditional Neural Processes (RCNPs), an effective approach to incorporate equivariances into any neural process model. Our proposed method extends the applicability and impact of equivariant neural processes to higher dimensions. We empirically demonstrate the competitive performance of RCNPs on a large array of tasks naturally containing equivariances.

## 1 Introduction

Conditional Neural Processes (CNPs; ) have emerged as a powerful family of metalearning models, offering the flexibility of deep learning along with well-calibrated uncertainty estimates and a tractable training objective. CNPs can naturally handle irregular and missing data, making them suitable for a wide range of applications. Various advancements, such as attentive (ACNP; ) and Gaussian (GNP; ) variants, have further broadened the applicability of CNPs. In principle, CNPs can be trained on other general-purpose stochastic processes, such as Gaussian Processes (GPs; ), and be used as an amortized, drop-in replacement for those, with minimal computational cost at runtime.

However, despite their numerous advantages, CNPs face substantial challenges when attempting to model equivariances, such as translation equivariance, which are essential for problems involving spatio-temporal components or for emulating widely used GP kernels in tasks such as Bayesian Optimization (BayesOpt; ). In the context of CNPs, kernel properties like stationarity and isotropy would correspond to, respectively, translational equivariance and equivariance to rigid transformations. Lacking such equivariances, CNPs struggle to scale effectively and emulate (equivariant) GPs even in moderate higher-dimensional input spaces (i.e., above two). Follow-up work has introduced Convolutional CNPs (ConvCNP; ), which leverage a convolutional deep sets construction toinduce translational-equivariant embeddings. However, the requirement of defining an input grid and performing convolutions severely limits the applicability of ConvCNPs and variants thereof (ConvGNP ; FullConvGNP ) to one- or two-dimensional equivariant inputs; both because higher-dimensional implementations of convolutions are poorly supported by most deep learning libraries, and for the prohibitive cost of performing convolutions in three or more dimensions. Thus, the problem of efficiently scaling equivariances in CNPs above two input dimensions remains open.

In this paper, we introduce Relational Conditional Neural Processes (RCNPs), a novel approach that offers a simple yet powerful technique for including a large class of equivariances into any neural process model. By leveraging the existing equivariances of a problem, RCNPs can achieve improved sample efficiency, predictive performance, and generalization (see Figure 1). The basic idea in RCNPs is to enforce equivariances via a relational encoding that only stores appropriately chosen _relative_ information of the data. By stripping away absolute information, equivariance is automatically satisfied. Surpassing the complex approach of previous methods (e.g., the ConvCNP family for translational equivariance), RCNPs provide a practical solution that scales to higher dimensions, while maintaining strong performance and extending to other equivariances. The cost to pay is increased computational complexity in terms of context size (size of the dataset we are conditioning on at runtime); though often not a bottleneck for the typical metalearning small-context setting of CNPs. Our proposed method works for equivariances that can be expressed relationally via comparison between pairs of points (e.g., their difference or distance); in this paper, we focus on translational equivariance and equivariance to rigid transformations.

Contributions.In summary, our contributions in this work are:

* _relational encoding_
- to encode exact equivariances directly into CNPs, in a way that easily scales to higher input dimensions.
* We propose two variants of relational encoding: one that works more generally ('Full'); and one which is simpler and more computationally efficient ('Simple'), and is best suited for implementing translation equivariance.
* We provide theoretical foundations and proofs to support our approach.
* We empirically demonstrate the competitive performance of RCNPs on a variety of tasks that naturally contain different equivariances, highlighting their practicality and effectiveness.

Outline of the paper.The remainder of this paper is organized as follows. In Section 2, we review the foundational work of CNPs and their variants. This is followed by the introduction of our proposed relational encoding approach to equivariances at the basis of our RCNP models (Section 3). We then provide in Section 4 theoretical proof that relational encoding achieves equivariance without losing essential information; followed in Section 5 by a thorough empirical validation of our claims in various tasks requiring equivariances, demonstrating the generalization capabilities and predictive performance of RCNPs. We discuss other related work in Section 6, and the limitations of our approach, including its computational complexity, in Section 7. We conclude in Section 8 with an overview of the current work and future directions.

Our code is available at https://github.com/acerbilab/relational-neural-processes.

Figure 1: **Equivariance in 1D regression. Left: Predictions for a CNP (a) and RCNP (b) in an _interpolation_ (INT) task, trained for \(20\) epochs to emulate a GP (c) with Matern-\(\) kernel and noiseless observations. The CNP underfits the context data (black dots), while the RCNP leverages translation equivariance to learn faster and yield better predictions. Right: The CNP (d) fails to predict in an _out-of-input-distribution_ (OOID) task, where the input context is outside the training range (note the shifted \(x\) axis); whereas the RCNP (e) generalizes by means of translational equivariance.**

Background: the Conditional Neural Process family

In this section, we review the Conditional Neural Process (CNP) family of stochastic processes and the key concept of equivariance at the basis of this work. Following , we present these notions within the framework of prediction maps . We denote with \(^{d_{x}}\) input vectors and \(^{d_{y}}\) output vectors, with \(d_{x},d_{y} 1\) their dimensionality. If \(f()\) is a function that takes as input elements of a set \(\), we denote with \(f()\) the set \(\{f()\}_{}\).

Prediction maps.A _prediction map_\(\) is a function that maps (1) a _context set_\((,)\) comprising input/output pairs \(\{(_{1},_{1}),,(_{N},_{N})\}\) and (2) a collection of _target inputs_\(^{}=(_{1}^{},,_{M}^{})\) to a distribution over the corresponding _target outputs_\(^{}=(_{1}^{},,_{M}^{})\):

\[(^{}|(,),^{})=p(^{}|),\] (1)

where \(=r((,),^{})\) is the _representation vector_ that parameterizes the distribution over \(^{}\) via the representation function \(r\). Bayesian posteriors are prediction maps, a well-known example being the Gaussian Process (GP) posterior:

\[(^{}|(,),^{})=(^{}|,),\] (2)

where the prediction map takes the form of a multivariate normal with representation vector \(=(,)\). The mean \(=m_{}((,),^{})\) and covariance matrix \(=k_{}(,^{})\) of the multivariate normal are determined by the conventional GP posterior predictive expressions .

Equivariance.A prediction map \(\) with representation function \(r\) is _\(\)-equivariant_ with respect to a group \(\) of transformations1 of the input space, \(:\), if and only if for all \(\):

\[r((,),^{})=r((,), ^{}),\] (3)

where \(()\) and \(\) is the set obtained by applying \(\) to all elements of \(\). Eq. 3 defines equivariance of a prediction map based on its representation function, and can be shown to be equivalent to the common definition of an _equivariant map_; see Appendix A. Intuitively, equivariance means that if the data (the context inputs) are transformed in a certain way, the predictions (the target inputs) transform correspondingly. Common groups of transformations include translations, rotations, reflections - all examples of rigid transformations. In kernel methods and specifically in GPs, equivariances are incorporated in the prior kernel function \(k(,^{})\). For example, translational equivariance corresponds to _stationarity_\(k_{}=k(-^{})\), and equivariance to all rigid transformations corresponds to _isotropy_, \(k_{}=k(||-^{}||_{2})\), where \(||||_{2}\) denotes the Euclidean norm of a vector. A crucial question we address in this work is how to implement equivariances in other prediction maps, and specifically in the CNP family.

Conditional Neural Processes.A CNP  uses an _encoder2\(f_{c}\)_ to produce an embedding of the context set, \(=f_{c}(,)\). The encoder uses a DeepSet architecture  to ensure invariance with respect to permutation of the order of data points, a key property of stochastic processes. We denote with \(_{m}=(,_{m}^{})\) the local representation of the \(m\)-th point of the target set \(^{}\), for \(1 m M\). CNPs yield a prediction map with representation \(=(_{1},,_{M})\):

\[(^{}|(,),^{})=p(^{}|)=_{m=1}^{M}q(_{m}^{}|( _{m})),\] (4)

where \(q(|)\) belongs to a family of distributions parameterized by \(\), and \(=f_{d}(_{m})\) is decoded in parallel for each \(_{m}\). In the standard CNP, the decoder network \(f_{d}\) is a multi-layer perceptron. A common choice for CNPs is a Gaussian likelihood, \(q(_{m}^{}|)=(_{m}^ {}|(_{m}),(_{m}))\), where \(\) and \(\) represent the predictive mean and covariance of each output, independently for each target (a _mean field_ approach). Given the closed-form likelihood, CNPs are easily trainable via maximum-likelihood optimization of parameters of encoder and decoder networks, by sampling batches of context and target sets from the training data.

Gaussian Neural Processes.Notably, standard CNPs do not model dependencies between distinct target outputs \(_{m}^{}\) and \(_{m^{}}^{}\), for \(m m^{}\). Gaussian Neural Processes (GNPs ) are a variant of CNPs that remedy this limitation, by assuming a joint multivariate normal structure over the outputs for the target set, \((^{}|(,),^{})= (|,)\). For ease of presentation, we consider now scalar outputs (\(d_{y}=1\)), but the model generalizes to the multi-output case. GNPs parameterize the mean as \(_{m}=f_{}(_{m})\) and covariance matrix \(_{m,m^{}}=kf_{}(_{m}),f_{}(_{m^{}})f_{}(_{m})f_{}(_{ m^{}})\), for target points \(_{m}^{},_{m^{}}^{}\), where \(f_{}\), \(f_{}\), and \(f_{}\) are neural networks with outputs, respectively, in \(\), \(^{d_{}}\), and \(^{+}\), \(k(,)\) is a positive-definite kernel function, and \(d_{}^{+}\) denotes the dimensionality of the space in which the covariance kernel is evaluated. Standard GNP models use the linear covariance (where \(f_{v}=1\) and \(k\) is the linear kernel) or the kvv covariance (where \(k\) is the exponentiated quadratic kernel with unit lengthscale), as described in .

Convolutional Conditional Neural Processes.The Convolutional CNP family includes the ConvCNP , ConvGNP , and FullConvGNP . These CNP models are built to implement translational equivariance via a ConvDeepSet architecture . For example, the ConvCNP is a prediction map \(p(^{}|(,),^{})=_{m=1}^ {M}q(_{m}^{}|_{,Y}(_{m}^{}))\), where \(_{,Y}()\) is a ConvDeepSet. The construction of ConvDeepSets involves, among other steps, gridding of the data if not already on the grid and application of \(d_{x}\)-dimensional convolutional neural networks (\(2d_{x}\) for FullConvGNP). Due to the limited scaling and availability of convolutional operators above two dimensions, ConvCNPs do not scale in practice for \(d_{x}>2\) translationally-equivariant input dimensions.

Other Neural Processes.The neural process family includes several other members, such as latent NPs (LNP; ) which model dependencies in the predictions via a latent variable - however, LNPs lack a tractable training objective, which impairs their practical performance. Attentive (C)NPs (A(C)NPs; ) implement an attention mechanism instead of the simpler DeepSet architecture. Transformer NPs  combine a transformer-based architecture with a causal mask to construct an autoregressive likelihood. Finally, Autoregressive CNPs (AR-CNPs ) provide a novel technique to deploy existing CNP models via autoregressive sampling without architectural changes.

## 3 Relational Conditional Neural Processes

We introduce now our Relational Conditional Neural Processes (RCNPs), an effective solution for embedding equivariances into any CNP model. Through relational encoding, we encode selected relative information and discard absolute information, inducing the desired equivariance.

Relational encoding.In RCNPs, the (full) _relational encoding_ of a target point \(_{m}^{}^{}\) with respect to the context set \((,)\) is defined as:

\[_{}(_{m}^{},(,))=_{ n,n^{}=1}^{N}f_{}(g(_{n},_{m}^{}), _{nn^{}}),_{nn^{}}(g( _{n},_{n^{}}),_{n},_{n^{}} ),\] (5)

where \(g:^{d_{}}\) is a chosen _comparison function3_ that specifies how a pair \(,^{}\) should be compared; \(\) is the _relational matrix_, comparing all pairs of the context set; \(f_{}:^{d_{}}^{d_{}+2d_{ }}^{d_{}}\) is the _relational encoder_, a neural network that maps a comparison vector and element of the relational matrix into a high-dimensional space \(^{d_{}}\); \(\) is a commutative aggregation operation (sum in this work) ensuring permutation invariance of the context set . From Eq. 5, a point \(^{}\) is encoded based on how it compares to the entire context set.

Intuitively, the comparison function \(g(,)\) should be chosen to remove all information that does not matter to impose the desired equivariance. For example, if we want to encode translational equivariance, the comparison function should be the difference of the inputs, \(g_{}(_{n},_{m}^{})=_{m}^{}- _{n}\) (with \(d_{}=d_{}\)). Similarly, isotropy (invariance to rigid transformations, i.e. rotations, translations, and reflections) can be encoded via the Euclidean distance \(g_{}(_{n},_{m}^{})=||_{m}^{ }-_{n}||_{2}\) (with \(d_{}=1\)). We will prove these statements formally in Section 4.

Full RCNP.The full-context RCNP, or FullRCNP, is a prediction map with representation \(=(_{1},,_{M})\), with \(_{m}=_{}(_{m}^{},(,))\) the relational encoding defined in Eq. 5:

\[(^{}|(,),^{})=p( ^{}|)=_{m=1}^{M}q(_{m}^{}|(_{m})),\] (6)

where \(q(|)\) belongs to a family of distributions parameterized by \(\), where \(=f_{d}(_{m})\) is decoded from the relational encoding \(_{m}\) of the \(m\)-th target. As usual, we often choose a Gaussian likelihood, whose mean and covariance (variance, for scalar outputs) are produced by the decoder network.

Note how Eq. 6 (FullRCNP) is nearly identical to Eq. 4 (CNP), the difference being that we replaced the representation \(_{m}=(,_{m}^{})\) with the relational encoding \(_{m}\) from Eq. 5. Unlike CNPs, in RCNPs there is no separate encoding of the context set alone. The RCNP construction generalizes easily to other members of the CNP family by plug-in replacement of \(_{m}\) with \(_{m}\). For example, a relational GNP (RGNP) describes a multivariate normal prediction map whose mean is parameterized as \(_{m}=f_{}(_{m})\) and whose covariance matrix is given by \(_{m,m^{}}=kf_{}(_{m}),f_{}(_{ m^{}})f_{v}(_{m})f_{v}(_{m^{}})\).

Simple RCNP.The full relational encoding in Eq. 5 is cumbersome as it asks to build and aggregate over a full relational matrix. Instead, we can consider the simple or 'diagonal' relational encoding:

\[_{}(_{m}^{},(,)) =_{n=1}^{N}f_{r}(g(_{n},_{m}^{}),g( _{n},_{n}),_{n}).\] (7)

Eq. 7 is functionally equivalent to Eq. 5 restricted to the diagonal \(n=n^{}\), and further simplifies in the common case \(g(_{n},_{n})=\), whereby the argument of the aggregation becomes \(f_{r}(g(_{n},_{m}^{}),_{n})\).

We obtain the simple RCNP model (from now on, just RCNP) by using the diagonal relational encoding \(_{}\) instead of the full one, \(_{}\). Otherwise, the simple RCNP model follows Eq. 6. We will prove, both in theory and empirically, that the simple RCNP is best for encoding translational equivariance. Like the FullRCNP, the RCNP easily extends to other members of the CNP family.

In this paper, we consider the FullRCNP, FullRCNP, RCNP and RGNP models for translations and rigid transformations, leaving examination of other RCNP variants and equivariances to future work.

## 4 RCNPs are equivariant and context-preserving prediction maps

In this section, we demonstrate that RCNPs are \(\)-equivariant prediction maps, where \(\) is a transformation group of interest (e.g., translations), for an appropriately chosen comparison function \(g:^{d_{}}\). Then, we formalize the statement that RCNPs strip away only enough information to achieve equivariance, but no more. We prove this by showing that the RCNP representation preserves information in the context set. Full proofs are given in Appendix A.

### RCNPs are equivariant

**Definition 4.1**.: Let \(g\) be a comparison function and \(\) a group of transformations \(:\). We say that \(g\) is _\(\)-invariant_ if and only if \(g(,^{})=g(,^{})\) for any \(,^{}\) and \(\).

**Definition 4.2**.: Given a comparison function \(g\), we define the _comparison sets_:

\[g((,),(,))= \{(g(_{n},_{n^{}}), _{n},_{n^{}})\}_{1 n,n^{} N},\] \[g((,),^{})= \{(g(_{n},_{m}^{}),_{n})\}_{1 n N,1 m M},\] (8) \[g(^{},^{})= \{g(_{m}^{},_{m^{}}^{}) \}_{1 m,m^{} M}.\]

If \(g\) is not symmetric, we can also denote \(g(^{},(,))=\{(g( _{m}^{},_{n}),_{n})\}_{1 n N,1  m M}\).

**Definition 4.3**.: A prediction map \(\) and its representation function \(r\) are _relational_ with respect to a comparison function \(g\) if and only if \(r\) can be written solely through set comparisons:

\[r((,),^{})=rg((,),( ,)),g((,),^{}),g(^{},(,)),g(^{},^{}).\] (9)

**Lemma 4.4**.: _Let \(\) be a prediction map, \(\) a transformation group, and \(g\) a comparison function. If \(\) is relational with respect to \(g\) and \(g\) is \(\)-invariant, then \(\) is \(\)-equivariant._From Lemma 4.4 and previous definitions, we derive the main result about equivariance of RCNPs.

**Proposition 4.5**.: _Let \(g\) be the comparison function used in a RCNP, and \(\) a group of transformations. If \(g\) is \(\)-invariant, the RCNP is \(\)-equivariant._

As useful examples, the _difference_ comparison function \(g_{}(,^{})=^{}-\) is invariant to translations of the inputs, and the _distance_ comparison function \(g_{}(,^{})=||^{}-||_{2}\) is invariant to rigid transformations; thus yielding appropriately equivariant RCNPs.

### RCNPs are context-preserving

The previous section demonstrates that any RCNP is \(\)-equivariant, for an appropriate choice of \(g\). However, a trivial comparison function \(g(,^{})=\) would also satisfy the requirements, yielding a trivial representation. We need to guarantee that, at least in principle, the encoding procedure removes only information required to induce \(\)-equivariance, and no more. A minimal request is that the context set is preserved in the prediction map representation \(\), modulo equivariances.

**Definition 4.6**.: A comparison function \(g\) is _context-preserving_ with respect to a transformation group \(\) if for any context set \((,)\) and target set \(^{}\), there is a submatrix \(^{}\) of the matrix \(_{mn^{}}=(g(_{n},_{m}^{}),g(_{n},_{n^{}}),_{n},_{n^{}})\), a reconstruction function \(\), and a transformation \(\) such that \((^{})=(,)\).

For example, \(g_{}\) is context-preserving with respect to the group of rigid transformations. For any \(m\), \(^{}=_{m::}\) is the set of pairwise distances between points, indexed by their output values. Reconstructing the positions of a set of points given their pairwise distances is known as the _Euclidean distance geometry_ problem , which can be solved uniquely up to rigid transformations with traditional multidimensional scaling techniques . Similarly, \(g_{}\) is context-preserving with respect to translations. For any \(m\) and \(n^{}\), \(^{}=_{m::n^{}}\) can be projected to the vector \(((_{1}-_{m}^{},_{1}),,(_{ N}-_{m}^{},_{N}))\), which is equal to \((_{m},)\) with the translation \(_{m}()=-_{m}^{}\).

**Definition 4.7**.: For any \((,),^{}\), a family of functions \(h_{}((,),^{}) ^{d_{}}\) is _context-preserving_ under a transformation group \(\) if there exists \(\), \(d_{}\), a _reconstruction function_\(\), and a transformation \(\) such that \((h_{}((,),^{})) ()=(,)\).

Thus, an encoding is context-preserving if it is possible at least in principle to fully recover the context set from \(\), implying that no relevant context is lost. This is indeed the case for RCNPs.

**Proposition 4.8**.: _Let \(\) be a transformation group and \(g\) the comparison function used in a FullRCNP. If \(g\) is context-preserving with respect to \(\), then the representation function \(r\) of the FullRCNP is context-preserving with respect to \(\)._

**Proposition 4.9**.: _Let \(\) be the translation group and \(g_{}\) the difference comparison function. The representation of the simple RCNP model with \(g_{}\) is context-preserving with respect to \(\)._

Given the convenience of the RCNP compared to FullRCNPs, Proposition 4.9 shows that we can use simple RCNPs to incorporate translation-equivariance with no loss of information. However, the simple RCNP model is _not_ context-preserving for other equivariances, for which we ought to use the FullRCNP. Our theoretical results are confirmed by the empirical validation in the next section.

## 5 Experiments

In this section, we evaluate the proposed relational models on several tasks and compare their performance with other conditional neural process models. For this, we used publicly available reference implementations of the neuralprocesses software package [1; 3]. We detail our experimental approach in Appendix B, and we empirically analyze computational costs in Appendix C.

### Synthetic Gaussian and non-Gaussian functions

We first provide a thorough comparison of our methods with other CNP models using a diverse array of Gaussian and non-Gaussian synthetic regression tasks. We consider tasks characterized by functions derived from (i) a range of GPs, where each GP is sampled using one of three different kernels (Exponentiated Quadratic (EQ), Matern-\(\), and Weakly-Periodic); (ii) a non-Gaussian sawtoothprocess; (iii) a non-Gaussian mixture task. In the mixture task, the function is randomly selected from either one of the three aforementioned distinct GPs or the sawtooth process, each chosen with probability \(\). Apart from evaluating simple cases with \(d_{x}=\{1,2\}\), we also expand our experiments to higher dimensions, \(d_{x}=\{3,5,10\}\). In these higher-dimensional scenarios, applying ConvCNP and ConvGNP models is not considered feasible. We assess the performance of the models in two distinct ways. The first one, _interpolation_ (INT), uses the data generated from a range identical to that employed during the training phase. The second one, _out-of-input-distribution_ (OOID), uses data generated from a range that extends beyond the scope of the training data.

Results.We first compare our translation-equivariant ('stationary') versions of RCNP and RGNP with other baseline models from the CNP family (Table 1). Comprehensive results, including all five regression problems and five dimensions, are available in Appendix D. Firstly, relational encoding of the translational equivariance intrinsic to the task improves performance, as both RCNP and RGNP models surpass their CNP and GNP counterparts in terms of INT results. Furthermore, the OOID results demonstrate significant improvement of our models, as they can leverage translational-equivariance to generalize outside the training range. RCNPs and RGNPs are competitive with convolutional models (ConvCNP, ConvGNP) when applied to 1D data and continue performing well in higher dimension, whereas models in the ConvCNP family are inapplicable for \(d_{x}>2\).

We further consider two GP tasks with isotropic EQ and Matern-\(\) kernels (invariant to rigid transformations). Within this set of experiments, we include the FullRCNP and FullRCNP models, each equipped with the 'isotropic' distance comparison function. The results (Table 2) indicate that RCNPs and FullRCNPs consistently outperform CNPs across both tasks. Additionally, we notice that FullRCNPs exhibit better performance compared to RCNPs as the dimension increases. When \(d_{x}=2\), the performance of our RGNPs is on par with that of ConvGNPs, and achieves the best results in terms of both INT and OOID when \(d_{x}>2\), which again highlights the effectiveness of our models in handling high-dimensional tasks by leveraging existing equivariances.

### Bayesian optimization

We explore the extent our proposed models can be used for a higher-dimensional meta-learning task, using Bayesian optimization (BayesOpt) as our application . The neural processes, ours as well as the baselines, serve as surrogates to find the global minimum \(f_{}=f(_{})\) of a black-box function. For this task, we train the models by generating random functions from a GP kernel sampled from a set of base kernels--EQ, Matern-\(\{,,\}\) as well as their sums and products--with randomly sampled hyperparameters. By training on a large distribution over kernels, we aim to exploit the metalearning capabilities of neural processes. The trained CNP models are then used as surrogates to minimize the Hartmann function [40, p.185] in three and six dimensions, a common BayesOpt test function. We use the _expected improvement_ acquisition function, which we can evaluate analytically. Specifics on the experimental setup and further evaluations can be found in Appendix E.

    & &  &  &  \\  & & & KL divergence() & & log-likelihood() & & & log-likelihood() & \\   & & \(d_{x}=1\) & \(d_{x}=3\) & \(d_{x}=5\) & \(d_{x}=1\) & \(d_{x}=3\) & \(d_{x}=5\) & \(d_{x}=1\) & \(d_{x}=3\) & \(d_{x}=5\) \\   &  & \(0.24 0.00\) & \(0.28 0.00\) & \(0.31 0.00\) & \(3.03 0.00\) & \( 0.01\) & \(0.44 0.00\) & \(0.20 0.01\) & \(0.10 0.00\) & -0.31 (0.03) \\  & & RGNP (sta) & \(0.03 0.00\) & \( 0.00\) & \( 0.00\) & \( 0.00\) & \( 0.01\) & \( 0.05\) & \(0.34 0.03\) & \( 0.01\) & \(0.04 0.02\) \\  & & ConvCMP & \( 0.00\) & - & \( 0.01\) & - & - & \( 0.02\) & - & - \\  & & ConvCMP & \( 0.00\) & - & - & \( 0.11\) & - & - & \( 0.15\) & - & - \\  & & CNP & \(0.31 0.00\) & **0.39** (0.00) & **0.42** (0.00) & **2.25** (0.02) & **0.36** (0.28) & - & \(0.03 0.10\) & \(0.01 0.01\) & -0.57 (0.11) & -0.72 (0.08) \\  & & CNP & \( 0.00\) & \( 0.01\) & \( 0.01\) & \( 0.04\) & \( 0.13\) & \(0.02 0.05\) & \(0.17 0.01\) & -0.17 (0.00) & -0.32 (0.00) \\    RCNP (sta) \\  } & \(0.24 0.00\) & \(0.28 0.01\) & \(0.31 0.00\) & \( 0.00\) & \( 0.01\) & \( 0.00\) & \(0.20 0.01\) & \(0.10 0.00\) & -0.31 (0.03) \\  & & RGNP (sta) & \(0.08 0.00\) & \( 0.00\) & \( 0.00\) & \( 0.01\) & \( 0.05\) & \(0.34 0.03\) & \( 0.01\) & \( 0.02\) \\  & & ConvCMP & \( 0.00\) & - & \( 0.04\) & - & - & \( 0.02\) & - & - \\  & & ConvCMP & \( 0.00\) & - & - & \( 0Results.As shown in Figure 2, RCNPs and RGNPs are able to learn from the random functions and come close to the performance of a Gaussian Process (GP), the most common surrogate model for BayesOpt. Note that the GP is refit after every new observation, while the CNP models can condition on the new observation added to the context set, without any retraining. CNPs and GNPs struggle with the diversity provided by the random kernel function samples and fail at the task. In order to remain competitive, they need to be extended with an attention mechanism (ACNP, AGNP).

### Lotka-Volterra model

Neural process models excel in the so-called sim-to-real task where a model is trained using simulated data and then applied to real-world contexts. Previous studies have demonstrated this capability by training neural process models with simulated data generated from the stochastic Lotka-Volterra predator-prey equations and evaluating them with the famous hare-lynx dataset . We run this benchmark evaluation using the simulator and experimental setup proposed by . Here the CNP models are trained with simulated data and evaluated with both simulated and real data; the learning tasks represented in the training and evaluation data include _interpolation_, _forecasting_, and _reconstruction_. The evaluation results presented in Table 3 indicate that the best model depends on the task type, but overall our proposed relational CNP models with translational equivariance perform comparably to their convolutional and attentive counterparts, showing that our simpler approach does not hamper performance on real data. Full results with baseline CNPs are provided in Appendix F.

### Reaction-Diffusion model

The Reaction-Diffusion (RD) model is a large class of state-space models originating from chemistry  with several applications in medicine  and biology . We consider here a reduced model representing the evolution of cancerous cells , which interact with healthy cells through the

    & &  & \)} \\  & &  &  \\   & & \(d_{s}=2\) & \(d_{s}=3\) & \(d_{s}=5\) & \(d_{s}=2\) & \(d_{s}=3\) & \(d_{s}=5\) \\   & RCNP (sta) & 0.26 (0.00) & 0.40 (0.01) & 0.45 (0.00) & 0.30 (0.00) & **0.39** (0.00) & 0.35 (0.00) \\  & RCNP (sta) & 0.03 (0.00) & **0.05** (0.00) & **0.11** (0.00) & 0.03 (0.00) & **0.05** (0.00) & **0.11** (0.00) \\  & FullRCNP (iso) & 0.26 (0.00) & 0.31 (0.00) & 0.35 (0.00) & 0.30 (0.00) & **0.32** (0.00) & **0.29** (0.00) \\  & FullRCNP (iso) & 0.08 (0.00) & 0.14 (0.00) & 0.25 (0.00) & 0.09 (0.00) & 0.16 (0.00) & 0.21 (0.00) \\  & ConCNP & 0.22 (0.00) & - & - & **0.26** (0.00) & - & - \\  & ConGNP & **0.01** (0.00) & - & - & **0.01** (0.00) & - & - \\  & CNP & 0.33 (0.00) & 0.44 (0.00) & 0.57 (0.00) & 0.39 (0.00) & 0.46 (0.00) & 0.47 (0.00) \\  & GNP & 0.05 (0.00) & 0.09 (0.01) & 0.19 (0.00) & 0.07 (0.00) & 0.11 (0.00) & 0.19 (0.00) \\    & RCNP (sta) & 0.26 (0.00) & 0.40 (0.01) & 0.45 (0.00) & 0.30 (0.00) & 0.39 (0.00) & 0.35 (0.00) \\  & RCNP (sta) & 0.03 (0.00) & **0.05** (0.00) & 0.11 (0.00) & 0.08 (0.00) & **0.04** (0.00) & **0.41** (0.00) \\  & FullRCNP (iso) & 0.26 (0.00) & 0.31 (0.00) & 0.35 (0.00) & 0.30 (0.00) & **0.32** (0.00) & 0.29 (0.00) \\  & FullRCNP (iso) & 0.08 (0.00) & 0.14 (0.00) & 0.25 (0.00) & 0.09 (0.00) & 0.16 (0.00) & 0.21 (0.00) \\  & ConGNP & 0.22 (0.00) & - & - & 0.26 (0.00) & - & - \\  & ConGNP & **0.01** (0.00) & - & - & **0.01** (0.00) & - & - \\  & CNP & 4.54 (1.76) & 3.30 (1.55) & 1.22 (0.09) & 6.75 (2.72) & 1.75 (0.42) & **0.93** (0.02) \\  & GNP & **2.25** (0.61) & **2.54** (1.44) & **0.74** (0.02) & **1.86** (0.26) & 1.23 (0.17) & **0.62** (0.02) \\   

Table 2: Comparison of the _interpolation_ (INT) and _out-of-input-distribution_ (OOID) performance of our RCNP models with different CNP baselines on two GP synthetic regression tasks with isotropic kernels of varying input dimensions.

Figure 2: **Bayesian Optimization. Error during optimization of a 3D/6D Hartmann function (lower is better). RCNP/RGNP improve upon the baselines, approaching the GP performance.**

production of acid. These three quantities (healthy cells, cancerous cells, acid concentration) are defined on a discretized space-time grid (2+1 dimensions, \(d_{x}=3\)). We assume we only observe the difference in number between healthy and cancerous cells, which makes this model a hidden Markov model. Using realistic parameters inspired by , we simulated \(4 10^{3}\) full trajectories, from which we subsample observations to generate training data for the models; another set of \(10^{3}\) trajectories is used for testing. More details can be found in Appendix G.

We propose two tasks: first, a _completion_ task, where target points at time \(t\) are inferred through the context at different spatial locations at time \(t-1\), \(t\) and \(t+1\); secondly, a _forecasting_ task, where the target at time \(t\) is inferred from the context at \(t-1\) and \(t-2\). These tasks, along with the form of the equation describing the model, induce translation invariance in both space and time, which requires the models to incorporate translational equivariance for all three dimensions.

Results.We compare our translational-equivariant RCNP models to ACNP, CNP, and their GNP variants in Table 4. Comparison with ConvCNP is not feasible, as this problem is three-dimensional. Our methods outperform the others on both the _completion_ and _forecasting_ tasks, showing the advantage of leveraging translational equivariance in this complex spatio-temporal modeling problem.

### Additional experiments

As further empirical tests of our method, we study our technique in the context of autoregressive CNPs in Appendix H.1, present a proof-of-concept of incorporating rotational symmetry in Appendix H.2, and examine the performance of RCNPs on image regression in Appendix H.3.

## 6 Related work

This work builds upon the foundation laid by CNPs  and other members of the CNP family, covered at length in Section 2. A significant body of work has focused on incorporating equivariances into neural network models [33; 4; 23; 41]. The concept of equivariance has been explored in Convolutional Neural Networks (CNNs; ) with translational equivariance, and more generally in Group Equivariant CNNs , where rotations and reflections are also considered. Work on DeepSets laid out the conditions for permutation invariance and equivariance . Set Transformers  extend

    & RCNP & RCNP & ACNP & ACNP & CNP & GNP \\  Completion & 0.22 (0.33) & **1.38** (0.62 ) & 0.17 (0.03) & 0.20 (0.03) & 0.10 (0.01) & 0.13 (0.03) \\  Forecasting & **0.07** (0.18) & **-0.18** (0.58) & -0.65 (0.31) & -1.58 (1.05) & -0.51 (0.20) & -0.50 (0.30) \\   

Table 4: Normalized log-likelihood scores in the Reaction-Diffusion problem for both tasks (higher is better). Mean and (standard deviation) from 10 training runs evaluated on a separate test dataset.

    & INT (S) & FOR (S) & REC (S) & INT (R) & FOR (R) & REC (R) \\  RCNP & -3.57 (0.02) & -**4.85** (0.00) & -4.20 (0.01) & -**4.24** (0.02) & -**4.83** (0.03) & **-4.55** (0.05) \\ RCNP & -3.51 (0.01) & **-4.27** (0.00) & -3.76 (0.00) & -**4.31** (0.06) & **-4.47** (0.03) & **-4.39** (0.11) \\  ConvCNP & -3.47 (0.01) & -**4.85** (0.00) & -**4.06** (0.00) & **-4.21** (0.04) & -5.01 (0.02) & **-4.75** (0.05) \\ ConvGNP & **-3.46** (0.00) & **-4.30** (0.00) & **-3.67** (0.01) & **-4.19** (0.02) & **-4.61** (0.03) & **-4.62** (0.11) \\  ACNP & -4.04 (0.06) & -4.87 (0.01) & -4.36 (0.03) & **-4.18** (0.05) & -4.79 (0.03) & **-4.48** (0.02) \\ AGNP & -4.12 (0.17) & -4.35 (0.09) & -4.05 (0.19) & -4.33 (0.15) & **-4.48** (0.06) & **-4.29** (0.10) \\   

Table 3: Normalized log-likelihood scores in the Lotka–Volterra experiments (higher is better). The mean and (standard deviation) reported for each model are calculated based on 10 training outcomes evaluated with the same simulated (S) and real (R) learning tasks. The tasks include _interpolation_ (INT), _forecasting_ (FOR), and _reconstruction_ (REC). Statistically significantly (see Appendix B.1) best results are **bolded**. RCNP and RGNP models perform on par with convolutional and attentive baselines.

this approach with an attention mechanism to learn higher-order interaction terms among instances of a set. Our work focuses on incorporating equivariances into prediction maps, and specifically CNPs.

Prior work on incorporating equivariances into CNPs requires a regular discrete lattice of the input space for their convolutional operations [15; 30]. EquivCNPs  build on work by  which operates on irregular point clouds, but they still require a constructed lattice over the input space. SteerCNPs  generalize ConvCNPs to other equivariances, but still suffer from the same scaling issues. These methods are therefore in practice limited to low-dimensional (one to two equivariant dimensions), whereas our proposal does not suffer from this constraint.

Our approach is also related to metric-based meta-learning, such as Prototypical Networks  and Relation Networks . These methods learn an embedding space where classification can be performed by computing distances to prototype representations. While effective for few-shot classification tasks, they may not be suitable for more complex tasks or those requiring uncertainty quantification. GSSM  aims to _learn_ relational biases via a graph structure on the context set, while we directly build _exact_ equivariances into the CNP architecture.

Kernel methods and Gaussian processes (GPs) have long addressed issues of equivariance by customizing kernel designs to encode specific equivariances . For instance, stationary kernels are used to capture globally consistent patterns , with applications in many areas, notably Bayesian Optimization . However, despite recent computational advances , kernel methods and GPs still struggle with high-dimensional, complex data, with open challenges in deep kernel learning  and amortized kernel learning (or metalearning) [26; 35], motivating our proposal of RCNPs.

## 7 Limitations

RCNPs crucially rely on a comparison function \(g(,^{})\) to encode equivariances. The comparison functions we described (e.g., for isotropy and translational equivariance) already represent a large class of useful equivariances. Notably, key contributions to the neural process literature focus only on translation equivariance (e.g., ConvCNP , ConvGNP , FullConvGNP ). Extending our method to other equivariances will require the construction of new comparison functions.

The main limitation of the RCNP class is its increased computational complexity in terms of context and target set sizes (respectively, \(N\) and \(M\)). The FullRCNP model can be cumbersome, with \(O(N^{2}M)\) cost for training and deployment. However, we showed that the simple or 'diagonal' RCNP variant can fully implement translational invariance with a \(O(NM)\) cost. Still, this cost is larger than \(O(N+M)\) of basic CNPs. Given the typical metalearning setting of small-data regime (small context sets), the increased complexity is often acceptable, outweighed by the large performance improvement obtained by leveraging available equivariances. This is shown in our empirical validation, in which RCNPs almost always outperformed their CNP counterparts.

## 8 Conclusion

In this paper, we introduced Relational Conditional Neural Processes (RCNPs), a new member of the neural process family which incorporates equivariances through relational encoding of the context and target sets. Our method applies to equivariances that can be induced via an appropriate comparison function; here we focused on translational equivariances (induced by the difference comparison) and equivariances to rigid transformations (induced by the distance comparison). How to express other equivariances via our relational approach is an interesting direction for future work.

We demonstrated with both theoretical results and extensive empirical validation that our method successfully introduces equivariances in the CNP model class, performing comparably to the translational-equivariant ConvCNP models in low dimension, but with a simpler construction that allows RCNPs to scale to larger equivariant input dimensions (\(d_{x}>2\)) and outperform other CNP models.

In summary, we showed that the RCNP model class provides a simple and effective way to implement translational and other equivariances into the CNP model family. Exploiting equivariances intrinsic to a problem can significantly improve performance. Open problems remain in extending the current approach to other equivariances which are not expressible via a comparison function, and making the existing relational approach more efficient and scalable to larger context datasets.