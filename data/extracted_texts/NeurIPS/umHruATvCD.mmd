# Learning Interpretable Characteristic Kernels via Decision Forests

Anonymous Author(s)

Affiliation

Address

email

###### Abstract

Decision forests are popular tools for classification and regression. These forests naturally generate proximity matrices that measure the frequency of observations appearing in the same leaf node. While other kernels are known to have strong theoretical properties such as being characteristic, there is no similar result available for decision forest-based kernels. In addition, existing approaches to independence and k-sample testing may require unfeasibly large sample sizes and are not interpretable. In this manuscript, we prove that the decision forest induced proximity is a characteristic kernel, enabling consistent independence and k-sample testing via decision forests. We leverage this to introduce kernel mean embedding random forest (KMERF), which is a valid and consistent method for independence and k-sample testing. Our extensive simulations demonstrate that KMERF outperforms other tests across a variety of independence and two-sample testing scenarios. Additionally, the test is interpretable, and its key features are readily discernible. This work therefore demonstrates the existence of a test that is both more powerful and more interpretable than existing methods, flying in the face of conventional wisdom of the trade-off between the two.

## 1 Introduction

Decision forests are ensemble method popularized by Breiman . It is highly effective in classification and regression tasks, particularly in high-dimensional settings [5; 6; 37]. This is achieved by randomly partitioning the feature set and using subsampling techniques to construct multiple decision trees from the training data. To measure the similarity between two observations, a proximity matrix can be constructed, defined as the percentage of trees in which both observations lie in the same leaf node . This proximity matrix serves as an induced kernel or similarity matrix for the decision forest. In general, any random partition algorithm may produce such a kernel matrix.

As the complexity of datasets grow, it becomes increasingly necessary to develop methods that can efficiently perform independence and k-sample testing. We also desire methods that are interpretable, lending insight into how and why statistically significant results were determined. Parametric methods are often highly interpretable, such as Pearson's correlation and its rank variants [22; 33; 16]. These methods are still popular to detect linear and monotonic relationships in univariate settings, but they are not consistent for detecting more complicated nonlinear relationships. Nonparametric methods can be very powerful. The more recent distance correlation (Dcorr) [36; 35] and the kernel correlation (HSIC) [10; 11] are consistent for testing independence against any distribution of finite second moments for any finite dimensionality; moreover, the energy-based statistics (such as Dcorr) and kernel-based statistics (such as HSIC) are known to be exactly equivalent for all finite samples [21; 30]. The theory supporting universal consistency of these methods (which we refer to as kernel methods hereafter, without loss of generality) depends on those kernels being characteristickernel [28; 18; 19; 30]. Unfortunately, the above tests do not attempt to further characterize the dependency structure. To the best of our knowledge, very few tests exist[38; 15].

In addition, although these methods all have asymptotic guarantees, for finite samples, performance can be impaired by poorly choosing a particular characteristic kernel. Choosing an appropriate kernel that properly summarize geometries within the data is often times non-obvious . High-dimensional data is particularly vexing [25; 38], and a number of extensions have been proposed to achieve better power such as adaptive metric kernel choice , low-dimensional projections , and marginal correlations .

In this paper, we leverage the popular random forest method  and a recent chi-square test  for a more powerful and interpretable method for hypothesis testing. We prove that the random forest induced kernel is a characteristic kernel, and the resulting kernel mean embedding random forest (KMERF) is a valid and consistent method for independence and k-sample testing. We then demonstrate its empirical advantage over existing tools for high-dimensional testing in a variety of dependence settings, suggesting that it will often be more powerful than existing approaches in real data. As random forest can directly estimate feature importances , the outputs of KMERF are also interpretable, KMERF therefore flies in the face of conventional wisdom that one must choose between power and interpretability: KMERF is both empirically more powerful and more interpretable than existing approaches.

## 2 Preliminaries

### Hypothesis Testing

The testing independence hypothesis is formulated as follows: suppose \(x_{i}^{p}\) and \(y_{i}^{q}\), and \(n\) samples of \((x_{i},y_{i})}{{}}F_{XY}\), i.e., \(x_{i}\) and \(y_{i}\) are realizations of random variables \(X\) and \(Y\). The hypothesis for testing independence is

\[H_{0} :F_{XY}=F_{X}F_{Y},\] \[H_{A} :F_{XY} F_{X}F_{Y}.\]

Given any kernel function \(k(,)\), we can formulate the kernel induced correlation measure as \(c_{k}^{n}(,)\) using the sample kernel matrices [10; 30], where \(=\{x_{i}\}\) and \(=\{y_{i}\}\). When the kernel function \(k(,)\) is characteristic, it has been shown that \(c_{k}^{n}(,) 0\) if and only if \(\) and \(\) are independent .

The k-sample hypothesis is formulated as follows: let \(u_{i}^{j}^{p}\) be the realization of random variable \(U_{j}\) for \(j=1,,l\) and \(i=1,,n_{j}\). Suppose the \(l\) datasets that are sampled i.i.d. from \(F_{1},,F_{l}\) and independently from one another. Then,

\[H_{0} :F_{1}=F_{2}==F_{l},\] \[H_{A} :\ j j^{}F_{j} F_{j^{}}.\]

By concatenating the \(l\) datasets and introducing an auxiliary random variable, the kernel correlation measure can be used for k-sample testing .

### Characteristic Kernel

**Definition 1**.: _Let \(\) be a separable metric space, such as \(^{p}\). A kernel function \(k(,):\) measures the similarity between two observations in \(\), and an \(n n\) kernel matrix for \(\{x_{i},i=1,,n\}\) is defined by \((i,j)=k(x_{i},x_{j})\)._

* _A kernel_ \(k(,):\) _is positive definite if, for any_ \(n 2\)_,_ \(x_{1},,x_{n}\) _and_ \(a_{1},,a_{n}\)_, it satisfies_ \[_{i,j=1}^{n}a_{i}a_{j}k(x_{i},x_{j}) 0.\]
* _A characteristic kernel is a positive definite kernel that has the following property: for any two random variables_ \(X_{1}\) _and_ \(X_{2}\) _with distributions_ \(F_{X_{1}}\) _and_ \(F_{X_{2}}\)_,_ \[E[k(,X_{1})]=E[k(,X_{2})]F_{X_{1}}=F_{X_{2}}.\] (1)Kmerf

The proposed approach for hypothesis testing, KMERF, involves the following steps:

1. Run random forest with \(m\) trees, with independent bootstrap samples of size \(n_{b} n\) used to construct each tree. The tree structures (partitions) within the forest \(\) are denoted as \(_{w}\), where \(w 1,,m\) and \(_{w}(x_{i})\) represents the partition assigned to \(x_{i}\).
2. Calculate the proximity kernel by \[_{ij}^{}=_{w=1}^{m}[(_{w}(x_{ i})=_{w}(x_{j}))],\] where \(()\) is the indicator function that checks whether the two observations lie in the same partition in each tree.
3. Compute the unbiased kernel transformation  on \(^{}\). Namely, let \[_{ij}^{}=_{ij}^{}-_{t=1}^{n}_{it}^{}- _{s=1}^{n}_{sj}^{}+ _{s,t=1}^{n}_{st}^{}&i j\\ 0&i=j\]
4. Let \(^{}\) be the Euclidean distance induced kernel by Shen and Vogelstein , or the proximity kernel in the case that dimensions of \(\) and \(\) is the same, that is \(p=q\), and compute \(^{}\) using the same unbiased transformation. Then the KMERF statistic for the induced kernel \(k\) is, \[c_{k}^{n}(,)=(^{ }^{}).\]
5. Compute the p-value via the following chi-square test : \[p=1-F_{_{1}^{2}-1}(n^{n}(,)}{ ^{n}(,) c_{k}^{n}(,)} }),\] where \(_{1}^{2}\) is the chi-square distribution of degree \(1\). Reject the independence hypothesis if the p-value is less than a specified type 1 error level, say \(0.05\).

In the numerical implementation, the standard supervised random forest is used with \(m=500\) (which is also applicable to the unsupervised version or other random forest variants ). In the second step, we simply compute the proximity kernel defined by the random forest induced kernel. In the third step, we normalize the proximity kernel to ensure it obtains a consistent dependence measure; this is the KMERF test statistic. We found that utilizing the multiscale version of the kernel correlation , which is equivalent for linear relationships while being better for nonlinear relationships, produced similar results to using distance correlation, but substantially increased runtimes.

Note that one could also compute a p-value for KMERF via the permutation test, which is a standard procedure for testing independence . Specifically, first compute a kernel on the observed \(\{x_{i}\}\) and \(\{y_{i}\}\). Then randomly permute the index of \(\{y_{i}\}\), repeat the kernel generation process for \(\{y_{i}\}\) for each permutation. This process involves training a new random forest for each permutation. Finally, compute the test statistic for each of the permutations, and the p-value equals the percentage the permuted statistics that are larger than the observed statistic. However, the permutation test is very slow for large sample size and almost always yields similar results as the chi-square test.

## 4 Theoretical Properties

Here, we show that the random forest kernel characteristic, and the induced test statistic used in KMERF allows for valid and universally consistent independence and k-sample testing. All proofs are in appendix.

For a kernel to be characteristic, it first needs to be positive definite, which is indeed the case for the forest-induced kernel:

**Theorem 1**.: _The random forest induced kernel \(^{}\) is always positive definite._

This theorem holds because the forest-induced kernel is a summation of a permuted block diagonal matrix, with each matrix coming from individual tree, that is positive definite ; and a summation of positive definite matrices is still positive definite.

Next, we show the kernel is characteristic when the tree partition area converges to zero. A similar property is also used for proving classification consistency for k-nearest-neighbors , and we shall denote \(N(_{w})\) as the maximum area of each part.

**Theorem 2**.: _Suppose as \(n,m\), \(N(_{w}) 0\) for each tree \(_{w}\) and each observation \(x_{i}\). Then the random forest induced kernel \(^{}\) is asymptotically characteristic._

Intuitively, for sufficiently many trees and sufficiently small leaf region, observations generated by two different distributions cannot always be in the same leaf region.

This leads to the validity and consistency result of KMERF:

**Corollary 2.1**.: _KMERF satisfies_

\[_{n}c_{k}^{n}(,)=c 0,\]

_with equality to 0 if and only if \(F_{XY}=F_{X}F_{Y}\). Moreover, for sufficiently large \(n\) and sufficiently small type 1 error level \(\), this method is valid and consistent for independence and k-sample testing._

By Gretton et al. , any characteristic-kernel based dependence measure converges to \(0\) if and only if \(X\) and \(Y\) are independent. Moreover, Shen et al.  showed that the chi-square distribution \(_{1}^{2}-1\) approximates and upper-tail dominates the true null distribution of any unbiased kernel when using distance correlation, making it a valid and consistent test.

## 5 Simulations

In this section we exhibit the consistency and validity of KMERF, and compare its testing power with other competitors in a comprehensive simulation set-up. We utilize the hyppo package in Python , which uses scikit-learn random forest with \(500\) trees and otherwise default hyper-parameters, and calculate the proximity matrix from this. The KMERF statistic and p-value then computed via the process in Section 3. The mathematical details for each simulation type is in the Appendix C.

### Testing Independence

In this section we compare KMERF to Multiscale Graph Correlation (MGC), Distance Correlation (Dcorr), Hilbert-Schmidt Independence Criterion (Hsic), and Heller-Heller-Gorfine (HHG) method, Canonical Correlation Analysis (CCA), and the RV coefficient. The HHG method has been shown to work extremely well against nonlinear dependencies . The MGC method has been shown to work well against linear, nonlinear, and high-dimensional dependencies . The CCA and RV coefficients are popular multivariant extensions of Pearson correlation. For each method, we use the corresponding implementation in hyppo with default settings.

We take \(20\) high-dimensional simulation settings , consisting of various linear, monotone, and strongly nonlinear dependencies with \(p\) increasing, \(q=1\), and \(n=100\). To estimate the testing power in each setting, we generate dependent \((x_{i},y_{i})\) for \(i=1,,n\), compute the test statistic for each method, repeat for \(r=10000\) times. Via the empirical alternative and null distribution of the test statistic, we estimate the testing power of each method at type 1 error level of \(=0.05\). The power result is shown in Figure 1 shows that KMERF achieves superior performance for most simulation modalities, except a few like circle and ellipse.

### Two Sample Testing

Here, we compare the performance in the two-sample testing regime. It has been shown that all independence measures can be used for two-sample testing [21; 30], allowing all previous independence testing methods to be compared here as well. Once again, we investigate statisticalpower differences with 20 simulation settings consisting of various linear and nonlinear, monotonic and nonmonotonic functions with dimension increasing from \(p=3,,10\), \(q=1\), and \(n=100\). We then apply a random rotation to this generated simulation and generate the second independent sample (via a rigid transformation).

Figure 2 shows that, once again, for the majority of simulations settings, KMERF performs at or better than other tests in nearly all simulations and simulation dimensions. For certain simulation settings, especially the exponential, cubic, and fourth root, KMERF vastly outperforms other metrics as dimensions increases.

### Interpretability

Not only does KMERF typically offer empirically better statistical power compared to alternatives, it also offers insights into which features are the most important within the data set. Figure 3 shows normalized 95% confidence intervals of relative feature importances for each simulation, where the black line shows the mean and the light grey line shows the 95% confidence interval. Mean and individual tree feature importances were normalized using min-max feature scaling. The simulations were modified such that the weighting of each feature decreased as feature importance increased, with the expectation that the algorithm would detect a decrease in feature importance as dimension increased. With these simulations, we are able to determine that exact feature importance trend,

Figure 1: Multivariate independence testing power for \(20\) different settings with increasing \(p\), fixed \(q=1\), and \(n=100\). For the majority of the simulations and simulation dimensions, KMERF performs as well as, or better than, existing multivariate independence tests in high-dimensional dependence testing.

except for a few of the more complex simulations. The process we used to generate this figure can be trivially extended to a two-sample or k-sample case.

## 6 Real Data

We then applied KMERF to a date set consisting of proteolytic peptides derived from the blood samples of 95 individuals harboring pancreatic (n=10), ovarian (n=24), colorectal cancer (n=28), and healthy controls (n=33) . The processed data included 318 peptides derived from 121 proteins (see Appendix D for full details). Figure 4 shows the p-values for KMERF between pancreatic and healthy subjects compared to the p-values for KMERF between pancreatic cancer and all other subjects. The test identifies neurogramin as a potentially valuable marker for pancreatic cancer, which the literature also corroborates [41; 40]. Meanwhile, while some of the other tests identified this biomarker, they identified others that are upregulated in other types of cancers as well (false positives). We also show in the figure that the biomarker chosen be KMERF provides better true positive detection when compared to the other tests (there is no ground truth in this case, so a leave-one-out k-nearest-neighbor classification approach was used instead).

Figure 2: Multivariate two-sample testing power for \(20\) different settings with increasing \(p\), fixed \(q=1\), and \(n=100\). For nearly all simulations and simulation dimensions, KMERF performs as well as, or better than, existing multivariate two-sample tests in high-dimensional dependence testing.

Figure 4: (A) For each peptide, the p-values for testing dependence between pancreatic and healthy subjects by KMERF is compared to the p-value for testing dependence between pancreatic and all other subjects. At the critical level 0.05, KMERF identifies a unique protein. (B) The true and false positive counts using a k-nearest neighbor (choosing the best \(k\)) leave-one-out classification using only the significant peptides identified by each method. The peptide identified by KMERF achieves the best true and false positive rates.

Figure 3: Normalized mean (black) and 95% confidence intervals (light grey) using min-max normalization for relative feature importances derived from random forest over five dimensions for each simulation tested for 100 samples. The features were sorted from most to least informative for all simulations except for the Independence simulation). As expected, estimated feature importance decreases as dimension increases. A feature of KMERF is insights into interpretability, and we show here which dimensions of our simulations influence the outcome of independence test the most.

## 7 Discussion

KMERF is, to the best of our knowledge, one of the first learned kernel that is proven to be characteristic. The empirical experiments presented here illustrate the potential advantages of learning kernels, specifically for independence and k-sample testing.

In fact, multiscale graph correlation [38; 31] can be thought of, in a sense, as kernel learning: given \(n\) samples, and a pair of kernel or distance functions, it chooses one of the approximately \(n^{2}\) sparsified kernels, by excluding all but the nearest neighbors for each data point [38; 31]. Because random forest can be thought of as a nearest neighbor algorithm , in a sense, the forest induced kernel is a natural extension of Vogelstein et al. , which leads to far more data-adaptive estimates of the nearest neighbors using supervised information. Moreover, proving that the random-forest induced kernel is characteristic is a first step towards building lifelong learning kernel machines with strong theoretical guarantees [24; 39].

As the choice of kernel is crucial for empirical performance, this manuscript offers a new kernel construction that is not only universally consistent for testing independence, but also exhibits strong empirical advantages, especially for high-dimensional testing. What is unique to this choice of kernel is the robustness and interpretability. It will be worthwhile to further understand the underlying theoretical mechanism of the induced characteristic kernel, as well as evaluating the performance of these forest induced kernels on other learning problems, including classification, regression, clustering, and embedding .