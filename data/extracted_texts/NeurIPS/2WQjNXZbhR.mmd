# Dendritic Integration Inspired Artificial Neural Networks Capture Data Correlation

Chongming Liu

Jingyang Ma

Songting Li

Douglas Zhou

###### Abstract

Incorporating biological neuronal properties into Artificial Neural Networks (ANNs) to enhance computational capabilities is under active investigation in the field of deep learning. Inspired by recent findings indicating that dendrites adhere to quadratic integration rule for synaptic inputs, this study explores the computational benefits of quadratic neurons. We theoretically demonstrate that quadratic neurons inherently capture correlation within structured data, a feature that grants them superior generalization abilities over traditional neurons. This is substantiated by few-shot learning experiments. Furthermore, we integrate the quadratic rule into Convolutional Neural Networks (CNNs) using a biologically plausible approach, resulting in innovative architectures--Dendritic integration inspired CNNs (Dit-CNNs). Our Dit-CNNs compete favorably with state-of-the-art models across multiple classification benchmarks, e.g., ImageNet-1K, while retaining the simplicity and efficiency of traditional CNNs. All source code are available at https://github.com/liuchongming1999/Dendritic-integration-inspired-CNN-NeurIPS-2024.

## 1 Introduction

While the Artificial Neural Network (ANN) framework has made significant advancements towards solving complex tasks, it still faces problems that are rudimentary to real brains . A notable distinction between the modern ANN framework and the human brain is that the former relies on a significant number of training samples, which consumes large amounts of energy, whereas the latter runs on extremely low power (<20 watts) and possesses a strong generalization capability based on few-shot learning. Studies have demonstrated that incorporating dendritic features in ANNs can alleviate these issues and enhance overall performance [52; 38; 30]. However, it is difficult to quantify the nonlinear integration of dendrites, which is an essential property that allows individual neurons to perform complex computations [43; 49]. As a result, dendritic-inspired models often employ linear integration with nonlinear activation functions such as ReLU and Sigmoid [21; 37].

In light of recent studies revealing that the somatic responses of biological neurons to multiple synaptic inputs on dendrites follow a quadratic integration rule [15; 28], we explore the computational benefit of the quadratic neuron model . This model substitutes the linear integration and nonlinearactivation function of traditional point neurons with quadratic integration as follows1:

\[f(x)=(w x+b) f(x)=x^{T}Ax+w x+b.\] (1)

The expected value of a quadratic term \(E_{x}[a_{ij}x_{i}x_{j}]\) closely relates to the covariance between variables \(x_{i}\) and \(x_{j}\) (\(x_{i}\) and \(x_{j}\) are the \(i\)-th and the \(j\)-th components of vector \(x\), respectively), suggesting an inherent capability for capturing input correlation. This attribute allows biological neurons to exhibit remarkable performance in correlation detection tasks [1; 29]. Here, we theoretically demonstrate that quadratic neurons indeed naturally capture correlation within structured data, which is pivotal in numerous machine learning applications , such as language generation  and video understanding . This intrinsic property endows quadratic neurons with superior generalization capabilities compared to traditional neurons, as evidenced by few-shot learning experiments. We further propose a biologically plausible method to integrate them into Convolutional Neural Networks (CNNs). Applying this approach to ResNet  and ConvNeXt  results in novel CNN models--Dendritic integration inspired ResNet (Dit-ResNet) and Dendritic integration inspired ConvNeXt (Dit-ConvNeXt), respectively. Evaluations on CIFAR-10 and CIFAR-100 datasets demonstrate that Dit-ResNets significantly enhance test accuracy by merely replacing a single layer of point neurons with quadratic neurons. On the ImageNet-1K dataset , Dit-ConvNeXts demonstrate significant enhancements in top-1 accuracy with only a one percent increase in parameters, exhibiting competitive performance compared to state-of-the-art models. Ablation studies further substantiate the effectiveness of quadratic neurons and their capability for capturing data correlation.

This paper is structured as follows: Section 2 reviews previous works related to dendritic-inspired models and high-order interactive operations in neural networks. Section 3 presents a theoretical analysis of the computational benefits derived from quadratic neurons, as demonstrated through few-shot learning experiments. Section 4 describes the architecture of Dit-CNNs and evaluates their performance on several computer vision benchmark datasets. Section 5 concludes the paper. All numerical experiments in this paper are conducted using Python and executed on 8 Tesla A100 computing cards with a 7nm GA100 GPU, featuring 6,912 CUDA cores and 432 tensor cores.

## 2 Related work

Dendritic-inspired computational models.The significant role of dendrites in the nonlinear integration within neural systems is well-recognized, enabling neurons to perform intricate tasks [43; 49; 2]. Recently, numerous studies have explored the implementation of dendritic features in ANNs from different aspects, yielding encouraging results. In [52; 21; 37], the integration of dendritic morphology into ANNs has led to higher test accuracies on simple tasks compared to traditional ANNs. Furthermore, dendritic plasticity rules have been employed to develop learning algorithms in [38; 36], effectively replacing the non-biological backpropagation algorithm and achieving enhanced performance on classification tasks with small data sizes. Additionally, in , dendritic nonlinearity is considered by adding input to each layer with the Hadamard product rule, which has shown improvements in approximation and classification experiments.

Neural networks with high-order interactions.Recent advancements in the design of deep learning architectures have been predominantly driven by the ability to capture high-order statistics among inputs and features. Over recent decades, there has been a pivotal shift in the foundational neural networks used for computer vision tasks: moving from convolution-based architectures [16; 24; 56] to Transformer-based models [11; 33; 9]. This transition has been driven by the development of the self-attention mechanism, which facilitates quadratic interactions among inputs. Additionally, a novel category of neural networks, referred to as Mamba , has demonstrated exceptional performance in tasks that require the processing of extensive high-order statistical information, such as video understanding [32; 59; 27]. The backbone of Mamba's architecture is a state space model that enables high-order interactions among inputs. Moreover, the integration of high-order spatial interactions into CNNs has also been shown to enhance performance significantly . These developments underscore the significance of high-order statistical information in tackling large-scale complex tasks and suggest that incorporating high-order interactions as a foundational aspect of model design is an effective strategy.

Previous works have explored the concept of quadratic integration as a sophisticated alternative to traditional linear summation. Table 1 summarizes various neuron models formulated in quadratic formats, along with their relevant references. The one-rank format of quadratic neurons, when applied across entire networks, is documented in [54; 12; 4; 53], demonstrating enhanced performance in classification tasks. Studies such as [48; 20; 35; 60] have implemented quadratic neurons on a pixel-wise basis within convolution layers, yielding modest improvements in test accuracy. In contrast, Table 1 illustrates how our Dit-CNNs approach departs from these models by integrating a channel-wise quadratic operations with biological interpretation and providing a theoretical analysis of the computational advantages offered by quadratic neurons.

## 3 Quadratic neurons possess enhanced generalization capabilities

This section provides a theoretical analysis demonstrating how quadratic neurons enhance generalization capabilities by effectively capturing correlation within structured data. Additional numerical experiments corroborate this assertion. The detailed proofs can be found in Appendix A.

### Binary classification for normal distributions

Assume that the data points of two classes are equally sampled from two different non-degenerate normal distributions: \(class_{1} N(_{1},_{1}),\;class_{2} N(_{2},_{2})\), where \(_{j}^{d},\;_{j}^{d d}\;(j=1,2)\). Then the optimal classifier \(y_{opt}(x):^{d}\{1,2\}\) can be defined according to the sampling probability:

\[y_{opt}(x)=*{arg\,max}_{j\{1,2\}}p_{j}(x),\] (2)

where \(p_{j}(x)\) is the probability (density function) of sampling point \(x\) from distribution \(N(_{j},_{j})\).

If a single quadratic neuron, as described in Equation (1) (\(f(x)=x^{T}Ax+w x+b\), where \(A\) is a symmetric matrix), is used to solve the above binary classification task, we can prove the following theorems:

**Theorem 1**.: _(Existence) The critical points with respect to the cross-entropy loss \(L(A,w,b)\) are given as follows:_

\[A^{*}=_{1}^{-1}-_{2}^{-1},\;w^{*}=-2(_{1}^{-1}_{1}- _{2}^{-1}_{2}),\;b^{*}=_{1}^{T}_{1}^{-1}_{1}-_{2}^{T}_ {2}^{-1}_{2}+(|}{|_{2}|}).\]

_i.e._

\[_{A^{*},w^{*},b^{*}}=0,\;\;\;_{A^{*},w^{*},b^{*}}=0,\;\;\; _{A^{*},w^{*},b^{*}}=0,\]

_where_

\[L(A,w,b)=[E_{x class_{1}}((1+e^{f(x)}))+E_ {x class_{2}}((1+e^{-f(x)}))].\]

_Moreover, the corresponding classifier generated by this formula is the same as the theoretically optimal classifier in Equation (2)._

**Theorem 2**.: _(Uniqueness) Consider the conditional cross-entropy loss defined on a set \((^{d})\), where \((^{d})\) denotes the Lebesgue \(\)-algebra on \(^{d}\):_

\[L(A,w,b)=[E_{x class_{1},x}( (1+e^{f(x)}))+E_{x class_{2},x}((1+e^{-f(x)}) )],\]

   Reference & Quadratic format & How quadratic operation is used & Biological interpretation & Theory for generalization \\ 
,,,  & \(f(x)=(w_{a}x)(w_{b}x)\) & Pixel-Wise & N & N \\ 
,,, & \(f(x)=x^{T}Ax\) & Pixel-Wise & N & N \\ 
**Dit-CNNs** & \(f(x)=x^{T}Ax\) & Channel-Wise & Y & Y \\   

Table 1: The overview of current works with quadratic format.

_where_

\[E_{x class_{j},x}(g(x))=_{}g(x)p_{j}(x)\,d,\]

_then the unique solution satisfying_

\[_{A^{*},w^{*},b^{*}}=0,\; _{A^{*},w^{*},b^{*}}=0,\; _{A^{*},w^{*},b^{*}}=0\]

_for every \((^{d})\) is given by:_

\[A^{*}=_{1}^{-1}-_{2}^{-1},\;w^{*}=-2(_{1}^{-1}_{1}- _{2}^{-1}_{2}),\;b^{*}=_{1}^{T}_{1}^{-1}_{1}-_{2}^{T} _{2}^{-1}_{2}+(|}{|_{2}|}).\]

Theorem 1 identifies an analytical critical point for a quadratic neuron with cross-entropy loss, which emerges as the optimal classifier. On the other hand, Theorem 2 establishes the uniqueness of this critical point under conditional cross-entropy loss, where \(\) denotes the regions occupied by distinct batches of data points. Under these conditions, the quadratic parameters will converge to this unique critical point when using the stochastic gradient descent (SGD) algorithm. It is demonstrated that, unlike traditional neuron, quadratic neuron inherently possesses the ability to converge to the optimal classification solution by directly including the covariance matrix. This finding is supported by simulated data in Task 1 (identical covariance) and Task 2 (non-identical covariance) shown in Figure 1, which indicates that quadratic neuron consistently achieves the theoretically optimal classification outcome. Moreover, in comparison to a multi-layer perceptron (MLP) with two layers, quadratic neuron surpass traditional neuron in Task 2 which require capturing correlation information from data distributions (require fewer training samples to converge to the optimal solution). This highlights the unique capability of quadratic neurons to identify internal correlations within structured data, offering a possible explanation for their superior generalization capability compared to traditional neurons, a topic that will be further explored in numerical experiments below.

Figure 1: **Visualization of classifier boundaries for two models (a single quadratic neuron with 2-dimensional inputs and a two-layer MLP comprising 2-ReLU(10)-1) across two distinct tasks.** For each task, the impact of varying training sample sizes is examined. Model comparisons are performed under uniform conditions, utilizing identical random seeds and hyperparameters for fairness. Training is executed using gradient descent with a learning rate of 0.1 over 10,000 epochs. The term “theoretical” denotes the optimal classifier’s boundary as specified in Equation (2), while “numerical” represents the empirical classification boundary obtained from the model.

### Few-shot learning experiments on MNIST and Arabic MNIST

From the above classification tasks for normal distributions as depicted in Figure 1, it is suggested that quadratic neurons may require fewer training samples in comparison to traditional neurons. We next examine this few-shot learning capability of quadratic neurons on the MNIST  and Arabic MNIST datasets . The results, presented in Figure 2, indicate that quadratic neurons outperform MLP with two layers when trained with a limited number of samples on both datasets. This evidence shows the enhanced generalization ability of quadratic neurons in applications. Further insights into the mechanism through which quadratic neurons capture data correlation in the MNIST dataset, and a related theorem concerning multi-class classification for normal distributions, are detailed in Appendix B.

## 4 Integrating quadratic neurons into CNNs with biological plausibility

Figure 3: **Schematic of the biological interpretation of Dit-CNNs.** Dit-CNNs is inspired by neural networks in the visual system. For example, different types of cone cells encode various color (channel) information, and retinal ganglion cells receive inputs from multiple types of cone cells , the responses can be modeled as having receptive fields (convolutional kernels) related to different color channels (\(w_{1}*x_{1},w_{2}*x_{2},w_{3}*x_{3}\)). When multiple channel inputs are present, traditional CNNs simply linearly sum the corresponding responses. In contrast, dendritic neurons integrate these inputs with an additional quadratic term based on the dendritic quadratic integration rule. This approach leads to the formulation of Dit-CNNs after simplification.

Figure 2: **Performance of two models on few-shot learning tasks with MNIST and Arabic MNIST datasets.** The first model consists of 10 quadratic neurons with a 784-dimensional input, while the second model is a MLP with the configuration of 784-ReLU(8000)-10. Both models are evaluated under identical conditions using the same training protocol, which includes SGD with a learning rate of 0.1 and a batch size of 100 across 20 epochs. The term ’Sample size’ refers to the ratio of the number of training samples to the full training set. Experiments are conducted for each model and sample ratio across ten runs, and the resulting test accuracy is depicted through an error bar plot (error bar represents the upper and lower bound for test accuracy).

The concept of convolution , inspired by the discovery of the receptive field in the cat visual cortex , laid the foundation for CNNs  which have been designed to efficiently tackle large-scale vision tasks. The underlying biological interpretation of convolution is that each neuron exhibits varying responses at different image locations based on its receptive field (convolution kernel). Traditionally, CNNs have focused solely on the linear integration of inputs from presynaptic neurons. Specifically, for an input \(X^{C_{in} H_{in} W_{in}}\), where \(C_{in}\) represents the number of input layer neurons (channels), and \(H_{in}\) and \(W_{in}\) denote the height and width of the input feature, respectively. Given convolution kernels \(w^{C_{in} C_{out}(2l+1)(2l+1)}\), the convolution output, \(Y=Conv(X)^{C_{out} H_{out} W_{out}}\), represents the response of neuron \(i\) at location \([j,k]\) to inputs from locations \([j-l:j+l,k-l:k+l]\) across all input layer neurons:

\[Y[i,j,k]=_{m=1}^{C_{in}}w[m,i,:,:]*X[m,j-l:j+l,k-l:k+l].\] (3)

However, acknowledging that neurons with dendrites obey a quadratic integration rule, we propose a novel approach that incorporates quadratic neurons into CNNs as depicted in Figure 3. This adaptation incorporates a quadratic integration term among inputs from different neurons (channels). Specifically, with a quadratic integration coefficient \(A^{C_{out} C_{in} C_{in}}\), the output neuron's response in Equation (3) is modified as follows:

\[Y[i,j,k]=_{m=1}^{C_{in}}w[m,i,:,:]*X[m,j-l:j+l,k-l:k+l]+[ :,j,k]A[i,:,:]X[:,j,k]}_{}.\] (4)

### Evaluations on CIFAR-10 and CIFAR-100

Dataset and models.Our initial experiments utilize the CIFAR dataset, which includes 50,000 training images and 10,000 testing images. The experimental setup follows the ResNet configurations as outlined in , comprising models such as ResNet-20, ResNet-32, ResNet-56 and ResNet-110. To reduce computational demands, we incorporate quadratic neurons specifically into one layer--namely, the second layer of the first block in the second stage of the ResNet architecture . Additionally, to address issues related to gradient dynamics, such as gradient explosion, a Layer Normalization (LN) layer  is implemented immediately before the layer equipped with quadratic neurons.

Experimental settings.The training process incorporates SGD with a weight decay of \(0.0001\), a batch size of \(128\), and a momentum of \(0.9\). Following the recommendations from , quadratic

   Model &  \# Param. \\ (CIFAR10) \\  &  Acc. \\ (CIFAR10) \\  &  \# Param. \\ (CIFAR100) \\  & 
 Acc. \\ (CIFAR100) \\  \\  ResNet-20 & 0.27M & 91.25\% & 0.30M & 67.26\(\)0.68\% \\ QResNet-20 & 0.81M & 92.22\% & 0.84M & 67.82\(\)0.52\% \\ QuadraResNet-20 & 0.81M & 92.21\% & 0.84M & 68.02\(\)0.44\% \\ Dit-ResNet-20 & 0.30M & **92.66\%** & 0.33M & **68.66\(\)0.34\%** \\  ResNet-32 & 0.46M & 92.49\% & 0.49M & 68.52\(\)0.55\% \\ QResNet-32 & 1.39M & 93.10\% & 1.42M & 69.41\(\)0.48\% \\ QuadraResNet-32 & 1.39M & 93.11\% & 1.42M & 69.54\(\)0.44\% \\ Dit-ResNet-32 & 0.49M & **93.17\%** & 0.52M & **69.68\(\)0.32\%** \\  ResNet-56 & 0.86M & 93.03\% & 0.89M & 70.17\(\)0.67\% \\ QResNet-56 & 2.55M & 93.66\% & 2.58M & 71.21\(\)0.44\% \\ QuadraResNet-56 & 2.55M & 93.79\% & 2.58M & 70.98\(\)0.76\% \\ Dit-ResNet-56 & 0.89M & **93.90\%** & 0.92M & **71.40\(\)0.35\%** \\  ResNet-110 & 1.73M & 93.57\% & 1.76M & 70.84\(\)0.76\% \\ QResNet-110 & 5.17M & 93.88\% & 5.20M & 71.58\(\)0.87\% \\ QuadraResNet-110 & 5.17M & 93.77\% & 5.20M & 71.72\(\)0.81\% \\ Dit-ResNet-110 & 1.76M & **94.33\%** & 1.79M & **72.40\(\)0.85\%** \\   

Table 2: Comparative performance of Dit-ResNets and structurally similar models on CIFAR.

integration parameters are initialized to zero, and a distinct learning rate is adopted for these parameters. Initially, the learning rate for the quadratic integration matrix is set at \(1\), and \(0.1\) for all other parameters. These rates is reduced by a factor of ten at 80 and 120 epochs, with training concluding at 160 epochs. Data augmentation procedures mirror the original protocol: each image is padded by 4 pixels on each side, follows by random cropping of a \(32 32\) section from the padded image or its horizontal flip. For testing, a single view of the original \(32 32\) image is evaluated.

Results.Table 2 presents a comparative analysis of CIFAR performance between our Dit-ResNets and the original ResNet model , as well as two adaptations that incorporate quadratic neurons , which have shown superior outcomes among existing approaches with quadratic formats . For the CIFAR-10 dataset, given the extensive benchmarking in prior studies, we conduct ten runs of our Dit-ResNets, reporting the optimal outcome for comparative analysis. For CIFAR-100, we independently execute all models, documenting average test accuracy and variance (mean\(\)std). Our experiments indicate significant performance enhancements on both CIFAR-10 and CIFAR-100 datasets after integrating quadratic neurons into a single layer of ResNet, confirming their efficacy. Furthermore, when comparing a deeper ResNet model with our Dit-ResNet (e.g., ResNet-110 vs. Dit-ResNet-56), our approach not only boosts performance but also reduces training overhead. This suggests that augmenting the model with quadratic integration parameters is more effective than merely increasing network depth--a traditionally favored strategy. In comparison to other models employing quadratic neurons , our Dit-ResNets consistently achieve the highest test accuracy with the fewest parameters. This underscores the efficacy of our approach in leveraging the computational advantages of quadratic neurons. These neurons, as discussed in Section 3, demonstrate enhanced generalization capabilities.

### Evaluations on ImageNet-1K

Dataset and settings.We extend our investigations to the ImageNet-1K dataset , which comprises 1.28 million training images and 50,000 validation images across 1,000 categories. Our analysis primarily focuses on top-1 accuracy on the validation set. Training is conducted at a resolution of \(224 224\), supplemented by a comprehensive suite of data augmentation and regularization strategies. These include RandAugment , mixup , cutmix , label smoothing , layer scale , random erasing , exponential moving average (EMA) , and stochastic depth . These techniques are inspired by the _timm_ library  and methodologies from Touvron et al. . Detailed descriptions of these hyperparameters are provided in Appendix C.

Model description.Mirroring our approach with the CIFAR dataset, we integrate quadratic neurons into a single layer of the ConvNeXt model  to conserve computational resources. This adaptation results in the creation of three distinct models: Dit-ConvNeXt-T, Dit-ConvNeXt-S, and Dit-ConvNeXt-B. Specific details about the layer replacement are discussed in the ablation study.

Results.Table 3 presents a comparative analysis of Dit-ConvNeXts against state-of-the-art models from the existing literature. Compared to the original ConvNeXt counterparts, our Dit-ConvNeXts exhibit an average increase of 0.5% in top-1 accuracy, with only a marginal average increase of 1% rise

Figure 4: Visualization of some performance results presented in Table 2 (left) and Table 3 (right).

in parameter count. Moreover, our models maintain competitive performance against Transformer-based, SSM-based, and CNN-based architectures, highlighting the efficacy and adaptability of Dit-ConvNeXts. Figure 4 further shows that our model consistently improves in accuracy as the model size increases, while other methods tend to saturate, indicating superior scaling property for our model.

### Ablation study

#### 4.3.1 Dit-CNNs capture data correlation

The expectation of the quadratic term for Gaussian variables can be expressed as follows:

\[E_{x N(,)}[x^{T}Ax]=^{T}A+(A).\]

This equation highlights that the term \((A)\) encapsulates the information derived from data correlation. Consequently, Table 4 examines the impact of this term on the performance of Dit-CNNs in tackling complex tasks. The data presented in Table 4 show a notable decline in accuracy for Dit-CNNs when this term is omitted. This underscores the significance of the quadratic neurons within Dit-CNNs, demonstrating their pivotal role in effectively capturing data correlation, which is essential for task performance.

   Arch. & Model & \# Param. & FLOPs & Top-1 acc. (\%) \\   & Swin-T  & 29M & 4.5G & 81.3 \\  & DeiT-S  & 22M & 4.6G & 79.8 \\   & VMamba-T  & 22M & 5.6G & 82.2 \\  & VideoMamba-S  & 26M & 4.3G & 81.2 \\   & ResNet-50  & 26M & 4.1G & 80.4 \\  & SLaK-T  & 30M & 5.0G & 82.5 \\  & QuadraNet36-T  & 24M & 4.1G & 82.2 \\  & DeepMAD-29M  & 29M & 4.5G & 82.5 \\  & ConvNeXt-T  & 29M & 4.5G & 82.1 \\   & Dit-ConvNeXt-T & 29M & 5.0G & **82.6** \\  Transformers & Swin-S  & 50M & 8.7G & 83.0 \\   & VMamba-S  & 44M & 11.2G & 83.5 \\  & VideoMamba-M  & 74M & 12.7G & 82.8 \\   & ResNet-101  & 45M & 7.8G & 81.5 \\  & ResNet-152  & 60M & 11.5G & 82.0 \\  & SLaK-S  & 55M & 9.8G & 83.8 \\  & QuadraNet36-S  & 50M & 8.9G & 83.8 \\  & DeepMAD-50M  & 50M & 8.7G & **83.9** \\  & ConvNeXt-S  & 50M & 8.7G & 83.1 \\  & Dit-ConvNeXt-S & 50M & 9.2G & 83.6 \\   & Swin-B  & 88M & 15.4G & 83.5 \\  & DeiT-B  & 87M & 17.6G & 81.8 \\   & VMamba-B  & 75M & 18.0G & 83.7 \\   & SLaK-B  & 95M & 17.1G & 84.0 \\  & QuadraNet36-B  & 90M & 15.8G & 84.1 \\   & DeepMAD-89M  & 89M & 15.4G & 84.0 \\   & ConvNeXt-B  & 89M & 15.4G & 83.8 \\   & Dit-ConvNeXt-B & 90M & 16.7G & **84.2** \\   

Table 3: Dit-ConvNeXts versus state-of-the-art (SOTA) models on ImageNet-1K. All models listed in the table are trained and validated at a resolution of \(224 224\).

#### 4.3.2 Incorporate quadratic neurons with minimal computational overhead

To manage computational costs effectively, only three layers within the ConvNeXt architecture have been identified as suitable candidates for the integration of quadratic neurons, as depicted in Figure 5. We explore the most effective layer for the integration of quadratic neurons and elucidate the rationale behind this choice. Figure 5 indicates that substituting traditional neurons with quadratic neurons in the first layer of the block 3 yields the most significant performance improvement. Additionally, a negative correlation is observed between the model's final accuracy and the accuracy after omitting the quadratic term from Dit-ConvNeXt-T post-training. This correlation underscores the critical influence of quadratic neurons on the model's efficacy. These findings suggest that Dit-CNNs demonstrating enhanced performance are those where quadratic neurons play a crucial role, highlighting the superior generalization capabilities of quadratic neurons compared to traditional neurons.

#### 4.3.3 Channel/Pixel-wise quadratic neuron utilizations on ImageNet-1K

As previously discussed, our Dit-CNNs employ quadratic neurons in a channel-wise manner, which provides a clear biological interpretation, as detailed in Section 4. Meanwhile, employing quadratic neurons on a pixel-wise basis, akin to the quadratic convolution outlined in , , and , leads to a scenario where dendritic integration occurs only when neurons receive synaptic inputs from the

   Model & Dataset &  Performance \\ (Original) \\  &  Performance \\ (minus \((A)\)) \\  & 
 Performance \\ (minus \(x^{T}Ax\)) \\  \\  Dit-ResNet-32 & CIFAR-10 & 93.17\% & 37.78\% & 12.11\% \\  Dit-ResNet-32 & CIFAR-100 & 69.68\% & 22.33\% & 1.32\% \\  Dit-ConvNeXt-T & ImageNet-1K & 82.6\% & 73.7\% & 70.6\% \\   

Table 4: The performance of Dit-CNNs and their counterparts, from which the covariance term \((A)\) and the quadratic term \(x^{T}Ax\) are omitted in quadratic neurons (\(\) is estimated from training samples).

Figure 5: **Left:** Structure of ConvNeXt highlighting three candidate layers (in red) for integrating quadratic neurons. **Right:** ImageNet-1k performance on different Dit-ConvNeXt-T, blue dots indicates top-1 accuracy (left blue vertical axis) while green dots indicates the accuracy post-removal of the quadratic term from Dit-ConvNeXt-T after training (right green vertical axis).

same neuron. This setup simplifies interactions between different neurons to a linear approximation, a method that diverges from biological plausibility within the brain. The superior performance achieved through the channel-wise application of quadratic neurons, as evidenced in Table 5, underscores the efficacy of our model. This result supports the hypothesis that models which more closely mirror brain-like mechanisms tend to exhibit enhanced performance.

## 5 Conclusion

In this paper, we first provide a theoretical demonstration of the computational advantages of quadratic neurons in capturing internal correlation within structured data. These neurons model dendritic integration rules observed in biological neurons. Our empirical evaluations using the MNIST and Arabic MNIST datasets for few-shot learning validate our theoretical assertions. Drawing from the biological interpretation of CNNs, we introduce a biologically plausible method to integrate quadratic neurons into CNN architectures, resulting in Dit-CNNs. These Dit-CNNs not only exhibit significant performance enhancements with minimal modifications to their original counterparts but also compete favorably with state-of-the-art models. The potential applicability of our approach to other architectures, such as Deep-MAD , hints at further performance improvements. The promising results of this research underscore the vast potential of brain-inspired models. Given that the quadratic integration rule of neurons could be confined to specific brain areas , future electrophysiological experiments could reveal other neuronal integration rules. Our findings could guide the development of brain-inspired deep neural networks by incorporating various integration rules corresponding to different brain areas in distinct layers. Additionally, how to theoretically analyze these new brain-inspired models will be an important issue. It is our aim to extend our analysis concerning high-order statistical information to explore the generalization error of these innovative brain-inspired models. We hope that our work will stimulate further investigations into brain-inspired models, ultimately contributing to the quest for artificial general intelligence (AGI).

## Limitations and Discussions

Theoretical results for quadratic neurons.Our theoretical framework is established on the assumption that training samples are normally distributed, a simplification that might not fully encapsulate the complexity inherent in real-world datasets. Despite this assumption, our models have demonstrated exceptional performance on various tasks including ImageNet-1K. This underscores the potential of quadratic neurons in capturing correlation within more intricate data distributions. Future efforts will focus on developing a more comprehensive theoretical foundation to better understand the mechanisms through which quadratic neurons achieve this capability.

Computational cost of quadratic neurons.While our Dendritic integration inspired Convolutional Neural Networks (Dit-CNNs) strategically limit the increase in the number of learnable parameters by selectively incorporating quadratic neurons in a singular layer, this modification undeniably raises the computational complexity, as evidenced by an increase in Floating Point Operations (FLOPs). Nevertheless, given the substantial enhancements our Dit-CNNs contribute to model performance, it warrants further investigation into optimizing the efficiency of quadratic neuron deployment. For instance, drawing inspiration from the inherent sparsity observed in the quadratic coefficients of biological neurons--specifically, the pronounced quadratic interactions among synaptic inputs on the same dendritic branch --it is conceivable to predefine a sparse configuration for the quadratic coefficients within our Dit-CNNs. This approach could potentially reduce computational demands while maintaining performance gains.