# Semi-Random Matrix Completion

via Flow-Based Adaptive Reweighting

 Jonathan A. Kelner

MIT, kelner@mit.edu.

Jerry Li

Microsoft Research, jerrl@microsoft.com.

Allen Liu

MIT, cliu568@mit.edu.

Aaron Sidford

Stanford University, sidford@stanford.edu.

Kevin Tian

University of Texas at Austin, kjtian@cs.utexas.edu.

###### Abstract

We consider the well-studied problem of completing a rank-\(r\), \(\)-incoherent matrix \(^{d d}\) from incomplete observations. We focus on this problem in the _semi-random setting_ where each entry is independently revealed with probability at least \(p=(r,, d)}{d}\). Whereas multiple nearly-linear time algorithms have been established in the more specialized _fully-random setting_ where each entry is revealed with probablity exactly \(p\), the only known nearly-linear time algorithm in the semi-random setting is due to , whose sample complexity has a polynomial dependence on the inverse accuracy and condition number and thus cannot achieve high-accuracy recovery. Our main result is the first high-accuracy nearly-linear time algorithm for solving semi-random matrix completion, and an extension to the noisy observation setting. Our result builds upon the recent short-flat decomposition framework of  and leverages fast algorithms for flow problems on graphs to solve adaptive reweighting subproblems efficiently.

## 1 Introduction

How can we ensure learning algorithms do not overfit to generative assumptions on their input data? Since worst-case statistical inference problems are often intractable (i.e., without distributional assumptions), to develop efficient algorithms, we often introduce a generative model as a proxy for average-case, real-world behavior. However, if these algorithms are to be useful practically, we must ensure they do not depend too strongly on any artificial properties of the generative model. Ideally, they should require only what is statistically and algorithmically necessary, and no more.

In this paper, we consider this question in the context of matrix completion, one of the most fundamental and well-studied linear inverse problems. In matrix completion, the goal is to recover a symmetric rank-\(r\) matrix \(^{d d}\) where \(r d\),6 given a small (typically sublinear) number of entries revealed independently and at random. In a seminal line of work , it was shown that under natural structural assumptions on \(\), one can fully recover \(\) given \( dr\) randomly revealed entries in polynomial time, via semidefinite programming (SDP). Subsequently, there has been extensive work attempting to improve upon statistical and computational guarantees for matrix completion. In an oversimplification of the literature (see Section 1.3 for a more detailed and nuanced discussion), this has resulted in two categories of matrix completion algorithms.

* **SDP-based algorithms** achieve the nearly-optimal sample complexity, but are based on solving SDPs, which requires time \((d^{3.5})\) even with state of the art solvers .
* **First-order iterative methods** require a slightly higher sample complexity, namely, they require \(m pd^{2}\) revealed entries for an observation probability \(p=((r, d))\). However,they use faster optimization primitives and can be implemented in time \((m(r, d))\).

Throughout the introduction, we will somewhat informally refer to such algorithms as _fast_.7

Because matrix completion is particularly interesting to study when the target rank \(r\) is substantially smaller than the dimension \(d\) (as information-theoretically, we require \( dr\) observations), it may appear that fast algorithms are able to nearly match the statistical guarantees of the SDP-based methods, while offering substantially lower runtimes. In light of this, naively one might expect that first-order iterative methods ought to be always preferred over the alternative. However, it has been noted that in many real-world situations, these first-order iterative methods actually fail to perform reliably, see e.g. . An oft-cited reason for this failure is exactly the tendency for these sorts of methods to demonstrate the type of generative overfitting described above . Indeed, in real-world applications, the distribution of revealed entries is usually very far from uniformly random , which can cause iterative methods that rely too heavily on this assumption to fail dramatically. This begs the following natural question:

_Can we design fast algorithms for matrix completion that are robust to generative overfitting?_

In light of this discussion, we consider a _semi-random_ variant of matrix completion, introduced in . Recall that in standard matrix completion, we assume every entry \((i,j)[d][d]\) is revealed independently with probability \(p\). In the _\(p\)-semi-random model_ (Definition 2), we only know \(p(0,1)\) such that every entry is revealed independently with an _unknown_ probability \(p_{ij} p\). Given that real-world data is unlikely to be generated exactly uniformly, it is reasonable to believe that the semi-random model is a more faithful model of matrix completion in practice.

In this model, the algorithm is given strictly more information than in the standard (uniform) matrix completion setup. This is because the semi-random model is equivalent to the setting where we are first given a standard matrix completion instance, and then additional (but potentially adversarially chosen) entries are revealed.8 It is straightforward to show that the SDP-based approaches generalize to succeed in the semi-random setting, since the adversary's revealed entries just translate to additional constraints in the convex program satisfied by the ground truth. However, as demonstrated in , Appendix A, previously known first-order iterative methods can fail to converge in this semi-random setting. We view this as strong theoretical evidence that such algorithms exhibit large amounts of generative overfitting. We can now more concretely rephrase the previous question:

_Can we design fast algorithms for semi-random matrix completion?_

This paper focuses on the theoretical complexity of the problem; the relationship of the techniques in this work to efficient matrix completion in practice is an interesting direction for future research. Precisely, we seek to design an algorithm for semi-random matrix completion which works for the same (or qualitatively similar) \(p\) as in the standard setting, and which runs in time \((m(r, d))\)? Here \(m\) denotes the total number of revealed entries, a proxy for the input size; note that in the semi-random setting, it could be that \(m pd^{2}\).

The main result of  is an efficient algorithm for semi-random matrix completion, albeit with some important caveats, as we discuss shortly. Roughly speaking, for \((0,1)\),  gave an algorithm which, given observations of rank-\(r\)\(\) with condition number \(\) from the \(p\)-semi-random model for \(p=((r,,))\), outputs \(^{}\) so that \(\|^{}-\|_{}^{2}\|\| _{}^{2}\) with high probability. Moreover, it runs in nearly-linear time \((m(r,,))\), where \(m\) is the number of revealed entries. While this is a promising first step towards matrix completion in the semi-random model, it leaves several important questions unanswered.

* **High-accuracy recovery.** Both the sample complexity and runtime of the algorithm in  depend polynomially on \(^{-1}\). In particular, the algorithm cannot recover the matrix \(\) to \(^{-1}=(d)\) levels of precision in nearly-linear time. Thus, even in the standard setting where we assume that the bit complexity of the entries of \(\) are polynomially bounded,  cannot achieve exact recovery of the matrix in nearly-linear time. Unfortunately, this dependence seems inherent to the techniques therein (see the discussion in Section 1.2).
* **Condition number dependence.** Another important downside of the  algorithm's guarantee is its polynomial dependences on \(\) in both the sample complexity and runtime. No dependence on \(\) is required information-theoretically, or by the SDP-based methods. This aspect also seems somewhat inherent to the non-convex iterative method used in .
* **Noise tolerance.** Finally, the guarantees in  require that \(\) is exactly low-rank, i.e. it does not generalize to handle noise in observations. In practice, this is unlikely, and guarantees should ideally hold even when we only assume that the observed \(\) is close to low-rank.

### Our results

Our main result is a new fast algorithm for semi-random matrix completion that addresses all three of the aforementioned issues. Specifically, we show the following main result.

**Theorem 1** (informal, see Theorem 2).: _Let \(^{}^{d d}\) be a symmetric, rank-\(r\), \(\)-incoherent matrix, let symmetric \(^{d d}\) satisfy \(_{}\), and let \(}=^{}+\). Let \((0,1)\). There is an algorithm that takes observations from \(}\) in the \(p\)-semi-random model for \(p=\), where \(Q=(r,,())\), and outputs \(\) as a rank-\(Q\) factorization such that with high probability,_

\[-^{}_{} Q( +^{}_{}).\]

_The algorithm runs in nearly-linear time \(O(mQ)\) where \(m\) is the number of revealed entries._

Here, \(_{}\) refers to the elementwise \(_{}\) norm of a matrix, and \(_{}\) refers to the Frobenius norm. We pause to comment on Theorem 2. First, in the noiseless setting, i.e., when \(=0\), our algorithm has a polylogarithmic dependence on \(^{-1}\), and thus achieves high-accuracy recovery in nearly-linear time. In particular, as long as the bit complexity of the underlying matrix is polynomially bounded, we achieve exact recovery in nearly-linear time. Moreover, our rate has no dependence on the condition number \(\). In both of these aspects, our result qualitatively improves upon , while retaining a nearly-linear runtime and \((d(r))\) sample complexity. We also remark that the incoherence assumption in Theorem 2 (or a less standard variant thereof), which is formally defined in Definition 1, is used in all low-rank matrix completion algorithms we are aware of, as the problem is intractable without subspace regularity assumptions (see, e.g., discussion in ).

Finally, we observe that our guarantee applies to the noisy setting, i.e., \(>0\). To our knowledge, this is the first such guarantee in the literature for semi-random matrix completion, even for SDP-based methods. We achieve guarantees in terms of the \(_{}\) norm of the noise matrix \(\); moreover, the overhead is a polynomial of only \(r()\), a preferable guarantee to the \((d)\) overheads known to be achievable in the standard, fully random setting by semidefinite programming .

We do note that in the standard matrix completion setting, error guarantees are typically phrased in terms of the Frobenius norm of the noise \(\). However, in the semi-random setting, some measure of the "element-wise" noise level is unavoidable, because the semi-random noise could be chosen to only reveal unusually large entries of the noise matrix. We believe it is an interesting open question whether or not one can achieve somewhat more global measures of error such as the Frobenius norm of large entries in the revealed noise, which are more comparable to guarantees recently attained for semi-random sparse linear regression .

### Our techniques

We achieve these results via a fundamentally different approach than the prior work of , detailed in Section 2. At a (very) high level,  relies on finding a single reweighting of the revealed entries of \(\) that is good in a global sense. This roughly corresponds to finding a mask of the revealed entries, so that the masked matrix is guaranteed to be a good spectral approximation of the ground truth matrix. Given such a mask,  then shows that a non-convex optimization method will recover the true matrix, with high probability. While this is a very natural approach, this also directly results in a \(()\) dependence in the runtime and sample complexity of the algorithm. This is because the quality of the spectral approximation directly goes into the final accuracy of their overall algorithm; ultimately, they need a \((1+)\)-multiplicative spectral approximation to the ground truth. However, all known techniques for finding such a high-quality spectral approximation require a \(()\) dependence in both sample complexity and runtime.

We instead devise an iterative method that is guaranteed to make progress, assuming that the iterate satisfies a local progress criterion. We then find a local reweighting of the matrix which allows us to find a step which satisfies this criterion. While this local progress criterion is much more involved to state, and this reweighting must now be done every iteration (rather than just once at the beginning of the algorithm), this framework has an important advantage over the global criterion used previously. Critically, now each step of our method need only seek a reweighting which achieves a constant factor of relative progress to the target matrix. Thus, rather than needing a \((1+)\)-multiplicative approximation at every step, we only need a constant multiplicative approximation to make progress. This is the main insight which allows us to achieve polylogarithmic rates in \(\).

Our algorithm builds upon the short-flat decomposition-based approach to designing iterative methods for linear inverse problems introduced in , and specialized in  to matrix completion in the standard setting. However, there are significant technical challenges in adapting this framework to semi-random models. At a high level, the matrix completion problem is not well-conditioned, so a crucial step in the  algorithm is dropping rows and columns of a difference matrix (between an iterate and the target) which are estimated as too large. Random observations (held out via sample splitting) are then used in matrix concentration inequalities to estimate the difference matrix on the remaining indices, and dropped indices are later recovered.

Unfortunately, all of these steps crucially rely on independent sampling and hence break in the presence of semi-random noise. We instead take a fresh look at what certificates of progress are possible in the semi-random setting. Our key technical contribution, Algorithm 2, is a subroutine which makes bicriteria progress: it either finds an adaptive reweighting which accurately estimates the difference on a large submatrix after denoising, or certifies that a few rows and columns are responsible for much of the difference and hence can be dropped. To achieve our nearly-linear runtime, we crucially exploit graphical structure of feasible reweightings present in the randomly-revealed entries, and use flow-based optimization algorithms to solve our reweighting subproblem.

### Related work

Here, we review related work on matrix completion and semi-random models. These lines of research are vast and we focus on the research most relevant to our setting. For a more comprehensive review, we refer the interested reader to surveys on these topics, e.g., .

Matrix completion.Matrix completion was first introduced in  in the context of collaborative filtering, e.g., for the Netflix Prize . Since then, it has found applications in various areas, such as signal processing , social network analysis , causal inference , and most recently, AI alignment . In a seminal line of work,  demonstrated that SDPs based on nuclear norm minimization can solve matrix completion with sublinear sample complexity, and in polynomial time. Another line of work focuses on first-order iterative methods for matrix completion, see e.g., . These works demonstrate that optimization frameworks, such as alternating minimization, can provably and efficiently recover the true matrix. Finally, yet another line of work seeks to understand the non-convex landscape of descent-based methods for matrix completion . In terms of theoretical runtimes, the state of the art \((dr^{3+o(1)})\), is given in . While some of the aforementioned papers have implementations, comprehensively evaluating the practical performance of all of these methods is an interesting direction for future work. As noted in , none of these fast methods are known to succeed in the presence of semi-random noise.

Semi-random models.Semi-random models were first introduced in a pair of seminal papers , in part as a means to test whether learning algorithms which succeed under random models generalize to more realistic generative modeling assumptions. Most early work on semi-random models focused on understanding semi-random variants of various constraint satisfaction problems, see e.g. . More recently, they have also been studied in the context of learning theory, in the context of clustering problems , sparse and overcomplete linear regression , planted clique , and more .

Of particular interest to us is the recent line of work on developing fast learning algorithms in semi-random models . The closest and most direct comparison to our work is the aforementioned , which we quantitatively improve upon in a number of ways. It is worth noting that the specific \((r) r^{6}\) dependence in the sample complexity of  is lower than in Theorem 1. We view our result as a promising proof-of-concept of the tractability of semi-randommatrix completion via fast algorithms; both our dependence and the dependence of  are somewhat large at the moment, limiting immediate practical deployment, but we believe that it is an interest direction for future research to tighten dependencies on problem parameters and seek more practical algorithms.

## 2 Technical overview

At a high level, our algorithm follows the paradigm in  for designing fast iterative algorithms that are robust to semi-random models, which used this framework for sparse linear regression. Broadly, the approach is to first find deterministic conditions which guarantee that a step of the iterative method, which descends along a reweighted direction based on semi-random observations, makes progress. We then develop a custom nearly-linear time algorithm for computing such a reweighting, using optimization tools specialized to the regularity of graph-structured polytopes.

Short-flat decompositions for progress.We first discuss the verifiable conditions we impose to certify progress of our iterative method. Throughout this section, we let our current iterate be \(^{d d}\) and let the ground truth be \(^{}^{d d}\), assume both matrices have rank at most \(r\), and focus on the noiseless setting \(=_{d d}\) for simplicity. We also assume for normalization purposes that \(\|-^{}\|_{}=1\), and our goal is to take a step \(^{}+\) so that

\[\|(+)-^{}\|_{}\,.\] (1)

Our conditions are inspired by the approach in , which gives improved iterative algorithms for standard matrix completion (without semi-random noise). Let \(p\) be the minimum observation probability, let \(\) be the set of observed entries from \(^{}\), and let \(^{}\) be the set of truly random observations, i.e., when each entry is revealed with probability exactly \(p\) (we can find a coupling so that \(^{}\) always). Inspired by a similar framework in the sparse recovery setting , the key certificate for guaranteeing progress of the form (1) in  is that of a short-flat decomposition. We say a candidate step \(\) has a short-flat decomposition if we can write \(=+\) where \(\|\|_{} 1\) and \(\|\|_{} cr^{-}\) for a small constant \(c\): here we think of \(\) as the "short" part and \(\) as the "flat" part of the decomposition. If we can further ensure that enough signal is captured, i.e., \(,^{}- 1-c\), then  give a short argument based on Holder's inequality that a step in \(\), combined with a low-rank projection for denoising, will imply (1).

Thus, it suffices to find a matrix \(\) that has these properties; for intuition, we would ideally like to take \(==^{}-\) and \(=_{d d}\), which would complete the matrix in one shot. Naturally, however, we are limited by the fact that \(\) should be supported on the observations \(\), as otherwise we cannot make any guarantees on \(,^{}-\). Naively, one would hope to simply take the empirical observations \(=[-^{}]_{^{}}\) (which zeroes entries outside \(^{}\)), an unbiased estimator of \(^{}-\), and appeal to matrix concentration arguments to show existence of a short-flat decomposition. Unfortunately, this is not the case, as entries, rows, or columns of \(^{}-\) which are too large can arbitrarily hinder matrix concentration bounds, and  gives simple examples showing this dependence on the imbalance is unavoidable for uniform reweightings.

However,  made the important insight that when matrix concentration fails, it must be that \(-^{}\) is overly concentrated on a few heavy rows and columns, and hence dropping a few rows and columns significantly reduces \(\|-^{}\|_{}\). These rows and columns can later be recovered using the low-rank assumption, once enough progress has been made. The algorithm in  then works by repeatedly estimating heavy rows and columns, explicitly dropping them to form a set \(S[d]\) of balanced indices, and then taking the step \(=[-^{}]_{^{}(S  S)}\). This necessity of dropping certain poorly-behaved indices is a crucial difference between the matrix completion setting and simpler semi-random inverse problems such as sparse linear regression , which possess stronger global regularity features such as the restricted isometry property (RIP).

Certifying progress in semi-random models.In the semi-random matrix completion problem, we need several modifications to the strategies of , to handle the interplay between estimating heavy indices and making certifiable progress. Firstly, we cannot accurately estimate the \(_{2}\) norm of rows and columns of the true difference matrix \(-^{}\) from observations, because the semi-random adversary can drastically distort our estimates. However, we show in Algorithm 1 that we can use the \(_{}\) norm of observed entries as a proxy for heaviness.9 Specifically, after dropping rows and columnswith the most observed large entries, we argue that all remaining rows and columns are well-balanced except on few sparse errors, which we exclude from our potential.

Next, even if appropriate rows and columns were dropped, we cannot use the empirical observations to construct our step, as semi-random observations are no longer unbiased and the fully-random indices \(^{}\) are unknown. Instead, we use \(^{}\) existentially to set up a convex program over reweightings \(w^{}\), to search for a candidate step \(:=_{(i,j)}w_{ij}[-^{}]_{ij}\). Roughly, our convex program aims to find \(w\) satisfying the following constraints (see Lemma 7 for a formal statement).

1. For all \(i[d]\), \(_{j[d]|(i,j)}w_{ij}[-^{}]_{ij}^{2}=O( )\).
2. There exists \(,^{d d}\) with \(=+\), \(\|\|_{} 1\), and \(\|\|_{}=O(r^{-})\).
3. For a sufficiently small constant \(c\), \(,-^{}=_{(i,j)}w _{ij}[-^{}]_{ij}^{2} 1-c\).

Item 1 serves to ensure our step \(\) is sufficiently spread out amongst its rows and columns, which serves two roles: it bounds how much progress is lost when excluding indices, and enforces crucial problem regularity for our reweighting optimization algorithm. On the other hand, Items 2 and 3 are the standard criteria for making iterative progress through the short-flat framework of . It is straightforward to see that the ideal candidate \([-^{}]_{^{}(S S)}\) we mentioned earlier is feasible for these constraints, and we show in Lemma 7 that finding any satisfying reweighting is enough to ensure progress (with some mild technical changes). It remains to discuss how to find a candidate \(\) to ensure rapid convergence through the progress bound (1).

Flow-based adaptive reweighting.A key technical contribution of this work is a nearly-linear time algorithm finding a reweighting \((w):=_{(i,j)}w_{ij}[-^{}]_{ij}\) that meets our progress criteria. Due to our focus on fast algorithms, we require this step to run in \(O(||(r))\) time. Our strategy is to form a joint objective over \(w\) obeying Item 1, and \(\|\|_{} 1\), of the form

\[F_{}(w,):=-(w),- ^{}+C((w)-).\] (2)

Here, the parameter \(C\) is used to trade off the progress objective \((w),-^{}\) (giving Item 3) and the flatness objective \(((w)-)\) (giving Item 2), where \(()=()\) is a smooth approximation to the operator norm. That is, letting \(:=\) be the product space between \(w^{}\) satisfying Item 1 and short matrices \(^{d d}\), we show that finding \((w,)\) with low suboptimality error in \(F_{}\) is enough to make progress through Lemma 7.

We optimize (2) by carefully applying a Frank-Wolfe method due to . While naive Frank-Wolfe analysis requires global smoothness of the objective, this provably does not hold for our \(F_{}\) (at least, with a smoothness bound of \((r)\) required for nearly-linear time algorithms). However, we leverage that the analysis in  shows that weaker regularity conditions, namely bounds on the curvature constant, suffice for efficient convergence of an iterative method. Specifically, we use the structure of our constraint set \(\) to show that \(F_{}\) is smooth _in a restricted set of directions within \(\)_ which implies suitable bounds on the curvature constant (see, e.g., Lemma 18) to remove a \(d\) factor from the naive smoothness bound. We also observe that \(\) is exactly a rescaling of a bipartite matching polytope, so that we can use flow-based graph algorithms, e.g., the approximate weighted matching algorithm of , to efficiently implement the linear optimization oracles over \(\) required by Frank-Wolfe methods. We then use techniques from numerical linear algebra and polynomial approximation to implement linear optimization oracles over \(\), completing our optimization subroutine for finding adaptive reweightings.

One other technical hurdle arises in implementing this framework for finding a reweighting: the existence of \(w^{}\) satisfying Items 1, 2, and 3 relies on having a good estimate of the current distance \(\|-^{}\|_{}\), after excluding certain rows and columns. In general, we can only upper bound this quantity, but we show that we can also use our optimization subroutine for (2) to certify non-existence of such a \(w^{}\). In this latter case, we show that it must have been the case that a few rows and columns were significantly heavier than the rest, and we can run Algorithm 1 to drop them. Our overall iterative method, Algorithm 2, makes bicriteria progress and either terminates with a "drop" step (when no feasible \(w^{}\) exists), or a "descent" step (when we find a good solution to (2)).

Recovering dropped rows and columns.Finally, we briefly discuss how to post-process an estimate \(\) that is close to \(^{}\) on a large submatrix to recover dropped rows and columns. The high-level outline is similar to , where we aim to find a "verified" set of rows and columns that we completed well, and use the low-rank assumption to obtain accurate estimates to the remainder via regression. However, there are several key differences. The main difference is that in the semi-random model, we need to track errors in our estimate entrywise instead of in \(_{2}\), since a semi-randomness could drastically distort the empirical \(_{2}\) error by adversarially revealing entries. We rely on a structural result using tools from the theory on volumetric spanners [5; 7] (see Lemma 27) showing that large entrywise errors must be localized to a small submatrix. At this point, we set up \(_{}\) regression problems to identify verified indices, solvable in nearly-linear time using recent advances in linear programming . Importantly, the overhead in the error guarantee of this recovery step is only \(((r))\) (see Proposition 3), allowing us to achieve a recovery guarantee in Theorem 1 that avoids \((d)\) overheads in the presence of noise.

## 3 Preliminaries

We now formally set up preliminaries required for analyzing our solution to the semi-random matrix completion problem. First, we recall the standard definition of incoherence for subspaces.

**Definition 1** (Incherence).: _We say a dimension-\(r\) subspace \(V^{d}\) is \(\)-incoherent if \(_{V}e_{i}_{2}}\) for all \(i[d]\) (where \(e_{i}\) is the \(i\)th standard basis vector), and \(^{m n}\) is \(\)-incoherent if \(\) has \(\)-incoherent row and column spans._

We will work with symmetric matrices. We denote the space of \(d d\) symmetric matrices by \(^{d d}\). In Appendix A, we give a reduction from general matrices to symmetric matrices showing that it suffices to work in the symmetric case, up to constant factor loss in parameters. Next, we define the semi-random observation model.

**Definition 2** (Semi-random model).: _For \(^{d d}\), we use \(_{p}^{}()\) to denote oracle access to \(\) in the \(p\)-semi-random model, where we call \(_{p}^{}\) a \(p\)-semi-random oracle. Specifically, in one call to \(_{p}^{}\) with input \(\), for unknown observation probabilities \(\{p_{}\}_{[d][d]}[p,1]\), each \([d][d]\) is independently included in a set \(\) with probability \(p_{}\). The oracle \(_{p}^{}\) then returns the set \(\{[]_{}\}_{}\). When an algorithm requires the ability to query \(_{p}^{}()\) for various values of \(p\) (specified in the algorithm description), we list the input as \(_{}^{}()\)._

Note that \(_{p}^{}()\) and \(_{}^{}()\) inherit their definitions in  when all \(p_{}=p\), for a fixed \(p\). Now we can formally define the semi-random matrix completion problem we study.

**Definition 3** (Semi-random matrix completion).: _In the semi-random matrix completion problem, parameterized by \(,_{ 0}\), \(p\), \(d\), and \(r^{}[d]\), there is an unknown rank-\(r^{}\)\(\)-incoherent matrix \(^{}^{d d}\). For some \(^{d d}\) satisfying \(_{}\), and \(:=^{}+\), we receive the output of one call to \(_{p}^{}()\), and wish to output an approximation to \(^{}\)._

**Remark 1**.: _In Appendix A, we give a sample-splitting reduction which lets us use \(_{p}^{}()\) to simulate \(\) independent accesses to \(_{q}^{}()\) for smaller values \(q p\)._

We will also need a few technical definitions to explain the main ingredients in our algorithm.

Entropy and softmax.We let \(^{d d}:=\{^{d d}_{ }()=1\}\) denote the \(d d\) spectraplex. We let \(H:^{d d}\) denote von Neumann (matrix) entropy, i.e., \(H():=,\), and let \(H^{}:^{d d}\) denote its convex conjugate, which we call matrix softmax, defined by \(H^{}():=\). More generally, for \(>0\), we define

\[H^{}_{}():=( ).\] (3)

We recall the following standard facts about \(H^{}_{}\).

**Fact 1** ().: _The function \(H^{}_{}\) defined in (3) satisfies the following for all \(>0\)._

1. _For all_ \(^{d d}\)_,_ \(_{1}() H^{}_{}()_{1}()+ d\)_, where_ \(_{1}()\) _is the maximum eigenvalue of_ \(\)_._
2. _For all_ \(^{d d}\)_,_ \( H^{}_{}()=)}{ {Tr}()}\)_._
3. \(H^{}_{}\) _is twice-differentiable and_ \(\)_-smooth with respect to the norm_ \(_{}\)_._To use \(H_{}^{*}\) to enforce operator norm bounds, we define the signed lift of a matrix \(^{d d}\) by

\[():=&_{d d}\\ _{d d}&-^{2d 2d}.\]

Note that slift signs and lifts \(\) so that its maximum eigenvalue is its operator norm.

Comparing matrices.We use the following comparison definition from .

**Definition 4** (Closeness on a submatrix).: _We say \(,^{}^{m n}\) are \(\)-close on a \(\)-submatrix if there exist subsets \(A[m]\), \(B[n]\) satisfying \(|A| m-(m,n)\), \(|B| n-(m,n)\), and_

\[\|[-^{}]_{A B}\|_{ }.\]

_If \(=0\), we say \(,^{}\) are \(\)-close. When \(,^{}\) are both symmetric, we require that \(A=B\)._

We note that in Definition 4, \(A\), \(B\) are unknown; our analysis only uses this definition existentially.

## 4 Outline of proof of Theorem 1

In this section, we overview the main components of the proof of Theorem 1, which are developed and proved in full in the appendix sections. Our final matrix completion algorithm, Algorithm 8, alternates between two main subroutines, Algorithm 3 (analyzed in the following Corollary 1) and Algorithm 7 (analyzed in the following Proposition 3). We prove Theorem 2, a formal version of Theorem 1, by combining Corollary 1 and Proposition 3 in Appendix E. We now explain and motivate each of these pieces in the context of our technical overview in Section 2.

The first main component of our final algorithm, Algorithm 8, takes as input parameters \(\) and \(_{}\), as well as an input matrix \(^{d d}\) satisfying \(\|-^{}\|_{}\) (i.e., \(\) and the target matrix \(^{}\) are \(\)-close), where \(\) is sufficiently larger than the noise level \(\|\|_{}\). Its goal is to use semi-random observations from \(}=^{}+\) to return another matrix \(^{}^{d d}\) such that \(}\) and \(\) are \(\)-close on a \(_{}\)-submatrix (Definition 4). In other words, we are willing to give up on a \(_{}\) fraction of rows and columns to make an \(\) factor progress on the remaining submatrix. To achieve this result, we first provide a helper claim in Proposition 1, an analysis of Algorithm 2 (DropOfDescent).

**Proposition 1**.: _Let \(^{d d}\) be given as a rank-\(r\) factorization, let \(^{}^{d d}\) be rank-\(r^{}\), and let \(U[d]\). Suppose we know for \(_{},_{}\), \(>0\), that \(|[d] U|_{}d\), and \(_{U U},_{U U}^{}\) are \(\)-close on a \(_{}\)-sub submatrix. Let \(,(0,)\), \(, 1\), and suppose for \(:=r+r^{}\) and an appropriate polynomial,_

\[p[}{d}()}{_{}}),1],\;_{} .\] (4)

_Given one call to \(_{p}^{}(^{}+)\) for \(\|\|_{}}}{ 20d}\), the following holds with probability \( 1-\)._

1. _If Algorithm_ 2 _returns on Line_ 12_, then_ \(_{U^{} U^{}},_{U^{} U^{}} ^{}\) _are_ \((1-)\)_-close on a_ \(_{}+}}\)_-submatrix, and_ \(|U U^{}| 40(_{}+(}{})^{1/2})d d\)_._
2. _If Algorithm_ 2 _returns on Line_ 16_, then_ \(_{U U}^{}\)_,_ \(_{U U}^{}\) _are_ \(10^{}\)_-close on a_ \(2_{}\)_-submatrix, and_ \(^{}\) _is given as a rank-_\((3r+2r^{})\) _factorization._

_The runtime of the algorithm is_

\[O(m(()}{_{ }})),\]

_where \(m d\) is the number of observed entries upon calling \(_{p}^{}(^{}+)\)._

Proposition 1 is used to analyze one step of Algorithm 2, after we have already taken a few iterations and explicitly removed \(_{}d\) rows and columns \([d] U\) from consideration, so that our remaining submatrices on \(U U\) are close on a \(_{}\)-submatrix. Our goal will ultimately to have \(_{}\), \(_{}}}{2}\) throughout the algorithm. Algorithm 2 provides bicriteria guarantees: in the "drop" case of Item 1, it slightly increases the \(_{}\) parameter, increases \(_{}\) by an even smaller amount (where the tradeoff is given by a degree of freedom \(\)), and makes a small amount of progress (decreasing the closeness by \(1-O()\)). In the "descent" case of Item 2, we instead decrease the closeness by a large \(O(())\) factor, while doubling \(_{}\) and roughly tripling the rank. By choosing the parameters appropriately, we show that iterating upon Proposition 1 yields Corollary 1, proven in Appendix B.

**Corollary 1**.: _Let \(,^{}^{d d}\) be rank-\(r^{}\), and suppose \(\|-^{}\|_{}\). Let \( 250\) and \(,_{}(0,)\). Algorithm 3 uses one call to \(_{p}^{}(^{}+)\), where for appropriate polynomials,_

\[\|\|_{}(}{_{}}())^{1+o(1)}},\ p=(}{ _{}}())^{o(1)}}{d},\]

_and computes a rank-\(r^{}^{o(1)}\)-matrix \(^{}\), such that with probability \( 1-\), \(^{}\) and \(^{}\) are \(\)-close on a \(_{}\)-submatrix. The runtime of the algorithm is \(m^{o(1)}(}{_{}}( ))\), where \(m\) is the number of observed entries upon calling \(_{p}^{}(^{}+)\)._

We now explain the key computational method develop to implement Proposition 1. We will solve a subproblem, defined in (5), to attempt to find a reweighting which makes progress. The subproblem enforces the conditions on candidate weights \(w\) mentioned in Section 2, and is formally stated below:

\[F_{}(x,) :=- 1_{},x+CH_{}^{*} ((()-) ),\] (5) \[(w) :=_{}w_{}_{},\ v_{ }=[]_{}^{2},.\]

The difference between (5) and our informal sketch in (2) is that we reparameterize \(x w v\) where \(v\) is the squared observations of the difference matrix \(\), and \(\) is the set of observations. We also introduce a tradeoff parameter \(\) for controlling additive error bounds via Fact 1. Thus, the component \( 1_{},x= v,w\) captures the progress condition (Item 3), and the component \(H_{}^{*}((()-))\) captures the flatness condition (Item 2). We optimize (5) over \(\|\|_{} 1+\), the set of short matrices, and \(x\) belonging to \(\) defined in (22) which we observe is a rescaling of a bipartite matching polytope by a parameter \(k\) (Fact 3). This polytope is used to enforce the spreadness condition (Item 1).

In the case we find a good solution to the subproblem, we can use the resulting solution to take a descent step and give the guarantee in Item 2 of Proposition 3. Otherwise, we have certified that excluding a few heavy rows and columns removed much of the progress that could be made (which we prove is the only way a good solution does not exist), and hence obtain the guarantee in Item 1. Our main guarantee for solving (5) at a fast rate is the following, proven in Appendix C.

**Proposition 2**.: _Let \([0,C]\) and \((0,1)\), and suppose \(|[]_{}|[,u]\) for all \(\). There is an algorithm computing \((x,)\), an \(\)-approximate minimizer to (5) with probability \( 1-\), in time_

\[O((||+d)T^{2}( }{}()))\]

_for \(R_{}:=^{-1}(2k+d})\) and \(T=^{2}}{}\), and \(\) is given as a rank-\(r=O(}{^{2}} T)\) factorization._

The last component in the proof of Theorem 1 is Algorithm 7, a "rounding" step where we take the output of Corollary 1, say \(\), which is guaranteed to be close to \(^{}\) on a large submatrix, and then postprocess it to a matrix \(}\) that is guaranteed to be close to \(^{}\) everywhere with some \((r,, d)\)-factor loss in the closeness. As long as the progress \(\) made by Corollary 1 is larger than this factor, we can simply alternate Corollary 1 and the rounding step to still make constant-factor progress in each iteration. We prove the following result in Appendix D.

**Proposition 3**.: _Let \(^{}^{d d}\) be rank-\(r^{}\) and \(\)-incoherent, and let \(}=^{}+\) for \(^{d d}\) with \(\|\|_{}\) for some \( 0\). Let \(^{d d}\) be given as a rank-\(r\) decomposition, with \(r r^{}\). Further, for \((0,1)\) suppose \(\) and \(^{}\) are \(d\)-close on a \(\)-submatrix. Finally, assume_

\[ r(d)}.\]

_Then for any \((0,1)\) if \(p( r())\) for an appropriate polynomial, Algorithm 7 uses one call to \(_{p}(})\) and with probability \( 1-\), outputs \(}^{d d}\) such that_

\[\|}-^{}\|_{}( r()).\] (6)_Also, \(}\) is given as a rank-\(( r())\) factorization. The algorithm runs in \(m( r())\) time, where \(m\) is the number of observed entries upon calling \(_{p}^{}(})\)._