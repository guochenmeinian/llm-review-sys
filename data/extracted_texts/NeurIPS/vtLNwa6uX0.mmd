# The Geometry of Neural Nets' Parameter Spaces

Under Reparametrization

 Agustinus Kristiadi

Vector Institute, University of Tubingen

akristiadi,fdangel@vectorinstitute.ai &Felix Dangel

University of Tubingen

Tubingen AI Center

philipp.hennig@uni-tuebingen.de

&Philipp Hennig

University of Tubingen

Tubingen AI Center

philipp.hennig@uni-tuebingen.de

###### Abstract

Model reparametrization, which follows the change-of-variable rule of calculus, is a popular way to improve the training of neural nets. But it can also be problematic since it can induce inconsistencies in, e.g., Hessian-based flatness measures, optimization trajectories, and modes of probability densities. This complicates downstream analyses: e.g. one cannot definitively relate flatness with generalization since arbitrary reparametrization changes their relationship. In this work, we study the invariance of neural nets under reparametrization from the perspective of Riemannian geometry. From this point of view, invariance is an inherent property of any neural net _if_ one explicitly represents the metric and uses the correct associated transformation rules. This is important since although the metric is always present, it is often implicitly assumed as identity, and thus dropped from the notation, then lost under reparametrization. We discuss implications for measuring the flatness of minima, optimization, and for probability-density maximization. Finally, we explore some interesting directions where invariance is useful.

## 1 Introduction

Neural networks (NNs) are parametrized functions. Since it is often desirable to assign meaning or interpretability to the parameters (weights) of a network, it is interesting to ask whether certain transformations of the parameters leave the network _invariant_--equivalent in some sense. Various notions of invariance have been studied in NNs, in particular under weight-space symmetry  and reparametrization . The former studies the behavior of a function \(()\) under some invertible \(T:\) where \(\) is a group; for any \(\), the function \(\) is _invariant under the symmetry \(T\)_ if and only if \(()=(T(,g))\) for all \(g\). For example, normalized NNs are symmetric under scaling of the weights, i.e. \(()=(c)\) for all \(c>0\)--similar scale-symmetry also presents in ReLU NNs . Meanwhile, _invariance under reparametrization_ studies the behavior of the NN when it is transformed under the change-of-variable rule of calculus: Given a transformed parameter \(=()\) under a bijective differentiable \(:\) that maps the original parameter space onto another parameter space, the function \(()\) becomes \(}():=(^{-1}())\). Note their difference (see Fig. 1): In the former, one works on a single parameter space \(\) and a single function \(\)--the map \(T\) acts as a symmetry of elements of \(\) under the group \(\). In contrast, the latter assumes two parameter spaces \(\) and \(\) which are connected by \(\) and hence two functions \(\) and \(}\).

Figure 1: The difference between the symmetry (**top**) and reparametrization problems (**bottom**). \(\) and \(\) are two different parameter spaces.

We focus on the latter. While in this scenario, we have \(}()=()\) whenever \(=()\), it is well-known that downstream quantities such as optimization trajectories [55; 71], the Hessian [20, Sec. 5], and probability densities over \(\)[59, Sec. 5.2.1.4] are not invariant under model reparametrization. These non-invariances are detrimental since an arbitrary reparametrization can affect the studied quantities and thus consistency cannot be guaranteed--parametrization muddles the analysis.

For instance, because of this, (i) one cannot relate Hessian-based sharpness measures with generalization and the correlation analyses between them [27; 33; 73] become meaningless, (ii) a good preconditioner cannot be guaranteed to be good anymore for optimizing the reparametrized model--this is one reason why invariant methods like natural gradient are desirable [37; 50; 58; 62], and (iii) under a reparametrization, a prior density might put low probability in a region that corresponds to the high-probability region in the original space, making posterior inference pathological .

To analyze this issue, in this work we adopt the framework of Riemannian geometry, which is a generalization of calculus studying the intrinsic properties of manifolds. "Intrinsic" in this context means that objects such as functions, vectors, and tensors defined on the manifold must be independent of how the manifold is represented via a coordinate system, and conserved under a change of coordinates . The parameter space of a neural network, which is by default assumed to be \(=^{d}\) with the Euclidean metric and the Cartesian coordinates, is a Riemannian manifold--model reparametrization is such a change in the coordinate system.

Why, then, can reparametrizations bring up the aforementioned inconsistencies? In this work, we discuss this discrepancy. We observe that this issue often arises because the Riemannian metric is left implicit, and dropped when computing downstream quantities such as gradients, Hessians, and volumes on the parameter space. This directly suggests the solution: Invariance under reparametrization is guaranteed if the Riemannian metric is not just implicitly assumed, but made explicit, and it is made sure that the associated transformation rules of objects such as vectors, covectors, and tensors are performed throughout. We show how these insights apply to common cases (Fig. 2).

LimitationOur work focuses on the _reparametrization consistency_ of prior and future methods that leverage geometric objects such as gradients and the Hessian. Thus, we leave the geometric analysis for invariance under symmetry as future work. In any case, our work is complimentary to other works that analyze invariance under symmetry [17; 22; 43; 67]. Moreover, this work's focus is not to propose "better" methods e.g. for better preconditioners or better generalization metrics. Rather, we provide a guardrail for existing and future methods to avoid pathologies due to reparametrization.

## 2 Preliminaries

This section provides background and notation on relevant concepts of neural networks and Riemannian geometry. For the latter, we frame the discussion in terms of linear algebra as close as possible to the notation of the ML community: We use regular faces to denote abstract objects and bold faces for

Figure 2: Invariance is retained (**bottom**) if the Riemannian metric is explicitly tracked and one transforms geometric objects such as vectors, covectors, and the metric itself properly under a reparametrization \(:\). **(a)** Gradient descent’s trajectories are therefore invariant under a change of parametrization. **(b)** Invariance of the modes of a probability density function is an inherent property under a natural base measure induced by the metric. **(c)** When the Hessian \(\) at a critical point is seen as a linear map \(\) with the help of the metric, its determinant is invariant.

their concrete representations in a particular parametrization. E.g., a linear map \(A\) is represented by a matrix \(\), a point \(z\) is represented by a tuple of numbers \(\), an inner product \(G(z)\) at \(z\) is represented by a matrix \(()\). The same goes for the differential \(\), \(\); and the gradient \(\), \(\).

### Some notation for neural networks

The following concepts are standard, introduced mostly to clarify notation. Let \(f:^{n}^{d}^{k}\) with \((,) f(;)\) be a model with input, output, and parameter spaces \(^{n}\), \(^{k}\), \(^{d}\). Let \(:=\{(_{i},_{i})\}_{i=1}^{m}\) be a dataset and write \(:=\{_{i}\}_{i=1}^{m}\) and \(:=\{_{i}\}_{i=1}^{m}\). We moreover write \(f(;):=(\{f(_{i};)\}_{i=1 }^{m})^{mk}\). The standard way of training \(f\) is by finding a point \(^{*}^{d}\) s.t. \(^{*}=*{arg\,min}_{^{d}}_{ i=1}^{m}(_{i},f(_{i};))=:*{arg\,min} _{^{d}}()\), for some loss function \(\). If we add a weight decay \(/2\|\|_{2}^{2}\) term to \(()\), the minimization problem has a probabilistic interpretation as _maximum a posteriori_ (MAP) estimation of the posterior density \(p()\) under the likelihood function \(p( f(;))(-_{i=1}^{m}(_{i}, f(_{i};)))\) and an isotropic Gaussian prior \(p()=(,^{-1})\). We denote the MAP loss by \(_{}\). In our present context, this interpretation is relevant because this probabilistic interpretation implies a probability density, and it is widely known that a probability density transforms nontrivially under reparametrization.

A textbook way of obtaining \(^{*}\) is gradient descent (GD): At each time step \(t\), obtain the next estimate \(^{(t+1)}=^{(t)}-|_{^{(t)}}\), for some \(>0\). This can be considered as the discretization of the gradient flow ODE \(}=-|_{}\). Among the many variants of GD is preconditioned GD which considers a positive-definite matrix field \(\) on \(^{d}\), yielding \(}=-()^{-1}\,|_{}\).

### The geometry of the parameter space

**Remark 1**.: We restrict our discussion to global coordinate charts since they are the default assumption in neural networks, and in a bid to make our discussion clear for people outside of the differential geometry community. We refer the curious reader to Appendix D for a discussion on local coordinate charts. Note that, our results hold in the general local-chart formulation. \(\)

The parameter space \(^{d}\) is the canonical example of a smooth manifold. We can impose upon this manifold a (global) coordinate chart: a homeomorphism that represents a point with an element of \(^{d}\). The standard choice is the Cartesian coordinate system \(:^{d}^{d}\), which we have used in the previous section. That is, the image of \(\) uniquely represents elements of \(^{d}\)--given a point \(z^{d}\), we write \(=(z)\) for its coordinate representation under \(\).

The choice of a coordinate system is not unique. Any homeomorphism \(:^{d}^{d}\) can be used as an alternative coordinate chart. For instance, the polar coordinates can be used to represent points in \(^{d}\) instead. Crucially, the images of any pair of coordinates must be connected through a diffeomorphism (a smooth function with a smooth inverse) \(:\). Such a map is called a _change of coordinates_ or _reparametrization_ since it acts on the parameter space.

**Example 2** (Reparametrization).: Reparametrization is ubiquitous in machine learning:

1. _Mean-Field Variational Inference._ Let \(q(;)\) be a variational approximation, which is often chosen to be \((,(^{2}))\), i.e. \(=\{^{d},^{2}^{d}_{>0}\}\). Common choices of reparametrization of \(^{2}\) include the log-space  or softplus  parametrization.
2. _Weight normalization._ Given a NN \(f_{}\), WeightNorm  applies the reparametrization \(=r\) where \(r_{>0}\) and \(:=/\|\|^{d-1}\). This is akin to the polar coordinates. (Note that we assume \(^{d}\{\}\) since otherwise \(\) is not a global diffeomorphism.) \(\)

At each point \(z^{d}\), there exists a vector space \(T_{z}^{d}\) called the _tangent space at \(z\)_, consisting of the so-called _tangent vectors_. An important example of a tangent vector is the gradient vector \(|_{z}\)

Figure 3: The implicit geometric assumption on the parameter space \(^{d}\). \(,\) are two different (global) coordinates on \(^{d}\).

of \(:^{d}\) at \(z\). The dual space \(T_{z}^{*}^{d}\) of \(T_{z}^{d}\) is referred to as the _cotangent space_ and consists of linear functionals of \(T_{z}^{d}\), called _tangent covectors_. An example of a tangent covector is the differential \(|_{z}\) of \(\) at \(z\). Under a coordinate system, one can think of both tangent vectors and covectors as vectors in the sense of linear algebra, i.e., tuples of numbers.

One can take an inner product of tangent vectors by equipping the manifold with a _Riemannian metric_\(G\), which, at each point in \(^{d}\) is represented by a positive-definite \(d d\) matrix \(\) whose coefficients depend on the choice of coordinates. With the help of the metric, there is an isomorphism between \(T_{z}^{d}\) and \(T_{z}^{*}^{d}\). In coordinates, it is given by the map \(T_{z}^{d} T_{z}^{*}^{d}: \) and its inverse \(T_{z}^{*}^{d} T_{z}^{d}: ^{-1}\). One important instance of this isomorphism is the fact that \(|_{z}\) is represented by \(^{-1}\) in coordinates. The natural gradient is a direct consequence of this fact. In practice, it is standard to assume the Cartesian coordinates on \(^{d}\), implying \(\). We have thus \(\), which only seems trivial at first sight, but reveals the relationship between the tangent vector on the l.h.s. and the cotangent vector on the r.h.s.

**Remark 3**.: Another common way to define a NN's parameter space is to define a manifold of probabilistic models \(M:=\{p( f(;)):^{d}\}\) and assume that \(p( f(;)) \) is the coordinate chart [19, 55, etc.]. The problem with this construction is that there is no bijection \(^{d} M\) in general. Indeed, the Jacobian of the map \(f(;\,\,):^{d} M\) is in practice surjective everywhere due to overparametrization . Therefore, one cannot define a proper metric on the parameter space \(^{d}\) that corresponds to a metric on the distribution space \(M\)[46, Prop. 13.9]. For instance, using this construction, the Fisher metric is singular for overparametrized NNs  and thus not a valid Riemannian metric. The pseudoinverse has been used to handle this but is mostly limited to theoretical analyses : As far as we know, in practice, damping--which breaks the interpretation of the Fisher on \(^{d}\) as the pullback metric from \(M\)--is _de facto_ for handling this due to its numerical stability .

By detaching from the distribution space, the definition used in this work does not have this issue. It enables more general constructions of metrics in the parameter space since _any_ positive-definite matrix is admissible. E.g. one is free to add damping or use any approximation to the Fisher--our results still apply to this case. Thus, our construction is closer to practice. 

#### 2.2.1 Transformation rules

Differential geometry is the study of coordinate-independent objects: Geometric objects must be _invariant_ under change-of-coordinates in the sense that any pair of representations must refer to the same (abstract) object. Suppose \(:\) is a reparametrization, with \(=()\). Coordinate independence is encoded in the following transformation rules, which involve the Jacobian \(()=(_{i}/ _{j})\) of \(\), and its inverse \(^{-1}()=(())^ {-1}=(_{i}/_{j})\)--the Jacobian of \(^{-1}\). (Color codes are used for clarity when different objects are present in a single expression later on.)

1. A function \(:\) in \(\)-coordinates transforms into \(=h^{-1}\) in \(\)-coordinates.
2. A tangent vector \(\) in \(\)-coordinates transforms into \(()\). In particular, a gradient vector \(|_{}\) transforms into \(()|_{ }\).
3. A tangent covector \(\) in \(\)-coordinates transforms into \(J^{-1}()^{}\). In particular, given a transformed function \(^{-1}:\), we have \((^{-1})|_{}=J^{-1}()^{} |_{^{-1}()}\), which we recognize as the standard chain rule.
4. A metric \(()\) becomes \(()^{-}() ()^{-1}=^{-1}()^{}()^{-1}()\). In general, this rule applies to any tensor that takes tangent vectors as its arguments, e.g. a bilinear map.

The following examples show how these rules ensure the invariance of geometric objects.

**Example 4**.: Let \(h\), \(\), \(\), \(\) respectively be a function, vector, covector, and metric in \(\)-coordinates at a point \(=(z)\), and let \(:\) be a reparametrization.

1. Let \(:=()\) and \(:=h^{-1}\) be \(\) and \(h\) expressed in \(\)-coordinates, respectively. Then, \(()=h(^{-1}())=h()\). That is, the actions of \(h\) and \(\) agree in both coordinates and thus they represent the same abstract function on \(^{d}\).
2. The action of \(\) on \(\) in \(\)-coordinates is given by the product \(^{}\). Let \(}:=()\) and \(}:=()^{-} \) be the transformed vector and covectors. Then, \[^{}}=(})^{}^{}(())=^{}.\] (1)That is, both \(\) and \(}\) are the representations of the same linear functional; \(\) and \(}\) are the representations of the same tangent vector.
3. Let \(}:=^{-}^{-1}\) be the transformed metric in the \(\)-coordinates. Then, \[}^{}}}=(})^{} }}}}}}=^{},\] and thus the transformation rules ensure that inner products are also invariant. 

Finally, we say that an ODE's dynamics is invariant if the trajectory in parametrization corresponds to the trajectory in another parametrization. Concretely, a trajectory \((_{t})_{t}\) in \(\) is invariant if under the reparametrization \(:\), the transformed trajectory \((_{t})_{t}\) is related to \((_{t})_{t}\) by \(_{t}=^{-1}(_{t})\) for each \(t\). See Fig. 2a for an illustration.

## 3 Neural Networks and Reparametrization

We discuss three aspects of the parameter space under reparametrization, as illustrated in Fig. 2. First, we address the non-invariance of Hessian-based flatness measures (e.g., , and show how taking the metric into account provides invariance. Second, we show that the invariance property often cited in favor of natural gradient is not unique, but an inherent property of any gradient descent algorithm when considering its ODE. Finally, we show that modes of probability density functions on the parameter space are invariant when the Lebesgue measure is generalized using the metric.

### Invariance of the Hessian

A growing body of literature connects the flatness of minima found by optimizers to generalization performance [8; 13; 23; 27; 36; 38; 48; 53]. However, as Dinh et al.  observed, this association does not have a solid foundation since standard sharpness measures derived from the Hessian of the loss function are not invariant under reparametrization.

From the Riemannian-geometric perspective, the Hessian of a function \(\) (or \(_{}\) or any other twice-differentiable function) on \(\)-coordinates is represented by a \(d d\) matrix with coefficients \(_{ij}():=}{, _{i}}()-_{k=1}^{d}_{ij}^{k}()}{_{k}}()\), for any \(\) and \(i,j=1,,d\), where \(_{ij}^{k}\) is a particular three-dimensional array. While it might seem daunting, when \(\) is a local minimum of \(\), the second term is zero since the partial derivatives of \(\) are all zero at \(\). Thus, \(()\) equals the standard Hessian matrix \((^{2}/_{i}_{j})\) at \(\).

Considering the Hessian as a bilinear function that takes two vectors and produces a number, it follows the covariant transformation rule, just like the metric (see Appendix B.1 for a derivation): Let \(:\) and \(=()\). The Hessian matrix \(()\) at a local minimum in \(\)-coordinates transforms into \(}()=^{-1}()^{}(^{-1}())^{-1}()\) in \(\)-coordinates--this is correctly computed by automatic differentiation, i.e. by chain and product rules. But, while this gives invariance of \(\) as a bilinear map (Example 4c), the determinant of \(\)--a popular flatness measure--is not invariant because

\[(})():=^{-1}()^{} (^{-1}())^{-1}()=(^{-1}())^{2}((^{-1}())),\]

and so in general, we do not have the relation \((})=()^{-1}\) that would make this function invariant under reparametrization (Example 4a).

The key to obtaining invariance is to employ the metric \(\) to transform the bilinear Hessian into a linear map/operator on the tangent space. This is done by simply multiplying \(\) with the inverse of the metric \(\), i.e. \(:=^{-1}H\). The determinant of \(\) is thus an invariant quantity.1 To show this, under \(\), the linear map \(\) transforms into

\[}()&=(^{-1}( )^{}(^{-1}())^{-1}())^{-1} ^{-1}()}^{-1}()^{-1}() \\ &=(^{-1}())^{-1}(^{-1}())(^{-1}())^{-1}(),\] (2)

due to the transformation of both \(\) and \(\). Hence, \(\) transforms into

\[(})()=^{-1}())^{} ^{-1}())}^{}^{-1}( ))}((^{-1}())(^{-1}() ))}=()(^{-1}())\]in \(\)-coordinates. Thus, we have the desired invariant transformation \((})=()^{-1}\). Note that \(\) is an arbitrary metric--this invariance thus also holds for the Euclidean case where \(\). Note further that the trace and eigenvalues of \(\) are also invariant; see Appendices B.2 and B.3. Finally, see Appendix A for a simple working example.

To obtain invariance in the Hessian-determinant, -trace, and -eigenvalues at a minimum of \(\), we explicitly write it as \(=^{-1}\), even when \(\), and transform it according to Section 2.2.1.

### Invariance of gradient descent

Viewed from the geometric perspective, both gradient descent (GD) and natural gradient descent (NGD) come from the same ODE framework \(}=-G()^{-1}|_{}\). But NGD is widely presented as invariant under reparametrization, while GD is not [51, 71, etc.]. Is the choice of the metric \(\) the cause? Here we will show from the framework laid out in Section 2.2 that _any_ metric is invariant _if_ its transformation rule is faithfully followed. And thus, the invariance of NGD is not unique. Rather, the Fisher metric used in NGD is part of the family of metrics that transform correctly under autodiff, and thus it is "automatically" invariant under standard deep learning libraries like PyTorch, TensorFlow, and JAX --see Appendix C.

The underlying assumption of GD is that one works in the Cartesian coordinates and that \(\). For this reason, one can ignore the metric \(\) in \(}=-G()^{-1}|_{}\), and simply write \(}=-|_{}\). This is correct but this simplification is exactly what makes GD not invariant under a reparametrization \(:\) where \(=()\). To see this, notice that while GD transforms \(\) correctly via the chain rule \(}=-J^{-1}()^{}|_{^{-1}()}\), by ignoring the metric \(\), one would miss the important fact that it must also be transformed into \(}()=^{-1}()^{}^{-1}()\). It is clear that \(}()\) does not equal \(\) in general. Thus, we cannot ignore this term in the transformed dynamics since it would imply that one uses a different metric--the dynamics are thus different in the \(\)- and \(\)-coordinates. When one explicitly considers the above transformation, one obtains

\[} =-}()^{-1}J^{-1}()^{}|_{^{-1}()}=-(^{-1}}()^{}^{-1 }())^{-1}}()^{}|_{ ^{-1}()}\] \[=-(^{-1}())|_{ ^{-1}()}.\]

This dynamics in \(\)-coordinates preserves the assumption that the metric is \(\) in \(\)-coordinates and thus invariant. In contrast, the dynamics \(}\) under just the chain rule implicitly changes the metric in \(\)-coordinates, i.e. from \(\) into \(}()^{}()\), and thus the trajectories are not invariant.

This discussion can be extended to _any_ metric \(\): Simply use the transformed metric \(}()=^{-1}()^{}(^{-1}())^{-1}()\), and we obtain the invariant dynamics of any preconditioned GD with preconditioner \(\) given by \(=-(^{-1}())(^{-1}())^{-1} |_{^{-1}()}\).

To obtain invariance in optimizers with any metric/preconditioner, explicitly write down the metric even if it is trivial, and perform the proper transformation under reparametrization.

**Remark 5**.: For the discretized dynamics, the larger the step size, the less exact the invariance. For instance, _discrete_ natural gradient update rule is only invariant up to the first-order . This is orthogonal to our work since it is about improving ODE solvers . __

The consequence is that we need a "geometric-aware" autodiff library such that the invariant dynamics above is always satisfied. In this case, _any_ preconditioner \(\) yields invariance under _any_ reparametrization, even nonlinear ones. This is in contrast to the current literature, e.g. under standard autodiff, Newton's method is only affine-invariant, and structured approximate NGD methods such as K-FAC are only invariant under a smaller class of reparametrizations .

### Invariance in probability densities

Let \(q_{}()\) be a probability density function (pdf) under the Lebesgue measure \(d\) on \(\). Under a reparametrization \(:\) with \(=()\), it transforms into \(q_{}()=q_{}(^{-1}())|^{-1}()|\). This transformation rule ensures \(q_{}\) to be a valid pdf under the Lebesgue measure \(d\) on \(\), i.e. \(_{}q_{}()\,d=1\). Notice, in general \(q_{} q_{}^{-1}\) due to the change-of-random-variable rule. Hence, pdfs transform differently than standard functions (Example 4a)and can thus have non-invariant modes (cf. 59, Sec. 5.2.1.4): Density maximization, such as the optimization of \(_{}\), is not invariant even if an invariant optimizer is employed.

Just like the discussion in the previous section, it is frequently suggested that to obtain invariance here, one must again employ the help of the Fisher matrix \(\). When applied to a prior, this gives rise to the famous _Jeffreys  prior_\(p_{J}()|()|^{}\) with normalization constant \(_{}|()|^{}\,d\). Is the Fisher actually necessary to obtain invariance in pdf maximization?

Here, we show that the same principle of "being aware of the implicit metric and following proper transformation rules" can be applied. A pdf \(q_{}()\) can be written as \(q_{}()=()\,d}{d}\) to explicitly show the dependency of the base measure \(d\)--this is the Lebesgue measure on \(\), which is the natural unit-volume2 measure in Euclidean space. Given a metric \(\), the natural volume-measurement device on \(\) is the _Riemannian volume form_\(dV_{G}\) which has \(\)-coordinate representation \(dV_{}:=|()|^{}\,d\). This object takes the role of the Lebesgue measure on a Riemannian manifold: Intuitively, it behaves like the Lebesgue measure but takes the (local) distortion due to the metric \(\) into account. Indeed, when \(\) we recover \(d\).

Here, we instead argue that invariance is an inherent property of the modes of probability densities, as long as we remain aware of the metric and transform it properly under reparametrization. Explicitly acknowledging the presence of the metric, we obtain

\[()\,d}{|()|^{}\,d }=q_{}()\,|()|^{-}=:q_{ }^{}().\] (3)

This is a valid probability density under \(dV_{}\) on \(\) since \(_{}q_{}^{}()\,dV_{}=1\). This formulation generalizes the standard Lebesgue density. And, it becomes clear that the Jeffreys prior is simply the uniform distribution under \(dV_{}\); its density is \(q_{}^{} 1\) under \(dV_{}\).

We can now address the invariance question. Under \(\), considering the transformation rules for both \(q_{}()\) and \(\), the density (3) thus becomes

\[ q_{}^{}()&=q_{ }(^{-1}())\,| J^{-1}()||(^{-1}()^{ }(^{-1}())^{-1}())|^{-}\\ &=q_{}(^{-1}())\,|(^{-1}( ))|^{-}=q_{}^{}(^{-1}()).\] (4)

This means, \(q_{}^{}\) transforms into \(q_{}^{}^{-1}\) and is thus invariant since it transforms as standard function--notice the lack of the Jacobian-determinant term here, compared to the standard change-of-density rule. In particular, just like standard unconstrained functions, the modes of \(q_{}^{}\) are invariant under reparametrization. Since \(\) is arbitrary, this also holds for \(\), and thus the modes of Lebesgue-densities are invariant, as long as \(\) is transformed correctly (Fig. 2b).

Note that, even if the transformation rule is now different from the one in standard probability theory, \(q_{}^{}\) is a valid density under the transformed volume form. This is because due to the transformation of the metric \(}\), we have \(dV_{}}=|(^{-1}())|^{}|^{-1}()\,d\). Thus, together with (4), we have \(_{}q_{}^{}()\,dV_{}}=1\). This also shows that the \(|^{-1}()|\) term in the standard change-of-density formula (i.e. when \(\)) is actually part of the transformation of \(d\).

Put another way, the standard change-of-density formula ignores the metric. The resulting density thus must integrate w.r.t. \(d\)--essentially assuming a change of geometry, not just a simple reparametrization--and thus \(q_{}\) can have different modes than the original \(q_{}\). While this non-invariance and change of geometry are useful, e.g. for normalizing flows , they cause issues when invariance is desirable, such as in Bayesian inference.

To obtain invariance in density maximization, transform the density function under the Riemannian volume form as an unconstrained function. In particular when \(\) in \(\), this gives the invariant transformation of a Lebesgue density \(q_{}\), i.e. \(q_{}=q_{}^{-1}\).

## 4 Related Work

While reparametrization has been extensively used specifically due to its "non-invariance", e.g. in normalizing flows [63; 68], optimization [19; 69], and Bayesian inference [64; 72], our work is not at odds with them. Instead, it gives further insights into the inner working of those methods: They are formulated by _not_ following the geometric rules laid out in Section 2.2, and thus in this case, reparametrization implies a change of metric and hence a change of geometry of the parameter space. They can thus be seen as methods for metric learning [35; 77], i.e. finding the "best" \(\) for the problem at hand, and are compatible with our work since we do not assume a particular metric.

Hessian-based sharpness measures have been extensively used to measure the generalization of neural nets [13; 27; 53]. However, as Dinh et al.  pointed out, they are not invariant under reparametrization. Previous work has proposed the Fisher metric to obtain invariant flatness measures [30; 38; 49]. In this work, we have argued that while the Fisher metric is a good choice due to its automatic-invariance property among other statistical benefits , any metric is invariant if one follows the proper transformation rules faithfully. That is, the Fisher metric is not necessary if invariance is the only criterion. Similar reasoning about the Fisher metric has been argued in optimization [3; 37; 51; 55; 65; 71] and MAP estimation [21; 32]: the Fisher metric is often used due to its invariance. However, we have discussed that it is not even the unique automatically invariant metric (Appendix C), so the invariance of the Fisher should not be the decisive factor when selecting a metric. By removing invariance as a factor, our work gives practitioners more freedom in selecting a more suitable metric for the problem at hand, beyond the usual Fisher metric.

Finally, the present work is not limited to just the parameter space. For instance, it is desirable for the latent spaces of variational autoencoders  to be invariant under reparametrization [2; 25]. The insights of our work can be implemented directly to latent spaces since they are also manifolds .

## 5 Some Applications

We present applications in infinite-width Bayesian NNs, model selection, and optimization, to show some directions where the results presented above, and invariance theory in general, can be useful. They are not exhaustive, but we hope they can be an inspiration and foundation for future research.

### Infinite-width neural networks

Bayesian NNs tend to Gaussian processes (GPs) as their widths go to infinity [57; 60]. It is widely believed that different parametrizations of an NN yield different limiting kernels . For instance, the _NTK parametrization (NTP)_ yields the NTK , and the _standard parametrization (SP)_ yields the NNGP kernel .

Due to their nomenclatures and the choice of priors, it is tempting to treat the NTP and SP as reparametrization of each other. That is, at each layer, the SP parameter \(\) transforms into

\(=()=/\) where \(h\) is the previous layer's width. This induces the same prior on both \(\) and \(\), i.e. \((,^{2}/h)\), and thus one might guess that invariance should be immediate. However, if they are indeed a reparametrization of the other, the NN \(f_{}\) must transform as \(f_{}:=f_{}^{-1}()\) and any downstream quantities, including the NTK, must also be invariant. This is a contradiction since the functional form of the NTP shown in Jacot et al.  does not equal \(f_{}\), and the NTK diverges in the SP but not in the NTP . The SP and NTP are thus not just reparametrizations.

Instead, we argue that the SP and NTP are two completely different choices of the NN's architectures, hyperparameters (e.g. learning rate), and priors--see Fig. 4 for intuition and Appendix E.1 for the details. Seen this way, it is thus not surprising that different "parametrization" yields different limiting behavior. Note that this argument applies to other "parametrizations" [e.g. 15; 70; 75; 76].

Altogether, our work complements previous work and opens up the avenue for constructing non-trivial infinite-width NNs in a "Bayesian" way, in the sense that we argue to achieve a desired limiting behavior by varying the model (i.e. architecture, functional form) and the prior (including over hyperparameters) instead of varying the parametrization. This way, one may leverage Bayesian analysis, e.g. model selection via the marginal likelihood [28; 54], for studying infinite-width NNs.

### The Laplace marginal likelihood

The Laplace log-marginal likelihood (LML) of a model \(\) with parameter in \(^{d}\)--useful for Bayesian model comparison (11, Sec. 4.4.1)--is defined by 

\[ Z(;_{}):=-(_{})+ p(_{})-(2)+ (_{}).\] (5)

Let \(:_{}_{}\) be a reparametrization of \(^{d}\). It is of interest to answer whether \( Z\) is invariant under \(\), because if it is not, then \(\) introduces an additional confounder in the model selection and therefore might yield spurious/inconsistent results. For instance, there might exists a reparametrization s.t. \( Z(_{1})(_{2})\) even though originally \( Z(_{1})< Z(_{2})\). The question of "which model is better" thus cannot be answered definitively.

As we have established in Section 3.3, the first and second terms, i.e. \(_{}\), are invariant under the Riemannian volume measure. At a glance, the remaining terms do not seem to be invariant due to the transformation of the bilinear-Hessian as discussed in Sec. 3.1. Here, we argue that these terms are invariant when one considers the derivation of the Laplace LML, not just the individual terms in (5).

In the Laplace approximation, the last two terms of \( Z\) are the normalization constant of the unnormalized density \(h():=(-^{}(_{ })d)\), where \(:=(-_{})\). Notice that \(\) is a tangent vector at \(_{}\) and \((_{})\) is a bilinear form acting on \(\). Using the transformation rules in Example 4, it is straightforward to show that \(h h^{-1}\)--see Appendix E.3. Since \(Z\) is fully determined by \(h\), this suggests that \( Z(;_{})\) also transforms into \((;^{-1}(_{}))\), where \(_{}=(_{})\). This is nothing but the transformation of a standard, unconstrained function on \(^{d}\), thus \( Z\) is invariant.

We show this numerically in Table 1. We train a network in the Cartesian parametrization and obtain its \( Z\). Then we reparametrize the net with WeightNorm and naively compute \( Z\) again. These \( Z\)'s are different because the WeightNorm introduces more parameters than the Cartesian one, even though the degrees of freedom are the same. Moreover, the Hessian-determinant is not invariant under autodiff. However, when transformed as argued above, \( Z\) is trivially invariant.

### Biases of preconditioned optimizers

The loss landscape depends on the metric assumed in the parameter space through the Hessian operator \(=^{-1}H\). The assumption on \(\) in practice depends on the choice of the optimizer. For instance, under the default assumption of the Cartesian coordinates, using GD implies the Euclidean metric \(\), and ADAM uses the gradient-2nd-moment metric .

We argue that explicitly including the metric is not just theoretically the correct thing to do (since it induces invariance, and by Riemannian geometry), but also practically beneficial. Fig. 5 compares measurements of sharpness of minima (Hessian-trace). Using the metric-aware Hessian operator \(\), one can show definitively (i.e. independent of the choice of parametrization) that ADAM tends to obtain much sharper minima than SGD. The benefits of using the Hessian operator have also been confirmed in previous work. Cohen et al.  argue that when analyzing the optimization dynamic of an adaptive/preconditioned GD algorithm, one should take the preconditioner into account when measuring Hessian-based sharpness measures. From this, they demonstrate a sharpness-evolution behavior known in vanilla GD, allowing a comparison between vanilla and adaptive GD.

 
**Param.** & \( Z\) & \(-\) & Rest \\  Cartesian & -212.7\(\)3.4 & -143.4\(\)0.0 & -69.3\(\)3.4 \\ WeightNorm & -227.1\(\)3.6 & -143.4\(\)0.0 & -83.7\(\)3.6 \\  

Table 1: The Laplace LML \( Z\) and its decomposition on a NN under different parametrizations. \(\) and “Rest” stand for the first and the remaining terms in (5).

Figure 5: The effect of viewing the Hessian as a linear map \(\) to measure sharpness at minima. ADAM finds much sharper minima when the geometry of the parameter space is taken into account.

Conclusion

In this work, we addressed the invariance and invariance associated with the reparametrization of neural nets. We started with the observation that the parameter space is a Riemannian manifold, albeit often a trivial one. This raises the question of why one should observe non-invariance in neural nets, whereas, by definition, Riemannian manifolds are invariant under a change of coordinates. As we showed, this discrepancy only arises if the transformations used in the construction of a neural net along with an optimizer ignore the implicitly assumed metric. By acknowledging the metric and using the transformation rules associated with geometric objects, invariance and invariance under reparametrization are then guaranteed. Our results provide a geometric solution towards full invariance of neural nets--it is compatible with and complementary to other works that focus on invariance under group-action symmetries, both in the weight and input spaces of neural networks.