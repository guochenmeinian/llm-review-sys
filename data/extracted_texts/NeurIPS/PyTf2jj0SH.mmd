# Caption used in this setting

ConvBench: A Multi-Turn Conversation Evaluation Benchmark with Hierarchical Ablation Capability for Large Vision-Language Models

Shuo Liu\({}^{1}\)

Kaining Ying\({}^{1}\)

Hao Zhang\({}^{1}\)

Yue Yang\({}^{1}\)

Yuqi Lin\({}^{1}\)

Tianle Zhang\({}^{1}\)

Chuanhao Li\({}^{1}\)

Yu Qiao\({}^{1}\)

Ping Luo\({}^{2,1}\)

Wenqi Shao\({}^{11\%}\)

Kaipeng Zhang\({}^{11\%}\)

###### Abstract

Multi-turn visual conversation is an important ability of real-world AI assistants. However, the related evaluation benchmark is missed. This paper presents ConvBench, a multi-turn conversation benchmark with hierarchical capabilities ablation evaluation for Large Vision-Language Models (LVLMs). ConvBench comprises 577 curated multi-turn conversations, encompassing 215 tasks. These tasks are broad and open-ended, which resemble real-world user behaviors. ConvBench progressively examines the LVLMs' perception, reasoning, and creativity capabilities in each conversation and can decouple these capabilities in evaluations and thus perform reliable error attribution. Besides, considering the diversity of open-ended questions, we introduce an efficient and reliable automatic evaluation framework. Experimental results reveal that ConvBench is a significant challenge for current LVLMs, even for GPT4V, which achieves only a 39.51% score. Besides, we have some insightful findings, such as the weak perception of LVLMs inhibits authentic strengths in reasoning and creation. We believe our design of hierarchical capabilities, decoupling capabilities evaluation, and multi-turn conversation can blaze a new trail in LVLMs evaluation. Code and benchmark are released at https://github.com/shirlyliu64/ConvBench.

## 1 Introduction

Open-ended multi-turn visual conversations are usual in our daily lives and should be one of the features of artificial general intelligence (AGI). Recent large vision language models (LVLMs, e.g., GPT4V , Claude , and InterVL-Chat ) have achieved impressive performance in such conversations, which is also the common usage of LVLMs. However, existing benchmarks [4; 5; 6; 7; 8; 9; 10; 11; 12; 13] for LVLMs conduct single-turn conversations, and most use close-ended multi-choice questions for evaluation. To this end, this paper presents a multi-turn conversation benchmark named ConvBench to measure the advancement of LVLMs. Beyond open-ended and multi-turn, our questions are hierarchical and real-world, and we introduce an automatic evaluation method. In particular, in each conversation, ConvBench can decouple different capabilities evaluations and perform reliable error attribution, blazing a new trail in LVLMs evaluation.

We first introduce the hierarchical structure of multi-turn questions. Previous benchmarks treat different multimodal capabilities independently using independent questions while ignoring the fact that multimodal capabilities are highly dependent on each other. It makes it hard to conduct error attribution. For example, when the model responds incorrectly to a reasoning question, it is unclear whether it is attributed to the perception or reasoning error of LVLMs (see Figure 1 (c)). Besides, humans reason based on their perceptions and generate new ideas through perceptual and reasoning skills. Therefore, ConvBench progressively examines the perception, reasoning, and creativity capabilities in a multi-turn conversation, seeing Figure 1 (b) for examples. In this way, we can conduct more accurate error attribution by giving reference answers to previous questions.

We then introduce the source of the multi-turn questions. We collect images and questions from the perspective of real-world user behavior. Specifically, VisIT-Bench  is a single-turn visual question answering (VQA) dataset collected from users' "wish list". ConvBench uses its images and extends its questions in our multi-turn hierarchical structure. The new questions align with real-world user behavior and involve many new tasks. We illustrate the dataset construction in Figure 3 and details in Section 3. Overall, ConvBench comprises \(577\) irreticulously curated multi-turn VQA samples, spanning \(71\), \(65\), and \(79\) distinct types of perception, reasoning, and creation tasks, respectively.

Unlike most existing benchmarks employing multi-choice questions, we allow LVLMs to output anything to resemble real-world user behavior. However, its evaluation is much more challenging since the correct answer is not only. Therefore, we introduce a reliable automated evaluation named ConvBenchEval for open-ended visual questions. Specifically, we annotate each question's reference answers and each image's question-aware caption. Additionally, we annotate assessment focus points for creation questions for more accurate evaluation. Based on the above annotations (see Figure 3 for examples) and LVLMs' responses, we then employ ChatGPT  to judge if the LVLM's response is better than the reference answer. We illustrate the evaluation pipeline in Figure 4. We also conduct a human evaluation on a subset of ConvBench while ChatGPT achieves 81.83% judgment agreement with human evaluation results, demonstrating the reliability of ChatGPT.

Figure 1: The comparison between previous evaluation benchmarks (a) and our ConvBench (b). Previous benchmarks assess capabilities independently in a single-turn conversation, while our ConvBench evaluates multi-turn conversation by hierarchically assessing perception, reasoning, and creativity. (c) shows that the error should be attributed to perception rather than reasoning, while ConvBench employs annotated reference answers that can do such accurate error attribution.

We assess 20 publicly available LVLMs. The evaluation results reveal several innovative findings: i) Our ConvBench provides a significant challenge for evaluating the follow-up capability of LVLMs' multi-turn conversation, notably GPT4V  achieves only \(39.51\)% overall score. ii) The novel hierarchical ablation evaluations of ConvBench conclude that the weakness of "OCR", "Fine-grained", and "Spatial" perception of current LVLMs may inhibit the performance of the next reasoning and creation tasks. The weakness of LVLMs' reasoning capability demanding "Professional Knowledge", "Emotional Intelligence", "Imagination", and "Sense of Space" may hinder the performance of the next creation. iii) The performances across different tasks of different LVLMs show a similar distribution, which suggests the development of current LVLMs are synchronous. iv) Performance improves as the language model size of LVLM increases. v) A declined performance between the first turn and subsequent turns shows that LVLMs tend to generate comprehension biases as the multi-turn conversation progresses or forget the information of previous turns. vi) The high-quality dialogue history provides important guidance to the LVLMs' responses and plays an important role in in-context learning examples.

The contributions of our work are summarized as follows. (1) We present the first open-ended multi-turn visual conversations benchmark ConvBench. It is challenging, real-world, and aligns with user behavior. (2) In each conversation, ConvBench progressively examines a three-level hierarchy of multimodal capabilities, including perception, reasoning, and creativity. It can decouple these capabilities in evaluations and conduct reliable error attribution. (3) We present an automatic evaluation to handle the challenging open-ended multi-turn conversations and achieve high agreement with the human evaluation. (4) Our experimental results reveal that ConvBench is challenging for current LVLMs, even for GPT4V. Besides, we have some insightful findings from experiments.

## 2 Related Work

### Large Vision-Language Models.

Building upon the achievements of Large Language Models (LLMs) [14; 15; 16], Large Vision-Language Models (LVLMs) [1; 17; 18; 19; 20; 21; 22; 23; 24; 25] have recently showcased remarkable proficiency across various tasks, demonstrating advanced perception, reasoning, and creative capabilities. A favored approach to enhancing LVLMs involves integrating visual knowledge into the semantic framework of LLMs, thereby leveraging the LLMs' strong performance in interpreting and responding to prompts. For instance, BLIP-2  introduces the Q-Former to synchronize vision foundation models with LLMs without modifying the underlying models. MiniGPT4  utilizes a straightforward fully connected layer, requiring only a minimal set of caption data. LLaVA  enhances the LLM with high-quality instructional data generated by GPT4. QWen-VL  undergoes fine-tuning with high-resolution images, employing multi-task training strategies. mPLUG-DocOwl  expands the capabilities of LVLMs to include document understanding.

### Large Vision-Language Models Benchmarks.

With the advancement of Vision-Language Models (LVLMs), existing standard evaluation benchmarks like MSCOCO , GQA , VQA [28; 29], etc., are no longer sufficient to assess the comprehensive multimodal abilities of LVLMs. In response, a variety of benchmarks have been developed specifically for LVLM evaluation, including OwlEval , LAMM , LVLM-eHub , SEED , MMBench , and MM-Vet . These benchmarks primarily focus on assessing basic perceptual abilities. In addition, VisIT-Bench  covers a broad spectrum of tasks, ranging from simple recognition to complex reasoning. Recent research has also introduced LVLM benchmarks requiring expert-level domain knowledge and intricate reasoning, such as MathVista  and MMMU  and MMT-Bench . However, these benchmarks tend to address perception, reasoning, and creation tasks in isolation without establishing connections among these tasks. Furthermore, the current benchmarks predominantly focus on single-turn interactions. The ConvBench addresses these gaps by not only offering a hierarchical ablation evaluation that moves from perception through reasoning to creation but also by evaluating LVLMs' capabilities in multi-turn conversational contexts.

### Benchmarks for Multi-turn Large Models.

For LLMs, there have been some classic work. MT-Bench  is the first multi-turn conversation benchmark, which focuses on two-turn follow-up dialogues across eight topics ("Writing", "Knowledge", "Math" and so on). It only focuses on the most basic and important multi-turn abilities of Context Memory and Anaphora Resolution. MT-Bench-101  is the first dataset to specifically focus on 13 fine-grained multi-turn dialogue abilities, such as, "Separate Input", "Topic Shift", "Content Confusion", "Content Rephrasing" and so on. Otherwise, there are other benchmarks: MT Bench++  is an eight-turn multi-turn conversation benchmark to qualitatively evaluate multi-turn instruction following ability. It is built on expanding MT-Bench by manually annotating six additional follow-up questions. MINT  is a benchmark that evaluates LLMs' ability to solve challenging tasks with multi-turn interactions by using tools and leveraging natural language feedback. For LVLMs, to the best of our knowledge, ConvBench is the first multi-turn visual conversation benchmark, which can also be considered as a multi-modal version of MT-Bench, but with the innovative addition of hierarchical ablation testing. ConvBench depends on manual annotation for obtaining the three-turn interactive instructions to allow evaluating multi-turn instruction-following ability for visual dialogue. ConvBench also mostly focuses on the most basic and important multi-turn visual abilities of Context Memory and Anaphora Resolution.

## 3 ConvBench

### Overview of ConvBench

The ConvBench includes 577 image-instruction pairs tailored for multi-turn dialogues. Each pair is structured with three sequential instructions, each targeting a distinct cognitive skill--beginning with perception, followed by reasoning, and culminating in creation. This structure underscores the cognitive evolution from basic perceptual comprehension to logical reasoning and finally to sophisticated creative expression. The detailed definition and division for perception, reasoning, and creation can be seen in the appendix. As shown in Figure 2, our benchmark, encompassing 215 tasks, is divided into 71 tasks focused on perception, 65 on reasoning, and 79 on creation. These practical tasks are creative, useful, and real-world demands, which are downstream users of language technologies are likely to need. Existed benchmarks for LVLMs almost focus on computer-vision deep learning tasks, which cannot assess the ability of LVLMs to solve the diverse and never-before-seen real-world tasks. VisIT-Bench  has first proposed a path to evaluate LVLMs for practical tasks. We inherit and extend it to evaluate multi-turn visual conversation capabilities and explore the hierarchical ablation evaluations for LVLMs.

### Data Curation Process

**Data Collection.** Our benchmark collection is structured into five distinct stages, as depicted in Figure 3. Annotators play a very important role in the process. These stages are as follows:

**i) Multi-turn Instruction Formation.** We extend each single-turn sample in VisIT-Bench to a sample with three-turn instructions. We ensure the first, second, and third turns are represented as perception, reasoning, and creation, respectively. The instructions are designed based on the last ones.

**ii) Task Category Induction.** We derive the task categories by inducing them from instructions. The bottom-up approach to task collection guarantees that the tasks under investigation are tailored to meet real-world requirements. ConvBench consists of 215 tasks, and we have included the details in the supplementary materials. Task category induction is done with human annotation to ensure reasonableness and accuracy. For example, if the question is "How to write the recipe for the food shown in the image?", the task is inducted as "Recipe Writing".

**iii) Instruction-Conditioned Caption Annotation.** VisIT-Bench  has first proposed the generation of the instruction-conditioned caption. The two important purposes for generating instruction-conditioned captions have been evaluated in the VisIT-Bench. One is to provide a comprehensive description of the image for building a robust automated assessment framework. The other is generating raw reference answers for humans to verify in the next "Reference Generation" step. Therefore, in this work, the image and the instructions are also provided for the annotators to generatea caption. We first prompt GPT4V with "Describe this image in detail." We then polish the responses by humans according to the instructions to obtain the final instruction-conditioned caption.

**iv) High-Quality Reference Generation.** Similar to VisIT-Bench , they verify each response with human annotators. For each sample, we feed GPT4V with the instruction-conditioned caption, the image, multi-turn instructions, and our well-designed prompt in a multi-turn conversation fashion to generate each instruction's response. We meticulously refine these responses as reference answers, removing their biases and enhancing their quality and relevance.

**v) Focus Point Annotation.** The creativity instruction is an open-ended question without a standard answer. Therefore, we annotate specific focus points related to each creation instruction. These annotations are used as criteria to assess whether the model produces instructive answers to the instruction, seeing Step 5 in Figure 3.

## 4 ConvBenchEval and Hierarchical Ablation Evaluation

Human evaluation is really meaningful for judging human preference responses. However, it is costly to obtain human judgments for new model submissions. Similar to VisIT-Bench  and MT-Bench , to support faster model development, we introduce \(()\), an automated evaluation pipeline designed for multi-turn visual conversation assessment and hierarchical ablation evaluation. It aligns best with human preferences, whose agreement with human evaluation reaches \(81.83\%\). More agreement experiment results can be found in the appendix, which can validate the effectiveness of our evaluation methodology.

\(()\) comprises four key components: perception, reasoning, creation, and overall conversation evaluation modules, as shown in Figure 4. We recursively use \(()\) in three settings as follows to conduct the hierarchical ablation evaluation for error attribution. For clarity, we denote the instruction, model response, reference, and focus points as \(I_{i},M_{i},R_{i}\), and \(F_{i}\) at each turn, respectively, where \(i\) indicates the turn index. Note that \(F_{1}\) and \(F_{2}\) are null focus points.

**i) ConvBenchEval for Evaluating the Performances of Each Turn and Overall Conversation.** The model response of each turn is obtained in the principle of multi-turn conversation. The response of each turn is generated based on the front instructions and responses in this setting. The formula for generating each turn's response can be expressed as \(M_{i}=f(\{I_{i}\}_{i=0}^{i-1},\{M_{i}\}_{i=0}^{i-1},I_{i}),i=1,2,3\), where \(f\) denotes the model inference function, \(I_{i}\) denotes the instruction at the turn \(i\), \(M_{i}\) denotes

Figure 2: Visualization of example tasks in ConvBench. It consists of 215 tasks constructed in perception, reasoning, and creation hierarchy.

the response at the turn \(i\). In the first turn, there is no front instruction (\(I_{0}\)) or response (\(M_{0}\)). The evaluation process can be expressed as \(S_{i},J_{i}=(\{M_{i}\}_{i=1}^{3},\{R_{i}\}_{i=1}^{3},F_{i};P_ {i}),i=1,2,3\) where \(S_{i}\) is the capability score to the turn \(i\), \(J_{i}\) is the judgment from the ChatGPT to the turn \(i\), and \(P_{i}\) is the prompt specific to the turn \(i\). Finally, we feed all instructions, responses, focus points, and judgments into ChatGPT to obtain the overall conversation score as given by \(S_{O},J_{O}=(\{M_{i}\}_{i=1}^{3},\{R_{i}\}_{i=1}^{3},\{J_ {i}\}_{i=1}^{3};P_{O})\) where \(S_{O}\) is the capability score for overall multi-turn conversation, \(J_{O}\) is the judgment from the ChatGPT to the overall multi-turn conversation, \(P_{O}\) is the prompt for evaluating the overall conversation capability.

**ii) Hierarchical Ablation Evaluation with Perfect Perception Condition.** In this setting, the influence of inaccurate perception on reasoning, creation, and overall conversation can be derived. We directly use the human-annotated perception answers as the responses of the perception turn, replacing the outputs from LVLMs at the perception turns, which can be represented as \(_{1}=R_{1}\). The model inference at the reasoning and creation turns can be written as \(_{i}=f(\{I_{i}\}_{i=1}^{i-1},\{_{i}\}_{i=1}^{i-1},I_{i}),i=2,3\), where \(f\) denotes the model inference function, \(I_{i}\) denotes the instruction at the turn \(i\), \(M_{i}\) denotes the response at the turn \(i\) with the perfect perception conditions. The evaluation process without considering perception error can be expressed as \(_{i},_{i}=(\{_{i}\}_{i=1}^{3},\{R_{i} \}_{i=1}^{3},F_{i};P_{i}),i=2,3\). Finally, the overall conversation score without considering perception score can be given by \(_{O},_{O}=(\{_{i}\}_{i=1}^{3},\{R_{ i}\}_{i=1}^{3},\{_{i}\}_{i=2}^{3};P_{O})\), where \(_{O}\) is the capability score for overall multi-turn conversation with perfect perception conditions, \(_{O}\) is the judgment from the ChatGPT for the overall multi-turn conversation with perfect perception conditions, \(P_{O}\) is the prompt for evaluating the overall conversation capability. By comparing \(S_{i}\) and \(_{i}\) (\(i=2,3,O\)), we can see how perception error affects the performance of reasoning, creativity, and overall conversation.

**iii) Hierarchical Ablation Evaluation with Perfect Perception and Reasoning Conditions.** In this setting, we further explore how reasoning errors affect creativity and overall conversation. The human-annotated perception answer and reasoning answer are directly used as the responses of the perception turn and reasoning turn, respectively, replacing the original responses from LVLMs, which can be represented as \(_{i}=R_{i},i=1,2\). Then, the model inference at the creation turn can be written as \(_{i}=f(\{I_{i}\}_{i=0}^{i-1},\{_{i}\}_{i=1}^{i-1},I_{i}),i=3\), where \(f\) denotes the model inference function, \(I_{i}\) denotes the instruction at the turn \(i\), \(_{i}\) denotes the response at the turn \(i\) with perfect perception and reasoning conditions. The evaluation process without considering perception and reasoning error can be expressed as \(_{i},_{i}=(\{_{i}\}_{i=1}^{3},\{ R_{i}\}_{i=1}^{3},F_{3};P_{i}),i=3\). Finally, the overall conversation score without considering perception and reasoning score can be given

Figure 3: The pipeline of data curation. We develop three multi-turn instructions for each image to assess the perception, reasoning, and creation capabilities. We also annotate the referenced answers to facilitate automatic evaluation and error analysis.

by \(_{O},_{O}=(\{_{i}\}_{i=1}^{3},\{R_{ i}\}_{i=1}^{3},_{3};P_{O})\), where \(_{O}\) is the capability score for overall multi-turn conversation with perfect perception and reasoning conditions, \(_{O}\) is the judgment from the ChatGPT to the overall multi-turn conversation with perfect perception and reasoning conditions, \(P_{O}\) is the prompt for evaluating the overall conversation capability. By comparing \(_{i}\) and \(_{i}\) (\(i=3,O\)), we can check how perception and reasoning errors affect the performance of creativity and overall conversation.

**Pairwise Grading.** Note that the function in i) - iii) \(()\) is a pairwise scheme as described in the following. Similar to VisIT-Bench , \(()\) also employs a pairwise grading scheme and an absolute metric. Specifically, all the LVLM responses and the references shown in Figure 4, are anonymously presented to the ChatGPT in a random order. The ChatGPT is tasked with a pairwise comparison to decide the responses from which is superior. The percentages of cases where the ChatGPT prefers the output from LVLM rather than the human-verified reference output are defined as the absolute metric, _i.e._ win rate. This metric can directly judge whether the LVLMs' performance reaches human preference. The system prompts designed for the pairwise grading scheme for obtaining the win rate, detailed in the Appendix, encourage the ChatGPT to engage in a step-by-step thought process, making its reasoning explicit. In our forced-choice setup, ties are not permitted; thus, if the ChatGPT deems the responses of equal quality, it is instructed to select one arbitrarily.

## 5 Experiment and Analysis

### Hierarchical Ablation Evaluation Comparisons and Analysis

We employed our proposed \(()\) methodology to perform a quantitative analysis. The outcomes of the evaluation results are detailed in Table 1. \(S_{1}\), \(S_{2}\), and \(S_{3}\) denote the scores for perception, reasoning, and creation, respectively. Meanwhile, \(_{2}\) and \(_{3}\) correspond to the scores for reasoning and creation, respectively, and under conditions of perfect perception. \(_{3}\) is the score for creation, assuming perfect conditions for both perception and reasoning. We present our principal insights from the evaluation results as follows:

(1) **ConvBench Provides Challenge for LVLMs.** This benchmark sets formidable challenges for modern models. GPT4V, despite being a sophisticated model, shows only modest achievements in perception, reasoning, and creation. Moreover, there is still a gap in performance between open-source models and closed-source models in various real-world use cases. ConvBench exposes a stark discrepancy between the performance of these models and that of humans.

(2) **Weak Perception Undermines LVLMs' Reasoning and Creation Performance.** Under conditions of perfect perception, we see significant improvements in reasoning and creation abilities, as indicated by the data in the \(_{2}\) and \(_{3}\) columns of Table 1. The figure reflects the enhancement in reasoning and creation attributed to impeccable perception. Across \(20\) LVLMs, the average increase

Figure 4: An illustration of the evaluation pipeline of ConvBench.  indicates the ChatGPT.

in reasoning and creation scores are \(11.38\) and \(5.65\), respectively. By analyzing these enhanced reasoning or creation cases with perfect perception information, we found that the **"OCR" Perception Task** may influence the performances of "In-context Visual Scene Understanding", "Meme Reasoning", and "Chart Reasoning". The **Fine-Grained Perception Tasks** like "Location Recognition" perception task may influence the performances of "Location Understanding" and "Travel Plan Writing" tasks, "Celebrity Recognition" perception task may influence the performances of "Celebrity Understanding", "PowerPoint Production" and "Cultural Knowledge Reasoning" tasks. The **Spatial Perception Tasks** like "Board Chess Position Description" may influence the performances of "Board Game Reasoning" and "Diagram Generation" tasks. Our benchmark clarifies the origins of these errors in reasoning and creativity, determining whether they stem from visual perception issues or language reasoning shortcomings. With the aid of human-verified visual comprehension, the authentic strengths of the language module in reasoning and creation will be more precisely evaluated.

(3) **Limited Reasoning Impacts LVLM's Creation Abilities.** Under ideal conditions for perception and reasoning, shifts in creation capabilities are documented in the \(_{3}\) column of Table 1. The numbers in brackets indicate adjustments in creation scores due to human-verified reasoning accuracy. Among the \(20\) LVLMs evaluated, there is an average increase of \(5.32\) in creation scores, which indicates that reasoning inaccuracies can adversely affect LVLMs' performance in creative tasks. By analyzing these enhanced creation cases with perfect reasoning information, we found that some reasoning tasks involving **Professional Knowledge** influence the next creation tasks. For example, "Physical Knowledge Reasoning" may influence the accuracy of "Physical Problem Computing". Some reasoning tasks needing **Emotional Intelligence** influence the relative creation tasks. For example, "Human Emotion Reasoning" may influence the performance of "Blog Writing", "Humanity Discussion" and "How Visual Content Arouses Emotions" tasks. Some reasoning tasks containing **Imagination** like "Tangram Speculation" may influence the corresponding creative task ("Tangram Segmentation"). Some reasoning tasks requiring **Sense of Space** like "Location Relative Position" may influence the "Navigation" task.

(4) **LVLM's Performance across Various Real-world Tasks.** As depicted in Figure 2 in the appendix, LVLMs demonstrate weak performance in fine-grained tasks, such as "Movie recognition", "Position Description", "Outfit recognition", "Make-up Description" and so on. Lack of real-world application data for pretraining and finetuning may lead to the weakness. The datasets used for pretraining and finetuning are more high-quality, the better performance will be. The performances of open source LVLMs, which are above \(14.00\), are all using various high-quality datasets for training. The superiority of model architecture may be constructed on high-quality datasets. Also as shown in

  Model & \(R_{1}\) & \(R_{2}\) & \(S_{1}\) & \(S_{3}\) & \(S_{4}\) & \(S_{5}\) & \(S_{6}\) & \(S_{7}(S_{5}-S_{5})\) & \(S_{8}(S_{9}-S_{5})\) & \(S_{9}(S_{9}-S_{5})\) & \(S_{9}(S_{10}-S_{10})\) \\  
**GPTAV** & **39.51** & **38.47** & 38.47 & **39.34** & **37.61** & **40.55** & **47.31**(+6.97) & 37.78(+6.61) & 37.61(+2.94) & 38.99(+1.21) & 38.30(+0.69) \\  Climbo  & 36.60 & 37.49 & **38.99** & 39.17 & 34.32 & 35.70 & 45.93(+6.76) & **38.99(+6.67)** & **43.15**(+7.45) & **38.16**(+0.17) & **40.21**(+2.94) \\  Rea Fun3L  & 25.60 & 24.65 & 23.15 & 27.56 & 21.32 & 25.62 & 23.95(+3.57) & 22.84(+5.56) & 25.82(+7.07) & 27.84(+19.56) & 26.00(+0.13) \\  IntervL-Climbo-V1-2  & 21.77 & 22.41 & 24.96 & 21.31 & 20.97 & 19.93 & 22.20(+7.05) & 23.26(+7.28) & 29.94(+7.1) & 33.65(+7.53) & 31.65(+5.54) \\  IntervVL-Climbo-V1-5  & 17.65 & 20.22 & 26.09 & 17.33 & 17.33 & 15.08 & 27.75(+10.40) & 23.40(+6.07) & 25.15(+10.05) & 23.24(+2.84) & 33.81(+6.67Figure 2 in the appendix, the performance of open-source models with better performance on different tasks shows a similar distribution as that of closed-source models. It means that the challenges of real-world cases faced by open-source and closed-source models are similar, which suggests the development of current LVLMs is synchronous. ConvBench aids in highlighting the strengths and weaknesses of the instruction-following LVLMs along various real-world use cases.

### Multi-Turn Conversation Comparisons and Analysis

The results of the multi-turn conversation evaluation are meticulously outlined in Table 1. \(S_{O}\) represents the scores for multi-turn conversation performance. Concurrently, \(_{O}\) signifies the scores for multi-turn conversation performance under the assumption of flawless perception. Moreover, \(_{O}\) reflects the score for multi-turn conversation performance, premised on ideal conditions for both perception and reasoning. We delineate our key findings from the experimental results as follows:

(1) **The Challenge of Follow-up Multi-Turn Conversation for LVLMs.** ConvBench is manually tailored by designing the questions based on the last responses, always referencing specific contents mentioned in the last responses. Like the real-world application of a general-purpose AI assistant, a user always asks for additional information based on the assistant's prior responses. ConvBench provides a benchmark for evaluating the LVLM's follow-up ability to engage in coherent conversations. Table 1 shows that closed-source LVLMs, including GPT4V, Claude, and Reka, generally outperform open-source ones in the follow-up capability of multi-turn dialogues. ConvBench presents substantial challenges in multi-turn visual conversations with LVLMs and plays a role in the LVLM field, such as MT-Bench  in the LLM field.

(2) **Per-Turn Performance.** According to Table 1, we compute the mean scores of \(20\) models for each turn, to explore the impact of turn count on LVLMs' performance. The average performances of LVLMs are \(15.44\), \(15.40\), and \(12.55\) in the first, second, and third turn, respectively, which shows a decline between the first turn and subsequent turns, which suggests that LVLMs tend to generate comprehension biases as the multi-turn dialogue progresses or forget the content of previous turns.

(3) **The Impact of Dialogue History in Multi-Turn Conversation for LVLMs.** When comparing the \(S_{O}\) column against the \(_{O}\) and \(_{O}\) columns in Table 1, the numbers in parentheses illustrate the improvement in overall multi-turn conversation performance resulting from flawless previous responses. The enhancement indicates that the LVLMs can leverage previous responses to improve the responses in the current turn. We also find that the high-quality dialogue history, which plays an important role in in-context learning examples, provides effective guidance for the LVLM's responses.

(4) **Effect of Language Model Size.** We find that the trend of increasing language model size is related to an improvement in LVLM's performance on multi-turn conversation by comparing the InterVL-Chat-v1-2, ShareGPT4-13B, and LLaVA-V1.5-13B with the InterVL-Chat-v1-5, ShareGPT4-7B, and LLaVA-V1.5-7B. The detailed model information is shown in the supplementary.

## 6 Conclusion and Limitation

We introduce a multi-turn conversation benchmark named ConvBench for LVLMs. It comprises 577 multi-turn conversations across 215 tasks. Each conversation consists of three-level hierarchical questions: perception, reasoning, and creation. It can decouple capacities in evaluation for more accurate error attribution. Besides, an automatic evaluation pipeline is proposed. From the experimental results, ConvBench is challenging for current LVLMs, and we also have some insightful findings.

**Limitation** Each conversation in ConvBench is constructed through meticulous annotations and large labor to ensure quality. Thus, the data scale is not too large. We will continually expand ConvBench in terms of data scale and the number of tasks in our future work.

**Broader Impact** We hope this work can blaze a new trail in LVLMs evaluation. We do not foresee obvious undesirable ethical/social impacts at this moment.