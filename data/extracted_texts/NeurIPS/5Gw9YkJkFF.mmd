# PAC Learning Linear Thresholds

from Label Proportions

 Anand Brahmbhatt

Google Research India

anandpareshb@google.com

&Rishi Saket

Google Research India

rishisaket@google.com

&Aravindan Raghuveer

Google Research India

araghuveer@google.com

###### Abstract

Learning from label proportions (LLP) is a generalization of supervised learning in which the training data is available as sets or _bags_ of feature-vectors (instances) along with the average instance-label of each bag. The goal is to train a good instance classifier. While most previous works on LLP have focused on training models on such training data, computational learnability of LLP was only recently explored by  who showed worst case intractability of properly learning _linear threshold functions_ (LTFs) from label proportions. However, their work did not rule out efficient algorithms for this problem on natural distributions.

In this work we show that it is indeed possible to efficiently learn LTFs using LTFs when given access to random bags of some label proportion in which feature-vectors are, conditioned on their labels, independently sampled from a Gaussian distribution \(N(,)\). Our work shows that a certain matrix - formed using covariances of the differences of feature-vectors sampled from the bags with and without replacement - necessarily has its principal component, after a transformation, in the direction of the normal vector of the LTF. Our algorithm estimates the means and covariance matrices using subgaussian concentration bounds which we show can be applied to efficiently sample bags for approximating the normal direction. Using this in conjunction with novel generalization error bounds in the bag setting, we show that a low error hypothesis LTF can be identified. For some special cases of the \(N(,)\) distribution we provide a simpler mean estimation based algorithm. We include an experimental evaluation of our learning algorithms along with a comparison with those of  and random LTFs, demonstrating the effectiveness of our techniques.

+
Footnote †: \(*\) – equal contribution

## 1 Introduction

In _learning from label proportions_ (LLP), the training data is aggregated into sets or _bags_ of feature-vectors (instances). For each bag we are given its constituent feature-vectors along with only the sum or average of their labels The goal is a to obtain a _good_ instance-level classifier - one that minimizes the classification error on a test set of instances or bags. In this work we study the LLP learnability over Gaussian distributions of linear threshold functions (LTFs), also called _linear classifiers_ or _halfspaces_, given by \(f()=(^{}+c)\) where \((a):=1\) if \(a>0\) and \(0\) otherwise.

The _probably approximately correct_ (PAC) model of  states that a _concept_ class \(\) of \(\{0,1\}\)-valued functions can be learnt by a _hypothesis_ class \(\) if there is an algorithm to efficiently obtain, using iid samples from a distribution on \((,f())\), a hypothesis \(h\) of arbitrarily high accuracy on that distribution, for any unknown \(f\). If \(=\) we say that \(\) is _properly_ learnable, fore.g. LTFs are known to be properly learnable using linear programming (). This notion can be extended to the LLP setting - which for brevity we call PAC-LLP - as follows: distribution \(D\) is over bags and their label proportions \((B,(B,f))\) where \(B=\{_{1},,_{q}\}\) is a bag of feature vectors and \((B,f)=\{f()\;\; B\}\). A bag \((B,(B,f))\) is said to be _satisfied_ by \(h\) iff \((B,h)=(B,f)\), and the accuracy of \(h\) is the fraction of bags satisfied by it.

With the above notion of PAC-LLP,  studied the learnability of LTFs and rather disturbingly showed that for any constant \(>0\) it is NP-hard to PAC-LLP learn an LTF using an LTF which satisfies \((1/2+)\)-fraction of the bags when all bags are of size at most \(2\), which was subsequently strengthened to a \((4/9+)\)-factor hardness by  who also proved \((1/q+)\)-factor hardness when the bag size is at most \(q\), for any \(q 3\). This is in contrast to the supervised learning (i.e, with unit-sized bags) in which an LTF can be efficiently learnt by an LTF using linear programming. On the algorithmic side,  gave a _semi-definite programming_ (SDP) based algorithm to find an LTF satisfying \((2/5)\)-fraction of bags of size \( 2\), which was extended by  to a fairly involved SDP yielding \((1/12)\)-approximation for bags of size \( 3\), while no non-trivial algorithms for bags of size \(>3\) are known. These results show that PAC-LLP learning LTFs using LTFs is intractable on _hard_ bag distributions, and even the non-trivial algorithms for bag sizes \( 3\) are via complicated convex programming techniques. A natural question therefore, is whether the problem is tractable on natural distributions that may arise out of real world scenarios.

We answer the above question in the affirmative when the feature-vectors are distributed according to some (unknown) Gaussian distribution \(=N(,)\) in \(d\)-dimensions. Gaussian distributions are ubiquitous in machine learning and in many applications the input data distribution is modeled as multivariate Gaussians, and several previous works [8; 30] have studied learnability in Gaussian distributions. An unknown target LTF is given by \(f():=(_{*}^{}+c_{*})\) where \(\|_{*}\|_{2}=1\). Let \(_{a}\) be the distribution of \(\) conditioned on \(f()=a\), for \(a\{0,1\}\). Using this we formalize the notion of a distribution \(\) on bags of size \(q\) and average label \(k/q\): a random bag \(B\) sampled from \(\) consists of \(k\) id samples from \(_{1}\) and \((q-k)\) iid samples from \(_{0}\). The case of \(k\{0,q\}\) is uninteresting as all instances in such bags are either labeled \(0\) or \(1\) and traditional PAC-learning for LTFs can be employed directly. Unlike [25; 26] our objective is to directly maximize the instance-level level accuracy on \(\). With this setup we informally describe our main result.

**Our PAC-LLP LTF Learner** (Informal): Assuming mild conditions on \(,\) and \(c_{*}\), for any \(q,k^{+}\) s.t. \(1 k q-1\) and \(,>0\), there is an algorithm that samples at most \(m\) bags from \(\) and runs in time \(O(t+m)\) and with probability \(1-\) produces an LTF \(h\) s.t.

\(_{}[f() h()]\) if \(k q/2\), and

\(_{}[f() h()]\) or \([f()(1-h())]\) if \(k=q/2\),

where \(t,m\) are fixed polynomials in \(d,q,(1/),(1/)\). We also obtain a more efficient algorithm when \(k q/2\), \(=\), \(c_{*}=0\) and \(=\). The ambiguity in the case of \(k=q/2\) is inherent since bags of label proportion \(1/2\) consistent with an LTF \(f()\) are also consistent with \((1-f())\).

**Remark 1.1** (Mixtures of \((q,k)\)).: _The training data could consist of bags of different sizes and label proportions, however typically the the maximum size of bags is bounded by (say) \(Q\), and in a large enough sample we would have at least \((1/Q^{2})\)-fraction of bags of a particular size and label proportion and we can apply our PAC-LLP LTF Learner above to that subsample._

### Related Work

The LLP problem is motivated by many real applications where labels are available not for each feature-vector but only as the average labels of bags of feature-vectors. This may occur because of privacy and legal ([24; 33])) reasons, supervision cost () or lack of labeling instrumentation (). Previous works ([9; 15; 20; 24]) on LLP have applied techniques such as such as clustering, and linear classifiers and MCMC. Specifically for LLP, assuming class conditional independence of bags,  gave an algorithm to learn an exponential generative model, which was further generalized by . On the other hand, the work of  proposed a novel _proportional_ SVM based algorithms which optimized the SVM loss over instance-labels which were constrained by bag-level loss w.r.t the given label-proportions. Subsequently, approaches based on deep neural nets for large-scale and multi-class data ([18; 11; 19; 21]), as well as bag pre-processing techniques ([28; 27]) have been developed. Recently, [4; 6] have proposed model training methods for either random or curated bags.

The LLP framework (as an analogue of PAC learning) was first formalized in the work of . They bounded the generalization error of a trained classifier when taking the (bag, label-proportion)-pairsas instances sampled iid from some distribution. Their loss function was different - a weaker notion than the strict bag satisfaction predicate of . A single-bag variant - _class ratio estimation_ - of LLP was studied by  in which learning LTFs has a simple algorithm (see Appendix G). Nevertheless, the study of computational learning in the LLP framework has been fairly limited, apart from the works of  whose results of learning LTFs in the LLP setting have been described earlier in this section.

In the fully supervised setting  showed that LTFs can be learnt using LTFs via linear programming without any distributional assumptions. Adversarial label noise makes the problem NP-hard to approximate beyond the trivial \(}{{2}}\)-factor even using constant degree polynomial thresholds as hypothesis (). However, under distributional assumptions a series of results () have given efficient algorithms to learn adversarially noisy LTFs.

Next, Sec. 1.2 mathematically defines our problem statement. Sec. 1.3 states the main results of this paper. Sec. 1.4 provides an overview of our techniques. Sec. 2 mentions some preliminary results which are used in our proofs. Sec. 3 defines and analyses a subroutine which we use in all our algorithms. Sec. 4 provides a complete proof for one of our main results. Sec 5 gives brief proof sketches of our other results. Sec. 6 mentions some experiments which support of our results.

### Problem Definition

**Definition 1.2** (Bag Oracle).: _Given distribution \(\) over \(^{d}\) and a target concept \(f:\{0,1\}\), the bag oracle for size \(q\) and label proportion \(k/q\) (\(1 k q-1\)), denoted by \((f,,q,k)\), generates a bag \(\{^{(i)}\}_{i=1}^{q}\) such that \(^{(i)}\) is independently sampled from_ (i)_\(_{f,1}\) for \(i=\{1,,k\}\), and_ (ii)_\(_{f,0}\) _for \(i=\{k+1,,q\}\), where \(_{f,a}\) is \(\) conditioned on \(f()=a\), for \(a\{0,1\}\)._

### Our results

We first state our result (proved in Appendix A) for the case of standard \(d\)-dimensional Gaussian distribution \(N(,)\), homogeneous target LTF and unbalanced bags.

**Theorem 1.3**.: _For \(q>2\) and \(k\{1,,q-1\}\) s.t. \(k q/2\) and LTF \(f():=(_{}^{})\), there is an algorithm that samples \(m\) iid bags from \((f,N(,),q,k)\) and runs in \(O(m)\) time to produce a hypothesis \(h():=(}^{})\) s.t. w.p. at least \(1-\) over the sampling, \(_{}[f() h()]\), for any \(,>0\), when \(m O((d/^{2})(d/))\)._

The above algorithm is based on estimating the mean of the bag vectors, which unfortunately does not work when \(k=q/2\) or for a general covariance matrix \(\). We instead use a covariance estimation based approach - albeit with a worse running time - for our next result which is proved in Sec. 4. \(_{}\) and \(_{}\) denote the minimum and maximum eigenvalues of the covariance matrix \(\).

**Theorem 1.4**.: _For \(q>2\), \(k\{1,,q-1\}\), \(f():=(_{}^{})\), and positive definite \(\) there is an algorithm that samples \(m\) iid bags from \((f,N(,),q,k)\) and runs in \((m)\) time to produce a hypothesis \(h():=(}^{})\) s.t. w.p. at least \(1-\) over the sampling_

* _if_ \(k q/2\)_,_ \(_{}[f() h()]\)_, and_
* _if_ \(k=q/2\)_,_ \(\{_{}[f() h()],_{}[f( )()]\}\)_, where_ \(():=(-}^{})\)__

_for any \(,>0\), when \(m O((d/^{4})(d/)(_{}/_{})^{6}q ^{8})\)._

Our general result stated below (proved in Appendix C), extends our algorithmic methods to the case of non-centered Gaussian space and non-homogeneous LTFs.

**Theorem 1.5**.: _For \(q>2\), \(k\{1,,q-1\}\), \(f():=(_{}^{}+c_{})\), and positive definite \(\) there is an algorithm that samples \(m\) iid bags from \((f,N(,),q,k)\) and runs in \((m)\) time to produce a hypothesis \(h():=(}^{}+)\) s.t. w.p. at least \(1-\) over the sampling_

* _if_ \(k q/2\)_,_ \(_{}[f() h()]\)_, and_
* _if_ \(k=q/2\)_,_ \(\{_{}[f() h()],_{}[ f()()]\}\)_, and_

_for any \(,>0\), when \(m O((d/^{4}))}{(()(1-()))^{ 2}}(d/)(}{_{}})^{4}( }+\|\|_{2}}{}} )^{4}q^{8})\) where \((.)\) is the standard Gaussian cdf and \(=-+_{}^{}}{\| ^{1/2}_{}\|_{2}}\)_The value of \(}\) output by our algorithms is a close estimate of \(_{*}\) (or possibly \(-_{*}\) in the case of balanced bags). Note that our algorithms do not require knowledge of \(\) or \(\), and only the derived parameters in Thms. 1.4 and 1.5 are used for the sample complexity bounds. They are based on the certain properties of the empirical mean-vectors and covariance matrices formed by sampling vectors or pairs of vectors from random bags of the bag oracle. An empirical mean based approach has been previously developed by  in the LLP setting to estimate the parameters of an exponential generative model, when bag distributions satisfy the so-called class conditioned independence.e., given its label, the feature-vector distribution is same for all the bags. These techniques were extended by  to linear classifiers with loss functions satisfying certain smoothness conditions. While the bag oracle in our setup satisfies such conditioned independence, we aim to minimize the instance classification error on which the techniques of  are not applicable.

For the case when \(q=1\) (ordinary classification), the sample complexity is \(O(d/(d/))\) as one can solve a linear program to obtain an LTF and then use uniform convergence to bound the generalization error. The sample complexity expressions in Theorems 1.3, 1.4 and 1.5 have the same dependence on \(d\) and \(\). However, they have higher powers of \(1/\). They also include other parameters like the bag size (\(q\)), condition number of \(\) (\(_{}/_{}\)) and the normalized distance of mean of the Gaussian to the LTF (\(l\)). The origins and significance of these discrepancies are discussed in Sec. 1.4.

### Our Techniques

**Theorem 1.3**: **Case \(N(,)\), \(f()=(_{*}^{})\), \(k q/2\).** Assume that \(k>q/2\). A randomly sampled bag with label proportion \(k/q\) has \(k\) vectors iid sampled from the positive side of the separating hyperplane passing through origin, and \((q-k)\) iid sampled from its negative side. It is easy to see that the expected sum of the vectors vanishes in all directions orthogonal to the normal vector \(_{*}\), and in the direction of \(_{*}\) it has a constant magnitude. The case of \(k<q/2\) is analogous with the direction of the expectation opposite to \(_{*}\). Sampling a sufficient number of bags and a random vector from each of them, and taking their normalized expectation (negating if \(k<q/2\)) yields the a close estimate \(}\) of \(_{*}\), which in turn implies low classification error. The sample complexity is the same as that for mean estimation bag-vectors (see Section 3) and thus the power of \(1/\) is \(2\).

This simple approach however does not work when \(r=q/2\), in which case the expectation vanishes completely, or for general Gaussian distributions which (even if centered) could be skewed in arbitrary directions. We present our variance based method to handle these cases.

**Theorem 1.4**: **Case \(N(,)\), \(f()=(_{*}^{})\).** To convey the main idea of our approach, consider two different ways of sampling two feature-vectors from a random bag of the oracle. The first way is to sample two feature-vectors \(_{1},_{2}\) independently and u.a.r from a random bag. In this case, the probability that they have different labels (given by \(f\)) is \(2k(q-k)/q^{2}\). The second way is to sample a random _pair_\(}_{1},}_{2}\) of feature-vectors i.e., without replacement. In this case, the probability of different labels is \(2k(q-k)/(q(q-1))\) which is strictly greater than \(2k(q-k)/q^{2}\). Since the labels are given by thresholding in the direction of \(_{*}\), this suggests that the variance of \((}_{1}-}_{2})\) w.r.t. that of \((_{1}-_{2})\) is maximized in the direction of \(_{*}\). Indeed, let be the _pair_ covariance matrix and let be the _bag_ covariance matrix. Then we show that \(_{*}=_{}()\) where \(():=^{}_{D}}{^{}_{B}}\). A simple transformation gives us that

\[_{*}=_{B}^{-1/2}( _{B}^{-1/2}_{D}_{B}^ {-1/2})\] (1)

This suggests the following algorithm: sample enough bags to construct the corresponding empirical estimates \(}_{D}\) and \(}_{B}\) and then compute the empirical proxy of the RHS of (1). We show that using close enough empirical estimates w.h.p the algorithm computes a vector \(}\) s.t. one of \(}\) is close to \(_{*}\), and via a geometric stability argument this implies that one of \(}\) yields an LTF that has small instance-level classification error.

At this point, if \(k=q/2\), there is no way to identify the correct solution from \(}\), since a balanced bag, if consistent with an LTF, is also consistent with its complement. On the other hand, if \(k q/2\) we can obtain the correct solution as follows. It is easy to show that since \(f\) is homogeneous and the instance distribution is a centered Gaussian, the measure of \(\{ f()=a\}\) is \(1/2\) for \(a=\{0,1\}\). Thus, one of \(h():=(}^{})\), \(()=(-}^{})\) will have a high bag satisfaction accuracy. Thus, a large enough sample of bags can be used to identify one of \(h,\) having a high bag satisfaction accuracy. Lastly, we use a novel generalization error bound (see below) to show that the identified LTF also has a high instance classification accuracy.

The algorithm incurs a sample complexity of \(O((q_{}/(_{}))^{4})\) to estimate \(_{D}\) and \(_{B}\) accurately so that the distance between \(}\) (or \(-}\)) and \(_{*}\) is sufficiently bounded. (see Lemmas 3.1 and 4.1). In fact, the distance is required to be \(O((/q)/_{}})\) to translate the geometric bound into an \(\)-miscalification error bound, thereby incurring a further factor of \(O(q^{4}(_{}/_{})^{2})\) in the sample complexity (see Lemma 2.3). The higher powers in the dependencies are mainly due to the second moment estimates which degrade with larger values of \((_{}/_{})\). Note that the sample complexity explicitly depends on the bag size \(q\) (and not just the label proportion \(k/q\)). This is because the probability of of sampling (without replacement from a bag) a pair of differently labeled feature-vectors is \(2(k/q)(1-k/q)/(1-1/q)\). Keeping \(k/q\) the same, this probability decreases with increasing bag size, thereby increasing the sample complexity for larger bags.

**Theorem 1.5**: **Case \(N(,)\), \(f()=(_{*}^{}+c_{*})\).** We show that (1) also holds in this case, and therefore we use a similar approach of empirically estimating the pair and bag covariance matrices solving (1) works in principle. However, there are complications, in particular the presence of \(\) and \(c_{*}\) degrades the error bounds in the analysis, thus increasing the sample complexity of the algorithm. This is because the measures of \(\{ f()=a\}\) for \(a=\{0,1\}\) could be highly skewed if \(\|\|_{2}\) and/or \(|c_{*}|\) is large. Moreover, the spectral algorithm only gives a solution \(}\) for \(_{*}\). An additional step is required to obtain an estimate of \(c_{*}\). This we accomplish using the following procedure which, given a sample of \(s\) bags and any \(\) outputs a \(\) which has the following property: if \(s^{*}=_{c}\{(^{} +c)\}\), then \(\) will satisfy at least \(s^{*}-1\) bags. This is done by ordering the values \(^{}\) of the vectors \(\) within each bag in decreasing order, and then constructing set of the \(k\)th values of each bag. Out of these \(s\) values, the one which taken as \(c\) in \((^{}+c)\) satisfies the most bags, is chosen to be \(\).

Due to the non-homogeneity of the Gaussian distribution and the LTF \(f\), the application of Lemma 2.3 instead incurs an \(O(q^{4}((}+\|\|_{2})/})^{ 4})\) factor in the sample complexity, while a factor of \(O(^{2})\) is towards the estimation of \(_{B}\) and \(_{D}\). Note that \(\) is the distance from \(\) to the hyperplane of \(f\) normalized by the stretch induced by \(\), and thus a larger value of \(\) implies a lower density near the hyperplane leading to an increased sample complexity. Lastly, a further blowup by \(1/(()(1-()))^{2}\) comes from bounding the sample error from geometric bound between \(}\) and \(_{*}\), and is required for a sufficiently accurate approximation of \(c_{*}\).

**Generalization Error Bounds.** We prove (Thm. 2.2) bounds on the generalization of the error of a hypothesis LTF \(h\) in satisfying sampled bags to its distributional instance-level error. Using this, we are able to distinguish (for \(k q/2\)) between the two possible solutions our principal component algorithm yields - the one which satisfies more of the sampled bags has w.h.p. low instance-level error. For proving these bounds, the first step is to use a bag-level generalization error bound shown by  using the techniques of . Next, we show that low distributional bag satisfaction error by \(h\) implies low instance level error. This involves a fairly combinatorial analysis of two independent binomial random variables formed from the incorrectly classified labels within a random bag. Essentially, unless \(h\) closely aligns with \(f\) at the instance level, with significant probability there will be an imbalance in these two random variables leading to \(h\) not satisfying the bag.

**Subgaussian concentration bounds.** The standard estimation bounds for Gaussians are not directly applicable in our case, since the random vector sampled from a random bag is biased according to its label given by \(f\), and is therefore not a Gaussian vector. To obtain sample complexity bounds linear in \((1/)\) we use subgaussian concentration bounds for mean and covariance estimation (). For this, we show \(O()\) bound on the expectation and subgaussian norm of the thresholded Gaussian given by \(\{g N(0,1)\ g>\}\) for some \(>0\). The random vectors of interest to us are (in a transformed space) distributed as a combination of thresholded Gaussians in one of the coordinates, and \(N(0,1)\) in the rest. We show that they satisfy the \(O()\) bound on their subgaussian norm and admit the corresponding subgaussian Hoeffding (for empirical mean) and empirical covariance concentration bounds. Based on this, in Sec. 3 we abstract out the procedure used in our learning algorithms for obtaining the relevant mean and covariance estimates.

**Experiments.** We include in Sec. 6 an experimental evaluation of our learning algorithms along with a comparison of with those of  and random LTFs, demonstrating the effectiveness of our techniques.

## 2 Preliminaries

We begin with some useful linear algebraic notions. Let \(_{}()\) and \(_{}()\) denote the maximum and minimum eigenvalue of a real symmetric matrix \(\). The _operator norm_\(\|\|_{2}:=_{\|\|_{2}=1}\|\|_{2}\) for such matrices is given by \(_{}()\).

We shall restrict our attention to symmetric _positive definite_ (p.d.) matrices \(\) which satisfy \(^{}>0\) for all non-zero vectors \(\), implying that \(_{}()>0\) and \(^{-1}\) exists and is symmetric p.d. as well. Further, for such matrices \(\), \(^{1/2}\) is well defined to be the unique symmetric p.d. matrix \(\) satisfying \(=\). The eigenvalues of \(^{1/2}\) are the square-roots of those of \(\). We have the following lemma which is proved in Appendix B.6.

**Lemma 2.1**.: _Let \(\) and \(\) be symmetric p.d. matrices such that \(\|-\|_{1}\|\|_{2}\). Let \(_{1},_{2}^{d}\) be two unit vectors such that \(\|_{1}-_{2}\|_{2}_{2}\). Then, \(\|_{1}}{\|_{1}\|_{2}}-_{2}} {\|_{2}\|_{2}}\|_{2} 4()}{ _{}()}(_{2}+_{1})\) when \(()}{_{}()}(_{2}+ _{1})\)._

**Bag Oracle and related statistics.** Let \(:=(f,,q,k)\) be any bag oracle with \(k\{1,,q-1\}\) for an LTF \(f():=_{}^{}+c_{}\) in \(d\)-dimensions, and let \(\) be a collection of \(m\) bags sampled iid from the oracle. Define for any hypothesis LTF \(h\),

\[_{}(h,f,,q,k):=_{B }[\{h()\;|\; B. \} k/q],\] (2) \[_{}(h,):=|\{B\;|\;\{h( )\;|\; B.\} k/q\}|/m.\] (3)

We define the following statistical quantities related to \(\). Let \(\) be a random feature-vector sampled uniformly from a random bag sampled from \(\). Let,

\[_{B}:=[]_{B}:=[(- _{B})(-_{B})^{ }]=[].\] (4)

Now, let \(=_{1}-_{2}\) where \((_{1},_{2})\) are a random pair of feature-vectors sampled (without replacement) from a random bag sampled from \(\). Clearly \([]=\). Define

\[_{D}:=[^{} ]=[].\] (5)

**Generalization and stability bounds.** We prove in Appendix D.1 the following generalization bound from bag classification error to instance classification error.

**Theorem 2.2**.: _For any \(<1/4q\) if \(_{}(h,)\) then,_

(i) _if \(k q/2\), \(_{}[f() h()] 4\), and_

(ii) _if \(k=q/2\), \(_{}[f() h()] 4\) or \([f()(1-h())] 4\), w.p. \(1-\), when \(m C_{0}d( q+(1/))/^{2}\), for any \(>0\) and absolute constant \(C_{0}>0\)._

In some cases we directly obtain geometric bounds on the hypothesis classifier and the following lemma (proved in Appendix D.2) allows us to straightaway bound the classification error.

**Lemma 2.3**.: _Suppose \(\|-}\|_{2}_{1}\) for unit vectors \(,}\). Then, \([(^{T}+c)( }^{T}+c)]\) where \(=_{1}(c_{0}}/_{}}+c_ {1}\|\|_{2}/}})\) for some absolute constants \(c_{0},c_{1}>0\) and \(_{}\),\(_{}\) are the maximum and minimum eigenvalues of \(\) respectively._

## 3 Bag distribution statistics estimation

We provide the following estimator for \(_{B},_{B}\) and \(_{D}\) defined in (4) and (5). We have the following lemma - which follows from the subgaussian distribution based mean and covariance concentration bounds shown for thresholded Gaussians (see Appendix E) - whose proof is given in Appendix E.3.

**Lemma 3.1**.: _If \(m O((d/^{2})O(^{2})(d/))\) where \(\) is as given in Lemma E.13 then Algorithm 1 returns \(}_{B},}_{B},}_{D}\) such that \(\|}_{B}-_{B}\|_{2}}}/2\), \(\|}_{B}-_{B}\|_{2}_ {}\), and \(\|}_{D}-_{D}\|_{2} _{}\), w.p. at least \(1-\), for any \(,>0\). Here \(_{}\) is the maximum eigenvalue of \(\)._

## 4 Proof of Theorem 1.4

For the setting of Theorem 1.4, we provide Algorithm 2. It uses as a subroutine a polynomial time procedure \(\) for the principal eigen-vector of a symmetric matrix, and first computes two LTFs given by a normal vector and its negation, returning the one that has lower error on a sampled collection of bags.

```
0:\(=(f,=N(,),q,k),m,s\), where \(f()=(_{*}^{})\), \(\|_{*}\|_{2}=1\).
1. Compute \(}_{B},}_{D}\) using \(\) with \(m\) samples.
2. \(}=}_{B}^{-1/2}( }_{B}^{-1/2}}_{D}} _{B}^{-1/2})\) if \(}_{B}^{-1/2}\) exists, else exit.
3. Let \(}=}/\|\|_{2}\). 4. If \(k=q/2\)return:\(h=(}^{})\), else
4. Let \(=(-}^{})\).
5. Sample a collection \(\) of \(s\) bags from \(\).
6. Return\(h^{*}\{h,\}\) which has lower \(_{}(h^{*},)\). ```

**Algorithm 2** PAC Learner for no-offset LTFs over \(N(,)\)

**Lemma 4.1**.: _For any \(,(0,1)\), if \(m O((d/^{4})(d/)(_{}/_{ })^{4}q^{4})\), then \(}\) computed in Step 3 of Alg. 2 satisfies \(\{\|}-_{*}\|_{2},\|}+_{* }\|_{2}\}\), w.p. \(1-/2\)._

The above, whose proof is deferred to Sec. 4.1, is used in conjunction with the following lemma.

**Lemma 4.2**.: _Let \(k q/2\), \(,(0,1)\) and suppose \(}\) computed in Step 3 of Alg. 2 satisfies \(\{\|}-_{*}\|_{2},\|}+_{* }\|_{2}\}\), Then, with \(s O(d( q+(1/))/^{2})\), \(h^{*}\) in Step. 3.c satisfies \(_{}[h^{*}() f()] 16c_{0}q }}{_{}}}\) w.p. \(1-/2\), where constant \(c_{0}>0\) is from Lem. 2.3._

With the above we complete the proof of Theorem 1.4 as follows.

Proof.: (of Theorem 1.4) Let the parameters \(,\) be as given in the statement of the theorem.

For \(k=q/2\), we use \(O(}/_{}})\) for the error bound in Lemma 4.1 thereby taking \(m=O((d/^{4})(d/)(_{}/_{ })^{6}q^{4})\) in Alg. 2, so that Lemma 4.1 along with Lemma 2.3 yields the desired misclassification error bound of \(\) for one of \(h\), \(\).

For \(k q/2\), we use \(O(}/_{}}/q)\) for the error bound in Lemma 4.1. Taking \(m=O((d/^{4})(d/)(_{}/_{ })^{6}q^{8})\) in Alg. 2 we obtain the following bound: \(\{\|}-_{*}\|_{2},\|}+_{* }\|_{2}\}}/_{}}/(16c_{ 0}q)\) with probability \(1-/2\). Using \(s O(d( q+(1/))q^{2}}}{ ^{2}_{}})\), Lemma 4.2 yields the desired misclassification error bound of \(\) on \(h^{*}\) w.p. \(1-\). 

Proof.: (of Lemma 4.2) Applying Lemma 2.3 we obtain that at least one of \(h\), \(\) has an instance misclassification error of at most \(O(}/_{}})\). WLOG assume that \(h\) satisfies this error bound i.e., \(_{}[f() h()] c_{0}}/_{}}^{}\). Since the separating hyperplane of the LTFpasses through the origin, and \(=N(,)\) is centered, \(_{}[f()=1]=_{}[f()=0]=1/2\). Thus,

\[_{}[h(x) f(x)\;\;f()=1],_{}[[h(x)  f(x)\;\;f()=0] 2^{}.\]

Therefore, the probability that a random bag from the oracle contains a feature vector on which \(f\) and \(h\) disagree is at most \(2q^{}\). Applying the Chernoff bound (see Appendix B.1) we obtain that with probability at least \(1-/6\), \(_{}(h,) 4q^{}\). Therefore, in Step 3.c. \(h^{*}\) satisfies \(_{}(h^{*},) 4q^{}\)

On the other hand, applying Theorem 2.2, except with probability \(/3\), \(_{}[f() h^{*}()] 16q^{}= 16c_{0}q}/_{}}\). Therefore, except with probability \(/2\), the bound in Lemma 4.2 holds. 

### Proof of Lemma 4.1

We define and bound a few useful quantities depending on \(k,q,_{}\) and \(_{}\) using \(1 k q-1\).

**Definition 4.3**.: _Define, (i) \(_{1}:=(-1)^{2}\) so that \(0_{1} 2/\), (ii) \(_{2}:=(1-)\) so that \(}_{2}\), (iii) \(_{3}:=}{1-_{1}}\) so that \(}_{3}\), and (iv) \(:=}}{_{}}(-_{2})}+})\) so that \(}}{_{}}}}{(1-2/)_{}}\)._

For the analysis we begin by showing in the following lemma that \(}\) in the algorithms is indeed \(_{*}\) if the covariance estimates were the actual covariances.

**Lemma 4.4**.: _The ratio \(():=^{}_{D}/^{}_{B}\) is maximized when \(=_{*}\). Moreover,_

\[()=2+)^{2}_{2}}{1-( )^{2}_{1}}():=^{} _{*}}{^{}}^{}_{*}_{*}}}\]

\[^{}_{B}=^{} (1-()^{2}_{1}),^{}_{D}=^{}(2-()^{2}(2_{1}-_{2}))\]

Proof.: Let \(:=^{1/2}\), then \( N(,)=\) where \( N(,)\). Further, \((^{})=( ^{})\) where \(=/\|\|_{2}\). Using this, we can let \(_{B}=_{B}\) as a random feature-vector sampled uniformly from a random bag sampled from \(\). Also, let \(_{D}=_{D}\) be the difference of two random feature vectors sampled uniformly without replacement from a random bag sampled from \(\). Observe that the ratio \(()=[^{}_{D}]/ [^{}_{B}]=[ ^{}_{D}]/[^{}_{B}]\).

Let \(_{*}:=_{*}/\|_{*}\|_{2}\), and \(g^{*}:=_{*}^{}\) which is \(N(0,1)\). For \(a\{0,1\}\), let \(_{a}\) be \(\) conditioned on \((_{*}^{})=a\). Let \(g_{a}^{*}:=_{*}^{}_{a}\), \(a\{0,1\}\), be the half normal distributions satisfying \([(g_{a}^{*})^{2}]=1\) and \([g_{a}^{*}]=(-1)^{1-a}\). With this setup, letting \(g_{B}^{*}:=_{*}^{}_{B}\) and \(g_{D}^{*}:=_{*}^{}_{D}\) we obtain (using Lemma B.2 in Appendix B.2)

\[[g_{B}^{*}]=1-_{1},[g_{D}^{*}]=2( 1-_{1})+_{2}\]

Now let \(}\) be a unit vector orthogonal to \(_{*}\). Let \(=}^{}\) be \(N(0,1)\). Also, let \(_{a}=}^{}_{a}\) for \(a\{0,1\}\). Since \(_{a}\) are given by conditioning \(\) only along \(_{*}\), \(_{a} N(0,1)\) for \(a\{0,1\}\). In particular, the component along \(\) of \(_{B}\) (call it \(_{B}\)) is \(N(0,1)\) and that of \(_{D}\) (call it \(_{D}\)) is the difference of two iid \(N(0,1)\) variables. Thus, \([_{B}]=1\) and \([_{D}]=2\). Moreover, due to orthogonality all these gaussian variables corresponding to \(}\) are independent of those corresponding to \(_{*}\) defined earlier. Now let \(=_{*}+}\), where \(=}\) be any unit vector. From the above we have,

\[[^{}_{D} ]}{[^{}_{B}]}= [ g_{D}^{*}+_{D}]}{ [ g_{B}^{*}+_{B}]}=[g_{D}^{*}]+^{2}[ _{D}]}{^{2}[g_{B}^{*}]+^{2} [_{B}]}= (1-_{1})+^{2}_{2}+2^{2}}{^{2}(1- _{1})+^{2}}\] \[= 2+_{2}}{1-^{2}_{1}}\] (6)

where the last equality uses \(=}\). Letting \(=/\|\|_{2}\) we obtain that \(=,

**Lemma 4.5**.: \(\|_{2}=1}{}()=_{B }^{-1/2}(_{B}^{-1/2}_{ D}_{B}^{-1/2})\)__

Proof.: This follows directly from its generalization in Appendix B.5. 

We now complete the proof of Lemma 4.1 (with \(\) instead of \(/2\) for convenience). By Lemma 3.1, taking \(m O((d/_{1}^{2})(d/))\) ensures that \(\|_{B}\|_{2}_{1}_{}\) and \(\|_{D}\|_{2}_{1}_{}\) w.p. at least \(1-\) where \(_{B}=}_{B}-_{B}\) and \(_{D}=}_{D}-_{D}\). We start by defining \(():=^{}_{B}} {^{}_{D}}\) which is the equivalent of \(\) using the estimated matrices. Observe that it can be written as \(()=^{}_{B}+ ^{}_{B}}{^{} _{D}+^{}_{D}}\). Using these we can obtain the following bound on \(\): for any \(^{d}\), \(|()-()|_{1}|( )|\) w.p. at least \(1-\) (*) as long as \(_{1})}{2}}}{_ {}}\), which we shall ensure (see Appendix B.4).

For convenience we denote the normalized projection of any vector \(\) as \(}:=^{1/2}}{\|^ {1/2}\|_{2}}\). Now let \(}^{d}\) be a unit vector such that \(\{\|}-}_{*}\|_{2},\|}+ }_{*}\|_{2}\}_{2}\). Hence, using the definitions from Lemma 4.4, \(|()| 1-_{2}^{2}/2\) while \((_{*})=1\) which implies \((_{*})-()_{3}_{2}^{2}/2\). Note that \(()(_{*})=2+_{3}\). Choosing \(_{1}<}{4(2+_{3})}_{2}^{2}\), we obtain that \((_{*})(1-_{1})>()(1+ _{1})\). Using this along with the bound (*) we obtain that w.p. at least \(1-\), \((_{*})>()\) when \(_{2}>0\). Since our algorithm returns \(}\) as the maximizer of \(\), w.p. at least \(1-\) we get \(\{\|}-}_{*}\|_{2},\|}+ }_{*}\|_{2}\}_{2}\). Using Lemma 2.1, \(\{\|}-_{*}\|_{2},\|}+_{*}\|_{2}\} 4}}{_{}}} _{2}\). Substituting \(_{2}=}}{_ {}}}\), \(\|-_{*}\|_{2}\) w.p. at least \(1-\). The conditions on \(_{1}\) are satisfied by taking it to be \( O(^{2}_{}}{(2+ _{3})_{}})\), and thus we can take \(m O((d/^{4})(d/)(} }{_{}})^{2}^{2}(}{_{3 }})^{2})=O((d/^{4})(d/)(}}{_{}})^{4}q^{4})\), using Defn. 4.3. This completes the proof.

## 5 Proof Sketches for Theorems 1.3 and 1.5

**Theorem 1.3**.: **Case \(N(,)\), \(f()=(_{*}^{})\), \(k q/2\).** The algorithm (Algorithm 3) and the proof is in Appendix A. We argue that a vector sampled uniformly at random from a bag is distributed as \(_{1}+(1-)_{0}\) where \(_{a} N(,)\) conditioned on \(f(_{a})=a\) and \(\) is an independent \(\{0,1\}-\)Bernoulli r.v. s.t. \(p(=1)=k/q\). This along with the fact that uncorrelated Gaussians are independent, allows us to show that the expectation is \(\) in any direction orthogonal to \(_{*}\) and to compute the expectation in the direction of \(_{*}\). We then use Lemma 3.1 to get the sample complexity expression.

**Theorem 1.5**.: **Case \(N(,)\), \(f()=(_{*}^{}+c_{*})\).** The algorithm (Algorithm 4) and the detailed proof is given in Appendix C. We start by generalizing the high probability geometric error bound in Lemma 4.1 to this case (Lemma C.1 proven in Appendix C.1) and appropriately generalize \(_{1},_{2},_{3}\) and \(\). The rest of the proof is similar to Section 4.1. An extra factor of \(O(^{2})\) is introduced to the sample complexity from Lemma 3.1. Next, assuming the geometric bound, we give a high probability bound on the generalization error in Lemma C.2. In this analysis, we bound the \(_{}(h,)\) where \(h()=(}^{}+c_{*})\) with \(}\) being the geometric estimate of \(_{*}\) and \(\) is a sample of bags. This introducing the dependencies on \((}+\|\|_{2})/}\) as well as on \(()\). Our subroutine to find \(\) ensures that \(h^{*}()=(}^{}+)\) satisfies \(_{}(h^{*},)_{ }(h,)\). We then use Theorem 2.2 to bound the generalization error of \(h^{*}\). Lemmas C.1 C.2 together imply Theorem 1.5.

## 6 Experimental Results

General Gaussian.We empirically evaluate our algorithmic technique on centered and general Gaussian distributions for learning homogeneous LTFs. For homogeneous LTFs the general case algorithm (Alg. 4 in Appendix C) boils down to Alg. 2 in Sec. 4. The experimental LLP datasets are created using samples from both balanced as well as unbalanced bag oracles. In particular, for dimension \(d\{10,50\}\), and each pair \((q,k)\{(2,1),(3,1),(10,5),(10,8),(50,25),(50,35)\}\)and \(m=\) we create 25 datasets as follows: for each dataset (i) sample a random unit vector \(^{*}\) and let \(f():=(^{*})\), (ii) sample \(\) and \(\) randomly (see Appendix F for details), (iii) sample \(m=2000\) training bags from \((f,N(,),q,k)\), (iv) sample 1000 test instances \((,f())\), \( N(,)\). We fix \(=\) for the centered Gaussian case.

For comparison we include the random LTF algorithm in which we sample 100 random LTFs and return the one that satisfies the most bags. In addition, we evaluate the Algorithm of  on \((q,k)=(2,1)\), and the Algorithm of  on \((q,k)=(3,1)\). We measure the accuracy of each method on the test set of each dataset. The algorithms of  are considerably slower and we use 200 training bags for them. The results for centered Gaussian are in Table 0(a) and for the general Gaussian are in Table 0(b). We observe that our algorithms perform significantly better in terms of accuracy than the comparative methods in all the bag distribution settings. Further, our algorithms have significantly lower error bounds (see Appendix F).

Notice in Tables 0(a) and 0(b) that the test accuracy for Algorithm 2 decreases with an increase in \(q\) and \(d\). This is consistent with the sample complexity expressions in Thm. 1.4 and Thm. 1.5. Also, notice that the test accuracy for Algorithm 2 for general Gaussian (Table 0(b)) is usually lesser than the same for centered Gaussian (Table 0(a)). This supports the theoretical result that the sample complexity increases with the increase in \(l\).

Appendix F has additional details and further experiments for the \(N(,)\) with homogeneous LTFs, \(N(,)\) with non-homogeneous LTFs as well as on noisy label distributions.

## 7 Conclusion and Future work

Our work shows that LTFs can be efficiently properly learnt in the LLP setting from random bags with given label proportion whose feature-vectors are sampled independently from a Gaussian space, conditioned on their underlying labels. For the simple case of \(N(,)\) distribution and bags with unbalanced labels we provide a mean estimation based algorithm. For the general scenarios we develop a more sophisticated approach using the principal component of a matrix formed from certain covariance matrices. To resolve the ambiguity between the obtained solutions we employ novel generalization error bounds from bag satisfaction to instance classification. We also show that subgaussian concentration bounds are applicable on the thresholded Gaussians, yielding efficient sample complexity bounds. Our experimental results validate the performance guarantees of our algorithmic techniques.

In future work, classes of distributions other than Gaussian could be similarly investigated. Classifiers other than LTFs are also interesting to study in the LLP setting.