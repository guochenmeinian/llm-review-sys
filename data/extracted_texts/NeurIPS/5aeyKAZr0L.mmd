# Approximately Equivariant Graph Networks

Ningyuan (Teresa) Huang

Johns Hopkins University

nhuang19@jhu.edu

&Ron Levie

Technion - Israel Institute of Technology

levieron@technion.ac.il

&Soledad Villar

Johns Hopkins University

svillar3@jhu.edu

###### Abstract

Graph neural networks (GNNs) are commonly described as being permutation equivariant with respect to node relabeling in the graph. This symmetry of GNNs is often compared to the translation equivariance of Euclidean convolution neural networks (CNNs). However, these two symmetries are fundamentally different: The translation equivariance of CNNs corresponds to symmetries of the fixed domain acting on the image signals (sometimes known as _active symmetries_), whereas in GNNs any permutation acts on both the graph signals and the graph domain (sometimes described as _passive symmetries_). In this work, we focus on the active symmetries of GNNs, by considering a learning setting where signals are supported on a fixed graph. In this case, the natural symmetries of GNNs are the automorphisms of the graph. Since real-world graphs tend to be asymmetric, we relax the notion of symmetries by formalizing approximate symmetries via graph coarsening. We present a bias-variance formula that quantifies the tradeoff between the loss in expressivity and the gain in the regularity of the learned estimator, depending on the chosen symmetry group. To illustrate our approach, we conduct extensive experiments on image inpainting, traffic flow prediction, and human pose estimation with different choices of symmetries. We show theoretically and empirically that the best generalization performance can be achieved by choosing a suitably larger group than the graph automorphism, but smaller than the permutation group.

## 1 Introduction

Graph Neural Networks (GNNs) are popular tools to learn functions on graphs. They are commonly designed to be permutation equivariant since the node ordering can be arbitrary (in the matrix representation of a graph). Permutation equivariance serves as a strong geometric prior and allows GNNs to generalize well . Yet in many applications, the node ordering across different graphs is matched or fixed a priori, such as a time series of social networks where the nodes identify the same users, or a set of skeleton graphs where the nodes represent the same joints. In such settings, the natural symmetries arise from graph automorphisms, which effectively only act on the graph signals; This is inherently different from the standard equivariance in GNNs that concerns all possible permutations acting on both the signals and the graph domain. A permutation of both the graph and the graph signal can be seen as a _change of coordinates_ since it does not change the object it represents, just the way to express it. This parallels the _passive symmetries_ in physics, where physical observables are independent of the coordinate system one uses to express them . In contrast, a permutation of the graph signal on a fixed graph potentially transforms the object itself, not only its representation. This parallels the _active symmetries_ in physics, where the coordinate system (in this case, the graph or domain) is fixed but a group transformation on the signal results in a predictabletransformation of the outcome (e.g., permuting left and right joints in the human skeleton changes a left-handed person to right-handed). The active symmetries on a fixed graph are similar to the translation equivariance symmetries in Euclidean convolutional neural networks (CNNs), where the domain is a fixed-size grid and the signals are images.

In this work, we focus on active symmetries in GNNs. Specifically, we consider a fixed graph domain \(G\) with \(N\) nodes, an adjacency matrix \(A^{N N}\), and input graph signals \(X^{N d}\). We are interested in learning equivariant functions \(f\) that satisfy (approximate) active symmetries

\[f( X) f(X)_{N}, \]

where \(\) is a subgroup of the permutation group \(_{N}\) that depends on \(G\). For example, \(\) can be the graph automorphism group \(_{G}=\{: A=A\}\). Each choice of \(\) induces a hypothesis class \(_{}\) for \(f\): the smaller the group \(\), the larger the class \(_{}\). We aim to select \(\) so that the learned function \(f_{}\) generalizes well, also known as the _model selection_ problem [6, Chp.4]. In contrast, standard graph learning methods (GNNs, spectral methods) are defined to satisfy passive symmetries, by treating \(A\) as input and requiring \(f( A^{}, X)= f(A,X)\) for all permutations \(_{N}\) and all \(N\). But for the fixed graph setting, we argue that active symmetries are more relevant. Thus we use \(A\) to define the hypothesis class rather than treating it an input. By switching from passive symmetries to active symmetries, we will show how to design GNNs for signals supported on a fixed graph with different levels of expressivity and generalization properties.

While enforcing symmetries has been shown to improve generalization when the symmetry group is known a priori , the problem of _symmetry model selection_ is not completely solved, particularly when the data lacks exact symmetries (see for instance  and references therein). Motivated by this, we study the symmetry model selection problem for learning on a fixed graph domain. This setting is particularly interesting since (1) the graph automorphism group serves as the natural _oracle_ symmetry; (2) real-world graphs tend to be asymmetric, but admit cluster structure or local symmetries. Therefore, we define _approximate symmetries of graphs_ using the cut distance between graphs from graphon analysis. An approximate symmetry of a graph \(G\) is a symmetry of any other graph \(G^{}\) that approximates \(G\) in the cut distance. In practice, we take \(G^{}\) as coarse-grainings (or clusterings) of \(G\), as these are typically guaranteed to be close in cut distance to \(G\). We show how to induce approximate symmetries for \(G\) via the automorphisms of \(G^{}\). Our main contributions include:

1. We formalize the notion of active symmetries and approximate symmetries of GNNs for signals supported on a fixed graph domain, which allows us to study the symmetry group model selection problem. (See Sections 2, 4)
2. We theoretically characterize the statistical risk depending on the hypothesis class induced from the symmetry group, and show a bias-variance tradeoff between the reduction in expressivity and the gain in regularity of the model. (See Sections 3, 4)
3. We illustrate our approach empirically for image inpainting, traffic flow prediction, and human pose estimation. (See Section 5 for an overview, and Appendix D for the details on how to implement equivariant graph networks with respect to different symmetry groups).

### Related Work

**Graph Neural Networks and Equivariant Networks.** Graph Neural Networks (GNNs)  are typically permutation-equivariant (with respect to node relabeling). These include message-passing neural networks (MPNNs) , spectral GNNs , and subgraph-based GNNs . Permutation equivariance in GNNs can extend to edge types  and higher-order tensors representing the graph structure . Equivariant networks generalize symmetries on graphs to other objects, such as sets , images , shapes , point clouds , manifolds , and physical systems  among many others. Notably, many equivariant machine learning problems concern the setting where the domain is fixed, e.g., learning functions on a fixed sphere , and thus focus on active symmetries rather than passive symmetries such as the node ordering in standard GNNs. Yet learning on a fixed graph domain arises naturally in many applications such as molecular dynamics modeling . This motivates us to consider active symmetries of GNNs for learning functions on a fixed graph. Our work is closely related to Natural Graph Networks (NGNs) in , which use global and local graph isomorphisms to design maximally expressive GNNs for distinguishing _different graphs_. In contrast, we focus on generalization and thus consider symmetry model selection on a _fixed_ graph.

**Generalization of Graph Neural Networks and Equivariant Networks.** Most existing works focus on the graph-level tasks, where in-distribution generalization bounds of GNNs have been derived using statistical learning-theoretic measures such as Rademacher complexity  and Vapnik-Chervonenkis (VC) dimension , or uniform generalization analysis based on random graph models ; Out-of-distribution generalization properties have been investigated using different notions including transferability  (also known as size generalization ), and extrapolation . In general, imposing symmetry constraints improves the sample complexity and the generalization error . Recently, Petrache and Trivedi  investigated approximation-generalization tradeoffs using approximate symmetries for general groups. Their results are based on uniform convergence generalization bounds, which measure the worst-case performance of all functions in a hypothesis class and differ from our non-uniform analysis.

**Approximate Symmetries.** For physical dynamical problems, Wang et al.  formalized approximate symmetries by relaxing exact equivariance to allow for a small equivariance error. For reinforcement learning applications, Finzi et al.  proposed Residual Pathway Priors to expand network layers into a sum of equivariant layers and non-equivariant layers, and thus relax strict equivariance into approximate equivariance priors. In the context of self-supervised learning, Suau et al. , Gupta et al.  proposed to learn structural latent representations that satisfy approximate equivariance. Inspired by randomized algorithms, Cotta et al.  formalizes probabilistic notions of invariances and universal approximation. In , scattering transforms (a specific realization of a CNN) are shown to be approximately invariant to small deformations in the image domain, which can be seen as a form of approximate symmetry. Scattering transforms on graphs are discussed in . Additional discussions on approximate symmetries can be found in [67, Section 6].

## 2 Problem Setup: Learning Equivariant Maps on a Fixed Graph

**Notations.** We let \(\), \(_{+}\) denote the reals and the nonegative reals, \(I\) denote the identity matrix and \(\) denote the all-ones matrix. For a matrix \(Y^{N k}\), we write the Frobenious norm as \(\|Y\|_{F}\). We denote by \(X Y\) the element-wise multiplication of matrices \(X\) and \(Y\). We write \([N]=\{1,,N\}\) and \(_{N}\) as the permutation group on the set \([N]\). Groups are typically noted by calligraphic letters. Given \(,\) groups, we denote a semidirect product by \(\). For a group \(\) with representation \(\), we denote by \(_{|_{}}\) the corresponding character (see Definition 6 in Appendix A.1). Denoting \(\) means that \(\) is a subgroup of \(\).

**Equivariance.** We consider a compact group \(\) with Haar measure \(\) (the unique \(\)-invariant probability measure on \(\)). Let \(\) act on spaces \(\) and \(\) by representations \(\) and \(\), respectively. We say that a map \(f:\) is \(\)-equivariant if for all \(g\), \(x,(g^{-1})f((g)\,x)=f(x)\). Given any map \(f:\), a projection of \(f\) onto the space of \(\)-equivariant maps can be computed by averaging over orbits with respect to \(\)

\[(_{}f)(x)=_{}(g^{-1})\,f((g) x)\,(g). \]

For \(u,v\), let \( u,v\) be a \(\)-invariant inner product, i.e. \((g)u,(g)v= u,v\), for all \(g\), for all \(u,v\). Given two maps \(f_{1},f_{2}:\), we define their inner product as \( f_{1},f_{2}_{}=_{} f_{1}(x),f_{2}(x) \,(x)\), where \(\) is a \(\)-invariant measure on \(\). Let \(V\) be the space of all (measurable) map \(f:\) such that \(\|f\|_{}=}<\).

**Graphs.** We consider edge-node weighted graphs \(G=([N],A,b)\), where \([N]\) is a finite set of nodes, \(A^{N N}\) is the adjacency matrix describing the edge weights, and \(b=\{b_{1},,b_{N}\}\) are the node weights. An edge weighted graph is a special case of \(G\) where all node weights are \(1\). A simple graph is a special case of an edge weighted graph, where all edge weights are binary. Let \(_{G}\) be the automorphism group of a graph defined as \(_{G}\{_{N}:\,A\,^{}=A,\,b=b\}\), which characterizes the symmetries of \(G\). Hereinafter, \(\) is assumed to be a subgroup of \(_{N}\).

**Graph Signals and Learning Task.** We consider graph signals supported in the nodes of a fixed graph \(G\) and maps between graphs signals. Let \(=^{N d},=^{N k}\) be the input and output graph signal spaces. We denote by \(f\) a map between graph signals, \(f:\). Even though the functions \(f\) depend on \(G\), we don't explicitly write \(G\) as part of the notation of \(f\) because \(G\) is fixed. Our goal is to learn a target map between graph signals \(f^{*}:\). To this end, we assume access to a training set \(\{(X_{i},Y_{i})\}\) where the \(X_{i}\) are i.i.d. sampled from an \(_{N}\)-invariant distribution \(\) on\(\) where \(_{n}\) acts on \(\) by permuting the rows, and \(Y_{i}=f^{*}(X_{i})+_{i}\) for some noise \(_{i}\). A natural assumption is that \(f^{*}\) is approximately equivariant with respect to \(_{G}\) in some sense. Our symmetry model selection problem concerns a sequence of hypothesis class \(\{_{}\}\) indexed by \(\), where we choose the best class \(_{}^{*}\) such that the estimator \(_{}^{*}\) gives the best generalization performance.

**Equivariant Graph Networks.** We propose to learn the target function \(f^{*}\) on the fixed graph using \(\)-_equivariant graph networks_ (\(\)-Net), which are equivariant to a chosen symmetry group \(\) depending on the graph domain. Using standard techniques from representation theory, such as Schur's lemma and projections to isotypic components , we can parameterize \(\)-Net by interleaving \(\)-equivariant linear layers \(f_{}:^{N d}^{N k}\) with pointwise nonlinearity, where the weights in the linear map \(f_{}\) are constrained in patterns depending on the group structure (also known as parameter sharing or weight tying , see Appendix A.1 for technical details). In practice, we can make \(\)-Net more flexible by systematically breaking the symmetry, such as incorporating graph convolutions (i.e., \(Af_{}\)) and locality constraints (i.e., \(A f_{}\)). Compared to standard GNNs (described in Appendix D.1), \(\)-Net uses a more expressive linear map to gather global information (i.e., weights are not shared among all nodes). Importantly, \(\)-Net yields a suite of models that allows us to flexibly choose the hypothesis class (and estimator) reflecting the active symmetries in the data, and subsumes standard GNNs that are permutation-equivariant with respect to passive symmetries (but not necessarily equivariant with respect to active symmetries).

**Graphons.** A graphon is a symmetric measurable function \(W:^{2}\). Graphons represent (dense) graph limits where the number of nodes goes to infinity; they can also be viewed as random graph models where the value \(W(x,y)\) represents the probability of having an edge between the nodes \(x\) and \(y\). Let \(\) denote the space of all graphons and \(\) denote the Lebesgue measure. Lovasz and Szegedy  introduced the cut norm on \(\) as

\[\|W\|_{}_{S,T<\\ S,T}|_{S T}W(u,v)\, (u)\,(v)|. \]

Based on the cut norm, Borgs et al.  defined the cut distance between two graphons \(W,U\),

\[_{}(W,U)_{f S_{}}\|W-U^{f}\|_{}, \]

where \(S_{}\) is the set of all measure-preserving bijective measurable maps between \(\) and itself, and \(U^{f}(x,y)=U(f(x),f(y))\). Note that the cut distance is "permutation-invariant," where measure preserving bijections are seen as the continuous counterparts of permutations.

## 3 Generalization with Exact Symmetry

In this section, we assume the target map \(f^{*}\) is \(_{G}\)-equivariant and study the symmetry model selection problem by comparing the (statistical) risk of different models. Concretely, the risk quantifies how a given model performs on average on any potential input. The smaller the risk, the better the model performs. Thus, we use the _risk gap_ of two functions \(f,f^{}\), defined as

\[(f,f^{})[\|Y-f(X)\|_{F}^{2}]- [\|Y-f^{}(X)\|_{F}^{2}], \]

as our model selection metric. Following ideas from Elesedy and Zaidi , we analyze the risk gap between any function \(f V\) and its \(\)-equivariant version, for \(_{N}\).

**Lemma 1** (Risk Gap).: _Let \(=^{N d},=^{N k}\) be the input and output graph signal spaces on a fixed graph \(G\). Let \(X\) where \(\) is a \(_{N}\)-invariant distribution on \(\). Let \(Y=f^{*}(X)+\), where \(^{N k}\) is random, independent of \(X\) with zero mean and finite variance and \(f^{*}:\) is \(_{G}\)-equivariant. Then, for any \(f V\) and for any compact group \(_{N}\), we can decompose \(f\) as_

\[f=_{}+f_{}^{},\]

_where \(_{}=_{}f,f_{}^{}=f- _{}\). Moreover, the risk gap satisfies_

\[(f,_{})=[\|Y-f(X)\|_{F}^{2}]- [\|Y-_{}(X)\|_{F}^{2}]=,f_{}^{}_{}}_{}+ }^{}\|_{}^{2}}_{}.\]Lemma 1, proven in Appendix B.1, adapts ideas from  to equate the risk gap to the symmetry "mismatch" between the chosen group \(\) and the target group \(_{G}\), plus the symmetry "constraint" captured by the norm of the anti-symmetric part of \(f\) with respect to \(\). Our symmetry model selection problem aims to find the group \(_{N}\) such that the risk gap is maximized. Note that for \(_{G}\), the mismatch term vanishes since \(f^{*}\) is \(_{G}\)-equivariant, but the constraint term decreases with \(\); When \(=_{G}\), we recover Lemma 6 in . On the other hand, for \(>_{G}\), the mismatch term can be positive, negative or zero (depending on \(f^{*}\)) whereas the constraint term increases with \(\).

### Linear Regression

In this section, we focus on the linear regression setting and analyze the risk gap of using the equivariant estimator versus the vanilla estimator. We consider linear estimator \(:^{N d}^{N k}\) that predicts \(=f_{}(X)=^{}X\). Given a compact group \(\) and any linear map \(\), we obtain its \(\)-equivariant version via projection to the \(\)-equivariant space (also known as intertwiner average),

\[_{}()=_{}(g)\,\,(g^{-1} )(g). \]

We denote \(_{}^{}()=-_{}()\) as the projection of \(\) to the orthogonal complement of the \(\)-equivariant space. Here \(_{}\) instantiates the orbit-average operator \(_{}\) (eqn. 2) for linear functions.

**Theorem 2** (Bias-Variance-Tradeoff).: _Let \(=^{N d},=^{N k}\) be the graph signals spaces on a fixed graph \(G\). Let \(_{N}\) act on \(\) and \(\) by permuting the rows with representations \(\) and \(\). Let \(\) be a subgroup of \(_{N}\) acting with restricted representations \(|_{}\) on \(\) and \(|_{}\) on \(\). Let \(X_{[i,j]}}{{}}(0,_{ }^{2})\) and \(Y=f^{*}(X)+\) where \(f^{*}(x)=^{}x\) is \(_{G}\)-equivariant and \(^{Nd Nk}\). Assume \(_{[i,j]}\) is random, independent of \(X\), with mean 0 and \([^{}]=_{}^{2}I<\). Let \(\) be the least-squares estimate of \(\) from \(n\) i.i.d. examples \(\{(X_{i},Y_{i}):i=1,,n\}\), \(_{}()\) be its equivariant version with respect to \(\). Let \((_{|_{}}_{|_{}})=_{ }_{|_{}}(g)_{|_{}}(g)d (g)\) denote the inner product of the characters. If \(n>Nd+1\) the risk gap is_

\[[(f_{},f_{_{}()})]=^{2}\,\|_{}^{}( )\|_{F}^{2}}_{}+^{2}dk- (_{|_{}}_{|_{}})}{n-Nd-1} }_{}.\]

The bias term depends on the anti-symmetric part of \(\) with respect to \(\), whereas the variance term captures the difference of the dimension between the space of linear maps \(^{Nd}^{Nk}\) (measured by \(N^{2}dk\)), and the space of \(\)-equivariant linear maps (measured by \((_{|_{}}_{|_{}})\); a proof can be found in [72, Section 2.2]). The bias term reduces to zero for \(_{G}\) and we recover Theorem 1 in  when \(=_{G}\). Notably, by using a larger group \(\), the dimension of the equivariant space measured by \((_{|_{}}_{|_{}})\) is smaller, and thus the variance term increases; meanwhile, the bias term decreases due to extra (symmetry) constraints on the estimator. We remark that \((_{|_{}}_{|_{}})\) depends on \(d,k\) as well: For \(d=k=1\), \((g)=(g)^{N N}\) are standard permutation matrices; For general \(d>1,k>1\), \((g)^{Nd Nd},(g)^{Nk Nk}\) are block-diagonal matrices, where each block is a permutation matrix.

To understand the significance of the risk gap, we note that (see Appendix B.1) when \(n>Nd+1\), the risk of the least square estimator is

\[[\|Y-^{}X\|_{F}^{2}]=_{}^{2} +_{}^{2}. \]

Thus, when \(n\) is small enough so that the risk gap in Theorem 2 is dominated by the variance term, enforcing equivariance gives substantial generalization gain of order \(dk-(_{|_{}}_{|_{}} )}{n-Nd-1}\).

**Example 3.1**.: In Appendix C.1 we construct an example with \(=^{3},=^{3}\), and \(x(0,_{}^{2}I_{d})\). We consider a target linear function \(f^{*}\) that is \(_{2}\)-equivariant and _approximately \(_{3}\)_-equivariant, and compare the estimators \(f_{_{_{2}}()}\) versus \(f_{_{_{3}}()}\). When the number of training samples \(n\) is small,using a \(_{3}\)-equivariant least square estimator yields better test error than the \(_{2}\)-equivariant one, as shown in figure inset. The dashed vertical line denotes the theoretical threshold \(n^{*} 35\), before which using \(_{3}\) yields better generalization than \(_{2}\). This is an example where enforcing more symmetries than the target (symmetry) results on better generalization properties.

## 4 Generalization with Approximate Symmetries

Large graphs are typically asymmetric. Therefore, the assumption of \(f^{*}\) being \(_{G}\)-equivariant in Section 3 becomes trivial. Yet, graphon theory asserts that graphs can be seen as living in a "continuous" metric space, with the cut distance (Definition 1). Since graphs are continuous entities, the combinatorial notion of exact symmetry of graphs is not appropriate. We hence relax the notion of exact symmetry to a property that is "continuous" in the cut distance. The regularity lemma [73, Theorem 5.1] asserts that any large graph can be approximated by a coarser graph in the cut distance. Since smaller graphs tend to exhibit more symmetries than larger ones, we are motivated to consider the symmetries of coarse-grained versions of graphs as their approximate symmetries (Definition 2, 3). We then present the notion of approximately equivariant mappings (Definition 4), which allows us to precisely characterize the bias-variance tradeoff (Corollary 3) in the approximate symmetry setting based on Lemma 1. Effectively, we reduce selecting the symmetry group to choosing the coarsened graph \(G^{}\). Moreover, our symmetry model selection perspective allows for exploring different coarsening procedures and choosing the one that works best for the problem.

**Definition 1** (Induced graphon).: _Let \(G\) be a (possibly edge weighted) graph with node set \([N]\) and adjacency matrix \(A=\{a_{i,j}\}_{i,j=1}^{N}\). Let \(_{N}=\{I_{N}^{n}=(,)\}_{n=1}^{N}\) be the equipartition of \(\) to \(N\) intervals (where formally the last interval is the closed interval \([,1]\)). We define the graphon \(W_{G}\) induced by \(G\) as_

\[W_{G}(x,y)=_{i,j=1}^{N}a_{i,j}_{I_{N}^{i}}(x)_{I_{N}^{ i}}(y),\]

_where \(_{I_{N}^{i}}\) is the indicator function of the set \(I_{N}^{i}\), \(i=1,,N\)._

We induce a graphon from an edge-node weighted graph \(G^{}=([M],A,b)\), with rational node weights \(b_{i}\), as follows. Let \(N\) be a value such that the node weights can be written in the form \(b=\{b_{m}=}{N}\}_{m=1}^{M},\) for some \(q_{m}_{ 0}\), \(m=1,,M\). We _blow-up_\(G^{}\) to an edge weighted graph \(_{N}\) of \( q_{m}\) nodes by splitting each node \(m\) of \(G^{}\) into \(q_{m}\) nodes \(\{n_{m}^{j}\}_{j=1}^{q_{m}}\) of weight \(1\), and defining the adjacency matrix \(_{N}\) with entries \(_{n_{m}^{j},n_{m^{}}^{j}}=a_{m,m^{}}\). Note that for any two choices \(N_{1},N_{2}\) in the above construction, \(_{}(W_{_{N_{1}}},W_{_{N_{2}}})=0\), where the infimum in the definition of \(_{}\) is realized on some permutation of intervals in \(\), as explained below. We hence define the induced graphon of the edge-node weighted graph \(G^{}\) by \(W_{G^{}}:=W_{_{N}}\) for some \(N\), where, in fact, each choice of \(N\) gives a representative of an equivalence class of piecewise constant graphons with zero \(_{}\) distance.

**Definition 2**.: _Let \(G^{}\) be an edge-node weighted graph with \(M\) nodes and node weights \(b=\{b_{m}=}{N^{}}\}_{m=1}^{M},\) satisfying \(\{q_{m}\}_{m}_{ 0}\) and \( q_{m}=N\), and let \(G\) be an edge weighted graph with nodes. We say that \(G^{}\)_ coarsens \(G\)_up to error \(\) if_

\[_{}(W_{G},W_{G^{}})<.\]

Suppose that \(G^{}\) coarsens \(G\). This implies the existence of an assignment of nodes of \(G\) to nodes of \(G^{}\). Namely, the supremum underlying the definition of \(_{}\) over the measure preserving bijections \(:\) is realized by a permutation \(:[N][N]\) applied on the intervals \(\{I_{N}^{i}\}_{i=1}^{N}\). With this permutation, the assignment \(C_{G G^{}}\) defined by \([N](n_{m}^{j}) m[M]\), for every \(m=1,,M\) and \(j=1,,q_{m}\), is called a _cluster assignment_ of \(G\) to \(G^{}\).

Let \(G^{}\), with \(M\) nodes, be a edge-node weighted graph that coarsens the simple graph \(G\) of \(N\) nodes. Let \(C_{G G^{}}\) be a cluster assignment of \(G\) to \(G^{}\). Let \(_{G^{}}\) be the automorphism group of \(G^{}\). Note that two nodes can belong to the same orbit of \(_{G^{}}\) only if they have the same node weight. Hence, for every two nodes \(m,m^{}\) in the same orbit of \(_{G^{}}\) (i.e., equivalent clusters), there exists a bijection \(_{m,m^{}}:C_{G G^{}}^{-1}\{m\} C_{G G^{}}^{-1} \{m^{}\}\). We choose a set of such bijections, such that for every \(m,m^{}\) on the same orbit of \(_{G^{}}\)

\[_{m,m^{}}=_{m^{},m}^{-1}.\]

We moreover choose \(_{m,m}\) as the identity mapping for each \(m=1,,M\). We identify each element \(g\) of \(_{G^{}}\) with an element \(\) in a permutation group \(}_{G^{}}\) of \([N]\), called the _blown-up symmetry group_, as follows. Given \(g_{G^{}}\), for every \(m[M]\) and every \(n C_{G G^{}}^{-1}\{m\}\), define

\[n_{m,gm}n.\]

**Definition 3**.: _Let \(G^{}\), with \(M\) nodes, be a edge-node weighted graph that coarsen the (simple or weighted) graph \(G\) of \(N\) nodes. Let \(C_{G G^{}}\) be a cluster assignment of \(G\) to \(G^{}\) with cluster sizes \(c_{1},,c_{M}\). Let \(_{G^{}}\) be the automorphism group of \(G^{}\), and \(}_{G^{}}\) be the blown-up symmetry group of \([N]\). For every \(m=1, M\), let \(_{c_{m}}\) be the symmetry group of \(C_{G G^{}}^{-1}\{m\}\). We call the group of permutations_

\[_{G G^{}}(_{c_{1}} _{c_{2}}_{c_{M}})}_{G^{}}_{N}\]

_the symmetry group of \(G\) induced by the coarsening \(G^{}\). We call any element of \(_{G G^{}}\) an approximate symmetry of \(G\)._

Specifically, every element \(s_{G G^{}}\) can be written in coordinates by

\[s=s_{1}s_{2} s_{M}a(s_{1},,s_{M},a),\]

for a unique choice of \(s_{j}_{c_{j}}\), \(j=1,,M\), and \(a}_{G^{}}\) (see Appendix B.2 for details).

In words, we consider the symmetry of the graph \(G\) of \(N\) nodes not by its own automorphism group, but via the symmetry of its coarsened graph \(G^{}\). The nodes in the same orbit in \(G\) are either in the same cluster of \(G^{}\) (i.e., from the same coarsened node), or in the equivalent clusters of \(G^{}\) (i.e., they belong to the same orbit of the coarsened graph and share the same cluster size). See Figure 1 (and Figure 9 in Appendix A.2) for examples.

**Definition 4**.: _Let \(G\) be a graph with \(N\) nodes, and \(=^{N d}\) be the space of graph signals. Let \(X\) where \(\) is a \(_{N}\)-invariant measure on \(^{N d}\). We call \(f^{*}:^{N d}^{N k}\) an approximately equivariant mapping if there exists a function \(:_{+}_{+}\) satisfying \(_{ 0}()=0\) (called the equivariance rate), such that for every \(>0\) and every edge-node weigted graph \(G^{}\) that coarsen \(G\) up to error \(\),_

\[\|f^{*}-f^{*}_{_{G G^{}}}\|_{}(),\]

_where \(f^{*}_{_{G G^{}}}=_{_{G G^{ }}}(f)\) is the intertwining projection of \(f^{*}\) with respect to \(_{G G^{}}\)._

**Example 4.1**.: Here we give an example of an approximately equivariant mapping in a natural setting. Suppose that \(G\) is a random geometric graph, namely, a graph that was sampled from a metric-probability space \(\) by randomly and independently sampling nodes \(\{x_{n}\}_{n=1}^{N}\). Hence, \(G\) can be viewed as a discretization of \(\), and the graphs that coarsen \(G\) can be seen as coarser discretizations of \(\). Therefore, the approximate symmetries are _local deformations_ of \(G\), namely, permutations that swap nodes if they are close in the metric of \(\). This now gives an interpretation for approximately equivariance mappings of geometric graphs: these are mappings that are stable to local deformations. In Appendix C.2 we develop this example in detail.

We are ready to present the bias-variance tradeoff in the setting with approximate symmetries. The proofs are deferred to Appendix B.2.

**Corollary 3** (Risk Gap via Graph Coarsening).: _Let \(=^{N d},=^{N k}\) be the input and output graph signal spaces on a fixed graph \(G\). Let \(X\) where \(\) is a \(_{N}\)-invariant distribution on \(\). Let \(Y=f^{*}(X)+\), where \(^{N k}\) is random, independent of \(X\) with zero mean and finite variance, and \(f^{*}:^{N d}^{N k}\) be an approximately equivariant mapping with equivariance rate \(\). Then, for any \(G^{}\) that coarsens \(G\) up to error \(\), for any \(f V\), we have_

Notably, Corollary 3 illustrates the tradeoff explicitly in the form of choosing the coarsened graph \(G^{}\): If we choose \(G^{}\) close to \(G\) such that the coarsening error \(\) and \(\|f^{}_{_{G G^{}}}\|_{}\) are small, then the mismatch term is close to zero; meanwhile the constraint term is also small since \(_{}\) is typically trivial. On the other hand, if we choose \(G^{}\) far from \(G\) that yields large coarsening error \(\), then the constraint term \(\|f^{}_{_{G G^{}}}\|_{}^{2}\) also increases.

**Corollary 4** (Bias-Variance-Tradeoff via Graph Coarsening).: _Consider the same linear regression setting in Theorem 2, except now \(f^{*}\) is an approximately equivariant mapping with equivariance rate \(\), and \(=_{G G^{}}\) is controlled by \(G^{}\) that coarsens \(G\) up to error \(\). Denote the canonical permutation representations of \(_{G G^{}}\) on \(,\) as \(^{},^{}\), respectively. Let \((_{^{}}_{^{}})=_{_{G G^{ }}}_{^{}}(g)(g)\) denote the inner product of the characters. If \(n>Nd+1\) the risk gap is bounded by_

\[[(f_{},f_{_{_{G G^{ }}}()})]-2()^{2} dk-(_{^{}}_{^{}})}{n-Nd-1}}+_ {}^{2}dk-(_{^{}}_{^{}})}{n-Nd-1}.\]

## 5 Experiments

We illustrate our theory in three real-world tasks for learning on a fixed graph: image inpainting, traffic flow prediction, and human pose estimation1. For image inpainting, we demonstrate the bias-variance tradeoff via graph coarsening using \(\)-Net; For the other two applications, we show that the tradeoff not only emerges from \(\)-Net that enforces strict equivariance, but also \(\)-Net augmented with symmetry-breaking modules, allowing us to _recover_ standard GNN architectures as a special case. Concretely, we consider the following variants (more details in Appendix D.4.2):

1. \(\)-Net with strict equivariance using equivariant linear map \(f_{}\).
2. \(\)-Net augmented with graph convolution \(Af_{}(x)\), denoted as \(\)-Net(gc).
3. \(\)-Net augmented with graph convolution and learnable edge weights: \(\)-Net(gc+ew).
4. \(\)-Net augmented with graph locality constraints \((A f_{})(x)\) and learnable edge weights, denoted as \(\)-Net(pt+ew).

\(\)-Net serves as a baseline to validate our generalization analysis for equivariant estimators (see Section 3). Yet in practice, we observe that augmenting \(\)-Net with symmetry breaking can further improve performance, thereby justifying the analysis for approximate symmetries (see Section 4). In particular, \(\)-Net(gc) and \(\)-Net(gc+ew) are motivated by graph convolutions used in standard GNNs; \(\)-Net(pt+ew) is inspired by the concept of locality in CNNs, where \(A f_{}\) effectively restricts the receptive field to the \(1\)-hop neighrborhood induced by the graph \(A\).

### Application: Image Inpainting

We consider a \(28 28\) grid graph as the fixed domain, with grey-scale images as the graph signals. The learning task is to reconstruct the original images given _masked_ images as inputs (i.e., image inpainting). We use subsets of MNIST  and FashionMNIST , each comprising 100 training samples and 1000 test samples. The input and output graph signals are \((m_{i} x_{i},x_{i})\), where \(x_{i}^{28 28}^{784}\) denotes the image signals and \(m_{i}\) denotes a random mask (size \(14 14\) for MNIST and \(20 20\) for FashionMNIST). We investigate the symmetry model selection problem by clustering the grid into \(M\) patches with size \(d d\), where \(d\{28,14,7,4,2,1\}\). Here \(d=28\) means one cluster (with \(_{N}\) symmetry); \(d=1\) is \(784\) singleton clusters with no symmetry (trivial).

We consider \(_{G G^{}}\)-equivariant networks \(\)-Net with ReLU nonlinearity. We parameterize the equivariant linear layer \(f:^{N}^{N}\) with respect to \(_{G G^{}}=(_{c_{1}}_{c_{M}})}_{G^{}}\) using the following block-matrix form (assuming the nodes are ordered by their cluster assignment), with \(f_{kl}\) denoting block matrices, and \(a_{k},b_{k},e_{kl}\) representing scalars:

\[f=f_{11}&&f_{1M}\\ &&f_{MM},\,f_{kk}=a_{k}I+b_{k}^{}, \,f_{kl}=e_{kl}^{}k l. \]

The coarsened graph symmetry \(_{G^{}}\) induces constraints on \(a_{k},b_{k},e_{kl}\). If \(_{G^{}}\) is trivial, then these scalars are unconstrained. In the experiment, we consider a reflection symmetry on the coarsened grid graph, i.e., \(_{G^{}}=_{2}\) which acts by reflecting the left (coarsened) patches to the right (coarsened) patches. Suppose the reflected patch pairs are ordered consecutively, then \(a_{k}=a_{k+1},b_{k}=b_{k+1}\) for \(k\{1,3,,M-1\}\), and \(e_{kl}=e_{k+1,l-1}\) for \(k\{1,3,,M-1\},l\{2,4,,M\}\) (see Figure inset for an illustration). In practice, we extend the parameterization to \(f:^{N d}^{N k}\). More details can be found in Appendix D.2.

Figure 2 shows the empirical risk first decreases and then increases as the group decreases, illustrating the bias-variance tradeoff from our theory. Figure 2 (left) compares a \(2\)-layer \(\)-Net with a \(1\)-layer linear \(\)-Net, demonstrating that the tradeoff occurs in both linear and nonlinear models. Figure 2 (right) shows that using reflection symmetry of the coarsened graph outperforms the trivial symmetry baseline, highlighting the utility of modeling coarsened graph symmetries.

### Application: Traffic Flow Prediction

The traffic flow prediction problem can be formulated as follows: Let \(X^{(t)}^{n d}\) represent the traffic graph signal (e.g., speed or volume) observed at time \(t\). The goal is to learn a function \(h()\) that maps \(T^{}\) historical traffic signals to \(T\) future graph signals, assuming the fixed graph domain \(G\):

\[[X^{(t-T^{}+1)},,X^{(t)};G][X^{(t+1)},,X^{(t+T)}].\]

The traffic graph is typically large and asymmetric. Therefore we leverage our approximate symmetries to study the symmetry model selection problem (Section 4). We use the METR-LA dataset which represents traffic volume of highways in Los Angeles (see figure inset). We use both the original graph \(G\) from , and its sparsified version \(G_{s}\) which is more faithful to the road geometry. In \(G_{s}\), the \((i,j)\)-edge exists if and only if nodes \(i,j\) lie on the same highway. We first validate our Definition 4 in such dataset (see Appendix D.3.1). In terms of the models, we consider a simplified version of DCRNN model proposed in , where the spatial module is modelled by a standard GNN, and the temporal module is modelled via an encoder-decoder architecture for sequence-to-sequence modelling. We follow the same temporal module and extending its GNN module using \(\)-Net(gc), which recovers DCRNN when choosing \(_{N}\). We then consider different choices of the coarsened graph \(G^{}\) (to induce approximate symmetries). As shown in Table 3, using \(2\) clusters as approximate symmetries yields better generalization error than using \(9\) clusters, or the full permutation group \(_{N}\).

Figure 2: Bias-variance tradeoff via graph coarsening. Left:2-layer \(\)-Net(blue) and \(1\)-layer linear \(\)-equivariant functions (orange), assuming the coarsened graph is asymmetric; Right: 2-layer \(\)-Net with both trivial and non-trivial coarsened graph symmetry. See Table 7 for more numerical details.

### Application: Human Pose Estimation

We consider the simple (loopy) graph \(G\) with adjacency matrix \(A\{0,1\}^{16 16}\) representing 16 human joints, illustrated on the right. The input graph signals \(X^{16 2}\) describe the joint 2D spatial location. The goal is to learn a map to reconstruct the 3D pose \(Y^{16 3}\). We use the standard benchmark dataset Human3.6M  and follow the evaluation protocol in . The generalization performance is measured by mean per joint position error (MPJPE) and MPJPE after alignment (P-MPJPE). We implement a \(4\)-Layer \(\)-Net(gc+ew), which recovers the same model architecture in SemGCN  when choosing \(=_{16}\) (further details are provided in Appendix D.4.1).

We consider the following symmetry groups: the permutation group \(_{16}\), the graph automorphism \(_{G}\), and the trivial symmetry. Note that the human skeleton graph has \(_{G}=(_{2})^{2}\) (corresponding to the arm flip and leg flip). Additionally, we investigate an approximate symmetry called Relax-\(_{16}\); This relaxes the global weight sharing in \(_{16}\) (that only learns \(2\) scalars per equivariant linear layer \(f_{_{16}}:^{N}^{N}\), where \(f_{_{16}}[i,i]=a,f_{_{16}}[i,j]=b\) for \(i j\)) to local weight sharing (that learns \(2 16=32\) scalars, where \(f_{_{16}}[i,i]=a_{i},f_{_{16 }}[i,j]=b_{i}\) for \(i j\)). Table 4 shows that the best performance is achieved at the hypothesis class induced from Relax-\(_{16}\), which is smaller than \(_{16}\) but differs from \(_{G}\). Furthermore, using \(\)-Net(gc+ew) with Relax-\(_{16}\) gives a substantial improvement over \(_{16}\) (representing SemGCN in ). This demonstrates the utility of enforcing active symmetries in GNNs that results in more expressive models. More details and additional ablation studies can be found in Appendix D.4.

## 6 Discussion

In this paper, we focus on learning tasks where the graph is fixed, and the dataset consists of different signals on the graph. We developed an approach for designing GNNs based on active symmetries and approximate symmetries induced by the symmetries of the graph and its coarse-grained versions. A layer of an approximately equivariant graph network uses a linear map that is equivariant to the chosen symmetry group; the graph is not used as an input but rather induces a hypothesis class. In practice, we further break the symmetry by incorporating the graph in the model computation, thus combining symmetric and asymmetric components.

We theoretically show a bias-variance tradeoff between the loss of expressivity due to imposing symmetries, and the gain in regularity, for settings where the target map is assumed to be (approximately) equivariant. For simplicity, the theoretical analysis focuses on the equivariant models without symmetry breaking; Theoretically analyzing the combination of symmetric and asymmetric components in machine learning models is an interesting open problem. The bias-variance formula is computed only for a simple linear regression model with white noise and in the underparamterized setting; Extending it to more realistic models and overparameterized settings is a promising direction.

As a proof of concept, our approximately equivariant graph networks only consider symmetries of the fixed graph. An interesting future direction is to extend our approach to also account for (approximate) symmetries in node features and labels, using suitably generalized cut distance (e.g., graphon-signal cut distance in ; see Appendix C.2 for an overview). Our network architecture consists only of linear layers and pointwise nonlinearity. Another promising direction is to incorporate pooling layers (e.g., [80; 22]) and investigate them through the lens of approximate symmetries. Finally, extending our analysis to general groups is a natural next step. Our techniques are based on compact groups with orthogonal representation. While we believe that the orthogonality requirement can be lifted straightforwardly, relaxing the compactness requirement appears to be more challenging (see related discussion in ).

   \(\)-Net(gc) & \(_{N}\) & \(_{i_{1}}_{n_{2}}\) & \(_{i_{2}}_{n_{N}}\) \\  Graph \(G\) & \(3.173 0.013\) & \(\) & \(3.204 0.006\) \\ Graph \(G\) & \(3.106 0.013\) & \(\) & \(3.174 0.013\) \\   

Table 3: Traffic forecasting with different choices of graph clustering. Table shows Mean Absolute Error (MAE) across \(3\) runs.