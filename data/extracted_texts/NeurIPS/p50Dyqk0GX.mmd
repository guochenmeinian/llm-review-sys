# Dual Risk Minimization: Towards Next-Level Robustness in Fine-tuning Zero-Shot Models

Kaican Li\({}^{1}\)1 Weiyan Xie\({}^{1}\)1 Yongxiang Huang\({}^{2}\) Didan Deng\({}^{2}\) Lanqing Hong\({}^{2}\)

Zhenguo Li\({}^{1,2}\) Ricardo Silva\({}^{3}\) Nevin L. Zhang\({}^{1}\)2

\({}^{1}\)The Hong Kong University of Science and Technology

\({}^{2}\)Huawei \({}^{3}\)University College London

###### Abstract

Fine-tuning foundation models often compromises their robustness to distribution shifts. To remedy this, most robust fine-tuning methods aim to preserve the pre-trained features. However, not all pre-trained features are robust and those methods are largely indifferent to which ones to preserve. We propose dual risk minimization (DRM), which combines empirical risk minimization with worst-case risk minimization, to better preserve the core features of downstream tasks. In particular, we utilize core-feature descriptions generated by LLMs to induce core-based zero-shot predictions which then serve as proxies to estimate the worst-case risk. DRM balances two crucial aspects of model robustness: expected performance and worst-case performance, establishing a new state of the art on various real-world benchmarks. DRM significantly improves the out-of-distribution performance of CLIP ViT-L/14@336 on ImageNet (75.9\(\)77.1), WILDS-iWildCam (47.1\(\)51.8), and WILDS-FMoW (50.7\(\)53.1); opening up new avenues for robust fine-tuning. Our code is available at https://github.com/vaynexie/DRM.

## 1 Introduction

Foundation models such as CLIP (Radford et al., 2021) and ALIGN (Jia et al., 2021) have revolutionized machine learning with their remarkable zero-shot and adaptive capabilities. Research has shown that such capabilities are mainly due to robust feature representations gained from large-scale training data (Fang et al., 2022; Xu et al., 2024). The models have been proven useful in various downstream tasks (Shen et al., 2022; Zhang et al., 2022; Betker et al., 2023; Pi et al., 2024) and are the cornerstones of large multimodal models (Alayrac et al., 2022; Liu et al., 2023; Zhu et al., 2024).

_Fine-tuning_ is one of the most common approaches to the downstream adaptation of foundation models (Bommasani et al., 2021; Shen et al., 2022). However, such adaptation often comes at the cost of robustness (Radford et al., 2021; Pham et al., 2023), resulting in larger gaps between downstream in-distribution (ID) and out-of-distribution (OOD) performance (Wortsman et al., 2022).

Kumar et al. (2022) showed that fine-tuning tends to distort pre-trained features, and the distortion is exacerbated by randomly initialized heads which would significantly alter the pre-trained features to fit ID examples. The proposed remedy, LP-FT, first learns a linear probe (LP) on frozen features, and then followed by regular fine-tuning (FT). Goyal et al. (2023) took this idea further by reusing the pre-trained text encoder of CLIP as the classification head for fine-tuning. This method improves LP-FT and is colloquially known as "fine-tune like you pre-train" (FLYP). WiSE-FT (Wortsman et al., 2022) investigated combining pre-trained models with their fine-tuned versions by weight averaging, which can be seen as yet another approach to recover robust features lost during fine-tuning.

While the existing approaches aim to preserve pre-trained features, the fine-tuning process is still guided by empirical risk minimization (ERM; Vapnik, 1998), which favors the most predictive but not necessarily the most robust features. In general, there are two kinds of robust features: _core features_ which essentially define the target classes, and _non-core features_ that may aid prediction when the core features are not _clear_(Gao et al., 2023). ERM models tend to exploit the non-core features even when the core features are clear (Geirhos et al., 2020; Shah et al., 2020). This often harms OOD performance as non-core features are generally less reliable out-of-distribution.

To better preserve the core features, we propose a new principle called _dual risk minimization_ (DRM) which combines ERM with worst-case risk minimization (WRM; Wald, 1945), a common principle for domain generalization (Arjovsky et al., 2019; Sagawa et al., 2020; Cha et al., 2021; Kirichenko et al., 2023). This combination rests on our view that robustness involves _two_ main aspects: the _expected_ (or average) performance and the _worst-case_ performance over all domains. While there is often a trade-off between these two aspects (Tsipras et al., 2019; Teney et al., 2023), Figure 1 illustrates how DRM balances the trade-off to improve overall robustness.

The main challenge of applying DRM to real-world tasks is to assess of worst-case risk. To this end, we use _concept descriptions_(Pratt et al., 2023)--short texts that describe the core features of each class--obtained with GPT-4 (Achiam et al., 2023). The description for _cougar_, for instance, is "_a large, lawny cat with a muscular build and a small head._" We feed these descriptions to a pre-trained CLIP text encoder (Radford et al., 2021) for the text embeddings, which are then used to construct soft class labels for each training image according to the similarity scores between the image and text embeddings. The risk w.r.t. the soft labels can be seen as a proxy of the worst-case risk and is thus minimized instead. Empirically, DRM significantly outperforms the state of the art on challenging benchmarks such as ImageNet (Deng et al., 2009) and WILDS (Koh et al., 2021).

In summary, we make the following key contributions in this paper:

* We propose _dual risk minimization_ (DRM), a novel approach that combines ERM and WRM to improve downstream robustness of zero-shot foundation models while addressing the intractability of WRM through innovative use of concept descriptions.
* We highlight that robustness for many real-world problems concerns both _expected_ and _worst-case_ performance while most previous works focus on only one. We then show that DRM offers a simple and effective way to balance these two important aspects of robustness.
* We establish a strong new state of the art on multiple real-world benchmarks, promising next-level robustness in fine-tuning zero-shot models. On CLIP ViT-L/14@336, DRM achieves a significant, over 5% relative improvement in OOD performance over the best baseline method.

Figure 1: **Dual risk minimization (DRM) combines empirical risk minimization (ERM) and worst-case risk minimization (WRM) to complement their weaknesses. In this simple binary classification task predicting if there are skis in a given image, (i) ERM underperforms when the core features (the appearance of ski) are _clear_ but the non-core features such as background/context are _spurious_ (i.e. negatively correlated with ski), and (ii) WRM underperforms when the core features are _unclear_ but the non-core features are _robust_ (i.e. positively correlated with ski). DRM outperforms ERM and WRM under mild conditions such that the core features are not always clear and the non-core features are more often robust than not.**

Related work

Robust fine-tuning of pre-trained models.Prior to the work of Kumar et al. (2022); Wortsman et al. (2022); Goyal et al. (2023) which we have introduced, Li et al. (2018) proposed to restrict the \(L^{2}\) distance between the parameters of pre-trained and fine-tuned models via regularization. Some other work explored updating only a small number of (pre-trained/add-on) parameters (Guo et al., 2019; Zhang et al., 2020; Gao et al., 2024b). Similar ideas (Kirkpatrick et al., 2017; Zenke et al., 2017) were also discussed in continual learning to mitigate catastrophic forgetting (McCloskey and Cohen, 1989). Apart from explicit constraints on model parameters, Ge and Yu (2017) turned to the source of robust features and proposed to incorporate a subset of pre-trained data for fine-tuning, while Cha et al. (2022) aimed to enhance the mutual information between pre-trained and fine-tuned features. Jiang et al. (2019); Zhu et al. (2020) added smoothness constraints on model predictions for adversarial examples (Szegedy et al., 2013) to help retain robust features. Andreassen et al. (2021) showed that OOD accuracy tends to improve initially but then plateaus as the fine-tuning proceeds. For more discussion on related work including concurrent ones, see Appendix B.

Worst-case risk minimization.The study of worst-case risk minimization (WRM) dates back to the work of Wald (1945), which has gradually evolved into what we know as robust optimization today (Ben-Tal et al., 2009). More recently, WRM has been considered (by many) a basic principle for domain generalization (DG; Blanchard et al., 2011; Muandet et al., 2013). A notable example is invariant risk minimization (IRM; Arjovsky et al., 2019), which aims to learn core-feature representations from multi-domain data. Such representations, under mild causal assumptions, give rise to classifiers that minimize the worst risk (Peters et al., 2016). Another key method, GroupDRO (Sagawa et al., 2020), imposes higher penalties on domains with higher empirical risks. Unlike DRM, neither IRM nor GroupDRO formulates WRM as an explicit optimization constraint for ERM. Eastwood et al. (2022) pointed out that sacrificing too much average performance for worst-case performance is not ideal for DG. Hence, they proposed to minimize the risk among the most likely domains. Lastly, our setup is partly similar to Alabdulmohsin et al. (2023) which also relies on external information.

Prompt design for zero-shot classification.To better leverage the capability of zero-shot models, various prompt designs have been proposed. Menon and Vondrick (2022); Pratt et al. (2023); Maniparambil et al. (2023) mainly explored prompts for zero-shot classification. Their prompts were generated by LLMs (Radford et al., 2019) with slightly different instructions than ours, not explicitly focusing on core features. For example, Pratt et al. (2023) used "_Describe an image from the internet of a(n)..._", which may inadvertently introduce descriptions of non-core features in the resulting prompts. The prompts considered by Yang et al. (2023); Yan et al. (2023) are closer to ours in this respect, where they used LLM-generated concept descriptions to build concept bottleneck models for interpretable image classification. More recently, Mao et al. (2024) proposed to use context-aware prompts such as "_a [context] of [class name]_," while Cheng et al. (2024) used both domain-invariant and domain-specific prompts generated by LLMs. However, both methods require either image context or specific domain information to generate the prompts.

## 3 Dual risk minimization

Data model.Let \(X\) and \(Y\) be the input and _ground-truth_ target variables for which we adopt the following data generation model:

\[ X& h_{X}(X_{},X_{ },),\\ Y& h_{Y}(X_{});\] (1)

where \((X_{},X_{})\) are latent variables and \(\) is exogenous noise. We call \(X_{}\)_core features_ and \(X_{}\)_non-core features_ of \((X,Y)\). \(X_{}\) and \(Y\) may be correlated due to hidden confounders of \((X_{},X_{})\) and direct causal mechanisms between \((X_{},X_{})\). Following Peters et al. (2016), we assume the causal mechanisms and the distribution of \(\) are invariant across domains. There are no other hidden variables or mechanisms. Similar models were widely adopted in the literature (Tenenbaum and Freeman, 1996; Mahajan et al., 2021; Mitrovic et al., 2021; Ahuja et al., 2021; Liu et al., 2021; Lv et al., 2022; Ye et al., 2022; Zhang et al., 2023; Gao et al., 2024a) where \(X_{}\) and \(X_{}\) are sometimes referred to as 'content' and'style'. We use calligraphic letters such as \(\) and \(\) to denote the set of possible outcomes of the random variables.

Ideal objective for robustness.Let \(\) be all possible domains of a task, and \(\) be some natural distribution over \(\). By definition, \((d)>0\) for all \(d\). Every domain \(d\) is associated with a data distribution \(p_{d}(x,y,x_{c},x_{})\) consistent with (1). Let \(_{}(y|x)\) be a prediction model parameterized by \(\). Its risk in terms of negative log-likelihood,

\[R_{d}()=_{(x,y) p_{d}}[-_{}(y|x)],\] (2)

can be seen as a measure of its performance in domain \(d\). Let \(d_{}\) be the training domain. For simplicity, we will omit \(d\) when it is clear from the context, e.g., \(R_{d_{}}()\) will be written as \(R_{}()\).

For real-world applications, we argue that a _robust_ model should optimize its _expected_ performance over \(\) while maintaining acceptable _worst-case_ performance across \(\). The expected performance implies how well the model would perform at the most general population level, while the worst-case performance tells us the model's performance in the worst scenario one may encounter. We note that Eastwood et al. (2022) and Zhang et al. (2023b) share a similar view with us on robustness.

We formalize the above intuition as the following constrained optimization problem, namely _idealized dual risk minimization_ (IDRM), which aims to minimize the empirical risk of \(_{}(y|x)\) while ensuring its worst-case risk is below some threshold value \(\):

\[_{}R_{}()_{ d}R_{d}().\] (IDRM)

IDRM generalizes ERM (Vapnik, 1998) and WRM (Wald, 1945) as it reduces to ERM when \(\) is large and to WRM when \(\) is small. IDRM also bears some resemblance to IRM (Arjovsky et al., 2019), which involves an implicit WRM constraint. The constraint, however, requires the classification head to be _optimal_ in all training domains and thus may be too demanding in practice. Another closely related work, GroupDRO (Sagawa et al., 2020), proposes to minimize the worst training-domain risk--a more empirical flavor of WRM. Both IRM and GroupDRO rely on ideally grouped training data to capture invariance across domains. In Section 4, we will show that this is largely unnecessary for zero-shot models and provide a practical solution for IDRM with just _single-domain_ data.

From IDRM to DRM.IDRM can be solved as the following unconstrained optimization problem due to strong duality (proof in Appendix A).

**Theorem 1**.: _Strong duality holds between IDRM and the following dual problem:_

\[_{^{} 0}_{}\;[R_{}( )+^{}_{d}R_{d}()]-^{ }.\] (3)

Let \(^{}\) be any solution of \(^{}\) to (3). By the strong duality, IDRM then reduces to \(_{}[R_{}()+^{}_{d}R_{d}()]\). The worst-case risk, i.e., \(_{d}R_{d}()\), is still intractable in itself, but it is closely related to the degree to which the model \(_{}(y|x)\) relies on core features to predict \(y\). This is because for a diverse set of domains \(\), leveraging non-core features would always lead to worse performance in certain domains (Arjovsky et al., 2019; Geirhos et al., 2020). To minimize the worst-case risk, therefore, the model must only utilize the core features to make prediction.

Suppose there is an oracle feature extractor \(f_{}\) that returns a faithful representation of the core features of any input \(x\). As the core features may not always be clear, \(f_{}(x)\) can be viewed as some distribution over the core features for each \(x\). Let \(p_{}(y|x)\) be the optimal model that can be built upon \(f_{}(x)\). The risk of \(_{}(y|x)\) w.r.t. \(p_{}(y|x)\) on the training domain \(d_{}\) is given by

\[R_{}^{c}()=_{x p_{}}_{y p_{ }(y|x)}[-_{}(y|x)].\] (4)

Assuming \(p_{}(x)\) is fairly diverse, the risk \(R_{}^{c}()\) measures the degree to which the model's prediction is based on the core features and thus can be viewed as a proxy for the worst-case risk. Hence, we can replace \(_{d}R_{d}()\) with \(R_{}^{c}()\) while still achieving a similar optimization effect.

In summary, we relax IDRM to the following DRM formulation:

\[_{}R_{}()+ R_{}^{c}()\] (DRM)

with some properly chosen \( 0\). Now the risk \(R_{}^{c}()\) can also be interpreted as a regularization term for ERM to help preserve the core features. In the following section, we demonstrate how to utilize zero-shot models like CLIP models (Radford et al., 2021) to estimate \(p_{}(y|x)\), and subsequently how to apply DRM to robustly fine-tune the same CLIP models.

## 4 Fine-tuning zero-shot models with DRM

Zero-shot models like CLIP typically consist of an image encoder \(f_{}\) and a text encoder \(g_{}\) with parameters \(=(,)\). Image classification with such models is usually done by first creating a text prompt \(t_{y}\) for each class label \(y\), and then assigning a probability for each \(y\) to an image \(x\) by

\[_{}(y|x)=(x,t_{y})/)}{_{y^{} }(A_{}(x,t_{y^{}})/)},\] (5)

where \(A_{}(x,t_{y})= f_{}(x),g_{}(t_{y})\) and \(\) is the temperature. The inner product \( f_{}(x),g_{}(t_{y})\) can be intuitively understood as the _affinity_ between \(x\) and \(t_{y}\), and we thus denote it as \(A_{}(x,t_{y})\).

The classifier (5) was originally introduced by Radford et al. (2021) for zero-shot classification. We follow Goyal et al. (2023) to directly fine-tune this classifier, viewing the text embeddings \(g_{}(t_{y})\) as the weights of a standard linear classification head for the image embeddings \(f_{}(x)\). Unlike standard classifiers, however, (5) additionally depends on the text prompt \(t_{y}\) of which the design is important.

### Dual prompts for fine-tuning zero-shot models

Let \(=\{t_{y}\,|\,y\}\) be the set of text prompts used to construct the classifier \(p_{}(y|x)\) according to Eq. (5). For such zero-shot classifiers, the general DRM objective (DRM) becomes

\[_{}R_{}(;)+ R_{}^{}(;),\] (6)

where the risks \(R_{}\) and \(R_{}^{}\) not only depend on the model parameters \(\) but also on the prompts \(\). For \(R_{}\), usually a set of _default prompts_, \(^{}=\{t_{y}^{}\,|\,y\}\), like "_an image of [class name]_" is used (Goyal et al., 2023; Oh et al., 2023). However, such prompts may not be suitable for \(R_{}^{}\) as they are not specifically designed to bind with core features. In fact, as we will show, default prompts elicit representation that is biased towards non-core features. It is also know that non-visual and spurious descriptions contribute significantly to CLIP's representation (Esfandirapoor et al., 2024).

To generate better prompts for \(R_{}^{}\), we ask GPT-4 (Achiham et al., 2023) to describe the _core visual features_ of each class, producing a set of _concept descriptions_, \(^{}=\{t_{y}^{}\,|\,y\}\). For example, to generate a concept description for _cougar_, we prompt GPT-4 with

"_Q: Generate a short sentence that describes the visual features of Cougar. Do not include its function, its surroundings, or the environment it usually inhabits. The sentence should be concise. For example, [goldfish: a long, golden body with back fins]_."

and the concept description returned is

"_a large, tawny cat with a muscular build and a small head._" 3

Figure 2: **Concept descriptions better capture core features than default prompts. The affinities between images and default prompts (df) are not stable w.r.t. changes in image background (BG) containing non-core features and are insensitive to changes in image foreground (FG) containing core features, as indicated by the relative changes (gray numbers in parentheses) w.r.t. the affinities of the original images. In contrast, the affinities between images and concept descriptions (cd) are stable w.r.t. to changes in BG while being highly responsive to changes in FG, making them a good detector for core features. See Appendix D.1 for more examples and a full quantitative study on this.**
Figureatively, the text embedding \(g_{}(t_{y}^{})\) of the concept description represents the core features of the class from the text side. We use it to "pull out" the core features from the image embedding \(f_{}(x)\) via inner-product. As illustrated in Figure 2, the affinity between an image and its concept description is indeed a much better measure of the significance of core visual features. Regarding this point, a full quantitative study can be found in Appendix D.1.

Together, the two sets of prompts naturally give rise to the following objective:

\[_{}R_{}(;^{})+ R _{}^{}(;^{}),\] (7)

where we use default prompts \(^{}\) for ERM and concept descriptions \(^{}\) for WRM. We do not use concept descriptions for ERM because it would erode the text-side core feature representation elicited by concept descriptions. This is supported by empirical evidence from our ablation study (Section 5.3) showing that (7) works best among various alternatives.

The dual prompts elicit separate predictions from the same model for the two sub-objectives of DRM. The ERM part, \(R_{}(;^{})\), is supervised by regular one-hot labels. The WRM part, \(R_{}^{}(;^{})\), is supervised by \(p_{}(y|x)\) as in (4). Next, we show how the same set of concept descriptions \(^{}\) can be used to obtain a good estimate of \(p_{}(y|x)\).

### Estimating \(p_{}(y|x)\) with concept descriptions

Recall that the oracle model \(p_{}(y|x)\) is based on a faithful representation of core features. Since we have demonstrated that concept descriptions bind well with core features on pre-trained CLIP models, a direct estimate for \(p_{}(y|x)\) can be obtained via (5) with \(t_{y} t_{y}^{}\) and \(_{0}\) where \(_{0}=(_{0},_{0})\) denote the pre-trained CLIP parameters.

However, there is a crucial caveat. To illustrate, consider an image \(x\) of class \(y\). For another class \(y^{}\) whose core features are _not_ present in \(x\), the affinity \(A_{_{0}}(x,t_{y}^{})\) should ideally be very small. In practice, however, we find this is seldom the case. These extraneous affinity values, which we call _artifact terms_, often vary among classes and lead to poor estimates of \(p_{}(y|x)\) with high entropy.

To mitigate the impact of artifact terms, we perform a simple min-max normalization on \((x,y)=(A_{_{0}}(x,t_{y}^{})/)\) w.r.t. all training images \(_{y}\) labeled the same class \(y\), as follows:

\[(x,y)=_{y}}(x^{}, y)}{_{x^{}_{y}}(x^{},y)-_{x^{} _{y}}(x^{},y)}.\] (8)

This effectively adjusts the affinity range of each class, reducing the difference in the artifact terms of different classes. Based on the normalization, the final estimation we propose for \(p_{}(y|x)\) is

\[_{}(y|x)=(x,y),&y=y_{x};\\ [1-(x,y_{x})] y _{x}}(x,y^{})},&y y_{x};\] (9)

where \(y_{x}\) is the ground-truth label of \(x\). This ensures that for every class \(y\), there exists at least one \(x_{y}\) for which \(_{}(y|x)=1\), promoting balanced learning. When \(_{}(y|x)<1\) for \(y=y_{x}\), the remaining probability is distributed to other classes \(y y_{x}\) according to the relative scale of the respective affinities.

We pre-compute the estimate \(_{}(y|x)\) with the pre-trained CLIP model \(_{0}\) before fine-tuning, which is now the learning target of \(R_{}^{}(;^{})\). Since the computation requires the class labels, \(_{}(y|x)\) can only be used for training (not for inference). Intuitively, learning from \(_{}(y|x)\) can be seen as a form of self-distillation targeted at core features. In comparison, standard self-distillation methods (Furlanello et al., 2018; Zhang et al., 2019; Ji et al., 2021; Zhang et al., 2021) are largely indifferent to what specific information should be distilled.

### Inference

The fine-tuning objective (7) involves two classifiers: the ERM classifier \(_{}^{}(y|x)\) induced by \(^{}\), and the WRM classifier \(_{}^{}(y|x)\) induced by \(^{}\). While either alone can be used for inference, we find that their mixture,

\[_{}^{}(y|x)=_{}^{}(y| x)+(1-)_{}^{}(y|x),\] (10)

where \((0,1)\), performs the best. This is expected as (10) essentially combines ERM with WRM as depicted in Figure 1. By default, we set \(=1/(1+)\) so to be as consistent with (7) as possible.

Experiments

In this section, we evaluate DRM on multiple real-world benchmarks and conduct ablation studies to assess the impacts of various design choices. We conduct our experiments on three varying sizes of pre-trained CLIP models: ViT-B/16, ViT-L/14 and ViT-L/14@336 (Radford et al., 2021). Finally, we analyze the reliability of LLM-generated concept descriptions and the impact of \(\) on performance.

### Setup

Datasets.ImageNet(Deng et al., 2009) comprises over a million natural images across 1,000 classes. We use the training set for fine-tuning and the validation set for assessing ID accuracy. For OOD evaluation, we consider ImageNet variants: ImageNet-V2 (Recht et al., 2019), ImageNet-R (Hendrycks et al., 2021), ImageNet-Sketch(Wang et al., 2019), ImageNet-A (Hendrycks et al., 2021), and ObjectNet(Barbu et al., 2019). We report accuracy for both ID/OOD performance.

WILDS-IWildCam(iWildCam)(Koh et al., 2021) contains camera-trap images for wildlife classification, with training images from 200 locations and OOD images from different locations. Both ID and OOD performances are measured using macro F1 scores.

WILDS-FMoW (FMoW) (Koh et al., 2021) is a dataset of satellite images from different years and continents for land use prediction. The dataset is split into training, validation, and testing domains based on the year of collection. There is also a notable shift between different continents. We report the ID testing accuracy and the worst-region OOD testing accuracy.

Dollar Street-DA and GeoYFCC-DA (Prabhu et al., 2022) are datasets for testing model generalization from images in specific countries to new ones. For Dollar Street-DA, training images are from North America and Europe, with testing images from other continents. GeoYFCC-DA has a similar setup. Model effectiveness is measured by accuracy in seen and unseen countries.

Baseline methods.The key baseline we compare our DRM method with is **FLYP**(Goyal et al., 2023). Following the FLYP paper, we include several baselines that do not utilize the text encoder. These methods are **LP** (linear probing), **FT** (fine-tuning), **L2-SP**(Li et al., 2018), and **LP-FT**(Kumar et al., 2022). In addition, we incorporate some more recent fine-tuning methods for zero-shot vision models. We also consider combining the weight-space averaging method, **WISE-FT**(Wortsman et al., 2022), with DRM and the baselines. For more introduction to these methods, see Appendix B.

Implementation details.We update both the image encoder and text encoder during fine-tuning, following FLYP (Goyal et al., 2023). Furthermore, FLYP uses the CLIP contrastive loss (Radford et al., 2021) instead of the standard cross-entropy loss. We adopt this approach for the ERM part of DRM to facilitate comparison. In short, when the hyperparameter \(=0\) in the DRM objective (7), the WRM loss term vanishes and DRM reduces to exactly FLYP.

We choose all hyperparameters of DRM and baseline methods based on the performance on the ID validation set, i.e., training-domain validation (Gulrajani and Lopez-Paz, 2021). The hyperparameter \(\) of DRM is picked from \(\{1,2,3,4,5\}\). More implementation details are presented in Appendix F.

### Main results

We report the main results on ImageNet, iWildCam, and FMoW in Table 1 and 2. The results on Dollar Street-DA and GeoYFCC-DA can be found in Appendix E.2. We also compare DRM with some concurrent methods in Appendix E.3. All performance statistics, except some reported by previous papers (which we simply reuse), are averaged over 5 runs with different random seeds. The 95% confidence intervals over the 5 runs are reported.

Table 1 shows the results on CLIP ViT-B/16, the smallest of the three CLIP models. DRM achieves consistently better OOD performance than the baselines across all datasets, with and without WiSE-FT. Without WiSE-FT, DRM attains 5.0%, 12.4%, and 11.1% relative improvements over the best baseline method, FLYP, on the three benchmarks respectively. With WiSE-FT, the improvements remain significant at 1.9%, 11.6%, and 9.8% respectively. In terms of ID performance, DRM is roughly on par with FLYP, with a notable advantage on iWildCam.

Table 2 compares the performance of DRM and FLYP on two larger CLIP models. DRM again consistently outperforms FLYP in all cases. The previous state-of-the-art OOD performance for iWildCam and FMoW are 47.1 and 50.6 respectively, both achieved by FLYP+WiSE-FT with CLIP ViT-L/14@336. DRM improves those scores by 10.0% and 5.0% to 51.8 and 53.1 respectively.

Although DRM incurs more computational costs compared to FLYP due to an additional pass through the text encoder, the training and inference costs for DRM only increase by about 20% from FLYP (see Appendix F.3). This cost is insignificant relative to the performance gain.

Overall, our experiments suggest that DRM is highly effective in the robust fine-tuning of zero-shot models, outperforming previous methods by a large margin while maintaining scalability. As a bonus, we show in Appendix E.6 that DRM is also effective in fine-tuning ImageNet pre-trained CNNs.

### Ablation study

We conduct our ablation study with CLIP ViT-L/14 on iWildCam. The main results are reported in Table 3 and discussed below. For additional results and details, see Appendix E.4 and E.5.

Impact of dual risks.The DRM objective (7) consists of an ERM term, \(R_{}(;^{})\), and a WRM term, \(R_{}^{}(;^{})\). From Table 3, we can see that, in terms of OOD performance, models fine-tuned with only the ERM term (Rows 4 & 5) significantly underperform models fine-tuned with both terms (Rows 1-3). Conversely, models fine-tuned with only the WRM term (Rows 6 & 7) have much worse ID performance. Although the OOD performance is improved, there is still a large gap from the DRM model (Row 1). Note that these hold regardless of the type of text prompts used by ERM/WRM.

Impact of dual prompts for fine-tuning.With both ERM and WRM in effect, we further investigate the impact of their prompts during fine-tuning. DRM uses two sets of prompts: default prompts \(^{}\) for ERM, and concept descriptions \(^{}\) for WRM. Rows 8 & 9 of Table 3 show the performance of models fine-tuned using the same set of prompts for ERM and WRM. In either case, the model underperforms DRM (Row 1), validating the use of tailored prompts for specific learning targets.

    &  &  &  \\   &  &  &  &  &  &  \\  Method & ID & OOD & ID & OOD & ID & OOD & ID & OOD & ID & OOD \\ 
0-shot & 68.3\(\)0.50 & 87.2\(\)0.0 & - & 8.7\(\)0.1 & 11.0\(\)0.0 & - & 20.4\(\)0.0 & 18.7\(\)0.0 & - & - \\ LP & 79.9\(\)0.50 & 57.2\(\)0.0 & 80.0\(\)0.0 & 45.6\(\)0.1 & 31.6\(\)0.4 & 45.5\(\)0.6 & 31.7\(\)0.4 & 48.2\(\)0.1 & 30.5\(\)0.3 & 48.7\(\)0.1 & 31.5\(\)0.3 \\ FT & 81.4\(\)0.1 & 54.8\(\)0.1 & 82.5\(\)0.1 & 61.3\(\)0.1 & 48.1\(\)0.5 & 35.0\(\)0.5 & 48.1\(\)0.5 & 35.0\(\)0.5 & 63.5\(\)0.1 & 39.2\(\)0.7 & 68.5\(\)0.1 & 41.5\(\)0.3 \\ L2-SP & 81.6\(\)0.1 & 57.9\(\)0.1 & 82.2\(\)0.1 & 58.9\(\)0.1 & 48.6\(\)0.4 & 35.3\(\)0.3 & 48.6\(\)0.4 & 35.3\(\)0.3 & 68.6\(\)0.1 & 39.4\(\)0.6 & 68.4\(\)0.1 & 40.3\(\)0.6 \\ LP-PT & 81.8\(\)0.1 & 60.5\(\)0.1 & 82.1\(\)0.1 & 61.8\(\)0.1 & 49.7\(\)0.5 & 34.7\(\)0.0 & 50.2\(\)0.5 & 35.7\(\)0.4 & 68.4\(\)0.2 & 40.4\(\)0.1 & 68.5\(\)0.2 & 42.4\(\)0.7 \\ FLYP & **82.6\(\)0.0** & 60.2\(\)0.2 & **82.9\(\)0.6** & 63.2\(\)0.1 & 52.2\(\)0.6 & 35.6\(\)1.2 & 52.5\(\)0.6 & 37.1\(\)1.2 & 68.6\(\)0.2 & 41.3\(\)0.8 & **68.9\(\)3.3** & 42.0\(\)0.9 \\  DRM & 82.0\(\)0.3 & **63.2\(\)0.2** & 82.4\(\)0.2 & **64.0\(\)0.2** & **54.1\(\)0.5** & **40.0\(\)0.6** & **55.3\(\)0.4** & **41.4\(\)0.7** & **68.7\(\)0.3** & **45.9\(\)1.4** & 68.7\(\)0.2 & **46.1\(\)0.8** \\   

Table 1: ID and OOD performances of DRM and baselines methods on CLIP ViT-B/16, with and without WiSE-FT. Best performances are highlighted in **bold**. For ImageNet, we report the average performance over its 5 OOD test sets. Results on individual test sets are provided in Appendix E.1.

    & &  &  &  \\  Pre-trained model & Method & ID & OOD & ID & OOD & ID & OOD \\   & FLYP & 84.6\(\)0.3 & 73.4\(\)0.1 & 56.0\(\)1.1 & 41.9\(\)0.7 & 71.2\(\)0.5 & 48.2\(\)0.5 \\  & FLYP+WiSE-FT & 85.1\(\)0.2 & 75.1\(\)0.1 & 57.2\(\)0.7 & 42.1\(\)0.5 & **72.0\(\)**0.44 & 49.1\(\)0.6 \\   & DRM & 85.0\(\)0.2 & 75.5\(\)0.2 & **61.8\(\)**0.5 & 49.2\(\)0.4 & 70.9\(\)0.8 & **51.3\(\)**0.7 \\  & DRM+WiSE-FT & **86.2\(\)**0.1 & **76.2\(\)**0.2 & 61.6\(\)0.3 & **49.8\(\)**0.4 & 71.4\(\)0.5 & **51.3\(\)**0.7 \\   & FLYP & 85.4\(\)0.2 & 75.0\(\)0.0 & 58.7\(\)0.6 & 45.4\(\)0.1 & 72.5\(\)0.3 & 50.5\(\)0.5 \\  & FLYP+WiSE-FT & 86.1\(\)0.2 & 75.9\(\)0.2 & 60.5\(\)0.5 & 47.1\(\)1.2 & 72.6\(\)0.3 & 50.7\(\)0.6 \\    & DRM & 85.9\(\)0.1 & 76.0\(\)0.2 & **62.8\(\)**0.6 & 51.4\(\)0.5 & **73.8\(\)**0.5 & 52.5\(\)0.9 \\    & DRM+WiSE-FT & **87.4\(\)**0.0 & **77.1\(\)**0.2 & 62.5\(\)0.4 & **51.8\(\)**0.5 & **73.8\(\)**0.3 & **53.1\(\)**0.6 \\   

Table 2: ID and OOD performances of DRM and FLYP on two larger CLIP models.

Intuitively, \(^{}\) is more aligned with WRM than \(^{}\) as the learning targets (9) for WRM are based on the predictions of the pre-trained model prompted by \(^{}\). So, reducing the WRM term \(R^{}_{}(;^{})\) in some sense limits the divergence of the predictions between the pre-trained models and the fine-tuned models, hence helping better preserve pre-trained (core) features. This explains why using \(^{}\) for both ERM and WRM would lead to poorer outcomes (Row 1 _vs._ 9). ERM targets, typically one-hot class labels, do not capture subtle core visual differences. As a result, ERM would weaken the bond between \(^{}\) and core visual features as perceived by the model during fine-tuning, and therefore reduce the power of WRM in preserving those features.

Interestingly, comparing Row 1 & 8, we find that \(^{}\) not only improves OOD performance but also ID performance. We hypothesize that this is because the core features are better preserved with \(^{}\) and thus the fine-tuned model relies on a more diverse set of features to make predictions, reducing overfitting and improving ID generalization as well.

Impact of dual prompts for inference.After DRM fine-tuning, we obtain a new CLIP model with updated parameters \(\). Since the model is fine-tuned with dual prompts, it is natural to use the same dual prompts for inference, as in (10). Indeed, we find that \(^{}_{}\) (Row 1) outperforms \(^{}_{}\) (Row 2) and \(^{}_{}\) (Row 3) in Table 3. In particular, while \(^{}_{}\) is better than \(^{}_{}\) in-distribution and the latter is better out-of-distribution, they underperform \(^{}_{}\) on both ID and OOD fronts. This is similar to the phenomenon we have observed in the fine-tuning scenario. Dual prompts generally reduce overfitting and the effect carries over from fine-tuning to inference.

Impact of affinity normalization.In Section 4.2, we pointed out a problematic issue regarding the direct estimate for \(p_{}(y|x)\) obtained via (5) with \(t_{y} t^{}_{y}\) and \(_{0}\). To address the issue, we proposed another estimate \(_{}(y|x)\) in (9) based on normalized affinities. Comparing the two approaches, Row 10 of Table 3 shows that the direct estimate leads to severe degradation in both ID and OOD performance, demonstrating the importance of affinity normalization.

### Reliability of LLM-generated concept descriptions

Consistency across repeated generations.The concept descriptions used in our experiments are generated by GPT-4. Since the generation process is stochastic, it might impact the performance of DRM. To evaluate this impact, we repeatedly ask GPT-4 to generate a concept description for each iWildCam class for three times and then find the standard deviation of the resulting image-text affinities. The average standard deviation over 20,000 randomly sampled images of iWildCam is 0.0061, which is very small compared to the mean affinity, 0.2659. This suggests that the affinities are robust to the randomness in the generation process, which is therefore unlikely to have any noticeable

    & ERM & WRM & Affinity & Inference &  \\ Row & \(R_{}(;)\) & \(R^{}_{}(;)\) & norm. & w/ model & ID & OOD \\ 
1 & & & & dual & **61.8** & **49.2** \\
2 & df & cd & ✓ & df & 60.4 & 45.1 \\
3 & & & & cd & 54.8 & 47.2 \\ 
4 & df & – & – & df & 56.0 & 41.9 \\
5 & cd & – & – & cd & 56.9 & 43.4 \\ 
6 & – & df & ✓ & df & 52.4 & 45.3 \\
7 & – & cd & ✓ & cd & 51.7 & 46.3 \\ 
8 & df & df & ✓ & df & 54.4 & 45.1 \\
9 & cd & cd & ✓ & cd & 54.0 & 46.0 \\ 
10 & df & cd & ✗ & dual & 32.1 & 24.2 \\   

Table 3: Results of ablation studies on DRM with CLIP ViT-L/14 performance and iWildCam. We use “df” and “cd” to denote the type of text prompts used to produce model predictions. “dual” refers to the mixture model (10) for inference. “–” means the corresponding loss term is not in use.

impact on the performance of DRM. Examples of the generated descriptions in Appendix D.2 show that the same core visual features are described quite consistently across generations.

Generation across different LLMs.We also experiment with different LLMs of various sizes (from 8B to over 400B parameters) to generate concept descriptions. The results are reported in Table 4. While larger and more advanced models lead to better performance, DRM is not sensitive to the specific choice of LLM for generating the concept descriptions. Even with a relatively small LLM, Llama-3 (8B), DRM still maintains a significant edge over FLYP on iWildCam. Examples of the concept descriptions generated by the LLMs are provided in Appendix D.3.

### Study on the effect of \(\) in DRM

As in (7), DRM involves a hyperparameter \(\) that balances the weight of the empirical risk and the worst-case risk. When \(=0\), only the empirical risk is involved during fine-tuning, resulting in an ERM model. As \(\) increases, the influence of the empirical risk reduces, and the resulting model becomes closer to a WRM model. In practice, we choose the value of \(>0\) based on ID validation, which is often positively correlated with OOD performance (Taori et al., 2020; Miller et al., 2021). In Table 5, we show the ID and OOD performance of CLIP ViT-L/14 fine-tuned on iWildCam and CLIP ViT-B/16 fine-tuned on ImageNet under different choices of \(\).

Compared to FLYP (\(=0\)), the results indicate that DRM maintains high-level OOD performance across \(\) between 1 and 5, suggesting that DRM is fairly insensitive to the choice of \(\). Furthermore, it is interesting to note that ID and OOD performance improve together on iWildCam as \(\) increases to 3. Our further analysis reveals that this occurs because DRM assists in reducing overfitting--there is a decrease in training accuracy (from 88.29 to 87.41) and an increase in validation accuracy (from 81.64 to 82.43) as \(\) increases (from 1 to 3)--thereby enhancing both ID and OOD performance. While DRM does not always improve ID performance (e.g., on ImageNet), the result suggests that it still achieves a good trade-off between ID and OOD performance (-0.6 ID performance for +3.2 OOD performance), which in turn could lead to a better Pareto front.

## 6 Conclusion

In conclusion, this paper introduces dual risk minimization (DRM), a novel method that enhances the robustness of models fine-tuned from zero-shot foundation models against distribution shifts. DRM combines ERM with WRM, focusing on the preservation of core features which essentially define the target classes. To guide the fine-tuning process, DRM utilizes concept descriptions generated by LLMs like GPT-4. By balancing expected and worst-case performance, DRM overcomes the traditional limitations of ERM and achieves significant OOD performance improvements on multiple real-world benchmarks, establishing a new state of the art. Potential future directions include a deeper theoretical investigation into DRM as a general principle, using DRM to improve the general robustness of zero-shot models across a broad range of tasks, and better understanding the role of concept descriptions in vision-language modeling.

   Method & LLM (\#params) & ID & OOD \\  FLYP & N/A & 52.2 & 35.6 \\  DRM & GPT-3.5 (20B?) & 53.4 & 38.7 \\ DRM & GPT-4 (?1T?) & 54.1 & 40.0 \\  DRM & Llama-3 (8B) & 53.8 & 39.2 \\ DRM & Llama-3 (70B) & 54.0 & 39.9 \\ DRM & Llama-3 (405B) & 53.9 & 40.5 \\   

Table 4: Performance of fine-tuned CLIP ViT-L/14 on iWildCam with concept descriptions generated by different LLMs of various sizes.

    &  &  \\  \(\) & ID & OOD & ID & OOD \\ 
0.0 & 56.0 & 41.9 & **82.6** & 60.2 \\
1.0 & 59.1 & 47.3 & 81.5 & 62.5 \\
2.0 & 60.0 & 48.1 & 81.8 & 63.1 \\
3.0 & **61.8** & **49.2** & 82.0 & 63.2 \\
4.0 & 60.9 & 48.6 & 81.9 & **63.4** \\
5.0 & 60.1 & 48.5 & 81.7 & 63.3 \\   

Table 5: Performance of DRM under different \(\) on iWildCam and ImageNet with CLIP ViT-L/14 and CLIP ViT-B/16 respectively.