# TOPA: Extending Large Language Models for

Video Understanding via Text-Only Pre-Alignment

 Wei Li\({}^{1,2}\)  Hehe Fan\({}^{1}\)1  Yongkang Wong\({}^{3}\)  Mohan Kankanhalli\({}^{3}\)  Yi Yang\({}^{1,2}\)

\({}^{1}\) ReLER Lab, CCAI, Zhejiang University, China

\({}^{2}\) The State Key Laboratory of Brain-Machine Intelligence, Zhejiang University, China

\({}^{3}\) School of Computing, National University of Singapore, Singapore

{weili6,hehehefan,yangyics}@zju.edu.cn

yongkang.wong@nus.edu.sg mohan@comp.nus.edu.sg

https://github.com/dhg-wei/TOPA

###### Abstract

Recent advancements in image understanding have benefited from the extensive use of web image-text pairs. However, video understanding remains a challenge despite the availability of substantial web video-text data. This difficulty primarily arises from the inherent complexity of videos and the inefficient language supervision in recent web-collected video-text datasets. In this paper, we introduce Text-Only Pre-Alignment (TOPA), a novel approach to extend large language models (LLMs) for video understanding, without the need for pre-training on real video data. Specifically, we first employ an advanced LLM to automatically generate _Textual Videos_ comprising continuous textual frames, along with corresponding annotations to simulate real video-text pairs. Then, these annotated textual videos are used to pre-align language-only LLMs with the video modality. To bridge the gap between textual and real videos, we employ the CLIP model as the feature extractor to align image and text modalities. During text-only pre-alignment, the continuous textual frames, encoded as a sequence of CLIP text features, are analogous to continuous CLIP image features, thus aligning the LLM with real video representation. Extensive experiments, including zero-shot evaluation and finetuning on various video understanding tasks, demonstrate that TOPA is an effective and efficient framework for aligning video modality with LLMs. In particular, without training on any video data, the TOPA-Llama2-13B model achieves a Top-1 accuracy of 51.0% on the challenging long-form video understanding benchmark, EgoSchema. This performance surpasses previous video-text pre-training approaches and is competitive with recent GPT-3.5-based video agents.

## 1 Introduction

Image-language understanding has made large advancements in both image-language alignment  and Multimodal Large Language Models (MLLMs) , aided by pre-training on large-scale noise-paired image-text data collected from the web . This raises a question: _Can we mirror this success in video-language understanding?_ Research  has explored pretraining video-language models on millions of web video-text data , achieving promising results in basic video tasks such as video-text retrieval, video captioning, and video question answering across conventional video benchmarks. However, recent research reveals that these models struggle with a challenging long-form video understanding benchmark, _i.e_., EgoSchema , which requires intrinsic temporal understanding capabilities. This highlights the gap in adapting web video-text pretrained models to more comprehensive video understanding tasks.

We attribute this gap to two primary factors: _1) The intrinsic complexity of the video modality._ Videos introduce intrinsic complexities in both spatial and temporal dimensions, which are not present in static images. These complexities require extensive training on larger-scale data to effectively capture video dynamics. Furthermore, representing videos typically involves processing multiple frames, significantly increasing computational demands compared to image modeling. The dual challenges of large-scale training and increased computational requirements make video-language modeling particularly challenging. _2) The limitations of web language supervision._ The language supervision in recent web video-text datasets primarily comes from subtitles or descriptions associated with the videos [3; 40]. However, subtitles often suffer from the issues of visual-textual misalignment [33; 17]. Moreover, the form of descriptive supervision is inefficient in building robust video reasoning capabilities, especially in terms of temporal reasoning. This mismatch between the complex video content and the limited supervision hinders effective video-language modeling.

In this paper, we propose an innovative approach to develop video understanding capabilities by using LLMs to simulate and understand video dynamics. Instead of directly aligning LLMs with real video representation, we first introduce a textual video representation -- a sequence of textual frames designed to mimic real visual dynamics. This textual video can be readily generated by advanced LLMs and effectively simulates various video dynamics by describing them in text. Specifically, we present a Textual Video (TextVid) dataset, automatically generated by LLMs. TextVid includes: 1) _Textual videos_ (hereinafter referred to as "**Tideo**"), which consist of a sequence of textual frames crafted to mimic the keyframes of real videos, and 2) _Tideo annotations_, including comprehensive Tideo-level dense descriptions and varied question-answer (QA) pairs. These annotations are of high quality and closely align with the Tideo content, by virtue of the powerful capability of LLM in language generation.

Building on the proposed TextVid dataset, we introduce the Text-Only Pre-Alignment (TOPA) framework, to effectively and efficiently pre-align LLMs with the video modality, reducing the need for costly video-text pre-training. We introduce three tasks for video-LLM pre-alignment: Tideo summarization, Tideo QA and multi-choice Tideo QA. To bridge the gap between textual Tideos and visual videos, we leverage the CLIP  model for feature extraction. Specifically, we employ the CLIP text encoder to extract frame-level representations for Tideos, and the CLIP visual encoder for real videos. During the text-only pre-alignment phase, the LLM learns to process continuous CLIP text features of Tideos. In the real video inference phase, it transitions to handling continuous CLIP image features of real video. Due to the aligned CLIP image-text feature space, the LLM can adapt to real video inputs despite being trained on textual representations. Our main contributions include:

(1) We propose a novel Text-Only Pre-Alignment (TOPA) framework to extend Large Language Models (LLMs) for video understanding. TOPA aligns LLMs with the video modality efficiently and effectively without the need for training on real videos, reducing the costs for video-text pre-training.

(2) We introduce TextVid, a textual video dataset automatically generated by advanced LLMs. TextVid dataset comprises 721K diverse Tideos along with associated high-quality annotations, which include detailed Tideo descriptions and a variety of question-answer pairs.

(3) Extensive experiments demonstrate TOPA's effectiveness across various video understanding tasks. Particularly, the TOPA-Llama2-13B model achieves 51.0% Top-1 accuracy in the challenging EgoSchema benchmark, outperforming previous video-text pretraining methods and competitive with recent GPT-3.5-based video agents.

## 2 Related Work

**Vision-language alignment.** CLIP  aligns the vision and language modalities in a common feature space via contrastive learning with large-scale web image-text data. MLLMs [1; 27; 34; 89] align the visual model with LLM via training on image-caption pairs and interleaved image-text data. Video-LLMs [7; 23; 32; 83] explore modeling video sequences within LLM spaces, leveraging LLM for video-language understanding. In this paper, we focus on video-LLM alignment. Rather than using multimodal data for vision-language alignment, we introduce a novel text-only pre-alignment framework to extend LLMs for video understanding without pre-training on real video-text data.

**LLMs for multimodal data augmentation.** Recent research explores the use of LLMs to enhance the multimodal data. A line of work [5; 12; 34] use LLMs for refining captions or extending the image caption pairs to diverse visual tasks like visual conversation and image editing. Another line of work [28; 37; 38; 47] further employ advanced LLM to enrich web video supervision for video instruction tuning. In this paper, rather than enhancing multimodal datasets, we propose generating text-only data consisting of "textual videos" and diverse language supervision, which aims to simulate real videos and their corresponding annotations.

**Long-form video understanding.** Long-form video understanding [39; 61; 71] presents significant challenges due to the intricate spatial and temporal dynamics. Conventional video-text pretraining approaches [4; 45; 65; 66; 77; 90] utilize extensive web video-caption data for video-language alignment. Recent research [28; 66; 83; 88] employ video instruction-tuning for video-LLM alignment to enhance video-language understanding. Another line of research [22; 54; 81; 49] seeks to adapt recent image MLLMs to video understanding. A parallel line of research [9; 42; 55; 60; 63; 13; 78; 82; 67; 21] combine the LLM with various VLM tools as video agents to perform video-understanding tasks. In this paper, we propose a novel text-only pre-alignment framework to efficiently and effectively align LLMs with videos without pre-training on real videos.

## 3 Method

In this section, we detail the TOPA framework. We first introduce the data generation pipeline of TextVid Dataset (Section 3.1). Next, we describe how to align the Tideo representation with LLM (Section 3.2). Finally, we discuss adapting the text-only aligned video-LLM model for real video inference (Section 3.3). An overview is illustrated in Figure 1.

### TextVid Dataset

This dataset, comprising textual videos (Tideos) and associated annotations, is generated by an advanced LLM (_i.e_., Gemini Pro 1.0 ). The data generation pipeline is detailed in Appendix D. Each Tideo is presented in a textual format and contains 5-15 sequential frames. Each frame includes a frame caption that describes the scene and multiple object captions. To enhance understanding and

Figure 1: Overview of the proposed Text-Only Pre-Alignment (TOPA) framework. _Left_: The pipeline used for generating the TextVid dataset. _Right_: The video-LLM alignment framework. During text-only pre-alignment, the LLM learns to process continuous CLIP text features. In zero-shot inference, the LLM uses projected CLIP visual features as input. Additionally, TOPA supports supervised fine-tuning on downstream video datasets to further improve the performance.

interaction with these Tideos, the dataset features a dense description summarizing the Tideo, as well as a set of multiple-choice questions and answers related to the Tideo content. The structure of each element is as follows:

 **Dataset Element**: \\
**Tideo**: Sequence of textual frames \(\{T_{1},T_{2},,T_{n}\},5 n 15\) \\ For each frame \(T_{i}\): \\ Frame caption: \(C_{i}\) \\ Object captions: \(D_{i,j}\) for main objects in \(T_{i}\) \\
**Annotations**: \\ Global Dense Description of the Tideo: \(D_{V}\) \\ Set of Questions-Options-Answers: \(\{(Q_{k},O_{k},A_{k})\}\) \\ 

There are two major advantages of the TextVid dataset. **(1) The large-scale and diverse Tideos.** As the dataset is text-only and fully generated by an LLM, the size of TextVid is practically unlimited. Moreover, the Tideos can cover a broad range of domains by simply prompting the language model with appropriate conditions. It is distinctly different from previous web video-text dataset like Howto100M  that are limited to specific human-centric instructional videos. In practice, we enhance the diversity of TextVid by randomly sampling video captions from WebVid-2M , video titles from Howto100m , video tasks from Ego4D  and object names with descriptions from WordNet  as a condition of prompts. These varied prompts enable the language model to generate a diverse dataset. **(2) The high-quality, consistent and free-form language supervision.** The language supervisions are generated along with Tideos. The advanced capabilities of LLM ensure the quality of these supervisions, making them less noisy than web video-text data. Moreover, both the Tideo and the supervision are in textual format, making the supervision closely aligned with the Tideo's content. Additionally, the format of the language supervision is unrestricted. For example, we prompt the LLM to generate dense descriptions and multi-choice QA pairs as language supervision.

### Text-Only Pre-Alignment

**Preliminary: Video-LLM alignment.** The goal of video-LLM alignment is to extend pre-trained LLMs for processing video inputs. Given a video sampled with \(n\) frames \(\{_{1},_{2},,_{n}\}\), Recent work [23; 77] uses a frozen CLIP model to extract the frame-level visual feature, formulated as \(_{v}^{v}=E_{}(_{i})\), where \(E_{}\) denotes CLIP image encoder. The CLIP features are then projected into the LLM space via a simple linear layer, denoted as \(G(P(_{v}^{v}),...,P(_{n}^{v}))\), where \(G\) denotes a language model and \(P\) denotes a projection layer that projects the CLIP feature to LLM space.

**Tideo representation.** In this work, we leverage Tideos (Section 3.1) for video-LLM pre-alignment instead of training on real videos. Specifically, given the textual frame \(T_{i}\), we employ CLIP text encoder to extract the frame representation from frame caption \(C_{i}\) and detailed object captions \(D_{i}\), represented as \(_{i}^{t}=F_{}(E_{}(C_{i}),E_{}(D_ {i,1}),...,E_{}(D_{i,j}))\), where \(F_{}\) is a fusion function such as simple average pooling, and \(E_{}\) denotes the CLIP text encoder. A Tideo with \(n\) textual frames is represented as \(^{t}=\{_{1}^{t},...,_{n}^{t}\}\).

**Text-only pre-alignment.** Given the Tideo \(T\), dense Tideo-level description \(D_{V}\), and QA pairs with multiple choices \(\{(Q_{k},O_{k},A_{k})\}\), we introduce the following tasks for Tideo-LLM alignment: (1) **Tideo Summarization**: Given the Tideo, generate a detailed description to summarize the Tideo; (2) **Tideo QA**: Given the Tideo and question, predict the answer; (3) **Multi-choice Tideo QA**: Given the Tideo, question and multiple choices, choose the correct answer from the candidates. We employ a unified auto-regressive Language Modeling (LM) objective for these three tasks:

\[_{}(_{G},_{P})=-_{i=1}^{|t|}  Gt_{i}|P(}),Z,t_{<i},\] (1)

where \(}\) denotes the Tideo representation, and \(}=\{_{1}^{t},...,_{n}^{t}\}\) during the text-only training, Z denotes the task specific condition tokens and \(t_{i}\) denotes the \(i_{th}\) target token. \(_{G}\) and \(_{P}\) denote the learnable parameters of the LLM adapter and the projection layer \(P\), respectively. In practice, we use the following format as the LLM input: \(\{\}\). Video:\(\{_{1}^{t},...,_{n}^{t}\}\). \(\{\}\). Answer: \(\{\}\). For the Tideo summarization task, the target is detailed Tideo descriptions. For Tideo QA task, the target is the answer and the condition is the question. For multi-choice TideoQA task, the target is the correct option and the condition consists with question and options. The details of the task-specific prompts are included in Appendix F.1.

### Adapting to Real Video Understanding

Section 3.2 introduces the text-only pre-alignment using the TextVid dataset. In this section, we detail how to adapt this text-only pre-aligned LLM for real video understanding. We introduce two approaches: one is zero-shot inference, which directly infers with real video data. And the other is supervised finetuning, where the pre-aligned model is further finetuned on downstream video data.

**Zero-shot inference.** During pre-alignment, we leverage the textual representation \(^{t}=\{_{1}^{t},...,_{n}^{t}\}\) as the Tidoe representation. During inference, we take real videos features as input, _i.e_., \(^{v}=\{_{1}^{v},...,_{n}^{v}\}\), where \(_{i}^{v}=E_{}(_{i})\). These two modality features \(^{t}\) and \(^{v}\) that come from CLIP image encoder and CLIP text encoder are aligned via CLIP pre-training. This aligned image-text representation makes it possible to perform zero-shot inference without additional finetuning. However, the _modality gap_ phenomenon [16; 30; 31; 44; 85], _i.e_., CLIP image feature and CLIP text feature are located in two completely separate regions of the feature space, prevents us from directly taking the visual feature \(^{v}\) as the textual feature \(^{t}\). To bridge this modality gap, we follow DeCap  to employ a support memory to project the CLIP visual feature into the CLIP text feature space. This training-free projection process is formulated as:

\[^{v t}=_{i=1}^{N}w_{i}*_{i}=_{i=1}^{N} _{i}^{}^{v})/)}{_{k=1}^{N}(( _{k}^{}^{v})/)}*_{i},\] (2)

where \(_{i}\) denotes CLIP text feature from a pre-constructed memory of size \(N\), \(^{v}\) denotes input frame feature of real video and \(^{v t}\) denotes the projected feature. During zero-shot inference, we take the \(^{v t}=\{_{1}^{v t},...,_{n}^{v t}\}\) as the real video's representation.

**Supervised finetuning.** On the other hand, the text-only pre-alignment can be viewed as a pretraining stage. Following the pretraining-finetuning paradigm, the pre-aligned LLMs can then be fine-tuned on real video data for improved downstream task performance. The finetuning process is similar to the text-only pre-alignment as detailed in Section 3.2, except that the LLM receives a sequence of CLIP visual features as input instead of CLIP textual features.

### Implementation Details

We leverage Llama2-7B, Llama2-13B  and Llama3-8B as the LLM backbone. Additionally, we employ the Llama-adapter  with an adaptation embedding length of 50. We utilize CLIP-ViT-L as the multimodal encoder. We employ a simple linear layer to project the CLIP feature into the LLM feature space. During training, the CLIP model and LLM backbone are frozen. The projection layer and additional Llama-adapter are trainable. For text-only pre-alignment, we uniformly sample the Tideos into 10 frames. We train the model on a mixture of tasks comprising Tidoe summarization, Tideo QA, multi-choice Tideo QA with the ratio of 1:1:2. For zero-shot inference, we construct a memory for cross-modal projection, consisting of 2M CLIP text features sampled from captions in the TextVid dataset. TOPA-Llama2-7B and TOPA-Llama3-8B are trained on four 40G-A100 GPUs in one day. TOPA-Llama2-13B is trained in two days. More training details of TOPA and baselines are included in Appendix E.2.

## 4 Experiments

TOPA enables the LLM to perform various video understanding tasks as shown in Figure 2. In this section, we evaluate TOPA on multi-choice video QA and video captioning tasks. Section 4.1 evaluates TOPA on NeXT-QA , STAR , TVQA , recent challenging EgoSchema  and MVBench benchmarks with the zero-shot setting. We further evaluate TOPA on multi-choice video QA with the finetuning setting (Section 4.2) and zero-shot video captioning task (Section 4.3). In Section 4.4, we conduct ablation study on the LLM prior and input video frames. We report Top-1 accuracy on multi-choice video QA benchmarks and CIDEr  score on video captioning benchmarks. We mainly compare TOPA with the following categories of video understanding approaches:

**(1) Web video pre-training approaches**. This line of work aims to develop general video-language models by leveraging extensive web videos, using associated video captions or audio as weak supervision signals.

**(2) Adapting image MLLMs for video understanding**. These approaches aim to extend the image understanding capabilities of recent vision-language models (VLMs) to video understanding. Specifically, SeViLa  utilizes BLIP-2 for localizing and understanding key frames of a video. IG-VLM  converts video into a composite image by arranging the video frames into a grid layout.

**(3) LLM-based video agents**. This line of work leverages LLMs like GPT-3.5 and GPT-4 as an agent to understand a video by designing and executing a series of actions. The language-only agents perceive visual information via recent foundation VLMs (CLIP , BLIP-2 , LaViLa  and PALI ).

**(4) Our text-only pre-alignment.** Different from the above works, TOPA leverages the proposed TextVid dataset for video-LLM pre-alignment, enabling the LLM to process continuous features. Thus, it can enable performing video understanding tasks.

### Zero-Shot Evaluation on Multi-Choice Video QA

#### 4.1.1 Zero-shot Results on EgoSchema

Table 1 shows the results on EgoSchema full set. We compare our method against a range of recent approaches in video understanding. Our proposed text-only pre-alignment framework, despite training without real videos, shows impressive results on the EgoSchema benchmark. TOPA outperforms previous image-based adaptation approach IG-VLM and video agents LLoVi and Vamos with the same scale LLM (Llama2-7B and Llama2-13B). Moreover, TOPA shows consistent improvements when scaled up with a larger LLM backbone, indicating the effectiveness of LLMs in complex video understanding tasks.

**Discussion 1: The necessity of high-quality language supervision for video understanding.** Recent video pre-training approaches like LongViVi  and InternVideo , despite training on million-level web video-text data, show inferior performance on EgoSchema evaluation. These results highlight the inefficacy and inefficiency of conventional contrastive pre-training in understanding long-form videos, primarily due to noisy and simplistic language supervision. In contrast, our TOPA, trained on 721K Tideoswith high-quality language supervision, shows impressive results on EgoSchema. It indicates that, unlike image understanding which significantly benefits from leveraging web language as supervision, video understanding may require more precise and accurate language supervision to better capture the complex visual dynamics.

Figure 2: Examples of TOPA-LLama2-13B for video-language understanding. Given a video, TOPA is able to summarize the video content and answer the questions.

**Discussion 2: Video agents versus end-to-end video-LLM modeling.** Video agents have shown impressive results on the EgoSchema benchmark, aided by advanced LLMs and VLMs. However, a significant limitation of these approaches is their heavy reliance on the powerful LLMs. For example, the accuracy of Vamos drops by -11.6% when the GPT-4 is replaced with Llama2-13B, largely falling behind the performance of the TOPA-Llama2-13B model. The reliance on powerful closed-source LLMs restricts its application fields and introduces external overheads. Moreover, video agents make decisions based on the language format clues collected by VLMs. Converting the video content into language clues may lead to a limited upper bound compared to end-to-end modeling. Additionally, the inference speed of these approaches is another concern, since it involves multiple interactions with both VLMs and LLMs. In contrast, end-to-end video-LLM models, which condense the video into a sequence of embeddings as the input of LLM, are more efficient.

#### 4.1.2 Zero-shot Results on NexT-QA, STAR and TVQA

Table 2 shows the multi-choice video QA results across various benchmarks. TOPA achieves impressive performance on the TVQA and EgoSchema benchmarks, significantly outperforming previous video pre-training models and image-to-video adaptation approaches. This indicates that our TOPA framework effectively enables LLMs to handle video input, despite not being pre-trained on real videos. However, for the NeXT-QA and STAR benchmarks, TOPA underperforms compared to SeViLA and IG-VLM. A major reason is that these benchmarks involve many fine-grained visual questions, including those about object locations and relationships. SeViLA and IG-VLM, benefiting from the advanced image-understanding capabilities of pre-trained VLMs such as LLaVA, excel in answering these fine-grained visual questions. In contrast, our TOPA framework primarily focuses on high-level semantic alignment. Moreover, during zero-shot inference, we project the visual features into the text feature space to bridge the modality gap, as described in Eq. 2. This cross-modal semantic projection process tends to overlook fine-grained visual details, such as object locations, which leads

    & **Core VLMs** & **Core LLMs** & **Acc@1** \\  Human Eval [NeurIPS 2023] & - & - & 75.0 \\ Gemini:1.5-Pro [arXiv2024.2 ] & - & Gemini:1.5-Pro & 63.2 \\   _(Pre-train on web video-text data)_ & & & \\ FrozenBiLM [NeurIPS 2022] & - & - & 26.9 \\ InternVideo [arXiv 2022.12] & - & - & 32.1 \\ LongViViT [CVPR 2024 ] & - & - & 33.3 \\ MC-ViT-L\({}^{}\) [ICML 2024 ] & - & - & 44.4 \\ InternVideo2\({}_{s3}\)-6B\({}^{}\) [arXiv 2024.3 ] & - & - & 41.1 \\  _(Adapt image MLLMs for video understanding)_ & & & \\ SeViLA [NeurIPS 2023] & BLIP-2 & FLAN-T5-XL  & 22.7 \\ MVU [arXiv 2024.3 ] & LLaVA-v1.5-13B & Vicuna-13B & 37.6 \\ IG-VLM [arXiv 2024.3 ] & LLaVA v1.6-7B & Vicuna-7B & 35.8\({}^{*}\) \\ IG-VLM [arXiv 2024.3 ] & LLaVA v1.6-13B & Vicuna-13B & 47.0\({}^{*}\) \\  _(LLM-based video agents)_ & & & \\ LangRepo [arXiv 2024.3 ] & CLIP-ViT-L & Mixtral-12B  & 41.2 \\ Vamos [arXiv 2023.11] & BLIP-2 & Llama2-13B & 36.7\({}^{*}\) \\ Vamos [arXiv 2023.11] & BLIP-2 & GPT-3.5 & 41.2\({}^{*}\) \\ Vamos [arXiv 2023.11] & BLIP-2 & GPT-4 & 48.3\({}^{*}\) \\ MoReVQA [CVPR 2024 ] & PALI-3-5B  & PaLM-2  & 51.7 \\ LLoVi [arXiv 2024.3 ] & LaViLa\({}^{}\) & GPT-3.5 & 50.3 \\ VideoAgent [ECCV 2024 ] & LaViLa\({}^{}\) & GPT-4 & 54.1 \\ LifelongMemory [arXiv 2024.3 ] & LaViLa\({}^{}\) & GPT-4 & 62.4 \\ VideoAgent [ECCV 2024] & Video-LLava & GPT-4 & 60.2 \\        & TOPA & CLIP-ViT-L & Llama2-7B & 41.2 \\  (Our Text-Only Pre-Alignment) \\  & TOPA & CLIP-ViT-L & Llama3-8B & 44.2 \\ 
 TOPA \\  & CLIP-ViT-L & Llama2-13B & 51.0 \\   

Table 1: Zero-shot results on EgoSchema  full set. Methods that leverage closed-source LLMs are marked in gray. \({}\) denotes the model is trained with in-domain egocentric videos from Ego4D . \({}^{*}\) denotes results on EgoSchema subset. Results of InternVideo and FrozenBiLM are sourced from . Results of SeViLA are sourced from .

[MISSING_PAGE_FAIL:8]

### Supervised Finetuning

In this section, we further finetune the pre-aligned TOPA models to study the benefits of TOPA for downstream supervised learning. During finetuning, TOPA directly takes the video feature as input without the cross-modal projection. More finetuning details for each dataset are provided in Appendix E.2. Table 4 shows the finetuning results on multi-choice video QA dataset. For comparison, we include baseline models without text-only pretraining. Our text-only pre-alignment consistently improves the performance across three benchmarks. Notably, TOPA-Llama2-7B achieves 67.1% accuracy on TVQA, outperforming other approaches by a large margin. These results suggest that our text-only pre-alignment, even without training on real videos, has a similar effect to conventional video-language pre-training.

**Data-efficient finetuning.** Figure 3 shows the results of finetuning LLMs with various ratios of training data. TOPA trained with 10% data achieves 64.7% Top 1 accuracy on NeXT-QA benchmark, significantly outperforming the baseline that without text-only pre-alignment. Besides, when trained with less than 20% data, the baseline model even performs worse than TOPA-zeroshot on NeXT-QA and TVQA, clearly demonstrating the effectiveness of TOPA in limited annotated data scenarios.

### Video Captioning

**Results on zero-shot video captioning.** We further perform zero-shot video captioning on MSR-VTT  and VATEX . As shown in Table 5, TOPA largely outperforms previous text-only approaches like Decap which is trained on captions sourced from CC3M . TOPA even outperforms the video-text pre-training approaches like VideoCoCa, which is pre-trained on millions of videos-text data, demonstrating that TOPA is an efficient and effective framework for video-LLM alignment.

    &  &  &  \\    & Tem. & &  &  & **Avg.** & &  & Seq. &  &  & **Avg.** \\  FrozenBiLM (10)  & - & - & - & - & - & - & - & - & - & - & 57.5 \\ InternVideo (8)  & 58.5 & 62.5 & 75.8 & 63.2 & 62.7 & 65.6 & 54.9 & 51.9 & 58.7 & 57.2 \\ BLIP-2\({}^{}\) (4)  & 65.2 & 70.1 & 80.1 & 70.1 & 52.3 & 54.8 & 49.0 & 51.2 & 51.8 & 54.5 \\ SeViLA (32 \(\) 4)  & 69.4 & 74.2 & 81.3 & 73.8 & 63.7 & 70.4 & 63.1 & 62.4 & 64.9 & 61.6 \\ Llama-VQA-7B (10)  & 69.2 & 72.7 & 75.8 & 72.0 & 66.2 & 67.9 & 57.2 & 52.7 & 65.4 & - \\  Baseline (10) & 65.3 & 69.0 & 72.6 & 68.4 & 60.8 & 61.5 & 49.2 & 49.8 & 59.4 & 63.8 \\ TOPA-Llama2-7B (10) & 71.3 & 74.2 & 78.5 & 73.9 & 66.8 & 68.9 & 59.1 & 55.5 & 66.4 & 67.1 \\  & +6.0 & +5.2 & +5.9 & +5.5 & +6.0 & +7.4 & +9.9 & +5.7 & +7.0 & +3.3 \\  Baseline (10) & 66.0 & 69.7 & 73.7 & 69.1 & 61.4 & 62.4 & 50.6 & 51.8 & 60.3 & 66.2 \\ TOPA-Llama3-8B (10) & 70.1 & 74.5 & 74.6 & 73.1 & 66.3 & 67.0 & 59.1 & 56.5 & 65.4 & 68.1 \\  & +4.1 & +4.8 & +0.9 & +4.0 & +4.9 & +4.6 & +8.5 & +4.7 & +5.1 & +1.9 \\  Baseline (10) & 67.8 & 71.6 & 75.2 & 70.9 & 58.7 & 59.5 & 54.3 & 51.8 & 58.2 & 66.6 \\ TOPA-Llama2-13B (10) & 72.1 & 75.8 & 79.3 & 75.1 & 66.8 & 68.3 & 61.0 & 55.1 & 66.3 & 69.0 \\  & +4.3 & +4.2 & +4.1 & +4.2 & +8.1 & +8.8 & +6.7 & +3.3 & +8.1 & +2.4 \\   

Table 4: Finetuning results on NExT-QA, STAR and TVQA.

Figure 3: Results of finetuning TOPA with various ratios of training data.

### Ablations

**LLM prior in video-language understanding.** To investigate the impact of LLM prior in multi-choice video QA, we conduct experiments on EgoSchema with the blind setting, where only the questions and choices are provided to the LLM. Table 6 shows the results. Bard and GPT-4-Turbo achieve 33.2% and 30.8% accuracy, respectively. Gemini-Pro-1.0 reaches 38.2% accuracy. These blind results of advanced LLMs suggest that in some video QA cases, LLMs can accurately choose the correct answer solely based on the question and choices, without visual input. However, the blind performance of Llama2-7B and Llama2-13B is inferior, potentially due to their smaller model size. After training on the TextVid dataset, TOPA-Llama2-13B achieves a blind accuracy of 37.5% (or +11.7%), closely approaching that of Gemini-Pro-1.0 model. These results suggest that text-only pre-alignment can effectively prepare LLMs for downstream video-language tasks by leveraging specialized text-only tasks, even in complex scenarios where the original LLMs are limited.

**The impact of video frames.** To better investigate TOPA's capability in understanding temporal dynamics of real videos, we conduct experiments with different number of frames. Table 7 shows the results. Multiple frames input consistently enhances performance on NeXT-QA and EgoSchema for both TOPA-Llama2-7B and TOPA-Llama2-13B. This indicates that the text-only pre-alignment effectively enables the LLM to handle multiple video frames, despite not being trained on real videos.

## 5 Conclusions

In this paper, we introduce TOPA, a text-only pre-alignment framework designed for aligning LLMs with video modality without requiring training on real videos. TOPA has demonstrated remarkable performance on the recent, challenging long-form video understanding benchmark, _i.e_., EgoSchema, showcasing that a text-only approach is effective in capturing the dynamics of long-form videos. Our approach, which includes data generation and text-only pre-alignment, has potential applications across various vision-language tasks where obtaining paired vision-language data is difficult.

    & Visual Input & ES Full \\  Random Selection & & 20.0 \\ GPT-4-Turbo \({}^{}\) & & 30.8 \\ Bard \({}^{}\) & & 33.2 \\ Gemini-Pro-1.0 & & 38.2 \\ Llama2-7B & & 20.1 \\ Llama2-13B & & 25.8 \\  TOPA-Llama2-7B & & 29.3 \\ TOPA-Llama2-13B & & 37.5 \\  TOPA-Llama2-7B & & 41.2 \\ TOPA-Llama2-13B & & 51.0 \\   

Table 6: Blind results on EgoSchema. \({}\) denotes results sourced from .

    & **Training data** & **MSR-VTT** & **VATEX** \\  _(Web video-text Pre-training)_ & & & \\ VideoCoCa-g  & 144M _VT_ & 27.1 & 22.8 \\ Flamingo-3B  & 27M _VT_ \& 2.1B _IT_ \& 43M _WP_ & - & 40.1 \\ Flamingo-9B  & 27M _VT_ \& 2.1B _IT_ \& 43M _WP_ & - & 39.5 \\ VideoPrism-B  w/ PaLM-2-1B & 618M _VT_ & 40.3 & 24.2 \\ VideoPrism-B  w/ PaLM-2-8B & 618M _VT_ & 38.5 & 31.7 \\  _(Text-only Pre-training)_ & & & \\ DeCap  & 3M _Captions_ & 18.6 & 18.7 \\ TOPA-Llama2-7B & 721K _TextVid_ & 32.9 & 31.0 \\ TOPA-Llama2-13B & 721K _TextVid_ & 33.4 & 32.0 \\   

Table 5: Zero-shot video captioning results. We report CIDEr score for all benchmarks. _VT_ denotes \( video clip, text\) pairs, _IT_ denotes \( image, text\) pairs, and _WP_ denotes webpages consisting of interleaved image and text data.

    & Visual Input & ES Full \\  Random Selection & & 20.0 \\ GPT-4-Turbo \({}^{}\) & & 30.8 \\ Bard \({}^{}\) & & 33.2 \\ Gemini-Pro-1.0 & & 38.2 \\ Llama2-7B & & 20.1 \\ Llama2-13B & & 25.8 \\  TOPA-Llama2-7B & & 29.3 \\ TOPA-Llama2-13B & & 37.5 \\  TOPA-Llama2-7B & & 41.2 \\ TOPA-Llama2-13B & & 51.0 \\   

Table 7: Ablation on video frames.