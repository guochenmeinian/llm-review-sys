# Scan and Snap: Understanding Training Dynamics

and Token Composition in 1-layer Transformer

 Yuandong Tian\({}^{1}\)  Yiping Wang\({}^{2,4}\)  Beidi Chen\({}^{1,3}\)  Simon Du\({}^{2}\)

\({}^{1}\)Meta AI (FAIR) \({}^{2}\)University of Washington \({}^{3}\)Carnegie Mellon University \({}^{4}\)Zhejiang University

{yuandong,beidic}@meta.com, {ypwang61,ssdu}@cs.washington.edu

###### Abstract

Transformer architectures have shown impressive performance in multiple research domains and have become the backbone of many neural network models. However, there is limited understanding on how Transformer works. In particular, with a simple predictive loss, how the representation emerges from the gradient _training dynamics_ remains a mystery. In this paper, we analyze the SGD training dynamics for 1-layer transformer with one self-attention plus one decoder layer, for the task of next token prediction in a mathematically rigorous manner. We open the black box of the dynamic process of how the self-attention layer combines input tokens, and reveal the nature of underlying inductive bias. More specifically, with the assumption (a) no positional encoding, (b) long input sequence, and (c) the decoder layer learns faster than the self-attention layer, we prove that self-attention acts as a _discriminative scanning algorithm_: starting from uniform attention, it gradually attends more to key tokens that are distinct for a specific next token to be predicted, and pays less attention to common key tokens that occur across different next tokens. Among distinct tokens, it progressively drops attention weights, following the order of low to high co-occurrence between the key and the query token in the training set. Interestingly, this procedure does not lead to winner-takes-all, but decelerates due to a _phase transition_ that is controllable by the learning rates of the two layers, leaving (almost) fixed token combination. We verify this _scan and snap_ dynamics on synthetic and real-world data (WikiText).

## 1 Introduction

The Transformer architecture  has demonstrated wide applications in multiple research domains, including natural language processing [2; 3; 4], computer vision [5; 6; 7], speech [8; 9], multimodality [10; 11], etc. Recently, large language models (LLMs) based on decoder-only Transformer architecture also demonstrate impressive performance [4; 12; 13], after fine-tuned with instruction data  or reward models . Why a pre-trained model, often supervised by simple tasks such as predicting the next word [4; 3; 13] or filling in the blanks [2; 16; 17], can learn highly valuable representations for downstream tasks, remains a mystery.

To understand how Transformer works, many previous works exist. For example, it has been shown that Transformer is a universal approximator , can approximate Turing machines [19; 20], and can perform a diverse set of tasks, e.g., hierarchical parsing of context-free grammar , if its weights are set properly. However, it is unclear whether the weights designed to achieve specific tasks are at a critical point, or can be learned by SoTA optimizers (e.g., SGD, Adam , AdaFactor , AdamW ). In fact, many existing ML models, such as \(k\)-NN, Kernel SVM, or MLP, are also universal approximators, while their empirical performance is often way below Transformer.

To demystify such a behavior, it is important to understand the _training dynamics_ of Transformer, i.e., how the learnable parameters change over time during training. In this paper, as a first step, we formally characterize the SGD training dynamics of 1-layer position-encoding-free Transformer fornext token prediction, a popular training paradigm used in GPT series [3; 4], in a mathematically rigorous manner. The 1-layer Transformer contains one softmax self-attention layer followed by one decoder layer which predicts the next token. Under the assumption that the sequence is long, and the decoder learns faster than the self-attention layer, we prove the following interesting dynamic behaviors of self-attention during training. **Frequency Bias**: it progressively pays more attention to key tokens that co-occur a lot with the query token, and loses attention to tokens that co-occur less. **Discriminative Bias**: it pays attention to distinct tokens that appear uniquely given the next token to be predicted, while loses interest to common tokens that appear across multiple next tokens. These two properties suggest that self-attention implicitly runs an algorithm of _discriminative scanning_, and has an inductive bias to favor unique key tokens that frequently co-occur with the query ones.

Furthermore, while self-attention layer tends to become more sparse during training, as suggested by Frequency Bias, we discover that it will not collapse to one-hot, due to a _phase transition_ in the training dynamics. In the end, the learning does not converge to any stationary points with zero gradient, but ventures into a region where the attention changes slowly (i.e., logarithmically over time), and appears frozen and learned. We further show that the onset of the phase transition are controlled by the learning rates: large learning rate gives sparse attention patterns, and given fixed self-attention learning rate, large decoder learning rate leads to faster phase transition and denser attention patterns. Finally, the SGD dynamics we characterize in this work, named **scan and snap**, is verified in both synthetic and simple real-world experiments on WikiText .

**Concurrent works on Transformer dynamics**. Compared to  that uses \(_{2}\) loss, our analysis focuses on cross-entropy, which is more realistic, imposes no prior knowledge on possible attention patterns inaccessible to training, and allows tokens to be shared across topics. Compared to  that analyzes "positional attention" that is independent of input data with symmetric initialization, our analysis focuses on attention on input data without symmetric assumptions. [28; 29; 30] give similar conclusions that self-attention attends to relevant tokens. In comparison, our work analyzes richer phenomena in 1-layer transformers related to frequency and discriminative bias, which has not been brought up by these works. For example, sparse attention patterns are connected with co-occurrence frequency of contextual token and query, characterization of such connection over training with softmax, including two-stage behaviors of attention logits, etc. We also leverage analytical solutions to certain nonlinear continuous dynamics systems that greatly simplifies the analysis. Detailed comparison can be found in Appendix B.

## 2 Related Works

**Expressiveness of Attention-based Models**. A line of work studies the expressive power of attention-based models. One direction focuses on the universal approximation power [18; 31; 32; 33; 20]. More recent works present fine-grained characterizations of the expressive power for certain functions in different settings, sometimes with statistical analyses [34; 35; 36; 37; 21; 38; 39; 40]. In particular, there is growing interest in explaining the capability of in-context learning  of Transformer, by mapping the gradient descent steps of learning classification/regression into feedforward steps of Transformer layers [42; 43; 44; 45; 37; 46]. Different from our work, the results in these papers are existential and do not take training dynamics into consideration.

**Training Dynamics of Neural Networks**. Previous works analyze the training dynamics in multi-layer linear neural networks [47; 48], in the student-teacher setting [49; 50; 51; 52; 53; 54; 55; 56; 57], and infinite-width limit [58; 59; 60; 61; 62; 63; 64; 65; 66; 67; 68; 69; 70; 71], including extentions to attention-based models [72; 73]. For self-supervised learning, works exist to analyze linear networks  and understand the role played by nonlinearity . Focusing on attention-based models, Zhang et al.  study adaptive optimization methods in attention models. Jelassi et al.  propose an idealized setting and show the vision transformer  trained by gradient descent can learn spatial structure. Li et al.  show that the 1-layer Transformer can learn a constrained topic model, in which any word belongs to one topic, with \(_{2}\) loss, BERT -like architecture and additional assumptions on learned attention patterns. Snell et al.  study the dynamics of a single-head attention head to approximate the learning of a Seq2Seq architecture. While these papers also study the optimization dynamics of attention-based models, they focus on different settings and do not explain the phenomena presented in our paper.

## 3 Problem Setting

**Notation.** Let \(\{_{k}\}_{k=1}^{M}\) be \(d\)-dimensional embeddings, and \(\{x_{t}\}\) be discrete tokens. For each token, \(x_{t}\) takes discrete values from \(1\) to \(M\), denoted as \(x_{t}[M]\), and \(_{t}:=_{x_{t}}^{M}\) is the corresponding one-hot vector, i.e., the \(x_{t}\)-th entry of \(_{t}\) is \(1\) while others are zero. \(_{x_{t}}\) is the token embedding at location \(t\) in a sequence.

Let \(U=[_{1},,_{M}]^{}^{M d}\) be the embedding matrix, in which the \(k\)-th row of \(U\) is the embedding vector of token \(k\). \(X=[_{1},,_{T-1}]^{}^{(T-1) M}\) is the data matrix encoding the sequence of length \(T-1\). \(XU^{(T-1) d}\) is the sequence of embeddings for a given sequence \(\{x_{1},,x_{T-1}\}\). It is clear that \(X_{M}=_{T-1}\).

We use \(X[i]\) to denote \(i\)-th sample in the sequence dataset. Similarly, \(x_{t}[i]\) is the token located at \(t\) in \(i\)-th sample. Let \(\) be the dataset used for training.

**1-Layer Transformer Architecture**. Given a sequence \(\{x_{1},,x_{T},x_{T+1}\}\), the embedding after \(1\)-layer self attention is:

\[}_{T}=_{t=1}^{T-1}b_{tT}_{x_{t}}, b_{tT}:=_{x_{T}}^{}W_{Q}W_{K}^{}_{x_{t}} /)}{_{t=1}^{T-1}(_{x_{T}}^{}W_{Q}W_{K}^{}_ {x_{t}}/)}\] (1)

Here \(b_{tT}\) is the normalized self-attention weights (\(_{t=1}^{T-1}b_{tT}=1\)). One important detail is that we mask the weight that the query token attends to itself, which is also being used in previous works (e.g., QK-shared architecture ). See Sec. 7 for discussions about residual connection. Let \(_{T}:=[b_{1T},,b_{T-1,T}]^{}^{T-1}\) be an attention vector, then \(_{T}^{}=1\) and \(}_{T}=U^{}X^{}_{T}\).

\(_{2}\)**-Normalization**. We consider adding a normalization to the output of the self-attention layer: \(}_{T}=U^{}(X^{}_{T})\), where \(():=/\|\|_{2}\). NormFormer and RMSNorm  also leverages this setting (up to a global constant). Our analysis can also be extended to standard LayerNorm , which also subtracts the mean of \(\), while  shows that mean subtraction may not affect the empirical results much. LLaMA  also uses RMSNorm. Empirically \(}_{T}\) (or \(W_{V}}_{T}\)) is normalized (instead of \(X^{}_{T}\)) and here we use an approximation to facilitate analysis: when the token embedding \(\{_{m}\}\) are approximately orthogonal to each other, then \(\|U^{}\|_{2}\|\|_{2}\) and thus \(}_{T}(}_{T})\).

**Objective.** We maximize the likelihood of predicted \((T+1)\)-th token using cross entropy loss:

\[_{W_{K},W_{Q},W_{V},U}J:=_{}[_{x_{T+1}}^{ }W_{V}}_{T}-_{l}(_{l}^{}W_{V}}_{T})]\] (2)

For simplicity, we consider single-head attention setting, and multiple-head attention can be regarded as single-head setting with simultaneous different initializations (see Sec. 4). We call \(x_{T}=m\) as the **query token** of the sequence, and \(x_{T+1}=n\) as the **next token** to be predicted. Other tokens \(x_{t}\) (\(1 t T-1\)) that are encoded in \(X\) are called **contextual tokens**. Both the contextual and query tokens can take values from \(1\) to \(M\) (i.e., \(m[M]\)) and next token takes the value from \(1\) to \(K\) (i.e., \(n[K]\)) where \(K M\). Fig. 1(a) shows the overall setting. For an overview of the notation used in the paper, please check Tbl. 1 in the Appendix.

### Reparameterization

Instead of studying the dynamics with respect to the parameters of token embedding \(U\), key, value and query projection matrices \(W_{K}\), \(W_{Q}\) and \(W_{V}\), we study the dynamics of two _pairwise token relation matrices_\(Y:=UW_{V}^{}U^{}^{M M}\) and \(Z:=UW_{Q}W_{K}^{}U^{}/^{M M}\). Intuitively, entries of \(Y\) and \(Z\) store the "logits" of pairs of tokens. We regard the empirical parameterization using \(U\), \(W_{K}\), \(W_{Q}\) and \(W_{V}\) as a specific way of parametrization of \(Y\) and \(Z\), in order to reduce the number of parameters to be estimated. Previous work also leverage similar parameterization for self-attention layers .

For real-world applications, the number of tokens \(M\) can be huge (e.g., the vocabulary size \(M=50,272\) in OPT-175B  and \(M=32,000\) in LLaMA ) and directly optimizing \(Y\) and \(Z\) would be prohibitive. However, as we will show in this work, from the theoretical perspective, treating \(Y\) and \(Z\) as independent variables has some unique advantages and leads to useful insights.

**Lemma 1** (Dynamics of 1-layer Transformer).: _The gradient dynamics of Eqn. 2 with batchsize 1 is:_

\[=_{Y}(X^{}_{T})(_{T+1}-)^{},=_{Z}_{T}(_{T+1}-)^{}Y^{}_{T}}^{}}{\|X^{}_{T}\|_{2}}X^{}(_{T})X\] (3)

_Here \(P_{}^{}:=I-^{}/\|\|_{2}^{2}\) projects a vector into \(\)'s orthogonal complementary space, \(_{Y}\) and \(_{Z}\) are the learning rates for the decoder layer \(Y\) and self-attention layer \(Z\), \(:=[_{1},,_{M}]^{}^{M}\) and \(_{m}:=(Y^{}(X^{}_{T}))/^{}(Y ^{}(X^{}_{T}))\)._

Please check Appendix C for the proof. We consider \(Y(0)=Z(0)=0\) as initial condition. This is reasonable since empirically \(Y\) and \(Z\) are initialized by inner product of \(d\)-dimensional vectors whose components are independently drawn by i.i.d Gaussian. This initial condition is also more realistic than  that assumes dominant initialization in diagonal elements. Since \((_{T+1}-)^{}=0\) and \(P_{X^{}_{T}}^{}X^{}(_{T})X=0\), we have \(==0\) and summation of rows of \(Z(t)\) and \(Y(t)\) remains zero. Since \(_{T}\) is a one-hot column vector, the update of \(Z=[_{1},_{2},,_{M}]^{}\) is done per row:

\[}_{m}=_{Z}X^{}[i](_{T}[i])X[i][i]_{T}[i]}^{}}{\|X^{}[i]_{T}[i]\|_{2}}Y(_{T+1 }[i]-[i])\] (4)

where \(m=x_{T}[i]\) is the query token for sample \(i\), \(_{m}\) is the \(m\)-th row of \(Z\) and \(}_{m^{}}=0\) for row \(m^{} m=x_{T}[i]\). Note that if \(x_{T}[i]=m\), then \(b_{T}[i]\) is a function of \(_{m}\) only (but not a function of any other \(_{m^{}}\)). Here we explicitly write down the current sample index \(i\), since batchsize is \(1\).

### Data Generation

Next we specify a data generation model (Fig. 1(b)), named _sequence class_, for our analysis.

**Sequence Class.** We regard the input data as a mixture of multiple _sequence classes_. Each sequence class is characterized by a triple \(s_{m,n}:=((l|m,n),m,n)\). To generate a sequence instance from the class, we first set \(x_{T}=m\) and \(x_{T+1}=n\), and then generate the contextual tokens with conditional probability \((l|m,n)\). Let \((m,n)\) be the subset of token \(l\) with \((l|m,n)>0\).

In this work, we consider the case that given a next token \(x_{T+1}=n\), the corresponding sequence always ends with a specific query token \(x_{T}=m=:(n)\). This means that we could index sequence class with next token \(x_{T+1}=n\) alone: \(s_{n}:=((l|(n),n),(n),n)\), \((l|m,n)=(l|n)\) and \((n):=((n),n)\).

Note that \(|^{-1}(m)|=1\) means that the occurrence of token \(m\) alone decides next token \(n\) to be predicted, regardless of other tokens in the sequence, which is a trivial case. When \(|^{-1}(m)| 2\), the same query token \(m\), combined with other token \(l\) in the sequence with non-zero probability \((l|m,n)>0\), determine the next token.

**Overlapping sequence class**. Two sequence classes \(s_{n}\) and \(s_{n^{}}\)_overlap_ if \((n)(n^{})\).

Figure 1: Overall of our setting. **(a)** A sequence with contextual tokens \(\{x_{1},,x_{T-1}\}\) and query token \(x_{T}\) is fed into 1-layer transformer (self-attention, normalization and decoding) to predict the next token \(x_{T+1}\). **(b)** The definition of sequence classes (Sec. 3.2). A sequence class specifies the conditional probability \((l|m,n)\) of the contextual tokens, given the query token \(x_{T}=m\) and the next token \(x_{T+1}=n\). For simplicity, we consider the case that the query token is determined by the next token: \(x_{T}=(x_{T+1})\) (and thus \((l|m,n)=(l|n)\)), while the same query token \(m\) may correspond to multiple next tokens (i.e., \(^{-1}(m)\) is not unique). We study two kinds of tokens: \(\) (CT) with \((l|n)>0\) for multiple sequence class \(n\), and distinct tokens (DT) with \((l|n)>0\) for a single sequence class \(n\) only.

**(Global) distinct and common tokens**. Let \((l):=\{n:(l|n)>0\}\) be the subset of next tokens that co-occur with contextual token \(l\). We now can identify two kinds of tokens: the _distinct_ token \(l\) which has \(|(l)|=1\) and the _common_ token \(l\) with \(|(l)|>1\). Intuitively, this means that there exists one common token \(l\) so that both \((l|n)\) and \((l|n^{})\) are strictly positive, e.g., common words like 'the', 'this', 'which' that appear in many sequence classes. In Sec. 5, we will see how these two type of contextual tokens behave very differently when self-attention layer is involved in the training: distinct tokens tend to be paid attention while common tokens tend to be ignored.

### Assumptions

To make our analysis easier, we make the following assumptions:

**Assumption 1**.: _We consider **(a)** no positional encoding, **(b)** The input sequence is long (\(T+\)) and **(c)** The decoder layer learns much faster than the self-attention layer (i.e., \(_{Y}_{Z}\))._

Assumption 1(a) suggests that the model is (almost) permutation-invariant. Given the next token to predict \(x_{T+1}=n\) and the query token \(x_{T}=m\) acted as query, the remaining tokens in the sequence may shuffle. Assumption 1(b) indicates that the frequency of a token \(l\) in the sequence approaches its conditional probability \((l|m,n):=(l|x_{T}=m,x_{T+1}=n)\).

Note that the assumptions are comparable with or even weaker than previous works, e.g.,  analyzes positional attention with symmetric initialization, without considering input data and  models the data distribution as discriminative/non-discriminative patterns, similar to our common/distinct tokens. Empirically, NoPE  shows that decoder-only Transformer models without positional encoding still works decently, justifying that Assumption 1(a) is reasonable.

Given the event \(\{x_{T}=m,x_{T+1}=n\}\), suppose for token \(l\), the conditional probability that it appears in the sequence is \((l|m,n)\). Then for very long sequence \(T+\), in expectation the number of token \(l\) appears in a sequence of length \(T\) approaches \(T(l|m,n)\). Therefore the _per-token_ self-attention weight \(c_{l|m,n}\) is computed as:

\[c_{l|m,n}:=(l|m,n)(z_{ml})}{_{l^{}}T( l^{}|m,n)(z_{ml^{}})}=(l|m,n)(z_{ml})}{ _{l^{}}(l^{}|m,n)(z_{ml^{}})}=:_{l|m,n}}{_{l^{}}_{l^{}|m,n}}\] (5)

Here \(z_{ml}\) is \(_{m}\)'s \(l\)-th entry and \(_{l|m,n}:=(l|m,n)(z_{ml})\) is un-normalized attention score.

**Lemma 2**.: _Given the event \(\{x_{T}=m,x_{T+1}=n\}\), when \(T+\), we have_

\[X^{}_{T}_{m,n}, X^{}(_{T})X(_{m,n})\] (6)

_where \(_{m,n}=[c_{1|m,n},c_{2|m,n},,c_{M|m,n}]^{}^{M}\). Note that \(_{m,n}^{}=1\)._

By the data generation process (Sec. 3.2), given the next token \(x_{T+1}=n\), the query token \(x_{T}=m\) is uniquely determined. In the following, we just use \(_{n}\) to represent \(_{m,n}\) (and similar for \(}_{n}\)).

## 4 Dynamics of \(Y\)

We first study the dynamics of \(Y\). From Assumption 1(c), \(Y\) learns much faster and we can treat the lower layer output (i.e., \(X^{}_{T}\)) as constant. From Lemma 2, when the sequence is long, we know

Figure 2: Overview of the training dynamics of self-attention map. Here \(_{l|m,n}:=(l|m,n)(z_{ml})\) is the un-normalized attention score (Eqn. 5). **(a)** Initialization stage. \(z_{ml}(0)=0\) and \(_{l|m,n}=(l|m,n)\). Distinct tokens (Sec. 3.2) shown in blue, common tokens in yellow. **(b)** Common tokens (CT) are suppressed (\(z_{ml}<0\), Theorem 2). **(c)** Winners-take-all stage. Distinct tokens (DT) with large initial value \(_{l|m,n}(0)\) start to dominate the attention map (Sec. 5, Theorem 3). **(d)** Once passing the phase transition, i.e., \(t t_{0}=O(K M/_{Y})\), attention appears (almost) frozen (Sec. 6) and token composition is fixed in the self-attention layer.

given the next token \(x_{T+1}=n\), \(X^{}_{T}\) becomes fixed. Therefore, the dynamics of \(Y\) becomes:

\[=_{Y}_{n}(_{n}-_{n})^{},_{ n}=_{n})}{^{}(Y^{}_{n})}\] (7)

Here \(_{n}:=_{T}}{\|X^{}_{T}\|_{2}} _{n}}{\|_{n}\|_{2}}^{M}\). Obviously \(\|_{n}\|_{2}=1\) and \(_{n} 0\). Define \(F=[_{1},,_{K}]\). Since the vocabulary size \(M\) typically is a huge number, and different sequence classes can cover diverse subset of vocabulary, we study the weak correlation case:

**Assumption 2** (Weak Correlations).: _We assume \(M K^{2}\) and \(\{_{n}\}_{n=1}^{K}\) satisfies \(F^{}F=I+E\), where the eigenvalues of \(E^{K K}\) satisfies \(|_{1}|<\) and \(|_{i}(E)|}, i[K]\)._

Assumption 2 means that \(_{n}\) share some weak correlations and it immediately leads to the fact that \(F^{}F\) is invertible and \(F\) is column full-rank. Note that the critical point \(Y^{*}\) of Eqn. 7 should satisfy that for any given \(x_{T+1}=n\), we need \(=_{n}\). But such \(Y^{*}\) must contain infinity entries due to the property of the exponential function in \(\) and we can not achieve \(Y^{*}\) in finite steps. To analyze Eqn. 7, we leverage a _reparameterized_ version of the dynamics, by setting \(W=[_{1},,_{K}]^{}:=F^{}Y^{K M}\) and compute gradient update on top of \(W\) instead of \(Y\):

**Lemma 3**.: _Given \(x_{T+1}=n\), the dynamics of \(W\) is (here \(_{j}=(_{j})/^{}(_{j})\)):_

\[}_{j}=_{Y}(j=n)(_{n}-_{n})\] (8)

_While we cannot run gradient update on \(W\) directly, it can be achieved by modifying the gradient of \(Y\) to be \(=_{Y}(_{n}-FE^{}_{n})(_{n}-_{n} )^{}\). If \(_{1}\) is small, the modification is small as well._

Please check Appendix D for the proof. Lemma 3 shows that for every fixed \(n\), only the corresponding row of \(W\) is updated, which makes the analysis much easier. We now can calculate the backpropagated gradient used in Eqn. 3.

**Theorem 1**.: _If Assumption 2 holds, the initial condition \(Y(0)=0\), \(M 100\), \(_{Y}\) satisfies \(M^{-0.99}_{Y}<1\), and each sequence class appears uniformly during training, then after \(t K^{2}\) steps of batch size \(1\) update, given event \(x_{T+1}[i]=n\), the backpropagated gradient \([i]:=Y(_{T+1}[i]-[i])\) takes the following form:_

\[[i]=(_{n}_{n}-_{n^{} n}_{nn^{ }}_{n^{}})\] (9)

_Here the coefficients \(_{n}(t)\), \(_{nn^{}}(t)\) and \((t)\) are defined in Appendix with the following properties:_

* _(a)_ \(_{n}(t):=(t)_{n n^{}}_{nn^{}}(t)_{n}^ {}(t)_{n^{}}(t)>0\) _for any_ \(n[K]\) _and any_ \(t\)_;_
* _(b)_ _The_ speed control coefficient \((t)>0\) _satisfies_ \((t)=O(_{Y}t/K)\) _when_ \(t}\) _and_ \((t)=O(t/K)}{_{Y}t})\) _when_ \(t)(M) K}{_{Y}}\) _with_ \(^{}=()\)_._

In the remark of Lemma 5 in Appendix, we analyze the original dynamics (Eqn. 7) with identical off-diagonal elements of \(E\), and Theorem 1 still holds with a smaller effective learning rate.

## 5 The dynamics of Self-attention

Now we analyze the dynamics of self-attention logits \(Z\), given the dynamics of upper layer \(Y\).

**Lemma 4** (Self-attention dynamics).: _With Assumption 1(b) (i.e., \(T+\)), Eqn. 4 becomes:_

\[}_{m}=_{Z}_{n^{-1}(m)}(_{n} )_{n^{} n}_{nn^{}}(_{n}_{n}^{}-I) {f}_{n^{}},\] (10)

Please check Appendix E for the proof. Now we study the dynamics of two types of contextual tokens (Sec. 3.2), namely _distinct tokens_ (DT) which appear only for a single next token (i.e., \(|(l)|=1\) with \((l):=\{n:(l|n)>0\}\)), and _common tokens_ (CT) that appear across multiple next tokens (\(|(l)|>1\)). We show their fates are very different: over training, _distinct tokens gain attention but common ones lose it_.

**Theorem 2** (Fates of contextual tokens).: _Let \(G_{CT}\) be the set of common tokens (CT), and \(G_{DT}(n)\) be the set of distinct tokens (DT) that belong to next token \(n\). Then if Assumption 2 holds, under the self-attention dynamics (Eqn. 10), we have:_

* _(a)_ _for any distinct token_ \(l G_{DT}(n)\)_,_ \(_{ml}>0\) _where_ \(m=(n)\)_;_
* _(b)_ _if_ \(|G_{CT}|=1\) _and at least one next token_ \(n^{-1}(m)\) _has at least one distinct token, then for the single common token_ \(l G_{CT}\)_,_ \(_{ml}<0\)_._

Now we know DTs grow and a single CT will shrink. For multiple CTs to shrink, the condition can be a bit involved (see Corollary 2 in Appendix E). The following theorem further shows that the growth rates of DTs critically depend on their initial conditions:

**Theorem 3** (Growth of distinct tokens).: _For a next token \(n\) and its two distinct tokens \(l\) and \(l^{}\), the dynamics of the **relative gain**\(r_{l/l^{}|n}(t):=f_{nl}^{2}(t)/f_{nl^{}}^{2}(t)-1=_{l|n }^{2}(t)/_{l^{}|n}^{2}(t)-1\) has the following analytic form (here the query token \(m=(n)\) and is uniquely determined by distinct token \(l\)):_

\[r_{l/l^{}|n}(t)=r_{l/l^{}|n}(0)e^{2(z_{ml}(t)-z_{ml}(0))}:=:r_{l/l ^{}|n}(0)_{l}(t)\] (11)

_where \(_{l}(t):=e^{2(z_{ml}(t)-z_{ml}(0))}\) is the **growth factor** of distinct token \(l\). If there exist a dominant token \(l_{0}\) such that the initial condition satisfies \(r_{l_{0}/l|n}(0)>0\) for all its distinct token \(l l_{0}\), and all of its common tokens \(l\) satisfy \(_{ml}<0\). Then both \(z_{ml_{0}}(t)\) and \(f_{nl_{0}}(t)\) are monotonously increasing over \(t\), and_

\[e^{2f_{nl_{0}}^{2}(0)B_{n}(t)}_{l_{0}}(t) e^{2B_{n}(t)}\] (12)

_here \(B_{n}(t):=_{Z}_{0}^{t}_{n}(t^{})t^{}\). Intuitively, larger \(B_{n}\) gives larger \(r_{l_{0}/l|n}\) and sparser attention map._

**Self-attention as an algorithm of token scanning**. From Eqn. 11, we could see that self-attention performs _token scanning_. To see that, consider the simplest initialization that \((0)=0\), which means that \(r_{l_{0}/l|n}(0)=((l_{0}|m,n)}{(l|m,n)})^{ 2}-1\). Therefore, distinct token \(l\) with low conditional probability \((l|m,n)\) will have \(r_{l_{0}/l|n}(0) 0\), According Eqn. 11, this leads to quickly growing ratio \(r_{l_{0}/l|n}(t)\), which means that the corresponding component \(f_{nl}\) will be quickly dwarfed by the dominating component \(f_{nl_{0}}\). On the other hand, token with high conditional probability \((l|m,n)\) will have smaller \(r_{l_{0}/l|n}(0)\), and the ratio \(r_{l_{0}/l|n}(t)\) grows slower, costing longer time for \(l_{0}\) to dominate \(l\).

**Initial value as prior information**. From the theorems, it is clear that the initial value \(r_{l/l^{}|n}(0):=((l|m,n)(z_{ml}(0))}{ (l|m,n)(z_{ml^{}}(0))})^{2}-1\) critically determines the fate of the dynamics. Two tokens \(l\) and \(l^{}\) with comparable conditional probability \((l|m,n)\) and \((l^{}|m,n)\) can be suppressed in either way, depending on their initial logits \(z_{ml}(0)\) and \(z_{ml^{}}(0)\). In the empirical implementation, the initial value of the logits are determined by the inner products of independently initialized high-dimensional vectors, which fluctuate around zero.

The concept of "initial value as prior" can explain empirical design choices such as _multi-head self-attention_. From this perspective, each head \(h\) has its own \(Z_{h}\) and is initialized independently, which could enable more diverse token combination (e.g., a combination of 1st, 3rd, 5th tokens, rather than a combination of 1st, 2nd, 3rd tokens).

## 6 The Moment of Snapping: When Token Combination is fixed

Theorem 3 suggests two possible fates of the self-attention weights: if \(_{n}(t)\) decays slowly (e.g., \(_{n}(t) 1/t\)), then \(B_{n}(t)+\) and all contextual tokens except for the dominant one will drop (i.e., \(f_{nl} 0\)) following the ranking order of their conditional probability \((l|m,n)\). Eventually, winner-takes-all happens. Conversely, if \(_{n}(t)\) drops so fast that \(B_{n}(t)\) grows very slowly, or even has an upper limit, then the self-attention patterns are "snapped" and token combination is learned and fixed.

The conclusion is not obvious, since \(_{n}(t)\) depends on the decay rate of \((t)\) and \(_{nn^{}}(t)\), which in turns depends on the inner product \(_{n}^{}(t)_{n^{}}(t)\), which is related to the logits of the common tokens that also decays over time.

Here we perform a qualitative estimation when there is only a single common token \(l\) and every next token shares a single token \(m\) (i.e., for any next token \(n\), \((n)=m\)). We assume all normalizationterms in \(_{n}\) are approximately constant, denoted as \(_{0}\), which means that \(_{n}^{}_{n^{}}(2z_{ml})/_{0}^{2}\) and \(_{nn^{}} E_{nn^{}}^{}_{n}^{}_{n^{}}(2z_{ml})/_{0}^{2}\) as well, and \(1-_{n}^{}_{n^{}} 1\) due to the fact that common token components are small, and will continue to shrink during training.

Under these approximations, its dynamics (Eqn. 10) can be written as follows (here \(C_{0}:=_{0}^{4}/K\)):

\[_{ml}=_{Z}_{n^{-1}(m)}f_{nl}_{n^{} n }_{nn^{}}(f_{nl}^{2}-1)f_{nl^{}}-C_{0}^{-1}_{Z}  e^{4z_{ml}},_{n}(t) C_{0}^{-1} e^{4z_{ml}}\] (13)

Surprisingly, we now find a _phase transition_ by combining the rate change of \((t)\) in Theorem 1:

**Theorem 4** (Phase Transition in Training).: _If the dynamics of the single common token \(z_{ml}\) satisfies \(_{ml}=-C_{0}^{-1}_{Z}(t)e^{4z_{ml}}\) and \(_{n}(t)=C_{0}^{-1}(t)e^{4z_{ml}}\), then we have:_

\[B_{n}(t)=\{(C_{0}+ }{KM^{2}}_{Y}_{Z}t^{2})&t<t_{0}^{}:= }\\ (C_{0}+}{M^{2}}}{_{Y}} ^{2}(M_{Y}t/K))&t t_{0}:=} .\] (14)

_As a result, there exists a phase transition during training:_

* _Attention scanning. At the beginning of the training,_ \((t)=O(_{Y}t/K)\) _and_ \(B_{n}(t) K^{-1}(_{0}^{4}+2_{Y}_{Z}t^{2})=O(  t)\)_. This means that the growth factor for dominant token_ \(l_{0}\) _is (sub-)linear:_ \(_{l_{0}}(t) e^{2f_{nl_{0}}^{2}(0)B_{n}(t)}[K^{-1}(_{0}^{4} +2_{Y}_{Z}t^{2})]^{0.5f_{nl_{0}}^{2}(0)}\)_, and the attention on less co-occurred token drops gradually._
* _Attention snapping. When_ \(t t_{0}:=2(1+^{})K M/_{Y}\) _with_ \(^{}=()\)_,_ \((t)=O(t/K)}{_{Y}t})\) _and_ \(B_{n}(t)=O( t)\)_. Therefore, while_ \(B_{n}(t)\) _still grows to infinite, the growth factor_ \(_{l_{0}}(t)=O( t)\) _grows at_ a much slower _logarithmic rate._

See proof in Appendix F. This gives a few insights about the training process: **(a)** larger learning rate \(_{Y}\) of the decoder \(Y\) leads to shorter phase transition time \(t_{0} 2K M/_{Y}\), **(b)** scaling up both learning rate (\(_{Y}\) and \(_{Z}\)) leads to larger \(B_{n}(t)\) when \(t+\), and thus sparser attention maps, and **(c)** given fixed \(_{Z}\), small learning rate \(_{Y}\) leads to larger \(B_{n}(t)\) when \(t t_{0}\), and thus sparser attention map. Fig. 3 shows numerical simulation results of the growth rate \(_{l}(t)\). Here we set \(K=10\) and \(M=1000\), and we find smaller \(_{Y}\) given fixed \(_{Z}\) indeed leads to later transition and larger \(B_{n}(t)\) (and \(_{l}(t)\)).

## 7 Discussion and Limitations

**Positional encoding**. While our main analysis does not touch positional encoding, it can be added easily following the relative encoding schemes that adds a linear bias when computing self attention (E.g., T5 , ALBi , MusicTransformer ). More specifically, the added linear bias \((z_{ml}+z_{0})=(z_{ml})(z_{0})\) corresponds to a prior of the contextual token to be learned in the self-attention layer.

Figure 3: Growth factor \(_{l}(t)\) (Theorem 3) over time with fixed \(_{Z}=0.5\) and changing \(_{Y}\). Each solid line is \(_{l}(t)\) and the dotted line with the same color corresponds to the transition time \(t_{0}\) for a given \(_{Y}\).

**Residue connection**. Residue connection can be added in the formulation, i.e., \(}_{T}=((}_{T})+_{x_{T}})\), where \(}_{T}\) is defined in Eqn. 1, and \(}_{T}\) is used instead in the objective (Eqn. 2). In this case, the \(_{nn^{}}\) in Theorem 1 now is approximately \(_{nn^{}}_{n}^{}_{n^{}}+((n )=(n^{}))\), which is much larger for sequence classes \(n\) and \(n^{}\) that share the same query token \(x_{T}\) than otherwise. In this case, Theorem 1 now gives \([i]=(_{n}_{n}-_{n n^{}^{-1}( (n))}_{nn^{}}_{n^{}})\) for \(x_{T+1}[i]=n\). Due to the additional constraint \(n^{}^{-1}((n))\) (i.e., \(n\) and \(n^{}\) shares the same query token), we can define _local_ distinct and common tokens to be _within_ the sequence class subset \(^{-1}(m)\) and Theorem 2 now applies within each subset. Empirically this makes more sense, since the query token \(x_{T}=m_{1}\) or \(m_{2}\) alone can already separate different subsets \(^{-1}(m_{1})\) and \(^{-1}(m_{2})\) and there should not be any interactions across the subsets. Here we just present the most straightforward analysis and leave this extension for future work.

**Possible future extension to multi-layer cases**. For multilayer training, a lasting puzzle is to explain how the input tokens get combined together to form high-level concepts. The analysis above shows that the training leads to sparse attention even among relevant tokens, and demonstrates that there is a priority in token combinations for 1-layer attention based on their co-occurrence: even if there are \(10\) relevant contextual tokens to the query, the self-attention may only pick 1-2 tokens to combine first due to attention sparsity. This can be regarded as a starting point to study how tokens are composed hierarchically. In comparison, [28; 29; 30] show that attention attends to all relevant tokens, which may not suggest a hierarchical / multi-layer architecture.

## 8 Experiments

We conduct experiments on both synthetic and real-world dataset to verify our theoretical findings.

**Syn-Small**. Following Sec. 3.2, we construct \(K=2\) sequence classes with vocabulary size \(M=30\). The first \(10\) tokens (0-9) are shared between classes, while the second and third \(10\) tokens (10-19 and 20-29) are distinct for class 1 and class 2, respectively. The conditional probability \((l|n)\) for tokens 10-19 is increasing monotonously (the same for 20-29). The 1-layer Transformer is parameterized with \(Y\) and \(Z\) (Sec. 3.1), is trained with initial condition \(Y(0)=Z(0)=0\) and SGD (with momentum \(0.9\)) using a batchsize of 128 and sequence length \(T=128\) until convergence.

Fig. 4 shows the simulation results. The attention indeed becomes sparse during training, and increasing \(_{Y}\) with fixed \(_{Z}\) leads to faster convergence but less sparse attention. Both are consistent with our theoretical predictions (Theorem 3 and Sec. 6). Interestingly, if we use Adam optimizer instead, self-attention with different learning rate \(_{Y}=_{Z}\) picks different subsets of distinct tokens to focus on, showing tune-able inductive bias (Fig. 5). We leave analysis on Adam for future work.

**Syn-Medium**. To further verify our theoretical finding, we now scale up \(K\) to create Syn-Medium and compute how attention sparsity for distinct tokens (in terms of entropy) changes with the learning rates (Fig. 6). We can see indeed the entropy goes down (i.e., attention becomes sparser) with larger \(_{Z}\), and goes up (i.e., attention becomes less sparse) by fixing \(_{Z}\) and increasing \(_{Y}\) passing the threshold \(_{Y}/_{Z} 2\), consistent with Sec. 6. Note that the threshold is due to the fact that our theory is built on Assumption 1(c), which requires \(_{Y}\) to be reasonably larger than \(_{Z}\).

Figure 4: Visualization of \(_{n}\) (\(n=1,2\)) in the training dynamics of 1-layer Transformer using SGD on Syn-Small setting. Top row for query token \(n=1\) and bottom row for query token \(n=2\). **Left:** SGD training with \(_{Y}=_{Z}=1\). Attention pattern \(_{n}\) becomes sparse and concentrated on highest \((l|n)\) (rightmost) for each sequence class (Theorem 3). **Right:** SGD training with \(_{Y}=10\) and \(_{Z}=1\). With larger \(_{Y}\), convergence becomes faster but the final attention maps are less sparse (Sec. 6).

**Real-world Dataset**. We also test our finding on WikiText  using both 1-layer and multi-layer Transformers with regular parameterization that computes \(Y\) and \(Z\) with embedding \(U\). In both cases, attentions of the first layer freeze (and become sparse) at some point (Fig. 7), even if the learning rate remains the same throughout training. More results are in Appendix G.

## 9 Conclusion and Future Work

In this work, we formally characterize SGD training dynamics of 1-layer Transformer, and find that the dynamics corresponds to a _scan and snap_ procedure that progressively pays more attention to key tokens that are distinct and frequently co-occur with the query token in the training set. To our best knowledge, we are the first to analyze the attention dynamics and reveal its inductive bias on data input, and potentially open a new door to understand how Transformer works.

Many future works follow. According to our theory, large dataset suppresses spurious tokens that are perceived as distinct in a small dataset but are actual common ones. Our finding may help suppress such tokens (and spurious correlations) with prior knowledge, without a large amount of data.

Figure 5: Visualization of (part of) \(_{n}\) for sequence class \(n=1\) in the training dynamics using Adam  on Syn-Small setting. **From left to right**: \(_{V}=_{Z}=0.1,0.5,1\). With different learning rate Adam seems to steer self-attention towards different subset of distinct tokens, showing tune-able inductive bias.

Figure 6: Average entropy of \(_{n}\) (Eqn. 5) on distinct tokens versus learning rate ratio \(_{Y}/_{Z}\) with more query tokens \(M\)/next tokens \(K\). We report mean values over 10 seeds and standard derivation of the mean.

Figure 7: Attention patterns in the lowest self-attention layer for 1-layer (top) and 3-layer (bottom) Transformer trained on WikiText2 using SGD (learning rate is 5). Attention becomes sparse over training.