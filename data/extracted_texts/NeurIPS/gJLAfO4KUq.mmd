# Pengi: An Audio Language Model for Audio Tasks

Soham Deshmukh\({}^{1}\)  Benjamin Elizalde\({}^{1}\)  Rita Singh\({}^{2}\)  Huaming Wang\({}^{1}\)

\({}^{1}\)Microsoft \({}^{2}\)Carnegie Mellon University

{sdeshmukh, benjaminm, huawang}@microsoft.com, rsingh@cs.cmu.edu

###### Abstract

In the domain of audio processing, Transfer Learning has facilitated the rise of Self-Supervised Learning and Zero-Shot Learning techniques. These approaches have led to the development of versatile models capable of tackling a wide array of tasks, while delivering state-of-the-art performance. However, current models inherently lack the capacity to produce the requisite language for open-ended tasks, such as Audio Captioning or Audio Question Answering. We introduce Pengi, a novel Audio Language Model that leverages Transfer Learning by framing all audio tasks as text-generation tasks. It takes as input, an audio recording, and text, and generates free-form text as output. The input audio is represented as a sequence of continuous embeddings by an audio encoder. A text encoder does the same for the corresponding text input. Both sequences are combined as a prefix to prompt a pre-trained frozen language model. The unified architecture of Pengi enables open-ended tasks and close-ended tasks without any additional fine-tuning or task-specific extensions. When evaluated on 21 downstream tasks, our approach yields state-of-the-art performance in several of them. Our results show that connecting language models with audio models is a major step towards general-purpose audio understanding 1.

## 1 Introduction

Machine Listening breaks down audio understanding into separate and independent audio tasks. For example, Sound Event and Scene Classification, Audio Retrieval, and Audio Captioning. Because these audio tasks are intrinsically related, we can leverage from Transfer Learning (TL). TL focuses on applying knowledge gained while solving one task to solve a related task. The learning method involves pre-training a model with a large compilation of datasets from different tasks followed by fine-tuning on a target dataset. These models have shown the potential to learn general-purpose audio

Figure 1: Examples of audio and text prompt inputs and their corresponding textual responses. Images are for illustration purposes only. Our proposed model Pengi enables close-ended tasks, such as classification or retrieval and open-ended tasks, such as captioning or question & answering.

representations  that can successfully be used in a variety of downstream tasks. To leverage from larger amounts of audio that is unlabeled, the community has employed Self-Supervised and Unsupervised Learning [50; 51; 44; 19]. These methods do not require labels [53; 7] and have achieved state-of-the-art performance. However, both methods require an additional fine-tuning step before they can be applied to any downstream task.

To address this drawback, another Transfer Learning (TL) method called Zero-Shot Learning provides direct inference capabilities and removes the need of fine-tuning. These models use contrastive objectives to learn the similarity between natural language descriptions and audio content to provide a score that identifies the most probable class label for a given testing audio. Examples are CLAP , Mulan , and LAION-CLAP . Despite not seeing the training data of a target task, Zero-Shot models achieve surprising performance in close-ended tasks, such as classification and retrieval. However, these models inherently lack the capacity to produce the requisite language for open-ended tasks, such as Audio Captioning or Audio Question Answering (AQA).

Current audio models that can perform open-ended tasks do not support or have not been evaluated on closed-ended tasks [37; 31]. It is yet to be explored how to leverage TL to enable both types of tasks in the audio domain. We drew inspiration from recent advances in Natural Language Processing (NLP) and Visual Language Models (VLM). In NLP, Raffel et. al.  explored a unified framework called T5 where all text-based tasks are framed as text input to text output problems. T5 was trained with a single objective function and supported a diverse set of tasks, like translation, question & answering, and classification. FLAN  showed that language models trained on a collection of text tasks phrased as instructions, enabled models to respond better to similar instructions at inference time. This TL technique showed performance improvement across a range of models, prompting setups, and evaluation tasks. On the other hand, VLM incorporates visual information by combining a language model and an image encoder to transfer knowledge across modalities. Tasks are framed as text and image input to text output problems. Captioning training consists of optimizing a text generation objective, and can transfer moderately well to visual question & answering in the zero-shot settings. Examples include, Frozen , Flamingo , and other models [54; 52; 2; 42; 39]. But their performance on close-ended tasks still lags behind contrastive models [47; 59]. In the audio domain, there are no models that resemble any of these capabilities, let alone that support both close-ended and open-ended audio tasks simultaneously.

In this paper, we introduce Pengi, a novel Audio Language Model (ALM) that takes as input, an audio recording and a text prompt, and generates free-form text as output. To the best of our knowledge, the following contributions are achieved for the first time in the literature:

* A novel Audio Language Model capable of supporting multiple close-ended and open-ended audio tasks without any additional fine-tuning or task-specific extensions of the architecture. Pengi draws inspiration from VLM but tackles intrinsic challenges in the audio domain.
* We propose a new learning framework where we frame all audio tasks as audio and text input to text output tasks. Our framework uses a single training procedure and a captioning objective function. For training, we designed new audio task templates inspired by Instruction Tuning.
* We extensively evaluated Pengi on 21 downstream tasks across various audio domains yielding state-of-the-art performance in several of them. Thus, establishing a baseline for general-purpose ALM.

## 2 Related Work

**Audio Language Models.** In the domain of audio processing, Transfer Learning has facilitated the rise of Self-Supervised Learning and Zero-Shot Learning techniques [50; 51; 43; 22; 25; 24; 3; 15; 26; 23; 57; 58; 41; 12; 14; 16]. These approaches have led to the development of versatile models capable of tackling a wide array of tasks, while delivering SoTA performance. However, current models can tackle either close-ended tasks or open-ended tasks. ALM pose a new learning paradigm for audio processing that can support all tasks. The language modeling approaches to audio find utility in generating audio given an input description [4; 1]. But it is yet to be explored how to train them for general-purpose audio understanding and what their performance would be.

**Language Models**. Transfer Learning has been extensively utilized in Natural Language Processing with the recent shift to Zero-Shot and Few-Shot Learning [29; 48; 5; 55]. The work by Raffel et. al.  explored a unified framework for text tasks by converting all text-based tasks into the text-to-textformat. The experimental results showed the methods can achieve SoTA results when combined and scaled. FLAN  released in 2022 uses instruction fine-tuning to fine-tune an existing language model on a large set of varied instructions. Pengi adapts a similar idea for the audio domain, where each audio-tasks is considered a text generation task conditional on the input text and input audio. This allows audio tasks to be represented in (audio-text)-text format and enables learning a single unified model for all the tasks. For training, we created (audio-text)-text templates for audio tasks and trained Pengi with them.

**Visual Language Models.** Inspired by the success of Transfer Learning and Few-Shot Learning in NLP, a host of VLM were proposed for vision tasks. VLM intend to extend the pre-trained language model and adapt them to incorporate visual information. VisualBERT  and SimVLM  explored different ways to convert images into tokens and jointly train the model on interleaved images and text. Inspired by prefix-tuning  and prompt-tuning , Frozen  and Clipcap , use a frozen language model and align the image embeddings for the language model. To better fuse image information, Flamingo  uses a gated-cross-attention dense layer in the language model. The interleaved image-text training also enables Flamingo to do few-shot learning. Drawing parallels with VLM, Pengi can be considered an ALM based on _audio conditional prefix tuning_ where the prompt is produced by an audio encoder.

## 3 Approach

In this section, we describe Pengi, a novel Audio Language Model that leverages Transfer Learning by framing all audio tasks as text generation tasks. It takes as input, an audio recording and a text prompt, and generates free-form text as output. The unified architecture in Figure 2 enables open-ended tasks and close-ended tasks without any additional fine-tuning or task-specific extensions of the architecture.

### Unified Architecture

**Audio Encoder.** The audio encoder \(a_{}\) transforms the raw audio input into an audio embedding. We used the audio transformer backbone from CLAP  as our audio encoder due to its success in diverse audio and multimodal tasks. Models in Computer Vision  use a frozen image encoder like CLIP, but CLAP is trained on a magnitude smaller collection of audio-text pairs. Therefore, we unfree its weights for our training procedure.

Figure 2: \(\) Pengi has a unified architecture that takes as input, an audio recording and a text prompt, and generates free-form text as output. At training, the architecture learns an audio encoder \(a_{}\) and a mapping network \(m_{1}\) to represent an input audio as a sequence of continuous embeddings. A frozen text encoder \(g_{}\) and a learnable mapping \(m_{2}\) do the same for the corresponding text input. Both sequences are concatenated as a prefix to leverage from a pre-trained frozen autoregressive language model \(f_{}\) to perform multiple tasks. At inference, the language model generates tokens autoregressively conditioned on the audio and text input.

**Text Encoder.** The text encoder \(g_{}\) transforms the input text prompt into a text embedding. The prompt can be any form of natural language, such as a task-specific prompt or a question. The text encoder is frozen so its weights are not updated during training. The text encoder can be any off-the-shelf text encoder and allows our architecture to learn and perform well in close-ended tasks.

**Mapping Networks and Prefix.** To construct the prefix to be fed to the causal language model, we used two mapping networks (\(m_{1}\) and \(m_{2}\)). The mapping networks  convert an embedding into a sequence of \(k\) embeddings. The audio embedding is transformed by \(m_{1}\) and the text embedding by \(m_{2}\), both are trainable. Both sequences are concatenated to form the fixed-length prefix.

**Causal Language Model.** To generate the text output we used a pre-trained autoregressive causal language model which is kept frozen during training and inference . Even though the language model is frozen, the audio prefix receives gradients enabling the parameters of mapping network (\(m_{1}\)) and audio encoder \(a_{}\) to be optimized with gradient descent and backpropagation. At inference, the language model generates tokens autoregressively conditioned on the audio and text prefix.

### Training and Inference

We propose a new learning framework where we frame all audio tasks as audio and text input to text output tasks. Our framework uses a single training procedure and objective function. Let the training data in audio-text-to-text format be referred to as {\(x^{i}\),\(t^{i}\),\(c^{i}\)} where \(x^{i}\), \(t^{i}\) and \(c^{i}\) are the \(i^{th}\) audio file, \(i^{th}\) input text, and \(i^{th}\) output text or caption respectively.

To create a prefix, the audio encoder \(a_{}\) and mapping network \(m_{1}\) projects the audio \(x^{i}\) into a sequence of \(k\) embeddings. Similarly, the text encoder \(g_{}\) and mapping network \(m_{2}\) projects the input text \(t^{i}\) into a sequence of \(k\) embeddings. Both sequences are concatenated to form prefix \(p^{i}\) for the pre-trained frozen language model \(f_{}\).

\[p^{i}=p^{i}_{1},...,p^{i}_{2k}=\{m_{1}(a_{}(x^{i})),m_{2}(g_{ }(t^{i}))\} \]

The language model \(f_{}\) is fed with the prefix-caption concatenation of all \(\{z_{i}\}_{i=1}^{N}\), where \(z_{i}\) is:

\[z^{i}=p^{i}_{1},...,p^{i}_{2k},c^{i}_{1},...,c^{i}_{l} \]

The model is trained as a standard captioning system, where it learns to predict a caption (text tokens) \(c^{i}\) conditioned on the prefix in an autoregressive fashion. We used Cross-Entropy as the loss function:

\[=-_{i=1}^{N}_{j=1}^{l} p_{}(c^{i}_{j}|p^{i}_{1},..,p^{i}_{2k},c^{i}_{1},...,c^{i}_{j-1}) \]

where \(\) denotes model's trainable parameters which include audio encoder parameters \(\) and parameters from both mapping networks. The text encoder and the causal language model are frozen.

At inference time, the prefix is constructed using the test audio and a text prompt. The causal language model \(f_{}\) generates the next token sequentially conditioned on the prefix. The language model assigns probabilities to all vocabulary tokens at each prediction, which are used to determine the next token depending on the choice of decoding. In our experiments, we used beam search decoding with a beam size of 5 for inference and downstream tasks.

## 4 Experiments

### Training Datasets and Templates

Our Audio Language Model Pengi is trained on a collection of audio-text tasks phrased as instruction templates. The templates are inspired by instruction tuning and enable models to respond better to similar instructions at inference time. This TL technique is novel for audio and yielded performance improvement across a range of input prompting examples and downstream tasks.

The training datasets are modified to adapt to our proposed framework (audio-text)-to-text format by constructing 8 audio-task templates. Before our study, there was no evidence that the templates could lead to good performance across open- and close-ended tasks. Each template consists of audio input, input text prompt, and text output. Examples are "this is the sound of", "this emotion is" or "question: {question}". All the templates are in Table 1, out of which one template is the Auxiliary task "generate metadata". With it, we add audio-text pairs that are not task-specific. Drawing parallels, this training data setup is inspired by instruction tuning format of FLAN . Defining new templates or variations of the ones proposed here is a promising direction to explore.

The training data is collected from multiple audio datasets coming from different sources. In all, we collected 3.4 million audio-text pairs and mapped them to the 8 templates. The number of training pairs makes this model one of the largest if not the largest non-speech audio model in literature. We use only the training set of each dataset. The datasets and their mapping to a task are the following. Sound Event Classification: AudioSet , FSD50K; Acoustic Scene Classification: CoolScene ; Speech Emotion and Sentiment Recognition: MSP Podcast , CMU MOSI , CMU MOSEI , MELD ; Music Analysis: NSynth , FMA ; Audio Captioning: AudioCaps , ClothoV2 ; Audio Question and Answering: ClothoAQA ; Auxiliary: WavText5K , SoundDescs , MACS , WavCaps , FreeSound  and FindSound2.

### Downstream Tasks

The unified architecture of Pengi enables open-ended tasks and close-ended tasks.

**Open-ended tasks.** This task type requires free-form text generation and there is flexibility in the correctness of the output. Examples are Audio Captioning and AQ&A. Pengi will take as input the testing audio and the desired prompt to generate the text output. It does not require any additional fine-tuning or task-specific components.

**Close-ended tasks.** This task type is restricted to predefined values that can be classes or numbers. Examples are classification and retrieval. Pengi will take as input the testing audio and the desired prompt. Ideally, the free-form text output from Pengi should contain the exact predefined value. For example, a predefined class is "dog" but Pengi may output "dog barking" or "canine". Although these answers are reasonable, they are incorrect under most metrics. To evaluate the correctness, we proposed two methods: Log-likelihood and Text matching (Fig. 3). Unless explicitly mentioned, all experiments in our paper use the Text-matching method for evaluation.

_Log-likelihood:_ We take the concatenated prefix from a testing audio, the prompt, and append one of the predefined values (e.g class name, number) to create a candidate output. We would have \(N\) candidate outputs corresponding to \(N\) predefined values. For example in classification, if we have 100 testing audios and 5 classes, we would have 5 output candidates per audio. The outputs and the predefined values are used to compute Log-likelihood scores and determine the model's prediction. This method is expensive for the extensive evaluation in our study.

_Text-matching:_ In this setup, the free-form output is matched to the predefined values using text embeddings (Fig.3). For example, in a classification setting, we compute sentence-level text embeddings for Pengi's output and for all the class labels in a given dataset. Then, we calculate cosine similarity to determine the model's prediction. We used Pengi's text encoder to compute the embeddings, but any off-the-shelf text encoder could be used.

   Task & Input prompt & Output format \\  Audio & & \\ Captioning & & \\  Audio & & \\  QA & question: \{question\} & \{answer\} \\  Sound Event Classification & & \\  Acoustic Scene Classification & & \\   

Table 1: The training datasets are modified to adapt to our proposed framework (audio-text)-to-text format by constructing 8 audio-task templates. Each template consists of audio input, input text prompt, and text output. The \(\{\}\) symbol indicates variable content. The Auxiliary task template allowed us to add audio-text pairs that are not task-specific.

Figure 3: Text-matching method used during inference for close-ended tasks. TE indicates Text Embedding.

**Downstream tasks.** We used 21 downstream tasks (Table 2) to benchmark the open-ended and close-ended capabilities of Pengi. The open-ended tasks consist of Audio Captioning and AQA. The close-ended tasks consist of classification, regression, and retrieval. Datasets like Clotho have more than one type of annotations, so they are used for multiple tasks like Audio Captioning and Text-to-Audio Retrieval.

### Implementation details

**Encoders and mappers.** We used the audio transformer HTSAT as our audio encoder and CLIP's  text encoder. The audio is sampled at 44.1 kHz and is converted to a log Mel spectrograms with 64 Mel bins, a hop size of 320 ms, and a window size of 1024 ms in the range of 50-8000 Hz. We randomly truncated all audio files to 7 seconds in length for HTSAT. The max length of the text encoder is set to 40 for computational efficiency. We performed another step of CLAP (Contrastive Language-Audio Pretraining) training using the above two encoders . This enables experiments where the audio encoder can be kept frozen to see the utility of CLAP's  audio embeddings similar to VLM . The mapping networks \(m_{1}\) and \(m_{2}\) each use an 8-layer transformer with a prefix length of 40. The total prefix length after concatenating the audio and text is 80. The hyper-parameters of the encoders and the CLAP training are mostly left as in the original papers, the details are in Appendix D.

**Causal Language Model.** We used the GPT2 line of models, specifically GPT2-base (124M). The model is kept frozen through all the experiments.

**Pre-training.** We used Adam Optimiser  for 60 epochs and with a batch size of 384 on 20 V100 GPUs. We used a linear schedule with 2000 warmup steps and a base learning rate of 1e-4.

## 5 Results

### Benchmarking Pengi

We assessed Pengi on 21 downstream tasks covering various domains. Pengi is the first audio model that can perform both, open-ended and close-ended tasks. A fair comparison against another model that can perform both is not possible. We chose CLAP  as the baseline because it is the only Zero-Shot model with a comprehensive evaluation (16 downstream tasks). The next best evaluation was only on 8 tasks. Thus, providing no evidence of performance across domains like speech and music, which tend to be the most difficult. Moreover, we compared against SoTA results even if it came from different models and learning methods. We compared against SoTA Zero-Shot models in Table 8, a subset of Table 3, for Sound Event Classification. Even against SoTA from supervised

   Domain & Dataset & Files & Dur. (secs) & Output Type & Metric & Setup \\  Audio & Clotho & 7k & 15 - 30 & Cap. & SPIDEr & train/val/test \\ Captioning & AudioCaps & 39k & 10 & Cap. & SPIDEr & train/val/test \\  Audio Question & & & & & & \\ Answering & ClothoAQA & 2k & 15 - 30 & Q&A & ACC & train/val/test \\   & ESCQ & 2k & 5 & MC (50) & ACC & 5 folds \\  & FSD50K & 51k & 0.3 - 30 & ML (200) & mAP & train/val/test \\  & UrbanSound8K & 8k & \(\) 4 & MC (10) & ACC & 10 folds \\  & DCASE2017 Task & 52k & 10 & MC (17) & ACC & train/val/test \\   & GT. Music Speech & 120 & 30 & B (2) & ACC & 10 folds \\  & GT. Music Genre & 1k & 30 & MC (10) & ACC & 10 folds \\  Instrument & Beijing Opera & 236 & 4.77 & MC (4) & ACC & 5 folds \\ Classification & NS. Instruments & 305k & 4 & MC (11) & ACC & train/val/test \\   & NS. Pitch & 305k & 4 & Reg. & ACC & train/val/test \\  & NS. Velocity & 305k & 4 & MC (11) & ACC & train/val/test \\  & NS. Sonic & 305k & 4 & ML (10) & ACC & train/val/test \\  Acoustic Scene & TUT 2017 & 6.3k & 10 & MC (15) & ACC & train/val/test \\ Classification & & & & & & \\  Emotion & CREMA-D & 7k & 5 & MC (6) & ACC & 5 folds \\ Recognition & RAVDESS & 2.5k & \(\) 5 & MC (8) & ACC & 5 folds \\  Vocal Sound & & & & & & \\ Classification & & & & & & \\  Surveillance & Surveil. & & 585 & \(\) 33 & MC (6) & ACC & train/val/test \\  Text-to-Audio & Clotho & 7k & 15 - 30 & Ret. & R@1 & train/val/test \\ Retrieval & AudioCaps & 39k & 10 & Ret. & R@1 & train/val/test \\   

Table 2: We extensively evaluated Pengi across 21 downstream tasks from various domains. The first two domains are open-ended tasks and the rest are close-ended tasks. For the “Output Type” column, Cap. refers to captioning, MC to multiclass, B indicates binary, Reg. indicates regression, and Ret. retrieval.

[MISSING_PAGE_FAIL:7]

**AQA.** Pengi outperformed the existing literature . Authors in  collected the only dataset available (ClothoAQA). They converted the AQA task into a classification task, instead of a generation task. Authors trained and fine-tuned a model in a supervised setup. In contrast, we used the free-form text from Pengi, where the answer is correct only when it directly matches the human response. Note that Pengi includes the training set of ClothoAQA among its training sets, but there is no further fine-tuning on this task. The results are shown in Table 5. The first column indicates three different baseline models from . Pengi achieved 64.5% and outperformed the existing supervised benchmark by a relative 1.5%.

### Zero-Shot Sound Event Classification

We compared Pengi's classification performance against Zero-Shot contrastive models in the literature. The existing literature restricts the training and evaluation tasks to a few sound event datasets. Hence, we matched our comparisons to sound event datasets. The downstream datasets of ESC50, US8k, DCASE17 Task4 contain audio files and labels not seen by Pengi during training. We considered these three datasets to constitute a zero-shot setup for Pengi. For FSD50k, the audio files in the training split have been used for training Pengi. Hence, we do not consider this a pure zero-shot setup but nonetheless, report numbers for insights.

On Zero-Shot ESC50 performance, Pengi beats AudioCLIP , CLAP , and LAION CLAP  by 32%, 11%, and 1% respectively (See Table 8). Interestingly, human performance on ESC50 is 81% accuracy and Pengi's performance is 92%. Mei et. al.  added ChatGPT augmented audio-text pairs to CLAP training  and showed an improvement in performance from 91% to 94% on ESC50. On US8k, Pengi performed better than Wav2CLIP and AudioCLIP but lower than CLAP and LAION CLAP. Overall, even though Pengi is a text generation model, its Zero-Shot performance on close-ended Sound Event Classification is competitive.

### Text-to-Audio Retrieval

For Text-to-Audio Retrieval in a contrastive learning setup, the user query is converted into a text embedding which is then used to retrieve the top \(k\) audios by their audio embeddings . Pengi is a generative model and does not allow a contrastive setup. Although Pengi has an audio encoder and a text encoder that could replicate the contrastive setup, we wanted to evaluate our model from the generative perspective. First, Pengi is used to index a database by generating audio captions for all the audio recordings. Second, the user text query is matched directly to the dataset captions. The associated audio files of the top \(k\) dataset captions are considered to be the top \(k\) retrieved audio. Note that the cosine similarity computation is between two text embeddings and not audio and text embeddings. Thus, the quality of generated captions for indexing the dataset is important for retrieval performance.

    & Eval. & BLUE\({}_{4}\) & BLUE\({}_{23}\) & BLUE\({}_{3}\) & BLUE\({}_{4}\) & METEOR & ROUGE\({}_{L}\) & CIDEr & SPICE & SPIDEr \\  Chen et al. & AudioCaps & 0.489 & 0.292 & 0.178 & 0.106 & 0.152 & 0.346 & 0.265 & 0.093 & 0.179 \\ Gontier et al. & AudioCaps & 0.635 & 0.461 & 0.322 & 0.219 & 0.208 & 0.450 & 0.612 & 0.153 & 0.383 \\ Mei et al. & AudioCaps & 0.682 & 0.507 & 0.369 & 0.266 & 0.238 & 0.488 & 0.701 & 0.166 & 0.434 \\ Kim et al. & AudioCaps & **0.708** & **0.547** & **0.402** & **0.283** & **0.238** & **0.499** & 0.710 & 0.167 & 0.438 \\  Pengi & AudioCaps & 0.691 & 0.419 & 0.371 & 0.253 & 0.232 & 0.482 & **0.752** & **0.182** & **0.467** \\  Chen et al. & Clotho & 0.516 & 0.325 & 0.215 & 0.141 & 0.153 & 0.350 & 0.314 & 0.102 & 0.208 \\ Gontier et al. & Clotho & 0.461 & 0.282 & 0.182 & 0.117 & 0.136 & 0.318 & 0.251 & 0.083 & 0.167 \\ Mei et al. & Clotho & 0.516 & 0.318 & 0.204 & 0.127 & 0.157 & 0.351 & 0.313 & 0.105 & 0.209 \\ Kim et al. & Clotho & 0.539 & 0.346 & 0.227 & 0.142 & 0.159 & 0.366 & 0.319 & 0.111 & 0.215 \\  Pengi & Clotho & **0.57** & **0.369** & **0.242** & **0.15** & **0.172** & **0.375** & **0.416** & **0.126** & **0.271** \\   

Table 7: Pengi outperforms the best Audio Captioning performance from supervised models. All models used both, AudioCaps and Clotho datasets in training. SPDEr is the metric used to rank models in IEEE DCASE Challenge. Higher is better for all metrics.

    &  \\  Model & ESC50 & FSD50K & US8K & DCASE17 \\  & Task 4 \\  Wav2CLIP & 0.414 & 0.030 & 0.404 & - \\ AudioCLIP & 0.694 & - & 0.653 & - \\ CLAP & 0.826 & 0.302 & 0.732 & 0.3 \\ LAION & 0.91 & - & **0.77** & - \\  Pengi & **0.92** & **0.468** & 0.719 & **0.338** \\   

Table 8: The literature on Zero-Shot audio models only reports performance on Sound Event Classification datasets. Pengi’s classification performance is competitive. The ‘-’ indicates numbers are not available. The evaluation metric for DCASE17 is the F1 score while FSD50K employs mAP, ESC50 and US8K use Accuracy.

In Table 6, we compared Pengi's Text-to-Audio retrieval performance against the literature. The models used for comparison are audio captioning models using the above-described procedure of indexing and query matching, and not the contrastive-like setup. Pengi outperforms the literature on R@1. However, contrastive models ,,  are substantially better than generative models for the task of directly matching text to audio for retrieval. An example of contrastive model performance is shown in Table 6 as a gray row.

### Next text-token prediction for learning audio representations

Pengi uses next-text token prediction to learn audio representations, hence a natural question is: "_Can next text-token prediction objective help in learning general purpose audio representations?"_. To answer this question, we performed linear probe  and shallow learning  experiments. After Pengi's pre-training, we took the audio encoder \(a_{}\) in Fig 2 and trained one, two, or three fully-connected linear layer(s) with cross-entropy on top. Note that, we kept Pengi's audio encoder frozen and it did not include the mapping network \(m_{1}\). We selected representative datasets from the domain of Sound Events, Music, and Speech Emotion for the linear probe experiment. Pengi's linear probe (one layer) and shallow learning (two or three layers) numbers are compared against the best single model submissions from the HEAR challenge  in Table 9. The results from HEAR challenge reported the maximum of both settings (\(L_{1}\) or \(L_{2}\), \(L_{3}\)). Apart from Wav2vec2 which is trained on speech data, all other models were trained on non-speech audio. Pengi's linear probe \(L_{1}\) and \(L_{3}\) performance is consistently better than CLAP . In the Sound Events and Music domain, Pengi outperformed other models. In the Speech Emotion domain, Pengi performed better than non-speech models but lower than models trained on speech (Wav2vec2). The experiment indicates that the next token prediction _does help_ in learning audio representations useful for various domains.

## 6 Limitations

**Trade-off between close-ended and open-ended tasks performance.** The classification and text generation performance of Pengi is competitive against contrastive models. However, text-based retrieval performance lags behind that of contrastive models [11; 58]. Although these models excel at retrieval, they are limited to close-ended tasks. Thus, there is a trade-off between both types of learning methods proposed so far in the literature.

**Limitations inherent to Language Models.** Pengi benefits from the encyclopedic knowledge of pre-trained Language Models (LM). However, as pretrained LM is a component of Pengi, they also inherit their limitations. For example, LM are known to hallucinate  and specific to Pengi, can produce responses not grounded or conditioned on audio. Similarly, Pengi falls back to LM behavior if no audio is provided or if the audio knowledge is limited. Therefore, the risks of LM, namely propagating stereotypes, and biases and potentially producing offensive language are still applicable to Pengi. The recent works [48; 56] in the NLP field try to address these issues. However, specifically studying risks and limitations can uncover new insights that can accelerate the development of ALMs.

## 7 Conclusions

We proposed Pengi, a novel Audio Language Model that leverages Transfer Learning by framing all audio tasks as text-generation tasks. It takes as input, an audio recording, and a text prompt, and generates free-form text as output. Pengi is capable of handling both, close-ended and open-ended audio tasks. We benchmarked Pengi on 21 downstream tasks and show it yields SoTA performance in several of them. Our findings break ground in prompting language models with audio for general-purpose audio understanding.

    &  &  &  \\  Model & ES50 & FSD50k & GIT2AN Genres & Opera & RAVDESS & CREMA-D \\  YAMNet & 0.8375 & - & 0.847 & 0.9405 & 0.479 & 0.4533 \\ Open L3 & 0.7050 & 0.4470 & 0.879 & 0.9746 & 0.604 & 0.5497 \\ Wav2CLIP & 0.7589 & 0.3617 & 0.748 & 0.9363 & **0.684** & 0.5116 \\ PanN & 0.9085 & - & 0.860 & 0.9112 & 0.429 & 0.5550 \\ Wav2Vec2 & 0.5610 & 0.3417 & 0.780 & 0.9067 & - & **0.6562** \\ CLAP (L\({}_{1}\)) & 0.8995 & 0.5024 & 0.73 & 0.6399 & 0.4044 & 0.2315 \\ CLAP (L\({}_{23}\)) & 0.9310 & 0.5690 & 0.8330 & 0.8263 & 0.4512 & 0.2830 \\  Pengi (25) & 0.9195 & 0.4676 & 0.3525 & 0.6229 & 0.2302 & 0.1846 \\ Pengi (L\({}_{1}\)) & 0.8915 & 0.5608 & 0.8000 & 0.9193 & 0.4774 & 0.5057 \\ Pengi (L\({}_{3}\)) & **0.9485** & **0.6235** & **0.9010** & **0.9883** & 0.6108 & 0.5916 \\   

Table 9: Shallow learning experiment where the audio encoder is frozen in all the experiments. ZS is zero-shot and \(L_{i}\) indicates \(i\) linear layers used. Unless specified, each model reports the best of \(L_{1}\), \(L_{2}\), and \(L_{3}\).