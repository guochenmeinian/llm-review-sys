# Diffusion-Inspired Truncated Sampler

for Text-Video Retrieval

 Jiamian Wang\({}^{1}\)

\({}^{1}\)Rochester Institute of Technology, \({}^{2}\)Amazon,

\({}^{3}\)Kent State University, \({}^{4}\)DEVCOM Army Research Laboratory

 Pichao Wang\({}^{2}\)

\({}^{1}\)Rochester Institute of Technology, \({}^{2}\)Amazon,

\({}^{3}\)Kent State University, \({}^{4}\)DEVCOM Army Research Laboratory

 Dongfang Liu\({}^{1}\)

\({}^{1}\)Rochester Institute of Technology, \({}^{2}\)Amazon,

\({}^{3}\)Kent State University, \({}^{4}\)DEVCOM Army Research Laboratory

 Qiang Guan\({}^{3}\)

\({}^{1}\)Rochester Institute of Technology, \({}^{2}\)Amazon,

\({}^{3}\)Kent State University, \({}^{4}\)DEVCOM Army Research Laboratory

 Sohail Dianat\({}^{1}\)

\({}^{1}\)Rochester Institute of Technology, \({}^{2}\)Amazon,

\({}^{3}\)Kent State University, \({}^{4}\)DEVCOM Army Research Laboratory

 Majid Rabbani\({}^{1}\)

\({}^{1}\)Rochester Institute of Technology, \({}^{2}\)Amazon,

\({}^{3}\)Kent State University, \({}^{4}\)DEVCOM Army Research Laboratory

 Raghuveer Rao\({}^{4}\)

\({}^{1}\)Rochester Institute of Technology, \({}^{2}\)Amazon,

\({}^{3}\)Kent State University, \({}^{4}\)DEVCOM Army Research Laboratory

 Zhiqiang Tao\({}^{1}\)

Corresponding authors: Jiamian Wang (jw4905@rit.edu) & Zhiqiang Tao (zhiqiang.tao@rit.edu)

###### Abstract

Prevalent text-to-video retrieval methods represent multimodal text-video data in a joint embedding space, aiming at bridging the relevant text-video pairs and pulling away irrelevant ones. One main challenge in state-of-the-art retrieval methods lies in the modality gap, which stems from the substantial disparities between text and video and can persist in the joint space. In this work, we leverage the potential of Diffusion models to address the text-video modality gap by progressively aligning text and video embeddings in a unified space. However, we identify two key limitations of existing Diffusion models in retrieval tasks: The \(_{2}\) loss does not fit the ranking problem inherent in text-video retrieval, and the generation quality heavily depends on the varied initial point drawn from the isotropic Gaussian, causing inaccurate retrieval. To this end, we introduce a new Diffusion-Inspired Truncated Sampler (DITS) that jointly performs progressive alignment and modality gap modeling in the joint embedding space. The key innovation of DITS is to leverage the inherent proximity of text and video embeddings, defining a truncated diffusion flow from the fixed text embedding to the video embedding, enhancing controllability compared to adopting the isotropic Gaussian. Moreover, DITS adopts the contrastive loss to jointly consider the relevant and irrelevant pairs, not only facilitating alignment but also yielding a discriminatively structured embedding. Experiments on five benchmark datasets suggest the state-of-the-art performance of DITS. We empirically find that DITS can also improve the structure of the CLIP embedding space. Code is available at https://github.com/Jiamian-Wang/DITS-text-video-retrieval

## 1 Introduction

Text-video retrieval aims to match textual descriptions with relevant video content and vice versa, utilizing both modalities to improve ranking accuracy . This task is challenging due to the multi-modality gap, which represents the inherent differences between textual and visual data representations . Bridging this gap effectively requires advanced methodologies to extract and align semantic information from both modalities into a joint embedding space .

Current state-of-the-art approaches have explored various strategies to narrow this gap. Advanced feature extraction techniques focus on capturing fine-grained details through temporal modeling  and multi-granularity matching strategies, such as frame-level, patch-level, word-level, and token-level representations . Additionally, leveraging large-scale data augmentation has been shown to enhance the capabilities of visual-language models .

et al., 2023; Wang et al., 2024), while incorporating additional modalities like audio has also improved retrieval performance (Ibrahimi et al., 2023). Despite the progress, the modality gap (Fig. 1 (a)) remains a persistent issue, significantly impacting the performance of retrieval systems.

A critical observation in text-video retrieval is that existing methods often struggle with the variability and complexity of the semantic alignment between text and video. This work addresses this challenge by investigating Diffusion models (Ho et al., 2019; Song et al., 2021) to bridge the modality gap. Our main contribution is developing a new Diffusion-inspired truncated sampler (DITS) for models to iteratively align video and text embeddings in a progressive manner. Unlike traditional approaches (Qiu et al., 2024; Nukrai et al., 2022) that impose fixed priors for the alignment (Fig. 1 (b)), our method starts from the text embedding and employs a truncated diffusion process to generate aligned video embeddings, addressing the inflexibility and inadequacy of fixed prior methods.

We first explore the generative capability of Diffusion models to mitigate the modality gap. Initially, we employ pre-trained text and video embeddings and model the gap using a diffusion process, where the initial state (isotropic Gaussian) represents a fixed prior, progressively denoised to align the embedding (Fig. 1 (c)). However, we found that the vanilla Diffusion model's \(_{2}\) loss does not fit the ranking problem inherent in text-video retrieval. The \(_{2}\) loss assumes a Gaussian distribution and minimizes the mean squared error, treating all errors equally and failing to prioritize the alignment of semantically relevant pairs over irrelevant ones. Moreover, the reverse process introduces diversity through initial Gaussian noise sampling, which impacts the generation quality and causes the misalignment. Therefore, it is non-trivial and necessary to tailor the Diffusion modeling from generation (where diversity is beneficial) to ranking (where precise alignment is crucial), catering to text-video retrieval. The focus of diffusion learning should be on closing relevant embeddings while pushing apart irrelevant pairs. This requires a different loss function and sampler for better alignment.

To address the above challenges, DITS leverages the inherent proximity of text and video embeddings in the joint space (Liang et al., 2022; Zhou et al., 2023) and incorporates a truncated diffusion process into the contrastive loss (depicted in Fig. 1 (d)). Different from the vanilla diffusion-based alignment, DITS adopts text embedding as a more meaningful intermediate latent state to initiate sampling, reducing the underlying variability and improving the alignment accuracy over Gaussian noises. The proposed DITS reformulates each diffusion step as modeling the modality gap and gradually controls the text-to-video alignment through the contrastive loss, acting on both aligned and video embeddings, which thus intrinsically promotes the alignment across timestamps in a more fine-grained manner. Empirical evidence shows that DITS can also enhance the structure of the CLIP embedding space. We summarize the contributions of this work as follows.

* This work studies the Diffusion model to bridge the modality gap of text-video retrieval, identifying two key limitations of the vanilla Diffusion model. A new sampler, namely DITS, is also proposed to enhance the multi-modality alignment and benefit the retrieval.
* DITS enables a new truncated diffusion process to conduct the video-text alignment progressively, starting from the intermediate latent state given by text embedding to approaching the relevant video embedding. Governed by the contrastive loss, the proposed DITS gradually models the gap distribution along each timestamp by learning from relevant/irrelevant pairs.

Figure 1: Multi-modality gap alignment for the text-video retrieval. **(a)** Considering the many-to-one correspondence of text-to-video, we define the modality gap stemming from the text (\(=-\)) for uniqueness. **(b)** Fixed prior alignment posts a Gaussian distribution, which is inflexible. **(c)** Diffusion-based alignment upon \(_{2}\) loss, which does not fit the retrieval task. The generation is heavily affected by the random samples from \((,)\). **(d)** The proposed diffusion-inspired truncated sampler (DITS) aligns from \(\) to \(\) and gradually models the gap, guided by the contrastive loss.

* Extensive experiments on five datasets (MSRVTT, LSMDC, DiDeMo, VATEX, and Cha-rades) suggest that DITS achieves state-of-the-art performance. Empirical evidence shows that DITS also improves the structure of the CLIP embedding space.

## 2 Related Work

**Text-video Retrieval**. State-of-the-art text-video retrieval methods (Huang et al., 2023, Gao et al., 2021, Yu et al., 2018, Deng et al., 2023, Jin et al., 2023, Croitoru et al., 2021) develop different strategies to facilitate the multi-modality alignment. Some adopt the temporal modeling (Li et al., 2023, Liu et al., 2023, Li et al., 2023, Han et al., 2022b) on word sequence or video frames, better extracting the semantic clues and enhancing the retrieval. Others extract multi-modality features at different granularity (Liu et al., 2022, Chen et al., 2020, Wang et al., 2023, Ma et al., 2022, Han et al., 2022, Wang et al., 2021), uncovering hierarchical relationships between word sequences and video clips. Apart from the intricate model designs (Guan et al., 2023, Wu et al., 2021, Bain et al., 2021, Liu et al., 2021, Mech et al., 2021), incorporating the large-scale text-video data via augmentation emerges as a promising approach (Luo et al., 2022, Wu et al., 2023, Falcon et al., 2022), which effectively amplifies the potential of the visual-language foundation models (Chen et al., 2024, Ko et al., 2023) and benefiting the retrieval. Additionally, harnessing new modality data, such as the audio (Ibrahimi et al., 2023, Akbari et al., 2021, Lin et al., 2022, Miech et al., 2018, Liu et al., 2022, Shvetsova et al., 2022), paves a new direction to bridge the text-video modalities, with abundant semantics clues. Recent advances (Fang et al., 2023, Wang et al., 2024, Gao et al., 2024, Ji et al., 2023, Chun et al., 2021, Nukrai et al., 2022) study different representation forms of the multi-modality data, among which T-MASS (Wang et al., 2024) achieves the outstanding performance. Despite the prosperity, the modality gap persists in the joint space. Distinguished from existing methods, this work studies how to learn the Diffusion model to model the modality gap in the joint embedding space, considering the substantial gap between the generation task and the ranking nature inherent to text-video retrieval.

**Diffusion models**. Diffusion models (Croitoru et al., 2023, Ho et al., 2019) define a transition from the isotropic Gaussian to the clean data with a learned diffusion process, emerging as powerful generative models for learning complex distributions. This approach has been extensively applied in image generation tasks (Yang et al., 2023), with models such as Denoising Diffusion Probabilistic Models (DDPM) (Ho et al., 2019), DDIM (Song et al., 2021), and LDM (Rombach et al., 2022) demonstrating impressive capabilities in generating high-fidelity images. Building upon these foundations, recent research (Zhang et al., 2023) has extended Diffusion models to the multimodal domain, not only enabling the generation of diverse and realistic data with distinct type (such as image, audio, etc.) (Yang et al., 2023), but also achieving the transition across varied modalities (_e.g._, text-to-image, text-to-video, etc.) (Ghosh et al., 2024). Nevertheless, leveraging the Diffusion model to the ranking tasks such as the text-video retrieval is non-trivial due to the significant difference in data, evaluation metrics, and the inherent challenges (_e.g._, diversity, coherence, and realism for generation _v.s._ relevance scores in retrieval). To our best knowledge, previous works of DiffusionRet (Jin et al., 2023) and MomentDiff (Li et al., 2023) combines Diffusion model with retrieval. Albeit the encouraging performance, their method adopts the Diffusion model to learn a mapping from random noise to the signal (e.g., similarity score or real moments) using the diffusion \(L_{1}\) or \(L_{2}\) loss, treating the ranking problem as a generation task without addressing the problem studied in this work.

## 3 Method

### Preliminaries

**Text-Video Retrieval**. We have a multi-modality dataset consisting of \(N_{v}\) video clips and \(N_{t}\) text descriptions, _e.g._, \(=\{v^{(i)},t^{(j)}\}\), where \(i=1,...,N_{v}\), \(j=1,...,N_{t}\). Each video clip \(v^{(i)}\) may correspond to one or more text samples \(t^{(j)}\). For simplicity, we let \(v\) and \(t\) to denote arbitrary video and text. The retrieval system adopts a multi-modality model, such as upon CLIP (Radford et al., 2021), to abstract the video and text features into a joint embedding space, _i.e._, \(\), \(^{d}\), where \(d\) denotes the embedding dimension of the joint space.

\[=F_{}(v);\ \ =G_{}(t),\] (1)where \(F_{}()\) denotes the advanced visual encoder and \(G_{}()\) denotes the text encoder. Existing works mainly adopt the symmetric-formed cross-entropy (Oord et al., 2018) loss to train the model

\[_{}=-_{i=1}^{B}^{( i)},^{(i)})}}{_{j}e^{s(^{(i)},^{(i)}) }}+^{(i)},^{(i)})}}{_ {j}e^{s(^{(j)},^{(i)})}},\] (2)

where the cosine similarity \(s(,)=|}{|| ||}\) is employed as a distance measuring function and \(B\) denotes the batch size. A learnable temperature scaling factor \(\) is employed to control the smoothness of the loss. The first term in Eq. (2) guides the text-to-video (\(t v\)) retrieval and the second term focuses on the video-to-text (\(v t\)) retrieval. Notably, the scenario of \(_{}=0\) presents a perfect alignment for all video and text embedding in the joint space, where the relevant \(}{||}\) and \(}{||}\) overlaps (\(s(,)=1\)), and the irrelevant text-video are orthogonal, _i.e._, \(s(,)=0\). While in practice, it is hard to achieve a perfect alignment considering the substantial disparities between the \(t\) and \(v\) data. To this end, a modality gap \(\) inevitably exists between the relevant text and video embedding as shown by Fig. 1 (a), which poses challenges in determining the relevancy. Nevertheless, how to model the modality gap \(\) and perform alignment accordingly can benefit the retrieval but does not seem to have been fully explored. Note that there are two ways to define the \(\) by either stemming from \(\) or \(\). Considering the one-to-many correspondence of video-to-text, as exampled in Fig. 2 (\(^{(1)}\)_v.s._\(\{^{(1)},^{(2)},^{(3)}\}\)), each \(\) associates to a unique \(\) but \(\) corresponds to multiple \(\) vectors. Accordingly, we consider \(\) stemming from \(\) and consistently adopt \(=-\) in this work.

**Fixed Prior Alignment**. We first posit a fixed prior for the multi-modality gap, similar to existing multi-modality alignment practices (Qiu et al., 2024; Nukrai et al., 2022), and have

\[(0,^{2}I),\] (3)

where \(^{2}\) determines the scale of the modality gap, _i.e._, to what extant \(\) deviates from the relevant \(\). Given the arbitrary text embedding \(\), we draw \(\) from the fixed prior as shown above and impose it onto the embedding as the alignment, _e.g._, \(+\). Then this aligned embedding will take effect in the retrieval system as a substitution of \(\). Unfortunately, there is a decrease in retrieval performances for variant priors (see Table 4). To elaborate, the fixed prior is far from enough to capture the delicate distribution of \(\) given the complicated structure of the CLIP embedding space (Paszke et al., 2019). Besides, the fixed prior is inflexible to adapt to varied text and video inputs, causing misalignment. These limitations naturally prompt us to resort to more advanced technologies for the gap modeling. As known, Diffusion models (Yang et al., 2023; Croitoru et al., 2023) approximate the sophisticated distributions without explicitly presuming a prior and has been proven to excel in bridging multi-modality representations (Zhou et al., 2023). In this work, we investigate the potential of the Diffusion models for the gap modeling under text-video retrieval.

### Proposed Diffusion-based Alignment

We define the target distribution of the Diffusion model upon the modality gap \(\). Following the previous works (Gorti et al., 2022; Guan et al., 2023), our proposed Diffusion model operates in the embedding space of the CLIP model and defines a Markov chain of the latent variables for \(\), _i.e._, \(_{1},...,_{T}\), where the initial state of the Diffusion model remains an isotropic Gaussian, _i.e._, \(_{T}(,)\), which in this case, can also be treated as a fixed prior for the modality gap \(\), and \(_{0}\) corresponds to the ground truth modality gap. It is non-trivial incorporating the proposed Diffusion model into existing retrieval pipeline due to different natures of the generation and retrieval. To achieve this, we reimplement the denoising network, develop a pretraining stage and a reverse process-engaged retrieval pipeline.

**Denoising Network Design**. We first design a denoising network by referring to the prevalent DiT (Peebles and Xie, 2023). The denoising network, denoted as \(_{}()\), consists of \(N\) DiT blocks and let \(\) denote the learnable parameters for the network. As shown in Fig. 2, the denoising network takes three inputs, including the timestamp \(t\), the latent embedding \(_{t}^{d}\), and the condition \(^{d}\). Since we define the modality gap \(\) stems from \(\) as in Section 3.1, we let the text embedding \(\) as the condition to guide the generation, _i.e._, \(=\), which substitutes the vanilla classifier-free guidance (Ho and Salimans, 2021) in DiT. Finally, since we still perform the retrieval in the same latent space, there is no need to perform a reshape operation at the end of the denoising network. As shown in Fig. 1 (c), the denoising network \(_{}()\) approximates the noise imposed on \(_{t}\) during the diffusion process. To facilitate the learning of \(_{}()\), we introduce a pretraining stage as follows.

**Diffusion model Pretraining**. The target distribution of \(\) can be highly adaptive to **v** and **t** given by Eq. (1). To stabilize the learning, we adopt the pretrained retrieval model and fix \(F_{}()\) and \(G_{}()\) during the pretraining. The learning objective of \(_{}()\) derives from the diffusion process (Qiu et al., 2024; Nukrai et al., 2022), which gradually adds the Gaussian noise to \(_{0}\) for \(T\) steps, where \(_{1}\),..., \(_{T}\) are the latent variables of the same dimension, _i.e._, \(_{t}^{d}\)

\[q(_{1:T}|_{0}):=_{t=1}^{T}q(_{t}|_{t-1} ),\;\;q(_{t}|_{t-1}):=(_{t};} _{t-1},_{t}),\] (4)

where \(_{1},...,_{T}\) can simply be held as constant (Ho et al., 2019) to form a variance schedule. By adopting the reparameterization (Kingma and Welling, 2013) upon Eq. (4), \(_{t}\) can be analytically represented in a closed form

\[q(_{t}|_{0})=(_{t};_{t}}_ {0},(1-_{t})),\;\;\;\;_{t}:=1-_{t},\;\;_{t}:=_{s=1}^{t}_{s}.\] (5)

To approximate \(P_{}(_{t-1}|_{t})\), we use the variational lower bound to optimize the negative log-likelihood \(- P_{}(_{0})\), which can be parameterized as a \(_{2}\) loss (Ho et al., 2019)

\[_{}=_{_{0},t,}[||-_{ }(}_{0}+_{t}},t, )||^{2}],\] (6)

where \((,)\), \(t(1,...,T)\) and **c** denotes the condition. Specifically, \(\), \(t\), and **c** are three inputs of the denoising network \(_{}()\). By minimizing the \(_{}\), the Diffusion model learns to transition from the isotropic Gaussian to the target distribution of \(_{0}\) through a reverse process, based on which this method aligns the input text embedding **t** (given as condition) as \(+_{0}\), to bridge its relevant counterpart **v**. We introduce the reverse process-engaged retrieval pipeline in the next.

**Reverse Process-Engaged Retrieval**. We perform the vanilla DDPM (Ho et al., 2019) sampling process in this work. Starting from the isotropic Gaussian \(_{T}(,)\), the reverse process samples the gap \(_{0}\) through a \(T\)-step Markov chain

\[_{t-1}=_{t}}(_{t}-}{ {1-_{t}}}_{}(,t,))+_{t} {z},\;\;\;\;(,),\] (7)

where \(t\) decreases from \(T\) to \(1\), \(=\) if \(t=1\), and \(_{t}^{2}=_{t}\). The reverse process of the Diffusion model will be plugged into the retrieval pipeline as the post-feature extraction alignment, collectively forming a reverse process-engaged retrieval pipeline. To this end, one can substitute **t** with the aligned embedding \(+_{0}\) and perform retrieval with regard to arbitrary **v**. Since the embedding space structure has been edited by the Diffusion model, it might be better to further rearrange the multi-modality embeddings with the contrastive loss. We opt to jointly tune all of the learnable parameters \(=\{,,\}\) upon this reverse process-engaged retrieval pipeline with the symmetric cross-entropy given in Eq. (2). We provide more details and discussion in Section 4.1.

Figure 2: Overview of DITS. Given the video embedding **v** from \(F_{}()\) and the text embedding **t** from \(G_{}()\), DITS performs a \(T^{}\)-steps sampling by starting from **t**, to gradually approach **v** (as exampled by a relevant pair \(^{(1)}\), \(^{(1)}\)) with the aligned embedding \(_{0}^{(1)}\) at the timestamp \(t=0\). Meanwhile the progressive alignment, DITS intrinsically approximates the distribution of the modality gap \( P_{t}\) at each timestamp \(t=T^{},...,1\). The contrastive loss is adopted to guide the alignment and modeling, with parameters: \(=\{,,\}\). We devise \(_{}()\) upon DiT (Peebles and Xie, 2023).

### Proposed Alignment by DITS

To sum up, this work first implements a feasible approach to leverage the generative power of the Diffusion model in the retrieval pipeline. We firstly pretrain the Diffusion model with Eq. (6) and then incorporate the reverse process in Eq. (7) for the alignment. Notably, we find that despite that the Diffusion model effectively bridging the relevant text and video embedding, either applying the pretrained model for the alignment or adopting the reverse process-engaged retrieval causes obvious performance declines (shown in Table 4). This prompt us to rethink Diffusion models in the context of our problem. Based on the designs in Section 3.2, we develop DITS in the following.

**Rethinking the Vanilla Diffusion model**. We find two-fold limitations of the Diffusion model defective for the task of retrieval. **First**, the \(_{2}\) loss (\(_{}\)) does not fit the ranking problem inherent in text-video retrieval. On one hand, \(_{}\) shown in Eq. (6) minimizes a mean squared error, which inherently lacks an prioritization effect for the relevant pairs over irrelevant ones. On the other, it is not clear what the desired alignment should be for the irrelevant pairs, making it hard to define a target distribution accordingly and implement \(_{}\) for the modeling. Nevertheless, there is no guarantee that the \(_{2}\) loss (_e.g._, \(_{}\)) can effectively pull away irrelevant pairs, even though the gap is mitigated for the relevant pairs as shown in Fig. 3. **Second**, the reverse process either fails to benefit the retrieval (Table 4) - the initial starting point of the reverse, _i.e._, of \(_{T}(,)\) can significantly impact the diversity and the generation quality (Zhou et al., 2023; Chung et al., 2022; Zheng et al., 2023). This can incur inaccurate alignment and retrieval. How to determine the \(_{T}\) without affecting the performance remains a challenge.

**Diffusion-Inspired Truncated Sampler**. Bearing the above findings, in this work, we introduce the diffusion-inspired truncated sampler (DITS) as an effective alignment tool to benefit the retrieval. Considering the neighboring nature of the CLIP embedding space - relevant \(\) and \(\) have already been within arm's length of the distance) (Liang et al., 2022; Nukrai et al., 2022) - there is no need to start sampling from the isotropic Gaussian as in the vanilla Diffusion models. Instead, DITS proposes a progressive alignment procedure that directly starts from the \(\) to \(\) as shown in Fig. 2_upper right_, forming truncation. Given \(\) by \(F_{}()\) and \(\) by \(G_{}()\)

\[_{t-1}=(_{t}-_{t}_{}(_{t},t, ))+_{t},\ \ \ \ t=T^{},...,1,\] (8)

where \(_{}()\) serves as an alignment network and \(T^{}\) denotes the number of the truncated timestamps. The starting point \(_{T^{}}=\) (taking \(^{(1)}\) as an example in Fig. 2) and \(_{t}=}{-_{t}_{t}}}\) denotes a pre-defined schedule. Besides, we let \((,)\) if \(t>1,\ \ =\). We set \(_{t}=_{t}\). Compared with vanilla sampling process given by Eq. (7), the initial latent of DITS becomes a text embedding point (_e.g._, \(^{(i)}\)) rather than a random sample from the isotropic Gaussian, which brings following advantages: (1) DITS gets rid of the effect of starting point random sampling. (2) the query text (such as for \(t v\)) initializes the alignment in a more straightforward way, rather than solely being a condition to guide the learning. Considering this, the condition \(\) is not used in \(_{}()\). In addition, we adopt the contrastive loss upon Eq. (2) to guide the learning, during which we substitute \(\) with the aligned embedding \(_{0}\). We jointly train all of the learnable parameters \(=\{,,\}\) as denoted in red in Fig. 2. By observing and considering both relevant and irrelevant pairs, DITS learns the alignment that potentially benefits the overall retrieval performance.

It turns out DITS intrinsically involves the modality gap modeling along the timestamps, (_i.e._, \( P_{t}\)) as shown in Fig. 2. One can derive the distribution of the modality gap \(\) from Eq. (8), which can be represented by using the reparameterization

\[P_{t}=_{t}_{t}_{}(_{t},t,)+ _{t},\] (9)

where \(_{}(_{t},t,)\) can be regarded as a bias of the modality gap and \(_{t}\) controls the scale of the gap modeling at the timestamp \(t\). Interestingly, DITS yields a remarkable performance boost over the baseline and achieves the state-of-the-art performance (Section 4.2). We also empirically find that DITS also serves as an effective tool to help improve the CLIP embedding space. See more details in Section 4.4.

Figure 3: Modality gap distribution for the **relevant** pairs before and after the Diffusion model alignment.

## 4 Experiment

### Experimental Settings

**Datasets**. We employ five benchmark datasets for evaluation. Firstly, we utilize MSRVTT [Xu et al., 2016], comprising \(10,000\) YouTube video clips (each having \(20\) captions) and follow the 1K-A testing split in Liu et al. . Secondly, LSMDC [Rohrbach et al., 2015] includes \(118,081\) text-video pairs, providing videos with longer duration. The testing set contains \(1000\) videos, as per Gabeur et al. , Gorti et al. . Thirdly, DiDeMo [Anne Hendricks et al., 2017] contains \( 40,000\) captions and \( 10,000\) video clips. We adhere to the data splits detailed in Luo et al. , Jin et al. . Fourthly, VATEX [Wang et al., 2019] comprises \(41,250\) video clips, where each is paired with ten English and ten Chinese descriptions. We follow the split in Chen et al. [2020a]. Lastly, Charades [Sigurdsson et al., 2016] contains \(9848\) video clips, each with multiple text descriptions detailing daily activities and actions. We adopt the split protocol of Lin et al. .

**Implementation Details**. Following previous works [Gorti et al., 2022, Li et al., 2023b], we resize the video to be the spatial size of \(224 224\) and uniformly sample \(12\) frames from the video for all datasets. For the retrieval model, we adopt X-Pool [Gorti et al., 2022] as the baseline, where both CLIP-ViT/B-32 and CLIP-VIT/B-16 are employed as the feature extractor. The dropout is set to \(0.3\). Different from DiT [Peebles and Xie, 2023], we set our denoising network (also used as the alignment network in DITS) with \(N=4\) blocks, with \(16\) heads and an MLP ratio of \(4.0\). We let the dimension \(d=512\) for the whole model. We find that a timestamp of \(T=10\) is enough for diffusion-based alignment. For DITS, we set the truncated timestamp \(T^{}=5\) for DiDeMo and \(T^{}=10\) for others. A linear variance schedule with \(=0.1\) and \(=0.99\) is adopted. All of the parameters \(=\{,,\}\) are trained with an AdamW [Loshchilov and Hutter, 2017] optimizer with weight decay of \(0.2\) and

   &  &  \\   & R@1 \(\) & R@5 \(\) & R@1 \(\) & MdR \(\) & MnR \(\) & R@1 \(\) & R@5 \(\) & R@10 \(\) & MdR \(\) & MnR \(\) \\ 
**CLIP-ViT-B/32** & & & & & & & & & & & \\ X-Pool [Gorti et al., 2022] & 46.9 & 72.8 & 82.2 & 2.0 & 14.3 & 25.2 & 43.7 & 53.5 & 8.0 & 53.2 \\ STAN [Liu et al., 2023] & 46.9 & 72.8 & 82.8 & 2.0 & – & 23.7 & 42.7 & 51.8 & 9.0 & – \\ ProSf [Li et al., 2023b] & 48.2 & 74.6 & 83.4 & 2.0 & 12.4 & 24.1 & 42.5 & 51.6 & 9.0 & 54.6 \\ Diffusionet [Jin et al., 2023] & 49.0 & 75.2 & 82.7 & 2.0 & 12.1 & 24.4 & 43.1 & 54.3 & 8.0 & **40.7** \\ UATVR [Wang et al., 2023] & 47.5 & 73.9 & 83.5 & 2.0 & 12.3 & – & – & – & – & – \\ UCOFIA [Wang et al., 2023] & 49.4 & 72.1 & – & – & 12.9 & – & – & – & – & – \\ TEFAL [Ibrahimi et al., 2023] & 49.4 & **75.9** & 83.9 & 2.0 & 12.0 & 26.8 & 46.1 & 56.5 & 7.0 & 44.4 \\ CLIP-ViP [Xu et al., 2023] & 50.1 & 74.8 & 84.6 & 1.0 & – & 25.6 & 45.3 & 54.4 & 8.0 & – \\ T-MASS [Wang et al., 2024] & 50.2 & 75.3 & **85.1** & 1.0 & 11.9 & **28.9** & **48.2** & **57.6** & 6.0 & 43.3 \\ DITS (Ours) & **51.9** & 75.7 & 84.6 & **1.0** & **11.6** & 28.2 & 47.3 & 56.6 & **6.0** & 43.7 \\ 
**CLIP-ViT-B/16** & & & & & & & & & & \\ X-Pool [Gorti et al., 2022] & 48.2 & 73.7 & 82.6 & 2.0 & 12.7 & 26.1 & 46.8 & 56.7 & 7.0 & 47.3 \\ UATVR [Wang et al., 2023] & 50.8 & 76.3 & 85.5 & 1.0 & 12.4 & – & – & – & – \\ CLIP-ViP [Xu et al., 2023] & 54.2 & 77.2 & 84.8 & 1.0 & – & 29.4 & 50.6 & 59.0 & 5.0 & – \\ T-MASS [Wang et al., 2024a] & 52.7 & 77.1 & 85.6 & 1.0 & 10.5 & 30.3 & 52.2 & **61.3** & 5.0 & 40.1 \\ DTIS (Ours) & **55.0** & **79.8** & **87.1** & **1.0** & **10.0** & **31.0** & **52.4** & 61.0 & **5.0** & **38.4** \\  

Table 1: Text-to-video retrieval performance on MSRVTT [Xu et al., 2016] and LSMDC [Rohrbach et al., 2015]. Bold denotes the best performance. “-” denotes that the result is unavailable.

   &  &  \\   & R@1 \(\) & R@5 \(\) & R@10 \(\) & MdR \(\) & MnR \(\) & R@1 \(\) & R@5 \(\) & R@10 \(\) & MdR \(\) & MnR \(\) \\ 
**CLIP-ViT-B/32** & & & & & & & & & & \\ X-Pool [Gorti et al., 2022] & 44.6 & 73.2 & 82.0 & 2.0 & 15.4 & 60.0 & 90.0 & 95.0 & 1.0 & 3.8 \\ ProSf [Li et al., 2023b] & 44.9 & 72.7 & 82.7 & 2.0 & 13.7 & 60.6 & 90.5 & 95.4 & 1.0 & 3.4 \\ STAN [Liu et al., 2023] & 46.5 & 71.5 & 80.9 & 2.0 & – & – & – & – & – \\ Diffusionet [Jin et al., 2023] & 46.7 & 74.7 & 82.7 & 2.0 & 14.3 & – & – & – & – \\ UATVR [Wang et al., 2023] & 43.1 & 71.8 & 82.3 & 2.0 & 15.1 & 61.3 & 91.0 & 95.6 & 1.0 & 3.3 \\ UCOFIA [Wang et al., 2023] & 46.5 & 74.8 & – & – & 13.4 & 61.1 & 90.5 & – & – & 3.4 \\ CLIP-ViP [Xu et al., 2023] & 48.6 & 77.1 & 84.4 & 2.0 & – & – & – & – & – \\ T-MASS [Wang et al., 2024a] & 50.9 & 77.2 & 85.3 & 1.0 & 12.1 & 63.0 & 92.3 & 96.4 & 1.0 & 3.2 \\ DTIS (Ours) & **51.1** & **77.9** & **85.8** & 1.0 & **12.1** & **64.1** & **92.7** & **97.0** & **1.0** & **2.9** \\ 
**CLIP-ViT-B/16** & & & & & & & & & & \\ X-Pool [Gorti et al., 2022] & 47.3 & 74.8 & 82.8 & 2.0 & 14.2 & 62.6 & 91.7 & 96.0 & 1.0 & 3.4 \\ UATVR [Wang et al., 2023] & 45.8 & 73.7 & 83.3 & 2.0 & 13.5 & 64.5 & 92.6 & 96.8 & 1.0 & 2.8 \\ CLIP-warmup rate of \(0.1\). We set the training epochs to \(5\) for all datasets and adopt the same seed of \(24\). We perform contrastive learning with a batch size of \(B=32\) for all datasets and backbones. Same as X-Pool (Gorti et al., 2022), the learning rate of the CLIP model is initialized as \(1 10^{-5}\). The learning rate for non-CLIP modules is \(3 10^{-5}\) for MSRVTT (Xu et al., 2016) and \(1 10^{-5}\) for all the other datasets. For a fair comparison, no post-processing techniques (Bogolin et al., 2022; Cheng et al., 2021) are employed in this work. We implement DITS with PyTorch (Paszke et al., 2019) and perform experiments on an NVIDIA A100 GPU.

**Compared Methods**. We compare DITS with X-Pool (Gorti et al., 2022), STAN (Liu et al., 2023), ProST (Li et al., 2023b), DiffusionRet (Jin et al., 2023), UATVR (Fang et al., 2023), UCOFIA (Wang et al., 2023), TEFAL (Ibrahimi et al., 2023), CLIP-ViP (Xue et al., 2023) and T-MASS (Wang et al., 2024a). Among them, TEFAL adopts additional audio data to train the model. CLIP-ViP further adopts WebVid-2.5M (Bain et al., 2021) and HD-VILA-100M (Xue et al., 2022) as the augmentation. Besides, some methods take larger batch sizes for the training (_e.g._, \(64\) in DiffusionRet (Jin et al., 2023) and UATVR, \(128\) for ProST, STAN, and UCOFIA except DiDeMo). We perform evaluation using metrics such as Recall at ranks \(\{1,5,10\}\), Median Rank (MdR), and Mean Rank (MnR).

### Performance Comparison

We compare the text-to-video retrieval performance of different methods in Table 1, 2, and 3. The proposed DITS can achieve the best performance on different CLIP backbones, datasets, and metrics. For example, on MSRVTT, DITS outperforms the baseline X-Pool by \(5\%\) at R@1 and outperforms T-MASS by \(1.7\%/2.3\%\) at R@1 on different backbones. On DiDeMo, DITS outperforms CLIP-ViT by \(2.2\%\) with CLIP-ViT-B/32 and \(5.3\%\) with CLIP-ViT-B/16, achieving a consistent boost on different metrics. Moreover, on the challenging dataset of Charades, DITS also brings a remarkable performance boost over X-Pool on both backbones, _e.g._, \(4.3\%\) and \(6.7\%\) at R@1, respectively. There is one scenario that T-MASS enables better retrieval than DITS on LSMDC with CLIP-ViT-B/32. However, our method gains an advantage with a larger CLIP backbone on all datasets, indicating better scalability on the retrieval model. We also note that DITS excels in retrieval top-ranked results, such as on MSRVTT, DiDeMo - a progressive alignment and gap modeling facilitates a narrower retrieval scope, encouraging retrieval precision. We provide in-depth analysis of this tendency in Section 4.4. Overall, extensive results on benchmark datasets demonstrate that DITS effectively aligns text-video embedding in the joint space, delivering promising retrieval results.

### Ablation Study

Table 4 provides an ablation study of the proposed method. Specifically, "Baseline" denotes the baseline method of X-Pool (Gorti et al., 2022), based on which we provide three types of the

  Methods & Alignment & \(_{2}\) & \(_{}\) & Truncation & R@1 & R@5 & R@10 & MdR & MnR \\   & ✗ & ✗ & ✓ & ✗ & 46.9 & 72.8 & 82.2 & 2.0 & 14.3 \\   & \(^{2}=1.0\) & ✓ & ✗ & ✓ & ✗ & 28.0 & 61.9 & 74.7 & 3.0 & 19.7 \\  & \(^{2}=0.1\) & ✓ & ✗ & ✓ & ✗ & 45.1 & 73.3 & 82.6 & 2.0 & 13.8 \\   & Pretrain & ✓ & ✓ & ✗ & ✗ & 35.4 & 66.7 & 78.3 & 3.0 & 14.4 \\  & Fine tune & ✓ & ✗ & ✓ & ✗ & 46.5 & 73.9 & 83.0 & 2.0 & 13.3 \\   & ✓ & ✗ & ✓ & ✓ & **51.9** & **75.7** & **84.6** & **1.0** & **11.6** \\  

Table 4: Ablation study of the proposed DITS on MSRVTT-1k. We adopt X-Pool (Gorti et al., 2022) as the “Baseline”. “Diffusion” denotes the Diffusion-based alignment method in Section 3.2.

  Methods & R@1 & R@5 & R@10 & MdR & MnR \\   & & & & \\ ClipBERT (Lei et al., 2021) & 6.7 & 17.3 & 25.2 & 32.0 & 149.7 \\ CLIP4Cip (Juo et al., 2022) & 9.9 & 27.1 & 36.8 & 21.0 & 85.4 \\ X-Pool (Gorti et al., 2022) & 11.2 & 28.3 & 38.8 & 20.0 & 82.7 \\ T-MASS (Wang et al., 2024a) & 14.2 & 36.2 & **48.3** & 12.0 & **54.8** \\ DITS (Ours) & **15.5** & **36.5** & 47.8 & **12.0** & 55.0 \\   & & & & \\ CLIP4Cip (Juo et al., 2022) & 16.0 & 38.2 & 48.5 & 12.0 & 54.1 \\ X-Pool (Gorti et al., 2022) & 20.7 & 42.5 & 53.5 & 9.0 & 47.4 \\ T-MASS (Wang et al., 2024a) & 26.7 & 51.7 & 63.9 & 5.0 & 30.0 \\ DITS (Ours) & **27.4** & **52.0** & **64.3** & **5.0** & **29.7** \\  

Table 3: Text-to-video comparisons on Charades (Sigurdsson et al., 2016). Bold denotes the best.

alignment methods, corresponding to Fig. 1. First, we show the naive modeling methods using two different fixed priors, such as \(^{2}=0.1,1.0\). Both of them bring performance descent. Fixed priors are inflexible, hardly benefiting the retrieval as expected. We notice that the sensitivity of the performance toward the prior. This also prompts us to leverage advanced methods of Diffusion model below. We provide the retrieval performance corresponding to both training stages of pretraining and fine-tuning as in Section 3.2. In pretraining, the reverse process is adopted for the generation. As shown, the pretraining with \(_{2}\) loss undermines the retrieval performance (_e.g._, \(35.4\%\)_v.s._\(46.9\%\)). Despite the fact that \(}_{2}\) loss effectively attracts relevant pairs (Fig. 3), it fails to align the irrelevant ones, leading to the performance descent. By comparison, when we fine-tune the model with the contrastive loss, we observe a remarkable boost of \(11.1\%\) at R@1 over pretraining and better results than baseline of X-Pool on metrics except R@1. This indicates that such a method, although enables a good retrieval scope (_e.g._, being favorable for R@5/10), but fails to performs accurate alignment and retrieval, underlying which, one main flaw lies in the random sampling in isotropic Gaussian. Drawing inspiration, the proposed DITS in the bottom line of Table 4 proposes a truncation process to alleviate the above concern, and adopts contrastive learning to guide the alignment. Encouragingly, we find an evident boost of implementing such a pipeline by following the variance schedule of the vanilla diffusion flow (_e.g._, \(_{t},_{t}\)). This prompt us to study the behavior of DITS in the following.

### Discussion on DITS

**Discussion on Truncated timestamps**. The number of the timestamps \(T^{}\) plays an important role for an accurate retrieval. We provide the performance of DITS at different timestamps in Table 4(a). The \(T^{}=0\) denotes the baseline of X-Pool. As shown, the performance of DITS first increases and then drops when we gradually enlarging \(T^{}\) from \(1\) to \(40\). DITS achieves the best performance at \(T^{}=10\). Specifically, one-step operation (\(T^{}=1\)) enables determining a rough retrieval scope, benefiting Recall over a larger range, _e.g._, R@10. Based on this, DITS requires more steps to narrow the scope for more accurate alignment and retrieval (_e.g._, \(T^{}=10\)). However, continually enlarging \(T^{}\) enforces DITS to learn an extremely precise result, sacrificing the flexibility to accommodate the testing data and in the end, resulting the loss of the generalization ability.

**Improving the CLIP Embedding Space**. We also study the effect of DITS to the CLIP in Table 4(b). There is a large performance gap when we fix CLIP or perform joint train for DITS. This indicates that solely performing the alignment in the original CLIP space is not enough. To this end, DITS not only plays the role of text-video embedding alignment, but also serves as a tool to guide the learning of CLIP, thus improving the CLIP space. As shown in Fig. 4, the distribution of the modality gap3 between two spaces deviates from each other, characterizing the aligning effect of DITS.

**Diffusion Model Conditions**. We perform different conditions of the \(_{}()\) in DITS, including with text condition **t**, with video condition **v**, and with both conditions **t** and **v**, as well as without any conditions (see discussions in Section 3.3). As shown in Table 5(a), we find that there is no need to introduce the conditions onto the network. The diffusion process for this task emphasizes "accurate alignment", unlike the general Diffusion models highlighting diversity in creative works. The previous intuition of the diffusion condition may not be applicable in this work: (1) Due to the one-to-many

Figure 4: DITS reduces the modality gap for the **relevant** pairs, serving as a tool to improve the space by guiding the CLIP’s learning.

Table 5: Discussion on DITS. Highlighted settings are adopted for the benchmark comparison.

mapping of video-to-text, one video can map to multiple gap vectors. Taking the video as a condition can guide the model learn the undesired modality gap, empirically dropping the performance. (2) DITS start from the text, ensuring the alignment starts from a semantically meaningful initial point. The text condition acts as the constraint or guideline for the learning. Simultaneously applying the text embedding as the starting point and the condition can cause conflicting instructions, empirically decreasing the performance. (3) Joint usage of text and video conditions can inherent both limitations, empirically leading to an unsatisfactory performance.

**Modality Gap Stemming from video embedding**. We also provide experiments when let \(=-\) in Table (b)b. As shown, such a setting brings sub-optimal performance. Imposing \(=-\) to the \(\) works like approximating \(\) for the retrieval, conducting the "text-to-text" retrieval, considering less semantic clues within the text, it might be hard to retrieve accurately. By comparison, the proposed method DITS implements \(=-\) to the \(\) works like approximating \(\) for the retrieval, performing the "video-to-video" retrieval. Since video contains much abundant clues, it might be easier to get better performance. Interesting, we notice that the previous work of Cap4video [Wu et al., 2023] also demonstrates similar tendencies in Table 5, denoted as Query-Caption/Video Only case.

## 5 Conclusion

This work studied the task of text-video retrieval by proposing a Diffusion-Inspired Truncated Sampler (DITS) for the multi-modality alignment. The primary contribution of this work was to offer insights on tailoring the Diffusion model for the ranking task inherent in retrieval. We uncovered two-fold limitations of the vanilla Diffusion models: the vanilla \(_{2}\) loss was inadequate for alignment by overlooking the irrelevant pairs, the the random initial sampling in isotropic Gaussian introduces variability, causing misalignment. We proposed DITS, which leveraged the inherent proximity of text and video embedding and directly started sampling from text to alleviate the sampling variability, and proposed to adopt the contrastive learning not only to guide the iterative alignment steps over time, but also to facilitate gap modeling. Extensive experiments demonstrated the state-of-the-art performance of DITS. We found DITS could encourage an improved embedding space by guiding the CLIP's learning. We hope DITS can inspire future explorations in studying Diffusion models in the task of text-video retrieval.

## 6 Acknowledgment

This research work was supported by the DEVCOM Army Research Laboratory under contract W911QX-21-D-0001.

Table 6: Discussion on DITS. Highlighted settings are adopted for the benchmark comparison.