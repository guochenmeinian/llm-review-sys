# Multivariate Probabilistic Time Series Forecasting

with Correlated Errors

 Vincent Zhihao Zheng

McGill University

Montreal, QC, Canada

zhihao.zheng@mail.mcgill.ca &Lijun Sun

McGill University

Montreal, QC, Canada

lijun.sun@mcgill.ca

Corresponding author.

###### Abstract

Accurately modeling the correlation structure of errors is critical for reliable uncertainty quantification in probabilistic time series forecasting. While recent deep learning models for multivariate time series have developed efficient parameterizations for time-varying contemporaneous covariance, but they often assume temporal independence of errors for simplicity. However, real-world data often exhibit significant error autocorrelation and cross-lag correlation due to factors such as missing covariates. In this paper, we introduce a plug-and-play method that learns the covariance structure of errors over multiple steps for autoregressive models with Gaussian-distributed errors. To ensure scalable inference and computational efficiency, we model the contemporaneous covariance using a low-rank-plus-diagonal parameterization and capture cross-covariance through a group of independent latent temporal processes. The learned covariance matrix is then used to calibrate predictions based on observed residuals. We evaluate our method on probabilistic models built on RNNs and Transformer architectures, and the results confirm the effectiveness of our approach in improving predictive accuracy and uncertainty quantification without significantly increasing the parameter size.

## 1 Introduction

Uncertainty quantification is crucial in time series forecasting, especially for applications that need more detailed insights than point forecasts. Probabilistic time series forecasting with deep learning (DL) has attracted attention for its ability to capture complex, nonlinear dependencies and provide the probability distribution of target variables [1; 2]. In multivariate time series, autoregressive models are widely used for probabilistic forecasting [3; 4; 5], modeling the joint one-step-ahead predictive distribution and generating multistep-ahead predictions in a rolling manner. To enable scalable learning, these models often assume that errors are independent over time. Typically, time series variables follow a Gaussian distribution \(_{t}=m(_{t})+_{t}\), where \(m()\) is the mean function and \(_{t}(,_{t})\) is a stochastic error process with contemporaneous covariance matrix \(_{t}\). The assumption of time-independence implies \((_{s},_{t})=\), \( s t\). This holds when the model can account for all correlations between successive time steps through hidden states determined by previous values. However, real-world data often violate this assumption, as residuals exhibit substantial cross-correlation due to omission of important covariates and model misspecification.

Modeling error autocorrelation (or cross-correlation) is a key area of research in statistical time series models. A common approach is to assume that the error series follows a dependent temporal process, such as an autoregressive integrated moving average (ARIMA) model . Deep learning models face similar challenges. Previous studies have attempted to incorporate temporally-correlated errors into the training process by modifying the loss function [7; 8]. However, these methods, based ondeterministic output, are not easily applicable to probabilistic forecasting models, particularly in a multivariate setting. A notable innovation is the batch training method introduced by Zheng et al. , which trains a univariate probabilistic forecasting model using generalized least squares (GLS) loss over batched errors. This approach parameterizes a dynamic covariance matrix to capture error autocorrelation, which is then used to calibrate the predictive distribution of time series variables. While this method consistently improves probabilistic forecasting performance compared to naive training (i.e., without considering autocorrelated errors); however, it is only applicable to univariate models, such as DeepAR .

In this paper, we introduce an efficient method for learning error cross-correlation in multivariate probabilistic forecasting models. Our focus is on deep learning models that are autoregressive with Gaussian-distributed errors. Modeling cross-correlation in multivariate models presents challenges due to increased dimensionality, as the covariance matrix scales with the number of time series \(N\). To address this computational challenge, we propose characterizing error cross-correlation through a set of independent latent temporal processes using a low-rank parameterization of the covariance matrix. This approach prevents the computational cost from growing with the number of time series. Our method offers a general-purpose approach to multivariate probabilistic forecasting models, offering significantly improved predictive accuracy.

**Contributions:**

1. We introduce a plug-and-play method for training autoregressive multivariate probabilistic forecasting models using a redesigned GLS loss. (SS4)
2. We propose an efficient parameterization of the error covariance matrix across multiple steps, enabling efficient computation of its inverse and determinant through matrix inversion and determinant lemmas. (SS4.1)
3. The learned covariance matrix is used to fine-tune the predictive distribution based on observed residuals. (SS4.2)
4. We demonstrate that the proposed method effectively captures error cross-correlation and improves prediction quality. Notably, these improvements are achieved through a statistical formulation without significantly increasing the size of model parameters. (SS5)

## 2 Probabilistic Time Series Forecasting

Denote \(_{t}=[z_{1,t},,z_{N,t}]^{}^{N}\) as the vector of time series variables at time step \(t\), where \(N\) is the number of time series. Probabilistic time series forecasting can be formulated as estimating the joint conditional distribution \(p(_{T+1:T+Q}_{T-P+1:T};_{T-P+1:T+Q})\) given the observed history \(\{_{t}\}_{t=1}^{T}\), where \(_{t_{1}:t_{2}}=[_{t_{1}},,_{t_{2}}]\) and \(_{t}\) are known time-dependent covariates (e.g., time of day, day of week) for all future time steps. In essence, the problem involves predicting the time series values for \(Q\) future time steps using all available covariates and \(P\) steps of historical time series data:

\[p(_{T+1:T+Q}_{T-P+1:T};_{T-P+1:T+Q} )=_{t=T+1}^{T+Q}p(_{t}_{t-P :t-1};_{t-P:t}),\] (1)

which becomes an autoregressive model that can be used for either one-step-ahead (\(Q=1\)) or multistep-ahead forecasting in a rolling manner. When performing multistep-ahead forecasting, samples are drawn in the prediction range (\(t T+1\)) and fed back for the next time step until the end of the desired prediction range. In neural networks, the conditioning information is commonly encoded into a state vector \(_{t}\). Hence, Eq. (1) can be expressed more concisely:

\[p(_{T+1:T+Q}_{T-P+1:T};_{T-P+1:T+Q} )=_{t=T+1}^{T+Q}p(_{t}_{t} ),\] (2)

where \(_{t}\) is mapped to the parameters of a parametric distribution (e.g., multivariate Gaussian).

Existing autoregressive models typically assume that the error at each time step is independent, meaning that \(_{t}\) follows a multivariate Gaussian distribution:

\[_{t}_{t}(( _{t}),(_{t})),\] (3)

where \(()\) and \(()\) map \(_{t}\) to the mean and covariance parameters of a multivariate Gaussian distribution. This formulation can be decomposed as \(_{t}=_{t}+_{t}\) with \(_{t}(,_{t})\). The temporally independent error assumption corresponds to \((_{s},_{t})=\) for any time points \(s\) and \(t\) where \(s t\). Fig. 1 provides an empirical example of the contemporaneous covariance matrix \((_{t},_{t})\) and cross-covariance matrix \((_{t-},_{t}),=1,2,3\). The results are calculated based on the prediction residuals of GPVar  on the m4_hourly dataset. While multivariate models primarily focus on contemporaneous covariance, the residuals clearly exhibit temporal dependence, as \((_{t-},_{t})\). This non-zero cross-covariance suggests that residuals still contain valuable information, which can be leveraged to improve predictions.

## 3 Related Work

### Probabilistic Time Series Forecasting

Probabilistic forecasting aims to model the probability distribution of target variables, unlike deterministic forecasting, which produces only point estimates. There are two main approaches: parametric probability density functions (PDFs) and quantile functions . For example, MQ-RNN  generates quantile forecasts using a sequence-to-sequence (Seq2Seq) RNN architecture. In contrast, PDF-based approaches assume a specific distribution (e.g., Gaussian, Poisson) and use neural networks to generate the distribution parameters. DeepAR , for instance, uses an RNN to model hidden state transitions, while its multivariate version, GPVar , employs a Gaussian copula to transform observations into Gaussian variables, assuming a joint multivariate Gaussian distribution.

Neural networks can also generate probabilistic model parameters. The deep state space model (SSM)  uses an RNN to generate SSM parameters. The normalizing Kalman filter (NKF)  combines normalizing flows (NFs) with the linear Gaussian state space model (LGM) to model nonlinear dynamics and evaluate the PDF of observations. NKF uses RNNs to produce LGM parameters at each time step, then transforms the LGM output into observations using NFs. Wang et al.  proposed the deep factor model, which includes a deterministic global component parameterized by an RNN and a random component from any classical probabilistic model (e.g., Gaussian white noise) to represent random effects. Some methods improve expressive conditioning for probabilistic forecasting by using Transformer instead of RNNs to model latent state dynamics, thus breaking the Markovian assumption in RNNs . Other approaches adopt more flexible distribution forms, including normalizing flows , diffusion models , and copulas [16; 17]. For a recent and comprehensive review, we refer readers to Benidis et al. .

### Modeling Correlated Errors

Error correlation in time series has been extensively studied in econometrics and statistics [18; 6; 19]. In multivariate time series, correlation structure is characterized by contemporaneous covariance \((_{t})=(_{t},_{t})\) and cross-covariance \((_{t-},_{t})\). Cross-covariance includes both the autocovariance of errors \((_{i,t-},_{i,t})\) and the cross-lag covariance \((_{i,t-},_{j,t})\) between pairs of components in the multivariate series. Contemporaneous covariance captures the correlation among individual time series at a specific point in time. In the univariate setting, DeepAR  achieves probabilistic forecasting by modeling the contemporaneous covariance, assuming that errors are independent over time. To address autocorrelation, Sun et al.  re-parameterized the input and output of neural networks to model first-order error autocorrelation, effectively capturing serially correlated errors using an AR\((1)\) process. This method improves the performance of one-step-ahead neural forecasting models, allowing joint optimization of base and error regressors, but is limited to deterministic models. In spatial modeling, Saha et al.  introduced the RF-GLS model, which uses random forests to estimate nonlinear covariate effects and Gaussian processes (GP) to model spatial random effects. The RF-GLS model assumes that the error process follows an AR\((p)\) process to accommodate autocorrelated errors. Zheng et al.  proposed training a probabilistic forecasting model with a GLS loss that explicitly models the time-varying autocorrelation of batched error terms, extending DeepAR to incorporate autocorrelated errors.

In the multivariate setting, most existing work focuses on modeling contemporaneous covariance, assuming that \(_{t}\) is independently distributed, which implies \((_{t-},_{t})=\). For example, GPVar  generalizes DeepAR  to account for correlations between time series by viewing the distribution of time series variables as a Gaussian process. In Seq2Seq models, correlations can span across series and forecasting steps, as predictions for future time steps are generated simultaneously. Since predictions for future time steps are generated simultaneously, we refer to these correlations as contemporaneous correlations within the scope of this study. Choi et al.  introduced a dynamic mixture of matrix Gaussian distributions to capture contemporaneous covariance of errors in Seq2Seq models. One exception that explicitly models error cross-correlation is , where the authors assume that the matrix-variate error term of a multivariate Seq2Seq model follows a matrix autoregressive (AR) process with seasonal lags. However, applying this technique to probabilistic forecasting models is not straightforward.

To the best of our knowledge, our work is the first to model cross-covariance in multivariate probabilistic time series forecasting. The closest related studies are by Zheng et al.  and Zheng et al. . Zheng et al.  applies GLS loss in the temporal domain to model autocorrelated errors, but their approach is tailored for univariate time series. Zheng et al.  models cross-covariance in multivariate forecasting models, but their method is limited to deterministic models and requires predefined seasonal lags in the error autoregressive process. Our work extends  to the multivariate setting, enabling the modeling of the correlation structure of multivariate errors across multiple steps. In addition, we distinguish our approach from methods that directly model the distribution of time series variables, such as Copulas [16; 17], where no decomposition of the error term is provided.

## 4 Our Method

Our methodology builds upon the formulation outlined in Eq. (2), employing an autoregressive model as its foundational framework. Using an RNN as an example, a probabilistic forecasting model consists of two components. Firstly, it incorporates a transition model \(f_{}\) to capture the dynamics of state transitions \(_{t}=f_{}(_{t-1},_{t-1},_{t })\), thus inherently having autoregressive properties. Second, it integrates a distribution head, represented by \(\), which maps \(_{t}\) to the parameters of the desired probability distribution. Following GPVar , our approach employs the multivariate Gaussian distribution as the distribution head. The time series variable can be decomposed into a deterministic mean component and a random error component \(_{t}=_{t}+_{t}\), where \(_{t}(,_{t})\). To efficiently model the covariance \(_{t}\) for large \(N\), GPVar adopts a low-rank-plus-diagonal parameterization \(_{t}=_{t}_{t}^{}+(_{t})\), where \(_{t}^{N R}\) (\(R N\)) and \(_{t}_{+}^{N}\). Autoregressive models based on Gaussian likelihood typically assume that \(_{t}\) are independently distributed following a multivariate Gaussian distribution. The log-likelihood of the distribution serves as the loss function for optimizing the model:

\[=_{t=1}^{T} p(_{t}( _{t}))_{t=1}^{T}-[] _{t}]+_{t}^{}_{t}^{- 1}_{t}].\] (4)

The parameters \((_{t})\) are parameterized as \((_{t},_{t},_{t})\), where \(_{t}^{N}\) represents the mean vector of the distribution. \(_{t}\) and \(_{t}\) correspond to the covariance factor and diagonal elements in the low-rank parameterization of the multivariate Gaussian distribution. We use shared mapping functions for all time series:

\[_{i}(_{i,t}) =(_{i,t})=_{}^{} _{i,t},\] (5) \[d_{i}(_{i,t}) =(_{i,t})=(1+(_{d}^{} _{i,t})),\] \[l_{i}(_{i,t}) =(_{i,t})=W_{i}_{i,t},\]where \(_{i,t}^{H}\), \(_{}^{H}\), \(_{d}^{H}\), and \(W_{l}^{R H}\) are parameters. Since the parameters of the mapping functions are shared across all time series, we can use a random subset of time series to compute the Gaussian likelihood-based loss in each optimization step, as any subset of \(_{t}\) will still follow a multivariate Gaussian distribution. In other words, we can train the model with a substantially reduced batch size \(B<N\).

### Training with Correlated Errors

We build upon the approach introduced in  to address cross-correlated errors in a multivariate context by introducing time-dependent error terms \(_{t}\) into the GLS loss. In many existing deep probabilistic forecasting models, such as GPVar , a training batch typically consists of a sample slice of \(B\) time series spanning a temporal length of \(P+Q\), where \(P\) is the conditioning range and \(Q\) is the prediction range. The Gaussian likelihood is evaluated independently at each time step within the prediction range through one-step-ahead predictions. However, this approach overlooks the serial correlation of errors across consecutive time steps. To address this limitation, we propose modifying the likelihood function by introducing a dynamic covariance that accommodates the temporal dependence of the error term, as illustrated in Fig. 2. To achieve this, we organize \(D\) smaller slices of time series with a temporal length of \(P+1\) (i.e., \(Q=1\)), sorted by the prediction start time in sequential order, where \(D\) represents the time horizon over which we consider cross-correlation. The new batch structure effectively reconstructs the conventional training batch, covering the same time horizon when \(D=Q\). An example of the collection of target time series variables in a batch covering cross-correlation horizon \(D\) is given by

\[_{t-D+1}&= _{t-D+1}+_{t-D+1},\\ _{t-D+2}&=_{t-D+2}+ _{t-D+2},\\ &\\ _{t}&=_{t}+_{t}, \] (6)

where for time point \(t^{}\), \(_{t^{}}\), \(_{t^{}}\) and \(_{t^{}}\) are the outputs of the model. The covariance parameterization in GPVar corresponds to

\[_{t^{}}=_{t^{}}_{t^{ }}+_{t^{}},\] (7)

where \(_{t^{}}(,_{R})\) is a low-dimensional latent variable, and \(_{t^{}}(,(_{t^{}}))\) is an additional error independent of \(_{t^{}}\). We denote \(_{t}^{}=(_{t-D+1:t}) ^{DB}\) as the collection of target time series variables in a batch, where \(()\) is an operator that stacks all the columns of a matrix into a vector. Similarly, we define \(_{t}^{}^{DB}\), \(_{t}^{}^{DR}\), \(_{t}^{}^{DB}\), \(_{t}^{}_{+}^{DB}\), and \(_{t}^{}=(\{_{t^{} }\}_{t^{}=t-D+1}^{t})^{DB D}R\), where \(_{t}^{}\) has a block diagonal structure (see Fig. 2). The batch-wise decomposition is then expressed as

\[_{t}^{}=_{t}^{}+ {L}_{t}^{}_{t}^{}+_{ t}^{}.\] (8)

Figure 2: Graphic illustration of Eq. (8), where \(B\) is the number of time series in a batch, \(R\) is the rank of the covariance factor, \(D\) is the time window we consider cross-correlation, \(P\) and \(Q\) are the conditioning range and prediction range. Cross-correlation is modeled by introducing correlation in each row of matrix \(_{t-D+1:t}\).

The default GPVar model assumes the latent variable \(_{t}\) is temporally independent, meaning \((_{s},_{t})=, s t\). However, this assumption cannot capture the potential cross-correlation in the errors. To address this, we introduce temporal dependencies in the latent variable within a batch by assuming \(_{t}^{}(,_{t}_{ R})\), where \(_{t}\) is a dynamic \(D D\) correlation matrix. This approach assumes that the rows in the matrix \(_{t-D+1:t}=[_{t-D+1},,_{t}]\) are independent and identically distributed, following \((,_{t})\). To efficiently capture dynamic patterns over time, we follow Zheng et al.  and express \(_{t}\) as a dynamic weighted sum base kernel matrices: \(_{t}=_{m=1}^{M}w_{m,t}_{m}\), where \(w_{m,t} 0\) (with \(_{m}w_{m,t}=1\)) represents the weights for each component. For simplicity, we model each component \(_{m}\) using a kernel matrix generated from a squared-exponential (SE) kernel function, where the \((i,j)\)-th entry is \(_{m}^{ij}=(-}{l_{m}^{2}})\), with different lengthscales \(l_{m}\) (e.g., \(l=1,2,3,\)). In addition, we incorporate an identity matrix into the additive structure to account for the independent noise process. This parameterization ensures that \(_{t}\) is a positive definite symmetric matrix with unit diagonals, making it a valid correlation matrix. The weights for these components are derived from the hidden state \(_{t}\) at each time step through a small neural network, with the number of nodes in the output layer set to \(M\) (i.e., the number of components). A softmax layer is used to ensure that these weights are summed up to 1. Note that the parameters of this network will be learned simultaneously with those of the base model.

Marginalizing out \(_{t}^{}\) in Eq.8, we have \(_{t}^{}(_{t}^{},_{t}^{})\) with covariance

\[_{t}^{}=(_{t}^{})(_{t}_{R})(_{t}^{})^{}+(_{t}^{}).\] (9)

It is straightforward to derive that for any \(i,j\{0,1,,D-1\}\) and \(i j\), the proposed model creates cross-covariance \((_{t-i},_{t-j})=_{t}^{ij}_{t-i}_{t-j}^{}\) between times \(t-i\) and \(t-j\), which is no longer \(\). While this parameterization results in a non-stationary multivariate process through varying coregionalization , a key difference is that both the coregionalization coefficient matrix \(_{t}\) and the temporal correlation \(_{t}\) are generated by a deep neural network. In this sense, our model can better characterize the empirical cross-covariance matrices of the residuals (see empirical examples in Fig.1). As \(_{t}^{}\), \(_{t}^{}\), and \(_{t}^{}\) are default outputs of the base probabilistic model, we can compute the overall likelihood (with overlapped data) as

\[=_{t=D}^{T} p(_{t}^{}_{t}^{},_{t}^{}).\] (10)

Here, computing the log-likelihood involves evaluating the inverse and the determinant of \(_{t}^{}\) with size \(DB DB\), for which a naive implementation has a prohibitive time complexity of \((D^{3}B^{3})\). However, our parameterization of \(_{t}^{}\) as \(+^{}\), where \(=(_{t}^{})\), \(=_{t}^{}\), and \(=_{t}_{R}\), allows us to leverage the Sherman-Morrison-Woodbury identity (matrix inversion lemma) and the companion matrix determinant lemma to simplify the computation:

\[(+^{})^{-1}&=^{-1 }-^{-1}(^{-1}+^{}^{-1})^{-1}^{ }^{-1},\\ +^{})}&=^{-1}+^{ }^{-1})})})}.\] (11)

Then, the likelihood calculation only requires computing the inverse and determinant of a \(DR DR\) matrix, specifically \(^{-1}+^{}^{-1}\). These computations can be efficiently performed using Cholesky factorization. Detailed computations are provided in Appendix SSA.2.

Figure 3: Illustration of the training process. Following , time series dimensions are randomly sampled, and the base model (e.g., RNNs) is unrolled for each dimension individually (e.g., 1, 2, 4, followed by 1, 3, 4 as depicted). The model parameters are shared across all time series dimensions. A batch of time series variables \(_{t}^{}\) contains time series vectors \(_{t}\) covering time steps from \(t-D+1\) to \(t\). In contrast to , our approach explicitly models dependencies over the extended temporal window from \(t-D+1\) to \(t\) during training.

Modeling the latent process \(_{t}\) offers several advantages. Firstly, because \(_{t}\) has a much lower dimension than \(_{t}\), modeling the cross-correlation of \(_{t}\) results in a significantly smaller \(DR DR\) covariance matrix compared to the \(DB DB\) covariance matrix of \(_{t}\). Secondly, since \(_{t}\) follows an isotropic Gaussian distribution, the covariance of \(_{t}^{}\) can be parameterized with a Kronecker structure \(_{t}_{R}\). This greatly simplifies the task into learning a \(D D\) correlation matrix shared by all time series in a batch. Lastly, similar to GPVar, we can still train the model in an end-to-end manner using a subset of time series in each iteration to ensure computational efficiency (Fig. 3).

### Multistep-ahead Rolling Prediction

Autoregressive models perform multistep-ahead forecasting in an iterative manner, where the model generates a sample at each time step during prediction, using it as input for the subsequent step, and continuing this process until the desired prediction range is reached. Our approach enhances this process, similar to Zheng et al. , by offering additional calibration based on the learned correlation matrix \(_{t}\). Assuming observations are available up to time step \(t\), the conditional distribution of \(_{t+1}\) given errors in the past \((D-1)\) steps, can be derived as

\[_{t+1}_{t},_{t-1},,_{t-D+2} (_{*}_{}^{-1}_{ {obs}},_{t+1}-_{*}_{}^{-1}_{*}^{}),\] (12)

where \(_{}=([_{t-D+2},,_{t-1},_{t}])^{(D-1)B}\) represents the set of residuals, accessible at forecasting step \(t+1\). Here, \(_{}\) is a \((D-1)B(D-1)B\) partition of \(_{t+1}^{}\) that captures the covariance of \(_{}\), and \(_{*}\) is a \(B(D-1)B\) partition of \(_{t+1}^{}\) representing the covariance between \(_{t+1}\) and \(_{}\), i.e., \(_{t+1}^{}=_{}&_{*}^{}\\ _{*}&_{t+1}\). For conciseness, we omit the time index \(t\) in \(_{}\), \(_{*}\) and \(_{}\). Since \(_{t+1}\) is a deterministic output from the base model, a sample of the target variables \(}_{t+1}\) can be derived by first drawing a sample \(}_{t+1}\) from Eq. (12), then combining it with the predicted mean vector \(_{t+1}\) as \(}_{t+1}=_{t+1}+}_{t+1}\). It should be noted that we can still leverage the Sherman-Morrison-Woodbury identity when computing the inverse \(_{}^{-1}\).

By taking the sample \(}_{t+1}\) as an observed residual, we can iteratively apply the process described in Eq. (12) to derive a trajectory of \(\{}_{t+q}\}_{q=1}^{Q}\). Repeating this procedure allows us to generate multiple samples, characterizing the predictive distribution at each time step.

## 5 Experiments

### Evaluation of Predictive Performance

**Datasets**. We use widely recognized time series benchmarking datasets from GluonTS . The prediction range (\(Q\)) for each dataset follows the configurations provided by GluonTS. We applied a sequential split into training, validation, and testing sets for each dataset. Each dataset was standardized using the mean and standard deviation from the training set, and predictions were rescaled to their original values for evaluation. Further details on the datasets can be found in Appendix SSA.1.

**Base probabilistic models**. We integrated the proposed method into two distinct autoregressive models: the RNN-based GPVar  and the decoder-only Transformer . These models are trained to generate distribution parameters as described in SS4. Our approach can be applied to other autoregressive multivariate models with minimal adjustments, provided the final prediction follows a multivariate Gaussian distribution. The implementation is based on using PyTorch Forecasting . Both models use lagged time series values and additional features or covariates as inputs. Details on model training ( SSA.3), hyperparameter tuning ( SSA.5), and the base model (SA.6) are provided in Appendix SSA. The code is available at https://github.com/rottenivy/mv_pts_correlatederr.

**Dynamic correlation matrix**. We introduce a limited number of additional parameters to project the state vector \(_{t}\) into component weights \(w_{m,t}\), which are used to generate the dynamic correlation matrix \(_{t}\). The number of base kernels (\(M\)) for generating \(_{t}\) and the associated lengthscale set \(\{l_{m}\}_{m=1}^{M-1}\) are treated as hyperparameters. We perform a grid search over \(M=2,3,4\) and two sets of lengthscales--\(\{0.5,1.5,\}\) and \(\{1.0,2.0,\}\). Models with the best validation loss are selected. These different lengthscales capture varying correlation decay rates, enabling the model to account for different temporal patterns. The time-varying component weights enable dynamic adaptation to changing correlation structures over time.

**Baselines**. We evaluate the proposed method by comparing it with a baseline model trained without accounting for error cross-correlation (Eq. (4)). The baseline model represents a special case of our model with \(_{t}=_{D}\). To ensure a straightforward and fair comparison, we align the cross-correlation range (\(D\)) with the prediction range (\(Q\)), ensuring identical data sampling processes for both methods. Additionally, we set \(P=Q\) following the default configuration in GluonTS. We also include VAR and GARCH as naive baseline models (see Appendix SSA.4).

**Metrics**. We use the Continuous Ranked Probability Score (CRPS)  as the main metric:

\[(F,z)=_{F}|Z-z|-_{F}|Z-Z^{ }|,\] (13)

where \(F\) is the cumulative distribution function (CDF) of the predicted variable, \(z\) is the observation, \(Z\) and \(Z^{}\) are independent copies of the prediction samples associated with the distribution \(F\). To evaluate multivariate dependencies in the time series data, we compute \(_{}\) by first summing both the forecast and ground-truth values across all time series and then calculating the \(\) over the resulting sums . As \(_{}\) may overlook model performance on individual dimensions , we also report additional metrics, e.g., the energy score , in Appendix SSB.1.

**Training dynamics**. Our approach incurs additional training costs per optimization step due to the more complex likelihood function. As shown in Appendix SSB.3, the training time per epoch for models using our method is generally longer than that of baseline methods. However, our parameterization allows for scalability to large time series datasets by using a small random subset of time series at each optimization step during training.

**Benchmark results**. The \(_{}\) results are presented in Table 1. Our method achieves an average improvement of 13.79% for GPVar and 6.91% for the Transformer model. It is important to note that the degree of performance enhancement varies across different base models and datasets, influenced by factors such as the inherent data characteristics and the performance of different model architectures. The alignment between the actual correlation structure and our kernel assumption also plays a crucial role in the effectiveness of our method. Additionally, our approach demonstrates consistent improvements across five different metrics, with significant gains in multivariate metrics such as the energy score (Appendix SSB.2).

To provide further insights, we compare the residual autocorrelation and cross-lag correlation with and without applying our method in Appendix SSB.5.1, showing that our method effectively reduces cross-correlations in many scenarios. We use ACF plot comparisons to illustrate the reduction in autocorrelation and cross-correlation plot comparisons to demonstrate the decrease in cross-lag correlation. The residuals generated by the model with our method exhibit weaker cross-correlations, which is particularly enhanced by the calibration process during prediction (SS4.2).

Furthermore, Appendix SSB.5.2 separates the accuracy improvement over forecast steps for each dataset. The performance improvement is shown to be related to both the absolute time across the

    & VAR & GARCH & GPVar &  \\   & & w/o & w/ & w/o & w/ \\  exchange\_rate & 0.003340.0000 & 0.043540.0001 & 0.006840.0004 & 0.011740.0004 & 0.005540.0002 & **0.004240.0002** \\ solar & 0.766340.0050 & 0.875240.0015 & 0.710340.0065 & **0.692940.0039** & 0.496040.0034 & **0.413240.0027** \\ electricity & 0.126404.0006 & 0.284740.0015 & 0.043040.0005 & **0.040340.0004** & 0.09440.0004 & 0.063840.0003 \\ traffic & 3.524140.0084 & 0.445900.0005 & 0.109540.0002 & **0.046940.0002** & 0.071700.0002 & 0.098140.0002 \\ wikipedia & 26.202540.0389 & 0.669940.00045 & 0.174540.0008 & **0.074340.0009** & 0.084140.0013 & **0.05004.0005** \\ m\_hourly & 0.235240.0008 & 0.275840.0006 & 0.061340.0004 & **0.035840.0002** & 0.065140.0004 & **0.06160.0003** \\ m\_quarterly & N/A & N/A & 0.394240.0003 & **0.353840.00017** & 0.444800.0027 & **0.436740.0028** \\ pens03 & 0.059840.0002 & 0.320240.0007 & 0.050540.0001 & **0.049140.0002** & 0.049040.0001 & **0.038640.0001** \\ uber\_hourly & N/A & N/A & 0.034240.0006 & **0.022240.0004** & 0.063240.0003 & **0.051340.0005** \\   

Table 1: \(_{}\) accuracy comparison. “w/o” denotes methods without time-dependent errors, while “w/” indicates our method. Bold values show models with time-dependent errors performing better. Mean and standard deviation are obtained from 10 runs of each model.

temporal span of the dataset (especially for time series with strong periodic patterns) and the relative time over the prediction horizon.

### Model Interpretation

Our method captures error cross-correlation through the dynamic construction of a covariance matrix, achieved by combining kernel matrices with varying lengthscales in a dynamically weighted sum. A small lengthscale corresponds to short-range positive correlations, while a large lengthscale captures positive correlations over longer lags.

In Fig. 4, we depict the dynamic component weights and the resulting autocorrelation function (the first row of the correlation matrix \(_{t}\)) for a batch of time series from the m4_hourly dataset spanning a four-day window. We also provide the covariance matrix of \(_{t}^{}\) using the correlation matrix and model outputs at a specific time of day. The component weight \(w_{3}\), corresponding to the identity matrix, dominates throughout the observation period. This suggests that the error correlation is generally mild over time. This behavior is influenced by the Kronecker structure used to parameterize the covariance over the low-dimensional latent variables \(_{t}\), which assumes all latent processes share the same autocorrelation structure. Given the Kronecker structure, the model tends to learn the mildest temporal correlation among the time series in a batch.

Moreover, we observe that the dynamic component weights adjust the correlation strengths. Specifically, when the weight assigned to the identity matrix (\(w_{3}\)) increases, the error process tends to be more independent. In contrast, when the weights assigned to the other kernel matrices (\(w_{0}\), \(w_{1}\), and \(w_{2}\)) are larger, the error process becomes more correlated, as the kernel matrices with different lengthscales combine to formulate a specific correlation structure. Fig. 4(a) demonstrates pronounced daily patterns in temporal correlation, particularly when errors exhibit increased correlation around 17:00 each day. The corresponding autocorrelation function is shown in Fig. 4(b). Fig. 4(c) illustrates the corresponding covariance matrix of the associated target variables within the cross-correlation horizon. The diagonal blocks represent the contemporaneous covariance \(_{t}\) of \(_{t}\) at each time step, while the off-diagonal blocks capture the cross-covariance \((_{t-},_{t})\) for \( 0\), effectively modeled by our approach. The zoomed-in view provides a \(3B 3B\) region that illustrates the cross-covariance within two lags. We observe that the cross-covariance is most pronounced at lag 1, consistent with the observation in Fig. 4(a) that the component weight \(w_{0}\), assigned to the base kernel matrix with lengthscale \(l=0.5\), is more pronounced than \(w_{1}\) and \(w_{2}\).

## 6 Discussion

In this section, we discuss factors that influence the performance of our method. Specifically, we highlight the effectiveness of our model in long-term forecasting across various scenarios. We also

Figure 4: (a) Component weights for generating \(_{t}\) for a batch of time series (\(B=8\)) from the m4_hourly dataset obtained by the GPVar model. Parameters \(w_{0},w_{1},w_{2}\) represent the component weights of the kernel matrices associated with lengthscales \(l=0.5,1.5,2.5\), and \(w_{3}\) is the component weight of the identity matrix. Shaded areas distinguish different days; (b) The autocorrelation function (ACF) indicated by the correlation matrix \(_{t}\) at 17:00. Given the rapid decay of the ACF, we only plot 12 lags to enhance visualization; (c) The corresponding covariance matrix of the associated target variables \(_{t}^{}\) at 17:00. A zoom-in view of a \(3B 3B\) region is illustrated in the plot, where the diagonal blocks represent \(B B\) covariance matrices \(_{t^{}}\) of \(_{t^{}}\) over three consecutive time steps. The off-diagonal blocks describe the cross-covariance \((_{t-},_{t})\), \( 0\). For visualization clarity, covariance values are clipped to the range \([0,0.03]\).

discuss the effect of scaling up to larger batch sizes during prediction. Additionally, we examine the impact of non-Gaussian errors on model performance.

**Long-term forecasting**. The advantage of modeling error correlation can vary in long-term forecasting, especially in autoregressive predictions where errors accumulate and propagate over time. Using residuals from previous time steps to calibrate forecasts may be beneficial for non-stationary segments of the time series. However, for time series with strong periodic effects, the model may also rely on seasonal lags. As shown in Fig. 21 and Fig. 22 of the Appendix, the advantage of modeling error correlation can decrease in longer-term forecasts compared to shorter-term forecasts for some datasets with strong periodic effects (e.g., the traffic dataset in Fig. 21). It is not necessarily true that the advantage diminishes for long-horizon predictions, as the effectiveness of our method depends on the quality of predictions during inference. In cases where the model provides accurate long-term forecasts, the benefit of modeling correlated errors may be less pronounced.

**Scalability**. Increasing the number of time series \(B\) in a batch leads to higher training costs. Because the model requires numerous iterations over the dataset for optimization, using a large \(B\) during training is not feasible. However, during prediction, the batch size can be increased to leverage more information. This may enhance both prediction accuracy and error calibration, provided sufficient memory is available. We demonstrate the effect of increasing batch size during prediction in Appendix SSB.4 through additional experiments. Both models, with and without our method, show improvement from increased batch sizes during prediction, as reflected by a decrease in \(_{}\).

**Non-Gaussian errors**. For the baseline model, assuming Gaussian errors may lead to model misspecification, resulting in more correlated residuals. To address this issue, we also trained the baseline models using the likelihood of a multivariate \(t\)-distribution; the results are presented in Table 15 of the Appendix. Although using an alternative distribution can lead to better performance on some datasets without our method, we observed that our method effectively closes the performance gap when the \(t\)-distribution outperforms the Gaussian assumption. We chose the Gaussian distribution for its beneficial properties, including its marginalization rule and well-defined conditional distributions, both essential for statistically consistent model training and reliable inference. Thus, a more effective approach could involve first transforming the original observations into Gaussian-distributed data using a Gaussian Copula , followed by applying our method.

## 7 Conclusion and Broader Impacts

This paper presents a novel approach for addressing error cross-correlation in multivariate probabilistic time series forecasting, specifically for models with autoregressive properties and Gaussian distribution outputs. We construct a dynamic covariance matrix using a small set of independent and identically distributed latent temporal processes. These latent processes effectively model temporal correlation and integrate seamlessly into the base model, where the contemporaneous covariance is parameterized by a low-rank-plus-diagonal structure. This approach enables the modeling and prediction of a time-varying covariance matrix for the target time series variables. The experimental results demonstrate its effectiveness in enhancing uncertainty quantification.

Our contributions are two-fold. First, our approach relaxes the time-independent error assumption during the training process for probabilistic forecasting models, addressing the reality that residuals are typically time-dependent. Second, the learned cross-correlation improves multistep-ahead predictions by refining the distribution output at each forecasting step. These enhancements to existing models have broader implications for fields such as finance, healthcare, and energy, where improved forecasts and uncertainty quantification can lead to more informed decisions.

There are several avenues for future research. First, the Kronecker structure \(_{t}_{R}\) for the covariance matrix of the latent variable \(_{t}^{}\) may be too restrictive for multivariate time series problems. Exploring more flexible covariance structures, such as employing different \(_{r,t}\) matrices for each latent temporal process as in the linear model of coregionalization (LMC, ), could be a promising direction for further investigation. Second, the parameterization of \(_{t}\) could be expanded. Instead of using SE kernels, \(_{t}\) could be parameterized as fully learnable positive definite symmetric Toeplitz matrices. For example, an AR(\(p\)) process has a covariance structure in Toeplitz form, allowing for the modeling of negative correlations. This alternative approach could offer greater flexibility in capturing complex correlation patterns in multivariate time series data.