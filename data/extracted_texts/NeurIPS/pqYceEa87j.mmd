# Spectral Editing of Activations for

Large Language Model Alignment

\({}^{1}\)Yifu Qiu, \({}^{1}\)Zheng Zhao, \({}^{3}\)Yftah Ziser,

\({}^{2}\)**Anna Korhonen, \({}^{1}\)Edoardo M. Ponti, \({}^{1}\)Shay B. Cohen**

\({}^{1}\)Institute for Language, Cognition and Computation, University of Edinburgh

\({}^{2}\)Language Technology Lab, University of Cambridge

\({}^{3}\)Nvidia Research

{yifu.qiu,zheng.zhao,eponti,scohen}@ed.ac.uk

###### Abstract

Large language models (LLMs) often exhibit undesirable behaviours, such as generating untruthful or biased content. Editing their internal representations has been shown to be effective in mitigating such behaviours on top of the existing alignment methods. We propose a novel _inference-time_ editing method, namely spectral editing of activations (SEA), to project the input representations into directions with _maximal_ covariance with the positive demonstrations (e.g., truthful) while _minimising_ covariance with the negative demonstrations (e.g., hallucinated). We also extend our method to non-linear editing using feature functions. We run extensive experiments on benchmarks concerning truthfulness and bias with six open-source LLMs of different sizes and model families. The results demonstrate the superiority of SEA in effectiveness, generalisation to similar tasks, as well as computation and data efficiency. We also show that SEA editing only has a limited negative impact on other model capabilities.1

## 1 Introduction

While large language models (LLMs) have taken a central place in the development of full-fledged natural language processing (NLP) applications, there is a fundamental problem that prevents them from being fully trusted in real-world deployment: LLMs still generate inaccurate or biased information that does not align with human preferences [25; 20; 22; 45; 33; 21; 7; 10; 29].

Previous work suggests that LLMs "know" more than what they "say" [21; 4]; their internal representations encode rich state information . In fact, examples of positive and negative LLM generations tend to define partly separate clusters within the activation space, as shown in Figure 1. Furthermore, Li et al.  trained a linear probe on the output representations from a subset of attention heads and achieved \(65.1\%\) accuracy in predicting whether an LLM is hallucinating or not. Inspired by these observations, we aim to steer LLMs' behaviour (e.g., to generate more truthful or less biased content) by editing their internal activations.

The idea of editing LLM activations by training a _target module_--e.g., a vector of shifts [21; 5] or an entire expert model

Figure 1: t-SNE plot of LLaMA-2-chat-7B’s activations for positive (blue) and negative (red) demonstrations from HaluEval and BBQ.

[22; 45]--then transforming the LLM activations during inference, has recently emerged as a prominent editing technique.

However, most of these methods require an expensive iterative optimisation to find such a _target module_. In contrast, we propose a novel _training-free_ method, spectral editing of activations (SEA), which edits activations by keeping them highly correlated with activations associated with positive behaviour (e.g., truthful) and decorrelated with negative behaviour (e.g., hallucinated). Our editing projections can be found with a closed-form spectral decomposition.

In practice, we first keep track of the LLM activations during inference for several demonstrations. For a given prompt, we extract the _neutral_ activations for a completion generated by the LLM. We also collect negative and positive activations for pairs of completions labelled as negative and positive, respectively. We then apply singular value decomposition (SVD) on the covariance matrices between the neutral and negative activations and between the neutral and positive activations. We then find the editing projections that prune highly co-varying directions between the neutral and negative ones while saving the highly co-varying directions between the neutral and positive ones in the latent projection space. However, SVD only allows for _linear_ editing. To overcome this limitation, we show that using an invertible non-linear feature function can perform the editing in a non-linear feature space and then transform the edited activations back to the original activation space. Finally, when the LLM is prompted with a new user query at inference time, we use these editing projections to find a representative of the model's activations that co-varies the _least_ with the negative demonstrations and the _most_ with the positive demonstrations, essentially removing the negative information from the LLM activations while retaining the positive information.

We conduct our experiments by evaluating two desirable properties of LLMs: truthfulness  and fairness . We observe SEA's advantages in improving these desirable properties while maintaining high inference efficiency. For example, applying linear SEA on the 7B LLaMA-2-chat model improves the MC1 score on TruthfulQA from 36.96 to 39.41 while only slightly increasing the inference time (\(+3.67\%\)). Moreover, non-linear SEA enhances the accuracy of the 7B LLaMA-2-chat model from 43.02 to 56.17 on the BBQ dataset. More broadly, we evaluate SEA in combination with six distinct LLMs of different sizes and architectures, and observe consistent improvements for both linear and non-linear SEA. Only 25 demonstrations are sufficient to yield a noticeable improvement in the model's truthfulness and fairness, which demonstrates SEA's data efficiency. We also show that editing LLMs' activations using SEA does not degrade other model capabilities such as commonsense or mathematical reasoning.

## 2 Method: Spectral Editing of Activations

We turn now to introducing our method for editing LLM activations. We first illustrate the framework of spectral decomposition to edit the model activations (SS2.1). We then detail the preparation (SS2.2) of the model activations for positive and negative demonstrations as well as neutral activations for calculating the editing projections (SS2.3). We finally apply the editing matrices to new LLM queries (SS2.4). An overview of SEA is given in Figure 2.

### Spectral Decomposition for Editing Activations

**Background and Notation.** For an integer \(N\), \([N]\) is the set \(\{1,,N\}\). For a vector \(\), we denote by \(\|\|_{2}\) its \(^{2}\)-norm. Matrices and vectors are in boldface font (with uppercase or lowercase letters, respectively). Random variables are denoted by uppercase boldface letters. Given a matrix \(\), we denote its \(j\)th column by \(_{j}\) (or by \(_{i:j}\) the matrix with columns \(_{q}\) for \(q=i,,j\)). All vectors

Figure 2: An overview of Spectral Editing of Activations (SEA). The method consists of two stages: (Left) the offline calculation of the editing projections using spectral decomposition with positive, negative and neutral demonstrations. (Right) the application of the calculated editing projections during LLM inference, thus manipulating predictions.

are assumed to be column vectors unless specified. We define a _demonstration_ as a (textual) prompt with a completion. There are three types of demonstrations: negative (a prompt with an undesirable completion), positive (a prompt with its desirable completion) and neutral (a prompt and a natural LLM completion). We assume three random vectors: \(^{+},^{-},^{d}\) with mean zero, where \(^{+}\) (or \(^{-}\) and \(\)) are the last-token activations for a positive (or a negative and a neutral) demonstration. Our objective is to maximise the covariance between \(^{+}\) and \(\), while minimising the covariance between \(^{-}\) and \(\). We assume \(n\) samples of \((^{+},^{-},)\), denoted by \((^{+(i)},^{-(i)},^{(i)})\) for \(i[n]\).

**Editing Framework.** Let \(^{A}\) and \(^{B}\) be two random vectors. The matrix of cross-covariance between \(^{A}\) and \(^{B}\) is \(=[^{A}(^{B})^{}]\) where \(_{ij}=Cov(^{A}_{i},^{B}_{j})\) for \(i,j[d]\).

The high-level intuition behind spectral decomposition for editing activation is to use the cross-covariance between two random activation vectors to search principal directions which maximise their covariance and project these two variables to those directions. Formally, we identify \(^{d d}\) and \(^{d d}\), and the objective of finding these two matrices (by columns) is formulated as:

\[(^{}_{i}^{A},^{}_{i}^{B})=_{(,)_{i}}(^ {}^{A},^{}^{B}),\] (1)

where \(_{i}\) is the set of all valid pairs of \((,)\) such that \(\|\|_{2}=\|\|_{2}=1\), while \(,\) are orthogonal to other column vectors in \(,\), respectively. We can use SVD on \(=^{}\) to solve this maximisation problem and find the needed matrices \(\) and \(\), where \(_{i},_{i}\) are the vectors projecting each feature of \(^{A},^{B}\) into the joint space such that they maximally covary, and the squared singular value along the diagonal matrix \(\) (denote the singular values \(_{i}=_{ii}\)), can be interpreted as the "importance" of \(_{i}\) and \(_{i}\). Now, we can use the largest or smallest left singular vectors of \(\) as \(}\) to project \(^{A}\), thus using the cross-covariance to find a representation of \(}^{A}\) that co-varies the _most_ or _least_ with \(^{B}\). Additionally, since \(\) is orthogonal, we can simply use the transpose of \(}\) to project the representation of \(^{A}\) in that joint space back to the original space, \(}^{A}=}}^{ }^{A}\), which essentially keeps the maximal or minimal covariance of \(^{A}\) with \(^{B}\) while minimising the editing, \([\|^{A}-}^{A}\|_{2}]\), to preserve model performance after editing its activations.

### Preparing the LLM Activations

To perform the spectral editing of activations, we need to define \((^{+},^{-},)\) where \(^{+},^{-}\) are the activations encoding the model's positive and negative behaviours, and \(\) denote the model's "default" activations. Then we can use the described method to edit \(\) to co-vary with \(^{+}\) the most while co-varying with \(^{-}\) the least. To maintain the training-free advantage of our method, we produce \((^{+},\,}^{-},)\) by feeding positive and negative demonstrations and prompts to LLMs and track its internal activations. However, it is also possible to separately train a pair of expert and anti-expert models to produce such activations [22; 45; 28].

Formally, assuming we have \(n\) positive and negative paired demonstrations \(\{(x_{1},y_{1}^{+}),,(x_{n},y_{n}^{+})\}\) and \(\{(x_{1},y_{1}^{-})...,(x_{n},y_{n}^{-})\}\), we first send each demonstration separately to the LLM to obtain the activations at the last token position for \(y\), capturing the latent states from the final part of each demonstration. Following , we target the output of each MLP layer of the Transformer block as the latent activations to edit. These captured activations from \(^{+}\) and \(^{-}\) are then considered as attributes summarising LLM's positive and negative behaviours. We then compute the neutral activations, \(\), by simply forwarding the prompt of demonstration \(x\) to the LLM, and again obtaining the last-token activations from the LLM output.

### Finding the Editing Projections

As depicted in Figure 2(a), once we have \((^{+},\,^{-},)\), we are ready to calculate the projections to edit the activations of the LLM. We first estimate their empirical cross-covariance through:

\[^{+}=_{i=1}^{n}^{(i)}(^{+(i)}) ^{},^{-}=_{i=1}^{n}^{(i)}( ^{-(i)})^{}.\] (2)The matrices \(^{+},^{-}^{d d}\) represent the cross-covariance for \((,\,^{+})\) and \((,\,^{-})\), respectively. The number of demonstrations is \(n\). We then perform SVD on \(^{+},^{-}\) to obtain the decompositions \((^{-},^{-},^{-}),(^{+},^{ +},^{+})\), respectively.

As our target is to edit \(\), we then sort the left singular values and keep the largest singular-valued vectors, as \(^{+}}=^{+}_{(1:k^{+})}\), to preserve the maximal covariance between \(\) and \(^{+}\). Similarly, we preserve the smallest left singular-valued vectors of \(^{-}\), as \(^{-}}=^{-}_{(k^{-}:d)}\), to remove the maximal covariance between \(\) and \(^{-}\). To decide the thresholds of selections, \(k^{+}\) and \(k^{-}\), we select the smallest integer \(k\) such that the sum of the normalised squared singular value, \(_{k}^{2}\), to be larger than a predefined hyperparameter \(K\), i.e., \(k=_{k}\{k[d]\,|\,_{j=1}^{k}^{2}}{_{ i=1}^{d}_{j}^{2}} K\}\), which can be interpreted as we keep the top-\(K\%\) and bottom-\(K\%\) of the explained variance ratio for \(^{+}\) and \(^{-}\), separately. Finally, we can use the editing matrices, \(^{+}}^{+}}^{}\) and \(^{-}}^{-}}^{}\) to edit \(\) and project it back into the original space.

### Editing Activations during Inference

We demonstrate the editing during inference in Figure 2(b). During this phase, we apply the paired editing matrices, \(^{+}}^{+}}^{}\) and \(^{-}}^{-}}^{}\), in parallel to the outputs of the MLP in each of the last \(L\) Transformer layer for every token position. Let \(T\) be the number of such tokens in a prompt and its completion, and let \(L\) be the number of layers such that \(_{}^{(t)}\) is the vector of activations for \(t[T]\) and \([L]\). Then, we define: \(}_{}^{(t+)}=^{+}}^{+}}^{}_{}^{(t)},}_{ }^{(t-)}=^{-}}^{-}}^{} _{}^{(t)}\). (3)

The vectors \(}_{}^{(t+)},}_{}^{(t-)}\) are the activations after editing negatively and positively in the \(\)-th layer. These two vectors are merged together to get the final edited activation vectors as follows, where \(i\) ranges over the coordinates of the vectors:

\[_{,i}^{(t)}=(_{,i}^{(t+)}+_{, i}^{(t-)})^{T}(z_{,i}^{(t)})^{2}}}{^{T}( z_{,i}^{(t+)}+z_{,i}^{(t-)})^{2}}}.\] (4)

### Non-linear Editing in Richer Space

Up to now, our SEA algorithm has been constrained to be linear, using SVD to maximise or minimise covariance. As depicted in Figure 1, the model activations exhibit linear separability on particular behaviours such as generating hallucinations (see the top panel of Figure 1); however, some model behaviours, e.g., producing biased responses, may not exhibit linear separability within the model's activation space (see the bottom panel of Figure 1).

To generalise SEA to a non-linear scenario, we introduce an invertible non-linear feature function, \(\), to first map the activations into \(\)'s non-linear space, where \(=()\). Once we apply the edits with the corresponding cross-covariance matrices of the \(\)-transformed activations (rather than the activations themselves), we apply the inverse of the non-linear function, \(^{-1}\), to transform the edited activations back to the original space.2

In practice, we can apply SEA on \(\) rather than \(\) to obtain the editing matrices, \(_{}^{+}}_{}^{+}}^{}\) and \(_{}^{-}}_{}^{-}}^{}\). Again, we first calculate the covariance \(_{}^{+},_{}^{-}\) for \((,^{+})\) and \((,^{-})\) following Eq. 2. Afterwards, we find \((_{}^{+},_{}^{+},_{}^{+})\) and \((^{-},_{}^{-},_{}^{-})\) using SVD. Finally, we can obtain the editing projections, \(_{}^{+}}_{}^{+}}^{}\) and \(_{}^{-}}_{}^{-}}^{}\). During inference, once we have edited the activations, \(}_{,}^{-}\) and \(}_{,}^{+}\), we apply the inverse of \(\) to transform the edited activations to the original activation space. Below, we experiment with three various non-linear feature functions,

\[()=(-^{2}}{2^{2}})& \\ )-(-)}{()+(-)}& \\ ()=,& 0\\ (()-1),&<0&,\] (5)

where \(\) is the hyperparameter for each feature function, respectively. We slightly modify their inverses to be a "pseudo" inverse, \(^{-1}()\), thus avoiding the numerical problem as follows,3

\[^{-1}()=-2^{2}(\{ ,\})&\\ (,-1+\},1- \}}{1-\{\{,-1+\},1-\}} )&\\ }^{-1}()=,&x  0\\ (,-1+\}}{}+1),&x<0 &,\] (6)

where \(\) is a very small threshold we use to project the inputs of the inverse function to the nearest point in the valid range. We refer to our nonlinear editing method as \(\)-SEA.

## 3 Experiments

We apply SEA to explore two critical attributes that make large language models useful: 1) truthfulness; and 2) unbiasedness. Truthfulness and bias evaluation are well-suited to activation editing techniques because they are editable phenomena that can be partially adjusted without re-training [35; 21]. In addition, these two attributes, unlike other attributes such as style or fluency, allow us to obtain the polarised positive and negative demonstrations required by SEA.

### Truthfulness

**Datasets.** We evaluate all compared methods on TruthfulQA , which consists of 817 questions in 38 subcategories, each paired with a single _best answer_, and multiple _correct/incorrect answers_. TruthfulQA contains two evaluation protocols: multiple-choice question answering, and generation. We mainly use the first to ensure the comparability with previous work [21; 22; 7]. We show an example of a TruthfulQA data instance together with the demonstrations we used in Appendix M.

Since TruthfulQA only provides questions for testing (and not training) purposes, we do not calculate the editing projections based on it. Instead, we use the instances from HaluEval  to calculate editing projections as Zhang et al. , then evaluate SEA on TruthfulQA. Each example of HaluEval contains a user query paired with factual and hallucinated LLM responses annotated by human evaluators. We randomly sample the questions from HaluEval and concatenate their factual and hallucinated responses as the positive and negative demonstrations applied to SEA.

**Evaluation Metrics.** Following , we conduct the evaluation on both the multiple-choice question answering and generation track. In the first track, we use MC-1/2 from TruthfulQA: MC1 assesses whether the model allocates the highest predicted likelihood to the _best answer_, while MC2 evaluates whether the normalised likelihood of all correct answers surpasses the incorrect ones. In the generation track, we follow Li et al.  to train two separate evaluators, GPT-Truth and GPT-Info. Each of evaluators predicts a score from \(0\) to \(1\) indicating the truthfulness and informativeness of a given response, respectively. Additionally, we report the Info*Truth metric which assesses a response if it is both informative and truthful. We also report the inference and training time for LoRA and SEA for the efficiency comparison between the gradient-based method and ours.

**Baselines.** We compare SEA against several baselines: 1) **In-Context Learning** (ICL): ICL shows that LLMs can learn from demonstrations in the prompt. Here, we test if LLMs can learn to generate truthful responses from the positive demonstrations. 2) **LoRA Fine-tuning** (LoRA-FT; Hu et al. 15): we use the same training data for SEA to construct an Alpaca-style  instruction-tuning dataset, and then we further fine-tune the LLM on this dataset with LoRA . 3) **Inference-time Intervention** (ITI; Li et al. 21): ITI trains a shifting module to capture truthfulness and then applies it to LLM's activations during inference. 4) **DoLa** attempts to improve the model's factuality by contrasting the model's predicted logits based on various layers' activations. 5) **Contrastive Decoding** (CD; Li et al. 22) manipulates the model's predicted logits by penalising the ones similar to a smaller model. 6) **Induce-then-Contrast Decoding** (ICD; Zhang et al. 45) follows the intuition of CD, but ICD replaces the small model with an induced hallucinated model. We use a prompt-based induced hallucinated model here for a fair comparison.

### Bias

**Dataset.** We assess the model bias on the Bias Benchmark for QA (BBQ; Parrish et al. 27), which is widely adopted in LLM evaluation [18; 31; 2]. BBQ formulates bias evaluation as a question-answering task, allowing us to easily construct the paired positive and negative demonstrations. BBQ contains 29,246 questions covering 11 diverse types of common bias. We randomly sampled 5,246 questions for evaluation, and the rest were used for training and validation purposes. We use the disambiguated version for a more comprehensive evaluation, providing the model with sufficiently informative contexts. This approach enables us to assess whether the model biases influence its selection of a correct answer choice. See Appendix M for example demonstrations.

**Evaluation Metric.** We use _accuracy_ as the main metric in bias evaluation : the model gets credit for assigning the highest predicted likelihood to the only correct answer. To understand the model's behaviour in a more fine-grained way, we rely on the _unknown-answer rate_ to measure the model's usefulness: there is always a correct answer that the model should predict in the non-ambiguous version of BBQ. Hence, the model should never predict the "unknown" candidate as its prediction. We further use two bias-related metrics: 1) _bias score_ measures the frequency of the model predicting a biased answer when it makes a non-unknown prediction. 2) _Stereotypical response rate_ measures the percentage of the model's stereotypical predictions on questions whose gold answer is anti-stereotypical.

**Baselines.** While truthfulness assessment methods have been extensively studied, strategies to alleviate model bias during inference have remained relatively overlooked. Thus, our primary comparison involves SEA, ICL, and the LoRA-FT baseline. We explore the utility of both linear and nonlinear SEA in mitigating bias.

  
**Method** & **MC1** & **MC2** & **Info** & **Truth** & **Info*Truth** & \(}\) & \(}\) \\  ICL (LLaMA-2) & 28.39 & 43.42 & - & - & - & - & - \\ ICL (LLaMA-2-Chat) & 36.96 & 54.68 & 69.40 & 47.36 & 33.29 & 4.90 & - \\  \(w\) Best-of-1 & - & - & - & 69.40 & 47.36 & 33.29 & - & - \\ \(w\) Best-of-2 & - & - & - & 76.50 & 57.03 & 44.55 & - & - \\ \(w\) Best-of-3 & - & - & - & 80.54 & 62.30 & 50.31 & - & - \\ LoRA-FT (LLaMA-2-Chat; \(N=1000\)) & 35.74 & 54.61 & 91.06 & 48.59 & **42.59** & 5.16 & 299 \\ LoRA-FT (LLaMA-2-Chat; \(N=2000\)) & 35.01 & 54.24 & **92.41** & 47.49 & 42.35 & 5.04 & 1190 \\  ITI & 37.01 & 54.66 & - & - & - & - & 5.82 & - \\ DoLA & 32.97 & **60.84** & - & - & - & - & 5.60 & - \\ CD (13B-Chat vs. 7B-Chat) & 28.15 & 54.87 & - & - & - & - & - \\ ICD (Prompt-version) & 37.87 & 57.77 & - & - & - & 9.67 & - \\  SEA (\(N=1000,K=99\%,L=4\)) & 38.31\({}^{}\) & 55.27\({}^{}\) & 70.38 & 48.96 & 35.25 & **5.08** & 140 \\ SEA (\(N=2000,K=99.8\%,L=21\)) & **39.41\({}^{}\)** & 57.15\({}^{}\) & 68.05 & **50.67** & 33.66 & 5.93 & **152** \\  \(w\) Best-of-1 & - & - & - & 68.05 & 50.67 & 33.66 & - & - \\ \(w\) Best-of-2 & - & - & - & 77.72 & 57.28 & 44.56 & - & - \\ \(w\) Best-of-3 & - & - & 82.01 & 63.04 & 51.30 & - & - \\   

Table 1: TruthfulQA results. All models are built on LLaMA-2-Chat-7B. \(}\) and \(}\) are the overall training and average inference time (seconds) per sample. \({}\): SEA significantly increases MC1/2 on ICL by pair-wise t-test with \(p<0.05\). Part of results are from . For ICL and SEA, we also report the performance in the Best-of-\(N\) distribution .

## 4 Results and Discussions

### Truthfulness Evaluation

**Main results.** Table 1 illustrates the results of various inference-only techniques aimed at boosting the performance of a 7B LLaMA-2 model on TruthfulQA. We observe a significant enhancement of the base LLaMA-2 model when further trained with the conversation-style alignment (LLaMA-2-Chat), as described in . On the other hand, we do not observe an improvement with LoRA, which indicates the difficulty in improving a well-trained model with LoRA-style fine-tuning.

In the multiple-choice track, the best result is from our proposed SEA method, which outperforms ICL by 2.45 and 2.47 points for MC1 and MC2, respectively. In the generation track, SEA has the better truthfulness compared with ICL and LoRA. The positive improvement is also observed in the Best-of-N distribution. When compared to alternative approaches, SEA achieves the highest MC1 score while incurring the _minimal_ sacrifice in inference speed when we only modify the last four layers. SEA has only a 3.67% increase in inference speed, contrasting with the larger increases of 18.78%, 14.29%, and 97.34% for ITI, DoLA, and ICD, respectively. We also highlight the training efficiency, i.e., computing editing projections, of SEA compared to gradient-based optimisation methods (e.g., LoRA), an advantage that becomes more significant with an increasing number of demonstrations.

**Ablation study.** We then conduct an ablation study on TruthfulQA to show the positive contribution of each individual design of SEA. Our first group of analysis is to only use the positive or negative editing projection (see Positive Editing Only and Negative Editing Only in Table 2), rather than combining both edited activations. However, we observe a significant drop in both experiments. This observation suggests that the activations edited with positive and negative projections may complement each other, compensating for any information lost during the editing process. This is because, in the positive projection, we retain the top \(K\%\) of covariance information, whereas in the negative projection, we retain the top \((1-K)\%\) of covariance information as we discussed in SS2.3.

Our second group of analysis is to confirm the advantage of using feature normalisation. We alter the combination of positively and negatively edited activations by simply averaging each neuron's activation between the two. We note a decrease in MC1 from 39.41 to 35.86, affirming the impact of our proposed normalisation technique. One potential explanation for this decline in performance is that basic averaging might disrupt the correlation between activations, thereby hindering subsequent layers of the model from processing the edited activations normally.

Finally, to ascertain whether SEA's positive and negative editing projections effectively capture information relevant to controlling the model's factual or hallucinated responses, we reverse the

    & **MC1** & **MC2** \\  SEA & 39.41 & 57.15 \\  Positive Editing only & 26.56 & 53.32 \\ Negative Editing only & 34.88 & 52.61 \\ Averaging Merging & 35.86 & 54.01 \\ Top-3 Layers Editing & 38.43 & 55.77 \\ Bottom-3 Layers Editing & 36.23 & 54.56 \\ Reverse Editing & 35.13 & 53.38 \\   

Table 2: Ablation study on 7B LLaMA-2-Chat model’s performance on TruthfulQA.

Figure 3: Left: Accuracy of all methods on each group of bias type in BBQ. Right: results on BBQ’s testing set. All methods are applied on LLaMA-2-Chat-7B. For accuracy (**A%**\(\)), higher values are better; for unknown-answer response rate (**U%\(\)**), bias score (**BS%\(\)**) and stereotypical response rate (**SR%\(\)**), lower is better. We use **bold** font for the best result in each column, and mark the methods that improve ICL. \(\): significant improvements on ICL in **A%** by pair-wise t-test with \(p<0.05\).

editing projections and assess if this reversal leads to a performance decline. In detail, we apply the editing projections aimed at preserving maximal covariance between the neutral and negative activations, denoted as \((,^{-})\), while minimising covariance between the neutral and positive activations, denoted as \((,^{+})\), which essentially encourages the model to be more hallucinated while less factual. We observe a decrease in MC1 from 39.41 to 35.13, which falls below the LLaMA-2-Chat baseline at 36.96. This ablation proves that our positive and negative projections indeed capture information regarding the model behaviour from positive and negative demonstrations.

### Bias Evaluation

**Main Results.** We present the results on BBQ in Figure 3. Similarly to the truthfulness evaluation, LoRA only marginally improves the accuracy on BBQ. The accuracy enhancement achieved by linear SEA in BBQ is modest, exhibiting a mere increase of 0.78%. However, this observation may stem from the inferior separability between positive and negative demonstrations in BBQ within the activation space, as emerges from Figure 1. Furthermore, we observe significant accuracy improvements with \(\)-SEA incorporating three nonlinear feature functions. The best \(\)-SEA with the squared-exponential feature function resulted in accuracy enhancements of 13.15%. The drops in unknown-answer rate (27% \(\) 8.1%) and bias metrics (bias score: 5.8% \(\) 2%; stereotypical response rate: 54% \(\) 42.6%) further prove that the boosted accuracy comes from the improvement on usefulness and fairness. Detailed accuracy scores across each bias type in BBQ reveal that SEA yields benefits across all genres of bias, while LoRA results in a decline in Physical_appearance, Race_ethnicity, Race_x_SES, and Race_x_gender.

### Generalisation across Various LLMs

We show the results on TruthfulQA and BBQ of applying ICL, Linear SEA and \(\)-SEA on six LLMs with different model families and sizes in Table 3. Specifically, we test them on the LLaMA-2 family , Gemma family  and Mistral 7B , which represent the state-of-the-art open-source LLMs. Linear SEA shows generalisable improvements across all tested LLMs on TruthfulQA. On BBQ, we observe a general trend across different LLMs that linear SEA can marginally improve accuracy, but the other two metrics are mixed. \(\)-SEA shows promising performance and can gain increased accuracy, lower unknown-answer rate and stereotypical response rate across all LLMs, except a negligible higher stereotypical response rate for Gemma-it-2B.

### Scaling the Number of Demonstrations

In Figure 4, we plot the 7B LLaMA-2-Chat model performance by varying the number of used demonstrations in calculating the editing projections. Our first observation is that SEA can start to increase MC1 with only 25 demonstrations. With even fewer demonstrations (e.g., 25), the model can positively improve the accuracy of BBQ. Both results demonstrate the data efficiency of SEA. Secondly, we show that SEA generally benefits from more demonstrations just like ICL ; however, unlike ICL, the offline calculations of SEA are not limited by the context length supported by an LLM. This advantage provides a new strategy for using demonstrations to better guide LLMs' generation.

    &  **LLAMA-2/7B** \\  } &  &  \\  & & ICL & Linear-SEA & \(\)-SEA & ICL & Linear-SEA & \(\)-SEA & ICL & Linear-SEA & \(\)-SEA \\   & **Acc.\%** & 43.0 & 43.8 (\(\)0.8) & 56.2 (\(\)13.2) & 47.1 & 47.3 (\(\)0.2) & 54.6 (\(\)7.5) & 45.8 & 45.9 (\(\)0.1) & 58.0 (\(\)12.2) \\  & **Clnk.\%** & 19.6 & 18.5 (\(\)11.1) & 8.0 (\(\)11.5) & 18.1 & 17.9 (\(\)0.2) & 12.3 (\(\)1.6) & 17.5 & 17.7 (\(\)0.2) & 13.4 (\(\)1.0) \\  & **88.\%** & 52.0 & 51.9 (\(\)0.1) & 42.6 (\(\)0.0) & 47.6 & 47.1 (\(\)0.5) & 44.6 (\(\)3.0) & 49.1 (\(\)0.9) & 49.1 (\(\)0.0) & 39.6 (\(\)9.0) \\   & **MC1** & 36.9 & 39.4 (\(\)2.5) & & / & 37.7 & 38.0 (\(\)0.3) & / & 37.7 & 37.8 (\(\)0.1) & / \\  & **MC2** & 54.6 & 57.1 (\(\)2.5) & & / & 55.7 & 55.6 (\(\)0.1) & / & 59.0 & 58.9 (\(\)0.1) & / \\   &  **German-14-B** \\  } &  &  &  \\  & & ICL & Linear-SEA & \(\)-SEA & ICL & Linear-SEA & \(\)-SEA & ICL & Linear-SEA & \(\)-SEA \\   & **Acc.\%** & 41.8 & 41.9 (\(\)0.1) & 44.5 (\(\)2.2) & 44.4 & 44.2 (\(\)0.2) & 48.1 (\(\)1.3) & 94.6 & 94.8 (\(\)0.2) & 95.7 (\(\)1.1) \\  & **Clnk.\%** & 20.4 & 19.3 (\(\)1.1) & 15.7 (\(\)4.7) & 30.1 & 31.2 (\(\)1.1) & 30.3 (\(\)0.2) & 0.80 & 0.70 (\(\)0.1) & 0.50 (\(\)0.3) \\  & **SR.\%** & 52.5 & 52.7 (\(\)0.2) & 52.6 (\(\)0.1) & 43.6 (\(\)0.1) & 40.0 (\(\)0.4) & 40.6 (\(\)3.4) & 4.20 & 43.0 (\(\)0.1) & 3.90 (\(\)0.3) \\   & **MC1** & 30.4 & 30.7 (\(\)0.3) & & / & 34.3 & 35.1 (\(\)0.8) & / & 55.8 & 56.4 (\(\)0.6) & / \\  & **MC2** & 48.2 & 48.2 (\(\)0.0) & & / & 52.9 & 53.6 (\(\)0.7) & / & 72.1 & 72.8 (\(\)0.7) & / \\   

Table 3: BBQ performance (in terms of accuracy, **Acc.\%\({}_{}\)**, unknown-answer rate, **Unk.\%\({}_{}\)** and stereotypical response rate, **SR%\({}_{}\)**) and TruthfulQA performance (in terms of MC1+/2+) after applying ICL, Linear SEA and non-linear SEA (\(\)-SEA) on six open-source LLMs. We highlight the improved and worsened metrics, respectively.

### Post-Editing Performance on Control Tasks

We then leverage six additional control tasks to analyse the editing effects of SEA on other LLM capabilities in Table 4. These tasks include MMLU , which serves as the most general benchmark. We then use HellaSwag  and Natural Questions  to assess the model's commonsense reasoning and question answering. We use GSM8K  and MathQA  to verify the model's ability to solve mathematical tasks. We rely on ToxiGen  for assessing the model's toxicity.

**Editing a model's activations has only a limited impact on other capabilities.** We first note that linear editing almost does not hurt the model's other capabilities (e.g., in commonsense and maths). The non-linear editing causes a small drop in performance in maths tasks, but we observe a more visible decrease in common-sense QA. We attribute this to the limitation mentioned in SS2.5, namely that the non-linear projection using feature functions is not theoretically lossless. Qualitative examples in Appendix G also show the high quality and fluency in outputs of SEA-edited models.

### Generalisation of Editing Effects to Similar Tasks

**SEA's editing effect can be generalised to other similar tasks.** Finally, we observe that the post-editing improvements can be generalised across new tasks, provided they share some similarities. For example, we observe a strong improvement of the 7B LLaMA-2-Chat model in ToxiGen in Table 4, after projecting the activations towards the less biased directions.

We further assess all SEA methods using BBQ's demonstrations on the CrowS-Pairs dataset , which evaluates a model's propensity to generate biased outputs (Table 5). We report the percentage of stereotypical sentences rated as more likely than non-stereotypical ones, where a lower percentage indicates less bias. Results demonstrate that both SEA variants effectively reduce model bias across most categories, with \(\)-SEA notably decreasing the generation of stereotypical sentences by 7%.

    & **Hella** & \(\) & **NO** & \(\) & **GSM8K** & \(\) & **MathQA** & \(\) & **MMLU** & \(\) & **TosiGen** & \(\) \\ 
**LLaMA-2-Chat-7B** & 57.78 & & & 22.83 & & 22.44 & & 31.69 & & 46.45 & & 51.17 & \\  w/ SEA-Truthful & 57.08 & \(\) & 0.72 & 21.6 & \(\) & 0.66 & 22.67 & \(\) & 0.23 & 31.36 & \(\) & 0.34 & 46.75 & \(\) & 0.30 & 49.60 & \(\) & 1.57 \\ w/ Linear-SEA-Fair & 57.78 & \(\) & 0.70 & 22.74 & 0.08 & 21.91 & \(\) & 0.53 & 31.62 & \(\) & 0.07 & 46.35 & \(\) & 0.10 & 52.55 & \(\) & 1.38 \\ w/ \(\)-SEA-Fair & 51.93 & 1.55 & 14.90 & 1.79 & 20.92 & 1.52 & 30.05 & 1.64 & 45.30 & \(\) & 1.15 & 56.38 & \(\) & 5.21 \\   

Table 4: Performance of LLaMA-2-Chat-7B and its three SEA edited models for truthfulness and fairness on six control tasks covering multi-task ability, commonsense question answering, and mathematical ability. Details of evaluation are provided in Appendix H.4.

Figure 4: MC1 scores of SEA by using a different number of demonstrations. A higher score indicates a better performance. We find that SEA can start to positively improve the baseline with only 25 demonstrations on both TruthfulQA and BBQ for the 7B LLaMA-2-Chat model.

### Spectral Analysis for the Source of Model Behaviours

We now present an analysis to substantiate why editing the top-\(L\) layers proves to be more precise compared to other editing schemes, such as the bottom-\(L\) layers. We follow [11; 46; 47] which proposed interpreting the _signature_ as the sum of singular values resulting from an SVD of the cross-covariance for two variables, indicating the degree of correlation between them. We extend this to identify which layers' activations contain the most information regarding the model's behaviour. The calculation of the signature value is in Appendix A.

In Figure 5, we depict the layer-wise normalised signatures of various LLMs calculated on HaluEval. Most LLMs exhibit a trend where the top layers contain the truthfulness information, aligning with recent findings suggesting that bottom layers capture fundamental linguistic features, while top layers contribute to high-level tasks . However, Gemma stands out, as both bottom and top layers hold significant truthfulness-related information. This suggests that LLMs, possibly due to variations in data mixtures, may distribute truthfulness information across layers differently.

To the same end, we also conduct an ablation, reported in Table 2. Comparing settings exclusively editing the top three layers with those editing the bottom three layers, we find that the first yields superior performance. We also find an advantage in editing more top layers, which is in line with the exponential trend we observe for LLaMA-2-Chat-7B in Figure 5.

## 5 Related Work

Modifying the activations of a trained model, thus altering the model's behaviour [30; 16; 24; 49; 6; 44] or internal knowledge , represents a lightweight method to control the model's generation. Li et al.  probes LLM's attention heads which are accountable for hallucinations, then edits activations toward truthful directions. Another way is extracting latent vectors directly from the trained model and leveraging these vectors to regulate the model's inference [41; 37; 32; 49]. Recently, Singh et al.  demonstrated the efficacy of fitting an optimal transport from negative to positive activations to facilitate effective non-linear editing. Activation editing finds application in decoding as well, either by contrasting activations from various layers  or by using a weaker model to edit activations from a stronger model [22; 45]. Distinct with previous works that use probing , contrasting activations [6; 49; 45; 22], or optimal transfer , we use the covariance information to find the editing directions for LLM's activations.

## 6 Conclusion

We present SEA, a new training-free activation editing method. This is aimed at guiding LLMs to generate desirable outputs through spectral decomposition. Our findings indicate that linear SEA yields improvements in truthfulness and bias over several baselines while imposing minimal additional computation overheads. We also extend SEA to incorporate non-linear capabilities through feature functions and their pseudo-inverse. An intriguing property of our method is that it can leverage an increased number of demonstrations without being constrained by context length. Finally, we establish that our approach generalises across LLMs of varying model sizes and families, without incurring degradation of other LLM capabilities.

Figure 5: Visualisation for the signature values in all LLM layers on HaluEval.