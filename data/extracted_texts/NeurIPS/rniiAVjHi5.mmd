# Universality of AdaGrad Stepsizes for Stochastic Optimization: Inexact Oracle, Acceleration and Variance Reduction

Universality of AdaGrad Stepsizes for Stochastic Optimization: Inexact Oracle, Acceleration and Variance Reduction

 Anton Rodomanov

CISPA

anton.rodomanov@cispa.de

&Xiaowen Jiang

Saarland University and CISPA

xiaowen.jiang@cispa.de

&Sebastian Stich

CISPA

stich@cispa.de

CISPA Helmholtz Center for Information Security, Saarbrucken, Germany

###### Abstract

We present adaptive gradient methods (both basic and accelerated) for solving convex composite optimization problems in which the main part is approximately smooth (a.k.a. \((,L)\)-smooth) and can be accessed only via a (potentially biased) stochastic gradient oracle. This setting covers many interesting examples including Holder smooth problems and various inexact computations of the stochastic gradient. Our methods use AdaGrad stepsizes and are adaptive in the sense that they do not require knowing any problem-dependent constants except an estimate of the diameter of the feasible set but nevertheless achieve the best possible convergence rates as if they knew the corresponding constants. We demonstrate that AdaGrad stepsizes work in a variety of situations by proving, in a unified manner, three types of new results. First, we establish efficiency guarantees for our methods in the classical setting where the oracle's variance is uniformly bounded. We then show that, under more refined assumptions on the variance, the same methods without any modifications enjoy implicit variance reduction properties allowing us to express their complexity estimates in terms of the variance only at the minimizer. Finally, we show how to incorporate explicit SVRG-type variance reduction into our methods and obtain even faster algorithms. In all three cases, we present both basic and accelerated algorithms achieving state-of-the-art complexity bounds. As a direct corollary of our results, we obtain universal stochastic gradient methods for Holder smooth problems which can be used in all situations.

## 1 Introduction

Motivation.Gradient methods are among the most popular and efficient optimization algorithms for solving machine learning problems. To achieve the best convergence speed for these algorithms, their stepsizes needs to be chosen properly. While there exist various theoretical recommendations, dictated by the convergence analysis, on how to select stepsizes based on various problem-dependent parameters, they are usually impractical because the corresponding constants may be unknown or their worst-case estimates might be too pessimistic. Furthermore, every applied problem usually belongs to multiple problem classes at the same time, and it is not always evident in advance which of them better suits the concrete problem instance one works with. For classical optimization algorithms, this problem is typically resolved by using a line search. This is a simple yet powerful mechanism which automatically chooses the best stepsize by checking at each iteration a certain condition involving the objective value, its gradient, etc.

However, the line-search approach is usually unsuitable for problems of stochastic optimization, where gradients are observed with random noise (unless some extra assumptions are made, see ). For these problems, it is common instead to apply so-called adaptive methods which set up their stepsizes by simply accumulating on-the-fly certain information about observed stochastic gradients. The first such an algorithm, AdaGrad [17; 39], was obtained from theoretical considerations but quickly inspired several other heuristic methods like RMSProp  and Adam  that are now at the forefront of training machine learning models.

Excellent practical performance of adaptive methods on various applied problems naturally sparked a lot of theoretical interest in these algorithms. An important observation was done by Levy, Yurtsever, and Cevher  who showed that AdaGrad possesses a certain universality property, in the sense that it works for several problem classes simultaneously. Specifically, they showed that AdaGrad converges both for nonsmooth problems with bounded gradient and also for smooth problems with Lipschitz gradient, without needing to know neither the corresponding Lipschitz constants, nor the oracle's variance but enjoying the rates which are characteristic for algorithms which have the knowledge of these constants. They also presented an accelerated version of AdaGrad with similar properties. An independent version of the accelerated AdaGrad including diagonal scaling was proposed by Deng, Cheng, and Lan . Further improvements and generalization of these ideas were considered in [18; 28; 30].

Nonsmooth and smooth problems are the extremes of the more general Holder class of problems. The fact that AdaGrad methods simultaneously work for these two extreme cases does not seem to be a coincidence and suggests that these algorithms should work more generally for any problem with intermediate level of smoothness. Some further confirmations to this were recently provided in  although in a rather restricted setting of deterministic problems and only for the basic AdaGrad method. The stochastic case and acceleration were constituting an open problem which was recently resolved in  for a slightly modified AdaGrad stepsize (see (4)).

All the previously discussed results were proved only for the classical stochastic optimization setting where the variance of stochastic gradients is assumed to be uniformly bounded. In a recent work, Attia and Koren  showed that the basic AdaGrad method for smooth problems works under the more general assumption when the variance is bounded by a constant plus a multiple of the squared gradient norm. On a related note, it was also shown recently that AdaGrad stepsizes can be used inside gradient methods with SVRG-type variance-reduction. The first such an algorithm was proposed in . The accelerated SVRG method enjoying optimal worst-case oracle complexity for smooth finite-sum optimization problems was later presented in .

Contributions.In this work, we further extend the results mentioned above by demonstrating that AdaGrad stepsizes are even more universal than was shown previously in the literature. Specifically, we consider the composite optimization problem where the main part is approximately smooth (a.k.a. \((,L)\)-smooth) and can be accessed only via a (potentially biased) stochastic gradient oracle. This setting is more general than typically considered in the literature on adaptive methods and covers many interesting examples, including smooth, nonsmooth and, more generally, Holder smooth problems, problems in which the objective function is given itself as another optimization problem whose solution can be computed only approximately, etc.

Our contributions can be summarized as follows:

1. We start, in Section 3, with identifying the key property of AdaGrad stepsizes, which allows us to apply these stepsizes, in a unified manner, in a variety of situations we consider later. We present our two mains algorithms, \(\) and \(\) which are the classical stochastic gradient method (SGD) and its accelerated version, respectively, equipped with AdaGrad stepsizes.
2. We then establish, in Section 4, efficiency guarantees for these methods in the classical setting where the oracle's variance is assumed to be uniformly bounded.
3. In Section 5, we complement these results by showing that, under additional assumptions that the variance is itself approximately smooth w.r.t. the objective function, the same \(\) and \(\) without any modifications enjoy implicit variance reduction properties allowing us to express their complexity estimates in terms of the variance only at the minimizer.
4. Under the additional assumption that one can periodically compute the full (inexact) gradient of the objective function, we show, in Section 6, how to incorporate explicit SVRG-type variance reduction into our methods, obtaining new \(\) and \(\) algorithms which enjoy even faster convergence rates by completely eliminating the variance.

Our results are summarized in Table 1 (in the BigO-notation). In all the situations, we present both basic and accelerated algorithms whose only essential parameter is an estimate \(D\) of the diameter of the feasible set; the methods automatically adapt to all other problem-dependent constants. In a number of special cases, our algorithms achieve known state-of-the-art complexity bounds, but not restricted to those special cases. In Section 7, we illustrate the significance of our results by demonstrating that complexities for our methods on stochastic optimization problems with Holder smooth components can be obtained as simple corollaries from our main results.

## 2 Preliminaries

Notation.We work in the space \(^{d}\) equipped with the standard inner product \(,\) and a certain Euclidean norm: \(\|x\|:= Bx,x^{1/2}\), where \(B\) is a fixed positive definite matrix. The dual norm is defined in the standard way: \(\|s\|_{*}:=_{\|x\|=1} s,x= s,B^{-1}s^{1/2}\).

For a convex function \(^{d}\{+\}\), its (effective) domain is the following set: \(:=\{x^{d}:(x)<+\}\). By \((x)\), we denote the subdifferential of \(\) at a point \(x\); the specific subgradients are typically denoted by \((x)\).

A convex function \(f^{d}\) is called \((,H)\)-Holder smooth for some \(\) and \(H 0\) iff \(\| f(x)- f(y)\|_{*} H\|x-y\|^{}\) for all \(x,y^{d}\) and all \( f(x) f(x)\), \( f(y) f(y)\). Apart from the special case of \(=0\), such a function \(f\) is differentiable at every point, i.e., \( f(x)\) is a singleton. A \((1,L)\)-Holder smooth function is usually called \(L\)-smooth.

For a convex function \(^{d}\{+\}\), point \(x^{d}\), vector \(g^{d}\), and coefficient \(M 0\), by \(_{}(x,g,M)_{y }\{ g,y+(y)+\|y-x\|^{2}\}\), we denote the proximal mapping. When \(M=0\), we allow the solution to be chosen arbitrarily.

For a convex function \(f^{d}\), points \(x,y^{d}\) and \( f(x) f(x)\), we denote the Bregman distance by \(_{f}^{ f(x)}(x,y) f(y)-f(x)- f(x),y-x \;( 0)\). When the specific subgradient \( f(x)\) is clear from the context, we use the simplified notation \(_{f}(x,y)\).

The positive part of \(t\) is \([t]_{+}\{t,0\}\). For \(>0\), we also use \(_{+}\{1,\}\).

Problem Formulation.In this paper, we consider the composite optimization problem

\[F^{*}_{x}F(x) f(x)+ (x),\] (1)

where \(f^{d}\) is a convex function, and \(^{d}\{+\}\) is a proper closed convex function which is assumed to be sufficiently simple in the sense that the proximal mapping \(_{}\) can be easily computed. We assume that this problem has a solution which we denote by \(x^{*}\).

To quantify the smoothness level of the objective function, we use the following assumption:

   Method & Convergence rate & SO complexity & Assumptions & Reference \\  \) (Alg. 1)} & \(D^{2}}{k}+}+_{f}\) &  &  & Thm. 4 \\  & \(+L_{})^{2}}{k}+}+_{f}+_{}\) & & & 1, 2, 6 & Thm. 7 \\  \) (Alg. 2)} & \(D^{2}}{k^{2}}+}+k_{f}\) &  &  & Thm. 5 \\  & \(D^{2}}{k^{2}}+}{k}+}+k _{f}+_{}\) & & & 1, 2, 6 & Thm. 8 \\  \) (Alg. 3)} & \(+L_{})^{2}}{k^{2}}+_{f}+_{}\) & +n t\)} &  & Thm. 10 \\  \) (Alg. 4)} & \(+L_{})^{2}}{n(t- n)^{2}}+t(_{f}+_{})\) &  &  & Thm. 11 \\   

Table 1: Summary of main results for solving problem (1) with our methods. “Convergence rate” is expressed in terms of the expected function residual at iteration \(k\) (or \(t\), depending on the method). “SO complexity” denotes the cumulative stochastic-oracle complexity of the method since its start and up to iteration \(k\) (or \(t\)), which is defined as the number of queries to the stochastic oracle \(\); for SVRG methods, we assume that querying the (inexact) full-gradient oracle \(\) is \(n\) times more expensive than \(\), and define the SO complexity as \(N_{}+nN_{}\), where \(N_{}\) and \(N_{}\) are the number of queries to \(\) and \(\), respectively. The second and third columns should be understood in terms of the BigO-notation which we omit for brevity.

**Assumption 1**.: _The function \(f\) in problem (1) is approximately smooth: there exist constants \(L_{f},_{f} 0\) and \(^{d}\), \(^{d}^{d}\) such that, for any \(x,y^{d}\), \(_{f,}(x,y) f(y)-(x)-(x),y-x\) satisfies the following inequality: \(0_{f,,}(x,y)}{2}\|x-y\|^{2}+_{f}\)._

Assumption 1 is well-known in the literature under the name \((,L)\)_-oracle_ and was originally introduced in . It covers many interesting examples. For instance, if \(f\) is \(L\)-smooth, then Assumption 1 is satisfied with \(=f\), \(= f\), \(_{f}=0\) and \(L_{f}=L\). More generally, if the function \(f\) is \((,H_{f}())\)-Holder smooth, then Assumption 1 is satisfied with \(=f\), \(= f\) (arbitrary selection of subgradients), any \(_{f}>0\) and \(L_{f}}^{}[H_{f}()]^{}\) (see Theorem 13). If \(f\) can be uniformly approximated by an \(L\)-smooth function \(\), i.e., \((x) f(x)(x)+\), then Assumption 1 is satisfied with \(=\), \(=\) and \(_{f}=\). If \(f\) represents another auxiliary optimization problem with a strongly concave objective, e.g., \(f(x)=_{u}(x,u)\), whose solution \((x)\) can only be found with accuracy \(\), then \(f\) satisfies Assumption 1 with \((x)=(x,(x))\), \((x)=_{u}(x,(x))\) and \(_{f}=\). For more details and other interesting examples, we refer the reader to .

In what follows, we assume that we have access to an unbiased stochastic oracle \(\) for \(\). Formally, this is a pair \(=(g,)\) consisting of a random variable \(\) and a mapping \(g^{d}^{d}\) (with \(\) being the image of \(\)). When queried at a point \(x\), the oracle automatically generates an independent copy \(\) of its randomness and then returns \(_{x}=g(x,)\) (notation: \(_{x}(x)\)). We call \(g\) and \(\) the function component and the random variable component of \(\), respectively. At this point, we only assume that our stochastic oracle \(\) is un unbiased estimator of \(\), and later make various assumptions on its variance.

Another important assumption on problem (1), that we need in our analysis, is the boundedness of the feasible set \(\).

**Assumption 2**.: _There exists \(D>0\) such that \(\|x-y\| D\) for any \(x,y\)._

Assumption 2 is rather standard in the literature on adaptive methods for stochastic convex optimization (see [16; 18; 30; 34; 36; 49]) and can always be ensured with \(D=2R_{0}\) whenever one has the knowledge of an upper bound \(R_{0}\) on the distance from the initial point \(x_{0}\) to the solution \(x^{*}\). To that end, it suffices to rewrite the problem (1) in the following equivalent form: \(_{x_{D}}[f(x)+_{D}(x)]\), where \(_{D}\) is the sum of \(\) and the indicator function of the ball \(B_{0}\{x^{d}:\|x-x_{0}\| R_{0}\}\). Note that this transformation keeps the function \(_{D}\) reasonably simple as its proximal mapping can be computed via that of \(\) by solving a certain one-dimensional nonlinear equation, which can be done very efficiently by Newton's method (at no extra queries to the stochastic oracle); in some special cases, the corresponding nonlinear equation can even be solved analytically, e.g., when \(=0\), the proximal mapping of \(_{D}\) is simply the projection on \(B_{0}\).

Throughout this paper, we refer to \(D\) from Assumption 2 as the diameter of the feasible set, and assume that its value is known to us. This will be the only essential parameter in our methods.

## 3 Main Algorithms and Stepsize Update Rules

We now present our two main algorithms for solving problem (1): \(\) (Algorithm 1), and its accelerated version, \(\) (Algorithm 2). Except the specific choice of the stepsize coefficients \(M_{k}\), both algorithms are rather standard: the first one is the classical SGD method, and the second one is the classical accelerated gradient method for stochastic optimization , also known as the Method of Similar Triangles (see, e.g., Section 6.1.3 in ).

Both methods are expressed in terms of a certain abstract stepsize update rule \(M_{+}()\) defined as follows. Given the current stepsize coefficient \(M 0\), constant \(>0\) (the scaled squared diameter),current point \(x\) with the stochastic gradient \(_{x}(x)\), next iterate \(_{+}=x_{+}(_{x})\) (which is the result of the deterministic function applied to \(_{x}\)), and the corresponding stochastic gradient \(_{x_{+}}(_{+})\), the update rule computes \(_{+}=M_{+}(M,,x,_{+},_{x},_{ x_{+}})\) (deterministic function of its arguments) such that \(_{+} M\) and the following inequality holds for any \(>c_{2}L_{f}\):

\[&[(_{+})+( _{+}-M)+_{f,,}(_{+},x)]\\ &}{-c_{2}L_{f}}\,[ {Var}_{}(_{+})+_{}(x)]+c_{ 3}_{f}+c_{4}\,[\{_{+},\}-M]_{+ }},\] (2)

where \((_{+})_{f,,}(x, {x}_{+})+(x)-_{x},_{+}-x-_{+}}{_{+}}\|_{+}-x\|^{2}\), \(c_{1},c_{2},c_{3},c_{4}>0\) are some absolute constants, and \(_{}(x)_{}[\|g(x,)- (x)\|_{*}^{2}]\) is the variance of \(\). The expectations in (2) are taken w.r.t. the randomness \((,_{+})\) coming from \(_{x} g(x,)\), \(_{x_{+}} g(_{+},_{+})\).

The main example is the following AdaGrad rule:

\[_{+}=+\|_{x_{+}}- _{x}\|_{*}^{2}}}.\] (3)

For this rule, we have \(c_{1}=\), \(c_{2}=4\), \(c_{3}=6\), \(c_{4}=2\) (see Lemma 20). Another interesting example recently suggested in  is \(_{+}\) found from the equation

\[(_{+}-M)=(_{x_{+}}-_{x}, _{+}-x)-_{+}}{2}\|_{+}-x\|^{2}_{ +}.\] (4)

This equation admits a unique solution which can be easily written down in closed form (see Lemma E.1 in ). For this rule, we have \(c_{1}=1\), \(c_{2}=2\), \(c_{3}=6\), \(c_{4}=2\) (see Lemma 21).

Inequality (2) is the only property we need from the stepsize update rule to establish all forthcoming results. This inequality is exactly what is typically used inside the convergence proofs for stochastic gradient methods with predefined stepsizes \(M_{k}\) (in which case \(M=_{+}=\)), where \(\) depends on problem-dependent constants. The key property of AdaGrad stepsizes (either (3) or (4)) is that they ensure the same inequality but now \(\) is the virtual stepsize existing only in the theoretical analysis. The price for this is the extra error term \([\{_{+},\}-M]_{+}\) appearing in the right-hand side of (2). The crucial property of this error term is that it is telescopic, \(_{i=0}^{k}[\{M_{i+1},\}-M_{i}]_{+}=[\{M_{k+1},\}-M_{0}]_{+}\) (see Lemma 18) and therefore its total cumulative impact is always bounded by the controllable constant \(\). Although a number of other works on theoretical analysis of AdaGrad methods for smooth optimization use some similar ideas about the virtual stepsize (e.g., [30; 34; 36]), this is the first time one has abstracted away all the technical details and identified the specific inequality (2) responsible for the universality of AdaGrad.

## 4 Uniformly Bounded Variance

In this section, we assume that the variance of our stochastic oracle is uniformly bounded.

**Assumption 3**.: _For the stochastic oracle \(\), we have \(^{2}_{x}_{ }(x)<+\), where \(_{}(x)_{}[\|g(x,)- (x)\|_{*}^{2}]\)._Under this assumption, we can establish the following efficiency estimates for our \(\) and \(\) methods (the proofs are deferred to Appendix C).

**Theorem 4**.: _Let Algorithm 1 with \(M_{0}=0\) be applied to problem (1) under Assumptions 1-3. Then, for the point \(_{N}\) generated by the algorithm, we have_

\[[F(_{N})]-F^{*}c_{4}L_{f}D^{2}}{N}+2 D c_{4}}{N}}+c_{3}_{f}.\]

**Theorem 5**.: _Let Algorithm 2 be applied to problem (1) under Assumptions 1-3. Then, for any \(k 1\),_

\[[F(x_{k})]-F^{*}c_{4}L_{f}D^{2}}{k(k+1)}+4 D c_{4}}{3k}}+}{3}(k+2)_{f}.\]

We see that, in contrast to \(\), the accelerated algorithm \(\) is not robust to the oracle's errors: it accumulates them with time at the rate of \(O(k)\). This is not surprising since the same phenomenon also occurs in the classical accelerated gradient method, even when the oracle is deterministic and the algorithm has the knowledge about all constants (see ).

The complexity results from Theorems 4 and 5 are similar to those from . However, it is important that our methods are adaptive and do not require knowing the constants \(L_{f}\) and \(\).

In the specific case when \(_{f}=0\), we recover the same convergence rates as in , although our methods work for the more general composite optimization problem and, in contrast to , do not require that \( f(x^{*})=0\).

## 5 Implicit Variance Reduction

The assumption of uniformly bounded variance may not hold for some problems, or the corresponding constant \(^{2}\) might be quite large, which is why there has recently been a growing interest in various alternative variance bound assumptions . One interesting option is expressing complexity bounds via the variance at the minimizer, \(_{*}^{2}:=_{}(x^{*})\), assuming that the stochastic oracle \(\) satisfies some extra smoothness conditions. Let us show that, for our Algorithms 1 and 2, we can also establish such bounds, moreover, this can be done _without any modifications to the algorithms_.

In this section, we study problem (1) under Assumptions 1 and 2 and also under the following additional smoothness assumption on the variance:

**Assumption 6**.: _There exist \(_{},L_{} 0\) such that \(_{}(x,y) 2L_{}[_{f,,}(x,y)+ _{}]\) for any \(x,y^{d}\), where \(_{}(x,y)_{}[\|[g(x,)-g(y,)] -[(x)-(y)]\|_{*}^{2}]\)._

Note that \(_{}(x,y)\) is the usual variance of the estimator \(g(x,)-g(y,)\) which uses the same randomness \(\) for both arguments. Hence, \(_{}(x,y)[\|g(x,)-g(y,)\|_{*}^{2}]\) for any \(x,y\). Furthermore, if \(_{b}\) is the mini-batch version of \(\) of size \(b\) (i.e., the average of \(b\) i.e., samples of \((x)\) at any point \(x\)), then \(_{_{b}}(x,y)=\,_{}(x,y)\) for any \(x,y\).

For instance, if \(f(x)=_{}[f_{}(x)]\), where each function \(f_{}\) is convex and \((_{},L_{})\)-approximately smooth with components \((_{},_{})\), then, the stochastic gradient oracle \(\), defined by \(g(x,)_{}(x)\) satisfies Assumption 6 with \((x)=_{}[_{}(x)]\), \((x)=_{}[_{}(x)]\), and \(_{}=}}\,_{}[L_{} _{}]\) (\(_{}[_{}]\)), \(L_{}=L_{}\), where \(L_{}_{}L_{}\) (see Lemma 16). Furthermore, if \(_{b}\) is the mini-batch version of \(\) of size \(b\), then \(_{b}\) satisfies Assumption 6 with the same \(_{_{b}}=_{}\) but \(L_{_{b}}=L_{}=L_{}\) which can be much smaller than \(L_{}\) when \(b\) is large enough.

Under the new assumption on the variance, \(\) enjoys the following convergence rate (see Appendix D.1 for the proof).

**Theorem 7**.: _Let Algorithm 1 with \(M_{0}=0\) be applied to problem (1) under Assumptions 1, 2 and 6, and let \(_{*}^{2}_{}(x^{*})\). Then, for the point \(_{N}\) produced by the method, we have_

\[[F(_{N})]-F^{*}(c_{2}L_{f}+12c_{1}L_{} )D^{2}}{N}+2_{*}Dc_{4}}{N}}+c_{3}_{f}+_{}.\]

Comparing the above result with Theorem 4, we see that we have essentially replaced the uniform bound \(\) with the more refined one \(_{*}\) at the cost of replacing \(L_{f}\) with \(L_{f}+L_{}\) and \(_{f}\) with \(_{f}+_{}\)This corresponds to classical results on the usual SGD for which we know all problem dependent-constants. However, our method is universal and works automatically under both assumptions from the previous section and the current one, and therefore enjoys the best among the rates given by Theorems 4 and 7.

For the accelerated algorithm, we have the following result (whose proof is located in Appendix D.2).

**Theorem 8**.: _Let Algorithm 2 be applied to problem (1) under Assumptions 1, 2 and 6, and let \(_{*}^{2}_{}(x^{*})\). Then, for any \(k 1\), we have_

\[[F(x_{k})]-F^{*}c_{4}L_{f}D^{2}}{k(k+1)}+c_{4}L_{}D^{2}}{k+1}+4_{*}Dc_{4}}{k}}+ {c_{3}}{3}(k+2)_{f}+_{}.\]

Comparing our previous complexity bound for \(\) under the assumption on uniformly bounded variance (Theorem 5) with the bound from Theorem 8, we see that, instead of simply replacing \(\) with \(_{*}\), \(L_{f}\) with \(L_{f}+L_{}\) and \(_{f}\) with \(_{f}+_{}\), which was the case for the basic method, the situation is now not that simple. Specifically, the \(L_{f}\) and \(L_{}\) terms now converge at different rates: \(O(})\) and \(O()\), respectively. While this may seem strange at first, this behavior is actually unavoidable, at least in the case when \(_{f}=_{}=0\) (see, e.g., Section E in ). For the case when \(_{f}=_{}=0\), the complexity result from Theorem 8 is similar to the results for the Accelerated SGD algorithm from . However, the latter paper studies a specific setting where \(f(x)=[f_{}(x)]\), where each component \(f_{}\) is \(L_{}\)-smooth and then assumes that \(f\) is also \(L_{}\)-smooth, instead of working with the constant \(L_{f}\) which can be much smaller than \(L_{}\). A similar separation of the constants \(L_{f}\) and \(L_{}\), which we do, was recently considered in , where the authors obtained some similar rates to our Theorem 8. However, it is important that, unlike the algorithms considered in , our \(\) is universal and does not require knowing any problem-dependent constants except \(D\). Furthermore, our results are more general because we allow the oracle to be inexact.

## 6 Explicit Variance Reduction with SVRG

Let us now show that we can also incorporate explicit SVRG-type variance reduction into our methods. In this section, we consider problem (1) under Assumptions 1, 2 and 6. All the proofs are deferred to Appendix E.

In addition to the stochastic oracle \(\), we now assume that we can also compute the (approximate) full-gradient oracle \(\). This allows us to define the following auxiliary _SVRG oracle_ induced by \(\) with center \(^{d}\) (notation \(=_{,}()\)) as the oracle with the same random variable component \(\) as \(\) and the function component given by \(G(x,)=g(x,)-g(,)+()\).

Our \(\) method is presented in Algorithm 3. This is the classical epoch-based SVRG algorithm which can be seen as the adaptive version of the SVRG++ method from . A similar scheme was suggested in , however, instead of accumulating gradient differences as in (3), their method accumulates gradients and therefore does not work without the additional assumption of \( f(x^{*})=0\) (which may not hold for constrained optimization).

Let us now present the complexity guarantees. To do so, we first need to introduce, one more assumption we need in our analysis.

**Assumption 9**.: _The variance of \(\) satisfies \(_{}(x,y) 4L_{}[_{f}^{ f(x)}(x,y)+2 _{}]\) for any \(x,y^{d}\) and any \( f(x) f(x)\)._

Assumption 9 is very similar to Assumption 6. The only difference between them is that the former contains the standard Bregman distance in the right-hand side, while the latter contains its approximation \(_{f,,}(x,y)\) involving the approximate function value \((x)\) and the approximate gradient \((x)\). Nevertheless, both assumptions are actually satisfied for the main examples we discussed after introducing Assumption 6 (see Lemma 16).

**Theorem 10**.: _Let \(\) (as defined by Algorithm 3) be applied to problem (1) under Assumptions 1, 2, 6 and 9. Then, for any \(t 1\) and \(_{3}\{c_{3},1\}\), we have_

\[[F(_{t})]-F^{*}c_{4}+1)L_{f}+48c_{1}c_{4}L_ {}]D^{2}}{2^{t}}+2_{3}_{f}+_{}.\]

_To construct \(_{t}\), the algorithm needs to make \(O(2^{t})\) queries to \(\) and \(O(t)\) queries to \(\)._

We now present an accelerated version of \(\), see Algorithm 4. As \(\), this method is also epoch-based, and its epoch is very similar to \(\) (Algorithm 4) in the sense that it also iterates similar-triangle steps. However, the triangles in \(\) are of the form \((,v_{k},v_{k+1})\), i.e., they always share the common vertex \(\), in contrast to the triangles \((x_{k},v_{k},v_{k+1})\) in \(\) (in \(\), the role of the average points \(y_{k}\) is played by \(x_{k}\)). We note that our \(\) is essentially the primal version of the VRADA method from , but equipped with AdaGrad stepsizes. Alternative accelerated SVRG schemes with AdaGrad stepsizes (3) were recently proposed in ; however, they seem to be much more complicated.

The special choice of the initial reference point \(_{0}\) at Line 1 is rather standard and motivated by the desire to keep the initial function residual appropriately bounded: \(F(_{0})-F^{*}L_{f}D^{2}+_{f}\); the simplest way to achieve this is to make the full gradient step from any feasible point (see Lemma 34).

**Theorem 11**.: _Let \(\) (Algorithm 4) be applied to problem (1) under Assumptions 1, 2 and 6, and let \(N 9\). Then, for any \(t 10_{2}_{3}N-1\ ( 0)\), it holds that_

\[[F(_{t})]-F^{*}c_{4}+)L_{f}+6 c_{1}c_{4}L_{}]D^{2}}{N(t-t_{0}+1)^{2}}+(c_{3}t+1)_{f}+t _{}.\]

_To construct \(_{t}\), the algorithm needs to make \(O(Nt)\) queries to \(\) and \(O(t)\) queries to \(\). Assuming that the complexity of querying \(\) is \(n\) times bigger than that of querying \(\) and choosing \(N=(n)\), we get the total stochastic-oracle complexity of \(O(nt)\)._

Note that Theorem 11, unlike Theorem 10, does not require the extra Assumption 9. This suggests that Assumption 9 might be somewhat artificial and could potentially be removed from Theorem 10 as well. However, we do not know how to do it, even in the simplest case when \(_{f}=_{}=0\) and the algorithm has the knowledge of the constants \(L_{f}\) and \(L_{}\) from Assumptions 1 and 6.

## 7 Application to Holder Smooth Problems

To illustrate how powerful our results are, let us quickly consider the specific example of solving the stochastic optimization problem with Holder smooth components.

_Example 12_.: Suppose that the function \(f\) in problem (1) is the expectation of other functions, \(f(x)=_{}[f_{}(x)]\), where each function \(f_{}\) is convex and \((,H_{}())\)-Holder smooth. Consider thestandard mini-batch stochastic gradient oracle \(_{b}\) of size \(b\), defined by \(g_{b}(x,_{[b]})=_{j=1}^{b} f_{_{j}}(x)\), where \(_{[b]}(_{1},,_{b})\) with \(b\) i.i.d. copies of \(\), and \( f_{}(x) f_{}(x)\) is an arbitrary selection of subgradients for each \(\). We define \(H_{f}()\) as the Holder constant for the function \(f\) and \(H_{}()_{}H_{}()\) as the worst among Holder constants for each \(f_{}\). Note that we always have \(H_{f}()_{}[H_{}()]\) but \(H_{f}()\) can, in principle, be much smaller than the right-hand side. Also, define \(^{2}_{x}_{ _{1}}(x)_{x}_{}[ \| f_{}(x)- f(x)\|_{s}^{2}]\) and \(_{*}^{2}_{_{1}}(x^{*}) _{}[\| f_{}(x^{*})- f(x^{*})\|_{s}^{2}]\). We assume that the computation of \(_{b}\) can be parallelized and the computation of \( f\) is \(n_{b}\) times more expensive than that of \(_{b}\).

To solve the above problem, we can apply any of the methods we presented before. The resulting oracle complexities (in terms of the BigO-notation) are summarized in Table 2; the precise statements the corresponding results and their proofs are deferred to Appendix F.

Note that our problem is characterized by a large number of parameters, \(\), \(H_{f}()\), \(H_{}()\), \(\), \(_{*}\). For each combination of these parameters, we get a certain complexity guarantee for each of our methods, and it is impossible to say in advance which combination results in the smaller complexity bound. However, it is not important for our methods since none of them needs to know any of these constants to ensure the corresponding bound. This means that our algorithms are _universal_: they automatically figure out the best problem class for a specific problem given to them.

## 8 Experiments

Let us illustrate the performance of our methods in preliminary numerical experiments2 on solving

\[f^{*}_{\|x\| R}f(x)_{i=1} ^{n}[ a_{i},x-b_{i}]_{+}^{q}},\] (5)

where \(a_{i},b_{i}^{d}\), \(q\) and \(R>0\).

This test problem covers several interesting applications. Indeed, if \(q=2\), we get the classical Least squares problem. If \(q=1\), this is the well-known Support-Vector Machines (SVM) problem. In both cases, the ball-constraint \(\|x\| R\) acts as a regularizer, and problem (5) is, in fact, equivalent to \(_{x^{d}}[f(x)+\|x\|^{2}]\) for a certain \( 0\) (this follows, e.g., from the KKT optimality conditions) such that \(\) decreases when \(R\) increases.

Another interesting application of (5), which we consider in this section, is the _polyhedron feasibility problem_: find \(x^{*}^{d}\), \(\|x^{*}\| R\), inside the polyhedron \(P=\{x: a_{i},x b_{i},\ i=1,,n\}\). Such a point exists iff \(f^{*}=0\). Note that (5) is a problem with Holder smooth components of degree \(=q-1\). By varying \(q\) in (5), we can therefore check the adaptivity of different methods to the unknown to them Holder characteristics of the objective function.

The data for our problem is generated randomly. First, we generate \(x^{*}\) uniformly from the sphere of radius \(0.95R\) centered at the origin. Then, we generate i.i.d. vectors \(a_{i}\) with components uniformly distributed on \([-1,1]\). We then make sure that \( a_{n},x^{*}<0\) by inverting the sign of \(a_{n}\) if necessary. Next, we generate positive reals \(s_{i}\) uniformly in \([0,-0.1c_{}]\), where \(c_{}_{i} a_{i},x^{*}<0\), and set \(b_{i}= a_{i},x^{*}+s_{i}\). By construction, \(x^{*}\) is a solution of our problem with \(f^{*}=0\), and the origin \(x_{0}=0\) lies outside the polyhedron since there exists \(j\) (corresponding to \(c_{}\)) such that \(b_{j}=c_{}+s_{j} 0.9c_{}<0\).

   Method &  & Reference \\  \(\) (Alg. 1) & \(()}{}^{}D^{2}+\{D^{2}}{},\,(()}{ })^{}D^{2}+D^{2}}{^{2}}\}}\) & Cors. 37, 40 \\  \(\) (Alg. 2) & \(()D^{1+}}{}^{}}+\{D^{2}}{^{2}},\,( ()}{})^{}D^{2}+D^{2}}{ ^{2}}\}}\) & Cors. 38, 41 \\  \(\) (Alg. 3) & \([N_{}():=()}{}^{}}D^{2}+()}{ }^{}D^{2}+n_{b}_{N_{}}()}\) & Cor. 43 \\  \(\) (Alg. 4) & \([^{}H_{f}()D^{1+}}{}]^{}}+[^{}H_{}()D^{1+}}{(1+)^{2}}+n_{b} n_{b}}\) & Cor. 44 \\   

Table 2: Corollaries of our results for the case when problem (1) has Holder smooth components, as defined in Example 12. “SO complexity” is the stochastic-oracle complexity for reaching accuracy \(\) in terms of the expected function residual, defined as in Table 1 but with \(=_{b}\), \(= f\), \(n=n_{b}\).

We compare \(\) (Algorithm 3) against AdaSVRG  (with parameters \(K=3\) and \(=D=2R\)). We next compare \(\) (Algorithm 4) against AdaVRAE and AdaVRAG . We also compare it with the FastSvrg method with constant stepsize, which is the primal version of the VRADA method from ; the stepsize is selected by doing a grid search over \(\{10^{j}:j=-3,,4\}\) and choosing the best value in the sense that the algorithm is neither too slow nor has a large error. We report \(\) (Algorithm 1) and \(\) (Algorithm 2) together with these methods. For \(\), contrary to the theoretical recommendation of choosing \(_{0}\) as the result of the full gradient step, we found it slightly more useful to simply set \(_{0}=x_{0}\). For all our methods, we use the AdaGrad stepsize (3); the other stepsize (4) works very similarly (see Appendix H.2 for a detailed comparison). For all methods, we use the standard mini-batch stochastic oracle of size \(b=256\).

The results are shown in Fig. 1, where we fix \(n=10^{4}\), \(d=10^{3}\), \(R=10^{6}\) and consider different values of \(q\{1,1.3,1.6,2\}\). We plot the total number of stochastic oracle calls against the function residual. We treat one mini-batch oracle computation as one stochastic oracle call. If we compute the full gradient, we count this as \(n/b\) stochastic oracle calls where \(n\) is the total number of samples and \(b\) denotes the mini-batch size.

We see that, except the AdaSVRG method, all SVRG algorithms typically converge much faster than the usual SGD methods without explicit variance reduction, at least after a few computations of the full gradient. Among the non-accelerated SVRG methods, \(\) converges consistently faster than AdaSVRG, while \(\) performs the best across the accelerated ones. Note that FastSvrg with constant stepsize is not converging when the problem is not Lipschitz smooth (\(q<2\)), in contrast to our universal methods.

In Fig. 2, we also illustrate the impact of the mini-batch size \(b\) on the convergence of our methods. We consider the same values of \(n\), \(d\), \(R\) as before and fix \(q=1.5\). As we can see, in the idealized situation, when one can implement the mini-batch oracle computations by perfect parallelism, there is a significant speedup in convergence when increasing the mini-batch size, as predicted by our theory.

For additional experiments, including the discussion of implicit variance reduction, see Appendix H.

## 9 Conclusions

In this paper, we showed that AdaGrad stepsizes can be applied, in a unified manner, in a large variety of situations, leading to universal methods suitable for multiple problem classes at the same time. Note that this does not come for free. We still need to know one parameter, the diameter \(D\) of the feasible set. While it is not necessary to know this parameter precisely, the cost of underestimating or overestimating it, can be high (all complexity bounds would be multiplied by the ratio between our guess and the true \(D\)). At the same time, there already exist some parameter-free methods which are based on AdaGrad and aim to solve precisely this problem . It is therefore interesting to consider extensions of our results to these more advanced algorithms. Another interesting direction is, of course, nonconvex problems.

Figure 1: Comparison of different methods on the polyhedron feasibility problem (5).

Figure 2: Impact of mini-batch size on performance of our methods.