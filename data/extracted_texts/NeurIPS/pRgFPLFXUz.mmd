# Debiasing Large Vision-Language Models by Ablating Protected Attribute Representations

Anonymous Author(s)

Affiliation

Address

email

###### Abstract

Large Vision Language Models (LVLMs) such as LLaVA have demonstrated impressive capabilities as general-purpose chatbots that can engage in conversations about a provided input image. However, their responses are influenced by societal biases present in their training datasets, leading to undesirable differences in how the model responds when presented with images depicting people of different demographics. In this work, we propose a novel debiasing framework for LVLMs by directly ablating biased attributes during text generation to avoid generating text related to protected attributes, or even representing them internally. Our method requires no training and a relatively small amount of representative biased outputs (\(\)1000 samples). Our experiments show that not only can we can minimize the propensity of LVLMs to generate text related to protected attributes, but we can even use synthetic data to inform the ablation while retaining captioning performance on real data such as COCO. Furthermore, we find the resulting generations from a debiased LVLM exhibit similar accuracy as a baseline biased model, showing that debiasing effects can be achieved without sacrificing model performance.

## 1 Introduction

Deep neural networks are well known to exhibit societal biases learned from their training datasets . Numerous prior works have observed such biases in modern Large Language Models (LLMs) , while recent work has shown that societal biases are even more prevelant in Large Vision Language Models (LVLMs)  such as LLaVA , that combine a vision backbone or VLM with a pretrained LLM. Given that LLMs are often pretrained on relatively uncurated web-scale data , the resulting LVLM inherits the particular biases of the chosen LLM. Without additional safety tuning, these pre-existing biases may be amplified further when an LLM is augmented with pretrained visual capabilities, which also come with a distinct set of implicit societal biases in the visual pretraining data. Evaluating and mitigating potentially harmful behaviors induced by these societal biases is becoming increasingly important in order to safely deploy multimodal generative AI systems that utilize LVLMs.

Recently, a variety of methods have been proposed for debiasing LLMs and VLMs individually . However, relatively little prior work has focused specifically on debiasing LVLMs. Furthermore, many of the existing debiasing approaches for LLMs and VLMs focus on training models with additional data to reduce bias. Attempting to debias models through additional training in this manner often results in other undesirable outcomes, such as a degradation in task-specific performance. This approach is also labor and computationally intensive, requiring the collection of an additional (likely large) dataset that can appropriately debias the model. Despite prior efforts , there remains no cannonical recipe for constructing such a dataset withrespect to a specific attribute. Training also lacks controllability of debiasing effects for inference while requiring the data and computational resources necessary to train LVLMs. In contrast, our work introduces a training-free approach to debiasing LVLMs that can be applied to any attribute at inference time (see Appendix A for additional discussion of related work).

We propose to adapt model steering techniques from mechanistic interpretability to reduce a form of bias in which LVLMs comment on protected attributes of depicted people (such as perceived race, age, or body features). This approach modifies outputs by intervening on the residual stream during text generation, assuming certain attributes or concepts are represented as linear directions in the feature space. By up- or down-weighting these directions, we can control bias exhibited by the model. Previous work has shown that concepts such as "refusal" in LLMs can be manipulated in this manner (Arditi et al., 2024), and we hypothesize that similar methods can be applied to protected attributes in LVLMs. In this work, we identify and remove directions associated with biases in LVLMs using contrastive differences over a small set of examples. By reducing the model's ability to reference protected attributes such as perceived race or physical appearance, we enable more relevant commentary on input images. Significantly, our experiments show that our method reduces generation of protected attributes by over \(50\%\) across three evaluation strategies. Furthermore, we demonstrate that ablation directions from synthetic data transfer well to real-world cases.

## 2 Methods

Our approach to debiasing LVLMs involves identifying and ablating the bias attribute in the model's internal representations. We achieve this by contrasting the model's activations for standard prompts against activations for prompts which elicit biased responses.

### Bias Attribute Estimation

Let \(\) denote an arbitrary LVLM, and \(^{(l)}^{d}\) represent the activations at layer \(l\), where \(d\) is the dimensionality of the hidden state. We use \(^{d}\) to denote the bias attribute, which is a vector that captures the direction of the bias in the model's internal representations, and define \(^{(l)}\) as the residual at layer \(l\).

To estimate the bias attribute, we collect a dataset of standard prompt-image pairs \(_{}=\{(_{i})=(_{i},_{ i})\}_{i=1}^{N_{}}\) and a dataset of prompt-image pairs which elicit biased responses \(_{}=\{(_{i})=(_{i},_{i})\}_ {i=1}^{N_{}}\). Here, \(_{i}\) represents the text prompt and \(_{i}\) represents the corresponding image. We compute the activations of the model on both datasets and calculate the difference in means:

\[=_{}|}_{ _{}}^{(l)}()-_{}|}_{_{}}^{(l)}( )\]

We normalize the bias attribute to have unit length: \(/\|\|_{2}\). To ablate the bias attribute, we project the residual at each layer onto the bias attribute and subtract the projection from the residual to get a new residual \(^{(l)^{}}=^{(l)}-^{}^{(l)}\). We apply this ablation process to every residual in the LVLM, effectively removing the bias attribute direction from the model's internal representations.

### Evaluation Details

Identifying biased content in model outputs requires a multi-faceted approach, as manual annotation of every generation is impractical. We employ three different methods to evaluate the presence of attribute-related text: bigram frequency matching, GPT-4o-based evaluation (Achiam et al., 2023), and the DSL framework (Egami et al., 2023). Each method offers a different balance between interpretability and accuracy, and collectively they provide robust evidence for the effectiveness of our debiasing strategy. All three methods converge on the same conclusion: _steering effectively reduces mentions of target protected attributes in model outputs_.

Our simplest method uses bigram frequencies to identify mentions of protected attributes. We define a list of target words related to the attribute in question and detect all bigrams in model generations beginning with these words. Since many attribute-related terms are polysemous, we hand-annotate the most frequent 50% of bigrams to filter out unrelated terms. This enables us to adjust for over or under-counting by including only those bigrams that have been verified as attribute-related or excluding those that have been annotated as unrelated. Despite being transparent and interpretable, bigram frequencies have limited accuracy.

For a more nuanced evaluation, we use GPT-4o as a judge to annotate the amount of attribute-related text in each generation. Using a two-shot prompt with OpenAI's Structured Output API, GPT-4o returns both the count of race or ethnicity-related phrases and the corresponding spans. This method has proven to be highly reliable, with minimal discrepancies between the reported counts and the identified spans. Manual inspection of GPT-4o's highlighted spans confirmed that it captures a broad but justified set of terms that refer to perceived race or ethnicity.

Finally, we apply the DSL framework to correct the GPT-4o and bigram annotations using human labels. This statistically rigorous method estimates the true count of race or ethnicity mentions by bias-correcting the imperfect predictors. While this approach adds confidence to our results, we acknowledge that our understanding of what constitutes a mention of a protected attribute is shaped by our own perspectives, which introduces some inherent subjectivity.

## 3 Experiments

**Datasets**: We use subsets of the SocialCounterfactuals dataset Howard et al. (2024) for constructing ablation directions and evaluating models. This dataset includes synthetic images of people varying in protected attributes such as perceived race and physical appearance, with around 10K image-prompt pairs for both the "perceived race" and "physical appearance" subsets. Additionally, we leverage a subset of Demographic Annotations on COCO (Chen et al., 2015) (DA-COCO) (Zhao et al., 2021) which aligns with perceived race annotations from the SocialCounterfactuals dataset.

**Selecting an Ablation Direction**: Using LLaVA 1.5 (Liu et al., 2024), we compute ablation directions by contrasting biased and benign text generations. Biased text is generated from a specific prompt applied to 1000 image samples, while benign text is sourced from the LLaVA-Instruct-80K dataset (Liu et al., 2024) by excluding instances with protected attributes. We evaluate 32 candidate ablation directions based on a held-out set of 5 image-prompt pairs, selecting the most effective direction for further experiments. Details of the experimental design can be found in section (B).

### Results

**Evaluation of Perceived Race and Physical Appearance steering directions.** It should be noted that identification of perceived race or physical appearance related text can be varied and personal, and there is no perfect judge. Hence, our use of multiple evaluation strategies, which all substantiate our claim that our model steering method substantially reduces the rate of protected attribute generation. Figure 1 shows our method produces a 62% reduction in attribute-related text on average according to a hand-annotated bigram set, and a 57% reduction according to GPT-4o annotations. These results further highlight that while our method shows significant results, each annotation method has limitations. Table 1 further highlights differences in annotation strategies while strongly showing that our method is able to significantly reduce generation of target attributes.

Figure 1: (Left) The generation frequencies of bigrams related to protected attributes from LLaVA (Baseline) vs steered LLaVA (Steered). We show results on perceived race and physical appearance subsets of SocialCounterfactuals (SC Body, SC Race) as well as the DA-COCO subset that corresponds to the perceived race attribute in SocialCounterfactuals (DA-COCO). (Right) we show the GPT-4o evaluations on the same datasets

**Impact of steering on token probabilities.** Figure 2 shows the effectiveness of steering techniques in reducing bias in LVLM token predictions. After intervening to ablate biased directions in the model's internal representations, we observe a shift toward more neutral, contextually appropriate tokens, with biased terms related to protected attributes being suppressed. This effect is consistent in both single-image examples and across 300 generations from the SocialCounterfactuals test set, using the prompt "What are 5 keywords that describe the characteristics of this person?"

**Generalization of Target Directions.** For computational reasons, we prefer that ablated representations generalize to new observations. To evaluate to what extent this holds, we apply the "Perceived Race" attribute direction found using the SocialCounterfactuals dataset to the DA-COCO dataset. All three of our metrics shown in Table 1) agree that our method results in a significant decrease in the output of biased text. In particular, our strongest estimation method DSL yields a 62% reduction in text related to perceived race than the baseline LVLM on DA-COCO.

Accuracy of generated responses.We employed the LLM-as-a-judge approach [Zheng et al., 2023] to investigate whether steering affects the accuracy of generated responses. We used GPT-4o to evaluate whether LLaVA's text responses, with and without steering, match the corresponding image. GPT-4o was given the image and prompt: "Does the description match the image? Answer with Yes or No." Manual analysis showed that GPT-4o responds "No" when the generation contains extra details not present in the image. The results (Table 2) show no significant difference in accuracy between baseline and steered LLaVA models, indicating that steering does not degrade performance.

## 4 Discussion

We introduce a training-free method for mitigating bias in LVLMs through model steering techniques at inference time, achieving a significant reduction in protected attribute text generation related to perceived race and physical appearance. Despite our best efforts to improve the fairness of generative AI models, we acknowledge that our choice of models, methodologies, and datasets may themselves contain latent biases which limit our ability to address this multi-faceted problem. Our method effectively reduces bias but relies on contrastive examples, which may introduce noise and limit the generalizability of ablation directions to unseen data. It primarily targets specific attributes, potentially overlooking the full range of societal biases present in LVLMs. Future work should aim to expand bias mitigation techniques to encompass a broader spectrum of attributes and assess the long-term impacts of steering interventions on model performance.

  
**Measure** & **Decrease \(\%\) (CI)** & \\  Bigram & \(-65.3\% 9.55\%\) & \\ GPT & \(-56.9\% 7.27\%\) & \\ DSL & \(-61.8\% 29.4\%\) & \\   

Table 1: Estimated decrease (%) in mention of perceived race/ethnicity on DA-COCO

   Model & SC Race & DA-COCO \\  Baseline & \(70.53\%\) & \(64.47\%\) \\ Steered & \(71.77\%\) & \(64.33\%\) \\   

Table 2: Percentage of LLaVA generations (%) evaluated by GPT-4o as matching the corresponding image.