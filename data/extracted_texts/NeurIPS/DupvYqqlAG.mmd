# Spectral Learning of Shared Dynamics Between Generalized-Linear Processes

Lucine L. Oganesian

Ming Hsieh Department of Electrical and Computer Engineering

University of Southern California

Los Angeles, CA

loganesi@usc.edu

Omid G. Sani

Ming Hsieh Department of Electrical and Computer Engineering

University of Southern California

Los Angeles, CA

omid.ghasemsani@usc.edu

Maryam M. Shanechi

Ming Hsieh Department of Electrical and Computer Engineering

Thomas Lord Department of Computer Science

Alfred E. Mann Department of Biomedical Engineering

Neuroscience Graduate Program

University of Southern California

Los Angeles, CA

shanechi@usc.edu

Corresponding author. Project code: [https://github.com/ShanechiLab/PGLDM](https://github.com/ShanechiLab/PGLDM)

###### Abstract

Generalized-linear dynamical models (GLDMs) remain a widely-used framework within neuroscience for modeling time-series data, such as neural spiking activity or categorical decision outcomes. Whereas the standard usage of GLDMs is to model a single data source, certain applications require jointly modeling two generalized-linear time-series sources while also dissociating their shared and private dynamics. Most existing GLDM variants and their associated learning algorithms do not support this capability. Here we address this challenge by developing a multi-step analytical subspace identification algorithm for learning a GLDM that explicitly models shared vs. private dynamics within two generalized-linear time-series. In simulations, we demonstrate our algorithm's ability to dissociate and model the dynamics within two time-series sources while being agnostic to their respective observation distributions. In neural data, we consider two specific applications of our algorithm for modeling discrete population spiking activity with respect to a secondary time-series. In both synthetic and real data, GLDMs learned with our algorithm more accurately decoded one time-series from the other using lower-dimensional latent states, as compared to models identified using existing GLDM learning algorithms.

Introduction

Generalized-linear dynamical models (GLDMs) are a commonly used framework for modeling dynamics using a low-dimensional latent variable that evolves over time [1; 2; 3]. Due to their interpretability, data efficiency, and amenability to real-time engineering operations, GLDMs remain a widely popular tool in neuroscience for modeling time-series data, whether it be Poisson spiking neural activity, Bernoulli/Binomial categorical task variables, or Gaussian behavior [4; 5; 6; 7; 8; 9]. Whereas most existing GLDM variants and their associated analytical learning algorithms focus on modeling the dynamics within a single time-series, there exist applications that require explicit dissociation of shared vs. private dynamics within two generalized-linear observation sources. For example, such functionality is helpful when modeling the dynamical relationship between recorded neural activity and certain behaviors of interest [10; 11; 12; 13; 14; 15].

Here we fill these methodological gaps by deriving a novel covariance-based subspace system identification (SSID) algorithm that is capable, with its multi-staged learning approach, of identifying the shared dynamics between two generalized-linear time-series with priority, before modeling the dynamics private to each observation. We design the method to seamlessly generalize to different observation distributions, such as Poisson or Bernoulli. To illustrate the method, we first show in simulations that our method successfully dissociates the shared dynamics within two generalized-linear time-series, agnostic of their respective observation models; to compare against existing GLDM methods, we focused on Poisson, Bernoulli, and Gaussian generalized-linear observations. Next we demonstrate our method on two public non-human primate (NHP) datasets of discrete population spiking activity recorded from different brain regions and during different contexts [16; 17; 18]. Compared with existing Poisson GLDMs and their learning algorithms, our method learned models that more accurately decoded one time-series from the other using lower-dimensional latent states, suggesting improved learning of shared dynamics.

## 2 Background

For linear state-space models with continuous Gaussian observations, subspace system identification (SSID) theory provides computationally efficient non-iterative algorithms for analytically learning state-space models, both with and without identification of shared dynamics [14; 19; 20; 21; 22; 23; 24]. These methods, however, either are not applicable to generalized-linear time-series with non-Gaussian observations [14; 19; 20; 23] or do not have the ability to dissociate shared vs. private dynamics between two time-series [21; 22; 24]. To help with the exposition of our method in section 3, we first review standard covariance-based SSID and an existing SSID method for modeling Poisson point-processes , a widely-used class of generalized-linear observations.

### Standard covariance-based SSID

The standard formulation for a linear state-space model with continuous Gaussian observations is as

\[\{_{k+1}&=&_{k}+_{ k}\\ _{k}&=&_{}_{k}+_{k}. \]

where \(_{k}^{n_{x}}\) is the latent state variable, \(_{k}^{n_{r}}\) corresponds to continuous Gaussian observations, and \(\) and \(\) are state and observation noise terms, respectively, with distributions \((_{k};,)\) and \((_{k};,)\), and cross-covariance \(\). Further, we define \(:=(_{k+1},_{k})\), as the covariance between future latent state and current observation, and \(_{_{0}}:=(_{k},_{k})\), as the instantaneous covariance of the observations. Standard covariance-based SSID learns the parameters of a latent dynamical system \(=(,_{},,_{_{0}})\) given training samples \(_{k}\) and hyperparameter \(n_{x}\) that specifies the latent state dimensionality. To do so, a future-past Hankel matrix, \(_{}\), is first constructed from the cross-covariances of the system's linear observations as [19; 20]

\[_{}:=(_{f},_{p})= _{_{i}}&_{_{i-1}}&&_{_{1}}\\ _{_{i+1}}&_{_{i}}&&_{_{2}}\\ &&&\\ _{_{2i-1}}&_{_{2i-2}}&&_{_{i}},\ \ \ _{f}:= _{i}\\ \\ _{2i-1},\ \ \ _{p}:= _{0}\\ \\ _{i-1}, \]

where the integer \(i\) denotes the user-specified maximum temporal lag (i.e., horizon) used to construct \(_{}\) and \(_{_{r}}:=(_{k+},_{k})\) is the \(\)-th lag cross-covariance for any timepoint \(k\), under time-stationary assumptions. We note that the rank of \(_{}\) must be at least \(n_{x}\) in order to identify a model with a latent dimension of \(n_{x}\). Thus the user-specified horizon \(i\) must satisfy \(i n_{r} n_{x}\). Covariance-based SSID then decomposes \(_{}\) into a product of observability \((_{})\) and controllability (\(\)) matrices as [19; 20]

\[_{}}{=}_{}= _{}\\ _{}\\ \\ _{}^{i-1}[^{i-1}&& &] \]

where \(\) is defined as above. The factorization of \(_{}\) is done by computing a singular value decomposition (SVD) of \(_{}\) and keeping the top \(n_{x}\) singular values and corresponding singular vectors. From the factors of \(_{}\), \(_{}\) is read off as the first \(n_{r}\) rows of \(_{}\) and \(\) is read off as the last \(n_{r}\) columns of \(\). \(\) is learned by solving \(}_{}=}_{}\), where \(}_{}\) and \(}_{}\) denote \(_{}\) from which the top or bottom \(n_{r}\) rows have been removed, respectively. This optimization problem has the following closed-form least-squares solution \(=}_{}^{}}_{ }\), with \(\) denoting the pseudo-inverse operation. The final parameter \(_{_{0}}\) is computed as the empirical covariance of \(_{k}\). See appendix A.4 on how \((,_{_{0}})\) specify \((,,)\).

### SSID for a single generalized-linear time-series

There has been some work extending SSID to generalized-linear time-series, such as Poisson and Bernoulli observations [21; 24]. These methods, however, only learn the dynamics of a single generalized-linear time-series rather than model shared vs. private dynamics between two time-series. Here we present one of our baselines, PLDSID , which models a single Poisson time-series, as an example. A Poisson linear dynamical system (PLDS) model is defined as

\[\{_{k+1}&=&_{k}+ _{k}\\ _{k}&=&_{}_{k}+\\ _{k}_{k}&&((_{k})) . \]

where \(_{k}^{n_{x}}\) is the latent state as before and \(_{k}^{n_{y}}\) corresponds to discrete (e.g., neural spiking) observations which, conditioned on the latent process \(_{k}\), are Poisson-distributed with a rate equal to the exponential of \(_{k}\) (i.e., log-rate). Finally, \(_{k}\) is Gaussian-distributed state noise with covariance parameter \(\), as before, and \(\) is a constant baseline log-rate. The PLDS model is commonly used for modeling Poisson process events, such as neural spiking activity [2; 4; 6; 21; 25]. Buesing et al.  developed a SSID algorithm, termed PLDSID, to learn the PLDS model parameters \(_{}=(,_{},,)\) given training samples \(_{k}\) and hyperparameter \(n_{x}\).

Standard covariance-based SSID algorithms (section 2.1) are not directly applicable to Poisson-distributed observations. This is because the log-rates \(_{k}\) that are linearly related to the latent states in equation (4) are not observable in practice - rather, only a stochastic Poisson emission from them (i.e., \(_{k}\)) is observed. As a result, the second moments constituting \(_{}\) (i.e., \(_{_{}}\)) cannot be directly estimated. The critical insight by Buesing et al.  was to leverage the log link function (i.e., \(^{-1}\)) and the known conditional distribution \(_{k}|_{k}\) to compute the first (\(_{^{}}\)) and second (\(_{^{}}\)) moments of the log-rate \(_{k}\) from the first (\(_{^{}}\)) and second (\(_{^{}}\)) moments of the discrete observations \(_{k}\). The \(\) denotes that moments are computed for the future-past stacked vector of observations \(^{}:=_{f}^{T}&_{p}^{T}^ {T}\) and \(^{}:=_{f}^{T}&_{p}^{T}^ {T}\), where

\[_{^{}}:=E[^{}]_{^{} }:=E[^{}]_{^{}}:=( ^{},^{})_{^{}}:= (^{},^{}).\]

To compute moments of the log-rate, Buesing et al.  derived the following moment conversion

\[_{_{m}^{}} = 2(_{_{m}^{}})-( _{_{m}^{}}+_{_{m}^{}}^{2}-_{_{m}^{}}^{}) \] \[_{_{m}^{}} = (_{_{m}^{}}+_{_{m}^{ }}^{2}-_{_{m}^{}}^{})-(_{_{m}^{ }}^{2})\] \[_{_{m}^{}} = (_{_{m}^{}}+_{_{m}^{ }}^{2}_{_{m}^{}}^{})-(_{_{m}^{ }}^{}_{_{m}^{}}^{})\]

where \(m n\) correspond to different indices of the first and second moments of the future-past stacked observation vectors \(^{}\) and \(^{}\), and \(n,m=1,,Kn_{y}\) where \(K\) is the total number of time points. With the first and second moments computed in the moment conversion above, the baseline log rate \(\) parameter is read off the first \(n_{r}\) rows of \(_{^{}}\) and the Hankel matrix, \(_{}\), is constructed as per equation (2). From here, we can proceed with the standard covariance-based SSID algorithm using \(_{}\), as outlined in section 2.1. Discussion regarding learning the state noise covariance parameters (e.g., \(\)) is postponed to appendix A.3, where we use an approach that - unlike Buesing et al.  - ensures validity of learned noise statistics.

Method

### Model definition and assumptions

Both the standard linear state-space model (equation (1)) and PLDS model (equation (4)) only model a single observation on its own. To enable identification of shared and private dynamics between two generalized-linear time-series, we write the following general multi-observation GLDM

\[\{_{k+1}&=&_{k}+_{ k}\\ _{k}&=&_{}_{k}+_{k}+\\ _{k}&=&_{}_{k}+_{k}+\\ _{k}|_{k}&&_{|}(_{k};\ g(_{k}))\\ _{k}|_{k}&&_{|}(_{k};\ h(_{k})). \]

where \(_{k}\), \(_{k}\), \(_{k}\), \(_{k}\), and \(\) are defined as in equations (1) and (4). We introduce \(_{k}^{n_{t}}\) to represent the second generalized-linear observation time-series, \(_{k}^{n_{x}}\) (with \(n_{t}=n_{z}\) by construction) to represent the latent process underlying this second observation, \(_{k}(_{k};,)\) to represent the associated noise term, and \(\) to represent the associated baseline value. We generically denote the probability distribution for \(_{k}\) conditioned on the latent \(_{k}\) with \(_{}\). For example, in the PLDS model \(_{|}:=((_{k}))\). \(_{|}\) is defined similarly but for \(_{k}\) and \(_{k}\). Finally, \(g()\) and \(h()\) correspond to the link function in the generalized-linear model, for example \(g(_{k})=(_{k})\) in PLDS models. In order to dissociate between shared and private dynamics within the observation time-series, we introduce the following definition:

**Definition 3.1**.: We take the system to be written in a block structure form as defined below , allowing us to dissociate shared from private latents

\[=_{11}&&\\ _{21}&_{22}&\\ &&_{33}_{}= _{}^{(1)}&&_{}^{(3)} _{}=_{}^{(1)}&_{}^{(2)}&=^{(1)}\\ ^{(2)}\\ ^{(3)} \]

where \(_{k}^{(1)}^{n_{1}}\) corresponds to latent states that drive both \(_{k}\) and \(_{k}\), \(_{k}^{(2)}^{n_{2}}\) corresponds to states that only drive \(_{k}\), and \(_{k}^{(3)}^{n_{3}}\) corresponds to states that only drive \(_{k}\) - with total states \(n_{x}=n_{1}+n_{2}+n_{3}\). The parameter \(\) can also be written in block partition format such that

\[=E_{k+1}^{(1)}\\ _{k+1}^{(2)}\\ _{k+1}^{(3)}_{k}^{T}-E _{k+1}^{(1)}\\ _{k+1}^{(2)}\\ _{k+1}^{(3)}\\ _{k+1}^{(3)}E[_{k}]^{T}=E[ _{k+1}^{(1)}_{k}^{T}]\\ E[_{k+1}^{(2)}_{k}^{T}]\\ E[_{k+1}^{(3)}_{k}^{T}]-E[ _{k+1}^{(1)}|E[_{k}]^{T}]\\ E[_{k+1}^{(2)}|E[_{k}]^{T}]\\ E[_{k+1}^{(3)}|E[_{k}]^{T}]= ^{(1)}\\ ^{(2)}\\ ^{(3)}.\]

We can further simplify the definition of \(\) using the following assumptions:

**Assumption 3.2**.: _The state noise covariance \(\) is assumed to have a block diagonal structure such that \(=(^{(1,2)},^{(3)})\), where \(^{(1,2)}\) is a square matrix of dimension \(n_{1}+n_{2}\) and \(^{(3)}\) is a square matrix of dimension \(n_{3}\). Formally, the superscript notation \((1,2)\) designates attribution of the parameter to the first and second set of latent states._

**Assumption 3.3**.: _Initial latent states are assumed to be mutually-independent, making \((_{0},_{0})\) diagonal._

These assumptions allow us to fully decouple the private latent states of the secondary time-series (\(^{(3)}\)) from the latent states driving the primary time-series (\(^{(1)},^{(2)}\)). As a result, we can take \(^{(3)}=(_{k+1}^{(3)},_{k})=\). From the perspective of state estimation, this simplification implies that \(_{k}\) provides no information to help estimate \(_{k+1}^{(3)}\), for all \(k\); this understanding is consistent with our definition of shared and private states.

### Prioritized generalized-linear dynamical modeling (PGLDM)

Our method, which we term Prioritized Generalized-Linear Dynamical Modeling (PGLDM), uses a multi-staged learning approach to model a primary generalized-linear time-series while prioritizing identification of the dynamics shared with a secondary time-series. Note, "primary" refers to the data source whose modeling is of primary interest and that can optionally be used to predict the 

[MISSING_PAGE_FAIL:5]

Lastly, to learn the shared dynamics summarized by the parameter \(_{11}\), we solve the optimization problem \(^{(1)}=_{11}}^{(1)}\) where \(^{(1)}\) and \(}^{(1)}\) denote \(^{(1)}\) from which \(n_{r}\) columns have been removed from the right or left, respectively. The closed-form least-squares solution for this problem is \(_{11}=^{(1)}(}^{(1)})^{}\). This concludes the learning of the desired parameters \((_{11},_{}^{(1)},_{}^{(1)},,)\), given hyperparameter \(n_{1}\).

#### 3.2.2 Stage 2: private dynamics in primary process

After learning the shared dynamics, our algorithm can learn the dynamics private to the primary process that were not captured by \(_{k}^{(1)}\). Specifically, we learn the following parameters from equation (7): \([[_{21}_{22}],_{}^{(2)}]\), with hyperparameter \(n_{2}\) determining the unshared latent dimensionality of \(\). To do so, we first compute a "residual" Hankel matrix, \(_{}^{(2)}\), using \(_{}^{(1)}\) and \(^{(1)}\) from stage 1 and decompose it using SVD, keeping the first \(n_{2}\) singular values and vectors

\[_{}^{(2)}=_{}-_{}^{(1)} ^{(1)}}{=}_{}^{(2)}^{(2)}. \]

With \(_{}^{(2)}\), which corresponds to the first \(n_{r}\) rows of \(_{}^{(2)}\), we construct \(_{r}=_{r}^{(1)}&_{r}^{(2)}\). We then use \(^{(2)}\) to form the controllability matrix \(^{(1,2)}\) as the concatenation of \(^{(1)}\) and \(^{(2)}\) (derivation in appendix A.1):

\[^{(1,2)}=^{(1,2)^{i-1}}^{(1,2)}&& ^{(1,2)}^{(1,2)}&^{(1,2)}=^{(1)}\\ ^{(2)}\]

where \(^{(1,2)}\) refers to the upper left block of the dynamics matrix \(\) that corresponds to the latent states \(^{(1)}\) and \(^{(2)}\). Given \(^{(1,2)}\), we extract \(_{21}&_{22}\) by solving the problem \(^{(2)}=_{21}&_{22} {}^{(1,2)}\) where

\[^{(2)}:=_{21}&_{22}^{ (1,2)^{i-2}}^{(1,2)}&&_{21}&_{22} ^{(1,2)},}:= ^{(1,2)^{i-2}}^{(1,2)}&&^{(1,2)}.\]

Concatenating the sub-blocks together, \(^{(1,2)}=_{11}&\\ _{21}&_{22}\). Thus, given hyperparameters \(n_{1}\) and \(n_{2}\), we now have all model parameters associated with the shared dynamics and dynamics private to the primary signal: \((^{(1,2)},_{},_{}^{(1)},,)\). The remaining model parameters \((_{33},_{}^{(3)})\) are learned in stage 3 (appendices A.1.4 and A.9).

#### 3.2.3 Supporting generalized-linear processes

The covariance-based SSID algorithm that we have just derived is what enables our framework to be broadly applicable to generalized-linear time-series data. As discussed in section 2.2, the variables that are linearly related to latent states \(\) are unobservable in generalized-linear models. However, because the algorithm outlined in sections 3.2.1-3.2.2 only relies on empirical covariances and cross-covariances of the two observation time-series, we can support generalized-linear processes by using moment-conversions (e.g., section 2.2) . When computationally tractable moment conversion equations exist, we can compute both a Hankel matrix \(_{}\), as described in section 2.2, and also a cross-term Hankel matrix \(_{}\). For example, for the scenario wherein the Poisson observations constitute the primary process and Gaussian observations the secondary process (i.e., the first four lines of equation (6)), we can compute a moment conversion to estimate joint moments of \(_{k}\) and \(_{k}\) from the joint moments of the observed signals \(_{k}\) and \(_{k}\) with the following equation (derived using the conditional statistical properties, see appendix A.1.5)

\[_{_{f_{m}}_{p_{n}}} = (_{f_{m}},_{p_{n}})\ /\ _{ _{p_{n}}} \]

where, similar to equation (5), \(m\) and \(n\) correspond to indices of the first and second moments of the observation vectors \(_{f}\) and \(_{p}\) (or \(_{p}\)), respectively. As another example, if both generalized-linear observations (e.g., \(\) and \(\) from equation (6)) are Poisson distributed, the joint moments can be computed as

\[_{_{f_{m}}_{p_{n}}} = ((_{f_{m}},_{p_{n}})+_ {_{f_{m}}}_{_{p_{n}}})-(_{_{f_{m} }}_{_{p_{n}}}). \]

For both of these scenarios the baseline log-rates are learned as in PLDSID (section 2.2). Thus, our novel multi-staged covariance-based SSID learning algorithm enables identification of shared vs. private dynamics across various generalized-linear processes.

## 4 Experimental Results

### Shared dynamics are accurately identified in generalized-linear simulations

To evaluate how well our method identified the shared dynamics between two generalized-linear time-series, we simulated observations from random dynamical models as per equation (6). All state and observation dimensions were randomly selected, and the corresponding system parameters were randomly generated to simulate stable and slow-decaying dynamics (see appendix A.7.1 for details). In our first experiment, we evaluated how well the shared dynamical subspace could be identified when learning models at the true shared dimensionality. We performed this analysis for four combinations of generalized-linear observation pairs (\(_{k}\) or \(_{k}\) and \(_{k}\) or \(_{k}\), respectively): (1) Gaussian/Gaussian, (2) Poisson/Gaussian, (3) Poisson/Poisson, and (4) Bernoulli/Gaussian. Within each configuration, we compared models learned by our method against models learned with either Laplace-EM (expectation-maximization)  or a SSID algorithm with the appropriate observation distribution. For the Gaussian/Gaussian case we also compare against PSID , an SSID algorithm that preferentially learns the shared dynamics between two Gaussian time-series. PGLDM (our method) and PSID were trained using both the primary and secondary time-series, whereas all other methods used only the primary time-series as they only model a single data source. We evaluated identification of shared dynamics by computing the normalized eigenvalue error between ground truth shared modes (i.e., eigenvalues of \(_{11}\) in equation (7)) and the identified modes (i.e., the learned \(_{11}\) for PGLDM/PSID or \(\) for the other baselines); see appendix A.8.1 for evaluation details. We report the results of this analysis for 20 systems per configuration in Table 1. For almost all conditions PGLDM more accurately identified the shared dynamics.

In our second simulation experiment, we studied the effect of latent state dimension on learning. We generated 16 systems with fixed dimensions for shared and private latent states given by \(n_{1}=4,n_{2}=12,\) and \(n_{3}=4\), accordingly. We swept the learned latent state dimension from 1 to the true dimensionality of the primary observation time-series \(n_{1}+n_{2}=16\), with the dimensionality of shared dynamics set to \((\ n_{x},n_{1})\). We found that our method accurately identified the shared modes with the minimal latent state dimension of 4; in contrast, PLDSID and Laplace-EM did not reach such high accuracy even when using higher latent state dimensions (figure 1c). In these simulations we also evaluated the predictive power (i.e., correlation coefficient, CC) of the model when using discrete Poisson observations to predict continuous Gaussian observations in a held-out test set (see appendix A.8.2). This second metric allowed us to test our hypothesis that PGLDM's explicit modeling of the shared subspace improved decoding of Gaussian observations from Poisson observations compared with our baselines PLDSID  and Laplace-EM . We observed that our method achieved higher decoding performance in low-dimensional regimes, even when using as few as 4 latent states, whereas PLDSID required much larger latent state dimensions (around 12) to reach comparable performance (figure 1a). We also evaluated Poisson self-prediction using area under the receiver operating characteristic curve, AUC (figure 1b). With the inclusion of stage 2 and sufficient model capacity, models learned by PGLDM were able to achieve comparable performance in self-prediction as compared to our baselines.

    & _{k}\) or \(_{k}\)) / Secondary time-series (\(_{k}\) or \(_{k}\))} \\  Method Name & Gaussian/Gaus. & Poisson/Gaus. & Pois./Pois. & Bernoulli/Gaus. \\  PGLDM (stage 1) & -2.757 \(\) 0.070 & **-2.707 \(\) 0.091** & **-1.969 \(\) 0.079** & **-2.864 \(\) 0.072** \\ Laplace-EM  & -1.320 \(\) 0.091 & -1.083 \(\) 0.119 & -1.088 \(\) 0.110 & -1.027 \(\) 0.067 \\ PSID (stage 1)  & **-2.985 \(\) 0.102** & ✗ & ✗ & ✗ \\ Covariance SSID  & -1.467 \(\) 0.080 & ✗ & ✗ & ✗ \\ PLDSID  & ✗ & -1.319 \(\) 0.132 & -1.203 \(\) 0.112 & ✗ \\ bestLDS  & ✗ & ✗ & ✗ & -1.209 \(\) 0.117 \\   

Table 1: Shared mode identification error (log10, i.e., -2 means 1%) at the shared latent dimensionality (\(n_{x}=n_{1}\)). ✗ indicates that a method (row) does not support the primary observation model (column).

### Modeling shared dynamics improves motor decoding from population spiking activity

As a demonstration on real data, we used PGLDM to model the shared dynamics between discrete Poisson population neural spiking activity and continuous Gaussian arm movements in a publicly available NHP dataset from the Sabes lab . The dataset is of a NHP moving a 2D-cursor in a virtual reality environment based on fingertip position. We use the 2D cursor position and velocity as the continuous observations \(\). For all methods we used 50ms binned multi-unit spike counts for the discrete observations \(\). We evaluated decoding performance of learned models using five-fold cross validation across six recording sessions (see appendix A.7.2 for cross-validation details). For PGLDM, we use the shared dynamics dimensionality of \(n_{1}=(\ n_{x},8)\), i.e., a maximum \(n_{1}\) of 8, because behavior decoding using stage 1 roughly plateaued at this dimension.

Compared with PLDSID and Laplace-EM, our method learned models that led to better behavioral decoding at all latent state dimensions, including at the maximum latent state dimension (figure 2a). This result suggests that our method better learns the shared dynamics between Poisson spiking and continuous movement observations due to its ability to dissociate shared vs. private latent states. Interestingly, despite the focus on learning the shared latent states in the first stage, PGLDM was also able to extract the private latent states in the Poisson observations because of its second stage. This stage led to improved neural self-prediction AUC, while still maintaining the more accurate behavioral decoding (figure 2b-c). Indeed, even with the inclusion of just two additional latent states to model private Poisson dynamics (\(n_{2}=2\), \(n_{x}=10\)), neural self-prediction was approaching that of models learned by PLDSID (figure 2b). Finally, given its analytical nature, PGLDM required a substantially lower training time compared with Laplace-EM (see appendix Table 2).

Figure 1: **In simulations, PGLDM more accurately learns the shared dynamical modes and better predicts Gaussian observations from Poisson observations, especially in low-dimensional regimes**. Solid traces show the mean and shaded areas denote the standard error of the mean, (s.e.m.) for each condition. (a-b) Predictive power as a function of latent state dimensionality for all learned models compared against oracle model, i.e., a model with the ground-truth parameters. Left panel (a) shows prediction CC for the Gaussian observations and right panel (b) Poisson self-prediction AUC. (c) The normalized identification error of the shared dynamical modes (in log10 scale) as a function of latent dimensionality. (d) Mode identification with models of size \(n_{x}=n_{1}=2\) for a sample Bernoulli/Gaussian system with true dimensions \(n_{1}=2,n_{2}=6,n_{3}=4\).

### PGLDM models better decode spiking activity of one visual area from another

As a second demonstration on real data but with a different combination of observation distributions (Poisson/Poisson), we used PGLDM to decode neural population spiking activity in one visual area from another. In a publicly available dataset from Zandvakili and Kohn [16; 18], simultaneous V1/V2 population recordings were performed in anaesthetized NHPs as they were presented visual stimuli. We used five-fold cross validation to evaluate learned model performance in decoding V1 activity from V2 activity and in V2 self-prediction. We again compare with PLDSID and Laplace-EM. For all learning algorithms we tested four latent state dimensions such that \(n_{x}=n_{1}+n_{2}\{2,4,6,8\}\). For PGLDM we used the first two stages, setting the shared dynamics dimensionality to \(n_{1}=(\ n_{x},4)\). Similar to results in figure 2, we chose a maximum \(n_{1}\) of 4 because decoding roughly plateaued at this dimension. The conclusions were consistent with those in figure 2: modeling the shared vs. private dynamics by PGLDM allowed for better decoding of V1 activity while maintaining comparable self-prediction of V2 activity. Analysis details are in appendix A.7.3. We present the results of the complementary analysis (i.e., predicting V1 from V2) in appendix A.12.

### Limitations

PGLDM, similar to other SSID methods, uses a time-invariant model which may not be suitable if the data exhibits non-stationarity, such as in chronic neural recordings. To handle non-stationarities, one would need to either intermittently refit the model after a predetermined duration of time, or develop adaptive extensions [27; 28]. As an example of the latter, one can gradually update the model parameters by incorporating a learning rate that weighs recent observations more heavily in the moment computations while gradually forgetting past observations . Moreover, as with other covariance-based SSID methods, PGLDM may be sensitive to the accuracy of the empirical estimates of the first- and second-order moments. However, with increasing number of samples these empirical estimates will approach true statistical values, thereby improving overall performance, as seen in appendix figure 4. Due to errors in the empirical estimates of the covariances, SSID methods may also occasionally learn unstable dynamics (see appendix A.5). Future work may address this by

Figure 2: **In NHP data, PGLDM improves movement decoding from Poisson population spiking activity.** (a) Solid traces show the average cross-validated kinematic prediction CC (shaded areas denote the s.e.m.) for models of different latent dimensions learned by PGLDM, PLDSID, and Laplace-EM. (b) Same as (a) but visualizing one-step ahead neural self-prediction AUC. (c) Kinematic prediction CC and neural self-prediction AUC for models of latent dimensionality \(n_{x}=12\). Asterisks indicate statistical significance (Wilcoxon signed-rank test) with *: \(p<0.05\) and ***: \(p<0.0005\). (d) Example decoding of cursor (x,y) position and velocity from test data.

incorporating techniques from control theory, such as mode stabilization and covariance matching . Finally, although the GLDMs that PGLDM learns are widely used (e.g., in neuroscience), such models may not be suitable for time-series with nonlinearly evolving states. We did not compare our method against nonlinear deep learning methods, such as recurrent neural networks and transformers , because the goals of these two modeling approaches are different. While nonlinear deep learning methods are typically used to boost overall decoding performance, GLDMs are used for their interpretability and utility in scientific investigations and in real-time, computationally-efficient engineering applications (e.g., brain-computer interfaces).

## 5 Discussion

We developed PGLDM, a novel analytical multi-staged covariance-based SSID algorithm for modeling two generalized-linear processes while also dissociating shared from private dynamics. In simulations we demonstrate that our method successfully achieves this capability agnostic to the generalized-linear observation distribution. As a result, our approach more accurately models system dynamics compared to several commonly-used GLDM variants and their corresponding learning algorithms. We also demonstrate our method's applicability to real data by modeling two distinct NHP datasets recorded under different contexts and from different brain regions. In both simulations and in real data, PGLDM's ability to dissociate shared from private dynamics improved decoding of a secondary time-series from a primary time-series despite using lower-dimensional latent states. Further, although here we specifically focused on modeling Gaussian, Poisson, and Bernoulli observations, our algorithm can be extended to alternate distributions described with generalized-linear models or to other link functions than the ones used here, as long as there exists a corresponding computationally tractable moment conversion equation. This is possible, if a closed-form equation exists, because the covariance-based approach of PGLDM only requires the second-order moments after moment conversion (equations (2), (5), (8), (11), (12)); as such, in these scenarios the moment conversion algorithm can be modified for the desired link function and/or generalized-linear observation model . Beyond neuroscience, due to the high-prevalence of GLDMs across various application domains, our method may be a useful tool for modeling the shared and private dynamics of joint generalized-linear processes with distinct observation distributions.

Figure 3: **In NHP data, PGLDM improves V1 decoding from V2 population spiking activity while maintaining comparable V2 self-prediction performance.** (a) Average cross-validated V1 decoding AUC (shaded areas denote the s.e.m.) for models of different latent dimensions. (b) Same as (a) but visualizing V2 one-step ahead self-prediction AUC. (c) V1 decoding AUC at \(n_{x}=8\). Whiskers correspond to s.e.m. Scatter points are individual trials. (d) Same as (c) but for V2 self-prediction.