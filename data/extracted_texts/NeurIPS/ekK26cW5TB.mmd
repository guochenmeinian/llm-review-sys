# AUCSeg: AUC-oriented Pixel-level Long-tail Semantic Segmentation

Boyu Han\({}^{1,2}\) Qianqian Xu\({}^{1,3}\) Zhiyong Yang\({}^{2}\) Shilong Bao\({}^{2}\)

Peisong Wen\({}^{1,2}\) Yangbangyan Jiang\({}^{2}\) Qingming Huang\({}^{2,1,4}\)

\({}^{1}\) Key Lab. of Intelligent Information Processing, Institute of Computing Technology, CAS

\({}^{2}\) School of Computer Science and Tech., University of Chinese Academy of Sciences

\({}^{3}\) Peng Cheng Laboratory

\({}^{4}\) Key Laboratory of Big Data Mining and Knowledge Management, CAS

{hanboyu23z,xuqianqian,wenpeisong20z}@ict.ac.cn,

{yangzhiyong21,baoshilong,jiangyangbangyan,qmhuang}@ucas.ac.cn

Corresponding authors.

###### Abstract

The _Area Under the ROC Curve (AUC)_ is a well-known metric for evaluating instance-level long-tail learning problems. In the past two decades, many AUC optimization methods have been proposed to improve model performance under long-tail distributions. In this paper, we explore AUC optimization methods in the context of pixel-level long-tail semantic segmentation, a much more complicated scenario. This task introduces two major challenges for AUC optimization techniques. On one hand, AUC optimization in a pixel-level task involves complex coupling across loss terms, with structured inner-image and pairwise inter-image dependencies, complicating theoretical analysis. On the other hand, we find that mini-batch estimation of AUC loss in this case requires a larger batch size, resulting in an unaffordable space complexity. To address these issues, we develop a pixel-level AUC loss function and conduct a dependency-graph-based theoretical analysis of the algorithm's generalization ability. Additionally, we design a _Tail-Classes Memory Bank (T-Memory Bank)_ to manage the significant memory demand. Finally, comprehensive experiments across various benchmarks confirm the effectiveness of our proposed AUCSeg method. The code is available at https://github.com/boyuh/AUCSeg.

## 1 Introduction

Semantic segmentation aims to categorize each pixel within an image into a specific class, which is a fundamental task in image processing and computer vision . Over the past decades, substantial efforts  have advanced the field of semantic segmentation. The mainstream paradigm is to develop innovative network architectures that encode more discriminative features for dense pixel-level classifications. Typical backbones include CNN-based  and newly emerging Transformer-based methods , which have achieved the state-of-the-art (SOTA) performance. Beyond this direction, researchers  have recently realized the _Pixel-level Long-tail issue in Semantic Segmentation (PLSS)_, as shown at the top of Figure 1. Similar to the flaws of traditional long-tail problems, the major classes will dominate the model learning process, causing the model to overlook the segmentation of minority classes in an image. Several remedies have been proposed to alleviate this . For example,  introduces a category-wise variation technique inversely proportional to distribution to achieve balanced segmentation;  introduces a sequence-based generative adversarial network for imbalanced medical image segmentation, and  develops a re-weighting scheme for semi-supervised segmentation.

Currently, mainstream studies fall into two camps. One is to develop carefully designed backbones for long-tail distributions but leave the effect of loss functions unconsidered. The other is to conduct empirical studies on the loss functions without exploring their theoretical impact on the generalization performance. A question then arises naturally:

_Can we find a theoretically grounded loss function for PLSS on top of SOTA backbones?_

This paper provides an affirmative answer from the AUC perspective and proposes a novel framework called _AUC-oriented Pixel-level Long-tail Semantic Segmentation (AUC-Seg)_. Specifically, AUC indicates the likelihood that a positive sample scores higher than a negative one, which has been proven to be **insensitive** to data distribution . Applying AUC to **instance-level long-tail** classifications has shown promising progress in the machine learning community . Motivated by its success, this paper starts an early trial to study AUC optimization for PLSS. The primary concern is to study its effectiveness for PLSS from a theoretical perspective. The key challenge is that the standard techniques for generalization analysis  require the loss function to be expressed as a sum of independent terms. Unfortunately, the proposed loss function does not satisfy this assumption due to the dual effect of structured inner-image dependency and pairwise inter-image dependency. This complicated structure poses a big challenge to understanding its generalization behavior. To address this, we decompose the loss function into inner-image and inter-image terms. On top of this reformulation, we deploy the dependency graph  to decouple the interdependency. Finally, we reach a bound of \(}()\), where \(\) behaves like an indicator for imbalance degree, and \(k\) denotes the number of pixels in each image. This suggests optimizing AUC loss could ensure a promising performance under PLSS.

Back to the practical AUC learning process, we realize that the stochastic gradient optimization (SGD) for structured pixel-level tasks imposes a greater computational burden compared to instance-level long-tail problems. Specifically, the SGD algorithm of AUC requires **at least one sample from each class in each mini-batch**. In light of this, the primary choice is to adopt the so-called stratified sampling on all images  for mini-batch generation (See Equation (7)). Unfortunately, as shown in Figure 3(a) and (b), this is hard to implement under PLSS because **pixel-level labels are densely coupled in each image**. Meanwhile, as shown in Proposition 1, we also argue that directly using random sampling to include all classes would require an extremely large batch size. This leads to unaffordable GPU memory demands for optimization, as described in the experiments in Appendix G.6.

To alleviate this, a novel _Tail-class Memory Bank (T-Memory Bank)_ is carefully designed. The main idea is to identify those missing pixel-level classes in each randomly generated mini-batch and then complete these absences using stored historical class information from the T-Memory Bank. This enables efficient optimization of AUCSeg with a light memory usage, enhancing the scalability of our proposed method, as shown in Figure 1. Finally, comprehensive empirical studies consistently speak to the efficacy of our proposed AUCSeg.

Our main contributions are summarized as follows:

* This paper starts the first attempt to explore the potential of AUC optimization in pixel-level long-tail problems.

Figure 1: Statistic information of pixel number for each class in the Cityscapes training set and the performance of previous methods (DeepLabV3+, HRNet and STDC) compared to our method (AUC-Seg). Our method aims to improve overall performance, particularly for tail classes. The dashed lines represent mIoU values for each class, while the solid lines represent the average mIoU for the head, middle, and tail classes.

* We theoretically demonstrate the generalization performance of AUCSeg in semantic segmentation. To our knowledge, this area remains underexplored in the machine-learning community.
* We introduce a Tail-class Memory Bank to reduce the optimization burden for pixel-level AUC learning.

## 2 Related Work

### Semantic Segmentation

Semantic segmentation is a subtask of computer vision, which has seen significant development since the inception of FCN . The most common framework for semantic segmentation networks is the encoder-decoder. For the encoder, researchers typically use general models such as ResNet  and ResNeXt . As the segmentation tasks become more challenging, some specialized networks have emerged, such as HRNet , ICNet , and multimodal networks [76; 42]. For the decoder, a series of studies focus on strengthening edge features [23; 107], capturing global context [43; 31; 45], and enhancing the receptive field [96; 67; 12; 13]. Recently, the transformer has shown immense potential, surpassing previous methods. A series of methods related to Vision Transformer [108; 83; 65; 15; 74] are proposed. SegNeXt , which is the current _state-of-the-art (SOTA)_ method, possesses the same powerful feature extraction capabilities as the Vision Transformer and the same low computational requirements as CNN. Apart from improving the network, some research [48; 56; 24; 11; 10; 66; 39] is directed toward addressing the issue of class imbalance in semantic segmentation. However, the effectiveness of these methods is not significant. In this paper, we aim to improve the performance of long-tailed semantic segmentation from an AUC optimization perspective.

### AUC Optimization

The development of AUC Optimization can be divided into two periods: the machine learning era and the deep learning era. As a pioneering study,  ushers in the era of AUC in machine learning. It studies the necessity of AUC research, which points out that AUC maximization and error rate minimization are inconsistent. After that, AUC gains significant attention in linear fields such as Logistic Regression  and SVM [46; 47]. Then researchers begin to explore the online [106; 28] and stochastic [94; 63] optimization extensions of the AUC maximization problem. Research from the perspectives of generalization analysis [2; 73; 17] and consistency analysis [1; 29] provides theoretical support for AUC optimization algorithms.  is the first to extend AUC optimization to deep neural networks, ushering in the era of AUC in deep learning. Meanwhile, a series of AUC variants [87; 86; 68; 88; 69; 90] emerge, gradually enriching AUC optimization algorithms. Furthermore, in practice, AUC optimization demonstrates its effectiveness in various class-imbalanced tasks, such as recommendation systems [5; 6; 4; 7], disease prediction [79; 30], domain adaptation , and adversarial training [40; 91].

Despite significant progress, existing studies of AUC optimization mainly pay attention to the instance-level imbalanced classification tasks. This paper starts an early trial to introduce AUC optimization to semantic segmentation. However, due to the high complexity of pixel-level multi-class AUC optimization, such a goal cannot be attained by simply using the current techniques in the AUC community.

## 3 Preliminaries

In this section, we briefly introduce the semantic segmentation task and the AUC optimization problem.

### Semantic Segmentation Training Framework

Let \(=\{(^{i},^{i})_{i=1}^{n}|^{i} ^{H W 3},^{i}\ ^{H W K}\}\) be the training dataset, where \(H\) and \(W\) represent the height and width of the images, and \(K\) denotes the total number of classes. Let \(f_{}\) be a semantic segmentation model (\(\) is the model parameters), which commonly follows an encoder-decoder backbone [75; 105; 108; 83; 32]. Let \(}^{i}=f_{}(^{i})^{H W K}\) be the dense pixel-level prediction, _i.e._,

\[}^{i}=f_{}(^{i})=f_{}^{d}(f_{}^{e}( ^{i})),\] (1)

where the encoder \(f_{}^{e}\) extracts features from the image \(^{i}\), and then the decoder \(f_{}^{d}\) predicts each pixel based on extracted features and outputs a dense segmentation map with the same size as \(^{i}\).

Furthermore, let \(^{i}_{u,v}\) and \(}^{i}_{u,v}\) represent the ground truth and prediction of the \((u,v)\)-th pixel of the \(i\)-th image, respectively. To train the model \(f_{}\), most current studies [14; 83; 65; 83] usually adopt the _cross-entropy (CE)_ loss:

\[_{ce}:=_{i=1}^{n}_{u=0}^{H-1}_{v=0}^{W-1}[- _{c=1}^{K}^{ic}_{u,v}(}^{ic}_{u,v})],\] (2)

where \(^{ic}_{u,v}\) and \(}^{ic}_{u,v}\) are the one-hot encoding of ground truth and the prediction of pixel \((^{i}_{u,v},^{i}_{u,v})\) in class \(c\), respectively.

### AUC Optimization

_Area under the Receiver Operating Characteristic Curve (AUC)_ is a well-known ranking performance metric for **binary classification** task, which measures the probability that a positive instance has a higher score than a negative one :

\[AUC(f_{})=(f_{}(^{+})>f_{}(^{-})|y^{+}=1,y^{-}=0),\] (3)

where \((^{+},y^{+})\) and \((^{-},y^{-})\) represent positive and negative samples, respectively. When \(AUC 1\), it indicates that the classifier can perfectly separate positive and negative samples.

According to [87; 88; 86], given finite datasets, maximizing \(AUC(h_{})\) is usually realized by maximizing its unbiased empirical estimation:

\[AC(h_{})=1-n^{-}}_{i=1}^{n^{+}}_{j=1}^{n^{ -}}(h_{}(^{+})-h_{}(^{-})),\] (4)

where \(\) is a differentiable surrogate loss  measuring the ranking error between two samples, \(n^{+}\) and \(n^{-}\) denote the number of positive and negative samples, respectively.

Moreover, we can directly optimize the following problem for AUC maximization:

\[_{}n^{-}}_{i=1}^{n^{+}}_{j=1}^{n^{-}}(h_ {}(^{+})-h_{}(^{-})).\] (5)

Note that, AUC has achieved significant progress in long-tailed classification [100; 88; 68]. Due to the limitations of space, we refer interested readers to the literature [86; 99] for more introductions to AUC. However, most existing studies merely focus on the **instance-level or image-level** problems. Inspired by its distribution-insensitive property , this paper starts an early trial to introduce AUC to PLSS.

## 4 AUC-Oriented Semantic Segmentation

In this section, we introduce our proposed AUCSeg method for semantic segmentation. A brief overview is provided in Figure 2. AUCSeg is a generic optimization method that can be directly applied to any SOTA backbone for semantic segmentation. Specifically, AUCSeg includes two crucial components: **(1) AUC optimization** where a theoretically grounded loss function is explored for PLSS and **(2) Tail-class Memory Bank**, an effective augmentation scheme to ensure efficient optimization of the proposed AUC loss. In what follows, we will go into more detail about them. For clarity, we include a table of symbol definitions in Appendix A.

### Pixel-level AUC Optimization

Semantic segmentation is a multi-class classification task. Therefore, to apply AUC, we follow a popular multi-class AUC manner, _i.e._, the One vs. One (ovo) strategy [64; 34; 86], which is an average of binary AUC score introduced in Section 3.2. Specifically, on top of the notation of Section 3.1, we further denote \(^{p}=\{(_{u,v}^{i},_{u,v}^{i})|i[1,n],u[0,H -1],v[0,W-1]\}\) as the set of all pixels; the \(j\)-th element (\(j[1,n(H-1)(W-1])\)) in \(^{p}\) is abbreviated as \((_{j}^{p},_{j}^{p})\) for convenience. Given the model prediction \(f_{}=(f_{}^{(1)},,f_{}^{(K)})\), \( c[K]\), \(f_{}^{(c)}\), where \(f_{}^{(c)}\) serves as a continuous score function supporting class \(c\), \(AUC_{seg}^{owo}\) calculates the average of binary AUC scores for every class pair:

\[AUC_{seg}^{ovo}=_{c=1}^{K}_{c c^{}}AUC_{cc^{ }}(f_{}),\] (6)

\[AUC_{cc^{}}(f_{})=(f_{}^{(c)}(_{m}^{p} )>f_{}^{(c)}(_{n}^{p})|_{m}^{p}=c,_{n}^{p} =c^{}).\]

To this end, as introduced in Section 3.2, the goal is to minimize the following unbiased empirical risk:

\[_{auc}:=_{c=1}^{K}_{c^{} c}_{_{m}^{p} _{c}}_{_{n}^{c}_{c^{}}}_{c}||_{c^{}}|}_{sq}^{c,c^{},m,n},\] (7)

where we adopt the widely used square loss \(_{sq}(x)=(1-x)^{2}\) as the surrogate loss ; \(_{sq}^{c,c^{},m,n}:=_{sq}(f_{}^{(c)}(_{m}^{p} )-f_{}^{(c)}(_{n}^{p}))\); \(_{c}=\{_{k}^{p}|_{k}^{p}=c\}\) represents the set of pixels with label \(c\) in the set \(^{p}\), and \(|_{c}|\) denotes the size of the set.

### Generalization Bound

In this section, we explore the theoretical guarantees of the AUC loss function in semantic segmentation tasks and demonstrate that AUCSeg can generalize well to unseen data.

A key challenge is that standard techniques for generalization analysis [62; 8; 21] require the loss function to be expressed as a sum of independent terms. Unfortunately, the proposed loss function does

Figure 2: An overview of AUCSeg.

not satisfy this assumption because there are **two layers of interdependency among the loss terms**. On one hand, semantic segmentation can be considered a structured prediction problem , where couplings between output substructures within a given image create the first layer of interdependency. On the other hand, the AUC loss creates a pairwise coupling between positive and negative pixels, so any pixel pairs sharing the same positive/negative instance are interdependent, resulting in the second layer of interdependency.

We present our main result in the following theorem and the proof is deferred to Appendix B.

**Theorem 1** (Generalization Bound for AUCSeg).: _Let \(_{}[}_{}(f)]\) be the population risk of \(}_{}(f)\). Assume \(\{f:^{H W K}\}\), where \(H\) and \(W\) represent the height and width of the image, and \(K\) represents the number of categories, \(}^{(i)}\) is the risk over \(i\)-th sample, and is \(\)-Lipschitz with respect to the \(l_{}\) norm, (i.e. \(\|}(x)-}(y)\|_{}\|x-y\|_{}\)). There exists three constants \(A>0\), \(B>0\) and \(C>0\), the following generalization bound holds with probability at least \(1-\) over a random draw of i.i.d training data (at the image-level):_

\[|}_{}(f)-_{ }[}_{}(f)]| + }+_{}}{}\] \[+3(}+K}))},\]

_where_

\[_{}=,_{}=2,=(_{c[K]}^{(c)}}{n_{}^{ (c)}})^{2},\]

\(n_{}^{(c)}=_{}n(^{(c)})\)_, \(n_{mean}^{(c)}=_{i=1}^{N}n(_{i}^{(c)})\), \(N=||\), \(k=H W\) and \(^{(c)}\) represents the pixel of class \(c\) in image \(\)._

**Remark 1**.: We achieve a bound of \(}()\), indicating reliable generalization with a large training set. Here, \(\) represents the degree of pixel-level imbalance. More interestingly, even though we have \(k\) classifiers for every single image, the generalization bound only has an algorithm dependent on \(k\), suggesting that pixel-level prediction doesn't hurt generalization too much.

### Tail-class Memory Bank

**Motivation.** Although we have examined the effectiveness of AUC for PLSS from the theoretical point of view, there is **a practical challenge** when conducting AUC optimization for semantic segmentation, as discussed in Section 1. Specifically, the stochastic AUC optimization, as defined in Equation (7), requires **at least one sample from each class** in a mini-batch. In **instance-level AUC optimization**, recent studies [87; 86] often use a stratified sampling technique that generates batches consistent with the original class distribution, as shown in Figure 3(a). Such a strategy will work well when each image belongs to a unique category (say, 'Banana', 'Apple', or 'Lemon') in traditional classifications. Yet it cannot apply to pixel-level cases because each sample involves multiple and coupled labels, making it hard to split them for stratified sampling, as illustrated in Figure 3(b). Meanwhile, we also provide a bound (Proposition 1) to show that simply adopting random sampling will suffer from an overlarge batch size \(B\), making an unaffordable GPU memory burden.

**Proposition 1**.: _Consider a dataset \(\) that includes images with \(K\) different pixel categories. Let \(p_{i}\) represent the probability of observing a pixel with label \(i\) in a given image. Randomly select \(B\) images from \(\) as training data, where_

\[B=(p_{i})}).\]

_Then with probability at least \(1-\), for any \(c[K]\), there exists \(\) in the training data that contains pixels of label \(c\)._

**Remark 2**.: The proof is deferred to Appendix C. Proposition 1 suggests that the value of \(B\) is inversely proportional to \(_{i}p_{i}\). Note that \(p_{i}\) will be smaller as the long-tail degree becomes more severe, leading to a larger \(B\). For example, in terms of the Cityscapes dataset with \(K=19\) classes, assuming \(=0.01\) and \(_{i}p_{i}=1\%\), \(B\) should be at least \(759\) to guarantee that each class of pixels appears at least once with a high probability. This results in a significant strain on GPU memory.

To address this, considering that the tail-class samples generally have less opportunity to be included in a mini-batch and are often more crucial for final performance, we thus develop a novel Tail-class Memory Bank (T-Memory Bank) to efficiently optimize Equation (7) and manage GPU usage effectively. As depicted in Figure 3(c), the high-level ideas of the T-Memory Bank are as follows: 1) identify missing tail classes of all images involved in a mini-batch and 2) randomly replace some pixels in the image with missing classes based on stored historical class information in T-Memory Bank. In this sense, we can obtain an approximated batch-version of Equation (7),,

\[_{auc}:=_{c=1}^{K}_{c^{} \\ n_{c}_{r}}_{_{m}^{p}_{c}_{c}\\ _{c}^{p}_{c^{}}_{c^{}} }_{c}||_{c^{}}|}_{sq}^{c,c^{},m,n}\] (8)

where \(_{c}\) and \(_{c}\) represent the set of pixels with label \(c\) in the original image and those pixels stored in the T-Memory Bank, respectively; \(_{sq}^{c,c^{},m,n}:=_{sq}(f_{}^{(c)}(}_{m}^{p})-f_{}^{(c)}(}_{n}^{p}))\); \(}^{p}\) represents the sample after replacing some pixels with tail classes pixels from the T-Memory Bank.

**Detailed Components.** As shown in Figure 2, T-Memory Bank comprises three main parts: **(1) Memory Branch** stores a set with \(S_{M}\) (the Memory Size) images for each tail class. We define the set as \(=\{_{c_{1}},,_{c_{n_{t}}}\}\), where \(_{t}=\{c_{i}\}_{i=1}^{n_{t}}\) denotes the labels of tail classes, and \(n_{t}\) is the total number of selected tail classes; **(2) Retrieve Branch** selects pixels from the Memory Branch to supplement the missing tail classes and **(3) Store Branch** updates the Memory Branch whenever a new image arrives. Algorithm 1 summarizes a short version of AUCSeg equipped with T-Memory Bank. **Please refer to the detailed version in Appendix D.** Note that we introduce CE loss as a regularization term for our proposed AUCSeg, which is widely used in the AUC community  to pursue robust feature learning. Experiments demonstrate that the performance is insensitive to the regularization weight \(\), as shown in Figure 4(d).

At the start of training, the Memory Branch is empty. In this case, we only calculate the loss function \(\) for the classes present in the mini-batch, while the Retrieve Branch will not take any action. Meanwhile, the Store Branch will continuously append pixel data of tail classes to the Memory Branch. As the Memory Branch reaches its maximum capacity \(S_{M}\), we adopt a random replacement strategy to update the Store Branch (Lines \(5\) to \(6\) in Algorithm 1 or Lines \(6\) to \(11\) in Algorithm 2).

As the training process progresses, if the Memory Branch is not empty, the Retrieve Branch kicks in to count the missing classes in each image of the mini-batch, denoted as \(_{miss}\). It then calculates the number of classes needed to be added for optimization, \(n_{sample}=|_{miss}| R_{S}\). Here, we introduce a tunable sample ratio \(R_{S}\) to strike a trade-off between the original and missing tail-class semantic information. Finally, it uniformly retrieves the corresponding pixels of \(n_{sample}\) missing classes from the Memory Branch, resizes them by the resize ratio \(R_{R}\), and randomly selects positions to overwrite (Lines \(7\) to \(8\) in Algorithm 1 or Lines \(13\) to \(18\) in Algorithm 2).

Figure 3: Instance-level and pixel-level task sampling.

Discussions.We recognize that Memory Bank [82; 36] has achieved great success in deep learning. However, our T-Memory Bank behaves differently compared to earlier studies. **The key difference is that the Memory Bank and T-Memory Bank are designed for different tasks.** The goal of the previous Memory Bank is to facilitate the traditional classifications by storing instance-level or image-level features, while our T-Memory Bank specifically stores the original pixels for each object. This strategy is particularly beneficial for our PLSS task. Additionally, the T-Memory Bank enables AUCSg without substantially increasing GPU workload as well as the number of samples per mini-batch by selectively replacing non-essential pixels. Our experiments, detailed in Appendix G.6, include a comparison of GPU overhead. For more discussion on the T-Memory Bank, please refer to Appendix E.

## 5 Experiments

In this section, we describe some details of the experiments and present our results. **Due to space limitations, please refer to Appendix F, Appendix G and Appendix H for an extended version.**

### Experimental Setups

The experiment includes three benchmark datasets: Cityscapes , ADE20K , and COCO-Stuff 164K . We use SegNeXt  as the backbone for our model and the _mean of Intersection over Union (mIoU)_ as the evaluation metric. We compare our method with \(13\) **recent advancements and \(6\) long-tail approaches** in semantic segmentation. All the long-tail methods also use SegNeXt as the backbone. To ensure fairness, we re-implement the listed methods using their publicly shared code and test them on the same hardware. Detailed introductions are deferred to Appendix F.

### Overall Performance

Table 1 shows the quantitative performance comparisons. We draw the following conclusions: First, most current algorithms perform poorly in long-tail scenarios. Specifically, performance drops sharply from head to tail classes. For instance, the performance gap for PointRend and OCRNet on the Cityscapes reaches up to \(40\%\). Second, models using long-tail approaches generally achieve better results than those that do not. However, they still fail to produce satisfactory outcomes. One possible reason is that these long-tail approaches focus on reweighting, giving too much attention to the tail classes and leading to overfitting. Additionally, our proposed AUCSeg method surpasses all competitors in most metrics. This success is due to the appealing properties of AUC. Our method consistently outperforms the runner-up by \(+1.21\%\), \(+0.75\%\), and \(+0.38\%\) in tail classes mIoU across the datasets. Overall mIoU also improves by \(+1.05\%\), \(+0.27\%\), and \(+0.31\%\). In some Head/Middle

metrics, AUCSeg does not achieve the best performance. Even in these cases, AUCSeg still secures the runner-up status. We analyze the performance trade-off between head and tail classes in Appendix H. These experimental results underscore the effectiveness of our proposed method. We further present the results for each tail class in Appendix G.1, and analyze the reasons for the varying performance improvements of tail classes across the three datasets in Appendix G.2.

Figure 4 displays the qualitative results on the Cityscapes validation set. Benefiting from our proposed AUC and T-Memory Bank techniques, AUCSeg segments objects in tail classes more accurately. It correctly distinguishes between bicycles and motorcycles and successfully identifies distant traffic lights, which other methods overlook. More qualitative results can be found in Appendix G.3.

### Backbone Extension

In Section 5.1, we select the current SOTA SegNeXt as the backbone. Nevertheless, AUCSeg can also adapt to other backbones, consistently delivering effective results. Table 2 presents the experimental results of AUCSeg when using DeepLabV3+, EMANet, OCRNet, and ISANet as backbones. These results reveal significant improvements in both overall mIoU and tail classes mIoU with AUCSeg. Notably, on ISANet, the increases are \(5.54\%\) and \(6.49\%\). Moreover, AUCSeg enhances performance across various model sizes and different pixel-level long-tail problems, as detailed in Appendix G.4 and Appendix G.4. **This demonstrates the superiority of our proposed AUCSeg for long-tailed semantic segmentation.**

### Ablation studies

We perform several ablation studies to test the effectiveness of different modules and hyperparameters. All experiments are conducted on the **ADE20K** validation set.

    &  &  &  &  \\  & **Overall** & **Head** & **Middle** & **Tail** & **Overall** & **Head** & **Middle** & **Tail** & **Overall** & **Head** & **Middle** & **Tail** \\  DeepLabV3+  & 31.95 & 75.88 & 51.96 & 26.01 & 66.53 & 90.11 & 57.16 & 54.36 & 29.11 & 51.11 & 32.93 & 24.82 \\ EncNet  & 32.12 & 75.34 & 51.60 & 26.32 & 71.34 & 91.62 & 60.76 & 63.03 & 27.31 & 49.89 & 30.41 & 23.09 \\ FastFCN  & 29.78 & 74.20 & 49.44 & 23.86 & 63.97 & 90.37 & 52.43 & 51.22 & 28.37 & 50.60 & 32.52 & 23.96 \\ EMANet  & 32.83 & 75.77 & 50.03 & 27.36 & 70.93 & 91.69 & 60.61 & 61.97 & 28.48 & 49.73 & 29.97 & 24.85 \\ DANet  & 33.83 & 74.62 & 51.01 & 28.52 & 67.57 & 89.66 & 55.30 & 54.26 & 26.83 & 49.60 & 31.14 & 22.29 \\ HRNet  & 31.83 & 75.35 & 49.98 & 26.19 & 73.40 & 91.98 & 65.79 & 64.00 & 28.65 & 48.00 & 30.74 & 25.16 \\ OCRNet  & 29.64 & 74.00 & 49.40 & 23.72 & 69.65 & 90.24 & 63.18 & 50.21 & 28.67 & 51.04 & 32.41 & 24.33 \\ DNLNet  & 33.24 & 75.90 & 51.16 & 27.69 & 76.08 & 91.98 & 59.90 & 61.66 & 30.23 & 50.71 & 33.05 & 26.41 \\ PointRend  & 17.77 & 61.18 & 37.60 & 11.46 & 60.67 & 89.79 & 53.92 & 41.49 & 11.17 & 21.17 & 13.64 & 9.04 \\ BiSeNetV2  & 10.26 & 60.38 & 28.72 & 4.10 & 73.04 & 92.00 & 63.52 & 64.93 & 10.30 & 34.96 & 12.71 & 5.92 \\ ISANet  & 29.53 & 74.34 & 48.77 & 23.64 & 70.63 & 91.67 & 61.50 & 60.43 & 26.37 & 48.87 & 30.78 & 21.86 \\ STDC  & 30.17 & 73.36 & 48.02 & 24.58 & 76.30 & 92.58 & 65.09 & 71.94 & 29.83 & 51.74 & 33.40 & 25.61 \\ SegNeXt  & 47.45 & 80.54 & **60.35** & 43.28 & 82.41 & **94.08** & 72.46 & 80.92 & 42.42 & **57.05** & 41.71 & 40.33 \\  VS  & 24.72 & 75.30 & 48.02 & 17.86 & 55.40 & 92.16 & 52.52 & 26.36 & 24.27 & 47.80 & 30.38 & 19.19 \\ LA  & 31.16 & 77.07 & 53.43 & 24.77 & 62.75 & 92.98 & 64.79 & 35.09 & 28.56 & 49.67 & 33.16 & 24.21 \\ LDAN  & 33.11 & 74.06 & 51.26 & 27.65 & 65.95 & 92.72 & 69.27 & 40.17 & 42.39 & 56.85 & 41.59 & 40.34 \\ Focal Loss  & 47.68 & 80.54 & 59.04 & 43.73 & 82.44 & 93.90 & **72.79** & 80.89 & 41.98 & 56.87 & 41.51 & 39.79 \\ DisAlign  & 48.15 & 80.33 & 59.14 & 44.31 & 81.94 & 93.61 & 72.12 & 80.36 & 42.10 & 55.20 & 41.24 & 40.28 \\ BLV  & 46.76 & 79.96 & 58.96 & 42.67 & 81.81 & 93.84 & 71.83 & 80.05 & 42.17 & 56.83 & 41.52 & 40.06 \\  AUCSeg (Ours) & **49.20** & **80.59** & **59.45** & **45.52** & **82.71** & **93.91** & **72.72** & **81.67** & **42.73** & **56.95** & **41.93** & **40.72** \\   

Table 1: Quantitative results on **Cityscapes, ADE20K** and **COCO-Stuff 164K** val set in terms of mIoU (%). The champion and the runner-up are highlighted in **bold** and underline.

  
**Backbone** & **AUCSeg** & **Overall** & **Tail** \\   & \(\) & 31.95 & 26.01 \\  & \(\) & **36.13** & **31.10** \\   & \(\) & 32.83 & 27.36 \\  & \(\) & **36.32** & **31.39** \\   & \(\) & 29.64 & 23.72 \\  & \(\) & **34.82** & **29.75** \\   & \(\) & 29.53 & 23.64 \\  & \(\) & **35.07** & **30.13** \\   

Table 2: Results of AUCSeg using different backbones in terms of mIoU (%).

**The Effectiveness of AUC Optimization and T-Memory Bank.** Table 3 details our step-by-step ablation study on the AUC and T-Memory Bank components of AUCSeg. Compared to the baseline SegNeXt, AUC enhances performance by \(+1.01\%\) and \(+1.42\%\) in overall and tail classes. The T-Memory Bank further addresses the imbalance issue, yielding improvements of \(+1.75\%\) overall and \(+2.24\%\) in tail classes. Our results also show that employing T-Memory Bank without AUC yields no significant improvements, underscoring the necessity of AUC optimization and TMB are deferred to Appendix G.6, G.7, G.8, and G.9.

**Ablation Study on Hyper-Parameters.** Figure 4(a) ablates the maximum number of images stored per class in the Memory Branch, referred to as Memory Size (\(S_{M}\)). For the ADE20K dataset, optimal performance occurs when \(S_{M}=5\), and performance shows little sensitivity to changes in \(S_{M}\). Figure 4(b) ablates the Sample Ratio (\(R_{S}\)), the fraction of classes sampled from the Memory Branch relative to the total number of missing tail classes. Figure 4(c) ablates the Resize Ratio (\(R_{R}\)), the scaling factor for the sampled pixels. For ADE20K, the best result is obtained when \(R_{S}=0.05\) and \(R_{R}=0.4\). A potential reason for their small value is that the original image is overwritten when \(R_{S}\) and \(R_{R}\) are too large, resulting in poor training performance. Figure 4(d) ablates the weight \(\) for \(_{ce}\) and \(_{ave}\), with \(=\) providing slightly better results. The influence of \(\) on performance is minimal. Detailed results from this hyper-parameter ablation study are available in Appendix G.10 and G.11.

## 6 Conclusion

This paper explores AUC optimization in the context of PLSS tasks. To begin with, we theoretically study the generalization performance of AUC-oriented PLSS by overcoming the two-layer coupling issue across the loss terms of AUCSeg therein. The corresponding results show that applying AUC optimization to PLSS could also enjoy a promising performance. Subsequently, we propose a novel T-Memory Bank to reduce the significant memory demand for the mini-batch optimization of AUCSeg. Finally, comprehensive experiments suggest the effectiveness of our proposed AUCSeg.

Figure 4: Qualitative results on the **Cityscapes** val set. Red rectangles highlight and magnify the image details in the lower left corner.

Figure 5: Ablation Study on Hyper-Parameters.

  
**Model** & **AUC** & **TMB** & **Overall** & **Tail** \\  SegNeXt & & & 47.45 & 43.28 \\ SegNeXt+AUC & ✓ & & 48.46 & 44.70 \\ SegNeXt+TMB & & ✓ & 47.86 & 43.86 \\ AUCSeg & ✓ & ✓ & **49.20** & **45.52** \\   

Table 3: Ablation study on the effectiveness of AUC Optimization and T-Memory Bank (TMB) in terms of mIoU (%).