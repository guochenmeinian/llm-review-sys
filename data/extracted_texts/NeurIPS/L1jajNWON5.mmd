# CondTSF: One-line Plugin of Dataset Condensation

for Time Series Forecasting

 Jianrong Ding1,2\({}^{*}\), Zhanyu Liu1\({}^{*}\), Guanjie Zheng1\({}^{}\), Haiming Jin1, Linghe Kong1

1 School of Electronic Information and Electrical Engineering, Shanghai Jiao Tong University

2 Zhiyuan College, Shanghai Jiao Tong University

{rafaelding,zhyliu00,gjzheng,jinhaiming,linghe.kong}@sjtu.edu.cn

###### Abstract

_Dataset condensation_ is a newborn technique that generates a small dataset that can be used in training deep neural networks (DNNs) to lower storage and training costs. The objective of dataset condensation is to ensure that the model trained with the synthetic dataset can perform comparably to the model trained with full datasets. However, existing methods predominantly concentrate on classification tasks, posing challenges in their adaptation to time series forecasting (TS-forecasting). This challenge arises from disparities in the evaluation of synthetic data. In classification, the synthetic data is considered well-distilled if the model trained with the full dataset and the model trained with the synthetic dataset yield identical labels for the same input, regardless of variations in output logits distribution. Conversely, in TS-forecasting, the effectiveness of synthetic data distillation is determined by the distance between predictions of the two models. The synthetic data is deemed well-distilled only when all data points within the predictions are similar. Consequently, TS-forecasting has a more rigorous evaluation methodology compared to classification. To mitigate this gap, we theoretically analyze the optimization objective of dataset condensation for TS-forecasting and propose a new one-line plugin of dataset condensation for TS-forecasting designated as Dataset **Cond**ensation for **T**ime **S**eries **F**orecasting (CondTSF) based on our analysis. Plugging CondTSF into previous dataset condensation methods facilitates a reduction in the distance between the predictions of the model trained with the full dataset and the model trained with the synthetic dataset, thereby enhancing performance. We conduct extensive experiments on eight commonly used time series datasets. CondTSF consistently improves the performance of all previous dataset condensation methods across all datasets, particularly at low condensing ratios.

## 1 Introduction

Dataset condensation is a strategy for mitigating the computational demands of training large models on extensive datasets. It is pointed out by previous works that building foundation models on time series forecasting (TS-forecasting) have become a hot topic. However, fine-tuning these large models using full time series datasets can entail considerable computational overhead. Hence, the employment of dataset condensation techniques becomes imperative. In recent years, various methods have been proposed in the field of dataset condensation, such as matching-based methods and kernel methods. To date, dataset condensation methods have achieved success in classification tasks, including image classification, graph classification and time series classification.

However, directly applying these dataset condensation methods designed for classification to the domain of time series forecasting (TS-forecasting) results in performance degradation. The objective of dataset condensation is to generate a synthetic dataset so that when the model \(_{s}\) trained with the synthetic dataset and the model \(_{f}\) trained with the full dataset are given identical input, the two models output **similar predictions**. However, the concept of **similar prediction** differs between classification and TS-forecasting. In classification, as shown in Fig.1(a), predictions are considered similar if \(_{s}\) and \(_{f}\) assign the same class label, irrespective of differences in the distribution of output logits. Conversely, in TS-forecasting, as illustrated in Fig.1(b), the similarity of predictions from \(_{s}\) and \(_{f}\) is indicated by the mean squared distance of the predictions. The predictions are deemed similar only when all data points within the predictions are similar. This distinction in evaluation indicates TS-forecasting imposes more stringent criteria in discerning **similar predictions** compared to classification. It poses a challenge that previous dataset condensation methods based on classification fail to provide adequate assurance for the similarity between predictions of \(_{s}\) and \(_{f}\) within the realm of TS-forecasting.

To mitigate the gap, we propose a novel one-line dataset condensation plugin designed specifically for TS-forecasting called **Condensation** for **T**ime **S**eries **F**orecasting (CondTSF) based on our theoretical analysis. We first formulate the optimization objective of dataset condensation for TS-forecasting. Then we transform the original optimization objective into minimizing the distance between predictions of \(_{s}\) and \(_{f}\). Furthermore, to minimize the distance between predictions of \(_{s}\) and \(_{f}\), we decompose the task into minimizing two terms, namely **gradient term** and **value term**. We theoretically prove that plugging CondTSF into previous methods can minimize the **value term** and **gradient term** synchronously. Therefore, CondTSF serves as an effective plugin to boost the performance of dataset condensation for TS-forecasting. As depicted in Fig.1(c), plugging CondTSF into previous methods yields a significant enhancement in performance.

In short, our contributions can be summarized as follows.

* To the best of our knowledge, we are the first to explore dataset condensation for TS-forecasting. We conduct a theoretical analysis of the optimization objective of dataset condensation for TS-forecasting, breaking it down into two optimizable terms to facilitate improved optimization.
* Leveraging insights from our theoretical analysis of TS-forecasting, we propose a simple yet effective dataset condensation plugin CondTSF. Plugging CondTSF into existing methods enables synchronous optimization of the two terms, leading to performance enhancement.
* We conduct extensive experiments on eight widely used time series datasets to prove the effectiveness of CondTSF. CondTSF notably improves the performance of all previous dataset condensation methods across all datasets, particularly under low condensing ratios.

## 2 Related Works

**Time Series Forecasting:** Time series forecasting (TS-forecasting) is the task of using historical, time-stamped data to predict future values. Previous works utilize different methods to achieve better

Figure 1: **Left:** Difference between evaluation of dataset condensation for classification tasks and time series forecasting tasks. **Right:** Comparison in performance of previous methods with and without CondTSF.

performance. These models can be mainly categorized into 3 types. **(1)** Transformer-based Models: Transformer have shown great success in natural language processing, and models based on transformers[53; 42; 24; 54] emerged in TS-forecasting fields. **(2)** MLP-based Models: Efforts to use MLP-based models have been put into TS-forecasting in recent years since DLinear triumph transformer-based models with a simple MLP structure. **(3)** Patch-based Models: These models[34; 48; 28; 29] focused on learning representation cross patches instead of learning attention at each time point. Therefore they used a patching strategy before feeding the data to transformers.

**Dataset Condensation:** Dataset condensation is a task that aims at distilling a large dataset into a smaller one so that when a model is trained on the small synthetic dataset and the full dataset separately, the testing performances of the trained models are similar. Previous works related to dataset condensation can be divided into 3 classes below. **(1)** Coreset Selecting Methods: These methods aim at selecting data with representative features from source dataset to construct a synthetic dataset[1; 4; 12; 37; 39]. **(2)** Matching-based Methods: These methods aim at minimizing a specific metric surrogate model learned from source dataset and synthetic dataset. The defined metrics are different, including gradient[51; 18; 46], features from the same class, distribution of synthetic data[50; 52] and training trajectories[3; 5; 7; 11; 8]. **(3)** Kernel-based Methods: These methods aim at obtaining a closed-form solution for the optimization problem utilizing kernel ridge-regression[20; 33]. In this way, the bi-level optimization problem of dataset condensation is reduced to a single-level optimization problem. Based on these results, the following works have made significant progress in different areas, including decreasing training cost and time, improving performance[30; 31].

## 3 Preliminaries

**Dataset Condensation for TS-forecasting** Given a time series dataset, we split the dataset into a train set and a test set. In this paper, we denote the train set as \(\) and the test set as \(\). We denote the synthetic dataset as \(\). The synthetic dataset \(\) is a small dataset distilled from the full train set \(\). Train set \(\), test set \(\), and synthetic dataset \(\) are all vectors. We employ \(_{}\) as a neural network parameterized by \(\). Without losing generality, we suppose the model \(_{}\) is using historical sequence \(_{t:t+m}\) with length \(m\) to predict future sequence \(_{t+m:t+m+n}\) with length \(n\). Given the test set \(\), we formulate the test error of \(_{}\) as the error between the prediction of \(_{}\) on test input \(_{t:t+m}\) and the test label \(_{t+m:t+m+n}\), as shown in Eq.1.

\[_{test}(_{},)_{t}||_{}(_{t:t+m})-_{t+m:t+m+n}||^{2}\] (1)

During **dataset condensation process**, a distribution of initial model parameters \(P_{}\) is available for training model parameter sampling, and the full train set \(\) is available for condensation. Subsequently, a synthetic dataset \(\) is distilled from the full train set \(\) using dataset condensation methods. During **testing process**, initial testing model parameter \(_{0,test}\) is sampled from \(P_{}\). Since \(_{0,test}\) is sampled in the testing process, it's unavailable during the previous dataset condensation process. Then model parameters \(_{s,test}\) and \(_{f,test}\) are obtained by training initial testing parameter \(_{0,test}\) on synthetic dataset \(\) and the full train set \(\) respectively. The objective of dataset condensation is to ensure model \(_{_{s,test}}\) and \(_{_{f,test}}\) have comparable performance on test set \(\). Therefore the practical optimization objective is to ensure that model \(_{_{s,test}}\) trained with synthetic dataset \(\) minimizes the test error \(_{test}\) on test set \(\). The optimization objective is formulated as Eq.2.

\[_{}_{test}(_{_{s,test}},)\] (2)

## 4 Method

Since test set \(\) is not available during the dataset condensation process, the original optimization objective for dataset condensation in Eq.2 is non-optimizable. To mitigate this gap, in the following sections, we transform the non-optimizable objective into two distinct optimizable terms. Then we develop methods to optimize the two terms, thereby indirectly optimizing the original objective.

### Decomposition

In this section, we decompose the optimization objective of dataset condensation in Eq.2 into two optimizable terms for better optimization. In the testing process, the initial testing model parameter \(_{0,test}\) is sampled from a distribution of initial model parameters \(P_{}\). Then we train \(_{0,test}\) on the synthetic dataset \(\) to get model parameter \(_{s,test}\), and train \(_{0,test}\) on the full train set \(\) to get model parameter \(_{f,test}\). Given test dataset \(\), the optimization objective is formulated as Eq.3.

\[}{}_{test}(_{_{s,test}},)\\ \ \ _{test}(_{_{s,test}},)=_{t}||_{_{s,test}}(_{t:t+m})-_{t+m:t+m+n}|| ^{2}\] (3)

Meanwhile, there is a non-optimizable error \(\) between the prediction of model \(_{_{f,test}}\) and the true label from the test dataset, which is formulated in Eq.4.

\[_{t+m:t+m+n}=_{_{f,test}}(_{t:t+m})+\] (4)

Then we decompose the upper bound of \(_{test}(_{_{s,test}},)\) into two terms, as shown in Thm.1. We utilize Taylor Expansion in the proof of Thm.1. For each real test data \(x_{t:t+m}\), we can arbitrarily choose position \(t^{}\) and get synthetic data \(s_{t^{}:t^{}+m}\). Then we can perform Taylor Expansion with \(s_{t^{}:t^{}+m}\) to obtain the value of \(_{_{s,test}}(_{t:t+m})\) and \(_{_{f,test}}(_{t:t+m})\).

**Theorem 1**.: _Given arbitrary synthetic data \(_{t^{}:t^{}+m}\), the upper bound of the optimization objective of dataset condensation \(_{test}(_{_{s,test}},)\) can be formulated as such_

\[_{test}(_{_{s,test}}, )&_{t}||||^{2}+_{_{s,test}}(_{t^{}:t^{}+m})-_{_{f,test}}( _{t^{}:t^{}+m})||^{2}}_{}\\ &+_{_{s,test}}(_{t^{ }:t^{}+m})-_{_{f,test}}(_{t^{}: t^{}+m}))^{}(_{t:t+m}-_{t^{}:t^{}+m})||^{2}}_{ }\] (5)

To prove Thm.1, we use linear models for further analysis since linear models can be both effective and efficient in TS-forecasting. Given a linear model \(_{}()=\), its second and higher order gradient is zero. Therefore first-order Taylor Expansion is sufficient to obtain the accurate prediction of the model. Meanwhile, if \(_{}\) is a non-linear model, we ignore the higher-order terms of Taylor Expansion. We prove Thm.1 by applying the property of the first-order Taylor Expansion and triangular inequality of norm functions. The complete proof is in App.A.1. Hence we decompose the optimization objective of dataset condensation for TS-forecasting into two optimizable terms, namely **value term** and **gradient term**. For **value term**, it ensures \(_{_{s,test}}\) and \(_{_{f,test}}\) are similar in prediction values. For **gradient term**, it ensures the predictions of \(_{_{f,test}}\) and \(_{_{f,test}}\) are similar in gradient. Optimizing these two terms can optimize the upper bound of the original optimization objective, and therefore indirectly optimize the original optimization objective in Eq.3.

### Gradient Term Optimization

We develop a method to optimize **gradient term** in this section. Given a linear model \(_{}()=\), its gradient on input is \(_{}()=^{}\). It indicates that the gradient of a linear model on input is the parameter of the model. We apply Cauchy-Schwarz Inequality to the gradient term and get its upper bound. We reformulate the gradient term and get its upper bound as shown in Eq.6.

\[&||(_{_{s,test}}(_{t^{ }:t^{}+m})-_{_{f,test}}(_{t^{}:t ^{}+m}))^{}(_{t:t+m}-_{t^{}:t^{}+m})||^{2} \\ =&||(_{s,test}-_{f,test})(_{t:t+m} -_{t^{}:t^{}+m})||^{2}\\ &||_{s,test}-_{f,test}||^{2}|| _{t:t+m}-_{t^{}:t^{}+m}||^{2}\] (6)

Figure 2: Complete process of dataset condensation using CondTSF.

Since test data \(_{t:t+m}\) is not available during the dataset condensation process, the distance between synthetic data and test data \(||_{t:t+m}-_{t^{}:t^{}+m}||^{2}\) is not optimizable. Therefore we only need to optimize the distance between parameters \(||_{s,test}-_{f,test}||^{2}\). All previous dataset condensation methods based on parameter matching can minimize this distance. Here we utilize MTT as an example to clarify the optimization process. The optimization objective of trajectory matching is

\[_{}-_{s,test}||^{2}}{||_{f,test}- _{0,test}||^{2}}\] (7)

However, since \(_{s,test}\) and \(_{f,test}\) are trained from testing initial parameter \(_{0,test} P_{}\), they are not available during dataset condensation process. Therefore, in practice, we sample \(_{0}^{0},,_{0}^{k} P_{}\) as initial parameters during dataset condensation process. The initial parameters are trained on synthetic dataset \(\) and full train set \(\) respectively to get \(_{s}^{0},,_{s}^{k}\) and \(_{f}^{0},,_{f}^{k}\). Then we substitute \(_{s,test}\), \(_{f,test}\) and \(_{0,test}\) in Eq.7 with parameters sampled in dataset condensation, making the optimization objective optimizable. The practical optimization objective is shown in Eq.8.

\[_{}_{i=0}^{k}^{i}-_{s}^{i}||^{2}}{|| _{f}^{i}-_{0}^{i}||^{2}}\] (8)

In practice, \(_{0}^{0},,_{0}^{k}\) and \(_{f}^{0},,_{f}^{k}\) are sampled, trained, and stored in a parameter buffer before dataset condensation process. It can be concluded that using trajectory matching methods is intuitively minimizing the distance between \(_{s}^{i}\) and \(_{f}^{i}\) for all initial parameters \(_{0}^{i} P_{}\). By minimizing the upper bound of the gradient term, trajectory matching methods indirectly optimize the gradient term.

### Value Term Optimization

We develop an optimization objective to optimize the **value term** in this section. Since \(_{f,test}\) is trained from \(_{0,test}\), it's unavailable in dataset condensation process. To mitigate this gap, we prove that although \(_{f,test}\) is unavailable in dataset condensation process, its prediction \(_{_{f,test}}(_{t^{}:t^{}+m})\) is still available. To prove this statement, we sample initial model parameters \(_{0}^{0},,_{0}^{k}\) from \(P_{}\). Then \(_{0}^{0},,_{0}^{k}\) are all trained with the same full train set \(\). After training, we get parameters \(_{f}^{0},,_{f}^{k}\). It is observed that models \(_{_{f}^{0}},,_{_{f}^{k}}\) predict similarly given arbitrary synthetic data \(_{t^{}:t^{}+m}\) as input.

Since initial testing parameter \(_{0,test}\) is also sampled from the same distribution \(P_{}\) and \(_{f,test}\) is trained from \(_{0,test}\) using the same full train set \(\), the prediction of \(_{_{f},test}\) is similar to predictions of an arbitrary expert model \(_{_{f}^{i}}\). The conclusion is formulated in Eq.9.

\[_{_{f},test}(_{t^{}:t^{}+m})_{_{f}^{0}}(_{t^{}:t^{}+m})_{ _{f}^{1}}(_{t^{}:t^{}+m})_{_{f}^{k}}(_{t^{}:t^{}+m})\] (9)

Figure 3: Given the same synthetic data as input, all expert models trained on the full train set \(\) provide similar predictions. The initial parameters of the models are sampled from the same distribution \(P_{}\). The visualization of this figure utilized MDS algorithm for dimension reduction.

Experiments have proved Eq.9 in Fig.3. As shown in Fig.3, for each synthetic data input \(_{t^{}:t^{}+m}\) (orange points), the predictions of corresponding expert models (yellow and blue points) are similar. Therefore, although \(_{f,test}\) is unavailable in the dataset condensation process, its prediction \(_{_{f,test}}(_{t^{}:t^{}+m})\) can be obtained using the prediction of an arbitrary expert model \(_{^{i}_{f}}(_{t^{}:t^{}+m})\). Now we reformulate the value term and transform it into a practical optimization objective. Firstly, We formulate the upper bound of the value term as shown in Thm.2.

**Theorem 2**.: _The upper bound of the value term can be formulated as such_

\[||_{_{s,test}}(_{t^{}:t^{}+m})-_ {_{f,test}}(_{t^{}:t^{}+m})||^{2} 2_{t^{ }}||_{_{f,test}}(_{t^{}:t^{}+m})- {s}_{t^{}+m:t^{}+m+n}||^{2}\] (10)

We prove Thm.2 by utilizing the triangular inequality and the prediction optimality of \(_{s,test}\) on synthetic data \(\). The complete proof is in App.A.2. According to Thm.2, we obtain an optimizable upper bound of the value term. Therefore the optimization objective for the value term can be naturally defined as minimizing the upper bound of the value term, as shown in Eq.11.

\[_{}_{label}\ \ \ \ _{label}=_{t^{ }}||_{_{f,test}}(_{t^{}:t^{}+m})- {s}_{t^{}+m:t^{}+m+n}||^{2}\] (11)

According to Thm.2, label error \(_{label}\) is the upper bound of the value term. Therefore, by minimizing the upper bound of the value term, the value term is indirectly minimized.

### CondTSF

In this section, we develop a one-line plugin called CondTSF to minimize the label error \(_{label}\) in Eq.11 so that the **value term** can be optimized. CondTSF is a lightweight one-line plugin, no backpropagation or gradient is required during the update. CondTSF utilizes a simple yet effective additive method to iteratively update the synthetic data \(\) and minimize the label error \(_{label}\). In TS-forecasting, when generating training data, the data is usually sampled overlap from the dataset. Inspired by the overlap property, we utilize an additive method in CondTSF to gradually update the synthetic data to avoid vibrations. In the \(i_{}\) update iteration, CondTSF uses the prediction of expert model \(_{f,test}(_{t^{}:t^{}+m})\) to update synthetic label \(_{t^{}+m:t^{}+m+n}\). The update process is shown in Eq.12.

\[_{t^{}+m:t^{}+m+n}^{(i+1)}=(1-)_{t^{} +m:t^{}+m+n}^{(i)}+_{_{f,test}}(_{t^{ }:t^{}+m}^{(i)})\] (12)

[MISSING_PAGE_FAIL:7]

set the length of the synthetic dataset as 48, as shown in Table.2. Each synthetic dataset can only generate one training pair. We conduct experiments with two larger distill ratios as shown in App.B.

**Model Settings:** We plug CondTSF into existing dataset condensation models based on parameter matching, including DC, MTT, PP, TESLA, FTD and DATM to prove the effectiveness of CondTSF. We also conduct experiments on non-parameter-matching based methods, including DM, IDM, KIP, FRePo to prove that optimizing value term only also helps boost the performance. The experiment setting and results are shown in App.E. We use DLinear as the expert model to perform dataset condensation since DLinear is a linear model.

**Metric Settings:** The source dataset is first divided into a train set and a test set. All synthetic data is initialized by randomly sampling data from the train set. After a synthetic dataset is finished distilling, it is used to train another five models. After the five models are trained, they are tested on the test set. Their average mean absolute error (MAE) and mean square error (MSE) are recorded. We repeat the process above five times and report the average and standard deviation. While testing the generalization ability of the dataset condensation methods, DLinear is used as the expert model to perform dataset condensation. Meanwhile, MLP, LSTM, and CNN are used as test models when testing the generalization ability of the dataset condensation methods.

    &  &  \\   & MLP &  &  &  &  &  &  &  &  &  &  \\  Random & 0.01316.0 & 1.24667 & 0.03460 & 1.1935.0 & 0.0160.0 & 1.2171.0 & 0.1534.0 & 0.0160.0 & 0.5326.0 & 0.5311.0 & 0.5396.0 & 0.5366.0 & 0.6553.0 \\  DC & 0.01316.0 & 1.24667 & 0.03467 & 0.0131 & 0.0160.0 & 0.0170.0 & 0.0170.0 & 0.0140.0 & 0.5426.0 & 0.516.0 & 0.5366.0 & 0.5376.0 & 0.5366.0 \\  DC & 0.01316.0 & 0.0170.0 & 0.0111 & 0.0154.0 & 0.0160.0 & 0.0170.0 & 0.0170.0 & 0.0140.0 & 0.5426.0 & 0.516.0 & 0.5366.0 & 0.5366.0 & 0.5371.0 \\  KD & 0.01316.0 & 0.0170.0 & 0.0111 & 0.0154.0 & 0.0160.0 & 0.0170.0 & 0.0170.0 & 0.0170.0 & 0.0160.0 & 0.0170.0 & 0.0170.0 & 0.0170.0 & 0.0170.0 \\ RFA & 0.01316.0 & 0.0170.0 & 0.0111 & 0.0154.0 & 0.0160.0 & 0.0170.0 & 0.0170.0 & 0.0170.0 & 0.0170.0 & 0.0170.0 & 0.0170.0 & 0.0170.0 & 0.0170.0 \\ RFA & 0.0140.0 & 0.0140.0 & 0.0140.0 & 0.0140.0 & 0.0140.0 & 0.0140.0 & 0.0140.0 & 0.0140.0 & 0.0140.0 & 0.0140.0 & 0.0140.0 & 0.0140.0 \\ MTF & 0.01421.0 & 0.0140.0 & 0.0140.0 & 0.0140.0 & 0.0140.0 & 0.0140.0 & 0.0140.0 & 0.0140.0 & 0.0140.0 & 0.0140.0 & 0.0140.0 & 0.0140.0 \\ RFA & 0.01421.0 & 0.0140.0 & 0.0140.0 & 0.0140.0 & 0.0140.0 & 0.0140.0 & 0.0140.0 & 0.0140.0 & 0.0140.0 & 0.0140.0 & 0.0140.0 & 0.0140.0 \\ RFA & 0.01421.0 & 0.0140.0 & 0.0140.0 & 0.0140.0 & 0.0140.0 & 0.0140.0 & 0.0140.0 & 0.0140.0 & 0.0140.0 & 0.0140.0 & 0.0140.0 & 0.0140.0 \\ RFA & 0.01421.0 & 0.0140.0 & 0.0140.0 & 0.0140.0 & 0.0140.0 & 0.0140.0 & 0.0140.0 & 0.0140.0 & 0.0140.0 & 0.0140.0 & 0.0140.0 & 0.0140.0 \\  CendT & 0.0140.0 & 0.0140.0 & 0.0140.0 & 0.0140.0 & 0.0140.0 & 0.0140.0 & 0.0140.0 & 0.0140.0 & 0.0140.0 & 0.0140.0 & 0.0140.0 & 0.0140.0 & 0.0140.0 \\   

Table 2: Information and condensation settings of time series datasets.

    &  &  \\   & MLP &  &  &  &  &  &  &  &  &  &  &  &  \\  Random & 0.01316.0 & 1.24667 & 0.03460 & 0.0171 & 0.0135.0 & 0.0160.0 & 0.1571.0 & 0.5344.0 & 0.0150.0 & 0.5326.0 & 0.5311.0 & 0.5396.0 & 0.5396.0 & 0.5366.0 & 0.6553.0 \\  DC & 0.01316.0 & 0.01316.0 & 0.0111 & 0.0154.0 & 0.0140.0 & 0.0140.0

**Implementation Details:** As a plugin module, we test CondTSF with all previous methods. Each synthetic dataset is optimized using a standard training process according to the chosen backbone model. CondTSF is set to update every 3 epochs and the additive update ratio \(\) is set to be 0.01. All the experiments are carried out on an NVIDIA RTX 3080Ti.

### Results

**Single Architecture Performance:** The results are summarized in Table.1. For each backbone method, the first line shows the performance of the backbone model, the second line shows the performance of a backbone model with CondTSF, and the third line shows the percentage of reduction in MAE and MSE after CondTSF is applied. There's a considerable reduction in error for all backbone models. The results suggest that CondTSF is effective in optimizing the value term and enhancing the performance in dataset condensation for TS-forecasting. However, using CondTSF on DC is not as effective as other methods. The reason is that instead of directly matching parameters, DC matches the gradient of parameters on loss in each iteration. Indirectly matching gradient leads to accumulating errors in parameters, making DC unable to lower parameter error as effectively as directly matching parameters. Therefore CondTSF is not effective enough when applied to DC.

**Cross Architecture Performance:** We also conduct experiments to evaluate the cross-architecture performance of dataset condensation methods. The results are summarized in Table.3. We test all models on all datasets with MLP, LSTM, and CNN as test models. All synthetic data is distilled using DLinear model as experts. We use MTT as the backbone for CondTSF. We observe that CondTSF based on MTT outperformed all other previous models.

### Discussion

**Test Performance and Errors:** We conduct experiments on ExchangeRate dataset with MTT and MTT+CondTSF. As shown in Fig.4, trajectory of parameter error \(-_{o}\|^{2}}{\|_{f}-_{o }\|^{2}}\), label error \(_{label}\) and test error \(_{test}\) through the distillation process are presented. Regarding the parameter error corresponding to the gradient term, both MTT and MTT+CondTSF converge quickly, suggesting that the incorporation of CondTSF doesn't impact parameter alignment. As for the label error corresponding to the value term, since the initial synthetic data \(\) is randomly sampled from the train set \(\) and the expert model is trained by the train set \(\), the label error of \(\) is small at the beginning. However, the utilization of MTT results in an elevation of label error, whereas employing CondTSF effectively mitigates this increase in label error. During the test, MTT+CondTSF notably outperforms MTT by concurrently optimizing both the value term and the gradient term.

## 6 Limitations

The limitation of this work is that we use linear models in our analysis so that the gradient of a model on input is the parameter of the model. Therefore, only linear models like DLinear are solid enough to be an expert model for dataset condensation. The analysis no longer holds when it comes to more complicated models. However, experiments in App.D and App.E show that CondTSF is

Figure 4: Changing trajectory of **Left:** parameter error which refer to gradient term, **Middle:** label error which refer to value term and **Right:** test error during dataset condensation process.

also effective with non-parameter-matching methods and non-linear models, which merits further exploration.

## 7 Conclusion

In this study, we provide abundant proof that previous dataset condensation methods based on classification are not suitable for dataset condensation for TS-forecasting. We elucidate that these earlier methods, predominantly focused on classification tasks, only address a portion of the optimization objective pertinent to TS-forecasting. To address this issue, we propose a plugin module called CondTSF that can collaborate with parameter matching based dataset condensation methods. CondTSF optimizes the optimization objective that previous methods have neglected and boosts the performance of dataset condensation methods on TS-forecasting. We conduct experiments on eight widely used time series datasets and prove the effectiveness of our proof and method. CondTSF consistently enhances the performance of all previous techniques across all datasets, substantiating its effectiveness in improving dataset condensation outcomes for TS-forecasting applications.