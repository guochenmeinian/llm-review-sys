# Low Precision Local Training is Enough for Federated Learning

Zhiwei Li\({}^{1,}\)1 & Yiqiu Li\({}^{1,}\)1 & Binbin Lin\({}^{2,4}\) & Zhongming Jin\({}^{3}\) &Weizhong Zhang\({}^{1,}\)2

\({}^{1}\)Fudan University & Zhejiang University & Zhejiang University &Alibaba Cloud Computing & Fullong Inc.

{zwli23, yiqiuli22}@m.fudan.edu.cn binbinlin@zju.edu.cn zhongming.jinzm@alibaba-inc.com weizhongzhang@fudan.edu.cn

Equal contributionCorresponding author: weizhongzhang@fudan.edu.cn

###### Abstract

Federated Learning (FL) is a prevalent machine learning paradigm designed to address challenges posed by heterogeneous client data while preserving data privacy. Unlike distributed training, it typically orchestrates resource-constrained edge devices to communicate via a low-bandwidth communication network with a central server. This urges the development of more computation and communication efficient training algorithms. In this paper, we propose an efficient FL paradigm, where the local models in the clients are trained with low-precision operations and communicated with the server in low precision format, while only the model aggregation in the server is performed with high-precision computation. We surprisingly find that high precision models can be recovered from the low precision local models with proper aggregation in the server. In this way, both the workload in the client-side and the communication cost can be significantly reduced. We theoretically show that our proposed paradigm can converge to the optimal solution as the training goes on, which demonstrates that low precision local training is enough for FL. Our paradigm can be integrated with existing FL algorithms flexibly. Experiments across extensive benchmarks are conducted to showcase the effectiveness of our proposed method. Notably, the models trained by our method with the precision as low as 8 bits are comparable to those from the full precision training. As a by-product, we show that low precision local training can relieve the over-fitting issue in local training, which under heterogeneous client data can cause the client models drift further away from each other and lead to the failure in model aggregation. Code is released at https://github.com/digbangbang/LPT-FL.

## 1 Introduction

Federated learning (FL) [3; 15; 22; 36] is a popular privacy preserving machine learning paradigm to collaboratively learn a global model over the decentralized data. In FL paradigm, the clients are responsible for local training and only have access to their private datasets, while the server plays an essential role in aggregating the clients' updates into a global model. Unlike large-scaled distributed training, FL typically orchestrates resource-constrained edge devices to communicate via a low-bandwidth communication network with a central server. This urges the development of more computation and communication efficient optimization algorithms. The most prevalent approach in FL is developed based on local-SGD , which is referred to as FedAvg. In each communication round, the clients individually train their local models for multiple steps and then send them to the server for aggregation. It can be expected that if longer local training process one uses, the greater communication cost saving one can achieve. However, long time local trainingcould cause the local models drift further away from each other and degrades the aggregated global model's performance or even make the training diverge, especially when the data on the clients are heterogeneous. Therefore, in order to prolong local training processes in FL, extensive efforts have been made in the recent years. For example, the studies [1; 16; 21; 22] modify the local training process by imposing regularization on the client models to enforce them not to drift away from the previous global model. Another line of research [5; 6; 24; 31; 35; 38] focuses on refining the global model in the server aggregation process. These methods typically require a large proxy dataset on the server. Some of them [5; 6; 24] use it to align the outputs of the global model with that of the client ensemble by knowledge distillation. Others develop handcrafted aggregation rules to reweight the updates based on the statistics of updates or performance on proxy data [31; 35; 38] or further tune the global model with proxy data in every communication round [5; 24]. Although promising experimental results have been reported in the literature, it is still unclear that whether there exists more concise and effective FL paradigm, which can reduce both the workload in the client-side and the communication cost.

In this paper, we propose a concise and efficient federated learning paradigm, where the local models in the clients are trained with low precision operations and communicated with the server in low precision format, while only the server-side information integration maintains high-precision computation to ensure the accuracy. Our basic idea is inspired from the Kolmogorov's law , that is, the sample average can converge almost surely to the expected value although the samples always contain noise. Therefore, in the server side, we perform the simple moving average on the received low precision models from the clients to recover a high precision global model. In this way, both the workload in the client-side and the communication cost can be significantly reduced. We theoretically proved that our proposed paradigm integrated with FedAVG can converge to the optimal solution as the training goes on, which indicates that low precision local training is enough for federated learning. We extend our method to various existing FL method to show its flexibility. Experiments across extensive benchmarks are conducted to showcase the effectiveness of our proposed method. Notably, the models trained by our method with the precision as low as 8 bits are comparable to those from the full precision federated learning. Compared with some efficient FL designs, our method can achieve significant savings in training memory overhead, and what is more attractive is that our accuracy performance is even better. Our method exhibits another appealing feature in relieving the over-fitting issue in local training. To be precise, in the local training steps, the models can be easily trained to over-fit the local training data as the local dataset is always insufficient. Under heterogeneous client data, it would further cause the client models drift further away from each other and lead to the failure in model aggregation. The experimental results show that our approach can effectively relieve the over-fitting issue since the local training is performed with low precision computation and the expressiveness of the local model is restricted.

Our main contributions are as follows:

* We propose an efficient federated learning paradigm that performs low precision computation during local training, saving computational overhead and communication costs, while being able to restore accuracy through high-precision aggregation on the server side.
* We theoretically proved that the efficient federated learning algorithm we proposed can achieve convergence at a rated of \((1/T)\) under certain assumptions for non-iid situations.
* Since the expressiveness of the local model is restricted due to the low precision local training, our approach can relieve the over-fitting issue, which would cause the client models drift further away from each other and lead to the failure in model aggregation when the local dataset are heterogeneous.
* The extensive experimental results demonstrate the effectiveness of our approach. Notably, the models trained by our method with the precision as low as 8 bits are comparable to those from the full precision federated learning.

## 2 Related Work

**Federated Learning.** Federated Learning is first proposed by  to realize model training without sharing client device data. Many works have continued to solve some challenges of FL such as heterogeneity [16; 22; 25], privacy , communication efficiency [11; 18]. Also, some works proposes new FL methods to alleviate data heterogeneity. The vanilla FL method was FedAvg .

FedProx  utilizes a regularization term while Scaffold  sets a control variate to reduce the drift in local training. FedGen  and FedFTG  maintain a generator, the former is used for local data augmentation, while the latter is used for fine-tuning the server.

**Efficient Federated Learning.** One challenge of FL is the limitation of low bandwidth and computing resources of client devices. [4; 20] assign each client a block mask, resulting in sparse local models.  took the transmission speed into consideration and chose the same method as  for uploading, uploading compressed gradients.  adopts boost training to client-side training overhead. [7; 28] only transmit the trained head to reduce transmission cost.  adopts the idea of Network Architecture Searching, e.g., each client selects a sub-network.  maintains a series of streamlined models in the server, from which the client selects a tiny model for training. In , the client selects a sub-model of the global model for training. In this paper, we address this issue by using low precision local training.

## 3 Preliminary

### Federated Learning

Given \(N\) clients with their private datasets \(_{k}=\{(x_{k,j},y_{k,j})\}_{j=1}^{|_{k}|}\), \(k=1,,N\), the optimization objective of FL is always defined as follows:

\[_{}F()_{k=1}^{N}p_{k}F_{k}( ),\] (1)

where \(^{d}\) represents the model parameters, \(F_{k}\) is denoted to be the empirical risk function of client \(k\), i.e., \(F_{k}()=_{_{k}}_{k}|}( ;)\) with \((,)\) being the loss function and \(p_{k}=_{k}|}{_{k=1}^{N}|_{k}|}\) denotes the proportion of data contained in client \(k\).

FL emphasizes data privacy protection and thus the server is not allowed to access these datasets \(_{k}\) directly in model training. The standard method to solve the above training problem of FL is FedAvg , which is developed based on local SGD. It is comprised by two steps, i.e., local training on the clients and model aggregation in the server side. The details are presented below.

* In local training, the central server would first randomly select partial clients and broadcasts the latest global model \(_{t}\) to them. We denote the selected clients set as \(_{t}\) and let \(K=|_{t}|\) be the number of selected clients. Then the client \(k\) with \(k_{t}\) would initialize its local model to be \(_{t}^{k}=_{t}\) and then performs local training with \(E( 1)\) iterations as follows: \[_{t+1}^{k}_{t}^{k}-_{t} F_{k}(_{t}^{k};_{t}^{k}),k_{t},\] (2) where \(_{t}^{k}\) is the weights of the \(k\)-th client in step \(t\), \(_{t}^{k}\) is a mini-batch of samples uniformly chosen from \(_{k}\), \(_{t}\) is the step size.
* In model aggregation, FedAvg updates the global model to be the weighted average of the received local models, i.e., \[_{t+E}_{k_{t}}}{q_ {t}}_{t+E}^{k},\] (3) where \(q_{t}=_{k_{t}}p_{k}\) normalize the coefficients.

### Block Floating Point Quantization

Fixed point quantization is a standard quantization technique. It uses stochastic rounding to round the numbers up or down at random such that \([Q(x)]=x\), where \(Q:\) is the quantization function defined as

\[Q(x)=(|],l,u&-,\\ (||,l,u)&1-( -),\] (4)

here \((x,a,b)=((x,b),a)\), \(=2^{-F}\) is the quantization gap represents the distance between successive representable fixed point numbers, \(u=2^{W-F-1}-2^{-F}\) and \(l=-2^{W-F-1}\) represent the upper and lower limits of the representable numbers, respectively. \(W\) is the bit width of quantized numbers, and \(F\) is the bit width of quantized numbers' fractional part. In order to improve the utilization efficiency of the bit width and better maintain numerical accuracy when data is unevenly distributed, we choose block floating point quantization. Given a block of numbers \(X\), it replaces \(=2^{-F}\) in fixed point quantization to be \(=2^{-(W-2-E(X))}\), where

\[E(X)=(_{2}(_{i}|X_{i}|),-2^{W-F-1},2^{W-F-1}- 1).\] (5)

The shared exponent \(E(X)\) is usually set to be the largest exponent in \(X\) to avoid overflow .

## 4 Method

In this section, we introduce our low precision federated learning paradigm. It is comprised of two modules,i.e., one is the low precision local training to reduce the computation and communication cost, the other is the high precision aggregation with moving average to maintain the model accuracy.

### Low Precision Local Training

In order to reduce the computation and communication cost, we apply block floating point quantization to all clients' device of local training. The simple version of low precision local training is to convert the local training step in Eqn.(2) into

\[_{t+1}^{k}=Q_{t}^{k}-_{t} F_{k}(_{t}^{k};_{t}^{k}).\] (6)

Note that in the above version, we only quantize the updated parameters. We give this version just for the convenience of the theoretical analysis in Section 5. In practice, we quantize the gradient, the activation of each layer, the back-propagation signals, and the momentum in SGD when SGD is adopted as the optimizer. The details are given in Algorithm 1.

```
0: Quantization functions \(Q_{A}\), \(Q_{E}\), \(Q_{G}\), \(Q_{M}\), \(Q_{W}\); Momentum coefficient \(\); L layers DNN \(\{f_{1},f_{2},,f_{L}\}\); Loss function \(\).
1:ClientUpdate(\(t,k,w_{t}^{k}\)):
2: Get batch (\(x_{k,j_{t}},y_{k,j_{t}}\)) from \(_{k}\)
3:Forward Propagation:
4:\((a_{t}^{k})^{(0)}=x_{k,j_{t}}\)
5:\((a_{t}^{k})^{(l)}=Q_{A}(f_{l}((a_{t}^{k})^{(l-1)},(w_{t}^{k})^{(l)})), l [1,L]\)
6:Backward Propagation:
7:\((e_{t}^{k})^{(L)}=_{(a_{t}^{k})^{(L)}}((a_{t}^{k})^{(L)},y_{k,j_{t}})\)
8:\((e_{t}^{k})^{(l-1)}=Q_{E}(((a_{t}^{k})^{(l-1)},(w_{t}^{k}) ^{(l)})}{(a_{t}^{k})^{(l-1)}}(e_{t}^{k})^{(l)}), l[1,L]\)
9:\((g_{t}^{k})^{(l)}=Q_{G}(((a_{t}^{k})^{(l-1)},(w_{t}^{k}) ^{(l)})}{(w_{t}^{k})^{(l)}}(e_{t}^{k})^{(l)}), l[1,L]\)
10:Low Precision SGD Update:
11:\((v_{t+1}^{k})^{(l)} Q_{M}((v_{t}^{k})^{(l)}+(g_{t}^{k})^{(l)}),  l[1,L]\)
12:\((w_{t+1}^{k})^{(l)} Q_{W}((w_{t}^{k})^{(l)}-_{t}(v_{t+1}^{k })^{(l)}), l[1,L]\)
13:Return:\(w_{t+1}^{k}\) ```

**Algorithm 1** Low Precision Local Training with All Numbers Quantized

### High Precision Aggregation

Although low precision training can reduce communication and training overhead, it would lead to a degradation in training accuracy. Inspired from the Kolmogorov's law , that is, the sample average can converge almost surely to the expected value although the samples always contain noise, we try to recover high-precision solution from the low-precision local model with a full precision aggregation process. It is implemented with the following two steps:

* Calculate the weighted average of the local models to partially reconvert the precision, i.e., \[_{t+E}_{k_{t}}}{q_ {t}}_{t+E}^{k}.\] (7)* Since in most cases clients' data is non-iid and quantization causes error, federated learning is harder to converge, however maintaining a moving average in the server can significantly alleviate the problem. Formally, we denote \(}_{t}\) as the moving average stored in the server, then after aggregation, we update \(}_{t}\) as follow \[}_{t} Q(}_{t-E}+(1-) _{t}),\] (8) where \(\) is a coefficient controlling the influence of current weight. This procedure can further compensate the accuracy degradation due to the low precision local training.

In the next round of local training, the quantized model \(Q(}_{t})\) will be distributed to the clients to utilize the local models. Our pseudocode in Algorithm 2 depicts the process of low precision local training on the client device and high precision aggregation on the server. In Algorithm 2, \(t^{}=t-E+1\) and \(_{E}=\{nE|n=1,2,\}\) represents the set of global synchronization steps.

```
1:Initialize:\(_{0},}_{0}_{0}\)
2:for\(t=0,1,,T-1\)do
3:if\(t 0\)then
4: Select \(K\) clients from \([N]\) to be \(_{t}\)
5:\(_{t}^{k} Q(}_{t}),k_{t}\)
6:endif
7:for\(k_{t}\)do
8:\(_{t+1}^{k} Q(_{t}^{k}-_{t} F_{k}( _{t}^{k};_{t}^{k}))\)\(\) Client update
9:endfor
10:if\(t+1_{E}\)then
11:\(_{t+1}_{k_{t^{}}}}{q_ {t^{}}}_{t+1}^{k}\)\(\) Server update
12:\(}_{t+1}}_{t^{}}+(1- )_{t+1}\)
13:endif
14:endfor
15:Return:\(}_{T}\) ```

**Algorithm 2** Federated Learning with Low Precision Local Training

## 5 Theoretical Analysis

In this section, we give the detailed theoretical results for our low precision FL paradigm. We will first introduce the convergence analysis in the full participation case where all client devices participate (i.e., \(K=N\)) and then we generalize the results of the analysis to scenarios that are more in line with reality. (i.e., \(K<N\)). The results demonstrate that we will explore aggregation strategies represented by the FederatedAveraging Algorithm (or FedAvg) and demonstrate that our proposed low precision FL framework can converge to the global optimal solution at a rate of \((1/T)\) for non-iid datasets based on strong convexity and smoothness assumptions.

### Assumptions and Notations

We need to make necessary assumptions about the objective function on the clients \(F_{k},k=1,,N\). Assumption 1 is about the smoothness and strong convexity of the loss function and Assumption 2 is about the boundness of the gradients. These assumptions are standard and widely adopted in the related studies .

**Assumption 1**.: \(F_{1},F_{2},...,F_{N}\) _are L-smooth and \(\)-strongly functions, which means that for all \(\) and \(\), the following inequalities hold:_

\[F_{k}()  F_{k}()+(-)^{T} F_{k}( )+\|-\|_{2}^{2}, (L-smooth)\] (9) \[F_{k}()  F_{k}()+(-)^{T} F_{k}( )+\|-\|_{2}^{2}, (-strong)\] (10)

_where \(\|\|^{2}\) represents the square of two norms and \(k=1,,N\)._

**Assumption 2**.: _Let \(_{t}^{k}\) be sample that randomly and uniformly sampled from the local data of the \(k\)-th client device. For \(k=1,2,,N\) and \(t=0,1,\), the variance of stochastic gradients in each client device and the expectation of squared two norm of stochastic gradients is bounded:_

\[ F_{k}(_{t}^{k};_{t}^{k})-  F_{k}(_{t}^{k})_{2}^{2}}^{2},\] (11) \[ F_{k}(_{t}^{k};_{t}^{k}) _{2}^{2} G^{2}.\] (12)

**Degree of Data Heterogeneity.** Let \(F^{*}\) denote the global minimum of the objective function \(F\), and let \(F_{k}^{*}\) represent the minimum of the local objective function \(F_{k}\) specific to the \(k\)-th client. We use the metric \(\) taking the form of

\[=F^{*}-_{k=1}^{N}p_{k}F_{k}^{*},\]

to measure the degree of heterogeneity of all clients' data distribution. When the data on each client device is iid, as the number of samples increase, \(\) evidently tends towards zero. However, when faced with non-iid situation, \(\) tends towards a positive constant and thus it can be used to measure the degree of heterogeneity in the data distribution of each client device.

### Convergence Analysis: Full Client Device Participation

First we analyze the convergence of the participation of all clients' device in this section. We integrate our low precision FL framework with FedAvg and train the model for \(T\) iterations to obtain the \(}_{T}\), and we expect \(T\) to be divided by \(E\) so that \(}_{T}\) is the weight after aggregation.

**Theorem 1**.: _Under the Assumptions 1 and 2, we set \(=\), \(=\{8,E\}-1\) and \(_{t}=\). When \(t\) satisfying \(^{2}_{t}^{2}G^{2}\), low precision FedAvg with full device participation satisfies:_

\[[F(}_{T})]-F^{*}( +\|_{1}-^{*}\|_{2}^{ 2}),\] (13)

_where_

\[B=2(+1+)G^{2}+16E^{2}G^{2}(2+3)+}_{k=1}^{N}{_{k}}^{2}+6L.\] (14)

### Convergence Analysis: Partial Client Device Participation

Compared to full participation situation, the partial participation situation is more in line with the reality. We need to make more assumption on how to choose \(_{t}\).

**Assumption 3**.: _Assume \(_{t}\) contains a subset of \(K\) indices uniformly sampled from \([N]\) without replacement. In addition, the data is balanced in the semete that \(p_{1}==p_{N}=\). The aggregation step of FedAvg performs \(_{t+E}_{k_{t}}_{t+E} ^{k}\)_

**Theorem 2**.: _Under the Assumptions 1 to 3, we choose \(=\), \(=\{8,E\}-1,_{t}=\) and \(B=2(+1+)G^{2}+16E^{2}G^{2}(2+3)+}_{k=1}^{N}{_{k}}^{2}+6L,C=E ^{2}G^{2}(2+3)+dG^{2}\). When \(t\) satisfying \(^{2}_{t}^{2}G^{2}\), low precision FedAvg with partial client device participation satisfies:_

\[[F(}_{T})]-F^{*}( +\|_{1}-^{*}\|_{ 2}^{2}).\] (15)

## 6 Experiments

In this section, we conduct extensive experiments to verify the effectiveness of our methods in the following five aspects:

* When integrated with FedAvg, the models trained by our method with the low precision are comparable to (if not better than) those from the full precision training. This would also verify our theoretical results (Section 6.1).
* Our method can effectively relieve the over-fitting issue in FL. See Section (Section 6.2).

* Our paradigm can be integrated with existing FL methods flexibly and preserve the performance even with a low precision local training (Section 6.3).
* Ablation studies on the effectiveness of moving average in model aggregation and the transferability over various neural networks (Section 6.4).
* Comparison with other efficient FL techniques (Section 6.5).

**Remark 1**.: _Similar with the existing low precision training studies , We do not give the results on the running time to show the real acceleration. The reason is that to achieve real acceleration, we need to implement our method integrated with the professional hardware. Moreover, such implementation is standard for the professional hardware platforms._

**Benchmark Datasets and Baseline.** We conduct experiments over four commonly used datasets: FashionMnist , CIFAR10 , CIFAR100  and CINIC10 . Four commonly used FL methods: 1) FedAvg ; 2) regularization-based strategy FedProx ; 3) data-dependent knowledge distillation strategy ABAvg  4) data-free knowledge distillation strategy FedFTG  and FedGen  are adopted as the baselines.

**Configurations.** We follow the configurations in the recent studies [26; 42; 27] for fair comparison. To be precise, for FashionMNIST, CIFAR10, CINIC10 and CIFAR100, we run 200 communication rounds with local epoch set to 1. There are 80 clients in total, and the participation ratio in each round is set to 40%. We use Dirichlet distribution to simulate non-iid data distribution and set \(\) to 0.01, 0.04, and 0.16. The smaller \(\) is, the more serious the data heterogeneity is. For the network choice, we use ConvNet following with 3 layers, and the hidden dimension is set to 128. The local learning rate is set to \(10^{-3}\) with Adam optimizer . We report the last \(5\) round global model's average performance evaluated using the test split of the datasets. For quantization method, we adopt the Block FLoating Point Quantization with the number of bits used set to 6, 8 and 32 (without quantization). Some of the other hyperparameter settings are included in the Appendix C.

### Results on FedAvg

We demonstrate the superior performance of our Low Precision FL method with FedAvg by conducting experiments over comprehensive datasets, various precision and heterogeneity values \(\).

**Heterogeneity.** As shown in Table 1, it is as expected that when the heterogeneity goes higher, that is, when \(\) decreases, the server performance worsens. Nevertheless, our proposed method can always maintain or improve the performance of the original case (bits = 32, w/o. avg) when using 8 quantizaiton bits with moving average, which empirically validates the effectiveness of our proposed method.

**Quantization Bits.** We conduct experiments on 3 quantization bits: 32, 8, 6 (shown in Table 1, Figure 1), as we observe that in most cases 8 bits is enough to hold the performance of full-precision and when the used bits is 6, the server performance begins to decrease due to the low precision level.

### Quantization Relieves Overfitting

We present the averaged local training loss and the global testing loss over training in Figure 2. It can be seen that our method can effectively reduce testing losses on the server. The commonality

  &  &  &  \\  } &  & PANIST & CIFAR100 & CINIC10 & CIFAR100 & F MNIST & CIFAR100 & CINIC10 & CIFAR100 & F MNIST & CIFAR100 & CINIC10 & CIFAR100 \\  \(w/o\) & 8 bit & 80.1 \(\) 0.7 & 53.3 \(\) 2.7 & 43.3 \(\) 1.2 & 15.2 \(\) 0.2 & 83.8 \(\) 0.1 & 57.4 \(\) 0.6 & 50.8 \(\) 1.2 & 34.1 \(\) 0.4 & 96.6 \(\) 0.3 & 72.4 \(\) 0.7 & 58.2 \(\) 1.5 & 42.9 \(\) 0.1 \\  \(w/o\) & 6 bit & 78.0 \(\) 1.2 & 93.3 \(\) 3.9 & 39.2 \(\) 1.5 & 12.4 \(\) 0.0 & 81.8 \(\) 0.1 & 53.6 \(\) 0.6 & 45.8 \(\) 1.6 & 22.4 \(\) 0.3 & 97.0 \(\) 0.5 & 66.8 \(\) 0.7 & 55.8 \(\) 0.0 \\  \(w/o\) & 2 bit & 58.7 \(\) 0.6 & 57.7 \(\) 0.6 & 41.0 \(\) 0.8 & 16.6 \(\) 0.0 & 81.6 \(\) 0.0 & 58.6 \(\) 0.3 & 37.8 \(\) 1.5 & 30.7 \(\) 0.3 & 90.7 \(\) 0.2 & 72.8 \(\) 0.9 & 55.6 \(\) 0.9 & 41.7 \(\) 0.3 \\  \(w/o\) & 8 bit & 77.2 \(\) 2.0 & 25.6 \(\) 1.0 & 20.9 \(\) 1.2 & 73.2 \(\) 0.4 & 29.3 \(\) 1.2 & 25.5 \(\) 1.6 & 28.3 \(\) 1.9 & 17.4 \(\) 2.1 & 87.4 \(\) 2.2 & 87.4 \(\) 2.2 & 87.4 \(\) 0.8 & 75.5 \(\) 1.9 & 86.6 \(\) 0.4 \\  \(w/o\) & 6 bit & 41.5 \(\) 4.8 & 32.8 \(\) 0.8 & 15.1 \(\) 1.0 & 13.6 \(\) 0.7 & 72.5 \(\) 1.7 & 30.8 \(\) 1.3 & 28.3 \(\) 1.5 & 6.7 \(\) 0.1 & 50.0 \(\) 1.3 & 41.6 \(\) 1.1 & 19.2 \(\) 1.4 & 96.2 \(\) 0.2 \\  \(w/o\) & 2 bit & 79.5 \(\) 2.7 & 41.1 \(\) 2.4 & 35.4 \(\) 4.1 & 12.5 \(\) 0.4 & 83.1 \(\) 2.1 & 54.4 \(\) 2.0 & 43.2 \(\) 1.6 & 28.2 \(\) 0.5 & 90.2 \(\) 0.5 & 71.9 \(\) 1.5 & 57.0 \(\) 1.3 & 39.1 \(\) 0.4 \\  

Table 1: Results of our method integrated with FedAvg over various levels of heterogeneity and precision. The results with moving average demonstrate that our method can match the performance of full-precision federated learning even with all numbers quantized down to 8 bits. The results in the bottom three rows indicates the without moving average, training with low precision would lead to performance degradation.

with the original method is that when each round of communication starts retraining, the training loss of the client will be greatly increased due to the heterogeneity of the data \(_{k}\). Subsequently, due to the highly imbalanced local data categories, the model quickly reached an overfitting state. It can be seen that at the beginning of each training round, under our method, the customer's training loss will not exceed the original training loss. At the same time, with some training steps, our training loss remains above the original training loss, which means our method can alleviate the overfitting problem of local training.

### Results on Other FL Methods

The four FL methods we used are each representative. ABAvg and FedFTG are similar to FedAvg in local training, but the former only performs weight adjustment on the server side, while the latter uses knowledge distillation to fine-tune the server. FedProx and FedGen are similar to FedAvg in the server side, but the former only has regularization constraints on local training, while the latter uses the generator for regularization adjustment in local training. As is demonstrated in Table 2, we can see that, regardless of the FL method chosen, our low precision FL algorithm has a significant improvement in prediction accuracy compared to the original FL method, especially in dataset CIFAR10, CINIC10 and CIFAR100.

Figure 1: Accuracy and loss of FedAvg with full precision (origin), our method with precision levels of 8 bit and 6 bit. We set \(=0.01\) on all the four datasets. Our method exhibits an effective reduction in fluctuation variance and improves the stability of training. The reason is that compared with the full precision training, our low precision local training can prevent the client models to drift further away from each other and overfit the local datasets, making the aggregation stable.

Figure 2: Effectiveness of our method in relieving the over-fitting issue. We present the averaged local **training** loss and the global **test** loss over training. We select a part of the training procedure (iteration 1000 to 1400) for display, and enlarge a part of the picture in the upper right corner to show more details. We can observe that the local training loss of FedAvg (full precision) is significantly lower than our method, however its global test loss is much higher than us and fluctuates dramatically.

[MISSING_PAGE_FAIL:9]

## 7 Conclusion

In this paper, we propose an efficient FL paradigm, where the local models in the clients are trained with low-precision operations and communicated with the server in low precision format, while only the model aggregation in the server is performed with high-precision computation. We theoretically show that our proposed paradigm can converge to the optimal solution as the training goes on, which demonstrates that low precision local training is enough for FL. Our paradigm can be integrated with existing FL algorithms flexibly. Experiments across extensive benchmarks are conducted to showcase the effectiveness of our proposed method. As a by-product, we show that low precision local training can relieve the over-fitting issue in local training.

## 8 Acknowledgements

Authors acknowledge the support in part by The National Nature Science Foundation of China grant No: 62472097, The National Nature Science Foundation of China grant No: 62273303, Yongjiang Talent Introduction Programme grant No: 2022A-240-G.

Figure 3: Results on CIFAR10 with \(=0.01\). ( ) denotes the percentage of models on the clients. We use the number of weights, activation, and gradients of local training to approximate the training cost (MB / client) and communication cost (MB / round).