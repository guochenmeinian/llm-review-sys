# Separate and Reconstruct:

Asymmetric Encoder-Decoder for Speech Separation

Ui-Hyeop Shin Sangyoun Lee Taehan Kim Hyung-Min Park

Department of Electronic Engineering, Sogang University, Seoul, Republic of Korea

{dmlguq123,leesy0882,taehank,hpark}@sogang.ac.kr

[https://dmlguq456.github.io/SepReformer_Demo](https://dmlguq456.github.io/SepReformer_Demo)

###### Abstract

In speech separation, time-domain approaches have successfully replaced the time-frequency domain with latent sequence feature from a learnable encoder. Conventionally, the feature is separated into speaker-specific ones at the final stage of the network. Instead, we propose a more intuitive strategy that separates features earlier by expanding the feature sequence to the number of speakers as an extra dimension. To achieve this, an asymmetric strategy is presented in which the encoder and decoder are partitioned to perform distinct processing in separation tasks. The encoder analyzes features, and the output of the encoder is split into the number of speakers to be separated. The separated sequences are then reconstructed by the weight-shared decoder, which also performs cross-speaker processing. Without relying on speaker information, the weight-shared network in the decoder directly learns to discriminate features using a separation objective. In addition, to improve performance, traditional methods have extended the sequence length, leading to the adoption of dual-path models, which handle the much longer sequence effectively by segmenting it into chunks. To address this, we introduce global and local Transformer blocks that can directly handle long sequences more efficiently without chunking and dual-path processing. The experimental results demonstrated that this asymmetric structure is effective and that the combination of proposed global and local Transformer can sufficiently replace the role of inter- and intra-chunk processing in dual-path structure. Finally, the presented model combining both of these achieved state-of-the-art performance with much less computation in various benchmark datasets.

## 1 Introduction

For the well-known cocktail party problem , single channel speech separation  has been improved since the introduction of time-domain audio separation network (TasNet) , which processes the audio separation in the latent space instead of the short-time Fourier transform (STFT) domain, as shown in Figure 1(a). In particular, in most speech separation methods, the process of separating the feature sequence for each speaker is typically positioned at the final stage of the network, as shown in Figure 1(b), which we refer to as _late split_. Therefore, a single feature sequence must encode all the information for all speakers to be separated. In addition, experimental results have shown that TasNet employing a convolution-based audio encoder/decoder performs better when the kernel length of the audio encoder is shortened , which requires modeling a long sequence. Indeed, expanding the sequence in channel and temporal dimensions is necessarily beneficial, since the separation process must include all the information for all speakers in the feature sequence.

As a solution to modeling long sequences, DPRNN  was proposed using a dual path model, in which it segments long sequences into chunks and processes in terms of intra-chunk and inter-chunkto model the local and global contexts. As a result, due to promising performances in modeling long sequences, many TasNet-based approaches have adopted the dual-path model and repeatedly achieved state-of-the-art performances in monaural speech separation [9; 66; 37; 38; 92; 57; 60; 51]. Meanwhile, some studies have tackled the high computational complexity of long sequences in the time domain approach and proposed using multi-scaled sequence models based on the recurrent or stacked U-Net structure [70; 32; 42]. They reduced the computations to some extent, however, they still could not show competitive performance compared to the dual-path method.

However, most studies have focused on handling long sequences rather than addressing the fundamental inefficiency of TasNet's late split structure, where a single feature sequence must encode all speaker information without discrimination, creating an information bottleneck. Also, forcing the separator to generate all separated features at once before the audio decoder makes the task difficult and can easily lead to local minima during training. To alleviate this challenge, we propose a more intuitive approach: expanding the feature sequence to include a dimension for the number of speakers in advance, as an _early split_. By splitting features earlier in the separator (Figure 1(c)), we adopt an asymmetric strategy where the encoder and decoder perform distinct roles. The encoder process a single feature sequence before the split layer, similar to conventional separators. After splitting into multiple sequences, the decoder focuses on capturing _discriminative_ characteristics between features using weight-sharing blocks [4; 11]. By employing this early split with a shared decoder (ESSD) structure, we ease the burden on the separator's encoder. This approach aligns with common practices in multi-channel speech processing, where processing is divided into two stages. Coarse separation is achieved through spatial filtering [31; 85; 24], followed by post-enhancement to refine results [80; 8; 81; 41].

Furthermore, dual-path model itself also has redundancy because the segmentation process may increase the amount of computation by twice when the overlap between adjacent chunks is set to 50%. Also, the inter-chunk blocks in the dual-path model are inefficient because their role is mainly to capture the global context. Therefore, we design unit blocks for both global and local processing, integrating them effectively to replace the dual-path model and directly process long sequences without chunking. Both of global and local blocks are based on Transformer block structure  where multi-head self-attention (MHSA) module and feed-foward network (FFN) module are stacked. As a global Transformer block, we modified MHSA module in Transformer block as an efficient gated attention (EGA) module to mainly capture the global dependency without redundancy. On the other hand, as a local block, the MHSA is replaced with convolutional local attention (CLA) to capture local contexts.

Consequently, we present the _Separation-Reconstruction Transformer (SepReformer)_ for more efficient time-domain separation. Based on the ESSD framework and efficient global and local Transformer unit block, the SepReformer employs an asymmetric encoder-decoder structure with skip connections based on a temporally multi-scaled sequence model. The encoder processes a single feature at different temporal resolutions, and each intermediate feature is used for skip connection. The decoder then gradually reconstructs fine-grained information from the temporal bottleneck features, focusing on the discriminative characteristics of separated speech with auxiliary loss. To achieve this, the weight-shared decoder is trained to discriminate between the features separated by the encoder. In addition, a cross-speaker block is utilized in the decoder to facilitate information interaction between sequences, as described in [15; 40]. Furthermore, we design unit blocks for both global and local processing, integrating them effectively to replace the dual-path model and directly process long sequences without chunking. The experimental results demonstrated that the ESSD is

Figure 1: **Block diagrams of (a) TasNet and separator designs of the (b) conventional and (c) proposed networks. The proposed network consists of separation encoder and reconstruction decoder based on weight sharing. After an encoder, separated features are independently processed by a decoder network.**

effective especially with a small network. Also, comprising the separator network with the proposed global and local blocks can sufficiently replace the inter- and intra-chunk processing in dual-path structure, suggesting effectiveness for long feature sequence processing in speech separation. As a consequence, the proposed SepReformer that includes both of these achieved state-of-the-art (SOTA) performance with much less computation than before in various benchmark datasets.

## 2 Related Works

TasNetConventional source separation has been performed in the STFT domain [30; 36; 12; 44]. In the time-frequency representation, a separator is modeled to estimate mask values or direct output representations. Then, the inverse STFT (iSTFT) is operated for the output representations to obtain separated signals [30; 36]. On the other hand, TasNet  replaces STFT with a 1D convolutional layer. Based on the encoder representation, mask values or direct output representations [65; 59; 40] are obtained in the separator. Then, the output representations are decoded by the audio decoder of 1D transposed convolution instead of iSTFT. Also, unlike the STFT, the convolutional encoder turns out to work well in a much shorter kernel size. Therefore, TasNet requires the separator to process the much longer sequences. Therefore, instead of an LSTM-based separator , Conv-TasNet  is proposed based on a temporal convolutional network (TCN) [71; 39] to design a separator for longer sequence, showing impressive separation results.

Dual-path model for long sequenceAfter Conv-TasNet, the dual-path model is extensively employed to handle long sequences. In the dual-path model, the sequence is segmented into smaller chunks, and the sequence is processed alternately as intra-chunk and inter-chunk, effectively inter-leaving between local and global contexts. This dual-path strategy has shown promising performance in TasNet and has been repeatedly adopted [9; 66; 37; 38; 92; 57; 60; 51; 53; 34]. Especially, it is shown that, compared to various efficient attention mechanisms [76; 2; 35], using the dual-path model with the original self-attention mechanism of Transformer  is effective for long sequence  in speech separation. However, modeling with the dual-path method can double computational costs due to the 50% overlap between adjacent chunks. The inter-chunk blocks in this model are somewhat redundant since they mainly capture global context. To reduce this redundancy, the quasi-dual-path network (QDPN)  replaces inter-chunk processing with downsampling. Inspired by QDPN, we design EGA and CLA modules to capture the global and local contexts without chunking process.

Multi-scale model for efficiencyInstead of the dual-path model, based on U-Net structure , some studies have suggested using multi-scaled sequence model [65; 49; 21; 32; 42; 7]. SuDoRM-RF model  used a stacked U-Net structure to reduce the computational cost. The SuDoRM-RF approach can be regarded as a substitution of the TCN block in Conv-TasNet with U-ConvBlock as U-Net sub-block. Although SuDoRM-RF reduces the computational cost, it still has the disadvantages of having a fixed receptive field size and not considering the global context. More recently, TDANet  has efficiently improved performance with top-down attention and unfolding as in A-FCRNN . However, these conventional methods with multi-scaled sequences prefer stacked or recurrent structures with U-Net sub-block to improve performance. Instead, we consider a single U-Net architecture and explicitly divide the roles of encoder and decoder as separation and reconstruction.

Discriminative learningWeight-sharing neural networks widely used in modern contrastive learning [11; 6; 25] including speaker verification [90; 58; 17]. On the other hand, some studies on speech separation proposed to exploit speaker identity as discriminative information to address the case that the similar voices are mixed [52; 51; 89]. Therefore, we utilized the weight-shared network to reconstruct separated speech by extracting distinct speech representations for corresponding speakers. To separate the mixture, weight-shared decoder directly learns to focus discriminative features without the need for additionally designed, for example, speaker loss using an additional speaker embedding extractor. As a result, based on discriminative learning, the weight-shared network in the decoder strengthens the dominant speaker's components on each separated sequence, respectively.

## 3 Method

### Overall pipeline

When input mixture \(^{1 N}\), the 1D convolution audio encoder, followed by GELU activation , encodes \(\) to the input representation, as \(=()^{F_{o} T}\),where \(F_{o}\) and \(T\) denote the number of convolutional filter of encoder and the number of frames, respectively. The kernel and stride size are \(L\) and \(H\), respectively. Then, the \(J\) output representations \(_{j}\) are estimated from the separator and decoded by the audio decoder, expressed as \(}_{j}=(_{j})^{1 N},1 j  J\). Following the recent works , we design the separator to directly map the output signals instead of masking.

### Architecture of separator

The detailed architecture of the separator of the proposed SepReformer is illustrated in Figure 2. The separator is constructed on the basis of the ESSD framework with a separation encoder and a reconstruction decoder in temporally multi-scaled U-Net structure.

Separation encoderThe input representation is first projected to \(F\) dimension by the input layer. The input layer is composed of the linear layer and Layer Normalization (LN)  applied to each frame independently. In the encoder, the projected feature sequence is successively downsampled \(R\) times from the sequence length of \(T\) to \(T/2^{R}\). The downsampling is performed by a 1D depth-wise convolution (Dconv) layer with a stride of 2 and a kernel size of 5, followed by Batch Normalization (BN) and GELU activation . Each encoder stage processes the single sequence feature by \(B_{E}\) stacks of global and local Transformer blocks.

Speaker splitThe encoded features in all stages of the encoder are expanded by the number of speakers \(J\) to transmit the speaker-wise features from the encoder to the decoder. Therefore, the speaker split layer is placed in the middle, and it commonly separates the intermediate encoder features used for skip connections as well as the bottleneck feature. As shown in Figure 3, this module consists of two linear layers with gated linear unit (GLU) activation . Each feature is then normalized by LN and processed by the decoder.

Reconstruction decoderFor temporal reconstruction, the upsampled sequence feature from the previous stage is concatenated with the skip connection followed by linear layer. Then, \(B_{D}\) stacks of global and local Transformer blocks process the \(J\) feature sequences as a weight-sharing network to discriminate between the separated features. By incorporating the separation objective function into the weight-sharing decoder, the network directly learns to capture the discriminative features. Then, the output of the last decoder stage is projected back to \(F_{o}\) dimension by an output layer. The output layer consists of two linear layers with GLU activation.

Cross-speaker (CS) TransformerDuring the discrimination process by the weight-sharing decoder, speech elements can be mistakenly clustered into other speaker channels. As a result, it would be beneficial to attend to each other in order to effectively recover the distorted speech elements. Therefore, to improve the interaction of contexts between speakers within the decoder, we incorporate a Transformer-based CS module as in . Based on MHSA module without positional encoding, the CS block performs an attention operation on speaker dimension while temporal dimension is processed independently. Therefore, the CS block learns to identify the interfering components of the opposing sequences within the same temporal frame. For convenience, we call ESSD with CS as a separation-and-reconstruction (SepRe) method.

Figure 3: Speaker split module

Figure 2: **The architecture of the separator in the proposed SepReformer. The separator consists of three parts: separation encoder, speaker split module, and reconstruction decoder.**

### Global and local Transformer for long sequences

Instead of the dual-path model based on chunking, we directly process a long sequence using global and local processing blocks, similar to QDPN  or Conformer . In particular, global and local blocks replace inter- and intra-chunk processing, respectively. The design of the blocks follows a Transformer block structure to ensure structural effectiveness [23; 68; 86; 29]. This structure consists of two sub-modules: temporal mixing and frame-wise channel mixing. These modules are stacked together with a pre-norm residual unit [75; 55] and LayerScale  to facilitate faster training of deep networks. Also, in all residual units, we apply dropout  for regularization.

Gated convolutional feed-forward network (GCFN)Instead of using the conventional feed-forward network (FFN) [23; 72] for channel mixing, we improve it by incorporating temporal Dconv with a small kernel size of 3 and substituting GELU with GLU activation  as shown in Figure 4(a). This GCFN can effectively process channel features by considering the adjacent frame context. Several studies also have demonstrated the effectiveness of these enhancements in FFN [63; 87; 77].

Global Transformer with efficient global attention (EGA)In Figure 4(a), the global block consists of an EGA module for temporal mixing and GCFN. The EGA module is based on the MHSA with relative positional encoding . However, to reduce the computation and focus on global information in the attention layer, the downsampled sequence is processed and upsampled back. Sequences \(T/2^{r}\) at all stages \(0 r R-1\) are downsampled to \(T/2^{R}\), which is equal to the length in the bottleneck. To compensate for downsampling, the upsampled features are multiplied by the gate value obtained from an additional branch with a linear layer and sigmoid function \(\). The simple strategy allows the effective capture of global contexts while maintaining local contexts.

Local Transformer with convolutional local attention (CLA)For the local block, we design a CLA module based on 1D temporal convolution with a large kernel of \(K\) in Figure 4(b). Inspired by [27; 84], the CLA module first processes the feature with the Pconv layer and GLU activation to facilitate capturing local contexts attentively. After the temporal Dconv, two Pconv layers are used. They have a hidden dimension of \(2F\) and employ BN and GELU activation.

### Boosting discriminative learning by multi-loss

The objective function is given as scale-invariant signal-to-noise ratio (SI-SNR) [62; 46] defined as

\[=-_{j=1}^{J}(20_{10}_ {j}\|_{2}}{\|_{j}_{j}-}_{j}\|_{2}},), \]

where \(_{j}=}_{j}^{T}_{j}/\|_{j}\|_{2}^{2}\) and \(\|\|_{2}\) denotes L2-norm. The clipping value of \(\) limits the influence of the best training prediction [89; 83]. Notably, the output of the decoder stages can be trained for progressive reconstruction as the feature sequences are already separated in the decoder stages as in the progressive multi-stage strategy [54; 91; 88; 16]. In particular, weight-sharing decoder in each stage

Figure 4: **Block diagrams of global and local Transformer for sequence processing**. \(\) and \(\) in EGA denote downsampling with average pooling and upsampling with nearest interpolation. Note that the point-wise convolution (Pconv) layer performs an equivalent operation to the linear layer as channel mixing. The hidden dimension of GCFN is set to \(3F\) after GLU to maintain a similar parameter size to the FFN with a hidden size of \(4F\). Therefore, while the FFN has parameter size of \(8F^{2}\), GCFN has a slightly larger size of about \(9F^{2}\).

can be trained clearly for discriminative learning with stage-specific separation objective. This multi-loss strategy is also considered to guide intermediate features in audio separation [53; 5; 59; 60; 40]. Therefore, the source signal can be estimated as \(}_{j,r}=_{r}(_{j,r}) ^{1 N}\) when \(_{j,r}^{F_{o} T}\) is estimated with additional output layers for \(_{j,r}\) and the nearest upsampling. \(\) denotes an element-wise multiplication, and \(_{r}()\) is an auxiliary audio decoder, which is also additional required with additional output layers. Therefore, we can set the auxiliary objective function as

\[_{r}=-_{j=1}^{J}(20_{10} _{j}\|_{2}}{\|_{j,r}_{j}-}_{j,r}\|_{ 2}},), \]

where \(_{j,r}=}_{j,r}^{T}_{j}/\|_{j}\|_{2}^ {2}\). Note that, when calculating the output from intermediate features, we opt for masking instead of direct estimation because the temporal resolutions of the feature sequences are deficient. Then, the multi-loss can be set to \(}=(1-)+_{r=1}^{R}_{r}/R\). Moreover, we alternatively calculate the intermediate loss \(_{r}\) using the magnitude values of \(_{j}\) and \(}_{j}\) in the STFT domain as it provided more stable training and no actual separated signals are required from the intermediate outputs.

## 4 Experimental Settings

### Dataset

We evaluated our proposed SepReformer on WSJ0-2Mix , WHAM! , WHAMR! , and LibriMix , which are popular datasets for monaural speech separation. To ensure generality, the mixtures in the test set were generated by the speakers that were not seen during training. For all the datasets, networks were trained with 4-s-long segments at a 8-kHz sampling rate while the model processes inputs of varying lengths in the evaluation.

WSJ0-2MixWSJ0-2Mix is the most popular dataset to benchmark the monaural speech separation task. It contains 30, 10, and 5 hours for training, validation, and evaluation sets, respectively. Each mixture was artificially generated by randomly selecting different speakers from the corresponding set and mixing them at a random relative signal-to-noise ratio (SNR) between -5 and 5 dB.

WHAM!/WHAMR!: WHAM!/WHAMR! is a noisy/noisy-reverberant version of the WSJ0-2Mix dataset. In the WHAM! dataset, speeches were mixed with noise recorded in scenes such as cafes, restaurants, and bars. The noise was added to get mixtures at SNRs uniformly sampled between -6dB and 3dB, making the mixtures more challenging than those in the WSJ0-2Mix.

Libri2MixIn Libri2Mix dataset, the target speech in each mixture was randomly selected from a subset of LibriSpeech's train-100  for faster training. Each source was mixed with uniformly sampled Loudness Units relative to Full Scale (LUFS) to get a mixture at an SNR between -25 and -33 dB. We used the clean version as in previous studies [9; 42].

### Training and model configuration

We trained the proposed SepReformer for a maximum of 200 epochs with an initial learning rate of \(1.0e^{-3}\). We used a warm-up training scheduler for the first epoch, and then the learning rate decayed by a factor of 0.8 if the validation loss did not improve in three consecutive epochs. As optimizer, AdamW  was used with a weight decay of \(0.01\), and gradient clipping with a maximum L2-norm of 5 was applied for stable training. All models were trained with Permutation Invariant Training (PIT) . When the multi-loss in Subsection 3.4 was applied, the \(\) was set to 0.4, and after 100 epochs, it decayed by a factor of 0.8 at every five epochs. \(\) was set to 30 as in . SI-SNRi and SDRi  were used as evaluation metrics. Also, we compared the parameter size and the number of multiply-accumulate operations (MACs) for 16000 samples. The number of heads in MSHA was commonly set to 8, and the kernel size \(K\) in the local block was set to \(65\). Also, we evaluated our model in various model sizes as follows:

* SepReformer-T/B/L: \(F=64/128/256\), \(F_{o}=256\), \(L=16\), \(H=4\), \(R=4\)
* SepReformer-S/M: \(F=64/128\), \(F_{o}=256\), \(L=8\), \(H=2\), \(R=5\)

We used a longer encoder length of \(L=32\) in the Large model when evaluating the WHAMR dataset to account for reverberation. Note that we did not train the models multiple times, as the deviations in the results are negligible below the significant digits. All experiments were conducted on a server with GeForce RTX 3090 \(\) 6.

## 5 Results

### Ablation studies of SepRe method

Decoder DesignIn Table 1(a), we evaluated various decoder structures (See Appendix A for detailed structures) to validate the effectiveness of weight-sharing decoder structure. As shown in Table 1(a), the computations increases about twice by using large decoder in late split or using early split methods. In particular, the model using multiple decoders after an early split yielded a performance comparable to that of using a large decoder after a late split. In contrast, by sharing a decoder after an early split, the separation result increased significantly, suggesting that the ESSD structure effectively discriminates between the separated features. This impact was more noticeable in the tiny models by showing increase of 1.5dB. Applying CS to ESSD improved the performance especially on the tiny model, leading to the SepRe mechanism. Although changing from a late split structure to an ESSD structure can increase computation if the channel size is kept constant, reducing the channel size allows us to still achieve better performance. This adjustment significantly improves computational efficiency in relation to performance. This is particularly evident when comparing the base model with late split to the tiny model with the proposed ESSD + CS (SepRe) structure. The latter model achieves a higher performance, with an SI-SNRi of 22.4 dB compared to 21.6 dB, while using fewer computations and a smaller model size, clearly demonstrating the efficiency of the model architecture.

Effects of multi-lossFurthermore, we experimented with the effects of multi-loss on various decoder structures in Table 1(b). Compared to a late split, the case with an early split increased more significantly with multi-loss because an early split structure could be trained with a clearer objective for discriminative learning using an intermediate loss at each stage. In particular, while applying only CS without multi-loss resulted in a marginal improvement, combining CS with multi-loss led to a substantial gain. The results demonstrated that stage-specific objective functions induce each CS-equipped weight-sharing decoder stage to effectively learn simultaneously how to discriminate between and attend to each other. As a result, our proposed SepRe method using ESSD and CS significantly improved separation performance by applying stage-specific objective functions and inducing progressive reconstruction of separated feature sequences.

Table 1: **Experimental evaluation of SepRe method on the WSJ0-2Mix dataset. ML denotes the multi-loss. In (a), all the methods were trained with ML, and the numbers in the left and right of the ’/’ symbol were obtained for the tiny and base models, respectively. In (b), when ML was used for training, we indicated the numbers of parameters including the additional output layer for an auxiliary output for \(}_{j}\), which were denoted with asterisk \({}^{*}\). Note that the additional output layers were not required during inference.**

Table 2: **Application of SepRe to other networks. From the original separator of Conv-TasNet and Sepformer, we applied the SepRe method with multi-loss (ML) and evaluated on the WSJ0-2Mix dataset.**

### Effects of the SepRe method in other networks

To validate the general applicability of the SepRe method, we incorporated the SepRe method with multi-loss into the original separators of Conv-TasNet  and Sepformer  and conducted experiments on WSJ0-2MIX. The experimental results in Table 2 demonstrated a significant performance improvement when ESSD was applied for both networks. Also, applying CS and multi-loss in addition to the ESSD framework improved the performance further, which confirms the effectiveness of SepRe with multi-loss.

### Ablation studies of unit blocks

Depth of encoder-decoderIn Table 3(a), we experimented the depth of encoder and decoder to determine the optimal configuration in terms of the number of block repetition \(B_{E}\) and \(B_{D}\). Generally, experimental results showed that using more blocks in the reconstruction decoder had a greater impact on performance improvement than in the separation encoder. It demonstrate that it is more important to discriminate the features more elaborately in weight-sharing decoder than to analyze the features in encoder in speech separation. In particular, optimal performance was achieved with \(B_{E}=2\) and \(B_{D}=3\), which were used as the common configuration for subsequent experiments.

EGA module designNext, we validated our proposed EGA module by ablating its components (see Appendix B for detailed structures) in Table 3(b). First of all, using vanilla MHSA on a long sequence without chunking was infeasible due to the extremely large computational requirements. Therefore, one approach was to perform downsampling before applying MHSA, similar to the method used in QDPN . However, this naive approach had the drawback of losing detailed frame-wise information. Although another consideration was to simply multiply the features to reflect the fine-grained frame-wise information, this method still could not significantly improve performance. In contrast, the optimal performance was achieved by estimating gate values based on a linear layer and a sigmoid function. As a result, it is shown that our proposed global Transformer with EGA module and local Transformer with CLA module have effectively replaced conventional sequence models with smaller computations.

FFN module designAlso, by improving the design of FFN with Dconv and GLU activation, we could achieve the significant improvement of performance with slight increase of parameters as shown in Table 3(c).

Comparison with various long sequence modelsIn Table 4, we evaluated the network by stacking our proposed global-local Transformer blocks to assess the performance of modeling a long sequence.

**Separator** & **Long sequence model** & **Param. (M)** & **MACs (G/s)** & **SI-SNRi** (dB) \\  Conv-TasNet  & TCN  & 5.1 & 10.5 & 15.6 \\ DPRNN  & Dual-path + BLSTM & 2.6 & 88.5 & 18.8 \\ SuDoRM-RF  & Multi-scale + Convolution & 6.4 & 10.1 & 18.9 \\ Sepformer  & Dual-path + Transformer & 26.0 & 86.9 & 20.4 \\ TDANet  & Multi-scale + Transformer & 2.3 & 9.1 & 18.5 \\ MossFormer(S)  & GAU  & 10.8 & 44.0 & 20.9 \\ S4M  & Multi-scale + SSM  & 3.6 & 38.4 & 20.5 \\  Ours & Global-Local Transformer & 11.9 & 43.1 & 21.3 \\ Ours + U-Net & Multi-scale + Global-Local Transformer & 11.6 & 18.3 & 21.2 \\  

Table 4: **Comparison with various long sequence models in speech separation of WSJ0-2Mix. MS denotes multi-scale. For our model, global and local blocks were repeated 22 times with \(F=128\).**

Table 3: **Ablation studies for unit blocks on our SepReformer-B on the WSJ0-2Mix dataset. Various configurations of \(B_{E}\) and \(B_{D}\) were evaluated to assess the relative importance of encoder and decoder. Also, we validated the proposed EGA and GCFN modules.**Note that we did not apply the ESSD structure and multi-loss to our separator in this experiment. We could observe that the network based on dual-path sequence models requires high computation resources in terms of MACs while multi-scale sequence models are more efficient. The recently proposed Mossformer based on efficient gate attention unit (GAU) mechanism  showed improved performance with relatively smaller computations compared to the networks with dual-path model. In particular, the proposed model showed improved separation performance with similar MACs, which demonstrated the capacity as a model for a long sequence. It also suggested that the proposed block can sufficiently replace the dual-path models with fewer computations. Furthermore, by combining the U-Net structure into global-local Transformer blocks, the network became more efficient with the similar separation performance.

### Comparison with existing models

Finally, we compared our SepReformer models with existing separation models on various benchmark datasets in Table 5. Although we evaluated SepReformer trained with standard pairs from the training set in Table 5(a), SepReformer-L was trained with dynamic mixing (DM)  for data augmentation and compared to other existing large models with DM in Table 5(b). When trained with DM, we set an initial learning rate of 2.0\(e^{-4}\) and fixed during first 50 epoch. In Table 5(a), with almost the smallest computational loads in terms of MACs, our tiny model showed the best performance except for TF-GridNet in the WSJ0-2Mix dataset which was a powerful model recently proposed. It demonstrated the efficiency of the SepRe method in speech separation. Also, SepReformer-M

  &  **Params.** \\ (M) \\  } &  **MACs** \\ (G/s) \\  } &  &  &  \\   & & &  **SI-SNRi** \\ (dB) \\  &  **SDRi** \\ (dB) \\  &  **SI-SNRi** \\ (dB) \\  &  **SDRi** \\ (dB) \\  &  **SI-SNRi** \\ (dB) \\  &  **SDRi** \\ (dB) \\  &  **SI-SNRi** \\ (dB) \\  & 
 **SDRi** \\ (dB) \\  \\   Conv-TasNet  & 5.1 & 10.5 & 15.3 & 15.6 & 12.7 & – & 12.2 & 12.7 \\ SuDokM-RF  & 6.4 & 10.1 & 18.9 & - & 13.7 & 14.1 & 14.0 & 14.4 \\ TDANet  & 2.3 & 9.1 & 18.5 & 18.7 & 15.2 & 15.4 & 17.4 & 17.9 \\ Sandglasset  & 2.3 & 28.8 & 20.8 & 21.0 & - & - & - & - \\ S4M  & 3.6 & 38.4 & 20.5 & 20.7 & - & - & 16.9 & 17.4 \\  SepReformer-T & 3.5 & 10.4 & 22.4 & 22.6 & 17.2 & 17.5 & 19.7 & 20.2 \\ SepReformer-S & 4.3 & 21.3 & 23.0 & 23.1 & 17.3 & 17.7 & 20.6 & 21.0 \\  DPRNN  & 2.6 & 88.5 & 18.8 & 19.0 & 13.7 & 14.1 & 16.1 & 16.6 \\ DPTNet  & 2.7 & 102.5 & 20.2 & 20.3 & 14.9 & 15.3 & 16.7 & 17.1 \\ Sepformer  & 26.0 & 86.9 & 20.4 & 20.5 & 14.7 & 16.8 & 16.5 & 17.0 \\ WaveSplit\({}^{}\) & 29.0 & - & 21.0 & 21.2 & 16.0 & 16.5 & 16.6 & 17.2 \\ A-FRCNN  & 6.1 & 125.0 & 18.3 & 18.6 & 14.5 & 14.8 & 16.7 & 17.2 \\ SFSRNet  & 59.0 & 124.2 & 22.0 & 22.1 & - & - & - & - \\ ISCIT\({}^{}\) & 58.4 & 252.2 & 22.4 & 22.5 & 16.4 & 16.8 & - & - \\ QDPN  & 200.0 & - & 22.1 & - & - & - & - & - \\ TF-GridNet  & 14.5 & 460.8 & 23.5 & 23.6 & - & - & - & - \\  SepReformer-B & 14.2 & 39.8 & 23.8 & 23.9 & 17.6 & 18.0 & 21.6 & 21.9 \\ SepReformer-M & 17.3 & 81.3 & 24.2 & 24.4 & 17.8 & 18.1 & 22.0 & 22.2 \\  

Table 5: **Evaluation on various benchmark dataset of WSJ0-2Mix, WHAM!, WHAM!, and Libri2Mix.** “\(\)” denotes that the networks use additional speaker information.

without DM in Table 5(a) showed competitive separation performance on WSJ0-2Mix compared to the large models with data augmentation in Table 5(b). In particular, SepReformer-L with DM achieved a state-of-the-art performance of 25.4 dB SI-SNRi on the WSJ0-2Mix dataset, demonstrating a significant improvement over other conventional methods.

In Table 5(a), the smallest SepReformer-T among the proposed models even showed significant improvements on WHAM! and Libri2Mix datasets compared to the conventional methods. It suggested that the proposed SepRe method can be efficiently applied to a speech separation task in general. Also, SepReformer-L with DM showed the SOTA performance on WHAM! and WHAMR! datasets, as well as WSJ0-2Mix, which demonstrated that the proposed method can be trained effectively in a large model. Figure 5 compares the separation performance of various existing methods in terms of SI-SNRi versus MACs on WSJ0-2MIX. From the figure, we can observe the significant effectiveness of the proposed SepReformer in the speech separation task with high computational efficiency. Especially, it is noteworthy that the SepReformer-T models outperformed the conventional Sepformer trained with DM with 10 times smaller computations.

## 6 Conclusion

In this work, we introduced the SepRe method, in which the asymmetric encoder and decoder perform separation and reconstruction, respectively. The encoder analyzes and separates a feature sequence, and the separated sequences are reconstructed by a weight-sharing network and a cross-speaker network. We demonstrated that the SepRe method can be applied to conventional separators in general and utilizing multi-loss significantly improves the performance. Moreover, we replaced the dual-path model with presented global and local Transformer blocks to address a long sequence. The separator using the presented unit blocks has shown enhanced separated results efficiently, and combining a U-Net structure to exploit the multi-scale sequence model has further increased the efficiency. Finally, not only did our presented SepReformer outperform the most conventional methods in speech separation even with almost the smallest computational resources, but our large models achieved SOTA performance with large margins compared to the conventional models on various speech separation datasets.

Limitations and future work.Our study focuses on 2-speaker mixture situation to assess our models in various model sizes and in the extensive datasets including noise and reverberation. Consequently, we believe that further investigation is needed to validate for more than 2-speaker mixture scenarios. Also, an important future direction is to separate mixtures for an unknown number of speakers as it is impractical to assume that the number of speakers to be separated is known in advance. Finally, although we experimentally validated our SepRe method, we believe that further investigation is necessary to figure out its underlying mechanism.

Figure 5: **Si-SNRi results on WSJ0-2Mix versus MACs (G/s) for the conventional methods and the proposed SepReformer. The check mark in the circle indicates the use of DM method for training. The radius of circle is proportional to the parameter size of the networks.**