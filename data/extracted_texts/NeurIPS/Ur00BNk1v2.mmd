# GenArtist: Multimodal LLM as an Agent for Unified Image Generation and Editing

Zhenyu Wang\({}^{1}\)  Aoxue Li\({}^{2}\)  Zhenguo Li\({}^{2}\)  Xihui Liu\({}^{3}\)

\({}^{1}\) Tsinghua University \({}^{2}\) Noah's Ark Lab, Huawei \({}^{3}\) The University of Hong Kong

wangzy20@mails.tsinghua.edu.cn, lax@pku.edu.cn,

Li.Zhenguo@huawei.com, xihuiliu@eee.hku.hk

This work is done when Zhenyu Wang was intern in HuaweiCorresponding author

###### Abstract

Despite the success achieved by existing image generation and editing methods, current models still struggle with complex problems including intricate text prompts, and the absence of verification and self-correction mechanisms makes the generated images unreliable. Meanwhile, a single model tends to specialize in particular tasks and possess the corresponding capabilities, making it inadequate for fulfilling all user requirements. We propose **GenArtist**, a _unified_ image generation and editing system, coordinated by a multimodal large language model (MLLM) agent. We integrate a comprehensive range of existing models into the tool library and utilize the agent for tool selection and execution. For a complex problem, the MLLM agent decomposes it into simpler sub-problems and constructs a tree structure to systematically plan the procedure of generation, editing, and self-correction with step-by-step verification. By automatically generating missing position-related inputs and incorporating position information, the appropriate tool can be effectively employed to address each sub-problem. Experiments demonstrate that GenArtist can perform various generation and editing tasks, achieving state-of-the-art performance and surpassing existing models such as SDXL and DALL-E 3, as can be seen in Fig. 1. Project page is https://zhenyuw16.github.io/GenArtist_page/.

## 1 Introduction

With the recent advancements in diffusion models , image generation and editing methods have rapidly progressed. Current improvements in image generation and editing can be broadly categorized into two tendencies. The first  involves training from scratch using more advanced model architectures  and larger-scale datasets, thereby scaling up existing models to achieve a more general generation or editing capability. These methods can usually enhance the overall controllability and quality of image generation. The second is primarily about finetuning or additionally designing pre-trained large-scale image generation models on specific datasets to extend their capability  or enhance their performance on certain tasks . These methods are usually task-specific and can demonstrate advantageous results on some particular tasks.

Despite this, current image generation or editing methods are still imperfect and confront some urgent challenges on the way to building a human-desired system: 1) The demand for image generation and editing is highly diverse and variable, like various requirements for objects and backgrounds, numerous demands about various operations in text prompts or instructions. Meanwhile, different models often possess different strengths and focus. General models may be weaker than some finetuned models in certain aspects, but they can exhibit better performance in out-of-distribution data.

## 1 Introduction

Figure 1: **Visualized examples from GenArtist.** It can accomplish various tasks, achieving unified generation and editing. For text-to-image generation, it obtains greater accuracy compared to existing models like SDXL and DALL-E 3. For image editing, it also excels in complex editing tasks.

Therefore, it is nearly impossible for a well-trained model to meet all human requirements, and the use of only a single model is often sub-optimal. 2) Models still struggle with complex problems, such as lengthy and intricate sentences in text-to-image tasks or complicated instructions with multiple steps in editing tasks. Scaling up or finetuning models can alleviate this issue. However, since texts are highly variable, flexible, and can be easy to combine, there are always complex problems that a trained model cannot effectively handle. 3) Although meticulously designed, models still inevitably encounter some failure cases. Generated images sometimes fail to accurately correspond to the content of user prompts. Existing models lack the ability to autonomously assess the correctness of generated images, not to mention self-correcting them, making generated images unreliable. What we truly desire, therefore, should be _a unified image generation and editing system_, which can satisfy nearly all human requirements while producing reliable image results.

In this paper, we propose a unified image generation and editing system called **GenArtist** to address the above challenges. Our fundamental idea is to utilize a multimodal large language model (MLLM) as an AI agent, which acts as an "artist" and "draws" images according to user instructions. Specifically, in response to user instructions, the agent will analyze the user requirements, decompose complex problems, and conduct planning comprehensively to formulate the specific solutions. Then, it executes image generation or editing operations by invoking external tools to meet the user demands. After images are obtained, it finally performs verification and correction on the generated results to further ensure the accuracy of the generated images. The core mechanisms of the agent are:

**Decomposition of intricate text prompts.** The MLLM agent first decomposes the complex problems into several simple sub-problems. For complicated text prompts in generation tasks, it extracts single-object concepts and necessary background elements. For complex instructions in editing tasks, it breaks down intricate operations into several simple single editing actions. The decomposition of complex problems significantly improves the reliability of model execution.

**Planning tree with step-by-step verification.** After decomposition, we construct a tree structure to plan the execution of sub-tasks. Each operation is a node in the tree, with subsequent operations as its child nodes, and different tools for the same action are its sibling nodes. Each node is followed by verification to ensure that its operation can be executed correctly. Then, both generation, editing, and self-correction mechanisms can be incorporated. Through this planning tree, the proceeding of the system can be considered as a traversal process and the whole system can be coordinated.

**Position-aware tool execution.** Most of object-level tools require position-related inputs, like the position of the object to be manipulated. These necessary inputs may not be provided by the user. Existing MLLMs are also position-insensitive, and cannot provide accurate positional guidance. We thus introduce a set of auxiliary tools to automatically complete these position-related inputs, and incorporate position information for the MLLM agent through detection models for tool execution.

Our main contributions can be summarized as follows:

* We propose GenArtist, a unified image generation and editing system. The MLLM agent serves as the "brain" to coordinate and manage the entire process. To the best of our knowledge, this is the first unified system that encompasses the vast majority of existing generation and editing tasks.
* Through viewing the operations as nodes and constructing the planning tree, our MLLM agent can schedule for generation and editing tasks, and automatically verify and self-correct generated images. This significantly enhances the controllability of user instructions over images.
* By incorporating position information into the integrated tool library and employing auxiliary tools for providing missing position-related inputs, the agent performs tool selection and invokes the most suitable tool, providing a unified interface for various tasks in generation and editing.

Extensive experiments demonstrate the effectiveness of our GenArtist. It achieves more than 7% improvement compared to DALL-E 3  on T2I-CompBench , a comprehensive benchmark for open-world compositional T2I generation, and also obtains the state-of-the-art performance on the image editing benchmark MagicBrush . As can be seen in the visualized examples in Fig. 1, GenArtist well serves as a unified image generation and editing system.

## 2 Related Work

**Image generation and editing.** With the development of diffusion models [10; 17], both image generation and editing have achieved remarkable success. Many general text-to-image generation [41;43, 37, 6] and editing methods  have been proposed and achieved high-quality generated images. Based on these general models, many methods conduct finetuning or design additional modules for some specialized tasks, like customized image generation , image generation with text rendering , exemplar-based image editing , image generation that focuses on persons . Meanwhile, some methods aim to improve the controllability of texts over images. For example, ControlNet  controls Stable Diffusion with various conditioning inputs like Canny edges,  adopts sketch images for conditions, and layout-to-image methods  synthesize images according to the given bounding boxes of objects. Despite the success, these methods still focus on specific tasks, thus unable to support unified image generation and editing.

**AI agent.** Large language models (LLMs), like ChatGPT, Llama , have demonstrated impressive capability in natural language processing. The involvement of vision ability for multimodal large language models (MLLMS), like LLaVA , Claude, GPT-4 , further enables the models to process visual data. Recently, LLMs begin to be adopted as agents for executing complex tasks. These works  apply LLMs to learn to use tools for tasks like visual interaction, speech processing, compositional visual tasks , software development , gaming , APP use  or math . Recently, the idea of AI agents has also begun to be applied to image generation related tasks. For example,  design scene layout with LLMs,  utilizes LLMs to assist self-correcting,  target at MLLMs in complex text-to-image generation problems, and  leverages LLM for model selection in the text-to-image generation task.

## 3 Method

The overview of our GenArtist is illustrated in Fig. 2. The MLLM agent coordinates the whole system. Its primary responsibilities center around decomposing the complicated tasks and constructing the planning tree with step-by-step verification for image generation, editing, and self-correction. It invokes tools from an image generation tool library and an editing tool library to execute the specific operations, and an auxiliary tool library serves to provide missing position-related values.

### Planning Tree with Step-by-Step Verification

**Decomposition.** When it comes to complicated prompt inputs, existing methods usually cannot understand all requirements, which hurts the controllability and reliability of model results. The MLLM agent thus first decomposes the complex problems. For generation tasks, it decomposes both object and background information according to the text prompts. It extracts the discrete objects embedded within the text prompts, along with their associated attributes. For background information, it mainly analyzes the overall scene and image style required by the input texts. For editing tasks, It decomposes complex editing operations into several specific actions, such as add, move, remove, into simple editing instructions. After decomposition, the simpler operations can be relatively easier to address, which thus improves the reliability of model execution.

Figure 2: **The overview of our GenArtist. The MLLM agent is responsible for decomposing problems and planning using a tree structure, then invoking tools to address the issues. Employing the agent as the ”brain” effectively realizes a unified generation and editing system.**

**Tree construction.** After decomposition, we organize all operations into a structure of tree for planning. Such a tree primarily consists of three types of nodes: initial nodes, generation nodes, and editing nodes. The initial node serves as the root of the tree, marking the beginning of the system. Generation nodes are about image generation using tools from the generation tool library, while editing nodes are about performing a single editing operation using the corresponding tools from the editing tool library. For pure image editing tasks, the generation nodes will be absent.

In practice, as the correctness of generated images cannot be guaranteed, we introduce the self-correction mechanism to assess and rectify the results of generation. Each generation node thus has a sub-tree consisting entirely of editing nodes for self-correction. After the tools in the generation nodes are invoked and verification is conducted, this sub-tree will be adaptively generated by the MLLM agent. Specifically, after verification, we instruct the MLLM agent to devise a series of corresponding editing operations to correct the images. Take the example in Fig. 2 for example, editing actions including "add a black bicycle", "edit the color of the scooter to blue", "add a bird" should be conducted. These operations are organized into a tree structure to be the sub-tree of the generation node, allowing for specific planning of self-correction.

Each generation or editing action corresponds to a node in the tree, with its subsequent operations as its child nodes. This construction initially forms a "chain", enabling a planning chain. Then, we note that we can usually utilize different tools to address the same problem. For example, for adding an object into the image, we can employ a tool specifically designed for object addition or instruction-based editing models by translating the adding operation into text instructions. Similarly, for attribute editing, we can use attribute editing models or utilize replacement or instruction-based editing models. Moreover, numerous generation tools can achieve text-to-image generation, and varying the random seeds can also produce different outputs. We consider these nodes as siblings, all serving as child nodes of their parent nodes. They also share the same sub-tree, containing subsequent editing operations. The tool selected by the MLLM agent will be placed as the optimal child node and positioned on the far left. In this way, we establish the structure of the tree. An illustration example for the Fig. 2 case is provided in Fig. 3 (we omit some sub-trees with identical structures or adaptively generated after generation nodes, and some nodes about varying random seeds for simplicity).

**Planning.** Once the tree is established, planning for self-correction or the whole system can be viewed as the pre-order traversal of the structure. For a particular node, its corresponding tool is invoked to conduct the operation, followed by verification to determine whether the editing is successful. If successful, the process proceeds to its leftmost child node for subsequent operations, and its sibling nodes are deleted. If unsuccessful, the process backtracks to its sibling nodes, and its sub-tree is removed. This process continues until the generated image is correct, _i.e._, when a node at the lowest level successfully executes. We can also limit the branching factor or the number of nodes of the tree for early termination, and require the agent to return the most accurate image.

**Verification.** As described above, the verification mechanism plays a crucial role both in tree construction and the execution process. Through the multimodal perception capability of the MLLM, the agent verifies the correctness of the generated images. The main aspect of verification involves the objects contained in the text, together with their own attributes like their color, shape, texture, the positions of the objects, the relationship among these different objects. Besides, the background, scene, overall style and the aesthetic quality of generated images are also considered. Since the perception ability of existing models tends to be superior to the generative ability, employing such verification allows for effectively assessing the correctness of generated images.

It is also worth mentioning that during verification, in addition to the accuracy of the generated images, the agent is also required to assess their aesthetic quality. If the overall quality is poor, the agent will utilize different generation tools or choose different random seeds to regenerate the images,

Figure 3: **Illustration of the tree for planning.** The sub-tree of the “alternative generation tool” node will be adaptively generated after verification, and the sub-tree of the “instruction” node is the same as the left.

in order to ensure their overall quality. Meanwhile, as an agent-centered system, the framework is also flexible in terms of human-computer interaction. During verification, human feedback can be appropriately integrated. By incorporating human evaluation and feedback on the overall quality of the images, the quality of the generated images can be further improved.

### Tool Library

After constructing the planning tree, the agent proceeds to execute each node by calling external tools, ultimately solving the problem. We first introduce the tools used in GenArtist. The primary tools that the MLLM agent utilizes can be generally divided into the image generation tool library and the editing tool library. The specific tools we utilize currently are listed in Tab. 1, and some new tools can be seamlessly added, allowing for the expansion of the tool library. To assist the subsequent tool selection, we need to convey information to the MLLM agent about the specific task performed by the tool, its required inputs, and its characteristics and advantages. The prompts for introducing tools consist of the following parts specifically:

* The tool skill and name. It briefly describes the tool-related task and its name, as listed in Tab. 1, such as (text-to-image, SDXL), (canny-to-image, ControlNet), (object removal, LaIa). It serves as a unique identifier, enabling the agent to differentiate the utilized tools.
* The tool required inputs. It pertains to the specific inputs required for the execution of the tool. For example, text-to-image models require "text" as input for generation, customization models also need "subject images" for personalized generation. Most of object-level editing tools demand instructions about "object name" and "object position".
* The tool characteristic and advantage. It primarily provides a more detailed introduction of the tool, including its specific characteristics, serving as a key reference for the agent during tool selection. For example, SDXL can be a general text-to-image generation model, LMD usually controls scene layout strictly and is suitable for compositional text-to-image generation, where text prompts usually contain multiple objects, BoxDiff controls scene layout relatively loosely, TextDiffuser is specially designed for image generation with text rendering.

### Position-Aware Tool Execution

With tool libraries, the MLLM agent will further perform tool selection and execution to utilize the suitable tool for fulfilling the image generation or editing task. Before tool execution, we compensate for the deficiency of position information in user inputs and the MLLM agent through two designs:

**Position-related input compensation.** In practice, it is common to encounter scenes where the agent selects a suitable tool but some necessary user inputs are missing. These user inputs are mostly related to positions. For example, for some complex text prompts where multiple objects exist, the layout-to-image tool can be suitable. However, users may not necessarily provide the scene layouts and usually only text prompts are provided. In such cases, due to the absence of some necessary inputs, these suitable tools cannot be directly invoked. We therefore introduce the auxiliary tool library to provide these position-related missing inputs. This auxiliary tool library mainly contains: 1) localization models like object detection  or segmentation  models, to provide position

    &  \\ skill & tool & skill & tool \\  text-to-image & SDXL  & & \\ text-to-image & PixArt-\(\) & object addition & AnyDoor  \\ image-to-image & Stable Diffusion v2  & object removal & LaMa  \\ layout-to-image & LMD  & object replacement & AnyDoor  \\ layout-to-image & BoxDiff  & attribute editing & DiffEdit  \\ single-object customization & BLIP-Diffusion  & instruction-based & MagicBrush  \\ multi-object customization & \(\)-ECLIPSE  & dragging (detail) & DragDiffusion  \\ super-resolution & SDXL  & dragging (object) & DragonDiffusion  \\ image with texts & TextDiffuser  & style transfer & InST  \\ \{canny, depth...}-to-image & ControlNet  & & \\   

Table 1: **GenArtist utilized tool library, including the tool names and their skills. The main tools are from the generation tool library and the editing tool library. The following models represent all the tools used in our current version, while new models can be seamlessly added.**information of objects for some object-level editing tools; 2) the preprocessors of ControlNet  like the pose estimator, canny edge map extractor, depth map extractor; 3) some LLM-implemented tools, like the scene layout generator [25; 13]. The MLLM agent can invoke these auxiliary tools automatically if necessary, to guarantee that the most suitable tool to address the user instruction can be utilized, rather than solely relying on user-provided inputs to select tools.

**Position information introduction.** Existing MLLMs primarily focus on text comprehension and holistic image perception, with relatively limited attention to precise position information within images. MLLMs can easily determine whether objects exist in the image, but sometimes struggle with discerning spatial relationships between objects, such as whether a specific object is to the left or right of another. It is also more challenging for these MLLMs to provide accurate guidance for tools that require position-related inputs, such as object-level editing tools. To address this, we employ an object detector on the input images, and include the detected objects along with their bounding boxes as part of the prompt, to provide a spatial reference for the MLLM agent. In this way, the agent can effectively determine the positions within the image where certain tools should operate.

The prompts for the agent to conduct tool selection thus mainly consist of the following parts:

* Task instruction. Its main purpose is to clarify the task of the agent, _i.e._, tool selection within a unified generation and editing system. Simultaneously, it takes user instructions as input and specifies the output format. We request the agent to output in the format of {"tool_name":tools, "input":inputs} and annotate missing inputs with the pre-defined specified identifier.
* Tool introductions. We input the description of each tool into the agent in the format as described earlier. The detailed information about the tools will serve as the crucial references for the tool selection process. We also state that the primary criterion for tool selection is the suitability of the tool, rather than the content of given inputs, since missing inputs can be generated automatically.
* Position information. The outputs from the object detector are utilized and provided to the MLLM agent to compensate for the lack of position information.

In summary, the basic steps for tool execution are as follows: First, determine whether the task pertains to image generation or editing. Next, conduct tool selection according to the instructions and the characteristics of the tools, and output in the required format. Finally, for missing inputs which are necessary for the selected tools, utilize auxiliary tools to complete them. Upon completing these steps, the agent will be able to correctly execute the appropriate tools, thereby initially meeting the requirements of users. The integration, selection, and execution of diverse tools significantly facilitate the development of a unified image generation and editing system.

## 4 Experiments

In this section, we demonstrate the effectiveness of our GenArtist and its unified ability through extensive experiments in image generation and editing. For image generation, we mainly conduct

    &  &  &  \\    & **Color\(\)** & & **Shape\(\)** & **Texture\(\)** & **Spatial\(\)** & **Non-Spatial\(\)** \\  Stable Diffusion v1.4  & 0.3765 & 0.3576 & 0.4156 & 0.1246 & 0.3079 & 0.3080 \\ Stable Diffusion v2  & 0.5065 & 0.4221 & 0.4922 & 0.1342 & 0.3096 & 0.3386 \\ DALL-E 2  & 0.5750 & 0.5464 & 0.6374 & 0.1283 & 0.3043 & 0.3696 \\ Composable Diffusion  & 0.4063 & 0.3299 & 0.3645 & 0.0800 & 0.2980 & 0.2898 \\ StructureDiffusion  & 0.4990 & 0.4218 & 0.4900 & 0.1386 & 0.3111 & 0.3355 \\ Attn-Exct  & 0.6400 & 0.4517 & 0.5963 & 0.1455 & 0.3109 & 0.3401 \\ GORS  & 0.6603 & 0.4785 & 0.6287 & 0.1815 & 0.3193 & 0.3328 \\ SDXL  & 0.5879 & 0.4687 & 0.5299 & 0.2133 & 0.3119 & 0.3237 \\ PixArt-\(\) & 0.6690 & 0.4927 & 0.6477 & 0.2064 & 0.3197 & 0.3433 \\ CompaAgent  & 0.7760 & 0.6105 & 0.7008 & 0.4837 & 0.3212 & 0.3972 \\ DALL-E 3  & 0.7785 & 0.6205 & 0.7036 & 0.2865 & 0.3003 & 0.3773 \\ 
**GenArtist (ours)** & **0.8482** & **0.6948** & **0.7709** & **0.5437** & **0.3346** & **0.4499** \\   

Table 2: **Quantitative Comparison on T2I-CompBench with existing text-to-image generation models and compositional methods**. Our method demonstrates superior compositional generation ability in both attribute binding, object relationships, and complex compositions. We use the officially updated code for evaluation, which updates the noun phrase number. Consequently, some metric values for certain methods may be lower than those reported in their original papers.

quantitative comparisons on the recent T2I-CompBench benchmark . It is mainly about image generation with complex text prompts, involving multiple objects together with their own attributes or relationships. For image editing, we mainly conduct comparisons on the MagicBrush benchmark , which involves multiple types of text instructions, both single-turn and multi-turn dialogs for image editing. We choose GPT-4V  as our MLLM agent. In quantitative comparative experiments, we constrain the editing tree to be a binary tree.

### Comparison with Image Generation Methods

We list the quantitative metric results of our GenArtist in Tab. 2 and compare with existing state-of-the-art text-to-image synthesis methods. It can be seen that our GenArtist consistently achieves better performance on all sub-categories. This demonstrates that for the text-to-image generation task, our system effectively achieves better control over text-to-image correspondence and higher accuracy in generated images, especially in the case of complicated text prompts. It can be observed that based on Stable Diffusion, both scaling-up models such as SDXL, PixArt-\(\), and those methods specifically designed for this context like Attn-Exct, GORS, can achieve higher accuracy. In contrast, our approach, by integrating various models as tools, effectively harnesses the strengths of these two categories of methods. Additionally, the self-correction mechanism further ensures the accuracy of the generated images. Compared to the current state-of-the-art model DALL-E 3, our method achieves nearly a 7% improvement in attribute binding, and a more than 20% improvement in spatial relationships, partly due to the inclusion of position-sensitive tools and the input of position information during tool selection. Compared to CompAgent, a method that also employs an AI agent for compositional text-to-image generation, GenArtist achieves a 6% improvement on average, partially because our system encompasses a more comprehensive framework for both generation and self-correction. The capability in image generation of our unified system can thus be demonstrated.

### Comparison with Image Editing Methods

We then list the comparative quantitative comparisons on the image editing benchmark MagicBrush in Tab. 3. Our GenArtist also achieves superior editing results, no matter in the single-turn or multi-turn setting, compared to both previous global description-guided methods like Null Text Inversion and instruction-guided methods like InstructPix2Pix and MagicBrush. The main reason is that editing operations are highly diverse, and it's challenging for a single model to achieve excellent performance across all these diverse editing operations. In contrast, our method can leverage the strengths of different models comprehensively. Additionally, the planning tree can effectively consider scenarios where model execution fails, making editing results more reliable and accurate. The capability in image editing of our unified system can thus be demonstrated.

### Ablation Study

We finally conduct the ablation study on the T2I-CompBench benchmark and list the results in Tab. 4. We present the results of Stable Diffusion as a reference. The top section includes various tools relevant to the task, including text-to-image, layout-to-image, and customized generation methods. It can be observed that through scaling up or additional design, these tools have generally achieved

  
**Settings** & **Methods** & **LI\(\)** & **L\(\)** & **CLIP-I\(\)** & **DINO\(\)** & **CLIP-T\(\)** \\   & Null Text Inversion  & 0.0749 & 0.0197 & 0.8827 & 0.8206 & 0.2737 \\  & HIVE  & 0.1092 & 0.0341 & 0.8519 & 0.7500 & 0.2752 \\  & InstructPix2Pix  & 0.1122 & 0.0371 & 0.8524 & 0.7428 & 0.2764 \\  & MagicBrush  & 0.0625 & 0.0203 & 0.9332 & 0.8987 & 0.2781 \\  & SmartEdit  & 0.0810 & - & 0.9140 & 0.8150 & 0.3050 \\  & **GenArtist (ours)** & **0.0536** & **0.0147** & **0.9403** & **0.9131** & **0.3129** \\   & Null Text Inversion  & 0.1057 & 0.0335 & 0.8468 & 0.7529 & 0.2710 \\  & HIVE  & 0.1521 & 0.0557 & 0.8004 & 0.6463 & 0.2673 \\   & InstructPix2Pix  & 0.1584 & 0.0598 & 0.7924 & 0.6177 & 0.2726 \\   & MagicBrush  & 0.0964 & 0.0353 & 0.8924 & 0.8273 & 0.2754 \\   & **GenArtist (ours)** & **0.0858** & **0.0298** & **0.9071** & **0.8492** & **0.3067** \\   

Table 3: **Quantitative Comparison on MagicBrush with existing image editing methods. Multi-turn setting evaluates images that iteratively edited on the previous source images in edit sessions.**better results than Stable Diffusion. After tool selection by the MLLM agent, the quantitative metrics outperform all these tools. This demonstrates that the agent can effectively choose appropriate tools based on the content of text prompts, thus achieving superior performance compared to all these tools. If we use a chain structure for planning to further correct the images, we achieve an average improvement of 3%, demonstrating the necessity of verification and correction of erroneous results. Furthermore, by utilizing a tree structure, we can further consider and handle cases where the editing tool fails, resulting in even more reliable output results. Such an ablation study illustrates the necessity of integrating multiple models as tools and utilizing tree structure for planning. The reasonableness of our agent-centric system designs can also be demonstrated.

Regarding position-aware tool execution, we list the corresponding ablation study in Tab. 5. We evaluate the performance on the spatial and complex aspects of T2I-CompBench, as these two aspects mainly involve position-sensitive text prompts for image generation. As multi-modal large models are usually not sensitive to position information, the performance is limited without the inclusion of position information, only a slight improvement over the tool selection results. After introducing position information, which enhances spatial awareness, there is a significant improvement in both the spatial and complex aspects. This validates the reasonability of our design.

We further list some visualized generation examples in Fig. 4 to illustrate our planning tree and how the system proceeds. In the first example, as the text prompts contain multiple objects, the agent chooses the LMD tool for generation. However, there are still some errors in the image. The agent first attempts to use the attribute editing tool to change the leftmost sheep to white, but it fails. The agent further attempts to modify the color using the replace tool, but after replacement, the size of the sheep becomes too small and not very noticeable. The agent then chooses to remove the black sheep and then adds a white sheep, successfully achieving the same effect as editing color. Finally, the agent uses the object addition tool to add a goat on the right side, ensuring that the image accurately matches the text prompt in the end. In the second example, due to the lack of clarity of the hair in the BoxDiff generated image, the editing tools cannot edit so that the hair correctly matches the description of "long black hair". Therefore, the agent invokes another generation tool to guarantee the final image is correct. Some image editing examples are also provided in Fig. 5.

    &  &  \\   & **Color** \(\) & **Shape**\(\) & **Texture**\(\) & **Spatial**\(\) & **Non-Spatial**\(\) & **Complex**\(\) \\  Stable Diffusion v2  & 0.5065 & 0.4221 & 0.4922 & 0.1342 & 0.3096 & 0.3386 \\ LMD  & 0.5736 & 0.5334 & 0.5227 & 0.2704 & 0.3073 & 0.3083 \\ BoxDiff  & 0.6374 & 0.4869 & 0.6100 & 0.2625 & 0.3158 & 0.3457 \\ \(\)-ECLIPSE  & 0.4581 & 0.4420 & 0.5084 & 0.1285 & 0.2922 & 0.3131 \\ SDXL  & 0.5879 & 0.4687 & 0.5299 & 0.2133 & 0.3119 & 0.3237 \\ PixArt-\(\) & 0.6690 & 0.4927 & 0.6477 & 0.2064 & 0.3197 & 0.3433 \\    & 0.7028 & 0.5764 & 0.6883 & 0.4305 & 0.3187 & 0.3739 \\  & 0.7509 & 0.6045 & 0.7192 & 0.4787 & 0.3216 & 0.4095 \\  & **0.8482** & **0.6948** & **0.7709** & **0.5437** & **0.3346** & **0.4499** \\   

Table 4: **Ablation Study on T2I-CompBench**. The upper section is about relevant tools from the generation tool library, then we study the tool selection and planning mechanisms respectively.

Figure 4: **Visualization of the planning tree for image generation tasks.**

   & **Spatial**\(\) & **Complex**\(\) \\  w/o position information & 0.4577 & 0.4083 \\ w/ position information & 0.5437 & 0.4499 \\   

Table 5: **Ablation Study on the position-aware tool execution on T2I-CompBench.**

### Error Case Analysis

We further analyze some error cases from our GenArtist in Fig. 6. As can be seen, sometimes, despite the agent correctly planning the specific execution of tools, the limitations of the tools themselves prevent correct execution, leading to incorrect results. For example, in the first case, it is required to add a very small blue cup. However, due to the lack of fine resolution ability in existing editing tools, the generated blue cup's size is inaccurate. In addition, as shown in the second case, errors in the output of localization tools can also affect the final result. For instance, when asked to remove the lettuce in the middle of a sandwich, the segmentation model fails to accurately identify the part of the object, leading to the erroneous removal operation. Utilizing more powerful tools or incorporating some human feedback during the verification stage can effectively address this issue.

## 5 Conclusion

In this paper, we propose GenArtist, a unified image generation and editing system coordinated by a MLLM agent. By decomposing input problems, employing the tree structure for planning and invoking external tools for execution, the MLLM agent acts as the "brain" to generate high-fidelity and accurate images for various tasks. Extensive experiments demonstrate that GenArtist well addresses complex problems in image generation and editing, and achieves state-of-the-art performance compared to existing methods. Its ability in a wide range of generation tasks also validates its unified capacity. We believe our approach of leveraging the agent to achieve a unified image generation and editing system with enhanced controllability can provide valuable insights for future research, and we consider it an important step toward the future of autonomous agents.