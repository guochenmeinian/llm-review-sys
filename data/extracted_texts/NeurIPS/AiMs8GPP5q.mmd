# Contrastive-Equivariant Self-Supervised Learning Improves Alignment with Primate Visual Area IT

Thomas Yerxa \({}^{1}\)1 &Jenelle Feather \({}^{1,2}\) &Eero P. Simoncelli \({}^{1,2}\) &SueYeon Chung\({}^{1,2}\)

\({}^{1}\)Center for Neural Science, New York University

\({}^{2}\)Center for Computational Neuroscience, Flatiron Institute, Simons Foundation

###### Abstract

Models trained with self-supervised learning objectives have recently matched or surpassed models trained with traditional supervised object recognition in their ability to predict neural responses of object-selective neurons in the primate visual system. A self-supervised learning objective is arguably a more biologically plausible organizing principle, as the optimization does not require a large number of labeled examples. However, typical self-supervised objectives may result in network representations that are overly invariant to changes in the input. Here, we show that a representation with structured variability to input transformations is better aligned with known features of visual perception and neural computation. We introduce a novel framework for converting standard invariant SSL losses into "contrastive-equivariant" versions that encourage preservation of input transformations without supervised access to the transformation parameters. We demonstrate that our proposed method systematically increases the ability of models to predict responses in macaque inferior temporal cortex. Our results demonstrate the promise of incorporating known features of neural computation into task-optimization for building better models of visual cortex.

## 1 Introduction

In the past decade, task-optimized deep neural networks (DNNs) have been used to predict responses of object-selective neurons in primates to natural image stimuli (Yamins et al., 2014; Schrimpf et al., 2020; Willeke et al., 2023). Such networks have a pronounced advantage over more traditional models for explaining responses in deeper areas with more abstract representations, such as inferior temporal cortex (IT). This observation naturally leads to the hypothesis that task optimization can provide a normative account for IT neuron tuning properties: late-stage visual representations are shaped by the need to perform ecologically relevant tasks.

However, the task that initially led to these advances was that of supervised object classification, a specific task that relies on an implausibly large number of labeled examples (Lindsay, 2021). More recently, computer vision has undergone a "self-supervised learning" (SSL) revolution. A variety of methods have been proposed to learn representations that match or surpass supervised training on multiple tasks by deriving sources of supervision from the data itself rather than relying on human annotations. For example, many popular SSL strategies aim to unify representations of different transformations of the same image (commonly referred to as "views"), while enforcing diversity among representations of distinct images. Additionally, self-supervised representations can predict primate neural responses with fidelity comparable to supervised representations (Zhuang et al., 2021; Konkle and Alvarez, 2022; Parthasarathy et al., 2024).

Both of these training objectives are forms of invariance learning: responses of an ideal object classification model should be invariant across different objects from the same class, and self-supervised learning strives to achieve invariance to the transformations used to generate different views. However biological visual representations are not fully invariant across views . Indeed it has been demonstrated that training according to either of these two objectives leads to representations that are invariant to stimulus perturbations that are salient to human observers . Additionally, even in Area IT, which is thought to subserve invariant object recognition, neural populations encode a significant amount of "category orthogonal" information (e.g., object pose or viewing conditions that are unrelated to semantic category) . Furthermore, such selectivity for object-orthogonal attributes is meaningfully organized within Area IT  (i.e. object orthogonal attributes are linearly decodable from population responses). Whether such structured variability emerges in invariance-trained networks is likely determined by the uncontrolled inductive biases of the network architecture .

Here, we develop an equivariant learning framework that encourages such structured variability in network representations. Our contributions are:

* We propose a novel framework that converts standard invariance-based self-supervised learning methods into "contrastive-equivariant" versions that produce structured, transformation-related variability. Unlike previous approaches, our method does not require supervised access to transformation parameters or costly modifications to the training procedure.
* We examine the tradeoff between invariance and structured variability through a series of representational analyses. We find that, relative to networks trained for invariance alone, our contrastive-equivariant network learns structured transformation variability that is shared across images and factorized with respect to variability related to changes in image content.
* We explore the impact of including an equivariant loss for predicting neural activity in IT, showing for the first time that explicitly encouraging structured variability via optimization leads to an improved ability to predict cortical responses to natural images.

## 2 Method

### Transformation-Invariant Self-Supervised Learning (iSSL)

The influential work of  showed that applying two random transformations (often called "augmentations") to a batch of images, then training a network to identify which pairs of transformed images originated from the same sample with a cross-entropy style loss (the InfoNCE loss, first formulated in ) yields representations that are competitive with supervised training for object classification. Many subsequent studies have developed alternative objective functions that produce similar results: Barlow Twins, VICReg , and W-MSE  enforce augmentation invariance along with a constraint that the global covariance matrix is the identity; SimSiam  and BYOL  employ architectural constraints that regularize towards uniform representations and simply optimize for transformation invariance. Other studies have formalized the problem in terms of maximizing information  or capacity , subject to an invariance constraint, which has enabled connections to normative theories of coding efficiency and manifold capacity .

To formalize the definition of iSSL, we denote a dataset of images (e.g., ImageNet) by \(X^{N D}\), where \(N\) is the number of images and \(D\) is their dimensionality (number of pixels). Let \((;):^{D}^{D}\) be a function parameterized by \(\) that maps images to images (for example, for \(\) a random crop operation, \(\) specifies the region to be cropped). The goal of iSSL algorithms is to learn the parameters \(W\) of some function \(f(;W):^{D}^{d}\) such that the variability over \(\) is minimal while preserving variability over \(X\) (to avoid trivial solutions such as \(f(;W)=0\) for all inputs). Many methods achieve this by observing pairs of randomly augmented views of a batch of images: \(X^{A}=(X;_{1})\), \(X^{B}=(X;_{2})\), with \(_{1},_{2} p()\) where \(p()\) is a pre-chosen probability distribution over augmentation parameters. Generally iSSL frameworks employ an objective function that operates on the outputs of \(f\), \(Z^{A}=f(X^{A};W)\), \(Z^{B}=f(X^{B};W)\). One popular framework is "Barlow Twins" (Zbontar et al., 2021), which uses the objective: \(_{BT}=_{i}(1-_{ii})^{2}+_{i,i j}( _{ij})^{2}\) where \(\) is the cross-correlation matrix between \(Z^{A}\) and \(Z^{B}\). The first term encourages the outputs in response to the same image subject to different augmentations to be correlated, while the second encourages the outputs in response to distinct images to be uncorrelated.

Because complete invariance to the transformations employed in iSSL is harmful for downstream tasks, most frameworks employ a learnable "projector network" that maps the outputs of the representation network to an embedding space before applying the loss. The nearly ubiquitous use of this "guillotine regularization" (Bordes et al., 2022), means that most iSSL methods aim to learn a function _from which an augmentation invariant subspace can be extracted_. While this approach does permit some transformation-related variability in the representation, there is no explicit control or encouragement of that variability, and no incentive for that variability to be usefully structured.

### Contrastive-Equivariant Self-Supervised Learning (CE-SSL)

To induce structured variability in learned representations, we require that an equivariant subspace can be extracted from \(f\) alongside the invariant subspace described above. A function is equivariant to a set of input transformations if there exists a corresponding set of output transformations that induce the same changes. In the self-supervised learning setting this property can be expressed as:

\[_{} P,\ \  x X,\ \  T_{}:f(_{}(x) )=T_{}(f(x)),\] (1)

where \(_{}=(;)\) and \(P\) is the set of possible values of transformation parameters. Note that invariance is a special case of equivariance, in which \(T_{}\) is an identity transformation for all \(\). To avoid this degenerate solution, we will require both that similarly transformed inputs be related by the same transformation in the output space, and that differently transformed inputs are related to each other by different transformations.

Our training methodology, summarized in Fig. 1, follows the principle first proposed by (Gupta et al., 2023): "Equivariance should be learned from pairs of data, as in invariant contrastive learning." First we split our dataset of images into two random non-overlapping equal-sized partitions \(X_{1},X_{2}^{ D}\). Next we apply a randomly selected augmentation to both \(X_{1}\) and \(X_{2}\), so that corresponding rows of \(X_{1}^{A/B}\) and \(X_{2}^{A/B}\) contain distinct images that have been subjected to the same augmentations.

Figure 1: Diagram of the proposed training method. At the beginning of each training epoch the dataset is randomly split into two non-overlapping halves. Left Gray Panel: corresponding images in each subset are augmented using the same set of two random transformations (so the total number of random transformations is halved relative to a standard iSSL training scheme). Every view is passed through a representation network \(f\) (ResNet-50 in this work) and the outputs are projected into two embedding spaces by different projector networks, \(g_{inv}\) and \(g_{equi}\). In the invariant embedding space a standard iSSL loss is applied, while in the equivariant embedding space the same iSSL loss is applied to the difference vectors between transformation-positive pairs (visualized on the right).

Note that this reduces the total number of random samples of \(\) by a factor of two relative to standard iSSL methods. Finally the resulting representation vectors are each fed through two distinct projector networks, \(z_{i}^{A/B}=g_{inv}(r_{i}^{A/B})\) and \(_{i}^{A/B}=g_{equi}(r_{i}^{A/B})\). These two embeddings are optimized to be invariant to transformations and discriminative across base images, or invariant to base images and discriminative across transformations, respectively. The overall objective (loss) functions is:

\[_{}&=(1- )_{iSSL}+_{CE-SSL}\\ &_{iSSL}&= ([z_{1}^{A},z_{2}^{A}],[z_{1}^{B},z_{2}^{B}]),\\ &_{CE-SSL}&= (z_{1}^{A}-z_{1}^{B},z_{2}^{A}-z_{2}^{B}),\] (2)

where both terms are written in terms of \(\), a self-supervised learning loss function that encourages invariance and uniformity (Wang and Isola, 2020) (e.g., \(L_{BT}\)) and \(\) is a hyperparameter that determines the relative importance of extracting an invariant or equivariant subspace from the shared representation. In the notation of Eq. (1), by designing \(_{CE-SSL}\) to encourage similar transformations to induce similar displacements in the output space, we are implicitly specifying that our output transformations are of the form \(T_{}(z)=z+z_{}\). Thus we leverage the principles underpinning contrastive invariance learning to encourage representations that contain useful transformation-related information; this choice differentiates this formulation from previous equivariant self-supervised learning approaches.

## 3 Results

### Implementation Details

Architecture and invariance ojective.For all experiments we use a ResNet-50 architecture (He et al., 2016) as the backbone representation network \(f\). Our training scheme is compatible with any choice of iSSL framework, as specified by the choice of \(_{iSSL}\). We experimented with three different base methods chosen to span the range from "instance contrastive" to "dimension contrastive" (Garrido et al., 2023): SimCLR (Chen et al., 2020), MMCR (Yerxa et al., 2024), and Barlow Twins (Zbontar et al., 2021). In each case, we define \(g_{inv}\) using the projector network architecture proposed in the original work. To retain the synergy between the normalization scheme, loss function, and projector architecture achieved by each framework we use the same architecture for both \(g_{inv}\) and \(g_{equi}\).

Pretraining dataset and augmentations.We train using the ImageNet-1k dataset and the standard set of augmentations first introduced in (Grill et al., 2020), which includes random resized cropping, color jittering, Gaussian blurring, solarization, and horizontal flips. See Appendix A.2 for exact training details.

Invariance-equivariance tradeoff.For each of the three choices of \(_{iSSL}\) we trained networks with hyperparameter values \(\{0.0,0.001,0.1,0.2,0.3,0.4,0.5\}\), yielding a total of 21 learned representations (note: \(=0\) corresponds to standard iSSL). We found that classification performance becomes severely degraded for values of \(\) larger than \(0.5\) (see Appendix A.4).

### Representational Analyses

Bures metric comparisons.We conducted a series of experiments to determine the extent to which various sources of variability in our dataset were meaningfully organized. The experiments utilized the Bures metric, which is the Wasserstein ("Earth Mover's") distance between mean-centered Gaussian distributions with covariance matrices \(C_{1}\) and \(C_{2}\):

\[D_{B}(C_{1},C_{2})=(C_{1}+C_{2}-2(C_{2}^{1/2}C_{ 1}C_{2}^{1/2})^{1/2}).\] (3)

When \(C_{1}\) and \(C_{2}\) are normalized to have a trace of 1, the maximal distance of \(2.0\) occurs when the variabilities lie in orthogonal subspaces (or are completely "factorized" from each other) and the minimum distance of \(0.0\) occurs when the covariances are equal. More generally, a large Bures distance indicates two sources of variability are factorized from each other and a low distance indicates shared structure. We first estimate the trace-normalized covariance of the outputs of some network \(f\) over two sources of variability and compute the Bures metric between the two.

Because we are mainly interested in the impact of the equivariance loss relative to the invariant baseline, each analysis below is carefully controlled to expose any structural differences. In particular, we estimate \(C_{1}\) and \(C_{2}\) over identical inputs for an invariant network and an equivariant network trained using the same base objective but a non-zero value of \(\). We then can directly compare the measured Bures distance for the invariant network and each equivariant network (\( 0\)): \( D_{B}=D_{B}(C_{1}^{=0},C_{2}^{=0})-D_{B}(C_{1}^{  0},C_{2}^{ 0})\); this measure quantifies the amount of alignment between two sources of variability in an equivariant network relative to the invariant network baseline. In each of the panels in the bottom row of Fig. 2 we show how \([ D_{B}]\) (denoted simply "Alignment") evolves as a function of \(\) when \(C_{1}\) and \(C_{2}\) are estimated over different sources of variability (y-axis labels indicate the ensembles over which the variabilities were estimated). We show the joint distribution of \((D_{B}(C_{1}^{=0},C_{2}^{=0}),D_{B}(C_{1}^{ 0},C_{2}^{  0}))\) and summarize the sources of variability in each experiment described below in Appendix A.5.

Augmentation-Augmentation alignment.First we determine the extent to which augmentation variability is shared across base images in each network (Fig. 2A). For these experiments, both \(C_{1}\) and \(C_{2}\) are estimated over many random transformations of single images in the validation set (we will refer to the responses to such a group as an "augmentation manifold"). The expectation is then over randomly sampled pairs of augmentation manifolds. The positive expected difference of distance indicates the equivariant networks consistently produce lower distance between augmentation manifolds, indicating more shared augmentation variability across base images. This structure is closely related to what is encouraged by the equivariance loss term and the orderly increase as a function of \(\) suggests that we are optimizing effectively.

Augmentation-Centroid factorization.We next investigate the extent to which variability over augmentations is factorized from variability over base images (Fig. 2B). We use the "centroid

Figure 2: Effects of equivariance on representational geometry. **A**: Alignment between augmentation manifolds (gray ellipsoids). **B**: Alignment between augmentation manifolds (gray ellipsoids) and the centroid manifold (blue disk). **C**: Alignment between spatial and photometric manifolds. Gray ellipsoids represent single augmentation manifolds, and blue/yellow points indicate the mean over the outputs from many transformations of a single view obtained via a photometric/spatial transformation, respectively. Expected distance is larger when the two sources of variability are factorized. **D**: Same as A., but for class manifolds. **E**: A schematic of the parameter regression experiment. In each panel, the bottom row depicts the results of each analysis described in the text of Sections 3.2 and 3.2. Shaded regions indicate 95% confidence intervals (estimated over the same comparisons the expected distance is estimated over for A-D and over 5 independent runs of the regression experiments for E). A summary of the sources of variability used to compute \(C_{1}\) and \(C_{2}\), and the ensemble used to estimate the expected alignment is measured can be found in Table A.5

manifold," (Yerxa et al., 2024) to characterize the variability over base images, by measuring the covariance over base images of the means over augmentations. That is, for these experiments (for each network respectively) \(C_{1}\) is the covariance of the centroids of all augmentation manifolds and \(C_{2}\) is the covariance of a randomly selected augmentation manifold. We observe that equivariant networks generally exhibit a larger distance between centroid and augmentation manifolds indicating increased factorization (or lower alignment) of image-content variability and image-augmentation variability. This structure was not explicitly encouraged by the objective and can be considered an emergent property of the equivariant learning procedure.

Spatial-Photometric factorization.Next we ask whether our equivariant training procedure induced increased factorization of variability to different types of input transformations (Fig. 2C). The standard augmentation procedure involves first taking a random crop (spatial variability) of a given image and then applying a series of pixel-level transformations (color-jittering, gaussian blurring, etc.) (photometric variability). To assess the impact of these two distinct classes of image transformations we first chose \(20\) random crops a given image, then applied the same set of \(20\) random photometric transformations to each individual crop, yielding 400 different views of each base image. \(C_{1}\) and \(C_{2}\) are then estimated over network responses that are averaged over different crops or different photometric transformations respectively, and the expectation is taken over different (single) base images. We observe the equivariant networks consistently exhibit increased factorization (i.e. larger Bures distances relative to the invariant trained network). This again is an emergent property of the equivariant learning procedure, and is particularly interesting in light of recent work that discovered that this form of transformation-factorization is more correlated with neural predictivity than transformation invariance (Lindsey and Issa, 2024).

Class-Class factorization.Finally we asked whether within-class variability was more or less shared between distinct classes in equivariant networks by estimating \(C_{1}\) and \(C_{2}\) over responses to all images in distinct classes in the validation set (the expectation is then taken over different random pairs of classes) (Fig. 2D). Increased sharing of variability between class manifolds has been demonstrated to increase manifold capacity, and can make representations better suited for multi-task evaluations (Wakhloo et al., 2023, 2024). We observe higher alignment (lower expected pairwise Bures distances) in the equivariant networks indicating that the "class manifolds" relative to the invariant networks.

Linear embedding of augmentation-related information.While the above experiments demonstrate that equivariant training induces increased alignment of transformation-related variability between images, this does not necessarily imply that this variability is coherently organized. To assess this more directly, we measure the extent to which augmentation parameters can be linearly decoded from the networks' representations. Specifically, we regress the concatenated outputs of a clean and transformed image onto the parameters of the applied augmentation. We report the resulting coefficient of determination (\(R^{2}\)) on a heldout set of validation images (Fig. 2). The equivariant training is seen to increase the amount of linearly accessible augmentation information relative to invariant training (the leftmost points plotted in Fig 2E). We further analyzed a set of equivariant models trained with weaker augmentation parameters (see A.6 for details). In these networks, we again observe that equivariant training increased the amount of linearly accessible augmentation information compared to invariant training. This holds not only for augmentation parameters within the training range (left panel 7) but also for parameter values beyond the training range (right panel 7). Thus, the equivariance properties of the models generalize beyond the training distribution. Future work could examine generalization to unseen types of augmentations.

### Neural Predictivity

We utilized the BrainScore evaluation pipeline Schrimpf et al. (2018) to measure the extent to which each learned representation can linearly predict neural responses measured in macaque area IT, for four different experimental datasets. At the time of testing, our highest performing model (Barlow Twins objective, \(=0.2\)) had the 10th highest average predictivity for area IT out of approximately 250 publicly available models on the Brain-Score leaderboard. Across a reasonably large range of values of \(\), the equivariant models improved the neural predictivity relative to the invariant baseline (\(=0\)) for all four datasets (Fig. 3). Many previous publications have noted that changes in training objective function have a small effect on neural predictivity, relative to other factorssuch as training dataset (Tuckute et al., 2022; Conwell et al., 2023; Yerxa et al., 2024). In contrast, encouraging equivariance produced much larger gains than choosing between different base invariant objectives: the range of predictivities over the sweep of \(\) was around 4 times larger than the range of predictivities over objective functions for the invariant baseline. We further contextualize the scale of predictivity improvements in Fig 4 by comparing models to all public submissions on the BrainScore leaderboard; our equivariant training procedure improves performance of the already-strong invariant models to nearly state-of-the-art levels of IT predictivity. By training the most predictive model for 1000 epochs (rather than 100), we achieved 0.5355 mean fraction of explained variance, which makes this the top IT brain prediction model. To ensure that the observed alignment increases are not architecture specific, we trained a smaller set of models using different backbone architectures and observed similar trends when using both smaller and larger networks (see Appendix A.7 for details).

We quantified the correlation between our various representational measurements and the neural predictivity for each of the four electrophysiology datasets in Table 1. We observed that the only representational metric with a correlation greater than \(0.4\) across all four neural datasets was the Spatial-Photometric distance, which is the metric most closely related to the factorization score described in (Lindsey and Issa, 2024). While this previous study described a correlation between structured variability and neural predictivity measured from a large set of pre-trained models, our results demonstrate that explicitly encouraging such structures can improve alignment between artificial and biological representations. In addition to the previously described representational measurements, we also looked at the linear decoding of the hue modulation parameter in isolation. Hue modulation is one of 12 augmentation parameters that are linearly decoded in the parameter regression measures described in Section 3.2. We observed a strong correlation between neural predictivity and hue modulation, particularly with the Sanghavi-Jozwik dataset, which is the only response dataset that included color image stimuli (last column of Table 1).

### Transfer Learning

Several previous studies that aim to reduce augmentation-invariance of self-supervised features have reported that the resulting representations generalize better to out-of-distribution classification datasets (Gupta et al., 2023; Xiao et al., 2020; Suau et al., 2023; Chavhan et al., 2022). However most of these studies focused on using the smaller ImageNet-100 dataset for training, in one case reporting that the transfer learning gains diminish or disappear when using ImageNet-1k (Chavhan et al., 2022). We tested our set of networks on 6 different downstream tasks and found limited evidence that the equivariant features confer an advantage in terms of out-of-distribution generalization when training on a sufficiently large and diverse dataset (see Table 2). To address this discrepancy with the literature we conducted additional experiments on networks trained using the ImageNet-100 dataset, and in this case observed improvement in generalization to diverse downstream tasks.

Figure 3: Brain-Score (noise-ceiled predictivity evaluated via ridge regression) for each value of \(\) (different colored bars) for each IT dataset (groups of columns) and base objective functions (different figure panels). For all datasets and base objectives the invariant network (\(=0\), lightest bars) is outperformed by at least one equivariant network, and the spread in predictivity over values of \(\) is significantly larger than the spread in predictivity over base objective functions for invariant networks.

It is also worth noting that CE-SSL trained networks do not outperform their invariant counterparts on in-distribution generalization (see A.2.1 and Fig. 5). This is not surprising in light of the fact that the suite of augmentations and architectures employed in SSL have been in some sense optimized by the community in order to improve performance on this task (by aligning the transformation invariance task with the standard in-distribution classification task). However, for out-of-distribution classification tasks where the task-alignment is worse, the equivariance task could mitigate this mismatch. A concrete example is the Flowers-102 dataset, where the color of petals is a much stronger predictor of class than color is in, say, the ImageNet-1k dataset (so the color insensitivity induced by the standard augmentations could be detrimental). For this dataset we do see marginal improvements, but note that the improvements are much more pronounced when pretraining on smaller datasets (ImageNet-100). There are at least 2 possible explanations for this: (1) for ImageNet-1k pretraining the performance of the networks is already quite high, and the task is saturated, or (2) there is a more fundamental reason that the improvements in transfer learning induced by equivariance decrease as the size and diversity of the pretraining dataset grows. Future work could explicitly disambiguate between these hypotheses to determine why the benefits of transformation-related variability for out of distribution generalization are outweighed by the gains of scaling the dataset. Furthermore this result shows that the increased neural predictivity we observe in ImageNet-1k trained networks cannot be explained by a need to perform better on a variety of invariant-classification tasks.

## 4 Relationship to Existing Augmentation-Sensitive SSL Methods

A key feature that differentiates our approach is that it encourages equivariant structure without explicit access to augmentation parameters. This is enabled by the "paired augmentation" data generation procedure, and to the best of our knowledge CARE [Gupta et al., 2023] is the only existing work that shares this feature. Our method has two advantages over CARE: (1) in CARE the equivariance loss is applied in the same space as the invariance constraint, and because there are no "negative equivariant pairs" in the CARE framework, learning an invariant representation would perfectly satisfy the equivariant constraint; and (2) in CARE the standard augmentation pipeline is

   Neural Dataset & Aug- & Aug- & Spatial- & Class- & Param & Hue \\  & Aug & Centroid & Photometric & Class & Regression & Regression \\  Majaj-Hong & 0.03 & 0.02 & 0.42 & 0.10 & 0.38 & 0.28 \\ Sanghavi-Jozwik & 0.86 & 0.7 & 0.83 & 0.77 & 0.91 & 0.91 \\ Sanghavi & 0.84 & 0.84 & 0.68 & 0.63 & 0.85 & 0.86 \\ Sanghavi-Murty & 0.33 & 0.41 & 0.42 & 0.14 & 0.56 & 0.48 \\   

Table 1: Absolute values of Pearson correlation coefficients (\(R^{2}\)) between various representational measurements and the neural predictivity across each of the four IT datasets. The correlation was measured over each value of \(\) and base objective function for a total of 21 networks. Each column corresponds to a panel in Fig. 2, except for hue, which is the regression score obtained for the random hue modulation parameter in isolation.

Figure 4: Histogram of neural predictivity scores for the 249 models on the public Brain-Score leaderboard at the time of testing, for each of the four considered IT datasets, as well as the mean over the four datasets. In each plot the vertical green line shows the score of the invariant Barlow Twins model, the red line shows the score for the equivariant Barlow Twins model with \(=0.2\), and the blue line shows a new model trained for 1000 epochs, also using Barlow Twins for the base loss and \(=0.2\).

used to optimize the base \(_{iSSL}\) loss and paired augmentation are used in parallel to optimize the equivariance term (so an increased number of passes through the network is necessary relative to standard training).

In EquiMod (Devillers and Lefort, 2023) the projector network is conditioned on the augmentation parameters by appending the parameters to the output of \(f\). The authors theorize that knowledge of the augmentation parameters could allow the projector network to better extract an invariant subspace tailored to each transformation, thus allowing for more structured variability in the representation space. Alternatively, the projector network could simply ignore the augmentation parameters, resulting in a structure that is identical to invariant SSL. In practice (Garrido et al., 2023b) have found this to be the case. Split-Invariant-Equivariant (SIE) and Amortised Invariance (AI) learning (Garrido et al., 2023b) each improve on this principle by using a hypernetwork approach: a separate network takes as inputs the augmentation parameters and outputs the parameters of either \(g\) or both \(f\) and \(g\) respectively. While the collapse issue of EquiMod is avoided, this comes at the expense of significantly complicating the network computation, and in the case of AI introduces new parameters that need to be tuned for every downstream task (when augmentation information is not available). Still other methods supplement the standard invariant SSL loss with an auxillary term that involves predicting the parameters of the input transformation (Lee et al., 2021; Dangovski et al., 2021). The relationship of our method to these is analogous to the relationship of transformation-invariant self-supervised learning to supervised classification.

    \\  Objective & Cifar-10 & Cifar-100 & Pets & DTD & Flowers-102 & Food-101 \\  MMCR & 84.3 & 63.3 & 67.0 & 66.1 & 83.1 & 60.9 \\ Barlow & 87.7 & 68.7 & 74.8 & 67.0 & 88.3 & 63.4 \\ SimCLR & 87.8 & 68.8 & 74.3 & 66.6 & 88.5 & 64.8 \\   CE-MMCR & 87.3 & 69.4 & 68.9 & 65.7 & 87.5 & 64.1 \\ CE-Barlow & 88.0 & 69.1 & 73.6 & 67.3 & 89.5 & 65.5 \\ CE-SimCLR & 87.9 & 68.2 & 72.6 & 67.5 & 88.6 & 65.2 \\   \\  Objective & Cifar-10 & Cifar-100 & Pets & DTD & Flowers-102 & Food-101 \\  MMCR & 92.2 & 76.9 & 85.3 & 75.4 & 93.9 & 73.8 \\ Barlow & 91.8 & 75.8 & 86.5 & 73.0 & 93.8 & 72.2 \\ SimCLR & 91.8 & 74.7 & 85.1 & 74.5 & 92.7 & 70.5 \\   CE-MMCR & 92.2 & 76.6 & 84.3 & 75.7 & 94.0 & 73.8 \\ CE-Barlow & 91.8 & 75.3 & 85.7 & 75.6 & 94.2 & 73.0 \\ CE-SimCLR & 91.0 & 73.6 & 82.3 & 73.9 & 92.2 & 70.5 \\   

Table 2: Frozen-Linear Evaluation for invariant and equivariant trained networks on 6 different downstream datasets: Cifar-10/100 (Krizhevsky et al., 2009), Oxford-Pets Parkhi et al. (2012), Describable Textures Database (Cimpoi et al., 2014), Flowers-102 (Nilsback and Zisserman, 2008), and Food-101 (Bossard et al., 2014). We closely follow the evaluation procedure from (Lee et al., 2021) (see Appendix A.4 for details) and report top1 accuracy for each objective/dataset. In all cases we report the mean over 5 runs of the evaluation procedure, we observed very little variability (maximum of.2%, over all evaluations, we report the standard deviation over runs in Appendix A.4. The equivariant networks are denoted by prepending a “CE” before the objective and were trained using \(=0.1\), which enabled a substantial amount of structure variability without significantly impacting frozen-linear classification on the SSL training dataset (see Appendix A.2). For ImageNet-1k trained networks out of distribution performance decreased for most evaluations, while for ImageNet-100 trained networks performance was improved in 15 of 18 cases.

Discussion

We've developed a new self-supervised objective that explicitly encourages structured variability in networks, and demonstrated that it can produce increased alignment with responses of neurons in primate visual area IT. While we are not the first to incorporate a notion of equivariance to self-supervised learning, our method improves on existing work in several ways: it require no extra passes through the network relative to invariance-based learning, it encourages diversity in the representation of transformation-related information by leveraging advances in invariance-based learning, and it does not rely on supervised access to transformation parameters. The parsimony of our approach (applying the same objective to both outputs of individual images and to displacements between similarly transformed images) allows our technique to be easily adapted to other settings such as temporal self-supervised learning (discussed below). Although in this work we focused on the visual domain, similar equivariant and invariant objectives could be investigated for other domains such as audio and langauge representation learning.

Our approach induced several interesting features in the learned representations: transformation variability is shared across base images and factorized with respect to variability over base images, the variability induced by distinct types of transformations are factorized from each other, there is increased alignment between class manifolds, and transformation related information is linearly encoded. Some of these properties are closely related to the imposed objective and some are emergent. We also confirmed that several of these representational properties are correlated with increased neural predictivity. Future work can extend these correlative observations to better understand how increasing transformation sensitivity improves neural alignment. For example, one could analyze the residuals of predicted neural firing rates of distinct models to determine how "overlapping" the variance predicted by each is (or alternatively, attempt to fit the residual variance of one model with another). Such analyses are becoming more feasible with the collection and release of larger scale datasets of neural responses to natural images (e.g., [Madan et al., 2024]). We view this result as demonstrating the promise of incorporating knowledge gained from experimental observations and large scale comparative studies into optimization procedures to produce better models.

Although our experiments reveal both induced and emergent benefits, the inclusion of an additive equivariance term in the objective does lead to fewer guarantees regarding the learned structure. For example in schemes where the output transformations (\(T_{}\)'s) are explicitly represented or learned, the resulting representation is "steerable" by default. It is of interest to investigate whether the output transformations could be reliably recovered from our learned representations. Additionally it would be interesting to consider other types of output transformations (CARE [Gupta et al., 2023] focuses on orthogonal transformations, and in the case where output transformations are learned they can be computed with nonlinear neural networks).

Finally, it is of interest to explore the use of more ecologically relevant sources of training data, e.g., by replacing synthetically transformed views of images with temporally adjacent frames of natural videos. This approach is particularly appealing from the perspective of biological plausibility, as the pairing of such training examples is readily available from natural visual experience. Several recent publications have shown that such a strategy can produce representations with competitive neural predictivity and performance on computer vision tasks [Zhuang et al., 2021, Parthasarathy et al., 2023, Venkataramanan et al., 2023]. In this context, the typical invariance loss can be thought of as incentivizing representational slowness [Foldak, 1991, Wiskott and Sejnowski, 2002]. The equivariance mechanism described in this work could be implemented by applying the same invariance-based loss function to the first temporal derivative of the responses, i.e. by encouraging the displacement between successive pairs of frames to be constant. Such a temporal-equivariance objective would incentivize representational straightness, which has been used to describe features of both human perception and neural activity in the ventral stream [Henaff et al., 2021, 2019]. Straightness in artificial representations has been found to be correlated with both neural predictivity and adversarial robustness [Lindsey and Issa, 2024, Harrington et al., 2022, Niu et al., 2024]. These connections provide an array of promising research directions.