# Provably Fast Finite Particle Variants of SVGD via Virtual Particle Stochastic Approximation

Aniket Das

Google Research

Bangalore, India

ketd@google.com

&Dheeraj Nagaraj

Google Research

Bangalore, India

dheerajnagaraj@google.com

###### Abstract

Stein Variational Gradient Descent (SVGD) is a popular particle-based variational inference algorithm with impressive empirical performance across various domains. Although the population (i.e, infinite-particle) limit dynamics of SVGD is well characterized, its behavior in the finite-particle regime is far less understood. To this end, our work introduces the notion of _virtual particles_ to develop novel stochastic approximations of population-limit SVGD dynamics in the space of probability measures, that are exactly realizable using finite particles. As a result, we design two computationally efficient variants of SVGD, namely VP-SVGD and GB-SVGD, with provably fast finite-particle convergence rates. Our algorithms can be viewed as specific random-batch approximations of SVGD, which are computationally more efficient than ordinary SVGD. We show that the \(n\) particles output by VP-SVGD and GB-SVGD, run for \(T\) steps with batch-size \(K\), are at-least as good as i.i.d samples from a distribution whose Kernel Stein Discrepancy to the target is at most \(O({d^{1/3}}/{(KT)^{1/6}})\) under standard assumptions. Our results also hold under a mild growth condition on the potential function, which is much weaker than the isoperimetric (e.g. Poincare Inequality) or information-transport conditions (e.g. Talagrand's Inequality \(_{1}\)) generally considered in prior works. As a corollary, we analyze the convergence of the empirical measure (of the particles output by VP-SVGD and GB-SVGD) to the target distribution and demonstrate a _double exponential improvement_ over the best known finite-particle analysis of SVGD. Beyond this, our results present the _first known oracle complexities for this setting with polynomial dimension dependence_, thereby completely eliminating the curse of dimensionality exhibited by previously known finite-particle rates.

## 1 Introduction

Sampling from a distribution over \(^{d}\) whose density \(^{}()(-F())\) is known only upto a normalizing constant, is a fundamental problem in machine learning , statistics , theoretical computer science  and statistical physics . A popular approach to this is the Stein Variational Gradient Descent (SVGD) algorithm introduced by Liu and Wang , which uses a positive definite kernel \(k\) to evolve a set of \(n\) interacting particles \((_{t}^{(i)})_{i[n],t}\) as follows:

\[_{t+1}^{(i)}_{t}^{(i)}-_{j= 1}^{n}[k(_{t}^{(i)},_{t}^{(j)}) F(_{ t}^{(j)})-_{2}k(_{t}^{(i)},_{t}^{(j)})] \]

SVGD exhibits remarkable empirical performance in a variety of Bayesian inference, generative modeling and reinforcement learning tasks  and usually converges rapidly to the target density while using only a few particles, often outperforming Markov Chain Monte Carlo (MCMC) methods. However, in contrast to its wide practical applicability, theoreticalanalysis of the behavior SVGD is a relatively unexplored problem. Prior works on the analysis of SVGD [26; 14; 30; 42; 7] mainly consider the population limit, where the number of particles \(n\). These works assume that the initial distribution of the (infinite number of) particles has a finite KL divergence to the target \(^{}\) and subsequently, interpret the dynamics of population-limit SVGD as projected gradient descent updates for the KL divergence on the space of probability measures, equipped with the Wasserstein geometry. Under appropriate assumptions on the target density, one can then use the theory of Wasserstein Gradient Flows to establish non-asymptotic (in time) convergence of population-limit SVGD to \(^{}\) in the Kernel Stein Discrepancy (KSD) metric.

While the framework of Wasserstein Gradient Flows suffices to explain the behavior of SVGD in the population limit, the same techniques are insufficient to effectively analyze SVGD in the finite-particle regime. This is primarily due to the fact that the empirical measure \(^{(n)}\) of a finite number of particles does not admit a density with respect to the Lebesgue measure, and thus, its KL divergence to the target is always infinite (i.e. \((^{(n)}\|^{})=\)). In such a setting, a direct analysis of the dynamics of finite-particle SVGD becomes prohibitively difficult due to complex inter-particle dependencies. To the best of our knowledge, the pioneering work of Shi and Mackey  is the only result that obtains an explicit convergence rate of finite-particle SVGD by tracking the deviation between the law of \(n\)-particle SVGD and that of its population-limit. To this end, Shi and Mackey  show that for subgaussian target densities, the empirical measure of \(n\)-particle SVGD converges to \(^{}\) at a rate of \(O((d)}{ n^{6(1/d)}}})\) in KSD 1. The obtained convergence rate is quite slow and fails to adequately explain the impressive practical performance of SVGD.

Our work takes a starkly different approach to this problem and deliberately deviates from tracking population-limit SVGD using a finite number of particles. Instead, we directly analyze the dynamics of KL divergence along a carefully constructed trajectory in the space of distributions. To this end, our proposed algorithm, Virtual Particle SVGD (VP-SVGD) devises an _unbiased stochastic approximation (in the space of measures) of the population-limit dynamics of SVGD_. We achieve this by considering additional particles called _virtual particles_2 which evolve in time but aren't part of the output (i.e. _real particles_). These virtual particles are used only to compute information about the current population-level distribution of the real particles, and enable exact implementation of our stochastic approximation to population-limit SVGD, while using only a finite number of particles.

Our analysis is similar in spirit to non-asymptotic analyses of stochastic gradient descent (SGD) that generally do not attempt to track gradient descent (analogous to population-limit SVGD in this case), but instead directly track the evolution of the objective function along the SGD trajectory using appropriate stochastic descent lemmas [24; 21; 11]. The key feature of our proposed stochastic approximation is the fact that it can be implemented using only a finite number of particles. This allows us to design faster variants of SVGD with provably fast finite-particle convergence.

### Contributions

**VP-SVGD and GB-SVGD** We propose two variants of SVGD that enjoy provably fast finite-particle convergence guarantees to the target distribution: Virtual Particle SVGD (VP-SVGD in Algorithm 1) and Global Batch SVGD (GB-SVGD in Algorithm 2). VP-SVGD is a conceptually elegant stochastic approximation (in the space of probability measures) of population-limit SVGD, and GB-SVGD is a practically efficient version of SVGD which achieves good empirical performance. Our analysis of GB-SVGD builds upon that of VP-SVGD. When the potential \(F\) is smooth and satisfies a quadratic growth condition (which holds under subgaussianity of \(^{}\), a common assumption in prior works [42; 43]), we show that the \(n\) particles output by \(T\) steps of our algorithms, run with batch-size \(K\), are at least as good as i.i.d draws from a distribution whose Kernel Stein Discrepancy to \(^{}\) is at most \(O(d^{}{{3}}}/(KT)^{}{{6}}})\). Our results also hold under a mild subquadratic growth condition for \(F\), which is much weaker than isoperimetric (e.g. Poincare Inequality) or information-transport (e.g. Talagrand's Inequality \(_{1}\)) assumptions generally considered in the sampling literature [47; 42; 43; 8; 2].

**State-of-the-art Finite Particle Guarantees** As corollaries of the above result, we establish that _VP-SVGD and GB-SVGD exhibit the best known finite-particle guarantees in the literature which significantly outperform that of prior works_. Our results are summarized in Table 1. In particular,under subgaussianity of the target distribution \(^{}\), we show that the empirical measure of the \(n\) particles output by VP-SVGD converges to \(^{}\) in KSD at a rate of \(O((}{{n}})^{}{{4}}}+(}{{n}})^{}{{2}}})\). Similarly, the empirical measure of the \(n\) output particles of GB-SVGD converges to \(^{}\) at a KSD rate of \(O(}}{{n^{}{{12}}}}}+(}{{n}})^{ }{{2}}})\). Both these results represent a **double exponential improvement** over the \(O((d)}{(}{{4}})}}})\) KSD rate of \(n\)-particle SVGD obtained by Shi and Mackey , which, to our knowledge, is the best known prior result for SVGD in the finite particle regime. When benchmarked in terms of gradient oracle complexity, i.e., the number of evaluations of \( F\) required by an algorithm to achieve \(_{^{}}(||^{})\), we demonstrate that for subgaussian \(^{}\), the oracle complexity of VP-SVGD is \(O(}}{{^{12}}})\) while that of GB-SVGD is \(O(}}{{^{18}}})\). To the best of our knowledge, our result presents the _first known oracle complexity guarantee with polynomial dimension dependence_, and consequently, does not suffer from a curse of dimensionality unlike prior works. Furthermore, as discussed above, the conditions under which our result holds is far weaker than subgaussianity of \(^{}\), and as such, includes sub-exponential targets and beyond. In particular, _our guarantees for sub-exponential target distributions are (to the best of our knowledge) the first of its kind._

**Computational Benefits:** VP-SVGD and GB-SVGD can be viewed as specific random batch approximations of SVGD. Our experiments (Section 8) show that GB-SVGD obtains similar performance as SVGD but requires fewer computations. In this context, a different kind of random batch method that divides the particles into random subsets of interacting particles, has been proposed by Li et al. . However, the objective in Li et al.  is to approximate finite-particle SVGD dynamics using the random batch method, instead of analyzing convergence of the random batch method itself. Beyond this, their guarantees also suffer from an exponential dependence on the time \(T\). As explained below, their approach is also conceptually different from our method since we use the _same_ random batch to evolve _every_ particle, allowing us to interpret this as a stochastic approximation in the space of distributions instead of in the path space.

### Technical Challenges

We resolve the following important conceptual challenges, which may be of independent interest.

**Stochastic Approximation in the Space of Probability Measures** Stochastic approximations are widely used in optimization, control and sampling [27; 52; 23]. In the context of sampling, stochastic approximations are generally implemented in path space, e.g., Stochastic Gradient Langevin Dynamics (SGLD)  takes a random batch approximation of the drift term via the update \(_{t+1}=_{t}-_{j=0}^{K-1} f( _{t},_{j})+_{t},\ _{t}( 0,)\) where \([f(_{t},_{j})|_{t}]=F(_{t})\). Such stochastic approximations are then analyzed using the theory of stochastic processes over \(^{d}\)[12; 40; 55; 25]. However, when viewed in the space of probability measures (i.e, \(_{t}=(_{t})\)), the time-evolution

 
**Result** & **Algorithm** & **Assumption** & **Rate** &  **Oracle** \\ **Complexity** \\  \\   & Population Limit &  Uniformly Bounded \\ \(_{^{}}(_{t}||^{})\) \\  & 
 Not \\ Implementable \\  \\   & Population Limit & \)} & \(}}{{}}}{}\) & Not \\  & SVGD & & & \\  Shi and Mackey  & SVGD & Sub-gaussian \(^{}\) & \((d)}{(}{{4}})}}}\) & \((d)}{^{2}}^{(}}{{ ^{29}}})^{}{{2}}}}\) \\ 
**Ours, Corollary 1** & **VP-SVGD** & **Sub-gaussian \(^{}\)** & \((}{{n}})^{}{{4}}}+(}{{n}} )^{}{{2}}}\) & \(}{{^{12}}}\) \\ 
**Ours, Corollary 1** & **GB-SVGD** & **Sub-gaussian \(^{}\)** & \(}}{{n^{}{{2}}}}}+(}{{n}} )^{}{{2}}}\) & \(}{{^{18}}}\) \\ 
**Ours, Corollary 1** & **VP-SVGD** & Sub-exponential \(^{}\) & \(}}{{n^{}{{2}}}}}}{}\) & \(}}{{^{16}}}\) \\ 
**Ours, Corollary 1** & **GB-SVGD** & Sub-exponential \(^{}\) & \(}}{{n^{}{{2}}}}}}{}\) & \(}}{{^{16}}}\) \\  

Table 1: Comparison of our results with prior works. \(d\), \(T\), and \(n\) denote the dimension, no. of iterations and no. of output particles respectively. Oracle Complexity denotes number of evaluations of \( F\) needed to achieve \(_{^{}}(||^{})\) and Rate denotes convergence rate w.r.t KSD metric. Note that: 1. Population Limit SVGD is not implementable as it requires infinite particles 2. The uniformly bounded \(_{^{}}(_{t}||^{})\) assumption cannot be verified apriori and is much stronger than subgaussianity (see  Lemma C.1)

[MISSING_PAGE_FAIL:4]

Background on Population-Limit SVGD

We briefly introduce the analysis of population-limit SVGD using the theory of Wasserstein Gradient Flows and refer the readers to Korba et al.  and Salim et al.  for a detailed treatment.

The space \(_{2}(^{d})\) equipped with the 2-Wasserstein metric \(_{2}\) is known as the Wasserstein space, which admits the following Riemannian structure : For any \(_{2}(^{d})\), the tangent space \(T_{}_{2}(^{d})\) can be identified with the Hilbert space \(L^{2}()\). We can then define differentiable functionals \(:_{2}(^{d})\) and compute their Wasserstein gradients, denoted as \(_{_{2}}\). Note that the target \(^{}\) is the unique minimizer over \(_{2}(^{d})\) for the functional \([]=(|\!|^{})\). The Wasserstein Gradient of \([]\) is \(_{_{2}}[]=_{}(}{^{}}())\). This powerful machinery has served as a backbone for the analysis of algorithms such as LMC  and population-limit SVGD .

The updates of population-limit SVGD can be viewed as Projected Gradient Descent in the Wasserstein space. Recall from Section 2 that the function \(h_{}()=P_{}((}{^{ }}))()= h(,)()\). Let \(_{t}^{n}\) denote the empirical measures of the SVGD particles \((_{t}^{(i)})_{i[n]}\) at timestep \(t\). We note that the SVGD updates in (1) can be recast as \(_{t^{n}+1}^{n}=(I- h_{_{t^{n}}})_{\#}_{n}^{n}\). In the limit of infinite particles \(n\), suppose the empirical measure \(_{t}^{n}\) converges to the population measure \(_{t}\). In this population limit, the updates of SVGD can be expressed as,

\[_{t+1}=(I-h_{_{t}})_{\#}\,_{t}=(I- P_{ _{t}}((_{t}}{d^{}}) ))_{\#}_{t}=(I- P_{_{t}}(_{ _{2}}(_{t}|\!|^{} )))_{\#}\,_{t}\]

Recall from Section 2 that \(P_{_{t}}:L^{2}(_{t})\) is the Hilbert adjoint of \(i_{_{t}}\). Since \( L^{2}(_{t})\), the updates of SVGD in the population limit can be seen as Projected Wasserstein Gradient Descent for \([]=(|\!|^{})\), with the Wasserstein Gradient at each step being projected onto the RKHS \(\). Assuming \((_{0}|\!|^{})<\), convergence of population limit SVGD is then established by tracking the evolution of \((_{t}|\!|^{})\) under appropriate structural assumptions (such as subgaussianity) on \(^{}\).

## 4 Algorithm and Intuition

In this section, we derive VP-SVGD (Algorithm 1), and build upon it to obtain GB-SVGD. Consider a countably infinite collection of particles \(_{0}^{(l)}^{d},\ l\{0\}\), sampled i.i.d from a measure \(_{0}\), having a density w.r.t. the Lebesgue measure. By the strong law of large numbers, the empirical measure of \(_{0}^{(l)}\) is almost surely equal to \(_{0}\) (see Dudley [13, Theorem 11.4.1]). Let batch size \(K\) denote the batch size, and \(_{t}\) denote the filtration \(_{t},\ t 0\) as \(_{t}=(\{_{0}^{(l)} l Kt-1\}),\ \,t \), with \(_{0}\) being the trivial \(\) algebra. For ease of exposition, we discuss the case of \(K=1\) in this section and present a complete derivation for arbitrary \(K 1\) in Appendix C. Recall from Section 3 that the updates of population-limit SVGD in \(_{2}(^{d})\) can be expressed as follows:

\[_{t+1}=(I- h_{_{t}})_{\#}_{t} \]

We aim to design a stochastic approximation in \(_{2}(^{d})\) for the updates (2), such that it admits a finite-particle realization. To this end, we propose the following dynamics in \(^{d}\)

\[_{t+1}^{(s)}=_{t}^{(s)}- h(_{t}^{(s)}, _{t}^{(t)}), s\{0\} \]

Now, for each time-step \(t\), we focus on the time evolution of the particles \((_{t}^{(l)})_{l t}\) (called the _lower triangular evolution_). From (3), we observe that for any \(t\) and \(l t\), \(_{t}^{(l)}\) depends only on \(_{0}^{(0)},,_{0}^{(t-1)},_{0}^{(l)}\). Hence, there exists a deterministic, measurable function \(H_{t}\) such that:

\[_{t}^{(l)}=H_{t}(_{0}^{(0)},,_{0}^{(t-1)}, _{0}^{(l)})\,;l t \]

Since \(_{0}^{(0)},,_{0}^{(t-1)},_{0}^{(l)}_{0}\), we conclude from (4) that \((_{t}^{(l)})_{l t}\) are i.i.d when conditioned on \(_{0}^{(0)},,_{0}^{(t-1)}\). To this end, we define the random measure \(_{t}|_{t}\) as the law of \(_{t}^{(t)}\) conditioned on \(_{t}\), i.e., \(_{t}|_{t}\) is a probability kernel \(_{t}(\,;_{0}^{(0)},,_{0}^{(t-1)})\), where \(_{0}|_{0}:=_{0}\). By the strong law of large numbers, \(_{t}|_{t}\) is equal to the empirical measure of \((_{t}^{(l)})_{l t}\) conditioned on \(_{t}\). We will use \(_{t}|_{t}\) and \(_{t}(\,;_{0}^{(0)},,_{0}^{(t-1)})\) interchangeably.

Define the random function \(g_{t}:^{d}^{d}\) as \(g_{t}():=h(,_{t}^{(t)})\). From (4), we note that \(g_{t}\) is \(_{t+1}\) measurable. From (3), we infer that the particles satisfy the following relation:

\[_{t+1}^{(s)}=(I- g_{t})(_{t}^{(s)}), s t+1\]

Recall that \(_{t+1}^{(s)}|_{0}^{(0)},,_{0}^{(t)}_ {t+1}|_{t+1}\) for any \(s t+1\). Furthermore, from Equation (4), we note that for \(s t+1\), \(_{t}^{(s)}\) depends only on \(_{0}^{(0)},,_{0}^{(t-1)}\) and \(_{0}^{(s)}\). Hence, we conclude that \((_{t}^{(s)}|_{0}^{(0)},,_{0}^ {(t)})=(_{t}^{(s)}|_{0}^{(0)},, _{0}^{(t-1)})=_{t}|_{t}\). With this insight, the dynamics of the lower-triangular evolution in \(_{2}(^{d})\) that the following holds almost surely:

\[_{t+1}|_{t+1}=(I- g_{t})_{\#}_{t}|_{t} \]

\(_{t}^{(t)}|_{t}_{t}|_{t}\) implies \([g_{t}()|_{t}]=h_{_{t}|_{t}}( )\). Thus _lower triangular dynamics_ (5) is a stochastic approximation in \(_{2}(^{d})\) to the population limit of SVGD (2). Setting the batch size to general \(K\) and tracking the evolution of the first \(KT+n\) particles, we obtain VP-SVGD (Algorithm 1).

**Input**: Number of steps \(T\), number of output particles \(n\), batch size \(K\), Initial positions \(_{0}^{(0)},,_{0}^{(n+KT-1)\ i.i.d.}\ _{0}\), Kernel \(k\), step size \(\).

```
1:for\(t\{0,,T-1\}\)do
2:for\(s\{0,,KT+n-1\}\)do
3:\(_{t+1}^{(s)}=_{t}^{(s)}-_{l=0}^{K-1} [k(_{t}^{(s)},_{t}^{(tK+l)}) F(_{t}^{(tK+ l)})-_{2}k(_{t}^{(s)},_{t}^{(tK+l)})]\)
4:endfor
5:endfor
6: Draw \(S\) uniformly at random from \(\{0,,T-1\}\)
7:Output \((^{(0)},,^{(n-1)})=(_{S}^{(TK)},, _{S}^{(TK+n-1)})\)
```

**Algorithm 1** Virtual Particle SVGD (VP-SVGD)

**Virtual Particles** In Algorithm 1, \((_{t}^{(l)})_{KT l KT+n-1}\) are the _real particles_ which constitute the output. \((_{t}^{(l)})_{l<KT}\) are _virtual particles_ which propagate information about the probability measure \(_{t}|_{t}\) to enable computation of \(g_{t}\), an unbiased estimate of the projected Wasserstein gradient \(h_{_{t}|_{t}}\).

**VP-SVGD as SVGD Without Replacement** VP-SVGD is a without-replacement random-batch approximation of SVGD (1), where a different batch is used across timesteps, but the same batch is across particles given a fixed timestep. With i.i.d. initialization, picking the 'virtual particles' in a fixed order or from a random permutation does not change the evolution of the real particles. With this insight, we design GB-SVGD (Algorithm 2) where we consider \(n\) particles _and_ output \(n\) particles (instead of wasting \(KT\) particles as 'virtual particles') via a random-batch approximation of SVGD.

**Input**: \(\#\) of time steps \(T\), \(\#\) of particles \(n\), \(_{0}^{(0)},,_{0}^{(n-1)\ i.i.d.}\ _{0}\), Kernel \(k\), step size \(\), Batch size \(K\), Sampling method \(\) {with replacement, without replacement}
1:for\(t\{0,,T-1\}\)do
2:\(_{t}\) random subset of \([n]\) of size \(K\) (via. sampling method)
3:for\(s\{0,,n-1\}\)do
4:\(_{t+1}^{(s)}=_{t}^{(s)}-_{r_{t}}[k(_{t}^{(s)},_{t}^{(r)}) F(_{t}^{(r)} )-_{2}k(_{t}^{(s)},_{t}^{(r)})]\)
5:endfor
6: Draw \(S\) uniformly at random from \(\{0,1,,T-1\}\)
7:Output \((}^{(0)},,}^{(n-1)})=(_{S}^{(0 )},,_{S}^{(n-1)})\) ```

**Algorithm 2** Global Batch SVGD (GB-SVGD)

In Algorithm 2, with replacement sampling means selecting a batch of \(K\) particles i.i.d. from the uniform distribution over \([n]\). Without replacement sampling means fixing a random permutation \(\) over \(\{0,,n-1\}\) and selecting the batches in the order specified by the permutation.

## 5 Assumptions

In this section, we discuss the key assumptions required for our analysis of VP-SVGD and GB-SVGD. Our first assumption is smoothness of \(F\), which is standard in optimization and sampling.

**Assumption 1** (L-Smoothness).: \( F\) _exists and is \(L\) Lipschitz. Moreover \(\| F(0)\|\)._

It is easy find a point such that \(\| F(^{})\|\) (e.g., using \((1)\) gradient descent steps ) and center the initialization at \(_{0}\) at \(^{}\). For clarity, we take \(^{}=0\) without loss of generality. We now impose the following growth condition on \(F\).

**Assumption 2** (Growth Condition).: _There exist \(,d_{1},d_{2}>0\) such that_

\[F() d_{1}\|\|^{}-d_{2} ^{d}\]

Note that Assumption 1 ensures \( 2\). Assumption 2 is essentially a tail decay assumption on the target density \(^{}() e^{-F()}\). In fact, as we shall show in Appendix B, Assumption 2 ensures that the tails of \(^{}\) decay as \( e^{-\|\|^{}}\). Consequently, Assumption 2 holds with \(=2\) when \(^{}\) is subgaussian and with \(=1\) when \(^{}\) is subexponential. Subgaussianity is equivalent to \(^{}\) satisfying the \(_{1}\) inequality [5; 49], commonly assumed in prior works on SVGD [42; 43]. We also note that subexponentiality is implied when \(^{}\) satisfies the Poincare Inequality [4, Section 4], which is considered a mild condition in the sampling literature [47; 8; 2; 12; 7]. This makes Assumption 1 significantly weaker than the isoperimetric or information-transport assumptions considered in prior works.

Next, we impose a mild assumption on the RKHS of the kernel \(k\), which has been used by several prior works [42; 26; 45; 43].

**Assumption 3** (Bounded RKHS Norm).: _For any \(^{d}\), \(k(,)\) satisfies \(\|k(,)\|_{_{0}} B\). Furthermore, \(_{2}k(,)\) and \(\|_{2}k(,)\|_{} B\)_

Assumption 3 ensures that the adjoint operator \(P_{}\), used in Sections 2 and 3, is well-defined. We also make the following assumptions on the kernel \(k\), which is satisfied by a large class of standard kernels such as Radial Basis Function kernels and Matern kernels of order \(\).

**Assumption 4** (Kernel Decay).: _The kernel \(k\) satisfies the following for constants \(A_{1},A_{2},A_{3}>0\)._

\[0 k(,)}{1+\|-\|^{ 2}},\|_{2}k(,)\| A_{2},\| _{2}k(,)\|^{2} A_{3}k(,)\]

Finally, we make the following mild assumption on the initialization.

**Assumption 5** (Initialization).: _The initial distribution \(_{0}\) is such that \(\,(_{0}\|^{})<\). Furthermore, \(_{0}\) is supported in \((R)\), the \(_{2}\) ball of radius \(R\)_

Since prior works usually assume Gaussian initialization [42; 47], Assumption 5 may seem slightly non-standard. However, this is not a drawback. In fact, whenever \(R=(+(}{{}}))\), Gaussian initialization can be made indistinguishable from \(((R))\) initialization, with probability at least \(1-\), via a coupling argument. To this end, we impose Assumption 5 for ease of exposition and our results can be extended to consider Gaussian initialization. In Appendix B we show that taking \(R=}{{L}}}\) and \(_{0}=((R))\) suffices to ensure \(\,(_{0}\|^{})=O(d)\).

## 6 Results

### Vp-Svgd

Our first result, proved in Appendix C, shows that the law of the _real particles of_ VP-SVGD, when conditioned on the virtual particles, is close to \(^{}\) in KSD. As a consequence, it shows that the particles output by VP-SVGD are i.i.d. samples from a random probability measure \((;_{0}^{(0)},,_{0}^{(KT-1)},S)\) which is close to \(^{}\) in KSD. We also present a high-probability version of this result in Appendix C.

**Theorem 1** (Convergence of VP-SVGD).: _Let \(_{t}\) be as defined in Section 4. Let Assumptions 1 2, 3, 4, and 5 be satisfied and let \(\{}{{2A_{1}L}},}{{(4+L)B}}\}\). There exist \((_{i})_{0 i 3}\) depending polynomiallyon \(A_{1},A_{2},A_{3},B,L,d_{1},d_{2}\) for any fixed \((0,2]\), such that whenever \(\), with \(=_{0}+_{1}( T)^{}{{}}}+_{2}(^{2 }T)^{}{{}}}+_{3}R^{}{{}}}\), the following holds:_

\[_{t=0}^{T-1}[^{2}_{^{*}}(_{t}| _{t}||^{*})](_{0}|\!|\!|\!|\! |^{*})}{ T}+}{K}\]

_Define the probability kernel \((\;;)\) as follows: For any \(x_{}^{d}\), \((KT)\) and \(s(T)\), \((\;;x_{0},,x_{KT-1},s):=_{s}(\;;x_{0},,x_{Ks- 1})\) and \((\;;x_{0},,x_{KT-1},s=0):=_{0}()\). Conditioned on \(^{(0)}_{}=x_{},\;S=s\) for every \((KT)\), the outputs \(^{(0)},,^{(n-1)}\) of VP-SVGD are i.i.d samples from \((\;;x_{0},,x_{KT-1},s)\). Furthermore,_

\[[^{2}_{^{*}}((\;;^{(0)}_{0}, ,^{(KT-1)}_{0},S)||^{*})](_{0 }|\!|\!|\!|^{*})}{ T}+}{K}\]

**Convergence Rates** Taking \(_{0}=((R))\) with \(R=}{{L}}}\) ensures \((_{0}|_{0}|\!|\!|\!|\!|^{*})=O(d)\) (see Appendix B). Under this setting, choosing \(=O(}{T^{}{{}}}})\) ensures that \([^{2}_{^{*}}(||^{*})]=O(}{{}}}}{(KT)^{}{{}}}})\) where \(=\). Thus, for \(=2\), (i.e, sub-Gaussian \(^{*}\)), \(^{2}=O(}{{}}}}{(KT)^{}{{ }}}})\). For \(=1\) (i.e, sub-Exponential \(^{*}\)), the rate (in squared KSD) becomes \(O(}{{}}}}{(KT)^{}{{}}}})\). To the best of our knowledge, our convergence guarantee for sub-exponential \(^{*}\) is the first of its kind.

**Comparison with Prior Works** Salim et al.  analyzes population-limit SVGD for subgaussian \(^{*}\), obtaining \(^{2}=O(}{{}}}}}{{T}}/r)\) rate. We note that population-limit SVGD (which requires infinite particles) is not implementable whereas VP-SVGD is an implementable algorithm whose outputs are conditionally i.i.d samples from a distribution with guaranteed convergence to \(^{*}\).

### Gb-Svgd

We now use VP-SVGD as the basis to analyze GB-SVGD. Assume \(n>KT\). Then, with probability at least \(1-T^{2}}}{{n}}\) (for with-replacement sampling) and \(1\) (for without-replacement sampling), the random batches \(_{t}\) in GB-SVGD (Algorithm 2) are disjoint and contain distinct elements. When conditioned on this event \(\), we note that the \(n-KT\) particles that were not included in any random batch \(_{t}\) evolve exactly like the \(n\) real particles of VP-SVGD. With this insight, we show that, conditioned on \(\), the outputs of VP-SVGD and GB-SVGD can be coupled such that the first \(n-KT\) particles of both the algorithms are exactly equal. This allows us to derive the following squared KSD bound between the empirical measures of the outputs of VP-SVGD and GB-SVGD. The proof of this result is presented in Appendix D.

**Theorem 2** (Ksd Bounds for Gb-Svgd).: _Let \(n>KT\) and let \(=(^{(0)},,^{(n-1)})\) and \(}=(}^{(0)},,}^{(n-1)})\) denote the outputs of VP-SVGD and GB-SVGD respectively. Moreover, let \(^{(n)}=_{i=0}^{n-1}_{^{(i)}}\) and \(^{(n)}=_{i=0}^{n-1}_{}^{(i)}}\) denote their respective empirical measures. Under the assumptions and parameter settings of Theorem 1, there exists a coupling of \(\) and \(}\) such that the following holds:_

\[[^{2}_{^{*}}(^{(n)}||^{(n)})] T^{2}^{2}}{n^{2}}&\\ T^{2}^{2}}{n^{2}}(1-T^{2}}{n})+T^ {2}^{2}}{n}& \]

### Convergence of the Empirical Measure to the Target

As a corollary of Theorem 1 and Theorem 2, we show that the empirical measure of the output of VP-SVGD and GB-SVGD rapidly converges to \(^{*}\) in KSD. We refer to Appendix E for the full statement and proof.

**Corollary 1** (**Vp-Svgd and Gb-Svgd: Fast Finite Particle Rates)**.: _Let the assumptions and parameter settings of Theorem 1 be satisfied. Let \(^{(n)}\) be the empirical measure of the \(n\) particles output by VP-SVGD, run with run with \(KT=d^{}\), \(R=}{{L}}}\) and appropriately chosen \(\). Then:_

\[[^{2}_{^{*}}(^{(n)}||^{*})] O(}{{}}}}}{n^{}{{}}}}+}{{}}}}{n})\]_Similarly, let \(^{(n)}\) be the empirical measure of the output of GB-SVGD without replacement, run with \(KT=\), \(R=}{{L}}}\) and appropriately chosen \(\). Then, the following holds:_

\[[_{^{}}^{2}(^{(n)}||^{})] O( }}}}{{{n}}}}{n}+}{n }+}{n^{}})\]

**Comparison to Prior Work** For subgaussian \(^{}\) (i.e. \(=2\)), VP-SVGD has a finite-particle rate of \([_{^{}}(^{(n)}||^{})]=O(( }{{n}})^{}{{4}}}+(}{{n}})^{}{{2}}})\) while that of GB-SVGD is \([_{^{}}(^{(n)}||^{})]=O( }}}}{{{n}^{}{{2}}}}}/n^{1/2}+ {{(d/n)^{1}}}{{2}})\). Both these rates are a _double exponential improvement_ over the \(((d)}{}})\) KSD rate obtained by Shi and Mackey  for SVGD with subgaussian \(^{}\). For subexponential \(^{}\) (i.e. \(=1\)) the KSD rate of VP-SVGD is \(O(}{{3}}}}{n^{}{{6}}}}+}{{2}}}})\) while that of GB-SVGD is \(O(}{{8}}}}{n^{}{{16}}}}+}{{2}}}})\). To our knowledge, both these results are the first of their kind.

**Oracle Complexity** As illustrated in Section E.3, for subgaussian \(^{}\), the oracle complexity of VP-SVGD to achieve \(\)-convergence in KSD is \(O(}}}}{{{^{12}}}})\) and that of GB-SVGD is \(O(}}}}{{{^{18}}}})\). To our knowledge, these results are the _first known oracle complexities for this problem with polynomial dimension dependence_, and significantly improve upon the \(O((d)}{^{2}}e^{((d)/ ^{2}}))\) oracle complexity of SVGD as implied by Shi and Mackey . For subexponential \(^{}\), the oracle complexity of VP-SVGD is \(O(}}}}{{{^{16}}}})\) and that of GB-SVGD is \(O(}}}}{{{^{24}}}})\).

## 7 Proof Sketch

We now present a sketch of our analysis. As shown in Section 4, the particles \((_{t}^{(l)})_{l Kt}\) are i.i.d conditioned on the filtration \(_{t}\), and the random measure \(_{t}|_{t}\) is the law of \((_{t}^{(kl)})\) conditioned on \(_{0}^{(0)},,_{0}^{(Kt-1)}\). Moreover, from equation (5), we know that \(_{t}|_{t}\) is a stochastic approximation of population limit SVGD dynamics, i.e., \(_{t+1}|_{t+1}=(I- g_{t})_{\#}_{t}|_{t}\). Lemma 1 (similar to Salim et al. [42, Proposition 3.1] and Korba et al. [26, Proposition 5]) shows that under appropriate conditions, the KL divergence between \(_{t}_{t}\) and \(^{}\) satisfies a (stochastic) descent lemma. Hence \(_{t}|_{t}\) admits a density and \((_{t}|_{t}\|^{})\) is almost surely finite.

**Lemma 1** (Descent Lemma for \(_{t}|_{t}\)).: _Let Assumptions 1, 3 and 5 be satisfied and let \(>1\) be an arbitrary constant. On the event \(\|g_{t}\|_{}\), the following holds almost surely_

\[(_{t+1}|_{t+1}\|^{})(_{t}|_{t}\|^{})- h_{_{t} |_{t}},g_{t}_{}+(^{2} +L)B}{2}\|g_{t}\|_{}^{2}\]

Lemma 1 is analogous to the noisy descent lemma which is used in the analysis of SGD for smooth functions. Notice that \([g_{t}|_{t}]=h_{_{t}|_{t}}\) (when interpreted as a Gelfand-Pettis integral , as discussed in Appendix B and Appendix C) and hence in expectation, the KL divergence decreases in time. In order to apply Lemma 1, we establish an almost-sure bound on \(\|g_{t}\|_{}\) below.

**Lemma 2**.: _Let Assumptions 1, 2, 3, 4 and 5 hold. For \(}{{2A_{1}L}}\), the following holds almost surely,_

\[\|g_{t}\|_{}=_{0}+_{1}( T)}{{ }}+_{2}(^{2}T)}{{}}+_{3}R^{}{{}}}\]

_where \(_{0},_{1},_{2}\) and \(_{3}\) which depend polynomially on \(A_{1},A_{2},A_{3},B,d_{1},d_{2},L\) for any fixed \(\)._

Let \(K=1\) for clarity. To prove Lemma 2, we first note via smoothness of \(F()\) and Assumption 3 that \(\|g_{t}\|_{} C_{0}\|_{t}^{(t)}\|+C_{1}\), and then bound \(\|_{t}^{(t)}\|\). Now, \(g_{s}()=k(,_{s}^{(s)}) F(_{s}^{(s)} )-_{2}k(,_{s}^{(s)})\). When \(\|_{s}^{(s)}-\|\) is large, \(\|g_{s}()\|\) is small due to decay assumptions on the kernel (Assumption 4) implying that the particle does not move much. When \(_{s}^{(s)}\), we have \(g_{s}() k(,_{s}^{(s)}) F()- _{2}k(,_{s}^{(s)})\) and \(k(,_{s}^{(s)}) 0\). This is approximately a gradient descent update on \(F()\) along with a bounded term \(_{2}k(,_{s}^{(s)})\). Thus, the value of \(F(_{t}^{(l)})\) cannot grow too large after \(T\) iterations. By Assumption 2, \(F(_{t}^{(l)})\) being small implies that \(\|_{t}^{(l)}\|\) is small.

Equipped with Lemma 2, we set the step-size \(\) to ensure that the descent lemma (Lemma 1) always holds. The remainder of the proof involves unrolling through Lemma 1 by taking iterated expectations on both sides. To this end we control \( h_{_{t}|_{t}},g_{t}_{}\) and \(\|g_{t}\|_{}^{2}\) in expectation, in Lemma 3.

**Lemma 3**.: _Let Assumptions 1,2,3,4,5 hold and \(\) be as defined in Lemma 2. Then, for \(}{{2A_{1}}}L\),_

\[[ h_{_{t}|_{t}},g_{t}_{ }|_{t}]=\|h_{_{t}|_{t}}\|_{}^{2}[\|g_{t}\|_{}^{2}]}}{{K}}+\|h_{_{t}|_{t}}\|_{}^{2}\]

## 8 Experiments

We compare the performance of GB-SVGD and SVGD. We take \(n=100\) and use the Laplace kernel with \(h=1\) for both. We pick the stepsize \(\) by a grid search for each algorithm. Additional details are presented in Appendix G. We observe that SVGD takes fewer iterations to converge, but the compute time for GB-SVGD is lower. This is similar to the typical behavior of stochastic optimization algorithms like SGD.

**Sampling from Isotropic Gaussian (Figure 1):** As a sanity check, we set \(^{}=(0,)\) with \(d=5\). We pick \(K=10\) for GB-SVGD. The metric of convergence is MMD with respect to the empirical measure of 1000 i.i.d. sampled Gaussians.

**Bayesian Logistic Regression (Figure 2)** We consider the Covertype dataset which contains \( 580,000\) data points with \(d=54\). We consider the same priors suggested in Gershman et al.  and implemented in Liu and Wang . We take \(K=40\) for GB-SVGD. For both VP-SVGD and GB-SVGD, we use AdaGrad with momentum to set the step-sizes as per Liu and Wang 

## 9 Conclusion

We develop two computationally efficient variants of SVGD with provably fast convergence guarantees in the finite-particle regime, and present a wide range of improvements over prior work. A promising avenue of future work could be to establish convergence guarantees for SVGD with general non-logconcave targets, as was considered in recent works on LMC and SGLD . Other important avenues include establishing minimax lower bounds for SVGD and related particle-based variational inference algorithms. Beyond this, we also conjecture that the rates of GB-SVGD can be improved even in the regime \(n KT\). However, we believe this requires new analytic tools.

Figure 1: Gaussian Experiment Comparing SVGD and GB-SVGD averaged over 10 experiments.

Figure 2: Covertype Experiment, averaged over 50 runs. The error bars represent 95% CI.