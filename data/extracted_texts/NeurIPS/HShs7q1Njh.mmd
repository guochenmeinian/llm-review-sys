# LLM Processes: Numerical Predictive Distributions Conditioned on Natural Language

James Requeima

University of Toronto

Vector Institute

requeima@cs.toronto.edu

&John Bronskill

University of Cambridge

jfb54@cam.ac.uk

Equal contribution.

Dami Choi

University of Toronto

choidami@cs.toronto.edu

Richard E. Turner

University of Cambridge

The Alan Turing Institute

ret26@cam.ac.uk

&David Duvenaud

University of Toronto

Vector Institute

duvenaud@cs.toronto.edu

###### Abstract

Machine learning practitioners often face significant challenges in formally integrating their prior knowledge and beliefs into predictive models, limiting the potential for nuanced and context-aware analyses. Moreover, the expertise needed to integrate this prior knowledge into probabilistic modeling typically limits the application of these models to specialists. Our goal is to build a regression model that can process numerical data and make probabilistic predictions at arbitrary locations, guided by natural language text which describes a user's prior knowledge. Large Language Models (LLMs) provide a useful starting point for designing such a tool since they 1) provide an interface where users can incorporate expert insights in natural language and 2) provide an opportunity for leveraging latent problem-relevant knowledge encoded in LLMs that users may not have themselves. We start by exploring strategies for eliciting explicit, coherent numerical predictive distributions from LLMs. We examine these joint predictive distributions, which we call LLM Processes, over arbitrarily-many quantities in settings such as forecasting, multi-dimensional regression, black-box optimization, and image modeling. We investigate the practical details of prompting to elicit coherent predictive distributions, and demonstrate their effectiveness at regression. Finally, we demonstrate the ability to usefully incorporate text into numerical predictions, improving predictive performance and giving quantitative structure that reflects qualitative descriptions. This lets us begin to explore the rich, grounded hypothesis space that LLMs implicitly encode.

## 1 Introduction

Incorporating prior knowledge into predictive models is highly challenging which can restrict the scope for detailed, context-sensitive analysis. In addition, the skill required to incorporate this prior knowledge into probabilistic modelling can restrict the use of these models to experts. In this work, our objective is to develop a probabilistic prediction model that facilitates user interaction through straightforward, natural language. For this purpose, we explore strategies for eliciting explicit, coherent numerical predictive distributions from LLMs.

Why go to so much effort to elicit predictions from a slow, expensive, and sometimes inconsistent model like an LLM? We expect their hypothesis class to be both rich, and grounded in exactly the kinds of high-level side information that we currently struggle to communicate to our numerical models. For instance, knowing that prices rarely go below zero, that certain kinds of sensors can saturate at particular values, or that trends almost always eventually level off, are easy to express in natural language, but not straightforward to incorporate into a model without getting lost in difficult-to-specify details about aspects of the domain that aren't well understood. To summarize, we want to develop such a model because it would allow users to 1) provide prior, potentially expert, information to the model about the problem setting in plain-language rather than attempting to capture this information in closed form priors (e.g. Gaussian Process kernels) and 2) it would allow users to access problem-relevant latent knowledge encoded in LLMs that users may not have themselves.

LLMs have recently been shown to be able to condition on the particular task being solved, leveraging contextual information to make better predictions or decisions . They have also been shown to competitively predict time series based only on a text tokenization of numerical data . In this work, we further push in both these directions; 1) using LLMs for numerical prediction tasks going beyond one-dimensional time series forecasting to multi-dimensional regression and density estimation and 2) exploring the ability of these models to condition on both numerical data and rich, unstructured text to improve these predictions. In this paper we make the following contributions:

* **We define LLM Processes (LLMPs) using methods we develop for eliciting numerical predictive distributions from LLMs.2 LLMPs go beyond one-dimensional time series forecasting to multi-dimensional regression and density estimation. We propose two approaches for defining this joint predictive distribution over a collection of query points and evaluate their compatibility in principle with the consistency axioms necessary to specify a valid statistical process. * **We develop effective prompting practices for eliciting joint numerical predictions.** We investigate various methods for conditioning LLMs on numerical data, including prompt formatting, ordering, and scaling. We characterize which schemes perform best on a set of synthetic tasks.
* **We show that LLMPs are competitive and flexible regressors even on messy data.** Through an extensive set of synthetic and real world experiments, including image reconstruction and black-box function optimization, we evaluate the zero-shot regression and forecasting performance of LLMPs. We demonstrate that LLMPs have well-calibrated uncertainty and are competitive with Gaussian Processes (GPs), LLMTime , and Optuna . We show that LLMPs use in-context learning to automatically leverage information from related datasets, can easily handle missing datapoints, perform image reconstruction, and output multimodal predictive distributions.
* **Lastly, we demonstrate the ability to usefully incorporate problem-relevant information provided through unstructured text into numerical predictions, visualized in Figure 1, resulting in quantitative structure that reflects qualitative descriptions. Other additions such as labelling features using text and specifying units allow LLMPs to make use of usually-ignored side information.

Figure 1: Predictive distributions from an LLMP conditioned on both data and text information. The tenth-percentiles from 50 samples are visualized in faded blue and the median is presented in dark blue with five random samples shown in various colours.

## 2 LLM Processes: Defining a Stochastic Process That Can Condition on Text

Our goal for this section is to use an LLM to elicit joint predictive distributions over arbitrary sized target sets that we can guide and modify using natural language. Formally, given a set of input and output observations \(D_{}=\{(x_{i},y_{i})\}_{i=1}^{M}\) and some text, \(T\), we would like to elicit the predictive distribution defined by an LLM at a collection of targets \(\{(x_{j}^{*},y_{j}^{*})\}_{j=1}^{N}\) denoted \(p_{}(y_{1}^{*},,y_{N}^{*} x_{1}^{*},,x_{N}^{*},D_{ },T)\).

Rejection sampling from an LLM allows us to access what we may interpret as the LLM's predictive distribution and gain insights into the model's inductive biases; sampling from the LLM's categorical distribution over text tokens while ignoring non-numerical tokens yields numerical samples from the LLM. The process of sampling from an LLM is depicted in Figure 2 and Algorithm 1. Sample prompts are in Appendix C. Since an accurate sampling-based empirical distribution incurs a high computational cost, next we define an approach to elicit continuous likelihoods from an LLM.

**Continuous Marginal Likelihoods From an LLM.** We approximate a continuous density over our target values by discretizing the space using bins with arbitrarily fine precision, similar to the method used in Gruver et al. (2017). Crucially, this hierarchical approach allows us to compute the probability of a bin with width \(10^{-n}\). For example, if \(n=1\) then \(\{y[1.0,1.1)\}=p(1)p(.|1)p(0|1.)\) because '1.0' is a prefix for all \(y[1.0,1.1)\). We can convert probability mass to probability density by assuming a uniform distribution within each bin, and dividing the mass by the bin width. A visualization of this construction is in Figures G.2 to G.4.

Unlike (Brockman et al., 2017), we do not rescale the values to remove decimal places. We hypothesize that such scaling removes prior information communicated to the LLM via the scale of the problem. We examine the effect of scaling values in Section 3. We also differ from (Brockman et al., 2017) by including a terminal token after every value in our prompt - for example, given a terminal token \( t\), we represent \(12\) as \(12 t\). Including a terminal token prevents numbers of varying orders of magnitude to share the same prefix - i.e. \(p(1)p(2|1)p( t|12)\) no longer includes the probability of numbers in (Kumar et al., 2019; Kumar et al., 2020, 1300), etc.

Note that this approach does not guarantee that \(P(12 t)\) yields the mass assigned by the LLM to values in the bin \(\) but we empirically observed that our predictive distribution closely matches the sampling distribution to our satisfaction. See Section G.1 for more details and comparison.

**Defining an LLM Process.** Thus far we have established a procedure defining the predictive distribution at a single target location, \(p_{}(y_{n}^{*} x_{n}^{*},D_{},T)\). We now outline two methods which we call independent marginal (I-LLMP) and autoregressive (A-LLMP) predictions, for defining the joint predictive distribution over a collection of target points:

\[p_{}(y_{1}^{*},...,y_{N}^{*} x_{1}^{*},...,x_{N} ^{*},D_{},T) =_{n=1}^{N}p_{}(y_{n}^{*}, x_{n}^{*},D_{ {train}},T) \] \[p_{}(y_{1}^{*},...,y_{N}^{*} x_{1}^{*},...,x_{N} ^{*},D_{},T) =_{n=1}^{N}p_{}(y_{n}^{*} y_{1}^{*},...,y_{n- 1}^{*},x_{1}^{*},...,x_{n}^{*},D_{},T) \]

We note that Equation (1) satisfies the Kolmogorov Extension Theorem (Brockman et al., 2017) therefore defining valid stochastic process (see Appendix A.3). However, it assumes conditional independence given the training set and model weights and the stochastistity represented by the model is via independent marginals. Equation (2) takes inspiration from the autoregressive structure of the LLMs predictive distribution and should yield much richer predictive distributions as we are now able to model

Figure 2: Sampling from an LLM using either independent marginal or autoregressive sampling.

dependencies between output variables. However, this definition is no longer guaranteed to give us a valid stochastic process as the predictive distribution is now target order dependent and will likely fail the Kolmogorov exchangability condition. We investigate both of these questions in Section 3.

**Connection to Neural processes** Neural Processes (NPs)  are a class of meta-learning models parametrized by neural networks and trained to learn a map from training (context) sets to predictive distributions, \(p_{}(y_{1}^{*},,y_{N}^{*} x_{1}^{*},,x_{N}^{*},D_{})\). The definitions in Equations 1 and 2 take inspiration from the joint distributions defined by Conditional NPs  as independent marginals conditioned on the training/context set and Autoregressive NPs  utilizing the chain rule of probability, respectively. Through this lens, LLMPs can be viewed as examples of NPs. However, NPs are directly trained to output this predictive distribution where as LLMPs are repurposing pretrained LLMs.

**Multi-dimensional Density Estimation and Handling Missing Data.** We highlight that, through the flexibility of the LLM prompt, we do not have to draw a distinction between which variables, or variable dimensions are to be modelled or conditioned and can easily handle missing values. Suppose we have a collection of variables \(\{x_{1},,x_{n}\}\) and \(\{y_{1},,y_{m}\}\) (or more), some subset of which we would like to regress on (including \(x\) and \(y\)-values) and the remainder we wish to condition on. To do so using an LLMP, we simply construct the training prompt such that the variables we would like to regress on occur at the end of the prompt and are blank (generated) when sampling from the LLMP. If any values are missing they can simply be removed from the prompt.

## 3 LLMP Configuration

**Experiment Details.** In all of the experiments in Sections 3 to 5, we use six different open source LLMs: Mixtral 8\(\)7B, Mixtral-8\(\)7B-Instruct , Llama-2 7B, Llama-2 70B , Llama-3 8B, and Llama-3 70B . Note that we never modify the LLM parameters via training or fine-tuning, we use only prompting. Our primary metrics are negative log probabilities (NLL) of the model evaluated at the true function values \(f(x^{*})\) averaged over the target locations and Mean Absolute Error (MAE) between the predictive median and the true function value. Unless otherwise stated, we use 50 samples from the LLM at each target location \(x^{*}\) and compute the median and the 95% confidence interval of the sample distribution. Details of the datasets are given in Appendix D. Since the LLMs used in our experiments have undisclosed training sets, we address the steps taken to mitigate the issue of data-leakage in Appendix E. Additional implementation details and processing times are in Appendix F.

**Prompt Engineering.** We perform a set of experiments for determining the best LLMP prompt configuration. We use the Sigmoid, Quadratic, and Linear+Cosine functions with 10, 20 and 75 training points, respectively (see Appendix D.1) with I-LLMP using the Mixtral-8\(\)7B LLM.

* _Prompt Formatting_ Two separators are required to achieve the best performance. One to separate the \(x\) and \(y\) values within a pair and another to separate the \(x,y\) pairs. Figure 3 (_left_) demonstrates that __._n is the best option in terms of performance and token efficiency.
* _Prompt Ordering_ Figure 3 (_middle_) shows that ordering the training points by distance to the current target point is best, outperforming both random and sequential ordering. We posit that ordering by distance provides a hint to the LLM to weigh the contribution of closer training points to the current target point to a greater degree.

Figure 3: NLL and MAE for various prompt formats ordered from the most to least token efficient (_left_), training data orderings (_middle_), and prompt \(y\)-scaling (_right_) using the Mixtral-8\(\)7B LLM. The height of each bar is the mean of 10 random seeds that determine the training point locations. The vertical black lines indicate the standard error. In the Prompt Formatting legend (_left_), the two __._ characters indicate the positions of the \(x\) and \(y\) values and \(\) represents a new line terminal token.

* _Prompt \(y\)-Scaling_ Figure 3 (_right_) shows that performance degrades as the range of the \(y\) components of the training points increases and when incorporating negative values. This is due to the fact that when the range is wider, the LLM must accurately generate more numerical digits and potentially a negative sign when predicting \(f(x^{*})\).
* _top-\(p\) and Temperature_ Figure 9 shows that performance is surprisingly insensitive to varying the LLM nucleus sampling parameter top-\(p\) and LLM softmax temperature.

**Autoregressive vs Independent Marginal Predictions.** Here we examine two questions: first, does the autoregressive defininiton of the joint predictive likelihood (A-LLMP) in Equation (2) improve performance versus the independent marginal definition of Equation (1) (I-LLMP). Second, "how close" is A-LLMP to a stochastic process in terms of performance variability across query orderings.

We first look at log-likelihoods and MAE for A-LLMP and I-LLMP using the random and distance training point orderings discussed earlier. Results can be seen in Figure 4 (_left_). Similar to our findings earlier, ordering the training values according to distance to target has a large effect, improving performance for both I-LLMP and A-LLMP. Unsurprisingly, the richer joint distribution given by A-LLMP gives us better predictive performance.

We next examine the variability in performance of A-LLMP when different autoregressive target orderings are used to get a sense of how far our method is from a stochastic process (which would be permutation invariant in the target points). The results of using ten sets of randomly ordered target points compared to I-LLMP and the ground truth log-likelihood of the test sample under the generative distribution are presented in Figure 4 (_center_). Note that the training data is distance sorted in all cases. We also present the result when ordering target points according to distance to the closest training point, from smallest to largest. We make three key observations: first, log-likelihood performance of all A-LLMP orderings is better than I-LLMP. Second, the variance of random orderings is small on the scale of the log-likelihood of the generative model. And third, distance ordering the targets gives better or at least competitive performance with a random ordering. These results present practitioners a choice: do you care more about using a valid statistical process or obtaining good predictive performance? If it is the latter, you would be better served using A-LLMP.

## 4 Evaluating LLMP Performance on Numerical Data

In this section, we evaluate the performance of LLMPs on purely numerical data in a wide variety of settings. Additional details and results for experiments in this section can be found in Appendix H.

**1D Synthetic Data Experiments.** To show that LLMPs are a viable regression model with well-calibrated uncertainties, we benchmark in Table 1 our A-LLMP method against a GP on the Function Dataset (Appendix D.1). The GP uses an RBF kernel with optimized length scale and noise. The Mixtral-8\(\)7B A-LLMP achieves the lowest negative log-likelihoods averaged over 7 function sizes and 3 seeds on 10 out of 12 of the functions and equal or better MAE on 8 of the functions. Visualizations of the predictive distributions and plots of MAE and A-LLMP are shown in Appendix H.1.

Figure 4: Autoregressive Experiments. _Left:_ NLL and MAE for A-LLMP and I-LLMP using different prompt orderings using the Mixtral-8x7B LLM. The height of each bar is the mean of 3 random seeds that determine the training point locations. The black lines indicate the standard error. _Center:_ Log-likelihood results of using various test set orderings with Llama-2-7B, Llama-2-70B and Mixtral-8x7B A-LLMP. The orange X indicates I-LLMP, the purple circles used distance ordered test points, and the blue whiskers are the mean and standard error of 10 randomly sampled test orderings. The red dashed line shows the log-likelihood of the test set under the generative process. _Right:_ Heatmap visualization of the Llama-3-70B A-LLMP predictive distribution conditioned on data from a bimodal generative process. Black dots are training points.

To verify that LLMPs are able to produce non-Gaussian, multimodal predictive distributions we sampled training data from synthetic, multimodal generative distribution (experimental details in Appendix H.2). The Llama-3-70B LLMP predictive distribution is visualized in Figure 4 (_right_).

**Comparison to LLMTime.** Figure 5 demonstrates that A-LLMP yields superior results in terms of MAE and NLL when compared to LLMTime using Llama-2-7B on a forecasting task using the weather dataset (described in Appendix D.2). Additional plots with missing training data are in Appendix H.3. We posit that A-LLMP betters LLMTime due to the fact that 1) A-LLMP naturally handles irregularly spaced \(x\) and \(y\) data whereas LLMTime uses only regularly spaced \(y\) information requiring imputation with NAN values where data is missing; and 2) A-LLMP performs no scaling on \(y\) values in contrast to LLMTime that scales data to eliminate the use of decimals and normalize the range of the data and as a result removes information that the LLM can potentially leverage.

**Comparison to From Words to Numbers.** We compare our I-LLMP method to the approach in  on their Original #1 dataset. The experimental set-up is as follows: There are 100 trials with each trial consisting of 50 training points and a single target point. The training and target points for each trial are randomly generated using the function described in . We use the code from their paper to generate the data and evaluate their approach and compare it to ours using identical numerical data. We use the Llama-2-7B LLM for both methods to ensure a fair comparison. I-LLMP achieved lower MAE on 78 of the 100 trials when compared to their method. When the errors are averaged over the 100 trials, the I-LLMP average error was 0.836 and theirs was 3.137. These results indicate that our LLMP approach is clearly superior. This is due to the facts that (i) we sort the training points according to distance to the current target point when creating the prompt whereas they do not, and (ii) we form a distributional estimate for the predicted point and then take the median sample value as the best estimate, whereas they generate a single point estimate.

In the next three experiments we showcase the ability of LLMPs to handle multi-dimensional data.

**Image Reconstruction** As a 2-dimensional input experiment, Figure 6 shows reconstruction results from images drawn from the Fashion-MNIST dataset . We convert pixel data into prompt data points by forming a series of (row, column, pixel value) tuples. Additional results and details are in Appendix H.4. Using 20% train pixels, the basic form is captured and at 50%, the reconstruction is accurate despite the sharp pixel intensity transitions.

    &  &  \\   & **Metric** & **Best** & **Exp** & **Gen Wave** & **Linear** & **Lin \(\)** & **Lin \(\)** & **Lin \(\)** & **Lin \(\)** & **Log** & **Quadratic** & **Signal** & **Size** & **X \(\)** **Size** \\   & MAE & 0.3140.0 & 0.3140.1 & **0.3040.1** & **0.3040.1** & **0.3040.0** & **0.3140.0** & **0.3140.0** & **0.3040.0** & **0.3170.0** & **0.3040.0** & **0.3170.0** & **0.3040.0** & **0.3140.0** & **0.3120.0** & **0.3120.0** & **0.3120.0** \\  & NLL & 0.3140.0 & -2.300.0 & -3.0140.0 & **-4.114.2** & -1.450.0 & **-2.444.0** & **-4.640.0** & **-1.380.0** & -2.3570.0 & -1.040.0 & -2.0360.1 & -1.440.0 & 0.2360.2 & **1.2640.1** \\   & MAE & **0.3180.01** & **0.3080.01** & **0.3040.01** & **0.3040.01** & **0.3040.01** & **0.3040.01** & **0.3040.01** & **0.3040.01** & **0.3040.01** & **0.3040.01** & **0.3040.01** & **0.3040.02** & **0.3040.02** & **0.3040.02** & **0.3

**Black-Box Function Optimization** Black-box optimization involves minimizing or maximizing a function where there is only access to the output of a function for a specified input. We benchmark the ability of LLMPs to perform maximization on six commonly used multi-dimensional functions. We compare our results using Llama-2-7B to Optuna , a commercial hyperparameter optimization framework. Results and implementation details are in Appendix H.5. In all cases, LLMPs obtain as good or better approximation to the true maximum value in a fewer number of trials.

**Simultaneous Temperature, Rainfall, and Wind Speed Regression** To examine how well an LLMP can model multi-dimensional outputs, we compare LLMP regression to a multi-output GP on the weather dataset described in Appendix D.2. Figure 7 shows the results for the Llama-3-8B LLM (_top_) and a 3 output RBF kernel GP with trained hyperparameters (_bottom_). The LLM is similar to and in most cases better than the GP in terms of MAE and NLL.

**In-context Learning Using Related Data Examples.** In this experiment, we investigate LLMPs' ability to learn from similar examples in-context to predict average monthly precipitation across 13 Canadian locations , one from each province and territory. For each location, we use the Mistral-8\(\)7B A-LLMP to forecast 32 months of average precipitation values given the previous four month observations taken from a random historical three-year period between 1913-2017 (conditional on data availability). It is then provided with 1-12 examples of random three year periods of historical values from the same location in-context. Results shown in Figure 8 and experimental details in Appendix H.6. Conditioning the LLMP on historical examples improves performance saturating after 4 years, and degrading slightly thereafter. Generally, the LLMP is able to use the examples to pick up on seasonal trends from history. We note that some locations do not have obvious or strong seasonal patterns but examples still help performance in these cases (see Appendix H.6).

## 5 Conditioning LLMPs on Textual Information

One of the most exciting directions of LLMPs is the potential to incorporate prior information about problems via text. Now that we can examine functional predictive distributions of LLMs, we can begin to explore their rich prior over functions by conditioning on both text and numerical data. In this section we present two experiments with details and additional experiments presented in Appendix I.

**Scenario-conditional Predictions.** In this experiment, we examine the influence of text providing information about various synthetic problem settings on the predictive distribution of an LLMPs. In

Figure 8: (_Left three plots_) Visualizations of the predictions given by the Mistral-8\(\)7B LLMP for Ranfurly, Alberta. Blue and black circles are training and test points, respectively. Red circles are median predictions and shaded areas indicate tenth-percentiles over 30 samples. (_Right_) NLL vs number of examples. Error bars show standard error over 13 locations.

Figure 7: Results for simultaneously predicting temperature, precipitation, and wind speed using the Llama-3-7B LLM (_top_) and a 3 output RBF kernel GP with trained hyperparameters (_bottom_).

all of the following examples, we provide the same two synthetic training points to the LLMP but change the prompting text that comes before the training data. We then use A-LLMP with Llama-3-70B to forecast trajectories 50 steps ahead. We begin by examining the predictive distribution with no prompt (Figure 8(a)). We prompt the LLMP to generate daily temperature measurements in degrees Celsius from Montreal in January (Figure 8(b)) and May (Figure 8(c)), and monthly precipitation values from San Diego, CA (Figure 8(d)) and Singapore (Figure 8(e)). Figure 1 shows the results of prompting the LLMP to generate (_left_) a stock price financial time series (_centre_) for a company that eventually goes out of business and (_right_) for a company whose price goes to zero on day 30.

Indeed, the LLMP modifies the predictive distribution accordingly relative to the no prompt predictions. We highlight the following observations: first, for prompts b) and c), the model moves about half of its predictive mass below zero for temperatures beginning in January and above zero for the May temperatures. Second, the LLMP is able to recall actual historical trends for average monthly precipitation for Singapore and San Diego to condition on prompts d) and e). Despite getting the trend correct, we note that the median prediction in d) seems to be biased toward the training values and not reflective of the actual monthly median.

Last, for stock price simulations, the model places all of its density on positive numbers since it is modelling prices. It is able to produce realistic trajectories and decreases them in expectation when prompted that the company goes out of business. The model is able to condition on the fact that the price goes to zero on day 30 which correctly interprets the meaning of the \(x\)-values as days starting from \(0\), that the \(y\)-axis is the price and the phrase "price goes to zero" corresponds to a \(y\)-value of 0.

**Labelling Features Using Text.** In the following example, we examine the performance of a Mistral-8x7B Instruct I-LLMP on predicting American housing prices. The dataset  contains 39980 housing prices and various variables around housing and demographics for the top 50 American cities by population. Note that this dataset was generated on 12/09/2023, however it contains data from the 2020 US Census and the 2022 American Community Survey (ACS) so we cannot guarantee that models did not see data within this dataset during training.

For each prediction task, we show the I-LLMP 10 randomly selected training examples from the dataset and predict on 20 randomly selected test examples. In the prompt, before the numerical value (price) we provide a string which encodes the datapoint index/features that the model can use. For our first experiment we examine the behaviour of the LLMP when more features are added to the prompt. We experiment with five ways of indexing the training and test points; For case (1), we provide latitude and longitude of the house as numerical values (eg. 32.74831, -97.21828) converted to strings similar to our method in previous experiments. For the remaining 4 cases, we provide additional labeled features, adding more features for each case with the prompt for case (5) containing

Figure 9: a)-e) predictive distributions from an A-LLMP using Llama-3-70B under various scenario prompts. Black points are two training points given to the LLM process, the same values for each scenario. The tenth-percentiles from 50 samples are visualized in faded blue and the median is presented in dark blue with five random samples shown in various colours. Figure f) shows the actual average monthly rainfall for Singapore from 1991-2020  and San Diego from 2000-2024 .

all labelled features, illustrated with the following example: (2) Location: Fort Worth, Texas, Latitude: 32.74831, Longitude: -97.21828, (3) Zip Code: 76112, Median Household Income: 71452.0, (4) Zip Code Population: 42404 people, Zip Code Density: 1445.0 people per square mile, (5) Living Space: 1620 square feet, Number of Bedrooms: 3, Number of Bathrooms: 2.

This procedure is repeated 10 times to compute statistics. Results are presented in Figure 10 (_left, centre_). Note that the LLMP is able to take advantage of the additional features provided to improve predictive performance. To see examine the effect of adding text labels to the features, we ran another set of experiments on 10 new random datasets providing the LLMP with either labeled or unlabelled numerical features. The following are example feature strings: (i) "30.45738, -97.75516" (ii) "Location: Austin, Texas, Latitude: 30.45738, Longitude: -97.75516" (iii) "30.45738, -97.75516, 78729, 107830.0, 30907, 1216.1, 1349, 3" (iv) "Location: Austin, Texas, Latitude: 30.45738, Longitude: -97.75516, Zip Code: 78729, Median Household Income: 107830.0, Zip Code Population: 30907 people, Zip Code Density: 1216.1 people per square mile, Living Space: 1349 square feet, Number of Bedrooms: 3, Number of Bathrooms: 2". Results of this experiment are presented in Figure 10 (_right_). Note that the LLMP is not able to use the raw feature values to improve performance from only 10 training examples, but is able to do so with labelled features suggesting that LLM is able to utilize the latent relationship between the feature and the price once the feature is identified. We found that the Mixtral-8\(\)7B Instruct model had the best performance on this task and was able to utilize text information better (results for other models in Appendix I.2).

## 6 Related Work

In this section, we discuss work related to eliciting distributions from LLMs including forecasting, regression, in-context learning, and neural processes among others.

**LLM Forecasting** The most closely related work to ours is LLMTime . LLMTime is capable of zero-shot extrapolation of one-dimensional time series data at a level comparable to trained purpose-built approaches. In addition, they develop a method for eliciting marginal probability distribution functions from LLM posteriors over functions, which we build on. They also begin to investigate the effect of conditioning on text. In contrast, we focus on (i) interpolation with multi-dimensional inputs and outputs; (ii) eliciting joint distributions over functions, not just marginals; and (iii) exploring the ability of models to condition simultaneously on both numerical data and text. More recently, TimesFM , a foundation model for one-dimensional zero-shot times series forecasting was introduced. However, TimesFM does not support interpolation or higher dimensional data and does not consider distributions. PromptCast  performs zero-shot time series forecasting by combining numerical data and text in a question answer format. Our approach for combining problem specific text along with numerical data differs in that it handles both interpolation and extrapolation and does not rely on a question-answer format. Hegselmann et al.  utilize LLMs to do zero-shot and few-shot classification on tabular data that compares favorably to standard ML approaches.

**LLM Regression** Pesut  do some initial investigations into the use of LLMs as regressors on 1D synthetic functions. Our work greatly expands on these early investigations. Vacareanu et al.  is concurrent work that shows that LLMs are capable linear and non-linear regressors. However, their work does not condition on any textual information, compute log probabilities, compare to Gaussian Processes, investigate the effect of prompt formatting, or employ auto-regressive sampling.

Figure 10: Results of a Mixtral-8x7B Instruct I-LLMP predicting US housing prices. _Left:_ Predictions for 10 randomly selected houses using index style 1) and 5). Xs are mean predictions using 30 samples from the LLMP and error bars indicate 2 standard deviations. _Centre and right:_ Average MAE and NLL performance of the LLMP over 10 experiments with error bars representing the standard error for experiments from Section 5.

**In-context learning (ICL) in LLMs** Xie et al.  point out that ICL can be seen as being equivalent to Bayesian inference in a latent variable model. More recently,  explain in-context learning in LLMs as kernel regression. Garg et al.  train transformers to do in-context learning on various function classes including linear (up to 50 dimensions), decision trees, and two-layer ReLU networks. Coda-Forno et al.  demonstrate that LLMs are capable of meta-in-context learning and that performance on 1-D linear regression and two-armed bandit tasks improves with multiple examples. TabPFN  is a trained transformer that is able to do tabular classification given in-context examples.

**LLM Hyperparameter Optimization** Zhang et al.  and Liu et al.  use LLMs to perform hyperparameter optimization, showing that LLMs can condition on a mixture of textual data as numerical observations to effectively optimize hyperparameters in machine learning models.

**Eliciting priors from LLMs** Binz and Schulz  fine-tune LLMs on data from psychological experiments to achieve accurate representations of human behavior. Choi et al.  show how using an LLM to assess the importance of features or the causal relationship between variables that can improve performance on tasks. Lipkin et al.  find that LLMs can derive human-like distributions over the interpretations of complex pragmatic utterances.

**Eliciting distributions from humans** Schulz et al.  look at compositional inductive biases in function learning, showing humans have compositional structure in their priors on functions.  catalogue standard strategies for eliciting distributions from expert humans.

**Neural processes** Neural Processes are a class of meta-learning models trained to learn a map from training (context) sets to predictive distributions, \(p_{}(y_{1}^{*},,y_{N}^{*} x_{1}^{*},,x_{N}^{*},D_{ })\). These models are parameterized using a neural network and there have been various proposals for different architectures using attention , transformers , Gaussian Process output layers , and diffusion models . The definitions of the joint distributions in equations 1 and 2 take inspiration from the joint distributions defined by Conditional Neural Processes  as independent marginals conditioned on the training/context set and Autoregressive Neural Processes  utilizing the chain rule of probability, respectively. Through this lens, LLMPs can be viewed as examples of Neural Processes. LLMPs differ from standard NPs in two main ways: (i) Training objective: Neural Processes are meta-trained using maximum likelihood to optimize \(p(y^{*}|x^{*},D_{})\) directly. LLMPs have a very indirect training procedure - they are trained to be language models i.e. autoregressive token predictors. One of the contributions of this paper is the demonstration that, despite this, they can perform zero-shot probabilistic regression. (ii) Architecture: NPs have an output layer that parametrizes the predictive distribution over targets directly. Since LLMPs are repurposing language models for regression, we need to define the mapping from distributions over language tokens to distributions over target variables. We note that LLMs themselves can be viewed as AR-CNPS  with a fixed, predefined target ordering.

## 7 Discussion, Limitations, and Societal Impact

Below we discuss our findings, the limitations and societal impact of the work presented. Further discussion on these issues can be found in Appendix J.

**Discussion** We defined LLMPs for eliciting numerical predictive distributions from LLMs and when used as a zero-shot muti-dimensional regression model are competitive with GPs. Excitingly, we demonstrated the ability to condition on text to improve predictions and probe the LLMs' hypothesis space. An interesting extension would be to condition on other modalities in addition to text.

**Limitations** Along with the flexibility of LLMs, LLMPs inherit their drawbacks. Maximum context sizes limit the size of tasks we can apply this method to and the amount of textual information we can condition on. LLMPs are also significantly more computationally expensive compared to Gaussian Processes and standard regression methods. All of experiments were performed on readily available open source LLMs that are smaller and generally less capable compared to proprietary LLMs.

**Societal Impact** Our work has demonstrated a new and useful zero-shot approach for generating probabilistic predictions using plain language to augment numerical data. It has the potential to allow practitioners from fields such as medical research and climate modelling to more easily access probabilistic modelling and machine learning. Like all machine learning technology, there is potential for abuse, and possible consequences from incorrect predictions made with LLMPs. Also, we do not know the biases in the underlying LLMs used and what effect they may have on LLMPs output.