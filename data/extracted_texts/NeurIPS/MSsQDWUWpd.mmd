# Analysis of Corrected Graph Convolutions

Robert Wang

Cheriton School of Computer Science, University of Waterloo

Aseem Baranwal

Cheriton School of Computer Science, University of Waterloo

Kimon Fontoulakis

Cheriton School of Computer Science, University of Waterloo

###### Abstract

Machine learning for node classification on graphs is a prominent area driven by applications such as recommendation systems. State-of-the-art models often use multiple graph convolutions on the data, as empirical evidence suggests they can enhance performance. However, it has been shown empirically and theoretically, that too many graph convolutions can degrade performance significantly, a phenomenon known as oversmoothing. In this paper, we provide a rigorous theoretical analysis, based on the two-class contextual stochastic block model (CSBM), of the performance of vanilla graph convolution from which we remove the principal eigenvector to avoid oversmoothing. We perform a spectral analysis for \(k\) rounds of corrected graph convolutions, and we provide results for partial and exact classification. For partial classification, we show that each round of convolution can reduce the misclassification error exponentially up to a saturation level, after which performance does not worsen. We also extend this analysis to the multi-class setting with features distributed according to a Gaussian mixture model. For exact classification, we show that the separability threshold can be improved exponentially up to \(O( n/ n)\) corrected convolutions.

## 1 Introduction

Graphs naturally represent complex relational information found in a plethora of applications such as social analysis (Backstrom and Leskovec, 2011), recommendation systems (Ying et al., 2018; Borisyuk et al., 2024), computer vision (Monti et al., 2017), materials science and chemistry (Reiser et al., 2022), statistical physics (Battaglia et al., 2016; Bapst et al., 2020), financial forensics (Zhang et al., 2017; Weber et al., 2019) and traffic prediction in Google Maps (Derrow-Pinion et al., 2021).

The abundance of relational information in combination with features of the corresponding entities has led to improved performance of machine learning models for classification and regression tasks. Central to the field of machine learning on graphs is the graph convolution operation. It has been shown empirically (Defferrard et al., 2016; Kipf and Welling, 2017; Gasteiger et al., 2019; Rossi et al., 2020) that using graph convolutions to the feature data enhances the prediction performance of a model, but too many graph convolutions can have the opposite effect (Oono and Suzuki, 2020; Chen et al., 2020; Keriven, 2022; Wu et al., 2023), an issue known as oversmoothing. Several solutions have been proposed for this problem, we refer the reader to the survey of Rusch et al. (2023).

In this paper we provide a rigorous spectral analysis, based on the contextual stochastic block model (Deshpande et al., 2018), to show that the oversmoothing phenomenon can be alleviated by excluding the principal eigenvector's component from the graph convolution matrix. This is similar to the state-of-the-art normalization approach used in Zhao and Akoglu (2020), Rusch et al. (2023) (PairNorm). However, our method explicitly uses the principal eigenvector. We provide below some intuition about why excluding the principal eigenvector helps to alleviate over-smoothing.

Let \(A\) be the adjacency matrix of the given graph, and \(D\) be the degree matrix. Vanilla graph convolutions are represented using matrices such as \(D^{-1}A\) or \(D^{-1/2}AD^{-1/2}\)[Kipf and Welling, 2017]. Suppose our graph is \(d-\)regular, meaning that each node has exactly \(d\) neighbors. In this case, both graph convolutions reduce to \(A\). The top eigenvector of \(A\) is \(\) with eigenvalue \(d\), where \(\) is the vector of all ones. This means that \(_{k}}A^{k}=^{}\), which implies that applying many convolutions is equivalent to projecting our data onto the all-ones vector. Thus, all feature values will converge to the same point. Therefore, we should expect, as verified by most real-world and synthetic experiments, that many rounds of the convolution \(xAx\) will lead to a large learning error. However, if we instead perform convolution with the corrected matrix \(:=A-^{}\), then the convergent behavior of \(x^{k}x\) would be equivalent to projecting \(x\) onto the _second_ eigenvector of \(A\). This eigenvector is known to capture information about sparse bipartitions in the graph \(G\)[Cheeger, 1970, Alon and Milman, 1985, Alon, 1986], and so for certain problems, like binary classification, we may expect this eigenvector to capture a larger amount of information about our signal. We note that another well-studied graph matrix is the Laplacian, D-A. In the regular case, this has the same eigenvectors as the adjacency matrix, but with reversed spectrum. The trivial eigenvector we remove is exactly the Nullspace of the Laplacian.

In our analysis, we study the classification problem in the contextual stochastic block model, with a focus on linear, binary classification. Our results are stated in terms of the following corrected convolution matrices:

\[=D^{-1/2}AD^{-1/2}-^{}D}D^{1/2} ^{}D^{1/2}=A- ^{},\] (1)

where \(d:=2|E|/n\) is the empirical average degree in \(A\), where \(|E|\) is the number of edges in the graph. Note that \(\) is derived from the _normalized_ adjacency matrix, while \(\) is (up to a scalar multiple) its _unnormalized_ counterpart. Briefly, we demonstrate that when the graph is of reasonable quality, the corrected graph convolutions exponentially improve both partial and exact classification guarantees. Depending on the density and quality of the given graph, improvement becomes saturated after \(( n)\) convolutions in our partial and exact classification results. However, in comparison to a similar analysis in [Wu et al., 2023] for vanilla graph convolutions (without correction), we show that classification accuracy does not become worse as the number of convolutions increases.

### Our Contributions

In this work, we provide, to our knowledge, the first theoretical guarantees on partial and exact classification after \(k\) rounds of graph convolutions in the contextual stochastic block model. Our main result is to show that each graph convolution with the corrected matrix reduces the classification error by a multiplicative factor until a certain point of "saturation" and the number of convolutions required until saturation depends on the amount of input feature variance. We show that the accuracy of the linear classifier at the point of saturation only depends on the strength of the signal from the graph. This is in contrast to the uncorrected convolution matrix, which will always exhibit a decrease in classification accuracy after many convolutions. Finally, we show that given slightly stronger assumptions on graph density and signal strength, the convolved data at the point of saturation will be linearly separable with high probability. To quantify our results, we let \(p\) and \(q\) be the intra- and inter-class edge probabilities with \((p,q)=|p-q|/(p+q)\) being the "relative signal strength" in the graph. Let \(\) be the expected degree of each vertex. Our results can be summarized as follows:

* If \(p+q(n}{n})\) and \((})\), each convolution with \(\) reduces classification error by a factor of about \(}\) until the fraction of errors is \(O(})\)
* If \(p+q(n}{n})\) and \((})\), each convolution with \(\) reduces classification error by a factor of about \(}\) until the fraction of errors is \(O(})\)
* If \(p+q(n}{n})\), \((k})\) and the input features has signal-to-noise ratio at least \((})\), the data is linearly separable after \(k\) rounds of convolutions with \(\).

To obtain our partial classification results, we use spectral analysis to bound the mean-squared-error between the convolved features and the true signal. For exact classification, we prove a concentration inequality on the total amount of message received by a vertex through "incorrect paths" of length \(k\) after \(k\) rounds of convolution through a combinatorial moment analysis. Using this, we establish entry-wise bounds on the deviation of the convolved feature vector from the true signal.

Finally, we extend our partial-recovery result to the multi-class setting. In this setting, we assume our features are distributed according to a Gaussian mixture model with \(L\) equal-sized clusters and our graph is distributed according to a \(L\)-block stochastic block model. Our analysis for partial recovery generalizes easily to the multi-class setting with the use of basic non-linear classifiers. Just as before, we show that convolution with the corrected, un-normalized adjacency matrix, \(\), reduces classification error by a constant fraction each round, until a point of saturation where no further improvement is made.

## 2 Literature review

Research on graph learning has increasingly focused on methods that integrate node features with relational information, particularly within the semi-supervised node classification framework, see, for example, Scarselli et al. (2009), Cheng et al. (2011), Gilbert et al. (2012), Dang and Viennet (2012), Gunnemann et al. (2013), Yang et al. (2013), Hamilton et al. (2017), Jin et al. (2019), Mehta et al. (2019), Chien et al. (2022), Yan et al. (2021). These studies have underscored the empirical advantages of incorporating graph structures when available.

The literature also addresses the expressive capacity (Lu et al., 2017, Balcilar et al., 2021) and generalization potential (Maskey et al., 2022) of Graph Neural Networks (GNNs), including challenges like oversmoothing (Keriven, 2022, Xu et al., 2021, Oono and Suzuki, 2020, Li et al., 2018, Rusch et al., 2023). In our paper, we ground our work on the contextual stochastic block model (Deshpande et al., 2018), a widely used statistical framework for analyzing graph learning and inference problems. Recent theoretical studies have extensively used the CSBM to illustrate several statistical, information-theoretic, and combinatorial results on relational data accompanied by node features. In Deshpande et al. (2018), Lu and Sen (2020), the authors investigate the classification thresholds for accurately classifying a significant portion of nodes from this model, given linear sample complexity and large but bounded degree. Additionally, Hou et al. (2020) introduces graph smoothness metrics to quantify the utility of graphical information. Further developments in Chien et al. (2021, 2022), Baranwal et al. (2021, 2023) extend the application of CSBM, establishing exact classification thresholds for graph convolutions in multi-layer networks, accompanied by generalization guarantees. A theoretical exploration of the graph attention mechanism (GAT) is provided by Fountoulakis et al. (2023), delineating conditions under which attention can improve node classification tasks. More recently, Baranwal et al. (2023) provide the locally Bayes optimal message-passing architecture for node classification for the general CSBM.

In this paper, we provide exact and partial classification guarantees for multiple graph convolution operations. Previous investigations have often been confined to a few convolution layers, limiting the understanding of their effects on variance reduction (see, for example, Baranwal et al. (2023)). Our findings contribute a novel spectral perspective on graph convolutions, describing how the fraction of recoverable nodes is influenced by the signal-to-noise ratio in the node features and the scaled difference between intra- and inter-class edge probabilities. We also demonstrate that the oversmoothing phenomenon can be alleviated by excluding the principal eigenvector's component from the adjacency matrix - a strategy somewhat akin to the normalization approach used in Zhao and Akglu (2020)(PairNorm), albeit our method explicitly uses the principal eigenvector and is grounded in rigorous spectral justifications.

In a relatively recent work (Wu et al., 2023) the authors rigorously analyze the phenomenon of oversmoothing in GNNs for the 2-block CSBM by identifying two competing effects of graph convolutions: the mixing effect, which homogenizes node representations across different classes, and the denoising effect, which homogenizes node representations within the same class. Their analysis shows that oversmoothing occurs when the mixing effect dominates the denoising effect, and they quantify the number of layers required for this transition. In contrast, we work with the corrected graph convolution in the 2-block CSBM and show that it improves performance exponentially up to saturation, after which more convolutions do not improve nor degrade performance. On a technical level, the previous work only analyzes the distribution of a single node's feature values after convolution and does not take into account correlations between nodes. In our work, we use spectral analysis of higher powers of the convolution matrix, which takes into account correlations between nodes to obtain our partial classification results over the whole dataset. To handle the modified convolution in the exact classification setting, we analyze the error more directly through matrix perturbation analysis rather than trying to directly count the higher-order neighbors of each vertex as in previous works .

## 3 Preliminaries and Model Description

Throughout this paper, we use \(1\) to denote the all-ones vector and \(e_{i}\) to denote the \(i^{th}\) standard basis vector in \(^{n}\). Given a vector \(x^{n}\), we use \(\|x\|\) to denote its Euclidean norm \(^{n}x(i)^{2}}\). We use \(\|x\|_{}\) to denote its infinity norm, \(_{i=1}^{n}|x(i)|\). For a matrix \(M^{n}\), we use \(\|M\|\) to denote its operator norm, \(_{x 0,\|x\|=1}\|Mx\|\). We use \(\|M\|_{F}=M_{i,j}^{2}}\) to denote its Frobenius norm. We also make routine use of the spectral theorem, which says that if \(M\) is a \(n n\) symmetric matrix, then it can be diagonalized with \(n\) orthogonal eigenvectors and real eigenvalues. In particular, there exist \(_{1},_{2},..._{n}\) and orthonormal vectors \(w_{1},w_{2},...w_{n}^{n}\) such that \(M=_{i=1}^{n}_{i}w_{i}w_{i}^{}\). Note that when \(M\) is symmetric, \(\|M\|=_{i}|_{i}|=_{x:\|x\|=1} |x^{}Mx|\).

Finally, we use the \((,)\) to a Gaussian distribution with mean \(\) and covariance matrix \(\). For one-dimensional Gaussians, we use \((,^{2})\). For \(X(,^{2})\), we will frequently use the Gaussian tail bound: \([\|X-|>t](-}{2})\).

### Contextual Stochastic Block Model

In this section, we formally describe the contextual stochastic block model introduced by . Our model is defined by parameters \(n,m\), \(p,q\), \(,,^{m}\) and \(^{+}\) In the model, we are given a random undirected graph, \(G=(V,E)\), where \(|V|=n\), drawn from the 2-block stochastic block model and features drawn from the Gaussian mixture model. Our vertices are partitioned into two classes, \(S\) and \(T\), of size \(n/2\), which we want to recover. For each pair of vertices \(i,j S\) and \(i,j T\), the edge \((i,j)\) is in \(E\) independently with probability \(p\) while for each pair \(i S\) and \(j T\), the edge \((i,j)\) is in \(E\) with probability \(q\). In addition to the graph, we are also given a feature matrix \(X^{n m}\) drawn from a Gaussian mixture model with two centers \(\) and \(\). For each \(i V\), we let \(g_{i}(0,^{2}I_{m})\) be an i.i.d. Gaussian noise vector. Now let \((x_{i})_{i n}\) be the rows of \(X\). For \(i S\), we have \(x_{i}=+g_{i}\) and for each \(i T\), we have \(x_{i}=+g_{i}\).

In the multi-class setting, our nodes are partitioned into \(L\) classes, \(_{1},..._{L}\), of size \(n/L\). The inter-class edge probability is \(p\) and intra-class edge probability is \(q\). We assume the features are generated by a Gaussian mixture with \(L\) centers \(c_{1},...c_{L}^{m}\). If node \(i\) is in class \(l\), then we observe its feature vector as \(x_{j}=c_{l}+g_{i}\). In addition, we will let \(_{i}:=c_{l}\), for \(i_{l}\), denote the center for vertex \(i\).

## 4 Results and Interpretation

In our analysis, there are two types of objectives. In the exact classification objective, the aim is to exactly recover \(S\) and \(T\) with probability \(1-o(1)\). In the partial classification, or "detection" problem, the goal is to correctly classify \(1-o(1)\) fraction of vertices correctly with probability \(1-o(1)\). We begin by stating our results for the partial classification regime. For ease of notation, we will assume \(p>q\) from this point forward. We show in Appendix B.1 that this assumption is made without loss of generality.

**Theorem 4.1**.: _Suppose we are given a 2-block \(m\)-dimensional CSBM with parameters \(n,p>q,,,\) satisfying \((p,q):=(})\) and \(p(n}{n})\). There exists a linear classifier such that after \(k\) rounds of convolution with \(\), will, with probability at least \(1-(-}{^{2}} )\), misclassify at most_

\[O(p}+}^{2k}}{\|-\|^{2}}n n)\]vertices, where \(C\) is an absolute constant. Furthermore, if \((})\) then with probability at least \(1-(-}{^{2}} )\), the same linear classifier after \(k\) rounds of convolution with \(\) will misclassify at most_

\[O(p}+} ^{2k}}{\|-\|^{2}}n n)\]

_vertices._

Now we take a closer look at the error bound. For brevity, we will focus on our results regarding convolutions with \(\). First, we see that an important ratio in our bound is the term \(1/(^{2}np)\). This term is small if \(^{2}\) is much larger than the inverse of the expected degree of each vertex, \((p+q)n/2\), which is at most \(np\). Our assumption that this ratio is upper bounded by a constant means that we need the signal from the graph to be sufficiently strong. Now, if we examine our misclassification error bound, and let \(=C/(^{2}np)\) where \(C\) is a sufficiently large constant, then we see that the _fraction_ of misclassified vertices is at most \(+^{k}^{2} n/\|-\|^{2}\). Our assumption on the parameters ensures that \(<1\). Note that only the second term depends on \(k\), and the feature's noise-to-signal ratio. This term measures the amount of error introduced by the variance in the features and exponentially decreases with \(k\). Moreover, after about \(k=_{1/}(^{2} n/(\|-\|^{2}))\) convolutions, the \(\) term, which only depends on graph parameters, will dominate over the variance term, indicating that more convolutions will not improve the quality of the convolved features beyond the quality of the signal from the graph. If \(/\|-\|\) is constant, we will always reach our optimal error bound of \(O()\) when \(k=O( n)\). Moreover, if \(=(1)\), as was assumed in Baranwal et al. (2021), then we will have \(1/(np)\). This means even when \(/\|-\|\), we will reach optimality in constant number of convolutions with high probability if the graph is moderately dense. For example, if \(p=1/\), then we only need \(3\) convolutions and if \(p=(1)\), then we only need \(2\). On the other hand, if \(\) is on the order of \((1/)\), then in the worst case, we may need \( n\) convolutions to reach our optimal bound. Next, we state our results for exact classification.

**Theorem 4.2**.: _Suppose we are given a 2-block \(m\)-dimensional CSBM with parameters \(n,p>q,,,\) satisfying \((p,q)(k})\) and \(pn}{n}\). Then after \(k=O( n)\) rounds of graph convolution with \(\), our data is linearly separable with probability \(1-n^{-(1)}\) if_

\[((},\ }^{k}))\]

_where \(C\) is an absolute constant._

Here, we bound the minimum signal-to-noise ratio required for exact classification as a function of \(p,q,n\) and \(k\). Just like the partial classification result, our function has a term that decreases exponentially with \(k\) and a term independent of \(k\). The rate of decrease for the dependent term is proportional to \(1/()\), or \(\). We see once again that with more convolutions, the requirement on the feature signal-to-noise ratio for exact classification becomes exponentially weaker. Moreover, since we assumed that \((k)\), as long as \(\|-\|()\), the data becomes linearly separable after \(k=O( n/ n)\) convolutions. Just as in the partial classification case, we observe that the larger \(\) is, the fewer convolutions we need to obtain the optimal bound. In particular, if \(=(1)\) and \(p=(1)\) then one convolution already gives the optimal bound, and if \(p=1/\), then two convolutions are enough. For technical reasons, we only analyze exact classification using convolution with the corrected un-normalized adjacency matrix, \(\). Similar bounds should hold for \(\) based on our simulation results, but we leave this for future work.

### Discussion on our Assumptions

Both Theorem 4.1 and Theorem 4.2 require a lower bound of \((1/)\), and this is to ensure that the signal from the graph is strong enough so that a convolution does not destroy the signal from the data. Also, implicit in the probability bound of Theorem 4.1 and in Theorem 4.2, is the assumption that our signal-to-noise ratio, \(\|-\|/\), is at least \((1/)\) so the feature noise does not completely down out the signal. Our lower-bound assumption on \(p\) is to ensure concentration in the behavior of the degrees and the adjacency matrix towards their expectation. In Theorem 4.2, we also assume that \(k=O( n)\). This is done mainly for technical reasons of our proof but we note that this assumption is made without loss of generality because as mentioned, the bound in Theorem 4.2 does not improve for \(k n\). Finally, we note that the case \(p>q\) corresponds to a homophilous graph, and the case \(p<q\) corresponds to a heterophilous graph (see Luan et al. (2021); Ma et al. (2022) for more). For binary classification, it has been shown (Baranwal et al., 2023) that one can assume \(p>q\) without loss of generality and make corresponding adjustments in the classifier. As such, we assume that \(p>q\). For more detail regarding this assumption, see Appendix B.1.

## 5 One-Dimensional CSBM

In Baranwal et al. (2021), the authors showed that analyzing the linear classifier for the \(m\)-dimensional CSBM reduces to analyzing the \(1\)-dimensional model. We say that a CSBM is **one-dimensional and centered** with parameters \(n,p,q,\) if it has one-dimensional features and means \(1/\) and \(-1/\). That is, we have one feature vector \(x^{n}\) given by \(x=s+g\), where \(g(0,^{2}I_{n})\) and \(s(i)=1/\) for \(i S\) and \(-1/\) for \(i T\). We will refer to \(s\) as our _signal_ vector and for ease of notation, we normalize it so that it always has unit norm. The following lemma allows us to reduce the analysis of the linear classifier for a general CSBM to the analysis of the 1-dimensional centered CSBM (proof in Appendix B.1). Thus, in the proofs of our main theorems, we will analyze the 1-dimensional case before applying Lemma 5.1.

**Lemma 5.1**.: _Given an \(m\)-dimensional 2-block \(CSBM\), there exist \(w^{m}\) and \(b^{n}\) such that \(Xw+b=s+g\) where for each vertex \(i\), \(g_{i}\) is i.i.d. \((0,^{ 2})\) with \(^{}=\|-\|}\)._

In the 1-dimensional model, it is clear how our signal \(s\) is present in our features. Our convolution matrix also captures the signal because it can be viewed as a perturbation of the matrix \(ss^{}\). This is especially evident with the un-normalized convolution matrix \(\), which satisfies the following

\[=}+R+d^{}^{}\] (2)

where \(:=(p-q)n/(2d)\) is the _signal strength_, \(d^{}:=((p+q)n/2-d)/(nd)\) is the _average_ degree deviation, and \(R\) is the "edge-deviation" matrix, where \(R_{i,j}=A_{i,j}-[A_{i,j}]\). Since \(R\) has i.i.d. zero-mean entries with variance at most \(p\), we can use standard matrix concentration inequalities to show it is not too big. Likewise, \(d^{}\) is small due to degree concentration, and together, these two concentration results imply that \(\) is close to \( ss^{}\). In fact, if we show degree concentration for all vertices, then we can show that \(\) also behaves like \(\). We state these concentration results below.

**Proposition 5.2**.: _Assume that \(p=(n}{n})\), and let \(=\). With probability \(1-n^{-(1)}\), we have the following concentration results_

1. \(|d^{}| O(1/n^{1.5})\)_, which implies that_ \((1 o(1))\)_._
2. \(\|R\| O()\)__
3. \(\|- ss^{}\| O(})\) _and_ \(\|- ss^{}\| O(})\)__

These concentration properties are crucial for spectral analysis. Details are given in Appendix B.1.

## 6 Partial Classification

In this section, we give a sketch of the proof of our partial classification result, Theorem 4.1. The full proofs can be found in Appendix C. We will show that partial classification can be achieved if the convolved vector is well-correlated with our signal vector \(s\) as defined in the beginning of Section 5 in the centered-1 dimensional CSBM. We will analyze our result for convolutions using the matrix \(M\{,\}\).

**Proposition 6.1**.: _Given a centered 1-dimensional 2 block CSBM with parameters \(n,p>q,\ \) and \((p,q)=\), suppose our convolution matrix \(M\) satisfies \(\|M- ss^{}\|\) and \( C\) for a large enough constant \(C\). Let \(x_{k}=M^{k}x\), be the result of applying \(k\) rounds of graph convolution to our input feature \(x\). Then with probability at least \(1-(-})\), there exists a scalar \(C_{k}\) and an absolute constant \(C^{}\) such that_

\[\|C_{k}x_{k}-s\|^{2} O(}{^{2}}+( }{})^{2k}n^{2} n)\]

The main idea of our analysis is to use the fact that the top eigenvector of \(M\), denoted \(\), is well correlated with our signal \(s\). Since \(\|M- ss^{}\|\) and \(>C\) by assumption, standard Matrix perturbation arguments imply that the spectrum of \(M\) will be in \((,,,...)\) with high probability. Given our assumption of \(>C\), there will be a large gap between the top eigenvalue of \(M\) and the rest of its eigenvalues. A well-known result Davis and Kahan implies that \(\|s-\|^{2} O(^{2}/^{2})\), i.e. \(s\) is close to \(\). Thus, we prove Proposition 6.1 by showing that the influence of the rest of the eigenvectors on our convolved vector, \(x_{k}=M^{k}x\), decreases exponentially with \(k\), which allows us to bound the squared norm distance between \(x_{k}\) and \(s\). In particular, we take our normalization constant to be \(_{k} 1/_{1}^{k}\), where \(_{1}\) is the maximum eigenvalue of \(M\). Note that \(_{k}(1/_{1}^{k})M^{k}=^{}\). Roughly speaking, we decompose our convolution vector as \(C_{k}x_{k}(1/_{1}^{k})M^{k}s+(1/_{1}^{k})M^{k}g\). To bound the distance of this vector from \(s\), we analyze the contribution of each of the two terms to our error separately. That is, we show that with high probability \(\|(1/_{1}^{k})M^{k}s-s\|^{2} O(^{2}/^{2})\) and \(\|M^{k}g\|^{2} O((/)^{2k}n^{2} n)\). Note that the first error term is from taking the convolution of the noisy graph with the true signal, and thus does not decrease with \(k\). The second error term, on the other hand, comes from variance in the features, \(g\), and thus decreases with our noise level \(\) and drops exponentially with each convolution.

Finally, given Proposition 6.1, we can prove the partial classification result by noting that if we partition the convolved 1-dimensional data around \(0\), then each misclassified vertex contributes \(1/n\) to the mean-squared error, which means the number of misclassified vertices is at most \(\|C_{k}x_{k}-s\|^{2}n\). This, combined with Lemma 5.1 to generalize to the \(m-\)dimensional case will prove Theorem 4.1

## 7 Exact Classification

In this section, we sketch the proof of Theorem 4.2 for exact classification using the un-normalized corrected convolution matrix \(\). Full proofs can be found in Appendix D. To show linear separability, we would like \(x_{k}=^{k}x\) to have positive entries for all vertices in \(S\) and negative entries for all vertices in \(T\). This means that we want to show \(\|C_{k}x_{k}-s\|_{}<1/\) for some appropriate scalar \(C_{k}\). In particular, we will take \(C_{k}\) to be \(1/^{k}\), where \(\) is our empirical estimate of \((p,q)\). In partial classification, it sufficed to bound the mean squared error \(\|C_{k}x_{k}-s\|_{2}^{2}\), using spectral analysis but bounding \(\|C_{k}x_{k}-s\|_{}\) requires more work because now we are bounding the _entrywise_ instead of average error. In our approach, we bound the volume of messages passed through "incorrect paths" in our graph and show that the contribution from these messages is small. Then, we show the other source of error, the feature variance, is reduced exponentially with each round of convolution. As with the partial classification result, we first prove our result in the 1-dimensional centered model:

**Proposition 7.1**.: _Suppose we are given a 1-dimensional centered 2-block CSBM with parameters \(n,p,q,\) and \(k=O( n)\) such that \(=(k})\), \(pn}{n}\), and \( O(})\). Then with probability at least \(1-n^{-(1)}\):_

\[\|}^{k}x-s\|_{}}+O((})^{k})\]

Given, Proposition 7.1, Theorem 4.2 follows immediately by applying Lemma 5.1. We now give a sketch of our proof for Proposition 7.1. To bound each entry of \(C_{k}x_{k}-s\), we must bound \(|e_{u}^{}^{k}x/^{k}-e_{u}^{}s|\) for all \(u V\). Similar to in partial classification, we will split our error into error from \(^{k}s\) and error from \(^{k}g\). That is, for each \(u V\), we separately upper bound \(|e_{u}^{}( ss^{}+R^{})^{k}s-e_{u}^{}s|\) and \(|e_{u}^{}( ss^{}+R^{})^{k}g|\), where \(R^{}=- ss^{}\). The matrix \(( ss^{}+R^{})^{k}\), when expanded out, can be written as \(^{k}ss^{}\) plus a sum of \(2^{k}-1\) terms, each of which is a non-commutative product of matrices of the form \( ss^{}\) or \(R^{}\). We group these error matrices into termsof order \(\) for \([k]\), where the \(^{th}\) order terms are comprised of products that contain \(\) copies of \(R^{}\) and \(k-\) copies of \( ss^{}\).

In our analysis, we first use degree concentration to show that instead of analyzing the error w.r.t. \(R^{}\), it suffices to analyze the error w.r.t. \(R\). To bound \(e_{u}^{}( ss^{}+R)^{k}s\), we expand it out and find that each term arising from an error matrix of order \(\) can be written as a multiple of \(e_{u}^{}R^{a_{1}}s s^{}R^{a_{2}}s...s^{}R^{a_{L}}s\) where \(a_{1},...a_{L}\) are non-negative integers satisfying \(a_{1}+a_{2}+...a_{L}=\). The symmetric terms can be bounded by showing \(s^{}R^{a}s()^{a}\) using simple spectral arguments. To control the asymmetric term, we show that with high probability, \(|e_{u}^{}R^{a_{1}}s|}(Cnp n)^{a_{1}/2}\) for a constant \(C\). This part is the most technical part and requires the slightly stronger graph density assumption: \(p(^{3}n/n)\). Thus, we have \(|e_{u}^{}R^{a_{1}}s s^{}R^{a_{2}}s...s^{}R^{a_{L}}s| }(Cn n)^{/2}\) for some constant \(C\). The analysis for bounding \(e_{u}^{k}g\) is similar. Finally, by combining these bounds with our assumptions that \(\) is large enough, we obtain Proposition 7.1.

Now, we take a closer look at the step of bounding the asymmetric term. Recall that \(R\) is a random symmetric matrix with i.i.d. zero mean entries. The term \(e_{u}^{}R^{}s\) can be expressed as \(_{w}R_{w(0),w(1)}R_{w(1),w(2)},...R_{w(-1),w(a)}s(w(a))\) where the sum is over all walks, \(w\), of length \(a\) in the complete graph over \(n\) vertices starting at \(w(0):=u\). From a message-passing perspective, one can interpret this as bounding the deviation between the amount of signal message \(u\) receives over paths of a certain length and the amount of message it _epects_ to receive. To bound this term, we use a path counting argument to control its higher moments, and then apply Markov's inequality: \([|e_{u}^{}R^{a}s|]<}[|e_{u}^{}R^{a}s|^{2}]\).

## 8 Multi-class Analysis on Gaussian Mixture Model

In this section, we will formally state and sketch our results for the multi-class analysis. Full proofs are in Appendix E. For simplicity, we will only analyze convolution with the un-normalized corrected convolution matrix, \(\). The reason that the corrected convolution still gives good performance is that when class sizes are balanced, the second eigenspace of the _expected_ adjacency matrix has multiplicity \(L-1\) and exactly captures the \(L\) clusters (see Lemma E.1). Before formally stating our result, we will introduce some useful notation:

* **Graph Signal:**\(:=\) is the strength of the signal from the graph.
* **Graph Noise:**\(:=C((+))\) for some constant \(C\). \(\) is an upper bound on the graph noise.
* Let \(U:=[X]\) be the matrix whose \(i^{th}\) column is \(_{i}\). We also assume our features are centered on expectation so that \(U^{}=0\). This is not restrictive since it can be satisfied by applying a linear shift to the features
* Let \(=_{i,j[n]}\|_{i}-_{j}\|\) be the minimum distance between the centers

**Theorem 8.1**.: _Given the CSBM with parameters, \(p,q,L,n,m\), suppose \((p,q)(n}{n})\) and \(||>4k\). Let \(X^{(k)}=}^{k}X\) be the feature matrix after \(k\) rounds of convolutions with scaling factor \(1/^{k}\). Let \(x_{i}^{(k)}\) be the \(i^{th}\) row of the matrix \(X^{(k)}\). Then with probability \(1-n^{-(1)}\), at least \(n-n_{e}\) nodes, \(i\), satisfy \(\|x_{i}^{(k)}-_{i}\|</2\) where_

\[n_{e}=O(k/||)^{2}^{2}}{^{ 2}}+(L+n(/||)^{2k})m n}{^{2}}.\]

_In particular, the quadratic classifer \(x(\|x-c_{l}\|^{2})_{l=1}^{L}\) will correctly classify at least \(n-n_{e}\) points, and when \(n_{e}=o(n)\), then we can correctly classify \(1-o(1)\) fraction of points._

Intuitively, our theorem states that each convolution will cause the points in each cluster to "contract" towards their means up until a certain saturation point is reached. If after this contraction, many points are closer to their own centers than to any other centers, the softmax classifier will correctly classify them. Just as in Theorem 4.1, our error bound consists of one component depending on the variance, \(^{2}\), that is large at the beginning and decreases exponentially with each convolution. The classification accuracy at the point of saturation (\(k n\)) depends on the squared product between graph's signal-to-noise ratio, \(/||\), and the "separation ratio" of the datasets: \(\|U\|_{F}/\). As in the two-class setting, our theorem captures both the homophilic and heterophilic settings. However, for large \(L\), our error parameter, \(\), can be much larger if \(q>p\). This observation of noisier graphs in the heterophilic setting, leading to less accurate performance, is consistent with observations from previous studies (Choi et al., 2023).

## 9 Experiments

In this section, we demonstrate our results empirically. For synthetic data, we show Theorems 4.1 and 4.2 for linear binary classification. For real data, we show that removing the principal component of the adjacency matrix exhibits positive effects on multi-class node classification problems as well.

### Synthetic Data

For synthetic data from the CSBM, we demonstrate the benefits of removing the principal component of the adjacency matrix before performing convolutions for both variants of convolution described in Equation (1). We choose \(n=2000\) nodes with \(20\) features for each node, sampled from a Gaussian mixture. The intra-edge probability is fixed to \(p=O(^{3}n/n)\). We perform linear classification to demonstrate the results in Theorem 4.1 and Theorem 4.2, training a one-layer GCN network both with and without the corrected convolutions and perform an empirical comparison.

We provide plots for two different settings: (1) Fix \(=|p-q|/(p+q)=2/3\) and vary signal-to-noise ratio of the node features, \(\|-\|/\), for different number of convolutions. We observe in Figure 1 that as the number of convolutions increases, the original GCN (Kipf and Welling, 2017) (in blue) starts performing poorly, while the corrected versions (in orange and green) retain the accuracy for lower signal-to-noise ratio; (2) Fix \(\|-\|/=1\) and vary the graph relative signal strength, \(\), for different number of convolutions. We observe the same trends in this setting, as depicted in Figure 2. The vertical lines represent the threshold for exact classification from Theorem 4.2.

### Real Data

Similar to synthetic data, we compare the results for corrected graph convolution to the original GCN on the following real graph benchmarks datasets: _CORA_, _CiteSeer_, and _Pubmed_ citation networks (Sen et al., 2008) in the multi-class setting. In Figure 3, we see that overall the accuracy of every learning method decreases as the number of convolutions increases but the corrected convolutions converge to an accuracy much higher than that of the uncorrected convolution. This is attributed to

Figure 1: Accuracy plot (average over 50 trials) against the signal-to-noise ratio of the features (ratio of the distance between the means to the standard deviation) for increasing number of convolutions. Here, \(v=D^{1/2}\) and the “GCN with \(vv^{}\) removed” refers to convolution with the corrected, normalized adjacency matrix. “GCN with \(11^{}\) removed” is the corrected, unnormalized matrix.

the fact that for multi-class classification on general graphs, the important information about class memberships is typically captured by the top \(C\) eigenvectors (except the first one) where \(C\) is greater than the number of classes [Lee et al., 2014]. In general, these eigenvectors could have different eigenvalues. Since the limiting behavior of many rounds of convolutions is akin to projecting the features onto the eigenvector(s) corresponding to the second eigenvalue, we only expect this to capture partial information about the multi-class structure. By contrast, we show, in Appendix E.1, that for synthetic data with balanced classes, the classification accuracy only increases with more convolutions if they are corrected to remove the top eigenvector.

## 10 Conclusion and Future Work

In this study, we utilized spectral methods to obtain partial and exact classification results for the linear classifier with corrected convolution matrices in the 2-block CSBM. Our spectral approach highlights, theoretically, how removing the top eigenvector can mitigate oversmoothing and improve classification accuracy. We prove that the removal of the top eigenvector results in reducing feature variance and correcting the asymptotic behavior of many rounds of convolution towards the second, rather than the top eigenvector of the adjacency matrix. Finally, we showed that our analysis can be generalized to the multi-class setting. We hope our analysis can lead to further developments in theoretical and practical studies of GNNs. A natural extension of this work would be to generalize our analysis to broader classes of multi-class models. For example, if the size of classes are unbalanced, the second eigenspace may not capture all the information about the clusters. In addition, the distribution of features may not follow a standard Gaussian mixture model, but more complicated distributions, possibly with multiple centers [Baranwal et al., 2023a]. Another natural setting to consider is when clusters in the feature distribution do not exactly match clusters in the graph. Extending our analysis to these settings will likely require more sophisticated network architectures and activation functions.

Figure 3: Accuracy plots (average over 50 trials) against the number of layers for real datasets.

Figure 2: Accuracy plot (average over 50 trials) against graph relative signal strength (\(=|p-q|/(p+q)\)) for various values of the number of convolutions.