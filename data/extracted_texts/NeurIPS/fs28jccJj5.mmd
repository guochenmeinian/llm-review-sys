# SpikedAttention: Training-Free and Fully Spike-Driven Transformer-to-SNN Conversion with Winner-Oriented Spike Shift for Softmax Operation

SpikedAttention: Training-Free and Fully Spike-Driven Transformer-to-SNN Conversion with Winner-Oriented Spike Shift for Softmax Operation

 Sangwoo Hwang

Electrical Engineering and Computer Science

Daegu Gyeongbuk Institute of Science and Technology (DGIST)

nemesis0523@dgist.ac.kr

Seunghyun Lee

School of Electrical Engineering

Korea University

coder@korea.ac.kr

&Dahoon Park

School of Electrical Engineering

Korea University

manyteacher93@korea.ac.kr

&Donghun Lee

School of Electrical Engineering

Korea University

dhleeids@korea.ac.kr

&Jaeha Kung

School of Electrical Engineering

Korea University

jhkung@korea.ac.kr

J. Kung is the corresponding author.

###### Abstract

Event-driven spiking neural networks (SNNs) are promising neural networks that reduce the energy consumption of continuously growing AI models. Recently, keeping pace with the development of transformers, transformer-based SNNs were presented. Due to the incompatibility of self-attention with spikes, however, existing transformer-based SNNs limit themselves by either restructuring self-attention architecture or conforming to non-spike computations. In this work, we propose a novel transformer-to-SNN conversion method that outputs an end-to-end spike-based transformer, named SpikedAttention. Our method directly converts the well-trained transformer without modifying its attention architecture. For the vision task, the proposed method converts Swin Transformer into an SNN without post-training or conversion-aware training, achieving state-of-the-art SNN accuracy on ImageNet dataset, i.e., \(80.0\%\) with 28.7M parameters. Considering weight accumulation, neuron potential update, and on-chip data movement, SpikedAttention reduces energy consumption by 42% compared to the baseline ANN, i.e., Swin-T. Furthermore, for the first time, we demonstrate that SpikedAttention successfully converts a BERT model to an SNN with only 0.3% accuracy loss on average consuming 58% less energy on GLUE benchmark. Our code is available at Github ( https://github.com/sangwoohwang/SpikedAttention ).

## 1 Introduction

Recently, many practical AI applications such as conversational question answering [(1)], intelligent code completion [(2)], weather forecasting [(3)], and text-to-image generation [(4)] have rapidly evolved with the advances in artificial neural networks (ANNs) based on transformer architecture. Transformers are powered by the self-attention mechanism, which extracts long-range dependencies between input tokens, e.g., words (5), image patches (6), or both (7). However, deploying state-of-the-art transformers requires a large amount of computations and a huge memory footprint. For instance, generating a single token on a GPT3-175B model requires at least 350 GFLOPs (8).

In order to reduce the ever-growing computational overhead of transformers, there were some efforts to improve their energy efficiency by utilizing spiking neural networks (SNNs). The major strength of an SNN, which mimics a biological neuron, is that only weight accumulations (ACs) are required when input spikes are encountered (_event-driven_), instead of power-hungry multiply-accumulate (MAC) operations in ANNs. The energy consumption of a 32-bit MAC is 5\(\) higher than that of a 32-bit accumulation (9). To exploit the high energy efficiency of SNNs, many CNN models based on SNNs [10; 11] have been proposed, and recently, SNN-based transformers have emerged. Implementing an SNN-based transformer is done either by direct training via a surrogate gradient [12; 13] or converting a well-trained transformer to an SNN .

When implementing the attention module in a spike-based transformer, there are two types of matrix multiplication. One is associated with generating \(Q\), \(K\), and \(V\) where _static_ weight matrices (\(W_{Q}\), \(W_{K}\), and \(W_{V}\)) are used. In this case, when an input neuron fires, its associated weight value gets accumulated to the output neuron's potential. Another type is the multiplication between _dynamically_ generated matrices, e.g., \(Q K^{T}\). In previous works [12; 14], logical AND operations are performed between two "rate-coded" spike trains, e.g., \(S_{Q(i,k)}\) and \(S_{K(j,k)}\), for each dot product. However, due to the probabilistic nature of the rate coding, it requires a long timestep (\(T>128\)) to maintain high ANN-to-SNN conversion accuracy . Unfortunately, this translates to high energy consumption in running SNNs (refer to MST in Fig. 1).

Another challenge with realizing spike-based transformers is the softmax operation. In the field of ANN quantization, there have been many efforts to approximate softmax operations [16; 17]. I-BERT  approximates the exponential function with a second-order polynomial, successfully converting it to an integer operation (_i-exp_), and FQ-ViT  combines _i-exp_ with logarithmic quantization. In SNN-based transformers, however, softmax has not been converted to a spike-based operation due to the presence of exponential functions. Since SNNs have binary inputs and outputs, previous approaches in ANN quantization cannot be directly applied.

**Our Contribution:** In this work, we present an end-to-end spike-based transformer, named _SpikedAttention_, solving the abovementioned challenges to achieve state-of-the-art accuracy (0.6% accuracy loss compared to ANN) among spike-based transformers (Fig. 1(a)). In addition, it consumes the minimum energy, which is 42% lower than the original Swin Transformer , for image classification task. Furthermore, for the first time, we demonstrate that a BERT model can be successfully converted

Figure 1: (a) Accuracy, energy consumption (refer to Appendix A.1), and parameter size of the proposed SpikedAttention and other spike-based SNNs on ImageNet classification . (b) The structure of the proposed fully spike-based attention module (more details are in Section 4).

to an SNN, i.e., SpikedAttention, with negligible accuracy loss (0.3% on average) while consuming 58% less energy on GLUE benchmark. Note that the SpikedAttention does not require any additional training since it is created by the direct transformer-to-SNN conversion method thanks to the proposed fully spike-based attention module (Fig. 1(b)). The main contributions are summarized below:

* **Fully spike-based transformer**: SpikedAttention encodes external inputs and all intermediate features in spikes. Especially, each element in intermediate features is encoded as a single spike for a given timestep \(T\) to minimize the energy consumption.
* **Trace-driven matrix multiplication**: To reduce the required timestep \(T\) in performing multiplication between two dynamically generated matrices, we propose a trace-driven matrix multiplication. Owing to the reduced timestep, we can reduce energy consumption.
* **Exponent-free spike-based softmax**: We present a winner-oriented spike shift that controls the output spike timing to realize the spike-based softmax. This allows us to keep the vanilla self-attention architecture of the original transformer to maintain high accuracy.

## 2 Related Work

Most SNN studies have focused on replacing CNNs to improve energy efficiency in vision tasks [(10; 18)]. With the recent advent of transformers, there have been increasing efforts to convert transformers into SNNs [(14)] or directly train spike-based transformers [(12; 13; 19; 20)]. When realizing a spike-based transformer, self-attention is the major hurdle in converting the transformer to a _fully spike-driven_ network. Here, "fully spike-driven" means that no real-valued multiplications or nonlinear functions are involved in any operations. Fig. 2(a) illustrates the required operations in vanilla self-attention (VSA) [(5)] but with \(Q\), \(K\), and \(V\) in the form of spikes [(14)].

In the original VSA module, real-valued multiplications (\(Q K^{T}\) and \(Attn V\)) and a softmax operation (\(Attn=softmax(Q K^{T}/)\)) are required. In MST [(14)], the authors encode \(Q\), \(K\), and \(V\) as spike trains (denoted by \(S_{Q}\), \(S_{K}\), and \(S_{V}\) in Fig. 2(a)). By doing so, the matrix multiplication (\(Q K^{T}\)) simply becomes AND operations followed by floating-point accumulations. To convey meaningful information (in the form of spikes) to the next layer, however, a long timestep \(T 128\) is required in [(14)]. This is due to the fact that the simultaneous firing of spikes at both \(S_{Q}\) and \(S_{K^{T}}\) is rare. This problem gets worse for temporal coding or phase coding, which produces much fewer spikes than the rate coding. Moreover, in MST, the complex softmax operations are done to compute attention maps, which requires non-spike computations (i.e., expensive exponential functions).

To remove the burden of computing the softmax, some previous studies on direct training of SNNs for transformers have restructured the attention module [(12; 13)]. As shown in Fig. 2(b), Spikformer [(12)] removes the softmax operation since '\(S_{Q} S_{K^{T}}\)' or '\(S_{K^{T}} S_{V}\)' only produces non-negative values. There was another attempt to modify the attention module, such as replacing '\(S_{Q} S_{K^{T}}\)' with a Hadamard product followed by the column summation to obtain a mask vector that masks out less important channels in \(S_{V}\)[(13)]. Since these prior works restructure the network topology, they need to train the model from scratch with surrogate gradients to improve the performance.

Figure 2: The computation of attention in previous SNN-based transformers (a) with the softmax [(14)], and (b) without the softmax operation [(12)]. Both involve non-spike computations. Total timestep \(T\) of each spike tensor is set to 1 for simplicity.

Preliminaries

### Neuron Model

There has been a long-standing effort in neuroscience to mathematically model biological neurons [(21; 22)]. Neuron models with high biological plausibility are computationally expensive, while models with low biological similarity are energy efficient. Due to a high volume of parameters and significant computations involved in ANNs, energy efficiency has become more of a priority than biological plausibility when replacing them with SNN counterparts. Therefore, the most energy efficient neuron model, i.e., the leaky integrated-and-fire (LIF) model [(23)], is widely used. The LIF neuron model is defined as

\[ v_{j}(t)&= v_{j}(t-1)+_{i= 1}^{N_{pre}}w_{ij}S_{i}(t),\\ S_{j}(t)=1,& v_{j}(t)&=v_{j}(t)-V_{ }\ \ \ \ \ v_{j}(t) V_{},\] (1)

where \(i\) or \(j\) is the index of a pre- or post-synaptic neuron, \(v_{j}\) is the potential of the neuron \(j\), and \(\) is the leak factor of potential that causes the \(v_{j}\) to gradually decay as the time \(t\) proceeds. The \(w_{ij}\) is the synaptic weight between the neuron \(i\) and \(j\) converted from a pre-trained ANN or directly trained. The \(S_{i}(t)\) represents the binary spike (\(0\) or \(1\)) from the neuron \(i\) at time \(t\). The \(N_{pre}\) is the number of pre-synaptic neurons. The pre-synaptic neurons that generate spikes activate the weight accumulation on \(v_{j}(t)\). When the \(v_{j}(t)\) exceeds the threshold \(V_{}\), the neuron \(j\) fires a spike (\(S_{j}(t)=1\)) and its potential resets to \(v_{j}(t)-V_{}\). Note that decreasing the potential by a threshold instead of resetting it to zero is a common technique in ANN-to-SNN conversion methods [(10; 24)].

### Spike Coding Schemes

To achieve higher SNN performance, it is essential to determine how real-valued inputs are encoded into a sequence of spikes, known as'spike coding'. The spike coding is a scheme that determines whether to generate a spike at time \(t\) when converting continuous values into a set of spikes. The most widely used coding scheme is rate coding, which generates multiple spikes at each neuron via Poisson sampling over pre-defined timestep \(T\)[(10; 25)]. The rate coding generates more spikes as it converts larger values into spikes. However, increasing the number of spikes and the timestep for higher resolution leads to higher energy consumption due to more weight accumulations in Eq. (1).

Unlike the rate coding, which generates multiple spikes at each neuron, temporal coding generates a single spike per neuron. In the temporal coding, each spike represents a different value at a different spike time \(t\) for a given total timestep \(T\). The spike at an earlier time represents a larger value. For instance, an earlier spike can represent a linearly larger value as \(\)[(11; 26)] or an exponentially larger value as \(e^{}\)[(27)]. The temporal coding is energy efficient but requires a long timestep \(T\) to increase the precision/resolution.

The alternative coding scheme, known as phase coding [(28; 29)], combines the characteristics of the temporal coding and the rate coding. Like the temporal coding, the phase coding assigns each timestep \(t\) a different value, i.e., \(B^{-t}\), where \(B\) is the base of the phase. As proven in Appendix B, the phase coding can be realized by replacing the leak factor (\(\)) of the widely used LIF neuron model in Eq. (1) with \(B\), making the spike at one timestep earlier has the \(B\) larger impact on the potential increase. When \(B=2\), we call it a binary coding. Unlike the temporal coding, the phase coding allows multiple spikes when encoding a value. For a fixed (total) timestep \(T\), the phase coding provides a higher resolution than other coding schemes. Recently proposed one-spike phase coding maximizes the energy efficiency, where the sequence of spikes is approximated by the first spike [(30)]. Note that the neuron ceases to update its potential after the first spike being generated because it is unnecessary to fire the following spikes at the cost of approximation error. In the one-spike phase coding, \(V_{}\) is tuned to the midpoint between two consecutive phase-based weights, i.e., \({}^{*}(B^{-t}+B^{-t-1})/2^{*}\), to reduce the approximation error. The authors in [(30)] have proposed a technique to reduce the approximation error by decreasing the base \(B\) of phase coding. However, decreasing the base is only guaranteed to reduce the error when \(T\) is large enough. In SpikedAttention, therefore, we use the one-spike phase coding by carefully selecting the proper base \(B\) and total timestep \(T\) to balance well between the accuracy and the energy efficiency. The details on selecting \(B\) and \(T\) are presented in Appendix E.

## 4 Fully Spike-based Attention: SpikedAttention

The main goal of designing the SpikedAttention was to encode external inputs and all intermediate features by binary spikes _without altering the VSA structure_ at all. Prior work on spike-based vision transformers [(12; 13; 14)] feed in floating-point inputs, i.e., 2D images, to the patch partition module instead of binary spikes. Also, the nonlinear nature of softmax and the high sparsity in \(S_{Q}\), \(S_{K}\), and \(S_{V}\) make things more challenging in making fully spike-based transformers. In the proposed SpikedAttention, all intermediate neurons fire a single spike using the one-spike phase coding (\(B<2\)). Only the external input allows multiple spikes with the binary coding, i.e., phase coding with \(B=2\). Until now, previous transformer-to-SNN conversion methods [(14)] could not achieve fully spike-based computations due to softmax operations, and they require a long timestep due to extremely high sparsity after AND operations between two spike-based vectors. The direct SNN training methods [(12; 13)] simplified the VSA structure by removing softmax, which degrades the performance. Therefore, we present two novel techniques to (i) _minimize the spike timestep_ by performing trace-driven matrix multiplication and (ii) exponent-free spike-based softmax to realize the transformer-to-SNN conversion method _without any training_.

### Trace-driven Matrix Multiplication

We focus on developing an efficient spike-based computing scheme for the multiplication between two dynamically generated spike-based matrices. We utilize a global trace that tracks the phase at time \(t\) to consider the values associated with every spike. With the rate coding, probabilistic multiplications are performed with logical AND between two spike trains. However, since we use one-spike phase coding, each spike must carry its information to the next layer. Consider an example of performing \(S_{Q} S_{K^{T}}\). The output value at (\(i\),\(j\)), denoted as \((QK^{T})_{i,j}\), is computed by the dot product which is \(_{k}S_{Q(i,k)}S_{K(j,k)}\). Each spike train in \(S_{Q}\) (or \(S_{K}\)) consists of only one spike which represents the value \(B^{-t_{Q(i,k)}}\), where \(t_{Q(i,k)}\) is the spike time of \(S_{Q(i,k)}\) and \(B\) is the base. Thus, the dot product output should become \(_{k}B^{-t_{Q(i,k)}} B^{-t_{K(j,k)}}\).

As mentioned earlier, our work exploits the'spike trace model' to record the value associated with the spike that fires first among the neuron pair, e.g., \(S_{Q(i,k)}\) and \(S_{K(j,k)}\). In neuroscience research, spike trace is a popular method to determine the correlation between connected neurons by relative spike timing [(31)]. Typically, the trace of a neuron '\(n\)', i.e., \(x_{n}(t)\), reflects the history of spikes during timestep \(T\). We simplify the trace model to globally track the value \(B^{-t}\) at spike time \(t\), which is

\[x^{g}(t)=x^{g}(t-1)/B,\] (2)

Figure 3: The proposed trace-driven dot product in SpikedAttention. A global trace decays by its base (\(B\)) at each timestep and the trace is transferred to each neuronâ€™s local memory when its first spike, from either \(S_{Q}\) or \(S_{K}\), is observed. The \(v_{(QK^{T})(1,1)}\) is the potential of the neuron \((QK^{T})_{1,1}\).

where \(x^{g}(t)\) is the global trace which is initialized to \(1\) at \(t=0\). With the one-spike neuron model, when a neuron encounters the first spike at \(t\), we store \(B^{-t}\) in its local memory as its own trace \(x_{n}(t)\), which will be used later to compute the dot product. We intentionally leveraged the trace-driven method since neuromorphic chips such as Loihi (32) already have the hardware module, e.g., trace memory, for updating neuron traces for widely used STDP learning.

Fig. 3 illustrates the process of a dot product for computing \((QK^{T})_{1,1}\) with spike traces. If \(S_{Q(1,1)}\) generates a spike earlier (\(t=3\)) than \(S_{K(1,1)}\), the trace value \(B^{-3}\) is stored in the local memory of the neuron \(S_{Q(1,1)}\). When the neuron \(S_{K(1,1)}\) exceeds the threshold and generates the spike at \(t=4\), the trace of \(S_{Q(1,1)}\) (\(=B^{-3}\)) propagates to update the potential of \((QK^{T})_{1,1}\). Since the current phase is \(B^{-4}\) at \(t=4\), propagation of \(B^{-3}\) at \(t=4\) (colored in red) is equivalent to '\(B^{-4} B^{-3}\)'. There might be a situation in which multiple neuron pairs observe the second spike simultaneously. For instance, \(S_{Q(1,2)}\)-\(S_{K(1,2)}\) and \(S_{Q(1,3)}\)-\(S_{K(1,3)}\) pairs have the second spike at \(t=3\) (it may coincide with the first spike like the \(S_{Q(1,3)}\)-\(S_{K(1,3)}\) pair). Thus, at \(t=3\), both pairs already have their stored trace for one of two neurons (\(S_{Q}\) or \(S_{K}\)) that fired the earlier spike in the pair. Then, pre-stored traces from those pairs that share the second spike time will be accumulated together, i.e., \((B^{-2}+B^{-3})\) in this case (colored in blue). The accumulated traces are weighted by the phase of the current timestep, i.e., \(B^{-3}\). By accumulating traces first, we can bound the number of multiplications to the timestep \(T\) of the neuron model irrespective of the number of input neurons. Usually, an input spike tensor \(S_{Q}\) or \(S_{K}\) lies in \(^{N D T}\) where \(N\) is the number of image patches (or token length), \(D\) is the embedding dimension, and \(T\) is the total timestep. The accumulations happen across the dimension \(D\), making the proposed approach energy efficient when \(T D\), which is the general case for transformers.

### Winner-Oriented Spike Shift (WOSS) for Softmax

The main issue with the typical softmax operation is the presence of exponential functions. Our work uses the normalized softmax similar to (33), which is expressed as follows:

\[(z_{i})=)}{_{j=0}^{N-1}(z_{j}))}=)}{(())}}{_{j=0}^{N-1})}{ (())}},\] (3)

where \(^{N}\) and \((z_{i})\) is the softmax output with respect to the \(i^{}\) neuron. Our SNN approximates \()}{(())}\) by using the proposed WOSS neuron model and scales the threshold (\(V_{}\)) of the following layer by their sum (\(_{j=0}^{N-1})}{(())}\)), leading to the final softmax result. Note that multiplying the threshold by the sum term has the effect of dividing inputs by the same amount.

Figure 4: The proposed winner-oriented spike shift (WOSS) for approximating softmax. (a) Each WOSS neuron increases its potential (\(V_{WOSS_{i}}\)) by the incoming input spike \(S_{(i)}(t)\). The first (winner) spike among \(S_{}(t)\) activates a (global) inhibitory neuron which depresses all neurons. (b) Potential at neuron \(i\) (winner) and its corresponding output spike at \(t=T+0\). We are shifting the first spike time from \(t=1\) to \(t=0\). The following spikes are time-shifted by the same amount. (c) Potential at neuron \(j\) (non-winner) and its corresponding output spike at \(t=T+2\) (effectively, \(t=2\)) due to the global inhibition and the threshold shift.

Since SpikedAttention generates a single spike per neuron, the objective is to approximate \()}{(())}\) with \(B^{-t_{i}}\). Before converting it to a spike-based representation, we extract the maximum value of \(\), i.e., \(M_{}\), by running a proxy training dataset. Then, by setting \(z_{i}=M_{}z_{i}^{}\) and by applying \(_{B}\) on the normalized exponential term, it can be expressed as

\[_{B}(}z_{i}^{})}{((M_{ }^{}))})=^{}-(^{ }))M_{}}{_{e}B}-t_{i},\] (4)

where \(t_{i}\) is the required spike time at neuron \(i\) to approximate the left-hand side of Eq. (4). The \(^{}\) arrives at WOSS neurons by spikes \(S_{}\) from the previous layer, i.e., each row of \(S_{(QK^{T})}\), with the form of the one-spike phase coding.

First, we need to compute '\(z_{i}^{}-(^{})\)' from the input spikes, i.e., \(S_{}\). To do so, we first find \((^{})\) by detecting the first incoming spike, called a _winner spike_. In Fig. 4(a), the winner spike is observed at neuron \(i\). For the neuron \(i\), its potential increases by 1 at \(t=1\) (Fig. 4(b)). Inspired by inhibitory neurons in (25), the winner spike activates the global inhibition so that potentials of all neurons (including \(i\)) within the same softmax group decrease by 1. The global inhibition performs the subtraction by \((^{})\) in Eq. (4). After the global inhibition at \(t=1\), non-winner neurons receive an input spike at \(t>1\). Fig. 4(c) shows how the potential of non-winner neuron \(j\) changes. At \(t=3\), the neuron \(j\) receives the input spike, increasing the potential by 1. At each timestep \(t\), the potential decays by the base \(B\) (at \(t=2\) and \(t=3\) in Fig. 4(c)), which is identical to the leak factor \(\) in Eq. (1). This decay factor allows us to assign a dedicated phase \(B^{-t}\) to each timestep. Thus, after the first "potential update" stage (\(t T\)), each neuron \(i\) has the fixed potential equal to

\[V_{WOSS(i)}(t T)=B^{(T-1)}(z_{i}^{}-(^{ })).\] (5)

Now, we need to generate a single output spike at each neuron by comparing its potential to the threshold (i.e., "spike generation" stage in Fig. 4(b-c)). The threshold is initially set to 0 so that the winner-spike is time-shifted and fires at \(t=0\) for a more precise approximation of softmax, i.e., utilizing full timestep \(T\) (Fig. 4(b)). The desired time \(t_{i}\) that each neuron should fire approximately equals to \(^{})-z_{i}^{})M_{}}{_{e}B}\). To find the \(t_{i}\), we shift the threshold by following

\[(t)}{dt}=-B^{T-1}B}{M_{}},\] (6)

and generate output spike comparing the threshold with the neuron's potential. Note that the right-hand side of Eq. (6) is a constant and thus can be pre-computed. The appearance of \(B^{T-1}\) is to match the scale of the potential after the "potential update" stage. The \(-B}{M_{}}\) term is to consider the \(M_{}/_{e}B\) in Eq. (4). Then, by comparing the Eq. (5) with the shifted threshold \(V_{}(t_{i})\), we can generate a spike if the threshold becomes more negative (Fig. 4(c)) that satisfies

\[t_{i}>((^{})-z_{i}^{})}}{_{e} B}.\] (7)

The output spike from the WOSS neuron, i.e., \(S_{WOSS}\), approximates \()}{(())}\). Thus, it needs to be divided by \()}{(())}\) to complete the softmax calculation. This is done by amplifying the threshold at the following trace-driven dot products (i.e., \(S_{Attn} S_{V}\)) by the sum term, which is the summation of spikes from the WOSS neurons in the same group, i.e., each row of \(S_{WOSS}\). Similar to Eq. (1), the threshold is incremented by \(S_{WOSS}(t)\) and multiplied by \(B\) at each timestep until it reaches \()}{(())}\), which is expressed as follows:

\[V_{(i,:)}(t+1)=B V_{(i,:)}(t)+_{j_{i}}S_{WOSS (i,j)}(t),\] (8)

where \(V_{}(t=0)\) is initially set to 1. Note that all elements at each row \(i\) share the same threshold \(V_{(i,:)}\). Finally, the output neuron at the final trace-driven dot product in Fig. 1(b) generates spikes that approximate'softmax\((S_{Q} S_{K^{T}}/) S_{V}\)'. Note that implementing WOSS neurons for softmax in the neuromorphic hardware incurs 9.88% area and 12.35% power overheads compared to the hardware only with LIF neurons with no support of softmax (details are discussed in Appendix C).

## 5 Experimental Results

### Conversion of Swin Transformer to SpikedAttention

The proposed SpikedAttention is implemented by SpikingJelly (34) and PyTorch computing with four NVIDIA GeForce RTX 4090 GPUs. For evaluation, Swin Transformer (6) is selected as a baseline and converted to an SNN, i.e., SpikedAttention, to perform image classification. Conventional transformers require GeLU and layer normalization, which are non-spike computations. Thus, as a pre-trained model, we replaced GeLU with ReLU and layer normalization with batch normalization according to (35). Since some activations are not followed by the ReLU function in Swin-T, those cannot be converted to unsigned/positive spikes. To cover both positive and negative values, we utilized a signed neuron (36) that generates a positive spike (\(+1\)) if the potential is greater than \(V_{}\) and a negative spike (\(-1\)) if the potential is less than \(V_{}\). Therefore, we trained two baselines, i.e., one with ReLUs at every layer and another with no change, to convert them to SNNs. Fig. 5 illustrates the correlation between the decoded spike values and the actual activations at trace-driven matrix multiplication and softmax layers of SpikedAttention. The earlier layers (e.g., First Block) have little conversion error, but the deeper layers (e.g., Last Block) have higher conversion error. Nevertheless, the classifier exhibits a high correlation between the actual activations and the decoded spikes.

Table 1 compares performance between the prior work and the proposed SpikedAttention (denoted as '**Ours**' in the table). For the fair comparison, we estimate the energy consumption of only weight accumulations (discussed in Appendix A.1) for the entire timestep similar to the prior work (12; 13). Previous SNNs based on vision transformers demonstrated better performance than CNN-based SNNs (10; 11; 24). However, existing SOTA transformer-based SNNs consume \(2 53\) higher energy than SpikedAttention (w/o ReLU) while presenting lower accuracy (\( 80\%\)). Other SNNs feed external inputs as floating point numbers, resulting in higher energy consumption. It is noteworthy that SpikedAttention minimizes the energy consumption even with \(T=40\) by allowing only one spike per neuron. To perform a hardware-realistic energy comparison between ANNs and SNNs, we estimated the total energy consumption by using the energy metric presented in (38) which considers the data movement and the membrane potential update as well (details can be found in Appendix A.2). As a result, SpikedAttention reduces the energy consumption by 42% with only 0.6% accuracy loss when converting an ANN (w/o ReLU) which consumes 45.4mJ. Similarly, the energy consumption of SpikedAttention is 47% lower than the ANN (w/ ReLU) which consumes

   Model & Method & Param (M) & Energy (mJ)\({}^{}\) & Timestep & Acc (\%) \\  Spikformer (12) & Direct Training & 66.3 & 21.5 & 4 & 74.8 \\ SDSA (13) & Direct Training & 66.3 & 6.1 & 4 & 77.1 \\ Meta-SpikeFormer (37) & Direct Training & 55.4 & 52.4 & 4 & 80.0 \\ Meta-SpikeFormer (37) & Direct Training & 55.4 & 13.0 & 1 & 79.1 \\ MST (14) & ANN-to-SNN & 28.5 & 158.6 & 128 & 77.9 \\
**Ours (w/o ReLU)** & ANN-to-SNN & 28.7 & 3.0 & 40 & 80.0 \\
**Ours (w/ ReLU)** & ANN-to-SNN & 28.7 & 1.8 & 40 & 77.4 \\    \({}^{}\)For the fair comparison with the prior work, only the energy consumption for weight accumulations is included.

Table 1: Comparison between SpikedAttention and the prior work in terms of the parameter size, the energy consumption, the required timestep, and the accuracy on ImageNet classification task

Figure 5: Scatter plots showing the correlation between the actual activation values in Swin Transformer (x-axis) and the decoded spike values in converted SpikedAttention (y-axis). The parameters are set to \(T=40,B=1.15\).

27.5 mJ. To directly compare with the previous ANN-to-SNN conversion method, i.e., MST (14), the pre-trained ANNs from MST were converted to SpikedAttention as well (Table 2). In Table 2, we computed the energy consumption of ANNs and SNNs using the hardware-realistic energy metric (Appendix A.2). Since the pre-trained models of MST embed ReLU in each layer, they were converted to SpikedAttention using unsigned neurons. The MST consumes significantly higher energy (\( 48\)) than SpikedAttention because multiple spikes are generated for a longer timestep \(T=64\) or \(128\). In addition, the first convolution layer is based on floating point-based MACs instead of spike-based accumulations. Compared to the MST, SpikedAttention achieves higher accuracy with shorter \(T\) thanks to the trace-driven matrix multiplication. As a result, we achieve SOTA SNN accuracy on ImageNet without any training or modifying the architecture while minimizing energy consumption.

### Conversion of BERT to SpikedAttention

Our proposed SpikedAttention is also applicable to NLP since it converts attention modules into spike-based computations. However, there are some functions, such as GeLU and LayerNorm, that are difficult to compute with an SNN for now. Since MA-BERT (39) replaces GELU with ReLU, converts LayerNorm to BatchNorm, and fuses normalization layers into adjacent linear layers, MA-BERT can be easily converted to SpikedAttention without any modification. Therefore, we converted MA-BERT (base-uncased) with 110M parameters to SpikedAttention. Note that accuracy loss of MA-BERT compared to traditional BERT is only 0.1%. Even though MA-BERT approximates the softmax with a two-layer neural network, we converted MA-BERT with the original softmax to SpikedAttention (thanks to WOSS). The output of the token embedding is binary coded and fed to our SNN model. By converting the pre-trained MA-BERT for text classification on GLUE benchmark (41), we observed only 0.3% accuracy loss on average without any additional training (Table 3). Note that Matthews correlation coefficient is reported for CoLA, while accuracy is reported for the other tasks. By estimating the energy consumption as presented in Appendix A.2, SpikedAttention reduces the energy by 58% on average compared to MA-BERT. The SpikedAttention is the first work that demonstrates the transformer-to-SNN conversion for NLP tasks. SpikingBERT (40) directly trains SNN as a student using knowledge distillation from a pre-trained BERT as a teacher. Compared to

   Dataset & Model & Param (M) & Energy (mJ) & Timestep & Accuracy (\%) \\  CIFAR-10 & ANN (w/ ReLU) & 27.5 & 42.1 & 1 & 97.5 \\  & MST (Unsigned) & 27.5 & 1089.9 & 64 & 96.3 \\  & Ours (Unsigned) & 27.5 & 21.3 & 24 & 97.3 \\  CIFAR-100 & ANN (w/ ReLU) & 27.6 & 47.1 & 1 & 87.7 \\  & MST (Unsigned) & 27.6 & 1157.1 & 64 & 85.4 \\  & Ours (Unsigned) & 27.6 & 23.7 & 24 & 86.3 \\  ImageNet & ANN (w/ ReLU) & 28.3 & 55.5 & 1 & 79.3 \\  & MST (Unsigned) & 28.3 & 1836.7 & 128 & 77.9 \\  & Ours (Unsigned) & 28.3 & 31.7 & 48 & 77.2 \\   

Table 2: Conversion results of pre-trained models from MST (14) to SpikedAttention

 
**Dataset** & **CoLa** & **MNLI** & **MRPC** & **QNLI** & **QQP** & **RTE** & **SST-2** & **WNLI** & **STS-B** \\   \\ 
**Accuracy (\%)** & 59.8 & 84.7 & 84.3 & 91.4 & 91.2 & 64.6 & 92.6 & 56.3 & 84.8 \\ 
**Energy (mJ)** & 189.7 & 189.7 & 189.7 & 189.7 & 189.7 & 189.7 & 189.7 & 189.7 \\   \\ 
**Accuracy (\%)** & - & 78.1 & 79.2 & 85.2 & 86.8 & 66.1 & 88.2 & - & 82.2 \\   \\ 
**Timestep** & 24 & 24 & 16 & 24 & 24 & 16 & 16 & 16 & 24 \\ 
**Accuracy (\%)** & 59.3 & 84.4 & 84.1 & 91.0 & 90.8 & 65.0 & 92.1 & 56.3 & 83.9 \\ 
**Energy (mJ)** & 81.5 & 82.1 & 77.5 & 81.6 & 82.1 & 79.1 & 79.9 & 77.7 & 79.9 \\  

Table 3: Comparison between SpikedAttention and other BERT models on GLUE BenchmarkSpikingBERT (\(T=125\)), SpikedAttention was able to achieve 3.6% higher accuracy on average as it directly converts a well-trained BERT into an SNN without training.

## 6 Conclusion

This paper presented trace-driven matrix multiplication and winner-oriented spike shift to convert the attention module with spike-based computations. Our proposed methods accurately approximate all activations in the self-attention module as spikes without performing expensive exponent computations. Thus, our work outperforms previous transformer-based SNNs in accuracy and consumes much less energy (25.6mJ for vision tasks and 80.2mJ for language tasks on average) without structural modifications or direct training. By converting ANNs with a high number of multiplications into addition-only SNNs without any training iterations, our work makes AI more accessible on energy-constrained devices. Since the presented conversion method does not require any additional training, we can obtain SNNs without increasing the amount of carbon emissions. However, there are some limitations: first, the timestep required for SpikedAttention to maintain high accuracy is longer than directly trained SNNs. Thus, our next goal would be reducing the timestep by learning the per-layer base for better one-spike phase coding. The second limitation is that SpikedAttention do not support GeLU and LayerNorm making it difficult to be generalized to any language models. Nevertheless, our work is an essential step towards converting large language models to SNNs, and converting LayerNorm and GeLU to spike-based computations remains as our future work.