# DiMSUM : Diffusion Mamba - A Scalable and Unified Spatial-Frequency Method for Image Generation

Hao Phung\({}^{*}\)\({}^{13}\) &Quan Dao\({}^{*}\)\({}^{12}\) &Trung Dao\({}^{1}\)

Hoang Phan\({}^{4}\) &Dimitris N. Metaxas\({}^{2}\) &Anh Tran\({}^{1}\)

\({}^{1}\)VinAI Research

\({}^{2}\)Rutgers University

\({}^{3}\)Cornell University

http26@cornell.edu

quan.dao@rutgers.edu

v.trungdt21@vinai.io

hvp2011@nyu.edu

dmm@cs.rutgers.edu

v.anhtt152@vinai.io

 Equal contributions.

###### Abstract

We introduce a novel state-space architecture for diffusion models, effectively harnessing spatial and frequency information to enhance the inductive bias towards local features in input images for image generation tasks. While state-space networks, including Mamba, a revolutionary advancement in recurrent neural networks, typically scan input sequences from left to right, they face difficulties in designing effective scanning strategies, especially in the processing of image data. Our method demonstrates that integrating wavelet transformation into Mamba enhances the local structure awareness of visual inputs and better captures long-range relations of frequencies by disentangling them into wavelet subbands, representing both low- and high-frequency components. These wavelet-based outputs are then processed and seamlessly fused with the original Mamba outputs through a cross-attention fusion layer, combining both spatial and frequency information to optimize the order awareness of state-space models which is essential for the details and overall quality of image generation. Besides, we introduce a globally-shared transformer to supercharge the performance of Mamba, harnessing its exceptional power to capture global relationships. Through extensive experiments on standard benchmarks, our method demonstrates superior results compared to DiT and DIFFUSSM, achieving faster training convergence and delivering high-quality outputs. The codes and pretrained models are released at https://github.com/VinAIResearch/DiMSUM.git.

## 1 Introduction

Diffusion models  are a trending generative model technique that has gained significant attention in machine learning and computer vision. The core idea behind diffusion models is to learn how to reverse the diffusion process by gradually transforming a simple initial distribution, like Gaussian noise, into a complex data distribution. The flexibility, robust performance, and high-quality outputs make them powerful tools for advancing the state-of-the-art in generative modeling, and large diffusion-based generators have revolutionized the field of image , video , and 3D synthesis . While diffusion models initially rely on UNet architectures, recent methods have shifted gear to build upon transformer backbones. A line of works  have shown that transformer-based diffusion models are scalable and consistently offer higher generation qualitythan the UNet-based counterparts. Even the most common open-source text-to-image tool, Stable Diffusion, has switched to use transformers in their upcoming release . Hence, transformers are becoming the new backbone standard for diffusion models. The power of this structure lies in the attention mechanism for capturing richer in-context relations. However, transformers have the drawback of a costly quadratic complexity, which might hinder their feasibility for high-dimensional data.

While transformers are taking over state-of-the-art diffusion backbones, a novel technique called state-space models (SSMs) has suddenly arrived, promising a better alternative. SSMs  have revolutionized the NLP field, favoring linear time complexity and excelling at long-context modeling. This type of network bears similarities to the recurrent process of RNNs while being capable of fully operating in parallel like convolutional networks. SSM promises to surpass transformers in most tasks, prioritizing compute efficiency, such as long-sequence modeling. Mamba  is a special type of SSM that offers greater quality by introducing time conditioning and context dependency to the hidden states. In the context of computer vision, within a very short period, this architecture has been used to address a variety of problems, including image perception , image restoration , and image generation . In diffusion-based image generation, Diffusion State Space models (DIFFUSSM)  already surpass their transformer-based counterparts.

Though showing many advantages, Mamba still has a critical weakness when processing 2D imagery data. Like vision transformers, images are divided into patches and then mapped into tokens. Mamba processes these tokens following a specific scanning order, thus introducing an inductive bias about 2D images into the model. Specifically, this order greatly impacts the interplay between image tokens, thereby affecting model performance. This characteristic is unfavorable, particularly when transformers have no such order-dependency issue. Many vision-based Mamba studies have focused on solving this problem on proposed advanced scanning mechanisms like bi-directional , cross-scanning , or 8-directions zigzag . Despite improving performance, these scanning techniques still fail to capture global and long-range relations and do not fully release Mamba's potential.

In this paper, we enhance Mamba-based diffusion models, specifically focusing on image generation. Previous models failed to address the scanning order issue due to their exclusive reliance on spatial processing, overlooking crucial long-range relations that manifest in the frequency spectrum. We suggest a novel approach integrating frequency scanning with the conventional spatial scanning mechanism. Although initial work in Mamba has explored this combination for a limited task of image deraining , it lacked a comprehensive analysis of the effective integration of these features.

Motivated by the above observation, this paper introduces DiMSUM, a novel architecture that harnesses Mamba's power to unlock diffusion models' generation capabilities. Our approach enhances sensitivity to local structures and long-range dependencies by integrating wavelet transforms and spatial information. Using a query-swapped cross-attention technique, we dynamically synergize spatial and frequency information, accelerating convergence and improving image synthesis quality. Consequently, this boosts image quality and enhances the efficiency and scalability of the training.

Additionally, we incorporate globally shared transformer blocks to address global context integration, a limitation of traditional Mamba models. The block can also be viewed as a token-mixing layer that enriches global relations among image tokens, addressing the weak inductive bias of the manually defined scanning orders in the original Mamba. Hence, DiMSUM can maintain high performance even with larger, more complex datasets. Extensive experiments show that DiMSUM achieves state-of-the-art FID scores and recall, setting a new benchmark in generative image modeling.

In summary, our contributions lie three-fold: (1) A novel Mamba architecture for diffusion models that leverages both spatial and frequency features to enhance the awareness of local structures within input images, leading to better image generation. (2) We interleave globally shared transformer blocks per a certain number of Mamba blocks. The transformer with a strong capacity for capturing global relationships significantly boosts generation results when integrating with Mamba. The transformer can also be seen as an order-invariant mixing layer that complements Mamba's loose assumptions about the order of 2D data. (3) Superior results across image generation benchmarks like ImageNet, CelebA-HQ, and LSUN Church. Additionally, our method maintains comparable GFLOPs and parameters with existing diffusion architectures while offering faster training convergence.

Related Work

### State Space Models and Their Applications in Vision Tasks

In control engineering and system identification, state space models (SSMs) are described by state variables and first-order differential equations but initially underperformed in deep learning. Recent enhancements, notably by S4  through the use of a HiPPO  initialization matrix, have significantly improved SSMs. Subsequent studies [18; 15; 17; 35; 1] show that SSMs can match transformers in long-range sequence modeling with the added benefit of linear time and space complexities. Notably, Mamba  has advanced over transformers in NLP by using a time-varying system with context-dependent parameters, enhancing the differentiation of hidden states over long sequences. This positions Mamba as a strong alternative to transformers across various domains.

In computer vision, ViM and VMamba [74; 41] are the first works to introduce Mamba as a building block in discriminative tasks. Sequentially, Mamba is explored in many computer vision tasks such as medical imaging [43; 39], point clouds [70; 34], and image generation [25; 73]. Similar to vision transformers, images are divided into patches, and the patches are mapped into tokens. The tokens are then arranged in a sequence following a scanning order. In vision transformers, the scanning order does not matter since attention scores are computed between every token pair. However, SSMs consider the order information, introducing an inductive bias about 2D images into the model. Therefore, scanning order is vital in setting vision models' performance. ViM  proposed a bidirectional scanning order (sweep-2) for discriminative tasks. VMamba  proposed cross-scanning (sweep-4) per each Mamba building block. This cross-scan improves the model performance but costs enormous overhead. MambaND  reduces that cost by introducing two methods: interleaved scanning and multi-head scanning. Interleaved scanning, which alternates the scanning order in the sequential blocks, is simpler but gains better performance in discriminative tasks. Recently, Zigma  proposed a zigzag-8 scanning order to preserve the locality property (i.e., each token is adjacent to its next and previous tokens). The zigzag-8 scanning order shows faster convergence compared to the bidirectional one. In this paper, we show that too many scanning orders, e.g., sweep-8 and zigzag-8, may introduce excessive information and lead to worse performance than sweep-4. Instead, sweep-4 offers the best performance (Section 4.4).

### Diffusion Architecture

Diffusion models [22; 57; 56; 51] are an emerging type of generative model that requires a sequential denoising process of several to thousands of steps to sample an image from initial Gaussian noise. Notably, most of them are based on Stochastic Differential Equations (SDE) that require an accumulation of additional stochastic noise at each generation step. Alternatively, there is a line of flow matching methods [36; 40; 44; 5] that emphasize deterministic trajectories from pure Gaussian noise to the target data distribution, favoring a straighter solution path. Their applications span across different tasks like image super-resolution , depth estimation , and motion synthesis . Recent works [44; 31] have proved that diffusion models and flow matching are strongly correlated and can be converted into each other. In this work, we only focus on the simple objective of flow matching for our design.

Meanwhile, most methods are originally based on Unet architecture, which utilizes convolution resblock2 to capture local information at multiscale resolution. The attention layer is also used, interleaving between resblock layers to capture global information. Recently, the vision transformer [9; 42] has emerged and largely surpassed CNNs in many tasks. For diffusion image modeling, several transformer-based architectures [48; 2; 14] have been recently introduced. Transformer-based architectures capture global information better than Unet ones and can generate higher-quality images. Specifically, UViT  replaced convolution resblock layers with transformer blocks and removed downsampling/upsampling blocks. DiT  directly replaced Unet with a vision transformer. Inspired by this, MDT  and MaskDiT  introduced a mask latent modeling approach to better capture contextual information and enhance training efficiency. Although transformer architectures achieve better image generation, these models suffer from quadratic time and memory complexity, slowing down training and inference processes. Recently, with the birth of Flash Attention [8; 7], both training and inference time of these transformer-based models are significantly reduced thanks to the reduced IO bottleneck. However, the computation time complexity remains quadratic. Recently, the S4  model has been introduced to effectively deal with long-range dependency in the NLP field. Furthermore, the S4 model favors the linear complexity time and space, which is more efficient than the transformer. Among S4 class models, Mamba  stands out for its high performance in capturing long-range dependency. In diffusion models, DiffuSSM  adopts S4D  as a building block for their model and achieves better FID compared to transformer counterparts. Recently, Zigma  utilized Mamba for diffusion architecture, using a zigzag scanning order to preserve locality-aware scanning property. Despite showing promising results, Mamba-based diffusion models still struggle to find an optimal scanning scheme to take advantage of the 2D inductive bias from images. We find these approaches stuck in spatial processing, thus failing to incorporate global and local relations. These relations can effectively captured in frequency spectrum, thus we propose to incorporate frequency scanning alongside the existing spatial scanning mechanism.

### Frequency-based networks

Employing frequency components extracted by Fourier, Cosine, or Wavelet transform in solving vision tasks was common in classical computer vision. Many modern works still find this practice useful in improving the performance of deep neural networks. In perception tasks, several works [37; 38; 67] integrate frequency processing in transformer architecture. NomMer  applied a discrete cosine transformer into global attention to efficiently yield synergistic context from both global and local contexts. To improve Masked Image Modeling, Ge-AE  introduces an additional frequency decoder using Fourier transform to reconstruct the high-frequency information better. Wave-Vit  applies wavelet into self-attention modules to reduce the time and space complexity of the transformer architecture while still preserving the performance. Recent work Simba  introduced Fourier-based layers (EinFFT) in combination with Mamba block to replace MLP layers for better channel mixing. To solve the image denoising problem, FreqMamba  applied a wavelet and Fourier transformer to process features injected into the Mamba block. In generative modeling, several works [66; 49; 69] corporate wavelet frequencies into generative framework. By explicitly decomposing features/images into high- and low-frequency bands through wavelet transform, the generative model can train stably and converge faster. Furthermore, the high frequencies can be learned more efficiently, leading to a sharper synthesis image. Observing the benefit of wavelet processing in generative modeling, we apply discrete wavelet transform on local features before feeding into the Mamba layers. By using cross-attention to fuse wavelet frequency features and spatial features, our method achieves significant improvement in image synthesis compared to merely spatial feature processing.

## 3 Method

This section presents DiMSUM, a novel architecture aiming for effective and high-quality image synthesis. Preliminary knowledge will be provided in section 3.1, then overview structure and mechanism of the proposed network (section 3.2), and finally its core components (sections 3.3 and 3.4).

### Preliminary

**State Space Model (SSM).** SSM is a new type of sequence model that uses an implicit hidden state \(h(t)^{N L}\) to map the \(1D\) input signal \(x(t)^{L}\) to its corresponding output signal \(y(t)^{L}\). This process is formulated by a parameter matrix \(^{N N}\) and two projection parameters \(^{N 1}\) and \(^{1 N}\):

\[h^{}(t)=h(t)+x(t), y(t)=h^{}(t).\]

For practical usage, the continuous parameters \((,)\) are discretized by a time-scale parameter \(\) to produce discrete parameters \((},})\), following a zero-order hold (ZOH) rule:

\[}=(),}=()^{-1}(( )-)).\]

Hence, the continuous system is rewritten as follows:

\[h_{t}=}h_{t-1}+}x_{t}, y_{t}= h_{t}.\]Mamba then proposes a selective mechanism to enrich the dynamic interactions of different sequential states. In other words, the constant parameters \((},,)\) are tuned into input-dependent parameters, enforcing the context awareness of input sequence states:

\[}_{t}=_{N}(x_{t}),_{t}=_{N}(x_{t}),}=(_{ }(_{}(}))),\]

where \(_{*}(.)\) is a projection layer to \(*\)-dimensional vector, \((.)=(1+(.))\), and \(_{L}(.)\) means duplicating a single-value vector to \(L\)-dimensional vector.

**Diffusion model.** Diffusion models [55; 22; 57; 58; 51] are also known as score-based models that learn the transitional trajectories from a Gaussian noise to signals in the target domain. Most methods are based on Stochastic Differential Equation (SDE), requiring a larger number of function evaluations to generate an image. Recently, Flow matching [36; 40; 5; 44] has proved to be a promising method that finds a deterministic mapping between input Gaussian noise and input data via solving Ordinary Differential Equation (ODE). Given an input data \(x\) belonging to the modeling distribution \(p(x)\) and a random noise \((0,)\), the forward process is formulated as:

\[x_{t}=x_{t}+_{t},\] (1)

where \(x_{t}\) is the noise-added data at a time step \(t\) and \((_{t},_{t})\) are time-dependent functions of \(t\). Particularly, these functions are constrained such that \(_{1}=_{0}=0\) and \(_{0}=_{1}=1\) to produce correct mapping between data \(x\) at \(t=0\) and noise \(\) at \(t=1\). Specifically, [36; 40; 5] use a simple linear function where \(_{t}=1-t\) and \(_{t}=t\). We employ Flow matching's training objective to estimate the velocity between noise \(\) and data \(x\):

\[=*{argmin}_{}_{t,x_{t}}[||x_{1} -x_{0}-v_{}(x_{t},c,t)||],\] (2)

where \(v_{}\) is a velocity estimator implemented by a neural network with parameters \(\) and \(c\) is an input condition (e.g., class or text). If no condition is used, \(c\) is set to empty.

**Wavelet transformation.** Among frequency transform techniques, wavelet transform stands out for its simplicity and efficiency. It preserves the structure of image space, with low-frequency subbands representing down-sampled approximations of the input image, while high-frequency ones emphasize local details such as vertical, horizontal, and diagonal edges. Particularly, Haar feature is the most prevalent wavelet transform, consisting of a low-pass filter \(L=}[1 1]\) and a high-pass filter \(H=}[-1 1]\). To decompose an image \(x^{H W}\), it needs to construct 4 kernels \(LL^{T},LH^{T},HL^{T},HH^{T}\), then applies them to the input image to extract corresponding subbands \(\{x_{LL},x_{LH},x_{HL},x_{HH} x_{s}^{H/2 W/2}\}\), respectively. This process is called discrete wavelet transform (DWT). Notably, these filters are pairwise orthogonal, so an invertible matrix exists to map the data back to the original image space, coined as discrete inverse wavelet transform (IDWT). Given its benefits, we propose to use wavelet transform to supplement the local structure of frequency components into the process of Mambo, thus leading to enhanced image quality and training convergence, as demonstrated through our empirical experiments in section 4.

### Overview of the proposed network

Inspired by the advancement of Mambo-based diffusion models and frequency-based networks, we design a novel architecture DiMSUM for effective and high-quality image synthesis with the structure

Figure 1: Overview of DiMSUM architecture.

presented in Fig. 1. Similar to Latent Diffusion Models , our method performs image generation on the latent space of a pre-trained encoder +. The method first receives an input image and encodes it to a latent map of size \(4 H W\). It then processes the latent map using our proposed Diffusion Mamba network, whose core is a sequence of DiMSUM blocks, each consisting of DiM blocks that employ a novel Mamba structure with spatial and frequency scanning fusion and a globally weight-shared transformer block. The processed latent is then decoded to the output image.

Footnote †: https://huggingface.co/stabilityai/sd-vae-ft-ema

### DiM block

A core component of our approach is the DiM block that relies on a novel Spatial-Frequency Mamba fusion technique. In this section, we will discuss in detail the ideas behind this vital component.

**Scanning in frequency space.** Mamba-based approaches in diffusion models often lack effective scanning schemes for preserving both local and global 2D spatial information. Although several works have proposed different heuristic scanning methods  to address this issue, these approaches are insufficient for capturing local pixel dependencies and long-range frequency relationships. Though LocalMamba  proposed a window scanning to mimic the convolution kernel, it often underperforms compared to previously mentioned scanning methods as it is limited to the dependencies of nearby-pixels within window.

DiMSUM addresses these challenges by decomposing the original image into frequency wavelet subbands. This approach is effective to capture long-range frequency while preserving relations across different subbands. We redesigned the window scanning, where each window corresponds to a subband of the frequency space as in Fig. 3. Consequently, each window captures the full range of low/high-frequency signals from the original image. This advantage sets us apart from traditional window scanning in image space. As the model progresses through different subbands, it incorporates spatial information represented at various low-to-high frequencies, adding valuable context to the denoising process.

**Wavelet Mamba.** We now examine the integration of the wavelet transform into the Mamba structure shown in Fig. 2. Wavelet Mamba first applies DWT to decompose input features into wavelet subbands. Our main setting uses two-level Haar wavelet to map input into low and high-frequency features. Given input feature \(x^{C H W}\), first-level wavelet transform is applied to produce 4 wavelet subbands of size \(^{C H/2 W/2}\). Each

Figure 3: Comparison of window scanning on image and wavelet space. For illustration, one-level wavelet transformation is applied and each subband is half the resoluton of original image.

Figure 2: Illustration of Wavelet Mamba (Best view in color). For illustration purpose, we plot wavelet representations of an input image but our real process is performed on encoded features of the input. Giving an image of size \((8,8)\), for example, it is first decomposed to four wavelet subbands of size \((4,4)\) where each is further transformed to 2nd-level subbands of size \((2,2)\). Green dots indicate pixel points within each wavelet subband and a window of size \(2 2\) is used to perform scanning across multiple wavelet subbands like the CNN kernel.

subband is then further decomposed into second-level wavelet subbands of size \(^{C H/4 W/4}\). This feature is pivotal, as we decompose every wavelet subband to evenly separate an input image into multiple wavelet patches, unlike conventional wavelet transformations that process only LL subbands at the next level. In Wavelet Mamba, we concatenate those subbands to form a 1D sequence, apply window scanning within each subband, and slide across the sequence for feature extraction. The window scanning is inspired by a CNN kernel proposed in  with two window sliding directions: left to right and top to bottom. Note that since low-frequency subbands capture the main content of image, it should be input first. Therefore, we do not use reverse scanning orders: right to left and bottom to top. After passing through Wavelet Mamba, the output features are transformed back to input shape by using IDWT twice. With wavelet module, our model can better capture local structure of frequency information. Thus, incorporating Wavelet Mamba with Spatial Mamba can offer better performance, yielding high-quality image generation (Spatial-frequency Mamba in Fig. 1).

**Cross-Attention fusion layer.** Given \(f_{s}\) and \(f_{w}\) are spatial and wavelet features obtained by sweep and window scan. We combine these features using a cross-attention fusion layer as follows:

\[q_{s},k_{s},v_{s}=(f_{s}), q_{w},k_{w},v_{w}= (f_{w}),\] \[f_{out}=(((q_{s},k_{w},v_{ w}),(q_{w},k_{s},v_{s}))).\]

More specifically, we first compute each feature's query (\(q_{*}\)), key (\(k_{*}\)), and value (\(v_{*}\)) using linear layers. To fuse the information between spatial and wavelet features, we do cross-attention by swapping the queries (\(q_{*}\)) of spatial and wavelet before applying a self-attention module onto each key, query, and value triplet. Finally, we concat the outputs of two cross attentions by channel followed by a linear projection to obtain the output feature \(f_{out}\) (see the last subfigure in Fig. 1).

**Conditional Mamba**. Unlike attention modules, conventional Mamba has no explicit mechanism to inject input conditions into its flow. We propose a simple technique that enables Mamba to take in any conditional input \(c\) via initializing the very first hidden state with the embedding \(c\) instead of zero, as in original Mamba. This can be considered as a type of prior injection into Mamba. Specifically, the recurrent process of Mamba can be rewritten as below:

\[h_{0}&=}h_{-1}+}x_{0}\\ y_{0}&=h_{0},h_{t}&=}h_{t-1}+}x_{t}\\ y_{t}&=h_{t}\] (3)

In conventional Mamba, \(h_{-1}\) is set to zero as there is no previous state at the beginning. Here, we set \(h_{-1}=_{D}(c)\) to inject context prior into flow of Mamba. As shown in ablation, conditional mamba effectively enhances model performance. This is beneficial for both unconditional and conditional generation. For unconditional image generation, we create an auxiliary learnable token to capture image space's global information, similar to vision transformers [10; 59]. For class-conditional generation task, we use a class embedding to condition on Mamba. Conditional Mamba is enabled by default in DiM blocks (Fig. 1).

### Globally-shared attention block

Since Mamba is better than transformer at long-range dependency [18; 15] but weaker than transformer at in-context learning , we propose a hybrid mamba-transformer architecture which favors both these properties as in recent work Jamba . Motivated by Zamba , we introduce the globally-shared transformer (attention) block. This shared attention block is added after each of four DiM blocks as shown in Fig. 1 since we want to preserve the continuity of the 4-sweep alternative scanning order. By using shared weights, we significantly reduce the number of parameters introduced by different attention blocks. This layer complements the flow of Mamba since transformers excel at extracting global relations without relying on manually defined orders of input sequences, as in Mamba. Hence, with this hybrid architecture, our method effectively addresses Mamba's order dependence while significantly reducing the FID with very few additional parameters.

## 4 Experiments

**Implementation.** We established a depth of 20 layers, a base width of 1024, and a patch size of 2 for network configuration (further information on hyperparameters in appendix A). We run experiments on standard datasets: CelebA-HQ, LSUN Church, and ImageNet. For the sampling method, we follow [44; 5; 25] to use adaptive ODE solver 'dopri5' for evaluation. We assessed performance using the Frechet Inception Distance (FID) and Recall by first generating 50K images and then comparing them with the full reference dataset. We also report GFLOPs and the number of training iterations per second to further illustrate the model efficiency.

### Image Generation

On the CelebA-HQ dataset at resolutions of 256x256 and 512x512 (Fig. 4), our method achieved state-of-the-art FID scores of 4.62 and 6.09, respectively, surpassing the scores reported in recent studies. Furthermore, DiMSUM demonstrated superior recall scores, indicating a greater diversity in the generated samples compared to other methods. This result is particularly impressive, given the majority of baseline methods are based on diffusion processes, which are known for excellent diversity. On LSUN Church (Fig. 4(a)), our method outperformed diffusion-based methods and achieved results nearly on par with GAN-based approaches. Moreover, our method recorded the highest recall metric of 0.56, significantly exceeding the recall of GAN methods, thereby underscoring its robustness in generating diverse and high-quality images.

   Model & NFE\(\) & FID\(\) & Recall\(\) & Epochs \\  Our & 73 & **3.76** & **0.56** & **395** \\ DIFFUSSM  & 250 & 3.94 & - & - \\  LFM-8  & 90 & 5.54 & 0.48 & 500 \\ LDM  & 250 & 4.02 & 0.52 & 400 \\ WaveDiff  & 2 & 5.06 & 0.40 & 500 \\ DPDM  & 1000 & 7.89 & - & 640 \\  StyleGAN  & 1 & 4.21 & - & - \\ StyleGAN2  & 1 & 3.86 & 0.36 & 13K \\   

Table 1: Class-conditional image generation on ImageNet \(256 256\) dataset.

Figure 4: Result of our model versus others upon CelebA-HQ. is our reproduced result based on Zigma’s official code, and is an adopted result from LFM paper.

Figure 5: Result of our model versus others upon LSUN Church \(256 256\) dataset.

On the ImageNet1k 256 dataset, our methodology attains a formidable FID of 2.26 using a guidance scale of 1.4, surpassing the DiT models across comparable and larger model configurations, such as DiT-XL/2. This performance superiority extends to other benchmarks, including the SSM-based DIFFUSSM-XL-G model. Although our model yields results similar to those of the SiT model, our model is approximately 30% smaller in size.

### Training convergence

As reported in Tables 1, 4, 5a, our method requires less training epochs/iterations to reach the optimal performance compared to the baseline approach, implying a strong and fast convergence. To better illustrate the training convergence comparison, we illustrate in Fig. 3(d) the performance of different diffusion-based models over training epochs regarding the FID-10K on CelebA-HQ 256. Notably, our proposed method demonstrates a superior convergence speed, rapidly decreasing FID score and stabilizing at a significantly lower value than the other methods like LFM, LDM, and DiT. This rapid descent is particularly evident within the first 150 epochs, after which our method maintains a low FID score and still with sight on decrement, suggesting a high-quality image generation capability. Compared to the learning curves of other methods, our method exhibits a more stable trajectory without significant oscillations between training epochs.

### Ablation of network design

In this section, we ablate the design choices for our network, using experiments on the CelebA-HQ 256 dataset. For the starting baseline, we adopt the same training settings from Zigma. We choose sweep-4 with interleave scanning order  by default. In Fig. 1(a), with our proposed conditional Mamba, the FID score is improved from 6.19 to 5.27, and the same trend is observed for recall. Meanwhile, adding Wavelet Mamba followed by a simple concatenation layer to combine spatial and wavelet features results in a worse score of 5.87 due to the weak alignment between these features. This demonstrates that our proposed cross-attention fusion layer is crucial for performance improvement, fully leveraging wavelet components to achieve a score boosted to 4.92. The performance is further enhanced to 4.66 by incorporating the weight-shared transformer block.

**Design choices of fusion layer.** In Fig. 1(c), we report metrics for different fusion layers, ranging from simple linear projection to cross-attention layers. As shown, our proposed cross-attention fusion

Table 2: Ablation studies on CelebA-HQ \(256 256\) dataset at epoch 250.

with swapped query achieves the best results while requiring fewer parameters and GFLOPs than the attention option, with only a marginal increase in computation compared to the linear option.

**Number of wavelet levels.** As shown in Fig. 2b, two wavelet levels provide the best performance on the CelebA-HQ 256 dataset. We argue that the choice of wavelet levels should be based on the input resolution. An input image of size \(256 256\), for instance, is encoded to a compact latent of size \(32 32\), which is further patchified by 2 to the small size of \(16 16\). Hence, applying 3 wavelet levels results in extremely small wavelet subbands of size \(2 2\), leading to reduced performance.

**Alternative frequency transform.** Apart from wavelet transform, we also consider different types of frequency techniques like DCT and Fourier Transform (Fig. 2d). For DCT, we propose a multi-order JPEG scanning strategy (illustrated in Fig. 8), based on JPEG compression  instead of the window scanning. For Fourier Features, we directly adopt the EinFFT block from SiMBA . In either case, the performance drops compared with the default choice of wavelet.

**Transformer layer.** In Fig. 2f, we assess the advantage of the transformer layer for both Conditional Mamboa and Spatial-Frequency Mamboa. As shown, the globally-shared transformer further boosts the performance of our Spatial-Frequency Mamboa. In contrast, applying this layer solely to spatial Mamboa increases the FID score by 0.13, highlighting the essence of our Spatial-Frequency Mamboa in conjunction with the transformer layer. Meanwhile, replacing this shared layer with independent transformers results in a decline of 0.43 in FID and 0.03 in Recall.

### Ablation of scanning orders

In Fig. 2e, we compare different scanning orders of Mamboa. We keep it simple by using only Mamboa block for all experiments without the globally-shared transformer and fusion layer. In Spatial-frequency Mamboa, we use "Sweep-4" scanning for spatial features by default and only test other scanning methods for wavelet features. Overall, Sweep-4 performs best when combined with our proposed Conditional Mamboa module. We also show that scanning with many-way orders like Sweep-8, Zigzag-8, and Jpeg-8 is not guaranteed to yield better performance than Sweep-4 scanning. In spatial-frequency Mamboa, it is demonstrated that our proposed window scanning for wavelet Mamboa provides a better outcome than conventional sweep-4 scanning.

## 5 Conclusion

Our paper introduces a novel, promising architecture that seamlessly integrates spatial and frequency features into Mamboa process. By leveraging wavelet transform within the Mamboa framework, our method enhances local structure awareness and ensures efficient spatial and frequency information fusion. This dual-focus strategy improves the detail and quality of generated images and accelerates training convergence. Our comprehensive experiments demonstrate that DiMSUM consistently outperforms state-of-the-art models of comparable size across multiple benchmarks, achieving lower FID scores and higher recall metrics, highlighting its ability to produce diverse and high-fidelity images. The proposed cross-attention fusion layer and globally shared transformer block also contribute to the model's robustness and scalability. Considering the promising results, we anticipate that future research in related domains, such as text-to-image synthesis, will adapt our backbone architecture and achieve comparable improvements in performance.

**Social impact and limitation.** We believe that our proposed network advances the architectural design of state-space models for image generation. This model can be extended to various tasks, such as large-scale text-to-image generation and multimodal diffusion. While there is a risk that our architecture could be misused for malicious purposes, posing a social security challenge, we are confident that this risk can be mitigated with the recent development of security-related research. Hence, the positives can outweigh the negatives, rendering the concern minor.

While our method outperforms other diffusion baselines in generation quality and training convergence, we acknowledge areas for improvement. These include designing a multiscale architecture and addressing manually defined scanning orders. Another advanced technique, such as masking training regularization [14; 72], is orthogonal to our approach and could lead to further improvements.