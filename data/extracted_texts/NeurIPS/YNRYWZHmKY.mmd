# [MISSING_PAGE_EMPTY:1]

[MISSING_PAGE_EMPTY:1]

The existing literature [1; 3; 16; 20; 13] has worked in image latents to address information loss, e.g., objects missing; however, there is no research on the main cause of the problem----text embedding. Therefore, this paper focuses on text embedding to investigate the issue. We first analyze the information bias in text embedding and find that the generated objects tend to bias towards the first mentioned object, as Table 1 shows, when the given prompt contains more than one object. The reason for biasing towards the first mentioned object is due to the causal manner in which making \(n^{th}\) token embedding contains the weighted attention of token embeddings between \(0\) to \((n-1)^{th}\) tokens, as illustrated in the right-bottom side of the Fig. 2. Moreover, we analyze the semantic contribution of the token embeddings by masking different tokens. Table 2 and Fig. 3 demonstrate that the causal manner would contribute to the accumulation of general information in the special token, e.g., end of token (<eot>), and padding token (<pad>). Masking the embeddings of given tokens still allows the T2I model to generate the expected information using the remaining special tokens' embeddings.

Due to the significant information loss issues in T2I models, such as object mixing and missing, we dissect the generative process of the T2I model to pinpoint the origins of these losses. In the text embedding, the causal manner would make the embedding of the \(n^{th}\) token mixed with the token embedding between \(0\) to \((n-1)^{th}\), which makes the later token embedding similar to the earlier embedding. In the case of a prompt "a cat and a dog", the causal manner would mix the <dog> embedding with the <cat> embedding. This similarity in embeddings results in similar distributions on cross-attention maps, as detailed in Sec.4.4. When denoising directions on these maps align too closely throughout the denoising steps, it can cause a mixed representation if responses are equally low, or one object may overshadow the other if its map elicits a stronger response, leading to object disappearance. These phenomena are visualized in Fig. 1. To address the issue of information bias and loss, we propose the Text Embedding Balance Optimization (TEBOpt) to promote distinctiveness between embeddings of equally important objects for preventing mixing and working alongside existing image latent optimization techniques to address object disappearance. The main contributions of this paper are outlined as follows:

* This paper examines how text embedding contributes to generated images in text-to-image diffusion models and demystifies how the causal manner leads to information bias and loss while contributing to general information.
* We propose the Text Embedding Balance Optimization solution containing one positive and one negative loss to optimize text embedding for tackling information bias with 125.42% improvement in stable diffusion.
* We propose an evaluation metric to measure information loss. Compared to the CLIP score for evaluating text-image similarity, and the CLIP-BLIP score for evaluating text-text similarity, our evaluation metric provides a concrete number for identifying whether the specified object exists in the generated image.

## 2 Preliminaries

Text-to-image diffusion models [11; 12; 14; 17; 18] typically contain the text encoder [15; 6], a variational autoencoder (VAE), and a denoising UNet, as Fig. 2 demonstrates. Given a text prompt, the text encoder would first obtain the text hidden states \(h_{s}^{N D}\) from the sum of token embedding and positional embedding, where \(N\) is the maximum token length in the text encoder and \(D\) represents the embedding dimension; then, it calculates the text embedding by going through the encoder layers with self-attention mechanism and causal masking manner. Next, given the text embedding \(\) and the initial image noise \(z_{t}\), the denoising UNet \(_{}(z_{t},,t)\) would gradually denoise latents in each timestep \(t\) to get the final image by iteratively predicting the noise residuals conditioned on the text embedding and the previous denoised latents.

Notably, the causal masking manner in the text encoder makes every token have information only from its previous tokens, which causes the text embedding to have information bias. The bottom of Fig. 2 illustrates how the causal manner works in the self-attention mechanism . The query \(\), key \(\), and value \(\) are calculated as follows:

\[=_{Q}(h_{s}),=_{K}(h_{s}),=_{V}(h_{s} ),,,^{h_{n} N D /h_{n}}\] (1)\[(,,)=(^{ }+}{}}),_{ij}= 0,&j i\\ -,&j>i,\] (2)

where \(_{Q}\), \(_{K}\), and \(_{V}\) are learned linear projections, and \(h_{n}\) represents the head number. \(d_{k}\) is the dimension of \(\) and \(\) is the causal mask. Upon receiving the attention weight \(^{}^{N N}\), the mechanism applies the causal mask \(\). Then, taking \(\) as a weight metric to get the weighted sum of attentions, representing the embedding for each token. Due to the causal manner, the embedding of the \(n^{th}\) token (\(_{n}\)) contains weighted information for the tokens \(0\) to \(n-1\). This representation causes two issues: First, the earlier token information accumulates the most since every subsequent token has it. Furthermore, the later token's embedding is not a pure representation of itself, potentially causing identity loss. For example, given a text prompt "a cat and a dog", the earlier token <cat>'s information would be mixed into the embedding of the later token <dog>. It makes the generated image have a higher possibility of generating two cats, as the middle row of Fig. 2 shows.

## 3 Analysis and method

In this section, we analyze how causal manner affects the text embedding as well as the generated images in text-to-image diffusion models.

### Information bias in the text embedding

We investigated 400 different prompts with different random seeds with structures (a) "a/an <object1> and a/an <object2>" and (b) "a/an <object2> and a/an <object1>" by exchanging the position of <object1> and <object2> in prompt (a), where objects are randomly sampled from 17 different animals.

Figure 2: Overview of the text-to-image generative model, including the details of the causal manner in attention mechanism. Because of the causal nature of the embedding, information is accumulated from the starting token through the end of the sequence, resulting in bias in the earlier token. To balance the critical information, we propose text embedding optimization for purifying the object token with equal weights within their corresponding embedding dimension.

Table 1 shows that within the same set of two animals and the same random seed, both bias on the earlier mentioned animal. In prompt (a), the statistics with pink background show that it causes 67.5% missing issue, including 46.0% generating only object 1. Similarly, in prompt (b), it causes 69.5% missing issue, including 47.0% generating only object 2. The information bias (Info bias) is defined by \(}{\#}\) and, in both prompts, they are 2.30 and 0.46 respectively, which are far from balanced (Info bias = 1).

#### 3.1.1 Causal effect for generating images with more than one object

Driven by the observation of information bias, we investigated the text embedding contribution of critical tokens. In Stable Diffusion, we first utilize CLIP  as the text encoder to obtain the text embedding \(_{i}^{N D}\), where \(i[0,N-1]\). Before the denoising UNet gets the text embedding and image latent, we mask the chosen embedding and generated the corresponding images. Table 2 provides the quantitative analysis in 400 different prompts with the same setting of (a) used in Sec. 3.1, extending with three masking settings corresponding to Fig. 3's each column.

Firstly, the default experiment in Table. 2 demonstrate stable diffusion has 20.25% mixture, 67.50% missing ratio, and 2.3x information bias to the first mentioned object. Regarding the visualization sample of the default experiment, the first column in Fig. 3 shows that animals (cat and dog) within one prompt in different orders with the same initial image latent would generate different results. The bottom one demonstrates the mixture issue, making the right animal simultaneously similar to dog and cat. Regarding the second experiment, we mask all the embeddings of the given tokens, where \(_{i}=-inf,i\). It reduces 2.5% mixture rate but increases 11.75% missing rate. With only

   Prompt & (a) A/An obj1 & (b) A/An obj2 \\  & and a/an obj2 & and a/an obj1 \\  
2 objects exist & 12.25\% & 11.75\% \\ mixtures & 20.25\% & 18.75\% \\ only obj1 exist & 46.00\% & 21.75\% \\ only obj2 exist & 20.00\% & 47.00\% \\ no target object & 1.50\% & 0.75\% \\  Info bias & 2.30 & 0.46 \\   

Table 1: Both prompts strongly bias towards the first mentioned object. The bias generally exists in more objects, reported in Supplement D.

Figure 3: Masking text embedding to identify the contribution of critical tokens, e.g., cat/dog, and special tokens, e.g., sct, ect, pad. The first row and the second row both contain cat and dog inside prompt but in different order. The analysis shows that special tokens contain general information about the given prompt. However, the cat/dog tokens carry more weight than the special tokens. In the last two columns, where one of the animal token embeddings is masked while retaining the special tokens’ embedding, the generated image is predominantly influenced by the remaining animal’s token embedding.

one object existing, the existing rate for object 1 and object 2 is more balanced as the embeddings of all given tokens are masked. It suggests that the remaining embeddings, which are special tokens, including <sob>, <eot>, and <pad>, contain the information of the masked embeddings due to causal manner in the text encoder.

An important insight here is that eliminating dominant embeddings reduces the mixture rate. One of the causes of the mixture issue is the attention mixture, as the text embedding in Fig. 2 shows. Given a prompt "a cat and a dog", as the text embedding is projected as key and value multiplied with the query to get cross-attention maps during denoising, the <cat> embedding would trigger the <cat> response while the <dog> embedding would trigger both <dog> and <cat> responses, causing animal mixtures. Once one of the object's responses dominates, the other animal disappears, leading to a missing issue. In the third and fourth columns, we mask one of the animals' text embeddings, resulting in generating only the remaining animal. These experiments suggest that, firstly, causal manner makes the information accumulate from earlier token to later token. Even masking critical text embeddings, special tokens, i.e., <sot>, <eot>, <pad>, can generate the given prompt information. In addition, although special tokens accumulate information to form general embeddings, when the text embeddings contains critical tokens' embeddings, the critical token would lead the generated results, as the last two columns show.

#### 3.1.2 De-causal effect for generating images with more than one object

Hypothesis._Replacing the token embedding of later mentioned object from the corresponding pure embedding, the hypothesis expects to solve the bias problem and information loss._

Following the setting of the prompt structure (a) in Sec. 3.1, with a text prompt "a/an <obj1> and a/an <obj2>", the token index of <obj1> and <obj2> are taken into the critical token set \(O=\{2,5\}\). It first generates the pure embedding of the later mentioned one, "a photo of a <obj2>" (\(_{p_{2}}\)). Then, replace the corresponding token embedding of <obj2> in the original embedding (\(\)) with the pure one. The combined text embedding (\(^{}\)) is as follows:

\[^{}=_{i}&}i O  i=(O)\\ _{p_{_{n}},5}&}\,,\] (3)

where n refers to the \(n^{th}\) object. Table 3 reflects that directly replacing original embedding with the pure embedding would balance the information of object 1 and 2; however, it would result in a 5.25% loss in 2 objects coexistence. A nearly equal probability of generating only objects 1 and 2 proves that replacing token embedding will pure embedding eliminates accumulated information about object 1.

### Method: balancing critical information in the text embedding

While the causal manner results in information bias and information loss, it contributes to generate image content aligned with the general prompt information. To eliminate the accumulated information

   Analysis & Default & Mask 1-5 & Mask 1-3 & Mask 3-5 \\  
2 objects exist & 12.25\% & 3.00\% & 2.75\% & 0.50\% \\ only mixture & 10.00\% & 11.50\% & 5.25\% & 3.50\% \\ obj1 + mixture & 6.50\% & 2.75\% & 0.00\% & 0.75\% \\ obj2 + mixture & 3.75\% & 3.50\% & 3.75\% & 0.00\% \\ mixture sum & 20.25\% & 17.75\% & 9.00\% & 4.25\% \\  only obj1 exist & 46.00\% & 33.25\% & 3.00\% & 89.00\% \\ only obj2 exist & 20.00\% & 42.00\% & 82.00\% & 1.75\% \\ no target objects & 1.50\% & 4.00\% & 3.25\% & 4.50\% \\ missing sum & 67.50\% & 79.25\% & 88.25\% & 95.25\% \\   Info bias & 2.30 & 0.79 & 0.04 & 50.86 \\   

Table 2: **Analysis of masking token embeddings.** Masking all the given token would reduce the mixture issue but increase the missing issue with balanced object 1 and object 2 existing rate. Masking one of the objects would not completely eliminate the masked object’s information but would significantly reduce its existing rate. The implementation details are in Supplement B.

   Prompt & Default & Hypothesis \\  
2 objects exist & 12.25\% & 7.00\% \\ only mixture & 10.00\% & 10.50\% \\ obj1 + mixture & 6.50\% & 3.25\% \\ obj2 + mixture & 3.75\% & 3.50\% \\ only obj1 exist & 46.00\% & 44.00\% \\ only obj2 exist & 20.00\% & 30.25\% \\ no target object & 1.50\% & 1.50\% \\   Info bias & 2.30 & 1.45 \\  

Table 3: **Analysis of Hypothesis.** Replacing the token embedding of later mentioned object from the corresponding pure embedding can balance the information but lead to a large drop of two objects coexistence.

but retain general information, we design a Text Embedding Balancing Optimization, called TEBOpt (uppermost in Fig. 2), cooperating with image latent optimization to address these issues. The TEBOpt tackles information bias and object mixture issues while the latent optimization tackles object mixture and missing issues. Since the missing object is caused by an insufficient response value and an inadequately activated region in the cross-attention map corresponding to that object, and the text embedding is unable to precisely determine the activated position, our method cooperates with existing latent optimization methods to address this issue.

Regarding a prompt containing two objects, we expect their corresponding text embedding to be unmixed instead of mixing earlier tokens' embeddings. Since the ultimate goal is to preserve the general information of the two objects, we cannot directly replace the original embedding with the pure object's text embedding. This would result in a high probability of losing one of the objects. Thus, our proposed TEBOpt contains a TEB loss in order to encourage the later mentioned token's embedding to be less similar to the earlier token's embedding, while at the same time being as similar to its pure embedding. Considering a text prompt with \(k\) objects in a set \(O\), we first obtain each text embeddings \(_{p}^{N D}\) and take the critical token embedding \(_{p,i}^{1 D}\) as the pure embedding. For example, the prompt "a dog and a cat" contains two objects and the pure prompt embedding is calculated in a format prompt of ["a photo of a <dog>", "a photo of a <cat>"]. In summary, the TEB loss is as follows:

\[^{pos}_{TEB}(,_{p})=_{i O}sim( _{i},_{p(i)}),\] (4)

\[^{neg}_{TEB}()=_{i O}_{ j=1\\ j i}^{m-1}sim(_{i},_{j}),\] (5)

\[_{TEB}=-^{pos}_{TEB}+^{neg}_{TEB},\] (6)

where \(sim(u,v)=_{i}_{i}}{\|_{i}\|\|_{i}\|}\) and m means the effective token count, where we do not include <sot> and <eot> for optimization. The implementation details are included in Supplement B. After the text embedding is optimized, the image latent would be updated conditioned on the loss design for cross-attention maps during the denoising process. For example, A&E  contains a loss function to ensure that each selected token activates some image patches in the cross-attention map. SynGen  designs a loss function to encourage the cross-attention map of the relative token to be similar and make the cross-attention map of the unrelative token dissimilar.

## 4 Experiment

### Experimental settings

Baselines.We compare our proposed method with the default Stable Diffusion 1.4 , and 3 state-of-the-art (SOTA) baselines, including Structure Diffusion , Attend-and-Excite , and SynGen . All of them focus on improving attribute bindings or solving object missing in text-to-image diffusion models. However, there is no one considering the information bias. In this literature, the objective is to analyze information balance caused by pretrained text encoders instead of surpassing existing SOTAs on solving object missing. Therefore, we would provide the experimental results of our proposed method on top of the baselines.

Data.We follow previous methods  to create a set of 400 prompts with the format "a/an <obj1> and a/an <obj2>" with corresponding random seeds. The objects are sampled from 17 different animals defined by previous methods , including cat, dog, bird, bear, lion, horse, elephant, monkey, frog, turtle, rabbit, mouse, panda, zebra, gorilla, penguin and chicken.

### Evaluation metrics

To analyse the issues of mixed objects and missing objects in generated images, we designed an automated evaluation method since existing metrics, e.g., text-image similarity using CLIP  or text-text similarity using CLIP and BLIP , used in SOTAs  cannot provide the exact counting number to indicate whether the object exists or not. Detailed discussions are provided in SupplementC. First, we employed a pre-trained object detection model, OWL-ViT , which is a SOTA in open-vocabulary object detection. We separate the text prompt into k objects and the model separately predicts bounding boxes and confidence scores for the corresponding objects. Mixture status is determined when the overlap of the two bounding boxes for different objects exceeds 90%. Here, we use two different thresholds to detect mixture objects and single objects, ensuring detection accuracy. Additionally, to validate the effectiveness of this automatic metric, we conducted a human evaluation to demonstrate that its results are highly correlated with human perception. We asked users to label 400 generated images, each categorized into one of five options: i) two objects exist, ii) mixture exists, iii) missing object 1, iv) missing object 2, or v) no objects exist. Our automatic evaluation metric achieves an accuracy of 81% based on human responses, demonstrating its effectiveness.

### Qualitative results

Fig. 4 demonstrated the visual comparison between all methods. The same color of the bounding box and the underline for the prompt indicate the same object. The bounding boxes are predicted based on our proposed evaluation method discussed in Sec. 4.2. As an object is wrapped by two bounding boxes with different colors, e.g., A&E  in the first row or Structure Diffusion  in the second row, it has a high potential to contain mixed objects. In the first row in Fig. 4, our TEBOpt (\(_{TEB}\)) helps Stable Diffusion generate the specified dog and help A&E to make the horse dissimilar from the dog. In the second row, our TEBOpt (\(_{TEB}\)) helps Stable Diffusion generate a cat test similar to a lion, which is initially equipped with a man as a visual signal of a lion. Regarding Fig. 5, we demonstrate the generated image with and without our proposed TEBOpt (\(_{TEB}\)). Using our TEBOpt, T2I models solved both the object mixture and missing issues, especially when trying to solve the problem of object mixtures. It is worth noting that objects mixture or missing are also affected by the denoising process in T2I models. Our text optimization mainly contributes to balancing the information in the text embedding and further reducing mixed and missing issue.

Figure 4: Qualitative comparison of all methods. Every prompt uses the same seed.

Figure 5: Qualitative comparison for the generated image with vs. without \(_{TEB}\) in Stable Diffusion 1.4. Every prompt uses the same seed.

### Quantitative results

Table 4 provides the comparative experiment for object mixture, missing and information bias. As Structure Diffusion  manipulates key and value from text embeddings in denoising UNet's cross-attention calculation, which works on text embedding as ours, our method working on top of Stable Diffusion can directly compare with it.

Among all methods with our TEBOpt (\(_{TEB}\)) can further improve the objects' existing balance with 125.42%1 in Stable Diffusion, 78.43% in A&E, and 10.65% in SynGen. Though the balance of Structure Diffusion is better than ours, it causes a 7.5% decrease in the probability of 2 objects coexisting. Since it directly combines pure text embeddings and original embedding, it loses general information with two objects in the text embedding, as the Sec. 3.1.2 discusses.

Furthermore, SynGen  generates a reverse trend between object 1 and 2 since it works to make the two objects' cross-attention maps' distance as far as possible, which would contribute to separating two objects leading to a large improvement in object missing and information balance. With our text optimization, we can further solve mixture issue and making the balance better since we tackle the issue from the front of the problem in text embedding. Thus, text embedding optimization and image latent optimization reach out a good cooperation for solving information bias and loss. The generalizability of \(_{TEB}\) in information bias for more than 2 objects is reported in Supplement D.

In Table 5, we evaluate the similarity of token embeddings and the distance of cross-attention maps between two objects within one prompt. Token embedding similarity (Token sim) is calculated by cosine similarity while the cross-attention map distance (Map dist) is calculated by the symmetric Kullback-Leibler divergence between two normalized cross-attention maps \(M_{i}\) and \(M_{j}\): \((D_{KL}(M_{i}||M_{j})+D_{KL}(M_{j}||M_{i}))\), where \(D_{KL}(M_{i}||M_{j})=_{pixels}M_{i}(M_{i}/M_{j})\). With our \(_{TEB}\), the token embedding similarity between two objects reduces 36.98% and the cross-attention map distance increases 13.62%.

#### Discussion of how the similarity of text embedding affects cross-attention maps' distance.

We calculated the cosine similarity between various text embeddings and displayed the results in Fig. 6 (a). The data indicates that objects with similar colors or sizes, e.g., penguin-panda or turtle-frog, tend to exhibit higher similarity in their text embeddings, which can be attributed to the training mechanism of CLIP. Additionally, we computed the distance between the cross-attention maps, using

    &  &  &  &  \\   & & +\(_{TEB}\) & & +\(_{TEB}\) & & +\(_{TEB}\) & Diffusion \\  
2 objects exist & 14.5\% & +0.5\% & 34.0\% & +1.0\% & 52.0\% & +4.5\% & 7.0\% \\ only mixture & 9.5\% & -1.0\% & 11.5\% & -4.0\% & 10.5\% & -1.0\% & 11.5\% \\ obj1 + mixture & 6.5\% & -1.5\% & 9.0\% & -1.5\% & 6.5\% & -3.5\% & 4.0\% \\ obj2 + mixture & 5.5\% & -2.0\% & 6.0\% & +0.5\% & 2.5\% & +1.5\% & 3.0\% \\ only obj1 exist & 45.0\% & -6.0\% & 26.0\% & -2.5\% & 10.5\% & -0.0\% & 39.0\% \\ only obj2 exist & 17.0\% & +11.0\% & 12.0\% & +5.0\% & 17.0\% & -2.5\% & 34.5\% \\ no target objs & 2.0\% & -1.0\% & 1.5\% & +1.5\% & 1.0\% & +1.0\% & 1.0\% \\   

Table 4: **Analysis of balancing performance**. Within the cases generating one object, we highlight the better balanced results in blue and red.

   Token sim \(\) & SD 1.4 & +\(_{TEB}\) & Map dist \(\) & SD 1.4 & +\(_{TEB}\) \\   Average & 0.454 & 0.286 (-36.98\%) & Average & 2.321 & 2.637 (+13.62\%) \\ \([,]\) & [0.291, 0.669] & [0.137, 0.537] & \([,]\) & [0.287, 5.722] & [0.553, 6.340] \\   

Table 5: Analysis of optimized token embedding similarity (Token sim) and cross-attention map distance (Map dist) between two objects within one prompt.

the same function in Table 5, generated by the two objects' text embeddings with the same initial latent in the early denoising steps. As shown in Fig. 6 (b), there is a positive correlation between the similarity of text embeddings and the distance of the cross-attention maps triggered by the two objects. Specifically, objects with similar text embeddings are more likely to activate overlapping areas during the denoising process. This confirms that similar text embeddings contribute to object mixture, while the short distance of cross-attention maps leading to object missing has been proven by SynGen .

## 5 Related works

Object mixture or missing.Stable Diffusion  pointed out that stable diffusion models have the issue of concept bleeding, which occurs by unintended merging or overlap of distinct visual elements, leading to object mixture or missing. Also, the root cause lies in the usage of pretrained text encoders, including CLIP  and OpenCLIP . However, all the existing methods investigate the issue in the denoising process instead of text encoders. For instance, Attend-and-Excite  proposed an optimization process to ensure every selected token triggers some image patches when calculating the cross-attention maps between text embedding and image features. SynGen  designed a loss function to increase cross-attention maps' similarity between modifier-entity pairs and enhance attribute binding. Structure Diffusion  leveraged linguistic structure to separate the given prompt into several noun phrases and modify the corresponding value to manipulate text embedding when calculating cross-attention maps in denoising steps. Predicted Diffusion  designed to decompose the prompt containing several objects into independent objects and minimize the loss between the generated image based on the combined independent prompt and that of the original prompt. Refocus  proposed to define each object's positional bounding boxes with GPT-4 and designed two loss functions to make the object only denoised inside its given region.

However, there is no task investigating the causal manner in the pre-trained text encoders. Therefore, we share a comprehensive analysis and propose a simple solution to tackle the issue of information bias, object mixture and missing, e.g., generating one animal with bear head and turtle shell when prompting "a turtle and a bear" or generating two cats when prompting "a cat and a dog".

## 6 Conclusion

In this study, we conducted a detailed analysis of text embedding's impact on text-to-image diffusion models, a topic rarely explored. Our findings indicate that the causal processing of text embedding leads to information accumulation, causing biases and loss. Directly replacing accumulated embeddings with purified embeddings, though resulting in decreased coexistence of two objects, enhances the balance between generating either object. We introduce a training-free Text Embedding Balance Optimization (TEBOpt) method that effectively eliminates problematic information in critical token embeddings, improving information balance handling in stable diffusion by 125.42% while preserving object coexistence performance. Additionally, due to the unreliability of existing metrics for assessing inaccuracies in generated images, we propose a new automatic evaluation metric to more effectively measure information loss.

Figure 6: (a) The cosine similarity of text embedding from single word. (b) The KL distance of cross-attention maps that are triggered by two words. The data is ordered by their text embedding similarity.

Acknowledgments

This work is partially supported by the National Science and Technology Council, Taiwan under Grants NSTC-112-2221-E-A49-059-MY3 and NSTC-112-2221-E-A49-094-MY3.