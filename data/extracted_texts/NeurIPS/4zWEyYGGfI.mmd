# A Unified Detection Framework for Inference-Stage Backdoor Defenses

Xun Xian

Department of ECE

University of Minnesota

xian0044@umn.edu

&Ganghua Wang

School of Statistics

University of Minnesota

wang9019@umn.edu

&Jayanth Srinivasa

Cisco Research

jasriniv@cisco.com

&Ashish Kundu

Cisco Research

ashkundu@cisco.com

&Xuan Bi

Carlson School of Management

University of Minnesota

xbi@umn.edu

&Mingyi Hong

Department of ECE

University of Minnesota

mhong@umn.edu

&Jie Ding

School of Statistics

University of Minnesota

dingj@umn.edu

###### Abstract

Backdoor attacks involve inserting poisoned samples during training, resulting in a model containing a hidden backdoor that can trigger specific behaviors without impacting performance on normal samples. These attacks are challenging to detect, as the backdoored model appears normal until activated by the backdoor trigger, rendering them particularly stealthy. In this study, we devise a unified inference-stage detection framework to defend against backdoor attacks. We first rigorously formulate the inference-stage backdoor detection problem, encompassing various existing methods, and discuss several challenges and limitations. We then propose a framework with provable guarantees on the false positive rate or the probability of misclassifying a clean sample. Further, we derive the most powerful detection rule to maximize the detection power, namely the rate of accurately identifying a backdoor sample, given a false positive rate under classical learning scenarios. Based on the theoretically optimal detection rule, we suggest a practical and effective approach for real-world applications based on the latent representations of backdoored deep nets. We extensively evaluate our method on 14 different backdoor attacks using Computer Vision (CV) and Natural Language Processing (NLP) benchmark datasets. The experimental findings align with our theoretical results. We significantly surpass the state-of-the-art methods, e.g., up to \(300\%\) improvement on the detection power as evaluated by AUCROC, over the state-of-the-art defense against advanced adaptive backdoor attacks.

## 1 Introduction

Deep neural networks (DNNs) have made remarkable advancements in various domains, thanks to their exceptional learning capabilities [1; 2]. However, these same learning capabilities that make DNNs so powerful have unfortunately rendered them vulnerable to a growing critical threat known as the backdoor attack [3; 4; 5; 6; 7; 8; 9; 10; 11; 12; 13; 14; 15; 16; 17; 18; 19]. In a typical backdoor attack, the attacker manipulates a small part of the training dataset by inserting specific triggers, such as pixel patches in images  or random word combinations in texts . These manipulated samples are labeled with a specific targetclass. When a DNN is trained on this poisoned dataset, it learns the original data patterns and the engineered relationship between the manipulated samples and the target class. As a result, the DNN exhibits unexpected and often malicious behavior, producing manipulated predictions for test inputs containing the backdoor trigger while functioning normally for unaltered test inputs. The malicious and stealthy nature of backdoor attacks underscores the critical need to defend against such threats.

In an effort to combat backdoor attacks in the inference stage, various empirical studies have been conducted  in both computer vision (CV) and natural language processing (NLP) domains. In these methods, given a trained-and-backdoored model, the defender will determine whether a future query input is clean or backdoor-triggered based on certain features of this query input, as demonstrated in Figure 1. For instance, in CV scenarios, the popular STRIP method  superimposes the future query image with other clean images and examines the model's prediction entropy on these superimposed versions. If the model's prediction entropy on these superimposed versions is abnormally low, the associated original test input is marked as backdoor-triggered. The ONION method  widely used in NLP applications removes individual tokens from the input text and measures the resulting change in perplexity. If removing a particular token leads to a significantly greater perplexity change than others, it is considered a backdoor trigger word.

However, a principled understanding of the backdoor detection problem is underexplored in the literature. First, a mathematical formulation for the backdoor detection problem is lacking, which may impede quantitative analysis of the backdoor challenges. Furthermore, while some existing methods are effective in certain scenarios, many of them lack theoretical guarantees regarding their detection performance, such as ensuring that the probability of misidentifying a clean sample as a backdoor one is no more than a certain threshold. This creates a complicated situation for researchers and practitioners in assessing the effectiveness of these methods.

Additionally, given the same nature of using excessive learning capability to conduct backdoor attacks for both CV and NLP contexts, it is natural to wonder if the defense methods developed for backdoor attacks in CV can be applied to backdoor attacks in NLP. However, applying CV defense methods to NLP is challenging due to the distinct data structures and models in each context . For example, images are treated as continuous while texts are discrete. Thus, the individual words utilized in  lack counterparts in images, preventing the use of techniques in  from defending against CV backdoors. Furthermore, the architectural differences between convolutional neural networks (CNNs) and recurrent neural networks (RNNs) add to the complexity. CNNs exhibit translational equivariance properties , whereas RNNs demonstrate auto-regressive characteristics . Hence, it remains unclear how the concept of CNN channels, as utilized in the backdoor defense of , can be translated to language models such as LSTM , and BERT .

### Our contributions

In this study, we introduce an approach for detecting backdoor test inputs with provable guarantees that is practically applicable to both CV and NLP domains. Contributions are summarized as follows.

**Novel framework of backdoor defenses with provable guarantees**. We first introduce a rigorous formulation for inference-stage backdoor detection problems, encompassing most existing online backdoor defense methods through binary classification/hypothesis testing concepts. This problem formulation enables precise discussions on the possible limitations of current backdoor defense

Figure 1: Illustration of inference-stage backdoor defenses. The backdoor triggers in text queries are indicated in red, while for the image queries, the backdoor triggers consist of a square patch located in the lower-right corner for the traffic sign and a hello kitty embedding added to an image of a dog.

approaches. Based on these findings, we propose a generic backdoor defense framework called _conformal backdoor_, inspired by the conformal prediction literature , for detecting and filtering future backdoor test samples with provable assurances on the false positive rate (\(\)), namely the probability of mistakenly classifying a clean sample as a backdoor one.

**Optimal detection rule and its real-world proxy**. To establish a theoretical comprehension of our proposed framework's detection power, we first show that successful backdoor attacks display distinctive properties under classical learning scenarios. In particular, we show that, among a set of potential backdoor triggers (and the corresponding backdoor data), the most effective (to be defined) backdoor trigger is unique. This uniqueness property allows the defender to devise the uniformly most powerful detection rule, in accordance with the Neyman-Pearson (NP) criterion, when clean and backdoor data distributions are known. To apply to real-world situations where neither clean nor backdoor data distribution is known, we propose a practical and efficient proxy based on the theoretically optimal detectors, which is applicable to both CV and NLP domains. The proxy detector utilizes the latent representations of a backdoored model, such as the penultimate layer, which can be widely applied and is generally independent of the specific data structure (e.g., discrete tokens in text) and the model's properties (e.g., the CNN's translation-invariant property).

**Extensive empirical studies on both CV and NLP** Our proposed technique was evaluated across various datasets for both CV, including CIFAR10 , GTSRB , and Tiny-ImageNet , and NLP, including SST2  and IMDB , and popular model architectures such as ResNet18/34 , VGG19 , base uncased-BERT , and 14 backdoor attacks including BadNets , Blended , TrojanNN , SIG , WaNet , SSBA , TaC , Adaptive Patch , Adaptive Blend , SOS (NLP) , and LWS (NLP) . Experimental results consistently endorse the effectiveness of our technique in terms of detection performance, e.g., up to \(300\%\) improvement on the detection power in terms of AUCROC over the state-of-the-art, STRIP , on adaptive backdoor attacks .

### Related work

**Backdoor attacks** Backdoor attacks modify DNN predictions for specific inputs using backdoor triggers, while leaving regular inputs unaffected. The majority of backdoor attacks, both in CV , and NLP , are implemented through data poisoning, and the major focus of them is on how to design effective backdoor triggers. For instance, in the field of CV, classical methods use square patches and hello kitty as backdoor triggers , which can be visually inspected. As a result, later techniques try to design human-imperceptible backdoor triggers through image warping  and generative modeling . Also, there is a series of methods focusing on creating embedding-level imperceptible backdoor triggers with adaptive methods . Despite the distinct characteristics of natural language data, NLP backdoor attacks exhibit similar techniques to those used in CV. For example, rare word combinations are used as backdoor triggers . In addition, researchers have proposed techniques to replace specific words in a text with new words searched from word embeddings while preserving the semantic meaning of the text . Very recently, a principled theory regarding when and how backdoor attacks work was developed in .

**Latent-space backdoor detection with access to training data** This line of work aims to filter out backdoor data from the training dataset and retrain a sanitized model based on the cleansed dataset . These methods rely on the assumption that there is a clear separation between clean and backdoor data in the latent space of backdoored models, and the defender can use such a separation to detect backdoor samples. In CV, researchers have applied PCA to project high-dimensional embeddings into lower dimensions and then used clustering methods to filter out clean and backdoor data , presuming that backdoor samples are fewer than clean samples. Recent techniques, such as those proposed in , leverage robust estimations techniques to improve the cluster analysis further. In NLP, the CUBE method  employs HDBSCAN  to identify embeddings clusters. In , authors use \(_{2}\) embedding distance to remove backdoor samples by measuring the distance between the embeddings of each training example and the nearest trigger test example using \(_{2}\) norm. Our work differs from these methods in that we do not have access to the training data. The defender has no prior knowledge of future test backdoor inputs, making it possibly more challenging for defending.

**Backdoor detection with access to validation data** This line of work detects backdoor samples by building statistics based on a clean validation dataset, which typically consists of IID samples drawn from clean training data distribution, and using the calculated statistics to decide if a new input is clean or backdoor . For example, the classical method STRIP  filters out poisoned samples by checking the randomness of the model's predictions when the input is perturbed several times , with the idea that the backdoor triggers dominate the rest information in the inputs regarding classification. These methods typically require a case-by-case design, depending on the data structure and models. Our method utilizes the latent representation of the backdoored model, which is not specific to a particular task.

## 2 Preliminary: backdoor defense and attack

**Notations.** Let \(^{d}\) be the input space and \(=\{1,,C\}\) (abbreviated as \([C]\)) be the label space. We denote a model trained on a dataset \(\) by \(f^{}:^{d}[C]\). If \(f^{}\) is a DNN, we decompose \(f^{}\) as \(f^{}=w^{}^{}\), where \(w^{}\) is the projection head and \(^{}\) is the feature extractor (to be specified later).

**Attacker's capability** In line with previous backdoor poisoning attacks, we make the standard assumption that the adversary has control over the training process and can manipulate the training dataset, but does not have access to the inference stage . Formally, let \(^{}=\{(_{i},_{i})\}_{i=1}^{n_{}}\) denote the clean training dataset of size \(n_{}\), independently drawn from a distribution \(_{}\) defined over \(^{d}\). Let \(_{1}:^{d}^{d}\) and \(_{2}:[C][C]\) denote the backdoor data and label transformations, respectively. We emphasize that attackers are allowed to use a wide range of backdoor transformations \(_{1}\), including a fixed patch  and dynamic/sample-specific patches . Let \(^{}=\{(_{1}(_{i}),_{2}(_{i})) \}_{i=1}^{n_{}}\) be the backdoor dataset under transformation \((_{1},_{2})\) with the joint distribution \(_{}\). By default, we consider \(_{2}(y)=1\) for \(y[C]\). Denote the learning model \(f^{},f^{}:^{d}[C]\) to be the model trained on \(^{}\) and \(^{}^{}^ {}\) respectively. We set the risk minimization algorithm as the default formulation for training models.

**Attacker's goal** Given a clean dataset \(^{}\), a set of learning models, and the backdoor transformations \((_{1},_{2})\), the attacker aims to to minimize **(I)** the error rate on backdoor data under backdoor model: \(_{XY}(f^{}(_{1}(X))_{2}(Y))\) under the constraint that the **(II)** the clean accuracy difference under the clean and the backdoor model: \(|_{XY}(f^{}(X)=Y)-_{XY {CLEAN}}(f^{}(X)=Y)|\) does not exceed a pre-specified threshold value.

**Defender's capability** We consider a scenario where the defender can query the fixed, trained model \(f^{}\) with any input but is unable to modify the model or access the poisoned training dataset \(^{}\). Specifically, the defender only has access to two kinds of information. The first **(I)** is the latent representation \(^{}(z)\) for any querying sample \(z^{d}\), of the learning model \(f^{}\), i.e., \(f^{}=w^{}^{}\) for \(f^{}\) being a DNN. For NLP classification tasks using BERT-based models, the output of \(^{}()\) is the [CLS] embedding. For CNNs, the output of \(^{}()\) is the avgpool2d layer immediately after all the convolutional layers. The **(2)** second is a set of validation data \(^{}=\{(x_{i},y_{i})\}_{i=1}^{n}\) which are IID samples from the same distribution of clean training data \(_{}\), with sample size \(n n_{}\) (sample size of the clean training data). By default, we set the value \(n\) to be \(1000\).

**Defender's goal** When given a future query \(X_{}\) and a backdoored model \(f^{}\), the defender should accurately and efficiently determine whether \(X_{}\) is a clean input or not. For example, a common objective for the defender is to maximize the true positive rate, which represents the correct identification of backdoor samples, while simultaneously minimizing the probability of misclassifying clean samples as backdoors.

## 3 Proposed framework of Conformal Backdoor (CBD) for backdoor defenses

We first thoroughly characterize the inference-stage backdoor detection problem by examining binary classification/hypothesis testing, which encompasses many existing online backdoor defense methods. We then discuss the difficulties associated with the problem and the possible limitations of existing methods before proposing our novel detection framework.

### Backdoor detection formulation

Given a backdoored model \(f^{}\) and a validation dataset \(^{}\), upon receiving a test query \(X_{}^{d}\), the defender faces a binary classification/hypothesis testing problem:

\[_{0}:T(X_{}) T(S); S_{},_{1}:T(X_{}) T(S); S_{ },\]

where \(T()\) is a defender-constructed transformation on the original input \(X_{}\) and \(X Y\) means that \(X\) and \(Y\) share the same distribution. Here, we note that the \(T()\) typically depends on both the backdoored model \(f^{}\) and the validation dataset \(^{}\), to reflect special properties of backdoor data, e.g., the predicted value \(T(X_{})=f^{}(X_{})\) and the latent representation \(T(X_{})=^{}(X_{})\). Also, it is worth mentioning that the true label \(Y_{}\) is not included since it is unknown to the defender.

**Remark 1** (Unknown backdoor distribution \(_{}\)).: The \(_{0}\) represents the scenario where the input originates from the clean distribution, while \(_{1}\) corresponds to the input not being derived from the clean distribution. It is important to emphasize that \(_{1}\) is not formulated to test if \(X_{}\) comes from a backdoor data distribution \(_{}\), specifically, \(_{1}:T(X_{}) T(S)\) for \(S_{}\). This is because, in real-world scenarios, the backdoor data distribution, \(_{}\), is unknown to the defender. This poses a significant challenge in deriving rigorous results on detection power. In particular, without any distributional assumption on \(_{}\), it is not possible to establish provable results on detection power.

To approach the above problem, defenders will construct a detector \(g()\), specified by

\[g(X;s,)=\{1&(), &s(T(X))\\ 0&(),&s(T(X))<.,\]

where \(\) is a threshold value and \(s()\) is a scoring function which takes the transformed input \(T(X_{})\) and output a value in \(\) to indicate its chance of being a clean sample.

**Remark 2** (Distinguishing our approach from backdoor defenses focused on training-data purification).: There is a series of training-stage backdoor defense methods that have been developed with the goal of creating a clean model by identifying and removing potentially contaminated samples within the training data . While these methods share the concept of distinguishing between clean and backdoor data, they differ from our approach in two significant ways. Firstly, these methods benefit from working with both clean and backdoor training data, giving them insights into both data categories. Secondly, their primary objective is to construct a clean model by singling out a subset of the training data based on specific characteristics of all training examples. Consequently, their evaluation criteria revolve around the purified model's ability to accurately classify clean data and protect against backdoor attacks.

In contrast, our approach serves as an inference-stage defense without access to the training data and no capability to influence the training process, including adjustments to model parameters. Similar to other inference-stage defenses , we operate under the assumption that the defender has access to a limited set of clean validation data without prior knowledge of future backdoor test inputs. Our goal is to detect forthcoming backdoor inputs, and our evaluation metric is based on the AUCROC score of the detector.

### Backdoor conformal detection

The main challenge for the defender is to devise an effective and efficient detector \(g()\) to identify backdoor samples. Several evaluation metrics can be utilized to assess the detector's performance, including AUCROC and F1 score. Given the stealthiness of backdoor attacks, it is crucial for the defender to employ an evaluation method that offers provable guarantees regarding detection performance. To meet this requirement, it becomes necessary to consider both Type-I and Type-II errors within the hypothesis testing framework. Thus, we adopt the Neyman-Pearson criterion for our evaluation, ensuring a comprehensive assessment of our detection approach. Formally, the performance of a detector \(g\) can be described by its true positives rate (\(\)): \(g(X;)=1 X}\) and its false positive rate (\(\)): \(g(X;)=1 X}\). The defender aims to find an optimal detection method \(g\) in the Neyman-Pearson (NP) paradigm:

\[\,g(X;)=1 X}g(X;)=1 X},\]

where \((0,1)\) is the defender pre-specified upper bound on the \(\). Proving guarantees on the (FPR) for a wide range of score functions or detectors is difficult because the defender lacks knowledge about clean data distribution. This limits their ability to determine the distribution of a score function \(s\) under \(_{}\), making it hard to control the \(\).

To ensure the performance of the false positive rate, we propose a detection method based on the conformal prediction framework . The conformal prediction is a paradigm for creating statistically rigorous uncertainty sets/intervals for the predictions of black-box models without explicit distribution assumption on data. The central idea is that by using an appropriate score function, the empirical rank or quantile of the distribution will converge to the population version, which is guaranteed by the uniform convergence of CDFs. To be more specific, as outlined in Procedure 1, the defender will begin by computing the scores of the validation points using a validation dataset \(D^{}\). Next, the defender will set the decision value \(_{,s}\) to satisfy the

\[_{}(_{,s})=1-+,\] (1)where \((0,1)\) is the violation rate describing the probability that the (\(\)) exceeds \(\), and \(_{}\) is the empirical cumulative distribution function of the scores on the validation data \(\{s(T(x_{i}))\}_{i=1}^{n}\). In the case where \(>\), we set the thresholding value \(\) to be the maximum of \(\{s(T(x_{i}))\}_{i=1}^{n}\).

```
0: querying input \(X_{}\), clean validation dataset \(D^{}=\{(X_{i},Y_{i})\}_{i=1}^{n}\), transformation method \(T()\), score function \(s(T())\), desired false positive rate \((0,1)\), violation rate \((0,1)\) ```

```
1: Receiving a future query sample \(X_{}\)
2:for\(i=1\) to \(n\)do
3: Calculate \(s_{i}=s(T(X_{i}))\) // \(X_{i} D^{}\)
4:endfor
5: Select the decision threshold \(_{,s}\) according to Equation (1).
6: Determine if \(X_{}\) is a clean sample based on \(s(T(X_{}))_{,s}\) ```

**Output:** The decision whether the sample \(X_{}\) is a clean or backdoor sample

**Algorithm 1** Conformal Backdoor Detection (CBD)

The next result shows that if a future test input comes from the same distribution as the validation data, the above algorithm can achieve a false positive rate bounded by \(\) with high probability.

**Theorem 1** (Conditional False positive rate of Algorithm 1).: Given any pre-trained backdoored classifier \(f\), suppose that the validation dataset \(D^{}\) and the test data \((X_{},Y_{})\) are IID drawn from the clean data distribution \(_{}\). Given any \((0,1)\), for any score function \(s\), such that the resulting scores \(\{s(X_{i})\}_{i=1}^{n}\) remain IID and continuous, the associated backdoor conformal detector \(g(;s,_{,s})\) as specified in Procedure 1 satisfies

\[g(X_{};s,_{,s})=1\,() D^{},\]

with probability at lease \(1-\) for any \((0,1)\) such that \(>\).

### Optimal score functions for CBD

In the preceding section, we presented the CBD framework and established that when utilizing suitable score functions, which preserve the independence and identical distribution of transformed samples, the associated backdoor conformal detector achieves an FPR within the range of \(\). This prompts an inquiry: what constitutes the optimal score function (and its corresponding conformal detector) that maximizes the TPR while maintaining an FPR within a predefined \(\) threshold?

At a high level, deriving theoretical results on the \(\) without distributional assumptions on the backdoor data is not possible, as previously discussed. Therefore, our first step is to investigate the unique structure of backdoor data. We note that the distribution of backdoor data depends on both the clean data distribution and the learning model used. This is because backdoor data is generated by transforming clean data with backdoor triggers \(_{1}\), and the effectiveness of the trigger in achieving the attacker's goal may vary under different models.

To aid in the theoretical analysis, we will be making certain assumptions about the components mentioned above. However, it is important to note that the practical algorithm presented in the next section does not rely on these assumptions. The development in this subsection primarily serves to derive technical insights. Specifically, we will assume that the Suppose that the \(()\) marginal clean data is normally distributed with mean \(0\) and covariance \(\), \(()\) the attacker employs a linear classifier \(f_{}(x)=\{^{}x>0\}\) for \(^{d}\), and \(()\) the attacker applies the backdoor transformation \(_{1}(x)=x+\), where \(_{}\{u^{d}\|u\|_{2}=c, c>0\}\). It is important to mention that the defender does not possess knowledge of the exact backdoor trigger \(^{*}\) utilized by the attacker. However, the defender is aware of the set of potential backdoor triggers \(_{c}\). Also, we note that the \(_{2}\) norm constraint on \(\) is practically important since backdoor triggers that are imperceptible to humans, i.e., those with a smaller norm, are more favorable than human-perceptible triggers, e.g., a square patch. Given the availability of distributional information, our analysis in this section will be conducted at the population level, directly working with the Attacker's Goal as specified in Section 3.

Taking into account all the aforementioned information, the defender can construct a uniformly most powerful detection rule if they possess knowledge of the exact backdoor trigger employed by the attacker, following the principles of the Neyman-Pearson Lemma . This raises the question for the defender: when they obtain the backdoored model based on the attacker's objective described in Section 3, are there multiple backdoor triggers \(_{c}\) that could lead to this specific model? If the answer is yes, the defender would have complete knowledge of the backdoor trigger and, consequently, the backdoor data distribution, enabling them to derive the most effective detection rule. To address this concern, we provide a definitive answer to the question as follows.

**Theorem 2** (Uniqueness of the backdoor trigger).: Under Assumptions \(()\), \(()\), and \(()\) as specified above, for any \(^{*}^{d}\) that satisfies the Attacker's Goal, as specified in Section 3, the backdoor trigger \(^{*}_{c}\) that corresponds to this attacker's backdoored classifier \(^{*}\), is _unique_ and admits an explicit form.

**Remark 3** (The defender has full knowledge about the form of the backdoor trigger and the backdoor data distribution).: The above result indicates that there is a unique correspondence between a backdoor trigger \(^{*}_{c}\) and the resulting backdoored classifier \(^{*}\) that fulfills the Attacker's Goal. Therefore, when the attacker provides the backdoored model to the defender, the defender is aware that only one specific backdoor trigger is associated with this particular model. As a result, the defender possesses complete knowledge of the distribution of backdoor data by utilizing the explicit form of the backdoor trigger, which is included in the appendix.

**Theorem 3** (Uniformly most powerful detection rule).: Under the same setup in Theorem 2, the defender's optimal score function resulting in the largest \(\) given a \(\) is in the form of:

\[s(Z)^{-1}Z-(Z-^{*})^{}^{-1}(Z- ^{*}))}.\] (2)

The above optimal detection rule is based on the likelihood ratio, which compares the probability of a future sample \(Z\) from the clean distribution with that from the backdoor distribution. Through empirical evaluation, we have found that the proposed score function, when using the actual backdoor trigger instead of \(^{*}\), effectively distinguishes between clean and backdoor data in various scenarios, including 12 state-of-the-art backdoor attacks in CV benchmarks (e.g., CIFAR10) and NLP benchmarks (such as SST-2). The results are illustrated in Figure 2.

### Practical proxy: Shrunk-Covariance Mahalanobis (SCM)

The previous analysis shows that the Mahalanobis distance between the clean and data is the optimal detection rule under known Gaussian distributions. However, in the real-world scenario, the defender knows neither the clean nor the backdoor data distributions. Therefore, the two exact distances as specified in Equation (2) above can not be computed.

To overcome this challenge, a proximal approach is proposed in the following. Recall that the defender has a small set of clean validation dataset which is assumed to have the same distribution as the clean training data. Hence, one feasible way to calculate the theoretically optimal detector in Equation (2) is to use the empirical Mahalanobis distance calculated based on a clean validation, namely \(M(z,D^{})(z-)^{}V(z-)\), where \(\) is the empirical mean of a \(D^{}\), and \(V\) is the inverse of the sample covariance matrix of \(D\). The choice of this selection is because we empirically observed that backdoor and clean data have significantly different values under the decision rule derived in Eq. (2) as shown in Figure 2.

A significant challenge in using Mahalanobis distance arises in high-dimensional scenarios, particularly when estimating the precision matrix, i.e., the inverse of the covariance matrix. The sample

Figure 2: The mean values of \(s(Z)\) (vertical-line bar for clean data, slash-line bar for backdoor data) in Eq. (2), by replacing the \(^{*}\) with the real backdoor trigger, on GTSRB with ResNet18 over different backdoor attacks.

covariance matrix, a popular choice, exhibits numerical instability in large data dimensions [57; 58], leading to an ill-conditioned matrix that is difficult to invert . The estimation errors increase rapidly with data dimension \(d\), causing issues for high-dimensional latent embeddings in deep neural networks, like ResNet18 and BERT. Consequently, MD may not preserve order information for \(D^{}\), impacting the \(\) in Theorem 1.

To address this issue, we consider regularizing the sample covariance matrix through shrinkage techniques [61; 62]. At a high level, the shrinkage technique is used to adjust the eigenvalues of a matrix. This adjustment involves changing the ratio between the smallest and largest eigenvalues of a dataset's covariance matrix. One way to do this is by shifting each eigenvalue by a certain amount, which in practice leads to the following, _Shrunk-Covariance Mahalanobis_ (SCM) score function, defined as,

\[s(^{}(z);f^{},D^{})(^{}(z)-)^{}M_{}^{-1}(^{}(z)-),\] (3)

where \(^{}()\) is the latent representation of the backdoored DNNs, \(\) is the mean of \(\{^{}(x_{i})\}_{i=1}^{n}\) and \(M_{}(1-)S+((S)/d)I_{d}\), with \(S\) being the sample covariance of \(\{^{}(x_{i})\}_{i=1}^{n}\), \(()\) is the trace operator, and \((0,1)\) is the shrinkage level. The use of the latent space representation \(^{}()\) instead of the original data information is motivated by the difficulty in modeling distributions over the original digital space . The shrinkage parameter can be selected via cross-validation . Empirically, we observed that the proposed SCM accurately distinguishes between clean and backdoor data in various backdoor attack scenarios, spanning both CV and NLP domains, as demonstrated from the histograms in Figure 3.

## 4 Experiments

In this section, we conduct extensive experiments to evaluate our proposed methods. The empirical results match our theoretical results in terms of the false positive rate (\(\)), and the detection power of our methods outperforms state-of-the-art defenses on a wide range of benchmarks. We further provide a runtime analysis of our method and ablation studies for hyperparameters in the Appendix. All backdoor datasets and models were obtained by re-running publicly available codes (details in the appendix) The experiments were conducted on cloud computing machines equipped with Nvidia Tesla V100s.

### Setups

**Datasets** We evaluate our proposed method on three popular **(I) CV** image classification datasets: CIFAR10 , GTSRB , and Tiny-ImageNet , and on two **(II) NLP** datasets: SST2 (Stanford Sentiment Treebank v2)  and IMDB . Results for SST2 are presented in the main paper, while those for IMDB are deferred to the Appendix. We include a short introduction to each dataset in the Appendix.

**Models (I) CV** We use ResNet18  and **(II) NLP** the base uncased-BERT  as the default model architecture for our experiments in the main text. Ablations studies on different model architectures, e.g., VGG 19  and WideResNet , and mode configurations are included in the Appendix.

**Metrics** We consider two evaluation metrics: (1) the \(\) (2) the area under the receiver operating characteristic curve (AUCROC). All experiments are independently repeated \(10\) times.

**Attacks (I) CV** In the main text, we evaluate our proposed method on ten popular attacks, including data-and-label poisoning attacks: BadNets , Blended , Trojan ), clean label attack: SIG , (3) attacks with invisible/sample-wise triggers: Dynamic , SSBA , WaNet , (4) and adaptive backdoor attacks: TaCT , Adaptive Blend , Adaptive Patch . **(II) NLP** We evaluate the proposed method on two state-of-the-art attacks: (1) SOS  and LWS . In the appendix, we assess the performance of our methods against two additional categories of backdoor

Figure 3: Histograms of our proposed SCM on CIFAR10 using ResNet 18 for different backdoor attacks in both CV and NLP domains. We noticed distinct separations in SCM scores between clean and backdoor data, in all scenarios.

attacks, which are tailored to exploit vulnerabilities in our techniques. Configuration details of these attacks, including trigger patterns, poisoning rates, and hyperparameters, are included in the Appendix.

**Performance of Attacks (I) CV** On CIFAR10 and GTSRB datasets, the backdoored model achieves clean accuracy rates exceeding \(93\%\) and backdoor accuracy rates exceeding \(98\%\) for all the previously mentioned backdoor attacks. The exceptions are the SIG and Adaptive Blend attacks, which result in backdoor accuracy rates of around \(80\%\). **(II) NLP** On SST2 and IMDB datasets, the backdoored model attains clean accuracy rates surpassing \(90\%\) and backdoor accuracy rates exceeding \(96\%\) for the mentioned backdoor attacks.

**Defenses** For both **(I) CV** and **(II) NLP**, in the main text, we compare our proposed method with three detection-based backdoor defenses in the literature, namely, STRIP , \(_{2}\) based-defense , and MAD (Mean Absolute Deviation)-based defense. In the appendix, we compare our methods with two recent detection-based defenses designed for CV backdoor attacks, SCALEUP and FREQ , as well as filtering-based training-stage defenses, SPECTRE  and ABL .

### Main results

**FPR on CIFAR10 and GTSRB** We assess the \(\) performance of our proposed method CBD-SCM (SCM score function within the CBD framework) on, CIFAR10 and GTSRB datasets by varying the defender pre-specified false positive rate \(\). Each plot corresponds to a specific backdoor attack and shows the \(\) performance of our proposed method under that attack. For each plot, we conduct ten independent experiment replications and calculate the mean, along with the \( 1\) standard error bars shown in the shaded regions in Fig. 4 for GTSRB and Fig. 5.1 for CIFAR10 in the appendix. The results demonstrate that the \(\) of our method (in red dots) consistently matches the theoretical upper bounds (in blue triangles), supporting the findings presented in Theorem 1.

**Detection Power (AUCROC) on CIFAR10 and GTSRB** We conduct a thorough evaluation of the detection performance of our proposed method against all ten distinct backdoor attacks and report the corresponding AUCROC values in Table 1. Our method achieves the best performance in **all cases** on both datasets. Specifically, we observe a remarkable increase in AUCROC, up to \(300\%\), for the more advanced backdoor attacks such as non-patch-based backdoor attacks (WaNet and SSBA) and latent space adaptive attacks (Adaptive Blend and Adaptive Patch).

**Detection Power (ROC) on Tiny-ImageNet** We further evaluate the detection performance of our proposed method on Tiny-ImageNet against both classical patch-based attacks (e.g., Blended) and more advanced non-patch-based attacks (e.g., WaNet). We visualize the ROC curves for our method in Figure 5 (a) and (b). It shows a significant improvement in the \(\) at low false positive rate regions compared with other methods.

**Detection Power (ROC) on SST2 (NLP)** We assess the detection performance of our proposed method on SST2 (NLP) using the base uncased-BERT model. Specifically, we generate ROC curves for our methods under two advanced backdoor attacks, as illustrated in Figure 5 (c) and (d). The results indicate that our method outperform the STRIP method significantly, but are relatively less

Figure 4: The mean \(\) of our proposed method on GTSRB are shown in each plot, which is independently replicated \(10\) times. The solid line represents the mean value, and the standard errors are \(<0.01\) for all cases. Our method’s \(\) consistently match the theoretical upper bounds.

effective under the SOS attack compared to the other two defenses that use latent representations. These findings suggest that the current NLP attacks retain a considerable amount of information in the latent representations that can be utilized to differentiate between clean and backdoor data.

## 5 Conclusion

This paper addresses defending against backdoor attacks by detecting backdoor samples during the inference stage. We introduce a mathematical formulation for the problem, highlight potential challenges, and propose a generic framework with provable false positive rate guarantees. Within this framework, we derive the most powerful detection rule under classical learning scenarios and offer a practical solution based on the optimal theoretical approach. Extensive experiments in both CV and NLP domains consistently validate our theory and demonstrate the efficacy of our method.

**Limitation and future work**. While our work presents a comprehensive framework for defending against inference-stage backdoor attacks, there are some limitations that warrant further investigation. Firstly, future studies could explore the optimal detection rule in more practical scenarios where only partial information regarding the clean and backdoor data distributions is available. An example is Assisted Learning [66; 67; 68], a decentralized learning scenario where learners only provide processed data information during the inference stage. Secondly, it would be interesting to extend the proposed approach to backdoor detection in variety of Federated Learning scenarios [69; 70; 17; 11]. Thirdly, it is worth investigating the use of general distributions conditional on side information (e.g., ) to design the optimal detection rule. Lastly, it would be helpful to explore information beyond the penultimate layer, e.g., the topology of neuron activations , for backdoor detection.

The **Appendix** contains additional details on the CBD framework, more introductions to the related work, ethical considerations, more extensive ablation studies, and all the technical proofs.

    &  &  \\  Defense \(\) & STRIP & \(_{2}\) & MAD & SCM & STRIP & \(_{2}\) & MAD & SCM \\  BadNets & \(\) & \(\) & \(0.99\) & \(\) & \(\) & \(0.97\) & \(0.97\) & \(\) \\ Blended & \(0.61\) & \(0.86\) & \(0.78\) & \(\) & \(0.80\) & \(0.69\) & \(0.70\) & \(\) \\ TrojanNN & \(0.55\) & \(0.90\) & \(0.76\) & \(\) & \(0.50\) & \(0.90\) & \(0.89\) & \(\) \\ SIG & \(0.72\) & \(0.51\) & \(0.33\) & \(\) & \(0.41\) & \(0.32\) & \(0.31\) & \(\) \\ Dynamic & \(0.84\) & \(0.99\) & \(0.99\) & \(\) & \(\) & \(\) & \(\) & \(\) \\ SSBA & \(0.68\) & \(0.67\) & \(0.66\) & \(\) & \(0.80\) & \(0.22\) & \(0.22\) & \(\) \\ WaNet & \(0.39\) & \(0.41\) & \(0.40\) & \(\) & \(0.54\) & \(0.31\) & \(0.21\) & \(\) \\ TacT & \(0.68\) & \(0.45\) & \(0.37\) & \(\) & \(0.51\) & \(0.55\) & \(0.44\) & \(\) \\ Adaptive Blend & \(0.69\) & \(0.84\) & \(0.75\) & \(\) & \(0.72\) & \(0.88\) & \(0.89\) & \(\) \\ Adaptive Patch & \(0.76\) & \(0.85\) & \(0.79\) & \(\) & \(0.33\) & \(0.56\) & \(0.49\) & \(\) \\   

Table 1: AUCROC results for CIFAR10 and GTSRB. Mean values of ten replicates are reported, with standard errors below \(0.01\) for all cases. The best-performing method(s) are indicated in **boldface**.

Figure 5: AUCROC on Tiny-ImageNet and SST-2 NLP. For the Tiny-ImageNet dataset, our method significantly outperforms the existing methods under both patch-based attacks (Blended) and the more advanced non-patch-attack (WaNet). On the NLP SST2 benchmark, our proposed method consistently outperforms the STRIP method.